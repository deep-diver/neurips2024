[{"type": "text", "text": "The Power of Resets in Online Reinforcement Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "Zakaria Mhammedi Google Research mhammedi@google.com ", "page_idx": 0}, {"type": "text", "text": "Dylan J. Foster Alexander Rakhlin Microsoft Research MIT dylanfoster@microsoft.com rakhlin@mit.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Simulators are a pervasive tool in reinforcement learning, but most existing algorithms cannot efficiently exploit simulator access\u2014particularly in high-dimensional domains that require general function approximation. We explore the power of simulators through online reinforcement learning with local simulator access (or, local planning), an RL protocol where the agent is allowed to reset to previously observed states and follow their dynamics during training. We use local simulator access to unlock new statistical guarantees that were previously out of reach: ", "page_idx": 0}, {"type": "text", "text": "1. We show that MDPs with low coverability [63]\u2014a general structural condition that subsumes Block MDPs and Low-Rank MDPs\u2014can be learned in a sample-efficient fashion with only $Q^{\\star}$ -realizability (realizability of the optimal state-value function); existing online RL algorithms require significantly stronger representation conditions. ", "page_idx": 0}, {"type": "text", "text": "2. As a consequence, we show that the notorious Exogenous Block MDP problem [22] is tractable under local simulator access. ", "page_idx": 0}, {"type": "text", "text": "The results above are achieved through a computationally-inefficient algorithm. We complement them with a more computationally efficient algorithm, RVFS (Recursive Value Function Search), which achieves provable sample complexity guarantees under strengthened statistical assumption known as pushforward coverability. RVFS can be viewed as a principled, provable counterpart to a successful empirical paradigm that combines recursive search (e.g., MCTS) with value function approximation. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Simulators are a widely used tool in reinforcement learning. Many of the most well-known benchmarks for reinforcement learning research make use of simulators (Atari [9], MuJoCo [55], OpenAI Gym [11], DeepMind Control Suite [53]), and high-quality simulators are available for a wide range of real-world control tasks, including robotic control [45, 2], autonomous vehicles [10, 6], and game playing [51, 52]. Simulators also provide a useful abstraction for planning with a known or learned model, an important building block for many RL techniques [48]. Yet, in spite of the ubiquity of simulators, almost all existing research into algorithm design\u2014empirical and theoretical\u2014has focused on the online reinforcement learning (where only trajectory-based feedback is available), and does not take advantage of the extra information available through the simulator. Relatively little is known about the full power of RL with simulator access, either in terms of algorithmic principles or fundamental limits. ", "page_idx": 0}, {"type": "text", "text": "We explore the power of simulators through online reinforcement learning with local simulator access (RLLS for short), also known as local planning [57, 40, 3, 59, 65, 66]. Here, the agent learns by repeatedly executing policies and observing the resulting trajectories (as in online RL), but is allowed to reset to previously observed states and follow their dynamics during training. ", "page_idx": 0}, {"type": "text", "text": "Empirically, algorithms based on local simulators have received limited investigation, but with promising results. Notably, the Go-Explore algorithm [19, 20] uses local simulator access to achieve state-of-the-art performance for Montezuma\u2019s Revenge (a difficult Atari game that requires systematic exploration), beating the performance of the best agents trained with online RL [7, 27] by a significant margin that has yet to be closed. The successful line of research on AlphaGo and successors [51, 52, 48] also uses local simulator access, albeit at test time in addition to training time. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "These results suggest that developing improved algorithm design principles for RL with local simulator access could have significant practical implications, but current theoretical understanding of local simulators is limited. Recent work has shown that local simulator access has provable beneftis for reinforcement learning with various types of linear function approximation [57, 40, 3, 65, 59], but essentially nothing is known for RL problems in large state spaces that demand general, potential neural function approximation. This leads us to ask: Can we develop algorithms for reinforcement learning with general function approximation that provably benefit from local simulator access? ", "page_idx": 1}, {"type": "text", "text": "From an algorithm design perspective, perhaps the greatest challenge in using local simulators to speed up learning is to understand which states are \u201cinformative\u201d in the sense that we should prioritize revisiting them. Here, we are faced with a chicken-and-egg problem: to understand which states to prioritize, we must explore and gather information, but it is unclear how to do so efficiently unless we already have a way to understand which states are informative. It is natural to let function approximation guide us; to this end, recent research [40, 65, 59] on linearly-parameterized RL with local simulators makes use of core-sets: small, adaptively chosen sets of informative state-action pairs designed to cover the feature space and enable efficient value function learning. Core-sets facilitate sample complexity guarantees for linear models that are not possible without local simulator access (e.g., [40]). Yet, for general function classes\u2014particularly rich models like neural networks that do not readily support extrapolation\u2014defining a suitable notion of core-set is challenging. Consequently, existing techniques have yet to meaningfully leverage local simulator access beyond the linear regime. ", "page_idx": 1}, {"type": "text", "text": "1.1 Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We show that local simulator access unlocks new guarantees for online RL with general value function approximation\u2014statistical and computational\u2014that were previously out of reach. ", "page_idx": 1}, {"type": "text", "text": "Sample-efficient learning. We show that MDPs with low coverability [63]\u2014a general structural condition that subsumes Block MDPs and Low-Rank MDPs\u2014can be learned in a sample-efficient fashion with only $Q^{\\star}$ -realizability (that is, realizability for the optimal state-action value function). This is achieved through a new algorithm, SimGolf, that augments the principle of global optimism with local simulator access, and improves upon the best existing guarantees for the fully online RL setting, which require significantly stronger representation conditions. As a consequence, we show for the first time that the notoriously challenging Exogenous Block MDP (ExBMDP) problem [22, 21] is tractable in its most general form under local simulator access. ", "page_idx": 1}, {"type": "text", "text": "Practical, computationally efficient learning. Our results above are achieved through a computationally inefficient algorithm. We complement them with a practical and computationally efficient algorithm, RVFS (\u201cRecursive Value Function Search\u201d), which achieves sample-efficient learning guarantees with general value function approximation under a strengthened, yet novel, statistical assumption known as pushforward coverability [62]. Assuming either i) realizability of the optimal state-value function $V^{\\star}$ and a state-action gap or ii) realizability of $V^{\\pi}$ for all $\\pi$ , RVFS achieves polynomial sample complexity in a computationally efficient fashion, and leads to guarantees for a new class of Exogenous Block MDPs with weakly correlated exogenous noise. RVFS explores by building core-sets with a novel value function-guided scheme, and can be viewed as a principled counterpart to algorithms including MCTS and AlphaZero [51, 52, 19, 20, 66], that combine recursive search with value function approximation. Compared to these approaches, RVFS is designed to provably address stochastic environments and distribution shift. ", "page_idx": 1}, {"type": "text", "text": "Paper organization. Section 2 introduces the local simulator framework. Section 3 presents our main sample complexity guarantees, and Section 4 gives computationally efficient algorithms. All proofs are deferred to the appendix. ", "page_idx": 1}, {"type": "text", "text": "2 Setup: Reinforcement Learning with Local Simulator Access ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We consider an episodic reinforcement learning setting. A Markov Decision Process (MDP) is a tuple $\\mathcal{M}=(\\mathcal{X},\\mathcal{A},T,\\bar{R},H)$ , where $\\mathcal{X}$ is a (large/potentially infinite) state space, $\\boldsymbol{\\mathcal{A}}$ is the action space (we abbreviate $A=\\left|{\\mathcal{A}}\\right|)$ , $H\\in\\mathbb{N}$ is the horizon, $R=\\{R_{h}\\}_{h=1}^{H}$ is the reward function (where $R_{h}:\\mathcal{X}\\times\\mathcal{A}\\to$ $[0,1])$ and $T=\\{T_{h}\\}_{h=0}^{H}$ is the transition distribution (where $T_{h}:\\mathcal{X}\\times\\mathcal{A}\\rightarrow\\Delta(\\mathcal{X}))$ , with the convention that $T_{0}(\\cdot\\,|\\,\\emptyset)$ is the initial state distribution. A policy is a sequence of functions $\\pi=\\{\\pi_{h}:\\mathcal{X}\\rightarrow$ $\\Delta(A)\\}_{h=1}^{H}$ ; we use $\\Pi_{S}$ to denote the set of all such functions. When a policy is executed, it generates a trajectory $(\\pmb{x}_{1},\\pmb{a}_{1},\\pmb{r}_{1}),\\dots,(\\pmb{x}_{H},\\pmb{a}_{H},\\pmb{r}_{h})$ via the process $\\begin{array}{r}{\\pmb{a}_{h}\\sim\\pi_{h}(\\hat{\\pmb{x}}_{h}),\\dot{\\pmb{r}}_{h}\\sim R_{h}(\\pmb{x}_{h},\\pmb{a}_{h}),\\pmb{x}_{h+1}\\sim}\\end{array}$ $T_{h}(\\cdot\\mid{\\pmb x}_{h},{\\pmb a}_{h})$ , initialized from $\\pmb{x}_{1}\\sim T_{0}(\\cdot\\,|\\,\\emptyset)$ (we use $x_{H+1}$ to denote a terminal state with zero reward). We write $\\mathbb{P}^{\\pi}[\\cdot]$ and $\\mathbb{E}^{\\pi}[\\cdot]$ to denote the law and expectation under this process. For a policy $\\pi$ , $\\begin{array}{r}{J(\\pi):=\\mathbb{E}^{\\pi}\\big[\\sum_{h=1}^{H}r_{h}\\big]}\\end{array}$ denotes expected reward, and the value functions are given by $\\begin{array}{r}{V_{h}^{\\pi}(x):=\\mathbb{E}^{\\pi}\\big[\\sum_{h^{\\prime}=h}^{H}r_{h^{\\prime}}\\mid x_{h}=x\\big].}\\end{array}$ , and $\\begin{array}{r}{Q_{h}^{\\pi}(x,a):=\\mathbb{E}^{\\pi}\\big[\\sum_{h^{\\prime}=h}^{H}r_{h^{\\prime}}\\;|\\;x_{h}=x,\\pmb{a}_{h}=a\\big]}\\end{array}$ . We denote by $\\pi^{\\star}$ the optimal deterministic policy that maximizes $Q^{\\pi^{\\star}}$ , and write $Q^{\\star}:=Q^{\\pi^{\\star}}$ and $V^{\\star}:=V^{\\pi^{\\star}}$ . ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Online reinforcement learning with a local simulator. In the standard online reinforcement learning framework, the learner repeatedly interacts with an (unknown) MDP by executing a policy and observing the resulting trajectory, with the goal of maximizing the total reward. Formally, for each episode $\\tau\\in\\big[N_{\\mathrm{episodes}}\\big]$ , the learner selects a policy $\\pi^{(\\tau)}\\,=\\,\\Bigl\\{\\pi_{h}^{(\\tau)}\\Bigr\\}_{h=1}^{H}$ , executes it in the underlying MDP $\\mathcal{M}^{\\star}$ and observes the trajectory $\\{(\\mathbf{x}_{h}^{(\\tau)},\\mathbf{a}_{h}^{(\\tau)},\\mathbf{r}_{h}^{(\\tau)})\\}_{h=1}^{H}$ . After all $N_{\\mathrm{episodes}}$ episodes conclude, the learner produces a policy with the goal of minimizing the risk given by $\\mathbb{\\bar{E}}[J(\\pi^{\\star})-J(\\widehat{\\pi})]$ . ", "page_idx": 2}, {"type": "text", "text": "In online RL with local simulator access, or RLLS, [57, 40, 65, 59, 66], we augment the online RL protocol as follows: At each episode $\\tau\\in[N]$ , instead of starting from a random initial state $x_{1}\\overset{.}{\\sim}T_{0}(\\cdot\\mid\\emptyset)$ , the agent can reset the MDP to any layer $h\\,\\in\\,[H]$ and any state $x_{h}$ previously encountered, and proceed with a new episode starting from $x_{h}$ . As in the online RL protocol, the goal is to produce a policy $\\widehat{\\pi}\\in\\Pi_{\\mathbb{S}}$ such that $\\mathbb{E}[J(\\pi^{\\star})-\\bar{J}(\\widehat{\\pi})]\\leq\\varepsilon$ with as few episodes of interaction as possible; our main results take $N_{\\mathrm{episodes}}=\\mathrm{poly}(C,\\varepsilon^{-1})$ for a suitable problem parameter $C$ . ", "page_idx": 2}, {"type": "text", "text": "Executable versus non-executable policies. We focus on learning policies that can be executed without access to a local simulator (in other words, the local simulator used at train time, but not test time). Some recent work using local simulators for RL with linear function approximation [57] considers a more permissive setting where the final policy $\\pi$ produced by the learner can be non-executable; our function approximation requirements can be slightly relaxed in this case. ", "page_idx": 2}, {"type": "text", "text": "Definition 2.1 (Non-executable policy). We refer to a policy $\\pi$ for which computing $\\pi(x)\\in\\Delta({\\mathcal{A}})$ for any $x\\in\\mathscr{X}$ requires n local simulator queries as a non-executable policy with sample complexity $n$ . ", "page_idx": 2}, {"type": "text", "text": "Additional notation. For any $m,n\\in\\mathbb{N}$ , we denote by $[m\\ldots n]$ the integer interval $\\{m,\\ldots,n\\}$ . We also let $[n]\\;:=\\;[1\\ldots n]$ . We refer to a scalar $c\\,>\\,0$ as an absolute constant to indicate that it is independent of all problem parameters and use $\\widetilde O(\\cdot)$ to denote a bound up to factors polylogarithmic in parameters appearing in the expression. We define $\\pi_{\\mathsf{u n i f}}\\;\\in\\;\\Pi_{\\mathsf{S}}$ as the random policy that selects actions in $\\boldsymbol{\\mathcal{A}}$ uniformly. We define the occupancy measure for policy $\\pi$ via $d_{h}^{\\pi}(x,a)\\ :=\\ \\mathbb{P}^{\\pi}[{\\pmb x}_{h}\\ =\\ x,{\\pmb a}_{h}\\ =\\ a].$ . For functions $g\\;:\\;\\mathcal{X}\\,\\times\\,\\mathcal{A}\\;\\rightarrow\\;\\mathbb{R}$ and $f:\\,\\mathcal{X}\\,\\rightarrow\\,\\mathbb{R}$ , we define Bellman backup operators by $\\begin{array}{r}{\\bar{\\mathcal{T}}_{h}[g](x,a)=\\mathbb{E}\\big[r_{h}+\\operatorname*{max}_{a^{\\prime}\\in\\mathcal{A}}g\\big(x_{h+1},a^{\\prime}\\big)\\bigm|x_{h}=x,a_{h}=a\\big]}\\end{array}$ and $\\mathcal{P}_{h}[f](x,a)=\\mathbb{E}[\\pmb{r}_{h}+f(\\pmb{x}_{h+1})\\mid\\pmb{x}_{h}=\\bar{x},\\pmb{a}_{h}=a]$ . For a stochastic policy $\\pi\\in\\Pi_{\\ S}$ , we will occasionally use the bold notation $\\pi_{h}(x)$ as shorthand for the random variable $\\pmb{a}_{h}\\sim\\pi_{h}(x)\\in\\Delta(\\mathcal{A})$ . For a function $f:A\\rightarrow\\mathbb{R}$ , we write $a^{\\prime}\\in\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}f(a)$ to denote the action that maximizes $f$ . If there are ties, we break them by picking the action with the smallest index; we assume without loss of generality that actions in $\\boldsymbol{\\mathcal{A}}$ are index from $1,\\ldots,|A|$ . ", "page_idx": 2}, {"type": "text", "text": "3 New Sample-Efficient Learning Guarantees via Local Simulators ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This section presents our most powerful results for RLLS. We present a new algorithm for learning with local simulator access, SimGolf (Section 3.1), and show that it enables sample-efficient RL for MDPs with low coverability [63] using only $Q^{\\star}$ -realizability (Section 3.2). We then give implications for the Exogenous Block MDP problem (Section 3.3). ", "page_idx": 2}, {"type": "text", "text": "Function approximation setup and coverability. To achieve sample complexity guarantees for online reinforcement learning that are suitable for large, high-dimensional state spaces, we appeal to value function approximation. We assume access to a function class $\\mathcal{Q}\\subset(\\mathcal{X}\\times\\mathcal{A}\\times[H]\\rightarrow[0,H])$ that contains the optimal state-action value function $Q^{\\star}$ ; we define $\\mathcal{Q}_{h}=\\left\\{Q_{h}\\mid Q\\in\\mathcal{Q}\\right\\}$ . ", "page_idx": 2}, {"type": "text", "text": "Assumption 3.1 ( $Q^{\\star}$ -realizability). For all $h\\in[H]$ , we have $Q_{h}^{\\star}\\in\\mathcal{Q}_{h}$ . ", "page_idx": 2}, {"type": "text", "text": "$Q^{\\star}$ -realizability is widely viewed as a minimal representation condition for online RL [61, 17, 16, 39, 58, 56]. The class $\\mathcal{Q}$ encodes the learner\u2019s prior knowledge about the MDP, and can be parameterized by rich function approximators like neural networks. We assume for simplicity of exposition that $\\mathcal{Q}$ and $\\Pi$ are finite, and aim for sample complexity guarantees scaling with $\\log\\!\\left|\\mathcal{Q}\\right|$ and $\\mathrm{log}|\\Pi|$ ; extending our results to infinite classes via standard uniform convergence arguments is straightforward. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Coverability. Beyond representation conditions like realizability, online RL algorithms require structural conditions that limit the extent to which deliberately designed algorithms can be surprised by substantially new state distributions. We focus on a structural condition known as coverability [63], which is inspired by connections between online and offline RL. ", "page_idx": 3}, {"type": "text", "text": "Assumption 3.2. The coverability coefficient is $\\begin{array}{r}{C_{\\mathsf{c o v}}:=\\operatorname*{max}_{h\\in[H]}\\operatorname*{inf}_{\\mu_{h}\\in\\Delta(\\mathcal{X}\\times\\mathcal{A})}\\operatorname*{sup}_{\\pi\\in\\Pi_{\\mathsf{S}}}\\left\\|\\frac{d_{h}^{\\pi}}{\\mu_{h}}\\right\\|_{\\infty}.}\\end{array}$ ", "page_idx": 3}, {"type": "text", "text": "Coverability is an intrinsic strutural property of the underlying MDP. Examples of MDP families with low coverability include (Exogenous) Block MDPs, which have $C_{\\mathsf{c o v}}\\leq|\\bar{S}||\\bar{A}|$ , where $\\boldsymbol{S}$ is the latent state space [63], and Low-Rank MDPs, which have $C_{\\mathsf{c o v}}\\leq d|\\mathcal{A}|$ , where $d$ is the feature dimension [29]; importantly, these settings exhibit high-dimensional state spaces and require nonlinear function approximation. As in prior work [63, 4], our algorithms require no prior knowledge of the distribution $\\mu_{h}$ that achieves the minimum in Assumption 3.2. ", "page_idx": 3}, {"type": "text", "text": "3.1 Algorithm ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our main algorithm, SimGolf, is displayed in Algorithm 1. The algorithm is a variant of the GOLF method of Jin et al. [32], Xie et al. [63] with novel adaptations to exploit the availability of a local simulator. Like GOLF, SimGolf explores using the principle of global optimisim: At each iteration $t\\in[N]$ , it maintains a confidence set (or, version space) $\\mathcal{Q}^{(t)}\\subset\\mathcal{Q}$ of candidate value functions with low squared Bellman error under the data collected so far, and chooses a new exploration policy $\\pi^{(t)}$ by picking the most \u201coptimistic\u201d value function in this set. As the algorithm gathers more data, the confidence set shrinks, leaving only near-optimal policies. ", "page_idx": 3}, {"type": "text", "text": "The main novelty in SimGolf arises in the data collection strategy and design of confidence sets. Like GOLF, SimGolf algorithm constructs the confidence set $\\mathcal{Q}^{(t)}\\subset\\mathcal{Q}$ such that all value functions $g\\in\\mathcal{Q}^{(t)}$ have small squared Bellman error: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\sum_{i<t}\\mathbb{E}^{\\pi^{(i)}}\\Big[\\big(g_{h}(\\pmb{x}_{h},\\pmb{a}_{h})-\\mathcal{T}_{h}[g_{h+1}](\\pmb{x}_{h},\\pmb{a}_{h})\\big)^{2}\\Big]\\lesssim\\log\\vert\\mathcal{Q}\\vert,\\quad\\forall h\\in[H].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Due to the presence of the Bellman backup $\\tau_{h}[g_{h+1}]$ in Eq. (1), naively estimating squared Bellman error leads to the notorious double sampling problem. To avoid this, the approach taken with GOLF and related work [67, 32] is to adapt a certain de-biasing technique to remove double sampling bias, but this requires access to a value function class that satisfies Bellman completeness, a representation significantly more restrictive than realizability (e.g., Foster et al. [26]). ", "page_idx": 3}, {"type": "text", "text": "The idea behind SimGolf is to use local simulator access to directly produce high-quality estimates for the Bellman backup function $\\tau_{h}[g_{h+1}]$ in Eq. (1). In particular, for a given state-action pair $(x,a)\\ \\in\\ \\mathcal{X}\\,\\times\\,\\mathcal{A}$ , we can estimate the Bellman backup $\\ T_{h}[g_{h+1}](x,a)$ for all functions $\\textit{g}\\in\\textit{Q}$ simultaneously by collecting $K$ next-state transitions $\\widetilde{\\pmb{x}}_{h+1}^{(1)},\\ldots,\\widetilde{\\pmb{x}}_{h+1}^{(K)}\\stackrel{\\mathrm{i.i.d.}}{\\sim}T_{h}(\\cdot\\mid x,a)$ and $K$ rewards $\\widetilde{r}_{h}^{(1)},\\ldots,\\widetilde{r}_{h}^{(K)}$ , r\u0303(hK) i.i\u223c.d. Rh(x,a), then taking the empirical mean: Th[gh+1](x,a) \u2248 $\\begin{array}{r l}&{\\frac{1}{K}\\sum_{k=1}^{K}(\\widetilde{\\pmb{r}}_{h}^{(k)}+\\operatorname*{max}_{a^{\\prime}\\in\\mathcal{A}}g_{h+1}(\\widetilde{\\pmb{x}}_{h+1}^{(k)},a^{\\prime}))}\\end{array}$ . Line 8 of SimGolf s this technique to directly estimate the Bellman residual backup under a trajectory gathered with $\\pi^{(t)}$ , sidestepping the double sampling problem and removing the need for Bellman completeness. We suspect this technique (estimation with respect to squared Bellman error using local simulator access) may find broader use. ", "page_idx": 3}, {"type": "text", "text": "3.2 Main Result ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We now state the main guarantee for SimGolf and discuss some of its implications. ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.1 (Main guarantee for SimGolf). Let $\\varepsilon,\\delta\\in(0,1)$ be given and suppose Assumption 3.1 ( $Q^{\\star}$ -realizability) and Assumption 3.2 (coverability) hold with $C_{\\mathsf{c o v}}>0$ . Then the policy $\\widehat{\\pi}$ produced by SimGolf $\\left(\\boldsymbol{\\mathcal{Q}},\\boldsymbol{C}_{\\mathtt{C o v}},\\boldsymbol{\\varepsilon},\\delta\\right)$ (Algorithm 1) has $J(\\dot{\\pi}^{\\star})-\\mathbb{E}[J(\\widehat{\\pi})]\\leq\\varepsilon$ with probability at least $1-\\delta$ . The total sample complexity in the RLLS framework is bounded by $\\widetilde{O}\\big(H^{5}C_{\\mathsf{c o v}}^{2}\\log(|\\mathcal{Q}|/\\delta)\\cdot\\varepsilon^{-4}\\big)$ . ", "page_idx": 3}, {"type": "text", "text": "This result (whose proof is in Appendix E) shows that under only $Q^{\\star}$ -realizability and coverability, SimGolf learns an $\\varepsilon$ -optimal policy with polynomial sample complexity, significantly relaxing the representation assumptions (Bellman completeness, weight function realizability) required by prior algorithms for coverability [63, 4]. This is the first instance we are aware of where local simulator ", "page_idx": 3}, {"type": "text", "text": "Algorithm 1 SimGolf: Global Optimism via Local Simulator Access ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "1: input: Value function class $\\mathcal{Q}$ , coverability $C_{\\mathsf{c o v}}>0$ , suboptimality $\\varepsilon>0$ , and confidence $\\delta>0$ .   \n2: Set $N\\gets\\widetilde\\Theta(H^{2}C_{\\mathsf{c o v}}\\beta/\\varepsilon^{2})$ , $\\beta_{s t a t}\\gets16\\log(2H N|\\mathcal{Q}|\\delta^{-1})$ , $\\beta\\gets2\\beta_{\\mathrm{stat}}$ , and $\\begin{array}{r}{K\\gets\\frac{8N}{\\beta_{\\mathrm{stat}}}}\\end{array}$ .   \n3: initialize: $\\mathcal{Q}^{(1)}\\gets\\mathcal{Q}$ .   \n4: for iteration $t=1,2,\\ldots,N$ do   \n5: Select $\\begin{array}{r}{g^{(t)}=\\arg\\operatorname*{max}_{g\\in\\mathcal{Q}^{(t)}}\\sum_{s=1}^{t-1}\\operatorname*{max}_{a\\in\\mathcal{A}}g_{1}\\big(\\pmb{x}_{1}^{(s)},a\\big)}\\end{array}$ .   \n6: For each $h\\in[H]$ and $x\\in\\mathscr{X}$ , define $\\pi_{h}^{(t)}(x)\\in\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}g_{h}^{(t)}(x,a)$ .   \n7: Execute $\\pi^{(t)}$ for an episode and observe $\\pmb{\\tau}^{(t)}:=\\big(\\pmb{x}_{1}^{(t)},\\pmb{a}_{1}^{(t)}\\big),\\dots,\\big(\\pmb{x}_{H}^{(t)},\\pmb{a}_{H}^{(t)}\\big)$ .   \n8: For $h\\in[H]$ , draw $K$ independent samples $\\mathbf{x}_{h+1}^{(t,k)}\\sim T_{h}\\big(\\cdot\\mid\\mathbf{x}_{h}^{(t)},\\mathbf{a}_{h}^{(t)}\\big)$ H, $\\pmb{r}_{h}^{(t,\\bar{k})}\\sim R_{h}\\big(\\pmb{x}_{h}^{(t)},\\pmb{a}_{h}^{(t)}\\big).$   \n9: Compute confidence set:   \n$\\mathcal{Q}^{(t+1)}\\leftarrow\\left\\{g\\in\\mathcal{Q}:\\sum_{s\\leq t}\\biggr(g_{h}\\big(x_{h}^{(s)},a_{h}^{(s)}\\big)-\\frac{1}{K}\\sum_{k=1}^{K}\\biggr(r_{h}^{(s,k)}+\\operatorname*{max}_{a\\in\\mathcal{A}}g_{h+1}\\big(x_{h+1}^{(s,k)},a\\big)\\biggr)\\right)^{2}\\leq\\beta,\\ \\forall h\\in[H]\\right\\}.$ ", "page_idx": 4}, {"type": "text", "text": "10: return: $\\widehat{\\pi}=\\mathsf{u n i f}\\big(\\pi^{(1)},\\ldots,\\pi^{(N)}\\big)$ . ", "page_idx": 4}, {"type": "text", "text": "access unlocks sample complexity guarantees for reinforcement learning with nonlinear function approximation that were previously out of reach; perhaps the most important technical idea here is our approach to combining global optimism with local simulator access, in contrast to greedy layer-by-layer schemes used in prior work on local simulators (with the exception of Weisz et al. [57]). In particular, we suspect that the idea of performing estimation with respect to squared Bellman error directly using local simulator access may find broader use beyond coverability. Improving the polynomial dependence on problem parameters is an interesting question for future work. ", "page_idx": 4}, {"type": "text", "text": "A conjecture. By analogy to results in offline reinforcement learning, where $Q^{\\star}$ -realizability and concentrability (the offline counterpart to coverability) alone are known to be insufficient for sample-efficient learning [12, 26], we conjecture that $Q^{\\star}$ -realizability and coverability alone are not sufficient for polynomial sample complexity in vanilla online RL. If true, this would imply a new separation between online RL with and without local simulators. ", "page_idx": 4}, {"type": "text", "text": "3.3 Implications for Exogenous Block MDPs ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We now apply SimGolf and Theorem 3.1 to the Exogenous Block MDP (ExBMDP) problem [22, 21, 38, 30], a challenging rich-observation reinforcement learning setting in which the observed states $\\boldsymbol{x}_{h}$ are high-dimensional, while the underlying dynamics of the system are low-dimensional, yet confounded by temporally correlated exogenous noise. ", "page_idx": 4}, {"type": "text", "text": "Formally, an Exogenous Block MDP $\\mathcal{M}=(\\mathcal{X},\\mathcal{S},\\Xi,\\Xi,\\boldsymbol{\\mathcal{A}},\\boldsymbol{H},T,\\boldsymbol{R},g)$ is defined by a latent state space and an observation space. We begin with the latent state space. Starting from an initial endogenous state $s_{1}\\,\\in\\,S$ and exogenous state $\\xi_{1}\\,\\in\\,\\Xi$ , the latent state $\\pmb{z}_{h}\\,=\\,\\left(\\pmb{s}_{h},\\pmb{\\xi}_{h}\\right)$ evolves for $h\\in[H]$ via $s_{h+1}\\sim T_{h}^{\\mathsf{e n d o}}\\big(\\cdot\\mid\\pmb{s}_{h},\\pmb{a}_{h}\\big)$ and $\\xi_{h+1}\\sim T_{h}^{\\mathsf{e x o}}(\\cdot\\,|\\,\\xi_{h})$ , where $\\pmb{a}_{h}\\in\\mathcal{A}$ is the agent\u2019s action at layer $h$ ; we adopt the convention that $s_{1}\\sim T_{0}^{\\mathsf{e n d o}}(\\cdot\\mid\\emptyset)$ and $\\xi_{1}\\sim T_{0}^{\\mathrm{exo}}(\\cdot\\mid\\emptyset)$ . Note that only the endogenous satgaeten ti sr eccaeuisvaelsl ya in nofbluseenrcveadti boyn $\\mathbf{\\boldsymbol{x}}_{h}\\in\\mathcal{X}$ ogne. nTerhaet leadt evnita 1s $\\begin{array}{r}{\\mathbf{\\emx}_{h}=g_{h}^{\\circ\\mathsf{b s}}\\big(\\pmb{\\mathscr{s}}_{h},\\pmb{\\xi}_{h}\\big)}\\end{array}$ d,;  iwnhsteerae $g_{h}^{\\mathsf{o b s}}:S\\times\\Xi\\^{\\mathsf{\\Delta}}\\to\\mathcal{X}$ $h$ hies the emission function. We assume the endogenous latent space $\\boldsymbol{S}$ and action space $\\boldsymbol{\\mathcal{A}}$ are finite, and define $S:=|S|$ and $A:=|{\\mathcal{A}}|$ . However, the exogenous state space $\\Xi$ and observation space $\\mathcal{X}$ may be arbitrarily large or infinite, with \u2223 $\\Xi|,|{\\boldsymbol{\\mathcal{X}}}|\\gg|{\\boldsymbol{\\mathcal{S}}}|$ .2 ", "page_idx": 4}, {"type": "text", "text": "The final property of the ExBMDP model is decodability, which asserts the existence of a decoder such that $\\phi_{\\star}:\\mathcal{X}\\rightarrow\\mathcal{S}$ such that $\\phi_{\\star}\\bigl({\\pmb x}_{h}\\bigr)={\\pmb s}_{h}$ a.s. for all $h\\in[H]$ with $\\begin{array}{r}{\\mathbf{\\emx}_{h}=g_{h}^{\\mathsf{o b s}}\\big(\\pmb{\\mathscr{s}}_{h},\\pmb{\\xi}_{h}\\big),}\\end{array}$ . Informally, decodability ensures the existence of an (unknown to the learner) mapping that allows one to perfectly recover the endogenous latent state from observations. In addition to decodability, we assume the rewards in the ExBMDP are endogenous; that is, the reward distribution $R_{h}({\\pmb x}_{h},{\\pmb a}_{h})$ only depends on the observations $\\left(x_{h}\\right)$ through the corresponding latent states $\\left(\\phi^{\\star}\\!\\left({\\pmb x}_{h}\\right)={\\pmb s}_{h}\\right)$ ). To enable sample-efficient learning, we assume access to a decoder class $\\Phi$ that contains $\\phi^{\\star}$ , as in prior work. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Assumption 3.3 (Decoder realizability). We have access to a decoder class $\\Phi$ such that $\\phi^{\\star}\\in\\Phi$ . ", "page_idx": 5}, {"type": "text", "text": "Applying SimGolf and Theorem 3.1. To apply Theorem 3.1 to the ExBMDP problem, we need to verify that $Q^{\\star}$ -realizability and coverability hold. Realizability is a straightforward consequence of decodability (Lemma D.1 in Part II of the appendix). For coverability, Xie et al. [63] show that ExBMDPs have $C_{\\mathsf{c o v}}\\,\\leq\\,S A$ under decodability, in spite of the time-correlated exogenous noise process $(\\pmb{\\xi}_{h})$ and potentially infinite observation space $\\mathcal{X}$ (interestingly, coverability is essentially the only useful structural property that ExBMDPs are known to satisfy, which is our primary motivation for studying it). This leads to the following corollary of Theorem 3.1. ", "page_idx": 5}, {"type": "text", "text": "Corollary 3.1 (SimGolf for ExBMDPs). Consider the ExBMDP setting. Suppose that Assumption 3.3 holds, and let $\\mathcal{Q}$ be constructed as in Lemma $D.I$ of Part II. Then for any $\\varepsilon,\\delta\\in(0,1)$ , the policy $\\widehat{\\pi}=\\mathsf{S i m G o l f}\\big(\\mathcal{Q},S A,\\varepsilon,\\delta\\big)$ has $J(\\pi^{\\star})-J(\\widehat\\pi)\\leq\\varepsilon$ with probability at least $1-\\delta$ . The total sample complexity in the RLLS framework is $N=\\widetilde{\\cal O}\\big(H^{5}S^{3}A^{3}\\log\\vert\\Phi\\vert\\cdot\\varepsilon^{-4}\\big)$ . ", "page_idx": 5}, {"type": "text", "text": "This shows for the first time that general ExBMDPs are learnable with local simulator access. Prior to this work, online RL algorithms for ExBMDPs required either (i) deterministic latent dynamics [22], or (ii) factored emission structure [21]. Xie et al. [63] observed that ExBMDPs admit low coverability, but their algorithm requires Bellman completeness, which is not satisfied by ExBMDPs (see Islam et al. [30]). See Appendix A for more discussion. ", "page_idx": 5}, {"type": "text", "text": "4 Computationally Efficient Learning with Local Simulators ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Our result in Section 3 show that local simulator access facilitates sample-efficient learning in MDPs with low coverability, a challenging setting that was previously out of reach. However, our algorithm SimGolf is computationally-inefficient because it relies on global optimism, a drawback found in most prior work on RL with general function approximation [31, 32, 18]. It remains an open question whether any form of global optimism can be implemented efficiently, and some variants have provable barriers to efficient implementation [14]. ", "page_idx": 5}, {"type": "text", "text": "To address this drawback, in this section we present a new algorithm, RVFS (Recursive Value Function Search; Algorithm 5), which requires stronger versions of the coverability and realizability assumptions in Section 3, but is computationally efficient in the sense that it reduces to convex optimization over the state-value function class $\\nu$ . RVFS makes use of a sophisticated recursive exploration scheme based on core-sets, sidestepping the need for global optimism. ", "page_idx": 5}, {"type": "text", "text": "4.1 Function Approximation and Statistical Assumptions ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To begin, we require the following strengthening of the coverability assumption in Assumption 3.2. ", "page_idx": 5}, {"type": "text", "text": "Assumption 4.1 (Pushforward coverability). The pushforward coverability coefficient $C_{\\mathsf{p u s h}}>0$ is $\\begin{array}{r}{C_{\\mathrm{push}}=\\operatorname*{max}_{h\\in[H]}\\operatorname*{inf}_{\\mu_{h}\\in\\Delta(\\mathcal{X})}\\operatorname*{sup}_{(x_{h-1},a_{h-1},x_{h})\\in\\mathcal{X}_{h-1}\\times A\\times\\mathcal{X}}\\frac{T_{h-1}\\left(x_{h}|x_{h-1},a_{h-1}\\right)}{\\mu_{h}\\left(x_{h}\\right)}.}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "Pushforward coverability is inspired by the pushforward concentrability condition used in offilne RL by [62, 26]. Concrete examples include, (i) Block MDPs with latent space $\\boldsymbol{S}$ , which admit $C_{\\mathsf{p u s h}}\\leq|{\\cal{S}}|$ , (ii) Low-Rank MDPs in dimension $d$ , which admit $C_{\\mathsf{p u s h}}\\leq d$ [62], and (iii) Exogenous Block MDPs for which the exogenous noise process satisfies a weak correlation condition that we introduce in Appendix B. Note that $C_{\\mathsf{c o v}}\\leq\\bar{C_{\\mathsf{p u s h}}}|\\mathcal{A}|$ , but the converse is not true in general. ", "page_idx": 5}, {"type": "text", "text": "Instead of state-action value function approximation as in SimGolf, in this section we make use of a state value function class $\\mathcal{V}\\subset(\\mathcal{X}\\times[H]\\rightarrow[0,H])$ , but require somewhat stronger representation conditions than in Section 3. We consider two complementary setups: ", "page_idx": 5}, {"type": "text", "text": "\u2022 Setup I: Assumptions 4.2 and 4.3 $\\smash{V^{\\star}/\\pi^{\\star}}$ -realizability) and Assumption 4.4 ( $\\Delta$ -gap) hold. ", "page_idx": 5}, {"type": "text", "text": "\u2022 Setup II: Assumption 4.5 ( $V^{\\pi}$ -realizability) and Assumption 4.6 $\\pi$ -realizability) hold. ", "page_idx": 5}, {"type": "text", "text": "We describe these assumptions in more detail below. ", "page_idx": 5}, {"type": "text", "text": "Function approximation setup I. First, instead of $Q^{\\star}$ -realizability, we consider the weaker $V^{\\star}$ - realizability [31, 57, 3]. ", "page_idx": 5}, {"type": "text", "text": "Assumption 4.2 ( $V^{\\star}$ -realizability). For all $h\\in[H]$ , we have $V_{h}^{\\star}\\in\\mathcal{V}_{h}$ . ", "page_idx": 5}, {"type": "text", "text": "Under $V^{\\star}$ -realizability, our algorithm learns a near-optimal policy, but the policy is non-executable (cf. Definition 2.1); this property is shared by prior work on local simulator access with value function realizability [57] . To produce executable policies, we additionally require access to a policy class $\\Pi\\subset\\Pi_{\\mathbb{S}}$ containing $\\pi^{\\star}$ ; we define $\\Pi_{h}=\\left\\{\\pi_{h}\\mid\\pi\\in\\Pi\\right\\}$ . ", "page_idx": 6}, {"type": "text", "text": "Assumption 4.3 ( $\\pi^{\\star}$ -realizability). The policy class $\\Pi$ contains the optimal policy $\\pi^{\\star}$ . ", "page_idx": 6}, {"type": "text", "text": "$V^{\\star}$ -realizability (Assumption 4.2) and $\\pi^{\\star}$ -realizability (Assumption 4.3) are both implied by $Q^{\\star}$ -realizability, and hence are weaker. However, we also assume the optimal $Q$ -function admits constant gap (this makes the representation conditions for Setup I incomparable to Assumption 3.1). ", "page_idx": 6}, {"type": "text", "text": "Assumption 4.4 ( $\\Delta$ -Gap). The optimal action $\\pi_{h}^{\\star}(x)$ is unique, and there exists $\\Delta>0$ such that for all $h\\in[H]$ , $x\\in\\mathscr{X}$ , and $a\\in\\mathcal{A}\\setminus\\{\\pi_{h}^{\\star}(x)\\}$ , $Q_{h}^{\\star}({\\dot{x_{,}}\\bar{\\pi}_{h}^{\\star}(x)})>Q_{h}^{\\star}(x,a)+\\Delta$ . ", "page_idx": 6}, {"type": "text", "text": "This condition has been used in a many prior works on computationally efficient RL with function approximation [16, 17, 24, 56]. ", "page_idx": 6}, {"type": "text", "text": "Function approximation setup II. We also provide guarantees under the assumption that the class $\\nu$ satisfies all-policy realizability [59, 65, 60] in the sense that $V^{\\pi}\\in\\mathcal{V}$ for all $\\pi\\in\\Pi_{\\ S}$ . ", "page_idx": 6}, {"type": "text", "text": "Assumption 4.5 ( $V^{\\pi}$ -realizability). The class $\\mathcal{V}=\\mathcal{V}_{1:H}$ has $V_{h}^{\\pi}\\in\\mathcal{V}_{h}$ for all $\\pi\\in\\Pi_{\\ S}$ and $h\\in[H]$ . ", "page_idx": 6}, {"type": "text", "text": "This assumption will be sufficient to learn a non-executable policy, but to learn executable policies we require an analogous strengthening of Assumption 4.5. ", "page_idx": 6}, {"type": "text", "text": "Assumption 4.6 $\\pi$ -realizability). For all $\\pi\\in\\Pi_{\\ S}$ , we have that $x\\mapsto\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}\\mathcal{P}_{h}[V_{h+1}^{\\pi}](\\cdot,a)\\in\\Pi.$ . ", "page_idx": 6}, {"type": "text", "text": "This assumption has been used by a number of prior works on computationally efficient RL [8, 44]. Assumptions 4.5 and 4.6 are both implied by the slightly simpler-to-state assumption of $Q^{\\pi}$ -realizability [59, 65, 60], which asserts access to a class $\\mathcal{Q}$ that contains $Q^{\\pi}$ for all $\\pi\\in\\Pi_{\\ S}$ . ", "page_idx": 6}, {"type": "text", "text": "4.2 Algorithm ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "For ease of exposition, we defer the full version of our algorithm, RVFS (Algorithm 5), to Appendix F and present a simplified version here (Algorithm 2). The algorithms are nearly identical, except that the simplified version assumes that certain quantities of interest (e.g., Bellman backups) can be computed exactly, while the full version (provably) approximates them from samples. ", "page_idx": 6}, {"type": "text", "text": "RVFS maintains a value function estimator $\\widehat V\\,=\\,\\widehat V_{1:H}$ that aims to approximate the optimal value function $V_{1:H}^{\\star}$ , as well as core sets $\\mathcal{C}_{1},\\ldots,\\mathcal{C}_{H}$ of state-action pairs that are used to perform estimation and guide exploration. At a high level, RVFS alternates between (i) ftiting the value function $\\widehat{V}_{h}$ for a given layer $h\\in[H]$ based on Monte-Carlo rollouts, and (ii) using the core-sets to test whether the current value function estimates $\\widehat{V}_{h+1:H}$ remain accurate as the roll-in policy induced by $\\widehat{V}_{h}$ changes. ", "page_idx": 6}, {"type": "text", "text": "In more detail, RVFS is based on recursion across the layers $h\\in[H]$ . When invoked for layer $h$ with value function estimates $\\widehat{V}_{h+1:H}$ and core-sets $\\mathcal{C}_{h},\\hdots,\\mathcal{C}_{H}$ , $\\mathsf{R V F S}_{h}$ performs two steps: ", "page_idx": 6}, {"type": "text", "text": "1. For each state-action pair $(x_{h-1},a_{h-1})\\ \\in\\ C_{h}$ ,4 the algorithm gathers $N_{\\mathrm{test}}$ trajectories by rolling out from $(x_{h-1},a_{h-1})$ with the greedy policy $\\widehat{\\pi}_{\\ell}(x)\\;\\;\\in\\;\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}\\mathcal{P}_{\\ell}\\big[\\widehat{V}_{\\ell+1}\\big](x,a)$ that optimizes the estimated value function; in the full version of RVFS (see Algorithm 5), we estimate the bellman backup $\\mathcal{P}_{\\ell}[\\widehat{V}_{\\ell+1}](x,a)$ using the local simulator. For all states $x_{\\ell-1}\\;\\;\\;\\in\\;\\;\\;\\{x_{h},\\ldots,x_{H-1}\\}$ encountered during this process, the algorithm checks whether $\\left|\\mathbb{E}\\big[\\widehat{V}_{\\ell}(\\pmb{x}_{\\ell})-V_{\\ell}^{\\star}(\\pmb{x}_{\\ell})\\;|\\;\\pmb{x}_{\\ell-1}=x_{\\ell-1},\\pmb{a}_{\\ell-1}=a_{\\ell-1}\\big]\\right|\\lesssim\\varepsilon$ for all $a_{\\ell-1}\\in A$ using a test based on (implicitly maintained) confidence sets. If the test fails, this indicates that distribution shift has occurred, and the algorithm adds the pair $(x_{\\ell-1},a_{\\ell-1})$ to $\\mathcal{C}_{\\ell}$ and recurses on layer $\\ell$ via $\\mathsf{R V F S}_{\\ell}$ . ", "page_idx": 6}, {"type": "text", "text": "2. If all tests above pass, this means that $\\widehat{V}_{h+1},\\ldots,\\widehat{V}_{H}$ are accurate, and no distribution shift has occurred. In this case, the algorithm ftis $\\widehat{V}_{h}$ by collecting Monte-Carlo rollouts from all state-action pairs in the core-set $\\ensuremath{\\mathcal{C}}_{h}$ with $\\begin{array}{r}{\\widehat{\\pi}_{\\ell}(x)\\in\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}\\mathcal{P}_{\\ell}[\\widehat{V}_{\\ell+1}](x,a)}\\end{array}$ (cf. Line 16), and returns. ", "page_idx": 6}, {"type": "text", "text": "When the tests in Item 1 succeed for all $h\\in[H]$ , the algorithm returns the estimated value functions $\\widehat{V}_{1:H}$ ; in this case, the greedy policy $\\begin{array}{r}{\\widehat{\\pi}_{\\ell}(x)\\in\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}\\mathcal{P}_{\\ell}[\\widehat{V}_{\\ell+1}](x,a)}\\end{array}$ is guaranteed to be near ", "page_idx": 6}, {"type": "text", "text": "1: parameters: Value function class $\\nu$ , suboptimality $\\varepsilon\\in(0,1)$ , confidence $\\delta\\in(0,1)$ .   \n2: input: Level $h\\in[0\\ldots H]$ , value functions $\\widehat{V}_{h+1:H}$ , confidence sets $\\widehat{\\mathcal{V}}_{h+1:H}$ , core-sets $\\mathcal{C}_{h:H}$ .   \n3: Initialize parameters $M$ , $N_{\\mathrm{test}}$ , $N_{\\mathsf{r e g}}$ , $\\varepsilon_{\\mathrm{reg}}^{2}$ , and $\\beta$ (see Algorithm 5 for parameter settings). $^{\\prime\\star}$ Test the fit for the estimated value functions $\\widehat{V}_{h+1:H}$ at future layers. $\\star/$ ", "page_idx": 7}, {"type": "text", "text": "4: for $(x_{h-1},a_{h-1})\\in{\\mathcal{C}}_{h}$ and $\\ell=H,\\dots,h+1$ do ", "page_idx": 7}, {"type": "text", "text": "5: for $n=1,\\ldots,N_{\\mathrm{test}}$ do   \n6: Draw $\\pmb{x}_{h}\\sim T_{h-1}(\\cdot\\mid x_{h-1},a_{h-1})$ , then draw $x_{\\ell-1}$ by rolling out with $\\widehat{\\pi}_{h:H}$ , where3 ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\forall\\,\\tau\\in[H],\\quad\\widehat{\\pi}_{\\tau}(\\cdot)\\in\\arg\\operatorname*{max}_{a\\in\\cal A}\\mathcal{P}_{\\tau}[\\widehat{V}_{\\tau+1}](\\cdot,a).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "7: ", "page_idx": 7}, {"type": "text", "text": "for $a_{\\ell-1}\\in A$ do $^{\\prime\\star}$ Test fit; if test fails, re-fit value functions $\\widehat{V}_{h+1:\\ell}$ up to layer $8.\\times1$ $\\begin{array}{r}{\\mathbf{if}\\operatorname*{sup}_{f\\in\\widehat{\\mathcal{V}}_{\\ell}}|\\big(\\mathcal{P}_{\\ell-1}[\\widehat{V}_{\\ell}]-\\mathcal{P}_{\\ell-1}\\big[f_{\\ell}\\big]\\big)\\big(\\pmb{x}_{\\ell-1},a_{\\ell-1}\\big)|>\\varepsilon+\\varepsilon\\cdot\\beta}\\end{array}$ then ", "page_idx": 7}, {"type": "text", "text": "8: ", "page_idx": 7}, {"type": "text", "text": "9: ", "page_idx": 7}, {"type": "text", "text": "10: ", "page_idx": 7}, {"type": "equation", "text": "$\\tau=\\ell,\\dots,h+1$ $$\n\\begin{array}{r l}&{\\mathrm{\\Lambda}^{\\top}=\\varepsilon,\\cdot\\cdot\\cdot,\\lceil h+1\\mathrm{\\bf~a}0\\rceil}\\\\ &{\\bigl(\\widehat{V}_{\\tau:H},\\widehat{\\mathcal{V}}_{h:H},\\mathcal{C}_{\\tau:H}\\bigr)\\leftarrow\\mathsf{R V F S}_{\\tau}\\bigl(\\widehat{V}_{\\tau+1:H},\\widehat{\\mathcal{V}}_{h+1:H},\\mathcal{C}_{\\tau:H};\\mathcal{V},\\varepsilon,\\delta\\bigr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "12: ", "page_idx": 7}, {"type": "text", "text": "go to line 4. ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "13: if $h=0$ then return: $(\\widehat{V}_{1:H},\\cdot,\\cdot,\\cdot)$ . /\\* Re-fit $\\widehat{V}_{h}$ and build a new confidence set. $\\star/$ for ", "page_idx": 7}, {"type": "text", "text": "5: Set $\\mathscr{D}_{h}\\big(x_{h-1},a_{h-1}\\big)\\gets\\emptyset$ . For $i=1,\\ldots,N_{\\mathrm{reg}}$ , sample $\\pmb{x}_{h}\\sim T_{h-1}(\\cdot\\mid x_{h-1},a_{h-1})$ and update $\\begin{array}{r}{\\mathcal{D}_{h}\\big(x_{h-1},a_{h-1}\\big)\\gets\\mathcal{D}_{h}\\big(x_{h-1},a_{h-1}\\big)\\cup\\{\\big(x_{h},\\mathbb{E}^{\\widehat\\pi_{h:H}}\\big[\\sum_{\\ell=h}^{H}r_{\\ell}\\,\\big|\\,x_{h}\\big]\\big)\\}.}\\end{array}$ ", "page_idx": 7}, {"type": "text", "text": "16: Let $\\begin{array}{r}{\\dot{V_{h}}:=\\arg\\operatorname*{min}_{f\\in\\widehat{\\mathcal{V}}}\\sum_{(x_{h-1},a_{h-1})\\in\\mathcal{C}_{h}}\\sum_{(x_{h},v_{h})\\in\\mathcal{D}_{h}(x_{h-1},a_{h-1})}(f(x_{h})-v_{h})^{2}}\\end{array}$ ", "page_idx": 7}, {"type": "text", "text": "17: Compute value function confidence set: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\widehat{\\mathcal{V}}_{h}:=\\left\\{f\\in\\mathcal{V}\\,\\Bigg|\\,\\sum_{(x_{h-1},a_{h-1})\\in\\mathcal{C}_{h}}\\frac{1}{N_{\\mathrm{reg}}}\\sum_{(x_{h},\\cdot)\\in\\mathcal{D}_{h}(x_{h-1},a_{h-1})}\\left(\\widehat{V}_{h}(x_{h})-f(x_{h})\\right)^{2}\\leq\\varepsilon_{\\mathrm{reg}}^{2}\\right\\}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "18: return $(\\widehat{V}_{h:H},\\widehat{\\mathcal{V}}_{h:H},\\mathcal{C}_{h:H})$ . ", "page_idx": 7}, {"type": "text", "text": "optimal. The full version of RVFS in Algorithm 5 uses local simulator access to estimate the Bellman backups $\\mathcal{P}_{h}[\\widehat{V}_{h+1}](x,a)$ for different state-action pairs $(x,a)$ . These backups are used to (i) compute actions of the greedy policy that maximizes $\\widehat{V}_{1:H}$ via (e.g., Eq. (2)); (ii) generate trajectories by rolling out from state-action pairs in the core-sets (Line 6); and (iii) perform the test in Item 1 (Line 8). ", "page_idx": 7}, {"type": "text", "text": "RVFS is inspired by the DMQ algorithm [16, 56] originally introduced in the context of online reinforcement learning with linearly realizable $Q^{\\star}$ . RVFS incorporates local simulator access (most critically, via core-set construction) to allow for more general nonlinear function approximation without restrictive statistical assumptions. Prior algorithms for RLLS have used core-sets of state-action pairs in a similar fashion [40, 65, 59], but in a way that is tailored to linear function approximation. ", "page_idx": 7}, {"type": "text", "text": "In what follows, we discuss various features of the algorithm in greater detail. ", "page_idx": 7}, {"type": "text", "text": "Bellman backup policies. Since RVFS works with state value functions instead of state-action value functions, we need a way to extract policies from the former. The most natural way to extract a policy from estimated value functions $\\widehat{V}_{1:H}\\,\\in\\,\\mathcal{V}$ is as follows: for all $h\\,\\in\\,[H]$ , define $\\begin{array}{r}{\\widehat{\\pi}_{h}(x)\\in\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}\\mathcal{P}_{h}[\\widehat{V}_{h+1}](x,a)}\\end{array}$ . In reality, we do not have access to $\\mathcal{P}_{h}[\\widehat{V}_{h+1}](x,a)$ directly, so the full version of RVFS (Algorithm 5) estimates this quantity on the fly using the local simulator using the following scheme (Algorithm 7 in Appendix F): Given a state $x$ , for each $a$ , we sample $K$ rewards $r_{h}\\sim\\bar{R}_{h}(x,a)$ and next-state transitions $\\mathbf{x}_{h+1}\\sim T_{h}(\\cdot\\mid x,a)$ , then approximate $\\mathcal{P}_{h}[\\widehat{V}_{h+1}](x,a)$ by the empirical mean. We remark that the use of these Bellman backup policies is actually crucial in the analysis for RVFS; even if we were to work with estimated state-action value functions $\\widehat{Q}_{1:H}$ instead, our analysis would require executing the Bellman backup policies $\\widehat{\\pi}_{h}(x)\\in\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}\\mathcal{T}_{h}[\\widehat{Q}_{h+1}](x,a)$ (instead of naively using $\\widehat{\\pi}_{h}(x)\\in\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}\\widehat{Q}_{h}(x,a))$ . ", "page_idx": 7}, {"type": "text", "text": "Invoking the algorithm. The base invocation of RVFS takes the form ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\widehat{V}_{1:H}\\gets\\mathsf{R V F S}_{0}\\big(\\widehat{V}_{1:H}=\\mathsf{a r b i t r a r y},\\widehat{\\mathcal{V}}_{1:H}=\\{\\mathcal{V}_{h}\\}_{h=1}^{H},\\mathcal{C}_{0:H}=\\{\\emptyset\\}_{h=0}^{H},;\\mathcal{V},\\varepsilon,\\delta\\big).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Whenever this call returns, the greedy policy induced by $\\widehat{V}_{1:H}$ is guaranteed to be near-optimal. Naively, the approximate Bellman backup policy induced by $\\widehat{V}_{1:H}$ (described above) is non-executable, and must be computed by invoking the local simulator. To provide an end-to-end guarantee to learn an executable policy, we give an outer-level algorithm, RVFS.bc (Algorithm 6, deferred to Appendix F for space), which invokes $\\mathsf{R V F S}_{0}$ , then extracts an executable policy from $\\widehat{V}_{1:H}$ using behavior cloning. Subsequent recursive calls to RVFS take the form $(\\widehat{V}_{h:H},\\widehat{\\nu}_{h:H},\\mathcal{C}_{h:H})\\;\\leftarrow$ $\\mathsf{R V F S}_{h}(\\widehat{V}_{h+1:H},\\widehat{\\mathcal{V}}_{h+1:H},\\mathcal{C}_{h:H};\\mathcal{V},\\varepsilon,\\delta)$ . The arguments here are: Importantly, the confidence sets $\\widehat{\\mathcal{V}}_{h+1:H}$ do not need to be explicitly maintained, and can be used implicitly whenever a regression oracle for the value function class is available (discussed below). ", "page_idx": 8}, {"type": "text", "text": "Remark 4.1 (Oracle-efficiency). RVFS is computationally efficient in the sense that it reduces to convex optimization over the value function class $\\nu$ . In particular, the only computationally intensive steps in the algorithm are $(i)$ the regression step in Line 16, and $(i i)$ the test in Line 8 involving the confidence set $\\widehat{\\nu}_{\\ell}$ . For the latter, we do not explicitly need to maintain $\\widehat{\\nu}_{\\ell}$ , as the optimization problem over this set in Line 8 (for the full version of RVFS in Algorithm 5) reduces to solving arg maxV \u2208V{ $\\begin{array}{r}{\\operatorname*{max}_{V\\in\\mathcal{V}}\\!\\left\\{\\pm\\sum_{i=1}^{n}V\\big(\\widetilde{x}^{(i)}\\big)\\mid\\sum_{i=1}^{n}\\!\\big(V\\big(x^{(i)}\\big)-y^{(i)}\\big)^{2}\\leq\\beta^{2}\\right\\}}\\end{array}$ for a dataset $\\left\\{\\left(\\boldsymbol{x}^{(i)},\\widetilde{\\boldsymbol{x}}^{(i)},\\boldsymbol{y}^{(i)}\\right)\\right\\}_{i=1}^{n}$ . This is convex optimization problem in function space, and in particular can be implemented in a provably efficient fashion whenever $\\nu$ is linearly parameterized. We expect that the problem can also be reduced to a square loss regression by adapting the techniques in Krishnamurthy et al. [37], Foster et al. [23], but we do not pursue this here. ", "page_idx": 8}, {"type": "text", "text": "4.3 Main Result ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We present the main guarantee for RVFS under the function approximation assumptions in Section 4.1. ", "page_idx": 8}, {"type": "text", "text": "Theorem 4.1 (Main guarantee for RVFS). Let $\\varepsilon,\\delta\\in(0,1)$ be given, and suppose that Assumption 4.1 (pushforward coverability) holds with $C_{\\mathsf{p u s h}}>0$ . Further, suppose that one the following holds: ", "page_idx": 8}, {"type": "text", "text": "\u2022 Setup I: Assumptions 4.2 and 4.3 $V^{\\star}/\\pi^{\\star}$ -realizability) and Assumption 4.4 ( $\\Delta$ -gap) hold, and $\\varepsilon\\leq6H\\cdot\\Delta$ . ", "page_idx": 8}, {"type": "text", "text": "\u2022 Setup II: Assumption 4.5 $V^{\\pi}$ -realizability) and Assumption 4.6 ( $\\pi$ -realizability) hold. ", "page_idx": 8}, {"type": "text", "text": "Then, RVFS.b $\\mathsf{c}(\\Pi,\\mathcal{V},\\varepsilon,\\delta)$ (Algorithm 6) returns a policy $\\widehat{\\pi}_{1:H}$ such that $J(\\pi^{\\star})-J(\\widehat{\\pi}_{1:H})\\le2\\varepsilon$ with probability at least $1-\\delta$ , and has total sample complexity bounded by ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\widetilde{\\cal O}\\left(C_{\\mathrm{push}}^{8}H^{23}A\\cdot\\varepsilon^{-13}\\right).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Furthermore, the algorithm makes at most poly $(C_{\\mathsf{p u s h}},H,A,\\varepsilon^{-1})$ calls to the convex optimization oracle over value function space described in Remark 4.1. ", "page_idx": 8}, {"type": "text", "text": "Theorem 4.1 shows for the first time that sample- and computationally-efficient RL with local simulator access is possible under pushforward coverability. In particular, RVFS is the first computationally efficient algorithm for RL with local simulator access that supports nonlinear function approximation. The assumptions in Theorem 4.1, while stronger than those in Section 3, are not known to enable sample-efficient RL without simulator access. Nonetheless, understanding whether RVFS can be strengthened to support general coverability or weaker function approximation is an important open problem. See Appendix H.1 for an overview of the analysis; we remark (Appendix I.1) that the result is actually proven under slightly weaker assumptions than those in Setup I/Setup II. ", "page_idx": 8}, {"type": "text", "text": "Connection to empirical algorithms. RVFS bears some similarity to Monte-Carlo Tree Search (MCTS) [13, 35] and AlphaZero [52], which perform planning with local simulator. Informally, MCTS can be viewed as a form of breadth-first search over the state space (where each node represents a state at a given layer), and AlphaZero is a particular instantiation of a MCTS that leverages $V$ \u2212value function approximation to allow for generalization across states. Compared to RVFS, MCTS and AlphaZero perform exploration via simple bandit-style heuristics, and are not explicitly designed to handle distribution shifts that arise in settings where actions have long-term downstream effects. What is more, MCTS requires finite states to iterate over all possible child nodes of each state, making it inapplicable in environments with continuous states. RVFS may be viewed as a provable counterpart that can handle continuous states and uses function approximation to address distribution shift in a principled fashion (in particular, through the use of confidence sets and the test in Line 14).5 ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Applying RVFS to Exogenous Block MDPs. ExBMDPs satisfy coverability (Assumption 3.2), but do not satisfy the pushforward coverability assumption (Assumption 4.1) in general. However, it turns out that ExBMDPs do satisfy pushforward coverability when the exogenous noise process is weakly correlated across time, a new statistical assumption we refer to the weak correlation condition. In Appendix B (Theorem B.1), we give a variant of RVFS for ExBMDPs that succeeds under (i) weak correlation, and (ii) decoder realizability, sidestepping the need for the $\\Delta$ -gap or $V^{\\pi}$ -realizability. ", "page_idx": 9}, {"type": "text", "text": "5 Discussion and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we demonstrated that resets can substantially expand the range of reinforcement learning (RL) settings that are tractable, both statistically and computationally. Our practical algorithm, RVFS, provides a principled counterpart to MCTS by supporting continuous state spaces and offering provable guarantees, setting it apart from traditional MCTS. Statistically, our results extend to MDPs with a finite Sequential Estimation Coefficient (SEC) [63], capturing a broader class of MDPs beyond those with finite coverability\u2014encompassing low Bellman Eluder MDPs [32] and MDPs with finite bilinear rank [18]. Although not formally developed here, it is possible to generalize push-forward coverability and our analysis to encompass a range of linear function approximation settings [57, 40, 3, 59, 65, 66], thereby recovering known positive results under local access in these settings. ", "page_idx": 9}, {"type": "text", "text": "While our focus has been on theoretical contributions\u2014analyzing the sample and computational complexity of RL with access to a local simulator\u2014this work also raises promising empirical questions. We are particularly interested in exploring these questions in future work, aiming to bridge these theoretical advances with empirical validation in practical RL settings. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] A. Agarwal, D. Hsu, S. Kale, J. Langford, L. Li, and R. Schapire. Taming the monster: A fast and simple algorithm for contextual bandits. In International Conference on Machine Learning, pages 1638\u20131646, 2014.   \n[2] I. Akkaya, M. Andrychowicz, M. Chociej, M. Litwin, B. McGrew, A. Petron, A. Paino, M. Plappert, G. Powell, R. Ribas, et al. Solving rubik\u2019s cube with a robot hand. arXiv preprint arXiv:1910.07113, 2019.   \n[3] P. Amortila, N. Jiang, D. Madeka, and D. P. Foster. A few expert queries suffices for sampleefficient rl with resets and linear value approximation. Advances in Neural Information Processing Systems, 35:29637\u201329648, 2022.   \n[4] P. Amortila, D. J. Foster, N. Jiang, A. Sekhari, and T. Xie. Harnessing density ratios for online reinforcement learning. International Conference on Learning Representations (ICLR), 2024.   \n[5] P. Amortila, D. J. Foster, and A. Krishnamurthy. Scalable online exploration via coverability. arXiv preprint arXiv:2403.06571, 2024.   \n[6] S. Aradi. Survey of deep reinforcement learning for motion planning of autonomous vehicles. IEEE Transactions on Intelligent Transportation Systems, 23(2):740\u2013759, 2020.   \n[7] A. P. Badia, B. Piot, S. Kapturowski, P. Sprechmann, A. Vitvitskyi, Z. D. Guo, and C. Blundell. Agent57: Outperforming the atari human benchmark. In International conference on machine learning, pages 507\u2013517. PMLR, 2020.   \n[8] J. Bagnell, S. M. Kakade, J. Schneider, and A. Ng. Policy search by dynamic programming. Advances in neural information processing systems, 16, 2003.   \n[9] M. Bellemare, J. Veness, and M. Bowling. Investigating contingency awareness using atari 2600 games. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 26, pages 864\u2013871, 2012.   \n[10] M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal, L. D. Jackel, M. Monfort, U. Muller, J. Zhang, et al. End to end learning for self-driving cars. arXiv preprint arXiv:1604.07316, 2016.   \n[11] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.   \n[12] J. Chen and N. Jiang. Information-theoretic considerations in batch reinforcement learning. In International Conference on Machine Learning, pages 1042\u20131051. PMLR, 2019.   \n[13] R. Coulom. Efficient selectivity and backup operators in monte-carlo tree search. In International conference on computers and games, pages 72\u201383. Springer, 2006.   \n[14] C. Dann, N. Jiang, A. Krishnamurthy, A. Agarwal, J. Langford, and R. E. Schapire. On oracleefficient PAC RL with rich observations. In Advances in neural information processing systems, pages 1422\u20131432, 2018.   \n[15] S. Du, A. Krishnamurthy, N. Jiang, A. Agarwal, M. Dudik, and J. Langford. Provably efficient RL with rich observations via latent state decoding. In International Conference on Machine Learning, pages 1665\u20131674. PMLR, 2019.   \n[16] S. S. Du, Y. Luo, R. Wang, and H. Zhang. Provably efficient Q-learning with function approximation via distribution shift error checking oracle. In Advances in Neural Information Processing Systems, pages 8060\u20138070, 2019.   \n[17] S. S. Du, S. M. Kakade, R. Wang, and L. F. Yang. Is a good representation sufficient for sample efficient reinforcement learning? In International Conference on Learning Representations, 2020.   \n[18] S. S. Du, S. M. Kakade, J. D. Lee, S. Lovett, G. Mahajan, W. Sun, and R. Wang. Bilinear classes: A structural framework for provable generalization in RL. International Conference on Machine Learning, 2021.   \n[19] A. Ecoffet, J. Huizinga, J. Lehman, K. O. Stanley, and J. Clune. Go-explore: a new approach for hard-exploration problems. arXiv preprint arXiv:1901.10995, 2019.   \n[20] A. Ecoffet, J. Huizinga, J. Lehman, K. O. Stanley, and J. Clune. First return, then explore. Nature, 590(7847):580\u2013586, 2021.   \n[21] Y. Efroni, D. J. Foster, D. Misra, A. Krishnamurthy, and J. Langford. Sample-efficient reinforcement learning in the presence of exogenous information. In Conference on Learning Theory, pages 5062\u20135127. PMLR, 2022.   \n[22] Y. Efroni, D. Misra, A. Krishnamurthy, A. Agarwal, and J. Langford. Provably filtering exogenous distractors using multistep inverse dynamics. In International Conference on Learning Representations, 2022.   \n[23] D. J. Foster, A. Agarwal, M. Dud\u00edk, H. Luo, and R. E. Schapire. Practical contextual bandits with regression oracles. International Conference on Machine Learning, 2018.   \n[24] D. J. Foster, A. Rakhlin, D. Simchi-Levi, and Y. Xu. Instance-dependent complexity of contextual bandits and reinforcement learning: A disagreement-based perspective. Conference on Learning Theory (COLT), 2020.   \n[25] D. J. Foster, S. M. Kakade, J. Qian, and A. Rakhlin. The statistical complexity of interactive decision making. arXiv preprint arXiv:2112.13487, 2021.   \n[26] D. J. Foster, A. Krishnamurthy, D. Simchi-Levi, and Y. Xu. Offline reinforcement learning: Fundamental barriers for value function approximation. In Conference on Learning Theory, pages 3489\u20133489. PMLR, 2022.   \n[27] Z. Guo, S. Thakoor, M. P\u00eeslar, B. Avila Pires, F. Altch\u00e9, C. Tallec, A. Saade, D. Calandriello, J.-B. Grill, Y. Tang, et al. Byol-explore: Exploration by bootstrapped prediction. Advances in neural information processing systems, 35:31855\u201331870, 2022.   \n[28] W. Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the American Statistical Association, 58(301):13\u201330, 1963.   \n[29] A. Huang, J. Chen, and N. Jiang. Reinforcement learning in low-rank mdps with density features. International Conference on Machine Learning (ICML), 2023.   \n[30] R. Islam, M. Tomar, A. Lamb, Y. Efroni, H. Zang, A. Didolkar, D. Misra, X. Li, H. van Seijen, R. T. d. Combes, et al. Agent-controller representations: Principled offline rl with rich exogenous information. International Conference on Machine Learning (ICML), 2023.   \n[31] N. Jiang, A. Krishnamurthy, A. Agarwal, J. Langford, and R. E. Schapire. Contextual decision processes with low Bellman rank are PAC-learnable. In International Conference on Machine Learning, pages 1704\u20131713, 2017.   \n[32] C. Jin, Q. Liu, and S. Miryoosef.i Bellman eluder dimension: New rich classes of RL problems, and sample-efficient algorithms. Neural Information Processing Systems, 2021.   \n[33] S. M. Kakade. On the sample complexity of reinforcement learning. University of London, University College London (United Kingdom), 2003.   \n[34] M. Kearns and S. Singh. Finite-sample convergence rates for q-learning and indirect algorithms. Advances in neural information processing systems, 11, 1998.   \n[35] L. Kocsis and C. Szepesv\u00e1ri. Bandit based monte-carlo planning. In European conference on machine learning, pages 282\u2013293. Springer, 2006.   \n[36] A. Krishnamurthy, A. Agarwal, and J. Langford. PAC reinforcement learning with rich observations. In Advances in Neural Information Processing Systems, pages 1840\u20131848, 2016.   \n[37] A. Krishnamurthy, A. Agarwal, T.-K. Huang, H. Daum\u00e9 III, and J. Langford. Active learning for cost-sensitive classification. In International Conference on Machine Learning, pages 1915\u20131924, 2017.   \n[38] A. Lamb, R. Islam, Y. Efroni, A. R. Didolkar, D. Misra, D. J. Foster, L. P. Molu, R. Chari, A. Krishnamurthy, and J. Langford. Guaranteed discovery of control-endogenous latent states with multi-step inverse models. Transactions on Machine Learning Research, 2023.   \n[39] T. Lattimore, C. Szepesvari, and G. Weisz. Learning with good feature representations in bandits and in rl with a generative model. In International Conference on Machine Learning, pages 5662\u20135670. PMLR, 2020.   \n[40] G. Li, Y. Chen, Y. Chi, Y. Gu, and Y. Wei. Sample-efficient reinforcement learning is feasible for linearly realizable mdps with limited revisiting. Advances in Neural Information Processing Systems, 34:16671\u201316685, 2021.   \n[41] Q. Liu, P. Netrapalli, C. Szepesvari, and C. Jin. Optimistic mle: A generic model-based algorithm for partially observable sequential decision making. In Proceedings of the 55th Annual ACM Symposium on Theory of Computing, pages 363\u2013376, 2023.   \n[42] Z. Mhammedi, D. J. Foster, and A. Rakhlin. Representation learning with multi-step inverse kinematics: An efficient and optimal approach to rich-observation rl. International Conference on Machine Learning (ICML), 2023.   \n[43] D. Misra, M. Henaff, A. Krishnamurthy, and J. Langford. Kinematic state abstraction and provably efficient rich-observation reinforcement learning. arXiv preprint arXiv:1911.05815, 2019.   \n[44] D. Misra, M. Henaff, A. Krishnamurthy, and J. Langford. Kinematic state abstraction and provably efficient rich-observation reinforcement learning. In International conference on machine learning, pages 6961\u20136971. PMLR, 2020.   \n[45] M. A. Qassem, I. Abuhadrous, and H. Elaydi. Modeling and simulation of 5 dof educational robot arm. In 2010 2nd International Conference on Advanced Computer Control, volume 5, pages 569\u2013574. IEEE, 2010.   \n[46] S. Ross and D. Bagnell. Efficient reductions for imitation learning. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 661\u2013668. JMLR Workshop and Conference Proceedings, 2010.   \n[47] T. Salimans and R. Chen. Learning montezuma\u2019s revenge from a single demonstration. arXiv preprint arXiv:1812.03381, 2018.   \n[48] J. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre, S. Schmitt, A. Guez, E. Lockhart, D. Hassabis, T. Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. Nature, 588(7839):604\u2013609, 2020.   \n[49] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz. Trust region policy optimization. In International conference on machine learning, pages 1889\u20131897. PMLR, 2015.   \n[50] A. Sidford, M. Wang, X. Wu, L. Yang, and Y. Ye. Near-optimal time and sample complexities for solving markov decision processes with a generative model. Advances in Neural Information Processing Systems, 31, 2018.   \n[51] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484\u2013489, 2016.   \n[52] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre, D. Kumaran, T. Graepel, et al. A general reinforcement learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):1140\u20131144, 2018.   \n[53] Y. Tassa, Y. Doron, A. Muldal, T. Erez, Y. Li, D. d. L. Casas, D. Budden, A. Abdolmaleki, J. Merel, A. Lefrancq, et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018.   \n[54] A. Tavakoli, V. Levdik, R. Islam, C. M. Smith, and P. Kormushev. Exploring restart distributions. arXiv preprint arXiv:1811.11298, 2018.   \n[55] E. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ international conference on intelligent robots and systems, pages 5026\u20135033. IEEE, 2012.   \n[56] Y. Wang, R. Wang, and S. M. Kakade. An exponential lower bound for linearly-realizable MDPs with constant suboptimality gap. Neural Information Processing Systems (NeurIPS), 2021.   \n[57] G. Weisz, P. Amortila, B. Janzer, Y. Abbasi-Yadkori, N. Jiang, and C. Szepesv\u00e1ri. On queryefficient planning in mdps under linear realizability of the optimal state-value function. In Conference on Learning Theory, pages 4355\u20134385. PMLR, 2021.   \n[58] G. Weisz, P. Amortila, and C. Szepesv\u00e1ri. Exponential lower bounds for planning in MDPs with linearly-realizable optimal action-value functions. In Algorithmic Learning Theory, pages 1237\u20131264. PMLR, 2021.   \n[59] G. Weisz, A. Gy\u00f6rgy, T. Kozuno, and C. Szepesv\u00e1ri. Confident approximate policy iteration for efficient local planning in $q^{\\pi}$ -realizable mdps. Advances in Neural Information Processing Systems, 35:25547\u201325559, 2022.   \n[60] G. Weisz, A. Gy\u00f6rgy, and C. Szepesv\u00e1ri. Online rl in linearly $q^{\\pi}$ -realizable mdps is as easy as in linear mdps if you learn what to ignore. arXiv preprint arXiv:2310.07811, 2023.   \n[61] Z. Wen and B. Van Roy. Efficient reinforcement learning in deterministic systems with value function generalization. Mathematics of Operations Research, 42(3):762\u2013782, 2017.   \n[62] T. Xie and N. Jiang. Batch value-function approximation with only realizability. In International Conference on Machine Learning, pages 11404\u201311413. PMLR, 2021.   \n[63] T. Xie, D. J. Foster, Y. Bai, N. Jiang, and S. M. Kakade. The role of coverage in online reinforcement learning. In The Eleventh International Conference on Learning Representations, 2023.   \n[64] L. Yang and M. Wang. Sample-optimal parametric Q-learning using linearly additive features. In International Conference on Machine Learning, pages 6995\u20137004. PMLR, 2019.   \n[65] D. Yin, B. Hao, Y. Abbasi-Yadkori, N. Lazic\u00b4, and C. Szepesv\u00e1ri. Efficient local planning with linear function approximation. In International Conference on Algorithmic Learning Theory, pages 1165\u20131192. PMLR, 2022.   \n[66] D. Yin, S. Thiagarajan, N. Lazic, N. Rajaraman, B. Hao, and C. Szepesvari. Sample efficient deep reinforcement learning via local planning. arXiv preprint arXiv:2301.12579, 2023.   \n[67] A. Zanette, A. Lazaric, M. Kochenderfer, and E. Brunskill. Learning near optimal policies with low inherent bellman error. In International Conference on Machine Learning, pages 10978\u201310989. PMLR, 2020.   \n[68] X. Zhang, Y. Song, M. Uehara, M. Wang, A. Agarwal, and W. Sun. Efficient reinforcement learning in block mdps: A model-free representation learning approach. In International Conference on Machine Learning, pages 26517\u201326547. PMLR, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: The main claims do accurately reflect the paper\u2019s contribution. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 14}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Justification: Yes, see the discussion section. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 14}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: We provide a proof-sketch in the main paper and detailed proofs in the appendix. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 15}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 15}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 15}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 16}, {"type": "text", "text": "Justification: This paper has only mathematical congtent. There are no experiments in this paper. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 16}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 16}, {"type": "text", "text": "Justification: This paper has only mathematical congtent. There are no experiments in this paper. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. ", "page_idx": 16}, {"type": "text", "text": "\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 17}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 17}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: This paper has only mathematical congtent. There are no experiments in this paper. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 17}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: This paper does conform to the code of ethics. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 18}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 18}, {"type": "text", "text": "Answer: [No] ", "page_idx": 18}, {"type": "text", "text": "Justification: This paper provides a purely mathematical contribution. As such, it is subject to the standard ethical concerns present for all mathematical papers, but no further ones. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 18}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: This paper has only mathematical congtent. There are no experiments in this paper. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 19}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 19}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: This paper has only mathematical congtent. There are no experiments in this paper. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 19}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: This paper has only mathematical congtent. There are no experiments in this paper. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used. ", "page_idx": 19}, {"type": "text", "text": "\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 20}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: This paper has only mathematical congtent. There are no experiments in this paper. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 20}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: This paper has only mathematical congtent. There are no experiments in this paper. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 20}, {"type": "text", "text": "I Additional Results 25 ", "page_idx": 21}, {"type": "text", "text": "B Applying RVFS to Exogenous Block MDPs 25 ", "page_idx": 21}, {"type": "text", "text": "C Helper Lemmas 28   \nC.1 Concentration and Probability . . 28   \nC.2 Regression . . . 29   \nC.3 Reinforcement Learning . . 30 ", "page_idx": 21}, {"type": "text", "text": "II Proofs for SimGolf (Section 3) 33 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "D Preliminary Lemmas for Proof of Theorem 3.1 33 ", "page_idx": 21}, {"type": "text", "text": "E Proof of Theorem 3.1 35 ", "page_idx": 21}, {"type": "text", "text": "III Proofs for RVFS (Section 4) 37 ", "page_idx": 21}, {"type": "text", "text": "F Full Version of RVFS 37   \nF.1 RVFS Pseudocode 39   \nF.2 RVF $S^{\\in\\times0}$ Pseudocode 41 ", "page_idx": 21}, {"type": "text", "text": "G Organization 43 ", "page_idx": 21}, {"type": "text", "text": "H Overview of Analysis and Preliminaries 43 ", "page_idx": 21}, {"type": "text", "text": "H.1 Overview of Analysis 43   \nH.2 Benchmark Policy Class and Randomized Policies 44   \nH.3 Additional Preliminaries . 46 ", "page_idx": 21}, {"type": "text", "text": "I Guarantee under $V^{\\pi}$ -Realizability (Proof of Theorem 4.1, Setup II) 46 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "I.1 Analysis: Proof of Theorem 4.1 (Setup II) . . 46   \nI.2 Proof of Lemma I.1 (Number of Test Failures) . 48   \nI.3 Proof of Lemma I.2 (Consequence of Passing the Tests) . 50   \nI.4 Proof of Lemma I.3 (Value Function Regression Guarantee) 52   \nI.5 Proof of Lemma I.4 (Guarantee for Confidence Sets) . . . 54   \nI.6 Proof of Theorem I.1 (Main Guarantee of RVFS) . 57   \nI.7 Proof of Theorem I.2 (Guarantee of RVFS.bc) . . . 58 ", "page_idx": 21}, {"type": "text", "text": "J Guarantee under $V^{\\star}$ -Realizability (Proof of Theorem 4.1, Setup I) 59 ", "page_idx": 21}, {"type": "text", "text": "J.1 Analysis: Proof of Theorem 4.1 (Setup I) . . . 59   \nJ.2 Proof of Lemma J.1 (Relaxed $V^{\\pi}$ -Realizability under Gap) . . . 60 ", "page_idx": 21}, {"type": "text", "text": "K Guarantee for Weakly Correlated ExBMDPs (Proof of Theorem B.1) 61 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "K.1 Analysis: Proof of Theorem B.1 61   \nK.2 Proof of Lemma K.1 (Endogenous Benchmark Policies) . . . 65   \nK.3 Proof of Lemma K.2 (Snapping Probability) . . . . 65   \nK.4 Proof of Lemma K.3 (Coverability in Weakly Correlated ExBMDP) . . . 67   \nK.5 Proof of Lemma K.6 (Confidence Sets) 67   \nK.6 Proof of Lemma K.7 (Main Guarantee of $\\mathsf{R V F S}^{\\mathsf{e x o}}$ ) 69 ", "page_idx": 21}, {"type": "text", "text": "L Additional Technical Lemmas 70 ", "page_idx": 21}, {"type": "text", "text": "M BehaviorCloning Algorithm and Analysis 73 ", "page_idx": 21}, {"type": "text", "text": "A Additional Related Work ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Local simulators: Theoretical research. RL with local simulators has received extensive interest in the context of linear function approximation. Most notably, Weisz et al. [57] show that reinforcement learning with linear $V^{\\star}$ is tractable with local simulator access, and Li et al. [40] show that RL with linear $Q^{\\star}$ and a state-action gap is tractable; online RL is known to be intractable under the same assumptions [57, 56]. Amortila et al. [3] show that the gap assumption can be removed if a small number of expert queries are available. Also of note are the works of Yin et al. [65], Weisz et al. [59], which give computationally efficient algorithms under linear $Q^{\\pi}$ -realizability for all $\\pi$ ; this setting is known to be tractable in the online RL model [60], but computationally efficient algorithms are currently only known for RLLS. ", "page_idx": 22}, {"type": "text", "text": "Global simulators\u2014in which the agent can query arbitrary state-action pairs and observe next state transitions\u2014have also received theoretical investigation, but like local simulators, results are largely restricted to tabular reinforcement learning and linear models [34, 33, 50, 17, 64, 39]. ", "page_idx": 22}, {"type": "text", "text": "Local simulators: Empirical research. The Go-Explore algorithm [19, 20] uses local simulator access to achieve state-of-the-art performance for the Atari games Montezuma\u2019s Revenge and Pitfall\u2014 both notoriously difficult games that require systematic exploration. To the best of our knowledge, the performance of Go-Explore on these tasks has yet to be matched by online reinforcement learning; the performing agents [7, 27] are roughly a factor of four worse in terms of cumulative reward. Interestingly, like RVFS, Go-Explore makes use of core sets of informative state-action pairs to guide exploration. However, Go-Explore uses an ad-hoc, domain specific approach to designing the core set, and does not use function approximation to drive exploration. ", "page_idx": 22}, {"type": "text", "text": "Recent work of Yin et al. [66] provides an empirical framework for online RL with local planning that can take advantage of deep neural function approximation, and is inspired by the theoretical works in Weisz et al. [57], Li et al. [40], Yin et al. [65], Weisz et al. [59]. This approach does not have provable guarantees, but achieves super-human performance at Montezuma\u2019s Revenge. ", "page_idx": 22}, {"type": "text", "text": "Other notable empirical works that incorporate local simulator access, as highlighted by Yin et al.   \n[66], include Schulman et al. [49], Salimans and Chen [47], Tavakoli et al. [54]. ", "page_idx": 22}, {"type": "text", "text": "Planning. RL with local simulator access is a convenient abstraction for the problem of planning: Given a known (e.g., learned) model, compute an optimal policy. Planning with a learned model is an important task in theory [25, 41] and practice (e.g., MuZero [48]). Since the model is known, computing an optimal policy is a purely computational problem, not a statistical problem. Nonetheless, for planning problems in large state spaces, where enumerating over all states is undesirable, algorithms for online RL with local simulator access can be directly applied, treating the model as if it were the environment the agent is interacting with. Here, any computationally efficient RLLS algorithm immediately yields an efficient algorithm for planning. ", "page_idx": 22}, {"type": "text", "text": "Empirically, Monte-Carlo Tree Search [13, 35] is a successful paradigm for planning, acting as a key component in AlphaGo [51] and AlphaZero [52].6Viewed as a planning algorithm, a potential advantage of RVFS is that it is well suited to stochastic environments, and provides a principled way to use estimated (neural) value function estimates to guide exploration. ", "page_idx": 22}, {"type": "text", "text": "Coverability. Xie et al. [63] introduced coverability as a structural parameter for online reinforcement leanring, inspired by connections between online and offline RL. Existing guarantees for the online RL framework based on coverability require either Bellman completeness [63], model-based realizability [5], or weight function realizability [4, 5]), and it is not currently known whether value function realizability is sufficient in this framework. ", "page_idx": 22}, {"type": "text", "text": "Exogenous Block MDPs. Our results in Section 3.3 (Corollary 3.1) show that general Exogenous Block MDPs are learnable with local simulator access. Prior work, on learning EXBMDPs in the online RL model requires additional assumptions: ", "page_idx": 22}, {"type": "text", "text": "\u2022 Deterministic ExBMDP [22]. In this setting, the latent transition distribution $T^{\\mathrm{endo}}$ is assumed to be deterministic. In this case, it suffices to learn open-loop policies (i.e., policies that play a deterministic sequence of actions). This avoids compounding errors due to ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "learning imperfect decoders that depend on the exogenous noise, making this setting much less challenging than the general ExBMDP setting.   \n\u2022 Factored ExMDP [21]. This is an ExBMDP setting with a restrictive structure in which the observation is a $d\\!\\cdot$ -dimensional vector and the latent state is a $k$ -dimensional subset of the observed coordinates. This structure prevents the setting from subsuming the basic (non-exogenous) Block MDP framework, and makes it possible to learn decoders that act only on the endogenous state, preventing compounding errors.   \n\u2022 Bellman completeness. Xie et al. [63] observed that ExBMDPs admit low coverability, but their algorithm requires Bellman completeness, which is not satisfied by ExBMDPs (see Efroni et al. [22], Islam et al. [30]). ", "page_idx": 23}, {"type": "text", "text": "Part I Additional Results ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "This section of the appendix contains additional results omitted from the main body due to space constraints. ", "page_idx": 24}, {"type": "text", "text": "B Applying RVFS to Exogenous Block MDPs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We now apply RVFS to the Exogenous Block MDP (ExBMDP) model introduced in Section 3.3. ExBMDPs satisfy coverability (Assumption 3.2), but do not satisfy the pushforward coverability assumption (Assumption 4.1) required by RVFS in general. However, it turns out that ExBMDPs do satisfy pushforward coverability when the exogenous noise process is weakly correlated across time; we refer to this new statistical assumption as the weak correlation condition. ", "page_idx": 24}, {"type": "text", "text": "Assumption B.1 (Weak correlation condition). For the underlying ExBMDP $\\mathcal{M}$ , there is a constant $C_{\\mathsf{e x o}}\\geq1$ such that for all $h\\in[H-1]$ and $(\\xi,\\xi^{\\prime})\\in\\Xi_{h-1}\\times\\Xi_{h}$ , we have7 ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\big[\\pmb{\\xi}_{h}=\\xi,\\pmb{\\xi}_{h+1}=\\xi^{\\prime}\\big]\\leq C_{\\mathrm{exo}}\\cdot\\mathbb{P}\\big[\\pmb{\\xi}_{h}=\\xi\\big]\\cdot\\mathbb{P}\\big[\\pmb{\\xi}_{h+1}=\\xi^{\\prime}\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The weak correlation property asserts that the joint law for the exogenous noise variables $\\xi_{h}$ and $\\xi_{h+1}$ is at most a multiplicative factor $C_{\\mathsf{e x o}}\\geq1$ larger than the corresponding product distribution obtained by sampling $\\xi_{h}$ and $\\xi_{h+1}$ independently from their marginals. This setting strictly generalizes the (non-exogenous) Block MDP model [36, 15, 43, 68, 42], by allowing for arbitrary stochastic dynamics for the endogenous state and an arbitrary emission process, but requires that temporal correlations in the exogenous noise decay over time. ", "page_idx": 24}, {"type": "text", "text": "We show that under Assumption B.1, pushforward coverability is satisfied with $C_{\\mathsf{p u s h}}\\leq C_{\\mathsf{e x o}}\\cdot S A$ (Lemma K.3 in Appendix K.1). In addition, $V^{\\star}$ -realizability is implied by decoder realizability (Lemma D.1). Thus, by applying Theorem 4.1 (Setup I), we conclude that RVFS efficiently learns a near-optimal policy for any weakly correlated ExBMDP for which the optimal value function has \u2206-gap. ", "page_idx": 24}, {"type": "text", "text": "An improved algorithm for ExBMDPs: RVFSexo. At first glance, removing the gap assumption for RVFS in ExBMDPs seems difficult: The $V^{\\pi}$ -realizability assumption required to invoke Theorem 4.1 (Setup II) is not satisfied by ExBMDPs, as decoder realizability only implies $V^{\\pi}$ realizability for endogenous policies $\\pi$ .8 In spite of this, we now show that with a slight modification, RVFS can efficiently learn any weakly correlated ExBMDP under decoder realizability alone (without gap or $V^{\\pi}$ -realizability). ", "page_idx": 24}, {"type": "text", "text": "Our new variant of RVFS, $\\mathsf{R V F S}^{\\mathsf{e x o}}$ , is presented in Algorithm 8 (deferred to Appendix F for space). The algorithm is almost identical to RVFS (Algorithm 5), with the main difference being that we use an additional randomized rounding step to compute the policies $\\widehat{\\pi}_{1:H}$ from the learned value functions $\\widehat{V}_{1:H}$ . In particular, instead of directly defining the policies $\\widehat{\\pi}_{1:H}$ based on the bellman backups $\\mathcal{P}_{h}[\\widehat{V}_{h+1}]$ as in Eq. (14), RVF $S^{\\mathrm{exo}}$ targets a \u201crounded\u201d version of the backup given by ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\varepsilon\\cdot\\lceil\\mathcal{P}_{h}[\\widehat{V}_{h+1}](x,a)/\\varepsilon+\\zeta_{h}\\rceil,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\varepsilon\\in(0,1)$ is a rounding parameter and $\\zeta_{1},\\L\\dots,\\zeta_{H}$ are i.i.d. random variables sampled uniformly at random from the interval $[0,1/2]$ (at the beginning of the algorithm\u2019s execution). Concretely, $\\mathsf{R V F S}^{\\mathsf{e x o}}$ estimates the bellman backup $\\mathcal{P}_{h}[\\widehat{V}_{h+1}](x,a)$ in Eq. (3) using the local simulator (as in Eq. (14) of Algorithm 5), and defines its policies via ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\widehat{\\pi}_{h}(\\cdot)\\in\\mathop{\\arg\\operatorname*{max}}_{a\\in\\mathcal{A}}[\\widehat{\\mathcal{P}}_{h}[\\widehat{V}_{h+1}](\\cdot,a)/\\varepsilon+\\zeta_{h}].\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "This rounding scheme, which quantizes the Bellman backup into $\\varepsilon^{-1}$ bins with a random offset, is designed to emulate certain properties implied by the $\\Delta$ -gap assumption (Assumption 4.4). Specifically, we show that with constant probability over the draw of $\\zeta_{1:H}$ , the policy $\\widehat{\\pi}$ in (4) \u201csnaps\u201d on to an endogenous policy $\\pi$ . This means that for $\\mathsf{R V F S}^{\\mathsf{e x o}}$ to succeed (with constant probability), it suffices to pass it a class $\\nu$ that realizes the value functions $(V_{h}^{\\pi})$ for endogenous policies $\\pi\\in\\Pi_{\\ S}$ . Fortunately, such a function class can be constructed explicitly under decoder realizability (Assumption 3.3). ", "page_idx": 25}, {"type": "text", "text": "Lemma B.1 ([21]). For the ExBMDP setting, under Assumption 3.3, the function class $\\mathcal{V}_{h}:=\\{x\\mapsto$ $f(\\phi(x)):f\\in[0,H]^{S},\\phi\\in\\Phi\\}$ is such that $V_{h}^{\\pi}\\in\\mathcal{V}_{h}$ for all endogenous policies $\\pi$ . Furthermore, the policy class $\\Pi_{h}:=\\left\\{\\pi(\\cdot)\\in\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}f\\bigl(\\phi(\\cdot),a\\bigr):f\\in[0,H]^{S\\times A},\\phi\\in\\Phi\\right\\}$ contains all endogenous policies. ", "page_idx": 25}, {"type": "text", "text": "A small technical challenge with the scheme above is that it is only guaranteed to succeed with constant probability over the draw of the rounding parameters $\\zeta_{1},\\dotsc,\\zeta_{H}$ . To address this, we provide an outer-level algorithm, $\\mathsf{R V F S}^{\\mathsf{e x o}}$ .bc (Algorithm 9, deferred to Appendix F for space), which performs confidence boosting by invoking $\\mathsf{R V F S}^{\\mathsf{e x o}}$ multiple times independently, and extracts a high-quality executable policy using behavior cloning. ", "page_idx": 25}, {"type": "text", "text": "Main result. We now state the main guarantee for $\\mathsf{R V F S}^{\\mathsf{e x o}}$ (the proof is in Appendix K). ", "page_idx": 25}, {"type": "text", "text": "Theorem B.1 (Main guarantee of RVF $S^{\\in\\times0}$ for EXBMDPs). Consider the ExBMDP setting. Suppose the decoder class $\\Phi$ satisfies Assumption 3.3, and that Assumption B.1 holds with $C_{\\mathsf{e x o}}>0$ . Let $\\varepsilon,\\delta\\in(0,1)$ be given, and let $\\lvert\\lambda_{h}$ and $\\Pi_{h}$ be as in Lemma B.1. Then RVF $S^{\\etimes\\circ}$ . $\\mathsf{b c}(\\Pi,\\mathcal{V}_{1:H},\\varepsilon,\\zeta_{1:H},\\delta)$ (Algorithm 9) produces a policy $\\widehat{\\pi}_{1:H}$ such that $J(\\pi^{\\star})-J(\\widehat{\\pi}_{1:H})\\leq\\varepsilon$ , and has total sample complexity ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\widetilde{\\cal O}\\left(C_{\\mathrm{exo}}^{8}S^{8}H^{36}A^{9}\\cdot\\varepsilon^{-26}\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "This result shows for the first time that sample- and computationally-efficient learning is possible for ExBMDPs beyond deterministic or factored settings [22, 21]. ", "page_idx": 25}, {"type": "text", "text": "We mention in passing that our use of randomized rounding to emulate certain consequences of the $\\Delta$ -gap assumption leverages the fact that ExBMDPs have a finite number of (endogenous) latent states. It is unclear if this technique can be used when the (latent) state space is large or infinite. ", "page_idx": 25}, {"type": "text", "text": "1: parameters: Value function class $\\nu$ , suboptimality $\\varepsilon\\in(0,1)$ , seeds $\\zeta_{1:H}\\in(0,1)$ , confidence $\\bar{\\delta}\\in(0,1)$ .   \n2: input: Level $h\\in[0\\ldots H]$ , value function estimates $\\widehat{V}_{h+1:H}$ , confidence sets $\\widehat{\\mathcal{V}}_{h+1:H}$ , stateaction collections $\\mathcal{C}_{h:H}$ , and buffers $\\boldsymbol{B}_{h:H}$ , and counters $t_{h:H}$ . $^{\\prime\\star}$ Initialize parameters. $\\star/$   \n3: Set $M\\gets\\lceil8\\varepsilon^{-2}C_{\\tt e x o}S A H\\rceil$ .   \n4: Set $N_{\\mathrm{test}}\\stackrel{.}{\\leftarrow}2^{8}M^{2}H\\varepsilon^{-2}\\operatorname*{lig}\\bigl(8M^{6}H^{8}\\varepsilon^{-2}\\delta^{-1}\\bigr)$ , $N_{\\mathrm{reg}}\\leftarrow2^{8}M^{2}\\varepsilon^{-2}\\log(8|\\mathcal{V}|H M^{2}\\delta^{-1}).$ .   \n5: Set $N_{\\mathrm{est}}(k)\\leftarrow2N_{\\mathrm{reg}}^{2}\\log(8A N_{\\mathrm{reg}}H k^{3}/\\delta)$ and $\\delta^{\\prime}\\stackrel{<}{\\leftarrow}\\delta/(4M^{7}N_{\\mathrm{test}}^{2}H^{8}|\\mathcal{V}|)$ .   \n6: Set \u03b5r2eg \u21909MH2 log(N8M 2H\u2223V\u2223/\u03b4)+ 34MH3 log(N8M 6N t2estH8/\u03b4).   \n7: Set $\\beta(t)\\leftarrow\\sqrt{\\log_{1/\\delta^{\\prime}}(8M A|\\mathcal{V}|t^{2}/\\delta)}$ . /\\* Test the fit for the estimated value functions $\\widehat{V}_{h+1:H}$ at future layers. \\*/   \n8: for $(x_{h-1},a_{h-1})\\in{\\mathcal{C}}_{h}$ do ", "page_idx": 26}, {"type": "text", "text": "10: ", "page_idx": 26}, {"type": "text", "text": "for $n=1,\\dots,N_{\\mathrm{test}}\\,\\epsilon$ do Draw $\\pmb{x}_{h}\\sim T_{h-1}(\\cdot\\mid x_{h-1},a_{h-1})$ , then draw $x_{\\ell-1}$ by rolling out with $\\widehat{\\pi}_{h+1:H}$ , where ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\forall\\,\\tau\\in[H],\\,\\,\\,\\widehat{\\pi}_{\\tau}(\\cdot)\\in\\underset{a\\in\\mathcal{A}}{\\arg\\operatorname*{max}}\\big[\\widehat{\\mathcal{P}}_{\\tau,\\varepsilon^{2},\\delta^{\\prime}}\\big[\\widehat{V}_{\\tau+1}\\big](\\cdot,a)\\cdot\\varepsilon^{-1}+\\zeta_{\\tau}\\big],\\quad\\mathrm{with}\\quad\\widehat{V}_{H+1}\\equiv0.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "for $a_{\\ell-1}\\in A$ do Update $t_{\\ell}\\gets t_{\\ell}+1$ . $^{\\prime\\star}$ Test fit; if test fails, re-fit value functions $\\widehat{V}_{h+1:\\ell}$ up to layer ", "page_idx": 26}, {"type": "text", "text": "\\* ", "page_idx": 26}, {"type": "text", "text": "15: ", "page_idx": 26}, {"type": "text", "text": "16: ", "page_idx": 26}, {"type": "text", "text": "17: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{if}\\operatorname*{sup}_{f\\in\\widehat{\\mathcal{V}}_{\\ell}}|(\\widehat{\\mathcal{P}}_{\\ell-1,\\varepsilon^{2},\\delta^{\\prime}}[\\widehat{\\mathcal{V}}_{\\ell}]-\\widehat{\\mathcal{P}}_{\\ell-1,\\varepsilon^{2},\\delta^{\\prime}}[f_{\\ell}])(x_{\\ell-1},a_{\\ell-1})|>\\varepsilon^{2}+\\varepsilon^{2}\\cdot\\beta(t_{\\ell})\\mathbf{\\,then}}\\\\ &{\\qquad\\mathcal{C}_{\\ell}\\gets\\mathcal{C}_{\\ell}\\cup\\big\\{\\left(x_{\\ell-1,\\,a_{\\ell-1}}\\right)\\big\\}\\mathrm{~and~}B_{\\ell}\\gets B_{\\ell}\\cup\\big\\{\\left(x_{\\ell-1,\\,a_{\\ell-1}},\\widehat{\\mathcal{V}}_{\\ell},\\widehat{\\mathcal{V}}_{\\ell},t_{\\ell}\\right)\\big\\}.}\\\\ &{\\qquad\\mathbf{for}\\ \\tau=\\ell,\\ldots,h+1\\mathbf{\\,d0}}\\\\ &{\\qquad\\qquad\\big(\\widehat{\\mathcal{V}}_{\\tau:H},\\widehat{\\mathcal{V}}_{\\tau:H},\\mathcal{C}_{\\tau:H},B_{\\tau:H},t_{\\tau:H}\\big)\\gets\\mathsf{R V S}_{\\tau}^{\\mathrm{exo}}(\\widehat{\\mathcal{V}}_{\\tau+1:H},\\widehat{\\mathcal{V}}_{\\tau+1:H},\\mathcal{C}_{\\tau:H},B_{\\tau:H},t_{\\tau:H};\\mathcal{V},\\varepsilon,\\zeta_{1:H})}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "18: ", "page_idx": 26}, {"type": "text", "text": "go to line 8. ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "19: if $h=0$ then return $(\\widehat{V}_{1:H},\\cdot,\\cdot,\\cdot,\\cdot)$ . /\\* Re-fit $\\widehat{V}_{h}$ and build a new confidence set. $\\star/$ ", "page_idx": 26}, {"type": "text", "text": "20: for $(x_{h-1},a_{h-1})\\in{\\mathcal{C}}_{h}$ do ", "page_idx": 26}, {"type": "text", "text": "21: Set $\\mathscr{D}_{h}\\big(x_{h-1},a_{h-1}\\big)\\gets\\emptyset$   \n22: for $i=1,\\ldots,N_{\\mathrm{reg}}\\;\\mathbf{do}$ ", "page_idx": 26}, {"type": "text", "text": "For each $a\\in\\mathcal{A}$ , let $\\widehat{V}_{h}({\\pmb x}_{h})$ be a Monte-Carlo estimate for $\\mathbb{E}^{\\widehat{\\pi}_{h:H}}\\left[\\sum_{\\ell=h}^{H}\\pmb{r}_{\\ell}\\mid\\pmb{x}_{h}\\right]$ computed by collecting $N_{\\mathrm{est}}(|\\mathcal{C}_{h}|)$ trajectories starting from $x_{h}$ and rolling out with $\\widehat{\\pi}_{h:H}$ . ", "page_idx": 26}, {"type": "text", "text": "26: Let $\\begin{array}{r}{V_{h}:=\\arg\\operatorname*{min}_{\\substack{c\\in\\mathcal{V}_{h}}}\\sum_{(x_{h-\\frac{1}{\\alpha},h-1})\\in\\mathcal{C}_{h}}\\sum_{(x_{h},v_{h})\\in\\mathcal{D}_{h}\\left(x_{h-1},a_{h-1}\\right)}(f(x_{h})-v_{h})^{2}.}\\end{array}$ .   \n27: Compute value function confidence set ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\widehat{\\mathcal{V}}_{h}:=\\left\\{f\\in\\mathcal{V}_{h}\\,\\Bigg|\\,\\sum_{(x_{h-1},a_{h-1})\\in\\mathcal{C}_{h}}\\frac{1}{N_{\\mathrm{reg}}}\\sum_{(x_{h},\\cdot)\\in\\mathcal{D}_{h}(x_{h-1},a_{h-1})}\\left(\\widehat{V}_{h}(x_{h})-f(x_{h})\\right)^{2}\\leq\\varepsilon_{\\mathrm{reg}}^{2}\\right\\}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "28: return $(\\widehat{V}_{h:H},\\widehat{\\mathcal{V}}_{h:H},\\mathcal{C}_{h:H},\\mathcal{B}_{h:H},t_{h:H})$ . ", "page_idx": 26}, {"type": "text", "text": "Algorithm 4 RVFSexo.bc: Learn an executable policy with $\\mathsf{R V F S}^{\\mathsf{e x o}}$ via imitation learning. ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1: input: Decoder class $\\Phi$ , suboptimality $\\varepsilon\\in(0,1)$ , confidence $\\delta\\in(0,1)$ . /\\* Set parameters for RVFS and define the value function and policy classes. \\*/   \n2: Set $\\varepsilon_{\\mathsf{R V F S}}\\gets\\varepsilon H^{-1}/48$ .   \n3: Set $\\mathcal{V}=\\mathcal{V}_{1:H}$ , where $\\mathcal{V}_{h}=\\left\\{x\\mapsto f(\\phi(x)):f\\in[0,H]^{S},\\phi\\in\\Phi\\right\\}$ , $\\forall h\\in[H]$ .   \n4: Set $\\textstyle\\prod=\\prod_{1:H}$ , where $\\begin{array}{r}{:\\Pi_{h}=\\big\\{\\pi(\\cdot)\\in\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}\\,f\\big(\\phi(\\cdot),a\\big):f\\in[0,H]^{S\\times A},\\phi\\in\\Phi\\big\\},\\,\\forall h\\in[H].}\\end{array}$ /\\* Set parameters for BehaviorCloning. $\\star/$   \n56::  SSeett $N_{\\mathrm{bc}}\\gets8H^{2}\\log(4H|\\Pi|/\\delta)/\\varepsilon$ $N_{\\mathrm{boost}}\\leftarrow\\log(1/\\delta)/\\log(24S A H\\varepsilon)$ $N_{\\mathrm{eval}}\\leftarrow16^{2}\\varepsilon^{-2}\\log(2N_{\\mathrm{boost}}/\\delta)$ $M\\gets\\lceil8\\varepsilon_{\\mathsf{R V F S}}^{-1}S A C_{\\mathsf{c o v}}H\\rceil,$ $N_{{\\mathrm{test}}}\\gets2^{8}M^{2}H\\varepsilon_{\\sf R V F S}^{-1}\\log\\bigl(80M^{6}H^{8}N_{{\\mathrm{boost}}}\\varepsilon_{\\sf R V F S}^{-2}\\delta^{-1}\\bigr)$ $\\begin{array}{r}{\\delta^{\\prime}=\\frac{\\delta}{40M^{7}N^{2}H^{8}|\\mathcal{V}|N_{\\mathrm{boost}}}}\\end{array}$   \n7: Set $N_{\\mathrm{reg}}\\gets2^{8}M^{2}\\varepsilon_{\\mathsf{R V F S}}^{-1}\\log(80|\\Phi|^{2}H M^{2}N_{\\mathsf{b o o s t}}\\delta^{-1})$ .   \n8: Set $\\widehat{V}_{1:H}\\gets$ arbitrary, $\\widehat{\\mathcal{V}}_{1:H}\\gets\\mathcal{V}$ , $\\mathcal{C}_{0:H}\\gets\\emptyset$ , $B_{0:H}\\gets\\emptyset$ , $i_{\\mathsf{o p t}}=1$ , and $J_{\\operatorname*{max}}=0$ . /\\* Repeatedly invoke RVFSexo and extract policy with BehaviorCloning to boost confidence. $\\star/$ 9: for $i=1,\\dots,N_{\\mathrm{boost}}\\,\\dot{\\mathbf{c}}$ do $^{\\prime\\star}$ Invoke RVFSexo. $\\star/$   \n10: $\\big(\\widehat{V}_{1:H}^{(\\ast)},\\cdot,\\cdot,\\cdot\\big)\\gets\\mathsf{R W F S}_{0}^{\\mathrm{exo}}(\\widehat{V}_{1:H},\\widehat{V}_{1:H},\\mathcal{C}_{0:H},\\mathcal{B}_{0:H};\\mathcal{V},N_{\\mathrm{reg}},N_{\\mathrm{test}},\\varepsilon_{\\mathsf{R W F S}},\\delta/(10N_{\\mathrm{boost}})).$ $^{\\prime\\star}$ fminpeu $\\star/$   \n1121:: $\\widehat{\\pi}_{h}^{\\mathsf{R V F S}}(\\cdot)\\in\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}\\widehat{\\mathcal{P}}_{h,\\varepsilon_{\\mathsf{R V F S}},\\delta^{\\prime}}[\\widehat{V}_{h+1}^{(i)}](\\cdot,a)$ $\\widehat{\\pi}_{1:H}^{(i)}\\gets$ $\\left<\\Pi,\\varepsilon,\\widehat{\\pi}_{1:H}^{\\mathsf{R V F S}},\\delta/(2N_{\\mathsf{b o o s t}})\\right>$ $^{\\prime\\star}$ Evaluate current policy. \\*/   \n13: $v=0$ .   \n14: $\\mathbf{for}=1,\\dots,N_{\\mathrm{eval}}$ do   \n1156:: SSaet $\\begin{array}{r}{\\boldsymbol{v}\\leftarrow\\boldsymbol{v}+\\sum_{h=1}^{H}\\boldsymbol{r}_{h}}\\end{array}$ $(\\pmb{x}_{1},\\pmb{a}_{1},\\pmb{r}_{1},\\dots,\\pmb{x}_{H},\\pmb{a}_{H},\\pmb{r}_{H})$ by executing $\\widehat{\\pi}_{1:H}^{(i)}$ .   \n17: Set $\\widehat{J}(\\widehat{\\pi}_{1:H}^{(i)})\\leftarrow v/N_{\\mathrm{eval}}$ .   \n18: if $\\widehat{J}(\\widehat{\\pi}_{1:H}^{(i)})>J_{\\operatorname*{max}}$ then   \n19: Set $i_{\\mathsf{o p t}}=i$ .   \n20: Set $J_{\\operatorname*{max}}=\\widehat{J}(\\widehat{\\pi}_{1:H}^{(i)})$ .   \n21: return: \u03c0\u03021\u2236H = \u03c0\u03021i\u2236oHpt . ", "page_idx": 27}, {"type": "text", "text": "C Helper Lemmas ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "This section of the appendix contains supporting lemmas used within the proofs of our main results. ", "page_idx": 27}, {"type": "text", "text": "C.1 Concentration and Probability ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Lemma C.1. Let $\\delta\\,\\in\\,(0,1)$ and $H\\ge1$ be given. If a sequence of events $\\mathcal{E}_{1},\\hdots,\\mathcal{E}_{H}$ satisfies $\\mathbb{P}[\\mathcal{E}_{h}\\mid\\mathcal{E}_{1},\\ldots,\\mathcal{E}_{h-1}]\\ge1-\\delta/H$ for all $h\\in[H],$ , then ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{P}[\\mathcal{E}_{1:H}]\\geq1-\\delta.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof of Lemma C.1. By the chain rule, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{P}[\\mathcal{E}_{1:H}]=\\prod_{h\\in[H]}\\mathbb{P}[\\mathcal{E}_{h}\\,|\\,\\mathcal{E}_{1},\\dots,\\mathcal{E}_{h-1}]\\ge\\prod_{h\\in[H]}\\left(1-\\delta/H\\right)=\\left(1-\\delta/H\\right)^{H}\\ge1-\\delta.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We make use of the following version of Freedman\u2019s inequality, due to Agarwal et al. [1, Lemma 9]: ", "page_idx": 27}, {"type": "text", "text": "Lemma C.2. Let $R>0$ be given and let $w_{1},\\ldots w_{n}$ be a sequence of real-valued random variables adapted to filtration $\\mathcal{H}_{1},\\cdots,\\mathcal{H}_{n}$ . Assume that for all $t\\in[n]$ , $w_{i}\\leq R$ and $\\mathbb{E}[{\\pmb w}_{i}\\mid\\mathcal{H}_{i-1}]=0$ . Define $\\begin{array}{r}{S_{n}\\;:=\\;\\sum_{t=1}^{n}w_{i}}\\end{array}$ and $\\begin{array}{r}{V_{n}\\;:=\\;\\sum_{t=1}^{n}\\mathbb{E}[\\pmb{w}_{i}^{2}\\;\\mid\\;\\mathcal{H}_{i-1}]}\\end{array}$ . Then, for any $\\delta\\,\\in\\,(0,1)$ and $\\lambda\\,\\in\\,[0,1/R]$ , with probability at least $1-\\delta$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\nS_{n}\\leq\\lambda V_{n}+\\log(1/\\delta)/\\lambda.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We will also use the following lemma, which is a standard consequence of Freedman\u2019s inequality. ", "page_idx": 27}, {"type": "text", "text": "Lemma C.3 (e.g., Foster et al. [25]). Let $(\\pmb{w}_{t})_{t\\leq T}$ be a sequence of random variables adapted to a filtration $(\\mathcal{H}_{t})_{t\\leq T}$ . If ${}^{c}0\\leq w_{t}\\leq R$ almost surely, then with probability at least $1-\\delta$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\pmb{w}_{t}\\leq\\frac{3}{2}\\sum_{t=1}^{T}\\mathbb{E}_{t-1}[\\pmb{w}_{t}]+4R\\log(2\\delta^{-1}),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{E}_{t-1}\\big[\\pmb{w}_{t}\\big]\\le2\\sum_{t=1}^{T}\\pmb{w}_{t}+8R\\log(2\\delta^{-1}).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "C.2 Regression ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Using Lemmas C.2 and C.3, we obtain the following concentration lemma, which will be used to prove guarantees for square loss regression within our algorithms. ", "page_idx": 28}, {"type": "text", "text": "Lemma C.4. Let $B>0$ and $n\\in\\mathbb{N}$ be given, and let $\\boldsymbol{\\mathscr{y}}$ be an abstract set. Further, let $\\mathcal{Q}\\subseteq\\{g:\\mathcal{Y}\\to}$ $[0,B]\\}$ be a finite function class and $y_{1},\\ldots,y_{n}$ be a sequence of random variables in $\\boldsymbol{\\wp}$ adapted to filtration a $\\mathcal{H}_{1},\\cdots,\\mathcal{H}_{n}$ . Then, for any $\\delta\\in(0,1)$ , with probability at least $1-\\delta$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\forall g\\in\\mathcal{Q},\\quad\\frac{1}{2}\\|g\\|^{2}-2B^{2}\\log(2|\\mathcal{Q}|/\\delta)\\leq\\|g\\|_{n}^{2}\\leq2\\|g\\|^{2}+2B^{2}\\log(2|\\mathcal{Q}|/\\delta),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$\\begin{array}{r}{\\|g\\|^{2}:=\\sum_{i\\in[n]}\\mathbb{E}[g(y_{i})^{2}\\mid\\mathcal{H}_{i-1}]\\;a n d\\;\\|g\\|_{n}^{2}:=\\sum_{i=1}^{n}g(\\pmb{y}_{i})^{2}.}\\end{array}$ ", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof of Lemma C.4. Fix $g\\in{\\mathcal{Q}}$ . Applying Lemma C.2 with $\\pmb{w}_{i}=g(\\pmb{y}_{i})^{2}-\\mathbb{E}[g(\\pmb{y}_{i})^{2}\\mid\\mathcal{H}_{i-1}]$ , for all $i\\in[n]$ , and $(R,\\lambda)=(B^{\\tilde{2}},1/B^{2})$ , we get that with probability at least $1-\\delta/(2|\\mathcal{Q}|)$ : ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\|g\\|_{n}^{2}-\\|g\\|^{2}\\le\\lambda B^{2}\\|g\\|^{2}+\\log(2|\\mathcal{Q}|/\\delta)/\\lambda.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By substituting $\\lambda=B^{-2}$ and rearranging, we get ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\|g\\|_{n}^{2}\\leq2\\|g\\|^{2}+B^{2}\\log(2|\\mathcal{Q}|/\\delta).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Similarly, applying Lemma C.2 with $\\pmb{w}_{i}=\\mathbb{E}[g(\\pmb{y}_{i})^{2}\\mid\\mathcal{H}_{i-1}]-g(\\pmb{y}_{i})^{2}$ , for all $i\\in[n]$ , and $(R,\\lambda)=$ $(B^{2},1/(2B^{2}))$ , we get that with probability at least $1-\\delta/(2|\\mathcal{Q}|)$ : ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\|g\\|^{2}-\\|g\\|_{n}^{2}\\le\\lambda B^{2}\\|g\\|^{2}+\\log(2|\\mathcal{Q}|/\\delta)/\\lambda.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By substituting $\\lambda=2^{-1}B^{-2}$ and rearranging, we get ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left\\|g\\right\\|_{n}^{2}\\geq\\frac{1}{2}\\|g\\|^{2}-2B^{2}\\log(2|\\mathcal{Q}|/\\delta).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Combining this with (6) and the union bound, we get the desired result. ", "page_idx": 28}, {"type": "text", "text": "With this lemma, we now prove the following key result for square loss regression. ", "page_idx": 28}, {"type": "text", "text": "Lemma C.5 (Generic regression guarantee). Let $B>0$ and $n\\in\\mathbb{N}$ be given and $\\boldsymbol{\\wp}$ be an abstract set. Further, let $\\mathcal{F}\\subseteq\\{f:\\bar{\\mathcal{V}}\\to[0,\\bar{B}]\\}$ be a finite function class, and suppose that there is a function $f_{\\star}\\in\\mathcal{F}$ and a sequence of random variables $(\\pmb{y}_{1},\\pmb{x}_{1}),\\dots,(\\pmb{y}_{n},\\pmb{x}_{n})\\in\\mathcal{Y}\\times\\mathbb{R}$ such that for all $i\\in[n]$ : ", "page_idx": 28}, {"type": "text", "text": "\u2022 $\\begin{array}{r}{{\\pmb x}_{i}=f_{\\star}\\!\\left({\\pmb y}_{i}\\right)+{\\pmb\\varepsilon}_{i}+{\\pmb b}_{i};}\\end{array}$   \n\u2022 $\\begin{array}{r}{\\left|b_{i}\\right|\\leq\\xi,}\\end{array}$ ;   \n\u2022 $\\varepsilon_{i}\\in[-B,B]$ ; and   \n\u2022 $\\mathbb{E}[\\varepsilon_{i}\\mid\\mathfrak{F}_{i}]=0$ , where $\\mathfrak{F}_{i}:=\\sigma\\big(\\pmb{y}_{1:i},\\pmb{\\varepsilon}_{1:i-1},\\pmb{x}_{1:i-1},\\pmb{b}_{1:i-1}\\big).$ ", "page_idx": 28}, {"type": "text", "text": "Then, for $\\begin{array}{r}{\\widehat{f}\\in\\arg\\operatorname*{min}_{f\\in\\mathcal{F}}\\sum_{i=1}^{n}(f(\\pmb{y}_{i})-\\pmb{x}_{i})^{2}}\\end{array}$ and any $\\delta\\in(0,1)$ , with probability at least $1-\\delta/2$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\|\\widehat{f}-f_{\\star}\\|_{n}^{2}\\le4B^{2}\\log(2|\\mathcal{F}|/\\delta)+4B\\sum_{i=1}^{n}|\\pmb{b}_{i}|,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\begin{array}{r}{\\|\\widehat{f}-f_{\\star}\\|_{n}^{2}:=\\sum_{i=1}^{n}(\\widehat{f}(y_{i})-f^{\\star}(y_{i}))^{2}}\\end{array}$ . ", "page_idx": 28}, {"type": "text", "text": "Proof of Lemma C.5. Fix $\\delta\\in(0,1)$ and let $\\begin{array}{r}{\\widehat{L}_{n}(f):=\\sum_{i=1}^{n}(f(\\pmb{y}_{i})-\\pmb{x}_{i})^{2}}\\end{array}$ , for $f\\in\\mathcal F$ , and note that since $\\widehat{f}\\in\\arg\\operatorname*{min}_{f\\in\\mathcal{F}}\\widehat{L}_{n}(f)$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n0\\geq\\widehat{L}_{n}\\big(\\widehat{f}\\big)-\\widehat{L}_{n}\\big(f_{\\star}\\big)=\\nabla\\widehat{L}_{n}\\big(f_{\\star}\\big)\\big[\\widehat{f}-f_{\\star}\\big]+\\big\\|\\widehat{f}-f_{\\star}\\big\\|_{n}^{2},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\nabla$ denotes directional derivative. Rearranging, we get that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\widehat{f}-f_{\\star}\\|_{n}^{2}\\leq-2\\nabla\\widehat{L}_{n}\\big(f_{\\star}\\big)\\big[\\widehat{f}-f_{\\star}\\big]-\\|\\widehat{f}-f_{\\star}\\|_{n}^{2},}\\\\ &{\\qquad\\qquad=4\\displaystyle\\sum_{i=1}^{n}(x_{i}-f_{\\star}(y_{i}))\\big(\\widehat{f}(y_{i})-f_{\\star}(y_{i})\\big)-\\big\\|\\widehat{f}-f_{\\star}\\big\\|_{n}^{2},}\\\\ &{\\qquad\\qquad\\leq4\\displaystyle\\sum_{i=1}^{n}(\\varepsilon_{i}+b_{i})\\big(\\widehat{f}(y_{i})-f_{\\star}(y_{i})\\big)-\\big\\|\\widehat{f}-f_{\\star}\\big\\|_{n}^{2},}\\\\ &{\\qquad\\qquad\\leq4\\displaystyle\\sum_{i=1}^{n}\\varepsilon_{i}\\cdot\\big(\\widehat{f}(y_{i})-f_{\\star}(y_{i})\\big)-\\big\\|\\widehat{f}-f_{\\star}\\big\\|_{n}^{2}+4\\displaystyle\\sum_{i=1}^{n}b_{i}\\cdot\\big(\\widehat{f}(y_{i})-f_{\\star}(y_{i})\\big)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Bounding Term $I.$ . To bound Term I, we apply Lemma C.2 with $\\pmb{w}_{i}=\\pmb{\\varepsilon}_{i}\\cdot\\left(\\widehat{f}(\\pmb{y}_{i})-f_{\\star}(\\pmb{y}_{i})\\right)$ , $R=B^{2}$ , $\\lambda=1/(8\\bar{B}^{2})$ , and $\\mathcal{H}_{i}=\\mathfrak{F}_{i+1}^{-}$ , and use ", "page_idx": 29}, {"type": "text", "text": "1. the union bound over $f\\in\\mathcal F$ ; and   \n2. the facts that $\\mathbb{E}[\\pmb{y}_{i}\\mid\\mathfrak{F}_{i}^{-}]=\\pmb{y}_{i}$ and $\\mathbb{E}[\\varepsilon_{i}\\mid\\mathfrak{F}_{i}^{-}]=0$ , ", "page_idx": 29}, {"type": "text", "text": "to get that with probability at least $1-\\delta/2$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n4\\sum_{i=1}^{n}\\pmb{\\varepsilon}_{i}\\cdot(\\widehat{f}(\\pmb{y}_{i})-f_{\\star}(\\pmb{y}_{i}))\\leq\\|\\widehat{f}-f_{\\star}\\|_{n}^{2}+4B^{2}\\log(2|\\mathcal{F}|/\\delta).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "By rearranging, we get that with probability at least $1-\\delta/2$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathrm{Term~I}\\leq4B^{2}\\log(2|\\mathcal{F}|/\\delta).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Bounding Term $I I.$ . We now bound the second term in (7). For this, note that since $\\|\\widehat{f}-f_{\\star}\\|_{\\infty}\\le B$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname{Term}\\operatorname{II}\\leq4B\\sum_{i=1}^{n}{\\big|}b_{i}{\\big|}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "This completes the proof. ", "page_idx": 29}, {"type": "text", "text": "C.3 Reinforcement Learning ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Lemma C.6 (Performance Difference Lemma (e.g., Kakade [33])). For any two policies \u03c0\u0302, $\\pi\\in\\Pi_{\\ S}$ and $t\\in[H]$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}^{\\pi}\\left[V_{t}^{\\pi}(\\pmb{x}_{t})-V_{t}^{\\widehat{\\pi}}(\\pmb{x}_{t})\\right]=\\mathbb{E}^{\\pi}\\left[\\sum_{h=t}^{H}Q_{h}^{\\widehat{\\pi}}(\\pmb{x}_{h},\\pmb{\\pi}_{h}(\\pmb{x}_{h}))-Q_{h}^{\\widehat{\\pi}}(\\pmb{x}_{h},\\widehat{\\pmb{\\pi}}_{h}(\\pmb{x}_{h}))\\right].\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "In particular, applying this for $t=1$ gives ", "page_idx": 29}, {"type": "equation", "text": "$$\nJ(\\pi)-J(\\widehat{\\pi})=\\mathbb{E}^{\\pi}\\left[\\sum_{h=1}^{H}Q_{t}^{\\widehat{\\pi}}\\big(\\pmb{x}_{h},\\pmb{\\pi}_{h}(\\pmb{x}_{h})\\big)-Q_{h}^{\\widehat{\\pi}}\\big(\\pmb{x}_{h},\\widehat{\\pmb{\\pi}}_{h}(\\pmb{x}_{h})\\big)\\right].\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Lemma C.7 (Potential lemma [63]). Fi $\\mathrm{~r~}h\\,\\mathrm{~\\in~}\\,[H]$ . Suppose we have a sequence of functions $g^{(1)},\\ldots,g^{(T)}\\in[0,B]$ and policies $\\pi^{(1)},\\bot\\ldots,\\pi^{(T)}$ such that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\forall t\\in[T],\\quad\\sum_{i<t}\\mathbb{E}^{\\pi^{(i)}}\\big[\\big(g^{(t)}\\big(\\pmb{x}_{h}\\big)\\big)^{2}\\big]\\leq\\beta^{2}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "for some $\\beta\\ge0$ . Then under Assumption 4.1, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{E}^{\\pi^{(t)}}\\big[g^{(t)}(\\pmb{x}_{h})\\big]\\leq2\\sqrt{\\beta^{2}C_{\\mathrm{push}}T\\log(2T)}+2B C_{\\mathrm{push}},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and consequently ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{t\\in[T]}\\mathbb{E}^{\\pi^{(t)}}[g^{(t)}(\\pmb{x}_{h})]\\leq2\\sqrt{\\frac{\\beta^{2}C_{\\mathrm{push}}\\log(2T)}{T}}+\\frac{2B C_{\\mathrm{push}}}{T}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof of Lemma C.8. See proof of [63, Theorem 1]. ", "page_idx": 30}, {"type": "text", "text": "The following result is a variant of the coverability-based potential argument given in Xie et al. [63]. Lemma C.8 (Pushforward coverability potential lemma). Fix $h\\in[H]$ . Suppose we have a sequence of functions $g^{(1)},\\ldots,g^{(T)}\\in[0,B]$ and state-action pairs $\\left(x^{(1)},a^{(1)}\\right),~~~,\\left(x^{(T)},a^{(T)}\\right)$ such that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\forall t\\in[T],\\quad\\sum_{i<t}\\mathbb{E}\\big[\\big(g^{(t)}\\big(x_{h}\\big)\\big)^{2}\\mid x_{h-1}=x^{(i)},\\mathbf{a}_{h-1}=a^{(i)}\\big]\\leq\\beta^{2}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "for some $\\beta\\ge0$ . Then under Assumption 4.1, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{E}\\big[g^{(t)}\\big(\\pmb{x}_{h}\\big)\\mid\\pmb{x}_{h-1}=\\pmb{x}^{(t)},\\pmb{a}_{h-1}=\\pmb{a}^{(t)}\\big]\\leq2\\sqrt{\\beta^{2}C_{\\mathrm{push}}T\\log(2T)}+2B C_{\\mathrm{push}},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and consequently ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{t\\in[T]}\\mathbb{E}\\big[g^{(t)}\\big(\\pmb{x}_{h}\\big)\\mid\\pmb{x}_{h-1}=\\pmb{x}^{(t)},\\pmb{a}_{h-1}=\\pmb{a}^{(t)}\\big]\\leq2\\sqrt{\\frac{\\beta^{2}C_{\\mathrm{push}}\\log(2T)}{T}}+\\frac{2B C_{\\mathrm{push}}}{T}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof of Lemma C.8. Define $d_{h}^{(t)}\\!\\left(x\\right):=\\mathbb{P}\\!\\left[\\pmb{x}_{h}=x\\mid\\pmb{x}_{h-1}=x^{(t)},\\pmb{a}_{h-1}=\\boldsymbol{a}^{(t)}\\right]$ , and let $\\begin{array}{r}{\\widetilde{d}_{h}^{\\scriptscriptstyle(t)}:=\\sum_{\\scriptscriptstyle i<t}d^{\\scriptscriptstyle(i)}}\\end{array}$ . Let ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\tau_{h}(x):=\\operatorname*{min}\\{t\\mid\\widetilde{d}_{h}^{(t)}(x)\\geq C_{\\mathsf{p u s h}}\\cdot\\mu_{h}(x)\\}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\big[g^{(t)}\\big(\\pmb{x}_{h}\\big)\\mid\\pmb{x}_{h-1}=x^{(t)},\\pmb{a}_{h-1}=a^{(t)}\\big]}\\\\ {\\displaystyle=\\sum_{t=1}^{T}\\sum_{x\\in\\mathcal{X}}d_{h}^{(t)}\\big(x\\big)g^{(t)}\\big(x\\big)}\\\\ {\\displaystyle\\leq\\sum_{t=1}^{T}\\sum_{x\\in\\mathcal{X}}d_{h}^{(t)}\\big(x\\big)g^{(t)}\\big(x\\big)\\mathbb{I}\\big\\{t\\geq\\tau_{h}(x)\\big\\}+B\\displaystyle\\sum_{t=1}^{T}\\sum_{x\\in\\mathcal{X}}d_{h}^{(t)}\\big(x\\big)\\mathbb{I}\\big\\{t<\\tau_{h}(x)\\big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "From the definition of pushforward coverability, we can bound ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\sum_{x\\in\\mathcal{X}}d_{h}^{(t)}(x)\\mathbb{I}\\{t<\\tau_{h}(x)\\}=\\sum_{x\\in\\mathcal{X}}\\widetilde{d}_{h}^{(\\tau_{h}(x))}(x)\\le2C_{\\mathrm{push}}\\sum_{x\\in\\mathcal{X}}\\mu_{h}(x)=2C_{\\mathrm{push}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "For the other term, we bound ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\sum_{x\\in\\mathcal{X}}d_{h}^{(t)}(x)g^{(t)}(x)\\mathbb{I}\\{t\\geq\\tau_{h}(x)\\}}\\\\ &{\\leq\\left(\\displaystyle\\sum_{t=1}^{T}\\sum_{x\\in\\mathcal{X}}\\frac{\\big(d_{h}^{(t)}(x)\\big)^{2}}{\\tilde{d}_{h}^{(t)}(x)}\\mathbb{I}\\{t\\geq\\tau_{h}(x)\\}\\right)^{1/2}\\cdot\\left(\\displaystyle\\sum_{t=1}^{T}\\sum_{x\\in\\mathcal{X}}\\widetilde{d}_{h}^{(t)}(x)\\big(g_{h}^{(t)}(x))^{2}\\right)^{1/2}}\\\\ &{=\\left(\\displaystyle\\sum_{t=1}^{T}\\sum_{x\\in\\mathcal{X}}\\frac{\\big(d_{h}^{(t)}(x)\\big)^{2}}{\\tilde{d}_{h}^{(t)}(x)}\\mathbb{I}\\{t\\geq\\tau_{h}(x)\\}\\right)^{1/2}\\cdot\\left(\\displaystyle\\sum_{t=1}^{T}\\sum_{i<t}\\mathbb{E}\\big[\\big(g^{(t)}(x_{h})\\big)^{2}\\mid x_{h-1}=x^{(t)},a_{h-1}=a^{(t)}\\big]\\right)^{1/2}}\\\\ &{\\leq\\left(\\displaystyle\\sum_{t=1}^{T}\\sum_{x\\in\\mathcal{X}}\\frac{\\big(d_{h}^{(t)}(x)\\big)^{2}}{\\tilde{d}_{h}^{(t)}(x)}\\mathbb{I}\\{t\\geq\\tau_{h}(x)\\}\\right)^{1/2}\\cdot\\sqrt{\\beta^{2}T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Finally, we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{t=1}^{T}\\sum_{x\\in\\mathcal{X}}\\frac{\\big(d_{h}^{(t)}(x)\\big)^{2}}{\\widetilde{d}_{h}^{(t)}(x)}\\mathbb{I}\\{t\\geq\\tau_{h}(x)\\}\\leq2\\sum_{t=1}^{T}\\sum_{x\\in\\mathcal{X}}\\frac{(d_{h}^{(t)}(x))^{2}}{\\widetilde{d}_{h}^{(t)}(x)+C_{\\mathrm{push}}\\mu_{h}(x)}}}\\\\ &{}&{\\quad\\le2C_{\\mathrm{push}}\\sum_{t=1}^{T}\\sum_{x\\in\\mathcal{X}}\\mu_{h}(x)\\frac{d_{h}^{(t)}(x)}{\\widetilde{d}_{h}^{(t)}(x)+C_{\\mathrm{push}}\\mu_{h}(x)}}\\\\ &{}&{\\quad=2C_{\\mathrm{push}}\\sum_{x\\in\\mathcal{X}}\\mu_{h}(x)\\sum_{t=1}^{T}\\frac{d_{h}^{(t)}(x)}{\\widetilde{d}_{h}^{(t)}(x)+C_{\\mathrm{push}}\\mu_{h}(x)}}\\\\ &{}&{\\quad=4C_{\\mathrm{push}}\\log(T+1),}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where the last line uses Lemma 4 of Xie et al. [63]. ", "page_idx": 31}, {"type": "text", "text": "Part II ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Proofs for SimGolf (Section 3) ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "As described in Section 3.1, the main difference between SimGolf and GOLF lies in the construction of the confidence sets. The most important new step in the proof of Theorem 3.1 is to show that the local simulator-based confidence set construction in Line 9 is valid in the sense that the property Eq. (1) holds with high probability. From here, the sample complexity bound follows by adapting the change-of-measure argument based on coverability from Xie et al. [63]. ", "page_idx": 32}, {"type": "text", "text": "To this end, this part of the appendix is organized as follows. We first state and prove technical lemmas concerning realizability (Lemma D.1) and the confidence set construction (Lemma D.2 and Lemma D.3) in Appendix D. Then, in Appendix E, we prove Theorem 3.1 as a consequence. ", "page_idx": 32}, {"type": "text", "text": "D Preliminary Lemmas for Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "For this section, we define ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\ell_{h}^{(t)}(g):=\\left(g_{h}\\big(x_{h}^{(t)},a_{h}^{(t)}\\big)-\\frac{1}{K}\\sum_{k=1}^{K}\\left(r_{h}^{(t,k)}+\\operatorname*{max}_{a\\in\\mathcal{A}}g_{h+1}\\big(x_{h+1}^{(t,k)},a\\big)\\right)\\right)^{2}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "and ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\bar{\\ell}_{h}^{(t)}\\big(g\\big):=\\mathbb{E}^{\\pi^{(t)}}\\Big[\\big(g_{h}\\big({\\pmb x}_{h},{\\pmb a}_{h}\\big)-\\mathcal T_{h}\\big[g_{h+1}\\big]\\big({\\pmb x}_{h},{\\pmb a}_{h}\\big)\\big)^{2}\\Big],\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $(\\mathbf{x}_{h}^{(t)},\\mathbf{a}_{h}^{(t)},\\mathbf{r}_{h}^{(t,k)},\\mathbf{x}_{h+1}^{(t,k)})$ xht,+k1 )are as in Algorithm 1. ", "page_idx": 32}, {"type": "text", "text": "Lemma D.1 ([21]). For the ExBMDP setting, under Assumption 3.3, the function class $\\mathcal{Q}_{h}\\;:=\\;$ $\\left\\{(x,a)\\mapsto g(\\phi(x),a):g\\in[0,H]^{S A},\\phi\\in\\Phi\\right\\}$ satisfies Assumption 3.1 and has $\\log\\!\\left|\\mathcal{Q}_{h}\\right|=\\log\\!\\left|\\Pi_{h}\\right|=$ ${\\widetilde{O}}(S A+\\log\\lvert\\Phi\\rvert)$ .9 ", "page_idx": 32}, {"type": "text", "text": "Lemma D.2. With probability at least $1-\\delta$ , for all $h\\in[H],\\,t\\in[N],$ , and $g\\in{\\mathcal{Q}}_{:}$ , ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\sum_{i\\leq t}\\ell_{h}^{(i)}(g)\\leq3\\sum_{i<t}\\mathbb{E}^{\\pi^{(i)}}\\Big[\\big(g_{h}({\\pmb x}_{h},{\\pmb a}_{h})-\\mathcal{T}_{h}[g_{h+1}]({\\pmb x}_{h},{\\pmb a}_{h})\\big)^{2}\\Big]+\\frac{8N}{K}+16\\log(2H N|Q|\\delta^{-1}),\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "and ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\sum_{i<t}\\mathbb{E}^{\\pi^{(i)}}\\Big[\\big(g_{h}({\\pmb x}_{h},{\\pmb a}_{h})-\\mathcal{T}_{h}[g_{h+1}]({\\pmb x}_{h},{\\pmb a}_{h})\\big)^{2}\\Big]\\leq4\\sum_{i<t}\\ell_{h}^{(i)}(g)+\\frac{8N}{K}+64\\log(2H N|Q|\\delta^{-1}).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof of Lemma D.2. Let t \u2208[N] and h \u2208[H] be fixed. Let us denote z(ht) = {(r(ht,k),x(ht,+k1) )}k\u2208[K]. Define a filtration ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathcal{H}^{(t)}=\\sigma\\big(\\tau^{(1)},z_{1}^{(1)},\\boldsymbol{\\cdot}\\boldsymbol{\\cdot},z_{H}^{(1)},\\boldsymbol{\\cdot}\\boldsymbol{\\cdot},\\tau^{(t)},z_{1}^{(t)},\\boldsymbol{\\cdot}\\boldsymbol{\\cdot},z_{H}^{(t)}\\big),\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $\\tau^{(i)}$ is the trajectory generated in the $i$ th iteration of Algorithm 1 (see Line 7). Fix $g\\,\\in\\,\\mathcal{Q}$ Observe that $\\ell_{h}^{(i)}(g)\\,\\overset{\\bullet}{\\in}\\,[0,\\dot{4}]$ , so Lemma C.3 ensures that with probability at least $1-\\delta$ , ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\sum_{i<t}\\ell_{h}^{(i)}(g)\\leq\\frac{3}{2}\\sum_{i<t}\\mathbb{E}\\big[\\ell_{h}^{(i)}(g)\\mid\\mathcal{H}^{(i-1)}\\big]+16\\log(2\\delta^{-1}),\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "and ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\sum_{i<t}\\mathbb{E}\\big[\\ell_{h}^{(i)}(g)\\;\\big|\\;\\mathcal{H}^{(i-1)}\\big]\\leq2\\sum_{i<t}\\ell_{h}^{(i)}(g)+32\\log(2\\delta^{-1}).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "By the AM-GM inequality, for all $i<t$ , we can bound ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\big[\\ell_{h}^{(i)}(g)\\,\\big|\\,\\mathcal{H}^{(i-1)}\\big]}\\\\ &{\\quad=\\mathbb{E}\\Bigg[\\Bigg(g_{h}\\big(x_{h}^{(i)},a_{h}^{(i)}\\big)-\\displaystyle\\frac{1}{K}\\sum_{k=1}^{K}\\bigg(r_{h}^{(i,k)}+\\operatorname*{max}_{a\\in\\mathcal{A}}g_{h+1}\\big(x_{h+1}^{(i,k)},a\\big)\\bigg)\\Bigg)^{2}\\,\\big|\\,\\mathcal{H}^{(i-1)}\\Bigg]}\\\\ &{\\quad\\le2\\,\\mathbb{E}\\bigg[\\big(g_{h}\\big(x_{h}^{(i)},a_{h}^{(i)}\\big)-7_{h}\\big[g_{h+1}\\big]\\big(x_{h}^{(i)},a_{h}^{(i)}\\big)\\big)^{2}\\,\\big|\\,\\mathcal{H}^{(i-1)}\\bigg]}\\\\ &{\\quad\\quad+\\,2\\,\\mathbb{E}\\Bigg[\\bigg(7_{h}\\big[g_{h+1}\\big]\\big(x_{h}^{(i)},a_{h}^{(i)}\\big)-\\displaystyle\\frac{1}{K}\\sum_{k=1}^{K}\\bigg(r_{h}^{(i,k)}+\\operatorname*{max}_{a\\in\\mathcal{A}}g_{h+1}\\big(x_{h+1}^{(i,k)},a\\big)\\bigg)\\Bigg)^{2}\\,\\big|\\,\\mathcal{H}^{(i-1)}\\Bigg].}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\big[\\ell_{h}^{(i)}(g)\\,\\big|\\,\\mathcal{H}^{(i-1)}\\big]}\\\\ &{\\geq\\displaystyle\\frac{1}{2}\\,\\mathbb{E}\\Big[\\big(g_{h}\\big(x_{h}^{(i)},a_{h}^{(i)}\\big)-\\mathcal{T}_{h}\\big[g_{h+1}\\big]\\big(x_{h}^{(i)},a_{h}^{(i)}\\big)\\big)^{2}\\,\\big|\\,\\mathcal{H}^{(i-1)}\\Big]}\\\\ &{\\quad-\\,\\mathbb{E}\\Bigg[\\bigg(\\mathcal{T}_{h}\\big[g_{h+1}\\big]\\big(x_{h}^{(i)},a_{h}^{(i)}\\big)-\\displaystyle\\frac{1}{K}\\sum_{k=1}^{K}\\bigg(r_{h}^{(i,k)}+\\operatorname*{max}_{a\\in A}g_{h+1}\\big(x_{h+1}^{(i,k)},a\\big)\\bigg)\\bigg)^{2}\\,\\big|\\,\\mathcal{H}^{(i-1)}\\Bigg].}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Big[\\big(g_{h}\\big(\\pmb{x}_{h}^{(i)},\\pmb{a}_{h}^{(i)}\\big)-\\mathcal{T}_{h}\\big[g_{h+1}\\big]\\big(\\pmb{x}_{h}^{(i)},\\pmb{a}_{h}^{(i)}\\big)\\Big)^{2}\\mid\\mathcal{H}^{(i-1)}\\Big]=\\mathbb{E}^{\\pi^{(i)}}\\Big[\\big(g_{h}\\big(\\pmb{x}_{h},\\pmb{a}_{h}\\big)-\\mathcal{T}_{h}\\big[g_{h+1}\\big]\\big(\\pmb{x}_{h},\\pmb{a}_{h}\\big)\\big)^{2}\\Big]\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\Bigg[\\Bigg(\\mathcal{T}_{h}\\big[g_{h+1}\\big]\\big(x_{h}^{(i)},a_{h}^{(i)}\\big)-\\displaystyle\\frac{1}{K}\\sum_{k=1}^{K}\\bigg(r_{h}^{(i,k)}+\\operatorname*{max}_{a\\in\\mathcal{A}}g_{h+1}\\big(x_{h+1}^{(i,k)},a\\big)\\bigg)\\Bigg)^{2}\\mid\\mathcal{H}^{(i-1)}\\Bigg]}\\\\ &{=\\mathbb{E}\\Bigg[\\mathbb{E}\\Bigg[\\Bigg(\\mathcal{T}_{h}\\big[g_{h+1}\\big]\\big(x_{h},a_{h}\\big)-\\displaystyle\\frac{1}{K}\\sum_{k=1}^{K}\\bigg(r_{h}^{(i,k)}+\\operatorname*{max}_{a\\in\\mathcal{A}}g_{h+1}\\big(x_{h+1}^{(i,k)},a\\big)\\bigg)\\Bigg)^{2}\\mid x_{h}=x_{h}^{(i)},a_{h}=a_{h}^{(i)}\\Bigg]\\mid\\mathcal{H}^{(i-1)}\\Bigg].}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Since ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{E}\\bigg[r_{h}^{(i,k)}+\\operatorname*{max}_{a\\in\\mathcal{A}}g_{h+1}\\big(x_{h+1}^{(i,k)},a\\big)\\;\\big|\\;x_{h}=x_{h}^{(i)},a_{h}=a_{h}^{(i)}\\bigg]=\\mathcal{T}_{h}\\big[g_{h+1}\\big]\\big(x_{h}^{(i)},a_{h}^{(i)}\\big)\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and $\\left\\{\\big(\\pmb{r}_{h}^{(i,k)},\\pmb{x}_{h+1}^{(i,k)}\\big)\\right\\}_{k\\in[K]}$ are i.i.d. conditioned on $(\\pmb{x}_{h},\\pmb{\\mathbf{a}}_{h})=\\big(\\pmb{x}_{h}^{(i)},\\pmb{\\mathbf{a}}_{h}^{(i)}\\big)$ , we have, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\Bigg[\\Bigg(\\mathcal{T}_{h}[g_{h+1}]({x}_{h},a_{h})-\\cfrac{1}{K}\\displaystyle\\sum_{k=1}^{K}\\Big({r}_{h}^{(i,k)}+\\operatorname*{max}_{a\\in\\mathcal{A}}g_{h+1}\\big({x}_{h+1}^{(i,k)},a\\big)\\Big)\\Bigg)^{2}\\mid{x}_{h}={x}_{h}^{(i)},a_{h}={a}_{h}^{(i)}\\Bigg]}\\\\ &{=\\cfrac{1}{K}\\,\\mathbb{E}\\Bigg[\\Big(\\mathcal{T}_{h}[g_{h+1}]\\big({x}_{h},a_{h}\\big)-\\Big({r}_{h}^{(i,k)}+\\operatorname*{max}_{a\\in\\mathcal{A}}g_{h+1}\\big({x}_{h}^{(i,k)},a\\big)\\Big)\\Big)^{2}\\mid{x}_{h}={x}_{h}^{(i)},a_{h}={a}_{h}^{(i)}\\Bigg]}\\\\ &{\\leq\\cfrac{4}{K},}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "so that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Bigg[\\Bigg(\\mathcal{T}_{h}\\big[g_{h+1}\\big]\\big(x_{h}^{(i)},a_{h}^{(i)}\\big)-\\frac{1}{K}\\sum_{k=1}^{K}\\Big(r_{h}^{(i,k)}+\\operatorname*{max}_{a\\in\\mathcal{A}}g_{h+1}\\big(x_{h+1}^{(i,k)},a\\big)\\Big)\\Bigg)^{2}\\mid\\mathcal{H}^{(i-1)}\\Bigg]\\leq\\frac{4}{K}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Combining these bounds with (8) and rearranging thus gives ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\sum_{i<t}\\ell_{h}^{(i)}(g)\\leq3\\sum_{i<t}\\mathbb{E}^{\\pi^{(i)}}\\Big[\\big(g_{h}({\\pmb x}_{h},{\\pmb a}_{h})-7_{h}[g_{h+1}]({\\pmb x}_{h},{\\pmb a}_{h})\\big)^{2}\\Big]+\\frac{8N}{K}+16\\log(2\\delta^{-1}),\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\sum_{i<t}\\mathbb{E}^{\\pi^{(i)}}\\Big[\\big(g_{h}\\big(x_{h},a_{h}\\big)-\\mathcal{T}_{h}\\big[g_{h+1}\\big]\\big(x_{h},a_{h}\\big)\\big)^{2}\\Big]\\le4\\sum_{i<t}\\ell_{h}^{(i)}(g)+\\frac{8N}{K}+64\\log(2\\delta^{-1}).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Taking a union bound yields the result. ", "page_idx": 33}, {"type": "text", "text": "Lemma D.3. Define $\\beta_{s t a t}=16\\log(2H N|\\mathcal{Q}|\\delta^{-1})$ . Suppose we set $\\begin{array}{r}{K\\ge\\frac{8N}{\\beta_{\\mathrm{stat}}}}\\end{array}$ and $\\beta\\ge2\\beta_{\\mathrm{stat}}$ . Then with probability at least $1-\\delta$ , for all $t\\in[N]$ and $h\\in\\mathcal H$ : ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\sum_{i<t}\\mathbb{E}^{\\pi^{(i)}}\\Big[\\big(g_{h}\\big(\\pmb{x}_{h},\\pmb{a}_{h}\\big)-\\mathcal{T}_{h}\\big[g_{h+1}\\big]\\big(\\pmb{x}_{h},\\pmb{a}_{h}\\big)\\big)^{2}\\Big]\\le9\\beta.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Proof of Lemma D.3. Condition on the event in Lemma D.2. For any fixed $t\\in[N]$ and $h\\in[H]$ , we have that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\sum_{i<t}\\ell_{h}^{(i)}(Q^{\\star})\\le3\\sum_{i<t}\\mathbb{E}^{\\pi^{(i)}}\\Big[\\big(Q_{h}^{\\star}(x_{h},a_{h})-7_{h}[Q_{h+1}^{\\star}](x_{h},a_{h})\\big)^{2}\\Big]+\\frac{8N}{K}+16\\log(2H N|Q|\\delta^{-1})}\\\\ {\\displaystyle\\le\\frac{8N}{K}+16\\log(2H N|Q|\\delta^{-1})\\le2\\beta_{\\mathrm{stat}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where the first inequality uses that $Q_{h}^{\\star}=\\mathcal{T}_{h}[Q_{h+1}^{\\star}]$ and the second inequality uses our choice for $K$ .   \nIt follows that $Q^{\\star}\\,\\bar{\\in}\\,\\mathcal{Q}^{(t)}$ as long as $\\beta\\geq2\\beta_{\\mathrm{stat}}$ . ", "page_idx": 34}, {"type": "text", "text": "To prove the second claim, we note that for all $g\\in\\mathcal{Q}^{(t)}$ , by construction, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i<t}\\mathbb{E}^{\\pi^{(i)}}\\Big[\\big(g_{h}({\\pmb x}_{h})-\\mathcal{T}_{h}[g_{h+1}]({\\pmb x}_{h},{\\pmb a}_{h})\\big)^{2}\\Big]\\leq4\\sum_{i<t}\\ell_{h}^{(i)}(g)+\\frac{8N}{K}+64\\log(2H N|\\mathcal{Q}|\\delta^{-1})}\\\\ &{\\quad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq4\\displaystyle\\sum_{i<t}\\ell_{h}^{(i)}(g)+5\\beta_{\\mathsf{s t a t}}\\leq9\\beta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "E Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Proof of Theorem 3.1. From Lemma D.3, the parameter setting in the theorem statement ensures that with probability at least $1-\\delta$ , for all $t\\in[2\\therefore N],Q^{\\star}\\in\\mathcal{Q}^{(t)}$ , and all $g\\in\\mathcal{Q}^{(t)}$ satisfy ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\sum_{i<t}\\mathbb{E}^{\\pi^{(i)}}\\Big[\\big(g_{h}\\big({\\pmb x}_{h}\\big)-\\mathcal T_{h}\\big[g_{h+1}\\big]\\big({\\pmb x}_{h},{\\pmb a}_{h}\\big)\\big)^{2}\\Big]\\le9\\beta.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "for all $h$ . Let us condition on this event going forward. First, note that since $Q^{\\star}\\,\\in\\,\\mathcal{Q}^{(t)}$ for all $t\\in[2\\ldots N]$ , we have that ", "page_idx": 34}, {"type": "equation", "text": "$$\nJ(\\pi^{\\star})\\leq\\mathbb{E}\\left[\\operatorname*{max}_{a\\in\\mathcal{A}}Q_{1}^{\\star}(x_{1},a)\\right]\\leq\\operatorname*{sup}_{g\\in\\mathcal{Q}^{(t)}}\\mathbb{E}\\left[\\operatorname*{max}_{a\\in\\mathcal{A}}g_{1}\\bigl(x_{1},a\\bigr)\\right].\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "On the other hand, we have $\\begin{array}{r}{g^{(t)}\\in\\arg\\operatorname*{max}_{g\\in\\mathcal{Q}^{(t)}}\\sum_{s<t}\\operatorname*{max}_{a\\in\\mathcal{A}}g_{1}\\big(\\pmb{x}_{1}^{(s)},a\\big)}\\end{array}$ , and so since $x_{1}^{(1)},x_{1}^{(2)},\\ldots$ are i.i.d. and any $g\\in\\mathcal{Q}^{(t)}$ take values in $[0,H]$ , we have that by Hoeffding\u2019s inequality, there is an event $\\mathcal{E}$ of probability at least $1-\\delta$ under which ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathrm{\\mathcal{I}}t\\in\\left[2\\ldots N\\right],\\forall g\\in\\mathcal{Q},\\quad\\left|\\mathbb{E}\\left[\\operatorname*{max}_{a\\in A}g_{1}(x_{1},a)\\right]-\\frac{1}{t-1}\\sum_{s<t}\\operatorname*{max}_{a\\in A}g_{1}(x_{1}^{(s)},a)\\right|\\leq\\sqrt{(t-1)^{-1}\\log(2N|\\mathcal{Q}|/\\delta)}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "This implies that under $\\mathcal{E}$ , we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\prime t\\in[2\\ldots N],\\quad\\underset{g\\in\\mathbb{Q}^{(t)}}{\\operatorname*{sup}}\\mathbb{E}\\left[\\underset{a\\in\\mathcal{A}}{\\operatorname*{max}}g_{1}(x_{1},a)\\right]\\le\\underset{g\\in\\mathbb{Q}^{(t)}}{\\operatorname*{sup}}\\frac{1}{t-1}\\sum_{s<t}\\underset{a\\in\\mathcal{A}}{\\operatorname*{max}}g_{1}(x_{1}^{(s)},a)+\\sqrt{(t-1)^{-1}\\log(2N|\\mathcal{Q}|/\\delta|)}}\\\\ &{\\qquad=\\cfrac{1}{t-1}\\underset{s<t}{\\sum\\operatorname*{max}}g_{1}^{(t)}(x_{1}^{(s)},a)+\\sqrt{(t-1)^{-1}\\log(2N|\\mathcal{Q}|/\\delta|)},}\\\\ &{\\qquad\\qquad\\le\\mathbb{E}\\left[\\underset{a\\in\\mathcal{A}}{\\operatorname*{max}}g_{1}^{(t)}(x_{1},a)\\right]+2\\sqrt{(t-1)^{-1}\\log(2N|\\mathcal{Q}|/\\delta|)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where in the last inequality we have used (11) with $f=g^{(t)}$ . Thus, summing (12) for $t=2,\\ldots N$ and using (10) gives that under $\\mathcal{E}$ : ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\sum_{t=2}^{N}J\\big(\\pi^{\\star}\\big)\\leq\\sum_{t=2}^{N}\\mathbb{E}\\big[g_{1}^{(t)}\\big(\\pmb{x}_{1},\\pmb{a}_{1}\\big)\\big]+4\\sqrt{N\\log\\big(2N|\\mathcal{Q}|/\\delta\\big)},\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "and so since $J(\\pi^{\\star})\\leq H$ , ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{N}J(\\pi^{\\star})\\leq\\sum_{t=1}^{N}\\mathbb{E}\\bigl[g_{1}^{(t)}(x_{1},a_{1})\\bigr]+4\\sqrt{N\\log\\bigl(2N|\\mathcal{Q}|/\\delta\\bigr)}+H.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "On the other hand, using that $g_{H+1}^{(t)}\\equiv0$ , we get ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{N}\\left(\\mathbb{E}\\left[g_{1}^{(t)}\\big(\\mathbf{x}_{1},a_{1}\\big)\\right]-J\\big(\\pi^{(t)}\\big)\\right)}\\\\ &{\\le\\displaystyle\\sum_{t=1}^{N}\\sum_{h=1}^{H}\\mathbb{E}^{\\pi^{(t)}}\\left[g_{h}^{(t)}\\big(\\mathbf{x}_{h},a_{h}\\big)-r_{h}-\\operatorname*{max}_{a\\in\\mathcal{A}}g_{h+1}^{(t)}\\big(x_{h+1},a\\big)\\right],}\\\\ &{=\\displaystyle\\sum_{t=1}^{N}\\sum_{h=1}^{H}\\mathbb{E}^{\\pi^{(t)}}\\left[g_{h}^{(t)}\\big(\\mathbf{x}_{h},a_{h}\\big)-\\mathbb{E}\\left[r_{h}+\\operatorname*{max}_{a\\in\\mathcal{A}}g_{h+1}^{(t)}\\big(x_{h+1},a\\big)\\mid x_{h},a_{h}\\right]\\right],\\quad\\mathrm{(law~of~total~expectation~}}\\\\ &{=\\displaystyle\\sum_{t=1}^{N}\\sum_{h=1}^{H}\\mathbb{E}^{\\pi^{(t)}}\\left[g_{h}^{(t)}\\big(\\mathbf{x}_{h},a_{h}\\big)-7_{h}\\big[g_{h+1}^{(t)}\\big]\\big(x_{h},a_{h}\\big)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "and so, by the potential lemma (Lemma C.7) and (9), we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\leq6H\\sqrt{C_{\\mathsf{c o v}}\\beta N\\log(2N)}+2H^{2}C_{\\mathsf{c o v}}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Combining this with (13), we obtain that with probability at least $1-2\\delta$ , ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{N}(J(\\pi^{\\star})-J(\\pi^{(t)}))\\leq6H\\sqrt{C_{\\mathrm{cov}}\\beta N\\log(2N)}+4\\sqrt{N\\log(2N|\\mathcal{Q}|/\\delta)}+3H^{2}C_{\\mathrm{cov}}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "It follows that if $N=\\widetilde{O}(H^{2}C_{\\mathsf{c o v}}\\beta/\\varepsilon^{2})$ , then the policy ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\widehat{\\pi}\\in\\mathsf{u n i f}\\big(\\pi^{(1)},\\ldots,\\pi^{(N)}\\big)\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "returned by SimGolf satisfies, with probability at least $1-\\delta$ : ", "page_idx": 35}, {"type": "equation", "text": "$$\nJ(\\pi^{\\star})-\\mathbb{E}[J(\\widehat{\\pi})]\\leq\\varepsilon.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Sample complexity. We now bound the number of episodes. Note that that within an iteration $t$ of SimGolf, the local simulator is called $K H$ times to update the confidence set, where $K\\,\\leq$ $N/\\log(2H N|\\mathcal{Q}|/\\delta)$ . Consequently, the total sample complexity is bounded by ", "page_idx": 35}, {"type": "equation", "text": "$$\nH N K\\leq\\widetilde{O}(H^{5}C_{\\mathsf{c o v}}^{2}\\log(|\\mathcal{Q}|/\\delta)/\\varepsilon^{4}).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Part III ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Proofs for RVFS (Section 4) ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "F Full Version of RVFS ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Algorithm 5 displays the full version of RVFS. Algorithm 6 contains an \u201couter-level\u201d wrapper for RVFS, RVFS.bc, which invokes RVFS and extracts an executable policy with imitation learning, and Algorithm 7 contains the subroutine used within Algorithm 5 to approximate Bellman backups for value functions using local simulator access. Additionally, we display the variant of RVFS for Exogenous Block MDPs, described in Appendix B, in Algorithms 8 and 9. Before diving into the proof, we first describe how the full version of the algorithm differs from the informal version presented in the main body in greater detail. ", "page_idx": 36}, {"type": "text", "text": "Differences between full version (Algorithm 5) and informal version (Algorithm 2) of RVFS. The main difference between Algorithm 2 and its full version in Algorithm 5 is that in the former we simply assume access to quantities involving conditional expectations such as: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The bellman backups $\\mathcal P_{h}[\\widehat{V}_{h+1}]$ , which are required to evaluate the actions of RVFS\u2019s policies (see (2)), and to perform the tests in Line 8; and   \n\u2022 The value functions $\\begin{array}{r}{\\mathbb{E}^{\\widehat{\\pi}_{h+1:H}}\\left[\\sum_{\\ell=h}^{H}r_{\\tau}\\;\\middle|\\;\\mathbf{x}_{h}=x,\\mathbf{a}_{h}=a\\right]}\\end{array}$ in Line 15, which are needed in the regression problem in Line 16. ", "page_idx": 36}, {"type": "text", "text": "These quantities are not available to the algorithm directly, but they can be estimated using the local simulator. This is reflected in the full version of RVFS in Algorithm 5. ", "page_idx": 36}, {"type": "text", "text": "Extracting policies from value functions. Let us briefly comment in more detail on how Algorithm 5 extracts the policy $\\pi^{(t)}$ from the optimistic value function $f^{(t)}\\in\\mathcal{V}$ at iteration $t$ . From the Bellman equation, the ideal choice would be to set $\\begin{array}{r}{\\pi_{h}^{(t)}\\bigl(x\\bigr)=\\arg\\operatorname*{max}_{a\\in\\cal A}\\mathcal{P}_{h}\\Bigl[f_{h+1}^{(t)}\\Bigr](x,a)}\\end{array}$ , but this requires knowledge of the transition distribution. Instead, given parameters $\\varepsilon,\\delta\\in(0,1)$ , SimGolf invokes Algorithm 7 via $\\pi_{h}^{(t)}\\!\\left(x\\right)\\,\\in\\,\\arg\\operatorname*{max}_{a\\in{\\cal A}}\\widehat{\\mathcal{P}}_{h,\\varepsilon,\\delta}\\!\\left[f_{h+1}^{(t)}\\right]\\!\\left(x,a\\right)$ . The operator $\\widehat{\\mathcal{P}}_{h,\\varepsilon,\\delta}[f]$ (Algorithm 7), when given input $(x,a)\\;\\in\\;\\mathcal{X}\\times\\mathcal{A}$ aPnd $f_{h+1}\\,:\\,\\mathcal{X}\\,\\rightarrow\\,\\mathbb{R}$ , uses the localP simulator to generate $N_{\\mathsf{s i m}}\\ge1$ next states $\\pmb{x}_{h+1}^{(1)},\\ldots,\\pmb{x}_{h+1}^{(N_{\\mathrm{sim}})}$ x(hN+s1im) i.i\u223c.d.Th(\u22c5\u2223x,a) to estimate the bellman back-up $\\mathcal{P}_{h}[f_{h+1}]$ via $\\begin{array}{r}{\\frac{1}{N_{\\mathrm{sim}}}\\sum_{i=1}^{N_{\\mathrm{sim}}}\\bigl(r_{h}^{(i)}+f_{h+1}\\bigl({\\pmb x}_{h+1}^{(i)}\\bigr)\\bigr)}\\end{array}$ , where $\\pmb{r}_{h}^{(1)},\\dots,\\pmb{r}_{h}^{(N_{\\mathrm{sim}})}$ r(hNsim)i.i\u223c.d.Rh(x,a). The number of samples $N_{\\mathrm{sim}}$ in Algorithm 7 is set as a function of $(\\varepsilon,\\delta)$ such that with probability at least $1-\\delta$ , $\\begin{array}{r}{|\\widehat{\\mathcal{P}}_{h,\\varepsilon,\\delta}[f_{h+1}](x,a)-\\mathcal{P}_{h}[f_{h+1}](x,a)|\\leq\\varepsilon.}\\end{array}$ . ", "page_idx": 36}, {"type": "text", "text": "Invoking the algorithm. The base invocation of RVFS takes the form ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\widehat{\\gamma}_{1:H}\\gets\\mathsf{R V F S}_{0}(\\widehat{V}_{1:H}=\\mathsf{a r b i t r a r y},\\widehat{\\mathcal{V}}_{1:H}=\\{\\mathcal{V}\\}_{h=1}^{H},\\mathcal{C}_{0:H}=\\{\\mathcal{D}\\}_{h=0}^{H},\\mathcal{B}_{0:H}=\\{\\mathcal{D}\\}_{h=0}^{H},t_{1:H}=\\{1\\}_{h=1}^{H};\\cdots\\}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Whenever this call returns, the greedy policy induced by $\\widehat{V}_{1:H}$ is guaranteed to be near-optimal. Naively, the policy induced by $\\overline{{V_{1:H}}}$ is non-executable, and must be computed by invoking the local simulator through Line 14. To provide an end-to-end guarantee to learn an executable policy, the outer-level algorithm, RVFS.bc (Algorithm 6, invokes $\\mathsf{R V F S}_{\\mathrm{0}}$ , then extracts an executable policy from $\\widehat{V}_{1:H}$ using imitation learning. ", "page_idx": 36}, {"type": "text", "text": "Subsequent recursive calls take the form ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(\\widehat{V}_{h:H},\\widehat{V}_{h:H},\\mathcal{C}_{h:H},\\mathcal{B}_{h:H},t_{h:H})\\gets\\mathsf{R V F S}_{h}\\big(\\widehat{V}_{h+1:H},\\widehat{\\mathcal{V}}_{h+1:H},\\mathcal{C}_{h:H},\\mathcal{B}_{h:H},t_{h:H};\\mathcal{V},\\varepsilon,\\delta\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "For such a call, the arguments above are: ", "page_idx": 36}, {"type": "text", "text": "\u2022 $\\widehat{V}_{h+1:H}$ : Value function estimates for subsequent layers.   \n\u2022 $\\widehat{\\mathcal{V}}_{h+1:H}$ : Value function confidence sets $\\widehat{\\nu}_{h+1:H}\\,\\subset\\,\\mathcal{V}_{h+1:H}$ , which are used in the test on   \nLine 14 to quantify uncertainty on new state-action pairs and decide whether to expand the core-sets.   \n\u2022 $\\mathcal{C}_{h:H}$ : Core-sets for current and subsequent layers. ", "page_idx": 36}, {"type": "text", "text": "\u2022 $\\boldsymbol{B}_{h:H}$ : Buffers of tuples $(x_{h-1},a_{h-1},\\widehat{V}_{h},\\widehat{\\mathcal{V}}_{h},t_{h})$ , which record relevant features of the algorithm\u2019s state whenever the test on Line 14 fails and a recursive call is performed. ", "page_idx": 37}, {"type": "text", "text": "\u2022 $t_{h:H}$ : Counters that track the number of times Algorithm 7 is called in the test on Line 14, which facilitate tuning of confidence parameters. ", "page_idx": 37}, {"type": "text", "text": "Importantly, the confidence sets $\\widehat{\\mathcal{V}}_{h+1:H}$ do not need to be explicitly maintained, and can be invoked implicitly whenever a regression oracle for the value function class is available (cf. discussion in Section 4). Likewise, the buffers $\\boldsymbol{B}_{h:H}$ are only used in our analysis, and do not need to be explicitly maintained. ", "page_idx": 37}, {"type": "text", "text": "1: parameters: Value function class $\\nu$ , suboptimality $\\varepsilon\\in(0,1)$ , confidence $\\delta\\in(0,1)$ .   \n2: input: \u2022 Level $h\\in\\{0,\\ldots,H\\}$ . \u2022 Value function estimates $\\widehat{V}_{h+1:H}$ , confidence sets $\\widehat{\\mathcal{V}}_{h+1:H}$ , state-action collections $\\mathcal{C}_{h:H}$ , buffers $\\boldsymbol{B}_{h:H}$ , and counters $t_{h:H}$ . /\\* Initialize parameters. $\\star/$   \n3: Set $M\\gets\\lceil8\\varepsilon^{-1}C_{\\mathsf{p u s h}}H\\rceil$ .   \n4: Set $N_{\\mathrm{test}}\\leftarrow2^{8}M^{2}H\\varepsilon^{-1}\\log(8M^{6}H^{8}\\varepsilon^{-2}\\delta^{-1}).$ , $N_{\\mathrm{reg}}\\leftarrow2^{8}M^{2}\\varepsilon^{-1}\\log(8|\\mathcal{V}|^{2}H M^{2}\\delta^{-1}).$ ,   \n5: Set $N_{\\mathrm{est}}(k)\\gets2N_{\\mathrm{reg}}^{2}\\log(8A N_{\\mathrm{reg}}H k^{3}/\\delta)$ and $\\delta^{\\prime}\\leftarrow\\delta/(8M^{7}N_{\\mathrm{test}}^{2}H^{8}|\\mathcal{V}|)$ .   \n6: Set \u03b5r2eg \u21909MH2 log(N8M 2H\u2223V\u22232/\u03b4)+ 34MH3 log(N8M 6Nt2estH8/\u03b4).   \n7: Set $\\beta(t)\\gets\\sqrt{2\\log_{1/\\delta^{\\prime}}(8A M|\\mathcal{V}|t^{2}/\\delta)}$ . $^{\\prime\\star}$ Test the fit for the estimated value functions $\\widehat{V}_{h+1:H}$ at future layers. \\*/   \n8: for $(x_{h-1},a_{h-1})\\in{\\mathcal{C}}_{h}$ do   \n9: for layer $\\ell=H,\\dots,h+1$ do   \n10: for $n=1,\\ldots,N_{\\mathrm{test}}$ do   \n11: Draw $\\pmb{x}_{h}\\sim T_{h-1}(\\cdot\\mid x_{h-1},a_{h-1})$ , then draw $x_{\\ell-1}$ by rolling out with $\\widehat{\\pi}_{h:H}$ , where10 $\\forall\\,\\tau\\in[H],\\quad\\widehat{\\pi}_{\\tau}(\\cdot)\\in\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}\\widehat{\\mathcal{P}}_{\\tau,\\varepsilon,\\delta^{\\prime}}[\\widehat{V}_{\\tau+1}](\\cdot,a),\\quad\\mathrm{with}\\quad\\widehat{V}_{H+1}\\equiv0.$ (14)   \n12: for $a_{\\ell-1}\\in A$ do $^{\\prime\\star}$ Number of times $\\widehat{\\mathcal{P}}_{\\ell-1,\\varepsilon,\\delta^{\\prime}}$ (Algorithm 7) is called in the test on Line 14. \\*/   \n13: Update $t_{\\ell}\\gets t_{\\ell}+1$ . $^{\\prime\\star}$ Test fit; if test fails, re-fit value functions $\\widehat{V}_{h+1:\\ell}$ up to layer \u2113. \\*/   \n14: $\\begin{array}{r l}&{\\mathbf{if}\\operatorname*{sup}_{f\\in\\widehat{\\mathcal{V}}_{\\ell}}|(\\widehat{\\mathcal{P}}_{\\ell-1,\\ell,\\delta^{\\prime}}[\\widehat{V}_{\\ell}]-\\widehat{\\mathcal{P}}_{\\ell-1,\\epsilon,\\delta^{\\prime}}[f_{\\ell}])(x_{\\ell-1},a_{\\ell-1})|>\\varepsilon+\\varepsilon\\cdot\\beta(t_{\\ell})\\,\\mathbf{then}}\\\\ &{\\qquad\\mathcal{C}_{\\ell}\\leftarrow\\mathcal{C}_{\\ell}\\cup\\{(x_{\\ell-1},a_{\\ell-1})\\}\\mathrm{~and~}B_{\\ell}\\leftarrow B_{\\ell}\\cup\\{(x_{\\ell-1},a_{\\ell-1},\\widehat{V}_{\\ell},\\widehat{\\mathcal{V}}_{\\ell},t_{\\ell})\\}.}\\\\ &{\\qquad\\mathbf{for}\\ \\tau=\\ell,\\ldots,h+1\\,\\mathbf{do}}\\\\ &{\\qquad\\qquad(\\widehat{V}_{\\tau:H},\\widehat{\\mathcal{V}}_{\\tau:H},\\mathcal{C}_{\\tau:H},\\mathcal{B}_{\\tau:H},t_{\\tau:H})\\leftarrow\\mathtt{R V F S}_{\\tau}(\\widehat{V}_{\\tau+1:H},\\widehat{\\mathcal{V}}_{\\tau+1:H},\\mathcal{C}_{\\tau:H},\\mathcal{B}_{\\tau:H},t_{\\tau:H};\\mathcal{V},\\varepsilon,\\delta).}\\end{array}$   \n15:   \n16:   \n17:   \n18: go to line 8.   \n19: if $h=0$ then return: $(\\widehat{V}_{1:H},\\cdot,\\cdot,\\cdot,\\cdot)$ . /\\* Re-fit $\\widehat{V}_{h}$ and build a new confidence set. $\\star/$   \n20: for $(x_{h-1},a_{h-1})\\in{\\mathcal{C}}_{h}$ do   \n21: Set $\\mathscr{D}_{h}\\big(x_{h-1},a_{h-1}\\big)\\gets\\emptyset$ .   \n22: for $i=1,\\ldots,N_{\\mathrm{reg}}\\;\\mathbf{do}$   \n23: Sample $\\mathbf{x}_{h}\\sim T_{h-1}(\\cdot\\mid x_{h-1},a_{h-1})$ .   \n24: Let $\\widehat{V}_{h}({\\pmb x}_{h})$ be a Monte-Carlo estimate for $\\mathbb{E}^{\\widehat{\\pi}_{h:H}}\\left[\\sum_{\\ell=h}^{H}\\pmb{r}_{\\ell}\\ |\\ \\pmb{x}_{h}\\right]$ computed by collecting $N_{\\mathrm{est}}(|\\mathcal{C}_{h}|)$ trajectories starting from and rolling out with $\\widehat{\\pi}_{h:H}$ .   \n25: Update $\\mathcal{D}_{h}(x_{h-1},a_{h-1})\\gets\\mathcal{D}_{h}(x_{h-1},a_{h-1})\\cup\\{(x_{h},\\widehat{V}_{h}(x_{h}))\\}$ .   \n26: Let V\u0302 $\\begin{array}{r}{\\dot{\\bar{\\gamma}_{h}}:=\\arg\\operatorname*{min}_{f\\in\\widehat{\\mathcal{V}}}\\sum_{(x_{h-1},a_{h-1})\\in\\mathcal{C}_{h}}\\sum_{(x_{h},v_{h})\\in\\mathcal{D}_{h}(x_{h-1},a_{h-1})}(f(x_{h})-v_{h})^{2},}\\end{array}$ 2.   \n27: Compute value function confidence set $\\widehat{\\mathcal{V}}_{h}:=\\left\\{f\\in\\mathcal{V}\\,\\Bigg|\\,\\sum_{(x_{h-1},a_{h-1})\\in\\mathcal{C}_{h}}\\frac{1}{N_{\\mathrm{reg}}}\\sum_{(x_{h},\\cdot)\\in\\mathcal{D}_{h}(x_{h-1},a_{h-1})}\\left(\\widehat{V}_{h}(x_{h})-f(x_{h})\\right)^{2}\\leq\\varepsilon_{\\mathrm{reg}}^{2}\\right\\}.$ (15) ", "page_idx": 38}, {"type": "text", "text": "28: return $(\\widehat{V}_{h:H},\\widehat{\\mathcal{V}}_{h:H},\\mathcal{C}_{h:H},\\mathcal{B}_{h:H},t_{h:H})$ . ", "page_idx": 38}, {"type": "text", "text": "1: input: Value function class $\\nu$ , policy class $\\Pi$ , suboptimality $\\varepsilon\\in(0,1)$ , confidence $\\delta\\in(0,1)$ . /\\* Set parameters for RVFS. $\\star/$   \n2: Set $\\varepsilon_{\\mathsf{R V F S}}\\leftarrow\\varepsilon H^{-1}/48.$ .   \n3: Set $\\widehat{V}_{1:H}\\gets$ arbitrary, $\\widehat{\\mathcal{V}}_{1:H}\\gets\\mathcal{V},\\mathcal{C}_{0:H}\\gets\\emptyset,\\mathcal{B}_{0:H}\\gets\\emptyset$ , and $t_{i}\\gets0$ , for all $i\\in[0\\dots H]$ .   \n4:   M \u2190\u23088\u03b5R\u2212V1FSC /S\\*e t eahnadv $N_{\\mathrm{test}}\\leftarrow2^{\\bar{8}}M^{2}H\\varepsilon_{\\mathtt{R V F S}}^{-1}\\log(80M^{6}H^{8}\\varepsilon_{\\mathtt{R V F S}}^{-2}\\delta^{-1})$ $\\star/$ .   \n5: $N_{\\mathsf{r e g}}\\gets2^{8}M^{2}\\varepsilon_{\\mathsf{R V F S}}^{-1}\\log(80|\\mathcal{V}|^{2}H M^{2}\\delta^{-1})$ and $\\begin{array}{r}{\\delta^{\\prime}=\\frac{\\delta}{40M^{7}N^{2}H^{8}|\\mathcal{V}|}}\\end{array}$ . $^{\\prime\\star}$ Get value functions from RVFS $\\star/$   \n6: $\\begin{array}{r}{(\\widehat{V}_{1:H},\\cdot,\\cdot,\\cdot)\\gets\\mathsf{R V F S}_{0}(\\widehat{V}_{1:H},\\widehat{V}_{1:H},\\mathcal{C}_{0:H},\\mathcal{B}_{0:H},t_{0:H};\\mathcal{V},N_{\\mathrm{reg}},N_{\\mathrm{test}},\\varepsilon_{\\mathsf{R V F S}},\\delta/10).}\\end{array}$ $^{\\prime\\star}$ Extract executable policy via BehaviorCloning algorithm for imitation learning. $\\star/$   \n7: Define $\\widehat{\\pi}_{h}^{\\mathsf{R V F S}}(\\cdot)\\in\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}\\overbrace{\\mathcal{P}_{h,\\varepsilon_{\\mathsf{R V F S}},\\delta^{\\prime}}[\\widehat{V}_{h+1}](\\cdot,a)}^{\\mathsf{\\widehat{R V F S}}}$ .   \n8: Compute $\\widehat{\\pi}_{1:H}\\gets$ BehaviorCloPning $(\\Pi,\\varepsilon,\\widehat{\\pi}_{1:H}^{\\mathsf{R V F S}},\\delta/2)$   \n9: return: $\\widehat{\\pi}_{1:H}$ . ", "page_idx": 39}, {"type": "text", "text": "Algorithm $7\\,\\widehat{\\mathcal{P}}_{h,\\varepsilon,\\delta}[f]$ : Estimate conditional expectation $\\mathbb{E}\\big[r_{h}+f\\big({\\pmb x}_{h+1}\\big)\\mid{\\pmb x}_{h}=\\cdot,{\\pmb a}_{h}=\\cdot\\big].$ ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "1: parameters: Layer $h$ , suboptimality $\\varepsilon\\in(0,1)$ , confidence $\\delta\\in(0,1)$ , target function $f$ .   \n2: input: $(x,a)\\in\\mathcal{X}\\times\\mathcal{A}$ .   \n3: Set $N_{\\mathrm{sim}}:=2\\log(1/\\delta)/\\varepsilon^{2}$ .   \n4: Set $\\mathcal{D}\\gets\\emptyset$   \n5: for $i=1,\\dots,N_{\\mathrm{sim}}\\,\\mathbf{d}$ o   \n6: Sample $\\pmb{r}_{h}\\sim R_{h}(x,a)$ and $\\mathbf{x}_{h+1}\\sim T_{h}(\\cdot\\mid x,a)$ . // Uses local simulator access.   \n7: Update $\\mathcal{D}\\leftarrow\\mathcal{D}\\cup\\left\\{\\left(\\boldsymbol{r}_{h},\\boldsymbol{x}_{h+1}\\right)\\right\\}$ .   \n8: return: $\\begin{array}{r}{N_{\\mathrm{sim}}^{-1}\\cdot\\sum_{(r,x)\\in\\mathcal{D}}(r+f(x))}\\end{array}$ . ", "page_idx": 39}, {"type": "text", "text": "Algorithm 8 RVFSehx o: Recursive Value Function Search for Exogenous Block MDPs   \n1: parameters: Value function class $\\nu$ , suboptimality $\\varepsilon\\in(0,1)$ , seeds $\\zeta_{1:H}\\in(0,1)$ , confidence $\\delta\\in(0,1)$ .   \n2: input: Level $h\\in[0\\ldots H]$ , value function estimates $\\widehat{V}_{h+1:H}$ , confidence sets $\\widehat{\\mathcal{V}}_{h+1:H}$ , stateaction collections $\\mathcal{C}_{h:H}$ , and buffers $\\boldsymbol{B}_{h:H}$ , and counters $t_{h:H}$ . $^{\\prime\\star}$ Initialize parameters. $\\star/$   \n3: Set $M\\gets\\lceil8\\varepsilon^{-2}C_{\\tt e x o}S A H\\rceil$ .   \n4: Set $N_{\\mathrm{test}}\\stackrel{.}{\\leftarrow}2^{8}M^{2}H\\varepsilon^{-2}\\operatorname*{lig}(8M^{6}H^{8}\\varepsilon^{-2}\\delta^{-1})$ , $N_{\\mathrm{reg}}\\leftarrow2^{8}M^{2}\\varepsilon^{-2}\\log(8|\\mathcal{V}|H M^{2}\\delta^{-1})$ .   \n6: Set \u03b5r2eg \u21909MH2 lorge(Ng8r egM 2H\u2223V\u2223/\u03b4)+ 34MH3 log(N8tMes t6N t2estH8/\u03b4). $\\delta^{\\prime}\\leftarrow\\delta/(4M^{7}N_{\\mathrm{test}}^{2}H^{8}|\\mathcal{V}|)$ .   \n7: Set $\\beta(t)\\leftarrow\\sqrt{\\log_{1/\\delta^{\\prime}}(8M A|\\mathcal{V}|t^{2}/\\delta)}$ . /\\* Test the fit for the estimated value functions $\\widehat{V}_{h+1:H}$ at future layers. \\*/   \n8: for $(x_{h-1},a_{h-1})\\in{\\mathcal{C}}_{h}$ do   \n9: for layer $\\ell=H,\\dots,h+1$ do   \n10: for $n=1,\\ldots,N_{\\mathrm{test}}$ do   \n11: Draw $\\pmb{x}_{h}\\sim T_{h-1}(\\cdot\\mid x_{h-1},a_{h-1})$ , then draw $x_{\\ell-1}$ by rolling out with $\\widehat{\\pi}_{h+1:H}$ , where $\\forall\\,\\tau\\in[H],\\,\\,\\,\\widehat{\\pi}_{\\tau}(\\cdot)\\in\\underset{a\\in\\mathcal{A}}{\\arg\\operatorname*{max}}\\big[\\widehat{\\mathcal{P}}_{\\tau,\\varepsilon^{2},\\delta^{\\prime}}\\big[\\widehat{V}_{\\tau+1}\\big](\\cdot,a)\\cdot\\varepsilon^{-1}+\\zeta_{\\tau}\\,\\big],\\quad\\mathrm{with}\\quad\\widehat{V}_{H+1}\\equiv0.$   \n12: for $a_{\\ell-1}\\in A$ do   \n13: Update $t_{\\ell}\\gets t_{\\ell}+1$ . $^{\\prime\\star}$ Test fit; if test fails, re-fit value functions $\\widehat{V}_{h+1:\\ell}$ up to layer \u2113. \\*/   \n14: $\\begin{array}{r l}&{\\mathbf{if}\\operatorname*{sup}_{f\\in\\widehat{\\mathcal{V}}_{\\ell}}|(\\widehat{\\mathcal{P}}_{\\ell-1,\\varepsilon^{2},\\delta^{\\prime}}[\\widehat{V}_{\\ell}]-\\widehat{\\mathcal{P}}_{\\ell-1,\\varepsilon^{2},\\delta^{\\prime}}[f_{\\ell}])(x_{\\ell-1},a_{\\ell-1})|>\\varepsilon^{2}+\\varepsilon^{2}\\cdot\\beta(t_{\\ell})\\,\\mathbf{then}}\\\\ &{\\qquad\\mathcal{C}_{\\ell}\\leftarrow\\mathcal{C}_{\\ell}\\cup\\big\\{\\left(x_{\\ell-1},a_{\\ell-1}\\right)\\big\\}\\mathrm{~and~}B_{\\ell}\\gets B_{\\ell}\\cup\\big\\{\\left(x_{\\ell-1},a_{\\ell-1},\\widehat{V}_{\\ell},\\widehat{\\mathcal{V}}_{\\ell},t_{\\ell}\\right)\\big\\}.}\\\\ &{\\qquad\\mathbf{for}\\ \\tau=\\ell,...\\ ,h+1\\,\\mathbf{do}}\\\\ &{\\qquad\\qquad(\\widehat{V}_{\\tau:H},\\widehat{\\mathcal{V}}_{\\tau:H},\\mathcal{C}_{\\tau:H},\\mathcal{B}_{\\tau:H},t_{\\tau:H})\\gets\\mathsf{R P F S}_{\\tau}^{\\mathrm{exo}}(\\widehat{V}_{\\tau+1:H},\\widehat{\\mathcal{V}}_{\\tau+1:H},\\mathcal{C}_{\\tau:H},\\mathcal{B}_{\\tau:H},t_{\\tau:H};\\mathcal{V}_{\\ell})}\\end{array}$   \n15:   \n16:   \n17:   \n18: go to line 8.   \n19: if $h=0$ then return $(\\widehat{V}_{1:H},\\cdot,\\cdot,\\cdot,\\cdot)$ . $^{\\prime\\star}$ Re-fit $\\widehat{V}_{h}$ and build a new confidence set. $\\star/$   \n20: for $(x_{h-1},a_{h-1})\\in{\\mathcal{C}}_{h}$ do   \n21: Set $\\mathscr{D}_{h}\\big(x_{h-1},a_{h-1}\\big)\\gets\\emptyset$ .   \n22: for $i=1,\\dots,N_{\\mathrm{reg}}\\;\\mathbf{do}$   \n23: Sample $\\mathbf{x}_{h}\\sim T_{h-1}(\\cdot\\mid x_{h-1},a_{h-1})$ .   \n24: For each $a\\in\\mathcal{A}$ , let $\\widehat{V}_{h}({\\pmb x}_{h})$ be a Monte-Carlo estimate for $\\mathbb{E}^{\\widehat{\\pi}_{h:H}}\\left[\\sum_{\\ell=h}^{H}\\pmb{r}_{\\ell}\\mid\\pmb{x}_{h}\\right]$ computed by collecting $N_{\\mathrm{est}}(|\\mathcal{C}_{h}|)$ trajectories starting from $x_{h}$ and rolling out with $\\widehat{\\pi}_{h:H}$ .   \n25: Update $\\mathcal{D}(x_{h-1},a_{h-1})\\gets\\mathcal{D}(x_{h-1},a_{h-1})\\cup\\{(\\pmb{x}_{h},\\widehat{V}_{h}(\\pmb{x}_{h}))\\}$ .   \n26: Let V\u0302h $\\begin{array}{r}{:=\\arg\\operatorname*{min}_{\\stackrel{f\\in\\mathcal{V}_{h}}{c}}\\sum_{(x_{h-\\stackrel{1}{\\times}},a_{h-1})\\in\\mathcal{C}_{h}}\\sum_{(x_{h},v_{h})\\in\\mathcal{D}_{h}(x_{h-1},a_{h-1})}(f(x_{h})-v_{h})}\\end{array}$ 2.   \n27: Compute value function confidence set $\\widehat{\\mathcal{V}}_{h}:=\\left\\{f\\in\\mathcal{V}_{h}\\,\\Bigg|\\,\\sum_{\\substack{(x_{h-1},a_{h-1})\\in\\mathcal{C}_{h}}}\\frac{1}{N_{\\mathrm{reg}}}\\sum_{\\substack{(x_{h},\\cdot)\\in\\mathcal{D}_{h}\\,(x_{h-1},a_{h-1})}}\\left(\\widehat{V}_{h}(x_{h})-f(x_{h})\\right)^{2}\\leq\\varepsilon_{\\mathrm{reg}}^{2}\\right\\}.$ (16) ", "page_idx": 40}, {"type": "text", "text": "28: return $(\\widehat{V}_{h:H},\\widehat{\\mathcal{V}}_{h:H},\\mathcal{C}_{h:H},\\mathcal{B}_{h:H},t_{h:H})$ . ", "page_idx": 40}, {"type": "text", "text": "1: input: Decoder class $\\Phi$ , suboptimality $\\varepsilon\\in(0,1)$ , confidence $\\delta\\in(0,1)$ . /\\* Set parameters for RVFS and define the value function and policy classes. \\*/ 2: Set $\\varepsilon_{\\mathsf{R V F S}}\\gets\\varepsilon H^{-1}/48$ . 3: Set $\\mathcal{V}=\\mathcal{V}_{1:H}$ , where $\\mathcal{V}_{h}=\\left\\{x\\mapsto f(\\phi(x)):f\\in[0,H]^{S},\\phi\\in\\Phi\\right\\}$ , $\\forall h\\in[H]$ . 4: Set $\\textstyle\\prod=\\prod_{1:H}$ , where $\\begin{array}{r}{:\\Pi_{h}=\\big\\{\\pi(\\cdot)\\in\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}\\,f\\big(\\phi(\\cdot),a\\big):f\\in[0,H]^{S\\times A},\\phi\\in\\Phi\\big\\},\\,\\forall h\\in[H].}\\end{array}$ /\\* Set parameters for BehaviorCloning. $\\star/$ 56::  SSeett $N_{\\mathrm{bc}}\\gets8H^{2}\\log(4H|\\Pi|/\\delta)/\\varepsilon$ $N_{\\mathrm{boost}}\\leftarrow\\log(1/\\delta)/\\log(24S A H\\varepsilon)$ $N_{\\mathrm{eval}}\\leftarrow16^{2}\\varepsilon^{-2}\\log(2N_{\\mathrm{boost}}/\\delta)$ . $M\\gets\\lceil8\\varepsilon_{\\mathsf{R V F S}}^{-1}S A C_{\\mathsf{c o v}}H\\rceil,$ $N_{{\\mathrm{test}}}\\gets2^{8}M^{2}H\\varepsilon_{\\sf R V F S}^{-1}\\log\\bigl(80M^{6}H^{8}N_{{\\mathrm{boost}}}\\varepsilon_{\\sf R V F S}^{-2}\\delta^{-1}\\bigr)$ $\\begin{array}{r}{\\delta^{\\prime}=\\frac{\\delta}{40M^{7}N^{2}H^{8}|\\mathcal{V}|N_{\\mathrm{boost}}}}\\end{array}$ 7: Set $N_{\\mathrm{reg}}\\gets2^{8}M^{2}\\varepsilon_{\\mathsf{R V F S}}^{-1}\\log(80|\\Phi|^{2}H M^{2}N_{\\mathsf{b o o s t}}\\delta^{-1})$ . 8: Set $\\widehat{V}_{1:H}\\gets$ arbitrary, $\\widehat{\\mathcal{V}}_{1:H}\\gets\\mathcal{V}$ , $\\mathcal{C}_{0:H}\\gets\\emptyset$ , $B_{0:H}\\gets\\emptyset$ , $i_{\\mathsf{o p t}}=1$ , and $J_{\\operatorname*{max}}=0$ . /\\* Repeatedly invoke RVFSexo and extract policy with BehaviorCloning to boost confidence. $\\star/$ 9: for $i=1,\\dots,N_{\\mathrm{boost}}\\,\\dot{\\mathbf{c}}$ do $^{\\prime\\star}$ Invoke RVFSexo. $\\star/$ 10: $\\big(\\widehat{V}_{1:H}^{(\\ast)},\\cdot,\\cdot,\\cdot\\big)\\gets\\mathsf{R W F S}_{0}^{\\mathrm{exo}}(\\widehat{V}_{1:H},\\widehat{V}_{1:H},\\mathcal{C}_{0:H},\\mathcal{B}_{0:H};\\mathcal{V},N_{\\mathrm{reg}},N_{\\mathrm{test}},\\varepsilon_{\\mathsf{R W F S}},\\delta/(10N_{\\mathrm{boost}})).$ 11: Define $^{\\prime\\star}$ Imitation learning with BehaviorCloning. $\\widehat{\\pi}_{h}^{\\mathsf{R V F S}}(\\cdot)\\in\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}\\widehat{\\mathcal{P}}_{h,\\varepsilon_{\\mathsf{R V F S}},\\delta^{\\prime}}[\\widehat{V}_{h+1}^{(i)}](\\cdot,a)$ $\\star/$ . 12: mpute $\\widehat{\\pi}_{1:H}^{(i)}\\gets$ BehaviorCloPning $\\langle\\Pi,\\varepsilon,\\widehat{\\pi}_{1:H}^{\\mathsf{R V F S}},\\delta/(2N_{\\mathsf{b o o s t}}))$ . $^{\\prime\\star}$ Evaluate current policy. \\*/ 13: $v=0$ . 1154:: $\\mathbf{for}=1,\\dots,N_{\\mathrm{eval}}$ odroy $(\\pmb{x}_{1},\\pmb{a}_{1},\\pmb{r}_{1},\\dots,\\pmb{x}_{H},\\pmb{a}_{H},\\pmb{r}_{H})$ by executing $\\widehat{\\pi}_{1:H}^{(i)}$ . 16: Set $\\begin{array}{r}{\\boldsymbol{v}\\leftarrow\\boldsymbol{v}+\\sum_{h=1}^{H}\\boldsymbol{r}_{h}}\\end{array}$ . 17: Set $\\widehat{J}(\\widehat{\\pi}_{1:H}^{(i)})\\leftarrow v/N_{\\mathrm{eval}}$ . 18: if $\\widehat{J}(\\widehat{\\pi}_{1:H}^{(i)})>J_{\\operatorname*{max}}$ then 19: Set $i_{\\mathsf{o p t}}=i$ . 20: Set $J_{\\operatorname*{max}}=\\widehat{J}(\\widehat{\\pi}_{1:H}^{(i)})$ . 21: return: \u03c0\u03021\u2236H = \u03c0\u03021i\u2236oHpt ", "page_idx": 41}, {"type": "text", "text": "G Organization ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "This remainder of Part III of the appendix contains the proofs for the main results concerning the RVFS algorithm (Theorem 4.1 and Theorem B.1). ", "page_idx": 42}, {"type": "text", "text": "\u2022 First, in Appendix H we give a brief overview of the analysis and introduce a restricted set of benchmark policies which will be used throughout the proofs for Theorem 4.1 and Theorem B.1. The benchmark policy class is central to the regret decomposition for RVFS, and facilitates an analysis that does not require optimism.   \n\u2022 In Appendix I, we prove Theorem 4.1 under Setup II ( $V^{\\pi}$ -realizability). This constitutes the main technical development for Theorem 4.1. The central technical results proven here are Theorem I.1, Theorem I.2 and which generalize Theorem 4.1.   \n\u2022 In Appendix J, we prove Theorem 4.1 under Setup I ( $V^{\\star}$ -realizability), as a straightforward consequence of the tools developed in Appendix I (Theorem I.1 and Theorem I.2).   \n\u2022 Finally, in Appendix K, we prove Theorem B.1 (analysis of $\\mathsf{R V F S}^{\\mathsf{e x o}}$ for the ExBMDP problem). This analysis has a similar structure to the proof of Theorem 4.1 under Setup II in Appendix I, and builds on the same analysis techniques, but requires specialized arguments due to extra technical challenges in the ExBMDP setting.   \n\u2022 Appendix M gives a self-contained presentation of the BehaviorCloning algorithm for imitation learning, which is used within RVFS.bc and RVF $S^{\\etimes\\ o}$ .bc. ", "page_idx": 42}, {"type": "text", "text": "H Overview of Analysis and Preliminaries ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "In this section, we present some notation, technical tools, and preliminary results we require for the analysis of RVFS in the settings we described in Section 4. We start by defining a set of restricted benchmark policies used in the regret decomposition for RVFS. ", "page_idx": 42}, {"type": "text", "text": "H.1 Overview of Analysis ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "In this section we give a brief overview of the analysis techniques behind Theorem 4.1 and Theorem B.1. We focus on Theorem 4.1 to begin. ", "page_idx": 42}, {"type": "text", "text": "Recall that RVFS is recursive in the sense that whenever the test in Line 14 fails for a layer $h\\in[H]$ , an recursive call $\\mathsf{R V F S}_{h}$ is initiated. Throughout the recursion, via the steps in Item 1 and Item 2, RVFS maintains the following invariant: whenever a call to ${\\mathsf{R V F S}}_{h}$ (an instance of RVFS initiated at layer $h$ ) terminates, the confidence sets $\\widehat{\\mathcal{V}}_{h+1:H}$ that it outputs satisfy, with high probability: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\forall\\ell\\in[h+1\\ldots H],\\quad V_{\\ell}^{\\star}\\in\\widehat{\\mathcal{V}}_{\\ell}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "In addition, $\\mathsf{R V F S}_{h}$ can only return if the value function tests in Line 14 (which involve the confidence sets $\\widehat{\\mathcal{V}}_{h+1:H})$ ) all succeed. From the invariant in (Inv1), it can be shown that the tests only succeed if the estimated value functions $\\widehat{V}_{h+1:H}$ satisfy: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\forall\\ell\\in[h+1\\,.\\,L],\\quad\\mathbb{P}^{\\widehat{\\pi}}\\left[\\operatorname*{sup}_{a\\in A}|(\\mathcal{P}_{\\ell}[\\widehat{V}_{\\ell+1}]-\\mathcal{P}_{\\ell}[V_{\\ell+1}^{\\star}])(x_{h},a)|\\geq3\\varepsilon\\right]\\leq\\varepsilon_{\\mathrm{test}},\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where $\\varepsilon_{\\tt t e s t}\\,>\\,0$ is a parameter set by the algorithm. We use pushforward coverability to show that RVFS can only expand the core-sets $\\mathcal{C}_{1:H}$ a polynomial number of times before the algorithm terminates and (Inv2) is satisfied. ", "page_idx": 42}, {"type": "text", "text": "The invariant in (Inv2) is useful because it ensures that the greedy policies ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\widehat{\\pi}_{h}(x)\\approx\\underset{a\\in\\mathcal{A}}{\\arg\\operatorname*{max}}\\,\\mathcal{P}_{h}[\\widehat{V}_{h+1}](x,a)\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "induced by the learned value functions $\\widehat{V}_{1:H}$ are near-optimal. To make this precise, recall that given parameters $\\varepsilon,\\delta\\in(0,1)$ , the action $\\widehat{\\pi}_{h}(x)$ of RVFS\u2019s policy at layer $h$ and state $x\\in\\mathscr{X}$ , is given by ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\widehat{\\pi}_{h}(x)\\in\\arg\\operatorname*{max}_{a\\in\\mathcal{A}\\atop a\\in\\mathcal{A}}\\widehat{\\mathcal{P}}_{h,\\varepsilon,\\delta}[\\widehat{V}_{h+1}](x,a),\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where $\\widehat{V}_{h+1}$ is the estimated value function at layer $h+1$ . The operator $\\widehat{\\mathcal{P}}_{h,\\varepsilon,\\delta}[\\widehat{V}_{h+1}]$ (Algorithm 7), when given input $(x,a)\\,\\in\\,\\mathcal{X}\\times\\mathcal{A}$ , ensures that probability at least $1-\\delta$ , $|\\widehat{\\mathcal{P}}_{h,\\varepsilon,\\delta}[\\widehat{V}_{h+1}](x,a)-$ ", "page_idx": 42}, {"type": "text", "text": "$\\mathcal{P}_{h}[\\widehat{V}_{h+1}](x,a)|\\le\\varepsilon$ . Combining this with the invariant in (Inv2) and the fact that $V_{h}^{\\star}\\equiv\\mathcal{P}_{h}[V_{h+1}^{\\star}]$ , one can see that with high probability (over the randomness in $x_{h}\\sim\\mathbb{P}^{\\widehat{\\pi}}$ and $\\widehat{\\mathcal{P}}$ ), we have: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{a\\in\\mathcal{A}}\\big|\\widehat{\\mathcal{P}}_{h,\\varepsilon,\\delta}\\big[\\widehat{V}_{h+1}\\big](\\boldsymbol{x}_{h},\\boldsymbol{a})-V_{h}^{\\star}\\big(\\boldsymbol{x}_{h},\\boldsymbol{a}\\big)\\big|\\leq4\\varepsilon.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Analysis under Setup I. For Theorem 4.1 (Setup I), Eq. (18) together with the definition of $\\widehat{\\pi}_{h}$ in Eq. (17) and the gap assumption (Assumption 4.4) implies that if $\\varepsilon\\le\\Delta/8$ , we have that with high probability (over the randomness in $x_{h}\\sim\\mathbb{P}^{\\widehat{\\pi}}$ and $\\widehat{\\mathcal{P}}$ ), ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\widehat{\\pi}_{h}(x_{h})\\in\\underset{a\\in\\mathcal{A}}{\\arg\\operatorname*{max}}\\mathcal{\\widehat{P}}_{h,\\varepsilon,\\delta}[\\widehat{V}_{h+1}](x_{h},a)=\\underset{a\\in\\mathcal{A}}{\\arg\\operatorname*{max}}V_{h}^{\\star}(x,a)=\\pi_{h}^{\\star}(x_{h}).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "This suffices to show that $\\widehat{\\pi}$ is near-optimal, since by the performance lemma [33], the suboptimality of $\\widehat{\\pi}$ can be bounded as ", "page_idx": 43}, {"type": "equation", "text": "$$\nJ(\\pi^{\\star})-J(\\widehat{\\pi})\\leq\\sum_{h=1}^{H}\\mathbb{P}^{\\widehat\\pi}[\\widehat{\\pi}_{h}(x_{h})\\neq\\pi_{h}^{\\star}(x_{h})].\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "This suffices to prove the performance bound in Theorem 4.1 under Setup I. ", "page_idx": 43}, {"type": "text", "text": "Analysis under Setup II. For Theorem 4.1 (Setup $\\mathbf{II}_{,}$ ), an immediate challenge in applying a similar analysis to Setup I is the lack of suboptimality gap $\\Delta$ , which makes it impossible to directly bound the probability that $\\widehat{\\pi}\\neq\\pi^{\\star}$ in Eq. (19). To address this, we introduce a restricted benchmark policy class $\\Pi_{\\varepsilon}\\subset\\Pi_{\\mathsf{S}}$ in Appendix H.2 below. The class $\\Pi_{\\varepsilon}\\subset\\Pi_{\\mathsf{S}}$ is constructed such that that there exists a policy $\\pi\\in\\Pi_{\\varepsilon}$ that (i) is $O(\\varepsilon)$ -suboptimal, and (ii) emulates certain properties of a gap. Together, these properties facilitate analysis similar to Setup I. Overall, this argument is similar to the \u201cvirtual policy iteration\u201d analysis in Yin et al. [65]. ", "page_idx": 43}, {"type": "text", "text": "Analysis of RVFSexo. The analysis of $\\mathsf{R V F S}^{\\mathsf{e x o}}$ for ExBMDPs (Theorem B.1) uses the same idea as the analysis for Setup $\\mathbf{II}$ , except that we can only realize $V^{\\pi}$ for endogenous policies that act on $\\phi^{\\star}({\\pmb x}_{h})$ . To address this, we use the randomized rounding scheme in $\\mathsf{R V F S}^{\\mathsf{e x o}}$ , and the crux of the proof is to show that with high probability, the rounded policies in RVF $S^{\\mathrm{exo}}$ \u201csnap\u201d onto endogenous policies, facilitating an argument similar to Setup II. ", "page_idx": 43}, {"type": "text", "text": "Generalizing the analysis. We mention in passing that RVFS can be slightly modified to recover other existing sample complexity guarantees for RL with linear function approximation and local simulator access that do not require pushforward coverability, including linear- $Q^{\\star}$ realizability with gap [40] and $Q^{\\pi}$ -realizability [65]; we leave a more general treatment for future work. ", "page_idx": 43}, {"type": "text", "text": "H.2 Benchmark Policy Class and Randomized Policies ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "As described above, central to our analysis is a set of $O(\\varepsilon)$ -suboptimal policies against which we benchmark the policies returned RVFS, which emulate certain consequences of the $\\Delta$ -gap assumption (Assumption 4.4). Before introducing this concept formally, we first define the notion of a randomized policy. ", "page_idx": 43}, {"type": "text", "text": "Induced stochastic policies. Given an arbitrary collection of independent random variables $\\widetilde{Q}=$ $(\\widetilde{Q}_{h}(x,a))_{(h,x,a)\\in[H]\\times\\mathcal{X}\\times A}$ , we say that a policy $\\pi$ is induced by $\\widetilde{Q}$ if $\\pi$ satisfies ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\forall h\\in[H],\\forall x\\in\\mathcal{X},\\quad\\pi_{h}(x)\\in\\arg\\operatorname*{max}_{a^{\\prime}\\in\\mathcal{A}}\\widetilde{Q}_{h}(x,a^{\\prime}),\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where we use the bold notation $\\pi_{h}(x)$ as shorthand for the random variable $\\pmb{a}_{h}\\sim\\pi_{h}(x)\\in\\Delta(\\mathcal{A})$ ; in other words, for each $x\\in\\mathscr{X}$ , $\\pi_{h}(x)\\in\\Delta({\\mathcal{A}})$ is the distribution induced by sampling $\\mathbf{Q}_{h}(x,\\cdot)$ and playing the action $\\pi_{h}(x)\\in\\arg\\operatorname*{max}_{a^{\\prime}\\in\\mathcal{A}}\\widetilde{Q}_{h}(x,a^{\\prime})$ . If there are ties in (20), we break them by picking the action with the smallest index; we assume without loss of generality that actions in $\\boldsymbol{\\mathcal{A}}$ are index from $1,\\ldots,|A|$ . ", "page_idx": 43}, {"type": "text", "text": "Benchmark policy class. We now define the benchmark policy class as follows. ", "page_idx": 43}, {"type": "text", "text": "Definition H.1 (Benchmark policy class). For $\\varepsilon\\;\\in\\;(0,1)$ , let $\\Pi_{\\varepsilon}\\ \\subseteq\\ \\Pi_{\\mathsf{S}}$ be the set of stochastic policies such that $\\pi\\in\\Pi_{\\varepsilon}$ if and only if there exists a collection of independent random variables $\\mathbf{\\bar{\\boldsymbol{Q}}}=(\\widetilde{\\pmb{Q}}_{h}(x,a))_{(h,x,a)\\in[H]\\times\\mathcal{X}\\times\\mathcal{A}}$ in $[0,H]$ such that: ", "page_idx": 43}, {"type": "text", "text": "\u2022 For all $(h,x,a)\\in[H]\\times\\mathcal{X}\\times\\mathcal{A},$ , we have $|(\\widetilde{\\pmb{Q}}_{h}-\\boldsymbol{Q}_{h}^{\\pi})(\\boldsymbol{x},a)|\\leq\\varepsilon$ , almost surely under the draw of $\\widetilde{Q}$ . ", "page_idx": 44}, {"type": "text", "text": "Intuitively, the set $\\Pi_{\\varepsilon}$ contains the set of all (stochastic) policies corresponding to (randomized) state-action value functions that are point-wise $O(\\varepsilon)$ close to $Q^{\\star}$ . We formalize this claim in the next lemma. ", "page_idx": 44}, {"type": "text", "text": "Lemma H.1 (Suboptimality of benchmark policies). Let $\\varepsilon\\,\\in\\,(0,1)$ be given. Let $\\tilde{\\pi}\\,\\in\\,\\Pi_{\\varepsilon}$ be $a$ stochastic policy induced by a collection of (independent) random state-action value functions $(\\widetilde{Q}_{h}(x,a))_{(h,x,a)\\in[H]\\times\\mathcal{X}\\times\\mathcal{A}}\\subset[0,H]$ such that for all $h\\in[H]$ and all $(x,a)\\in\\mathcal{X}\\times\\mathcal{A}$ : ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{|\\widetilde{Q}_{h}(x,a)-Q_{h}^{\\tilde{\\pi}}(x,a)|\\leq\\varepsilon}&{a l m o s t\\;s u r e l y,\\;a n d}&{\\tilde{\\pi}_{h}(x)\\in\\underset{a^{\\prime}\\in\\mathcal{A}}{\\arg\\operatorname*{max}}\\widetilde{Q}_{h}(x,a^{\\prime}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Then, for all $h\\in[H]$ , ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\forall x\\in\\mathcal{X},\\quad V_{h}^{\\star}(x)\\leq V_{h}^{\\tilde{\\pi}}(x)+3H\\varepsilon.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Proof of Lemma H.1. Using backward induction over $\\ell=H,\\ldots,1$ , we start by showing that all $\\ell$ ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\forall(x,a)\\in\\mathcal{X}\\times\\mathcal{A},\\quad Q_{\\ell}^{\\star}(x,a)\\leq\\widetilde Q_{\\ell}(x,a)+2\\varepsilon\\cdot(H-\\ell+1).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "almost surely. We then instantiate this with $\\ell=1$ and use the fact that $\\|\\widetilde{\\mathbf{Q}}_{h}-Q_{h}^{\\widetilde{\\pi}}\\|_{\\infty}\\le\\varepsilon$ to get the desired result. ", "page_idx": 44}, {"type": "text", "text": "Base case $[\\ell\\;=\\;H]$ . By definition of the state-action value function, we have, for all $\\pi\\,\\in\\,\\Pi_{\\ S}$ , $Q_{H}^{\\star}\\equiv Q_{H}^{\\pi}$ . Thus, since $\\operatorname*{sup}_{(x,a)\\in\\mathcal{X}\\times\\mathcal{A}}|(\\widetilde{Q}_{H}-Q_{H}^{\\tilde{\\pi}})(x,a)|\\leq\\varepsilon$ almost surely (by definition of $\\widetilde{Q}_{1:H}$ ), we get that ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\forall(x,a)\\in\\mathcal{X}\\times\\mathcal{A},\\quad|\\widetilde{Q}_{H}(x,a)-Q_{H}^{\\star}(x,a)|\\leq\\varepsilon,\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "almost surely. This implies (21) for $\\ell=H$ . ", "page_idx": 44}, {"type": "text", "text": "General case $[\\ell<h]$ . ${\\mathrm{Fix~}}h\\in[H-1]$ and suppose that (22) holds for all $\\ell\\in[h+1,\\ldots,H]$ almost surely. We show that it holds for $\\ell=h$ almost surely. Fix $(x,a)\\in\\mathcal{X}\\times\\mathcal{A}$ . We have ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{h}^{\\star}(x,a)-\\widetilde{Q}_{h}(x,a)=\\mathcal{T}_{h}[Q_{h+1}^{\\star}](x,a)-\\mathcal{T}_{h}[\\widetilde{Q}_{h+1}](x,a)+\\mathcal{T}_{h}[\\widetilde{Q}_{h+1}](x,a)-\\widetilde{Q}_{h}(x,a),}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq2\\varepsilon\\cdot(H-h)+\\mathcal{T}_{h}[\\widetilde{Q}_{h+1}](x,a)-\\widetilde{Q}_{h}(x,a),}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "almost surely, where the last step follows by the induction hypothesis. We now bound $T_{h}[\\widetilde{\\pmb{Q}}_{h+1}](x,\\dot{a})-\\widetilde{\\pmb{Q}}_{h}(x,a)$ . We have, almost surely, that ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{r_{h}[\\widetilde{Q}_{h+1}](x,a)-\\widetilde{Q}_{h}(x,a)=\\mathcal{T}_{h}[\\widetilde{Q}_{h+1}](x,a)-\\mathcal{P}_{h}[V_{h+1}^{\\widetilde{n}}](x,a)+\\mathcal{P}_{h}[V_{h+1}^{\\widetilde{n}}](x,a)-\\widetilde{Q}_{h}(x,a),}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\mathcal{T}_{h}[\\widetilde{Q}_{h+1}](x,a)-\\mathcal{P}_{h}[V_{h+1}^{\\widetilde{n}}](x,a)+Q_{h}^{\\widetilde{n}}(x,a)-\\widetilde{Q}_{h}(x,a),}\\\\ &{\\qquad\\quad\\qquad\\qquad=\\mathcal{T}_{h}[\\widetilde{Q}_{h+1}](x,a)-\\mathcal{P}_{h}[V_{h+1}^{\\widetilde{n}}](x,a)+\\varepsilon,\\quad\\mathrm{(by~the~asumption~on~}\\widetilde{Q}_{h})}\\\\ &{\\qquad\\quad\\qquad\\qquad=\\mathbb{E}\\left[\\operatorname*{max}_{a\\in\\mathcal{A}}\\widetilde{Q}_{h+1}(x_{h+1},a^{\\prime})-Q_{h+1}^{\\widetilde{n}}(x_{h+1},\\widetilde{\\pi}_{h+1}(x_{h+1}))\\mid x_{h}=x,a_{h}=a\\right]}\\\\ &{\\qquad\\quad\\qquad\\qquad=\\mathbb{E}\\left[\\widetilde{Q}_{h+1}(x_{h+1},\\widetilde{\\pi}_{h+1}(x_{h+1}))-Q_{h+1}^{\\widetilde{n}}(x_{h+1},\\widetilde{\\pi}_{h+1}(x_{h+1}))\\mid x_{h}=x,a_{h}\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\left.(2\\varepsilon,}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where the penultimate inequality follows by the definition of $\\tilde{\\pi}_{h+1}$ , and the last inequality follows by the fact that $\\|\\widetilde{\\mathbfit{Q}}_{h+1}-Q_{h+1}^{\\widetilde{\\pi}}\\|_{\\infty}\\le\\varepsilon$ almost surely, by assumption. Plugging (24) into (23) completes the induction, and so we have that ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\forall(x,a)\\in\\mathcal{X}\\times\\mathcal{A},\\quad Q_{1}^{\\star}(x,a)\\leq\\widetilde{Q}_{1}(x,a)\\leq2H\\varepsilon.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "In particular, taking the max over $a$ on both sides and using the definition of $\\tilde{\\pi}$ , we get that ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\forall x\\in\\mathcal{X},\\quad V_{1}^{\\star}(x)\\leq\\widetilde Q_{1}\\big(x,\\widetilde\\pi_{1}(x)\\big)\\leq2H\\varepsilon,\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "almost surely. Combining this with the fact that $\\widetilde{Q}_{1}(x,\\widetilde{\\pi}_{1}(x))\\leq Q_{1}^{\\tilde{\\pi}}(x,\\widetilde{\\pi}_{1}(x))+\\varepsilon$ , almost surely (since $\\|\\widetilde{Q}_{1}-Q_{1}^{\\widetilde{\\pi}}\\|_{\\infty}\\leq\\varepsilon$ almost surely by assumption) implies that ", "page_idx": 44}, {"type": "equation", "text": "$$\nV_{1}^{\\star}(x)\\leq Q_{1}^{\\pi}(x,\\tilde{\\pi}_{1}(x))+2H\\varepsilon+\\varepsilon.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Taking expectation over $\\tilde{\\pi}_{1}(x)$ and bounding $2H\\varepsilon+\\varepsilon$ by $3H\\varepsilon$ leads to the desired result. ", "page_idx": 44}, {"type": "text", "text": "H.3 Additional Preliminaries ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "The following lemma gives a guarantee for the Bellman backup approximation algorithm $\\widehat{\\mathcal{P}}$ (Algorithm 7) that is tailored to the analysis of RVFS. ", "page_idx": 45}, {"type": "text", "text": "Lemma H.2. Let $\\varepsilon,\\delta,\\delta^{\\prime}\\,\\in\\,(0,1)$ , $B\\,>\\,0$ , and $h\\,\\in\\,[H]$ , be given and let $\\nu$ be a finite function class. For any sequence $(x_{i})_{i\\geq1}\\subset\\mathcal{X}$ of state action pairs, the outputs $(\\widehat{\\mathcal{P}}_{h,\\varepsilon,\\delta^{\\prime}}[f](x_{i},a))_{i\\geq1,a\\in\\mathcal{A}}\\;\\mathrm{o}\\ r\\to\\infty,$ of Algorithm 7 satisfy, with probability at least $1-\\delta$ , ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\forall i\\geq1,\\forall f\\in\\mathcal{V},\\forall a\\in\\mathcal{A},\\quad|\\widehat{\\mathcal{P}}_{h,\\varepsilon,\\delta^{\\prime}}[f](x_{i},a)-\\mathcal{P}_{h}[f](x_{i},a)|\\leq\\varepsilon\\cdot\\sqrt{2\\log_{1/\\delta^{\\prime}}(2A i^{2}|\\mathcal{V}|/\\delta)}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Proof of Lemma H.2. By Hoeffding\u2019s inequality [28] and the union bound over $a\\in\\mathcal{A}$ and $f\\in\\mathcal{V}$ , we have that for any $i\\geq1$ , with probability at least $\\bar{1}-\\bar{\\delta}/(2i^{2})$ , ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\forall\\,f\\in\\mathcal{V},\\forall a\\in A,\\quad|\\widehat{\\mathcal{P}}_{h,\\varepsilon,\\delta^{\\prime}}[f](x_{i},a)-\\mathcal{P}_{h}[f](x_{i},a)|\\leq\\varepsilon\\cdot\\sqrt{2\\log_{1/\\delta^{\\prime}}(2A i^{2}|\\mathcal{V}|/\\delta)}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "The desired result follows by the union bound over $i\\geq1$ and the fact that $\\textstyle\\sum_{i\\geq1}1/i^{2}=\\pi^{2}/6\\leq2$ . ", "page_idx": 45}, {"type": "text", "text": "I Guarantee under $V^{\\pi}$ -Realizability (Proof of Theorem 4.1, Setup II) ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "In this section, we prove Theorem 4.1 under Setup II. First, in Appendix I.1 we state a number of supporting technical lemmas, then use them to prove a more general version of Theorem 4.1, Theorem I.2, which holds under a weaker realizability assumption (informally, $V^{\\pi}$ -realizability only for near-optimal policies $\\pi$ ); Theorem 4.1 follows as an immediate consequence. The remainder of the section (Appendix I.2 through to Appendix I.6) contains the proofs for the intermediate results. ", "page_idx": 45}, {"type": "text", "text": "I.1 Analysis: Proof of Theorem 4.1 (Setup II) ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "We analyze RVFS in the setting of Theorem 4.1 (Setup II), where we have a function class $\\nu$ satisfying $V^{\\pi}$ -realizability (Assumption 4.5). We will actually show that the conclusion of Theorem 4.1 holds under a weaker function approximation setup we refer to as relaxed $V^{\\pi}$ -realizability: instead of requiring $V^{\\pi}$ -realizability for all $\\pi\\in\\Pi_{\\ S}$ , we only require it for policies $\\pi$ in the set of near-optimal policies corresponding to the benchmark policy class $\\Pi_{\\varepsilon_{\\mathsf{r e a l}}}$ for some $\\varepsilon_{\\mathsf{r e a l}}>0$ $\\Pi_{\\varepsilon}$ is defined in Appendix H). ", "page_idx": 45}, {"type": "text", "text": "Assumption I.1 (Relaxed $V^{\\pi}$ -realizability). For $\\varepsilon_{\\mathsf{r e a l}}>0$ and all $\\pi\\in\\Pi_{\\varepsilon_{\\mathrm{real}}}$ and $h\\in[H]$ , we have $V_{h}^{\\pi}(x)\\,\\,\\bar{\\epsilon}\\,\\mathcal{V}\\subseteq\\{f:\\mathcal{X}\\to[0,H]\\}$ . ", "page_idx": 45}, {"type": "text", "text": "We will analyze RVFS under Assumption I.1 and Assumption 4.1. However, it turns out that all of the main results for RVFS can be derived under this assumption: As we will see in Appendix J in the sequel, when the $\\Delta$ -gap assumption (Assumption 4.4) is satisfied, then $\\Pi_{\\varepsilon_{\\mathrm{real}}}=\\{\\pi^{\\star}\\}$ for all $\\varepsilon_{\\mathsf{r e a l}}<\\Delta$ , allowing us to prove Theorem 4.1 under Setup I as a special case of relaxed $V^{\\pi}$ -realizability. Our analysis for the ExBMDP setting in Appendix K requires more work, but uses that for ExBMDPs, Assumption I.1 is satisfied for a subset of $\\Pi_{\\varepsilon_{\\mathsf{r e a l}}}$ corresponding to endogenous policies. ", "page_idx": 45}, {"type": "text", "text": "We begin with our analysis under Setup $\\mathbf{II}$ by bounding the number of times the test in Line 14 fails. Since the sizes of the core sets $\\mathcal{C}_{1:H}$ in RVFS are directly related to the number of test failures, the next result, which bounds $\\left|\\mathcal{C}_{h}\\right|$ for $h\\in[H]$ , allows us to show that RVFS terminates in polynomial iterations with high probability. The proof is in Appendix I.2. ", "page_idx": 45}, {"type": "text", "text": "Lemma I.1 (Bounding the number of test failures). Let $\\delta,\\varepsilon\\,\\in\\,(0,1)$ be given, and suppose that Assumption 4.1 (pushforward coverability) holds with parameter $C_{\\mathsf{p u s h}}>0$ . Further, let $f\\in\\mathcal V$ , be given, where $\\nu$ is an arbitrary function class. Then there is an event $\\mathcal{E}$ of probability at least $1-\\delta$ under which any call $\\mathsf{R V F S}_{0}(f,\\mathcal V_{1:H},\\otimes,\\otimes,0;\\mathcal V,\\varepsilon,\\delta)$ (Algorithm 5) terminates, and throughout the execution of $\\mathsf{R V F S}_{0}$ , we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\forall h\\in[H],\\quad|\\mathcal{C}_{h}|\\leq\\lceil8\\varepsilon^{-1}C_{\\mathsf{p u s h}}H\\rceil.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "In particular, Lemma I.1 ensures that with high probability, every call to $\\mathsf{R V F S}_{h}$ (made recursively via the call to $\\mathsf{R V F S}_{\\mathrm{0}}$ ) terminates in polynomial iterations. When $\\mathsf{R V F S}_{h}$ terminates, all the tests in Line 14 must have passed for all $\\ell>h$ . Using this and a standard concentration argument, we get the following guarantee for the estimated value functions and confidence sets returned by each call to $\\mathsf{R V F S}_{h}$ . The proof is in Appendix I.3. ", "page_idx": 45}, {"type": "text", "text": "Lemma I.2 (Consequence of passing the tests). Let $h\\,\\in\\,[0\\ldots H]$ and $\\varepsilon,\\delta\\,\\in\\,(0,1)$ be given and consider a call to $\\mathsf{R V F S}_{\\mathrm{0}}$ in the setting of Lemma I.1. Further, let $\\mathcal{E}$ be the event of Lemma I.1. There exists an event $\\mathcal{E}_{h}^{\\prime}$ of probability at least $1-\\delta/H$ such that under $\\mathcal{E}\\cap\\mathcal{E}_{h}^{\\prime}$ , if a call to $\\mathsf{R V F S}_{h}$ within the execution of $\\mathsf{R V F S}_{0}$ terminates and returns $(\\widehat{V}_{h:H},\\widehat{\\mathcal{V}}_{h:H},\\mathcal{C}_{h:H},\\mathcal{B}_{h:H},t_{h:H})$ , then for any $(x_{h-1},a_{h-1})\\in{\\mathcal{C}}_{h}$ and $\\ell\\in[h+1\\ldots H+1]$ : ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\mathcal{I}\\left[\\underset{f\\in\\tilde{V}_{\\ell}}{\\operatorname*{sup}}\\ \\underset{a\\in A}{\\operatorname*{max}}\\ \\big|\\mathcal{P}_{\\ell-1}\\big[\\widehat{V}_{\\ell}\\big]-\\mathcal{P}_{\\ell-1}\\big[f_{\\ell}\\big]\\big(\\mathbf{x}_{\\ell-1},a\\big)\\big|>3\\varepsilon\\ \\big|\\ x_{h-1}=x_{h-1},a_{h-1}=a_{h-1}\\right]\\leq\\frac{4\\log\\big(8M^{6}N_{\\mathrm{test}}^{2}H^{8}\\big)}{N_{\\mathrm{test}}}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where $(\\widehat{\\pi}_{\\tau})_{\\tau\\geq h}$ is the stochastic policy induced by $\\widehat{V}_{h:H}$ and $M$ and $N_{\\mathrm{test}}$ are defined as in Algorithm 5. Furthermore, under the event $\\mathcal{E}$ , the total number of times the operator $\\widehat{\\mathcal{P}}$ is called in the test of Line 14 of Algorithm 5 is at most $O(C_{\\mathrm{push}}N_{\\mathrm{test}}H^{4}\\varepsilon^{-\\ddot{1}})$ . ", "page_idx": 46}, {"type": "text", "text": "We now give a guarantee for the estimated value functions $\\widehat{V}_{1:H}$ computed within RVFS in Line 26 (the proof is in Appendix I.4). ", "page_idx": 46}, {"type": "text", "text": "Lemma I.3 (Value function regression guarantee). Let $h\\in[0\\ldots H]$ and $\\delta,\\varepsilon^{\\prime}\\in(0,1)$ be given, and consider a call to $\\mathsf{R V F S}_{\\mathrm{0}}$ in the setting of Lemma I.1. Further, let $\\Pi^{\\prime}\\subseteq\\Pi_{\\mathbb{S}}$ be a finite policy class such that the class $\\nu$ realizes the value function\u2032\u2032s $V^{\\pi}$ for $\\pi\\in\\Pi^{\\prime}$ (i.e. $\\nu$ satisfies Assumption I.1 with $\\Pi_{\\varepsilon_{\\mathsf{r e a l}}}$ replaced by $\\Pi^{\\prime}$ ). Then, there is an event $\\mathcal{E}_{h}^{\\prime\\prime}$ of probability at least $1-\\delta/H$ under which for all $k\\geq1$ , $i f$ ", "page_idx": 46}, {"type": "text", "text": "1. ${\\mathsf{R V F S}}_{h}$ gets called for the kth time during the execution of $\\mathsf{R V F S}_{0}$ ; and ", "page_idx": 46}, {"type": "text", "text": "2. this kth call terminates and returns $(\\widehat{V}_{h:H},\\widehat{\\mathcal{V}}_{h:H},\\mathcal{C}_{h:H},\\mathcal{B}_{h:H},t_{h:H}),$ then i $^{f}(\\widehat{\\pi}_{\\tau})_{\\tau\\geq h}$ is the policy induced by $\\widehat{V}_{h:H}$ and $N_{\\mathrm{reg}}$ is set as in Algorithm $^{5}$ , we have that for all $\\pi\\in\\Pi^{\\prime}$ , ", "page_idx": 46}, {"type": "text", "text": "", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{x_{h-1},a_{h-1})\\in\\mathcal{C}_{h}}\\frac{1}{N_{\\mathrm{reg}}}\\sum_{\\substack{(x_{h},-)\\in\\mathcal{D}_{h}\\,(x_{h-1},a_{h-1})}}\\left(\\widehat{V}_{h}(x_{h})-V_{h}^{\\pi}(x_{h})\\right)^{2}}\\\\ &{\\le\\displaystyle\\frac{9k H^{2}\\log(8k^{2}H|\\Pi^{\\prime}||\\mathcal{V}|/\\delta)}{N_{\\mathrm{reg}}}+8H^{2}\\sum_{\\substack{(x_{h-1},a_{h-1})\\in\\mathcal{C}_{h}\\,\\tau=h}}\\sum_{\\tau=h}^{H}\\mathbb{E}^{\\widehat{\\pi}}\\left[D_{\\mathrm{tv}}(\\widehat{\\pi}_{\\tau}(x_{\\tau}),\\pi_{\\tau}(x_{\\tau}))\\mid x_{h-1}=x_{h-1},a_{h-1}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where the datasets $\\{\\mathcal{D}_{h}(x,a):(x,a)\\in\\mathcal{C}_{h}\\}$ are as in the definition of $\\widehat{\\mathcal{V}}_{h}$ in (15). ", "page_idx": 46}, {"type": "text", "text": "Next, we use this result to show that the confidence sets $\\widehat{\\mathcal{V}}_{1:H}$ returned by $\\mathsf{R V F S}_{\\mathrm{0}}$ are \u201cvalid\u201d in the sense that they contain a value function $\\big(V_{h}^{\\tilde{\\pi}}\\big)$ corresponding to a near-optimal stochastic policy $\\tilde{\\pi}$ in the benchmark class $\\Pi_{4\\varepsilon}$ . In the sequel, we use this fact to substitute $V_{\\ell}^{\\tilde{\\pi}}$ for $f_{\\ell}$ in Eq. (26) and bound the suboptimality of the learned policy $\\widehat{\\pi}$ . ", "page_idx": 46}, {"type": "text", "text": "Lemma I.4 (Confidence sets). Let $\\varepsilon,\\delta\\in(0,1)$ be given and suppose that Assumption 4.1 (pushforward coverability) holds with parameter $C_{\\mathsf{p u s h}}>0$ . Let $f\\in\\mathcal{V}$ be arbitrary, and suppose that $\\nu$ satisfies Assumption I.1 with $\\varepsilon_{\\mathsf{r e a l}}~=~4\\varepsilon$ . Then, there is an event $\\mathcal{E}^{\\prime\\prime\\prime}$ of probability at least $1-3\\delta$ under which a call to $\\mathsf{R V F S}_{0}(f,\\mathcal{V},\\mathcal{D},\\otimes,0;\\mathcal{V},\\varepsilon,\\delta)$ (Algorithm 5) terminates and returns tuple $(\\widehat{V}_{1:H},\\widehat{\\mathcal{V}}_{1:H},\\mathcal{C}_{1:H},\\mathcal{B}_{1:H},t_{1:H})$ such that ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\forall h\\in[H],\\quad V_{h}^{\\tilde{\\pi}}\\in\\widehat{\\mathcal{V}}_{h},\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where $\\tilde{\\pi}_{1:H}\\in\\Pi_{\\mathcal{S}}$ is the stochastic policy defined recursively via ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\iota_{X}\\in\\mathcal{X},\\ \\tilde{\\pi}_{\\tau}(x)\\in\\underset{a\\in\\mathcal{A}}{\\arg\\operatorname*{max}}\\left\\{\\begin{array}{l l}{\\widehat{Q}_{\\tau}(x,a),}&{i f\\lVert\\widehat{Q}_{\\tau}(x,\\cdot)-\\mathcal{P}_{\\tau}[V_{\\tau+1}^{\\tilde{\\pi}}](x,\\cdot)\\rVert_{\\infty}\\leq4\\varepsilon,}\\\\ {\\mathcal{P}_{\\tau}[V_{\\tau+1}^{\\tilde{\\pi}}](x,a),}&{o t h e r w i s e,}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where $\\widehat{\\pmb{Q}}_{\\tau}(x,a):=\\widehat{\\pmb{{\\mathscr{P}}}}_{\\tau,\\varepsilon,\\delta^{\\prime}}[\\widehat{V}_{\\tau+1}](x,a)$ is a realization of the stochastic output of the $\\widehat{\\mathcal{P}}$ operator in Algorithm 7 given input $(x,a)$ , and $\\delta^{\\prime}$ is as in Algorithm 5. Furtherore, we have $\\tilde{\\pi}\\in\\Pi_{4\\varepsilon}$ . ", "page_idx": 46}, {"type": "text", "text": "The proof of the lemma is in Appendix I.5. ", "page_idx": 46}, {"type": "text", "text": "Equipped with the preceding lemmas, we now state the main technical result of this section, Theorem I.1, a generalization of Theorem 4.1 which holds under relaxed $V^{\\pi}$ -realizability (Assumption I.1). The proof is in Appendix I.6. ", "page_idx": 46}, {"type": "text", "text": "Theorem I.1 (Guarantee for RVFS under relaxed $V^{\\pi}$ -realizability). Let $\\delta,\\varepsilon\\in(0,1)$ be given, and suppose that Assumption 4.1 (pushforward coverability) holds with parameter $C_{\\mathsf{p u s h}}>0$ . Let $f\\in\\mathcal{V}$ be arbitrary, and assume that $\\nu$ that satisfies Assumption I.1 with $\\varepsilon_{\\mathsf{r e a l}}=4\\varepsilon$ . Then, with probability at least $1-5\\delta$ , $\\mathsf{R V F S}_{0}(f,\\mathcal{V},\\mathcal{D},\\otimes,0;\\mathcal{V},\\varepsilon,\\delta)$ (Algorithm $^{5}$ ) terminates and returns value functions $\\widehat{V}_{1:H}$ that satisfy ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\forall h\\in[H],\\quad\\mathbb{E}^{\\widehat\\pi}[D_{\\mathrm{tv}}(\\widehat\\pi_{h}(x_{h}),\\widetilde\\pi_{h}(x_{h}))]\\leq\\frac{\\varepsilon}{4H^{3}C_{\\mathrm{push}}},\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "where $\\widehat{\\pi}_{h}(x)\\;\\in\\;\\arg\\operatorname*{max}_{a\\in\\cal A}\\widehat{\\mathcal{P}}_{h,\\varepsilon,\\delta^{\\prime}}[\\widehat{V}_{h+1}](x,a)$ for all $h\\ \\in\\ [H],$ , with $\\tilde{\\pi}\\ \\in\\ \\Pi_{4\\varepsilon}$ defined as in Lemma I.4 and $\\delta^{\\prime}$ defined as Pin Algorithm $^{5}$ . Furthermore, the number of episodes is bounded by ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\widetilde{\\cal O}(C_{\\mathrm{push}}^{8}H^{10}A\\cdot\\varepsilon^{-13}).\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Next, we state a guarantee for the outer-level algorithm, RVFS.bc, under relaxed $V^{\\pi}$ -realizability. Recall that RVFS.bc invokes $\\mathsf{R V F S}_{\\mathrm{0}}$ , then extracts an executable policy by applying the BehaviorCloning algorithm (see Appendix M), with the \u201cexpert\u201d policy set to be the output of RVFS. ", "page_idx": 47}, {"type": "text", "text": "Theorem I.2 (Main guarantee of RVFS.bc). Let $\\delta,\\varepsilon\\in(0,1)$ be given, and define $\\varepsilon_{\\mathsf{R V F S}}=\\varepsilon H^{-1}/48$ . Suppose that ", "page_idx": 47}, {"type": "text", "text": "\u2022 Assumption 4.1 (pushforward coverability) holds with parameter $C_{\\mathsf{p u s h}}>0$ ; \u2022 the function class $\\nu$ satisfies Assumption I.1 with $\\varepsilon_{\\mathsf{r e a l}}=1$ (i.e. all $\\pi$ -realizability); and \u2022 the policy class \u03a0 satisfies Assumption 4.3. ", "page_idx": 47}, {"type": "text", "text": "Then, with probability at least $1-\\delta$ , \u03c0\u03021\u2236H = RVFS.bc $(\\Pi,\\nu,\\varepsilon,\\delta)$ (Algorithm 6) satisfies ", "page_idx": 47}, {"type": "equation", "text": "$$\nJ(\\pi^{\\star})-J(\\widehat{\\pi}_{1:H})\\leq\\varepsilon.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Furthermore, the total number sample complexity in the RLLS framework is bounded by ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\widetilde{\\cal O}\\left(C_{\\mathrm{push}}^{8}H^{23}A\\epsilon^{-13}\\right).\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "The proof is in Appendix I.7. Note that Theorem I.2 is a restatement of Theorem 4.1 in Setup II (restated for convenience). As a result, Theorem 4.1 is an immediate corollary. ", "page_idx": 47}, {"type": "text", "text": "Proof of Theorem 4.1. The result follows from Theorem I.2, since Assumption 4.5 is stronger than Assumption I.1. \u53e3 ", "page_idx": 47}, {"type": "text", "text": "I.2 Proof of Lemma I.1 (Number of Test Failures) ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Proof of Lemma I.1. Fix $h\\in[H]$ . We note that the size of $\\ensuremath{\\mathcal{C}}_{h}$ corresponds to the number of times the test in Line 14 fails for $\\ell=h$ throughout the execution of $\\mathsf{R V F S}_{0}(f,\\mathcal{V},\\emptyset,\\emptyset;\\mathcal{V},\\varepsilon,\\delta)$ . ", "page_idx": 47}, {"type": "text", "text": "Let $M:=\\lceil8\\varepsilon^{-1}C_{\\mathsf{p u s h}}H\\rceil$ denote the desired upper bound on $|\\mathcal{C}_{h}|$ . Suppose that the test in Line 14 fails at least twice for $\\ell=h$ (if the test fails at most twice, then $|{\\mathcal{C}}_{h}|\\le2$ and so (25) holds for $\\ell=h$ trivially), and let ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\big(x_{h-1}^{(1)},a_{h-1}^{(1)},\\widehat{V}_{h}^{(1)},\\widehat{V}_{h}^{(1)},t_{h}^{(1)}\\big),\\big(x_{h-1}^{(2)},a_{h-1}^{(2)},\\widehat{V}_{h}^{(2)},\\widehat{V}_{h}^{(2)},t_{h}^{(2)}\\big),\\dots\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "denote the elements of the set $\\boldsymbol{B}_{h}$ in the order at which they are added to the latter in Line 15 of Algorithm 5. Note that $\\left|\\boldsymbol{B}_{h}\\right|=\\left|\\boldsymbol{\\mathcal{C}}_{h}\\right|$ . Note also that $t_{h}^{\\left(i\\right)}$ represents the number of times the subroutine $\\widehat{\\mathcal{P}}_{h-1,\\varepsilon,\\delta^{\\prime}}$ has been called in the test of Line 14 throughout the execution of $\\mathsf{R V F S}_{\\mathrm{0}}$ and up to the time the test failed for $(x_{h-1}^{(i)},a_{h-1}^{(i)})$ . We will use this fact in a concentration argument in the sequel. ", "page_idx": 47}, {"type": "text", "text": "By definition of $(\\widehat{\\mathcal{V}}_{h}^{(i)})$ and Lemma C.4 (Freedman\u2019s inequality) instantiated with ", "page_idx": 47}, {"type": "text", "text": "$\\begin{array}{r l}{\\mathbf{\\bullet}\\,\\,\\mathcal{Q}=\\big\\{\\widehat{V}_{h}^{(i)}-f_{h}:f\\in\\widehat{\\mathcal{V}}_{h}^{(i)}\\big\\};}\\\\ {\\mathbf{\\bullet}\\,\\,\\pmb{y}_{h}=\\mathbf{x}_{h};}\\end{array}$ \u2022 $B=H$ ; and ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\cdot\\ n=N_{\\sf r e g}\\cdot i;\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "and the union bound over $i\\,\\in\\,\\bigl[M\\wedge|{\\mathcal{C}}_{h}|\\bigr]$ , we get that there is an event $\\mathcal{E}_{h}$ of probability at least $1-\\delta/(2H)$ under which ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\forall i\\in[M\\land|\\mathcal{C}_{h}|],\\forall f\\in\\widehat{\\mathcal{V}}_{h}^{(i)},}&{\\ \\sum_{j<i}\\mathbb{E}\\big[(\\widehat{V}_{h}^{(i)}(x_{h})-f_{h}(x_{h}))^{2}\\mid x_{h-1}=x_{h-1}^{(j)},a_{h-1}=a_{h-1}^{(j)}\\big]}\\\\ &{\\ \\leq\\tilde{\\varepsilon}_{r e g}^{2}:=2\\varepsilon_{r e g}^{2}+\\frac{4H^{2}\\log\\left(4M H\\vert\\mathcal{V}\\vert/\\delta\\right)}{N_{r e g}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Now, define $\\begin{array}{r}{f_{h}^{(i)}\\in\\arg\\operatorname*{max}_{f\\in\\widehat{\\mathcal{V}}_{h}^{(i)}}\\big|\\mathbb{E}\\left[\\widehat{V}_{h}^{(i)}\\big({\\pmb x}_{h}\\big)-f_{h}\\big({\\pmb x}_{h}\\big)\\;\\big|\\;{\\pmb x}_{h-1}=x_{h-1}^{(i)},{\\pmb a}_{h-1}={\\pmb a}_{h-1}^{(i)}\\right]\\big|.}\\end{array}$ From (29), we have that under $\\mathcal{E}_{h}$ : ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\forall i\\in\\big[M\\land|{\\mathcal C}_{h}|\\big],\\quad\\sum_{j<i}\\mathbb{E}\\big[(\\widehat V_{h}^{(i)}({\\pmb x}_{h})-f_{h}^{(i)}({\\pmb x}_{h}))^{2}\\mid{\\pmb x}_{h-1}=x_{h-1}^{(j)},{\\pmb a}_{h-1}={\\pmb a}_{h-1}^{(j)}\\big]\\leq\\widetilde{\\varepsilon}_{\\mathrm{reg}}^{2}.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "We now use this to bound the number of times the test in Line 14 fails for $\\ell=h$ . Suppose for the sake of contradiction that the test fails at least $N$ times for some $N\\geq M$ (i.e. $|{\\mathcal{C}}_{h}|=N\\geq M)$ . Conditioned on $\\mathcal{E}_{h}$ , we have by Lemma C.8 and Eq. (30), ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{i\\in[M]}{\\operatorname*{min}}\\ \\underset{f\\in\\widehat{\\mathcal{V}}_{h}^{(i)}}{\\operatorname*{sup}}\\left|\\mathbb{E}\\left[\\widehat{V}_{h}^{(i)}\\big({\\pmb x}_{h}\\big)-f_{h}\\big({\\pmb x}_{h}\\big)\\mid{\\pmb x}_{h-1}=x_{h-1}^{(i)},{\\pmb a}_{h-1}=a_{h-1}^{(i)}\\right]\\right|,}\\\\ &{=\\underset{i\\in[M]}{\\operatorname*{min}}\\left|\\mathbb{E}\\left[\\widehat{V}_{h}^{(i)}\\big({\\pmb x}_{h}\\big)-f_{h}^{(i)}\\big({\\pmb x}_{h}\\big)\\mid{\\pmb x}_{h-1}=x_{h-1}^{(i)},{\\pmb a}_{h-1}=a_{h-1}^{(i)}\\right]\\right|,}\\\\ &{\\leq2\\left(\\frac{C_{\\mathrm{push}}}{M^{2}}M\\tilde{\\varepsilon}_{\\mathrm{reg}}^{2}\\log(2M)\\right)^{1/2}+\\frac{2C_{\\mathrm{push}}H}{M}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "rNitohwm,  s5,u bwseti tguetting the expression of $\\tilde{\\varepsilon}_{r\\mathrm{eg}}^{2}$ in (29) and using the definition of $\\varepsilon_{\\mathrm{reg}}^{2}$ in Line 6 of Algo", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=2\\left(\\frac{C_{\\mathrm{push}}}{M}\\cdot\\left(2\\varepsilon_{\\mathrm{reg}}^{2}+\\frac{4M H^{2}\\log(4M H|\\mathcal{V}|/\\delta)}{N_{\\mathrm{reg}}}\\right)\\right)^{1/2}+\\frac{2C_{\\mathrm{push}}H}{M},}\\\\ &{\\leq2\\left(\\frac{C_{\\mathrm{push}}}{M}\\cdot\\left(\\frac{22M H^{2}\\log(8M^{2}H|\\mathcal{V}|^{2}/\\delta)}{N_{\\mathrm{reg}}}+\\frac{68M H^{3}\\log(8M^{6}N_{\\mathrm{test}}^{2}H^{8}/\\delta)}{N_{\\mathrm{test}}}\\right)\\right)^{1/2}+\\frac{2C_{\\mathrm{push}}H}{M},}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "\u2264\u03b5, ", "page_idx": 48}, {"type": "text", "text": "where the last inequality uses that $M=\\left\\lceil8\\varepsilon^{-1}C_{\\mathsf{p u s h}}H\\right\\rceil$ and ", "page_idx": 48}, {"type": "equation", "text": "$$\nN_{\\mathrm{reg}}=2^{8}M^{2}\\varepsilon^{-1}\\log(8|\\mathcal{V}|^{2}H M^{2}\\delta^{-1})\\;\\mathrm{~and~}\\;N_{\\mathrm{test}}=2^{8}M^{2}H\\varepsilon^{-1}\\log(8M^{6}H^{8}\\varepsilon^{-2}\\delta^{-1});\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "see Line 5 of Algorithm 5. ", "page_idx": 48}, {"type": "text", "text": "On the other hand, by Lemma H.2, there is an event $\\mathcal{E}_{h}^{\\prime}$ of probability at least $1-\\delta/(2M H)$ under which for all $f\\in\\mathcal{V}$ , all $i\\in[M]$ , and $\\delta^{\\prime}$ as in Algorithm 5: ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{|\\mathcal{P}_{h-1,\\varepsilon,\\delta^{\\prime}}[\\widehat{V}_{h}^{(i)}](x_{h-1}^{(i)},a_{h-1}^{(i)})-\\widehat{\\mathcal{P}}_{h-1,\\varepsilon,\\delta^{\\prime}}[f_{h}](x_{h-1}^{(i)},a_{h-1}^{(i)})|}\\\\ &{\\leq\\big|\\mathcal{P}_{h-1}[\\widehat{V}_{h}^{(i)}-f_{h}](x_{h-1}^{(i)},a_{h-1}^{(i)})\\big|+\\varepsilon\\cdot\\sqrt{2\\log_{1/\\delta^{\\prime}}(4M A H|\\mathcal{V}|(t_{h}^{(i)})^{2}/\\delta)},}\\\\ &{=\\big|\\mathcal{P}_{h-1}[\\widehat{V}_{h}^{(i)}-f_{h}](x_{h-1}^{(i)},a_{h-1}^{(i)})\\big|+\\varepsilon\\cdot\\beta(t_{h}^{(i)}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where $\\beta\\big(t_{h}^{(i)}\\big)$ is as in Algorithm 5. Thus, under $\\mathcal{E}_{h}^{\\prime}$ , the test in Line 14 fails for $\\ell=h$ at least $M$ times only if ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\forall i\\in[M],\\quad\\varepsilon<\\operatorname*{sup}_{f\\in\\hat{\\mathcal{V}}_{h}^{(i)}}\\big|(\\widehat{\\mathcal{P}}_{h-1,\\varepsilon,\\delta^{\\prime}}[\\widehat{V}_{h}^{(i)}]-\\widehat{\\mathcal{P}}_{h-1,\\varepsilon,\\delta^{\\prime}}[f_{h}])\\big(x_{h-1}^{(i)},a_{h-1}^{(i)}\\big)\\big|}\\\\ &{\\qquad-\\varepsilon\\cdot\\beta(t_{h}^{(i)}),}\\\\ &{\\leq\\operatorname*{sup}_{f\\in\\hat{\\mathcal{V}}_{h}^{(i)}}\\big|\\mathbb{E}\\big[\\widehat{V}_{h}^{(i)}(x_{h})-f_{h}\\big(x_{h}\\big)\\;\\big|\\;x_{h-1}=x_{h-1}^{(i)},a_{h-1}=a_{h-1}^{(i)}\\big]\\big|\\quad(\\mathrm{by}\\;(32)),}\\\\ &{<\\operatorname*{sup}_{f\\in\\hat{\\mathcal{V}}_{h}^{(i)}}\\big|\\mathbb{E}\\big[\\widehat{V}_{h}^{(i)}(x_{h})-f_{h}\\big(x_{h}\\big)\\;\\big|\\;x_{h-1}=x_{h-1}^{(i)},a_{h-1}=a_{h-1}^{(i)}\\big]\\big|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Unless $N<M$ , this is a contradiction to Eq. (31). We conclude that under the event $\\mathcal{E}_{h}\\cap\\mathcal{E}_{h}^{\\prime}$ , the test in Line 14 fails at most $N<M=\\lceil8\\varepsilon^{-1}C_{\\tt p u s h}H\\rceil$ times for $\\ell=h$ , and so under $\\mathcal{E}_{1}\\cap\\mathcal{E}_{1}^{\\prime}\\cap\\cdots\\cap\\mathcal{E}_{H}\\cap\\mathcal{E}_{H}$ , we have ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\forall h\\in[H],\\quad|\\mathcal{C}_{h}|\\leq\\lceil8\\varepsilon^{-1}C_{\\mathsf{p u s h}}H\\rceil.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "By the union bound, we have $\\mathbb{P}[\\mathcal{E}_{1}\\cap\\mathcal{E}_{1}^{\\prime}\\cap\\cdots\\cap\\mathcal{E}_{H}\\cap\\mathcal{E}_{H}^{\\prime}]\\geq1-\\delta$ , which completes the proof. ", "page_idx": 49}, {"type": "text", "text": "I.3 Proof of Lemma I.2 (Consequence of Passing the Tests) ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Proof of Lemma I.2. Let $h\\in[H]$ be given. Fix $\\ell\\in[h+1\\ldots H]$ and let $\\pmb{x}_{\\ell-1}^{(1)},\\pmb{x}_{\\ell-1}^{(2)},\\dots$ denote the sequence of states used in the tests of Line 14 throughout the execution of $\\mathsf{R V F S}_{\\mathrm{0}}$ ; we assume that the sequence is ordered in the sense that if $i<j$ , then $\\pmb{x}_{\\ell-1}^{\\bar{(}i)}$ is used in the test of Line 14 before ${\\pmb x}_{\\ell-1}^{(j)}$ . Let $\\begin{array}{r}{T_{\\ell}\\in\\mathbb{N}\\cup\\{+\\infty\\}}\\end{array}$ be the random variable representing the total number of times the operator $\\widehat{\\mathcal{P}}_{\\ell-1,\\varepsilon,\\delta^{\\prime}}$ is invoked in Line 14 throughout the execution of $\\mathsf{R V F S}_{\\mathrm{0}}$ ( ${\\bf\\nabla}T_{\\ell}$ is also the random length of the sPequence x(\u21131\u2212)1,x(\u21132\u2212)1,...; if RVFS0 terminates, then T\u2113is finite. The first step of the proof will be to show that under the event $\\mathcal{E}$ of Lemma I.1, ${\\mathbf{}}T_{\\ell}$ is no larger than $M^{3}N_{\\mathrm{test}}H^{3}$ at any point during the execution of $\\mathsf{R V F S}_{\\mathrm{0}}$ . This will help us establish key concentration results, leading to the desired inequality (26). ", "page_idx": 49}, {"type": "text", "text": "Bounding ${\\mathbf{}}T_{\\ell}$ under $\\mathcal{E}$ . First, note that under the event $\\mathcal{E}$ of Lemma I.1, we have that for any $\\tau\\in[H]$ , ", "page_idx": 49}, {"type": "equation", "text": "$$\n|\\mathcal{C}_{\\tau}|\\leq M:=\\lceil8\\varepsilon^{-1}C_{\\mathsf{p u s h}}H\\rceil,\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "and so $\\mathsf{R V F S}_{\\tau}$ gets called at most $M$ times throughout the execution of $\\mathsf{R V F S}_{\\mathrm{0}}$ . For the rest of this paragraph, we condition on $\\mathcal{E}$ and fix $\\tau\\in[0\\ldots H]$ . Within any given call to $\\mathsf{R V F S}_{\\tau}$ (throughout the execution of $\\mathsf{R V F S}_{\\mathrm{0}}$ ), the operator $\\widehat{\\mathcal{P}}_{\\ell-1,\\varepsilon,\\delta^{\\prime}}$ is invoked at most ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\mathcal{C}_{\\tau}|N_{\\mathrm{test}}H\\qquad\\qquad\\times\\qquad\\qquad\\qquad\\underbrace{H M}_{\\qquad}\\qquad\\qquad\\qquad\\leq M^{2}N_{\\mathrm{test}}H^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "times. This is because the for-loop in Line 8 of $\\mathsf{R V F S}_{\\tau}$ resumes whenever a test in Line 14 fails for one of the layers $\\tau+1,\\dots,H$ (see Line 18) once the recursive calls return, and the total number of test failures across all these layers is bounded by $H M$ (by (33)). Now, since $\\mathsf{R V F S}_{\\tau}$ gets called at most $M$ times throughout the execution of $\\mathsf{R V F S}_{\\mathrm{0}}$ (as argued in the prequel), the total number of times the operator $\\widehat{\\mathcal{P}}_{\\ell-1,\\varepsilon,\\delta^{\\prime}}$ is invoked in Line 14 within $\\mathsf{R V F S}_{\\tau}$ is at most ", "page_idx": 49}, {"type": "equation", "text": "$$\nM^{3}N_{\\mathrm{test}}H^{2}.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Finally, the total number of times the operator $\\widehat{\\mathcal{P}}_{\\ell-1,\\varepsilon,\\delta^{\\prime}}$ is called in Line 14 throughout the execution of $\\mathsf{R V F S}_{\\mathrm{0}}$ is at most $H$ times larger (accounting for the contributions from $\\mathsf{R V F S}_{\\tau}$ for all $\\tau\\in[H])$ ; that is, it is at most $M^{3}N_{\\mathrm{test}}H^{3}$ . We conclude that the random variable ${\\mathbf{}}T_{\\ell}$ satisfies ", "page_idx": 49}, {"type": "equation", "text": "$$\nT_{\\ell}\\le M^{3}N_{\\mathrm{test}}H^{3}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "under $\\mathcal{E}$ ", "page_idx": 49}, {"type": "text", "text": "Specifying $\\mathcal{E}_{h}^{\\prime}$ . In this paragraph, we no longer condition on $\\mathcal{E}$ . We will specify the event $\\mathcal{E}_{h}^{\\prime}$ in the lemma statement. Let $\\delta^{\\prime}$ be defined as in Algorithm 5. By Lemma H.2, we have that there is an event $\\mathcal{E}_{h,\\ell}^{\\prime}$ of probability at least $1-\\delta/(2H^{2})$ under which: ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\emph{\\prime i}\\in[T_{\\ell}],\\forall a_{\\ell-1}\\in A:\\operatorname*{sup}_{i}|(\\widehat{\\mathcal{P}}_{\\ell-1,\\varepsilon,\\delta^{\\prime}}[\\widehat{V}_{\\ell}]-\\widehat{\\mathcal{P}}_{\\ell-1,\\varepsilon,\\delta^{\\prime}}[f_{\\ell}])(x_{\\ell-1}^{(i)},a_{\\ell-1})|-\\varepsilon-\\varepsilon\\cdot\\beta(T_{\\ell})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\times}\\\\ &{\\quad=\\operatorname*{sup}_{i}|(\\widehat{\\mathcal{P}}_{\\ell-1,\\varepsilon,\\delta^{\\prime}}[\\widehat{V}_{\\ell}]-\\widehat{\\mathcal{P}}_{\\ell-1,\\varepsilon,\\delta^{\\prime}}[f_{\\ell}])(x_{\\ell-1}^{(i)},a_{\\ell-1})|-\\varepsilon-\\varepsilon\\cdot\\sqrt{2\\log_{1/\\delta^{\\prime}}(8A H^{2})}}\\\\ &{\\quad\\xrightarrow[f\\in\\hat{V}]{\\textnormal{z u p}}|(\\mathcal{P}_{\\ell-1}[\\widehat{V}_{\\ell}]-\\mathcal{P}_{\\ell-1}[f_{\\ell}])(x_{\\ell-1}^{(i)},a_{\\ell-1})|-\\varepsilon-\\varepsilon\\cdot\\sqrt{2\\log_{1/\\delta^{\\prime}}(8A H^{2}M|\\mathcal{V}|T_{\\ell})}}\\\\ &{\\qquad\\quad\\geq\\varepsilon\\cdot\\sqrt{2\\log_{1/\\delta^{\\prime}}(4A H^{2}M|\\mathcal{V}|^{2}/\\delta)},\\quad\\mathrm{(Lemma~H.2)}}\\\\ &{\\qquad\\quad\\geq\\operatorname*{sup}|(\\mathcal{P}_{\\ell-1}[\\widehat{V}_{\\ell}]-\\mathcal{P}_{\\ell-1}[f_{\\ell}])(x_{\\ell-1}^{(i)},a_{\\ell-1})|-\\varepsilon-2\\varepsilon\\cdot\\sqrt{2\\log_{1/\\delta^{\\prime}}(8A H^{2}M|\\mathcal{V}|^{2}}}\\\\ &{\\quad\\forall\\ell\\widehat{V}_{\\ell}}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "On the other hand, for $k\\ \\in\\ [T_{\\ell}\\mathrm{~-~}N_{\\mathrm{test}}+1]$ , we have by Lemma C.4 (Freedman\u2019s inequality) instantiated with ", "page_idx": 49}, {"type": "text", "text": "\u2022 $~n~=~N_{\\mathrm{test}}$ and $\\pmb{\\mathscr{y}}_{i}\\;=\\;\\mathbb{I}\\left\\{\\operatorname*{sup}_{f\\in\\widehat{\\mathscr{V}}_{\\ell}}\\operatorname*{max}_{a\\in\\pmb{\\mathscr{A}}}\\left|\\left(\\mathcal{P}_{\\ell-1}[\\widehat{V}_{\\ell}]-\\mathcal{P}_{\\ell-1}\\right)[f_{\\ell}](\\pmb{x}_{\\ell-1}^{(k+i)},a)\\right|>3\\varepsilon\\right\\}$ , for all $i\\in\\left[N_{\\mathrm{test}}\\right]$ ;   \n\u2022 $\\mathcal{Q}=\\{\\mathrm{id}\\}$ ;   \n\u2022 $B=1$ ; and   \n\u2022 \u03bb = 1; ", "page_idx": 50}, {"type": "text", "text": "that there is an event $\\mathcal{E}_{h,\\ell,k}^{\\prime\\prime}$ of probability at least $1-\\delta/(4k^{2}H^{2})$ under which ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{0\\leq i<N_{\\mathrm{test}}}\\mathbb{P}\\Bigg[\\underset{f\\in\\hat{\\mathcal{V}}_{\\ell}}{\\operatorname*{sup}}\\operatorname*{max}\\left|\\big(\\mathcal{P}_{\\ell-1}[\\widehat{V}_{\\ell}]-\\mathcal{P}_{\\ell-1}[f_{\\ell}]\\big)(\\pmb{x}_{\\ell-1}^{(k+i)},a)\\right|>3\\varepsilon\\Bigg]}\\\\ &{\\leq4\\log(8H^{2}T_{\\ell}^{2}/\\delta)+\\displaystyle\\sum_{0\\leq i<N_{\\mathrm{test}}}\\mathbb{I}\\left\\{\\underset{f\\in\\hat{\\mathcal{V}}_{\\ell},a\\in A}{\\operatorname*{sup}}\\vert\\big(\\mathcal{P}_{\\ell-1}[\\widehat{V}_{\\ell}]-\\mathcal{P}_{\\ell-1}[f_{\\ell}]\\big)(\\pmb{x}_{\\ell-1}^{(k+i)},a)\\vert>3\\epsilon\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Now, let $\\begin{array}{r}{\\mathcal{E}_{h,\\ell}^{\\prime\\prime}:=\\bigcap_{k\\in[T_{\\ell}-N_{\\mathrm{test}}+1]}\\mathcal{E}_{h,\\ell,k}^{\\prime\\prime}}\\end{array}$ . By the union bound and the fact that $\\Sigma_{k\\geq1}\\,1/k^{2}=\\pi^{2}/6\\leq2$ , we have that $\\mathbb{P}[\\mathcal{E}_{h,\\ell}^{\\prime\\prime}]\\geq1-\\delta/(2H^{2})$ . Furthermore, under $\\mathcal{E}_{h,\\ell}^{\\prime\\prime}$ , we have ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall k\\in[T_{\\ell}-N_{\\mathrm{test}}+1],}\\\\ &{\\underbrace{\\sum_{\\ell=1<N_{\\mathrm{test}}}\\mathbb{P}\\Bigg[\\underset{f\\in\\hat{\\mathcal{V}}_{\\ell}}{\\operatorname*{sup}}\\underset{a\\in\\mathcal{A}}{\\operatorname*{max}}\\left|(\\mathcal{P}_{\\ell-1}[\\widehat{V}_{\\ell}]-\\mathcal{P}_{\\ell-1}[f_{\\ell}])(x_{\\ell-1}^{(k+i)},a)\\right|>3\\varepsilon\\Bigg]}_{\\odot:i<N_{\\mathrm{test}}}\\Bigg]}\\\\ &{\\leq4\\log(8H^{2}T_{\\ell}^{2}/\\delta)+\\underset{0\\leq i<N_{\\mathrm{test}}}{\\sum_{\\ell=i<N_{\\mathrm{test}}}}\\mathbb{I}\\left\\{\\underset{f\\in\\hat{\\mathcal{V}}_{\\ell},a\\in\\mathcal{A}}{\\operatorname*{sup}}\\left|(\\mathcal{P}_{\\ell-1}[\\widehat{V}_{\\ell}]-\\mathcal{P}_{\\ell-1}[f_{\\ell}])(x_{\\ell-1}^{(k+i)},a)\\right|>3\\varepsilon\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "We define $\\mathcal{E}_{h}^{\\prime}:=\\mathcal{E}_{h,1}^{\\prime}\\cap\\mathcal{E}_{h,1}^{\\prime\\prime}\\cap\\cdots\\cap\\mathcal{E}_{h,H}^{\\prime}\\cap\\mathcal{E}_{h,H}^{\\prime\\prime}.$ . Note that by the union bound, we have $\\begin{array}{r}{\\mathbb{P}[\\mathcal{E}_{h}^{\\prime}]\\ge1-\\frac{\\delta}{H}}\\end{array}$ as desired. ", "page_idx": 50}, {"type": "text", "text": "Termination of $\\mathsf{R V F S}_{h}$ under $\\mathcal{E}\\cap\\mathcal{E}_{h}^{\\prime}$ . We now show that under $\\mathcal{E}\\cap\\mathcal{E}_{h}^{\\prime}$ , if ${\\mathsf{R V F S}}_{h}$ terminates, its output satisfies (26). For the rest of the proof, we condition on $\\mathcal{E}\\cap\\mathcal{E}_{h}^{\\prime}$ . Suppose that $\\mathsf{R V F S}_{h}$ terminates and returns $(\\widehat{V}_{h:H},\\widehat{\\mathcal{V}}_{h:H},\\mathcal{C}_{h:H},\\mathcal{B}_{h:H},t_{h:H})$ . In this case, the value function $\\widehat{V}_{\\ell}$ must have passed the tests in Line 14 for all $(x_{h-1},a_{h-1})\\in{\\mathcal{C}}_{h}$ , $n\\in N_{\\mathrm{test}}$ , and $a_{\\ell-1}\\in A$ . Fix $(x_{h-1},a_{h-1})\\in{\\mathcal{C}}_{h}$ and let $k\\in[T_{\\ell}-N_{\\mathrm{test}}\\cdot A+1]$ be such that $(\\pmb{x}_{\\ell-1}^{k+j})_{j\\in[0,..N_{\\mathrm{test}}-1]}$ represents a subsequence of states that pass the tests in Line 14 at layer $\\ell$ for $(x_{h-1},a_{h-1})$ within the call to $\\mathsf{R V F S}_{h}$ . The fact that the sequence $(\\mathbf{x}_{\\ell-1}^{(i)})_{i\\geq1}$ is ordered (see definition in the first paragraph of this proof) and that $(\\pmb{x}_{\\ell-1}^{k+j})_{j\\in[0,..N_{\\mathrm{test}}-1]}$ pass the tests imply that ", "page_idx": 50}, {"type": "text", "text": "1. The states $(\\pmb{x}_{\\ell-1}^{(k+i)})_{i\\in[0,..N_{\\mathrm{test}}-1]}$ at layer $\\ell-1$ are i.i.d., and are obtained by rolling out with $\\widehat{\\pi}_{h:H}$ starting from $(x_{h-1},a_{h-1})$ ; and ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\forall j\\in\\big[0\\ldots N_{\\mathrm{test}}-1\\big],\\forall a_{\\ell-1}\\in\\mathcal{A},}&{\\operatorname*{sup}_{I^{(\\widehat{\\mathcal{P}}_{\\ell-1,\\varepsilon,\\delta^{\\prime}}[\\widehat{V}_{\\ell}]-\\widehat{\\mathcal{P}}_{\\ell-1,\\varepsilon,\\delta^{\\prime}}[f_{\\ell}])(x_{\\ell-1}^{(k+j)},a_{\\ell-1})}\\big|}\\\\ &{f\\epsilon\\widehat{\\nu}_{\\ell}}\\\\ &{\\leq\\varepsilon+\\varepsilon\\cdot\\beta(k+j),}\\\\ &{\\leq\\varepsilon+\\varepsilon\\cdot\\sqrt{2\\log_{1/\\delta^{\\prime}}(8A M|\\mathcal{V}|(k+j)^{2}/\\delta)},}\\\\ &{\\leq\\varepsilon+\\varepsilon\\cdot\\sqrt{2\\log_{1/\\delta^{\\prime}}(8A M|\\mathcal{V}|T_{\\ell}^{2}/\\delta)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "This implies that ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall i\\in[0\\,.\\,N_{\\mathrm{test}}-1],\\forall a_{\\ell-1}\\in A:}\\\\ &{\\operatorname*{sup}_{j\\in\\hat{V}_{\\ell}}[\\mathcal{\\hat{V}}_{\\ell-1}[\\mathcal{\\hat{V}}_{\\ell}-f_{\\ell}](x_{\\ell-1}^{(k+i)},a_{\\ell-1})|-3\\varepsilon}\\\\ &{\\le\\operatorname*{sup}_{j\\in\\hat{V}_{\\ell}}\\big|\\mathcal{P}_{\\ell-1}[\\widehat{V}_{\\ell}-f_{\\ell}](x_{\\ell-1}^{(k+i)},a_{\\ell-1})\\big|-\\varepsilon-2\\varepsilon\\cdot\\sqrt{2\\log_{1/\\delta^{\\prime}}(4A M H^{2}|\\mathcal{V}|T_{\\ell}^{2}\\delta)},}\\\\ &{\\overset{(3\\mathrm{1)}}{\\le}\\operatorname*{sup}_{j\\in\\hat{V}_{\\ell}}|(\\widehat{\\mathcal{P}}_{\\ell-1,\\varepsilon,\\delta^{\\prime}}[\\widehat{V}_{\\ell}]-\\widehat{\\mathcal{P}}_{\\ell-1,\\varepsilon,\\delta^{\\prime}}[f_{\\ell}])(x_{\\ell-1}^{(k+i)},a_{\\ell-1})|-\\varepsilon-\\varepsilon\\cdot\\sqrt{2\\log_{1/\\delta^{\\prime}}(4A M H^{2}|\\mathcal{V}|T_{\\ell}^{2}/\\delta)},}\\\\ &{\\overset{f\\in\\hat{V}_{\\ell}}{\\le}(\\mathrm{by~Item~2~})}\\\\ &{\\le0.\\quad(\\mathrm{by~Item~2~})}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "where (37) follows by (34) and the choice of $\\delta^{\\prime}$ in Algorithm 5. ", "page_idx": 51}, {"type": "text", "text": "Now, by Item 1, we have that ${\\pmb x}_{\\ell-1}^{(k+i)}$ has probability law $\\mathbb{P}^{\\widehat{\\pi}_{h:H}}\\left[\\cdot\\mid\\pmb{x}_{h-1}=x_{h-1},\\pmb{a}_{h-1}=a_{h-1}\\right]$ for all $i\\in\\left[0\\dots N_{\\mathrm{test}}-1\\right]$ , and so by (36), we have: ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}^{\\widehat{\\pi}}\\Bigg[\\underset{f\\in\\widehat{\\mathcal{V}}_{\\ell}}{\\operatorname*{sup}}\\underset{a\\in A}{\\operatorname*{max}}\\left|(\\mathcal{P}_{\\ell-1}[\\widehat{V}_{\\ell}]-\\mathcal{P}_{\\ell-1}[f_{\\ell}])(x_{\\ell-1},a)\\right|>3\\varepsilon\\mid x_{h-1}=x_{h-1},a_{h-1}=a_{h-1}\\Bigg]}\\\\ &{\\leq\\frac{4\\log(8H^{2}T_{\\ell}^{2}/\\delta)}{N_{\\mathrm{test}}}+\\frac{1}{N_{\\mathrm{test}}}\\underset{0\\leq i<N_{\\mathrm{test}}}{\\sum}\\mathbb{1}\\Bigg\\{\\underset{f\\in\\widehat{\\mathcal{V}}_{\\ell},a\\in A}{\\operatorname*{sup}}\\left|\\mathcal{P}_{\\ell-1}[\\widehat{V}_{\\ell}]-\\mathcal{P}_{\\ell-1}[f_{\\ell}])(x_{\\ell-1}^{(k+i)},a)\\right|>3\\epsilon\\Bigg\\},}\\\\ &{\\leq\\frac{4\\log(8M^{6}N_{\\mathrm{test}}^{2}H^{8}/\\delta)}{N_{\\mathrm{test}}}\\quad(\\mathrm{using~(34)~and~the~fact~that~all~the~tests~pass,~i.e.~(38)}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Concluding. We have established that under $\\mathcal{E}\\cap\\mathcal{E}_{h}^{\\prime}$ , we have for all $\\ell\\,\\in\\,[h+1\\ldots H]$ and all $(x_{h-1},a_{h-1})\\in{\\mathcal{C}}_{h}$ : ", "page_idx": 51}, {"type": "equation", "text": "$$\n)\\pi\\left[\\operatorname*{sup}_{f\\in\\tilde{V}_{\\ell}}\\operatorname*{max}_{a\\in A}\\left|\\left(\\mathcal{P}_{\\ell-1}[\\widehat{V}_{\\ell}]-\\mathcal{P}_{\\ell-1}[f_{\\ell}]\\right)(x_{\\ell-1},a)\\right|>3\\varepsilon\\;\\big|\\;x_{h-1}=x_{h-1},a_{h-1}=a_{h-1}\\right]\\leq\\frac{4\\log(8M^{6}N_{\\mathrm{test}}^{2}R_{\\mathrm{test}}^{2}a_{h-1})}{N_{\\mathrm{test}}}\\leq\\frac{\\pi\\varepsilon}{3},\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Furthermore, we have $\\mathbb{P}[\\mathcal{E}_{h}^{\\prime}]\\geq1-\\delta/H$ . This completes the proof. ", "page_idx": 51}, {"type": "text", "text": "I.4 Proof of Lemma I.3 (Value Function Regression Guarantee) ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Proof of Lemma I.3. Fix $\\pi\\in\\Pi^{\\prime}\\subseteq\\Pi_{\\mathsf{S}}$ and $k\\geq1$ , and consider the $k$ th call to $\\mathsf{R V F S}_{h}$ as per the lemma statement, and let $\\ensuremath{\\mathcal{S}}_{k}$ be the state of $\\mathsf{R V F S}_{\\mathrm{0}}$ during the $k^{\\mathrm{th}}$ call to $\\mathsf{R V F S}_{h}$ and immediately before Line 20, i.e. immediately before gathering data for the regression step in $\\mathsf{R V F S}_{h}$ . ", "page_idx": 51}, {"type": "text", "text": "Relating the regression targets to $V_{h}^{\\widehat{\\pi}}$ . Observe that $\\widehat{V}_{h}$ is the least-squares solution of the objective in Line 26, where the targets are empirical estimates of $V_{h}^{\\widehat{\\pi}}$ . In particular, if we let $\\{\\mathcal{D}_{h}(x,a):$ $(x,a)\\,\\in\\,{\\mathcal{C}}_{h}\\}$ be the datasets in the definition of $\\widehat{\\mathcal{V}}_{h}$ in (15), then for any $(x_{h-1},a_{h-1})\\ \\in\\mathcal{C}_{h}$ and $(x_{h},v_{h})\\in\\dot{\\cal D}_{h}(x_{h-1},a_{h-1})$ , the target $v_{h}$ satisfies ", "page_idx": 51}, {"type": "equation", "text": "$$\nv_{h}={\\widehat{V}}_{h}(x_{h}),\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "where $\\widehat{V}_{h}(x_{h})$ is an empirical estimate of $V_{h}^{\\widehat{\\pi}}(x_{h})$ obtained by sampling $N_{\\mathrm{est}}(\\vert\\mathcal{C}_{h}\\vert)\\;=\\;N_{\\mathrm{est}}(k)$ episodes (for $N_{\\mathrm{est}}(\\cdot)$ defined as in Algorithm 5) by rolling out with $\\widehat{\\pi}$ after starting from $x_{h}$ and playing action $a$ at layer $h$ ; note that $|{\\mathcal{C}}_{h}|=k$ because we are considering the $k$ th call to $\\mathsf{R V F S}_{h}$ . Thus, by Hoeffding\u2019s inequality and the union bound over $(x_{h-1},a_{h-1})\\in{\\mathcal{C}}_{h}$ and $(x_{h},-)\\in\\mathcal{D}_{h}(x_{h-1},a_{h-1})$ , there is an event $\\mathcal{E}_{h,k}^{\\bar{\\prime\\prime}}(\\bar{S_{k}})$ of probability at least $1-\\delta/(8k^{2}H)$ under which ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall\\big(x_{h-1},a_{h-1}\\big)\\in\\mathcal{C}_{h},\\forall(x_{h},-)\\in\\mathcal{D}_{h}\\big(x_{h-1},a_{h-1}\\big):}\\\\ &{\\big|V_{h}^{\\widehat{\\pi}}(x_{h})-\\widehat{V}_{h}(x_{h})\\big|\\leq H\\sqrt{\\frac{2\\log\\left(8\\lvert\\mathcal{C}_{h}\\rvert N_{\\mathrm{reg}}H k^{2}/\\delta\\right)}{N_{\\mathrm{est}}(k)}}\\leq H\\sqrt{\\frac{2\\log\\left(8N_{\\mathrm{reg}}H k^{3}/\\delta\\right)}{N_{\\mathrm{est}}(k)}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "where $N_{\\mathsf{r e g}}$ is as in Line 5, and the last inequality follows by $|{\\mathcal{C}}_{h}|\\leq k$ since we are considering the $k$ th call to $\\mathsf{R V F S}_{h}$ . Thus, under $\\mathcal{E}_{h,k}^{\\prime\\prime}$ , we have ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall(x_{h-1},a_{h-1})\\in\\mathcal{C}_{h},\\forall(x_{h},v_{h})\\in\\mathcal{D}_{h}(x_{h-1},a_{h-1}):}\\\\ &{|V_{h}^{\\widehat{\\pi}}(x_{h})-v_{h}|=\\left|V_{h}^{\\widehat{\\pi}}(x_{h})-\\widehat{V}_{h}(x_{h})\\right|\\leq H\\sqrt{\\frac{2\\log\\left(8N_{\\mathrm{reg}}H k^{3}/\\delta\\right)}{N_{\\mathrm{est}}(k)}}\\leq\\frac{H}{N_{\\mathrm{reg}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "where the second-to-last inequality is by (39) and the last inequality follows by the choice of $N_{\\mathrm{est}}$ in Algorithm 5. ", "page_idx": 52}, {"type": "text", "text": "Bounding the discrepancy $V_{h}^{\\widehat{\\pi}}-V_{h}^{\\pi}$ . On the other hand, by the performance difference lemma, the value function $V_{h}^{\\widehat{\\pi}}$ satisfies: ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\forall x\\in\\mathcal{X},\\ }&{|V_{h}^{\\widehat{\\pi}}(x)-V_{h}^{\\pi}(x)|\\leq\\displaystyle\\sum_{\\tau=h}^{H}\\mathbb{E}^{\\widehat{\\pi}}[|Q_{\\tau}^{\\pi}(x_{\\tau},\\pi_{\\tau}(x_{\\tau}))-Q_{\\tau}^{\\pi}(x_{\\tau},\\widehat{\\pi}_{\\tau}(x_{\\tau}))|\\ |\\ x_{h}=x],}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq H\\displaystyle\\sum_{\\tau=h}^{H}\\mathbb{E}^{\\widehat{\\pi}}[D_{\\mathrm{tv}}(\\widehat{\\pi}_{\\tau}(x_{\\tau}),\\pi_{\\tau}(x_{\\tau}))\\ |\\ x_{h}=x].}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Now, let $(x_{h-1}^{(1)},a_{h-1}^{(1)}),(x_{h-1}^{(2)},a_{h-1}^{(2)}),\\ldots$ denote the elements of $\\ensuremath{\\mathcal{C}}_{h}$ in the order in which they are added to the latter in Line 15. By Lemma C.2 (Freedman\u2019s inequality) instantiated with ", "page_idx": 52}, {"type": "equation", "text": "$$\nw_{i}\\,=\\,|V_{h}^{\\pi}({\\pmb x}_{h}^{(i)})-V_{h}^{\\widehat{\\pi}}({\\pmb x}_{h}^{(i)})|-\\mathbb{E}[|V_{h}^{\\pi}({\\pmb x}_{h})-V_{h}^{\\widehat{\\pi}}({\\pmb x}_{h})|\\mid{\\pmb x}_{h-1}\\,=\\,x_{h-1}^{(j)},{\\pmb a}_{h-1}\\,=\\,a_{h-1}^{(j)}],\n$$", "text_format": "latex", "page_idx": 52}, {"type": "equation", "text": "$$\n\\mathbf{\\mathcal{H}}_{i}=\\sigma\\big(\\pmb{x}_{h}^{(1)},\\dots\\pmb{x}_{h}^{(i-1)}\\big),\\,\\mathrm{for\\,}\\mathrm{all}\\;i\\in[n];\n$$", "text_format": "latex", "page_idx": 52}, {"type": "equation", "text": "$$\n\\lambda=1/H;\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "we get that there is an event $\\widetilde{\\mathcal{E}}_{h,k,\\pi}^{\\prime\\prime}(S_{k})$ of probability at least $1-\\delta/\\big(8k^{2}H|\\Pi^{\\prime}|\\big)$ under which: ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{x_{h-1},a_{h-1})\\in\\mathcal{D}_{h}}\\sum_{(x_{h},\\ldots)\\in\\mathcal{D}_{h}(x_{h-1},a_{h-1})}\\left\\vert V_{h}^{\\pi}(x_{h})-V_{h}^{\\widehat{\\pi}}(x_{h})\\right\\vert}\\\\ &{=2N_{\\mathrm{reg}}\\displaystyle\\sum_{(x_{h-1},a_{h-1})\\in\\mathcal{D}_{h}}\\mathbb{E}\\left[\\left\\vert V_{h}^{\\pi}(x_{h})-V_{h}^{\\widehat{\\pi}}(x_{h})\\right\\vert\\Bigm|x_{h-1}=x_{h-1},a_{h-1}=a_{h-1}\\right]+H\\log(8k^{2}\\lvert\\Pi^{\\prime}\\rvert H/\\delta),}\\\\ &{\\leq2H N_{\\mathrm{reg}}\\displaystyle\\sum_{(x_{h-1},a_{h-1})\\in\\mathcal{D}_{h}}\\sum_{\\tau=h}^{H}\\mathbb{E}^{\\widehat{\\pi}}\\left[D_{\\mathrm{tv}}(\\widehat{\\pi}_{\\tau}(x_{\\tau}),\\pi_{\\tau}(x_{\\tau}))\\bigm|x_{h-1}=x_{h-1},a_{h-1}=a_{h-1}\\right]+H\\log(8k^{2}\\lvert\\Pi^{\\prime}\\rvert\\Pi^{\\prime})}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "where the last inequality follows by (41) and the law of total expectation. ", "page_idx": 52}, {"type": "text", "text": "Regression guarantee. Since $\\pi\\,\\in\\,\\Pi^{\\prime}\\subseteq\\,\\Pi_{S}$ and Assumption I.1 holds, Lemma C.5 (regression guarantee) instantiated with ", "page_idx": 52}, {"type": "text", "text": "$\\begin{array}{l}{{\\bullet\\;\\,f_{\\star}(x)=V_{h}^{\\pi}(x);}}\\\\ {{\\bullet\\;\\,B=H;}}\\end{array}$   \n\u2022 $\\pmb{b}_{i}=\\pmb{v}_{h}-V_{h}^{\\pi}\\big(\\pmb{x}_{h}\\big)$ (where $\\begin{array}{r}{\\pmb{v}_{h}:=\\operatorname*{max}_{a\\in\\mathcal{A}}\\widehat{Q}_{h}(\\pmb{x}_{h},a))}\\end{array}$ ; and \u2022 $\\xi=H$ ; ", "page_idx": 52}, {"type": "text", "text": "implies that there is an event E\u02d8\u2032h\u2032,k,\u03c0(Sk) of probability at least 1 \u2212\u03b4/(4k2H\u2223\u03a0\u2032\u2223) under which we have: ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{(x_{h-1,a_{h-1}})\\in C_{h}}{\\sum}\\frac{1}{N_{r\\in g}}\\underset{(x_{h},-)\\in\\mathcal{D}_{h}(x_{h-1},a_{h-1})}{\\sum}\\big(\\widehat{V}_{h}(x_{h})-V_{h}^{\\pi}(x_{h})\\big)^{2}}\\\\ &{\\le\\frac{4k H^{2}\\log(4k^{2}H|\\Pi^{\\prime}||\\mathcal{V}||\\delta)}{N_{r\\in g}}+\\frac{4H}{N_{r\\in g}}\\underset{(x_{h-1,a_{h-1}})\\in C_{h}}{\\sum}\\underset{(x_{h},v_{h})\\in\\mathcal{D}_{h}(x_{h-1},a_{h-1})}{\\sum}|V_{h}^{\\pi}(x_{h})-v_{h}|,}\\\\ &{\\le\\frac{4k H^{2}\\log(4k^{2}H|\\Pi^{\\prime}||\\mathcal{V}||\\delta)}{N_{r\\in g}}+\\frac{4H}{N_{r\\in g}}\\underset{(x_{h-1,a_{h-1}})\\in\\mathcal{C}_{h}}{\\sum}\\underset{(x_{h},v_{h})\\in\\mathcal{D}_{h}(x_{h-1},a_{h-1})}{\\sum}|V_{h}^{\\pi}(x_{h})-v_{h}|}\\\\ &{\\quad+\\ \\frac{4H}{N_{r\\in g}}\\underset{(x_{h-1,a_{h-1}})\\in\\mathcal{C}_{h}(x_{h},v_{h})\\in\\mathcal{D}_{h}(x_{h-1},a_{h-1})}{\\sum}|V_{h}^{\\pi}(x_{h})-V_{h}^{\\pi}(x_{h})|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "where the last step follows by the triangle inequality. Thus, by plugging (42) and (40) into (43), we get that under $\\mathcal{E}_{h,k}^{\\prime\\prime}(S_{k})\\cap\\widetilde{\\mathcal{E}}_{h,k,\\pi}^{\\prime\\prime}(S_{k})\\cap\\breve{\\mathcal{E}}_{h,k,\\pi}^{\\prime\\prime}(S_{k})$ : ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{x_{h-1},a_{h-1})\\in\\mathcal{C}_{h}}\\frac{1}{N_{\\mathrm{reg}}}\\sum_{\\substack{(x_{h},-)\\in\\mathcal{D}_{h}\\,(x_{h-1},a_{h-1})}}\\left(\\widehat{V}_{h}(x_{h})-V_{h}^{\\pi}(x_{h})\\right)^{2}}\\\\ &{\\le\\displaystyle\\frac{9k H^{2}\\log(8k^{2}H|\\Pi^{\\prime}||\\mathcal{V}|/\\delta)}{N_{\\mathrm{reg}}}+8H^{2}\\sum_{\\substack{(x_{h-1},a_{h-1})\\in\\mathcal{C}_{h}\\,\\tau=h}}\\sum_{\\tau=h}^{H}\\mathbb{E}^{\\widehat{\\pi}}\\left[D_{\\mathrm{tv}}(\\widehat{\\pi}_{\\tau}(x_{\\tau}),\\pi_{\\tau}(x_{\\tau}))\\mid x_{h-1}=x_{h-1},a_{h-1}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Applying the union bound to conclude. Let $\\pmb{S}_{k}$ be the random state of $\\mathsf{R V F S}_{\\mathrm{0}}$ during the $k^{\\mathrm{th}}$ call to $\\mathsf{R V F S}_{h}$ and immediately before Line 20, i.e. im mSediately before gathering data for the regression step in $\\mathsf{R V F S}_{h}$ . Further, let $\\pmb{\\mathcal{S}}_{k}^{+}$ be the random state of $\\mathsf{R V F S}_{\\mathrm{0}}$ during the $k^{\\mathrm{th}}$ call to $\\mathsf{R V F S}_{h}$ and immediately before Line 26, i.e. im Smediately before the regression step in $\\mathsf{R V F S}_{h}$ . If $\\mathsf{R V F S}_{\\mathrm{0}}$ terminates before the $k^{t h}$ call to $\\mathsf{R V F S}_{h}$ , we use the convention that $\\pmb{S}_{k}=\\pmb{S}_{k}^{+}=\\mathbf{t}$ , where t denotes a terminal state, and define $\\begin{array}{r}{\\mathcal{E}_{h,k}^{\\prime\\prime}\\big(\\mathbf{t}\\big)=\\widetilde{\\mathcal{E}}_{h,k,\\pi}^{\\prime\\prime}\\big(\\mathbf{t}\\big)=\\breve{\\mathcal{E}}_{h,k,\\pi}^{\\prime\\prime}\\big(\\mathbf{t}\\big)=\\big\\{\\mathbf{t}\\big\\}.}\\end{array}$ . F uSrther,  Swe define ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\mathcal{E}_{h}^{\\prime\\prime}:=\\left\\{\\prod_{k\\in\\mathbb{N},\\pi\\in\\Pi^{\\prime}}\\mathbb{I}\\{\\pmb{S}_{k}^{+}\\in\\mathcal{E}_{h,k}^{\\prime\\prime}(\\pmb{S}_{k})\\cap\\widetilde{\\mathcal{E}}_{h,k,\\pi}^{\\prime\\prime}(\\pmb{S}_{k})\\cap\\check{\\mathcal{E}}_{h,k,\\pi}^{\\prime\\prime}(\\pmb{S}_{k})\\}=1\\right\\}.\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Note that by the argument in the sequel and the union bound, we have that ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\forall k\\geq1,\\forall S_{k},\\quad\\mathbb{P}[\\mathcal{S}_{k}^{+}\\in\\mathcal{E}_{h,k}^{\\prime\\prime}(S_{k})\\cap\\widetilde{\\mathcal{E}}_{h,k,\\pi}^{\\prime\\prime}(S_{k})\\cap\\check{\\mathcal{E}}_{h,k,\\pi}^{\\prime\\prime}(S_{k})]\\geq1-\\frac{\\delta}{2k^{2}H},\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "where $\\ensuremath{\\mathcal{S}}_{k}$ denotes the state of $\\mathsf{R V F S}_{\\mathrm{0}}$ during the $k^{\\mathrm{th}}$ call to $\\mathsf{R V F S}_{h}$ and immediately before Line 20. By letting $\\pmb{{\\cal S}}_{1}^{\\prime},\\pmb{{\\cal S}}_{2}^{\\prime},\\ldots$ denote an identical, independent copy of the sequence $\\pmb{{\\cal S}}_{1},\\pmb{{\\cal S}}_{2},\\ldots$ , we have by the cha iSn rulSe: ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}[\\pmb{\\mathcal{E}}_{h}^{\\prime\\prime}]=\\mathbb{E}_{\\pmb{S}_{1}^{\\prime},\\pmb{S}_{2}^{\\prime},\\dots}\\left[\\prod_{k\\geq1}\\mathbb{P}[\\pmb{S}_{k}^{+}\\in\\mathcal{E}_{h,k}^{\\prime\\prime}(\\pmb{S}_{k})\\cap\\widetilde{\\mathcal{E}}_{h,k,\\pi}^{\\prime\\prime}(\\pmb{S}_{k})\\cap\\check{\\mathcal{E}}_{h,k,\\pi}^{\\prime\\prime}(\\pmb{S}_{k})\\mid\\pmb{S}_{k}=\\pmb{S}_{k}^{\\prime}]\\right],}\\\\ &{\\quad\\quad\\geq\\displaystyle\\prod_{k\\geq1}\\left(1-\\frac{\\delta}{2k^{2}H}\\right),\\quad(\\mathrm{by}\\ (45))}\\\\ &{\\quad\\quad\\geq1-\\frac{\\delta}{H},}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "where the last inequality follows from the fact that for any sequence $x_{1},x_{2},\\cdot\\cdot\\cdot\\in\\big(0,1\\big)$ , we have $\\begin{array}{r}{\\prod_{k\\geq1}(1-x_{k})\\geq1-\\sum_{k\\geq1}x_{k}}\\end{array}$ . Combining (46) with (44) implies that $\\mathcal{E}_{h}^{\\prime\\prime}$ gives the desired result. ", "page_idx": 53}, {"type": "text", "text": "I.5 Proof of Lemma I.4 (Guarantee for Confidence Sets) ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "To prove Lemma I.4, we need one additional result pertaining to the order in which the instances $\\bigl(\\mathsf{R V F S}_{h}\\bigr)_{h\\in[H]}$ are called. ", "page_idx": 53}, {"type": "text", "text": "Lemma I.5. Let $h\\in[0\\ldots H]$ be given, and consider the setting of Lemma I.4. Further, consider a call to ${\\sf R V F S}_{0}(f,\\mathcal{V},\\widetilde{\\emptyset},\\widetilde{\\emptyset};\\mathcal{V},\\bar{\\varepsilon},\\delta)$ that terminates, and let $h\\in[H]$ be any layer such that $\\mathsf{R V F S}_{h}$ is called during the execution of $\\mathsf{R V F S}_{\\mathrm{0}}$ . Then, after the last call to $\\mathsf{R V F S}_{h}$ terminates, no instance of RVFS in $(\\mathsf{R V F S}_{\\tau})_{\\tau>h}$ is called before $\\mathsf{R V F S}_{\\mathrm{0}}$ terminates. ", "page_idx": 54}, {"type": "text", "text": "Proof of Lemma I.5. Suppose there is an instance of RVFS in $(\\mathsf{R V F S}_{\\tau})_{\\tau>h}$ that is called after the last call to $\\mathsf{R V F S}_{h}$ terminates. Let $\\tau>h$ be the lowest layer where $\\mathsf{R V F S}_{\\tau}$ is called after the last call to $\\mathsf{R V F S}_{h}$ terminates. Let RVF $\\mathsf{S}_{\\tau}^{\\mathrm{last}}$ denote the corresponding instance of $\\mathsf{R V F S}_{\\tau}$ . Further, let $\\ell<\\tau$ be such that $\\mathsf{R V F S}_{\\ell}$ is the parent instance of $\\mathsf{R V F S}_{\\tau}^{\\tt L a s t}$ (i.e. the instance that called RVF $\\mathsf{S}_{\\tau}^{\\mathrm{last}}$ ). Note that we cannot have $\\ell=h$ as this would imply that an instance of $\\mathsf{R V F S}_{h}$ terminates after $\\mathsf{R V F S}_{\\tau}^{\\tt L a s t}$ , and we have assumed that ${\\sf R V F S}_{\\tau}^{\\mathrm{Last}}$ terminates after the last call $\\mathsf{R V F S}_{h}$ . It is also not possible to have $\\ell>h$ as this would imply that $\\tau$ is not the lowest layer where $\\mathsf{R V F S}_{\\tau}$ is called after the last call to $\\mathsf{R V F S}_{h}$ terminates. Thus, we must have that $\\ell<h$ . Now, the for-loop in Line 16 ensures that that there is an instance of $\\mathsf{R V F S}_{h}$ that is called after RVF $\\mathsf{S}_{\\tau}^{\\mathrm{last}}$ terminates and before $\\mathsf{R V F S}_{\\ell}$ does. This contradicts the assumption that RVF $\\mathsf{S}_{\\tau}^{\\mathrm{last}}$ is called after the last call to $\\mathsf{R V F S}_{h}$ . \u53e3 ", "page_idx": 54}, {"type": "text", "text": "Proof of Lemma I.4. We start by showing that $\\tilde{\\pi}\\in\\Pi_{4\\varepsilon}$ by constructing the corresponding collection of random state-action value functions $(\\widetilde{Q}_{h}(x,a))_{(h,x,a)\\in[H]\\times\\mathcal{X}\\times\\mathcal{A}}\\subset[0,H]$ in the definition of $\\Pi_{4\\varepsilon}$ . In particular, for $(h,x,a)\\in[H]\\times\\mathcal{X}\\times\\mathcal{A}$ , we define ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widetilde{Q}_{h}(x,a)=\\left\\{\\begin{array}{l l}{\\widehat{Q}_{h}(x,a),\\quad}&{\\mathrm{if~}\\|\\widehat{Q}_{h}(x,\\cdot)-\\mathcal{P}_{h}[V_{h+1}^{\\widetilde{\\pi}}](x,\\cdot)\\|_{\\infty}\\leq4\\varepsilon,}\\\\ {\\mathcal{P}_{h}[V_{h+1}^{\\widetilde{\\pi}}](x,a),}&{\\mathrm{otherwise},}\\end{array}\\right.\\quad\\mathrm{for~}h=H,\\ldots,1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "where $\\widehat{\\pmb{Q}}_{\\tau}(x,a):=\\widehat{\\pmb{{\\mathscr{P}}}}_{\\tau,\\varepsilon,\\delta^{\\prime}}[\\widehat{V}_{\\tau+1}](x,a)$ . Note that $\\widetilde{Q}_{h}(x,a)$ only depends on the randomness of $\\widehat{\\mathcal{P}}_{h,\\varepsilon,\\delta^{\\prime}}[\\widehat{V}_{h+1}](x,a)$ , and so $(\\widetilde{Q}_{h}(x,a))_{(h,x,a)\\in[H]\\times\\mathcal{X}\\times A}$ are independent random variables. Furthermore, since $\\mathcal{P}_{h}[V_{h+1}^{\\tilde{\\pi}}]\\equiv Q_{h}^{\\tilde{\\pi}}$ , we have that ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\forall(x,a)\\in\\mathcal{X}\\times\\mathcal{A},\\quad\\|\\widetilde{Q}_{h}(x,a)-Q_{h}^{\\tilde{\\pi}}(x,a)\\|\\leq4\\varepsilon.\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Finally, since $\\widetilde{\\pi}_{h}(\\cdot)\\in\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}\\widetilde{Q}_{h}(\\cdot,a)$ , we have that $\\tilde{\\pi}\\in\\Pi_{4\\varepsilon}$ . ", "page_idx": 54}, {"type": "text", "text": "We show $V_{h}^{\\tilde{\\pi}}\\in\\widehat{\\mathcal{V}}_{h}$ . We prove that $V_{h}^{\\tilde{\\pi}}\\in\\widehat{\\mathcal{V}}_{h}$ , for all $h\\in[H]$ , under the event $\\mathcal{E}^{\\prime\\prime\\prime}:=\\mathcal{E}\\cap\\mathcal{E}_{1}^{\\prime}\\cap\\mathcal{E}_{1}^{\\prime\\prime}\\cap\\cdots\\cap$ $\\mathcal{E}_{H}^{\\prime}\\cap\\mathcal{E}_{H}^{\\prime\\prime}$ , whhere $\\mathcal{E}$ , $(\\mathcal{E}_{h}^{\\prime})$ , and $(\\mathcal{E}_{h}^{\\prime\\prime})$ arhe the e\u2032v\u2032e\u2032nts defined in Lemma I.1, Lemma I.2, and Lemma I.3, respectively. Throughout, we condition on . First, note that by Lemma I.1, $\\mathsf{R V F S}_{\\mathrm{0}}$ terminates. Let $(\\widehat{V_{1:H}},\\widehat{\\mathcal{V}}_{1:H},\\mathcal{C}_{1:H},\\bar{\\mathcal{B}}_{1:H},t_{1:H})$ be the tuple it returns. ", "page_idx": 54}, {"type": "text", "text": "We will show via backwards induction over $\\ell=H+1,\\ldots,1$ , that ", "page_idx": 54}, {"type": "equation", "text": "$$\nV_{\\ell}^{\\tilde{\\pi}}\\in\\widehat{\\mathcal{V}}_{\\ell},\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "where $\\tilde{\\pi}_{1:H}$ is the stochastic policy defined recursively via ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{\\pi}_{\\tau}(x)\\in\\underset{a\\in A}{\\arg\\operatorname*{max}}\\left\\{\\begin{array}{l l}{\\widehat{Q}_{\\tau}(x,a):=\\widehat{\\mathcal{P}}_{\\tau,\\varepsilon,\\delta^{\\prime}}[\\widehat{V}_{\\tau+1}](x,a),}&{\\mathrm{if~}\\lVert\\widehat{Q}_{\\tau}(x,\\cdot)-\\mathcal{P}_{\\tau}[V_{\\tau+1}^{\\tilde{\\pi}}](x,\\cdot)\\rVert_{\\infty}\\leq4\\varepsilon,}\\\\ {\\mathcal{P}_{\\tau}[V_{\\tau+1}^{\\tilde{\\pi}}](x,a),}&{\\mathrm{otherwise},}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "where $\\widehat{\\pmb{Q}}_{\\tau}(x,a):=\\widehat{\\pmb{{\\mathscr{P}}}}_{\\tau,\\varepsilon,\\delta^{\\prime}}[\\widehat{V}_{\\tau+1}](x,a)$ . ", "page_idx": 54}, {"type": "text", "text": "Base case $[\\ell=H+1]$ . This holds trivially because $V_{H+1}^{\\pi}\\equiv0$ for any $\\pi\\in\\Pi_{\\ S}$ by convention. ", "page_idx": 54}, {"type": "text", "text": "General case $[\\ell\\leq H]$ . Fix $h\\in[H]$ and suppose that (47) holds for all $\\ell\\in[h+1\\ldots H+1]$ . We show as a consequence that (47) holds for $\\ell=h$ . First, note that if $\\mathsf{R V F S}_{h}$ is never called during the execution of $\\mathsf{R V F S}_{\\mathrm{0}}$ , then $\\widehat{\\mathcal{V}}_{h}=\\mathcal{V}$ , and so (47) trivially holds for $\\ell=h$ under Assumption I.1 with $\\varepsilon_{\\mathsf{r e a l}}=4\\varepsilon$ . ", "page_idx": 54}, {"type": "text", "text": "Now, suppose that $\\mathsf{R V F S}_{h}$ is called at least once, and let $(\\widehat{V}_{h:H}^{+},\\widehat{\\mathcal{V}}_{h:H}^{+},\\mathcal{C}_{h:H}^{+},\\mathcal{B}_{h:H}^{+},t_{h:H}^{+})$ be the output of the last call to $\\mathsf{R V F S}_{h}$ during the execution of $\\mathsf{R V F S}_{\\mathrm{0}}$ . We claim that ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\big(\\widehat{V}_{h:H}^{+},\\widehat{\\mathcal{V}}_{h:H}^{+},\\mathcal{C}_{h:H}^{+}\\big)=\\big(\\widehat{V}_{h:H},\\widehat{\\mathcal{V}}_{h:H},\\mathcal{C}_{h:H}\\big).\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "To see this, first note that the for-loop in Line 16 ensures that no instance of $(\\mathsf{R V F S}_{\\tau})_{\\tau>h}$ can be called after the last call to $\\mathsf{R V F S}_{h}$ (by Lemma I.5). Thus, the estimated value functions, confidence sets, and core sets for layers $h+1,\\ldots,H$ remain unchanged after the last call to $\\mathsf{R V F S}_{h}$ ; that is, (48) holds. ", "page_idx": 54}, {"type": "text", "text": "Thus, by Lemma I.2, and since we are conditioning on $\\mathcal{E}_{h+1:H}^{\\prime}$ , we have that for all $(x_{h-1},a_{h-1})\\in{\\mathcal{C}}_{h}$ and $\\ell\\in[h+1\\ldots H+1]$ : ", "page_idx": 55}, {"type": "text", "text": "$)\\pi\\left[\\operatorname*{sup}_{f\\in\\tilde{V}_{\\ell}}\\operatorname*{max}_{a\\in A}\\left|\\left(\\mathcal{P}_{\\ell-1}[\\widehat{V}_{\\ell}]-\\mathcal{P}_{\\ell-1}[f_{\\ell}]\\right)(x_{\\ell-1},a)\\right|>3\\varepsilon\\;\\big|\\;x_{h-1}=x_{h-1},a_{h-1}=a_{h-1}\\right]\\leq\\frac{4\\log(8M^{6}N_{\\mathrm{test}}^{2}R_{\\mathrm{test}}^{2}a_{h-1})}{N_{\\mathrm{test}}}\\leq\\frac{\\pi\\varepsilon}{3},$ 8/\u03b4) \u6b63   \n(49) where $M=\\left\\lceil8\\varepsilon^{-1}C_{\\mathsf{p u s h}}H\\right\\rceil$ . Now, by the induction hypothesis, we have $V_{\\ell}^{\\tilde{\\pi}}\\in\\widehat{\\mathcal{V}}_{\\ell}$ , and so substituting $V_{\\ell}^{\\tilde{\\pi}}$ for $f_{\\ell}$ in (49), we get that for all $(x_{h-1},a_{h-1})\\in{\\mathcal{C}}_{h}$ and $\\ell\\in[h+1\\ldots H+1]$ :   \n${}^{\\mathcal{P}}\\bigg[\\operatorname*{max}_{a\\in A}\\Big|\\big(\\mathcal{P}_{\\ell-1}[\\widehat{V}_{\\ell}]-\\mathcal{P}_{\\ell-1}[V_{\\ell}^{\\tilde{\\pi}}]\\big)(x_{\\ell-1},a)\\Big|>3\\varepsilon\\ \\big|\\ x_{h-1}=x_{h-1},a_{h-1}=a_{h-1}\\Big]\\leq\\frac{4\\log\\bigl(8M^{6}N_{\\mathrm{test}}^{2}H^{8}/N_{\\mathrm{test}}^{2}\\bigr)}{N_{\\mathrm{test}}}$ \u03b4) Therefore, by Lemma L.1 (instantiated with $\\mu[\\cdot]=\\mathbb{P}^{\\widehat{\\pi}}[\\cdot\\mid\\pmb{x}_{h-1}=x_{h-1},\\pmb{a}_{h-1}=a_{h-1}]$ , $\\tau=\\ell-1$ , and $V_{\\tau+1}=V_{\\ell}^{\\tilde{\\pi}})$ , we have that $(x_{h-1},a_{h-1})\\in{\\mathcal{C}}_{h}$ and $\\ell\\in[h+1\\ldots H+1]$ :   \n$\\mathbb{E}^{\\widehat\\pi}\\big[D_{\\mathrm{tv}}\\big(\\widehat\\pi_{\\ell-1}(x_{\\ell-1}),\\widetilde\\pi_{\\ell-1}({\\pmb x}_{\\ell-1})\\big)\\mid x_{h-1}=x_{h-1},\\pmb{a}_{h-1}=\\pmb{a}_{h-1}\\big]\\le\\frac{4\\log(8M^{6}N_{\\mathrm{test}}^{2}H^{8}/\\delta)}{N_{\\mathrm{test}}}+\\delta^{\\prime},$ ", "page_idx": 55}, {"type": "text", "text": "where $\\delta^{\\prime}$ is as in Algorithm 5. ", "page_idx": 55}, {"type": "text", "text": "Applying the regression guarantee to conclude the induction. Note that $\\tilde{\\pi}\\in\\Pi^{\\prime}$ , where $\\Pi^{\\prime}\\subset\\Pi_{\\mathbb{S}}$ is the set of stochastic policies such that $\\pi\\in\\Pi^{\\prime}$ if and only if there exists $V_{1:H}\\in\\mathcal{V}$ such that $\\pi$ is defined recursively as ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pi_{\\tau}(x)\\in\\underset{a\\in A}{\\arg\\operatorname*{max}}\\left\\{\\begin{array}{l l}{Q_{\\tau}(x,a):=\\widehat{\\mathcal{P}}_{\\tau,\\varepsilon,\\delta^{\\prime}}[V_{\\tau+1}](x,a),}&{\\mathrm{if~}\\lVert Q_{\\tau}(x,\\cdot)-\\mathcal{P}_{\\tau}[V_{\\tau+1}^{\\pi}](x,\\cdot)\\rVert_{\\infty}\\leq4\\varepsilon,}\\\\ {\\mathcal{P}_{\\tau}[V_{\\tau+1}^{\\pi}](x,a),}&{\\mathrm{otherwise},}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "for $\\tau=H,\\dots,1$ , where $\\pmb{Q}_{\\tau}(x,a):=\\widehat{\\mathscr{P}}_{\\tau,\\varepsilon,\\delta^{\\prime}}[V_{\\tau+1}](x,a)$ . The policy class $\\Pi^{\\prime}$ is finite and $\\left\\vert\\Pi^{\\prime}\\right\\vert\\leq\\left\\vert\\mathcal{V}\\right\\vert$ Furthermore, we have $\\Pi^{\\prime}\\subseteq\\Pi_{4\\varepsilon}$ as Pshown at the beginning of this proof. Therefore, if we let $\\{\\mathcal{D}_{h}(x,a):(x,a)\\in\\mathcal{C}_{h}\\}$ be the datasets in the definition of $\\widehat{\\nu}_{h}$ in (15), we have by Lemma I.3 (under Assumption I.1 with $\\varepsilon_{\\mathsf{r e a l}}=4\\varepsilon_{\\mathsf{r}}^{\\prime}$ ) and the conditioning on $\\mathcal{E}_{h+1:H}^{\\prime\\prime}$ and $\\mathcal{E}$ : ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{x_{h-1},a_{h-1})\\in C_{h}}\\frac{1}{N_{\\mathrm{reg}}}\\sum_{\\substack{(x_{h},-)\\in\\mathcal{D}_{h}\\,(x_{h-1},a_{h-1})}}\\left(\\widehat{V}_{h}(x_{h})-V_{h}^{\\tilde{\\pi}}(x_{h})\\right)^{2}}\\\\ &{\\leq\\displaystyle\\frac{9|C_{h}|H^{2}\\log(8|\\mathcal{C}_{h}|^{2}H|\\mathcal{V}|^{2}/\\delta)}{N_{\\mathrm{reg}}}+8H^{2}\\sum_{\\substack{(x_{h-1},a_{h-1})\\in\\mathcal{C}_{h}\\,\\tau=h}}\\sum_{\\substack{\\tau=h}}^{H}\\mathbb{E}^{\\widehat{\\pi}}\\left[D_{\\mathrm{tv}}(\\widehat{\\pi}_{\\tau}(x_{\\tau}),\\widetilde{\\pi}_{\\tau}(x_{\\tau}))\\mid x_{h-1}=x_{h-1},a_{h}\\right]}\\\\ &{\\leq\\displaystyle\\frac{9M H^{2}\\log(8M^{2}H|\\mathcal{V}|^{2}/\\delta)}{N_{\\mathrm{reg}}}+8H^{2}\\sum_{\\substack{(x_{h-1},a_{h-1})\\in\\mathcal{C}_{h}\\,\\tau=h}}\\prod_{\\tau=h}^{H}\\mathbb{E}^{\\widehat{\\pi}}\\left[D_{\\mathrm{tv}}(\\widehat{\\pi}_{\\tau}(x_{\\tau}),\\widetilde{\\pi}_{\\tau}(x_{\\tau}))\\mid x_{h-1}=x_{h-1},a_{h-1}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "where the last inequality follows by the fact that $|{\\mathcal{C}}_{h}|\\leq M$ under $\\mathcal{E}$ . Combining (51) with (50), implies that ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{(x_{h-1},a_{h-1})\\in{\\cal C}_{h}}\\frac{1}{N_{\\mathrm{reg}}}\\sum_{(x_{h},-)\\in{\\cal D}_{h}(x_{h-1},a_{h-1})}\\left(\\widehat{V}_{h}(x_{h})-V_{h}^{\\widetilde{\\pi}}(x_{h})\\right)^{2}}\\\\ &{\\displaystyle\\leq\\frac{9M H^{2}\\log(8M^{2}H|\\mathcal{V}|^{2}/\\delta)}{N_{\\mathrm{reg}}}+8M H^{3}\\cdot\\frac{4\\log(8M^{6}N_{\\mathrm{test}}^{2}H^{8}/\\delta)}{N_{\\mathrm{test}}}+8M H^{3}\\delta^{\\prime},}\\\\ &{\\displaystyle=\\frac{9M H^{2}\\log(8M^{2}H|\\mathcal{V}|^{2}/\\delta)}{N_{\\mathrm{reg}}}+8M H^{3}\\cdot\\frac{4\\log(8M^{6}N_{\\mathrm{test}}^{2}H^{8}/\\delta)}{N_{\\mathrm{test}}}+8M H^{3}\\frac\\delta{4M^{7}N_{\\mathrm{test}}^{2}H^{8}|\\mathcal{V}|^{\\prime}}{N_{\\mathrm{test}}^{2}H^{8}|\\mathcal{V}|},}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "where the last inequality follows by the fact that $\\delta\\ \\in\\ (0,1)$ and the definition of $\\varepsilon_{\\mathrm{reg}}^{2}$ in Algorithm 5. By the definition of $\\widehat{\\mathcal{V}}_{h}$ in (15), (52) implies that $V_{h}^{\\tilde{\\pi}}\\in\\widehat{\\mathcal{V}}_{h}$ , which completes the induction. ", "page_idx": 55}, {"type": "text", "text": "I.6 Proof of Theorem I.1 (Main Guarantee of RVFS) ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Proof of Theorem I.1. We condition on the event $\\widetilde{\\mathcal{E}}:=\\mathcal{E}\\cap\\mathcal{E}^{\\prime\\prime\\prime}\\cap\\mathcal{E}_{1}^{\\prime}\\cap\\cdots\\cap\\mathcal{E}_{H}^{\\prime}$ , where $\\mathcal{E},\\mathcal{E}^{\\prime\\prime\\prime}$ , and $(\\mathcal{E}_{h}^{\\prime})$ are the events in Lemma I.1, Lemma I.4, and Lemma I.2, respectively. Note that by the union bound, we have $\\mathbb{P}[\\widetilde{\\mathcal{E}}]\\ge1-5\\delta$ . By Lemma I.2, we have that ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\forall h\\in[H],\\quad\\mathbb{P}^{\\widehat{\\pi}}\\bigg[\\operatorname*{sup}_{f\\in\\widehat{\\mathcal{V}}_{h}}\\operatorname*{max}_{a\\in A}\\big|(\\mathcal{P}_{h-1}[\\widehat{V}_{h}]-\\mathcal{P}_{h-1}[f_{h}])(x_{h-1},a)\\big|>3\\varepsilon\\bigg]\\leq\\frac{4\\log(8M^{6}N_{\\mathrm{test}}^{2}H^{8}/\\delta)}{N_{\\mathrm{test}}},\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "where $M\\,=\\,\\lceil8\\varepsilon^{-1}C_{\\mathsf{p u s h}}H\\rceil$ and $N_{\\mathrm{test}}\\,=\\,2^{8}M^{2}H\\varepsilon^{-1}\\log(8M^{6}H^{8}\\varepsilon^{-2}\\delta^{-1})$ . On the other hand, by Lemma I.4, we have ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\forall h\\in[H],\\quad V_{h}^{\\tilde{\\pi}}\\in\\widehat{\\mathcal{V}}_{h}.\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Thus, substituting $V_{h}^{\\tilde{\\pi}}$ for $f_{h}$ in (53) we get that for all $h\\in[H+1]$ . ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\mathbb{P}^{\\widehat{\\pi}}\\bigg[\\underset{a\\in A}{\\mathrm{max}}\\big|\\big(\\mathcal{P}_{h-1}[\\widehat{V}_{h}]-\\mathcal{P}_{h-1}[V_{h}^{\\widetilde{\\pi}}]\\big)(x_{h-1},a)\\big|>3\\varepsilon\\bigg]\\leq\\frac{4\\log\\bigl(8M^{6}N_{\\mathrm{test}}^{2}H^{8}/\\delta\\bigr)}{N_{\\mathrm{test}}}.\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "T\u2032his together with Lemma L.1, instantiated with $\\mu\\big[\\cdot\\big]=\\mathbb{P}^{\\widehat{\\pi}}\\big[\\cdot\\big];\\,\\tau=h-1;\\,V_{\\tau+1}=V_{h}^{\\tilde{\\pi}}$ ; and $\\delta=\\delta^{\\prime}$ (with $\\delta^{\\prime}$ as in Algorithm 5), translates to: ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall h\\in[H],\\ \\ \\ \\mathbb{E}^{\\widehat\\pi}[D_{\\mathrm{tv}}(\\widehat\\pi_{h}(x_{h}),\\widetilde\\pi_{h}(x_{h}))]\\leq\\frac{4\\log\\left(8M^{6}N_{\\mathrm{test}}^{2}H^{8}/\\delta\\right)}{N_{\\mathrm{test}}}+\\delta^{\\prime},}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\frac{4\\log\\left(8M^{6}N_{\\mathrm{test}}^{2}H^{8}/\\delta\\right)}{N_{\\mathrm{test}}}+\\frac{\\delta}{4M^{7}N_{\\mathrm{test}}^{2}H^{8}|\\mathcal{V}|},}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\frac{\\varepsilon}{4H^{3}C_{\\mathrm{push}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "where the last step follows from the fact that $N_{\\mathrm{test}}=2^{8}M^{2}H\\varepsilon^{-1}\\log(8M^{6}H^{8}\\varepsilon^{-2}\\delta^{-1})$ (with $M$ as in Line 3). ", "page_idx": 56}, {"type": "text", "text": "Bounding the sample complexity. We now bound the number of episodes used by Algorithm 5 under the event $\\widetilde{\\mathcal{E}}$ . First, we fix $h\\,\\in\\,[H]$ , and focus on the number of episodes used within a to call $\\mathsf{R V F S}_{h}$ , excluding any episodes used by any subsequent calls to $\\mathsf{R V F S}_{\\tau}$ for $\\tau\\,>\\,h$ . We start by counting the number of episodes used to test the fit of the estimated value functions $\\widehat{V}_{h+1:H}$ . Starting from Line 8, there are for-loops over $(x_{h-1},a_{h-1})\\ \\in\\ C_{h}$ , $\\ell\\ =\\ H,\\ldots,h+1$ and $n~\\in~\\left[N_{\\mathrm{test}}\\right]$ to collect partial episodes using the learned policy $\\widehat{\\pi}$ in Algorithm 5, where $N_{\\mathrm{test}}~=~\\dot{2}^{8}M^{2}\\dot{H}\\varepsilon^{-1}\\log(8\\dot{M}^{6}H^{8}\\varepsilon^{-2}\\dot{\\delta}^{-1})$ and $\\breve{M}~=~\\lceil8\\varepsilon^{-1}C_{\\mathrm{push}}\\dot{H}\\rceil$ . Note that executing $\\widehat{\\pi}$ requires the local simulator and uses $N_{\\mathrm{sim}}\\,=\\,2\\log(4M^{7}N_{\\mathrm{test}}^{2}H^{2}|\\mathcal{V}|/\\delta)/\\varepsilon^{2}$ local simulator queries to output an action at each layer (since Algorithm 5 calls Algorithm 7 with confidence level $\\delta^{\\prime}\\;=\\;\\dot{\\delta}/(8M^{7}N_{\\mathrm{test}}^{2}H^{8}|\\gamma|))$ . Also, note that whenever a test fails in Line 14 and the recursive RVFS calls return, the for-loop in Line 8 resumes. We also know (by Lemma I.1) that the number of times the test fails in Line 14 is at most $M$ . Thus, the number of times the for-loop in Line 8 resumes is bounded by $H M$ ; here, $H$ accounts for possible test failures across all layers $\\tau\\,\\in\\,[h+1\\ldots H]$ . Thus, the total sample complexity required to generate episodes between lines Line 8 and Line 11 is bounded by ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\#\\mathrm{\\;episodes\\;for\\;roll-outs}\\leq\\underbrace{M H}_{\\#\\mathrm{\\;of\\;times\\;Line\\;s\\;resumes}}\\cdot\\underbrace{M H^{2}N_{\\mathrm{test}}N_{\\mathrm{sim}}}_{\\mathrm{Sample\\;complexity\\;in\\;case\\;of\\;no\\;test\\;failures}}.\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Note that the test in Line 14 also uses episodes because it calls the operator $\\widehat{\\mathcal{P}}$ for every $a\\in\\mathcal{A}$ . Thus, the number of episodes used for the test in Line 14 is bounded by ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\#\\mathrm{~episodes~for~the~tests}\\leq\\underbrace{M H}_{\\#\\mathrm{~of~times~Line~8~resumes}}\\cdot\\underbrace{M H A N_{\\mathrm{test}}N_{\\mathrm{sim}}}_{\\mathrm{Sample~complexity~for~Line~14}}.\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "We now count the number of episodes used to re-fti the value function in Line 16 and onwards. Note that starting from Line 16, there are for-loops over $(x_{h-1},a_{h-1})\\in{\\mathcal{C}}_{h}$ and $i\\,\\in\\,\\bigl[N_{\\mathrm{reg}}\\bigr]$ to generate $A\\cdot N_{\\mathrm{est}}(|\\mathcal{C}_{h}|)\\leq A\\cdot N_{\\mathrm{est}}(M)$ partial episodes using $\\widehat{\\pi}$ , where $N_{\\mathrm{est}}(k)=2N_{\\mathrm{reg}}^{2}\\log(8A N_{\\mathrm{reg}}H k^{3}/\\delta)$ ", "page_idx": 56}, {"type": "text", "text": "is defined as in Algorithm 5. Since $\\widehat{\\pi}$ uses the local simulator and requires $N_{\\mathrm{est}}$ samples (see Algorithm 7) to output an action at each layer, the number of episodes used to refti the value function is bounded by ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\#\\mathrm{\\,episodes\\,\\,for\\,}V\\mathrm{\\-refitting\\,}\\leq M N_{\\mathrm{reg}}A N_{\\mathrm{est}}(M)H N_{\\mathrm{sim}}.\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Therefore, by (54), (55), and (56), the number of episodes used within a single call to ${\\mathsf{R V F S}}_{h}$ (not accounting for episodes used by recursive calls to $\\mathsf{R V F S}_{\\tau}$ , for $\\tau>h$ ) is bounded by ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\mathsf{R V F S}_{h}\\leq M^{2}H\\big(H+A\\big)N_{\\mathrm{test}}N_{\\mathsf{s i m}}+M N_{\\mathrm{reg}}A N_{\\mathrm{est}}(M)H N_{\\mathsf{s i m}}.\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Finally, by Lemma I.1, $\\mathsf{R V F S}_{h}$ may be called at most $M$ times throughout the execution of $\\mathsf{R V F S}_{\\mathrm{0}}$ . Using this together with (57) and accounting for the number of episodes from all layers $h\\in[H]$ , we get that the total number of episodes is bounded by ", "page_idx": 57}, {"type": "equation", "text": "$$\nM^{3}H^{2}(H+A)N_{\\mathrm{test}}N_{\\mathrm{sin}}+M^{2}H^{2}N_{\\mathrm{reg}}A N_{\\mathrm{est}}(M)N_{\\mathrm{sin}}.\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Substituting the expressions of $M,N_{\\mathrm{test}},N_{\\mathrm{est}},N_{\\mathrm{sim}}.$ and $N_{\\mathrm{reg}}$ from Algorithm 5 and Algorithm 7, we obtain the desired number of episodes, which concludes the proof. \u53e3 ", "page_idx": 57}, {"type": "text", "text": "I.7 Proof of Theorem I.2 (Guarantee of RVFS.bc) ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Proof of Theorem I.2. Let $\\widehat{V}_{1:H}$ be the value function estimates produced by $\\mathsf{R V F S}_{\\mathrm{0}}$ within Algorithm 6, and \u2032l et $\\widehat{\\pi}_{h}^{\\mathsf{R V F S}}(\\cdot)\\,\\in\\,\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}\\widehat{\\mathcal{P}}_{h,\\varepsilon_{\\mathsf{R V F S}},\\delta^{\\prime}}[\\widehat{V}_{h+1}](\\cdot,a)$ , for all $h\\,\\in\\,[H]$ with $\\widehat{V}_{H+1}\\,\\equiv\\,0$ with $\\varepsilon_{\\mathsf{R V F S}}$ and as in Algorithm 6. FurtherP, let and let $\\tilde{\\pi}_{1:H}\\in\\Pi_{\\mathcal{S}}$ be the stochastic policy defined recursively via ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\forall x\\in\\mathcal{X},\\;\\;\\tilde{\\pi}_{\\tau}(x)\\in\\underset{a\\in\\mathcal{A}}{\\arg\\operatorname*{max}}\\left\\{\\begin{array}{l l}{\\widehat{Q}_{\\tau}(x,a),}&{\\mathrm{if}\\;\\|\\widehat{Q}_{\\tau}(x,\\cdot)-\\mathcal{P}_{\\tau}[V_{\\tau+1}^{\\tilde{\\pi}}](x,\\cdot)\\|_{\\infty}\\leq4\\varepsilon_{\\mathtt{R W F S}},}\\\\ {\\mathcal{P}_{\\tau}[V_{\\tau+1}^{\\tilde{\\pi}}](x,a),}&{\\mathrm{otherwise},}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "for $\\tau=H,\\dots,1$ , where $\\widehat{\\mathbf{Q}}_{\\tau}(x,a):=\\widehat{\\mathcal{P}}_{\\tau,\\varepsilon_{\\mathsf{R V F S}},\\delta^{\\prime}}[\\widehat{V}_{\\tau+1}](x,a)$ . By Theorem I.1, there is an event $\\widetilde{\\mathcal{E}}$ of probability at least $1-\\delta/2$ under whicPh: ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\Tilde{\\pi}\\in\\Pi_{4\\varepsilon_{\\mathtt{R V F S}}},\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "and ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\forall h\\in[H],\\quad\\mathbb{E}^{\\widehat\\pi^{\\mathrm{wrs}}}\\big[D_{\\mathrm{tv}}(\\widehat\\pi_{h}^{\\mathsf{R V F S}}(x_{h}),\\widetilde\\pi_{h}(x_{h}))\\big]\\leq\\frac{\\varepsilon_{\\mathsf{R V F S}}}{4H^{3}C_{\\mathrm{push}}}\\leq\\frac{\\varepsilon}{4H^{2}},\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "where the last inequality follows by the choice of $\\varepsilon_{\\mathsf{R V F S}}$ in Algorithm 6. ", "page_idx": 57}, {"type": "text", "text": "For the rest of the proof, we condition on $\\widetilde{\\mathcal{E}}$ . By (59), (63), and Proposition M.1 instantiated with $\\varepsilon_{\\mathfrak{m}\\mathrm{i}s}=0$ (due to all $\\pi$ -realizability), we have that there is an event $\\widetilde{\\mathcal{E}^{\\prime}}$ of probability at least $1-\\delta/2$ under which the policy $\\widehat{\\pi}_{1:H}$ produced by BehaviorCloning ensures that ", "page_idx": 57}, {"type": "equation", "text": "$$\nJ(\\widehat{\\pi}_{1:H}^{\\mathsf{R V F S}})-J(\\widehat{\\pi}_{1:H})\\leq\\frac{\\varepsilon}{2}.\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Now, by Lemma C.6 (the performance difference lemma), we have for $\\tilde{\\pi}$ as in (58): ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J\\big(\\widetilde{\\pi}\\big)-J\\big(\\widehat{\\pi}_{1:H}^{\\mathsf{R V F S}}\\big)=\\displaystyle\\sum_{h=1}^{H}\\mathbb{E}^{\\widehat{\\pi}^{\\mathsf{R V F S}}}\\big[Q_{h}^{\\widetilde{\\pi}}(x_{h},\\widetilde{\\pi}_{h}(x_{h})\\big)-Q_{h}^{\\widetilde{\\pi}}(x_{h},\\widehat{\\pi}_{h}^{\\mathsf{R V F S}}(x_{h}))\\big],}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq H\\displaystyle\\sum_{h=1}^{H}\\mathbb{E}^{\\widehat{\\pi}^{\\mathsf{R V F S}}}\\big[D_{\\mathrm{tv}}(\\widehat{\\pi}_{h}^{\\mathsf{R V F S}}(x_{h}),\\widetilde{\\pi}_{h}(x_{h}))\\big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "and so by (63), we have ", "page_idx": 57}, {"type": "equation", "text": "$$\nJ(\\tilde{\\pi})-J(\\widehat{\\pi}_{1:H}^{\\mathsf{R V F S}})\\leq\\varepsilon/4.\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Finally, since $\\tilde{\\pi}\\in\\Pi_{4\\varepsilon_{\\mathtt{R V F S}}}$ (see (59)), we have by Lemma H.1, ", "page_idx": 57}, {"type": "equation", "text": "$$\nJ(\\pi^{\\star})-J(\\tilde{\\pi})\\leq12H\\varepsilon_{\\sf R V F S}\\leq\\varepsilon/4,\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "where the last inequality follows by the choice $\\varepsilon_{\\mathsf{R V F S}}$ in Algorithm 6. Combining this with (61) and (62), we conclude that under $\\widetilde{\\mathcal{E}}\\cap\\widetilde{\\mathcal{E}^{\\prime}}$ : ", "page_idx": 57}, {"type": "equation", "text": "$$\nJ(\\pi^{\\star})-J(\\widehat{\\pi}_{1:H})\\leq\\varepsilon.\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "By the union bound, we have $\\mathbb{P}[\\widetilde{\\mathcal{E}}\\cap\\widetilde{\\mathcal{E}}^{\\prime}]\\ge1-\\delta$ , and so the desired suboptimality guarantee in (28) holds with probability at least $1-\\delta$ . ", "page_idx": 57}, {"type": "text", "text": "Bounding the sample complexity. The sample complexity is dominated by the call to $\\mathsf{R V F S}_{\\mathrm{0}}$ within RVFS.bc (Algorithm 6). Since RVFS.bc calls $\\mathsf{R V F S}_{\\mathrm{0}}$ with $\\dot{\\varepsilon}=\\varepsilon_{\\mathsf{R V F S}}=\\varepsilon H^{-1}/48$ , we conclude from Theorem I.1 that the total sample complexity is bounded by ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\widetilde{\\cal O}\\left(C_{\\mathrm{push}}^{8}H^{23}A\\cdot\\varepsilon^{-13}\\right).\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "J Guarantee under $V^{\\star}$ -Realizability (Proof of Theorem 4.1, Setup I) ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "In this section, we prove Theorem 4.1 under Setup I $(V^{\\star}/\\pi^{\\star}$ -realizability (Assumptions 4.2 and 4.3) and $\\Delta$ -gap (Assumption 4.4)). We prove this result as a consequence of the more general results (Theorem I.2) in Appendix I by appealing to the relaxed $V^{\\pi}$ -realizability condition in Assumption I.1. ", "page_idx": 58}, {"type": "text", "text": "J.1 Analysis: Proof of Theorem 4.1 (Setup I) ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "We begin by showing that Assumption 4.2 and Assumption 4.4 together imply that Assumption I.1 holds for any $\\varepsilon_{\\mathsf{r e a l}}\\ \\bar{\\leq}\\ \\Delta/2$ ; we prove this by showing that the benchmark policy class $\\Pi_{\\varepsilon^{\\prime}}$ (Appendix H.2) reduces to $\\{\\pi^{\\star}\\}$ when $\\varepsilon^{\\prime}\\le\\Delta/2$ . ", "page_idx": 58}, {"type": "text", "text": "Lemma J.1. Assume that $\\nu$ satisfies Assumption 4.2 ( $V^{\\star}$ -realizability), and that Assumption 4.4 (gap) holds with $\\Delta>0$ . Then, for all $\\varepsilon^{\\prime}\\le\\Delta/2$ , we have $\\Pi_{\\varepsilon^{\\prime}}=\\left\\{\\pi^{\\star}\\right\\}$ and $\\nu$ satisfies Assumption I.1 with $\\varepsilon_{\\mathsf{r e a l}}=\\varepsilon^{\\prime}$ . ", "page_idx": 58}, {"type": "text", "text": "Informally Lemma J.1, whose proof is in Appendix J.2, states that under Assumption 4.2, Assumption 4.4 with $\\Delta>0$ , and Assumption 4.1 (pushforward coverability) with $C_{\\mathsf{p u s h}}>0$ , we are essentially in the setting of Theorem I.1 (guarantee of RVFS under relaxed $V^{\\pi}$ -realizability), as long as we choose $\\varepsilon_{\\mathsf{r e a l}}\\leq\\Delta/2$ . With this, we now state and prove a central guarantee for RVFS under $V^{\\star}$ -realizability with a gap. ", "page_idx": 58}, {"type": "text", "text": "Lemma J.2 (Intermediate guarantee for RVFS under Setup I). Let $\\delta\\in(0,1)$ be given, and suppose that: ", "page_idx": 58}, {"type": "text", "text": "\u2022 Assumption 4.1 (pushforward coverability) holds with parameter $C_{\\mathsf{p u s h}}>0$ ;   \n\u2022 Assumption 4.4 (gap) holds with parameter $\\Delta>0$ ;   \n\u2022 The function class $\\nu$ satisfies Assumption 4.2 $V^{\\star}$ -realizability). ", "page_idx": 58}, {"type": "text", "text": "Then, for any $f\\,\\in\\,\\mathcal{V}$ and $\\varepsilon\\,\\in\\,(0,\\Delta/8)$ , with probability at least $1-\\delta$ , R $\\prime\\mathsf{F S}_{0}(f,\\mathcal{V},\\mathcal{O},\\mathcal{O};\\mathcal{V},\\varepsilon,\\delta)$ (Algorithm 5) terminates and returns value functions $\\widehat{V}_{1:H}$ that satisfy ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\forall h\\in[H],\\quad\\mathbb{P}^{\\widehat\\pi}[\\widehat\\pi_{h}(x_{h})\\neq\\pi_{h}^{\\star}(x_{h})]\\leq\\frac{\\varepsilon}{4C_{\\mathrm{push}}H^{3}},\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "where $\\begin{array}{r}{\\widehat{\\pmb{\\pi}}_{h}(x)\\in\\arg\\operatorname*{max}_{a\\in\\cal A}\\widehat{\\pmb{\\mathscr{P}}}_{\\tau,\\varepsilon,\\delta^{\\prime}}[\\widehat{V}_{h+1}](x,a),}\\end{array}$ , for all $h\\in[H].$ , with $\\delta^{\\prime}$ is defined as in Algorithm 5. ", "page_idx": 58}, {"type": "text", "text": "Proof of Lemma J.2. From Lemma J.1, we have that $\\Pi_{4\\varepsilon}=\\left\\{\\pi^{\\star}\\right\\}$ , and so Theorem I.1 implies that with probability at least $1-\\delta$ , ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\forall h\\in[H],\\quad\\frac{\\varepsilon}{4C_{\\mathrm{push}}H^{3}}\\geq\\mathbb{E}^{\\widehat\\pi}[D_{\\mathrm{tv}}(\\widehat\\pi_{h}(x_{h}),\\pi_{h}^{\\star}(x_{h})\\big]=\\mathbb{P}^{\\widehat\\pi}[\\widehat\\pi_{h}(x_{h})\\neq\\pi_{h}^{\\star}(x_{h})],\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "where the equality follows by the fact that $\\pi^{\\star}$ is deterministic. ", "page_idx": 58}, {"type": "text", "text": "From here, Theorem 4.1 follows swiftly as a consequence. ", "page_idx": 58}, {"type": "text", "text": "Proof of Theorem 4.1 (Setup I). Let $\\widehat{V}_{1:H}$ be the value function estimates produced by $\\mathsf{R V F S}_{\\mathrm{0}}$ within Algorithm 6, and let $\\widehat{\\pi}_{h}^{\\mathsf{R V F S}}(\\cdot)\\,\\in\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}\\widehat{\\mathcal{P}}_{h,\\varepsilon_{\\mathsf{R V F S}},\\delta^{\\prime}}[\\widehat{V}_{h+1}](\\cdot,a)$ , for all $h\\in[H]$ with $\\widehat{V}_{H+1}\\equiv0$ with $\\varepsilon_{\\mathsf{R V F S}}$ and $\\delta^{\\prime}$ as in Algorithm 6. By Lemma J.2, there is an event $\\widetilde{\\mathcal{E}}$ of probability at least $1-\\delta/2$ under which: ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\mathbb{P}^{\\widehat\\pi}[\\widehat\\pi_{h}(\\pmb{x}_{h})\\neq\\pi_{h}^{\\star}(\\pmb{x}_{h})]\\leq\\frac{\\varepsilon_{\\mathsf{R V F S}}}{4H^{3}C_{\\mathsf{p u s h}}}\\leq\\frac{\\varepsilon}{4H^{2}},\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "where the last inequality follows by the choice of $\\varepsilon_{\\mathsf{R V F S}}$ in Algorithm 6. ", "page_idx": 58}, {"type": "text", "text": "For the rest of the proof, we condition on $\\widetilde{\\mathcal{E}}$ . By (63) and Assumption 4.3 ( $\\pi^{\\star}$ -realizability), the policy $\\widehat{\\pi}_{1:H}^{\\mathsf{R V F S}}$ returned by $\\mathsf{R V F S}_{\\mathrm{0}}$ satisfies the condition in Proposition M.1 with $\\varepsilon_{\\mathrm{{mis}}}=\\varepsilon/(4C_{\\mathrm{{push}}}H^{3})$ . Thus, by Proposition M.1, there is an event ${\\widetilde{\\mathcal{E}}}^{\\prime}$ of probability at least $1-\\delta/2$ under which the policies $\\widehat{\\pi}_{1:H}$ produced by RVFS.bc satisfy ", "page_idx": 59}, {"type": "equation", "text": "$$\nJ(\\widehat{\\pi}_{1:H}^{\\mathsf{R V F S}})-J(\\widehat{\\pi}_{1:H})\\leq\\frac{\\varepsilon}{H}+\\frac{\\varepsilon}{2}\\leq\\frac{3\\varepsilon}{2}.\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "We now condition on $\\widetilde{\\mathcal{E}}\\cap\\widetilde{\\mathcal{E}}^{\\prime}$ . By Lemma C.6 (performance difference lemma), we have ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J(\\pi^{\\star})-J(\\widehat{\\pi}_{1:H}^{\\mathsf{R W F S}})=\\displaystyle\\sum_{h=1}^{H}\\mathbb{E}^{\\widehat{\\pi}^{\\mathsf{R W F S}}}[Q_{h}^{\\pi^{\\star}}(x_{h},\\pi_{h}^{\\star}(x_{h}))-Q_{h}^{\\pi^{\\star}}(x_{h},\\widehat{\\pi}_{h}^{\\mathsf{R W F S}}(x_{h}))],}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq H\\displaystyle\\sum_{h=1}^{H}\\mathbb{P}^{\\widehat{\\pi}}[\\widehat{\\pi}_{h}(x_{h})\\neq\\pi_{h}^{\\star}(x_{h})],}\\\\ &{\\qquad\\qquad\\qquad\\leq\\varepsilon/(4H),}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "where the last inequality follows by (63). ", "page_idx": 59}, {"type": "text", "text": "Finally, by the union bound, we have $\\mathbb{P}[\\widetilde{\\mathcal{E}}\\cap\\widetilde{\\mathcal{E}^{\\prime}}]\\ge1-\\delta$ , and so the desired suboptimality guarantee in (28) holds with probability at least $1-\\delta$ . ", "page_idx": 59}, {"type": "text", "text": "Bounding the sample complexity. The sample complexity is dominated by the call to $\\mathsf{R V F S}_{\\mathrm{0}}$ within RVFS.bc (Algorithm 6). Since RVFS.bc calls $\\mathsf{R V F S}_{\\mathrm{0}}$ with $\\dot{\\varepsilon}=\\varepsilon_{\\mathsf{R V F S}}=\\varepsilon H^{-1}/48$ , we conclude from Theorem I.1 that the total number of episodes is bounded by ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\widetilde{\\cal O}\\left(C_{\\mathrm{push}}^{8}H^{23}A\\cdot\\varepsilon^{-13}\\right).\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Bounding the number of oracle calls. We now bound the number of calls to the oracle described in Remark 4.1 that RVFS makes. To do this, note that bounding the number of calls to the oracle is equivalent to bounding the number of executions of Line 14 in Algorithm 5, or, equivalently, the number of times the operator $\\widehat{\\mathcal{P}}$ in Line 14 is invoked during the entire execution of RVFS, including all subsequent recursive calls to RVFS. The proof of Lemma I.2 establishes a direct bound on the number of times the operator $\\widehat{\\mathcal{P}}$ is called, giving an upper limit of $O(C_{\\mathsf{p u s h}}N_{\\mathsf{t e s t}}H^{4}\\varepsilon^{-1})$ , where $N_{\\mathrm{test}}$ is a polynomial in the problem parameters (see Algorithm 5) and does not depend on $\\mathcal{X}$ . ", "page_idx": 59}, {"type": "text", "text": "J.2 Proof of Lemma J.1 (Relaxed $V^{\\pi}$ -Realizability under Gap) ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Proof of Lemma J.1. Fix $\\varepsilon^{\\prime}\\in(0,1)$ and $\\pi\\in\\Pi_{\\varepsilon^{\\prime}}$ . Let $(\\widetilde{Q}_{h}(x,a))_{(h,x,a)\\in[H]\\times\\mathcal{X}\\times A}$ be independent random variables such that ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\forall h\\in[H],\\quad\\pi_{h}(\\cdot)\\in\\underset{a^{\\prime}\\in\\mathcal{A}}{\\arg\\operatorname*{max}}\\widetilde{Q}_{h}(\\cdot,a^{\\prime})\\quad\\mathrm{and}\\quad\\|\\widetilde{Q}_{h}-Q_{h}^{\\pi}\\|_{\\infty}\\leq\\varepsilon^{\\prime},\\ \\mathrm{almost}\\ \\mathrm{surely}.\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Such a collection of random state-action value functions $(\\widetilde{\\pmb{Q}}_{h}(x,a))_{(h,x,a)\\in[H]\\times\\mathcal{X}\\times\\mathcal{A}}$ is guaranteed to exist for $p i$ by the definition of $\\Pi_{\\varepsilon^{\\prime}}$ . We will show via backward induction over $\\ell=H+1,\\ldots,1$ that ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\forall x\\in\\mathcal{X}_{\\ell},\\quad\\pi_{\\ell}(x)=\\pi_{\\ell}^{\\star}(x)\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "almost surely, with the convention that $\\pi_{H+1}\\equiv\\pi_{H+1}^{\\star}\\equiv\\pi_{\\mathsf{u n i f}}$ . This convention makes the base case, $\\ell=H+1$ , hold trivially. ", "page_idx": 59}, {"type": "text", "text": "Now, we consider the general case. Fix $h\\in[H]$ and suppose that (64) holds for all $\\ell\\in[h+1\\ldots H+1]$ We will show that it holds for $\\ell=h$ . ", "page_idx": 59}, {"type": "text", "text": "Thanks to the induction hypothesis, we have for all $x\\in\\mathcal{X}_{h+1}$ and $a\\in\\mathcal{A}$ : ", "page_idx": 59}, {"type": "equation", "text": "$$\nV_{h+1}^{\\pi}(x)=V_{h+1}^{\\star}(x),\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "and so ", "page_idx": 59}, {"type": "equation", "text": "$$\nQ_{h}^{\\pi}\\equiv\\mathcal{P}_{h}[V_{h+1}^{\\pi}]\\equiv\\mathcal{P}_{h}[V_{h+1}^{\\star}]=V_{h}^{\\star}.\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Fix $x\\in\\mathscr{X}$ . We will show that $\\pi_{h}(x)=\\pi_{h}^{\\star}(x)$ almost surely. Note that thanks to (65), the fact that $\\|\\widetilde{\\mathbf{Q}}_{h}-\\mathcal{T}_{h}[Q_{h+1}^{\\pi}]\\|_{\\infty}\\leq\\varepsilon^{\\prime}$ almost surely, implies that ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\|\\widetilde{\\mathbfcal{Q}}_{h}-Q_{h}^{\\star}\\|_{\\infty}\\le\\varepsilon^{\\prime},\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "almost surely. Using this, we have, almost surely ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{h}^{\\star}(x,\\pi_{h}(x))\\geq\\widetilde{Q}_{h}(x,\\pi_{h}(x))-\\varepsilon^{\\prime},}\\\\ &{\\qquad\\qquad\\qquad\\geq\\widetilde{Q}_{h}(x,\\pi_{h}^{\\star}(x))-\\varepsilon^{\\prime},}\\\\ &{\\qquad\\qquad\\qquad\\geq Q_{h}^{\\star}(x,\\pi_{h}^{\\star}(x))-2\\varepsilon^{\\prime}=Q_{h}^{\\star}(x,\\pi_{h}^{\\star}(x))-\\Delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "On the other hand, if $\\pi_{h}(x)\\neq\\pi_{h}^{\\star}(x)$ , then ", "page_idx": 60}, {"type": "equation", "text": "$$\nQ_{h}^{\\star}(x,\\pmb{\\pi}_{h}(x))<Q_{h}^{\\star}(x,\\pi_{h}^{\\star}(x))-\\Delta,\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "which would contradict (66). Thus, $\\pi_{h}(x)=\\pi_{h}^{\\star}(x)$ , which concludes the induction and shows that $\\pi\\equiv\\pi^{\\star}$ . We conclude that $\\Pi_{\\varepsilon^{\\prime}}=\\left\\{\\pi^{\\star}\\right\\}$ . \u53e3 ", "page_idx": 60}, {"type": "text", "text": "K Guarantee for Weakly Correlated ExBMDPs (Proof of Theorem B.1) ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "In this section, we prove Theorem B.1, the main guarantee for $\\mathsf{R V F S}^{\\mathsf{e x o}}$ . First, in Appendix K.1 we state a number of supporting technical lemmas and use them to prove Theorem B.1. The remainder of the section (Appendix K.2 through Appendix K.6) contains the proofs for the intermediate results. ", "page_idx": 60}, {"type": "text", "text": "K.1 Analysis: Proof of Theorem B.1 ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Recall that the the $V^{\\pi}$ -realizability assumption required by RVFS for Theorem 4.1 is not satisfied in ExBMDPs, as the value functions for policies that act on the exogenous noise variables cannot be realized as a function of the true decoder $\\phi^{\\star}$ . In RVF $S^{\\in\\times0}$ , we address this issue by applying the randomized rounding technique in Line 11 to the learned value functions. The crux of the analysis will be to show that for an appropriate choice of the rounding parameters $\\zeta_{1:H}$ , the policies produced by $\\mathsf{R V F S}^{\\mathsf{e x o}}$ are endogenous in the sense that we can write $\\pi(x)=\\pi(\\phi^{\\star}(x))$ for all $x\\in\\mathscr{X}$ . This will allow us to leverage the decoder realizability (Assumption 3.3), which implies that the function class $\\mathcal{V}=\\mathcal{V}_{1:H}$ given by ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\mathcal{V}_{h}:=\\{x\\mapsto f(\\phi(x)):f\\in[0,H]^{S},\\phi\\in\\Phi\\},\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "satisfies $V^{\\pi}$ -realizability for all endogenous policies $\\pi$ . ", "page_idx": 60}, {"type": "text", "text": "In what follows, we first motivate the randomized rounding approach in $\\mathsf{R V F S}^{\\mathsf{e x o}}$ in detail and prove that it succeeds, then use this result to proceed with an analysis similar to that of Theorem 4.1 (Setup II), re-using many of the technical tools developed for Theorem 4.1. ", "page_idx": 60}, {"type": "text", "text": "K.1.1 Randomized Rounding for Endogeneity ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Naively, to ensure that the policies we execute are endogenous, it would seem that we require knowledge of the true decoder $\\phi^{\\star}$ . Alas, knowing $\\phi^{\\star}$ trivializes the ExBMDP problem by reducing it to the tabular setting. To avoid requiring knowledge of $\\phi^{\\star}$ , we apply a randomized rounding to the policies learned by $\\mathsf{R V F S}^{\\mathsf{e x o}}$ to ensure their endogeneity. ", "page_idx": 60}, {"type": "text", "text": "Let $\\varepsilon>0$ be fixed going forward. Recall that compared to RVFS, RVFSexo (Algorithm 8) takes an additional input $\\zeta_{1:H}\\subset(0,1/2)$ and executes the following coarsened policies: ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\widehat{\\pi}_{h}(\\cdot)\\in\\mathop{\\arg\\operatorname*{max}}_{a\\in\\mathcal{A}}[\\widehat{\\mathcal{P}}_{h,\\varepsilon,\\delta}[\\widehat{V}_{h+1}](\\cdot,a)/\\varepsilon+\\zeta_{h}].\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "The rounding parameters $\\zeta_{1:H}$ , which can be thought of as an offset, are chosen randomly; this will be elucidated in the sequel. ", "page_idx": 60}, {"type": "text", "text": "Following a similar analysis to Appendix I (Setup II), we can associate a near-optimal benchmark policy $\\tilde{\\pi}\\in\\Pi_{2\\varepsilon}$ with $\\widehat{\\pi}$ in order to emulate certain properties of the $\\Delta$ -gap assumption. In particular, generalizing the construction in Eq. (27), we define a near-optimal benchmark policy $\\tilde{\\pi}$ recursively via: ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\langle x\\in\\mathcal{X},\\ \\tilde{\\pi}_{\\tau}(x;\\zeta_{1:H},\\varepsilon,\\delta)\\in\\arg\\operatorname*{max}\\left\\{\\begin{array}{l l}{[\\widehat{Q}_{\\tau}(x,a)/\\varepsilon+\\zeta_{\\tau}],}&{\\mathrm{if~}\\Vert\\widehat{Q}_{\\tau}(x,\\cdot)-\\mathcal{P}_{\\tau}[V_{\\tau+1}^{\\tilde{\\pi}}](x,\\cdot)\\Vert_{\\infty}\\leq1,}\\\\ {[\\mathcal{P}_{\\tau}[V_{\\tau+1}^{\\tilde{\\pi}}](x,a)/\\varepsilon+\\zeta_{\\tau}],}&{\\mathrm{otherwise},}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Naively, to use the benchmark policy $\\tilde{\\pi}$ within the analysis based on relaxed $V^{\\pi}$ -realizability (Assumption I.1) in Appendix I , we would require the function class $\\nu$ to realize $\\big(V_{h}^{\\tilde{\\pi}}\\big)$ . However, as argued earlier, this is not feasible unless $\\tilde{\\pi}$ is an endogenous policy. Fortunately, it turns out that if $\\zeta_{1:H}$ (the additional input to $\\mathsf{R V F S}^{\\mathsf{e x o}}$ ) are drawn randomly from uniform distribution over $[0,1/2]$ , then with constant probability, $\\tilde{\\pi}$ is indeed endogenous. What\u2019s more, under such an event, and for all possible choices of $\\overline{{(\\widehat{V}_{h}}})$ in (68) uniformly, $\\tilde{\\pi}$ \u201csnaps\u201d onto the stochastic endogenous policy $\\bar{\\pi}(\\cdot;\\zeta_{1:H},\\varepsilon)$ defined recursively as follows: ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\bar{\\pi}_{h}\\bigl(\\cdot;\\zeta_{1:H},\\varepsilon\\bigr)\\in\\underset{a\\in\\mathcal{A}}{\\arg\\operatorname*{max}}\\bigl[\\mathcal{P}_{h}[V_{h+1}^{\\bar{\\pi}}](x,a)/\\varepsilon+\\zeta_{h}\\bigr],\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "for $h\\ =\\ H,\\ldots,1$ . Informally, this happens because, as long as $\\zeta_{1:H}~\\subset~(0,1/2)$ avoid certain pathological locations in $(0,1/2)$ , the coarsened state-action value functions $\\varepsilon\\colon\\lceil\\mathcal{P}_{\\tau}^{\\cdot}[\\dot{V}_{\\tau+1}^{\\bar{\\pi}}](x,a)/\\varepsilon+$ $\\zeta_{h}]$ defining $\\bar{\\pi}$ exhibit a \u201cgap\u201d of order $\\Theta(\\varepsilon^{2})$ separating optimal actions from the rest. This \u201csnapping\u201d behavior is analogous to what happens in Setup I with $V^{\\star}$ -realizability and $\\Delta$ -gap, where $\\Pi_{\\varepsilon}$ reduces to $\\{\\pi^{\\star}\\}$ for all $\\varepsilon<\\Delta/2$ (see Lemma J.1). We formalize these claims in the next two lemmas. We start by showing that $\\bar{\\pi}$ is endogenous and that $\\bar{\\pi}\\in\\Pi_{2\\varepsilon}$ . The proof is in Appendix K.2. ", "page_idx": 61}, {"type": "text", "text": "Lemma K.1 (Endogenous Benchmark policies). For any $\\delta\\in(0,1)$ , $\\varepsilon\\in(0,1/2)$ , and $\\zeta_{1:H}\\subset(0,1/2)$ , the stochastic policy $\\bar{\\pi}(\\cdot;\\zeta_{1:H},\\varepsilon)$ defined in $E q$ . (69) is endogenous, and we have $\\bar{\\pi}(\\cdot;\\zeta_{1:H},\\varepsilon)\\in\\Pi_{2\\varepsilon}$ . ", "page_idx": 61}, {"type": "text", "text": "Next, we show that $\\tilde{\\pi}$ \u201csnaps\u201d onto $\\bar{\\pi}$ for the certain choices of $\\zeta_{1:H}$ . The proof is in Appendix K.3. ", "page_idx": 61}, {"type": "text", "text": "Lemma K.2 (Snapping probability). Let $\\delta\\ \\in\\ (0,1)$ , $\\varepsilon\\ \\in\\ (0,1/2)$ be given, and $\\mathbb{P}\\zeta$ denote the probability law of $\\zeta_{1},\\dots,\\zeta_{H}\\sim\\mathsf{u n i f}\\big([0,1/2]\\big)$ . Then, there is an event $\\mathcal{E}_{\\mathrm{rand}}$ of probability at least $1-24S A H\\varepsilon$ under $\\zeta_{1:H}\\sim\\mathbb{P}^{\\zeta}$ such that for all $\\widetilde{V}\\in(\\mathcal{X}\\times[H]\\rightarrow[0,H])$ simultaneously, ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\forall h\\in[H],\\quad\\tilde{\\pi}_{h}(\\cdot;\\widetilde{V},\\zeta_{1:H},\\varepsilon,\\delta)=\\bar{\\pi}_{h}\\big(\\cdot;\\zeta_{1:H},\\varepsilon\\big),\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "where $\\tilde{\\pi}_{h}(\\cdot;\\widetilde{V},\\zeta_{1:H},\\varepsilon,\\delta)$ is defined as in (68) with $\\widehat V=\\widetilde V$ , and $\\bar{\\pi}$ is defined as in (69). ", "page_idx": 61}, {"type": "text", "text": "The lemma together, with Lemma K.1, implies that with constant probability under $\\mathbb{P}\\zeta$ , the benchmark policies $\\left(\\tilde{\\pi}_{h}\\right)$ used in the analysis of RVF $S^{\\mathrm{exo}}$ are endogenous and satisfy $\\tilde{\\pi}\\in\\Pi_{2\\varepsilon}$ . ", "page_idx": 61}, {"type": "text", "text": "K.1.2 Pushforward Coverability ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "In order to proceed with the analysis strategy in Appendix I, we need to verify that pushforward coverability is satisfied for ExBMDPs under the weak correlation assumption. We do so in the next lemma; see Appendix K.4 for a proof. ", "page_idx": 61}, {"type": "text", "text": "Lemma K.3 (Pushforward coverability). A weakly correlated ExBMDP with constant $C_{\\mathrm{exo}}$ (see Assumption B.1) satisfies $C_{\\mathsf{p u s h}}$ -pushforward coverability (Assumption 4.1) with $C_{\\mathsf{p u s h}}=C_{\\mathsf{e x o}}\\cdot S A_{\\mathsf{i}}$ , where $S\\in\\mathbb{N}$ is the number of endogenous states. ", "page_idx": 61}, {"type": "text", "text": "Equipped with the preceding lemmas, we proceed with an analysis similar to the approach for Theorem 4.1 (Setup $\\mathbf{II}$ ) in Appendix I. In what follows, we state a number of technical lemmas that apply the relevant results from Appendix I to the ExBMDP setting we consider here. ", "page_idx": 61}, {"type": "text", "text": "K.1.3 Bounding the Number of Test Failures ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Since the size of the core sets $\\mathcal{C}_{1:H}$ in RVF $S^{\\mathrm{exo}}$ is directly proportional to the number of test failures, the next result, which bounds $|{\\mathcal{C}}_{h}|$ for all $h\\in[H]$ , allows us to show that $\\mathsf{R V F S}^{\\mathsf{e x o}}$ (Algorithm 8) terminates in a polynomial number of iterations. ", "page_idx": 61}, {"type": "text", "text": "Lemma K.4 (Bounding the number of test failures). Let $\\delta,\\varepsilon\\in(0,1)$ and $\\zeta_{1:H}\\in[0,1/2]$ be given, and suppose that Assumption B.1 (weak correlation) holds with $C_{\\mathsf{e x}\\circ}>0$ . Let $f\\in\\mathcal{V}$ , be given, where $\\nu$ is an arbitrary function class. Then, there is an event $\\mathcal{E}$ of probability at least $1-\\delta$ under which the cofa lRl VtoF $\\mathsf{R V F S}_{0}^{\\mathrm{exo}}\\bar{(f,\\mathcal{V}^{H},\\emptyset,\\emptyset,0;\\mathcal{V},\\varepsilon,\\zeta_{1:H},\\delta)}$ (Algorithm 8) terminates, and throughout the execution $\\mathsf{S}_{0}^{\\mathsf{e x o}}$ ", "page_idx": 61}, {"type": "equation", "text": "$$\n|\\mathcal{C}_{h}|\\leq\\lceil8\\varepsilon^{-2}C_{\\tt e x o}S A H\\rceil.\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "Proof of Lemma K.4. The results follows from Lemma K.3 and Lemma I.1. ", "page_idx": 61}, {"type": "text", "text": "K.1.4 Value Function Regression Guarantee ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "We next give a guarantee for the estimated value functions $\\widehat{V}_{1:H}$ computed within $\\mathsf{R V F S}^{\\mathsf{e x o}}$ in Line 26 of Algorithm 8. ", "page_idx": 62}, {"type": "text", "text": "Lemma K.5 (Value function regression guarantee). Let $h\\in[0\\ldots H]$ , $\\delta,\\varepsilon\\in(0,1)$ , and $\\zeta_{1:H}\\in[0,1/2]$ be given, and consider a call to RVF $\\mathsf{S}_{0}^{\\mathsf{e x o}}$ in the setting of Lemma K.4. Further, let $\\nu$ be defined as in Eq. (67), and assume that $\\Phi$ satisfies Assumption 3.3. Then, for any endogenous policy $\\pi$ in $\\Pi_{\\ S}$ , there is an event $\\mathcal{E}_{h}^{\\prime\\prime}$ of probability at least $1-\\delta/H$ under which for any $k\\geq1$ , $i f$ ", "page_idx": 62}, {"type": "text", "text": "\u2022 RVF $S_{h}^{\\mathsf{e x o}}$ gets called for the kth time during the execution of $\\mathsf{R V F S}_{\\mathrm{0}}^{\\mathsf{e x o}}$ ; and \u2022 this kth call terminates and returns $(\\widehat{V}_{h:H},\\widehat{\\mathcal{V}}_{h:H},\\mathcal{C}_{h:H},\\mathcal{B}_{h:H},t_{h:H}),$ ", "page_idx": 62}, {"type": "text", "text": "then $i f(\\widehat{\\pi}_{\\tau})_{\\tau\\geq h}$ is the policy induced by $\\widehat{V}_{h:H}$ and $N_{\\mathsf{r e g}}$ is set as in Algorithm 8, we have ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{x_{h-1},a_{h-1})\\in\\mathcal{C}_{h}}\\frac{1}{N_{\\mathrm{reg}}}\\sum_{(x_{h},-)\\in\\mathcal{D}_{h}(x_{h-1},a_{h-1})}\\left(\\widehat{V}_{h}(x_{h})-V_{h}^{\\pi}(x_{h})\\right)^{2}}\\\\ &{\\le\\displaystyle\\frac{9k H^{2}\\log(8k^{2}H|\\mathcal{V}|/\\delta)}{N_{\\mathrm{reg}}}+8H^{2}\\sum_{(x_{h-1},a_{h-1})\\in\\mathcal{C}_{h}}\\sum_{\\tau=h}^{H}\\mathbb{E}^{\\widehat\\pi}\\left[D_{\\mathrm{tv}}(\\widehat{\\pi}_{\\tau}(x_{\\tau}),\\pi_{\\tau}(x_{\\tau}))\\mid x_{h-1}=x_{h-1},a_{h-1}=\\delta\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "where the datasets $\\{\\mathcal{D}_{h}(x,a):(x,a)\\in\\mathcal{C}_{h}\\}$ are as in the definition of $\\widehat{\\nu}_{h}$ in (16). ", "page_idx": 62}, {"type": "text", "text": "Proof of Lemma K.5. Since $\\Phi$ satisfies Assumption 3.3, the function class $\\mathcal{V}\\,=\\,\\mathcal{V}_{1:H}$ satisfies $V^{\\pi}$ -realizability for all endogenous policies $\\pi$ (see Lemma D.1). Thus, the proof of Lemma K.5 follows from that of Lemma I.3 (see Appendix I.4). \u53e3 ", "page_idx": 62}, {"type": "text", "text": "K.1.5 Confidence Sets ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "We now state a version of the confidence set validity lemma (Lemma I.4) that supports the ExBMDP setting. ", "page_idx": 62}, {"type": "text", "text": "Lemma K.6 (Confidence sets). Let $\\varepsilon\\in(0,1/2)$ and $\\zeta_{1:H}\\subset[0,1/2]$ be given, and suppose that ", "page_idx": 62}, {"type": "text", "text": "\u2022 Assumption B.1 holds with $C_{\\mathsf{e x o}}>0$ ;   \n\u2022 The decoder class $\\Phi$ satisfies Assumption 3.3;   \n\u2022 $\\zeta_{1:H}\\in\\mathcal{E}_{r a n d}$ , where $\\mathcal{E}_{\\mathrm{rand}}$ is the event in Lemma K.2. ", "page_idx": 62}, {"type": "text", "text": "Let $f\\in\\mathcal{V}$ be arbitrary. There is an event $\\mathcal{E}^{\\prime\\prime\\prime}$ of probability at least $1-3\\delta$ under which a call to $\\mathsf{R V F S}_{0}^{\\mathrm{exo}}(f,\\mathcal V,\\mathcal D,\\emptyset,0;\\mathcal V,\\varepsilon,\\zeta_{1:H},\\delta)$ terminates and returns tuple $(\\widehat{V}_{1:H},\\widehat{\\mathcal{V}}_{1:H},\\mathcal{C}_{1:H},\\mathcal{B}_{1:H},t_{1:H})$ such that ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\forall h\\in[H],\\quad V_{h}^{\\bar{\\pi}}\\in\\widehat{\\mathcal{V}}_{h},\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "where $\\bar{\\pi}_{1:H}$ is the policy defined recursively via ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\bar{\\pi}_{\\tau}(x)\\in\\operatorname*{arg\\,max}_{a\\in\\cal{A}}[\\mathcal{P}_{\\tau}[V_{\\tau+1}^{\\bar{\\pi}}](x,a)/\\varepsilon+\\zeta_{h}],\\quad f o r\\,\\tau=H,\\dots,1.\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "While the proof of this lemma is very similar to that of Lemma I.4, we need a dedicated treatment to handle the rounding in $\\mathsf{R V F S}^{\\mathsf{e x o}}$ . The fully proof of Lemma K.8 is in Appendix I.5. ", "page_idx": 62}, {"type": "text", "text": "K.1.6 Main Guarantee for RVFSexo ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "We now state the central technical guarantee for $\\mathsf{R V F S}^{\\mathsf{e x o}}$ , Lemma K.7, which shows that the base invocation of the algorithm returns a set of value functions $\\widehat{V}_{1:H}$ that induce a near-optimal policy $\\widehat{\\pi}$ , as long as the randomized rounding parameters $\\zeta_{1:H}$ satisfy $\\zeta_{1:H}\\in\\mathcal{E}_{r a n d}$ , where $\\mathcal{E}_{\\mathrm{rand}}$ is the success event in Lemma K.2. The proof of the theorem is in Appendix K.6. ", "page_idx": 62}, {"type": "text", "text": "Lemma K.7 (Main guarantee for $\\mathsf{R V F S}^{\\mathsf{e x o}}$ ). Let $\\delta,\\varepsilon\\in(0,1)$ and $\\zeta_{1:H}\\subset[0,\\frac{1}{2}]$ be given, and suppose that ", "page_idx": 62}, {"type": "text", "text": "\u2022 Assumption B.1 holds with $C_{\\mathsf{e x o}}>0$ ;   \n\u2022 The decoder class $\\Phi$ satisfies Assumption 3.3;   \n\u2022 $\\zeta_{1:H}\\in\\mathcal{E}_{r a n d}$ , where $\\mathcal{E}_{\\mathrm{rand}}$ is the event in Lemma $K.2$ . ", "page_idx": 62}, {"type": "text", "text": "Then, for any $f\\in\\mathcal{V}$ , with probability at least $1-5\\delta$ , a call to $\\mathsf{R V F S}_{0}^{\\mathsf{e x o}}(f,\\mathcal{V}^{H},\\emptyset,\\emptyset,0;\\mathcal{V},\\varepsilon,\\zeta_{1:H},\\delta)$ (Algorithm 8) terminates and returns value functions $\\widehat{V}_{1:H}$ such that ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\forall h\\in[H],\\quad\\mathbb{P}^{\\widehat\\pi}[\\widehat\\pi_{h}(x_{h})\\neq\\bar{\\pi}_{h}(x_{h})]\\leq\\frac{\\varepsilon^{2}}{4H^{3}S A C_{\\mathrm{ex}}},\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "where $\\begin{array}{r}{\\widehat{\\pi}_{h}(x)\\in\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}[\\widehat{\\mathcal{P}}_{h,\\varepsilon,\\delta^{\\prime}}[\\widehat{V}_{h+1}](x,a)/\\varepsilon+\\zeta_{h}],}\\end{array}$ for all $h\\in[H].$ , $\\bar{\\pi}$ is defined as in Eq. (69), and $\\delta^{\\prime}$ is defined as in AlgorithPm 8. Furthermore, the number of episodes used by RVF $\\mathsf{S}_{0}^{\\mathsf{e x o}}$ is bounded by ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\widetilde{\\cal O}\\left(C_{\\mathrm{exo}}^{8}S^{8}H^{10}A^{9}\\cdot\\varepsilon^{-26}\\right).\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "K.1.7 Concluding: Main Guarantee for $\\mathsf{R V F S}^{\\mathsf{e x o}}$ .bc ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "To conclude, we prove Theorem B.1, which shows that $\\mathsf{R V F S}^{\\mathsf{e x o}}$ .bc succeeds with high probability. Recall that $\\mathsf{R V F S}^{\\mathsf{e x o}}$ .bc (i) invokes RVF $S^{\\in\\times0}$ multiple times for random samples $\\zeta_{1:H}$ to ensure that the success event for Lemma K.7 occurs for at least one invocation, and (ii) extracts an executable policy using behavior cloning. Regarding the former point, note that the probability of the success event aosf  lLoenmg masa .is2  pcoalny bneo mbioaolsltye lda rbgye ,s awmitphl ihnigg hi. ip.dr.o $\\zeta_{1:H}^{(1)},\\dots,\\zeta_{1:H}^{(n)}\\sim\\mathbb{P}^{\\zeta}$ ionf ptuhtes  itnop $\\mathsf{R V F S}^{\\mathsf{e x o}}$ $n\\geq1$ $n$ $\\zeta_{1:H}^{(1)},\\ldots,\\zeta_{1:H}^{(n)}$ will satisfy the conclusion of Lemma K.2. Thus, it suffices to pick the policy with the highest value function among the different calls to $\\mathsf{R V F S}^{\\mathsf{e x o}}$ . Using this, we prove Theorem B.1. ", "page_idx": 63}, {"type": "text", "text": "Proof of Theorem B.1. Recall that Algorithm 9 picks the final policy \u03c0\u0302(1i\u2236oHpt) based on empirical value function estimates. In particular, for every $i\\in[N_{\\mathrm{boost}}]$ (with $N_{\\mathrm{boost}}$ as in Algorithm 9), the estimate $\\widehat{J}(\\widehat{\\pi}_{1:H}^{(i)})$ for $J(\\widehat{\\pi}_{1:H}^{(i)})$ is computed using $N_{\\mathrm{eval}}$ episodes. Thus, by Hoeffding\u2019s inequality and the union bound, we have that there is an event $\\breve{\\mathscr E}$ of probability at least $1-\\delta/4$ under which ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\forall i\\in\\big[N_{\\mathrm{boost}}\\big],\\quad\\big|J\\big(\\widehat{\\pi}_{1:H}^{(i)}\\big)-\\widehat{J}\\big(\\widehat{\\pi}_{1:H}^{(i)}\\big)\\big|\\leq\\sqrt{2\\log\\bigl(2N_{\\mathrm{boost}}/\\delta\\bigr)/N_{\\mathrm{eval}}}.\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "Therefore, by definition of $i_{\\tt o p t}$ in Algorithm 9, we have that under $\\breve{\\mathscr{E}}$ : ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\forall i\\in[N_{\\mathrm{boost}}],\\quad J(\\widehat\\pi_{1:H}^{(i)})\\leq J(\\widehat\\pi_{1:H}^{(i_{\\mathrm{opt}})})+\\sqrt{2\\log(2N_{\\mathrm{boost}}/\\delta)/N_{\\mathrm{eval}}},}\\\\ &{\\qquad\\qquad\\leq J(\\widehat\\pi_{1:H}^{(i_{\\mathrm{opt}})})+\\varepsilon/8,}\\end{array}\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "where the last inequality follows by the choice of $N_{\\mathsf{e v a l}}$ in Algorithm 9. On the other hand, by Lemma K.2, there is an event $\\mathcal{E}^{\\mathtt{s u c c e s s}}$ of $\\mathbb{P}\\zeta$ -probability at least ", "page_idx": 63}, {"type": "equation", "text": "$$\n1-(24S A H\\varepsilon)^{N_{\\mathrm{boost}}}\\geq1-\\delta/4\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "under which there exists $j\\in\\left[N_{\\mathrm{boost}}\\right]$ such that $\\zeta_{1:H}^{(j)}\\in\\mathcal{E}_{\\sf r a n d}$ , where $\\mathcal{E}_{\\mathrm{rand}}$ is defined as in Lemma K.2.   \nIn what follows, we condition on the event $\\mathcal{E}^{\\mathtt{s u c c e s s}}$ and let $j\\in\\left[N_{\\mathrm{boost}}\\right]$ be such that $\\zeta_{1:H}^{(j)}\\in\\mathcal{E}_{\\sf r a n d}$   \nFurther, we use $\\widehat{\\pi}_{1:H}^{\\mathsf{R V F S}}$ to denote the policy returned by the instance of $\\mathsf{R V F S}^{\\mathsf{e x o}}$ that is used by   \n$\\mathsf{R V F S}^{\\mathsf{e x o}}$ .bc to learn $\\widehat{\\pi}_{1:H}^{(j)}$ 1\u2236H ", "page_idx": 63}, {"type": "text", "text": "By Proposition M.1 (instantiated with $\\varepsilon_{\\mathfrak{m}\\mathrm{i}s}=0$ ), there is an event ${\\widetilde{\\mathcal{E}}}^{\\prime}$ of probability at least $1-\\delta/4$ under which the policy $\\widehat{\\pi}^{(j)}$ produced by BehaviorCloning satisfies ", "page_idx": 63}, {"type": "equation", "text": "$$\nJ\\big(\\widehat{\\pi}_{1:H}^{\\mathsf{R V F S}}\\big)-J\\big(\\widehat{\\pi}_{1:H}^{(j)}\\big)\\leq\\frac{\\varepsilon}{2}.\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "By Lemma K.7 and the fact that $\\zeta_{1:H}^{(j)}\\in\\mathcal{E}_{\\mathsf{r a n d}}$ , there is an event $\\widetilde{\\mathcal{E}}$ of probability at least $1-\\delta/2$ under which: ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\forall h\\in[H],\\quad\\forall h\\in[H],\\quad\\mathbb{P}^{\\widehat\\pi}[\\widehat\\pi_{h}(x_{h})\\neq\\bar{\\pi}_{h}(x_{h})]\\leq\\frac{\\varepsilon_{\\mathtt{R W F S}}^{2}}{4H^{3}S A C_{\\mathrm{exo}}}\\leq\\frac{\\varepsilon}{4H^{2}},\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "where $\\bar{\\pi}\\big(\\cdot\\big):=\\bar{\\pi}\\big(\\cdot;\\zeta_{1:H}^{(j)},\\varepsilon_{\\mathsf{R V F S}}\\big)$ which is defined in (69). ", "page_idx": 63}, {"type": "text", "text": "Moving forward, we condition on $\\breve{\\mathscr E}\\cap\\widetilde{\\mathscr E}\\cap\\widetilde{\\mathscr E^{\\prime}}$ . By Lemma C.6 (the performance difference lemma), we have ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J\\big(\\bar{\\pi}\\big)-J\\big(\\widehat{\\pi}_{1:H}^{\\mathsf{R V F S}}\\big)=\\displaystyle\\sum_{h=1}^{H}\\mathbb{E}^{\\widehat{\\pi}^{\\mathsf{R V F S}}}\\big[Q_{h}^{\\widetilde{\\pi}}(x_{h},\\bar{\\pi}_{h}(x_{h}))-Q_{h}^{\\bar{\\pi}}\\big(x_{h},\\widehat{\\pi}_{h}^{\\mathsf{R V F S}}(x_{h})\\big)\\big],}\\\\ &{\\qquad\\qquad\\qquad\\leq H\\displaystyle\\sum_{h=1}^{H}\\mathbb{P}^{\\widehat{\\pi}}\\big[\\widehat{\\pi}_{h}(x_{h})\\neq\\bar{\\pi}_{h}(x_{h})\\big],}\\\\ &{\\qquad\\qquad\\qquad\\leq\\varepsilon/4,}\\end{array}\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "where the last inequality follows by (73). Now, by Lemma K.1, we have $\\bar{\\pi}\\,\\in\\,\\Pi_{2\\varepsilon_{\\mathtt{R V F S}}}$ , and so by Lemma H.1, ", "page_idx": 64}, {"type": "equation", "text": "$$\nJ(\\pi^{\\star})-J(\\bar{\\pi})\\le6H\\varepsilon_{\\sf R V F S}\\le\\varepsilon/8,\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "where the last inequality follows by the choice of $\\varepsilon_{\\mathsf{R V F S}}$ in Algorithm 9. Combining (75) with (71), (72), and (74), we get that ", "page_idx": 64}, {"type": "equation", "text": "$$\nJ(\\pi^{\\star})-J(\\widehat{\\pi}_{1:H})=J(\\pi^{\\star})-J(\\widehat{\\pi}_{1:H}^{(i_{\\mathrm{opt}})})\\leq\\varepsilon.\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "Finally, by the union bound, we have $\\mathbb{P}[\\breve{\\mathscr{E}}\\cap\\widetilde{\\mathscr{E}}\\cap\\widetilde{\\mathscr{E}^{\\prime}}]\\geq1-\\delta$ , and so the desired suboptimality guarantee holds with probability at least $1-\\delta$ . ", "page_idx": 64}, {"type": "text", "text": "Bounding the sample complexity. The sample complexity is dominated by the calls to $\\mathsf{R V F S}_{\\mathrm{0}}^{\\mathsf{e x o}}$ within $\\mathsf{R V F S}^{\\mathsf{e x o}}$ .bc (Algorithm 9). Since RVF $S^{\\mathrm{exo}}$ .bc calls $\\mathsf{R V F S}_{\\mathrm{0}}^{\\mathrm{exo}}$ with suboptimality parameter $\\varepsilon_{\\mathsf{R V F S}}=\\varepsilon H^{-1}/48$ , we get by Lemma K.7 that the total sample complexity is bounded by ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\widetilde{\\cal O}\\left(C_{\\mathrm{exo}}^{8}S^{8}H^{36}A^{9}\\cdot\\varepsilon^{-26}\\right).\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "K.2 Proof of Lemma K.1 (Endogenous Benchmark Policies) ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Proof of Lemma K.1. Fix $\\delta\\in(0,1)$ , $\\varepsilon\\,\\in\\,(0,1/2)$ , and $\\zeta_{1:H}^{\\prime}\\,\\subset\\,[0,1/2]$ . We show via backward induction over $\\ell\\,=\\,H\\,+\\,1,\\ldots,1$ that $\\bar{\\pi}_{\\tau}(\\cdot;\\zeta_{1:H}^{\\prime},\\varepsilon)$ is endogenous for all $\\tau\\in[\\ell\\ldots H+1]$ , with the convention that $\\bar{\\pi}_{H+1}=\\pi_{\\mathrm{unif}}$ . The base case holds trivially by convention. ", "page_idx": 64}, {"type": "text", "text": "Fix $h\\in[H]$ and suppose that the induction hypothesis holds for all $\\ell\\in\\left[h+1\\ldots H\\!+\\!1\\right]$ . We show that it holds for $\\ell=h$ . First, by the induction hypothesis, $\\bar{\\pi}_{\\ell}(\\cdot;\\zeta_{1:H}^{\\prime},\\varepsilon)$ is endogenous for all $\\ell\\in[h+1\\ldots H]$ Thus, there exists a function $f_{h+1}:S\\to\\bar{[0,H-h]}$ such that ", "page_idx": 64}, {"type": "equation", "text": "$$\nV_{h+1}^{\\bar{\\pi}}(x^{\\prime})=f_{h+1}(\\phi^{\\star}(x^{\\prime})),\\quad\\forall x^{\\prime}\\in\\mathcal{X}.\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "Therefore, we have for all $(x,a)\\in\\mathcal{X}\\times\\mathcal{A}$ : ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{P}_{h}[V_{h+1}^{\\bar{\\pi}}](x,a)=r_{h}(x,a)+\\mathbb{E}\\big[f_{h+1}(\\phi^{\\star}(x_{h+1}))\\mid x_{h}=x,a_{h}=a\\big],}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=r_{h}(x,a)+\\mathbb{E}\\big[f_{h+1}(s_{h+1})\\mid x_{h}=x,a_{h}=a\\big],}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=r_{h}(x,a)+\\mathbb{E}\\big[f_{h+1}(s_{h+1})\\mid s_{h}=\\phi^{\\star}(x),a_{h}=a\\big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "where the last equality follows by the ExBMDP transition structure. Eq. (76) together with the fact that the rewards are endogenous (by assumption) implies that there exists $g_{h}:S\\times{\\cal A}\\to[0,H-h+1]$ such that ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\forall(x,a)\\in\\mathcal{X}\\times\\mathcal{A},\\quad\\mathcal{P}_{h}[V_{h+1}^{\\bar{\\pi}}](x,a)=g_{h}(\\phi^{\\star}(x),a),\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "which in turn implies that $x\\mapsto\\lceil\\mathcal{P}_{h}[V_{h+1}^{\\bar{\\pi}}](x,a)/\\varepsilon+\\zeta_{h}^{\\prime}\\rceil$ is only a function of $x$ through $\\phi^{\\star}(x)$ for all $a\\in\\mathcal{A}$ . Thus, $\\bar{\\pi}_{h}$ is an endogenous policy and the induction is completed. ", "page_idx": 64}, {"type": "text", "text": "For the second claim, observe that for the functions $\\widetilde{Q}_{1},\\cdots,\\widetilde{Q}_{H}\\in[0,H]^{\\mathcal{X}\\times\\mathcal{A}}$ defined as ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\forall h\\in[H],\\forall(x,a)\\in\\mathcal{X}\\times\\mathcal{A},\\quad\\widetilde{Q}_{h}(x,a)=\\varepsilon\\cdot\\lceil\\mathcal{P}_{h}[V_{h+1}^{\\bar{\\pi}}](x,a)/\\varepsilon+\\zeta_{h}^{\\prime}\\rceil,\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "we have ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\forall h\\in[H],\\quad\\overline{\\pi}_{h}\\big(\\cdot;\\zeta_{1:H}^{\\prime},\\varepsilon\\big)\\in\\underset{a\\in\\mathcal{A}}{\\arg\\operatorname*{max}}\\widetilde Q_{h}\\big(\\cdot,a\\big)\\quad\\mathrm{and}\\quad\\|\\widetilde Q_{h}-Q_{h}^{\\pi}\\|_{\\infty}\\leq2\\varepsilon,\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "which implies that $\\bar{\\pi}(\\cdot;\\zeta_{1:H}^{\\prime},\\varepsilon)\\in\\Pi_{2\\varepsilon}$ . ", "page_idx": 64}, {"type": "text", "text": "K.3 Proof of Lemma K.2 (Snapping Probability) ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Proof of Lemma K.2. Fix $\\varepsilon\\,\\in\\,(0,1)$ and $\\delta\\,\\in\\,(0,1/2)$ . For $\\tau\\,\\leq\\,\\ell\\,\\in\\,[H]$ , let $\\mathbb{P}_{\\tau:\\ell}^{\\zeta}$ denote the probability law of $\\zeta_{\\tau},\\dots,\\zeta_{\\ell}$ . We also use the shorthand $\\mathbb{P}_{\\tau}^{\\zeta}$ for $\\mathbb{P}_{\\tau:\\tau}^{\\zeta}$ , for all $\\tau\\in[H]$ . We show via backward induction over $\\ell=H+1,\\ldots,1$ that there exists an event $\\mathscr{E}_{\\ell}$ of $\\mathbb{P}_{\\ell:H}^{\\zeta}$ -probability at least $1-24S A(H-\\ell+1)\\varepsilon$ under which for all $\\widetilde{V}\\in(\\mathcal{X}\\times[H]\\rightarrow[0,H])$ : ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\forall\\tau\\in[\\ell\\ldots H],\\quad\\tilde{\\pi}_{\\tau}(\\cdot;\\widetilde{V},\\zeta_{1:H},\\varepsilon,\\delta)=\\bar{\\pi}_{\\tau}(\\cdot;\\zeta_{1:H},\\varepsilon),\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "with the convention that $\\bar{\\pi}_{H+1}\\equiv\\tilde{\\pi}_{H+1}\\equiv\\pi_{\\mathsf{u n i f}}$ . We then set $\\mathcal{E}_{\\mathrm{rand}}=\\mathcal{E}_{1}$ . ", "page_idx": 65}, {"type": "text", "text": "The base case follows trivially by convention. ", "page_idx": 65}, {"type": "text", "text": "We now proceed with the inductive step. Fix $h\\in[H]$ and suppose that the induction hypothesis holds for all $\\ell\\in[h+1\\ldots H]$ . We show that it holds for $\\ell=h$ . Throughout, we condition on $\\mathcal{E}_{h+1}$ . By definition of $\\mathcal{E}_{h+1}$ , we have for all $\\widetilde{V}\\in(\\mathcal{X}\\times[H]\\rightarrow[0,H])$ : ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\forall\\ell\\in[h+1\\ldots H],\\quad\\bar{\\pi}_{\\ell}(\\cdot;\\zeta_{1:H},\\varepsilon)=\\tilde{\\pi}_{\\ell}(\\cdot;\\widetilde{V},\\zeta_{1:H},\\varepsilon,\\delta).\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "This implies that for all $\\widetilde{V}\\in(\\mathcal{X}\\times[H]\\rightarrow[0,H])$ : ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathit{\\Pi}^{\\prime}x\\in\\mathcal{X},\\;\\;\\tilde{\\pi}_{h}(x;\\widetilde{V},\\zeta_{1:H},\\varepsilon,\\delta)\\in\\arg\\operatorname*{max}\\left\\{\\begin{array}{l l}{[\\widehat{Q}_{h}(x,a)/\\varepsilon+\\zeta_{h}],}&{\\mathrm{if~}\\lVert\\widehat{Q}_{h}(x,\\cdot)-\\mathcal{P}_{h}[V_{h+1}^{\\pi}](x,\\cdot)}\\\\ {\\left[\\mathcal{P}_{h}[V_{h+1}^{\\pi}](x,a)/\\varepsilon+\\zeta_{h}\\right],}&{\\mathrm{otherwise},}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "by the definition of $\\tilde{\\pi}_{h}$ in (68), where $\\widehat{\\mathbf{Q}}_{h}(\\cdot,a):=\\widehat{\\mathcal{P}}_{h,\\varepsilon,\\delta}[\\widetilde{V}_{h+1}](\\cdot,a)$ . From this, we see that to prove $\\tilde{\\pi}_{h}(\\cdot;\\widetilde{V},\\zeta_{1:H},\\varepsilon,\\delta)=\\bar{\\pi}_{h}(\\cdot;\\zeta_{1:H},\\varepsilon)$ for all $\\widetilde{V}$ , it suffices to show that for all $x\\in\\mathscr{X}$ and $\\widetilde{V}$ , $\\hat{\\gamma}_{h}(x,a)/\\varepsilon+\\zeta_{h}\\,{\\ l}=\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}[\\mathcal{P}_{h}[V_{h+1}^{\\bar{\\pi}}](x,a)/\\varepsilon+\\zeta_{h}],\\mathrm{~whenever~}\\,|\\widehat{Q}_{h}(x,a)-\\mathcal{P}_{h}[V_{h+1}^{\\bar{\\pi}}](x,a)|\\le4\\varepsilon^{2}.$ Observe that a sufficient condition for this to hold is that ", "page_idx": 65}, {"type": "text", "text": "", "page_idx": 65}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{/x\\in\\mathcal{X},\\forall a\\in\\mathcal{A},\\forall\\delta\\in[-4\\varepsilon^{2},4\\varepsilon^{2}],\\quad\\left[(\\mathcal{P}_{h}[V_{h+1}^{\\bar{\\pi}}](x,a)+\\delta)\\cdot\\varepsilon^{-1}+\\zeta_{h}\\right]=\\left[\\mathcal{P}_{h}[V_{h+1}^{\\bar{\\pi}}](x,a)\\cdot\\varepsilon^{-1}+\\zeta_{h}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "where $\\delta$ represents all the possible values that the difference $\\widehat{\\pmb{Q}}_{h}(x,a)-\\mathscr{P}_{h}[V_{h+1}^{\\bar{\\pi}}](x,a)$ is allowed to take. By Lemma K.1, we know that $\\bar{\\pi}$ is endogenous, and so there exists a function $g_{h}:S\\times A\\to$ $[0,H-h+1]$ such that ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\forall x\\in\\mathcal{X},a\\in\\mathcal{A},\\quad\\mathcal{P}_{h}[V_{h+1}^{\\bar{\\pi}}](x,a)=g_{h}(\\phi^{\\star}(x),a).\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "Toward proving Eq. (77), observe that for any $(s,a)\\in{\\mathcal{S}}\\in A$ , if $\\zeta_{h}$ is such that ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g_{h}(s,a)/\\varepsilon+\\zeta_{h}+4\\varepsilon\\le\\lceil g_{h}(s,a)/\\varepsilon+\\zeta_{h}\\rceil,}\\\\ &{g_{h}(s,a)/\\varepsilon+\\zeta_{h}-4\\varepsilon>\\lceil g_{h}(s,a)/\\varepsilon+\\zeta_{h}\\rceil-1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "then, for all $\\delta\\in[-4\\varepsilon^{2},4\\varepsilon^{2}]$ and all $x\\in\\mathscr{X}$ such that $\\phi^{\\star}(x)=s$ , we have ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(\\mathcal{P}_{h}[V_{h+1}^{\\bar{\\pi}}](x,a)+\\delta)/\\varepsilon+\\zeta_{h}]=\\lceil\\left(g_{h}(s,a)+\\delta\\right)/\\varepsilon+\\zeta_{h}\\rceil=\\lceil g_{h}(s,a)/\\varepsilon+\\zeta_{h}\\rceil=\\lceil\\mathcal{P}_{h}[V_{h+1}^{\\bar{\\pi}}](x,a)/\\varepsilon+\\mathcal{O}(\\delta)/\\varepsilon\\rceil=\\lceil\\mathcal{P}_{h}[V_{h+1}^{\\bar{\\pi}}](x,a)/\\varepsilon+\\mathcal{O}(\\delta)/\\varepsilon\\rceil=1}\\end{array}\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "Therefore, if we let $\\mathcal{E}_{h}(s,a)$ denote the event in (78), then under $\\textstyle\\bigcap_{(s,a)\\in{\\cal S}\\times{\\cal A}}{\\mathcal{E}}_{h}(s,a)$ , the desired condition in (77) holds. At this point, setting $\\begin{array}{r}{\\mathcal{E}_{h}=\\left(\\bigcap_{(s,a)\\in S\\times A}\\mathcal{E}_{h}(s,a)\\right)\\cap\\mathcal{E}_{h+1}}\\end{array}$ would complete the induction as long as $\\mathbb{P}_{h:H}^{\\zeta}[\\mathcal{E}_{h}]\\geq1-24S A(H-h+1)\\varepsilon$ . We now show that this is indeed the case by bounding the probability of the event $\\textstyle\\bigcap_{(s,a)\\in{\\cal S}\\times{\\cal A}}{\\mathcal{E}}_{h}(s,a)$ . By the union bound, we have ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\mathbb{P}_{h:H}^{\\zeta}\\left[\\bigcap_{\\left(s,a\\right)\\in S\\times A}\\mathcal{E}_{h}(s,a)\\mid\\mathcal{E}_{h+1}\\right]\\ge1-\\sum_{\\left(s,a\\right)\\in S\\times A}\\mathbb{P}_{h:H}^{\\zeta}\\left[\\mathcal{E}_{h}(s,a)^{c}\\mid\\mathcal{E}_{h+1}\\right],\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "where $\\mathcal{E}_{h}(s,a)^{c}$ denotes the complement of $\\mathcal{E}_{h}(s,a)$ . We now bound the probability ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\mathbb{P}_{h:H}^{\\zeta}\\left[\\mathcal{E}_{h}(s,a)^{c}\\mid\\mathcal{E}_{h+1}\\right].\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "Fix $(s,a)\\in{\\mathcal{S}}\\times{\\mathcal{A}}$ . We have that $\\zeta_{h}\\in\\mathcal{E}(s,a)^{c}$ if and only if ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g_{h}(s,a)/\\varepsilon+\\zeta_{h}+4\\varepsilon>\\lceil g_{h}(s,a)/\\varepsilon+\\zeta_{h}\\rceil,}\\\\ {\\mathrm{or}\\quad}&{g_{h}(s,a)/\\varepsilon+\\zeta_{h}-4\\varepsilon\\leq\\lceil g_{h}(s,a)/\\varepsilon+\\zeta_{h}\\rceil-1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "Now, since $\\zeta_{h}\\in[0,1/2]$ , Lemma L.3 (instantiated with $\\left(x,\\zeta,\\nu\\right)=\\left(g_{h}(s,a)/\\varepsilon,\\zeta_{h},4\\varepsilon\\right))$ implies that (80) holds only if ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\left[g_{h}(s,a)/\\varepsilon\\right]-4\\varepsilon\\leq g_{h}(s,a)/\\varepsilon+\\zeta_{h}\\leq\\left[g_{h}(s,a)/\\varepsilon\\right]+4\\varepsilon\\quad\\mathrm{or}\\quad0\\leq\\zeta_{h}\\leq4\\varepsilon.\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "Further, note that since $\\zeta_{h}$ is uniformly distributed over $[0,1/2]$ , the $\\mathbb{P}_{h}^{\\zeta}$ -probability of the event in (81) is at most the sum of the lengths of the intervals ", "page_idx": 65}, {"type": "equation", "text": "$$\n[[g_{h}(s,a)/\\varepsilon]-g_{h}(s,a)/\\varepsilon-4\\varepsilon,[g_{h}(s,a)/\\varepsilon]-g_{h}(s,a)/\\varepsilon+4\\varepsilon]\\quad\\mathrm{and}\\quad[0,4\\varepsilon],\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "multiplied by 2, which is equal to $24\\varepsilon$ . Therefore, we have ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}_{h:H}^{\\zeta}\\left[\\mathcal{E}_{h}(s,a)^{c}\\mid\\mathcal{E}_{h+1}\\right]}\\\\ &{\\quad\\le\\mathbb{P}_{h}^{\\zeta}\\left[\\left[g_{h}(s,a)/\\varepsilon\\right]-4\\varepsilon\\le g_{h}(s,a)/\\varepsilon+\\zeta_{h}\\le\\left[g_{h}(s,a)/\\varepsilon\\right]+4\\varepsilon\\;\\;\\mathrm{or}\\;\\;0\\le\\zeta_{h}\\le4\\varepsilon\\right]\\le24\\varepsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "Combining this with (79), we obtain ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\mathbb{P}_{h:H}^{\\zeta}\\left[\\bigcap_{\\left(s,a\\right)\\in S\\times A}\\mathcal{E}_{h}(s,a)\\mid\\mathcal{E}_{h+1}\\right]\\geq1-\\sum_{\\left(s,a\\right)\\in S\\times A}\\mathbb{P}_{h:H}^{\\zeta}\\left[\\mathcal{E}_{h}(s,a)^{c}\\mid\\mathcal{E}_{h+1}\\right]\\geq1-24S A\\varepsilon.\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "Thus, by setting $\\begin{array}{r}{\\mathcal{E}_{h}=\\left(\\bigcap_{(s,a)\\in S\\times A}\\mathcal{E}_{h}(s,a)\\right)\\cap\\mathcal{E}_{h+1}}\\end{array}$ , we get that ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbb{P}_{h:H}^{\\zeta}[\\mathcal{E}_{h}]\\geq\\mathbb{P}_{h+1:H}^{\\zeta}[\\mathcal{E}_{h+1}]\\cdot\\mathbb{P}_{h:H}^{\\zeta}[\\mathcal{E}_{h}\\mid\\mathcal{E}_{h+1}]\\geq(1-24S A(H-h)\\varepsilon)(1-24S A\\varepsilon),}\\\\ &{}&{\\geq1-24S A(H-h+1)\\varepsilon,\\qquad\\qquad\\qquad\\qquad\\qquad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "which completes the induction. ", "page_idx": 66}, {"type": "text", "text": "K.4 Proof of Lemma K.3 (Coverability in Weakly Correlated ExBMDP) ", "text_level": 1, "page_idx": 66}, {"type": "text", "text": "Proof of Lemma K.3. Fix $h\\in[2\\ldots H]$ and define the measure $\\mu$ as ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\mu(x):=\\sum_{\\xi^{\\prime}\\in\\Xi}q(x^{\\prime}\\,|\\,(\\phi_{h}^{\\star}(x^{\\prime}),\\xi^{\\prime}))\\cdot{\\mathbb P}[\\xi_{h}=\\xi^{\\prime}]\\cdot{\\mathbb P}[s_{h}=\\phi_{h}^{\\star}(x^{\\prime})\\,|\\,\\,s_{h-1}=\\phi_{h-1}^{\\star}(x),a_{h-1}=a],\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "for all $h\\in[H]$ and $x\\in\\mathscr{X}$ . We show that $\\mu$ satisfies Assumption 4.1 with $C_{\\mathsf{p u s h}}=C_{\\mathsf{e x o}}\\cdot S A$ . First, note that $\\mu$ is indeed a probability measure over $\\mathcal{X}$ . Fix $({\\boldsymbol{x}},{\\bar{\\boldsymbol{a}}},{\\boldsymbol{x}}^{\\prime})\\in{\\mathcal{X}}\\times{\\mathcal{A}}\\times{\\dot{\\boldsymbol{x}}}$ . We have ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\boldsymbol{x}_{h}=\\boldsymbol{x}^{\\prime}\\mid\\boldsymbol{x}_{h-1}=\\boldsymbol{x},\\boldsymbol{a}_{h-1}=\\boldsymbol{a}\\right|}\\\\ &{=\\frac{\\mathbb{P}\\left[\\boldsymbol{x}_{h}=\\boldsymbol{x}^{\\prime},\\boldsymbol{x}_{h-1}=\\boldsymbol{x}\\mid\\boldsymbol{a}_{h-1}=\\boldsymbol{a}\\right]}{\\mathbb{P}\\left[\\boldsymbol{x}_{h-1}=\\boldsymbol{x}\\mid\\boldsymbol{a}_{h-1}=\\boldsymbol{a}\\right]},}\\\\ &{=\\frac{\\mathbb{P}\\left[\\boldsymbol{x}_{h}=\\boldsymbol{x}^{\\prime},\\boldsymbol{x}_{h-1}=\\boldsymbol{x}\\mid\\boldsymbol{a}_{h-1}=\\boldsymbol{a}\\right]}{\\mathbb{P}\\left[\\boldsymbol{x}_{h-1}=\\boldsymbol{x}\\right]},}\\\\ &{=\\frac{\\mathbb{P}\\left[\\boldsymbol{x}_{h-1}=\\boldsymbol{x}^{\\prime},\\boldsymbol{x}_{h-1}=\\boldsymbol{x}\\mid\\boldsymbol{a}_{h-1}=\\boldsymbol{a}\\right]}{\\mathbb{P}\\left[\\boldsymbol{x}_{h-1}=\\boldsymbol{x}\\right]},}\\\\ &{=\\frac{\\sum_{\\xi,\\xi^{\\prime}\\in\\Xi}q\\left(\\boldsymbol{x}^{\\prime}\\mid\\left(\\phi_{h}^{\\star}(\\boldsymbol{x}^{\\prime}),\\xi^{\\prime}\\right)\\right)\\cdot\\boldsymbol{q}\\left(\\boldsymbol{x}\\mid(\\phi_{h-1}^{\\star}(\\boldsymbol{x}),\\xi)\\right)\\cdot\\mathbb{P}\\left[\\boldsymbol{s}_{h}=\\phi_{h}^{\\star}(\\boldsymbol{x}^{\\prime}),\\xi_{h}=\\xi^{\\prime},\\boldsymbol{s}_{h-1}=\\phi_{h-1}^{\\star}(\\boldsymbol{x}),\\xi_{h-1}=\\boldsymbol{a}\\right]}{\\sum_{\\xi\\in\\Xi}q\\left(\\boldsymbol{x}\\mid(\\phi_{h-1}^{\\star}(\\boldsymbol{x}),\\xi)\\right)\\cdot\\mathbb{P}\\left[\\boldsymbol{s}_{h-1}=\\phi_{h-1}^{\\star}(\\boldsymbol{x}),\\xi_{h-1}=\\xi\\right]}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "and so by the ExBMDP structure: ", "page_idx": 66}, {"type": "equation", "text": "$$\n=\\frac{\\sum_{\\xi,\\xi^{\\prime}\\in\\Xi}q(x^{\\prime}\\,|\\,(\\phi_{h}^{\\star}(x^{\\prime}),\\xi^{\\prime}))\\cdot q(x\\,|\\,(\\phi_{h-1}^{\\star}(x),\\xi))\\cdot\\mathbb{P}[\\xi_{h}=\\xi^{\\prime},\\xi_{h-1}=\\xi]\\cdot\\mathbb{P}[s_{h}=\\phi_{h}^{\\star}(x^{\\prime})\\,|\\,s_{h-1}=\\phi_{h}^{\\star}(x),\\xi_{h}=\\xi_{h}]}{\\sum_{\\xi\\in\\Xi}q(x\\,|\\,(\\phi_{h}^{\\star}(x),\\xi))\\cdot\\mathbb{P}[\\xi_{h-1}=\\xi]}\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "and by Assumption B.1 ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq C_{\\mathrm{exo}}\\frac{\\sum_{\\xi,\\xi^{\\prime}\\in\\Xi}q(x^{\\prime}\\mid(\\phi_{h}^{\\star}(x^{\\prime}),\\xi^{\\prime}))\\cdot q(x\\mid(\\phi_{h-1}^{\\star}(x),\\xi))\\cdot\\mathbb{P}[\\xi_{h}=\\xi^{\\prime}]\\cdot\\mathbb{P}[\\xi_{h-1}=\\xi]\\cdot\\mathbb{P}[s_{h}=\\phi_{h}^{\\star}(x^{\\prime})\\mid s_{h}]}{\\sum_{\\xi\\in\\Xi}q(x\\mid(\\phi_{h}^{\\star}(x),\\xi))\\cdot\\mathbb{P}[\\xi_{h-1}=\\xi]}}\\\\ &{=C_{\\mathrm{exo}}\\underbrace{\\sum_{\\phi\\in\\Xi}q(x^{\\prime}\\mid(\\phi_{h}^{\\star}(x^{\\prime}),\\xi^{\\prime}))\\cdot\\mathbb{P}[\\xi_{h}=\\xi^{\\prime}]\\cdot\\mathbb{P}[s_{h}=\\phi_{h}^{\\star}(x^{\\prime})\\mid s_{h-1}=\\phi_{h-1}^{\\star}(x),a_{h-1}=a]}_{\\xi^{\\prime}\\in\\Xi},}\\\\ &{\\phantom{=}\\cdots\\sum_{\\phi\\in\\Xi}\\quad,\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 66}, {"type": "equation", "text": "$=C_{\\mathsf{e x o}}S A\\cdot\\mu(x^{\\prime}),$ ", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "This completes the proof. ", "page_idx": 66}, {"type": "text", "text": "K.5 Proof of Lemma K.6 (Confidence Sets) ", "text_level": 1, "page_idx": 66}, {"type": "text", "text": "To prove Lemma K.6, we need the following consequence of tests in Line 14 passing for all $\\ell\\in\\bar{[}h+1\\ldots H]$ . ", "page_idx": 66}, {"type": "text", "text": "Lemma K.8 (Consequence of passed tests). Let $h\\in[0\\ldots H]$ , $\\varepsilon>0$ , and $\\zeta_{1:H}\\in[0,1/2]$ be given and consider a call to RVF $\\mathsf{S}_{0}^{\\mathsf{e x o}}$ (Algorithm 8) in the setting of Lemma K.4. Further, let $\\mathcal{E}$ be the event of Lemma K.4. There exists an event $\\mathcal{E}_{h}^{\\prime}$ of probability at least $1-\\delta/H$ such that under $\\mathcal{E}\\cap\\mathcal{E}_{h}^{\\prime}$ , if a call to RVF $S_{h}^{\\mathsf{e x o}}$ during the execution of RVF $S_{0}^{\\e x_{0}}$ terminates and returns $(\\widehat{V}_{h:H},\\widehat{\\mathcal{V}}_{h:H},\\mathcal{C}_{h:H},\\mathcal{B}_{h:H},t_{h:H})$ , then for any $(x_{h-1},a_{h-1})\\in{\\mathcal{C}}_{h}$ and $\\ell\\in[h+1\\ldots H+1]$ :   \n$\\geqslant\\left[\\operatorname*{sup}_{f\\in\\tilde{V}_{\\ell}}\\operatorname*{max}_{a\\in A}\\left|\\left(\\mathcal{P}_{\\ell-1}[\\widehat{V}_{\\ell}]-\\mathcal{P}_{\\ell-1}[f_{\\ell}]\\right)(x_{\\ell-1},a)\\right|>3\\varepsilon^{2}\\mid x_{h-1}=x_{h-1},a_{h-1}=a_{h-1}\\right]\\leq\\frac{4\\log(8M^{6}N_{\\mathrm{test}}^{2}}{N_{\\mathrm{test}}}$ H8/\u03b4) where $(\\widehat{\\pi}_{\\tau})_{\\tau\\geq h}\\subset\\Pi_{\\mathsf{S}},\\,M_{\\tau}$ , and $N_{\\mathrm{test}}$ are as in RVF $S_{h}^{\\mathsf{e x o}}$ (Algorithm 8). ", "page_idx": 66}, {"type": "text", "text": "Proof of Lemma K.8. This is just a restatement of Lemma I.2, and the proof is exactly the same as the latter. \u53e3 ", "page_idx": 67}, {"type": "text", "text": "We will also use Lemma I.5; even though this result is stated in section for the $V^{\\pi}$ -realizable setting, it is also applicable to the ExBMDP variant of RVFS as it merely says something about the order in which the $\\left(\\mathsf{R V F S}_{h}^{\\mathsf{e x o}}\\right)$ ) instances are called. With this, we now prove Lemma K.6. ", "page_idx": 67}, {"type": "text", "text": "Proof of Lemma K.6. The proof is very similar to that of Lemma I.2, with differences to account for the \u201ccoarsening\u201d of the learned and benchmark policies. ", "page_idx": 67}, {"type": "text", "text": "We prove the desired result for $\\mathcal{E}^{\\prime\\prime\\prime}:=\\mathcal{E}\\cap\\mathcal{E}_{1}^{\\prime}\\cap\\mathcal{E}_{1}^{\\prime\\prime}\\cap\\cdots\\cap\\mathcal{E}_{H}^{\\prime}\\cap\\mathcal{E}_{H}^{\\prime\\prime}$ , where $\\mathcal{E}$ , $(\\mathcal{E}_{h}^{\\prime})$ , and $(\\mathcal{E}_{h}^{\\prime\\prime})$ are the events in Lemma K.4, Lemma K.8, and Lemma K.5, respectively. Throughout, we condition on $\\mathcal{E}^{\\prime\\prime\\prime}$ . First, note that by Lemma K.4, RVF $\\mathsf{S}_{0}^{\\mathsf{e x o}}$ terminates. Let $(\\widehat{V}_{1:H},\\widehat{\\mathcal{V}}_{1:H},\\mathcal{C}_{1:H},\\mathcal{\\bar{B}}_{1:H},t_{1:H})$ be its returned tuple. ", "page_idx": 67}, {"type": "text", "text": "We show via backward induction over $\\ell=H+1,\\ldots,1$ , that ", "page_idx": 67}, {"type": "equation", "text": "$$\nV_{\\ell}^{\\tilde{\\pi}}\\in\\widehat{\\mathcal{V}}_{\\ell},\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "where $\\tilde{\\pi}_{1:H}$ is the stochastic policy defined recursively via ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\langle x\\in\\mathcal{X},\\ \\tilde{\\pi}_{\\tau}(x;\\zeta_{1:H},\\varepsilon,\\delta)\\in\\arg\\operatorname*{max}\\left\\{\\begin{array}{l l}{[\\widehat{Q}_{\\tau}(x,a)/\\varepsilon+\\zeta_{\\tau}],}&{\\mathrm{if~}\\Vert\\widehat{Q}_{\\tau}(x,\\cdot)-\\mathcal{P}_{\\tau}[V_{\\tau+1}^{\\tilde{\\pi}}](x,\\cdot)\\Vert_{\\infty}\\leq1,}\\\\ {[\\mathcal{P}_{\\tau}[V_{\\tau+1}^{\\tilde{\\pi}}](x,a)/\\varepsilon+\\zeta_{\\tau}],}&{\\mathrm{otherwise},}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "for $\\tau\\,=\\,H,\\dots,1$ , where $\\widehat{\\pmb{Q}}_{\\tau}(\\cdot,a):=\\widehat{\\pmb{{\\mathscr{P}}}}_{\\tau,\\varepsilon,\\delta}[\\widehat{V}_{\\tau+1}](\\cdot,a)$ . Note that since $\\zeta_{1:H}\\,\\in\\,\\mathcal{E}_{\\sf r a n d}$ (for $\\mathcal{E}_{\\mathrm{rand}}$ is defined in Lemma K.2), we have $\\tilde{\\pi}\\,\\equiv\\,\\bar{\\pi}$ , where $\\bar{\\pi}$ is as in (70). Thus, instantiating the induction hypothesis with $\\ell\\,=\\,h$ and using the definition of the confidence sets $(\\widehat{\\mathcal{V}}_{\\ell})$ in (16) together with $V_{h}^{\\tilde{\\pi}}\\equiv V_{h}^{\\bar{\\pi}}$ (since $\\tilde{\\pi}\\equiv\\bar{\\pi}$ ) implies the desired result. ", "page_idx": 67}, {"type": "text", "text": "Base case $[\\ell=H+1]$ . Holds trivially since $V_{H+1}^{\\pi}\\equiv0$ for any $\\pi\\in\\Pi_{\\ S}$ by convention. ", "page_idx": 67}, {"type": "text", "text": "General case $[\\ell\\leq H]$ . Fix $h\\in[H]$ and suppose that (82) holds for all $\\ell\\in[h+1\\ldots H+1]$ . We sohf t, htihse rne ,u ea nfdo r $\\ell=h$ .)  Fhiorlsdt,s  nfootr ,i fs $\\mathsf{R V F S}_{h}^{\\mathsf{e x o}}$ inse veenrd coaglelendo udsu ruinndge tr , $\\mathsf{R V F S}_{\\mathrm{0}}^{\\mathsf{e x o}}$ $\\widehat{\\mathcal{V}}_{h}=\\mathcal{V}_{h}$ $\\ell=h$ ${\\tilde{\\pi}}={\\bar{\\pi}}$ $\\zeta_{1:H}\\in\\mathcal{E}_{\\mathsf{r a n d}}$ where $\\mathcal{E}_{\\mathrm{rand}}$ is the event in Lemma K.2. ", "page_idx": 67}, {"type": "text", "text": "Now, suppose that RVF $S_{h}^{\\mathsf{e x o}}$ is called at least once, and let $(\\widehat{V}_{h:H}^{+},\\widehat{\\mathcal{V}}_{h:H}^{+},\\mathcal{C}_{h:H}^{+},\\mathcal{B}_{h:H}^{+},t_{h:H}^{+})$ be the output of the last call to RVFSehx throughout the execution of RVF $\\mathsf{S}_{0}^{\\mathsf{e x o}}$ . Next, we show that ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\big(\\widehat{V}_{h:H}^{+},\\widehat{\\mathcal{V}}_{h:H}^{+},\\mathcal{C}_{h:H}^{+}\\big)=\\big(\\widehat{V}_{h:H},\\widehat{\\mathcal{V}}_{h:H},\\mathcal{C}_{h:H}\\big).\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "The for-loop in Line 16 ensures that no instance of $\\big(\\mathsf{R V F S}_{\\tau}^{\\mathsf{e x o}}\\big)_{\\tau>h}$ can be called after the last call to $\\mathsf{R V F S}_{h}^{\\mathsf{e x o}}$ (see Lemma I.5). Thus, the estimated value functions, confidence sets, and core sets for layers $h+1,\\ldots,H$ remain unchanged after the last call to RVF $S_{h}^{\\mathsf{e x o}}$ ; that is, (83) holds. Thus, by Lemma K.8, and since we are conditioning on $\\mathcal{E}_{h+1:H}^{\\prime}$ , we have that for all $(x_{h-1},a_{h-1})\\in{\\mathcal{C}}_{h}$ and $\\ell\\in[h+1\\ldots H+1]$ : ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\geqslant\\left[\\operatorname*{sup}_{f\\in\\tilde{V}_{\\ell}}\\operatorname*{max}_{a\\in A}\\left|\\left(\\mathcal{P}_{\\ell-1}[\\widehat{V}_{\\ell}]-\\mathcal{P}_{\\ell-1}[f_{\\ell}]\\right)(x_{\\ell-1},a)\\right|>3\\varepsilon^{2}\\mid x_{h-1}=x_{h-1},a_{h-1}=a_{h-1}\\right]\\leq\\frac{4\\log(8M^{6}N_{\\mathrm{test}}^{2}}{N_{\\mathrm{test}}}\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "Now, by the induction hypothesis, we have $V_{\\ell}^{\\tilde{\\pi}}\\in\\widehat{\\mathcal{V}}_{\\ell}$ , and so substituting $V_{\\ell}^{\\tilde{\\pi}}$ for $f_{\\ell}$ in (84), we get that for all $(x_{h-1},a_{h-1})\\in{\\mathcal{C}}_{h}$ and $\\ell\\in[h+1\\ldots H+1]$ : ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\Im\\bigg[\\underset{a\\in\\mathcal{A}}{\\operatorname*{max}}\\bigg|\\big(\\mathcal{P}_{\\ell-1}[\\widehat{V}_{\\ell}]-\\mathcal{P}_{\\ell-1}[V_{\\ell}^{\\tilde{\\pi}}]\\big)(x_{\\ell-1},a)\\bigg|>3\\varepsilon^{2}\\mid x_{h-1}=x_{h-1},a_{h-1}=a_{h-1}\\bigg]\\leq\\frac{4\\log(8M^{6}N_{\\mathrm{test}}^{2}H^{8})}{N_{\\mathrm{test}}}\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "Therefore, by Lemma L.2 (instantiated with $\\mu[\\cdot]\\,=\\,\\mathbb{P}^{\\widehat{\\pi}}\\bigl[\\cdot\\mid\\pmb{x}_{h-1}\\,=\\,x_{h-1},\\pmb{a}_{h-1}\\,=\\,a_{h-1}\\bigr]$ , $\\tau\\,=\\,\\ell\\,-\\,1$ , $\\varepsilon^{\\prime}=\\varepsilon^{2}$ , and $V_{\\tau+1}=V_{\\ell}^{\\tilde{\\pi}},$ ), we have that for all $(x_{h-1},a_{h-1})\\in{\\mathcal{C}}_{h}$ and $\\ell\\in[h+1\\ldots H+1]$ : ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\mathbb{E}^{\\widehat\\pi}\\big[D_{\\mathrm{tv}}\\big(\\widehat\\pi_{\\ell-1}(x_{\\ell-1}),\\widetilde\\pi_{\\ell-1}({\\pmb x}_{\\ell-1})\\big)\\mid x_{h-1}=x_{h-1},\\pmb{a}_{h-1}=\\pmb{a}_{h-1}\\big]\\le\\frac{4\\log(8M^{6}N_{\\mathrm{test}}^{2}H^{8}/\\delta)}{N_{\\mathrm{test}}}+\\delta^{\\prime}.\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "Now, since $\\bar{\\pi}(\\cdot;\\zeta_{1:H},\\varepsilon)$ is endogenous and $\\tilde{\\pi}\\equiv\\bar{\\pi}$ (thanks to $\\zeta_{1:H}\\in\\mathcal{E}_{\\mathsf{r a n d}})$ , Lemma K.5 (applied with $\\pi=\\bar{\\pi}$ ) and the conditioning on $\\bar{\\mathcal{E}}_{h+1:H}^{\\prime\\prime}$ imply that: ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{x_{h-1},a_{h-1})\\in\\mathcal{C}_{h}}\\frac{1}{N_{\\mathrm{reg}}}\\sum_{(x_{h},-)\\in\\mathcal{D}_{h}(x_{h-1},a_{h-1})}\\left(\\widehat{V}_{h}(x_{h})-V_{h}^{\\tilde{\\pi}}(x_{h})\\right)^{2}}\\\\ &{\\le\\displaystyle\\frac{9k H^{2}\\log(8k^{2}H|\\mathcal{V}|/\\delta)}{N_{\\mathrm{reg}}}+8H^{2}\\sum_{(x_{h-1},a_{h-1})\\in\\mathcal{C}_{h}}\\sum_{\\tau=h}^{H}\\mathbb{E}^{\\widehat{\\pi}}\\left[D_{\\mathrm{tv}}(\\widehat{\\pi}_{\\tau}(x_{\\tau}),\\tilde{\\pi}_{\\tau}(x_{\\tau}))\\mid x_{h-1}=x_{h-1},a_{h-1}=\\delta\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "where the datasets $\\{\\mathcal{D}_{h}(x,a):(x,a)\\in\\mathcal{C}_{h}\\}$ are as in the definition of $\\widehat{\\mathcal{V}}_{h}$ in (16). Combining (86) with (85), we conclude that ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{(x_{h-1},a_{h-1})\\in{\\cal C}_{h}}\\frac{1}{N_{\\mathrm{reg}}}\\sum_{(x_{h},-)\\in{\\cal D}_{h}(x_{h-1},a_{h-1})}\\left(\\widehat{V}_{h}(x_{h})-V_{h}^{\\tilde{\\pi}}(x_{h})\\right)^{2}}\\\\ &{\\displaystyle\\leq\\frac{9M H^{2}\\log(8M^{2}H|\\mathcal{V}|/\\delta)}{N_{\\mathrm{reg}}}+8M H^{3}\\cdot\\frac{4\\log(8M^{6}N_{\\mathrm{test}}^{2}H^{8}/\\delta)}{N_{\\mathrm{test}}}+8M H^{3}\\delta^{\\prime},}\\\\ &{\\displaystyle=\\frac{9M H^{2}\\log(8M^{2}H|\\mathcal{V}|/\\delta)}{N_{\\mathrm{reg}}}+8M H^{3}\\cdot\\frac{4\\log(8M^{6}N_{\\mathrm{test}}^{2}H^{8}/\\delta)}{N_{\\mathrm{test}}}+8M H^{3}\\frac\\delta{4M^{7}N_{\\mathrm{test}}^{2}H^{8}|\\mathcal{V}|}{4M^{7}N_{\\mathrm{test}}^{2}H^{8}|\\mathcal{V}|},}\\end{array}\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "where we have used that $|{\\mathcal{C}}_{h}|\\leq M$ . By the definition of $\\widehat{\\mathcal{V}}_{h}$ in (16), (87) implies that $V_{h}^{\\tilde{\\pi}}\\in\\widehat{\\mathcal{V}}_{h}$ , which completes the induction. \u53e3 ", "page_idx": 68}, {"type": "text", "text": "K.6 Proof of Lemma K.7 (Main Guarantee of $\\mathsf{R V F S}^{\\mathsf{e x o}}$ ) ", "text_level": 1, "page_idx": 68}, {"type": "text", "text": "Proof of Lemma K.7. We condition on the event $\\widetilde{\\mathcal{E}}:=\\mathcal{E}\\cap\\mathcal{E}^{\\prime\\prime\\prime}\\cap\\mathcal{E}_{1}^{\\prime}\\cap\\cdots\\cap\\mathcal{E}_{H}^{\\prime}$ , where $\\mathcal{E},\\mathcal{E}^{\\prime\\prime\\prime}$ , and $(\\mathcal{E}_{h}^{\\prime})$ are the events in Lemma K.4, Lemma K.6, and Lemma K.8, respectively. Note that by the union bound, we have $\\mathbb{P}[\\widetilde{\\mathcal{E}}]\\ge1-5\\delta$ . By Lemma K.6, we have that ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\forall h\\in[H],\\quad\\mathbb{P}^{\\widehat{\\boldsymbol{\\pi}}}\\left[\\operatorname*{sup}_{f\\in\\widehat{\\mathcal{V}}_{h}}\\operatorname*{max}_{a\\in A}\\big|(\\mathcal{P}_{h-1}[\\widehat{V}_{h}]-\\mathcal{P}_{h-1}[f_{h}])(x_{h-1},a)\\big|>3\\varepsilon^{2}\\right]\\leq\\frac{4\\log(8M^{6}N_{\\mathrm{test}}^{2}H^{8}/\\delta)}{N_{\\mathrm{test}}},\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "where $M=\\left\\lceil8\\varepsilon^{-2}C_{\\mathsf{e x o}}S A H\\right\\rceil$ and $N_{\\mathrm{test}}=2^{8}M^{2}H\\varepsilon^{-2}\\log(8M^{6}H^{8}\\varepsilon^{-2}\\delta^{-1})$ . On the other hand, by Lemma K.6, we have ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\forall h\\in[H],\\quad V_{h}^{\\tilde{\\pi}}\\in\\widehat{\\mathcal{V}}_{h}.\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "Thus, substituting $V_{h}^{\\tilde{\\pi}}$ for $f_{h}$ in (88) we get that for all $h\\in[H+1]$ . ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\mathbb{P}^{\\widehat{\\pi}}\\bigg[\\operatorname*{max}_{a\\in A}\\big|\\big(\\mathcal{P}_{h-1}[\\widehat{V}_{h}]-\\mathcal{P}_{h-1}[V_{h}^{\\widetilde{\\pi}}]\\big)\\big(x_{h-1},a\\big)\\big|>3\\varepsilon^{2}\\bigg]\\leq\\frac{4\\log\\big(8M^{6}N_{\\mathrm{test}}^{2}H^{8}/\\delta\\big)}{N_{\\mathrm{test}}}.\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "T\u2032h is together with Lemma L.2, instantiated with $\\mu\\big[\\cdot\\big]=\\mathbb{P}^{\\widehat{\\pi}}\\big[\\cdot\\big];\\,\\tau=h-1;\\,V_{\\tau+1}=V_{h}^{\\tilde{\\pi}}$ ; and $\\delta=\\delta^{\\prime}$ (with $\\delta^{\\prime}$ as in Algorithm 5), translates to: ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall h\\in[H],\\quad\\mathbb{E}^{\\widehat\\pi}[D_{\\mathrm{tv}}(\\tilde{\\pi}_{h}(x_{h}),\\widehat{\\pi}_{h}(x_{h}))]\\leq\\frac{4\\log\\left(8M^{6}N_{\\mathrm{test}}^{2}H^{8}/\\delta\\right)}{N_{\\mathrm{test}}}+\\delta^{\\prime},}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\frac{4\\log\\left(8M^{6}N_{\\mathrm{test}}^{2}H^{8}/\\delta\\right)}{N_{\\mathrm{test}}}+\\frac{\\delta}{4M^{7}N_{\\mathrm{test}}^{2}H^{8}|\\mathcal{V}|},}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\frac{\\varepsilon^{2}}{4H^{3}S A C_{\\mathrm{exo}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "where the last step follows from the fact that $N_{\\mathrm{test}}=2^{8}M^{2}H\\varepsilon^{-2}\\log(8M^{6}H^{8}\\varepsilon^{-2}\\delta^{-1})$ (with $M$ as in Line 3). Now, since $\\zeta_{1:H}\\in\\mathcal{E}_{\\mathsf{r a n d}}$ (by assumption), Lemma K.2 implies that $\\tilde{\\pi}\\equiv\\bar{\\pi}$ , where the latter is the deterministic policy defined in (69). Thus, by (89), we have ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\forall h\\in[H],\\quad\\mathbb{P}^{\\widehat{\\pi}}[\\Bar{\\pi}_{h}(x_{h})\\neq\\widehat{\\pi}_{h}(x_{h})]=\\mathbb{E}^{\\widehat{\\pi}}[D_{\\mathrm{tv}}(\\Tilde{\\pi}_{h}(x_{h}),\\widehat{\\pi}_{h}(x_{h}))]\\leq\\frac{\\varepsilon^{2}}{4H^{3}S A C_{\\mathrm{ex}}},\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "where the first equality follows by the fact that $\\mathbb{P}[\\bar{\\pi}_{h}(x)\\neq\\widehat{\\pi}_{h}(x)]=D_{\\mathrm{tv}}(\\tilde{\\pi}_{h}(x),\\widehat{\\pi}_{h}(x)).$ , for all $x\\in\\mathscr{X}$ , since $\\bar{\\pi}_{h}$ is deterministic. ", "page_idx": 68}, {"type": "text", "text": "Bounding the sample complexity. We now bound the number of episodes used by Algorithm 8 under $\\widetilde{\\mathcal{E}}$ . First, we fix $h\\,\\in\\,[H]$ , and focus on the number of episodes used within a to call $\\mathsf{R V F S}_{h}^{\\mathsf{e x o}}$ ; excluding any episodes used by any recursive calls to $\\mathsf{R V F S}_{\\tau}^{\\mathsf{e x o}}$ for $\\tau\\,>\\,h$ . We start by counting the number of episodes used to test the fit of the estimated value functions $\\widehat{V}_{h+1:H}$ . Starting from Line 8, there are for-loops over $(x_{h-1},a_{h-1})\\ \\in\\ C_{h}.$ , $\\ell\\ =\\ H,\\ldots,h+1$ , and $n\\,\\in\\,\\lceil N_{\\mathrm{test}}\\rceil$ to collected partial episodes using the learned policy $\\widehat{\\pi}$ in Algorithm 8, where $N_{\\mathrm{test}}\\,=\\,\\dot{2}^{8}M^{2}\\dot{H}\\varepsilon^{-2}\\log\\bigl(8M^{6}\\dot{H}^{8}\\varepsilon^{-2}\\delta^{-1}\\bigr)$ and $M\\,=\\,\\lceil8\\varepsilon^{-2}C_{\\tt e x o}S\\dot{A}H\\rceil$ . Note that $\\widehat{\\pi}$ uses the local simulator and requires $\\bar{N}_{\\mathrm{sim}}=2\\log(4M^{7}\\dot{N}_{\\mathrm{test}}^{2}H^{2}|\\nu|/\\delta)/\\varepsilon^{2}$ samples to output an action at each layer (since Algorithm 8 calls Algorithm 7 with confidence level $\\delta^{\\prime}\\stackrel{\\cdot}{=}\\delta/(8M^{7}\\dot{N}_{\\mathrm{test}}^{2}H^{8}|\\mathcal{V}|))$ . Also, note that whenever a test fails in Line 14, the for-loop in Line 8 resumes. We also know (by Lemma K.4) that the number of times the test fails in Line 14 is at most $M$ . Thus, the number of times the for-loop in Line 8 resumes is bounded by $H M$ ; here, $H$ accounts for test failures for all layers $\\tau\\in[h+1\\ldots H]$ . Thus, the number of episodes required to between lines Line 8 and Line 11 is bounded by ", "page_idx": 69}, {"type": "equation", "text": "$$\n\\mathrm{\\#~episodes~for~roll-outs\\leq\\underbrace{MH}_{\\mathrm{\\#~of~times~Line~8~resumes}}.}\\mathrm{\\underbrace{~}\\underbrace{M H^{2}N_{\\mathrm{test}}N_{\\mathrm{sim}}}_{\\mathrm{Number~of~episodes~in~case~of~no~test~fimes~Ret~ciant~Alposit}}.}\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "Note that the test in Line 14 also uses local simulator access because it calls the operator $\\widehat{\\mathcal{P}}$ for every $a\\in\\mathcal{A}$ . Thus, the number of episodes used for the test in Line 14 is bounded by ", "page_idx": 69}, {"type": "equation", "text": "$$\n\\#\\mathrm{\\;episodes\\;for\\;the\\;tests}\\leq\\underbrace{M H}_{\\#\\mathrm{\\;of\\;times\\;Line\\;8\\;resumes}}\\cdot\\underbrace{M H A N_{\\mathrm{test}}N_{\\mathrm{sim}}}_{\\mathrm{Number\\;of\\;episodes\\;used\\;in\\;Line\\;14}}.\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "We now count the number of episodes used to re-fit the value function; Line 16 onwards. Note that starting from Line 16, there are for-loops over $(x_{h-1},a_{h-1})\\in{\\mathcal{C}}_{h}$ and $i\\,\\in\\,\\bigl[N_{\\mathrm{reg}}\\bigr]$ to generate $A\\cdot N_{\\mathrm{est}}(|\\mathcal{C}_{h}|)\\leq A\\cdot N_{\\mathrm{est}}(M)$ partial episodes using $\\widehat{\\pi}$ , where $N_{\\mathrm{est}}(k)=2N_{\\mathrm{reg}}^{2}\\log(8A N_{\\mathrm{reg}}H k^{3}/\\delta)$ is as in Algorithm 8. And, since uses local simulator access and requires $N_{\\mathrm{est}}$ samples (see Algorithm 7) to output an action at each layer, the number of episodes used to refti the value function is bounded by ", "page_idx": 69}, {"type": "equation", "text": "$$\n\\#\\mathrm{\\,episodes\\,\\,for\\,}V\\mathrm{\\-refitting\\,}\\leq M N_{\\mathrm{reg}}A N_{\\mathrm{est}}(M)H N_{\\mathrm{sim}}.\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "Therefore, by (90), (91), and (92), the number of episodes used within a single call to $\\mathsf{R V F S}_{h}^{\\mathsf{e x o}}$ (not accounting for episodes used by recursive calls to $\\mathsf{R V F S}_{\\tau}^{\\mathsf{e x o}}$ , for $\\tau>h$ ) is bounded by ", "page_idx": 69}, {"type": "equation", "text": "$$\n\\mathsf{F S}_{h}^{\\mathsf{e x o}}\\le M^{2}H(H+A)N_{\\mathrm{test}}N_{\\mathsf{s i m}}+M N_{\\mathrm{reg}}A N_{\\mathsf{e s t}}(M)H N_{\\mathsf{s i m}}.\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "Finally, by Lemma K.4, RVF $S_{h}^{\\mathsf{e x o}}$ may be called at most $M$ times throughout the execution of RVF $\\mathsf{S}_{0}^{\\mathsf{e x o}}$ Using this together with (93) and accounting for the number of episodes from all layers $h\\in[H]$ , we get that the total number of episodes is bounded by ", "page_idx": 69}, {"type": "equation", "text": "$$\nM^{3}H^{2}(H+A)N_{\\mathrm{test}}N_{\\mathrm{sin}}+M^{2}H^{2}N_{\\mathrm{reg}}A N_{\\mathrm{est}}(M)N_{\\mathrm{sin}}.\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "Substituting the expressions of $M,N_{\\mathrm{test}},N_{\\mathrm{est}},N_{\\mathrm{sim}},$ and $N_{\\mathsf{r e g}}$ from Algorithm 8 and Algorithm 7, we obtain the desired number of episodes, which concludes the proof. \u53e3 ", "page_idx": 69}, {"type": "text", "text": "L Additional Technical Lemmas ", "text_level": 1, "page_idx": 69}, {"type": "text", "text": "Lemma L.1. Let $\\tau\\in[H]$ and $\\varepsilon,\\delta,\\nu\\in(0,1)$ be given. Consider two value functions $V_{\\tau+1},\\widehat{V}_{\\tau+1}\\in$ $[0,H]$ and a measure $\\mu\\in\\Delta(\\mathcal{X})$ such that ", "page_idx": 69}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\mathbf{x}_{\\tau}\\sim\\mu}\\bigg[\\mathbb{I}\\left\\{\\operatorname*{max}_{a\\in\\mathcal{A}}\\big|\\big(\\mathcal{P}_{\\tau}[\\widehat{V}_{\\tau+1}]-\\mathcal{P}_{\\tau}[V_{\\tau+1}]\\big)(\\mathbf{x}_{\\tau},a)\\big|>3\\varepsilon\\right\\}\\bigg]\\leq\\nu.\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "Further, for $x\\in\\mathscr{X}$ , let $\\widehat{\\pi}_{\\tau}(x)\\in\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}\\widehat{Q}_{\\tau}(x,a):=\\widehat{\\mathcal{P}}_{\\tau,\\varepsilon,\\delta}[\\widehat{V}_{\\tau+1}](x,a)$ and inductively define $a$ randomized policy $\\tilde{\\pi}$ via ", "page_idx": 69}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{\\pi}_{\\tau}(x)\\in\\underset{a\\in A}{\\arg\\operatorname*{max}}\\left\\{\\begin{array}{l l}{\\widehat{Q}_{\\tau}(x,a),}&{i f\\|\\widehat{Q}_{\\tau}(x,\\cdot)-\\mathcal{P}_{\\tau}[V_{\\tau+1}](x,\\cdot)\\|_{\\infty}\\leq4\\varepsilon,}\\\\ {\\mathcal{P}_{\\tau}[V_{\\tau+1}](x,a),}&{o t h e r w i s e.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "Then, we have ", "page_idx": 69}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\pmb{x}_{\\tau}\\sim\\mu}[D_{\\mathrm{tv}}(\\widehat{\\pi}_{\\tau}(\\pmb{x}_{\\tau}),\\tilde{\\pi}_{\\tau}(\\pmb{x}_{\\tau}))]\\leq\\nu+\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "Proof of Lemma L.1. In this proof, we let $\\mathbb{P}_{\\mu}$ denote the probability law of $\\pmb{x}_{\\tau}$ and $\\mathbb{P}_{\\mathcal{P}}$ denote the probability law of $\\widehat{\\mathcal{P}}_{\\tau,\\varepsilon,\\delta}$ . Denote by $\\mathcal{E}$ the $\\mathbb{P}_{\\mu}$ -measurable event that $\\begin{array}{r}{\\operatorname*{max}_{a\\in A}\\big|\\big(\\mathcal P_{\\tau}[\\widehat{V}_{\\tau+1}]-\\mathcal P_{\\tau}[V_{\\tau+1}]\\big)(\\pmb{x}_{\\tau},a)\\big|\\leq3\\varepsilon}\\end{array}$ . Fix $x\\in\\mathcal{E}$ , and let ${\\mathcal E}_{x}$ be the $\\mathbb{P}_{\\mathcal{P}}$ -measurable event th $\\begin{array}{r}{\\mathrm{at}\\operatorname*{max}_{a\\in\\mathcal{A}}|\\widehat{\\mathcal{P}}_{\\tau,\\varepsilon,\\delta}[\\widehat{V}_{\\tau+1}](x,a)-\\mathcal{P}_{\\tau}[V_{\\tau+1}](x,a)|\\leq\\mathcal{L}}\\end{array}$ 4\u03b5. From the definition of $\\tilde{\\pi}_{\\tau}$ , we have that ", "page_idx": 70}, {"type": "text", "text": "Dtv(\u03c0\u0302\u03c4(x), \u03c0\u02dc\u03c4(x)) $\\begin{array}{r l}{\\lefteqn{=\\frac{1}{2}\\sum_{a\\in A}\\left|\\mathbb{P}_{\\mathcal P}\\left[\\widehat{\\pi}_{\\tau}(x)=a\\right]-\\mathbb{P}_{\\mathcal P}\\left[\\tilde{\\pi}_{\\tau}(x)=a\\right]\\right|,}}\\\\ &{=\\frac{1}{2}\\sum_{a\\in A}\\left|\\mathbb{P}_{\\mathcal P}\\left[\\mathcal E_{x}\\right]\\mathbb{P}_{\\mathcal P}\\left[\\widehat{\\pi}_{\\tau}(x)=a\\mid\\mathcal E_{x}\\right]+\\mathbb{P}_{\\mathcal P}\\left[\\mathcal E_{x}^{c}\\right]\\mathbb{P}_{\\mathcal P}\\left[\\widehat{\\pi}_{\\tau}(x)=a\\mid\\mathcal E_{x}^{c}\\right]-\\mathbb{P}_{\\mathcal P}\\left[\\mathcal E_{x}\\right]\\mathbb{P}_{\\mathcal P}\\left[\\tilde{\\pi}_{\\tau}(x)=a\\mid\\mathcal E_{x}\\right]}\\\\ &{\\leq\\displaystyle\\frac{1}{2}\\sum_{a\\in A}\\mathbb{P}_{\\mathcal P}[\\mathcal E_{x}]\\cdot\\left|\\mathbb{P}_{\\mathcal P}\\left[\\widehat{\\pi}_{\\tau}(x)=a\\mid\\mathcal E_{x}\\right]-\\mathbb{P}_{\\mathcal P}\\left[\\tilde{\\pi}_{\\tau}(x)=a\\mid\\mathcal E_{x}\\right]\\right|}\\\\ &{\\quad+\\displaystyle\\sum_{a\\in A}\\mathbb{P}_{\\mathcal P}[\\mathcal E_{x}^{c}]\\cdot\\left|\\mathbb{P}_{\\mathcal P}\\left[\\widehat{\\pi}_{\\tau}(x)=a\\mid\\mathcal E_{x}^{c}\\right]-\\mathbb{P}_{\\mathcal P}\\left[\\tilde{\\pi}_{\\tau}(x)=a\\mid\\mathcal E_{x}^{c}\\right]\\right|,\\quad\\mathrm{~(Jensen's~inequality)~}}\\end{array}$ \u2212PP[Exc]PP [\u03c0\u02dc\u03c4(x) = a \u2223E and since $\\begin{array}{r}{\\mathbb{P}_{\\mathcal{P}}\\left[\\widehat{\\pi}_{\\tau}(x)=a\\;\\middle\\vert\\;\\mathcal{E}_{x}\\right]=\\mathbb{P}_{\\mathcal{P}}\\left[\\tilde{\\pi}_{\\tau}(x)=a\\;\\middle\\vert\\;\\mathcal{E}_{x}\\right]\\,\\forall a\\in\\mathcal{A},}\\end{array}$ , we have that ", "page_idx": 70}, {"type": "text", "text": "", "page_idx": 70}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle=\\frac{1}{2}\\sum_{a\\in A}\\mathbb{P}_{\\mathcal{P}}[\\mathcal{E}_{x}^{c}]\\cdot|\\mathbb{P}_{\\mathcal{P}}\\left[\\widehat{\\pi}_{\\tau}(x)=a\\mid\\mathcal{E}_{x}^{c}\\right]-\\mathbb{P}_{\\mathcal{P}}\\left[\\tilde{\\pi}_{\\tau}(x)=a\\mid\\mathcal{E}_{x}^{c}\\right]|\\,,}\\\\ &{\\displaystyle\\leq\\mathbb{P}_{\\mathcal{P}}\\left[\\mathcal{E}_{x}^{c}\\right],}\\\\ &{\\displaystyle=\\mathbb{P}_{\\mathcal{P}}\\left[\\operatorname*{max}_{a\\in A}|\\widehat{\\mathcal{P}}_{\\tau,\\varepsilon,\\delta}[\\widehat{V}_{\\tau+1}](x,a)-\\mathcal{P}_{\\tau}[V_{\\tau+1}](x,a)|>4\\varepsilon\\right],}\\\\ &{\\displaystyle\\leq\\mathbb{P}_{\\mathcal{P}}\\left[\\operatorname*{max}_{a\\in A}|\\widehat{\\mathcal{P}}_{\\tau,\\varepsilon,\\delta}[\\widehat{V}_{\\tau+1}](x,a)-\\mathcal{P}_{\\tau}[\\widehat{V}_{\\tau+1}](x,a)|>\\varepsilon\\right],\\quad\\mathrm{(see~below)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "where (95) follows from $x\\in\\mathcal{E}$ and the last inequality follows from Lemma H.2. Therefore, we have ", "page_idx": 70}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mu}\\!\\left[D_{\\mathrm{tv}}(\\widehat{\\pi}_{\\tau}(\\mathbf{x}_{\\tau}),\\tilde{\\pi}_{\\tau}(\\mathbf{x}_{\\tau}))\\right]\\leq\\mathbb{P}_{\\mu}\\!\\left[\\boldsymbol{\\mathcal{E}}\\right]\\cdot\\mathbb{E}_{\\mu}\\!\\left[D_{\\mathrm{tv}}(\\widehat{\\pi}_{\\tau}(\\mathbf{x}_{\\tau}),\\tilde{\\pi}_{\\tau}(\\mathbf{x}_{\\tau}))\\mid\\boldsymbol{\\mathcal{E}}\\right]+\\mathbb{P}_{\\mu}\\!\\left[\\boldsymbol{\\mathcal{E}}^{c}\\right],}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\delta+\\nu,}\\end{array}\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "where the first inequality follows by the fact that the total variation distance is bounded by 1, and the last inequality follows by (94) and (96). \u53e3 ", "page_idx": 70}, {"type": "text", "text": "Lemma L.2. Let $\\tau\\in[H]$ and $\\varepsilon^{\\prime},\\delta,\\nu\\in(0,1)$ , and $\\zeta_{1:H}\\in[0,1/2]$ be given. Further, consider two value functions $V_{\\tau+1},\\widehat{V}_{\\tau+1}\\in[0,H]$ and measure $\\mu\\in\\Delta(\\mathcal{X})$ such that ", "page_idx": 70}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\mathbf{x}_{\\tau}\\sim\\mu}\\bigg[\\mathbb{I}\\left\\{\\operatorname*{max}_{a\\in\\mathcal{A}}\\big|\\big(\\mathcal{P}_{\\tau}[\\widehat{V}_{\\tau+1}]-\\mathcal{P}_{\\tau}[V_{\\tau+1}]\\big)(\\mathbf{x}_{\\tau},a)\\big|>3\\varepsilon^{\\prime}\\right\\}\\bigg]\\leq\\nu.\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "Further, for $x\\in\\mathrm{~\\mathcal~{~X~}~}_{\\tau}$ , let $\\begin{array}{r l r}{\\widehat{\\pi}_{\\tau}(x)}&{{}\\in}&{\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}[\\widehat{Q}_{\\tau}(x,a)/\\varepsilon^{\\prime}\\;+\\;\\zeta_{\\tau}],}\\end{array}$ , where $\\widehat{Q}_{\\tau}(x,a)\\;\\;:=\\;\\;$ $\\widehat{\\mathcal{P}}_{\\tau,\\varepsilon^{\\prime},\\delta}[\\widehat{V}_{\\tau+1}](x,a)$ , and inductively define ", "page_idx": 70}, {"type": "equation", "text": "$$\n\\tilde{\\pi}_{\\tau}(x)\\in\\mathop{\\arg\\operatorname*{max}}_{a\\in\\mathcal{A}}\\left\\{\\begin{array}{l l}{\\left[\\widehat{Q}_{\\tau}(x,a)/\\varepsilon^{\\prime}+\\zeta_{\\tau}\\right],}&{i f\\|\\widehat{Q}_{\\tau}(x,\\cdot)-\\mathcal{P}_{\\tau}[V_{\\tau+1}](x,\\cdot)\\|_{\\infty}\\leq4\\varepsilon^{\\prime},}\\\\ {\\left[\\mathcal{P}_{\\tau}[V_{\\tau+1}](x,a)/\\varepsilon^{\\prime}+\\zeta_{\\tau}\\right],}&{o t h e r w i s e.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "Then, we have ", "page_idx": 70}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\pmb{x}_{\\tau}\\sim\\mu}[D_{\\mathrm{tv}}(\\widehat{\\pi}_{\\tau}(\\pmb{x}_{\\tau}),\\tilde{\\pi}_{\\tau}(\\pmb{x}_{\\tau}))]\\leq\\nu+\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "Proof of Lemma L.2. In this proof, we let $\\mathbb{P}_{\\mu}$ denote the probability law of $\\pmb{x}_{\\tau}$ and $\\mathbb{P}_{\\mathcal{P}}$ denote the probability law of $\\widehat{\\mathcal{P}}_{\\tau,\\varepsilon,\\delta}$ . Denote by $\\mathcal{E}$ be the $\\mathbb{P}_{\\mu}$ -measurable event that $\\begin{array}{r}{\\operatorname*{max}_{a\\in A}\\big|\\big(\\mathcal{P}_{\\tau}[\\widehat{V}_{\\tau+1}]-\\mathcal{P}_{\\tau}[V_{\\tau+1}]\\big)(\\pmb{x}_{\\tau},a)\\big|\\leq3\\varepsilon^{\\prime}}\\end{array}$ . Fix $x\\in\\mathcal{E}$ , and let ${\\mathcal E}_{x}$ be the $\\mathbb{P}_{\\mathcal{P}}$ -measurable event ", "page_idx": 70}, {"type": "text", "text": "that $\\begin{array}{r}{\\operatorname*{max}_{a\\in\\mathcal{A}}|\\widehat{\\mathcal{P}}_{\\tau,\\varepsilon^{\\prime},\\delta}[\\widehat{V}_{\\tau+1}](x,a)-\\mathcal{P}_{\\tau}[V_{\\tau+1}](x,a)|\\leq4\\varepsilon^{\\prime}}\\end{array}$ . From the definition of $\\tilde{\\pi}_{\\tau}$ , we have that ", "page_idx": 71}, {"type": "text", "text": "Dtv(\u03c0\u0302\u03c4(x), \u03c0\u02dc\u03c4(x)) $\\begin{array}{l}{\\displaystyle=\\frac{1}{2}\\sum_{a\\in A}|\\mathbb{P}_{\\mathcal{P}}\\left[\\widehat{\\pi}_{\\tau}(x)=a\\right]-\\mathbb{P}_{\\mathcal{P}}\\left[\\tilde{\\pi}_{\\tau}(x)=a\\right]|,}\\\\ {\\displaystyle=\\frac{1}{2}\\sum_{a\\in A}|\\mathbb{P}_{\\mathcal{P}}[\\mathcal{E}_{x}]\\mathbb{P}_{\\mathcal{P}}\\left[\\widehat{\\pi}_{\\tau}(x)=a\\ |\\ \\mathcal{E}_{x}\\right]+\\mathbb{P}_{\\mathcal{P}}[\\mathcal{E}_{x}^{c}]\\mathbb{P}_{\\mathcal{P}}\\left[\\widehat{\\pi}_{\\tau}(x)=a\\ |\\ \\mathcal{E}_{x}^{c}\\right]-\\mathbb{P}_{\\mathcal{P}}[\\mathcal{E}_{x}]\\mathbb{P}_{\\mathcal{P}}\\left[\\tilde{\\pi}_{\\tau}(x)=a\\ |\\ \\mathcal{E}_{x}\\right]}\\\\ {\\displaystyle\\leq\\frac{1}{2}\\sum_{a\\in A}\\mathbb{P}_{\\mathcal{P}}[\\mathcal{E}_{x}]\\cdot|\\mathbb{P}_{\\mathcal{P}}\\left[\\widehat{\\pi}_{\\tau}(x)=a\\ |\\ \\mathcal{E}_{x}\\right]-\\mathbb{P}_{\\mathcal{P}}\\left[\\tilde{\\pi}_{\\tau}(x)=a\\ |\\ \\mathcal{E}_{x}\\right]|}\\\\ {\\displaystyle\\ +\\sum_{a\\in A}\\mathbb{P}_{\\mathcal{P}}[\\mathcal{E}_{x}^{c}]\\cdot|\\mathbb{P}_{\\mathcal{P}}\\left[\\widehat{\\pi}_{\\tau}(x)=a\\ |\\ \\mathcal{E}_{x}^{c}\\right]-\\mathbb{P}_{\\mathcal{P}}\\left[\\tilde{\\pi}_{\\tau}(x)=a\\ |\\ \\mathcal{E}_{x}^{c}\\right]|,\\quad\\mathrm{~(Jensen's~inequality)~}}\\end{array}$ \u2212PP[Exc]PP [\u03c0\u02dc\u03c4(x) = a \u2223E ", "page_idx": 71}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\displaystyle\\frac12\\displaystyle\\sum_{a\\in A}\\mathbb{P}_{\\mathcal{P}}\\big[\\mathcal{E}_{x}^{c}\\big]\\cdot\\big|\\mathbb{P}_{\\mathcal{P}}\\left[\\widehat{\\pi}_{\\tau}(x)=a\\ \\big|\\ \\mathcal{E}_{x}^{c}\\right]-\\mathbb{P}_{\\mathcal{P}}\\left[\\tilde{\\pi}_{\\tau}(x)=a\\ \\big|\\ \\mathcal{E}_{x}^{c}\\right]\\big|\\,,}\\\\ &{\\leq\\mathbb{P}_{\\mathcal{P}}\\left[\\mathcal{E}_{x}^{c}\\right],}\\\\ &{=\\mathbb{P}_{\\mathcal{P}}\\left[\\displaystyle\\operatorname*{max}_{a\\in A}\\big|\\widehat{\\mathcal{P}}_{\\tau,\\varepsilon^{\\prime},\\delta}\\big[\\widehat{V}_{\\tau+1}\\big](x,a)-\\mathcal{P}_{\\tau}\\big[V_{\\tau+1}\\big](x,a)\\big|>4\\varepsilon^{\\prime}\\right],}\\\\ &{\\leq\\mathbb{P}_{\\mathcal{P}}\\left[\\displaystyle\\operatorname*{max}_{a\\in A}\\big|\\widehat{\\mathcal{P}}_{\\tau,\\varepsilon^{\\prime},\\delta}\\big[\\widehat{V}_{\\tau+1}\\big](x,a)-\\mathcal{P}_{\\tau}\\big[\\widehat{V}_{\\tau+1}\\big](x,a)\\big|>\\varepsilon^{\\prime}\\right],\\quad\\mathrm{(see~below)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 71}, {"type": "text", "text": "where (98) follows from $x\\in\\mathcal{E}$ and the last inequality follows from Lemma H.2. Therefore, we have ", "page_idx": 71}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathbf{x}_{\\tau}\\sim\\mu}[D_{\\mathrm{tv}}(\\widehat{\\pi}_{\\tau}(\\mathbf{x}_{\\tau}),\\tilde{\\pi}_{\\tau}(\\mathbf{x}_{\\tau}))]\\leq\\mathbb{P}_{\\mu}[\\mathcal{E}]\\cdot\\mathbb{E}_{\\mathbf{x}_{\\tau}\\sim\\mu}[D_{\\mathrm{tv}}(\\widehat{\\pi}_{\\tau}(\\mathbf{x}_{\\tau}),\\tilde{\\pi}_{\\tau}(\\mathbf{x}_{\\tau}))\\mid\\mathcal{E}]+\\mathbb{P}_{\\mu}[\\mathcal{E}^{c}],}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\delta+\\nu,}\\end{array}\n$$", "text_format": "latex", "page_idx": 71}, {"type": "text", "text": "where the first inequality follows by the fact that the total variation is bounded by 1, and the last inequality follows by (97) and (99). \u53e3 ", "page_idx": 71}, {"type": "text", "text": "Lemma L.3. Let $x\\in\\mathbb{R}$ and $\\nu\\in(0,1/2)$ be given. Further, let $\\zeta\\in(0,1/2)$ . Then, ", "page_idx": 71}, {"type": "equation", "text": "$$\nx+\\zeta+\\nu>\\left[x+\\zeta\\right]\\quad o r\\quad x+\\zeta-\\nu\\leq\\left[x+\\zeta\\right]-1,\n$$", "text_format": "latex", "page_idx": 71}, {"type": "text", "text": "only if ", "page_idx": 71}, {"type": "equation", "text": "$$\n\\left\\lceil x\\right\\rceil-\\nu\\leq x+\\zeta\\leq\\left\\lceil x\\right\\rceil+\\nu\\quad o r\\quad\\zeta\\leq\\nu.\n$$", "text_format": "latex", "page_idx": 71}, {"type": "text", "text": "Proof of Lemma L.3. To prove the claim, it suffices to show the following items: ", "page_idx": 71}, {"type": "text", "text": "We start by showing the first item. We proceed by showing the contrapositive; that is, we will show that if $x+\\zeta\\leq\\lceil x\\rceil-\\nu$ or $x+\\zeta>\\lceil x\\rceil$ , then $x+\\zeta+\\nu\\leq\\left\\lceil x+\\zeta\\right\\rceil$ . Suppose that $x+\\zeta\\leq\\left\\lceil x\\right\\rceil-\\nu$ . This, together with the fact that $\\zeta\\geq0$ , implies that ", "page_idx": 71}, {"type": "equation", "text": "$$\n\\left\\lceil x+\\zeta\\right\\rceil=\\left\\lceil x\\right\\rceil\\geq x+\\zeta+\\nu.\n$$", "text_format": "latex", "page_idx": 71}, {"type": "text", "text": "Now, suppose that $x+\\zeta>\\lceil x\\rceil$ . Then, we have ", "page_idx": 71}, {"type": "equation", "text": "$$\n\\lceil x+\\zeta\\rceil\\geq\\left\\lceil x\\right\\rceil+1\\geq\\left\\lceil x\\right\\rceil+\\zeta+\\nu\\geq x+\\zeta+\\nu,\n$$", "text_format": "latex", "page_idx": 71}, {"type": "text", "text": "where the penultimate inequality follows by $\\zeta,\\nu\\in(0,1/2)$ . ", "page_idx": 71}, {"type": "text", "text": "We now prove the second claim. Again, we proceed by showing the contrapositive; that is, we will show that if $\\{\\lceil x\\rceil+\\nu<x+\\zeta$ or $\\lceil x\\rceil\\geq x+\\zeta\\}$ and $\\zeta>\\nu$ , then $x+\\zeta-\\nu>\\lceil x^{\\bar{+}}\\zeta\\rceil-1$ . ", "page_idx": 71}, {"type": "text", "text": "Suppose that $\\left\\lceil x\\right\\rceil+\\nu\\,<\\,x+\\zeta$ and $\\zeta\\,>\\,\\nu$ . The first inequality together with $\\nu\\,\\geq\\,0$ implies that $\\lceil x+\\zeta\\rceil>\\lceil x\\rceil$ . On the other hand, since $\\zeta\\leq1/2$ , we have $\\lceil{\\bar{x}}+\\zeta\\rceil\\leq\\lceil{\\bar{x}}\\rceil+1$ , and so ", "page_idx": 71}, {"type": "equation", "text": "$$\n\\left\\lceil x+\\zeta\\right\\rceil-1=\\left\\lceil x\\right\\rceil<x+\\zeta-\\nu,\n$$", "text_format": "latex", "page_idx": 71}, {"type": "text", "text": "where the last inequality follows by the current assumption that $\\lceil x\\rceil+\\nu<x+\\zeta$ . ", "page_idx": 72}, {"type": "text", "text": "Now, suppose that $\\left\\lceil x\\right\\rceil\\geq x+\\zeta$ and that $\\zeta>\\nu$ . Then, we have ", "page_idx": 72}, {"type": "equation", "text": "$$\n\\lceil x+\\zeta\\rceil\\leq\\lceil x\\rceil\\leq x+1<x+\\zeta-\\nu+1,\n$$", "text_format": "latex", "page_idx": 72}, {"type": "text", "text": "where the last inequality follows by $\\zeta>\\nu$ . Rearranging (100) completes the proof. ", "page_idx": 72}, {"type": "text", "text": "M BehaviorCloning Algorithm and Analysis ", "text_level": 1, "page_idx": 72}, {"type": "text", "text": "In this section, we give a self-contained presentation and analysis for the standard behavior cloning algorithm for imitation learning (e.g., Ross and Bagnell [46]), displayed in Algorithm 10. Given access to trajectories from an expert policy $\\widehat{\\pi}_{1:H}$ (which may be non-executable in the sense of Definition 2.1) the algorithm learns an executable policy $\\pi^{\\mathrm{bc}}$ with similar performance. We use this scheme within RVFS.bc and RVF $S^{\\in\\times0}$ .bc. ", "page_idx": 72}, {"type": "text", "text": "Algorithm 10 BehaviorCloning: Imitation Learning Algorithm. ", "text_level": 1, "page_idx": 72}, {"type": "text", "text": "1: input: Policy class $\\Pi\\subseteq\\Pi_{S}$ , expert policy $\\widehat{\\pi}_{1:H}$ , suboptimality $\\varepsilon\\ \\in\\ (0,1)$ , and confidence   \n$\\delta\\in(0,1)$ .   \n2: Set $\\dot{N_{\\mathrm{bc}}}=16H^{2}\\log(|\\Pi|/\\delta)/\\varepsilon$ .   \n3: Set $\\mathcal{D}\\gets\\emptyset$ .   \n4: for $i=1,\\dots,N_{\\mathrm{bc}}$ do   \n5: Generate trajectory $\\pmb{\\tau}=\\left(\\left(\\pmb{x}_{1},\\pmb{a}_{1}\\right),\\pmb{\\ldots}.\\ldots,\\left(\\pmb{x}_{H},\\pmb{a}_{H}\\right)\\right)\\sim\\mathbb{P}^{\\widehat{\\pmb{\\pi}}}$ .   \n6: Update $D\\leftarrow D\\cup\\{\\tau\\}$ .   \n7: Compute $\\begin{array}{r}{\\pi^{\\mathrm{bc}}\\in\\arg\\operatorname*{min}_{\\pi\\in\\Pi}\\sum_{\\left(\\left(x_{1},a_{1}\\right),\\ldots,\\left(x_{H},a_{H}\\right)\\right)\\in\\mathcal{D}}\\sum_{h\\in[H]}\\mathbb{I}\\{\\pi_{h}(x_{h})\\neq a_{h}\\}.}\\end{array}$   \n8: Return $\\pi^{\\mathrm{bc}}$ . ", "page_idx": 72}, {"type": "text", "text": "Proposition M.1. Let $\\varepsilon,\\delta\\in(0,1)$ be given and let $\\Pi\\subseteq\\Pi_{S}$ and $\\widehat{\\pi}_{1:H}$ be an expert policy such that ", "page_idx": 72}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\pi\\in\\Pi}\\sum_{h=1}^{H}\\mathbb{P}^{\\widehat\\pi}[\\widehat\\pi_{h}(\\pmb{x}_{h})\\neq\\pmb{\\pi}_{h}(\\pmb{x}_{h})]\\leq\\varepsilon_{\\mathsf{m i s}}.\n$$", "text_format": "latex", "page_idx": 72}, {"type": "text", "text": "Then, the policy $\\pi_{1:H}^{\\mathsf{b c}}=$ BehaviorCloning $(\\Pi,\\varepsilon,\\widehat{\\pi}_{1:H},\\delta)$ returned by Algorithm 10 satisfies, with probability at least $1-\\delta$ , ", "page_idx": 72}, {"type": "equation", "text": "$$\nJ(\\widehat{\\pi})-J(\\pi^{\\mathsf{b c}})\\leq4H\\varepsilon_{\\sf m i s}+\\varepsilon/2.\n$$", "text_format": "latex", "page_idx": 72}, {"type": "text", "text": "Proof of Proposition M.1. First, by the performance difference lemma, we have ", "page_idx": 72}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[V_{1}^{\\widehat{\\pi}}({\\pmb x}_{1})]-\\mathbb{E}[V_{1}^{\\pi^{\\mathrm{bc}}}({\\pmb x}_{1})]=\\displaystyle\\sum_{h=1}^{H}\\mathbb{E}^{\\widehat{\\pi}}[Q_{h}^{\\pi^{\\mathrm{bc}}}({\\pmb x}_{h},\\widehat{\\pi}_{h}({\\pmb x}_{h}))-Q_{h}^{\\pi^{\\mathrm{bc}}}({\\pmb x}_{h},\\pi_{h}^{\\mathrm{bc}}({\\pmb x}_{h}))],}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq H\\displaystyle\\sum_{h=1}^{H}\\mathbb{P}^{\\widehat{\\pi}}[\\widehat{\\pi}_{h}({\\pmb x}_{h})\\neq\\pi_{h}^{\\mathrm{bc}}({\\pmb x}_{h})].}\\end{array}\n$$", "text_format": "latex", "page_idx": 72}, {"type": "text", "text": "We now bound the probability terms on the right-hand side. Fix $h\\in[H]$ and let $\\mathcal{D}$ be the dataset in Algorithm 10, which consists of $N_{\\mathsf{b c}}$ i.i.d. trajectories $((x_{1},\\pmb{a}_{1}),\\Bar{\\dots},\\Bar{(\\pmb{x}_{H},\\pmb{a}_{H})})$ generated by rolling with $\\widehat{\\pi}_{1:H}$ . By Lemma C.4 (with i.i.d. data, $B=H$ , and $\\mathcal{Q}=\\Pi$ ), we have that, with probability at least $1-\\delta$ , ", "page_idx": 72}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\forall\\pi\\in\\Pi,}&{}&{\\displaystyle\\sum_{((x_{1},a_{H}),\\ldots,(x_{H},a_{H}))\\in\\mathcal{D}\\,h\\in[H]}\\mathbb{I}\\{\\pi_{h}(x_{h})\\neq\\widehat{\\pi}(x_{h})\\}\\leq2\\sum_{h\\in[H]}\\mathbb{P}^{\\widehat{\\pi}}[\\pi_{h}(x_{h})\\neq\\widehat{\\pi}_{h}(x_{h})]}\\\\ &{}&{\\displaystyle+\\,\\frac{2H\\log(2|\\Pi|/\\delta)}{N_{\\mathrm{bc}}},}&{(1+\\frac{2H^{2}}{N_{\\mathrm{bc}}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 72}, {"type": "text", "text": "and ", "page_idx": 72}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall\\pi\\in\\Pi,\\ \\ \\displaystyle\\sum_{h\\in[H]}\\mathbb{P}^{\\widehat\\pi}[\\pi_{h}(x_{h})\\neq\\widehat\\pi_{h}(x_{h})]\\leq2\\sum_{((x_{1},a_{H}),\\ldots,(x_{H},a_{H}))\\in\\mathcal{D}}\\sum_{h\\in[H]}\\mathbb{I}\\{\\pi_{h}(x_{h})\\neq\\widehat\\pi(x_{h})\\}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\,\\displaystyle\\frac{4H\\log(2|\\Pi|/\\delta)}{N_{\\mathrm{bc}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 72}, {"type": "text", "text": "Taking the infimum over $\\pi$ on both sides of (103) and using the definition of $\\pi_{h}^{\\mathsf{b c}}$ in Algorithm 10 gives: ", "page_idx": 73}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{((x_{1},a_{H}),\\dots,(x_{H},a_{H}))\\in{\\mathcal D}}\\displaystyle\\sum_{h\\in[H]}\\mathbb{I}\\{\\pi_{h}^{\\mathrm{bc}}(x_{h})\\neq\\widehat\\pi(x_{h})\\}\\leq2\\operatorname*{inf}_{\\pi\\in\\Pi}\\displaystyle\\sum_{h\\in[H]}\\mathbb{I}^{\\#}[\\pi_{h}(x_{h})\\neq\\widehat\\pi_{h}(x_{h})]}&{}\\\\ {\\displaystyle+\\,\\frac{2H\\log(2|\\Pi|/\\delta)}{N_{\\mathrm{bc}}},}&{}\\\\ {\\displaystyle}&{\\leq2\\varepsilon_{\\mathtt{m i s}}+\\frac{2H\\log(2|\\Pi|/\\delta)}{N_{\\mathrm{bc}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 73}, {"type": "text", "text": "where the last inequality follows from (101). Using this together with (104), instantiated with $\\pi\\equiv\\pi^{\\mathsf{b c}}$ , we get that with probability at least $1-\\delta$ : ", "page_idx": 73}, {"type": "equation", "text": "$$\n\\sum_{h\\in[H]}\\mathbb{P}^{\\widehat\\pi}[\\pi_{h}^{\\mathrm{bc}}(x_{h})\\neq\\widehat\\pi_{h}(x_{h})]\\leq4\\varepsilon_{\\mathrm{mis}}+\\frac{8H\\log(2|\\Pi|/\\delta)}{N_{\\mathrm{bc}}}.\n$$", "text_format": "latex", "page_idx": 73}, {"type": "text", "text": "Plugging this into (102), we get that with probability at least $1-\\delta$ : ", "page_idx": 73}, {"type": "equation", "text": "$$\n\\mathbb{E}[V_{1}^{\\widehat{\\pi}}(\\alpha_{1})]-\\mathbb{E}[V_{1}^{\\pi^{\\mathrm{bc}}}(x_{1})]\\le4H\\varepsilon_{\\mathtt{m i s}}+\\frac{8H^{2}\\log(2|\\Pi|/\\delta)}{N_{\\mathrm{bc}}}\\le4H\\varepsilon_{\\mathtt{m i s}}+\\varepsilon/2,\n$$", "text_format": "latex", "page_idx": 73}, {"type": "text", "text": "where the last inequality follows by the fact that $N_{\\mathrm{bc}}\\,=\\,16H^{2}\\log(2|\\Pi|/\\delta)/\\varepsilon$ . This completes the proof. \u53e3 ", "page_idx": 73}]