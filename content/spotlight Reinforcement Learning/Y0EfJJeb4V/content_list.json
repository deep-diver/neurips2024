[{"type": "text", "text": "Goal Reduction with Loop-Removal Accelerates RL and Models Human Brain Activity in Goal-Directed Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Huzi Cheng Department of Psychological and Brain Sciences Indiana University Bloomington hzcheng15@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Joshua W. Brown Department of Psychological and Brain Sciences Indiana University Bloomington jwmbrown@iu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Goal-directed planning presents a challenge for classical RL algorithms due to the vastness of the combinatorial state and goal spaces, while humans and animals adapt to complex environments, especially with diverse, non-stationary objectives, often employing intermediate goals for long-horizon tasks. Here, we propose a goal reduction mechanism for effectively deriving subgoals from arbitrary and distant original goals, using a novel loop-removal technique.1 The product of the method, called goal-reducer, distills high-quality subgoals from a replay buffer, all without the need for prior global environmental knowledge. Simulations show that the goal-reducer can be integrated into RL frameworks like Deep Q-learning and Soft Actor-Critic. It accelerates performance in both discrete and continuous action space tasks, such as grid world navigation and robotic arm manipulation, relative to the corresponding standard RL models. Moreover, the goal-reducer, when combined with a local policy, without iterative training, outperforms its integrated deep RL counterparts in solving a navigation task. This goal reduction mechanism also models human problem-solving. Comparing the model\u2019s performance and activation with human behavior and fMRI data in a treasure hunting task, we found matching representational patterns between a goal-reducer agent\u2019s components and corresponding human brain areas, particularly the vmPFC and basal ganglia. The results suggest that humans may use a similar computational framework for goal-directed behaviors. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Humans and animals must develop capabilities to pursue time-varying goals in continuously changing environments. The pressure for survival prohibits slow, linear adaptation to different goals, i.e., learning value functions from scratch for each new objective. A quick and versatile paradigm is necessary for such goal-directed learning scenarios. However, traditional Reinforcement Learning (RL) algorithms are not specifically designed for this, encountering challenges in the goal-directed learning context. They are highly optimized for scenarios with relatively fixed goals, e.g., winning in Go, or reducing building energy consumption [51]. In these situations, iterative methods like the ", "page_idx": 0}, {"type": "text", "text": "Bellman equation are effective for approximating value/advantage/negative cost functions across various states, maintaining stability over time. Nonetheless, if the goal changes during training, classical RL becomes highly inefficient due to: 1) The significant increase in state space caused by the introduction of the goal set; 2) The inability to reuse experiences across different goals. ", "page_idx": 1}, {"type": "text", "text": "On the other hand, traditional heuristic algorithms like Dijkstra\u2019s shortest path algorithm [12] and the $\\mathbf{A}^{*}$ path-finding algorithm [24] excel in these problems. These algorithms leverage goalindependent environment knowledge to construct real-time goal-conditioned maps during navigation. They use selective intermediate goal states to simplify the problem, thus reducing pathfinding time. By breaking down a distant goal into nearer ones, these algorithms utilize local knowledge, independent of goal changes, to solve the problem. More importantly, this principle resembles ways humans use to deal with complex problems. Recently, a number of studies have shown that, besides stimuli and association representation, human and animal brains leverage goal and reduced subgoal representations to solve tasks in various settings [44, 11, 45, 40]. But a common drawback of these algorithms is that they require predefined representations of all states in the task, prohibiting them from scaling to large-scale realistic tasks. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we try to bridge such goal reduction mechanisms with neural models. The resulting algorithm is an effective goal-reducer that can accelerate and beat standard RL in different tasks through recursively reducing complex goals into simpler subgoals. The main contributions of this paper are two-fold: 1) Computationally, we propose novel methods to train an effective goal-reducer. After just random walking, it extracts nearly optimal subgoals when the state-goal combination space is large without prior knowledge about the cost/effort between them. This is based on the effective representation structure learned during training. This mechanism, as shown below, can not only be used to accelerate standard RL algorithms in various settings but also to guide navigation in a multi-goal setting without iterative Bellman equation convergence processes and outperforms RL accelerated with it, let alone standard RL itself. 2) Biologically, by comparing the model\u2019s activity with brain fMRI data in the same cognitive task, we show the similarity between the model\u2019s component, e.g., the goal-reducer, and brain regions like vmPFC and putamen, highlighting the potential of using this model to explain how brains solve goal-directed problems. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Goal-directed RL in deep learning ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Developing an effective goal-directed algorithm has been a long-lasting open question in the RL community [2]. Various methods have been proposed to mitigate the experience sparsity issue in goal-directed learning. For an extensive review, we refer the reader to [35]. Here, we introduce some representative solutions, where some focus on the modification of reward functions, using tricks like average action cost [25] and sibling trajectory average goals [49] to alleviate the sharpness in reward distribution in the early stage of training. Another line of work uses planning to solve the problem. For example, an explicit graph can be constructed from sensory inputs for traditional path-finding algorithms [16, 55, 34, 26]. However, we see these approaches as not completely neural-based models, and the pathfinding algorithm part may prohibit them from smoothly scaling to tasks with larger state spaces. ", "page_idx": 1}, {"type": "text", "text": "Another family of algorithms uses subgoal generation as a core mechanism to resolve the same issue. [18] trained a GAN to predict intermediate goals. There are also attempts to construct subgoals with different heuristics: [56] uses the uncertainty of Q-functions to help train subgoals. [9] treats the midpoint of value functions as optimal subgoals during training. [3] optimizes subgoal generation by minimizing integrated segment costs produced by \u201clocal\u201d subgoals. These subgoalrelated approaches are intriguing as they match the \u201cdivide and conquer\u201d principle in a neural way, but a common limitation of these methods is that they work in a bootstrapping manner, i.e., the quality of the Q-function indirectly determines the quality of subgoals generated, as it is involved in the subgoal sampling process. ", "page_idx": 1}, {"type": "text", "text": "2.2 Goal-directed learning and subgoal generation in neuroscience ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "There exists a duality of interest in neuroscience on goal-directed learning that can be dated back to the 1940s, when [48] famously showed that rats can take shortcuts that hadn\u2019t been experienced before to reach goals in a maze. Later research showed that there are representations for different goals in the brain that are tailored for planning optimal paths to the ultimate goal [13]. Models have been built for these processes to reveal possible mechanisms of how the brain may make use of these goal representations to calculate subgoals (or called \u201clandmarks\u201d) for navigation [46, 47]. Recently, more empirical and modeling work has come out showing that the subgoal navigation hypothesis, and the underlying cognitive map theory that supports it [5], could be implemented by animals [23, 45] and humans [50, 15, 54]. Altogether, these studies imply the existence of a subgoal generation mechanism in the brain to support effective planning in complex environments. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "However, many models for subgoal generation in neuroscience still either rely on manual/one-hot coding of states [15, 54] or focus on revealing the existence rather than the potential development of subgoals during training [45]. We therefore think that, in these two fields, the community of deep RL and neuroscience, there is space for a neurally plausible algorithm that can provide a biological model of how subgoals are naturally generated from the brain during training and can show its computational efficiency over plain RL algorithms. ", "page_idx": 2}, {"type": "text", "text": "Our resulting work presents a trained goal-reducer neural network that generates nearly optimal subgoals without requiring additional value information from the environment. This approach distinguishes itself from other subgoal generators that depend on Q-functions. It can integrate seamlessly with standard RL frameworks and independently operate with a local policy that just learns associations in neighboring states. Using the latter approach, we also demonstrate its capability in solving cognitive tasks in a human-like manner and its correspondence with various brain regions, indicating its potential for modeling human problem-solving processes. ", "page_idx": 2}, {"type": "text", "text": "3 Methods ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Problem formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "For a goal-directed Markov decision process, we characterize it with $({\\cal S},{\\cal A},{\\mathcal G})$ , where $\\boldsymbol{S}$ and $\\boldsymbol{\\mathcal{A}}$ are the observation and action spaces, respectively, and $\\mathcal{G}$ is the goal space. One interaction step in this environment can be written as $(s_{t},g,a_{t},r_{t},\\hat{g}_{t},s_{t+1})$ , where $s_{t}$ is the current observation, $g$ is the assigned goal, $a_{t}$ is the action executed, $r_{t}$ is the reward, ${\\hat{g}}_{t}$ is the achieved goal, and $s_{t+1}$ is the next state. In some cases, ${\\mathcal{G}}={\\mathcal{S}}$ occurs when a possible goal is among one of all states, e.g., spatial navigation when the input and goal space is all plausible locations. In other words, the goal and state space are the same. In such settings, we may use $\\mathcal{G}$ and $\\boldsymbol{S}$ interchangeably, and the interaction can be reduced to $\\left({{s_{t}},{g,a_{t}},{r_{t}},{s_{t+1}}}\\right)$ . But in some real-world tasks, $\\mathcal{G}$ may be in a different space from $\\boldsymbol{S}$ $\\mathcal{G}$ could be a target coordinate in allocentric space when $\\boldsymbol{S}$ is the pose of a multi-joint robot arm. ", "page_idx": 2}, {"type": "text", "text": "With this formulation, we first consider the simple case when ${\\mathcal{G}}={\\mathcal{S}}$ as the general case can be easily extended from it. In goal-directed learning, $g\\in{\\mathcal{G}}$ may change substantially, making conventional RL algorithms inadequate due to the enormous size of ${\\mathcal{S}}\\times{\\mathcal{G}}$ . Additionally, in many scenarios, the reward is sparse, and the agent only receives a positive reward or avoids punishment when it reaches the goal state, i.e., when ${\\mathcal{G}}={\\mathcal{S}}$ and $r(s_{t},a_{t},\\bar{g})=\\mathbb{1}(s_{t}=g)$ . This, combined with the complexity of the state space, further complicates goal-directed learning. ", "page_idx": 2}, {"type": "text", "text": "We propose that, given a specific $g$ , if an agent can effectively reduce a goal $g$ into a subgoal $s_{g}\\in\\mathcal{G}$ that is \u201ccloser\u201d to its current state $s_{t}$ , it may alleviate the task\u2019s difficulty. Furthermore, this can be applied recursively to find subgoals that are arbitrarily close to the current state $s_{t}$ . ", "page_idx": 2}, {"type": "text", "text": "3.2 Effective goal-reducer through Loop-removal sampling ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The most straightforward solution to the problem above is to train a function $\\Phi$ that generates $s_{g}$ , which we refer to as a goal-reducer: $\\Phi(s_{t},g)\\,:\\,S\\times\\mathcal{G}\\,\\rightarrow\\,\\mathcal{G}$ . The goal-reducer can reduce the computational burden of a policy $\\pi(a|s,g)$ as it can now generate a subgoal $s_{g}=\\Phi(s,g)$ for a hard task, under a basic assumption that harder problems can be decomposed into simpler problems as long as these problems are in the optimal path towards the final solution (Fig. 1A). Training such a goal-reducer, however, can be challenging. There are two intuitive strategies. First, one can sample $s_{g}$ uniformly from $\\boldsymbol{S}$ , as done by [9]. This approach is referred to as Random sampling (Fig. 1B left). An alternative is to sample $s_{g}$ from past experiences, a strategy known as Trajectory sampling. Assuming a sequence of interactions $(s_{t},g,a_{t},r_{t},s_{t+1})$ for $t=1,2,\\cdots,T$ is stored in memory, $s_{g}$ can be sampled from the $s_{t}$ in this sequence (Fig. 1B middle). A common technique in goal-directed ", "page_idx": 2}, {"type": "text", "text": "RL, Hindsight Experience Replay (HER), proposed by [2], conceptually resembles this approach, as it encourages the agent to learn associations between current states and some of its future states in the same trajectory. Other studies, such as [41], also find trajectory-based sampling effective in goal-directed learning, though expert experience is not required in our case [41]. Furthermore, neuroscience research indicates that episodic memories, i.e., sequences of experienced states, are vital for learning [20]. ", "page_idx": 3}, {"type": "text", "text": "However, this strategy does not guarantee the sampled $s_{g}$ to be effective: an agent with limited environmental knowledge may simply engage in a random walk within the state transition graph, rendering the states experienced in the episodic memory ineffective for connecting the trajectory\u2019s start and end points. To address this, we introduce a third strategy, termed Loop-removal sampling (Fig. 1B right), which may mitigate this issue. The underlying rationale is that when the agent has minimal knowledge of the environment, the trajectories it creates will likely be random and involve numerous \u201cloops\u201d. A \u201cloop\u201d occurs when the agent revisits the same state at least twice within a trajectory. The Loop-removal sampling posits that by eliminating these \u201cloops\u201d from the episodic memory, the $s_{g}$ sampled from the remaining trajectories will be more advantageous, as ineffective experiences are excluded, potentially resulting in a trajectory that more closely resembles a linear or shortest path in the best-case scenario. ", "page_idx": 3}, {"type": "text", "text": "While Loop-removal sampling is effective when $\\boldsymbol{S}$ is discrete, i.e., observations are either the same or different, it faces challenges in environments where $s_{i}\\in\\mathcal S$ can be infinitely close but not the same to each other, e.g., when the observation is an image input for an agent\u2019s current location. This issue is also encountered in earlier concepts like [28]. To overcome this, we adopt an idea from persistent homology [14], defining a filtration process to determine the existence of \u201cloops.\u201d ", "page_idx": 3}, {"type": "image", "img_path": "Y0EfJJeb4V/tmp/d9dec8b809355f24eb2fb47cc54761da4533d7f8a302a4ccd657d23348b828ed.jpg", "img_caption": ["Fig. 1: A: A schematic view of how a goal-reducer can be integrated with a policy network to generate actions. B: 3 types of subgoal sampling strategies. Red triangles: current states, green squares: goals, light green squares: subgoals sampled. C: An diagram of filtration radius changes trajectory connectivity. D: An example of proper filtered trajectory (black) compared with original random walk (gray) in 3D space. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "This process is schematically depicted in Fig. 1C: In a continuous space, trajectories are sparse, as distances between states are always greater than zero, making exact state overlaps unlikely. However, by assigning a filtration radius $\\rho$ to each state and incrementally increasing it, the connectivity of the episodic memory trajectory changes. At a certain point, the algorithm detects a \u201cloop\u201d (proper case in Fig. 1C), where Loop-removal sampling is most effective. If the flitration radius continues to increase, the trajectory becomes fully interconnected, leading to Loop-removal sampling failure. We demonstrate that with an appropriate filtration parameter, a random walk path in 3D space can be effectively simplified (Fig. 1D) with $\\rho$ set to 0.8. The optimal radius is determined through a grid search between 0 and the longest distance between any pair of points in the space, with the objective of maximizing the rate at which task performance improves with learning. This search process is also used in later experiments to maximize learning efficiency in terms of two efficiency metrics, Optimality and Equidex (described in detail below). ", "page_idx": 3}, {"type": "text", "text": "This approach may seem akin to the Search on the Replay Buffer by [17], but there is a key distinction: Search on the Replay Buffer depends on a value function to estimate distances among states, whereas Loop-removal sampling operates independently of any prior environmental knowledge. Instead, it relies on a minimal assumption applicable to all state spaces: loops indicate redundancy. Following this, we developed an online mechanism ( Algorithm 1 in appendix) to train the goal-reducer, $\\Phi$ , using loop-removed trajectories stored in $\\mathcal{D}^{\\prime}$ , which is refined from the replay buffer $\\mathcal{D}$ . ", "page_idx": 4}, {"type": "text", "text": "To test if a goal-reducer trained with Loop-removal sampling generates better subgoals compared to Trajectory sampling and Random sampling, we developed two metrics to quantify the quality of generated subgoals: Optimality and Equidex. First, we represent the effort required for an agent to reach goal $g$ from state $s$ as $||s,g||$ , the distance across legal transitions of the state graph. This concept mirrors the idea of shortest distances, though it may not be symmetrical, i.e., $||s,g||\\neq||g,s||$ , due to potentially irreversible state transitions. Also, the state space graph of legal transitions may not be fully connected \u2013 because barriers may exist in the state space so that some transitions are not possible, $s_{g}$ is generally not simply the linear midpoint between $s$ and $g\\in S$ . Building on this, Optimality is defined as: ", "page_idx": 4}, {"type": "equation", "text": "$$\nO p t i m a l i t y(s,g,s_{g})=||s,g||/\\big(||s,s_{g}||+||s_{g},g||\\big).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "When Optimality $\\rightarrow1$ , the total effort expended by an agent to reach $g$ via $s_{g}$ is near the optimal effort. However, a $s_{g}$ with an Optimality close to 1 may not be informative if $s_{g}$ is set to either $g$ or $s$ , where no new information is provided. To address this, Equidex is introduced: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{E q u i d e x(s,g,s_{g})=\\big(||s_{g},g||-||s,s_{g}||\\big)/\\big(||s,s_{g}||+||s_{g},g||\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "When Equidex $\\rightarrow0$ , it indicates that the effort from $s$ to subgoal $s_{g}$ is similar to the effort from $s_{g}$ to the final goal $g$ . This suggests that $s_{g}$ is situated on the hyperplane formed by midpoints in the state space between $s$ and $g$ . Accordingly, as Equidex $\\rightarrow1$ , the subgoal becomes closer to the current state, while a value nearing $^{-1}$ suggests the subgoal is closer to the ultimate goal. In summary, the quality of a single subgoal can be characterized by both Optimality and Equidex. The ideal subgoal is one where Optimality equals 1 and Equidex equals 0. ", "page_idx": 4}, {"type": "image", "img_path": "Y0EfJJeb4V/tmp/d05c93354db1483e4665baf9fb061105b3163f7c262e57c125d879db4c3fec50.jpg", "img_caption": ["Fig. 2: goal-reducer training results of geometric random graph (top) and the four-room gridworld task (bottom) with different strategies. A: Environment examples. B: Left: training loss, middle: training Optimality, right: training Equidex. C: Left: Optimality change when applying a trained goal-reducer recursively, right: same, but for Equidex. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Using these indices, we assess the performance of the goal-reducer on two datasets. The first dataset derives from a constructed state graph, characterized by random connections among states and allowing self-connections (Fig. 2A top). We execute undirected random walks on this graph. The second is based on a four-room gridworld task, commonly used in multi-goal RL benchmark tests (Fig. 2A bottom). In this dataset, we simulate an agent exploring the environment without any prior knowledge of its structure. ", "page_idx": 4}, {"type": "text", "text": "For both datasets, we use the goal-reducer architecture to learn the $s,g\\to s_{g}$ association, employing an VAE [31]. This network accepts concatenated representations of $s$ and $g$ as inputs, initially producing a latent probabilistic representation $z$ through the encoder $\\mathrm{Enc}_{\\Phi}$ . A prediction of $s_{g}$ is then generated by ${\\mathrm{Dec}}_{\\Phi}$ . To circumvent sparse and discrete encoding in both datasets, and also to avoid introducing prior knowledge, random embeddings are applied for each. Consequently, $s$ , $g$ , and $s_{g}$ are represented as high-dimensional random vectors. All random embeddings remain fixed during training. The adopted loss function mirrors that of a classical VAE, comprising a subgoal reconstruction loss and a weighted $(\\alpha)$ KL divergence penalty with a prior latent distribution $p(z)\\sim\\mathcal{N}(0,I)$ : ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\nL=-\\alpha D_{K L}(\\mathrm{Enc}_{\\Phi}(s_{g}|s,g)||p(z))+\\mathbb{E}_{\\mathrm{Enc}_{\\Phi}(z|s,g)}[\\log\\mathrm{Dec}_{\\Phi}(s_{g}|z)].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Training results (Fig. 2B left) indicate that in both datasets Loop-removal sampling surpasses and is more stable than Trajectory sampling, which in turn outperforms Random sampling in terms of loss. The Optimality values follow a similar trend (Fig. 2B middle): ", "page_idx": 5}, {"type": "text", "text": "Loop-removal sampling $>$ Trajectory sampling $\\gg$ Random sampling. ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Given that all subgoals are uniformly sampled from the dataset, the Equidex for all strategies approximates 0 (Fig. 2B right). To further examine the capability and stability of the trained goalreducer, we assess its proficiency in recursive subgoal reduction. In this process, we input the predicted subgoal embedding into the goal-reducer, treating it as $g$ to generate a subsequent subgoal $s_{g}^{\\prime}$ . By iterating this procedure for several steps, we postulate that the goal-reducer, particularly when trained with Loop-removal sampling, will produce subgoals that are both effective and sufficiently close to the current states to facilitate easier navigation. The experimental results reveal that, after three iterations of recursive goal reduction $~t\\,=\\,3$ , in Fig. 2C), the goal-reducer trained with Loop-removal sampling achieves the most favorable optimality distribution (Fig. 2C left). ", "page_idx": 5}, {"type": "text", "text": "In terms of Equidex, as illustrated in the right column of Fig. 2C, recursive goal reductions enhance the goal-reducer\u2019s ability to predict subgoals that are nearer to the current states. This improvement is evidenced by the shift in the equidex distribution from $t=1$ to $t=3$ . Together, the results in Fig. 2 demonstrate the superiority of Loop-removal sampling as a training strategy for goal-reducer to produce proper subgoals. ", "page_idx": 5}, {"type": "text", "text": "3.3 goal-reducer integrated with RL ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The above experiments have shown the superiority of Loop-removal sampling over other strategies in training goal-reducer from environments without external knowledge about the \u201cdistance\u201d between any two states in them. Next we integrate this process with RL algorithms into tasks with discrete and continuous action spaces using Deep Q-learning (DQL) [38] and Soft Actor-Critic (SAC) [22]. ", "page_idx": 5}, {"type": "text", "text": "Discrete case In the discrete setting, DQL is used. The algorithm is trained to optimize a value function $Q$ through Bellman equation convergence: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{\\phi_{k+1}}=\\arg\\operatorname*{min}_{\\phi}\\cfrac{1}{2}\\mathbb{E}_{(s_{t},g,a_{t},s_{t+1})\\sim D}\\left[Q_{t}^{*}-Q_{\\phi}\\left(s_{t},g,a_{t}\\right)\\right]^{2}}\\\\ &{\\quad\\quad Q_{t}^{*}=r\\left(s_{t},g,a_{t}\\right)+\\gamma\\mathbb{E}_{a_{t+1}\\sim\\pi\\left(..\\vert s_{t+1},g\\right)}Q_{\\phi_{k}}\\left(s_{t+1},g,a_{t+1}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Since we assume goal-reducer, $\\Phi(s,g)$ , can learn to generate subgoals without a mature $Q$ , we use it to accelerate the convergence of Eq. 4 by updating $Q$ \u2019s parameter using an extra regularization loss ", "page_idx": 5}, {"type": "equation", "text": "$$\nL_{s_{g}}=\\sum_{s_{t},g}w_{s_{t},g}\\cdot D_{K L}\\bigg[\\pi\\big(a_{t}|s_{t},g\\big)\\bigg|\\bigg|\\pi\\big(a_{t}|s_{t},\\Phi(s,g)\\big)\\bigg],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the policy $\\pi\\left(\\boldsymbol{a}_{t}|\\boldsymbol{s}_{t},\\boldsymbol{g}\\right)$ is a categorical distribution softmax $Q(s_{t},g)$ and the loss is weighted by the entropy of policy, $H[\\pi(a_{t}|s_{t},g)]$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\nw_{s_{t},g}=\\binom{1,}{0,}\\cdot\\mathrm{~if~}H[\\pi(a_{t}|s_{t},g)]\\geq H[\\pi(a_{t}|s_{t},\\Phi(s,g))]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Continuous case In continuous action space, SAC is used. Since $\\pi\\big(a_{t}|s_{t},g\\big)$ cannot be calculated explicitly, we approximate it with an online sampling process of actions executed $a_{t}^{\\prime}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\nL_{s_{g}}=\\sum_{s_{t},a_{t}^{\\prime},g}w_{s_{t},g}\\cdot\\pi\\big(a_{t}^{\\prime}|s_{t},g\\big)\\cdot\\log\\pi\\big(a_{t}^{\\prime}|s_{t},\\Phi(s,g)\\big),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the $w_{s_{t},g}=1$ if $\\pi\\big(a_{t}^{\\prime}|s_{t},g\\big)\\,<\\,\\pi\\big(a_{t}^{\\prime}|s_{t},\\Phi(s,g)\\big)$ and otherwise 0. In both cases, $w_{s_{t},g}=1$ means $Q$ is more uncertain about the ultimate goal when compared with a trustworthy goal generated by the goal-reducer. ", "page_idx": 6}, {"type": "text", "text": "For both cases, for baseline RL methods (plain DQL and SAC) and their goal-reducer augmented version, to accelerate learning, we used Hindsight Experience Replay (HER) during training, a standard technique used to improve RL algorithm\u2019s performance in goal-directed learning [2]. ", "page_idx": 6}, {"type": "text", "text": "3.4 Standalone goal-reducer ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "As shown in previous Optimality and Equidex experiments, goal-reducer can gradually reduce the \u201cdistance\u201d between the agent and the goal through recursive goal reduction. We thus test if this mechanism alone can solve some tasks that are usually handled with model-free RL using Bellman equation iteration. To do this, we take an unsupervised approach: When an agent is initialized, it explores the environment randomly. We train the goal-reducer using such exploration trajectories. At the same time, we train a local policy $\\pi_{\\mathrm{local}}\\bigl(a_{t}|\\breve{s}_{t},g\\bigr)$ that only learns goals that are one step away from $s_{t}$ and generates a uniform distribution otherwise. When these two components are trained, we execute the planning process by detecting reachable goals using the entropy of $\\pi_{\\mathrm{local}}$ (for details, see the appendix). ", "page_idx": 6}, {"type": "text", "text": "4 Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 goal-reducer accelerates standard RL ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Four-room maze navigation task In a modified mini-grid environment [10] (Fig. 3A left), an agent receives two images as inputs and outputs an action indicating which direction to go among four possible directions. One image is the partial observation in a four-room grid world maze, $s_{t}$ , while the other image is a similar \u201cpicture\u201d from the \u201cgoal\u201d location, $g$ . No other visual cue beyond the \u201cpicture\u201d of the goal location is given, preventing the agent from cheating using easy visual landmarks like the green dots used in the original version of this task. In each episode within this environment, $g$ and $s_{0}$ are uniformly sampled from all possible locations. The agent receives a constant negative reward every step until it reaches $g$ . In this experiment, the DQL algorithm is represented as DRL, while goal-reducer augmented DQL is denoted $\\mathbf{D}\\mathbf{R}\\mathbf{L}\\mathbf{+}\\mathbf{G}\\mathbf{R}$ . The results clearly show that $\\mathbf{D}\\mathbf{R}\\mathbf{L}\\mathbf{+}\\mathbf{G}\\mathbf{R}$ outperforms DRL in terms of convergence time (Fig. 3A right). ", "page_idx": 6}, {"type": "text", "text": "Robot arm reach task We adopted panda-gym [19] to implement an environment where a robot arm with 7 degrees of freedom is trained to reach an arbitrary location sampled uniformly in the space (Fig. 3B left). Like the navigation task, the agent receives a constant negative reward every interaction before reaching within a close region centered on the specified goal location. In this experiment, for plain DQL we used SAC (denoted as DRL) and goal-reducer augmented SAC $\\scriptstyle(\\mathbf{D}\\mathbf{R}\\mathbf{L}+\\mathbf{G}\\mathbf{R})$ ). The results (Fig. 3B right) are consistent with the navigation task. ", "page_idx": 6}, {"type": "image", "img_path": "Y0EfJJeb4V/tmp/e12c5c6d3a90a5349836eec852af72adedface1c298b53374c93e4f9f81c4c17.jpg", "img_caption": ["Fig. 3: goal-reducer accelerates standard RL. A: An example input in the four-room navigation task (left) and performance comparison (right). B: Robot arm reach task (left) and performance comparison (right). C: How a goal-reducer agent works with only a local policy (left) and performance comparison of 3 algorithms (right). "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.2 Standalone goal-reducer surpasses goal-reducer-accelerated RL ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The next question we ask is related to the results in recursive goal reduction (Fig. 2C): Can one use just the goal-reducer to perform a task as it seems to reduce Equidex recursively to neighbor goals? This requires a standalone goal-reducer and a \u201clocal\u201d policy $\\pi_{\\mathrm{local}}$ that can learn how to associate $s_{t}$ and $s_{t+1}$ with $a_{t}$ . The Four-room navigation task naturally ftis this need, as a $\\pi_{\\mathrm{local}}$ in it can be easily defined as a policy that learns to associate two connecting grids. Under this setting, a goal-reducer can recursively generate subgoals using existing goals/subgoals until $\\pi_{\\mathrm{local}}$ finds proper subgoals that are close enough to make a proper decision. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "This time a $19\\mathrm{x}19$ maze is used to make the task harder, as goal-reducer\u2019s training effect may not be obvious in smaller environments. In this environment, we dropped the Bellman equation (for details, see appendix) and compared its performance (denoted as GR) with the previous winner, goal-reducer augmented DQL $\\mathbf{G}\\mathbf{R}{+}\\mathbf{D}\\mathbf{R}\\mathbf{L})$ ), and the baseline plain DQL (DRL). Results in Fig. 3C right column clearly show that GR outperforms both $\\mathbf{GR+DRL}$ and DRL, while the latter two\u2019s performance relationship is consistent with the right column in Fig. 3A. ", "page_idx": 7}, {"type": "text", "text": "4.3 goal-reducer in the Brain ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The efficiency the goal-reducer has shown in previous experiments, when compared to plain RLs, naturally leads us to wonder if the brain adopts a similar strategy to solve goal-directed behaviors. To address this, we used a cognitive task, treasure-hunting (Fig. 4A), that necessitates flexible goal representation changes. In the task, subjects were placed in one of four possible starting states on one of two maps (Fig. 4B) and were required to reach states designated as a \u201cchest\u201d (the ultimate goal). But having a \u201ckey\u201d is necessary when reaching the \u201cchest\u201d to obtain a reward. The locations of the \u201ckey\u201d and \u201cchest\u201d are presented to the subject at the start of each episode and change randomly across episodes. The goal-reducer, $\\Phi(s,g)$ , when paired with a $\\pi_{\\mathrm{local}}$ , forms an agent and is also trained on the same task, using the same strategy adopted in Fig. 3C. ", "page_idx": 7}, {"type": "image", "img_path": "Y0EfJJeb4V/tmp/d95f7a2ed1fecc7a20497ab0003458e233cc53af76746ef6402567c1ea03f762.jpg", "img_caption": ["Fig. 4: goal-reducer in the treasure hunting task. A: The treasure hunting task description. B: Two configurations of maps used in the task. C: Population ${\\bf Z}$ -maps of $S,{\\mathcal{G}}.$ , and $\\Phi$ in vmPFC. D: The relative representation z-value distribution for the three centers marked as $S,{\\mathcal{G}}$ , and $\\Phi$ in vmPFC. E: Population z-maps of $\\boldsymbol{S}$ , Int. $\\boldsymbol{S}$ , and $\\Phi$ in bilateral putamen. F: Same as $\\mathbf{D}$ , but for bilateral putamen. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "After training, we analyzed the goal-reducer agent\u2019s neural activation and compared it with human subjects\u2019 brain activity measured via fMRI. The fMRI data, including human subjects\u2019 participation details, were reported previously, and human subjects were compensated $\\mathbb{S}25/\\mathrm{hr}$ for fMRI participation [54]. In particular, subjects were apprised of the mild risks of fMRI including boredom, fatigue, and loss of confidentiality. All research was approved by the institution\u2019s IRB, and all subjects provided full informed consent. Our fMRI analysis used representational similarity analysis (RSA) [32]. RSA considers all conditions that occurred in the task and compares the activity similarity of the model/brain between each pair of conditions via Pearson correlation, thereby forming a symmetric representational dissimilarity matrix (RDM). The entries in the RDM range from 0 to 2 (1 minus the possible correlation ranging from -1 to 1), where a lower value indicates higher similarity. RDMs are calculated for all voxels in human subjects\u2019 brains and for different components in the goal-reducer agent, including the input representation $\\boldsymbol{S}$ , the goal representation $\\mathcal{G}$ , and the goal-reducer, $\\Phi(s,g)$ . ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "The results show that the activity in the ventromedial prefrontal cortex (vmPFC) corresponds to $g\\in{\\mathcal{G}}$ in the goal-reducer agent. Next, we evaluated the activity matching the internal neurons of the goal-reducer $\\Phi(s,g)$ . As depicted in Fig. 4C, there are several regions whose activities correlate with $\\Phi$ , including parts of the vmPFC (indicated by the right circle in Fig. 4C) and the left accumbens. This finding is particularly interesting for two reasons: 1) numerous studies have established that the vmPFC is related to value and goal representations [29, 43]; 2) the close spatial relationship between regions matching $\\mathcal{G}$ and $\\Phi$ (Fig. 4C middle and right columns) reflects the goal-reducer\u2019s organization (Fig. 4C right), wherein $\\Phi$ and $\\mathcal{G}$ are interconnected through the generation and input of different goals. This recurrent connectivity suggests that the widespread recurrent connections in the vmPFC [37] may fulfill the role of goal reduction or perform reverse future simulation [6], thereby simplifying the task. The finding is consistent with a posterior-to-anterior hierarchy of goal planning in the prefrontal cortex [7], and our results may suggest the function of such a hierarchy - namely that anterior regions provide goal reduction functionality for more posterior regions. ", "page_idx": 8}, {"type": "text", "text": "Following this, we next investigated the $Z$ map of state representations and indeed found a region above $\\Phi$ and $\\mathcal{G}$ in the vmPFC that is correlated with $\\boldsymbol{S}$ (as denoted by the top circle in Fig. 4C left). The average ${\\bf Z}$ -values in these regions show differential representational loading on the goal-reducer agent components (Fig. 4D), although the ROI $\\mathbf{Z}$ -value comparison is a descriptive statistic only given that the regions were selected for strong loading on the various model layers [33]. ", "page_idx": 8}, {"type": "text", "text": "Aside from the goal reduction, we also compared $\\boldsymbol{S}$ in the goal-reducer agent with brain data. The RSA results indicate significantly elevated representational loading for these two layers in the bilateral putamen (Fig. 4E), while there is a lower level of loading for the goal representation layer and an even lesser extent for the goal reduction layer. The putamen, a component of the larger basal ganglia system, is recognized for its involvement in habitual behaviors [53] and goal-directed actions [4, 27]. This suggests that such an area should engage in both local policy enactment (the habitual aspect) and goal reduction (goal-directed component). The congruence between the state representation and its intermediate layer with this role may illuminate why activation related to the goal and its reduction is lower in this region (Fig. 4F), since they are not as essential for the habitual component of the local policy. ", "page_idx": 8}, {"type": "text", "text": "5 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Limitations and future work In this work, the trained goal reduction mechanism has shown its capability in terms of its computational advantage and as a biological model of human goal-directed behaviors. This suggests a possible bridge between efficient human problem-solving in multi-goal settings and machine learning. However, a key part of the training of the goal-reducer, the loop removal sampling process, does not have a clear mechanistic biological process correspondence. Two ways exist to address this issue in the future. Computationally, it may be possible to derive a purely neural model to perform the loop removal process, making the sampling process also biologically plausible. A potential solution will be to leverage a lateral-inhibition-like [8] mechanism to inhibit all associative synaptic connections between neurons when the same group of neurons are activated more than once in a time range, simulating canceling the \u201cloop\u201d in a memory trajectory. Empirically, it may be possible to collect brain activity data during training of the same or similar tasks, or during offline replay, which may incorporate the loop removal as part of memory consolidation. ", "page_idx": 8}, {"type": "text", "text": "Conclusion We developed a novel general goal reduction mechanism using the loop removal trick and trained a network goal-reducer that can learn to predict nearly optimal subgoals from distant ultimate goals. We show that this approach does not rely on prior knowledge about the global structure/distance of the environment and uses just random explorations. We further demonstrate that it can be integrated into various existing RL frameworks and outperforms them. Besides, after removing the Bellman equation part, when applied recursively, this framework can perform goaldirected learning and even outperform goal-reducer augmented RL methods. This goal-reducer agent was next applied to a cognitive task and compared with human fMRI data. Our analyses show that various regions in the brain, including the vmPFC and basal ganglia, can be mapped to the goal reduction representation and state representations in the goal-reducer agent, implying that the brain may instantiate a similar computational circuit to perform goal-directed learning. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] A. Abraham, F. Pedregosa, M. Eickenberg, P. Gervais, A. Mueller, J. Kossaif,i A. Gramfort, B. Thirion, and G. Varoquaux. Machine learning for neuroimaging with scikit-learn. Frontiers in neuroinformatics, 8:14, 2014. [2] M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder, B. McGrew, J. Tobin, O. Pieter Abbeel, and W. Zaremba. Hindsight experience replay. Advances in neural information processing systems, 30, 2017.   \n[3] S. Ao, T. Zhou, G. Long, Q. Lu, L. Zhu, and J. Jiang. Co-pilot: Collaborative planning and reinforcement learning on sub-task curriculum. Advances in Neural Information Processing Systems, 34:10444\u201310456, 2021.   \n[4] B. W. Balleine and J. P. O\u2019doherty. Human and rodent homologies in action control: corticostriatal determinants of goal-directed and habitual action. Neuropsychopharmacology, 35(1):48\u201369, 2010.   \n[5] T. E. Behrens, T. H. Muller, J. C. Whittington, S. Mark, A. B. Baram, K. L. Stachenfeld, and Z. Kurth-Nelson. What is a cognitive map? organizing knowledge for flexible behavior. Neuron, 100(2):490\u2013509, 2018.   \n[6] R. G. Benoit, D. J. Davies, and M. C. Anderson. Reducing future fears by suppressing the brain mechanisms underlying episodic simulation. Proceedings of the National Academy of Sciences, 113(52):E8492\u2013E8501, 2016.   \n[7] I. K. Brunec and I. Momennejad. Predictive representations in hippocampal and prefrontal hierarchies. J. Neurosci., 42(2):299\u2013312, Jan. 2022.   \n[8] G. Buzs\u00e1ki. Feed-forward inhibition in the hippocampal formation. Progress in neurobiology, 22(2):131\u2013153, 1984.   \n[9] E. Chane-Sane, C. Schmid, and I. Laptev. Goal-conditioned reinforcement learning with imagined subgoals. ArXiv, abs/2107.00541, 2021.   \n[10] M. Chevalier-Boisvert, B. Dai, M. Towers, R. de Lazcano, L. Willems, S. Lahlou, S. Pal, P. S. Castro, and J. Terry. Minigrid & miniworld: Modular & customizable reinforcement learning environments for goal-oriented tasks. CoRR, abs/2306.13831, 2023.   \n[11] J. Crivelli-Decker, A. Clarke, S. A. Park, D. J. Huffman, E. Boorman, and C. Ranganath. Goal-centered representations in the human hippocampus. bioRxiv, pages 2021\u201308, 2021.   \n[12] E. W. Dijkstra. A note on two problems in connexion with graphs. Numerische Mathematik, 1:269\u2013271, 1959.   \n[13] D. Dupret, J. O\u2019neill, B. Pleydell-Bouverie, and J. Csicsvari. The reorganization and reactivation of hippocampal maps predict spatial memory performance. Nature neuroscience, 13(8):995\u2013 1002, 2010.   \n[14] H. Edelsbrunner and J. L. Harer. Computational topology: an introduction. American Mathematical Society, 2022.   \n[15] R. A. Epstein, E. Z. Patai, J. B. Julian, and H. J. Spiers. The cognitive map in humans: spatial navigation and beyond. Nature neuroscience, 20(11):1504\u20131513, 2017.   \n[16] B. Eysenbach, R. Salakhutdinov, and S. Levine. Search on the replay buffer: Bridging planning and reinforcement learning. In Neural Information Processing Systems, 2019.   \n[17] B. Eysenbach, R. R. Salakhutdinov, and S. Levine. Search on the replay buffer: Bridging planning and reinforcement learning. Advances in Neural Information Processing Systems, 32, 2019.   \n[18] C. Florensa, D. Held, X. Geng, and P. Abbeel. Automatic goal generation for reinforcement learning agents. In International conference on machine learning, pages 1515\u20131528. PMLR, 2018.   \n[19] Q. Gallou\u00e9dec, N. Cazin, E. Dellandr\u00e9a, and L. Chen. panda-gym: Open-source goalconditioned environments for robotic learning. arXiv preprint arXiv:2106.13687, 2021.   \n[20] S. J. Gershman and N. D. Daw. Reinforcement learning and episodic memory in humans and animals: an integrative framework. Annual review of psychology, 68:101\u2013128, 2017.   \n[21] D. Ha and J. Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018.   \n[22] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning, pages 1861\u20131870. PMLR, 2018.   \n[23] D. A. Hamilton, C. S. Rosenfelt, and I. Q. Whishaw. Sequential control of navigation by locale and taxon cues in the morris water task. Behavioural brain research, 154(2):385\u2013397, 2004.   \n[24] P. E. Hart, N. J. Nilsson, and B. Raphael. A formal basis for the heuristic determination of minimum cost paths. IEEE transactions on Systems Science and Cybernetics, 4(2):100\u2013107, 1968.   \n[25] K. Hartikainen, X. Geng, T. Haarnoja, and S. Levine. Dynamical distance learning for semisupervised and unsupervised skill discovery. arXiv preprint arXiv:1907.08225, 2019.   \n[26] C. Hoang, S. Sohn, J. Choi, W. Carvalho, and H. Lee. Successor feature landmarks for longhorizon goal-conditioned reinforcement learning. In Neural Information Processing Systems, 2021.   \n[27] M. Jahanshahi, I. Obeso, J. C. Rothwell, and J. A. Obeso. A fronto\u2013striato\u2013subthalamic\u2013pallidal network for goal-directed and habitual inhibition. Nature Reviews Neuroscience, 16(12):719\u2013 732, 2015.   \n[28] Z. Jin, J. Jin, and W. Liu. Autonomous discovery of subgoals using acyclic state trajectories. In Information Computing and Applications: First International Conference, ICICA 2010, Tangshan, China, October 15-18, 2010. Proceedings 1, pages 49\u201356. Springer, 2010.   \n[29] J. W. Kable and P. W. Glimcher. The neural correlates of subjective value during intertemporal choice. Nature neuroscience, 10(12):1625\u20131633, 2007.   \n[30] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[31] D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.   \n[32] N. Kriegeskorte, M. Mur, and P. A. Bandettini. Representational similarity analysis-connecting the branches of systems neuroscience. Frontiers in systems neuroscience, page 4, 2008.   \n[33] N. Kriegeskorte, W. K. Simmons, P. S. F. Bellgowan, and C. I. Baker. Circular analysis in systems neuroscience: the dangers of double dipping. Nat. Neurosci., 12(5):535\u2013540, May 2009.   \n[34] H. Lai, J. Shen, W. Zhang, and Y. Yu. Bidirectional model-based policy optimization. ArXiv, abs/2007.01995, 2020.   \n[35] M. Liu, M. Zhu, and W. Zhang. Goal-conditioned reinforcement learning: Problems and solutions. arXiv preprint arXiv:2201.08299, 2022.   \n[36] N. Makris, J. M. Goldstein, D. Kennedy, S. M. Hodge, V. S. Caviness, S. V. Faraone, M. T. Tsuang, and L. J. Seidman. Decreased volume of left and total anterior insular lobule in schizophrenia. Schizophrenia research, 83(2-3):155\u2013171, 2006.   \n[37] E. K. Miller. The prefontral cortex and cognitive control. Nature reviews neuroscience, 1(1):59\u2013 65, 2000.   \n[38] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep reinforcement learning. nature, 518(7540):529\u2013533, 2015.   \n[39] H. Nili, C. Wingfield, A. Walther, L. Su, W. Marslen-Wilson, and N. Kriegeskorte. A toolbox for representational similarity analysis. PLoS computational biology, 10(4):e1003553, 2014.   \n[40] N. Nyberg, \u00c9. Duvelle, C. Barry, and H. J. Spiers. Spatial goal coding in the hippocampal formation. Neuron, 110(3):394\u2013422, 2022.   \n[41] S. Paul, J. Vanbaar, and A. Roy-Chowdhury. Learning from trajectories via subgoal discovery. Advances in Neural Information Processing Systems, 32, 2019.   \n[42] W. D. Penny, K. J. Friston, J. T. Ashburner, S. J. Kiebel, and T. E. Nichols. Statistical parametric mapping: the analysis of functional brain images. Elsevier, 2011.   \n[43] M. F. Rushworth, M. P. Noonan, E. D. Boorman, M. E. Walton, and T. E. Behrens. Frontal cortex and reward-guided learning and decision-making. Neuron, 70(6):1054\u20131069, 2011.   \n[44] A. B. Satpute, K. N. Ochsner, and D. Badre. The neuroscience of goal-directed behavior. In Goal-directed behavior, pages 49\u201384. Psychology Press, 2012.   \n[45] P. Shamash, S. F. Olesen, P. Iordanidou, D. Campagner, N. Banerjee, and T. Branco. Mice learn multi-step routes by memorizing subgoal locations. Nature neuroscience, 24(9):1270\u20131279, 2021.   \n[46] H. J. Spiers and S. J. Gilbert. Solving the detour problem in navigation: a model of prefrontal and hippocampal interactions. Frontiers in human neuroscience, 9:125, 2015.   \n[47] K. L. Stachenfeld, M. M. Botvinick, and S. J. Gershman. The hippocampus as a predictive map. Nature neuroscience, 20(11):1643\u20131653, 2017.   \n[48] E. C. Tolman. Cognitive maps in rats and men. Psychological review, 55(4):189, 1948.   \n[49] A. Trott, S. Zheng, C. Xiong, and R. Socher. Keeping your distance: Solving sparse reward tasks using self-balancing shaped rewards. Advances in Neural Information Processing Systems, 32, 2019.   \n[50] A. Verma and B. Mettler. Investigating human learning and decision-making in navigation of unknown environments. IFAC-PapersOnLine, 49(32):113\u2013118, 2016.   \n[51] T. Wei, Y. Wang, and Q. Zhu. Deep reinforcement learning for building hvac control. In Proceedings of the 54th annual design automation conference 2017, pages 1\u20136, 2017.   \n[52] J. Weng, H. Chen, D. Yan, K. You, A. Duburcq, M. Zhang, Y. Su, H. Su, and J. Zhu. Tianshou: A highly modularized deep reinforcement learning library. Journal of Machine Learning Research, 23(267):1\u20136, 2022.   \n[53] H. H. Yin and B. J. Knowlton. The role of the basal ganglia in habit formation. Nature Reviews Neuroscience, 7(6):464\u2013476, 2006.   \n[54] N. Zarr and J. W. Brown. Foundations of human spatial problem solving. Scientific Reports, 13, 2023.   \n[55] L. Zhang, G. Yang, and B. C. Stadie. World model as a graph: Learning latent landmarks for planning. ArXiv, abs/2011.12491, 2020.   \n[56] Y. Zhang, P. Abbeel, and L. Pinto. Automatic curriculum learning through value disagreement. Advances in Neural Information Processing Systems, 33:7648\u20137659, 2020. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "The appendix is divided into four sections. Section A.1 introduces the computational resources used for all experiments. Section A.2 describes the training algorithm used for Loop-removal sampling. Section A.3 details the experiment setup and extra results of how goal-reducer is used to accelerate RL. Section A.4 is similar to Section A.3, but focuses on the standalone goal-reducer experiment. The last section, A.5, includes training and analysis details of the goal-reducer-brain fMRI data comparison experiment. ", "page_idx": 12}, {"type": "text", "text": "A.1 Computation resources ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We evaluated all simulations on a server with an 11 GB NVIDIA 2080Ti GPU and a computational node with a 32 GB Tesla V100 GPU. The fMRI data processing was executed on a server with 128 GB RAM and an Intel(R) Xeon(R) X7560 CPU. ", "page_idx": 12}, {"type": "text", "text": "A.2 Loop-removal sampling ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In this experiment, three types of sampling strategies are used. For Random sampling, we uniformly sample subgoals from the set of all possible state representations. For Trajectory sampling, in a single trajectory, subgoals are uniformly sampled from states that are between the first (start state) and the last state (goal). For Loop-removal sampling, we implemented a flitering algorithm to remove loops from trajectories. Below (Algorithm 1) is the pseudocode we used to implement the Loop-removal sampling in all experiments: ", "page_idx": 12}, {"type": "table", "img_path": "Y0EfJJeb4V/tmp/9707bfba5cfbcef345298676b61c11b923970b87a05ae5b40422fda92e36f7ec.jpg", "table_caption": [], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "Training For the two environments in Fig. 2A, we used the same goal-reducer architecture for all sampling: a VAE with two architecturally identical 3-layer MLPs for encoding and decoding. The Adam optimizer [30] is used for training. For some important hyperparameters, see Table 1. ", "page_idx": 12}, {"type": "table", "img_path": "Y0EfJJeb4V/tmp/05332ff48d7de69f392766d9591c1a3fc2d167df52fe5bc9574e4eee48118bcf.jpg", "table_caption": ["Table 1: Hyperparameters used in A.2 "], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "A.3 goal-reducer accelerates DRL ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Task In the four-room navigation task, we created a new environment based on Minigrid [10]. Specifically, we: ", "page_idx": 12}, {"type": "text", "text": "\u2022 uniformly sampled initial locations and goal locations among all plausible locations.   \n\u2022 changed the action space from a composition of turning angle and moving forward to a plain four directions (up, down, left, right).   \n\u2022 made the environment partially observable to the agent by allowing it to see only a limited squared image of its surroundings (a 13x13 image with the agent in the center).   \n\u2022 made the goal (the green square) invisible to the agent, i.e., the agent will only receive a picture taken as if it is in the goal location as its goal input. ", "page_idx": 13}, {"type": "text", "text": "In the robot arm reach task, we adopted the panda-gym library [19]. Initial joint angles and goal coordinates are also uniformly sampled from plausible value ranges. ", "page_idx": 13}, {"type": "text", "text": "Scalability One reason we used the four-room navigation task to test the capability of goal-reducer is that we can easily adjust the size of the maze. This gives us a sense about how goal-reducer with Loop-removal sampling scales. As shown in Fig. 3A, a four-room environment is composed of two rooms and three walls (left border, middle, right border) if one looks from a single side, so the legal border size takes the form of $3+2k$ , where $k$ is the size of the room. We initially tested sizes 15 and 19, as shown in the main text. Next, we examined other sizes and formed results for 13, 15, 17, 19, and 21. From Fig.S. 1, we can see a trend that $\\mathbf{D}\\mathbf{R}\\mathbf{L}\\mathbf{+}\\mathbf{G}\\mathbf{R}$ outperforms DRL in all cases. Though the gap in a finite step becomes smaller as the sizes increase, given the increased task difficulty, the performance difference is positive and consistent across different conditions. ", "page_idx": 13}, {"type": "image", "img_path": "Y0EfJJeb4V/tmp/5519d7cc39f1516bd6d078652d5cee2bc21d38d4df8a6c28f1fe316d71e2c43a.jpg", "img_caption": ["Fig.S. 1: Performance comparison between DRL ${\\mathbf{+GR}}$ and DRL in the four-room navigation task with different sizes. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "Training For the DRL (DQL) and $\\mathbf{D}\\mathbf{R}\\mathbf{L}\\mathbf{+}\\mathbf{G}\\mathbf{R}$ (DQL+goal-reducer) agent implementation, we used and modified Tianshou [52] to implement the baseline algorithm and the trainer. To handle pixel-like inputs in the four-room navigation task, an extra single CNN (ObsEnc) is used to preprocess the $s_{t}$ and $g$ (this is not used in the robot arm reach task). Following that, an MLP is used as the Q-net in DQL. Two Adam optimizers are used during training. One is used to train the DQL part, while another is turned on only when the goal-reducer is ON. In both DRL and DRL ${\\bf+G R}$ , we applied the same Hindsight Experience Replay [2] buffer to accelerate training. ", "page_idx": 13}, {"type": "text", "text": "For some important hyperparameters used, see Table 2. ", "page_idx": 13}, {"type": "text", "text": "A.4 goal-reducer surpasses DRL $^{+}$ GR training details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Task In this experiment, we compared a standalone local policy, GR, with DRL+GR and DRL in a four-room gridworld navigation task. ", "page_idx": 13}, {"type": "text", "text": "Planning process In the GR case, a standalone goal-reducer and a local policy are used to generate actions for every possible $s_{t},g$ combination. The pseudocode for this process is listed in Algorithm ", "page_idx": 13}, {"type": "table", "img_path": "Y0EfJJeb4V/tmp/81dff6f278ef43a96d9805c5429df7e165ceecb8697e49ade7c81fa6ea8a8a5f.jpg", "table_caption": ["Table 2: Hyperparameters used in A.3 "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "2. Using this algorithm, the goal-reducer agent can navigate in the environment by recursively producing a batch of subgoals over time, until at least one of them is close enough to the current location. The local policy\u2019s entropy plays the role of epistemic uncertainty [56] to help goal-reducer decide whether a goal/subgoal is close enough. An example is shown in Fig.S. 2. ", "page_idx": 14}, {"type": "text", "text": "Algorithm 2 Action generation of goal-reducer with a local policy ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Input: Goal reduction step $t=0$ , max allowed goal reduction steps $K$ , state representation $s_{t}$ ,   \ngoal representation $g$ , parallel goal numbers $M$ , entropy threshold $\\eta$   \n$a_{t}=\\pi_{\\mathrm{local}}(s_{t},g)$   \nif Entropy $(a_{t})<\\eta$ then return $a_{t}$   \nend if   \nInitialize subgoal list $\\mathcal{G M}=[g]\\times M$   \nwhile $t<K$ do for $i=1$ to $M$ do $\\mathcal{G M}[i]=\\Phi\\bigl(s_{t},\\mathcal{G M}[i]\\bigr)$ end for if mi $\\mathrm{n}(\\mathrm{Entropy}(\\mathcal{G}\\mathcal{M}[i]))<\\eta$ then $g^{*}=\\arg\\operatorname*{min}(\\mathrm{Entropy}(\\mathcal{G}\\mathcal{M}))$ $a_{t}=\\pi_{\\mathrm{local}}(s_{t},g^{*})$ return $a_{t}$ end if $t=t+1$   \nend while   \nreturn $a_{t}$ ", "page_idx": 14}, {"type": "image", "img_path": "Y0EfJJeb4V/tmp/42e3d25b0f23bcb2f2b3d6b43d6c1565f0b63e1dfd94bbc18485e9912e2f6709.jpg", "img_caption": ["Fig.S. 2: An example of the goal-reducer planning process. Red dots show the agent\u2019s location, dark green dots (upper left) show the goal, and shadowed green circles show subgoals generated by goal-reducer over time. Darker green indicates more subgoals at the same location. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Training The training and agent settings are the same as above for $\\mathbf{D}\\mathbf{R}\\mathbf{L}\\mathbf{+}\\mathbf{G}\\mathbf{R}$ and DRL. For the GR case, besides the optimizers for the DQL part and the goal-reducer part, we introduced an extra Adam optimizer for the CNN preprocessing part in the pipeline. Without the constraint from a $\\mathrm{\\DeltaQ}$ function to learn effective state representations, an extra self-supervised learning approach must be used. Similar to [21], we introduced a \u201cworld model\u201d (WorldModel) and a decoder (ObsDec) for the agent. Specifically, a decoder and an abstract next state representation are introduced: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h_{t+1}=\\mathsf{W o r1d M o d e1}\\big(\\mathsf{0b s E n c}\\big(s_{t}\\big),a_{t}\\big)}\\\\ &{\\hat{s}_{t+1}=\\mathsf{0b s D e c}\\big(h_{t+1}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\hat{s}_{t+1}$ is the prediction of $s_{t+1}$ . To train this model, BCE loss between $\\hat{s}_{t+1}$ and $s_{t+1}$ is used, paired with a weighted (0.001) L2-norm penalty of $h_{t+1}$ to avoid exploding gradients. For some important hyperparameters used, see Table 3. ", "page_idx": 15}, {"type": "table", "img_path": "Y0EfJJeb4V/tmp/781dbab964853ea9032d77010c24493139b06b64f19bff0e0ceb90387127d50f.jpg", "table_caption": ["Table 3: Hyperparameters used in A.4 "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "A.5 goal-reducer fMRI data analysis ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Task For human subjects, the treasure hunting task is presented with a series of images and verbal instructions on the screen (see Fig. 4A). To train the GR agent to perform this task, we turned all presentations into verbal instructions (see Fig.S. 3) and then encoded them with OpenAI\u2019s ada-002 text embedding model to transform them into vectors. ", "page_idx": 15}, {"type": "image", "img_path": "Y0EfJJeb4V/tmp/4391902d8593cf35899e9936fe9770bd1a8dadd36ec96c3e2f881fb1b2361d42.jpg", "img_caption": ["Fig.S. 3: An example of text input in the treasure hunting task for the goal-reducer agent "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "This unifies the representation of this task with all previous tasks and allows future generalization to novel tasks without modifying the agent itself. ", "page_idx": 15}, {"type": "text", "text": "Training The network architecture is the same as the GR agent used in previous experiments. For some important hyperparameters used, see Table 4. ", "page_idx": 15}, {"type": "table", "img_path": "Y0EfJJeb4V/tmp/0a2757a23d806af1acc971019d280e1a8f4c090cf7e98c48ea17d618cd110200.jpg", "table_caption": ["Table 4: Hyperparameters used in A.5 "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "fMRI data analysis We included 24 human subjects\u2019 fMRI data for analysis.2 All research involving human subjects was approved by the institution\u2019s IRB. Subjects provided informed consent, and the procedures adhered to applicable guidelines and the Declaration of Helsinki. The data were re-used from [54], which also provides the full methods. ", "page_idx": 15}, {"type": "text", "text": "In the RDM analysis, each RDM for a brain voxel or model component is a $64\\times64$ matrix, where $64=4\\times8\\times2$ . Here, 4 represents the four possible current subject/model locations on the map. 8 represents all possible episode configurations (comprising four two-step key-chest configurations when the subject/model is at the starting location, two one-step \u201ckey-chest\u201d configurations when the subject/model is at the starting location, and two two-step key-chest configurations when the subject/model is at the middle location). The factor 2 accounts for the two phases of each movement (planning and action phase). ", "page_idx": 16}, {"type": "text", "text": "For the human fMRI data, such RDMs are calculated for each voxel in the brain for each subject, with a neighborhood radius set to $10\\;\\mathrm{mm}$ . For the model, RDMs are first calculated for learned $\\boldsymbol{S}$ and $\\mathcal{G}$ representations and the intermediate representations in the corresponding neural networks. Moreover, we calculated RDMs for the goal-reducer\u2019s hidden representations as well when the goal reduction is performed. The action layer, i.e., the output distribution at each step is also calculated. Together, these form the following model RDMs: $\\boldsymbol{S}$ RDM for state representations, Int. $\\boldsymbol{S}$ RDM for state representations in hidden layers, $\\mathcal{G}$ RDM for goal representations, Int. $\\mathcal{G}$ RDM for goal representations in hidden layers, $\\pi$ RDM for action representation, and $\\Phi$ RDM for goal-reducer\u2019s hidden layers. ", "page_idx": 16}, {"type": "text", "text": "Once RDMs are calculated, Pearson correlation between each voxel and a model RDM\u2019s upper triangular part, given their symmetrical nature, is calculated. This forms a series of fMRI RSA maps for each subject, showing whether certain brain regions are positively correlated with the aforementioned model components. Second-level analysis is then performed to extract the population effect. To do so, we first transform the individual correlations into z-scores using the Fisher transform. Then, all z-score maps are projected into a shared space, and a 1-sample T-test is performed for each voxel in the shared brain space to examine whether the corresponding correlation is significant across subjects. After that, cluster-based correction is performed to fliter out statistically significant regions. We used nilearn [1], rsatoolbox [39]\u2019s Python version, and SPM5 [42] to perform this process. ", "page_idx": 16}, {"type": "text", "text": "With the above analysis pipeline, we found that different brain regions have a high match with distinct components in the goal-reducer agent. For a complete list of all clusters, see Table 5. ", "page_idx": 16}, {"type": "table", "img_path": "Y0EfJJeb4V/tmp/3a3d527cb5515b27dd65992ef192f321332ad9dd11074aa59def5d20121c80a7.jpg", "table_caption": ["Table 5: Significant similarity clusters derived from SPM5 [42] for RSA analysis. Anatomical region labels are derived from [36]. The $p_{c}$ corrected and cluster size $(\\mathrm{cluster}_{k_{E}}$ ) values are both corrected using SPM5. Note that some layers may involve more than one region due to the probabilistic nature of atlases in [36]. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We developed a goal reduction model and test it in both simulations and cognitive tasks and compared it with brain fMRI data. All experiments results match the clasim we made in the abstract and intro. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: Limitations of the work are discussed in the last part, the discussion section, of the main text. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: This paper is mostly about empirical observations and has no theorems. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Code is released along the manuscript. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: For the simulation part, people can reproduce the results with provided code and without any extra data. For the brain imaging analysis part, data will be available upon reasonable request. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Experiment settings are revealed in the appendix. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: For all experiments we\u2019ve included errorbar (STE). But notice that in Fig. 2 there seem to be no error bars, because the STE is too small. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We\u2019ve disclosed resources in A.1. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Research in this paper conform the NeurIPS Code of Ethics. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: This paper\u2019s central results are not about social impact. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: We have no such risks. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We\u2019ve credited 3rd party code bases we used in the research in the appendix. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 22}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: We have no new assets in this paper. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The human subjects data were reused from a previous paper, which is cited in the text and reports all the details of the human subjects participation. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The human subjects data were reused from a previous paper, which is cited in the text and reports all the details of the human subjects participation. In particular, we also enumerate in the main text the risks, IRB approval, and full informed consent obtained from all subjects. ", "page_idx": 23}]