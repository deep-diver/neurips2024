{"references": [{"fullname_first_author": "Abbasi-Yadkori", "paper_title": "Regret bounds for the adaptive control of linear quadratic systems", "publication_date": "2011", "reason": "This paper is foundational for the theoretical analysis of nonepisodic RL in linear systems, providing a basis for extending the theoretical analysis to nonlinear systems."}, {"fullname_first_author": "Kakade", "paper_title": "Information theoretic regret bounds for online nonlinear control", "publication_date": "2020", "reason": "This paper establishes state-of-the-art regret bounds for episodic nonlinear control, serving as a crucial benchmark and inspiration for the nonepisodic setting."}, {"fullname_first_author": "Curi", "paper_title": "Efficient model-based reinforcement learning through optimistic policy search and planning", "publication_date": "2020", "reason": "This paper introduces the principle of optimism in the face of uncertainty in model-based RL, a core concept used and extended by NEORL."}, {"fullname_first_author": "Chowdhury", "paper_title": "On kernelized multi-armed bandits", "publication_date": "2017", "reason": "This paper provides crucial calibration results for Gaussian processes, which are used in NEORL for well-calibrated uncertainty estimates."}, {"fullname_first_author": "Srinivas", "paper_title": "Information-theoretic regret bounds for gaussian process optimization in the bandit setting", "publication_date": "2012", "reason": "This paper introduces information gain as a measure of complexity in Gaussian process bandit optimization which is leveraged for the regret bound analysis in NEORL."}]}