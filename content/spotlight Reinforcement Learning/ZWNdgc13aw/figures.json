[{"figure_path": "ZWNdgc13aw/figures/figures_6_1.jpg", "caption": "Figure 1: Average reward A(\u03c0) and cumulative regret RT over ten different seeds for all environments. We report the mean performance with one standard error as shaded regions. During all experiments, the environment is never reset. For all baselines, we model the dynamics with probabilistic ensembles, except in the Pendulum-GP experiment, where GPs are used instead. NEORL significantly outperforms all baselines and converges to the optimal average reward, A(\u03c0*) = 0, showing sublinear cumulative regret RT for all environments.", "description": "This figure compares the performance of NEORL against other baselines across several reinforcement learning environments.  The key takeaway is that NEORL consistently achieves the optimal average reward while exhibiting sublinear cumulative regret, meaning its performance improves more efficiently over time compared to the baselines. The experiments highlight NEORL's ability to learn effectively in a nonepisodic setting (without resets), a significant advantage in real-world applications where resets are often impractical.", "section": "4 Experiments"}, {"figure_path": "ZWNdgc13aw/figures/figures_8_1.jpg", "caption": "Figure 1: Average reward A(\u03c0) and cumulative regret RT over ten different seeds for all environments. We report the mean performance with one standard error as shaded regions. During all experiments, the environment is never reset. For all baselines, we model the dynamics with probabilistic ensembles, except in the Pendulum-GP experiment, where GPs are used instead. NEORL significantly outperforms all baselines and converges to the optimal average reward, A(\u03c0*) = 0, showing sublinear cumulative regret RT for all environments.", "description": "This figure compares the performance of NEORL against several baselines across multiple reinforcement learning environments.  The key takeaway is that NEORL consistently achieves the optimal average reward and demonstrates sublinear cumulative regret, outperforming other methods. The experiment is notable because the environment is never reset, showcasing NEORL's ability to learn and adapt from a single, continuous trajectory.", "section": "Experiments"}]