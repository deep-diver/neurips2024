[{"figure_path": "QFUsZvw9mx/figures/figures_2_1.jpg", "caption": "Figure 1: Context shift of COMRL in Ant-Dir. Left: Given a task M\u00b2 specified by a goal direction (dashed line), the RL agent is trained on data generated by a variety of behavior policies trained on the same task M\u00b2 (red). At test time, however, the context might be collected by behavior policies trained on different tasks {M} (blue), causing a context shift of OOD behavior policies (Section 3.3). Middle: Against OOD context, UNICORN (red) is more robust than baselines such as FOCAL (green) in terms of navigating the Ant robot towards the right direction. Right: Besides behavior policy, the task distribution (e.g., goal positions in Ant) can induce significant context shift (Section 4.2), which is also a challenging scenario for COMRL models to generalize.", "description": "This figure demonstrates the concept of context shift in context-based offline meta-reinforcement learning (COMRL). The left panel shows how training data (red) from a single task is used to train the RL agent, but at test time, the context might be from different tasks (blue), leading to a context shift. The middle panel shows UNICORN's superior robustness to this context shift compared to FOCAL, as evidenced by successful navigation towards the goal. Finally, the right panel illustrates how task distribution variation (e.g., different goal positions) can also induce significant context shift, highlighting the generalization challenge for COMRL models.", "section": "2 Method"}, {"figure_path": "QFUsZvw9mx/figures/figures_3_1.jpg", "caption": "Figure 2: Graphical Models of COMRL.", "description": "This figure presents two graphical models illustrating the causal relationships between variables in context-based offline meta-reinforcement learning (COMRL). The top model shows a simple Markov chain where the task variable (M) influences the context (X), which in turn influences the latent representation (Z).  The bottom model provides a more nuanced perspective, breaking down the context (X) into two components: behavior-related (X<sub>b</sub>) and task-related (X<sub>t</sub>).  The dashed lines indicate a weaker or indirect causal link, while solid lines represent a stronger, more direct relationship. This model highlights the distinctions between spurious correlations (X<sub>b</sub> to Z) and causal relationships (X<sub>t</sub> to Z) when learning task representations.", "section": "2.2 A Unified Information Theoretic Framework"}, {"figure_path": "QFUsZvw9mx/figures/figures_5_1.jpg", "caption": "Figure 3: Meta-learning procedure of UNICORN-SS. The supervised variant UNICORN-SUP simply replaces the decoder by a classifier p\u03b8(M|z) and optimize a cross-entropy loss instead of Lrecon and LFOCAL.", "description": "This figure shows the meta-learning procedure of the self-supervised variant of UNICORN (UNICORN-SS). It is composed of several components: a context encoder that processes the context information C from a replay buffer, a decoder that reconstructs the task-related component Xt of the context X, and a policy network \u03c0\u03b8(s,z) that outputs actions conditioned on the state s and latent representation z. The training process involves minimizing the reconstruction loss Lrecon, the contrastive loss LFOCAL, and the actor-critic losses Lactor and Lcritic. The supervised variant UNICORN-SUP replaces the decoder with a classifier to directly predict the task label M from the latent representation z, optimizing a cross-entropy loss instead.", "section": "2.3 Instantiations of UNICORN"}, {"figure_path": "QFUsZvw9mx/figures/figures_7_1.jpg", "caption": "Figure 1: Context shift of COMRL in Ant-Dir. Left: Given a task M\u00b2 specified by a goal direction (dashed line), the RL agent is trained on data generated by a variety of behavior policies trained on the same task M\u00b2 (red). At test time, however, the context might be collected by behavior policies trained on different tasks {M} (blue), causing a context shift of OOD behavior policies (Section 3.3). Middle: Against OOD context, UNICORN (red) is more robust than baselines such as FOCAL (green) in terms of navigating the Ant robot towards the right direction. Right: Besides behavior policy, the task distribution (e.g., goal positions in Ant) can induce significant context shift (Section 4.2), which is also a challenging scenario for COMRL models to generalize.", "description": "This figure demonstrates the concept of context shift in offline meta-reinforcement learning (OMRL).  The left panel shows how training data might be collected with policies specific to one task, while testing occurs in a shifted context using policies from different tasks. The middle panel highlights UNICORN's superior robustness to this context shift, contrasted with FOCAL.  The right panel illustrates that variations in task distributions (e.g., the goal locations in the Ant environment) can also cause substantial context shifts, challenging the generalization of OMRL models.  The figure visually represents the challenge of COMRL in handling unexpected context shifts during testing and showcases the improved performance of UNICORN.", "section": "2 Method"}, {"figure_path": "QFUsZvw9mx/figures/figures_9_1.jpg", "caption": "Figure 5: Testing returns for OOD tasks. The learning curve is averaged by 6 random seeds.", "description": "This figure displays the average testing return for out-of-distribution (OOD) tasks across different algorithms over 200k training steps.  The performance of various offline meta-reinforcement learning algorithms, including UNICORN-SUP, UNICORN-SS with and without a model-based component, Supervised, Prompt-DT, MACAW, FOCAL, CORRO, and CSRO are compared.  The results show the average return across multiple trials (averaged over 6 random seeds), highlighting the algorithms' generalization capability in handling contexts from different behavior policies.", "section": "3 Experiments"}, {"figure_path": "QFUsZvw9mx/figures/figures_17_1.jpg", "caption": "Figure 6: The 2D projection of the learned task representation space in Ant-Dir. Points are uniformly sampled from out-of-distribution data. Tasks of given goals from 0 to 6 are mapped to rainbow colors, ranging from purple to red.", "description": "This figure shows the 2D projection of task representations learned by three different algorithms: UNICORN-0, UNICORN, and FOCAL.  Each point represents a task, colored according to its goal direction (0 to 6, purple to red).  The algorithms' ability to cluster tasks based on similarity is visualized. UNICORN-0 shows some clustering but less distinct separation than UNICORN. FOCAL shows distinct clusters, but less smooth transition between clusters than UNICORN.", "section": "C.2 Visualization of Task Embeddings"}, {"figure_path": "QFUsZvw9mx/figures/figures_17_2.jpg", "caption": "Figure 4: Testing returns of UNICORN against baselines on six benchmarks. Solid curves refer to the mean performance of trials over 6 random seeds, and the shaded areas characterize the standard deviation of these trials.", "description": "This figure compares the performance of UNICORN against other baselines (FOCAL, CORRO, CSRO, Supervised, MACAW, Prompt-DT) across six different MuJoCo and Metaworld benchmark tasks.  The y-axis represents the average return of the RL agent, and the x-axis represents the number of training steps.  Solid lines show the average performance over six trials for each algorithm on each task, and the shaded region represents the standard deviation across those trials, illustrating the variability in performance.  This allows for a direct comparison of the algorithms' learning curves and their final performance.", "section": "3.2 Few-Shot Generalization to In-Distribution Data"}, {"figure_path": "QFUsZvw9mx/figures/figures_18_1.jpg", "caption": "Figure 8: Different hyper-parameter settings of 1-\u03b1 \u03b1 on Ant-Dir. The learning curve is averaged by 6 random seeds.", "description": "This figure shows the ablation study on the effect of the hyperparameter \u03b1 on the performance of the UNICORN-SS algorithm.  It shows that as \u03b1 increases, performance generally improves, but excessively high values lead to decreased performance.  This is consistent with the theoretical analysis presented in the paper which demonstrates a tradeoff between maximizing causal correlations and minimizing spurious correlations. The plot shows the mean and shaded area representing standard deviation across 6 random seeds.", "section": "C.4 Ablation Study"}]