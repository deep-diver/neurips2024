[{"type": "text", "text": "Optimizing Automatic Differentiation with Deep Reinforcement Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jamie Lohoff Emre Neftci Peter Gr\u00fcnberg Institute Peter Gr\u00fcnberg Institute Forschungszentrum J\u00fclich & RWTH Aachen Forschungszentrum J\u00fclich & RWTH Aachen ja.lohoff@fz-juelich.de e.neftci@fz-juelich.de ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Computing Jacobians with automatic differentiation is ubiquitous in many scientific domains such as machine learning, computational fluid dynamics, robotics, and finance. Even small savings in the number of computations or memory usage in Jacobian computations can already incur massive savings in energy consumption and runtime. While there exist many methods that allow for such savings, they generally trade computational efficiency for approximations of the exact Jacobian. In this paper, we present a novel method to optimize the number of necessary multiplications for Jacobian computation by leveraging deep reinforcement learning (RL) and a concept called cross-country elimination while still computing the exact Jacobian. Cross-country elimination is a framework for automatic differentiation that phrases Jacobian accumulation as ordered elimination of all vertices on the computational graph where every elimination incurs a certain computational cost. We formulate the search for the optimal elimination order that minimizes the number of necessary multiplications as a single player game which is played by an RL agent. We demonstrate that this method achieves up to $33\\%$ improvements over state-of-the-art methods on several relevant tasks taken from diverse domains. Furthermore, we show that these theoretical gains translate into actual runtime improvements by providing a cross-country elimination interpreter in JAX that can efficiently execute the obtained elimination orders. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Automatic Differentiation (AD) is widely utilized for computing gradients and Jacobians across diverse domains including machine learning (ML), computational fluid dynamics (CFD), robotics, differential rendering, and finance [Baydin et al., 2018, Margossian, 2018, Forth et al., 2004a, Tadjouddine et al., 2002b, Giftthaler et al., 2017, Kato et al., 2020, Schmidt et al., 2022, Capriotti and Giles, 2011, Savine and Andreasen, 2021]. To many researchers in the machine learning community, AD is synonymous with the backpropagation algorithm [Linnainmaa, 1976, Schmidhuber, 2014]. However, backpropagation is just one particular way of algorithmically computing the Jacobian that is very efficient in terms of computations for \u201cfunnel-like\u201d functions, i.e. with many inputs and a single scalar output such as in neural networks. In many other domains, we may find functions that do not have this particular property and thus backpropagation might not be optimal for computing the respective Jacobian [Albrecht et al., 2003, Capriotti and Giles, 2011, Naumann, 2020]. In fact, there exists a wide variety of AD algorithms, each of them coming with its own advantages and drawbacks regarding computational cost and memory consumption depending on the function they are applied to. Many of these AD algorithms can be viewed as special cases of cross-country elimination [Griewank and Walther, 2008]. Cross-County Elimination frames AD as an ordered vertex elimination problem on the computational graph with the goal of reducing the required number of multiplications and additions. However, finding the optimal elimination procedure is a NP-complete problem [Naumann, ", "page_idx": 0}, {"type": "image", "img_path": "hVmi98a0ki/tmp/dcd840288f52773835c3a68aedc597af78ec1fd11d3951cf3b3610c00fc4b3e8.jpg", "img_caption": ["Figure 1: Summary of the AlphaGrad pipeline. We trained a neural network to produce new Automatic Differentiation (AD) algorithms using Deep RL that can be used in JAX. The resulting algorithms significantly outperform the current state of the art. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "2008]. Inspired by recent advances in finding optimal matrix-multiplication and sorting algorithms [Fawzi et al., 2022, Mankowitz et al., 2023], we demonstrate that deep RL successfully finds efficient elimination orders which translate into new automatic differentiation algorithms and practical runtime gains (figure 1). Cross-country elimination is particularly amenable for automatization since it provably yields the exact Jacobian for every elimination order. The solution we seek thus reduces to only finding an optimal elimination order, without the need to evaluate the quality of Jacobian approximations (such as in Neural Architecture Search). An important body of prior work aimed to find more efficient elimination techniques through heuristics, simulated annealing or dynamic programming and minimizing related quantities such as fill-in [Naumann, 1999, 2020]. However, none of these works was successful in optimizing with respect to relevant quantities such as number of multiplications or memory consumption. We set up our optimization problem by formulating crosscountry elimination as a single player RL game called VertexGame. At each step of VertexGame, the agent selects a vertex to eliminate from the computational graph according to a certain scheme called vertex elimination. The reward is equal to the negative of the number of multiplications incurred by the particular choice of vertex. VertexGame is played by an AlphaZero-based agent [Silver et al., 2017, Schrittwieser et al., 2019, Danihelka et al., 2022] with policy and value functions modeled with a transformer architecture that processes the graph representation and predicts the next vertex to eliminate, thereby incrementally building the AD algorithm. ", "page_idx": 1}, {"type": "text", "text": "Our approach discovers from scratch new vertex elimination orders, i.e. new AD algorithms that are tailored to specific functions and improve over the established methods such as minimal Markowitz degree. We further demonstrate the efficacy of the discovered algorithms on real world tasks by including Graphax, a novel sparse AD package which builds on JAX [Bradbury et al., 2018] and enables the user to differentiate Python code with cross-country elimination. Our main contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We demonstrate that optimizing elimination order can be phrased as a reinforcement learning game by leveraging the graph view of AD,   \n\u2022 We show that a deep RL agent finds new, tailored AD algorithms that improve the state-ofthe-art on several relevant tasks,   \n\u2022 We investigate how the discovered novel elimination procedures translate into actual runtime improvements by implementing Graphax, a cross-country elimination interpreter in JAX allowing the efficient execution of newly found elimination orders. ", "page_idx": 1}, {"type": "text", "text": "1.1 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "RL for Algorithm Research: AlphaTensor and AlphaDev successfully demonstrated that modelbased deep RL finds new and improved matrix-multiplication and sorting algorithms [Fawzi et al., 2022, Mankowitz et al., 2023]. In particular, AlphaTensor used an extension of the AlphaZero agent to search for new matrix-multiplications algorithms that require fewer multiplication operations by directly using this quantity as a reward. The key insight is that different matrix-multiplication algorithms have a common, simple representation through the three-dimensional matrix-multiplication tensor which can be manipulated by taking different actions, resulting in algorithms of varying efficiency. Feeding this tensor into the RL agent, they successfully improved on matrix-multiplication algorithms for $4\\mathrm{x}4$ matrices by beating Strassen\u2019s algorithm, the current state-of-the-art, with an improvement from 49 to 47 multiplications. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "In a similar vein, AlphaDev improved simple sorting algorithms by representing the sorting algorithm as a series of CPU instructions which then have to be arranged in the correct way to achieve the correct sorting output. Instead of using the number of CPU operations as an optimization target, the agent was trained on actual execution times. Our work follows in these footsteps by tackling the difficult problem of finding new and improved AD algorithms for arbitrary functions and hence we termed our method AlphaGrad. ", "page_idx": 2}, {"type": "text", "text": "RL for Compiler Optimization: A number of works have tackled the complex issue of optimizing the compilation of various computational graphs with deep RL. Knossos [Jinnai et al., 2019] leverages the $\\mathbf{A}^{*}$ algorithm to optimize the compilation of simple neural networks. It employs a model to estimate computational cost and utilizes expression rewriting techniques to enhance performance. While Knossos is hardware agnostic, it needs to be trained from scratch for every new computational graph. GO and REGAL both improve on this shortcoming and generalize to new, unseen graphs at the cost of losing the hardware-agnostic property[Paliwal et al., 2019, Zhou et al., 2020]. ", "page_idx": 2}, {"type": "text", "text": "REGAL learns a graph neural network-based policy using a REINFORCE-based genetic algorithm to optimize the scheduling of the individual operations of a graph to the set of available devices, thereby successfully reducing peak memory usage for different deep learning workloads. Only GO is directly trained on actual wall time and handles all relevant optimizations jointly, including device placement, operation fusion, and operation scheduling. GO learns a policy based on graph neural networks and recurrent attention using PPO and successfully demonstrates improvements over Tensorflow\u2019s default compilation strategy. While our work also makes use of the computational graph, the goal is to find novel AD algorithms instead of optimizing compilation itself, although these problems are related since the new AD algorithm is compiled before execution. ", "page_idx": 2}, {"type": "text", "text": "Optimization of AD While no prior work directly aims at improving AD with deep RL, several studies aimed at enhancing AD by other methods. This includes Enzyme [Moses and Churavy, 2020], which presents a reverse-mode AD package that operates on the intermediate representation level using LLVM. Enzyme is thus distinct from other AD packages because it can synthesize gradients for many different high-level languages. Another related work is LAGrad [Peng and Dubach, 2023], a source-to-source AD package written in Julia which introduces a set of new static optimizations to accelerate reverse-mode AD. It leverages high-level MLIR information, such as sparsity structure and control flow semantics of the computational graph to produce more efficient differentiated code. While LAGrad can improve the performance of AD workloads by orders of magnitude, it is currently limited to the use of reverse-mode AD which can be suboptimal for certain tasks. In both works, a suboptimal choice of algorithm might nullify the beneftis gained from careful engineering. Our work aims to close this gap by additionally providing a novel way of finding the optimal AD algorithm using Deep RL. ", "page_idx": 2}, {"type": "text", "text": "The closest related work is [Naumann, 1999] where simulated annealing was applied to reduce the number of multiplications necessary for Jacobian accumulation. The algorithms struggled to significantly outperform state-of-the-art even when it was initialized with a reasonably good elimination order. In a similar manner, [Naumann, 2020] directly optimized the elimination order with dynamic programming albeit with respect to a different optimization target called fill-in on randomly generated graphs that do not necessary represent well-defined, executable functions.t Our work directly optimizes for the number of multiplications required to accumulate the Jacobian on real-world problems. Another approach described in [Chen et al., 2012] utilized integer linear programming to find optimal elimination orders with respect to number of multiplications, but only dealt with very small problems with up to twenty intermediate vertices. Our approach successfully finds new AD algorithms from scratch for complex problems with hundreds of intermediate vertices. ", "page_idx": 2}, {"type": "text", "text": "2 Automatic Differentiation and Cross-Country Elimination ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "AD is a systematic approach to computing the derivatives of dependent variables $\\mathbf{y}=f(\\mathbf{x})\\in\\mathbb{R}^{m}$ with respect to the independent variables $\\mathbf{x}\\in\\mathbb{R}^{n}$ utilizing the chain rule. AD enables the precise and efficient calculation of gradients, Jacobians, Hessians, and higher-order derivatives [Linnainmaa, 1976]. Unlike methods that rely on finite differences or symbolic differentiation, AD offers a systematic way to compute derivatives up to machine precision, making it an indispensable tool in ", "page_idx": 2}, {"type": "image", "img_path": "hVmi98a0ki/tmp/0f45b03d8f043897147a22c0731938d83df814055a4f996822d270061b783537.jpg", "img_caption": ["(c) Elimination of vertex 2. "], "img_footnote": [], "page_idx": 3}, {"type": "image", "img_path": "hVmi98a0ki/tmp/5298d044157ea89696bd39994b3bb676497ee4e95bf0434b62dbe5d9e7942c1c.jpg", "img_caption": ["(d) Graph after eliminating vertex 1. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: Step-by-step description of cross-country elimination with the simple example function $f({\\bar{x}}_{1},x_{2})=({\\bar{\\log}}\\sin({\\bar{x}}_{1}x_{2}),{\\bar{x_{1}}}x_{2}-\\sin(x_{1}x_{2}))^{\\top}.$ . (a) Initial computational graph. (b) The partial derivatives are added to the edges of the computational graph. The intermediate variables $v_{1}$ and $v_{2}$ are defined through $v_{1}=x_{1}x_{2}$ and $v_{2}=\\sin v_{1}$ . (c) Elimination of vertex 2 associated with the sin operation. The dotted red lines represent the edges that are deleted. (d) Final bipartite graph after both intermediate vertices have been eliminated. All remaining edges contain entries of the Jacobian. ", "page_idx": 3}, {"type": "text", "text": "many numerical scientific problems and machine learning [Baydin et al., 2018, Griewank and Walther, 2008]. AD leverages the fact that most computer programs can be broken down into a sequence of simple elemental operations, for example additions, multiplications and trigonometric functions. Partial derivatives of these elemental operations are coded into the AD software and the Jacobian is accumulated by recursively applying the chain rule to the evaluation procedure. Since the partial derivatives are known up to machine precision, AD gives the Jacobian up to machine precision. ", "page_idx": 3}, {"type": "text", "text": "2.1 Graph View and Vertex Elimination ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We take the graph view of AD where a function is defined through its computational graph $\\mathcal{G}=(V,E)$ with its vertices $\\mathcal{V}$ being the elemental operations $\\phi_{j}$ and directed edges $\\mathcal{E}$ that describe the data dependencies between the operations (figure 2a). The relation $i\\prec j$ states that vertex $i$ has an edge connecting it with vertex $j$ , meaning that the output $v_{i}$ of $\\phi_{i}$ is an input of $\\phi_{j}$ . The partial derivatives of the elemental operations with respect to their dependents are assigned to the connecting edges (figure 2b). We can then identify the edges of the graph with their respective partial derivatives $\\begin{array}{r}{c_{j i}=\\frac{\\partial\\phi_{j}}{\\partial v_{i}}}\\end{array}$ . The cross-country elimination algorithm computes the Jacobian by a procedure called vertex elimination. ", "page_idx": 3}, {"type": "text", "text": "Definition 1 [Griewank and Walther, 2008] For a computational graph $\\mathcal{G}=(V,E)$ with partial derivatives $c_{i j}$ , vertex elimination of vertex $j$ is defined as the update ", "page_idx": 3}, {"type": "equation", "text": "$$\nc_{k i}\\,+=c_{k j}c_{j i}\\,\\forall(i,k)\\in\\mathcal{V}\\times\\mathcal{V}\\,\\mathrm{where}\\;i\\prec j\\;\\mathrm{and}\\;j\\prec k\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and then setting $c_{j i}=c_{k j}=0$ for all involved vertices. The $\\mathrel{+{=}}$ operator creates a new edge if there is no edge $c_{k i}$ and otherwise adds the new value to the existing value. ", "page_idx": 3}, {"type": "text", "text": "Intuitively, vertex elimination can be understood as the local application of the chain rule to a single vertex in the graph since the multiplication $c_{i j}c_{j k}$ is exactly the result of applying the chain rule to $(\\phi_{k}\\circ\\phi_{j})(v_{i})$ . If a vertex has multiple incoming and outgoing edges, all combinations of incoming and outgoing edges are resolved to create new edges. If an edge already exists, we add the result of the product to it, in accordance with the rules for total derivatives. After the new edges are added to the graph, we delete all edges connected to the eliminated vertex since all the derivative information is now contained in the new edges (figure $_{2\\mathrm{c}}$ ). Note that the new graph resulting from a vertex elimination no longer directly represents the data dependencies of the function since the eliminated vertex is now disconnected. Furthermore, the application of vertex elimination as described above requires the computational graph to be static and precludes the use of control flow $\\stackrel{.}{.}$ -statements, for-loops) in the differentiated code. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "2.2 Cross Country and Elimination Orders ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The repeated application of the vertex elimination procedure to a computational graph (i.e. cross country elimination) until all intermediate vertices are eliminated will yield a graph where the input vertices and output vertices are directly connected by edges (no intermediate vertices left, see figure 2d). This is called a bipartite graph and the edges of this graph contain the components of the Jacobian of the function $f$ . In particular, as long as all intermediate vertices are eliminated, this Jacobian will always be exact up to machine precision [Griewank and Walther, 2008]. There is no restriction on the order in which the vertices are eliminated, but the choice will significantly influence computational cost and memory [Tadjouddine et al., 2006]. In the graph view, computational cost is straightforward to measure since every vertex elimination incurs a known number of multiplications that depends on the shapes of the elemental Jacobians which can be used as a proxy for execution time. We can ignore the cost of evaluating the partial derivatives since they have to be performed regardless of the elimination order. Thus, we use the number of multiplications as the optimization target for the remainder of this work. The two most common choices for elimination orders are to either eliminate the vertices in the forward or reverse order. These two modes are called forward-mode AD and reverse-mode AD (backpropagation), respectively. ", "page_idx": 4}, {"type": "text", "text": "Forward-mode AD, where vertices are eliminated in the same order as the computational graph is traversed, is particularly efficient for functions where the number of input variables $n$ is much smaller than the number of output variables $m$ , i.e. $n\\ll m$ . In contrast, reverse-mode AD traverses the graph in the opposite direction and is particularly suited for the cases where $n\\gg m$ . This is the case in machine learning and neural networks using scalar loss functions, which is why reverse-mode AD is the default choice in such workloads. ", "page_idx": 4}, {"type": "text", "text": "2.3 Minimal Markowitz Degree ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "A more advanced technique is to eliminate vertices with the lowest Markowitz degree first Griewank and Walther [2008]. The Markowitz degree of a vertex is defined as the number of incoming vertices times the number of outgoing vertices, i.e. $\\mathrm{Mark}(j)\\,=\\,|i\\,\\prec\\,j||j\\,\\prec\\,k|$ , where $|\\cdot|$ denotes the cardinality of the sets $i\\prec j$ and $j\\prec k$ for fixed $j$ . Thus the elimination order is constrained by finding the vertex with the lowest Markowitz degree first, eliminating it and then finding the next vertex with minimal Markowitz degree on the resulting graph. This elimination scheme is one of the best known heuristics for finding efficient elimination orders and can incur savings of up to $20\\%$ over forward- and reverse-mode AD [Albrecht et al., 2003, Griewank and Walther, 2008]. However, for computational graphs that have many inputs and few outputs, it is often outperformed by reverse-mode AD. ", "page_idx": 4}, {"type": "text", "text": "2.4 Vector-valued Functions ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In most applications, vector-valued functions are used as elemental building blocks of more complex functions. While in most cases, these vectorized operations could be broken down into scalar operations, this would be impractical since it would increase the size of the computational graph representation and action space by orders of magnitude. Thus, it is best to allow vertices of the computational graph to be vector-valued which results in the partial derivatives assigned to the edges becoming Jacobians in their own right. The multiplication operations during vertex elimination are then accordingly replaced with matrix multiplications or higher-order contractions of the elemental Jacobians. For many operations, the Jacobians themselves have a particular internal sparsity structure which can be exploited when performing the eliminations. A simple example is the multiplication of a vector with a matrix followed by the application of a non-linear function $\\bar{f}(\\mathbf{x},\\mathbf{W})=\\operatorname{tanh}{\\left(\\mathbf{W}\\cdot\\mathbf{x}\\right)}$ . The input vertices are given by the input $\\mathbf{x}$ and weights $\\mathbf{W}$ and the intermediate vertex is matrix multiplication $\\begin{array}{r}{a_{i}=\\sum_{j}\\Bar W_{i j}x_{j}}\\end{array}$ with the partial derivatives ", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\frac{\\partial a_{i}}{\\partial W_{k l}}}=x_{l}\\delta_{i k},\\qquad\\qquad\\qquad\\qquad{\\frac{\\partial a_{i}}{\\partial x_{k}}}=W_{i k}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "image", "img_path": "hVmi98a0ki/tmp/d59e39c58cc0a4d90bd45fe56f409169c9cf4a9795f76ebb61cd8995bca7dded.jpg", "img_caption": ["(a) Sparse vertex elimination. "], "img_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "hVmi98a0ki/tmp/e70ad1f918252a2329b270659b49876ddb7790a2c25d7f7b413e5b2e86c20f7a.jpg", "img_caption": ["(b) Computational graph representation. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 3: (a) Graphax implements sparse vertex elimination to benefit from the advantages of cross country elimination. (b) Sketch of the three-dimensional adjacency tensor that represents the computational graph. The colored surfaces represent the five different values encoded in the third dimension. The red and blue surfaces together contain the shape of the Jacobians while the green surface encodes their sparsity. The vertical dotted slices represent the input connectivity of a single vertex. In this work, we compress and feed the vertical slices as tokens into the transformer backbone such that we build a sequence running in direction of the black arrow. ", "page_idx": 5}, {"type": "text", "text": "The output vertex represents the application of the activation function $y_{i}=\\operatorname{tanh}a_{i}$ with the partial derivative ", "page_idx": 5}, {"type": "equation", "text": "$$\n{\\frac{\\partial y_{i}}{\\partial a_{j}}}=\\delta_{i j}\\big(1-\\operatorname{tanh}^{2}a_{i}\\big).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "According to the vertex elimination rule, upon elimination of the intermediate vertex the two Jacobians in equation (2) are assigned to the incoming edges are contracted together with the Jacobian from the outgoing edge in equation (3): ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial y_{i}}{\\partial W_{k l}}=\\displaystyle\\sum_{j}(1-\\operatorname{tanh}^{2}a_{i})\\delta_{i j}\\delta_{j k}x_{l}=\\delta_{i k}(1-\\operatorname{tanh}^{2}a_{i})x_{l},}\\\\ &{\\frac{\\partial y_{i}}{\\partial x_{k}}=\\displaystyle\\sum_{j}(1-\\operatorname{tanh}^{2}a_{i})\\delta_{i j}W_{j k}=(1-\\operatorname{tanh}^{2}a_{i})W_{k i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In both cases, instead of a matrix multiplication, one can perform simple element-wise multiplications as shown in figure 3a. For vectorized cross-country elimination to be efficient, it is paramount to exploit this property. Current state-of-the-art AD frameworks typically lack the ability to perform cross-country elimination and subsequently can not deal with sparse Jacobians. The only exception the authors are aware of is EliAD, an AD interpreter in $C++$ which is fully capable of processing given elimination orders and create the derivative source code [Tadjouddine et al., 2002a]. However, we developed Graphax as a novel AD interpreter that builds on Google\u2019s JAX [Bradbury et al., 2018] in order to leverage it\u2019s defining features such as JIT compilation, automated batching, device parallelism and a user-friendly Python front-end. Graphax is a fully fledged AD interpreter capable of performing cross-country elimination as described above and outperforms JAX\u2019 AD on the relevant tasks by several orders of magnitude (appendix B). Graphax and AlphaGrad are available under and https://github.com/jamielohoff/graphax and https://github.com/jamielohoff/alphagrad. ", "page_idx": 5}, {"type": "text", "text": "2.5 Computational Graph Representation and Network Architecture ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We describe here how the computational graph is represented for optimization in the RL algorithm, as well as the network architecture that is optimized with AlphaZero. In the scalar case, the computational graph can be represented by its adjacency matrix, meaning that for every pair of vertices $(i,j)$ that share an edge, we set the $i$ -th row and the $j$ -th column of the matrix to 1. For the vectorized case, we define an extended adjacency tensor by extending the matrix into the third dimension. Along this third dimension, we store 5 values that describe the sparsity pattern and shape of the Jacobian associated with the respective edge. The first value is an integer between -10 and 10 which encodes the sparsity type of the Jacobian. Details about the supported sparsity types can be found in Appendix C. The next four values contain the shape of the Jacobian associated with the respective edge and thus imply that this representation can at most deal with Jacobians of the shape \u2202\u2202xyikjl where the first two values describe the shape of xkl and the other two values describe the shape of $y_{i j}$ . This can be expanded to arbitrary tensor sizes of $x$ and $y$ , but then also requires the definition of new sparsity types to account for the additional dimensions. Figure 3b shows the representation of the entire computational graph and a single selected edge with a Jacobian of shape $(4,2,4,2)$ with sparsity type 3. A horizontal or vertical slice of the extended adjacency tensor gives the input or output connectivity of a particular vertex. These slices can be compressed and used as tokens to be fed into a transformer where they are processed simultaneously by the attention mechanism so that the model gets a full view of the graph\u2019s connectivity. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "In this work, we compress vertical slices into tokens using a convolutional layer with kernel size (3, 5) and use a linear projection to create a 64-dimensional embedding. We found it helpful to apply a positional encoding to the tokens [Vaswani et al., 2017]. The output of the transformer is then fed into a policy and a value head. The policy head is a MLP mapped across every token separately, thus creating a probability distribution over the vertices to determine the next one to be eliminated. Already eliminated vertices are masked. Similarly, the value head is also a MLP that predicts a score for every token. These scores are then summed to give the value prediction of the network. ", "page_idx": 6}, {"type": "text", "text": "3 Reinforcement Learning for Optimal Elimination Orders ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Cross-country elimination is typically introduced as a means to reduce the computational cost of computing the Jacobian. We cast the problem of finding an efficient vertex elimination order as a single-player RL game called VertexGame. At every step of the game, the agent selects the next vertex to be eliminated by observing the current connectivity of the computational graph. Since it is difficult to directly optimize for execution time, it is common to use the number of multiplications incurred by the elimination order as a proxy value [Tadjouddine et al., 2006, 2002b, Albrecht et al., 2003]. Thus, we chose the negative number of multiplications incurred by eliminating the selected vertex as reward. We use action masking to prevent the agent from eliminating the same vertex twice. This also ensures that the accumulated Jacobian is always exact and has a clear terminal condition: when the extended computational graph is bipartite, the game ends. ", "page_idx": 6}, {"type": "text", "text": "Between elimination orders, the magnitude of the reward can range across multiple orders of magnitude. To tackle this, we rescale the cumulative reward using a monotonous function. For the functions with scalar inputs as well as RoeFlux_3d and random function $f$ , we found the method presented in [Kapturowski et al., 2019] performed well, i.e. we scaled with $s(r)=\\mathrm{sgn}(r)(\\sqrt{|r|+1}-1)+\\epsilon r$ where $\\epsilon=10^{-3}$ . For MLP and TransformerEncoder tasks, the best performance was achieved with logarithmic scaling $s(r)=\\log r$ [Hafner et al., 2024]. VertexGame is played by an AlphaZero agent, which successfully finds new AD algorithms. To reduce the computational cost of the AlphaZero agent [Silver et al., 2017], we employed Gumbel action sampling Danihelka et al. [2022]. Gumbel AlphaZero is a policy improvement algorithm based on sampling actions without replacement which utilizes the Gumbel softmax trick and other augmentations. This algorithm is guaranteed to improve the policy while significantly reducing the number of necessary Monte-Carlo Tree Search (MCTS) simulations. On most tasks, we found that 50 MCTS simulations were sufficient to reach satisfactory performance. Appendix D contains more details about the training of the agent. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To demonstrate the effectiveness of our approach, we devised a set of tasks sampled from different scientific domains where AD is used to compute Jacobians. More details concerning the tasks are listed in appendix A. ", "page_idx": 6}, {"type": "text", "text": "Deep Learning is a prime example for the success of large-scale AD. We analyze a two-layer MLP with layer norm as described in [Goodfellow et al., 2016] and a small-scale version of the transformer encoder [Dosovitskiy et al., 2020]. ", "page_idx": 6}, {"type": "text", "text": "Computational Fluid Dynamics relies on AD for computation of the flux Jacobian on the boundaries of the simulation grid cells. The RoeFlux is particularly relevant and has been studied extensively with vertex elimination in the past [Roe, 1981, Tadjouddine et al., 2002b, Zubair et al., 2023]. We test on the 1D and the 3D variants of this problem. ", "page_idx": 6}, {"type": "text", "text": "Differential Kinematics uses Jacobians to quantify the behavior of a robot or other mechanical system with respect to their controllable parameters (e.g. joints, actuators). There has been a surge in interest of computing the Jacobian using AD[Giftthaler et al., 2017]. We chose the forward kinematics of a 6-DOF robot arm as a representative problem and follow [Dikmenli, 2022] for the implementation. ", "page_idx": 6}, {"type": "table", "img_path": "hVmi98a0ki/tmp/49e53dabf60ef2d253308e81907050c6dc5bd4bb1378ee6095d17154cf698fe8.jpg", "table_caption": ["Table 1: Number of multiplications required by the best discovered elimination order for a batch size of one. Results obtained from VertexGame played by the AlphaZero agent with 50 MCTS simulations and a Gumbel noise scale of 1.0. $\\dagger$ marks the experiments where we employed a log-scaling of the cumulative reward instead of the default scaling. The values in parentheses were obtained for 250 MCTS simulations. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Non-Linear Equation Solving requires the computation of large Jacobians to apply state-of-theart solvers. The MINPACK problem collection provides a set of problems derived from real-life applications of non-linear optimization and designed to be representative of commonly encountered problems. In particular, we analyze the HumanHeartDiple and PropaneCombustion tasks, for which vertex elimination has also been analyzed thoroughly in [Forth et al., 2004b, Averick et al., 1992]. ", "page_idx": 7}, {"type": "text", "text": "Computational Finance makes use of AD for fast computation of the so called \u201cgreeks\u201d which measure the sensitivities of the value of an option to the model parameters[Naumann, 2010, Savine and Andreasen, 2021]. Here, we compute the second-order greeks of the Black-Scholes equation using AD by computing the Hessian of the Black-Scholes equation through evaluation of the Jacobian of the Jacobian Black and Scholes [1973]. This way, this task serves a two-fold purpose by also demonstrating how our approach is also useful for finding good AD algorithms for higher-order derivatives. ", "page_idx": 7}, {"type": "text", "text": "Random Functions are also commonly used to evaluate the performance of new AD algorithms [Albrecht et al., 2003]. We generated two random functions $f$ and $g$ with vector-valued and only scalar inputs respectively. The random code generator used to generate these arbitrary functions is included in the accompanying software package. ", "page_idx": 7}, {"type": "text", "text": "4.1 Finding Optimal Elimination Orders ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Table 1 shows the number of multiplications required by the best elimination order found over 6 runs with different seeds. The model was trained from scratch on each task separately with a batch size of 1 to to keep the rewards as small as possible. The resulting AD algorithms are nonetheless scalable to arbitrary batch sizes. We use forward-mode, reverse-mode and the minimal Markowitz degree method as baselines for comparison. The first six tasks are simple functions with only scalar inputs and simple operations and the cumulative reward stays within the same order of magnitude, making them easier to solve. For all tasks, our approach was able find new elimination orders with improvements ranging from $2\\%$ to almost $20\\%$ . We found that even for only 5 MCTS simulations, the agent was able to find better than state-of-the-art solutions for the scalar tasks. ", "page_idx": 7}, {"type": "text", "text": "On the opposite spectrum, our experiments with 250 MCTS simulations yielded no significant improvement over the results presented in table 1. The four remaining tasks are arguably more difficult since the vector-valued inputs and large variance within possible rewards provide an additional challenge. The RoeFlux_3d and random function $f$ were solved successfully with 50 MCTS simulations and yielded improvements of up to $33\\%$ . This is in stark contrast to prior work such as [Naumann, 1999], where algorithms such as simulated annealing or dynamic programming struggled to even beat common heuristics such as minimal Markowitz or reverse-mode AD. With a budget of only 50 MCTS simulations, AlphaGrad failed to find improvements for both deep learning tasks. ", "page_idx": 7}, {"type": "table", "img_path": "hVmi98a0ki/tmp/d0493e5c91af3b5e32603634d55ea0bb4a739e98f5c3f5544c776607cfcd0e47.jpg", "table_caption": ["Table 2: Median runtimes for the results obtained in table 1. Results were measured with Graphax for batch size 512 on an AMD EPYC 9684X $2\\mathbf{x}96\\cdot$ -Core processor. Uncertainties are given as 2.5- and 97.5-percentiles over 1000 trials. Execution time is given in milliseconds and default XLA compilation flags were used for all experiments. The size of the networks, i.e. the number of neurons were increased for the MLP and Transformer Encoder by a factor of 16 to create a more realistic sample. GPU experiments were run on a NVIDIA RTX 4090 with JIT compilation. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "The authors conjecture that this is not only due to difficulties presented above, but because reversemode AD (backpropagation) is already a very well-suited algorithm for computing Jacobians of \u201cfunnel-like\u201d computational graphs with many inputs and a single, scalar output. Despite this, with an increase to 250 MCTS simulations, the agent marginally outperformed backpropagation for both deep learning models. Appendix E contains more information about the experiments, including reward curves, the actual elimination orders and more details about their implementation. Note that the results in table 1 were obtained by separately training on each single function/graph. ", "page_idx": 8}, {"type": "text", "text": "We also include joint training runs where the agent was trained on all tasks at once. While the results were inferior to the separate training mode, the agent found new, improved elimination orders for almost all tasks except the $M L P$ , TransformerEncoder, RoeFlux_3d and PropaneCombustion tasks. For the random function $f$ and BlackScholes_Jacobian task, the multi-task training outperformed the results in table 1 with new best results of 5884 and 307 respectively, thereby showing that the algorithm search might benefit from training on diverse tasks simultaneously. This also hints at the possibility of building a more general statistical model of AD applicable to workloads from many different domains. We also experimented with PPO as an alternative (appendix F). ", "page_idx": 8}, {"type": "text", "text": "4.2 Runtime Improvements and the Graphax library ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The results in table 1 are mainly of theoretical value. Here, we investigate how these translate into actual runtime improvements. For this purpose, we implemented Graphax, to our knowledge the first Python-based AD interpreter able to leverage cross-country elimination. Graphax builds a second program that computes the Jacobian by leveraging the elimination orders found by AlphaGrad and using the source code of the function as a template by analyzing its Jaxpression. The Jaxpression is JAX\u2019 own representation of the computational graph of the function in question. ", "page_idx": 8}, {"type": "text", "text": "Table 2 shows runtime improvements for the elimination orders found in table 1 for a batch size of 512 with varying levels of improvement. This is due to the fact that the number of multiplications alone was only a proxy to capture the complexity of the entire program. It ignores other relevant quantities such as memory accesses and operation fusion during compilation. Nonetheless, a particularly impressive gain over the state-of-the-art methods can be observed for the RoeFlux tasks and the RobotArm_6DOF task. Remarkably, we also observe a minor improvement for both deep learning tasks when executed on GPUs. Note that the TransformerEncoder task was evaluated on a batch size of 1 because VertexGame only supports two-dimensional input tensors. Figure 4 shows how some of AlphaGrad\u2019s algorithms scale with growing batch size. Appendix B provides an in-depth comparison of our work and JAX\u2019 own AD modes. In general, the combination of AlphaGrad and Graphax is able to outperform the JAX AD modes in most cases, sometimes by orders of magnitude. While [Forth et al., 2004b] and [Tadjouddine et al., 2006] present state-of-the-art results for some of the investigated tasks, we were unable to reproduce the experiments given their implementation details. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "image", "img_path": "hVmi98a0ki/tmp/663076e146804849fe155f2916b4956e76637401ffdf9e22a0244e0183010093.jpg", "img_caption": ["Roe_3d evaluation times for different modes and batch sizes MLP evaluation times for different modes and batch sizes ", "Figure 4: Runtime measurements over 1000 trials for the vectorized RoeFlux_ $_{3d}$ and $M L P$ tasks with different batch sizes using the same setup as in table 2. The MLP network sizes were scaled up with growing batch size by a constant factor. The exact procedure of scaling is explained in appendix B. Error bars are the 2.5- and 97.5-percentiles of the runtimes. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work we successfully demonstrated that AlphaGrad discovers new AD algorithms that outperform the state-of-the-art. We demonstrated that these theoretical gains translate into measurable runtime improvements with Graphax, a Python-based interpreter we developed that leverages the AD algorithms discovered by AlphaGrad. However, AlphaGrad currently only optimizes for multiplications which cannot capture the entire complexity of the AD algorithm. Future work could explore other optimization targets such as execution time, memory accesses, quantization and different hardware backends using a hardware model for efficient simulation. Another promising research avenue would be the implementation of a much more general framework for vertex elimination. Inspiration could be drawn from Enzyme, which operates on the intermediate representations (IR) using LLVM and is therefore less bound by the choice of programming language. Leveraging the LLVM approach would also open up new directions regarding AD-specific compiler optimizations similar to what was presented in LAGrad. ", "page_idx": 9}, {"type": "text", "text": "Two of the main shortcomings of our work are the lack of support for dynamic control flow and dynamically changing functions as well as the need for retraining of the algorithm for every single computational graph. To circumvent the first issue, it would be possible to implement a version of vertex elimination that can deal with dynamic control flow although this would require some significant changes to Graphax. However, as demonstrated by the wide range of benchmark tasks, several applications are already possible without this feature. ", "page_idx": 9}, {"type": "text", "text": "The second issue, addressed partially in our work, is the training of the agent on multiple graphs at once. While the best results were still achieved with single-graph training, the multi-graph training still outperformed the other existing methods on most benchmarks. This hints at the possibility to train our agent on a large set of computational graphs at once, thereby effectively building a statistical model of AD. Finally, VertexGame offers a novel way to evaluate existing RL algorithms on a real-world problem that poses diverse challenges such as rewards across multiple scales and large action spaces. Thus, it could complement existing benchmarks such as OpenAI gym and MuJoCo [Todorov et al., 2012, Towers et al., 2023]. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was sponsored by the Federal Ministry of Education, Germany (projects NEUROTEC-II grant no. 16ME0398K and 16ME0399 as well as GreenEdge-FuE, funding no. 16ME0521). The authors also gratefully acknowledge the Gauss Centre for Supercomputing e.V. (www.gauss-centre.eu) for funding this project by providing computing time on the GCS Supercomputer JUWELS at J\u00fclich Supercomputing Centre (JSC). Furthermore, we thank Mark Sch\u00f6ne, Christian Pehle, Uwe Naumann and Matthew Johnson for the helpful discussions regarding the project. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Andreas Albrecht, Peter Gottschling, and Uwe Naumann. Markowitz-type heuristics for computing jacobian matrices efficiently. In Peter M. A. Sloot, David Abramson, Alexander V. Bogdanov, Yuriy E. Gorbachev, Jack J. Dongarra, and Albert Y. Zomaya, editors, Computational Science \u2014 ICCS 2003, pages 575\u2013584, Berlin, Heidelberg, 2003. Springer Berlin Heidelberg. ISBN 978-3-540-44862-4.   \nB. M. Averick, R. G. Carter, Guo-Liang Xue, and J. J. More. The minpack-2 test problem collection. Technical report, University of Illinois, June 1992. URL https://digital.library.unt.edu/ ark:/67531/metadc734562/. Accessed May 11, 2024.   \nAtilim Gunes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. Automatic differentiation in machine learning: a survey. Journal of Machine Learning Research, 18(153):1\u201343, 2018. URL http://jmlr.org/papers/v18/17-468.html.   \nLukas Biewald. Experiment tracking with weights and biases, 2020. URL https://www.wandb. com/. Software available from wandb.com.   \nFischer Black and Myron Scholes. The pricing of options and corporate liabilities. The Journal of Political Economy, 81(3):637\u2013654, 1973. URL http://www.jstor.org/stable/1831029.   \nJames Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.   \nLuca Capriotti and Michael B. Giles. Algorithmic differentiation: Adjoint greeks made easy. Available at SSRN, 2011. doi: 10.2139/ssrn.1801522. URL https://ssrn.com/abstract $=$ 1801522. April 2.   \nJieqiu Chen, Paul Hovland, Todd Munson, and Jean Utke. An integer programming approach to optimal derivative accumulation. Lecture Notes in Computational Science and Engineering, 87, 01 2012. doi: 10.1007/978-3-642-30023-3_20.   \nIvo Danihelka, Arthur Guez, Julian Schrittwieser, and David Silver. Policy improvement by planning with gumbel. In International Conference on Learning Representations, 2022. URL https: //openreview.net/forum?id $=$ bERaNdoegnO.   \nDeepMind, Igor Babuschkin, Kate Baumli, Alison Bell, Surya Bhupatiraju, Jake Bruce, Peter Buchlovsky, David Budden, Trevor Cai, Aidan Clark, Ivo Danihelka, Antoine Dedieu, Claudio Fantacci, Jonathan Godwin, Chris Jones, Ross Hemsley, Tom Hennigan, Matteo Hessel, Shaobo Hou, Steven Kapturowski, Thomas Keck, Iurii Kemaev, Michael King, Markus Kunesch, Lena Martens, Hamza Merzic, Vladimir Mikulik, Tamara Norman, George Papamakarios, John Quan, Roman Ring, Francisco Ruiz, Alvaro Sanchez, Laurent Sartran, Rosalia Schneider, Eren Sezener, Stephen Spencer, Srivatsan Srinivasan, Milo\u0161 Stanojevi\u00b4c, Wojciech Stokowiec, Luyu Wang, Guangyao Zhou, and Fabio Viola. The DeepMind JAX Ecosystem, 2020. URL http://github.com/google-deepmind.   \nSerap Dikmenli. Forward & inverse kinematics solution of 6-dof robots those have offset & spherical wrists. Eurasian Journal of Science Engineering and Technology, 3, 05 2022. doi: 10.55696/ejset. 1082648.   \nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. CoRR, abs/2010.11929, 2020. URL https://arxiv.org/abs/2010.11929.   \nAlhussein Fawzi, Matej Balog, Andrew Huang, et al. Discovering faster matrix multiplication algorithms with reinforcement learning. Nature, 610:47\u201353, 2022. doi: 10.1038/s41586-022-05172-4. URL https://doi.org/10.1038/s41586-022-05172-4.   \nShaun A. Forth, John D. Pryce, Mohamed Tadjouddine, and John K. Reid. Jacobian code generated by source transformation and vertex elimination can be as efficient as hand-coding. ACM Transactions on Mathematical Software (TOMS), 30(3):266\u2013299, 2004a.   \nShaun A. Forth, Mohamed Tadjouddine, John D. Pryce, and John K. Reid. Jacobian code generated by source transformation and vertex elimination can be as efficient as hand-coding. ACM Trans. Math. Softw., 30(3):266\u2013299, sep 2004b. ISSN 0098-3500. doi: 10.1145/1024074.1024076. URL https://doi.org/10.1145/1024074.1024076.   \nMarkus Giftthaler, Michael Neunert, Markus St\u00e4uble, Marco Frigerio, Claudio Semini, and Jonas Buchli. Automatic differentiation of rigid body dynamics for optimal control and estimation. CoRR, abs/1709.03799, 2017. URL http://arxiv.org/abs/1709.03799.   \nIan J. Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, Cambridge, MA, USA, 2016. http://www.deeplearningbook.org.   \nA. Griewank and A. Walther. Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation, Second Edition. Other Titles in Applied Mathematics. Society for Industrial and Applied Mathematics (SIAM, 3600 Market Street, Floor 6, Philadelphia, PA 19104), 2008. ISBN 9780898717761. URL https://books.google.de/books?id $=$ xoiiLaRxcbEC.   \nDanijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models, 2024.   \nPeter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning that matters. In AAAI Conference on Artificial Intelligence, 2017. URL https://api.semanticscholar.org/CorpusID:4674781.   \nShengyi Huang, Rousslan Fernand Julien Dossa, Antonin Raffin, Anssi Kanervisto, and Weixun Wang. The 37 implementation details of proximal policy optimization. In ICLR Blog Track, 2022. URL https://iclr-blog-track.github.io/2022/ 03/25/ppo-implementation-details/. https://iclr-blog-track.github.io/2022/03/25/ppoimplementation-details/.   \nYuu Jinnai, Arash Mehrjou, Kamil Ciosek, Anna Mitenkova, Alan Lawrence, Tom Ellis, Ryota Tomioka, Simon Peyton Jones, and Andrew Fitzgibbon. Knossos: Compiling ai with ai. 9 2019.   \nSteven Kapturowski, Georg Ostrovski, Will Dabney, John Quan, and Remi Munos. Recurrent experience replay in distributed reinforcement learning. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id $\\equiv$ r1lyTjAqYX.   \nHiroharu Kato, Deniz Beker, Mihai Morariu, Takahiro Ando, Toru Matsuoka, Wadim Kehl, and Adrien Gaidon. Differentiable rendering: A survey. CoRR, abs/2006.12057, 2020. URL https: //arxiv.org/abs/2006.12057.   \nPatrick Kidger and Cristian Garcia. Equinox: neural networks in JAX via callable PyTrees and flitered transformations. Differentiable Programming workshop at Neural Information Processing Systems 2021, 2021.   \nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014. URL https://api.semanticscholar.org/CorpusID:6628106.   \nSeppo Linnainmaa. Taylor expansion of the accumulated rounding error. BIT Numerical Mathematics, 16:146\u2013160, 1976. URL https://api.semanticscholar.org/CorpusID:122357351.   \nDaniel J. Mankowitz, Andrea Michi, Anton Zhernov, et al. Faster sorting algorithms discovered using deep reinforcement learning. Nature, 618:257\u2013263, 2023. doi: 10.1038/s41586-023-06004-9. URL https://doi.org/10.1038/s41586-023-06004-9.   \nCharles C. Margossian. A review of automatic differentiation and its efficient implementation. CoRR, abs/1811.05031, 2018. URL http://arxiv.org/abs/1811.05031.   \nWilliam S. Moses and Valentin Churavy. Instead of rewriting foreign code for machine learning, automatically synthesize fast gradients. In Proceedings of the 34th Conference on Neural Information Processing Systems (NeurIPS 2020), page Page Numbers, Vancouver, Canada, 2020. MIT CSAIL. URL https://proceedings.neurips.cc/paper/2020/file/ 9332c513ef44b682e9347822c2e457ac-Paper.pdf.   \nUwe Naumann. Save - simulated annealing applied to the vertex elimination problem in computational graphs. Technical Report RR-3660, INRIA, 1999. URL https://hal.inria.fr/ inria-00073012.   \nUwe Naumann. Optimal jacobian accumulation is np-complete. Mathematical Programming, 112:427\u2013441, 2008. doi: 10.1007/s10107-006-0042-z. URL https://doi.org/10.1007/ s10107-006-0042-z.   \nUwe Naumann. Exact first- and second-order greeks by algorithmic differentiation. 01 2010.   \nUwe Naumann. Optimization of generalized jacobian chain products without memory constraints, 2020.   \nAditya Sanjay Paliwal, Felix Gimeno, Vinod Nair, Yujia Li, Miles Lubin, Pushmeet Kohli, and Oriol Vinyals. Regal: Transfer learning for fast optimization of computation graphs. $A r X i\\nu$ , abs/1905.02494, 2019. URL https://api.semanticscholar.org/CorpusID:146808144.   \nMai Jacob Peng and Christophe Dubach. Lagrad: Statically optimized differentiable programming in mlir. In Proceedings of the 32nd ACM SIGPLAN International Conference on Compiler Construction, CC 2023, page 228\u2013238, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9798400700880. doi: 10.1145/3578360.3580259. URL https://doi.org/ 10.1145/3578360.3580259.   \nP.L Roe. Approximate riemann solvers, parameter vectors, and difference schemes. Journal of Computational Physics, 43(2):357\u2013372, 1981. ISSN 0021-9991. doi: https://doi.org/10.1016/ 0021-9991(81)90128-5. URL https://www.sciencedirect.com/science/article/pii/ 0021999181901285.   \nA. Savine and J. Andreasen. Modern Computational Finance: Scripting for Derivatives and xVA. Wiley, 2021. ISBN 9781119540786. URL https://books.google.de/books?id= mBZDEAAAQBAJ.   \nJ\u00fcrgen Schmidhuber. Deep learning in neural networks: An overview. CoRR, abs/1404.7828, 2014. URL http://arxiv.org/abs/1404.7828.   \nPatrick Schmidt, J. Born, D. Bommes, M. Campen, and Leif Kobbelt. Tinyad: Automatic differentiation in geometry processing made simple. Computer Graphics Forum, 41:113\u2013124, 10 2022. doi: 10.1111/cgf.14607.   \nJulian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy P. Lillicrap, and David Silver. Mastering atari, go, chess and shogi by planning with a learned model. CoRR, abs/1911.08265, 2019. URL http://arxiv.org/abs/1911.08265.   \nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017.   \nDavid Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, L. Sifre, Dharshan Kumaran, Thore Graepel, Timothy P. Lillicrap, Karen Simonyan, and Demis Hassabis. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. ArXiv, abs/1712.01815, 2017. URL https://api.semanticscholar.org/ CorpusID:33081038.   \nM. Tadjouddine, F. Bodman, J. D. Pryce, and S. A. Forth. Improving the performance of the vertex elimination algorithm for derivative calculation. In Martin B\u00fccker, George Corliss, Uwe Naumann, Paul Hovland, and Boyana Norris, editors, Automatic Differentiation: Applications, Theory, and Implementations, pages 111\u2013120, Berlin, Heidelberg, 2006. Springer Berlin Heidelberg.   \nMohamed Tadjouddine, Shaun A. Forth, John D. Pryce, and John K. Reid. Performance issues for vertex elimination methods in computing jacobians using automatic differentiation. In Peter M. A. Sloot, Alfons G. Hoekstra, C. J. Kenneth Tan, and Jack J. Dongarra, editors, Computational Science \u2014 ICCS 2002, pages 1077\u20131086, Berlin, Heidelberg, 2002a. Springer Berlin Heidelberg. ISBN 978-3-540-46080-0.   \nMohamed Tadjouddine, Shaun A. Forth, John D. Pryce, and John K. Reid. Performance issues for vertex elimination methods in computing jacobians using automatic differentiation. In Peter M. A. Sloot, Alfons G. Hoekstra, C. J. Kenneth Tan, and Jack J. Dongarra, editors, Computational Science \u2014 ICCS 2002, pages 1077\u20131086, Berlin, Heidelberg, 2002b. Springer Berlin Heidelberg. ISBN 978-3-540-46080-0.   \nEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026\u20135033. IEEE, 2012. doi: 10.1109/IROS.2012.6386109.   \nEdan Toledo, Laurence Midgley, Donal Byrne, Callum Rhys Tilbury, Matthew Macfarlane, Cyprien Courtot, and Alexandre Laterre. Flashbax: Streamlining experience replay buffers for reinforcement learning with jax, 2023. URL https://github.com/instadeepai/flashbax/.   \nMark Towers, Jordan K. Terry, Ariel Kwiatkowski, John U. Balis, Gianluca de Cola, Tristan Deleu, Manuel Goul\u00e3o, Andreas Kallinteris, Arjun KG, Markus Krimmel, Rodrigo Perez-Vicente, Andrea Pierr\u00e9, Sander Schulhoff, Jun Jet Tai, Andrew Tan Jin Shen, and Omar G. Younis. Gymnasium, March 2023. URL https://zenodo.org/record/8127025.   \nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS\u201917, page 6000\u20136010, Red Hook, NY, USA, 2017. Curran Associates Inc. ISBN 9781510860964.   \nYanqi Zhou, Sudip Roy, AmirAli Abdolrashidi, Daniel Wong, Peter C. Ma, Qiumin Xu, Hanxiao Liu, Mangpo Phitchaya Phothilimtha, Shen Wang, Anna Goldie, Azalia Mirhoseini, and James Laudon. Transferable graph optimizers for ml compilers. ArXiv, abs/2010.12438, 2020. URL https://api.semanticscholar.org/CorpusID:225062396.   \nMuhammad Zubair, Dinesh Ranjan, Andrew Walden, Grigore Nastac, Eric Nielsen, Boris Diskin, Michael Paterno, Seung Jung, and James H. Davis. Efficient gpu implementation of automatic differentiation for computational fluid dynamics. In IEEE 30th International Conference on High Performance Computing, Data, and Analytics (HiPC), page 11, 2023. doi: 10.1109/HiPC58850. 2023.00055. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Task Descriptions ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "This section describes in-depth the mathematical formulation of the tasks that were evaluated in table 1 and table 2. An implementation of the functions can be found in the accompanying software package. Note that if a set of variables $(x_{1},\\ldots,x_{n})$ is referred to as $\\{x_{i}\\}$ , they are treated as a separate scalar inputs to the function while $\\mathbf{x}$ treats them as a single vectorized input. ", "page_idx": 14}, {"type": "text", "text": "A.1 RoeFlux_1d ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "For the implementation of the RoeFlux_1d task we thoroughly followed [Roe, 1981]. The pressure $p_{\\mathrm{1d}}$ , enthalpy $H_{\\mathrm{1d}}$ and flux term $F_{\\mathrm{1d}}$ of the one-dimensional Euler equations are defined through: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{p_{1\\mathrm{d}}}(u_{0},u_{1},u_{2})=(\\gamma-1)(u_{2}-\\displaystyle\\frac{u_{1}^{2}}{2u_{0}}),}\\\\ {{\\displaystyle H_{1\\mathrm{d}}}(u_{0},u_{2},p)=\\displaystyle\\frac{u_{2}+p}{u_{0}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\nF_{\\mathrm{1d}}(u_{0},u_{1},u_{2},p)=\\Big(u_{1},p+\\frac{u_{1}^{2}}{u_{0}},\\frac{u_{1}}{u_{0}}(p+u_{2})\\Big).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The Roe flux $\\phi_{\\mathrm{Roe}}(u_{10},u_{11},u_{12},u_{13},u_{14},u_{\\mathrm{r0}},u_{\\mathrm{r1}},u_{\\mathrm{r2}},u_{\\mathrm{r3}},u_{\\mathrm{r4}})$ between two adjacent cells $u_{\\mathrm{l}}$ and $u_{\\mathrm{r}}$ is computed using the routine described below. First, we define some averaged quantities to simplify formulation: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Delta u_{0}=u_{10}-u_{\\mathrm{r0}},}\\\\ {u_{\\mathrm{lr0}}=\\sqrt{u_{10}u_{\\mathrm{r0}}},}\\\\ {w_{1}=\\sqrt{u_{10}}-\\sqrt{u_{\\mathrm{r0}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then we define some state variables for the left cell through ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{v_{1}=\\frac{u_{11}}{u_{10}},}}\\\\ {\\displaystyle{p_{1}=p_{1\\mathrm{d}}\\big(u_{10},u_{11},u_{12}\\big),}}\\\\ {\\displaystyle{h_{1}=H_{1\\mathrm{d}}\\big(u_{10},u_{12},p_{1}\\big).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Similarly, we define the same variables for the right cell through ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{v_{\\mathrm{r}}=\\frac{u_{\\mathrm{r1}}}{u_{\\mathrm{r0}}},}\\\\ {p_{\\mathrm{r}}=p_{\\mathrm{1d}}\\big(u_{\\mathrm{r0}},u_{\\mathrm{r1}},u_{\\mathrm{r2}}\\big),}\\\\ {h_{\\mathrm{r}}=H_{\\mathrm{1d}}\\big(u_{\\mathrm{r0}},u_{\\mathrm{r2}},p_{\\mathrm{r}}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We define some differences between the state variables of the two cells though ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Delta p=p_{\\mathrm{l}}-p_{\\mathrm{r}},\\;\\;\\;\\Delta v=v_{\\mathrm{l}}-v_{\\mathrm{r}},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We proceed with the introduction of some auxiliary variables for further computation so that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{u=\\frac{\\sqrt{u_{\\mathrm{l0}}}v_{\\mathrm{l}}+\\sqrt{u_{\\mathrm{r0}}}v_{\\mathrm{r}}}{w_{\\mathrm{l}}},}\\\\ {h=\\frac{\\sqrt{u_{\\mathrm{l0}}}h_{\\mathrm{l}}+\\sqrt{u_{\\mathrm{r0}}}h_{\\mathrm{r}}}{w_{\\mathrm{l}}},}\\\\ {a_{2}=(\\gamma-1)(h-\\frac{1}{2}q_{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $q_{2}=u^{2}$ , $a=\\sqrt{a_{2}}$ and $n=u_{\\mathrm{lr0}}a$ . We define $l_{\\mathrm{p}}=|u+a|,l=|u|$ and $l_{\\mathrm{n}}=|u|$ and proceed to writing: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{c_{0}=\\left(\\Delta u_{0}-\\displaystyle\\frac{\\Delta p}{a_{2}}\\right)l}}\\\\ {{c_{1}=\\left(\\Delta v+\\displaystyle\\frac{\\Delta p}{n}\\right)l_{\\mathrm{p}}}}\\\\ {{c_{2}=\\left(\\Delta v-\\displaystyle\\frac{\\Delta p}{n}\\right)l_{\\mathrm{n}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Next, we compute the fluxes between the cells through ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F_{\\mathrm{l0}},F_{\\mathrm{l1}},F_{\\mathrm{l2}}=F_{\\mathrm{ld}}(u_{\\mathrm{l0}},u_{\\mathrm{l1}},u_{\\mathrm{l2}},p_{\\mathrm{l}})}\\\\ &{F_{\\mathrm{r0}},F_{\\mathrm{r1}},F_{\\mathrm{r2}}=F_{\\mathrm{ld}}(u_{\\mathrm{r0}},u_{\\mathrm{r1}},u_{\\mathrm{r2}},p_{\\mathrm{r}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and then define $F_{i}=F_{1i}+F_{\\mathrm{r}i}$ and $\\begin{array}{r}{\\alpha=\\frac{1}{2a}u_{\\mathrm{lr0}}}\\end{array}$ . The flux differences are then given through ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\Delta F_{0}=c_{0}+\\alpha c_{1}-\\alpha c_{2},}\\\\ {\\Delta F_{1}=c_{0}u+\\alpha c_{1}(u+a)-\\alpha c_{2}(u-a),}\\\\ {\\Delta F_{2}=\\cfrac{1}{2}c_{0}q_{2}+\\alpha c_{1}(h+u a)-\\alpha c_{2}(h-u a).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then the output of the function is given through $\\phi_{0},\\phi_{1},\\phi_{2}$ with $\\begin{array}{r}{\\phi_{i}=\\frac{1}{2}(F_{i}-\\Delta F_{i})}\\end{array}$ . ", "page_idx": 15}, {"type": "text", "text": "A.2 RoeFlux_3d ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The implementation of the RoeFlux_ $_{3d}$ task is similar to the RoeFlux_1d task. Again, we follow [Roe, 1981] for the implementation and start by defining functions for the pressure $p_{\\mathrm{{3d}}}$ and enthalpy $H_{\\mathrm{{3d}}}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle p_{\\mathrm{3d}}(u_{0},{\\bf u},u_{4})=(\\gamma-1)\\left(u_{4}-\\frac{|{\\bf u}|^{2}}{2u_{0}}\\right),}}\\\\ {{\\displaystyle H_{\\mathrm{3d}}(u_{0},u_{4},p)=\\frac{u_{4}+p}{u_{0}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note that now instead of a one-dimensional state variable $u_{1}$ , we have a three-dimensional state vector $\\mathbf{u}$ and the role of $u_{2}$ is now taken over by $u_{4}$ . The flux in three dimensions is given through ", "page_idx": 15}, {"type": "equation", "text": "$$\nF_{3\\mathrm{d}}(u_{0},\\mathbf{u},u_{4},\\mathbf{v},p)=(u_{1},\\mathbf{p}+\\mathbf{u}v,v_{1}(p+u_{4}))\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $u_{1},u_{2},u_{3}$ and $v_{1},v_{2},v_{3}$ are the three components that make up $\\mathbf{u}$ and $\\mathbf{v}$ respectively and $\\mathbf{p}=(p,0,0)^{\\intercal}$ . We then again define the finite differences ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta u_{0}=u_{10}-u_{\\mathrm{r0}},}\\\\ &{\\Delta\\mathbf{u}=\\mathbf{u}_{1}-\\mathbf{u}_{\\mathrm{r}},}\\\\ &{\\Delta u_{4}=u_{14}-u_{\\mathrm{r4}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and furthermore set $\\begin{array}{r}{\\mathbf{v}_{\\mathrm{{l}}}=\\frac{\\mathbf{u}_{\\mathrm{{l}}}}{u_{\\mathrm{{l0}}}}}\\end{array}$ and $\\begin{array}{r}{\\mathbf{v}_{\\mathrm{r}}=\\frac{\\mathbf{u}_{\\mathrm{r}}}{u_{\\mathrm{r0}}}}\\end{array}$ We continue with defining some auxiliary variables ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{w_{1}=\\sqrt{u_{\\mathrm{l0}}}-\\sqrt{u_{\\mathrm{r0}}}.\\qquad\\qquad}\\\\ {\\mathbf{t}=\\frac{\\sqrt{u_{\\mathrm{l0}}}\\mathbf{v}_{\\mathrm{l}}+\\sqrt{u_{\\mathrm{r0}}}\\mathbf{v}_{\\mathrm{l}}}{w_{1}}.\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then we define some state variables for the left cell through ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{p_{\\mathrm{l}}=p_{\\mathrm{3d}}(u_{\\mathrm{l0}},\\mathbf{u}_{\\mathrm{l}},u_{\\mathrm{l4}}),}\\\\ {h_{\\mathrm{l}}=H_{\\mathrm{3d}}(u_{\\mathrm{l0}},u_{\\mathrm{l4}},p_{\\mathrm{l}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Similarly, we define the same variables for the right cell through ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{p_{\\mathrm{r}}=p_{\\mathrm{3d}}\\bigl(u_{\\mathrm{r0}},\\mathbf{u}_{\\mathrm{r}},u_{\\mathrm{r2}}\\bigr),}\\\\ {h_{\\mathrm{r}}=H_{\\mathrm{3d}}\\bigl(u_{\\mathrm{r0}},u_{\\mathrm{r4}},p_{\\mathrm{r}}\\bigr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then we introduce ", "page_idx": 15}, {"type": "equation", "text": "$$\nh=\\frac{\\sqrt{u_{\\mathrm{l0}}}h_{\\mathrm{l}}+\\sqrt{u_{\\mathrm{r0}}}h_{\\mathrm{r}}}{w_{\\mathrm{1}}},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and set $q_{2}=|\\mathbf{t}|^{2}$ and $a_{2}=(\\gamma-1)(h-{\\textstyle{\\frac{1}{2}}}q_{2})$ and $a=\\sqrt{a_{2}}$ . Furthermore, we define the eigenvalues of the Roe flux problem as $l_{\\mathrm{p}}=t_{1}+a,\\bar{l}=t_{1},l_{\\mathrm{n}}=t_{1}-a$ . Then coefficients $c_{i}$ are given through ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{c_{0}=\\frac{k_{1}-k_{2}}{2}l_{\\mathrm{m}},}\\\\ &{c_{1}=l\\left(\\frac{\\Delta u_{2}}{t_{2}}-\\Delta u_{0}\\right),}\\\\ &{c_{2}=l\\left(\\frac{\\Delta u_{3}}{t_{3}}-\\Delta u_{1}\\right),}\\\\ &{c_{3}=l\\left(\\frac{\\gamma-1}{a_{2}}\\left((h-q_{2})\\Delta u_{0}+\\mathbf{t}\\cdot\\Delta\\mathbf{u}-\\Delta u_{4}\\right)\\right),}\\\\ &{c_{4}=\\frac{k_{1}+k_{2}}{2}l_{\\mathrm{p}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where we defined $k_{1}=\\Delta u_{0}-c_{3}$ and $k_{2}=\\frac{\\Delta u_{1}-t_{1}\\Delta u_{0}}{a}.$ \u2206u1 \u2212t1\u2206u0. The definition of the fluxes of each cell is similar to the formulation in the one-dimensional case. The flux changes are given through ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta F_{0}=c_{0}+c_{3}+c_{4}l_{\\mathrm{p}},}\\\\ &{\\Delta F_{1}=c_{0}l_{\\mathrm{n}}+c_{3}t_{1}+c_{4}l_{\\mathrm{p}},}\\\\ &{\\Delta F_{2}=c_{0}t_{2}+c_{1}t_{2}+c_{2}t_{2}+c_{3}t_{2}+c_{4}t_{2},}\\\\ &{\\Delta F_{3}=c_{0}t_{3}+c_{2}t_{3}+c_{3}t_{3}+c_{4}t_{3},}\\\\ &{\\Delta F_{4}=c_{0}(h-t_{1}a)+c_{1}t_{2}^{2}+c_{2}t_{3}^{2}+\\frac{c_{3}q_{2}}{2}+c_{4}(h+t_{1}a).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The results of $\\Delta F_{1},\\Delta F_{2}$ and $\\Delta F_{3}$ are concatenated into the vector $\\Delta\\mathbf{F}$ . The output of the function is then $\\left(\\phi_{0},\\phi,\\phi_{4}\\right)$ as defined in the one-dimensional case. ", "page_idx": 16}, {"type": "text", "text": "A.3 RobotArm_6DOF ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The RobotArm_ $6D O F$ task models the forward differential kinematics of a 6-degree-of-freedom (6-DOF) robot arm as is often found in robotics labs and industrial manufacturing sites. For the implementation, we followed [Dikmenli, 2022] and define $c_{i}=\\cos x_{i}$ and $s_{i}=\\sin x_{i}$ . Furthermore, we define the functions ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{S(u,v)=\\cos(u)\\sin(v)+\\sin(u)\\cos(v),}}\\\\ {{C(u,v)=\\cos(u)\\cos(v)-\\sin(u)\\sin(v).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then we also define $s_{i j}=S(t_{i},t_{j})$ and $c_{i j}=C(t_{i},t_{j})$ for the input variables $\\{t_{i}\\}$ of the problem. Then we proceed with calculating the auxiliary intermediates ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{a_{y}=s_{5}\\big(c_{1}c_{23}c_{4}+s_{1}s_{4}\\big)+c_{1}s_{23}c_{5},}\\\\ &{a_{y}=s_{5}\\big(s_{1}c_{23}c_{4}-c_{1}s_{4}\\big)+s_{1}s_{23}c_{5},}\\\\ &{a_{z}=s_{23}c_{4}s_{5}-c_{23}c_{5},}\\\\ &{n_{z}=c_{6}\\big(c_{23}s_{5}+s_{23}c_{4}c_{5}\\big)-s_{23}s_{4}s_{6},}\\\\ &{o_{z}=-s_{6}\\big(c_{23}s_{5}+s_{23}c_{4}c_{5}\\big)-s_{23}s_{4}c_{6}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then, we define the Tait-Bryan angles through: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{z=\\arctan\\frac{a_{y}}{a_{x}}\\ }}\\\\ {{\\hat{y}=\\arctan\\frac{\\sqrt{1-a_{z}^{2}}}{a_{z}}}}\\\\ {{\\hat{z}=\\arctan\\left(-\\frac{o_{z}}{n_{z}}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Next, we calculate the positional parts of the kinematics by starting with the $x$ -component: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x_{1}=185(s_{5}(c_{1}c_{23}c_{4}+s_{1}s_{4})+c_{1}s_{23}c_{5}),}\\\\ &{x_{2}=c_{1}(175+890c_{2}+50c_{23}+1035s_{23}),}\\\\ &{p_{x}=x_{1}+x_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We continue with the $y$ -component: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{y_{1}=185(s_{5}(c_{1}c_{23}c_{4}+c_{1}s_{4})+s_{1}s_{23}c_{5}),}\\\\ &{y_{2}=s_{1}(175+890c_{2}+50c_{23}+1035s_{23}),}\\\\ &{p_{y}=y_{1}+y_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Finally, the $z$ -component is given through: ", "page_idx": 17}, {"type": "equation", "text": "$$\np_{z}=575+890s_{2}+50s_{23}-1035c_{23}+185(s_{23}c_{4}s_{5}-c_{23}c_{5})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The function then returns the six values $(p_{x},p_{y},p_{z},z,\\hat{y},\\hat{z})$ ", "page_idx": 17}, {"type": "text", "text": "A.4 HumanHeartDipole ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The HumanHeartDipole task is derived from the experimental electrolytic determination of the resultant dipole moment in the human heart. For the implementation, we followed [Averick et al., 1992] with $\\{x_{i}\\}=(x_{1},\\ldots,x_{8})$ such that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{1}(\\{x_{i}\\})=x_{1}+x_{2}-\\sigma_{\\mathrm{mx}}}\\\\ &{f_{2}(\\{x_{i}\\})=x_{3}+x_{4}-\\sigma_{\\mathrm{my}}}\\\\ &{f_{3}(\\{x_{i}\\})=x_{5}x_{1}+x_{6}x_{2}-x_{7}x_{3}-x_{8}x_{4}-\\sigma_{\\mathrm{A}}}\\\\ &{f_{4}(\\{x_{i}\\})=x_{7}x_{1}+x_{8}x_{2}+x_{5}x_{3}+x_{6}x_{4}-\\sigma_{\\mathrm{B}}}\\\\ &{f_{5}(\\{x_{i}\\})=x_{1}(x_{5}^{2}-x_{7}^{2})-2x_{1}x_{5}x_{7}+x_{2}(x_{6}^{2}-x_{8}^{2})-2x_{4}x_{6}x_{8}-\\sigma_{\\mathrm{C}}}\\\\ &{f_{6}(\\{x_{i}\\})=x_{3}(x_{5}^{2}-x_{7}^{2})+2x_{1}x_{5}x_{7}+x_{4}(x_{6}^{2}-x_{8}^{2})+2x_{2}x_{6}x_{8}-\\sigma_{\\mathrm{D}}}\\\\ &{f_{7}(\\{x_{i}\\})=x_{1}x_{5}(x_{5}^{2}-3x_{7}^{2})+x_{3}x_{7}(x_{7}^{2}-3x_{5}^{2})+x_{2}x_{6}(x_{6}^{2}-3x_{8}^{2})+x_{4}x_{8}(x_{8}^{2}-3x_{6}^{2})-\\sigma_{\\mathrm{E}}}\\\\ &{f_{8}(\\{x_{i}\\})=x_{3}x_{5}(x_{5}^{2}-3x_{7}^{2})-x_{1}x_{7}(x_{7}^{2}-3x_{5}^{2})+x_{4}x_{6}(x_{6}^{2}-3x_{8}^{2})-x_{2}x_{8}(x_{8}^{2}-3x_{6}^{2})-\\sigma_{\\mathrm{F}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with $\\sigma_{\\mathrm{m}\\mathrm{x}},\\sigma_{\\mathrm{m}\\mathrm{y}},\\sigma_{\\mathrm{A}},\\sigma_{\\mathrm{B}},\\sigma_{\\mathrm{C}},\\sigma_{\\mathrm{D}},\\sigma_{\\mathrm{E}},\\sigma_{\\mathrm{F}}$ being some arbitrary measured constants. ", "page_idx": 17}, {"type": "text", "text": "A.5 PropaneCombustion ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The PropaneCombustion task arises in the determination of the chemical equilibrium of the combustion of propane in air. Each unknown is related to a product concentration given in mols formed during the combustion process. We implemented the PropaneCombustion task as defined in [Averick et al., 1992] where with $\\{x_{i}\\}:=\\left(x_{1},\\bar{\\cdot}\\cdot.\\,,x_{11}\\right)$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{1}(\\zeta_{1})=x_{1}+x_{2}-3}\\\\ &{f_{2}(\\zeta_{2})=2x_{1}+x_{2}+x_{4}+x_{7}+x_{8}+x_{9}+2x_{10}-R}\\\\ &{f_{3}(\\zeta_{1})=2x_{3}+2x_{5}+x_{6}+x_{7}-8}\\\\ &{f_{4}(\\zeta_{2})=3x_{3}+3x_{6}}\\\\ &{f_{5}(\\zeta_{4})=R5\\sqrt{x_{21}}+R\\sqrt{x_{3}}}\\\\ &{f_{6}(\\zeta_{4})=R\\sqrt{x_{12}}-\\sqrt{\\zeta_{4}}x_{7}\\sqrt{\\frac{f_{7}}{x_{11}}}}\\\\ &{f_{7}(\\zeta_{4})=R\\sqrt{x_{13}}-\\sqrt{x_{4}x_{7}}\\sqrt{\\frac{f_{7}}{x_{11}}}}\\\\ &{f_{8}(\\zeta_{1})=K x_{11}-x_{4}x_{8}\\frac{p}{x_{11}}}\\\\ &{f_{9}(\\zeta_{1})=K x_{3}x_{1}\\sqrt{x_{3}}-x_{14}x_{9}\\sqrt{\\frac{p}{x_{11}}}}\\\\ &{f_{10}(\\zeta_{1})=K x_{11}x_{1}^{2}-x_{4}x_{1}\\frac{p}{p}}\\\\ &{f_{11}(\\zeta_{1})=x_{11}-x_{10}-x_{10}-x_{1}-x_{2}-x_{3}-x_{4}-x_{3}-x_{2}-x_{1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Here, $K_{5},\\ldots,K_{10}$ are measured constants and $R=10$ is the relative amount of air and fuel, while $p=40$ is the pressure in atmospheres. ", "page_idx": 17}, {"type": "text", "text": "A.6 Black-Scholes Equation ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The BlackScholes_Jacobian task is derived from riskless portfolio management where the goal is the computation of so called second-order greeks which give insights about the price evolution of an option. The Black-Scholes partial differential equation is given through ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\frac{\\partial V}{\\partial t}}+{\\frac{1}{2}}\\sigma^{2}S^{2}{\\frac{\\partial^{2}V}{\\partial S^{2}}}=r V-r S{\\frac{\\partial V}{\\partial S}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In this equation, $\\sigma$ models the volatility of the underlying geometric Brownian motion, while $S$ describes the stock price of the underlying asset and $r$ is the risk-free interest rate. $V$ is the price of the option at time $t$ . For certain conditions described in [Black and Scholes, 1973], this equation can be solved analytically. We define ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\phi(x)=1+{\\frac{1}{\\sqrt{\\pi}}}\\int_{-\\infty}^{x}e^{-{\\frac{1}{2}}z^{2}}\\mathrm{d}z\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which is the cumulative distribution function of the Gaussian distribution (also known as the error function). Next, we define ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{d_{1}=\\frac{1}{\\sigma\\sqrt T}\\left(\\log\\left(\\frac{F}{K}\\right)+\\frac{\\sigma^{2}}{2T}\\right)}}\\\\ {\\displaystyle{d_{2}=d_{1}-\\sigma\\sqrt T}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Phi(F,K,r,\\sigma,T)=e^{-r T}\\left(F\\phi(d_{1})-K\\phi(d_{2})\\right)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $K$ is the payoff. Then, the solution to the Black-Scholes equation is given through ", "page_idx": 18}, {"type": "equation", "text": "$$\nf(S,K,r,\\sigma,T)=\\Phi(F(r,T,S),K,r,\\sigma,T)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $F(r,T,S)=e^{r T}S.$ . We are interested in the second-order derivatives with respect to $S,K,\\sigma,r$ , so we first calculate the Jacobian of equation (A.6) using reverse-mode AD with Graphax using graphax.jacve(f, order $\\mathbf{\\Psi}=\\mathbf{\\Psi}^{\\prime\\prime}r e\\nu\\mathbf{\\Psi}^{\\prime\\prime}$ ). The resulting computational graph is used to learn an optimal elimination order to compute the second-order derivatives with AD, i.e. the Hessian. ", "page_idx": 18}, {"type": "text", "text": "A.7 Multi-Layer Perceptron ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The $M L P$ task describes a simple 2-layer Perceptron with a layer norm between the first and the second hidden layer. The input-size of the network is 4 and hidden layers have size 8 while the output layer has a size of 4. The activation functions are tanh-functions and the output is processed using a softmax cross-entropy loss. For the actual runtime experiments, we scaled the sizes of the inputs, outputs and hidden layers by a factor of 16 to create a more realistic example of a MLP. ", "page_idx": 18}, {"type": "text", "text": "A.8 Transformer Encoder ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The TransformerEncoder task is inspired by recent advances in natural language processing and image classification. The network consists of two attention blocks with a single head only. The block itself consists of a softmax attention layer with residual connections followed by a layer norm and a MLP with a single hidden layer of size 4 and sigmoid linear unit activations. We stack two of these layers and process the output with a softmax cross-entropy loss function. The input embeddings have size 4 with a sequence length of 4. For the actual runtime experiments, we scaled the sizes of the inputs, outputs and hidden layers by a factor of 16 to create a realistic example of a transformer encoder. ", "page_idx": 18}, {"type": "text", "text": "A.9 Random Functions f and g ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The random functions $f$ and $g$ are randomly generated using a custom JAX interpreter which consists of a repository of elemental operations such as cos, log or $^+$ but also matrix multiplications and array reshape operations. The number of input and output variables can be specified as well as the number of intermediate vertices that will be eliminated by the vertex elimination algorithm. The random function generator then randomly samples elemental functions from the repository. The number of functions that are unary, binary or perform accumulation or reshape operations can be controlled by adjusting the respective sampling probabilities. At every step, the function is checked to guarantee that it is executable and well-defined. The function generator is part of the accompanying AlphaGrad software package. ", "page_idx": 18}, {"type": "text", "text": "B Comparison to JAX ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Figures 5 and 6 contain an in-depth analysis of the performance benefits of the combination of AlphaGrad and Graphax over default JAX forward-mode AD and reverse-mode AD. Unless stated otherwise, all runtimes were measured on an AMD EPYC 9684X $2{\\bf x}96$ -Core processor. On the RoeFlux_1d, RobotArm_6DOF, random function $g$ and BlackScholes_Jacobian tasks, our work significantly outperforms both JAX AD modes for all batch sizes, sometimes by almost an order of magnitude. For the HumanHeartDipole and PropaneCombustion tasks, AlphaGrad and Graphax outperform both JAX modes significantly but we can observe a crossover from batch size 1024 to 2048. For the RoeFlux_3d and random function $f$ tasks, we found that the combination of AlphaGrad and Graphax consistently outperforms JAX\u2019 AD modes for large batch sizes. ", "page_idx": 19}, {"type": "text", "text": "The deep learning tasks, i.e. the $M L P$ and TransformerEncoder tasks, were analyzed slightly differently. Both functions were vectorized only over their inputs and labels as is typically done in deep learning applications. Furthermore, we evaluated both networks for different scale factors where the entire network was scaled up by a constant factor and only compare to JAX reverse-mode AD since this is the default training mode for these kinds of networks. Also, both networks were evaluated on GPU as well as this is the typical hardware backend on which they are executed. For the MLP task we found that the AlphaGrad and Graphax combination outperforms JAX reverse-mode AD for large scale factors, i.e. large networks with a large batch size. Since the gain through AlphaGrad is small, the authors conjecture that the gain in performance is mainly due to the sparse implementation of the AD routines. Note that the batch size was also consistently scaled up with the other components of the network with a starting batch size of 8. ", "page_idx": 19}, {"type": "image", "img_path": "hVmi98a0ki/tmp/7bf3d8a2b4d2a91365d6b2ba6f30a7c44c3f73d211dee32337d6f1fdbea30000.jpg", "img_caption": ["Figure 5: Runtime measurements over 100 trials for the scalar tasks. Error bars are the 2.5- and 97.5-percentiles of the runtimes. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "For the TransformerEncoder task, we were only able to evaluate the runtime for a batch size of 1 since the VertexGame implementation of AlphaGrad supports only inputs with a maximum dimensionality of 2 while a typical batched transformer input has the shape (batch size, sequence length, embedding dimension). We found that the combination of AlphaGrad and Graphax performed on par with JAX\u2019 reverse-mode AD for all scale factors except 8 where we found a significant difference in performance. ", "page_idx": 20}, {"type": "image", "img_path": "hVmi98a0ki/tmp/a8894d4a5e04d88ac2c7aa367bf77a98ab7cf9073151e6faebcd5abad9622608.jpg", "img_caption": ["Figure 6: Runtime measurements over 100 trials for the vectorized tasks with different batch sizes. The MLP network sizes were scaled up with growing batch size. The transformer could only be evaluated on with a batch size of 1 due to limitations of VertexGame but the network size was scaled up as well. Both deep learning tasks were also evaluated on GPUs. Error bars are the 2.5- and 97.5-percentiles of the runtimes. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "C Sparsity Types ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The sparsity of the Jacobians associated with the edges in the computational graph representation is described with a number ranging from -10 to 10. Table 3 contains an overview of the sparsity types and an example tensor that is represented by the corresponding number. Our approach is directed only at diagonal sparsity, meaning that we mainly consider tensors that can be decomposed into products of lower-dimensional tensors and Kronecker symbols $\\delta_{i j}$ . Since we might have operations like transposing or slicing in our computational graph, which just change the shape but not the value of the edge Jacobians when eliminated, we introduce a \u201ccopy gradient\u201d sparsity type with value -1 to represent these operations. Furthermore, we make a distinction between multiplication with the unit tensor $\\delta_{i l}\\delta_{j k}$ and a constant multiple of the unit tensor $c\\delta_{i l}\\delta_{j k}$ since the latter incurs actual multiplications while the former can just be treated as a renaming of indices as demonstrated by the ", "page_idx": 20}, {"type": "text", "text": "Table 3: Different sparsity types that are required to represent all the possible tensor shapes that occur in tensor vertex elimination with vectors and matrices. ", "page_idx": 21}, {"type": "table", "img_path": "hVmi98a0ki/tmp/0e7c4ba5a2f48d433ee55663cc3ff5e1b9886f88c0bb30d6a9af42c4c8b3bdc4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "following examples with tensors $S_{k l m n}=\\delta_{k m}\\delta_{l n}$ and $U_{k l m n}=c\\delta_{k m}\\delta_{l n}$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k l}T_{i j}\\delta_{i k}\\delta_{j l}\\cdot\\delta_{k m}\\delta_{l n}=T_{i j}\\delta_{i m}\\delta_{j n},}\\\\ &{\\displaystyle\\sum_{k l}T_{i j}\\delta_{i k}\\delta_{j l}\\cdot c\\delta_{k m}\\delta_{l n}=c T_{i j}\\delta_{i m}\\delta_{j n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "In the first equation, we can just rename the indices of the tensor, but in the second equation we have to perform the product $c T_{i j}$ , which incurs $|i|\\cdot|j|$ multiplications. If two edges are multiplied with each other, we determine the resulting sparsity type of the new edge by looking up the combination in a large handcrafted table and then writing the result into the new graph representation. The new shape of the Jacobian is determined according to the rules of tensor contraction. Finally we delete the old edges from the representation as required by the vertex elimination procedure. The sparse matrix multiplication table for the computational graph representation can be found in the accompanying source code. The number of multiplications incurred by a multiplication of two edge Jacobians is computed from the sparsity types involved and the Jacobian shapes. By multiplying all values of the two Jacobian shapes with each other and masking out certain values according to the sparsity types involved, we arrive at the correct number of multiplications. For examples consider two tensors $S_{i j k l}=a_{i j}\\delta_{i k}\\delta_{j l}$ with sizes $(2,3,2,3)$ with sparsity type 6 and $T_{i j k l}=b_{i}\\delta_{i l}\\delta_{j k}$ with shape $(2,3,2,{\\bar{3}})$ and sparsity type 9. Thus, in the computational graph representation, we would have two non-zero entries $(6,2,3,2,3)$ and $(9,2,3,2,3)$ . Then their contraction is given through ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sum_{k l}S_{i j k l}T_{k l m n}=\\sum_{k l}a_{i j}\\delta_{i k}\\delta_{j l}\\cdot b_{k}\\delta_{k n}\\delta_{l m}=a_{i j}b_{n}\\delta_{i n}\\delta_{j m}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then we form the product $|i|\\cdot|j|\\cdot|k|\\cdot|l|\\cdot|m|\\cdot|n|=2\\cdot3\\cdot2\\cdot3\\cdot2\\cdot3$ where $\\big|\\cdot\\big|$ gives the size of the dimension associated with the index. Then we use the sparsity types 7 and 9 to mask out the relevant values. In this case, we mask out $|k|,|l|$ and $|m|$ such that we arrive at $|i|\\cdot|j|\\cdot|n|=2\\cdot3\\cdot3=18$ multiplications which is exactly the number of multiplications incurred by the dense multiplication of $a_{i j}b_{n}$ . ", "page_idx": 21}, {"type": "text", "text": "D RL Algorithm and Network Architecture ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We solved VertexGame with PPO [Schulman et al., 2017] and AlphaZero [Schrittwieser et al., 2019] and used a similar architecture backbone in both cases: ", "page_idx": 22}, {"type": "text", "text": "\u2022 Convolutional embedding to compress the computational graph representation down using a 3x5 kernel mapped across the sequence dimension, i.e. the 3x5 filter was applied to all tokens to reduce the feature dimension. Since the computational graph representation is very sparse as each vertex typically only has a few connections to other vertices.   \n\u2022 Linear projection to an embedding size of 64.   \n\u2022 Positional encoding as described in [Vaswani et al., 2017].   \n\u2022 Multiple transformer layers with embedding size 64, softmax-attention, layer norm and two MLP layers with a hidden layer size of 256. ", "page_idx": 22}, {"type": "text", "text": "In the PPO case, we used two such backbones of with 5 and 4 transformer layers for separate policy and value networks. The policy head was an MLP of with hidden layers of sizes (256, 128) and output size 1 while the value head used an MLP with hidden layers of sizes (256, 128, 64) again with output size 1. The AlphaZero agent used the same heads on a single transformer backbone with 6 transformer layers. In all cases, the MLPs are mapped over the sequence dimension. For the policy head, this produces an unnormalized probability distribution over the actions while for the value head, we first sum all outputs to get an estimate of the input state value. ", "page_idx": 22}, {"type": "text", "text": "PPO Implementation Details We tightly followed [Huang et al., 2022] for the implementation of the PPO agent. Unless otherwise specified, all models were trained on 32 parallel environments with 4 minibatches and a clipping parameter of 0.2. We set the value and entropy weights to 1.0 and 0.01 respectively, while the learning rate was fixed to $2.5\\cdot10^{-4}$ . However, we found that the agent performed best if the rollout length was set to be equal to the number of intermediate vertices, although this is technically an implementation error. ", "page_idx": 22}, {"type": "text", "text": "AlphaZero Implementation Details The implementation is loosely based on the implementation of Gumbel MuZero and uses the mctx package provided by Google DeepMind [DeepMind et al., 2020]. However, we modified the implementation by replacing the trainable model and reward functions with our deterministic implementation of the VertexGame environment, thus effectively creating a Gumbel AlphaZero agent. Unless otherwise specified, all models were trained on 32 parallel environments with batch size 4096 and 50 MCTS simulations with 5 considered actions. We set the value and L2 weights to 10.0 and 0.01 respectively, while the learning rate was set to $1\\cdot10^{-3}$ with a cosine learning rate annealing over 5000 episodes. For Gumbel MuZero to work properly, it is necessary to rescale the rewards so that they lie in the interval [0,1). In the Gumbel MuZero paper, the authors designed a specific transformation that normalizes the rewards and simultaneously completes the missing values using the Gumbel distribution. The parameters of this transformation have to be carefully tuned to enable learning. In our case, we set the corresponding parameters to $c_{\\mathrm{visit}}=25$ and $c_{\\mathrm{scale}}=0.01$ for all cases. We trained the agent using adaptive momentum gradient-based learning [Kingma and Ba, 2014] with an initial learning rate of $10^{-3}$ and cosine learning rate scheduling over 5000 episodes on two to four NVIDIA RTX 4090 GPUs. ", "page_idx": 22}, {"type": "text", "text": "E AlphaZero Results ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "This section contains the reward curves and elimination orders for the results presented in tables 1 and 2. Reward curves are shown for six different random seeds, namely 42, 123, 541, 1337, 1743 and 250197. The elimination orders can be directly used with the accompanying Graphax package using the graphax.jacve command that creates the Jacobian for a given function $f$ and a given order through graphax.jacve(f, order $=$ order, argnums $=$ argnums) $({}^{*}\\!x s)$ . The orders given in tables 4 and 5 can be directly used to compute the Jacobians. ", "page_idx": 22}, {"type": "text", "text": "In addition to the single-graph experiments where the agent was trained only on a single task, we also experimented with training the agent on all tasks at once. For this, we randomly sampled from the 10 defined tasks to create 32 random environments. The agent was trained with the same configuration as for the single task experiments. The results are shown in figures 9 and 10 and the best achieved numbers of multiplication are displayed in table 6. ", "page_idx": 22}, {"type": "image", "img_path": "hVmi98a0ki/tmp/7a2ab712860208c6264fd43226d5c27203d2939e7c77527117bc91f7e79cca4c.jpg", "img_caption": ["Figure 7: Learning curves of the scalar tasks for the same six random seeds with the AlphaZero-based agent. AlphaGrad manages to find better elimination orders for all tasks. The best result for each task is displayed in table 1. ", "(e) Random function g ", "(f) BlackScholes_Jacobian "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "hVmi98a0ki/tmp/a558c4dc3818d75315223dcfb15f086ce63f46e98512b9cfd895fc308c364fb4.jpg", "img_caption": ["Figure 8: Learning curves of the vectorized tasks for the same six random seeds for the AlphaZerobased agent. AlphaGrad manages to find better elimination orders for all tasks. The best result for each task is displayed in table 1. ", "(c) Multi-Layer Perceptron ", "(d) Transformer Encoder "], "img_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "hVmi98a0ki/tmp/a97ad060a84710e8394c2b487b92715068eaf6fa6a55c906460f61fa8da24c3f.jpg", "table_caption": ["Table 4: Elimination orders for the results obtained with AlphaGrad presented in table 1 "], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "hVmi98a0ki/tmp/3f8c72719094aac333e015b8fd59196527cdc0d354f8eb56b8720fa1e5ac001d.jpg", "table_caption": ["Table 5: Elimination orders for the results obtained with AlphaGrad presented in table 1 (ctd.). "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "Table 6: Best number of multiplications achieved in joint training mode with the AlphaZero-based agent. ", "page_idx": 27}, {"type": "table", "img_path": "hVmi98a0ki/tmp/6e9a2bcacee97b8fbd344f7ba65ec6dd352378df90745ecef38e19882a17da1d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "hVmi98a0ki/tmp/794df25d3b4ef1cf0263e2abdf9bf21b9ad7820d3e1090e2d38952fee38fa471.jpg", "img_caption": ["Figure 9: Learning curves of the scalar tasks in joint training mode for the same three random seeds with the AlphaZero-based agent. The best result for each task is displayed in table 6. ", "(e) Random function g ", "(f) BlackScholes_Jacobian "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "hVmi98a0ki/tmp/c02788ab7f5e4d55c4568e77e53d9f7e897c283420a46871dad280e3b7af080e.jpg", "img_caption": ["Figure 10: Learning curves of the vectorized tasks in joint training mode for the same three random seeds for the AlphaZero-based agent. The best result for each task is displayed in table 6. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "F PPO Results ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We ran all experiments with the configuration described in D and used the random seeds from the AlphaZero experiments. The PPO agent also manages to find better elimination orders that improve over the state-of-the-art but is outperformed by the AlphaZero agent on all tasks, sometimes by a significant margin as for example in the RoeFlux tasks and random function $f$ or the $M L P$ and TransformerEncoder where it does not find a better elimination order at all. This is to be expected since the AlphaZero agent can make use of the available model and thus select actions through planning. In other cases, the performance comes very close to the AlphaZero agent, for example in the BlackScholes_Jacobian or random function $f$ tasks. Thus, the PPO-based agent might still be a viable choice because it is trained within minutes on a single NVIDIA RTX 4090 GPU, even for large tasks such as the random function $f$ and still find well-performing elimination orders. Table 7 shows the number of multiplications required by the best elimination order found by the PPO-agent. Figures 11 and 12 contain the corresponding reward curves. We did not succeed in training a joint model using the PPO agent. ", "page_idx": 28}, {"type": "table", "img_path": "hVmi98a0ki/tmp/2d4c27aaa53ec3ec3b203a62b9444f14cc3f6bc8f2acb5bedd010f12b08a6105.jpg", "table_caption": ["Table 7: Best number of multiplications achieved by the PPO-based agent. "], "table_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "hVmi98a0ki/tmp/39f51ca7d30815ffc34692213d11a9e56751a9eff102446f25f363f411f32a01.jpg", "img_caption": ["(e) Random function g ", "(f) BlackScholes_Jacobian "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "Figure 11: Learning curves of the scalar tasks for the same six random seeds with the PPO-based agent. It manages to find better elimination orders for many of the tasks. The best result for each task is displayed in table 7. ", "page_idx": 29}, {"type": "image", "img_path": "hVmi98a0ki/tmp/a08a27fefd7d9d7f919b470f47e10f36682a023333ec4e54bfbd0cee9d8f70f1.jpg", "img_caption": ["Figure 12: Learning curves of the vectorized tasks for the same six random seeds for the PPO-based agent. It manages to find better elimination orders for all tasks. The best result for each task is displayed in table 7. ", "(c) Multi-Layer Perceptron ", "(d) Transformer Encoder "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The claims contained in the abstract reflect exactly the content of the work. In the abstract, we make statements about theoretical and practical improvements in automatic differentiation. In the experiments section we provide empirical proof for both. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: The paper clearly describes the limitations of the approach by including negative outcomes in the the experimental results table and provides a more in-depth analysis in the discussion and appendix sections of the paper. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 31}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The entire codebase as well as a comprehensive tutorial will be provided with the paper as part of a GitHub repository. Furthermore, the appendix includes implementation details on the RL algorithms and neural networks as well as an in-depth description of the functions that were analyzed and all the best performing elimination orders. Furthermore, the reviewers will be provided with full access to the entire codebase on submission. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. ", "page_idx": 32}, {"type": "text", "text": "In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 33}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The entire codebase along with a comprehensive introduction will be published on release of the work. Furthermore, the reviewers will be provided with full access to the entire codebase on submission. Also, all novel elimination orders are provided in the appendix. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 33}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: All necessary experimental parameters like learning rates, RL configuration etc. are provided in the appendix and as .yaml files in the source code of the model. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 33}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The key results of the paper are runtime measurements that do not necessarily follow a Gaussian normal distribution. The authors therefore decided to display results using the median and 2.5- and 97.5-percentiles to quantify the uncertainty. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Details about the hardware requirements are included in the appendix of the paper. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 34}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] . ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: The authors have read the NeurIPS ethics code and are sure that the research conducted in this paper as well as the results of the paper did not violate any of the points raised in the guidelines. The main result of the paper is a way to find faster and more energy-efficient automatic differentiation algorithms which to the authors best knowledge does not pose any harm to humanity and nature. Furthermore, all libraries used in this work are publicly available under very lenient licensing. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 34}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: The main result of the paper is a way to find faster and more energy-efficient automatic differentiation algorithms which most likely has a positive impact on the environment as it might reduce the energy consumption of many computational workloads. The authors are not aware of any possible negative influences. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 35}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 35}, {"type": "text", "text": "Justification: ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 35}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: All the external libraries used in this work have been properly cited. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Two new libraries will be released together with this work. Both libaries are well documented and the author of the paper is also the sole author of the libraries. The source code is well documented and the package usage and installation guidelines are provided. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 36}, {"type": "text", "text": "Justification: ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 36}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] . Justification: Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 37}]