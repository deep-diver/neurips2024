[{"figure_path": "hVmi98a0ki/figures/figures_1_1.jpg", "caption": "Figure 1: Summary of the AlphaGrad pipeline. We trained a neural network to produce new Automatic Differentiation (AD) algorithms using Deep RL that can be used in JAX. The resulting algorithms significantly outperform the current state of the art.", "description": "This figure illustrates the AlphaGrad pipeline, which uses deep reinforcement learning to discover new automatic differentiation (AD) algorithms.  It starts with a function defined in Python and JAX, which is then converted into a computational graph representation and an adjacency matrix. This representation serves as input to the AlphaZero-based reinforcement learning agent, which selects actions (eliminating vertices in the graph) to minimize the computational cost. The resulting optimal elimination order, along with the original Python code, is then fed into the Graphax interpreter (a JAX-based AD package), producing an optimal AD algorithm in the form of a JAX \"jaxpr\" for efficient execution. This workflow leads to AD algorithms that show significant improvements over existing state-of-the-art methods.", "section": "1 Introduction"}, {"figure_path": "hVmi98a0ki/figures/figures_3_1.jpg", "caption": "Figure 2: Step-by-step description of cross-country elimination with the simple example function f(x1,x2) = (log sin(x1x2), x1x2 sin(x1x2))T. (a) Initial computational graph. (b) The partial derivatives are added to the edges of the computational graph. The intermediate variables v1 and v2 are defined through v\u2081 = x1x2 and v2 = sin v\u2081. (c) Elimination of vertex 2 associated with the sin operation. The dotted red lines represent the edges that are deleted. (d) Final bipartite graph after both intermediate vertices have been eliminated. All remaining edges contain entries of the Jacobian.", "description": "This figure demonstrates the step-by-step process of cross-country elimination using a simple example function. It shows how partial derivatives are added to the edges, how vertices are eliminated (using the chain rule), and how the final bipartite graph contains the Jacobian entries.", "section": "2.1 Graph View and Vertex Elimination"}, {"figure_path": "hVmi98a0ki/figures/figures_3_2.jpg", "caption": "Figure 2: Step-by-step description of cross-country elimination with the simple example function f(x1,x2) = (log sin(x1x2), x1x2 sin(x1x2))T. (a) Initial computational graph. (b) The partial derivatives are added to the edges of the computational graph. The intermediate variables v1 and v2 are defined through v\u2081 = x1x2 and v2 = sin v\u2081. (c) Elimination of vertex 2 associated with the sin operation. The dotted red lines represent the edges that are deleted. (d) Final bipartite graph after both intermediate vertices have been eliminated. All remaining edges contain entries of the Jacobian.", "description": "This figure shows a step-by-step illustration of the cross-country elimination algorithm applied to a simple example function. It demonstrates how partial derivatives are added to the edges of the computational graph (b), a vertex is eliminated (c), and the process continues until a final bipartite graph is obtained (d), where the edges represent the Jacobian.", "section": "2.1 Graph View and Vertex Elimination"}, {"figure_path": "hVmi98a0ki/figures/figures_5_1.jpg", "caption": "Figure 3: (a) Graphax implements sparse vertex elimination to benefit from the advantages of cross country elimination. (b) Sketch of the three-dimensional adjacency tensor that represents the computational graph. The colored surfaces represent the five different values encoded in the third dimension. The red and blue surfaces together contain the shape of the Jacobians while the green surface encodes their sparsity. The vertical dotted slices represent the input connectivity of a single vertex. In this work, we compress and feed the vertical slices as tokens into the transformer backbone such that we build a sequence running in direction of the black arrow.", "description": "This figure shows how Graphax implements sparse vertex elimination (a) and the three-dimensional adjacency tensor used to represent the computational graph (b).  The tensor encodes the shape and sparsity of Jacobians, and its vertical slices are fed into a transformer network.", "section": "Computational Graph Representation and Network Architecture"}, {"figure_path": "hVmi98a0ki/figures/figures_5_2.jpg", "caption": "Figure 3: (a) Graphax implements sparse vertex elimination to benefit from the advantages of cross country elimination. (b) Sketch of the three-dimensional adjacency tensor that represents the computational graph. The colored surfaces represent the five different values encoded in the third dimension. The red and blue surfaces together contain the shape of the Jacobians while the green surface encodes their sparsity. The vertical dotted slices represent the input connectivity of a single vertex. In this work, we compress and feed the vertical slices as tokens into the transformer backbone such that we build a sequence running in direction of the black arrow.", "description": "This figure shows how Graphax (a novel sparse AD package) benefits from sparse vertex elimination in cross-country elimination.  Panel (a) illustrates the sparse elimination process; (b) displays the 3D adjacency tensor used to represent the computational graph. This tensor encodes 5 aspects of each edge: input/output shape and sparsity type of the associated Jacobian.  These vertical slices, representing a single vertex's input connectivity, are compressed and input to the transformer network as tokens, creating a sequence processed by the model.", "section": "2.5 Computational Graph Representation and Network Architecture"}, {"figure_path": "hVmi98a0ki/figures/figures_9_1.jpg", "caption": "Figure 3: (a) Graphax implements sparse vertex elimination to benefit from the advantages of cross country elimination. (b) Sketch of the three-dimensional adjacency tensor that represents the computational graph. The colored surfaces represent the five different values encoded in the third dimension. The red and blue surfaces together contain the shape of the Jacobians while the green surface encodes their sparsity. The vertical dotted slices represent the input connectivity of a single vertex. In this work, we compress and feed the vertical slices as tokens into the transformer backbone such that we build a sequence running in direction of the black arrow.", "description": "This figure shows how Graphax leverages sparse vertex elimination for efficiency and the three-dimensional adjacency tensor used to represent the computational graph for the reinforcement learning model.  The tensor encodes information about the shape and sparsity of Jacobians, enabling efficient processing by the transformer network.", "section": "2.5 Computational Graph Representation and Network Architecture"}, {"figure_path": "hVmi98a0ki/figures/figures_19_1.jpg", "caption": "Figure 3: (a) Graphax implements sparse vertex elimination to benefit from the advantages of cross country elimination. (b) Sketch of the three-dimensional adjacency tensor that represents the computational graph. The colored surfaces represent the five different values encoded in the third dimension. The red and blue surfaces together contain the shape of the Jacobians while the green surface encodes their sparsity. The vertical dotted slices represent the input connectivity of a single vertex. In this work, we compress and feed the vertical slices as tokens into the transformer backbone such that we build a sequence running in direction of the black arrow.", "description": "This figure shows how Graphax leverages sparse matrix multiplication for efficient cross-country elimination.  Panel (a) illustrates the concept of sparse vertex elimination, highlighting its efficiency compared to dense methods.  Panel (b) details the 3D tensor representation of the computational graph used by the reinforcement learning agent, where each dimension encodes information about the graph structure (adjacency), Jacobian shape, and sparsity. The visualization helps explain how the graph is processed by the transformer network in the AlphaGrad pipeline.", "section": "Computational Graph Representation and Network Architecture"}, {"figure_path": "hVmi98a0ki/figures/figures_20_1.jpg", "caption": "Figure 3: (a) Graphax implements sparse vertex elimination to benefit from the advantages of cross country elimination. (b) Sketch of the three-dimensional adjacency tensor that represents the computational graph. The colored surfaces represent the five different values encoded in the third dimension. The red and blue surfaces together contain the shape of the Jacobians while the green surface encodes their sparsity. The vertical dotted slices represent the input connectivity of a single vertex. In this work, we compress and feed the vertical slices as tokens into the transformer backbone such that we build a sequence running in direction of the black arrow.", "description": "This figure shows how Graphax leverages sparse vertex elimination for efficiency and illustrates the three-dimensional adjacency tensor used to represent the computational graph in the reinforcement learning model.  The tensor encodes information about the shape and sparsity of Jacobians, which are then processed by a transformer network.", "section": "2.5 Computational Graph Representation and Network Architecture"}, {"figure_path": "hVmi98a0ki/figures/figures_23_1.jpg", "caption": "Figure 5: Runtime measurements over 100 trials for the scalar tasks. Error bars are the 2.5- and 97.5-percentiles of the runtimes.", "description": "This figure presents the runtime measurements obtained for six different scalar tasks across various batch sizes.  The performance of AlphaGrad and Graphax is compared against JAX's forward and reverse-mode AD.  Error bars represent the 2.5th and 97.5th percentiles of the runtime across 100 trials.  The results illustrate the efficiency gains achieved by AlphaGrad and Graphax, particularly at larger batch sizes.", "section": "B Comparison to JAX"}, {"figure_path": "hVmi98a0ki/figures/figures_24_1.jpg", "caption": "Figure 3: (a) Graphax implements sparse vertex elimination to benefit from the advantages of cross country elimination. (b) Sketch of the three-dimensional adjacency tensor that represents the computational graph. The colored surfaces represent the five different values encoded in the third dimension. The red and blue surfaces together contain the shape of the Jacobians while the green surface encodes their sparsity. The vertical dotted slices represent the input connectivity of a single vertex. In this work, we compress and feed the vertical slices as tokens into the transformer backbone such that we build a sequence running in direction of the black arrow.", "description": "This figure shows how Graphax leverages sparse vertex elimination for efficiency and illustrates the three-dimensional adjacency tensor used to represent the computational graph in the AlphaGrad algorithm.  The tensor encodes information about the shape and sparsity of Jacobians associated with each edge in the graph, allowing the algorithm to optimize the elimination order effectively.", "section": "Computational Graph Representation and Network Architecture"}, {"figure_path": "hVmi98a0ki/figures/figures_27_1.jpg", "caption": "Figure 3: (a) Graphax implements sparse vertex elimination to benefit from the advantages of cross country elimination. (b) Sketch of the three-dimensional adjacency tensor that represents the computational graph. The colored surfaces represent the five different values encoded in the third dimension. The red and blue surfaces together contain the shape of the Jacobians while the green surface encodes their sparsity. The vertical dotted slices represent the input connectivity of a single vertex. In this work, we compress and feed the vertical slices as tokens into the transformer backbone such that we build a sequence running in direction of the black arrow.", "description": "This figure shows how Graphax leverages sparse vertex elimination for efficiency and illustrates the 3D adjacency tensor used to represent the computational graph in the AlphaGrad RL system.  The tensor encodes the shapes and sparsity of Jacobians, which are fed as tokens into a transformer network.", "section": "2.5 Computational Graph Representation and Network Architecture"}, {"figure_path": "hVmi98a0ki/figures/figures_28_1.jpg", "caption": "Figure 3: (a) Graphax implements sparse vertex elimination to benefit from the advantages of cross country elimination. (b) Sketch of the three-dimensional adjacency tensor that represents the computational graph. The colored surfaces represent the five different values encoded in the third dimension. The red and blue surfaces together contain the shape of the Jacobians while the green surface encodes their sparsity. The vertical dotted slices represent the input connectivity of a single vertex. In this work, we compress and feed the vertical slices as tokens into the transformer backbone such that we build a sequence running in direction of the black arrow.", "description": "This figure shows how Graphax leverages sparse matrix multiplications for efficient computation of Jacobians using cross-country elimination.  Panel (a) illustrates the concept of sparse vertex elimination in Graphax, contrasting it with the standard method. Panel (b) details the three-dimensional tensor representation used to encode the computational graph's structure, Jacobian shapes, and sparsity information.  Each dimension within this tensor holds specific information about the graph and facilitates efficient processing by the deep reinforcement learning model.", "section": "Computational Graph Representation and Network Architecture"}, {"figure_path": "hVmi98a0ki/figures/figures_29_1.jpg", "caption": "Figure 3: (a) Graphax implements sparse vertex elimination to benefit from the advantages of cross country elimination. (b) Sketch of the three-dimensional adjacency tensor that represents the computational graph. The colored surfaces represent the five different values encoded in the third dimension. The red and blue surfaces together contain the shape of the Jacobians while the green surface encodes their sparsity. The vertical dotted slices represent the input connectivity of a single vertex. In this work, we compress and feed the vertical slices as tokens into the transformer backbone such that we build a sequence running in direction of the black arrow.", "description": "This figure shows how Graphax, a novel sparse AD package, implements sparse vertex elimination.  Panel (a) illustrates the process by highlighting the advantages of element-wise multiplication instead of matrix multiplication.  Panel (b) details the three-dimensional adjacency tensor used to represent the computational graph. The tensor encodes the shape and sparsity of the Jacobians, with each vertical slice representing the input connectivity of a single vertex. These slices are compressed into tokens that are fed into a transformer for processing.", "section": "Computational Graph Representation and Network Architecture"}, {"figure_path": "hVmi98a0ki/figures/figures_30_1.jpg", "caption": "Figure 3: (a) Graphax implements sparse vertex elimination to benefit from the advantages of cross country elimination. (b) Sketch of the three-dimensional adjacency tensor that represents the computational graph. The colored surfaces represent the five different values encoded in the third dimension. The red and blue surfaces together contain the shape of the Jacobians while the green surface encodes their sparsity. The vertical dotted slices represent the input connectivity of a single vertex. In this work, we compress and feed the vertical slices as tokens into the transformer backbone such that we build a sequence running in direction of the black arrow.", "description": "This figure shows how Graphax implements sparse vertex elimination and the three-dimensional adjacency tensor used to represent the computational graph in the RL algorithm.  The adjacency tensor encodes information about the shapes and sparsity of the Jacobians associated with each edge in the graph. These are compressed and fed into a transformer for processing.", "section": "Computational Graph Representation and Network Architecture"}]