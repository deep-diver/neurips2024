[{"figure_path": "QpKWFLtZKi/tables/tables_2_1.jpg", "caption": "Table 1: Comparison of exploration methods using different measures (marked in blue) of state discrepancy as exploration bonus.  \u03a6 = f(s) : features encoder; Nep(s) : episodic (pseudo) count of visits to state s; rRND(s): the RND [11] bonus; \u03b1: normalized coefficient; dinv: inverse dynamic bisimulation metric used in [63]; so: initial state; \u03bb: the scaling hyper-parameter; de: our proposed effective metric for state discrepancy evaluation; (rs): variance of predictions from an ensemble of reward models: M: maximum reward scaling.", "description": "This table compares four different exploration methods, including the proposed EME method.  It highlights the objective function of each method, showing how state discrepancy and a scaling factor are used to calculate an exploration bonus.  The table also notes whether each method uses episodic counts, has an approximation gap, and has good scalability.", "section": "3 Limitations of Existing Metric-based Exploration Bonus"}, {"figure_path": "QpKWFLtZKi/tables/tables_2_2.jpg", "caption": "Table 2: Mean success rate comparison (values below 0.5 are marked in red)", "description": "This table compares the mean success rates of several exploration methods (RIDE, RIDE without episodic term, NovelD, NovelD without episodic term, LIBERTY) across three different environments (Robosuite, MiniGrid, Habitat).  The results highlight the significant impact of the episodic count term on the performance of RIDE and NovelD, especially in complex environments. LIBERTY shows comparatively better performance in most of the cases.", "section": "3 Limitations of Existing Metric-based Exploration Bonus"}, {"figure_path": "QpKWFLtZKi/tables/tables_7_1.jpg", "caption": "Table 3: Average testing results of different Minigrid environments for EME and other baselines.", "description": "This table presents the average testing results of various MiniGrid environments for the proposed Effective Metric-based Exploration Bonus (EME) method and several other baseline methods.  The environments are categorized into different types (Multi-Room, Key Corridor, and Obstructed Maze), each with variations in size and complexity. For each environment, the mean success rate (with standard deviation) is shown for each method. The results illustrate the performance of EME compared to other approaches on these challenging exploration tasks.", "section": "5.2 Discrete-action Games"}, {"figure_path": "QpKWFLtZKi/tables/tables_20_1.jpg", "caption": "Table 3: Average testing results of different Minigrid environments for EME and other baselines.", "description": "This table presents the average testing results for various MiniGrid environments.  The results compare the performance of the proposed Effective Metric-based Exploration bonus (EME) method against several other baseline methods (ICM, RND, RIDE, NovelD, E3B, LIBERTY). Each environment is tested across five different seeds, providing a measure of the robustness of each method's performance. The table shows the mean success rate and standard deviation for each method in each environment.  The MiniGrid environments vary in complexity and challenge the agent's exploration capabilities.", "section": "5.2 Discrete-action Games"}, {"figure_path": "QpKWFLtZKi/tables/tables_21_1.jpg", "caption": "Table 3: Average testing results of different Minigrid environments for EME and other baselines.", "description": "This table shows the average testing results of different MiniGrid environments for the proposed EME method and other baseline methods.  The results are presented as mean \u00b1 standard deviation for various MiniGrid environments, categorized by the type of maze (Multi-Room, Key Corridor, Obstructed Maze) and the presence or absence of added noise (e.g., -NT for Noisy TV).  Each environment represents a different challenge in terms of exploration difficulty. The table highlights the relative performance of EME compared to other methods like ICM, RND, RIDE, NovelD, E3B and LIBERTY across various exploration scenarios. The results demonstrate EME's superior performance across many of the tested environments, indicating the effectiveness of the proposed method in hard exploration tasks.", "section": "5.2 Discrete-action Games"}, {"figure_path": "QpKWFLtZKi/tables/tables_24_1.jpg", "caption": "Table 1: Comparison of exploration methods using different measures (marked in blue) of state discrepancy as exploration bonus.  \u03a6 = f(s) : features encoder; Nep(s) : episodic (pseudo) count of visits to state s; rRND(s): the RND [11] bonus; a: normalized coefficient; dinv: inverse dynamic bisimulation metric used in [63]; s0: initial state; \u03bb: the scaling hyper-parameter; dE: our proposed effective metric for state discrepancy evaluation; (rs): variance of predictions from an ensemble of reward models: M: maximum reward scaling.", "description": "This table compares four different exploration methods: RIDE, NovelD, LIBERTY, and the proposed EME method.  It highlights the key differences in their approaches to evaluating state discrepancy (the core of their exploration bonus) and their scalability.  Specifically, it shows how RIDE and NovelD rely on simple Lp norms and episodic visit counts, while LIBERTY uses the more complex bisimulation metric (which often requires approximation).  The table emphasizes that the EME method addresses the limitations of prior approaches by proposing a more robust and theoretically grounded metric, along with a novel diversity-enhanced scaling factor.", "section": "3 Limitations of Existing Metric-based Exploration Bonus"}, {"figure_path": "QpKWFLtZKi/tables/tables_25_1.jpg", "caption": "Table 6: Hyper-parameters in Robosuite experiments", "description": "This table lists the hyperparameters used in the Robosuite experiments.  It includes settings for horizon steps, control frequency, state and action dimensions, controller types, maximum reward scaling, maximum training steps, actor and critic learning rates, batch size, discount factor, soft update weight, alpha learning rate, hidden layer sizes, returns estimation steps, buffer size, and steps per update.  Different parameters are specified for each of the three Robosuite environments: Pick and Place, Door Opening, and Table Wiping.", "section": "D.1 Robosuite Manipulation Tasks"}, {"figure_path": "QpKWFLtZKi/tables/tables_25_2.jpg", "caption": "Table 1: Comparison of exploration methods using different measures (marked in blue) of state discrepancy as exploration bonus.  \u03a6 = f(s) : features encoder; Nep(s) : episodic (pseudo) count of visits to state s; rRND(s): the RND [11] bonus; \u03b1: normalized coefficient; dinv: inverse dynamic bisimulation metric used in [63]; s0: initial state; \u03bb: the scaling hyper-parameter; de: our proposed effective metric for state discrepancy evaluation; (rs): variance of predictions from an ensemble of reward models: M: maximum reward scaling.", "description": "This table compares different existing metric-based exploration methods (RIDE, NovelD, LIBERTY) with the proposed EME method. It highlights the key differences in their approaches to state discrepancy evaluation, scaling factor, episodic approximation gap and scalability. The table clearly shows how EME addresses the limitations of existing methods, particularly focusing on its resilient metric and diversity-enhanced scaling factor.", "section": "3 Limitations of Existing Metric-based Exploration Bonus"}, {"figure_path": "QpKWFLtZKi/tables/tables_25_3.jpg", "caption": "Table 1: Comparison of exploration methods using different measures (marked in blue) of state discrepancy as exploration bonus.  \u03a6 = f(s) : features encoder; Nep(s) : episodic (pseudo) count of visits to state s; rRND(s): the RND [11] bonus; a: normalized coefficient; dinv: inverse dynamic bisimulation metric used in [63]; s0: initial state; \u03bb: the scaling hyper-parameter; de: our proposed effective metric for state discrepancy evaluation; (rs): variance of predictions from an ensemble of reward models: M: maximum reward scaling.", "description": "This table compares several state-of-the-art exploration methods that leverage state discrepancy measures. It highlights the differences in how they quantify state discrepancy, the scaling factors used, and their approximation gaps and scalability. The table also introduces the proposed EME method and its components for comparison.", "section": "3 Limitations of Existing Metric-based Exploration Bonus"}, {"figure_path": "QpKWFLtZKi/tables/tables_26_1.jpg", "caption": "Table 1: Comparison of exploration methods using different measures (marked in blue) of state discrepancy as exploration bonus. $ = f(s) : features encoder; Nep(s) : episodic (pseudo) count of visits to state s; rRND(s): the RND [11] bonus; a: normalized coefficient; dinv: inverse dynamic bisimulation metric used in [63]; so: initial state; \u03bb: the scaling hyper-parameter; de: our proposed effective metric for state discrepancy evaluation; (rs): variance of predictions from an ensemble of reward models: M: maximum reward scaling.", "description": "This table compares several existing metric-based exploration methods with the proposed EME method.  It highlights the key differences in how they calculate state discrepancy (the core novelty measure for exploration), the scaling factors used to adjust the exploration bonus, and the potential limitations of each approach regarding approximation gaps and scalability. The table also lists the objective function for each method to show the different ways state discrepancy and scaling factor are combined to incentivize exploration.", "section": "3 Limitations of Existing Metric-based Exploration Bonus"}]