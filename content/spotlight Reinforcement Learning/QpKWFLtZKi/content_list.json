[{"type": "text", "text": "Rethinking Exploration in Reinforcement Learning with Effective Metric-Based Exploration Bonus ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yiming Wang1, Kaiyan Zhao1,2, Furui Liu3, Leong Hou U1\u2217 ", "page_idx": 0}, {"type": "text", "text": "1State Key Laboratory of Internet of Things for Smart City, University of Macau, Macao SAR, China 2School of Computer Science, Wuhan University, Wuhan, China 3Zhejiang Lab, Hangzhou, China wang.yiming@connect.um.edu.mo,zhao.kaiyan@whu.edu.cn liufurui@zhejianglab.com,ryanlhu@um.edu.mo ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Enhancing exploration in reinforcement learning (RL) through the incorporation of intrinsic rewards, specifically by leveraging state discrepancy measures within various metric spaces as exploration bonuses, has emerged as a prevalent strategy to encourage agents to visit novel states. The critical factor lies in how to quantify the difference between adjacent states as novelty for promoting effective exploration. Nonetheless, existing methods that evaluate state discrepancy in the latent space under $L_{1}$ or $L_{2}$ norm often depend on count-based episodic terms as scaling factors for exploration bonuses, significantly limiting their scalability. Additionally, methods that utilize the bisimulation metric for evaluating state discrepancies face a theory-practice gap due to improper approximations in metric learning, particularly struggling with hard exploration tasks. To overcome these challenges, we introduce the Effective Metric-based Exploration-bonus (EME). EME critically examines and addresses the inherent limitations and approximation inaccuracies of current metric-based state discrepancy methods for exploration, proposing a robust metric for state discrepancy evaluation backed by comprehensive theoretical analysis. Furthermore, we propose the diversity-enhanced scaling factor integrated into the exploration bonus to be dynamically adjusted by the variance of prediction from an ensemble of reward models, thereby enhancing exploration effectiveness in particularly challenging scenarios. Extensive experiments are conducted on hard exploration tasks within Atari games, Minigrid, Robosuite, and Habitat, which illustrate our method\u2019s scalability to various scenarios. The project website can be found at https://sites.google.com/view/effective-metric-exploration. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Reinforcement learning (RL) has made significant strides, yielding breakthroughs across various domains, including video gaming [43], autonomous driving [65], and robotic control [1, 37]. However, for many real-world tasks, defining a dense reward function is non-trivial, yet a sparse reward function based on success or failure is directly available, which makes learning effective policies difficult, as they demand efficient exploration of the state space, highlighting exploration in sparse-reward environments as a core challenge in RL [59]. ", "page_idx": 0}, {"type": "text", "text": "Methods that quantify the state discrepancy between adjacent steps using specific measures within different metric spaces have shown remarkable success in tasks characterized by sparse rewards. These methods, by leveraging a measure of state discrepancy as an exploration bonus, facilitate agents in discovering novel states. For instance, RIDE [51] employs the difference between two consecutive state embeddings, measured under $L_{2}$ norm, as an exploration bonus. Similarly, NovelD [69] introduces a bonus based on the state discrepancy, as represented by the RND [11] bonus, but under the $L_{1}$ norm. However, the effectiveness of these $L_{p}$ norms-based methods is contingent on the scaling factor expressed as the episodic count term $\\dot{N}_{e p}(s)$ , which is the number of times state $s$ has been visited during the current episode. The episodic term becomes ineffective when each state is unique and cannot be counted, posting a significant limitation when it comes to more complex, dynamic, and noisy environments [32]. LIBERTY [63] aims to surmount this hurdle by evaluating state discrepancy through the bisimulation metric [25], which links state differences to value differences within the bisimulation metric space, thus incentivizing exploration of novel states with greater value differences and substantially enhancing learning efficiency. Nevertheless, due to the high computational cost or infeasibility of accurately computing bisimulation metrics, the approximation, and relaxations over the metric [13, 68, 63] have been used to optimize efficiency. Our analysis reveals that approximation gap in LIBERTY might break the theoretical guarantees of the bisimulation metric, potentially undermining exploration performance. Furthermore, the efficacy of bonuses based on state discrepancies declines in hard exploration tasks or scenarios where state differences are minimal, such as in the \u201cNoisy-TV\u201d [11] problem and vision-based real indoor environments [60], posing a significant constraint on the scalability of these methods. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "image", "img_path": "QpKWFLtZKi/tmp/120815be0c590748c74ff421c6d1d9364f79efe838b645a2172e1d1c9f7df33b.jpg", "img_caption": ["Figure 1: Trajectories of policies trained with different exploration algorithms in the real-life indoor environment. (a) episodic count-based method under $L_{p}$ norm (b) bisimulation metric-based method (c) our method. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "To address this aforementioned limitations, we introduce the Effective Metric-based Explorationbonus (EME) for exploration. Our method features a more resilient metric for evaluating state differences, simultaneously eliminating approximation gap with a theoretical guarantee. EME significantly boosts learning efficiency by forging a closer connection to the value differences between states, while maintaining fidelity to its theoretical framework. Furthermore, we have developed a diversity-enhanced scaling factor to augment exploration efficacy in hard exploration tasks, where state differences are notably subtle. Our scaling factor dynamically adjusts the exploration bonus based on the variance of predictions from an ensemble of reward models, which is higher when agents encounter novel state space, thereby encouraging more effective exploration. As illustrated in Figure 1, we present the trajectories of policies trained using count-based state discrepancy bonus, bisimulation-based exploration bonus, and our method within a real-life indoor environment [15] (Full results in Figure 10 of Appendix C.3). Our method successfully explores a significantly larger portion of the space compared to the others, demonstrating the superior effectiveness of EME. ", "page_idx": 1}, {"type": "text", "text": "The main contributions of this paper are as follows. Firstly, we conduct a comprehensive analysis of the limitations inherent in current metric-based state discrepancy methods for exploration. Based on the analysis, we introduce an effective metric for evaluating the behavioral similarity between states supported by theoretical assurances. Secondly, we propose a diversity-enhanced scaling factor for exploration bonuses based on the variance of predictions from an ensemble of reward models, which is scalable and effective in hard exploration tasks. Lastly, extensive experiments are conducted in Robosuite, Atari games, MiniGrid, and Habitat. The results demonstrate that our algorithm can effectively enhance exploration and accelerate training across various environments. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We focus on exploration bonuses to incentivize exploration in reinforcement learning (RL). To foster the reader\u2019s understanding, we first introduce standard notation and common practices. ", "page_idx": 1}, {"type": "text", "text": "Markov Decision Processes. We assume the underlying environment is a Markov decision process (MDP), defined by the tuple $\\mathcal{M}=(\\mathcal{S},\\mathcal{A},P,R,\\gamma)$ , where $\\boldsymbol{S}$ is the state space, $\\boldsymbol{\\mathcal{A}}$ is the action space, $P\\left(s^{\\prime}\\mid s,a\\right)$ is state transition function from state $s\\in S$ to state $s^{\\prime}\\in\\bar{S}$ , $R$ is the reward function and $\\gamma\\in\\left[0,1\\right)$ is the discount factor. Generally, the policy of an agent in an MDP is a mapping $\\pi:S\\times A\\rightarrow[0,1]$ . An agent chooses actions $a\\in{\\mathcal{A}}$ according to a policy function $a\\sim\\pi(s)$ , which updates the system state $\\overline{{s}}^{\\prime}\\sim P(s,a)$ yielding a reward $r\\,=\\,R(s,a)$ . In this paper, we denote a policy by $\\pi_{\\theta}$ , where $\\theta$ is the parameter of the policy function. The goal of the agent is to optimize the parameter $\\theta$ for maximizing the expected accumulative rewards, $\\begin{array}{r}{\\bar{J}(\\pi_{\\theta})=\\mathbb{E}_{\\pi_{\\theta}}\\,\\overline{{\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}\\dot{R}\\left(s_{t},a_{t}\\right)\\right]}}}\\end{array}$ . ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "State Discrepancy as Exploration Bonuses. Addressing the sparse reward challenge prevalent in many environments necessitates augmenting the external reward function, $r^{e}$ , with an intrinsic reward bonus, $b$ , resulting in a combined reward: $\\boldsymbol{r}=\\boldsymbol{r}^{e}+\\boldsymbol{b}$ . A number of intrinsic bonuses2 that encourage exploration have been proposed. More recent methodologies employ an exploration bonus that varies based on the novelty of the current state relative to previously visited states, evaluated within different metric spaces, which can be generally defined as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\boldsymbol{b}=\\boldsymbol{\\mathrm{SD}}*\\boldsymbol{\\mathrm{SF}}=d(\\boldsymbol{\\Phi}(\\boldsymbol{s}),\\boldsymbol{\\Phi}(\\boldsymbol{s}^{\\prime}))*\\lambda^{\\mathrm{SF}}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where SD denotes the state discrepancy, $\\mathrm{SE}$ denotes the scaling factor, $d:S\\times S\\rightarrow\\mathbb{R}$ represents the evaluation metric that quantifies the degree of difference or distance between two projected states, and $\\Phi:S\\rightarrow{\\mathcal{Z}}$ is the projection function which maps the state to a representation space $\\mathcal{Z}$ . ", "page_idx": 2}, {"type": "table", "img_path": "QpKWFLtZKi/tmp/24e106a51975f5c5a6ee091d2126ad9d093d58e21b30684c3907cc85d73255d0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "Table 1: Comparison of exploration methods using different measures (marked in blue) of state discrepancy as exploration bonus. $\\phi=f(s)$ : features encoder; $\\bar{N}_{e p}(s)$ : episodic (pseudo) count of visits to state $s$ ; $\\dot{\\overline{{r}}}^{\\mathrm{RND}}(s)$ : the RND [11] bonus; $\\alpha$ : normalized coefficient; $d_{i n v}$ : inverse dynamic bisimulation metric used in [63]; $s_{0}$ : initial state; $\\lambda$ : the scaling hyper-parameter; $d_{E}$ : our proposed effective metric for state discrepancy evaluation; $\\zeta(r_{s})$ : variance of predictions from an ensemble of reward models: $M$ : maximum reward scaling. ", "page_idx": 2}, {"type": "text", "text": "The comparison of recent metric-based exploration methods is summarized in Table 1 These bonuses reward high dissimilarity between adjacent states, encouraging exploration by leveraging measures such as the $L_{2}$ distance for state embeddings (as used by RIDE [51]) and the $L_{1}$ distance for the disparity in RND bonuses between adjacent states (as utilized by NovelD [69]). However, these approaches, grounded in $L_{p}$ norms and dependent on episodic state visit counts, face significant scalability challenges when exploring various environments. On the other hand, LIBERTY [63] proposes the inverse dynamic bisimulation metric to evaluate the state difference, which links state and value differences to enhance learning efficiency. Yet, the approximation inaccuracies inherent in bisimulation metric learning compromise theoretical assurances, breaking theoretical integrity of bisimulation metric. In the following sections, we detail the shortcomings of these methods. ", "page_idx": 2}, {"type": "text", "text": "3 Limitations of Existing Metric-based Exploration Bonus ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Latent Vector Space under $L_{p}$ Norms ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "$L_{p}$ norms such as $L_{1}$ and $L_{2}$ norms are widely used in measuring the \"distance\" between states or embeddings. For example, considering a continuous map between the states of an MDP and a latent vector space: $\\Phi:S\\rightarrow\\mathcal{Z}\\subseteq\\mathbb{R}^{n}$ . We can equip this vector space with a Euclidean norm to obtain a Euclidean metric space $\\left(\\mathcal{Z},\\|\\cdot\\|_{2}\\right)$ , and the state discrepancy is measured as ${\\mathrm{SD}}=\\|\\Phi(s)-\\Phi(s^{\\prime})\\|_{2}$ . ", "page_idx": 2}, {"type": "text", "text": "ference between adjacent states in the $L_{p}$ norm space. The efficacy of the exploration bonus heavily lies in the episodic scaling factor (SF) measured by the count of visited states $N_{e p}(s)$ presented in Table 1. However, the count-based episodic bonus suffers from a fundamental limitation, which is similar to that faced by count-based approaches [5] in general: if each state is unique, then ", "page_idx": 2}, {"type": "table", "img_path": "QpKWFLtZKi/tmp/5f322a787e9e0b25f047af57f197cfe0f20b6078c355701d857cc89128aaf1a8.jpg", "table_caption": ["Reliance on Count-based Scaling Factor. The state-difference based exploration bonus employed by RIDE [51] and Noveld [69] can be regarded as the dif", "Table 2: Mean success rate comparison (values below 0.5 are marked in red) "], "table_footnote": ["2The detailed discussion of related work can be found in Appendix F "], "page_idx": 2}, {"type": "text", "text": "$N_{e p}(s_{t})$ will always be 1 and the episodic bonus is no longer meaningful, which is the case for many real-world applications. For example, a household robot\u2019s state as recorded by its camera might include moving trees outside the window, clocks showing the time, or images on a television screen which are not relevant to its tasks, but nevertheless make each state unique. As shown in Table 2, without the episodic count term (denoted as \"w/o EP\"), the performance of RIDE and Noveld exhibits a significant decline in all different types of environments. ", "page_idx": 3}, {"type": "text", "text": "Limited Expressiveness of Novelty. As indicated in Table 2, the performance of RIDE and NovelD significantly degrades in environments with high-dimensional visual observations or continuous control tasks, where the success rate falls below 0.5. The significant decline highlights the inadequacies of utilizing state differences within vector spaces, computed using the $L_{p}$ norm, to effectively express novelty. Therefore, a more robust and effective metric is essential to accurately assess and characterize the novelty among visited states. ", "page_idx": 3}, {"type": "text", "text": "3.2 State Discrepancy within the Bisimulation Metric Space ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The bisimulation metric [25, 26] defines a pseudometric $d:S\\times S\\rightarrow\\mathbb{R}$ to measure the similarity between two states. The recently proposed variant, $\\pi$ -bisimulation metric [13], focuses on behaviors relative to a particular policy $\\pi$ , which consists of a reward difference term and a Wasserstein distance in dynamics models between states. ", "page_idx": 3}, {"type": "text", "text": "Definition 1 ( $\\pi$ -bisimulation metric). Given a fixed policy $\\pi$ , the following $\\pi$ -bisimulation metric exists and is unique: ", "page_idx": 3}, {"type": "equation", "text": "$$\nd(s_{i},s_{j})=|\\mathbb{E}_{a_{i}\\sim\\pi}r_{s_{i}}^{a_{i}}-\\mathbb{E}_{a_{j}\\sim\\pi}r_{s_{j}}^{a_{j}}|+\\gamma W_{1}(d)(P^{\\pi}(\\cdot\\mid s_{i}),P^{\\pi}(\\cdot\\mid s_{j}))\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\;_{i}{\\sim}\\pi_{.}^{r_{s i}^{a_{i}}}\\;=\\;\\mathbb{E}_{a_{i}{\\sim}\\pi(\\cdot|s_{i})}r(s_{i},a_{i}),P^{\\pi}(\\cdot\\;\\mid\\;s_{i})\\;=\\;\\mathbb{E}_{a{\\sim}\\pi(\\cdot|s_{i})}P(\\cdot\\;\\mid\\;s_{i},a)$ and $W_{1}$ is the $^{\\,l}$ Wasserstein distance. ", "page_idx": 3}, {"type": "text", "text": "Approximation Gap. LIBERTY [63] proposes exploration bonuses based on state discrepancy within the inverse dynamic bisimulation metric space, which adds the difference between action output by the inverse dynamic model based on the bisimulation metric. The difference between states evaluated under the bisimulation metric is directly associated with the value difference (see Theorem 3.3 in [26] and Theorem 2 in [63] for detailed proof). The loss function of LIBERTY metric learning is: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\phi)=\\mathbb{E}\\left[\\left(d_{i n v}(s_{i},s_{j};\\phi)-|r_{s_{i}}^{a_{i}}-r_{s_{j}}^{a_{j}}|-\\gamma W_{2}(P_{\\tilde{s}_{i}}^{\\pi}(\\eta),P_{\\tilde{s}_{j}}^{\\pi}(\\eta))-\\gamma|I_{\\tilde{s}_{i}}^{s_{i+1}}(\\theta)-I_{\\tilde{s}_{j}}^{s_{j+1}}(\\theta)|\\right)^{2}\\right]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\bar{s}$ denotes state with stop gradients, $P_{\\bar{s_{i}}}^{\\pi}(\\eta)$ indicates probabilistic dynamics model parameterized with $\\eta$ and $I_{\\bar{s_{i}}}^{s_{i+1}}(\\theta)$ is the inverse dynamic model parameterized with $\\theta$ . As the 1-Wasserstein distance is usually difficult to estimate, LIBERTY and prior methods [68] propose to use 2-Wasserstein distance $W_{2}$ to replace $W_{1}$ , as $W_{2}$ has a convenient closed-form of a Gaussian distribution with respect to the $L_{2}$ distance. ", "page_idx": 3}, {"type": "text", "text": "Proposition 1 (Relaxation Divergence). Relaxing the $W_{1}$ metric to $W_{2}$ breaks the theoretical integrity of the inverse dynamic bisimulation metric [63] in scenarios where the transition dynamic model $P(s,a)$ or the policy $\\pi$ is stochastic. ", "page_idx": 3}, {"type": "text", "text": "Proof in Appendix B. Based on Proposition 1, we can see that the relaxation divergence of 1- Wasserstein distance between dynamic models can only be ignored when both the transition dynamics model and policy $\\pi$ are deterministic. However, this assumption may be too strong to hold in practice. ", "page_idx": 3}, {"type": "text", "text": "Besides, the metric learning process encounters a theory-practice gap due to the relaxation of reward expectations. Specifically, computing the expected reward differences (first term of Equation (2)), $|\\mathbb{E}_{a_{i}\\sim\\pi}r_{s_{i}}^{a_{i}}-\\mathbb{E}_{a_{j}\\sim\\pi}r_{s_{j}}^{a_{j}}|$ , proves to be computationally daunting and challenging to accurately estimate, even with sampling techniques. Following other bisimulation metric-based methods [13, 68], LIBERTY [63] shifts the expectation operator outside the absolute value of reward differences (second term of Equation (3)) and differences between action outputs predicted by inverse dynamic models, as in $\\mathbb{E}_{a_{i}\\sim\\pi,a_{j}\\sim\\pi}[|r_{s_{i}}^{a_{i}}-r_{s_{j}}^{a_{j}}|+|I_{\\bar{s}_{i}}^{s_{i+1}}-I_{\\bar{s}_{j}}^{s_{j+1}}|]$ , which facilitates more efficient sampling by avoiding the direct estimation of reward and action expectations. ", "page_idx": 3}, {"type": "text", "text": "Proposition 2 (Shifted LIBERTY Distance). The relaxation of expectation during learning process shifts the original LIBERTY distance and introduces a looser value difference bound. ", "page_idx": 3}, {"type": "text", "text": "We extend the concept of the shifted MICo distance [14] introduced in [35, 16]. The proof is in appendix B. Based on Proposition 2, as the shifted LIBERTY distance has a looser value difference bound, it may be less relevant to the value function. As a result, the learned metric may not be able to capture the state similarity within bisimulation metric space accurately. , as the shifted LIBERTY distance has a looser value difference bound, it may be less relevant to the value function. As a result, the learned metric may not be able to capture the state similarity within bisimulation metric space accurately. ", "page_idx": 4}, {"type": "text", "text": "Scalability Constraints. As shown in Table 2, LIBERTY\u2019s performance significantly drops in hard exploration tasks within MiniGrid and in the realistic scenarios of the Habitat environment [60]. Specifically, in Habitat, agents must navigate through photorealistic simulations of actual indoor spaces. In these settings, the subtle variations between states diminish the impact of the exploration bonus, significantly lowering the efficiency of exploration. This limitation becomes even more apparent in the procedurally-generated MiniGrid environment, where the inherent stochasticity further impedes LIBERTY\u2019s performance. Those findings highlight the scalability challenges that bisimulation metric-based methods encounter in complex environments. ", "page_idx": 4}, {"type": "text", "text": "4 Effective Metric-based Exploration Bonus ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we introduce the Effective Metric-based Exploration-bonus (EME), conceived to overcome the limitations identified in existing methods that use metric-based state discrepancy as exploration bonuses. Our development is driven by two primary objectives. Firstly, we aim to refine the metric learning process to achieve a more effective exploration without incurring the approximation gap, thereby upholding the theoretical integrity of value function bound. Secondly, we seek to obviate the reliance on scaling factor of episodic counts, ensuring effective exploration across different environments. To this end, we propose a novel metric that more precisely evaluates the behavioral similarity between states, establishing a more effective exploration strategy compared with previous approaches. Furthermore, we introduce an innovative approach that utilizes the variance between outputs of an ensemble of reward models in metric learning as a dynamically-adjusted scaling factor, which significantly enhances exploration efficiency, especially in hard exploration tasks, thereby improving scalability. The next two sections describe our method in detail. ", "page_idx": 4}, {"type": "text", "text": "4.1 The EME Metric ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In order to avoid the approximation gap introduced by relaxation of Wasserstein distance and reward expectations mentioned in Proposition 1 and 2, along with the reliance on the episodic count, we eliminate the calculation of the Wasserstein distance, and propose the EME metric: ", "page_idx": 4}, {"type": "text", "text": "Definition 2 (EME Distance Function). Let met be the space of bounded pseudo-metrics on state space $\\mathcal{S}$ , the EME metric $d_{E}:S\\times S\\to\\mathbb{R}$ , the EME distance function $\\mathcal{F}(d_{E},\\pi):\\mathfrak{m e t}\\to\\mathfrak{m e t}$ is defined as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{F}(d_{E},\\pi)(s_{i},s_{j})=|\\mathbb{E}_{a_{i}\\sim\\pi}r_{s_{i}}^{a_{i}}-\\mathbb{E}_{a_{j}\\sim\\pi}r_{s_{j}}^{a_{j}}|+\\gamma\\mathbb{E}_{a_{i}\\sim\\pi}d_{E}(s_{i}^{\\prime},s_{j}^{\\prime})+\\gamma D_{\\mathrm{KL}}(\\pi(\\cdot|s_{i})||\\pi(\\cdot|s_{j}))\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $D_{\\mathrm{KL}}(\\cdot\\|\\cdot)$ represents the Kullback\u2013Leibler $(K L)$ divergence. ", "page_idx": 4}, {"type": "text", "text": "Behavioral Similarity Between States. We strictly calculate the expectation of reward difference without relaxation. The EME metric measures the distribution distance between dynamics models by computing the distance between sampled subsequent states following representation learning method [14] to avoid the computation of the Wasserstein distance. Additionally, we integrate the Kullback\u2013Leibler (KL) divergence between policy distributions to more robustly model the \u201cbehavioral similarity\u201d between states, which is more effective and scalable across diverse environments with different observations, especially in addressing the critical \u201cNoisy-TV\u201d problem [11] during exploration. The \u201cNoisy-TV\u201d problem started as a thought experiment in [11] and is commonly discussed in exploration literature [51, 52, 42]. Imagine that an RL agent is rewarded with seeking the novel experience, a TV with unpredictable random noise outputs would be able to attract the agent\u2019s attention forever. The agent obtains new rewards from state discrepancy caused by noisy TV consistently, but it fails to make any meaningful action. To counter this, we utilize differences in policy distributions across states, promoting the exploration of diverse actions rather than repetitive behavior. ", "page_idx": 4}, {"type": "text", "text": "Proof in Appendix B. Theorem 1 provides a convergence guarantee for the EME distance that by iterating ${\\mathcal{F}}(d_{E},\\pi)$ , distance will converge to the fixed-point $\\hat{d}_{E}$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 2. (Guaranteed Value difference bound) Given the EME metric $d_{E}$ , states $s_{i}$ and $s_{j}$ , and $a$ policy $\\pi$ , we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n|V^{\\pi}\\left(s_{i}\\right)-V^{\\pi}\\left(s_{j}\\right)|\\leq d_{E}\\left(s_{i},s_{j}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Improved Exploration Efficiency. The proof can be found in Appendix B. Theorem 2 demonstrates that the EME distance between states provides an upper bound on the difference in their state values. Let\u2019s recap the definition of one-step TD error: $\\delta_{t}\\bar{=}\\bar{r}_{t+1}+\\gamma V(s_{t+1})-V(s_{t})$ . Intuitively, if there is a large value difference between states $s_{t}$ and $s_{t+1}$ , the corresponding exploration bonus $b$ which is calculated using $d_{E}\\big(s_{t},s_{t+1}\\big)$ will also be large. Consequently, the total reward for the next step, $r_{t+1}=r^{e}+b$ (where $r^{e}$ represents the external environment reward), increases accordingly. The increase of reward leads to a larger TD error when there is a significant value difference between adjacent states, incentivizing the agent to prioritize transitions with large TD errors, which not only enhances the agent\u2019s exploration capabilities but also significantly boosts training efficiency. ", "page_idx": 5}, {"type": "text", "text": "4.2 Tractable Optimization of EME ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Based on Equation (4), between any pair of states $s_{i}$ and $s_{j}$ , we can define the loss function of the EME metric learning as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Sigma(\\phi)=\\mathbb{E}\\left[\\left(d_{E}^{\\phi}(s_{i},s_{j})-|\\mathbb{E}_{a_{i}\\sim\\pi}r_{s_{i}}^{a_{i}}-\\mathbb{E}_{a_{j}\\sim\\pi}r_{s_{j}}^{a_{j}}|-\\gamma\\mathbb{E}_{a_{i}\\sim\\pi}d_{E}(s_{i}^{\\prime},s_{j}^{\\prime})-\\gamma D_{\\mathrm{KL}}(\\pi(\\cdot|s_{i})||\\pi(\\cdot|s_{j}))\\right)^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $d_{E}^{\\phi}$ is the EME metric encoder parameterized by $\\phi$ . The last three terms in the loss function (6) can be regarded as the regression of target metric, which approximates the difference between rewards, distances of next states and distances of policy distributions, respectively. However, the reward expectations $\\mathbb{E}_{a_{i}\\sim\\pi}r_{s_{i}}^{a_{i}}$ and $\\mathbb{E}_{a_{j}\\sim\\pi}r_{s_{j}}^{a_{j}}$ are computationally intractable and also difficult to estimate even based on sampling [13]. And the relaxation of reward expectations $\\mathbb{E}_{a_{i}\\sim\\pi}|r_{s_{i}}^{a_{i}}-r_{s_{j}}^{a_{j}}|$ used by previous methods [63, 68, 13] will lead to the learned metric having a looser value difference bound than the original bisimulation metric as proved in Proposition 2. Inspired by metric-based representation learning methods [35, 16], we propose to use an ensemble of reward models to approximate the reward difference more accurately. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Proposition 3. Let $r_{s}$ be a random variable over the action distribution defined by $p(r_{s}=r_{s}^{a})=$ $\\pi(a|s),\\,\\nu a r(r_{s_{i}})$ denote the variance of variable $r_{s_{i}}$ , we can have: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\mathbb{E}_{a_{i}\\sim\\pi}r_{s_{i}}^{a_{i}}-\\mathbb{E}_{a_{j}\\sim\\pi}r_{s_{j}}^{a_{j}}|=\\sqrt{\\mathbb{E}_{a_{i}\\sim\\pi}\\left[\\left|r_{s_{i}}^{a_{i}}-r_{s_{j}}^{a_{j}}\\right|^{2}\\right]-\\nu a r(r_{s_{i}})-\\nu a r(r_{s_{j}})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Proof. (Sketch) The proof is based on [14, 16] by expanding $\\mathbb{E}_{a_{i}\\sim\\pi}[|r_{s_{i}}^{a_{i}}-r_{s_{j}}^{a_{j}}|^{2}]-|\\mathbb{E}_{a_{i}\\sim\\pi}r_{s_{i}}^{a_{i}}-$ $\\mathbb{E}_{a_{j}\\sim\\pi}r_{s_{j}}^{a_{j}}|^{2}$ . See detailed proof in Appendix B. \u53e3 ", "page_idx": 5}, {"type": "text", "text": "Ensemble of Reward Models. We train an ensemble of reward models $\\{g(\\eta_{1}),\\dots,g(\\eta_{k})\\}$ to predict the reward from the state-action pairs sampling from the buffer $\\mathcal{D}_{\\tau}$ : $g(s,a;\\eta):S\\times A\\rightarrow\\mathcal{R}$ by minimizing the prediction error $\\Vert\\bar{g}(s_{t},a_{t};\\eta)-\\bar{r}_{t+1}\\Vert^{2}$ . Consequently, the variance of reward across the output of different models in the ensemble is defined as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{var}(r_{s_{i}})\\approx\\zeta(r_{s_{i}}^{a_{i}})=\\mathbb{E}_{a_{i}\\sim\\pi(s_{i})}\\left\\{\\mathbb{E}_{\\eta}\\left[\\Vert g(s_{i},a_{i},\\eta)-\\mathbb{E}_{\\eta}[g(s_{i},a_{i},\\eta)\\Vert_{2}^{2}\\right]\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Based on Equation (6), Equation (7) and Equation (8), the tractable EME loss without approximation gap can be defined as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}(\\phi)=\\mathbb{E}_{\\mathcal{D}_{\\tau}}[(d_{E}^{\\phi}(s_{i},s_{j})\\!-\\!\\sqrt{|r_{s_{i}}^{a_{i}}-r_{s_{j}}^{a_{j}}|^{2}-\\zeta(r_{s_{i}}^{a_{i}})-\\zeta(r_{s_{j}}^{a_{j}})}-\\gamma\\mathbb{E}_{a_{i}\\sim\\pi}d_{E}^{\\phi}(s_{i}^{\\prime},s_{j}^{\\prime})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad-\\gamma\\mathbb{E}_{\\pi(\\cdot|s_{i})}[\\log\\pi(\\cdot|s_{i})-\\log\\pi(\\cdot|s_{j})])^{2}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Diversity-Enhanced Scaling Factor. The scaling factor of previous methods is either not scalable [51, 69, 2] or hand-crafted hyper-parameter [63], in light of this, we propose a scalable diversity-enhanced scaling factor for the intrinsic reward to further improve the efficacy of exploration. Let\u2019s recap the ensemble of reward models $\\{g(\\eta_{1}),\\dots,g(\\eta_{k})\\}$ used in metric training (9). To maintain the diversity across the individual models $g(\\eta)$ , we initialize each model\u2019s parameters differently and train each of them on a subset of data randomly sampled with replacement. Each model in our ensemble is trained to predict the ground truth reward. Hence, the parts of reward obtained within the state space that have been well explored by the agent will have gathered enough data to train all models, resulting in a low variance of reward predictions from the models, when generalizing to unseen but similar parts of the reward from unvisited state-space, the areas which are novel and unexplored would still have high prediction error for all models as none of them are yet trained on such examples, resulting in higher variance of the reward prediction. Therefore, we use the variance of reward predictions as the scaling factor of intrinsic reward to encourage exploration on unvisited state space. With the maximum reward scaling $M$ , the exploration bonus is defined as: ", "page_idx": 6}, {"type": "equation", "text": "$$\nb_{t+1}=d_{E}(s_{t},s_{t+1})*m i n\\{m a x\\{\\zeta(r_{s_{t}}),1\\},M\\}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "As a result, the agent receives greater rewards when encountering novel states during training, which facilitates more effective exploration, the ablation study on scaling factor and visualization of exploration bonus is in Appendix C.5. See Algorithm 1 of Appendix D.2 for detailed description. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The overall objective of our experiment is to evaluate the performance of EME in comparison to other baselines, we conduct comprehensive experiments on various settings of continuous control tasks, discrete-action hard exploration games, and real indoor environments3 to assess the effectiveness and scalability of our algorithm. The implementation details can be found in Appendix D. ", "page_idx": 6}, {"type": "text", "text": "Baselines. We compare it against following competitive baseline methods. ICM [47]: a famous curiosity-driven method. RND [11]: intrinsic rewards are the prediction errors of the distillation network. E3B [32]: proposing count-based episodic bonuses under continuous state spaces. RIDE [51]: using state difference under latent space as exploration bonuses. NovelD [69]: using the state difference measured by RND bonus as intrinsic rewards. LIBERTY [63]: using the state discrepancy evaluated under the bisimulation metric space as shaping reward. ", "page_idx": 6}, {"type": "image", "img_path": "QpKWFLtZKi/tmp/fb38caaa73457a48f806899a3c1e8e3de18faa25a1e6c712909e8a05ef7559e7.jpg", "img_caption": ["Figure 2: Results for various hard exploration tasks from Robosuite. The $\\mathbf{X}{\\cdot}$ -axis represents the number of steps (1e7) in training. The y-axis represents the mean success rate(standard deviations in shade). "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "5.1 Continuous Control ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In our continuous control experiments conducted on the Robosuite platform [70], we assess the exploration capabilities of agents across various challenging tasks, which include Door Opening, ", "page_idx": 6}, {"type": "table", "img_path": "QpKWFLtZKi/tmp/3719f09dd64d34b9be38467387a94333a75465b40ad9ba308dd66c9482ffc70c.jpg", "table_caption": ["Table 3: Average testing results of different Minigrid environments for EME and other baselines. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table Wiping, and Pick-and-Place. The detailed description of environments can be found in Appendix D.1.1. Each task represents a demanding context for robotic control, characterized by sparse rewards and significant exploratory challenges. The overall results are depicted in Figure 2, where our method consistently outperforms others, demonstrating its superiority in handling continuous control tasks. LIBERTY achieves the second-best performance in the Door Opening and Table Wiping tasks, underscoring the advantages of state discrepancy-based novelty within the bisimulation metric space. Conversely, episodic count-based methods such as RIDE, NovelD, and E3B lag behind, as episodic counts become less effective in environments with high-dimensional states. Furthermore, curiosity-driven approaches like ICM and RND struggle due to insufficient exploration. We also provide additional ablation studies on the ensemble size and max reward scaling in Appendix C. ", "page_idx": 7}, {"type": "text", "text": "EME Combined with Feature Encoder. The EME metric can be integrated with any state representation derived from various encoders $\\Phi(\\cdot)$ , expressed as $d_{E}(\\Phi(s_{t}),\\Bar{\\Phi}(s_{t+1}))$ . The encoder may include the inverse dynamic model [47, 51, 63] which isolates environmental factors that do not affect the agent\u2019s behavior, the bisimulation-based encoder [68, 35, 14, 13], and random embeddings. The results can be found in Figure 8 and 9 in Appendix C.2, the performance of EME under the inverse dynamic encoder and bisimulation-based encoder exhibits a decline, because the exploration bonus under these encoders get close to zero quickly since their representations are very compact and every state looks more similar with the convergence of the encoder, which harms the efficacy of exploration. With respect to random embedding, the variance is larger, resulting in more unstable performance. ", "page_idx": 7}, {"type": "image", "img_path": "QpKWFLtZKi/tmp/dbb9a7bcc99b335ef212f1293bb3123bce25a51a7317b0c7195a9a8567f6d60c.jpg", "img_caption": ["Figure 3: Comparison results for different hard exploration Atari games. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.2 Discrete-action Games ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Atari Games. To assess EME in scenarios involving pixel-based observations and discrete actions, we evaluate our method on the hard exploration games identified by [6, 69, 63] within the common ", "page_idx": 7}, {"type": "text", "text": "Atari benchmark [8], including Asterix, Alien, Breakout, BeamRider, Mspacman, PrivateEye, Pong, SpaceInvaders, Qbert, and UpNDown. The results are presented in Figure 3. Our method consistently achieves similar or higher average returns compared to other baselines across all tested tasks. Notably, EME demonstrates the fastest convergence, particularly in Asterix and UpNDown, highlighting our method\u2019s superior learning efficiency. While LIBERTY and NovelD exhibit competitive performance in 6 games, their effectiveness significantly diminishes in the challenging games, PrivateEye and UpNDown. As for other methods like RIDE, E3B, ICM, and RND struggle across all tasks, primarily due to inadequate exploration capabilities. To isolate the impact of our proposed EME metric, we also include the performance of RIDE and LIBERTY with Diversity-enhanced Scaling Factor, denoted as RIDE and LIBERTY with DSF. EME still achieves the best performance which demostrate the superiority of EME metric. ", "page_idx": 8}, {"type": "text", "text": "MiniGrid Environments. MiniGrid [17] presents a series of procedurally-generated, challenging environments. We focus on three settings from MiniGrid: Multi-Room (MR), Key Corridor (KC), and Obstructed Maze (OM). For example, MRN12S10-NT stands for MultiRoom-N12-S10-NoisyTV. Details on the specific environmental settings are available in Appendix D.1.2. Table 3 presents the testing performance of EME and all baseline methods across five different seeds. EME successfully solves all hard-level exploration environments within MiniGrid, achieving the best performance in 10 out of 12 settings. While Multi-Room environments are relatively easy, all baselines except for ICM and RND, demonstrate competency. However, as room size and number increase, the complexity of the tasks also rises, with EME consistently outperforming other baselines, a trend that extends to the Key-Corridor environments. In the particularly challenging Obstructed Maze environments, where obstructions block doors, our agent also excels by learning to remove these obstructions, further demonstrating the effectiveness and scalability of our approach. ", "page_idx": 8}, {"type": "text", "text": "Noisy TV Problem. In addition to standard MiniGrid tasks, we also tested the model\u2019s ability to deal with stochasticity in the environment by adding a manually-made Noisy TV setting introduced in [51], where some blocks change color at every time step. As illustrated in Table 3, EME maintains strong performance even under these conditions. Notably, EME is the only method that successfully completes the KCS5R3-NT task, underscoring the robustness of our method. ", "page_idx": 8}, {"type": "image", "img_path": "QpKWFLtZKi/tmp/83cdfeed169c3347168b3b8b736c27534f600b3aabb3d7771e0ed00b60f0a43b.jpg", "img_caption": ["Figure 4: Results of exploration tasks on Habitat. Error bars represent std, deviations over 5 seeds. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.3 Real-life Habitat Environment ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To test our model\u2019s applicability to real-world environments, we investigate hard exploration tasks in Habitat [60]. Habitat is a platform for embodied AI research which provides an interface for agents to navigate and act in photorealistic (Figure 5) simulations of real ", "page_idx": 8}, {"type": "text", "text": "indoor environments. Full details on the environmental setting can be found in Appendix D.1.3. We evaluate our methods on three embodied AI tasks of Habitat benchmark. As shown in Figure 4, EME consistently outperforms all baselines, demonstrating its superior scalability to high-dimensional visualbased observations and confirming its broad applicability. The performances of count-based methods like E3B, NovelD and RIDE are comparable, whereas curiosity-driven methods such as ICM and RND lag behind. The underperformance is primarily due to their inadequate exploration capabilities, which are Figure 5: Habitat particularly challenged by the complex and rich visual observations inherent in real-world settings. ", "page_idx": 8}, {"type": "image", "img_path": "QpKWFLtZKi/tmp/6a86994d305f02b482f375872a11ecdfdfd4c2cb7012d8b94a1143cdfdede3d9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Reward-free Exploration. we investigate reward-free exploration by evaluating the performance of all baselines using exploration bonus-only. Figure 10 displays the trajectories for all baselines on one of the test maps, clearly demonstrating that EME explores a significantly larger portion of the space compared to other methods. The full experiment results can be found in Appendix C.3. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we identify the limitations of existing state discrepancy-based exploration methods: the reliance on episodic count-based scaling factor and theory-practice approximation gap, which leads to limited scalability especially for hard exploration tasks within realistic environments. To address the issues, we propose the Effective Metric-based Exploration-bonus (EME), which addresses the inherent limitations by proposing a robust metric for state discrepancy evaluation backed by comprehensive theoretical analysis. Furthermore, we propose the diversity-enhanced scaling factor integrated into the exploration bonus to enhance exploration effectiveness in particularly challenging scenarios. Extensive experiments on hard exploration tasks from continuous control, discrete-action games and realistic environments have demonstrated the effectiveness and scalability of our method. ", "page_idx": 9}, {"type": "text", "text": "7 Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by National Science and Technology Major Project (2023ZD0121401), the Science and Technology Development Fund Macau SAR (0003/2023/RIC, 0052/2023/RIA1, 0031/2022/A, 001/2024/SKL for SKL-IOTSC), the Research Grant of University of Macau (MYRG2022-00252-FST), Shenzhen-Hong Kong-Macau Science and Technology Program Category C (SGDX20230821095159012), and Wuyi University Hong Kong and Macau joint Research Fund (2021WGALH14). This work was performed in part at SICC which is supported by SKL-IOTSC, University of Macau. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal J\u00f3zefowicz, Bob McGrew, Jakub W. Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, Jonas Schneider, Szymon Sidor, Joshua Tobin, Peter Welinder, Lilian Weng, and Wojciech Zaremba. Learning dexterous in-hand manipulation. The International Journal of Robotics Research, 39:20 \u2013 3, 2018.   \n[2] Adri\u00e0 Puigdom\u00e8nech Badia, Pablo Sprechmann, Alex Vitvitskyi, Daniel Guo, Bilal Piot, Steven Kapturowski, Olivier Tieleman, Martin Arjovsky, Alexander Pritzel, Andrew Bolt, et al. Never give up: Learning directed exploration strategies. In International Conference on Learning Representations, 2019.   \n[3] Babak Badnava, Mona Esmaeili, Nasser Mozayani, and Payman Zarkesh-Ha. A new potentialbased reward shaping for reinforcement learning agent. In 2023 IEEE 13th Annual Computing and Communication Workshop and Conference (CCWC), pages 01\u201306. IEEE, 2023.   \n[4] Chenjia Bai, Peng Liu, Kaiyu Liu, Lingxiao Wang, Yingnan Zhao, Lei Han, and Zhaoran Wang. Variational dynamic for self-supervised exploration in deep reinforcement learning. IEEE Transactions on neural networks and learning systems, 34(8):4776\u20134790, 2021.   \n[5] Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation. Advances in neural information processing systems, 29, 2016.   \n[6] Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation. Advances in neural information processing systems, 29, 2016.   \n[7] Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation. Advances in neural information processing systems, 29, 2016.   \n[8] Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253\u2013279, 2013.   \n[9] Bernadette Bucher, Anton Arapin, Ramanan Sekar, Marc Badger, Feifei Duan, Oleh Rybkin, and Kostas Daniilidis. Perception-driven curiosity with bayesian surprise. 2019.   \n[10] Yuri Burda, Harri Edwards, Deepak Pathak, Amos Storkey, Trevor Darrell, and Alexei A Efros. Large-scale study of curiosity-driven learning. arXiv preprint arXiv:1808.04355, 2018.   \n[11] Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network distillation. In International Conference on Learning Representations, 2018.   \n[12] Andres Campero, Roberta Raileanu, Heinrich K\u00fcttler, Joshua B Tenenbaum, Tim Rockt\u00e4schel, and Edward Grefenstette. Learning with amigo: Adversarially motivated intrinsic goals. arXiv preprint arXiv:2006.12122, 2020.   \n[13] Pablo Samuel Castro. Scalable methods for computing state similarity in deterministic markov decision processes. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 10069\u201310076, 2020.   \n[14] Pablo Samuel Castro, Tyler Kastner, Prakash Panangaden, and Mark Rowland. Mico: Improved representations via sampling-based state similarity for markov decision processes. Advances in Neural Information Processing Systems, 34:30113\u201330126, 2021.   \n[15] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning from rgb-d data in indoor environments. International Conference on 3D Vision (3DV), 2017.   \n[16] Jianda Chen and Sinno Pan. Learning representations via a robust behavioral metric for deep reinforcement learning. Advances in Neural Information Processing Systems, 35:36654\u201336666, 2022.   \n[17] Maxime Chevalier-Boisvert, Bolun Dai, Mark Towers, Rodrigo de Lazcano, Lucas Willems, Salem Lahlou, Suman Pal, Pablo Samuel Castro, and Jordan Terry. Minigrid & miniworld: Modular & customizable reinforcement learning environments for goal-oriented tasks. CoRR, abs/2306.13831, 2023.   \n[18] C Colas, T Karch, O Sigaud, and PY Oudeyer. Intrinsically motivated goal-conditioned reinforcement learning: A short survey. arxiv 2020. arXiv preprint arXiv:2012.09830.   \n[19] Michael Dennis, Natasha Jaques, Eugene Vinitsky, Alexandre Bayen, Stuart Russell, Andrew Critch, and Sergey Levine. Emergent complexity and zero-shot transfer via unsupervised environment design. Advances in neural information processing systems, 33:13049\u201313061, 2020.   \n[20] Sam Michael Devlin and Daniel Kudenko. Dynamic potential-based reward shaping. In Proceedings of the 11th international conference on autonomous agents and multiagent systems, pages 433\u2013440. IFAAMAS, 2012.   \n[21] Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep reinforcement learning for continuous control. In International conference on machine learning, pages 1329\u20131338. PMLR, 2016.   \n[22] Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O Stanley, and Jeff Clune. Go-explore: a new approach for hard-exploration problems. arXiv preprint arXiv:1901.10995, 2019.   \n[23] Meng Fang, Tianyi Zhou, Yali Du, Lei Han, and Zhengyou Zhang. Curriculum-guided hindsight experience replay. Advances in neural information processing systems, 32, 2019.   \n[24] Norm Ferns, Prakash Panangaden, and Doina Precup. Metrics for finite markov decision processes. In UAI, volume 4, pages 162\u2013169, 2004.   \n[25] Norman Ferns, Prakash Panangaden, and Doina Precup. Metrics for finite markov decision processes. arXiv preprint arXiv:1207.4114, 2012.   \n[26] Norman Ferns and Doina Precup. Bisimulation metrics are optimal value functions. In UAI, pages 210\u2013219, 2014.   \n[27] Norman Ferns and Doina Precup. Bisimulation metrics are optimal value functions. In UAI, pages 210\u2013219, 2014.   \n[28] Yannis Flet-Berliac, Johan Ferret, Olivier Pietquin, Philippe Preux, and Matthieu Geist. Adversarially guided actor-critic. arXiv preprint arXiv:2102.04376, 2021.   \n[29] Yang Gao and Francesca Toni. Potential based reward shaping for hierarchical reinforcement learning. In Twenty-Fourth International Joint Conference on Artificial Intelligence, 2015.   \n[30] Marek Grzes and Daniel Kudenko. Learning potential for reward shaping in reinforcement learning with tile coding. In Proceedings AAMAS 2008 Workshop on Adaptive and Learning Agents and Multi-Agent Systems (ALAMAS-ALAg 2008), pages 17\u201323, 2008.   \n[31] Anna Harutyunyan, Sam Devlin, Peter Vrancx, and Ann Now\u00e9. Expressing arbitrary reward functions as potential-based advice. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 29, 2015.   \n[32] Mikael Henaff, Roberta Raileanu, Minqi Jiang, and Tim Rockt\u00e4schel. Exploration via elliptical episodic bonuses. Advances in Neural Information Processing Systems, 35:37631\u201337646, 2022.   \n[33] Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. Vime: Variational information maximizing exploration. Advances in neural information processing systems, 29, 2016.   \n[34] Minqi Jiang, Edward Grefenstette, and Tim Rockt\u00e4schel. Prioritized level replay. In International Conference on Machine Learning, pages 4940\u20134950. PMLR, 2021.   \n[35] Mete Kemertas and Tristan Aumentado-Armstrong. Towards robust bisimulation metric learning. Advances in Neural Information Processing Systems, 34:4764\u20134777, 2021.   \n[36] Hyoungseok Kim, Jaekyeom Kim, Yeonwoo Jeong, Sergey Levine, and Hyun Oh Song. Emi: Exploration with mutual information. arXiv preprint arXiv:1810.01176, 2018.   \n[37] Jens Kober, J. Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. The International Journal of Robotics Research, 32:1238 \u2013 1274, 2013.   \n[38] Weichen Li, Rati Devidze, and Sophie Fellenz. Potential-based reward shaping for learning to play text-based adventure games. arXiv preprint arXiv:2302.10720, 2023.   \n[39] Zhixuan Lin, Pierluca D\u2019Oro, Evgenii Nikishin, and Aaron Courville. The curse of diversity in ensemble-based exploration. arXiv preprint arXiv:2405.04342, 2024.   \n[40] Marlos C Machado, Marc G Bellemare, and Michael Bowling. Count-based exploration with the successor representation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 5125\u20135133, 2020.   \n[41] Jarryd Martin, Suraj Narayanan Sasikumar, Tom Everitt, and Marcus Hutter. Count-based exploration in feature space for reinforcement learning. arXiv preprint arXiv:1706.08090, 2017.   \n[42] Pietro Mazzaglia, Ozan Catal, Tim Verbelen, and Bart Dhoedt. Curiosity-driven exploration via latent bayesian surprise. In Proceedings of the AAAI conference on artificial intelligence, volume 36, pages 7752\u20137760, 2022.   \n[43] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin A. Riedmiller, Andreas Kirkeby Fidjeland, Georg Ostrovski, Stig Petersen, Charlie Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518:529\u2013533, 2015.   \n[44] Andrew $\\mathbf{Y}\\,\\mathbf{Ng}$ , Daishi Harada, and Stuart Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In Icml, volume 99, pages 278\u2013287. Morgan Kaufmann, 1999.   \n[45] Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via bootstrapped dqn. Advances in neural information processing systems, 29, 2016.   \n[46] Georg Ostrovski, Marc G Bellemare, A\u00e4ron Oord, and R\u00e9mi Munos. Count-based exploration with neural density models. In International conference on machine learning, pages 2721\u20132730. PMLR, 2017.   \n[47] Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised prediction. In International conference on machine learning, pages 2778\u2013 2787. PMLR, 2017.   \n[48] Deepak Pathak, Dhiraj Gandhi, and Abhinav Gupta. Self-supervised exploration via disagreement. In International conference on machine learning, pages 5062\u20135071. PMLR, 2019.   \n[49] R\u00e9my Portelas, C\u00e9dric Colas, Katja Hofmann, and Pierre-Yves Oudeyer. Teacher algorithms for curriculum learning of deep rl in continuously parameterized environments. In Conference on Robot Learning, pages 835\u2013853. PMLR, 2020.   \n[50] Sebastien Racaniere, Andrew K Lampinen, Adam Santoro, David P Reichert, Vlad Firoiu, and Timothy P Lillicrap. Automated curricula through setter-solver interactions. arXiv preprint arXiv:1909.12892, 2019.   \n[51] Roberta Raileanu and Tim Rockt\u00e4schel. RIDE: rewarding impact-driven exploration for procedurally-generated environments. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.   \n[52] Nikolay Savinov, Anton Raichuk, Rapha\u00ebl Marinier, Damien Vincent, Marc Pollefeys, Timothy Lillicrap, and Sylvain Gelly. Episodic curiosity through reachability. arXiv preprint arXiv:1810.02274, 2018.   \n[53] J\u00fcrgen Schmidhuber. A possibility for implementing curiosity and boredom in model-building neural controllers. 1991.   \n[54] Younggyo Seo, Lili Chen, Jinwoo Shin, Honglak Lee, Pieter Abbeel, and Kimin Lee. State entropy maximization with random encoders for efficient exploration. In International Conference on Machine Learning, pages 9443\u20139454. PMLR, 2021.   \n[55] Christopher Stanton and Jeff Clune. Curiosity search: producing generalists by encouraging individuals to continually explore and acquire skills throughout their lifetime. PloS one, 11(9):e0162235, 2016.   \n[56] Christopher Stanton and Jeff Clune. Deep curiosity search: Intra-life exploration improves performance on challenging deep reinforcement learning problems. arXiv preprint arXiv:1806.00553, 2018.   \n[57] Alexander L Strehl and Michael L Littman. An analysis of model-based interval estimation for markov decision processes. Journal of Computer and System Sciences, 74(8):1309\u20131331, 2008.   \n[58] Sainbayar Sukhbaatar, Zeming Lin, Ilya Kostrikov, Gabriel Synnaeve, Arthur Szlam, and Rob Fergus. Intrinsic motivation and automatic curricula via asymmetric self-play. arXiv preprint arXiv:1703.05407, 2017.   \n[59] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.   \n[60] Andrew Szot, Alex Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Chaplot, Oleksandr Maksymets, Aaron Gokaslan, Vladimir Vondrus, Sameer Dharur, Franziska Meier, Wojciech Galuba, Angel Chang, Zsolt Kira, Vladlen Koltun, Jitendra Malik, Manolis Savva, and Dhruv Batra. Habitat 2.0: Training home assistants to rearrange their habitat. In Advances in Neural Information Processing Systems (NeurIPS), 2021.   \n[61] Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI Xi Chen, Yan Duan, John Schulman, Filip DeTurck, and Pieter Abbeel. # exploration: A study of count-based exploration for deep reinforcement learning. Advances in neural information processing systems, 30, 2017.   \n[62] Ruo Yu Tao, Vincent Fran\u00e7ois-Lavet, and Joelle Pineau. Novelty search in representational space for sample efficient exploration. Advances in Neural Information Processing Systems, 33:8114\u20138126, 2020.   \n[63] Yiming Wang, Ming Yang, Renzhi Dong, Binbin Sun, Furui Liu, et al. Efficient potential-based exploration in reinforcement learning using inverse dynamic bisimulation metric. Advances in Neural Information Processing Systems, 36, 2024.   \n[64] Eric Wiewiora, Garrison W Cottrell, and Charles Elkan. Principled methods for advising reinforcement learning agents. In Proceedings of the 20th international conference on machine learning (ICML-03), pages 792\u2013799, 2003.   \n[65] Peter R. Wurman, Samuel Barrett, Kenta Kawamoto, James MacGlashan, Kaushik Subramanian, Thomas J. Walsh, Roberto Capobianco, Alisa Devlic, Franziska Eckert, Florian Fuchs, Leilani Gilpin, Piyush Khandelwal, Varun Kompella, HaoChih Lin, Patrick MacAlpine, Declan Oller, Takuma Seno, Craig Sherstan, Michael D. Thomure, Houmehr Aghabozorgi, Leon Barrett, Rory Douglas, Dion Whitehead, Peter D\u00fcrr, Peter Stone, Michael Spranger, and Hiroaki Kitano. Outracing champion gran turismo drivers with deep reinforcement learning. Nature, 602:223 \u2013 228, 2022.   \n[66] Denis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Reinforcement learning with prototypical representations. In International Conference on Machine Learning, pages 11920\u201311931. PMLR, 2021.   \n[67] Daochen Zha, Wenye Ma, Lei Yuan, Xia Hu, and Ji Liu. Rank the episodes: A simple approach for exploration in procedurally-generated environments. arXiv preprint arXiv:2101.08152, 2021.   \n[68] Amy Zhang, Rowan McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine. Learning invariant representations for reinforcement learning without reconstruction. arXiv preprint arXiv:2006.10742, 2020.   \n[69] Tianjun Zhang, Huazhe Xu, Xiaolong Wang, Yi Wu, Kurt Keutzer, Joseph E Gonzalez, and Yuandong Tian. Noveld: A simple yet effective exploration criterion. Advances in Neural Information Processing Systems, 34:25217\u201325230, 2021.   \n[70] Yuke Zhu, Josiah Wong, Ajay Mandlekar, Roberto Mart\u00edn-Mart\u00edn, Abhishek Joshi, Soroush Nasiriany, and Yifeng Zhu. robosuite: A modular simulation framework and benchmark for robot learning. In arXiv preprint arXiv:2009.12293, 2020. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Broader Impact Statement ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "This work introduces a reinforcement learning (RL) exploration method by introducing an effective metric-based exploration bonus, a versatile framework applicable to a wide array of decision-making problems, which can be applicable to diverse scenarios, including autonomous driving, household robotics, video gaming, online recommendation or advertisement optimization and etc. Similar to other RL exploration algorithms, our method is designed to facilitate the learning of a policy that maximizes a reward function defined by the designer. The implementation of such a policy, depending on the objectives set by the reward function designer, could lead to either positive or negative outcomes. Additionally, our approach significantly reduces the burden of reward tuning for researchers and algorithm engineers, and concurrently offers insights into the quality of the designed rewards through the shaping weights derived from our methods. ", "page_idx": 14}, {"type": "text", "text": "B Proofs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proposition 1 (Relaxation Divergence). Relaxing the $W_{1}$ metric to $W_{2}$ breaks the theoretical integrity of the inverse dynamic bisimulation metric [63] in scenarios where the transition dynamic model $P(s,a)$ or the policy $\\pi$ is stochastic. ", "page_idx": 14}, {"type": "text", "text": "Proof. First, let\u2019s recall the proof of the existence of a unique fixed-point in the inverse dynamic bisimulation metric: ", "page_idx": 14}, {"type": "text", "text": "Lemma 1 ([63]). Given a fixed policy $\\hat{\\pi}$ . Define $\\mathcal{H}^{\\hat{\\pi}}:\\mathfrak{m e t}\\mapsto\\mathfrak{n}$ et by $\\mathcal{H}(d,\\hat{\\pi})(s_{i},s_{j})=|r_{s_{i}}^{\\hat{\\pi}}-r_{s_{j}}^{\\hat{\\pi}}|+$ $\\gamma W(d)(\\mathcal{P}_{s_{i}}^{\\hat{\\pi}},\\mathcal{P}_{s_{j}}^{\\hat{\\pi}})+\\|I^{\\hat{\\pi}}(\\cdot\\mid s_{i},s_{i+1})-I^{\\hat{\\pi}}(\\cdot\\mid s_{j},s_{j+1})\\|_{1},$ , then $\\mathcal{H}^{\\hat{\\pi}}$ has a least fixed point $d^{\\hat{\\pi}}$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. This proof mimics the proof of Theorem 4.5 from [24]. We make use of the same pointwise ordering on met: $d\\leq d^{\\prime}$ iff $d(\\bar{s},t)\\leq d^{\\prime}(s,t)$ for all $s,t\\in S$ , which gives us an $\\omega$ -cpo with bottom $\\perp$ , which is the everywhere-zero metric. Since Lemma 4.4 from [24] (Wasserstein metric $W$ is continuous) also applies in our definition, it only remains to show that $\\mathcal{H}(d,\\hat{\\pi})$ is continuous: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathcal{H}^{\\#}\\left(\\bigcup_{w\\in\\mathbb{N}}\\right)(x_{t_{i}},y_{t})=|r_{s_{i}}^{\\#}-r_{s_{i}^{\\#}}^{\\#}|+\\gamma W\\left(\\bigcup_{w\\in\\mathbb{N}}\\right)(\\mathcal{P}_{s_{i}}^{k},\\mathcal{P}_{s_{i}^{\\#}}^{k})}&{}\\\\ &{\\ +\\|I^{\\#}\\{s_{i},s_{i+1}\\}-I^{\\#}\\{s,s_{i},s_{i+1}\\}\\|_{1}}\\\\ &{=|r_{s_{i}^{\\#}}^{\\#}-r_{s_{i}^{\\#}}^{\\#}|+\\gamma\\operatorname*{sup}W\\left(\\mathcal{P}_{s_{i}}\\cap_{s_{i}^{\\#}}^{k},\\mathcal{P}_{s_{i}^{\\#}}^{k}\\right)}\\\\ &{\\ +\\|I^{\\#}(x_{s_{i}},s_{i+1})-I^{\\#}\\{s,s_{i},s_{i+1}\\}\\|_{1}}\\\\ &{\\ +\\operatorname{sypcoinitity~of~}W^{\\#}}\\\\ &{=\\operatorname*{sup}(|r_{s_{i}^{\\#}}-r_{s_{i}^{\\#}}^{\\#}|)+\\gamma W(x_{n})(\\mathcal{P}_{s_{i}}^{k},\\mathcal{P}_{s_{i}^{\\#}}^{k})}\\\\ &{\\ +\\|I^{\\#}\\{s_{i},s_{i+1}\\}-I^{\\#}\\{s,s_{i},s_{i+1}\\}\\|_{1}}\\\\ &{=\\operatorname*{sup}\\left\\{\\mathcal{H}^{\\#}\\left(x_{n},s_{i}\\right)\\right\\}}\\\\ &{=\\left(\\bigcup_{w\\in\\mathbb{N}}\\right)\\left\\{\\mathcal{H}^{\\#}\\left(x_{n},s_{j}\\right)\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The rest of the proof follows in the same way as in [24]. ", "page_idx": 14}, {"type": "text", "text": "In equation (11), the existence of a unique fixed-point in the bisimulation metric requires the continuity and monotonicity of $W_{1}$ with respect to $d$ . The properties of continuity and monotonicity do not hold with $W_{2}$ . Therefore there is no more guarantee about the fixed-point existence in LIBERTY and other approximation-based methods [68] except that both the dynamics model and the policy $\\pi$ are deterministic, in which case $W_{2}(d)$ degenerates to $d$ [24] and Banach\u2019s fixed-point exists [35]. As for scenarios when the transition model and policy are stochastic, the theoretical guarantee of LIBERTY is broken. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "Proposition 2 (Shifted LIBERTY Distance). The relaxation of expectation during learning process shifts the original LIBERTY distance and introduces a looser value difference bound. ", "page_idx": 15}, {"type": "text", "text": "Proof. The proof is based on top of [14, 16]. ", "page_idx": 15}, {"type": "text", "text": "Lemma 2. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathfrak{L}_{a_{i}\\sim\\pi,a_{j}\\sim\\pi}[|r_{s_{i}}^{a_{i}}-r_{s_{j}}^{a_{j}}|+|I_{s_{i}}^{s_{i+1}}-I_{s_{j}}^{s_{j+1}}|]\\ge|\\mathbb{E}_{a_{i}\\sim\\pi}r_{s_{i}}^{a_{i}}-\\mathbb{E}_{a_{j}\\sim\\pi}r_{s_{j}}^{a_{j}}|+|\\mathbb{E}_{a_{i}\\sim\\pi}I_{s_{i}}^{s_{i+1}}-\\mathbb{E}_{a_{j}\\sim\\pi}I_{s_{j}}^{s_{j+1}}|\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Since $r_{s_{i}}^{a_{i}}$ and $r_{s_{j}}^{a_{j}}$ are rewards which are scalars, we have $|r_{s_{i}}^{a_{i}}\\,-\\,r_{s_{j}}^{a_{j}}|\\;\\geq\\;r_{s_{i}}^{a_{i}}\\,-\\,r_{s_{j}}^{a_{j}}$ and $|r_{s_{j}}^{a_{j}}-r_{s_{i}}^{a_{i}}|\\geq r_{s_{j}}^{a_{j}}-r_{s_{i}}^{a_{i}}$ . By taking the expectation over $a_{i}$ and $a_{j}$ , we can have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{a_{i}\\sim\\pi,a_{j}\\sim\\pi}{|r_{s_{i}}^{a_{i}}-r_{s_{j}}^{a_{j}}|}\\ge\\mathbb{E}_{a_{i}\\sim\\pi,a_{j}\\sim\\pi}{[r_{s_{i}}^{a_{i}}-r_{s_{j}}^{a_{j}}]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{a_{i}\\sim\\pi,a_{j}\\sim\\pi}|r_{s_{j}}^{a_{j}}-r_{s_{i}}^{a_{i}}|\\ge\\mathbb{E}_{a_{i}\\sim\\pi,a_{j}\\sim\\pi}[r_{s_{j}}^{a_{j}}-r_{s_{i}}^{a_{i}}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By combining Equation (13) and Equation (14), we can have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{a_{i}\\sim\\pi,a_{j}\\sim\\pi}|r_{s_{i}}^{a_{i}}-r_{s_{j}}^{a_{j}}|\\ge\\mathbb{E}_{a_{i}\\sim\\pi,a_{j}\\sim\\pi}[r_{s_{i}}^{a_{i}}-r_{s_{j}}^{a_{j}}]=|\\mathbb{E}_{a_{i}\\sim\\pi}r_{s_{i}}^{a_{i}}-\\mathbb{E}_{a_{j}\\sim\\pi}r_{s_{j}}^{a_{j}}|}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Similarly, we can have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{a_{i}\\sim\\pi,a_{j}\\sim\\pi}|I_{\\bar{s}_{i}}^{s_{i+1}}-I_{\\bar{s}_{j}}^{s_{i+j}}|\\ge\\mathbb{E}_{a_{i}\\sim\\pi,a_{j}\\sim\\pi}[I_{\\bar{s}_{i}}^{s_{i+1}}-I_{\\bar{s}_{j}}^{s_{i+j}}]=|\\mathbb{E}_{a_{i}\\sim\\pi}I_{\\bar{s}_{i}}^{s_{i+1}}-\\mathbb{E}_{a_{j}\\sim\\pi}I_{\\bar{s}_{j}}^{s_{j+1}}|}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "So we can get ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{\\bar{\\Sigma}}_{a_{i}\\sim\\pi,a_{j}\\sim\\pi}[|r_{s_{i}}^{a_{i}}-r_{s_{j}}^{a_{j}}|+|I_{s_{i}}^{s_{i+1}}-I_{s_{j}}^{s_{j+1}}|]\\ge|\\mathbb{E}_{a_{i}\\sim\\pi}r_{s_{i}}^{a_{i}}-\\mathbb{E}_{a_{j}\\sim\\pi}r_{s_{j}}^{a_{j}}|+|\\mathbb{E}_{a_{i}\\sim\\pi}I_{s_{i}}^{s_{i+1}}-\\mathbb{E}_{a_{j}\\sim\\pi}I_{s_{j}}^{s_{j+1}}|}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Definition 3 (Shifted LIBERTY Distance). The shifted LIBERTY distance function $\\hat{\\mathcal{H}}^{\\pi}$ is defined as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\mathcal{H}}^{\\pi}(d)(s_{i},s_{j})=\\mathbb{E}_{a_{i}\\sim\\pi}|r_{s_{i}}^{a_{i}}-r_{s_{j}}^{a_{j}}|-\\gamma W_{1}(P_{\\bar{s}_{i}}^{\\pi},P_{\\bar{s}_{j}}^{\\pi})-\\gamma|I_{\\bar{s}_{i}}^{s_{i+1}}-I_{\\bar{s}_{j}}^{s_{j+1}}|}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Define the MDP for RL by a tuple $\\langle S,A,\\mathcal{R},\\mathcal{P},\\gamma\\rangle$ . We consider a lifted MDP constructed by a tuple $\\langle\\hat{S},\\hat{A},\\hat{\\mathcal{R}},\\hat{\\mathcal{P}},\\gamma\\rangle$ , where state space $\\hat{S}=\\mathcal{S}\\times\\mathcal{S}$ , action space $\\hat{\\mathcal{A}}=\\mathcal{A}\\times\\mathcal{A}$ , transition distribution $\\hat{\\mathcal{P}}_{(s_{i},s_{j})}^{(a_{i},a_{j})}\\,=\\,P_{s_{i}}^{a_{i}}P_{s_{j}}^{a_{j}}$ , and reward function $\\begin{array}{r}{\\mathcal{\\hat{R}}((s_{i},s_{j}))\\,=\\,|\\mathbb{E}_{a_{i}\\sim\\pi}r_{s_{i}}^{a_{i}}\\,-\\,\\mathbb{E}_{a_{j}\\sim\\pi}r_{s_{j}}^{a_{j}}|}\\end{array}$ . The Bellman operator $T^{\\pi}$ under policy $\\pi(a_{i},a_{j}|s_{i},s_{j})=\\pi(a_{i}|s_{i})\\pi(a_{j}|s_{j})$ is: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T^{\\pi}(d^{\\pi})((s_{i},s_{j}))=\\displaystyle\\sum_{(a_{i},a_{j})}\\pi(a_{i},a_{j}|s_{i},s_{j})\\displaystyle\\sum_{(s_{i}^{\\prime},s_{j}^{\\prime})}\\hat{P}_{(s_{i},s_{j})}^{(a_{i},a_{j})}(s_{i}^{\\prime},s_{j}^{\\prime})\\left[\\hat{\\mathcal{R}}((s_{i},s_{j}))+\\gamma|I_{s_{i}}^{s_{i}^{\\prime}}-I_{s_{j}^{\\prime}}^{s_{j}^{\\prime}}\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=\\left|\\mathbb{E}_{a_{i}\\sim\\pi}r_{s_{i}^{\\prime}}^{a_{i}}-\\mathbb{E}_{a_{j}\\sim\\pi}r_{s_{j}^{\\prime}}^{a_{j}}\\right|+\\gamma W(P_{\\bar{s}_{i}}^{\\pi},P_{\\bar{s}_{j}}^{\\pi})+\\gamma|I_{s_{i}^{\\prime}}^{s_{i}^{\\prime}}-I_{s_{j}^{\\prime}}^{s_{j}^{\\prime}}|}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=\\mathcal{H}^{\\pi}(d^{\\pi})((s_{i},s_{j})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "So we can construct the lifted MDP of shift LIBERTY by a tuple $\\langle\\hat{S},\\hat{A},\\tilde{\\mathcal{R}},\\hat{\\mathcal{P}},\\gamma\\rangle$ which is the same as the lifted MDP in Definition 3 except for the reward function $\\Tilde{\\mathcal{R}}((s_{i},s_{j}),(a_{i},a_{j}))=|r_{s_{i}}^{a_{i}}-r_{s_{j}}^{a_{j}}|$ and shifted metric $\\tilde{d}$ . The Bellman operator ${\\tilde{T}}^{\\hat{\\pi}}$ under policy $\\hat{\\pi}$ is: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\tilde{\\mathrm{\\Delta}}^{\\tilde{\\pi}}(\\tilde{d}^{\\tilde{\\pi}})((s_{i},s_{j}))=\\sum_{\\tilde{(a_{i},a_{j})}}\\hat{\\pi}(a_{i},a_{j}|s_{i},s_{j})\\sum_{\\tilde{(s_{i}^{\\prime},s_{j}^{\\prime})}}\\hat{P}_{(s_{i},s_{j})}^{(a_{i},a_{j})}(s_{i}^{\\prime},s_{j}^{\\prime})\\left[\\tilde{\\mathcal{R}}((s_{i},s_{j}),(a_{i},a_{j}))+\\gamma|I_{s_{i}}^{s_{i}^{\\prime}}-I_{s_{j}}^{s_{j}^{\\prime}}\\right]}}\\\\ &{}&{=\\mathcal{H}^{\\tilde{\\pi}}(\\tilde{d}^{\\tilde{\\pi}})((s_{i},s_{j}))}\\\\ &{}&{\\cdots\\quad\\cdots\\quad\\cdots\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "As proven in LIBERTY [63], for the inverse dynamic bisimulation metric $d$ , we can get ", "page_idx": 15}, {"type": "equation", "text": "$$\n|V^{\\pi}(s_{i})-V^{\\pi}(s_{j})|\\leq d^{\\pi}(s_{i},s_{j})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "As proved in [63, 24, 27], $d^{\\pi}$ is the value function of lifted MDP $\\langle\\hat{S},\\hat{A},\\hat{\\mathcal{R}},\\hat{\\mathcal{P}},\\gamma\\rangle$ . $d^{\\pi}$ can be expanded as the sum of discounted future rewards, ", "page_idx": 16}, {"type": "equation", "text": "$$\nd^{\\pi}(s_{i},s_{j})=\\mathbb{E}_{\\hat{\\pi}}\\left[\\sum_{t}\\gamma^{t}\\left(\\mathbb{E}_{a_{i}^{(t)}\\sim\\pi}r_{s_{i}^{(t)}}^{a_{i}^{(t)}}-\\mathbb{E}_{a_{j}^{(t)}\\sim\\pi}r_{s_{j}^{(t)}}^{a_{j}^{(t)}}\\right)\\bigg|s_{i}^{(0)}=s_{i},s_{j}^{(0)}=s_{j}\\right].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "$\\tilde{d}^{\\pi}$ can be expanded as, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\tilde{d}^{\\pi}(s_{i},s_{j})=\\mathbb{E}_{\\hat{\\pi}}\\left[\\sum_{t}\\gamma^{t}\\left(\\left|\\mathbb{E}_{a_{i}^{(t)}\\sim\\pi}r_{s_{i}^{(t)}}^{a_{i}^{(t)}}-\\mathbb{E}_{a_{j}^{(t)}\\sim\\pi}r_{s_{j}^{(t)}}^{a_{j}^{(t)}}\\right|\\right)\\bigg|s_{i}^{(0)}=s_{i},s_{j}^{(0)}=s_{j}\\right].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with si $s_{i}^{(0)}=s_{i},s_{j}^{(0)}=s_{j}$ , The difference between $d$ and $\\tilde{d}$ can be regarded as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{t}^{\\pi}(s_{i},s_{j})-d^{\\pi}(s_{i},s_{j})=\\mathbb{E}_{\\hat{\\pi}}\\left[\\sum_{t}\\gamma^{t}\\left(\\left|\\mathbb{E}_{a_{i}^{(t)}\\sim\\pi^{T}}r_{s_{i}^{(t)}}^{a_{i}^{(t)}}-\\mathbb{E}_{a_{j}^{(t)}\\sim\\pi^{T}}r_{s_{j}^{(t)}}^{a_{j}^{(t)}}\\right|-\\left|\\mathbb{E}_{a_{i}^{(t)}\\sim\\pi^{T}}r_{s_{i}^{(t)}}^{a_{i}^{(t)}}-\\mathbb{E}_{a_{j}^{(t)}\\sim\\pi^{T}}r_{s_{j}^{(t)}}^{a_{j}^{(t)}}\\right|\\right.\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\left.\\left.=\\mathbb{E}_{\\hat{\\pi}}\\left[\\sum_{t}\\gamma^{t}\\hat{\\mathcal{R}}_{\\Delta}((s_{i}^{(t)},s_{j}^{(t)}),(a_{i}^{(t)},a_{j}^{(t)}))\\right]\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\left.\\left.\\dots\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "As proved in Lemma 2, we can get: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\tilde{d}^{\\pi}(s_{i},s_{j})-d^{\\pi}(s_{i},s_{j})\\geq0\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Combined with Equation (21), we can have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n|V^{\\pi}(s_{i})-V^{\\pi}(s_{j})|\\leq d^{\\pi}(s_{i},s_{j})\\leq\\tilde{d}^{\\pi}(s_{i},s_{j})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "So the shifted LIBERTY distance introduces a looser value bound. ", "page_idx": 16}, {"type": "text", "text": "Proposition 3. Let $r_{s}$ be a random variable over the action distribution defined by $p(r_{s}=r_{s}^{a})=$ $\\pi(a|s),\\,\\nu a r(r_{s_{i}})$ denote the variance of variable $r_{s_{i}}$ , we can have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\mathbb{E}_{a_{i}\\sim\\pi}r_{s_{i}}^{a_{i}}-\\mathbb{E}_{a_{j}\\sim\\pi}r_{s_{j}}^{a_{j}}|=\\sqrt{\\mathbb{E}_{a_{i}\\sim\\pi}\\left[\\left|r_{s_{i}}^{a_{i}}-r_{s_{j}}^{a_{j}}\\right|^{2}\\right]-\\nu a r(r_{s_{i}})-\\nu a r(r_{s_{j}})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. The proof is based on [14, 16] by expanding the difference between $\\mathbb{E}_{a_{i}\\sim\\pi,a_{j}\\sim\\pi}\\left[\\bar{\\left(r_{s_{i}}^{a_{i}}-r_{s_{j}}^{a_{j}}\\right)}^{2}\\right]$ and $\\left(\\mathbb{E}_{a_{i}\\sim\\pi}r_{s_{i}}^{a_{i}}-\\mathbb{E}_{a_{j}\\sim\\pi}r_{s_{j}}^{a_{j}}\\right)^{2}$ . ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{a_{i}\\sim\\pi,a_{j}\\sim\\pi}\\left[\\left(r_{s_{i}}^{a_{i}}-r_{s_{j}}^{a_{j}}\\right)^{2}\\right]-\\left(\\mathbb{E}_{a_{i}\\sim\\pi r}r_{s_{i}}^{a_{i}}-\\mathbb{E}_{a_{j}\\sim\\pi r}r_{s_{j}}^{a_{j}}\\right)^{2}}\\\\ &{=\\mathbb{E}_{a_{i}\\sim\\pi}\\left[\\left(r_{s_{i}}^{a_{i}}\\right)^{2}\\right]+\\mathbb{E}_{a_{j}\\sim\\pi}\\left[\\left(r_{s_{j}}^{a_{j}}\\right)^{2}\\right]-2\\mathbb{E}_{a_{i}\\sim\\pi r,a_{j}\\sim\\pi r}\\left[r_{s_{i}}^{a_{i}}r_{s_{j}}^{a_{j}}\\right]}\\\\ &{\\quad-\\left(\\left(\\mathbb{E}_{a_{i}\\sim\\pi r}r_{s_{i}}^{a_{i}}\\right)^{2}+\\left(\\mathbb{E}_{a_{j}\\sim\\pi r}r_{s_{j}}^{a_{j}}\\right)^{2}-2\\mathbb{E}_{a_{i}\\sim\\pi r}r_{s_{i}}^{a_{i}}\\mathbb{E}_{a_{j}\\sim\\pi r}r_{s_{j}}^{a_{j}}\\right)}\\\\ &{=\\mathbb{E}_{a_{i}\\sim\\pi}\\left[\\left(r_{s_{i}}^{a_{i}}\\right)^{2}\\right]-\\left(\\mathbb{E}_{a_{i}\\sim\\pi r}r_{s_{i}}^{a_{i}}\\right)^{2}+\\mathbb{E}_{a_{j}\\sim\\pi r}\\left[\\left(r_{s_{j}}^{a_{j}}\\right)^{2}\\right]-\\left(\\mathbb{E}_{a_{j}\\sim\\pi r}r_{s_{j}}^{a_{j}}\\right)^{2}}\\\\ &{\\quad-2\\left(\\mathbb{E}_{a_{i}\\sim\\pi,a_{j}\\sim\\pi r}\\left[r_{s_{i}}^{a_{i}}r_{s_{j}}^{a_{j}}\\right]-\\mathbb{E}_{a_{i}\\sim\\pi r}r_{s_{i}}^{a_{i}}\\mathbb{E}_{a_{j}\\sim\\pi r}r_{s_{j}}^{a_{j}}\\right)}\\\\ &{=\\mathrm{var}[r_{s_{i}}]+\\mathrm{var}(r_{s_{j}})-2\\mathrm{cov}(r_{s_ \n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "$r_{s_{i}}$ and $r_{s_{j}}$ are independent variables, we can get $\\mathrm{cov}(r_{s_{i}},r_{s_{j}})=0$ , so we can have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\vert\\mathbb{E}_{a_{i}\\sim\\pi}r_{s_{i}}^{a_{i}}-\\mathbb{E}_{a_{j}\\sim\\pi}r_{s_{j}}^{a_{j}}\\vert=\\sqrt{\\mathbb{E}_{a_{i}\\sim\\pi}\\left[\\left\\vert r_{s_{i}}^{a_{i}}-r_{s_{j}}^{a_{j}}\\right\\vert^{2}\\right]-\\mathrm{var}(r_{s_{i}})-\\mathrm{var}(r_{s_{j}})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. Let $d_{E},d_{E}^{\\prime}\\in\\mathbb{M}$ . We have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left|{\\mathcal F}(d_{E})\\left(s_{i},s_{j}\\right)-{\\mathcal F}\\left(d_{E}^{\\prime}\\right)\\left(s_{i},s_{j}\\right)\\right|=\\displaystyle\\left\\lvert\\gamma\\sum_{a_{i},a_{j}}\\pi\\left(a_{i}\\ \\right\\rvert s_{i}\\right)\\pi\\left(a_{j}\\ \\middle\\lvert\\ s_{j}\\right)\\left(d_{E}-d_{E}^{\\prime}\\right)\\left(s_{i}^{\\prime},s_{j}^{\\prime}\\right)\\right\\rvert}&{}\\\\ {\\leq\\gamma\\left\\lVert d_{E}-d_{E}^{\\prime}\\right\\rVert_{\\infty}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, $\\mathcal{F}$ is a contraction mapping w.r.t. the $L_{\\infty}$ norm and there exists a unique fixed-point for $\\mathcal{F}$ due to Banach\u2019s fixed-point theorem. This completes the proof. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Theorem 2. (Guaranteed Value difference bound) Given the EME metric $d_{E}$ , states $s_{i}$ and $s_{j}$ , and $a$ policy $\\pi$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n|V^{\\pi}\\left(s_{i}\\right)-V^{\\pi}\\left(s_{j}\\right)|\\leq d_{E}\\left(s_{i},s_{j}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. The proof mimics [14]. We follow the assumption that $\\begin{array}{r}{\\sum_{s^{\\prime}}P_{s}^{a}(s^{\\prime})V^{\\pi}(s^{\\prime})=V^{\\pi}(\\mathbb{E}_{s^{\\prime}\\sim P_{s}^{a}}[s^{\\prime}])}\\end{array}$ . We will first show that if $\\forall s_{i},s_{j}\\in S,|V^{\\pi}(s_{i})-V^{\\pi}(s_{j})|\\leq d(s_{i},s_{j})$ , then $|V^{\\pi}(s_{i})-V^{\\pi}(s_{j})|\\leq$ $\\mathcal{F}(d_{E},\\pi)(s_{i},s_{j})$ . Suppose $|V^{\\pi}(s_{i})-V^{\\pi}(s_{j})|\\leq d(s_{i},s_{j})$ holds, we can have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V^{\\pi}(s_{i})-V^{\\pi}(s_{j})}\\\\ &{=\\mathbb{E}_{a_{i}\\sim\\pi^{T}}s_{i}^{a_{i}}-\\mathbb{E}_{a_{j}\\sim\\pi^{T}}s_{j}^{a_{j}}+\\sum_{a_{i}}\\pi(a_{i}|s_{i})\\sum_{s_{i}^{\\prime}}P_{s_{i}}^{a_{i}}(s_{i}^{\\prime})V^{\\pi}(s_{i}^{\\prime})-\\displaystyle\\sum_{a_{j}}\\pi(a_{j}|s_{j})\\sum_{s_{j}^{\\prime}}P_{s_{j}}^{a_{j}}(s_{j}^{\\prime})V^{\\pi}(s_{j}^{\\prime})}\\\\ &{\\le\\Big|\\mathbb{E}_{a_{i}\\sim\\pi^{T}}r_{s_{i}^{a_{i}}}^{a_{i}}-\\mathbb{E}_{a_{j}\\sim\\pi^{T}}r_{s_{j}^{\\prime}}^{a_{j}}\\Big|+\\mathbb{E}_{a_{i}\\sim\\pi}\\big(\\sum_{s_{i}^{\\prime}}P_{s_{i}}^{a_{i}}(s_{i}^{\\prime})V^{\\pi}(s_{i}^{\\prime})\\big)-\\mathbb{E}_{a_{j}\\sim\\pi}\\big(\\sum_{s_{j}^{\\prime}}P_{s_{j}}^{a_{j}}(s_{j}^{\\prime})V^{\\pi}(s_{j}^{\\prime})\\big)}\\\\ &{\\le\\Big|\\mathbb{E}_{a_{i}\\sim\\pi^{T}}s_{i}^{a_{i}}-\\mathbb{E}_{a_{j}\\sim\\pi^{T}}s_{j}^{a_{j}}\\Big|+\\mathbb{E}_{a_{i}\\sim\\pi}\\left(V^{\\pi}\\left(\\mathbb{E}_{s_{i}^{\\prime}\\sim P_{s_{i}^{\\prime}}^{a_{i}}}\\left|s_{i}^{\\prime}\\right|\\right)-V^{\\pi}\\left(\\mathbb{E}_{s_{j}^{\\prime}\\sim P_{s_{j}^{\\prime}}^{a_{j}}}\\left|s_{j}^{\\prime}\\right|\\right)\\right)}\\\\ &{\\le\\Big|\\mathbb{E}_{a_{i}\\sim\\pi^{T}}s_{i}^{a_{i}}-\\mathbb{E}_{a_{j}\\sim\\pi^{T}}s_{j}^{a_{j}}\\Big|+\\mathbb{E}_{a_{i}\\sim\\pi^\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Similarly, $V^{\\pi}(s_{j})-V^{\\pi}(s_{i})\\leq{\\mathcal{F}}(d,\\pi(s_{j},s_{i}))$ , so we can have ", "page_idx": 17}, {"type": "equation", "text": "$$\n|V^{\\pi}(s_{i})-V^{\\pi}(s_{j})|\\leq\\mathcal{F}(d,\\pi)(s_{i},s_{j}))\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Assuming that we have an initial distance $d_{0}$ which $|V^{\\pi}(s_{i})-V^{\\pi}(s_{j})|\\leq d_{0}(s_{i},s_{j})$ , and base on Theorem 1, ${\\mathcal{F}}(d,\\pi)$ is contraction mapping on $d$ . By repeatedly applying ${\\mathcal{F}}(d,{\\bar{\\pi}})$ on $d,\\,d$ will eventually converge to the fixed-point $d_{E}$ , the fixed point $d_{E}$ satisfies: ", "page_idx": 17}, {"type": "equation", "text": "$$\n|V^{\\pi}(s_{i})-V^{\\pi}(s_{j})|\\leq d_{E}(s_{i},s_{j}))\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "C Additional Experiments ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "C.1 Additional Ablation Experiments ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "To isolate the impact of different hyper-parameters, we carry additional experiments on the ensemble size of reward models and the max reward scaling $M$ . First, $M$ sets an upper limit on the bonus. We set $M=10\\mathrm{as}$ the default setting. As we can see from Figure 6, if $M=1$ , the scaling factor is fixed to 1, resulting in a significant performance decline. Higher $M$ encourages more extensive exploration. The performance with $M=5$ slightly lags behind the default setting. The performance with $M=20$ and $M=40$ is comparable and almost the same, indicating that the performance stabilizes as $M$ increases. Practically, the value of $M$ can be adjusted depending on the specific task and environment to determine the intensity of exploration. ", "page_idx": 17}, {"type": "image", "img_path": "QpKWFLtZKi/tmp/33207307fc81c3fc8ffa4220ea1f07e48f2fe4e81da35a2963c4096ec5512bd8.jpg", "img_caption": ["Figure 6: Ablation study on the max reward scaling $M$ $(M=1,5,10,20,40)$ ). "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "QpKWFLtZKi/tmp/2142afe761d4be2720271e94eded32b8b35611ec7d2444e63812512b429af3a1.jpg", "img_caption": ["Figure 7: Ablation study on the ensemble size $(\\mathrm{ES}=3,6,9,12$ ) and the other baseline with best performance. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "QpKWFLtZKi/tmp/df5125b82ee32975b1b65a4214fae3c39f7bac938c098827be61a4488a928b05.jpg", "img_caption": ["Figure 8: Results for EME and its variants combined with different feature encoders. The $\\mathbf{X}$ -axis represents the number of steps (1e7) in training. The y-axis represents the mean success rate(standard deviations in shade). "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "QpKWFLtZKi/tmp/7be313a9ce1f4a5dfb6032584b26fd90b3543eef285bcec0455d9aa755452156.jpg", "img_caption": ["Figure 9: Results for EME and its variants combined with different feature encoders on more Atari games. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "As shown in Figure 7, we observe that as the ensemble size (ES) increases, the performance of ES $=6$ and $\\mathrm{ES}=9$ surpasses that of $\\mathrm{ES}=3$ . The performance of $\\mathrm{ES}=6$ and $\\mathrm{ES}=9$ is comparable, with no significant difference. However, when the size is further increased to $\\mathrm{ES}=12$ , there is a decline in performance, particularly in the Table Wiping and Pick and Place tasks. The performance change can be analyzed from two perspectives. First, the number of reward models is related to the accuracy of the reward variance prediction used to calculate the loss function. As the ensemble size increases, the overall prediction error decreases because the models can average out individual errors more effectively. This leads to a more accurate approximation of the variance $\\zeta(s_{t})$ . Second, with a larger ensemble size, the variance of reward predictions decreases, resulting in a lower scale of the exploration bonus. Thus, there is a balance or trade-off between estimation accuracy and exploration, explaining why the performance of $\\mathrm{ES}=12$ lags behind $\\mathrm{ES}=9$ . The optimal number of ensemble models may vary depending on the specific tasks and environments. Regarding computational cost, it increases with the ensemble size. Therefore, we set $\\mathrm{ES}=6$ as our default setting, where the performance is nearly the same as $\\mathrm{ES}=9$ but with lower computational overhead. It is also noteworthy that even with an ensemble size of 3, EME still outperforms the best baseline methods, further demonstrating the robustness of EME. ", "page_idx": 18}, {"type": "text", "text": "C.2 EME Combined with Feature Encoder ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Our method EME can be integrated with any state representation derived from different state encoders $\\Phi(\\cdot)$ , expressed as $d_{E}(\\Phi(s_{t}),\\Phi(s_{t+1}))$ . The encoder may include the inverse dynamic model [47, 51, 63] which isolates environmental factors that do not affect the agent\u2019s behavior, the bisimulation-based encoder [68, 35, 14, 13] which learns a compact state representation grouped by the bisimulation metric, and random embeddings captured by random embedded states. We denote the EME variants as follows: EME with an inverse dynamic model-based encoder is referred to as EME-IDM; with a bisimulation metric-based encoder as EME-BM; and with a random embedding as EME-Random. The results of continuous control tasks and hard exploration atari games can be found in Figure 8 and Figure 9, respectively. EME\u2019s performance declines due to the rapid reduction of the exploration bonus. The performance of EME under the inverse dynamic encoder and bisimulation-based encoder exhibits a decline, because the exploration bonus under these encoders gets close to zero quickly since their representations are very compact and every state looks more similar with the convergence of the encoder, which harms the efficacy of exploration. With respect to random embedding, the variance is larger, resulting in more unstable performance. ", "page_idx": 19}, {"type": "image", "img_path": "QpKWFLtZKi/tmp/ab313eb6c7c71f117bad9aeb9c33144c3a3723c0142878c59ffdd74bcc1b7dbd.jpg", "img_caption": ["Figure 10: Trajectories of policies trained with different exploration algorithms on the Habitat environment. Our method EME reveals the largest portion of the map than other methods. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "QpKWFLtZKi/tmp/6025735de914c62fff9f9d5154cd6b638c0240bc85338e60e8c6df73b6469726.jpg", "img_caption": ["Figure 11: Results of reward-free exploration tasks on Habitat. Error bars represent std, deviations over 5 seeds. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "C.3 Reward-free Exploration ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We examine reward-free exploration by assessing the performance of all baselines within Habitat using only exploration bonuses. During training, agents are initialized in varied environments for each episode and are subsequently tested on a set of environments not encountered during training. Figure 10 displays the trajectories of all baselines on one of the test maps, highlighting that EME explores a substantial portion of the space, unlike other methods. Specifically, ICM and RND agents remain confined to a single room, while RIDE, LIBERTY, and NovelD agents explore two rooms before becoming stuck, distant from the goal. E3B exhibits the second-best exploration coverage of the map. Quantitative results from other reward-free tasks are shown in Figure 11, where EME consistently demonstrates robust performance and achieves the best outcomes. In contrast, other methods experience significant declines, underscoring the efficacy of our approach in a reward-free setting. ", "page_idx": 20}, {"type": "table", "img_path": "QpKWFLtZKi/tmp/2bfab6063c86ec9fd5a95663bbb51cf10850a8dd702b193d1b55044c6fc7b749.jpg", "table_caption": ["Table 4: Table of quantitative results comparison between EME and other baseline methods in different environments of Mujoco with the delayed reward setting. The best and the runner-up results are (bold) and (underline) "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "C.4 Delayed Reward Setting of Mujoco ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We present results from the delayed reward setting in the MuJoCo environment [21], following the methodology outlined in [63], where accumulated rewards are delayed every 10, 20, and 30 steps. The experiments assess six tasks: HalfCheetah, Hopper, Walker2d, Ant, Swimmer, and Humanoid. As shown in Table 4, our method achieves the best or second-best performance in 15 out of 18 delayed reward tasks. This indicates that EME can facilitate effective exploration and maintain high performance even under sparse reward conditions. Notably, as rewards become sparser, EME\u2019s performance improves and becomes more robust. In contrast, other metric-based exploration bonus methods such as RIDE and NovelD struggle due to their less expressive metrics. Curiosity-driven methods like ICM and RND also become less effective as rewards grow sparser. These results provide further evidence of EME\u2019s ability to promote efficient exploration effectively, even in environments characterized by delayed rewards. ", "page_idx": 20}, {"type": "image", "img_path": "QpKWFLtZKi/tmp/66e087f04e296564c31424d1058ecfe290b39524bb1616b1b4bc3bd983605605.jpg", "img_caption": ["Figure 12: The scale of exploration bonus across all MiniGrid environments "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "QpKWFLtZKi/tmp/98de741aab4290034c1e08c02d50e2ec641787acf97adb35942a04ee0615e842.jpg", "img_caption": ["Figure 13: Results of EME, EME with a static scaling factor, and EME with an episodic count from Robosuite. The $\\mathbf{X}$ -axis represents the number of steps (1e7) in training. The y-axis represents the mean success rate(standard deviations in shade). "], "img_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "QpKWFLtZKi/tmp/0d56266bbb16cbf637498c97eaac86622b9e3786a662d1a9dd06283b4a642ffd.jpg", "table_caption": ["Table 5: Average testing results of different Minigrid environments for EME and its variants. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "C.5 Ablation Study on Scaling Factor ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "To demonstrate the effectiveness of our proposed scaling factor, we conducted an ablation study focusing on this component. We compared EME with two variants: \u2019EME-EP,\u2019 which incorporates episodic counts, and \u2019EME-Static,\u2019 which uses a static scaling factor. The results from continuous tasks in the Robosuite and MiniGrid environments are illustrated in Figure 13, Figure 10 and summarized in Table 5, respectively. Without the diversity-enhanced scaling factor, EME shows a noticeable decline in performance. \u2019EME-EP\u2019 performs better in MiniGrid environments compared to continuous tasks in Robosuite, highlighting the efficacy of episodic state visitation counts in grid-based games but demonstrating limited effectiveness in environments with high-dimensional states. Additionally, we visualize the exploration bonus of all MiniGrid environments in Figure 12. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "D Implementation Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "D.1 Environments Settings ", "text_level": 1, "page_idx": 22}, {"type": "image", "img_path": "QpKWFLtZKi/tmp/f8e1f0bb4e2f5c4d5888850c3a5fd1fcd41b94eac0246ecce411dba291b28dfe.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "D.1.1 Robosuite Manipulation Tasks ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We choose the three manipulation tasks in the Robosuite platform [70]. ", "page_idx": 22}, {"type": "text", "text": "Table Wiping. The environment consists of a table with a whiteboard surface and some markings is placed in front of a single robot arm, which has a whiteboard eraser mounted on its hand. The shape of the dirty region is correlated to the position of the cube. When the dirty region is diagonal, the cube is on the right-hand side of the robot arm. The goal of the agent is to learn to wipe the whiteboard surface and clean all of the markings. The whiteboard markings are randomized at the beginning of each episode. ", "page_idx": 22}, {"type": "text", "text": "Door Opening. The environment consists of a door with a handle mounted in free space in front of a single robot arm. The height of the handle and the position of the door are correlated. When the door is closed to the robot arm, the handle is in a low position. When the door is far from the robot arm, the handle is in a high position. The goal of the agent is to learn to turn the handle and open the door. The initial state distribution of the door location is randomized at the beginning of each episode. ", "page_idx": 22}, {"type": "text", "text": "Pick and Place. The environment consists of four objects placed in a bin in front of a single robot arm. There are four containers next to the bin. The goal of the agent is to place each object into its corresponding container. This task also has easier single-object variants. The initial state distribution of object locations is randomized at the beginning of each episode. ", "page_idx": 22}, {"type": "text", "text": "D.1.2 MiniGrid Environments ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "MiniGrid [17] is a set of procedurally-generated grid-worlds. In MiniGrid, the world is a partially observable grid of size $N\\times N$ . Each tile in the grid contains exactly zero or one object. The possible object types are wall, door, key, ball, box, and goal. Each object in MiniGrid has an associated discrete color, which can be one of red, green, blue, purple, yellow, or grey. By default, walls are always grey and goal squares are always green. Rewards are sparse for all MiniGrid environments. There are seven actions in MiniGrid: turn left, turn right, move forward, pick up an object, drop an object, toggle and done. The agent can use the turn left and turn right action to rotate and face one of 4 possible directions (north, south, east, west). ", "page_idx": 22}, {"type": "text", "text": "The move forward action makes the agent move from its current tile onto the tile in the direction it is currently facing, provided there is nothing on that tile, or that the tile contains an open door. The agent can open doors if they are right in front of it by using the toggle action. Observations in MiniGrid are partial and egocentric. By default, the agent sees a square of $7\\times7$ tiles in the direction it is facing. These include the tile the agent is standing on. The agent cannot see through walls or closed doors. The observations are provided as a tensor of shape $7\\times7\\times3$ . However, note that these are not RGB images. Each tile is encoded using 3 integer values: one describing the type of object contained in the cell, one describing its color, and a flag indicating whether doors are open or closed. This compact encoding was chosen for space efficiency and to enable faster training. For all tasks, the agent gets an egocentric view of its surroundings, consisting of $3\\times3$ pixels. A neural network parameterized as a CNN is used to process the visual observation. ", "page_idx": 23}, {"type": "image", "img_path": "QpKWFLtZKi/tmp/4249af8634c63fa821ba0e3393b0952af9e42c8d9f9affd1ddc584806012fb3b.jpg", "img_caption": ["Figure 14: Rendering of the MultiRoomN12S10 in MiniGrid. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "MultiRoom. The MultiRoomNXSY environment consists of X rooms, with a size at most Y, connected in random orientations. The agent is placed in the first room and must navigate to a green goal square in the most distant room from the agent. The agent receives an egocentric view of its surroundings, consisting of $3\\times3$ pixels. The task increases in difficulty with X and Y. Episodes finish with a positive reward when the agent reaches the green goal square. Otherwise, episodes are terminated with zero reward after a maximum of $20\\times N$ steps. ", "page_idx": 23}, {"type": "text", "text": "KeyCorridor. the agent has to pick up an object which is behind a locked door. The key is hidden in another room, and the agent has to explore the environment to find it. Episodes finish with a positive reward when the agent picks up the ball behind the locked door or after a maximum of 270 steps. ", "page_idx": 23}, {"type": "text", "text": "ObstructedMaze. The agent has to pick up a box which is placed in a corner of a maze. The doors are locked, the keys are hidden in boxes and the doors are obstructed by balls. Episodes finish with a positive reward when the agent picks up the ball behind the door or after a maximum of 576 steps. ", "page_idx": 23}, {"type": "image", "img_path": "QpKWFLtZKi/tmp/dcf7b2f89f730c63d3512993a97e549f78e78dedfd28f6305da694084172a306.jpg", "img_caption": ["Figure 15: Visual observations in Habitat (RGB, semantic, and depth) "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "D.1.3 Habitat ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The Habitat platform relies on several key abstractions that model the domain of embodied agents and the tasks they can perform in three-dimensional indoor simulation environments. ", "page_idx": 23}, {"type": "text", "text": "This platform consists of a virtually embodied agent, such as a robot, equipped with a suite of sensors that can observe the environment and take actions to alter the agent\u2019s state or the environment\u2019s state. Each sensor is associated with a specific agent and can return observation data from the environment at a specified frequency. The 3D environment includes a scene mesh, objects, agents, and sensors, organized into regions and objects through a hierarchical representation called the Scene, which can be programmatically manipulated. All components of the Scene are present on the SceneGraph. Additionally, a simulator backend instance can update the state of agents and SceneGraphs based on given actions for a set of configured agents and SceneGraphs, and it provides observations for all active sensors possessed by the agents. We use the Matterport3D (MP3D) dataset [15], which consists of high-quality renderings of indoor scenes. To measure exploration coverage, we compute the area revealed by the agent\u2019s line of sight using the function provided by the Habitat codebase [32]. As depicted in Figure 16, exploration is measured as the proportion of the environment revealed by the agent\u2019s line of sight over the course of the episode. ", "page_idx": 23}, {"type": "image", "img_path": "QpKWFLtZKi/tmp/f513560097f49b75c81c6df529d009bf17b50ace25bdda62140691a3506bfdec.jpg", "img_caption": ["Figure 16: Explored area in Habitat "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "D.2 Algorithm Details ", "text_level": 1, "page_idx": 24}, {"type": "table", "img_path": "QpKWFLtZKi/tmp/91a556fd9abed0e9483ab01b57d1cdcaeb3b97d349f4ef67be210db99459cf3b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "D.3 Computation Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "All of our experiments are conducted on 4 GPUs with 16 CPU threads, which include AMD Ryzen 9 $\\mathrm{CPU}@1.20\\mathrm{GHz}$ (16 core) CPU, NVIDIA GeForce GTX 3080Ti GPUs, and 64GB memory. ", "page_idx": 24}, {"type": "text", "text": "D.4 RL Hyperparameters ", "text_level": 1, "page_idx": 24}, {"type": "table", "img_path": "QpKWFLtZKi/tmp/b38a4954374e46a00008b405452f22e46fe6bf8b695378e9331cb0e65302ca1d.jpg", "table_caption": ["Table 6: Hyper-parameters in Robosuite experiments "], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "QpKWFLtZKi/tmp/ea7c07212b138374b360079744d22a44cb8e6fec8e79bc205d891f56ea5ea163.jpg", "table_caption": ["Table 7: Common Hyperparameters for MiniGrid "], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "QpKWFLtZKi/tmp/5a96dfa0f1e501fbc4f9a7cb3410c0230eb30ca9a4bae4788b759bd7e82fd6d1.jpg", "table_caption": ["Table 8: Common Hyperparameters for Habitat "], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "QpKWFLtZKi/tmp/572c461659eec415685d9c2e69750e832cf9d281283b35c90bbe43b6039ecda6.jpg", "table_caption": [], "table_footnote": ["Table 9: The hyperparameters of the tested benchmark algorithms in the Atari experiment "], "page_idx": 26}, {"type": "text", "text": "D.5 Codebases Used ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Our codebase was built atop the following codebases: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The official NovelD codebase: https://github.com/tianjunz/NovelD (Creative Commons Attribution-NonCommercial 4.0 license) for NovelID, RND, RIDE and count-based baselines (this codebase is built atop the official RIDE codebase below)   \n\u2022 The official RIDE codebase: https://github.com/facebookresearch/impact-driven-exploration (Creative Commons Attribution-NonCommercial 4.0 license)   \n\u2022 The official LIBERTY codebase: https://github.com/Mingle0228/liberty (MIT License)   \n\u2022 The official E3B codebase: https://github.com/facebookresearch/e3b(Creative Commons Attribution-NonCommercial 4.0 license) ", "page_idx": 27}, {"type": "text", "text": "E Limitation ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "While our method has shown success in several challenging exploration tasks, it does not consistently achieve top performance in environments with less sparse rewards, as observed in the MuJoCo continuous control benchmarks. Additionally, our method has not been tested in reinforcement learning domains characterized by large action spaces. Addressing these limitations and developing a more generalized solution for diverse RL environments remains a goal for future work. ", "page_idx": 27}, {"type": "text", "text": "F Related Work ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Exploration in RL. Exploration remains a long-standing problem in RL. Common approaches include $\\epsilon$ -greedy [59], count-based exploration [7, 46, 61, 41, 40, 57], ensemble-based exploration [45, 39, 48] and curiosity-based exploration [53, 55, 56, 10]. Several exploration strategies use a dynamics model to provide intrinsic rewards [47, 11, 33, 48, 36, 2]. Latent variable dynamics have also been studied for exploration [4, 9, 62]. Maximum entropy in the state representation has also been used for exploration [54, 66]. Other intrinsic motivation methods have recently been developed for exploration in context MDPs [67, 28, 51, 69, 32], which automatically generate curricula over variations of the MDP to encourage efficient learning, effectively performing a form of curiositydriven exploration in the context space, including goal-conditioned [23, 50, 12, 18, 22] and goal-free variants [58, 49, 34, 19]. Reward shaping refers to modifying the original reward function with a shaping reward function which incorporates domain knowledge. Considering the most general form, namely the additive form, of reward shaping. The first approach to guarantee policy invariance is potential-based reward shaping (PBRS) [44], which defines the shaping reward function as the difference between values assessed through the potential function based on prior knowledge. There are lots of variants of PBRS [20, 38, 31, 3, 64, 30, 29, 63] which have shifted their focus to different areas within the field of reinforcement learning. ", "page_idx": 27}, {"type": "text", "text": "Metric-based Exploration Bonus. Metric space is widely used in encoding state representations [68, 14, 13, 16, 35]. Metric-based exploration bonus is based on the evaluation metric that quantifies the degree of difference or distance between two projected states for the measure of novelty. RIDE [51] evaluates the novelty as between two successive state representations under the $L_{2}$ norm. NovelD [69] uses the disparity in RND bonuses between adjacent states under the $L_{1}$ distance as the exploration bonus. LIBERTY [63] uses the difference between potential functions of adjacent states under the bisimulation metric space. Note that we propose an effective metric that measures the distribution distance between dynamics models by computing the distance between sampled subsequent states. Additionally, we integrate the Kullback\u2013Leibler (KL) divergence between policy distributions to more robustly model the \u201cbehavioral similarity\u201d between states, which is more effective and scalable across diverse environments with different observations, especially in addressing the critical Noisy-TV problem during exploration. ", "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope?[Yes] In the introduction section (page 1).   \n2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors?[Yes] In Appendix E.   \n3. Theory Assumptions and Proofs Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?[Yes] In the Appendix B.   \n4. Experimental Result Reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?[Yes] In Appendix D.   \n5. Open access to data and code Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?[Yes] In the anonymous link https://anonymous.4open.science/r/EME-4F8B/README.md.   \n6. Experimental Setting/Details Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?[Yes] In Appendix D.4.   \n7. Experiment Statistical Significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?[Yes] In experiment section (page 5) and Appendix C.   \n8. Experiments Compute Resources Question: For each experiment, does the paper provide sufficient information on the computer resources (type of computing workers, memory, time of execution) needed to reproduce the experiments?[Yes] In Appendix D.3.   \n9. Code Of Ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/ EthicsGuidelines?[Yes]   \n10. Broader Impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?[Yes] In Appendix A.   \n11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pre-trained language models, image generators, or scraped datasets)?[NA]   \n12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited, and are the license and terms of use explicitly mentioned and properly respected? [Yes] In Appendix D.5.   \n13. New Assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?[NA]   \n14. Crowdsourcing and Research with Human Subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? [NA]   \n15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?[NA] ", "page_idx": 28}]