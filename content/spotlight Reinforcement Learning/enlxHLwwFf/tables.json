[{"figure_path": "enlxHLwwFf/tables/tables_5_1.jpg", "caption": "Table 1: Cost in time and memory for performing a single total gradient estimation using either AID or FuncID and assuming the prediction model is learned. Time cost: Th and Ta represent the time cost of evaluating both prediction and adjoint models h and a, while Tin is the time cost for evaluating the inner objective once the outputs of h are computed. The factors \u03b3 and \u03b4 are multiplicative overheads for evaluating hessian-vector products and gradient. Memory cost: Mh and Ma represent the memory cost of storing the intermediate outputs of h and a, pin and d\u2082 are the memory costs of storing the Hessian-vector product for AID and FuncID respectively and \u03b2 is a multiplicative constant that depends on a particular implementation.", "description": "This table compares the computational cost (time and memory) of using AID vs FuncID for total gradient estimation in bilevel optimization.  It breaks down the costs into components related to prediction and adjoint model evaluations, inner objective computations, and overheads for Hessian-vector products and gradient calculations.  The table highlights the potential memory savings of FuncID over AID, particularly for deep networks, because FuncID does not require the computationally expensive computation of Hessian-vector products for the prediction network.", "section": "Methods for Functional Bilevel Optimization in L2 Spaces"}, {"figure_path": "enlxHLwwFf/tables/tables_6_1.jpg", "caption": "Table 1: Cost in time and memory for performing a single total gradient estimation using either AID or FuncID and assuming the prediction model is learned. Time cost: Th and Ta represent the time cost of evaluating both prediction and adjoint models h and a, while Tin is the time cost for evaluating both inner objectives once the outputs of h and a are computed. The factors \u03b3 and \u03b4 are multiplicative overheads for evaluating hessian-vector products and gradient. Memory cost: Mh and Ma represent the memory cost of storing the intermediate outputs of h and a, pin and d\u2082 are the memory costs of storing the Hessian-vector product for AID and FuncID respectively and \u03b2 is a multiplicative constant that depends on a particular implementation.", "description": "This table compares the computational cost (time and memory) of using AID and FuncID methods for a single total gradient estimation. It breaks down the cost into the time for evaluating the prediction and adjoint models, the time for inner objective evaluation, and overheads for Hessian-vector products and gradients. The memory cost is analyzed in terms of the cost of storing intermediate outputs, Hessian-vector products, and model parameters.", "section": "Methods for Functional Bilevel Optimization in L2 Spaces"}, {"figure_path": "enlxHLwwFf/tables/tables_40_1.jpg", "caption": "Table 1: Cost in time and memory for performing a single total gradient estimation using either AID or FuncID and assuming the prediction model is learned. Time cost: Th and Ta represent the time cost of evaluating both prediction and adjoint models h and a, while Tin is the time cost for evaluating the inner objective once the outputs of h are computed. The factors \u03b3 and \u03b4 are multiplicative overheads for evaluating hessian-vector products and gradient. Memory cost: Mh and Ma represent the memory cost of storing the intermediate outputs of h and a, pin and d\u2082 are the memory costs of storing the Hessian-vector product for AID and FuncID respectively and \u03b2 is a multiplicative constant that depends on a particular implementation.", "description": "This table compares the computational cost (time and memory) of using AID vs FuncID for estimating the total gradient in bilevel optimization.  It breaks down the costs into components related to model evaluation, inner-objective computations and Hessian-vector products.  The table shows that FuncID generally has a lower computational cost, particularly in terms of memory, especially when the prediction model is a large neural network.", "section": "Methods for Functional Bilevel Optimization in L2 Spaces"}]