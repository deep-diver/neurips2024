[{"figure_path": "gojL67CfS8/figures/figures_0_1.jpg", "caption": "Figure 1: Generated samples from Visual AutoRegressive (VAR) transformers trained on ImageNet. We show 512x512 samples (top), 256x256 samples (middle), and zero-shot image editing results (bottom).", "description": "This figure showcases image generation results from the Visual Autoregressive (VAR) model. The top row displays images generated at a resolution of 512x512 pixels. The middle row shows images generated at a lower resolution of 256x256 pixels. The bottom row demonstrates the model's zero-shot image editing capabilities.  The figure highlights the model's ability to generate high-quality images across different resolutions and perform image editing tasks without explicit training.", "section": "Abstract"}, {"figure_path": "gojL67CfS8/figures/figures_1_1.jpg", "caption": "Figure 2: Standard autoregressive modeling (AR) vs. our proposed visual autoregressive modeling (VAR). (a) AR applied to language: sequential text token generation from left to right, word by word; (b) AR applied to images: sequential visual token generation in a raster-scan order, from left to right, top to bottom; (c) VAR for images: multi-scale token maps are autoregressively generated from coarse to fine scales (lower to higher resolutions), with parallel token generation within each scale. VAR requires a multi-scale VQVAE to work.", "description": "This figure compares three different autoregressive generative models: standard autoregressive text generation (next-token prediction), standard autoregressive image generation (next-image-token prediction), and the proposed visual autoregressive image generation (next-scale prediction). The figure illustrates how the input data is processed in each model and highlights the key differences, especially in how VAR leverages a multi-scale VQVAE for efficient and high-resolution image generation.", "section": "Introduction"}, {"figure_path": "gojL67CfS8/figures/figures_1_2.jpg", "caption": "Figure 3: Scaling behavior of different model families on ImageNet 256\u00d7256 generation benchmark. The FID of the validation set serves as a reference lower bound (1.78). VAR with 2B parameters reaches an FID of 1.73, surpassing L-DiT with 3B or 7B parameters.", "description": "This figure compares different image generation models on the ImageNet 256x256 dataset using Fr\u00e9chet Inception Distance (FID) as a metric for image quality.  It demonstrates how VAR (Visual Autoregressive) model significantly outperforms other models, including various Autoregressive (AR) models and Diffusion Transformers (DiT). Specifically, the VAR model with 2 billion parameters achieves a FID of 1.73, which is substantially lower than other models, indicating superior image quality.  The x-axis represents the inference time, showing that VAR is also more efficient.", "section": "1 Introduction"}, {"figure_path": "gojL67CfS8/figures/figures_4_1.jpg", "caption": "Figure 4: VAR involves two separated training stages. Stage 1: a multi-scale VQ autoencoder encodes an image into K token maps R = (r1,r2,...,rk) and is trained by a compound loss (5). For details on \"Multi-scale quantization\" and \"Embedding\", check Algorithm 1 and 2. Stage 2: a VAR transformer is trained via next-scale prediction (6): it takes ([s], r1,2,...,K\u22121) as input to predict (r1,2,3,...,rk). The attention mask is used in training to ensure each re can only attend to r\u2264k. Standard cross-entropy loss is used.", "description": "This figure illustrates the two-stage training process of the Visual Autoregressive (VAR) model. Stage 1 involves training a multi-scale VQ autoencoder to encode images into multiple token maps.  Stage 2 trains a VAR transformer using a next-scale prediction approach, where the transformer predicts the next higher-resolution token map based on previous maps.  The use of a causal attention mask is highlighted.", "section": "3 Method"}, {"figure_path": "gojL67CfS8/figures/figures_5_1.jpg", "caption": "Figure 7: Scaling model size N and training compute C improves visual fidelity and soundness. Zoom in for a better view. Samples are drawn from VAR models of 4 different sizes and 3 different training stages. 9 class labels (from left to right, top to bottom) are: flamingo 130, arctic wolf 270, macaw 88, Siamese cat 284, oscilloscope 688, husky 250, mollymawk 146, volcano 980, and catamaran 484.", "description": "This figure visualizes the impact of scaling up both model size and training compute on the quality of images generated by the VAR model. It shows image samples generated by VAR models with different depths (6, 16, 26, 30) at various training stages (20%, 60%, 100% of total tokens). The improved visual fidelity and coherence demonstrate a clear positive correlation between model scale and image quality.", "section": "A Visualization of scaling effect"}, {"figure_path": "gojL67CfS8/figures/figures_7_1.jpg", "caption": "Figure 5: Scaling laws with VAR transformer size N, with power-law fits (dashed) and equations (in legend). Small, near-zero exponents a suggest a smooth decline in both test loss L and token error rate Err when scaling up VAR transformer. Axes are all on a logarithmic scale. The Pearson correlation coefficients near -0.998 signify a strong linear relationship between log(N) vs. log(L) or log(N) vs. log(Err).", "description": "This figure shows the scaling laws observed when training VAR transformers of different sizes.  The plots demonstrate a power-law relationship between the model's size (number of parameters) and its performance metrics: test loss and token error rate. The near-perfect correlation coefficients highlight the strong linear relationship between the logarithm of the model size and the logarithm of the loss/error, validating the scalability of the VAR model.", "section": "5.2 Power-law scaling laws"}, {"figure_path": "gojL67CfS8/figures/figures_8_1.jpg", "caption": "Figure 6: Scaling laws with optimal training compute Cmin. Line color denotes different model sizes. Red dashed lines are power-law fits with equations in legend. Axes are on a logarithmic scale. Pearson coefficients near -0.99 indicate strong linear relationships between log(Cmin) vs. log(L) or log(Cmin) vs. log(Err).", "description": "This figure shows the scaling laws observed when training compute (Cmin) is optimized.  The plots demonstrate a strong power-law relationship between compute and both test loss (L) and token error rate (Err), regardless of whether the loss is calculated for all scales or just the final scale. The high correlation coefficients (near -0.99) confirm the strong linear relationship between logarithmic values of Cmin, L, and Err, providing strong evidence for the power-law scaling behavior of VAR models.", "section": "5.2 Power-law scaling laws"}, {"figure_path": "gojL67CfS8/figures/figures_10_1.jpg", "caption": "Figure 7: Scaling model size N and training compute C improves visual fidelity and soundness. Zoom in for a better view. Samples are drawn from VAR models of 4 different sizes and 3 different training stages. 9 class labels (from left to right, top to bottom) are: flamingo 130, arctic wolf 270, macaw 88, Siamese cat 284, oscilloscope 688, husky 250, mollymawk 146, volcano 980, and catamaran 484.", "description": "This figure visualizes the effect of scaling up the model size (N) and training compute (C) on the quality of images generated by the VAR model.  It shows image samples generated by VAR models with different depths (16, 20, 24, 30) and training stages (20%, 60%, 100% of training tokens).  The improved visual fidelity and detail in images from larger models and with more training demonstrate the scaling law behaviour of the model.", "section": "A Visualization of scaling effect"}, {"figure_path": "gojL67CfS8/figures/figures_11_1.jpg", "caption": "Figure 8: Zero-shot evaluation in downstream tasks containing in-painting, out-painting, and class-conditional editing. The results show that VAR can generalize to novel downstream tasks without special design and finetuning. Zoom in for a better view.", "description": "This figure shows the zero-shot generalization ability of the Visual Autoregressive (VAR) model on three downstream tasks: image in-painting, out-painting, and class-conditional image editing.  In each task, the model is given an image with masked or specified regions, and it successfully generates realistic and coherent results without any further training or fine-tuning. This demonstrates that the VAR model learns a generalizable representation of images that can be applied to various tasks.", "section": "5.2 Zero-shot task generalization"}, {"figure_path": "gojL67CfS8/figures/figures_11_2.jpg", "caption": "Figure 9: Token dependency plotted. The normalized heat map of attention scores in the last self-attention layer of VQGAN encoder is visualized. 4 random 256\u00d7256 images from ImageNet validation set are used.", "description": "This figure visualizes the attention scores in the last self-attention layer of the VQGAN encoder.  The heatmaps show how strongly each token attends to other tokens in four randomly selected 256x256 images from ImageNet's validation set. This helps illustrate the level of token dependency in the VQGAN model.", "section": "C Token dependency in VQVAE"}, {"figure_path": "gojL67CfS8/figures/figures_13_1.jpg", "caption": "Figure 10: Model comparison on ImageNet 256x256 benchmark. More generated 512x512 samples by VAR can be found in the submitted Supplementary Material zip file.", "description": "This figure compares image generation results from four different models: BigGAN, VQVAE-2, MaskGIT, and VAR (the authors' model). Each model generated images of Bald Eagles, Jellyfish, and Ducks. The comparison highlights the superior quality and detail of the images generated by the VAR model.", "section": "5.1 State-of-the-art image generation"}, {"figure_path": "gojL67CfS8/figures/figures_14_1.jpg", "caption": "Figure 11: Some generated 256x256 samples by VAR trained on ImageNet. More generated 512x512 samples by VAR can be found in the submitted Supplementary Material zip file.", "description": "This figure shows a collection of 256x256 images generated using the Visual Autoregressive (VAR) model, trained on the ImageNet dataset.  The images depict a wide variety of subjects and scenes, demonstrating the model's ability to generate diverse and visually appealing images. The caption also indicates that higher resolution (512x512) samples are available in supplementary material.", "section": "Empirical Results"}]