[{"figure_path": "Ddak3nSqQM/tables/tables_6_1.jpg", "caption": "Table 1: Performance of different policies in Tic-Tac-Toe. Each policy plays as 'X' and moves first, tested across 100 matches (50 for LLM-based methods).", "description": "This table presents the win, draw, loss rates and net win rates of different Tic-Tac-Toe playing policies.  Each policy starts as 'X' and goes first. The policies compared are LLM-as-agent, LLM-RAG, URI (the authors' proposed method), Minimax (optimal policy), and a Minimax policy with added noise.  The table shows the head-to-head performance of each policy against the others, highlighting URI's superior performance in achieving high net win rates against various baselines, even when compared to a near-optimal policy with added randomness.", "section": "6.2 Policy Performance Evaluation"}, {"figure_path": "Ddak3nSqQM/tables/tables_7_1.jpg", "caption": "Table 2: Performance Comparison of Different Policies Against Built-in AI Levels in a GRF 11 vs 11 settings, where the performance of URI is averaged among three different seeds, LLM-as-agent, LLM-RAG is tested with 10 matches, and URI policy and random policy is tested with 40 matches.", "description": "This table compares the performance of different policy approaches against various levels of built-in AI opponents within the Google Research Football (GRF) 11 vs 11 game environment.  The policies tested include LLM-as-agent, LLM-RAG, Random Policy, and the proposed URI method. Results are shown in terms of win rate, draw rate, loss rate, and goal difference per match (GDM), averaged over multiple matches for each policy and AI difficulty level.", "section": "6.2 Policy Performance Evaluation"}, {"figure_path": "Ddak3nSqQM/tables/tables_9_1.jpg", "caption": "Table 2: Performance Comparison of Different Policies Against Built-in AI Levels in a GRF 11 vs 11 settings, where the performance of URI is averaged among three different seeds, LLM-as-agent, LLM-RAG is tested with 10 matches, and URI policy and random policy is tested with 40 matches.", "description": "This table compares the performance of different policy approaches (LLM-as-Agent, LLM-RAG, Random Policy, URI, and Rule-based-AI) against various difficulty levels (Easy, Medium, Hard) of built-in AI opponents in the Google Research Football environment.  The table shows win rates, draw rates, loss rates, and goal difference per match (GDM) for each policy and difficulty level.  The URI policy's performance is averaged over three different random seeds.", "section": "6.2 Policy Performance Evaluation"}, {"figure_path": "Ddak3nSqQM/tables/tables_16_1.jpg", "caption": "Table 1: Performance of different policies in Tic-Tac-Toe. Each policy plays as 'X' and moves first, tested across 100 matches (50 for LLM-based methods).", "description": "This table presents the performance comparison of different Tic-Tac-Toe playing policies.  Each policy starts as 'X' and makes the first move. The results are based on 100 matches played for each policy; however, for LLM-based methods, only 50 matches were used for performance evaluation. The table shows win rate (W), draw rate (D), and loss rate (L) for each policy and the net win rate (W-L) representing the difference between the win rate and loss rate.", "section": "6.2 Policy Performance Evaluation"}, {"figure_path": "Ddak3nSqQM/tables/tables_27_1.jpg", "caption": "Table 1: Performance of different policies in Tic-Tac-Toe. Each policy plays as 'X' and moves first, tested across 100 matches (50 for LLM-based methods).", "description": "This table presents the win rate, draw rate, loss rate, and net win rate for different Tic-Tac-Toe playing policies, including LLM-as-agent, LLM-RAG, URI (the proposed method), Minimax (optimal policy), and Random Policy.  Each policy plays as 'X' and goes first. The results are based on 100 matches for each policy (50 for the LLM-based methods), demonstrating the relative performance of different approaches in this relatively simple game.", "section": "6.2 Policy Performance Evaluation"}, {"figure_path": "Ddak3nSqQM/tables/tables_36_1.jpg", "caption": "Table 1: Performance of different policies in Tic-Tac-Toe. Each policy plays as 'X' and moves first, tested across 100 matches (50 for LLM-based methods).", "description": "This table presents the performance comparison of different policies in the game of Tic-Tac-Toe.  Each policy plays as 'X' and makes the first move. The results are based on 100 matches for each policy, with LLM-based methods using 50 matches. The table shows the win rate (W), draw rate (D), loss rate (L), and the net win rate (W-L) for each policy against various opponents. The policies compared include LLM-as-agent, LLM-RAG, URI (the proposed method), Minimax (the optimal policy), and Random Policy.", "section": "6.2 Policy Performance Evaluation"}, {"figure_path": "Ddak3nSqQM/tables/tables_42_1.jpg", "caption": "Table 1: Performance of different policies in Tic-Tac-Toe. Each policy plays as 'X' and moves first, tested across 100 matches (50 for LLM-based methods).", "description": "This table presents the win, draw, loss rates and win-loss difference of different Tic-Tac-Toe policies, tested in 100 matches.  Each policy plays as 'X' and starts first. The policies include LLM-as-agent, LLM-RAG, the proposed URI method, a minimax policy (optimal strategy), and a noisy minimax policy which adds 30% randomness to mimic imperfect human play. The table shows URI performs comparably to optimal minimax against different opponents while other baseline methods significantly underperform.", "section": "6.2 Policy Performance Evaluation"}, {"figure_path": "Ddak3nSqQM/tables/tables_43_1.jpg", "caption": "Table 1: Performance of different policies in Tic-Tac-Toe. Each policy plays as 'X' and moves first, tested across 100 matches (50 for LLM-based methods).", "description": "This table compares the performance of different game-playing policies in the game of Tic-Tac-Toe.  Each policy starts as player X and goes first.  The table shows the win, draw, and loss rates for each policy against several opponents.  The \"W-L\" column represents the net win rate (win rate minus loss rate). The results are based on 100 matches for each policy, with 50 matches used for the LLM-based methods.", "section": "6.2 Policy Performance Evaluation"}, {"figure_path": "Ddak3nSqQM/tables/tables_43_2.jpg", "caption": "Table 1: Performance of different policies in Tic-Tac-Toe. Each policy plays as 'X' and moves first, tested across 100 matches (50 for LLM-based methods).", "description": "This table presents the win rate, draw rate, loss rate, and net win rate of different Tic-Tac-Toe playing policies against various opponents.  Each policy starts as 'X' and makes the first move.  The policies compared are LLM-as-agent, LLM-RAG, URI (the authors' proposed method), Minimax (optimal strategy), and Random Policy. The results highlight the superior performance of URI compared to the other methods.", "section": "6.2 Policy Performance Evaluation"}, {"figure_path": "Ddak3nSqQM/tables/tables_44_1.jpg", "caption": "Table 2: Performance Comparison of Different Policies Against Built-in AI Levels in a GRF 11 vs 11 settings, where the performance of URI is averaged among three different seeds, LLM-as-agent, LLM-RAG is tested with 10 matches, and URI policy and random policy is tested with 40 matches.", "description": "This table presents a comparison of the performance of different policies (LLM-as-agent, LLM-RAG, Random Policy, URI, and Rule-based-AI) against various difficulty levels (Easy, Medium, Hard) of the built-in AI in the Google Research Football (GRF) 11 vs 11 environment.  The URI policy's performance is averaged across three different random seeds.  LLM-as-agent and LLM-RAG were each tested in 10 matches, while the URI and Random policies were each tested in 40 matches. The metrics used for comparison include win rate, draw rate, loss rate, and Goal Difference per Match (GDM).", "section": "6.2 Policy Performance Evaluation"}]