[{"figure_path": "0NMzBwqaAJ/tables/tables_4_1.jpg", "caption": "Table 1: Few-shot CoT reasoning results of math pretraining. All models are tested with few-shot prompting. Previous best results are highlighted in blue, while our best results are in purple. *Only unique math-related tokens are calculated. For RHO-1, we calculate only the selected tokens that are used for training. We use OpenAI's MATH subset [Lightman et al., 2023] for evaluation, since some original test samples have been used in public training sets such as PRM800k. #The SAT only has 32 four-choice problems, so we average our results over the last three checkpoints, if available.", "description": "This table presents the few-shot chain-of-thought (CoT) reasoning results on mathematical problems for various language models.  It compares the performance of different models, including baselines and the proposed RHO-1 model, across several benchmark datasets.  Results are shown for both smaller (1-2B parameters) and larger (>7B parameters) models.  The table highlights the improvements achieved by RHO-1 in few-shot settings, particularly the significant gains in accuracy compared to baselines, while using a considerably smaller number of training tokens.", "section": "3.2 Math Pre-training Results"}, {"figure_path": "0NMzBwqaAJ/tables/tables_5_1.jpg", "caption": "Table 1: Few-shot CoT reasoning results of math pretraining. All models are tested with few-shot prompting. Previous best results are highlighted in blue, while our best results are in purple. *Only unique math-related tokens are calculated. For RHO-1, we calculate only the selected tokens that are used for training. We use OpenAI's MATH subset [Lightman et al., 2023] for evaluation, since some original test samples have been used in public training sets such as PRM800k. #The SAT only has 32 four-choice problems, so we average our results over the last three checkpoints, if available.", "description": "This table presents the few-shot chain-of-thought (CoT) reasoning results on math pre-training for various language models.  It compares the performance of the proposed RHO-1 model against several baseline and state-of-the-art models across various math benchmarks (GSM8k, MATH, SVAMP, ASDiv, MAWPS, TAB, MQA, STEM, SAT).  The results show the few-shot accuracy for each model, highlighting the improvement achieved by RHO-1, especially when considering its significantly reduced number of training tokens compared to other models.  The table also indicates the number of unique training tokens used by each model.", "section": "3.2 Math Pre-training Results"}, {"figure_path": "0NMzBwqaAJ/tables/tables_7_1.jpg", "caption": "Table 1: Few-shot CoT reasoning results of math pretraining. All models are tested with few-shot prompting. Previous best results are highlighted in blue, while our best results are in purple. *Only unique math-related tokens are calculated. For RHO-1, we calculate only the selected tokens that are used for training. We use OpenAI's MATH subset [Lightman et al., 2023] for evaluation, since some original test samples have been used in public training sets such as PRM800k. #The SAT only has 32 four-choice problems, so we average our results over the last three checkpoints, if available.", "description": "This table presents the few-shot chain-of-thought (CoT) reasoning results on the MATH dataset for various language models, including base models and those continually pre-trained with the proposed Selective Language Modeling (SLM) method.  It compares the performance of RHO-1 models against existing state-of-the-art models in terms of accuracy on several math reasoning benchmarks. The table highlights the improvements achieved by RHO-1, particularly its efficiency in achieving comparable performance to much larger models with significantly fewer training tokens.  The table also notes that the SAT results are averaged across the last three checkpoints due to the limited number of questions in the dataset.", "section": "3.2 Math Pre-training Results"}, {"figure_path": "0NMzBwqaAJ/tables/tables_21_1.jpg", "caption": "Table 1: Few-shot CoT reasoning results of math pretraining. All models are tested with few-shot prompting. Previous best results are highlighted in blue, while our best results are in purple. *Only unique math-related tokens are calculated. For RHO-1, we calculate only the selected tokens that are used for training. We use OpenAI's MATH subset [Lightman et al., 2023] for evaluation, since some original test samples have been used in public training sets such as PRM800k. #The SAT only has 32 four-choice problems, so we average our results over the last three checkpoints, if available.", "description": "This table presents the few-shot chain-of-thought (CoT) reasoning results on mathematical reasoning tasks for various language models.  It compares the performance of the proposed RHO-1 model against several baseline and state-of-the-art models.  Results are shown for various metrics across multiple benchmarks, highlighting the improvement achieved by RHO-1, especially considering its reduced training data usage.", "section": "3.2 Math Pre-training Results"}, {"figure_path": "0NMzBwqaAJ/tables/tables_21_2.jpg", "caption": "Table 1: Few-shot CoT reasoning results of math pretraining. All models are tested with few-shot prompting. Previous best results are highlighted in blue, while our best results are in purple. *Only unique math-related tokens are calculated. For RHO-1, we calculate only the selected tokens that are used for training. We use OpenAI's MATH subset [Lightman et al., 2023] for evaluation, since some original test samples have been used in public training sets such as PRM800k. #The SAT only has 32 four-choice problems, so we average our results over the last three checkpoints, if available.", "description": "This table presents the few-shot chain-of-thought (CoT) reasoning results on mathematical problems for various language models.  It compares the performance of the proposed RHO-1 model against several existing models, highlighting the improvements achieved through selective language modeling (SLM). The table shows results across multiple benchmarks (GSM8K, MATH, SVAMP, etc.) and includes model size and training data information to facilitate a comprehensive comparison.  The use of unique math tokens and averaging over multiple checkpoints (for SAT) adds nuance and clarity to the evaluation.", "section": "3.2 Math Pre-training Results"}]