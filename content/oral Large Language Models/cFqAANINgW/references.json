{"references": [{"fullname_first_author": "Tom B. Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-06", "reason": "This paper is foundational to the field of large language models (LLMs) and demonstrates that LLMs can be effective few-shot learners, a crucial concept for the FUNCODER framework."}, {"fullname_first_author": "Mark Chen", "paper_title": "Evaluating large language models trained on code", "publication_date": "2021-07-03", "reason": "This paper introduces HumanEval, a benchmark used in the paper to evaluate code generation capabilities, and demonstrates the limitations of LLMs on complex code generation tasks, motivating the need for FUNCODER."}, {"fullname_first_author": "Jacob Austin", "paper_title": "Program synthesis with large language models", "publication_date": "2021-08-07", "reason": "This paper introduces the MBPP benchmark, another dataset used in the paper to evaluate code generation and highlights the challenges of generating complex programs, which FUNCODER aims to address."}, {"fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring mathematical problem solving with the MATH dataset", "publication_date": "2021-12-07", "reason": "This paper introduces the MATH dataset, a benchmark used in this paper to evaluate mathematical reasoning capabilities, which is important as FUNCODER aims to be applicable to complex reasoning problems."}, {"fullname_first_author": "Bei Chen", "paper_title": "CodeT: Code generation with generated tests", "publication_date": "2023-05-01", "reason": "This paper introduces CodeT, a state-of-the-art method that uses self-testing for code generation, which FUNCODER improves upon by using functional consensus instead of self-testing for improved reliability and generality."}]}