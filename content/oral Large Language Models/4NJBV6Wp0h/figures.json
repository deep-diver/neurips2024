[{"figure_path": "4NJBV6Wp0h/figures/figures_1_1.jpg", "caption": "Figure 1: The strength of self-preference bias is linearly correlated with the LLM's self-recognition capability. Each point represents a model evaluated on the two properties on the CNN/Dailymail (left) and XSUM (right) datasets. We fine-tune GPT-3.5 and Llama 2 for self-recognition or control tasks using both in- and out-of-domain data. The scores represented by both axes can be interpreted as measures of the LLM's confidence on these properties.", "description": "This figure shows the correlation between self-preference and self-recognition in LLMs.  The x-axis represents the self-recognition score (how well an LLM can identify its own outputs), and the y-axis represents the self-preference score (how much more highly an LLM rates its own outputs compared to others). Each point represents a specific LLM model evaluated on two summarization datasets (CNN/Dailymail and XSUM). Different colors and shapes represent different models (Llama 2, GPT-3.5, GPT-4) and fine-tuning conditions (none, control tasks, self-recognition with different numbers of training examples).  The plot demonstrates a positive linear correlation: as self-recognition ability increases, so does self-preference.", "section": "Measuring correlation between self-preference and self-recognition"}, {"figure_path": "4NJBV6Wp0h/figures/figures_3_1.jpg", "caption": "Figure 2: Self-recognition before fine-tuning, evaluated using pairwise (left) and individual (right) measurements; scores are aggregated over two datasets.", "description": "This figure displays the self-recognition capabilities of three LLMs (Llama 2, GPT-3.5, GPT-4) and humans, before any fine-tuning.  The left panel shows the results of a pairwise comparison task, where the LLM is given two summaries\u2014one its own and one from another source\u2014and asked to identify its own. The right panel presents the results of an individual recognition task, where the LLM is provided a single summary and must determine if it was generated by itself.  In both tasks, summaries from other LLMs and humans are included as comparison points.  The scores represent the models' accuracy in correctly identifying their own summaries.  The data is aggregated across two datasets.", "section": "2.3 Out-of-the-box self-recognition"}, {"figure_path": "4NJBV6Wp0h/figures/figures_3_2.jpg", "caption": "Figure 3: Confidence in self-recognition by the evaluator (row) on texts from various sources (column) measured in the individual setting; scores are aggregated over two datasets. GPT-4 stands out as the only discerning model under this setting, but it is also extremely unwilling to predict any text as having been generated by itself, including those actually generated by itself.", "description": "This figure shows the raw self-recognition scores for three LLMs (Llama 2, GPT-3.5, GPT-4) and human-written summaries.  Each LLM was asked to identify if a given text was generated by itself. The rows represent the evaluator LLMs, and the columns represent the source of the text.  Notice that GPT-4 shows high accuracy but very low confidence when predicting its own texts, whereas Llama 2 and GPT-3.5 are less accurate but more confident.", "section": "Out-of-the-box self-recognition"}, {"figure_path": "4NJBV6Wp0h/figures/figures_3_3.jpg", "caption": "Figure 4: Self-preference before fine-tuning, evaluated using pairwise (left) and individual (right) measurements; scores are aggregated over two datasets.", "description": "This figure displays the self-preference scores of Llama 2, GPT-3.5, GPT-4, and human-generated summaries, evaluated using both pairwise and individual measurement methods.  The pairwise method presents two summaries to the LLM and asks it to choose the better one, while the individual method asks the LLM to rate a single summary.  The scores are aggregated across the two datasets (CNN/DailyMail and XSUM).  The figure visually represents the degree to which each LLM evaluator favors its own outputs compared to those generated by other LLMs or humans.", "section": "2 Definition and measurement of self-preference and self-recognition"}, {"figure_path": "4NJBV6Wp0h/figures/figures_4_1.jpg", "caption": "Figure 5: Evaluator responses when treated as a binary response without considering confidence scores. Results are marked \"ambiguous\" if the response reverses when the options are swapped.", "description": "This figure shows the results of treating LLM evaluator responses as binary (without considering confidence scores).  Each bar represents the proportion of responses for a given LLM (Llama 2, GPT-3.5, GPT-4) when comparing its own summary to others. The bars are categorized into \"Ambiguous\" (responses that changed when the order of the summaries was reversed), \"Correct\" (LLM correctly identified its own summary), \"Incorrect\" (LLM incorrectly identified its own summary), \"Prefers Self\" (LLM preferred its own summary), and \"Prefers Other\" (LLM preferred the other summary).  The ambiguity highlights the LLMs' ordering bias, showing that they are not always consistent in their judgements.", "section": "2.4 Out-of-the-box self-preference"}, {"figure_path": "4NJBV6Wp0h/figures/figures_5_1.jpg", "caption": "Figure 6: GPT-3.5 pairwise results on XSUM after 0, 10, and 500 fine-tuning examples show a pattern of increasing absolute confidence scores.", "description": "This figure displays the distribution of self-recognition and self-preference scores for GPT-3.5 model on the XSUM dataset after different amounts of fine-tuning.  The x-axis represents the self-recognition score, and the y-axis represents the density. Three distributions are shown for each metric: one for the model without fine-tuning, one after 10 fine-tuning examples, and one after 500 fine-tuning examples. The distributions shift towards higher scores (closer to 1) as the amount of fine-tuning increases, indicating that fine-tuning enhances both self-recognition and self-preference abilities.", "section": "3 Fine-tuning for self-recognition"}, {"figure_path": "4NJBV6Wp0h/figures/figures_6_1.jpg", "caption": "Figure 1: The strength of self-preference bias is linearly correlated with the LLM's self-recognition capability. Each point represents a model evaluated on the two properties on the CNN/Dailymail (left) and XSUM (right) datasets. We fine-tune GPT-3.5 and Llama 2 for self-recognition or control tasks using both in- and out-of-domain data. The scores represented by both axes can be interpreted as measures of the LLM's confidence on these properties.", "description": "This figure shows the linear correlation between self-preference and self-recognition capabilities of LLMs.  The x-axis represents self-recognition score, and the y-axis represents the strength of self-preference bias. Each point represents a specific LLM model evaluated on two summarization datasets (CNN/Dailymail and XSUM).  The models were either used out-of-the-box or fine-tuned for self-recognition or control tasks using in-domain and out-of-domain data.  The plot demonstrates that as self-recognition accuracy increases, so does the tendency for LLMs to favor their own generations.", "section": "Measuring correlation between self-preference and self-recognition"}, {"figure_path": "4NJBV6Wp0h/figures/figures_7_1.jpg", "caption": "Figure 8: Self-preference scores in the pairwise setting with the two summaries labeled with their sources either correctly or with the labels swapped.", "description": "This figure shows the self-preference scores obtained in a pairwise setting, where the two summaries are labeled with their sources (either correctly or incorrectly).  The results are broken down by model (Llama 2, GPT-3.5, GPT-4) and dataset (CNN/DailyMail, XSUM).  The purpose of this experiment was to test whether the LLMs' preference for their own summaries is genuine or influenced by the labels indicating authorship.", "section": "2.4 Out-of-the-box self-preference"}]