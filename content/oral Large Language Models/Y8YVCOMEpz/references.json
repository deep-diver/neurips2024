{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduced the Transformer architecture, which is the foundation upon which many of the linear attention models discussed in the current paper are based."}, {"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper demonstrated the effectiveness of large language models (LLMs) on various downstream tasks, motivating the need for more efficient attention mechanisms such as those explored in this paper."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "publication_date": "2023-02-13", "reason": "This paper introduced LLaMA, a prominent open-source LLM which is used as a benchmark model in the current paper's experiments and highlights the need for efficient models."}, {"fullname_first_author": "Angelos Katharopoulos", "paper_title": "Transformers are rnns: Fast autoregressive transformers with linear attention", "publication_date": "2020-06-01", "reason": "This paper introduced Linformer, one of the first linear attention models, providing a crucial foundation for the current paper's investigation into linear approximations of softmax attention."}, {"fullname_first_author": "Albert Gu", "paper_title": "Efficiently modeling long sequences with structured state spaces", "publication_date": "2022-01-01", "reason": "This paper introduced the State Space Model (SSM) approach to modeling sequences, another key linear attention model that is analyzed and unified within this paper's framework."}]}