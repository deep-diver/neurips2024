[{"figure_path": "Y8YVCOMEpz/tables/tables_4_1.jpg", "caption": "Table 1: From our general form to existing linear models (* indicates the bias term is omitted).", "description": "This table demonstrates how existing linear attention models (LinFormer, SSM, and LinRNN) can be derived from a unified general form.  It shows the functions used for query (fq), key (fk), value (fv), decay (fa), and output gate (fg), along with the dimensionality of key/query (dk), value (dv), and hidden state (H) for each model.  The table highlights the key differences in parameterization and hidden state representation across these model types.", "section": "3 General Form of LinFormer/SSM/LinRNN Mechanisms"}, {"figure_path": "Y8YVCOMEpz/tables/tables_5_1.jpg", "caption": "Table 2: A review of existing linear models. According to definition 4.1, existing linear models all have some deficiencies: i) Models without dynamic decay At have no ability to memorize dynamically (not satisfying C1); ii) LinRNNs lack the selection ability brought by Q (not satisfying C2), and the approximation ability is poor owing to the small hidden state; iii) Models with K have redundant parameters (not satisfying C3), which probably leads to higher learning difficulty.", "description": "This table summarizes the capabilities of existing linear models in terms of satisfying three necessary conditions for optimal linear approximation to softmax attention: Dynamic memory ability, Static approximation ability, and Least parameter approximation.  Each model is evaluated based on whether it satisfies these conditions (represented by checkmarks or crosses). The table highlights the deficiencies of existing models and motivates the proposed MetaLA model.", "section": "4 Optimal Linear Approximation to the Softmax Attention Map"}, {"figure_path": "Y8YVCOMEpz/tables/tables_6_1.jpg", "caption": "Table 1: From our general form to existing linear models (* indicates the bias term is omitted).", "description": "This table demonstrates how existing linear models (LinFormer, SSM, LinRNN) can be derived from the unified linear attention form proposed in the paper.  It shows the specific functions used for Query (fq), Key (fk), Value (fv), decay (fa), and output gate (fg) for each model, as well as the dimensions (dk, dv, d) used. The table highlights the key differences between these linear models in terms of parameter functions and hidden state sizes.", "section": "3 General Form of LinFormer/SSM/LinRNN Mechanisms"}, {"figure_path": "Y8YVCOMEpz/tables/tables_7_1.jpg", "caption": "Table 4: Performance Comparison on SuperGLUE. PS: parameter size (billion). T: tokens (billion). \u2020 means the results reported by [20]. For baselines that need to be compared, if they do not have public checkpoints, we train and test them under identical conditions with MetaLA. MetaLA: MetaLA with tied embedding trained using 100B tokens. MetaLA: MetaLA trained with 300B tokens.", "description": "This table compares the performance of MetaLA and other models on the SuperGLUE benchmark.  It shows parameter size, number of tokens used for training, and accuracy scores across multiple tasks.  Note that some baselines were retrained for fair comparison with MetaLA.", "section": "Experiments"}, {"figure_path": "Y8YVCOMEpz/tables/tables_7_2.jpg", "caption": "Table 5: Performance Comparison on Commonsense Reasoning. # indicates testing using open-source checkpoints. HS: HellaSwag. WG: WinoGrande. OBQA: OpenbookQA.", "description": "This table compares the performance of different language models on commonsense reasoning tasks.  The models are evaluated on several benchmarks, including LOGIQA, WSC273, BOOLQ, PIQA, HellaSwag, Winogrande, ARC-c, and OpenbookQA. The table shows the performance of each model in terms of accuracy or F1 score, depending on the specific benchmark.  Some models used open-source checkpoints for testing.", "section": "Experiments"}, {"figure_path": "Y8YVCOMEpz/tables/tables_8_1.jpg", "caption": "Table 7: Performances Comparison on the Long Range Arena. We cite baselines from HGRN [20].", "description": "This table compares the performance of MetaLA and other state-of-the-art models on the Long Range Arena benchmark.  The benchmark consists of several tasks evaluating different aspects of long-range sequence modeling capabilities, including ListOps, Text Retrieval, Image Pathfinder, and Path-X.  The table shows the performance of each model on each task, as well as the average performance across all tasks.  The results demonstrate MetaLA's competitive performance compared to existing methods.", "section": "7 Performances Comparison on the Long Range Arena"}, {"figure_path": "Y8YVCOMEpz/tables/tables_8_2.jpg", "caption": "Table 8: Ablation studies. Ablation study results on the 360M model trained for 15B tokens. We compared the model variants on zero-shot experiments of the Comparison on Commonsense Reasoning benchmark. HS: HellaSwag. WG: WinoGrande. OBQA: OpenbookQA.", "description": "This table presents the ablation study results for the 360M MetaLA model trained on 15 billion tokens.  It compares the performance of the full MetaLA model against variants where different components (self-augmentation, short convolution, and the key matrix) are removed.  The results are evaluated using several zero-shot commonsense reasoning benchmarks, including HellaSwag (HS), WinoGrande (WG), and OpenbookQA (OBQA), with LOGIQA and WSC273 also included.  The table helps to determine the contribution of each component to the overall model performance.", "section": "Ablation studies"}, {"figure_path": "Y8YVCOMEpz/tables/tables_8_3.jpg", "caption": "Table 6: Results on ImageNet-1k.", "description": "This table presents the results of image classification experiments on the ImageNet-1k dataset.  It compares the performance of MetaLA against several other linear models (HGRN, GLA, Mamba) and a transformer-based model (Deit). The comparison includes accuracy and the number of model parameters (in millions).  The results show that MetaLA achieves the highest accuracy among linear models.", "section": "6 Experiments"}, {"figure_path": "Y8YVCOMEpz/tables/tables_15_1.jpg", "caption": "Table 1: From our general form to existing linear models (* indicates the bias term is omitted).", "description": "This table shows how the general recurrent form of linear attention can be specialized to existing linear models such as LinFormer, GLA, LinRNN, TransNormer, GLRU, RWKV-4, Mamba and SSMs.  It illustrates the differences in the functions used for query, key, value, decay, output gate, and dimension settings for each model.  The table highlights how variations in the hidden state size and the method used to maintain that state affect the overall model design and functionality. This demonstrates that the main difference between LinFormer, SSM and LinRNN lies in hidden state size, how to maintain the hidden state, and how to perform parameter mapping.", "section": "3 General Form of LinFormer/SSM/LinRNN Mechanisms"}, {"figure_path": "Y8YVCOMEpz/tables/tables_16_1.jpg", "caption": "Table A2: From general form to SSMs.", "description": "This table shows how several State-Space Models (SSMs) can be derived from the general recurrent linear form presented earlier in the paper.  It details the functions used for query, key, value, decay, and output gate for different SSM models like DSS, S4D, H3, S5, and Mamba.  The table also specifies the dimensions used in each model, highlighting differences in parameterization and the usage of independent parameters across channels.", "section": "A1.2 SSM"}, {"figure_path": "Y8YVCOMEpz/tables/tables_17_1.jpg", "caption": "Table 1: From our general form to existing linear models (* indicates the bias term is omitted).", "description": "This table shows how the general recurrent form of linear attention used in the paper can be specialized to existing linear models like Linformer, GLRU, and Mamba. It highlights the differences in the functions used for query, key, value, decay, and output gate, and the dimensions used in each model.  It helps to unify different linear attention models under a common framework.", "section": "3 General Form of LinFormer/SSM/LinRNN Mechanisms"}, {"figure_path": "Y8YVCOMEpz/tables/tables_23_1.jpg", "caption": "Table A4: Hyper-parameters of MetaLA on LRA. d is the dimension of model. d1 is the dimension of dq and dk, d2 is the hidden dimension in GLU. num-warmup and max-step are used for cosine warmup.", "description": "This table shows the hyperparameters used for training the MetaLA model on the Long Range Arena (LRA) benchmark.  It specifies the depth of the network, the dimensions of various parameters (d, d1, d2), the dropout rate, the learning rate, batch size, weight decay, number of warmup steps, and the maximum number of training steps. These settings were tailored for optimal performance on each specific subtask of LRA.", "section": "A4 Experimental Details"}, {"figure_path": "Y8YVCOMEpz/tables/tables_24_1.jpg", "caption": "Table 5: Performance Comparison on Commonsense Reasoning. # indicates testing using open-source checkpoints. HS: HellaSwag. WG: WinoGrande. OBQA: OpenbookQA.", "description": "This table compares the performance of different language models on commonsense reasoning tasks.  It shows the performance (in terms of accuracy or other relevant metrics) of various models, including MetaLA, on tasks such as LOGIQA, WSC273, BOOLQ, PIQA, HellaSwag, WinoGrande, ARC-c, and OpenbookQA.  The table helps to demonstrate the effectiveness of MetaLA by comparing its performance against established baselines.", "section": "Experiments"}, {"figure_path": "Y8YVCOMEpz/tables/tables_24_2.jpg", "caption": "Table 5: Performance Comparison on Commonsense Reasoning. # indicates testing using open-source checkpoints. HS: HellaSwag. WG: WinoGrande. OBQA: OpenbookQA.", "description": "This table compares the performance of various language models on commonsense reasoning tasks.  The models are evaluated on several benchmarks, including LOGIQA, WSC273, BOOLQ, PIQA, HellaSwag (HS), Winogrande (WG), ARC-c, and OpenbookQA (OBQA).  The table shows the performance of different models in terms of accuracy or other relevant metrics on these benchmarks. The size (PS) and number of training tokens (T) of the models are also included.  The '#' symbol indicates whether open-source checkpoints were used for testing.", "section": "Experiments"}, {"figure_path": "Y8YVCOMEpz/tables/tables_24_3.jpg", "caption": "Table 2: A review of existing linear models. According to definition 4.1, existing linear models all have some deficiencies: i) Models without dynamic decay At have no ability to memorize dynamically (not satisfying C1); ii) LinRNNs lack the selection ability brought by Q (not satisfying C2), and the approximation ability is poor owing to the small hidden state; iii) Models with K have redundant parameters (not satisfying C3), which probably leads to higher learning difficulty.", "description": "This table summarizes the capabilities of existing linear models (Linformer, SSM, LinRNN) in terms of the three criteria defined in the paper for optimal linear approximation to softmax attention: dynamic memory ability, static approximation ability, and least parameter approximation. It shows which models satisfy each criterion and highlights their deficiencies.", "section": "4 Optimal Linear Approximation to the Softmax Attention Map"}, {"figure_path": "Y8YVCOMEpz/tables/tables_24_4.jpg", "caption": "Table A7: Results on MQAR with sequence length 512 and retrieval key-value pairs 80.", "description": "This table presents the results of the Multi-Query Associative Recall (MQAR) task, a synthetic benchmark designed to evaluate memory ability.  The experiment uses sequences of length 512 and 80 key-value pairs.  The table compares the performance of a Transformer model, the Mamba model and the MetaLA model across different model dimensions (64 and 128).  It shows that the Transformer model achieves near-perfect accuracy, while Mamba performs poorly. MetaLA demonstrates improved performance compared to Mamba, indicating its effectiveness in handling longer sequences and more information.", "section": "A5 Additional Experiments"}, {"figure_path": "Y8YVCOMEpz/tables/tables_25_1.jpg", "caption": "Table 7: Performances Comparison on the Long Range Arena. We cite baselines from HGRN [20].", "description": "This table presents the performance comparison of different models on the Long Range Arena benchmark.  The benchmark evaluates the ability of models to handle long sequences. The models compared include various linear attention models (S4, DSS-softmax, TNN, S5, Mega, SGConv, LRU, HGRN, Mamba) and the standard Transformer model. The performance is measured across several subtasks: ListOps, Text Retrieval, Image Pathfinder, Path-X.  The average performance across all subtasks is also reported, providing a comprehensive comparison of model performance in handling long-range dependencies.", "section": "7 Performances Comparison on the Long Range Arena"}, {"figure_path": "Y8YVCOMEpz/tables/tables_26_1.jpg", "caption": "Table 2: A review of existing linear models. According to definition 4.1, existing linear models all have some deficiencies: i) Models without dynamic decay At have no ability to memorize dynamically (not satisfying C1); ii) LinRNNs lack the selection ability brought by Q (not satisfying C2), and the approximation ability is poor owing to the small hidden state; iii) Models with K have redundant parameters (not satisfying C3), which probably leads to higher learning difficulty.", "description": "This table categorizes several existing linear attention models based on three criteria for optimal linear approximation to softmax attention: dynamic memory ability, static approximation ability, and least parameter approximation.  It shows which models satisfy each criterion, highlighting the shortcomings of existing methods.", "section": "4 Optimal Linear Approximation to the Softmax Attention Map"}]