[{"figure_path": "VKKY3Uv7vi/figures/figures_1_1.jpg", "caption": "Figure 1: The learning process of BPQP: the previous layer outputs y and generates the optimal solution z* in the forward pass; the backward pass propagates the loss gradient for end-to-end learning; the process is accelerated by reformulating and simplifying the problem first and then adopting efficient solvers.", "description": "This figure illustrates the learning process of the BPQP framework.  The forward pass solves a convex optimization problem to produce an optimal solution z*, given inputs y from the preceding layer.  The backward pass, crucial for end-to-end learning, is simplified and decoupled by reformulating it as a Quadratic Programming (QP) problem, thus enabling efficient gradient calculation.  The use of efficient first-order solvers for both the forward and backward passes speeds up the overall training process.", "section": "Methodology"}, {"figure_path": "VKKY3Uv7vi/figures/figures_8_1.jpg", "caption": "Figure 1: The learning process of BPQP: the previous layer outputs y and generates the optimal solution z* in the forward pass; the backward pass propagates the loss gradient for end-to-end learning; the process is accelerated by reformulating and simplifying the problem first and then adopting efficient solvers.", "description": "This figure illustrates the BPQP framework's learning process.  The forward pass involves solving a convex optimization problem (using an arbitrary solver, with a first-order solver as the default) to obtain the optimal solution z*, given input y.  The backward pass, crucial for end-to-end learning, is simplified to a decoupled Quadratic Programming (QP) problem using the structural properties of the Karush-Kuhn-Tucker (KKT) matrix.  This reformulation makes the backward pass significantly more efficient than traditional methods that directly solve the original KKT linear system and allows for flexibility in using efficient QP solvers. The decoupling of the forward and backward passes is a key aspect of BPQP's performance gains.", "section": "1 Introduction"}, {"figure_path": "VKKY3Uv7vi/figures/figures_8_2.jpg", "caption": "Figure 1: The learning process of BPQP: the previous layer outputs y and generates the optimal solution z* in the forward pass; the backward pass propagates the loss gradient for end-to-end learning; the process is accelerated by reformulating and simplifying the problem first and then adopting efficient solvers.", "description": "This figure illustrates the learning process of the BPQP framework.  The forward pass involves solving a convex optimization problem with input y to obtain the optimal solution z*. The backward pass is simplified by reformulating it as a quadratic programming (QP) problem, which is then solved using an efficient solver (ADMM, by default, is used but the framework can leverage other solvers). This decoupling of the forward and backward passes significantly reduces computational costs, enabling efficient end-to-end learning.", "section": "1 Introduction"}, {"figure_path": "VKKY3Uv7vi/figures/figures_17_1.jpg", "caption": "Figure 1: The learning process of BPQP: the previous layer outputs y and generates the optimal solution z* in the forward pass; the backward pass propagates the loss gradient for end-to-end learning; the process is accelerated by reformulating and simplifying the problem first and then adopting efficient solvers.", "description": "This figure illustrates the BPQP learning process. The forward pass uses an arbitrary solver to find the optimal solution (z*) to a convex optimization problem, given inputs (y) from the previous layer.  The backward pass, instead of using computationally expensive methods like implicit differentiation on the Jacobian matrix, leverages the properties of the Karush-Kuhn-Tucker (KKT) matrix to reformulate the gradient calculation as a simplified quadratic programming (QP) problem. This allows for the use of efficient first-order optimization algorithms, significantly improving overall efficiency. The figure visually depicts this decoupling of the forward and backward passes and the simplified QP problem used in the backward pass.", "section": "1 Introduction"}]