[{"figure_path": "xZxXNhndXU/figures/figures_0_1.jpg", "caption": "Figure 1: Summary. Given a set of heterogeneous input sequences that capture a common geographic area in varying environmental conditions (e.g., weather, season, and lighting) with distinct dynamic objects (e.g., vehicles, pedestrians, and cyclists), we optimize a single dynamic scene representation that permits rendering of arbitrary viewpoints and scene configurations at interactive speeds.", "description": "This figure summarizes the paper's main idea.  It shows how diverse input sequences (images and videos of an urban scene under various conditions) are used to create a unified 3D representation. This representation, based on dynamic 3D Gaussian fields and neural fields, enables the generation of novel views of the scene from arbitrary viewpoints at high speed.", "section": "1 Introduction"}, {"figure_path": "xZxXNhndXU/figures/figures_3_1.jpg", "caption": "Figure 2: Overview. To render an image of sequence s at time t, we first evaluate the scene graph G = (V, E) which stores latent codes w at its nodes V and coordinate transformations [R|t] at its edges E, i.e. the configuration of the dynamic objects and the overall scene. We then use the scene configuration to determine the active sets of 3D Gaussians G. The 3D Gaussians G and the latent codes w serve as conditioning signals to the neural fields \u03c6 and \u03c8, which output, for each 3D Gaussian gk \u2208 G, an appearance conditioned color ct, an opacity correction term vt for static Gaussians modeling transient geometry, and a dynamic deformation dt for non-rigid dynamic 3D Gaussians modeling e.g. pedestrians. Finally, the retrieved information is used to compose a set of 3D Gaussians that represent the dynamic scene at (s, t) from which we render the image.", "description": "This figure illustrates the overall process of rendering an image using the proposed 4DGF method. It starts with a scene graph representing the scene's configuration, including latent codes and transformations for dynamic objects.  This graph determines which 3D Gaussians are active and feed into neural fields that predict color, opacity corrections for static elements, and deformations for dynamic objects. The results are combined to generate the final rendered image.", "section": "3.2 Representation"}, {"figure_path": "xZxXNhndXU/figures/figures_9_1.jpg", "caption": "Figure 9: Additional qualitative results on Argoverse 2 [81]. We show four examples where the upper two are from the residential area and the lower two are from the downtown area.", "description": "This figure provides a qualitative comparison of the proposed 4DGF method against two state-of-the-art methods, SUDS and ML-NSG, on the Argoverse 2 dataset.  The comparison showcases RGB images and depth maps, highlighting the superior visual quality and detail preservation of 4DGF, especially in challenging areas such as the residential and downtown areas within the dataset.", "section": "4.1 Comparison to state-of-the-art"}, {"figure_path": "xZxXNhndXU/figures/figures_9_2.jpg", "caption": "Figure 4: Qualitative results on Waymo Open [23]. We show a sequence of evaluation views synthesized by our model (top-left to bottom-right). As the woman (marked with a red box) gets out of the car and walks away, we successfully model her articulated motion and changing body poses.", "description": "This figure shows a sequence of images generated by the model, demonstrating its ability to accurately represent the articulated motion of a person getting out of a car and walking away. The red box highlights the person for better tracking of their movements.", "section": "Experiments"}, {"figure_path": "xZxXNhndXU/figures/figures_16_1.jpg", "caption": "Figure 3: Qualitative results on Argoverse 2 [81]. Our method produces significantly sharper renderings both in foreground dynamic and static background regions, with much fewer artifacts e.g. in areas with transient geometry such as tree branches (left). Best viewed digitally.", "description": "This figure compares the qualitative results of novel view synthesis on the Argoverse 2 dataset.  It showcases the superior sharpness and reduced artifacts produced by the proposed 4DGF method compared to existing methods (SUDS [16] and ML-NSG [17]). The comparison highlights the improved rendering of both dynamic (moving objects) and static (stationary objects) elements in the scene, particularly in areas with transient geometry (elements that change over time, such as tree branches).", "section": "4 Experiments"}, {"figure_path": "xZxXNhndXU/figures/figures_17_1.jpg", "caption": "Figure 6: Qualitative examples of transient geometry. We show four relevant examples from the residential split of Argoverse 2 [81]. We observe a large disparity between our full model and ours without transient geometry modeling (App. only). Transient objects like a banner (left bottom) are completely missing and there are severe depth and color artifacts (e.g. trees). Best viewed digitally.", "description": "This figure shows a comparison of the results obtained using the full model and a model without transient geometry.  The full model correctly renders transient objects such as a banner and trees.  The model without transient geometry has missing objects and artifacts.  The comparison is done for both RGB and depth images.", "section": "4.2 Ablation studies"}, {"figure_path": "xZxXNhndXU/figures/figures_18_1.jpg", "caption": "Figure 7: Runtime comparison of neural fields vs. spherical harmonics. We compare the runtime of querying neural fields \u03c6 and \u03c8 to a spherical harmonics function of degree 3. We report time-per-query in nanoseconds.", "description": "This figure compares the efficiency of using neural fields versus spherical harmonics for rendering.  It shows that while spherical harmonics are faster for individual queries, the overall runtime difference is not significant due to other computational factors in the rendering pipeline.  The neural field approach offers greater flexibility in representing complex scenes and appearance variations, making it more suitable for large-scale, dynamic urban scene rendering.", "section": "4.3 Runtime analysis"}, {"figure_path": "xZxXNhndXU/figures/figures_18_2.jpg", "caption": "Figure 8: Histogram of mean 3D Gaussian scales. We use our model trained on Argoverse 2 (residential split). Both axes are in logarithmic scale. The vast majority of 3D Gaussians have a small scale, while there are a few outliers with huge scales. The scene is approximately within [-1, 1].", "description": "This figure shows a histogram of the mean 3D Gaussian scales used in the model trained on the Argoverse 2 dataset. The x-axis represents the mean scale (in logarithmic scale), and the y-axis represents the frequency (also in logarithmic scale). The histogram shows that most of the 3D Gaussians have small scales, with only a few outliers having very large scales. The scene is approximately bounded within [-1, 1].", "section": "4 Experiments"}, {"figure_path": "xZxXNhndXU/figures/figures_20_1.jpg", "caption": "Figure 9: Additional qualitative results on Argoverse 2 [81]. We show four examples where the upper two are from the residential area and the lower two are from the downtown area.", "description": "This figure compares the qualitative novel view synthesis results of three different methods (SUDS [16], ML-NSG [17], and 4DGF (Ours)) against the ground truth on the Argoverse 2 dataset.  The top two rows show examples from a residential area, while the bottom two rows showcase examples from a downtown area. The comparison highlights the differences in visual quality, specifically in terms of sharpness, artifact reduction, and overall fidelity to the ground truth.  4DGF demonstrates superior performance in generating sharper, more realistic images compared to the other two methods.", "section": "4.1 Comparison to state-of-the-art"}, {"figure_path": "xZxXNhndXU/figures/figures_21_1.jpg", "caption": "Figure 10: Additional qualitative results on Waymo Open [23]. We show a sequence of evaluation views synthesized by our model (top to bottom). We see two pedestrians on the left and right being faithfully modeled across varying body poses while also carrying objects such as a stroller (right) or a shopping bag (left).", "description": "This figure shows qualitative results of the proposed method on the Waymo Open dataset. The figure shows a sequence of images rendering a street scene with pedestrians.  The results demonstrate the model's ability to accurately model articulated motion (pedestrians walking and carrying objects) and non-rigid deformations.  It highlights that the model can faithfully generate novel views of dynamic scenes.", "section": "4 Experiments"}]