[{"figure_path": "Mwj57TcHWX/figures/figures_3_1.jpg", "caption": "Figure 1: Overview of DiffTORI for model-based RL. In contrast to prior work in model-based RL [38] that uses non-differentiable MPPI (left), we utilize differentiable trajectory optimization to generate actions (right). DiffTORI computes the policy gradient loss on the generated actions and back-propagates it through the optimization process, to optimize the encoder as well as other latent space models (latent reward predictor and latent dynamics function) to maximize task performance.", "description": "This figure compares the model-based reinforcement learning approaches of TD-MPC and DiffTORI.  TD-MPC uses a non-differentiable Model Predictive Path Integral (MPPI) to generate actions.  DiffTORI, in contrast, uses differentiable trajectory optimization.  The key difference is that DiffTORI computes the policy gradient loss on the generated actions and backpropagates this loss to optimize not only the encoder but also the latent reward predictor and latent dynamics function.  This end-to-end optimization aims to maximize task performance.", "section": "4 Method"}, {"figure_path": "Mwj57TcHWX/figures/figures_5_1.jpg", "caption": "Figure 2: Overview of our method on Imitation Learning. DiffTORI (right) learns a cost function via differentiable trajectory optimization and performs test-time optimization with it, which is different from prior work (left) that uses an explicit policy or diffusion without test-time optimization. Although implicit policy shares the same spirit as DiffTORI, we observe that the training procedure of DiffTORI using differentiable trajectory optimization leads to better performance compared to the EBM approach used in prior work [2], which can suffer from training instability.", "description": "This figure compares different policy architectures for deep imitation learning.  Explicit policies directly map observations to actions using a feedforward network. Implicit policies, like EBMs, learn an energy function; actions are obtained by minimizing this function at test time. Diffusion policies generate actions by iteratively refining noise through a diffusion process.  DiffTORI differs by learning a cost function through differentiable trajectory optimization.  Actions are generated at test time by optimizing this learned cost function, offering a different training process with reported stability advantages.", "section": "4.3 Differentiable Trajectory Optimization applied to imitation learning"}, {"figure_path": "Mwj57TcHWX/figures/figures_6_1.jpg", "caption": "Figure 3: Performance of DiffTORI, in comparison to 4 prior state-of-the-art model-based and model-free RL algorithms, on 15 tasks from DeepMind control suite. DiffTORI achieves the best performance when averaged across all tasks. Results are averaged with 4 seeds, and the shaded regions represent the 95% confidence interval.", "description": "This figure shows the learning curves of different reinforcement learning algorithms across 15 tasks from the DeepMind Control Suite.  The algorithms compared are SAC, DrQ-v2, TD-MPC, and DiffTORI.  The y-axis represents the average reward achieved, and the x-axis shows the number of environment steps.  The shaded areas around the lines indicate 95% confidence intervals.  The figure demonstrates that DiffTORI significantly outperforms the other algorithms across most tasks.", "section": "5 Experiments"}, {"figure_path": "Mwj57TcHWX/figures/figures_9_1.jpg", "caption": "Figure 4: By using a CVAE, DiffTORI can learn multimodal objectives functions via sampling different latent vectors from CVAE (right). By performing trajectory optimization with these two different objective functions, DiffTORI can generate multimodal actions (left).", "description": "This figure demonstrates the ability of DiffTORI, when using a Conditional Variational Autoencoder (CVAE), to learn multimodal objective functions and generate corresponding multimodal actions.  The left subplot shows how, starting from the same initial action, DiffTORI produces two different actions (a1 and a2) depending on the sampled latent vector from the CVAE.  The middle and right subplots illustrate the distinct objective function landscapes associated with these different latent samples, highlighting how the optimization process leads to different optimal actions. This showcases DiffTORI's capacity to handle complex scenarios with multiple possible solutions, reflecting a key advantage over methods that only learn unimodal policies.", "section": "4.3 Differentiable Trajectory Optimization applied to imitation learning"}, {"figure_path": "Mwj57TcHWX/figures/figures_13_1.jpg", "caption": "Figure 3: Performance of DiffTORI, in comparison to 4 prior state-of-the-art model-based and model-free RL algorithms, on 15 tasks from DeepMind control suite. DiffTORI achieves the best performance when averaged across all tasks. Results are averaged with 4 seeds, and the shaded regions represent the 95% confidence interval.", "description": "This figure compares the performance of DiffTORI with four other state-of-the-art reinforcement learning algorithms across 15 tasks from the DeepMind Control Suite.  Each subplot shows the average reward over time for a specific task.  The shaded area represents the 95% confidence interval, illustrating the variability in performance across multiple runs.  The figure demonstrates that DiffTORI outperforms all other algorithms in most of the tasks, achieving the best overall average performance.", "section": "5 Experiments"}, {"figure_path": "Mwj57TcHWX/figures/figures_14_1.jpg", "caption": "Figure 6: Ablation study of DiffTORI to examine the contribution of each loss terms towards the final performance, on a subset of 4 tasks. We find the reward prediction loss, action initialization, and dynamics prediction loss are all essential for DiffTORI to achieve good performance.", "description": "This ablation study analyzes the impact of removing individual loss terms from the DiffTORI objective function on four DeepMind Control Suite tasks.  The results demonstrate the importance of all three loss components (reward prediction, action initialization, and dynamics prediction) for achieving strong performance. Removing any one of these terms significantly degrades performance, indicating the critical role each term plays in the overall effectiveness of the algorithm.", "section": "A.1.3 Ablation study on the loss terms"}, {"figure_path": "Mwj57TcHWX/figures/figures_15_1.jpg", "caption": "Figure 3: Performance of DiffTORI, in comparison to 4 prior state-of-the-art model-based and model-free RL algorithms, on 15 tasks from DeepMind control suite. DiffTORI achieves the best performance when averaged across all tasks. Results are averaged with 4 seeds, and the shaded regions represent the 95% confidence interval.", "description": "This figure compares the performance of DiffTORI against four other state-of-the-art reinforcement learning algorithms across fifteen tasks from the DeepMind Control Suite.  The y-axis represents the average reward achieved, and the x-axis shows the number of environment steps.  Shaded areas represent the 95% confidence interval, indicating the variability in performance across multiple trials.  The results show that DiffTORI consistently outperforms the other algorithms, demonstrating its superior performance in model-based reinforcement learning.", "section": "5 Experiments"}, {"figure_path": "Mwj57TcHWX/figures/figures_21_1.jpg", "caption": "Figure 8: Visualization of the tasks for imitation learning in RoboMimic and ManiSkill.", "description": "This figure visualizes keyframes from several imitation learning tasks within the RoboMimic and ManiSkill datasets.  It shows a sequence of images for each task, illustrating the robot's actions and the changes in the environment's state as the task progresses.  The figure provides a visual representation of the complexity and diversity of the tasks used to evaluate the DiffTORI method for imitation learning.", "section": "5.2 Imitation Learning"}, {"figure_path": "Mwj57TcHWX/figures/figures_22_1.jpg", "caption": "Figure 9: Visualization of the tasks for imitation learning in Metaworld.", "description": "This figure visualizes the keyframes of 22 robotic manipulation tasks from the MetaWorld benchmark used for imitation learning evaluation. Each task shows a sequence of images depicting the robot's interaction with the objects, showcasing the complexity and diversity of the tasks.", "section": "5.2.1 Meta World"}]