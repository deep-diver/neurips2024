{"references": [{"fullname_first_author": "Jacob Devlin", "paper_title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "publication_date": "2019-00-00", "reason": "This paper introduces BERT, a highly influential language model used for encoding textual descriptions in Text2CAD, significantly impacting its performance."}, {"fullname_first_author": "Mohammad Sadil Khan", "paper_title": "Cad-signet: Cad language inference from point clouds using layer-wise sketch instance guided attention", "publication_date": "2024-00-00", "reason": "This paper introduces a methodology for representing CAD construction sequences, a key concept leveraged in Text2CAD for parametric CAD model generation."}, {"fullname_first_author": "Rundi Wu", "paper_title": "Deepcad: A deep generative network for computer-aided design models", "publication_date": "2021-10-00", "reason": "DeepCAD is the primary dataset used in Text2CAD, providing the foundation for training and evaluation, making it a crucial reference."}, {"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-00-00", "reason": "The Transformer architecture, fundamental to Text2CAD's design, is based on the concepts introduced in this highly influential paper."}, {"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-00-00", "reason": "This paper's findings on few-shot learning in language models are relevant to Text2CAD's ability to generate CAD models from diverse text prompts."}]}