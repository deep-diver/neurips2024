[{"type": "text", "text": "Automatically Learning Hybrid Digital Twins of Dynamical Systems ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Samuel Holt,\u2217 Tennison Liu\u2217& Mihaela van der Schaar DAMTP, University of Cambridge Cambridge, UK {sih31, tl522, mv472}@cam.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Digital Twins (DTs) are computational models that simulate the states and temporal dynamics of real-world systems, playing a crucial role in prediction, understanding, and decision-making across diverse domains. However, existing approaches to DTs often struggle to generalize to unseen conditions in data-scarce settings, a crucial requirement for such models. To address these limitations, our work begins by establishing the essential desiderata for effective DTs. Hybrid Digital Twins (HDTwins) represent a promising approach to address these requirements, modeling systems using a composition of both mechanistic and neural components. This hybrid architecture simultaneously leverages (partial) domain knowledge and neural network expressiveness to enhance generalization, with its modular design facilitating improved evolvability. While existing hybrid models rely on expertspecified architectures with only parameters optimized on data, automatically specifying and optimizing HDTwins remains intractable due to the complex search space and the need for flexible integration of domain priors. To overcome this complexity, we propose an evolutionary algorithm (HDTwinGen) that employs Large Language Models (LLMs) to autonomously propose, evaluate, and optimize HDTwins.2 Specifically, LLMs iteratively generate novel model specifications, while offline tools are employed to optimize emitted parameters. Correspondingly, proposed models are evaluated and evolved based on targeted feedback, enabling the discovery of increasingly effective hybrid models. Our empirical results reveal that HDTwinGen produces generalizable, sample-efficient, and evolvable models, significantly advancing DTs\u2019 efficacy in real-world applications. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Digital Twins (DTs) are computational models that accurately simulate the states and temporal dynamics of real-world systems [1, 2]. They are particularly useful in modeling dynamical systems, which consist of multiple interdependent components that evolve over time [3, 4]. Take, for example, the epidemiological dynamics of a contagious disease containing various components, including infection rates, recovery rates, population movement, and intervention strategies. DTs can integrate these factors to simulate future outcomes (e.g. predict disease spread), understand system changes (e.g. examining shifts in disease dynamics for varying demographics), and evaluate the impact of control measures (e.g. to curb disease transmission) [5, 6]. ", "page_idx": 0}, {"type": "text", "text": "Desiderata. A notable differentiator between DTs and general machine learning (ML) models is the emphasis on generalization. DTs are designed to simulate completely unseen scenarios or interventions at inference time. Therefore, a crucial consideration is $\\blacktriangleright$ [P1] out-of-distribution generalization: the ability to generalize to state-action distributions beyond those observed during training. This challenge is often compounded by the scarcity of observational data available to accurately learn dynamics, highlighting the importance of $\\blacktriangleright$ [P2] sample-efficient learning. Additionally, the model should be $\\blacktriangleright$ [P3] evolvable: capable of efficiently adapting (i.e. with minimal retraining) to changes in the underlying system dynamics. This is particularly crucial in healthcare domains, such as epidemiological modeling and treatment planning, where DTs are regularly updated to reflect fundamental changes in disease transmission patterns (caused by viral mutations, vaccination coverage) or evolving drug resistance mechanisms, often with minimal additional data of emergent dynamics [6, 7]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Existing approaches for creating DTs primarily utilize two approaches: mechanistic models or MLbased neural models. Mechanistic models, denoted as $f_{\\mathrm{mech}}$ , are closed-form equations grounded in domain knowledge such as biological or physical principles. They offer high accuracy and generalization given sufficient domain understanding but are limited in their ability to model systems where scientific knowledge is incomplete [8, 9]. Of related note, techniques have been introduced to discover governing equations directly from data, but face challenges in scaling to more complex problem settings [10, 11]. Conversely, neural approaches, $f_{\\mathrm{neural}}$ , leverage neural networks (NN) to learn DTs directly from data, often requiring minimal knowledge [12, 13, 14, 15]. Such models are effective given sufficient training data that provides adequate coverage of state-action distributions, but struggle in data-scarce settings and are difficult to evolve to reflect changing conditions due to their overparameterized, monolithic nature. ", "page_idx": 1}, {"type": "text", "text": "Key considerations. Informed by this context, Hybrid Digtal Twins (HDTwins) combine the strengths of both approaches through compositions of neural and mechanistic components, i.e. $f=f_{\\mathrm{mech}}\\circ$ $f_{\\mathrm{neural}}$ . Here, $f_{\\mathrm{mech}}$ symbolically incorporates domain-grounded priors, improving generalization and regularization while simplifying the complexity of patterns that have to be learned by the neural component. In other terms, $f_{\\mathrm{neural}}$ complements the mechanistic component by modeling complex temporal patterns in regions where the mechanistic model might be oversimplified or incomplete. Consequently, HDTwins can more accurately and robustly capture system dynamics, particularly in settings with (limited) empirical data and (partial) domain knowledge. ", "page_idx": 1}, {"type": "text", "text": "Conceptually, hybrid modeling involves two stages: model specification, determining the model structure (e.g. neural architecture, symbolic equations), and model parameterization, estimating model parameters (e.g. neural weights, coefficients). This process, with model specification in particular, has traditionally relied heavily on human expertise to craft problem-specific models [16, 17, 18, 19]. In this work, we investigate the feasibility of automatically designing hybrid models with minimal expert involvement, which would significantly enhance the efficiency and scalability of model development. This task is challenging, as it requires searching for optimal specification and corresponding parameters within a vast combinatorial model space [20, 21]. To address this, we introduce HDTwinGen, a novel evolutionary framework that autonomously and efficiently designs HDTwins. At a high level, our method represents hybrid model specifications in code and leverages large language models (LLMs) for their domain knowledge, contextual understanding, and learning capabilities to propose symbolically represented models and search the model space [22, 23, 24]. This is coupled with offline optimization tools to empirically estimate model parameters from training data. More specifically, HDTwinGen utilizes two LLM agents: the modeling agent, whose task is to generate novel model specifications, and the evaluation agent, which analyzes performance and provides targeted recommendations for improvement. Through multiple iterations, HDTwinGen efficiently evolves better performing hybrid models with informed modifications. ", "page_idx": 1}, {"type": "text", "text": "Contributions: $\\textcircled{1}$ Conceptually, we present the first work in automated hybrid model design, jointly optimizing model specification and parameterization of hybrid digital twins. $\\textcircled{2}$ Technically, we introduce HDTwinGen, a novel evolutionary framework employing LLMs and offline optimization tools to propose, evaluate, and iteratively enhance hybrid models. $\\Supset$ Empirically, we demonstrate that our method learns more accurate DTs, achieving $\\blacktriangleright$ better out-of-distribution generalization, $\\blacktriangleright$ sample-efficient learning, and $\\blacktriangleright$ increased flexibility for modular evolvability. ", "page_idx": 1}, {"type": "text", "text": "2 Digital Twins of Dynamical Systems ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "A dynamical system $\\boldsymbol{S}:=(\\boldsymbol{\\mathcal{X}},\\boldsymbol{\\mathcal{U}},\\boldsymbol{\\Phi})$ is a tuple of its $d_{\\mathcal{X}}$ -dimensional state space $\\mathcal{X}\\subseteq\\mathbb{R}^{d_{\\mathcal{X}}}$ , an (optional) $d_{\\mathcal{U}}$ -dimensional action space $\\mathcal{U}\\subseteq\\mathbb{R}^{d_{\\mathcal{U}}}$ , and a dynamics model $\\Phi$ . The state at time $t\\in\\mathcal{T}\\subseteq\\mathbb{R}_{+}$ is represented as a vector, $x(t)\\in\\mathcal{X}$ and similarly the action taken is represented as a vector $u(t)\\in\\mathcal{U}$ . The continuous-time dynamics of the system can be described by $\\mathrm{d}x(t)/\\mathrm{d}t=$ $\\Phi(x(t),u(t),t)$ , where $\\Phi:\\mathcal{X}\\times\\mathcal{U}\\times\\mathcal{T}\\rightarrow\\mathcal{X}$ . We optionally consider the existence of some policy $\\pi:{\\mathcal{X}}\\to P({\\mathcal{U}})$ that acts on the system by mapping a state $x(t)$ to a distribution over actions $u(t)$ . ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Digital Twins. Digital twins (DTs) aim to approximate $\\Phi:\\mathcal{X}\\times\\mathcal{U}\\times\\mathcal{T}\\rightarrow\\mathcal{X}$ using a computational model $f_{\\theta,\\omega(\\theta)}\\in\\mathcal{F}$ learned from data. Here, we use $\\theta\\in\\Theta$ to denote the specification of the model (e.g. linear) and $\\omega(\\theta)\\in\\Omega(\\theta)$ to indicate the set of parameters specified by $\\theta$ . Additionally, ${\\mathcal{F}},\\Theta$ , and $\\Omega(\\theta)$ are the spaces of all possible models, specifications, and parameters, respectively. Next, we outline the key desiderata for a DT: ", "page_idx": 2}, {"type": "text", "text": "[P1] Generalization to unseen state-action distributions. As DTs are required to simulate varying conditions, they should extrapolate to state-action distributions not observed during training time. Formally, the generalization error $\\mathbb{E}_{({x}(t),{u}(t),{y}(t))\\sim p_{O O D}}[\\mathcal{L}(f_{\\theta,\\omega({\\theta})}({x}(t),{u}(t)),{y}(t))]$ should be minimized, where $\\mathcal{L}$ is some loss function, and $p_{O O D}$ represents the out-of-distribution scenario. [P2] Sample-efficient learning. Given the often limited availability of real-world data, DTs should learn robustly from minimal empirical data. In other words, they must have good sample complexity, achieving the desired level of generalization with a limited number of observations [25]. ", "page_idx": 2}, {"type": "text", "text": "[P3] Evolvability. Dynamical systems are, by nature, non-stationary and evolve over time [26, 27]. From a modeling perspective, the DT should be easily evolved to reflect changing underlying dynamics, minimizing the need for additional data or expensive model re-development, i.e. $\\theta$ and $\\dot{\\omega_{}}(\\theta)$ should be easily adjustable to reflect changing system dynamics. ", "page_idx": 2}, {"type": "text", "text": "For the purpose of model learning, we assume access to an offline dataset containing $N\\in\\mathbb{N}^{+}$ trajectories, where the measurements of the systems are made at discrete time points $[T]=[t_{1},t_{2},\\dots T]$ . This dataset, $\\mathcal{D}\\,=\\,\\{\\{(x^{(n)}(t),u^{(n)}(t),\\dot{y}^{(n)}(t))\\ |\\ t\\,\\in\\,[T]\\}\\}_{n=1}^{N}$ , contains state-action trajectories sampled regularly over time, where $y^{(n)}(t)=x^{(n)}(t+\\Delta t)$ represents the subsequent state. ", "page_idx": 2}, {"type": "text", "text": "3 Hybrid Digital Twins ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "HDTwin. A Hybrid Digital Twin is a composition of mechanistic and neural components, represented as $f_{\\theta,\\omega(\\theta)}=f_{\\mathrm{mech}}\\circ f_{\\mathrm{neural}}\\,[18,28$ ]. This class of hybrid models offers several advantages that align with our desiderata. The mechanistic component allows partial knowledge to be encoded through its symbolic form, which, while not sufficient alone to accurately predict underlying dynamics, is complemented by the neural components that learn from available data. This combination aids in generalization ([P1]), especially moving beyond conditions observed in training, and improves sample complexity ([P2]). Furthermore, the mechanistic component can be quickly and easily updated with new parameters due to its simpler, lower-dimensional structure, allowing the overall model to adapt efficiently to remain accurate in changing conditions ([P3]). In this work, we focus on additive compositions, $f_{\\theta,\\omega(\\theta)}=f_{\\mathrm{mech}}+f_{\\mathrm{neural}}$ , as they are more interpretable. Additionally, it enables individual contributions of mechanistic and neural components to be easily disentangled and simplifies the optimization to allow gradient-based methods [16]. Nonetheless, we encourage future works to investigate alternative composition strategies (e.g. branching composition) to develop more advanced HDTwins [29]. ", "page_idx": 2}, {"type": "text", "text": "Learning the hybrid model can be decomposed into two steps: $(I)$ model specification, or learning the structure, $\\theta\\in\\Theta$ , of the dynamics function that describes how the system evolves over time; and (2) model parameterization, which estimates the specific values of parameters $\\omega(\\theta)\\in\\Omega(\\theta)$ for a given specification $\\theta$ . For instance, the logistic-growth model specifies a structure for population growth, while parameterization involves estimating the growth rate and carrying capacity.3 More generally, this learning problem can be mathematically formulated as a bilevel optimization problem: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\boldsymbol{\\theta}^{*}=\\underset{\\boldsymbol{\\theta}\\in\\Theta}{\\arg\\operatorname*{min}}\\,\\mathcal{L}_{\\mathrm{outer}}(\\boldsymbol{\\theta},\\boldsymbol{\\omega}^{*}(\\boldsymbol{\\theta})),\\quad\\mathrm{where}\\quad\\boldsymbol{\\omega}^{*}(\\boldsymbol{\\theta})=\\underset{\\boldsymbol{\\omega}\\in\\Omega(\\boldsymbol{\\theta})}{\\arg\\operatorname*{min}}\\,\\mathcal{L}_{\\mathrm{inner}}(\\boldsymbol{\\theta},\\boldsymbol{\\omega}(\\boldsymbol{\\theta}))\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Here, the upper-level problem involves finding the optimal specification $\\theta^{*}$ that minimizes the outer objective ${\\mathcal{L}}_{\\mathrm{outer}}$ , while the lower-level problem involves finding the optimal parameters $\\omega^{\\ast}(\\theta)$ for a given specification $\\theta$ that minimizes the inner objective function $\\mathcal{L}_{\\mathrm{inner}}$ . To be more concrete, the outer objective measures the generalization performance, empirically measured on the validation set $\\mathcal{L}_{\\mathrm{val}}$ , while the inner objective measures the fitting error, as evaluated on the training set $\\mathcal{L}_{\\mathrm{train}}$ . ", "page_idx": 2}, {"type": "image", "img_path": "SOsiObSdU2/tmp/b345aeb89abe6a12acccd24b07cd0316c43cae4570f9a1e1c849578b5e695900.jpg", "img_caption": ["Figure 1: HDTwinGen: evolutionary framework. The process begins with user-provided modeling context $S^{\\mathrm{context}}$ and $\\mathcal{D}\\;=\\;\\{\\mathcal{D}_{\\mathrm{train}},\\mathcal{\\dot{D}}_{\\mathrm{val}}\\}$ . 1) In iteration $g$ , the modeling agent generates model specification as a Python program $f_{\\theta,\\omega({\\theta})}.\\;2)$ Parameters are optimized using the offline optimization tool to yield $f_{\\theta,\\omega^{*}(\\theta)}$ . 3) The HDTwin is evaluated based on model loss $\\upsilon$ and component-wise loss $\\delta$ . Subsequently, the model pool $\\mathcal{P}^{(g)}$ is updated with top- $\\cal{K}$ models. 4) The evaluation agent provides targeted feedback for model improvement $H^{(g)}$ by analyzing models in $\\mathcal{P}^{(g)}$ using performance metrics requirements outlined in $S^{c o n t e x t}$ . This iterative loop repeats for $G$ iterations. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Combinatorial search space. The space of possible specifications $\\Theta$ (e.g. different networks, functional forms) is discrete and combinatorially large, while $\\Omega(\\theta)$ represents the continuous space of parameters to be optimized. Selecting the optimal $\\theta,\\omega(\\theta)$ thus involves searching through a vast combinatorial space. Performing this search through traditional means, such as genetic programming [21] or evolutionary algorithms [20], is computationally challenging, time-consuming, and often technically infeasible. To the best of our knowledge, our work is the first to address the problem of automatic HDTwin development, where we incorporate LLMs (combined with offline optimization tools) to automatically optimize both the specification and the parameterization of hybrid models. ", "page_idx": 3}, {"type": "text", "text": "4 HDTwinGen: Automatic Design of HDTwins ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Human experts craft models by making strategic design decisions based on their domain knowledge, starting with a sensible initial model specification and performing intelligent modifications based on empirical evaluations. Our key insight is that LLMs can effectively emulate these capabilities to efficiently navigate the search space in Equation (1) and autonomously design HDTwins. More specifically, our method utilizes LLMs for three major purposes: $\\blacktriangleright$ source of domain knowledge, where LLMs inject domain-consistent knowledge into the model specification, particularly through the symbolic representation $f_{\\mathrm{mech}}$ ; $\\blacktriangleright$ efficient search, by making intelligent modifications to the specification to converge more efficiently on the optimal hypothesis; and $\\blacktriangleright$ contextual understanding, enabling the algorithm to incorporate task-specific context and targeted feedback for model improvement [22, 23, 24]. ", "page_idx": 3}, {"type": "text", "text": "Overview. We operationalize this insight through HDTwinGen, an evolutionary algorithm that iteratively evolves a population of candidate solutions to automatically search for the best HDTwin. Our approach employs a framework comprising three key elements: $(I)$ human experts provide an initial system description, modeling objectives, and requirements as a structured prompt; (2) a modeling agent proposes new model specifications, optimizes their parameters on a training dataset, and collects validation performance metrics; (3) an evaluation agent assesses the proposed models using both data-driven performance metrics and qualitative evaluations against expert-defined objectives and requirements. The agents communicate using natural language and a custom code format representing the HDTwin model, facilitating autonomous and iterative model enhancement. An overview of our method is presented in Figure 1, with pseudocode in Appendix E.1. ", "page_idx": 3}, {"type": "text", "text": "Initial prompt design. The optimization process begins with a human expert providing a structured prompt, referred to as the modeling context $S^{\\mathrm{context}}$ . This modeling context outlines the system description, modeling objectives $\\mathcal{L}$ , and requirements $\\mathcal{R}$ : ", "page_idx": 3}, {"type": "text", "text": "1. The system description semantically describes the system, including state and action variables, giving the algorithm the contextual understanding necessary for informed model development. 2. The modeling objective specifies quantitative performance requirements via a metric $\\mathcal{L}$ . 3. The modeling requirements $\\mathcal{R}$ are qualitative and described in natural language, detailing aspects such as interpretability (e.g. fully mechanistic or hybrid model) and additional scientific knowledge (e.g. a log-linear relationship between variables). ", "page_idx": 4}, {"type": "text", "text": "In practice, $\\mathcal{R}$ can incorporate various requirements, allowing for the design of both purely mechanistic and hybrid models, a flexibility that we demonstrate experimentally. The model is represented in Python, where purely mechanistic specifications are represented in native Python and neural components are represented using PyTorch [31]. Moreover, $S^{\\mathrm{context}}$ includes a skeleton code to guide the synthesis of executable code in a predetermined format. For illustrative purposes, an example of $S^{\\mathrm{context}}$ is provided in Appendix E.4. ", "page_idx": 4}, {"type": "text", "text": "Evolutionary optimization overview. Given $S^{\\mathrm{context}}$ as input, HDTwinGen performs $G$ iterations of optimization, where $G\\in\\mathbb{N}^{+}$ . The population of proposed HDTwins at iteration $g$ is represented as $\\bar{\\mathcal{P}}^{(g)}$ . Each iteration creates a new candidate model based on previously created models in $\\mathcal{P}^{(g)}$ and feedback. Only the top $K$ models are retained after each iteration, except when $g<K$ , in which case all generated models are kept, i.e. $\\operatorname*{max}_{g\\in[G]}|\\mathcal{P}^{(g)}|=K$ . Each model in $\\mathcal{P}^{(g)}$ is characterized by a tuple containing its model specification (represented symbolically through code) and validation metrics. After completing $G$ iterations, the model with the best validation performance in $\\mathcal{P}^{(G)}$ is selected as the final model. ", "page_idx": 4}, {"type": "text", "text": "4.1 Modeling Agent ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Proposing HDTwins. The goal of the modeling step is to propose novel HDTwins based on previously proposed models and feedback from the evaluation agent. Specifically, on the $g$ -th iteration, the modeling agent takes as input \u25b6 $\\mathcal{P}^{(g-1)}$ : the set of top- $\\mathcal{K}$ previously generated models; \u25b6 $H^{(g-1)}$ : the most recent feedback produced by the evaluation agent (where on the initial step, $g=1$ , both are empty, i.e., $H^{(0)}\\,=\\,\\bar{\\otimes}$ , $P^{(0)}\\;=\\;\\emptyset)$ ; and \u25b6 $S^{\\mathrm{context}}$ : the modeling context. The modeling agent generates a model specification $\\theta$ using a predefined code format (i.e. skeleton code). By observing multiple previously best-performing models and their performances, the modeling agent can exploit this context as a rich form of in context-learning and evolve improved specifications in subsequent generations [22]. Each generated specification emits its corresponding parameters, $\\omega(\\theta)$ are fitted to the training set $\\ensuremath{\\mathcal{D}}_{\\mathrm{train}}$ . More formally, we represent this generative procedure as $f_{\\theta,\\omega(\\theta)}\\sim\\mathrm{LLM}_{\\mathrm{model}}(H,\\mathcal{P}^{(g)},S^{\\mathrm{context}})$ . ", "page_idx": 4}, {"type": "text", "text": "Model specification. To generate model specifications, the modeling agent decomposes the system into a set of components, with each component describing the dynamics of a specific state variable. In other words, for a system with $d_{\\mathcal{X}}$ state variables, there will be $d_{\\mathcal{X}}$ components. Each component is characterized by its own set of inputs and a unique dynamics function that describes the dynamics of its associated state variable over time. This modular representation enables independent analysis and optimization of individual components. In cases where $\\mathcal{R}$ specifies purely mechanistic equations, the component dynamics are entirely defined using closed-form equations. Conversely, in a hybrid model, the mechanistic equation can be augmented with a neural network (implemented in PyTorch) to model residuals (i.e. in an additive fashion). The choice between mechanistic and hybrid models is left to the user, balancing the trade-off between transparency and predictive performance. Concretely, the specification step involves \u2018filling in\u2019 the skeleton code with a detailed body of code, specifying the decomposition, and delineating each component\u2019s dynamics function as a separate code structure (for a generated HDTwin example, see Appendix I). ", "page_idx": 4}, {"type": "text", "text": "Model optimization. The generated specification emits $\\omega(\\theta)$ , which are treated as placeholder values, and are then optimized against the training dataset. Specifically, we optimize the mean squared error for the parameters that minimize this loss as $\\begin{array}{r}{\\omega^{*}(\\theta)=\\arg\\operatorname*{min}_{\\omega(\\theta)}\\;\\mathcal{L}(f_{\\theta,\\omega(\\theta)},\\mathcal{D}_{\\mathrm{train}}).}\\end{array}$ . In this work, we consider $\\omega(\\theta)$ to be continuous variables, and as such, we optimize $\\theta$ by stochastic gradient descent, using the Adam optimizer [32]. However, we note other optimization algorithms, such as black-box optimizers, could also be used (for more details, see Appendix F, Equation (5)). The parameter optimization step then yields the complete model, $f_{\\theta,\\omega^{*}({\\theta})}$ . ", "page_idx": 4}, {"type": "text", "text": "Quantitative evaluation. For each generated model, we evaluate them quantitatively. Specifically, we collect the validation mean squared error loss per component, which we denote as $\\delta\\;=\\;[\\delta_{1},\\delta_{2},.\\,.\\,.\\,,\\delta_{d_{x}}]$ (Appendix F, Equation (6)). We also compute the validation loss of the overall model as well as $\\bar{v^{\\big}}\\bar{=}\\,\\mathcal{L}\\big(f_{\\theta,\\omega^{*}(\\theta)},\\bar{D}_{\\mathrm{val}}\\big)$ . Finally, the generated model and its validation losses are included in a tuple and added to the top- $K$ models $\\mathcal{P}^{(g)}\\leftarrow\\mathcal{P}^{(g-1)}\\oplus\\left(f_{\\theta,\\omega^{*}(\\theta)},\\delta,\\upsilon\\right)$ , where $\\mathcal{P}^{(g)}$ automatically removes the lowest performing models, and also only adds a new model to $\\mathcal{P}^{(g)}$ if it is unique. We highlight that we consider the top- $K$ models only to apply selection pressure, such that only the best-performing models are considered when generating the next HDTwin [33]. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "4.1.1 Evaluation Agent ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Model evaluation. The goal of the evaluation step is to reflect on the current set of top- $\\mathcal{K}$ models, $\\mathcal{P}^{(g)}$ against requirements $\\mathcal{R}$ and provide actionable and detailed feedback to the modeling agent for model improvement: $H^{(g)}\\sim\\mathrm{LLM}_{\\mathrm{eval}}(\\mathcal{R},\\mathcal{P}^{(g)})$ . We note that $H^{(g)}$ is provided in natural language and can be viewed as a dense feedback signal, a notable distinction from traditional learning methods, where feedback often takes the form of simple scalar values, such as loss gradients or rewards. Leveraging natural language feedback allows the agent to $(I)$ engage in comparative analysis, identifying effective specifications in $\\mathcal{P}^{(g)}$ contributing to higher performance and discerning patterns common in less effective models, informing its suggestions for further model improvement; (2) qualitatively evaluate models against qualitative requirements $\\mathcal{R}$ \u2014leveraging the LLM\u2019s capacity to reason about proposed HDTwins to reflect these requirements via model improvement feedback. ", "page_idx": 5}, {"type": "text", "text": "Enhancing search. By providing rich feedback to improve model specification, the evaluation and modeling agent collaborate to efficiently evolve high-performing models. Empirically, in Appendix J, we observe that the evaluation agent provides targeted and specific feedback, including componentspecific suggestions, proposing alternative decompositions, removing parameters, or introducing non-linear terms. It is noteworthy that the feedback $H^{(g)}$ , expressed flexibly in natural language, could easily be further enriched through direct human feedback. We demonstrate this human-inthe-loop capability by including expert feedback during the optimization process through $H^{(g)}$ and observed that it was integrated into newly generated HDTwins. Though further investigation is beyond the scope of this work, this demonstration highlights promising avenues for augmenting human-machine collaboration in the autonomous design of DTs. ", "page_idx": 5}, {"type": "text", "text": "5 Related Works ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "For an extended related work, refer to Appendix B. Our work focuses on autonomously learning DTs from data, with several relevant research strands: ", "page_idx": 5}, {"type": "text", "text": "Neural sequence models. ML approaches commonly address learning system dynamics as a sequential modeling problem. In these settings, $f_{\\theta,\\omega({\\theta})}$ are typically black-box models, where $\\theta\\in\\Theta$ is the NN architecture and $\\omega(\\theta)$ are its weights. Early models like Hidden Markov Models [34] and Kalman filters [35] made simplifying Markovian and linearity assumptions, later extended to nonlinear settings [36, 37]. Subsequent models, including recurrent neural networks [38], along with their advanced variants [39, 40, 41], introduced the capability to model longer-term dependencies. More recent advancements include attention mechanisms [42] and Transformer models [43], significantly improving the handling of long-term dependencies in sequence data. Another line of work, Neural Ordinary Differential Equations (NODE) [14, 44, 45], interprets neural network operations as differential equations. These methods have found utility in modeling a range of complex systems [46, 47, 48, 49]. While deep sequence models are proficient at capturing complex dynamics, they are heavily reliant on training data for generalization ([P1, P2]), and their monolithic and overparameterized structures limit evolvability ([P3]). ", "page_idx": 5}, {"type": "text", "text": "Mechanistic (discovery) models. Beyond purely neural approaches, another line of work aims to discover a system\u2019s governing equations directly from data. Here $\\theta\\in\\Theta$ are closed-form equations and $\\omega(\\theta)$ are their parameters. These include symbolic regression techniques [10], Eureqa [50], SINDy [11], D-CODE [51, 52], among others [52, 53] that search for $\\theta$ and $\\omega(\\theta)$ from data. These techniques struggle to scale to higher-dimensional settings and rely on experts to perform variable selection and define the function set and primitives available to the search algorithms. ", "page_idx": 5}, {"type": "text", "text": "Hybrid models. Recent efforts have also created hybrid models by integrating physical laws with neural models. Physics-informed neural networks [15, 54], and methods including Hamiltonian Neural Networks [55], Lagrangian Neural Networks [56] integrate structural priors of physical systems to improve generalization. These techniques introduce specialized mechanisms to incorporate precisely known physical principles. Additionally, [57] integrates prior ODE/PDE knowledge into a hybrid model, using specialized regularization to penalize the neural component\u2019s information content. [58, 59] consider settings where an expert equation is known, but equation variables are latent and unobserved. Correspondingly, they employ two sets of latent variables: one governed by expert equations and another linked to neural components. [60] performs data augmentation by sampling out-of-distribution trajectories from expert models. While existing approaches rely on expert models to perform the hybrid model design, HDTwinGen is an automated approach to jointly optimize hybrid model specification and its parameters. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "6 Experiments and Evaluation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we evaluate HDTwinGen and verify that it significantly outperforms state-of-the-art methods in modeling system dynamics over time from an observed dataset and corresponding system description.4 ", "page_idx": 6}, {"type": "text", "text": "Benchmark datasets. We evaluate against six real-world complex system datasets; where each dataset is either a real-world dataset or has been sampled from an accurate simulator designed by human experts. Three are derived from a state-of-the-art biomedical Pharmacokinetic-Pharmacodynamic (PKPD) model of lung cancer tumor growth, used to simulate the combined effects of chemotherapy and radiotherapy in lung cancer [61] (Equation (2))\u2014this has been extensively used by other works [62, 63, 64]. Here we use this bio-mathematical lung cancer model to create three variations of lung cancer under the effect of no treatments (Lung Cancer), chemotherapy only (Lung Cancer (with Chemo.)), and chemotherapy combined with radiotherapy (Lung Cancer (with Chemo. & Radio.)). We also compare against an accurate and complex COVID-19 epidemic agent-based simulator (COVID-19) [65], which is capable of modeling non-pharmaceutical interventions, such as physical distancing during a lockdown. Furthermore, we compare against an ecological model of a microcosm of algae, flagellate, and rotifer populations (Plankton Microcosm)\u2014replicating an experimental three-species prey-predator system [66]. Moreover, we also compare against a real-world dataset of hare and lynx populations (Hare-Lynx), replicating predator-prey dynamics [67]. We detail all benchmark datasets details in Appendix C. ", "page_idx": 6}, {"type": "text", "text": "Evaluation Metrics. We employ mean squared error (MSE) to evaluate the benchmark methods on a held-out test dataset of state-action trajectories, denoted as $\\mathcal{D}_{\\mathrm{test}}$ , using the loss defined in Equation (5) and report this as $\\mathcal{T}_{M S E}$ . Each metric is averaged over ten runs with different random seeds, and we present these averages along with their $95\\%$ confidence intervals, further detailed in Appendix G. ", "page_idx": 6}, {"type": "text", "text": "Benchmark methods. To assess whether HDTwinGen is state-of-the-art, we compare it with the most competitive and popular neural network models, which, when modeling the dynamics of a system over time, becomes a form of ODE model, that is a neural ODE [14] with action inputs (DyNODE) [68]. Moreover, we also compare against a recurrent neural network (RNN) [69] and a state-of-the-art transformer (Transformer) [64]. We also compare against mechanistic dynamical equations derived from equation discovery methods for ODEs, including Genetic Programming (GP) [10] and Sparse Identification of Nonlinear Dynamics (SINDy) [11]. Lastly, we compare against a hybrid model (APHYNITY) that integrates prior knowledge in the form of ODEs into hybrid models, while penalizing the information content from the neural component [57]. Moreover, we compare against the ablations of our method, of the zero-shot generated HDTwin (ZeroShot) and this model with subsequently optimized parameters (ZeroOptim). We provide method implementation, hyperparameter, and experimental details in Appendix D. ", "page_idx": 6}, {"type": "text", "text": "7 Main Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We evaluated all our benchmark methods across all our datasets tabulated in Table 1. HDTwinGen models the system the most accurately, achieving the lowest test prediction mean squared error on the held-out test dataset of state-action trajectories. In the interest of space, we include additional experimental evaluations in the appendix. Specifically, we also evaluate $\\blacktriangleright$ HDTwinGen performance on a suite of synthetically and procedurally generated benchmarks (Appendix H.9); $\\blacktriangleright$ comparisons ", "page_idx": 6}, {"type": "text", "text": "Table 1: Benchmark method performance. Reporting the test prediction MSE $(T_{M S E})$ of the produced system models on held-out test datasets across all benchmark datasets. HDTwinGen achieves the lowest test prediction error. The results are averaged over ten random seeds, with $\\pm$ indicating $95\\%$ confidence intervals. ", "page_idx": 7}, {"type": "table", "img_path": "SOsiObSdU2/tmp/24f02a296409d7ac27926839bdb2a53fd0bbaf8daa49efcd2dda147e1804f9dd.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "SOsiObSdU2/tmp/85e7f89adc1f2a98270e7bb1b56a8ad9e5358aa8cfde09c5f23807be55c36deb.jpg", "table_caption": ["Table 2: Out of distribution shifts. On a variation of the Lung Cancer (with Chemo. & Radio.), HDTwinGen is more robust to OOD shifts in unseen state-action distributions. "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "SOsiObSdU2/tmp/9f842a6f791b06ad621e33ddb4f4fe134d64592618f968feb20dc49dc8852346.jpg", "img_caption": ["Figure 2: Sample efficiency. Analyzing performance as a function of the number of training trajectories in the Lung Cancer (with Chemo. & Radio.) dataset. We observe that HDTwinGen achieves the lowest test prediction error, even in the very challenging low data regime. This highlights the role of priors embedded in HDTwin in sample-efficient generalization. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "against domain-specific baselines (Appendix H.8) and $\\blacktriangleright$ various ablation experiments, including ablation of LLM hyperparameters, prompt design, and algorithm settings (Appendices H.5 to H.7). ", "page_idx": 7}, {"type": "text", "text": "7.1 Insight Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "This section provides an in-depth analysis of HDTwinGen\u2019s effectiveness related to its benchmark counterparts. Specifically, we examine the core desiderata for an effective DT described in Section 2: [P1] out-of-distribution generalization, [P2] sample-efficient learning, and [P3] evolvability. ", "page_idx": 7}, {"type": "text", "text": "[P1] Can an HDTwin generalize to out-of-distribution shifts? To explore out-of-distribution shifts, we adapt the Lung Cancer (with Chemo. & Radio.) to produce a training dataset of states in a range that is outside those observed in the test set over all trajectories (Appendix H.1). We tabulate this in Table 2. Empirically, we find that HDTwinGen is more robust to out-of-distribution shifts than existing methods, benefiting from explicit decomposition and robust hybrid models. Notably, the neural network method DyNODE shows the largest relative error increase from IID to OOD by two orders of magnitude, while the mechanistic method SINDy exhibits a smaller increase by only one order of magnitude. This demonstrates the importance of hybrid models that leverage both neural and mechanistic components to enhance generalization performance under distribution shifts. ", "page_idx": 7}, {"type": "text", "text": "[P2] Can HDTwinGen improve sample-efficiency in model learning? To explore the low data settings, we re-ran all benchmark methods with fewer samples in their training dataset on the Lung Cancer (with Chemo. & Radio.) dataset. We plot this in Figure 2. Empirically, we observe that HDTwinGen can achieve lower performance errors, especially in lower-sample regimes. ", "page_idx": 7}, {"type": "text", "text": "[P3] Can HDTwinGen evolve its modular HDTwin to fit the system? We analyze this from an empirical point of view to determine if HDTwinGen can correctly evolve the generated HDTwin and reduce its prediction error over subsequent generations. We observe that HDTwinGen can indeed understand, reason, and iteratively evolve the generated code representation of the HDTwin to incorporate a better fitting HDTwin, as observed in Figure 3. In particular, the annotated results demonstrate that HDTwinGen effectively refines the hybrid model by strategically adjusting its neural and mechanistic components (in a fashion akin to human experts), leading to significant improvements in accuracy and robustness. This iterative evolution process demonstrates HDTwinGen\u2019s ability to adapt and optimize its modular components. ", "page_idx": 7}, {"type": "image", "img_path": "SOsiObSdU2/tmp/89a74ba187c86be2e2652859bdacc94497cce5f9dccff13d9b76b6d20562206e.jpg", "img_caption": ["Figure 3: HDTwinGen effectively evolves HDTwin. Validation MSE of the HDTwin generated in each iteration, showing the Pareto-front of the best generated HDTwin (Top-1 HDTwin), and the generated HDTwin per generation step\u2014additionally with a few of the HDTwins labeled with their model descriptions. HDTwinGen can efficiently understand, modify, and hence evolve the HDTwin to achieve a better-fitting model (Appendix H.4). "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "SOsiObSdU2/tmp/b91ef40f8a4aa31c5d73a44b18fd7fcf5f232ea4f56961d727dc9fdfc3bee6b5.jpg", "img_caption": ["Figure 4: COVID-19 unobserved intervention. The symbolic code-based representation of HDTwin can be easily adapted to unobserved interventions through targeted adjustments of parameters. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Can HDTwinGen Understand and Modify Its HDTwin? We investigate whether large language model (LLM) agents can take an optimized high-dimensional twin (HDTwin) from an existing benchmark dataset and adapt it to model an unobserved intervention that is not present in the training data. We note that this intervention emulates scenarios where the dynamics of the underlying system changes. We affirmatively answer this question by constructing a scenario where our COVID-19 simulator incorporates an unobserved intervention of a lockdown policy, which reduces physical interactions between individuals (Appendix H.2). As demonstrated in Figure 4, we observe that the code-model representation of the HDTwin can be (1) understood by the modeling agent LLM and (2) adapted in its parameters to accurately model and reflect this intervention. We find that HDTwinGen is the only method capable of changing the overall functional behavior by modifying a single parameter in the model; in contrast, all other existing data-driven methods require a dataset of state-action trajectories under the new dynamics introduced by this intervention. ", "page_idx": 8}, {"type": "text", "text": "Ablation Studies. We conducted ablation studies on HDTwinGen and found several key insights. First, retaining the top- $K$ models within the LLM context leads to improved model generation (Appendix H.5). Additionally, HDTwinGen is compatible with various LLMs and different temperature settings (Appendix H.6). It also benefits from including textual descriptions of the variables to be modeled as prior information (Appendix H.7). Finally, HDTwinGen can be specifically instructed to generate mechanistic white-box models if desired (Appendix H.10). ", "page_idx": 8}, {"type": "text", "text": "8 Limitations and Discussions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In summary, this work addresses the problem of learning digital twins for continuous-time dynamical systems. After establishing clear learning objectives and key requirements, we introduce Hybrid Digital Twins (HDTwins)\u2014a promising approach that combines mechanistic understanding with neural architectures. HDTwins encode domain knowledge symbolically while leveraging neural networks for enhanced expressiveness. Conventional hybrid models, however, rely heavily on expert specification with learning limited to parameter optimization, constraining their scalability and applicability. To overcome these limitations, we propose a novel approach to automatically specify and parameterize HDTwins through HDTwinGen, an evolutionary framework that leverages LLMs to iteratively search for and optimize high-performing hybrid twins. Our empirical results demonstrate that evolved HDTwins consistently outperform existing approaches across multiple criteria, exhibiting superior out-of-distribution generalization, enhanced sample efficiency, and improved modular evolvability. ", "page_idx": 9}, {"type": "text", "text": "Limitations. While our results are promising, several important limitations remain. HDTwinGen\u2019s efficacy depends critically on human experts providing initial system specifications and on the underlying LLM\u2019s domain knowledge and model generation capabilities. Our current implementation focuses exclusively on continuous-time systems, which, although broadly applicable, represent only a subset of real-world systems. Future work could extend our approach through human-in-the-loop feedback mechanisms, integration with external tools, and expansion to broader system classes. ", "page_idx": 9}, {"type": "text", "text": "Ethical implications. We acknowledge the risk of bias transmission from the black-box LLMs into the evolved models. While our hybrid approach enables greater expert scrutiny through its human-interpretable components, we strongly recommend a comprehensive evaluation of evolved models for fairness, bias, and privacy concerns before deployment in sensitive applications. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank the anonymous reviewers, area and program chairs, members of the van der Schaar lab, and Andrew Rashbass for many insightful comments and suggestions. TL and SH would like to acknowledge and thank AstraZeneca for their sponsorship and support. This work was supported by Microsoft\u2019s Accelerate Foundation Models Academic Research initiative. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] F. Tao, J. Cheng, Q. Qi, M. Zhang, H. Zhang, and F. Sui, \u201cDigital twin-driven product design, manufacturing and service with big data,\u201d The International Journal of Advanced Manufacturing Technology, vol. 94, pp. 3563\u20133576, 2018.   \n[2] J. Corral-Acero, F. Margara, M. Marciniak, C. Rodero, F. Loncaric, Y. Feng, A. Gilbert, J. F. Fernandes, H. A. Bukhari, A. Wajdan et al., \u201cThe \u2018digital twin\u2019to enable the vision of precision cardiology,\u201d European heart journal, vol. 41, no. 48, pp. 4556\u20134564, 2020.   \n[3] H. A. Simon, The sciences of the artificial. MIT press, 1996.   \n[4] J. Ladyman, J. Lambert, and K. Wiesner, \u201cWhat is a complex system?\u201d European Journal for Philosophy of Science, vol. 3, pp. 33\u201367, 2013.   \n[5] Q. Qi and F. Tao, \u201cDigital twin and big data towards smart manufacturing and industry 4.0: 360 degree comparison,\u201d Ieee Access, vol. 6, pp. 3585\u20133593, 2018.   \n[6] V. Iranzo and S. P\u00e9rez-Gonz\u00e1lez, \u201cEpidemiological models and covid-19: a comparative view,\u201d History and Philosophy of the Life Sciences, vol. 43, no. 3, p. 104, 2021.   \n[7] I. Bozic, J. G. Reiter, B. Allen, T. Antal, K. Chatterjee, P. Shah, Y. S. Moon, A. Yaqubie, N. Kelly, D. T. Le et al., \u201cEvolutionary dynamics of cancer in response to targeted combination therapy,\u201d elife, vol. 2, p. e00747, 2013.   \n[8] R. Rosen, G. Von Wichert, G. Lo, and K. D. Bettenhausen, \u201cAbout the importance of autonomy and digital twins for the future of manufacturing,\u201d Ifac-papersonline, vol. 48, no. 3, pp. 567\u2013572, 2015.   \n[9] T. Erol, A. F. Mendi, and D. Do\u02d8gan, \u201cThe digital twin revolution in healthcare,\u201d in 2020 4th international symposium on multidisciplinary studies and innovative technologies (ISMSIT). IEEE, 2020, pp. 1\u20137.   \n[10] J. R. Koza, \u201cGenetic programming as a means for programming computers by natural selection,\u201d Statistics and computing, vol. 4, pp. 87\u2013112, 1994.   \n[11] S. L. Brunton, J. L. Proctor, and J. N. Kutz, \u201cDiscovering governing equations from data by sparse identification of nonlinear dynamical systems,\u201d Proceedings of the national academy of sciences, vol. 113, no. 15, pp. 3932\u20133937, 2016.   \n[12] D. Ha and J. Schmidhuber, \u201cRecurrent world models facilitate policy evolution,\u201d Advances in neural information processing systems, vol. 31, 2018.   \n[13] J. Yoon, D. Jarrett, and M. Van der Schaar, \u201cTime-series generative adversarial networks,\u201d Advances in neural information processing systems, vol. 32, 2019.   \n[14] R. T. Chen, Y. Rubanova, J. Bettencourt, and D. K. Duvenaud, \u201cNeural ordinary differential equations,\u201d Advances in neural information processing systems, vol. 31, 2018.   \n[15] M. Raissi, P. Perdikaris, and G. E. Karniadakis, \u201cPhysics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations,\u201d Journal of Computational physics, vol. 378, pp. 686\u2013707, 2019.   \n[16] L. Faure, B. Mollet, W. Liebermeister, and J.-L. Faulon, \u201cA neural-mechanistic hybrid approach improving the predictive power of genome-scale metabolic models,\u201d Nature Communications, vol. 14, no. 1, p. 4669, 2023.   \n[17] J. Pinto, J. R. Ramos, R. S. Costa, and R. Oliveira, \u201cA general hybrid modeling framework for systems biology applications: Combining mechanistic knowledge with deep neural networks under the sbml standard,\u201d AI, vol. 4, no. 1, pp. 303\u2013318, 2023.   \n[18] P. Wang, Z. Zhu, W. Liang, L. Liao, and J. Wan, \u201cHybrid mechanistic and neural network modeling of nuclear reactors,\u201d Energy, vol. 282, p. 128931, 2023.   \n[19] R. Cheng, A. Verma, G. Orosz, S. Chaudhuri, Y. Yue, and J. Burdick, \u201cControl regularization for reduced variance reinforcement learning,\u201d in International Conference on Machine Learning. PMLR, 2019, pp. 1141\u20131150.   \n[20] E. Real, S. Moore, A. Selle, S. Saxena, Y. L. Suematsu, J. Tan, Q. V. Le, and A. Kurakin, \u201cLarge-scale evolution of image classifiers,\u201d in International conference on machine learning. PMLR, 2017, pp. 2902\u20132911.   \n[21] T. N. Mundhenk, M. Landajuela, R. Glatt, C. P. Santiago, D. M. Faissol, and B. K. Petersen, \u201cSymbolic regression via neural-guided genetic programming population seeding,\u201d in Proceedings of the 35th International Conference on Neural Information Processing Systems, 2021, pp. 24 912\u201324 923.   \n[22] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., \u201cLanguage models are few-shot learners,\u201d Advances in neural information processing systems, vol. 33, pp. 1877\u20131901, 2020.   \n[23] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou et al., \u201cChain-ofthought prompting elicits reasoning in large language models,\u201d Advances in neural information processing systems, vol. 35, pp. 24 824\u201324 837, 2022.   \n[24] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann et al., \u201cPalm: Scaling language modeling with pathways,\u201d Journal of Machine Learning Research, vol. 24, no. 240, pp. 1\u2013113, 2023.   \n[25] M. J. Kearns and U. Vazirani, An introduction to computational learning theory. MIT press, 1994.   \n[26] H. A. Simon, \u201cThe architecture of complexity,\u201d Proceedings of the American philosophical society, vol. 106, no. 6, pp. 467\u2013482, 1962.   \n[27] T. L. Rogers, B. J. Johnson, and S. B. Munch, \u201cChaos is not rare in natural ecosystems,\u201d Nature Ecology & Evolution, vol. 6, no. 8, pp. 1105\u20131111, 2022.   \n[28] M. Sokolov, M. von Stosch, H. Narayanan, F. Feidl, and A. Butt\u00e9, \u201cHybrid modeling\u2014a key enabler towards realizing digital twins in biopharma?\u201d Current Opinion in Chemical Engineering, vol. 34, p. 100715, 2021.   \n[29] S. Chaudhuri, K. Ellis, O. Polozov, R. Singh, A. Solar-Lezama, Y. Yue et al., \u201cNeurosymbolic programming,\u201d Foundations and Trends\u00ae in Programming Languages, vol. 7, no. 3, pp. 158\u2013 243, 2021.   \n[30] A. Tsoularis and J. Wallace, \u201cAnalysis of logistic growth models,\u201d Mathematical biosciences, vol. 179, no. 1, pp. 21\u201355, 2002.   \n[31] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., \u201cPytorch: An imperative style, high-performance deep learning library,\u201d Advances in neural information processing systems, vol. 32, 2019.   \n[32] D. P. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d arXiv preprint arXiv:1412.6980, 2014.   \n[33] J. H. Holland, \u201cGenetic algorithms,\u201d Scientific american, vol. 267, no. 1, pp. 66\u201373, 1992.   \n[34] L. R. Rabiner, \u201cA tutorial on hidden markov models and selected applications in speech recognition,\u201d Proceedings of the IEEE, vol. 77, no. 2, pp. 257\u2013286, 1989.   \n[35] R. Kalman, \u201cA new approach to linear filtering and prediction problems,\u201d Trans. ASME, D, vol. 82, pp. 35\u201344, 1960.   \n[36] L. Li, Y. Zhao, D. Jiang, Y. Zhang, F. Wang, I. Gonzalez, E. Valentin, and H. Sahli, \u201cHybrid deep neural network\u2013hidden markov model (dnn-hmm) based speech emotion recognition,\u201d in 2013 Humaine association conference on affective computing and intelligent interaction. IEEE, 2013, pp. 312\u2013317.   \n[37] R. G. Krishnan, U. Shalit, and D. Sontag, \u201cDeep kalman filters,\u201d arXiv preprint arXiv:1511.05121, 2015.   \n[38] J. L. Elman, \u201cFinding structure in time,\u201d Cognitive science, vol. 14, no. 2, pp. 179\u2013211, 1990.   \n[39] S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.   \n[40] K. Cho, B. Van Merri\u00ebnboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio, \u201cLearning phrase representations using rnn encoder-decoder for statistical machine translation,\u201d arXiv preprint arXiv:1406.1078, 2014.   \n[41] I. Sutskever, O. Vinyals, and Q. V. Le, \u201cSequence to sequence learning with neural networks,\u201d Advances in neural information processing systems, vol. 27, 2014.   \n[42] D. Bahdanau, K. Cho, and Y. Bengio, \u201cNeural machine translation by jointly learning to align and translate,\u201d arXiv preprint arXiv:1409.0473, 2014.   \n[43] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d Advances in neural information processing systems, vol. 30, 2017.   \n[44] E. Dupont, A. Doucet, and Y. W. Teh, \u201cAugmented neural odes,\u201d Advances in neural information processing systems, vol. 32, 2019.   \n[45] S. I. Holt, Z. Qian, and M. van der Schaar, \u201cNeural laplace: Learning diverse classes of differential equations in the laplace domain,\u201d in International Conference on Machine Learning. PMLR, 2022, pp. 8811\u20138832.   \n[46] M. A. Zaytar and C. El Amrani, \u201cSequence to sequence weather forecasting with long short-term memory recurrent neural networks,\u201d International Journal of Computer Applications, vol. 143, no. 11, pp. 7\u201311, 2016.   \n[47] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBert: Pre-training of deep bidirectional transformers for language understanding,\u201d arXiv preprint arXiv:1810.04805, 2018.   \n[48] L. Sehovac and K. Grolinger, \u201cDeep learning for load forecasting: Sequence to sequence recurrent neural networks with attention,\u201d Ieee Access, vol. 8, pp. 36 411\u201336 426, 2020.   \n[49] S. Holt, A. H\u00fcy\u00fck, Z. Qian, H. Sun, and M. van der Schaar, \u201cNeural laplace control for continuous-time delayed systems,\u201d in International Conference on Artificial Intelligence and Statistics. PMLR, 2023, pp. 1747\u20131778.   \n[50] M. Schmidt and H. Lipson, \u201cDistilling free-form natural laws from experimental data,\u201d science, vol. 324, no. 5923, pp. 81\u201385, 2009.   \n[51] Z. Qian, K. Kacprzyk, and M. van der Schaar, \u201cD-CODE: Discovering closed-form ODEs from observed trajectories,\u201d in International Conference on Learning Representations, 2022. [Online]. Available: https://openreview.net/forum?id $\\equiv$ wENMvIsxNN   \n[52] K. Kacprzyk, Z. Qian, and M. van der Schaar, \u201cD-cipher: discovery of closed-form partial differential equations,\u201d Advances in Neural Information Processing Systems, vol. 36, 2024.   \n[53] K. Kacprzyk, T. Liu, and M. van der Schaar, \u201cTowards transparent time series forecasting,\u201d in The Twelfth International Conference on Learning Representations, 2024. [Online]. Available: https://openreview.net/forum?id=TYXtXLYHpR   \n[54] S. Cuomo, V. S. Di Cola, F. Giampaolo, G. Rozza, M. Raissi, and F. Piccialli, \u201cScientific machine learning through physics\u2013informed neural networks: Where we are and what\u2019s next,\u201d Journal of Scientific Computing, vol. 92, no. 3, p. 88, 2022.   \n[55] S. Greydanus, M. Dzamba, and J. Yosinski, \u201cHamiltonian neural networks,\u201d Advances in neural information processing systems, vol. 32, 2019.   \n[56] M. Cranmer, S. Greydanus, S. Hoyer, P. Battaglia, D. Spergel, and S. Ho, \u201cLagrangian neural networks,\u201d arXiv preprint arXiv:2003.04630, 2020.   \n[57] Y. Yin, V. Le Guen, J. Dona, E. de B\u00e9zenac, I. Ayed, N. Thome, and P. Gallinari, \u201cAugmenting physical models with deep networks for complex dynamics forecasting,\u201d Journal of Statistical Mechanics: Theory and Experiment, vol. 2021, no. 12, p. 124012, 2021.   \n[58] N. Takeishi and A. Kalousis, \u201cPhysics-integrated variational autoencoders for robust and interpretable generative modeling,\u201d Advances in Neural Information Processing Systems, vol. 34, pp. 14 809\u201314 821, 2021.   \n[59] Z. Qian, W. Zame, L. Fleuren, P. Elbers, and M. van der Schaar, \u201cIntegrating expert odes into neural odes: pharmacology and disease progression,\u201d Advances in Neural Information Processing Systems, vol. 34, pp. 11 364\u201311 383, 2021.   \n[60] A. Wehenkel, J. Behrmann, H. Hsu, G. Sapiro, G. Louppe, and J.-H. Jacobsen, \u201cRobust hybrid learning with expert augmentation,\u201d Transactions on Machine Learning Research, 2023. [Online]. Available: https://openreview.net/forum?id=oe4dl4MCGY   \n[61] C. Geng, H. Paganetti, and C. Grassberger, \u201cPrediction of Treatment Response for Combined Chemo- and Radiation Therapy for Non-Small Cell Lung Cancer Patients Using a Bio-Mathematical Model,\u201d Scientific Reports, vol. 7, no. 1, p. 13542, Oct. 2017.   \n[62] I. Bica, A. M. Alaa, J. Jordon, and M. van der Schaar, \u201cEstimating counterfactual treatment outcomes over time through adversarially balanced representations,\u201d in International Conference on Learning Representations, 2020.   \n[63] N. Seedat, F. Imrie, A. Bellot, Z. Qian, and M. van der Schaar, \u201cContinuous-time modeling of counterfactual outcomes using neural controlled differential equations,\u201d arXiv preprint arXiv:2206.08311, 2022.   \n[64] V. Melnychuk, D. Frauen, and S. Feuerriegel, \u201cCausal transformer for estimating counterfactual outcomes,\u201d in International Conference on Machine Learning. PMLR, 2022, pp. 15 293\u2013 15 329.   \n[65] C. C. Kerr, R. M. Stuart, D. Mistry, R. G. Abeysuriya, K. Rosenfeld, G. R. Hart, R. C. N\u00fa\u00f1ez, J. A. Cohen, P. Selvaraj, B. Hagedorn et al., \u201cCovasim: an agent-based model of covid-19 dynamics and interventions,\u201d PLOS Computational Biology, vol. 17, no. 7, p. e1009149, 2021.   \n[66] T. Hiltunen, L. Jones, S. Ellner, and N. G. Hairston Jr, \u201cTemporal dynamics of a simple community with intraguild predation: an experimental test,\u201d Ecology, vol. 94, no. 4, pp. 773\u2013 779, 2013.   \n[67] E. P. Odum and G. W. Barrett, \u201cFundamentals of ecology,\u201d The Journal of Wildlife Management, vol. 36, no. 4, p. 1372, 1972.   \n[68] V. M. M. Alvarez, R. Ro\u00b8sca, and C. G. F\u02d8alcut\u00b8escu, \u201cDynode: Neural ordinary differential equations for dynamics modeling in continuous control,\u201d arXiv preprint arXiv:2009.04278, 2020.   \n[69] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, \u201cLearning representations by backpropagating errors,\u201d nature, vol. 323, no. 6088, pp. 533\u2013536, 1986.   \n[70] S. Holt, A. H\u00fcy\u00fck, and M. van der Schaar, \u201cActive observing in continuous-time control,\u201d Advances in Neural Information Processing Systems, vol. 36, 2024.   \n[71] S. Holt, Z. Qian, and M. van der Schaar, \u201cDeep generative symbolic regression,\u201d in The Eleventh International Conference on Learning Representations, 2023. [Online]. Available: https://openreview.net/forum?id=o7koEEMA1bR   \n[72] K. Kacprzyk, S. Holt, J. Berrevoets, Z. Qian, and M. van der Schaar, \u201cODE discovery for longitudinal heterogeneous treatment effects inference,\u201d in The Twelfth International Conference on Learning Representations, 2024. [Online]. Available: https://openreview.net/forum?id=pxI5IPeWgW   \n[73] T. Liu, N. Astorga, N. Seedat, and M. van der Schaar, \u201cLarge language models to enhance bayesian optimization,\u201d in The Twelfth International Conference on Learning Representations, 2024. [Online]. Available: https://openreview.net/forum?id=OOxotBmGol   \n[74] C. Yang, X. Wang, Y. Lu, H. Liu, Q. V. Le, D. Zhou, and X. Chen, \u201cLarge language models as optimizers,\u201d in The Twelfth International Conference on Learning Representations, 2024. [Online]. Available: https://openreview.net/forum?id=Bb4VGOWELI   \n[75] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. D. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman et al., \u201cEvaluating large language models trained on code,\u201d arXiv preprint arXiv:2107.03374, 2021.   \n[76] S. Holt, M. R. Luyten, and M. van der Schaar, \u201cL2MAC: Large language model automatic computer for extensive code generation,\u201d in The Twelfth International Conference on Learning Representations, 2024. [Online]. Available: https://openreview.net/forum?id=EhrzQwsV4K   \n[77] S. Holt, Z. Qian, T. Liu, J. Weatherall, and M. van der Schaar, \u201cData-driven discovery of dynamical systems in pharmacology using large language models,\u201d in The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024.   \n[78] N. Astorga, T. Liu, N. Seedat, and M. van der Schaar, \u201cPartially observable cost-aware activelearning with large language models,\u201d in The Thirty-Eighth Annual Conference on Neural Information Processing Systems, 2024.   \n[79] W. Bonnaff\u00e9 and T. Coulson, \u201cFast fitting of neural ordinary differential equations by bayesian neural gradient matching to infer ecological interactions from time-series data,\u201d Methods in Ecology and Evolution, vol. 14, no. 6, pp. 1543\u20131563, 2023.   \n[80] S. K. Kumar, \u201cOn weight initialization in deep neural networks,\u201d arXiv preprint arXiv:1704.08863, 2017.   \n[81] A. Graves, S. Fern\u00e1ndez, and J. Schmidhuber, \u201cMulti-dimensional recurrent neural networks,\u201d in International conference on artificial neural networks. Springer, 2007, pp. 549\u2013558.   \n[82] B. K. Petersen, M. L. Larma, T. N. Mundhenk, C. P. Santiago, S. K. Kim, and J. T. Kim, \u201cDeep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients,\u201d in International Conference on Learning Representations, 2020.   \n[83] S. Hsiang, D. Allen, S. Annan-Phan, K. Bell, I. Bolliger, T. Chong, H. Druckenmiller, L. Y. Huang, A. Hultgren, E. Krasovich et al., \u201cThe effect of large-scale anti-contagion policies on the covid-19 pandemic,\u201d Nature, vol. 584, no. 7820, pp. 262\u2013267, 2020.   \n[84] O. N. Bj\u00f8rnstad, K. Shea, M. Krzywinski, and N. Altman, \u201cThe seirs model for infectious disease dynamics.\u201d Nature methods, vol. 17, no. 6, pp. 557\u2013559, 2020.   \n[85] F. Brauer, C. Castillo-Chavez, and C. Castillo-Chavez, Mathematical models in population biology and epidemiology. Springer, 2012, vol. 2. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A HDTwinGen Overview 17 ", "page_idx": 15}, {"type": "text", "text": "B Extended Related Work 17 ", "page_idx": 15}, {"type": "text", "text": "C Benchmark Dataset Environment Details 18 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "C.1 Cancer PKPD 18   \nC.2 COVID-19 19   \nC.3 Plankton Microcosm 19   \nC.4 Hare-Lynx 20 ", "page_idx": 15}, {"type": "text", "text": "D Benchmark Method Implementation Details 20 ", "page_idx": 15}, {"type": "text", "text": "E HDTwinGen Implementation Details 21 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "E.1 HDTwin pseudocode 21   \nE.2 Training HDTwins 21   \nE.3 HDTwinGen Prompt Templates 22   \nE.4 HDTwinGen System Requirements Prompts 23 ", "page_idx": 15}, {"type": "text", "text": "F Model Optimization Losses 27 ", "page_idx": 15}, {"type": "text", "text": "G Evaluation Metrics 27 ", "page_idx": 15}, {"type": "text", "text": "H Additional Results 28 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "H.1 Out-of-distribution Experiment and Setup 28   \nH.2 COVID-19 Unobserved Intervention Experiment and Setup 28   \nH.3 HDTwinGen top-1 decreases over time 28   \nH.4 HDTwinGen Evolution 29   \nH.5 HDTwinGen Ablation No Memory 32   \nH.6 Evaluating Different LLMs 32   \nH.7 Prompt Ablations with Varying Amounts of Prior Information 32   \nH.8 Domain-Specific Baselines 33   \nH.9 Procedurally Generated Synthetic Model Benchmark 34   \nH.10 Interpretability Scale, Performance of only White-Box Models 35   \nH.11 HDTwinGen Flexibly Integrates Expert-in-the-loop Feedback 35   \nH.12 HDTwinGen Accelerates Model Development and Enhances Performance 36 ", "page_idx": 15}, {"type": "text", "text": "I Hybrid Model Output Examples 38 ", "page_idx": 15}, {"type": "text", "text": "HDTwinGen can reason about HDTwins 40 ", "page_idx": 15}, {"type": "text", "text": "A HDTwinGen Overview ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We provide an illustrative example of HDTwinGen working in practice in Figure 5. ", "page_idx": 16}, {"type": "image", "img_path": "SOsiObSdU2/tmp/857b84bbcdc7c64effd9919df4ede8e10eecb68a99057842789fd5fa7d3d6899.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 5: HDTwinGen Illustrative Example in Operation. HDTwinGen can generate and further evolve HDTwins for a particular system based on user-given system requirements and a dataset $P=\\{\\mathcal{D}_{\\mathrm{train}},\\mathcal{D}_{\\mathrm{test}}\\}$ of state-action trajectories. First, the system requirements\u2014which include dataset statistics are incorporated into a prompt and fed into the modeling agent that returns the code for the HDTwin. This HDTwin is then trained on the training dataset $\\ensuremath{\\mathcal{D}}_{\\mathrm{train}}$ , and a validation loss is computed with $\\mathcal{D}_{\\mathrm{val}}$ . In subsequent generations, the evaluation agent is given the existing generated top- $K$ HDTwins, their corresponding validation losses, and validation losses per component, and is asked to reflect on how to improve the HDTwin. This provides detailed, actionable feedback, leveraged from its inherent understanding, and provides this as detailed verbal feedback as $H$ , whereby this feedback is next used with the modeling agent to generate the next HDTwin [P3]. This process iterates several generation times, and the best-performing HDTwin (w.r.t. validation performance) is returned. Overall, this produces an HDTwin that fulfills [P1-P3]. ", "page_idx": 16}, {"type": "text", "text": "B Extended Related Work ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Sequence models. ML approaches frequently address system dynamics as a sequential modeling problem. Initial models like Hidden Markov Models [34] and Kalman filters [35] made simplifying Markovian and linearity assumptions, later extended to nonlinear settings [36, 37]. Subsequent models, including recurrent neural networks [38], along with their advanced variants [39, 40, 41], introduced the capability to model longer-term dependencies. More recent advancements include attention mechanisms [42] and transformer models [43], significantly improving the handling of long-term dependencies in sequence data. Another line of work, Neural Ordinary Differential Equations (NODE) [14, 44, 45], interprets neural network operations as integrations of differential equations to model continuous-time processes. Despite being initially driven by natural language processing applications [47], these methods have found utility in modeling complex systems like weather forecasting [46] and energy systems [48]. Furthermore, sequence models can be used in model-based RL [70]. ", "page_idx": 16}, {"type": "text", "text": "Physics-inspired models. Beyond purely data-centric approaches, recent efforts have focused on integrating physical laws into neural system models. Physics-informed neural networks [15, 54] embed physical laws, often as partial differential equations, directly into the learning process. Other notable methods include Hamiltonian Neural Networks [55] and Lagrangian Neural Networks [56], which respect the structural principles of physical systems. These methods are primarily concerned with modeling physics-related phenomenon and require relatively precise knowledge about the system being modeled (e.g. specific differential equations or energy conservation principles) and specialized mechanisms to incorporate them. Regardless, they have demonstrated that the integration of known principles can significantly improve extrapolation abilities beyond the range of training data. We are similarly inspired to incorporate prior knowledge. In contrast, our work aims to integrate more general or partial knowledge flexibly into a hybrid model using LLMs within a evolutionary multi-agent framework, while introducing more generalized mechanisms to incorporate loose-form prior knowledge. ", "page_idx": 16}, {"type": "text", "text": "Discovering closed-form models. Closely aligned with our research are techniques aimed at discovering closed-form mathematical expressions from data. Symbolic regression [10, 71, 72] and methods like Eureqa [50], SINDy [11], and D-CODE [51, 52] have showcased their prowess in discovering physical laws from experimental observations. However, these techniques can struggle in higher-dimensional settings and rely on experts to perform the system decomposition to identify the most relevant variables before feeding this information to the algorithm. Additionally, they also rely on experts to specify the function set and mathematical operations that the algorithm uses to search for symbolic expressions. In contrast, our method autonomously learns both the system decomposition and the functional forms of component dynamics, potentially enhancing scalability and efficiency. Moreover, the incorporation of LLMs facilitates the flexible integration of prior knowledge at various stages of the search process [73, 74]. Furthermore, using LLMs to generate code, prior work has shown LLM multi-agent frameworks\u2019 ability to excel at large code-generation tasks [75, 76], which we could expect to apply here to scale up the size of the generated models in future works. Such future work could also explore acquiring features as well [77, 78]. ", "page_idx": 17}, {"type": "text", "text": "C Benchmark Dataset Environment Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In the following, we outline the six real-world system dynamics datasets, where each dataset is either a real-world dataset or has been sampled from an accurate simulator designed by human experts. ", "page_idx": 17}, {"type": "text", "text": "C.1 Cancer PKPD ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Three of our environments that we sample a dataset from are derived from a state-of-the-art biomedical Pharmacokinetic-Pharmacodynamic (PKPD) model of lung cancer tumor growth, used to simulate the combined effects of chemotherapy and radiotherapy in lung cancer [61]\u2014this has been extensively used by other works [62, 63, 64]. Here we use this bio-mathematical lung cancer model to create three variations of lung cancer under the effect of no treatments (Lung Cancer), chemotherapy only (Lung Cancer (with Chemo.)), and chemotherapy combined with radiotherapy (Lung Cancer (with Chemo. & Radio.)); for each model we sample a respective dataset. First, let us detail the general case of Lung Cancer (with Chemo. & Radio.), which comes from the general model (Cancer PKPD Model), and then detail the variations. ", "page_idx": 17}, {"type": "text", "text": "Cancer PKPD Model. This is a state-of-the-art biomedical Pharmacokinetic-Pharmacodynamic (PKPD) model of tumor growth, that simulates the combined effects of chemotherapy and radiotherapy in lung cancer [61] (Equation (2))\u2014this has been extensively used by other works [62, 63, 64]. Specifically, this models the volume of the tumor $x(t)$ for days $t$ after the cancer diagnosis\u2014where the outcome is one-dimensional. The model has two binary treatments: (1) radiotherapy $u_{t}^{r}$ and (2) chemotherapy $u_{t}^{c}$ . ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{d x(t)}{d t}=\\big(\\underbrace{\\rho\\log\\bigg(\\frac{K}{x(t)}\\bigg)}_{\\mathrm{Tumorgrowth}}-\\underbrace{\\beta_{c}C(t)}_{\\mathrm{Chemotherapy}}-\\underbrace{(\\alpha_{r}d(t)+\\beta_{r}d(t)^{2})}_{\\mathrm{Radiotherapy}}\\big)x(t)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Where the parameters $K,\\rho,\\beta_{c},\\alpha_{r},\\beta_{r}$ for each simulated patient are detailed in [61], which are also described in Table 3. Additionally, the chemotherapy drug concentration $c(t)$ follows an exponential ", "page_idx": 17}, {"type": "table", "img_path": "SOsiObSdU2/tmp/eca01b3e415443f0b8c8c58c0c9e9104196278fa2be1c8a309d737f95432e7ef.jpg", "table_caption": ["Table 3: Cancer PKPD parameter values. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "decay relationship with a half-life of one day: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{d c(t)}{d t}=-0.5c(t)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the chemotherapy binary action represents increasing the $c(t)$ concentration by $5.0\\mathrm{mg/m^{3}}$ of Vinblastine given at time $t$ . Whereas the radiotherapy concentration $d(t)$ represents $2.0G y$ fractions of radiotherapy given at timestep $t$ , where Gy is the Gray ionizing radiation dose. ", "page_idx": 18}, {"type": "text", "text": "Time-dependent confounding. We introduce time-varying confounding into the data generation process. This is accomplished by characterizing the allocation of chemotherapy and radiotherapy as Bernoulli random variables. The associated probabilities, $p_{c}$ and $p_{r}$ , are determined by the tumor diameter as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\np_{c}(t)=\\sigma\\left(\\frac{\\gamma_{c}}{D_{\\operatorname*{max}}}(\\bar{D}(t)-\\delta_{c})\\right)\\qquad\\qquad p_{r}(t)=\\sigma\\left(\\frac{\\gamma_{r}}{D_{\\operatorname*{max}}}(\\bar{D}(t)-\\delta_{r})\\right),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $D_{\\mathrm{max}}=13\\mathrm{cm}$ represents the largest tumor diameter, $\\theta_{c}=\\theta_{r}=D_{\\operatorname*{max}}/2$ and $\\bar{D}(t)$ signifies the mean tumor diameter. The parameters $\\gamma_{c}$ and $\\gamma_{r}$ manage the extent of time-varying confounding. We use $\\gamma_{c}=\\gamma_{r}=2$ . ", "page_idx": 18}, {"type": "text", "text": "Sampling datasets. Using the above Cancer PKPD model, we sample $N\\,=\\,1,000$ trajectories, which equates to $N=10,000$ patients, where we sample their initial tumor volumes from a uniform distribution $x(0)\\sim\\mathcal{U}(0,1149)$ , and use the Cancer PKPD Equation (2) along with the action policy of Equation (4) to forward simulate patient trajectories for 60 days, using a Euler stepwise solver. This forms one dataset sample. We repeat this process with independent random seeds to generate $D_{\\mathrm{train}},D_{\\mathrm{val}},D_{\\mathrm{test}}$ . Specifically for each benchmark method run for random seed, we re-sample the datasets. For each variation described above, we either include the chemotherapy dosing action, chemotherapy and radiotherapy dosing action or neither. We further outline this dataset\u2019s system description and variable descriptions with the following prompt template as given in Appendix E.4. ", "page_idx": 18}, {"type": "text", "text": "C.2 COVID-19 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We use the accurate and complex epidemic agent-based simulator of COVASIM [65] to simulate COVID-19 epidemics. This is an advanced simulator that is capable of simulating non-pharmaceutical interventions (such as lockdowns through social distancing, and school closures) and pharmaceutical interventions (such as vaccinations). As this is an agent-based simulator, each agent is an individual in a population, and they can be in one of the following states minimally, of being susceptible to COVID-19, exposed, infectious or recovered (which includes deaths). We use the simulator with the default parameters set by the open source implementation of the simulator 5. COVASIM runs a simulation for a population of individuals. To ensure an accurate simulation, we simulate 24 countries collecting trajectories for each, wherein each simulation we use a population size of 1, 000, 000 individuals, and simulate each individual separately (disabling simulation rescaling) and start with a random number of individuals who are infected with COVID-19, $I(0)=\\mathcal{U}(1\\bar{0},000,100,000)$ , and forward simulate the simulation for 60 days. We repeat this process with independent random seeds to generate $D_{\\mathrm{train}},D_{\\mathrm{val}},D_{\\mathrm{test}}$ . Specifically for each benchmark method run for random seed, we re-sample the datasets. We further outline this dataset\u2019s system description and variable descriptions with the following prompt template as given in Appendix E.4. ", "page_idx": 18}, {"type": "text", "text": "C.3 Plankton Microcosm ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "This describes an ecological model of a microcosm of algae, flagellate, and rotifer populations, thus replicating an experimental three-species prey-predator system [66]. We use the dataset made available by $[79]^{6}$ . The dataset consists of a single trajectory of 102 time steps, and we use a train, val, test split of $70\\%$ , $15\\%$ and $15\\%$ , ensuring the splits are along the time dimension to maintain the integrity of temporal causality, following their chronological order. We further outline this dataset\u2019s system description and variable descriptions with the following prompt template as given in Appendix E.4. ", "page_idx": 18}, {"type": "text", "text": "C.4 Hare-Lynx ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "This describes a real-world dataset of hare and lynx populations, replicating predator-prey dynamics [67]. We use the dataset made available by [79]. The dataset consists of a single trajectory of 92 time steps, and we use a train, val, test split of $70\\%$ , $15\\%$ and $15\\%$ , ensuring the splits are along the time dimension to maintain the integrity of temporal causality, following their chronological order. We further outline this dataset\u2019s system description and variable descriptions with the following prompt template as given in Appendix E.4. ", "page_idx": 19}, {"type": "text", "text": "D Benchmark Method Implementation Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "To be competitive we compare against popular black-box models, which, when modeling the dynamics of a system over time, becomes a form of ODE model, that is a neural ODE [14] with action inputs (DyNODE) [68]. We also compare against transparent dynamical equations derived from equation discovery methods for ODEs such as Sparse Identification of Nonlinear Dynamics (SINDy) [11]. Moreover, we compare against the ablations of our method, of the zero-shot generated HDTwin (ZeroShot) and this model with subsequently optimized parameters (ZeroOptim). ", "page_idx": 19}, {"type": "text", "text": "DyNODE ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "DyNODE is a black-box neural network-based dynamics model [68], that models the underlying dynamics of a system by incorporating control into the standard neural ordinary differential equation framework [14]. We use a DyNODE model with 3-layer Multilayer Perceptron (MLP), with a hidden dimension of 128 units, with tanh activation functions, and make it competitive by using Xavier weight initialization [80]. To be competitive we use the same objective, optimizer and same hyperparameters for the optimizer that we use in HDTwinGen. That of an Adam optimizer [32], with a learning rate of 0.01, with a batch size of 1,000 and early stopping with a patience of 20, and train it for 2,000 epochs to ensure it converges. ", "page_idx": 19}, {"type": "text", "text": "Causal Transformer ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Causal Transformer is a state-of-the-art transformer model for estimating counterfactual outcomes [64]. Due to the complexity of the Causal Transformer, incorporating three separate transformer networks, each one for processing covariates, past treatments, and past outcomes, respectively\u2014 which is unique to estimating counterfactual outcomes in treatment effect settings; we implemented only a single transformer to model the past outcomes, which is applicable to our datasets and task domains. Specifically, this consists of a standard transformer encoder, where the input dataset is normalized to the training dataset. We encode input observed dimension of the state-action into an embedding vector dimension of size 250 through a linear layer, followed by the addition of a standard positional encoder [64]; this is then fed into a transformer encoder layer, with a head size of 10, dropout 0.1, and the output of this is then fed into a linear layer to reconstruct the next step ahead state, of size of the state dimension. We train this model using the AdamW [32] optimizer with a learning rate of 0.00005 and a step learning rate scheduler of step size 1.0 and gamma 0.95; we also implement gradient clipping to 0.7, with a batch size of 1,000 and early stopping with a patience of 20, and train it for 2,000 epochs to ensure it converges. ", "page_idx": 19}, {"type": "text", "text": "RNN ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Recurrent Neural Network [81] is a standard baseline that is widely used in autoregressive time series next step ahead prediction. We implement this where the input dataset is normalized to the training dataset. It consists of a gated recurrent unit RNN taking the state-action dimension in mapping it to a hidden dimension of size 250, with two layers. The output is then fed to a linear layer to convert the hidden dimension back to the state dimension to predict the next step ahead. To be competitive we use the same objective, optimizer and same hyperparameters for the optimizer that we use in HDTwinGen. That of an Adam optimizer [32], with a learning rate of 0.01, with a batch size of 1,000 and early stopping with a patience of 20, and train it for 2,000 epochs to ensure it converges. ", "page_idx": 19}, {"type": "text", "text": "SINDy ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Sparse Identification of Nonlinear Dynamics (SINDy) [11], is a data-driven framework that aims to discover the governing dynamical system equations directly from time-series data, discovering a white-box closed-form mathematical model. The algorithm works by iteratively performing sparse regression on a library of candidate functions to identify the sparsest yet most accurate representation of the dynamical system. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "In our implementation, we use a polynomial library of order two, which is a feature library of $\\mathcal{L}=\\{1,\\bar{x_{0}},x_{1},x_{0}x_{1}\\}$ . Finite difference approximations are used to compute time derivatives from the input time-series data, of order one. Here the alpha parameter is kept constant at 0.5 across all experiments, and the sparsity threshold is set to 0.02 for all experiments, apart from the COVID-19 dataset where it is set to $1\\times10^{-5}$ . ", "page_idx": 20}, {"type": "text", "text": "APHYNITY ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "APHYNITY [60] is implemented using domain-specific expert models as defined in Appendix H.8 combined with a 3-layer MLP, with the same hyper-parameters as in [60]. ", "page_idx": 20}, {"type": "text", "text": "GP ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Genetic programming (GP) is implemented using the implementation and hyper-parameters from the baseline in [82]. ", "page_idx": 20}, {"type": "text", "text": "HDTwinGen ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "See the section Appendix E for the implementation details. Specifically, ZeroShot and ZeroOptim are ablations of our method using the exact same setup, hyperparameters and prompts. Here ZeroShot generates one HDTwin, and does not fit its parameters, thus evaluating the loss of the model output directly from the LLM. Whereas ZeroOptim, repeats ZeroShot with the additional step of optimizing the parameters of the HDTwin that was generated\u2014again using the same training as detailed in Appendix E.2. ", "page_idx": 20}, {"type": "text", "text": "E HDTwinGen Implementation Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Our proposed method follows the framework as described in Section 4. We present pseudocode in Appendix E.1, how the code-generated HDTwins are trained in Appendix E.2, prompt templates in Appendix E.3, system requirements prompts in Appendix E.4 for each dataset, and we provide examples of training runs in Appendix J. Specifically, we find a top-K, where $K=16$ is sufficient. Additionally, we use the LLM of GPT4-1106-Preview, with a temperature of 0.7. ", "page_idx": 20}, {"type": "text", "text": "E.1 HDTwin pseudocode ", "text_level": 1, "page_idx": 20}, {"type": "table", "img_path": "SOsiObSdU2/tmp/64697d307b685775a0d51ca990cf047ec6cb759b7fc6178679c6ae18d2f851b9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "E.2 Training HDTwins ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Once the modeling agent has generated an HDTwin $f_{\\theta,\\omega({\\theta})}$ , it generates it as code. Specifically, it outputs code for a PyTorch [31] neural network module, where this code string is executed, and the module is then trained on the training dataset. The agent importantly observes a code skeleton within its system requirements context $S^{c o n t e x t}$ , of which examples of such a skeleton are given in Appendix E.4. However we stipulate that the skeleton must be a \u201ctorch.nn.Module\u201d, be called \u201cStateDifferential\u201d, and the parameters must be initialized, and it must define a forward function for computing the state differential of the state, where the state and action for that system are input variables to the function. The LLM is instructed to not to modify the code skeleton, only complete it, and return it. This makes it straightforward to process this from text, execute the module, and then train the model. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "Specifically, we train the model on the training dataset, using the standard MSE loss Equation (5), optimizing using the Adam optimizer [32]. We use the same optimizer hyperparameters as the black-box neural network method, that of a learning rate of 0.01, with a batch size of 1,000 and early stopping with a patience of 20, and train it for 2,000 epochs to ensure it converges, to ensure fair comparison. ", "page_idx": 21}, {"type": "text", "text": "Once the model is trained, we compute the val MSE and val MSE per component, which corresponds to the val loss per state output dimension Equation (6). Notably, when we append the trained HDTwin back into $\\mathcal{P}_{g}$ we include a string representation of it, which includes the values of any named parameters that were initialized in the model. We observe that feeding in the previous optimized parameters helps the LLM in subsequent generations to suggest good starting initial values for the named parameters, from which they can be further refined with the optimization step. ", "page_idx": 21}, {"type": "text", "text": "E.3 HDTwinGen Prompt Templates ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In the following we detail the prompt templates used. We always use the system prompt when interacting with the LLM. ", "page_idx": 21}, {"type": "text", "text": "System prompt ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "O b j e c t i v e : Write code to c r e a t e an e f f e c t i v e d i f f e r e n t i a l e qu a ti o n s i m u l a t o r f o r a given t a s k .   \nPlease note t h a t the code should be f u l l y f u n c t i o n a l . No p l a c e h o l d e r s .   \nYou must a c t autonomously and you w i l l r e c e i v e no human i n p u t a t any s t a g e . You have to r e t u r n as output the complete code f o r completing t h i s task , and c o r r e c t l y improve the code to c r e a t e the most a c c u r a t e and r e a l i s t i c s i m u l a t o r p o s s i b l e .   \nYou always w r i t e out the code c o n t e n t s . You always i n d e n t code with t a b s .   \nYou cannot v i s u a l i z e any g r a p h i c a l output . You e x i s t within a machine . The code can i n c l u d e black box multi \u2212l a y e r p e r c e p t i o n s where r e q u i r e d .   \nUse the f u n c t i o n s provided . When c a l l i n g f u n c t i o n s only provide a RFC8259 compliant JSON r e q u e s t f o l l o w i n g t h i s format without d e v i a t i o n . ", "page_idx": 21}, {"type": "text", "text": "Defined function schema prompt ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\"name \" : \" c o m p l e t e _ S t a t e D i f f e r e n t i a l _ c o d e \" ,   \n\" d e s c r i p t i o n \" : \" Write out the code body f o r the \u2018 S t a t e D i f f e r e n t i a l \u2018 t o r c h model . \" ,   \n\" parameters \" : { \" type \" : \" o b j e c t \" , \" p r o p e r t i e s \" : { \" S t a t e D i f f e r e n t i a l _ c o d e \" : { \" type \" : \" s t r i n g \" , \" d e s c r i p t i o n \" : \u2019Code f o r the \u2018 S t a t e D i f f e r e n t i a l \u2018 t o r c h model , i n c l u s i v e of the model d e f i n i t i o n . I f you are unsure , take your b e s t guess . This must be a nonempty s t r i n g . \u2019 , } , \" c o d e _ d e s c r i p t i o n \" : { \" type \" : \" s t r i n g \" , \" d e s c r i p t i o n \" : \u2019A co ncise d e s c r i p t i o n of the code model , i n d i c a t i n g i f i t i s a white box only or white and black box model . \u2019 , } } , \" r e q u i r e d \" : [ \" S t a t e D i f f e r e n t i a l _ c o d e \" , \" c o d e _ d e s c r i p t i o n \" ] ,   \n} , ", "page_idx": 21}, {"type": "text", "text": "Modeling agent first task prompt ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "You w i l l get a system d e s c r i p t i o n to code a d i f f e r e n t i a l eq u at i on s i m u l a t o r f o r . System D e s c r i p t i o n : \u2018 \u2018   \n{ s y s t e m _ d e s c r i p t i o n } ", "page_idx": 21}, {"type": "text", "text": "Modelling goals : \u2018 \u2018 \u2018 \\* The parameters of the model w i l l be optimized to an observed t r a i n i n g d a t a s e t with the given s i m u l a t o r . \\* The observed t r a i n i n g d a t a s e t has very few samples , and the model must be able to g e n e r a l i z e to unseen data . ", "page_idx": 21}, {"type": "text", "text": "Skeleton code to f i l l in : { skeleton_code } ", "page_idx": 21}, {"type": "text", "text": "\\* The model d e f i n e s the s t a t e d i f f e r e n t i a l and w i l l be used with an ODE s o l v e r to f i t the observed t r a i n i n g d a t a s e t .   \n\\* You can use any parameters you want and any black box n e u r a l network components ( multi \u2212l a y e r p e r c e p t r o n s ) ; however , you have to d e f i n e t h e s e .   \n\\* I t i s p r e f e r a b l e to decompose the system i n t o d i f f e r e n t i a l e q u a t i o n s ( compartments ) i f p o s s i b l e .   \n$*$ You can use any unary f u n c t i o n s , f o r example log , exp , power e t c .   \n\\* Under no circumstance can you change the s k e l e t o n code f u n c t i o n d e f i n i t i o n s , only f i l l in the code .   \n\\* The i n p u t t e n s o r s are v e c t o r s of shape ( b a t c h _ s i z e ) .   \n\\* Use i n i t i a l l y white box models f i r s t and then switch to hybrid white and black box models f o r the r e s i d u a l s , only a f t e r no f u r t h e r b e s t program i t e r a t i o n improvement with white box models .   \n\\* Make sure your code follows the exact code s k e l e t o n s p e c i f i c a t i o n .   \n\\* Use PyTorch .   \n\u2018 \u2018 ", "page_idx": 22}, {"type": "text", "text": "Think step \u2212by\u2212step , and then give the complete f u l l working code . You are g e n e r a t i n g code f o r i t e r a t i o n { c u r r e n t _ i t e r a t i o n } out of { g e n e r a t i o n s } . ", "page_idx": 22}, {"type": "text", "text": "Reflection prompt ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\"\"\"   \nYou g e n e r a t e d the f o l l o w i n g code completions , which then had t h e i r parameters optimized to the t r a i n i n g d a t a s e t . Please r e f l e c t on how you can improve the code to minimize the v a l i d a t i o n l o s s to 1e\u22126 or l e s s . The code examples are d e l i n e a t e d by ###.   \nHere are your p r ev i ou s i t e r a t i o n s the b e s t programs g e n e r a t e d . Use i t to see i f you have exhausted white box models , i . e . when a white box model r e p e a t s with the same val l o s s and then only add black box models to the white box models : \u2018 \u2018 \u2018   \n{ h i s t o r y _ b e s t _ c o m p l e t i o n s _ s t r } \u2018 \u2018 \u2018   \nHere are the top code completions so f a r t h a t you have generated , s o r t e d f o r the lowest v a l i d a t i o n l o s s l a s t : \u2018 \u2018   \n{ completions }   \nPlease r e f l e c t on how you can improve the code to f i t the d a t a s e t as a c c u r a t e l y as p o s s i b l e , and be i n t e r p r e t a b l e . Think step \u2212by\u2212 s t e p . Provide only a c t i o n a b l e feedback , t h a t has d i r e c t changes to the code . Do not w r i t e out the code , only d e s c r i b e how i t can be improved . Where a p p l i c a b l e use the values of the optimized parameters to reason how the code can be improved to f i t the d a t a s e t as a c c u r a t e l y as p o s s i b l e . This i s f o r g e n e r a t i n g new code f o r the next i t e r a t i o n { i t e r a t i o n } out of { s e l f . config . run . g e n e r a t i o n s } . ", "page_idx": 22}, {"type": "text", "text": "Modeling agent in subsequent generations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\"\"\"   \nPlease now r e g e n e r a t e the code function , with the aim to improve the code to achieve a lower v a l i d a t i o n e r r o r . Use the feedback where a p p l i c a b l e . You are g e n e r a t i n g code f o r i t e r a t i o n { g e n e r a t i o n _ i d } out of { s e l f . config . run . g e n e r a t i o n s } t o t a l i t e r a t i o n s . When g e n e r a t i n g code i f you are unsure about something , take your b e s t guess . You have to g e n e r a t e code , and cannot give an empty s t r i n g answer .   \nPlease always only f i l l in the f o l l o w i n g code s k e l e t o n : \u2018 \u2018   \n{ prompts . g e t _ s k e l e t o n _ c o d e ( s e l f . env . env_name ) }   \nYou cannot change the code skeleton , or i n p u t v a r i a b l e s . \"\"\" ", "page_idx": 22}, {"type": "text", "text": "E.4 HDTwinGen System Requirements Prompts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "By following our proposed system requirements format, we constructed prompts for each of the datasets that we evaluated against, which are listed in the following. ", "page_idx": 22}, {"type": "text", "text": "Lung Cancer (with Chemo. & Radio.) ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "You w i l l get a system d e s c r i p t i o n to code a d i f f e r e n t i a l eq u at i on s i m u l a t o r f o r .   \nSystem D e s c r i p t i o n : \u2018 \u2018 \u2018   \nP r e d i c t i o n of Treatment Response f o r Combined Chemo and R a d i a t i o n Therapy f o r Non\u2212Small Cell Lung Cancer P a t i e n t s Using a Bio \u2212 Mathematical Model   \nHere you must model the s t a t e d i f f e r e n t i a l of tumor_volume , and chemotherapy_drug_concentration ; with the i n p u t a c t i o n s of chemotherapy_dosage , and r a d i o t h e r a p y _ d o s a g e .   \nD e s c r i p t i o n of the v a r i a b l e s :   \n\\* tumor_volume : Volume of the tumor with u n i t s $\\mathrm{cm}^{\\wedge}3$   \n\\* chemotherapy_drug_concentration : C o n c e n t r a t i o n of the chemotherapy drug v i n b l a s t i n e with u n i t s $\\mathfrak{m}\\mathrm{g}/\\mathfrak{m}^{\\wedge}3$   \n\\* chemotherapy_dosage : Dosage of the chemotherapy drug v i n b l a s t i n e with u n i t s $\\mathrm{mg}/\\mathrm{m}^{\\wedge}3$   \n\\* r a d i o t h e r a p y _ d o s a g e : Dosage of the r a d i o t h e r a p y with u n i t s Gy   \nThe time u n i t s i s in days .   \nA d d i t i o n a l l y t h e s e v a r i a b l e s have the ranges of :   \n\\* tumor_volume : [0.01433 , 1170.861]   \n\\* chemotherapy_drug_concentration : [0 , 9.9975]   \n\\* chemotherapy_dosage : [0 , 5 . 0 ]   \n\\* r a d i o t h e r a p y _ d o s a g e : [0 , 2 . 0 ]   \nThe t r a i n i n g d a t a s e t c o n s i s t s of 1000 p a t i e n t s , where each p a t i e n t i s observed f o r 60 days .   \n\u2018 \u2018 ", "page_idx": 22}, {"type": "text", "text": "Modelling goals : \u2018 \u2018 \u2018 \\* The parameters of the model w i l l be optimized to an observed t r a i n i n g d a t a s e t with the given s i m u l a t o r . \\* The observed t r a i n i n g d a t a s e t has very few samples , and the model must be able to g e n e r a l i z e to unseen data ", "page_idx": 22}, {"type": "text", "text": "def _ _ i n i t _ _ ( s e l f ) : super ( S t a t e D i f f e r e n t i a l , s e l f ) . _ _ i n i t _ _ ( ) # TODO: F i l l in the code here ", "page_idx": 23}, {"type": "text", "text": "def forward ( s e l f , tumor_volume : t o r c h . Tensor , chemotherapy_drug_concentration : t o r c h . Tensor , chemotherapy_dosage : t o r c h . Tensor , r a d i o t h e r a p y _ d o s a g e : t o r c h . Tensor ) $->$ Tuple [ t o r c h . Tensor , t o r c h . Tensor ] : # TODO: F i l l in the code here r e t u r n ( d_tumor_volume__dt , d_chemotherapy_drug_concentration__dt ) ", "page_idx": 23}, {"type": "text", "text": "Useful to know : \u2018 \u2018 \u2018   \n\\* You are a code evolving machine , and you w i l l be c a l l e d 20 times to g e n e r a t e code , and improve the code to achieve the lowest p o s s i b l e v a l i d a t i o n l o s s .   \n\\* The model d e f i n e s the s t a t e d i f f e r e n t i a l and w i l l be used with an ODE s o l v e r to f i t the observed t r a i n i n g d a t a s e t .   \n\\* You can use any parameters you want and any black box n e u r a l network components ( multi \u2212l a y e r p e r c e p t r o n s ) ; however , you have to d e f i n e t h e s e .   \n\\* I t i s p r e f e r a b l e to decompose the system i n t o d i f f e r e n t i a l e q u a t i o n s ( compartments ) i f p o s s i b l e .   \n\\* You can use any unary f u n c t i o n s , f o r example log , exp , power e t c .   \n\\* Under no circumstance can you change the s k e l e t o n code f u n c t i o n d e f i n i t i o n s , only f i l l in the code .   \n\\* The i n p u t t e n s o r s are v e c t o r s of shape ( b a t c h _ s i z e ) .   \n\\* Use i n i t i a l l y white box models f i r s t and then switch to hybrid white and black box models f o r the r e s i d u a l s , only a f t e r no f u r t h e r b e s t program i t e r a t i o n improvement with white box models .   \n\\* Make sure your code follows the exact code s k e l e t o n s p e c i f i c a t i o n .   \n\\* Use PyTorch . ", "page_idx": 23}, {"type": "text", "text": "Think step \u2212by\u2212step , and then give the complete f u l l working code . You are g e n e r a t i n g code f o r i t e r a t i o n 0 out of 20. ", "page_idx": 23}, {"type": "text", "text": "Lung Cancer (with Chemo.) ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "You w i l l get a system d e s c r i p t i o n to code a d i f f e r e n t i a l eq u at i on s i m u l a t o r f o r .   \nSystem D e s c r i p t i o n : \u2018 \u2018 \u2018   \nP r e d i c t i o n of Treatment Response f o r Combined Chemo and R a d i a t i o n Therapy f o r Non\u2212Small Cell Lung Cancer P a t i e n t s Using a Bio \u2212 Mathematical Model   \nHere you must model the s t a t e d i f f e r e n t i a l of tumor_volume , and chemotherapy_drug_concentration ; with the i n p u t a c t i o n s of chemotherapy_dosage   \nD e s c r i p t i o n of the v a r i a b l e s :   \n\\* tumor_volume : Volume of the tumor with u n i t s $\\mathrm{cm}^{\\wedge}3$   \n\\* chemotherapy_drug_concentration : C o n c e n t r a t i o n of the chemotherapy drug v i n b l a s t i n e with u n i t s $\\mathfrak{m}\\mathrm{g}/\\mathfrak{m}^{\\wedge}3$   \n\\* chemotherapy_dosage : Dosage of the chemotherapy drug v i n b l a s t i n e with u n i t s $\\mathrm{mg}/\\mathrm{m}^{\\wedge}3$ ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "The t r a i n i n g d a t a s e t c o n s i s t s of 1000 p a t i e n t s , where each p a t i e n t i s observed f o r 60 days ", "page_idx": 23}, {"type": "text", "text": "Modelling goals : \u2018 \u2018 \u2018 \\* The parameters of the model w i l l be optimized to an observed t r a i n i n g d a t a s e t with the given s i m u l a t o r . \\* The observed t r a i n i n g d a t a s e t has very few samples , and the model must be able to g e n e r a l i z e to unseen data ", "page_idx": 23}, {"type": "text", "text": "Requirement S p e c i f i c a t i o n : \u2018 \u2018 \u2018 \\* The code g e n e r a t e d should achieve the lowest p o s s i b l e v a l i d a t i o n loss , of 1e\u22126 or l e s s . \\* The code g e n e r a t e d should be i n t e r p r e t a b l e , and f i t the d a t a s e t as a c c u r a t e l y as p o s s i b l e ", "page_idx": 23}, {"type": "text", "text": "Skeleton code to f i l l in : \u2018 \u2018 \u2018   \nc l a s s S t a t e D i f f e r e n t i a l ( nn . Module ) : def _ _ i n i t _ _ ( s e l f ) : super ( S t a t e D i f f e r e n t i a l , s e l f ) . _ _ i n i t _ _ # TODO: F i l l in the code here   \nUseful to know : \u2018 \u2018 \u2018   \n\\* You are a code evolving machine , and you w i l l be c a l l e d 20 times to g e n e r a t e code , and improve the code to achieve the lowest p o s s i b l e v a l i d a t i o n l o s s .   \n\\* The model d e f i n e s the s t a t e d i f f e r e n t i a l and w i l l be used with an ODE s o l v e r to f i t the observed t r a i n i n g d a t a s e t .   \n\\* You can use any parameters you want and any black box n e u r a l network components ( multi \u2212l a y e r p e r c e p t r o n s ) ; however , you have to d e f i n e t h e s e .   \n\\* I t i s p r e f e r a b l e to decompose the system i n t o d i f f e r e n t i a l e q u a t i o n s ( compartments ) i f p o s s i b l e .   \n\\* You can use any unary f u n c t i o n s , f o r example log , exp , power e t c .   \n\\* Under no circumstance can you change the s k e l e t o n code f u n c t i o n d e f i n i t i o n s , only f i l l in the code .   \n\\* The i n p u t t e n s o r s are v e c t o r s of shape ( b a t c h _ s i z e ) .   \n\\* Use i n i t i a l l y white box models f i r s t and then switch to hybrid white and black box models f o r the r e s i d u a l s , only a f t e r no f u r t h e r b e s t program i t e r a t i o n improvement with white box models .   \n\\* Make sure your code follows the exact code s k e l e t o n s p e c i f i c a t i o n .   \n\\* Use PyTorch . ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "Think step \u2212by\u2212step , and then give the complete f u l l working code . You are g e n e r a t i n g code f o r i t e r a t i o n 0 out of 20. ", "page_idx": 23}, {"type": "text", "text": "Lung Cancer ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "You w i l l get a system d e s c r i p t i o n to code a d i f f e r e n t i a l eq u at i on s i m u l a t o r f o r .   \nSystem D e s c r i p t i o n : \u2018 \u2018 \u2018   \nP r e d i c t i o n of Treatment Response f o r Combined Chemo and R a d i a t i o n Therapy f o r Non\u2212Small Cell Lung Cancer P a t i e n t s Using a Bio \u2212 Mathematical Model   \nHere you must model the s t a t e d i f f e r e n t i a l of tumor_volume . There are not t r e a t m e n t s a p p l i e d . ", "page_idx": 23}, {"type": "text", "text": "D e s c r i p t i o n of the v a r i a b l e s : \\* tumor_volume : Volume of the tumor with u n i t s $\\mathrm{cm}^{\\wedge}3$ ", "page_idx": 24}, {"type": "text", "text": "The time u n i t s i s in days . ", "page_idx": 24}, {"type": "text", "text": "A d d i t i o n a l l y t h e s e v a r i a b l e s have the ranges of \\* tumor_volume : [0.64196031 , 4852.45734281] ", "page_idx": 24}, {"type": "text", "text": "The t r a i n i n g d a t a s e t c o n s i s t s of 1000 p a t i e n t s , where each p a t i e n t i s observed f o r 60 days ", "page_idx": 24}, {"type": "text", "text": "Modelling goals : \u2018 \u2018 \u2018 \\* The parameters of the model w i l l be optimized to an observed t r a i n i n g d a t a s e t with the given s i m u l a t o r . \\* The observed t r a i n i n g d a t a s e t has very few samples , and the model must be able to g e n e r a l i z e to unseen data ", "page_idx": 24}, {"type": "text", "text": "Requirement S p e c i f i c a t i o n : \u2018 \u2018 \u2018 \\* The code g e n e r a t e d should achieve the lowest p o s s i b l e v a l i d a t i o n loss , of 1e\u22126 or l e s s . \\* The code g e n e r a t e d should be i n t e r p r e t a b l e , and f i t the d a t a s e t as a c c u r a t e l y as p o s s i b l e ", "page_idx": 24}, {"type": "text", "text": "Skeleton code to f i l l in : \u2018   \nc l a s s S t a t e D i f f e r e n t i a l ( nn . Module ) : def _ _ i n i t _ _ ( s e l f ) : super ( S t a t e D i f f e r e n t i a l , s e l f ) . _ _ i n i t _ _ ( ) # TODO: F i l l in the code here   \nUseful to know : \u2018 \u2018 \u2018   \n\\* You are a code evolving machine , and you w i l l be c a l l e d 20 times to g e n e r a t e code , and improve the code to achieve the lowest p o s s i b l e v a l i d a t i o n l o s s .   \n\\* The model d e f i n e s the s t a t e d i f f e r e n t i a l and w i l l be used with an ODE s o l v e r to f i t the observed t r a i n i n g d a t a s e t .   \n\\* You can use any parameters you want and any black box n e u r a l network components ( multi \u2212l a y e r p e r c e p t r o n s ) ; however , you have to d e f i n e t h e s e .   \n\\* I t i s p r e f e r a b l e to decompose the system i n t o d i f f e r e n t i a l e q u a t i o n s ( compartments ) i f p o s s i b l e .   \n\\* You can use any unary f u n c t i o n s , f o r example log , exp , power e t c .   \n\\* Under no circumstance can you change the s k e l e t o n code f u n c t i o n d e f i n i t i o n s , only f i l l in the code .   \n\\* The i n p u t t e n s o r s are v e c t o r s of shape ( b a t c h _ s i z e ) .   \n\\* Use i n i t i a l l y white box models f i r s t and then switch to hybrid white and black box models f o r the r e s i d u a l s , only a f t e r no f u r t h e r b e s t program i t e r a t i o n improvement with white box models .   \n\\* Make sure your code follows the exact code s k e l e t o n s p e c i f i c a t i o n .   \n\\* Use PyTorch . ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "Think step \u2212by\u2212step , and then give the complete f u l l working code . You are g e n e r a t i n g code f o r i t e r a t i o n 0 out of 20. ", "page_idx": 24}, {"type": "text", "text": "Hare-Lynx ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "You w i l l get a system d e s c r i p t i o n to code a d i f f e r e n t i a l eq u at i on s i m u l a t o r f o r ", "page_idx": 24}, {"type": "text", "text": "System D e s c r i p t i o n : \" Modeling Di\u2212Trophic Prey \u2212P r e d a t o r Dynamics in a Hare and Lynx E c o l o g i c a l System ", "page_idx": 24}, {"type": "text", "text": "Here you must model the s t a t e d i f f e r e n t i a l of hare_population , and l y n x _ p o p u l a t i o n ; with the a d d i t i o n a l i n p u t of t i m e _ i n _ y e a r s . This aims to s i m u l a t e the p o p u l a t i o n dynamics within a s i m p l i f i e d di \u2212t r o p h i c e c o l o g i c a l system comprising prey ( hares ) , and p r e d a t o r s ( lynxes ) . The i n t e r a c t i o n s i n c l u d e d i r e c t p r e d a t i o n and c o m p e t i t i o n f o r resources , m i r r o r i n g n a t u r a l predator \u2212prey mechanisms . ", "page_idx": 24}, {"type": "text", "text": "D e s c r i p t i o n of the v a r i a b l e s : \\* h a r e _ p o p u l a t i o n : Annual count of hare p e l t s , s e r v i n g as a proxy f o r the hare p o p u l a t i o n size , in t e n s of thousands . \\* l y n x _ p o p u l a t i o n : Annual count of lynx p e l t s , s e r v i n g as a proxy f o r the lynx p o p u l a t i o n size , in t e n s of thousands . ", "page_idx": 24}, {"type": "text", "text": "The model should c a p t u r e the dynamics of t h e s e populations , r e f l e c t i n g the di \u2212t r o p h i c prey \u2212p r e d a t o r i n t e r a c t i o n s , and p r e d i c t the p o p u l a t i o n s i z e s based on h i s t o r i c a l data . The data e x h i b i t s 10\u2212year long c h a r a c t e r i s t i c o s c i l l a t i o n s due to prey \u2212p r e d a t o r dynamics .   \nA d d i t i o n a l l y t h e s e v a r i a b l e s have the ranges of :   \n\\* h a r e _ p o p u l a t i o n : [ 1 . 8 0 , 152.65]   \n\\* l y n x _ p o p u l a t i o n : [ 3 . 1 9 , 79.35]   \n\\* t i m e _ i n _ y e a r s : [1845 , 1935] ", "page_idx": 24}, {"type": "text", "text": "The t r a i n i n g d a t a s e t c o n s i s t s of 63 time steps , v a l i d a t i o n and t r a i n i n g d a t a s e t c o n s i s t s of 14 time s t e p s each ", "page_idx": 24}, {"type": "text", "text": "Modelling goals : \u2018 \u2018 \u2018 \\* The parameters of the model w i l l be optimized to an observed t r a i n i n g d a t a s e t with the given s i m u l a t o r . \\* The observed t r a i n i n g d a t a s e t has very few samples , and the model must be able to g e n e r a l i z e to unseen data . ", "page_idx": 24}, {"type": "text", "text": "Requirement S p e c i f i c a t i o n : \u2018 \u2018 \u2018 \\* The code g e n e r a t e d should achieve the lowest p o s s i b l e v a l i d a t i o n loss , of 1e\u22126 or l e s s . \\* The code g e n e r a t e d should be i n t e r p r e t a b l e , and f i t the d a t a s e t as a c c u r a t e l y as p o s s i b l e ", "page_idx": 24}, {"type": "text", "text": "Skeleton code to f i l l in : \u2018 \u2018   \nc l a s s S t a t e D i f f e r e n t i a l ( nn . Module ) : def _ _ i n i t _ _ ( s e l f ) : super ( S t a t e D i f f e r e n t i a l , s e l f ) . _ _ i n i t _ _ ( ) # TODO: F i l l in the code here ", "page_idx": 24}, {"type": "text", "text": "def forward ( s e l f , h a r e _ p o p u l a t i o n : t o r c h . Tensor , l y n x _ p o p u l a t i o n $:$ t o r c h . Tensor , t i m e _ i n _ y e a r s : t o r c h . Tensor ) $->$ Tuple [ t o r c h Tensor , t o r c h . Tensor ] : # TODO: F i l l in the code here r e t u r n ( d_hare_population__dt , d_lynx_population__dt ) ", "page_idx": 24}, {"type": "text", "text": "Useful to know : \u2018 \u2018 \u2018   \n\\* You are a code evolving machine , and you w i l l be c a l l e d 20 times to g e n e r a t e code , and improve the code to achieve the lowest p o s s i b l e v a l i d a t i o n l o s s .   \n\\* The model d e f i n e s the s t a t e d i f f e r e n t i a l and w i l l be used with an ODE s o l v e r to f i t the observed t r a i n i n g d a t a s e t .   \n\\* You can use any parameters you want and any black box n e u r a l network components ( multi \u2212l a y e r p e r c e p t r o n s ) ; however , you have to d e f i n e t h e s e .   \n\\* I t i s p r e f e r a b l e to decompose the system i n t o d i f f e r e n t i a l e q u a t i o n s ( compartments ) i f p o s s i b l e .   \n\\* You can use any unary f u n c t i o n s , f o r example log , exp , power e t c .   \n$*$ Under no circumstance can you change the s k e l e t o n code f u n c t i o n d e f i n i t i o n s , only f i l l in the code .   \n\\* The i n p u t t e n s o r s are v e c t o r s of shape ( b a t c h _ s i z e ) .   \n\\* Use i n i t i a l l y white box models f i r s t and then switch to hybrid white and black box models f o r the r e s i d u a l s , only a f t e r no f u r t h e r b e s t program i t e r a t i o n improvement with white box models .   \n\\* Make sure your code follows the exact code s k e l e t o n s p e c i f i c a t i o n .   \n\\* Use PyTorch . ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "Plankton Microcosm ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "You w i l l get a system d e s c r i p t i o n to code a d i f f e r e n t i a l eq u at i on s i m u l a t o r f o r .   \nSystem D e s c r i p t i o n : \u2018 \u2018 \u2018   \n\" Modeling A r t i f i c i a l Tri \u2212Trophic Prey \u2212P r e d a t o r O s c i l l a t i o n s in a S i m p l i f i e d E c o l o g i c a l System   \nHere you must model the s t a t e d i f f e r e n t i a l of al g ae _ po p ul a ti o n , f l a g e l l a t e _ p o p u l a t i o n , and r o t i f e r _ p o p u l a t i o n ; with no i n p u t a c t i o n s . This aims to s i m u l a t e the p o p u l a t i o n dynamics within a s i m p l i f i e d t r i \u2212t r o p h i c e c o l o g i c a l system comprising prey ( algae ) , i n t e r m e d i a t e p r e d a t o r s ( f l a g e l l a t e s ) , and top p r e d a t o r s ( r o t i f e r s ) . The i n t e r a c t i o n s i n c l u d e d i r e c t p r e d a t i o n and c o m p e t i t i o n f o r resources , m i r r o r i n g n a t u r a l i n t r a g u i l d p r e d a t i o n mechanisms .   \nD e s c r i p t i o n of the v a r i a b l e s :   \n\\* p r e y _ p o p u l a t i o n : Total count of algae , s e r v i n g as the primary prey   \n\\* i n t e r m e d i a t e _ p o p u l a t i o n : Total count of f l a g e l l a t e s , a c t i n g as i n t e r m e d i a t e p r e d a t o r s and prey   \n\\* t o p _ p r e d a t o r s _ p o p u l a t i o n : Total count of r o t i f e r s , r e p r e s e n t i n g top p r e d a t o r s   \nThe d a t a s e t e n c a p s u l a t e s d a i l y p o p u l a t i o n counts a c r o s s m u l t i p l e s i m u l a t e d ecosystems over a period of 100 days , allowing f o r the a n a l y s i s of temporal o s c i l l a t i o n s and phase l a g s between s p e c i e s .   \nA d d i t i o n a l l y t h e s e v a r i a b l e s have the ranges of :   \n\\* p r e y _ p o p u l a t i o n : [0.095898 , 2.469735]   \n\\* i n t e r m e d i a t e _ p o p u l a t i o n : [0.008438 , 1.500000]   \n\\* t o p _ p r e d a t o r s _ p o p u l a t i o n : [0.030316 , 0.739244] ", "page_idx": 25}, {"type": "text", "text": "The t r a i n i n g d a t a s e t c o n s i s t s of 70 time steps , v a l i d a t i o n and t r a i n i n g d a t a s e t c o n s i s t s of 15 time s t e p s each ", "page_idx": 25}, {"type": "text", "text": "Modelling goals : \u2018 \u2018 \u2018 \\* The parameters of the model w i l l be optimized to an observed t r a i n i n g d a t a s e t with the given s i m u l a t o r . \\* The observed t r a i n i n g d a t a s e t has very few samples , and the model must be able to g e n e r a l i z e to unseen data ", "page_idx": 25}, {"type": "text", "text": "Requirement S p e c i f i c a t i o n : \u2018 \u2018 \u2018 \\* The code g e n e r a t e d should achieve the lowest p o s s i b l e v a l i d a t i o n loss , of 1e\u22126 or l e s s . \\* The code g e n e r a t e d should be i n t e r p r e t a b l e , and f i t the d a t a s e t as a c c u r a t e l y as p o s s i b l e ", "page_idx": 25}, {"type": "text", "text": "Skeleton code to f i l l in : \u2018   \nc l a s s S t a t e D i f f e r e n t i a l ( nn . Module ) : def _ _ i n i t _ _ ( s e l f ) : super ( S t a t e D i f f e r e n t i a l , s e l f ) . _ _ i n i t _ _ ( ) # TODO: F i l l in the code here ", "page_idx": 25}, {"type": "text", "text": "def forward ( s e l f , p r e y _ p o p u l a t i o n : t o r c h . Tensor , i n t e r m e d i a t e _ p o p u l a t i o n $\\boldsymbol{:}$ t o r c h . Tensor , t o p _ p r e d a t o r s _ p o p u l a t i o n : t o r c h . Tensor ) \u2212> Tuple [ t o r c h . Tensor , t o r c h . Tensor , t o r c h . Tensor ] : # TODO: F i l l in the code here r e t u r n ( d_prey_population__dt , d _ i n t e r m e d i a t e _ p o p u l a t i o n _ _ d t , d _ t o p _ p r e d a t o r s _ p o p u l a t i o n _ _ d t ) ", "page_idx": 25}, {"type": "text", "text": "Useful to know : \u2018 \u2018 \u2018   \n$*$ You are a code evolving machine , and you w i l l be c a l l e d 20 times to g e n e r a t e code , and improve the code to achieve the lowest p o s s i b l e v a l i d a t i o n l o s s .   \n\\* The model d e f i n e s the s t a t e d i f f e r e n t i a l and w i l l be used with an ODE s o l v e r to f i t the observed t r a i n i n g d a t a s e t .   \n\\* You can use any parameters you want and any black box n e u r a l network components ( multi \u2212l a y e r p e r c e p t r o n s ) ; however , you have to d e f i n e t h e s e .   \n\\* I t i s p r e f e r a b l e to decompose the system i n t o d i f f e r e n t i a l e q u a t i o n s ( compartments ) i f p o s s i b l e .   \n\\* You can use any unary f u n c t i o n s , f o r example log , exp , power e t c .   \n\\* Under no circumstance can you change the s k e l e t o n code f u n c t i o n d e f i n i t i o n s , only f i l l in the code .   \n\\* The i n p u t t e n s o r s are v e c t o r s of shape ( b a t c h _ s i z e ) .   \n\\* Use i n i t i a l l y white box models f i r s t and then switch to hybrid white and black box models f o r the r e s i d u a l s , only a f t e r no f u r t h e r b e s t program i t e r a t i o n improvement with white box models .   \n\\* Make sure your code follows the exact code s k e l e t o n s p e c i f i c a t i o n .   \n\\* Use PyTorch . ", "page_idx": 25}, {"type": "text", "text": "Think step \u2212by\u2212step , and then give the complete f u l l working code . You are g e n e r a t i n g code f o r i t e r a t i o n 0 out of 20. ", "page_idx": 25}, {"type": "text", "text": "COVID-19 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "You w i l l get a system d e s c r i p t i o n to code a d i f f e r e n t i a l eq u at i on s i m u l a t o r f o r ", "page_idx": 25}, {"type": "text", "text": "P r e d i c t i o n model of COVID\u221219 Epidemic Dynamics ", "page_idx": 25}, {"type": "text", "text": "Here you must model the s t a t e d i f f e r e n t i a l of s u s c e p t i b l e , exposed , i n f e c t e d and recovered ; with the i n p u t a c t i o n of a c o n s t a n t t o t a l _ p o p u l a t i o n . There are no i n t e r v e n t i o n s a p p l i e d . Here the s t a t e s are normalized r a t i o s of the t o t a l f i x e d p o p u l a t i o n ", "page_idx": 25}, {"type": "text", "text": "D e s c r i p t i o n of the v a r i a b l e s : \\* s u s c e p t i b l e : Ratio of the p o p u l a t i o n t h a t i s s u s c e p t i b l e to the v i r u s .   \n\\* exposed : Ratio of the p o p u l a t i o n t h a t i s exposed to the virus , not yet i n f e c t i o u s .   \n\\* i n f e c t e d : Ratio of the p o p u l a t i o n t h a t i s a c t i v e l y c a r r y i n g and t r a n s m i t t i n g the v i r u s .   \n\\* recovered : Ratio of the p o p u l a t i o n t h a t have recovered from the virus , i n c l u d i n g those who are deceased .   \n\\* t o t a l _ p o p u l a t i o n : Total p o p u l a t i o n of the country , a c o n s t a n t .   \nThe time u n i t s i s in days . ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "A d d i t i o n a l l y t h e s e v a r i a b l e s have the ranges of : \\* s u s c e p t i b l e : [0 , 1] ", "page_idx": 25}, {"type": "text", "text": "\\* exposed : [0 , 1]   \n\\* i n f e c t e d : [0 , 1]   \n\\* recovered : [0 , 1]   \n\\* t o t a l _ p o p u l a t i o n : [10000 , 10000]   \nThe t r a i n i n g d a t a s e t c o n s i s t s of 24 c o u n t r i e s , where each country i s observed f o r 60 days .   \nModelling goals : \u2018 \u2018 \u2018   \n\\* The parameters of the model w i l l be optimized to an observed t r a i n i n g d a t a s e t with the given s i m u l a t o r .   \n\\* The observed t r a i n i n g d a t a s e t has very few samples , and the model must be able to g e n e r a l i z e to unseen data .   \n\u2018 \u2018 \u2018   \nRequirement S p e c i f i c a t i o n : \u2018 \u2018 \u2018   \n\\* The code g e n e r a t e d should achieve the lowest p o s s i b l e v a l i d a t i o n loss , of 1e \u221210 or l e s s .   \n\\* The code g e n e r a t e d should be i n t e r p r e t a b l e , and f i t the d a t a s e t as a c c u r a t e l y as p o s s i b l e .   \nSkeleton code to f i l l in : \u2018 \u2018 \u2018   \nc l a s s S t a t e D i f f e r e n t i a l ( nn . Module ) : def _ _ i n i t _ _ ( s e l f ) : super ( S t a t e D i f f e r e n t i a l , s e l f ) . _ _ i n i t _ _ ( ) # TODO: F i l l in the code here def forward ( s e l f , s u s c e p t i b l e : t o r c h . Tensor , exposed : t o r c h . Tensor , i n f e c t e d : t o r c h . Tensor , recovered : t o r c h . Tensor , t o t a l _ p o p u l a t i o n : t o r c h . Tensor ) $->$ Tuple [ t o r c h . Tensor , t o r c h . Tensor , t o r c h . Tensor , t o r c h . Tensor ] : # TODO: F i l l in the code here r e t u r n ( d _ s u s c e p t i b l e _ _ d t , d_exposed__dt , d_in fected__dt , d_recovered__dt )   \nUseful to know : \u2018 \u2018 \u2018   \n$*$ You are a code evolving machine , and you w i l l be c a l l e d 20 times to g e n e r a t e code , and improve the code to achieve the lowest p o s s i b l e v a l i d a t i o n l o s s .   \n\\* The model d e f i n e s the s t a t e d i f f e r e n t i a l and w i l l be used with an ODE s o l v e r to f i t the observed t r a i n i n g d a t a s e t .   \n\\* You can use any parameters you want and any black box n e u r a l network components ( multi \u2212l a y e r p e r c e p t r o n s ) ; however , you have to d e f i n e t h e s e .   \n\\* I t i s p r e f e r a b l e to decompose the system i n t o d i f f e r e n t i a l e q u a t i o n s ( compartments ) i f p o s s i b l e .   \n\\* You can use any unary f u n c t i o n s , f o r example log , exp , power e t c .   \n\\* Under no circumstance can you change the s k e l e t o n code f u n c t i o n d e f i n i t i o n s , only f i l l in the code .   \n\\* The i n p u t t e n s o r s are v e c t o r s of shape ( b a t c h _ s i z e ) .   \n\\* Use i n i t i a l l y white box models f i r s t and then switch to hybrid white and black box models f o r the r e s i d u a l s , only a f t e r no f u r t h e r b e s t program i t e r a t i o n improvement with white box models .   \n$*$ Make sure your code follows the exact code s k e l e t o n s p e c i f i c a t i o n .   \n\\* Use PyTorch .   \nThink step \u2212by\u2212step , and then give the complete f u l l working code . You are g e n e r a t i n g code f o r i t e r a t i o n 0 out of 20. ", "page_idx": 26}, {"type": "text", "text": "F Model Optimization Losses ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We consider the optimization loss of mean squared error on a dataset $\\mathcal{D}$ and also consider a higher fidelity mean squared error loss per component. ", "page_idx": 26}, {"type": "text", "text": "MSE Loss. Specifically, we optimize the following mean squared error objective, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\theta,\\mathcal{D})=\\frac{1}{N\\times T}\\sum_{n=1}^{N}\\sum_{i=0}^{T^{n}}||f_{\\theta,\\omega(\\theta)}(x^{(n)}(t_{i}),u^{(n)}(t_{i}),t_{i})\\Delta t-y^{(n)}(t_{i})||^{2}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $N\\times T$ is the total number of state-action pairs in the dataset. For a given model find the parameters $\\theta^{*}$ that minimize this loss, i.e. $\\theta^{*}\\,=\\,\\arg\\operatorname*{min}_{\\theta}$ $\\mathcal{L}(\\theta,\\mathcal{D}_{\\mathrm{train}})$ . Here we optimize $\\theta$ by stochastic gradient descent, using the Adam optimizer [32], however, we note other optimization algorithms could also be used such as black box optimizers. ", "page_idx": 26}, {"type": "text", "text": "MSE Loss per component. We seek to collect detailed quantitative statistics on how well the generated trained system model performs. Therefore, we collect the validation loss per component. Here, we use $(j)$ to indicate the predictions for the $j^{t h}$ component. ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\omega_{m}(\\theta^{*},\\mathcal{D}_{\\mathrm{val}})=\\frac{1}{N_{\\mathrm{val}}\\times T}\\sum_{n=1}^{N_{\\mathrm{val}}}\\sum_{i=0}^{T_{\\mathrm{val}}^{n}}\\left(f_{\\theta,\\omega(\\theta)}^{(j)}(x^{(n)}(t_{i}),u^{(n)}(t_{i}),t_{i})\\Delta t-y_{j}^{(n)}(t_{i})\\right)^{2}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and collect these scalar validation losses per component into a vector $\\omega=[\\omega_{1},\\omega_{2},\\ldots,\\omega_{m}]$ , and compute its mean as m1  jm=1 \u03c9j(\u03b8\u2217, Dval) i.e. the validation loss. ", "page_idx": 26}, {"type": "text", "text": "G Evaluation Metrics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We employ mean squared error (MSE) to evaluate the benchmark methods on a held-out test dataset of state-action trajectories, denoted as $\\mathcal{D}_{\\mathrm{test}}$ , using the loss defined in Equation (5) and report this as $\\mathcal{T}_{M S E}$ . Each metric is averaged over ten runs with different random seeds, and we present these averages along with their $95\\%$ mean confidence intervals7. For each random seed run, we generate a new train, validation and test dataset independently, when we have access to a simulator. Additionally, when sampling a dataset from a simulator, we collect datasets of the same number of trajectories for the validation and test set as was used to generate the training set. We then train each baseline on the training dataset and use the validation dataset for early stopping when the method supports this. We then evaluate the performance of each baseline on the test dataset. We repeat this process for each random seed run. We perform all experiments and training using a single Intel Core i9-12900K CPU $@$ 3.20GHz, 64GB RAM with an Nvidia RTX3090 GPU 24GB. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "H Additional Results ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "H.1 Out-of-distribution Experiment and Setup ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "To explore the out-of-distribution shifts, we adapt the Lung Cancer (with Chemo. & Radio.) simulator to have a training dataset of tumor volumes in a range that is outside of the tumor volume range within the test set over all trajectories. To do this we sampled a training dataset, with starting state tumor volumes sampled from $x\\sim\\mathcal{U}(0,574)$ and then collected 1, 000 trajectories for 60 time steps. We then sampled a test dataset out of distribution from that seen in training, with a starting initial state of $x\\sim\\mathcal{U}(804,1149)$ . To ensure that the test state-action states are completely out of the training distribution we slowed down the time interval of the simulator to that of per hour, rather than day, i.e., simulating at $\\begin{array}{r}{\\Delta t=\\frac{1}{24}}\\end{array}$ rather than $\\Delta t=1$ day resolution. We then also verified that the range of cancer volumes seen within the test set do not overlap at all with those in the training set, especially throughout and at the end of the trajectory. ", "page_idx": 27}, {"type": "text", "text": "H.2 COVID-19 Unobserved Intervention Experiment and Setup ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "To assess whether HDTwinGen can understand and modify its HDTwin, we setup an experiment to see if the modeling agent can adapt an already optimized and best-discovered HDTwin for an unobserved intervention, that is a change of the true system state function dynamics, that are latent. ", "page_idx": 27}, {"type": "text", "text": "We constructed a scenario with our COVID-19 simulator, to have a realistic intervention of a lockdown policy, which reduces the physical distance of individuals to one another. In COVASIM we implemented this, where this happens at day 19, dramatically altering how the COVASIM simulator behaves after the intervention is applied. This is known to reduce the effective contact rate $\\beta$ parameter in an SEIR model [83], for the intervention this approximately corresponds to the $\\beta$ parameter reducing its value by $75\\%$ after the lockdown intervention is applied. Importantly we now sampled a training dataset, and validation dataset from the simulator for before the intervention was applied, and only sampled a test dataset after the intervention was applied. ", "page_idx": 27}, {"type": "text", "text": "We took the best-found trained HDTwin model for COVID-19, itself created a HDTwin model that is an SEIR model with black-box residual components, as seen in Appendix I. The agent then was specifically instructed to attempt to adapt this HDTwin model to this unobserved intervention, that was purely described in words. The LLM was able to reason about it, and similarly decreased the HDTwin models internal $\\beta$ parameter by approximately $70\\%$ , leading to un-observed modelling of such an intervention, when rolling out from the current state, from day 19. This shows the utility of such a hybrid DT framework. Interestingly, the competing methods, such as the black-box method, DyNODE and SINDy incorrectly continue the expected trajectory as they are unaware that the underlying system has been intervened on, and its behavior is now different from what they have learnt to optimize to. ", "page_idx": 27}, {"type": "text", "text": "H.3 HDTwinGen top-1 decreases over time ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We observe in Figure 6, that averaged over 10 random seed runs, the top-K, specifically the top-K HDTwin found at each iteration step decreases, implying on average the HDTwin improves in the iteration loop in the beginning stages. ", "page_idx": 27}, {"type": "image", "img_path": "SOsiObSdU2/tmp/6dc956c13d0393d6f97f6814b41168dc8e5e020126f73c6cff432882d347143c.jpg", "img_caption": ["Figure 6: Average evolution over generations for HDTwinGen\u2014for the Lung Cancer (with Chemo. & Radio.) "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "H.4 HDTwinGen Evolution ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Here we list out the specific HDTwins that have been generated at generation steps and are labeled in Figure 3. ", "page_idx": 28}, {"type": "text", "text": "Iteration 1. Val Loss: 4.414876937866211 Description: White box only. ", "page_idx": 28}, {"type": "text", "text": "Val Loss : 4.41 ( Where the val l o s s per dimension i s tumor_volume val l o s s : 2.77 , chemotherapy_drug_concentration val l o s s : 6 . 0 6 ) I t e r a t i o n : 1   \n###   \n\u2018 \u2018 \u2018   \nimport t o r c h   \nimport t o r c h . nn as nn   \nfrom typing import Tuple   \nc l a s s S t a t e D i f f e r e n t i a l ( nn . Module ) : def _ _ i n i t _ _ ( s e l f ) : super ( S t a t e D i f f e r e n t i a l , s e l f ) . _ _ i n i t _ _ ( ) # Parameters f o r the tumor growth model s e l f . alpha $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 1 ) ) # Tumor growth r a t e s e l f . beta $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 0 5 ) ) # Tumor death r a t e due to chemotherapy s e l f . gamma $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 0 3 ) ) # Tumor death r a t e due to r a d i o t h e r a p y def forward ( s e l f , tumor_volume : t o r c h . Tensor , chemotherapy_drug_concentration : t o r c h . Tensor , chemotherapy_dosage : t o r c h . Tensor , r a d i o t h e r a p y _ d o s a g e : t o r c h . Tensor ) $->$ Tuple [ t o r c h . Tensor , t o r c h . Tensor ] : # Tumor volume change r a t e d_tumor_volume__dt $=$ s e l f . alpha $*$ tumor_volume \u2212 s e l f . beta $^*$ chemotherapy_drug_concentration $^*$ tumor_volume \u2212 s e l f . gamma \\* r a d i o t h e r a p y _ d o s a g e $^\\ast$ tumor_volume # Chemotherapy drug c o n c e n t r a t i o n change r a t e d_chemotherapy_drug_concentration__dt $=$ chemotherapy_dosage \u2212 chemotherapy_drug_concentration # Assuming a simple model where the drug i s a d m i n i s t e r e d and then decays r e t u r n ( d_tumor_volume__dt , d_chemotherapy_drug_concentration__dt )   \n\u2018 \u2018 \u2018   \noptimized_parameters $=$ { \u2019 alpha \u2019 : 0.04550161585211754 , \u2019 beta \u2019 : 0.02731170691549778 , \u2019gamma \u2019 : 0.0489218533039093}   \n### ", "page_idx": 28}, {"type": "text", "text": "Iteration 2. Val Loss: 4.233582019805908 Description: White box model with logistic growth for tumor volume and decay rate for chemotherapy drug concentration. ", "page_idx": 28}, {"type": "text", "text": "Val Loss : 4.23 ( Where the val l o s s per dimension i s tumor_volume val l o s s : 8.47 , chemotherapy_drug_concentration val l o s s : 8.95 e \u221214) I t e r a t i o n : 2 ", "page_idx": 28}, {"type": "text", "text": "###   \n\u2018 \u2018 \u2018   \nimport t o r c h   \nimport t o r c h . nn as nn   \nfrom typing import Tuple   \nc l a s s S t a t e D i f f e r e n t i a l ( nn . Module ) : def $\\_\\mathrm{i}\\,\\mathrm{n}\\,\\mathrm{i}\\,\\mathrm{t}_{--}$ ( s e l f ) : super ( S t a t e D i f f e r e n t i a l , s e l f ) . _ _ i n i t _ _ ( ) # Parameters f o r the tumor growth model s e l f . alpha $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 1 ) ) # Tumor growth r a t e s e l f . beta $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 0 5 ) ) # Tumor death r a t e due to chemotherapy s e l f . gamma $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 0 3 ) ) # Tumor death r a t e due to r a d i o t h e r a p y s e l f . kappa = nn . Parameter ( t o r c h . t e n s o r ( 1 0 0 0 . 0 ) ) # Carrying c a p a c i t y of the environment s e l f . d e l t a $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 0 1 ) ) # Decay r a t e of the chemotherapy drug def forward ( s e l f , tumor_volume : t o r c h . Tensor , chemotherapy_drug_concentration : t o r c h . Tensor , chemotherapy_dosage : t o r c h . Tensor , r a d i o t h e r a p y _ d o s a g e : t o r c h . Tensor ) $->$ Tuple [ t o r c h . Tensor , t o r c h . Tensor ] : # L o g i s t i c growth model f o r tumor volume d_tumor_volume $\\_\\mathrm{dt}\\ =$ s e l f . alpha $*$ tumor_volume $^*$ (1 \u2212 tumor_volume / s e l f . kappa ) \u2212 s e l f . beta $^*$ chemotherapy_drug_concentration $^*$ tumor_volume \u2212 s e l f . gamma $^\\ast$ r a d i o t h e r a p y _ d o s a g e $^*$ tumor_volume # Chemotherapy drug c o n c e n t r a t i o n change r a t e with decay d_chemotherapy_drug_concentration__dt $=$ chemotherapy_dosage \u2212 s e l f . d e l t a \\* chemotherapy_drug_concentration r e t u r n ( d_tumor_volume__dt , d_chemotherapy_drug_concentration__dt )   \n\u2018 \u2018 \u2018   \noptimized_parameters $=$ { \u2019 alpha \u2019 : 0.05583872273564339 , \u2019 beta \u2019 : 0.026076778769493103 , \u2019gamma \u2019 : 0.037345148622989655 , \u2019 kappa \u2019 : 1015.9735107421875 , \u2019 de l t a \u2019 : 0.5000000596046448}   \n### ", "page_idx": 28}, {"type": "text", "text": "Iteration 3. Val Loss: 2.902040958404541 Description: White box model with logistic growth for tumor volume, interaction term for combined therapy effect, and bi-exponential decay for chemotherapy drug concentration. ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Val Loss : 2.9 ( Where the val l o s s per dimension i s tumor_volume val l o s s : 5.36 , chemotherapy_drug_concentration val l o s s : 0.448) I t e r a t i o n : 3   \n###   \n\u2018 \u2018 \u2018   \nimport t o r c h   \nimport t o r c h . nn as nn   \nfrom typing import Tuple   \nc l a s s S t a t e D i f f e r e n t i a l ( nn . Module ) : def _ _ i n i t _ _ ( s e l f ) : super ( S t a t e D i f f e r e n t i a l , s e l f ) . _ _ i n i t _ _ ( ) # Parameters f o r the tumor growth model s e l f . alpha $=$ nn . Parameter ( t o r c h . t e n s o r (0.05583872273564339) ) # Tumor growth r a t e s e l f . beta $=$ nn . Parameter ( t o r c h . t e n s o r (0.026076778769493103) ) # Tumor death r a t e due to chemotherapy s e l f . gamma $=$ nn . Parameter ( t o r c h . t e n s o r (0.037345148622989655) ) # Tumor death r a t e due to r a d i o t h e r a p y s e l f . kappa $=$ nn . Parameter ( t o r c h . t e n s o r (1015.9735107421875) ) # Carrying c a p a c i t y of the environment s e l f . d e l t a $=$ nn . Parameter ( t o r c h . t e n s o r (0.5000000596046448) ) # Decay r a t e of the chemotherapy drug s e l f . e t a $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 0 1 ) ) # I n t e r a c t i o n term between chemotherapy and r a d i o t h e r a p y def forward ( s e l f , tumor_volume : t o r c h . Tensor , chemotherapy_drug_concentration : t o r c h . Tensor , chemotherapy_dosage : t o r c h . Tensor , r a d i o t h e r a p y _ d o s a g e : t o r c h . Tensor ) $->$ Tuple [ t o r c h . Tensor , t o r c h . Tensor ] : # L o g i s t i c growth model f o r tumor volume with i n t e r a c t i o n term d_tumor_volume__dt $=$ s e l f . alpha \\* tumor_volume \\* $\\mathrm{~\\textit~{~1~}~}-$ tumor_volume / s e l f . kappa ) \u2212 s e l f . beta \\* chemotherapy_drug_concentration \\* tumor_volume \u2212 s e l f . gamma \\* r a d i o t h e r a p y _ d o s a g e $^*$ tumor_volume \u2212 s e l f . e t a \\* chemotherapy_drug_concentration \\* r a d i o t h e r a p y _ d o s a g e \\* tumor_volume # Bi\u2212e x p o n e n t i a l decay model f o r chemotherapy drug c o n c e n t r a t i o n d_chemotherapy_drug_concentration__dt $=$ chemotherapy_dosage \u2212 s e l f . d e l t a $^*$ chemotherapy_drug_concentration \\*\\*2 r e t u r n ( d_tumor_volume__dt , d_chemotherapy_drug_concentration__dt )   \n\u2018 \u2018 \u2018   \noptimized_parameters $=$ { \u2019 alpha \u2019 : 0.0490952804684639 , \u2019 beta \u2019 : 0.023840805515646935 , \u2019gamma \u2019 : 0.03170385956764221 , \u2019 kappa \u2019 : 1032.3638916015625 , \u2019 de l t a \u2019 : 0.06566499918699265 , \u2019 eta \u2019 : 0.002417867537587881}   \n### ", "page_idx": 29}, {"type": "text", "text": "Iteration 4: Val loss: 1.2533262968063354 Description: White and black box model with logistic growth for tumor volume, Michaelis-Menten saturation for chemotherapy effect, sigmoid function for radiotherapy effect, resistance development, and a black box MLP for residuals. ", "text_level": 1, "page_idx": 29}, {"type": "image", "img_path": "SOsiObSdU2/tmp/ed1220e088bf701b2d37052033a516eadcb4e3d896f467240092642139ed29df.jpg", "img_caption": [], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "Iteration 5. Val loss: 0.1756206750869751 Description: White and black box model with dynamic carrying capacity and decay rate, enhanced interaction term, and an expanded black box MLP for residuals. ", "text_level": 1, "page_idx": 30}, {"type": "image", "img_path": "SOsiObSdU2/tmp/783313daf2537eeb731619a8c36395170b64232e26a16871c684a1ae2171e576.jpg", "img_caption": [], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "Iteration 12. Val loss: 0.07228073477745056 Description: White and black box model with logistic growth for tumor volume, Michaelis-Menten saturation for chemotherapy effect, sigmoid function for radiotherapy effect, resistance development, and a simplified black box MLP for residuals. ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Val Loss : 0.0723 ( Where the val l o s s per dimension i s tumor_volume val l o s s : 0.133 , chemotherapy_drug_concentration val l o s s : 0.012) I t e r a t i o n : 11 ", "page_idx": 30}, {"type": "text", "text": "###   \n\u2018 \u2018 \u2018   \nc l a s s S t a t e D i f f e r e n t i a l ( nn . Module ) : def _ _ i n i t _ _ ( s e l f ) : super ( S t a t e D i f f e r e n t i a l , s e l f ) . _ _ i n i t _ _ ( ) # Parameters f o r the tumor growth model s e l f . alpha $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 0 3 ) ) # Tumor growth r a t e s e l f . beta $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 4 ) ) # Tumor death r a t e due to chemotherapy s e l f . gamma $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 0 8 ) ) # Tumor death r a t e due to r a d i o t h e r a p y s e l f . kappa_base $=$ nn . Parameter ( t o r c h . t e n s o r ( 1 0 3 0 . 0 ) ) # Base c a r r y i n g c a p a c i t y of the environment s e l f . kappa_mod $=$ nn . Parameter ( t o r c h . t e n s o r ( \u22122.0) ) # Modifier f o r c a r r y i n g c a p a c i t y based on t r e a t m e n t s e l f . d e l t a _ b a s e $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 1 ) ) # Base decay r a t e of the chemotherapy drug s e l f . delta_mod $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 0 0 0 3 ) ) # Modifier f o r decay r a t e based on tumor volume s e l f . e t a $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 0 0 4 ) ) # I n t e r a c t i o n term between chemotherapy and r a d i o t h e r a p y s e l f . t h e t a $=$ nn . Parameter ( t o r c h . t e n s o r ( 1 0 . 0 ) ) # Michaelis \u2212Menten s a t u r a t i o n c o n s t a n t f o r chemotherapy s e l f . rho $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 3 ) ) # Sigmoid s t e e p n e s s f o r r a d i o t h e r a p y e f f e c t s e l f . z e t a $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 1 5 ) ) # R e s i s t a n c e development r a t e # Black box component f o r c a p t u r i n g r e s i d u a l s s e l f . r e s i d u a l _ m l p $=$ nn . S e q u e n t i a l ( nn . Linear (4 , 16) , nn . LeakyReLU ( 0 . 1 ) , nn . Linear (16 , 8) , nn . LeakyReLU ( 0 . 1 ) , nn . Linear (8 , 2) ", "page_idx": 30}, {"type": "text", "text": "def forward ( s e l f , tumor_volume : t o r c h . Tensor , chemotherapy_drug_concentration : t o r c h . Tensor , chemotherapy_dosage : t o r c h . Tensor , r a d i o t h e r a p y _ d o s a g e : t o r c h . Tensor ) $->$ Tuple [ t o r c h . Tensor , t o r c h . Tensor ] : # Adjusted c a r r y i n g c a p a c i t y based on t r e a t m e n t kappa $=$ s e l f . kappa_base $^+$ s e l f . kappa_mod $^\\ast$ ( chemotherapy_dosage $^+$ r a d i o t h e r a p y _ d o s a g e ) # Adjusted decay r a t e based on tumor volume d e l t a $=$ s e l f . d e l t a _ b a s e $^+$ s e l f . delta_mod $^\\ast$ tumor_volume # L o g i s t i c growth model f o r tumor volume with i n t e r a c t i o n term and r e s i s t a n c e r e s i s t a n c e $\\mathit{\\mathrm{~\\ensuremath~{~\\mu~}~}}=\\mathit{\\mathrm{~\\ensuremath~{~1~}~}}+$ s e l f . z e t a $^*$ tumor_volume d_tumor_volume__dt $=$ s e l f . alpha $*$ tumor_volume $^*$ (1 \u2212 tumor_volume / kappa ) \u2212 ( s e l f . beta $^*$ chemotherapy_drug_concentration tumor_volume ) / ( s e l f . t h e t a $^+$ chemotherapy_drug_concentration ) \u2212 s e l f . gamma $*$ r a d i o t h e r a p y _ d o s a g e $^\\ast$ tumor_volume / $1+$ t o r c h . exp ( \u2212s e l f . rho $^*$ ( r a d i o t h e r a p y _ d o s a g e \u2212 1) ) ) \u2212 s e l f . e t a \\* chemotherapy_drug_concentration \\* r a d i o t h e r a p y _ d o s a g e $^\\ast$ tumor_volume / r e s i s t a n c e # Non\u2212l i n e a r decay model f o r chemotherapy drug c o n c e n t r a t i o n d_chemotherapy_drug_concentration__dt $=$ chemotherapy_dosage \u2212 d e l t a $*$ t o r c h . pow ( chemotherapy_drug_concentration , 1 . 5 ) # Black box r e s i d u a l component r e s i d u a l s $=$ s e l f . r e s i d u a l _ m l p ( t o r c h . s t a c k ( ( tumor_volume , chemotherapy_drug_concentration , chemotherapy_dosage , r a d i o t h e r a p y _ d o s a g e ) , dim ) ) # Combine white box model with r e s i d u a l s d_tumor_volume__dt $+=$ r e s i d u a l s [ : , 0] d_chemotherapy_drug_concentration__dt $+=$ r e s i d u a l s [ : , 1] r e t u r n ( d_tumor_volume__dt , d_chemotherapy_drug_concentration__dt ) \u2018 \u2018 \u2018 optimized_parameters $=$ { \u2019 alpha \u2019 : 0.016903197392821312 , \u2019 beta \u2019 : 0.5582640171051025 , \u2019gamma \u2019 : 0.08495774865150452 , \u2019 kappa_base \u2019 : 1029.802490234375 , \u2019 kappa_mod \u2019 : \u22122.4545891284942627 , \u2019 delta_base \u2019 : 0.12832798063755035 , \u2019 delta_mod \u2019 : \u22123.1435782148037106 e \u221205 , \u2019 eta \u2019 : 0.006932476069778204 , \u2019 t h e t a \u2019 : 13.699580192565918 , \u2019 rho \u2019 : 0.24815633893013 , \u2019 zeta \u2019 : 0.13727830350399017} ### ", "page_idx": 31}, {"type": "text", "text": "H.5 HDTwinGen Ablation No Memory ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Ablation Study. We also ablate HDTwin by removing its memory, only keeping the last hybrid model it generated. We observe decreased performance as shown in Table 4. ", "page_idx": 31}, {"type": "table", "img_path": "SOsiObSdU2/tmp/be14288f9c1b78baee0d097246b15054e8da0c5a2cd520e25a341de5670393a6.jpg", "table_caption": ["Table 4: HDTwinGen Ablation "], "table_footnote": [], "page_idx": 31}, {"type": "text", "text": "H.6 Evaluating Different LLMs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "We performed a complete re-run of our main experiments under the same settings, now using a different LLM within our HDTwinGen framework, GPT-3.5. These new results are included in Table 5. HDTwinGen can operate fully with a less capable LLM model. However, the generated and discovered models\u2019 performance correlates to the underlying LLM model\u2019s performance as expected. ", "page_idx": 31}, {"type": "text", "text": "Table 5: Ablation of using different LLMs. Test MSE $(T_{M S E})$ averaged over ten random seeds. HDTwinGen is capable of using other LLM models, however, the best performance results are provided with better-performing LLMs (e.g. GPT-4). The results are presented with $\\pm$ indicating $\\bar{9}5\\%$ confidence intervals. ", "page_idx": 31}, {"type": "table", "img_path": "SOsiObSdU2/tmp/36143c74a36549c11d3de0f2c84420eadecd367fa43ec39f2d7c002181603a58.jpg", "table_caption": [], "table_footnote": [], "page_idx": 31}, {"type": "text", "text": "We also explored the effect of changing the LLMs underlying temperature hyperparameter, here using GPT-4, varying the temperature from 0, 0.7 to 2.0, where we used 0.7 throughout all our experiments (Appendix E). As outlined in Table 6, we observe that HDTwinGen is still able to operate with a different underlying temperature of the LLM. ", "page_idx": 31}, {"type": "text", "text": "H.7 Prompt Ablations with Varying Amounts of Prior Information ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "We conducted a complete re-run of our main experiments, ablating the prompt, which provides the prior information in the form of a textual prior to HDTwinGen. We provide the ablation results in Table 7. Specifically the prompt as outlined in Appendix $\\mathrm{E}$ is structured to include separately a {system description}, {skeleton code}, and {useful to know} components. Here, prior information is conveyed through the {system description}, which describes the system to be modeled, its features, and their ranges; minor system prior information is also conveyed through the {skeleton code}, as this includes task-specific feature names as input variables. The other components of the prompt (e.g. {useful to know}), do not include any task-specific information, and are there to provide general instructions to make the framework work, such as generate a pytorch model as code in the response. ", "page_idx": 31}, {"type": "table", "img_path": "SOsiObSdU2/tmp/511c07f52a6d01a4a8132b906b9d91159300937971b26e81418cde1ad3f5d9e6.jpg", "table_caption": ["Table 6: Ablation of varying the LLMs temperature. HDTwinGen can still operate with different LLM temperatures. Reporting the test prediction MSE $(T_{M S E})$ averaged over ten random seeds. "], "table_footnote": [], "page_idx": 32}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "We ablate these components of the prompt, first by removing the task-specific prior {system description} labeled HDTwinGen (Partial Context); second, by removing all taskspecific priors removing both {system description} and {skeleton code} (where we change the feature names to meaningless names such as x1, x2, etc.) labeled HDTwinGen (No Context); third, by removing only the {useful to know} information that helps the framework, such as instructions to decompose the system, and or combine white-box models with black-box models for the white-box model residuals, labeled HDTwinGen (No Instructions). We observe in the tabulated results (Table 7) that HDTwinGen can still operate without any task-specific prior information, however having textual prior aids in generating better-performing models, and partially removing HDTwinGen operation instructions, makes it generate slightly less good models. ", "page_idx": 32}, {"type": "text", "text": "Table 7: Prompt Ablations with Varying Amounts of Prior Information. Test MSE $\\mathcal{T}_{M S E}$ averaged over ten random seeds. HDTwinGen can still operate without any task-specific prior information, however having textual prior aids in generating better-performing models. The results are averaged over ten random seeds, with $\\pm$ indicating $95\\%$ confidence intervals. ", "page_idx": 32}, {"type": "table", "img_path": "SOsiObSdU2/tmp/e691eaddf69cec610ee32463e4c663c8d3cc23f575b1a76a5e113d750feb2cdd.jpg", "table_caption": [], "table_footnote": [], "page_idx": 32}, {"type": "text", "text": "H.8 Domain-Specific Baselines ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "We performed a complete re-run of our main datasets using domain-specific white-box baselines, as determined by a human expert, as shown in Table 8. Specifically, for COVID-19 modeling, we fit a SEIR model [84], a Lotka\u2013Volterra model for predator-prey population dynamics (Hare-Lynx & Plankton Microcosm datasets) [85], and a logistic tumor growth model with chemo. & radio. effects modeling. HDTwinGen still models the system most accurately, achieving the lowest test prediction MSE on the held-out test dataset of state-action trajectories. ", "page_idx": 32}, {"type": "text", "text": "SEIR Model for COVID-19 Modeling. The SEIR model is a compartmental model used in epidemiology to simulate how a disease spreads through a population. It divides the population into four compartments: susceptible (S), exposed (E), infectious (I), and recovered (R). The transitions between these compartments are governed by the following differential equations: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{d S}{d t}=-\\beta\\displaystyle\\frac{S I}{N},}\\\\ {\\displaystyle\\frac{d E}{d t}=\\beta\\displaystyle\\frac{S I}{N}-\\sigma E,}\\\\ {\\displaystyle\\frac{d I}{d t}=\\sigma E-\\gamma I,}\\\\ {\\displaystyle\\frac{d R}{d t}=\\gamma I,}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "table", "img_path": "SOsiObSdU2/tmp/c471287a222522578f576906792436999e836c6268d55d22d5063f6c096a9f12.jpg", "table_caption": [], "table_footnote": [], "page_idx": 33}, {"type": "text", "text": "where $N$ is the total population (assumed constant), $\\beta$ is the transmission rate, $\\sigma$ is the rate at which exposed individuals become infectious, and $\\gamma$ is the recovery rate. These parameters are crucial for capturing the dynamics of the disease spread and are estimated from data or literature. ", "page_idx": 33}, {"type": "text", "text": "Lotka\u2013Volterra Model for Predator-Prey Dynamics. The Lotka-Volterra model describes the dynamics of biological systems in which two species interact, one as a predator and the other as prey. The model is represented by a set of first-order, non-linear, differential equations: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{\\frac{d x}{d t}=\\alpha x-\\beta x y,}}\\\\ {\\displaystyle{\\frac{d y}{d t}=\\delta x y-\\gamma y,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $x$ and $y$ represent the prey and predator populations, respectively. The parameters $\\alpha,\\beta,\\gamma$ , and $\\delta$ denote the prey reproduction rate, the predation rate upon the prey, the predator mortality rate, and the rate at which predators increase by consuming prey, respectively. ", "page_idx": 33}, {"type": "text", "text": "Lotka-Volterra Triple Species Model. The extended Lotka-Volterra model incorporating a third species involves additional interactions that can represent various ecological relationships such as competition, predation, or mutualism. For the sake of illustration, let\u2019s consider a system with two predators and one prey. The model is described by the following set of differential equations: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{d x}{d t}=x(\\alpha-\\beta y-\\delta z),}\\\\ {\\displaystyle\\frac{d y}{d t}=y(-\\gamma+\\epsilon x),}\\\\ {\\displaystyle\\frac{d z}{d t}=z(-\\mu+\\nu x),}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where: $x$ represents the prey population. $y$ and $z$ represent the two predator populations. $\\alpha$ is the natural growth rate of the prey in the absence of predation. $\\beta$ and $\\delta$ are the predation rates of the first and second predators on the prey, respectively. $\\gamma$ and $\\mu$ are the natural death rates of the first and second predators, respectively, in the absence of the prey. $\\epsilon$ and $\\nu$ are the growth rates of the first and second predators per unit of prey consumed. ", "page_idx": 33}, {"type": "text", "text": "Logistic Tumor Growth Model with Treatment Effects. The logistic tumor growth model with chemotherapy and radiotherapy effects incorporates the logistic growth model\u2019s capacity to simulate the saturation effect observed in tumor growth, alongside treatment effects. The model can be described as: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\frac{d N}{d t}=r N\\left(1-\\frac{N}{K}\\right)-C(N)-R(N),\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $N$ is the tumor cell population, $r$ is the intrinsic growth rate of the tumor, and $K$ is the carrying capacity of the environment. $C(N)$ and $R(N)$ represent the effects of chemotherapy and radiotherapy on the tumor cell population, respectively. These treatment functions are often modeled based on dose-response curves and can vary depending on the specific drugs and radiation doses used. ", "page_idx": 33}, {"type": "text", "text": "H.9 Procedurally Generated Synthetic Model Benchmark ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "We performed a complete re-run of our main baselines on a new entirely procedurally generated synthetic model benchmark. Specifically, by procedurally generating synthetic models, this allows us to test how HDTwinGen performs when the LLM has never seen such a model. To create diverse synthetic models, we modified the structure of underlying cancer with chemo and radio models to incorporate non-biological random modifications, which include the use of trigonometric operators and division operators. In the following, we provide the exact changes made and the structure of the underlying equation. ", "page_idx": 33}, {"type": "text", "text": "Synthetic 1 (inc. $\\gamma\\sin(\\omega t))$ . Here the underlying equation is ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\frac{d x(t)}{d t}=\\left(\\rho\\log\\left(\\frac{K}{x(t)}\\right)-\\beta_{c}C(t)-(\\alpha_{r}d(t)+\\beta_{r}d(t)^{2})+\\gamma\\sin(\\omega t)\\right)x(t)\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Synthetic 2 (inc. $-\\delta I(t))$ ) ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\frac{d x(t)}{d t}=\\left(\\rho\\log\\left(\\frac{K}{x(t)}\\right)-\\beta_{c}C(t)-(\\alpha_{r}d(t)+\\beta_{r}d(t)^{2})-\\delta I(t)\\right)x(t)\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Synthetic 3 (inc. $\\scriptstyle\\log\\left({\\frac{K}{x(t)+N(t)}}\\right)$ ) ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\frac{d x(t)}{d t}=\\left(\\rho\\log\\left(\\frac{K}{x(t)+N(t)}\\right)-\\beta_{c}C(t)-(\\alpha_{r}d(t)+\\beta_{r}d(t)^{2})\\right)x(t)\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Synthetic 4 (inc. $\\epsilon\\cos(\\phi t))$ ) ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\frac{d x(t)}{d t}=\\left(\\rho\\log\\left(\\frac{K}{x(t)}\\right)-\\beta_{c}C(t)-(\\alpha_{r}d(t)+\\beta_{r}d(t)^{2})+\\epsilon\\cos(\\phi t)\\right)x(t)\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Synthetic 5 (inc. $\\theta C(t)d(t))$ ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\frac{d x(t)}{d t}=\\left(\\rho\\log\\left(\\frac{K}{x(t)}\\right)-\\beta_{c}C(t)-(\\alpha_{r}d(t)+\\beta_{r}d(t)^{2})-\\theta C(t)d(t)\\right)x(t)\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We observe in the tabulated results in Table 9 that HDTwinGen can still generate models that perform well. ", "page_idx": 34}, {"type": "text", "text": "Table 9: Procedurally Generated Synthetic Model Benchmark. Test MSE $\\mathcal{T}_{M S E}$ averaged over ten random seeds. Reporting the test prediction MSE $(T_{M S E})$ of the produced system models on held-out test datasets across all synthetic datasets. HDTwinGen achieves the lowest test prediction error. The results are averaged over ten random seeds, with $\\pm$ indicating $95\\%$ confidence intervals. ", "page_idx": 34}, {"type": "table", "img_path": "SOsiObSdU2/tmp/6d769ccad729f159add7e33102c53f4a25ff7a78a28a1e4ae93140353113e877.jpg", "table_caption": [], "table_footnote": [], "page_idx": 34}, {"type": "text", "text": "H.10 Interpretability Scale, Performance of only White-Box Models ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "To investigate questions, of how well do the white-box models that HDTwinGen generates perform, we explore an ablation of HDTwinGen where we constrain the generated models to be white-box only, i.e., a mathematical equation with no black-box neural network components. We tabulate this in Table 10, and observe that even when HDTwinGen is constrained to only generate white-box models (HDTwinGen (Only White-Box)) it still performs acceptably, indicating that the white-box generated models are modeling well the underlying system when fitted to the dataset. ", "page_idx": 34}, {"type": "text", "text": "Table 10: Interpretability Scale, Performance of only White-Box Models. Reporting the test prediction MSE $(T_{M S E})$ of the produced system models on held-out test datasets across all benchmark datasets. The results are averaged over ten random seeds, with $\\pm$ indicating $95\\%$ confidence intervals. ", "page_idx": 34}, {"type": "table", "img_path": "SOsiObSdU2/tmp/501adaf16f8982ac5115eb6546a6e6f6df509bacd48eb366c7e96a88b5921f97.jpg", "table_caption": [], "table_footnote": [], "page_idx": 34}, {"type": "text", "text": "H.11 HDTwinGen Flexibly Integrates Expert-in-the-loop Feedback ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Experts play an active role in model development in two main ways: ", "page_idx": 34}, {"type": "text", "text": "1. Initial prompt. Experts can describe the system, and specify modeling goals and taskspecific requirements through $S^{c o n t e x t}$ (Appendix E).   \n2. Direct model feedback. In each iteration of HDTwinGen, the expert can provide direct feedback through $H$ to guide model improvement. ", "page_idx": 35}, {"type": "text", "text": "To further demonstrate the flexibility of expert involvement in the modeling process, we supply two additional experiments: ", "page_idx": 35}, {"type": "text", "text": "1. Expert specifies instructions to produce a fully white-box model through $S^{c o n t e x t}$ , which is provided in Appendix H.10.   \n2. Expert provides specific feedback on model improvement during the development process through $H$ . We performed this experiment by stopping HDTwinGen during its iterations, providing human expert targeted feedback, and then observing if the subsequent model generated was able to incorporate the feedback by making targeted changes to the underlying model. We confirm that this is the case and provide insight that the underlying LLM is able to interpret the HDTwin code model and selectively change parts. We provide a figure to illustrate this result, as seen in Figure 7. ", "page_idx": 35}, {"type": "image", "img_path": "SOsiObSdU2/tmp/275d8683529ede684780eb36e324644333b9ec707dc7f80217159c86a49b43c6.jpg", "img_caption": ["Figure 7: HDTwinGen can flexibly integrate expert-in-the-loop feedback, if it is provided. "], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "H.12 HDTwinGen Accelerates Model Development and Enhances Performance ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "We seek to determine the runtimes of HDTwinGen model development compared to human-clock time from a human experiment (Mechanical Turk) experiment where participants are asked to refine models and how performant these models are compared to Bayesian optimization (BO) for a fixed model budget. To assess this we performed the additional experiments of: ", "page_idx": 35}, {"type": "text", "text": "1. Human-driven model development: Hiring two experienced software engineers (Mechanical Turks) to develop and refine models, providing them with exactly the same prompts that HDTwinGen uses, using a human instead of the LLM in the model development loop.   \n2. AutoML: Hyperparameter tuning (HPT) using for DyNODE and SINDy using BO (HPT search space detailed in Table 11). ", "page_idx": 35}, {"type": "text", "text": "We performed these two experiments on the Lung Cancer (with Chemo. & Radio.) dataset, and the results are tabulated in Table 12. Here we present, the time to generate an individual model (which includes generating the model and training time); the test MSE $\\mathcal{T}_{M S E}$ after one hour of run-time, and test MSE separately for a budget of only 15 model evaluations. ", "page_idx": 35}, {"type": "text", "text": "Analyzing the results provides the following insights: ", "page_idx": 35}, {"type": "text", "text": "HDTwinGen Makes Model Development Notably Faster Compared to Human-Clock Time: HDTwinGen takes an average of 45.56 minutes to complete an experiment using 15 ", "page_idx": 35}, {"type": "table", "img_path": "SOsiObSdU2/tmp/7d2e79bcba81ab5702e1e4e8cf9ef8ac391d589f35a5f75821030442e9661b1e.jpg", "table_caption": ["Table 11: HPT details. BO HPT search space for Figure 8. "], "table_footnote": [], "page_idx": 36}, {"type": "text", "text": "Table 12: Method performance comparison. Reporting the time to generate a model (in minutes) and the test MSE $(T_{M S E})$ after one hour of run-time and with a budget of only 15 model evaluations. Here \u2217indicates $73.11\\%$ of iteration time is consumed by LLM querying. ", "page_idx": 36}, {"type": "table", "img_path": "SOsiObSdU2/tmp/e33027658dc4662ceb2906fb6c233c1c2ebd38de35490e7daf1f75a17625349f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 36}, {"type": "text", "text": "model evaluations/generations, whereas human experts took 148.1 minutes (2 hours and 28 minutes) to generate and iterate 15 models. Therefore, using HDTwinGen is considerably faster than human-clock time, which is also an advantage of HDTwinGen. ", "page_idx": 36}, {"type": "text", "text": "2. HDTwinGen Generates Better Performing Models for a Fixed Model Budget: HDTwinGen generates better-performing models compared to the human experts and the Bayesian hyperparameter optimization (HPO) of the baselines of DyNode and SINDy, for a fixed budget of 15 model evaluations. We provide a figure showing the best-performing model performance against each generation in a Figure 8. This figure shows that across model evaluations, HDTwinGen still generates models that perform well. ", "page_idx": 36}, {"type": "image", "img_path": "SOsiObSdU2/tmp/189544d41ed78abb29ecf1dd1a522f84d8c66e1589f393d20c07667ede4ab841.jpg", "img_caption": ["Figure 8: Best-performing model performance against each generation, for setup in Appendix H.12. HDTwinGen Generates Better Performing Models for a Fixed Model Budget. "], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "I Hybrid Model Output Examples ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "These are the final discovered hybrid models generated from our method HDTwin, for each respective environment. ", "page_idx": 37}, {"type": "text", "text": "Cancer (with Chemo & Radio) ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "c l a s s S t a t e D i f f e r e n t i a l ( nn . Module ) :   \ndef $-\\,\\mathrm{i}\\,\\mathrm{n}\\,\\mathrm{i}\\,\\mathrm{t}_{--}$ ( s e l f ) : super ( S t a t e D i f f e r e n t i a l , s e l f ) . _ i n i t _ _ ( ) # Parameters f o r the tumor growth model s e l f . alpha $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 0 3 ) ) # Tumor growth r a t e s e l f . beta $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 4 ) ) # Tumor death r a t e due to chemotherapy s e l f . gamma $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 0 8 ) ) # Tumor death r a t e due to r a d i o t h e r a p y s e l f . kappa_base $=$ nn . Parameter ( t o r c h . t e n s o r ( 1 0 3 0 . 0 ) # Base ca r ry i ng c a p a c i t y of the environment s e l f . kappa_mod $=$ nn . Parameter ( t o r c h . t e n s o r ( \u22122.0) ) # Modifier f o r ca r ry i ng c a p a c i t y based on t r e a t m e n t s e l f . d e l t a _ b a s e = nn . Parameter ( t o r c h . t e n s o r ( 0 . 1 ) ) # Base decay r a t e of the chemotherapy drug s e l f . delta_mod $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 0 0 0 3 ) ) # Modifier f o r decay r a t e based on tumor volume s e l f . e t a $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 0 0 4 ) ) # I n t e r a c t i o n term between chemotherapy and r a d i o t h e r a p y s e l f . t h e t a $=$ nn . Parameter ( t o r c h . t e n s o r ( 1 0 . 0 ) ) # Michaelis \u2212Menten s a t u r a t i o n c o n s t a n t f o r chemotherapy s e l f . rho $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 3 ) ) # Sigmoid s t e e p n e s s f o r r a d i o t h e r a p y e f f e c t s e l f . z e t a $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 1 5 ) ) # R e s i s t a n c e development r a t e # Black box component f o r c a p t u r i n g r e s i d u a l s s e l f . r e s i d u a l _ m l p = nn . S e q u e n t i a l ( nn . Linear (4 , 16) , nn . LeakyReLU ( 0 . 1 ) , nn . Linear (16 , 8) , nn . LeakyReLU ( 0 . 1 ) , nn . Linear (8 , 2) ,   \ndef forward ( s e l f , tumor_volume : t o r c h . Tensor , chemotherapy_drug_concentration : t o r c h . Tensor , chemotherapy_dosage : t o r c h . Tensor , r a d i o t h e r a p y _ d o s a g e : t o r c h . Tensor ,   \n) $->$ Tuple [ t o r c h . Tensor , t o r c h . Tensor ] : # Adjusted c ar r yi n g c a p a c i t y based on t r e a t m e n t kappa $=$ s e l f . kappa_base $^+$ s e l f . kappa_mod $^*$ ( chemotherapy_dosage $^+$ r a d i o t h e r a p y _ d o s a g e ) # Adjusted decay r a t e based on tumor volume d e l t a $=$ s e l f . d e l t a _ b a s e $^+$ s e l f . delta_mod $^*$ tumor_volume # L o g i s t i c growth model f o r tumor volume with i n t e r a c t i o n term and r e s i s t a n c e r e s i s t a n c e $=\\mathrm{~1~+~\\,~s~c~l~f~}$ . z e t a $^*$ tumor_volume d_tumor_volume__dt $=$ ( s e l f . alpha $^\\ast$ tumor_volume $^*$ (1 \u2212 tumor_volume / kappa ) \u2212 ( s e l f . beta $^*$ chemotherapy_drug_concentration $^*$ tumor_volume ) / ( s e l f . t h e t a $^+$ chemotherapy_drug_concentration ) \u2212 s e l f . gamma \\* r a d i o t h e r a p y _ d o s a g e \\* tumor_volume / $^{\\mathrm{~\\sc~\\leftmoon~}}_{\\mathrm{~\\rightmoon~}}+$ t o r c h . exp ( \u2212s e l f . rho $^*$ ( r a d i o t h e r a p y _ d o s a g e \u2212 1) ) ) \u2212 s e l f . e t a \\* chemotherapy_drug_concentration \\* r a d i o t h e r a p y _ d o s a g e \\* tumor_volume / r e s i s t a n c e ) # Non\u2212l i n e a r decay model f o r chemotherapy drug c o n c e n t r a t i o n d_chemotherapy_drug_concentration__dt $=$ chemotherapy_dosage \u2212 d e l t a $^\\ast$ t o r c h . pow( chemotherapy_drug_concentration , 1.5 ) # Black box r e s i d u a l component r e s i d u a l s $=$ s e l f . r e s i d u a l _ m l p ( t o r c h . s t a c k ( ( tumor_volume , chemotherapy_drug_concentration , chemotherapy_dosage , radiotherapy_dosage , ) , dim =1 , ) # Combine white box model with r e s i d u a l s d_tumor_volume__dt $+=$ r e s i d u a l s [ : , 0] d_chemotherapy_drug_concentration__dt $+=$ r e s i d u a l s [ : , 1] ", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 37}, {"type": "text", "text": "COVID-19 ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "c l a s s S t a t e D i f f e r e n t i a l ( nn . Module ) :   \ndef $-\\,\\mathrm{i}\\,\\mathrm{n}\\,\\mathrm{i}\\,\\mathrm{t}_{--}$ ( s e l f ) : super ( S t a t e D i f f e r e n t i a l , s e l f ) . _ _ i n i t _ _ ( ) # I n i t i a l i z e the parameters f o r the SEIR model using o p t i m i z e d values s e l f . beta $=$ nn . Parameter ( t o r c h . t e n s o r (0.2607165277004242) ) # Transmission r a t e s e l f . sigma $=$ nn . Parameter ( t o r c h . t e n s o r (0.23686641454696655) ) # I n c u b a t i o n r a t e s e l f . gamma $=$ nn . Parameter ( t o r c h . t e n s o r (0.105068139731884) ) # Recovery r a t e s e l f . d e l t a $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 0 1 ) ) # Death r a t e # Define a neural network f o r c a p t u r i n g complex p a t t e r n s # Adjust the a r c h i t e c t u r e based on p re v io u s i t e r a t i o n s s e l f . r e s i d u a l _ n n $=$ nn . S e q u e n t i a l ( nn . Linear (4 , 12) , nn . ReLU ( ) , nn . Linear (12 , 12) , nn . ReLU ( ) , nn . Linear (12 , 4) ", "page_idx": 38}, {"type": "text", "text": "def forward ( s e l f , s u s c e p t i b l e : t o r c h . Tensor , exposed : t o r c h . Tensor , i n f e c t e d : t o r c h . Tensor , recovered : t o r c h . Tensor , t o t a l _ p o p u l a t i o n : t o r c h . Tensor ) $->$ Tuple [ t o r c h . Tensor , t o r c h . Tensor , t o r c h . Tensor , t o r c h . Tensor ] : # SEIR model d i f f e r e n t i a l e q u a t i o n s d _ s u s c e p t i b l e _ _ d t $=$ \u2212s e l f . beta $^*$ s u s c e p t i b l e $^\\ast$ i n f e c t e d d_exposed__dt $=$ s e l f . beta $*$ s u s c e p t i b l e $^*$ i n f e c t e d \u2212 s e l f . sigma $^*$ exposed d _ i n f e c t e d _ _ d t $=$ s e l f . sigma $^*$ exposed \u2212 ( s e l f . gamma $^+$ s e l f . d e l t a ) $*$ i n f e c t e d d_recovered__dt $=$ s e l f . gamma $*$ i n f e c t e d \u2212 s e l f . d e l t a $^*$ i n f e c t e d # C a l c u l a t e r e s i d u a l s using the neural network s t a t e s $=$ t o r c h . s t a c k ( [ s u s c e p t i b l e , exposed , i n f e c t e d , recovered ] , dim =1) r e s i d u a l s $=$ s e l f . r e s i d u a l _ n n ( s t a t e s ) # Add r e s i d u a l s to the d i f f e r e n t i a l e q u a t i o n s d _ s u s c e p t i b l e _ _ d t += r e s i d u a l s [ : , 0] d_exposed__dt $+=$ r e s i d u a l s [ : , 1] d _ i n f e c t e d _ _ d t $+=$ r e s i d u a l s [ : , 2] d_recovered__dt $+=$ r e s i d u a l s [ : , 3] ", "page_idx": 38}, {"type": "text", "text": "return ( d _ s u s c e p t i b l e _ _ d t , d_exposed__dt , d_infected __dt , d_recovered__dt ) ", "page_idx": 38}, {"type": "text", "text": "Plankton Microcosm ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "c l a s s S t a t e D i f f e r e n t i a l ( nn . Module ) :   \ndef _ _ i n i t _ _ ( s e l f ) : super ( S t a t e D i f f e r e n t i a l , s e l f ) . _ _ i n i t _ _ ( ) # Define the parameters f o r the improved model with c a rr y in g c a p a c i t i e s s e l f . alpha $=$ nn . Parameter ( t o r c h . t e n s o r (0.022967826575040817) ) # Prey growth r a t e s e l f . beta $=$ nn . Parameter ( t o r c h . t e n s o r (0.6899635791778564) ) # Prey death r a t e due to p r e d a t i o n s e l f . gamma $=$ nn . Parameter ( t o r c h . t e n s o r (0.15562176704406738) ) # I n t e r m e d i a t e predator e f f i c i e n c y s e l f . d e l t a $=$ nn . Parameter ( t o r c h . t e n s o r (0.8135092854499817) ) # Top predator e f f i c i e n c y # Carrying c a p a c i t i e s f o r each p o p u l a t i o n s e l f . K_prey $=$ nn . Parameter ( t o r c h . t e n s o r (0.4680666923522949) ) # Carrying c a p a c i t y f o r the prey p o p u l a t i o n s e l f . K_intermediate $=$ nn . Parameter ( t o r c h . t e n s o r (0.8180080652236938) ) # Carrying c a p a c i t y f o r the i n t e r m e d i a t e predator p o p u l a t i o n s e l f . K_top $=$ nn . Parameter ( t o r c h . t e n s o r (0.4186957776546478) ) # Carrying c a p a c i t y f o r the top predator p o p u l a t i o n # Competition c o e f f i c i e n t s s e l f . sigma $=$ nn . Parameter ( t o r c h . t e n s o r ( \u22120.27261480689048767) ) # Competition c o e f f i c i e n t f o r prey and i n t e r m e d i a t e p r e d a t o r s s e l f . e t a $=$ nn . Parameter ( t o r c h . t e n s o r (0.06442223489284515) ) # Competition c o e f f i c i e n t f o r i n t e r m e d i a t e p r e d a t o r s and top p r e d a t o r s # MLP f o r r e s i d u a l s with r e f i n e d a r c h i t e c t u r e s e l f . r e s i d u a l _ m l p = nn . S e q u e n t i a l ( nn . Linear (3 , 128) , nn . LeakyReLU ( 0 . 0 1 ) , nn . Dropout ( 0 . 2 5 ) , nn . Linear (128 , 128) , nn . LeakyReLU ( 0 . 0 1 ) , nn . Dropout ( 0 . 2 5 ) , nn . Linear (128 , 3) ", "page_idx": 38}, {"type": "text", "text": "def forward ( s e l f , p r e y _ p o p u l a t i o n : t o r c h . Tensor , i n t e r m e d i a t e _ p o p u l a t i o $:$ t o r c h . Tensor , t o p _ p r e d a t o r s _ p o p u l a t i o n $:$ t o r c h . Tensor ) $->$ Tuple [ t o r c h . Tensor , t o r c h . Tensor , t o r c h . Tensor ] : # Improved d i f f e r e n t i a l e q u a t i o n s with c ar r yi n g c a p a c i t i e s d_prey_population__dt $=$ s e l f . alpha \\* p r e y _ p o p u l a t i o n $^*$ (1 \u2212 p r e y _ p o p u l a t i o n / s e l f . K_prey ) \u2212 s e l f . beta $^*$ p r e y _ p o p u l a t i o n \\* i n t e r m e d i a t e _ p o p u l a t i o n d _ i n t e r m e d i a t e _ p o p u l a t i o n _ _ d t $=$ ( s e l f . gamma $^*$ p r e y _ p o p u l a t i o n $*$ i n t e r m e d i a t e _ p o p u l a t i o n $/$ $1\\ +$ p r e y _ p o p u l a t i o n ) ) \u2212 s e l f . d e l t a \\* i n t e r m e d i a t e _ p o p u l a t i o n \\* t o p _ p r e d a t o r s _ p o p u l a t i o n \u2212 s e l f . sigma $^\\ast$ i n t e r m e d i a t e _ p o p u l a t i o n $\\approx\\!\\!\\!\\otimes\\!\\!\\!\\!\\!^{\\!}\\!\\!\\!\\!\\!\\!2\\ensuremath{\\!\\!\\!\\!\\!\\!\\!\\mathrm{~\\ensuremath~{~\\ast~}~}}$ (1 \u2212 i n t e r m e d i a t e _ p o p u l a t i o n / s e l f . K_intermediate ) d _ t o p _ p r e d a t o r s _ p o p u l a t i o n _ _ d t $=$ \u2212s e l f . gamma $^\\ast$ t o p _ p r e d a t o r s _ p o p u l a t i o n $^+$ s e l f . d e l t a $^\\ast$ i n t e r m e d i a t e _ p o p u l a t i o n \\* t o p _ p r e d a t o r s _ p o p u l a t i o n \u2212 s e l f . e t a $^*$ t o p _ p r e d a t o r s _ p o p u l a t i o n \\*\\*2 \\* (1 \u2212 t o p _ p r e d a t o r s _ p o p u l a t i o n / s e l f . K_top ) ", "page_idx": 38}, {"type": "text", "text": "# Correct the d i f f e r e n t i a l e q u a t i o n s with the r e s i d u a l s d_prey_population $\\_{\\/-}\\mathrm{d}{\\mathfrak{t}}\\quad+=$ r e s i d u a l s [ : , 0] d _ i n t e r m e d i a t e _ p o p u l a t i o n _ _ d t $+=$ r e s i d u a l s [ : , 1] d _ t o p _ p r e d a t o r s _ p o p u l a t i o n _ _ d t $+=$ r e s i d u a l s [ : , 2] ", "page_idx": 38}, {"type": "text", "text": "return ( d_prey_population__dt , d _ i n t e r m e d i a t e _ p o p u l a t i o n _ _ d t , d _ t o p _ p r e d a t o r s _ p o p u l a t i o n _ _ d t ) ", "page_idx": 38}, {"type": "text", "text": "Hare-Lynx ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "c l a s s S t a t e D i f f e r e n t i a l ( nn . Module ) : def _ _ i n i t _ _ ( s e l f ) : super ( S t a t e D i f f e r e n t i a l , s e l f ) . _ _ i n i t _ _ ( ) # Define the parameters f o r the modified Lotka \u2212V o l t e r r a e q u a t i o n s s e l f . alpha $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 0 6 7 ) ) # Prey b i r t h r a t e s e l f . beta $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 0 0 5 ) ) # Predation r a t e ", "page_idx": 38}, {"type": "text", "text": "s e l f . gamma $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 2 5 3 ) ) # Predator death r a t e   \ns e l f . d e l t a $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 0 0 2 8 ) ) # Predator r e p r o d u c t i o n r a t e   \n# A d d i t i o n a l parameters f o r c ar r yi n g c a p a c i t y and time \u2212dependent e f f e c t s   \ns e l f . kappa $=$ nn . Parameter ( t o r c h . t e n s o r ( 1 . 0 ) ) # Carrying c a p a c i t y f o r hares   \ns e l f . omega $=$ nn . Parameter ( t o r c h . t e n s o r ( $^{\\mathrm{~2~}}\\ast$ t o r c h . pi / 10) ) # Frequency of o s c i l l a t i o n s (10\u2212year period )   \ns e l f . phi $=$ nn . Parameter ( t o r c h . t e n s o r ( 0 . 0 ) ) # Phase s h i f t f o r o s c i l l a t i o n s   \n# Neural network f o r c a p t u r i n g r e s i d u a l s   \ns e l f . r e s i d u a l _ m l p $=$ nn . S e q u e n t i a l ( nn . Linear (3 , 10) , nn . ReLU ( ) , nn . Linear (10 , 2) ", "page_idx": 39}, {"type": "text", "text": "def forward ( s e l f , h a r e _ p o p u l a t i o n : t o r c h . Tensor , l y n x _ p o p u l a t i o n : t o r c h . Tensor , t i m e _ i n _ y e a r s : t o r c h . Tensor ) $->$ Tuple [ t o r c h . Tensor , t o r c h . Tensor ] : # Modified Lotka \u2212V o l t e r r a e q u a t i o n s with c ar r yi n g c a p a c i t y and time \u2212dependent e f f e c t s d _ h a r e _ p o p u l a t i o n _ _ d t $=$ s e l f . alpha $*$ h a r e _ p o p u l a t i o n $^*$ (1 \u2212 h a r e _ p o p u l a t i o n / s e l f . kappa ) \u2212 s e l f . beta $^*$ h a r e _ p o p u l a t i o n \\* l y n x _ p o p u l a t i o n d_lynx_population__dt $=$ \u2212s e l f . gamma $^*$ l y n x _ p o p u l a t i o n $^+$ s e l f . d e l t a $^*$ h a r e _ p o p u l a t i o n $*$ l y n x _ p o p u l a t i o n # Time\u2212dependent o s c i l l a t o r y component t i m e _ e f f e c t $=$ t o r c h . s i n ( s e l f . omega $*$ t i m e _ i n _ y e a r s $^+$ s e l f . phi ) # Combine white box model with neural network r e s i d u a l s r e s i d u a l s $=$ s e l f . r e s i d u a l _ m l p ( t o r c h . s t a c k ( ( hare_population , lynx_population , t i m e _ e f f e c t ) , dim =1) ) d _ h a r e _ p o p u l a t i o n _ _ d t $+=$ r e s i d u a l s [ : , 0] d_lynx_population__dt $+=$ r e s i d u a l s [ : , 1] return ( d_hare_population__dt , d_lynx_population__dt ) ", "page_idx": 39}, {"type": "text", "text": "J HDTwinGen can reason about HDTwins ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Worked log output of HDTwinGen of part of a run, running on the Plankton Microcosm dataset. It can reason about structures and parameters. ", "page_idx": 39}, {"type": "text", "text": "You w i l l get a system d e s c r i p t i o n to code a d i f f e r e n t i a l eq u at i on s i m u l a t o r f o r .   \nSystem D e s c r i p t i o n : \u2018 \u2018 \u2018   \n\" Modeling A r t i f i c i a l Tri \u2212Trophic Prey \u2212P r e d a t o r O s c i l l a t i o n s in a S i m p l i f i e d E c o l o g i c a l System   \nHere you must model the s t a t e d i f f e r e n t i a l of al g ae _ po p ul a ti o n , f l a g e l l a t e _ p o p u l a t i o n , and r o t i f e r _ p o p u l a t i o n ; with no i n p u t a c t i o n s . This aims to s i m u l a t e the p o p u l a t i o n dynamics within a s i m p l i f i e d t r i \u2212t r o p h i c e c o l o g i c a l system comprising prey ( algae ) , i n t e r m e d i a t e p r e d a t o r s ( f l a g e l l a t e s ) , and top p r e d a t o r s ( r o t i f e r s ) . The i n t e r a c t i o n s i n c l u d e d i r e c t p r e d a t i o n and c o m p e t i t i o n f o r resources , m i r r o r i n g n a t u r a l i n t r a g u i l d p r e d a t i o n mechanisms .   \nD e s c r i p t i o n of the v a r i a b l e s :   \n\\* p r e y _ p o p u l a t i o n : Total count of algae , s e r v i n g as the primary prey   \n\\* i n t e r m e d i a t e _ p o p u l a t i o n : Total count of f l a g e l l a t e s , a c t i n g as i n t e r m e d i a t e p r e d a t o r s and prey   \n\\* t o p _ p r e d a t o r s _ p o p u l a t i o n : Total count of r o t i f e r s , r e p r e s e n t i n g top p r e d a t o r s   \nThe d a t a s e t e n c a p s u l a t e s d a i l y p o p u l a t i o n counts a c r o s s m u l t i p l e s i m u l a t e d ecosystems over a period of 100 days , allowing f o r the a n a l y s i s of temporal o s c i l l a t i o n s and phase l a g s between s p e c i e s .   \nA d d i t i o n a l l y t h e s e v a r i a b l e s have the ranges of :   \n\\* p r e y _ p o p u l a t i o n : [0.095898 , 2.469735]   \n\\* i n t e r m e d i a t e _ p o p u l a t i o n : [0.008438 , 1.500000]   \n\\* t o p _ p r e d a t o r s _ p o p u l a t i o n : [0.030316 , 0.739244] ", "page_idx": 39}, {"type": "text", "text": "Modelling goals : \u2018 \u2018 \u2018 \\* The parameters of the model w i l l be optimized to an observed t r a i n i n g d a t a s e t with the given s i m u l a t o r . \\* The observed t r a i n i n g d a t a s e t has very few samples , and the model must be able to g e n e r a l i z e to unseen data . ", "page_idx": 39}, {"type": "text", "text": "Requirement S p e c i f i c a t i o n : \u2018 \u2018 \u2018 \\* The code g e n e r a t e d should achieve the lowest p o s s i b l e v a l i d a t i o n loss , of 1e\u22126 or l e s s . \\* The code g e n e r a t e d should be i n t e r p r e t a b l e , and f i t the d a t a s e t as a c c u r a t e l y as p o s s i b l e def forward ( s e l f , p r e y _ p o p u l a t i o n : t o r c h . Tensor , i n t e r m e d i a t e _ p o p u l a t i o n $\\boldsymbol{:}$ t o r c h . Tensor , t o p _ p r e d a t o r s _ p o p u l a t i o n : t o r c h . Tensor ) $->$ Tuple [ t o r c h . Tensor , t o r c h . Tensor , t o r c h . Tensor ] : # TODO: F i l l in the code here r e t u r n ( d_prey_population__dt , d _ i n t e r m e d i a t e _ p o p u l a t i o n _ _ d t , d _ t o p _ p r e d a t o r s _ p o p u l a t i o n _ _ d t ) ", "page_idx": 39}, {"type": "text", "text": "", "page_idx": 39}, {"type": "text", "text": "Useful to know : \u2018 \u2018 \u2018   \n\\* You are a code evolving machine , and you w i l l be c a l l e d 20 times to g e n e r a t e code , and improve the code to achieve the lowest p o s s i b l e v a l i d a t i o n l o s s .   \n$*$ The model d e f i n e s the s t a t e d i f f e r e n t i a l and w i l l be used with an ODE s o l v e r to f i t the observed t r a i n i n g d a t a s e t .   \n\\* You can use any parameters you want and any black box n e u r a l network components ( multi \u2212l a y e r p e r c e p t r o n s ) ; however , you have to d e f i n e t h e s e .   \n\\* I t i s p r e f e r a b l e to decompose the system i n t o d i f f e r e n t i a l e q u a t i o n s ( compartments ) i f p o s s i b l e .   \n\\* You can use any unary f u n c t i o n s , f o r example log , exp , power e t c .   \n\\* Under no circumstance can you change the s k e l e t o n code f u n c t i o n d e f i n i t i o n s , only f i l l in the code .   \n\\* The i n p u t t e n s o r s are v e c t o r s of shape ( b a t c h _ s i z e ) .   \n\\* Use i n i t i a l l y white box models f i r s t and then switch to hybrid white and black box models f o r the r e s i d u a l s , only a f t e r no f u r t h e r b e s t program i t e r a t i o n improvement with white box models .   \n\\* Make sure your code follows the exact code s k e l e t o n s p e c i f i c a t i o n .   \n\\* Use PyTorch . ", "page_idx": 39}, {"type": "text", "text": "You g e n e r a t e d the f o l l o w i n g code completions , which then had t h e i r parameters optimized to the t r a i n i n g d a t a s e t . Please r e f l e c t on how you can improve the code to minimize the v a l i d a t i o n l o s s to 1e\u22126 or l e s s . The code examples are d e l i n e a t e d by ###. ", "page_idx": 40}, {"type": "text", "text": "Here are your p r ev i ou s i t e r a t i o n s the b e s t programs g e n e r a t e d . Use i t to see i f you have exhausted white box models , i . e . when a white box model r e p e a t s with the same val l o s s and then only add black box models to the white box models : \u2018 \u2018 \u2018   \nI t e r a t i o n 0. Best Val Loss : 0.012224067933857441. Model d e s c r i p t i o n : white box model   \nI t e r a t i o n 1. Best Val Loss : 0.003129902994260192. Model d e s c r i p t i o n : white box model with a d d i t i o n a l parameters f o r c o m p e t i t i o n and c a r r y i n g capacity , and improved d i f f e r e n t i a l e q u a t i o n s to b e t t e r c a p t u r e the t r i \u2212t r o p h i c dynamics . ", "page_idx": 40}, {"type": "text", "text": "Here are the top code completions so f a r t h a t you have generated , s o r t e d f o r the lowest v a l i d a t i o n l o s s l a s t ", "page_idx": 40}, {"type": "text", "text": "Val Loss : 0.0122 ( Where the val l o s s per dimension i s p r e y _ p o p u l a t i o n val l o s s : 0.0316 , i n t e r m e d i a t e _ p o p u l a t i o n val l o s s : 2.13 e \u221205 , t o p _ p r e d a t o r s _ p o p u l a t i o n val l o s s : 0.00505) I t e r a t i o n : 0 ", "page_idx": 40}, {"type": "text", "text": "### ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "import t o r c h", "page_idx": 40}, {"type": "text", "text": "import t o r c h . nn as nn from typing import Tuple ", "page_idx": 40}, {"type": "text", "text": "c l a s s S t a t e D i f f e r e n t i a l ( nn . Module ) ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "def _ _ i n i t _ _ ( s e l f ) : super ( S t a t e D i f f e r e n t i a l , s e l f ) . _ _ i n i t _ _ ( ) # Define the parameters f o r the Lotka \u2212V o l t e r r a e q u a t i o n s s e l f . alpha $=$ nn . Parameter ( t o r c h . rand ( 1 ) ) # Prey growth r a t e s e l f . beta $=$ nn . Parameter ( t o r c h . rand ( 1 ) ) # Prey death r a t e due to p r e d a t i o n s e l f . gamma $=$ nn . Parameter ( t o r c h . rand ( 1 ) ) # P r e d a t o r growth r a t e due to p r e d a t i o n s e l f . d e l t a $=$ nn . Parameter ( t o r c h . rand ( 1 ) ) # P r e d a t o r death r a t e ", "page_idx": 40}, {"type": "text", "text": "def forward ( s e l f , p r e y _ p o p u l a t i o n : t o r c h . Tensor , i n t e r m e d i a t e _ p o p u l a t i o n $\\boldsymbol{:}$ t o r c h . Tensor , t o p _ p r e d a t o r s _ p o p u l a t i o n $\\boldsymbol{:}$ t o r c h . Tensor ) $->$ Tuple [ t o r c h . Tensor , t o r c h . Tensor , t o r c h . Tensor ] : # Lotka \u2212V o l t e r r a e q u a t i o n s d_prey_population $\\_{\\/-}\\ d\\mathfrak{t}\\ =$ s e l f . alpha $*$ p r e y _ p o p u l a t i o n \u2212 s e l f . beta \\* p r e y _ p o p u l a t i o n $^*$ i n t e r m e d i a t e _ p o p u l a t i o n d _ i n t e r m e d i a t e _ p o p u l a t i o n _ _ d t $=$ s e l f . gamma \\* p r e y _ p o p u l a t i o n $^\\ast$ i n t e r m e d i a t e _ p o p u l a t i o n \u2212 s e l f . d e l t a \\* i n t e r m e d i a t e _ p o p u l a t i o n $*$ t o p _ p r e d a t o r s _ p o p u l a t i o n d _ t o p _ p r e d a t o r s _ p o p u l a t i o n _ _ d t $=$ \u2212s e l f . gamma $*$ t o p _ p r e d a t o r s _ p o p u l a t i o n $^+$ s e l f . d e l t a $*$ i n t e r m e d i a t e _ p o p u l a t i o n \\* t o p _ p r e d a t o r s _ p o p u l a t i o n ", "page_idx": 40}, {"type": "text", "text": "optimized_parameters $=$ { \u2019 alpha \u2019 : 0.10977201908826828 , \u2019 beta \u2019 : 0.68703693151474 , \u2019gamma \u2019 : 0.12234053760766983 , \u2019 de l t a 0.8221619725227356} ", "page_idx": 40}, {"type": "text", "text": "### ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Val Loss : 0.00313 ( Where the val l o s s per dimension i s p r e y _ p o p u l a t i o n val l o s s : 0.00776 , i n t e r m e d i a t e _ p o p u l a t i o n val l o s s : 2.31 e \u221205 , t o p _ p r e d a t o r s _ p o p u l a t i o n val l o s s : 0.00161) I t e r a t i o n : 1 ", "page_idx": 40}, {"type": "text", "text": "### ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "import t o r c h import t o r c h . nn as nn from typing import Tuple ", "page_idx": 40}, {"type": "text", "text": "l a s s S t a t e D i f f e r e n t i a l ( nn . Module ) : ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "def _ _ i n i t _ _ ( s e l f ) : super ( S t a t e D i f f e r e n t i a l , s e l f ) . _ _ i n i t _ _ ( ) # Define the parameters f o r the improved model s e l f . alpha $=$ nn . Parameter ( t o r c h . t e n s o r (0.10977201908826828) ) # Prey growth r a t e s e l f . beta $=$ nn . Parameter ( t o r c h . t e n s o r (0.68703693151474) ) # Prey death r a t e due to p r e d a t i o n s e l f . gamma $=$ nn . Parameter ( t o r c h . t e n s o r (0.12234053760766983) ) # P r e d a t o r growth r a t e due to p r e d a t i o n s e l f . d e l t a $=$ nn . Parameter ( t o r c h . t e n s o r (0.8221619725227356) ) # P r e d a t o r death r a t e # A d d i t i o n a l parameters f o r c o m p e t i t i o n and c a r r y i n g c a p a c i t y s e l f . sigma $=$ nn . Parameter ( t o r c h . rand ( 1 ) ) # Competition c o e f f i c i e n t f o r prey and i n t e r m e d i a t e p r e d a t o r s s e l f . e t a $=$ nn . Parameter ( t o r c h . rand ( 1 ) ) # Competition c o e f f i c i e n t f o r i n t e r m e d i a t e p r e d a t o r s and top p r e d a t o r s s e l f . $\\mathrm{~K~}=$ nn . Parameter ( t o r c h . rand ( 1 ) ) # Carrying c a p a c i t y f o r the prey p o p u l a t i o n ", "page_idx": 40}, {"type": "text", "text": "def forward ( s e l f , p r e y _ p o p u l a t i o n : t o r c h . Tensor , i n t e r m e d i a t e _ p o p u l a t i o n $\\boldsymbol{:}$ t o r c h . Tensor , t o p _ p r e d a t o r s _ p o p u l a t i o n : t o r c h . Tensor ) Tuple [ t o r c h . Tensor , t o r c h . Tensor , t o r c h . Tensor ] : # Improved d i f f e r e n t i a l e q u a t i o n s d_prey_population__dt $=$ s e l f . alpha $*$ p r e y _ p o p u l a t i o n $^*$ (1 \u2212 p r e y _ p o p u l a t i o n / s e l f .K) \u2212 s e l f . beta $*$ p r e y _ p o p u l a t i o n $^*$ i n t e r m e d i a t e _ p o p u l a t i o n d _ i n t e r m e d i a t e _ p o p u l a t i o n _ _ d t $=$ s e l f . gamma $^*$ p r e y _ p o p u l a t i o n $^\\ast$ i n t e r m e d i a t e _ p o p u l a t i o n \u2212 s e l f . d e l t a $^*$ i n t e r m e d i a t e _ p o p u l a t i o n $*$ t o p _ p r e d a t o r s _ p o p u l a t i o n \u2212 s e l f . sigma $*$ i n t e r m e d i a t e _ p o p u l a t i o n \\*\\*2 d _ t o p _ p r e d a t o r s _ p o p u l a t i o n _ _ d t $=$ \u2212s e l f . gamma \\* t o p _ p r e d a t o r s _ p o p u l a t i o n $^+$ s e l f . d e l t a \\* i n t e r m e d i a t e _ p o p u l a t i o n $*$ t o p _ p r e d a t o r s _ p o p u l a t i o n \u2212 s e l f . e t a $^*$ t o p _ p r e d a t o r s _ p o p u l a t i o n \\*\\*2 ", "page_idx": 40}, {"type": "text", "text": "r e t u r n ( d_prey_population__dt , d _ i n t e r m e d i a t e _ p o p u l a t i o n _ _ d t , d _ t o p _ p r e d a t o r s _ p o p u l a t i o n _ _ d t ) ", "page_idx": 40}, {"type": "text", "text": "optimized_parameters $=$ { \u2019 alpha \u2019 : \u22120.013438135385513306 , \u2019 beta \u2019 : 0.6186316609382629 , \u2019gamma \u2019 : 0.36335352063179016 , \u2019 d e l t a \u2019 0.6213762760162354 , \u2019 sigma \u2019 : 0.47441932559013367 , \u2019 eta \u2019 : \u22120.43212178349494934 , \u2019K\u2019 : 0.3493500351905823} ", "page_idx": 40}, {"type": "text", "text": "### ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Please r e f l e c t on how you can improve the code to f i t the d a t a s e t as a c c u r a t e l y as p o s s i b l e , and be i n t e r p r e t a b l e . Think step \u2212by\u2212 s t e p . Provide only a c t i o n a b l e feedback , t h a t has d i r e c t changes to the code . Do not w r i t e out the code , only d e s c r i b e how i t can be improved . Where a p p l i c a b l e use the values of the optimized parameters to reason how the code can be improved to f i t the d a t a s e t as a c c u r a t e l y as p o s s i b l e . This i s f o r g e n e r a t i n g new code f o r the next i t e r a t i o n 2 out of 20. ", "page_idx": 40}, {"type": "text", "text": "MainProcess | 2024\u221201\u221229 00:02:01 ,082 ,82 m u l t i p r o c e s s i n g INFO [ A s s i s t a n t ] To improve the code f o r i t e r a t i o n 2 , c o n s i d e r the f o l l o w i n g a c t i o n a b l e feedback : ", "page_idx": 40}, {"type": "text", "text": "1. \\*\\* I n c o r p o r a t e Resource L i m i t a t i o n f o r I n t e r m e d i a t e P r e d a t o r s and Top P r e d a t o r s \\*\\*: The c u r r e n t model i n c l u d e s a c a r r y i n g c a p a c i t y term (K) f o r the prey p o p u l a t i o n but does not account f o r r e s o u r c e l i m i t a t i o n f o r i n t e r m e d i a t e p r e d a t o r s and top p r e d a t o r s . I n t r o d u c e c a r r y i n g c a p a c i t y parameters f o r t h e s e p o p u l a t i o n s to model t h e i r growth s a t u r a t i o n . ", "page_idx": 40}, {"type": "text", "text": "2. \\*\\* Refine I n t e r a c t i o n Terms \\*\\*: The i n t e r a c t i o n terms c u r r e n t l y use simple m u l t i p l i c a t i v e f a c t o r s between p o p u l a t i o n s . Consider more complex i n t e r a c t i o n s t h a t may i n c l u d e n o n l i n e a r e f f e c t s or f u n c t i o n a l responses , such as a Holling Type I I f u n c t i o n a l response , to b e t t e r c a p t u r e the dynamics of p r e d a t i o n . ", "page_idx": 40}, {"type": "text", "text": "3. \\*\\* Adjust Negative Growth Rate \\*\\*: The optimized parameter \u2019 alpha \u2019 f o r the prey growth r a t e has a n e g a t i v e value , which i s b i o l o g i c a l l y u n r e a l i s t i c as i t would imply the prey p o p u l a t i o n d e c r e a s e s in the absence of p r e d a t o r s . Ensure t h a t the growth r a t e parameters are c o n s t r a i n e d to be p o s i t i v e during o p t i m i z a t i o n . ", "page_idx": 40}, {"type": "text", "text": "4. \\*\\*Review Competition C o e f f i c i e n t s \\*\\*: The optimized parameters \u2019 sigma \u2019 and \u2019 eta \u2019 f o r c o m p e t i t i o n are p o s i t i v e and negative , r e s p e c t i v e l y . While \u2019 sigma \u2019 being p o s i t i v e i s c o n s i s t e n t with competition , \u2019 eta \u2019 being n e g a t i v e i s not t y p i c a l f o r a c o m p e t i t i o n c o e f f i c i e n t . Re\u2212e v a l u a t e the c o m p e t i t i o n terms in the e q u a t i o n s to ensure they are b i o l o g i c a l l y meaningful and c o n s t r a i n the c o e f f i c i e n t s to be non\u2212n e g a t i v e during o p t i m i z a t i o n .   \n5. \\*\\* I n t r o d u c e Density \u2212Dependent M o r t a l i t y f o r Top P r e d a t o r s \\*\\*: The model c u r r e n t l y does not i n c l u d e a term f o r density \u2212dependent m o r t a l i t y of top p r e d a t o r s . Add a term to the d i f f e r e n t i a l eq u at i on f o r top p r e d a t o r s t h a t accounts f o r i n t r a s p e c i f i c c o m p e t i t i o n or o t h e r density \u2212dependent m o r t a l i t y f a c t o r s .   \n6. \\*\\* Parameter I n i t i a l i z a t i o n Based on Optimized Values \\*\\*: Use the optimized parameter values from i t e r a t i o n 1 as i n i t i a l values f o r the parameters in i t e r a t i o n 2. This can provide a b e t t e r s t a r t i n g p o i n t f o r the o p t i m i z a t i o n p rocess .   \n7. \\*\\* Consider Time Delays \\*\\*: In r e a l ecosystems , t h e r e can be time delays between p r e d a t i o n and changes in p o p u l a t i o n s i z e s due to f a c t o r s l i k e g e s t a t i o n or m a t u r a t i o n p e r i o d s . I f the v a l i d a t i o n l o s s does not improve s i g n i f i c a n t l y with the above changes , c o n s i d e r adding time delay terms to the model .   \n8. \\*\\* Hybrid White and Black Box Model \\*\\*: I f the v a l i d a t i o n l o s s does not improve below the t h r e s h o l d of 1e\u22126 with the above white box model improvements , c o n s i d e r i n t r o d u c i n g a black box component , such as a multi \u2212l a y e r perceptron , to c a p t u r e the r e s i d u a l s of the white box model . This hybrid approach can p o t e n t i a l l y model complex i n t e r a c t i o n s t h a t are not c a p tu r e d by the white box d i f f e r e n t i a l e q u a t i o n s .   \nBy implementing t h e s e changes , the model should become more b i o l o g i c a l l y r e a l i s t i c and may f i t the d a t a s e t more a c c u r a t e l y , p o t e n t i a l l y a c h i e v i n g a lower v a l i d a t i o n l o s s .   \nMainProcess | 2024\u221201\u221229 00:02:01 ,082 ,82 m u l t i p r o c e s s i n g INFO [ User ]   \nPlease now r e g e n e r a t e the code function , with the aim to improve the code to achieve a lower v a l i d a t i o n e r r o r . Use the feedback where a p p l i c a b l e . You are g e n e r a t i n g code f o r i t e r a t i o n 2 out of 20 t o t a l i t e r a t i o n s . When g e n e r a t i n g code i f you are unsure about something , take your b e s t guess . You have to g e n e r a t e code , and cannot give an empty s t r i n g answer .   \nPlease always only f i l l in the f o l l o w i n g code s k e l e t o n : \u2018 \u2018 \u2018   \nc l a s s S t a t e D i f f e r e n t i a l ( nn . Module ) : def _ _ i n i t _ _ ( s e l f ) : super ( S t a t e D i f f e r e n t i a l , s e l f ) . _ _ i n i t _ _ ( ) # TODO: F i l l in the code here ", "page_idx": 41}, {"type": "text", "text": "def forward ( s e l f , p r e y _ p o p u l a t i o n : t o r c h . Tensor , i n t e r m e d i a t e _ p o p u l a t i o n $\\boldsymbol{:}$ t o r c h . Tensor , t o p _ p r e d a t o r s _ p o p u l a t i o n : t o r c h . Tensor ) \u2212> Tuple [ t o r c h . Tensor , t o r c h . Tensor , t o r c h . Tensor ] : # TODO: F i l l in the code here r e t u r n ( d_prey_population__dt , d _ i n t e r m e d i a t e _ p o p u l a t i o n _ _ d t , d _ t o p _ p r e d a t o r s _ p o p u l a t i o n _ _ d t ) ", "page_idx": 41}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: The abstract and introduction clearly state the main claims, which are backed up by empirical evidence. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 42}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Justification: The main limitations of the work are discussed in Section 8, including the assumption of semantic priors and types of system studied. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 42}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: This paper does not introduce any theoretical results. Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 43}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: In Section 6 and Appendices $\\textrm{C}$ to G, we thoroughly outline the experimental procedure. This, combined with the released code, will enable all results in the paper to be reproduced. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 43}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: We provide code at https://github.com/samholt/HDTwinGen. Additionally, the datasets and preprocessing procedure are described in Appendix C. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 44}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Justification: Appendices $\\textrm{C}$ to $\\mathrm{G}$ details all training and test details, including data splits and hyperparameter tuning procedures for all baselines. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 44}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Justification: We report results over 10 seeded runs for all methods, reporting mean and $95\\%$ confidence interval of results. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 44}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 45}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: In Appendix G, we report all computer resources required to reproduce the results. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 45}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: The authors have read the Code of Ethics and confirm that the paper conforms to the code. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 45}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] Justification: Section 8 discusses the societal impacts of the work. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 45}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 46}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 46}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 46}, {"type": "text", "text": "Justification: This paper does not release any pretrained models or collated datasets that might pose a risk to misuse. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 46}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: The paper cites original owners of all code (for baselines) and datasets used in the paper. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 46}, {"type": "text", "text": "", "page_idx": 47}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: This paper does not release any new assets. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 47}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: This work does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 47}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 47}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 48}]