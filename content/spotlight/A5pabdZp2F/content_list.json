[{"type": "text", "text": "MultiOOD: Scaling Out-of-Distribution Detection for Multiple Modalities ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Hao Dong1 Yue Zhao2 Eleni Chatzi1 Olga Fink3 ", "page_idx": 0}, {"type": "text", "text": "1ETH Z\u00fcrich 2University of Southern California 3EPFL {hao.dong, chatzi}@ibk.baug.ethz.ch, yzhao010@usc.edu, olga.fink@epfl.ch ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Detecting out-of-distribution (OOD) samples is important for deploying machine learning models in safety-critical applications such as autonomous driving and robot-assisted surgery. Existing research has mainly focused on unimodal scenarios on image data. However, real-world applications are inherently multimodal, which makes it essential to leverage information from multiple modalities to enhance the efficacy of OOD detection. To establish a foundation for more realistic Multimodal OOD Detection, we introduce the first-of-its-kind benchmark, MultiOOD, characterized by diverse dataset sizes and varying modality combinations. We first evaluate existing unimodal OOD detection algorithms on MultiOOD, observing that the mere inclusion of additional modalities yields substantial improvements. This underscores the importance of utilizing multiple modalities for OOD detection. Based on the observation of Modality Prediction Discrepancy between in-distribution (ID) and OOD data, and its strong correlation with OOD performance, we propose the Agree-to-Disagree $(A2D)$ algorithm to encourage such discrepancy during training. Moreover, we introduce a novel outlier synthesis method, $N P{-}M i x$ , which explores broader feature spaces by leveraging the information from nearest neighbor classes and complements $A2D$ to strengthen OOD detection performance. Extensive experiments on MultiOOD demonstrate that training with $A2D$ and $N P{-}M i x$ improves existing OOD detection algorithms by a large margin. To support accessibility and reproducibility, our source code and MultiOOD benchmark are available at https://github.com/donghao51/MultiOOD. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Most existing machine learning (ML) models are trained under the closed-world assumption, where the test data is assumed to be drawn i.i.d. from the same distribution as the training data, referred to as in-distribution (ID). However, in open-world scenarios, test samples can be out-of-distribution (OOD), thus impacting model robustness and safety [67]. OOD detection aims to detect samples with semantic shifts that are undesirable for the model to generalize [67] and is critical for deploying ML models in safety-critical domains such as autonomous driving [21], robotics [18, 4], and diagnostics for critical assets [24]. Numerous OOD detection algorithms have been developed, ranging from classification-based to distance-based methods [66]. Classification-based methods typically derive confidence directly from the classifier, employing post-hoc processing techniques such as Maximum Softmax Probability (MSP) [31] and Energy [43] or training strategies such as logit normalization [65] and outlier synthesis [22]. Distance-based methods typically measure distances in high-dimensional feature spaces to distinguish between ID and OOD [41, 59]. Additionally, other methodologies explore density estimation [1, 50] and reconstruction techniques [70] for OOD detection. ", "page_idx": 0}, {"type": "text", "text": "Current research in OOD detection has predominantly focused on unimodal settings, often involving images as inputs [66]. While several recent works [47, 63] have investigated vision-language models [53] to enhance OOD performance, their evaluations are still limited to benchmarks containing solely images. Consequently, existing methods fall short in fully leveraging the complementary information from various modalities, such as LiDAR and camera in autonomous driving [21], as well as video, audio, and optical flow in action recognition [56]. To underscore the importance of using multiple modalities in OOD detection, we evaluate representative OOD algorithms across various modalities on the HMDB51 [39] dataset within our MultiOOD benchmark. This is an action recognition task and all models are trained solely using cross-entropy loss between a one-hot target vector and the softmax output. Results in Fig. 1 show that even a simple fusion of video and optical flow modalities can substantially enhance OOD detection performance. ", "page_idx": 0}, {"type": "image", "img_path": "A5pabdZp2F/tmp/9b37791630fbf5cf4f05721201950b56a3c2de2342f663b48aa0971a6ec4848c.jpg", "img_caption": ["Figure 1: The FPR95 (lower is better) and AUROC (higher is better) on HMDB51 dataset across various modalities. Multimodal OOD substantially improves unimodal OOD w/o bells and whistles. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To establish a foundation for more realistic Multimodal OOD Detection, we introduce a novel OOD benchmark named MultiOOD (Fig. 2), which is the first benchmark for Multimodal OOD Detection and covers diverse dataset sizes and modalities. MultiOOD comprises five video datasets with over 85, 000 video clips in total. The datasets vary in the number of classes, ranging from 7 to 229, and in size, spanning from $3k$ to $57k$ . Video, optical flow, and audio are used as different types of modalities. While most existing unimodal OOD algorithms designed for images can be directly applied to Multimodal OOD Detection, such approaches may yield suboptimal results without accounting for the interaction and complementary nature of diverse modalities. As depicted in Fig. 1, the AUROC performance is very close for all unimodal baselines, underscoring the importance of developing OOD detection algorithms tailored to effectively exploit information from multiple modalities. ", "page_idx": 1}, {"type": "text", "text": "In this work, we first identify and illustrate the Modality Prediction Discrepancy phenomenon on the MultiOOD benchmark, where the discrepancies of softmax predictions across different modalities are shown to be negligible for ID data while significant for OOD data (Fig. 3). We discover a strong correlation between such discrepancies and the OOD detection performance (Fig. 4). Motivated by these observations, we introduce the Agree-to-Disagree (A2D) algorithm, which aims to enhance such discrepancies during training. The algorithm is designed so that different modalities should Agree on the prediction of the ground-truth class, and Disagree on other classes by maximizing the distance between their predictions. Additionally, we propose a novel outlier synthesis algorithm named NP-Mix, designed to use the information from nearest neighbor classes to explore broader feature spaces and complement $A2D$ to strengthen the OOD detection performance. ", "page_idx": 1}, {"type": "text", "text": "Extensive experiments on the MultiOOD benchmark demonstrate the superiority of $A2D$ and NP-Mix. The integration of $A2D$ and $N P{-}M i x$ yields substantial performance enhancements over existing unimodal OOD detection algorithms. For instance, on the UCF101 [57] dataset within MultiOOD, our approach reduces the FPR95 from $32.14\\%$ to $10.68\\%$ for ASH [16] method, representing a noteworthy absolute $21.46\\%$ improvement over the baseline. Our contributions include: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We highlight the significance of integrating more modalities for OOD detection and introduce MultiOOD, the first benchmark for Multimodal OOD Detection encompassing diverse dataset sizes and various combinations of modalities.   \n\u2022 We conduct comprehensive evaluations of existing unimodal OOD algorithms on MultiOOD, revealing their limitations in multimodal scenarios.   \n\u2022 We propose a novel $A2D$ training algorithm, inspired by the observation of the Modality Prediction Discrepancy phenomenon, alongside a new outlier synthesis algorithm NP-Mix that explores broader feature spaces and complements $A2D$ to strengthen the OOD detection performance.   \n\u2022 Extensive experiments conducted on MultiOOD underscore the effectiveness of $A2D$ and $N P{-}M i x$ . Our source code and MultiOOD benchmark will be made publicly available, facilitating future research endeavors in Multimodal OOD Detection. ", "page_idx": 1}, {"type": "image", "img_path": "A5pabdZp2F/tmp/3c4d4bdd9257f4d8190bd6c668819ad9778a4a642a5fe5744ca480be3b1c9d29.jpg", "img_caption": ["Figure 2: An overview of MultiOOD Benchmark. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "2 Preliminaries: Multimodal Out-of-distribution Detection ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Multimodal OOD Detection aims to detect samples with semantic shifts using multiple modalities. We consider a training set $\\mathbb{D}_{i n}=\\{(\\mathbf{x}_{i},y_{i})\\}_{i=1}^{n}$ drawn i.i.d. from the joint data distribution $P_{X\\mathcal{Y}}$ , where $\\mathcal{X}$ is the input space and $\\mathcal{V}=\\{1,2,...,C\\}$ is the discrete label space. Each training sample $\\mathbf{x}_{i}$ is comprised of $M$ modalities, denoted as $\\dot{\\mathbf{x}}_{i}\\,=\\,\\{x_{i}^{k}\\ |\\ \\boldsymbol{k}\\,=\\,1,\\cdots,M\\}$ . Let ${\\mathcal{P}}_{\\mathrm{in}}$ denote the marginal distribution on $\\mathcal{X}$ and $f:\\mathcal{X}\\mapsto\\mathbb{R}^{C}$ be a neural network trained on samples in $P_{X\\mathcal{Y}}$ that predicts the label of each input sample. The $f$ in Multimodal OOD Detection comprises of $M$ feature extractors $g_{k}(\\cdot)$ and a classifier $h(\\cdot)$ . Each feature extractor $g_{k}(\\cdot)$ extracts an embedding $\\mathbf{Z}^{k}$ for its corresponding modality $k$ , and the classifier $h(\\cdot)$ takes the combined embeddings from all modalities as input and outputs a prediction probability $\\hat{p}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{p}=\\delta(f(\\mathbf{x}))=\\delta(h([g_{1}(x^{1}),...,g_{M}(x^{M})]))=\\delta(h([\\mathbf{Z}^{1},...,\\mathbf{Z}^{M}])),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\delta(\\cdot)$ is the softmax function. We further include a separate classifier $h_{k}(\\cdot)$ for each modality $k$ to get predictions from each modality separately, with the prediction probability from the $k$ -th modality as ${\\hat{p}}^{k}=\\delta(h_{k}(g_{k}(x^{k})))$ . ", "page_idx": 2}, {"type": "text", "text": "When deploying $f$ in the real world, it should not only accurately classify known samples as ID, but also identify any \u201cunknown\u201d sample as OOD. A separate score function $S(\\mathbf{x})$ is often used to decide whether a sample $\\mathbf{x}\\in\\mathcal{X}$ is from $\\mathcal{P}_{\\mathrm{in}}$ (ID) or not (OOD): ", "page_idx": 2}, {"type": "equation", "text": "$$\nG_{\\eta}(x)=\\left\\{\\!\\!\\begin{array}{l l}{{\\mathrm{ID}}}&{{S(\\mathbf{x})\\geq\\eta}}\\\\ {{\\mathrm{OOD}}}&{{S(\\mathbf{x})<\\eta}}\\end{array}\\!\\!,\\right.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where samples with higher scores $S(\\mathbf{x})$ are classified as ID and vice versa, $\\eta$ is the threshold. Existing OOD detection studies predominantly focus on unimodal scenarios, with a detailed literature review offered in Appendix A. To establish a foundation for more realistic Multimodal OOD Detection, we introduce a novel MultiOOD benchmark (Sec. 3) and propose an effective multimodal training strategy (Sec. 4) that yields significant enhancements over existing unimodal approaches. ", "page_idx": 2}, {"type": "text", "text": "3 MultiOOD Benchmark ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We create the MultiOOD benchmark to understand the existing gap in Multimodal OOD Detection research. OOD detection primarily focuses on detecting semantic shifts, with two main approaches used for constructing OOD benchmarks. A common method involves considering an entire dataset as in-distribution (ID) and further collects datasets, which comprise similar tasks but are disconnected from any ID categories, as OOD datasets. In this scenario, both semantic and domain shifts are present between the ID and OOD samples. We term this setup as Far-OOD in our benchmark. Another approach is to partition the categories of existing datasets into two subsets, referred to as closed (ID) and open set (OOD). Here, both ID and OOD samples originate from the same distribution, with only semantic shifts existing between them. We denote this setup as Near-OOD within our benchmark; ", "page_idx": 2}, {"type": "image", "img_path": "A5pabdZp2F/tmp/bf3a75aa3836aba4c6f3a79ba1ad0798a6e6c24944e2434faf5d3061bceafb33.jpg", "img_caption": ["Figure 3: An example of softmax outputs for ID and OOD data. The predictions from video and optical flow demonstrate uniformity across ID data and exhibit variability across OOD data. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "a setup that poses greater challenges compared to Far-OOD. This setup is also referred to as open set recognition (OSR) in some studies [61, 37, 9]. Notably, OSR and OOD detection both share the same goal of identifying test samples with semantic shifts without compromising the accuracy of ID classification [67]. In our benchmark, we treat OSR and OOD as synonymous concepts and adopt OOD as the general term. MultiOOD comprises five action recognition datasets (EPIC-Kitchens [48], HAC [20], HMDB51 [39], UCF101 [57], and Kinetics-600 [6]) with over 85, 000 video clips in total. The datasets vary in the number of classes, ranging from 7 to 229, and in size, spanning from $3k$ to $57k$ . Video, optical flow, and audio are used as different types of modalities. An overview of the MultiOOD benchmark is provided in Fig. 2, with additional details available in Appendix B. ", "page_idx": 3}, {"type": "text", "text": "3.1 Multimodal Near-OOD Benchmark ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In the Near-OOD setup, we include four datasets. EPIC-Kitchens 4/4 is derived from the EPICKitchens Domain Adaptation dataset [48], where the dataset is partitioned into four classes for training as ID and four classes for testing as OOD, with a total of 4, 871 video clips. Similarly, HMDB51 25/26 and UCF101 50/51 are constructed based on HMDB51 [39] and UCF101 [57], with a total of 6, 766 and 13, 320 video clips respectively. In the case of Kinetics-600 129/100, we select 229 action classes from the Kinetics-600 dataset [6], with each class comprising approximately 250 video clips and a total of 57, 205 video clips. Within this setup, 129 classes are designated for training as ID, while the remaining 100 classes are allocated for testing as OOD. ", "page_idx": 3}, {"type": "text", "text": "3.2 Multimodal Far-OOD Benchmark ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In the Far-OOD setup, we include HMDB51 and Kinetics-600 as ID datasets. ", "page_idx": 3}, {"type": "text", "text": "HMDB51 dataset as ID. For the OOD datasets, we utilize UCF101, EPIC-Kitchens, HAC, and Kinetics-600 datasets. All of these datasets are carefully curated to remove samples that belong to ID classes in HMDB51. Given the relatively small number of classes in the EPIC-Kitchens and HAC datasets, we remove 8 classes in the HMDB51 dataset that overlap with EPIC-Kitchens and HAC, with 43 classes left as ID classes. For UCF101, we remove 31 overlapping classes with HMDB51, resulting in 70 classes designated as OOD classes for evaluation. For other datasets, no class overlap exists and we utilize their original classes as OOD. ", "page_idx": 3}, {"type": "text", "text": "Kinetics-600 dataset as ID. Similarly, we adopt UCF101, EPIC-Kitchens, HAC, and HMDB51 datasets as OOD datasets, with careful selection undertaken to exclude samples belonging to ID classes in Kinetics-600. We carefully selected a subset of 229 action classes from Kinetics-600 in the Near-OOD setup to mitigate the potential overlap with other datasets. For the UCF101 dataset, we remove 11 overlapping classes with Kinetics-600, leaving 90 classes as OOD classes for evaluation. For other datasets, there are no class overlap issues and we use their original classes as OOD. ", "page_idx": 3}, {"type": "text", "text": "4 Methodology ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we first identify the Modality Prediction Discrepancy phenomenon on the MultiOOD benchmark and demonstrate its substantial correlation with the OOD detection performance (Sec. 4.1). Subsequently, we introduce the Agree-to-Disagree (A2D) algorithm aimed at enhancing such discrep", "page_idx": 3}, {"type": "image", "img_path": "A5pabdZp2F/tmp/859ff4a8f5a3218a62ae8df93e48ae3ba7795a34ca478f9a231a2499f86a833a.jpg", "img_caption": ["Figure 4: Average prediction $L_{1}$ distances between ID and OOD data $(l_{O O D}-l_{I D})$ before and after $A2D$ training across various datasets within the MultiOOD, where Energy [43] is used as score function. The distances are highly correlated to the ultimate OOD performance. $A2D$ training amplifies such distances, consequently enhancing the efficacy of OOD detection. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "ancies during training (Sec. 4.2). Finally, we propose the novel outlier synthesis algorithm named NP-Mix that complements $A2D$ to further strengthen the OOD detection performance (Sec. 4.3). ", "page_idx": 4}, {"type": "text", "text": "4.1 Modality Prediction Discrepancy ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We first explore the predictive behaviors demonstrated by various modalities when using both ID and OOD data as input on MultiOOD. We compute the prediction probabilities employing classifiers for video and optical flow on the same ID and OOD samples and calculate their $L_{1}$ distances. As depicted in Fig. 3, for ID data, the prediction probabilities of both video $\\hat{p}^{1}$ and optical flow $\\hat{p}^{2}$ are generally exhibited consistent with each other on the ground-truth label, consequently yielding a small $L_{1}$ distance $\\lVert\\hat{p}^{1}-\\hat{p}^{2}\\rVert_{1}$ between them. Conversely, for OOD data, each modality tends to express varying confidence preferences towards distinct classes, resulting in a notable increase in the $L_{1}$ distance between their predictions. We refer to this phenomenon as Modality Prediction Discrepancy between ID and OOD data. This discrepancy can be attributed to the unavailability of semantic information on OOD data during model training, stimulating each modality to generate conjectures based on its unique characteristics upon encountering OOD data during testing. ", "page_idx": 4}, {"type": "text", "text": "To verify the universality of this phenomenon, we calculate the average $L_{1}$ distance between predictions of video $\\hat{p}^{1}$ and optical flow $\\hat{p}^{2}$ on both ID and OOD data within the HMDB51 dataset. The average prediction $L_{1}$ distance is 0.63 for ID data $(l_{I D})$ and 1.42 for OOD data $(l_{O O D})$ , revealing a substantial discrepancy. In addition, we evaluate other datasets in the Near-OOD setup within the MultiOOD benchmark and observe similar discrepancies $(l_{O O D}-l_{I D})$ . Such discrepancies have a positive correlation with the OOD detection performance, as illustrated in Fig. 4. ", "page_idx": 4}, {"type": "text", "text": "4.2 Agree-to-Disagree Algorithm ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Motivated by the Modality Prediction Discrepancy and its strong correlation with Multimodal OOD Detection performance, we introduce the Agree-to-Disagree $(A2D)$ algorithm to foster the amplification of such discrepancies during training. The underlying idea is that different modalities should Agree on the prediction regarding the ground-truth class, while they should Disagree on the remaining classes by maximizing their prediction distance. $A2D$ enables the model to diversify predictions across modalities, consequently yielding high prediction discrepancies for OOD data during testing. ", "page_idx": 4}, {"type": "text", "text": "Given a training sample $\\mathbf{x}$ with label $c$ , we obtain prediction probabilities $\\hat{p}$ from the combined embeddings of all modalities, and $\\hat{p}^{1},\\hat{p}^{2}$ from individual modality, all of which are of shape $[1,C]$ , where $C$ represents the number of classes. By removing the $c$ -th value from $\\hat{p}^{1}$ and $\\hat{p}^{2}$ (different modalities should Agree on the prediction regarding the ground-truth class), we derive new prediction probabilities without ground-truth classes, denoted as $\\bar{p}^{1}$ and $\\bar{p}^{2}$ with shapes $[1,C-1]$ . Subsequently, we aim to maximize the discrepancy between $\\bar{p}^{1}$ and $\\bar{p}^{2}$ , which can be defined as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{D i s c r}=-D i s c r(\\bar{p}^{1},\\bar{p}^{2}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "image", "img_path": "A5pabdZp2F/tmp/56e4f15ada097ab2cbd8fec6e21b5787d8555b68c95bc1c6f20c80bf52a5d6d0.jpg", "img_caption": ["Figure 5: An overview of the proposed framework for Multimodal OOD Detection. We introduce $A2D$ algorithm to encourage enlarging the prediction discrepancy across modalities. Additionally, we propose a novel outlier synthesis algorithm, NP-Mix, designed to explore broader feature spaces, which complements $A2D$ to strengthen the OOD detection performance. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "where $D i s c r(\\cdot)$ is a distance metric quantifying the similarity between two probability distributions. We utilize the Hellinger distance [49] and explore the efficacy of alternative distance metrics in our ablation study. The Hellinger distance between two probability distributions is defined as: ", "page_idx": 5}, {"type": "equation", "text": "$$\nD(\\bar{p}^{1},\\bar{p}^{2})=\\sqrt{\\frac{1}{2}\\sum_{i=1}^{C-1}\\left(\\sqrt{\\bar{p}_{i}^{1}}-\\sqrt{\\bar{p}_{i}^{2}}\\right)^{2}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Furthermore, to ensure that the ground-truth classes possess the highest probabilities, we incorporate the cross-entropy loss $C E(\\cdot)$ for each prediction, defined as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{c l s}=\\frac{1}{3}(C E(\\hat{p},y)+C E(\\hat{p}^{1},y)+C E(\\hat{p}^{2},y)).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The final loss is obtained as the weighted sum of the previously defined losses: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\mathcal{L}_{c l s}+\\gamma\\mathcal{L}_{D i s c r},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the hyperparameter $\\gamma$ controls the relative importance of the discrepancy term. ", "page_idx": 5}, {"type": "text", "text": "4.3 Nearest Neighbor Prototype-based Mixup for Outlier Synthesis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Outlier synthesis [22, 60] has demonstrated its efficacy in OOD detection by imposing regularization on the model\u2019s decision boundary during training. Introducing the discrepancy loss in $A2D$ to the synthesized outlier data for model regularization has the potential to further enhance OOD detection performance. However, existing outlier synthesis methods [22, 60] typically generate outliers near the ID data (Fig. 7), neglecting to explore the broader embedding spaces, thereby potentially leading to suboptimal performance. Inspired by the recent approach introduced in [19], we introduce a novel algorithm termed Nearest Neighbor Prototype-based Mixup (NP-Mix), aimed at synthesizing outliers capable of spanning wider embedding spaces by leveraging the information from nearest neighbor classes, as shown in Fig. 5 and Fig. 7. To synthesize outliers, we concatenate the embeddings of all modalities $(\\mathbf{Z}=[\\mathbf{Z}^{1},\\bar{\\mathbf{Z}^{2}}])$ and treat them as a unified entity. Subsequently, we compute a prototype embedding $\\hat{\\mathbf{Z}}_{\\mathbf{c}}$ for each class $c$ by calculating the mean of all embeddings within that class. For each prototype embedding $\\hat{\\mathbf{Z}}_{\\mathbf{c}}$ , we identify its top $N$ nearest neighbor prototypes and randomly select one prototype $\\hat{\\mathbf{Z}}_{\\mathbf{s}}$ from them for mixing. Two samples, $\\mathbf{Z_{1}}$ and $\\mathbf{Z_{2}}$ , are randomly chosen from class $c$ and class $s$ respectively. The outlierZ is generated by their convex combination: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widetilde{\\bf Z}=\\lambda{\\bf Z_{1}}+(1-\\lambda){\\bf Z_{2}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "table", "img_path": "A5pabdZp2F/tmp/16af105bb06403e71749154b2f3b0a08b637a63bd71faae43ba5e7d72c6357b2.jpg", "table_caption": ["Table 1: Multimodal Near-OOD Detection using video and optical flow. $\\uparrow$ indicates larger values are better and vice versa. Training with $A2D$ and NP-Mix yields considerable performance enhancements. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "where $\\lambda\\,\\sim\\,\\mathrm{Beta}(\\alpha,\\alpha)$ , for $\\alpha\\,\\in\\,(0,\\infty)$ . In our experiments, we opt for $\\alpha\\,>\\,1$ to ensure the synthesized outliers reside in the intermediary space between two prototypes, rather than near the ID data. We then partition $\\widetilde{\\mathbf Z}$ into different modalities as $\\widetilde{\\mathbf{Z}}=[\\widetilde{\\mathbf{Z}}^{1},\\widetilde{\\widetilde{\\mathbf{Z}}}^{2}]$ , where ${\\widetilde{\\mathbf{Z}}}^{1}$ and ${\\widetilde{\\mathbf{Z}}}^{2}$ represent the synthesized outlier embeddings for each modality. Subsequently, the corresponding prediction probabilities are computed as $\\widetilde{p}^{1}=\\delta(h_{1}(\\widetilde{\\mathbf{Z}}^{1}))$ and $\\widetilde{p}^{\\widetilde{2}}=\\delta(h_{2}(\\widetilde{\\mathbf{Z}}^{2}))$ . ", "page_idx": 6}, {"type": "text", "text": "For synthesized outliers, we aim to maximize the prediction discrepancies between different modalities, similar to Eq. (2) for ID training samples. In this context, there is no need to remove any value from the prediction, as the outliers are assumed not to belong to any ID class. The discrepancy loss between $\\bar{\\widetilde{p}}^{1}$ and $\\widetilde{p}^{2}$ can be defined as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{D i s c r\\_s y n}=-D i s c r(\\widetilde{p}^{1},\\widetilde{p}^{2}).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Moreover, we seek to mitigate the overconfidence of predictions for synthesized outliers towards any existing ID class. Therefore, we maximize the entropy of predictions: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{E n t}=-\\frac{1}{2}(H(\\widetilde{p}^{1})+H(\\widetilde{p}^{2})),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $H(\\widetilde{p})$ is the entropy of prediction $\\widetilde{p}$ and can be calculated as $\\begin{array}{r}{H(\\widetilde{p})\\,=\\,-\\sum_{c}\\widetilde{p}_{c}\\log\\widetilde{p}_{c}}\\end{array}$ . The final loss is  obtained as the weighted sum   of the previously defined losses : ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\mathcal{L}_{c l s}+\\gamma(\\mathcal{L}_{D i s c r}+\\mathcal{L}_{D i s c r\\_s y n}+\\mathcal{L}_{E n t}).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.1 Experimental Setting ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Baselines. Our objective is to enhance existing unimodal OOD algorithms through multimodal training utilizing the proposed $A2D$ and NP-Mix. To validate the efficacy and versatility of our proposed approaches, we include representative algorithms that leverage information from the probability space (MSP [31], GEN [44]), logit space (Energy [43], MaxLogit [30]), feature space (Mahalanobis [41], KNN [59]), both logit and feature spaces (VIM [62]), penultimate activation manipulations (ReAct [58], ASH [16]), and training-time regularization (LogitNorm [65]). ", "page_idx": 6}, {"type": "text", "text": "Evaluation metrics. We evaluate the performance via the use of the following metrics: (1) the false positive rate (FPR95) of OOD samples when the true positive rate of ID samples is at $95\\%$ , (2) the area under the receiver operating characteristic curve (AUROC), and (3) ID classification accuracy (ID ACC). For each baseline algorithm, we report the results both with and without $A2D$ training and $N P{-}M i x$ outlier synthesis. Additional implementation details are provided in Appendix D. ", "page_idx": 6}, {"type": "table", "img_path": "A5pabdZp2F/tmp/e41829ea1e5ad909c67bddd09338ce1f3729eb62bb6a108d5e5adc5d06b6524e.jpg", "table_caption": ["Table 2: Multimodal Far-OOD Detection using video and optical flow, with HMDB51 as ID. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "A5pabdZp2F/tmp/ada7757150d4f44a23a12ac1f9ffdd3d1af77f28ed56201197a222a799f3af34.jpg", "table_caption": ["Table 3: Multimodal Far-OOD Detection using video and optical flow, with Kinetics-600 as ID. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "5.2 Multimodal Near-OOD Detection ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We commence our evaluation by assessing the efficacy of $A2D$ training and $N P{-}M i x$ outlier synthesis within the Multimodal Near-OOD Detection setup. As presented in Tab. 1, the simple incorporation of $A2D$ yields substantial enhancements in OOD performance across nearly all scenarios. The average prediction $L_{1}$ distances between ID and OOD data $(l_{O O D}-l_{I D})$ before and after $A2D$ training across various datasets are depicted in Fig. 4. Notably, across all datasets, $A2D$ training serves to further enlarge the Modality Prediction Discrepancy and consequently improve OOD detection performance, verifying the strong correlation between them. Specifically, $A2D$ training reduces the FPR95 by up to absolute $19.6\\bar{2}\\%$ on UCF101 50/51 dataset for ASH and increases AUROC up to $14.82\\%$ for Mahalanobis on Kinetics-600 129/100. Combining $A2D$ training with $N P{-}M i x$ yields additional improvements in OOD detection performance, underscoring the efficacy of outlier synthesis in model regularization. Additionally, $A2D$ training and $N P{-}M i x$ outlier synthesis exhibit notable efficacy across all baseline OOD detection algorithms despite their different underlying principles, indicating the versatility of our proposed method. ", "page_idx": 7}, {"type": "text", "text": "5.3 Multimodal Far-OOD Detection ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Tab. 2 and Tab. 3 present the results of Multimodal Far-OOD Detection with HMDB51 and Kinetics600 as ID datasets, respectively. Similar to the Near-OOD setup, training with $A2D$ and $N P{-}M i x$ yields considerable enhancements in OOD detection performance across most cases for all baseline algorithms. Specifically, training with both $A2D$ and NP-Mix achieves reductions in FPR95 of up to absolute $23.3\\bar{8}\\%$ on the HMDB51 dataset for ASH and up to $14.43\\%$ for ReAct on the Kinetics-600 dataset. Due to space limits, we provide comprehensive results in Appendix E (Tab. 7 and Tab. 8). Interestingly, we observe that the performance on the HMDB51 dataset generally surpasses that of the Kinetics-600 dataset. This finding aligns with observations from unimodal OOD detection benchmarks [66], where performance on datasets with fewer classes (e.g., CIFAR-10 [38]) tends to outperform those on datasets with a greater number of classes (e.g., ImageNet [14]). ", "page_idx": 7}, {"type": "table", "img_path": "A5pabdZp2F/tmp/bad111db07fb7125a122cb99c73a96d9bc3cf86e8fafcc99e77440746efa50f3.jpg", "table_caption": ["Table 4: Multimodal Near-OOD Detection using video, audio, and optical flow. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "A5pabdZp2F/tmp/99507f4062faf2097d804cfa67f0cd3850f55ed6a5656810130b2834ba4f5b3d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "A5pabdZp2F/tmp/fe271eb7f29eb96d0111d514f7ec6f77541fa8a59a349497b8a9bb982586bdc1.jpg", "table_caption": ["Table 6: Ablation on Outlier Synthesis Methods for Near-OOD Detection on HMDB51 dataset. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.4 Ablation Studies ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Multimodal Near-OOD Detection with other modalities. We demonstrate the efficacy of $A2D$ training and NP-Mix outlier synthesis across various combinations of modalities, not limited to video and optical flow, as detailed in Tab. 4 and Tab. 9 in Appendix E. Notably, the performance of most algorithms is consistently improved with $A2D$ and $N P{-}M i x$ , regardless of which combinations of modalities are used. ", "page_idx": 8}, {"type": "text", "text": "Effectiveness of other distance functions. We substitute the distance metric for measuring probability distributions in the discrepancy loss with alternative distance functions, including $L_{1}$ , $L_{2}$ , and Wasserstein [2] distances. As depicted in Tab. 5, $A2D$ training exhibits robustness across various distance functions. Regardless of the specific distance metric employed, substantial improvements are consistently observed compared to the baseline approach without $A2D$ training. ", "page_idx": 8}, {"type": "text", "text": "Compared with other outlier synthesis methods. To evaluate the effectiveness of other outlier synthesis methods, we replace NP-Mix with VOS [22], NPOS [60], and Mixup [69] and train with $A2D$ . All baseline methods yield improvements in OOD performance, underscoring the significance of outlier synthesis for model regularization. Notably, $N P{-}M i x$ emerges as the most effective among the various baselines by synthesizing outliers that span broader embedding spaces. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this paper, we introduce the Multimodal Out-of-distribution Detection problem and present a novel benchmark, MultiOOD, which includes diverse dataset sizes and various combinations of modalities. Motivated by the Modality Prediction Discrepancy phenomenon observed within MultiOOD, we propose a novel $A2D$ training algorithm to encourage the enlargement of such discrepancy during model training, along with a new outlier synthesis algorithm, NP-Mix, that complements $A2D$ . Extensive experiments on MultiOOD and under Near-OOD and Far-OOD setups verify the efficacy of the proposed approaches. We hope our work will inspire future research endeavors in Multimodal OOD Detection. ", "page_idx": 8}, {"type": "text", "text": "Limitation and Future Work. The performance on Near-OOD benchmarks and on datasets with a large number of classes still exhibits potential for enhancement. Moreover, the exploration of Outlier Exposure [32] deserves attention as a potential to better learn ID/OOD discrepancy. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The authors acknowledge the support of \"In-service diagnostics of the catenary/pantograph and wheelset axle systems through intelligent algorithms\" (SENTINEL) project, supported by the ETH Mobility Initiative. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Abati, D., Porrello, A., Calderara, S., Cucchiara, R.: Latent space autoregression for novelty detection. In: CVPR (2019)   \n[2] Arjovsky, M., Chintala, S., Bottou, L.: Wasserstein generative adversarial networks. In: ICML (2017)   \n[3] Behley, J., Garbade, M., Milioto, A., Quenzel, J., Behnke, S., Stachniss, C., Gall, J.: SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences. In: ICCV (2019)   \n[4] Blum, H., Sarlin, P.E., Nieto, J., Siegwart, R., Cadena, C.: The fishyscapes benchmark: Measuring blind spots in semantic segmentation. International Journal of Computer Vision 129(11), 3119\u20133135 (2021)   \n[5] Breunig, M.M., Kriegel, H.P., Ng, R.T., Sander, J.: Lof: identifying density-based local outliers. In: Proceedings of the 2000 ACM SIGMOD international conference on Management of data. pp. 93\u2013104 (2000)   \n[6] Carreira, J., Noland, E., Banki-Horvath, A., Hillier, C., Zisserman, A.: A short note about kinetics-600. arXiv preprint arXiv:1808.01340 (2018)   \n[7] Carreira, J., Zisserman, A.: Quo vadis, action recognition? a new model and the kinetics dataset. In: CVPR (2017)   \n[8] Chan, R., Lis, K., Uhlemeyer, S., Blum, H., Honari, S., Siegwart, R., Fua, P., Salzmann, M., Rottmann, M.: Segmentmeifyoucan: A benchmark for anomaly segmentation. arXiv preprint arXiv:2104.14812 (2021)   \n[9] Chen, G., Peng, P., Wang, X., Tian, Y.: Adversarial reciprocal points learning for open set recognition. arXiv preprint arXiv:2103.00953 (2021)   \n[10] Chen, H., Xie, W., Vedaldi, A., Zisserman, A.: Vggsound: A large-scale audio-visual dataset. In: ICASSP (2020)   \n[11] Contributors, M.: Openmmlab\u2019s next generation video understanding toolbox and benchmark. https://github.com/open-mmlab/mmaction2 (2020)   \n[12] Cortinhal, T., Tzelepis, G., Aksoy, E.E.: Salsanext: Fast, uncertainty-aware semantic segmentation of lidar point clouds for autonomous driving. arXiv preprint arXiv:2003.03653 (2020)   \n[13] Damen, D., Doughty, H., Farinella, G.M., Fidler, S., Furnari, A., Kazakos, E., Moltisanti, D., Munro, J., Perrett, T., Price, W., Wray, M.: Scaling egocentric vision: The epic-kitchens dataset. In: ECCV (2018)   \n[14] Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierarchical image database. In: CVPR (2009)   \n[15] Deng, L.: The mnist database of handwritten digit images for machine learning research [best of the web]. IEEE signal processing magazine 29(6), 141\u2013142 (2012)   \n[16] Djurisic, A., Bozanic, N., Ashok, A., Liu, R.: Extremely simple activation shaping for out-ofdistribution detection. arXiv preprint arXiv:2209.09858 (2022)   \n[17] Dong, H., Chatzi, E., Fink, O.: Towards multimodal open-set domain generalization and adaptation through self-supervision. In: ECCV (2024)   \n[18] Dong, H., Chen, X., S\u00e4rkk\u00e4, S., Stachniss, C.: Online pole segmentation on range images for long-term lidar localization in urban environments. Robotics and Autonomous Systems 159, 104283 (2023)   \n[19] Dong, H., Frusque, G., Zhao, Y., Chatzi, E., Fink, O.: Nng-mix: Improving semi-supervised anomaly detection with pseudo-anomaly generation. arXiv preprint arXiv:2311.11961 (2023)   \n[20] Dong, H., Nejjar, I., Sun, H., Chatzi, E., Fink, O.: SimMMDG: A simple and effective framework for multi-modal domain generalization. In: NeurIPS (2023)   \n[21] Dong, H., Zhang, X., Xu, J., Ai, R., Gu, W., Lu, H., Kannala, J., Chen, X.: Superfusion: Multilevel lidar-camera fusion for long-range hd map generation. arXiv preprint arXiv:2211.15656 (2022)   \n[22] Du, X., Wang, Z., Cai, M., Li, Y.: Vos: Learning what you don\u2019t know by virtual outlier synthesis. In: ICLR (2022)   \n[23] Feichtenhofer, C., Fan, H., Malik, J., He, K.: Slowfast networks for video recognition. In: ICCV (2019)   \n[24] Fink, O., Wang, Q., Svens\u00e9n, M., Dersin, P., Lee, W.J., Ducoffe, M.: Potential, challenges and future directions for deep learning in prognostics and health management applications. Engineering Applications of Artificial Intelligence 92, 103678 (2020). https://doi.org/https://doi.org/10.1016/j.engappai.2020.103678   \n[25] Gong, Y., Chung, Y.A., Glass, J.: Ast: Audio spectrogram transformer. arXiv preprint arXiv:2104.01778 (2021)   \n[26] Gorishniy, Y., Rubachev, I., Khrulkov, V., Babenko, A.: Revisiting deep learning models for tabular data. NeurIPS (2021)   \n[27] Gupta, S., Hoffman, J., Malik, J.: Cross modal distillation for supervision transfer. In: CVPR (2016)   \n[28] Han, S., Hu, X., Huang, H., Jiang, M., Zhao, Y.: Adbench: Anomaly detection benchmark. In: Neural Information Processing Systems (NeurIPS) (2022)   \n[29] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR (2016)   \n[30] Hendrycks, D., Basart, S., Mazeika, M., Zou, A., Kwon, J., Mostajabi, M., Steinhardt, J., Song, D.: Scaling out-of-distribution detection for real-world settings. ICML (2022)   \n[31] Hendrycks, D., Gimpel, K.: A baseline for detecting misclassified and out-of-distribution examples in neural networks. In: ICLR (2017)   \n[32] Hendrycks, D., Mazeika, M., Dietterich, T.: Deep anomaly detection with outlier exposure. In: ICLR (2019)   \n[33] Jaritz, M., Vu, T.H., de Charette, R., Wirbel, E., P\u00e9rez, P.: xMUDA: Cross-modal unsupervised domain adaptation for 3D semantic segmentation. In: CVPR (2020)   \n[34] Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan, S., Viola, F., Green, T., Back, T., Natsev, P., et al.: The kinetics human action video dataset. arXiv preprint arXiv:1705.06950 (2017)   \n[35] Khosla, P., Teterwak, P., Wang, C., Sarna, A., Tian, Y., Isola, P., Maschinot, A., Liu, C., Krishnan, D.: Supervised contrastive learning. In: NeurIPS (2020)   \n[36] Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. ICLR (2015)   \n[37] Kong, S., Ramanan, D.: Opengan: Open-set recognition via open data generation. In: ICCV (2021)   \n[38] Krizhevsky, A., Nair, V., Hinton, G.: Cifar-10 and cifar-100 datasets. URl: https://www. cs. toronto. edu/kriz/cifar. html 6(1), 1 (2009)   \n[39] Kuehne, H., Jhuang, H., Garrote, E., Poggio, T., Serre, T.: Hmdb: a large video database for human motion recognition. In: ICCV (2011)   \n[40] LeCun, Y., Chopra, S., Hadsell, R., Ranzato, M., Huang, F.: A tutorial on energy-based learning. Predicting structured data 1(0) (2006)   \n[41] Lee, K., Lee, K., Lee, H., Shin, J.: A simple unified framework for detecting out-of-distribution samples and adversarial attacks. In: NeurIPS (2018)   \n[42] Liang, S., Li, Y., Srikant, R.: Enhancing the reliability of out-of-distribution image detection in neural networks. In: ICLR (2018)   \n[43] Liu, W., Wang, X., Owens, J.D., Li, Y.: Energy-based out-of-distribution detection. In: NeurIPS (2020)   \n[44] Liu, X., Lochman, Y., Zach, C.: Gen: Pushing the limits of softmax-based out-of-distribution detection. In: CVPR (2023)   \n[45] Liu, Z., Ning, J., Cao, Y., Wei, Y., Zhang, Z., Lin, S., Hu, H.: Video swin transformer. In: CVPR (2022)   \n[46] Van der Maaten, L., Hinton, G.: Visualizing data using t-sne. Journal of machine learning research 9(11) (2008)   \n[47] Ming, Y., Cai, Z., Gu, J., Sun, Y., Li, W., Li, Y.: Delving into out-of-distribution detection with vision-language representations. In: NeurIPS (2022)   \n[48] Munro, J., Damen, D.: Multi-modal domain adaptation for fine-grained action recognition. In: CVPR (2020)   \n[49] Pardo, L.: Statistical inference based on divergence measures. Chapman and Hall/CRC (2018)   \n[50] Pidhorskyi, S., Almohsen, R., Adjeroh, D.A., Doretto, G.: Generative probabilistic novelty detection with adversarial autoencoders. In: NeurIPS (2018)   \n[51] Planamente, M., Plizzari, C., Alberti, E., Caputo, B.: Domain generalization through audiovisual relative norm alignment in first person action recognition. In: WACV (2022)   \n[52] Prokhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A.V., Gulin, A.: Catboost: unbiased boosting with categorical features. NeurIPS (2018)   \n[53] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., Sutskever, I.: Learning transferable visual models from natural language supervision. In: ICML (2021)   \n[54] Ruff, L., Vandermeulen, R., Goernitz, N., Deecke, L., Siddiqui, S.A., Binder, A., M\u00fcller, E., Kloft, M.: Deep one-class classification. In: International conference on machine learning. pp. 4393\u20134402. PMLR (2018)   \n[55] Ruff, L., Vandermeulen, R.A., G\u00f6rnitz, N., Binder, A., M\u00fcller, E., M\u00fcller, K., Kloft, M.: Deep semi-supervised anomaly detection. In: ICLR (2020)   \n[56] Simonyan, K., Zisserman, A.: Two-stream convolutional networks for action recognition in videos. In: NeurIPS (2014)   \n[57] Soomro, K., Zamir, A.R., Shah, M.: Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402 (2012)   \n[58] Sun, Y., Guo, C., Li, Y.: React: Out-of-distribution detection with rectified activations. In: NeurIPS (2021)   \n[59] Sun, Y., Ming, Y., Zhu, X., Li, Y.: Out-of-distribution detection with deep nearest neighbors. ICML (2022)   \n[60] Tao, L., Du, X., Zhu, X., Li, Y.: Non-parametric outlier synthesis. arXiv preprint arXiv:2303.02966 (2023)   \n[61] Vaze, S., Han, K., Vedaldi, A., Zisserman, A.: Open-set recognition: A good closed-set classifier is all you need. In: ICLR (2022)   \n[62] Wang, H., Li, Z., Feng, L., Zhang, W.: Vim: Out-of-distribution with virtual-logit matching. In: CVPR (2022)   \n[63] Wang, H., Li, Y., Yao, H., Li, X.: Clipn for zero-shot ood detection: Teaching clip to say no. In: ICCV (2023)   \n[64] Wang, L., Xiong, Y., Wang, Z., Qiao, Y., Lin, D., Tang, X., Van Gool, L.: Temporal segment networks: Towards good practices for deep action recognition. In: ECCV (2016)   \n[65] Wei, H., Xie, R., Cheng, H., Feng, L., An, B., Li, Y.: Mitigating neural network overconfidence with logit normalization. In: ICML (2022)   \n[66] Yang, J., Wang, P., Zou, D., Zhou, Z., Ding, K., Peng, W., Wang, H., Chen, G., Li, B., Sun, Y., et al.: Openood: Benchmarking generalized out-of-distribution detection. In: NeurIPS (2022)   \n[67] Yang, J., Zhou, K., Li, Y., Liu, Z.: Generalized out-of-distribution detection: A survey. arXiv preprint arXiv:2110.11334 (2021)   \n[68] Zach, C., Pock, T., Bischof, H.: A duality based approach for realtime tv-l 1 optical flow. In: Pattern Recognition (2007)   \n[69] Zhang, H., Cisse, M., Dauphin, Y.N., Lopez-Paz, D.: mixup: Beyond empirical risk minimization. In: ICLR (2018)   \n[70] Zhou, Y.: Rethinking reconstruction autoencoder-based out-of-distribution detection. In: CVPR (2022)   \n[71] Zhou, Y., Song, X., Zhang, Y., Liu, F., Zhu, C., Liu, L.: Feature encoding with autoencoders for weakly supervised anomaly detection. IEEE Transactions on Neural Networks and Learning Systems 33(6), 2454\u20132465 (2021)   \n[72] Zhuang, Z., Li, R., Jia, K., Wang, Q., Li, Y., Tan, M.: Perception-aware multi-sensor fusion for 3d lidar semantic segmentation. In: ICCV (2021) ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Related Work ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Out-of-Distribution (OOD) Detection aims to detect test samples with semantic shift without losing the ID classification accuracy. Numerous OOD detection algorithms have been developed, with post hoc methods and training-time regularization as major categories [66]. Post hoc methods aim to design OOD scores based on the classification output of neural networks, offering the advantage of being easy to use without modifying the training procedure and objective. Early approaches include utilizing Maximum Softmax Probability (MSP) [31] as OOD score, often coupled with temperature scaling and input perturbation [42]. Instead of using softmax probabilities, MaxLogit [30] employs maximum logit as OOD scores rather than softmax. Energy-based algorithm [43] demonstrated the efficacy of energy function [40] in quantifying OOD-ness. Other approaches like ReAct [58] improved existing scoring functions by truncating the activations with high values. Similarly, ASH [16] prunes a large portion of the input activations and adjusts the remaining activations using pruning, binarizing, or scaling. Methods like Mahalanobis [41] and $k$ -Nearest Neighbor (KNN) [59] use distance metrics in feature space for OOD detection, while Virtual-logit Matching (VIM) [62] integrates information from both feature and logit spaces to define the OOD score. Recently, Generalized Entropy (GEN) [44] proposed an entropy-based score function that proves to be both simple and effective. ", "page_idx": 13}, {"type": "text", "text": "Training-time regularization methods such as LogitNorm [65] address prediction overconfidence by imposing a constant vector norm on the logits during training. Outlier Exposure [32] leverages external OOD samples from other datasets during training to facilitate the learning of better ID and OOD discrepancy. Additionally, some approaches [22, 60] proposed synthesizing virtual outliers for training-time regularization. However, all previous approaches were designed for unimodal scenarios, without accounting for the interaction and complementary nature of diverse modalities. ", "page_idx": 13}, {"type": "text", "text": "Multimodal OOD Detection. Recent endeavors [47, 63] have explored vision-language models [53] to enhance OOD performance, which are also referred to as multimodal in some of the studies. Maximum Concept Matching (MCM) [47] defines OOD score by aligning visual features with textual concepts. CLIPN [63] equips CLIP [53] with the capability of distinguishing OOD and ID samples using positive-semantic prompts and negation-semantic prompts. However, the evaluations of all these works are still limited to benchmarks exclusively containing images. Consequently, existing methods fall short in fully leveraging the complementary information from various modalities, such as LiDAR and camera in autonomous driving [21], as well as video, audio, and optical flow in action recognition [56]. There is also a lack of benchmark datasets that facilitate the evaluation of Multimodal OOD Detection. Therefore, we aim to develop a more practical and challenging benchmark incorporating multiple combinations of modalities (i.e., video, audio, and optical flow). This enables the creation of OOD detection algorithms that are specifically designed to leverage the complementary nature of various modalities effectively. ", "page_idx": 13}, {"type": "text", "text": "Anomaly Detection and Open Set Recognition are two closely related fields to OOD Detection. Anomaly Detection (AD) aims to detect patterns that deviate from the predefined normality during testing [67] and treats all in-distribution samples as a single class. Therefore, AD algorithms can be applied to OOD detection by ignoring all the labels for ID data. Typical AD algorithms include unsupervised [54, 5], semi-supervised [55, 71], and supervised [26, 52], depending on the availability of labels [28]. Open Set Recognition (OSR) [61, 37, 9, 17] focuses on accurately classifying test samples from \u201cknown known classes\u201d (ID) and detecting test samples from \u201cunknown unknown classes\u201d (OOD). While OOD detection benchmarks always take one dataset as ID and find several other datasets with non-overlapping categories as OOD, OSR benchmarks usually split one multiclass classification dataset into ID and OOD parts according to classes. However, both OSR and OOD detection share the same goal of identifying test samples with semantic shifts without compromising the accuracy of ID classification [67]. Therefore, in our benchmark, we treat OSR and OOD as synonymous concepts and adopt OOD as the general term. Our Near-OOD Benchmark is similar to traditional OSR setup and our Far-OOD Benchmark is the same as general OOD setup. ", "page_idx": 13}, {"type": "text", "text": "OOD Benchmarks. Early works [31] in OOD detection primarily focus on small-scale image datasets such as MNIST [15] and CIFAR-10/100 [38]. Recognizing the need for evaluating OOD detection at scale, studies such as [62] introduce new OOD datasets based on ImageNet [14]. Additionally, some OOD benchmarks [30, 8] are specifically designed for semantic segmentation tasks. OpenOOD [66] offers a comprehensive OOD benchmark comprising datasets from previous works and incorporating over 30 common OOD methods. However, all of these benchmarks are limited to image data. In contrast, our MultiOOD is the first public OOD benchmark that encompasses different combinations of modalities, facilitating future research endeavors in Multimodal OOD Detection. ", "page_idx": 13}, {"type": "image", "img_path": "A5pabdZp2F/tmp/36c7a79dbcf5514a86c17e1fe37c9fc5a794d6c7847da5b81f53fb5e1c28c604.jpg", "img_caption": ["Figure 6: Visualization of action recognition datasets used in our MultiOOD benchmark. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "B More Details on the MultiOOD Benchmark ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Datasets Used in MultiOOD Benchmark ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Action recognition is inherently multimodal and serves as the primary task within our benchmark, and we include five action recognition datasets accordingly of varying sizes, as shown in Fig. 6. ", "page_idx": 14}, {"type": "text", "text": "EPIC-Kitchens [13]. The EPIC-Kitchens dataset is a large-scale egocentric dataset collected by 32 participants in their native kitchen environments. The participants were asked to capture all their daily kitchen activities and record sequences regardless of their duration. The start and end times for each action are annotated. We use a subset of the EPIC-Kitchens dataset introduced in the Multimodal Domain Adaptation paper [48], which comprises 4, 871 video clips from 8 largest action classes in sequence P22. These actions include \u2018put\u2019, \u2018take\u2019, \u2018open\u2019, \u2018close\u2019, \u2018wash\u2019, \u2018cut\u2019, \u2018mix\u2019, and \u2018pour\u2019, with provided modalities including video, optical flow, and audio. ", "page_idx": 14}, {"type": "text", "text": "HAC [20]. The HAC dataset encompasses seven actions (\u2018sleeping\u2019, \u2018watching tv\u2019, \u2018eating\u2019, \u2018drinking\u2019, \u2018swimming\u2019, \u2018running\u2019, and \u2018opening door\u2019) performed by humans, animals, and cartoon figures. There are 3, 381 video clips in total from seven actions. Modalities provided in this dataset include video, optical flow, and audio. ", "page_idx": 14}, {"type": "text", "text": "HMDB51 [39]. The HMDB51 dataset is a video action recognition dataset, comprising 6, 766 video clips spanning 51 action categories. The video clips are extracted from a variety of sources ranging from digitized movies to YouTube. Available modalities in this dataset include video and optical flow. ", "page_idx": 14}, {"type": "text", "text": "UCF101 [57]. UCF101 is another video action recognition dataset collected from YouTube, comprising 13, 320 video clips from 101 actions. UCF101 offers substantial diversity in action types and encompasses significant variations in camera motion, object appearance and pose, object scale, viewpoint, cluttered background, illumination conditions, etc. Modalities provided in this dataset include video and optical flow. ", "page_idx": 14}, {"type": "text", "text": "Kinetics-600 [6]. Kinetics-600 is a large-scale action recognition dataset, featuring approximately $480k$ videos spanning 600 action categories. Each video in the dataset is a 10-second clip of action moment annotated from YouTube videos. In our benchmark, we carefully selected a subset of 229 action classes from Kinetics-600 to mitigate the potential category overlap with other datasets, with each class comprising roughly 250 video clips, yielding a total of 57, 205 video clips. The original dataset provides video and audio modalities. To make it consistent with the other dataset, our benchmark further provides the extracted optical flow for all video clips, amounting to a total of 114, 410 optical flow samples. The dense optical flow is extracted at 24 frames per second using the TV-L1 algorithm [68]. ", "page_idx": 14}, {"type": "text", "text": "B.2 Multimodal Near-OOD Benchmark ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In the Near-OOD setup, we incorporate four datasets. EPIC-Kitchens 4/4 is derived from the EPICKitchens Domain Adaptation dataset [48], where the dataset is randomly partitioned into four classes for training as ID and four classes for testing as OOD. Similarly, HMDB51 25/26 and UCF101 50/51 are constructed based on HMDB51 [39] and UCF101 [57], respectively. In the case of Kinetics-600 129/100, we curate 229 action classes from the Kinetics-600 dataset [6], with each class comprising approximately 250 video clips. Within this setup, 129 classes are randomly designated for training as ID, while the remaining 100 classes are allocated for testing as OOD. ", "page_idx": 14}, {"type": "image", "img_path": "A5pabdZp2F/tmp/2f1ac821daa09129287bcaaa3617f4fe86223f7b0a64557311b73fe6ec599b48.jpg", "img_caption": ["Figure 7: Visualization of synthesized outliers compared against other methods. VOS and NPOS tend to generate outliers near the ID data, neglecting to explore the broader embedding space. Mixup randomly selects samples from all classes to mix and inadvertently introduces unwanted noise samples within the distribution of ID data. $N P{-}M i x$ excels at generating synthesized outliers by effectively utilizing information from neighbor classes and spanning wider embedding spaces. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "B.3 Multimodal Far-OOD Benchmark ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In the Far-OOD setup, we incorporate HMDB51 and Kinetics-600 as ID datasets. ", "page_idx": 15}, {"type": "text", "text": "HMDB51 dataset as ID. For the OOD datasets, we utilize UCF101, EPIC-Kitchens, HAC, and Kinetics-600 datasets. All of these datasets are carefully curated to remove samples that should belong to ID classes. Given the relatively small number of classes in the EPIC-Kitchens and HAC datasets, we remove 8 classes in the HMDB51 dataset that overlap with EPIC-Kitchens and HAC, including \u2018chew\u2019, \u2019climb_stairs\u2019, \u2018drink\u2019, \u2018eat\u2019, \u2018pick\u2019, \u2018pour\u2019, \u2019ride_horse\u2019, \u2019run\u2019, leaving 43 classes as ID classes. In the case of UCF101, we remove 31 overlapping classes with HMDB51, resulting in 70 classes designated as OOD classes for evaluation. For Kinetics-600, we use the same subset of 229 classes as in the Near-OOD setup, which are carefully selected to mitigate the potential category overlap with other datasets. For other datasets, no class overlap exists and we utilize their original classes as OOD. ", "page_idx": 15}, {"type": "text", "text": "Kinetics-600 dataset as ID. Similarly, we adopt UCF101, EPIC-Kitchens, HAC, and HMDB51 datasets as OOD datasets, with careful curation undertaken to exclude samples belonging to ID classes. We carefully selected a subset of 229 action classes from Kinetics-600 in the Near-OOD setup to mitigate the potential overlap with other datasets. For the UCF101 dataset, we remove 11 overlapping classes with Kinetics-600, leaving 90 classes as OOD classes for evaluation. For other datasets, there are no class overlap issues, and we use their original classes as OOD. ", "page_idx": 15}, {"type": "image", "img_path": "A5pabdZp2F/tmp/9d991d5f8ec9bd312532f2d6a586809b09673c090f3164c5cfade4334423de5c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 8: Score distributions of different baseline methods on the HMDB51 25/26 dataset before and after training with $A2D$ and $N P{-}M i x$ . ", "page_idx": 16}, {"type": "image", "img_path": "A5pabdZp2F/tmp/4ae3d1fcdc4869d439d3966c41945b813e6bbb5d21b46fd811054e41319dc836.jpg", "img_caption": ["Figure 9: Visualization of the learned embeddings on ID and OOD data using t-SNE on the HMDB51 25/26 dataset before and after training with $A2D$ and $N P{-}M i x$ . "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "C Visualization of Results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Visualization of Synthesized Outliers. We visualize the outliers generated by different outlier synthesis algorithms, including VOS [22], NPOS [60], Mixup [69], and our proposed NP-Mix. As shown in Fig. 7, VOS and NPOS generate outliers near the ID data, neglecting to explore the broader embedding space. Mixup randomly selects samples from all classes to mix and introduces unwanted noise samples within the distribution of ID data. NP-Mix excels at generating synthesized outliers by effectively utilizing information from neighboring classes and spanning wider embedding spaces. ", "page_idx": 16}, {"type": "text", "text": "Score Distributions. Fig. 8 illustrates the score distributions generated by various baseline methods on the HMDB51 25/26 dataset before and after training with $A2D$ and NP-Mix. Score distributions produced by $A2D$ and NP-Mix lead to better ID/OOD separation, resulting in strengthened OOD detection performance. ", "page_idx": 16}, {"type": "text", "text": "Visualization of Learned Embeddings for ID and OOD Data. Fig. 9 shows the visualization of the learned embeddings using t-SNE [46] on the HMDB51 25/26 dataset before and after training with $A2D$ and $N P{-}M i x$ . The embedding of ID and OOD data are more separable after $A2D$ training and NP-Mix outlier synthesis. ", "page_idx": 16}, {"type": "text", "text": "D More Implementation Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "D.1 Training Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We conduct experiments across three modalities: video, audio, and optical flow. We adopt the MMAction2 [11] toolkit for experiments. To encode the visual information, we utilize SlowFast network [23], initialized with pre-trained weights from Kinetics-400 [34]. For the audio encoder, we employ ResNet-18 [29] , initializing the weights from the VGGSound pre-trained checkpoint [10]. Similarly, we use the SlowFast network with a slow-only pathway, again leveraging pre-trained weights from Kinetics-400 [34] for the optical flow encoder. We use the Adam optimizer [36] with a learning rate of 0.0001 and a batch size of 16. Additionally, we set the hyperparameters as follows: $\\gamma=0.5$ , mixup $\\alpha=10.0$ , nearest neighbor $N=2$ . We train the network for 50 epochs on an RTX 3090 GPU and select the model with the best performance on the validation dataset. ", "page_idx": 17}, {"type": "text", "text": "D.2 Extension to More Modalities ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Our framework is not limited to two modalities and can be easily extended to $M$ modalities. Given a training sample $\\mathbf{x}$ with label $c$ and $M$ modalities, we obtain prediction probabilities $\\hat{p}$ from the combined embeddings of all modalities, and $\\hat{p}^{1},\\,\\hat{p}^{2},\\,...,\\,\\hat{p}^{M}$ from each modality, all of which are of shape $[1,C]$ , where $C$ represents the number of classes. By removing the $c$ -th value from each prediction, we derive new prediction probabilities without ground-truth classes, denoted as $\\bar{p}^{1}$ , $\\bar{p}^{2},...,$ $\\dot{p}^{M}$ with shapes $[1,C-1]$ . Subsequently, we aim to maximize the discrepancy between $\\bar{p}^{1},\\bar{p}^{2},$ ..., $\\bar{p}^{M}$ , which can be defined as: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{L}_{D i s c r}=-\\frac{2}{M(M-1)}\\sum_{i=1}^{M-1}\\sum_{j=i+1}^{M}D i s c r(\\bar{p}^{i},\\bar{p}^{j}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $D i s c r(\\cdot)$ is a distance metric quantifying the similarity between two probability distributions. Similarly, for $\\mathcal{L}_{c l s}$ , $\\mathcal{L}_{D i s c r\\_s y n}$ and $\\mathcal{L}_{E n t}$ , we can define as: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{L}_{c l s}=\\frac{1}{M+1}(C E(\\hat{p},y)+\\sum_{i=1}^{M}C E(\\hat{p}^{i},y)),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{L}_{D i s c r_{-}s y n}=-\\frac{2}{M(M-1)}\\sum_{i=1}^{M-1}\\sum_{j=i+1}^{M}D i s c r(\\widetilde{p}^{i},\\widetilde{p}^{j}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{L}_{E n t}=-\\frac{1}{M}\\sum_{i=1}^{M}H(\\widetilde{p}^{i}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The final loss is obtained as the weighted sum of the previously defined losses: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\mathcal{L}_{c l s}+\\gamma(\\mathcal{L}_{D i s c r}+\\mathcal{L}_{D i s c r\\_s y n}+\\mathcal{L}_{E n t}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "D.3 Inference Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "For algorithms that define OOD score or truncate activations leveraging information from the feature space (Mahalanobis [41], KNN [59], VIM [62], ReAct [58], ASH [16]), we use the combined embedding ${\\mathbf Z}=[{\\mathbf Z}^{1},{\\mathbf Z}^{2},...,{\\mathbf Z}^{M}]$ from all modalities. For algorithms that define the OOD score leveraging information from the probability space or logit space (MSP [31], GEN [44], Energy [43], MaxLogit [30], VIM [62], LogitNorm [65]), we use the prediction probabilities $\\hat{p}$ or prediction logits $h(\\mathbf{Z})$ obtained from the combined embeddings of all modalities. ", "page_idx": 17}, {"type": "text", "text": "E Further Experimental Results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Multimodal Far-OOD Detection. Tab. 7 and Tab. 8 present comprehensive results for Multimodal Far-OOD Detection on the HMDB51 and Kinetics-600 datasets. Training with $A2D$ and $N P{-}M i x$ significantly improves OOD performance in most cases across all baseline algorithms, underscoring the versatility of our proposed method. ", "page_idx": 17}, {"type": "table", "img_path": "A5pabdZp2F/tmp/905438d37695a42832ea52afca3c9ed65efcc4c120de437306d7e94dd32040f9.jpg", "table_caption": ["Table 7: Multimodal Far-OOD Detection using video and optical flow, with HMDB51 as ID. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "A5pabdZp2F/tmp/bc7b800a119a5dc32e6f2cb7b08d23d95476093aa8994c2018008ed6757befb5.jpg", "table_caption": ["Table 8: Multimodal Far-OOD Detection using video and optical flow, with Kinetics-600 as ID. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Multimodal Near-OOD Detection with Other Combination of Modalities. We demonstrate the effectiveness of $A2D$ and $N P{-}M i x$ across various combinations of modalities, not limited to video and optical flow, as shown in Tab. 9. The performance of different baseline algorithms improves significantly with $A2D$ training and $N P{-}M i x$ outlier synthesis, regardless of whether the input modalities are video-audio, flow-audio, or video-audio-flow combinations. ", "page_idx": 18}, {"type": "text", "text": "F More Ablations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Compared with Other Multimodal Tasks. We compare $A2D$ and NP-Mix with other multimodal self-supervised training tasks, including Contrastive Loss [35], Relative Norm Alignment (RNA) Loss [51], Cross-modal Distillation [27], and Cross-modal Translation [20], as shown in Tab. 10. While contrastive loss demonstrates effectiveness in enhancing OOD performance, other tasks significantly decrease the performance. $A2D$ and NP-Mix show substantial superiority over other multimodal self-supervised tasks. ", "page_idx": 18}, {"type": "text", "text": "Influences of $N$ and $\\alpha$ in NP-Mix. In this section, we investigate the parameter sensitivity of NP-Mix on the HMDB51 25/26 dataset. For the Nearest Neighbor parameter $N$ , we test values of 1, 2, 3, and 4. Regarding the Mixup parameter $\\alpha$ , we evaluate values of 2.0. 4.0, and 10.0. As shown in Fig. 10 and Fig. 11, NP-Mix demonstrates robustness across different parameter settings and yields substantial enhancements in OOD performance for all cases. ", "page_idx": 18}, {"type": "table", "img_path": "A5pabdZp2F/tmp/bfc2547e0b266739c865a9a275f51a1ee33042034a7ef72d66f3770b45b94d1a.jpg", "table_caption": ["Table 9: Multimodal Near-OOD Detection using different combination of modalities. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "A5pabdZp2F/tmp/227f86a33236e8f0d758bbdf3031224c4217e6aace6e3ab568b53c9544adf54f.jpg", "table_caption": ["Table 10: Ablation on Multimodal Training Tasks for Near-OOD Detection on HMDB51 dataset. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "Different Architectures. In this section, we demonstrate that the strong performance of training with $A2D$ and $N P{-}M i x$ is maintained across different network architectures. We replaced the architecture of the video backbone with Inflated 3D ConvNet (I3D) [7] and the optical flow backbone with Temporal Segment Network (TSN) [64]. As shown in Tab. 12, $A2D$ and NP-Mix consistently deliver significant improvement in OOD detection performance with these new architectures. Furthermore, we replaced the architecture of the video backbone architecture with Video Swin Transformer (Swin-T) [45] and the audio backbone with Audio Spectrogram Transformer (AST) [25]. As illustrated in Tab. 13, $A2D$ and $N P{-}M i x$ also consistently achieve significant improvement in OOD detection performance using Transformer-based architectures. ", "page_idx": 19}, {"type": "table", "img_path": "A5pabdZp2F/tmp/5cdbca53d1895436d7297ab108e2b91eec25fcef86a199be49d86a43964fd694.jpg", "table_caption": ["Table 11: Ablation on the ensemble of different OOD scores on a single modality. "], "table_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "A5pabdZp2F/tmp/f161326b0fd4abe099e3f79bc45826d05e756cfef246bcd139c9c9c8fdf9086b.jpg", "img_caption": ["Figure 10: Influences of Mixup $\\alpha$ for OOD performance in $N P{-}M i x$ . "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "A5pabdZp2F/tmp/f80b807dedc8ca867b41b679d826f034b84314c1206189e7ce7a62e3b8c523a9.jpg", "img_caption": ["Figure 11: Influences of Nearest Neighbor $N$ for OOD performance in $N P{-}M i x$ . "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Ensemble of Multiple Unimodal OOD Methods. In this section, we add evaluations on HMDB51 25/26 for the ensemble of multiple unimodal OOD methods for each modality to demonstrate the importance of studying the multimodal OOD detection problem. We first evaluate the ensemble of different OOD scores on a single modality. We choose three scores for the ensemble: probability space (MSP), logit space (Energy), and feature space (Mahalanobis). For all scores, we normalize their values between 0 and 1 and calculate the ", "page_idx": 20}, {"type": "table", "img_path": "A5pabdZp2F/tmp/dc33f7e22b8faad5bc7a29848c84f47b6c3d2a7651a7c145d62f6a981eefe3d4.jpg", "table_caption": ["Table 14: Ablation on the ensemble of different OOD scores on different modalities. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "ensemble score as: score $=\\alpha\\ast s c o r e_{1}+(1-\\alpha)\\ast s c o r e_{2}$ . For $\\alpha$ , we do a grid search from 0.1 to 0.9 with a 0.1 interval and report the one with the best performance. As shown in Tab. 11, combining MSP or Energy with Mahalanobis can bring significant improvement, especially for video. However, there is still a large gap compared with our proposed solution, demonstrating the importance of studying the multimodal OOD detection problem. We then evaluate the ensemble of OOD scores on different modalities and calculate the ensemble score as: score $=\\alpha\\ ^{*}\\ s c o r e_{v i d e o}+(1-\\alpha)\\ ^{*}$ $\\mathit{s c o r e}_{f l o w}$ . For $\\alpha$ , we also do a grid search from 0.1 to 0.9 with a 0.1 interval and report the one with the best performance. As shown in Tab. 14, combining more modalities always brings performance improvements, but still has a large gap compared with our proposed solution, further demonstrating the importance of studying multimodal OOD detection problems. ", "page_idx": 20}, {"type": "table", "img_path": "A5pabdZp2F/tmp/200df8738ba98cec855a2f3c33f52cd43ac97b7dde4214dc8d94bb930e6289ef.jpg", "table_caption": ["Table 12: Multimodal Near-OOD Detection using video and flow on HMDB51 25/26 with I3D and TSN backbones. "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "A5pabdZp2F/tmp/3b92697e9151f56a4e05c88a6a430f066524ed74309b4ad0c08d80b5e8ccf434.jpg", "table_caption": ["Table 13: Multimodal Near-OOD Detection using video and audio on EPIC-Kitchens 4/4 with Swin-T and AST backbones. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Robustness under Missing-modalities. In our framework, we train a classifier for each modality to get predictions from each modality separately. By default, we use the predictions obtained from the combined embeddings of all modalities to calculate the OOD score. However, when one modality is missing, we can use the predictions from the remaining modality to calculate the OOD score. We add evaluations on HMDB51 25/26 under this challenging condition and use Energy as the OOD score. As shown in Tab. 15, when one modality is missing, the performance drops a little, especially in the case when the video is missing (A2D+NP-Mix (Flow)). However, compared with training on one modality alone (Video-only and Flow-only), training with A2D and NP-Mix can also bring significant improvements for each modality when another modality is missing. For example, A2D+NP-Mix (Video), the case when optical flow is missing, yields a $16.56\\%$ relative improvement on FPR95 compared with Video-only. This underscores the importance of cross-modal training for multimodal OOD detection. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "table", "img_path": "A5pabdZp2F/tmp/1b9e6cc311766f76c03954c5495d5f348178d1664ed6680e5b9848ad7364d916.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Beyond Action Recognition Task. To further demonstrate the versatility of the proposed A2D training, we add another task on 3D semantic segmentation using LiDAR point cloud and RGB images. We evaluate on SemanticKITTI [3] dataset and set all vehicle classes as OOD classes. During training, we set the labels of OOD classes to void and ignore them. During inference, we aim to segment the known classes with high Intersection over Union (IoU) score, and detect OOD classes as unknown. We adopt three metrics for evaluation, including FPR95, AUROC, and $m I O U_{c}$ (mean Intersection over Union for known classes). We use ResNet-34 [29] and ", "page_idx": 21}, {"type": "table", "img_path": "A5pabdZp2F/tmp/a0850dc931310ad03fd9bfc07d95da84f691172cb1ad3580492c822fd239229c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Table 16: Ablation on the 3D semantic segmentation using LiDAR point cloud and RGB images. ", "page_idx": 21}, {"type": "text", "text": "SalsaNext [12] as the backbones of the camera stream and LiDAR stream. We compare our A2D with basic LiDAR-only and Late Fusion, as well as two multimodal 3D Semantic Segmentation baselines PMF [72] and XMUDA [33]. As shown in Tab. 16, our A2D also demonstrates strong performance under this new task (3D Semantic Segmentation) with different combinations of modalities (LiDAR point cloud and RGB images). ", "page_idx": 21}, {"type": "image", "img_path": "A5pabdZp2F/tmp/3ca9b2af9afb1b9f9c3a178f7e5545658324d1416926f2c62c63d4c2819a9e30.jpg", "img_caption": ["Figure 12: Experiments using three random seeds for Multimodal Near-OOD Detection on the HMDB51 25/26. Foreground points in bold show results averaged across three different seeds while background points, shown feint, indicate results from the underlying individual seeds. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "A5pabdZp2F/tmp/566b78acc341381ad2f792c0267a243737a0880cfb332d3fbde30cb6286b1f40.jpg", "img_caption": ["Figure 13: Experiments using five random dataset splits for Multimodal Near-OOD Detection on the HMDB51 25/26. Foreground points in bold show results averaged across five different splits while background points, shown feint, indicate results from the underlying individual splits. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "G Statistical Significance Tests ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Different Random Seeds. We run each experiment three times using different seeds for Multimodal Near-OOD Detection on the HMDB51 25/26 dataset in our MultiOOD benchmark, and then calculate the mean AUROC and FPR95 to demonstrate the statistical significance of our methods. As shown in Fig. 12, training with $A2D$ and $N P{-}M i x$ is statistically stable and significantly surpasses the baselines under different random seeds. ", "page_idx": 22}, {"type": "text", "text": "Different Random Splits. We run each experiment five times using different dataset splits for Multimodal Near-OOD Detection on the HMDB51 25/26 dataset in our MultiOOD benchmark, and then calculate the mean AUROC and FPR95 to demonstrate the statistical significance of our methods. As shown in Fig. 13, training with $A2D$ and NP-Mix is statistically stable and surpasses the baselines significantly under different dataset splits. ", "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our abstract and introduction clearly state the claims made. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: In the conclusion part, we discussed limitations and future work. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not include theoretical results. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our paper discloses all the information needed to reproduce the main experimental results. Our source code and benchmark datasets are provided in Supplementary Material and will also be made publicly available. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We provide the code in Supplementary Material to reproduce the main experimental results. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We specify all the training and test details in the paper and in the provided code. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We provide Statistical Significance Tests in Appendix G. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We provide details in implementation details part. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Our paper conforms, in every respect, with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: There is no societal impact of the work performed. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: They are properly credited in our paper. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 27}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The new assets introduced in the paper are well documented. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]