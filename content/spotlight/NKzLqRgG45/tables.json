[{"figure_path": "NKzLqRgG45/tables/tables_5_1.jpg", "caption": "Table 1: Comparison with baseline on COCO val2017. We report the number of parameters and FLOPs of the backbone. Underline indicates FLOPs or metrics on par with the baseline. AP50 and APm represent box AP and mask AP, respectively.", "description": "This table compares the performance of the proposed PIIP networks with baseline models (ViTDet-B and ViTDet-L) on the COCO val2017 dataset for object detection and instance segmentation.  It shows the number of parameters, FLOPs (floating point operations), and metrics (AP50, APm) for each model.  The underlined values indicate where PIIP achieves comparable or better performance than the baseline while using significantly fewer FLOPs, demonstrating the efficiency of the proposed method.", "section": "4.2 Object Detection and Instance Segmentation"}, {"figure_path": "NKzLqRgG45/tables/tables_6_1.jpg", "caption": "Table 1: Comparison with baseline on COCO val2017. We report the number of parameters and FLOPs of the backbone. Underline indicates FLOPs or metrics on par with the baseline. AP5 and APm represent box AP and mask AP, respectively.", "description": "This table compares the proposed PIIP method with baseline methods (ViTDet-B and ViTDet-L) on the COCO val2017 dataset for object detection and instance segmentation.  It shows the number of parameters, FLOPs (floating point operations), and performance metrics (AP50, AP75, AP, AP50m, AP75m, APm) for each model.  The underlined values highlight results comparable to the baselines, demonstrating the efficiency gains of PIIP with comparable or even improved accuracy.", "section": "4.2 Object Detection and Instance Segmentation"}, {"figure_path": "NKzLqRgG45/tables/tables_6_2.jpg", "caption": "Table 3: Experiments on the large-scale vision foundation model InternViT-6B.", "description": "This table presents the results of experiments conducted using the InternViT-6B model.  It compares the performance of the original InternViT-6B model with the PIIP-LH6B model (the proposed Parameter-Inverted Image Pyramid network applied to InternViT-6B) across object detection and semantic segmentation tasks.  The table shows the number of parameters, FLOPs, resolution, AP<sup>b</sup> (Average Precision for bounding boxes), AP<sup>m</sup> (Average Precision for masks), crop size, and mIoU (mean Intersection over Union).  Different configurations of the PIIP-LH6B model are evaluated, each with varying input resolutions, demonstrating its impact on performance and computational cost. The results highlight the improved efficiency and performance of PIIP-LH6B compared to the baseline model.", "section": "4.2 Object Detection and Instance Segmentation"}, {"figure_path": "NKzLqRgG45/tables/tables_7_1.jpg", "caption": "Table 4: Semantic segmentation performance on ADE20K using UperNet.", "description": "This table compares the performance of different semantic segmentation models on the ADE20K dataset using the UperNet framework.  The models compared include Swin-B, ConvNeXt-B, RepLKNet-31B, SLaK-B, InternImage-B, PIIP-TSB, Swin-L, RepLKNet-31L, ConvNeXt-L, ConvNeXt-XL, InternImage-L, and PIIP-SBL.  The table shows the mIoU (mean Intersection over Union) achieved by each model and the crop size used for each model.  The results demonstrate the performance of the PIIP models compared to state-of-the-art baselines.", "section": "4.3 Semantic Segmentation"}, {"figure_path": "NKzLqRgG45/tables/tables_7_2.jpg", "caption": "Table 4: Semantic segmentation performance on ADE20K using UperNet.", "description": "This table presents a comparison of semantic segmentation performance on the ADE20K dataset using the UperNet model.  It compares different backbone networks, including Swin-B, ConvNeXt-B, RepLKNet-31B, SLaK-B, and InternImage-B, against the proposed PIIP-TSB and PIIP-SBL methods.  The table shows the crop size used for each model, the number of FLOPs (floating point operations), and the mIoU (mean Intersection over Union) score achieved, demonstrating the efficiency and performance gains of PIIP.", "section": "4.3 Semantic Segmentation"}, {"figure_path": "NKzLqRgG45/tables/tables_7_3.jpg", "caption": "Table 6: Image classification performance on ImageNet. Underline indicates FLOPs or metrics on par with the baseline.", "description": "This table presents a comparison of image classification performance on the ImageNet dataset between different models.  It shows the resolution, number of FLOPs (floating point operations), and top-1 accuracy for each model.  The baseline model's results are shown for comparison, and the underlined values indicate models that achieve comparable performance while potentially using fewer FLOPs (indicating better computational efficiency).", "section": "4.4 Image Classification"}, {"figure_path": "NKzLqRgG45/tables/tables_7_4.jpg", "caption": "Table 1: Comparison with baseline on COCO val2017. We report the number of parameters and FLOPs of the backbone. Underline indicates FLOPs or metrics on par with the baseline. AP5 and APm represent box AP and mask AP, respectively.", "description": "This table compares the performance of the proposed Parameter-Inverted Image Pyramid Networks (PIIP) with baseline models (ViTDet-B and ViTDet-L) on the COCO val2017 dataset for object detection and instance segmentation tasks.  It shows the number of parameters, FLOPs (floating-point operations), and the average precision (AP) metrics for both box and mask predictions for different models.  The results demonstrate that PIIP achieves comparable or better performance with significantly reduced computational cost compared to the baselines.", "section": "4.2 Object Detection and Instance Segmentation"}, {"figure_path": "NKzLqRgG45/tables/tables_8_1.jpg", "caption": "Table 9: Ablation on attention type and number of interactions with PIIP-TSB 1120/896/448.", "description": "This ablation study investigates the impact of different attention mechanisms (regular vs. deformable) and varying numbers of cross-branch interactions on the performance of the PIIP-TSB model with a specific resolution configuration (1120/896/448).  The table shows the FLOPs, APb (average precision for bounding boxes), APm (average precision for masks), AP50, and AP75 metrics for each configuration, allowing for comparison of performance and computational cost.", "section": "4.4 Ablation Study"}, {"figure_path": "NKzLqRgG45/tables/tables_14_1.jpg", "caption": "Table 1: Comparison with baseline on COCO val2017. We report the number of parameters and FLOPs of the backbone. Underline indicates FLOPs or metrics on par with the baseline. AP50 and APm represent box AP and mask AP, respectively.", "description": "This table compares the performance of the proposed PIIP network with baseline models (ViTDet-B and ViTDet-L) on the COCO val2017 dataset for object detection and instance segmentation tasks.  It shows the number of parameters, FLOPs (floating point operations), box AP (average precision), and mask AP for each model.  The results demonstrate PIIP's superior performance and computational efficiency compared to the baselines.", "section": "4.2 Object Detection and Instance Segmentation"}, {"figure_path": "NKzLqRgG45/tables/tables_15_1.jpg", "caption": "Table 12: Full results of PIIP variants under different resolution configurations.", "description": "This table presents the complete results of experiments using different variations of the Parameter-Inverted Image Pyramid (PIIP) network with varying input resolutions.  The performance metrics (APb, AP, APm) for object detection and instance segmentation are shown for various combinations of ViT model sizes and image resolutions using the Mask R-CNN evaluation protocol.  Different PIIP configurations (PIIP-TSB, PIIP-SBL, PIIP-TSBL) are included, indicating different numbers of branches and the specific ViT models (T, S, B, L) used in each branch.", "section": "A.2 Full Detection Results"}, {"figure_path": "NKzLqRgG45/tables/tables_15_2.jpg", "caption": "Table 13: From-scratch pre-training results on ImageNet-1K.", "description": "This table presents the architecture configuration and performance of a three-branch PIIP network trained from scratch on ImageNet-1K.  It details the number of layers, embedding dimension, number of heads, resolution, number of parameters, and FLOPs for each branch (Branch 1, Branch 2, Branch 3), the interaction modules, and the branch merging module.  The table also shows the resulting Top-1 accuracy.  This configuration differs from experiments in the main paper where pretrained models were used.", "section": "4.4 Image Classification"}, {"figure_path": "NKzLqRgG45/tables/tables_15_3.jpg", "caption": "Table 13: From-scratch pre-training results on ImageNet-1K.", "description": "This table shows the results of training a from-scratch model PIIP-B (Parameter-Inverted Image Pyramid) on the ImageNet-1K dataset. It compares the performance of PIIP-B with ViT-B (Vision Transformer). The table presents the model configuration, including the number of layers, embedding dimension, number of heads, resolution, number of parameters, number of FLOPs (floating-point operations), and top-1 accuracy.  PIIP-B demonstrates a competitive performance compared to ViT-B while showcasing the effectiveness of the parameter-inverted image pyramid approach in from-scratch pre-training.", "section": "4.4 Image Classification"}]