{"references": [{"fullname_first_author": "Michael Ahn", "paper_title": "ROBEL: Robotics Benchmarks for Learning with Low-Cost Robots", "publication_date": "2020-05-01", "reason": "This paper introduces ROBEL, a benchmark for robotics research using low-cost robots, which is directly relevant to the democratization of robotics research, a core theme of the target paper."}, {"fullname_first_author": "Albert Bou", "paper_title": "TorchRL: A data-driven decision-making library for PyTorch", "publication_date": "2023-11-01", "reason": "TorchRL is the reinforcement learning library used in the target paper's platform, BricksRL; therefore, understanding TorchRL is crucial for understanding the target paper's methodology and contributions."}, {"fullname_first_author": "Scott Fujimoto", "paper_title": "Addressing Function Approximation Error in Actor-Critic Methods", "publication_date": "2018-10-01", "reason": "This paper addresses a key challenge in reinforcement learning, function approximation error, which is relevant to the target paper's use of reinforcement learning algorithms for robotic control."}, {"fullname_first_author": "Tuomas Haarnoja", "paper_title": "Soft Actor-Critic Algorithms and Applications", "publication_date": "2019-01-01", "reason": "This paper introduces the Soft Actor-Critic algorithm, one of the algorithms used in the target paper's experiments, making it crucial for understanding the experimental results and comparisons."}, {"fullname_first_author": "Takuya Hiraoka", "paper_title": "Dropout Q-Functions for Doubly Efficient Reinforcement Learning", "publication_date": "2022-03-01", "reason": "This paper introduces the DroQ algorithm, another algorithm used in the target paper, and its efficiency in sample-based learning is relevant to the paper's focus on real-world robotic learning."}]}