[{"figure_path": "8iytZCnXIu/figures/figures_1_1.jpg", "caption": "Figure 1: Communication overview of the agent, environment and robot.", "description": "This figure illustrates the communication flow and data exchange between the three main components of the BricksRL platform: the TorchRL agent, the TorchRL environment, and the LEGO robot.  The agent sends actions (as byte actions) to the environment via TensorDict. The environment, incorporating Pybricks Class for robot control and a reward function, processes these actions, interacts with the robot (via Client.py), receives state information (as byte states) from the robot, and provides the reward and next state back to the agent via TensorDict.", "section": "RL agents"}, {"figure_path": "8iytZCnXIu/figures/figures_3_1.jpg", "caption": "Figure 2: Three robots that we used in the experiments: (a) 2Wheeler, (b) Walker, (c) RoboArm.", "description": "This figure shows three different LEGO robots used in the experiments described in the paper.  (a) shows a simple two-wheeled robot, (b) shows a quadrupedal walking robot, and (c) shows a robotic arm. These robots represent a range of complexity and demonstrate the versatility of the BricksRL platform for building and controlling various robotic designs.", "section": "3.3 LEGO Robots"}, {"figure_path": "8iytZCnXIu/figures/figures_6_1.jpg", "caption": "Figure 3: Training results for 2Wheeler robot for the RunAway-v0 and the Spinning-v0 environment.", "description": "This figure shows the training performance of the 2Wheeler robot on two different tasks: RunAway-v0 and Spinning-v0.  The left subplot (RunAway-v0) illustrates the reward obtained by four different reinforcement learning algorithms (Random, SAC, TD3, DroQ) over 40 training episodes.  It shows the average reward with shaded standard deviation areas. The right subplot (Spinning-v0) displays the same information but over 15 episodes for the Spinning-v0 task.  The figure visually demonstrates the learning progress of the algorithms on each task and allows a comparison of their performance.", "section": "4 Experiments"}, {"figure_path": "8iytZCnXIu/figures/figures_7_1.jpg", "caption": "Figure 4: Training performance for Walker robot for the Walker-v0 and the WalkerSim-v0 environment.", "description": "This figure compares the training performance of different reinforcement learning algorithms (Random, SAC, TD3, DroQ) on two environments for the Walker robot: the real-world Walker-v0 environment and its simulated counterpart, WalkerSim-v0.  The x-axis represents the episode number, and the y-axis shows the reward obtained during training. Shaded areas indicate the standard deviation across multiple training runs.  The figure illustrates the learning curves of each algorithm, highlighting their performance in both real-world and simulated scenarios, and how they compare against a random baseline.", "section": "4.2 Walker"}, {"figure_path": "8iytZCnXIu/figures/figures_8_1.jpg", "caption": "Figure 5: Training outcomes for the RoboArm robot in both the RoboArm-v0 and RoboArmSim-v0 environments. The plot also includes the final error at the epoch\u2019s last step and the total number of episode steps.", "description": "This figure shows the training performance comparison of different reinforcement learning algorithms (Random, SAC, TD3, DroQ) for the RoboArm robot in two environments: the real-world RoboArm-v0 and the simulated RoboArmSim-v0.  For each environment and algorithm, three plots are presented: reward, final error (the difference between the robot's final pose and the target pose at the end of each episode), and episode steps (the number of steps taken to complete each episode). The shaded areas represent the standard deviation across multiple training runs.", "section": "4.3 RoboArm"}, {"figure_path": "8iytZCnXIu/figures/figures_8_2.jpg", "caption": "Figure 6: Training performance of the RoboArm robot in the RoboArm_mixed-v0 environment, showing both the reward and the number of episode steps required to reach the target location.", "description": "This figure displays the training performance of the RoboArm robot within the RoboArm_mixed-v0 environment.  The left subplot shows the reward obtained across training episodes for four different reinforcement learning algorithms (Random, SAC, TD3, DroQ).  The right subplot shows the number of episode steps taken to reach the target location for each algorithm. The shaded areas represent the standard deviation across multiple trials for each algorithm. This figure demonstrates the algorithms' learning curves and how quickly each method converges (or fails to converge) on the task, which integrates both motor angle controls and a webcam image for more complex decision-making compared to the simpler robot setups.", "section": "4 Experiments"}, {"figure_path": "8iytZCnXIu/figures/figures_20_1.jpg", "caption": "Figure 7: Comparison of communication frequencies for the DroQ agent on the Walker-v0 task, illustrating the differences between the operational frequencies of 11Hz and 2Hz.", "description": "This figure compares the performance of the DroQ algorithm on the Walker-v0 task using two different communication frequencies: 11 Hz and 2 Hz.  The plot shows the reward obtained over 70 episodes. It demonstrates that a lower communication frequency (2 Hz) leads to faster and more stable convergence, potentially due to the effect being similar to frame skipping in RL, which simplifies decision-making.", "section": "4.2 Walker"}, {"figure_path": "8iytZCnXIu/figures/figures_20_2.jpg", "caption": "Figure 8: Final distance and (mean) action taken over one episode for the RunAway-v0 task.", "description": "This figure shows the results of the RunAway-v0 task.  The left subplot (a) displays the final distance achieved by three different reinforcement learning algorithms (SAC, TD3, DroQ) over multiple episodes. The shaded area represents the standard deviation. The right subplot (b) shows the mean action taken by each algorithm over the same episodes. Again, the shaded area represents the standard deviation.  This illustrates the different strategies adopted by each algorithm to maximize the distance travelled while avoiding hitting a wall. ", "section": "4.1 2Wheeler"}]