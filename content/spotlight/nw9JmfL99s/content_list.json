[{"type": "text", "text": "Nonlinear dynamics of localization in neural receptive fields ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Leon Lufkin Andrew Saxe Erin Grant Yale University Gatsby Unit & SWC, UCL Gatsby Unit & SWC, UCL leon.lufkin@yale.edu a.saxe@ucl.ac.uk erin.grant@ucl.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Localized receptive fields\u2014neurons that are selective for certain contiguous spatiotemporal features of their input\u2014populate early sensory regions of the mammalian brain. Unsupervised learning algorithms that optimize explicit sparsity or independence criteria replicate features of these localized receptive fields, but fail to explain directly how localization arises through learning without efficient coding, as occurs in early layers of deep neural networks and might occur in early sensory regions of biological systems. We consider an alternative model in which localized receptive fields emerge without explicit top-down efficiency constraints\u2014a feedforward neural network trained on a data model inspired by the structure of natural images. Previous work identified the importance of non-Gaussian statistics to localization in this setting but left open questions about the mechanisms driving dynamical emergence. We address these questions by deriving the effective learning dynamics for a single nonlinear neuron, making precise how higher-order statistical properties of the input data drive emergent localization, and we demonstrate that the predictions of these effective dynamics extend to the many-neuron setting. Our analysis provides an alternative explanation for the ubiquity of localization as resulting from the nonlinear dynamics of learning in neural circuits.1 ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "A striking feature of peripheral responses in the animal nervous system is localization\u2014that is, the linear receptive fields of simple-cell neurons often respond to contiguous regions much smaller than their full input domain. In vision, retinal ganglion cells approximate localized center-surround fliters that tile the input $\\scriptstyle\\left[\\mathrm{Dac}+\\mathrm{O}0\\right]$ ; $\\mathrm{Doi}+12$ ; KK78], and simple cells downstream in primary visual cortex have localized filters that are selective for spatial frequency and orientation [HW59; HW68; RT95; NS08; WMG11; RSH02; Rin02]. In primary somatosensory cortex, neurons respond to stimulation of restricted regions of skin $\\scriptstyle[\\mathrm{Cro}+11]$ and in primary auditory cortex, spatiotemporal receptive fields are typically localized in both time and frequency domains [DWZ03; HDZ08]; see Fig. 1 (left). ", "page_idx": 0}, {"type": "text", "text": "By contrast, artificial learning systems do not always learn localized filters. Principal component analysis tends to fti weights that span the entire input signal, as do unregularized autoencoder neural network architectures and restricted Boltzmann machines [Sax+11]. This difference has prompted the search for artificial learning models that can learn localized receptive fields from naturalistic stimuli, the most notable of which are sparse coding [OF96; OF97] and independent component analysis [ICA; BS97; vHvdS98]. Sparse coding, ICA, and related compression methods that produce localized receptive fields from naturalistic data share a top-down approach\u2014they find an efficient representation of the input signal by optimizing an explicit sparsity criterion, or an independence criterion that necessitates sparsity in a critically parameterized regime [Fie99; Sax+11]. ", "page_idx": 0}, {"type": "image", "img_path": "nw9JmfL99s/tmp/5966ab078651e9a7f045a87c74a36d4449d480b2f1224facc0d3f0a2d53b1dba.jpg", "img_caption": ["Figure 1: (Left) Localization in spatial receptive fields (RFs) measured from non-human primate (NHP) primary visual cortex [Rin02, Fig. 2] and in spatiotemporal RFs measured from NHP [dBM98, Fig. 2] and ferret $[\\mathrm{Sin}+18$ , Fig. 2] primary auditory cortex. (Center) Half-slice of the localized first-layer kernels of AlexNet trained for ImageNet classification [KSH12]. (Right) Localized receptive fields learned from the task of Section 2.3 in 2-D using ICA [HO00] and the soft committee machine (SCM; M1 with fixed second-layer weights) of Section 2.1. Localization\u2014spatial and/or temporal selectivity\u2014appears across settings, as measured by response maximization in biological systems (left) and by inspecting linear fliters in artificial systems (center, right). "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Though sparsity is appealing as a potentially unifying explanation for localization, localization also emerges naturally in networks trained to perform classification tasks without any explicit sparsity regularization [KSH12; ZF13; Yos $+15$ ; Sen+18]; see Fig. 1 (center) for an example. Ingrosso and Goldt [IG22] distilled such examples of emergent localization by demonstrating that localized receptive fields emerge in simple feedforward neural networks trained on a data model with properties meant to approximate natural visual input, in particular, locality structure (statistical independence of non-collocated dimensions) and non-Gaussianity (higher-order cumulants are non-null). In simulations, Ingrosso and Goldt [IG22] tie the dynamical emergence of localization to increased tuning to higher-order statistics of the input, and demonstrate that even a single neuron is sufficient to learn a localized receptive field in this setting. ", "page_idx": 1}, {"type": "text", "text": "In this work, we build on the demonstration of Ingrosso and Goldt [IG22] with the aim of describing the mechanisms behind the emergence of a localized receptive field in this minimal setting. The higher-order input statistics that drive localization are challenging to analyze with existing tools that exploit implied Gaussianity $[\\mathrm{Gol}+20]$ . By separating two stages of learning, we are able to derive equations for the effective early-time learning dynamics of the single neuron model that learns a localized receptive field from idealized naturalistic data. Our analytical model identifies a concise description of the higher-order statistics that drive emergence, and we validate both positive and negative predictions of this analytical model via simulations with many neurons; see Fig. 1 (right). These findings suggest an alternative path to account for the ubiquity of localization in early neural responses as resulting from the interaction of the nonlinear dynamics of learning in neural circuits and naturalistic data with higher-order statitistical structure, rather than an explicit efficiency criterion. ", "page_idx": 1}, {"type": "text", "text": "2 Modeling approach ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We extend the setting of Ingrosso and Goldt [IG22], a minimal example of a neural network that learns localized receptive fields from idealized naturalistic data. We analyze the dynamics of learning in this setting in Section 3 and validate our analytical model with simulations in Section 4. ", "page_idx": 1}, {"type": "text", "text": "2.1 Neural network architecture and learning algorithm ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We consider a two-layer feedforward neural network with nonlinear activation and scalar output. While simple, this architecture is highly expressive, capable of approximating arbitrary integrable univariate functions with appropriate scaling [Bar93; Pin99], and exhibits rich feature learning dynamics that underlie the performance of models at scale $[\\mathrm{Woo}{+}20]$ , making this architecture the ongoing subject of theoretical neural network analyses [MMN18; Gol+19; $\\mathrm{Vei}{+22}]$ . We denote a two-layer network with $N$ -dimensional input, $M$ hidden units, and one-dimensional scalar output as where $\\sigma:\\mathbb{R}\\rightarrow\\mathbb{R}$ is a pointwise nonlinearity such as the rectified linear unit (ReLU) or sigmoid function, $\\mathbf{w}_{m}^{(1)}\\in\\mathbb{R}^{N}$ and $w_{m}^{(2)}\\in\\mathbb{R}$ are learnable weights, $b_{m}^{(1)},b^{(2)}\\in\\mathbb{R}$ are learnable bias terms, and $\\langle\\cdot,\\cdot\\rangle$ denotes the standard Euclidean inner (dot) product on $\\mathbb{R}^{N}$ . When the second-layer parameters are fixed, this model is known as a soft-committee machine [SCM; SS95], which [IG22] notes learns less noisy receptive fields but exhibits similar localization behavior. The many-neuron architecture in M1 is the focus of our simulations (Section 4), but the dynamics of this model are too complex to analyze directly, even for the idealized naturalistic data model considered here. In order to derive analytical results (Section 3), we consider the simplest neural network exhibiting the desired localization phenomenon, a single hidden neuron without bias and with rectified linear unit activation, written as ", "page_idx": 1}, {"type": "table", "img_path": "nw9JmfL99s/tmp/b0ece2bbe2cd6b564b9b6852d14ccf643369191bb9df90a408c5184449780d4e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Model 2 (single-neuron architecture). ", "page_idx": 2}, {"type": "text", "text": "(M2) ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{y}(\\mathbf x)=\\operatorname{ReLU}\\left(\\langle\\mathbf w,\\mathbf x\\rangle\\right)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathrm{ReLU}(x)=\\operatorname*{max}(x,0)$ , applied pointwise to vectorial input. As Ingrosso and Goldt [IG22] demonstrate, the localized receptive fields learned by the many- and single-neuron models defined in M1 and M2 are qualitatively similar up to spatial translation, which permits us to generalize insights from analyzing the learning dynamics of the single-neuron M2 to the many-neuron M1. For simulations, we initialize the weights and biases as independent draws from an isotropic Gaussian distribution with scaled variance, and train with batch gradient descent with a fixed learning rate on the mean-squared error (MSE) evaluated on input-output pairs from the task; see Section 2.3 for task sampling procedures. ", "page_idx": 2}, {"type": "text", "text": "2.2 Stimulus properties ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The data model of Ingrosso and Goldt [IG22] can be shown to satisfy three conditions that enable the analysis we give in Section 3. We consider several other data models that share the below properties but differ in generative mechanism in order to probe the effect of these properties on localization. In particular, we consider data $\\mathbf{X}$ sampled from distributions $p$ on $\\mathbb{R}^{N}$ satisfying the following: ", "page_idx": 2}, {"type": "text", "text": "Stimulus properties 1\u20133 (idealization of natural images). ", "page_idx": 2}, {"type": "text", "text": "(S1) (Positional) weak dependence: for any fixed $\\rho\\in(0,1)$ , as $N\\rightarrow\\infty$ , $\\alpha(N)\\triangleq\\operatorname*{sup}_{A\\subseteq\\mathbb{R},B\\subseteq\\mathbb{R}^{(1-\\rho)N}}|\\,\\mathbb{P}(X_{1}\\in A,X_{>\\rho N}\\in B)-\\mathbb{P}(X_{1}\\in A)\\,\\mathbb{P}(X_{>\\rho N}\\in B)|\\to0\\,,$   \n(S2) Translation invariance: $p(\\mathbf{X}=\\mathbf{x})\\,=\\,p(\\mathbf{X}\\,=\\,S\\mathbf{x})$ for all $\\mathbf{x}\\,\\in\\,\\mathbb{R}^{N}$ , where $\\boldsymbol{S}$ is the circular shift operator, and   \n(S3) Sign symmetry: $p(\\mathbf{X}=\\mathbf{x})=p(\\mathbf{X}=-\\mathbf{x}){\\mathrm{~for~all~}}\\mathbf{x}\\in\\mathbb{R}^{N}.$ ", "page_idx": 2}, {"type": "text", "text": "Properties S1 and S2 are defining characteristics of natural image data [HHH09]. Property S3 can also be seen to hold for natural images after centering and is convenient analytically because it implies that $\\mathbb{E}[\\mathbf{X}]=0$ . Property S1 assumes that $p$ is implicitly parameterized by $N$ in order to state that the statistical dependence between entries of $\\mathbf{X}$ vanishes as their separation increases. ", "page_idx": 2}, {"type": "text", "text": "We denote the covariance of $\\mathbf{X}$ by $\\Sigma\\triangleq\\operatorname{Cov}[\\mathbf{X}]$ , the square of principal-diagonal entries (the variance of each entry of $\\mathbf{X}$ ) by $\\sigma^{2}$ , and the $i$ -th row by $\\sigma_{i}$ . Weak dependence (S1) implies that entries far from the principal diagonal of $\\Sigma$ will be 0, while translation-invariance (S2) implies that $\\Sigma$ is circulant (i.e., entries along each diagonal are equal) and thus identifiable by a single row; see Fig. 2 (center). ", "page_idx": 2}, {"type": "image", "img_path": "nw9JmfL99s/tmp/5eda503d9e2313e989ef6e7fbdd4d79e4bfcfa02d080b6d6bc1a113ac67da6b0.jpg", "img_caption": ["Figure 2: From left: Long- and short-lengthscale samples x, covariances $\\Sigma$ for one lengthscale, and marginals $p(X_{i})$ for the data models described in Section 2.3: Ising (with $J\\,=\\,1.2,0.3$ for left, right samples), the nonlinear Gaussian process [NLGP; IG22], and the controllable kurtosis model, Kur (with $\\xi=5,1$ for left, right samples). Each model generates samples centered about zero and with covariances that can be constrained to be similar, but with differing higher-order statistics, as can be seen from the dimension-wise marginals. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "2.3 Lengthscale discrimination task ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Ingrosso and Goldt [IG22] develop a minimal task for which localization emerges in a feedforward neural network: binary discrimination between inputs from two distributions that differ in the lengthscale of the correlations between their entries. This lengthscale discrimination task can be seen as a pretext task for self-supervised learning [KZB19; Che $+20$ ] of representations [cf. unsupervised: OF96; BS97]. More precisely, we generate data $(\\mathbf{X},Y)$ for supervised training according to ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\bf X}\\mid Y=y\\sim p({\\bf X};\\Sigma_{y})\\;,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $p$ is to be defined, $\\Sigma_{y}$ are distinct covariance matrices for each $y$ , and we sample $Y$ uniformly among a set of increasing lengthscale correlation classes $y\\in\\{0,1,\\ldots\\}$ , which correspond to the strength of correlation between distant positions. For instance, in the case of two classes $(y=0,1)$ , we take $\\Sigma_{0}$ to be closer to $\\sigma^{2}\\mathbb{I}_{N}$ than $\\Sigma_{1}$ , where $\\mathbb{I}_{N}$ is the $N\\times N$ identity matrix and $\\sigma$ is a fixed value. This construction isolates, via distinct covariance matrices per class, the second-order statistics, which we will see below enter into the learning dynamics separately from other properties of $p(\\mathbf{X})$ , including, most critically, the implied marginal distributions, $p(X_{i})$ . ", "page_idx": 3}, {"type": "text", "text": "Ising. The first distribution we consider is the one-dimensional Ising model. It is of interest as a distribution that satisfies S1 to S3 with marginals $p(X_{i})$ with extreme support on $\\{\\pm1\\}$ , making it the distribution that promotes localization most strongly, as we will see in Section 3. In the absence of an external field, the Ising distribution is ", "page_idx": 3}, {"type": "equation", "text": "$$\np_{\\mathrm{Ising}}(\\mathbf{X}=\\mathbf{x})=p_{\\mathrm{Ising}}(X_{1}=x_{1},\\ldots,X_{N}=x_{N})=e^{-\\sum_{i=1}^{N}J x_{i}x_{i+1}}/\\mathcal{Z},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $J$ is a chosen pairwise interaction strength, $\\mathcal{Z}$ is the normalizing constant, and we enforce a periodic boundary constraint via ${\\boldsymbol{x}}_{N+1}\\equiv{\\boldsymbol{x}}_{1}$ . As $J$ increases, the lengthscale of the correlations in $\\mathbf{X}$ also increases. For simulations, we sample from $p_{\\mathtt{I s i n g}}$ using a Gibbs sampler [GG84]. Discrimination tasks in the simulations in Section 4 use $J_{1}=0.7$ (for $y=1$ ) and $J_{0}=0.3$ (for $y=0$ ). ", "page_idx": 3}, {"type": "text", "text": "${\\tt N L G P}(g)$ . We also consider the data model used in Ingrosso and Goldt [IG22], the nonlinear Gaussian process (NLGP), which enables one to interpolate between distributions that do and do not yield localization via a single parameter, $g$ . A sample $\\mathbf{X}\\mid Y=y$ from the NLGP is constructed by first sampling a Gaussian $\\mathbf{Z}\\mid Y=y\\sim\\mathcal{N}(0,\\tilde{\\Sigma}_{y})$ and then transforming it via ", "page_idx": 3}, {"type": "equation", "text": "$$\nX_{i}\\triangleq\\operatorname{erf}(g Z_{i})/\\mathcal{Z}(g)\\qquad1\\leq i\\leq N,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where erf is the Gauss error function, $\\mathcal{Z}$ is a normalization constant to ensure that the variances of $X_{i}$ and $Z_{i}$ are the same, and $\\tilde{\\Sigma}_{y}$ is a covariance matrix for $\\mathbf{Z}$ , where we use $(\\tilde{\\Sigma}_{y})_{i j}=\\exp(-(i-j)^{2}/\\xi^{2})$ for a lengthscale parameter $\\xi$ [IG22]. If $g\\approx0$ (where localization is not observed), $g Z_{i}$ will tend to lie in the linear regime of erf, so $Z_{i}$ will be untransformed, i.e., $\\mathbf{X}$ is Gaussian. However, as $g\\rightarrow\\infty$ (where localization is observed), $g Z_{i}$ will tend to saturate erf, so $X_{i}$ will have support on $\\{\\pm1\\}$ . ", "page_idx": 3}, {"type": "text", "text": "$\\mathtt{K u r}(k)$ . The final family we consider is chosen to give us flexibility over the kurtosis $\\kappa$ of the marginals $p(X_{i})$ . In the Ising model, the excess kurtosis $(\\kappa-3)$ of the marginals is fixed at $-2$ , while in ${\\tt N L G P}(g)$ , it varies from $-2$ to 0. This family allows us to vary the excess kurtosis from negative through positive values. We sample $\\mathbf{X}\\mid Y=y$ from this family via inverse transform sampling to vary the marginals while enforcing dependence via Gaussian copulas. More concretely, we sample $\\mathbf{Z}\\mid\\dot{Y}=y\\sim\\mathcal{N}(0,\\tilde{\\Sigma}_{y})$ and then transform it via ", "page_idx": 4}, {"type": "equation", "text": "$$\nX_{i}\\triangleq f^{-1}(\\Phi(Z_{i}/\\tilde{\\sigma}))/\\mathcal{Z},\\qquad1\\leq i\\leq N,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\tilde{\\sigma}$ is the standard deviation of $Z_{i}$ , $\\Phi$ is the standard Gaussian cumulative distribution function (CDF), $f$ is the CDF of the desired marginal distribution for $X_{i}$ , and $\\mathcal{Z}$ is a normalization constant, which we compute numerically. We define $\\tilde{\\Sigma}_{y}$ as for NLGP. We choose $f$ to be the generalized algebraic sigmoid function (see Appendix A.2) for $k\\ >\\ 0$ to make use of its tractable inverse, simplifying the procedure in Eq. (4). We denote the corresponding distribution by $\\mathtt{K u r}(k)$ . Though we are able to continuously vary excess kurtosis, we lack an explicit form; however, numerical computation shows that for $k\\lessapprox5.8$ , excess kurtosis is positive, while for $k\\gtrapprox5.9$ , it is negative. ", "page_idx": 4}, {"type": "text", "text": "3 Theoretical results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We derive an analytical model for the localization dynamics of the single-neuron architecture in M2. This result establishes necessary and sufficient conditions for localization under Properties S1 to S3 for the minimal case of a binary response, i.e., $y=0,1$ . The conditions for localization in the singleneuron architecture in M2 are demonstrated in Section 4 to also hold empirically for the many-neuron architecture in M1. Further, we use this model to derive a negative prediction about localization, that the architectures in M1 and M2 fail to learn a localized receptive field on elliptical distributions despite their non-Gaussian\u2014in particular, significantly positive kurtosis\u2014statistics [cf. positive kurtosis as an objective or diagnostic for localization, HO00; IG22]. ", "page_idx": 4}, {"type": "text", "text": "3.1 An analytical model for the dynamics of localization in a single neuron ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Previous approaches to obtain analytical dynamics in the architectures in M1 and M2 have studied the gradient flow under the assumption that the preactivation $\\langle\\mathbf{w},\\mathbf{X}\\rangle$ is approximately Gaussian $[\\mathrm{Gol}+20\\$ ; Ger $+20$ ; $_{\\mathrm{Gol}+22]}$ , but this assumption fails to capture the propagation of higher-order statistics through a neural network that promotes localization [IG22]. Happily, the idealized visual input setting set out in S1 to S3 permits us some simplification. In particular, the translation-invariance of the data $\\mathbf{X}$ under S2 and the architecture of M2 allow us to work with the marginal distributions of each input dimension, $X_{i}$ rather than the full joint distribution of $\\mathbf{X}$ . ", "page_idx": 4}, {"type": "text", "text": "We now give the analytical simplifications that allow us to derive an analytical model for the localization dynamics of the single neuron architecture in M2, namely two assumptions on $\\mathbf{X}\\mid X_{i}$ for all $i\\in\\{1,\\ldots,N\\}$ as a well as a mild condition on the weights that is satisfied at initialization. These are, where $\\sigma_{i}^{y}$ to denotes the $i$ -th row of $\\Sigma_{y}$ : ", "page_idx": 4}, {"type": "text", "text": "Analytical simplifications 1\u20133 (early-time, limiting dynamics). ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "(A1) $\\mathbb{E}[\\mathbf{X}\\mid X_{i}=x_{i},Y=y]=x_{i}\\sigma_{i}^{y}$ , i.e., the conditional mean scales linearly with $x_{i}$ .   \n(A2) $\\operatorname{Cov}[\\mathbf{X}\\mid X_{i}=x_{i},Y=y]=\\Sigma_{y}-\\sigma_{i}^{y}\\sigma_{i}^{y\\top}$ , i.e., the conditional covariance is smaller near $i$ , but independent of the exact value of $x_{i}$ .   \n(A3) Lindeberg\u2019s condition holds for the sequence $w_{1}X_{1},\\ldots,w_{N}X_{N}\\mid X_{i}=x_{i}$ as $N\\rightarrow\\infty$ for all xi. ", "page_idx": 4}, {"type": "text", "text": "Our motivation for Assumptions A1 to A3 is that they replicate the kurtosis of the marginal distributions $X_{i}$ (discussed further below) of two important and distinct limiting cases where localization does and does not appear, respectively: when $\\mathbf{X}$ has support on the vertices of the hypercube ${\\{\\pm1\\}}^{N}$ (satisfied by Ising for any $J$ ), and when $\\mathbf{X}$ is Gaussian (satisfied by NLGP with $g\\approx0$ ). ", "page_idx": 4}, {"type": "text", "text": "The gradient flow in Lemma 3.1 also relies on Assumption A3 that Lindeberg\u2019s condition holds for the sequence $w_{i}X_{i}$ , which ensures that no single term $w_{i}X_{i}$ in the sequence can dominate. If this holds, then we can conclude that $\\langle\\mathbf{w},\\mathbf{X}\\rangle\\mid X_{i}$ is approximately Gaussian. As we discuss in Appendix C.2, this is almost always satisfied for a Gaussian initialization of w, and for slight deviations therefrom, and is satisfied by the settings of Ingrosso and Goldt [IG22]. Using this fact, we obtain an explicit form for the gradient flow early in training, stated in Lemma 3.1. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Lemma 3.1. Under Assumptions $_{A I}$ and $A2$ , the gradient flow for the single ReLU neuron in M2 early in training with $y=0,1$ trained using MSE loss is ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{2}{\\tau}\\frac{\\mathrm{d}\\mathbf{w}}{\\mathrm{d}t}=\\varphi\\left(\\frac{\\Sigma_{1}\\mathbf{w}}{\\sqrt{\\langle\\Sigma_{1}\\mathbf{w},\\mathbf{w}\\rangle}}\\right)-(\\Sigma_{0}+\\Sigma_{1})\\mathbf{w}+o_{N}(1),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $o_{N}(1)$ vanishes as $N\\to\\infty,$ , and where $\\varphi:(-1,1)\\to\\mathbb{R}$ is defined as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\varphi(a)=\\mathbb{E}_{X_{1}|Y=1}\\left[X_{1}\\operatorname{erf}\\left(X_{1}\\operatorname{alg}^{-1}(a)/\\sqrt{2}\\right)\\right]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and $\\arg^{-1}(x)=x/\\sqrt{1-x^{2}}$ , the inverse of the algebraic sigmoid function $\\arg(x)=x/{\\sqrt{1+x^{2}}}$ . ", "page_idx": 5}, {"type": "text", "text": "Lemma 3.1 reduces the study of higher-order statistics to the marginal distributions, $X_{1}$ , where, by translation invariance, all marginals have the same distribution, so we refer to $X_{1}$ without loss of generality. While Lemma 3.1 technically only holds early in training and breaks down if w becomes localized due to violation of A3, the gradient flow in Eq. (5) holds sufficiently long to detect the emergence of localization in the weights w. In particular, numerically integrating Eq. (5) yields localized weights w as $t\\to\\infty$ . Moreover, the location of the peak of final weights from Eq. (5) corresponds closely to the actual peak of the weight, when we observe localization; see Section 4.2 for empirical validation of this fact. The primary difference observed is that the localized bump from Eq. (5) is less peaked than when computed exactly; see Fig. 3 for a comparison between experimentally observed localized receptive fields and theoretical predictions. ", "page_idx": 5}, {"type": "text", "text": "3.2 Necessary and sufficient conditions for emergent localization ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To establish an exact threshold at which localization emerges requires solving Eq. (5), which is not possible exactly for general nonlinear differential equations.3Nevertheless, the form of Eq. (5) reveals that localization is driven solely by the first term. Indeed, the second term depends only on the second-order statistics of the data, and so can be held fixed as $\\mathbf{X}$ is varied from a distribution that induces localization to one that does not. Secondly, one can see that the first term in Eq. (5) does not change as w is scaled, in contrast to the second term. As such, the second term in Eq. (5) serves to constrain the scale of w, distinct from localization, while the first is primarily concerned with the shape of w, and thus localization. This further motivates the first term, and thus $\\varphi$ , which we will refer to as the amplifier and which itself depends on properties of the data distribution $p(\\mathbf{X})$ , as a focus of study for understanding localization. ", "page_idx": 5}, {"type": "text", "text": "We present an analysis of $\\varphi$ in Appendix B.1 that reveals the role of the marginal distribution of the data in driving localization. For each marginal, $\\varphi(a)\\approx({\\sqrt{2/\\pi}})a$ for $a\\approx0$ . For larger $a,\\varphi$ depends more strongly on the data distribution and can be super-linear (sub-linear), i.e., greater (smaller) than $({\\sqrt{2/\\pi}})a$ . Super-linear $\\varphi$ encourage entries in w that are large in some neighborhood to grow faster than those that are smaller, yielding localization. Linear and sub-linear $\\varphi$ are the opposite, encouraging oscillatory or flat weights by suppressing neighborhoods in w. However, super- and sub-linearity may not hold uniformly, as $\\varphi$ can be both over its domain (see Fig. 3, bottom row, black line). As an approximation, we consider a third-order Taylor expansion (red lines in Fig. 3, second column), which reveals that for the canonical setting of $\\sigma^{2}=1$ , negative excess kurtosis of the marginals yields super-linearity, while positive excess kurtosis yields sub-linearity; see Appendix B.1. This leads us to the following claim, which is validated by our simulations in Section 4: ", "page_idx": 5}, {"type": "text", "text": "Claim 3.2. For sufficiently large $N$ , if the data $\\mathbf{X}\\,\\in\\,\\mathbb{R}^{N}$ satisfies conditions $_{S I}$ to $S3$ and has marginal distributions with sufficiently negative excess kurtosis, then Model M2 will learn localized receptive fields. Conversely, if the excess kurtosis is sufficiently positive, it will not. ", "page_idx": 5}, {"type": "text", "text": "As a minimal positive example, the distribution with the most negative excess kurtosis is the symmetric Bernoulli, with a value of $-2$ . In our setting, this corresponds to a data vector $\\mathbf{X}$ with support on the vertices of the hypercube, ${\\{\\pm1\\}}^{N}$ . As mentioned above, it can be seen from the law of total covariance combined with sign-symmetry that A1 and A2 hold exactly. Note that $\\varphi$ is the same for all such distributions, which leads us to Claim 3.2 that any distribution satisfying conditions S1 to S3 whose marginals are maximally concentrated will induce a localized receptive field in M2. Importantly, this claim includes the limiting case of Ingrosso and Goldt [IG22], $g\\rightarrow\\infty$ in NLGP. It also includes the Ising model as another example, corroborating an observation for restricted Boltzmann machines $[\\mathrm{Har}{+}20]$ that Ising data induces localization in a learning model. These claims are validated for the single-neuron model in Fig. 3 and in Section 4 for the many-neuron model. ", "page_idx": 5}, {"type": "image", "img_path": "nw9JmfL99s/tmp/84dd7dc031ccebdb498f3404d32513242cc5ae0d476f20b66cfd16c87245a4cf.jpg", "img_caption": ["Figure 3: From left and for the same Ising, NLGP, and Kur data models as in Fig. 2: the marginals $p(X_{i})$ , the amplifier $\\varphi$ defined in Theorem 3.1 and kurtosis $\\kappa$ , and the evolution of simulated receptive fields for the single-neuron model (M2) trained on its data, and lastly the receptive field given by numerically integrating Eq. (5) with $\\varphi$ expanded to a third-order Taylor approximation for the same data; training or evolution time is indicated by line color (blue for early-time; red for late-time). See Section 4.1 for exposition. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "3.3 Case study: Elliptical distributions fail to produce localization ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Above, we assume weak dependence (S1) as it enables a focus on how the marginals control localization. As a first investigation into departures from this regime, we consider data $\\mathbf{X}$ sampled from an elliptical distribution, where weak dependence may not hold. We specialize the definition of an elliptical distribution [Fra04] to our setting of multiple class labels and sign-symmetry: ", "page_idx": 6}, {"type": "text", "text": "Def 1. Samples $\\mathbf{X}\\in\\mathbb{R}^{N}$ satisfy an elliptical distribution $i f$ we can write $\\mathbf{X}\\mid Y=y\\ {\\stackrel{(d)}{=}}\\ R_{y}\\Lambda_{y}\\mathbf{U}_{y}$ where $R_{y}$ is a nonnegative random variable, $\\boldsymbol{\\Lambda}_{y}\\,\\in\\,\\mathbb{R}^{N\\times D}$ is such that \u039by\u039by\u22a4 $\\Lambda_{y}\\Lambda_{y}^{\\top}\\,=\\,\\Sigma_{y},$ and $\\mathbf{U}_{y}$ is independent of $R_{y}$ and uniformly distributed on the $D$ -dimensional sphere. ", "page_idx": 6}, {"type": "text", "text": "The class of elliptical distributions is broad, imposing only the constraint that the contours of the density be ellipses; the multivariate Gaussian and Student- $\\cdot t$ distributions are examples. As such, they can vary greatly in measures of non-Gaussianity, including kurtosis, while maintaining enough structure for analytical convenience. Proposition 3.3 states that training on elliptical data prevents localization in the single ReLU neuron model. ", "page_idx": 6}, {"type": "text", "text": "Proposition 3.3. Assume the data $\\mathbf{X}$ are sign-symmetric (S3), translation-invariant (S2), and follow an elliptical distribution such that the MSE on the task in Section 2.3 is always finite. If $\\Sigma_{0},\\Sigma_{1}$ are such that the ratio of their i-th eigenvalues, $\\lambda_{i}(\\Sigma_{0})/\\lambda_{i}(\\Sigma_{1})$ , assumes a particular value for at most two distinct i, then the steady states of the weight of $M2$ are sinusoids, i.e., not localized. ", "page_idx": 6}, {"type": "text", "text": "The condition on the number $i$ such that the ratio of the $i$ -th eigenvalues are the same constrains the number of Fourier components that can be non-zero in the steady state of w. While opaque, this requirement seems to always hold in practice, as even slight increases in length-scale correlation can dramatically change the spectrum of $\\Sigma_{y}$ . ", "page_idx": 6}, {"type": "image", "img_path": "nw9JmfL99s/tmp/7f6560db4b05d1996fb37bdb9618ec3ed74481a856748d4bc0a6f536eb460a5b.jpg", "img_caption": ["Figure 4: Evolution of receptive fields learned by the single-neuron model (M2), along with sinusoids fti to final states (red dashes) when trained on data from three elliptical distributions: $t_{40}(\\nu=3)$ (left), the surface of an ellipse (middle), and a custom elliptical distribution that places its mass near the outside of an ellipse (right). In all cases, the learned receptive field is oscillatory (a sinusoid), as predicted by Proposition 3.3. The $\\ell_{2}$ distances between the ftited oscillatory weights and empirical RFs, as a ratio of the $\\ell_{2}$ norm of the empirical RFs, are (left) $9.77\\%$ , (center) $3.75\\%$ , and (right) $4.14\\%$ . See Section 4.3 for exposition. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "The proposition is surprising because it reveals that the kurtosis of the preactivation is not an appropriate metric for explaining localization. Consider the example of the $N$ -dimensional Student- $\\cdot t$ distribution with $\\nu$ degrees of freedom, $t_{N}(\\nu)$ . If $\\mathbf{X}\\sim{t_{N}(\\nu)}$ , then $\\langle w,\\mathbf{X}\\rangle\\,\\sim\\,t_{1}(\\nu)$ . Note the kurtosis of $t_{1}(\\nu)$ is non-zero, and can be very large or even infinite for small $\\nu$ . This prediction is validated in Section 4.3. The condition also reveals that not all symmetries in the data (here, elliptical symmetry) induce structure in the trained model weights, if localization is to be seen as a sparsity more structured than oscillatory weights [cf. God+23]; indeed, translational symmetry (S2) is more relevant for localization than elliptical symmetry. ", "page_idx": 7}, {"type": "text", "text": "4 Experimental results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We describe experiments to validate the generalizability of the analytical results from Section 3. We run all experiments on a single CPU machine locally or on a compute cluster. Since all datasets are procedurally generated, training depends on both the model architecture and the complexity of sampling the data, but is between 10 and 60 minutes for any single simulation run. ", "page_idx": 7}, {"type": "text", "text": "4.1 Validating Claim 3.2 with positive and negative predictions ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In Fig. 5, we validate Claim 3.2 first via the single-neuron model (M2) with 30 initial conditions trained across a range of excess kurtoses for the ${\\tt N L G P}(g)$ and $\\mathtt{K u r}(k)$ data models. We use the inverse participation ratio (IPR), defined in Appendix A.3. This measure, also used by Ingrosso and Goldt [IG22], is large when proportionally few weight dimensions \u201cparticipate\u201d (have large magnitude), and small when weight dimension magnitudes are more uniform. We see that when $g$ and $k$ assume values that yield a negative excess kurtosis, IPR is close to its maximum of 1.0, suggesting the weights are localized; if the excess kurtosis is positive, IPR is nearly zero, suggesting the weights are not localized. The IPR is extremely consistent across random initializations, suggesting that localization is determined by data statistics and not initialization. The trend in IPR vs. excess kurtosis is very similar between the ${\\tt N L G P}(g)$ and $\\mathtt{K u r}(k)$ data models, demonstrating that excess kurtosis is a primary driver of localization and localization is largely independent from other properties of the data distribution. ", "page_idx": 7}, {"type": "image", "img_path": "nw9JmfL99s/tmp/ebad5ee2e76018ac127e0857f15932eb3d89f712b71bf00b347594b5197c54a7.jpg", "img_caption": ["Figure 5: IPR vs. excess kurtosis for NLGP and Kur data models, with mean and std. dev. across 30 re-initializations for the single-neuron model (M2); error bars are small and may not be visible. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 3 further validates Claim 3.2 with specific examples. We maintain constant initial conditions for our model and train on the Ising, $\\mathtt{N L G P}(g=0.01)$ , and $\\mathtt{K u r}(k\\ =\\ 5)$ data models. The marginals of the Ising model have an excess kurtosis of $-2$ , the smallest possible value for any distribution. As a result, we see that the amplifier $\\varphi$ for Ising (top left) is super-linear (the dark line exceeds the dashed light line for larger $a$ ), which drives localization via its role in Eq. (5). Integrating Eq. (5) with $\\varphi$ expanded via a third-order Taylor approximation (red line) yields a similar localized receptive field to that from simulation (two right panels), validating this approximation. ", "page_idx": 7}, {"type": "image", "img_path": "nw9JmfL99s/tmp/26f9ff4c6bafff8b95fd1945c6c52628b03a8424a9382eee3f496e7432b232c6.jpg", "img_caption": ["Figure 6: (Left, Center) Receptive fields learned by many-neuron (M1) soft committee machines (second-layer weights fixed at $\\frac{1}{K}$ ) trained on the $\\mathtt{K u r}(10)$ and $\\mathtt{K u r}(4)$ datasets, respectively. The models had $N=40$ input units, $K=10$ hidden units, and an initialization variance of 0.1. (Right) A random subset of 10 components from the 40 learned by the FastICA algorithm from scikit-learn [Hyv99; Ped+11] on the $\\mathtt{K u r}(3)$ dataset with length-scale correlation values of $\\xi_{0}=1$ and $\\xi_{1}=3$ . See Section 4.4 for exposition. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "For the remaining distributions (middle and bottom rows) that elicit oscillatory (sinusoidal) weights, Claim 3.2 is validated due to their positive excess kurtosis. The dynamical steady state (far right) assumes a more negative value than in the simulation (to the left), a difference that is the result of deviations of our early-time gradient flow in Eq. (5), but these deviations remain mild enough nevertheless to recover the qualitative structure of the learned receptive field. ", "page_idx": 8}, {"type": "text", "text": "4.2 Validating Eq. (5) with localization position prediction ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The simulated and integrated receptive fields in Fig. 3 demonstrate that our analytical model is able to meaningfully reproduce localization in receptive fields from neural network training. For the Ising model, we see that the integration even has a peak in the exact same position as the simulation (at index $i=6$ ), suggesting precision in our approximation. Indeed, we simulated the condition in Fig. 3 for the Ising model under 28 different initial conditions (weight initializations), and found that in 26 of them $(93\\%)$ , the peaks of the integrated and simulated receptive fields matched exactly. In the two cases where the peaks differed, they did so substantially (see Fig. 8 for an example). ", "page_idx": 8}, {"type": "text", "text": "4.3 Validating Proposition 3.3: Elliptical distributions fail to localize ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Proposition 3.3 claims that the single-neuron model (M2) trained on elliptical data will yield sinusoidal receptive fields, subject to a condition on the spectra of $\\Sigma_{0}$ and $\\Sigma_{1}$ . We verify this claim in Fig. 4 with three distinct elliptical distributions. The first, $t_{40}(\\nu=3)$ , gives preactivations $\\langle\\mathbf{w},\\mathbf{X}\\rangle$ that have infinite kurtosis, yet our theory predicts the final receptive field will be sinusoidal. This is confirmed in Fig. 4, where the learned receptive field is indeed a sinusoid with period 1 and intercept at zero. ", "page_idx": 8}, {"type": "text", "text": "We also consider data sampled from the surface of an ellipse, which is done by fixing $R_{y}\\equiv1$ in Definition 1. Here, we observe that the learned receptive field is a near-constant function at $-0.04$ (note that $\\cos(2\\pi\\cdot0\\cdot x)\\equiv1$ is a sinusoid, allowing nonzero intercepts and constant functions). Finally, we consider an unconventional elliptical distribution where the density of $R$ is given by $p_{R}(r)\\dot{=}(4e^{2r+4})/(e^{2r}+e^{4})^{2}\\cdot\\mathbb{1}(r\\geq2)$ . This particular density places most of its mass near $r=2$ before rapidly falling off, imposing a minimum norm on $\\mathbf{X}$ and pushing support near the surface of an ellipse. This distribution, too, yields an oscillatory steady state, as shown in Fig. 4 (right). We confirm our visual observations by fitting sinusoids to the final receptive fields and see the relative errors are quite low. ", "page_idx": 8}, {"type": "text", "text": "4.4 Extensions to many-neuron model and ICA ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "All of our analysis thus far has concerned single-neuron models with ReLU activation and without hidden-to-output or bias terms, assumptions which were made to make our analysis tractable. Here, we depart from that regime by considering the SCM and the full two-layer network (Model M1). In Fig. 6 (left) and (center), we train a SCM with 10 hidden units and sigmoid activation on the $\\mathtt{K u r}(10)$ and $\\mathtt{K u r}(4)$ datasets, which have excess kurtoses of $-0.93$ and 3.28, respectively. So, based on our single-neuron analysis, we do and do not expect to see localization for these distributions. Indeed, this is precisely what we observe in Fig. 6, where the receptive fields are sharply localized for the former distribution, while they look like low-frequency oscillations for the latter. ", "page_idx": 8}, {"type": "image", "img_path": "nw9JmfL99s/tmp/e9955129f5c077f9789e9c466f2e5ce0f6c031b84ec02b5a9751f0971b29562c.jpg", "img_caption": ["Figure 7: Receptive fields learned by the manyneuron model (M1) with learnable second-layer weights, $N\\,=\\,40$ , $K\\,=\\,10$ . (Top) A random subset of 4 receptive fields from a model with sigmoid activation, trained on Kur(4) (positive excess kurtosis of 3.28). As predicted by Claim 3.2, the receptive fields are not localized, and appear as high-frequency oscillations. (Bottom) A random subset of 4 receptive fields from a model with ReLU activation, trained on $\\mathtt{K u r}(30)$ (negative excess kurtosis of $-1.17)$ . Receptive fields are localized (left three) or exhibit low-frequency oscillations (right). See Section 4.4 for exposition. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "In Fig. 7, we train many-neuron models with $N=$ 40 input units and $K=10$ hidden units, where all weights are learnable. In general, adding flexibility in the second layer leads to more varied structure in the first layer. We train on $\\mathtt{K u r}(4)$ (top), which has an excess kurtosis of 3.28, and $\\mathtt{K u r}(30)$ (bottom), which has an excess kurtosis of $-1.17$ . The receptive fields from the former are not localized, as in the singleneuron model; however, they appear more like highfrequency oscillations than low-frequency sinusoids. For $\\mathtt{K u r}(30)$ , where we expect localization, we see that the first three receptive fields exhibit localization, but less so than for a single neuron. Importantly, not all receptive fields are localized, a result of a variable second-layer weight effectively changing the variance $\\sigma^{2}$ in the third-derivative term in Lemma B.1. ", "page_idx": 9}, {"type": "text", "text": "We further compare these predictions against ICA, another framework that has been used to model receptive fields in visual cortex. We train on the $\\mathtt{K u r}(3)$ dataset, which has marginals with excess kurtosis 7.66, ftiting 10 components using the FastICA implementation from scikit-learn [sc; HO00]. We observe in Fig. 6 (right) that we learn localized receptive fields; this contrasts our neural network models, which require negative excess kurtosis. This stems from ICA\u2019s objective to maximize non-Gaussianity, regardless of how specifically it is done. The sign of the excess kurtosis is irrelevant, so long as it is nonzero. This deviation between our analytical model and ICA is an interesting avenue for future study, perhaps by validation with natural images. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We derive effective learning dynamics for the minimal example of emergent localization in a neural receptive field given by Ingrosso and Goldt [IG22]. The analytical approach we take relies on the assumption that the conditional preactivation is Gaussian, a refinement of previous work that assumes Gaussianity of the unconditioned preactivation as asserted by the Gaussian equivalence property targeted by Ingrosso and Goldt [IG22]. This approach may prove extensible beyond our specialized setting and may enable further analysis of how statistics of an input task drive emergent structure in neural network learning. ", "page_idx": 9}, {"type": "text", "text": "Emergence as an alternative mechanism to top-down constraints like sparsity is in line with recent work that reformulates data-distributional properties as a driver for complex behavior $[\\mathrm{Cha}+22]$ . Via these analytical effective dynamics, we observe that specific data properties\u2014the covariance structure and the marginals\u2014shape localization in neural receptive fields. Though we cannot capture dynamical interactions between neurons that may shape receptive fields in other settings with the single-neuron analytical model, our empirical validations with many neurons suggest that these interactions do not, in fact, play a significant role in shaping localization [cf. Har $+20$ ]. ", "page_idx": 9}, {"type": "text", "text": "The data model we consider is a simplified abstraction of the task faced by early sensory systems, and, as a consequence, we do not yet capture certain features of receptive fields that are observed in early sensory systems. In particular, we do not observe orientation nor phase selectivity, features of simple-cell receptive fields in early sensory cortices and in artificial neural networks that can be seen in a subset of receptive fields in Fig. 1 (left and center, respectively). To capture orientation selectivity, it may be fruitful to follow the approach of Karklin and Simoncelli [KS11], who tie orientation selectivity in a population-based efficient-coding framework to the presence of noise. Furthermore, on-center-off-surround-filtering input data, including the idealized data, gives receptive fields with subfields in our simulations, but is difficult to analyze. Lastly, we do not yet look at the distribution of receptive field shapes and do not validate against other models of receptive field learning beyond a brief comparison with ICA [cf. Sax+11], but these are exciting avenues for future work. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported by a Schmidt Science Polymath Award to A.S., and the Sainsbury Wellcome Centre Core Grant from Wellcome (219627/Z/19/Z) and the Gatsby Charitable Foundation (GAT3850). A.S. is a CIFAR Azrieli Global Scholar in the Learning in Machines & Brains program. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[Bar+08] J.-M. Bardet, P. Doukhan, G. Lang, and N. Ragache. \u201cDependent Lindeberg central limit theorem and some applications\u201d. In: ESAIM: Probability and Statistics 12 (2008), pp. 154\u2013172 (cit. on p. 3).   \n[Bar93] A. R. Barron. \u201cUniversal approximation bounds for superpositions of a sigmoidal function\u201d. In: IEEE Transactions on Information theory 39.3 (1993), pp. 930\u2013945 (cit. on p. 2).   \n[Bra07] R. C. Bradley. \u201cIntroduction to strong mixing conditions\u201d. In: (No Title) (2007) (cit. on p. 17).   \n[BS97] A. J. Bell and T. J. Sejnowski. \u201cThe \"Independent Components\" of Natural Scenes Are Edge Filters\u201d. In: Vision Research 37.23 (Dec. 1, 1997), pp. 3327\u20133338. URL: https: //www.sciencedirect.com/science/article/pii/S0042698997001211 (cit. on pp. 1, 4).   \n[Cha+22] S. C. Y. Chan, A. Santoro, A. K. Lampinen, J. X. Wang, A. Singh, P. H. Richemond, J. McClelland, and F. Hill. \u201cData Distributional Properties Drive Emergent In-Context Learning in Transformers\u201d. In: Advances in Neural Information Processing Systems. Conference on Neural Information Processing Systems. Oct. 10, 2022. arXiv: 2205. 05055 [cs]. URL: http://arxiv.org/abs/2205.05055 (cit. on p. 10).   \n[Che+20] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton. \u201cA Simple Framework for Contrastive Learning of Visual Representations\u201d. In: Proceedings of the 37th International Conference on Machine Learning. International Conference on Machine Learning. Pmlr, Nov. 21, 2020, pp. 1597\u20131607. URL: https://proceedings.mlr.press/v119/ chen20j.html (cit. on p. 4).   \n$[\\mathrm{Cro}{+}11]$ S. Crochet, J. F. Poulet, Y. Kremer, and C. C. Petersen. \u201cSynaptic Mechanisms Underlying Sparse Coding of Active Touch\u201d. In: Neuron 69.6 (Mar. 2011), pp. 1160\u20131175. URL: https://linkinghub.elsevier.com/retrieve/pii/S0896627311001206 (cit. on p. 1).   \n[Dac+00] D. Dacey, O. S. Packer, L. Diller, D. Brainard, B. Peterson, and B. Lee. \u201cCenter Surround Receptive Field Structure of Cone Bipolar Cells in Primate Retina\u201d. In: Vision Research 40.14 (June 1, 2000), pp. 1801\u20131811. URL: https://www.sciencedirect. com/science/article/pii/S0042698900000390 (cit. on p. 1).   \n[dBM98] R. C. deCharms, D. T. Blake, and M. M. Merzenich. \u201cOptimizing Sound Features for Cortical Neurons\u201d. In: Science 280.5368 (1998), pp. 1439\u20131444. eprint: https: //www.science.org/doi/pdf/10.1126/science.280.5368.1439. URL: https://www.science.org/doi/abs/10.1126/science.280.5368.1439 (cit. on p. 2).   \n[Doi+12] E. Doi, J. L. Gauthier, G. D. Field, J. Shlens, A. Sher, M. Greschner, T. A. Machado, L. H. Jepson, K. Mathieson, D. E. Gunning, A. M. Litke, L. Paninski, E. J. Chichilnisky, and E. P. Simoncelli. \u201cEfficient Coding of Spatial Information in the Primate Retina\u201d. In: The Journal of Neuroscience 32.46 (Nov. 14, 2012), pp. 16256\u201316264. URL: https: //www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.4036- 12.2012 (cit. on p. 1).   \n[DWZ03] M. R. DeWeese, M. Wehr, and A. M. Zador. \u201cBinary Spiking in Auditory Cortex\u201d. In: The Journal of Neuroscience 23.21 (Aug. 27, 2003), pp. 7940\u20137949. URL: https: //www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.23-21-07940.2003 (cit. on p. 1).   \n[EC24] O. Elkabetz and N. Cohen. \u201cContinuous vs. discrete optimization of deep neural networks\u201d. In: Proceedings of the 35th International Conference on Neural Information Processing Systems. Curran Associates Inc., 2024 (cit. on p. 17).   \n[Fie99] D. J. Field. \u201cWavelets, Vision and the Statistics of Natural Scenes\u201d. In: Philosophical Transactions of the Royal Society of London. Series A: Mathematical, Physical and Engineering Sciences 357.1760 (Sept. 1999). Ed. by B. W. Silverman and J. C. Vassilicos, pp. 2527\u20132542. URL: https://royalsocietypublishing.org/doi/10. 1098/rsta.1999.0446 (cit. on p. 1).   \n[Fra04] G. Frahm. \u201cGeneralized elliptical distributions: theory and applications\u201d. PhD thesis. Universit\u00e4t zu K\u00f6ln, 2004 (cit. on pp. 7, 19).   \n[Ger+20] F. Gerace, B. Loureiro, F. Krzakala, M. M\u00e9zard, and L. Zdeborov\u00e1. \u201cGeneralisation error in learning with random features and the hidden manifold model\u201d. In: Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event. Vol. 119. Proceedings of Machine Learning Research. PMLR, 2020, pp. 3452\u20133462. URL: http://proceedings.mlr.press/v119/gerace20a.html (cit. on pp. 5, 16).   \n[GG84] S. Geman and D. Geman. \u201cStochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images\u201d. In: IEEE Transactions on Pattern Analysis and Machine Intelligence Pami-6.6 (Nov. 1984), pp. 721\u2013741. URL: https://ieeexplore.ieee. org/document/4767596 (cit. on p. 4).   \n[God+23] C. Godfrey, D. Brown, T. Emerson, and H. Kvinge. On the Symmetries of Deep Learning Models and Their Internal Representations. Mar. 24, 2023. arXiv: 2205.14258 [cs]. URL: http://arxiv.org/abs/2205.14258. preprint (cit. on p. 8).   \n$[\\mathrm{Gol}+19]$ S. Goldt, M. Advani, A. M. Saxe, F. Krzakala, and L. Zdeborov\u00e1. \u201cDynamics of stochastic gradient descent for two-layer neural networks in the teacher-student setup\u201d. In: Advances in neural information processing systems 32 (2019) (cit. on p. 2).   \n$[\\mathrm{Gol}+20]$ S. Goldt, M. M\u00e9zard, F. Krzakala, and L. Zdeborov\u00e1. \u201cModelling the Influence of Data Structure on Learning in Neural Networks: The Hidden Manifold Model\u201d. In: Physical Review X 10.4 (Dec. 3, 2020), p. 041044. arXiv: 1909.11500 [cond-mat, stat]. URL: http://arxiv.org/abs/1909.11500 (cit. on pp. 2, 5, 16).   \n$[\\mathrm{Gol}+22]$ S. Goldt, B. Loureiro, G. Reeves, F. Krzakala, M. M\u00e9zard, and L. Zdeborov\u00e1. \u201cThe gaussian equivalence of generative models for learning with shallow neural networks\u201d. In: Mathematical and Scientific Machine Learning. Pmlr. 2022, pp. 426\u2013471 (cit. on pp. 5, 16).   \n[Har+20] M. Harsh, J. Tubiana, S. Cocco, and R. Monasson. \u201c\u2019Place-cell\u2019 Emergence and Learning of Invariant Data with Restricted Boltzmann Machines: Breaking and Dynamical Restoration of Continuous Symmetries in the Weight Space\u201d. In: Journal of Physics A: Mathematical and Theoretical 53.17 (Apr. 2020), p. 174002. URL: https://dx.doi. org/10.1088/1751-8121/ab7d00 (cit. on pp. 7, 10).   \n[HDZ08] T. Hrom\u00e1dka, M. R. DeWeese, and A. M. Zador. \u201cSparse Representation of Sounds in the Unanesthetized Auditory Cortex\u201d. In: PLOS Biology 6.1 (Jan. 29, 2008), e16. URL: https://journals.plos.org/plosbiology/article?id=10.1371/journal. pbio.0060016 (cit. on p. 1).   \n[HHH09] A. Hyv\u00e4rinen, J. Hurri, and P. O. Hoyer. Natural Image Statistics. Red. by M. Viergever, G. Borgefors, R. Deriche, T. S. Huang, K. Ikeuchi, T. Jiang, R. Klette, A. Leonardis, H.-O. Peitgen, and J. K. Tsotsos. Vol. 39. Computational Imaging and Vision. London: Springer, 2009. URL: http://link.springer.com/10.1007/978-1-84882-491- 1 (cit. on p. 3).   \n[HO00] A. Hyv\u00e4rinen and E. Oja. \u201cIndependent Component Analysis: Algorithms and Applications\u201d. In: Neural Networks 13.4 (June 1, 2000), pp. 411\u2013430. URL: https: //www.sciencedirect.com/science/article/pii/S0893608000000265 (cit. on pp. 2, 5, 10).   \n[HW59] D. H. Hubel and T. N. Wiesel. \u201cReceptive Fields of Single Neurones in the Cat\u2019s Striate Cortex\u201d. In: The Journal of Physiology 148.3 (Oct. 1959), pp. 574\u2013591. pmid: 14403679 (cit. on p. 1).   \n[HW68] D. H. Hubel and T. N. Wiesel. \u201cReceptive Fields and Functional Architecture of Monkey Striate Cortex\u201d. In: The Journal of Physiology 195.1 (Mar. 1968), pp. 215\u2013243. pmid: 4966457 (cit. on p. 1).   \n[Hyv99] A. Hyvarinen. \u201cFast and Robust Fixed-Point Algorithms for Independent Component Analysis\u201d. In: IEEE Transactions on Neural Networks 10.3 (May 1999), pp. 626\u2013634. URL: http://ieeexplore.ieee.org/document/761722/ (cit. on p. 9).   \n[IG22] A. Ingrosso and S. Goldt. \u201cData-driven emergence of convolutional structure in neural networks\u201d. In: Proceedings of the National Academy of Sciences 119.40 (2022), e2201854119 (cit. on pp. 2\u20138, 10, 16, 20).   \n[KK78] E. I. Knudsen and M. Konishi. \u201cSpace and frequency are represented separately in auditory midbrain of the owl\u201d. In: Journal of Neurophysiology 41.4 (1978), pp. 870\u2013884 (cit. on p. 1).   \n[KS11] Y. Karklin and E. Simoncelli. \u201cEfficient Coding of Natural Images with a Population of Noisy Linear-Nonlinear Neurons\u201d. In: Advances in Neural Information Processing Systems. Vol. 24. Curran Associates, Inc., 2011. URL: https://proceedings.neurips. cc/paper/2011/hash/12e59a33dea1bf0630f46edfe13d6ea2-Abstract.html (cit. on p. 10).   \n[KSH12] A. Krizhevsky, I. Sutskever, and G. E. Hinton. \u201cImageNet Classification with Deep Convolutional Neural Networks\u201d. In: Advances in Neural Information Processing Systems. Vol. 25. Curran Associates, Inc., 2012. URL: https : / / proceedings . neurips . cc / paper % 5C % 5Ffiles / paper / 2012 / hash / c399862d3b9d6b76c8436e924a68c45b-Abstract.html (cit. on p. 2).   \n[KZB19] A. Kolesnikov, X. Zhai, and L. Beyer. Revisiting Self-Supervised Visual Representation Learning. Jan. 25, 2019. arXiv: 1901.09005 [cs]. URL: http://arxiv.org/abs/ 1901.09005. preprint (cit. on p. 4).   \n[MMN18] S. Mei, A. Montanari, and P.-M. Nguyen. \u201cA mean field view of the landscape of twolayer neural networks\u201d. In: Proceedings of the National Academy of Sciences 115.33 (2018), E7665\u2013e7671 (cit. on p. 2).   \n[NS08] C. M. Niell and M. P. Stryker. \u201cHighly Selective Receptive Fields in Mouse Visual Cortex\u201d. In: The Journal of Neuroscience 28.30 (July 23, 2008), pp. 7520\u20137536. URL: https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.0623- 08.2008 (cit. on p. 1).   \n[OF96] B. A. Olshausen and D. J. Field. \u201cEmergence of Simple-Cell Receptive Field Properties by Learning a Sparse Code for Natural Images\u201d. In: Nature 381.6583 (June 1, 1996), pp. 607\u2013609. URL: https://doi.org/10.1038/381607a0 (cit. on pp. 1, 4).   \n[OF97] B. A. Olshausen and D. J. Field. \u201cSparse Coding with an Overcomplete Basis Set: A Strategy Employed by V1?\u201d In: Vision Research 37.23 (Dec. 1997), pp. 3311\u20133325. URL: https://linkinghub.elsevier.com/retrieve/pii/ S0042698997001697 (cit. on p. 1).   \n[Ped+11] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. \u201cScikit-learn: Machine Learning in Python\u201d. In: Journal of Machine Learning Research 12 (2011), pp. 2825\u20132830 (cit. on p. 9).   \n[Pin99] A. Pinkus. \u201cApproximation theory of the MLP model in neural networks\u201d. In: Acta numerica 8 (1999), pp. 143\u2013195 (cit. on p. 2).   \n[Rin02] D. L. Ringach. \u201cSpatial Structure and Symmetry of Simple-Cell Receptive Fields in Macaque Primary Visual Cortex\u201d. In: Journal of Neurophysiology 88.1 (July 2002), pp. 455\u2013463. URL: https://journals.physiology.org/doi/full/10.1152/ jn.2002.88.1.455 (cit. on pp. 1, 2).   \n[Ros56] M. Rosenblatt. \u201cA central limit theorem and a strong mixing condition\u201d. In: Proceedings of the national Academy of Sciences 42.1 (1956), pp. 43\u201347 (cit. on p. 3).   \n[RSH02] D. L. Ringach, R. M. Shapley, and M. J. Hawken. \u201cOrientation Selectivity in Macaque v1: Diversity and Laminar Dependence\u201d. In: The Journal of Neuroscience 22.13 (July 1, 2002), pp. 5639\u20135651. URL: https://www.jneurosci.org/lookup/doi/10. 1523/JNEUROSCI.22-13-05639.2002 (cit. on p. 1).   \n[RT95] E. T. Rolls and M. J. Tovee. \u201cSparseness of the Neuronal Representation of Stimuli in the Primate Temporal Visual Cortex\u201d. In: Journal of Neurophysiology 73.2 (Feb. 1, 1995), pp. 713\u2013726. URL: https://www.physiology.org/doi/10.1152/jn. 1995.73.2.713 (cit. on p. 1).   \n[Sax+11] A. Saxe, M. Bhand, R. Mudur, B. Suresh, and A. Ng. \u201cUnsupervised Learning Models of Primary Cortical Receptive Fields and Receptive Field Plasticity\u201d. In: Advances in Neural Information Processing Systems. Vol. 24. Curran Associates, Inc., 2011. URL: https : / / proceedings . neurips . cc / paper / 2011 / hash / e19347e1c3ca0c0b97de5fb3b690855a-Abstract.html (cit. on pp. 1, 10).   \n[Sen+18] A. Sengupta, C. Pehlevan, M. Tepper, A. Genkin, and D. Chklovskii. \u201cManifold-Tiling Localized Receptive Fields Are Optimal in Similarity-Preserving Neural Networks\u201d. In: Advances in Neural Information Processing Systems. Vol. 31. Curran Associates, Inc., 2018. URL: https : / / proceedings . neurips . cc / paper / 2018 / hash / ee14c41e92ec5c97b54cf9b74e25bd99-Abstract.html (cit. on p. 2).   \n$[\\mathrm{Sin}{+}18]$ Y. Singer, Y. Teramoto, B. D. Willmore, J. W. Schnupp, A. J. King, and N. S. Harper. \u201cSensory Cortex Is Optimized for Prediction of Future Input\u201d. In: eLife 7 (June 18, 2018). Ed. by J. L. Gallant and S. Kastner, e31557. URL: https://doi.org/10. 7554/eLife.31557 (cit. on p. 2).   \n[SS95] D. Saad and S. A. Solla. \u201cOn-line learning in soft committee machines\u201d. In: Physical Review E 52.4 (1995), p. 4225 (cit. on p. 3).   \n[Vei+22] R. Veiga, L. Stephan, B. Loureiro, F. Krzakala, and L. Zdeborov\u00e1. \u201cPhase Diagram of Stochastic Gradient Descent in High-Dimensional Two-Layer Neural Networks\u201d. In: Advances in Neural Information Processing Systems. Neural Information Processing Systems. Vol. 35. Dec. 6, 2022, pp. 23244\u201323255. URL: https : / / papers . nips . cc / paper % 5C % 5Ffiles / paper / 2022 / hash / 939bb847ebfd14c6e4d3b5705e562054- Abstract- Conference.html (cit. on p. 2).   \n[vHvdS98] J. H. van Hateren and A. van der Schaaf. \u201cIndependent Component Filters of Natural Images Compared with Simple Cells in Primary Visual Cortex\u201d. In: Proceedings of the Royal Society of London. Series B: Biological Sciences 265.1394 (Mar. 7, 1998), pp. 359\u2013366. URL: https://royalsocietypublishing.org/doi/10.1098/ rspb.1998.0303 (cit. on p. 1).   \n[WMG11] B. D. B. Willmore, J. A. Mazer, and J. L. Gallant. \u201cSparse Coding in Striate and Extrastriate Visual Cortex\u201d. In: Journal of Neurophysiology 105.6 (June 2011), pp. 2907\u2013 2919. pmid: 21471391. URL: https://www.ncbi.nlm.nih.gov/pmc/articles/ PMC3118756/ (cit. on p. 1).   \n$[\\mathbf{W}00\\substack{+20}]$ B. Woodworth, S. Gunasekar, J. D. Lee, E. Moroshko, P. Savarese, I. Golan, D. Soudry, and N. Srebro. \u201cKernel and Rich Regimes in Overparametrized Models\u201d. In: Proceedings of Thirty Third Conference on Learning Theory. Conference on Learning Theory. Pmlr, July 15, 2020, pp. 3635\u20133673. URL: https://proceedings.mlr.press/ v125/woodworth20a.html (cit. on p. 2).   \n$[\\mathrm{Yos}{+}15]$ J. Yosinski, J. Clune, A. Nguyen, T. Fuchs, and H. Lipson. \u201cUnderstanding Neural Networks through Deep Visualization\u201d. In: (June 22, 2015). arXiv: 1506.06579 [cs]. URL: http://arxiv.org/abs/1506.06579. preprint (cit. on p. 2).   \n[ZF13] M. D. Zeiler and R. Fergus. \u201cVisualizing and Understanding Convolutional Networks\u201d. In: (Nov. 28, 2013). arXiv: 1311.2901 [cs]. URL: http://arxiv.org/abs/1311. 2901. preprint (cit. on p. 2). ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Definitions and Notation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Notation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We use $[n]$ to refer to the set $\\left\\{i\\in\\mathbb{N}:1\\leq i\\leq n\\right\\}$ . ", "page_idx": 14}, {"type": "text", "text": "A.2 Algebraic sigmoid ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "For $k>0$ , the generalized algebraic sigmoid function is defined as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname{alg}_{k}(x)\\triangleq{\\frac{1}{2}}\\left(1+{\\frac{x}{{\\big(}1+{\\big|}x{\\big|}^{k}{\\big)}^{1/k}}}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Following the main text, we drop the subscript when $k=2$ . ", "page_idx": 14}, {"type": "text", "text": "A.3 Inverse participation ratio (IPR) ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The IPR is defined as: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname{IPR}(\\mathbf{w})\\triangleq\\left(\\sum_{i=1}^{D}w_{i}^{4}\\right)/\\left(\\sum_{i=1}^{D}w_{i}^{2}\\right)^{2},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $w_{i}$ is the magnitude of dimension $i$ of weight w. ", "page_idx": 14}, {"type": "text", "text": "B Extensions beyond the scope of the main text ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Analytical properties of the amplifier $\\varphi$ ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We present several properties of the amplifying function $\\varphi$ defined in Lemma B.1. ", "page_idx": 14}, {"type": "text", "text": "Lemma B.1. The localization amplifier $\\varphi$ in Eq. (6) satisfies $\\varphi(-a)=-\\varphi(a).$ , for all $a\\in(-1,1)$ . Moreover, its derivatives satisfy, where $\\sigma^{2}$ and $\\kappa$ denote the variance and kurtosis of $X_{1}$ , respectively: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\varphi^{\\prime}(0)=\\sqrt{\\frac{2}{\\pi}}\\sigma^{2}\\,,\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ a n d\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\varphi^{\\prime\\prime\\prime}(0)=-\\sqrt{\\frac{2}{\\pi}}(\\kappa^{4}\\sigma^{4}-3\\sigma^{2})\\,\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "To gain some understanding of how the marginal distributions of $\\mathbf{X}$ impact localization, we use the derivatives in Lemma B.1 to construct a third-order Taylor approximation of $\\varphi$ about 0. The derivatives in Lemma B.1 reveal that every distribution for $X_{1}$ with constant variance will look like the same linear function near 0. $\\varphi$ only looks nonlinear once we move sufficiently far away from zero when the third-order term becomes relevant. For the case of $\\sigma^{2}=1$ (where the variance of $X_{1}$ is equal to the value of the larger target $y=1$ ) the third order term suggests that $\\varphi$ is super-linear when $\\kappa<3$ , i.e., the excess kurtosis is positive, and sub-linear otherwise. ", "page_idx": 14}, {"type": "text", "text": "A super-linear $\\varphi$ will encourage entries where $\\Sigma_{1}\\mathbf{w}$ is large to grow at a faster rate than other entries, which are all subject to the same linear norm constraint through the second term in Eq. (5). The covariance $\\Sigma_{1}$ , as a circulant matrix, acts as the convolution operator between some vector and $\\sigma_{1}^{1}$ (the first row in $\\Sigma_{1}$ ). Since $y=1$ corresponds to the class with a larger length-scale correlation, $\\sigma_{1}^{1}$ will decay relatively slowly and act like a weight local average. Thus, $\\Sigma_{1}\\mathbf{w}$ is the weighted local average for each entry in w. So, entries where w is large within some neighborhood will be encouraged to grow faster than those which are smaller, an effect that compounds as Eq. (5) is integrated. Thus, super-linearity encourages localization. ", "page_idx": 14}, {"type": "text", "text": "As we will see in Theorem 3.3 for the setting of elliptical data, if $\\varphi$ is linear, w learns to be sinusoidal, and thus not localized. In the case of sub-linearity, we expect suppression of larger values, rather than promotion, as in the super-linear setting. Thus, to a first approximation, the sign of the excess kurtosis, $\\kappa-3$ (for $\\sigma^{2}=1\\$ ), indicates whether w localizes. ", "page_idx": 14}, {"type": "text", "text": "However, simply studying $\\varphi^{\\prime\\prime\\prime}(0)$ is not sufficient to fully characterize how the marginals impact localization. A function can be sub-linear for small $a$ and super-linear for larger $a$ , making it unclear whether it will yield localization. For marginal distributions where $\\kappa\\approx3$ that do not exhibit strict super- or sub-linearity, this condition is no longer precise enough to determine whether we see localization. ", "page_idx": 14}, {"type": "text", "text": "B.2 PDE limit of Eq. (5) ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "By taking $N$ to be large and treating $w$ as a continuous function with respect to position, i.e., $w=$ $w(x,t)$ , one can treat Eq. (5) as a partial differential equation (PDE). Finding its steady state then amounts to solving ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\varphi\\left(\\frac{\\sigma^{1}\\star w}{\\sqrt{\\langle\\sigma^{1}\\star w,w\\rangle}}\\right)-\\frac{1}{2}(\\sigma^{0}+\\sigma^{1})\\star w\\equiv0\\ ,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $w:[0,1]\\rightarrow\\mathbb{R}$ is periodic and $\\sigma^{y}$ is the convolution corresponding to the limiting case of the matrix $\\Sigma_{y}$ . This equation does not appear to have an explicit solution for non-identity $\\Sigma_{1}$ , and thus, it may not be possible to solve the steady states of Eq. (5) exactly in this PDE limit or for finite $N$ . ", "page_idx": 15}, {"type": "text", "text": "B.3 Assumptions A1 and A2 vs. Gaussian equivalence ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Assumptions A1 and A2 are equivalent to approximating $\\langle\\mathbf{w},\\mathbf{X}\\rangle\\mid X_{i}$ as Gaussian early in training. Similar ideas have been used to derive gradient flow dynamics for neural networks, including in developing the Gaussian equivalence property of $[\\mathrm{Gol}+20\\$ ; Ger $+20$ ; $\\mathrm{Gol}+22$ ]. However, these works model the unconditional preactivation $\\langle\\mathbf{w},\\mathbf{X}\\rangle$ as a Gaussian, rather than first conditioning on $X_{i}$ . How this arises is that these previous works assert the Gaussian approximation prior to differentiating the loss function for the gradient flow dynamics. However, an approximation at that stage neglects the presence of a multiplicative factor $\\mathbf{X}$ that appears as a result of the chain rule applied to $\\langle\\bar{\\bf w},{\\bf X}\\rangle$ . Abstractly, this approach assumes that ${\\mathcal{L}}_{\\mathrm{exact}}\\,\\rightarrow\\,{\\mathcal{L}}_{\\mathrm{Gauss}}$ implies $\\nabla_{\\mathbf{w}}\\,\\mathcal{L}_{\\mathrm{exact}}\\,\\rightarrow\\,\\nabla_{\\mathbf{w}}\\,\\mathcal{L}_{\\mathrm{Gauss}}$ , but, in general, this does not follow, and here in particular this assumption does not capture the interplay of learning and higher-order input statistics. This contributes to the failure of Gaussian equivalence in [IG22]. In contrast, we can account for the additional $\\mathbf{X}$ term in the derivation of Lemma 3.1 by assuming that $\\langle\\mathbf{w},\\mathbf{X}\\rangle\\mid X_{i}$ rather than $\\langle\\mathbf{w},\\mathbf{X}\\rangle$ is Gaussian. This conditioning approach, along with the translation invariance of the data (Property S2), also motivates considering the marginal distributions $X_{i}$ as the object of study to obtain gradient flows for neural networks trained on non-Gaussian inputs. ", "page_idx": 15}, {"type": "text", "text": "C Proofs of theoretical results ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "C.1 Gradient flow for mean-squared error (MSE) loss ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The loss is given by: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}=\\mathbb{E}_{\\mathbf{X},Y}\\left[\\left(Y-\\mathrm{ReLU}(\\langle\\mathbf{w},\\mathbf{X}\\rangle)\\right)^{2}\\right]}\\\\ &{\\quad=\\mathbb{E}_{\\mathbf{X},Y}\\left[Y^{2}\\right]-2\\underbrace{\\mathbb{E}_{\\mathbf{X},Y}\\left[Y\\,\\mathrm{ReLU}(\\langle\\mathbf{w},\\mathbf{X}\\rangle)\\right]}_{\\triangleq(I)}+\\underbrace{\\mathbb{E}_{\\mathbf{X},Y}\\left[\\mathrm{ReLU}^{2}(\\langle\\mathbf{w},\\mathbf{X}\\rangle)\\right]}_{\\triangleq(I I)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The assumption of sign symmetry (S3) gives that $\\langle\\mathbf{w},\\mathbf{X}\\rangle$ is also sign-symmetric. First, this implies that P(\u27e8w, X\u27e9> 0) = 12, so: ", "page_idx": 15}, {"type": "equation", "text": "$$\n(I I)=\\frac{1}{2}\\operatorname{\\mathbb{E}}_{\\mathbf{X},Y\\mid\\langle\\mathbf{w},\\mathbf{X}\\rangle\\geq0}\\left[\\operatorname{ReLU}^{2}(\\langle\\mathbf{w},\\mathbf{X}\\rangle)\\right]=\\frac{1}{2}\\operatorname{\\mathbb{E}}_{\\mathbf{X},Y\\mid\\langle\\mathbf{w},\\mathbf{X}\\rangle\\geq0}\\left[(\\langle\\mathbf{w},\\mathbf{X}\\rangle)^{2}\\right].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Second, sign-symmetry of $\\langle\\mathbf{w},\\mathbf{X}\\rangle$ implies that we can drop the conditioning on $\\langle\\mathbf{w},\\mathbf{X}\\rangle\\geq0$ , since $\\langle\\mathbf{w},\\mathbf{X}\\rangle\\stackrel{d}{=}-\\langle\\mathbf{w},\\mathbf{X}\\rangle$ . Thus, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle(I I)=\\frac{1}{2}\\mathbb{E}_{\\mathbf{X},Y}\\left[(\\langle\\mathbf{w},\\mathbf{X}\\rangle)^{2}\\right]}\\\\ {\\displaystyle=\\frac{1}{2}\\mathbf{w}^{\\top}\\mathbb{E}_{\\mathbf{X},Y}\\left[\\mathbf{X}\\mathbf{X}^{\\top}\\right]\\mathbf{w}}\\\\ {\\displaystyle=\\frac{1}{2}\\mathbf{w}^{\\top}\\left(\\frac{1}{K}\\sum_{y=0}^{K-1}\\Sigma_{y}\\right)\\mathbf{w}}\\\\ {\\displaystyle=\\frac{1}{2K}\\sum_{y=0}^{K-1}\\mathbf{w}^{\\top}\\Sigma_{y}\\mathbf{w}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $K$ is the number of values (classes) of discrete $y$ . Finally, we differentiate $\\mathcal{L}$ with respect to w: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\nabla_{\\mathbf{w}}\\,\\mathcal{L}=2\\,\\mathbb{E}_{\\mathbf{X},Y}\\left[Y\\mathbb{1}(\\langle\\mathbf{w},\\mathbf{X}\\rangle\\ge0)\\mathbf{X}\\right]+\\frac{1}{K}\\sum_{y=0}^{K-1}\\Sigma_{y}\\mathbf{w}\\ .\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The gradient flow [EC24] is given by $\\begin{array}{r}{\\frac{\\mathrm{d}\\mathbf{w}}{\\mathrm{d}t}=-\\tau\\nabla_{\\mathbf{w}}\\,\\mathcal{L}}\\end{array}$ , where $\\tau$ is the learning rate. Thus, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{1}{\\tau}\\frac{\\mathrm{d}\\mathbf{w}}{\\mathrm{d}t}=2\\mathbb{E}_{\\mathbf{X},Y}\\left[Y\\mathbb{1}(\\langle\\mathbf{w},\\mathbf{X}\\rangle\\geq0)\\mathbf{X}\\right]-\\frac{1}{K}\\sum_{y=1}^{K}\\Sigma_{y}\\mathbf{w}\\;.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "C.2 Proof of lemma 3.1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Setting $K=2$ in equation (10), we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{1}{\\tau}\\frac{\\mathrm{d}\\mathbf{w}}{\\mathrm{d}t}=\\mathbb{E}_{\\mathbf{X}|Y=1}[\\mathbb{1}(\\langle\\mathbf{w},\\mathbf{X}\\rangle\\geq0)\\mathbf{X}]-\\frac{1}{2}\\left(\\Sigma_{0}+\\Sigma_{1}\\right)\\mathbf{w}\\;.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We wish to express the first term explicitly. Note that the first term is a vector in $\\mathbb{R}^{N}$ . We consider each of its entries separately by using the law of total expectation to write the $i$ -th entry as: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\mathbf{X}|Y=1}\\big[\\mathbb{1}(\\langle\\mathbf{w},\\mathbf{X}\\rangle\\ge0)X_{i}\\big]=\\mathbb{E}_{X_{i}|Y=1}\\left[X_{i}\\,\\mathbb{P}_{\\mathbf{X}|X_{i}=x_{i},Y=1}\\left[\\langle\\mathbf{w},\\mathbf{X}\\rangle\\ge0\\right]\\right]\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By Assumption A3, $\\{w_{i}X_{i}\\mid1\\,\\leq\\,i\\,\\leq\\,N\\}$ satisfies Lindeberg\u2019s condition as $N\\to\\infty$ . This is also known as a uniform integrability requirement. Before formally stating it, let us introduce two variables: $\\begin{array}{r}{S_{N}\\triangleq\\sum_{j=1}^{N}w_{j}(X_{j}-\\mu_{j\\vert x_{i}})}\\end{array}$ , the partial sums, and their variance, $\\sigma_{N}^{2}\\triangleq\\mathbb{E}[S_{N}^{2}]$ , where $\\underline{{\\mu}}_{j\\mid x_{i}}\\triangleq\\mathbb{E}[X_{j}\\mid X_{i}=x_{i}]$ is the conditional mean of the $j$ -entry given that $i$ -th entry has value $x_{i}$ Then, Lindeberg\u2019s condition is formally stated as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{N}\\mathbb{E}_{S_{N}|X_{i}=x_{i}}\\left[\\left|\\frac{S_{N}^{2}}{\\sigma_{N}^{2}}\\right|\\mathbb{1}\\left(\\left|\\frac{S_{N}^{2}}{\\sigma_{N}^{2}}\\right|>x\\right)\\right]\\to0\\quad\\mathrm{as}\\quad x\\to\\infty\\;.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This condition effectively states that no term in the partial sum $w_{i}X_{i}$ will dominate. Under this condition, along with weak dependence (Property S1), we conclude from [Bra07, Theorems 1.19, 10.2] that $S_{N}/\\sigma_{N}\\stackrel{d}{\\rightarrow}\\mathcal{N}(0,1)$ . Note that ", "page_idx": 16}, {"type": "equation", "text": "$$\nS_{N}=\\langle{\\bf w},{\\bf X}\\rangle-\\langle{\\bf w},\\mu_{|x_{i}}\\rangle\\qquad\\mathrm{and}\\qquad{\\boldsymbol{\\sigma}}_{N}=\\sqrt{{\\bf w}^{\\top}\\Sigma_{|x_{i}}^{1}{\\bf w}}\\;,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\mathbf{w},\\mathbf{X}$ are $N$ -dimensional vectors, $\\mu_{|x_{i}}=\\mathbb{E}[\\mathbf{X}\\mid X_{i}=x_{i}]$ is the vector of conditional means, and $\\Sigma_{|x_{i}}^{1}\\triangleq\\operatorname{Cov}[\\mathbf{X}\\mid X_{i}=x_{i},Y=1]=\\Sigma_{1}-\\sigma_{i}^{1}\\sigma_{i}^{1}{}^{\\top}$ . Since $\\sigma_{N}$ and $\\mu_{|x_{i}}$ are constant, we can write ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}_{\\mathbf{X}|X_{i}=x_{i},Y=1}\\left[\\langle\\mathbf{w},\\mathbf{X}\\rangle\\geq0\\right]=\\mathbb{P}_{\\mathbf{X}|X_{i},Y=1}\\left[\\frac{\\langle\\mathbf{w},\\mathbf{X}\\rangle-\\langle\\mathbf{w},\\mu_{j|x_{i}}\\rangle}{\\sigma_{N}}\\geq-\\frac{\\langle\\mathbf{w},\\mu_{j|x_{i}}\\rangle}{\\sigma_{N}}\\right]}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\mathbb{P}_{Z\\sim\\mathcal{N}(0,1)}\\left[Z\\geq-\\frac{\\langle\\mathbf{w},\\mu_{j|x_{i}}\\rangle}{\\sigma_{N}}\\right]+o_{N}(1)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =1-\\frac{1}{2}\\left[1+\\mathrm{erf}\\left(-\\frac{\\langle\\mathbf{w},\\mu_{j|x_{i}}\\rangle}{\\sqrt{2}\\sigma_{N}}\\right)\\right]+o_{N}(1)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\frac{1}{2}+\\frac{1}{2}\\operatorname{erf}\\left(\\frac{\\langle\\mathbf{w},\\mu_{j|x_{i}}\\rangle}{\\sqrt{2}\\sigma_{N}}\\right)+o_{N}(1)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the second step, in which we acquire $o_{N}(1)$ , follows from the definition of convergence in distribution. Under Assumptions A1 and A2, we may express this as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\mathbf{X}|X_{i}=x_{i},Y=1}\\left[\\left\\langle\\mathbf{w},\\mathbf{X}\\right\\rangle\\geq0\\right]=\\frac{1}{2}+\\frac{1}{2}\\operatorname{erf}\\left(\\frac{\\left\\langle\\mathbf{w},x_{i}\\sigma_{i}^{y}\\right\\rangle}{\\sqrt{2}\\sqrt{\\mathbf{w}^{\\top}\\Sigma_{1}\\mathbf{w}-\\left(\\left\\langle\\mathbf{w},\\sigma_{i}^{y}\\right\\rangle\\right)^{2}}}\\right)+o_{N}(1)\\;.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathbf{X}|Y=1}[\\mathbf{l}(\\langle\\mathbf{w},\\mathbf{X}\\rangle\\geq0)X_{i}]}\\\\ &{=\\mathbb{E}_{X_{i}|Y=1}\\left[X_{i}\\left(\\frac{1}{2}+\\frac{1}{2}\\operatorname{erf}\\left(\\frac{X_{i}}{\\sqrt{2}}\\,\\cdot\\frac{\\langle\\mathbf{w},\\sigma_{i}^{y}\\rangle}{\\sqrt{\\mathbf{w}^{\\top}\\Sigma_{1}\\mathbf{w}-(\\langle\\mathbf{w},\\sigma_{i}^{y}\\rangle)^{2}}}\\right)+o_{N}(1)\\right)\\right]}\\\\ &{=\\frac{1}{2}\\mathbb{E}_{X_{i}|Y=1}\\left[X_{i}\\operatorname{erf}\\left(\\frac{X_{i}}{\\sqrt{2}}\\,\\cdot\\frac{\\langle\\mathbf{w},\\sigma_{i}^{y}\\rangle/\\sqrt{\\mathbf{w}^{\\top}\\Sigma_{1}\\mathbf{w}}}{\\sqrt{1-(\\langle\\mathbf{w},\\sigma_{i}^{y}\\rangle/\\sqrt{\\mathbf{w}^{\\top}\\Sigma_{1}\\mathbf{w}})^{2}}}\\right)\\right]+\\mathbb{E}_{X_{i}|}[X_{i}|]o_{N}(1)}\\\\ &{=\\frac{1}{2}\\mathbb{E}_{X_{i}|Y=1}\\left[X_{i}\\operatorname{erf}\\left(\\frac{X_{i}}{\\sqrt{2}}\\,\\cdot\\mathrm{alg}^{-1}\\left(\\frac{\\langle\\mathbf{w},\\sigma_{i}^{y}\\rangle}{\\sqrt{\\mathbf{w}^{\\top}\\Sigma_{1}\\mathbf{w}}}\\right)\\right)\\right]+\\mathbb{E}_{X_{i}}[\\lvert X_{i}\\rvert]o_{N}(1)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Defining $\\varphi_{i}(a)\\triangleq\\mathbb{E}_{X_{i}|Y=1}[X_{i}\\operatorname{erf}(X_{i}\\operatorname{alg}^{-1}(a)/\\sqrt{2})]$ we can write ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{X}|Y=1}[\\mathbb{1}(\\langle\\mathbf{w},\\mathbf{X}\\rangle\\geq0)X_{i}]=\\frac{1}{2}\\varphi_{i}\\left(\\frac{\\langle\\mathbf{w},\\sigma_{i}^{y}\\rangle}{\\sqrt{\\mathbf{w}^{\\top}\\Sigma_{1}\\mathbf{w}}}\\right)+\\mathbb{E}_{X_{i}}[|X_{i}|]o_{N}(1)\\;.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note that $\\mathbb{E}_{X_{i}}[|X_{i}|]\\ \\leq\\ {\\sqrt{\\mathbb{E}_{X_{i}}[X_{i}^{2}]}}\\ =\\ \\sigma$ by Cauchy-Schwarz. So, $\\mathbb{E}_{X_{i}}[|X_{i}|]o_{N}(1)\\;=\\;o_{N}(1)$ . Moreover, by translation-invariance, all $X_{i}$ have the same marginal, so $\\varphi_{i}\\equiv\\varphi_{1}\\triangleq\\varphi$ . Thus, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{X}|Y=1}[\\mathbb{1}(\\langle\\mathbf{w},\\mathbf{X}\\rangle\\geq0)X_{i}]=\\frac{1}{2}\\varphi\\left(\\frac{\\langle\\mathbf{w},\\sigma_{i}^{y}\\rangle}{\\sqrt{\\mathbf{w}^{\\top}\\Sigma_{1}\\mathbf{w}}}\\right)+o_{N}(1)\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This form holds for all entries $i$ . Concatenating them, we obtain ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{X}|Y=1}[\\mathbb{1}(\\langle\\mathbf{w},\\mathbf{X}\\rangle\\geq0)\\mathbf{X}]=\\frac{1}{2}\\varphi\\left(\\frac{\\Sigma_{1}\\mathbf{w}}{\\sqrt{\\mathbf{w}^{\\top}\\Sigma_{1}\\mathbf{w}}}\\right)+o_{N}(1)\\,,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which gives the desired result. ", "page_idx": 17}, {"type": "text", "text": "C.3 Proof of lemma B.1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Property 1 follows from the fact that alg and erf are odd functions. This implies Property 4 since an odd function must have zero for even Taylor coefficients. ", "page_idx": 17}, {"type": "text", "text": "Properties 2 and 3 follow from differentiating Eq. (6). The first derivative is given by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\varphi^{\\prime}(a)=\\frac{\\sqrt{2}}{\\sqrt{\\pi}(1-a^{2})^{\\frac{3}{2}}}\\,\\mathbb{E}_{X_{1}}\\left[X_{1}^{2}\\mathrm{e}^{-\\frac{X_{1}^{2}a^{2}}{2(1-a^{2})}}\\right].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Setting $a=0$ , we get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\varphi^{\\prime}(0)={\\sqrt{\\frac{2}{\\pi}}}\\operatorname{\\mathbb{E}}\\left[X_{1}^{2}\\right]={\\sqrt{\\frac{2}{\\pi}}}\\sigma^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The third derivative is given by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\prime^{\\prime\\prime\\prime}(a)=\\frac{\\sqrt{2}}{\\sqrt{\\pi}(1-a^{2})^{\\frac{7}{2}}(a^{2}-1)^{2}}\\mathbb{E}_{X_{1}}\\left[X_{1}^{2}(12a^{6}+(9X_{1}^{2}-21)a^{4}+(X_{1}^{4}-8X_{1}^{2}+6)a^{2}-X_{1}^{2}+3X_{1}^{2})a^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Again setting $a=0$ gives ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\varphi^{\\prime\\prime\\prime}(0)={\\sqrt{\\frac{2}{\\pi}}}\\operatorname{\\mathbb{E}}_{X_{1}}\\left[X_{1}^{2}(-X_{1}^{2}+3)\\right]={\\sqrt{\\frac{2}{\\pi}}}\\left(3\\operatorname{\\mathbb{E}}_{X_{1}}[X_{1}^{2}]-\\operatorname{\\mathbb{E}}_{X_{1}}[X_{1}^{4}]\\right)=-{\\sqrt{\\frac{2}{\\pi}}}(\\kappa^{4}\\sigma^{4}-3\\sigma^{2})\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "C.4 Proof of Proposition 3.3 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The pdf of $\\begin{array}{r}{X\\sim\\mathcal{E}_{N}(\\mu,\\Sigma,\\phi)}\\end{array}$ is ", "page_idx": 18}, {"type": "equation", "text": "$$\np_{X}(x)={\\frac{1}{\\sqrt{\\operatorname*{det}(\\Sigma)}}}g((x-\\mu)^{\\top}\\Sigma^{-1}(x-\\mu))\\;,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for some function $g:\\mathbb{R}_{\\geq0}\\rightarrow\\mathbb{R}_{\\geq0}$ [Fra04]. A key property of elliptical distributions is that if $\\begin{array}{r}{X\\sim\\mathcal{E}_{N}(\\mu,\\Sigma,\\phi)}\\end{array}$ , then its affine transformation is also elliptical: $\\langle\\mathbf{w},\\mathbf{X}\\rangle\\sim\\mathcal{E}_{1}(\\langle\\mathbf{w},\\mu\\rangle,\\mathbf{w}^{\\top}\\Sigma\\mathbf{w},\\phi)$ for any $\\mathbf{w}\\in\\mathbb{R}^{N}$ . Thus, ", "page_idx": 18}, {"type": "equation", "text": "$$\np_{\\langle\\mathbf{w},\\mathbf{X}\\rangle}(s)=\\frac{1}{\\sqrt{\\mathbf{w}^{\\top}\\Sigma\\mathbf{w}}}\\tilde{g}\\left(\\frac{\\left(s-\\langle\\mathbf{w},\\mu\\rangle\\right)^{2}}{\\mathbf{w}^{\\top}\\Sigma\\mathbf{w}}\\right),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for some other function g\u02dc : R 0 \u2192R 0. ", "page_idx": 18}, {"type": "text", "text": "From our assumption of sign-symmetry, we have $\\mu=0$ . For brevity, we define $\\sigma^{2}\\triangleq\\mathbf{w}^{\\top}\\Sigma\\mathbf{w}$ and $S\\triangleq\\langle\\mathbf{w},\\mathbf{X}\\rangle$ . We begin by computing (I) in Eq. (9). Recall that we have $y=0,1\\;(i.e.,K=2)$ . So, ", "page_idx": 18}, {"type": "equation", "text": "$$\n2\\times(I)=\\mathbb{E}_{S|Y=1}[\\mathrm{ReLU}(S)]=\\int_{0}^{\\infty}\\frac{1}{\\sigma}\\tilde{g}\\left(\\frac{s^{2}}{\\sigma^{2}}\\right)s\\,\\mathrm{d}s.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "At this point, we apply a $u$ -substitution with $u=s^{2}/\\sigma^{2}$ , and thus $\\mathrm{d}u=2s\\;\\mathrm{d}s/\\sigma^{2}\\,\\Longleftrightarrow\\,\\sigma\\mathrm{d}u/2=$ $s\\;\\mathrm{d}s/\\sigma$ . This yields ", "page_idx": 18}, {"type": "equation", "text": "$$\n2\\times(I)=\\frac{\\sigma}{2}\\underbrace{\\int_{0}^{\\infty}\\tilde{g}(u)\\,\\mathrm{d}u}_{\\triangleq C}=\\frac{C}{2}\\sqrt{\\mathbf{w}^{\\top}\\Sigma_{1}\\mathbf{w}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Recall that we assume the MSE loss $\\mathcal{L}$ is finite. The first term in Eq. (9) is clearly finite for $y=0,1$ . The term $(I I)$ is also easily seen to be finite, since it evaluates to $\\mathbf{w}^{\\top}(\\Sigma_{0}+\\Sigma_{1})\\mathbf{\\dot{w}}/4$ . Thus, $\\mathcal{L}$ being finite implies $(I)$ in Eq. (9) is finite, which implies $C<\\infty$ . We computed (II) from Eq. (9) above as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{1}{4}{\\mathbf{w}}^{\\top}\\left(\\Sigma_{0}+\\Sigma_{1}\\right){\\mathbf{w}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for $K=2$ . Plugging in (I) and (II) and differentiating with respect to w, we get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{1}{\\tau}\\frac{\\mathrm{d}\\mathbf{w}}{\\mathrm{d}t}=\\frac{\\Sigma_{1}\\mathbf{w}}{\\sqrt{\\mathbf{w}^{\\top}\\Sigma_{1}\\mathbf{w}}}-\\frac{1}{2}\\left(\\Sigma_{0}+\\Sigma_{1}\\right)\\mathbf{w}\\;.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The steady states of Eq. (11) thus satisfy ", "page_idx": 18}, {"type": "equation", "text": "$$\nC\\frac{\\Sigma_{1}\\mathbf{w}}{\\sqrt{\\mathbf{w}^{\\top}\\Sigma_{1}\\mathbf{w}}}=\\frac{1}{2}\\left(\\Sigma_{0}+\\Sigma_{1}\\right)\\mathbf{w}\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Recall that translation-invariance implies that $\\Sigma_{0}$ and $\\Sigma_{1}$ are circulant, and since they are covariance matrices, they are symmetric. Then, they diagonalize in the basis given by the real and imaginary parts of the first $n/2$ Fourier components in the discrete Fourier transform, which we denote by the $n\\times n$ real matrix $\\mathcal{F}$ . Note that $\\mathcal{F}$ is orthogonal. Define v = F\u22a4w and $\\boldsymbol{\\Lambda}_{y}=\\boldsymbol{\\mathcal{F}}^{\\intercal}\\boldsymbol{\\Sigma}_{y}\\boldsymbol{\\mathcal{F}}$ for $y=0,1$ . Thus, the steady states satisfy ", "page_idx": 18}, {"type": "equation", "text": "$$\nC\\frac{\\Lambda_{1}\\mathbf{v}}{\\sqrt{\\mathbf{v}^{\\top}\\Lambda_{1}\\mathbf{v}}}=\\frac{1}{2}\\left(\\Lambda_{0}+\\Lambda_{1}\\right)\\mathbf{v}\\;.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This holds iff for all $i\\in[n]$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\nC(\\Lambda_{1})_{i i}({\\bf{v}}^{\\top}\\Lambda_{1}{\\bf{v}})^{-\\frac{1}{2}}v_{i}=\\frac{1}{2}((\\Lambda_{0})_{i i}+(\\Lambda_{1})_{i i})v_{i}\\;.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus, if $v_{i}\\neq0$ , we must have that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{(\\Lambda_{0})_{i i}}{(\\Lambda_{1})_{i i}}=2C({\\bf v}^{\\top}\\Lambda_{1}{\\bf v})^{-\\frac{1}{2}}-1\\;.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "That is, the ratio of the $i$ -th eigenvalues of $\\Sigma_{0}$ and $\\Sigma_{1}$ must be constant for all $i$ s.t. $v_{i}\\neq0$ . The eigenvalues of these matrices always come in pairs because of how we defined $\\mathcal{F}$ using both the real and imaginary parts of the discrete Fourier transform. In general, we observe that each pair assumes a unique value. So, since $C$ is finite, the condition above can hold for at most two distinct values of $i$ . Therefore, $v_{i}=0$ for all but at most two $i\\in[n]$ , implying that the steady state $w$ is of the form $a\\cos(2\\pi k x)+b\\sin(2\\pi k x)$ , i.e., it is oscillatory. As such, it is not localized. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "D Additional experiments ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "D.1 Visualizing breakdown of Assumption A3 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Fig. 8 demonstrates that our analytical model holds for long enough during training to capture the emergence of localization in the single ReLU neuron (M2). In the first three columns, we visualize the IPR of the weights from our empirical and analytical models, as well as the $\\ell_{2}$ difference between these two weights. In the first four rows, we visualize these metrics for four random initializations of the model, training each on $\\mathtt{N L G P}(g\\mathrm{~=~}100)$ with $\\xi_{0}\\,=\\,0.3$ and $\\xi_{1}\\,=\\,0.7.$ , where we expect from Theorem 3.2 to see localization. We see the error rapidly increase shortly after IPR increases, indicating the formation of localized receptive fields. The last three columns confirm this, as they show snapshots from before, during, and after the divergence between the empirical and analytical weights. We observe the weights are nearly identical before, differ only slightly at the most localized point during, and are both localized after, but possibly with different magnitudes and positions. The difference that emerges during is due to a breakdown of Assumption A3 used to create our analytical model, which is violated when the norm of w is dominated by just a few entries, i.e., it is localized. While [IG22] also observe a breakdown in their analytical model as localization emerges, ours, crucially, holds for long enough to characterize the emergence of localization. ", "page_idx": 19}, {"type": "text", "text": "We discuss the individual subplots in more detail. In all but the third row of Fig. 8, the analytical predictions are near-exact; in the third row, we predict localization, but at the wrong position. Focusing again on the first row, we see that at $t=20$ , the weights have not yet become localized (from IPR, left, first, and visually) and analytical and empirical weights match nearlt exactly, as confirmed by the small distance in left, center above. At $t=30$ , a localized peak around $i=21$ begins to emerge, violating Assumption A3 and weakening analytical precision. The analytical model then underestimates the degree to which the main peak at $i=21$ dominates, while it overestimates the size of competing peaks at $i=30$ , 37, and 90. Despite this, at $t=50$ , we see that predictions from the analytical model retain a match to the empirical model. ", "page_idx": 19}, {"type": "text", "text": "In the last row of Fig. 8, we use the same initialization and setting as in the first row, except that we train on $\\mathtt{N L G P}(g=0.01)$ data instead. From Theorem 3.2, we do not expect to see localization. The evolution of IPR confirms this, as it stays low in magnitude. We also see that, because localization never emerges, Assumption A3 is never violated, and so our analytical model accounts for the empirical model nearly perfectly. ", "page_idx": 19}, {"type": "image", "img_path": "nw9JmfL99s/tmp/dd5fd19eb86e3e965885e4602a5ea579d69869bb1a96ffd6d48dcad2cb1e2631.jpg", "img_caption": ["Figure 8: (Top) Four initializations trained on NL ${\\mathsf{G P}}(g=100)$ with $\\xi_{0}\\,=\\,0.3$ and $\\xi_{1}\\,=\\,0.7$ . As expected, weights always localize. In (Left, First) we plot IPR for empirical and analytical receptive fields (RFs) across time (defined as $\\#$ of gradient steps) $\\times\\ \\tau$ , the learning rate). In (Left, Second) we plot the time-evolution of $\\ell_{2}$ distance between the empirical and analytical RFs. In (Left, Third) we zoom in on (Left, First), restricting the range to [0, 0.1] to more closely see divergence in IPR early in training. In (Right, First) and (Right, Second), we snapshot the empirical and analytical RFs at a time before and just after, respectively, the analytical model breaks down (according to IPR and $\\ell_{2}$ distance) due to localization. Finally, in (Right, Third), we snapshot at the end of the training period. (Bottom) Same initialization as first row in top, but trained on $\\mathtt{N L G P}(g=0.01)$ ) data, again with $\\xi_{0}=0.3$ and $\\xi_{1}=0.7$ . As expected, weights do not localize. We plot the same quantities as above, but here the predictions of our analytical model hold throughout the entire training process as localization never emerges and so assumption (A3) is not violated as above. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Yes, in the abstract and introduction we claim to derive an analytical model for the effective dynamics of localization (Section 3) in the setting described in Section 2 and validate this model with experiments (Section 4). ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Yes, in the conclusion we note that our exact analysis is retricted to the singleneuron architecture M2 in the data setting described in Section 2, and that validations beyond that scope (like in Section 4) remain empirical even if they show promise. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Yes, our assumptions are rigourously stated in Section 3 and the proofs are provided in Appendix C. ", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Our setup (Section 4) and experiments (Section 4) provides full details on the data, hyperparameters, and training procedures used to obtain the figures. We also provide code (referenced on the first page). ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Yes, we provide a link to the code repository in the first page of the paper, and the repository contains all the code and data needed to reproduce the experiments. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Yes, we provide primary details in Section 4 and all the details in the code. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Yes, for our prediction experiment in Section 4.2 we report a measure of variation over multiple runs. ", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] Justification: Yes, we provide this information at the beginning of Section 4. ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Yes, we have reviewed the NeurIPS Code of Ethics and believe that our work conforms to it. ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: Since our work is primarily theoretical analysis of an existing phenomenon, we do not consider it to have direct societal impacts. ", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?   \nAnswer: [NA]   \nJustification: Our work does not involve the release of data or models that have a significant risk for misuse. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We wrote our own code for all experiments to procedurally generate data and train and analyze models. Visualizations from prior work in Fig. 1 are properly credited. ", "page_idx": 22}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We document our code (the only asset we release) in the accompanying repository. ", "page_idx": 22}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?   \nAnswer: [NA]   \nJustification: Our paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 22}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] Justification: Our paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 22}]