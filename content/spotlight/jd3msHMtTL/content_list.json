[{"type": "text", "text": "Small coresets via negative dependence: DPPs, linear statistics, and concentration ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "R\u00e9mi Bardenet \u2217 ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Subhroshekhar Ghosh ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Univ. Lille, CNRS, Centrale Lille, UMR 9189 \u2013 CRIStAL, F-59000 Lille, France remi.bardenet@cnrs.fr ", "page_idx": 0}, {"type": "text", "text": "Department of Mathematics National University of Singapore 10 Lower Kent Ridge Road, 119076, Singapore subhrowork@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Hugo Simon-Onfroy ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Hoang Son Tran \u2217\u2020 ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Universit\u00e9 Paris-Saclay, CEA, Irfu D\u00e9partement de Physique des Particules 91191, Gif-sur-Yvette, France hugo.simon@cea.fr ", "page_idx": 0}, {"type": "text", "text": "Department of Mathematics National University of Singapore 10 Lower Kent Ridge Road, 119076, Singapore hoangson.tran@u.nus.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Determinantal point processes (DPPs) are random configurations of points with tunable negative dependence. Because sampling is tractable, DPPs are natural candidates for subsampling tasks, such as minibatch selection or coreset construction. A coreset is a subset of a (large) training set, such that minimizing an empirical loss averaged over the coreset is a controlled replacement for the intractable minimization of the original empirical loss. Typically, the control takes the form of a guarantee that the average loss over the coreset approximates the total loss uniformly across the parameter space. Recent work has provided significant empirical support in favor of using DPPs to build randomized coresets, coupled with interesting theoretical results that are suggestive but leave some key questions unanswered. In particular, the central question of whether the cardinality of a DPP-based coreset is fundamentally smaller than one based on independent sampling remained open. In this paper, we answer this question in the affirmative, demonstrating that $D P P s$ can provably outperform independently drawn coresets. In this vein, we contribute a conceptual understanding of coreset loss as a linear statistic of the (random) coreset. We leverage this structural observation to connect the coresets problem to a more general problem of concentration phenomena for linear statistics of DPPs, wherein we obtain effective concentration inequalities that extend well-beyond the state-of-the-art, encompassing general non-projection, even non-symmetric kernels. The latter have been recently shown to be of interest in machine learning beyond coresets, but come with a limited theoretical toolbox, to the extension of which our result contributes. Finally, we are also able to address the coresets problem for vector-valued objective functions, a novelty in the coresets literature. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Let $\\mathcal{X}=\\{x_{i}\\mid i\\in[\\![1,n]\\!]\\}$ be a set of $n$ points in a Euclidean space, called the data set. Let $\\mathcal{F}$ be a set of nonnegative f unct ions on $\\mathcal{X}$ , called queries. Many classical learning problems, supervised or ", "page_idx": 0}, {"type": "text", "text": "unsupervised, are formulated as finding a query $f^{*}$ in $\\mathcal{F}$ that minimizes an additive loss function of the form ", "page_idx": 1}, {"type": "equation", "text": "$$\nL(f):=\\sum_{x\\in\\mathcal{X}}\\mu(x)f(x),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\mu:\\mathcal{X}\\to\\mathbb{R}_{+}$ is a weight function. ", "page_idx": 1}, {"type": "text", "text": "Example 1 ( $k$ -means). For $\\mathcal{X}\\subset\\mathbb{R}^{d}$ and $k\\in\\mathbb{N}$ , the goal of $k$ -means clustering is to find a set $\\mathcal{C}^{*}$ of $k$ \u201ccluster centers\" by minimizing (1) over ", "page_idx": 1}, {"type": "equation", "text": "$$\n{\\mathcal{F}}=\\left\\{f_{\\mathcal{C}}:x\\mapsto\\operatorname*{min}_{q\\in{\\mathcal{C}}}\\lVert x-q\\rVert_{2}^{2}\\ \\mid\\ {\\mathcal{C}}\\subset\\mathbb{R}^{d},|{\\mathcal{C}}|=k\\right\\}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Here, each query $f$ is indexed by a set of $k$ cluster centers, and the loss (1) is the quantization error. Example 2 (linear regression). When $\\mathcal{X}=\\{x_{i}:=(y_{i},z_{i})\\mid i\\in[\\![1,n]\\!]\\}\\subset\\mathbb{R}^{d+1}$ , linear regression corresponds to minimizing (1) over ", "page_idx": 1}, {"type": "equation", "text": "$$\n{\\mathcal{F}}=\\left\\{(y,z)\\mapsto(a^{\\top}y+b-z)^{2}\\mid a\\in\\mathbb{R}^{d},b\\in\\mathbb{R}\\right\\}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Penalty terms can be added to each function, to cover e.g. ridge or lasso regression. ", "page_idx": 1}, {"type": "text", "text": "In many machine learning applications, the complexity of the corresponding optimization problem grows with the cardinality $n$ of the dataset. When $n\\gg1$ makes optimization intractable, one is tempted to reduce the amount of data, using only a tractable number of representative samples. This is the idea formalized by coresets; we refer to (Bachem, Lucic, and Krause, 2017) for a survey, and to (Huang, Li, and Wu, 2024; Cohen-Addad, Larsen, Saulpic, Schwiegelshohn, and Sheikh-Omar, 2022) for specific coreset constructions for $k$ -means and Euclidean clustering. An $\\varepsilon$ -coreset is a subset $s\\subset\\mathcal{X}$ , possibly with corresponding weights $\\omega(x),x\\in S$ , such that ", "page_idx": 1}, {"type": "equation", "text": "$$\nL_{S}(f):=\\sum_{x\\in S}\\omega(x)f(x)\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "is within $\\varepsilon$ of $L(f)$ , uniformly in $f\\in\\mathcal F$ . If the cardinality $m$ of $\\boldsymbol{S}$ is significantly smaller than the intractable size $n$ of the original data set, one has reduced the complexity of the algorithm at a little cost in accuracy. ", "page_idx": 1}, {"type": "text", "text": "Many randomized coreset constructions, where such guarantees are shown to hold with large probability, are built by drawing elements independently from the data set $\\mathcal{X}$ (Bachem et al., 2017, Chapter 3). Because a representative coreset should intuitively be made of diverse data points, negative dependence between the coreset elements has been proposed as an effective possibility to improve their performance (Tremblay, Barthelm\u00e9, and Amblard, 2019). In particular, the authors advocate the use of Determinantal Point Processes (DPPs), a family of probability distributions over subsets of $\\mathcal{X}$ parametrized by an $n\\times n$ kernel matrix $\\mathbf{K}$ that enforces diversity, all of this while coming with a polynomial-time exact sampling algorithm. ", "page_idx": 1}, {"type": "text", "text": "Tremblay et al., 2019 give extensive theoretical and empirical justification for the use of DPPs in randomized coreset construction. In one of their key results, using concentration results in (Pemantle and Peres, 2011), Tremblay et al., 2019 bound the cardinality of a DPP-based $\\varepsilon$ -coreset, and their bound is $\\mathcal{O}(\\varepsilon^{-2})$ . However, it is known that the best $\\varepsilon$ -coresets built with independent samples are also of cardinality $\\mathcal{O}(\\varepsilon^{-2})$ . Thus, the crucial question of whether DPP-based coresets can provide a strict improvement remained to be settled; given the computational simplicity of independent schemes, this would be fundamental to justify the deployment of DPP-based methods. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we settle this question in the affirmative, demonstrating that for carefully chosen kernels, DPP-based coresets provably yield significantly better accuracy guarantees than independent schemes; equivalently, to achieve similar accuracy it suffices to use significantly smaller coresets via DPPs. In particular, we will show that DPP-based coresets actually can achieve cardinality $m=\\mathcal{O}(\\varepsilon^{-2/(1+\\delta)})$ . The quantity $\\delta$ depends on the variance of the subsampled loss under the considered DPP, and some DPPs yields $\\delta>0$ . A cornerstone of our approach is a structural understanding of the coreset loss (2) as a so-called linear statistic of the random point set $\\boldsymbol{S}$ , which enables us to go beyond earlier results that were based on concentration properties of general Lipschitz functions of a DPP (Pemantle and Peres, 2011). ", "page_idx": 1}, {"type": "text", "text": "In this endeavour, we obtain very widely-applicable concentration inequalities for linear statistics of DPPs compared to the state of the art; cf. (Breuer and Duits, 2013) that mostly focuses on scalarvalued statistics for finite rank ensembles on $\\mathbb{R}$ . In particular, we are able to address all DPPs that have appeared so far in the ML literature. Specifically, our results are able to handle non-symmetric kernels and vector-valued linear statistics. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "DPPs with non-symmetric kernels have recently been shown to be of significant interest in machine learning, such as recommendation systems (Gartrell, Brunel, et al., 2019; Gartrell, Han, et al., 2020; Han et al., 2022), but they come with a limited theoretical toolbox, to which this paper makes a contribution. On the other hand, vector-valued statistics arise naturally in many learning problems, including coreset settings such as the gradient estimator in Stochastic Gradient Descent (Bardenet, Ghosh, et al., 2021). However, the literature on coresets for vector-valued statistics is scarce, and in this paper we inaugurate their study with effective approximation guarantees via DPPs. ", "page_idx": 2}, {"type": "text", "text": "The rest of the paper is organized as follows. Section 2 contains background on DPPs and coresets. Section 3 contains our contributions. Section 4 provides numerical illustrations. Section 5 contains a discussion on limitations and future work. ", "page_idx": 2}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We introduce here the two key notions of determinantal point process and coreset, and observe that a coreset guarantee is a uniform control over specific linear statistics of a point process. ", "page_idx": 2}, {"type": "text", "text": "Determinantal point processes. A point process $\\boldsymbol{S}$ on a Polish space $\\mathcal{X}$ is a random locally finite subset of $\\mathcal{X}$ . Given a reference measure $\\mu$ on $\\mathcal{X}$ (e.g., the Lebesgue measure if $\\mathcal{X}\\,=\\,\\mathbb{R}^{d}$ or the counting measure if $\\mathcal{X}$ is discrete), a point process $\\boldsymbol{S}$ is called a DPP (w.r.t. $\\mu)$ ) if there exists a measurable function $K:\\mathcal{X}\\times\\mathcal{X}\\rightarrow\\mathbb{C}$ such that ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Big[\\sum_{\\neq}f(x_{i_{1}},\\dots,x_{i_{k}})\\Big]=\\int_{\\mathcal{X}^{k}}f(x_{1},\\dots,x_{k})\\operatorname*{det}[K(x_{i},x_{j})]_{k\\times k}\\,\\mathrm{d}\\mu^{\\otimes k}(x_{1},\\dots,x_{k}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the sum in the LHS ranges over all pairwise distinct $k$ -tuples of the random locally finite subset $\\boldsymbol{S}$ , for all bounded measurable $f:\\mathcal{X}^{k}\\xrightarrow{}\\bar{\\mathbb{R}}$ and for all $k\\in\\mathbb{N}$ . Such a function $K$ is called a kernel for the DPP $\\boldsymbol{S}$ , and $\\mu$ is called the background measure. ", "page_idx": 2}, {"type": "text", "text": "When the ground set $\\mathcal{X}$ is of finite cardinality $n$ , an equivalent but more intuitive way to define DPPs is as follows: a random subset $\\boldsymbol{S}$ of $\\mathcal{X}$ is called a DPP if there exists an $n\\times n$ -matrix $\\mathbf{K}$ such that ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}(T\\subset\\mathcal{S})=\\operatorname*{det}[{\\bf K}_{T}],\\quad\\forall\\,T\\subset\\mathcal{X},}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where ${\\bf K}_{T}$ denotes the submatrix of $\\mathbf{K}$ with rows and columns indexed by $T$ . ", "page_idx": 2}, {"type": "text", "text": "In a similar vein to Gaussian processes, all the statistical properties of a DPP are encoded in this kernel function $K$ and background measure $\\mu$ . A feature of DPPs with far-reaching implications for machine learning is that sampling and inference with DPPs are tractable. We refer the reader to (Hough et al., 2006; Kulesza and Taskar, 2012) for general references. Originally introduced in electronic optics (Macchi, 1975), they have been turned into generic statistical models for repulsion in spatial statistics (Lavancier et al., 2014; Biscio and Lavancier, 2017) and machine learning (Kulesza and Taskar, 2012; Belhadji et al., 2020a; Brunel, 2018; Derezinski and Mahoney, 2019; Derezinski, Liang, et al., 2020; Gartrell, Brunel, et al., 2019; Ghosh and Rigollet, 2020). ", "page_idx": 2}, {"type": "text", "text": "Example 3 ( $L$ -ensemble and m-DPP). Let $\\mathcal{X}$ be a finite set of cardinality $n$ , $\\mu$ be the counting measure, and $\\mathbf{L}$ be a positive semi-definite $n\\times n$ -matrix. The $L$ -ensemble with parameter $\\mathbf{L}$ is the point process $\\boldsymbol{S}$ on $\\mathcal{X}$ such that, for all $T\\subset\\mathcal{X}$ , $\\mathbb{P}(S=T)\\propto\\operatorname*{det}[\\mathbf{L}_{T}]$ , where $\\mathbf{L}_{T}$ is the square submatrix of $\\mathbf{L}$ corresponding to the rows and columns indexed by the subset $T$ . It can be shown that $\\boldsymbol{S}$ is a DPP on $\\mathcal{X}$ with kernel $\\mathbf{K}:=\\mathbf{L}(\\mathbf{I}+\\mathbf{L})^{-1}$ . In general, the cardinality of $\\boldsymbol{S}$ is a random variable. By conditioning on the event $\\{|S|=m\\}$ , we obtain the so-called $m$ -DPPs (Kulesza and Taskar, 2012). ", "page_idx": 2}, {"type": "text", "text": "Example 4 (Multivariate OPE; Bardenet and Hardy, 2020). Let $\\mathcal{X}=\\mathbb{R}^{d}$ and $\\mu$ be a measure on $\\mathbb{R}^{d}$ having all moments finite, let $(p_{k})_{k\\in\\ensuremath{\\mathbb{N}}^{d}}$ be the orthonormal sequence resulting from applying the Gram-Schmidt procedure to the monomials $x_{1}^{k_{1}}\\cdot\\cdot\\cdot x_{d}^{k_{d}}$ , taken in the graded lexical order. The kernel $\\begin{array}{r}{K_{\\mu}^{(m)}(x,y)\\,:=\\,\\sum_{k=0}^{m-1}p_{k}(x)p_{k}(y)}\\end{array}$ then defines a projection DPP on $\\mathbb{R}^{d}$ , called the multivariate Orthogonal Pol ynomial Ensemble (OPE) of rank and reference measure $\\mu$ . ", "page_idx": 2}, {"type": "text", "text": "Multivariate OPEs were used in (Bardenet and Hardy, 2020) as nodes for numerical integration, leading to a Monte Carlo estimator with mean squared error decaying in $m^{-1-1/d}$ , faster than under independent sampling. In (Bardenet, Ghosh, and Lin, 2021), the authors investigated the problem of DPP-based minibatch sampling for Stochastic Gradient Descent (SGD), and exploited a delicate interplay between a finite dataset and its ambient data distribution to leverage this fast decay for improved approximation guarantees. In particular, they proposed the following DPP defined on a (large) finite ground set. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Example 5 (Discretized multivariate OPE; Bardenet, Ghosh, et al., 2021). Let $n\\in\\mathbb N$ and $\\chi=$ $\\{x_{1},\\ldots,x_{n}\\}\\ \\subset\\ [-1,1]^{d}$ . Let $q(x)d x$ be a probability measure on $[-1,1]^{d}$ . Let $K_{q}^{(m)}$ be the multivariate OPE kernel of rank $m$ with reference measure $q(x)d x$ , as defined in Example 4. Let $\\tilde{\\gamma}:[-1,1]^{d}\\to\\mathbb{R}_{+}$ be a function, assumed to be positive on $\\mathcal{X}$ , and consider ", "page_idx": 3}, {"type": "equation", "text": "$$\nK_{q,\\tilde{\\gamma}}^{(m)}(x,y):=\\sqrt{\\frac{q(x)}{\\tilde{\\gamma}(x)}}K_{q}^{(m)}(x,y)\\sqrt{\\frac{q(y)}{\\tilde{\\gamma}(y)}},\\quad x,y\\in[-1,1]^{d}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Consider then the n \u00d7 n matrix K\u02dc = Kq(,m\u03b3\u02dc) |X \u00d7X . K\u02dc is symmetric and positive semidefinite, and we let $\\mathbf{K}$ be the matrix with the same eigenvectors, the $m$ largest eigenvalues replaced by 1, and the remaining eigenvalues replaced by 0. Then $\\mathbf{K}$ defines a DPP on $\\mathcal{X}$ . ", "page_idx": 3}, {"type": "text", "text": "Coresets. Let $\\varepsilon\\ >\\ 0$ and $\\mathcal{X}$ be a set of cardinality $n$ . The classical definition of a coreset is multiplicative. ", "page_idx": 3}, {"type": "text", "text": "Definition 1 (multiplicative coreset). A subset3 $S\\subset\\mathcal{X}$ is an $\\varepsilon$ -multiplicative coreset if ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\forall f\\in\\mathcal{F},\\,\\left|\\frac{L_{S}(f)}{L(f)}-1\\right|\\leq\\varepsilon,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $L$ and $L_{S}$ are respectively defined in (1) and (2). ", "page_idx": 3}, {"type": "text", "text": "An immediate and important consequence of (2) is that the ratio of the minimum value of $L_{S}$ by that of $L$ is within $O(\\varepsilon)$ of 1 (Bachem et al., 2017, Theorem 2.1). ", "page_idx": 3}, {"type": "text", "text": "One way to satisfy (2) with high probability for a single $f$ is through importance sampling, taking $\\boldsymbol{S}$ to be formed of $m>0$ i.i.d. samples from some instrumental density $q$ on $\\mathcal{X}$ , and taking $\\omega=\\mu/q$ in (2). Langberg and Schulman, 2010 showed that a suitable choice of $q$ actually yields the uniform guarantee (2). It suffices to take for instrumental pdf $q(x)\\propto\\mu(x)s(x)$ , where $s$ upper-bounds the so-called sensitivity ", "page_idx": 3}, {"type": "equation", "text": "$$\ns(x)\\geq\\operatorname*{sup}_{f\\in\\mathcal{F}}\\frac{f(x)}{\\sum_{y\\in\\mathcal{X}}\\mu(y)f(y)},\\quad\\forall x\\in\\mathcal{X}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For $\\delta>0$ , $\\begin{array}{r}{k\\ge\\frac{S^{2}}{2\\varepsilon^{2}}\\log2/\\delta}\\end{array}$ independent draws are then enough to build an $\\varepsilon$ -multiplicative coreset, where $\\begin{array}{r}{S=\\sum_{x\\in\\mathcal{X}}\\mu(x)s(x)}\\end{array}$ ; see (Bachem et al., 2017)[Section 2.3]. The tighter the bound (5), the smaller the  size of the coreset. One important limitation is that finding a tight bound is nontrivial. ", "page_idx": 3}, {"type": "text", "text": "Although not standard, a natural alternative definition of a coreset is that of an additive coreset. ", "page_idx": 3}, {"type": "text", "text": "Definition 2 (additive coreset). A subset $s\\subset\\mathcal{X}$ is an $\\varepsilon$ -additive coreset if ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\left|L s(f)-L(f)\\right|\\leq\\varepsilon,\\quad\\forall f\\in\\mathcal{F}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Note the arbitrary scaling factor $1/n$ in (6) compared to (2), which we adopt to simplify comparisons between the two coreset definitions. With an additive coreset, the minimal value of $L_{S}$ is guaranteed to be within $\\pm n\\varepsilon$ of the minimal value of $L$ : Similarly to a multiplicative coreset, with $\\varepsilon$ suitably small one should be happy to train one\u2019s algorithm only on $\\boldsymbol{S}$ . ", "page_idx": 3}, {"type": "text", "text": "Coreset guarantee and linear statistics. Let $\\boldsymbol{S}$ be a point process on a finite $\\mathcal{X}=\\{x_{1},\\ldots,x_{n}\\}$ . For a test function $\\varphi:\\mathcal X\\to\\mathbb R$ , we denote by $\\textstyle\\Lambda(\\varphi):={\\mathrm{\\bar{\\sum}}}_{x\\in{\\bar{S}}}\\,\\varphi(x)$ the so-called linear statistic of $\\varphi$ . In a coreset problem, for a query $f\\in\\mathcal F$ , the estimated loss $L_{S}(f)$ in (2) is the linear statistic $\\Lambda(\\omega f)$ . When $\\boldsymbol{S}$ is a DPP with a kernel $\\mathbf{K}$ on $\\mathcal{X}$ (w.r.t. the counting measure), we will choose the weight $\\omega(x)=\\mathbf{K}(x,x)^{-1}$ , where for $x=x_{i}\\in\\mathcal{X}$ , we define $\\mathbf{K}(x,x)$ to be $\\mathbf{K}_{i i}$ . By (3), this choice makes $L_{S}(f)$ an unbiased estimator for $L(f)$ . Guaranteeing a coreset guarantee such as (6) with high probability thus corresponds to a uniform-in- $f$ concentration inequality for the linear statistic $\\Lambda(\\omega f)$ . This motivates studying the concentration of linear statistics under a DPP, to which we now turn. ", "page_idx": 3}, {"type": "text", "text": "3 Theoretical results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We first give new results on the concentration of linear statistics under very general DPPs. These results are of interest in their own right, and should find applications in ML beyond coresets. Next we examine the implications of the concentration of linear statistics for coresets, showing that a suitable DPP does yield a coreset size of size $o\\big(\\varepsilon^{-2}\\big)$ , thus beating independent sampling. ", "page_idx": 4}, {"type": "text", "text": "Concentration inequalities for linear statistics of DPPs. We start with Hermitian kernels. ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Theorem 1 (Hermitian kernels). Let $\\boldsymbol{S}$ be a DPP on a Polish space $\\mathcal{X}$ with reference measure $\\mu$ and Hermitian kernel $K$ . Then for any bounded test function $\\varphi:\\mathcal X\\to\\mathbb R$ , we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{P}\\big(|\\Lambda(\\varphi)-\\mathbb{E}[\\Lambda(\\varphi)]|\\ge\\varepsilon\\big)\\le2\\exp\\Big(-\\frac{\\varepsilon^{2}}{4A\\,\\mathbb{V}\\mathrm{ar}\\,[\\Lambda(\\varphi)]}\\Big),\\quad\\forall0\\le\\varepsilon\\le\\frac{2A\\,\\mathbb{V}\\mathrm{ar}\\,[\\Lambda(\\varphi)]}{3\\|\\varphi\\|_{\\infty}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $A>0$ is a universal constant. ", "page_idx": 4}, {"type": "text", "text": "Our Theorem 1 is similar in spirit to a seminal concentration inequality by Breuer and Duits, 2013. However, their result only applies to DPPs with Hermitian projection kernels of finite rank. We emphasize that our Theorem 1 is applicable to all Hermitian kernels on general Polish spaces. ", "page_idx": 4}, {"type": "text", "text": "In view of recent interest in machine learning on DPPs with non-symmetric kernels, we present here a concentration inequality for such DPPs. We propose a novel approach to control the Laplace transform in the non-symmetric case (which can also be applied to the symmetric setting). As a trade-off, the range for $\\varepsilon$ becomes a bit smaller. For simplicity, we present the result for a finite ground set, but the proof applies more generally. ", "page_idx": 4}, {"type": "text", "text": "Theorem 2 (Non-symmetric kernels). Let $\\boldsymbol{S}$ be a DPP on a finite set $\\mathcal{X}=\\{x_{1},\\ldots,x_{n}\\}$ with $a$ non-symmetric kernel K. Then for any bounded test function $\\varphi:\\mathcal X\\to\\mathbb R_{}$ , we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n^{3}(|\\Lambda(\\varphi)-\\mathbb{E}[\\Lambda(\\varphi)]|\\ge\\varepsilon)\\le2\\exp\\Big(-\\frac{\\varepsilon^{2}}{4\\,\\mathbb{V}\\mathrm{ar}\\left[\\Lambda(\\varphi)\\right]}\\Big),\\forall0\\le\\varepsilon\\le\\frac{\\mathbb{V}\\mathrm{ar}\\left[\\Lambda(\\varphi)\\right]^{2}}{40\\|\\varphi\\|_{\\infty}^{3}\\cdot\\operatorname*{max}(1,\\|\\mathbf{K}\\|_{\\mathrm{op}}^{2})\\cdot\\|\\mathbf{K}\\|_{*}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\|\\cdot\\|_{\\mathrm{op}}$ denotes the spectral norm and $\\|\\cdot\\|_{*}$ denotes the nuclear norm of a matrix. ", "page_idx": 4}, {"type": "text", "text": "Remark 2.1. For simplicity, we will use the concentration inequality in Theorem 1 from now on. However, we keep in mind that we always can apply Theorem 2 to deduce analogous results for non-symmetric kernels. ", "page_idx": 4}, {"type": "text", "text": "We conclude with a concentration inequality for linear statistics of vector-valued functions. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3 (Vector-valued statistics). Let $\\boldsymbol{S}$ be a DPP on a Polish space $\\mathcal{X}$ with reference measure $\\mu$ and Hermitian kernel $K$ . Let $\\Phi=(\\varphi_{1},\\ldots,\\varphi_{p})^{\\top}:\\mathcal{X}\\to\\mathbb{R}^{p}$ be a vector-valued test function, and we denote by $\\Lambda(\\Phi),\\,\\mathbb{V}(\\Phi)$ the vectors $(\\Lambda(\\varphi_{i}))_{i=1}^{p}$ and $(\\mathbb{V}\\mathrm{ar}\\left[\\Lambda(\\varphi_{i})\\right]^{1/2})_{i=1}^{p}$ , respectively. Let $\\begin{array}{r}{\\|x\\|_{\\omega}^{2}:=\\sum_{i=1}^{p}\\dot{\\omega_{i}^{2}}|x_{i}|^{2}}\\end{array}$ be a weighted norm on $\\mathbb{R}^{p}$ for some weights $\\omega_{1},\\ldots\\omega_{p}\\geq0$ . Then, for some universal constant $A>0$ , we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\mathbb{P}(\\|\\Lambda(\\Phi)-\\mathbb{E}[\\Lambda(\\Phi)]\\|_{\\omega}\\ge\\varepsilon)\\le2p\\exp\\Big(-\\frac{\\varepsilon^{2}}{4A\\|\\mathbb{V}(\\Phi)\\|_{\\omega}^{2}}\\Big),}\\\\ {\\displaystyle\\le\\varepsilon\\le\\frac{2A\\|\\mathbb{V}(\\Phi)\\|_{\\omega}}{3}\\operatorname*{min}_{1\\le i\\le p}\\frac{\\sqrt{\\mathbb{V}\\mathrm{ar}[\\Lambda(\\varphi_{i})]}}{\\|\\varphi_{i}\\|_{\\infty}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "DPPs for coresets. We demonstrate the effectiveness of concentration inequalities for linear statistics of DPPs in the coresets problem, achieving uniform approximation guarantees over function classes. To accommodate as many ML settings as possible, we shall consider two natural types of function classes: vector spaces of functions (A.1) and parametrized function spaces (A.2). ", "page_idx": 4}, {"type": "text", "text": "For vector spaces of functions, we assume that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\dim\\operatorname{span}_{\\mathbb{R}}({\\mathcal{F}})=D<\\infty{\\mathrm{~for~some~}}D.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This assumption covers common situations like linear regression in Example 2, where we observe that each $f\\in\\mathcal F$ is a quadratic function in $(d+1)$ variables. Thus the dimension of the linear span of $\\mathcal{F}$ is at most $(d+1)^{2}\\,\\dot{+}\\,(d+1)\\,+\\,1$ . Another popular class of queries, originating in signal processing problems, is the class of band-limited functions. A function $f:\\ensuremath{\\mathbb{T}^{d}}\\mapsto\\ensuremath{\\mathbb{R}}$ (where $\\mathbb{T}^{d}$ denotes the $d$ -dimensional torus) is said to be band-limited if there exists $B\\in\\mathbb{N}$ such that its Fourier coefficients $\\hat{f}(k_{1},\\ldots,k_{d})=0$ whenever there is a $k_{j}$ such that $|k_{j}|>B$ . It is easy to see that the space $\\mathcal{F}$ of $B$ -bandlimited functions satisfies $\\mathrm{dim}\\,\\mathcal{F}\\leq(2B+1)^{d}$ . ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Another common scenario is when $\\mathcal{F}$ is parametrized by a finite-dimensional parameter space: ", "page_idx": 5}, {"type": "text", "text": "Conditions (A.2) and (A.3) cover e.g. the $k$ -means problem of Example 1, as well as (non-)linear regression settings. For $k$ -means, for instance, each query is parametrized by its cluster centers $\\bar{C}=\\{q_{1},\\ldots,q_{k}\\}$ , which can be viewed as a parameter $\\overline{{(q_{1},\\ldots,\\stackrel{\\ldots}{q_{k}})\\in\\mathbb{R}^{k d}}}$ . ", "page_idx": 5}, {"type": "text", "text": "Finally, with the idea in mind to derive multiplicative coresets from additive ones, we note that since $L(f)$ is typically of order $n$ (for any $f$ whose effective support covers a positive fraction of the ground set), it is natural to assume that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{1}{n}|L(f)|\\ge c,\\;\\mathrm{for}\\;\\mathrm{some}\\;c>0,\\mathrm{uniformly\\;on}\\;\\mathcal{F}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Theorem 4. Let $\\boldsymbol{S}$ be a DPP with a Hermitian kernel $\\mathbf{K}$ on a finite set ${\\mathcal{X}}=\\{x_{1},\\ldots,x_{n}\\}$ and $m=\\mathbb{E}[|S|]$ . Assume that for all $i\\in\\{1,\\ldots,n\\}$ , $\\mathbf{K}_{i i}\\ge\\boldsymbol{\\rho}\\cdot\\boldsymbol{m}/n$ for some $\\rho>0$ not depending on $m,n$ . Let $V\\geq\\operatorname*{sup}_{f\\in{\\mathcal{F}}}\\mathbb{V}\\mathrm{ar}\\left[n^{-1}L_{{\\mathcal{S}}}(f)\\right]$ . Under (A.1) and (A.4), ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(\\exists f\\in\\mathcal{F}:\\Big|\\frac{L_{S}(f)}{L(f)}-1\\Big|\\ge\\varepsilon\\Big)\\le2\\exp\\Big(6D-\\frac{c^{2}\\varepsilon^{2}}{16A V}\\Big),\\quad0\\le\\varepsilon\\le\\frac{4A\\rho m V}{3c\\operatorname*{sup}_{f\\in\\mathcal{F}}\\|f\\|_{\\infty}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Assuming (A.2), (A.3), (A.4) and $|S|\\leq B\\cdot m$ a.s. for some $B>0,$ , we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(\\exists f\\in\\mathcal{F}:\\Big|\\frac{L s(f)}{L(f)}-1\\Big|\\ge\\varepsilon\\Big)\\le2\\exp\\Big(C D-D\\log\\varepsilon-\\frac{c^{2}\\varepsilon^{2}}{16A V}\\Big),\\ 0\\le\\varepsilon\\le\\frac{4A\\rho m V}{3c\\operatorname*{sup}_{f\\in\\mathcal{F}}\\|f\\|_{\\infty}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Here $A>0$ is a universal constant and $C=C(\\Theta,B,\\rho,\\ell,c)>0$ is some constant. ", "page_idx": 5}, {"type": "text", "text": "Remark 4.1. For a bounded query $f$ , Var $\\left[n^{-1}L_{\\mathcal{S}}(f)\\right]=\\mathcal{O}(m^{-1})$ for i.i.d. sampling. In comparison, sampling with DPPs often yields smaller variance for linear statistics, in $O(m^{-(1+\\delta)})$ for some $\\delta>0$ ; see Section 3 for an example. Thus, the upper bound for the range of $\\varepsilon$ for which we could use our concentration result is $\\dot{\\mathcal{O}}(m^{-\\delta})$ . Plugging in $\\varepsilon=m^{-\\alpha}$ for $\\alpha\\geq\\delta$ gives the upper bounds $2\\exp(6D-C^{\\prime}m^{1+\\delta-2\\alpha})$ and $2\\exp(C D+\\alpha D\\log m-C^{\\prime}m^{1+\\delta-2\\alpha})$ respectively ( $C$ and $C^{\\prime}$ are some positive constants independent of m and $n$ ), which both converge to $0$ as $m\\rightarrow\\infty$ as long as $\\alpha<(1+\\delta)/2$ . In other words, the accuracy rate $\\varepsilon$ can be chosen to be as small as $m^{-1/2-\\delta^{\\prime}/2}$ , for any $0<\\delta^{\\prime}<\\delta$ , which is strictly smaller than the best accuracy rate $m^{-1/2}$ of i.i.d. sampling. ", "page_idx": 5}, {"type": "text", "text": "Remark 4.2. For i.i.d. sampling $\\boldsymbol{S}$ with expected size m, $\\mathbb{P}(x\\in S)=m/n$ for all $x\\in\\mathscr{X}$ . For $a$ DPP $\\boldsymbol{S}$ with kernel $\\mathbf{K}$ , one has $\\mathbb{P}(x_{i}\\in\\mathcal{S})=\\mathbf{K}_{i i}$ . Thus, assuming that for all $i$ , $\\mathbf{K}_{i i}\\ge\\boldsymbol{\\rho}\\cdot\\boldsymbol{m}/n$ for some $\\rho>0$ means that every point in the dataset $\\mathcal{X}$ should have a reasonable chance to be sampled. This also guarantees that the estimated loss $\\begin{array}{r}{L_{S}(f)=\\sum_{x\\in S}f(x)/\\mathbf{K}(x,x)}\\end{array}$ will not blow up, where for $x=x_{i}\\in\\mathcal{X}$ , we write $\\mathbf{K}(x,x)$ for $\\mathbf{K}_{i i}$ . ", "page_idx": 5}, {"type": "text", "text": "Remark 4.3. For the parametrized function spaces, the assumption $|S|\\,\\le\\,B\\,\\cdot\\,m$ a.s. is not strictly necessary, and is introduced here only for the sake of simplicity in presenting the results. A version of Theorem $^{4}$ without this assumption will be discussed in Appendix A.4. In fact, we only need $\\begin{array}{r}{n^{-1}\\sum_{x\\in S}\\mathbf{K}(x,x)^{-1}}\\end{array}$ to be bounded with high probability, which follows from the condition $\\mathbf{K}(x,x)\\geq\\boldsymbol\\rho\\cdot\\boldsymbol m/n$ and the fact that $|{\\mathcal{S}}|$ is highly concentrated around its mean m. ", "page_idx": 5}, {"type": "text", "text": "Remark 4.4. However, we remark that the assumption $|S|\\leq B\\cdot m\\,a$ .s. holds for most kernels of interest; DPPs with projection kernels being typical and significant examples. In machine learning terms, it entails that the coresets are not much bigger than their expected size $m$ ; whereas in practice, sampling schemes typically produce coresets of a fixed size (such as with projection $D P P s$ ). ", "page_idx": 5}, {"type": "text", "text": "Remark 4.5. It is straightforward to derive a version for additive coresets from Theorem 1. In fact, we will not need assumption (A.4) in the additive setting. ", "page_idx": 5}, {"type": "text", "text": "For the coresets problem for vector-valued functions, let $\\mathcal{F}$ consist of $\\mathbf{f}:\\mathcal{X}\\rightarrow\\mathbb{R}^{p}$ . For each f $\\in\\mathcal{F}$ , we denote by $\\bar{L_{S}}(\\mathbf{f}),L(\\mathbf{f})$ and $\\mathbb{V}(\\mathbf{f})$ the vectors in $\\mathbb{R}^{p}$ whose $i$ -coordinates are $L_{S}(f_{i}),L(f_{i})$ and Var $\\left[n^{-1}L_{S}(f_{i})\\right]^{1/2}$ , respectively. Let $\\begin{array}{r}{\\|\\boldsymbol{x}\\|_{\\omega}^{2}:=\\sum_{i=1}^{p}\\omega_{i}^{2}|\\boldsymbol{x}_{i}|^{2}}\\end{array}$ be a weighted norm on $\\mathbb{R}^{p}$ . ", "page_idx": 6}, {"type": "text", "text": "Theorem 5. Let $\\boldsymbol{S}$ be a DPP as in Theorem 4. Let $V\\geq\\operatorname*{sup}_{\\mathbf{f}\\in{\\mathcal{F}}}\\operatorname*{max}_{1\\leq i\\leq p}\\omega_{i}^{2}\\,\\mathbb{V}\\!\\arg\\big[n^{-1}L_{S}(f_{i})\\big]$ . Assuming (A.1), then ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(\\exists\\,\\mathbf{f}\\in\\mathcal{F}:\\frac{1}{n}\\|L_{\\mathcal{S}}(\\mathbf{f})-L(\\mathbf{f})\\|_{\\omega}\\geq\\varepsilon\\Big)\\leq2p\\exp\\Big(6D-\\frac{c^{2}\\varepsilon^{2}}{16A V}\\Big),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\begin{array}{r}{0\\leq\\varepsilon\\leq\\frac{4A\\rho m V}{3c}\\cdot(\\operatorname*{sup}_{\\mathbf{f}\\in\\mathcal{F}}\\operatorname*{max}_{i=1,\\dots,p}\\|f_{i}\\|_{\\infty})^{-1}.}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "Application: Discretized multivariate OPE. We revisit Example 5. It has been shown in (Bardenet, Ghosh, et al., 2021) that sampling with the DPP $\\boldsymbol{S}$ constructed in this example yields significant variance reduction for a wide class of linear statistics on $\\mathcal{X}$ . To be more precise, in their setting, $\\mathcal{X}=\\{x_{1},\\ldots,x_{n}\\}$ is a random data set, where $x_{i}$ \u2019s are i.i.d. samples from a distribution $\\gamma$ with support inside a $d$ -dimensional hypercube; $\\tilde{\\gamma}$ is a density estimator for $\\gamma$ and $q(x)d x$ is a reference measure on that hypercube. The DPP $\\boldsymbol{S}$ is then defined by the kernel $\\mathbf{K}$ in Example 5 w.r.t. the empirical measure $n^{-1}\\sum_{x\\in\\mathcal{X}}\\delta_{x}$ . We normalize the kernel by setting $\\hat{\\mathbf{K}}:=n^{-1}\\bar{\\mathbf{K}}$ , so that $\\boldsymbol{S}$ is a DPP with kernel $\\hat{\\bf K}$ w.r.t. the counting measure on $\\mathcal{X}$ . Then, under some mild assumptions on $\\tilde{\\gamma}$ and $q(x)d x$ , with high probability in the data set $\\mathcal{X}$ , we have Va $\\mathrm{~r~}\\big[n^{-1}L s(f)\\big]=\\mathcal{O}\\big(m^{-(1+1/d)}\\big)$ for any test function $f$ satisfying some mild regularity conditions. For more details, we refer the reader to (Bardenet, Ghosh, et al., 2021). ", "page_idx": 6}, {"type": "text", "text": "The significant reduction on the variance of linear statistics motivates us to apply Theorem 4 to this setting. We also remark that all assumptions on the kernel in Theorem 4 are satisfied for $\\hat{\\bf K}$ with high probability in the data set $\\mathcal{X}$ (see Appendix A.6). Let $\\mathcal{F}$ be a family of test functions on $\\mathcal{X}$ satisfying regularity conditions as in Bardenet, Ghosh, et al., 2021. Then we can state: ", "page_idx": 6}, {"type": "text", "text": "Theorem 6. For $\\varepsilon=O(m^{-1/d})$ , $w.h.p$ . in the data set $\\mathcal{X}$ , we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{P}_{S}\\Big(\\exists f\\in\\mathcal{F}:\\Big|\\frac{L_{S}(f)}{L(f)}-1\\Big|\\ge\\varepsilon\\Big)\\le2\\exp\\Big(6D-C^{\\prime}\\varepsilon^{2}m^{1+1/d}\\Big),\\quad a s s u m i n g\\ (\\mathrm{A}.1)\\,,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "and ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\cal{S}}\\Big(\\exists f\\in\\mathcal{F}:\\Big|\\frac{L_{\\cal{S}}(f)}{L(f)}-1\\Big|\\ge\\varepsilon\\Big)\\le2\\exp\\Big(C D-D\\log\\varepsilon-C^{\\prime}\\varepsilon^{2}m^{1+1/d}\\Big),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mathbb{P}_{\\mathcal{S}}$ indicates the randomness only in $\\boldsymbol{S}$ and $C,C^{\\prime}>0$ do not depend on $m,n$ . ", "page_idx": 6}, {"type": "text", "text": "Remark 6.1. Theorem $^ \u1e0a 6 \u1e0c$ confirms the discussion in Remark 4.1 for this particular example of DPP. More precisely, Theorem $^ \u1e0a 6 \u1e0c$ implies that, with probability tending to 1, sampling with this $D P P$ gives $|L_{S}(f)/L(f)-1|\\leq m^{-(\\frac{1}{2}+\\frac{1}{2d})^{-}}$ , $\\forall f\\in{\\mathcal{F}}$ , where $\\textstyle{\\left({\\frac{1}{2}}+{\\frac{1}{2d}}\\right)^{-}}$ denotes any positive number strictly smaller than $\\textstyle{\\frac{1}{2}}+{\\frac{1}{2d}}$ . Meanwhile, for i.i.d. sampling, the accuracy rate $\\varepsilon$ is at best $m^{-1/2}$ . ", "page_idx": 6}, {"type": "text", "text": "Remark 6.2. It may be noted that, DPPs being Hilbert space-based models, they interact well with linear projection based methods. As such, our method can be applied on dimensionally reduced data, wherein the $d$ in Remark 6.1 can be taken to be the reduced dimension, which is usually quite small. As such, the improvement in the approximation guarantees is substantial, especially for large scale problems entailing large $m$ . ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we compare randomized coresets for the $k$ -means problem of Example 1, on different datasets. One virtue of $k$ -means as a benchmark is that asymptotically tight upper-bound on sensitivity (5) can easily be computed (Bachem et al., 2017, Lemma 2.2). ", "page_idx": 6}, {"type": "text", "text": "Competing approaches. We compare 6 different coreset samplers.4 Each sampler takes as input a finite dataset $\\mathcal{X}$ , an integer $m$ , and sampler-specific parameters. It returns a random subset $s\\subset\\mathcal{X}$ of cardinality $m$ . For the associated weight function $\\omega$ in (2), we always take the inverse of the marginal probability of inclusion, i.e. $\\omega(x)=\\mathrm{{1}}/\\mathbb{P}(x\\in S)$ . ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "The first two baselines use independent sampling. The uniform method returns $m$ samples from $\\mathcal{X}$ , uniformly and without replacement, and runs in $\\mathcal{O}(m)$ . The second method, sensitivity, is specific to the $k$ -means problem. It corresponds to the classical sensitivity-based importance sampling coreset of Langberg and Schulman, 2010 described in Section 2. It runs in $O(n k+n m)$ . ", "page_idx": 7}, {"type": "text", "text": "The rest of the methods use negative dependence. The third method, termed $\\mathsf{G-m D P P}$ , uses an $m$ -DPP sampler where the likelihood kernel is a Gaussian kernel, with adjustable bandwidth denoted by $h$ . It is basically Algorithm 1 of Tremblay et al., 2019, except we do not approximate the likelihood kernel using random features. We prefer avoiding approximations in this paper to isolatedly probe the benefti of negative dependence, but our choice comes at the cost ${\\mathcal{O}}(n^{3})$ of performing SVD as a preprocessing, in addition to the usual $\\mathcal{O}(n m^{2})$ sampling time. Similarly, we compute the marginal probabilities of inclusion of $m$ -DPPs exactly, via Equation (205) and Algorithm 7 of Kulesza and Taskar, 2012. These costly steps will likely be approximated in real data applications; see the discussion of complexity to Section 5. The fourth method, OPE, is the discretized OPE of Example 5. We take $q$ to be a product of univariate beta pdfs, with parameters tuned to match the marginal moments of the dataset, as in (Bardenet, Ghosh, et al., 2021). We take $\\tilde{\\gamma}$ to be a kernel density estimator (KDE) built on $\\mathcal{X}$ , using the Epanechnikov kernel, with Scott\u2019s bandwidth selection method, as implemented in the scikit-learn package (Pedregosa et al., 2011). When KDE estimation is precomputed as in our experiments, the method runs in $\\mathcal{O}(n m^{2})$ , and $O(n^{2}+n m^{2})$ otherwise. Note that there is no cubic power of $n$ , as one can perform the eigenvalue thresholding in Example 1 by a reduced SVD of the $m\\times n$ feature matrix $\\bar{(}p_{k}(x_{i}))$ . The fifth method, termed Vdm-DPP, is Algorithm 2 of Tremblay et al., 2019, which runs in $\\mathcal{O}(n m^{2})$ . It is an OPE in the sense of Example 4, but where the reference measure $\\mu$ is the discrete empirical measure of the dataset. Although we have no result on how its linear statistics scale, its similarity with the discretized OPE, as well as its numerical performance in the experiments of Tremblay et al., 2019, make us expect Vdm-DPP to behave similarly to OPE. The sixth method, stratified, is a stratified sampling baseline limited to the case where $\\mathcal{X}\\subset[-1,1]^{d}$ and $\\mathcal{X}$ is \u201cwell-spread\". It partitions $[-1,1]^{\\hat{d}}$ into a grid of $m$ bins, and then independently draws one element uniformly in the intersection of $\\mathcal{X}$ with each bin. It is a special case of projection DPP, which runs in $\\mathcal{O}(n m)$ and has obvious pitfalls, like requiring that $\\mathcal{X}$ has a non-empty intersection with each bin, which is unlikely to be the case for non-uniformly spread datasets and high dimensions. Yet, this is a simple solution that one would likely implement to probe the benefits of negative dependence. ", "page_idx": 7}, {"type": "text", "text": "The performance metric. To investigate the cardinality of a coreset for a given error, we let $Q_{S}$ denote the quantile function of sup $\\overline{{|L_{S}(f)_{-}-L(f)|}}/\\dot{L}(f)$ , the supremum over all queries of the relative error. Intuitively, $Q_{S}(0.9)\\,=\\,10^{-2}$ means that $90\\%$ of the sampled coresets have a worst case relative error below $10^{\\overset{.}{-}2}$ . We shall look at how an estimated $Q_{S}(0.9)$ varies with $m$ , especially its slope in log-log plots with respect to $m$ . Now, the set $\\mathcal{F}$ of all queries for $k$ -means in combinatorially large, even for small values of $k$ . Therefore, each time we need to evaluate the supremum of the relative error, we rather uniformly sample without replacement $k$ elements of $\\mathcal{X}$ , 100 times and independently, and we take the maximum value of the relative error among these 100 values. Moreover, for each method and each coreset size $m$ , the quantile function $Q_{S}(0.9)$ is estimated by an empirical quantile over 100 independent coresets sampled for each value of $m$ . ", "page_idx": 7}, {"type": "text", "text": "Results. We first consider a synthetic dataset of $n=1024$ data points, sampled uniformly and independently in $[-1,1]^{d}$ ; see Figure 1a. We consider $d=2$ for demonstration purposes, but we have observed similar results for other small dimensions. Figure 1b depicts our estimate of $Q_{S}(0.9)$ as a function of the coreset size $m$ , in log-log format. The two i.i.d. baselines decrease as $m^{-1/2}$ , as expected. The stratified baseline, intuitively well-suited to uniformly-spread datasets, outperforms all other methods with a $m^{-1}$ rate, consistent with its known optimal variance reduction (Novak, 1988). Finally, the $m$ -DPP and the two DPPs also yield a faster decay, eventually outperforming the i.i.d. baselines as $m$ grows. This is expected for the discretized OPE, as it follows from the theoretical results from Section 3; but it is interesting to see that the Gaussian $m$ -DPP and the Vdm-DPP seem to reach a similar $m^{-3/4}$ fast rate. For the Gaussian $m$ -DPP, however, the performance depends on the value of the bandwidth of the Gaussian kernel: in Figure 1c, we see that the rate of decay can go from i.i.d.-like to OPE-like as the bandwidth increases; this is expected from results like (Barthelm\u00e9 et al., 2023). Note that the color code of Figure 1c differs from other figures. ", "page_idx": 7}, {"type": "image", "img_path": "jd3msHMtTL/tmp/aae5f9e9d59ddad45020ca7797f32dd9e8d06dfbd0aa55533ab860de5f6bb3e3.jpg", "img_caption": ["Figure 1: Results for the uniform dataset. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "In the uniform dataset of Fig. 1a, the sensitivity function is almost flat, which makes sensitivity behave like uniform. To give an edge to sensitivity, we now consider the trimodal dataset shown in Fig. 2a, with an OPE sample superimposed. The performance of sensitivity improves; see Figure 2b, while the determinantal samplers still outperform the independent ones thanks to a faster decay. For this dataset, it is not easy to stratify, and we thus do not show results for stratified. We note that the size of a marker placed at $x$ is proportional to the corresponding weight $1/K(x,x)$ in the estimator of the average loss. Equivalently, the marker size is inversely proportional to the marginal probability of $x$ being included in the DPP sample. ", "page_idx": 8}, {"type": "text", "text": "Finally, we consider the classical MNIST dataset, after a PCA of dimension 4. Figure 2c shows again the faster decay of the performance metric for the two DPPs (OPE and Vdm-DPP), compared to the two independent methods. However, the advantage progressively disappears as the dimension increases beyond 4 (unshown), as expected from the gain in variance of the discretized multivariate OPE, which becomes negligible when $d\\gg1$ ; see Section 5 for suggestions on how to prove a dimension-independent decay. The source code used in this work is available at github.com/hsimonfroy/DPPcoresets, where DPP samplers are built upon the Python package DPPy (Gautier et al., 2019). ", "page_idx": 8}, {"type": "image", "img_path": "jd3msHMtTL/tmp/1b4bcae9cc0c509863318e92915b23444a2a2e93e79c00e39647c7bc04949ec3.jpg", "img_caption": ["Figure 2: Results on other datasets. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Limitations. Our paper is a theoretical contribution, and our approach has several limitations before it can be a practical addition to the coreset toolbox. The improvement over independent sampling relies on a variance scaling for linear statistics of a particular DPP, which itself relies on both 1) an Ansatz that the dataset was generated i.i.d. from some pdf $\\gamma$ with a large support, and 2) the availability of a good approximation to $\\gamma$ ; see Section 3 and (Bardenet, Ghosh, et al., 2021). While Item 1) is usually deemed to be reasonable in a wide range of situations, we solve Item 2) by relying on a kernel density estimator, which is costly to manipulate. Another limitation is that the improvement over independent sampling is in $1/d$ and thus progressively vanishes as the dimension increases. Finally, a classical caveat is that although tractable, sampling a DPP still costs $\\mathcal{O}(n m^{2})$ , provided the kernel is available in diagonalized form. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Future work. The limitations above set up a research program. In particular, an intriguing observation in our empirical studies is the comparative performance of various DPP-based coreset samplers; several of them exhibit effective performance. While we have sharp theoretical guarantees for the discretized OPE-based scheme, obtaining similar guarantees and parameter-tuning protocols for other samplers, like $m$ -DPPs, will be of great practical interest as they would bypass the need, e.g., for an approximation to the data-generating mechanism $\\gamma$ . The DPP called Vdm-DPP in Section 4, which is itself an OPE for a discrete measure, might be a bridge between OPEs and $m$ -DPPs, as Vdm-DPP can be seen as a limit of Gaussian $m$ -DPPs (Barthelm\u00e9 et al., 2023). On a more general note, improving the computational complexity of sampling DPPs remains an active topic, and we should examine which techniques, e.g. by leveraging low-rank structures, preserve the small coreset property. Any breakthrough in the complexity of DPP sampling would also have salutary consequences for the broader program of negative dependence as a toolbox for machine learning. On the dependence of the rate to the dimension, we propose to investigate the impact of smoothness of the test functions on the rate: in numerical integration with mixtures of DPPs, smoothness does bring dimension-independent rates (Belhadji et al., 2020b). Finally, in a more theoretical direction, extending concentration inequalities for linear statistics beyond the restricted range of $\\varepsilon$ appearing e.g. in Theorem 1 is a mathematically challenging problem, with potential learning-theoretic consequences. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "RB and HSO acknowledge support from ERC grant Blackjack (ERC-2019-STG-851866) and ANR AI chair Baccarat (ANR-20-CHIA-0002). SG was supported in part by the MOE grants R-146-000- 250-133, R-146-000-312-114, A-8002014-00-00 and MOE-T2EP20121-0013. HST was supported by the NUS Research Scholarship. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Olivier Bachem, Mario Lucic, and Andreas Krause. \u201cPractical Coreset Constructions for Machine Learning\u201d. In: arXiv: Machine Learning (2017). URL: https : / / api . semanticscholar.org/CorpusID:88517375.   \n[2] R\u00e9mi Bardenet, Subhroshekhar Ghosh, and Meixia Lin. \u201cDeterminantal point processes based on orthogonal polynomials for sampling minibatches in SGD\u201d. In: Advances in Neural Information Processing Systems 34 (2021), pp. 16226\u201316237.   \n[3] R\u00e9mi Bardenet and Adrien Hardy. \u201cMonte Carlo with Determinantal Point Processes\u201d. In: Annals of Applied Probability (2020). URL: https://hal.archives-ouvertes.fr/hal01311263.   \n[4] Simon Barthelm\u00e9, Nicolas Tremblay, Konstantin Usevich, and Pierre-Olivier Amblard. \u201cDeterminantal point processes in the flat limit\u201d. In: Bernoulli 29.2 (2023), pp. 957\u2013983.   \n[5] Ayoub Belhadji, R\u00e9mi Bardenet, and Pierre Chainais. \u201cA determinantal point process for column subset selection\u201d. In: Journal of machine learning research 21.197 (2020), pp. 1\u201362.   \n[6] Ayoub Belhadji, R\u00e9mi Bardenet, and Pierre Chainais. \u201cKernel interpolation with continuous volume sampling\u201d. In: International Conference on Machine Learning. PMLR. 2020, pp. 725\u2013 735.   \n[7] Christophe Ange Napol\u00e9on Biscio and Fr\u00e9d\u00e9ric Lavancier. \u201cContrast estimation for parametric stationary determinantal point processes\u201d. In: Scandinavian Journal of Statistics 44.1 (2017), pp. 204\u2013229.   \n[8] Jonathan Breuer and Maurice Duits. \u201cThe Nevai condition and a local law of large numbers for orthogonal polynomial ensembles\u201d. In: Advances in Mathematics 265 (2013), pp. 441\u2013484. URL: https://api.semanticscholar.org/CorpusID:119731958.   \n[9] Victor-Emmanuel Brunel. \u201cLearning signed determinantal point processes through the principal minor assignment problem\u201d. In: Advances in Neural Information Processing Systems 31 (2018).   \n[10] Vincent Cohen-Addad, Kasper Green Larsen, David Saulpic, Chris Schwiegelshohn, and Omar Ali Sheikh-Omar. \u201cImproved Coresets for Euclidean k-Means\u201d. English. In: Advances in Neural Information Processing Systems 35 - 36th Conference on Neural Information Processing Systems, NeurIPS 2022. Ed. by S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh. Advances in Neural Information Processing Systems. Publisher Copyright: $\\copyright$ 2022 Neural information processing systems foundation. All rights reserved.; 36th Conference on Neural Information Processing Systems, NeurIPS 2022 ; Conference date: 28-11-2022 Through 09-12-2022. Neural Information Processing Systems Foundation, 2022.   \n[11] Michal Derezinski, Feynman Liang, and Michael Mahoney. \u201cBayesian experimental design using regularized determinantal point processes\u201d. In: International Conference on Artificial Intelligence and Statistics. PMLR. 2020, pp. 3197\u20133207.   \n[12] Michal Derezinski and Michael Mahoney. \u201cDistributed estimation of the inverse Hessian by determinantal averaging\u201d. In: Advances in Neural Information Processing Systems 32. Ed. by H. Wallach, H. Larochelle, A. Beygelzimer, F. d Alch\u00e9-Buc, E. Fox, and R. Garnett. Curran Associates, Inc., 2019, pp. 11401\u201311411.   \n[13] Mike Gartrell, Victor-Emmanuel Brunel, Elvis Dohmatob, and Syrine Krichene. \u201cLearning nonsymmetric determinantal point processes\u201d. In: Advances in Neural Information Processing Systems 32 (2019).   \n[14] Mike Gartrell, Insu Han, Elvis Dohmatob, Jennifer Gillenwater, and Victor-Emmanuel Brunel. \u201cScalable learning and MAP inference for nonsymmetric determinantal point processes\u201d. In: arXiv preprint arXiv:2006.09862 (2020).   \n[15] Guillaume Gautier, Guillermo Polito, R\u00e9mi Bardenet, and Michal Valko. \u201cDPPy: DPP Sampling with Python\u201d. In: Journal of Machine Learning Research 20.180 (2019), pp. 1\u20137. URL: http://jmlr.org/papers/v20/19-179.html.   \n[16] Subhroshekhar Ghosh and Philippe Rigollet. \u201cGaussian determinantal processes: A new model for directionality in data\u201d. In: Proceedings of the National Academy of Sciences 117.24 (2020), pp. 13207\u201313213.   \n[17] Insu Han, Mike Gartrell, Elvis Dohmatob, and Amin Karbasi. \u201cScalable MCMC sampling for nonsymmetric determinantal point processes\u201d. In: International Conference on Machine Learning. PMLR. 2022, pp. 8213\u20138229.   \n[18] J. Ben Hough, Manjunath Krishnapur, Yuval Peres, and B\u00e1lint Vir\u00e1g. \u201cDeterminantal Processes and Independence\u201d. In: Probability Surveys 3 (2006). URL: https://doi.org/10.1214% 2F154957806000000078.   \n[19] Lingxiao Huang, Jian Li, and Xuan Wu. \u201cOn Optimal Coreset Construction for Euclidean (k,z)- Clustering\u201d. In: Proceedings of the 56th Annual ACM Symposium on Theory of Computing. STOC 2024. Vancouver, BC, Canada: Association for Computing Machinery, 2024, pp. 1594\u2013 1604. ISBN: 9798400703836. DOI: 10.1145/3618260.3649707. URL: https://doi.org/ 10.1145/3618260.3649707.   \n[20] Kurt Johansson and Gaultier Lambert. \u201cGaussian and non-Gaussian fluctuations for mesoscopic linear statistics in determinantal processes\u201d. In: The Annals of Probability 46.3 (2018), pp. 1201\u20131278. DOI: 10.1214/17-AOP1178. URL: https://doi.org/10.1214/17- AOP1178.   \n[21] Alex Kulesza and Ben Taskar. \u201cDeterminantal Point Processes for Machine Learning\u201d. In: Foundations and Trends\u00ae in Machine Learning 5.2\u20133 (2012), pp. 123\u2013286. ISSN: 1935-8237. DOI: 10.1561/2200000044. URL: http://dx.doi.org/10.1561/2200000044.   \n[22] Michael Langberg and Leonard J. Schulman. \u201cUniversal epsilon-approximators for integrals\u201d. In: Proceedings of the 2010 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA). 2010, pp. 598\u2013607. DOI: 10.1137/1.9781611973075.50. eprint: https://epubs.siam. org/doi/pdf/10.1137/1.9781611973075.50. URL: https://epubs.siam.org/doi/ abs/10.1137/1.9781611973075.50.   \n[23] Fr\u00e9d\u00e9ric Lavancier, Jesper M\u00f8ller, and Ege Rubak. \u201cDeterminantal Point Process Models and Statistical Inference\u201d. In: Journal of the Royal Statistical Society Series B: Statistical Methodology 77.4 (Dec. 2014), pp. 853\u2013877. ISSN: 1467-9868. DOI: 10.1111/rssb.12096. URL: http://dx.doi.org/10.1111/rssb.12096.   \n[24] Odile Macchi. \u201cProcessus ponctuels et coincidences \u2013 Contributions \u00e0 l\u2019\u00e9tude th\u00e9orique des processus ponctuels, avec applications \u00e0 l\u2019optique statistique et aux communications optiques\u201d. PhD thesis. Universit\u00e9 Paris-Sud, 1972.   \n[25] Odile Macchi. \u201cThe Coincidence Approach to Stochastic Point Processes\u201d. In: Advances in Applied Probability 7.1 (1975), pp. 83\u2013122. ISSN: 00018678. URL: http://www.jstor. org/stable/1425855 (visited on 05/22/2024).   \n[26] Erich Novak. Deterministic and Stochastic Error Bounds in Numerical Analysis. Vol. 1349. Lecture Notes in Mathematics. Berlin, Heidelberg: Springer, 1988. ISBN: 978-3-540-50368-2. DOI: 10.1007/BFb0079792. URL: http://link.springer.com/10.1007/BFb0079792.   \n[27] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. \u201cScikit-learn: Machine Learning in Python\u201d. In: Journal of Machine Learning Research 12 (2011), pp. 2825\u20132830.   \n[28] Robin Pemantle and Yuval Peres. \u201cConcentration of Lipschitz Functionals of Determinantal and Other Strong Rayleigh Measures\u201d. In: Combinatorics Probability and Computing 23 (Aug. 2011). DOI: 10.1017/S0963548313000345.   \n[29] A. Soshnikov. \u201cDeterminantal random point fields\u201d. In: Russian Mathematical Surveys 55 (2000), pp. 923\u2013975.   \n[30] Nicolas Tremblay, Simon Barthelm\u00e9, and Pierre-Olivier Amblard. \u201cDeterminantal point processes for coresets\u201d. In: Journal of Machine Learning Research 20.168 (2019), pp. 1\u201370. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Appendix / supplemental material ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Proof of Theorem 1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We present here the proof for the general setting, i.e., when $\\mathcal{X}$ is a Polish space and $\\boldsymbol{S}$ is a DPP with a Hermitian kernel $K$ w.r.t a background measure $\\mu$ . By abuse of notation, we will also denote by $K$ the integral operator ", "page_idx": 12}, {"type": "equation", "text": "$$\nK:L^{2}(\\mathcal{X},\\mu)\\to L^{2}(\\mathcal{X},\\mu)\\quad,\\quad f(x)\\mapsto\\int_{\\mathcal{X}}K(x,y)f(y)\\mathrm{d}\\mu(y).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "We denote by $C_{k}(\\varphi),k\\geq1$ the cumulants of $\\Lambda(\\varphi)$ , i.e. ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\log\\mathbb{E}[e^{t\\Lambda(\\varphi)}]=\\sum_{k\\geq1}\\frac{C_{k}(\\varphi)}{k!}t^{k},\\quad\\mathrm{~for~}t\\mathrm{~near~}0.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Note that $C_{1}(\\varphi)=\\mathbb{E}[\\Lambda(\\varphi)],C_{2}(\\varphi)=\\mathbb{V}\\mathrm{ar}\\left[\\Lambda(\\varphi)\\right]$ . In general, we have the formula (see Johansson and Lambert, 2018) ", "page_idx": 12}, {"type": "equation", "text": "$$\nC_{k}(\\varphi)=\\sum_{q=1}^{k}{\\frac{(-1)^{q+1}}{q}}\\sum_{\\stackrel{k_{1},\\ldots,k_{q}\\geq1}{k_{1}+\\ldots+k_{q}=k}}{\\frac{k!}{k_{1}!\\cdot\\ldots k_{q}!}}\\mathrm{Tr}[\\Phi^{k_{1}}K\\,\\ldots\\Phi^{k_{q}}K],\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $\\Phi:L^{2}(\\mathcal{X},\\mu)\\rightarrow L^{2}(\\mathcal{X},\\mu)$ is the operator $f(x)\\mapsto\\varphi(x)f(x)$ . ", "page_idx": 12}, {"type": "text", "text": "By the Macchi-Soshnikov theorem (Macchi, 1972; Soshnikov, 2000), $0\\preceq K\\preceq I$ , and we can write $C_{2}(\\varphi)=\\mathrm{Tr}[\\Phi(I-K)\\Phi K]=\\mathrm{Tr}[\\sqrt{K}\\Phi(I-K)\\Phi\\sqrt{K}]=\\|\\sqrt{I-K}\\Phi\\sqrt{K}\\|_{\\mathrm{HS}}^{2},$ ", "page_idx": 12}, {"type": "text", "text": "where $\\|\\cdot\\|_{\\mathrm{HS}}$ denotes the Hilbert-Schmidt norm of an operator. ", "page_idx": 12}, {"type": "text", "text": "Lemma 1. For $k\\geq1$ , we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\sqrt{I-K}\\Phi^{k}\\sqrt{K}\\|_{\\mathrm{HS}}\\leq k\\|\\varphi\\|_{\\infty}^{k-1}\\|\\sqrt{I-K}\\Phi\\sqrt{K}\\|_{\\mathrm{HS}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $\\begin{array}{r}{\\|\\varphi\\|_{\\infty}:=\\operatorname*{sup}_{x\\in\\mathcal{X}}|\\varphi(x)|}\\end{array}$ . ", "page_idx": 12}, {"type": "text", "text": "Proof. One has ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\sqrt{I-K}\\Phi^{k}\\sqrt{K}\\|_{\\mathrm{HS}}^{2}}&{=}&{\\mathrm{Tr}[\\sqrt{K}\\Phi^{k}(I-K)\\Phi^{k}\\sqrt{K}]}\\\\ &{=}&{\\mathrm{Tr}[\\Phi^{k}(I-K)\\Phi^{k}K]}\\\\ &{=}&{\\mathrm{Tr}[\\Phi^{2k}K]-\\mathrm{Tr}[\\Phi^{k}K\\Phi^{k}K]}\\\\ &{=}&{\\displaystyle\\int\\varphi(x)^{2k}K(x,x)\\mathrm{d}\\mu(x)-\\iint\\varphi(x)^{k}K(x,y)\\varphi(y)^{k}K(y,x)\\mathrm{d}\\mu(x)\\mathrm{d}\\mu(y)}\\\\ &{=}&{\\displaystyle\\int\\varphi(x)^{2k}\\Big(K(x,x)-\\int K(x,y)K(y,x)\\mathrm{d}\\mu(y)\\Big)\\mathrm{d}\\mu(x)}\\\\ &{+}&{\\displaystyle\\frac{1}{2}\\iint(\\varphi(x)^{k}-\\varphi(y)^{k})^{2}K(x,y)K(y,x)\\mathrm{d}\\mu(x)\\mathrm{d}\\mu(y).}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Since $0\\preceq K\\preceq I$ , we have $K^{2}\\preceq K$ , which implies $\\begin{array}{r}{K(x,x)\\ge\\int K(x,y)K(y,x)\\mathrm{d}\\mu(y)}\\end{array}$ for $\\mu$ -a.e. $x$ . Thus, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ \\displaystyle\\int\\varphi(x)^{2k}\\Big(K(x,x)-\\int K(x,y)K(y,x)\\mathrm{d}\\mu(y)\\Big)\\mathrm{d}\\mu(x)}\\\\ {\\leq\\ \\ }&{\\|\\varphi||_{\\infty}^{2k-2}\\displaystyle\\int\\varphi(x)^{2}\\Big(K(x,x)-\\int K(x,y)K(y,x)\\mathrm{d}\\mu(y)\\Big)d\\mu(x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "On the other hand, by the symmetry of $K$ , we have $K(x,y)K(y,x)\\,=\\,|K(x,y)|^{2}\\,\\geq\\,0$ for all $x,y\\in\\mathcal{X}$ . Note that ", "page_idx": 12}, {"type": "equation", "text": "$$\n|\\varphi(x)^{k}-\\varphi(y)^{k}|=|\\varphi(x)-\\varphi(y)|\\Big|\\sum_{j=0}^{k-1}\\varphi(x)^{j}\\varphi(y)^{k-1-j}\\Big|\\leq k\\|\\varphi\\|_{\\infty}^{k-1}|\\varphi(x)-\\varphi(y)|.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Combining all ingredients, we deduce that $\\begin{array}{r}{\\|\\sqrt{I-K}\\Phi^{k}\\sqrt{K}\\|_{\\mathrm{HS}}^{2}\\leq k^{2}\\|\\varphi\\|_{\\infty}^{2k-2}\\|\\sqrt{I-K}\\Phi\\sqrt{K}\\|_{\\mathrm{HS}}^{2},}\\end{array}$ as desired. ", "page_idx": 12}, {"type": "text", "text": "Lemma 2. For $k\\geq3$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{|C_{k}(\\varphi)|}{k!}\\leq\\frac{1}{\\sqrt{2\\pi}}e^{k}k^{3/2}\\|\\varphi\\|_{\\infty}^{k-2}C_{2}(\\varphi).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. We recall the formula (7), observe that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\sum_{q=1}^{k}{\\frac{(-1)^{q+1}}{q}}\\sum_{{k_{1},\\ldots,k_{q}\\geq1\\atop k_{1}+\\ldots+k_{q}=k}}{\\frac{k!}{k_{1}!\\ldots k_{q}!}}=0.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then one can write ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{{\\cal C}_{k}(\\varphi)}}&{{=}}&{{\\displaystyle\\sum_{q=1}^{n}\\frac{(-1)^{q+1}}{q}\\sum_{\\stackrel{k_{1},\\ldots,k_{q}\\geq1}{k_{1}+\\ldots+k_{q}=k}}\\frac{k!}{k_{1}!\\ldots k_{q}!}\\Big(\\mathrm{Tr}[\\Phi^{k_{1}}K\\,\\ldots\\Phi^{k_{q}}K]-\\mathrm{Tr}[\\Phi^{k}K]\\Big)}}\\\\ {{}}&{{=}}&{{\\displaystyle\\sum_{q=2}^{n}\\frac{(-1)^{q+1}}{q}\\sum_{\\stackrel{k_{1},\\ldots,k_{q}\\geq1}{k_{1}+\\ldots+k_{q}=k}}\\frac{k!}{k_{1}!\\ldots k_{q}!}\\Big(\\mathrm{Tr}[\\Phi^{k_{1}}K\\,\\ldots\\Phi^{k_{q}}K]-\\mathrm{Tr}[\\Phi^{k}K]\\Big).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "For any $k_{1},\\hdots,k_{q}\\geq1$ such that $k_{1}+...+k_{q}=k$ , we observe that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\mathrm{Tr}[\\Phi^{k_{1}}K\\dots\\Phi^{k_{q-2}}K\\Phi^{k_{q-1}+k_{q}}K]-\\mathrm{Tr}[\\Phi^{k_{1}}K\\dots\\Phi^{k_{q-2}}K\\Phi^{k_{q-1}}K\\Phi^{k_{q}}K]|}\\\\ {=}&{|\\mathrm{Tr}[\\Phi^{k_{1}}K\\dots\\Phi^{k_{q-2}}K\\Phi^{k_{q-1}}(I-K)\\Phi^{k_{q}}K]|}\\\\ {=}&{|\\mathrm{Tr}[\\sqrt{K}\\Phi^{k_{1}}K\\dots\\Phi^{k_{q-2}}\\sqrt{K}\\sqrt{K}\\Phi^{k_{q-1}}\\sqrt{I-K}\\sqrt{I-K}\\Phi^{k_{q}}\\sqrt{K}]|}\\\\ {\\leq}&{\\|\\sqrt{K}\\Phi^{k_{1}}K\\dots\\Phi^{k_{q-2}}\\sqrt{K}\\sqrt{K}\\Phi^{k_{q-1}}\\sqrt{I-K}\\|_{\\mathrm{HS}}\\cdot\\|\\sqrt{I-K}\\Phi^{k_{q}}\\sqrt{K}\\|_{\\mathrm{HS}}}\\\\ {\\leq}&{\\|\\sqrt{K}\\Phi^{k_{1}}K\\dots\\Phi^{k_{q-2}}\\sqrt{K}\\|_{\\mathrm{op}}\\cdot\\|\\sqrt{K}\\Phi^{k_{q-1}}\\sqrt{I-K}\\|_{\\mathrm{HS}}\\cdot\\|\\sqrt{I-K}\\Phi^{k_{q}}\\sqrt{K}\\|_{\\mathrm{HS}}}\\\\ {\\leq}&{\\|\\sqrt{K}\\Phi^{k_{1}}K\\dots\\Phi^{k_{q-2}}\\sqrt{K}\\|_{\\mathrm{op}}\\cdot k_{q-1}k_{q}\\|\\varphi\\|_{\\infty}^{k_{q-1}+k_{q}-2}\\|\\sqrt{I-K}\\Phi^{k_{q}}\\sqrt{K}\\|_{\\mathrm{HS}}}\\\\ {\\leq}&{k_{q-1}k_{q}\\|\\varphi\\|_{\\infty}^{k_{-2}}C_{2}(\\varphi),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "here we used Lemma 1, the fact that $0~\\preceq~K~\\preceq~I$ , $\\|\\Phi\\|_{\\mathrm{op}}\\;=\\;\\|\\varphi\\|_{\\infty}$ and the $\\|\\cdot\\|_{\\mathrm{op}}$ norm is submultiplicative. Since $k_{j}\\leq k$ for all $1\\le j\\le q$ , using a telescoping argument gives ", "page_idx": 13}, {"type": "equation", "text": "$$\n|\\mathrm{Tr}[\\Phi^{k_{1}}K\\,.\\,.\\,.\\,\\Phi^{k_{q}}K]-\\mathrm{Tr}[\\Phi^{k}K]|\\leq q k^{2}\\|\\varphi\\|_{\\infty}^{k-2}C_{2}(\\varphi).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Hence ", "page_idx": 13}, {"type": "equation", "text": "$$\n|C_{k}(\\varphi)|\\leq\\sum_{q=2}^{k}\\sum_{\\stackrel{k_{1},\\ldots,k_{q}\\geq1}{k_{1}+\\ldots+k_{q}=k}}\\frac{k!}{k_{1}!\\ldots k_{q}!}k^{2}\\|\\varphi\\|_{\\infty}^{k-2}C_{2}(\\varphi).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Now observe that for $k\\geq3$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\sum_{q=2\\atop k_{1}+\\ldots+k_{q}\\geq1}^{k}{\\frac{1}{k_{1}!\\ldots k_{q}!}}<{\\frac{k^{k}}{k!}}\\leq{\\frac{e^{k}}{\\sqrt{2\\pi k}}}\\cdot\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Thus, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{|C_{k}(\\varphi)|}{k!}\\leq\\frac{1}{\\sqrt{2\\pi}}e^{k}k^{3/2}\\|\\varphi\\|_{\\infty}^{k-2}C_{2}(\\varphi).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Combining all ingredients above, one can show that. ", "page_idx": 13}, {"type": "text", "text": "Lemma 3. For $|t|\\leq1/(3\\|\\varphi\\|_{\\infty})$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n|\\log\\mathbb{E}[e^{t\\Lambda(\\varphi)}]-t\\mathbb{E}[\\Lambda(\\varphi)]|\\leq A t^{2}\\,\\mathbb{V}\\mathrm{ar}\\left[\\Lambda(\\varphi)\\right],\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $A>0$ is an universal constant. ", "page_idx": 13}, {"type": "text", "text": "Proof. For $|t|\\leq\\frac{1}{3\\|\\varphi\\|_{\\infty}}$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\log\\mathbb{E}[e^{t\\Delta(\\varphi)}]-t\\mathbb{E}[\\Lambda(\\varphi)]|}&{=}&{\\Big|\\displaystyle\\sum_{k\\geq2}\\frac{C_{k}(\\varphi)}{k!}t^{k}\\Big|}\\\\ &{\\leq}&{\\displaystyle\\sum_{k\\geq2}\\frac{|C_{k}(\\varphi)|}{k!}|t|^{k}}\\\\ &{\\leq}&{|t|^{2}C_{2}(\\varphi)\\Big(\\frac{1}{2}+\\displaystyle\\sum_{k\\geq3}\\frac{1}{\\sqrt{2\\pi}}e^{k}k^{3/2}\\|\\varphi\\|_{\\infty}^{k-2}|t|^{k-2}\\Big)}\\\\ &{\\leq}&{|t|^{2}C_{2}(\\varphi)\\Big(\\frac{1}{2}+\\displaystyle\\frac{1}{\\sqrt{2\\pi}}e^{2}\\displaystyle\\sum_{k\\geq3}k^{3/2}(e/3)^{k-2}\\Big)}\\\\ &{=}&{A t^{2}\\Psi\\Psi[\\Lambda(\\varphi)]\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $A>0$ is some universal constant. ", "page_idx": 14}, {"type": "text", "text": "We can finish the proof of Theorem 1 as follows. ", "page_idx": 14}, {"type": "text", "text": "Proof of Theorem 1. Let $\\varepsilon>0$ . We have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\log\\mathbb{P}(\\Lambda(\\varphi)-\\mathbb{E}[\\Lambda(\\varphi)]\\geq\\varepsilon)}&{\\leq}&{\\operatorname*{inf}_{t}\\left(\\log\\mathbb{E}[e^{t\\Lambda(\\varphi)}]-t\\mathbb{E}[\\Lambda(\\varphi)]-t\\varepsilon\\right)}\\\\ &{\\leq}&{\\operatorname*{inf}_{t}\\Big(-t\\varepsilon+t^{2}A\\,\\mathbb{V}\\mathrm{ar}\\left[\\Lambda(\\varphi)\\right]\\Big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the infimum is taken on $t\\in(0,1/(3\\|\\varphi\\|_{\\infty})]$ . ", "page_idx": 14}, {"type": "text", "text": "For $\\begin{array}{r}{0\\leq\\varepsilon\\leq\\frac{2A\\operatorname{\\mathbb{V}a r}[\\Lambda(\\varphi)]}{3\\|\\varphi\\|_{\\infty}}}\\end{array}$ 2A Var[\u039b(\u03c6)], choosing ", "page_idx": 14}, {"type": "equation", "text": "$$\nt_{0}={\\frac{\\varepsilon}{2A\\operatorname{\\mathbb{V}}\\!\\arg[\\Lambda(\\varphi)]}}\\leq{\\frac{1}{3\\|\\varphi\\|_{\\infty}}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "gives ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\log\\mathbb{P}(\\Lambda(\\varphi)-\\mathbb{E}[\\Lambda(\\varphi)]\\geq\\varepsilon)\\leq-\\frac{\\varepsilon^{2}}{4A\\,\\mathbb{V}\\mathrm{ar}\\left[\\Lambda(\\varphi)\\right]}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "as desired. ", "page_idx": 14}, {"type": "text", "text": "A.2 Proof of Theorem 2 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Denote by $\\Phi$ the diagonal matrix $\\mathrm{Diag}(\\varphi)\\in\\mathbb{R}^{n\\times n}$ . For each $t\\in\\mathbb R$ , we define ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{G}_{t}:=\\mathbf{I}-\\exp(t\\Phi)=-\\sum_{k=1}^{\\infty}\\frac{t^{k}}{k!}\\Phi^{k}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By the Campbell formula, we have $\\mathbb{E}[e^{t\\Lambda(\\varphi)}]=\\operatorname*{det}[\\mathbf{I}-\\mathbf{G}_{t}\\mathbf{K}],t\\in\\mathbb{R}$ . By choosing $t\\geq0$ small enough such that $\\|\\mathbf{G}_{t}\\mathbf{K}\\|_{\\mathrm{op}}<1$ , one can expand ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\log\\operatorname*{det}[{\\bf I}-{\\bf G}_{t}{\\bf K}]=-\\sum_{k=1}^{\\infty}\\frac{1}{k}\\mathrm{Tr}[({\\bf G}_{t}{\\bf K})^{k}].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Observe that $\\|\\mathbf{G}_{t}\\|_{\\mathrm{op}}\\leq e^{|t|\\|\\varphi\\|_{\\infty}}-1\\leq2|t|\\|\\varphi\\|_{\\infty}$ for all $|t|\\leq1/(3\\|\\varphi\\|_{\\infty})$ . From now on, we will consider $t\\geq0$ such that ", "page_idx": 14}, {"type": "equation", "text": "$$\n0\\leq t\\|\\varphi\\|_{\\infty}M\\leq\\frac{1}{3},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $M:=\\operatorname*{max}(||\\mathbf{K}||_{\\mathrm{op}},1)$ . This choice for $t$ will particularly imply that $\\|\\mathbf{G}_{t}\\mathbf{K}\\|_{\\mathrm{op}}\\leq2/3$ . ", "page_idx": 14}, {"type": "text", "text": "For $k=1$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{-\\mathrm{Tr}[\\mathbf{G}_{t}\\mathbf{K}]}&{=}&{\\displaystyle\\sum_{p=1}^{\\infty}\\frac{t^{p}}{p!}\\mathrm{Tr}[\\Phi^{p}\\mathbf{K}]}\\\\ &{\\leq}&{t\\mathbb{E}[\\Lambda(\\varphi)]+\\displaystyle\\frac{t^{2}}{2}\\mathrm{Tr}[\\Phi^{2}\\mathbf{K}]+\\sum_{p\\geq3}\\frac{t^{p}}{p!}[\\mathrm{Tr}[\\Phi^{p}\\mathbf{K}]|}\\\\ &{\\leq}&{t\\mathbb{E}[\\Lambda(\\varphi)]+\\displaystyle\\frac{t^{2}}{2}\\mathrm{Tr}[\\Phi^{2}\\mathbf{K}]+\\sum_{p\\geq3}\\frac{t^{p}}{p!}\\|\\varphi\\|_{\\infty}^{p}\\|\\mathbf{K}\\|_{*}}\\\\ &{\\leq}&{t\\mathbb{E}[\\Lambda(\\varphi)]+\\displaystyle\\frac{t^{2}}{2}\\mathrm{Tr}[\\Phi^{2}\\mathbf{K}]+t^{3}\\|\\varphi\\|_{\\infty}^{3}\\|\\mathbf{K}\\|_{*}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For $k=2$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{-\\displaystyle\\frac{1}{2}\\mathrm{Tr}[({\\bf G}_{t}{\\bf K})^{2}]}&{=}&{\\displaystyle-\\frac{1}{2}\\sum_{p,q\\geq1}\\frac{t^{p+q}}{p!q!}\\mathrm{Tr}[\\Phi^{p}{\\bf K}\\Phi^{q}{\\bf K}]}\\\\ &{=}&{\\displaystyle-\\frac{t^{2}}{2}\\mathrm{Tr}[\\Phi{\\bf K}\\Phi{\\bf K}]-\\frac{1}{2}\\sum_{p+q\\geq3}\\frac{t^{p+q}}{p!q!}\\mathrm{Tr}[\\Phi^{p}{\\bf K}\\Phi^{q}{\\bf K}]}\\\\ &{\\leq}&{\\displaystyle-\\frac{t^{2}}{2}\\mathrm{Tr}[\\Phi{\\bf K}\\Phi{\\bf K}]+\\frac{1}{2}\\sum_{l\\geq3}\\frac{t^{l}}{l!}2^{l}\\|\\varphi\\|_{\\infty}^{l}\\|{\\bf K}\\|_{\\infty}\\|{\\bf K}\\|_{*}}\\\\ &{\\leq}&{\\displaystyle-\\frac{t^{2}}{2}\\mathrm{Tr}[\\Phi{\\bf K}\\Phi{\\bf K}]+t^{3}\\|\\varphi\\|_{\\infty}^{3}\\|{\\bf K}\\|_{*}\\|{\\bf K}\\|_{\\infty}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For $k\\geq3$ , we observe that ", "page_idx": 15}, {"type": "text", "text": "| $\\mathrm{Tr}[(\\mathbf G_{t}\\mathbf K)^{k}]\\vert\\leq\\|(\\mathbf G_{t}\\mathbf K)^{k}\\|_{*}\\leq\\|\\mathbf G_{t}\\mathbf K\\|_{\\mathrm{op}}^{k-3}\\|\\mathbf G_{t}\\|_{\\mathrm{op}}^{3}\\|\\mathbf K\\|_{\\mathrm{op}}^{2}\\|\\mathbf K\\|_{*}\\leq\\Big(\\frac{2}{3}\\Big)^{k-3}(2t\\|\\varphi\\|_{\\infty})^{3}\\|\\mathbf K\\|_{\\mathrm{op}}^{2}\\|\\mathbf K\\|_{*}.$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{k\\geq3}\\frac{1}{k}\\vert\\mathrm{Tr}[(\\mathbf{G}_{t}\\mathbf{K})^{k}]\\vert\\leq8t^{3}\\|\\varphi\\|_{\\infty}^{3}\\|\\mathbf{K}\\|_{\\mathrm{op}}^{2}\\|\\mathbf{K}\\|_{*}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Combining all ingredients, we deduce that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\log\\mathbb{E}[e^{t\\Lambda(\\varphi)}]}&{\\leq}&{t\\mathbb{E}[\\Lambda(\\varphi)]+\\displaystyle\\frac{t^{2}}{2}\\,\\mathbb{V}\\mathrm{ar}\\left[\\Lambda(\\varphi)\\right]+t^{3}\\|\\varphi\\|_{\\infty}^{3}\\|{\\bf K}\\|_{*}(1+\\|{\\bf K}\\|_{\\mathrm{op}}+8\\|{\\bf K}\\|_{\\mathrm{op}}^{2})}\\\\ &{\\leq}&{t\\mathbb{E}[\\Lambda(\\varphi)]+\\displaystyle\\frac{t^{2}}{2}\\,\\mathbb{V}\\mathrm{ar}\\left[\\Lambda(\\varphi)\\right]+10t^{3}\\|\\varphi\\|_{\\infty}^{3}\\|{\\bf K}\\|_{*}M^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Let $\\varepsilon>0$ . We have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\log\\mathbb{P}(\\Lambda(\\varphi)-\\mathbb{E}[\\Lambda(\\varphi)]\\geq\\varepsilon)}&{\\leq}&{\\displaystyle\\operatorname*{inf}\\left(\\log\\mathbb{E}[e^{t\\Lambda(\\varphi)}]-t\\mathbb{E}[\\Lambda(\\varphi)]-t\\varepsilon\\right)}\\\\ &{\\leq}&{\\displaystyle\\operatorname*{inf}_{t}\\Big(-t\\varepsilon+\\frac{t^{2}}{2}\\,\\Psi\\mathrm{ar}\\left[\\Lambda(\\varphi)\\right]+t^{3}\\cdot10\\|\\varphi\\|_{\\infty}^{3}\\|\\mathbf{K}\\|_{*}M^{2}\\Big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the infimum is taken on $t\\in(0,1/(3\\|\\varphi\\|_{\\infty}M)]$ . ", "page_idx": 15}, {"type": "text", "text": "$\\begin{array}{r}{0\\leq\\varepsilon\\leq\\frac{\\mathbb{V}\\mathrm{ar}[\\Lambda(\\varphi)]^{2}}{40\\|\\varphi\\|_{\\infty}^{3}\\|\\mathbf{K}\\|_{*}M^{2}}}\\end{array}$ , we choose ", "page_idx": 15}, {"type": "equation", "text": "$$\nt_{0}=\\frac{\\varepsilon}{\\mathbb{V}\\mathrm{ar}\\left[\\Lambda(\\varphi)\\right]}\\le\\frac{\\mathbb{V}\\mathrm{ar}\\left[\\Lambda(\\varphi)\\right]}{40\\|\\varphi\\|_{\\infty}^{3}\\|\\mathbf{K}\\|_{*}M^{2}}\\le\\frac{2M\\|\\varphi\\|_{\\infty}^{2}\\|\\mathbf{K}\\|_{*}}{40\\|\\varphi\\|_{\\infty}^{3}\\|\\mathbf{K}\\|_{*}M^{2}}<\\frac{1}{3\\|\\varphi\\|_{\\infty}M}\\cdot\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This choice yields ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\log\\mathbb{P}(\\Lambda(\\varphi)-\\mathbb{E}[\\Lambda(\\varphi)]\\geq\\varepsilon)\\leq-\\frac{\\varepsilon^{2}}{2\\,\\mathbb{V}\\mathrm{ar}\\left[\\Lambda(\\varphi)\\right]}+t_{0}^{3}\\cdot10\\|\\varphi\\|_{\\infty}^{3}\\|\\mathbf{K}\\|_{*}M^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note that ", "page_idx": 16}, {"type": "equation", "text": "$$\nt_{0}^{3}\\cdot10\\|\\varphi\\|_{\\infty}^{3}\\|\\mathbf{K}\\|_{*}M^{2}=\\frac{\\varepsilon^{3}}{\\Psi\\mathrm{ar}\\left[\\Lambda(\\varphi)\\right]^{3}}\\cdot10\\|\\varphi\\|_{\\infty}^{3}\\|\\mathbf{K}\\|_{*}M^{2}\\leq\\frac{\\varepsilon^{2}}{4\\,\\Psi\\mathrm{ar}\\left[\\Lambda(\\varphi)\\right]}\\cdot\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This implies ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\log\\mathbb{P}(\\Lambda(\\varphi)-\\mathbb{E}[\\Lambda(\\varphi)]\\ge\\varepsilon)\\le-\\frac{\\varepsilon^{2}}{4\\,\\mathbb{V}\\mathrm{ar}\\left[\\Lambda(\\varphi)\\right]}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "as desired. ", "page_idx": 16}, {"type": "text", "text": "A.3 Proof of Theorem 3 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof of Theorem 3. By a scaling argument, it suffices to prove for the case $\\omega_{1}=...=\\omega_{p}=1$ . We have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{l l l}{\\mathbb{P}(\\|\\boldsymbol{\\Lambda}(\\Phi)-\\mathbb{E}[\\boldsymbol{\\Lambda}(\\Phi)]\\|_{2}\\geq\\varepsilon)}&{=}&{\\mathbb{P}\\Big(\\displaystyle\\sum_{i=1}^{p}|\\boldsymbol{\\Lambda}(\\varphi_{i})-\\mathbb{E}[\\boldsymbol{\\Lambda}(\\varphi_{i})]|^{2}\\geq\\varepsilon^{2}\\Big)}\\\\ &{=}&{\\mathbb{P}\\Big(\\displaystyle\\sum_{i=1}^{p}|\\boldsymbol{\\Lambda}(\\varphi_{i})-\\mathbb{E}[\\boldsymbol{\\Lambda}(\\varphi_{i})]|^{2}\\geq\\varepsilon^{2}\\displaystyle\\sum_{i=1}^{p}\\displaystyle\\frac{\\mathrm{Var}\\left[\\boldsymbol{\\Lambda}(\\varphi_{i})\\right]}{\\|\\nabla(\\Phi)\\|_{2}^{2}}\\Big)}\\\\ &{\\leq}&{\\displaystyle\\sum_{i=1}^{p}\\mathbb{P}\\Big(|\\boldsymbol{\\Lambda}(\\varphi_{i})-\\mathbb{E}[\\boldsymbol{\\Lambda}(\\varphi_{i})]|\\geq\\varepsilon\\displaystyle\\frac{\\sqrt{\\mathrm{Var}\\left[\\boldsymbol{\\Lambda}(\\varphi_{i})\\right]}}{\\|\\nabla(\\Phi)\\|_{2}}\\Big).}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For each $1\\leq i\\leq p$ , applying Theorem 1 gives ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\small\\gamma\\Big(|\\Lambda(\\varphi_{i})-\\mathbb{E}[\\Lambda(\\varphi_{i})]|\\geq\\varepsilon\\frac{\\sqrt{\\mathbb{V}\\mathrm{ar}\\left[\\Lambda(\\varphi_{i})\\right]}}{\\|\\mathbb{V}(\\Phi)\\|_{2}}\\Big)\\leq2\\exp\\Big(-\\frac{\\varepsilon^{2}}{4A\\|\\mathbb{V}(\\Phi)\\|_{2}^{2}}\\Big),\\forall0\\leq\\varepsilon\\leq\\frac{2A\\|\\mathbb{V}(\\Phi)\\|_{2}\\sqrt{\\mathbb{V}\\mathrm{ar}\\left[\\Lambda(\\varphi_{i})\\right]}}{3\\|\\varphi_{i}\\|_{\\infty}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The theorem follows. ", "page_idx": 16}, {"type": "text", "text": "A.4 Proof of Theorem 4 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Using (A.4), we deduce that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left|\\frac{L_{S}(f)}{L(f)}-1\\right|\\leq\\frac{1}{c n}|L_{S}(f)-L(f)|,\\quad\\forall f\\in\\mathcal{F}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This implies ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(\\exists f\\in\\mathcal{F}:\\Big|\\frac{L_{S}(f)}{L(f)}-1\\Big|\\geq\\varepsilon\\Big)\\leq\\mathbb{P}\\Big(\\exists f\\in\\mathcal{F}:\\frac{1}{n}|L_{S}(f)-L(f)|\\geq c\\varepsilon\\Big).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus, it suffices to bound the RHS. For each $f\\in\\mathcal F$ , let $V\\geq\\mathbb{V}\\mathrm{ar}\\left[n^{-1}L_{S}(f)\\right]$ , we apply Theorem 1 for the linear statistic ${\\cal L}_{S}(f)=\\Lambda(f/{\\bf K})$ to obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(\\frac{1}{n}|L_{\\mathcal{S}}(f)-L(f)|\\geq c\\varepsilon\\Big)\\leq2\\exp\\Big(-\\frac{c^{2}\\varepsilon^{2}}{4A V}\\Big),\\quad\\forall0\\leq\\varepsilon\\leq\\frac{2A n V}{3c\\|f/\\mathbf{K}\\|_{\\infty}}\\cdot\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Using $\\mathbf{K}(x,x)\\geq\\rho m/n$ , we deduce that the above inequality holds for any $\\begin{array}{r}{0\\leq\\varepsilon\\leq\\frac{2A\\rho m V}{3c\\|f\\|_{\\infty}}}\\end{array}$ ", "page_idx": 16}, {"type": "text", "text": "Proof of Theorem 4: Assuming (A.1). We let $\\mathcal{F}_{\\mathrm{sym}}\\,:=\\,\\{\\lambda f\\,:\\,|\\lambda|\\,\\leq\\,1,f\\,\\in\\,\\mathcal{F}\\}$ , and let $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ be the convex hull of $\\mathcal{F}_{\\mathrm{sym}}$ . Since $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ is a symmetric convex body in $\\overline{{\\mathcal{F}}}$ , there exists a norm $\\Vert\\cdot\\Vert_{\\mathcal{F}}$ in $\\overline{{\\mathcal{F}}}$ such that $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ is the unit ball in $(\\overline{{\\mathcal{F}}},\\|\\cdot\\|_{\\mathcal{F}})$ . ", "page_idx": 16}, {"type": "text", "text": "Define ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{L}(f):=\\frac{1}{n}\\Big(L_{\\mathcal{S}}(f)-L(f)\\Big),\\quad f\\in\\overline{{\\mathcal{F}}},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "then it is clear that $\\mathcal{L}(f)$ is linear in $f$ . Moreover, for any $f,g\\in{\\overline{{\\mathcal{F}}}}$ , one has ", "page_idx": 16}, {"type": "equation", "text": "$$\n|{\\mathcal{L}}(f)-{\\mathcal{L}}(g)|=|{\\mathcal{L}}(f-g)|=\\|f-g\\|_{\\mathcal{F}}\\cdot\\left|{\\mathcal{L}}\\Big(\\frac{f-g}{\\|f-g\\|_{\\mathcal{F}}}\\Big)\\right|\\leq\\|f-g\\|_{\\mathcal{F}}\\operatorname*{sup}_{h\\in B}|{\\mathcal{L}}(h)|.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For each $\\delta>0$ , let $\\boldsymbol{{\\mathit{\\varepsilon}}}_{\\delta\\delta}$ be a $\\delta$ -net for $(B,\\parallel\\cdot\\parallel_{\\mathcal{F}})$ . By definition of a $\\delta$ -net, for any $f\\in B$ , there exists an $f_{0}\\in B_{\\delta}$ such that $\\|f-f_{0}\\|_{\\mathcal{F}}\\leq\\delta$ . Thus, for every $f\\in B$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n|\\mathcal{L}(f)|\\leq|\\mathcal{L}(f_{0})|+|\\mathcal{L}(f)-\\mathcal{L}(f_{0})|\\leq|\\mathcal{L}(f_{0})|+\\delta\\operatorname*{sup}_{h\\in B}|\\mathcal{L}(h)|\\leq\\operatorname*{sup}_{g\\in B_{\\delta}}|\\mathcal{L}(g)|+\\delta\\operatorname*{sup}_{h\\in B}|\\mathcal{L}(h)|.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This implies $\\begin{array}{r}{\\operatorname*{sup}_{f\\in\\mathcal{B}}|\\mathcal{L}(f)|\\leq\\frac{1}{1-\\delta}\\operatorname*{sup}_{f\\in\\mathcal{B}_{\\delta}}|\\mathcal{L}(f)|,\\,\\forall0<\\delta<1.}\\end{array}$ In particular, choosing $\\delta=1/2$ gives ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{f\\in\\mathcal{B}}|\\mathcal{L}(f)|\\leq2\\operatorname*{sup}_{f\\in\\mathcal{B}_{1/2}}|\\mathcal{L}(f)|.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(\\underset{f\\in B}{\\operatorname*{sup}}\\,|\\mathcal{L}(f)|\\geq c\\varepsilon\\Big)\\leq\\mathbb{P}\\Big(2\\underset{f\\in B_{1/2}}{\\operatorname*{sup}}|\\mathcal{L}(f)|\\geq c\\varepsilon\\Big)=\\mathbb{P}\\Big(\\exists f\\in B_{1/2}:\\frac{1}{n}|L s(f)-L(f)|\\geq c\\varepsilon/2\\Big).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Let $N(B,\\|\\cdot\\|_{\\mathcal{F}},1/2)$ be the $1/2$ -covering number of $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ , then ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(\\exists f\\in\\mathcal{B}:\\frac{1}{n}|\\hat{L}_{\\mathcal{S}}(f)-L(f)|\\ge c\\varepsilon\\Big)\\le N(\\mathcal{B},\\|\\cdot\\|_{\\mathcal{F}},1/2)\\cdot2e^{-c^{2}\\varepsilon^{2}/16A V},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for any ", "page_idx": 17}, {"type": "equation", "text": "$$\nV\\geq\\operatorname*{sup}_{f\\in\\mathcal{B}}\\mathbb{V}\\mathrm{ar}\\left[\\frac{1}{n}L_{S}(f)\\right]\\quad,\\quad0\\leq\\varepsilon\\leq\\frac{4A\\rho m V}{3c\\operatorname*{sup}_{f\\in\\mathcal{B}}\\left\\|f\\right\\|_{\\infty}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note that for a finite dimensional normed vector space, for $0<\\delta<1$ , one has ", "page_idx": 17}, {"type": "equation", "text": "$$\nN({\\mathcal{B}},\\|\\cdot\\|_{\\mathcal{F}},\\delta)\\leq\\left({\\frac{3}{\\delta}}\\right)^{\\dim{\\overline{{\\mathcal{F}}}}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This implies ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(\\exists f\\in\\mathcal{B}:\\frac{1}{n}|L_{\\mathcal{S}}(f)-L(f)|\\ge c\\varepsilon\\Big)\\le2\\exp\\Big(6D-\\frac{c^{2}\\varepsilon^{2}}{16A V}\\Big).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since ${\\mathcal{F}}\\subset B$ , it is clear that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\Big(\\exists f\\in\\mathcal{F}:|L_{S}(f)-L(f)|\\ge n c\\varepsilon\\Big)\\le\\mathbb{P}\\Big(\\exists f\\in\\mathcal{B}:|L_{S}(f)-L(f)|\\ge n c\\varepsilon\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "On the other hand, for each $f\\,\\in\\,\\mathcal B$ , there exist $0\\,\\leq\\,t\\,\\leq\\,1,|\\lambda_{i}|\\,\\leq\\,1,f_{i}\\,\\in\\,\\mathcal{F},i\\,=\\,1,2$ such that $f=t\\lambda_{1}f_{1}+(1-t)\\lambda_{2}f_{2}$ . Therefore ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathbb{V}\\mathrm{ar}\\left[L_{S}(f)\\right]^{1/2}}&{=}&{\\mathbb{V}\\mathrm{ar}\\left[L_{S}(t\\lambda_{1}f_{1})+L_{S}((1-t)\\lambda_{2}f_{2})\\right]^{1/2}}\\\\ &{\\leq}&{\\mathbb{V}\\mathrm{ar}\\left[L_{S}(t\\lambda_{1}f_{1})\\right]^{1/2}+\\mathbb{V}\\mathrm{ar}\\left[L_{S}((1-t)\\lambda_{2}f_{2})\\right]^{1/2}}\\\\ &{\\leq}&{t\\,\\mathbb{V}\\mathrm{ar}\\left[L_{S}(f_{1})\\right]^{1/2}+(1-t)\\,\\mathbb{V}\\mathrm{ar}\\left[L_{S}(f_{2})\\right]^{1/2}}\\\\ &{\\leq}&{\\underset{g\\in\\mathcal{F}}{\\operatorname*{sup}}\\,\\mathbb{V}\\mathrm{ar}\\left[L_{S}(g)\\right]^{1/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Moreover, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|f\\|_{\\infty}=\\operatorname*{sup}_{x\\in\\mathcal{X}}|f(x)|\\leq t\\operatorname*{sup}_{x\\in\\mathcal{X}}|f_{1}(x)|+(1-t)\\operatorname*{sup}_{x\\in\\mathcal{X}}|f_{2}(x)|\\leq\\operatorname*{sup}_{g\\in\\mathcal{F}}\\|g\\|_{\\infty}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{f\\in\\mathcal{B}}\\mathbb{V}\\mathrm{ar}\\left[L_{\\mathcal{S}}(f)\\right]=\\operatorname*{sup}_{f\\in\\mathcal{F}}\\mathbb{V}\\mathrm{ar}\\left[L_{\\mathcal{S}}(f)\\right]\\quad,\\quad\\operatorname*{sup}_{f\\in\\mathcal{B}}\\|f\\|_{\\infty}=\\operatorname*{sup}_{f\\in\\mathcal{F}}\\|f\\|_{\\infty}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "From (8), (9), (10), the theorem follows. ", "page_idx": 17}, {"type": "text", "text": "Proof of Theorem 4: Assuming (A.2), (A.3). We define $\\begin{array}{r}{\\mathcal{L}(\\theta):=\\frac{1}{n}(L_{S}(f_{\\theta})-L(f_{\\theta})),\\theta\\in\\Theta}\\end{array}$ . Then ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(\\exists f\\in\\mathcal{F}:\\frac{1}{n}|L_{S}(f)-L(f)|\\ge c\\varepsilon\\Big)=\\mathbb{P}(\\exists\\,\\theta\\in\\Theta:|\\mathcal{L}(\\theta)|\\ge c\\varepsilon)=\\mathbb{P}(\\operatorname*{sup}_{\\theta\\in\\Theta}|\\mathcal{L}(\\theta)|\\ge c\\varepsilon).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Using (A.3), we have $|L(f_{\\theta})-L(f_{\\theta^{\\prime}})|\\leq n\\ell\\|\\theta-\\theta^{\\prime}\\|$ and ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\lvert L_{S}(f_{\\theta})-L_{S}(f_{\\theta^{\\prime}})\\rvert\\le\\ell\\lVert\\theta-\\theta^{\\prime}\\rVert\\Big(\\sum_{x\\in S}\\frac{1}{\\mathbf{K}(x,x)}\\Big)\\le\\ell\\lVert\\theta-\\theta^{\\prime}\\rVert n\\rho^{-1}m^{-1}|S|.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This implies $|\\mathcal{L}(\\theta)-\\mathcal{L}(\\theta^{\\prime})|\\leq C\\|\\theta-\\theta^{\\prime}\\|$ a.s., for some constant $C$ depending on $B,\\rho,\\ell.$ . Let $\\Gamma$ be a $\\frac{c\\varepsilon}{2C}$ -net for $\\Theta$ , then ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\theta\\in\\Theta}|\\mathcal{L}(\\theta)|\\leq\\operatorname*{sup}_{\\theta^{\\prime}\\in\\Gamma}|\\mathcal{L}(\\theta^{\\prime})|+\\frac{c\\varepsilon}{2}\\cdot\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\theta\\in\\Theta}\\,|\\mathcal{L}(\\theta)|\\ge c\\varepsilon)\\le\\mathbb{P}_{\\theta^{\\prime}\\in\\Gamma}\\,|\\mathcal{L}(\\theta^{\\prime})|\\ge c\\varepsilon/2)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We note that $|\\Gamma|=O(\\varepsilon^{-D})$ . This completes the proof. ", "page_idx": 18}, {"type": "text", "text": "Remark 6.3. Without the assumption $|S|\\,\\le\\,B\\,\\cdot\\,m\\ .$ a.s., one can continue from (11) as follows. Denote by $\\lambda_{1}\\geq.\\,.\\,.\\geq\\lambda_{n}\\geq0$ the eigenvalues of $\\mathbf{K}$ , it is known that $|S|=\\!^{d}\\,X_{1}+.\\,.\\,.\\,+X_{n}$ , where $X_{i}\\sim B e r(\\lambda_{i})$ are independent. Let $B>0$ , then using a multiplicative Chernoff bound for the sum of independent Bernoulli variables gives ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{P}(|S|>(B+1)m)=\\mathbb{P}\\Big(\\sum_{i=1}^{n}X_{i}>(B+1)m\\Big)\\le\\exp\\Big(-\\frac{B^{2}}{B+2}m\\Big).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By choosing $B$ large, this event will have small probability. Meanwhile, on the event $\\{|S|\\ \\leq$ $(B+1)m\\}$ , we can use exactly the same argument as in the proof above. ", "page_idx": 18}, {"type": "text", "text": "A.5 Proof of Theorem 5 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof of Theorem 5. It suffices to show for the case $\\omega_{1}\\,=\\,.\\ldots\\,=\\,\\omega_{p}\\,=\\,1$ . For each f $\\in\\mathcal{F}$ , by applying Theorem 1 and an union bound argument, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(\\frac{1}{n}\\|L_{S}(\\mathbf{f})-L(\\mathbf{f})\\|\\geq\\varepsilon\\Big)\\leq2p\\exp\\Big(-\\frac{c^{2}\\varepsilon^{2}}{4A V}\\Big),\\quad\\forall0\\leq\\varepsilon\\leq\\frac{2A\\rho m V}{3c\\operatorname*{max}_{i}\\|f_{i}\\|_{\\infty}}\\cdot\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Using the same argument as in the proof of Theorem 4 under assumption (A.1) gives the result. ", "page_idx": 18}, {"type": "text", "text": "A.6 Proof of Theorem 6 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof of Theorem 6. We remark that Var $\\left[n^{-1}L_{S}(f)\\right]=\\mathcal{O}(m^{-(1+1/d)})$ uniformly for all $f\\in\\mathcal F$ , w.h.p. in the data set $\\mathcal{X}$ . Hence, Theorem 6 is a direct application of Theorem 4 with $V\\,=$ $C m^{-(1+1/d)}$ for some constant $C\\,>\\,0$ . As we discussed in the Remark 4.1, the range for $\\varepsilon$ is $O(m^{-1/d})$ . Thus, it suffices to check the conditions on $\\hat{\\bf K}$ . Since $\\hat{\\bf K}$ is a projection of rank $m$ , $|S|=m$ a.s. Moreover, we have $n\\hat{\\bf K}(x,x)={\\bf K}(x,x)$ , which is typically of order $m$ , where we used an uniform CLT result and an asymptotic for multivariate OPE kernels (see Bardenet, Ghosh, et al., 2021 for more details). ", "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We claim theoretical results, which are established as theorems in the main text. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We discuss limitations in Section 5. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We give our assumptions and results in Section 3. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We give experimental details in Section 4. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We will release code to reproduce the experimental section in a public GitHub repository upon acceptance. We stress that our contributions are the theoretical results in the main paper; the code is there only to support our theoretical claims. ", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We give details in Section 4. ", "page_idx": 19}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [No] ", "page_idx": 19}, {"type": "text", "text": "Justification: Our metric is a quantile of a worst-case error; there is no standard error bar. ", "page_idx": 19}, {"type": "text", "text": "8. Experiments Compute Resources ", "page_idx": 19}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Our experiments are small-scale and were performed on a personal computer;   \nwe mention this in Section 4. ", "page_idx": 19}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] Justification: It is a theoretical paper, and we cannot see any potential harmful consequence. ", "page_idx": 20}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: It is a theoretical paper, so societal impact is a long way downstream. A positive impact we can see in the study of coresets is the possibility of saving energy when training large models. ", "page_idx": 20}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper poses no such risk. ", "page_idx": 20}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: we properly cite the Python libraries we use. ", "page_idx": 20}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 20}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 20}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 20}]