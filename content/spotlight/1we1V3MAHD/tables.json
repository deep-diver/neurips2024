[{"figure_path": "1we1V3MAHD/tables/tables_7_1.jpg", "caption": "Table 1: Quantitative comparison for motion-aware customized video generation.", "description": "This table presents a quantitative comparison of MotionBooth against other state-of-the-art methods for motion-aware customized video generation.  The comparison uses two different text-to-video models, Zeroscope and LaVie, and evaluates the performance across several metrics. These metrics assess the quality of the generated videos by considering subject fidelity (using R-CLIP and R-DINO scores, which measure the similarity between the generated subject and the input subject), text-to-video alignment (using CLIP-T score), temporal consistency (T-Cons), and flow error.  The results illustrate the performance improvement achieved by MotionBooth in generating high-quality, motion-consistent videos with customized subjects, outperforming the comparative models on most metrics.", "section": "4.2 Main Results"}, {"figure_path": "1we1V3MAHD/tables/tables_7_2.jpg", "caption": "Table 2: Quantitative comparison for camera movement control.", "description": "This table compares the performance of MotionBooth's camera movement control method against several baselines. The metrics used are: FVD (Frechet Video Distance), CLIP-T (CLIP Text-Image similarity), T-Cons (Temporal consistency), and Flow Error.  MotionBooth is shown to significantly outperform the baselines in flow error, and it shows comparable or better results in other metrics while having significantly less weight storage. The \"No Training\" indicates that MotionBooth does not require additional training for camera control, unlike the baselines.", "section": "4.2 Main Results"}, {"figure_path": "1we1V3MAHD/tables/tables_8_1.jpg", "caption": "Table 3: Ablation study for training technologies. \"mask\" means subject region loss. \u201cSTCA\u201d means subject token cross-attention loss. \"video\" means video preservation loss. \"w/ class video\" means utilizing class-specific videos in video preservation loss. The results are evaluated on LaVie.", "description": "This table presents the results of an ablation study conducted to evaluate the impact of different training technologies on the performance of the MotionBooth model. The study specifically analyzes the contributions of subject region loss, subject token cross-attention loss, and video preservation loss, as well as the effect of using class-specific videos instead of general videos for video preservation loss. The results are reported in terms of several metrics, including R-CLIP, R-DINO, CLIP-T, T-Cons, and flow error, all evaluated using the LaVie model.", "section": "4.3 Ablation Studies"}, {"figure_path": "1we1V3MAHD/tables/tables_15_1.jpg", "caption": "Table 2: Quantitative comparison for camera movement control.", "description": "This table presents a quantitative comparison of different methods for camera movement control in video generation.  It compares MotionBooth (with Zeroscope and LaVie models) against several baselines (Text2Video-Zero, AnimateDiff, CameraCtrl, and MotionCtrl). The metrics used are: FVD (Frechet Video Distance), a measure of video quality; CLIP-T (CLIP Image-Text Similarity), measuring the alignment between generated video frames and text prompts; T-Cons. (Temporal Consistency), evaluating the consistency of video frames; and Flow error, representing the difference between predicted and ground truth optical flow, showing the accuracy of camera motion generation.  The table highlights that MotionBooth achieves superior performance in terms of Flow error compared to other methods, demonstrating its effectiveness in camera motion control.", "section": "4.2 Main Results"}, {"figure_path": "1we1V3MAHD/tables/tables_15_2.jpg", "caption": "Table 4: Comparison with More Baselines. (a) Comparison of the latent shift method and text guidance for camera motion control. (b) Comparison of subject motion control with more baselines.", "description": "This table presents a quantitative comparison of the proposed MotionBooth method against several baseline methods for both camera and subject motion control. The results are presented in two parts: (a) compares the latent shift method with text guidance for camera motion control and (b) compares the subject motion control capabilities of MotionBooth with other methods, assessing region CLIP similarity (R-CLIP), region DINO similarity (R-DINO), CLIP image-text similarity (CLIP-T), temporal consistency (T-Cons), and flow error. The table highlights MotionBooth's superior performance in both camera and subject motion control compared to the baselines.", "section": "A.3 Comparison with More Baselines"}, {"figure_path": "1we1V3MAHD/tables/tables_16_1.jpg", "caption": "Table 5: Ablation study for the number of video preservation data.", "description": "This table presents the results of an ablation study investigating the impact of varying the amount of video preservation data used during training on the performance of the MotionBooth model.  The study assesses the effects on several key metrics including R-CLIP, R-DINO, CLIP-T, T-Cons., and Flow error, which capture different aspects of video generation quality.  The results show that increasing the number of training videos from 100 to 900 does not significantly impact performance across these metrics.", "section": "4.3 Ablation Studies"}, {"figure_path": "1we1V3MAHD/tables/tables_17_1.jpg", "caption": "Table 1: Quantitative comparison for motion-aware customized video generation.", "description": "This table presents a quantitative comparison of MotionBooth against several baseline methods for motion-aware customized video generation.  The comparison is done using two different text-to-video models (Zeroscope and LaVie). Metrics include region CLIP similarity (R-CLIP), region DINO similarity (R-DINO), CLIP image-text similarity (CLIP-T), temporal consistency (T-Cons.), and flow error. Higher values are generally better for R-CLIP, R-DINO, CLIP-T, and T-Cons., while a lower value is better for flow error.", "section": "4.2 Main Results"}]