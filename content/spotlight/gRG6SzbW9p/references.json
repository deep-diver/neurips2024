{"references": [{"fullname_first_author": "Paul F. Christiano", "paper_title": "Deep reinforcement learning from human preferences", "publication_date": "2017-XX-XX", "reason": "This paper is foundational to the RLHF field and introduces the basic framework used in this research."}, {"fullname_first_author": "L. Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-XX-XX", "reason": "This paper is highly influential in the application of RLHF to large language models and is a key reference for recent work in this area."}, {"fullname_first_author": "A. Siththaranjan", "paper_title": "Distributional preference learning: Understanding and accounting for hidden context in RLHF", "publication_date": "2023-XX-XX", "reason": "This paper addresses the limitations of unimodal RLHF and introduces the concept of distributional preference learning, which is directly related to the proposed VPL method."}, {"fullname_first_author": "E. Biyik", "paper_title": "Batch active preference-based learning of reward functions", "publication_date": "2018-XX-XX", "reason": "This paper introduces techniques for active learning of reward functions which are adapted in this research."}, {"fullname_first_author": "E. Biyik", "paper_title": "Learning reward functions from diverse sources of human feedback: Optimally integrating demonstrations and preferences", "publication_date": "2022-XX-XX", "reason": "This paper expands on active reward learning and also considers multiple sources of preference data, a relevant concept to the work in this paper."}]}