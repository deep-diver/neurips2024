[{"figure_path": "AbTpJl7vN6/figures/figures_2_1.jpg", "caption": "Figure 1: The open-ended learning setting and the modeling approach. A. Example of the blocked curriculum with two tasks. B. Neural Task Abstraction (NTA) model updates WP through gradient descent, but also the gating variables CP, leading to task abstractions emerging in the gating layer.", "description": "Figure 1 shows the model and learning setup used in the paper. Panel A illustrates a blocked curriculum in which two tasks are presented sequentially for some time before switching to the next task.  The order of presentation is not random; it is designed to test the model's ability to adapt flexibly to task changes over time. Panel B shows the Neural Task Abstraction (NTA) model architecture. This is a one-layer linear gated neural network. The model receives an input (x) and produces an output (y) through a weighted sum of P neural pathways, where each pathway has its own weight matrix (WP) and gating variable (CP). The gating variables control which pathways are most influential in generating the output. The model aims to learn both the optimal weights for each task and the appropriate gating strategy for switching between tasks. The focus of the paper is to show how task abstractions emerge from the joint training of weights and gates in this model.", "section": "3 Approach"}, {"figure_path": "AbTpJl7vN6/figures/figures_3_1.jpg", "caption": "Figure 2: Joint gradient descent on gates and weights enables fast adaptation through gradual specialization. Learning on the blocked curriculum from Fig. 1 with te = 0.03, \u03c4\u03c9 = 1.3, and block length TB = 1.0. x-axis indicates time as multiples of TB. (Black) Flexible NTA model Eq. (1, NTA), (gray) forgetful NTA model with Tc = Tw and Anonneg = Xnorm = 0. Simulation averaged over 10 random seeds with standard error indicated. A. Loss of both models over time. B. Gate activity of flexible NTA. C. Student-teacher weight alignment W*mWpT, normalized and averaged over rows (cosine similarity) for each student-teacher pair. D., E. Norm of updates to WP and c. Dashed: norm of students correlating with update size of c. F. Time to Ltask = 0.1 for both models over blocks.", "description": "This figure shows the results of a simulation comparing two models: a flexible NTA model and a forgetful NTA model.  Both models were trained on a blocked curriculum with two alternating tasks. The flexible NTA model shows a rapid decrease in loss and faster task adaptation compared to the forgetful model.  The figure also displays the gate activity, weight alignment, update norms, and time to reach a loss of 0.1 for both models across multiple training blocks, demonstrating the benefits of the flexible gating mechanism. ", "section": "4 Task abstractions emerge through joint gradient descent"}, {"figure_path": "AbTpJl7vN6/figures/figures_4_1.jpg", "caption": "Figure 3: Flexible model generalizes to compositional tasks. A. Task composition consists of new tasks that sum sets of teachers previously encountered. B. Subtask composition consists of new tasks that concatenate alternating rows of sets of teachers previously encountered. Loss of models trained on generalization to task composition (C.) and subtask composition (D.) for the flexible (black) and forgetful (gray) NTA. \u2018New tasks\u2019 indicates the start of the generalization phase when the task curriculum is changed to cycle through the compositional tasks.", "description": "This figure demonstrates the model's ability to generalize to new tasks that are compositions of previously learned tasks.  Panel A shows task composition, where new tasks are created by summing the weight matrices of previously learned tasks. Panel B shows subtask composition, where new tasks are created by concatenating rows from previously learned tasks. Panels C and D show the loss curves for both the flexible and forgetful models on these new compositional tasks. The flexible model (black line) shows significantly faster learning on the new tasks than the forgetful model (gray line), highlighting the advantage of task abstraction in compositional generalization.", "section": "4 Task abstractions emerge through joint gradient descent"}, {"figure_path": "AbTpJl7vN6/figures/figures_5_1.jpg", "caption": "Figure 4: Mechanism of gradual task specialization in effective 2D subspace. A. Sketch of the reduced model and dynamic feedback. Out-of-subspace students gradually align to teacher axes. B. Trajectories of student weight matrices (blue, orange) in the teacher subspace during complete adaptation following a context switch from teacher 1 to teacher 2 in the flexible regime. Gray stripes indicate associated gate activation. The student weight matrices move little. C. Like (B), but for the forgetful regime. Student weight matrices entirely remap and gates do not turn off. D. Gradient of the task loss on cp as a function of the weight alignment. E. Trajectories in the specialization subspace as a function of gate timescale for values te = 0.1, 0.18, 0.32, 0.56, 1.00 comparing (color) simulations and (dashed black) analytical predictions from exact solutions under symmetry in the flexible regime. Simulations begin from initial conditions of complete specialization and separation wm = \u03b4pm, CP = \u03b4p1 and follow a complete adaptation from teacher 1 to teacher 2 over the course of a block, reaching Ltask < 10-2 for all Te.", "description": "This figure describes the mechanism of gradual task specialization in a simplified 2D subspace of the model. It shows how student weight matrices and gates interact to achieve fast adaptation in the flexible learning regime, contrasting it with the forgetful regime. The figure also illustrates the self-reinforcing feedback loops between weight specialization and gate updates, showing how faster adapting gates drive weight specialization and vice-versa.  Finally, it validates the model's dynamics using both simulations and analytical predictions.", "section": "Mechanisms of learning flexible task abstractions"}, {"figure_path": "AbTpJl7vN6/figures/figures_7_1.jpg", "caption": "Figure 5: Model specialization emerges as a function of block length, gate learning rate, and regularization strength. The colorbar indicates total alignment (cosine similarity) between all sets of students and teachers considered collectively.", "description": "This figure shows the results of grid searches on the hyperparameters of the model to determine the conditions under which specialization emerges.  The hyperparameters varied are block length, gate learning rate, and regularization strength.  The colorbar represents the total alignment (cosine similarity) between the teachers and students, acting as a measure of specialization.  The results illustrate that  a longer block length, faster gate learning rate, and sufficient regularization strength all contribute to the emergence of a specialized, flexible learning regime in the model.", "section": "6 Quantifying the range of the flexible regime across block length, regularization strength, and gate speed"}, {"figure_path": "AbTpJl7vN6/figures/figures_8_1.jpg", "caption": "Figure 6: Task-specialized gating emerges in the second layer of a 2-layer network with faster second-layer learning rate and regularization. The sorted second layer weights at the last timestep of two different task blocks (one seed).", "description": "This figure shows the weights of the second layer of a two-layer fully connected network after training on two different tasks.  The weights are sorted to show how they are specialized for different tasks (Task A and Task B). The diagonal shows the gating behavior, where the weights are strongly activated for one task and less so for the other. This demonstrates how fast learning rates and regularization on the second layer leads to the formation of task-specific gating.", "section": "7 Inducing the flexible regime in a deep fully-connected neural network"}, {"figure_path": "AbTpJl7vN6/figures/figures_8_2.jpg", "caption": "Figure 7: Learning flexible neural task abstractions in a nonlinear character recognition setting. A. We formulate two tasks, the original and a permuted version of MNIST. B. We embed the NTA system into a larger pretrained convolutional neural network architecture. C. Accuracy reached on the MNIST test set as a function of time for both (black) the NTA network and (gray) the original CNN. The two tasks are presented sequentially in blocks for both (blue shading) MNIST and (orange shading) the permuted version. D. The activation of the two gating units as a function of time. We show mean and standard error with 10 seeds.", "description": "This figure demonstrates the application of the Neural Task Abstraction (NTA) model to a non-linear classification problem using the MNIST dataset.  Panel A shows the two tasks: standard MNIST digit recognition and a permuted version where digits are reordered based on parity. Panel B illustrates the model architecture, showing how the NTA module is integrated into a pre-trained convolutional neural network (CNN). Panel C presents the accuracy results for both the NTA-enhanced CNN and a standard CNN over time, highlighting the faster adaptation of the NTA model to task switches. Panel D displays the activation patterns of the gating units in the NTA module, revealing how they selectively activate for each task.", "section": "8 Flexible remapping of representations in nonlinear networks in two MNIST tasks"}, {"figure_path": "AbTpJl7vN6/figures/figures_9_1.jpg", "caption": "Figure 8: Comparing performance after a task switch in humans and NTA model. A. Steyvers et al. [2019] report performance of humans learning two alternating tasks (CC BY-NC-ND 4.0 license). B. After a block switch, loss comparison between the flexible (left) and the forgetful (right) NTA model shows opposite trends with further training on switching speed. Bars are standard error with 10 seeds.", "description": "This figure compares the performance of humans and two different NTA models (flexible and forgetful) on a task-switching experiment. Panel A shows human data from a previous study, illustrating faster task switching with more practice. Panel B contrasts the two NTA models, demonstrating that the flexible model shows faster task switching with more training blocks while the forgetful model shows the opposite trend. Error bars represent standard error across 10 simulations.", "section": "Relations to multi-task learning in humans"}, {"figure_path": "AbTpJl7vN6/figures/figures_16_1.jpg", "caption": "Figure A.1: Simulation of dynamics of full and reduced model. The equivalent reduced model effectively captures the dynamics of the full model in terms of loss (A1., B\u2081.), gates (A2., B2.), and singular value magnitude (A3., B3).", "description": "This figure compares the dynamics of the full model and the reduced 2D model. The results show that the reduced model accurately captures the essential dynamics of the full model, as measured by the loss function, gate activation patterns, and singular value magnitudes. This validates the use of the simplified 2D model for theoretical analysis in the paper.", "section": "A Additional details on main text"}, {"figure_path": "AbTpJl7vN6/figures/figures_16_2.jpg", "caption": "Figure A.2: High-dimensional students learn slower. Gate change \u03c4dc/dt as a function of teacher dimensionality/rank (i.e., non-zero singular values). Weight scaling is chosen such that input and output components take unit scale, yi = O(1), xj = O(1).", "description": "This figure shows the relationship between the speed at which gates change (vertical axis) and the dimensionality of the teacher (horizontal axis).  It demonstrates that high-dimensional students (those with many singular values) learn slower than low-dimensional students. This is because high-dimensional students have more parameters to adjust during learning, making it slower to adapt their parameters when new information or tasks are introduced.", "section": "A.2 High-dimensional students learn slower"}, {"figure_path": "AbTpJl7vN6/figures/figures_17_1.jpg", "caption": "Figure A.1: Simulation of dynamics of full and reduced model. The equivalent reduced model effectively captures the dynamics of the full model in terms of loss (A1., B1.), gates (A2., B2.), and singular value magnitude (A3., B3.).", "description": "This figure compares the dynamics of a full linear gated neural network with its reduced 2D equivalent model.  The comparison demonstrates that the reduced model accurately reflects the full model's behavior in terms of loss function, gate activation patterns, and the magnitude of singular values. This validates the use of the simpler 2D model for analytical purposes, as it captures the essential dynamics of the more complex full model.", "section": "A Additional details on main text"}, {"figure_path": "AbTpJl7vN6/figures/figures_17_2.jpg", "caption": "Figure A.12: Gated model generalizes to compositional tasks. A. Task composition consists of new tasks that sum sets of teachers previously encountered. B. Subtask composition consists of new tasks that concatenate alternating rows of sets of teachers previously encountered. Loss (C.,D.), gating activity (E,F.), and student-teacher alignment (G.,H.) of models on generalization to task composition (top) and subtask composition (bottom).", "description": "This figure shows that the flexible gated model generalizes to compositional tasks.  In task composition, new tasks are created by summing previously learned tasks (teachers). In subtask composition, new tasks are formed by combining rows from different teachers. The figure demonstrates that the flexible model is able to adapt quickly to both task and subtask composition, showing low loss, appropriate gating activation, and strong student-teacher alignment.", "section": "A.8 Gated model generalizes to perform task and subtask composition"}, {"figure_path": "AbTpJl7vN6/figures/figures_18_1.jpg", "caption": "Figure 2: Joint gradient descent on gates and weights enables fast adaptation through gradual specialization. Learning on the blocked curriculum from Fig. 1 with te = 0.03, \u03c4\u03c9 = 1.3, and block length TB = 1.0. x-axis indicates time as multiples of TB. (Black) Flexible NTA model Eq. (1, NTA), (gray) forgetful NTA model with Tc = Tw and Anonneg = Xnorm = 0. Simulation averaged over 10 random seeds with standard error indicated. A. Loss of both models over time. B. Gate activity of flexible NTA. C. Student-teacher weight alignment W*mWpT, normalized and averaged over rows (cosine similarity) for each student-teacher pair. D., E. Norm of updates to WP and c. Dashed: norm of students correlating with update size of c. F. Time to Ltask = 0.1 for both models over blocks.", "description": "This figure displays the results of a simulation comparing two models, a flexible NTA model and a forgetful NTA model.  It demonstrates how joint gradient descent on gates and weights leads to fast adaptation through gradual specialization in the flexible model.  Multiple subplots show the loss over time, gate activity, student-teacher weight alignment, norm of updates, and time to reach a specific loss threshold, highlighting the differences between the two models and showcasing the flexible model's ability to adapt quickly to changing tasks.", "section": "4 Task abstractions emerge through joint gradient descent"}, {"figure_path": "AbTpJl7vN6/figures/figures_19_1.jpg", "caption": "Figure A.5: Regularized, but not non-regularized, fully-connected network specializes single neurons in each row as \u2018gates\u2019 per task and exhibits specificity based on task. Visualization of the unsorted second hidden layer of the flexible (left) and forgetful (right) fully-connected network for a single seed.", "description": "This figure visualizes the unsorted second hidden layer of a fully-connected network after training on two tasks. The left panel shows a regularized network exhibiting specialization with single neurons in each row acting as gates, each specific to one task. In contrast, the right panel shows a non-regularized network lacking this specificity and thus exhibiting a lack of task-specific gating.", "section": "A.4 Fully-connected network"}, {"figure_path": "AbTpJl7vN6/figures/figures_19_2.jpg", "caption": "Figure A.5: Regularized, but not non-regularized, fully-connected network specializes single neurons in each row as \u2018gates\u2019 per task and exhibits specificity based on task. Visualization of the unsorted second hidden layer of the flexible (left) and forgetful (right) fully-connected network for a single seed.", "description": "This figure shows the weight matrices of a fully connected network trained with (left) and without (right) regularization. The flexible network shows clear specialization of single neurons as gates for each task, while the forgetful network does not show this specificity. This demonstrates that regularization is crucial for the emergence of task-specific gating in a fully connected network.", "section": "A.4 Fully-connected network"}, {"figure_path": "AbTpJl7vN6/figures/figures_20_1.jpg", "caption": "Figure 5: Model specialization emerges as a function of block length, gate learning rate, and regularization strength. The colorbar indicates total alignment (cosine similarity) between all sets of students and teachers considered collectively.", "description": "This figure shows the results of a grid search over different hyperparameters: block length, gate learning rate (inverse of gate timescale), and regularization strength. The heatmaps show the total alignment (cosine similarity) between the weights of all students and teachers after training.  Higher alignment indicates greater specialization of the network's weights towards the different tasks. The figure helps illustrate the conditions under which the network enters the 'flexible' learning regime, characterized by rapid adaptation to task switches and preserved knowledge.", "section": "6 Quantifying the range of the flexible regime across block length, regularization strength, and gate speed"}, {"figure_path": "AbTpJl7vN6/figures/figures_21_1.jpg", "caption": "Figure A.8: Specialized students and gates accelerate adaptation. Heatmaps of the dot product \\((\\frac{dy}{d\\theta})^T \\epsilon\\) contributions for different terms of the Neural Tangent Kernel (NTK), depending on specialization of weight vectors \\(w^1\\) (blue), \\(w^2\\) (orange), of which three pairs corresponding to different degrees of specialization are shown here (pairs are formed by vectors that are symmetric along the diagonal). \\(c^1, c^2\\) are scaled so that the sum lies on the dashed black line (given by L1 regularization). A. shows the total contribution of both terms of Eq. (20) combined, B. isolates the contribution from the \\(\\sum_p w^p w^{pT}\\) term, and C. displays the contribution from the \\(\\sum_p c^p c^{pT}\\) term. Dashed lines indicate possible solutions.", "description": "This figure shows the contribution of different terms of the Neural Tangent Kernel (NTK) to the adaptation speed. The NTK is used to analyze how the model output changes in response to a task switch. The figure shows that the adaptation is accelerated by two factors: student-teacher alignment and selective gating. The dashed lines show the possible solutions.", "section": "A.7 Exact solutions to the learning dynamics describe protection and adaptation under symmetry in the flexible regime"}, {"figure_path": "AbTpJl7vN6/figures/figures_22_1.jpg", "caption": "Figure A.9: Larger blocks enable faster specialization. A. Two trajectories of differing block length (faint: \u03c4B, orange: \u03c4'B = 2\u03c4B) of two students (blue, orange) in teacher subspace as in Fig. 4. B. Student dynamics in an approximate loss landscape in early learning. Subpanels 1\u20134 are time points in a simulation. Background: active context m. The linear first-order loss does not lead to separation, as a block switch will exactly reverse any changes to specialization. In contrast, the curvature from the second order term enables students to accumulate an initial advantage in specialization. C. Like B., but loss is in terms of the specialization variable w. The effective loss over blocks depends on block size \u03c4B: the longer the \u03c4B is, the longer the students will have to fall down the landscape in B.. The first-order term, corresponding to infinitely short blocks, does not prefer specialization.", "description": "This figure shows how longer training blocks lead to faster specialization in a neural network model.  Panel A illustrates how longer blocks allow students (weight matrices) to move further toward specialization before a task switch reverses their progress. Panels B and C demonstrate the effect on loss and specialization (respectively) using a simplified 1D model with first-order and second-order terms in the loss function.  The first-order term (due to linear gradient descent) is shown to be insufficient for specialization, while the second-order term leads to a non-linear, double-well loss landscape that promotes stable specialization.  Longer blocks allow sufficient time for the students to descend toward a specialized state within this landscape.", "section": "A.6 Larger blocks enable faster specialization"}, {"figure_path": "AbTpJl7vN6/figures/figures_25_1.jpg", "caption": "Figure A.8: Specialized students and gates accelerate adaptation. Heatmaps of the dot product (\u03b5T dy/d\u03b8) contributions for different terms of the Neural Tangent Kernel (NTK), depending on specialization of weight vectors w\u00b9 (blue), w\u00b2 (orange), of which three pairs corresponding to different degrees of specialization are shown here (pairs are formed by vectors that are symmetric along the diagonal). c\u00b9, c\u00b2 are scaled so that the sum lies on the dashed black line (given by L\u00b9 regularization). A. shows the total contribution of both terms of Eq. (20) combined, B. isolates the contribution from the wPwPT term, and C. displays the contribution from the CPCP term. Dashed lines indicate possible solutions.", "description": "This figure analyzes how the Neural Tangent Kernel (NTK) contributes to the accelerated adaptation observed in the flexible regime.  It decomposes the NTK into contributions from specialized weights (wPwPT) and selective gates (CPCP). Heatmaps illustrate how these contributions vary with different degrees of student specialization, showing that the combination of both factors leads to faster adaptation.", "section": "5.2 Specialization emerges due to self-reinforcing feedback loops"}, {"figure_path": "AbTpJl7vN6/figures/figures_26_1.jpg", "caption": "Figure A.11: Learning occurs within and outside of the specialization subspace. A. First component of the error following a task switch from task A to task B for different values of Tc. B. Second component of the error across the same timeframe. C. Adaptation of weight matrices in the specialization space. D. Orthogonal component of learning that measures adaptation of both teachers for the current task. E. Gate change in the specialization subspace.", "description": "This figure shows the learning dynamics of the model's parameters when a task switch occurs. Panel A and B shows how the error in the specialization subspace changes over time for different values of the gate timescale (Tc). Panel C shows how the weight matrices adapt in the specialization subspace, while panel D shows the orthogonal component of learning. Finally, panel E shows how the gate change varies over time in the specialization subspace. Overall, the figure illustrates that learning happens both inside and outside of the specialization subspace, and that gate timescale plays an important role in the adaptation speed.", "section": "A Additional details on main text"}, {"figure_path": "AbTpJl7vN6/figures/figures_26_2.jpg", "caption": "Figure A.12: Gated model generalizes to compositional tasks. A. Task composition consists of new tasks that sum sets of teachers previously encountered. B. Subtask composition consists of new tasks that concatenate alternating rows of sets of teachers previously encountered. Loss (C.,D.), gating activity (E,F.), and student-teacher alignment (G.,H.) of models on generalization to task composition (top) and subtask composition (bottom).", "description": "This figure shows that a gated neural network model can generalize to new tasks formed by combining previously learned tasks (compositional generalization).  The top row illustrates task composition, where new tasks are created by summing the weights of previously learned tasks. The bottom row shows subtask composition, where new tasks are created by combining rows from previously learned tasks.  The model's performance (loss), gate activation, and student-teacher alignment are shown for both task composition and subtask composition, demonstrating the model's ability to leverage previously learned representations for new tasks.", "section": "A.8 Gated model generalizes to perform task and subtask composition"}, {"figure_path": "AbTpJl7vN6/figures/figures_27_1.jpg", "caption": "Figure A.13: Robustness to relaxing orthogonality between teachers. A. Illustration of changing teacher cosine similarity. B. Adaptation speed as measured by the loss after a block switch (black) and student specialization (gray), both as a function of the teacher similarity. 0 represents the orthogonal case studied in the main text.", "description": "This figure demonstrates the robustness of the model's flexible learning regime to deviations from the assumption of orthogonality between tasks. Panel A illustrates how the cosine similarity between teachers changes as the angle between their vectors varies.  Panel B shows how the adaptation speed (measured by the loss after a task switch) and the degree of student specialization vary as a function of teacher correlation.  It is observed that as teachers become less orthogonal (correlation approaches 1), both the adaptation speed and specialization decrease, indicating a graceful degradation from the idealized orthogonal case.", "section": "A.9 Non-orthogonal teachers"}, {"figure_path": "AbTpJl7vN6/figures/figures_28_1.jpg", "caption": "Figure A.12: Gated model generalizes to compositional tasks. A. Task composition consists of new tasks that sum sets of teachers previously encountered. B. Subtask composition consists of new tasks that concatenate alternating rows of sets of teachers previously encountered. Loss (C.,D.), gating activity (E,F.), and student-teacher alignment (G.,H.) of models on generalization to task composition (top) and subtask composition (bottom).", "description": "This figure demonstrates the model's ability to generalize to compositional tasks.  Panel A shows task composition, where new tasks are created by summing previously learned teacher tasks. Panel B shows subtask composition, where new tasks are created by concatenating rows from different teachers. The figure displays loss, gating activity, and student-teacher alignment to illustrate how the model handles these novel compositions. Results show that the flexible model quickly adapts to compositional tasks, while the forgetful model struggles.", "section": "A.8 Gated model generalizes to perform task and subtask composition"}, {"figure_path": "AbTpJl7vN6/figures/figures_29_1.jpg", "caption": "Figure A.15: NTA quickly adapts across fashionMNIST for (left) an orthogonal sorting based on upper-to-lower items of clothing and (right) a correlated sorting for warm-to-cold weather clothing. The panels show (top) accuracy on the test set and (bottom) activity of the gates. We show mean and standard error with 10 seeds.", "description": "This figure shows the results of applying the Neural Task Abstraction (NTA) model to the FashionMNIST dataset. Two different task orderings were used: one orthogonal (upper-to-lower clothing items) and one non-orthogonal (warm-to-cold weather clothing items). The top panels show the accuracy on the test set for both the flexible and forgetful NTA models over time. The bottom panels show the activity of the gating units over time.  Error bars representing the mean and standard error across 10 different seeds are included. The results demonstrate that the flexible NTA model adapts more quickly to task switches in both scenarios, highlighting its adaptability and robustness across different task structures.", "section": "A.9.1 Experiments on fashionMNIST dataset"}, {"figure_path": "AbTpJl7vN6/figures/figures_29_2.jpg", "caption": "Figure 2: Joint gradient descent on gates and weights enables fast adaptation through gradual specialization. Learning on the blocked curriculum from Fig. 1 with te = 0.03, \u03c4\u03c9 = 1.3, and block length TB = 1.0. x-axis indicates time as multiples of TB. (Black) Flexible NTA model Eq. (1, NTA), (gray) forgetful NTA model with Tc = Tw and Anonneg = Xnorm = 0. Simulation averaged over 10 random seeds with standard error indicated. A. Loss of both models over time. B. Gate activity of flexible NTA. C. Student-teacher weight alignment W*mWpT, normalized and averaged over rows (cosine similarity) for each student-teacher pair. D., E. Norm of updates to WP and c. Dashed: norm of students correlating with update size of c. F. Time to Ltask = 0.1 for both models over blocks.", "description": "This figure compares the performance of two NTA models (flexible and forgetful) during a task-switching experiment. The flexible model uses a faster timescale for gates than weights. The figure displays the loss over time, gate activity, student-teacher weight alignment, norm of updates to weights and gates, and time to reach a specific loss threshold. The flexible model shows faster adaptation and weight specialization compared to the forgetful model.", "section": "4 Task abstractions emerge through joint gradient descent"}, {"figure_path": "AbTpJl7vN6/figures/figures_32_1.jpg", "caption": "Figure B.1: The effect of regularization on gating variables. Regularization encourages competition between gates while preventing degeneracy of solutions. Importantly, regularization does not force gating variables to be specialized, as illustrated by the red \u00d7. This holds for two regularizers we consider, A. Lnorm-L1 and B. Lnorm-L2.", "description": "This figure illustrates how different regularization methods affect the gating variables in the model.  The Lnorm-L1 regularization (left panel) encourages sparsity in the gating variables, while the Lnorm-L2 regularization (right panel) allows for multiple gates to be active. The key point is that neither regularization method *forces* specialization; there are solutions that involve both gates being active at similar levels. However, regularization makes the solutions with one gate significantly dominant much more likely.", "section": "B.3 Regularization"}]