[{"figure_path": "8jB6sGqvgQ/tables/tables_5_1.jpg", "caption": "Table 1: The combined number of forward (F) and backward (B) passes to compute a single adversarial example for different AT types. The total number of F&B for the whole training and the number of training iterations and batch size are are shown. Time is the wallclock time for a single batch weight update (measured on 1 A100 with Mistral).", "description": "This table compares the computational efficiency of three different adversarial training algorithms: R2D2, CAT, and CAPO. It shows the number of forward and backward passes required to compute a single adversarial example, the total number of forward and backward passes for the entire training process, the number of training iterations, batch size, and the time taken for a single batch weight update. The table highlights the significant computational advantage of CAT and CAPO over R2D2, demonstrating that continuous adversarial attacks can be far more efficient than discrete attacks.", "section": "4 Experimental Details"}, {"figure_path": "8jB6sGqvgQ/tables/tables_14_1.jpg", "caption": "Table 2: Hyperparameters for the model trained with CAT", "description": "This table lists the hyperparameter settings used for training the various language models using the Continuous-Adversarial UL (CAT) algorithm.  The hyperparameters cover learning rate, batch size, number of epochs, optimizer, adversarial learning rate, epsilon (attack strength), beta (IPO parameter, only relevant for CAPO), cutoff values for away and toward losses, the utility data ratio, maximum sequence length, and weights for away, toward and utility losses. Quantization level is also included.", "section": "A Hyperparameter choices"}, {"figure_path": "8jB6sGqvgQ/tables/tables_14_2.jpg", "caption": "Table 3: Hyperparameters for the model trained with CAPO", "description": "This table shows the hyperparameter settings used for training models using the CAPO algorithm.  It includes parameters related to learning rate, batch size, number of epochs, optimizer, adversarial learning rate, epsilon (attack strength), beta (IPO parameter), cutoffs for away and toward losses, utility data ratio, maximum sequence length, loss weights, and quantization.", "section": "A.1 Adversarial Training"}, {"figure_path": "8jB6sGqvgQ/tables/tables_14_3.jpg", "caption": "Table 4: Summary of models used in this work.", "description": "This table lists the six large language models (LLMs) used in the paper's experiments.  For each model, it provides the model name, a reference to its source, and a URL where it can be accessed.", "section": "A.2 Models"}, {"figure_path": "8jB6sGqvgQ/tables/tables_15_1.jpg", "caption": "Table 5: All models and utility / robustness before / after our adversarial training.", "description": "This table presents a comprehensive evaluation of different Language Models (LLMs) before and after applying two novel adversarial training algorithms: Continuous-Adversarial UL (CAT) and Continuous-Adversarial IPO (CAPO).  It compares their performance to a baseline model (ZEPHYR + R2D2) using several metrics, including utility benchmarks (MMLU, ARC-E, ARC-C, MT-BENCH, HARMLESS) and robustness against various adversarial attacks (GCG, AutoDAN, PAIR, ICL).  The table shows the trade-off between model utility and robustness to different attack strategies, highlighting the effectiveness of CAT and CAPO in improving model robustness.", "section": "Results"}, {"figure_path": "8jB6sGqvgQ/tables/tables_15_2.jpg", "caption": "Table 6: Attack success rate [%] of the simple adaptive attack proposed by Andriushchenko et al. [2]. A single example (id 7) for Zephyr-C-AdvUL never converged and we show robustness to 39 standard behavior examples of the Harmbench dataset.", "description": "This table presents the attack success rates of the simple adaptive attack proposed by Andriushchenko et al. [2] on several models.  The simple adaptive attack's success rate is measured against 39 standard behavior examples from the Harmbench dataset.  One model (Zephyr-C-AdvUL) failed to converge on a single example (id 7), which is noted. The results show the effectiveness of different adversarial training methods in mitigating the impact of this specific attack.", "section": "Results"}, {"figure_path": "8jB6sGqvgQ/tables/tables_15_3.jpg", "caption": "Table 7: One-step training ablation. Difference to the base model is shown.", "description": "This table shows the results of an ablation study comparing the performance of a one-step adversarial training approach to the multi-step approach.  It indicates the changes in MMLU, ARC-E, ARC-C, and GCG metrics when using one-step adversarial training compared to the baseline model.", "section": "B.1 One-Step Adversarial Training"}, {"figure_path": "8jB6sGqvgQ/tables/tables_16_1.jpg", "caption": "Table 8: No adversarial training ablation. Difference to the base model is shown.", "description": "This table presents the results of an ablation study where the model was trained using IPO and NPO methods without adversarial attacks.  It compares the performance on MMLU, ARC-E, ARC-C, and GCG to the base model, showcasing the impact of removing adversarial training from the training process. The difference from the base model in terms of MMLU score (higher is better), ARC-E score (higher is better), ARC-C score (higher is better), and GCG loss (lower is better) is presented.", "section": "B.2 Training without Attacks"}, {"figure_path": "8jB6sGqvgQ/tables/tables_16_2.jpg", "caption": "Table 8: No adversarial training ablation. Difference to the base model is shown.", "description": "This table presents the results of an ablation study where the models were fine-tuned using IPO and NPO without adversarial training.  The results show the difference in MMLU, ARC-E, ARC-C, and GCG scores compared to the baseline models. It demonstrates that  neither IPO nor NPO without adversarial attacks improve robustness.", "section": "B.2 Training without Attacks"}, {"figure_path": "8jB6sGqvgQ/tables/tables_19_1.jpg", "caption": "Table 10: Number of refusals of MMLU questions when using the chat template.", "description": "This table presents the number of times each model refused to answer a question from the MMLU benchmark when the chat template was enabled.  The models listed include both baseline models and models trained using different adversarial training techniques (UL, IPO).  The results highlight a potential failure mode where models trained for adversarial robustness become overly cautious and refuse to answer even benign questions.", "section": "GMMLU refusal under chat template"}, {"figure_path": "8jB6sGqvgQ/tables/tables_20_1.jpg", "caption": "Table 11: ASR under POLITEHARMBENCH", "description": "This table presents the attack success rate (ASR) for different models on the POLITEHARMBENCH dataset.  The POLITEHARMBENCH dataset is a modified version of the original Harmbench dataset, where harmful prompts are rephrased in a polite manner.  This table shows how the politeness of the prompts affects the model's vulnerability to adversarial attacks.  The models include various versions of GEMMA, PHI-3-MINI, MISTRAL-7B, ZEPHYR-7B, and ZEPHYR + R2D2, both with and without adversarial training (UL and IPO) applied. The results highlight the potential vulnerabilities even when adversarial attacks are expressed politely.", "section": "H POLITEHARMBENCH"}]