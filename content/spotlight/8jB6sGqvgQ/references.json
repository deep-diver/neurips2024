{"references": [{"fullname_first_author": "Andy Zou", "paper_title": "Universal and Transferable Adversarial Attacks on Aligned Language Models", "publication_date": "2023-07-15", "reason": "This paper introduces universal and transferable adversarial attacks against aligned language models, a foundational threat model for the current research."}, {"fullname_first_author": "Maksym Andriushchenko", "paper_title": "Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks", "publication_date": "2024-04-02", "reason": "This paper demonstrates highly effective adaptive attacks against leading safety-aligned LLMs, highlighting the severity of the robustness issue."}, {"fullname_first_author": "Aleksander Madry", "paper_title": "Towards Deep Learning Models Resistant to Adversarial Attacks", "publication_date": "2018-00-00", "reason": "This seminal work provides foundational theory and methods for adversarial training in deep learning, which underpins much of the adversarial training research for LLMs."}, {"fullname_first_author": "Mantas Mazeika", "paper_title": "Harmbench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal", "publication_date": "2024-02-02", "reason": "This paper introduces Harmbench, a standardized benchmark for evaluating the robustness and safety of LLMs against adversarial attacks, which is directly used for evaluation in the current research."}, {"fullname_first_author": "Leo Schwinn", "paper_title": "Adversarial Attacks and Defenses in Large Language Models: Old and New Threats", "publication_date": "2023-10-19", "reason": "This paper explores various adversarial attacks and defenses in large language models, offering a comprehensive overview of the threat landscape and relevant prior art."}]}