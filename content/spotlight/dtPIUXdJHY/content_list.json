[{"type": "text", "text": "Generalization Analysis for Label-Specific Representation Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yi-Fan Zhang1,3, Min-Ling Zhang2,3\u2217 ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 School of Cyber Science and Engineering, Southeast University, Nanjing 210096, China   \n2 School of Computer Science and Engineering, Southeast University, Nanjing 210096, China   \n3 Key Laboratory of Computer Network and Information Integration (Southeast University), Ministry of Education, China {yfzh, zhangml}@seu.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Label-specific representation learning (LSRL), i.e., constructing the representation with specific discriminative properties for each class label, is an effective strategy to improve the performance of multi-label learning. However, the generalization analysis of LSRL is still in its infancy. The existing theory bounds for multilabel learning, which preserve the coupling among different components, are invalid for LSRL. In an attempt to overcome this challenge and make up for the gap in the generalization theory of LSRL, we develop a novel vector-contraction inequality and derive the generalization bound for general function class of LSRL with a weaker dependency on the number of labels than the state of the art. In addition, we derive generalization bounds for typical LSRL methods, and these theoretical results reveal the impact of different label-specific representations on generalization analysis. The mild bounds without strong assumptions explain the good generalization ability of LSRL. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Multi-label learning has received continued attention in machine learning community due to its widespread encounters in real-world applications, where each object is represented by a single instance associated with multiple class labels [Zhang and Zhou, 2014, Liu et al., 2022]. The goal of multi-label learning is to model real-world objects with multiple semantics. It has made important advances in multimedia content annotation [Cabral et al., 2011, You et al., 2020], text categorization [Rubin et al., 2012, Xun et al., 2020], bioinformatics [Cesa-Bianchi et al., 2012, Chen et al., 2017] and other fields [Yu et al., 2005]. The failure to take into account that each class label may possess its own discriminative properties results in the most straightforward strategy which exploits the identical representation of an instance for dealing with multi-label data being perhaps suboptimal [Zhang and Wu, 2015, Huang et al., 2016, Hang and Zhang, 2022]. In recent years, label-specific representation learning (LSRL) [Zhang and Wu, 2015] has been proposed to facilitate the discrimination of each class label by tailoring its own representations. Due to its ability to model distinct characteristics for each class label, LSRL has become an effective strategy to improve the performance of multi-label learning [Huang et al., 2015, 2016, 2018, Yu and Zhang, 2022, Hang and Zhang, 2022, Hang et al., 2022]. Although LSRL has achieved impressive empirical advances in multi-label learning, the problem of understanding LSRL theoretically remains completely under-explored. ", "page_idx": 0}, {"type": "text", "text": "In recent years, efforts to explain why multi-label models generalize well is an important open problem in multi-label learning community. The empirical success of LSRL makes the generalization analysis of LSRL an important problem in multi-label learning. However, existing theoretical results and analysis methods are not applicable to LSRL, which leads to a serious lack of progress in its generalization analysis. A satisfactory and complete study of the generalization analysis for LSRL should include two aspects: 1) the dependency of the generalization bounds on the number of labels (i.e., $c)$ should be weaker than square-root, and 2) the relationship among components should be decoupled in the generalizability analysis. First, existing bounds with a linear or square-root dependency on $c$ are difficult to explain empirical success of multi-label learning. For example, the bounds with a linear dependency on $c$ are vacuous (i.e., no longer less than 1) for commonly used multi-label datasets such a\u221as CAL500, core $15\\mathbf{k}$ , rcv1-s1, Corel16k-s1, delicious, iaprtc12, espgame, etc., since $c$ is larger than $\\sqrt{n}$ (the number of examples) for these datasets. The bounds with a linear or square-root dependency on $c$ are vacuous for commonly used extreme multi-label datasets such as Wiki 10, Amazon-670K, etc., since $c$ is larger than $n$ . The above failure of existing bounds for multilabel learning also applies to LSRL. Hence, this suggests that only the bounds with weaker dependency on $c$ can provide effective theoretical guarantees. Second, existing theoretical results preserve the coupling among different components [Lei et al., 2015, Wu and Zhu, 2020, Wu et al., 2021a,b]. However, LSRL decomposes the multi-label learning problem into multiple binary classification problems, which means that the relationship among different components needs to be decoupled in the generalization analysis. Hence, we need to develop new analysis methods for LSRL. As a matter of fact, theoretical research on LSRL can also promote a better understanding of multi-label learning. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this paper, we derive novel and tighter bounds based on the Rademacher complexity for LSRL. Specifically, we develop a novel vector-contraction inequality with the assumption that the loss function is Lipschitz continuous w.r.t. the $\\ell_{\\infty}$ norm, then we derive the bound for general function classes of LSRL with no dependency on $c$ , up to logarithmic terms, which is tighter than the state of the art. In addition, we also analyze the bounds for several typical LSRL methods, and we show that the construction method of label-specific representations will affect the generalization bound. ", "page_idx": 1}, {"type": "text", "text": "Our bounds for LSRL improve the dependency on $c$ . Major contributions of the paper include: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We develop a novel vector-contraction inequality for $\\ell_{\\infty}$ norm Lipschitz continuous loss, which overcomes the limitations of existing theoretical results and provides a theoretical tool for the generalization analysis of LSRL. \u2022 We derive bounds for general function classes of LSRL with a weaker dependency on $c$ than the state of the art, which provides a general theoretical guarantee for LSRL. \u2022 We derive bounds for typical LSRL methods, which reveals the impact of different labelspecific representations on the generalization analysis. The theoretical techniques and results on $k$ -means clustering, Lasso, and DNNs involved here may be of independent interest. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Multi-Label Learning ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Multi-label learning is one of the most studied and important machine learning paradigms in practice [Zhang and Zhou, 2014, Liu et al., 2022]. To cope with the challenge that the output space is exponential in size to the number of class labels, modeling label correlations is adopted as a feasible strategy to facilitate the learning process. Generally speaking, existing methods can be roughly grouped into three major categories based on the order of label correlations being considered, namely first-order methods [Boutell et al., 2004, Zhang and Zhou, 2007, Zhang et al., 2018] which tackle multi-label learning problem by decomposing it into a number of independent binary classification problems, second-order methods [Elisseeff and Weston, 2001, Zhu et al., 2018, Sun and Zhang, 2021] which tackle multi-label learning problem by considering pairwise relationships between labels, and high-order methods [Ji et al., 2010, Xu and Guo, 2021] which tackle multi-label learning problem by exploiting high-order relationships among labels. Recent years, beneftiing from the good generalization performance of deep learning, deep methods such as recurrent neural networks [Yazici et al., 2020], graph neural networks[Chen et al., 2020], and embedding models [Bai et al., 2020, Dahiya et al., 2021] have been explored to model label correlations. ", "page_idx": 1}, {"type": "text", "text": "As a complement to the exploitation of label correlations, label-specific representation learning (LSRL) have been proven to be another effective strategy to improve multi-label learning by manipulating the input space. Existing methods can be roughly grouped into three major categories based on the construction method of label-specific representations, i.e., prototype-based label-specific representation transformation methods [Zhang and Wu, 2015, Zhang et al., 2015, Weng et al., 2018, Guo et al., 2019, Zhang and Li, 2021] which generate label-specific representations by treating the prototypes of each class label as the transformation bases, label-specific feature selection methods [Huang et al., 2015, 2016, 2018, Weng et al., 2020, Yu and Zhang, 2022] which generate label-specific representations by retaining a feature subset as the most pertinent features for each class label, and deep label-specific representation methods [Hang and Zhang, 2022, Hang et al., 2022] which learn label-specific representations in an end-to-end manner by exploiting deep models. In particular, $k$ -means clustering-based LSRL, i.e., LIFT [Zhang and Wu, 2015], is the representative method of prototype-based label-specific representation transformation methods, which constructs label-specific representations by querying the distances between the original inputs and the cluster centers for each class label. Lasso-based LSRL, i.e., LLSF [Huang et al., 2016], is the representative method of label-specific feature selection methods, which presents a Lasso-based framework with the constraint of pairwise label correlations for each class label. DNN-based LSRL, i.e., CLIF [Hang and Zhang, 2022], is the representative method of deep label-specific representation methods, which proposes to learn label semantics and label-specific representations in a collaborative way. In this paper, we will analyze the generalization bounds of these three representative LSRL methods. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2.2 Generalization Bounds for Multi-Label Learning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Dembczynski et al. [2010] derived the relationship between the expectations of Hamming and Subset loss based on the regret analysis on Hamming and Subset loss, and Dembczynski et al. [2012] further performed regret analysis on Ranking loss, which provided preliminary theoretical insights for understanding Hamming, Subset and Ranking loss. With the typical vector-contraction inequality [Maurer, 2016] (i.e., assume that the loss functio\u221an is Lipschitz continuous w.r.t. the $\\ell_{2}$ norm), one can obtain generalization bounds of order $O(c/{\\sqrt{n}})$ for multi-label learning. Wu and Zhu [2020], Wu et al. [2021a] showed that the order of the generalization bounds for Subset loss, Hamming Loss and reweighted convex surrogate univariate loss can be improved to $O({\\sqrt{c/n}})$ , which preserved the coupling among different components and exploited the relationship between loss functions. Liu et al. [2018] also obtained a generalization bound of order $O({\\sqrt{c/n}})$ for the dual set multi-label learning, which was analyzed under the margin loss and kernel function classes. ", "page_idx": 2}, {"type": "text", "text": "Wu et al. [2021b] derived a generalization bound of order $O(\\log(n c)/n\\sigma)$ for norm regularized kernel function classes with the assumptions that the loss function is Lipschitz continuous w.r.t. the $\\ell_{\\infty}$ norm and the regularizer is $\\sigma$ -strongly c\u221aonvex with respect to some norms. Yu et al. [2014] obtained a generalization bound of order $O(1/{\\sqrt{n}})$ for trace norm regularized linear function classes with Decomposable loss. $\\mathrm{Xu}$ et al. [2016] used the local Rademacher complexity to derive a generalization bound of order ${\\widetilde O}(1/n)$ for trace norm regularized linear function classes with the assumption that the singular values of the weight matrix decay exponentially. These theoretical results al\u221al preserved the coupling among different components. In addition, Wu et al. [2023] obtained $O(1/{\\sqrt{n}})$ bounds for Macro-Averaged AUC and gave thorough discussions about its relationships with the label-wise class imbalance, which transformed the macro-averaged maximization problem in multi-label learning into the problem of learning multiple tasks with graph-dependent examples. Here we obtain $\\widetilde{O}(1/\\sqrt{n})$ bounds with the state-of-the-art dependency on the number of labels for LSRL function cla sses under the assumption that the loss function is Lipschitz continuous w.r.t. the $\\ell_{\\infty}$ norm. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Let $[n]:=\\{1,\\ldots,n\\}$ for any natural number $n$ . In the context of multi-label learning, given a dataset $D=\\{(\\pmb{x}_{1},\\pmb{y}_{1})\\,,\\dots,(\\pmb{x}_{n},\\pmb{y}_{n})\\}$ with $n$ examples which are identically and independently distributed (i.i.d.) from a probability distribution $P$ on $\\mathcal X\\times\\mathcal Y$ , where $\\mathcal{X}\\subset\\mathbb{R}^{d}$ denotes the $d$ -dimensional input space and $\\boldsymbol{\\wp}$ denotes the label space with $c$ class labels, $\\pmb{x}\\in\\mathcal{X}$ , $\\pmb{{y}}\\in\\mathcal{V}\\subseteq\\{-1,+1\\}^{c}$ , i.e., each $\\pmb{y}\\,=\\,(y_{1},...\\,,y_{c})$ is a binary vector and $y_{j}\\;=\\;1\\;(y_{j}\\;=\\;-1)$ denotes that the $j$ -th label is relevant (irrelevant), $j\\,\\in\\,[c]$ . The task of multi-label learning is to learn a multi-label classifier $\\pmb{h}\\in\\mathcal{H}:\\mathcal{X}\\mapsto\\{-1,+1\\}^{\\bar{c}}$ which assigns each instance with a set of relevant labels. A common strategy is to learn a vector-valued function ${\\pmb f}=(f_{1},\\dots,f_{c}):\\mathcal{X}\\mapsto\\mathbb{R}^{c}$ and derive the classifier by a thresholding function which divides the label space into relevant and irrelevant label sets. ", "page_idx": 2}, {"type": "text", "text": "For any vector-valued function $\\pmb{f}:\\mathcal{X}\\mapsto\\mathbb{R}^{c}$ , the prediction quality on the example $\\left({\\pmb x},{\\pmb y}\\right)$ is measured by a loss function $\\ell:\\mathbb{R}^{c}\\times\\{-1,+1\\}^{c}\\mapsto\\mathbb{R}_{+}$ . The goal of learning is to find a hypothesis $\\boldsymbol{f}\\in\\mathcal{F}$ with good generalization performance from the dataset $D$ by optimizing the loss $\\ell$ . The generalization performance is measured by the expected risk: $R(\\pmb{f})\\,=\\,\\mathbb{E}_{(\\pmb{x},\\pmb{y})\\sim P}[\\ell(\\pmb{f}(\\pmb{x}),\\pmb{y})]$ . We denote the empirical risk w.r.t. the training dataset $D$ as $\\begin{array}{r}{\\widehat{R}_{D}(\\pmb{f})=\\frac{1}{n}\\sum_{i=1}^{n}\\ell(\\pmb{f}(\\pmb{x}_{i}),\\pmb{y}_{i})}\\end{array}$ . We denote the optimal risk as $R^{*}=\\operatorname*{inf}_{\\pmb{f}\\in\\mathcal{F}}R(\\pmb{f})$ , the minimizer of the optimal risk as $\\pmb{f}^{*}=\\arg\\operatorname*{min}_{\\pmb{f}\\in\\mathcal{F}}R(\\pmb{f})$ and denote the minimizer of the empirical risk as $\\begin{array}{r}{\\hat{\\pmb f}^{*}=\\arg\\operatorname*{min}_{\\pmb f\\in\\mathcal{F}}\\widehat{R}_{D}(\\pmb f)}\\end{array}$ . In addition, we define the loss function space as $\\mathcal{L}=\\{\\ell(\\pmb{f}(\\pmb{x}),\\pmb{y}):\\pmb{f}\\in\\mathcal{F}\\}$ , where $\\mathcal{F}$ is t he vector-valued function class. ", "page_idx": 3}, {"type": "text", "text": "3.1 Label-Specific Representation Learning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Label-specific representation learning aims to construct the representation with specific discriminative properties for each class label to facilitate its discrimination process. The basic idea of LSRL is to decompose the multi-label learning problem into $c$ binary classification problems, i.e., decoupling the relationship among different components, where each binary classification problem corresponds to a possible label in the label space. We consider the prediction function for each label of the general form $f_{j}(\\pmb{x})=\\langle\\pmb{w}_{j},\\zeta_{j}(\\phi_{j}(\\pmb{\\bar{x}}))\\rangle$ , where the inner nonlinear mapping $\\phi_{j}$ corresponds to the nonlinear transformation induced by the construction method of label-specific representation, while the outer nonlinear mapping $\\zeta_{j}$ refers to the nonlinear mapping corresponding to the classifier learned on the generated label-specific representation. We define a vector-valued function class of LSRL as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathscr F}=\\{x\\mapsto f(x):f(x)=(f_{1}(x),\\ldots,f_{c}(x)),}\\\\ &{\\qquad\\qquad\\qquad\\qquad f_{j}(x)=h_{j}\\,(\\phi_{j}(x))=w_{j}^{\\top}\\zeta_{j}(\\phi_{j}(x)),w=(w_{1},\\ldots,w_{c})\\in{\\mathbb R}^{d\\times c},}\\\\ &{\\qquad\\qquad\\qquad\\alpha(w)\\leq\\Lambda,\\beta(\\zeta_{j}(\\cdot))\\leq A,\\gamma(\\phi_{j}(\\cdot))\\leq C,x\\in{\\mathscr X},j\\in[c],\\Lambda,A,C>0\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\alpha$ represents a functional that constrains weights, $\\beta$ represents a functional that constrains nonlinear mappings $\\zeta_{j},\\gamma$ represents a functional that constrains nonlinear mappings $\\phi_{j}$ . ", "page_idx": 3}, {"type": "text", "text": "3.2 Related Evaluation Metrics ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "A number of evaluation metrics are proposed to measure the generalization performance of different multi-label learning methods. Here we focus on commonly used evaluation metrics, i.e., Hamming Loss, Subset Loss, Ranking Loss and Decomposable Loss. However, the above mentioned loss functions are typically $0/1$ losses, which are actually hard to handle in optimization. Hence, one usually consider their surrogate losses, which are defined as follows: ", "page_idx": 3}, {"type": "text", "text": "Hamming Loss ", "text_level": 1, "page_idx": 3}, {"type": "equation", "text": "$$\n\\ell_{H}(\\pmb{f}(\\pmb{x}),\\pmb{y})=\\frac{1}{c}\\sum_{j=1}^{c}\\ell_{b}\\left(y_{j}f_{j}(\\pmb{x})\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the base convex surrogate loss $\\ell_{b}$ can be various popular forms, such as the hinge loss, the logistic loss, the exponential loss and the squared loss. ", "page_idx": 3}, {"type": "text", "text": "Subset Loss : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\ell_{S}(\\pmb{f}(\\pmb{x}),\\pmb{y})=\\operatorname*{max}_{j\\in[c]}\\left\\{\\ell_{b}\\left(y_{j}f_{j}(\\pmb{x})\\right)\\right\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Ranking Loss : $\\ell_{R}(\\pmb{f}(\\pmb{x}),\\pmb{y})=\\frac{1}{|\\pmb{Y}^{+}|\\,|\\pmb{Y}^{-}|}\\sum_{p\\in Y^{+}}\\sum_{q\\in Y^{-}}\\ell_{b}\\left(f_{p}(\\pmb{x})-f_{q}(\\pmb{x})\\right),$ ", "page_idx": 3}, {"type": "text", "text": "where $Y^{+}\\,(Y^{-})$ denotes the relevant (irrelevant) label index set induced by $\\textit{\\textbf{y}}$ , and $\\big|\\cdot\\big|$ denotes the cardinality of a set. ", "page_idx": 3}, {"type": "text", "text": "Decomposable Loss : $\\ell_{D}(\\pmb{f}(\\pmb{x}),\\pmb{y})=\\sum_{j=1}^{c}\\ell_{b}\\left(y_{j}f_{j}(\\pmb{x})\\right).$ ", "page_idx": 3}, {"type": "text", "text": "3.3 Related Complexity Measures ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Here we introduce the complexity measures involved in theoretical analysis. The Rademacher complexity is used to perform generalization analysis for LSRL. ", "page_idx": 3}, {"type": "text", "text": "Definition 1 (Rademacher complexity). Let $\\mathcal{G}$ be a class of real-valued functions mapping from $\\mathcal{X}$ to $\\mathbb{R}$ . Let $D=\\{\\pmb{x}_{1},\\pmb{\\dots},\\pmb{x}_{n}\\}$ be a set with $n$ i.i.d. samples. The empirical Rademacher complexity over $\\mathcal{G}$ is defined by $\\hat{\\mathfrak{R}}_{D}(\\mathcal{G})=\\mathbb{E}_{\\epsilon}$ $\\begin{array}{r}{\\left[\\operatorname*{sup}_{g\\in\\mathcal{G}}\\frac{1}{n}\\sum_{i=1}^{n}\\epsilon_{i}g\\left(\\pmb{x}_{i}\\right)\\right]}\\end{array}$ , where $\\epsilon_{1},\\ldots,\\epsilon_{n}$ are i.i.d. Rademacher random variables, and we refer to the expectation $\\Re(\\mathcal{G})=\\mathbb{E}_{D}[\\hat{\\Re}_{D}(\\mathcal{G})]$ as the Rademacher complexity of $\\mathcal{G}$ . In addition, we define the worst-case Rademacher complexity as $\\tilde{\\mathfrak{R}}_{n}(\\mathcal{G})=\\operatorname*{sup}_{D\\in\\mathcal{X}^{n}}\\hat{\\mathfrak{R}}_{D}(\\mathcal{G})$ , and its expectation is denoted as $\\tilde{\\mathfrak{R}}(\\mathcal{G})=\\mathbb{E}_{D}[\\tilde{\\mathfrak{R}}_{n}(\\mathcal{G})]$ . ", "page_idx": 4}, {"type": "text", "text": "In multi-label learning, $\\textbf{\\em f}\\in\\mathcal{F}$ is a vector-valued function, which makes traditional Rademacher complexity analysis methods invalid. Hence, we need to convert the Rademacher complexity of a loss function space associated with the vector-valued function class $\\mathcal{F}$ into the Rademacher complexity of a tractable scalar-valued function class. The Rademacher complexity can be bounded by other scale-sensitive complexity measures, such as the covering number and fat-shattering dimension [Srebro et al., 2010, Zhang and Zhang, 2023]. The relevant definitions are provided in the appendix. ", "page_idx": 4}, {"type": "text", "text": "4 General Bounds for LSRL ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we first introduce the assumptions used. Then, we develop a novel vector-contraction inequality for the Rademacher complexity of the loss function space associated with the vector-valued function class $\\mathcal{F}$ . Finally, with the novel vector-contraction inequality, we derive the generalization bound for general function classes of LSRL with no dependency on the number of labels, up to logarithmic terms, which is tighter than the state of the art. The detailed proofs of the theoretical results in this paper are provided in the appendix. ", "page_idx": 4}, {"type": "text", "text": "Assumption 1. Assume that the input features, the loss function and the components of the vectorvalued function are bounded: $\\|\\pmb{x}_{i}\\|_{2}\\leq R,$ , $\\ell(\\cdot,\\cdot)\\leq M$ , $|\\dot{f}_{j}(\\cdot)|\\le B$ for $i\\in[n]$ , $j\\in[c]$ where $R>0$ , $M>0$ and $B>0$ are constants. ", "page_idx": 4}, {"type": "text", "text": "Assumption 2. Assume that the loss function $\\ell$ is $\\rho$ -Lipschitz continuous w.r.t. the $\\ell_{\\infty}$ norm, that is: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~\\left|\\ell({\\pmb f}({\\pmb x}),\\cdot)-\\ell({\\pmb f}^{\\prime}({\\pmb x}),\\cdot)\\right|\\leq\\rho\\left\\|{\\pmb f}({\\pmb x})-{\\pmb f}^{\\prime}({\\pmb x})\\right\\|_{\\infty},}\\\\ &{\\operatorname*{max}_{j\\in[c]}\\left|t_{j}\\right|f o r\\,{\\pmb t}=(t_{1},\\ldots,t_{c}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\rho>0$ ", "page_idx": 4}, {"type": "text", "text": "Assumption 1 and 2 are mild assumptions. For Assumption 1, normalization of input features is a common data preprocessing operation. When we consider the function class (1), we often use the assumptions $\\|\\bar{\\pmb{w}}_{j}\\|_{2}\\leq\\Lambda$ , $\\|\\bar{\\zeta}_{j}(\\bar{\\cdot})\\|_{2}\\leq A$ for any $j\\in[c]$ to replace the boundedness of the components of the vector-valued function, i.e., $B:=\\Lambda A$ . For Assumption 2, the Lipschitz continuity w.r.t. the $\\ell_{\\infty}$ norm has been considered in some literature [Foster and Rakhlin, 2019, Lei et al., 2019, Wu et al., 2021b]. The following Proposition 1 further illustrates that the commonly used loss functions in multi-label learning actually satisfy Assumption 2. ", "page_idx": 4}, {"type": "text", "text": "Proposition 1. Assume that the base loss $\\ell_{b}$ defined in Subsection 3.2 is $\\mu$ -Lipschitz continuous, then the surrogate Hamming Loss is $\\mu$ -Lipschitz w.r.t. the $\\ell_{\\infty}$ norm, the surrogate Subset Loss is $\\mu$ -Lipschitz w.r.t. the $\\ell_{\\infty}$ norm, the surrogate Ranking Loss is $2\\mu$ -Lipschitz w.r.t. the $\\ell_{\\infty}$ norm, and the surrogate Decomposable Loss is c\u00b5-Lipschitz w.r.t. the $\\ell_{\\infty}$ norm. ", "page_idx": 4}, {"type": "text", "text": "We define a function class $\\mathcal{P}$ consisting of projection operators $p_{j}\\;:\\;\\mathbb{R}^{c}\\;\\mapsto\\;\\mathbb{R}$ for any $j~\\in~[c]$ which project the $c$ -dimensional vector onto the $j$ -th coordinate. Then, we have ${\\mathcal{P}}({\\mathcal{F}})\\ =$ $\\{(j,\\pmb{x})\\stackrel{*}{\\mapsto}\\dot{p_{j}}(\\pmb{f}(\\pmb{x})):p_{j}(\\pmb{f}(\\pmb{x}))=f_{j}(\\pmb{x}),\\pmb{f}\\in\\mathcal{F},(j,\\stackrel{*}{\\pmb{x}})\\in[c]\\times\\mathcal{X}\\}.$ . The projection function class decouples the relationship among different components. With the assumption of $\\ell_{\\infty}$ norm Lipschitz loss and the above definitions, we show that the Rademacher complexity of the loss function space associated with $\\mathcal{F}$ can be bounded by the worst-case Rademacher complexity of the projection function class ${\\mathcal{P}}({\\mathcal{F}})$ . We develop the following novel vector-contraction inequality: ", "page_idx": 4}, {"type": "text", "text": "Lemma 1. Let $\\mathcal{F}$ be a vector-valued function class of LSRL defined by $(I)$ . Let Assumptions $^{\\,I}$ and 2 hold. Given a dataset $D$ of size $n$ . Then, we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{\\Re}_{D}(\\mathcal{L})\\leq12\\sqrt{2}\\rho\\sqrt{c}\\widetilde{\\Re}_{n c}(\\mathcal{P}(\\mathcal{F}))\\left(1+\\log^{\\frac{1}{2}}(8e^{2}n^{3}c^{3})\\cdot\\log\\frac{M\\sqrt{n}}{\\rho B}\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\begin{array}{r}{\\hat{\\mathfrak{R}}_{D}(\\mathcal{L})=\\mathbb{E}_{\\epsilon}\\left[\\operatorname*{sup}_{\\ell\\in\\mathcal{L},f\\in\\mathcal{F}}\\frac{1}{n}\\sum_{i=1}^{n}\\epsilon_{i}\\ell\\left(f(\\pmb{x}_{i})\\right)\\right]}\\end{array}$ is the empirical Rademacher complexity of the loss function space associated with $\\mathcal{F}$ , and $\\widetilde{\\mathfrak{R}}_{n c}(\\mathcal{P}(\\mathcal{F}))$ is the worst-case Rademacher complexity of the projection function class. ", "page_idx": 4}, {"type": "text", "text": "Proof Sketch. First, the Rademacher complexity of the loss function space associated with $\\mathcal{F}$ can be bounded by the empirical $\\ell_{\\infty}$ norm covering number with the refined Dudley\u2019s entropy integral inequality. Second, according to the Lipschitz continuity w.r.t the $\\ell_{\\infty}$ norm, the empirical $\\ell_{\\infty}$ norm covering number of $\\mathcal{F}$ can be bounded by that of ${\\mathcal{P}}({\\mathcal{F}})$ . Third, the empirical $\\ell_{\\infty}$ norm covering number of ${\\mathcal{P}}({\\mathcal{F}})$ can be bounded by the fat-shattering dimension, and the fat-shattering dimension can be bounded by the worst-case Rademacher complexity of $\\mathcal{P}(\\mathcal{F})$ . Hence, the problem is transferred to the estimation of the worst-case Rademacher complexity. Finally, we estimate the lower bound of the worst-case Rademacher complexity of $\\mathcal{P}(\\mathcal{F})$ , and then combined with the above steps, the Rademacher complexity of the loss function space associated with $\\mathcal{F}$ can be bounded. \u53e3 ", "page_idx": 5}, {"type": "text", "text": "With the vector-contraction inequality above, we can derive the following tight bound for LSRL: ", "page_idx": 5}, {"type": "text", "text": "Theorem 1. Let $\\mathcal{F}$ be a vector-valued function class of LSRL defined by $(I)$ . Let Assumptions $^{\\,I}$ and 2 hold. Given a dataset $D$ of size $n$ . Then, for any $0<\\delta<1$ , with probability at least $1-\\delta$ , the following holds for any $\\pmb{f}\\in\\mathcal{F}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\nR(f)\\leq\\widehat{R}_{D}(f)+\\frac{24\\sqrt{2}\\rho\\Lambda A\\left(1+\\log^{\\frac{1}{2}}(8e^{2}n^{3}c^{3})\\cdot\\log\\frac{M\\sqrt{n}}{\\rho B}\\right)}{\\sqrt{n}}+3M\\sqrt{\\frac{\\log\\frac{2}{\\delta}}{2n}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Proof Sketch. We first upper bound the worst-case Rademacher complexity $\\tilde{\\mathfrak{R}}_{n c}(\\mathcal{P}(\\mathcal{F}))$ , and then combined with Lemma 1, the desired bound can be derived. \u53e3 ", "page_idx": 5}, {"type": "text", "text": "Remark 1. Although Lemma 1 shows a factor of ${\\sqrt{c}},$ , the term $\\widetilde{\\mathfrak{R}}_{n c}(\\mathcal{P}(\\mathcal{F}))\\,\\le\\,\\Lambda A/\\sqrt{n c}$ , which makes the Rademacher complexity of the loss function space associated with $\\mathcal{F}$ (i.e., $\\hat{\\mathfrak{R}}_{D}(\\mathcal{L}))$ actually independent on c, up to logarithmic terms, and results in a tighter bound than the existing $O(c/{\\sqrt{n}})$ and $O({\\sqrt{c/n}})$ bounds with a faster convergence rate $\\widetilde{O}(1/\\sqrt{n})$ . The bound in Theorem 1 with no dependency on c can provide a general theoretical guarantee for LSRL, even for extreme multi-label learning where the number of labels far exceeds the number of examples $I Y\\!u$ et al., 2014, Prabhu and Varma, $20I4$ , Yen et al., 2016, Liu and Shen, 2019], since it is easy to get that $\\log c$ is much smaller than $\\sqrt{n}$ . The main challenge of generalization analysis for LSRL is that existing theoretical results and existing generalization analysis methods for multi-label learning are not applicable to LSRL. Specifically, existing theoretical bounds often involve the typical vectorc\u221aontraction inequality [Maurer, 2016] for $\\ell_{2}$ Lipschitz loss: $\\mathbb{E}_{\\epsilon}$ $\\begin{array}{r l}&{\\left[\\operatorname*{sup}_{\\pmb{f}\\in\\mathcal{F}}\\frac{1}{n}\\sum_{i=1}^{n}\\dot{\\epsilon_{i}}\\dot{\\ell}(\\pmb{f}(\\pmb{x}_{i}))\\right]\\leq}\\end{array}$ $\\begin{array}{r}{\\sqrt{2}\\mu\\mathbb{E}_{\\epsilon}\\left[\\operatorname*{sup}_{f\\in\\mathcal{F}}\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{j=1}^{c}\\epsilon_{i j}f_{j}\\left(\\pmb{x}_{i}\\right)\\right]}\\end{array}$ , which will lead to bounds with a linear dependency on c for multi-label learning. Lei et al. $I\\bar{2}O I{\\bar{S}}J,$ Wu and Zhu [2020], Wu et al. [2021a] improve the dependency of the bounds on c to square-root, which preserve the coupling among different components reflected by the constraint $\\|w\\|\\leq\\Lambda.$ . Lei et al. [2019] also improves the bounds of multi-class classification to be independent on $c$ (up to logarithmic terms) for $\\ell_{\\infty}$ Lipschitz loss by preserving the coupling among different components, and Wu et al. [2021b] further generalizes these results to multi-label learning. However, LSRL decomposes the multi-label learning problem into c binary classification problems, which means that the relationship among different components needs to be decoupled in the generalization analysis. Hence, how to develop novel vector-contraction inequalities that can induce $\\widetilde{O}(1/\\sqrt{n})$ bounds and deal with the case of decoupling the relationship among different components are the two most critical difficulties in deriving tighter bounds for LSRL. The introduction of the projection function class plays an important \u221arole in solving these two difficulties. It improves the vector-contraction inequalities by a factor of $\\sqrt{c}$ and decouples the relationship among different components (which is also reflected by the constraint $\\|\\pmb{w}_{j}\\|\\leq\\Lambda$ for any $j\\in[c])$ . Our tighter $\\widetilde{O}(1/\\sqrt{n})$ bound in Theorem 1 with no dependency on c (up to logarithmic terms) solve the limitations of existing theoretical results for multi-label learning and can provide $a$ general theoretical guarantee for LSRL. ", "page_idx": 5}, {"type": "text", "text": "The differences in generalization bounds of different LSRL methods are mainly reflected in two aspects. On the one hand, the Lipschitz constant of the loss functions, as we proved in Proposition 1, the Lipschitz constants $\\rho$ corresponding to different loss functions are various. On the other hand, the nonlinear mappings induced by different LSRL methods. In fact, when we analyze the generalization for LSRL, we will further have $\\|\\zeta(\\cdot)\\|\\,\\leq\\,A\\,:=\\,\\kappa R$ ( $\\kappa$ is the Lipschitz constant of the nonlinear mappings $\\zeta(\\cdot))$ to take into account the differences or characteristics of different LSRL methods. We provide detailed analysis for $A$ of typical LSRL methods in the next section. ", "page_idx": 5}, {"type": "text", "text": "5 Generalization Bounds for Typical LSRL Methods ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we analyze the generalization bounds for several typical LSRL methods, i.e., $k$ -means clustering-based [Zhang and Wu, 2015], Lasso-based [Huang et al., 2016] and DNN-based [Hang and Zhang, 2022] LSRL methods. We show that different construction methods of label-specific representation will lead to significant differences in the constant $A$ of the generalization bound in Theorem 1. For each LSRL method, we first give a brief introduction, then give its formal definition corresponding to the class of LSRL defined in (1), and finally derive the generalization bound. ", "page_idx": 6}, {"type": "text", "text": "5.1 Generalization Bounds for $k$ -Means Clustering-Based LSRL Method ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "As a seminal work, $k$ -means clustering-based LSRL method, i.e., LIFT [Zhang and Wu, 2015], uses $k$ -means clustering to construct label-specific representation that effectively capture the specific characteristics of each label. Specifically, first, for each label, $k$ -means clustering is used to divide the training instances into $K$ clusters, and the centers of the $K$ clusters are obtained, which is denoted as $c_{k}^{j}$ for the $j$ -th label, $k\\,\\in\\,[K],\\,j\\,\\in\\,[c]$ . Then, these $K$ centers are used to construct the label-specific representation, i.e., in the vector-valued function class of LSRL defined by (1), $\\phi_{j}(\\pmb{x})=\\bar{\\Big[d(\\pmb{x},\\pmb{c}_{1}^{j}),\\bar{\\cdot}\\cdot\\cdot,d(\\pmb{x},\\pmb{c}_{K}^{j})\\Big]}$ , $d(\\pmb{x},\\pmb{c}_{k}^{j})=\\|\\pmb{x}-\\pmb{c}_{k}^{j}\\|$ . Finally, a family of $c$ classifiers $f_{j}$ with $\\kappa$ -Lipschitz nonlinear mapping are induced based on the generated label-specific representations. ", "page_idx": 6}, {"type": "text", "text": "Next, we formally define the process of $k$ -means clustering. Here we follow some of the settings and definitions in [Li and Liu, 2021]. Assume that $V:\\mathcal{X}^{2}\\overset{=}{\\rightarrow}\\mathbb{R}_{+}$ is a pairwise distance-based function used to measure the dissimilarity between pair observations, and $Z=[Z_{1},\\dots,Z_{K}]$ is a collection of $K$ partition functions $Z_{k}:{\\dot{\\mathcal{X}}}^{2}\\rightarrow\\mathbb{R}_{+}{\\mathrm{for}}\\;k\\in[K]$ . The clustering framework can be cast as the problem of minimizing the following criterion: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\widehat{R}_{D}(V,Z)=\\frac{1}{n(n-1)}\\sum_{\\substack{i,j=1,i\\neq j}}^{n}\\sum_{k=1}^{K}V\\left(x_{i},x_{j}\\right)Z_{k}\\left(x_{i},x_{j}\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "over all possible functions $V$ and $Z_{k}$ for $k\\,\\in\\,[K]$ . In $k$ -means clustering, we have $V\\left(\\pmb{x}_{i},\\pmb{x}_{j}\\right)=$ $\\lVert{\\boldsymbol{x}}_{i}-{\\boldsymbol{x}}_{j}\\rVert_{2}^{2}$ , and $Z_{k}\\left(\\pmb{x}_{i},\\pmb{x}_{j}\\right)=\\mathbb{I}\\left\\{\\left(\\pmb{x}_{i},\\pmb{x}_{j}\\right)\\in C_{k}^{2}\\right\\}$ , $\\begin{array}{r}{\\pmb{c}_{k}\\,=\\,\\frac{1}{|C_{k}|}\\sum_{\\pmb{x}\\in C_{k}}\\pmb{x}}\\end{array}$ , where $C_{1},\\L\\dots,C_{K}$ are the partitions of the feature space $\\mathcal{X}$ . Let $g_{k}\\left(\\pmb{x}_{i},\\pmb{x}_{j}\\right)=V\\left(\\pmb{x}_{i},\\pmb{x}_{j}\\right)Z_{k}\\left(\\pmb{x}_{i},\\pmb{x}_{j}\\right)$ and $\\pmb{g}=(g_{1},\\dots,g_{K})$ be a vector-valued function, $\\begin{array}{r}{\\ell_{c l u}(g(\\pmb{x},\\pmb{x}^{\\prime}))=\\sum_{k=1}^{K}g_{k}\\left(\\pmb{x},\\pmb{x}^{\\prime}\\right)}\\end{array}$ , then $\\widehat{R}_{D}(V,Z)$ can be written as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\widehat{R}_{D}(V,Z)=\\frac{1}{n(n-1)}\\sum_{\\substack{i,j=1,\\,i\\neq j}}^{n}\\ell_{c l u}(g(\\pmb{x}_{i},\\pmb{x}_{j}))=\\frac{1}{n(n-1)}\\sum_{\\substack{i,j=1,\\,i\\neq j}}^{n}\\sum_{k=1}^{K}g_{k}\\left(\\pmb{x}_{i},\\pmb{x}_{j}\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We then define a vector-valued function class of $k$ -means clustering as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\mathcal{G}=\\{(\\pmb{x},\\pmb{x}^{\\prime})\\mapsto g(\\pmb{x},\\pmb{x}^{\\prime}):g(\\pmb{x},\\pmb{x}^{\\prime})=(g_{1}(\\pmb{x},\\pmb{x}^{\\prime}),\\dots,g_{K}(\\pmb{x},\\pmb{x}^{\\prime})),}\\\\ {g_{k}(\\pmb{x},\\pmb{x}^{\\prime})=\\|\\pmb{x}-\\pmb{x}^{\\prime}\\|_{2}^{2}\\cdot\\mathbb{I}\\left\\{(\\pmb{x},\\pmb{x}^{\\prime})\\in C_{k}^{2}\\right\\},\\pmb{x},\\pmb{x}^{\\prime}\\in\\mathcal{X},k\\in[K]\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $|g_{k}(\\cdot,\\cdot)|\\leq G$ for $k\\in[K]$ and $G>0$ are constants. ", "page_idx": 6}, {"type": "text", "text": "We denote the function class of $k$ -means clustering corresponding to the $j$ -th label as $\\mathcal{G}^{j}$ . With the above definitions, we can derive the tight bound for $k$ -means clustering-based LSRL method: ", "page_idx": 6}, {"type": "text", "text": "Theorem 2. Let $\\mathcal{F}$ be a vector-valued function class of $k$ -means clustering-based LSRL defined by $(I)$ . Let Assumptions $^{\\,l}$ and 2 hold. Given a dataset $D$ of size $n$ . Then, for any $0\\,<\\,\\delta\\,<\\,1$ , with probability at least $1-\\delta$ , the following holds for any $\\pmb{f}\\in\\mathcal{F}$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R(f)\\leq\\widehat{R}_{D}(f)+3M\\sqrt{\\frac{\\log\\frac{2}{\\delta}}{2n}}+\\frac{48\\sqrt{2}\\rho\\kappa\\Lambda\\sqrt{K}R\\,\\Big(1+\\log^{\\frac{1}{2}}(8e^{2}n^{3}c^{3})\\cdot\\log\\frac{M\\sqrt{n}}{\\mu B}\\Big)}{\\sqrt{n}}}\\\\ &{\\qquad+\\frac{24^{2}\\sqrt{2}\\rho\\sqrt{K}G}{\\sqrt{n}}\\left(1+\\log^{\\frac{1}{2}}(e^{2}n^{3}c^{3})\\cdot\\log\\frac{M\\sqrt{n c}}{G}\\right)\\left(1+\\log^{\\frac{1}{2}}(8e^{2}n^{3}c^{3})\\cdot\\log\\frac{M\\sqrt{n}}{\\mu B}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Remark 2. There are three key points in the generalization analysis of $k$ -means clustering-based LSRL method. 1) Since $k$ -means clustering-based LSRL method is two-stage, i.e., the centers of clusters is generated by using $k$ -means clustering in the first stage, then these centers are exploited to generate label-specific representations which are used to learn a multi-label classifier in the second stage, we cannot formally express these two stages in a closed-form expression through a composite function ( $\\mathcal{K}$ centers are generated by the arg min function). Furthermore, $K$ centers generated in the first stage are actually used as fixed parameters rather than inputs in the second stage. Hence, in order to fully consider the capacity of the model corresponding to the first stage, it is reasonable to define the whole function class as the sum of the function classes $\\mathcal{F}+\\mathcal{L}_{c l u}\\circ\\mathcal{G}$ corresponding to the methods of these two stages. Then, combined with Lemma 1, the generalization analysis is transformed into the bounding of the complexity of the projection function class $\\mathcal{P}(\\mathcal{F}\\!+\\!\\mathcal{L}_{c l u}\\circ\\mathcal{G})$ . The introduction of class $\\mathcal{G}$ induces an additional increase in complexity, i.e., the last term in Theorem 2. 2) The generalization analysis of $k$ -means clustering-based LSRL method involves the generalization analysis for $k$ -means clustering. However, since the $k$ -means clustering framework involves pairwise functions, a sequence of pairs of i.i.d. individual observation in $k$ -means clustering is no longer independent, which makes standard techniques in the i.i.d case for traditional Rademacher complexity inapplicable for $k$ -means clustering. We convert the non-sum-of-i.i.d pairwise function to a sum-ofi.i.d form by using permutations in $U$ -process [Cl\u00e9men\u00e7on et al., 2008]. We show that the empirical Rademacher complexity of a loss function space associated with the vector-valued function class $\\mathcal{G}$ can be bounded by $\\begin{array}{r}{\\Hat{\\Re}_{D^{\\prime}}(\\mathcal{L}_{c l u}\\circ\\mathcal{G}):=\\mathbb{E}_{\\epsilon}\\left[\\operatorname*{sup}_{g\\in\\mathcal{G}}\\frac{1}{\\left\\lfloor\\frac{n}{2}\\right\\rfloor}\\sum_{i=1}^{\\lfloor\\frac{n}{2}\\rfloor}\\epsilon_{i}\\ell_{c l u}(g(\\pmb{x}_{i},\\pmb{x}_{i+\\lfloor\\frac{n}{2}\\rfloor}))\\right]}\\end{array}$ . 3) In order to derive tight bounds for $k$ -means clustering, we develop a novel vector-contraction inequality that can induce bounds with a square-root dependency on the number of clusters. The theoretical techniques and results involved here may be of independent interest. The generalization bound of $k$ -means clustering-based LSRL method is tighter than the state of the art with a faster convergence rate ${\\tilde{O}}({\\sqrt{K/n}})$ , which is independent on the number of labels. Since the lower bound for clustering is $\\Omega({\\sqrt{K/n}})$ [Bartlett et al., 1998], our bound is (nearly) optimal, up to logarithmic terms, even from the pe\u221arspective of clustering. The constant $A$ of the generalization bound in Theorem 1 corresponds to $2\\kappa\\sqrt{K}R$ here. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "5.2 Generalization Bounds for Lasso-Based LSRL Method ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Lasso-based LSRL method, i.e., LLSF [Huang et al., 2016], assumes that the label-specific representation of each label should have sparsity and sharing properties. For sparsity, LLSF uses Lasso as the model corresponding to each label. The property of sharing is achieved by considering that two strongly correlated labels will share more features with each other than two uncorrelated or weakly correlated labels and the corresponding weights will be similar, i.e., their inner product will be large. ", "page_idx": 7}, {"type": "text", "text": "Formally, since each label corresponds to a Lasso, this means that in the class of LSRL defined by (1), the base loss $\\ell_{b}$ is the squared loss, the nonlinear mappings $\\zeta(\\cdot)$ and $\\phi(\\cdot)$ are both identity transformations for any $j\\in[c]$ , and the constraint $\\alpha(w)$ is $\\|\\pmb{w}_{j}\\|_{1}\\leq\\Lambda$ for any $j\\in[c]$ . For the $j$ -th label, the property of sharing is reflected by the additionally introduced constraint $\\textstyle\\sum_{i}^{c}(1-$ $s_{j i}){\\pmb w}_{j}^{\\,\\,\\top}{\\pmb w}_{i}\\leq\\tau$ , where $s_{j i}$ is the cosine similarity between labels $y_{j}$ and $y_{i}$ , here we refer to it as the sharing constraint. The loss function used by Lasso-based LSRL method is the Decomposable loss. ", "page_idx": 7}, {"type": "text", "text": "Since the squared loss is not Lipschitz continuous, the theoretical results on the Lipschitz continuity of the loss functions in Proposition 1 cannot be applied to Lasso-based LSRL method. To overcome this challenge, we define the pseudo-Lipschitz function, which is also used in the theoretical analysis of approximate message passing algorithms [Bayati and Montanari, 2011]. ", "page_idx": 7}, {"type": "text", "text": "Definition 2. For $k\\geq1$ , we say that a function $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ is pseudo-Lipschitz of order $k$ if there exists a constant $L>0$ such that the following inequality holds for all $x,y\\in\\mathbb{R}$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\n|f(x)-f(y)|\\leq L\\left(1+|x|^{k-1}+|y|^{k-1}\\right)|x-y|.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Note that any pseudo-Lipschitz function of order 1 is Lipschitz continuous. The following Proposition shows that the Decomposable loss is still Lipschitz continuous if the base loss is the squared loss. ", "page_idx": 7}, {"type": "text", "text": "Proposition 2. The squared loss is 1 pseudo-Lipschitz of order 2, the surrogate Decomposable Loss is $(3+2B)c$ -Lipschitz w.r.t. the $\\ell_{\\infty}$ norm if the base loss $\\ell_{b}$ is the squared loss. ", "page_idx": 7}, {"type": "text", "text": "With the above definitions, we can derive the generalization bound for Lasso-based LSRL method: Theorem 3. Let $\\mathcal{F}$ be a vector-valued function class of Lasso-based LSRL defined by $(I)$ . Let Assumptions $^{\\,l}$ and 2 hold. Given a dataset $D$ of size $n$ . Then, for any $0<\\delta<1$ , with probability at ", "page_idx": 7}, {"type": "text", "text": "least $1-\\delta$ , the following holds for any $\\pmb{f}\\in\\mathcal{F}$ : ", "page_idx": 8}, {"type": "equation", "text": "$$\nR(f)\\le\\widehat R_{D}(f)+\\frac{24\\sqrt{2}(3+2B)c\\Lambda R\\left(1+\\log^{\\frac{1}{2}}(8e^{2}n^{3}c^{3})\\cdot\\log\\frac{M\\sqrt{n}}{\\rho B}\\right)}{\\sqrt{n}}+3M\\sqrt{\\frac{\\log\\frac{2}{\\delta}}{2n}}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Remark 3. The complexity of the LLSF function class can be bounded by the complexity of the LSRL function class where each label corresponds to a Lasso, since the introduction of the sharing constraint in the LLSF function class reduces the complexity of the function class compared with the LSRL function class where each label corresponds to a Lasso. Hence, the complexity analysis of the LLSF function class can be converted into upper bounding the Rademacher complexity of the LSRL function class where each label corresponds to a Lasso. The constant $A$ of the generalization bound in Theorem 1 corresponds to $R$ here, and the value of $\\rho$ is $(3+2B)c$ , which induce the ${\\widetilde{O}}(c/{\\sqrt{n}})$ bound here. If other loss functions are used, e.g., Hamming, Subset or Ranking loss, instead of Decomposable loss, the dependency of the bounds for Lasso-based LSRL method on c can be improved from linear to independent, up to logarithmic terms. ", "page_idx": 8}, {"type": "text", "text": "5.3 Generalization Bounds for DNN-Based LSRL Method ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "DNN-based LSRL method, i.e., CLIF [Hang and Zhang, 2022], exploits the powerful representation learning capability of deep neural networks (DNNs) to learn label-specific representation in an endto-end manner. Since the construction of label-specific representation involves graph convolutional networks (GCNs), we first introduce the relevant definitions for GCN. ", "page_idx": 8}, {"type": "text", "text": "Let $\\mathcal{G}=\\{\\mathcal{V},\\mathcal{E}\\}$ be a given undirected graph, where $\\mathcal{V}=\\{\\pmb{x}_{1},\\pmb{x}_{2},\\dots,\\pmb{x}_{n}\\}$ is the set of nodes with size $|\\gamma|=n$ and $\\mathcal{E}$ is the set of edges. Let $A$ and $D$ be the adjacency matrix and the diagonal degree matrix respectively, where $\\textstyle D_{i i}=\\sum_{j=1}^{n}A_{i j}$ . Let $\\tilde{A}=\\left(D+I_{n}\\right)^{-\\frac{1}{2}}\\left(A+I_{n}\\right)\\left(D+I_{n}\\right)^{-\\frac{1}{2}}$ denote the normalized adjacency matrix with self-connections, where $I$ is the identity matrix. The feature propagation process of a two-layer GCN is $\\sigma(\\tilde{A}\\sigma(\\tilde{A}X W_{1})W_{2})$ , where $W_{1}$ and $W_{2}$ are parameter matrices, $X$ is the node feature matrix, and the $i$ -th row $X_{i*}$ is the node feature $\\pmb{x}_{i}$ . ", "page_idx": 8}, {"type": "text", "text": "Specifically, for DNN-based LSRL method, first, a graph (here we call it the label graph) is constructed over the label space and a GCN is used to generate the label embeddings, i.e., the label embeddings can be denoted by $\\psi(Y)=\\sigma_{R e L U}(\\tilde{A}\\sigma_{R e L U}(\\tilde{A}Y W_{1})W_{2})$ , where $Y$ is the node feature matrix of the label graph with size $c$ and the nodes are also bounded by $R$ , $\\sigma_{R e L U}$ is the ReLU activation. Second, the label embedding of the $j$ -th label is decoded into the importance vector by a one-layer fully-connected neural network, i.e., $\\sigma_{s i g}(W_{3}\\psi(Y)_{j})$ , where $\\sigma_{s i g}$ is the sigmoid activation, and the input feature is mapped into the latent representation through a one-layer fully-connected neural network, i.e., $\\sigma_{R e L U}(W_{4}{\\pmb x})$ . Third, for the $j$ -th label, the Hadamard product of the importance vector and the latent representation is defined as the pertinent representation, and then the label-specific representation for the $j$ -th label is obtained by feeding the pertinent representation into another one-layer fully-connected neural network. Hence, the label-specific representation for the $j$ -th label is ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\phi_{j}({\\pmb x})=\\sigma_{R e L U}\\left\\{W_{5}\\cdot\\left[\\sigma_{R e L U}(W_{4}{\\pmb x})\\odot\\sigma_{s i g}(W_{3}{\\psi}(Y)_{j})\\right]\\right\\}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Finally, the $j$ -th model is implemented by a fully-connected layer, i.e., $f_{j}(\\pmb{x})=\\sigma_{s i g}(\\pmb{w}_{j}^{\\top}\\phi_{j}(\\pmb{x}))$ . ", "page_idx": 8}, {"type": "text", "text": "In the generalization analysis of the above deep neural network, we introduce the following assumption, which is a common assumption in the generalization analysis for DNNs [Bartlett et al., 2017, Golowich et al., 2018, Zhang and Zhang, 2023, Tang and Liu, 2023]. ", "page_idx": 8}, {"type": "text", "text": "Assumption 3. Assume that the parameter metrices in DNN-based LSRL method are bounded, i.e., $\\lVert W_{i}\\rVert\\leq D$ , $i\\in$ [5], where $D>0$ is a constant. ", "page_idx": 8}, {"type": "text", "text": "With the above definitions, we can derive the generalization bound for DNN-based LSRL method: ", "page_idx": 8}, {"type": "text", "text": "Theorem 4. Let $\\mathcal{F}$ be a vector-valued function class of DNN-based LSRL defined by $(I)$ . Let Assumptions $^{\\,l}$ , 2, and $^3$ hold. Given a dataset $D$ of size $n$ . Then, for any $0<\\delta<1$ , with probability at least $1-\\delta$ , the following holds for any $\\pmb{f}\\in\\mathcal{F}$ : ", "page_idx": 8}, {"type": "equation", "text": "$$\nR(f)\\leq\\widehat{R}_{D}(f)+\\frac{6\\sqrt{2}\\rho\\Lambda D^{5}R^{2}(g_{\\mathrm{max}}+1)\\left(1+\\log^{\\frac{1}{2}}(8e^{2}n^{3}c^{3})\\cdot\\log\\frac{M\\sqrt{n}}{\\rho B}\\right)}{\\sqrt{n}}+3M\\sqrt{\\frac{\\log\\frac{2}{\\delta}}{2n}},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $g_{\\mathrm{max}}$ is the maximum node degree of the label graph. ", "page_idx": 8}, {"type": "text", "text": "Remark 4. The term $\\widetilde{\\mathfrak{R}}_{n c}(\\mathcal{P}(\\mathcal{F}))\\,\\le\\,\\Lambda D^{5}R^{2}(g_{\\mathrm{max}}+1)/4\\sqrt{n c}$ , which makes the Rademacher complexity $\\hat{\\mathfrak{R}}_{D}(\\mathcal{L})$ actually independent on c, up to logarithmic terms, and results in a tight bound for DNN-based LSRL method with a faster convergence rate $\\widetilde{O}(1/\\sqrt{n})$ . The constant $A$ of the generalization bound in Theorem 1 corresponds to $\\bar{D}^{5}R^{2}(g_{\\mathrm{max}}+1)\\dot{/}4$ here. For deep GCNs, the increase in depth means that the generalization performance will deteriorate, which is consistent with empirical performance and guides us not to design too many layers of GCN. In addition, the bound is linearly dependent on the maximum node degree of the label graph, which suggests that when the performance of the model is always unsatisfactory, we can check whether the maximum node degree is large and consider using some techniques to remove some edges, e.g., DropEdge [Rong et al., 2020], to alleviate the over-fitting problem. We will further explore more network structures to learn more effective label-specific representations, e.g., hypernetworks [Galanti and Wolf, 2020, Chen et al., 2023, Shen et al., 2023], deep kernel networks [Zhang and Liao, 2020, Zhang and Zhang, 2023], and provide generalization analysis for the corresponding DNN-based LSRL methods. ", "page_idx": 9}, {"type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Compared with existing methods considering label correlations, which mainly focus on the processing of the label space by exploiting or modeling relationships between labels, LSRL mainly focuses on the operation on the input space and implicitly considers label correlations in the process of constructing label-specific representations. For example, in the construction of label-specific representations in LLSF and CLIF, the label correlation information is embedded into the label-specific representations in the input space by introducing the sharing constraint and using a GCN over the label graph to generate label embeddings, respectively. LSRL methods are more effective since the label correlation information is considered in the construction of label-specific representations. ", "page_idx": 9}, {"type": "text", "text": "Our theoretical results explain why LSRL is an effective strategy to improve the generalization performance of multi-label learning. On the one hand, existing results can improve the dependency of the bound on $c$ from linear to square-root by preserving the coupling among different components, which corresponds to high-order label correlations in\u221aduced by norm regularizers.\u221a However, the improvement in the preservation of coupling by a factor of $\\sqrt{c}$ beneftis from replacing $\\sqrt{c}\\Lambda$ with $\\Lambda$ in the constraint to some extent, and preserving the coupling corresponds to the stricter assumption [Zhang and Zhang, 2024]. Our results for LSRL decouple the relationship among different components, and the bounds with a weaker dependency on $c$ are tighter than the existing results that preserve the coupling, which also explains why LSRL methods outperform the multi-label methods that consider high-order label correlations induced by norm regularizers. On the other hand, based on our results, we can find that LSRL methods substantially increase the data processing, i.e., the process of constructing label-specific representations. From the perspective of model capacity, compared with traditional multi-label methods, since the introduction of construction methods of label-specific representations, the capacity of the model is significantly increased, especially if deep learning methods are used to generate label-specific representations, which improves the representation ability of the model. Or more intuitively, LSRL means an increase in model capacity and stronger representation ability, which makes it easier to find the hypotheses with better generalization in the function class. ", "page_idx": 9}, {"type": "text", "text": "The vector-contraction inequality and the theoretical tools developed here are applicable to the theoretical analysis of other problem settings, such as multi-class classification, or more general vectorvalued learning problem. For multi-class classification, multi-class margin-based loss, multinomial logistic loss, Top- $k$ hinge loss, etc. are all $\\ell_{\\infty}$ Lipschitz [Lei et al., 2019]. For multi-label learning, the surrogate loss for Macro-Averaged AUC is also $\\ell_{\\infty}$ Lipschitz [Zhang and Zhang, 2024]. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we propose a novel vector-contraction inequality for $\\ell_{\\infty}$ norm Lipschitz continuous loss, and derive bounds for general function classes of LSRL with a weaker dependency on $c$ than the state of the art. In addition, we analyze the bounds for several typical LSRL methods, and study the impact of different label-specific representations on the generalization analysis. ", "page_idx": 9}, {"type": "text", "text": "In future work, we will extend our bounds to more LSRL methods, and derive tighter bounds for LSRL with a faster convergence rate w.r.t. the number of examples, and further design efficient models and algorithms to construct label-specific representations with good generalization performance. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The authors wish to thank the anonymous reviewers for their helpful comments and suggestions. This work was supported by the National Science Foundation of China (62225602). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Junwen Bai, Shufeng Kong, and Carla P. Gomes. Disentangled variational autoencoder based multilabel classification with covariance-aware multivariate probit model. In Christian Bessiere, editor, Proceedings of the 29th International Joint Conference on Artificial Intelligence, number IJCAI 2020, pages 4313\u20134321, 2020.   \nPeter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3:463\u2013482, 2002.   \nPeter L. Bartlett, Tam\u00e1s Linder, and G\u00e1bor Lugosi. The minimax distortion redundancy in empirical quantizer design. IEEE Transactions on Information Theory, 44(5):1802\u20131813, 1998.   \nPeter L. Bartlett, Dylan J. Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural networks. Advances in Neural Information Processing Systems, 30(NIPS 2017):6240\u20136249, 2017.   \nMohsen Bayati and Andrea Montanari. The dynamics of message passing on dense graphs, with applications to compressed sensing. IEEE Transactions on Information Theory, 57(2):764\u2013785, 2011.   \nMatthew R. Boutell, Jiebo Luo, Xipeng Shen, and Christopher M. Brown. Learning multi-label scene classification. Pattern Recognition, 37(9):1757\u20131771, 2004.   \nRicardo Silveira Cabral, Fernando De la Torre, Jo\u00e3o Paulo Costeira, and Alexandre Bernardino. Matrix completion for multi-label image classification. Advances in Neural Information Processing Systems, 24(NIPS 2011):190\u2013198, 2011.   \nNicol\u00f2 Cesa-Bianchi, Matteo Re, and Giorgio Valentini. Synergy of multi-label hierarchical ensembles, data fusion, and cost-sensitive methods for gene functional inference. Machine Learning, 88 (1-2):209\u2013241, 2012.   \nDi Chen, Yexiang Xue, Daniel Fink, Shuo Chen, and Carla P. Gomes. Deep multi-species embedding. In Proceedings of the 26th International Joint Conference on Artificial Intelligence, number IJCAI 2017, pages 3639\u20133646. ijcai.org, 2017.   \nSirui Chen, Yuan Wang, Zijing Wen, Zhiyu Li, Changshuo Zhang, Xiao Zhang, Quan Lin, Cheng Zhu, and Jun Xu. Controllable multi-objective re-ranking with policy hypernetworks. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, number KDD 2023, pages 3855\u20133864, 2023.   \nTianshui Chen, Liang Lin, Riquan Chen, Xiaolu Hui, and Hefeng Wu. Knowledge-guided multi-label few-shot learning for general image recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(3):1371\u20131384, 2020.   \nSt\u00e9phan Cl\u00e9men\u00e7on, G\u00e1bor Lugosi, and Nicolas Vayatis. Ranking and empirical minimization of u-statistics. The Annals of Statistics, 36(2):844\u2013874, 2008.   \nKunal Dahiya, Ananye Agarwal, Deepak Saini, K Gururaj, Jian Jiao, Amit Singh, Sumeet Agarwal, Purushottam Kar, and Manik Varma. Siamesexml: Siamese networks meet extreme classifiers with 100m labels. In Proceedings of the 38th International Conference on Machine Learning, pages 2330\u20132340, 2021.   \nKrzysztof Dembczynski, Willem Waegeman, Weiwei Cheng, and Eyke H\u00fcllermeier. Regret analysis for performance metrics in multi-label classification: The case of hamming and subset zero-one loss. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, volume 6321, pages 280\u2013295, 2010.   \nKrzysztof Dembczynski, Wojciech Kotlowski, and Eyke H\u00fcllermeier. Consistent multilabel ranking through univariate losses. arXiv:1206.6401, 2012.   \nAndr\u00e9 Elisseeff and Jason Weston. A kernel method for multi-labelled classification. In Advances in Neural Information Processing Systems, number NIPS 2001, pages 681\u2013687, 2001.   \nDylan J. Foster and Alexander Rakhlin. $\\ell_{\\infty}$ vector contraction for rademacher complexity. arXiv:1911.06468v1, 2019.   \nTomer Galanti and Lior Wolf. On the modularity of hypernetworks. In Advances in Neural Information Processing Systems, volume 33, pages 10409\u201310419, 2020.   \nNoah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of neural networks. International Conference on Computational Learning Theory, 75(COLT 2018): 297\u2013299, 2018.   \nYumeng Guo, Fulai Chung, Guozheng Li, Jiancong Wang, and James C. Gee. Leveraging labelspecific discriminant mapping features for multi-label learning. ACM Transactions on Knowledge Discovery from Data, 13(2):24:1\u201324:23, 2019.   \nJun-Yi Hang and Min-Ling Zhang. Collaborative learning of label semantics and deep label-specific features for multi-label classification. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(12):9860\u20139871, 2022.   \nJun-Yi Hang, Min-Ling Zhang, Yanghe Feng, and Xiaocheng Song. End-to-end probabilistic labelspecific feature learning for multi-label classification. In Proceedings of the 36th AAAI Conference on Artificial Intelligence, number AAAI 2022, pages 6847\u20136855, 2022.   \nJun Huang, Guorong Li, Qingming Huang, and Xindong Wu. Learning label specific features for multi-label classification. In Charu C. Aggarwal, Zhi-Hua Zhou, Alexander Tuzhilin, Hui Xiong, and Xindong Wu, editors, Proceedings of the 15th IEEE International Conference on Data Mining, number ICDM 2015, pages 181\u2013190, 2015.   \nJun Huang, Guorong Li, Qingming Huang, and Xindong Wu. Learning label-specific features and class-dependent labels for multi-label classification. IEEE Transactions on Knowledge and Data Engineering, 28(12):3309\u20133323, 2016.   \nJun Huang, Guorong Li, Qingming Huang, and Xindong Wu. Joint feature selection and classification for multilabel learning. IEEE Transactions on Cybernetics, 48(3):876\u2013889, 2018.   \nShuiwang Ji, Lei Tang, Shipeng Yu, and Jieping Ye. A shared-subspace learning framework for multi-label classification. ACM Transactions on Knowledge Discovery from Data, 4(2):1\u201329, 2010.   \nAntoine Ledent, Waleed Mustafa, Yunwen Lei, and Marius Kloft. Norm-based generalisation bounds for deep multi-class convolutional neural networks. In Proceedings of the 35th AAAI Conference on Artificial Intelligence, volume 35, pages 8279\u20138287, 2021.   \nYunwen Lei, \u00dcr\u00fcn Dogan, Alexander Binder, and Marius Kloft. Multi-class svms: From tighter data-dependent generalization bounds to novel algorithms. In Advances in Neural Information Processing Systems, volume 28, pages 2035\u20132043, 2015.   \nYunwen Lei, \u00dcr\u00fcn Dogan, Ding-Xuan Zhou, and Marius Kloft. Data-dependent generalization bounds for multi-class classification. IEEE Transactions on Information Theory, 65(5):2995\u20133021, 2019.   \nShaojie Li and Yong Liu. Sharper generalization bounds for clustering. In Proceedings of the 38th International Conference on Machine Learning, volume 139, pages 6392\u20136402, 2021.   \nChong Liu, Peng Zhao, Sheng-Jun Huang, Yuan Jiang, and Zhi-Hua Zhou. Dual set multi-label learning. In Proceedings of the 32nd AAAI Conference on Artificial Intelligence, number AAAI 2018, pages 3635\u20133642, 2018.   \nWeiwei Liu and Xiaobo Shen. Sparse extreme multi-label learning with oracle property. In Proceedings of the 36th International Conference on Machine Learning, volume 97, pages 4032\u20134041, 2019.   \nWeiwei Liu, Haobo Wang, Xiaobo Shen, and Ivor W. Tsang. The emerging trends of multi-label learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(11):7955\u20137974, 2022.   \nFran\u00e7oise Lust-Piquard and Gilles Pisier. Non commutative khintchine and paley inequalities. Arkiv f\u00f6r matematik, 29:241\u2013260, 1991.   \nAndreas Maurer. A vector-contraction inequality for rademacher complexities. In Proceedings of the 27th International Conference on Algorithmic Learning Theory, volume 9925, pages 3\u201317, 2016.   \nYashoteja Prabhu and Manik Varma. Fastxml: a fast, accurate and stable tree-classifier for extreme multi-label learning. In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, number KDD 2014, pages 263\u2013272, 2014.   \nYu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph convolutional networks on node classification. In Proceedings of the 8th International Conference on Learning Representations, number ICLR 2020, 2020.   \nTimothy N. Rubin, America Chambers, Padhraic Smyth, and Mark Steyvers. Statistical topic models for multi-label document classification. Machine Learning, 88(1-2):157\u2013208, 2012.   \nChenglei Shen, Xiao Zhang, Wei Wei, and Jun Xu. Hyperbandit: Contextual bandit with hypernewtork for time-varying user preferences in streaming recommendation. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, pages 2239\u20132248, 2023.   \nNathan Srebro, Karthik Sridharan, and Ambuj Tewari. Smoothness, low noise and fast rates. In Advances in Neural Information Processing Systems, volume 23, pages 2199\u20132207, 2010.   \nYan-Ping Sun and Min-Ling Zhang. Compositional metric learning for multi-label classification. Frontiers of Computer Science, 15(5):155320, 2021.   \nHuayi Tang and Yong Liu. Towards understanding the generalization of graph neural networks. arXiv:2305.08048v1, 2023.   \nWei Weng, Yaojin Lin, Shunxiang Wu, Yuwen Li, and Yun Kang. Multi-label learning based on label-specific features and local pairwise label correlation. Neurocomputing, 273:385\u2013394, 2018.   \nWei Weng, Yan-Nan Chen, Chin-Ling Chen, Shunxiang Wu, and Jinghua Liu. Non-sparse label specific features selection for multi-label classification. Neurocomputing, 377:85\u201394, 2020.   \nGuoqiang Wu and Jun Zhu. Multi-label classification: do hamming loss and subset accuracy really confilct with each other? Advances in Neural Information Processing Systems, 33(NeurIPS 2020), 2020.   \nGuoqiang Wu, Chongxuan Li, Kun Xu, and Jun Zhu. Rethinking and reweighting the univariate losses for multi-label ranking: Consistency and generalization. Advances in Neural Information Processing Systems, 34(NeurIPS 2021):14332\u201314344, 2021a.   \nGuoqiang Wu, Chongxuan Li, and Yilong Yin. Towards understanding generalization of macro-auc in multi-label learning. In Proceedings of the 40th International Conference on Machine Learning, volume 202, pages 37540\u201337570, 2023.   \nLiang Wu, Antoine Ledent, Yunwen Lei, and Marius Kloft. Fine-grained generalization analysis of vector-valued learning. In Proceedings of the 35th AAAI Conference on Artificial Intelligence, number AAAI 2021, pages 10338\u201310346, 2021b.   \nChang Xu, Tongliang Liu, Dacheng Tao, and Chao Xu. Local rademacher complexity for multi-label learning. IEEE Transactions on Image Processing, 25(3):1495\u20131507, 2016.   \nMiao Xu and Lan-Zhe Guo. Learning from group supervision: the impact of supervision deficiency on multi-label learning. Science China Information Sciences, 64(3), 2021.   \nGuangxu Xun, Kishlay Jha, Jianhui Sun, and Aidong Zhang. Correlation networks for extreme multi-label text classification. In Proceedings of the 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, number KDD 2020, pages 1074\u20131082, 2020.   \nVacit Oguz Yazici, Abel Gonzalez-Garcia, Arnau Ramisa, Bartlomiej Twardowski, and Joost van de Weijer. Orderless recurrent models for multi-label classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, number CVPR 2020, pages 13440\u201313449, 2020.   \nIan En-Hsu Yen, Xiangru Huang, Pradeep Ravikumar, Kai Zhong, and Inderjit S. Dhillon. Pdsparse : A primal and dual sparse approach to extreme multiclass and multilabel classification. In Proceedings of the 33nd International Conference on Machine Learning, volume 48, pages 3069\u20133077, 2016.   \nRenchun You, Zhiyao Guo, Lei Cui, Xiang Long, Yingze Bao, and Shilei Wen. Cross-modality attention with semantic graph embedding for multi-label classification. In Proceedings of the 34th AAAI Conference on Artificial Intelligence, number AAAI 2020, pages 12709\u201312716, 2020.   \nHsiang-Fu Yu, Prateek Jain, Purushottam Kar, and Inderjit S. Dhillon. Large-scale multi-label learning with missing labels. In Proceedings of the 31th International Conference on Machine Learning, volume 32, pages 593\u2013601, 2014.   \nKai Yu, Shipeng Yu, and Volker Tresp. Multi-label informed latent semantic indexing. In Ricardo A. Baeza-Yates, Nivio Ziviani, Gary Marchionini, Alistair Moffat, and John Tait, editors, Proceedings of the 28th Annual International Conference on Research and Development in Information Retrieval, number SIGIR 2005, pages 258\u2013265, 2005.   \nZe-Bang Yu and Min-Ling Zhang. Multi-label classification with label-specific feature generation: A wrapped approach. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(9): 5199\u20135210, 2022.   \nChunyu Zhang and Zhanshan Li. Multi-label learning with label-specific features via weighting and label entropy guided clustering ensemble. Neurocomputing, 419:59\u201369, 2021.   \nJujie Zhang, Min Fang, and Xiao Li. Multi-label learning with discriminative features for each label. Neurocomputing, 154:305\u2013316, 2015.   \nMin-Ling Zhang and Lei Wu. Lift: Multi-label learning with label-specific features. IEEE Transactions on Pattern Analysis and Machine Intelligence, 37(1):107\u2013120, 2015.   \nMin-Ling Zhang and Zhi-Hua Zhou. ML-KNN: A lazy learning approach to multi-label learning. Pattern Recognition, 40(7):2038\u20132048, 2007.   \nMin-Ling Zhang and Zhi-Hua Zhou. A review on multi-label learning algorithms. IEEE Transactions on Knowledge and Data Engineering, 26(8):1819\u20131837, 2014.   \nMin-Ling Zhang, Yu-Kun Li, Xu-Ying Liu, and Xin Geng. Binary relevance for multi-label learning: an overview. Frontiers of Computer Science, 12(2):191\u2013202, 2018.   \nYi-Fan Zhang and Shizhong Liao. A kernel perspective for the decision boundary of deep neural networks. In Proceedings of the 32nd IEEE International Conference on Tools with Artificial Intelligence, number ICTAI 2020, pages 653\u2013660, 2020.   \nYi-Fan Zhang and Min-Ling Zhang. Nearly-tight bounds for deep kernel learning. In Proceedings of the 40th International Conference on Machine Learning, volume 202, pages 41861\u201341879, 2023.   \nYi-Fan Zhang and Min-Ling Zhang. Generalization analysis for multi-label learning. In Proceedings of the 41st International Conference on Machine Learning, number ICML 2024, 2024.   \nYue Zhu, James T. Kwok, and Zhi-Hua Zhou. Multi-label learning with global and local label correlation. IEEE Transactions on Knowledge and Data Engineering, 30(6):1081\u20131094, 2018. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Appendix Outline ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In the appendix, we give the detailed proofs of those theoretical results in the main paper. Our main proofs include: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The $\\ell_{\\infty}$ Lipschitz continuity of the commonly used losses for LSRL (Proposition 1).   \n\u2022 The novel vector-contraction inequality for $\\ell_{\\infty}$ Lipschitz loss (Lemma 1).   \n\u2022 The generalization bound of the general LSRL class with no dependency on $c$ (Theorem 1).   \n\u2022 The generalization bound for $k$ -means clustering-based LSRL method (Theorem 2).   \n\u2022 The (pseudo-) Lipschitz continuity of the squared and Decomposable loss (Proposition 2).   \n\u2022 The generalization bound for Lasso-based LSRL method (Theorem 3).   \n\u2022 The generalization bound for DNN-based LSRL method (Theorem 4). ", "page_idx": 14}, {"type": "text", "text": "A.2 Preliminaries ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.2.1 Definitions of the corresponding complexity measures ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Definition 3 $\\ell_{\\infty}$ norm covering number). Let $\\mathcal{F}$ be a class of real-valued functions mapping from $\\mathcal{X}$ to $\\mathbb{R}_{}$ . Let $D=\\{x_{1},\\ldots,x_{n}\\}$ be a set with $n$ i.i.d. samples. For any $\\epsilon>0$ , the empirical $\\ell_{\\infty}$ norm covering number $\\mathcal{N}_{\\infty}(\\epsilon,\\mathcal{F},D)$ w.r.t. $D$ is defined as the minimal number m of a collection of vectors $\\pmb{v}^{1},\\dots,\\pmb{v}^{m}\\in\\mathbb{R}^{n}$ such that $\\begin{array}{r}{\\operatorname*{max}_{i\\in[n]}\\left|f\\left(\\pmb{x}_{i}\\right)-\\pmb{v}_{i}^{j}\\right|\\leq\\epsilon\\left(\\pmb{v}_{i}^{j}\\right.}\\end{array}$ is the i-th component of the vector $\\pmb{v}^{j}$ ). In this case, we call $\\{v^{1},\\ldots,v^{m}\\}$ an $(\\epsilon,\\ell_{\\infty})$ -cover of $\\mathcal{F}$ with respect to $D$ . We also define $\\begin{array}{r}{\\mathcal{N}_{\\infty}(\\epsilon,\\mathcal{F},n)=\\operatorname*{sup}_{D}\\mathcal{N}_{\\infty}(\\dot{\\epsilon},\\mathcal{F},D)}\\end{array}$ . ", "page_idx": 14}, {"type": "text", "text": "Definition 4 (Fat-shattering dimension). Let $\\mathcal{F}$ be a class of real-valued functions mapping from $\\mathcal{X}$ to $\\mathbb{R}$ . We define the fat-shattering dimension $f\\!a t_{\\epsilon}(\\mathcal{F})$ at scale $\\epsilon\\mathrm{~>~0~}$ as the largest $p\\,\\in\\,\\mathbb{N}$ such that there exist $p$ points $\\mathbf{\\boldsymbol{x}}_{1},\\dotsc,\\mathbf{\\boldsymbol{x}}_{p}\\in\\mathcal{X}$ and witnesses $s_{1},\\ldots,s_{p}\\,\\in\\,\\mathbb{R}$ satisfying: for any $\\delta_{1},\\ldots,\\delta_{p}\\in\\{-1,+1\\}$ there exists $f\\in\\mathcal F$ with $\\delta_{i}\\left(f(\\pmb{x}_{i})-s_{i}\\right)\\geq\\epsilon$ , $\\forall i=1,\\ldots,p$ . ", "page_idx": 14}, {"type": "text", "text": "A.2.2 The Bound for the loss function space ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "For any training dataset $D\\,=\\,\\left\\{(\\mathbf{}x_{i},\\pmb{y}_{i}):i\\in[n]\\right\\}$ , let $D^{\\prime}\\,=\\,\\{(\\pmb{x}_{i},\\pmb{y}_{i}):i\\in[n]\\}$ be the training dataset with only one sample different from $D$ , where the $k$ -th sample is replaced by $(\\pmb{x}_{k}^{\\prime},\\pmb{y}_{k}^{\\prime})$ . Let $\\begin{array}{r}{\\Phi(D)=\\operatorname*{sup}_{f\\in\\mathcal{F}}[\\mathbb{E}_{(\\mathbf{x},\\mathbf{y})\\sim P}[\\ell(f(\\mathbf{x}),\\mathbf{y})]-\\frac{1}{n}\\sum_{i=1}^{n}\\ell(f(\\pmb{x}_{i}),\\pmb{y}_{i})]=\\operatorname*{sup}_{f\\in\\mathcal{F}}[R(f)-\\widehat{R}_{D}(f)].}\\end{array}$ , then ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\Phi\\left(D^{\\prime}\\right)-\\Phi(D)}\\\\ &{=\\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}[R(f)-\\widehat{R}_{D^{\\prime}}(f)]-\\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}[R(f)-\\widehat{R}_{D}(f)]}\\\\ &{\\leq\\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}[\\widehat{R}_{D}(f)-\\widehat{R}_{D^{\\prime}}(f)]}\\\\ &{=\\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}\\frac{[\\ell(f(x_{k}),y_{k})-\\ell(f(x_{k}^{\\prime}),y_{k}^{\\prime})]}{n}}\\\\ &{\\leq\\frac{M}{n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "According to McDiarmid\u2019s inequality, for any $0<\\delta<1$ , with probability at least $\\textstyle1-{\\frac{\\delta}{2}}$ over the training dataset $D$ , the following holds: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Phi(D)\\leq\\mathbb{E}_{D}[\\Phi(D)]+M\\sqrt{\\frac{\\ln(2/\\delta)}{2n}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then, we will estimate the upper bound of $\\mathbb{E}_{D}[\\Phi(D)]$ . ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{D}\\big(\\Phi(D)\\big)}\\\\ &{=\\mathbb{E}_{D}\\Bigg[\\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}\\;\\Big[\\hat{\\mathcal{U}}_{D^{\\prime}}\\Big[\\hat{H}_{D^{\\prime}}\\big(f\\big)-\\hat{H}_{D}(f)\\Big]\\Big]\\Bigg]}\\\\ &{\\leq\\mathbb{E}_{D}\\chi_{D^{\\prime}}\\left[\\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}\\;\\Bigg[\\hat{H}_{D^{\\prime}}\\big(f\\big)-\\hat{H}_{D}(f)\\Bigg]\\right]}\\\\ &{=\\mathbb{E}_{D}\\chi_{D^{\\prime}}\\left[\\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}\\;\\Bigg[\\sum_{\\ell=1}^{D}\\Big(f\\big(x_{\\ell}^{\\prime}\\big)_{\\ell}\\Big)-\\ell\\big(f\\big(x_{\\ell}^{\\prime}\\big)_{\\ell}y\\Big)\\Bigg]\\Bigg]}\\\\ &{=\\mathbb{E}_{\\xi_{D},D^{\\prime}}\\left[\\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}\\;\\Bigg[\\sum_{i=1}^{D}\\Big(f\\big(f\\big(x_{\\ell}^{\\prime}\\big)_{\\ell}y_{i}^{\\prime}\\big)-\\ell\\big(f\\big(x_{\\ell}^{\\prime}\\big)_{\\ell}y_{i}\\big)\\Big)\\Bigg]\\Bigg]}\\\\ &{\\leq\\mathbb{E}_{\\xi_{D},D^{\\prime}}\\left[\\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}\\;\\Bigg[\\sum_{i=1}^{D}\\frac{1}{\\xi_{i}(\\ell)}\\Big(f\\big(x_{\\ell}^{\\prime}\\big)_{\\ell}y_{i}^{\\prime}\\Big)+\\mathbb{E}_{\\xi_{D},D}\\left[\\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}\\;\\frac{1}{D}\\sum_{i=1}^{D}-\\ell\\big(f\\big(x_{\\ell}^{\\prime}\\big)_{\\ell}y_{i}\\big)\\right]\\right]}\\\\ &{=\\mathbb{E}_{\\xi_{D},D}\\left[\\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}\\;\\frac{1}{D}\\in\\mathcal{G}\\Big(f\\big(x_{\\ell}^{\\prime}\\big)_{\\ell}y_{i}\\Big)\\right]+\\mathbb{E}_{\\xi_{D},D}\\left[\\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}\\;\\frac{1}{D}\\sum_{i=1}^{D}-\\ell \n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then apply McDiarmid\u2019s inequality to $\\begin{array}{r}{\\mathbb{E}_{\\epsilon}\\left[\\operatorname*{sup}_{\\pmb{f}\\in\\mathcal{F}}\\frac{1}{n}\\sum_{i=1}^{n}\\epsilon_{i}\\ell(\\pmb{f}(\\pmb{x}_{i}),\\pmb{y}_{i})\\right]}\\end{array}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\epsilon,D}\\left[\\operatorname*{sup}_{f\\in\\mathcal{F}}\\frac{1}{n}\\sum_{i=1}^{n}\\epsilon_{i}\\ell(f(\\pmb{x}_{i}),\\pmb{y}_{i})\\right]\\leq\\mathbb{E}_{\\epsilon}\\left[\\operatorname*{sup}_{f\\in\\mathcal{F}}\\frac{1}{n}\\sum_{i=1}^{n}\\epsilon_{i}\\ell(f(\\pmb{x}_{i}),\\pmb{y}_{i})\\right]+M\\sqrt{\\frac{\\ln(2/\\delta)}{2n}},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "i.e., ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\Re(\\mathcal{L})\\leq\\hat{\\Re}_{D}(\\mathcal{L})+M\\sqrt{\\frac{\\ln(2/\\delta)}{2n}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Combining with (2), (3) and (4), then ", "page_idx": 15}, {"type": "equation", "text": "$$\nR(\\pmb{f})\\leq\\widehat{R}_{D}(\\pmb{f})+2\\Re_{D}(\\mathcal{L})+3M\\sqrt{\\frac{\\log\\frac{2}{\\delta}}{2n}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "A.3 General Bounds for LSRL ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.3.1 Proof of Proposition 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We first prove that the surrogate Hamming Loss is $\\mu$ -Lipschitz continuous with respect to the $\\ell_{\\infty}$ norm. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad|\\ell_{H}(f(x),y)-\\ell_{H}(f^{\\prime}(x),y)|}\\\\ &{=\\left|\\displaystyle\\Biggl|\\frac{1}{c}\\displaystyle\\sum_{j=1}^{c}\\ell_{b}\\left(y_{j}f_{j}(x)\\right)-\\frac{1}{c}\\sum_{j=1}^{c}\\ell_{b}\\left(y_{j}f_{j}^{\\prime}(x)\\right)\\right|}\\\\ &{=\\displaystyle\\frac{1}{c}\\displaystyle\\sum_{j=1}^{c}|\\ell_{b}\\left(y_{j}f_{j}(x)\\right)-\\ell_{b}\\left(y_{j}f_{j}^{\\prime}(x)\\right)|}\\\\ &{\\leq\\displaystyle\\frac{1}{c}\\displaystyle\\sum_{j=1}^{c}\\mu\\left|f_{j}(x)-f_{j}^{\\prime}(x)\\right|}\\\\ &{\\leq\\displaystyle\\frac{1}{c}\\mu\\sum_{j=1}^{c}|f_{j}(x)-f_{j}^{\\prime}(x)|}\\\\ &{=\\mu\\|f(x)-f^{\\prime}(x)\\|_{\\infty}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Second, with the elementary inequality ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left|\\operatorname*{max}\\left\\{a_{1},\\ldots,a_{c}\\right\\}-\\operatorname*{max}\\left\\{b_{1},\\ldots,b_{c}\\right\\}\\right|\\leq\\operatorname*{max}\\left\\{\\left|a_{1}-b_{1}\\right|,\\ldots,\\left|a_{c}-b_{c}\\right|\\right\\},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "we prove that the surrogate Subset Loss is $\\mu$ -Lipschitz continuous with respect to the $\\ell_{\\infty}$ norm. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~~\\left|\\ell_{S}({\\pmb f}({\\pmb x}),{\\pmb y})-\\ell_{S}\\left({\\pmb f}^{\\prime}({\\pmb x}),{\\pmb y}\\right)\\right|}\\\\ &{=\\left|\\displaystyle\\operatorname*{max}_{j\\in[{\\pmb\\xi}]}\\ell_{b}\\left(y_{j}f_{j}({\\pmb x})\\right)-\\operatorname*{max}_{j\\in[{\\pmb\\xi}]}\\ell_{b}\\left(y_{j}f_{j}^{\\prime}({\\pmb x})\\right)\\right|}\\\\ &{\\le\\displaystyle\\operatorname*{max}_{j\\in[{\\pmb\\xi}]}\\left|\\ell_{b}\\left(y_{j}f_{j}({\\pmb x})\\right)-\\ell_{b}\\left(y_{j}f_{j}^{\\prime}({\\pmb x})\\right)\\right|}\\\\ &{\\le\\displaystyle\\mu\\operatorname*{max}_{j\\in[{\\pmb\\xi}]}\\left|f_{j}({\\pmb x})-f_{j}^{\\prime}({\\pmb x})\\right|}\\\\ &{=\\mu\\left\\|f({\\pmb x})-f^{\\prime}({\\pmb x})\\right\\|_{\\infty}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Third, we prove that the surrogate Ranking Loss is $2\\mu$ -Lipschitz continuous with respect to the $\\ell_{\\infty}$ norm. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad|\\ell_{R}(f(x),y)-\\ell_{R}(f^{\\prime}(x),y)|}\\\\ &{=\\frac{1}{|Y^{+}|\\,|Y^{-}|}\\left|\\displaystyle\\sum_{p\\in\\mathcal{Y}^{+}}\\sum_{q\\in\\mathcal{Y}^{-}}\\left(\\ell_{b}\\left(f_{p}(x)-f_{q}(x)\\right)-\\ell_{b}\\left(f_{p}^{\\prime}(x)-f_{q}^{\\prime}(x)\\right)\\right)\\right|}\\\\ &{\\le\\displaystyle\\operatorname*{max}_{p\\in\\mathcal{Y}^{+}\\neq\\,q\\in\\mathcal{Y}^{-}}\\left|\\ell_{b}\\left(f_{p}(x)-f_{q}(x)\\right)-\\ell_{b}\\left(f_{p}^{\\prime}(x)-f_{q}^{\\prime}(x)\\right)\\right|}\\\\ &{\\le\\displaystyle\\mu_{p\\in\\mathcal{Y}^{+}\\setminus\\mathcal{Y}^{+}}\\left|(f_{p}(x)-f_{q}(x))-(f_{p}^{\\prime}(x)-f_{q}^{\\prime}(x))\\right|}\\\\ &{\\le\\displaystyle\\mu\\left(\\frac{1}{p\\in\\mathcal{Y}^{+}}\\left|f_{p}(x)-f_{p}^{\\prime}(x)\\right|+\\displaystyle\\operatorname*{max}_{q\\in\\mathcal{Y}^{-}}\\left|f_{q}(x)-f_{q}^{\\prime}(x)\\right|\\right)}\\\\ &{\\le2\\mu\\frac{1}{p\\in\\mathcal{Y}}\\left|f_{j}(x)-f_{j}^{\\prime}(x)\\right|}\\\\ &{=2\\mu\\left\\|f(x)-f^{\\prime}(x)\\right\\|_{\\infty}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Finally, we prove that the surrogate Decomposable Loss is $c\\mu$ -Lipschitz continuous with respect to the $\\ell_{\\infty}$ norm. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad|\\ell_{D}(f(\\mathbf{x}),y)-\\ell_{D}\\left(f^{\\prime}(\\mathbf{x}),y\\right)|}\\\\ &{=\\displaystyle\\left|\\sum_{j=1}^{c}\\ell_{b}\\left(y_{j}f_{j}(x)\\right)-\\sum_{j=1}^{c}\\ell_{b}\\left(y_{j}f_{j}^{\\prime}(x)\\right)\\right|}\\\\ &{=\\displaystyle\\sum_{j=1}^{c}|\\ell_{b}\\left(y_{j}f_{j}(x)\\right)-\\ell_{b}\\left(y_{j}f_{j}^{\\prime}(x)\\right)|}\\\\ &{\\leq\\displaystyle\\sum_{j=1}^{c}\\mu\\left|f_{j}(x)-f_{j}^{\\prime}(x)\\right|}\\\\ &{\\leq c\\mu\\operatorname*{max}\\left|f_{j}(x)-f_{j}^{\\prime}(x)\\right|}\\\\ &{=c\\mu\\left\\|f(x)-f^{\\prime}(x)\\right\\|_{\\infty}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "A.3.2 Proof of Lemma 1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof Sketch: First, the Rademacher complexity of the loss function space associated with $\\mathcal{F}$ can be bounded by the empirical $\\ell_{\\infty}$ norm covering number with the refined Dudley\u2019s entropy integral inequality. Second, according to the Lipschitz continuity w.r.t the $\\ell_{\\infty}$ norm, the empirical $\\ell_{\\infty}$ norm covering number of $\\mathcal{F}$ can be bounded by the empirical $\\ell_{\\infty}$ norm covering number of ${\\mathcal{P}}({\\mathcal{F}})$ . Third, the empirical $\\ell_{\\infty}$ norm covering number of $\\mathcal{P}(\\mathcal{F})$ can be bounded by the fat-shattering dimension, and the fat-shattering dimension can be bounded by the worst-case Rademacher complexity of $\\mathcal{P}(\\mathcal{F})$ . ", "page_idx": 16}, {"type": "text", "text": "Hence, the problem is transferred to the estimation of the worst-case Rademacher complexity. Finally, we estimate the lower bound of the worst-case Rademacher complexity of ${\\mathcal{P}}({\\mathcal{F}})$ , and then combined with the above steps, the Rademacher complexity of the loss function space associated with $\\mathcal{F}$ can be bounded. ", "page_idx": 17}, {"type": "text", "text": "We first introduce the following lemmas: ", "page_idx": 17}, {"type": "text", "text": "Lemma 2 (Khintchine-Kahane inequality [Lust-Piquard and Pisier, 1991]). Let $\\pmb{v}_{1},\\ldots,\\pmb{v}_{n}\\in\\mathcal{H}$ , where $\\mathcal{H}$ is a Hilbert space with $\\Vert\\cdot\\Vert$ being the associated $p$ -th norm. Let $\\epsilon_{1},\\ldots,\\epsilon_{n}$ be a sequence of independent Rademacher variables. Then, for any $p\\geq1$ there holds ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{min}(\\sqrt{p-1},1)\\left[\\sum_{i=1}^{n}\\|v_{i}\\|^{2}\\right]^{\\frac{1}{2}}\\leq\\left[\\mathbb{E}_{\\epsilon}\\left\\|\\sum_{i=1}^{n}\\epsilon_{i}v_{i}\\right\\|^{p}\\right]^{\\frac{1}{p}}\\leq\\operatorname*{max}(\\sqrt{p-1},1)\\left[\\sum_{i=1}^{n}\\|v_{i}\\|^{2}\\right]^{\\frac{1}{2}},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\epsilon}\\left\\|\\sum_{i=1}^{n}\\epsilon_{i}\\pmb{v}_{i}\\right\\|\\geq2^{-\\frac{1}{2}}\\left[\\sum_{i=1}^{n}\\|\\pmb{v}_{i}\\|^{2}\\right]^{\\frac{1}{2}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Lemma 3 (Lemma A.2 in [Srebro et al., 2010]). For any function class $\\mathcal{F}$ , any $S$ with a finite sample of size $n$ and any $\\epsilon>\\hat{\\Re}_{S}(\\mathcal{F})$ , we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{fat}_{\\epsilon}(\\mathcal{F})\\leq\\frac{4n\\Re_{S}^{2}(\\mathcal{F})}{\\epsilon^{2}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Lemma 4 ([Srebro et al., 2010, Lei et al., 2019]). If any function in class $\\mathcal{F}$ takes values in $[-B,B]$ , then for any $S$ with a finite sample of size $n$ , any $\\epsilon>0$ with $\\operatorname{fat}_{\\epsilon}(\\mathcal{F})<n_{*}$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\log\\mathcal{N}_{\\infty}\\left(\\epsilon,\\mathcal{F},S\\right)\\leq\\mathrm{fat}_{\\epsilon}(\\mathcal{F})\\log\\frac{2e B n}{\\epsilon}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The following lemma is a refined result of Proposition 22 in [Ledent et al., 2021], where we replace the function class taking values in $[0,1]$ with the $B$ -bounded function class, the refinement is obvious. ", "page_idx": 17}, {"type": "text", "text": "Lemma 5 (Refined Dudley\u2019s entropy integral inequality). Let $\\mathcal{F}$ be a real-valued function class with $f\\leq B$ , $f\\in\\mathcal F$ , $B>0$ , and assume that $0\\in\\mathcal{F}$ . Let $S$ be a finite sample of size $n$ . For any $2\\leq p\\leq\\infty,$ , we have the following relationship between the Rademacher complexity $\\hat{\\mathfrak{R}}_{S}(\\mathcal{F})$ and the covering number $\\mathcal{N}_{p}(\\epsilon,\\mathcal{F},S)$ . ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\hat{\\Re}_{S}(\\mathcal{F})\\leq\\operatorname*{inf}_{\\alpha>0}\\left(4\\alpha+\\frac{12}{\\sqrt{n}}\\int_{\\alpha}^{B}\\sqrt{\\log\\mathcal{N}_{p}(\\epsilon,\\mathcal{F},S)}d\\epsilon\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Step 1: We first derive the relationship between the empirical $\\ell_{\\infty}$ norm covering number $\\mathcal{N}_{\\infty}(\\epsilon,\\mathcal{L},D)$ and the empirical $\\ell_{\\infty}$ norm covering number $\\mathcal{N}_{\\infty}(\\epsilon,\\bar{\\mathcal{P}}(\\mathcal{F}),[c]\\times D)$ . ", "page_idx": 17}, {"type": "text", "text": "For the dataset $D=\\{(\\pmb{x}_{1},\\pmb{y}_{1}),\\dots,(\\pmb{x}_{n},\\pmb{y}_{n})\\}$ with $n$ i.i.d. examples: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\operatorname*{max}_{i}|\\ell(f(x_{i}),y_{i})-\\ell(f^{\\prime}(x_{i}),y_{i})|}\\\\ &{\\le\\rho\\operatorname*{max}_{i}\\|f(x_{i})-f^{\\prime}(x_{i})\\|_{\\infty}\\quad\\mathrm{(Use~Assumption~2)}}\\\\ &{\\le\\rho\\operatorname*{max}_{i}\\operatorname*{max}_{j}|f_{j}\\left(x_{i}\\right)-f_{j}^{\\prime}\\left(x_{i}\\right)|}\\\\ &{\\le\\rho\\operatorname*{max}_{i}\\operatorname*{max}_{j}|p_{j}(f(x_{i}))-p_{j}(f^{\\prime}(x_{i})|.\\quad\\mathrm{(The~definition~of~the~projection~function~class~}\\mathcal{P}(\\mathcal{A}))|}\\end{array}\n$$$\\mathcal{P}(\\mathcal{F}),$ ", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then, according to the definition of the empirical $\\ell_{\\infty}$ covering number, we have that an empirical $\\ell_{\\infty}$ cover of ${\\mathcal{P}}({\\mathcal{F}})$ at radius $\\epsilon/\\rho$ is also an empirical $\\ell_{\\infty}$ cover of the loss function space associated with $\\mathcal{F}$ at radius $\\epsilon$ , and we can conclude that: ", "page_idx": 17}, {"type": "equation", "text": "$$\nN_{\\infty}\\left(\\epsilon,\\mathcal{L},D\\right)\\leq\\mathcal{N}_{\\infty}\\left(\\frac{\\epsilon}{\\rho},\\mathcal{P}(\\mathcal{F}),[c]\\times D\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Step 2: We show that the empirical $\\ell_{\\infty}$ norm covering number of $\\mathcal{P}(\\mathcal{F})$ can be bounded by the fatshattering dimension, and the fat-shattering dimension can be bounded by the worst-case Rademacher complexity of ${\\mathcal{P}}({\\mathcal{F}})$ . ", "page_idx": 18}, {"type": "text", "text": "According to Lemma 3, for any $\\epsilon>\\hat{\\Re}_{[c]\\times D}(\\mathcal{P}(\\mathcal{F}))$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{fat}_{\\epsilon}(\\mathcal{P}(\\mathcal{F}))\\leq\\frac{4n c\\hat{\\mathfrak{R}}_{[c]\\times D}^{2}(\\mathcal{P}(\\mathcal{F}))}{\\epsilon^{2}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then, combining with Lemma 4, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\log\\mathcal{N}_{\\infty}\\left(\\epsilon,\\mathcal{P}(\\mathcal{F}),[c]\\times D\\right)\\leq\\frac{4n c\\hat{\\Re}_{[c]\\times D}^{2}(\\mathcal{P}(\\mathcal{F}))}{\\epsilon^{2}}\\log\\frac{2e B n c}{\\epsilon}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Use inequality $\\epsilon>\\hat{\\Re}_{[c]\\times D}(\\mathcal{P}(\\mathcal{F}))$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\log\\mathcal{N}_{\\infty}(\\epsilon,\\mathcal{P}(\\mathcal{F}),[c]\\times D)\\leq\\frac{4n c\\hat{\\Re}_{[c]\\times D}^{2}(\\mathcal{P}(\\mathcal{F}))}{\\epsilon^{2}}\\log\\frac{2e B n c}{\\hat{\\Re}_{[c]\\times D}(\\mathcal{P}(\\mathcal{F}))}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Step 3: According to Assumption 1 in the main paper, we can obtain the lower bound of the worst-case Rademacher complexity $\\widetilde{\\mathfrak{R}}_{n c}(\\mathcal{P}(\\mathcal{F}))$ by the Khintchine-Kahane inequality with $p=1$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overline{{\\mathbf{S}}}_{\\mathbf{h}\\alpha}(P(\\mathcal{X}))}\\\\ &{=\\underset{\\{i,j=1,2\\}\\cup\\{i,j\\}\\cup\\{i,j\\}}{\\operatorname*{sup}}\\frac{\\overline{{\\mathbf{S}}}_{\\mathbf{h}\\alpha}}{\\alpha_{i}(P(\\mathcal{X}))}\\mathrm{e}^{\\mathrm{i}\\alpha(P(\\mathcal{X}))}}\\\\ &{=\\underset{\\{i,j=1,2\\}\\cup\\{i,j\\}\\cup\\{i,j\\}}{\\operatorname*{sup}}\\mathrm{e}^{\\mathrm{i}\\alpha(P(\\mathcal{X}))}\\mathrm{e}^{-\\mathrm{i}\\alpha(P(\\mathcal{X}))}\\frac{1}{\\alpha_{i}(Q_{i})}\\sum_{i=1,2\\}^{n}c_{i,j\\}(\\rho(\\alpha_{i}))\\Big]}\\\\ &{=\\underset{\\{i,j=1,2\\}\\cup\\{i,j\\}\\cup\\{i,j\\}}{\\operatorname*{sup}}\\frac{\\mathbf{E}_{\\alpha}}{\\alpha_{i}(P(\\mathcal{X}))}\\left[\\underset{\\mathcal{X}\\geq}{\\operatorname*{sup}}\\frac{1}{\\alpha_{i}}\\sum_{i=1}^{n}c_{i,j\\}(\\rho_{\\alpha_{i}})\\right]}\\\\ &{=\\underset{\\{i,j=1,2\\}\\cup\\{i,j\\}\\cup\\{i,j\\}\\cup\\{i,j\\}}{\\operatorname*{sup}}\\frac{\\mathbf{E}_{\\alpha}}{\\alpha_{i}(P(\\mathcal{X}))}\\left[\\underset{\\mathcal{X}\\geq}{\\operatorname*{sup}}\\left[\\underset{\\mathcal{X}\\geq}{\\operatorname*{sup}}\\ \\sum_{i=i,j\\geq1\\}^{n}c_{i,j\\}(\\rho_{\\alpha_{i}})\\zeta_{i,j}(\\rho_{\\alpha_{i}}))\\right]}\\\\ &{=\\underset{\\{i,j=1,2\\}\\cup\\{i,j\\}\\cup\\{i,j\\}\\cup\\{i,j\\}\\cup\\{i,j\\}\\cup\\{i,j\\}\\cup\\left[\\frac{N}{\\alpha_{i}}\\sum_{i=1}^{n}\\sum_{i=1}^{n}c_{i,j\\}(\\rho_{\\alpha_{i}})(\\rho_{\\alpha_{i}})(\\lambda_{j})\\right]}{\\operatorname*{sup}}\\right]}\\\\ &{=\\underset{\\{i,j=1,2\\}\\cup\\{i,j\\}\\cup\\{i,j\\}\\cup\\{i,j\\}\\cup\\{i,j\\}\\cup\\{i,j\\}\\cup\\left[\\frac{N}{\\alpha_{i}}\\sum_{i=1}^{n}\\sum_{i=1}^{n}c\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since $\\begin{array}{r}{\\|\\zeta_{j}(\\phi_{j}(\\pmb{x}_{i}))\\|_{2}\\leq A,\\operatorname{we}\\operatorname{set}\\operatorname*{sup}_{\\|\\zeta_{j}(\\phi_{j}(\\pmb{x}_{i}))\\|_{2}\\leq A:i\\in[n],j\\in[c]}\\frac{1}{n c}\\left[\\sum_{i=1}^{n}\\sum_{j=1}^{c}\\|\\zeta_{j}(\\phi_{j}(\\pmb{x}_{i}))\\|^{2}\\right]^{\\frac{1}{2}}=}\\end{array}$ $\\frac{A}{\\sqrt{n c}}$ . So, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\widetilde{\\mathfrak{R}}_{n c}(\\mathcal{P}(\\mathcal{F}))\\ge\\frac{\\Lambda A}{\\sqrt{2n c}}=\\frac{B}{\\sqrt{2n c}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Step 4: According to Lemma 5 and combined with the above steps, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~\\leq\\operatorname*{inf}_{n\\leq0}\\ \\Biggl(4\\alpha+\\frac{12}{\\sqrt{n}}\\int_{\\alpha}^{M}\\sqrt{\\log\\mathcal{N}_{\\infty}(\\epsilon,\\mathcal{L},D)}d\\epsilon\\Biggr)}\\\\ &{\\leq\\operatorname*{inf}_{n\\leq0}\\left(4\\alpha+\\frac{12}{\\sqrt{n}}\\int_{\\alpha}^{M}\\sqrt{\\log\\mathcal{N}_{\\infty}(\\frac{\\epsilon}{\\rho},\\mathcal{P}(\\mathcal{F}),[\\epsilon]\\times D)}d\\epsilon\\right)\\quad(\\mathrm{Use~inequality~}(6))}\\\\ &{\\leq\\operatorname*{inf}_{n\\leq0}\\left(4\\alpha+\\frac{12}{\\sqrt{n}}\\int_{\\alpha}^{M}\\sqrt{\\frac{4n{\\epsilon}\\rho^{2}\\widehat{\\mathcal{R}}_{[\\epsilon]\\times D}^{2}(\\mathcal{P}(\\mathcal{F}))}{\\epsilon^{2}}\\log\\frac{2e B n\\epsilon}{\\widehat{\\mathcal{R}}_{[\\epsilon]\\times D}(\\mathcal{P}(\\mathcal{F}))}}d\\epsilon\\right)\\quad(\\mathrm{Use~inequality~}(7))}\\\\ &{\\leq\\operatorname*{inf}_{n\\leq0}\\left(4\\alpha+\\frac{12}{\\sqrt{n}}\\int_{\\alpha}^{M}\\sqrt{\\frac{4n{\\epsilon}\\rho^{2}\\widehat{\\mathcal{R}}_{[\\epsilon]}^{2}(\\mathcal{P}(\\mathcal{F}))}{\\epsilon^{2}}\\log(2\\sqrt{2}e B n^{\\frac{3}{2}}\\epsilon^{\\frac{3}{2}})}d\\epsilon\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "(Use inequality (8) and the definition of the worst-case Rademacher complexity) ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\operatorname*{inf}\\Bigg(4\\alpha+12\\sqrt{2}\\rho\\sqrt{c\\widetilde{\\mathfrak{R}}_{n c}(\\mathcal{P}(\\mathcal{F}))\\log^{\\frac{1}{2}}(8e^{2}n^{3}c^{3})}\\int_{\\alpha}^{M}\\epsilon^{-1}d\\epsilon\\Bigg)}\\\\ &{\\le\\!12\\sqrt{2}\\rho\\sqrt{c\\widetilde{\\mathfrak{R}}_{n c}(\\mathcal{P}(\\mathcal{F}))+12\\sqrt{2}\\rho\\sqrt{c\\widetilde{\\mathfrak{R}}_{n c}(\\mathcal{P}(\\mathcal{F}))\\log^{\\frac{1}{2}}(8e^{2}n^{3}c^{3})}\\cdot\\log\\frac{M}{3\\sqrt{2}\\rho\\sqrt{c\\widetilde{\\mathfrak{R}}_{n c}(\\mathcal{P}(\\mathcal{F}))}}}\\\\ &{\\quad(\\mathrm{Chose}\\,\\alpha=3\\sqrt{2}\\rho\\sqrt{c\\widetilde{\\mathfrak{R}}_{n c}(\\mathcal{P}(\\mathcal{F}))})}\\\\ &{\\le\\!12\\sqrt{2}\\rho\\sqrt{c\\widetilde{\\mathfrak{R}}_{n c}(\\mathcal{P}(\\mathcal{F}))+12\\sqrt{2}\\rho\\sqrt{c\\widetilde{\\mathfrak{R}}_{n c}(\\mathcal{P}(\\mathcal{F}))\\log^{\\frac{1}{2}}(8e^{2}n^{3}c^{3})}\\cdot\\log\\frac{M\\sqrt{n}}{\\rho B}\\quad\\mathrm{(Us~inequality~(8))}}\\\\ &{=\\!12\\sqrt{2}\\rho\\sqrt{c\\widetilde{\\mathfrak{R}}_{n c}(\\mathcal{P}(\\mathcal{F}))\\left(1+\\log^{\\frac{1}{2}}(8e^{2}n^{3}c^{3})\\cdot\\log\\frac{M\\sqrt{n}}{\\rho B}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "A.3.3 Proof of Theorem 1 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We upper bound the worst-case Rademacher complexity $\\widetilde{\\mathfrak{R}}_{n c}(\\mathcal{P}(\\mathcal{F}))$ as the following: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{\\bar{W}}_{n}\\left({P}(F)\\right)=\\underset{|\\bar{\\mathcal{H}}()|\\leq n}{\\underbrace{\\operatorname{Memp}}}\\mathbb{E}_{\\alpha}\\left(\\mathcal{P}(F)\\right)}\\\\ &{=\\underset{|\\bar{\\mathcal{H}}()|\\leq n}{\\underbrace{\\operatorname{Memp}}}\\underset{|\\bar{\\mathcal{H}}()|\\leq n}{\\underbrace{\\operatorname{Memp}}}\\left(\\underset{|\\bar{\\mathcal{H}}()|\\leq n}{\\underbrace{\\operatorname{Memp}}}\\right)\\underset{|\\bar{\\mathcal{H}}()|\\leq n}{\\underbrace{\\frac{1}{\\sqrt{n}}}}\\sum_{i=1}^{n}\\underset{|\\bar{\\mathcal{H}}()|\\leq n}{\\underbrace{\\hat{C}}}\\left(f(x_{1})\\right)\\right|}\\\\ &{=\\underset{|\\bar{\\mathcal{H}}()|\\leq n}{\\underbrace{\\operatorname{Memp}}}\\underset{|\\bar{\\mathcal{H}}()|\\leq n}{\\underbrace{\\operatorname{Memp}}}\\left(\\underset{|\\bar{\\mathcal{H}}()|\\leq n}{\\underbrace{\\operatorname{Memp}}}\\right)\\underset{|\\bar{\\mathcal{H}}()|\\leq n}{\\underbrace{\\sum}}\\underset{|\\bar{\\mathcal{H}}()|\\leq n}{\\underbrace{\\sum}}\\left(\\underset{|\\bar{\\mathcal{H}}()|\\leq n}{\\underbrace{\\operatorname{Memp}}}\\right)}\\\\ &{=\\underset{|\\bar{\\mathcal{H}}()|\\leq n}{\\underbrace{\\operatorname{Memp}}}\\underset{|\\bar{\\mathcal{H}}()|\\leq n}{\\underbrace{\\operatorname{Memp}}}\\left(\\underset{|\\bar{\\mathcal{H}}()|\\leq n}{\\underbrace{\\operatorname{Memp}}}\\right)\\underset{|\\bar{\\mathcal{H}}()|\\leq n}{\\underbrace{\\sum}}\\left(\\underset{|\\bar{\\mathcal{H}}()|\\leq n}{\\underbrace{\\operatorname{Memp}}}\\right)\\underset{|\\bar{\\mathcal{H}}()|\\leq n}{\\underbrace{\\sum}}}\\\\ &{=\\underset{|\\bar{\\mathcal{H}}()|\\leq n}{\\underbrace{\\operatorname{Memp}}}\\underset{|\\bar{\\mathcal{H}}()|\\leq n}{\\underbrace{\\operatorname{Memp}}}\\left(\\underset{|\\bar{\\mathcal{H}}()|\\leq n}{\\underbrace{\\operatorname{Memp}}}\\right)\\underset{|\\bar{\\mathcal{H}}()|\\leq n}{\\underbrace{\\sum}}\\left(\\underset{|\\bar{\\mathcal{H}}()|\\leq n}{\\underbrace{\\operatorname{C}}}\\left(f_{1}(\\phi_{1})\\right)\\right|\\right)}\\\\ &{\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\Re}_{D}(\\mathcal{F})\\leq\\!12\\sqrt{2}\\rho\\sqrt{c}\\widetilde{\\Re}_{n c}(\\mathcal{P}(\\mathcal{F}))\\left(1+\\log^{\\frac{1}{2}}(8e^{2}n^{3}c^{3})\\cdot\\log\\frac{M\\sqrt{n}}{\\rho B}\\right)}\\\\ &{\\qquad\\quad\\leq\\!\\frac{12\\sqrt{2}\\rho\\Lambda A\\left(1+\\log^{\\frac{1}{2}}(8e^{2}n^{3}c^{3})\\cdot\\log\\frac{M\\sqrt{n}}{\\rho B}\\right)}{\\sqrt{n}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Combining with (5), then ", "page_idx": 20}, {"type": "equation", "text": "$$\nR(f)\\leq\\widehat{R}_{D}(f)+\\frac{24\\sqrt{2}\\rho\\Lambda A\\left(1+\\log^{\\frac{1}{2}}(8e^{2}n^{3}c^{3})\\cdot\\log\\frac{M\\sqrt{n}}{\\rho B}\\right)}{\\sqrt{n}}+3M\\sqrt{\\frac{\\log\\frac{2}{\\delta}}{2n}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "A.4 Generalization Bounds for Typical LSRL Methods ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "A.4.1 Proof of Theorem 2 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The generalization analysis of LIFT involves the generalization analysis for $k$ -means clustering. According to the definitions of $k$ -means clustering in Subection 5.1, we have the empirical Rademacher complexity of a loss function space associated with the vector-valued function class $\\mathcal{G}$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\hat{\\Re}_{D}(\\mathcal{L}_{c l u}\\circ\\mathcal{G})=\\mathbb{E}_{\\epsilon}\\left[\\operatorname*{sup}_{g\\in\\mathcal{G}}\\frac{1}{n(n-1)}\\sum_{\\substack{i,j=1,i\\neq j}}^{n}\\epsilon_{i}\\ell_{c l u}(\\pmb{g}(\\pmb{x}_{i},\\pmb{x}_{j}))\\right].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Rademacher complexity has proved to be a powerful data-dependent measure of hypothesis space complexity. However, since the $k$ -means clustering framework involves pairwise functions, a sequence of pairs of i.i.d. individual observation in $k$ -means clustering is no longer independent, which makes standard techniques in the i.i.d case for traditional Rademacher complexity inapplicable for $k$ - means clustering. We convert the non-sum-of-i.i.d pairwise function to a sum-of-i.i.d form by using permutations in U-process [Cl\u00e9men\u00e7on et al., 2008]. ", "page_idx": 20}, {"type": "text", "text": "We first proof the following lemma: ", "page_idx": 20}, {"type": "text", "text": "Lemma 6. Let $q_{\\tau}:\\mathcal{X}\\times\\mathcal{X}\\mapsto\\mathbb{R}$ be real-valued functions indexed by $\\tau\\in T$ where $T$ is some set. If $\\pmb{x}_{1},\\dots,\\pmb{x}_{s}$ and $\\pmb{x}_{1}^{\\prime},\\ldots,\\pmb{x}_{t}^{\\prime}$ are i.i.d., $r=\\operatorname*{min}\\{s,t\\}$ , then for any convex non-decreasing function $\\psi$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}\\psi\\left(\\operatorname*{sup}_{\\tau\\in T}\\frac{1}{s t}\\sum_{i=1}^{s}\\sum_{j=1}^{t}q_{\\tau}\\left(\\pmb{x}_{i},\\pmb{x}_{j}^{\\prime}\\right)\\right)\\leq\\mathbb{E}\\psi\\left(\\operatorname*{sup}_{\\tau\\in T}\\frac{1}{r}\\sum_{i=1}^{r}q_{\\tau}\\left(\\pmb{x}_{i},\\pmb{x}_{i}^{\\prime}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. The proof of this lemma is inspired by [Cl\u00e9men\u00e7on et al., 2008]. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\psi\\left(\\underset{\\tau\\in T}{\\operatorname*{sup}}\\frac{1}{n}\\sum_{u=1}^{s}\\ell_{\\tau}\\left(x_{t},x_{t}^{\\prime}\\right)\\right)}\\\\ &{=\\!\\mathbb{E}\\psi\\left(\\underset{\\tau\\in T}{\\operatorname*{sup}}\\frac{1}{n}\\sum_{u=1}^{s}\\frac{1}{t}\\sum_{w=t}^{\\frac{1}{r}}\\frac{1}{r}\\sum_{\\tau=1}^{r}\\left(x_{\\tau(i)},x_{\\pi(i)}^{\\prime}\\right)\\right)}\\\\ &{\\leq\\!\\mathbb{E}\\psi\\left(\\frac{1}{n}\\sum_{u=1}^{r}\\frac{1}{t}\\sum_{w=\\tau}^{\\infty}\\frac{1}{r}\\frac{1}{t-1}\\tau_{\\tau}\\left(x_{\\tau(i)},x_{\\pi(i)}^{\\prime}\\right)\\right)\\quad\\mathrm{(fe~is~nondecreasing~})}\\\\ &{\\leq\\!\\frac{1}{s}\\!\\sum_{w=\\tau}^{\\infty}\\!\\!\\!\\!\\!\\frac{1}{t}\\sum_{w=\\tau}^{\\infty}\\!\\psi\\left(\\underset{\\tau\\in T}{\\operatorname*{sup}}\\frac{1}{r}\\sum_{u=1}^{r}\\left(x_{\\tau(i)},x_{\\pi(i)}^{\\prime}\\right)\\right)\\quad\\mathrm{(fensen\\'s~inequality)}}\\\\ &{=\\!\\mathbb{E}\\psi\\left(\\underset{\\tau\\in T}{\\operatorname*{sup}}\\frac{1}{r}\\sum_{w=\\tau}^{\\infty}q_{r}\\left(x_{\\tau},x_{t}^{\\prime}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "According to Lemma 6, $\\hat{\\mathfrak{R}}_{D}(\\mathcal{L}_{c l u}\\circ\\mathcal{G})$ can be bounded by ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\hat{\\Re}_{D^{\\prime}}(\\mathcal{L}_{c l u}\\circ\\mathcal{G}):=\\mathbb{E}_{\\epsilon}\\left[\\operatorname*{sup}_{g\\in\\mathcal{G}}\\frac{1}{\\lfloor\\frac{n}{2}\\rfloor}\\sum_{i=1}^{\\lfloor\\frac{n}{2}\\rfloor}\\epsilon_{i}\\ell_{c l u}(g(\\boldsymbol{x}_{i},\\boldsymbol{x}_{i+\\lfloor\\frac{n}{2}\\rfloor}))\\right],\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where each $\\epsilon_{i}$ is an independent Rademacher random variable. ", "page_idx": 21}, {"type": "text", "text": "In order to obtain tighter generalization bounds for $k$ -means clustering, we develop the following novel vector-contraction inequality: ", "page_idx": 21}, {"type": "text", "text": "Lemma 7. Let $\\mathcal{G}$ be a vector-valued function class of $k$ -means clustering defined in Subection 5.1. Given a dataset $D$ of size $n$ . Then, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\Re}_{D}(\\mathcal{L}_{c l u}\\circ\\mathcal{G})\\leq12\\sqrt{K}\\operatorname*{max}_{k}\\mathfrak{F}_{\\lfloor\\frac{n}{2}\\rfloor}(\\mathcal{G}_{k})\\left(1+\\log^{\\frac{1}{2}}(e^{2}n^{3})\\cdot\\log\\frac{M}{\\mathfrak{F}_{\\lfloor\\frac{n}{2}\\rfloor}(\\mathcal{G}_{k})}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\widetilde{\\mathfrak{R}}_{\\lfloor\\frac{n}{2}\\rfloor}\\left(\\mathcal{G}_{k}\\right)$ is the worst-case Rademacher complexity, $\\mathcal{G}_{k}$ is the restriction of the function class along the $k$ -th coordinate, $g_{k}\\in\\mathcal{G}_{k}$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. We first introduce the following lemma: ", "page_idx": 21}, {"type": "text", "text": "Lemma 8 (Lemma 1 in [Foster and Rakhlin, 2019]). Let $\\mathcal{F}\\subseteq\\left\\{f:\\mathcal{X}\\to\\mathbb{R}^{K}\\right\\}$ , and let $\\phi_{1},\\ldots,\\phi_{n}$ each be L Lipschitz with respect to the $\\ell_{\\infty}$ norm. For any $D$ with a finite sample of size $n$ and $\\epsilon>0$ , we have that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\log\\mathcal{N}_{2}\\left(\\epsilon,\\phi\\circ\\mathcal{F},D\\right)\\leq K\\operatorname*{max}_{k}\\log\\mathcal{N}_{\\infty}\\left(\\frac{\\epsilon}{L},\\mathcal{F}_{k},D\\right),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\mathcal{F}_{k}$ is the restriction of the function class along the $k$ -th coordinate, $f_{k}\\in\\mathcal{F}_{k}$ , $k\\in[K]$ . ", "page_idx": 21}, {"type": "text", "text": "The empirical $\\ell_{\\infty}$ norm covering number of $\\mathcal{G}_{k}$ can be bounded by the fat-shattering dimension, and the fat-shattering dimension can be bounded by the worst-case Rademacher complexity of $\\mathcal{G}_{k}$ . Combined with the above steps, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\hat{\\Re}_{D}(\\mathcal{L}_{c l u}\\circ\\mathcal{G})\\leq\\hat{\\Re}_{D^{\\prime}}(\\mathcal{L}_{c l u}\\circ\\mathcal{G})}\\\\ &{\\leq\\underset{\\alpha>0}{\\operatorname*{inf}}\\left(4\\alpha+\\frac{12}{\\sqrt{n}}\\int_{\\alpha}^{M}\\sqrt{\\log{\\mathcal{N}_{2}(\\epsilon,\\mathcal{L}_{c l u}\\circ\\mathcal{G},D^{\\prime})}}d\\epsilon\\right)\\quad\\mathrm{(Use~Lemma~5~)}}\\\\ &{\\leq\\underset{\\alpha>0}{\\operatorname*{inf}}\\left(4\\alpha+\\frac{12}{\\sqrt{n}}\\int_{\\alpha}^{M}\\sqrt{K\\operatorname*{max}_{k}\\log{\\mathcal{N}_{\\infty}(\\epsilon,\\mathcal{G}_{k},D^{\\prime})}}d\\epsilon\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "(Use inequality (10) and $\\ell_{c l u}(\\cdot)$ is 1-Lipschitz w.r.t. $\\ell_{\\infty}$ norm for $k$ -means clustering) ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\underset{\\alpha>0}{\\operatorname*{inf}}\\left(4\\alpha+\\frac{12}{\\sqrt{n}}\\int_{\\alpha}^{M}\\sqrt{K\\operatorname*{max}\\mathbb{f a t}_{\\varepsilon}(\\mathcal{G}_{k})\\log\\frac{2e B\\left[\\frac{n}{2}\\right]}{\\epsilon}}d\\epsilon\\right)\\quad\\mathrm{~(Use~Lemma~4)~}}\\\\ &{\\leq\\underset{\\alpha>0}{\\operatorname*{inf}}\\left(4\\alpha+\\frac{12}{\\sqrt{n}}\\int_{\\alpha}^{M}\\sqrt{K\\operatorname*{max}\\frac{4\\left\\lfloor\\frac{n}{2}\\right\\rfloor\\hat{\\mathbb{R}}_{D^{\\prime}}^{2}(\\mathcal{G}_{k})}{\\epsilon^{2}}\\log\\frac{2e G\\left\\lfloor\\frac{n}{2}\\right\\rfloor}{\\hat{\\mathbb{R}}_{D^{\\prime}}(\\mathcal{G}_{k})}}d\\epsilon\\right)\\quad\\mathrm{~(Use~Lemma~3)~}}\\\\ &{\\leq\\underset{\\alpha>0}{\\operatorname*{inf}}\\left(4\\alpha+\\frac{12}{\\sqrt{n}}\\int_{\\alpha}^{M}\\sqrt{K\\operatorname*{max}\\frac{4\\left\\lfloor\\frac{n}{2}\\right\\rfloor\\sqrt{\\mathfrak{F}}_{\\frac{1}{2}}^{2}(\\mathcal{G}_{k})}{\\epsilon^{2}}\\log\\frac{2e G\\left\\lfloor\\frac{n}{2}\\right\\rfloor}{\\mathfrak{F}_{\\lfloor\\frac{n}{2}\\rfloor}(\\mathcal{G}_{k})}}d\\epsilon\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "(The definition of the worst-case Rademacher complexity) ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\leq\\operatorname*{inf}_{\\alpha>0}\\left(4\\alpha+\\frac{12}{\\sqrt{n}}\\int_{\\alpha}^{M}\\sqrt{\\frac{2n K\\operatorname*{max}_{k}\\widetilde{\\mathfrak{N}}_{\\lfloor\\frac{n}{2}\\rfloor}^{2}(\\mathcal{G}_{k})}{\\epsilon^{2}}\\log(e n^{\\frac{3}{2}})}d\\epsilon\\right)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "(Use the similar technique to the proof in inequality (8), the lower bound of $\\widetilde{\\Re}_{\\lfloor\\frac{n}{2}\\rfloor}(\\mathcal{G}_{k})\\geq\\frac{G}{\\sqrt{2\\lfloor\\frac{n}{2}\\rfloor}}\\;)$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\underset{\\alpha>0}{\\operatorname*{inf}}\\left(4\\alpha+12\\sqrt{K}\\operatorname*{max}_{k}\\widetilde{\\Re}_{\\lfloor\\frac{n}{2}\\rfloor}(\\mathcal{G}_{k})\\log^{\\frac{1}{2}}(e^{2}n^{3})\\int_{\\alpha}^{M}\\epsilon^{-1}d\\epsilon\\right)}\\\\ &{\\leq12\\sqrt{K}\\operatorname*{max}_{k}\\widetilde{\\Re}_{\\lfloor\\frac{n}{2}\\rfloor}(\\mathcal{G}_{k})+12\\sqrt{K}\\operatorname*{max}_{k}\\widetilde{\\Re}_{\\lfloor\\frac{n}{2}\\rfloor}(\\mathcal{G}_{k})\\log^{\\frac{1}{2}}(e^{2}n^{3})\\cdot\\log\\frac{M}{3\\sqrt{K}\\operatorname*{max}_{k}\\widetilde{\\Re}_{\\lfloor\\frac{n}{2}\\rfloor}(\\mathcal{G}_{k})}}\\\\ &{\\ \\ \\ (\\mathrm{Chose}\\ \\alpha=3\\sqrt{K}\\operatorname*{max}_{k}\\widetilde{\\Re}_{\\lfloor\\frac{n}{2}\\rfloor}(\\mathcal{G}_{k}))}\\\\ &{=12\\sqrt{K}\\operatorname*{max}_{k}\\widetilde{\\Re}_{\\lfloor\\frac{n}{2}\\rfloor}(\\mathcal{G}_{k})\\left(1+\\log^{\\frac{1}{2}}(e^{2}n^{3})\\cdot\\log\\frac{M}{\\widetilde{\\Re}_{\\lfloor\\frac{n}{2}\\rfloor}(\\mathcal{G}_{k})}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since $k$ -means clustering-based LSRL method is two-stage, the $k$ -means clustering is used to generate label-specific representations in the first stage, and the second stage is conventional multi-label learning. Therefore, the corresponding whole function class is actually denoted as $\\mathcal{H}=\\mathcal{F}+\\mathcal{L}_{c l u}\\circ\\mathcal{G}$ . Since $\\tilde{\\mathfrak{R}}_{n c}(\\mathcal{P}(\\mathcal{H}))\\leq\\tilde{\\mathfrak{R}}_{n c}(\\mathcal{P}(\\bar{\\mathcal{F}}))+\\tilde{\\mathfrak{R}}_{n c}(\\mathcal{P}(\\mathcal{L}_{c l u}\\circ\\mathcal{G})$ [Bartlett and Mendelson, 2002], with Lemma 1, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathfrak{F}_{D}(\\mathcal{L}\\circ\\mathcal{H})\\leq12\\sqrt{2}\\rho\\sqrt{c}\\left(\\widetilde{\\mathfrak{R}}_{n c}(\\mathcal{P}(\\mathcal{F}))+\\widetilde{\\mathfrak{R}}_{n c}(\\mathcal{P}(\\mathcal{L}_{c l u}\\circ\\mathcal{G}))\\right)\\left(1+\\log^{\\frac{1}{2}}(8e^{2}n^{3}c^{3})\\cdot\\log\\frac{M\\sqrt{n}}{\\mu B}\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "According to Lemma 7, we have that the worst-case Rademacher complexity of the loss function space associated with $\\mathcal{P}(\\mathcal{G})$ can be bounded by the worst-case Rademacher complexity of the restriction of the function class along each coordinate $\\mathcal{P}(\\mathcal{G}_{k})$ . Hence, for $k$ -means clustering-based LSRL method, Lemma 7 involves the upper and lower bounds of $\\widetilde{\\mathfrak{R}}_{\\lfloor\\frac{n c}{2}\\rfloor}(\\mathcal{P}(\\mathcal{G}_{k}))$ . ", "page_idx": 22}, {"type": "text", "text": "We then obtain the lower bound of the worst-case Rademacher complexity $\\widetilde{\\mathfrak{R}}_{\\lfloor\\frac{n c}{2}\\rfloor}(\\mathcal{P}(\\mathcal{G}_{k}))$ by the Khintchine-Kahane inequality with $p=1$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Psi_{1}|\\psi_{1}|\\mathcal{P}(\\ell_{2}|_{3})=\\underset{\\left\\{|\\gamma^{*}(\\alpha,\\pi_{1}^{*})|\\leq1\\right\\}}{\\operatorname*{sup}}\\ \\frac{\\Psi_{1}|\\psi_{1}|_{\\ell-1}}{\\|\\gamma^{*}(\\ell_{1})\\|_{L^{1}}}\\mathcal{P}(\\ell_{2}|_{3})}\\\\ &{=\\underset{\\left\\{|\\gamma^{*}(\\alpha,\\pi_{1}^{*})|\\leq1\\right\\}}{\\operatorname*{sup}}\\ \\mathbb{E}_{\\alpha}\\left[\\gamma(\\mu(\\alpha,\\alpha_{1}^{*}|_{\\ell_{1}}))\\mathcal{P}(\\psi_{0}|_{\\ell}|_{3})\\frac{\\frac{1}{\\sqrt{3}}}{\\mu}\\sum_{i=1}^{1}c_{j}\\psi_{0}|_{\\mathcal{P}(\\ell_{1}}(\\alpha_{1},x_{i+1};|\\boldsymbol{j}|))\\right]}\\\\ &{=\\underset{\\left\\{|\\gamma^{*}(\\alpha,\\pi_{1}^{*})|\\leq1\\right\\}}{\\operatorname*{sup}}\\ \\mathbb{E}_{\\alpha}\\left[\\frac{1}{\\mu(\\alpha_{1}^{*}|_{\\ell_{2}}|_{L^{1}})}\\underset{j\\in\\frac{1}{3}}{\\prod_{s=1}^{1}}\\underset{\\beta_{1}^{2}}{\\sum_{i\\in\\mathcal{I}_{s}}}\\ (x_{i},x_{i+1};|\\boldsymbol{j}|)\\right]}\\\\ &{=\\underset{\\left\\{|\\gamma^{*}(\\alpha,\\alpha_{1}^{*})|\\leq1\\right\\}}{\\operatorname*{sup}}\\ \\mathbb{E}_{\\alpha}\\left[\\frac{1}{\\mu(\\alpha_{1}^{*}|_{\\ell_{2}}|_{L^{1}})}\\underset{j\\in\\frac{1}{3}}{\\prod_{s=1}^{1}}\\underset{j\\in\\mathcal{I}_{s}}{\\operatorname*{sup}}\\ \\frac{1}{\\|\\gamma^{*}(\\ell_{1},x_{i+1};|\\boldsymbol{j}|)\\right]}}\\\\ &{=\\underset{\\left\\{|\\gamma^{*}(\\alpha,\\alpha_{1}^{*})|\\leq1\\right\\}}{\\operatorname*{sup}}\\ \\frac{\\mathbb{E}_{\\alpha}}{\\alpha_{1}}\\left[\\nu(w,\\alpha_{1+1}^{*};|\\boldsymbol{j\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathop{\\mathbb{1}}[Z_{k}^{j}(x_{i},x_{i+\\lfloor\\frac{n}{2}\\rfloor})\\mathop{\\mathbb{1}}]\\le1,\\operatorname{we}\\operatorname{set}\\operatorname*{sup}_{\\mathop{\\mathbb{1}}Z_{k}^{j}(x_{i},x_{i+\\lfloor\\frac{n}{2}\\rfloor})\\mathop{\\mathbb{1}}\\le1}\\frac{1}{\\mathop{\\mathbb{1}}_{2}^{n}}\\left[\\sum_{i=1}^{\\lfloor\\frac{n}{2}\\rfloor}\\sum_{j=1}^{c}\\|Z_{k}^{j}(x_{i},x_{i+\\lfloor\\frac{n}{2}\\rfloor})\\|^{2}\\right]^{\\frac{1}{2}}=}\\\\ &{\\frac{-}{\\mathop{\\mathbb{1}}c}.\\operatorname{So},\\tilde{\\Re}_{\\lfloor\\frac{n}{2}\\rfloor}(\\mathcal{P}(\\mathcal{G}_{k})))\\ge\\frac{4R^{2}}{\\sqrt{n c}}=\\frac{G}{\\sqrt{n c}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Then, we replace the lower bound of $\\widetilde{\\mathfrak{R}}_{\\lfloor\\frac{n}{2}\\rfloor}(\\mathcal{G}_{k})$ in the proof of Lemma 7 with the lower bound of $\\widetilde{\\mathfrak{R}}_{\\lfloor\\frac{n c}{2}\\rfloor}(\\mathcal{P}(\\mathcal{G}_{k})))$ , and we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widetilde{\\mathfrak{R}}_{n c}(\\mathcal{P}(\\mathcal{L}_{c l u}\\circ\\mathcal{G}))\\leq12\\sqrt{K}\\operatorname*{max}_{k}\\widetilde{\\mathfrak{R}}_{\\lfloor\\frac{n c}{2}\\rfloor}(\\mathcal{P}(\\mathcal{G}_{k})))\\left(1+\\log^{\\frac{1}{2}}(e^{2}n^{3}c^{3})\\cdot\\log\\frac{M\\sqrt{n c}}{G}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We then upper bound the worst-case Rademacher complexity $\\widetilde{\\mathfrak{R}}_{\\lfloor\\frac{n c}{2}\\rfloor}(\\mathcal{P}(\\mathcal{G}_{k}))$ as the following: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{B}_{\\mathbb{R}\\times\\mathbb{R}^{n}}\\mathbb{P}(\\theta_{t})}\\\\ &{=\\underset{(u,v,w,w,w)}{\\operatorname{supsut}}\\ \\mathbb{B}_{\\theta_{t}\\times\\mathcal{R}^{n}}(\\mathcal{P}(\\theta_{t}))}\\\\ &{=\\underset{(u,v,w,w,w)}{\\operatorname{supsut}}\\ \\mathbb{B}_{\\theta_{t}\\times\\mathcal{R}^{n}}\\Bigg[\\frac{1}{\\Theta_{t}}\\sum_{i=1}^{n}\\sum_{\\underline{{i}}}\\frac{1}{\\Theta_{t}}\\exp(\\theta_{t}(u,v,w_{i+1}))\\Bigg]}\\\\ &{\\stackrel{(d)\\,\\,,\\,\\,\\exp}{\\operatorname{supsut}}\\ \\mathbb{B}_{\\theta_{t}\\times\\mathcal{R}^{n}}\\Bigg[\\frac{1}{\\Theta_{t}}\\sum_{i=1}^{n}\\sum_{\\underline{{i}}}\\sum_{\\underline{{i}}}\\omega_{t}(u,v,w_{i+1}))\\Bigg]}\\\\ &{=\\underset{(u,v,w,w,w)}{\\operatorname{supsut}}\\ \\mathbb{B}_{\\theta_{t}\\times\\mathcal{R}^{n}}\\Bigg[\\frac{1}{\\Theta_{t}}\\sum_{i=1}^{n}\\sum_{\\underline{{i}}}\\frac{1}{\\Theta_{t}}\\exp(u,v,w_{i+1})\\Bigg]}\\\\ &{\\stackrel{(d)\\,\\,\\,\\,\\exp}{\\operatorname{supsut}}\\ \\mathbb{B}_{\\theta_{t}\\times\\mathcal{R}^{n}}\\Bigg[\\frac{1}{\\Theta_{t}}\\sum_{i=1}^{n}\\sum_{\\underline{{i}}}\\omega_{t}(u,v,w_{i+1}))\\Bigg]}\\\\ &{=\\underset{(u,v,w,w_{i+1})}{\\operatorname{supsut}}\\ \\mathbb{E}_{\\theta_{t}\\times\\mathcal{R}^{n}}\\Bigg[\\underset{(u,v,w,w_{i+1})}{\\operatorname{supsut}}\\ \\sum_{i=1}^{n}\\sum_{\\underline{{i}}}\\omega_{t}(v,w_{i+1}))\\mathbb{Z}[(v,w_{i+1})]\\ Z[(w_{t},w_{i+1}))\\Bigg]}\\\\ &{\\stackrel{(d)\\,\\,\\,\\,\\,\\exp}{\\operatorname{supsut}}\\ \\mathbb{B}_{\\theta_{t}\\times\\mathcal{R}^{n}}\\Bigg[\\frac{1}{\\Theta_{t}}\\sum_{i=1}^{n}\\sum \n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Hence, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\widetilde{\\mathfrak{R}}_{n c}(\\mathcal{P}(\\mathcal{L}_{c l u}\\circ\\mathcal{G}))\\leq\\frac{24\\sqrt{K G}}{\\sqrt{n c}}\\left(1+\\log^{\\frac{1}{2}}(e^{2}n^{3}c^{3})\\cdot\\log\\frac{M\\sqrt{n c}}{G}\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Use the similar technique to the proof of the inequality above, the upper bound of $\\widetilde{\\mathfrak{R}}_{n c}(\\mathcal{P}(\\mathcal{F}))$ is $\\frac{2\\kappa\\Lambda\\sqrt{K}R}{\\sqrt{n c}}$ \u221a, since $\\begin{array}{r}{\\|\\phi_{j}(\\pmb{x})\\|\\,=\\,\\sqrt{\\sum_{k=1}^{K}(d(\\pmb{x},c_{k}^{j}))^{2}}\\,\\le\\,\\sqrt{K}\\operatorname*{max}_{k}\\|\\pmb{x}-c_{k}^{j}\\|\\,\\le\\,\\sqrt{K}\\operatorname*{max}_{k}(\\|\\pmb{x}\\|+\\pmb{x})\\,,}\\end{array}$ $\\lVert c_{k}^{j}\\rVert)\\leq2\\sqrt{K}R$ . ", "page_idx": 23}, {"type": "text", "text": "Finally, combining with the above inequalities and (5), we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R(f)\\leq\\widehat{R}_{D}(f)+3M\\sqrt{\\frac{\\log\\frac{2}{\\delta}}{2n}}+\\frac{48\\sqrt{2}\\rho\\kappa\\Lambda\\sqrt{K}R\\left(1+\\log^{\\frac{1}{2}}(8e^{2}n^{3}c^{3})\\cdot\\log\\frac{M\\sqrt{n}}{\\mu B}\\right)}{\\sqrt{n}}}\\\\ &{\\qquad+\\frac{24^{2}\\sqrt{2}\\rho\\sqrt{K}G}{\\sqrt{n}}\\left(1+\\log^{\\frac{1}{2}}(e^{2}n^{3}c^{3})\\cdot\\log\\frac{M\\sqrt{n c}}{G}\\right)\\left(1+\\log^{\\frac{1}{2}}(8e^{2}n^{3}c^{3})\\cdot\\log\\frac{M\\sqrt{n}}{\\mu B}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "A.4.2 Proof of Proposition 2 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "First, we prove that the squared loss is 1 pseudo-Lipschitz of order 2. ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left|\\ell_{b}\\left(y_{j}f_{j}(x)\\right)-\\ell_{b}\\left(y_{j}f_{j}^{\\prime}(x)\\right)\\right|}\\\\ &{=\\left|\\left(y_{j}-f_{j}(x)\\right)^{2}-\\left(y_{j}-f_{j}^{\\prime}(x)\\right)^{2}\\right|}\\\\ &{=\\left|\\left(y_{j}-f_{j}(x)\\right)+\\left(y_{j}-f_{j}^{\\prime}(x)\\right)\\right|\\cdot\\left|\\left(y_{j}-f_{j}(x)\\right)-\\left(y_{j}-f_{j}^{\\prime}(x)\\right)\\right|}\\\\ &{\\le\\left(|y_{j}-f_{j}(x)|+\\left|y_{j}-f_{j}^{\\prime}(x)\\right|\\right)\\cdot\\left|\\left(y_{j}-f_{j}(x)\\right)-\\left(y_{j}-f_{j}^{\\prime}(x)\\right)\\right|}\\\\ &{\\le\\left(1+|y_{j}-f_{j}(x)|+\\left|y_{j}-f_{j}^{\\prime}(x)\\right|\\right)\\cdot\\left|\\left(y_{j}-f_{j}(x)\\right)-\\left(y_{j}-f_{j}^{\\prime}(x)\\right)\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "According to the definition of the pseudo-Lipschitz function, the squared loss is 1 pseudo-Lipschitz of order 2. Then, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left|\\ell_{b}\\left(y_{j}f_{j}(\\pmb{x})\\right)-\\ell_{b}\\left(y_{j}f_{j}^{\\prime}(\\pmb{x})\\right)\\right|}\\\\ &{\\le\\left(1+|y_{j}-f_{j}(\\pmb{x})|+\\left|y_{j}-f_{j}^{\\prime}(\\pmb{x})\\right|\\right)\\cdot\\left|\\left(y_{j}-f_{j}(\\pmb{x})\\right)-\\left(y_{j}-f_{j}^{\\prime}(\\pmb{x})\\right)\\right|}\\\\ &{\\le\\left(1+2\\,|y_{j}|+|f_{j}(\\pmb{x})|+\\left|f_{j}^{\\prime}(\\pmb{x})\\right|\\right)\\left|f_{j}(\\pmb{x})-f_{j}^{\\prime}(\\pmb{x})\\right|}\\\\ &{\\le(3+2B)\\left|f_{j}(\\pmb{x})-f_{j}^{\\prime}(\\pmb{x})\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Second, we prove that the surrogate Decomposable Loss is $(3+2B)c$ -Lipschitz continuous with respect to the $\\ell_{\\infty}$ norm if the base loss $\\ell_{b}$ is the squared loss. ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad|\\ell_{D}(f(x),y)-\\ell_{D}\\left(f^{\\prime}(x),y\\right)|}\\\\ &{=\\left|\\displaystyle\\sum_{j=1}^{c}\\ell_{b}\\left(y_{j}f_{j}(x)\\right)-\\displaystyle\\sum_{j=1}^{c}\\ell_{b}\\left(y_{j}f_{j}^{\\prime}(x)\\right)\\right|}\\\\ &{=\\displaystyle\\sum_{j=1}^{c}|\\ell_{b}\\left(y_{j}f_{j}(x)\\right)-\\ell_{b}\\left(y_{j}f_{j}^{\\prime}(x)\\right)|}\\\\ &{\\leq\\displaystyle\\sum_{j=1}^{c}(3+2B)\\left|f_{j}(x)-f_{j}^{\\prime}(x)\\right|}\\\\ &{\\leq\\!c(3+2B)\\operatorname*{max}\\left|f_{j}(x)-f_{j}^{\\prime}(x)\\right|}\\\\ &{=\\!(3+2B)c\\|f(x)-f^{\\prime}(x)\\|_{\\infty}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "A.4.3 Proof of Theorem 3 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Compared with the function class of LSRL where each label corresponds to a Lasso, the function class of LLSF introduces the additional sharing constraint. In fact, the introduction of the sharing constraint reduces the complexity of the function class compared with the original class. Then, the complexity for the function class of LLSF can be bounded by the complexity of the function class of LSRL where each label corresponds to a Lasso. Hence, the complexity analysis of the function class of LLSF can be converted into giving the bound of the Rademacher complexity of the LSRL function class where each label corresponds to a Lasso. ", "page_idx": 24}, {"type": "text", "text": "According to the definition, the function class of LSRL where each label corresponds to a Lasso means that in the class of LSRL defined by (1), the base loss $\\ell_{b}$ is the squared loss, the nonlinear mappings $\\zeta(\\cdot)$ and $\\phi(\\cdot)$ are both identity transformations for any $j\\in[c]$ , and the constraint $\\alpha(w)$ is $\\|\\pmb{w}_{j}\\|_{1}\\leq\\Lambda$ for any $j\\in[c]$ . The proof process is similar to Lemma 1 and Theorem 1, but the upper bound of the worst-case Rademacher complexity $\\widetilde{\\mathfrak{R}}_{n c}(\\mathcal{P}(\\mathcal{F}))$ here is different from Theorem 1. According to the above definitions, we have ", "page_idx": 24}, {"type": "text", "text": "$\\begin{array}{r l}&{\\tilde{\\mathbf{E}}_{\\mathbf{u}}=\\{\\mathcal{P}(\\mathbf{E})\\}}\\\\ &{=\\phantom{\\sum_{\\mathbf{u}}}\\rho_{0}\\cdots\\mathbf{u}}\\\\ &{=\\phantom{\\sum_{\\mathbf{u}}}\\rho_{0};\\quad\\mathrm{loss}\\quad\\mathbf{u},(\\rho(\\mathbf{E}))}\\\\ &{\\tilde{\\mathbf{E}}_{\\{u,v\\},\\mathbf{u}}^{[1]}[\\phi_{0,\\mathbf{E}}^{[2]},~\\hat{\\mathbf{E}}_{\\mathbf{u}}^{[1]}]\\times\\frac{1}{\\sqrt{2}}\\sum_{\\mathbf{u},\\mathbf{u}^{\\prime}}^{[1]}\\rho_{0}(\\mathbf{E})\\Biggr\\}}\\\\ &{\\tilde{\\mathbf{E}}_{\\{u,v\\},\\mathbf{u\\},\\mathbf{u}^{\\prime}}^{[2]}=\\frac{1}{\\sqrt{2}}\\sum_{\\mathbf{u},\\mathbf{u}^{\\prime}}\\sum_{\\{u,v\\}\\in\\mathcal{E}_{r}}\\frac{1}{\\sqrt{2}}\\sum_{\\mathbf{u}^{\\prime}\\in\\mathcal{E}_{r}}\\rho_{0}(\\mathbf{E}_{u,v})\\Biggr\\}}\\\\ &{\\tilde{\\mathbf{E}}_{\\{u,v\\},\\mathbf{u\\},\\mathbf{u}^{\\prime}}^{[3]}=\\frac{\\sum_{\\mathbf{u}^{\\prime}}\\rho_{0}}{\\sqrt{2}}\\sum_{\\mathbf{u},\\mathbf{u}^{\\prime}}^{[3]}\\sum_{\\mathbf{u}^{\\prime}}^{[2]}\\rho_{0}(\\mathbf{E}_{u,v})\\Biggr\\}}\\\\ &{=\\phantom{\\sum_{\\mathbf{u}}}\\rho_{0};\\quad\\mathrm{loss}\\quad\\mathbf{u},\\quad\\quad\\tilde{\\mathbf{E}}_{\\mathbf{u}^{\\prime}}\\equiv\\left[\\frac{1}{\\sqrt{2}}\\sum_{\\mathbf{u}^{\\prime}}^{[3]}\\rho_{0}(\\mathbf{E}_{u,v})\\right]}\\\\ &{\\tilde{\\mathbf{E}}_{\\{u,v\\},\\mathbf{u\\},\\mathbf{u}^{\\prime}}^{[2]}\\times\\frac{1}{\\sqrt{2}}\\sum_{\\mathbf{u}^{\\prime}}\\rho_{0}(\\mathbf{E}_{u,v})\\Biggr\\}}\\\\ &{=\\phantom{\\sum_{\\mathbf{u}}}\\rho_{0};\\quad\\mathrm{loss}\\quad\\mathbf{u},\\quad\\tilde{\\mathbf{E}}_{\\mathbf{u}^{\\prime}}\\equiv\\left[\\phantom{$ (Use Ho\u00a8lder\u2019s Inequality) s Inequality)   \n\u2225xij \u22252\u2264R:i\u2208[n],j\u2208[c]nc i=1j=1   \n\u039bR (1   \nnc ", "page_idx": 25}, {"type": "text", "text": "Combining with Lemma 1, inequalities (11), (5), and $\\rho=(3+2B)c$ , then we have ", "page_idx": 25}, {"type": "equation", "text": "$$\nR(f)\\le\\widehat{R}_{D}(f)+\\frac{24\\sqrt{2}(3+2B)c\\Lambda R\\left(1+\\log^{\\frac{1}{2}}(8e^{2}n^{3}c^{3})\\cdot\\log\\frac{M\\sqrt{n}}{\\rho B}\\right)}{\\sqrt{n}}+3M\\sqrt{\\frac{\\log\\frac{2}{\\delta}}{2n}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "A.4.4 Proof of Theorem 4 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "First, we upper bound the worst-case Rademacher complexity $\\widetilde{\\mathfrak{R}}_{n c}(\\mathcal{P}(\\mathcal{F}))$ for DNN-based LSRL method. With the definitions in the main paper, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\tilde{\\mathbb{R}}_{n c}(P(\\mathcal{F}))}}\\\\ {{\\displaystyle=}}\\\\ {{\\displaystyle\\ln\\operatorname*{sup}_{[\\epsilon]\\times D\\in\\epsilon[\\epsilon]\\times X^{n}}\\hat{\\mathbb{R}}_{[\\epsilon]\\times D}(\\mathcal{F}(\\mathcal{F}))}}\\\\ {{\\displaystyle=}}\\\\ {{\\displaystyle\\ln\\operatorname*{sup}_{[\\epsilon]\\times D\\in\\epsilon[\\epsilon]\\times X^{n}}\\mathbb{E}_{\\epsilon}\\left[\\operatorname*{sup}_{[\\epsilon]\\in[\\epsilon]\\times P}\\frac{1}{n c}\\sum_{i=1}^{n}\\sum_{j=1}^{c}\\epsilon_{i j}p_{j}(f(x_{i}))\\right]}}\\\\ {{\\displaystyle=}}\\\\ {{\\displaystyle\\operatorname*{sup}_{[\\epsilon]\\times D\\in\\epsilon[\\epsilon]\\times X^{n}}\\mathbb{E}_{\\epsilon}\\left[\\operatorname*{sup}_{[\\epsilon]\\in\\mathcal{F}_{j}}\\frac{1}{n c}\\sum_{i=1}^{n}\\sum_{j=1}^{c}\\epsilon_{i j}f_{j}\\left(x_{i}\\right)\\right]}}\\\\ {{\\displaystyle=}}\\\\ {{\\displaystyle\\operatorname*{sup}_{[\\epsilon]\\times D\\in\\epsilon[\\epsilon]\\times X^{n}}\\mathbb{E}_{\\epsilon}\\left[\\operatorname*{sup}_{[\\epsilon]\\in X}\\frac{1}{n c}\\sum_{i=1}^{n}\\epsilon_{i j}\\sigma_{s i j}(w_{j}^{\\top}\\phi_{j}(x_{i}))\\right]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "$\\leq2\\cdot\\frac{1}{4}\\operatorname*{sup}_{[c]\\times D\\in[c]\\times\\mathcal{X}^{n}}\\mathbb{E}_{\\epsilon}\\left[\\operatorname*{sup}_{\\|w_{j}\\|\\leq\\Lambda,\\phi_{j}}\\frac{1}{n c}\\sum_{i=1}^{n}\\sum_{j=1}^{c}\\epsilon_{i j}w_{j}^{\\top}\\phi_{j}(\\pmb{x}_{i})\\right]$   \n(The Lipschitz constant of sigmoid activation is bounded by $\\frac{1}{4}$ )   \n$\\begin{array}{r l}&{\\leq\\displaystyle\\frac{1}{2}\\Lambda\\operatorname*{sup}_{[c]\\times D\\in[c]\\times X^{n}}\\mathbb{E}_{\\epsilon}\\operatorname*{sup}_{\\theta_{j}}\\frac{1}{n c}\\left\\|\\sum_{i=1}^{n}\\sum_{j=1}^{c}\\epsilon_{i j}\\phi_{j}(\\pmb{x}_{i})\\right\\|}\\\\ &{\\leq\\displaystyle\\frac{1}{2}\\Lambda_{\\{c\\}\\times D\\in[c]\\times X^{n}}\\mathbb{E}_{\\epsilon}\\operatorname*{sup}_{n c}\\frac{1}{n c}\\left\\|\\sum_{i=1}^{n}\\sum_{j=1}^{c}\\epsilon_{i j}\\sigma_{R e L U}\\left\\{W_{5}\\cdot\\big[\\sigma_{R e L U}(W_{4}\\pmb{x}_{i})\\odot\\sigma_{s i g}(W_{3}\\psi(Y)_{j})\\big]\\right\\}\\right\\|}\\\\ &{\\leq2\\cdot\\displaystyle\\frac{1}{2}\\Lambda_{\\{c\\}\\times D\\in[c]\\times X^{n}}\\mathbb{E}_{\\epsilon}\\operatorname*{sup}_{\\|W_{5}\\|\\leq D}\\frac{1}{n c}\\left\\|\\sum_{i=1}^{n}\\sum_{j=1}^{c}\\epsilon_{i j}W_{5}\\cdot\\big[\\sigma_{R e L U}(W_{4}\\pmb{x}_{i})\\odot\\sigma_{s i g}(W_{3}\\psi(Y)_{j})\\big]\\right\\|}\\end{array}$ (ReLU activation is 1-Lipschitz)   \n$\\begin{array}{r l}&{\\langle\\Delta U\\rangle_{i+\\lambda,\\gamma\\partial\\sigma_{i}}\\underbrace{u\\phantom{(1)}}_{\\mathrm{\\tiny{i,mall}}}\\underbrace{\\phantom{\\frac{1}{\\gamma}}\\sum_{i=1}^{K}\\gamma_{i,\\sigma_{i}}}\\Big|\\frac{1}{m_{i+1}}\\sum_{j=1}^{K}c_{i,j}\\big|\\bar{\\mu}_{\\bar{\\pi}_{A L G}}(U_{i,\\lambda})\\big|\\sigma_{i\\bar{\\pi}_{B L G}}(Y_{i,\\lambda})\\big|\\Big|}\\\\ &{\\leq\\Lambda D M\\left[\\sigma_{i\\pi_{\\theta}}(W_{i,\\lambda}|\\gamma)\\right]_{i,j=1}+\\sum_{i=1}^{M}\\sum_{i=1}^{K}\\sum_{i=1}^{K}\\bigg|\\frac{1}{m_{i+1}}D\\sigma_{i\\bar{\\pi}_{A L G}}\\bigg|\\bigg|\\sum_{i=1}^{K}c_{i,j}\\rho_{\\bar{\\pi}_{A L G}}\\big|(W_{i,\\lambda})\\bigg|\\sigma_{i\\bar{\\pi}_{B L G}}\\bigg|}\\\\ &{\\leq2\\Lambda D\\exp\\left|\\sigma_{i\\pi_{\\theta}}(W_{i,\\lambda}|\\gamma)\\right|\\sum_{i=1}^{K}\\sum_{i=1}^{K}\\sum_{i=1}^{m}\\bigg|\\frac{1}{m_{i+1}}D\\sigma_{i\\bar{\\pi}_{A L G}}\\bigg|\\bigg|\\sum_{i=1}^{K}c_{i,j}\\big|\\sigma_{i\\bar{\\pi}_{A L G}}\\bigg|}\\\\ &{\\leq2\\Lambda D^{2}\\exp\\left|\\sigma_{i\\pi_{\\theta}}(W_{i,\\lambda}|\\gamma)\\right|\\sum_{i=1}^{K}\\sum_{i=1}^{K}\\sum_{i=1}^{K}\\rho_{i\\bar{\\pi}_{A L G}}\\bigg|\\bigg|\\sum_{i=1}^{I}c_{i,j}\\rho_{i\\bar{\\pi}_{A L G}}\\bigg|}\\\\ &{\\leq2\\Lambda D^{2}\\exp\\left|\\sigma_{i\\pi_{\\theta}}(W_{i,\\lambda}|\\gamma)\\right|\\sum_{i=1}^{I}\\sum_{i=1}^{K}\\sum_{i=1}^{I}\\rho_{i\\bar{\\pi}_{A L G}}\\bigg|\\bigg|\\sum_{i=1}^{I}c_{i,j}\\rho_{i\\bar{\\pi}_{A L G}}\\bigg|,}\\\\ &{\\leq2\\Lambda D^{2}\\exp\\left|\\sigma_{i\\pi_{\\theta}}(W_{i,\\$ nequality) (12) Then, we have to bound sup $\\|\\sigma_{s i g}(W_{3}\\psi(Y)_{j})\\|$ ,   \n$\\begin{array}{r l}&{\\quad\\operatorname*{sup}_{\\boldsymbol{j}}\\displaystyle\\|\\sigma_{s i g p}(W_{3}\\psi(Y)_{j})\\|}\\\\ &{\\le\\!\\frac{1}{4}\\operatorname*{sup}_{\\|W_{3}\\|\\le D}\\|W_{3}\\psi(Y)_{j}\\|\\quad\\mathrm{(The~Lipschitz~constant~of~sigmoid~activation~is~bounded~by~\\frac14)}}\\\\ &{\\le\\!\\frac{1}{4}D\\operatorname*{sup}_{\\boldsymbol{j}}\\|\\psi(Y)_{j}\\|\\quad\\mathrm{(Use~Cauchy.Schwarz~Inequality)}}\\\\ &{=\\!\\frac{D}{4}\\operatorname*{sup}_{\\boldsymbol{j}}\\operatorname*{sup}_{\\|W_{2}\\|\\le D}\\|\\sigma_{R e L U}(\\tilde{A}\\sigma_{R e L U}(\\tilde{A}Y_{j*}W_{1})W_{2})\\|}\\\\ &{\\le\\!\\frac{D}{4}\\operatorname*{sup}_{\\boldsymbol{j}}\\operatorname*{sup}_{\\|W_{2}\\|\\le D}\\|\\tilde{A}\\sigma_{R e L U}(\\tilde{A}Y_{j*}W_{1})\\|\\|W_{2}\\|}\\end{array}$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\frac{D^{2}}{4}\\operatorname*{sup}_{j\\in\\mathcal{N}_{i}\\setminus\\mathcal{N}_{i}}\\{\\lambda_{j}\\}}\\\\ &{\\leq\\frac{D^{2}}{4}\\operatorname*{sup}_{j\\in\\mathcal{N}_{i}}\\sum_{\\stackrel{{\\scriptstyle\\mathrm{\\tiny~{i}}}}{i=j}}\\delta_{j,{\\scriptstyle\\mathrm{\\tiny~{\\it~\\it~\\alpha}}},j}\\Big(\\sum_{\\stackrel{{\\scriptstyle\\mathrm{\\tiny~{i}}}}{i=j}}\\sum_{\\stackrel{{\\scriptstyle\\mathrm{\\tiny~{i}}}}{i=j}}\\Big(\\sum_{\\stackrel{{\\scriptstyle\\mathrm{\\tiny~{i}}}}{i=j}}\\sum_{\\stackrel{{\\scriptstyle\\mathrm{\\tiny~{i}}}}{i=j}}W_{\\infty,j}W_{1j}\\Big)\\Big|}\\\\ &{\\leq\\frac{D^{2}}{4}\\operatorname*{sup}_{j\\in\\mathcal{N}_{i}}\\sum_{\\stackrel{{\\scriptstyle\\mathrm{\\tiny~{i}}}}{i=j}}\\Big(\\sum_{\\stackrel{{\\scriptstyle\\mathrm{\\tiny~{i}}}}{i=j}}\\sum_{\\stackrel{{\\scriptstyle\\mathrm{\\tiny~{i}}}}{i=j}}\\Big(\\sum_{\\stackrel{{\\scriptstyle\\mathrm{\\tiny~{i}}}}{i=j}}\\sum_{\\stackrel{{\\scriptstyle\\mathrm{\\it~\\beta}}}{i=j}}W_{\\infty,j}W_{1j}\\Big)\\Big|}\\\\ &{\\leq\\frac{D^{2}}{4}\\operatorname*{sup}_{j\\in\\mathcal{N}_{i}\\setminus\\mathcal{N}_{i}}\\sum_{\\stackrel{{\\scriptstyle\\mathrm{\\tiny~{i}}}}{i=j}}\\sum_{\\stackrel{{\\scriptstyle\\mathrm{\\it~\\beta}}}{i=j}}\\sum_{\\stackrel{{\\scriptstyle\\mathrm{\\it~\\it~\\alpha}}}{i=j}}\\sum_{\\stackrel{{\\scriptstyle\\mathrm{\\it~\\beta}}}{i=j}}\\sum_{\\stackrel{{\\scriptstyle\\mathrm{\\it~\\it{i}}}}{i=j}}\\sum_{\\stackrel{{\\scriptstyle\\mathrm{\\it~\\it~\\beta}}}{i=j}}\\sum_{\\stackrel{{\\scriptstyle\\mathrm{\\it~\\it{i}}}}{i=j}}\\sum_{\\stackrel{{\\scriptstyle\\mathrm{\\it~\\it{\\it~\\it{i}}}}}{i=j}}}\\\\ &{\\leq\\frac{D^{3}}{4}\\operatorname*{sup}_{j\\in\\mathcal{N}_{i}}\\sum_{\\stackrel{{\\scriptstyle\\mathrm{\\it~\\ldots}}}{i=j}}\\sum_{\\stackrel{{\\scriptstyle\\mathrm{\\it~\\ldots}}}{i=j}}\\sum_{\\stackrel{{\\\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We denote $N(j)$ as the index set of the one-hop neighbors of the $j$ -th node, and denote $g$ as the node degree, $g_{\\mathrm{max}}$ as the maximum node degree. Then, we bound $\\|\\tilde{A}\\|_{\\infty}$ as follows ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{j=1}^{n}\\widetilde{A}_{i j}=\\sum_{j=1}^{n}\\frac{A_{i j}}{\\sqrt{g_{i}+1}\\sqrt{g_{j}+1}}}\\quad}&{}\\\\ &{=\\!\\frac{1}{\\sqrt{g_{i}+1}}\\left(\\frac{1}{\\sqrt{g_{i}+1}}+\\sum_{j\\in N(i)}\\frac{1}{\\sqrt{g_{j}+1}}\\right)}\\\\ &{\\le\\!\\frac{1}{\\sqrt{g_{i}+1}}\\left(\\frac{1}{\\sqrt{1+1}}+\\sum_{j\\in N(i)}\\frac{1}{\\sqrt{1+1}}\\right)}\\\\ &{\\le\\!\\frac{1}{\\sqrt{g_{i}+1}}\\frac{g_{i}+1}{\\sqrt{2}}=\\sqrt{\\frac{g_{i}+1}{2}}\\le\\sqrt{\\frac{g_{\\operatorname*{max}}+1}{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Combining with Lemma 1, inequalities (12), (13), (14), and (5), then we have ", "page_idx": 27}, {"type": "equation", "text": "$$\nR(f)\\leq\\widehat{R}_{D}(f)+\\frac{6\\sqrt{2}\\rho\\Lambda D^{5}R^{2}(g_{\\mathrm{max}}+1)\\left(1+\\log^{\\frac{1}{2}}(8e^{2}n^{3}c^{3})\\cdot\\log\\frac{M\\sqrt{n}}{\\rho B}\\right)}{\\sqrt{n}}+3M\\sqrt{\\frac{\\log\\frac{2}{\\delta}}{2n}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper\u2019s contributions and scope are reflected accurately by the main claims made in the abstract and introduction. Please see Abstract and Introduction sections. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: This is a purely theoretical work which improves existing theoretical results. The assumptions involved in the theoretical results are explained in detail and are satisfied in the relevant settings. Please see Conclusion section. ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: This is a purely theoretical work which improves existing theoretical results. The assumptions involved in the theoretical results are explained in detail and are satisfied in the relevant settings. In the appendix, we give the detailed proofs of those theoretical results in the main paper. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: This is a purely theoretical work. This paper does not include experiments. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: This is a purely theoretical work. This paper does not include experiments requiring code. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: This is a purely theoretical work. This paper does not include experiments. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: This is a purely theoretical work. This paper does not include experiments. ", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] Justification: This is a purely theoretical work. This paper does not include experiments. ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: This is theoretical research and the research conducted in the paper conform with the NeurIPS Code of Ethics. ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This is theoretical research that does not have a potential negative societal impact. ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] Justification: This is a purely theoretical work. This paper poses no such risks. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: This is a purely theoretical work. ", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This is a purely theoretical work. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This is a purely theoretical work. This paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This is a purely theoretical work. This paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 29}]