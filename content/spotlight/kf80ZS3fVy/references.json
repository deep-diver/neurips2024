{"references": [{"fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 technical report", "publication_date": "2023-03-08", "reason": "This paper is a technical report for GPT-4, a significant multimodal LLM, which is directly relevant to the study of multimodal knowledge editing."}, {"fullname_first_author": "Siyuan Cheng", "paper_title": "Can we edit multimodal large language models?", "publication_date": "2023-12-00", "reason": "This paper directly addresses the core challenge of the current work, proposing a benchmark for multimodal knowledge editing and highlighting the limitations of existing methods."}, {"fullname_first_author": "Damai Dai", "paper_title": "Knowledge neurons in pretrained transformers", "publication_date": "2021-04-00", "reason": "This paper introduces the concept of knowledge neurons, which is a relevant concept for intrinsic knowledge editing within LLMs, and directly informs the design of UniKE."}, {"fullname_first_author": "Nicola De Cao", "paper_title": "Editing factual knowledge in language models", "publication_date": "2021-04-00", "reason": "This paper is an early work on knowledge editing in LLMs, providing a foundation for the field and influencing the design choices in the proposed UniKE method."}, {"fullname_first_author": "Eric Mitchell", "paper_title": "Memory-based model editing at scale", "publication_date": "2022-00-00", "reason": "This paper proposes a memory-based method for model editing, which provides a valuable approach for comparison and contrast with the method proposed in the paper."}]}