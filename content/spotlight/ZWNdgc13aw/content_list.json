[{"type": "text", "text": "NEORL: Efficient Exploration for Nonepisodic RL ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Bhavya Sukhija\\*, Lenart Treven, Florian Dorfer, Stelian Coros, Andreas Krause ETH Zurich, Switzerland ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study the problem of nonepisodic reinforcement learning (RL) for nonlinear dynamical systems, where the system dynamics are unknown and the RL agent has to learn from a single trajectory, i.e., adapt online and without resets. This setting is ubiquitous in the real world, where resetting is impossible or requires human intervention. We propose Nonepisodic Optimistic RL (NEoRL), an approach based on the principle of optimism in the face of uncertainty. NEoRL uses well-calibrated probabilistic models and plans optimistically w.r.t. the epistemic uncertainty about the unknown dynamics. Under continuity and bounded energy assumptions on the system, we provide a first-of-its-kind regret bound of $O(\\beta_{T}\\bar{\\sqrt{T}}\\Gamma_{T})$ for general nonlinear systems with Gaussian process dynamics. We compare NEoRL to other baselines on several deep RL environments and empirically demonstrate that NEoRL achieves the optimal average cost while incurring the least regret. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In recent years, data-driven control approaches, such as reinforcement learning (RL), have demonstrated remarkable achievements. However, most RL algorithms are devised for an episodic setting, where during each episode, the agent interacts in the environment for a predetermined episode length or until a termination condition is met. After the episode, the agent is reset back to an initial state from where the next episode commences. Episodes prevent the system from blowing up, i.e., maintain stability, while also restricting exploration to states that are relevant to the task at hand. Moreover, resets ensure that the agent explores close to the initial states and does not end up at undesirable parts of the state space that exhibit low reward. In simulation, resetting is typically straightforward. However, if we wish to enable agents to learn and adapt by interacting online with the real world, resets are often prohibitive since they typically involve manual intervention. Instead, agents should be able to learn autonomously (Sharma et al., 2021b) i.e., from a single trajectory. This problem is extensively studied in adaptive control (Astrom & Wittenmark, 2013), where classical works focus on controller design (Lai & Wei, 1982, 1987; Krstic et al., 1992, 1995; Annaswamy, 2023) and not on the exploration/learning aspect of the problem. Only a few works consider these two aspects jointly (Abbasi-Yadkori & Szepesvari, 2011; Cohen et al., 2019; Dean et al., 2020; Simchowitz & Foster, 2020; Zha0 et al., 2024). However, these works study linear systems with quadratic costs, i.e., the LQR setting. While several works in the Deep RL community have also studied this problem, (c.f., Section 5), the theoretical results for this setting are fairly limited. In particular, theoretical results mostly exist for the finite state and action spaces (Kearns & Singh, 2002; Brafman & Tennenholtz, 2002; Jaksch et al., 2010) and the extension to nonlinear systems with continuous spaces is much less understood. In our work, we address this gap and propose a practical RL algorithm that is grounded in theory. In particular, we make the following contributions. ", "page_idx": 0}, {"type": "text", "text": "Contributions ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1. We propose, NEoRL, a novel model-based RL algorithm based on the principle of optimism in the face of uncertainty. NEoRL operates in a nonepisodic setting and picks average cost optimal policies optimistically w.r.t. to the model's epistemic uncertainty. ", "page_idx": 0}, {"type": "text", "text": "2. We show that when the dynamics lies in a reproducing kernel Hilbert space (RKHS) of kernel $k$ , NEoRL exhibits a regret of $O(\\beta_{T}\\sqrt{T\\Gamma_{T}})$ , where the regret, akin to prior work, is measured w.r.t to the optimal average cost under known dynamics, $T$ is the number of environment steps, $\\beta_{T}$ the calibration coefficient (Chowdhury & Gopalan, 2017; Srinivas et al., 2012) and $\\Gamma_{T}$ the maximum information gain of kernel $k$ (Srinivas et al., 2012). Our regret bound is similar to the ones obtained in the episodic setting (Kakade et al., 2020; Curi et al., 2020; Sukhija et al., 2024; Treven et al., 2024) and Gaussian process (GP) bandit optimization (Srinivas et al., 2012; Chowdhury & Gopalan, 2017; Scarlett et al., 2017) and is sublinear for common kernel such as the exponential kernel. To the best of our knowledge, we are the first to obtain regret bounds for the setting. ", "page_idx": 1}, {"type": "text", "text": "3. We evaluate NEoRL on several RL benchmarks against common model-based RL baselines. Our experimental results demonstrate that NEoRL consistently achieves sublinear regret, also when neural networks are employed instead of GPs for modeling dynamics. Moreover, in all our experiments, NEoRL converges to the optimal average cost. ", "page_idx": 1}, {"type": "text", "text": "2 Problem Setting ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We consider a discrete-time dynamical system with running costs $c$ ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\pmb{x}_{t+1}=\\pmb{f}^{*}(\\pmb{x}_{t},\\pmb{u}_{t})+\\pmb{w}_{t},\\left(\\pmb{x}_{t},\\pmb{u}_{t}\\right)\\in\\mathcal{X}\\times\\mathcal{U},\\ \\pmb{x}(0)=\\pmb{x}_{0}}\\\\ {c(\\pmb{x},\\pmb{u})\\in\\mathbb{R}_{\\ge0}}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Here $\\pmb{x}_{t}\\in\\mathcal{X}\\subseteq\\mathbb{R}^{d_{\\pmb{x}}}$ is the state, $\\pmb{u}_{t}\\in\\mathcal{U}\\subseteq\\mathbb{R}^{d_{\\pmb{u}}}$ the control input, and $\\pmb{w}_{t}\\in\\mathcal{W}\\subseteq\\mathbb{R}^{\\pmb{w}}$ the process noise. The dynamics $f^{*}$ are unknown and the cost $c$ is assumed to be known. ", "page_idx": 1}, {"type": "text", "text": "Task  In this work, we study the average cost RL problem (Puterman, 2014), i.e., we want to learn the solution to the following minimization problem ", "page_idx": 1}, {"type": "equation", "text": "$$\nA(\\pi^{*},x_{0})=\\operatorname*{min}_{\\pi\\in\\Pi}A(\\pi,x_{0})=\\operatorname*{min}_{\\pi\\in\\Pi}\\operatorname*{lim}_{T\\to\\infty}\\operatorname*{sup}_{T}\\frac{1}{\\mathbb{E}}_{\\pi}\\left[\\sum_{t=0}^{T-1}c(x_{t},u_{t})\\right].\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Moreover, we consider the nonepisodic RL setting where the system starts at an initial state $\\pmb{x}_{0}\\in\\mathcal{X}$ but never resets back during learning, that is, we seek to learn online from a single trajectory. After eachstep $t$ in the environment, the RL system receives a transition tuple $({\\boldsymbol{x}}_{t},{\\boldsymbol{u}}_{t},{\\boldsymbol{x}}_{t+1})$ and updates its policy based on the data $\\mathcal{D}_{t}$ collected thus far during learning. The average cost formulation is common for the nonepisodic setting (Jaksch et al., 2010; Abbasi-Yadkori & Szepesvari, 2011; Cohen et al., 2019; Dean et al., 2020; Simchowitz & Foster, 2020), and the cumulative regret for the learning algorithm in this case is defined as ", "page_idx": 1}, {"type": "equation", "text": "$$\nR_{T}=\\sum_{t=0}^{T-1}\\mathbb{E}_{{\\pmb x}_{t},{\\pmb u}_{t}|{\\pmb x}_{0}}[c({\\pmb x}_{t},{\\pmb u}_{t})-A({\\pmb\\pi}^{*},{\\pmb x}_{0})].\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Studying the average cost criterion for general continuous state-action spaces is challenging even when the dynamics are known, since the average cost exists only for special classes of nonlinear systems (Arapostathis et al., 1993). In the following, we impose assumptions on the dynamics and policyclass $\\Pi$ that enable our theoretical analysis. ", "page_idx": 1}, {"type": "text", "text": "2.1  Assumptions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Imposing continuity on $f^{*}$ is quite common in the control theory (Khalil, 2015) and reinforcement learning literature (Curi et al., 2020; Sussex et al., 2023; Sukhija et al., 2024). To this end, for our analysis, we make the following assumption. ", "page_idx": 1}, {"type": "text", "text": "Assumption 2.1 (Continuity of $f^{*}$ and $\\pi$ ). The dynamics model $f^{*}$ and all $\\pi\\in\\Pi$ are continuous.   \nNext, we make an assumption on the system's stochastic disturbances. ", "page_idx": 1}, {"type": "text", "text": "Assumption 2.2 (Process noise distribution). The process noise is i.i.d. Gaussian with variance $\\sigma^{2}$ ,ie, ${\\pmb w}_{t}^{i.i.d}\\!\\mathcal{N}({\\bf0},\\sigma^{2}{\\cal I})$ ", "page_idx": 1}, {"type": "text", "text": "Our analysis can be extended for the more general heteroscedastic case, where $\\sigma$ dependson $\\textbf{\\em x}$ . However, for simplicity, we focus on the homoscedastic setting. In the following, we make assumptions on our policy class. To this end, we first introduce the class of $\\kappa_{\\infty}$ functions. ", "page_idx": 1}, {"type": "text", "text": "Definition 2.3 $\\kappa_{\\infty}$ -functions). The function $\\xi:\\mathbb{R}_{\\geq0}\\rightarrow\\mathbb{R}_{\\geq0}$ is of class $\\kappa_{\\infty}$ , if it is continuous, strictly increasing, $\\xi(0)=0$ and $\\xi(s)\\to\\infty$ for $s\\to\\infty$ ", "page_idx": 2}, {"type": "text", "text": "Assumption 2.4 (Policies with bounded energy). We assume there exists $\\kappa,\\xi\\,\\in\\,\\kappa_{\\infty}$ , positive constants $K,C_{u},C_{l}$ with $C_{u}>C_{l}$ , and $\\gamma\\in(0,1)$ such that for each $\\pi\\in\\Pi$ we have, ", "page_idx": 2}, {"type": "text", "text": "Bounded energy: There exists a Lyapunov function $V^{\\pi}:\\mathcal{X}\\rightarrow[0,\\infty)$ for which $\\forall\\pmb{x},\\pmb{x}^{\\prime}\\in\\mathcal{X}$ ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{c}{|V^{\\pi}(\\pmb{x})-V^{\\pi}(\\pmb{x}^{\\prime})|\\leq\\kappa(\\|\\pmb{x}-\\pmb{x}^{\\prime}\\|)}\\\\ {C_{l}\\xi(\\|\\pmb{x}\\|)\\leq V^{\\pi}(\\pmb{x})\\leq C_{u}\\xi(\\|\\pmb{x}\\|)}\\\\ {\\mathbb{E}_{\\pmb{x}+\\vert\\pmb{x},\\pi}[V^{\\pi}(\\pmb{x}_{+})]\\leq\\gamma V^{\\pi}(\\pmb{x})+K}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where ${\\pmb x}_{+}=f^{*}({\\pmb x},\\pi({\\pmb x}))+{\\pmb w}$ ", "page_idx": 2}, {"type": "text", "text": "Bounded norm of cost: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{{\\pmb x}\\in{\\mathcal{X}}}\\frac{c({\\pmb x},{\\pmb\\pi}({\\pmb x}))}{1+V^{\\pi}({\\pmb x})}<\\infty\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Boundedness of the noise with respect to $\\kappa$ ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\pmb{w}}\\left[\\kappa(\\|\\pmb{w}\\|)\\right]<\\infty,\\;\\mathbb{E}_{\\pmb{w}}\\left[\\kappa^{2}(\\|\\pmb{w}\\|)\\right]<\\infty\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The drift condition states that the energy between two timesteps can increase at most by $K$ .In particular, the Lyapunov function $V^{\\pi}$ can be viewed as an energy function for the dynamical system, and the bounded energy condition above ensures that the system is not \u201c\"blowing up'. We do not perceive this as restrictive for real-world engineered systems. Other works that study learning nonlinear dynamics (Foster et al., 2020; Sattar & Oymak, 2022; Lale et al., 2021) in the nonepisodic setting also make stability assumptions such as global exponential stability for their analysis. In similar spirit, we make the bounded energy assumption for our policy class. The drift condition on the Lyapunov function is also used to study the ergodicity of Markov chains for continuous state spaces (Meyn & Tweedie, 2012; Hairer & Mattingly, 2011), which is crucial for our analysis of the infinite horizon behavior of the system. Moreover, for a very rich class of problems, the drift condition is satisfied. We highlight this in the corollary below. ", "page_idx": 2}, {"type": "text", "text": "Lemma 2.5. Assume $f^{*}$ is uniformly continuous and for all $\\pi\\,\\in\\,\\Pi$ $\\pmb{x}\\in\\mathcal{X}$ $\\|\\pi(\\pmb{x})\\|\\,\\leq\\,u_{\\mathrm{max}}$ Further assume,there exists $\\pi_{s}\\in\\Pi$ such that we have constants $K,C_{u},C_{l}$ with $C_{u}>C_{l}$ \uff0c $\\gamma\\in(0,1)$ $\\kappa,\\alpha\\in\\kappa_{\\infty}$ and a Lyapunov function $V:\\mathcal{X}\\to[0,\\infty)$ for which $\\forall{\\pmb x},{\\pmb x}^{\\prime}\\in\\mathcal X$ \uff0c ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|V(\\pmb{x})-V(\\pmb{x}^{\\prime})|\\le\\kappa(\\|\\pmb{x}-\\pmb{x}^{\\prime}\\|)}\\\\ &{\\qquad\\quad\\;C_{l}\\xi(\\|\\pmb{x}\\|)\\le V(\\pmb{x})\\le C_{u}\\xi(\\|\\pmb{x}\\|)}\\\\ &{\\mathbb{E}_{\\pmb{x}_{+}|\\pmb{x},\\pmb{\\pi}_{s}}[V(\\pmb{x}_{+})]\\le\\gamma V(\\pmb{x})+K,}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where ${\\pmb x}_{+}=f^{*}({\\pmb x},\\pi({\\pmb x}))+{\\pmb w}$ .Then, $V$ also satisfies the drift condition for all $\\pi\\in\\Pi$ i.e, is a Lyapunov function for all policies. ", "page_idx": 2}, {"type": "text", "text": "We prove this lemma in Appendix A. Intuitively, if the inputs are bounded, the energy inserted into the system by another policy is also bounded. Nearly all real-world systems have bounded inputs due to the physical limitations of actuators. For these systems, it suffices if only one policy in $\\Pi$ satisfies the drift condition. ", "page_idx": 2}, {"type": "text", "text": "The boundedness assumptions for the cost and the noise in Assumption 2.4 are satisfied for a rich classofcost and $\\kappa_{\\infty}$ functions. ", "page_idx": 2}, {"type": "text", "text": "Under these assumptions, we can show the existence of the average cost solution. ", "page_idx": 2}, {"type": "text", "text": "Theorem 2.6 (Existence of Average Cost Solution). Let Assumption $2.1-2.4\\$ hold. Consider any $\\pi\\in\\Pi$ and let $P^{\\pi}$ denote its transition kernel, i.e., $P^{\\pi}(x,A)=\\mathbb{P}(x_{+}\\in A|x,\\pi(x))$ for ${\\mathcal{A}}\\subseteq{\\mathcal{X}}$ Then $P^{\\pi}$ admits a unique invariant measure $\\bar{P}^{\\pi}$ ,and there exists $C_{2},C_{3}\\in(0,\\infty)$ $\\lambda\\in(0,1)$ such that ", "page_idx": 2}, {"type": "text", "text": "Average Cost; ", "page_idx": 2}, {"type": "equation", "text": "$$\nA(\\pi)=\\operatorname*{lim}_{T\\to\\infty}\\frac{1}{T}\\mathbb{E}_{\\pi}\\left[\\sum_{t=0}^{T-1}c(\\pmb{x}_{t},\\pmb{u}_{t})\\right]=\\mathbb{E}_{\\pmb{x}\\sim\\bar{P}^{\\pi}}\\left[c(\\pmb{x},\\pmb{\\pi}(x))\\right]\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Bias Cost; Letting $\\begin{array}{r}{B(\\pmb{\\pi},\\pmb{x}_{0})=\\operatorname*{lim}_{T\\rightarrow\\infty}\\mathbb{E}_{\\pmb{\\pi}}\\left[\\sum_{t=0}^{T-1}c(\\pmb{x}_{t},\\pmb{u}_{t})-A(\\pmb{\\pi})\\right]}\\end{array}$ denote the bias, we have ", "page_idx": 3}, {"type": "equation", "text": "$$\n|B(\\pmb{\\pi},\\pmb{x}_{0})|\\leq C_{2}(1+V^{\\pi}(\\pmb{x}_{0}))\\frac{1}{1-\\lambda}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "for all $\\pmb{x}_{0}\\in\\mathcal{X}$ ", "page_idx": 3}, {"type": "text", "text": "Theorem 2.6 is a crucial result for our analysis since it implies that the average cost is bounded and independentof theinitialstate $\\pmb{x}_{0}$ . Furthermore, it also shows that the bias is bounded. The average cost criterion satisfies the following Bellman equation (Puterman, 2014) below ", "page_idx": 3}, {"type": "equation", "text": "$$\nB(\\pi,x)+A(\\pi)=c({\\pmb x},{\\pmb\\pi}({\\pmb x}))+\\mathbb{E}_{{\\pmb x}_{+}}[B(\\pi,{\\pmb x}_{+})|{\\pmb x},\\pi]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Accordingly, the bias term plays an important role in the regret analysis (also notice its similarity to our regret term in Equation (3)). ", "page_idx": 3}, {"type": "text", "text": "Thus far, we have only made assumptions that make the average cost problem tractable. In the following, we make an assumption on the dynamics that allow us to learn it from data. Moreover, we assume that at each step $n$ we learn a mean estimate $\\pmb{\\mu}_{n}$ of $f^{*}$ and can quantify our uncertainty $\\sigma_{n}$ over the estimate. More formally, we learn a well-calibrated statistical model of $f^{*}$ as defined below. ", "page_idx": 3}, {"type": "text", "text": "Definition 2.7 (Well-calibrated statistical model of $f^{*}$ , Rothfuss et al. (2023)). Let $\\mathcal{Z}\\,\\overset{\\mathrm{def}}{=}\\mathcal{X}\\times\\mathcal{U}$ An all-time well-calibrated statistical model of the function $f^{*}$ is a sequence $\\{\\mathcal{M}_{n}(\\delta)\\}_{n\\ge0}$ ,where ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{M}_{n}(\\delta)\\overset{\\mathrm{def}}{=}\\left\\{f:\\mathcal{Z}\\rightarrow\\mathbb{R}^{d_{x}}\\mid\\forall z\\in\\mathcal{Z},\\forall j\\in1,\\dots,d_{x}:|\\mu_{n,j}(z)-f_{j}(z)|\\leq\\beta_{n}(\\delta)\\sigma_{n,j}(z)\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "if, with probability at least $1-\\delta$ , we have $\\pmb{f}^{*}\\in\\bigcap_{n\\geq0}\\mathcal{M}_{n}(\\delta)$ .Here, $f_{j},\\mu_{n,j}$ and $\\sigma_{n,j}$ denote the $j$ -th element in the vector-valued functions $\\boldsymbol{\\textbf{\\textit{f}}}$ $\\pmb{\\mu}_{n}$ and $\\sigma_{n}$ respectively, and $\\beta_{n}(\\delta)\\in\\mathbb{R}_{\\geq0}$ is a scalar function that depends on the confidence level $\\delta\\in(0,1]$ and which is monotonically increasing in $n$ ", "page_idx": 3}, {"type": "text", "text": "Next, we assume that $f^{*}$ resides in a Reproducing Kernel Hilbert Space (RKHS) of vector-valued functions and show that this is sufficient for us to obtain a well-calibrated model. ", "page_idx": 3}, {"type": "text", "text": "Assumption 2.8. We assume that the functions $f_{j}^{*}$ $j\\,\\in\\,1,\\ldots,d_{x}$ lie in a RKHS with kernel $k$ and aveabound om $B$ $\\pmb{f}^{*}\\in\\mathcal{H}_{k,B}^{d_{x}}$ $\\mathcal{H}_{k,B}^{d_{x}}=\\{\\pmb{f}\\ |\\ \\|f_{j}\\|_{k}\\leq B,j=1,\\ldots,d_{x}\\}$ Moreover, we assume that $k(\\mathbf{x},\\mathbf{x})\\leq\\sigma_{\\mathrm{max}}$ for all $x\\in\\mathcal{X}$ ", "page_idx": 3}, {"type": "text", "text": "Assumption 2.8 allows us to model $f^{*}$ with GPs for which the mean and epistemic uncertainty $(\\pmb{\\mu_{n}}(\\pmb{z})^{\\top}=[\\mu_{n,j}(\\pmb{z})]_{j\\leq d_{x}}$ , and $\\pmb{\\sigma}_{n}(z)=[\\sigma_{n,j}(z)]_{j\\leq d_{x}})$ have an analytical formula ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{n,j}(z)=k_{n}^{\\top}(z)(K_{n}+\\sigma^{2}I)^{-1}y_{1:n}^{j},}\\\\ &{\\sigma_{n,j}^{2}(z)=k(x,x)-k_{n}^{\\top}(z)(K_{n}+\\sigma^{2}I)^{-1}k_{n}(x),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, $\\boldsymbol{y}_{1:n}^{j}$ corresponds to the noisy measurements of $f_{j}^{*}$ , i.e., the observed next state from the transitions dataset $\\mathcal{D}_{1:n},k_{n}=[k(z,z_{i})]_{i\\leq n T},z_{i}\\in\\mathcal{D}_{1:n}$ and $\\pmb{K}_{n}=[k(z_{i},z_{l})]_{i,l\\leq n T},z_{i},z_{l}\\in\\mathcal{D}_{1:n}$ is the data kernel matrix. The restriction on the kernel $k(\\mathbf{x},\\mathbf{x})\\leq\\sigma_{\\mathrm{max}}$ implies boundedness of $f^{*}$ and has also appeared in works studying the episodic setting for nonlinear systems (Mania et al., 2020; Kakade et al., 2020; Curi et al., 2020; Sukhija et al., 2024; Wagenmaker et al., 2023). We can also define $f^{*}$ such that ${\\pmb x}_{k}={\\pmb x}_{k-1}+{\\pmb f}^{*}({\\pmb x}_{k-1},{\\pmb u}_{k-1})+{\\pmb w}_{k-1}$ in which case the boundedness of $f^{*}$ captures many real-world systems. ", "page_idx": 3}, {"type": "text", "text": "Lemma29 Wellcalratd conencevals frRKH Ross tal. (2). $f^{*}\\in\\mathcal{H}_{k,B}^{d_{x}}$ Suppose $\\pmb{\\mu}_{n}$ and $\\sigma_{n}$ are the posterior mean and variance of a $G P$ with kernel $k$ $c.f.$ ,Equation (5). There exists $\\beta_{n}(\\delta)\\propto\\sqrt{\\Gamma_{n}},$ for which the tuple $(\\pmb{\\mu}_{n},\\pmb{\\sigma}_{n},\\beta_{n}(\\delta))$ is a well-calibrated statistical model of $f^{*}$ ", "page_idx": 3}, {"type": "text", "text": "In summary, in the RKHS setting, a GP is a well-calibrated model. For more general models like Bayesian neural networks (BNNs), methods such as Kuleshov et al. (2018) can be used for calibration. Our results can also be extended beyond the RKHS setting to other classes of well-calibrated models similar to Curi et al. (2020). ", "page_idx": 3}, {"type": "text", "text": "Algorithm 1 NEORL: NONEPISODIC OPTIMISTIC RL ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Init: Aleatoric uncertainty $\\sigma$ , Probability $\\delta$ , Statistical model $(\\mu_{0},\\pmb{\\sigma}_{0},\\beta_{0}(\\delta)),H_{0}$ for $n=1,\\ldots,N$ do ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pi_{n}=\\underset{\\pi\\in\\Pi}{\\mathrm{arg}\\,\\mathrm{min}}\\,\\underset{f\\in\\mathcal{M}_{n-1}\\cap\\mathcal{M}_{0}}{\\mathrm{min}}\\,A(\\pi,f)}\\\\ &{H_{n}=2H_{n-1}}\\\\ &{{\\mathcal{D}}_{n}\\gets\\mathsf{R o L L O U T}(\\pi_{n})}\\\\ &{\\mathsf{U p d a t e}\\,\\left(\\mu_{n},\\sigma_{n},\\beta_{n}\\right)\\gets{\\mathcal{D}}_{n}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "\u300b Prepare policy   \n\u300bSet horizon   \n$\\blacktriangleright$ Collect measurements for horizon $H_{n}$   \n$\\blacktriangleright$ Update statistical model $\\mathcal{M}_{n}$ ", "page_idx": 4}, {"type": "text", "text": "3 NEORL ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In the following, we present our algorithm: Nonepisodic Optimistic RL (NEoRL) for efficient nonepisodic exploration in continuous state-action spaces. NEoRL builds on recent advances in episodic RL (Kakade et al., 2020; Curi et al., 2020; Sukhija et al., 2024; Treven et al., 2024) and leverages the optimism in the face of uncertainty paradigm to pick policies that are optimistic w.r.t. the dynamics within our calibrated statistical model as follows ", "page_idx": 4}, {"type": "equation", "text": "$$\n(\\pi_{n},f_{n})\\,{\\stackrel{\\mathrm{def}}{=}}\\,\\underset{\\pi\\in\\Pi,~f\\in\\mathcal{M}_{n-1}\\cap\\mathcal{M}_{0}}{\\arg\\operatorname*{min}}\\,A(\\pi,f).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, $\\pmb{f}_{n}$ is a dynamical system such that the cost by controlling $\\pmb{f}_{n}$ with its optimal policy $\\pi_{n}$ is the lowest among all the plausible systems from $\\mathscr{M}_{n-1}\\cap\\mathscr{M}_{0}$ . Note, from Lemma 2.9 we have that $\\pmb{f}^{*}\\in\\mathcal{M}_{n-1}\\cap\\mathcal{M}_{0}$ (with high probability) and therefore the solution to Equation (6) gives an optimistic estimate for the average cost. We take the intersection of $\\mathcal{M}_{n-1}$ with $\\mathcal{M}_{0}$ to ensure that we maintain at least the same confidence about our model as at the beginning, i.e., $n=0$ , during learning. NEoRL proceeds in the following manner. Similar to Jaksch et al. (2010), we bin the total time $T$ the agent spends interacting in the environment into $N$ \u201cartificial' episodes. At each episode, we pick a policy according to Equation (6) and roll it out for $H_{n}$ steps on the system. Next, we use the data collected during the rollout to update our statistical model. Finally, we double the horizon $H_{n+1}=2H_{n}$ , akin to Simchowitz & Foster (2020), and continue to the next episode without resetting the system back to the initial state $\\scriptstyle x_{0}$ . Intuitively, in the beginning, when our model estimate is not accurate, we update our model more frequently, and with more episodes as our model gets better we reduce the frequency of updates. The algorithm is summarized in Algorithm 1. ", "page_idx": 4}, {"type": "text", "text": "3.1  Theoretical Results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In the following, we study the theoretical properties for NEoRL and provide a first-of-its-kind bound on the cumulative regret for the average cost criterion for general nonlinear dynamical systems. Our bound depends on the maximum information gain of kernel $k$ (Srinivas et al., 2012), defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Gamma_{T}(k)=\\operatorname*{max}_{\\substack{A\\subset\\mathcal{X}\\times\\mathcal{U};|A|\\leq T}}\\frac{1}{2}\\log\\left|I+\\sigma^{-2}K_{T}\\right|.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "$\\Gamma_{T}$ represents the complexity of learning $f^{*}$ from $T$ data points and is sublinear for a very rich class of kermels (e.g. $\\mathcal{O}(\\log^{\\stackrel{\\cdot}{d_{x}}+d_{u}+1}(T))$ fortheexponentialRBF\uff09kmel $\\mathcal{O}((d_{x}+d_{u})\\log(T))$ for the linear kernel). In Appendix A, we report the dependence of $\\Gamma_{T}$ on $T$ in Table 1. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.1 (Cumulative Regret of NEoRL). Let Assumption 2.1 - 2.8 hold, and define $H_{0}$ asthe smallestintegersuchthat ", "page_idx": 4}, {"type": "equation", "text": "$$\nH_{0}>\\frac{\\log{\\left({C_{u}}/{C_{l}}\\right)}}{\\log{\\left({1}/{\\gamma}\\right)}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Thenwith probability at least $1-\\delta$ we have the following regret for NEoRL ", "page_idx": 4}, {"type": "equation", "text": "$$\nR_{T}\\leq D_{4}({\\pmb x}_{0},{\\cal K},\\gamma)\\beta_{T}\\sqrt{T\\Gamma_{T}}+D_{5}({\\pmb x}_{0},{\\cal K},\\gamma)\\log_{2}\\left(\\frac{T}{H_{0}}+1\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "with $D_{4}({\\pmb x}_{0},K,\\gamma),\\,D_{5}({\\pmb x}_{0},K,\\gamma)$ being bounded constants for bounded $\\|\\pmb{x}_{0}\\|$ $K$ and $\\gamma<1$ ", "page_idx": 4}, {"type": "text", "text": "From Lemma 2.9 we have that $\\beta_{T}\\propto\\sqrt{\\Gamma_{T}}$ and therefore Theorem 3.1 gives sublinear regret for a rich class of RKHS functions. Moreover, it also gives a minimal horizon $H_{0}$ that we need to maintain before switching to the next policy. Even for the linear case, fast switching between stable controllers can destabilize the closed-loop system. We ensure this does not happen in our case by having a minimal horizon of $H_{0}$ . Theorem 3.1 can also be derived beyond the RKHS setting for a more general class of well-calibrated models. In this case, the maximum information gain is replaced by the model complexity from Curi et al. (2020) (c.f., Curi et al. (2020); Sukhija et al. (2024) for further detail). ", "page_idx": 5}, {"type": "text", "text": "In the following, we give an intuitive proof sketch for Theorem 3.1. The detailed proof is provided in AppendixA. ", "page_idx": 5}, {"type": "text", "text": "Proof sketch  The proof can be split into three main steps. First, we show the ergodicity of the closed-loop system, a sufficient condition for showing the existence of the average cost and bias term, i.e., Theorem 2.6, for every policy $\\pi\\in\\Pi$ under Assumption $2.1-2.4$ . For this, we use elementary results on Markov chains in measurable spaces from Meyn & Tweedie (2012); Hairer & Mattingly (2011). Second, we show that under Assumption 2.8, the optimistic system selected in Equation (6), retains the same properties as the true system $f^{*}$ , e.g., stability, and therefore also is ergodic. Crucial to show this is that the true system $f^{*}$ and the optimistic system $\\pmb{f}_{n}$ are at most $\\beta_{n}\\pmb{\\sigma}_{n}$ apart. Finally, in the third step, we show that as we update our model and policy every $H_{n}$ steps, the doubling of the horizon retains the system properties from above, and our accumulated model uncertainties across $T$ environment steps grow with the rate $\\Gamma_{T}$ . For the latter, we use the analysis from Kakade et al. (2020) for the episodic case, to bound the deviation between the optimistic average cost and the true average cost. ", "page_idx": 5}, {"type": "text", "text": "3.2 Practical Modifications ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "For testing NEoRL, we make three modifications that simplify its deployment in practice in terms of implementation and computation time. First, instead of doubling the horizon $H_{n}$ we pick a fixed horizon $H$ during the experiment. This makes the planning and training of the agent easier. Next, we use a receding horizon controller, i.e., model predictive control (MPC) (Garcia et al., 1989), instead of directly optimizing for the average cost in Equation (6). MPC is widely used to obtain a feedback controller for the infinite horizon setting. Moreover, while for linear systems, the Riccati equations (Anderson & Moore, 2007) provide an analytical solution to Equation (2), no such solution exists for the nonlinear case and MPC is commonly used as an approximation. Further, under additional assumptions on the cost and dynamics, MPC also obtains a policy with bounded average cost, which is crucial for the nonepisodic case (c.f., Assumption 2.4). We use the iCEM optimizer for planning (Pinneri et al., 2021). Finally, instead of optimizing over $\\mathcal{M}_{n}\\cap\\mathcal{M}_{0}$ , we optimize directly over ${\\mathcal{M}}_{n}$ . This allows us to use the reparameterization trick from Curi et al. (2020) and obtain a simple and tractable optimization problem. In summary, for each step $t$ in the environment, we solve the following optimization problem ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\substack{{\\pmb u}_{0:H_{\\mathrm{MPC}}}\\,-1\\,,{\\pmb\\eta}_{0;H_{\\mathrm{MPC}}}\\,-1}}{\\mathbb{E}}\\left[\\sum_{h=0}^{H_{\\mathrm{MPC}}\\,-1}c({\\hat{\\pmb x}}_{h},{\\pmb u}_{h})\\right],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "s.t. $\\hat{\\pmb{x}}_{h+1}=\\pmb{\\mu}_{n-1}(\\hat{\\pmb{x}}_{h},\\pmb{u}_{h})+\\beta_{n-1}(\\delta)\\pmb{\\sigma}_{n-1}(\\hat{\\pmb{x}}_{h},\\pmb{u}_{h})\\pmb{\\eta}_{h}+\\pmb{w}_{h}$ and $\\hat{\\pmb x}_{0}=\\pmb x_{t}$ ", "page_idx": 5}, {"type": "text", "text": "Here $H_{\\mathrm{MPC}}$ is the MPC horizon. We take the first input from the solution of the problem above, i.e., $\\pmb{u}_{0}^{*}$ , and execute this in the system. We then repeat this procedure for $H$ steps and then update our statistical model $\\mathcal{M}_{n}$ . The resulting optimization above considers a larger action space as it includes the hallucinated controls $\\eta$ as additional input variables. The hallucinated controls are introduced through the reparameterization trick from (Curi et al., 2020) and are used to directly optimize over models in $\\textbf{\\textit{f}}\\in\\,\\mathcal{M}_{n-1}$ . Moreover, the final algorithm can be seen as a natural extension to $\\mathrm{H}\\cdot$ UCRL (Curi et al., 2020) for the nonepisodic setting. We summarize the algorithm in Appendix B Algorithm 2. Note while these modifications deviate from our theoretical analysis, empirically they work well for GP and BNN models, c.f., Section 4. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We evaluate NEoRL on the Pendulum-v1 and MountainCar environment from the OpenAI gym benchmark suite (Brockman et al., 2016), Cartpole, Reacher, and Swimmer from the DeepMind control suite (Tassa et al., 2018), the racecar simulator from Kabzan et al. (2020), and a soft robotic arm from Tekinalp et al. (2024). The swimmer and the soft robotic arm are fairly high-dimensional systems - the swimmer has a 28-dimensional state and 5-dimensional action space, and the soft arm is represented by a 58-dimensional state and has a 12-dimensional action space. All environments are never reset during learning. Moreover, the Pendulum-v1, MountainCar, CartPole, and Reacher environments operate within a bounded domain and thus inherently satisfy Assumption 2.4. The swimmer, racecar, and soft arm can operate in an unbounded domain but have a cost function that penalizes the distance between the system's state $\\pmb{x}_{t}$ and a target state $x^{*}$ .Therefore, the cost encourages the system to move towards the target and remain within a bounded domain. ", "page_idx": 5}, {"type": "image", "img_path": "ZWNdgc13aw/tmp/4e0a6ba551ed48cbc8e7deab2b629b2f201684aa5df9cc9436b83199db1fc246.jpg", "img_caption": ["Figure 1: Average reward $A(\\pi)$ and cumulative regret $R_{T}$ over ten different seeds for all environments. We report the mean performance with one standard error as shaded regions. During all experiments, the environment is never reset. For all baselines, we model the dynamics with probabilistic ensembles, except in the Pendulum-GP experiment, where GPs are used instead. NEoRL significantly outperforms all baselines and converges to the optimal average reward, $A(\\pi^{*})=0$ showing sublinear cumulative regret $R_{T}$ for all environments. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Baselines In the episodic setting, resets can be used to control the exploration space for the agent. However, in the absence of resets, the agent can explore arbitrarily and end up in states that are irrelevant to the task at hand. Moreover, the agent has to follow an uninterrupted chain of experience, which makes the nonepisodic setting the most challenging one in RL (Kakade, 2003). Accordingly, there are only a few algorithms that consider this setting (c.f., Section 5). In this work, we focus on model-based RL (MBRL) algorithms due to their sample efficiency. In particular, we adopt common MBRL methods for our setting. MBRL algorithms typically differentiate in three ways; (i) propagating dynamics for planning (Chua et al., 2018; Osband & Van Roy, 2017; Kakade et al., 2020; Curi et al., 2020), (i) representation of the dynamics model (Ha & Schmidhuber, 2018; Hafner et al., 2019; Kipf et al., 2019), and $(i i i)$ types of planners (Williams et al., 2017; Hafner et al., 2020; Pinneri et al., 2021). NEoRL is independent to the choice of representation or planners. Therefore, we focus on (i) and use probabilistic ensembles (Lakshminarayanan et al., 2017) and GPs for modeling our dynamics and MPC with iCEM (Pinneri et al., 2021) as the planner. Common techniques to propagate the dynamics for planning are using the mean, trajectory sampling (Chua et al., 2018), and Thompson sampling (Osband & Van Roy, 2017). We adapt these three for our setting similar to as discussed in Section 3.2. For all experiments with probabilistic ensembles, we consider TS1 from Chua et al. (2018) for trajectory sampling, and for the GP experiment, we use distribution sampling from Chua et al. (2018). We call the three baselines NEMEAN (nonepisodic mean), NEPETS (nonepisodic PETS), and NETS (nonepisodic Thompson sampling). NEMEAN and NEPETS are greedy w.r.t. the current estimate of the dynamics, i.e., do not explicitly encourage exploration. In our experiments, we show that being greedy does not sufice to converge to the optimal average cost, that is, obtain sublinear regret. The code for our experiments is available online.2 ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Convergence to the optimal average cost  In Figure 1 we report the normalized average cost and cumulative regret of NEoRL, NEMEAN, NEPETS, and NETS. The normalized average cost is defined such that $A(\\pi^{*})=0$ for all environments. We observe that NEMEAN fails to converge to the optimal average cost for the Pendulum-v1 environment for both probabilistic ensembles and a GP model. It also fails to solve the MountainCar environment and is unstable for the Reacher and CartPole. In general, NEMEAN performs the worst among all methods. This is similar to the episodic case, where using the mean model often leads to the policy \u201coverfitting\"\u201d to the model inaccuracies (Chua et al., 2018). NEPETS performs better than the mean, however still significantly worse than NEoRL. Even in the episodic setting, PETS tends to underexplore (Curi et al., 2020). We observe the same for the nonepisodic case, especially for the MountainCar task, which is a challenging RL environment with a sparse cost. Here NEPETS is also not able to achieve the optimal average cost and thus does not have sublinear cumulative regret. NETS performs similarly to NEPETS and is also not able to solve the MountainCar task. ", "page_idx": 7}, {"type": "text", "text": "NEoRL performs the best among the baselines for all experiments and converges to the optimal average cost achieving sublinear cumulative regret using only $\\sim\\,10^{3}$ environment interactions. Moreover, this observation is consistent between different dynamics models (GPs and probabilistic ensembles) and environments. Even in environments that are unbounded, i.e., Swimmer, SoftArm, and RaceCar, we observe that NEoRL converges to the optimal average cost the fastest. We believe this is due to the feedback control from MPC, which has a stabilizing effect. ", "page_idx": 7}, {"type": "text", "text": "Calling reset when needed  All the experiments in Figure 1 considered the nonepisodic setting where the system was never reset during learning. A special case of our theoretical analysis is the class ofpolicies $\\Pi$ that may call for a reset / \u201cask for help\u201d whenever they end up in an undesirable part of the state space. In this setting, the system is typically restricted to a compact subset of the statespace $\\mathcal{X}$ , and the policy class satisfies Assumption 2.4. For many real-world applications, such a policy class can be derived. To simulate this experiment, we consider the CartPoleBalance task in Figure 2, where the goal is to balance the pole in the upright position. A reset is triggered whenever the pole drops. We again observe that NEoRL achieves the best performance, i.e., lowest cumulative regret and thus learns to solve the task the fastest. Moreover, it also requires fewer resets than NEMEAN, NEPETS, and NETS. ", "page_idx": 7}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Average cost RL for finite state-action spaces  A significant amount of work studies the average cost/reward RL setting for finite-state action spaces. Moreover, seminal algorithms such as $\\mathrm{{E^{3}}}$ (Kearns & Singh, 2002) and R-max (Brafman & Tennenholtz, 2002) have established PAC bounds for the nonepisodic setting. These bounds are further improved for communicating MDPs by the UCRL2 (Jaksch et al., 2010) algorithm, which, similar to NEoRL, is based on the optimism in the face of uncertainty paradigm and picks policies that are optimistic w.r.t. to the estimated dynamics. Their result is extended for weakly-communicating MDPs by REGAL (Bartlett & Tewari, 2012), similar results are derived for Thompson sampling based exploration (Ouyang et al., 2017), and for factored-MDP (Xu & Tewari, 2020). Albeit the significant amount of work for the finite case, progress for continuous state-action spaces has mostly been limited to linear dynamical systems. ", "page_idx": 7}, {"type": "text", "text": "Nonepisodic RL for linear systems  There is a large body of work for nonepisodic learning with linear systems (Abbasi-Yadkori & Szepesvari, 2011; Cohen et al., 2019; Simchowitz & Foster, 2020; Dean et al., 2020; Lale et al., 2020; Faradonbeh et al., 2020; Abeille & Lazaric, 2020; Treven et al., 2021). For linear systems with quadratic costs, the average reward problem, also known as the linear quadratic-Gaussian (LQG), has a closed-form solution which is obtained via the Riccati equations (Anderson & Moore, 2007). Moreover, for LQG, stability and optimality are intertwined, making studying linear systems much easier than their nonlinear counterpart. For studying nonlinear systems, additional assumptions on their stability are usually made. ", "page_idx": 7}, {"type": "image", "img_path": "ZWNdgc13aw/tmp/e3a2088ebe3ca992e44288b10e7d05826514fc4d4faf13a4b9f2aa68c7a4e73e.jpg", "img_caption": ["Figure 2: Total number of resets and cumulative regret $R_{T}$ for the cart pole balancing task over ten different seeds. We report the mean performance with one standard errors as the shaded region. The environment is automatically reset whenever the agent drops the pole. All baselines solve the task, but NEoRL converges the fastest requiring fewer resets and suffering smaller regret. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Episodic RL for nonlinear systems  In the case of nonlinear systems, guarantees have mostly been established for the episodic setting (Mania et al., 2020; Kakade et al., 2020; Curi et al., 2020; Wagenmaker et al., 2023; Sukhija et al., 2024; Treven et al., 2024). In this setting, the agent begins each episode from an initial state $s_{\\mathrm{0}}$ (or initial state distribution) and interacts with the environment for a fixed horizon $H$ . It uses the data collected from the interactions to update its model. After each episode, the agent is reset back to $\\ s_{\\mathrm{0}}$ . The works mentioned above theoretically study this setting for finite-horizon MDPs and establish regret bounds for general nonlinear systems. Particularly Kakade et al. (2020); Curi et al. (2020); Sukhija et al. (2024); Treven et al. (2024) also use an optimism-based approach similar to ours. Compared to the nonepisodic case, the analysis of episodic RL methods is simpler as resets restrict the agent's exploration around the initial state $s_{\\mathrm{0}}$ and prevent the system from blowing up or visiting states from which the agent cannot recover. However, as discussed in Section 1, resets are often prohibitive and RL agents that learn non-episodically are preferred for many real-world applications. ", "page_idx": 8}, {"type": "text", "text": "Nonepisodic RL beyond linear systems  Only a few works consider the nonepisodic/singletrajectory case. For instance, a line of work studies data-driven MPC approaches focusing mostly on establishing system-theoretic guarantees such as closed-loop stability and robustness (Berberich & Allgower, 2024). From the learning side, Foster et al. (2020); Sattar & Oymak (2022) study the problem of system identification of a closed-loop globally exponentially stable dynamical system from a single trajectory. Lale et al. (2021) study the nonepisodic setting for nonlinear systems with MPC. Moreover, they consider finite-order or exponentially fading NARX systems that lie in the RKHS of infinitely smooth functions, which they further approximate with random Fourier features (Rahimi & Recht, 2007) $\\phi$ with feature size $D$ . Further, they assume access to bounded persistently exciting inputs w.r.t. the feature matrix $\\Phi_{t}\\Phi_{t}^{\\top}$ . This assumption is generally tough to verify and common excitation strategies such as random exploration often don't perform well for nonlinear systems (Sukhija et al., 2024). The algorithm also operates in two stages, where in the first stage it performs pure exploration for system identification and in the second stage exploitation, i.e., acting greedily w.r.t. the estimated dynamics, akin to NEMEAN. Additionally, the algorithm requires the feature size $D$ to increase with the horizon $T$ . They give a regret bound of $\\mathcal{O}\\left(T^{\\bar{2}/3}\\right)$ where the regret is measured w.r.t. to the oracle MPC with access to the true dynamics. Lale et al. (2021) also assume exponential input-to-output stability of the system to avoid blow-up during exploration. Our work considers more general RKHS, naturally trades-off exploration and exploitation, does not require apriori knowledge of persistently exciting inputs and gives a regret bound of $O(\\beta_{T}\\sqrt{T\\Gamma_{T}})$ w.r.t. the optimal average cost criterion. Moreover, our regret bound is similar to the ones obtained for nonlinear systems in the episodic case and Gaussian process bandits (Srinivas et al., 2012; Chowdhury & Gopalan, 2017; Scarlett et al., 2017). To the best of our knowledge, we are the first to give such a regret bound for nonlinear systems. ", "page_idx": 8}, {"type": "text", "text": "Nonepisodic Deep RL Standard deep RL approaches often fail in the nonepisodic setting (Sharma et al., 2021b). To this end, deep RL algorithms have also been developed for the nonepisodic case. Mostly, these works focus on learning to reset and formulate it from the perspective of safety (Eysenbach et al., 2018) (avoiding undesirable states), chaining multiple controllers (Han et al., 2015), skill discovery/intrinsic exploration (Zhu et al., 2020; Xu et al., 2020), curriculum learning (Sharma et al., 2021a), and learning initial state distributions from demonstrations (Sharma et al., 2022). However, in contrast to us, none of the works above provide any theoretical guarantees. There are several extensions of model-free deep RL algorithms to the average reward setting (TRPO (Zhang & Ross, 2021), PPO (Ma et al., 2021), and DDPG (Saxena et al., 2023)). However, they mostly focus on maximizing the long-term behavior of the RL agent and allow for resets during learning. Overall, extending RL algorithms for the discounted case to the average one is still an open problem (Dewanto et al., 2020). However, future work in this direction will benefit NEoRL. Since average-reward optimizers can be used in combination with NEoRL to directly minimize the average cost in a model-based policy optimization (Janner et al., 2019) manner. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We propose, NEoRL, a novel model-based RL algorithm for the nonepisodic setting with nonlinear dynamics and continuous state and action spaces. NEoRL seeks for average-cost optimal policies and leverages the model's epistemic uncertainty to perform optimistic exploration. Similar to the episodic case (Kakade et al., 2020; Curi et al., 2020), we provide a regret bound for NEoRL of $\\dot{\\mathcal{O}}(\\beta_{T}\\sqrt{T\\Gamma_{T}})$ for Gaussian process dynamics. To our knowledge, we are the first to obtain this result in the nonepisodic setting. We compare NEoRL to other model-based RL methods on standard deep RL benchmarks. Our experiments demonstrate that NEoRL, converges to the optimal average costof $A(\\pi^{*})=0$ across all environments, suffering sublinear regret even when Bayesian neural networks are used to model the dynamics. Moreover, NEoRL outperforms all our baselines across all environments requiring only $\\sim10^{3}$ samples for learning. ", "page_idx": 9}, {"type": "text", "text": "Future work may consider deriving lower bounds on the regret of NEoRL, studying different assumptionson $f^{*}$ and $\\Pi$ , and investigating different notions of optimality such as bias optimality in the nonepisodic setting (Mahadevan, 1996). ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We would like to thank Mohammad Reza Karimi, Scott Sussex, and Armin Lederer for the insightful discussions and feedback on this work. This project has received funding from the Swiss National Science Foundation under NCCR Automation, grant agreement 51NF40 180545, and the Microsoft Swiss Joint Research Center. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Abbasi-Yadkori, Y. and Szepesvari, C. Regret bounds for the adaptive control of linear quadratic systems. In Conference on Learning Theory, 2011.   \nAbeille, M. and Lazaric, A. Efficient optimistic exploration in linear-quadratic regulators via lagrangian relaxation. In International Conference on Machine Learning, 2020.   \nAnderson, B. D. and Moore, J. B. Optimal control: linear quadratic methods. Courier Corporation, 2007.   \nAnnaswamy, A. M. Adaptive control and intersections with reinforcement learning. Annual Review of Control, Robotics, and Autonomous Systems, 2023.   \nArapostathis, A., Borkar, V. S., Fernandez-Gaucherand, E., Ghosh, M. K., and Marcus, S. I. Discrete-time controlled markov processes with average cost criterion: A survey. SIAM Journal on Control and Optimization, 1993.   \nAstrom, K. J. and Wittemark, B. Adaptive Control. Courier Corporation, 2013.   \nBartlett, P. L. and Tewari, A. Regal: A regularization based algorithm for reinforcement learning in weakly communicating mdps. arXiv preprint arXiv:1205.2661, 2012.   \nBerberich, J. and Allgower, F. An overview of systems-theoretic guarantees in data-driven model predictive control, 2024. URL https : //arxiv. org/abs/2406. 04130.   \nBrafman, R. I. and Tenenholt, M. R-max-a general polynomial me algorithm for near-optimal reinforcment learning. Journal of Machine Learning Research, 2002.   \nBrockman, G., Cheg, V,Petersn, L. Scheider, J., Shulman, J., Tang, J., and Zaremba, W. Oenai g. arXiv preprint arXiv: 1606.01540, 2016.   \nChowdhury, S. R. and Gopalan, A. On kernelized multi-armed bandits. In ICML, 2017.   \nChua,KCalraR,MAlis,  and i, pornt ain iha f trials probabilistic dynamics models. In NeurIPS, 2018.   \nCohen, A, Koren, T., and Mansour, Y. Learing linear-quadratic regulators effciently with only $\\sqrt{T}$ regret. In International Conference on Machine Learning, 2019.   \nCuri, S., Berkenkamp, F., and Krause, A. Efficient model-based reinforcement learning through optimistic policy search and planning. NeurIPS, 33:14156-14170, 2020.   \nDean, S., Mania, H., Matni, N., Recht, B., and Tu, S. On the sample complexity of the linear quadratic regulator. Foundations of Computational Mathematics, 20(4):633-679, 2020.   \nDewanto, V, Dunn, G, Eshragh, A., Gallagher, M., and Roosta, F. Average-reward model-free reinforcement learning: a systematic review and literature mapping. arXiv preprint arXiv:2010.08920, 2020.   \nEysenbach, B., Gu, S., Ibarz, J., and Levine, S. Leave no trace: Learning to reset for safe and autonomous reinforcement learning. International Conference on Learning Representations, 2018.   \nFaradonbeh, M. K. S., Tewari, A., and Michailidis, G. Optimism-based adaptive regulation of linear-quadratic systems. IEEE Transactions on Automatic Control, 2020.   \nFoster, D., Sarkar, T, and Rakhlin, A. Learning nonlinear dynamical systems from a single trajectory. In Learning for Dynamics and Control, 2020.   \nGarciaC.E, Pret,D M., and Morari, M Mel preditive conrol: Thery and pratic a survey Amatia, PP. 335-348, 1989.   \nHa, D. and Schmidhuber, J. Recurrent world models facilitate policy evolution. Advances in neural information processing systems, 31, 2018.   \nHafer, D., Lillicra, T, Fher, I, Villegas, R,Ha,D., L,H, and Davidn, J. Lain latt dya fo planning from pixels. In International conference on machine learning, 2019.   \nHafner, D, Lillicrap, T, Ba J, and Norouzi, M. Dream to control: Learning behaviors by ltent imagination. ICLR, 2020.   \nHairer, M. and Mattingly, J. C. Yet another look at harris' ergodic theorem for markov chains. In Seminar on Stochastic Analysis, Random Fields and Applications VI: Centro Stfano Franscini, Ascona, May 2008,pp. 109-117. Springer, 2011.   \nHan, W., Levine, S., and Abbeel, P Leaming compound multi-step controllers uder unknown dynamics. In Intelligent Robots and Systems (IROS), 2015.   \nJaksch, T, Ortner, R., and Auer, P. Near-optimal regret bounds for reinforcement leaning. Journal of Machine Learning Research, 2010.   \nJanner, M., Fu, J., Zhang, M., and Levine, S. When to trust your model: Model-based policy optimization. Advances in neural information processing systems, 2019.   \nKabzan, J,Vl, M I, Rejwart, J,Hdrikx,HF,Eke, C.,Praaat, M,Ber,A, Gsala, N, Ga, M., Sivanesan, R., e al. Amz driverless: The full autonomous racing system. Journal of Field Robotics, 2020.   \nKakade, S., Krishnamurthy, A., Lowrey, K., Ohnishi, M., and Sun, W. Information theoretic regret bounds for online nonlinear control. NeurIPS, 33:15312-15325, 2020.   \nKakade, S. M. On the sample complexity of reinforcement learning. University of London, University College London (United Kingdom), 2003.   \nKearns, M. and Singh, S. Near-optimal reinforcement learning in polynomial time. Machine learning, 2002.   \nKhalil, H. K. Nonlinear control, volume 406. Pearson New York, 2015.   \nKipf, T, VanderPol E, and Welling, M. Contrastive learning of structured world mdels. arXiv prent arXiv: 1911.12247, 2019.   \nKrstic, M., Kanellakopoulos, I., and Kokotovic, P. Adaptive nonlinear control without overparametrization. Systems & Control Letters, 1992.   \nKrstic, M., Kokotovic, P. V., and Kanellakopoulos, I. Nonlinear and adaptive control design. John Wiley & Sons, Inc., 1995.   \nKuleshov, V., Fener, N, and Ermon, S. Accurate uncertainties for deep arning using calibrated regression. In ICML, Pp. 2796-2804. PMLR, 2018.   \nLai, T. L. and Wei, C.Z. Least squares estimates in stochastic regression models with applications to identification and control of dynamic systems. The Annals of Statistics, 1982.   \nLai, T. L. and Wei, C.-Z. Asymptotically effcient self-tuning regulators. SIAM Journal on Control and Optimization, 1987.   \nLakshminarayanan, B., Pritzel, A., and Blundell, C. Simple and scalable predictive uncertainty estimation using deep ensembles, 2017.   \nLale, S., Azizzadenesheli, K., Hassibi, B., and Anandkumar, A. Logarithmic regret bound in partially observable linear dynamical systems. Advances in Neural Information Processing Systems, 2020.   \nLale, S., Azizzadenesheli, K., Hassibi, B., and Anandkumar, A. Model learning predictive control in nonlinear dynamical systems. In Conference on Decision and Control (CDC). IEEE, 2021.   \nMa,X, Tan, X., Xia, L, Yan, J., and Zha, Q.Averag-reward refrcmnt learning with trust rin methods. International Joint Conference on Artificial Intelligence, 2021.   \nMahadevan, S.Average reward reinforcment earing: Foundations, alorithms, and mpirica results. Machine learning, 1996.   \nMania, H, Jrdan, ML,ad Reht, .Activ leaing for nnlinear system idntifcatin with uarate. arXiv preprint arXiv:2006.10277, 2020.   \nMeyn, S. P. and Tweedie, R. L. Markov chains and stochastic stability. Springer Science & Business Media, 2012.   \nOsband, I. and Van Roy, B. Why is posterior sampling beter than optimism for reinforcement learning? In International conference on machine learning, 2017.   \nOuyang,YGaraniM,Nayar,A and JinLeain kwmardis proes  tn sampling approach. Advances in neural information processing systems, 30, 2017.   \nPinneri, C., Sawant, S., Blaes, S., Achterhold, J., Stueckler, J., Rolinek, M., and Martius, G. Sample-efficient cross-entropy method for real-time planning. In CORL, Proceedings of Machine Learning Research, pp. 1049-1065, 2021.   \nPuterman, M. L. Markov decision processes: discrete stochastic dynamic programming. John Wiley & Sons, 2014.   \nRahimi, A. and Recht, B. Random features for large-scale kernel machines. Advances in neural information processing systems, 20, 2007.   \nRothfuss, J., Sukhija, B., Birchler, T., Kassraie, P., and Krause, A. Hallucinated adversarial control for conservative offine policy evaluation. UAl, 2023.   \nSattar, Y. and Oymak, S. Non-asymptotic and accurate learning of nonlinear dynamical systems. Journal of Machine Learning Research, 2022.   \nSaxena, , Khastagir, S., Kolathaya, S, ad hataar, S.Off-poliy averae rward actorcritc wth deterministic policy search. In International Conference on Machine Learning, 2023.   \nScarlet,guvi, aCh, Lweus n ret  Gaiasbat za In Conference on Learning Theory, 2017.   \nSharma, A., Gupta, A., Levine, S., Hausman, K., and Finn, C. Autonomous reinforcement learning via subgoal curricula. Advances in Neural Information Processing Systems, 2021a.   \nSharma, A., Xu, K., Sardana, N., Gupta, A., Hausman, K., Levine, S., and Finn, C. Autonomous reinforcement learning: Formalism and benchmarking. arXiv preprint arXiv:2112.09605, 2021b.   \nSharma, A., Ahmad, R., and Finn, C. A state-distribution matching approach to non-episodic reinforcement learning. International Conference on Machine Learning, 2022.   \nSimchowitz, M. and Foster, D. Naive exploration is optimal for online lqr. In International Conference on Machine Learning. PMLR, 2020.   \nSrinivas, N., Krause, A., Kakade, S. M., and Seeger, M. W. Information-theoretic regret bounds for gaussian process optimization in the bandit setting. IEEE Transactions on Information Theory, 2012.   \nSukhija, B., Treven, L., Sancaktar, C., Blaes, S., Coros, S., and Krause, A. Optimistic active exploration of dynamical systems. NeurIPS, 2024.   \nSussex, S., Makarova, A., and Krause, A. Model-based causal bayesian optimization. In ICLR, May 2023.   \nTassa, Y, Doron, Y., Muldal, A., Erez, T., Li, Y., Casas, D. d. L., Budden, D., Abdolmaleki, A., Merel, J., Lefrancq, A., et al. Deepmind control suite. arXiv preprint arXiv: 1801.00690, 2018.   \nTekinalp, A., Kim, S. H., Bhosale, Y, Parthasarathy, T, Naughton, N., Albazroun, A., Joon, R., Cui, S., Nasiriziba, I., Stolzle, M., Shih, C.-H. C., and Gazzola, M. Gazzolalab/pyelastica: v0.3.2, 2024.   \nTreven, L., Curi, S., Mutny, M., and Krause, A. Learning stabilizing controllers for unstable linear quadratic regulators from a single trajectory. In Learning for Dynamics and Control, 2021.   \nTreven, L., Hubotter, J., Sukhija, B., Dorfler, F, and Krause, A. Efficient exploration in continuous-time model-based reinforcement learning. NeurIPS, 2024.   \nVakili, S., Khezeli, K., and Picheny, V. On information gain and regret bounds in gaussian proces bandits. In AISTATS, 2021.   \nWagenmaker, A., Shi, G., and Jamieson, K. Optimal exploration for model-based rl in nonlinear systems. arXiv preprint arXiv:2306.09210, 2023.   \nWilliams, G., Wagener, N., Goldfain, B., Drews, P, Rehg, J. M., Boots, B., and Theodorou, E. A. Information theoretic mpc for model-based reinforcement learning. In ICRA, 2017.   \nXu, K., Verma, S., Finn, C., and Levine, S. Continual learning of control primitives: Skill discovery via reset-games. Advances in Neural Information Processing Systems, 2020.   \nXu, Z. and Tewari, A. Reinforcement learning in factored mdps: Oracle-efficient algorithms and tighter regret bounds for the non-episodic setting. Advances in Neural Information Processing Systems, 2020.   \nZhang, Y. and Ross, K. W. On-policy deep reinforcement learning for the average-reward criterion. In International Conference on Machine Learning, 2021.   \nZhao, F., Dorfler, F, Chiuso, A., and You, K. Data-enabled policy optimization for direct adaptive learning of the lqr. arXiv preprint arXiv:2401.14871, 2024.   \nZhu, H., Yu, J., Gupta, A., Shah, D., Hartikainen, K., Singh, A., Kumar, V., and Levine, S. The ingredients of real-world robotic reinforcement learning. arXiv preprint arXiv:2004.12570, 2020. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendices ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we prove Theorem 2.6 and Theorem 3.1. First, we start with the proof of Lemma 2.5. ", "page_idx": 13}, {"type": "text", "text": "Proof of Lemma 2.5. We first analyze the following term $\\mathbb{E}_{w}[V(f^{*}(x,\\pi(x))~+~w)~-$ $V(f^{*}(x,\\pi_{s}(x))+w)]$ for any $\\pi\\in\\Pi$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{\\mathbb{E}_{w}[V(f^{*}(x,\\pi(x))+w)-V(f^{*}(x,\\pi_{s}(x))+w)]}\\\\ &{\\quad\\le\\mathbb{E}_{w}[\\kappa(\\|f^{*}(x,\\pi(x))+w-(f^{*}(x,\\pi_{s}(x))+w)\\|)]}&&{\\quad{\\mathrm{(Uniform~continuity~of~}}V)}\\\\ &{\\quad=\\kappa(\\|f^{*}(x,\\pi(x))-f^{*}(x,\\pi_{s}(x))\\|)}\\\\ &{\\quad\\le\\kappa(\\kappa f^{*}(\\|\\pi(x)-\\pi_{s}(x)\\|))}&&{\\quad{\\mathrm{(Uniform~continuity~of~}}f^{*})}\\\\ &{\\quad\\le\\kappa(\\kappa_{f^{*}}(2u_{\\mathrm{max}})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Therefore, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}_{\\boldsymbol{x}^{\\prime}\\mid\\pi,\\boldsymbol{x}}[V(\\boldsymbol{x}^{\\prime})]=\\mathbb{E}_{\\boldsymbol{w}}[V(f^{*}(\\boldsymbol{x},\\pi(\\boldsymbol{x}))+\\boldsymbol{w})]}}\\\\ &{}&{\\le\\mathbb{E}_{\\boldsymbol{w}}[V(f^{*}(\\boldsymbol{x},\\pi_{s}(\\boldsymbol{x}))+\\boldsymbol{w})]+\\kappa(\\kappa_{f^{*}}(2u_{\\mathrm{max}}))}\\\\ &{}&{=\\mathbb{E}_{\\boldsymbol{x}^{\\prime}\\mid\\pi_{s},x}[V(\\boldsymbol{x}^{\\prime})]+\\kappa(\\kappa_{f^{*}}(2u_{\\mathrm{max}}))}\\\\ &{}&{\\le\\gamma V(\\boldsymbol{x})+K+\\kappa(\\kappa_{f^{*}}(2u_{\\mathrm{max}}))}\\\\ &{}&{=\\gamma V(\\boldsymbol{x})+\\tilde{K}}&{(\\tilde{K}=K+\\kappa(\\kappa_{f^{*}}(2u_{\\mathrm{max}})))}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Hence, $V$ satisfies the drift condition for $\\pi$ . Furthermore, since $V$ also satisfies positive definiteness by assumption, the bounded energy condition holds for all $\\pi\\in\\Pi$ \u53e3 ", "page_idx": 13}, {"type": "text", "text": "A.1 Proof of Theorem 2.6 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "For proving Theorem 2.6, we invoke the results from (Hairer & Mattingly, 2011, Theorem 1.2 - 1.3). For this we require that the Markov chain induced by a policy $\\pi$ satisfies the drift condition. In our setting, this corresponds to Assumption 2.4. Next, we show that the chain satisfies the following minorisation condition. ", "page_idx": 13}, {"type": "text", "text": "Lemma A.1 (Minorisation condition). Consider the system in Equation (1) and let Assumption $2.I-2.4$ hold. Let $P^{\\pi}$ denote the transition kernel for the policy $\\pi\\in\\Pi_{:}$ i.e.. ${\\cal P}^{\\pi}(x,{\\mathcal A})=$ $\\mathbb{P}(\\pmb{x}_{+}\\in\\mathcal{A}|\\pmb{x},\\pmb{\\pi}(\\pmb{x}))$ . Then, for all $\\pi\\in\\Pi$ . exists a constant $\\alpha\\in(0,1)$ and aprobabilitymeasure $\\zeta(\\cdot)$ S.t., ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\pmb{x}\\in\\mathcal{C}}P^{\\pi}(\\pmb{x},\\cdot)\\geq\\alpha\\zeta(\\cdot)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "with ${\\mathcal{C}}\\,{\\stackrel{\\mathrm{def}}{=}}\\,\\{{\\pmb x}\\in{\\mathcal{X}};V^{\\pmb{\\tau}}({\\pmb x})\\leq R\\}$ for some R > 2K/1- ", "page_idx": 13}, {"type": "text", "text": "Proof. We prove it in 3 steps. First, we show that $\\mathcal{C}$ is contained in a compact domain. From the Assumption 2.4 we pick the function $\\xi\\in\\mathcal{K}_{\\infty}$ . Since $\\begin{array}{r}{C_{l}\\xi(0)=0,\\operatorname*{lim}_{s\\to\\infty}\\bar{\\xi}(s)=+\\infty}\\end{array}$ and $C_{l}\\xi$ is continuous, there exists $M$ such that $C_{l}\\xi(M)=R$ . Then for $\\|\\pmb{x}\\|>M$ we have: ", "page_idx": 13}, {"type": "equation", "text": "$$\nV^{\\pi}(x)\\geq C_{l}\\xi(\\|x\\|)>\\xi(M)=R.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Thereforewehave: ${\\mathcal{C}}\\ \\subseteq\\ B(\\mathbf{0},M)\\,{\\stackrel{\\mathrm{def}}{=}}\\{\\mathbf{x}\\ |\\ \\|x-\\mathbf{0}\\|\\ \\leq\\ M\\}$ .In the second step we show that $\\mathbf{\\vec{f}}(\\mathcal{C},\\pi(\\mathcal{C}))$ is bounded, in particular we show that there exists $B\\,>\\,0$ such that: $\\pmb{f}(\\mathcal{C},\\pi(\\mathcal{C}))\\subseteq$ $B(\\mathbf{0},B)$ . This is true since continuous image of compact set is compact and the observation: ", "page_idx": 13}, {"type": "equation", "text": "$$\n{\\mathcal{C}}\\subseteq\\mathcal{B}(\\mathbf{0},M)\\implies f({\\mathcal{C}},\\pi({\\mathcal{C}}))\\subseteq f({\\mathcal{B}}(\\mathbf{0},M),\\pi({\\mathcal{B}}(\\mathbf{0},M))).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Since ${\\pmb f}({\\cal B}({\\bf0},M),{\\pmb\\pi}({\\cal B}({\\bf0},M)))$ is compact there exists $B$ such that $\\pmb{f}(\\mathcal{C},\\pi(\\mathcal{C}))\\,\\subseteq\\,\\mathcal{B}(\\mathbf{0},B)$ . In the last step we prove that $\\alpha\\stackrel{\\mathrm{def}}{=}2^{-d_{x}}e^{-B^{2}/\\sigma^{2}}$ and $\\zeta$ with law of $\\textstyle N\\left(0,{\\frac{\\sigma^{2}}{2}}\\right)$ satisfy condition of Lemma A.1. It is enough to show that $\\forall\\pmb{\\mu}\\in\\mathcal{B}(\\mathbf{0},B),\\forall\\pmb{x}\\in\\mathbb{R}^{d_{\\mathbf{x}}}$ we have: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\alpha\\frac{1}{(2\\pi)^{\\frac{d_{x}}{2}}\\left(\\frac{\\sigma^{2}}{2}\\right)^{\\frac{d_{x}}{2}}}e^{-\\frac{\\|{\\bf x}\\|^{2}}{\\sigma^{2}}}\\leq\\frac{1}{(2\\pi)^{\\frac{d_{x}}{2}}(\\sigma^{2})^{\\frac{d_{x}}{2}}}e^{-\\frac{\\|{\\bf x}-{\\boldsymbol{\\mu}}\\|^{2}}{2\\sigma^{2}}}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "which can be proven with simple algebraic manipulations. ", "page_idx": 13}, {"type": "text", "text": "Through the minorisation condition and Assumption 2.4, we can prove the ergodicity of the closedloop system for a given policy $\\pi\\in\\Pi$ ", "page_idx": 14}, {"type": "text", "text": "Theorem A.2 (Ergodicity of closed-loop system). Let Assumption 2.1 - 2.4, consider any probability measures $\\zeta_{1},\\,\\zeta_{2}$ and $\\theta>0$ define $P^{\\pi}\\zeta$ $\\|\\varphi\\|_{1+\\theta V^{\\pi}},\\,\\rho_{\\theta}^{\\pi}$ as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(P^{\\pi}\\zeta)\\left(A\\right)=\\displaystyle\\int_{X}P^{\\pi}(x,A)\\zeta(d x)}\\\\ &{\\|\\varphi\\|_{1+\\theta V^{\\pi}}=\\displaystyle\\operatorname*{sup}_{x\\in\\mathcal X}\\frac{|\\varphi(x)|}{1+\\theta V^{\\pi}(x)}}\\\\ &{\\rho_{\\theta}^{\\pi}(\\zeta_{1},\\zeta_{2})=\\displaystyle\\operatorname*{sup}_{\\varphi:\\|\\varphi\\|_{1+\\theta V^{\\pi}}\\leq1}\\int_{\\mathcal X}\\varphi(x)(\\zeta_{1}-\\zeta_{2})(d x)=\\displaystyle\\int_{X}(1+\\theta V^{\\pi}(x))|\\zeta_{1}-\\zeta_{2}|(d x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We have for all $\\pi\\in\\Pi$ that $P^{\\pi}$ admits a unique invariant measure $\\bar{P}^{\\pi}$ . Furthermore, there exist constants $C_{1}>0$ $\\theta>0$ $\\lambda\\in(0,1)$ such that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\rho_{\\theta}^{\\pi}(P^{\\pi}\\zeta_{1},P^{\\pi}\\zeta_{2})\\leq\\lambda\\rho_{\\theta}^{\\pi}(\\zeta_{1},\\zeta_{2})}\\\\ {\\left\\|{\\mathbb{E}_{x\\sim(P^{\\pi})^{t}}}\\left[\\varphi(\\pmb{x})\\right]-{\\mathbb{E}_{\\pmb{x}\\sim\\tilde{P}^{\\pi}}}\\left[\\varphi(\\pmb{x})\\right]\\right\\|_{1+V^{\\pi}}\\leq C_{1}\\lambda^{t}\\left\\|\\varphi-{\\mathbb{E}_{\\pmb{x}\\sim\\tilde{P}^{\\pi}}}\\left[\\varphi(\\pmb{x})\\right]\\right\\|_{1+V^{\\pi}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "holds for every measurable function $\\varphi:\\mathcal{X}\\rightarrow\\mathcal{R}$ with $\\|\\varphi\\|_{1+V^{\\pi}}<\\infty$ Here $(P^{\\pi})^{t}$ denotes the $t$ -step transition kernel under the policy $\\pi$ ", "page_idx": 14}, {"type": "text", "text": "Moreover, $\\theta=\\alpha_{0}/\\kappa$ and ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\lambda=\\operatorname*{max}\\left\\{1-(\\alpha-\\alpha_{0}),\\frac{2+{R}/{K\\alpha_{0}\\gamma_{0}}}{2+{R}/{K\\alpha_{0}}}\\right\\}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for any $\\alpha_{0}\\in(0,\\alpha)$ and $\\gamma_{0}\\in(\\gamma+2^{K}/R,1)$ ", "page_idx": 14}, {"type": "text", "text": "Proof. From Assumption 2.4, we have a value function for each policy that satisfies the drift condition. Furthermore, in Lemma A.1 we show that our system also satisfies the minorisation condition for all policies. Under these conditions, we can use the results from Hairer & Mattingly (2011, Theorem 1.2. -1.3.). \u53e3 ", "page_idx": 14}, {"type": "text", "text": "Note that $\\lVert\\cdot\\rVert_{1+\\theta V^{\\pi}}$ represents a family of equivalent norms for any $\\theta>0$ . Now we prove Theorem 2.6. ", "page_idx": 14}, {"type": "text", "text": "Proof of Theorem 2.6. From Theorem A.2, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\rho_{\\theta}^{\\pi}((P^{\\pi})^{t+1},(P^{\\pi})^{t})=\\rho_{\\theta}^{\\pi}(P^{\\pi}(P^{\\pi})^{t},P^{\\pi}(P^{\\pi})^{t-1})\\le\\lambda^{t}\\rho_{\\theta}^{\\pi}(P^{\\pi}\\delta_{x_{0}},\\delta_{x_{0}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\delta_{{\\pmb x}_{0}}$ is the dirac measure. Therefore, $(P^{\\pi})^{t}$ is a Cauchy sequence. Furthermore, $\\rho_{\\boldsymbol{\\theta}}^{\\pi}$ is complete for the set of probability measures integrating $V$ , thus $\\rho_{\\theta}^{\\pi}((\\bar{P^{\\pi}})^{\\bar{t}},\\bar{P}^{\\pi})\\rightarrow0$ for $t\\to\\infty$ (c.f., Hairer & Mattingly (2011) for more details). In particular, we have for $\\varphi$ such that $\\|\\varphi\\|_{1+\\theta V^{\\pi}}\\leq1$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{t\\rightarrow\\infty}\\int_{\\mathcal{X}}\\varphi(\\pmb{x})(P^{\\pi})^{t}(d\\pmb{x})=\\int_{\\mathcal{X}}\\varphi(\\pmb{x})\\bar{P}^{\\pi}(d\\pmb{x}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Note that since all $\\lVert\\cdot\\rVert_{1+\\theta V^{\\pi}}$ norms are equivalent for $\\theta>0$ if $\\|c\\|_{1+V^{\\pi}}\\leq C$ (Assumption 2.4), then $\\|c\\|_{1+\\theta V^{\\pi}}\\leq C^{\\prime}$ for some $C^{\\prime}\\in(0,\\infty)$ . Furthermore, note that $c(\\cdot)\\geq0$ . Therefore, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\int_{\\mathcal{X}}c(\\mathbf{x})\\bar{P}^{\\pi}(d\\mathbf{x})=\\operatorname*{lim}_{t\\to\\infty}\\int_{\\mathcal{X}}c(\\mathbf{x})(P^{\\pi})^{t}(d\\mathbf{x})}}\\\\ &{\\leq C\\displaystyle\\operatorname*{lim}_{t\\to\\infty}\\int_{\\mathcal{X}}(1+V^{\\pi}(\\mathbf{x}))(P^{\\pi})^{t}(d\\mathbf{x})}\\\\ &{=C+C\\displaystyle\\operatorname*{lim}_{t\\to\\infty}\\mathbb{E}_{\\mathbf{x}\\sim(P^{\\pi})^{t}}[V^{\\pi}(\\mathbf{x})]}\\\\ &{=C+C\\displaystyle\\operatorname*{lim}_{t\\to\\infty}\\mathbb{E}_{\\mathbf{x}\\sim(P^{\\pi})^{t-1}}[\\mathbb{E}_{\\mathbf{x}^{\\prime}\\sim(P^{\\pi})}[V^{\\pi}(\\mathbf{x}^{\\prime})|\\mathbf{x}]]}\\\\ &{\\leq C+C\\left(\\displaystyle\\operatorname*{lim}_{t\\to\\infty}\\gamma\\mathbb{E}_{\\mathbf{x}\\sim(P^{\\pi})^{t-1}}[V^{\\pi}(\\mathbf{x})]+K\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\leq C+C\\displaystyle\\operatorname*{lim}_{t\\rightarrow\\infty}\\gamma^{t}V^{\\pi}(\\pmb{x}_{0})+K\\frac{1-\\gamma^{t}}{1-\\gamma}}}\\\\ {{=C\\left(1+K\\frac{1}{1-\\gamma}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "In summary, we have $\\begin{array}{r}{\\mathbb{E}_{{\\pmb x}\\sim\\bar{P}^{\\pi}}\\left[c({\\pmb x})\\right]\\le C\\left(1+K\\frac{1}{1-\\gamma}\\right)}\\end{array}$ ", "page_idx": 15}, {"type": "text", "text": "Consider any $t>0$ , and note that from Theorem A.2 we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\mathbb{E}_{\\mathbf{x}\\sim(P^{\\pi})^{t}}\\left[c(\\pmb{x})\\right]-\\mathbb{E}_{\\mathbf{x}\\sim\\bar{P}^{\\pi}}\\left[c(\\pmb{x})\\right]\\right\\|_{1+V^{\\pi}}=\\underset{x_{0}\\in\\mathcal{X}}{\\operatorname*{sup}}\\frac{\\left|\\mathbb{E}_{\\mathbf{x}\\sim(P^{\\pi})^{t}}\\left[c(\\pmb{x})\\right]-\\mathbb{E}_{\\mathbf{x}\\sim\\bar{P}^{\\pi}}\\left[c(\\pmb{x})\\right]\\right|}{1+V^{\\pi}\\left(x_{0}\\right)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq C_{1}\\lambda^{t}\\left\\|c-\\mathbb{E}_{\\mathbf{x}\\sim\\bar{P}^{\\pi}}\\left[c(\\pmb{x})\\right]\\right\\|_{1+V^{\\pi}}\\quad\\mathrm{~(Theorem~\\mathbb{A}.2)}}\\\\ &{\\qquad\\qquad\\qquad\\leq C_{1}\\lambda^{t}\\left\\|c\\right\\|_{1+V^{\\pi}}+C_{1}\\lambda^{t}\\mathbb{E}_{\\mathbf{x}\\sim\\bar{P}^{\\pi}}\\left[c(\\pmb{x})\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=C_{2}\\lambda^{t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\begin{array}{r}{C_{2}=C_{1}(\\|c\\|_{1+V^{\\pi}}+C K\\frac{1}{1-\\gamma})}\\end{array}$ ", "page_idx": 15}, {"type": "text", "text": "Moreover, since the inequality holds for all $\\scriptstyle x_{0}$ ,we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{|\\mathbb{E}_{{\\pmb x}\\sim(P^{\\pi})^{t}}\\left[c({\\pmb x})\\right]-\\mathbb{E}_{{\\pmb x}\\sim\\bar{P}^{\\pi}}\\left[c({\\pmb x})\\right]|}{1+V^{\\pi}({\\pmb x}_{0})}\\leq C_{2}{\\lambda}^{t}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "In summary, ", "page_idx": 15}, {"type": "equation", "text": "$$\n|\\mathbb{E}_{{\\pmb x}\\sim(P^{\\pi})^{t}}\\left[c({\\pmb x})\\right]-\\mathbb{E}_{{\\pmb x}\\sim\\bar{P}^{\\pi}}\\left[c({\\pmb x})\\right]|\\le C_{2}(1+V^{\\pi}({\\pmb x}_{0}))\\lambda^{t}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Consider any $T\\geq0$ , and define with $\\bar{c}=\\mathbb{E}_{{\\pmb{x}}\\sim\\bar{P}^{\\pi}}\\left[c({\\pmb{x}},{\\pmb\\pi}(x))\\right]$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\sum_{t=0}^{T-1}c(\\boldsymbol x_{t},\\boldsymbol u_{t})-\\bar{c}\\right]=\\sum_{t=0}^{T-1}\\mathbb{E}_{(P^{\\pi})^{t}}\\left[c(\\boldsymbol x_{t},\\boldsymbol u_{t})\\right]-\\bar{c}}\\\\ {\\displaystyle\\leq\\sum_{t=0}^{T-1}\\left|\\mathbb{E}_{(P^{\\pi})^{t}}\\left[c(\\boldsymbol x_{t},\\boldsymbol u_{t})\\right]-\\bar{c}\\right|}\\\\ {\\displaystyle\\leq C_{2}(1+V^{\\pi}(\\boldsymbol x_{0}))\\sum_{t=0}^{T-1}\\lambda^{t}}\\\\ {\\displaystyle=C_{2}(1+V^{\\pi}(\\boldsymbol x_{0}))\\frac{1-\\lambda^{T}}{1-\\lambda}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Hence, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{T\\to\\infty}\\left|\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\sum_{t=0}^{T-1}c(\\boldsymbol x_{t},\\boldsymbol u_{t})-\\bar{c}\\right]\\right|\\leq C_{2}(1+V^{\\pi}(\\boldsymbol x_{0}))\\frac{1}{1-\\lambda},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and for any $\\pmb{x}_{0}$ in a compact subset of $\\mathcal{X}$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{T\\to\\infty}\\frac{1}{T}\\mathbb{E}_{\\pmb{\\pi}}\\left[\\sum_{t=0}^{T-1}c(\\pmb{x}_{t},\\pmb{u}_{t})-\\bar{c}\\right]=0.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Moreover, ", "page_idx": 15}, {"type": "equation", "text": "$$\n|B(\\pmb{\\pi},\\pmb{x}_{0})|\\leq C_{2}(1+V^{\\pi}(\\pmb{x}_{0}))\\frac{1}{1-\\lambda}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Another interesting, inequality that follows from the proof above is the difference in bias inequality. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}_{\\mathbf{x}_{0}\\sim\\zeta_{1}}[B(\\pmb{\\pi},\\mathbf{x}_{0})]-\\mathbb{E}_{\\mathbf{x}_{0}\\sim\\zeta_{2}}[B(\\pmb{\\pi},\\mathbf{x}_{0})]\\right|\\le\\frac{C_{3}}{1-\\lambda}\\int_{\\mathcal{X}}(1+V^{\\pi}(\\pmb{x}))\\left|\\zeta_{1}-\\zeta_{2}\\right|(d x)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "for all probability measures $\\zeta_{1},\\zeta_{2}$ . To show this holds, define $\\begin{array}{r}{C^{\\prime}=\\operatorname*{max}_{\\pmb{\\pi}\\in\\Pi}\\|c(\\pmb{x},\\pmb{\\pi}(\\pmb{x}))\\|_{1+\\theta V^{\\pi}}}\\end{array}$ Furthermore, note that $C^{\\prime}<\\infty$ from Assumption 2.4 and $\\left\\|c(\\pmb{x},\\pmb{\\pi}(\\pmb{x}))\\middle/C^{\\prime}\\right\\|_{1+\\theta V^{\\pi}}\\leq1$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\big|\\mathbb{E}_{x\\sim(P^{\\pi})^{t}\\zeta_{1}}c(x,\\pi(x))-\\mathbb{E}_{x\\sim(P^{\\pi})^{t}\\zeta_{2}}c(x,\\pi(x))\\big|=\\bigg|\\int_{\\mathcal{X}}c(x,\\pi(x))((P^{\\pi})^{t}\\zeta_{1}-(P^{\\pi})^{t}\\zeta_{2})(d x)\\bigg|}\\\\ &{=C^{\\prime}\\bigg|\\int_{\\mathcal{X}}\\frac{1}{C^{\\prime}}c(x,\\pi(x))((P^{\\pi})^{t}\\zeta_{1}-(P^{\\pi})^{t}\\zeta_{2})(d x)\\bigg|}\\\\ &{\\le C^{\\prime}\\quad\\operatorname*{sup}_{\\varphi:\\vert\\varphi\\vert_{1+\\theta^{\\nu\\pi}}\\le1}\\int_{\\mathcal{X}}\\varphi(x)((P^{\\pi})^{t}\\zeta_{1}-(P^{\\pi})^{t}\\zeta_{2})(d x)=C^{\\prime}\\rho_{\\theta}^{\\pi}((P^{\\pi})^{t}\\zeta_{1},(P^{\\pi})^{t}\\zeta_{2})}\\\\ &{\\le C^{\\prime}\\lambda\\rho_{\\theta}^{\\pi}((P^{\\pi})^{t-1}\\zeta_{1},(P^{\\pi})^{t-1}\\zeta_{2})}\\\\ &{\\le C^{\\prime\\prime}\\lambda^{t}\\cdot\\pi(\\varphi\\cdot\\big)\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Also, note that there xists $C_{\\theta}\\in(0,\\infty)$ such that $C_{\\theta}\\left\\|\\varphi\\right\\|_{1+\\theta V^{\\pi}}\\geq\\|\\varphi\\|_{1+V^{\\pi}}$ due to the equivalence of the two norms. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\rho_{\\theta}^{\\pi}(\\zeta_{1},\\zeta_{2})=\\underset{\\varphi:\\,\\|\\varphi\\|_{1+\\theta V^{\\pi}}\\leq1}{\\operatorname*{sup}}\\int_{\\mathcal X}\\varphi(\\pmb{x})(\\zeta_{1}-\\zeta_{2})(d\\pmb{x})}\\\\ &{\\qquad\\qquad\\leq\\underset{\\varphi:\\,\\|\\varphi\\|_{1+V^{\\pi}}\\leq C_{\\theta}}{\\operatorname*{sup}}\\int_{\\mathcal X}\\varphi(\\pmb{x})(\\zeta_{1}-\\zeta_{2})(d\\pmb{x})}\\\\ &{\\qquad=C_{\\theta}\\,\\underset{\\varphi:\\,\\|\\varphi\\|_{1+V^{\\pi}}\\leq1}{\\operatorname*{sup}}\\int_{\\mathcal X}\\varphi(\\pmb{x})(\\zeta_{1}-\\zeta_{2})(d\\pmb{x})}\\\\ &{\\qquad=C_{\\theta}\\rho_{1}^{\\pi}(\\zeta_{1},\\zeta_{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, for the bias we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbb{E}_{x_{0}\\sim\\zeta_{1}}[B(\\pi,x_{0})]-\\mathbb{E}_{x_{0}\\sim\\zeta_{2}}[B(\\pi,x_{0})]\\|}\\\\ &{\\le\\displaystyle\\operatorname*{lim}_{T\\to\\infty}\\sum_{t=0}^{T-1}\\Big|\\mathbb{E}_{x\\sim(P^{\\pi})^{t}\\zeta_{1}}c(x,\\pi(x))-\\mathbb{E}_{x\\sim(P^{\\pi})^{t}\\zeta_{2}}c(x,\\pi(x))\\Big|}\\\\ &{\\le C^{\\prime}\\rho_{\\theta}^{\\pi}(\\zeta_{1},\\zeta_{2})\\displaystyle\\operatorname*{lim}_{T\\to\\infty}\\sum_{t=0}^{T-1}\\lambda^{t}=\\displaystyle\\frac{C^{\\prime}}{1-\\lambda}\\rho_{\\theta}^{\\pi}(\\zeta_{1},\\zeta_{2})}\\\\ &{\\le\\displaystyle\\frac{C^{\\prime}C_{\\theta}}{1-\\lambda}\\rho_{1}^{\\pi}(\\zeta_{1},\\zeta_{2})=\\displaystyle\\frac{C^{\\prime}C_{\\theta}}{1-\\lambda}\\int_{x}(1+V^{\\pi}(x))\\left|\\zeta_{1}-\\zeta_{2}\\right|(d x)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Set $C_{3}=C^{\\prime}C_{\\theta}$ ", "page_idx": 16}, {"type": "text", "text": "A.2 Proof of bounded average cost for the optimistic system ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we show that the results from Theorem 2.6 also transfer over to the optimistic dynamics. ", "page_idx": 16}, {"type": "text", "text": "Theorem A.3 (Existence of Average Cost Solution for the Optimistic System). Let Assumption $2.l-$ 2.8hold.Considerany $n>0$ and let $\\pi_{n},f_{n}$ denote thesolutiontoEquation(6), $P^{\\pi,f_{n}}$ its transition kernel. Then $P^{\\pi,f_{n}}$ admits $a$ uniqueinvariantmeasure ${\\bar{P}}^{\\pi_{n},f_{n}}$ and there exists $C_{2},C_{3}\\in(0,\\infty)$ \uff0c $\\hat{\\lambda}\\in(0,1)$ such that ", "page_idx": 16}, {"type": "text", "text": "Average Cost; ", "page_idx": 16}, {"type": "equation", "text": "$$\nA(\\pi_{n},f_{n})=\\operatorname*{lim}_{T\\to\\infty}\\frac{1}{T}\\mathbb{E}_{\\pi_{n},f_{n}}\\left[\\sum_{t=0}^{T-1}c(x_{t},u_{t})\\right]=\\mathbb{E}_{x\\sim\\bar{P}^{\\pi_{n},f_{n}}}\\left[c(x,\\pi_{n}(x))\\right]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Bias Cost; ", "page_idx": 16}, {"type": "equation", "text": "$$\n|B(\\pi_{n},f_{n},x_{0})|=\\left|\\operatorname*{lim}_{T\\to\\infty}\\mathbb{E}_{\\pi_{n},f_{n}}\\left[\\sum_{t=0}^{T-1}c(x_{t},u_{t})-A(\\pi_{n},f_{n})\\right]\\right|\\le C_{2}(1+V^{\\pi_{n}}(x_{0}))\\frac{1}{1-\\hat{\\lambda}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for all $\\pmb{x}_{0}\\in\\mathcal{X}$ ", "page_idx": 16}, {"type": "text", "text": "Difference in Bias; ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}_{x_{0}\\sim\\zeta_{1}}[B(\\pi_{n},f_{n},x_{0})]-\\mathbb{E}_{x_{0}\\sim\\zeta_{2}}[B(\\pi_{n},f_{n},x_{0})]\\right|\\le\\frac{C_{3}}{1-\\hat{\\lambda}}\\int_{\\mathcal{X}}(1+V^{\\pi}(x))\\left|\\zeta_{1}-\\zeta_{2}\\right|(d x)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for all probability measures $\\zeta_{1},\\zeta_{2}$ ", "page_idx": 17}, {"type": "text", "text": "Theorem A.3 shows that the optimistic dynamics $\\pmb{f}_{n}$ retain the boundedness property from the truedynamics $f^{*}$ and give a well-defined solution w.r.t. average cost and the bias cost. To prove Theorem A.3 we show that the optimistic system also satisfies the drift and minorisation condition. Then we can invoke the result from Hairer & Mattingly (2011) similar to the proof of Theorem 2.6. ", "page_idx": 17}, {"type": "text", "text": "Lemma A.4 (Stability of optimistic system). Let Assumption $2.I-2.8$ hold, then we have with probability at least $1-\\delta$ for all $n\\geq0$ $\\pi\\in\\Pi$ $\\pmb{f}\\in\\mathcal{M}_{n}\\cap\\mathcal{M}_{0}$ , that there exists a constant K > 0 such that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\mathbb E}_{{\\pmb{x}}_{+}|{\\pmb{x}},{\\pmb{f}},{\\pmb{\\pi}}}[V^{\\pmb{\\pi}}({\\pmb{x}}_{+})]\\leq\\gamma V^{\\pmb{\\pi}}({\\pmb{x}})+\\hat{K},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where ${\\pmb x}_{+}={\\pmb f}({\\pmb x},{\\pmb\\pi}({\\pmb x})+{\\pmb w}$ ", "page_idx": 17}, {"type": "text", "text": "Proof.Note,that $V^{\\pi}$ is uniformly continuous w.r.t. $\\kappa$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n|V^{\\pi}({\\pmb x})-V^{\\pi}({\\pmb x}^{\\prime})|\\leq\\kappa(\\|{\\pmb x}-{\\pmb x}^{\\prime}\\|).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Furthermore, since $\\pmb{f}\\in\\mathcal{M}_{n}\\cap\\mathcal{M}_{0}$ and therefore $\\textbf{\\em f}\\in\\mathcal{M}_{0}$ , we have that there exists some $\\pmb{\\eta}\\in$ $[-1,1]^{d x}$ such that ", "page_idx": 17}, {"type": "equation", "text": "$$\nf(x,\\pi(x))=\\mu_{0}(x.\\pi(x))+\\beta_{0}\\pmb{\\sigma}_{0}(x,\\pi(x))\\eta(x).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}_{w}[V^{\\pi}(\\mu_{0}(x.\\pi(x))+\\beta_{0}\\sigma_{0}(x,\\pi(x))\\eta(x)+w)]-\\mathbb{E}_{w}[V^{\\pi}(f^{*}(x.\\pi(x))+w)]}}\\\\ &{\\le\\kappa\\left(\\|\\mu_{0}(x.\\pi(x))+\\beta_{0}\\sigma_{0}(x,\\pi(x))\\eta(x)-f^{*}(x.\\pi(x))\\|\\right)}\\\\ &{\\le\\kappa\\left(\\|\\mu_{0}(x.\\pi(x))-f^{*}(x.\\pi(x))\\|+\\|\\beta_{0}\\sigma_{0}(x,\\pi(x))\\eta(x)\\|\\right)}\\\\ &{\\le\\kappa\\left(\\left(1+\\sqrt{d_{x}}\\right)\\beta_{0}\\sqrt{d_{x}}\\sigma_{\\mathrm{max}}\\right).}&{(\\mathrm{Assumpt})}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{x+|x,f,\\pi}[V^{\\pi}(x_{+})]\\leq\\mathbb{E}_{x_{+}|x,f^{*},\\pi}[V^{\\pi}(x_{+}^{*})]+\\kappa\\left(\\left(1+\\sqrt{d_{x}}\\right)\\beta_{0}\\sqrt{d_{x}}\\sigma_{\\operatorname*{max}}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}_{x_{+}|x,f^{*},\\pi}[V^{\\pi}(x_{+}^{*})]+\\kappa\\left(\\left(1+\\sqrt{d_{x}}\\right)\\beta_{0}\\sqrt{d_{x}}\\sigma_{\\operatorname*{max}}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\gamma V^{\\pi}(x)+K+\\kappa\\left(\\left(1+\\sqrt{d_{x}}\\right)\\beta_{0}\\sqrt{d_{x}}\\sigma_{\\operatorname*{max}}\\right),\\qquad\\mathrm{(Assumption~}2.4\\mathrm{)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we denoted ${\\pmb x}_{+}^{*}={\\pmb f}^{*}({\\pmb x},{\\pmb\\pi}({\\pmb x})+{\\pmb w}$ Define $\\widehat{K}=K+\\kappa\\left(\\left(1+\\sqrt{d_{x}}\\right)\\beta_{0}\\sqrt{d_{x}}\\sigma_{\\operatorname*{max}}\\right)$ ", "page_idx": 17}, {"type": "text", "text": "Lemma A.5 (Minorisation condition optimistic ystem). Consider the system ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\pmb{x}_{+}=\\pmb{f}(\\pmb{x}.\\pi(\\pmb{x}))+\\pmb{w}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for any $n\\geq0$ $\\pi\\in\\Pi$ and $\\pmb{f}\\in\\mathcal{M}_{n}\\cap\\mathcal{M}_{0}$ Let Assumption $2.I-2.8$ hold. Let $P^{\\pi,f}$ denote the transition kernel for the policy $\\pi\\in\\Pi$ i.e., $\\quad P^{\\pi,f}(x,{\\mathcal{A}})=\\mathbb{P}(x_{+}\\in{\\mathcal{A}}|x,\\pi(x),f)$ . Then, there exists a constant $\\hat{\\alpha}\\in(0,1)$ and a probability measure $\\hat{\\zeta}(\\cdot)$ independent of $n$ s.t., ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{x\\in{\\mathcal{C}}}P^{\\pi,f}(x,\\cdot)\\geq{\\hat{\\alpha}}{\\hat{\\zeta}}(\\cdot)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with $\\mathcal{C}\\,\\overset{\\mathrm{def}}{=}\\{x\\in\\mathcal{X};V^{\\pi}({\\pmb x})<\\hat{R}\\}\\,f o r\\,s o m e\\;\\hat{R}>{^{2\\widehat{K}}\\!\\left/1-\\gamma\\right.}$ ", "page_idx": 17}, {"type": "text", "text": "Proof. First, we show that $\\mathcal{C}$ is contained in a compact domain. From the Assumption 2.4 we pick the function $\\xi\\in\\mathcal{K}_{\\infty}$ . Since $\\begin{array}{r}{C_{l}\\xi(0)=0,\\operatorname*{lim}_{s\\to\\infty}\\bar{\\xi}(s)=+\\infty}\\end{array}$ and $C_{l}\\xi$ is continuous, there exists $M$ such that $C_{l}\\xi(M)=\\hat{R}$ . Then for $\\|\\pmb{x}\\|>M$ we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\nV^{\\pi}(x)\\geq C_{l}\\xi(\\|x\\|)>\\xi(M)=\\hat{R}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore we have: $c\\ \\subseteq\\ B(\\mathbf{0},M)\\,{\\stackrel{\\mathrm{def}}{=}}\\{x\\ \\ |\\ \\ \\|x-\\mathbf{0}\\|\\ \\leq\\ M\\}$ .Since for any. $\\textbf{\\em x}\\in\\mathcal{C}$ we have $\\|f(x,\\pi(x))\\|\\,\\le\\,\\|f^{*}(x,\\pi(x))\\|+\\beta_{0}\\sigma_{\\mathrm{max}}$ .Since $\\pmb{f}^{\\ast}~\\mathrm{is}$ continuous, there exists a $B$ such that $f^{*}(\\mathcal{C},\\pi(\\mathcal{C}))\\subset B(\\mathbf{0},B)$ Therefore we have: $\\pmb{f}(\\mathcal{C},\\pi(\\mathcal{C}))\\subset B(\\mathbf{0},B_{1})$ where $B_{1}=B+\\beta_{0}\\sigma_{\\mathrm{max}}$ In the ast stp we prove that $\\alpha\\stackrel{\\mathrm{def}}{=}2^{-d_{\\mathbf{x}}}e^{-B_{1}^{2}/\\sigma^{2}}$ and $\\zeta$ with law of $\\textstyle N\\left(0,{\\frac{\\sigma^{2}}{2}}\\right)$ satisfy condition of Lemma A.1. It is enough to show that $\\forall\\pmb{\\mu}\\in\\mathcal{B}(\\mathbf{0},B_{1}),\\forall\\pmb{x}\\in\\mathbb{R}^{d_{\\mathbf{x}}}$ we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\alpha\\frac{1}{(2\\pi)^{\\frac{d_{x}}{2}}\\left(\\frac{\\sigma^{2}}{2}\\right)^{\\frac{d_{x}}{2}}}e^{-\\frac{\\|{\\bf x}\\|^{2}}{\\sigma^{2}}}\\leq\\frac{1}{(2\\pi)^{\\frac{d_{x}}{2}}(\\sigma^{2})^{\\frac{d_{x}}{2}}}e^{-\\frac{\\|{\\bf x}-{\\boldsymbol{\\mu}}\\|^{2}}{2\\sigma^{2}}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which can be proven with simple algebraic manipulations. ", "page_idx": 18}, {"type": "text", "text": "Proof of Theorem A.3. As for the true system, the drift condition from Lemma A.4 and the minorisation condition from Lemma A.5 are sufficient to show ergodicity of the optimistic system (c.f., Theorem A.2 or Hairer & Mattingly (2011)). The rest of the proof is similar to Theorem 2.6. ", "page_idx": 18}, {"type": "text", "text": "A.3Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Since NEoRL works in artificial episodes $n\\in\\{0,N-1\\}$ of varying horizons $H_{n}$ . We denote with $\\pmb{x}_{k}^{n}$ the state visited during episode $n$ at time step $k\\leq H_{n}$ . Crucial, to our regret analysis is bounding the first and second moment of $V^{\\pmb{\\pi}_{n}}(\\pmb{x}_{k}^{n})$ for all $n,k$ . Given the nature of Assumption 2.4, this requires analyzing geometric series. Thus, we start with the following elementary result of geometric series. ", "page_idx": 18}, {"type": "text", "text": "Corollary A.6. Consider the sequence $\\{S_{n}\\}_{n\\ge0}$ with $S_{n}\\geq0$ for all $n$ Let the following hold ", "page_idx": 18}, {"type": "equation", "text": "$$\nS_{n}\\leq\\rho S_{n-1}+C\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for $\\rho\\in(0,1)$ and $C>0,$ Then we have ", "page_idx": 18}, {"type": "equation", "text": "$$\nS_{n}\\leq\\rho^{n}S_{0}+C\\frac{1}{1-\\rho}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. ", "page_idx": 18}, {"type": "equation", "text": "$$\nS_{n}\\leq\\rho S_{n-1}+C\\leq\\rho^{2}S_{n-2}+C(1+\\rho)\\leq\\rho^{n}S_{0}+C\\sum_{i=0}^{n}\\rho^{i}\\leq\\rho^{n}S_{0}+C\\frac{1}{1-\\rho}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Lemma A.7. Let Assumption $2.I-2.8$ hold and let $H_{0}$ be the smallest integer such that ", "page_idx": 18}, {"type": "equation", "text": "$$\nH_{0}>\\frac{\\log{\\left(^{C_{u}}/{C_{l}}\\right)}}{\\log{\\left(^{1}/{\\gamma}\\right)}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Moreover, defne $\\begin{array}{r}{\\nu\\;=\\;\\frac{C_{u}}{C_{l}}\\gamma^{H_{0}}}\\end{array}$ . Note, by defnition of $H_{0}$ \uff0c $\\nu\\ <\\ 1$ Then we have fo ll $k\\ \\in$ $\\{0,\\ldots,H_{n}\\}$ and $n>0$ ", "page_idx": 18}, {"type": "text", "text": "Bounded expectation over horizon ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\pmb{x}_{k}^{n},\\ldots,\\pmb{x}_{1}^{0}|\\pmb{x}_{0}}[V^{\\pi_{n}}(\\pmb{x}_{k}^{n})]\\leq\\gamma^{k}\\mathbb{E}_{\\pmb{x}_{0}^{n},\\ldots,\\pmb{x}_{1}^{0}|\\pmb{x}_{0}}[V^{\\pi_{n}}(\\pmb{x}_{0}^{n})]+K/(1-\\gamma).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Bounded expectation over episodes ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\pmb{x}_{0}^{n},\\dots,\\pmb{x}_{1}^{0}|\\pmb{x}_{0}}[V^{\\pi_{n}}(\\pmb{x}_{0}^{n})]\\leq\\nu^{n}V^{\\pi_{0}}(\\pmb{x}_{0})+\\frac{C_{u}}{C_{l}}K/(1-\\gamma)\\frac{1}{1-\\nu}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Moreover, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\mathbb{E}_{{\\pmb x}_{k}^{n},\\ldots,{\\pmb x}_{1}^{0}|{\\pmb x}_{0}}[V^{\\pi_{n}}({\\pmb x}_{k}^{n})]\\leq D({\\pmb x}_{0},K,\\gamma,\\nu),}\\\\ {D({\\pmb x}_{0},K,\\gamma,\\nu)=V^{\\pi_{0}}({\\pmb x}_{0})+K/(1-\\gamma)\\left(\\frac{C_{u}}{C_{l}}\\frac{1}{1-\\nu}+1\\right)}\\end{array}\n$$with ", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. We start with proving the first claim ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbb{E}_{\\pmb{x}_{k}^{n},\\dots,\\pmb{x}_{1}^{0}|\\pmb{x}_{0}}[V^{\\pi_{n}}(\\pmb{x}_{k}^{n})]=\\mathbb{E}_{\\pmb{x}_{k-1}^{n},\\dots,\\pmb{x}_{1}^{0}|\\pmb{x}_{0}}[\\mathbb{E}_{\\pmb{x}_{k}^{n}|\\pmb{x}_{k-1}^{n}}[V^{\\pi_{n}}(\\pmb{x}_{k}^{n})]]}\\\\ &{}&{\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\mathbb{E}_{\\pmb{x}_{k-1}^{n},\\dots,\\pmb{x}_{1}^{0}|\\pmb{x}_{0}}[\\gamma V^{\\pi_{n}}(\\pmb{x}_{k-1}^{n})+K]}\\\\ &{}&{=\\gamma\\mathbb{E}_{\\pmb{x}_{k-1}^{n},\\dots,\\pmb{x}_{1}^{0}|\\pmb{x}_{0}}[V^{\\pi_{n}}(\\pmb{x}_{k-1}^{n})]+K\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We can apply Corollary A.6 to prove the claim. For the second claim, we note that for any $\\pi,\\pi^{\\prime}$ and $\\pmb{x}\\in\\mathcal{X}$ we have from Assumption 2.4 ", "page_idx": 19}, {"type": "equation", "text": "$$\nV^{\\pi}(x)\\leq C_{u}\\alpha(\\|x\\|)\\leq\\frac{C_{u}}{C_{l}}V^{\\pi^{\\prime}}(x).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{E}_{\\pmb{x}_{0}^{n},\\ldots,\\pmb{x}_{1}^{0}|\\pmb{x}_{0}}[V^{\\pi_{n}}(\\pmb{x}_{0}^{n})]}\\quad}&{}\\\\ &{\\leq\\frac{C_{u}}{C_{l}}\\mathbb{E}_{\\pmb{x}_{0}^{n},\\ldots,\\pmb{x}_{1}^{0}|\\pmb{x}_{0}}[V^{\\pi_{n-1}}(\\pmb{x}_{0}^{n})]}\\\\ &{=\\frac{C_{u}}{C_{l}}\\mathbb{E}_{\\pmb{x}_{H_{n}}^{n-1},\\ldots,\\pmb{x}_{1}^{0}|\\pmb{x}_{0}}[V^{\\pi_{n-1}}(\\pmb{x}_{H_{n}}^{n-1})]}\\\\ &{\\leq\\left(\\frac{C_{u}}{C_{l}}\\gamma^{H_{n}}\\right)\\mathbb{E}_{\\pmb{x}_{0}^{n-1},\\ldots,\\pmb{x}_{1}^{0}|\\pmb{x}_{0}}[V^{\\pi_{n-1}}(\\pmb{x}_{0}^{n-1})]+\\frac{C_{u}}{C_{l}}K/(1-\\gamma)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For our choice of $H_{0}$ , we have for all $n\\geq0$ that $\\begin{array}{r}{\\frac{C_{u}}{C_{l}}\\gamma^{H_{n}}\\leq\\frac{C_{u}}{C_{l}}\\gamma^{H_{0}}\\leq\\nu<1}\\end{array}$ . From Corollary A.6, we get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{\\mathbf{x}_{0}^{n},\\dots,\\mathbf{x}_{1}^{0}|\\mathbf{x}_{0}}[V^{\\pi_{n}}(x_{0}^{n})]\\le\\left(\\displaystyle\\frac{C_{u}}{C_{l}}\\gamma^{H_{n}}\\right)\\mathbb{E}_{\\mathbf{x}_{0}^{n-1},\\dots,\\mathbf{x}_{1}^{0}|\\mathbf{x}_{0}}[V^{\\pi_{n-1}}(x_{0}^{n-1})]+\\displaystyle\\frac{C_{u}}{C_{l}}K/(1-\\gamma)}&{}\\\\ {\\le\\nu\\mathbb{E}_{\\mathbf{x}_{0}^{n-1},\\dots,\\mathbf{x}_{1}^{0}|\\mathbf{x}_{0}}[V^{\\pi_{n-1}}(x_{0}^{n-1})]+\\displaystyle\\frac{C_{u}}{C_{l}}K/(1-\\gamma)}&{}\\\\ {\\le\\nu^{n}V^{\\pi_{0}}(x_{0})+\\displaystyle\\frac{C_{u}}{C_{l}}K/(1-\\gamma)\\displaystyle\\frac{1}{1-\\nu}.}&{\\mathrm{(Corollary)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathbb{E}_{\\mathbf{x}_{k}^{n},\\ldots,\\mathbf{x}_{1}^{0}|\\mathbf{x}_{0}}[V^{\\pi_{n}}(x_{k}^{n})]\\le\\gamma^{k}\\mathbb{E}_{\\mathbf{x}_{0}^{n},\\ldots,x_{1}^{0}|\\mathbf{x}_{0}}[V^{\\pi_{n}}(\\mathbf{x}_{0}^{n})]+K/(1-\\gamma)}&{\\mathrm{(Equal~})}\\\\ {\\le\\mathbb{E}_{\\mathbf{x}_{0}^{n},\\ldots,x_{1}^{0}|\\mathbf{x}_{0}}[V^{\\pi_{n}}(\\mathbf{x}_{0}^{n})]+K/(1-\\gamma)}\\\\ {\\le\\nu^{n}V^{\\pi_{0}}(\\mathbf{x}_{0})+\\displaystyle\\frac{C_{u}}{C_{l}}K/(1-\\gamma)\\displaystyle\\frac{1}{1-\\nu}+K/(1-\\gamma)}&{\\mathrm{(Equal~})}\\\\ {\\le V^{\\pi_{0}}(\\mathbf{x}_{0})+\\displaystyle\\frac{C_{u}}{C_{l}}K/(1-\\gamma)\\displaystyle\\frac{1}{1-\\nu}+K/(1-\\gamma)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Lemma A.8. Let Assumption $2.I-2.8$ hold and let $H_{0}$ be the smallest integer such that ", "page_idx": 19}, {"type": "equation", "text": "$$\nH_{0}>\\frac{\\log{\\left({C_{u}}/{C_{l}}\\right)}}{\\log{\\left({^{1}}/{\\gamma}\\right)}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Moreover, define $\\begin{array}{r}{\\nu=\\frac{C_{u}}{C_{l}}\\gamma^{H_{0}}}\\end{array}$ . Note, by definition of $H_{0},\\,\\nu<1$ ", "page_idx": 19}, {"type": "text", "text": "Then we have for all $k\\in\\{0,\\ldots,H_{n}\\}$ and $n>0$ ", "page_idx": 19}, {"type": "text", "text": "Bounded second moment over horizon ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x_{k}^{n},\\ldots,x_{1}^{0}|x_{0}}\\left[\\left(V^{\\pi_{n}}(x_{k}^{n})\\right)^{2}\\right]\\leq\\gamma^{2k}\\mathbb{E}_{x_{0}^{n},\\ldots,x_{1}^{0}|x_{0}}\\left[\\left(V^{\\pi_{n}}(x_{0}^{n})\\right)^{2}\\right]+\\frac{D_{2}(x_{0},K,\\gamma,\\nu)}{1-\\gamma^{2}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Wih $D_{2}(x_{0},K,\\gamma,\\nu)\\;\\;=\\;\\;2K\\gamma D(x_{0},K,\\gamma,\\nu)\\,+\\,K^{2}\\,+\\,C_{w}$ .and $C_{w}\\;\\;=\\;\\;\\mathbb{E}_{w}\\left[\\kappa^{2}(\\|w\\|)\\right]\\;+$ $3(\\mathbb{E}_{w}\\left[\\kappa(\\|w\\|)\\right])^{2}$ ", "page_idx": 19}, {"type": "text", "text": "Bounded second moment over episodes ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x_{0}^{n},\\ldots,x_{1}^{0}|x_{0}}\\left[\\left(V^{\\pi_{n}}(x_{0}^{n})\\right)^{2}\\right]\\le\\nu^{2n}\\left(V^{\\pi_{0}}(x_{0})\\right)^{2}+\\left(\\frac{C_{u}}{C_{l}}\\right)^{2}\\frac{D_{2}(x_{0},K,\\gamma,\\nu)}{1-\\gamma^{2}}\\frac{1}{1-\\nu^{2}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{D_{3}(x_{0},K,\\gamma,\\nu)=\\left(V^{\\pi_{0}}(x_{0})\\right)^{2}+D_{2}(x_{0},K,\\gamma,\\nu)\\left(\\left(\\frac{C_{u}}{C_{l}}\\right)^{2}\\frac{1}{1-\\gamma^{2}}\\frac{1}{1-\\nu^{2}}+\\frac{1}{1-\\gamma^{2}}\\right)\\!.}\\\\ {\\mathbb{E}_{x_{k}^{n},\\ldots,x_{1}^{0}|x_{0}}\\left[\\left(V^{\\pi_{n}}(x_{k}^{n})\\right)^{2}\\right]\\leq D_{3}(x_{0},K,\\gamma,\\nu)\\qquad\\qquad\\qquad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. Note that, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{{\\mathbf x}_{k}^{n}|{\\mathbf x}_{k-1}^{n}}\\left[\\left(V^{\\pi_{n}}(x_{k}^{n})\\right)^{2}\\right]=\\left(\\mathbb{E}_{{\\mathbf x}_{k}^{n}|{\\mathbf x}_{k-1}^{n}}\\left[V^{\\pi_{n}}({\\mathbf x}_{k}^{n})\\right]\\right)^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,\\mathbb{E}_{{\\mathbf x}_{k}^{n}|{\\mathbf x}_{k-1}^{n}}\\left[\\left(V^{\\pi_{n}}({\\mathbf x}_{k}^{n})-\\mathbb{E}_{{\\mathbf x}_{k}^{n}|{\\mathbf x}_{k-1}^{n}}\\left[V^{\\pi_{n}}({\\mathbf x}_{k}^{n})\\right]\\right)^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We first bound the second term. Let $\\bar{\\pmb{x}}_{k}^{n}=\\pmb{f}^{*}(\\pmb{x}_{k-1}^{n},\\pmb{\\pi}_{n}(\\pmb{x}_{k-1}^{n}))$ , i.e., the next state in the absence of transition noise. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{x_{k}^{n}|x_{k-1}^{n}}\\left[\\left(V^{\\pi_{n}}(x_{k}^{n})-\\mathbb{E}_{x_{k}^{n}|x_{k-1}^{n}}\\left[V^{\\pi_{n}}(x_{k}^{n})\\right]\\right)^{2}\\right]}&{}\\\\ {=\\mathbb{E}_{x_{k}^{n}|x_{k-1}^{n}}\\left[\\left(V^{\\pi_{n}}(x_{k}^{n})-V^{\\pi_{n}}(\\bar{x}_{k}^{n})+V^{\\pi_{n}}(\\bar{x}_{k}^{n})-\\mathbb{E}_{x_{k}^{n}|x_{k-1}^{n}}\\left[V^{\\pi_{n}}(x_{k}^{n})\\right]\\right)^{2}\\right]}&{}\\\\ {=\\mathbb{E}_{x_{k}^{n}|x_{k-1}^{n}}\\left[\\left(V^{\\pi_{n}}(x_{k}^{n})-V^{\\pi_{n}}(\\bar{x}_{k}^{n})+\\mathbb{E}_{x_{k}^{n}|x_{k-1}^{n}}\\left[V^{\\pi_{n}}(\\bar{x}_{k}^{n})-V^{\\pi_{n}}(x_{k}^{n})\\right]\\right)^{2}\\right]}&{}\\\\ {\\leq\\mathbb{E}_{w}\\left[(\\kappa(\\|w\\|)+\\mathbb{E}_{w}[\\kappa(\\|w\\|)])^{2}\\right]}&{\\mathrm{(uniform~continuity~of~}}\\\\ {=\\mathbb{E}_{w}\\left[\\kappa^{2}(\\|w\\|)\\right]+3(\\mathbb{E}_{w}\\left[\\kappa(\\|w\\|)\\right])^{2}}&{}\\\\ {=C_{w}}&{\\mathrm{(Assumption~}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{x_{k}^{n}|x_{k-1}^{n}}\\left[\\left(V^{\\pi_{n}}(x_{k}^{n})\\right)^{2}\\right]=\\left(\\mathbb{E}_{x_{k}^{n}|x_{k-1}^{n}}\\left[V^{\\pi_{n}}(x_{k}^{n})\\right]\\right)^{2}+C_{w}}\\\\ &{\\phantom{=}\\leq\\left(\\gamma V^{\\pi_{n}}(x_{k}^{n})+K\\right)^{2}+C_{w}}\\\\ &{\\phantom{=}=\\gamma^{2}\\left(V^{\\pi_{n}}(x_{k-1}^{n})\\right)^{2}+2K\\gamma V^{\\pi_{n}}(x_{k-1}^{n})+K^{2}+C_{w}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathbf{x}_{k}^{n},\\dots,\\mathbf{x}_{1}^{0}|\\mathbf{x}_{0}}\\left[(V^{\\pi_{n}}(x_{k}^{n}))^{2}\\right]}\\\\ &{=\\mathbb{E}_{\\mathbf{x}_{k-1}^{n},\\dots,\\mathbf{x}_{1}^{0}|\\mathbf{x}_{0}}\\left[\\mathbb{E}_{\\mathbf{x}_{k}^{n}|\\mathbf{x}_{k-1}^{n}}\\left[(V^{\\pi_{n}}(\\mathbf{x}_{k}^{n}))^{2}\\right]\\right]}\\\\ &{\\le\\gamma^{2}\\mathbb{E}_{\\mathbf{x}_{k-1}^{n},\\dots,\\mathbf{x}_{1}^{0}|\\mathbf{x}_{0}}\\left[\\left(V^{\\pi_{n}}(x_{k-1}^{n})\\right)^{2}\\right]+2K\\gamma\\mathbb{E}_{\\mathbf{x}_{k-1}^{n},\\dots,\\mathbf{x}_{1}^{0}|\\mathbf{x}_{0}}\\left[V^{\\pi_{n}}(x_{k-1}^{n})\\right]+K^{2}+C_{w}}\\\\ &{\\le\\gamma^{2}\\mathbb{E}_{\\mathbf{x}_{k-1}^{n},\\dots,\\mathbf{x}_{1}^{0}|\\mathbf{x}_{0}}\\left[\\left(V^{\\pi_{n}}(x_{k-1}^{n})\\right)^{2}\\right]+2K\\gamma D(\\mathbf{x}_{0},K,\\gamma,\\nu)+K^{2}+C_{w}.\\qquad\\mathrm{(Lemma~A.7)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Let $D_{2}(x_{0},K,\\gamma,\\nu)=2K\\gamma D(x_{0},K,\\gamma,\\nu)+K^{2}+C_{w}$ . Applying Corollary A.6 we get ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x_{k}^{n},\\ldots,x_{1}^{0}|x_{0}}\\left[\\left(V^{\\pi_{n}}(x_{k}^{n})\\right)^{2}\\right]\\leq\\gamma^{2k}\\mathbb{E}_{x_{0}^{n},\\ldots,x_{1}^{0}|x_{0}}\\left[\\left(V^{\\pi_{n}}(x_{0}^{n})\\right)^{2}\\right]+\\frac{D_{2}(x_{0},K,\\gamma,\\nu)}{1-\\gamma^{2}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Similar to the ft t, welera t $\\begin{array}{r}{V^{\\pmb{\\pi}_{n}}(\\pmb{x})\\leq\\frac{C_{u}}{C_{l}}V^{\\pmb{\\pi}_{n-1}}(\\pmb{x})}\\end{array}$ for all $\\begin{array}{r}{\\mathbf{\\boldsymbol{x}}\\in\\mathcal{X},\\frac{C_{u}}{C_{l}}\\gamma^{H_{n-1}}\\leq\\nu,}\\end{array}$ and get, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\pmb{x}_{0}^{n},\\dots,\\pmb{x}_{1}^{0}|\\pmb{x}_{0}}\\left[\\left(V^{\\pmb{\\pi}_{n}}(\\pmb{x}_{0}^{n})\\right)^{2}\\right]\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\left(\\displaystyle\\frac{C_{u}}{C_{l}}\\right)^{2}\\mathbb{E}_{\\mathbf{x}_{0}^{n_{1}},\\ldots,\\mathbf{x}_{0}^{0}|\\mathbf{x}_{0}}\\left[\\left(V^{\\pi_{n-1}}(\\mathbf{x}_{0}^{n})\\right)^{2}\\right]}\\\\ &{=\\left(\\displaystyle\\frac{C_{u}}{C_{l}}\\right)^{2}\\mathbb{E}_{\\mathbf{x}_{H_{n}}^{n_{1}},\\ldots,\\mathbf{x}_{0}^{0}|\\mathbf{x}_{0}}\\left[\\left(V^{\\pi_{n-1}}(\\mathbf{x}_{H_{n}}^{n-1})\\right)^{2}\\right]}\\\\ &{\\le\\left(\\displaystyle\\frac{C_{u}}{C_{l}}\\gamma^{H_{n}}\\right)^{2}\\mathbb{E}_{\\mathbf{x}_{0}^{n-1},\\ldots,\\mathbf{x}_{1}^{0}|\\mathbf{x}_{0}}\\left[\\left(V^{\\pi_{n-1}}(\\mathbf{x}_{0}^{n-1})\\right)^{2}\\right]+\\left(\\displaystyle\\frac{C_{u}}{C_{l}}\\right)^{2}\\frac{D_{2}\\left(x_{0},K,\\gamma,\\nu\\right)}{1-\\gamma^{2}}\\ \\mathrm{(Equa~}}\\\\ &{\\le\\nu^{2}\\mathbb{E}_{\\mathbf{x}_{0}^{n-1},\\ldots,\\mathbf{x}_{1}^{0}|\\mathbf{x}_{0}}\\left[\\left(V^{\\pi_{n-1}}(\\mathbf{x}_{0}^{n-1})\\right)^{2}\\right]+\\left(\\displaystyle\\frac{C_{u}}{C_{l}}\\right)^{2}\\frac{D_{2}\\left(x_{0},K,\\gamma,\\nu\\right)}{1-\\gamma^{2}}}\\\\ &{\\le\\nu^{2n}\\left(V^{\\pi_{n}}(x_{0})\\right)^{2}+\\left(\\displaystyle\\frac{C_{u}}{C_{l}}\\right)^{2}\\frac{D_{2}\\left(x_{0},K,\\gamma,\\nu\\right)}{1-\\gamma^{2}}\\frac{1}{1-\\gamma^{2}}\\qquad\\qquad\\qquad\\qquad\\quad(\\mathrm{Coro}\\ \\ \\nu)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Moreover, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\alpha_{k}^{n},\\ldots,x_{1}^{0}|x_{0}}\\left[(V^{\\pi_{n}}(x_{k}^{n}))^{2}\\right]}\\\\ &{\\le\\gamma^{2\\ensuremath{\\mathbb{E}}}\\mathbb{E}_{\\alpha_{0}^{n},\\ldots,x_{1}^{0}|x_{0}}\\left[(V^{\\pi_{n}}(x_{0}^{n}))^{2}\\right]+\\frac{D_{2}(x_{0},K,\\gamma,\\nu)}{1-\\gamma^{2}}}\\\\ &{\\le\\mathbb{E}_{\\alpha_{0}^{n},\\ldots,x_{1}^{0}|x_{0}}\\left[(V^{\\pi_{n}}(x_{0}^{n}))^{2}\\right]+\\frac{D_{2}(x_{0},K,\\gamma,\\nu)}{1-\\gamma^{2}}}\\\\ &{\\le\\nu^{2n}\\left(V^{\\pi_{0}}(x_{0})\\right)^{2}+\\left(\\frac{C_{u}}{C_{l}}\\right)^{2}\\frac{D_{2}(x_{0},K,\\gamma,\\nu)}{1-\\gamma^{2}}\\frac{1}{1-\\nu^{2}}+\\frac{D_{2}(x_{0},K,\\gamma,\\nu)}{1-\\gamma^{2}}}\\\\ &{\\le(V^{\\pi_{0}}(x_{0}))^{2}+D_{2}(x_{0},K,\\gamma,\\nu)\\left(\\left(\\frac{C_{u}}{C_{l}}\\right)^{2}\\frac{1}{1-\\gamma^{2}}\\frac{1}{1-\\nu^{2}}+\\frac{1}{1-\\gamma^{2}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Finally, we prove the regret bound of NEoRL. ", "page_idx": 21}, {"type": "text", "text": "Proof of Theorem 3.1. In the following, let $\\hat{\\pmb{x}}_{k+1}^{n}=\\pmb{f}_{n}(\\pmb{x}_{k}^{n},\\pmb{\\pi}_{n}(\\pmb{x}_{k}^{n}))\\!+\\!\\pmb{w}_{k}^{n}$ denote the state predicted under the optimistic dynamics and $\\pmb{x}_{k+1}^{n}=\\pmb{f}_{n}^{*}(\\pmb{x}_{k}^{n},\\pmb{\\pi}_{n}(\\pmb{x}_{k}^{n}))+\\pmb{w}_{k}^{n}$ the true state. ", "page_idx": 21}, {"type": "equation", "text": "$$\n{\\begin{array}{r l r}{\\mathbb{E}\\left[\\sum_{n=0}^{\\lfloor N-1\\rfloor-1}c(x_{1}^{\\prime\\prime},x_{n}^{\\prime\\prime}(x_{1}^{\\prime\\prime}))-A(\\pi^{*})\\right]}&{}&\\\\ {\\leq}&{\\leq\\operatorname{lim}\\left[\\sum_{n=0}^{\\lfloor N-1\\rfloor-1}c(x_{1}^{\\prime\\prime},\\pi_{n}^{*}(x_{1}^{\\prime\\prime}))-A(\\pi_{n},f_{n})\\right]}&{{\\mathrm{(forimisnor~}}}\\\\ &{=\\mathbb{E}\\left[\\sum_{n=0}^{\\lfloor N-1\\rfloor-1}b(\\pi_{n},f_{n},x_{1}^{\\prime\\prime})-B(\\pi_{n},f_{n},\\tilde{x}_{1}^{\\prime\\prime})\\right]}&{{\\mathrm{(Beliman~equation~(Equation~(G))}}}\\\\ &{=\\mathbb{E}\\left[\\sum_{n=0}^{\\lfloor N-1\\rfloor-1}b(\\pi_{n},f_{n},x_{1}^{\\prime\\prime})-B(\\pi_{n},f_{n},x_{1}^{\\prime\\prime})+B(\\pi_{n},f_{n},x_{1}^{\\prime\\prime})-B(\\pi_{n},f_{n},\\tilde{x}_{1}^{\\prime\\prime}+1)\\right]}\\\\ &{=\\mathbb{E}\\left[\\sum_{n=0}^{\\lfloor N-1\\rfloor-1}b(\\pi_{n},f_{n},x_{1}^{\\prime\\prime})-B(\\pi_{n},f_{n},x_{1}^{\\prime\\prime}+1)+B(\\pi_{n},f_{n},x_{1}^{\\prime\\prime}+1)-B(\\pi_{n},f_{n},\\tilde{x}_{1}^{\\prime\\prime}+1)\\right]}&{{\\mathrm{(forime~}}f_{n},}\\\\ &{=\\sum_{n=0}^{\\lfloor N-1\\rfloor-1}\\mathbb{E}\\left[B(\\pi_{n},f_{n},x_{1}^{\\prime\\prime})-B(\\pi_{n},f_{n},\\tilde{x}_{1}^{\\prime\\prime}+1)\\right]}&{{\\mathrm{(A})}}\\\\ &{\\leq\\sum_{n=0}^{\\lfloor N-1\\rfloor-1}\\mathbb{E}\\left[B(\\pi_{n},f_{n},x_{1}^{\\prime\\prime})-B(\\pi_{n},f_{n},\\tilde{x}_{1}^{\\prime\\prime}+1) \n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "First, we study the term (A). ", "page_idx": 21}, {"type": "text", "text": "Proof for A): Note that because ${f_{n}\\in\\mathcal{M}_{n}}$ , there exists a $\\pmb{\\eta}\\;\\in\\;[-1,1]^{d_{x}}$ such that $\\hat{\\pmb{x}}_{k+1}^{n}\\,=$ $\\mu_{n}(\\pmb{x}_{k}^{n},\\pmb{\\pi}_{n}(\\pmb{x}_{k}^{n}))+\\beta_{n}\\pmb{\\sigma}_{n}(\\pmb{x}_{k}^{n},\\pmb{\\pi}_{n}(\\pmb{x}_{k}^{n}))\\pmb{\\eta}(\\pmb{x}_{k}^{n})+\\pmb{w}_{k}^{n}$ .Furthermore, ${\\pmb x}_{k+1}^{n}={\\pmb f}^{*}({\\pmb x}_{k}^{n},{\\pmb\\pi}_{n}({\\pmb x}_{k}^{n}))+{\\pmb w}_{k}^{n}$ and the transition noise is Gaussian. Let $\\zeta_{2,k}^{n}$ and $\\zeta_{1,k}^{n}$ denote the respective distributions of the ", "page_idx": 21}, {"type": "text", "text": "two random variables, i.e., $\\zeta_{1,k}^{n}\\sim\\mathcal{N}(f^{*}({\\boldsymbol{\\mathbf{x}}}_{k}^{n},\\pi_{n}({\\boldsymbol{\\mathbf{x}}}_{k}^{n})),\\sigma^{2}I)$ and $\\zeta_{2,k}^{n}\\sim\\mathcal{N}(\\pmb{f}_{n}(\\pmb{x}_{k}^{n},\\pmb{\\pi}_{n}(\\pmb{x}_{k}^{n})),\\sigma^{2}\\pmb{I})$ Next, define $\\bar{B}=\\mathbb{E}_{{\\pmb x}\\sim\\zeta_{2,k}^{n}}\\left[B({\\pmb\\pi}_{n},{\\pmb f}_{n},{\\pmb x})\\right]$ , and consider the function $h(\\pmb{x})\\,=\\,B(\\pmb{\\pi}_{n},\\pmb{f}_{n},\\pmb{x})-\\bar{B}$ Then we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{w_{k}^{n}}\\left[B(\\pmb{\\pi}_{n},f_{n},\\pmb{x}_{k+1}^{n})-B(\\pmb{\\pi}_{n},f_{n},\\pmb{\\hat{x}}_{k+1}^{n})\\right]}\\\\ &{\\qquad=\\mathbb{E}_{\\pmb{x}\\sim\\zeta_{1,k}^{n}}\\left[B(\\pmb{\\pi}_{n},f_{n},\\pmb{x})\\right]-\\mathbb{E}_{\\pmb{x}\\sim\\zeta_{2,k}^{n}}\\left[B(\\pmb{\\pi}_{n},f_{n},\\pmb{x})\\right]}\\\\ &{\\qquad=\\mathbb{E}_{\\pmb{x}\\sim\\zeta_{1,k}^{n}}\\left[B(\\pmb{\\pi}_{n},f_{n},\\pmb{x})-\\bar{B}\\right]-\\mathbb{E}_{\\pmb{x}\\sim\\zeta_{2,k}^{n}}\\left[B(\\pmb{\\pi}_{n},f_{n},\\pmb{x})-\\bar{B}\\right]}\\\\ &{\\qquad=\\mathbb{E}_{\\pmb{x}\\sim\\zeta_{1,k}^{n}}[h(\\pmb{x})]-\\mathbb{E}_{\\pmb{x}\\sim\\zeta_{2,k}^{n}}[h(\\pmb{x})].}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Note that $\\mathbb{E}_{\\pmb{x}\\sim\\zeta_{2,k}^{n}}[h(\\pmb{x})]=0$ by the definition of $h$ and thus, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\mathbf{x}\\sim\\zeta_{1,k}^{n}}[h(\\pmb{x})]-\\mathbb{E}_{\\mathbf{x}\\sim\\zeta_{2,k}^{n}}[h(\\pmb{x})]=\\mathbb{E}_{\\mathbf{x}\\sim\\zeta_{1,k}^{n}}[h(\\pmb{x})]\\leq\\sqrt{\\mathbb{E}_{\\mathbf{x}\\sim\\zeta_{1,k}^{n}}[h^{2}(\\pmb{x})]}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "In the following, we bound the term above w.r.t. the Chi-squared distance ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbb{E}_{w_{k}^{n}}\\left[B(\\pi_{n},f_{n},x_{k+1}^{n})-B(\\pi_{n},f_{n},\\hat{x}_{k+1}^{n})\\right]=\\mathbb{E}_{x\\sim\\zeta_{1,k}^{n}}[h(x)]-\\mathbb{E}_{x\\sim\\zeta_{2,k}^{n}}[h(x)]}\\\\ &{}&{\\quad=\\int_{\\mathcal{X}}h(x)\\left(1-\\frac{\\zeta_{2,k}^{n}}{\\zeta_{1,k}^{n}}\\right)\\zeta_{1,k}^{n}(d x)\\le\\sqrt{\\mathbb{E}_{x\\sim\\zeta_{1,k}^{n}}\\left[h^{2}(x)\\right]}\\sqrt{d_{\\chi}(\\zeta_{2,k}^{n},\\zeta_{1,k}^{n})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "(Kakade et al., 2020, Lemma C.2.,) ", "page_idx": 22}, {"type": "text", "text": "With $d_{\\chi}(\\zeta_{2,k}^{n},\\zeta_{1,k}^{n})$ being the Chi-squared distance. ", "page_idx": 22}, {"type": "equation", "text": "$$\nd_{\\chi}(\\zeta_{2,k}^{n},\\zeta_{1,k}^{n})=\\int_{\\mathcal{X}}\\frac{\\left(\\zeta_{1,k}^{n}-\\zeta_{2,k}^{n}\\right)^{2}}{\\zeta_{1,k}^{n}}(d x)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since both bounds from Equation (17) and bound we got by applying (Kakade et al., 2020, Lemma C.2.,), we can apply minimum and have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\mathbf{w}_{k}^{n}}\\left[B(\\pi_{n},f_{n},x_{k+1}^{n})-B(\\pi_{n},f_{n},\\hat{x}_{k+1}^{n})\\right]\\leq\\sqrt{\\mathbb{E}_{\\mathbf{x}\\sim\\zeta_{1,k}^{n}}\\left[h^{2}(x)\\right]}\\sqrt{\\operatorname*{min}\\left\\{d_{\\chi}(\\zeta_{2,k}^{n},\\zeta_{1,k}^{n}),1\\right\\}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, following Kakade et al. (2020, Lemma C.2.,) we get ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{w_{k}^{n}}\\left[B(\\pi_{n},f_{n},x_{k+1}^{n})-B(\\pi_{n},f_{n},\\hat{x}_{k+1}^{n})\\right]}\\\\ &{\\qquad\\leq\\sqrt{\\mathbb{E}_{x\\sim\\zeta_{1,k}^{n}}\\left[h^{2}(x)\\right]}\\operatorname*{min}\\left\\{1/\\sigma\\left\\|f^{*}(x_{k}^{n},\\pi_{n}(x_{k}^{n}))-f_{n}(x_{k}^{n},\\pi_{n}(x_{k}^{n}))\\right\\|,1\\right\\}}\\\\ &{\\qquad\\leq\\sqrt{\\mathbb{E}_{x\\sim\\zeta_{1,k}^{n}}\\left[h^{2}(x)\\right]}(1+\\sqrt{d_{x}})^{\\beta_{n}\\slash\\sigma}\\left\\|\\sigma_{n}(x_{k}^{n},\\pi_{n}(x_{k}^{n}))\\right\\|.\\quad\\mathrm{((Sukhija~et~al.~}2024,\\mathrm{Cor.~}3))}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{n=0}^{N-1}\\sum_{k=0}^{H_{n-1}}\\mathbb{E}_{x_{k}^{n-1},x_{i}^{n}|\\log\\Big[\\mathbb{E}_{w_{k}^{n}}\\left[B(\\pi_{n},f_{n},x_{k+1}^{n})-B(\\pi_{n},f_{n},\\hat{x}_{k+1}^{n})\\right]\\Big]}\\\\ &{\\displaystyle\\leq\\sum_{n=0}^{N-1}\\sum_{k=0}^{H_{n-1}-1}\\mathbb{E}_{z_{k}^{n},\\cdots,x_{i}^{n}|\\log\\Big[\\sqrt{\\mathbb{E}_{x^{n-1},k_{1}}}\\left[h^{2}(\\pi)\\right](1+\\sqrt{d_{x}})^{\\beta_{x}}/\\rho\\left\\|\\sigma_{n}(x_{k}^{n},\\pi_{n}(x_{k}^{n}))\\right\\|\\Big]}\\\\ &{\\displaystyle\\leq\\sum_{n=0}^{N-1}\\sum_{k=0}^{H_{n-1}-1}\\left(1+\\sqrt{d_{x}}\\right)\\beta_{x}\\Big/\\sqrt{\\mathbb{E}_{x_{k}^{n-1},x_{i}^{n}|\\log\\Big[\\mathbb{E}_{x^{n-1},k_{1}}\\left[h^{2}(x)\\right]\\Big]}\\mathbb{E}_{x_{k}^{n-1},x_{i}^{n}|\\log\\Big[\\left\\|\\sigma_{n}(x_{k}^{n},\\pi_{n}(x_{k}^{n}))\\right\\|^{2}\\Big]}}\\\\ &{\\displaystyle\\leq(1+\\sqrt{d_{x}})^{\\beta_{x}}/\\sqrt{\\sum_{n=0}^{N-1}\\sum_{k=0}^{H_{n-1}-1}\\mathbb{E}_{x_{k}^{n-1},\\cdots,x_{i}^{n}|\\log\\Big[\\mathbb{E}_{x^{n-1},k_{1}}\\left[h^{2}(x)\\right]\\Big]}}\\\\ &{\\displaystyle\\times\\left\\{\\sum_{n=0}^{N-1}\\sum_{k=0}^{H_{n-1}-1}\\mathbb{E}_{x_{k}^{n-1},x_{i}^{n}|\\log\\Big[\\left\\|\\sigma_{n}(x_{k}^{n},\\pi_{n}(x_{k}^{n}))\\right\\|^{2}\\Big]}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Here, for the second and third inequality, we use Cauchy-Schwarz. Now we bound the two terms above individually. ", "page_idx": 23}, {"type": "text", "text": "First we bound $\\mathbb{E}_{{\\pmb x}\\sim\\zeta_{1,k}^{n}}\\left[h^{2}({\\pmb x})\\right]$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{\\mathbf{z}\\sim\\zeta_{t,k}^{n}}\\left[h^{2}(x)\\right]=\\mathbb{E}_{\\mathbf{z}\\sim\\zeta_{t,k}^{n}}\\left[\\left(B(\\pi_{n},f_{n},x)-\\bar{B}\\right)^{2}\\right]}\\\\ &{\\qquad\\qquad=\\mathbb{E}_{\\mathbf{z}\\sim\\zeta_{t,k}^{n}}\\left[\\left(B(\\pi_{n},f_{n},x)-\\mathbb{E}_{\\mathbf{z}\\sim\\zeta_{t,k}^{n}}\\left[B(\\pi_{n},f_{n},x)\\right]\\right)^{2}\\right]}\\\\ &{\\qquad\\qquad\\leq\\left(\\frac{C_{2}}{1-\\bar{\\lambda}}\\right)^{2}\\mathbb{E}_{\\mathbf{z}\\sim\\zeta_{t,k}^{n}}\\left[\\left(2+V^{\\pi_{n}}(x)+\\mathbb{E}_{\\mathbf{z}\\sim\\zeta_{t,k}^{n}}\\left[V^{\\pi_{n}}(x)\\right]\\right)^{2}\\right]\\quad\\mathrm{~(Theorem~A.3)~}}\\\\ &{\\qquad\\leq\\left(\\frac{C_{2}}{1-\\bar{\\lambda}}\\right)^{2}\\mathbb{E}_{\\mathbf{z}\\sim\\zeta_{t,k}^{n}}\\left[\\left(2+V^{\\pi_{n}}(x)+\\gamma V^{\\pi_{n}}(x_{k}^{n})+\\bar{K}\\right)^{2}\\right]\\quad\\mathrm{~(Lemma~A.4)~}}\\\\ &{\\qquad\\leq\\left(\\frac{\\sqrt{2}C_{2}}{1-\\bar{\\lambda}}\\right)^{2}\\mathbb{E}_{\\mathbf{z}\\sim\\zeta_{t,k}^{n}}\\left[\\left(V^{\\pi_{n}}(x)\\right)^{2}+\\left(2+\\gamma V^{\\pi_{n}}(x_{k}^{n})+\\bar{K}\\right)^{2}\\right]}\\\\ &{\\qquad\\leq\\left(\\frac{\\sqrt{2}C_{2}}{1-\\bar{\\lambda}}\\right)^{2}\\left(\\mathbb{E}_{\\mathbf{z}\\sim\\zeta_{t,k}^{n}}\\left[\\left(V^{\\pi_{n}}(x)\\right)^{2}\\right]+2\\gamma^{2}(V^{\\pi_{n}}(x_{k}^{n}))^{2}+2(2+\\bar{K})^{2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Furthermore, we have from Lemma A.8. ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{T}_{x_{k}^{n},\\ldots,x_{1}^{0}|x_{0}}\\left[\\mathbb{E}_{\\mathbf{x}_{k+1}^{n}|x_{k}^{n}}\\left[(V^{\\pi_{n}}(x_{k+1}))^{2}\\right]+2\\gamma^{2}(V^{\\pi_{n}}(x_{k}^{n}))^{2}\\right]}\\\\ &{=\\mathbb{E}_{x_{k+1}^{n},\\ldots,x_{1}^{0}|x_{0}}\\left[(V^{\\pi_{n}}(x_{k+1}))^{2}\\right]+2\\gamma^{2}\\mathbb{E}_{x_{k}^{n},\\ldots,x_{1}^{0}|x_{0}}\\left[(V^{\\pi_{n}}(x_{k}^{n}))^{2}\\right]\\leq(1+2\\gamma^{2})D_{3}(x_{0},K,\\gamma,\\nu).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In the end, we get ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sqrt{\\displaystyle\\sum_{n=0}^{N-1}\\sum_{k=0}^{H_{n}-1}\\mathbb{E}_{\\alpha_{k}^{n}\\sim\\mathcal{A}_{1}^{0}\\vert\\alpha_{n}}\\left[\\mathbb{E}_{x\\sim\\zeta_{1,k}^{n}}\\left\\vert h^{2}(x)\\right\\vert\\right]}}\\\\ &{\\le\\left(\\frac{\\sqrt{2}C_{2}}{1-\\hat{\\lambda}}\\right)\\sqrt{\\displaystyle\\sum_{n=0}^{N-1}\\sum_{k=0}^{H_{n}-1}\\left(1+2\\gamma^{2}\\right)}D_{3}(x_{0},K,\\gamma,\\nu)+2(2+\\hat{K})^{2}}\\\\ &{=\\left(\\frac{\\sqrt{2}C_{2}}{1-\\hat{\\lambda}}\\right)\\sqrt{(1+2\\gamma^{2})D_{3}(x_{0},K,\\gamma,\\nu)+2(2+\\hat{K})^{2}}\\sqrt{\\displaystyle\\sum_{n=0}^{N-1}H_{n}}}\\\\ &{=\\left(\\frac{\\sqrt{2}C_{2}}{1-\\hat{\\lambda}}\\right)\\sqrt{(1+2\\gamma^{2})D_{3}(x_{0},K,\\gamma,\\nu)+2(2+\\hat{K})^{2}}\\sqrt{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Next, we use the bound from Curi et al. (2020, Lemma 17.) for the second term. ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sqrt{\\sum_{n=0}^{N-1}\\sum_{k=0}^{H_{n}-1}\\mathbb{E}_{x_{k}^{n},\\ldots x_{1}^{0}|x_{0}}\\left[\\left\\Vert\\sigma_{n}(x_{k}^{n},\\pi_{n}(x_{k}^{n}))\\right\\Vert^{2}\\right]}\\leq C^{\\prime}\\sqrt{\\Gamma_{T}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Here $\\Gamma_{T}$ is the maximum information gain. ", "page_idx": 23}, {"type": "text", "text": "If we set $\\begin{array}{r}{D_{4}(x_{0},K,\\gamma)=\\frac{C^{\\prime}(1+\\sqrt{d_{x}})}{\\sigma}\\left(\\frac{\\sqrt{2}C_{2}}{1-\\hat{\\lambda}}\\right)\\sqrt{(1+2\\gamma^{2})D_{3}(x_{0},K,\\gamma,\\nu)+2(2+\\hat{K})^{2}}}\\end{array}$ we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{n=0}^{N-1}\\sum_{k=0}^{H_{n}-1}\\mathbb{E}_{\\pmb{x}_{k}^{n},\\dots\\pmb{x}_{1}^{0}|\\pmb{x}_{0}}\\left[\\mathbb{E}_{\\pmb{w}_{k}^{n}}\\left[B(\\pi_{n},f_{n},\\pmb{x}_{k+1}^{n})-B(\\pi_{n},f_{n},\\hat{\\pmb{x}}_{k+1}^{n})\\right]\\right]}\\\\ &{\\displaystyle\\leq(1+\\sqrt{d}_{x})^{\\beta_{T}}/\\sigma\\sqrt{\\sum_{n=0}^{N-1}\\sum_{k=0}^{H_{n}-1}\\mathbb{E}_{\\pmb{x}_{k}^{n},\\dots\\pmb{x}_{1}^{0}|\\pmb{x}_{0}}\\left[\\mathbb{E}_{\\pmb{x}\\sim\\zeta_{1,k}^{n}}\\left[h^{2}(\\pmb{x})\\right]\\right]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\times\\sqrt{\\displaystyle\\sum_{n=0}^{N-1}\\sum_{k=0}^{H_{n}-1}\\mathbb{E}_{x_{k}^{n},\\ldots,x_{1}^{0}|x_{0}}\\left[\\left\\Vert\\sigma_{n}(x_{k}^{n},\\pi_{n}(x_{k}^{n}))\\right\\Vert^{2}\\right]}}\\\\ &{\\le(1+\\sqrt{d}_{x})^{\\beta}r/\\sigma\\left(\\displaystyle\\frac{\\sqrt{2}C_{2}}{1-\\hat{\\lambda}}\\right)\\sqrt{(1+2\\gamma^{2})D_{3}(x_{0},K,\\gamma,\\nu)+2(2+\\hat{K})^{2}}\\sqrt{T}C^{\\prime}\\sqrt{\\Gamma_{T}}}\\\\ &{\\le D_{4}(x_{0},K,\\gamma)\\beta_{T}\\sqrt{T\\Gamma_{T}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof for (B): ", "text_level": 1, "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{n=0}^{N-1}\\sum_{k=0}^{H_{n}-1}\\mathbb{E}\\left[B(\\pi,f_{n},x_{k}^{n})-B(\\pi,f_{n},x_{k+1}^{n})\\right]=\\sum_{n=0}^{N-1}\\mathbb{E}\\left[B(\\pi,f_{n},x_{0}^{n})-B(\\pi,f_{n},x_{H_{n}}^{n})\\right]}}\\\\ &{}&{\\leq\\frac{C_{2}}{1-\\hat{\\lambda}}\\sum_{n=0}^{N-1}\\left(2+\\mathbb{E}\\left[V^{\\pi}(x_{0}^{n})+V^{\\pi}(x_{H_{n}}^{n})\\right]\\right)}\\\\ &{}&{\\leq\\frac{2C_{2}}{1-\\hat{\\lambda}}\\sum_{n=0}^{N-1}(1+D(x_{0},K,\\gamma))}\\\\ &{}&{=\\frac{2C_{2}}{1-\\hat{\\lambda}}(1+D(x_{0},K,\\gamma))N}\\\\ &{}&{=D_{5}(x_{0},K,\\gamma)N.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Here $\\begin{array}{r}{D_{5}(\\mathbf{x}_{0},K,\\gamma)=\\frac{2C_{2}}{1-\\hat{\\lambda}}(1+D(\\mathbf{x}_{0},K,\\gamma))}\\end{array}$ . Finally, for our choice, $H_{n}=H_{0}2^{n}$ , we get ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{n=0}^{N-1}H_{n}=H_{0}\\sum_{n=0}^{N-1}2^{n}=H_{0}(2^{N}-1)=T.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore, $\\begin{array}{r}{N=\\log_{2}\\left(\\frac{T}{H_{0}}+1\\right).}\\end{array}$ To this end, we get for our regret ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle R_{T}=\\mathbb{E}\\left[\\sum_{n=0}^{N-1}\\sum_{k=0}^{H_{n}-1}c({\\pmb x}_{k}^{n},{\\pmb\\pi}_{n}({\\pmb x}_{k}^{n}))-A({\\pmb\\pi}^{*})\\right]}}\\\\ {~~~~\\leq{\\cal D}_{4}({\\pmb x}_{0},{\\pmb K},\\gamma)\\beta_{T}\\sqrt{T\\Gamma_{T}}+{\\cal D}_{5}({\\pmb x}_{0},{\\pmb K},\\gamma)N}}\\\\ {~~~\\leq{\\cal D}_{4}({\\pmb x}_{0},{\\pmb K},\\gamma)\\beta_{T}\\sqrt{T\\Gamma_{T}}+{\\cal D}_{5}({\\pmb x}_{0},{\\pmb K},\\gamma)\\log_{2}\\left(\\frac{T}{H_{0}}+1\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "This regret is sublinear for a very rich class of functions. We summarize bounds on $\\Gamma_{T}$ from Vakili et al. (2021) in Table 1. Furthermore, note that $D_{4}({\\bf{x}}_{0},K,\\gamma)\\in(0,\\infty)$ for all $\\pmb{x}_{0}\\in\\mathcal{X}$ with $\\|\\pmb{x}_{0}\\|<\\infty$ $K<\\infty$ $\\gamma\\,\\in\\,(0,1)$ . The same holds for $D_{5}({\\pmb x}_{0},K,\\gamma)$ . Moreover, since $V^{\\pi}({\\pmb x})$ is $\\Theta(\\zeta(\\|{\\pmb x}\\|))$ , both $D_{4}$ and $D_{5}$ are $\\Theta(\\zeta(\\|\\pmb{x}_{0}\\|))$ ", "page_idx": 24}, {"type": "table", "img_path": "ZWNdgc13aw/tmp/a9fb8383c73cdc190e6347653e8a624260ec2f2558a99a2e461dc5d27bb8fdc8.jpg", "table_caption": ["Table 1: Maximum information gain bounds for common choice of kernels. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "B Practical algorithm and Experimental Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this section, we provide the practical algorithm Algorithm 2, provide all hyperparameters used in our experiments in Table 2, and the cost function for the environments. All our experiments within 1-8 hours? on a GPU (NVIDIA GeForce RTX $2080\\ \\mathrm{Ti}$ ).For NEoRL,we use $\\beta_{n}\\,=\\,2$ for all the experiments, except for the Swimmer and the SoftArm environment where we use $\\beta_{n}=1$ ", "page_idx": 25}, {"type": "table", "img_path": "ZWNdgc13aw/tmp/80192c9967a0b921e647f70d925ddbd72ee5aae59626c3f28c52b996b5122029.jpg", "table_caption": ["Table 2: Hyperparameters for results in Section 4. "], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "ZWNdgc13aw/tmp/2c0121aa75119b6a1dd9ea993d278ea04fb95547fae9e6e688ded1952aa4cf70.jpg", "table_caption": ["Table 3: Cost function for the environments presented in Section 4. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We highlight the problem setting, algorithm, and theoretical and empirical results in the abstract and introduction. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 26}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: In Section 2 we highlight the assumptions of our work, which also correspond to the limitations of our theoretical analysis and also the setting for which our algorithm yields theoretical guarantees. Further, in ?? we discuss an alternative set of assumptions to the one made in the main paper. Limitations to the theoretical algorithm are discussed in Section 3.2, where also practical modifications are proposed. In our experiments (Section 4) we evaluate our algorithm on settings where the assumptions are not necessarily satisfied. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should refect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: All theoretical results are accompanied by the relevant assumptions that are listed in Section 2 and we provide all proofs in Appendix A. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We use open-source benchmarks, disclose all hyperparameters in Appendix B, and the practical algorithm is explained in Section 3.2. Furthermore, we provide the code as supplementary material. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 27}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We provide the code as supplementary material and the hyperparameters used in our experiments in Appendix B. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so ^No\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : / /nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: All experiment details are explained thoroughly in Section 3.2 and Section 4.   \nFurthermore, all hyperparameters are listed in Appendix B. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental seting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: All experiments are run with 10 seeds and mean performance with standard error is reported in all our plots. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce theexperiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We give details on computation time in Appendix B ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Our research conforms, in every respect, to the NeurIPs Code of Ethics. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This paper proposes a method to improve exploration in reinforcement learning in the nonepisodic setting, and is not tied to specific applications. As such, it shares the many potential societal consequences that are associated with reinforcement learning and automation as a whole, spanning from environmental impact to concerns on ethics and alignment. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: We do not release high-risk data or models ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We cite all creators whose code we used in our experiments in Section 4. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets. ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We provide the code as supplementary material including a readme that explains how to install and run the code. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose asset isused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}]