[{"type": "text", "text": "MKGL: Mastery of a Three-Word Language ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Lingbing $\\mathbf{Guo^{1,2}}$ , Zhongpu $\\mathbf{Bo}^{3}$ , Zhuo Chen1,2, Yichi Zhang1,2, Jiaoyan Chen4, Yarong Lan1,2, Mengshu $\\mathbf{Sun^{3}}$ , Zhiqiang Zhang3, Yangyifei $\\mathbf{Luo}^{5}$ , Qian $\\mathbf{Li}^{6}$ , Qiang Zhang1,2, Wen Zhang7 2 and Huajun Chen1,2\u2217 ", "page_idx": 0}, {"type": "text", "text": "1College of Computer Science and Technology, Zhejiang University 2ZJU-Ant Group Joint Lab of Knowledge Graph 3Ant Group 4Department of Computer Science, The University of Manchester 5School of Computer Science and Engineering, Beihang University 6School of Computer Science, Beijing University of Posts and Telecommunications 7School of Software Technology, Zhejiang University ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large language models (LLMs) have significantly advanced performance across a spectrum of natural language processing (NLP) tasks. Yet, their application to knowledge graphs (KGs), which describe facts in the form of triplets and allow minimal hallucinations, remains an underexplored frontier. In this paper, we investigate the integration of LLMs with KGs by introducing a specialized KG Language (KGL), where a sentence precisely consists of an entity noun, a relation verb, and ends with another entity noun. Despite KGL\u2019s unfamiliar vocabulary to the LLM, we facilitate its learning through a tailored dictionary and illustrative sentences, and enhance context understanding via real-time KG context retrieval and KGL token embedding augmentation. Our results reveal that LLMs can achieve fluency in KGL, drastically reducing errors compared to conventional KG embedding methods on KG completion. Furthermore, our enhanced LLM shows exceptional competence in generating accurate three-word sentences from an initial entity and interpreting new unseen terms out of KGs. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Knowledge graphs (KGs) are important resources for many data-driven applications, offering structured repositories of factual information that empower a variety of intelligent tasks [1, 2]. Yet, the strides made through the rapid advancement of large language models (LLMs) have challenged the conventional reliance on KGs. Nonetheless, LLMs are often critiqued for their susceptibility to generating factually incorrect or nonsensical outputs\u2013a phenomenon known as the \u201challucination problem\u201d [3, 4]. Many recent studies propose to resort KGs to mitigate this problem [5\u20138]. ", "page_idx": 0}, {"type": "text", "text": "In this paper, we investigate the capacity of LLMs to assimilate and generate knowledge graph facts proficiently. For example, the natural language sentence, \u201cWendee Lee is an actor in Mighty Morphin Power Rangers,\u201d translates into a KG triplet format as (Wendee Lee, actor of, Mighty Morphin Power Rangers). It is worth noting that, English names such as Wendee Lee and Mighty Morphin Power Rangers, while can serve as identifiers for entities, are perceived as atomic elements within the KG framework. They are indivisible and distinct from their constituent words or characters. ", "page_idx": 0}, {"type": "text", "text": "When the LLMs interpret these text identifiers as mere sequences of tokens, they risk producing output that misrepresents entities or relations, therefore compromising the integrity of KG-based tasks. Consequently, existing research that integrates LLMs with KGs tends to limit its scope to relatively straightforward tasks. Examples of these limitations include validating the correctness of ", "page_idx": 0}, {"type": "image", "img_path": "eqMNwXvOqn/tmp/ae171780c62d50cdd7899017264c7a0318aa84f3bef5278a1c7952a1aa3e4523.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: A workflow of MKGL (from bottom to top). The instruction to the LLM includes a dictionary exemplifying the entity $e_{i}$ and relation $r_{k}$ . The task is to construct new KG sentences initialized with $e_{i}r_{k}$ . The tokenizer first tokenizes the input text, where the entities and relations are represented as special tokens out of the original vocabulary. (a) To process these special tokens, MKGL collects the embeddings of their constituting text tokens; (b) Then, a retriever performs a 4-step process to aggregate textual and relational information into KGL token embeddings. The first and the last steps are LoRA-like down-scaling and up-scaling operations [12]; (c) The output is assigned as the embeddings of these special KGL tokens; (d) Similar to the context retriever, we design a score retriever to retriever the score information. (f) The output is in a form of probability distribution among candidate entities. ", "page_idx": 1}, {"type": "text", "text": "fully-formed triplets [9], or picking an appropriate entity from a limited set of options [10]. Given the sheer volume of entities in a KG, such narrow applications fall short in addressing more complicated tasks like KG completion, wherein a model predicts missing components of a provided incomplete triplet, e.g., identifying the unknown tail entities in (Wendee Lee, actor of, ?) against thousands of candidates. While these methods may lean on pretrained KG embedding models to narrow down possible candidates, the process remains inefficient. ", "page_idx": 1}, {"type": "text", "text": "To transcend the limitations on predictive scope, we propose a novel approach, named MKGL, to instruct an LLM in the lexicon of the unique $K G$ language (KGL). KGL sentences are strictly threeword sentences, starting with an entity noun, followed by a relation verb, and ending with another entity noun. The vocabulary of KGL does not immediately resonate with the machines. A common triplet like (Wendee Lee, actor of, Mighty Morphin Power Rangers) is encoded abstractly as $e_{i}r_{k}e_{j}$ , with $e_{i}$ , $e_{j}$ symbolizing the entity nouns and $r_{j}$ denoting the relation verb. For an LLM such as Llama-2 [11], these symbols are entirely alien, absent from its pretraining corpus. Our investigation thus centers on how an LLM can navigate and master this specialized, atomic language of KGs. ", "page_idx": 1}, {"type": "text", "text": "As illustrated in Figure 1, to bridge this comprehension gap, we introduce an English-KGL dictionary, and the LLM is supposed to assemble new KG sentences using the provided linguistic building blocks. The basic elements of KGL, while different from our natural language, are familiar to the LLM as they are constructed from the pretrained token embeddings. We leverage a context retriever to retrieve the text information and relational information of a KGL token, which transforms the sequential token embeddings of its name into an embedding vector. Subsequently, we update the LLM token embedding layer with new KGL token embeddings. In the scoring layer, we also employ a KG score retriever to supplement the LLM with extra KG relational information for prediction. ", "page_idx": 1}, {"type": "text", "text": "Instructing an LLM in KGL offers three main advantages over prompt-based methods [10, 13] or conventional KG embedding methods [14, 15]: (1) Broadened applicability. KGL tokens originate from textual tokens of an LLM, thus our method does not mandate that all entities be observed during training. (2) End-to-end framework. Unlike recent LLM-based methods that necessitate pre-sorted results from conventional KG embedding methods, our approach can rapidly rank all candidate entities at one-step. (3) High efficiency. The representations of KG tokens are derived from pretrained token embeddings rather than learned from scratch. The proposed KGL context and score retrievers also leverage a LoRA-like adaption. Using Llama-2-7b [11] as the base LLM, the number of training parameters is less than $0.3\\%$ . ", "page_idx": 1}, {"type": "text", "text": "However, instructing an LLM in KGL also has its limitations, as it demands more computational resources compared with conventional methods. For instance, fine-tuning MKGL (Llama-2-7b) on the FB15k-237 dataset [16] to outperform most conventional methods requires only 1 epoch. Nevertheless, with 8 A100 GPUs, it still takes half an hour, which is comparable to training a TransE model from scratch with a single GPU. ", "page_idx": 2}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We category the related works into two groups: ", "page_idx": 2}, {"type": "text", "text": "Knowledge Graph Completion KG completion can be regarded as a classification problem like many NLP tasks [17\u201321], such as node classification and semantic role labeling. However, its label space is significantly larger than most NLP tasks. For example, the WN18RR [22] dataset contains over 40,000 different entities, making it impractical to simply feed them all as possible results and let the LLM select one as output. Most conventional KG completion methods are embedding-based methods, including the triplet-based methods [23\u201327], e.g, TransE [23], ComplEx [25], RotatE [26]; the GNN-based methods [15, 28\u201332], e.g., DAN [15], CompGCN [28], CoKE [29]; and other neuralbased methods [22, 33\u201335], e.g., ConvE [22] and RSN [34]. Despite differences in their neural methods and input forms, all these methods focus on relational information and are not good at utilizing other types of information such as textual attributes. ", "page_idx": 2}, {"type": "text", "text": "Pretrained Language Models for Knowledge Graphs Leveraging Pretrained language models for KG completion has been explored for many years [36]. Some works treat BERT as a GNN model to encode graph features [30, 37], while others consider the textual information of KGs and use pretrained BERT to encode the textual labels of entities and relations [14, 38\u201340]. The resulting outputs are regarded as the entity and relation embeddings or their concatenations. ", "page_idx": 2}, {"type": "text", "text": "With the rapid advancements in leveraging LLMs for KG completion, recent works have begun designing prompts or instructions to guide LLMs in this task. Initially, the results were not promising [41], as it appeared that even state-of-the-art LLMs without context information could not outperform basic KG embedding models like TransE. However, subsequent works such as KGLlama [42] and KoPA [13] discovered that LLMs might perform better in triplet classification, i.e., estimating the correctness of a given triplet. ", "page_idx": 2}, {"type": "text", "text": "More recently, KICGPT [10] has proposed leveraging in-context learning [43, 44] to provide explicit instructions and guide the behavior of LLMs. This involves a triplet-based KG embedding model to generate the initial rankings of the top-k entities, followed by a multi-round interaction with the LLM, providing textual information and triplet demonstrations for the query entity and relation. The LLM should then re-rank the initial list. KICGPT has achieved state-of-the-art results on KG completion tasks. However, its performance not only depends on the LLM and the instructions but also on the pretrained KG embedding model. Additionally, KICGPT cannot be deployed offilne due to the demand of commercial LLMs [45]. It also cannot provide embeddings for downstream tasks. ", "page_idx": 2}, {"type": "text", "text": "In contrast, the proposed MKGL has an embedding module based on the LLM token embeddings and KG relational information, which overcomes the weaknesses of existing KG embedding methods that cannot provide embeddings for unseen entities. The context information is implicitly encoded into the KGL token embeddings and efficiently captured by the LLM during fine-tuning. ", "page_idx": 2}, {"type": "text", "text": "3 Mastery of KG Language ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we discuss the details of MKGL. We first introduce the general architecture of an LLM and how to convert a KG triplet into a fine-tuning instruction. Then, we present the details of constructing KGL token embeddings and scores. Finally, we illustrate how to train an MKGL and analyze its complexity. ", "page_idx": 2}, {"type": "text", "text": "3.1 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We start by a brief introduction to KGs and LLMs. ", "page_idx": 2}, {"type": "text", "text": "Knowledge Graph and Knowledge Graph Language Knowledge graphs are conceptualized as directed, multi-relational graphs. We describe a knowledge graph by $\\bar{\\boldsymbol{\\mathcal{G}}}=(\\mathcal{T},\\mathcal{E},\\mathcal{R})$ , where $\\tau,\\varepsilon$ , $\\mathcal{R}$ are the sets of triplets, entities, and relations, respectively. KG language (KGL) is construed as a rigorously defined three-word construct, mirroring the structure of a simple sentence. Specifically, a KGL sentence $e_{i}r_{k}e_{j}$ invariably commences with an entity noun $e_{i}\\in\\mathcal{E}$ , proceeds with a relation verb $r_{k}\\in\\mathcal{R}$ , and culminates with another entity noun $e_{j}\\in\\mathcal{E}$ . Analogous to the syntactic conventions in Chinese, KGL sentences eschew the use of spaces or commas to demarcate KGL terms. ", "page_idx": 3}, {"type": "text", "text": "Knowledge Graph Completion KG completion is one of the most important tasks in the KG area. The target of KG completion is to predict the head entity $e_{i}$ given the relation and tail entity $(?,r_{j},e_{k})$ , or predict the tail entity $e_{k}$ given $(e_{i},r_{j},?)$ . In the scenario of KGL, this task is equivalent to completing the KG sentence $\\because r_{j}e_{k}$ or $e_{k}r_{j}?$ . ", "page_idx": 3}, {"type": "text", "text": "The inductive KG completion focus on completing an unobserved KG $\\mathcal{G}_{\\mathrm{ind}}\\;=\\;(\\mathcal{T}_{\\mathrm{ind}},\\mathcal{E}_{\\mathrm{ind}},\\mathcal{R}_{\\mathrm{ind}})$ . Specifically, the relation set ${\\mathcal{R}}_{\\mathrm{ind}}$ is identical to the original set $\\mathcal{R}$ , but the inductive entity set $\\mathcal{E}_{\\mathrm{ind}}$ shares no elements with $\\mathcal{E}$ , i.e., $\\mathcal{E}_{\\mathrm{ind}}\\cap\\mathcal{E}=\\mathcal{D}$ . The triplet set ${\\mathcal{T}}_{\\mathrm{ind}}$ is further split into the fact set ${\\mathcal{T}}_{\\mathrm{ind-fact}}$ and test set $\\mathcal{T}_{\\mathrm{ind-test}}$ . We train a model on the original triplet set $\\tau$ and use the fact set ${\\mathcal{T}}_{\\mathrm{ind}}$ -fact as context to evaluate it on the test set $\\mathcal{T}_{\\mathrm{ind-test}}$ . ", "page_idx": 3}, {"type": "text", "text": "Large Language Models As depicted on the left side of Figure 1, the architecture of a typical LLM can be divided into four main components: ", "page_idx": 3}, {"type": "text", "text": "\u2022 Tokenizer, which breaks down the input sequence of words $w_{0},w_{1},...,w_{m}$ into tokens $t_{0},t_{1},...,t_{n}$ ; \u2022 Token embedding, which maps the input tokens $t_{0},t_{1},...,t_{n}$ to a sequence of low-dimensional vectors $\\mathbf{t}_{0},\\mathbf{t}_{1},...,\\mathbf{t}_{n}$ ; \u2022 Transformer $\\mathcal{M}$ , the core of the LLM, which consists of multiple attention-based blocks that process the input token embeddings into hidden states: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{h}_{0},\\mathbf{h}_{1},...,\\mathbf{h}_{n}={\\mathcal{M}}(\\mathbf{t}_{0},\\mathbf{t}_{1},...,\\mathbf{t}_{n});\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "\u2022 Score layer, which features a weight matrix $\\textbf{S}\\in\\mathbb{R}^{N\\times d}$ with an identical shape to the token embedding matrix $\\mathbf{T}\\in\\mathbb{R}^{N\\times d}$ , where $N$ , $d$ denote the vocabulary size and hidden size, respectively. The score layer projects the output of Transformer at the $n$ -th step to a probability distribution $\\mathbf{p}_{n+1}$ for predicting the next token $t_{n+1}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{p}_{n+1}=\\mathbf{h}_{n}\\mathbf{S},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "3.2 Instruct an LLM in KG Language ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Recent studies reveal that LLMs harbor the potential to acquire unfamiliar natural languages [46, 47]. Given this premise, it is of particular interest to investigate how LLMs might interpret and operate within our KGL. We first design a prototype instructional text for this purpose. For a given triplet (Wendee Lee, actor of, Mighty Morphin Power Rangers), suppose that the task is to predict the tail entity Mighty Morphin Power Rangers, the instructional text is formatted as follows: ", "page_idx": 3}, {"type": "text", "text": "Instruction 3.1. Supposed that you are a linguist versed in an esoteric three-word knowledge graph language. Given the following dictionary comprising two words from this language, please kindly complete the following sentence: <kgl: Wendee Lee><kgl: actor of> ", "page_idx": 3}, {"type": "table", "img_path": "eqMNwXvOqn/tmp/ab6e6ca1acb91abdd2c7a46f7afba1b95eae768461434b466191f003be45e6b1.jpg", "table_caption": [], "table_footnote": ["Table 1: An illustrative KGL-to-English dictionary. "], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Here, <kgl: Wendee Lee> denotes the definitive KGL token (corresponding to $e_{i}$ in previous sections and Figure 1) assigned to the entity Wendee Lee. We enrich the tokenizer\u2019s vocabulary with all pertinent KGL tokens, thereby enabling it to translate these KGL tokens into token IDs, which append sequentially to the LLM\u2019s original vocabulary range. It is worth noting that we only provide at most one example KGL sentence for each KGL word. Our intention is to introduce the schematics of KGL sentences to the LLM, rather than leveraging augmented KG data for in-context learning. To mitigate potential biases, the example sentences are sampled randomly. ", "page_idx": 3}, {"type": "image", "img_path": "eqMNwXvOqn/tmp/c9110bd82d8cf4a5a7828fae908a3876424de5af9e90ace3efaf2a3999e594e6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 2: Illustration of LoRA-based KGL Context Retriever. (a) The token embeddings are first scaled down to lower-dimensional vectors; (b) For each input KGL token, their constituting textual token embeddings are aggregated by a PNA encoder; (c) The output embeddings are further aggregated by multi-layered PNA encoders to retrieve neighboring information within KG; (e) The final embeddings are assigned to the KGL tokens. ", "page_idx": 4}, {"type": "text", "text": "3.3 In-Context Learning versus Special Token Embedding ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The practice of incorporating supplementary context information alongside instructional prompts, known as in-context learning (ICL), has proven effective in enhancing performance across many NLP tasks [43, 44]. However, the concatenation of retrieved context on KGs with the input text can easily exceed the input length constraints of LLMs. Processing such long input sequences remains computationally intensive even with truncation. To address these constraints, we propose an alternative approach to encode context information into compact vector representations. Our experiments in Section 4.6 also demonstrate its superiority in terms of both efficiency and performance. ", "page_idx": 4}, {"type": "text", "text": "3.4 LoRA-based KGL Context Retriever ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We propose the low-rank adaption (LoRA)-based KGL context Retriever $R_{\\mathrm{context}}$ to effectively aggregate textual and KG information into KGL token embeddings. Typically, the vocabulary scope of a KG (comprising both entities and relations) usually surpasses that of an LLM. For instance, WN18RR is a KG completion dataset set sampled from WordNet [48]. It has over 40,000 unique entities, while the vocabulary size of Llama-2-7b is 32,000. Therefore, initializing new token embeddings for each KG elements and optimizing them from scratch would be prohibitively resource-intensive. ", "page_idx": 4}, {"type": "text", "text": "Moreover, the dynamic nature of real-world KGs consistently introduces new entities. This is analogous to the evolution of human language, where new words are often synthesized or derived from existing ones. Drawing inspiration from this linguistic adaptability, we propose leveraging existing textual tokens to generate new KGL tokens, thereby avoiding the computational burden of learning unique embeddings for every KG element. ", "page_idx": 4}, {"type": "text", "text": "Scale Down As illustrated in Figure 2, the first step is to reduce the dimensionality of LLM token embeddings to lower computational demands during text and KG context aggregation. Inspired by LoRA [12], we leverage a projection matrix $\\mathbf{W}_{T}\\in\\mathbb{R}^{d\\times r}$ to transform the token embedding matrix $\\mathbf{T}\\in\\mathbb{R}^{N\\times d}$ into a reduced space $\\mathbb{R}^{N\\times r}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{T}_{r}=\\mathbf{T}W_{T},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{T}_{r}\\in\\mathbb{R}^{N\\times{r}}$ represents the compact token embedding matrix. ", "page_idx": 4}, {"type": "text", "text": "Retrieve Text Information We leverage a text encoder to encode the textual token embeddings of each KGL token into a unified vector. For example, the entity name \u201cMighty Morphin Power Rangers\u201d would be converted into individual token embeddings $\\mathbf{t}_{e_{i},0},\\mathbf{t}_{e_{i},1},...,\\mathbf{t}_{e_{i},n}$ , which are then aggregated into a single vector for the entity $e_{i}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{t}_{e_{i}}=\\mathscr{E}_{\\mathrm{text}}(\\mathbf{t}_{e_{i},0},\\mathbf{t}_{e_{i},1},...,\\mathbf{t}_{e_{i},n}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{t}_{e_{i}}$ is the textual token embedding for $e_{i}$ . The choice of the encoder $\\mathcal{E}_{\\mathrm{text}}$ is free. In this paper, we leverage principal neighbourhood aggregation (PNA) [49], which can be roughly understood as applying multiple pooling operations (including max, min, mean, std etc.) on the token embedding sequences. A detailed introduction to PNA can be found in Appendix C. ", "page_idx": 5}, {"type": "text", "text": "Retrieve KG Information We employ a multi-layered PNA encoder $\\mathcal{E}_{\\mathrm{kg}}$ to aggregate the KG information of $e_{i}$ and its adjacent entities, which can be formulated as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{t}_{e_{i}}^{\\prime}=\\mathcal{E}_{\\mathrm{kg}}(\\mathbf{t}_{e_{i}},\\mathcal{N}(e_{i})),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathcal{N}(e_{i})$ denotes the neighboring entities to $e_{i}$ . The adoption of PNA for encoding both textual and relational data of KGL tokens is due to its parameter efficiency and superior performance compared to attention-based alternatives like GAT [50]. An empirical comparison of different encoders can be found in Appendix F. ", "page_idx": 5}, {"type": "text", "text": "Scale Up To finalize, we adjust the dimensionality of the output embeddings to align with the LLM input requirements: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{t}_{e_{i}}^{\\prime\\prime}=\\mathbf{t}_{e_{i}}^{\\prime}\\mathbf{W}_{B}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "For the sake of clarity, we will continue to use $\\mathbf{t}_{e_{i}}$ to represent the KGL token embedding in subsequent discussions. For efficiency, we retrieve the KG information only for entities. This operation also make the embeddings of entities and relations distinguishable. ", "page_idx": 5}, {"type": "text", "text": "3.5 Reconstructing Vocabulary or Constraining the Output Space ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "While recent studies have adapted LLMs to various tasks by either restricting the output space or reformulating tasks into multiple-choice questions [9, 10, 51\u201353], such strategies pose challenges for KG completion. Specifically, the existing methods are inapplicable to entity constrastive learning as their main objective is optimized against text tokens instead of entities. Also, they incur significantly slow inference times, as the LLM must traverse to the output tree\u2019s leaf nodes to generate predictions. Even then, the generation of top- $k$ results, dependent on beam search parameters, may not accurately reflect the true likelihoods of entities. ", "page_idx": 5}, {"type": "text", "text": "In contrast, in this paper we propose a new approach to reconstruct the KGL scores through LLM\u2019s score layer and hidden states, providing a one-shot probability distribution for all candidates. Our method seamlessly integrates with contrastive loss and negative sampling techniques [54], making it highly compatible with prevalent KG completion frameworks. This compatibility also ensures that MKGL has the potential of being applied for downstream KG embedding tasks [32, 55]. ", "page_idx": 5}, {"type": "text", "text": "3.6 LoRA-based KGL Score Retriever ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We propose a LoRA-based KGL score retriever $R_{\\mathrm{score}}$ to produce the probability distribution of KGL tokens, which can be formulated as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{S}^{\\prime}=\\mathbf{S}W_{S},\\quad\\mathbf{h}_{n}^{\\prime}=\\mathbf{h}_{n}\\mathbf{W}_{H}}\\\\ {\\mathbf{s}_{j}^{\\prime}=S_{\\mathrm{text}}(\\mathbf{s}_{j,0}^{\\prime},\\mathbf{s}_{j,1}^{\\prime},...,\\mathbf{s}_{j,n}),}\\\\ {\\mathbf{s}_{e_{j}\\mid e_{i},r_{k}}^{\\prime\\prime}=S_{\\mathrm{kg}}([\\mathbf{h}_{n}^{\\prime},\\mathbf{s}_{j}^{\\prime}],\\mathcal{N}(e_{j}))}\\\\ {p_{e_{j}\\mid e_{i},r_{k}}=\\mathbf{s}_{e_{j}\\mid e_{i},r_{k}}^{\\prime\\prime}\\mathbf{W}_{O}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{(Down\\;Scaling)}}\\\\ &{\\mathrm{(Text\\;Information\\;Retrieval)}}\\\\ &{\\mathrm{(Conditioned\\;Retrieval)}}\\\\ &{\\mathrm{(Score\\;Estimation)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The score retriever also starts from a down-scaling layer to reduce the dimensionality of the score matrix $\\mathbf{S}\\in\\mathbb{R}^{N\\times d}$ to $\\mathbf{S}_{R}\\in\\mathbb{R}^{N\\times r}$ with $\\mathbf{W}_{S}$ , and similarly scales down the LLM\u2019s output hidden vector $\\mathbf{h}_{n}$ with $\\mathbf{W}_{H}$ . Subsequently, the text information (i.e., the token score vectors $\\mathbf{s}_{j,0}^{\\prime},\\mathbf{s}_{j,1}^{\\prime},...,\\mathbf{s}_{j,n})$ associated with target entity $e_{j}$ is fed to the score text encoder $\\mathcal{S}_{\\mathrm{text}}$ to construct the KGL score vector $\\mathbf{s}_{j}^{\\prime}$ . It is then concatenated with the LLM hidden state $\\mathbf{h}_{n}^{\\prime}$ to obtain the conditioned input $[\\mathbf{h}_{n}^{\\prime},\\mathbf{s}_{j}^{\\prime}]$ . Upon gathering the neighboring information of the target entities via a multi-layered PNA ${\\mathcal{S}}_{\\mathrm{kg}}$ , an output matrix $\\mathbf{W_{O}}\\,\\in\\,\\mathbb{R}^{r\\times1}$ is employed to map the result $\\mathbf{s}_{e_{j}\\,|e_{i},r_{k}}^{\\prime\\prime}\\,\\in\\,\\mathbb{R}^{r}$ to the 1-d probability estimate $p_{e_{j}|e_{i},r_{k}}\\in\\mathbb{R}$ . ", "page_idx": 5}, {"type": "table", "img_path": "eqMNwXvOqn/tmp/52a62d8dd92c2bb1cd643aa7ccafc669e5d847b0fb004613259fb9c1da22812b.jpg", "table_caption": ["Table 2: The KG completion results on FB15k-237 and WN18RR. The best and second-best results are boldfaced and underlined, respectively. \u2191: higher is better; \u2193: lower is better. -: unavailable entry. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Optimization With the above score retriever, estimating the probability for any candidate entity becomes straightforward at a single step. To refine MKGL, we consider a contrastive loss leveraged in most existing KG embedding methods [22, 23, 28, 34], expressed as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\sum_{(e_{i},r_{k},e_{j})\\in\\mathcal{T}_{\\mathrm{ruin}}}\\Big[-\\log(p_{e_{j}|e_{i},r_{k}})+\\frac{1}{|\\mathcal{N}_{\\mathrm{neg}}(e_{j})|}\\sum_{e_{\\mathrm{neg}}\\in\\mathcal{N}_{\\mathrm{neg}}(e_{j})}\\log(1-p_{e_{\\mathrm{neg}}|e_{i},r_{k}})\\Big],\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mathcal{N}_{\\mathrm{neg}}(e_{j})\\;=\\;\\{e_{\\mathrm{neg}}|e_{\\mathrm{neg}}\\;\\neq\\;e_{j},e_{\\mathrm{neg}}\\;\\in\\;\\mathcal{E}\\}$ is the sampled negative entity set for the target entity $e_{j}$ . The loss function $\\mathcal{L}$ is in a form of a binary cross-entropy, contrasting the likelihood of correctly predicting the relation $p_{e_{j}|e_{i},r_{k}}$ as positive example, against the probabilities of erroneously predicting relations $(e_{i},r_{k},e_{\\mathrm{neg}})$ as negative examples. We also present an algorithm to demonstrate the step-by-step fine-tuning process, please refer to Appendix $\\mathrm{D}$ for details. ", "page_idx": 6}, {"type": "text", "text": "3.7 Complexity ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "It is clear that the primary computational cost for MKGL lies in the LLM. By employing LoRA-based KGL retrievers to retrieve context vectors instead of texts, we can significantly reduce the major expenditure. For instance, our retrievers can reduce the average input lengths from 811.2 to 91.4 on the FB15k-237 dataset, compared to using one-hop neighbors for in-context learning. All operations within the LoRA-based retrievers are performed under low dimensionality. Furthermore, the token embeddings and score matrix of the LLM are frozen during fine-tuning, thus ignoring their gradient computation. In the worst case, the complexity of text information retrieval is $\\bar{O}(N_{\\mathrm{kgl}}\\bar{L}_{\\mathrm{kgl}}r)$ , where $N_{\\mathrm{kgl}}$ , $L_{\\mathrm{kgl}}$ , $r$ are the number of KGL tokens, maximum text token lengths of KGL tokens, and the reduced dimensionality, respectively. Subsequently, the complexity of KG information retrieval in the worst case is linear to the number of triplets, i.e., $\\bar{\\mathcal{O}}(|\\mathcal{T}|N_{\\mathrm{layer}}\\bar{r})$ , where $|\\tau|$ , $N_{\\mathrm{layer}}$ denote the number of triplets in the KG and the number of PNA layer, respectively. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we evaluate the performance of the proposed MKGL through extensive experiments, comparing it against both LLM-based and KG embedding methods. The source code and datasets are available at github.com/zjukg/MKGL. ", "page_idx": 6}, {"type": "text", "text": "4.1 Datasets ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We evaluate MKGL on the FB15k-237 and WN18RR datasets, which are widely used by most KG completion methods [22, 23, 26, 28, 34, 56, 57]. We also evaluate MKGL on the inductive version of ", "page_idx": 6}, {"type": "image", "img_path": "eqMNwXvOqn/tmp/7b89fa4678a7bab6c5b1c0c23c47673494adbc72a9451cd62c11fe844b89b945.jpg", "img_caption": ["Figure 3: Illustration of KGL modeling. The left shows the performance degradation (in lighter shades) from consecutive predictions of relations and entities. The right presents sentences generated by MKGL, with deeper hues indicating higher probabilities. In the final column, colors grey, green, yellow, and red represent existing, valid, valid but not within the KG, and invalid, respectively. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "these two datasets [58]. We follow REDGNN [59] to evaluate MKGL on all entities rather than 50 sampled candidates. Please refer to Appendix E for dataset statistics. ", "page_idx": 7}, {"type": "text", "text": "4.2 Settings ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "For our experiments, we employ Llama-2-7b [11] as the base LLM and train MKGL using 8 A100 GPUs. A standard LoRA adaptation is applied to the query and value layers of the LLM. Full hyper-parameter details are available in Appendix D. We evaluate performance using MRR (mean reciprocal rank of target entities) and Hits $@k$ (percentage of target entities ranked in the top $k$ ). ", "page_idx": 7}, {"type": "text", "text": "Our baselines include conventional KG embedding methods such as TransE [23], RotatE [26], and TuckER [56]; GNN-based methods like CompGCN [28], DAN [15], and CoKE [29]; methods that integrate language models including KG-BERT [14], StAR [38], KGLM [40], FTL-LM [39], and DET [30]; and LLM-based methods: KG-Llama [42], GPT 3.5 [41], and KICGPT [10]. In the inductive scenario, we compare against rule-based reasoning methods such as RuleN [60], NeuralLP [33], DRUM [61], GraIL [58] and RED-GNN [59], acknowledging that standard methods fail to predict relations without entity embeddings. ", "page_idx": 7}, {"type": "text", "text": "4.3 Knowledge Graph Completion ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The knowledge graph completion results are presented in Table 2. MKGL outperforms other baselines in nearly all metrics. Notably, MKGL and KICGPT significantly surpass other LLM-based methods, demonstrating the importance of KG relational information. Contrarily, many BERT-based methods fall short against GNN-based methods, suggesting that merely incorporating text information may not yield the anticipated beneftis. In summary, the proposed MKGL clearly outshines its counterparts, particularly those founded on commercial LLMs. ", "page_idx": 7}, {"type": "text", "text": "To our knowledge, existing LLM-based methods have not addressed the inductive KG completion challenge. We benchmark MKGL against the state-of-the-art inductive methods. Although we can not assess KICGPT [10] due to unavailable source code, it is worth mentioning that our MKGL could potentially augment KICGPT by supplying a candidate list, facilitating seamless integration between the two methods. We present the results in Table 3. MKGL significantly outperforms all the baseline methods across metrics. As most inductive reasoning methods do not have an embedding module for entities, the proposed MKGL represents the first embedding-based method to deliver state-of-the-art results in inductive KG completion. ", "page_idx": 7}, {"type": "table", "img_path": "eqMNwXvOqn/tmp/e21e0eebc6e375868cd864c56d537b8702bb13ffa32538e67a6bf3f06040c261.jpg", "table_caption": ["Table 4: Ablation studies on FB15k-237 and WN18RR. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "4.4 Knowledge Graph Language Modeling ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Beyond its capability as a KG completion tool, MKGL also serves as a language model for KG languages. To evaluate its proficiency in generating KGL sentences, we employ a sequence generation loss and remove the relation context from the input prompts. We leverage the second-to-last output of the LLM for relation prediction. ", "page_idx": 8}, {"type": "text", "text": "The results are shown in Figure 3. The left section contrasts the sequence prediction results against standard KG completion, revealing only a modest loss in performance. MKGL still outperforms many conventional methods, especially on WN18RR dataset. The right panel displays sample sentences generated by MKGL, illustrating its potential to discover legitimate KGL sentences absent from the existing KG. We observe that WN18RR is more difficult than anticipated as it contains many plausible entities that challenge even an LLM\u2019s discernment. ", "page_idx": 8}, {"type": "text", "text": "4.5 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We conduct ablation studies to assess the importance of each module, as detailed in Table 4. The unmarked cells indicate that we either substitute the text retrieval module with a learnable embedding module or remove the KG retrieval module. Clearly, the method with complete features achieves best results, while replacing or removing either module significantly impacts performance. Notably, removing the KG retrieval module yields more performance loss on WN18RR, as many entities in this dataset have similar names. For example, there are 14 different entities named \u201ccall\u201d. In this case, incorporating with KG information becomes necessary. ", "page_idx": 8}, {"type": "text", "text": "4.6 Computational Cost ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We examine the computational efficiency of our method (MKGL) relative to \u201cin-context\u201d baselines. Specifically, we develop several variants: LLM randomly-initialized new entity token embeddings (NewToken), LLM with KGL context from 1-hop neighbors (NewToken (1-hop)), LLM with KGL context from 2-hop neighbors (NewToken (2-hop)), and MKGL without score retriever (MKGL w/o ", "page_idx": 8}, {"type": "text", "text": "SR). The results are shown in Figure 4. MKGL surpasses all alternatives in performance. NewToken variants slightly lag behind MKGL w/o SR, but notably, our proposed methods demand fewer trainable parameters than NewToken variants. By encoding all context information within KGL token embeddings, the average input length is significantly reduced, which decreases training time considerably. Moreover, MKGL supports larger batch sizes during both training and inference phases, enhancing computational efficiency. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we propose MKGL to instruct the LLM in the language of KGs. MKGL employs a context retriever that efficiently provides LLMs with pertinent textual and relational context, markedly reducing input lengths relative to in-context-learning and supervised fine-tuning methods. Meanwhile, MKGL also leverages a score retriever to supply score information and aid in KGL inference. Extensive experiments confirm the superiority of MKGL in terms of both performance and computational efficiency. The proposed context and score retrievers point out a new direction in incorporating LLMs with semantic data, such as question answering and entity linking. They may also shed lights on a more broaden area where the input cannot be precisely represented by text, e.g., node classification and protein representation learning. Furthermore, the construction of KGL vocabulary enables contrastive learning not only limited on tokens, which may provide insights on general machine learning. Therefore, there are plenty of future directions. We would like to pretrain LLM using the mixture corpora of KG and natural languages, such that the LLM could understand and create responses with linked data. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is founded by National Natural Science Foundation of China (NSFC62306276/NSFCU23B2055/NSFCU19B2027/NSFC6240072039), Zhejiang Provincial Natural Science Foundation of China (No. LQ23F020017), Yongjiang Talent Introduction Programme (2022A-238-G), and Fundamental Research Funds for the Central Universities (226-2023-00138). This work was supported by AntGroup. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and Philip S. Yu. A survey on knowledge graphs: Representation, acquisition and applications. CoRR, abs/2002.00388, 2020.   \n[2] Andrea Rossi, Donatella Firmani, Antonio Matinata, Paolo Merialdo, and Denilson Barbosa. Knowledge graph embedding for link prediction: A comparative analysis. CoRR, abs/2002.00819, 2020.   \n[3] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):1\u201338, 2023.   \n[4] Vipula Rawte, Amit Sheth, and Amitava Das. A survey of hallucination in large foundation models. arXiv preprint arXiv:2309.05922, 2023.   \n[5] Linyao Yang, Hongyang Chen, Zhao Li, Xiao Ding, and Xindong Wu. Chatgpt is not enough: Enhancing large language models with knowledge graphs for fact-aware language modeling. arXiv preprint arXiv:2306.11489, 2023.   \n[6] Xinyan Guan, Yanjiang Liu, Hongyu Lin, Yaojie Lu, Ben He, Xianpei Han, and Le Sun. Mitigating large language model hallucinations via autonomous knowledge graph-based retroftiting. In AAAI, pages 18126\u201318134, 2024.   \n[7] Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xindong Wu. Unifying large language models and knowledge graphs: A roadmap. IEEE Transactions on Knowledge and Data Engineering, 2024.   \n[8] Jeff Z Pan, Simon Razniewski, Jan-Christoph Kalo, Sneha Singhania, Jiaoyan Chen, Stefan Dietze, Hajira Jabeen, Janna Omeliyanenko, Wen Zhang, Matteo Lissandrini, et al. Large language models and knowledge graphs: Opportunities and challenges. arXiv preprint arXiv:2308.06374, 2023.   \n[9] Yichi Zhang, Zhuo Chen, Wen Zhang, and Huajun Chen. Making large language models perform better in knowledge graph completion. arXiv preprint arXiv:2310.06671, 2023.   \n[10] Yanbin Wei, Qiushi Huang, James T Kwok, and Yu Zhang. Kicgpt: Large language model with knowledge in context for knowledge graph completion. arXiv preprint arXiv:2402.02389, 2024.   \n[11] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[12] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.   \n[13] Yichi Zhang, Zhuo Chen, Wen Zhang, and Huajun Chen. Making large language models perform better in knowledge graph completion. arXiv preprint arXiv:2310.06671, 2023.   \n[14] Liang Yao, Chengsheng Mao, and Yuan Luo. Kg-bert: Bert for knowledge graph completion. arXiv preprint arXiv:1909.03193, 2019.   \n[15] Lingbing Guo, Zhuo Chen, Jiaoyan Chen, Yichi Zhang, Zequn Sun, Zhongpu Bo, Yin Fang, Xiaoze Liu, Huajun Chen, and Wen Zhang. Distributed representations of entities in open-world knowledge graphs. Knowledge-Based Systems, page 111582, 2024.   \n[16] Kurt D. Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. Freebase: A collaboratively created graph database for structuring human knowledge. In SIGMOD, pages 1247\u20131250, 2008.   \n[17] Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In KDD, pages 855\u2013864, 2016.   \n[18] William L. Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In NIPS, pages 1024\u20131034, 2017.   \n[19] Chaoyi Ai and Kewei Tu. Frame semantic role labeling using arbitrary-order conditional random fields. In AAAI, pages 17638\u201317646. AAAI Press, 2024.   \n[20] Roberto Navigli, Marco Pinto, Pasquale Silvestri, Dennis Rotondi, Simone Ciciliano, and Alessandro Scir\u00e8. Nounatlas: Filling the gap in nominal semantic role labeling. In $A C L\\left(l\\right)$ , pages 16245\u201316258. Association for Computational Linguistics, 2024.   \n[21] Jinan Zou, Maihao Guo, Yu Tian, Yuhao Lin, Haiyao Cao, Lingqiao Liu, Ehsan Abbasnejad, and Javen Qinfeng Shi. Semantic role labeling guided out-of-distribution detection. In LREC/COLING, pages 14641\u201314651. ELRA and ICCL, 2024.   \n[22] Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. Convolutional 2D knowledge graph embeddings. In AAAI, pages 1811\u20131818, 2018.   \n[23] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Dur\u00e1n, Jason Weston, and Oksana Yakhnenko. Translating embeddings for modeling multi-relational data. In NIPS, pages 2787\u20132795, 2013.   \n[24] Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. Embedding entities and relations for learning and inference in knowledge bases. In ICLR, 2015. URL http: //arxiv.org/abs/1412.6575.   \n[25] Th\u00e9o Trouillon, Johannes Welbl, Sebastian Riedel, \u00c9ric Gaussier, and Guillaume Bouchard. Complex embeddings for simple link prediction. In ICML, pages 2071\u20132080, 2016.   \n[26] Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. Rotate: Knowledge graph embedding by relational rotation in complex space. In ICLR, 2019. URL https://openreview. net/forum?id $\\fallingdotseq$ HkgEQnRqYQ.   \n[27] Zequn Sun, Jiacheng Huang, Wei Hu, Muhao Chen, Lingbing Guo, and Yuzhong Qu. Transedge: Translating relation-contextualized embeddings for knowledge graphs. In ISWC, volume 11778 of Lecture Notes in Computer Science, pages 612\u2013629. Springer, 2019.   \n[28] Shikhar Vashishth, Soumya Sanyal, Vikram Nitin, and Partha P. Talukdar. Composition-based multi-relational graph convolutional networks. In ICLR, 2020. URL https://openreview. net/forum?id $\\cdot$ BylA_C4tPr.   \n[29] Quan Wang, Pingping Huang, Haifeng Wang, Songtai Dai, Wenbin Jiang, Jing Liu, Yajuan Lyu, Yong Zhu, and Hua Wu. Coke: Contextualized knowledge graph embedding. arXiv preprint arXiv:1911.02168, 2019.   \n[30] Lingbing Guo, Qiang Zhang, and Huajun Chen. Unleashing the power of transformer for graphs. CoRR, abs/2202.10581, 2022.   \n[31] Yichi Zhang, Zhuo Chen, Lingbing Guo, Yajing Xu, Binbin Hu, Ziqi Liu, Wen Zhang, and Huajun Chen. Native: Multi-modal knowledge graph completion in the wild. In SIGIR, pages 91\u2013101. ACM, 2024.   \n[32] Zhuo Chen, Jiaoyan Chen, Wen Zhang, Lingbing Guo, Yin Fang, Yufeng Huang, Yuxia Geng, Jeff Z Pan, Wenting Song, and Huajun Chen. Meaformer: Multi-modal entity alignment transformer for meta modality hybrid. arXiv preprint arXiv:2212.14454, 2022.   \n[33] Fan Yang, Zhilin Yang, and William W Cohen. Differentiable learning of logical rules for knowledge base reasoning. In NIPS, pages 2319\u20132328, 2017.   \n[34] Lingbing Guo, Zequn Sun, and Wei Hu. Learning to exploit long-term relational dependencies in knowledge graphs. In ICML, pages 2505\u20132514, 2019.   \n[35] Mingyang Chen, Wen Zhang, Wei Zhang, Qiang Chen, and Huajun Chen. Meta relational learning for few-shot link prediction in knowledge graphs. In EMNLP, pages 4216\u20134225, 2019.   \n[36] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT, pages 4171\u20134186, 2019.   \n[37] Sanxing Chen, Xiaodong Liu, Jianfeng Gao, Jian Jiao, Ruofei Zhang, and Yangfeng Ji. Hitter: Hierarchical transformers for knowledge graph embeddings. In EMNLP, pages 10395\u201310407, 2021.   \n[38] Bo Wang, Tao Shen, Guodong Long, Tianyi Zhou, Ying Wang, and Yi Chang. Structureaugmented text representation learning for efficient knowledge graph completion. In The Web Conference, pages 1737\u20131748, 2021.   \n[39] Qika Lin, Rui Mao, Jun Liu, Fangzhi Xu, and Erik Cambria. Fusing topology contexts and logical rules in language models for knowledge graph completion. Information Fusion, 90: 253\u2013264, 2023.   \n[40] Jason Youn and Ilias Tagkopoulos. Kglm: Integrating knowledge graph structure in language models for link prediction. arXiv preprint arXiv:2211.02744, 2022.   \n[41] Yuqi Zhu, Xiaohan Wang, Jing Chen, Shuofei Qiao, Yixin Ou, Yunzhi Yao, Shumin Deng, Huajun Chen, and Ningyu Zhang. Llms for knowledge graph construction and reasoning: Recent capabilities and future opportunities. arXiv preprint arXiv:2305.13168, 2023.   \n[42] Liang Yao, Jiazhen Peng, Chengsheng Mao, and Yuan Luo. Exploring large language models for knowledge graph completion. arXiv preprint arXiv:2308.13916, 2023.   \n[43] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey on in-context learning. arXiv preprint arXiv:2301.00234, 2022.   \n[44] Noam Wies, Yoav Levine, and Amnon Shashua. The learnability of in-context learning. NeurIPS, 36, 2024.   \n[45] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   \n[46] Zoltan Csaki, Bo Li, Jonathan Li, Qiantong Xu, Pian Pawakapan, Leon Zhang, Yun Du, Hengyu Zhao, Changran Hu, and Urmish Thakker. Sambalingo: Teaching large language models new languages. arXiv preprint arXiv:2404.05829, 2024.   \n[47] Xuan-Phi Nguyen, Wenxuan Zhang, Xin Li, Mahani Aljunied, Qingyu Tan, Liying Cheng, Guanzheng Chen, Yue Deng, Sen Yang, Chaoqun Liu, et al. Seallms\u2013large language models for southeast asia. arXiv preprint arXiv:2312.00738, 2023.   \n[48] George A. Miller. WordNet: An electronic lexical database. Communications of the ACM, 38, 1995.   \n[49] Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Li\u00f2, and Petar Velic\u02c7kovic\u00b4. Principal neighbourhood aggregation for graph nets. NeurIPS, 33:13260\u201313271, 2020.   \n[50] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, and Yoshua Bengio. Graph attention networks. In ICLR, 2018. URL https://openreview.net/forum? id=rJXMpikCZ.   \n[51] Senbao Shi, Zhenran Xu, Baotian Hu, and Min Zhang. Generative multimodal entity linking. arXiv preprint arXiv:2306.12725, 2023.   \n[52] Nicola De Cao, Gautier Izacard, Sebastian Riedel, and Fabio Petroni. Autoregressive entity retrieval. In ICLR. OpenReview.net, 2021. URL https://openreview.net/forum?id= 5k8F6UU39V.   \n[53] Nicola De Cao, Ledell Wu, Kashyap Popat, Mikel Artetxe, Naman Goyal, Mikhail Plekhanov, Luke Zettlemoyer, Nicola Cancedda, Sebastian Riedel, and Fabio Petroni. Multilingual autoregressive entity linking. Transactions of the Association for Computational Linguistics, 10: 274\u2013290, 2022.   \n[54] Michael Gutmann and Aapo Hyv\u00e4rinen. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In AISTAS, pages 297\u2013304, 2010.   \n[55] Semih Yavuz, Kazuma Hashimoto, Yingbo Zhou, Nitish Shirish Keskar, and Caiming Xiong. Modeling multi-hop question answering as single sequence prediction. In ACL, pages 974\u2013990, 2022.   \n[56] Ivana Balazevic, Carl Allen, and Timothy M. Hospedales. Tucker: Tensor factorization for knowledge graph completion. In EMNLP-IJCNLP, pages 5184\u20135193, 2019.   \n[57] Farahnaz Akrami, Lingbing Guo, Wei Hu, and Chengkai Li. Re-evaluating embedding-based knowledge graph completion methods. In CIKM, pages 1779\u20131782. ACM, 2018.   \n[58] Komal Teru, Etienne Denis, and Will Hamilton. Inductive relation prediction by subgraph reasoning. In ICML, pages 9448\u20139457, 2020.   \n[59] Yongqi Zhang and Quanming Yao. Knowledge graph reasoning with relational digraph. In Proceedings of the ACM web conference 2022, pages 912\u2013924, 2022.   \n[60] Christian Meilicke, Manuel Fink, Yanjie Wang, Daniel Ruffinelli, Rainer Gemulla, and Heiner Stuckenschmidt. Fine-grained evaluation of rule-and embedding-based systems for knowledge graph completion. In ISWC, pages 3\u201320, 2018.   \n[61] Ali Sadeghian, Mohammadreza Armandpour, Patrick Ding, and Daisy Zhe Wang. Drum: End-to-end differentiable rule mining on knowledge graphs. Advances in Neural Information Processing Systems, 32, 2019.   \n[62] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In ICLR, 2017. URL https://openreview.net/forum?id $\\mathsf{=S}$ JU4ayYgl.   \n[63] Michael Sejr Schlichtkrull, Thomas N. Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max Welling. Modeling relational data with graph convolutional networks. In ESWC, pages 593\u2013607, 2018. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Limitations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We would like to discuss the potential limitations of our method from the following three aspects: ", "page_idx": 14}, {"type": "text", "text": "Efficiency. As MKGL is an LLM-based fine-tuning method, it inevitably demands more computational resources. In the main experiments, MKGL significantly outperforms all the conventional and LLM-based methods. The later analysis also reveal that the trainable parameters and runtime of MKGL are less than general fine-tuning framework. Therefore, we believe that MKGL is still an efficient LLM-based method. ", "page_idx": 14}, {"type": "text", "text": "Robustness. MKGL leverage multiple retrievers to retrieve text and KG information for constructing both input embeddings and score estimations, which may accumulate more errors during fine-tuning. Even though, most modules are learnable with back-propagation. To avoid biased evaluation and occasional results, we also report the averaged results of multiple runs with variance statistics. Thus, we believe MKGL is a robust method. ", "page_idx": 14}, {"type": "text", "text": "Generality. The advances in LLMs have revolutionized many NLP tasks, and it is important for an LLM-based method where the LLM makes use of the proposed modules and whether the performance can continually improves as the LLM get promotion. We have conducted experiments to visualize the KGL embeddings and compare the performance with different base LLMs. The results empirically demonstrate the generality and potential of MKGL. ", "page_idx": 14}, {"type": "text", "text": "B Broader Impacts ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Our work focuses on the integration of Large Language Models (LLMs) with Knowledge Graphs (KGs) through the introduction of a specialized KG Language (KGL), has substantial broader impacts spanning technological advancements, societal implications, and educational benefits. Here, we outline the diverse and far-reaching impacts of our research. ", "page_idx": 14}, {"type": "text", "text": "Technological Advancements Our research contributes to the cutting-edge of artificial intelligence, pushing the boundaries of what LLMs can achieve when combined with the structured knowledge represented by KGs. This can potentially unlock new capabilities in AI, ranging from more accurate context-aware natural language understanding to enhanced machine reasoning across diverse domains such as healthcare, finance, and legal systems. Additionally, our approach of using a specialized language (KGL) and the method of contextual embedding augmentation can inspire novel AI architectures and learning paradigms that bridge the gap between unstructured and structured forms of knowledge. ", "page_idx": 14}, {"type": "text", "text": "Societal Implications The enhancement of AI systems with a more profound understanding of structured knowledge has broad societal implications. For one, it could lead to the development of AI assistants that provide more accurate, consistent, and reliable information, thus improving decisionmaking in critical sectors. In healthcare, for instance, AI systems equipped with our technology could offer more precise recommendations by thoroughly understanding medical knowledge graphs. Moreover, by reducing the propensity for errors and hallucinations, our approach could foster greater trust in AI technologies among the general public, paving the way for wider acceptance and integration into daily life. ", "page_idx": 14}, {"type": "text", "text": "Ethical Considerations As with any advancement in AI, our work prompts important ethical considerations. Ensuring our technology is used responsibly involves critical discussions around privacy, bias, and transparency, especially as AI systems become more adept at interpreting and generating human-like text. We advocate for the continued examination of these aspects in tandem with technological development to ensure AI benefits society equitably and ethically. Our work, by facilitating error reduction in AI outputs, also contributes to the broader effort of minimizing harm and bias in AI-generated content. ", "page_idx": 14}, {"type": "text", "text": "Educational Benefits Our integration of LLMs with KGs presents a novel avenue for educational tools and applications. AI tutors or educational platforms powered by our enhanced LLMs can offer students personalized and accurate learning experiences. Such systems could better understand and integrate complex academic content structured within knowledge graphs, from history timelines to scientific concepts, thereby improving the quality of automated tutoring services and opening up new methods for engaging with educational material. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Future Directions This research opens up exciting avenues for future exploration, including refining the KGL for broader applicative scopes, exploring the ethical considerations of nuanced AI-generated content, and expanding our understanding of AI\u2019s potential when it deeply integrates diverse forms of knowledge. The cross-disciplinary nature of this work invites collaboration among computer scientists, ethicists, domain experts, and policymakers to harness the full potential of AI for societal benefit. ", "page_idx": 15}, {"type": "text", "text": "In conclusion, the integration of LLMs with KGs not only represents a significant step forward in AI capabilities but also poses thoughtful considerations for societal impact, ethical use, and educational applications. Our work underscores the importance of continuous exploration, responsible innovation, and cross-disciplinary collaboration to harness the transformative potential of AI technologies. ", "page_idx": 15}, {"type": "text", "text": "C Principal Neighborhood Aggregation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Graph Neural Networks (GNNs) have emerged as a powerful family of neural models for learning on graph-structured data [17, 18, 62]. Among the recent advances is the principal neighborhood aggregation (PNA) mechanism [49], which enhances the representational capacity of GNNs by diversifying the aggregation functions applied to neighboring nodes. ", "page_idx": 15}, {"type": "text", "text": "PNA leverages a combination of multiple aggregators such as sum, mean, and standard deviation, together with a scalable degree-specific weighting scheme. This approach is designed to address the shortcomings associated with simple aggregation functions that may fail to capture the complexity and diversity of neighborhood structures in graphs. ", "page_idx": 15}, {"type": "text", "text": "The key component of PNA is its aggregation scheme, which is formally defined as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\na_{v}^{(l+1)}=\\delta\\left(\\bigoplus_{\\rho\\in\\mathcal{R}}A G G_{\\rho}(\\{h_{u}^{(l)},\\forall u\\in\\mathcal{N}(v)\\}),W^{(l)}\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Please note that the symbols used in describing PNA are independent to the main paper for clarity. Here, $a_{v}^{(l+1)}$ is the aggregated information for node $v$ at layer $l+1$ , $\\mathcal{N}(v)$ denotes the set of neighbors of $v$ , $h_{u}^{(l)}$ represents the hidden features of neighbor nodes at layer $l$ , $\\oplus$ is a concatenation operator over all aggregators in the set $\\mathcal{R}$ , $A G G_{\\rho}$ is an aggregation function (e.g., sum, mean, max), $\\delta$ is a nonlinear activation function such as ReLU, and $W^{(l)}$ is a learnable weight matrix at layer $l$ . ", "page_idx": 15}, {"type": "equation", "text": "$$\na_{v}^{(l+1)}=\\delta\\left(\\bigoplus_{\\rho\\in\\mathcal{R}}A G G_{\\rho}\\big(\\{h_{u}^{(l)}\\otimes h_{r}^{(l)},\\forall(v,r,u)\\in\\mathcal{T}\\}\\big),W^{(l)}\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "PNA\u2019s distinctive blend of multiple aggregation functions and degree-aware weighting significantly enhances the expressive power of GNNs, allowing for more complex feature representations and, consequently, improved performance on downstream tasks. We also follow the KG embedding methods [15, 28, 63] to incorporate the relation embeddings into PNA as relational PNA. ", "page_idx": 15}, {"type": "text", "text": "D Implementation Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We introduce Algorithm 1 to demonstrate the fine-tuning process of MKGL for KG completion. We first construct input instructions following Instruction 3.1 and tokenize them into IDs. For those in the score of original LLM vocabulary, their embeddings can be looked up from $\\mathbf{T}$ , while those of out of scope will be retrieved by our context retriever $R_{\\mathbf{context}}$ . After assembling the input embeddings, we feed them to the LLM to obtain output hidden states and then obtain the scores from the score retriever $R_{\\mathrm{score}}$ . Finally, we optimize MKGL by minimizing the constrastive loss $\\mathcal{L}$ . The main hyper-parameter settings are summarized in Table 5. ", "page_idx": 15}, {"type": "text", "text": "Algorithm 1 MKGL for KG Completion   \n1: Input: the training KG $\\mathcal{G}$ , the language model $\\mathcal{M}$ , the token embedding matrix T, the original   \nvocabulary of the LLM $\\nu_{\\mathrm{llm}}$ , the KGL context retriever $R_{\\mathrm{context}}$ , the KGL score retriever $R_{\\mathrm{score}}$ ;   \n2: for each batched data in the training set do   \n3: Construct input instructions according to Instruction 3.1;   \n4: Tokenize the input instructions with the tokenizer;   \n5: for each input token sequence $\\{t_{0},t_{1},t_{2},\\ldots\\}$ do   \n6: for each token $t_{k}$ do   \n7: if $t_{k}\\in\\mathcal{V}_{\\mathrm{llm}}$ then   \n8: $\\mathbf{t}_{k}\\gets\\mathbf{T}_{k}$ ; // look up embedding from the token matrix   \n9: else   \n10: $\\mathbf{t}_{k}\\gets R_{\\mathrm{context}}(t_{k})$ (Equations (3-6));   \n11: end if   \n12: end for   \n13: end for   \n14: Compute the batched output hidden states of the LLM $\\mathcal{M}$ (Equation 1);   \n15: Compute the batched scores with $R_{\\mathrm{score}}$ (Equations (7-10));   \n16: Compute and minimize the constrastive loss $\\mathcal{L}$ (Equation (11));   \n17: end for ", "page_idx": 16}, {"type": "table", "img_path": "eqMNwXvOqn/tmp/50d48836793cfb18d8f469def8bb58e61c2545e8d4b5a4986efae74bfb34d3b2.jpg", "table_caption": ["Table 5: Hyper-parameter settings in the main experiments. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "E Dataset Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We use the following benchmark datasets to evaluate the performance of MKGL, and summarize the statistics in Table 6: ", "page_idx": 16}, {"type": "text", "text": "\u2022 FB15k-237: This dataset is a subset of the original FB15k dataset [23] and is created by removing inverse relations that may lead to test set leakage.   \n\u2022 WN18RR: This dataset is a subset of the original WN18 dataset [23] and is created by removing inverse relations that may lead to test set leakage.   \n\u2022 FB15k-237-ind: The \u2018ind\u2019 suffix denotes the inductive setting adopted in FB15k-237 [58]. It includes new entities in the validation and test sets that are not present during training, thus requiring models to generalize beyond the transductive assumptions of previously seen entities.   \n\u2022 WN18RR-ind: Similarly to FB15k-237-ind, the WN18RR-ind dataset is adapted for inductive KG completion on the WordNet [48]. ", "page_idx": 16}, {"type": "text", "text": "These datasets have been instrumental in the development and benchmarking of advanced KG completion models, enabling comparison of different approaches and understanding of their effectiveness in both conventional and inductive settings. ", "page_idx": 16}, {"type": "table", "img_path": "eqMNwXvOqn/tmp/f0cc92ca64a5a4b249f2dd8da3d782b7cea9640a41e980427ae085a56e1df572.jpg", "table_caption": ["Table 6: Dataset statistics. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "eqMNwXvOqn/tmp/8668fe0cb6e0baba46d9153bc556bb49e6aae8b91ecb9931b0e1b5f7ca80462e.jpg", "table_caption": ["Table 7: The detailed inductive KG completion results, where v1-v4 represent four different subsets. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "F Additional Experiment Results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "F.1 More Examples on Knowledge Graph Language Modeling ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We present additional examples of KGL modeling in Table 5, which demonstrates that MKGL can not only generate KGL sentences seen during training but also produce previously unseen triplets within the testing set. ", "page_idx": 17}, {"type": "text", "text": "F.2 Details Results on Inductive Knowledge Graph Completion ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We present detailed results on all inductive KG completion benchmarks in Table 7, where MKGL consistently and significantly outperforms all state-of-the-art baselines. ", "page_idx": 17}, {"type": "image", "img_path": "eqMNwXvOqn/tmp/953443ed9ead6962d9112bed932c9a0bed9879bbb1265ab643620ce1739c37a6.jpg", "img_caption": ["Figure 5: More examples on KGL modeling. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "F.3 Different Layer Numbers ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We conduct experiments to analyze the influence of layer numbers in the KGL retrievers. The results are illustrated in Figure 6. Clearly, increasing the number of layers enhances performance across all datasets and metrics. Additionally, we observe that a small number of layers (i.e., 2) significantly impairs performance. ", "page_idx": 18}, {"type": "image", "img_path": "eqMNwXvOqn/tmp/1b301716eb7fe8e6735264afd31dbdd8f2d1b0457a3fc0b118c84076d1cccc5a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 6: The performance of MKGL on FB15k-237 and WN18RR, with respect to the layer number of the retrievers. ", "page_idx": 19}, {"type": "image", "img_path": "eqMNwXvOqn/tmp/8f55d9f79a485258085ee0ce65a25af4cfe36af6b4a65e1a99511c76b0068690.jpg", "img_caption": ["Figure 7: The performance of MKGL on FB15k-237 and WN18RR, with respect to different encoders in the retrievers. "], "img_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "eqMNwXvOqn/tmp/5b43cf63f6318aa1cc1b4979f25ea82d509e899ddb79939d9e44b5a6fffafd0c.jpg", "table_caption": ["Table 8: Detailed results of Table 2. The KG completion results on FB15k-237 and WN18RR. The best and second-best results are boldfaced and underlined, respectively. $\\uparrow$ : higher is better; $\\downarrow:$ : lower is better. -: unavailable entry. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "F.4 Different Encoders ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We conduct experiments to explore the impact of different encoders in the retrievers. The results are depicted in Figure 7. We find that the MKGL is not highly sensitive to the choice of encoders. The performance when using GAT [50] is slightly lower than when using PNA. ", "page_idx": 19}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: The paper discusses the limitations of the work. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: This paper does not include theoretical result. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: The paper fully discloses all the information needed to reproduce the main experimental results. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The source code is uploaded. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The training and test details are included in appendices. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: The main results are based on the average of multiple runs. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper provides sufficient information on the computer resources. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 23}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The research fully conforms with the NeurIPS Code of Ethics. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper discusses both potential positive societal impacts and negative societal impacts in appendices. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: There is no such risk for this paper. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 24}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: All assets are fully licensed. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 24}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: All new assets are well documented. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 25}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: No crowdsourcing experiment has been involved. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: This research has no such risk. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 25}]