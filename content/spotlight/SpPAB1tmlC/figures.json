[{"figure_path": "SpPAB1tmlC/figures/figures_1_1.jpg", "caption": "Figure 1: Overview of large (A) encoder-based and (B) encoder-free vision-language models. Encoder-based VLMs contain vision encoders (VE) and large language models (LLM), with a projector as a vision-language bridge, while encoder-free VLMs exclude vision encoders and handle vision perception and linguistic instruction simultaneously with one unified architecture.", "description": "This figure compares and contrasts the architectures of encoder-based and encoder-free vision-language models (VLMs).  Encoder-based VLMs use a vision encoder to process images, converting them into a format that can be integrated with a large language model (LLM). The two components are connected through an intermediate layer.  In contrast, encoder-free VLMs directly process both image and text data within a single unified architecture, eliminating the need for a separate vision encoder. The figure highlights the advantages and disadvantages of each approach, addressing issues such as image resolution flexibility, deployment overhead, and the balance of capacity between the vision and language components.", "section": "1 Introduction"}, {"figure_path": "SpPAB1tmlC/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of our proposed EVE. We start by encoding images using a concise (a) patch embedding layer, and then concatenate the patch and word features into a decoder-only network. Next, we facilitate image perception through visual representation supervision using a (b) patch aligning layer, and achieve linguistic conceptual alignment using a next-word prediction pretext task.", "description": "The figure illustrates the architecture of EVE, an encoder-free vision-language model.  It shows how image patches are embedded using a patch embedding layer and then combined with text embeddings in a causal decoder-only architecture. A patch aligning layer uses visual representation supervision to enhance image perception and linguistic conceptual alignment. The model learns to predict the next word, aligning visual and linguistic information.", "section": "3.1 Model Architecture"}, {"figure_path": "SpPAB1tmlC/figures/figures_3_2.jpg", "caption": "Figure 3: Overview of (a) patch embedding and (b) patch aligning layer. The former layer encodes images into patch features and employs cross-attention (CA1) within a limited receptive field to enhance representations. Meanwhile, a special <CLS> token provides a holistic view of each patch feature in the subsequent backbone. The latter layer removes all meaningless tokens and adjusts the size of patch features to align with the semantics of the same positional features in the vision encoder.", "description": "This figure details the architecture of two key components in the EVE model: the Patch Embedding Layer and the Patch Aligning Layer.  The Patch Embedding Layer efficiently encodes image information into patch features, leveraging cross-attention to refine representations and a special <CLS> token for holistic understanding. The Patch Aligning Layer then aligns these patch features with those from a vision encoder to further enhance visual perception.  It achieves this by removing unnecessary tokens and adjusting patch feature sizes to match the semantic representations of the vision encoder. The process involves layer-wise cross-attention and minimizes the Mean Squared Error (MSE) between EVE and the vision encoder.", "section": "3.1 Model Architecture"}, {"figure_path": "SpPAB1tmlC/figures/figures_4_1.jpg", "caption": "Figure 4: Overview of training procedure with three successive stages. We perform initial vision-language alignment guided by a frozen LLM in Stage 1, and then update the entire backbone for Stage 2 and Stage 3. We empirically find that Stage 1 is quite crucial to avoid collapse and accelerate convergence, thereby enhancing training efficiency. Notably, PAL is removed during inference.", "description": "This figure illustrates the three-stage training process for the EVE model.  Stage 1 involves LLM-guided pre-alignment, where a frozen LLM guides the initial vision-language alignment.  Stage 2 is generative pre-training, where the entire backbone is updated. Stage 3 is supervised fine-tuning.  The figure highlights the crucial role of Stage 1 in preventing model collapse and accelerating convergence. It also notes that the Patch Aligning Layer (PAL) is removed during inference.", "section": "3.2 Training Procedure"}, {"figure_path": "SpPAB1tmlC/figures/figures_7_1.jpg", "caption": "Figure 5: Training loss in Stage 2 using various strategies. Optimization remains unstable and prone to collapse, despite searching learning rate from 2e-5 to 1e-3.", "description": "This figure shows the training loss curves for different training strategies in Stage 2 of the EVE model training.  The blue curve represents training from scratch without the LLM-guided pre-aligning stage, showing significant instability and model collapse. The red and green curves represent training without and with the LLM-guided pre-aligning stage (Stage 1), respectively. The green curve demonstrates much more stable and faster convergence, emphasizing the crucial role of Stage 1 in efficient and robust training.", "section": "3.2 Training Procedure"}, {"figure_path": "SpPAB1tmlC/figures/figures_8_1.jpg", "caption": "Figure 7: Scaling performance on GQA and SEED using LLaVA-1.5 as an encoder-based baseline. We first train its projector in Stage 1 with EVE-cap16/33M. Here, (VE)-(LLM) indicates training Vision Encoder in Stage 2 and LLM in Stage 3, where we train the projector across all stages.", "description": "This figure shows the scaling performance of EVE-7B and EVE-7B (HD) on GQA and SEED benchmarks, comparing them to LLaVA-1.5 (an encoder-based baseline).  It illustrates how performance changes with increasing amounts of training data in Stage 2.  The different line styles represent different training strategies: (VE)-(ALL) trains the vision encoder in Stage 2 and all model parameters in Stage 3; (VE)-(LLM) trains the vision encoder in Stage 2 and the language model in Stage 3; (ALL)-(ALL) trains all model parameters in both stages; (ALL)-(LLM) trains all model parameters in Stage 2 and the language model in Stage 3. The results demonstrate that EVE consistently improves with more training data, approaching the performance of the encoder-based LLaVA-1.5 baseline.", "section": "4.3 Ablation Studies"}, {"figure_path": "SpPAB1tmlC/figures/figures_8_2.jpg", "caption": "Figure 8: Scaling performance on GQA and SEED w/ or w/o vision encoder supervision (PAL).", "description": "This figure demonstrates the impact of vision encoder supervision (PAL) on the performance of EVE-7B and EVE-7B (HD) models across different training data sizes on GQA and SEED benchmarks.  The plot shows that the models consistently improve with more pre-training data, regardless of whether or not PAL is used. The effect of PAL is more pronounced with smaller datasets and diminishes as the amount of training data increases.", "section": "4.3 Ablation Studies"}, {"figure_path": "SpPAB1tmlC/figures/figures_16_1.jpg", "caption": "Figure 1: Overview of large (A) encoder-based and (B) encoder-free vision-language models. Encoder-based VLMs contain vision encoders (VE) and large language models (LLM), with a projector as a vision-language bridge, while encoder-free VLMs exclude vision encoders and handle vision perception and linguistic instruction simultaneously with one unified architecture.", "description": "This figure compares and contrasts the architectures of encoder-based and encoder-free vision-language models (VLMs).  Encoder-based VLMs utilize a vision encoder to process images, converting them into a visual representation that is then passed to a large language model (LLM) for further processing.  A projector module often bridges the two components. In contrast, encoder-free VLMs process images and text directly within a single unified architecture, eliminating the need for a separate vision encoder and thereby simplifying the model and improving flexibility. The figure highlights the key differences in architecture and workflow and points to several limitations (image resolution, deployment overhead, capacity balance) of the encoder-based model that motivate the exploration of the encoder-free approach.", "section": "1 Introduction"}, {"figure_path": "SpPAB1tmlC/figures/figures_16_2.jpg", "caption": "Figure 1: Overview of large (A) encoder-based and (B) encoder-free vision-language models. Encoder-based VLMs contain vision encoders (VE) and large language models (LLM), with a projector as a vision-language bridge, while encoder-free VLMs exclude vision encoders and handle vision perception and linguistic instruction simultaneously with one unified architecture.", "description": "This figure compares and contrasts the architectures of encoder-based and encoder-free Vision-Language Models (VLMs).  Encoder-based VLMs consist of a vision encoder to process images, a large language model for text understanding, and a projector that bridges the two modalities.  In contrast, encoder-free VLMs use a single, unified architecture that processes both visual and textual inputs simultaneously without a dedicated vision encoder.  The figure highlights several key distinctions and challenges of each architecture, including differences in image resolution/aspect ratio handling, deployment overhead, and capacity balance between the visual and language components.", "section": "1 Introduction"}]