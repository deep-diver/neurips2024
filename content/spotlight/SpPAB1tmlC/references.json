{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "This paper introduces CLIP, a foundational model for vision-language tasks that EVE builds upon and improves."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "publication_date": "2023-02-23", "reason": "This paper introduces the LLaMA language model, which serves as the base for EVE, providing the linguistic capabilities."}, {"fullname_first_author": "Yuxin Fang", "paper_title": "EVA: exploring the limits of masked visual representation learning at scale", "publication_date": "2023-06-01", "reason": "This paper introduces EVA, a large vision model, whose visual encoder capabilities EVE aims to match without explicit encoding."}, {"fullname_first_author": "Wei-Lin Chiang", "paper_title": "Vicuna: An open-source chatbot impressing GPT-4 with 90%* chatgpt quality", "publication_date": "2023-03-30", "reason": "This paper introduces Vicuna, a large language model that EVE is based on, providing a strong foundation for instruction following."}, {"fullname_first_author": "Junnan Li", "paper_title": "BLIP: bootstrapping language-image pre-training for unified vision-language understanding and generation", "publication_date": "2022-04-01", "reason": "This paper introduces BLIP, a significant vision-language model that EVE is compared against, representing a state-of-the-art benchmark."}]}