[{"figure_path": "SpPAB1tmlC/tables/tables_5_1.jpg", "caption": "Table 1: Details of pre-training datasets. We only extract 16/33M data for Stage 1, and utilize the full 33M data for Stage 2", "description": "This table details the datasets used for pre-training the EVE model.  It shows that the model was pre-trained in two stages. Stage 1 used a subset of the data (16 million or 33 million samples), while Stage 2 used the full 33 million samples.  The datasets include SA-1B, OpenImages, and LAION, with each dataset's image captions generated by either LLaVA-1.5 or Emu2.", "section": "4.1 Training Settings"}, {"figure_path": "SpPAB1tmlC/tables/tables_5_2.jpg", "caption": "Table 2: Details of fine-tuning datasets. We only adopt LLaVA-mix-665K for standard EVE-7B, and further train EVE (HD) with high resolution and all datasets", "description": "This table presents the details of the datasets used for fine-tuning the EVE model.  It shows that the standard EVE-7B model was fine-tuned using the LLaVA-mix-665K dataset. A high-resolution version of EVE (EVE-HD) was also trained, using the LLaVA-mix-665K dataset along with additional datasets, namely AI2D [33], Synthdog [34], DVQA [32], ChartQA [57], DocVQA [11], Vision-Flan [82], and Bunny-695K [27], resulting in a larger number of samples for the high-resolution training.", "section": "4 Experiments"}, {"figure_path": "SpPAB1tmlC/tables/tables_6_1.jpg", "caption": "Table 3: Comparison with state-of-the-art VLMs on vision-language benchmarks. #Samples: the number of samples in the pretraining/supervised fine-tuning stage. AR: image aspect ratio. HD: high image resolution. We evaluate VLMs on VQAV2: VQA-v2 [25]; GQA [29]; VizWiz [26]; SQA\u00b9: ScienceQA-IMG [54]; VQAT: TextVQA [67]; POPE [47]; MME [23]; MMB: MMBench [52]; SEED: SEED-Bench [41]; MM-Vet [89]. Includes in-house data that is not publicly accessible", "description": "This table compares the performance of EVE-7B and EVE-7B (HD) against various state-of-the-art encoder-based and encoder-free vision-language models across multiple benchmark datasets.  The metrics used for comparison include scores on different vision-language tasks (VQA-v2, GQA, VizWiz, ScienceQA-IMG, TextVQA, POPE, MME, MMBench, SEED, and MM-Vet). The table also indicates the size of LLMs used, the amount of training data, and whether high-resolution images were used.", "section": "4.2 Main Results"}, {"figure_path": "SpPAB1tmlC/tables/tables_7_1.jpg", "caption": "Table 4: Configurations of PEL and PAL. EVE0.5M, EVE4M, EVE8M utilize LLaVA-cap558K, EVE-cap4/33M, EVE-cap8/33M in Stage 1-2 with LLaVA-mix665K in Stage 3. MSENP denotes the next-patch alignment", "description": "This table presents an ablation study on the configurations of the Patch Embedding Layer (PEL) and Patch Aligning Layer (PAL) within the EVE model.  It shows the performance (VQAv2, GQA, MMB, SEED scores) of different EVE model variants, each differing in the inclusion/exclusion or specific configuration of the PEL and PAL components.  Different sizes of training datasets were also used for comparison, helping to analyze the effectiveness of PEL and PAL at various scales.", "section": "4.3 Ablation Studies"}, {"figure_path": "SpPAB1tmlC/tables/tables_7_2.jpg", "caption": "Table 3: Comparison with state-of-the-art VLMs on vision-language benchmarks. #Samples: the number of samples in the pretraining/supervised fine-tuning stage. AR: image aspect ratio. HD: high image resolution. We evaluate VLMs on VQAV2: VQA-v2 [25]; GQA [29]; VizWiz [26]; SQA\u00b9: ScienceQA-IMG [54]; VQAT: TextVQA [67]; POPE [47]; MME [23]; MMB: MMBench [52]; SEED: SEED-Bench [41]; MM-Vet [89]. Includes in-house data that is not publicly accessible", "description": "This table compares the performance of EVE and other state-of-the-art Vision-Language Models (VLMs) across various vision-language benchmarks.  It highlights EVE's performance relative to encoder-based models, considering factors like the number of training samples, image aspect ratios, and high-resolution image capabilities.  The benchmarks used cover a range of tasks and dataset scales.", "section": "4.2 Main Results"}, {"figure_path": "SpPAB1tmlC/tables/tables_8_1.jpg", "caption": "Table 6: Model FLOPs and inference latency", "description": "This table compares the computational cost (FLOPs) and inference speed (Time) for both the vision and LLM components of four different models: LLaVA-1.5, EVE-7B, LLaVA-1.6 (HD), and EVE-7B (HD).  It highlights the significant reduction in FLOPs and inference time achieved by EVE models, especially EVE-7B, compared to their LLaVA counterparts, demonstrating the efficiency gains from the encoder-free architecture. The (HD) versions represent high-resolution models.", "section": "4.2 Main Results"}, {"figure_path": "SpPAB1tmlC/tables/tables_15_1.jpg", "caption": "Table 7: Supervised Fine-tuning Data Mixture", "description": "This table lists the datasets used in the supervised fine-tuning stage of the EVE model training.  It shows the proportion of each dataset in the mixture (Sample Ratio), the number of samples from that dataset (Size), and the type of task the data is relevant to (Task).  The datasets are diverse, covering conversation, text-only, general question answering, knowledge-based question answering, OCR-based question answering, grounding, multi-task learning, chart question answering, science question answering, and document-related tasks.", "section": "4 Experiments"}, {"figure_path": "SpPAB1tmlC/tables/tables_15_2.jpg", "caption": "Table 8: Hyperparameter settings", "description": "This table shows the hyperparameter settings used during the three training stages of the EVE model.  It details the batch size, learning rate, learning rate schedule (cosine decay), learning rate warmup ratio, weight decay, number of epochs, optimizer (AdamW), and DeepSpeed stage used in each stage.", "section": "A.3 Training Details"}, {"figure_path": "SpPAB1tmlC/tables/tables_16_1.jpg", "caption": "Table 3: Comparison with state-of-the-art VLMs on vision-language benchmarks. #Samples: the number of samples in the pretraining/supervised fine-tuning stage. AR: image aspect ratio. HD: high image resolution. We evaluate VLMs on VQAV2: VQA-v2 [25]; GQA [29]; VizWiz [26]; SQA\u00b9: ScienceQA-IMG [54]; VQAT: TextVQA [67]; POPE [47]; MME [23]; MMB: MMBench [52]; SEED: SEED-Bench [41]; MM-Vet [89]. Includes in-house data that is not publicly accessible", "description": "This table compares the performance of EVE and other state-of-the-art Vision-Language Models (VLMs) across various vision-language benchmarks.  It shows the different models used, the number of training samples, image aspect ratio, high-resolution image usage, and the performance scores on various benchmarks.  The benchmarks include academic-task-oriented, open-world multi-modal understanding and scientific problem benchmarks. The table highlights EVE's competitive performance, especially considering its smaller size and reliance on publicly available data.", "section": "4.2 Main Results"}]