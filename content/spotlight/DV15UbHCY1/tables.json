[{"figure_path": "DV15UbHCY1/tables/tables_2_1.jpg", "caption": "Table 1: Statistics for all datasets used in reference methods [50, 22, 15].", "description": "This table presents the characteristics of eight datasets used in the paper to evaluate the performance of language models in time series forecasting.  The datasets cover various domains such as energy, weather, traffic, and finance, and exhibit different characteristics in terms of the number of channels, sampling rates, and total timesteps.", "section": "3.3 Datasets and Evaluation Metrics"}, {"figure_path": "DV15UbHCY1/tables/tables_2_2.jpg", "caption": "Table 2: Three popular methods for time series forecasting with Large Language Models.", "description": "This table summarizes three popular methods for time series forecasting that utilize large language models (LLMs).  It shows the base model used (GPT-2 or LLaMA), how the LLM parameters are handled (learnable or frozen), whether positional and word embeddings are used, and if the method is multimodal.", "section": "3.1 Reference Methods for Language Models and Time Series"}, {"figure_path": "DV15UbHCY1/tables/tables_5_1.jpg", "caption": "Table 3: Forecasting performance of all models \u2013 Time-LLM, CALF, and OneFitsAll and results from our ablations. All results are averaged across different prediction lengths, though full results are available in Appendix E.1. Results in Red denote the best-performing model. # Wins refers to the number of times the method performed best, and # Params is the number of model parameters. \u201c-\u201d means the dataset is not included in the original paper.", "description": "This table presents the forecasting performance results for three popular LLM-based time series forecasting methods (Time-LLM, CALF, OneFitsAll) and their corresponding ablation models (w/o LLM, LLM2Attn, LLM2Trsf). The results are averaged over different prediction lengths, with the best-performing model highlighted in red.  The table also provides the number of times each model achieved the best performance (# Wins) and the number of model parameters (# Params).  Datasets not included in the original papers are indicated with a '-'.", "section": "4 Results"}, {"figure_path": "DV15UbHCY1/tables/tables_6_1.jpg", "caption": "Table 4: In time series tasks, LLM (LLaMA and GPT-2) significantly increases training time. The table shows the number of model parameters (in millions) and total training time (in minutes) for three methods predicting over a length of 96 on ETTh1 and Weather data. Compared with original method \"w/ LLM\" are \"w/o LLM\", \"LLM2Attn\" and \"LLM2Trsf\".", "description": "This table compares the computational cost (in terms of model parameters and training time) of three different methods for time series forecasting: Time-LLM, OneFitsAll, and CALF.  For each method, it shows the resources required when using the full language model (\"w/ LLM\") and after applying three ablations: removing the LLM (\"w/o LLM\"), replacing the LLM with an attention layer (\"LLM2Attn\"), and replacing the LLM with a transformer block (\"LLM2Trsf\"). The table highlights the significant increase in computational cost associated with using the full LLMs for time series forecasting tasks.", "section": "3 Experimental Setup"}, {"figure_path": "DV15UbHCY1/tables/tables_7_1.jpg", "caption": "Table 5: Randomly initializing LLM parameters and training from scratch (woPre) achieved better results than using a pretrained (Pre) model. \u201cwoFT\u201d and \u201cFT\u201d refer to whether the LLM parameters are frozen or trainable.", "description": "This table presents the results of an ablation study comparing four different training approaches for LLMs in time series forecasting. The methods compared are: Pretraining + Finetuning (Pre+FT), Random Initialization + Finetuning (woPre+FT), Pretraining + No Finetuning (Pre+woFT), and Random Initialization + No Finetuning (woPre+woFT).  The table shows the MAE and MSE for each method across eight different datasets. The results demonstrate that randomly initializing the LLM parameters and training from scratch generally outperforms using a pretrained model, regardless of whether fine-tuning is used.", "section": "4.3 Does language model pretraining help performance on forecasting tasks? (RQ3)"}, {"figure_path": "DV15UbHCY1/tables/tables_7_2.jpg", "caption": "Table 3: Forecasting performance of all models \u2013 Time-LLM, CALF, and OneFitsAll and results from our ablations. All results are averaged across different prediction lengths, though full results are available in Appendix E.1. Results in Red denote the best-performing model. # Wins refers to the number of times the method performed best, and # Params is the number of model parameters. \u201c-\u201d means the dataset is not included in the original paper.", "description": "This table presents a comparison of the forecasting performance (MAE and MSE) of three popular LLM-based time series forecasting methods (Time-LLM, CALF, OneFitsAll) and their corresponding ablations (without LLM, LLM replaced with attention, LLM replaced with transformer).  The results are averaged across various prediction lengths and presented for thirteen datasets.  The table highlights the best-performing model for each dataset and metric and shows the number of times each model achieved the best performance (# Wins) and the number of parameters (#Params) for each model.", "section": "4 Results"}, {"figure_path": "DV15UbHCY1/tables/tables_8_1.jpg", "caption": "Table 3: Forecasting performance of all models \u2013 Time-LLM, CALF, and OneFitsAll and results from our ablations. All results are averaged across different prediction lengths, though full results are available in Appendix E.1. Results in Red denote the best-performing model. # Wins refers to the number of times the method performed best, and # Params is the number of model parameters. \u201c-\u201d means the dataset is not included in the original paper.", "description": "This table presents the forecasting performance (MAE and MSE) of three popular LLM-based time series forecasting methods (Time-LLM, CALF, OneFitsAll) and their corresponding ablation methods (w/o LLM, LLM2Attn, LLM2Trsf). The results are averaged across various prediction lengths for better evaluation. The table highlights the best-performing model for each dataset and metric, providing a clear comparison of the performance gain or loss due to the LLM component. Additionally, it shows the number of times each method achieved the best performance and the number of parameters used in each model.", "section": "4 Results"}, {"figure_path": "DV15UbHCY1/tables/tables_8_2.jpg", "caption": "Table 3: Forecasting performance of all models \u2013 Time-LLM, CALF, and OneFitsAll and results from our ablations. All results are averaged across different prediction lengths, though full results are available in Appendix E.1. Results in Red denote the best-performing model. # Wins refers to the number of times the method performed best, and # Params is the number of model parameters. \u201c-\u201d means the dataset is not included in the original paper.", "description": "This table presents a comparison of the forecasting performance (MAE and MSE) of three popular LLM-based time series forecasting methods (Time-LLM, CALF, OneFitsAll) and their corresponding ablation versions (without LLM, LLM replaced with attention, LLM replaced with transformer).  The results are averaged across various prediction lengths and shown for multiple benchmark datasets.  The table highlights the best-performing model for each dataset and metric, and counts the number of times each model achieves the best performance (Wins). It also provides the number of parameters for each model.", "section": "4 Results"}, {"figure_path": "DV15UbHCY1/tables/tables_15_1.jpg", "caption": "Table 3: Forecasting performance of all models \u2013 Time-LLM, CALF, and OneFitsAll and results from our ablations. All results are averaged across different prediction lengths, though full results are available in Appendix E.1. Results in Red denote the best-performing model. # Wins refers to the number of times the method performed best, and # Params is the number of model parameters. \u201c-\u201d means the dataset is not included in the original paper.", "description": "This table presents the forecasting performance results for three popular LLM-based time series forecasting methods (Time-LLM, CALF, OneFitsAll) and their corresponding ablation methods.  The performance is measured using MAE and MSE metrics, averaged across multiple prediction lengths.  The table also indicates the number of times each method achieved the best performance (\"# Wins\") and the number of model parameters (\"# Parameters\").  The \"-\" symbol signifies that a specific dataset was not used in the original paper's experiments.", "section": "4 Results"}, {"figure_path": "DV15UbHCY1/tables/tables_15_2.jpg", "caption": "Table 3: Forecasting performance of all models \u2013 Time-LLM, CALF, and OneFitsAll and results from our ablations. All results are averaged across different prediction lengths, though full results are available in Appendix E.1. Results in \\textbf{Red} denote the best-performing model. # Wins refers to the number of times the method performed best, and # Params is the number of model parameters. \u201c-\u201d means the dataset is not included in the original paper.", "description": "This table presents the forecasting performance of three popular LLM-based time series forecasting methods (Time-LLM, CALF, OneFitsAll) and their corresponding ablation studies (removing the LLM component, replacing it with a basic attention layer, or a basic transformer block). The performance is evaluated using MAE and MSE metrics across multiple datasets and prediction lengths.  The table highlights the best-performing model for each scenario and indicates the number of times each method achieved the best performance.  The number of model parameters is also shown for each model.", "section": "4 Results"}, {"figure_path": "DV15UbHCY1/tables/tables_16_1.jpg", "caption": "Table 3: Forecasting performance of all models \u2013 Time-LLM, CALF, and OneFitsAll and results from our ablations. All results are averaged across different prediction lengths, though full results are available in Appendix E.1. Results in Red denote the best-performing model. # Wins refers to the number of times the method performed best, and # Params is the number of model parameters. \u201c-\u201d means the dataset is not included in the original paper.", "description": "This table presents the forecasting performance results (MAE and MSE) for Time-LLM, CALF, and OneFitsAll models, along with their ablation variants. Results are averaged across different prediction lengths, providing a comprehensive comparison.  The table highlights the best-performing models for each dataset and metric, indicating the number of times each method achieved the best performance. Note that some datasets are missing from certain methods' original papers, represented by hyphens.", "section": "4 Results"}, {"figure_path": "DV15UbHCY1/tables/tables_16_2.jpg", "caption": "Table 3: Forecasting performance of all models \u2013 Time-LLM, CALF, and OneFitsAll and results from our ablations. All results are averaged across different prediction lengths, though full results are available in Appendix E.1. Results in Red denote the best-performing model. # Wins refers to the number of times the method performed best, and # Params is the number of model parameters. \u201c-\u201d means the dataset is not included in the original paper.", "description": "This table presents the forecasting performance results of three popular LLM-based time series forecasting methods (Time-LLM, CALF, OneFitsAll) and their corresponding ablation models (w/o LLM, LLM2Attn, LLM2Trsf). The performance is evaluated using MAE and MSE metrics across thirteen datasets and four prediction lengths.  Results are color-coded to highlight the best-performing model for each scenario, and the number of times each model achieved the best performance is also provided. The table includes the number of parameters used by each model.", "section": "4 Results"}, {"figure_path": "DV15UbHCY1/tables/tables_17_1.jpg", "caption": "Table 3: Forecasting performance of all models \u2013 Time-LLM, CALF, and OneFitsAll and results from our ablations. All results are averaged across different prediction lengths, though full results are available in Appendix E.1. Results in Red denote the best-performing model. # Wins refers to the number of times the method performed best, and # Params is the number of model parameters. \u201c-\u201d means the dataset is not included in the original paper.", "description": "This table presents the forecasting performance results of three popular LLM-based time series forecasting methods (Time-LLM, CALF, and OneFitsAll) and their corresponding ablations (w/o LLM, LLM2Attn, and LLM2Trsf) across thirteen datasets. The performance is evaluated using MAE and MSE metrics averaged over different prediction lengths.  The table also indicates which method achieved the best performance (# Wins) for each dataset and provides the number of parameters for each model.  Results highlighted in red indicate the best-performing model for each metric and dataset combination.", "section": "4 Results"}, {"figure_path": "DV15UbHCY1/tables/tables_17_2.jpg", "caption": "Table 3: Forecasting performance of all models \u2013 Time-LLM, CALF, and OneFitsAll and results from our ablations. All results are averaged across different prediction lengths, though full results are available in Appendix E.1. Results in Red denote the best-performing model. # Wins refers to the number of times the method performed best, and # Params is the number of model parameters. \u201c-\u201d means the dataset is not included in the original paper.", "description": "This table presents a comparison of the forecasting performance (MAE and MSE) of three popular LLM-based time series forecasting methods (Time-LLM, CALF, OneFitsAll) and their corresponding ablations (w/o LLM, LLM2Attn, LLM2Trsf) across thirteen datasets. The results are averaged across different prediction lengths.  The table highlights the best performing model for each dataset and metric and provides the number of times each model achieved the best performance (#Wins) and the number of model parameters.", "section": "4 Results"}, {"figure_path": "DV15UbHCY1/tables/tables_18_1.jpg", "caption": "Table 3: Forecasting performance of all models \u2013 Time-LLM, CALF, and OneFitsAll and results from our ablations. All results are averaged across different prediction lengths, though full results are available in Appendix E.1. Results in Red denote the best-performing model. # Wins refers to the number of times the method performed best, and # Params is the number of model parameters. \u201c-\u201d means the dataset is not included in the original paper.", "description": "This table presents the forecasting performance results for three popular LLM-based time series forecasting methods (Time-LLM, CALF, OneFitsAll) and their corresponding ablation models (w/o LLM, LLM2Attn, LLM2Trsf). The results are averaged across various prediction lengths, with detailed results provided in Appendix E.1.  The table highlights the best-performing model for each dataset and metric using red font, and indicates the number of times each model achieved the best performance (\"# Wins\").  It also indicates the number of parameters for each model.", "section": "4 Results"}, {"figure_path": "DV15UbHCY1/tables/tables_18_2.jpg", "caption": "Table 3: Forecasting performance of all models \u2013 Time-LLM, CALF, and OneFitsAll and results from our ablations. All results are averaged across different prediction lengths, though full results are available in Appendix E.1. Results in Red denote the best-performing model. # Wins refers to the number of times the method performed best, and # Params is the number of model parameters. \u201c-\u201d means the dataset is not included in the original paper.", "description": "This table presents the forecasting performance of three popular LLM-based time series forecasting methods (Time-LLM, CALF, OneFitsAll) and their corresponding ablation models (without LLM, LLM replaced with attention, LLM replaced with transformer).  The performance is evaluated across 13 datasets using Mean Absolute Error (MAE) and Mean Squared Error (MSE) metrics, averaged across different prediction horizons. The table highlights the best-performing model for each dataset and metric and indicates the number of times each model achieved the best performance. It also shows the number of parameters for each model and indicates where datasets were not present in the original papers.", "section": "4 Results"}, {"figure_path": "DV15UbHCY1/tables/tables_19_1.jpg", "caption": "Table 3: Forecasting performance of all models \u2013 Time-LLM, CALF, and OneFitsAll and results from our ablations. All results are averaged across different prediction lengths, though full results are available in Appendix E.1. Results in Red denote the best-performing model. # Wins refers to the number of times the method performed best, and # Params is the number of model parameters. \u201c-\u201d means the dataset is not included in the original paper.", "description": "This table presents the forecasting performance results for three state-of-the-art LLM-based time series forecasting methods (Time-LLM, CALF, and OneFitsAll) and their corresponding ablation models.  The results are averaged over different prediction horizons and presented for multiple benchmark datasets. The table highlights the best-performing model for each dataset/metric combination and shows the number of times each method achieved the best performance (\"Wins\") and the number of parameters for each model.  Note that some datasets are not included in the original papers' results.", "section": "4 Results"}, {"figure_path": "DV15UbHCY1/tables/tables_21_1.jpg", "caption": "Table 3: Forecasting performance of all models \u2013 Time-LLM, CALF, and OneFitsAll and results from our ablations. All results are averaged across different prediction lengths, though full results are available in Appendix E.1. Results in \\textbf{Red} denote the best-performing model. # Wins refers to the number of times the method performed best, and # Params is the number of model parameters. \u201c-\u201d means the dataset is not included in the original paper.", "description": "This table presents the forecasting performance results for three popular LLM-based time series forecasting methods (Time-LLM, CALF, OneFitsAll) and their corresponding ablation models (w/o LLM, LLM2Attn, LLM2Trsf).  The results are averaged across different prediction lengths and presented for multiple datasets and metrics (MAE and MSE). The best performing model in each case is highlighted in red.  The '# Wins' column shows the number of times each model achieved the best performance across all datasets and prediction lengths, while '# Parameters' indicates the number of model parameters.", "section": "4 Results"}, {"figure_path": "DV15UbHCY1/tables/tables_22_1.jpg", "caption": "Table 3: Forecasting performance of all models \u2013 Time-LLM, CALF, and OneFitsAll and results from our ablations. All results are averaged across different prediction lengths, though full results are available in Appendix E.1. Results in Red denote the best-performing model. # Wins refers to the number of times the method performed best, and # Params is the number of model parameters. \u201c-\u201d means the dataset is not included in the original paper.", "description": "This table presents the forecasting performance (MAE and MSE) for three popular LLM-based time series forecasting methods (Time-LLM, CALF, OneFitsAll) and their corresponding ablation studies (w/o LLM, LLM2Attn, LLM2Trsf) across thirteen benchmark datasets.  The results are averaged over different prediction lengths. The table highlights the best-performing method for each dataset and metric, indicating the number of times each method achieved the best performance. The number of model parameters is also included for comparison.", "section": "4 Results"}, {"figure_path": "DV15UbHCY1/tables/tables_23_1.jpg", "caption": "Table 3: Forecasting performance of all models \u2013 Time-LLM, CALF, and OneFitsAll and results from our ablations. All results are averaged across different prediction lengths, though full results are available in Appendix E.1. Results in Red denote the best-performing model. # Wins refers to the number of times the method performed best, and # Params is the number of model parameters. \u201c-\u201d means the dataset is not included in the original paper.", "description": "This table presents a comparison of the forecasting performance (MAE and MSE) of three popular LLM-based time series forecasting methods (Time-LLM, CALF, OneFitsAll) and their corresponding ablations (w/o LLM, LLM2Attn, LLM2Trsf) across thirteen datasets. The results are averaged across multiple prediction lengths.  The table highlights the best-performing model for each dataset and metric and indicates the number of times each method achieved the best performance.", "section": "4 Results"}, {"figure_path": "DV15UbHCY1/tables/tables_24_1.jpg", "caption": "Table 3: Forecasting performance of all models \u2013 Time-LLM, CALF, and OneFitsAll and results from our ablations. All results are averaged across different prediction lengths, though full results are available in Appendix E.1. Results in Red denote the best-performing model. # Wins refers to the number of times the method performed best, and # Params is the number of model parameters. \u201c-\u201d means the dataset is not included in the original paper.", "description": "This table presents the forecasting performance results for three popular LLM-based time series forecasting methods (Time-LLM, CALF, OneFitsAll) and their corresponding ablation variants. The performance is evaluated using MAE and MSE metrics across thirteen benchmark datasets and four prediction lengths.  The table highlights the best performing method for each dataset and metric combination and indicates the number of times each model achieved the best performance. The number of model parameters for each model is also shown.", "section": "4 Results"}, {"figure_path": "DV15UbHCY1/tables/tables_25_1.jpg", "caption": "Table 21: Results for input shuffling/masking for Time-LLM, CALF, and OneFitsAll methods on ETTh1 (predict length are \"96, 192, 336 and 720\") and Illness (predict length are \"24, 36, 48 and 60\"), the impact of shuffling the input on the degradation of time series forecasting performance does not change significantly before and after model modifications.", "description": "This table presents the results of experiments evaluating the effect of input shuffling and masking on the performance of three time series forecasting methods: Time-LLM, CALF, and OneFitsAll.  The experiments were conducted on two datasets, ETTh1 and Illness, with various prediction lengths. The results show that shuffling or masking the input data does not significantly impact the forecasting performance, regardless of whether or not the large language model component is included in the model.", "section": "4.4 Do LLMs represent sequential dependencies in time series? (RQ4)"}, {"figure_path": "DV15UbHCY1/tables/tables_25_2.jpg", "caption": "Table 3: Forecasting performance of all models \u2013 Time-LLM, CALF, and OneFitsAll and results from our ablations. All results are averaged across different prediction lengths, though full results are available in Appendix E.1. Results in Red denote the best-performing model. # Wins refers to the number of times the method performed best, and # Params is the number of model parameters. \u201c-\u201d means the dataset is not included in the original paper.", "description": "This table presents the forecasting performance results (MAE and MSE) for three popular LLM-based time series forecasting models (Time-LLM, CALF, OneFitsAll) and their corresponding ablation methods (without LLM, LLM replaced with attention, LLM replaced with transformer).  Results are shown for thirteen datasets, averaged over different prediction lengths.  The best performing model for each dataset and metric is highlighted in red, and the number of times each model outperformed others is also noted (# Wins).  The table also shows the number of parameters used by each model.", "section": "4 Results"}, {"figure_path": "DV15UbHCY1/tables/tables_25_3.jpg", "caption": "Table 3: Forecasting performance of all models \u2013 Time-LLM, CALF, and OneFitsAll and results from our ablations. All results are averaged across different prediction lengths, though full results are available in Appendix E.1. Results in Red denote the best-performing model. # Wins refers to the number of times the method performed best, and # Params is the number of model parameters. \u201c-\u201d means the dataset is not included in the original paper.", "description": "This table presents the forecasting performance results (MAE and MSE) for three popular LLM-based time series forecasting methods (Time-LLM, CALF, OneFitsAll) and their corresponding ablation versions.  The results are averaged across various prediction lengths.  The table highlights the best performing model for each dataset and metric, and shows the number of times each method achieved the best performance (#Wins) and the number of model parameters (#Params). Datasets not included in the original papers are marked with '-'.", "section": "4 Results"}]