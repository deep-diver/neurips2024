[{"figure_path": "DV15UbHCY1/figures/figures_3_1.jpg", "caption": "Figure 1: Overview of all LLM ablation methods. Figure (a) represents time series forecasting using an LLM as the base model. In some works, the LLM components are frozen [15, 14], while in others, they undergo fine-tuning [50, 22, 4]. Figure (b) shows the model with the LLM components removed, retaining only the remaining structure. Figure (c) replaces the LLM components with a single-layer self-attention mechanism. Figure (d) replaces the LLM components with a simple Transformer.", "description": "This figure illustrates the four different models used in the ablation study.  (a) shows a standard LLM-based model for time series forecasting, where the LLM can be either frozen or fine-tuned.  (b), (c), and (d) show ablation models where the LLM is removed, replaced with a self-attention layer, and replaced with a Transformer block, respectively.  Each ablation isolates the effect of the LLM to understand its contribution to the forecasting performance.", "section": "3.2 Proposed Ablations"}, {"figure_path": "DV15UbHCY1/figures/figures_4_1.jpg", "caption": "Figure 1: Overview of all LLM ablation methods. Figure (a) represents time series forecasting using an LLM as the base model. In some works, the LLM components are frozen [15, 14], while in others, they undergo fine-tuning [50, 22, 4]. Figure (b) shows the model with the LLM components removed, retaining only the remaining structure. Figure (c) replaces the LLM components with a single-layer self-attention mechanism. Figure (d) replaces the LLM components with a simple Transformer.", "description": "This figure illustrates four different models for time series forecasting.  Model (a) uses a pre-trained large language model (LLM) as the core component. In models (b), (c), and (d), the LLM is ablated: (b) the LLM is entirely removed; (c) the LLM is replaced with a single self-attention layer; (d) the LLM is replaced with a simple Transformer block.  This allows the authors to isolate the impact of the LLM on forecasting performance.", "section": "3.2 Proposed Ablations"}, {"figure_path": "DV15UbHCY1/figures/figures_6_1.jpg", "caption": "Figure 3: Ablation methods consume less time for inference while providing better forecasting performance. The figure above shows the inference time and prediction accuracy of Time-LLM, OneFitsAll, and CALF on ETTm2, Traffic, and Electricity datasets, averaged across prediction lengths. For more datasets and MSE metrics refer to Figure 7 and Figure 8 in the Appendix.", "description": "This figure compares the inference time and prediction accuracy (MAE) of three popular LLM-based time series forecasting methods (Time-LLM, OneFitsAll, and CALF) against their ablated versions (w/o LLM, LLM2Attn, LLM2Trsf) across three different datasets (ETTm2, Traffic, and Electricity). The results are averaged across various prediction lengths.  The key takeaway is that the ablation methods generally achieve comparable or better forecasting accuracy while significantly reducing inference time, suggesting the LLM component is not essential for good performance.", "section": "3.3 Datasets and Evaluation Metrics"}, {"figure_path": "DV15UbHCY1/figures/figures_8_1.jpg", "caption": "Figure 1: Overview of all LLM ablation methods. Figure (a) represents time series forecasting using an LLM as the base model. In some works, the LLM components are frozen [15, 14], while in others, they undergo fine-tuning [50, 22, 4]. Figure (b) shows the model with the LLM components removed, retaining only the remaining structure. Figure (c) replaces the LLM components with a single-layer self-attention mechanism. Figure (d) replaces the LLM components with a simple Transformer.", "description": "This figure illustrates the four different methods used for time series forecasting in the paper. (a) shows the standard method of using an LLM. (b) shows a model without the LLM, (c) shows one using a self-attention layer instead of the LLM and (d) one with a Transformer instead of the LLM.", "section": "3.2 Proposed Ablations"}, {"figure_path": "DV15UbHCY1/figures/figures_19_1.jpg", "caption": "Figure 2: In the above examples, only OneFitsAll \u201cw/ LLM\u201d performs better than the ablation methods on ETTh1, but there is substantial overlap in bootstraped confidence intervals. The figures show the comparison of OneFitsAll, CALF, and Time-LLM using LLMs and ablations (i.e., w/o LLM, LLM2Attn, and LLM2Trsf) on ETTh1, ETTm2, and Electricity, and the vertical dashed lines represent the results from the original work. Others Figures for MSE and other datasets are available in Figure 5 and Figure 6 in the Appendix.", "description": "The figure compares the performance of three popular LLM-based time series forecasting methods (OneFitsAll, CALF, and Time-LLM) against their ablations (removing the LLM component or replacing it with simpler structures).  The results show that in most cases, simpler methods perform comparably or even better than the original LLM-based methods, especially considering the substantial reduction in computational cost. The figure showcases this performance comparison across three different datasets (ETTh1, ETTm2, and Electricity) and using the MAE metric.  Bootstrapped confidence intervals are used to account for variability in the results.", "section": "Results"}, {"figure_path": "DV15UbHCY1/figures/figures_20_1.jpg", "caption": "Figure 1: Overview of all LLM ablation methods. Figure (a) represents time series forecasting using an LLM as the base model. In some works, the LLM components are frozen [15, 14], while in others, they undergo fine-tuning [50, 22, 4]. Figure (b) shows the model with the LLM components removed, retaining only the remaining structure. Figure (c) replaces the LLM components with a single-layer self-attention mechanism. Figure (d) replaces the LLM components with a simple Transformer.", "description": "This figure illustrates the four different ablation methods used in the paper to evaluate the impact of LLMs in time series forecasting. The first setup uses a pretrained LLM, while the others progressively remove or replace parts of the LLM with simpler components to analyze the contribution of the LLM to the overall performance.  Each panel shows a simplified diagram of the model architecture.", "section": "3.2 Proposed Ablations"}, {"figure_path": "DV15UbHCY1/figures/figures_26_1.jpg", "caption": "Figure 1: Overview of all LLM ablation methods. Figure (a) represents time series forecasting using an LLM as the base model. In some works, the LLM components are frozen [15, 14], while in others, they undergo fine-tuning [50, 22, 4]. Figure (b) shows the model with the LLM components removed, retaining only the remaining structure. Figure (c) replaces the LLM components with a single-layer self-attention mechanism. Figure (d) replaces the LLM components with a simple Transformer.", "description": "This figure illustrates the four different models used in the ablation study.  The first model uses a pre-trained Large Language Model (LLM) as the core of the time series forecasting model, showing both frozen and fine-tuned variations. The next three models demonstrate the ablations: removing the LLM entirely, replacing it with a self-attention layer, and replacing it with a Transformer block.  Each ablation modifies the original LLM-based model to isolate the impact of the LLM on forecasting performance.", "section": "3.2 Proposed Ablations"}, {"figure_path": "DV15UbHCY1/figures/figures_27_1.jpg", "caption": "Figure 5: Ablation studies indicate that when different methods remove the LLM (\"w/o LLM\") or replace it with a single-layer attention (\"LLM2Attn\") or Transformer (\"LLM2Trsf\"), the performance on time series forecasting tasks with MAE metric does not decline and even improves, compared with original methods, such as \u201cGPT-2\u201d or \u201cLLaMA\u201d. The vertical dashed line in the figures represents the results from the original paper. Above figures are from 'ETTh2', 'ETTm1', 'Illness', 'Weather', and 'Traffic' datasets.", "description": "This figure shows the results of ablation studies on three popular LLM-based time series forecasting methods.  It demonstrates that removing the LLM component or replacing it with simpler architectures (a single-layer attention or a transformer block) does not negatively impact forecasting performance, and in many cases, even improves it.  The results are shown using the MAE (Mean Absolute Error) metric across several datasets, comparing the original LLM-based models with their ablated versions. The vertical dashed lines represent the results reported in the original papers for comparison.", "section": "4 Results"}, {"figure_path": "DV15UbHCY1/figures/figures_27_2.jpg", "caption": "Figure 3: Ablation methods consume less time for inference while providing better forecasting performance. The figure above shows the inference time and prediction accuracy of Time-LLM, OneFitsAll, and CALF on ETTm2, Traffic, and Electricity datasets, averaged across prediction lengths. For more datasets and MSE metrics refer to Figure 7 and Figure 8 in the Appendix.", "description": "The figure shows that using LLMs for time series forecasting increases inference time by orders of magnitude, while not improving forecasting accuracy.  Ablation studies, which remove or replace the LLM component with simpler models, show comparable or better performance with significantly reduced inference time.  This suggests that the computational overhead of LLMs does not translate to better forecasting accuracy in the context of time series analysis.", "section": "3.3 Datasets and Evaluation Metrics"}]