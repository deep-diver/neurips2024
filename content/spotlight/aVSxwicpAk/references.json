{"references": [{"fullname_first_author": "Jordan Hoffmann", "paper_title": "An empirical analysis of compute-optimal large language model training", "publication_date": "2022-12-01", "reason": "This paper is foundational for the current work, as it empirically investigates the compute-optimal scaling laws for LLMs, which the current work seeks to theoretically analyze and explain."}, {"fullname_first_author": "Jared Kaplan", "paper_title": "Scaling laws for neural language models", "publication_date": "2020-01-28", "reason": "This work first introduced the concept of neural scaling laws, establishing the relationship between model size, dataset size, and computational cost for language models and is crucial to the context of the current paper."}, {"fullname_first_author": "Blake Bordelon", "paper_title": "Learning Curves for SGD on Structured Features", "publication_date": "2022-01-01", "reason": "This paper provides valuable theoretical insights into the behavior of SGD and scaling laws, particularly for models with structured features, which is closely related to and relevant to the context of the main study."}, {"fullname_first_author": "Yasaman Bahri", "paper_title": "Explaining neural scaling laws", "publication_date": "2024-06-26", "reason": "This work offers a complementary analysis, expanding on previous empirical studies to provide a more comprehensive theoretical understanding of neural scaling laws."}, {"fullname_first_author": "Alexander Maloney", "paper_title": "A Solvable Model of Neural Scaling Laws", "publication_date": "2022-10-26", "reason": "This paper presents a solvable model for neural scaling laws, enabling a theoretical understanding of the key scaling phenomena investigated in the main paper and offering a direct comparison and contrast."}]}