[{"figure_path": "aVSxwicpAk/figures/figures_1_1.jpg", "caption": "Figure 1: Toy scaling problem. We plot the loss function, P(0r; d) as a function of flops using (2). Consider a fixed number of flops f = 107 (dashed line). If we had chosen, e.g., d = 1600, we can run for a long time, but our model does not have a lot of capacity and thus the value of the loss function remains high. On the hand, we can increase capacity by choosing a large number of parameters (e.g., d = 51, 200), but because our compute is fixed we can not run our algorithm for very long. Thus the loss value is still large. The optimal choice is d \u2248 6, 400. When done for every choice of f gives the compute-optimal curve (red line). This choice of (\u03b1, \u03b2) (Phase I) is an example of where model capacity controls the compute-optimal curve, but it is not the only behavior we show. In other phases the compute-optimal is controlled by poor model embedding (Phase II, III) and SGD noise (Phase III, IV).", "description": "This figure illustrates a toy scaling problem. It shows how the loss function changes as a function of compute (flops) for different model sizes (d) with a fixed compute budget. The optimal model size that minimizes loss for a given compute budget is highlighted, demonstrating that there is an optimal model size to minimize loss given a fixed compute budget.", "section": "Main contributions"}, {"figure_path": "aVSxwicpAk/figures/figures_2_1.jpg", "caption": "Figure 2: Phase Diagram and Cartoon Plots of Loss Curves in Different Phases. (a) Phase Diagram. Colored regions represent where the training of the risk/compute-optimal curves look qualitatively and quantitatively different depending on \u03b1 and \u03b2. This, in term, yields different scaling law (\u03b7) and parameter count (\u03be) exponents for each of the phases. Critical point at \u03b1 = \u03b2 = 1/2 where all behaviors are observed. The other plots illustrate the components of F (via Fo, Fpp, Fac) and Kpp which dominate the loss curve for each phase (see Sec. C.4.1 & Sec. C.4.1 for proofs); tradeoff between the functions where the compute-optimal point occurs is also indicated (see Sec. 2.1 for definitions and Sec. 3.1 & Sec. D for proofs).", "description": "This figure shows a phase diagram and cartoon plots of loss curves for different phases in a three-parameter neural scaling model. The phase diagram illustrates regions where the training dynamics and optimal scaling laws change depending on data and target complexity. The cartoon plots visually represent how the loss curves behave for each phase, highlighting the dominant factors influencing the optimal parameter count and loss (model capacity, SGD noise, feature embedding).", "section": "The 4 Phases"}, {"figure_path": "aVSxwicpAk/figures/figures_3_1.jpg", "caption": "Figure 3: Compute-Optimal Front in Phase II-III boundary. (a) The Volterra equations perfectly captures the training dynamics of SGD when model-parameter count ranges from d = 200 \u2192 12800. (b) We apply IsoFLOP approach [23] to our toy model to extract the optimal-compute front: (compute-optimal loss) (highlighted in red in (a)) and the optimal model size: (compute-optimal model size) (scattered in purple in (c)). Power-law fitting compute-optimal front gives a measurement of the scaling law exponent 0.648 (vs. theoretical prediction 0.643 in Table 2). In (c), we power-law fit the relation between compute and (empirical) optimal model size via Approach 1 and 2 used in [23]: d* = f0.508 and d* = f0.525, resp. (vs. theory, d* = f0.5). See Sec. J for details.", "description": "This figure demonstrates the compute-optimal front and model size in Phase II-III boundary of the parameter space.  It compares empirical measurements with the theoretical predictions, validating the model's accuracy. The figure includes plots showing the compute-optimal loss curve, iso-FLOP slices, and the relation between optimal model size and compute.", "section": "2 Learning Dynamics of SGD"}, {"figure_path": "aVSxwicpAk/figures/figures_5_1.jpg", "caption": "Figure 4: (a) Scaling Law Exponents. The heatmap displays scaling law exponents (7) in the (\u03b1, \u03b2)-plane. Hatched lines represent region with universal scaling behavior, d* = f0.5, independent of (a, \u03b2). (b) Exponent Measurements. Compare empirical exponents (following [23]; see Sec.J for details) to theoretical predictions, traversing the phase diagram horizontally at a = 0.7 from Phases Ia \u2192 II \u2192 III as \u1e9e \u2191.", "description": "This figure shows the scaling law exponents (\u03b7) and parameter count exponents (\u03be) in the (\u03b1,\u03b2)-plane. The heatmap (a) shows the scaling law exponent \u03b7 and the parameter count exponent \u03be for each phase. The hatched lines show the region where the universal scaling behavior d* = f0.5 is observed, independent of the values of \u03b1 and \u03b2. The plot (b) compares the empirical exponents (measured using the method in [23]) and theoretical predictions. The plot shows that the theoretical predictions are consistent with the empirical measurements, especially when the data complexity \u03b1 is high and the target complexity \u03b2 is low.", "section": "3 The 4 Phases"}, {"figure_path": "aVSxwicpAk/figures/figures_5_2.jpg", "caption": "Figure 4: (a) Scaling Law Exponents. The heatmap displays scaling law exponents (7) in the (\u03b1, \u03b2)-plane. Hatched lines represent region with universal scaling behavior, d* = f0.5, independent of (\u03b1, \u03b2). (b) Exponent Measurements. Compare empirical exponents (following [23]; see Sec.J for details) to theoretical predictions, traversing the phase diagram horizontally at \u03b1 = 0.7 from Phases Ia \u2192 II \u2192 III as \u03b2 \u2191.", "description": "This figure shows the scaling law exponents in the (\u03b1, \u03b2)-plane. The heatmap in (a) visualizes how the scaling law exponent changes depending on the data complexity (\u03b1) and target complexity (\u03b2). The hatched lines indicate a region where the scaling behavior is universal, meaning d* (optimal parameter count) is always proportional to f0.5 (flops) regardless of \u03b1 and \u03b2 values.  Part (b) compares these theoretical predictions with empirical measurements obtained by traversing the phase diagram horizontally (\u03b1 = 0.7) while increasing \u03b2. The graph demonstrates how the measured exponents transition through different phases (Ia, II, III) as \u03b2 increases.", "section": "3 The 4 Phases"}, {"figure_path": "aVSxwicpAk/figures/figures_6_1.jpg", "caption": "Figure 5: Finite-size effects. (a) The ratio of the exact solution of eq. (10) to the estimate in eq. (17) is bounded by constants for all r, confirming the validity of eq. (17); shown here is (\u03b1, \u03b2) = (0.7, 1.2). (b) For non-asymptotic d, the estimate in eq. (17) (solid curves) predicts both the magnitudes and trends of the measured exponents of the empirical compute-optimal frontier (points), shown here for (\u03b1, \u03b2) = (0.7, 1.2) computed using Approach 0 (see Appendix J) to capture the instantaneous slope; the dashed lines show the asymptotic exponents from Table 2. (c) The finite-size behavior relaxes to the asymptotic predictions over horizons whose length can grow exceedingly large, especially in the vicinity of the phase transition, shown here for \u03b2 = 0.7 approaching the Phase 4a 4b boundary.", "description": "This figure demonstrates the finite-size effects on the compute-optimal curves. Panel (a) shows the ratio of the exact solution of the Volterra equation to an estimate, confirming the estimate's validity. Panel (b) compares the estimate with empirical measurements, showing good agreement even for non-asymptotic d values. Panel (c) demonstrates that the finite-size effects diminish over longer training times, particularly near phase transitions.", "section": "2 Learning Dynamics of SGD"}, {"figure_path": "aVSxwicpAk/figures/figures_39_1.jpg", "caption": "Figure 6: Spectra of empirical and theory weighted by D1/2\u03b2. Empirical spectra (blue) averaged over 100 randomly generated matrices W \u2208 Rvxd. Point mass at z = 0 was manually removed. Theory (orange) computed using the resolvent formula (9) and solved with Newton method (10 iterations for each z; z-values were spaced at 0.1d-2a with an imaginary part at d-20). There is a continuous part that evolves into a pure point outliers.", "description": "This figure compares the empirical and theoretical spectra of the random matrix K = D^(1/2)WWT D^(1/2), weighted by D^(1/2)hat(\u03b2). The empirical spectra are obtained by averaging over 100 randomly generated matrices W, while the theoretical spectra are computed using the resolvent formula (9) and a Newton method. The figure shows the three distinct parts of the spectrum: a point mass at z=0, pure point outliers, and an absolutely continuous part. The point mass at z=0 was manually removed from the empirical spectra before the comparison.", "section": "E Spectrum of K: random matrix theory"}, {"figure_path": "aVSxwicpAk/figures/figures_54_1.jpg", "caption": "Figure 7: Contour of \u0393 + \u0393\u03bf. This is used to estimate the m and derive expressions for the forcing function and kernel function. The important part of the contour is \u0393\u03bf, which contains the point mass at 0 (blue) and \u0393c (purple) which contains the bulk of the spectrum of deterministic equivalent of K. There is a left spectral gap which occurs at d\u22122\u03b1. Moreover we have a change of behavior at d\u2212\u03b1 in the contour to account for the change of behavior from pure point to absolutely continuous bulk part of the spectrum.", "description": "This figure shows the contour used to estimate the forcing and kernel functions. The contour is split into three parts: \u0393\u03bf, \u0393c, and \u0393caps. \u0393\u03bf is a small contour around 0, \u0393c is the contour for the bulk of the spectrum of K, and \u0393caps connects the ends of \u0393c. The spectral gap occurs at d\u22122\u03b1. The figure also shows how the contour changes behavior at d\u2212\u03b1 due to the transition from pure points to absolutely continuous spectrum.", "section": "F Analysis of Volterra equation"}, {"figure_path": "aVSxwicpAk/figures/figures_70_1.jpg", "caption": "Figure 4: (a) Scaling Law Exponents. The heatmap displays scaling law exponents (7) in the (\u03b1, \u03b2)-plane. Hatched lines represent region with universal scaling behavior, d* = f0.5, independent of (a, \u03b2). (b) Exponent Measurements. Compare empirical exponents (following [23]; see Sec.J for details) to theoretical predictions, traversing the phase diagram horizontally at a = 0.7 from Phases Ia \u2192 II \u2192 III as \u03b2 \u2191.", "description": "This figure shows the scaling law exponents in the (\u03b1, \u03b2) plane. The heatmap shows the scaling law exponent values for different combinations of \u03b1 and \u03b2. The hatched lines in the heatmap indicate the universal scaling regime where the optimal parameter count is proportional to the square root of the compute budget. The second part of the figure shows a comparison of empirical and theoretical exponent measurements across different phases for a fixed \u03b1 of 0.7.  As \u03b2 increases, the system transitions through phases Ia, II, and III. The plot compares the empirical scaling law exponents measured using a method from a previous study to the theoretical predictions derived in the current paper.", "section": "3 The 4 Phases"}, {"figure_path": "aVSxwicpAk/figures/figures_70_2.jpg", "caption": "Figure 3: Compute-Optimal Front in Phase II-III boundary. (a) The Volterra equations perfectly captures the training dynamics of SGD when model-parameter count ranges from d = 200 \u2192 12800. (b) We apply IsoFLOP approach [23] to our toy model to extract the optimal-compute front: (compute-optimal loss) (highlighted in red in (a)) and the optimal model size: (compute-optimal model size) (scattered in purple in (c)). Power-law fitting compute-optimal front gives a measurement of the scaling law exponent 0.648 (vs. theoretical prediction 0.643 in Table 2). In (c), we power-law fit the relation between compute and (empirical) optimal model size via Approach 1 and 2 used in [23]: d* = f0.508 and d* = f0.525, resp. (vs. theory, d* = f0.5). See Sec. J for details.", "description": "Figure 3 shows the compute-optimal front in the boundary between Phase II and III.  Panel (a) shows that the Volterra equation captures the training dynamics of SGD across a range of model sizes. Panel (b) uses the IsoFLOP method to extract the compute-optimal front, shown in red and panel (c) shows the power-law fit to the optimal model size. The scaling law exponent was measured as 0.648, similar to the theoretical prediction of 0.643.", "section": "Compute-Optimal Front in Phase II-III boundary"}, {"figure_path": "aVSxwicpAk/figures/figures_71_1.jpg", "caption": "Figure 3: Compute-Optimal Front in Phase II-III boundary. (a) The Volterra equations perfectly captures the training dynamics of SGD when model-parameter count ranges from d = 200 \u2192 12800. (b) We apply IsoFLOP approach [23] to our toy model to extract the optimal-compute front: (compute-optimal loss) (highlighted in red in (a)) and the optimal model size: (compute-optimal model size) (scattered in purple in (c)). Power-law fitting compute-optimal front gives a measurement of the scaling law exponent 0.648 (vs. theoretical prediction 0.643 in Table 2). In (c), we power-law fit the relation between compute and (empirical) optimal model size via Approach 1 and 2 used in [23]: d* = f0.508 and d* = f0.525, resp. (vs. theory, d* = f0.5). See Sec. J for details.", "description": "This figure shows the results of applying the IsoFLOP approach to the toy model used in the paper.  Panel (a) displays the compute-optimal front, which is a curve showing the optimal loss achievable for a given compute budget.  This curve is compared to the theoretical predictions from the Volterra equations. Panel (b) focuses on a specific IsoFLOP slice (a vertical line in (a)) to show the relationship between model size and loss. Panel (c) fits a power law to the optimal model size across various compute budgets, comparing it to existing theoretical predictions. The figure validates the theoretical predictions with experimental results, especially for the scaling law exponent.", "section": "Compute-Optimal Front in Phase II-III boundary"}, {"figure_path": "aVSxwicpAk/figures/figures_71_2.jpg", "caption": "Figure 10: 2 different IsoFLOP windows for measuring the parameter count exponent with Approach 1 for (\u03b1, \u03b2) = (0.5, 0.7).", "description": "This figure shows how sensitive the parameter count exponent is to the choice of IsoFLOP window used in Approach 1.  The plots show the optimal parameter counts (number of parameters) plotted against the compute budget (in FLOPs) for two different IsoFLOP windows: [1e6, 1e8] (a) and [2e6, 0.5e8] (b). For both plots, the data points represent empirical measurements obtained from training runs. The lines represent power law fits to the data, where the exponent of the power law represents the parameter count exponent.  The different exponents in (a) and (b) highlight that the exponent changes when different windows are selected. This implies that the value of the parameter count exponent is sensitive to this hyperparameter.", "section": "J Experimental Results"}, {"figure_path": "aVSxwicpAk/figures/figures_71_3.jpg", "caption": "Figure 3: Compute-Optimal Front in Phase II-III boundary. (a) The Volterra equations perfectly captures the training dynamics of SGD when model-parameter count ranges from d = 200 \u2192 12800. (b) We apply IsoFLOP approach [23] to our toy model to extract the optimal-compute front: (compute-optimal loss) (highlighted in red in (a)) and the optimal model size: (compute-optimal model size) (scattered in purple in (c)). Power-law fitting compute-optimal front gives a measurement of the scaling law exponent 0.648 (vs. theoretical prediction 0.643 in Table 2). In (c), we power-law fit the relation between compute and (empirical) optimal model size via Approach 1 and 2 used in [23]: d* = f0.508 and d* = f0.525, resp. (vs. theory, d* = f0.5). See Sec. J for details.", "description": "This figure shows the results of applying the IsoFLOP approach to a toy model to extract the compute-optimal front and optimal model size.  Panel (a) shows the compute-optimal front, which is the curve connecting the minimum losses achievable for different compute budgets. Panel (b) shows how the scaling law exponent is measured from the compute-optimal front via power-law fitting. Panel (c) displays a power-law fit of optimal model size as a function of compute budget.  The results are compared to theoretical predictions, showing good agreement between measurement and theory.", "section": "Compute-Optimal Front in Phase II-III boundary"}, {"figure_path": "aVSxwicpAk/figures/figures_71_4.jpg", "caption": "Figure 3: Compute-Optimal Front in Phase II-III boundary. (a) The Volterra equations perfectly captures the training dynamics of SGD when model-parameter count ranges from d = 200 \u2192 12800. (b) We apply IsoFLOP approach [23] to our toy model to extract the optimal-compute front: (compute-optimal loss) (highlighted in red in (a)) and the optimal model size: (compute-optimal model size) (scattered in purple in (c)). Power-law fitting compute-optimal front gives a measurement of the scaling law exponent 0.648 (vs. theoretical prediction 0.643 in Table 2). In (c), we power-law fit the relation between compute and (empirical) optimal model size via Approach 1 and 2 used in [23]: d* = f0.508 and d* = f0.525, resp. (vs. theory, d* = f0.5). See Sec. J for details.", "description": "This figure shows the results of applying the IsoFLOP method to a toy model.  Panel (a) demonstrates how well the Volterra equations capture the training dynamics of SGD. Panel (b) shows the compute-optimal front and the associated optimal model size. A power-law fit of the compute-optimal front yields a scaling law exponent, which is compared to a theoretical prediction.  Panel (c) shows a power-law fit of the relationship between compute and optimal model size, comparing the empirical results to theoretical predictions.", "section": "2 Learning Dynamics of SGD"}, {"figure_path": "aVSxwicpAk/figures/figures_72_1.jpg", "caption": "Figure 4: (a) Scaling Law Exponents. The heatmap displays scaling law exponents (7) in the (\u03b1, \u03b2)-plane. Hatched lines represent region with universal scaling behavior, d* = f0.5, independent of (a, \u03b2). (b) Exponent Measurements. Compare empirical exponents (following [23]; see Sec.J for details) to theoretical predictions, traversing the phase diagram horizontally at a = 0.7 from Phases Ia \u2192 II \u2192 III as \u1e9e \u2191.", "description": "This figure shows the scaling law exponents in the (\u03b1, \u03b2)-plane, which are obtained from the theoretical analysis using the Volterra equation. The heatmap in (a) shows that the scaling law exponents depend on the data complexity \u03b1 and target complexity \u03b2. The hatched lines represent the region with universal scaling behavior, d* = f0.5, which is independent of \u03b1 and \u03b2. The plot (b) compares the empirical exponents with the theoretical predictions. The empirical exponents are obtained from the experiments using the Chinchilla approach, and the theoretical predictions are obtained from the theoretical analysis.", "section": "3 The 4 Phases"}, {"figure_path": "aVSxwicpAk/figures/figures_73_1.jpg", "caption": "Figure 3: Compute-Optimal Front in Phase II-III boundary. (a) The Volterra equations perfectly captures the training dynamics of SGD when model-parameter count ranges from d = 200 \u2192 12800. (b) We apply IsoFLOP approach [23] to our toy model to extract the optimal-compute front: (compute-optimal loss) (highlighted in red in (a)) and the optimal model size: (compute-optimal model size) (scattered in purple in (c)). Power-law fitting compute-optimal front gives a measurement of the scaling law exponent 0.648 (vs. theoretical prediction 0.643 in Table 2). In (c), we power-law fit the relation between compute and (empirical) optimal model size via Approach 1 and 2 used in [23]: d* = f0.508 and d* = f0.525, resp. (vs. theory, d* = f0.5). See Sec. J for details.", "description": "This figure shows the compute-optimal front obtained from the Volterra equations and the IsoFLOP approach. Panel (a) shows the training loss as a function of floating-point operations, highlighting the compute-optimal loss. Panel (b) shows the IsoFLOP curve fitting to obtain the scaling law exponent. Panel (c) shows the optimal model size, comparing the empirical results from the IsoFLOP approach with theoretical predictions.", "section": "Compute-Optimal Front in Phase II-III boundary"}, {"figure_path": "aVSxwicpAk/figures/figures_73_2.jpg", "caption": "Figure 14: Negative \u03b2. Sweeping \u03b2 = -0.2 to \u03b2 = 0.2. We see good agreement between Volterra (theory) dynamics and SGD dynamics.", "description": "This figure shows the experimental results for negative values of \u03b2.  It presents multiple plots, each corresponding to a different \u03b2 value ranging from -0.2 to 0.2, with \u03b1 held constant at 0.7. For each \u03b2, multiple curves represent different model parameter counts (d), compared to the theoretical prediction from the Volterra equation. The plots visually demonstrate the close agreement between the theoretical predictions and empirical observations across various values of \u03b2, validating the theoretical model's ability to accurately capture the learning dynamics.", "section": "J Experimental Results"}, {"figure_path": "aVSxwicpAk/figures/figures_74_1.jpg", "caption": "Figure 3: Compute-Optimal Front in Phase II-III boundary. (a) The Volterra equations perfectly captures the training dynamics of SGD when model-parameter count ranges from d = 200 \u2192 12800. (b) We apply IsoFLOP approach [23] to our toy model to extract the optimal-compute front: (compute-optimal loss) (highlighted in red in (a)) and the optimal model size: (compute-optimal model size) (scattered in purple in (c)). Power-law fitting compute-optimal front gives a measurement of the scaling law exponent 0.648 (vs. theoretical prediction 0.643 in Table 2). In (c), we power-law fit the relation between compute and (empirical) optimal model size via Approach 1 and 2 used in [23]: d* = f0.508 and d* = f0.525, resp. (vs. theory, d* = f0.5). See Sec. J for details.", "description": "This figure shows the compute-optimal front and optimal model size obtained from the three-parameter neural scaling model. The left panel (a) shows the compute-optimal front, which is the curve representing the lowest loss for each compute budget. The middle panel (b) shows how the model size (number of parameters) changes to achieve this optimal loss in the Phase II-III boundary.  The right panel (c) presents a power-law fit for the optimal model size as a function of the compute budget, comparing the empirical findings with the theoretical prediction, validating the scaling law exponent of approximately 0.5.", "section": "Compute-Optimal Front in Phase II-III boundary"}, {"figure_path": "aVSxwicpAk/figures/figures_75_1.jpg", "caption": "Figure 3: Compute-Optimal Front in Phase II-III boundary. (a) The Volterra equations perfectly captures the training dynamics of SGD when model-parameter count ranges from d = 200 \u2192 12800. (b) We apply IsoFLOP approach [23] to our toy model to extract the optimal-compute front: (compute-optimal loss) (highlighted in red in (a)) and the optimal model size: (compute-optimal model size) (scattered in purple in (c)). Power-law fitting compute-optimal front gives a measurement of the scaling law exponent 0.648 (vs. theoretical prediction 0.643 in Table 2). In (c), we power-law fit the relation between compute and (empirical) optimal model size via Approach 1 and 2 used in [23]: d* = f0.508 and d* = f0.525, resp. (vs. theory, d* = f0.5). See Sec. J for details.", "description": "This figure shows the results of applying the IsoFLOP approach to the toy model, comparing theoretical predictions with empirical measurements.  Panel (a) shows the compute-optimal front using the Volterra equations.  Panels (b) and (c) showcase power-law fitting of compute-optimal loss and model size respectively, validating the theoretical predictions.", "section": "2.1 Approximation solution for P"}]