[{"figure_path": "aVSxwicpAk/tables/tables_4_1.jpg", "caption": "Table 1: Large d behavior of the forcing function and kernel function. See Sec. H for proofs.", "description": "This table shows the asymptotic behavior of different components of the forcing function and the kernel function as the model parameter count (d) becomes large.  These functions are crucial for understanding the learning dynamics of SGD, specifically for predicting the compute-optimal curves of the loss function. The table lists the asymptotic expressions for Fo(r), Fpp(r), Fac(r), and Kpp(r), which represent different aspects of the loss landscape, such as gradient descent, model capacity, and SGD noise. The asymptotic behavior is determined based on whether 2\u03b2 > 1 or 2\u03b2 < 1 and in the case that 2\u03b2 > 1, there are different expressions for Fac(r) depending on whether 2\u03b1 > 1 or 2\u03b1 < 1.   These results provide important theoretical building blocks for the analysis of the compute-optimal curves. ", "section": "2 Learning Dynamics of SGD"}, {"figure_path": "aVSxwicpAk/tables/tables_8_1.jpg", "caption": "Table 2: Loss description for P(r) and compute-optimal curves for P(r, d) across the 4 phases.", "description": "This table summarizes the four phases of compute-optimal curves identified in the paper. For each phase, it provides a description of the loss function P(r), the trade-off between the dominant terms in the loss, and the resulting compute-optimal curves (P*(f) and d*(f)) derived using a specific three parameter model.", "section": "The 4 Phases"}, {"figure_path": "aVSxwicpAk/tables/tables_15_1.jpg", "caption": "Table 3: Comparison of the source/capacity parameters across various related work. We note this table is taken from Table 1 in [DLM24]\u00b9 with the addition of [Lin+24]5. We note that both [DLM24] and [Lin+24] appeared concurrently with this article.", "description": "This table compares the notation and parameters of different works related to the paper. It shows how the input dimension, number of features, iterations/samples, capacity, source, and target decay parameters are defined and denoted in various papers, including the current work and several others. The table aims to clarify the relationships between these parameters across different research efforts in neural scaling laws.", "section": "A Additional Related Work"}, {"figure_path": "aVSxwicpAk/tables/tables_16_1.jpg", "caption": "Table 2: Loss description for P(r) and compute-optimal curves for P(, d) across the 4 phases.", "description": "This table summarizes the key characteristics of the four phases identified in the paper regarding compute-optimal neural scaling laws.  For each phase, it provides a description of the loss function P(r) in terms of the dominant components (Fpp, Fac, Fo, Kpp), highlighting the trade-off between these components that determines the compute-optimal behavior.  Furthermore, it presents the compute-optimal curves for P(, d), including the expressions for the optimal parameter count d*(f) and the compute-optimal loss P*(f).", "section": "The 4 Phases"}, {"figure_path": "aVSxwicpAk/tables/tables_27_1.jpg", "caption": "Table 2: Loss description for P(r) and compute-optimal curves for P(r, d) across the 4 phases.", "description": "This table summarizes the characteristics of the four phases identified in the paper, namely Phase I, II, III, and IV. For each phase, it provides a description of the loss function P(r), indicating which terms (Fpp(r), Fac(r), Fo(r), and Kpp(r)) are dominant.  Additionally, it specifies the trade-off between these terms that leads to compute-optimal curves, and it gives the resulting expressions for the compute-optimal curves, P*(f), and the optimal parameter count, d*(f).  The table helps to understand how the different components of the loss function interact and how they shape the compute-optimal behavior in each phase.", "section": "2 Learning Dynamics of SGD"}, {"figure_path": "aVSxwicpAk/tables/tables_33_1.jpg", "caption": "Table 2: Loss description for \\(\\mathcal{P}(r)\\) and compute-optimal curves for \\(\\tilde{\\mathcal{P}}(f, d)\\) across the 4 phases.", "description": "This table summarizes the characteristics of the four phases identified in the paper regarding compute-optimal neural scaling laws.  For each phase, it provides a description of the loss function (\\(\\mathcal{P}(r)\\)), the trade-off between the dominant terms in the loss function, the compute-optimal curves (\\(\\tilde{\\mathcal{P}}(f, d)\\)), and the compute-optimal parameter count (\\(d^*(f)\\)). The phases are categorized by the relative importance of model capacity, optimizer noise, and embedding of features, leading to distinct scaling behaviors. The table helps to understand the different regimes and predict the optimal neural network size based on available computational resources.", "section": "2 Learning Dynamics of SGD"}, {"figure_path": "aVSxwicpAk/tables/tables_36_1.jpg", "caption": "Table 2: Loss description for \\(\\mathcal{P}(r)\\) and compute-optimal curves for \\(\\mathcal{P}(\\frac{f}{d \\cdot B}; d)\\) across the 4 phases.", "description": "This table summarizes the key characteristics of the four phases identified in the paper's compute-optimal analysis.  For each phase, it provides a description of the loss function (\\(\\mathcal{P}(r)\\)), the trade-off that determines the compute-optimal point, and the resulting compute-optimal curves and parameter count exponents.  The table helps to understand how different combinations of data complexity (\u03b1) and target complexity (\u03b2) lead to distinct behaviors in the compute-optimal scaling laws.  Specific mathematical expressions for the optimal curves and exponents are given in the paper.", "section": "The 4 Phases"}]