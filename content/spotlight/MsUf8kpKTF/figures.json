[{"figure_path": "MsUf8kpKTF/figures/figures_1_1.jpg", "caption": "Figure 1: Examples of gridworld environment tasks. The agent (black triangle) begins each episode in the center of the environment. Blue jewels provide +1 reward, red jewels provide -1 reward, and dark grey walls prevent movement. Objects are placed randomly.", "description": "This figure shows three examples of gridworld environments used in the paper's experiments.  Each environment is a grid with walls (dark gray), positive reward locations (blue jewels), and negative reward locations (red jewels). The agent, represented by a black triangle, starts in the center of the grid and must navigate to collect rewards while avoiding the negative rewards and walls. The key point is that the placement of jewels and walls is randomized in each example, representing variations of the same type of task that the agent must learn to solve. These variations are used to simulate environmental distribution shift and investigate the impact on the agent's plasticity (ability to adapt to new environments).", "section": "Simulating environmental distribution shift"}, {"figure_path": "MsUf8kpKTF/figures/figures_3_1.jpg", "caption": "Figure 2: Performance in the gridworld environment under each of the three distribution shift conditions. The degradation between rounds is evidence of plasticity loss. (A): Epoch-level training performance for permute modification. Dotted vertical lines indicate the end of each round, before a new environmental distribution shift is applied. (B): Round-level training performance for the Permute modification. Data points correspond to normalized mean reward in final 50 episodes of round. Shaded regions correspond to standard error. (C, D): Round-level training performance for the Window and Expand conditions. Top row: Training performance. Bottom row: Test performance.", "description": "This figure shows the training and testing performance of a reinforcement learning agent in a gridworld environment under three different distribution shift conditions: Permute, Window, and Expand.  The plots demonstrate the phenomenon of plasticity loss, where the agent's performance degrades across rounds as the environment changes. Epoch-level and round-level performance metrics (normalized mean reward) are provided, along with standard error shaded regions for better visualization. The figure highlights that even with warm-started models, plasticity loss is prominent under the experimental distribution shifts.", "section": "4.1 Plasticity loss is apparent in on-policy RL"}, {"figure_path": "MsUf8kpKTF/figures/figures_4_1.jpg", "caption": "Figure 3: Top: Correlation plots of normalized mean reward in the gridworld environment compared against identified metrics. Each point is averaged over five replicates, and shows the final values produced after a ten-round experiment. Values for each measurement are normalized by its baseline level at initialization. Measurements that significantly correlate (p < 0.05) with normalized reward are bolded. First row: Training distribution performance. Second row: Test distribution performance. Bottom: Values of the three predictive metrics during the course of training for each intervention and window change condition as compared to training performance. Shaded regions correspond to standard error. Final values of plots like these correspond to a single point in the correlation plots above.", "description": "This figure shows the correlation between various metrics (policy entropy, weight magnitude, weight difference, gradient norm, and dead units) and the normalized mean reward in a gridworld environment.  The top section displays correlation plots showing the relationship between these metrics and both training and testing performance after ten rounds of experiments with various interventions. The bottom section shows the change in these metrics over the course of training for each intervention.  Strong correlations (p<0.05) are highlighted in bold.", "section": "4.2 Predictors of plasticity loss and generalization"}, {"figure_path": "MsUf8kpKTF/figures/figures_5_1.jpg", "caption": "Figure 4: Performance of intervention methods compared to warm-start and reset-all baselines on the Gridworld environment. Final round mean reward is normalized by the performance at end of the first round, and interval bars denote standard error. Top: Train performance. Bottom: Test performance.", "description": "This figure presents the performance comparison of several intervention methods to mitigate plasticity loss against two baselines (warm-start and reset-all) in the Gridworld environment under three different distribution shift conditions: Permute, Window, and Expand. The performance is measured by the normalized mean reward, comparing both training and testing phases. It showcases that some methods consistently improve performance across various conditions, while others have inconsistent effects.", "section": "4 Experiments"}, {"figure_path": "MsUf8kpKTF/figures/figures_6_1.jpg", "caption": "Figure 4: Performance of intervention methods compared to warm-start and reset-all baselines on the Gridworld environment. Final round mean reward is normalized by the performance at the end of the first round, and interval bars denote standard error. Top: Train performance. Bottom: Test performance.", "description": "This figure shows the performance of different plasticity loss intervention methods compared to the baseline methods (warm-start and reset-all) in the Gridworld environment across three different distribution shift conditions (Permute, Window, Expand).  The y-axis represents the normalized mean reward (normalized by the performance at the end of the first round) and the x-axis shows the different interventions. The top panel displays the training performance while the bottom panel shows the test performance.  Error bars represent standard error.  The results illustrate the effectiveness of various methods in mitigating plasticity loss in on-policy RL.", "section": "4 Experiments"}, {"figure_path": "MsUf8kpKTF/figures/figures_7_1.jpg", "caption": "Figure 6: A comparison between RND agents trained with and without a plasticity-loss mitigating intervention. Experiments are done over twenty replicates and shaded regions show standard error.", "description": "This figure compares the performance of three different approaches to training reinforcement learning agents on the Montezuma's Revenge game.  The x-axis represents the number of timesteps of training. The y-axis represents the cumulative reward achieved by the agent. The three lines represent different training methods:\n\n1. **Baseline:** A standard RND agent trained without any plasticity-loss mitigation techniques.\n2. **Regenerative (L2):** An RND agent trained with regenerative regularization (L2) to mitigate plasticity loss.\n3. **Soft shrink+perturb:** An RND agent trained with the soft shrink+perturb method to mitigate plasticity loss.\n\nThe shaded regions around each line represent the standard error, indicating the variability in performance across the twenty replicate experiments.  The figure clearly shows that the two methods designed to mitigate plasticity loss (Regenerative (L2) and Soft shrink+perturb) yield significantly better performance than the baseline method, demonstrating the effectiveness of these techniques in combating plasticity loss in the challenging Montezuma's Revenge environment.", "section": "4.7 Plasticity loss in Montezuma's Revenge"}, {"figure_path": "MsUf8kpKTF/figures/figures_14_1.jpg", "caption": "Figure 2: Performance in the gridworld environment under each of the three distribution shift conditions. The degradation between rounds is evidence of plasticity loss. (A): Epoch-level training performance for permute modification. Dotted vertical lines indicate the end of each round, before a new environmental distribution shift is applied. (B): Round-level training performance for the Permute modification. Data points correspond to normalized mean reward in final 50 episodes of round. Shaded regions correspond to standard error. (C, D): Round-level training performance for the Window and Expand conditions. Top row: Training performance. Bottom row: Test performance.", "description": "This figure shows the performance of a model trained using Proximal Policy Optimization (PPO) in a gridworld environment under three different types of distribution shift: permute, window, and expand. The results demonstrate the presence of plasticity loss, which is characterized by a degradation in the model's ability to fit new data as training progresses. The figure includes both epoch-level and round-level performance measures. The epoch-level plots provide a more fine-grained view of the learning process, while the round-level plots show a more concise summary of the performance at each round. The round-level plots also include standard error bars, which indicate the variability of the performance across different runs.", "section": "4.1 Plasticity loss is apparent in on-policy RL"}, {"figure_path": "MsUf8kpKTF/figures/figures_14_2.jpg", "caption": "Figure 7: Performance of intervention methods compared to baseline (warm-start) on Gridworld task. Shaded region corresponds to normalized mean episodic reward. Top row: Training distribution performance. Bottom row: Test distribution performance.", "description": "This figure shows the performance of different plasticity loss mitigation methods compared to a warm-start baseline in a gridworld environment.  The experiment uses three types of distribution shifts: permute, window, and expand.  For each shift, the figure presents both training and testing performance across multiple rounds.  Shaded areas represent the standard error of the mean. The results highlight how different methods impact performance under different distribution shift scenarios, showing which methods effectively mitigate plasticity loss and improve generalization.", "section": "4.1 Plasticity loss is apparent in on-policy RL"}, {"figure_path": "MsUf8kpKTF/figures/figures_15_1.jpg", "caption": "Figure 7: Performance of intervention methods compared to baseline (warm-start) on Gridworld task. Shaded region corresponds to normalized mean episodic reward. Top row: Training distribution performance. Bottom row: Test distribution performance.", "description": "This figure displays the performance of different intervention methods in mitigating plasticity loss in a gridworld environment. The experiment uses three types of distribution shift: Permute, Window, and Expand.  For each shift type, the training and test performance of each method is shown over ten rounds, comparing it to a warm-start baseline and a reset-all baseline.  The shaded regions represent the standard error around the mean reward.", "section": "4.1 Plasticity loss is apparent in on-policy RL"}, {"figure_path": "MsUf8kpKTF/figures/figures_15_2.jpg", "caption": "Figure 8: Performance of intervention methods compared to baseline (warm-start) on CoinRun task. Shaded region corresponds to normalized mean episodic reward. Top row: Training distribution performance. Bottom row: Test distribution performance.", "description": "This figure presents the results of an experiment evaluating different methods for mitigating plasticity loss in the CoinRun environment, a procedurally generated game.  The experiment is conducted with three different types of distribution shift (Permute, Window, Expand). Each type of shift is shown across two rows. The top row displays training performance (normalized mean episodic reward), and the bottom row displays testing performance (normalized mean episodic reward) across ten rounds of training.  Each method's performance is compared to a 'warm-start' baseline (training from a previously trained model) and a 'reset-all' baseline (re-initializing the model each round). The shaded area represents the standard error. This allows for a comparison of how well each method manages the plasticity loss (reduction in performance across rounds) compared to the baselines in training and testing performance under each distribution shift condition.", "section": "4.1 Plasticity loss is apparent in on-policy RL"}, {"figure_path": "MsUf8kpKTF/figures/figures_16_1.jpg", "caption": "Figure 11: Effect of learning under various conditions on five diagnostic metrics of interest.", "description": "This figure shows the effect of different continual learning interventions on five diagnostic metrics across three different distribution shifts (Permute, Window, Expand).  The metrics are: policy entropy, weight magnitude, weight difference, gradient norm, and dead units.  Each metric is plotted against the round number, providing a visual representation of how these metrics evolve as the agent learns across multiple tasks under each condition.  The shaded area represents the standard deviation across multiple experiment runs. The plots illustrate the impact of each intervention on plasticity and generalization performance.", "section": "4.2 Predictors of plasticity loss and generalization"}]