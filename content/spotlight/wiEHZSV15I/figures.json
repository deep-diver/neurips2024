[{"figure_path": "wiEHZSV15I/figures/figures_3_1.jpg", "caption": "Figure 1: An overview of the SSCNN. The grids are used to exemplify the selection maps I* and E* as defined in the main text, with Tin, Tout and N instantiated as 4, 4 and 3, respectively.", "description": "This figure provides a visual representation of the Selective Structured Components-based Neural Network (SSCNN) architecture.  It shows the flow of data through the different components of the model: embedding, inference of long-term, seasonal, and short-term components (using T-AttnNorm and selection maps I*), inference of spatial components (using S-AttnNorm and selection maps I*), extrapolation of components (using Extrapolate and selection maps E*), and finally component fusion using polynomial regression. The grids illustrate the selection maps, showing how the model selectively focuses on specific parts of the input sequence for each component.", "section": "Selective Structured Components-based Neural Network"}, {"figure_path": "wiEHZSV15I/figures/figures_7_1.jpg", "caption": "Figure 2: Examination of parameter scale and computation scale against the forward window size and the backward window size on the ECL dataset.", "description": "This figure shows the parameter scale and computational cost (measured in FLOPs) of SSCNN and other state-of-the-art models across varying forward and backward window sizes for the ECL dataset.  It demonstrates SSCNN's efficiency in terms of both parameters and computation, especially when compared to other Transformer-based models. The plots visualize the relationship between window size and model complexity, highlighting the parsimonious nature of SSCNN.", "section": "5.2 Comparative Analysis with Baselines"}, {"figure_path": "wiEHZSV15I/figures/figures_8_1.jpg", "caption": "Figure 3: Impacts of backward window size.", "description": "This figure shows the impact of the backward window size on the MSE of different models for four different settings. The settings vary the dataset (ECL or ETTh2) and the forward window size (24 or 336). In general, increasing the backward window size improves the MSE. However, the improvement is more significant for certain settings than for others. For example, in (a) ECL Tout=24, increasing the backward window size from 96 to 336 results in a significant drop in MSE. However, in (d) ETTh2 Tout=336, the improvement is marginal.", "section": "5.2 Comparative Analysis with Baselines"}, {"figure_path": "wiEHZSV15I/figures/figures_8_2.jpg", "caption": "Figure 4: Sensitivity analysis of hyper-parameters on the ECL and Traffic datasets.", "description": "This figure displays the sensitivity analysis of five hyperparameters (number of layers, hidden dimension, kernel size, cycle length, and short-term span) on the performance of the SSCNN model, using the ECL and Traffic datasets.  Each subplot shows how changes in a single hyperparameter affect the model's performance, while holding the others constant. This analysis aims to identify the most influential hyperparameters for tuning the model's accuracy.", "section": "5.3 Comparative Analysis of Model Configurations"}, {"figure_path": "wiEHZSV15I/figures/figures_8_3.jpg", "caption": "Figure 5: Performance comparison with various components in ECL and Traffic dataset.", "description": "This ablation study analyzes the impact of each component (long-term, seasonal, short-term, spatial) and attention mechanisms within the SSCNN model on two datasets (ECL and Traffic).  Removing components or attention mechanisms individually shows significant performance degradation, highlighting their importance for accurate forecasting.  The use of a fully-connected network (FCN) shows no improvement and may be redundant.", "section": "Ablation Study"}, {"figure_path": "wiEHZSV15I/figures/figures_18_1.jpg", "caption": "Figure 6: Disentangling structured components from the original time series data.", "description": "This figure visualizes the decomposition of a time series into its structured components (long-term, seasonal, and short-term) and a residual component.  For each component, the figure shows the original time series (blue), the extracted component (orange), the mean, and the standard deviation of the component. The decomposition process helps in isolating the different patterns that contribute to the overall time series.", "section": "3 Selective Structured Components-based Neural Network"}, {"figure_path": "wiEHZSV15I/figures/figures_18_2.jpg", "caption": "Figure 7: (a)(b)(c)(d): The evolution of conditional correlation between each pair of series when progressively controlling for three distinct types of factors. (e)(f)(g)(h):The evolution of conditional auto-correlation when progressively controlling for three distinct types of factors.", "description": "This figure visualizes how conditional correlations and autocorrelations change as the long-term, seasonal, and short-term components are progressively controlled for.  The top row shows the autocorrelations for the original time series and after removing each component. The bottom row shows the corresponding conditional correlation matrices. This illustrates how removing structured components reveals the remaining, less correlated components.", "section": "D.2 Temporal Regularity"}, {"figure_path": "wiEHZSV15I/figures/figures_19_1.jpg", "caption": "Figure 8: Visualization of input-168-predict-96 results on the Traffic dataset.", "description": "This figure visualizes the prediction results of six different time series forecasting models (SSCNN, iTransformer, PatchTST, TimeMixer, TimesNet, and Crossformer) on the Traffic dataset.  The input sequence length was 168 time steps, and the prediction horizon was 96 time steps. The figure allows a visual comparison of the accuracy and patterns captured by each model in comparison to the ground truth.", "section": "5.2 Comparative Analysis with Baselines"}, {"figure_path": "wiEHZSV15I/figures/figures_19_2.jpg", "caption": "Figure 8: Visualization of input-168-predict-96 results on the Traffic dataset.", "description": "This figure visualizes the performance of six different time series forecasting models (SSCNN, iTransformer, PatchTST, TimeMixer, TimesNet, and Crossformer) on the Traffic dataset.  Each model's prediction is plotted against the ground truth for a sequence length of 168 inputs and 96 predictions. This allows for a direct visual comparison of the accuracy and quality of the different forecasting methods.  The visual comparison allows for a quick understanding of each model's ability to capture the trends and patterns within the Traffic dataset.", "section": "5 Evaluations"}]