[{"figure_path": "6qr3932RWe/figures/figures_1_1.jpg", "caption": "Figure 1: A high-level diagram of 3D Gaussian Mapping (3DGM). Given multitraverse RGB videos, 3DGM outputs a Gaussian-based environment map (EnvGS) and 2D ephemerality segmentation (EmerSeg). Note that the proposed framework is LiDAR-free and self-supervised.", "description": "This figure illustrates the overall pipeline of the 3D Gaussian Mapping (3DGM) framework.  The input is multi-traverse RGB videos from the same location, meaning the same route is driven multiple times.  The 3DGM processes these videos to produce two outputs: a 3D environmental map represented as Gaussian splatting (EnvGS), and a 2D segmentation mask highlighting ephemeral (temporary) objects (EmerSeg).  Importantly, the method doesn't use LiDAR and is self-supervised, meaning it learns without human-provided labels.", "section": "1 Introduction"}, {"figure_path": "6qr3932RWe/figures/figures_4_1.jpg", "caption": "Figure 2: An overall illustration of 3DGM. Given RGB camera observations collected at different times, we use COLMAP to obtain the camera poses and initial Gaussian points. Then we utilize splatting-based rasterization to render both RGB images and robust features from the environmental Gaussians. We further leverage feature residuals to extract the object masks by mining spatial information of the residuals. Finally, we utilize the ephemerality masks to finetune the 3D Gaussians.", "description": "This figure illustrates the overall workflow of the 3D Gaussian Mapping (3DGM) method. It starts with multisession data collection from driving trajectories using a monocular camera to capture RGB images.  COLMAP is used to estimate camera poses and initialize 3D Gaussian points representing the environment.  These Gaussians are then used in a splatting-based rasterizer to generate rendered RGB images and robust features.  A loss function compares these to ground truth data, creating a feature loss map.  From the loss map, feature residual mining extracts 2D ephemerality masks which identify transient objects. Finally, these masks are used to refine the 3D environmental Gaussians, leading to an improved environment map and object segmentation.", "section": "4 3DGM: 3D Gaussian Mapping"}, {"figure_path": "6qr3932RWe/figures/figures_6_1.jpg", "caption": "Figure 3: IoU at 20 locations in Ithaca, NY.", "description": "This figure shows the Intersection over Union (IoU) scores achieved by the proposed EmerSeg method across 20 different locations within Ithaca, NY.  The x-axis represents the location index, and the y-axis represents the IoU score. The graph visually depicts the variability in performance across different locations, highlighting areas where the method performs exceptionally well (high IoU) and areas where performance is less strong (low IoU). This provides a comprehensive assessment of the EmerSeg algorithm's robustness and consistency across various geographic settings within a city.", "section": "5.1 Unsupervised 2D Ephemeral Object Segmentation"}, {"figure_path": "6qr3932RWe/figures/figures_7_1.jpg", "caption": "Figure IV: Visualizations of Mapverse-nuPlan dataset (locations 11-20). Each row represents different image observations of the same location captured during multiple traversals, with five shown for brevity. The images cover various environments in Las Vegas, including city streets with overpasses, iconic buildings, palm trees, billboards, and varied traffic conditions. The sequence progresses from urban settings with heavy infrastructure and prominent landmarks to broader streets and intersections, capturing different times of day and lighting conditions. Columns provide comparative views of the same locations under different circumstances, showcasing the dynamic and ever-changing urban landscape of Las Vegas as recorded in the Mapverse-nuPlan dataset.", "description": "This figure shows sample images from the Mapverse-nuPlan dataset, specifically locations 11 through 20.  Each row represents a different location, and each column shows the same location captured at different times (different traversals).  The images show diverse urban scenes in Las Vegas, illustrating the variation in appearance and traffic conditions over multiple traversals.", "section": "B.2 Mapverse-nuPlan"}, {"figure_path": "6qr3932RWe/figures/figures_8_1.jpg", "caption": "Figure 5: Qualitative and quantitative evaluation of 3D geometry in Mapverse-Ithaca365.", "description": "This figure presents a qualitative and quantitative comparison of the 3D geometry reconstruction results obtained using the proposed method (EnvGS) and a baseline method (DepthAnything). The left panel (a) shows depth visualizations for 20 locations in the Mapverse-Ithaca365 dataset, demonstrating EnvGS's ability to generate accurate and smooth depth maps. The right panel (b) presents a quantitative comparison in the form of a bar chart, showing the Chamfer Distance (CD) between the reconstructed depth and the ground truth LIDAR depth for each location.  The chart reveals that EnvGS significantly outperforms DepthAnything in terms of accuracy across all locations, with lower CD values indicating higher precision.", "section": "5.2 3D Environment Reconstruction"}, {"figure_path": "6qr3932RWe/figures/figures_9_1.jpg", "caption": "Figure 6: Qualitative evaluations of the environment rendering. Our method demonstrates robust performance against transient objects, and can even outperform the method equipped with a pretrained model in some cases. Notably, this includes the effective removal of object shadows.", "description": "This figure shows a qualitative comparison of environment rendering results between different methods. The top row displays original RGB images, followed by results from 3DGS, 3DGS+SegFormer, and EnvGS (the proposed method). The results demonstrate that EnvGS outperforms other methods in handling transient objects and removing object shadows, maintaining robust performance even without a pretrained model.", "section": "5.3 Neural Environment Rendering"}, {"figure_path": "6qr3932RWe/figures/figures_18_1.jpg", "caption": "Figure I: Visualizations of Mapverse-Ithaca365 dataset (locations 1\u201310). Each row represents image observations of the same location captured during different traversals, with five traversals shown for brevity. The figure encompasses diverse environments in the Ithaca area, from residential neighborhoods with houses, trees, and varying traffic, to suburban streets with signage and seasonal foliage changes, and finally to rural roads and highways with expansive landscapes. The columns provide comparative views of these locations under different conditions, highlighting the dynamic nature of the Mapverse-Ithaca365 dataset.", "description": "This figure visualizes sample data from the Mapverse-Ithaca365 dataset, showcasing various environments and conditions across multiple traversals of ten different locations in Ithaca, NY.  Each row shows images from the same location at different times, highlighting the consistent background elements and the changes in transient objects like vehicles and pedestrians.", "section": "B.3 Visualization of Sample Data"}, {"figure_path": "6qr3932RWe/figures/figures_19_1.jpg", "caption": "Figure I: Visualizations of Mapverse-Ithaca365 dataset (locations 1\u201310). Each row represents image observations of the same location captured during different traversals, with five traversals shown for brevity. The figure encompasses diverse environments in the Ithaca area, from residential neighborhoods with houses, trees, and varying traffic, to suburban streets with signage and seasonal foliage changes, and finally to rural roads and highways with expansive landscapes. The columns provide comparative views of these locations under different conditions, highlighting the dynamic nature of the Mapverse-Ithaca365 dataset.", "description": "This figure shows example images from the Mapverse-Ithaca365 dataset.  Each row displays images from the same location taken during different traversals, showcasing how the scene changes over time.  The dataset includes diverse environments, ranging from residential areas to highways, with varying weather and traffic conditions.", "section": "B.3 Visualization of Sample Data"}, {"figure_path": "6qr3932RWe/figures/figures_20_1.jpg", "caption": "Figure III: Visualizations of Mapverse-nuPlan dataset (locations 1-10). Each row represents different image observations of the same location captured during multiple traversals, with five shown for brevity. The images encompass diverse environments in Las Vegas, including wide city streets with iconic buildings, billboards, palm trees, pedestrian bridges, and varying traffic conditions. Columns provide comparative views of the same locations under different conditions, illustrating the variability and complexity of the cityscape as captured in the Mapverse-nuPlan dataset.", "description": "This figure shows sample images from the Mapverse-nuPlan dataset, specifically locations 1 through 10.  Each row represents a single location, with 5 different traversals shown across the columns. The images depict diverse Las Vegas environments, highlighting the variability in cityscapes across multiple driving conditions.  This showcases the dataset's complexity and variety.", "section": "B.2 Mapverse-nuPlan"}, {"figure_path": "6qr3932RWe/figures/figures_21_1.jpg", "caption": "Figure III: Visualizations of Mapverse-nuPlan dataset (locations 1-10). Each row represents different image observations of the same location captured during multiple traversals, with five shown for brevity. The images encompass diverse environments in Las Vegas, including wide city streets with iconic buildings, billboards, palm trees, pedestrian bridges, and varying traffic conditions. Columns provide comparative views of the same locations under different conditions, illustrating the variability and complexity of the cityscape as captured in the Mapverse-nuPlan dataset.", "description": "This figure shows example images from the Mapverse-nuPlan dataset, focusing on locations 1-10. Each row displays images of the same location taken during multiple traversals of the area, demonstrating how the visual appearance of a location can vary across different traversals due to changing traffic, weather, and lighting. The variety of scenes reflects different areas within Las Vegas.", "section": "B.2 Mapverse-nuPlan"}, {"figure_path": "6qr3932RWe/figures/figures_22_1.jpg", "caption": "Figure V: Qualitative evaluations of the emerged object masks. Our method demonstrates robust performance across a range of lighting and weather conditions, effectively handling diverse categories including cars, buses, and pedestrians. Some failure cases are highlighted with red rectangles.", "description": "This figure presents qualitative results of the 2D ephemeral object segmentation.  It shows the original RGB images and the corresponding object masks generated by the proposed method. The masks highlight objects like cars, buses, and pedestrians that are considered transient or ephemeral.  Red rectangles point out some cases where the segmentation was not successful. The figure demonstrates the method's robustness to different lighting and weather conditions. ", "section": "C Mapverse-Ithaca365: Additional Results of 2D Segmentation"}, {"figure_path": "6qr3932RWe/figures/figures_23_1.jpg", "caption": "Figure VI: Qualitative comparisons of our method and other supervised and unsupervised segmentation baselines. This image demonstrates a comparison between our mask extraction and those derived from other semantic segmentation methods. The results indicate that our masks maintain superior integrity and detail in complex environments. Meanwhile, our method significantly outperforms unsupervised semantic segmentation models [82, 83] and is roughly equivalent to the masks generated by InternImage [79] and SegVit [76]. Although Mask2Former [77], PSPNet [87], and SegFormer [78] have advantages in recognizing people and other fine-grained objects, they can also lead to incorrect segmentation and noise in certain scenarios.", "description": "This figure compares the results of the proposed method with five supervised and two unsupervised segmentation methods. The proposed method shows better performance in terms of mask integrity and detail, especially in complex scenes, compared to the other unsupervised methods.  It performs similarly to some of the supervised methods, but the supervised methods also show some errors such as incorrect segmentations.", "section": "C.2 Visualizations of Supervised and Unsupervised Segmentation"}, {"figure_path": "6qr3932RWe/figures/figures_24_1.jpg", "caption": "Figure VII: IoU performance over iterations for different feature resolutions (110x180 and 140x210) and corresponding visualizations of ephemerality masks and feature residuals. Visualizations at various iterations (500 to 10000) illustrate that higher iterations lead to more detailed and accurate segmentation. The results highlight the efficiency of the 110x180 resolution and the fast convergence of our method for effective segmentation.", "description": "This figure shows the Intersection over Union (IoU) performance of the model over different training iterations for two feature resolutions: 110x180 and 140x210.  The graph illustrates that higher resolution (110x180) converges faster and achieves higher IoU. The visualization shows ephemerality masks and feature residuals at various iterations.  It demonstrates that more training iterations lead to better segmentation accuracy, but the improvement diminishes after around 4000 iterations.", "section": "C.3 Performance over Training Iterations"}, {"figure_path": "6qr3932RWe/figures/figures_25_1.jpg", "caption": "Figure VIII: Visualizations of EmerSeg with inputs from different numbers of traversals. Each row represents a different scene of a location. The first column shows the original RGB images. The subsequent columns show the segmentation outputs from EmerSeg with 1, 2, 3, 7, and 10 traversals.", "description": "This figure demonstrates the impact of the number of traversals on the performance of the Emergent Ephemeral Segmentation (EmerSeg) method.  Each row shows a different scene from the dataset. The first column displays the original RGB image from that scene. The remaining columns show the segmentation results generated by EmerSeg using 1, 2, 3, 7, and 10 traversals of that scene, respectively. The visualization highlights how increasing the number of traversals improves the accuracy and completeness of the segmentation by providing more information for the model to identify and segment transient objects.", "section": "C.4 Ablation Study on Number of Traversal: Visualization and Discussion"}, {"figure_path": "6qr3932RWe/figures/figures_26_1.jpg", "caption": "Figure IX: Visualizations of ephemerality masks and feature residuals at different feature dimensions. The RGB images (leftmost column) are processed to generate ephemerality masks and feature residuals. As the feature dimensions increase, the segmentation accuracy improves, with the highest dimension (64) capturing the most detailed and accurate object masks. The residuals are more discriminative with higher dimensions, indicating better feature representation. The colored circles highlight specific areas to illustrate differences in segmentation quality across dimensions.", "description": "This figure shows an ablation study on the impact of feature dimension on the quality of 2D ephemerality segmentation.  It presents RGB images alongside their corresponding ephemerality masks and feature residuals, at four different feature dimensions (4, 8, 16, and 64). The results demonstrate that higher-dimensional features lead to more accurate and detailed object masks, because higher dimensions offer a more discriminative and comprehensive feature representation.", "section": "C.5 Ablation Study on Feature Dimension: Visualization and Discussion"}, {"figure_path": "6qr3932RWe/figures/figures_27_1.jpg", "caption": "Figure X: Visualizations of ephemerality masks and feature residuals at different feature (spatial) resolutions. The RGB images (leftmost column) are processed to generate ephemerality masks and feature residuals. As the feature resolution increases, the segmentation accuracy improves, with the highest resolution (110\u00d7180) capturing the most detailed and accurate object masks. The residuals are more informative with higher resolutions, indicating better feature representation and reduced segmentation errors. The colored circles highlight specific areas to illustrate differences in segmentation quality across resolutions.", "description": "This figure shows an ablation study on the effect of different spatial resolutions on the quality of ephemerality masks and feature residuals generated by the model.  As the resolution increases from 25x40 to 110x180, the accuracy and detail of the masks and residuals improve significantly. This demonstrates that higher resolutions provide better feature representation, leading to more accurate object segmentation.", "section": "C.6 Ablation Study on Feature Resolution: Visualization and Discussion"}, {"figure_path": "6qr3932RWe/figures/figures_28_1.jpg", "caption": "Figure XI: Comparison of ephemerality masks and feature residuals using different versions of the DINO model. The figure includes raw and denoised versions of DINOv1 and DINOv2, as well as raw and denoised versions of DINOv2 with a registration module (DINOv2-Register). Denoising enhances the quality of feature residuals, while registration does not yield notable gains.", "description": "This figure compares the performance of different versions of the DINO model in generating ephemerality masks and feature residuals.  It shows that denoising significantly improves the quality of the feature residuals, leading to more accurate masks, while adding a registration module to DINOv2 provides no additional benefit.", "section": "C.7 Ablation Study on Vision Foundation Model: Visualization and Discussion"}, {"figure_path": "6qr3932RWe/figures/figures_29_1.jpg", "caption": "Figure XII: Left: SfM Initialized Points. Right: Gaussian Points after Optimization.", "description": "This figure compares the 3D point clouds generated using Structure from Motion (SfM) and Gaussian Points after optimization. The left side shows the initial points obtained from SfM, which are scattered and less organized. The right side shows the refined Gaussian points after optimization, resulting in more coherent and precise representation of the scene. This highlights the optimization process in improving the accuracy and clarity of 3D reconstruction.", "section": "Mapverse-Ithaca365: Additional Visualizations of 3D Reconstruction"}, {"figure_path": "6qr3932RWe/figures/figures_30_1.jpg", "caption": "Figure III: Visualizations of Mapverse-nuPlan dataset (locations 1-10). Each row represents different image observations of the same location captured during multiple traversals, with five shown for brevity. The images encompass diverse environments in Las Vegas, including wide city streets with iconic buildings, billboards, palm trees, pedestrian bridges, and varying traffic conditions. Columns provide comparative views of the same locations under different conditions, illustrating the variability and complexity of the cityscape as captured in the Mapverse-nuPlan dataset.", "description": "This figure shows examples of the Mapverse-nuPlan dataset, focusing on locations 1-10 in Las Vegas. Each row displays images of the same location from different traversals, illustrating the variation in appearance across time and highlighting the diversity of urban environments in the dataset.  The images show various settings, including city streets with iconic buildings, palm trees, billboards, and varied traffic conditions.", "section": "B.2 Mapverse-nuPlan"}, {"figure_path": "6qr3932RWe/figures/figures_31_1.jpg", "caption": "Figure XIV: Qualitative evaluations of the environment rendering. Our method demonstrates robust performance against transient objects.", "description": "This figure presents a qualitative comparison of environment rendering results using three different methods: 3DGS, 3DGS+SegFormer, and EnvGS (the proposed method). For each method, the figure shows several rendered images alongside the original image.  The red circles highlight areas where transient objects are present in the original images, and the comparison helps to visually assess the effectiveness of each method in removing transient objects and rendering a clean environment map.", "section": "Mapverse-Ithaca365: Additional Visualizations of Neural Rendering"}, {"figure_path": "6qr3932RWe/figures/figures_32_1.jpg", "caption": "Figure III: Visualizations of Mapverse-nuPlan dataset (locations 1-10). Each row represents different image observations of the same location captured during multiple traversals, with five shown for brevity. The images encompass diverse environments in Las Vegas, including wide city streets with iconic buildings, billboards, palm trees, pedestrian bridges, and varying traffic conditions. Columns provide comparative views of the same locations under different conditions, illustrating the variability and complexity of the cityscape as captured in the Mapverse-nuPlan dataset.", "description": "This figure shows example images from the Mapverse-nuPlan dataset, specifically locations 1 through 10. Each row displays images of the same location captured during five different traversals (out of many), highlighting the variation in environmental conditions, such as time of day and traffic volume. The overall goal is to showcase the diversity and complexity of the urban environment captured by this dataset.", "section": "B.2 Mapverse-nuPlan"}, {"figure_path": "6qr3932RWe/figures/figures_33_1.jpg", "caption": "Figure VI: Qualitative comparisons of our method and other supervised and unsupervised segmentation baselines. This image demonstrates a comparison between our mask extraction and those derived from other semantic segmentation methods. The results indicate that our masks maintain superior integrity and detail in complex environments. Meanwhile, our method significantly outperforms unsupervised semantic segmentation models [82, 83] and is roughly equivalent to the masks generated by InternImage [79] and SegVit [76]. Although Mask2Former [77], PSPNet [87], and SegFormer [78] have advantages in recognizing people and other fine-grained objects, they can also lead to incorrect segmentation and noise in certain scenarios.", "description": "This figure compares the results of the proposed EmerSeg method against other supervised and unsupervised semantic segmentation methods. The comparison highlights EmerSeg's ability to maintain superior integrity and detail in complex scenarios.  While EmerSeg performs similarly to some supervised methods, it significantly outperforms unsupervised methods. Supervised methods show some advantages in identifying fine-grained objects but can also suffer from incorrect segmentations and noise.", "section": "C.2 Visualizations of Supervised and Unsupervised Segmentation"}, {"figure_path": "6qr3932RWe/figures/figures_34_1.jpg", "caption": "Figure V: Qualitative evaluations of the emerged object masks. Our method demonstrates robust performance across a range of lighting and weather conditions, effectively handling diverse categories including cars, buses, and pedestrians. Some failure cases are highlighted with red rectangles.", "description": "This figure shows qualitative results of the proposed method's ability to segment ephemeral objects in images.  It displays several example images alongside their corresponding segmentation masks generated by the algorithm.  The results show the method is effective in various lighting and weather conditions and across diverse object categories, but some failure cases, particularly with shadows, are also highlighted.", "section": "C Mapverse-Ithaca365: Additional Results of 2D Segmentation"}, {"figure_path": "6qr3932RWe/figures/figures_35_1.jpg", "caption": "Figure XVI: Qualitative results of EmerSeg for multiple traversals of location 1 of Mapverse-nuPlan. From left to right: raw RGB image, extracted 2D ephemeral object masks, and normalized feature residuals visualized using a jet color map.", "description": "This figure shows a qualitative comparison of the proposed EmerSeg method for unsupervised 2D ephemeral object segmentation.  It presents multiple traversals of a single location from the Mapverse-nuPlan dataset. Each row shows a sequence of images from the same location, captured at different times (traversals). The columns depict the original RGB images, the resulting 2D ephemeral object masks generated by EmerSeg, and the normalized feature residual maps. The feature residuals are visualized using a jet colormap, where brighter colors indicate higher feature residuals, likely corresponding to transient objects.", "section": "F Mapverse-nuPlan: Unsupervised 2D Segmentation"}, {"figure_path": "6qr3932RWe/figures/figures_36_1.jpg", "caption": "Figure XVI: Qualitative results of EmerSeg for multiple traversals of location 1 of Mapverse-nuPlan. From left to right: raw RGB image, extracted 2D ephemeral object masks, and normalized feature residuals visualized using a jet color map.", "description": "This figure shows the qualitative results of the proposed Emergent Scene Decomposition method (EmerSeg) on location 1 of the Mapverse-nuPlan dataset.  It visually compares the original RGB images from multiple traversals with the corresponding 2D ephemeral object masks generated by EmerSeg and the normalized feature residual maps. The jet color map is used to visualize the feature residuals, where higher intensity indicates higher residual values representing transient objects.", "section": "F Mapverse-nuPlan: Unsupervised 2D Segmentation"}, {"figure_path": "6qr3932RWe/figures/figures_37_1.jpg", "caption": "Figure IV: Visualizations of Mapverse-nuPlan dataset (locations 11-20). Each row represents different image observations of the same location captured during multiple traversals, with five shown for brevity. The images cover various environments in Las Vegas, including city streets with overpasses, iconic buildings, palm trees, billboards, and varied traffic conditions. The sequence progresses from urban settings with heavy infrastructure and prominent landmarks to broader streets and intersections, capturing different times of day and lighting conditions. Columns provide comparative views of the same locations under different circumstances, showcasing the dynamic and ever-changing urban landscape of Las Vegas as recorded in the Mapverse-nuPlan dataset.", "description": "This figure visualizes a subset of the Mapverse-nuPlan dataset, showing images from 10 different locations in Las Vegas, each captured across multiple traversals under diverse conditions (lighting, time of day, traffic, etc.). Each row displays images of a single location across several traversals, illustrating the variability and complexity of the urban landscape.", "section": "B.2 Mapverse-nuPlan"}, {"figure_path": "6qr3932RWe/figures/figures_38_1.jpg", "caption": "Figure XVI: Qualitative results of EmerSeg for multiple traversals of location 1 of Mapverse-nuPlan. From left to right: raw RGB image, extracted 2D ephemeral object masks, and normalized feature residuals visualized using a jet color map.", "description": "This figure shows the results of the proposed Emergent Scene Decomposition from Multitraverse (3DGM) method on a specific location (location 1) in the Mapverse-nuPlan dataset.  The images demonstrate the method's ability to segment ephemeral objects from a series of images taken at different times across multiple traversals of the same location.  The three columns illustrate the input RGB images, the resulting 2D ephemeral object masks, and a visualization of the normalized feature residuals used to help isolate those objects. The jet color map helps to visually represent the residuals, where higher values (warmer colors) indicate a higher likelihood of transient objects.", "section": "F Mapverse-nuPlan: Unsupervised 2D Segmentation"}, {"figure_path": "6qr3932RWe/figures/figures_39_1.jpg", "caption": "Figure IV: Visualizations of Mapverse-nuPlan dataset (locations 11-20). Each row represents different image observations of the same location captured during multiple traversals, with five shown for brevity. The images cover various environments in Las Vegas, including city streets with overpasses, iconic buildings, palm trees, billboards, and varied traffic conditions. The sequence progresses from urban settings with heavy infrastructure and prominent landmarks to broader streets and intersections, capturing different times of day and lighting conditions. Columns provide comparative views of the same locations under different circumstances, showcasing the dynamic and ever-changing urban landscape of Las Vegas as recorded in the Mapverse-nuPlan dataset.", "description": "This figure shows sample images from the Mapverse-nuPlan dataset, specifically locations 11-20.  Each row displays images from the same location across multiple drives, illustrating the dynamic nature of the urban environment due to changing traffic, lighting, and time of day. The variety of locations (city streets, intersections, etc.) and conditions is showcased.", "section": "B.2 Mapverse-nuPlan"}, {"figure_path": "6qr3932RWe/figures/figures_40_1.jpg", "caption": "Figure 6: Qualitative evaluations of the environment rendering. Our method demonstrates robust performance against transient objects, and can even outperform the method equipped with a pretrained model in some cases. Notably, this includes the effective removal of object shadows.", "description": "This figure showcases qualitative results of environment rendering using three different methods: 3DGS, 3DGS+SegFormer, and EnvGS (the proposed method). The results demonstrate that the proposed EnvGS method is robust against transient objects and effectively removes object shadows, while outperforming other methods in some cases.  Each row shows an original image alongside its renderings from the three methods, highlighting the effectiveness of EnvGS in producing high-quality renderings that accurately reflect the environment without being affected by transient objects and their shadows.", "section": "5.3 Neural Environment Rendering"}, {"figure_path": "6qr3932RWe/figures/figures_41_1.jpg", "caption": "Figure V: Qualitative evaluations of the emerged object masks. Our method demonstrates robust performance across a range of lighting and weather conditions, effectively handling diverse categories including cars, buses, and pedestrians. Some failure cases are highlighted with red rectangles.", "description": "This figure showcases qualitative results of the proposed method's ability to segment ephemeral objects (e.g., cars, buses, pedestrians) from multi-traverse RGB video sequences.  The images demonstrate the method's robustness across different lighting and weather conditions. Red rectangles highlight instances where the method failed to accurately segment the objects.", "section": "C Mapverse-Ithaca365: Additional Results of 2D Segmentation"}, {"figure_path": "6qr3932RWe/figures/figures_42_1.jpg", "caption": "Figure V: Qualitative evaluations of the emerged object masks. Our method demonstrates robust performance across a range of lighting and weather conditions, effectively handling diverse categories including cars, buses, and pedestrians. Some failure cases are highlighted with red rectangles.", "description": "This figure shows qualitative results of the 2D ephemeral object segmentation method (EmerSeg).  It presents several examples of RGB images from the Mapverse-Ithaca365 dataset alongside their corresponding segmentation masks. The masks highlight objects identified as ephemeral, primarily vehicles and pedestrians. Red rectangles highlight cases where the method struggled to accurately segment objects, while the successful segmentations are shown without special marking. The caption highlights the method's robustness across various conditions, but also acknowledges some failure cases.", "section": "C Mapverse-Ithaca365: Additional Results of 2D Segmentation"}, {"figure_path": "6qr3932RWe/figures/figures_42_2.jpg", "caption": "Figure V: Qualitative evaluations of the emerged object masks. Our method demonstrates robust performance across a range of lighting and weather conditions, effectively handling diverse categories including cars, buses, and pedestrians. Some failure cases are highlighted with red rectangles.", "description": "This figure showcases qualitative results of the proposed method's object mask generation.  The results show the method is fairly robust across varying lighting and weather conditions, correctly identifying various object types like cars, buses, and pedestrians. However, red rectangles highlight cases where the method failed, indicating areas for improvement.", "section": "C Mapverse-Ithaca365: Additional Results of 2D Segmentation"}, {"figure_path": "6qr3932RWe/figures/figures_42_3.jpg", "caption": "Figure XXIV: Failure cases of shadow segmentation.", "description": "This figure shows failure cases of shadow segmentation in the proposed method. Each row represents a different scene. The left column shows the original RGB images; the middle column displays the segmentation output of the proposed method; the right column highlights the areas where shadow removal failed, indicated by red circles. While there are some successful cases marked by green circles, the method lacks consistency across different scenes.", "section": "I.1 Unsupervised 2D Segmentation"}]