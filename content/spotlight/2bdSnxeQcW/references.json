{"references": [{"fullname_first_author": "John Schulman", "paper_title": "Proximal policy optimization algorithms", "publication_date": "2017-07-06", "reason": "This paper introduces the Proximal Policy Optimization algorithm, a foundational algorithm in reinforcement learning that is frequently used and referenced in modern RL research."}, {"fullname_first_author": "Timothy P Lillicrap", "paper_title": "Continuous control with deep reinforcement learning", "publication_date": "2015-09-09", "reason": "This paper demonstrates the successful application of deep reinforcement learning to continuous control tasks, a major breakthrough that opened up new possibilities for RL in robotics and other domains."}, {"fullname_first_author": "Scott Fujimoto", "paper_title": "Addressing function approximation error in actor-critic methods", "publication_date": "2018-07-01", "reason": "This work addresses a critical challenge in reinforcement learning, specifically the function approximation error in actor-critic methods, improving the stability and performance of these algorithms."}, {"fullname_first_author": "Tuomas Haarnoja", "paper_title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor", "publication_date": "2018-07-01", "reason": "This paper introduces the Soft Actor-Critic algorithm, which is known for its sample efficiency and ability to learn robust policies in complex environments."}, {"fullname_first_author": "Aviral Kumar", "paper_title": "Conservative Q-learning for offline reinforcement learning", "publication_date": "2020-07-01", "reason": "This paper introduces Conservative Q-learning, a key algorithm in offline reinforcement learning which addresses the overestimation bias problem and improves the stability and performance of offline RL algorithms."}]}