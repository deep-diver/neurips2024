[{"figure_path": "2bdSnxeQcW/tables/tables_6_1.jpg", "caption": "Table 1: Performance comparison: Normalized average return results", "description": "This table presents a comparison of the normalized average return results achieved by various offline reinforcement learning algorithms across different benchmark tasks. The algorithms include behavior cloning (BC), 10% BC, TD3+BC, CQL (paper), CQL (reprod.), Onestep, IQL, MCQ, MISA, and the proposed EPQ algorithm. The benchmark tasks are categorized into MuJoCo locomotion tasks, Adroit manipulation tasks, and AntMaze navigation tasks, each with multiple datasets representing varying levels of data quality and policy performance.", "section": "4 Experiments"}, {"figure_path": "2bdSnxeQcW/tables/tables_18_1.jpg", "caption": "Table 1: Performance comparison: Normalized average return results", "description": "This table presents a comparison of the normalized average return achieved by different offline reinforcement learning algorithms across various tasks from the D4RL benchmark.  The algorithms compared include Behavior Cloning (BC), 10% BC, TD3+BC, CQL (paper), CQL (reproduced), Onestep, IQL, MCQ, MISA, and the proposed EPQ method.  The tasks cover MuJoCo locomotion, Adroit manipulation, and AntMaze navigation, each with multiple datasets representing varying levels of data quality and policy behavior.", "section": "4 Experiments"}, {"figure_path": "2bdSnxeQcW/tables/tables_19_1.jpg", "caption": "Table 1: Performance comparison: Normalized average return results", "description": "This table presents a comparison of the average return achieved by different offline reinforcement learning algorithms across various benchmark tasks.  The results are normalized between 0 and 100, with 0 representing random performance and 100 representing expert performance.  The algorithms compared include behavior cloning (BC), 10% BC, TD3+BC, CQL (from the original paper and a reproduction), Onestep, IQL, MCQ, MISA, and the proposed EPQ. The tasks are categorized into MuJoCo locomotion tasks, Adroit manipulation tasks, and AntMaze navigation tasks, each with multiple datasets representing varying data quality and policy characteristics. The table allows for a comprehensive comparison of the performance of different methods in offline RL.", "section": "4 Experiments"}, {"figure_path": "2bdSnxeQcW/tables/tables_20_1.jpg", "caption": "Table 1: Performance comparison: Normalized average return results", "description": "This table compares the performance of EPQ with other state-of-the-art offline reinforcement learning algorithms across various tasks from the D4RL benchmark.  The tasks are categorized into Mujoco locomotion tasks, Adroit manipulation tasks, and AntMaze navigation tasks.  Each task includes several datasets representing different levels of difficulty and data collection methods (random, medium, expert, replay).  The table shows the normalized average return for each algorithm and dataset, providing a quantitative assessment of relative performance.", "section": "4 Experiments"}, {"figure_path": "2bdSnxeQcW/tables/tables_21_1.jpg", "caption": "Table 1: Performance comparison: Normalized average return results", "description": "This table presents a comparison of the average return achieved by different offline reinforcement learning algorithms across various tasks from the D4RL benchmark.  The results are normalized between 0 and 100, where 0 represents random performance and 100 represents expert performance.  The algorithms compared include behavior cloning (BC), 10% BC, TD3+BC, CQL (paper), CQL (reprod.), Onestep, IQL, MCQ, MISA, and the proposed EPQ method.  The tasks are categorized into Mujoco locomotion tasks, Adroit manipulation tasks, and AntMaze navigation tasks, each with multiple datasets representing different data distributions (e.g., random, medium, expert).", "section": "4 Experiments"}, {"figure_path": "2bdSnxeQcW/tables/tables_23_1.jpg", "caption": "Table 1: Performance comparison: Normalized average return results", "description": "This table compares the performance of the proposed EPQ algorithm against several existing offline reinforcement learning algorithms across various tasks from the D4RL benchmark.  The tasks are categorized into Mujoco Locomotion, Adroit Manipulation, and AntMaze Navigation tasks, with different dataset variations for each.  The results show normalized average returns (0-100 scale), providing a clear comparison of the algorithms' performance in different scenarios.", "section": "4 Experiments"}]