[{"figure_path": "eezCLKwx6T/figures/figures_5_1.jpg", "caption": "Figure 1: Overview of ADD. After the agent is trained on environments produced by the environment generator, the environment critic is updated using the episodic results. Then, the environment critic guides the diffusion-based environment generator with the regret to produce adversarial environments. By repeating this process, the agent learns a policy that is robust to environmental changes.", "description": "This figure illustrates the ADD (Adversarial Environment Design via Regret-Guided Diffusion Models) framework.  It shows a closed-loop system where an agent interacts with environments generated by a diffusion model. The agent's performance, measured as regret, is fed back into the system to guide the diffusion model's generation of new, more challenging environments. This iterative process allows the agent to learn a robust policy that generalizes well to unseen environments.", "section": "4.4 Adversarial Environment Design via Regret-Guided Diffusion Models"}, {"figure_path": "eezCLKwx6T/figures/figures_7_1.jpg", "caption": "Figure 2: Partially observable navigation results. (a): Zero-shot performance on the 12 test environments. We report results across five random seeds, each evaluated over 100 independent episodes per environment. (b): Training curves on two challenging test environments. (c): Complexity metrics of the generated environments during training. (d): t-SNE embedding of the generated environments during training.", "description": "This figure presents the results of the partially observable navigation task.  Subfigure (a) shows the zero-shot performance of the trained agent on 12 unseen test environments, with results averaged over five random seeds and 100 episodes per environment. Subfigure (b) displays the training curves for two challenging test environments (Labyrinth and Maze), illustrating the learning progress of different methods. Subfigure (c) presents complexity metrics (number of blocks and shortest path length) of generated environments across training iterations, showing the curriculum generated by various methods. Lastly, subfigure (d) visualizes the generated environments using t-SNE embeddings, demonstrating the diversity of environments explored by each approach.", "section": "5.1 Partially Observable Navigation Task"}, {"figure_path": "eezCLKwx6T/figures/figures_8_1.jpg", "caption": "Figure 3: 2D bipedal locomotion task results. (a): Zero-shot performance on the six test environments. We report results across five random seeds, each evaluated over 100 independent episodes per environment. (b): Complexity metrics of the generated environments and episodic return achieved during training.", "description": "This figure shows the results of the 2D bipedal locomotion experiments.  Subfigure (a) presents a bar chart comparing the zero-shot performance (average return over 100 episodes per environment, across five random seeds) of different reinforcement learning algorithms on six test environments. Subfigure (b) displays line graphs tracking several complexity metrics (stump height, stair height, pit gap, stair steps, ground roughness) of the training environments generated by each algorithm, alongside the episodic return achieved during training.  The algorithms are compared against domain randomization (DR) as a baseline.", "section": "5 Experiments"}, {"figure_path": "eezCLKwx6T/figures/figures_9_1.jpg", "caption": "Figure 7: Examples of environments generated at the beginning of the agent's training in the partially observable task. The figure shows 18 example environments that are generated right after the initiation of the agent's learning.", "description": "This figure shows 18 example environments generated by the algorithm at the beginning of the agent's training in the partially observable navigation task.  It visually demonstrates the initial environments generated for the agent to learn from.  These are relatively simple mazes, setting the stage for the progressively more challenging environments that will be generated as training progresses.", "section": "C.1 Partially Observable Navigation Task"}, {"figure_path": "eezCLKwx6T/figures/figures_17_1.jpg", "caption": "Figure 5: Maze environment generation using diffusion models. We represent the maze environment with a parameter \u03b8 \u2208 R13\u00d713\u00d73, with each channel indicating the location of walls, the agent, and the goal. After training the diffusion-based environment generator on a dataset of randomly generated environment parameters, we can sample maze environments by solving the reverse process (5).", "description": "This figure shows how the diffusion model generates maze environments. The maze is represented as a 13x13x3 tensor where each channel represents walls, agent, and goal.  The model, trained on random mazes, generates new ones by reversing a diffusion process, starting from noise and guided by the learned score function.", "section": "B Experiment Details and Hyperparameters"}, {"figure_path": "eezCLKwx6T/figures/figures_19_1.jpg", "caption": "Figure 6: Zero-shot test environments for the partially observable navigation task. SimpleCrossing and FourRooms environments are adopted from Chevalier-Boisvert et al. [53], and other test environments are adopted from Dennis et al. [14] and Jiang et al. [19, 20].", "description": "This figure shows the twelve unseen test environments used to evaluate the zero-shot generalization performance of the trained RL agent in the partially observable navigation task.  The environments vary in complexity, with some being simple and others more complex.  The environments from Chevalier-Boisvert et al. [53] and Jiang et al. [19, 20] provide a diverse range of challenges for the agent.", "section": "C.1 Partially Observable Navigation Task"}, {"figure_path": "eezCLKwx6T/figures/figures_20_1.jpg", "caption": "Figure 7: Examples of environments generated at the beginning of the agent's training in the partially observable task. The figure shows 18 example environments that are generated right after the initiation of the agent's learning.", "description": "This figure shows 18 example environments generated at the beginning of the agent's training in the partially observable navigation task.  These are generated immediately after the training begins. The images show a variety of maze layouts with varying degrees of complexity and openness.  The agent starts at the light blue triangle and needs to navigate to the green square.", "section": "C.1 Partially Observable Navigation Task"}, {"figure_path": "eezCLKwx6T/figures/figures_21_1.jpg", "caption": "Figure 7: Examples of environments generated at the beginning of the agent's training in the partially observable task. The figure shows 18 example environments that are generated right after the initiation of the agent's learning.", "description": "This figure shows 18 example maze environments generated by the proposed ADD algorithm at the very beginning of agent training.  The agent starts learning, and the environments are generated, showcasing the initial conditions.  The green square represents the goal, the blue triangle the agent, and the black squares the walls of the maze.", "section": "C.1 Partially Observable Navigation Task"}, {"figure_path": "eezCLKwx6T/figures/figures_21_2.jpg", "caption": "Figure 9: t-SNE plots of training environments in the partially observable navigation task. The figure shows t-SNE plots of ADD and baselines. The results are obtained by mapping the environment parameters to a latent space using the encoder of the learned environment critic, followed by training a t-SNE.", "description": "This figure shows t-SNE plots visualizing the training environments generated by different methods in the partially observable navigation task.  The dimensionality reduction technique t-SNE is applied to the latent space representations of the environments after encoding them with the environment critic's encoder.  The plots provide a visual comparison of environment diversity and distribution across methods such as ADD, DR, PAIRED, PLR+, ACCEL, and ADD without guidance, giving insights into the effectiveness of each method in generating diverse and informative training environments.", "section": "C.1 Partially Observable Navigation Task"}, {"figure_path": "eezCLKwx6T/figures/figures_22_1.jpg", "caption": "Figure 10: Test environments for the 2D bipedal locomotion task. Basic and Hardcore environments are adopted from OpenAI Gym [56], and other four test environments are adopted from Parker Holder et al. [20].", "description": "This figure shows six test environments for the 2D bipedal locomotion task.  The environments vary in difficulty, ranging from a simple flat surface (Basic) to more challenging terrains including stairs, pits, stumps, and uneven ground (Hardcore, Stair, PitGap, Stump, Roughness). The Basic and Hardcore environments were sourced from OpenAI Gym [56], while the other four were adapted from a previous study by Parker Holder et al. [20]. These diverse environments are used to evaluate the generalization capabilities of the trained agents.", "section": "5.2 2D Bipedal Locomotion Task"}, {"figure_path": "eezCLKwx6T/figures/figures_23_1.jpg", "caption": "Figure 1: Overview of ADD. After the agent is trained on environments produced by the environment generator, the environment critic is updated using the episodic results. Then, the environment critic guides the diffusion-based environment generator with the regret to produce adversarial environments. By repeating this process, the agent learns a policy that is robust to environmental changes.", "description": "This figure shows a schematic overview of the Adversarial Environment Design via Regret-Guided Diffusion Models (ADD) algorithm.  It illustrates the cyclical process:  The agent trains on environments generated by a diffusion model, the episodic results are fed to an environment critic to estimate the agent's regret, and this regret guides the diffusion model to generate new, more challenging environments. This loop continues until the agent develops a robust policy.", "section": "4 Proposed Method"}, {"figure_path": "eezCLKwx6T/figures/figures_23_2.jpg", "caption": "Figure 1: Overview of ADD. After the agent is trained on environments produced by the environment generator, the environment critic is updated using the episodic results. Then, the environment critic guides the diffusion-based environment generator with the regret to produce adversarial environments. By repeating this process, the agent learns a policy that is robust to environmental changes.", "description": "This figure illustrates the ADD (Adversarial Environment Design via Regret-Guided Diffusion Models) algorithm's workflow.  It shows how the agent, environment generator, and environment critic interact to create challenging environments for robust policy learning. The agent trains on generated environments, the environment critic assesses agent performance based on the results, and then guides the environment generator using the calculated regret to produce more challenging environments. This iterative process continues until the agent develops a robust policy that generalizes well to unseen environments.", "section": "4 Proposed Method"}, {"figure_path": "eezCLKwx6T/figures/figures_24_1.jpg", "caption": "Figure 9: t-SNE plots of training environments in the partially observable navigation task. The figure shows t-SNE plots of ADD and baselines. The results are obtained by mapping the environment parameters to a learned latent space using the encoder of the learned environment critic, followed by training a t-SNE.", "description": "This figure compares the diversity of training environments generated by different methods (DR, PAIRED, PLR+, ACCEL, ADD w/o guidance, and ADD) using t-SNE.  t-SNE is a dimensionality reduction technique that visualizes high-dimensional data in a 2D space, allowing for a visual comparison of the distribution of the generated environments.  The plot shows ADD generates a more diverse set of environments than other methods, indicating a more robust policy.", "section": "C.1 Partially Observable Navigation Task"}, {"figure_path": "eezCLKwx6T/figures/figures_24_2.jpg", "caption": "Figure 1: Overview of ADD. After the agent is trained on environments produced by the environment generator, the environment critic is updated using the episodic results. Then, the environment critic guides the diffusion-based environment generator with the regret to produce adversarial environments. By repeating this process, the agent learns a policy that is robust to environmental changes.", "description": "This figure shows the overall process of the proposed Adversarial Environment Design via Regret-Guided Diffusion Models (ADD) method. It illustrates how the agent, environment generator, and environment critic interact to create a curriculum of challenging environments that improve the agent's robustness to environmental changes.  The agent trains on environments generated by the diffusion model, which is guided by the regret signal from the environment critic. This critic estimates the performance difference between the current policy and the optimal policy and this difference is used to update the environment generator. The process repeats iteratively to enhance the agent's generalization capacity.", "section": "4 Proposed Method"}]