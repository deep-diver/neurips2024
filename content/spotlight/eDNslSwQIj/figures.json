[{"figure_path": "eDNslSwQIj/figures/figures_1_1.jpg", "caption": "Figure 1: 3D-aware editing with our Neural Asset representations. Given a source image and object 3D bounding boxes, we can translate, rotate, and rescale the object. In addition, we support compositional generation by transferring objects or backgrounds across images.", "description": "This figure demonstrates the capabilities of the Neural Assets model in manipulating objects within images.  Starting from a single image, the model allows for several edits including translation, rotation, and scaling of individual objects. Furthermore, it showcases the model's capacity for compositional generation, enabling the transfer of objects and backgrounds between different images.", "section": "1 Introduction"}, {"figure_path": "eDNslSwQIj/figures/figures_3_1.jpg", "caption": "Figure 2: Neural Assets framework. (a) We train our model on pairs of video frames, which contain objects under different poses. We encode appearance tokens from a source image with RoIAlign, and pose tokens from the objects' 3D bounding boxes in a target image. They are combined to form our Neural Asset representations. (b) An image diffusion model is conditioned on Neural Assets and a separate background token to reconstruct the target image as the training signal. (c) During inference, we can manipulate the Neural Assets to control the objects in the generated image: rotate the object's pose (blue) or replace an object by a different one from another image (pink).", "description": "This figure illustrates the Neural Assets framework.  Panel (a) shows how Neural Assets are created by combining appearance and pose features extracted from paired video frames.  Panel (b) depicts the training process, where a diffusion model learns to reconstruct a target image based on the Neural Assets and a background token.  Finally, panel (c) demonstrates inference, where Neural Assets are manipulated to control object pose and composition in a generated image.", "section": "3 Method: Neural Assets"}, {"figure_path": "eDNslSwQIj/figures/figures_5_1.jpg", "caption": "Figure 3: Single-object editing results on OBJect unseen object subset. We evaluate on the Translation, Rotation, and Removal tasks. We follow 3DIT [67] to compute metrics inside the edited object's bounding box. Our results are averaged over 3 random seeds.", "description": "The figure shows the quantitative results of single-object editing on the unseen object subset of the OBJect dataset.  Three editing tasks are evaluated: translation, rotation, and removal.  Performance is measured using PSNR, SSIM, and LPIPS, calculated within the bounding box of the edited object. The results show that the proposed method outperforms the baselines (Chained and 3DIT) on all three tasks.", "section": "4.2 Main Results"}, {"figure_path": "eDNslSwQIj/figures/figures_5_2.jpg", "caption": "Figure 4: Multi-object editing results on MOVi-E, Objectron, and Waymo Open (denoted as Waymo in the figures). We compute metrics inside the edited objects' bounding boxes.", "description": "The figure shows a comparison of multi-object editing results on three datasets: MOVi-E, Objectron, and Waymo Open.  The results are evaluated using PSNR, SSIM, and LPIPS metrics, which are calculated within the bounding boxes of the edited objects to isolate the editing quality from the surrounding image context.  The figure visually demonstrates the superior performance of the proposed 'Ours' method compared to two baselines ('Chained' and '3DIT').", "section": "4 Experiments"}, {"figure_path": "eDNslSwQIj/figures/figures_6_1.jpg", "caption": "Figure 1: 3D-aware editing with our Neural Asset representations. Given a source image and object 3D bounding boxes, we can translate, rotate, and rescale the object. In addition, we support compositional generation by transferring objects or backgrounds across images.", "description": "This figure shows examples of 3D-aware image editing using Neural Assets.  It demonstrates the ability to manipulate individual objects within a scene by translating, rotating, rescaling, replacing, or changing the background.  The figure highlights the model's capacity for fine-grained control and compositional generation.", "section": "1 Introduction"}, {"figure_path": "eDNslSwQIj/figures/figures_7_1.jpg", "caption": "Figure 6: Object translation and rotation by manipulating 3D bounding boxes on Waymo Open. See our project page for videos and additional object rescaling results.", "description": "This figure shows examples of object translation and rotation on the Waymo Open dataset.  The results demonstrate that by manipulating the 3D bounding boxes provided as input, the model can successfully translate and rotate objects within the scene.  The green boxes highlight the objects before and after the transformation. To see the changes more clearly, videos of these edits are available on the project's webpage.", "section": "4.3 Controllable Scene Generation"}, {"figure_path": "eDNslSwQIj/figures/figures_7_2.jpg", "caption": "Figure 7: Compositional generation results on Waymo Open. By composing Neural Assets, we can remove and segment objects, as well as transfer and recompose objects between scenes.", "description": "This figure shows several image editing results using the proposed Neural Asset method on the Waymo Open dataset.  The top row demonstrates the model's ability to reconstruct the original image, remove objects, segment objects, replace objects, and recompose objects from different scenes. The bottom row shows similar edits on another image. The results illustrate the versatility and control offered by the Neural Asset approach for complex scene manipulation.", "section": "Experiments"}, {"figure_path": "eDNslSwQIj/figures/figures_8_1.jpg", "caption": "Figure 8: Transfer backgrounds between scenes by replacing the background token on Waymo Open. The objects can adapt to new environments, e.g., the car lights are turned on at night.", "description": "This figure demonstrates the capability of the model to transfer backgrounds between different scenes. By replacing the background token, the objects in the foreground seamlessly integrate into the new background, adapting to lighting and other environmental changes.  The example shows how car headlights are correctly rendered when a nighttime background is applied.", "section": "4.3 Controllable Scene Generation"}, {"figure_path": "eDNslSwQIj/figures/figures_8_2.jpg", "caption": "Figure 9: Comparison of (a) visual encoders, (b) background modeling, and (c) training strategies on Objectron. Bold entry denotes our full model. See text for each variant. We report PSNR and LPIPS computed within object bounding boxes, and leave other metrics to Appendix B.2.", "description": "This figure shows the ablation study results on the Objectron dataset.  It compares different components of the Neural Assets model: visual encoders (CLIP, MAE, DINO, and fine-tuned DINO), background modeling (with and without background, with and without pose), and training strategies (single frame, single frame without positional encoding, and paired frames).  The results, measured using PSNR and LPIPS within object bounding boxes, demonstrate the effectiveness of the chosen components in the full model.", "section": "4.4 Ablation Study"}, {"figure_path": "eDNslSwQIj/figures/figures_9_1.jpg", "caption": "Figure 10: Failure case analysis. Our model mainly has two failure cases: (a) symmetry ambiguity, where the handle of the cup gets flipped when it rotates by 180 degrees; (b) camera-object motion entanglement, where the background also moves when we translate the foreground object. Both issues will likely be resolved if we train our Neural Assets model on more diverse data.", "description": "This figure shows two failure cases of the Neural Assets model. The first case demonstrates symmetry ambiguity where rotating an object by 180 degrees causes a flipped appearance (e.g., a cup's handle). The second case illustrates camera-object motion entanglement, where moving a foreground object also results in background movement.  These limitations suggest the need for training data with greater diversity to improve model robustness.", "section": "Limitations and future works"}, {"figure_path": "eDNslSwQIj/figures/figures_20_1.jpg", "caption": "Figure 11: More qualitative results on MOVi-E, Objectron, and Waymo Open.", "description": "This figure shows a qualitative comparison of the results obtained by three different methods (Chained, 3DIT, and Ours) on three different datasets (MOVi-E, Objectron, and Waymo Open) for the task of multi-object editing. Each row corresponds to one dataset, and shows the source image, the results of each method, and the target image. The green boxes in the images highlight the objects that are being edited. The results show that the proposed method (Ours) outperforms the baselines by maintaining better object identity and consistency, resulting in a more realistic image.", "section": "4 Experiments"}, {"figure_path": "eDNslSwQIj/figures/figures_22_1.jpg", "caption": "Figure 1: 3D-aware editing with our Neural Asset representations. Given a source image and object 3D bounding boxes, we can translate, rotate, and rescale the object. In addition, we support compositional generation by transferring objects or backgrounds across images.", "description": "This figure demonstrates the capabilities of the Neural Asset model for 3D-aware multi-object scene editing.  Starting from a source image with identified object bounding boxes, the model can precisely manipulate individual objects by translating, rotating, and rescaling them. Furthermore, it showcases the model's ability to perform compositional generation, enabling the transfer of objects or background elements between different images.", "section": "1 Introduction"}, {"figure_path": "eDNslSwQIj/figures/figures_22_2.jpg", "caption": "Figure 13: Transfer backgrounds between scenes by replacing the background token on Objectron. Note how the global camera viewpoint is adjusted to fit the foreground object. In addition, the generator is able to synthetic lighting effects such as shadows on the surfaces.", "description": "This figure shows the results of replacing the background token on the Objectron dataset. The model successfully adapts the foreground objects to new backgrounds, demonstrating an understanding of scene context and lighting.  The consistent object appearances and lighting effects in the generated images showcases the effectiveness of the Neural Asset framework in handling multi-object scenes.", "section": "4.3 Controllable Scene Generation"}, {"figure_path": "eDNslSwQIj/figures/figures_23_1.jpg", "caption": "Figure 14: Illustration of our object pose representation (a) and two examples (b). We project four corners P0, P1, P2, P3 of a 3D bounding box to the 2D image plane and concatenate them to obtain the pose token. The projected four corners form a local coordinate system of the object.", "description": "This figure illustrates how the 3D pose of an object is represented using its projected corners in the image plane.  The four projected corners (P0, P1, P2, P3) form a local coordinate system for the object, capturing its position, orientation, and scale in the scene.  The example images (b and c) show how this representation translates to real-world scenes, where the green lines denote the projected corners and the object's pose.", "section": "3 Method: Neural Assets"}]