[{"figure_path": "dg3tI3c2B1/figures/figures_5_1.jpg", "caption": "Figure 2: Illustration of online learning LPT. For each shift iteration, we plot the densities of docking scores E using AutoDock-GPU. The increase of the docking scores indicates better binding affinity.", "description": "This figure shows the distribution of docking scores (E) obtained using AutoDock-GPU at each iteration of the online learning process for the Latent Prompt Transformer (LPT) model.  The x-axis represents the docking scores, and the y-axis implicitly represents the density of molecules with those scores. The color gradient represents the iteration number, showing how the distribution shifts towards higher docking scores (indicating better binding affinity) as the online learning progresses. This visualization demonstrates the model's ability to effectively improve the binding affinity of generated molecules over time.", "section": "3.4 Optimization via Online Learning with LPT"}, {"figure_path": "dg3tI3c2B1/figures/figures_8_1.jpg", "caption": "Figure 4: (a) Structure-constrained Optimization. Conditionally generated compounds C2 and C3 closely resemble the human-designed compounds C2 and C3 shown in Fig. 3. Additionally, the right column also presents further optimized compounds that achieve improved KD scores. (b) Illustration of generated molecules binding to PHGDH with docking poses generated by AutoDock-GPU. The left panel visualizes the molecule generated through multi-objective optimization, while the right panel displays the molecule generated via structure-constrained optimization.", "description": "This figure demonstrates the results of structure-constrained optimization and multi-objective optimization for PHGDH.  Part (a) shows how the model generates molecules (C2 and C3) similar to human-designed ones, and further optimizes them to achieve improved KD scores (a measure of binding affinity). Part (b) visualizes the binding poses of generated molecules to the PHGDH protein using AutoDock-GPU.  The left panel showcases the molecule from multi-objective optimization, while the right shows one from structure-constrained optimization.", "section": "5.2 Binding Affinity Maximization"}, {"figure_path": "dg3tI3c2B1/figures/figures_18_1.jpg", "caption": "Figure 5: PHGDH with NAD binding site.", "description": "This figure shows the binding site of NAD+ to the Phosphoglycerate dehydrogenase (PHGDH) enzyme.  Key residues involved in the interaction are highlighted, including hydrophobic residues (P176, Y174, L151, L193, L216, T213, T207, L210) and interactions with the nicotinamide moiety (A285, C233, D259), sugar moieties (T206, D174), and phosphate linker (R154, I155).  The figure illustrates the complex three-dimensional interactions that contribute to the binding affinity of NAD+ to PHGDH.", "section": "A.5 Background of PHGDH and its NAD binding site"}, {"figure_path": "dg3tI3c2B1/figures/figures_19_1.jpg", "caption": "Figure 8: Molecules produced during the multi-objective optimization for PHGDH. The legends denote KD (nM) \u2193, SA\u2193 and QED\u2191.", "description": "This figure shows 20 molecules generated by the Latent Prompt Transformer (LPT) model during a multi-objective optimization process targeting the Phosphoglycerate dehydrogenase (PHGDH) protein.  The optimization aimed to minimize the dissociation constant (KD, a measure of binding affinity), minimize the synthetic accessibility score (SA), and maximize the quantitative estimate of drug-likeness (QED). Each molecule is depicted with its corresponding KD (nM), SA, and QED values.  The figure illustrates the model's capability to discover diverse molecules with favorable properties.", "section": "5.2 Binding Affinity Maximization"}, {"figure_path": "dg3tI3c2B1/figures/figures_19_2.jpg", "caption": "Figure 4: (a) Structure-constrained Optimization. Conditionally generated compounds C2 and C3 closely resemble the human-designed compounds C2 and C3 shown in Fig. 3. Additionally, the right column also presents further optimized compounds that achieve improved KD scores. (b) Illustration of generated molecules binding to PHGDH with docking poses generated by AutoDock-GPU. The left panel visualizes the molecule generated through multi-objective optimization, while the right panel displays the molecule generated via structure-constrained optimization.", "description": "This figure shows the results of structure-constrained optimization and multi-objective optimization for PHGDH.  (a) shows that the model can generate molecules similar to those designed by humans and even improve upon them. (b) illustrates the docking poses of molecules generated by both methods, highlighting the differences in binding.", "section": "5.2 Binding Affinity Maximization"}, {"figure_path": "dg3tI3c2B1/figures/figures_20_1.jpg", "caption": "Figure 1: Left: Overview of Latent Prompt Transformer (LPT). The latent vector z \u2208 Rd is a neural transformation of zo, i.e., z = Ua(zo), where zo ~ N(0, Id). Given z, x and y are independent. PB(x|z) is the molecule generation model and py(y|z) predicts the property value or constraint based on z. Right: Illustration of molecule generation model pp(x|z). The latent vector z is used as a prompt in the p\u00df(x|z) via cross-attention.", "description": "The figure shows the architecture of the Latent Prompt Transformer (LPT) model.  The left panel illustrates the overall model, highlighting the three main components: a learnable prior for a latent vector (z), a molecule generation model (PB(x|z)) that uses the latent vector as a prompt, and a property prediction model (py(y|z)) that predicts the molecule's properties based on the latent vector. The right panel provides a detailed view of the molecule generation model, emphasizing the use of the latent vector (z) as a prompt via cross-attention in the causal Transformer architecture.", "section": "3 Latent Prompt Transformer"}, {"figure_path": "dg3tI3c2B1/figures/figures_20_2.jpg", "caption": "Figure 1: Left: Overview of Latent Prompt Transformer (LPT). The latent vector z \u2208 Rd is a neural transformation of zo, i.e., z = Ua(20), where zo ~ N(0, Ia). Given z, x and y are independent. PB(x|z) is the molecule generation model and py(y|z) predicts the property value or constraint based on z. Right: Illustration of molecule generation model pp(x|z). The latent vector z is used as a prompt in the p\u1e9e(x|z) via cross-attention.", "description": "The figure shows the architecture of the Latent Prompt Transformer (LPT), a generative model for molecule design.  The left panel illustrates the overall model, highlighting three components: a latent vector z with a learnable prior, a molecule generation model p\u03b2(x|z) based on a causal Transformer that uses z as a prompt, and a property prediction model py(y|z) that estimates the target property values given the latent prompt. The right panel shows a zoomed-in view of the molecule generation model, illustrating how the latent vector z is used as a prompt via cross-attention.", "section": "3 Latent Prompt Transformer"}, {"figure_path": "dg3tI3c2B1/figures/figures_21_1.jpg", "caption": "Figure 1: Left: Overview of Latent Prompt Transformer (LPT). The latent vector z \u2208 Rd is a neural transformation of zo, i.e., z = Ua(zo), where zo ~ N(0, Id). Given z, x and y are independent. PB(x|z) is the molecule generation model and py(y|z) predicts the property value or constraint based on z. Right: Illustration of molecule generation model p\u00df(x|z). The latent vector z is used as a prompt in the p\u00df(x|z) via cross-attention.", "description": "The figure illustrates the architecture of the Latent Prompt Transformer (LPT), a generative model for molecule design.  The LPT consists of three main components: a latent vector with a learnable prior distribution (left panel), a molecule generation model based on a causal Transformer that uses the latent vector as a prompt (right panel), and a property prediction model that predicts the molecule's properties using the latent prompt.  The left panel shows how the latent vector z is generated from a neural transformation of Gaussian noise,  while the right panel shows how the latent vector is used as a prompt for the molecule generation model via cross-attention, guiding the autoregressive generation process.", "section": "3 Latent Prompt Transformer"}, {"figure_path": "dg3tI3c2B1/figures/figures_21_2.jpg", "caption": "Figure 1: Left: Overview of Latent Prompt Transformer (LPT). The latent vector z \u2208 Rd is a neural transformation of zo, i.e., z = Ua(zo), where zo ~ N(0, Id). Given z, x and y are independent. PB(x|z) is the molecule generation model and py(y|z) predicts the property value or constraint based on z. Right: Illustration of molecule generation model PB(x|z). The latent vector z is used as a prompt in the PB(x|z) via cross-attention.", "description": "The figure shows the architecture of the Latent Prompt Transformer (LPT) model.  The left panel depicts the overall model, showing the three main components: a latent vector with a learnable prior distribution (z), a molecule generation model based on a causal Transformer that uses the latent vector as a prompt (PB(x|z)), and a property prediction model that predicts the molecule's properties using the latent prompt (py(y|z)). The right panel zooms in on the molecule generation model, illustrating how the latent vector z is used as a prompt via cross-attention.", "section": "3 Latent Prompt Transformer"}, {"figure_path": "dg3tI3c2B1/figures/figures_22_1.jpg", "caption": "Figure 12: Molecules produced during the structure-constrained optimization from C1 to C2 for PHGDH. The legends denote KD(\u00b5M) \u2193, SA\u2193 and QED\u2191.", "description": "This figure shows molecules generated by a structure-constrained optimization process. Starting from a core structure (C1), the model generated molecules (C2) aiming to improve binding affinity (KD), while maintaining other desirable properties. The figure shows the generated molecules with their corresponding KD values, synthetic accessibility scores (SA), and quantitative estimate of drug-likeness (QED). Lower KD values indicate stronger binding affinity. Lower SA values indicate easier synthesis. Higher QED values indicate better drug-likeness. ", "section": "A.6.2 Structure-constrained Optimization"}, {"figure_path": "dg3tI3c2B1/figures/figures_22_2.jpg", "caption": "Figure 12: Molecules produced during the structure-constrained optimization from C1 to C2 for PHGDH. The legends denote KD(\u00b5M) \u2193, SA\u2193 and QED\u2191.", "description": "This figure shows molecules generated by structure-constrained optimization.  The optimization started with a core structure (C1) and aimed to generate molecules similar to a human-designed molecule (C2) while improving binding affinity to the PHGDH protein. KD represents the dissociation constant (lower is better), SA is the synthetic accessibility score (lower is better), and QED is a measure of drug-likeness (higher is better).  The figure visually demonstrates the structural similarity between the generated molecules and the target molecule (C2), highlighting the success of the structure-constrained optimization approach.", "section": "A.6.2 Structure-constrained Optimization"}]