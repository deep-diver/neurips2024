[{"figure_path": "L86glqNCUj/figures/figures_8_1.jpg", "caption": "Figure 3: RMD comparisons between training regimes, for different values of N, at the end of an SI-initialized training for Ne epochs. Each column corresponds to a teacher with, respectively, an arbitrary, WI and SI distribution. Row 1 displays RMD\u00b2(vN, (vN)G) for the different regimes, in order to evaluate to what extent the training remained within EG. Row 2 displays the RMD between DA, FA and vanilla training regimes; and Row 3 does the same for each of them against EA.", "description": "This figure compares the performance of different training schemes (vanilla, data augmentation (DA), feature averaging (FA), and equivariant architectures (EA)) on a shallow neural network model. The training is initialized with strongly invariant (SI) distributions, and three different types of teachers (arbitrary, weakly invariant (WI), and SI) are used. The figure presents the relative measure distance (RMD) between the resulting particle distributions and their symmetrized or projected versions for different numbers of particles (N). It aims to show the effect of symmetry-leveraging techniques in preserving the symmetry of the parameter distribution during training.", "section": "4 Numerical experiments and architecture-discovery heuristic"}, {"figure_path": "L86glqNCUj/figures/figures_9_1.jpg", "caption": "Figure 2: Heuristic method applied on teacher (squares) and student (dots) particles. Row 1: aerial view of hyperplane EG. Row 2: parallel view, to verify student particles always remain in EG (red line). Column 1: step j = 0 after training; particles leave E0 = {0}. Column 2: initialization of step j = 1 on E\u2081 = (VE). Column 3: step j = 1 after training; particles leave E\u2081 (Row 1), but not EG.", "description": "This figure shows the application of the heuristic algorithm to discover the largest subspace of parameters supporting SI distributions. The algorithm iteratively trains a student neural network with parameters initialized in a subspace, and checks if the parameters remain within the subspace after training. If they do, the subspace is considered a potential EA parameter space. The figure shows the results for three iterations of the algorithm. The first column shows that the parameters escape from the trivial subspace. The second column shows the next iteration, and the third shows that the parameters do not escape the final subspace, which could be the best one.", "section": "4 Numerical experiments and architecture-discovery heuristic"}, {"figure_path": "L86glqNCUj/figures/figures_51_1.jpg", "caption": "Figure 3: RMD comparisons between training regimes, for different values of N, at the end of an SI-initialized training for Ne epochs. Each column corresponds to a teacher with, respectively, an arbitrary, WI and SI distribution. Row 1 displays RMD\u00b2(vN, (vN)) for the different regimes, in order to evaluate to what extent the training remained within EG. Row 2 displays the RMD between DA, FA and vanilla training regimes; and Row 3 does the same for each of them against EA.", "description": "This figure shows the results of multiple experiments with different numbers of particles (N) and training schemes (vanilla, DA, FA, EA).  The goal is to assess the impact of each training method on the distribution of learned parameters, particularly whether the distribution remains within the equivariant subspace EG.  The figure displays the Relative Measure Distance (RMD) between the final distributions obtained by each method, helping to quantify their similarity. The different columns represent different teacher models (arbitrary, WI, SI).", "section": "4 Numerical experiments and architecture-discovery heuristic"}, {"figure_path": "L86glqNCUj/figures/figures_52_1.jpg", "caption": "Figure 4: Visualization of the NN particles after training under the vanilla, DA, FA and EA schemes, for a single realization of the experiment. Squares represent the teacher particles (which are WI), dots represent the student particles, and the hyperplane is EG. The bigger plots show an aerial view of the global particle distribution after training; and the minor plots below them show a viewpoint at the level of (and parallel to) EG. The student particles were all initialized to be SI (and to coincide at initialization between the different schemes), and trained with equation (5) correspondingly applying the proper SL technique.", "description": "This figure visualizes the final positions of student NN particles after training with different symmetry-leveraging (SL) techniques: vanilla, data augmentation (DA), feature averaging (FA), and equivariant architectures (EA). The teacher particles are WI (weakly invariant). The student particles are initialized to be SI (strongly invariant) and the hyperplane represents EG (subspace of invariant parameters). The figure shows that the SI-initialized training with vanilla scheme stays within EG, while DA, FA, and EA schemes also stay within EG and converge toward teacher particles as the number of particles N increases.", "section": "4 Numerical experiments and architecture-discovery heuristic"}, {"figure_path": "L86glqNCUj/figures/figures_52_2.jpg", "caption": "Figure 2: Heuristic method applied on teacher (squares) and student (dots) particles. Row 1: aerial view of hyperplane EG. Row 2: parallel view, to verify student particles always remain in EG (red line). Column 1: step j = 0 after training; particles leave E0 = {0}. Column 2: initialization of step j = 1 on E\u2081 = (VE). Column 3: step j = 1 after training; particles leave E\u2081 (Row 1), but not EG.", "description": "This figure visualizes the heuristic algorithm proposed in the paper for discovering the largest subspace of parameters that support strongly invariant distributions.  The algorithm iteratively trains a neural network, starting from a subspace (E0) and checking if the training remains within that subspace or escapes. If it escapes, a new subspace (Ej+1) is constructed, extending the previous subspace until a subspace is found (EG) within which training consistently remains after the training iterations, despite its not being enforced explicitly. The figure shows this progression over three steps (j=0,1,2). The teacher particles (squares) are fixed, and the goal is to discover the parameters supporting SI distributions.", "section": "4 Numerical experiments and architecture-discovery heuristic"}, {"figure_path": "L86glqNCUj/figures/figures_52_3.jpg", "caption": "Figure 2: Heuristic method applied on teacher (squares) and student (dots) particles. Row 1: aerial view of hyperplane EG. Row 2: parallel view, to verify student particles always remain in EG (red line). Column 1: step j = 0 after training; particles leave E0 = {0}. Column 2: initialization of step j = 1 on E\u2081 = (VE). Column 3: step j = 1 after training; particles leave E\u2081 (Row 1), but not EG.", "description": "This figure visualizes the heuristic algorithm proposed in the paper for discovering the largest subspace of parameters supporting strongly invariant (SI) distributions.  It shows the evolution of student particles during training (dots), compared to teacher particles (squares), across three steps (columns). Each step involves training on a larger subspace, iteratively building towards the target subspace EG. The top row provides an aerial view, while the bottom row offers a side view to emphasize that student particles remain within the subspace EG (red line), even after leaving the initial subspaces.", "section": "4 Numerical experiments and architecture-discovery heuristic"}, {"figure_path": "L86glqNCUj/figures/figures_52_4.jpg", "caption": "Figure 4: Visualization of the NN particles after training under the vanilla, DA, FA and EA schemes,\nfor a single realization of the experiment. Squares represent the teacher particles (which are WI),\ndots represent the student particles, and the hyperplane is EG. The bigger plots show an aerial view\nof the global particle distribution after training; and the minor plots below them show a viewpoint at\nthe level of (and parallel to) EG. The student particles were all initialized to be SI (and to coincide at\ninitialization between the different schemes), and trained with equation (5) correspondingly applying\nthe proper SL technique.", "description": "This figure visualizes the positions of the neural network (NN) particles after training using four different methods: vanilla, data augmentation (DA), feature averaging (FA), and equivariant architectures (EA).  The teacher particles, represented as squares, have a weakly invariant (WI) distribution. Student particles, shown as dots, were initialized with a strongly invariant (SI) distribution. The plots illustrate the particle distributions for each training method, showing both an aerial view and a side view parallel to the EG hyperplane. The side views provide a clearer illustration of how close the particle distributions are to EG.", "section": "Numerical experiments and architecture-discovery heuristic"}, {"figure_path": "L86glqNCUj/figures/figures_53_1.jpg", "caption": "Figure 4: Visualization of the NN particles after training under the vanilla, DA, FA and EA schemes,\nfor a single realization of the experiment. Squares represent the teacher particles (which are WI),\ndots represent the student particles, and the hyperplane is EG. The bigger plots show an aerial view\nof the global particle distribution after training; and the minor plots below them show a viewpoint at\nthe level of (and parallel to) EG. The student particles were all initialized to be SI (and to coincide at\nintialization between the different schemes), and trained with equation (5) correspondingly applying\nthe proper SL technique.", "description": "This figure visualizes the positions of student and teacher particles in a 4D parameter space after training with four different methods: vanilla, DA, FA, and EA.  The training used an SI initialization and a WI teacher. The top row shows a 3D projection of the 4D parameter space and the bottom row shows a 2D projection emphasizing the hyperplane representing the SI parameter subspace. The figure demonstrates how the different methods result in different particle distributions, with the vanilla method resulting in particles spread out more than the other methods which leverage symmetry.", "section": "4 Numerical experiments and architecture-discovery heuristic"}, {"figure_path": "L86glqNCUj/figures/figures_53_2.jpg", "caption": "Figure 2: Heuristic method applied on teacher (squares) and student (dots) particles. Row 1: aerial view of hyperplane EG. Row 2: parallel view, to verify student particles always remain in EG (red line). Column 1: step j = 0 after training; particles leave E0 = {0}. Column 2: initialization of step j = 1 on E\u2081 = (VE). Column 3: step j = 1 after training; particles leave E\u2081 (Row 1), but not EG.", "description": "This figure visualizes the heuristic algorithm for discovering the largest subspace of parameters supporting SI distributions. It shows the positions of teacher and student particles during the algorithm's iterations. The red line indicates when the student particles escape the subspace. The results suggest that the algorithm can successfully discover the largest subspace.", "section": "4 Numerical experiments and architecture-discovery heuristic"}, {"figure_path": "L86glqNCUj/figures/figures_53_3.jpg", "caption": "Figure 2: Heuristic method applied on teacher (squares) and student (dots) particles. Row 1: aerial view of hyperplane EG. Row 2: parallel view, to verify student particles always remain in EG (red line). Column 1: step j = 0 after training; particles leave E0 = {0}. Column 2: initialization of step j = 1 on E\u2081 = (VE). Column 3: step j = 1 after training; particles leave E\u2081 (Row 1), but not EG.", "description": "This figure visualizes the heuristic algorithm for discovering EA parameter spaces.  It shows the positions of teacher and student particles in a 3D parameter space (Z) during an iterative process. The algorithm aims to find the largest subspace (EG) of Z that supports strongly-invariant (SI) distributions. The figure shows three steps (columns) of the process.  In each step, the student particles are trained (using a specific technique) and their positions are plotted.  The red line in the bottom row indicates the subspace (EG) being sought. The figure demonstrates that the algorithm successfully discovers EG, even though the student particles initially move outside of the desired subspace during training.", "section": "4 Numerical experiments and architecture-discovery heuristic"}, {"figure_path": "L86glqNCUj/figures/figures_53_4.jpg", "caption": "Figure 4: Visualization of the NN particles after training under the vanilla, DA, FA and EA schemes, for a single realization of the experiment. Squares represent the teacher particles (which are WI), dots represent the student particles, and the hyperplane is EG. The bigger plots show an aerial view of the global particle distribution after training; and the minor plots below them show a viewpoint at the level of (and parallel to) EG. The student particles were all initialized to be SI (and to coincide at initialization between the different schemes), and trained with equation (5) correspondingly applying the proper SL technique.", "description": "This figure shows the visualization of the particle distribution of student networks trained using different methods (vanilla, DA, FA, EA). The teacher network has WI particles. The student networks were initialized with SI particles and trained using equation (5), applying the corresponding SL techniques. The figure shows both an aerial view of the particle distribution and a side view showing the projection onto the EG hyperplane. This visualization helps to understand how the different methods affect the learning process and how they approximate the teacher particle distribution.", "section": "4 Numerical experiments and architecture-discovery heuristic"}, {"figure_path": "L86glqNCUj/figures/figures_54_1.jpg", "caption": "Figure 3: RMD comparisons between training regimes, for different values of N, at the end of an SI-initialized training for Ne epochs. Each column corresponds to a teacher with, respectively, an arbitrary, WI and SI distribution. Row 1 displays RMD\u00b2(vN, (vN)) for the different regimes, in order to evaluate to what extent the training remained within EG. Row 2 displays the RMD between DA, FA and vanilla training regimes; and Row 3 does the same for each of them against EA.", "description": "This figure shows the results of comparing different training methods (vanilla, DA, FA, EA) for different numbers of particles (N) in a teacher-student setting, where the student is initialized with strongly invariant (SI) particles.  The three columns represent different teacher models: arbitrary, weakly invariant (WI), and strongly invariant (SI).  Row 1 shows how close the final student particle distribution remains to the subspace EG (the subspace of parameters defining equivariant architectures) for each training method. Rows 2 and 3 show pairwise comparisons of the relative measure distances (RMD) between training methods.", "section": "4 Numerical experiments and architecture-discovery heuristic"}, {"figure_path": "L86glqNCUj/figures/figures_54_2.jpg", "caption": "Figure 3: RMD comparisons between training regimes, for different values of N, at the end of an SI-initialized training for Ne epochs. Each column corresponds to a teacher with, respectively, an arbitrary, WI and SI distribution. Row 1 displays RMD\u00b2(vN, (vN)) for the different regimes, in order to evaluate to what extent the training remained within EG. Row 2 displays the RMD between DA, FA and vanilla training regimes; and Row 3 does the same for each of them against EA.", "description": "This figure displays the results of several experiments that compare different training schemes for neural networks.  The training schemes used are vanilla (no symmetry-leveraging), data augmentation (DA), feature averaging (FA), and equivariant architectures (EA).  The experiments are run with different numbers of particles (N) and three types of teacher models (arbitrary, weakly invariant (WI), and strongly invariant (SI)). The figure shows three key metrics: the relative measure distance (RMD) to the projected version of the distribution, the RMD between the different training schemes, and the RMD of each scheme versus the EA scheme.  The purpose is to evaluate the behavior of different symmetry-leveraging techniques under various conditions and assess their impact on the resulting model.", "section": "4 Numerical experiments and architecture-discovery heuristic"}, {"figure_path": "L86glqNCUj/figures/figures_55_1.jpg", "caption": "Figure 3: RMD comparisons between training regimes, for different values of N, at the end of an SI-initialized training for Ne epochs. Each column corresponds to a teacher with, respectively, an arbitrary, WI and SI distribution. Row 1 displays RMD\u00b2(vN, (vN)) for the different regimes, in order to evaluate to what extent the training remained within EG. Row 2 displays the RMD between DA, FA and vanilla training regimes; and Row 3 does the same for each of them against EA.", "description": "This figure shows the comparison of the performance of different training methods (vanilla, DA, FA, and EA) on the task of learning a teacher model under varying conditions (arbitrary, WI, and SI teacher models). The results are presented in terms of RMD which measures the distance between the learned model's parameters and their projected/symmetrized versions in the parameter space EG (i.e. how close the model's parameters are to exhibiting the desired symmetries). The top row shows the RMD between the learned model and its projected version, indicating how well the model learned to respect the symmetry. The bottom two rows compares RMD between different training techniques, showing how different methods evolve the model towards satisfying the symmetries of the teacher model.", "section": "4 Numerical experiments and architecture-discovery heuristic"}, {"figure_path": "L86glqNCUj/figures/figures_55_2.jpg", "caption": "Figure 3: RMD comparisons between training regimes, for different values of N, at the end of an SI-initialized training for Ne epochs. Each column corresponds to a teacher with, respectively, an arbitrary, WI and SI distribution. Row 1 displays RMD\u00b2(vN, (vN)) for the different regimes, in order to evaluate to what extent the training remained within EG. Row 2 displays the RMD between DA, FA and vanilla training regimes; and Row 3 does the same for each of them against EA.", "description": "This figure displays the results of several experiments that compare different training schemes of a shallow neural network. The goal is to compare vanilla training, with data augmentation (DA), feature averaging (FA), and equivariant architectures (EA). Three different teacher models are used: an arbitrary teacher, a weakly invariant (WI) teacher, and a strongly invariant (SI) teacher. The training starts from a strongly invariant initialization. The figure shows the relative measure distance (RMD) between the final student distribution and its projected version, which indicates how well the training remained within the invariant subspace EG. It also shows the RMD between the different training schemes and against EA. The experiments are performed for various numbers of particles N (5, 10, 50, 100, 500, 1000, 5000).", "section": "4 Numerical experiments and architecture-discovery heuristic"}, {"figure_path": "L86glqNCUj/figures/figures_57_1.jpg", "caption": "Figure 10: RMD comparison between the empirical student particle distribution, v, to both  P<sub>E<sub>j</sub></sub>#v and (v)<sup>G</sup> (where j is the heuristic step). These are performed at the beginning and the end of training on every fixed heuristic step. The red line is placed at the value 10<sup>-2</sup> and represents a possible threshold d<sub>j</sub>, to be used in the heuristic to determine whether training left E<sub>j</sub> or not.", "description": "This figure shows the results of applying the proposed heuristic algorithm for discovering EA parameter spaces.  It compares the relative measure distance (RMD) between the empirical distribution of student particles (v) and its projection onto the subspace E<sub>j</sub> (P<sub>E<sub>j</sub></sub>#v), as well as its symmetrized version ((v)<sup>G</sup>).  The red line represents a threshold (d<sub>j</sub>) for deciding if the training remained in E<sub>j</sub>.  The figure shows that for the first two steps of the heuristic, the distribution left the original subspace, while for the third step it remained within the subspace. This supports the heuristic's ability to discover EA parameter spaces.", "section": "4 Numerical experiments and architecture-discovery heuristic"}]