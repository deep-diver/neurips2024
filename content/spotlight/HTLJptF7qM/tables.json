[{"figure_path": "HTLJptF7qM/tables/tables_8_1.jpg", "caption": "Table 1: Average classification accuracy using machine annotators in CIFAR-10 and STL-10 datasets under different labeling scenarios. Bold black represents the best and blue represents the second best.", "description": "This table presents the average classification accuracy of different methods on CIFAR-10 and STL-10 datasets using machine-generated noisy labels.  The results are compared across three noise levels (High, Medium, Low) and several different methods for handling noisy labels.  The best-performing method for each scenario is highlighted in bold black, with the second-best highlighted in blue.  The table allows for a comparison of the effectiveness of different techniques in handling various levels of label noise in image classification tasks. ", "section": "5.1 Experiments with Machine Annotations"}, {"figure_path": "HTLJptF7qM/tables/tables_8_2.jpg", "caption": "Table 2: Average classification accuracy on CIFAR-10N, LabelMe, and ImageNet-15N datasets, labeled by human annotators. Bold black represents the best and blue represents the second best.", "description": "This table presents the average classification accuracy achieved by different methods on three real-world datasets (CIFAR-10N, LabelMe, and ImageNet-15N).  The datasets were annotated by human annotators, introducing real-world noise.  The table compares the performance of the proposed COINNet method against various baselines, including instance-dependent and instance-independent methods, and noise-robust loss function-based methods.  The results show the superior performance of COINNet, highlighting its robustness to noisy labels from human annotators.", "section": "5.2 Experiments Using Real Annotations"}, {"figure_path": "HTLJptF7qM/tables/tables_15_1.jpg", "caption": "Table 1: Average classification accuracy using machine annotators in CIFAR-10 and STL-10 datasets under different labeling scenarios. Bold black represents the best and blue represents the second best.", "description": "This table presents the average classification accuracy of different methods on CIFAR-10 and STL-10 datasets using machine annotations under different noise levels (high, medium, low).  It compares the proposed COINNet method against several baselines, including existing crowdsourcing methods and instance-dependent noisy learning approaches.  The results are presented for different noise levels to demonstrate the robustness of each method under varying amounts of label noise.  Bold black font highlights the best performing method for each scenario, while blue indicates the second-best.", "section": "5.1 Experiments with Machine Annotations"}, {"figure_path": "HTLJptF7qM/tables/tables_26_1.jpg", "caption": "Table 1: Average classification accuracy using machine annotators in CIFAR-10 and STL-10 datasets under different labeling scenarios. Bold black represents the best and blue represents the second best.", "description": "This table presents the average test accuracy of different noisy label learning methods on CIFAR-10 and STL-10 datasets using machine-generated noisy labels. The results are categorized by different noise levels (High, Medium, Low) and show the performance of the proposed COINNet method against several baselines. The best performing method for each scenario is highlighted in bold black, while the second-best is shown in blue.  The table allows for a comparison of the proposed COINNet model against various existing noisy label learning techniques under different levels of label noise.", "section": "5.1 Experiments with Machine Annotations"}, {"figure_path": "HTLJptF7qM/tables/tables_26_2.jpg", "caption": "Table 1: Average classification accuracy using machine annotators in CIFAR-10 and STL-10 datasets under different labeling scenarios. Bold black represents the best and blue represents the second best.", "description": "This table presents the average classification accuracy of different methods on CIFAR-10 and STL-10 datasets under three different noise levels (high, medium, low).  The methods compared include several end-to-end crowdsourcing methods, instance-dependent noisy learning approaches, and noise-robust loss function-based approaches. The table highlights the superior performance of COINNet (the proposed method) across various scenarios. For the baselines trained using single annotators, majority voting was used to obtain the final labels.", "section": "5.1 Experiments with Machine Annotations"}, {"figure_path": "HTLJptF7qM/tables/tables_28_1.jpg", "caption": "Table 6: Average classification accuracy on CIFAR-10 and STL-10 datasets using machine annotators; results are averaged over 3 random trials.", "description": "This table presents the average classification accuracy achieved by the proposed COINNet model and several baseline methods on the CIFAR-10 and STL-10 datasets.  The results are categorized by three different noise levels (High, Medium, Low) and show the performance of the COINNet model with various hyperparameter settings (\u00b51 and \u00b52). The table demonstrates the model's performance across different noise conditions, highlighting its robustness and effectiveness in noisy settings.", "section": "5.1 Experiments with Machine Annotations"}, {"figure_path": "HTLJptF7qM/tables/tables_28_2.jpg", "caption": "Table 7: Average classification accuracy on CIFAR-10 using synthetic annotators; results are averaged over 3 random trials.", "description": "This table presents the average classification accuracy achieved by different methods on the CIFAR-10 dataset using synthetic annotators. The results are averaged over three random trials.  The table shows the performance of COINNet under different parameter settings (\u03bc1 and \u03bc2) and noise rates (\u03c4 = 0.2 and \u03c4 = 0.4) with different levels of instance-dependent noise (\u03b7).", "section": "5.1 Experiments with Machine Annotations"}, {"figure_path": "HTLJptF7qM/tables/tables_28_3.jpg", "caption": "Table 8: Average classification accuracy over 3 random trials on real datasets.", "description": "This table presents the average classification accuracy of the proposed COINNet model and several baseline methods on three real-world datasets: CIFAR-10N, LabelMe, and ImageNet-15N.  The results are averaged over three random trials.  Different hyperparameter settings (\u03bc\u2081 and \u03bc\u2082) for COINNet are explored, demonstrating the model's robustness across various parameter configurations. The table highlights COINNet's superior performance compared to other methods on these challenging real-world noisy label datasets.", "section": "5.2 Experiments Using Real Annotations"}, {"figure_path": "HTLJptF7qM/tables/tables_29_1.jpg", "caption": "Table 9: Average classification accuracy v.s. missing rate on CIFAR-10 using synthetic annotators; M = 3, \u03c4 = 0.2, \u03b7 = 0.3; results are averaged over 3 random trials.", "description": "This table presents the average classification accuracy of three different methods (MaxMIG, GeoCrowdNet (F), and COINNet) on the CIFAR-10 dataset under varying missing rates (0.1, 0.2, 0.3, 0.4, and 0.5).  The experiment uses synthetic annotators with a fixed noise rate (\u03c4 = 0.2) and a proportion of outliers (\u03b7 = 0.3).  The results show COINNet's superior performance across all missing rates, highlighting its robustness to missing data.", "section": "H.2 Ablation Study"}, {"figure_path": "HTLJptF7qM/tables/tables_29_2.jpg", "caption": "Table 10: Average classification accuracy v.s. missing rate on CIFAR-10 using synthetic annotators; \u041c = 3, \u03c4 = 0.2, \u03b7 = 0.5; results are averaged over 3 random trials.", "description": "This table presents the average classification accuracy of three different methods (MaxMIG, GeoCrowdNet (F), and COINNet) on the CIFAR-10 dataset with varying missing rates (0.1 to 0.5). The experiment was conducted using synthetic annotators with a fixed noise rate (\u03c4 = 0.2) and outlier ratio (\u03b7 = 0.5). The results are averages over three random trials.", "section": "5.1 Experiments with Machine Annotations"}, {"figure_path": "HTLJptF7qM/tables/tables_30_1.jpg", "caption": "Table 11: Average classification accuracy in the scenario where each image of CIFAR-10 is labeled by only one randomly chosen annotator from M = 3 synthetic annotators; results are averaged over 3 random trials.", "description": "This table shows the average classification accuracy on the CIFAR-10 dataset when each image is labeled by only one randomly selected synthetic annotator out of three. The results are displayed for two different noise levels (\u03c4 = 0.2, \u03b7 = 0.3 and \u03c4 = 0.2, \u03b7 = 0.5), and for three different methods: MaxMIG, GeoCrowdNet (F), and COINNet (Ours). The table demonstrates that COINNet outperforms the other two methods across both noise levels, indicating its robustness to noisy labels generated by a single annotator.", "section": "5.1 Experiments with Machine Annotations"}, {"figure_path": "HTLJptF7qM/tables/tables_30_2.jpg", "caption": "Table 2: Average classification accuracy on CIFAR-10N, LabelMe, and ImageNet-15N datasets, labeled by human annotators. Bold black represents the best and blue represents the second best.", "description": "This table presents the average classification accuracy of different methods on three real-world datasets (CIFAR-10N, LabelMe, and ImageNet-15N) with human-provided noisy labels.  The results are compared across multiple methods, including several baselines and the proposed COINNet approach.  Bold black font indicates the best performing method for each dataset, while blue font indicates the second-best. The table showcases the performance of COINNet compared to other methods under real-world noisy annotation scenarios.", "section": "5.2 Experiments Using Real Annotations"}, {"figure_path": "HTLJptF7qM/tables/tables_30_3.jpg", "caption": "Table 13: Average classification accuracy with different initializations for the confusion matrices. We use machine annotations with the same setting as described in Sec. 5.1 in the manuscript. Results are averaged over 3 random trials.", "description": "This table presents the average classification accuracy for three different initialization strategies for the confusion matrices (Am's) in the COINNet model. The strategies are: initializing with an identity matrix, initializing using the GeoCrowdNet (F) after training 10 epochs and the setting used in the current experiments (close to an identity matrix). The results are for high, medium and low noise level scenarios from using machine annotations, averaged over three random trials.", "section": "5.1 Experiments with Machine Annotations"}]