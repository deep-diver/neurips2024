[{"figure_path": "owuEcT6BTl/figures/figures_1_1.jpg", "caption": "Figure 1: Concept Learning Geometry underlies emergence. (a) Top: A multimodal model learns to generate the concepts in the order of \u201castronaut\u201d, \u201chorse\u201d, and finally \u201criding\u201d as it scales up (adapted from Yu et al. [45]). Middle: \u201cblue square apple\u201d is generated in the order of \u201capple\u201d, \u201cblue\u201d, and \u201csquare\u201d (adapted from Li et al. [46]). Bottom: Despite its simplicity, our model trained on synthetic data shows concept learning dynamics where it first learns \u201cshape\u201d and then \u201ccolor\u201d. (b) Concept space is an abstract coordinate space where individual axes correspond to different concepts and a given point corresponds to a \u201cconcept class\u201d, i.e., a predefined collection of concepts (e.g., large blue circles on bottom left corner). Traversal along axes of the concept space yield change in a specific property of the sample (e.g., going from large blue circle to large red circle along object color axis). Trajectories show a model\u2019s dynamics in concept space for learning to generate classes shown in-distribution (blue nodes) versus out of distribution (pink/red nodes). As we show, dynamics in concept space are highly interpretable, enabling precise comments on which concepts the model learns first, why, and what order it follows. (c) Measuring how accurately a model generates samples from a given concept class, showing an order of concept learning: first background color is learned, then size, and then object color.", "description": "This figure demonstrates the concept of concept learning geometry and how it relates to the emergence of capabilities in generative models.  It shows that the order in which a model learns concepts (e.g., astronaut, horse, riding; apple, blue, square) is reflected in its learning trajectory in a multi-dimensional concept space. Each axis in this concept space represents an independent concept (e.g., shape, color, size), and the model's progress through the space reveals the order and speed of its learning.  The figure illustrates how the 'concept signal,' or the degree to which the data-generating process is sensitive to changes in a specific concept, influences learning speed and order.  Sudden changes in learning direction in concept space correspond to the emergence of 'hidden' capabilities\u2014the model is able to manipulate a concept internally, but this capability is not yet demonstrable via naive input prompting.", "section": "1 Introduction"}, {"figure_path": "owuEcT6BTl/figures/figures_3_1.jpg", "caption": "Figure 13: Different distributions in concept values result in different concept signal. (Left) The color separation between the classes is stronger than the size separations resulting in a stronger concept signal in the color dimension. (right) The size separation between the classes is stronger, thus resulting in a stronger concept signal in size", "description": "This figure shows how different distributions of concept values (color and size) affect the concept signal strength.  The left panel illustrates a scenario where the difference in color between classes is greater than the size difference, resulting in a stronger color concept signal. The right panel demonstrates the opposite: a larger size difference yields a stronger size concept signal. This difference in concept signal strength influences how quickly the model learns each concept.", "section": "3.1 Experimental and Evaluation Setup"}, {"figure_path": "owuEcT6BTl/figures/figures_4_1.jpg", "caption": "Figure 3: Concept signal determines learning speed. The speed of concept learning as an inverse of the time in gradient steps when the separation in color (left) and size (right) between different classes increases. Concept learning is faster when pixel differences among concept class and hence concepts are larger.", "description": "This figure shows the relationship between concept signal and learning speed in a generative model.  The left panel illustrates that as the color difference (distance) between classes increases, the speed at which the concept of color is learned increases. Similarly, the right panel shows this same trend for the size concept.  The faster learning speed is correlated with a larger pixel difference between the classes.", "section": "4.1 Concept Signal Determines Learning Speed"}, {"figure_path": "owuEcT6BTl/figures/figures_4_2.jpg", "caption": "Figure 4: Concept signal governs generalization dynamics. (a) Learning dynamics in the concept space for in-distribution concept class 00 (bottom left). (b) Learning dynamics for out-of-distribution (OOD) concept class 11 (top right). We plot the accuracy for color on the x-axis and size on the y-axis. The [0,1) normalized color concept signal level is color coded. Two trajectories for 01 and 10 are shown to illustrate concept memorization. See App. D.3 for uncertainties.", "description": "This figure visualizes the learning dynamics of a model in concept space, a coordinate system representing underlying concepts. Panel (a) shows the in-distribution learning trajectory for concept class 00, while panel (b) illustrates the out-of-distribution (OOD) learning for concept class 11. The x-axis represents color accuracy, and the y-axis represents size accuracy.  The color intensity corresponds to the concept signal strength, showing how sensitive the data generation process is to changes in the concept's value. The trajectories reveal how the model learns and generalizes, showing memorization effects (biased towards previously seen data) and a sudden transition to OOD generalization.", "section": "4.2 Concept Signal Governs Generalization Dynamics"}, {"figure_path": "owuEcT6BTl/figures/figures_5_1.jpg", "caption": "Figure 10: Underspecification hinders out-of-distribution (OOD) generalization. (a) The learning dynamics with varying levels of prompt masking, from 0% to 100%, and the generated images. At 0% masking (top right image), the model correctly produces an image of blue triangle from the prompt \"blue triangle.\" As the masking increases (from right to left), the images gradually shift towards the incorrect color, red. (b) The simulation of the learning dynamics under underspecification in concept space based on Eq. 2. Our toy model replicates a trained network's learning dynamics.", "description": "This figure shows how underspecification (masking words in prompts) affects a model's ability to generalize to out-of-distribution (OOD) examples. Panel (a) displays the generated images for different levels of masking, demonstrating a shift from correct blue triangles (no masking) to incorrect red triangles (100% masking). Panel (b) presents a simplified model that successfully replicates the learning dynamics observed in (a), providing a theoretical explanation for the effect of underspecification.", "section": "5 Effect of Underspecification on Learning Dynamics in Concept Space"}, {"figure_path": "owuEcT6BTl/figures/figures_6_1.jpg", "caption": "Figure 6: Emergence of hidden capabilities. We plot accuracy as a function of gradient steps for five different runs, using three different protocols for prompting the model to generate outputs for OOD concept classes. (a) The baseline, naive prompting protocol; (b) linear latent intervention, applied in the activation space; and (c) overprompting, akin to an intervention on the input space.", "description": "This figure shows the accuracy of generating out-of-distribution (OOD) samples over the number of gradient steps using three different prompting methods: naive prompting, linear latent intervention, and overprompting.  Each method's accuracy is plotted for five separate runs, demonstrating that while naive prompting may fail to elicit a hidden capability, linear latent intervention and overprompting reliably do so earlier in the training process. This suggests that the model acquires the capability to manipulate the relevant concepts before this capability is readily apparent using standard input prompting.", "section": "4.4 Sudden Transitions in Concept Learning Dynamics"}, {"figure_path": "owuEcT6BTl/figures/figures_7_1.jpg", "caption": "Figure 7: Validating our Findings on CelebA. (a) Concept space dynamics of generated images concepts Gender and With Hat. We find the generalization class (Female, With Hat) ends up near (Female, No Hat). (b) Compositional Generalization Accuracy when performing latent interventions vs. naive prompting. This rise of accuracy near 5 \u00d7 105 gradient steps clearly demonstrates that the model is capable of generalizing out-of-distribution, but is not performant when evaluated via naive prompting.", "description": "This figure validates the findings of the paper using the CelebA dataset. Panel (a) shows the concept space dynamics for the concepts \"Gender\" and \"With Hat.\" It demonstrates that the model initially memorizes concepts from the training data, exhibiting a bias toward certain concept combinations. However, as the training progresses, the model transitions to out-of-distribution generalization, with the generated images moving closer to the target class (Female, With Hat). Panel (b) quantitatively assesses the model's compositional generalization ability by comparing the accuracy achieved using latent interventions versus naive prompting. The results reveal a significant increase in accuracy near 500,000 gradient steps when latent interventions are used, showing the model's hidden capability to perform well on unseen data, which is not apparent under standard prompting.", "section": "4.5 Towards a Landscape Theory of Learning Dynamics"}, {"figure_path": "owuEcT6BTl/figures/figures_7_2.jpg", "caption": "Figure 8: Underspecification delays out-of-distribution (OOD) generalization. The number of gradient steps required to reach accuracy above 0.8, as the percentage of masked prompts increases. A higher proportion of masked prompts slows down the speed of concept learning.", "description": "This figure shows how the speed of learning a concept decreases as the percentage of masked prompts (representing underspecification) increases.  The y-axis shows the number of gradient steps required to reach 80% accuracy.  Underspecification hinders the model's ability to quickly disentangle and learn concepts. The slower learning is due to correlations introduced between concepts by the masked prompts, making it more difficult for the model to learn the separate concepts independently.", "section": "5 Effect of Underspecification on Learning Dynamics in Concept Space"}, {"figure_path": "owuEcT6BTl/figures/figures_8_1.jpg", "caption": "Figure 9: Underspecification and Concept Learning. (a) The state-of-the-art generative models [104] erroneously produces a red strawberry (top right corner) for the prompt \"yellow strawberry\". (b) Without underspecification in the training data, a model F accurately learns the concepts of shape and color, successfully generalizes to the unseen node blue triangle (leftmost). As masks are applied to the word red for the prompt red triangle, concept signal for triangle increasingly starts to correlate with the concept red. This causes the output images to change from blue to purple as the level of masking increases (panels left to right). Eventually, the color dimension for triangle collapses, biasing the model towards generating solely red triangles (rightmost).", "description": "This figure demonstrates the effect of underspecification on concept learning in a generative model.  Panel (a) shows an example of a state-of-the-art model failing to generate a yellow strawberry when prompted with \"yellow strawberry\", instead producing a red one. This highlights the issue of underspecification in real-world prompts.  Panel (b) uses a simplified model to illustrate how increasing levels of masking (removing the word \"red\" from \"red triangle\") causes the model's learning to become biased towards red, even when prompted with \"blue triangle\".  The results show that underspecification hinders the ability of the model to disentangle concepts and generalize correctly to unseen data.", "section": "5 Effect of Underspecification on Learning Dynamics in Concept Space"}, {"figure_path": "owuEcT6BTl/figures/figures_8_2.jpg", "caption": "Figure 10: Underspecification hinders out-of-distribution (OOD) generalization. (a) The learning dynamics with varying levels of prompt masking, from 0% to 100%, and the generated images. At 0% masking (top right image), the model correctly produces an image of blue triangle from the prompt \"blue triangle.\" As the masking increases (from right to left), the images gradually shift towards the incorrect color, red. (b) The simulation of the learning dynamics under underspecification in concept space based on Eq. 2. Our toy model replicates a trained network's learning dynamics.", "description": "This figure shows how underspecification in training data affects the model's ability to generalize to out-of-distribution (OOD) examples.  Panel (a) presents empirical results illustrating how increasing levels of prompt masking (removing the color word from the prompt) causes the model to generate images with increasingly incorrect colors, even though the correct color is specified in the prompt's remaining words.  Panel (b) displays results from a simplified mathematical model that mirrors the trends observed in the empirical results, providing further support for the hypothesis that underspecification hinders the learning process and generalization performance.", "section": "5 Effect of Underspecification on Learning Dynamics in Concept Space"}, {"figure_path": "owuEcT6BTl/figures/figures_9_1.jpg", "caption": "Figure 6: Emergence of hidden capabilities. We plot accuracy as a function of gradient steps for five different runs, using three different protocols for prompting the model to generate outputs for OOD concept classes. (a) The baseline, naive prompting protocol; (b) linear latent intervention, applied in the activation space; and (c) overprompting, akin to an intervention on the input space. Fig. 4. We hypothesize there is a phase change underlying this decomposition and the model acquires the capability to alter concepts at this point of phase change. We investigate this next.", "description": "This figure shows the accuracy of generating out-of-distribution (OOD) samples using three different prompting methods: naive prompting, linear latent intervention, and overprompting.  The results demonstrate that while naive prompting may not elicit the model's hidden capabilities, latent interventions and overprompting can successfully generate OOD samples. This supports the hypothesis that generative models possess hidden capabilities that are learned suddenly and consistently during training but may not be immediately apparent due to limitations of naive input prompting. This figure belongs to the \"Sudden Transitions in Concept Learning Dynamics\" section of the paper.", "section": "4.4 Sudden Transitions in Concept Learning Dynamics"}, {"figure_path": "owuEcT6BTl/figures/figures_18_1.jpg", "caption": "Figure 12: The Concept Space Framework. G is an invertible data-generating that maps process maps sampled vectors z ~ P(Z) (where Z C Rd) to an observation x \u2208 R\" (an image in this work). The concept space, S := {z|z ~ P(Z)}, is defined as a space of all possible concept vectors generated from a compositional prior P(Z). The mixing function M masks some concept variables, the masked concept variables are thus underspecified.", "description": "This figure illustrates the concept space framework.  It shows a data-generating process (G) that maps a vector z from a concept space (S) to an observation x (an image). The concept space has dimensions representing independent concepts like size, shape, color, and location.  A mixing function (M) then maps z and x to h and x, representing how some concepts might be underspecified in the conditioning information used to generate images.", "section": "3 Concept Space: A Framework for Analyzing Concept Learning Dynamics"}, {"figure_path": "owuEcT6BTl/figures/figures_19_1.jpg", "caption": "Figure 13: Different distributions in concept values result in different concept signal. (Left) The color separation between the classes is stronger than the size separations resulting in a stronger concept signal in the color dimension. (right) The size separation between the classes is stronger, thus resulting in a stronger concept signal in size.", "description": "This figure shows two examples of concept spaces with different concept signals. In the left panel, the color difference between classes is greater than the size difference, resulting in a stronger concept signal for color.  Conversely, in the right panel, the size difference between classes is greater than the color difference, leading to a stronger concept signal for size. This illustrates how variations in the data-generating process, specifically the distances between concept classes, impact the strength of the concept signal for each concept.", "section": "3.1 Experimental and Evaluation Setup"}, {"figure_path": "owuEcT6BTl/figures/figures_21_1.jpg", "caption": "Figure 14: Loss vs. Accuracy vs. Concept Space. Loss and accuracy do not always intuitively reflect what capabilities the model has acquired during training. However, as one can see in the rightmost panel, the point of sudden turn in concept space corresponds to when the capability has emerged, i.e., the moment when well-defined prompting protocols can elicit the desired output from the model, which we indicate using the pink star (50% capability).", "description": "This figure shows a comparison of three different metrics (loss, training accuracy, and test accuracy) plotted against the number of gradient steps during training. The leftmost panel displays the training loss in log scale which reveals the general training progress. The middle panel depicts the training and test accuracies showing the generalization ability of the model.  The rightmost panel shows the model's learning trajectory in the concept space. A sudden shift in the trajectory indicates the emergence of a hidden capability, a point where the model suddenly starts to generate correct outputs despite the lack of explicit signal from naive prompting. The pink star in the rightmost panel highlights the moment when the capability emerges.", "section": "4 Learning Dynamics in Concept Space"}, {"figure_path": "owuEcT6BTl/figures/figures_21_2.jpg", "caption": "Figure 4: Concept signal governs generalization dynamics. (a) Learning dynamics in the concept space for in-distribution concept class 00 (bottom left). (b) Learning dynamics for out-of-distribution (OOD) concept class 11 (top right). We plot the accuracy for color on the x-axis and size on the y-axis. The [0,1) normalized color concept signal level is color coded. Two trajectories for 01 and 10 are shown to illustrate concept memorization. See App. D.3 for uncertainties.", "description": "This figure shows the learning dynamics of a generative model in a concept space with 'color' and 'size' as axes. Panel (a) displays the model's learning trajectory for an in-distribution concept class (00), while panel (b) shows the trajectory for an out-of-distribution (OOD) class (11). The color of the trajectories represents the level of color concept signal, illustrating how the signal influences the learning process.  The figure demonstrates that concept memorization occurs before OOD generalization, showing a shift in learning dynamics. The plot highlights the role of concept signal in shaping the model's generalization capabilities and learning trajectory. ", "section": "4.2 Concept Signal Governs Generalization Dynamics"}, {"figure_path": "owuEcT6BTl/figures/figures_21_3.jpg", "caption": "Figure 4: Concept signal governs generalization dynamics. (a) Learning dynamics in the concept space for in-distribution concept class 00 (bottom left). (b) Learning dynamics for out-of-distribution (OOD) concept class 11 (top right). We plot the accuracy for color on the x-axis and size on the y-axis. The [0,1) normalized color concept signal level is color coded. Two trajectories for 01 and 10 are shown to illustrate concept memorization. See App. D.3 for uncertainties.", "description": "This figure shows the learning dynamics of a generative model in a 2D concept space (color and size). Panel (a) shows the learning trajectory for an in-distribution class (00), while panel (b) shows the learning trajectory for an out-of-distribution class (11). The color of the trajectory represents the color concept signal. The figure illustrates how concept signal influences the learning dynamics and how the model initially memorizes concepts before generalizing out-of-distribution. The uncertainty of the model is represented by color coding.", "section": "4.2 Concept Signal Governs Generalization Dynamics"}, {"figure_path": "owuEcT6BTl/figures/figures_22_1.jpg", "caption": "Figure 6: Emergence of hidden capabilities. We plot accuracy as a function of gradient steps for five different runs, using three different protocols for prompting the model to generate outputs for OOD concept classes. (a) The baseline, naive prompting protocol; (b) linear latent intervention, applied in the activation space; and (c) overprompting, akin to an intervention on the input space. Fig. 4. We hypothesize there is a phase change underlying this decomposition and the model acquires the capability to alter concepts at this point of phase change. We investigate this next.", "description": "This figure shows the accuracy of generating out-of-distribution (OOD) samples using three different prompting methods: naive prompting, linear latent intervention, and overprompting.  It demonstrates that while naive prompting may fail to elicit the model's hidden capabilities, latent interventions and overprompting can successfully generate the desired outputs much earlier in the training process. This suggests a two-stage learning process: first, the model acquires the latent capability, and second, it learns to map inputs to outputs.", "section": "4.4 Sudden Transitions in Concept Learning Dynamics"}, {"figure_path": "owuEcT6BTl/figures/figures_23_1.jpg", "caption": "Figure 7: Validating our Findings on CelebA. (a) Concept space dynamics of generated images concepts Gender and With Hat. We find the generalization class (Female, With Hat) ends up near (Female, No Hat). (b) Compositional Generalization Accuracy when performing latent interventions vs. naive prompting. This rise of accuracy near 5 \u00d7 105 gradient steps clearly demonstrates that the model is capable of generalizing out-of-distribution, but is not performant when evaluated via naive prompting.", "description": "This figure shows the results of experiments conducted on the CelebA dataset to validate the findings of the paper on a more realistic dataset.  Panel (a) displays the concept space dynamics for generating images based on the \"Gender\" and \"With Hat\" concepts. It reveals that even though the model learns to generate images of (Female, With Hat), the model's performance using naive prompting is suboptimal; the generated images cluster closer to (Female, No Hat). Panel (b) demonstrates improved generalization when using latent interventions rather than naive prompting, indicating that the model possesses hidden capabilities that are not readily apparent under naive prompting.", "section": "4.5 Additional Results on Realistic Data"}, {"figure_path": "owuEcT6BTl/figures/figures_23_2.jpg", "caption": "Figure 19: Concept Learning Geometry underlies emergence Success and Failure modes of out-of-distribution (OOD) generalization. (a) A success case where color carries the strongest concept signal. We see aspects of concept memorization from the class 011. (b) A failure case where the background color didn't generalize. In this case, the model does not produce the right background color while the concept space suggest that this capability is present.", "description": "This figure shows two examples of concept learning dynamics in a 3D concept space, illustrating both successful and unsuccessful out-of-distribution (OOD) generalization. In (a), the strong concept signal for color leads to successful OOD generalization, while in (b), the weaker concept signal for background color results in failure to generalize.  The trajectories in the 3D concept space visualize the model's learning process, revealing how concept learning order and OOD generalization are affected by concept signal strength.", "section": "4.3 Towards a Landscape Theory of Learning Dynamics"}, {"figure_path": "owuEcT6BTl/figures/figures_24_1.jpg", "caption": "Figure 10: Underspecification hinders out-of-distribution (OOD) generalization. (a) The learning dynamics with varying levels of prompt masking, from 0% to 100%, and the generated images. At 0% masking (top right image), the model correctly produces an image of blue triangle from the prompt \"blue triangle.\" As the masking increases (from right to left), the images gradually shift towards the incorrect color, red. (b) The simulation of the learning dynamics under underspecification in concept space based on Eq. 2. Our toy model replicates a trained network's learning dynamics.", "description": "This figure shows the effect of underspecification (masking words in prompts) on a model's ability to generalize to out-of-distribution data.  Panel (a) demonstrates that as more words are masked from the prompts (e.g., masking \"blue\" in \"blue triangle\"), the generated images increasingly shift towards an incorrect color (red). Panel (b) shows a simplified model which successfully recreates the same pattern of learning dynamics observed in the experimental results. This supports the idea that underspecification affects the learning process and generalization.", "section": "5 Effect of Underspecification on Learning Dynamics in Concept Space"}, {"figure_path": "owuEcT6BTl/figures/figures_24_2.jpg", "caption": "Figure 15: Concept Space Dynamics for all classes (00, 01, 10, 11). The experiment is identical as in Fig. 4. The [0,1) normalized color concept signal is color coded in every trajectory. Two training data trajectories are shown in gray in the last panel to illustrate concept memorization.", "description": "This figure shows the concept learning dynamics for all four classes (00, 01, 10, 11) in the concept space. The color of the trajectories represents the normalized concept signal, indicating the relative strength of the concept signal for color. The two gray trajectories are from the training set and illustrate how concept memorization occurs before generalization.", "section": "4 Learning Dynamics in Concept Space"}, {"figure_path": "owuEcT6BTl/figures/figures_25_1.jpg", "caption": "Figure 22: Embedding patching. We patch the embedding module (an MLP) used for transforming the conditioning information into an embedding that the model processes from the last checkpoint to intermediate checkpoints. Panel (a) shows the baseline accuracy for out-of-distribution (OOD) generalization across five different seed runs, while panel (b) shows accuracy achieved when the patched embedding module is used.", "description": "This figure shows the results of an experiment where the embedding module of a diffusion model was patched with an embedding module from an intermediate checkpoint during training. The goal was to determine if the model had already learned the ability to generate out-of-distribution (OOD) samples before naive prompting methods could elicit this ability.  Panel (a) shows the baseline accuracy for OOD generalization using naive prompting. Panel (b) shows the improvement in accuracy when using the patched embedding module, demonstrating that the model had already acquired the capability earlier than revealed by naive prompting.", "section": "D.7 Patching the embedding module"}, {"figure_path": "owuEcT6BTl/figures/figures_25_2.jpg", "caption": "Figure 4: Concept signal governs generalization dynamics. (a) Learning dynamics in the concept space for in-distribution concept class 00 (bottom left). (b) Learning dynamics for out-of-distribution (OOD) concept class 11 (top right). We plot the accuracy for color on the x-axis and size on the y-axis. The [0,1) normalized color concept signal level is color coded. Two trajectories for 01 and 10 are shown to illustrate concept memorization. See App. D.3 for uncertainties.", "description": "This figure visualizes the learning dynamics of a generative model in a concept space, where each axis represents a concept (e.g., color and size). Panel (a) shows the learning trajectory for an in-distribution class, while panel (b) shows the trajectory for an out-of-distribution class. The color coding represents the concept signal strength. The trajectories illustrate that the model undergoes a phase of memorization followed by a sudden transition to OOD generalization.", "section": "4.2 Concept Signal Governs Generalization Dynamics"}]