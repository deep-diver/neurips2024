[{"figure_path": "dkkgKzMni7/figures/figures_2_1.jpg", "caption": "Figure 1: Example of a one-dimensional manifold and its medial axis. Its reach is given by the minimum distance of the medial axis to the manifold.", "description": "This figure illustrates the concept of \"reach\" in a manifold.  A one-dimensional manifold (a curve) is shown, along with its medial axis (the set of points that have more than one closest point on the manifold). The reach is defined as the shortest distance between the manifold and its medial axis.  It provides a measure of the manifold's overall smoothness and is important for determining whether a manifold is easily sampleable (thus learnable in the context of this paper). A small reach indicates that the manifold is relatively smooth, while a large reach indicates that the manifold has sharper curves or more complex structure.", "section": "2 Background"}, {"figure_path": "dkkgKzMni7/figures/figures_4_1.jpg", "caption": "Figure 2: Learnability of neural networks depends on the regularity and smoothness properties of the input data manifold. In the efficiently sampleable regime corresponding to manifolds which can be approximated well with samples, neural networks are learnable via simple interpolation arguments. In the regime where manifolds are bounded solely by their curvature and intrinsic dimension, we show classes of manifolds that obstruct the learnability of algorithms. Real-world data likely lives in an intermediate regime with heterogeneous properties (e.g., manifolds with varying intrinsic dimension; see Section 4.2).", "description": "This figure summarizes the main findings of the paper regarding the learnability of neural networks trained on data drawn from different types of manifolds.  It highlights three regimes: efficiently sampleable manifolds (learnable), heterogeneous manifolds (potentially learnable), and manifolds with bounded curvature but unbounded volume (provably hard).  The figure visually represents these regimes with different manifold shapes and provides examples of each.", "section": "3 Learnability results"}, {"figure_path": "dkkgKzMni7/figures/figures_7_1.jpg", "caption": "Figure 3: (a) Learning is successful when inputs are drawn from a d = 10 intrinsic dimensional hypersphere living in ambient space of dimension n \u2013 an instance of the bounded positive curvature model in Proposition 3.5. Target functions are single hidden layer networks taken from the class of hard to learn functions in the Gaussian i.i.d. input model [36], which are no longer hard to learn in the input distribution considered here. (b) When the ambient dimension is large, learning algorithm struggles to learn a single hidden layer neural network drawn from the class of functions in the setting of Theorem 3.6 where the input data manifold has intrinsic dimension d = 1 and reach R = 0.5. The network trained to learn this target function is over-parameterized with respect to the target. Data is aggregated over five random realizations.", "description": "This figure shows the results of training neural networks on two different types of data manifolds: (a) shows a learnable manifold (efficiently sampleable), while (b) shows a provably hard manifold.  The results confirm the theoretical findings of the paper; neural networks learn efficiently on the learnable manifold, but struggle on the hard manifold when the ambient dimension is large.", "section": "Experiments"}, {"figure_path": "dkkgKzMni7/figures/figures_7_2.jpg", "caption": "Figure 3: (a) Learning is successful when inputs are drawn from a d = 10 intrinsic dimensional hypersphere living in ambient space of dimension n \u2013 an instance of the bounded positive curvature model in Proposition 3.5. Target functions are single hidden layer networks taken from the class of hard to learn functions in the Gaussian i.i.d. input model [36], which are no longer hard to learn in the input distribution considered here. (b) When the ambient dimension is large, learning algorithm struggles to learn a single hidden layer neural network drawn from the class of functions in the setting of Theorem 3.6 where the input data manifold has intrinsic dimension d = 1 and reach R = 0.5. The network trained to learn this target function is over-parameterized with respect to the target. Data is aggregated over five random realizations.", "description": "This figure shows the results of learning experiments on two types of manifolds: efficiently sampleable manifolds (learnable) and manifolds with bounded curvature but unbounded volume (hard to learn). The left subplot shows that learning is successful on efficiently sampleable manifolds, even when the target functions are those proven hard to learn under the i.i.d. Gaussian input model.  The right subplot shows that learning is significantly more difficult on manifolds with bounded curvature but unbounded volume as the ambient dimension increases, highlighting the hardness result.", "section": "Experiments"}, {"figure_path": "dkkgKzMni7/figures/figures_8_1.jpg", "caption": "Figure 2: Learnability of neural networks depends on the regularity and smoothness properties of the input data manifold. In the efficiently sampleable regime corresponding to manifolds which can be approximated well with samples, neural networks are learnable via simple interpolation arguments. In the regime where manifolds are bounded solely by their curvature and intrinsic dimension, we show classes of manifolds that obstruct the learnability of algorithms. Real-world data likely lives in an intermediate regime with heterogeneous properties (e.g., manifolds with varying intrinsic dimension; see Section 4.2).", "description": "This figure summarizes the main findings of the paper regarding the learnability of neural networks trained on data lying on manifolds.  The x-axis represents different regimes of manifolds categorized by their properties (efficiently sampleable, heterogeneous, provably hard). The y-axis implicitly represents the learnability of neural networks in each regime. Efficiently sampleable manifolds are those that can be well-approximated by a relatively small number of samples, rendering neural network training efficient. Heterogeneous manifolds represent real-world scenarios where manifold properties may vary across the data. Provably hard manifolds are those where learning is proven to be computationally difficult, even with relatively simple network architectures. The figure highlights that real-world data likely falls in the heterogeneous regime, where learnability remains an open question.", "section": "Learnability results"}, {"figure_path": "dkkgKzMni7/figures/figures_19_1.jpg", "caption": "Figure 6: Shape of constructed manifold M3.", "description": "This figure shows a 3D plot of a one-dimensional manifold M3 constructed using the method described in the paper. The manifold resembles a space-filling curve that wraps around the unit cube, visiting many of its corners.  It demonstrates a low-dimensional manifold embedded in a higher-dimensional space (3D in this case).  The shape illustrates the construction technique used in the paper to create manifolds with bounded curvature but unbounded volume. This is relevant to their study of the hardness of learning under manifold assumptions. The curve touches many quadrants of the unit hypercube, which is important in proving their hardness results.", "section": "C Space-filling manifold"}, {"figure_path": "dkkgKzMni7/figures/figures_25_1.jpg", "caption": "Figure 3: (a) Learning is successful when inputs are drawn from a d = 10 intrinsic dimensional hypersphere living in ambient space of dimension n \u2013 an instance of the bounded positive curvature model in Proposition 3.5. Target functions are single hidden layer networks taken from the class of hard to learn functions in the Gaussian i.i.d. input model [36], which are no longer hard to learn in the input distribution considered here. (b) When the ambient dimension is large, learning algorithm struggles to learn a single hidden layer neural network drawn from the class of functions in the setting of Theorem 3.6 where the input data manifold has intrinsic dimension d = 1 and reach R = 0.5. The network trained to learn this target function is over-parameterized with respect to the target. Data is aggregated over five random realizations.", "description": "This figure shows the results of experiments on learning neural networks with inputs sampled from two different types of manifolds.  (a) demonstrates successful learning when inputs are from a hypersphere with bounded positive curvature. (b) shows difficulty in learning when the inputs are from a manifold with bounded curvature and unbounded volume, especially as the ambient dimension increases.  The results confirm the theoretical findings about the relationship between manifold properties and learnability.", "section": "Experiments"}, {"figure_path": "dkkgKzMni7/figures/figures_25_2.jpg", "caption": "Figure 3: (a) Learning is successful when inputs are drawn from a d = 10 intrinsic dimensional hypersphere living in ambient space of dimension n \u2013 an instance of the bounded positive curvature model in Proposition 3.5. Target functions are single hidden layer networks taken from the class of hard to learn functions in the Gaussian i.i.d. input model [36], which are no longer hard to learn in the input distribution considered here. (b) When the ambient dimension is large, learning algorithm struggles to learn a single hidden layer neural network drawn from the class of functions in the setting of Theorem 3.6 where the input data manifold has intrinsic dimension d = 1 and reach R = 0.5. The network trained to learn this target function is over-parameterized with respect to the target. Data is aggregated over five random realizations.", "description": "This figure shows the results of the experiments conducted to verify the main findings of the paper.  (a) shows that neural networks are learnable when inputs come from an efficiently sampleable manifold (d=10 hypersphere in higher dimensional space).  (b) demonstrates that learning is hard when the input manifold has bounded curvature but unbounded volume (reach R=0.5, d=1).  The results confirm the theory developed in the paper.", "section": "Experiments"}, {"figure_path": "dkkgKzMni7/figures/figures_27_1.jpg", "caption": "Figure 2: Learnability of neural networks depends on the regularity and smoothness properties of the input data manifold. In the efficiently sampleable regime corresponding to manifolds which can be approximated well with samples, neural networks are learnable via simple interpolation arguments. In the regime where manifolds are bounded solely by their curvature and intrinsic dimension, we show classes of manifolds that obstruct the learnability of algorithms. Real-world data likely lives in an intermediate regime with heterogeneous properties (e.g. manifolds with varying intrinsic dimension; see Section 4.2).", "description": "This figure summarizes the main findings of the paper regarding the learnability of neural networks trained on data sampled from different types of manifolds.  It highlights three regimes: an efficiently sampleable regime where learnability is guaranteed, a provably hard regime where learning is difficult, and an intermediate heterogeneous regime representing real-world data, where learnability remains an open question. The figure visually represents these regimes and illustrates how the geometric properties of the manifold (smoothness, curvature, volume) impact the learnability of the neural network.", "section": "3 Learnability results"}, {"figure_path": "dkkgKzMni7/figures/figures_28_1.jpg", "caption": "Figure 2: Learnability of neural networks depends on the regularity and smoothness properties of the input data manifold. In the efficiently sampleable regime corresponding to manifolds which can be approximated well with samples, neural networks are learnable via simple interpolation arguments. In the regime where manifolds are bounded solely by their curvature and intrinsic dimension, we show classes of manifolds that obstruct the learnability of algorithms. Real-world data likely lives in an intermediate regime with heterogeneous properties (e.g., manifolds with varying intrinsic dimension; see Section 4.2).", "description": "This figure illustrates the relationship between the learnability of neural networks and the geometric properties of the input data manifold.  It shows that efficient learnability is possible for manifolds that can be well-approximated by samples (efficiently sampleable), using a simple interpolation argument. Conversely, for manifolds characterized only by curvature and intrinsic dimension bounds, there exist classes that make learning computationally hard.  Real-world data manifolds likely exhibit heterogeneous features, falling within an intermediate regime between these two extremes.", "section": "3 Learnability results"}]