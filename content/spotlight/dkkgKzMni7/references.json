{"references": [{"fullname_first_author": "E. Aamari", "paper_title": "Estimating the reach of a manifold", "publication_date": "2019-00-00", "reason": "This paper provides foundational definitions and theoretical results for manifold reach, a key concept used throughout the current paper's analysis."}, {"fullname_first_author": "E. Abbe", "paper_title": "SGD learning on neural networks: leap complexity and saddle-to-saddle dynamics", "publication_date": "2023-00-00", "reason": "This paper offers a relevant analysis of the complexity of training neural networks using stochastic gradient descent, a method closely related to the learning problems studied in the current paper."}, {"fullname_first_author": "S. Goel", "paper_title": "Superpolynomial lower bounds for learning one-layer neural networks using gradient descent", "publication_date": "2020-00-00", "reason": "This paper provides lower bounds for learning a specific type of neural network, which is highly relevant to the hardness results presented in the current work."}, {"fullname_first_author": "A. Daniely", "paper_title": "From local pseudorandom generators to hardness of learning", "publication_date": "2021-00-00", "reason": "This paper demonstrates a connection between the hardness of learning and pseudorandom generators, a concept foundational to cryptographic hardness arguments used in this paper."}, {"fullname_first_author": "C. Fefferman", "paper_title": "Testing the manifold hypothesis", "publication_date": "2016-00-00", "reason": "This paper provides a formal analysis of the manifold hypothesis, a key assumption underlying this paper's investigation into the learnability of neural networks."}]}