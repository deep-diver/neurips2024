[{"type": "text", "text": "Axioms for AI Alignment from Human Feedback ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Luise Ge Daniel Halpern Evi Micha Washington University in St. Louis Harvard University Harvard University g.luise@wustl.edu dhalpern@g.harvard.edu emicha@seas.harvard.edu ", "page_idx": 0}, {"type": "text", "text": "Ariel D. Procaccia Harvard University arielpro@g.harvard.edu ", "page_idx": 0}, {"type": "text", "text": "Itai Shapira Harvard University itaishapira@g.harvard.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In the context of reinforcement learning from human feedback (RLHF), the reward function is generally derived from maximum likelihood estimation of a random utility model based on pairwise comparisons made by humans. The problem of learning a reward function is one of preference aggregation that, we argue, largely falls within the scope of social choice theory. From this perspective, we can evaluate different aggregation methods via established axioms, examining whether these methods meet or fail well-known standards. We demonstrate that both the Bradley-Terry-Luce Model and its broad generalizations fail to meet basic axioms. In response, we develop novel rules for learning reward functions with strong axiomatic guarantees. A key innovation from the standpoint of social choice is that our problem has a linear structure, which greatly restricts the space of feasible rules and leads to a new paradigm that we call linear social choice. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The alignment of AI models with human values is widely recognized as a crucial task. A prominent method for this task, reinforcement learning with human feedback (RLHF), has been used in different applications, such as robotics [4, 17] and recommendations [28, 1]. Recently, RLHF has attracted significant attention as a tool for fine-tuning large language models (LLMs) [22, 32, 26]. A typical implementation of RLHF involves learning a reward model using a pre-trained LLM, which is then utilized to fine-tune an existing LLM. During the learning step, human feedback is provided in the form of ordinal comparisons, and a reward function is learned from these. The most common learning method assumes an underlying random utility model such as the Bradley-Terry-Luce (BTL) model [5, 22, 8] and computes a reward function that corresponds to a maximum likelihood estimator for the observed comparisons. ", "page_idx": 0}, {"type": "text", "text": "Is this the \u201cright\u201d way of aggregating individual preferences towards a socially desirable reward function? To answer this question, we draw on social choice theory, a field that studies collective decision making through a mathematical lens [6]. The maximum likelihood estimation approach is in line with a well-established body of work that assumes that different human participants have preferences stemming from noisy estimation of a common ground truth, and the goal is to learn this ground truth as accurately as possible [29]. But this is not the case when it comes to questions of AI alignment, where individuals can have legitimate differences of opinion rooted in different values or priorities. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "We argue that when preferences are truly heterogeneous, the axiomatic approach \u2014 which rose to prominence in social choice with the work of Arrow [2] \u2014 may be more suitable. This approach analyzes the desirability of aggregation methods by their satisfaction of certain axioms that capture notions of consensus, fairness, and economic efficiency. Specifically, we are interested in the axiomatic properties of aggregation methods that take ordinal preferences as input and output a reward function. We address the following two research questions: What axioms are satisfied by aggregation methods used by existing RLHF algorithms? And are there alternative aggregation methods that offer stronger axiomatic guarantees? ", "page_idx": 1}, {"type": "text", "text": "1.1 Our Approach ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In social choice theory, axioms are typically defined for rules that map rankings over candidates to a single winner (social choice functions) or a ranking of the candidates (social welfare functions). By contrast, we are interested in rules that assign a reward to each candidate. This gap is easy to bridge, though: we simply consider a ranking of the candidates by in descending reward order. ", "page_idx": 1}, {"type": "text", "text": "A much more significant gap is that in classical social choice, all relevant candidates appear in the input preferences, whereas in our setting (where candidates correspond, e.g., to prompts and their responses), we are only given preferences over a relatively small set of candidates identified by their (known) features, and we need to generalize from this information. In practice, this entails using a restricted\u2014commonly, parametric\u2014class of reward models which map candidate features to real-valued rewards, and which we fit to existing data. ", "page_idx": 1}, {"type": "text", "text": "Specifically, we assume that a linear reward function defined by a parameter vector determines the reward of each candidate by computing the inner product of the parameter vector and the feature vector of the candidate; these modeling choices are consistent with prior and concurrent work [31, 30, 15] and aim to capture the practice of RLHF.1 Each human participant (henceforth referred to as a voter) is associated with a parameter vector, which is unknown to us and is used to specify ordinal preferences over the candidates. Our task is to design linear rank aggregation rules, which aggregate rankings induced by these individual linear functions2into a collective ranking that is also induced by a linear function; this is a new paradigm in social choice, for which we coin the term linear social choice. ", "page_idx": 1}, {"type": "text", "text": "To evaluate linear rank aggregation rules, we adapt fundamental axioms from social choice theory [6]. The first is Pareto optimality $(P O)$ , which requires that if a candidate $a$ is ranked above candidate $b$ in every input ranking, then the resulting ranking should rank $a$ above $b$ . This is seen as a basic requirement and is satisfied by every standard voting method in the classical setting. ", "page_idx": 1}, {"type": "text", "text": "The second axiom is pairwise majority consistency (PMC): If there exists a reward function that generates a ranking where, for each pair of candidates, a majority of voters agree with the ranking, then the resulting ranking should match that ranking. This axiom is an extension of Condorcet consistency to rankings, and is satisfied by some, but not all, standard voting methods in the classical setting. ", "page_idx": 1}, {"type": "text", "text": "1.2 Our Results ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We start by examining, in Section 3.1, a family of loss-based rules that finds a ranking induced by a parameter vector that optimizes a measure of loss; this measure increases for every disagreement with a voter on a pair of alternatives, where the larger the difference in rewards, the larger the penalty. Crucially, by plugging in binary cross-entropy loss we can recover the BTL model. Our first main result is that whenever the loss function is weakly convex and nondecreasing, or strictly convex \u2014 conditions satisfied by binary cross-entropy loss, as well as, e.g., exponential and hinge loss \u2014 the corresponding rule fails both PMC and PO. This result suggests that the prevailing practice of RLHF is flawed from an axiomatic viewpoint. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "In Section 3.2, we take a first step towards addressing this shortcoming. We modify the loss-based formulation to focus on majority preferences rather than individual preferences. This modification defines a family of rules that are PMC, but we show that all of them fail PO by establishing an even stronger impossibility result: In stark contrast to the classical setting, any linear rank aggregation rule that depends only on majority preferences must fail PO. ", "page_idx": 2}, {"type": "text", "text": "In order to achieve both PO and PMC, we design (in Section 4) a linear rank aggregation rule that we call Leximax Copeland subject to PO. Not only does it satisfy our two main axioms, it also satisfies two additional ones, majority consistency and winner monotonicity. ", "page_idx": 2}, {"type": "text", "text": "To summarize, while widely applied rules fail to meet basic axioms, there are alternative methods that are desirable from this viewpoint. Our approach, therefore, provides a discriminative lens through which to evaluate RLHF methods and AI alignment methods more broadly. ", "page_idx": 2}, {"type": "text", "text": "1.3 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "During the eight months in which we have actively worked on this project (from September 2023 until May 2024) \u2014 and especially in the first few months of 2024 \u2014 a slew of independent, concurrent papers seeking to build bridges between social choice and RLHF have become publicly available [10, 12, 19, 30, 23, 7, 15, 27, 25]; this surge of interest points, in our view, to the importance of the agenda. ", "page_idx": 2}, {"type": "text", "text": "Three of those papers are position papers that conceptually support our work in that they discuss the possibility of applying an axiomatic approach to RLHF [10, 12, 19], although they do not provide any technical results. By contrast, existing technical papers on RLHF do not take an axiomatic approach. Of the concurrent technical papers, the one that is most closely related to ours is that of Siththaranjan et al. [25]. They show, among other results, that the ranking induced by the reward function that the MLE estimator of the Bradley-Terry-Luce Model returns follows the famous Borda count rule when unrestricted reward functions are allowed. In the classical setting, Borda count has strong axiomatic guarantees, including PO (but not PMC). However, it cannot be realized as a linear rank aggregation rule, and it is arguably impractical for RLHF. ", "page_idx": 2}, {"type": "text", "text": "Our work builds on an earlier study by Noothigattu et al. [21], which explores the axiomatic properties of reward functions defined as MLE estimators of underlying random utility models. The key difference is that their approach allows for general reward functions, not just linear ones, and they do not consider features at all. Unlike our findings, they show that the BTL model satisfies Pareto Optimality under these conditions. Additionally, they find that pairwise majority consistency is violated even without assuming linearity. However, their results strongly depend on varying the number of comparisons across different pairs of candidates. By contrast, our findings demonstrate that pairwise majority consistency is violated even when the number of comparisons is equal across all pairs of candidates. ", "page_idx": 2}, {"type": "text", "text": "2 The Linear Social Choice Model ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Let $C$ be a set of $m$ distinct prompt/responses, referred to as candidates, and let $V=\\{1,\\ldots,n\\}$ be a set of $n$ human participants, known as voters. We denote by $\\mathbb{R}^{d}$ the $d$ -dimensional real space in which both candidate feature vectors and chosen parameter vectors lie. ", "page_idx": 2}, {"type": "text", "text": "Each candidate $c\\in\\ C$ is associated with a distinct feature vector $\\mathbf{x}_{c}\\,\\in\\,\\mathbb{R}^{d}$ . A parameter vector $\\boldsymbol{\\theta}\\in\\mathbb{R}^{d}$ induces a linear reward function $r_{\\theta}:C\\mapsto R$ defined by taking the dot product with feature vectors $r_{\\theta}(c)=\\langle\\theta,\\mathbf{x}_{c}\\rangle$ . We will primarily be interested in how these parameterized functions rank the candidates by reward. Let $R^{a\\succ b}=\\{\\theta\\mid r_{\\theta}(a)\\geq r_{\\theta}(b)\\}$ be the region where the reward of $a$ is at least as large as that of $b$ . Note that $R^{a\\succ b}$ and $R^{b\\succ a}$ split $\\mathbb{R}^{d}$ into two half spaces, separated by the hyperplane orthogonal to $\\mathbf{x}_{a}-\\mathbf{x}_{b}$ . Parameter vectors $\\theta$ on the hyperplane have $r_{\\theta}(a)=r_{\\theta}(b)$ , while rankings in the interior of either half-space strictly rank one over the other. ", "page_idx": 2}, {"type": "text", "text": "For a ranking $\\sigma$ over the candidates, we say that $\\theta$ induces $\\sigma$ , denoted $\\theta\\triangleright\\sigma$ , if $a\\;\\succ_{\\sigma}\\;b$ implies $r_{\\theta}(a)\\,\\geq\\,r_{\\theta}({\\bar{b}})$ . Let $R^{\\sigma}\\,=\\,\\{\\theta\\,\\mid\\,\\theta\\triangleright\\sigma\\}$ be the set of vectors $\\theta$ that induce it. Note that this can be written as the intersection of corresponding half spaces $\\textstyle R^{\\sigma}\\,=\\,\\bigcap_{a,b:a\\succ_{\\sigma}b}R^{a\\succ b}$ . Further, the collection of $\\{R^{\\sigma}\\}$ essentially form a partition of $\\mathbb{R}^{d}$ , covering the space and intersecting only at their boundaries. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "We call a $\\theta$ non-degenerate if it is fully on one side of each of the separating hyperplanes, i.e., $r_{\\theta}(a)\\neq r_{\\theta}(b)$ for all $a,b\\in C$ . Non-degenerate parameter vectors lie in the interior of some $R^{\\sigma}$ , and thus induce exactly one ranking. We call $\\sigma$ feasible if $R^{\\sigma}$ has a nonempty interior, i.e., is induced by some nondegenerate $\\theta$ .3 ", "page_idx": 3}, {"type": "text", "text": "Each voter $i\\in V$ submits a ranking over the candidate $\\sigma_{i}$ . We assume that the feature space is rich enough that voter preferences can be captured via non-degenerate parameter vectors. In other words, we assume that each $\\sigma_{i}$ is feasible. We refer to the vector of voter rankings $\\pi=(\\sigma_{i})_{i\\in V}$ as a profile. Further, for two candidates $a,b$ , we write $n_{a\\succ b}(\\pi):=|\\{i\\in V\\mid a\\succ_{\\sigma_{i}}\\bar{b}\\}|$ for the number of voters that prefer $a$ to $b$ , and $w_{a\\succ b}(\\pi)=n_{a\\succ b}(\\pi)/n$ for the proportion of such voters. When the profile $\\pi$ is clear from context, we may shorten these to $n_{a\\succ b}$ and $w_{a\\succ b}$ , respectively. ", "page_idx": 3}, {"type": "text", "text": "We define a parameter aggregation rule as a function that takes as input a profile $\\pi$ and outputs a parameter vector $\\theta^{*}$ . Our goal is to design parameter aggregation rules such that $r_{\\theta^{*}}$ satisfies desirable properties with respect to the voter preferences. However, as the properties we care about will only be with respect to how $r_{\\theta^{*}}$ ranks the candidates, it will be more convenient to work with what we call linear rank aggregation rules that take as input a profile $\\pi$ and output a feasible ranking $\\sigma$ . There is a natural way to interpret a parameter aggregation rule as a linear rank aggregation rule, namely, output any feasible ranking induced by $\\theta^{*}$ . The exact properties of the parameter aggregation rule could in principle be sensitive to the tie-breaking of non-degenerate outputs, however, all of our results will be robust to such tie-breaking.4 ", "page_idx": 3}, {"type": "text", "text": "We pay special attention to a prominent family of rules from social choice theory referred to as $C1$ rules [14], whose outputs depend only on majority relationships, i.e., they only need to know for each pair of candidates $(a,b)$ whether the majority prefers $a$ or $b$ . ", "page_idx": 3}, {"type": "text", "text": "In our study, we examine several axioms borrowed from social choice theory to evaluate the reasonableness (fairness) of our aggregation mechanisms. These axioms include: ", "page_idx": 3}, {"type": "text", "text": "Definition 2.1 (Pareto Optimality). A linear rank aggregation rule $f$ satisfies Pareto optimality $i f,$ whenever every voter prefers candidate a over candidate $b$ on $\\pi$ , i.e., $w_{a\\succ b}(\\pi)=1_{\\!}$ , then candidate a is ranked higher than candidate $b$ in the output ranking, i.e., $a\\succ_{f(\\pi)}b$ . ", "page_idx": 3}, {"type": "text", "text": "Definition 2.2 (Pairwise Majority Consistency (PMC)). A ranking $\\sigma$ is called $a$ PMC ranking for profile $\\pi$ if for all $a,b\\in C$ , $a\\succ_{\\sigma}b$ if and only if a majority of voters rank $a\\succ_{\\sigma_{i}}b,$ i.e., $w_{a\\succ b}>1/2$ . $A$ linear rank aggregation rule satisfies PMC $i f,$ , when a PMC ranking $\\sigma$ exists for the input profile $\\pi$ and $\\sigma$ is feasible, then $f(\\pi)=\\sigma$ . ", "page_idx": 3}, {"type": "text", "text": "Note that a PMC ranking for each $\\pi$ need not exist, but when one does, it is unique. The words \u201c $\\sigma$ is feasible\u201d allude to the possibility that no non-degenerate parameter vector $\\theta$ induces the unique PMC ranking. Indeed, we have such an example; see Appendix B for details. ", "page_idx": 3}, {"type": "text", "text": "Our research question, then, is whether these axioms can be simultaneously satisfied by linear rank aggregation rules. Our approach seeks to provide a concrete illustration of how theoretical insights from social choice can inform practical algorithm design in RLHF. ", "page_idx": 3}, {"type": "text", "text": "3 Loss-Based Rules ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "3.1 Standard Loss Formulation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We begin our study of linear social choice by considering a quite broad yet natural class of rules that capture how RLHF is currently being done. Their core idea is the following: when considering parameter vector $\\theta$ , for each voter $i$ that ranks a pair of candidates $a\\succ_{i}b$ , we should incur some loss for giving $b$ a higher reward than $a$ . To formalize this, let $\\ell:\\mathbb{R}\\rightarrow\\mathbb{R}$ be a loss function, which we assume is nonnegative. We can then choose a parameter vector minimizing ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\theta;\\pi,\\ell)=\\sum_{a\\neq b\\in C}n_{a\\succ b}(\\pi)\\cdot\\ell(r_{\\theta}(b)-r_{\\theta}(a)).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Note that the BTL model fits within this framework using $\\ell(x)\\,=\\,\\ln(1+e^{x})$ , i.e., binary crossentropy loss.5 One caveat to this approach, however, is that an optimal $\\theta$ need not be well-defined: it is possible that no minimum is attained. Fortunately, since we only care about rankings induced by optimal parameter vectors, we can conveniently remedy this by saying the output is any ranking that is induced by parameter vectors that are arbitrarily close to optimal. More formally, we say that a linear rank aggregation rule $f$ minimizes $\\ell$ if for all $\\sigma=f(\\pi)$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\theta:\\theta\\in R^{\\sigma}}\\mathcal L(\\theta;\\pi,\\ell)=\\operatorname*{inf}_{\\theta}\\mathcal L(\\theta;\\pi,\\ell).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Even if no minimum is attained, there is always a choice of feasible ranking $\\sigma$ such that Equation (1) is satisfied. ", "page_idx": 4}, {"type": "text", "text": "With this definition in hand, we proceed to our first main result, which spells rather bad news for this class of rules: Any loss-based aggregation rule using a nondecreasing and convex loss function (of which BTL is one, and hinge loss is another) will fail our two core axioms, PMC and PO. This paints a negative picture for current RLHF methods with respect to their social choice guarantees. Note that we will exclude the discussion of loss functions with a global minimum at zero, like ReLU, because the loss minimizer will be zero, making all rankings vacuous consequently. And we have focused on convex loss functions due to their practical optimization ease. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.1. If a linear rank aggregation rule $f$ optimizes a loss function $\\ell$ that satisfies $\\operatorname*{inf}_{x}\\ell(x)\\,<\\,\\ell(0)$ and is either nondecreasing and weakly convex, or strictly convex (and possibly nonmonotone), then $f$ fails PMC and PO. ", "page_idx": 4}, {"type": "text", "text": "Proof. Fix a loss function $\\ell$ satisfying the theorem conditions. Note that since $\\ell$ is convex, we may also assume it is continuous [24, Corollary 10.1.1]. Furthermore, since $\\operatorname*{inf}_{x}\\ell(x)<\\ell(0)$ , we know that there exists $x~\\ne~0$ such that $\\ell(x)\\;<\\;\\ell(0)$ . The case where $x~>~0$ is relatively simple (as such loss functions lead to unnatural behavior), and we handle it at the end of the proof. For now, we assume that there exists $x\\,<\\,0$ such that $\\ell(x)\\,<\\,\\ell(0)$ . Note that this also implies that for all $y\\geq0$ , $\\ell$ is lower bounded by the affine linear function connecting $(x,\\ell(x))$ and $\\bar{(0,\\bar{\\ell}(0))}$ , and thus, $\\begin{array}{r}{\\operatorname*{lim}_{x\\rightarrow\\infty}\\ell(x)=\\infty.}\\end{array}$ . ", "page_idx": 4}, {"type": "text", "text": "We begin with a small instance of just three candidates $C^{c o r e}=\\{a,b,c\\}$ to gain some traction on how $\\ell$ behaves. We will later extend this instance with additional candidates to demonstrate a profile where PO and PMC fail. The candidates will have feature vectors $\\mathbf{x}_{a}:=(2,1)$ , $\\mathbf{x}_{b}:=(1,1)$ , and $\\mathbf{x}_{c}:=(0,0)$ , respectively. Furthermore, a $p$ -fraction of voters (for $p$ to be chosen later) will rank $a\\succ b\\succ c$ , while the remaining $\\left(1-p\\right)$ -fraction will have inverted preferences, ranking $c\\succ b\\succ a$ .6 ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}^{c o r e}(\\theta):=\\sum_{x\\neq y\\in C^{c o r e}}w_{x\\succ y}\\ell(r_{\\theta}(y)-r_{\\theta}(x))\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "with $w_{x\\succ y}\\in\\{1-p,p\\}$ be the loss function on this instance (scaling $n_{x\\succ y}$ down to $w_{x}{\\succ}y$ leads to an equivalent formulation). Let $g(x)=p\\cdot\\ell(-x)+(1-p)\\ell(x)$ . Note that we can rewrite $\\mathcal{L}^{c o m}$ as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}^{c o r e}(\\theta)=g(r_{\\theta}(a)-r_{\\theta}(b))+g(r_{\\theta}(a)-r_{\\theta}(c))+g(r_{\\theta}(b)-r_{\\theta}(c)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Note that $r_{\\theta}(c)=0$ for all $\\theta$ , so we can simplify this to ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}^{c o r e}(\\theta)=g(r_{\\theta}(a)-r_{\\theta}(b))+g(r_{\\theta}(a))+g(r_{\\theta}(b)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We will consider an unconstrained version of this problem where we are free to choose rewards $r_{a},r_{b}\\in\\mathbb{R}$ arbitrarily, and later show by which vectors $\\theta$ these optimal values can be induced. That is, we will first find $r_{a},r_{b}\\in\\mathbb{R}$ minimizing ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}^{u n c o n s t r}(r_{a},r_{b}):=g(r_{a}-r_{b})+g(r_{a})+g(r_{b}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Let $O P T^{c o r e}=\\{\\theta\\mid\\mathcal{L}^{c o r e}(\\theta)=\\operatorname*{inf}_{\\theta^{\\prime}}\\mathcal{L}^{c o r e}(\\theta^{\\prime})\\}$ and ${\\cal O P T}^{u n c o n s t r}\\{(r_{a},r_{b})\\mid{\\mathcal L}^{u n c o n s t r}(r_{a},r_{b})=$ $\\begin{array}{r l}&{\\operatorname*{inf}_{r_{a}^{\\prime},r_{b}^{\\prime}}\\mathcal{L}^{u n c o n s t\\bar{r}}\\big(r_{a}^{\\prime},r_{b}^{\\prime}\\big)\\big\\}}\\end{array}$ be the set of minimizers for these two loss functions. In Appendix A, we establish the following results about these optimal sets. ", "page_idx": 5}, {"type": "text", "text": "Lemma 3.2. There exists a rational $p\\,\\in\\,(1/2,1]$ and values $A_{1}<A_{2}$ with $A_{2}~>~0$ such that OPT unconstr is nonempty and for all $(r_{a},r_{b})\\in O P T^{u n c o n s t r}$ , $r_{a}>A_{2}$ and $r_{b}\\leq A_{1}$ . ", "page_idx": 5}, {"type": "text", "text": "Lemma 3.3. Suppose Lemma 3.2 holds for values $p,A_{1}$ and $A_{2}$ , then, for this same choice of $p_{i}$ , $O P T^{c o r e}$ is nonempty and there exist $A_{3}$ and $A_{4}$ with $A_{3}>0$ such that for all $(\\theta_{1},\\theta_{2})\\in O P T^{c o r e}$ , $\\theta_{1}>A_{3}$ and $\\theta_{2}<A_{4}$ . ", "page_idx": 5}, {"type": "text", "text": "We will now explicitly construct a family of instances with candidate feature vectors parameterized by a value $\\varepsilon\\in\\mathbb R$ such that for sufficiently small $\\varepsilon>0$ , the output of $f$ fails the two axioms. Fix $p,A_{3}$ and $A_{4}$ from Lemma 3.3, and choose $\\delta$ with $0<\\delta<1$ such that $\\delta A_{4}-A_{3}<0$ $\\mathit{\\dot{\\beta}}<A_{3}/A_{4}$ works if ${{A}_{4}}>0$ , and otherwise, any $0<\\delta<1$ will do). ", "page_idx": 5}, {"type": "text", "text": "Each instance will have six candidates, which we will think of as two groups of three, $C=C^{c o r e}\\cup$ $C^{c o p i e s}$ . The first group $C^{c o r e}\\,=\\,\\{a,b,c\\}$ will be the same as the three-candidate instance from above, while the second group $C^{c o p i e s}=\\{a^{\\prime},b^{\\prime},c^{\\prime}\\}$ will be new. The candidates $a,b,c$ will still be located at $\\mathbf{x}_{a}:=(2,1)$ , $\\mathbf{x}_{b}:=(1,1)$ , and $\\mathbf{x}_{c}:=(0,0)$ , respectively. The candidates $a^{\\prime},\\,b^{\\prime},\\,c^{\\prime}$ will be located near their undecorated counterparts at $\\mathbf{x}_{a^{\\prime}}:=\\mathbf{x}_{a}+(-\\varepsilon,0)$ , $\\mathbf{x}_{b^{\\prime}}:=\\mathbf{x}_{b}+(-\\varepsilon,0)$ and $\\mathbf{x}_{c^{\\prime}}:=\\mathbf{x}_{c}+\\left(-\\boldsymbol{\\varepsilon},\\boldsymbol{\\delta}\\cdot\\boldsymbol{\\varepsilon}\\right)$ . ", "page_idx": 5}, {"type": "text", "text": "Next, we describe the voter preferences. A $p$ -fraction of voters will have the ranking $a\\succ a^{\\prime}\\succ b\\succ$ $b^{\\prime}\\succ c^{\\prime}\\succ c$ , and the remaining $\\left(1\\!-\\!p\\right)$ -fraction of voters will have ranking $c^{\\prime}\\succ c\\succ b^{\\prime}\\succ b\\succ a^{\\prime}\\succ a$ . As long as $0<\\varepsilon<1$ (which will be the case for our final chosen $\\varepsilon$ ), these are both feasible rankings. The former is induced by the nondegenerate feature vector $(1,1)^{7}$ and the latter by $(-1,0)$ .8 ", "page_idx": 5}, {"type": "text", "text": "For each $\\varepsilon\\in\\mathbb{R}$ , let ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}^{\\varepsilon}(\\theta)=\\sum_{x\\neq y\\in C}w_{x\\succ y}\\ell(r_{\\theta}(y)-r_{\\theta}(x))\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "with $w_{x\\succ y}\\;\\in\\;\\{0,1\\,-\\,p,p,1\\}$ be the loss function we are optimizing using candidate locations parameterized by $\\varepsilon$ . ", "page_idx": 5}, {"type": "text", "text": "We will show that for sufficiently small $\\varepsilon\\,>\\,0$ , $\\operatorname*{inf}_{\\theta\\in R^{c^{\\prime}\\succ c}}\\mathcal{L}^{\\varepsilon}(\\theta)\\,>\\,\\operatorname*{inf}_{\\theta}\\mathcal{L}^{\\varepsilon}(\\theta)$ . This means that $f$ must output a ranking with $c\\succ c^{\\prime}$ . Observe that this is a PO violation because all voters agree that $c^{\\prime}\\succ c$ . Furthermore, this is a PMC violation because a majority of voters have the ranking $a\\succ a^{\\prime}\\succ b\\succ b^{\\prime}\\succ c^{\\prime}\\succ c,$ , yet this is not the output. ", "page_idx": 5}, {"type": "text", "text": "Let $O P T(\\varepsilon)$ be the set of vectors optimizing $\\mathcal{L}^{\\varepsilon}$ . The rest of the proof will follow from the following two lemmas, whose proofs are in Appendix A. ", "page_idx": 5}, {"type": "text", "text": "Lemma 3.4. $O P T(0)\\subseteq{\\overline{{R^{c^{\\prime}\\succ c}}}}$ . ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Lemma 3.5. Suppose $O P T(0)\\subseteq{\\overline{{R^{c^{\\prime}\\succ c}}}}$ , then, for sufficiently small $\\varepsilon>0$ , $\\operatorname*{inf}_{\\theta:\\theta\\in R^{c^{\\prime}\\succ c}}\\mathcal{L}^{\\varepsilon}(\\theta)>$ $\\operatorname*{inf}_{\\theta}{\\mathcal{L}}^{\\varepsilon}(\\theta)$ . ", "page_idx": 5}, {"type": "text", "text": "Finally, we handle the case that exists $x\\,>\\,0$ such that $\\ell(x)\\,<\\,\\ell(0)$ . Note that by convexity, this implies that for all $y\\,<\\,0$ , $\\ell(y)\\;>\\;\\ell(0)\\;>\\;\\ell(x)$ , so $\\operatorname{inf}_{y\\leq0}\\ell(y)\\,<\\,\\operatorname{inf}_{y}\\ell(y)$ . Now, consider an instance with two candidates $\\{a,b\\}$ located at $\\mathbf{x}_{a}\\;=\\;(1,\\overline{{0}})$ and $\\mathbf{x}_{b}\\,=\\,(0,1)$ , and a single voter ranking $a\\succ b$ (feasible via the parameter vector $(1,0);$ ). It is possible to achieve a loss of $\\ell(x)$ , e.g., by outputting the parameter vector $(0,x)$ . On the other hand, any $\\theta$ inducing the ranking $a\\succ b$ will be lower bounded by $\\ell(0)>\\ell(x)$ from above. Hence, $f$ must output $b\\succ a$ , which is both a PO and PMC violation. \u53e3 ", "page_idx": 6}, {"type": "text", "text": "3.2 Majority-Based Loss Formulation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Despite the negative results for loss-function-based rules, we may hope for a remedy using slightly different information. Specifically, we consider a similar loss-based function that rather than getting penalized for disagreeing with each voter only gets penalized if it disagrees with a majority of voters. That is, we choose $\\theta$ minimizing ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}^{m a j}(\\theta;\\pi,\\ell)=\\sum_{a\\neq b\\in C}\\mathbb{I}[w_{a\\succ b}(\\pi)>1/2]\\cdot\\ell(r_{\\theta(b)}-r_{\\theta(a)}).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Defining a parameter aggregation function based on this loss suffers from the same caveat as before, that in some cases no optimal $\\theta$ exists. Nevertheless, we can apply an analogous fix for a ranking variant. We say that a linear rank aggregation rule $f$ minimizes $\\ell$ in the majority formulation if for all $\\sigma=f(\\pi)$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\theta:\\theta\\in R^{\\sigma}}\\mathcal{L}^{m a j}(\\theta;\\pi,\\ell)=\\operatorname*{inf}_{\\theta}\\mathcal{L}^{m a j}(\\theta;\\pi,\\ell).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We first show (in Appendix A.5) that this does indeed help achieve PMC with essentially all loss functions. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.6. Fix a nondecreasing loss function $\\ell$ with $\\ell(0)>\\operatorname*{inf}_{x}\\ell(x)$ . If a linear rank aggregation rule $f$ minimizes $\\ell$ in the majority formulation, then $f$ satisfies PMC. ", "page_idx": 6}, {"type": "text", "text": "Note that if the $\\ell(0)>\\operatorname*{inf}_{x}\\ell(x)$ condition is not satisfied, i.e., $\\ell(0)=\\operatorname*{inf}_{x}\\ell(x)$ , then all linear rank aggregation rules $f$ minimize $\\ell$ in the majority formulation, so satisfying this is a vacuous condition. Indeed, the parameter vector 0 of all 0s achieves optimal loss of $\\ell(0)$ for each pair and is consistent with every ranking $\\sigma$ . Therefore, the condition $\\ell({\\bar{0}})>\\operatorname*{inf}_{x}\\ell(x)$ is as innocuous as possible to rule out these edge cases. ", "page_idx": 6}, {"type": "text", "text": "However, despite this good news for PMC, we show that this does not help in achieving PO. In fact, our negative result extends to every $C1$ linear rank aggregation rule. Note that if $f$ minimizing $\\ell$ in the majority formulation breaks ties consistently (i.e., if multiple feasible rankings are optimal, then it consistently chooses the same one), then it is C1. We then have the following result, whose proof is relegated to Appendix A.6. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.7. All C1 linear rank aggregation rules fail PO. ", "page_idx": 6}, {"type": "text", "text": "This result is quite unfortunate, because if there were a rule that is both $C1$ and PO, we would automatically achieve PMC: Whenever there is a feasible PMC ranking, a $C1$ rule cannot distinguish between this profile and a profile where all voters submit this ranking, hence, under the PO criterion, it must output it. Furthermore, whenever there is a PMC ranking, outputting it is necessarily PO, as for every pair, a majority of voters agree with the PMC ranking. Interestingly, in the proof, we construct a profile which has a PMC ranking, yet it is not feasible, and no matter how a $C1$ linear rank aggregation rule breaks ties, there is an underlying profile in which this output violates PO. ", "page_idx": 6}, {"type": "text", "text": "4 Social Choice-Based Rule ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In light of the above negative results, in this section, we ask whether there are linear rank aggregation rules that concurrently satisfy our two core axioms, PO and PMC. We answer this question affirmatively by presenting a new method based on a prominent rule from voting theory. ", "page_idx": 6}, {"type": "text", "text": "The Copeland rule assigns a Copeland score to each alternative equal to the number of other alternatives it beats in a pairwise competition, i.e., the score for $a$ is $|\\{b\\mid w_{a\\succ b}>1/2\\}|$ . It then ranks the candidates in descending order according to their Copeland scores (breaking ties arbitrarily). It is known that Copeland satisfies PO, PMC, and additional axiomatic properties. However, in linear social choice, since not every ranking is feasible, we cannot always output the Copeland ranking. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "We, therefore, define a new linear rank aggregation rule, which we call leximax Copeland. This rule chooses a feasible ranking as follows. It ranks first the candidate with the highest Copeland score that can be feasibly ranked first under some parameter vector $\\theta$ . Subject to this first position, it ranks second the candidate with the highest Copeland score which can be feasibly ranked second, and continues this process for subsequent positions. ", "page_idx": 7}, {"type": "text", "text": "Copeland\u2019s rule is a $C1$ rule because it only requires the majority relationships between the candidates. Analogously, leximax Copeland is also a $C1$ linear rank aggregation rule. Therefore, by Theorem 3.7, it does not satisfy the PO criterion. To address this issue, we define a variant called leximax Copeland subject to $P O\\ (L C P O)$ , which incorporates the PO criterion. Under LCPO, for every pair of alternatives where one dominates the other, the rule restricts rankings to place the dominating alternative above the dominated one. ", "page_idx": 7}, {"type": "text", "text": "The rule remains well-defined since the set of feasible rankings when enforcing the PO criterion is non-empty, as whenever $a$ dominates $b$ , all the rankings in the input profile rank $a$ above $b$ . Note that if the Copeland ranking is feasible, then this rule outputs that ranking, since unrestricted Copeland satisfies PO. ", "page_idx": 7}, {"type": "text", "text": "In addition to PO and PMC, we wish to show that LCPO satisfies two additional properties, which we define presently. ", "page_idx": 7}, {"type": "text", "text": "Definition 4.1 (majority consistency). A linear rank aggregation rule satisfies majority consistency if when a candidate a is ranked first by a majority of voters in the input profile, a is ranked first in the output ranking. ", "page_idx": 7}, {"type": "text", "text": "Majority consistency ensures that the collective decision reflects the preference of the majority when there is a clear favorite. This principle aligns with PMC, but specifically focuses on the majority\u2019s favorite alternative. However, as we discussed above, a PMC ranking does not necessarily exist, and even when it exists, it is not necessarily feasible. By contrast, when a majority winner exists, this candidate is necessarily ranked first by a majority of voters in the input profile, who themselves (by assumption) submit feasible rankings. Therefore, we need not handle the case where it is impossible to rank the majority winner first. ", "page_idx": 7}, {"type": "text", "text": "Definition 4.2 (winner monotonicity). A linear rank aggregation rule satisfies winner monotonicity if, when a candidate a is ranked first in the output ranking, elevating a in any voter\u2019s preference does not cause a to lose their top position in the updated aggregate ranking. ", "page_idx": 7}, {"type": "text", "text": "Winner monotonicity ensures that improving a leading candidate\u2019s position among individual voters will not result in that candidate\u2019s demotion. ", "page_idx": 7}, {"type": "text", "text": "We now state and prove the main result of this section. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.3. LCPO satisfies PO, PMC, majority consistency and winner monotonicity. ", "page_idx": 7}, {"type": "text", "text": "Proof. LCPO trivially satisfies PO since it always outputs a ranking that respects the PO criterion. Moreover, since Copeland satisfies PMC, and whenever Copeland\u2019s ranking is in the domain, leximax Copeland subject to PO returns this ranking, it clearly satisfies PMC. ", "page_idx": 7}, {"type": "text", "text": "Note that if an alternative $a$ is ranked first by at least half of the voters, then $a$ has the highest Copeland score, meaning that leximax Copeland subject to PO will rank this candidate first if this is possible. We see that this is indeed possible, by noticing that there is at least one feasible ranking in the input proflie where $a$ is ranked first, and any such input ranking is feasible (by assumption) and satisfies the PO requirement. Therefore, majority consistency is satisfied. ", "page_idx": 7}, {"type": "text", "text": "It remains to show that LCPO satisfies winner monotonicity. Suppose that on input profile $\\pi$ , the rule outputs a ranking $\\sigma$ where candidate $a$ is ranked first. Now, consider a profile $\\pi^{\\prime}$ which is similar to $\\pi$ with the only exception being a ranking in which $a$ is placed in a higher position. Let $S$ be the set of agents that ranked above $a$ in Copeland\u2019s ranking under $\\pi$ and let $S^{\\prime}$ be the set of agents that ranked above $a$ in Copeland\u2019s ranking under $\\pi^{\\prime}$ . Note that $S^{\\prime}\\subseteq S$ , since when moving from $\\pi$ to $\\pi^{\\prime}$ , only the Copeland score of $a$ can increase, and therefore it is not possible for a candidate $b$ to beat $a$ under $\\pi^{\\prime}$ but not under $\\pi$ . ", "page_idx": 7}, {"type": "text", "text": "Now, suppose that $R$ and $R^{\\prime}$ are the set of rankings that satisfy the PO criterion with respect to $\\pi$ and $\\pi^{\\prime}$ , respectively. We show that $R^{\\prime}\\subseteq R$ . First, note that since $a$ is ranked first under $\\pi$ , no alternative dominates $a$ in $\\pi$ , as otherwise the PO criterion would be violated. Therefore, we get that no other alternative dominates $a$ in $\\pi^{\\prime}$ as well. Moreover, note that if $b$ dominates $c$ in $\\pi$ , then this remains true in $\\pi^{\\prime}$ as well. On the other hand, it is possible that $a$ dominates an alternative $b$ in $\\pi^{\\prime}$ but not in $\\pi$ . From all of the above, we conclude that $R^{\\prime}\\subseteq R$ . ", "page_idx": 8}, {"type": "text", "text": "Since $a$ is ranked first in $\\sigma$ , we get that for every candidate $b\\in S$ , there is no ranking in $R$ in which $b$ is ranked first, since otherwise, LCPO would output such a ranking. This also means that for every candidate $b$ in $S^{\\prime}$ , there is no ranking in $R^{\\prime}$ in which $b$ is ranked first, since $R^{\\prime}\\subseteq R$ and $S^{\\prime}\\subseteq{\\dot{S}}$ . Moreover, note that every ranking in $R$ in which $a$ is ranked first is also in $R^{\\prime}$ since it satisfies all the PO restrictions of $\\pi^{\\prime}$ . Therefore, under $\\pi^{\\prime}$ , LCPO outputs a ranking in which $a$ is ranked first. ", "page_idx": 8}, {"type": "text", "text": "Leximax Copeland subject to PO can be implemented in polynomial time by solving $O(|C|^{2})$ relatively small linear programs. Specifically, given an input profile, we sequentially choose the candidate that is ranked in position $r+1$ as follows. We denote by $\\sigma_{r}$ the partial ranking, where the first $r$ positions have been fixed. For each candidate $c$ that has not been ranked yet, we want to check if there is a parameter vector that adheres to the partial ranking $\\sigma_{r}$ , respects the Pareto optimality criterion and ranks $c$ at position $r+1$ . Since all these constraints can be expressed as pairwise comparisons, we can use a linear program such as the one described in Footnote 4 to check if such a feasible ranking exists. Among the candidates meeting this criterion, we select the one with the highest Copeland score for position $r$ . ", "page_idx": 8}, {"type": "text", "text": "5 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We conclude with a discussion of several extensions and limitations of our approach and results. ", "page_idx": 8}, {"type": "text", "text": "First of all, we wish to emphasize that our results are theoretical. While they highlight some shortcomings of the current practice of RLHF, our goal was not to \u201coutperform\u201d existing RLHF methods. Rather, we see our model as giving a framework for understanding and comparing rules and methods \u2014 it is a (useful, we believe) lens through which researchers and engineers can examine their AI alignment methods. ", "page_idx": 8}, {"type": "text", "text": "Second, as written, our model has voters give their complete rankings, while in practice, this would be infeasible. In the real world, we are likely to elicit only relatively few pairwise comparisons per person. For our negative results, this assumption only makes them stronger: the BTL model fails both PO and PMC even with access to complete voter rankings. By contrast, for the positive results, specifically implementing leximax Copeland subject to PO, this ostensibly seems like a serious limitation. However, the complete rankings are not necessary for computing this rule, rather, all we need to know are PO dominance relationships and majority directions. We can therefore apply the rule whenever we can approximate this information, for example, through sampling. An alternative approach is to infer a complete ranking of each voter by fitting a parameter vector based on their pairwise responses; this process of learning a complete ranking and then running voting rules has been used before in a variety of settings [20, 18]. ", "page_idx": 8}, {"type": "text", "text": "Third, our work initiates the study of the axiomatic method in our linear social choice model. However, we leave open many questions about which axioms are compatible and finding rules that achieve them. It should be clear by now that the primary challenge in linear social choice is that not every ranking over the candidates can be output. This means that essentially all known aggregation rules cannot directly be used without at least some modification. A natural direction to tackle is to try to find methods of converting known voting rules into linear aggregation ones while maintaining some of their axiomatic properties. To this end, we conclude with some preliminary results, and somewhat surprising findings within this space. ", "page_idx": 8}, {"type": "text", "text": "Some rules which optimize over rankings can be naturally transformed. For example, consider the Kemeny rule, which returns the ranking with the smallest pairwise disagreement over all votes. This can easily be transformed to the linear setting by simply outputting the optimal feasible ranking. In fact, in Appendix C.2, we show that this rule carries over the property of separability,9 a social choice axiom that is violated by Copeland (in the classical setting) and leximax Copeland subject to ", "page_idx": 8}, {"type": "text", "text": "PO (in our setting). We show this in Appendix C.1. However, quite strikingly, although separability remains, this transformation makes Kemeny no longer PO (Appendix C.2). ", "page_idx": 9}, {"type": "text", "text": "Finally, note that the \u201cleximax\u201d portion of leximax Copeland can be seen as a general purpose tool for mapping traditional rules to linear aggregation rules. In Appendix C.3, we explore leximax plurality (run leximax on the ranking of candidates by plurality scores), and show that it satisfies majority consistency, winner monotonicity, and separability. Additionally, the \u201csubject to PO\u201d can be seen as another \u201ctool\u201d for enforcing the Pareto optimality criterion when a rule does not independently satisfy it. However, enforcing PO can again cause somewhat surprising results. For example, in Appendix C.2, we show that linear Kemeny subject to PO, while now trivially satisfying PO, again violates separability. These observations indicate the challenges inherent in linear social choice, and we hope these open questions inspire fruitful follow-up research. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research was partially supported by the National Science Foundation under grants IIS-2147187, IIS-2229881, CCF-2007080, IIS-1905558, and IIS-2214141; and by the Office of Naval Research under grants N00014-20-1-2488 and N00014-24-1-2704. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Nir Ailon and Mehryar Mohri. Preference-based learning to rank. Machine Learning, 80: 189\u2013211, 2010. [2] Kenneth Arrow. Social Choice and Individual Values. Wiley, 1951.   \n[3] Claude Berge. Topological Spaces. Oliver and Boyd, 1963. [4] Erdem B\u0131y\u0131k, Nicolas Huynh, Mykel J Kochenderfer, and Dorsa Sadigh. Active preferencebased Gaussian process regression for reward learning. arXiv preprint arXiv:2005.02575, 2020. [5] Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324\u2013345, 1952. [6] Felix Brandt, Vincent Conitzer, Ulle Endriss, Jer\u02c6ome Lang, and Ariel D. Procaccia, editors. Handbook of Computational Social Choice. Cambridge University Press, 2016. [7] Souradip Chakraborty, Jiahao Qiu, Hui Yuan, Alec Koppel, Furong Huang, Dinesh Manocha, Amrit Singh Bedi, and Mengdi Wang. MaxMin-RLHF: Towards equitable alignment of large language models with diverse human preferences. arXiv preprint arXiv:2402.08925, 2024. [8] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in Neural Information Processing Systems, 30, 2017. [9] F.H. Clarke. Optimization and Nonsmooth Analysis. Wiley New York, 1983.   \n[10] Vincent Conitzer, Rachel Freedman, Jobst Heitzig, Wesley H Holliday, Bob M Jacobs, Nathan Lambert, Milan Mosse\u00b4, Eric Pacuit, Stuart Russell, Hailey Schoelkopf, Emanuel Tewolde, and William S. Zwicker. Social choice for AI alignment: Dealing with diverse human feedback. arXiv preprint arXiv:2404.10271, 2024.   \n[11] Thomas M Cover. The number of linearly inducible orderings of points in $d_{\\cdot}$ -space. SIAM Journal on Applied Mathematics, 15(2):434\u2013439, 1967.   \n[12] Jessica Dai and Eve Fleisig. Mapping social choice theory to RLHF. arXiv preprint arXiv:2404.13038, 2024.   \n[13] H. Edelsbrunner. Algorithms in Combinatorial Geometry, volume 10 of EATCS Monographs on Theoretical Computer Science. Springer, 1987.   \n[14] Peter C Fishburn. Condorcet social choice functions. SIAM Journal on applied Mathematics, 33(3):469\u2013489, 1977.   \n[15] Luise Ge, Brendan Juba, and Yevgeniy Vorobeychik. Learning Linear Utility Functions From Pairwise Comparison Queries. arXiv preprint arXiv:2405.02612, 2024.   \n[16] Henry W Gould. A note on the number of linearly inducible orderings of points in $d_{\\cdot}$ -space. SIAM Journal on Applied Mathematics, 26(3):528\u2013530, 1974.   \n[17] Andras Kupcsik, David Hsu, and Wee Sun Lee. Learning dynamic robot-to-human object handover from human feedback. Robotics Research: Volume 1, pages 161\u2013176, 2018.   \n[18] Min Kyung Lee, Daniel Kusbit, Anson Kahng, Ji Tae Kim, Xinran Yuan, Allissa Chan, Daniel See, Ritesh Noothigattu, Siheon Lee, Alexandros Psomas, et al. Webuildai: Participatory framework for algorithmic governance. In Proceedings 22nd ACM Conference on ComputerSupported Cooperative Work and Social Computing,, pages 1\u201335, 2019.   \n[19] Abhilash Mishra. AI alignment and social choice: Fundamental limitations and policy implications. arXiv preprint arXiv:2310.16048, 2023.   \n[20] Ritesh Noothigattu, Snehalkumar Gaikwad, Edmond Awad, Sohan Dsouza, Iyad Rahwan, Pradeep Ravikumar, and Ariel Procaccia. A voting-based system for ethical decision making. In Proceedings of the 32th AAAI Conference on Artificial Intelligence, pages 1587\u20131594, 2018.   \n[21] Ritesh Noothigattu, Dominik Peters, and Ariel D Procaccia. Axioms for learning from pairwise comparisons. Advances in Neural Information Processing Systems, 33:17745\u201317754, 2020.   \n[22] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022.   \n[23] Chanwoo Park, Mingyang Liu, Kaiqing Zhang, and Asuman Ozdaglar. Principled RLHF from heterogeneous feedback via personalization and preference aggregation. arXiv preprint arXiv:2405.00254, 2024.   \n[24] R Tyrrell Rockafellar. Convex Analysis. Princeton University Press, 1970.   \n[25] Anand Siththaranjan, Cassidy Laidlaw, and Dylan Hadfield-Menell. Distributional preference learning: Understanding and accounting for hidden context in RLHF. arXiv preprint arXiv:2312.08358, 2023.   \n[26] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008\u20133021, 2020.   \n[27] Gokul Swamy, Christoph Dann, Rahul Kidambi, Zhiwei Steven Wu, and Alekh Agarwal. A minimaximalist approach to reinforcement learning from human feedback. arXiv preprint arXiv:2401.04056, 2024.   \n[28] Paolo Viappiani and Craig Boutilier. Optimal Bayesian recommendation sets and myopically optimal choice query sets. Advances in Neural Information Processing Systems, 23, 2010.   \n[29] H Peyton Young. Condorcet\u2019s theory of voting. The American Political Science Review, 82(4): 1231\u20131244, 1988.   \n[30] Huiying Zhong, Zhun Deng, Weijie J Su, Zhiwei Steven Wu, and Linjun Zhang. Provable multi-party reinforcement learning with diverse human feedback. arXiv preprint arXiv:2403.05006, 2024.   \n[31] Banghua Zhu, Michael Jordan, and Jiantao Jiao. Principled reinforcement learning with human feedback from pairwise or $k$ -wise comparisons. In Proceedings of the 40th International Conference on Machine Learning, pages 43037\u201343067, 2023.   \n[32] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 12}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 12}, {"type": "text", "text": "Justification: ", "page_idx": 12}, {"type": "text", "text": "Guidelines: ", "page_idx": 12}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 12}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 12}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 12}, {"type": "text", "text": "Justification: ", "page_idx": 12}, {"type": "text", "text": "Guidelines: ", "page_idx": 12}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 12}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 12}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 12}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 13}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 13}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 13}, {"type": "text", "text": "Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 13}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 13}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 14}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 14}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 14}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 14}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 15}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 15}, {"type": "text", "text": "Justification: ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 15}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 15}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: We discuss the potential for positive societal impacts. Negative societal impacts are implausible. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. \u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. ", "page_idx": 15}, {"type": "text", "text": "\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 16}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 16}, {"type": "text", "text": "Justification: ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 16}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 16}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 16}, {"type": "text", "text": "Justification: ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. ", "page_idx": 16}, {"type": "text", "text": "\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 16}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 17}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 17}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 17}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 17}, {"type": "text", "text": "A Deferred Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "A.1 Proof of Lemma 3.2 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof. We begin with some observations on $g$ . First, we have that since $\\ell$ is nonnegative, $g$ must also be nonnegative. This along with the fact that $\\begin{array}{r}{\\operatorname*{lim}_{x\\rightarrow\\infty}\\ell(x)=\\infty}\\end{array}$ , we have that both $\\operatorname*{lim}_{x\\to\\infty}g(x)=$ $\\infty$ and $\\begin{array}{r}{\\operatorname*{lim}_{x\\to-\\infty}g(x)\\stackrel{{}}{=}\\infty}\\end{array}$ . Together, these imply that $\\mathcal{L}^{u n c o n s t r}$ attains a minimum. Indeed, $\\mathcal{L}^{u n c o n s t r}(0,0)\\;=\\;3g(0)$ , and there is some bound $B$ such that for all $x~>~B$ and $x\\;<\\;-B$ , $g(x)\\;>\\;3g(0)$ . We can therefore restrict the optimization problem to $r_{a},r_{b}\\;\\in\\;[-B,B]$ without changing the solutions. Since $\\mathcal{L}^{u n c o n s t r}$ is continuous and $[-B,B]^{2}$ is compact, a minimum is attained. ", "page_idx": 18}, {"type": "text", "text": "Next, note that $g$ is convex because compositions of convex functions with monotonic functions and convex combinations of convex functions are convex [24]. From this, we claim that if there is an optimal solution $(r_{a},r_{b})$ , then $(r_{a},r_{a}/2)$ is also an optimal solution. Indeed, fix such an $(r_{a},r_{b})$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}^{u n c o n s t r}(r_{a},r_{a}/2)=g(r_{a}-r_{a}/2)+g(r_{a})+g(r_{a}/2)}\\\\ &{\\hphantom{\\mathcal{L}^{u n c o n s t r}(r_{a},r_{a}/2)}=g(r_{a})+2g(r_{a}/2)}\\\\ &{\\hphantom{\\mathcal{L}^{u n c o n s t r}(r_{a},r_{a}/2)}=g(r_{a})+2g(1/2(r_{a}-r_{b})+1/2r_{b})}\\\\ &{\\hphantom{\\mathcal{L}^{u n c o n s t r}(r_{a},r_{a}/2)}\\leq g(r_{a})+2(1/2g(r_{a}-r_{b})+1/2g(r_{b}))}\\\\ &{\\hphantom{\\mathcal{L}^{u n c o n s t r}(r_{a},r_{b})}=\\mathcal{L}^{u n c o n s t r}(r_{a},r_{b}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the inequality comes from convexity. This implies that $(r_{a},r_{a}/2)$ is also optimal. ", "page_idx": 18}, {"type": "text", "text": "By above, we have that if $(r_{b},r_{a})$ is optimal, it must be the case that $r_{a}$ minimizes ", "page_idx": 18}, {"type": "equation", "text": "$$\nh(r_{a}):=2g(r_{a}/2)+g(r_{a}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Observe that $h$ is again convex by monotonic composition and convex combinations. ", "page_idx": 18}, {"type": "text", "text": "Next, we will make use of the following facts about convex functions. Although they need not be differentiable, right- and left-hand derivatives always exist. For a function $k$ these are defined as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{k_{+}^{\\prime}(x)=\\displaystyle\\operatorname*{lim}_{h\\rightarrow0^{+}}\\frac{k(x+h)-k(x)}{h}}}\\\\ {{k_{-}^{\\prime}(x)=\\displaystyle\\operatorname*{lim}_{h\\rightarrow0^{-}}\\frac{k(x+h)-k(x)}{h}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Further, if $k$ is convex, we have that [24]: ", "page_idx": 18}, {"type": "equation", "text": "$$\nk_{-}^{\\prime}(x)\\leq k_{+}^{\\prime}(x)\\quad\\mathrm{for~every}\\;x,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "They also follow standard linearity and chain rule properties, which allow for simpler computation. For example: ", "page_idx": 18}, {"type": "text", "text": "if $k(x)=a\\alpha(x)+b\\beta(x)$ , then $k_{+}^{\\prime}(x)=a\\alpha_{+}^{\\prime}(x)+b\\beta_{+}^{\\prime}(x)$ , if $k(x)=\\alpha(\\gamma x)$ , then $k_{+}^{\\prime}(x)=\\gamma\\alpha_{+}^{\\prime}(\\gamma x)$ if $\\gamma\\geq0$ , and $k_{+}^{\\prime}(x)=\\gamma\\alpha_{-}^{\\prime}(\\gamma x)$ if $\\gamma<0$ , ", "page_idx": 18}, {"type": "text", "text": "(all of these hold for left-hand derivatives by swapping the positions of $^+$ and \u2212) [9]. ", "page_idx": 18}, {"type": "text", "text": "Next, we claim that we can find a valid $p$ (rational with $1/2\\,<\\,p\\,<\\,1)$ and $w~>~0$ such that $g_{+}^{\\prime}(w)>0$ , while $h_{+}^{\\prime}(w)<0$ . To that end, expanding the first derivative, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\ng_{+}^{\\prime}(x)=-p\\ell_{-}^{\\prime}(-x)+(1-p)\\ell_{+}^{\\prime}(x).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "As long as $\\ell_{-}^{\\prime}(-x)+\\ell_{+}^{\\prime}(x)>0$ , this is strictly more than 0 for $p$ satisfying ", "page_idx": 18}, {"type": "equation", "text": "$$\np<\\frac{\\ell_{+}^{\\prime}(x)}{\\ell_{-}^{\\prime}(-x)+\\ell_{+}^{\\prime}(x)}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For $h$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h_{+}^{\\prime}(x)=2\\cdot1/2g_{+}^{\\prime}(x/2)+g_{+}^{\\prime}(x)}\\\\ &{\\qquad=-p\\ell_{-}^{\\prime}(-x/2)+(1-p)\\ell_{+}^{\\prime}(x/2)-p\\ell_{-}^{\\prime}(-x)+(1-p)\\ell_{+}^{\\prime}(x).}\\\\ &{\\qquad=-p\\big[\\ell_{-}^{\\prime}(-x/2)+\\ell_{-}^{\\prime}(-x)\\big]+(1-p)\\big[\\ell_{+}^{\\prime}(x/2)+\\ell_{+}^{\\prime}(x)\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "As long as $\\ell_{-}^{\\prime}(-x/2)+\\ell_{-}^{\\prime}(-x)+\\ell_{+}^{\\prime}(x/2)+\\ell_{+}^{\\prime}(x)>0$ , then this is strictly less than 0 for ", "page_idx": 19}, {"type": "equation", "text": "$$\np>\\frac{\\ell_{+}^{\\prime}(x/2)+\\ell_{+}^{\\prime}(x)}{\\ell_{-}^{\\prime}(-x/2)+\\ell_{-}^{\\prime}(-x)+\\ell_{+}^{\\prime}(x/2)+\\ell_{+}^{\\prime}(x)}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Now, choose $w>0$ such that $\\ell_{-}^{\\prime}(-w/2)>\\ell_{-}^{\\prime}(-w)\\geq0$ . This is possible by using the following procedure. We know $\\ell_{-}^{\\prime}(0)>0$ (as otherwise $\\ell(0)<\\ell(x)$ for all $x<0$ , contradicting our assumption on $\\ell$ ). We will split into cases depending on whether $\\ell$ is nondecreasing or strictly-convex (at least one must be true by the theorem assumptions). ", "page_idx": 19}, {"type": "text", "text": "First, suppose $\\ell$ is non-decreasing. This implies that $\\ell_{-}^{\\prime}(x)\\,\\geq\\,0$ for all $x$ . We know that there is a point $x\\,<\\,0$ such that $\\ell_{-}^{\\prime}(x)\\,<\\,\\ell_{-}^{\\prime}(0)$ (as otherwise $\\ell$ would eventually become negative). Let $d=\\ell_{-}^{\\prime}(x)$ . Take $w$ such that ", "page_idx": 19}, {"type": "equation", "text": "$$\n-w=\\operatorname*{sup}\\{x\\mid\\ell_{-}^{\\prime}(x)=d\\}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that $\\ell_{-}^{\\prime}(-w)=d$ because $\\ell_{-}^{\\prime}$ is left continuous (from (iv) above). Since $-w<0$ , so $-\\,{\\frac{w}{2}}\\,>$ $-w$ . This implies that $\\ell_{-}^{\\prime}\\!\\left(-\\frac{w}{2}\\right)>d$ , as otherwise $-w$ would not be the supremum of such points. In addition, we have that $d\\geq0$ because $\\ell$ is nondecreasing. ", "page_idx": 19}, {"type": "text", "text": "Next, suppose $\\ell$ is strictly convex. then $\\ell_{-}^{\\prime}$ is strictly increasing. Further, since it is left continuous and $\\ell_{0}^{\\prime}(0)>0$ , there is a $\\gamma>0$ such that for all $x\\in[-\\gamma,0],\\ell_{-}^{\\prime}(x)\\geq0.$ . Therefore, choosing $w=\\gamma$ will do: $\\ell_{-}^{\\prime}(-\\gamma)\\geq0$ by choice of $\\gamma$ , and $-\\gamma/2>-\\gamma$ , so $\\ell_{-}^{\\prime}(-\\gamma/2)>\\ell_{-}^{\\prime}(-\\gamma)$ since $\\ell_{-}^{\\prime}$ is strictly increasing. ", "page_idx": 19}, {"type": "text", "text": "For this choice of $w$ , we first claim that the preconditions of denominators being positive hold for (2) and (3). Indeed, let us write $z_{1},z_{2},z_{3},z_{4}$ for $\\ell_{-}^{\\prime}(-2w),\\ell_{-}^{\\prime}(-w),\\ell_{+}^{\\prime}(w),\\ell_{+}^{\\prime}(\\bar{2w})$ . We know that ", "page_idx": 19}, {"type": "equation", "text": "$$\nz_{1}\\leq z_{2}\\leq z_{3}\\leq z_{4}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "by properties of convexity, and since $\\ell_{-}^{\\prime}(-w/2)>\\ell_{-}^{\\prime}(-w)\\geq0$ , we have that $0\\leq z_{1}<z_{2}$ . The denominators are of the form $z_{1}+z_{4}$ and $z_{1}+z_{2}+z_{3}+z_{4}$ , which are now both necessarily positive. In addition, we also claim that a rational $p$ with $1/2<p<1$ satisfying both inequalities (2) and (3) will exist. Note that inequality (2) can now be represented as z1z+4z4 , and we have that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{z_{4}}{z_{1}+z_{4}}\\leq1\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "because $0\\leq z_{1}<z_{4}$ . Additionally, inequality (3) can be represented as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{z_{3}+z_{4}}{z_{1}+z_{2}+z_{3}+z_{4}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which is at least $1/2$ because $z_{3}+z_{4}>z_{1}+z_{2}$ . Finally, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{z_{3}}{z_{2}+z_{3}}\\leq\\frac{z_{4}}{z_{2}+z_{4}}<\\frac{z_{4}}{z_{1}+z_{4}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which implies that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{z_{3}+z_{4}}{z_{1}+z_{2}+z_{3}+z_{4}}<\\frac{z_{4}}{z_{1}+z_{4}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Hence, there exists some rational $p$ in this interval, which is necessarily between $1/2$ and 1, as needed. ", "page_idx": 19}, {"type": "text", "text": "To summarize, we have found a valid $p$ and value $w>0$ such that $h_{+}^{\\prime}(w)<0$ while $g_{+}^{\\prime}(w)>0$ . Because $h_{+}^{\\prime}$ is right continuous (from (iv) above), there is some $\\gamma>0$ such that $h_{+}^{\\prime}(w+\\gamma)<0$ as well. We will now show for all $(r_{a},r_{b})\\in O P T^{u n c o n s t r}$ , $r_{a}>w+\\gamma$ and $r_{b}\\leq w$ . Indeed, note that $r_{a}$ must minimize $h$ , so $h_{+}^{\\prime}(r_{a})\\geq0$ (from (iii) above), which implies $r_{a}>w+c$ . For $r_{b}$ , suppose for a contradiction $r_{b}>w$ as well. Note that $g_{+}^{\\prime}(w)>0$ means $g$ is increasing to the right of $w$ . Let $d=\\operatorname*{min}(r_{a},r_{b})-w>0$ . Consider $(r_{a}^{\\prime},r_{b}^{\\prime})=\\dot{(r_{a}-d,r_{b}-d)}$ . We then have ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}^{u n c o n s t r}(r_{a}^{\\prime},r_{b}^{\\prime})=g(r_{a}^{\\prime}-r_{b}^{\\prime})+g(r_{a}^{\\prime})+g(r_{b}^{\\prime})}\\\\ &{\\qquad\\qquad\\qquad=g(r_{a}-r_{b})+g(r_{a}^{\\prime})+g(r_{b}^{\\prime})}\\\\ &{\\qquad\\qquad\\quad<g(r_{a}-r_{b})+g(r_{a})+g(r_{b})}\\\\ &{\\qquad\\qquad\\quad=\\mathcal{L}^{u n c o n s t r}(r_{a},r_{b}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the equality holds because $r_{a}^{\\prime}-r_{b}^{\\prime}=r_{a}-r_{b}$ and the inequality because $g$ is increasing to the right of $w$ . Therefore, we reach a contradiction, because then $(r_{a},r_{b})$ would not be optimal. Thus, we have found values satisfying the lemma statement with $A_{1}=w$ and $A_{2}=w+\\gamma$ . \u53e3 ", "page_idx": 20}, {"type": "text", "text": "A.2 Proof of Lemma 3.3 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proof. Fix $p$ inducing a nonempty $O P T^{u n c o n s t r}$ satisfying Lemma 3.2 with values $A_{1}$ and $A_{2}$ . We first claim that for any $(r_{a},r_{b})$ (regardless of optimality), it is possible to find a $\\theta$ such that $r_{\\theta}(a)=r_{a}$ and $r_{\\theta}(b)=r_{b}$ . Indeed, note that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left(2\\!\\!\\begin{array}{c c}{{1}}&{{1}}\\\\ {{1}}&{{1}}\\end{array}\\!\\!\\right)\\left(\\!\\!\\begin{array}{c}{{\\theta_{1}}}\\\\ {{\\theta_{2}}}\\end{array}\\!\\!\\right)=\\left(\\!\\!\\begin{array}{c}{{r_{\\theta}(a)}}\\\\ {{r_{\\theta}(b)}}\\end{array}\\!\\!\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since $M=\\left(\\!\\!{\\begin{array}{c c}{{2}}&{{1}}\\\\ {{1}}&{{1}}\\end{array}}\\!\\!\\right)$ is invertible with inverse ", "page_idx": 20}, {"type": "equation", "text": "$$\nM^{-1}=\\left({1\\atop-1}\\quad{-1\\atop2}\\right),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for any $r_{a},r_{b}$ , we can simply set ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\theta=M^{-1}\\left(r_{a}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus, $O P T^{c o m}$ is nonempty, and is simply the image of $O P T^{u n c o n s t r}$ under $M^{-1}$ . Now, fix $(r_{a},r_{b})\\in O P T^{u n c o n s t r}$ , and let $\\theta=M^{-1}\\,\\binom{r_{a}}{r_{b}}$ . By assumption, we have $r_{a}>A_{2}$ and $r_{b}\\leq A_{1}$ . This implies that $\\theta_{1}\\,=\\,r_{a}\\,-\\,r_{b}\\,>\\,A_{2}\\,-\\,A_{1}$ , while $\\theta_{2}\\,=\\,2r_{b}\\,-\\,r_{a}\\,<\\,2A_{1}\\,-\\,A_{2}$ . Thus, setting $A_{3}=A_{2}-A_{1}$ and $A_{4}=2A_{1}-A_{2}$ satisfy the desired properties. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "A.3 Proof of Lemma 3.4 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proof. We first claim that $O P T(0)=O P T^{c o r e}$ . This will follow from showing ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{L}^{0}(\\theta)=4\\cdot\\mathcal{L}^{c o r e}(\\theta)+3\\cdot\\ell(0).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Indeed, in ${\\mathcal{L}}^{0}$ , the copied candidates are in exactly the same location as their counterparts. Hence, each term in $\\mathcal{L}^{c o r e}$ appears 4 times, one for each combination of original and copy. In addition to these, there are the 6 terms for each ordered pair of $(a,a^{\\prime}),\\,(b,b^{\\prime})$ , and $(c,c^{\\prime})$ . Note that, each $r_{\\theta}(x)=r_{\\theta}(x^{\\prime})$ for each $x\\in\\{a,b,c\\}$ regardless of $\\theta$ since they are in the same location. Therefore, the $\\ell$ portion is always $\\ell(0)$ , and the corresponding $w_{x\\succ x^{\\prime}}$ and $w_{x^{\\prime}\\succ x}$ terms add up to one for each pair. Hence, the total sum of these terms is $3\\cdot\\ell(0)$ . Since ${\\mathcal{L}}^{0}$ is equivalent to $\\mathcal{L}^{c o r e}$ up to positive scaling and translation, they have the same optima. ", "page_idx": 20}, {"type": "text", "text": "Finally, fix a $\\theta\\in O P T(0)$ . Since $\\theta\\in O P T^{c o r e}$ , by Lemma 3.3, $\\theta_{1}>A_{3}$ and $\\theta_{2}<A_{4}$ . Thus, ", "page_idx": 20}, {"type": "equation", "text": "$$\nr_{\\theta}(c^{\\prime})=\\varepsilon\\cdot(-\\theta_{1}+\\delta\\theta_{2})<\\varepsilon(-A_{3}+\\delta A_{4})<0=r_{\\theta}(c)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "by choice of $\\delta$ . Hence, $\\theta\\in\\overline{{R^{c^{\\prime}\\succ c}}}$ ", "page_idx": 20}, {"type": "text", "text": "A.4 Proof of Lemma 3.5 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proof. We first show that when optimizing each $\\mathcal{L}^{\\varepsilon}$ , it is sufficient to consider only $\\theta$ coming from a bounded region. Indeed, observe that $\\begin{array}{r}{\\mathcal{L}^{\\varepsilon}(\\mathbf{0})\\,=\\,{\\binom{6}{2}}\\ell(0)}\\end{array}$ for all $\\varepsilon$ . Since $\\begin{array}{r}{\\operatorname*{lim}_{x\\rightarrow\\infty}\\ell(x)=\\infty}\\end{array}$ , we ", "page_idx": 20}, {"type": "text", "text": "can find some $B\\,>\\,0$ such that for all $x\\,>\\,B$ , $\\begin{array}{r}{\\ell(x)\\,>\\,{\\frac{{\\binom{6}{2}}\\ell(0)}{1-p}}\\,=\\,{\\frac{{\\mathcal{L}}^{\\varepsilon}(\\mathbf{0})}{1-p}}}\\end{array}$ = L1\u03b5\u2212(0p) . For a pair of candidates $x\\neq y\\in C^{c o m}$ , in the two terms concerning these candidates, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad w_{x\\succ y}\\ell(r_{\\theta}(y)-r_{\\theta}(x))+w_{y\\succ x}\\ell(r_{\\theta}(x)-r_{\\theta}(y))}\\\\ &{\\geq(1-p)\\left(\\ell(r_{\\theta}(y)-r_{\\theta}(x))+\\ell(r_{\\theta}(x)-r_{\\theta}(y))\\right)}\\\\ &{\\geq(1-p)\\ell(|r_{\\theta}(y)-r_{\\theta}(x)|).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Applying this to $\\{a,b\\}$ and $\\{b,c\\}$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathscr{L}}^{\\varepsilon}(\\theta)\\geq(1-p)\\left(\\ell(|r_{\\theta}(a)-r_{\\theta}(b)|+\\ell(|r_{\\theta}(b))-r_{\\theta}(c)|)\\right)}\\\\ &{\\qquad=(1-p)(\\ell(|\\theta_{1}|)+\\ell(|\\theta_{1}+\\theta_{2}|)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This implies that we may restrict our attention to $\\theta$ in the region ", "page_idx": 21}, {"type": "equation", "text": "$$\nR^{b o u n d e d}=\\{\\theta\\mid|\\theta_{1}|\\leq B,|\\theta_{2}|\\leq2B\\}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Indeed, for $\\theta\\notin\\ R^{b o u n d e d}$ , either $|\\theta_{1}|\\,>\\,B$ or $|\\theta_{1}+\\theta_{2}|\\,>\\,B$ . In either case, we have ${\\mathcal{L}}^{\\varepsilon}(\\theta)\\geq$ $(1-p)\\ell(B)>\\mathcal{L}^{\\varepsilon}(\\mathbf{0})$ . ", "page_idx": 21}, {"type": "text", "text": "Note that $C^{\\varepsilon}(\\theta)$ is continuous not only in $\\theta$ , but also in $\\varepsilon$ . Additionally, $R^{b o u n d e d}$ is closed and bounded, and hence, compact. Therefore, by Berge\u2019s Maximum Theorem, $O P T(\\varepsilon)$ is nonempty and upper semi-continuous in $\\varepsilon$ [3]. As per the definition of upper semi-continuous, since $O P T(0)\\subseteq$ $\\overline{{R^{c^{\\prime}\\succ c}}}$ , an open set, for sufficiently small $\\varepsilon\\;>\\;0,\\,O P T(\\varepsilon)\\;\\subseteq\\;\\overline{{{R^{c}}^{\\prime}\\!\\succ\\!c}}$ . Finally, note that $R^{c^{\\prime}\\succ c}\\cap$ $R^{b o u n d e d}$ is compact, so a minimum is attained, and this minimum must therefore be strictly larger than the values attained by members of $O P T(\\varepsilon)$ . \u53e3 ", "page_idx": 21}, {"type": "text", "text": "A.5 Proof of Theorem 3.6 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Proof. Without loss of generality, we may assume that $\\operatorname*{inf}_{x}\\ell(x)=0$ , as otherwise we could translate $\\ell$ without affecting the optimization problem. Fix a profile $\\pi$ with feasible PMC ranking $\\sigma$ , and let $\\theta^{P M C}$ be a non-degenerate parameter vector that induces $\\sigma$ . ", "page_idx": 21}, {"type": "text", "text": "First, we show that $\\mathrm{nf}_{\\theta}\\,\\mathcal{L}^{m a j}(\\theta;\\pi,\\ell)=0$ . Indeed, note that $\\boldsymbol{c}\\cdot\\theta^{P M C}\\in R^{\\pi}$ for all $c>0$ . Further, note that for any $a,b$ with $w_{a\\succ b}(\\pi)>1/2$ , $r_{\\theta^{P M C}}(b)\\,-\\,r_{\\theta^{P M C}}(a)\\,<\\,0$ . Therefore, by making $c$ large, the nonzero terms in $\\mathcal{L}^{m a j}$ will have an input to $\\ell$ negative and becoming arbitrarily large in magnitude. Since $\\ell$ is nondecreasing, these approach the infemum of 0. ", "page_idx": 21}, {"type": "text", "text": "Next, for any $\\sigma^{\\prime}\\neq\\sigma$ , $\\operatorname*{inf}_{\\theta}\\mathcal{L}^{m a j}(\\theta;\\pi,\\ell)\\geq\\ell(0)$ . Indeed, there must be some pair of candidates $a,b$ with $a\\succ_{\\sigma}b$ and $b\\succ_{\\sigma^{\\prime}}a$ . For any $\\theta\\in R^{\\sigma^{\\prime}}$ , $r_{\\theta}(b)\\geq r_{\\theta}(a)$ , so $\\ell(r_{\\theta}(b)-r_{\\theta}(a))\\geq\\ell(0)$ , and this lower bounds the loss function. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "A.6 Proof of Theorem 3.7 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Proof. We construct an explicit instance and pairwise majority relationships such that no matter what feasible ranking a rule picks, there is an underlying profile where that output was a PO violation. ", "page_idx": 21}, {"type": "text", "text": "We will have 9 candidates; 8 will be labeled $c_{i}^{+}$ and $c_{i}^{-}$ for $i\\,=\\,1,2,3,4$ , and one labeled $c^{*}$ . They will have feature vectors in $\\mathbb{R}^{4}$ . Each $c_{i}^{\\pm}$ will be located at $\\mathbf{x}_{c_{i}^{\\pm}}=\\pm e_{i}$ where $e_{i}$ is the $i$ \u2019th standard basis vector, i.e., $c_{2}^{+}$ is at $(0,1,0,0)$ and $c_{4}^{-}$ is at $(0,0,0,-1)$ . Finally, $c^{*}$ will be located at $(1/5,1/5,1/5,1/5)$ . ", "page_idx": 21}, {"type": "text", "text": "There will be 5 voters. Their pairwise majority graph will be as follows. Candidate $c^{*}$ will pairwise beat all others. In addition, each $c_{i}^{+}$ will pairwise beat each $c_{j}^{-}$ . Among the $c_{i}^{+}$ candidates, there will be a cycle $c_{1}^{+}\\succ c_{2}^{+}\\succ c_{3}^{+}\\succ c_{4}^{+}\\succ c_{1}^{+}$ and between the remaining two pairs $c_{1}^{+}\\succ c_{3}^{+}$ and $c_{2}^{+}\\succ c_{4}^{+}$ . The $c_{i}^{-}$ candidates will be the exact reverse of this, i.e., a cycle $c_{4}^{-}\\,\\succ\\,c_{3}^{-}\\,\\succ\\,c_{2}^{-}\\,\\succ\\,c_{1}^{-}\\,\\succ\\,c_{4}^{-}$ , along with $c_{3}^{-}\\succ c_{1}^{-}$ and $c_{4}^{-}\\succ c_{2}^{-}$ . A pictorial representation can be found in Figure 1. ", "page_idx": 21}, {"type": "text", "text": "A C1 rule must pick a $\\theta$ solely based on the pairwise majority graph. We will show that regardless of what $\\theta$ it outputs, this will lead to a PO violation. ", "page_idx": 21}, {"type": "image", "img_path": "cmBjkpRuvw/tmp/b8e45a917375bf6e42107ab492189a326511cc339a26d0b0a9e449d95614da80.jpg", "img_caption": ["Figure 1: Graph showing pairwise majority relationship between candidates. Regular edges show relationships among $c_{i}^{+}$ candidates and among $c_{i}^{-}$ candidates. Thick edges indicate that $c^{*}$ pairwise beats all candidates, and each $c_{i}^{+}$ pairwise beats each $c_{j}^{-}$ candidate. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "To that end, the first fact we will show is that no $\\theta$ can rank $c^{*}$ first. Indeed, for any $\\theta$ , $r_{\\theta}(c^{*})=$ $\\begin{array}{r}{\\frac{1}{5}\\sum_{i}\\theta_{i}\\leq\\frac{1}{5}\\sum_{i}|\\theta_{i}|<\\operatorname*{max}_{i}|\\theta_{i}|}\\end{array}$ . On the other hand, for $i$ maximizing $|\\theta_{i}|$ , at least one of $c_{i}^{\\pm}$ will achieve this reward, strictly larger than $r_{\\theta}(c^{*})$ . Hence, regardless of the output $\\theta$ , some candidate must be ranked above $c^{*}$ . We will show that this leads to a PO violation. ", "page_idx": 22}, {"type": "text", "text": "To construct profiles consistent with the pairwise majority graph, voters will always have rankings of the following form: ", "page_idx": 22}, {"type": "equation", "text": "$$\nc_{i}^{+}\\succ c^{*}\\succ c_{j}^{+}\\succ c_{k}^{+}\\succ c_{\\ell}^{+}\\succ c_{\\ell}^{-}\\succ c_{k}^{-}\\succ c_{j}^{-}\\succ c_{i}^{-}-\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for some $\\{i,j,k,\\ell\\}=\\{1,2,3,4\\}$ . In other words, they will rank a single $^+$ candidate above $c^{*}$ and the rest in some order, followed by all \u2212candidates in the reverse order. This is always achievable with the voter vector that puts values $1,3\\varepsilon,2\\varepsilon,1\\varepsilon$ in entires $i,j,k$ , and $\\ell$ , respectively, for some small $\\varepsilon>0$ . ", "page_idx": 22}, {"type": "text", "text": "Fix an output $\\theta$ with induced ranking $\\sigma$ . There must be at least one candidate $c\\neq c^{*}$ ranked above $c^{*}$ . We now split into cases depending on which candidate this is. For each choice, we will construct a profile consistent with the pairwise majority graph where the candidate above $c^{*}$ is Pareto dominated by $c^{*}$ . We will describe each voter\u2019s ranking only by an ordering over the $^+$ candidates, assuming they otherwise take the form described in (4). Note that $c^{*}$ is always ranked second, so if a candidate is never ranked first, they are Pareto dominated by $c^{*}$ . The profiles for each candidate $c_{i}^{+}$ can be found in the following table. One can check that all pairwise relationships are satisfied, and the corresponding $c_{i}^{+}$ is never ranked first. ", "page_idx": 22}, {"type": "text", "text": "Table 1: Profiles with 5 voters and consistent with the pairwise majority graph where where the corresponding candidate is PO-dominated by $c^{*}$ . The notation $1:(2,1,3,4)$ implies one voter has the ranking in the form of (4) with $(i,j,k,\\ell)=(2,1,3,4)$ . ", "page_idx": 22}, {"type": "table", "img_path": "cmBjkpRuvw/tmp/75f274a776417ab9159d9c557e794b96d774db7626e73e0d44b4c2b228c79240.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Finally, if a \u2212candidate is ranked above $c^{*}$ , then any of the following profiles work, as all \u2212 candidates are Pareto dominated by $c^{*}$ with rankings shown in (4). \u53e3 ", "page_idx": 22}, {"type": "text", "text": "B PMC Infeasibility Example ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Consider the case with $d\\ \\ =\\ \\ 3$ and seven candidates: one special candidate $a^{*}$ located at $(1/4,1/4,1/4)$ , and six others $c_{i}^{\\pm}$ located at standard basis vectors $e_{i}^{\\pm}$ . We have three voters with parameter vectors $(1,2\\varepsilon,\\varepsilon)$ , $(2\\varepsilon,1,\\varepsilon)$ , and $(2\\varepsilon,\\varepsilon,1)$ , where $\\varepsilon\\,<\\,1/5$ is a small positive number. These voters have the following induced rankings: ", "page_idx": 22}, {"type": "table", "img_path": "cmBjkpRuvw/tmp/c8a2d9201952621ca23125fa6d0162871ccf9a66e96d24da8e7c700f384f759b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "We argue the ranking $a^{*}\\succ c_{1}^{+}\\succ c_{2}^{+}\\succ c_{3}^{+}\\succ c_{3}^{-}\\succ c_{2}^{-}\\succ c_{1}^{-}$ is a PMC ranking but no linear reward function can position $a^{*}$ at the top of this ranking. ", "page_idx": 23}, {"type": "text", "text": "For any reward vector $\\theta=(\\theta_{1},\\theta_{2},\\theta_{3})\\in\\mathbb{R}^{3}$ , the reward for $a^{*}$ is : ", "page_idx": 23}, {"type": "equation", "text": "$$\nr_{a^{*}}^{\\theta}=\\frac{1}{4}(\\theta_{1}+\\theta_{2}+\\theta_{3})\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Given the placement of $c_{i}^{\\pm}$ at the standard basis vectors, each $c_{i}^{\\pm}$ achieves a reward equivalent to one of the absolute values of the components of $\\theta$ , thus surpassing $r_{a^{*}}^{\\theta}$ since ", "page_idx": 23}, {"type": "equation", "text": "$$\nr_{a^{*}}^{\\theta}<\\operatorname*{max}_{i}|\\theta_{i}|.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "C Additional Axiomatic Properties of Social Choice-Based Rules ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We begin by stating another prominent axiom from social choice theory. ", "page_idx": 23}, {"type": "text", "text": "Definition C.1 (Separability). A ranking aggregation rule satisfies ranking separability (or separability for short) $i f,$ when two profiles yield identical output rankings, when combined into a single profile, this should also produce the same output ranking. ", "page_idx": 23}, {"type": "text", "text": "Ranking separability preserves consistency in aggregation outputs and ensures stable decisions across similar preference distributions. ", "page_idx": 23}, {"type": "text", "text": "C.1 Copeland Violates Separability ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Theorem C.2. Both Copeland (in traditional social choice) and LCPO (in linear social choice) fail separability. ", "page_idx": 23}, {"type": "text", "text": "Proof. Consider the following two profiles on 7 candidates with five and three voters each: ", "page_idx": 23}, {"type": "table", "img_path": "cmBjkpRuvw/tmp/53d06d67c614e849c80420c6e3f676e78bda90e3b6d2e827246bd1fec0c500bd.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "In the first profile, the Copeland scores are $5,4,3,3,3,2,1$ for candidates $a,b,c,d,e,f,g$ , respectively. Similarly, in the second profile, they are $6,5,3,3,3,1,0$ . So under any consistent tie-breaking rule, both of these profiles would output the ranking $a\\succ b\\succ c\\succ d\\succ e\\succ f\\succ g$ . ", "page_idx": 23}, {"type": "text", "text": "However, if we combine these two profiles, then the score of $a$ is 5 while the score of $b$ is 6, and thus, $b$ will be ranked above $a$ , violating separability. ", "page_idx": 23}, {"type": "text", "text": "To see that this also holds for LCPO, note that when every ranking is feasible, LCPO coincides with Copeland. We can simply have 7 candidates in $\\mathbb{R}^{7}$ all located at unit vectors, and voters with inputs from the above profiles. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "C.2 Linear Kemeny Rule ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Next, we consider a different rule from social choice theory, the Kemeny rule, which can be transformed to the linear setting while maintaining the separability can be achieved along with PMC. Given an input profile $\\pi$ , the Kemeny rule returns a ranking $\\sigma^{*}$ that minimizes the total number of pairwise disagreements with voters rankings, i.e. ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sigma^{*}\\in\\underset{\\sigma\\in S^{m}}{\\arg\\operatorname*{min}}\\sum_{i\\in V}\\sum_{(a,b):a\\succ_{\\sigma_{i}}b}\\mathbb{1}(b\\succ_{\\sigma}a)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $S^{m}$ contains all possible permutations of the $m$ candidates. This expression can be equivalently written as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sigma^{*}\\in\\underset{\\sigma\\in S^{m}}{\\arg\\operatorname*{min}}\\sum_{a\\neq b\\in C}n_{a\\succ b}(\\pi)\\cdot\\mathbb{1}(b\\succ_{\\sigma}a).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Here, we define the linear Kemeny rule, which outputs a parameter vector $\\theta^{*}$ that induces a ranking that minimizes the total number of pairwise disagreements with voters\u2019 rankings, i.e., ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\theta^{*}\\in\\underset{\\theta\\in\\mathbb{R}^{d}}{\\arg\\operatorname*{min}}\\sum_{a\\neq b\\in C}n_{a\\succ b}(\\pi)\\cdot\\mathbb{1}(r_{\\theta}(b)>r_{\\theta}(a)).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Note that this rule conforms to the standard loss formulation, where the loss function is binary: it is 0 if two rankings agree with respect to the relative ranking of a pair of candidates and 1 otherwise. Since binary loss is not convex, it does not fit in the impossibility result of Theorem 3.1. ", "page_idx": 24}, {"type": "text", "text": "Note that Kemeny is generally NP-hard to compute; however, even for linear Kemeny, there is at least an exponential time aglorithm by brute-force computing the score of every ranking, and determining whether or not it is feasible. ", "page_idx": 24}, {"type": "text", "text": "Theorem C.3. Linear Kemeny satisfies PMC and separability. ", "page_idx": 24}, {"type": "text", "text": "Proof. PMC holds because the linear Kemeny score minimizes disagreements even among nontransitive rankings, making the PMC ranking the optimal choice whenever it is feasible. Separability is evident as the Kemeny score of a ranking over two datasets is simply the sum of the scores in each dataset. If the same ranking minimizes the score in both datasets independently, it will also minimize the score in their combination. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "Theorem C.4. Linear Kemeny does not satisfy PO or majority consistency. ", "page_idx": 24}, {"type": "text", "text": "Proof. Consider the scenario with 20 candidates whose feature vectors are represented in the table below: ", "page_idx": 24}, {"type": "table", "img_path": "cmBjkpRuvw/tmp/0d73ea4ee051396d1691c10ae3660612c3cfb904941853a14a3f5eb6c8a056a5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "Each vector is constructed such that candidates are prioritized based on the magnitude of their first non-zero entry, leading to a natural ordering within grouped subsets: $\\{1,2\\}^{-}\\succ\\{3,4,5\\}\\ \\succ$ $\\{6,7,8\\}\\,\\succ\\,\\{9,10,11\\}\\,\\succ\\,\\mathbf{\\bar{\\{\\{\\,12,13,14\\}}}}\\,\\succ\\,\\{15,\\bar{16},17\\}\\,\\succ\\,\\{1\\bar{8},19,20\\}$ . We will have six voters, with rankings induced by the parameter vectors described below. ", "page_idx": 25}, {"type": "table", "img_path": "cmBjkpRuvw/tmp/9e476d2b724193e86d1d73c57de8c42d71789154acd2a73bf228f0512bedd722.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "Each voter ranks candidates within the group in increasing order except for a single reversed group. For instance, $v_{1}$ ranks candidates as $1\\succ2\\succ5\\succ4\\succ3$ and so forth. Under this setup, no parameter vector $\\theta$ can create a ranking that satisfies all voters\u2019 preferences due to the cyclic nature and individual group preferences. ", "page_idx": 25}, {"type": "text", "text": "We can check that linear Kemeny rule outputs a ranking in which candidate 2 is ranked above 1.   \nFrom this analysis, we also conclude that the rules does not satisfy majority consistency either. ", "page_idx": 25}, {"type": "text", "text": "A possibly easy fix to the problem that linear Kemeny does not satisfy PO would be to enforce the PO criterion, as we did for the LCPO, i.e., to restrict to parameter vectors that respect PO. However, we show that if we do that, then linear Kemeny subject to PO does not satisfy separability anymore. ", "page_idx": 25}, {"type": "text", "text": "Theorem C.5. Linear Kemeny subject to PO violates separability and majority consistency. ", "page_idx": 25}, {"type": "text", "text": "Proof. First, consider a set of candidates and a profile similar to the one that is given in the proof of Theorem C.4. When we restrict to reward functions that 1 above 2, then we can check that Linear Kemeny subject to PO outputs one of the rankings that are in the input profile. Without loss of generality, assume that it outputs the ranking of $v_{1}$ . ", "page_idx": 25}, {"type": "text", "text": "Second, consider the same set of candidates and three voters, with rankings induced by the following parameter vectors: ", "page_idx": 25}, {"type": "table", "img_path": "cmBjkpRuvw/tmp/3f3b0378c77e6e0705a04fda86a5844ef7c25a97968c933e01a46240427cf3f3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "In this case, Linear Kemeny subject to PO outputs the ranking of $v_{1}^{\\prime}$ which is the same with this of voter v1. ", "page_idx": 25}, {"type": "text", "text": "When the two profiles are combined, then we do not anymore restrict on rankings in which 1 is above 2, since 1 does not Pareto dominates 2 anymore. Then, we can check that linear Kemeny subject to PO outputs a different ranking than before in which 2 is ranked above 1. From this example, we see that linear Kemenery subject to PO still violates majority consistency. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "C.3 Leximax Plurality ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Plurality is probably the most ubiquitous voting rule in the world. Its ranking variant ranks the candidates in decreasing order with respect to their plurality scores. The plurality score of a candidate is equal to the number of her appearances in the first position. This rule is known to satisfy several axioms but in linear social choice cannot be directly applied, as not all rankings are feasibly. ", "page_idx": 25}, {"type": "text", "text": "Similarly to leximax Copeland, we define Leximax Plurality as follows. It ranks first the candidate with the highest plurality score that can be ranked first under some parameter vector. Subject to this first position, it ranks second the candidate with the highest plurality score that can feasibly be ranked second, and so on, until all the positions are filled. ", "page_idx": 25}, {"type": "text", "text": "Proof. Note that leximax Plurality always returns a ranking in which the candidate with the highest plurality score is ranked first, since there exists at least one feasible ranking in which this candidate is ranked first. From this observation, we immediately see that leximax Plurality satisfies majority consistency. ", "page_idx": 26}, {"type": "text", "text": "Now, suppose that on input profile $\\pi$ , the rule outputs a ranking such that candidate $a$ is ranked first. This means that $a$ has the highest plurality score. Now, consider a profile $\\pi^{\\prime}$ which is similar to $\\pi$ with the only exception being a ranking in which $a$ is ranked in a higher position. It is clear that $a$ continues to have the highest plurality score and therefore leximax Plurality will output a ranking in which $a$ is ranked first. Therefore, winner monotonicity is satisfied. . ", "page_idx": 26}, {"type": "text", "text": "It remains to prove that the rule satisfies separability. Suppose that under two different profiles $\\pi_{1}$ and $\\pi_{2}$ , the rule outputs $\\sigma$ , and under the aggregated profile $\\pi_{3}$ , it outputs $\\sigma^{\\prime}$ . We will show that $\\sigma=\\sigma^{\\prime}$ . One main observation is that if a candidate $a$ has a higher plurality score than a candidate $b$ under both $\\pi_{1}$ and $\\pi_{2}$ , then $a$ has a higher plurality score than $b$ under $\\pi_{3}$ as well. We will show the desired property by induction on the positions of the ranking $\\sigma$ . Start from the first position in which say candidate $a$ is ranked first. From above, we know that $a$ has the highest plurality score under both $\\pi_{1}$ and $\\pi_{2}$ , which remains true in $\\pi_{3}$ , and therefore $a$ is ranked first in $\\sigma^{\\prime}$ . Now, assume that up to position $t-1$ , $\\sigma$ and $\\sigma^{\\prime}$ are similar and denote with $a^{\\prime}$ the candidate that is ranked at position $t$ of $\\sigma$ . We denote by $S_{1}$ , $S_{2}$ and $S_{3}$ the set of candidates that are not ranked among the first $t$ positions in $\\sigma$ and have higher plurality score than $a^{\\prime}$ under $\\pi_{1},\\,\\pi_{2}$ , and $\\pi_{3}$ respectively. Since, $a^{\\prime}$ is ranked at the $t$ -th postion in $\\sigma$ , we get that, subject to the fixed first $t-1$ position, no candidate in $S_{1}\\cup S_{2}$ can be ranked at the $t$ -th position. The theorem follows by noticing that $S_{3}\\subseteq S_{1}\\cup S_{2}$ , which follows from the main observation above. \u53e3 ", "page_idx": 26}]