[{"figure_path": "yBrxziByeG/figures/figures_1_1.jpg", "caption": "Figure 1: Our proposed explicit coupling paradigm of multi-modal information fusion and diffusion.", "description": "This figure illustrates the proposed explicit coupling paradigm of multi-modal information fusion and diffusion.  It compares the existing method DDFM (ICCV 2023) with the authors' proposed method, Text-DiFuse. The diagram shows how two source images (Source Image 1 and Source Image 2) are processed through separate diffusion processes before being explicitly fused using a diffusion fusion module. The fusion happens at the feature level within the diffusion process, allowing for the adaptive degradation removal and multi-modal information integration. The resulting fused image is generated after multiple steps of the diffusion process. The figure highlights the key difference between the existing approach and the proposed Text-DiFuse method in terms of integrating information fusion into the diffusion process.", "section": "1 Introduction"}, {"figure_path": "yBrxziByeG/figures/figures_3_1.jpg", "caption": "Figure 2: The pipeline of our Text-DiFuse. (a) The text-controlled diffusion fusion process; (b) training diffusion model for degradation removal; (c) the detailed structure of fusion control module.", "description": "This figure illustrates the pipeline of the Text-DiFuse model, an interactive multi-modal image fusion framework based on a text-modulated diffusion model.  Panel (a) shows the overall process, starting with multi-modal images and text input. The model uses a text-controlled diffusion fusion process, with independent conditional diffusion steps for degradation removal.  Multi-modal information is fused using a Fusion Control Module (FCM). Finally, a text-controlled re-modulation strategy is applied to highlight objects of interest. Panel (b) details the training of the diffusion model for degradation removal, showing how random degradation is introduced and then processed through the encoder-decoder network. Panel (c) zooms in on the FCM structure, highlighting its use of channel and spatial attention modules to manage feature integration in a multi-modal context.", "section": "3 Methodology"}, {"figure_path": "yBrxziByeG/figures/figures_6_1.jpg", "caption": "Figure 3: Visual comparison of image fusion methods.", "description": "This figure presents a visual comparison of image fusion results obtained using different methods on various datasets.  The top two rows showcase infrared and visible image fusion, highlighting improvements in color correction, noise reduction, and overall clarity achieved by the proposed method (Ours) compared to other state-of-the-art techniques (TarDAL, DeFusion, LRRNet, DDFM, MRFS). The following rows demonstrate medical image fusion, again showing that the proposed method excels at preserving important physiological structures while maintaining functional distribution, outperforming other approaches in image quality and information preservation. The red boxes highlight regions of interest to emphasize the differences in fusion quality.", "section": "4 Experiments"}, {"figure_path": "yBrxziByeG/figures/figures_7_1.jpg", "caption": "Figure 4: Visual comparison of enhancement plus image fusion methods.", "description": "This figure compares the results of various image fusion methods after pre-processing with enhancement techniques (CLIP-LIT, SDAP, and AWB). The top two rows show infrared and visible image fusion results, while the bottom rows demonstrate medical image fusion results (SPECT-MRI, PET-MRI, and CT-MRI).  Each column represents a different method: TarDAL, DeFusion, LRRNet, DDFM, MRFS, and the proposed \"Ours\" method. The figure visually demonstrates the performance of each fusion method in improving image quality by comparing the fused images with the original source images.", "section": "4 Experiments"}, {"figure_path": "yBrxziByeG/figures/figures_7_2.jpg", "caption": "Figure 3: Visual comparison of image fusion methods.", "description": "This figure provides a visual comparison of different image fusion methods, including the proposed Text-DiFuse method and several state-of-the-art techniques. The comparison is performed on both infrared and visible image fusion (IVIF) and medical image fusion (MIF) scenarios. The results visually demonstrate the superior performance of Text-DiFuse in terms of noise reduction, color correction, detail preservation, and overall image quality.", "section": "4 Experiments"}, {"figure_path": "yBrxziByeG/figures/figures_8_1.jpg", "caption": "Figure 6: Visual results of re-modulation verification.", "description": "This figure shows the visual results of the re-modulation verification experiments.  Three different scenarios are presented, each with a different text prompt for customization.  The left side of each scenario shows the results of the base fusion model, and the right side shows the results after applying the text-controlled re-modulation strategy. The goal is to demonstrate the ability of the Text-DiFuse model to interactively enhance the salience of specific objects based on user-provided text instructions.  Each scenario includes segmentation results to illustrate the impact of the text prompt on object identification.", "section": "3.3 Text-controlled Fusion Re-modulation Strategy"}, {"figure_path": "yBrxziByeG/figures/figures_8_2.jpg", "caption": "Figure 2: The pipeline of our Text-DiFuse. (a) The text-controlled diffusion fusion process; (b) training diffusion model for degradation removal; (c) the detailed structure of fusion control module.", "description": "This figure illustrates the pipeline of the Text-DiFuse model, a novel interactive multi-modal image fusion framework. It shows three main parts: (a) the text-controlled diffusion fusion process, where text input guides the fusion of multi-modal images; (b) the training of the diffusion model for degradation removal, which learns to remove various degradations from input images; and (c) the detailed structure of the fusion control module (FCM), the core component that integrates multi-modal features and manages their fusion during the diffusion process.  The FCM uses spatial and channel attention mechanisms to weigh the importance of features from different modalities. This comprehensive diagram details how the Text-DiFuse model combines text-guided image fusion with a diffusion model for robust and high-quality results.", "section": "3 Methodology"}, {"figure_path": "yBrxziByeG/figures/figures_9_1.jpg", "caption": "Figure 8: Visual results of ablation studies.", "description": "This figure presents visual results from ablation studies on the Text-DiFuse model.  The top row shows the results under various modifications, removing components like the fusion control module (FCM) or different fusion strategies. The bottom row shows a comparison between the basic version of the Text-DiFuse and the modulatable version. Each column represents an input image (infrared and visible), and the subsequent images show fusion results of different versions, illustrating the impact of each removed component or strategy on the final fusion outcome.  The aim is to show how different components contribute to the overall performance of the Text-DiFuse.", "section": "4 Experiments"}]