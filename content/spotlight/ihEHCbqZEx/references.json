{"references": [{"fullname_first_author": "N. Shazeer", "paper_title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer", "publication_date": "2017-01-17", "reason": "This paper introduces the Sparse Mixture-of-Experts (SMoE) layer, a crucial building block of Flex-MoE, which significantly enhances the efficiency and performance of the model."}, {"fullname_first_author": "M. I. Jordan", "paper_title": "Adaptive mixtures of local experts", "publication_date": "1991-01-01", "reason": "This foundational paper introduces the Mixture-of-Experts (MoE) model, the conceptual precursor to SMoE, providing a critical theoretical framework for Flex-MoE's expert-based architecture."}, {"fullname_first_author": "T. Baltru\u0161aitis", "paper_title": "Multimodal machine learning: A survey and taxonomy", "publication_date": "2018-01-01", "reason": "This survey paper provides a comprehensive overview of the multimodal learning field, contextualizing Flex-MoE's contribution within the broader landscape of multimodal learning research."}, {"fullname_first_author": "B. Mustafa", "paper_title": "Multimodal contrastive learning with LIMOE: the language-image mixture of experts", "publication_date": "2022-06-02", "reason": "This recent work explores the use of mixture-of-experts for multimodal learning, directly informing Flex-MoE's design choices and evaluation benchmarks."}, {"fullname_first_author": "G. Lee", "paper_title": "Predicting alzheimer's disease progression using multi-modal deep learning approach", "publication_date": "2019-01-01", "reason": "This paper addresses the challenge of Alzheimer's Disease prediction using multimodal data, a key application domain for Flex-MoE, providing an important comparison baseline and highlighting the practical significance of Flex-MoE's approach."}]}