[{"figure_path": "ihEHCbqZEx/figures/figures_1_1.jpg", "caption": "Figure 2: Data statistics from a real-world multimodal dataset (e.g., the Alzheimer's Disease Neuroimaging Initiative (ADNI)), where patients exhibit unique combinations of available modalities. Existing approaches focus on either (a) single-modality data or (b) complete multimodal data, losing the potential to leverage other combinations. Our approach incorporates all possible modality combinations, offering a more robust solution to the missing modality scenario.", "description": "This figure uses the ADNI dataset as an example to illustrate the limitations of existing multimodal learning approaches and highlights the advantages of the proposed Flex-MoE method. Panel (a) shows that existing methods often rely on single-modality data, while panel (b) demonstrates the reliance on complete multimodal data, ignoring numerous combinations with missing modalities. In contrast, panel (c) shows that the proposed method utilizes all possible combinations, handling arbitrary modality combinations robustly.", "section": "1 Introduction"}, {"figure_path": "ihEHCbqZEx/figures/figures_1_2.jpg", "caption": "Figure 1: Multimodal AD.", "description": "This figure illustrates the multimodal nature of Alzheimer's Disease (AD).  It shows that diagnosis of AD often involves integrating information from various sources, including clinical records (symptoms), imaging data (MRI scans, PET scans), genetic profiles, and biospecimens (blood, urine, cerebrospinal fluid). The challenge is that not all of these modalities are always available for each patient, making it difficult for existing models to accurately predict AD stages.", "section": "1 Introduction"}, {"figure_path": "ihEHCbqZEx/figures/figures_3_1.jpg", "caption": "Figure 3: The comprehensive illustration of our proposed methodology, Flex-MoE. (a) Overall framework of Flex-MoE. Given samples with diverse modality combinations, we first sort the samples based on their number of available modalities in descending order, and then pass through the modality-specific encoder. (b) Each encoder is only trained with their available samples. For the missing embeddings, we introduce a missing modality bank containing learnable embeddings given the observed modality combination with their corresponding missing modality index. Equipped with this embedding, Flex-MoE passes through the Transformer where the FFN layer is replaced with a Sparse MoE layer. Here, (c) full modality samples take charge of training generalized experts in a balanced manner via G-router, then (d) the remaining few modality combinations further specialize the expert knowledge with S-Router, which fixes the top-1 gate as the corresponding observed modality combination expert. In this figure, top-2 selection of experts is illustrated as an example.", "description": "This figure illustrates the Flex-MoE framework, showing the process of sorting samples by modality availability, handling missing modalities using a missing modality bank, and employing a Sparse Mixture-of-Experts (SMoE) layer with both generalized and specialized experts. The G-Router handles samples with full modalities, while the S-Router specializes in handling samples with fewer modalities.", "section": "3.2 Our approach: Flex-MoE"}, {"figure_path": "ihEHCbqZEx/figures/figures_8_1.jpg", "caption": "Figure 3: The comprehensive illustration of our proposed methodology, Flex-MoE. (a) Overall framework of Flex-MoE. Given samples with diverse modality combinations, we first sort the samples based on their number of available modalities in descending order, and then pass through the modality-specific encoder. (b) Each encoder is only trained with their available samples. For the missing embeddings, we introduce a missing modality bank containing learnable embeddings given the observed modality combination with their corresponding missing modality index. Equipped with this embedding, Flex-MoE passes through the Transformer where the FFN layer is replaced with a Sparse MoE layer. Here, (c) full modality samples take charge of training generalized experts in a balanced manner via G-router, then (d) the remaining few modality combinations further specialize the expert knowledge with S-Router, which fixes the top-1 gate as the corresponding observed modality combination expert. In this figure, top-2 selection of experts is illustrated as an example.", "description": "This figure shows a comprehensive illustration of the Flex-MoE model's architecture. It details the process of handling missing modalities using a missing modality bank and using a two-stage routing mechanism (G-Router and S-Router) for training both generalized and specialized experts to handle various modality combinations.", "section": "3.2 Our approach: Flex-MoE"}, {"figure_path": "ihEHCbqZEx/figures/figures_8_2.jpg", "caption": "Figure 5: Modality combination activation ratio.", "description": "This figure visualizes the activation ratio of input modality combinations across different expert indices in the Flex-MoE model.  It demonstrates how the model utilizes both generalized knowledge from samples with complete modalities and specialized knowledge from samples with fewer modalities, leading to effective handling of various modality combinations.", "section": "3.2 Our approach: Flex-MoE"}]