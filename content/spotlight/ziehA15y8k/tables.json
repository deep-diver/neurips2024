[{"figure_path": "ziehA15y8k/tables/tables_7_1.jpg", "caption": "Table 1: Dataset statistics.", "description": "This table presents the key statistics for the two datasets used in the paper's experiments: Weibo and Pheme.  For each dataset, it shows the number of nodes (representing messages, users, and comments), edges (relationships between nodes), rumors, non-rumors, unique users, and comments.", "section": "4 Experiments"}, {"figure_path": "ziehA15y8k/tables/tables_8_1.jpg", "caption": "Table 2: The attack performance of policy reconstruction evaluated using the average value of the last 100 episodes in the metric \u2206LA as shown in Eq. (18). A higher \u2206LA means better performance. The rows and columns correspond to the IRL methods and attack models used to generate expert samples, respectively. The row Expert is the average performance of the expert samples. In the column of Mixture, we display the performance of the policy reconstructed with expert samples from all low or high cost attack methods.", "description": "This table presents the performance comparison of different inverse reinforcement learning (IRL) methods in reconstructing attack policies.  The performance is measured by the average attack loss (\u2206LA) over the last 100 episodes of the reconstruction process.  It compares three IRL methods (Apprenticeship, EntIRL, MoE-BiEntIRL) against four attack methods (PRBCD, AdRumor, PageRank, GC-RWCS) at different attack budgets (T=5, T=20) and on different datasets (Weibo, Pheme). A higher \u2206LA indicates better performance in approximating the original attack policy.", "section": "3.3 Attack performance evaluation of policy reconstruction"}, {"figure_path": "ziehA15y8k/tables/tables_8_2.jpg", "caption": "Table 2: The attack performance of policy reconstruction evaluated using the average value of the last 100 episodes in the metric \u2206LA as shown in Eq. (18). A higher \u2206LA means better performance. The rows and columns correspond to the IRL methods and attack models used to generate expert samples, respectively. The row Expert is the average performance of the expert samples. In the column of Mixture, we display the performance of the policy reconstructed with expert samples from all low or high cost attack methods.", "description": "This table presents the performance of three different inverse reinforcement learning (IRL) methods in reconstructing attack policies.  It compares the performance of Apprenticeship Learning, EntIRL, and the proposed MoE-BiEntIRL on four different types of graph adversarial attacks (PR-BCD, AdRumor-RL, PageRank, GC-RWCS).  The performance is measured using the average \u2206LA (attack loss difference) over the last 100 episodes of training for each method and attack type, with higher values representing better performance. A mixture of low and high cost attack samples is also tested for each method.", "section": "3.3 Improvement mechanism"}, {"figure_path": "ziehA15y8k/tables/tables_9_1.jpg", "caption": "Table 3: The test accuracy decline (%) of the GCN rumor detector with T = 5 on Weibo dataset. The first row displays the attack method. The first column is the way to enhance the robustness with adversarial samples. The column under w/o Att. reflects test accuracy without attacks, while results under other columns reflect accuracy decline. The second row (w/o Def.) shows the accuracy (decline) without any defense method. Boldfaced font and * mean the best performance and the runner-up among all methods respectively.", "description": "This table shows the test accuracy decline of a GCN rumor detector on the Weibo dataset after applying different defense methods against various graph adversarial attacks (PageRank, GC-RWCS, PR-BCD, AdRumor-RL).  It compares the accuracy decline without any defense (w/o Def), with data augmentation using expert samples (EDA), data augmentation using samples generated by MoE-BiEntIRL (DA), and adversarial training (AT).  The best and second-best performing methods are highlighted.", "section": "4 Experiments"}, {"figure_path": "ziehA15y8k/tables/tables_9_2.jpg", "caption": "Table 4: The top-8 important features for subgraph selection with T = 5 on Weibo dataset, reflected by AdRumor-RL expert samples and the learned reward function, respectively. The overlapping features are marked with the gray background. The features are described in Appendix C.", "description": "This table shows the top 8 most important features for subgraph selection in the Adversarial Rumor-RL attack model, as determined by both expert samples and the learned reward function.  Features are categorized into \"Source Subgraph\", \"Destination Subgraph\", and a few additional features. Overlapping features between the expert sample analysis and the reward function are highlighted with a gray background. More details on the features themselves are available in Appendix C of the paper.", "section": "3.3 Improvement mechanism"}, {"figure_path": "ziehA15y8k/tables/tables_14_1.jpg", "caption": "Table 3: The test accuracy decline (%) of the GCN rumor detector with T = 5 on Weibo dataset. The first row displays the attack method. The first column is the way to enhance the robustness with adversarial samples. The column under w/o Att. reflects test accuracy without attacks, while results under other columns reflect accuracy decline. The second row (w/o Def.) shows the accuracy (decline) without any defense method. Boldfaced font and * mean the best performance and the runner-up among all methods respectively.", "description": "This table presents the test accuracy decline of a Graph Convolutional Network (GCN) rumor detector on the Weibo dataset under different attack scenarios. It shows the impact of using adversarial samples generated by various methods to improve robustness, along with comparison to the results without using adversarial training and data augmentation.", "section": "Experiments"}, {"figure_path": "ziehA15y8k/tables/tables_15_1.jpg", "caption": "Table 3: The test accuracy decline (%) of the GCN rumor detector with T = 5 on Weibo dataset. The first row displays the attack method. The first column is the way to enhance the robustness with adversarial samples. The column under w/o Att. reflects test accuracy without attacks, while results under other columns reflect accuracy decline. The second row (w/o Def.) shows the accuracy (decline) without any defense method. Boldfaced font and * mean the best performance and the runner-up among all methods respectively.", "description": "This table presents the test accuracy decline of a GCN rumor detector on the Weibo dataset with different defense methods against various attack strategies. The results show the accuracy decline when no defense is applied and when various robustness enhancement techniques are used.  The table also highlights the best-performing method(s) for each attack type.", "section": "Experiments"}, {"figure_path": "ziehA15y8k/tables/tables_16_1.jpg", "caption": "Table 7: This table outlines the time complexity and runtime of the MoE-BiEntIRL model proposed herein, alongside two baseline models. Time complexity is delineated across three principal phases: (I) interaction, (II) reward acquisition, and (III) policy update. The reported total running time denotes the average duration of a single episode. Additionally, specific attention is given to the runtime of reward acquisition phase for clarity and comprehensive evaluation. The runtime of experiments on Weibo and Pheme is displayed with T=5 and N=3.", "description": "This table compares the time complexity and runtime of three models: MoE-BiEntIRL, Apprenticeship, and EntIRL.  The comparison is broken down into three phases: interaction, reward acquisition, and policy update.  The table shows the time complexity for each phase and the total runtime for experiments performed on Weibo and Pheme datasets with specific parameter values (T=5 and N=3).", "section": "Experiments"}]