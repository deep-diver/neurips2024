[{"figure_path": "Y13gSfTjGr/tables/tables_15_1.jpg", "caption": "Table 1: Model configurations for scaling law experiments. We provide an overview of the model sizes and hyperparameters for the different models in the scaling experiments.", "description": "This table lists the configurations of the different transformer models used in the scaling experiments. For each model size (33M, 53M, ..., 360M), it shows the values of several hyperparameters: the embedding dimension (d_model), the number of layers (n_layers), the feed-forward network size (ffw_size), the key/value vector size (kv_size), and the number of attention heads (n_heads). These hyperparameters directly impact the model's capacity and computational cost.", "section": "A.1 Overview"}, {"figure_path": "Y13gSfTjGr/tables/tables_16_1.jpg", "caption": "Table 1: Model configurations for scaling law experiments. We provide an overview of the model sizes and hyperparameters for the different models in the scaling experiments.", "description": "This table lists the configurations of various transformer models used in the scaling experiments.  Each model is identified by its total number of parameters, and then various architectural hyperparameters are listed: The dimensionality of the model's embedding, the number of layers, the feedforward network size, the key/value size, and the number of attention heads.", "section": "A.1 Overview"}, {"figure_path": "Y13gSfTjGr/tables/tables_16_2.jpg", "caption": "Table 3: Model configurations for larger runs. The batch size and learning rate for the 1B model tokens were estimated using DeepSeek scaling laws for 100B tokens. The two values for BS and the total steps of the 1B model distinguish the runs (100B,460B). For the 8B model, the architecture is identical to Llama3 and the batch size was set according to the available GPU limits on our cluster at the time of running experiments.", "description": "This table presents the hyperparameters used for training the 1B and 8B parameter models.  Note that the batch size and learning rate for the 1B model were determined using DeepSeek scaling laws. The different values for batch size and total steps reflect experiments with different token counts (100B and 460B). The 8B model architecture follows that of Llama3, with the batch size adjusted to match the available GPU resources.", "section": "A.2 Setup for the 1B and 8B Runs"}, {"figure_path": "Y13gSfTjGr/tables/tables_25_1.jpg", "caption": "Table 4: Final evaluation results after 100B tokens. Both cosine and the cooldown schedules have comparable final numbers, with only slight differences for certain benchmarks.", "description": "This table presents the final evaluation results obtained after training a 1B parameter model on 100B tokens.  It compares the performance of four different learning rate schedules: cosine decay to 10% of the maximum learning rate, cosine decay to 0, a cooldown schedule using a 1-sqrt decay function (with 20% of the steps allocated to the cooldown), and a linear cooldown (also with 20% of steps). The metrics evaluated are an aggregated score and several individual benchmarks including MMLU, ARC, OpenBookQA, PIQA, HellaSwag, CommonSenseQA, SIQA, and Winogrande.", "section": "3.3 Scaling Up: 1B and 8B Models"}, {"figure_path": "Y13gSfTjGr/tables/tables_25_2.jpg", "caption": "Table 5: Final evaluation results after 460B tokens. The findings of Table 4 transfer to much longer training runs with 460B tokens, where the performances of cosine and cooldowns match well. Notably, longer cooldowns do not necessarily improve the metrics (e.g. going from 5% to 20%).", "description": "This table presents the final evaluation results obtained after training language models with 460B tokens using various cooldown schedules. It compares the aggregated scores and individual benchmark results (MMLU, ARC, OpenBookQA, PIQA, HellaSwag, CommonSenseQA, SIQA, Winogrande) for different cooldown lengths (5%, 10%, 20%) and a cosine schedule (decay to 0). The results show that while the performance of cosine and cooldown schedules are comparable, longer cooldown durations do not necessarily lead to better performance. This finding supports the proposed cooldown schedule as a practical alternative to the more computationally expensive cosine schedule.", "section": "3.3 Scaling Up: 1B and 8B Models"}]