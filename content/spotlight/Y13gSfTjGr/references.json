{"references": [{"fullname_first_author": "Hoffmann", "paper_title": "Training compute-optimal large language models", "publication_date": "2022-12-01", "reason": "This paper introduced the concept of compute-optimal training for LLMs, which heavily influenced the current work's investigation into alternative training schedules."}, {"fullname_first_author": "Kaplan", "paper_title": "Scaling laws for neural language models", "publication_date": "2020-01-01", "reason": "This foundational paper established the scaling laws for language models, which provided the theoretical basis for the current work's scaling experiments."}, {"fullname_first_author": "Touvron", "paper_title": "Llama: Open and efficient foundation language models", "publication_date": "2023-02-23", "reason": "This paper introduced the Llama model and its training techniques, influencing the current work's choice of model architecture and training recipes."}, {"fullname_first_author": "Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper demonstrated the capabilities of large language models in few-shot learning, which is a key motivation for the current work's exploration of efficient training methods."}, {"fullname_first_author": "Radford", "paper_title": "Improving language understanding by generative pre-training", "publication_date": "2018-01-01", "reason": "This seminal paper introduced the GPT model and generative pre-training, which laid the groundwork for many subsequent large language models, including those used in the current work."}]}