[{"type": "text", "text": "What type of inference is planning? ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Miguel L\u00e1zaro-Gredilla Li Yang Ku Kevin P. Murphy Dileep George Google Deepmind {lazarogredilla, liyangku, kpmurphy, dileepgeorge}@google.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Multiple types of inference are available for probabilistic graphical models, e.g., marginal, maximum-a-posteriori, and even marginal maximum-a-posteriori. Which one do researchers mean when they talk about \u201cplanning as inference\u201d? There is no consistency in the literature, different types are used, and their ability to do planning is further entangled with specific approximations or additional constraints. In this work we use the variational framework to show that, just like all commonly used types of inference correspond to different weightings of the entropy terms in the variational problem, planning corresponds exactly to a different set of weights. This means that all the tricks of variational inference are readily applicable to planning. We develop an analogue of loopy belief propagation that allows us to perform approximate planning in factored-state Markov decisions processes without incurring intractability due to the exponentially large state space. The variational perspective shows that the previous types of inference for planning are only adequate in environments with low stochasticity, and allows us to characterize each type by its own merits, disentangling the type of inference from the additional approximations that its practical use requires. We validate these results empirically on synthetic MDPs and tasks posed in the International Planning Competition. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "There are many kinds of probabilistic inference, such as marginal, maximum-a-posteriori (MAP), or marginal MAP (MMAP) that are used in the planning as inference literature (Attias, 2003; Levine, 2018; Cui et al., 2019; Palmieri et al., 2022; Wu and Khardon, 2022). In this work we show that planning is a distinct type of inference, and that under stochastic dynamics does not correspond exactly with any of the above methods. Furthermore, we show how to rank the above methods in terms of quality as it pertains to planning. ", "page_idx": 0}, {"type": "text", "text": "Our approach is based on a variational perspective, which allows direct comparison between different inference types, and to develop analogues of existing approximate inference algorithms for this \u201cplanning inference\u201d task. Given the \u201cflat\u201d Markov Decision Process (MDP) from Fig 1[Left], we show that planning inference provides the same (exact) results as value iteration. Using an analogue of loopy belief propagation (LBP), we show how to apply approximate planning inference to the factored MDP in Fig. 1[Right], which has an exponentially large state space, and for which exact solutions are no longer tractable. Under moderate stochasticity in the dynamics, we show that this approximate planning inference might be superior to other more established types of inference. ", "page_idx": 0}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "2.1 Markov Decision processes (MDPs) and notation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "A finite-horizon Markov decision process (MDP) is a tuple $(\\mathcal{X},\\mathcal{A},p(x_{1}),\\mathcal{P},\\mathcal{R},T)$ , where $\\mathcal{X}$ is the state space, $\\boldsymbol{\\mathcal{A}}$ an action space with cardinality $N_{a}$ , $p(x_{1})$ the starting state distribution, $\\mathcal{P}$ the transition probabilities $P(x_{t+1}|a_{t},x_{t})$ (i.e., the dynamics of the process), $R_{t}(x_{t},a_{t},x_{t+1})$ the reward for transitioning from $x_{t}$ to $x_{t+1}$ under action $a_{t}$ at time step $t$ , and $T$ is the horizon. For simplicity of exposition we will consider discrete states and actions, but analogous results apply in the continuous case. Solving an MDP corresponds to finding a policy $\\pi_{t}(a_{t}|x_{t})$ that maximizes the expectation of (a function of) the sum of the reward at each time step. The optimal policy can be different at each time step, i.e., non-stationary. We do not use a discount factor, but it can be trivially included in the reward function, since it is also non-stationary. Policies, states and actions at all time steps will be noted $\\pi\\equiv\\{\\pi_{t}(a_{t}|x_{t})\\}_{t=1}^{T-1}$ , $\\pmb{x}\\equiv\\{x_{t}\\}_{t=1}^{T}$ and $\\pmb{a}\\equiv\\{a_{t}\\}_{t=1}^{T-1}$ . ", "page_idx": 0}, {"type": "image", "img_path": "TXsRGrzICz/tmp/f544d9c424cbf1dd9cb67f4906adfd8557eac59e7c32b03a410f44e7590e4fcb.jpg", "img_caption": ["Figure 1: Factor graphs: [Left] Standard MDP [Right] Factored MDP with sparse factor connectivity. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In a state-factored MDP (factored MDP for short), each state $x_{t}$ factorizes into $N_{e}$ r.v. or entities $\\{x_{t}^{(i)}\\}_{i=1}^{N_{e}}$ , each with cardinality $N_{s}$ , so it has an exponentially large state space of size $N_{s}^{N_{e}}$ . Transitions factorize as $\\begin{array}{r}{P(x_{t+1}|a_{t},x_{t})=\\prod_{i=1}^{N_{e}}P(x_{t+1}^{(i)}|x_{t}^{\\mathrm{pa}(i)},a_{t})}\\end{array}$ , where xtpa(i)is the subset of xt on which $x_{t+1}^{(i)}$ depends, which we will assume to be small to allow for a tabular definition of the transition without exponential cost $({\\mathsf{p a}}(i)$ stands in for \u201cparents of $i^{\\bullet}$ ). For simplicity of notation, we will assume that the reward of factored MDPs at each time step depends only on the current state, and for tractability, that it can be decomposed in multiple additive subterms $\\begin{array}{r}{R_{t}(x_{t})=\\sum_{i=N_{e}+1}^{N_{e}+N_{r}}R_{t}(x_{t}^{\\mathrm{pa}(i)})}\\end{array}$ where $\\boldsymbol x_{t}^{\\mathrm{pa}(i)}$ is a small subset of $x_{t}$ on which the $(i-N_{e})$ -th reward depen ds, for a total of $N_{r}$ reward subterms. As before, this allows for a compact tabular representation of the reward. Including additional dependencies in the reward (on actions, or on the next state of an entity) is straightforward. ", "page_idx": 1}, {"type": "text", "text": "For a more comprehensive introduction to MDPs, refer to (Puterman, 2014; Sutton, 2018). ", "page_idx": 1}, {"type": "text", "text": "2.2 Variational inference ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Let\u2019s consider running different types of inference in the unnormalized1 factor graph shown in Fig. 1[Left]. Marginal inference would compute the sum over all the $x,a$ configurations (the partition function); MAP inference would compute the maximizing configuration (the posterior mode); and marginal MAP (MMAP) inference (see Liu and Ihler, 2013) would find the assignment of $\\textbf{\\em a}$ that maximizes the summation over $\\textbf{\\em x}$ conditional on that $\\textbf{\\em a}$ . Although marginal, MAP, and MMAP inference are distinct, a lot is shared. They all target a \u201cquantity of interest\u201d (e.g., partition function, maximum probability state, best conditional partition function); they all produce a distribution as a result of inference (respectively, full posterior, delta at the mode, best conditional posterior); na\u00efve computation requires a number of summations and/or maximizations exponential in the number of variables; and finally, they can all be represented as a variational inference (VI) problem.2 The VI representation naturally leads to efficient message-passing solutions and approximate inference algorithms, as we show below. ", "page_idx": 1}, {"type": "text", "text": "For the factor graph $f({\\boldsymbol{x}},{\\boldsymbol{a}})$ from Fig. 1[Left], the VI problem3 is $\\begin{array}{r}{\\operatorname*{max}_{q(\\mathbf{x},a)}\\langle\\log f(\\mathbf{x},a)\\rangle_{q(\\mathbf{x},a)}+}\\end{array}$ $H_{q}^{\\mathrm{type}}({\\pmb x},{\\pmb a})$ where $q(x,a)$ is an arbitrary variational distribution over the variables of the factor ", "page_idx": 1}, {"type": "text", "text": "Table 1: Different types of inference from a variational perspective, including a proper \u201cplanning\u201d inference type. They all share the same energy term $E_{\\lambda}(\\pmb q)$ defined in Eq. (3), and differ only in the entropy term. The closed-form expressions provide the optimal value of the bound, but are generally intractable. The general tractability of the bound maximization for MDPs is marked in the Tr column. All bounds are monotonically related, and listed in descending order, except the last two, whose relative ordering only applies when dynamics are deterministic. See Section 4.1 for details. ", "page_idx": 2}, {"type": "table", "img_path": "TXsRGrzICz/tmp/bf81c78ae8ec67fff37b816ea1d3466413bc14d9b0564ceeb48466be9e589a9e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "graph, the term $-\\langle\\log f({\\pmb x},{\\pmb a})\\rangle_{q({\\pmb x},{\\pmb a})}$ is known as the energy term, and $H_{q}^{\\mathrm{type}}(\\pmb{x},\\pmb{a})$ is a particular entropy choice that will determine the inference type. It is a standard result, (e.g., Jordan et al., 1999) that using the Shannon entropy $H_{q}(x,a)$ results in marginal inference. Setting it to zero (aka the zero-temperature limit) corresponds to MAP inference (Weiss et al., 2012; Sontag et al., 2011; Wainwright et al., 2005; Kolmogorov, 2005; Martins et al., 2015). Setting it to the conditional entropy $H_{q}({\\pmb x}|{\\pmb a})=H_{q}({\\pmb x},{\\pmb a})-H_{q}({\\pmb a})$ results in MMAP inference (Liu and Ihler, 2013). Note that the only difference across VI problems is the weighting of the entropy terms. ", "page_idx": 2}, {"type": "text", "text": "Despite the similarities, the computational complexity of these types of inference can differ significantly, even for tree-structured graphs. In particular, the entropy term for MMAP is not concave, and inference is NP-hard (Liu and Ihler, 2013), whereas for marginal and MAP inference the entropy term is concave (the energy term is always linear), and inference is polynomial. ", "page_idx": 2}, {"type": "text", "text": "3 Methods ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section we introduce our VI framework, which we will use to derive a novel linear programming formulation of planning problems, a novel value belief propagation (VBP) algorithm and a novel closed form (sampling-free) approach to determinization. ", "page_idx": 2}, {"type": "text", "text": "3.1 VI for standard MDPs ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The main quantity of interest in this paper is the best exponential utility, which we will refer to simply as the utility. Given an MDP with horizon $T$ and a risk parameter $\\lambda>0$ , the utility is defined as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle F_{\\lambda}^{\\mathrm{plaming}}=\\frac{1}{\\lambda}\\log\\operatorname*{max}_{\\pi}\\sum_{x,a}\\exp\\Big(\\lambda\\sum_{t=1}^{T-1}R_{t}(x_{t},a_{t},x_{t+1})\\Big)P(x_{1})\\prod_{t=1}^{T-1}P(x_{t+1}|a_{t},x_{t})\\pi_{t}(a_{t}|x_{t})}}\\\\ {{\\displaystyle\\qquad=\\frac{1}{\\lambda}\\operatorname*{max}_{\\pi}\\log\\langle\\exp(\\lambda R(x,a))\\rangle_{P(x|a)\\pi(a|x)},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\begin{array}{r}{P(\\pmb{x}|\\pmb{a})\\equiv P(\\pmb{x}_{1})\\prod_{t=1}^{T-1}P(\\pmb{x}_{t+1}|a_{t},\\pmb{x}_{t}),R(\\pmb{x},\\pmb{a})\\equiv\\sum_{t=1}^{T-1}R_{t}(x_{t},a_{t},x_{t+1}),}\\end{array}$ and $\\pi(\\pmb{a}|\\pmb{x})\\equiv$ $\\textstyle\\prod_{t=1}^{T-1}\\pi_{t}(a_{t}|x_{t})$ . ", "page_idx": 2}, {"type": "text", "text": "Observe that we can always set $\\lambda\\to0^{+}$ to recover the standard planning setting in which we seek the best expected additive reward, so here we are tackling a strictly more general case. To be precise, if we take the limit $\\begin{array}{r}{F_{\\lambda\\rightarrow0^{+}}^{\\mathrm{planning}}\\equiv\\operatorname*{lim}_{\\lambda\\rightarrow0^{+}}F_{\\lambda}^{\\mathrm{planning}}=\\operatorname*{max}_{\\pi}\\langle R(\\pmb{x},\\pmb{a})\\rangle_{P(\\pmb{x}|\\pmb{a})\\pi(\\pmb{a}|\\pmb{x})}}\\end{array}$ (Marthe et al., 2023). ", "page_idx": 2}, {"type": "text", "text": "The motivation for the introduction of $\\lambda$ is two-fold. On the one hand, by using a more general formulation of the reward, we can trade off between risk-neutral $\\left.\\lambda\\right.\\rightarrow0^{+}$ ) and risk-seeking $(\\lambda>0)$ ) ", "page_idx": 2}, {"type": "text", "text": "policies, adding a tunable parameter that makes the model more flexible, see (Marthe et al., 2023; F\u00f6llmer and Schied, 2011; Shen et al., 2014) for more details. On the other hand, it allows us to express the expected reward as a proper factor graph: note that Eq. (1) can be expressed as a product of factors involving not only the dynamics terms, but also the reward terms, allowing us to write it as $\\begin{array}{r}{F_{\\lambda}^{\\mathrm{planning}}=\\frac{1}{\\lambda}\\log\\operatorname*{max}_{\\pi}\\sum_{x,a}f(x,a)\\pi(a|x)}\\end{array}$ , where $f({\\boldsymbol{x}},{\\boldsymbol{a}})$ is the factor graph of Fig. 1[Left]. This factorization would not have been possible if we had simply used an additive reward. But at the same time, notice that we are not losing generality, since the additive reward case can be recovered by setting $\\lambda\\to0^{+}$ . Alternatively, we could have achieved a factorized model by introducing an additional latent \u201cselector\u201d variable connected to all the rewards, but this would complicate our upcoming formulation and analysis. Furthermore, this formulation allows us to encompass prior work on planning as inference that uses $\\lambda>0$ , such as (Levine, 2018). ", "page_idx": 3}, {"type": "text", "text": "We can turn this new quantity of interest, the utility, into the solution of a VI problem on the factor graph of Fig. 1[Left]. Crucially, the factor graph includes the known dynamics and rewards terms, but not any policy term, since the policy is the outcome of inference. ", "page_idx": 3}, {"type": "text", "text": "Theorem 1 (Variational formulation of planning). Given known dynamics $P(x_{t+1}|a_{t},x_{t})$ , an initial distribution $P(x_{1})$ and reward functions $R_{t}(x_{t},a_{t},x_{t+1})$ , the best exponential utility $F_{\\lambda}^{p l a n n i n g}$ from Eq. (1) can be expressed as the result of a concave variational optimization problem ", "page_idx": 3}, {"type": "equation", "text": "$$\nF_{\\lambda}^{p l a n n i n g}=\\operatorname*{max}_{q}F_{\\lambda}^{p l a n n i n g}(q);\\;\\;\\;\\;\\;\\;F_{\\lambda}^{p l a n n i n g}(q)=\\frac{1}{\\lambda}(-E_{\\lambda}(q)+H^{p l a n n i n g}(q))\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "with energy $E_{\\lambda}(\\pmb q)$ and entropy $H^{p l a n n i n g}(\\pmb q)$ terms ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle E_{\\lambda}({\\pmb q})=-\\langle\\log P({\\pmb x}_{1})\\rangle_{{\\pmb q}(x_{1})}-\\sum_{t=1}^{T-1}\\langle\\log P({\\scriptstyle x}_{t+1}|a_{t},x_{t})+\\lambda R_{t}(x_{t},a_{t},x_{t+1})\\rangle_{{\\pmb q}(x_{t+1},x_{t},a_{t})}}\\\\ {\\displaystyle H^{p l a m i n g}({\\pmb q})=H_{q}(x_{1})+\\sum_{t=1}^{T-1}H_{q}(x_{t+1}|a_{t},x_{t})}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where ${\\pmb q}\\equiv q({\\pmb x},{\\pmb a})$ is an arbitrary distribution over the space of states and actions. ", "page_idx": 3}, {"type": "text", "text": "Proof is in Appendix A. This entropy thus corresponds to \u201cplanning inference\u201d. The optimal policy at each time step corresponds to the optimal variational distribution $q(a_{t}|x_{t})$ . Table 1 lists the types of inference problems and their associated entropies (see Appendix E for their derivation and corresponding references). As we will discuss in Section 4, they display a monotonic ordering (in almost all cases). ", "page_idx": 3}, {"type": "text", "text": "The VI problem Eq. (2) reduces to the standard one when $\\lambda=1$ , and extends VI in a meaningful way in the presence of rewards, regardless of the type of inference used: rewards interact in an additive way when $\\lambda\\to0^{+}$ , rather than the default multiplicative (or more precisely, exponentiated summation) interaction of $\\lambda=1$ . Furthermore, it turns out that it is possible to take the $\\bar{\\lambda}\\overset{-}{\\to}0^{+}$ limit exactly, to obtain the dual LP formulation of an MDP (Puterman, 2014). ", "page_idx": 3}, {"type": "text", "text": "Corollary 1.1 (Additive limit). In the limit $\\lambda\\to0^{+}$ , the concave problem Eq. (2) becomes the following linear program $(L P)$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle=\\operatorname*{max}_{\\boldsymbol{q}}{F_{\\lambda\\to0^{+}}^{p l a n i n g}(\\boldsymbol{q})}=\\operatorname*{max}_{\\{\\boldsymbol{q}(\\boldsymbol{x}_{t},a_{t})\\}_{t=1}^{T-1}}\\sum_{t=1}^{T-1}\\langle R_{t}(x_{t},a_{t},x_{t+1})\\rangle_{P(x_{t+1}|a_{t},x_{t})q(x_{t},a_{t})}}\\\\ &{s.t.\\ q(x_{1})=P(x_{1});\\quad\\displaystyle\\sum_{a_{t+1}}^{}q(x_{t+1},a_{t+1})=\\sum_{x_{t},a_{t}}P(x_{t+1}|a_{t},x_{t})q(x_{t},a_{t})\\ \\forall_{t};}\\\\ &{\\quad q(x_{t})=\\sum_{a_{t}}q(x_{t},a_{t})\\ \\forall_{t};\\quad q(x_{t},a_{t})\\ge0\\ \\forall t,}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "$F_{\\lambda\\to0^{+}}^{p l a n n i n g}=\\operatorname*{max}_{\\pi}\\langle R(\\pmb{x},\\pmb{a})\\rangle_{P(\\pmb{x}|\\pmb{a})\\pi(\\pmb{a}|\\pmb{x})}.$ ", "page_idx": 3}, {"type": "text", "text": "See Appendix B for proof. ", "page_idx": 3}, {"type": "text", "text": "3.2 VI LP and VBP for factored MDPs ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Factored MDPs (e.g., Fig. 1[Right]) are loopy factor graphs with an exponentially large state space, so the previous approaches cannot be applied directly. An effective approximate marginal inference approach for this type of problem is loopy belief propagation (LBP). Since planning is now seen as a type of inference, we can create an analogue to LBP which we call value belief propagation (VBP). ", "page_idx": 4}, {"type": "text", "text": "Following LBP, we make two approximations to Eq. (2) to make it tractable. ", "page_idx": 4}, {"type": "text", "text": "First, we replace the variational distribution $\\pmb q$ with pseudo-marginals $\\tilde{\\pmb q}$ . Eqs. (3) and (4) never access the full joint $q(x,a)$ , but only the local marginals of each factor. Pseudo-marginals are the collection of such local distributions, consistent at each variable, but not necessarily marginals of any distribution. Just like $\\pmb q$ is defined in a convex region called the marginal polytope $\\mathcal{M}$ , $\\tilde{\\pmb q}$ is defined in an outer convex region called the local polytope $\\mathcal{L}$ that contains $\\mathcal{M}$ (Weller et al., 2014). ", "page_idx": 4}, {"type": "text", "text": "Second, we replace the entropy $H^{\\mathrm{planning}}(\\pmb q)$ with its Bethe approximation ", "page_idx": 4}, {"type": "equation", "text": "$$\nH_{\\mathrm{Bethe}}^{\\mathrm{plaming}}(\\tilde{q})=\\sum_{i=1}^{N_{e}}H_{q}(x_{1}^{(i)})+\\sum_{t=1}^{T-1}\\bigg(\\sum_{i=1}^{N_{e}}H_{q}(x_{t+1}^{(i)}|x_{t}^{\\mathrm{pa}(i)},a_{t})-\\sum_{i=1}^{N_{e}+N_{r}}I_{q}(x_{t}^{\\mathrm{pa}(i)})\\bigg)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\begin{array}{r}{I_{q}(x_{t}^{\\mathrm{pa}(i)})=\\sum_{k\\in\\mathrm{pa}(i)}H_{q}(x_{t}^{(k)})-H_{q}(x_{t}^{\\mathrm{pa}(i)})}\\end{array}$ is the mutual information of the parents of $x_{t+1}^{(i)}$ see Appendix C for details. This approximation is tractable as long as the factors (transition and rewards) are tractable, typically by connecting to a small number of parent variables. Note that the policy (which would connect to all the state variables in a time slice and introduce exponential cost) is not a factor in the graph. ", "page_idx": 4}, {"type": "text", "text": "The term $I_{q}(x_{t}^{\\mathrm{pa}(i)})$ is key. It always non-negative but neither concave nor convex in general and can be interpreted as the mutual information correcting the discrepancy between (a) the entropy of a collection of variables considered independently (as the output of the previous time step) and (b) the entropy of the same collection when considered jointly (as parents for the current time step). It makes the optimization problem harder, but also more accurate. ", "page_idx": 4}, {"type": "text", "text": "For non-factored MDPs, $I_{q}(x_{t}^{\\mathrm{pa}(i)})=0$ and $\\tilde{\\boldsymbol{q}}=\\boldsymbol{q}$ , so we recover Eq. (2). In general factored MDPs this is not true. We can still choose to ignore this correction to obtain a concave bound ", "page_idx": 4}, {"type": "equation", "text": "$$\nH_{\\mathrm{concave}}^{\\mathrm{planning}}(\\tilde{q})=\\sum_{i=1}^{N_{e}}H_{q}(x_{1}^{(i)})+\\sum_{t=1}^{T-1}\\sum_{i=1}^{N_{e}}H_{q}(x_{t+1}^{(i)}|x_{t}^{\\mathrm{pa}(i)},a_{t})\\geq H_{\\mathrm{Bethe}}^{\\mathrm{planning}}(\\tilde{q}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Then, the planning Bethe approximation of the variational bound and its concave upper bound are ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{F}_{\\lambda}^{\\mathrm{plaming}}=\\displaystyle{\\operatorname*{max}_{q}\\tilde{F}_{\\lambda}^{\\mathrm{plaming}}(\\tilde{q})}=\\displaystyle{\\operatorname*{max}_{q}\\frac{1}{\\lambda}(-E(\\tilde{q})+H_{\\mathrm{Bethe}}^{\\mathrm{plaming}}(\\tilde{q}))}\\ \\ s.t.\\ \\tilde{q}\\in\\mathcal{L}}\\\\ &{\\hat{F}_{\\lambda}^{\\mathrm{plaming}}=\\displaystyle{\\operatorname*{max}_{q}\\hat{F}_{\\lambda}^{\\mathrm{plaming}}(\\tilde{q})}=\\displaystyle{\\operatorname*{max}_{q}\\frac{1}{\\lambda}(-E(\\tilde{q})+H_{\\mathrm{concave}}^{\\mathrm{plaming}}(\\tilde{q}))}\\ \\ s.t.\\ \\tilde{q}\\in\\mathcal{L}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We see that $\\hat{F}_{\\lambda}^{\\mathrm{planning}}\\,\\geq\\,\\tilde{F}_{\\lambda}^{\\mathrm{planning}}$ and $\\hat{F}_{\\lambda}^{\\mathrm{planning}}\\,\\geq\\,F_{\\lambda}^{\\mathrm{planning}}$ . The former is trivial giv he negative term removed. The latter follows from (a) switching the optimization domain from $\\mathcal{M}$ to , which can only increase the value of the bound, and (b) Eq. (5) corresponds to Eq. (4), but with joint entropies over entities replaced with sums of the entropies, which is an upper bound. The fact that F\u02c6 \u03bbplanning(q\u02dc) is concave and upper bounds the exact utility has two advantages: it can be computed without local minima problems, and it is an admissible heuristic of the original utility, meaning that it can be used as a heuristic for algorithms that emit a certificate of optimality or infeasibility. ", "page_idx": 4}, {"type": "text", "text": "Lemma (Additive limit for factored MDPs). In the limit $\\lambda\\to0^{+}$ , the concave problem Eq. (7) becomes the following $V I L P$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\hat{F}_{\\lambda\\to0^{+}}^{p l a m i n g}=\\operatorname*{max}_{\\bar{q}}\\hat{F}_{\\lambda\\to0^{+}}^{p l a m i n g}(\\tilde{q})=\\operatorname*{max}_{\\bar{q}}\\sum_{t=1}^{T}\\sum_{i=N_{e}}^{N_{e}+N_{r}}\\langle R_{t}(x_{t}^{p a(i)})\\rangle_{q(x_{t}^{p a(i)})}}}\\\\ &{}&{s.t.\\quad q(x_{1}^{(i)})=P(x_{1}^{(i)})\\ \\forall i;\\quad\\tilde{q}\\in\\mathcal{L}}\\\\ &{}&{q(x_{t+1}^{(i)})=\\displaystyle\\sum_{x_{t}^{p a(i)},a_{t}}P(x_{t+1}^{(i)}|x_{t}^{p a(i)},a_{t})q(x_{t}^{p a(i)},a_{t})\\ \\forall x_{t+1}^{(i)},t,1\\leq i\\leq N_{e}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "which upper bounds the max. expected reward $\\begin{array}{r}{\\hat{F}_{\\lambda\\to0^{+}}^{p l a n n i n g}\\geq F_{\\lambda\\to0^{+}}^{p l a n n i n g}=\\operatorname*{max}_{\\pi}\\langle R(\\pmb{x},\\pmb{a})\\rangle_{P(\\pmb{x}|\\pmb{a})\\pi(\\pmb{a}|\\pmb{x})}.}\\end{array}$ Alternatively, the same expression can be obtained from Corollary 1.1 by relaxing the marginal polytope into the local polytope. Since it is a relaxation, the upper bounding is trivial. ", "page_idx": 5}, {"type": "text", "text": "To the best of our knowledge, this is a novel VI LP formulation and it can be used to tractably (over) estimate the optimal expected reward in factored MDPs. Similarities with (Koller and Parr, 1999; Guestrin et al., 2003) are only surface level, see Section 5. ", "page_idx": 5}, {"type": "text", "text": "$\\hat{F}_{\\lambda}^{\\mathrm{planning}}(\\tilde{q})$ ef fcraonm  bEe q.m (a6x)i ims izmeodr ew cithha llae cnoginnicg .s oClovnevr e(noire natnl y,L $\\lambda\\to0^{+}$ )s.t  lTihkee $\\tilde{F}_{\\lambda}^{\\mathrm{planning}}(\\tilde{q})$ $\\tilde{F}_{\\lambda}^{\\mathrm{planning}}(\\tilde{q})$ the Bethe free energy that motivates LBP, but with a different weighting of the local entropy terms. ", "page_idx": 5}, {"type": "text", "text": "Multiple works consider modifying the entropy weighting in LBP, usually with the aim of \u201cconcavifying\u201d the overall entropy term and developing convergent alternatives to LBP. In particular, (Hazan and Shashua, 2010) provides fixed-point message updates for arbitrary entropy weights. For the specific weighting of HBpleatnhnein $H_{\\mathrm{Bethe}}^{\\mathrm{planning}}(\\tilde{q})$ the message updates approach a singularity, which we will avoid by using $(1-\\epsilon)H_{\\mathrm{Bethe}}^{\\mathrm{planning}}(\\tilde{q}))+\\epsilon H_{\\mathrm{Bethe}}^{\\mathrm{marginal}}(\\tilde{q})$ . The resulting message passing algorithm updates are well-defined for any $\\epsilon>0$ , interpolate between \u201cplanning inference\u201d and marginal inference, and can get arbitrarily close to the former by making $\\overline{{\\epsilon}}\\rightarrow0^{+}$ . The use of $\\epsilon$ is not just a mathematical trick, but can be practically used to continuously anneal from LBP into VBP, which can improve convergence. See (Liu and Ihler, 2013) for an analogous technique with the same purpose. ", "page_idx": 5}, {"type": "text", "text": "VBP inherits many of the properties of LBP: the message updates are not guaranteed to converge, but if they do, they do so at a fixed point of Eq. (6). Convergence can be improved by the use of damping and annealing. The precise message updates for the general case are provided in Appendix D. ", "page_idx": 5}, {"type": "text", "text": "Computation associated with VBP scales as expected, where $N_{e},N_{r},N_{a},N_{s}$ have been defined in Section 2.1. Note that the derivation is straightforward. $\\mathcal{O}(T(\\sum_{i=1}^{N_{e}}N_{s}N_{s\\_i}^{|\\mathrm{pa}(\\mathrm i)|+1}{+}\\!\\sum_{i=N_{e}+1}^{N_{r}}N_{s}^{|\\mathrm{pa}(\\mathrm i)|}))$ Each VBP iteration involves computing message updates for each factor in the graph. The cost is dominated by the blue factors $N_{e}$ of them per time step) and green factors $N_{r}$ of them per time step) in Fig. 1[Right]. There are a total of $T$ time steps. And finally the number of possible configurations is $N_{a}N_{s}^{\\mathrm{|pa(i)|+1}}$ for blue factors and $N_{s}^{\\mathrm{|pa(i)|}}$ for green factors. ", "page_idx": 5}, {"type": "text", "text": "3.3 VBP for standard MDPs ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "It is instructive to look at the VBP updates for a standard, non-factored MDP. In this case, it is possible to take the limit $\\epsilon\\to0^{+}$ and get well-defined updates. For $\\lambda=1$ and a single reward at $T$ ", "page_idx": 5}, {"type": "text", "text": "Backward updates $\\centering\\begin{array}{l}{:m_{\\mathfrak{b}}(x_{T})=e^{R_{T}(x_{T})};\\ \\ m_{\\mathfrak{b}}(x_{t})=\\displaystyle\\operatorname*{max}_{a_{t}}Q(x_{t},a_{t});}\\\\ {:m_{\\mathsf{f}}(x_{1})=P(x_{1});\\ \\ m_{\\mathsf{f}}(x_{t+1})=\\displaystyle\\sum_{x_{t},a_{t}}p(x_{t+1}|x_{t},a_{t})\\delta_{a_{t},\\mathrm{argmax}_{a_{t}^{\\prime}}}Q(x_{t},a_{t}^{\\prime})m_{\\mathsf{f}}(x_{t})}\\\\ {:q(x_{t+1},x_{t},a_{t})\\propto m_{\\mathfrak{b}}(x_{t+1})p(x_{t+1}|x_{t},a_{t})\\delta_{a_{t},\\mathrm{argmax}_{a_{t}^{\\prime}}}Q(x_{t},a_{t}^{\\prime})m_{\\mathsf{f}}(x_{t})}\\end{array}$ Forward updates   \nOptimal dist. ", "page_idx": 5}, {"type": "text", "text": "where $\\begin{array}{r}{Q(x_{t},a_{t})=\\sum_{x_{t+1}}m_{\\mathsf{b}}(x_{t+1})p(x_{t+1}|x_{t},a_{t})}\\end{array}$ and $\\delta_{j,k}$ is a standard Kronecker delta that equals 1 when $j=k$ and 0 otherwise. Iterating these updates converges in a single backward and forward pass to the global optimum. The backward messages correspond to the value function (hence the name VBP), and the familiar intermediate quantity $Q(x_{t},a_{t})$ matches the Q-function. The forward messages correspond to occupancy probabilities under the optimal policy. Thus, in a non-factored MDP we recover the standard Bellman backups, implementing value iteration and providing the exact solution. The same happens, conceptually, in a factored MDP, but only approximately, with the forward messages helping to determine where the backward approximation should be more precise. ", "page_idx": 5}, {"type": "text", "text": "3.4 Determinization in hindsight ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The previous presentation implies that all VI tricks are now applicable to planning. As an example, we can show that determinization (Yoon et al., 2008), a technique from the planning literature to extend deterministic planning algorithms to stochastic domains, and that is usually computed via sampling, can be obtained in closed form by leveraging the variational formulation. ", "page_idx": 5}, {"type": "text", "text": "To be more precise, we can compute F \u03bbde=t. 1planning(for MDPs) and F\u02c6 \u03bbde=t. 1planning(for factored MDPs) planning problem is solved with a standard LP MAP relaxation (Sontag et al., 2011). Additionally we can prove that for factored MDPs F\u02c6 \u03bbpl=an1ning\u2264F\u02c6 \u03bbde=t. 1planning(i.e., the superiority of the bound introduced here wrt determinization in the case of factored MDPs). See Appendix G for further details. ", "page_idx": 6}, {"type": "text", "text": "4 The different types of inference and their adequacy for planning ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Ranking inference types for planning ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "As we show in Section 5, the term \u201cplanning as inference\u201d has been used in the literature to refer to different inference types, none of which corresponds, to the best of our knowledge, with the \u201cplanning inference\u201d from this work, which is exact. Table 1 associates each type of inference to a corresponding lower bound on its quantity of interest. Turns out that by inspecting the entropy term (since the energy is the same for all of them), we can also relate those lower bounds to one another for a given variational distribution $\\pmb q$ , resulting in $\\begin{array}{r l}&{F_{\\lambda}^{\\mathrm{MAP}}(\\pmb{q})}\\\\ &{F_{\\lambda}^{\\mathrm{marginal}^{\\mathrm{U}}}(\\pmb{q})\\overset{\\vee}{\\longrightarrow}F_{\\lambda}^{\\mathrm{MMAP}}(\\pmb{q})\\leq F_{\\lambda}^{\\mathrm{planning}}(\\pmb{q})\\leq F_{\\lambda}^{\\mathrm{marginal}}(\\pmb{q}).}\\end{array}$ This in turn means that for the optimal variational distribution of each type of inference we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F_{\\lambda}^{\\mathrm{MAP}}\\,\\,\\,}\\\\ &{F_{\\lambda}^{\\mathrm{marginal}^{\\mathrm{U}}}\\bigg\\}\\leq F_{\\lambda}^{\\mathrm{MMAP}}\\leq F_{\\lambda}^{\\mathrm{planning}}\\leq F_{\\lambda}^{\\mathrm{marginal}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "See Appendix F for proof. VI aims to maximize a lower bound on the quantity of interest, with tighter bounds generally indicating better performance. Since MMAP is, among the lower bounds, the tightest, it follows that MMAP inference is expected to be no worse and potentially better than all other common types of inference. However, as noted in Section 2.2, MMAP inference is particularly hard, even in trees, meaning that in the case of a non-factorial MDP like the one in Fig. 1[Left], $F_{\\lambda}^{\\mathrm{MMAP}}$ raiycs t tlioyn  tcmroaacmxtiapmbultieaz,be  leietv. e wnWr tt ,t u bgwuhet   acwlali ntt hhtoreau tco ttgahubelaryr  aqcnuotaemnetpsi tuiotefes  ,fi siin ntdchilneu gld oitnhwege  rto hpbetoi uomnnadel $F_{\\lambda}^{\\mathrm{planning}}$   \n$F_{\\lambda}^{\\mathrm{MMAP}}(q)\\leq\\dot{F}_{\\lambda}^{\\mathrm{MMAP}}$ $\\pmb q$   \nvalue. Thus, among the common inference types, MMAP seems a better choice, but it is either intractable or, if using VI, can run into local minima problems. This seems more acceptable in the factored MDP case, but it is disappointing that the problem persists for standard, non-factored MDPs. ", "page_idx": 6}, {"type": "text", "text": "4.2 The stochasticity of the dynamics is key ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The energy term Eq. (3), which is common to all inference methods, contains subterms $\\langle\\log P(x_{t+1}|a_{t},x_{t})\\rangle_{q(x_{t+1},x_{t},a_{t})}^{-}$ (and $\\langle\\log P(x_{1})\\rangle_{q(x_{1})}$ for the first state). When dynamics are deterministic (which we assume to also imply that $P(x_{1})$ is deterministic, i.e., the first state is known), this forces the optimal variational conditional to be ${q}(x_{t+1}|a_{t},x_{t})=P(x_{t+1}|a_{t},x_{t})$ (and $q(x_{1})=P(x_{1})$ for the first state), since any other choice would make those subterms, and therefore the bound, $-\\infty$ . This affects the relationships of the quantities of interest, which are now (proof in Appendix F): ", "page_idx": 6}, {"type": "equation", "text": "$$\nF_{\\lambda}^{\\mathrm{marginal}^{\\mathrm{U}}}\\le F_{\\lambda}^{\\mathrm{MAP}}=F_{\\lambda}^{\\mathrm{MMAP}}=F_{\\lambda}^{\\mathrm{planning}}\\le F_{\\lambda}^{\\mathrm{marginal}},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "and justifies the use of MAP and MMAP inference as planning when dynamics are deterministic. When using approximate inference, if dynamics are close to deterministic, it might make more sense to choose the type of inference based on the quality of the approximation, rather than its tightness. If dynamics are stochastic, the suboptimality of MMAP can be explained as a lack of reactivity to the environment. Indeed, if we reduce the planning problem to a non-reactive policy $\\dot{\\pi}(a_{t}|x_{t})\\stackrel{.}{=}\\pi(a_{t})$ we recover MMAP inference as optimal. We test this experimentally in Section 6 and further expand on it in Appendix H.2. MAP has the same problem, but additionally lacks integration over observation sequences (\u201ctrajectories\u201d). Even with deterministic dynamics, marginal inference might not produce good utility estimates, but its action posterior will be proportional to the reward of the action sequence, so if we additionally assume $\\exp(\\lambda R({\\pmb x}))\\in\\{0,1\\}$ (i.e., pure planning where we want to attain any of a subset of states), it will produce optimal planning choices. ", "page_idx": 6}, {"type": "text", "text": "5 Related work ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "As stated, the meaning of \u201cplanning as inference\u201d is uneven across the literature. (Toussaint and Storkey, 2006) introduce the policy in the MDP factor graph and maximize the likelihood wrt to its parameters using EM. This is an exact approach, although it is more appropriate to say that it is planning as learning rather than a type of inference, since the EM process updates the parameters of the factor graph and inference typically operates on a graph with fixed parameters. (Levine, 2018) is a well-known reference that uses MAP inference to plan. It only considers the multiplicative setting $\\lambda=1$ . Acknowledging the limitations of MAP inference under stochastic dynamics, it suggests a structured variational distribution where $q(x_{t+1}|x_{t},a_{t})=P(x_{t+1}|x_{t},a_{t})$ . Using this choice in Eq. (2) results in entropy terms cancelling with energy ones, which results in a simpler problem. Still, the rigid variational distribution prevents the optimal solution to the original problem from being found in general. In contrast, our entropy term allows for an unrestricted variational distribution. (Cui et al., 2015) introduces ARollout, which can be seen as running a single-forward-pass LBP to approximate marginal inference for each possible initial action, and then choosing the highest scoring initial action5, and is applicable to factored MDPs. In the follow-up works (Cui and Khardon, 2016; Cui et al., 2019) the authors develop conformant SOGBOFA, which approximates marginal-MAP inference by using ARollout in an inner loop and gradient descent to optimize over the action prior in an outer loop. A number of refinements are added for superior performance. This is a strong baseline and was the runner-up in the international probabilistic planning competition (IPPC) 2018, which agrees with our analysis from Section 4. (Lee et al., 2014; Lee et al., 2016) provide initial results on the connection between conformant planning and MMAP inference. Most works, such as (Attias, 2003) choose MAP inference for planning. ", "page_idx": 7}, {"type": "text", "text": "Two frameworks (Palmieri et al., 2022; Wu and Khardon, 2022) have been recently introduced to analyze planning from a message-passing perspective. The former analyzes six update rules and their qualitative effect on the plans; the latter focuses on disentangling the inference direction6 (either forward \u2014from causes to outcomes\u2014 or backward \u2014from outcomes to causes\u2014) from the approximation type. This work provides two novel message-passing algorithms for factored MDPs: MFVI (mean field VI) and CSVI (collapsed state VI), using the planning tasks from IPPC 2011 as benchmark. We will compare with their results in Section 6. ", "page_idx": 7}, {"type": "text", "text": "Influence diagrams (Matheson, 2005; Shachter, 2007) are used to represent general decision problems, and various approximate inference approaches have been developed, e.g., (Lee et al., 2018; Lee et al., 2020). Closest to our work, (Cheng et al., 2013; Chen et al., 2015) tackle graph-based MDPs, similar to factored MDPs, but with a factorized action space: multiple actions are taken at each time step, each locally affecting a single entity. This locality results in additional efficiencies, so direct application to a state-only-factored MDPs would still result in exponential cost. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "LP formulations for the solution non-stationary, finite-horizon MDPs have received significant attention over the last decade (e.g., Kumar et al., 2015; Bhattacharya and Kharoufeh, 2017; Altman, 2021; Bhat et al., 2023), but they lack a variational perspective and do not generalize easily to handle state-factored MDPs. The LPs in (Koller and Parr, 1999; Guestrin et al., 2003; Malek et al., 2014) on the other hand do handle factored MDPs and have a closer connection to our work. The problem setup is slightly different, infinite-horizon MDPs with a stationary policy in their case vs our finite horizon MDPs, which allows us to use local non-stationary policies. More importantly, their solution approximately decomposes the value function into a linear combination of user-provided basis functions. The quantity and quality of these will determine the quality of the approximation, introducing additional design decisions. In VI planning we use the structure of the graph to determine the approximation, which is multiplicative over the value of backward messages, rather than additive. ", "page_idx": 8}, {"type": "text", "text": "6 Empirical validation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Synthetic MDPs We generate 5, 000 synthetic factored MDPs structured as in Fig. 1[Right] with random dynamics, all-or-nothing reward at the last time step, and controlled normalized entropies, defined as $\\begin{array}{r}{H_{\\mathrm{MDP}}=\\frac{\\sum_{i,x_{t},a_{t}}H(p(\\overline{{x_{t+1}^{(i)}|x_{t},a_{t}))}}}{N_{e}N_{a}N_{s}^{N_{e}}\\log N_{s}}\\in[0,1]}\\end{array}$ . See Appendix H.1 for more details. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "They are purposefully small so that we can compute $F^{\\mathrm{marginal}^{\\mathrm{U}}}$ , $F^{\\mathrm{MAP}}$ , $F^{\\mathrm{MMAP}}$ , $F^{\\mathrm{planning}}$ exactly, even though they are intractable in general. We also compute $\\tilde{F}^{\\mathrm{marginal}}$ (tractable), and $\\tilde{F}^{\\mathrm{MMAP}}$ (tractable bound, generally intractable optimization), which correspond to ARollout (Cui et al., 2015) and optimal SOGBOFA-LC\\* (Cui et al., 2019), respectively. Finally, we include VBP (tractable, imperfect optimization of $\\tilde{F}^{\\mathrm{planning}}(q))$ and the tractable VI LP F\u02c6 \u03bbpl=an0ningand VI CVX F\u02c6 $\\hat{F}_{\\lambda=1}^{\\mathrm{planning}}$ ", "page_idx": 8}, {"type": "text", "text": "Fig. 2 shows the effect of stochasticity in the estimation of the utility [Left] and the next best action [Right]. For high stochasticity, VBP, even if approximate, dominates all other types of inference. The concave upper bounds $\\hat{F}_{\\lambda=0}^{\\mathrm{planning}}$ and $\\hat{F}_{\\lambda=1}^{\\mathrm{planning}}$ also improve over exact MMAP but not as much as VBP. For low stochasticity, exact MAP and MMAP dominate VBP. The (intractably optimal) SOGBOFA-LC\\* remains close to the exact MMAP. These results agree well with the theory and observations laid out in Section 4. We see good correlation between the accuracy of the utility estimation and the quality of the planning choices ([Left] and [Right] panels). ARollout and exact marginal seem to be an exception to this; this is explained in Section 4.2: for pure planning problems, with low stochasticity both methods are a constant away from the right utility and make good choices. ", "page_idx": 8}, {"type": "text", "text": "Reactivity avoidance We craft a multi-entity MDP in which the agent controls the level of reactivity (see Section 4.2) needed to solve the environment, but is penalized for lower ones. VBP keeps the reactivity at a maximum, to achieve a reward of 1. SOGBOFA-LC\\* \u201caware\u201d that it cannot plan reactively (despite replanning), takes step to reduce it, getting a reward of 0.33. See Appendix H.2. ", "page_idx": 8}, {"type": "text", "text": "International probabilistic planning competition tasks (IPPC) We follow (Wu and Khardon, 2022) and compare on the same tasks and with the same methods. We use the 6 different domains from IPPC2011, each with 10 instances (factored MDPs) of increasing difficulty, with given dynamics and (stationary) rewards, 40-step episodes, and mildly stochastic dynamics. As baselines, we use MFVI-Bwd (Wu and Khardon, 2022), CSVI-Bwd (Wu and Khardon, 2022), ARollout ( F\u02dc \u03bbm=ar0gi see Cui et al., 2015), and SOGBOFA-LC ( F\u02dc \u03bbM=M0AP, see Cui et al., 2019). We provide details about LP these competing methods in Appendix H.3. From our proposed variational framework, we use7 VI ( F\u02c6 \u03bb=0 $(\\hat{F}_{\\lambda=0}^{\\mathrm{planning}})$ , and $\\mathrm{VBP^{8}}$ . $(\\tilde{F}_{\\lambda\\approx0}^{\\mathrm{planning}})$ H Fig. 3 shows the average cumulative reward for all domains and methods. Four domains are highly deterministic $[H_{\\mathrm{MDP}}<0.05)$ , but planning inference manages to be competitive wrt the best baselines. ", "page_idx": 8}, {"type": "image", "img_path": "TXsRGrzICz/tmp/5ed257765ab25cdb460ca16fda8acb96377a58d3b6f74caa651927aa2583a447.jpg", "img_caption": ["Figure 3: Cumulative rewards on 6 problem domains from the ICAPS 2011 IPPC. A small horizontal jitter was introduced in all data points for visual clarity. Each cumulative reward is averaged over 30 simulations per instance. Datasets are ordered from left to right and top to bottom by increasing normalized entropy levels. Only the last two have a significant stochasticity level ${>}5\\%$ . "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "The other two are Game of Life and SysAdmin, which have an average $H_{\\mathrm{MDP}}$ of 0.18 and 0.23 respectively. We notice a significant advantage of our proposals wrt the most sophisticated method, sStOocGhBasOtiFciAt-yL (Cs $(\\tilde{F}_{\\lambda=0}^{\\mathrm{MMAP}})$ )n.  4T)h.i sE ilse vcaotnosriss tiesn tw welilt hk noourw enx fpoerc titasti cohn aollf eMngMinAg Pr edwegarraddsi (nCg uwi iteth  ailn.c, r2e0as1e5d) and the only one for which ARollout performs noticeably worse. In this domain, VBP manages to match or exceed SOGBOFA-LC on most instances. Overall, we observe that VBP is more consistent across varying stochasticities, matching the performance of the best method for each dataset. None of these domains reach the larger stochasticity levels shown in Fig. 2 where VBP dominates. VI LP also performs generally well, although not as well as VBP due to the missing mutual information mentioned in Section 3.2. See Appendix H.3 for details. ", "page_idx": 9}, {"type": "text", "text": "7 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The variational framework offers a powerful tool to analyze and understand how different existing types of inference approximate planning, the key role of stochasticity, what the ideal type of inference for planning is, and how to design new approximations. We hope that the introduced VI perspective will further the understanding of existing methods and lead to novel planning algorithms. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank Roni Khardon, Junkyu Lee, and the anonymous referees for their feedback on an earlier version of this paper, which helped us to improve its clarity and presentation. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Altman, Eitan (2021). Constrained Markov decision processes. Routledge.   \nAttias, Hagai (2003). \u201cPlanning by probabilistic inference\u201d. In: International workshop on artificial intelligence and statistics. PMLR, pp. 9\u201316.   \nBareinboim, Elias and Judea Pearl (2016). \u201cCausal inference and the data-fusion problem\u201d. In: Proceedings of the National Academy of Sciences 113.27, pp. 7345\u20137352.   \nBhat, Sanjay P, Veeraruna Kavitha, Nandyala Hemachandra, et al. (2023). \u201cFinite-Horizon Constrained MDPs With Both Additive And Multiplicative Utilities\u201d. In: arXiv preprint arXiv:2303.07834.   \nBhattacharya, Arnab and Jeffrey P Kharoufeh (2017). \u201cLinear programming formulation for nonstationary, finite-horizon Markov decision process models\u201d. In: Operations Research Letters 45.6, pp. 570\u2013574.   \nChen, Feng, Qiang Cheng, Jianwu Dong, Zhaofei Yu, Guojun Wang, and Wenli Xu (2015). \u201cEfficient approximate linear programming for factored MDPs\u201d. In: International Journal of Approximate Reasoning 63, pp. 101\u2013121.   \nCheng, Qiang, Qiang Liu, Feng Chen, and Alexander T Ihler (2013). \u201cVariational planning for graph-based MDPs\u201d. In: Advances in Neural Information Processing Systems 26.   \nCui, Hao, Thomas Keller, and Roni Khardon (2019). \u201cStochastic planning with lifted symbolic trajectory optimization\u201d. In: Proceedings of the International Conference on Automated Planning and Scheduling. Vol. 29, pp. 119\u2013127.   \nCui, Hao and Roni Khardon (2016). \u201cOnline Symbolic Gradient-Based Optimization for Factored Action MDPs.\u201d In: IJCAI, pp. 3075\u20133081.   \nCui, Hao, Roni Khardon, Alan Fern, and Prasad Tadepalli (2015). \u201cFactored MCTS for large scale stochastic planning\u201d. In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 29. 1.   \nCui, Hao Jackson, Radu Marinescu, and Roni Khardon (2018). \u201cFrom stochastic planning to marginal MAP\u201d. In: Advances in Neural Information Processing Systems 31.   \nF\u00f6llmer, Hans and Alexander Schied (2011). Stochastic finance: an introduction in discrete time. Walter de Gruyter.   \nGeffner, Hector and Blai Bonet (2022). A concise introduction to models and methods for automated planning. Springer Nature.   \nGuestrin, Carlos, Daphne Koller, Ronald Parr, and Shobha Venkataraman (2003). \u201cEfficient solution algorithms for factored MDPs\u201d. In: Journal of Artificial Intelligence Research 19, pp. 399\u2013468.   \nHazan, Tamir and Amnon Shashua (2010). \u201cNorm-product belief propagation: Primal-dual messagepassing for approximate inference\u201d. In: IEEE Transactions on Information Theory 56.12, pp. 6294\u2013 6316.   \nHelmert, Malte (2006). \u201cThe fast downward planning system\u201d. In: Journal of Artificial Intelligence Research 26, pp. 191\u2013246.   \nHoffmann, J\u00f6rg and Bernhard Nebel (2001). \u201cThe FF planning system: Fast plan generation through heuristic search\u201d. In: Journal of Artificial Intelligence Research 14, pp. 253\u2013302.   \nJordan, Michael I, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul (1999). \u201cAn introduction to variational methods for graphical models\u201d. In: Machine learning 37, pp. 183\u2013233.   \nKoller, Daphne and Ronald Parr (1999). \u201cComputing factored value functions for policies in structured MDPs\u201d. In: IJCAI. Vol. 99, pp. 1332\u20131339.   \nKolmogorov, Vladimir (2005). \u201cConvergent tree-reweighted message passing for energy minimization\u201d. In: International Workshop on Artificial Intelligence and Statistics. PMLR, pp. 182\u2013189.   \nKumar, Atul, Veeraruna Kavitha, and Nandyala Hemachandra (2015). \u201cFinite horizon risk sensitive MDP and linear programming\u201d. In: 2015 54th IEEE Conference on Decision and Control (CDC). IEEE, pp. 7826\u20137831.   \nLee, Junkyu, Alexander Ihler, and Rina Dechter (2018). \u201cJoin Graph Decomposition Bounds for Influence Diagrams.\u201d In: UAI, pp. 1053\u20131062.   \nLee, Junkyu, Radu Marinescu, and Rina Dechter (2014). \u201cApplying marginal map search to probabilistic conformant planning: Initial results\u201d. In: Workshops at the Twenty-Eighth AAAI Conference on Artificial Intelligence.   \nLee, Junkyu, Radu Marinescu, and Rina Dechter (2016). \u201cApplying Search Based Probabilistic Inference Algorithms to Probabilistic Conformant Planning: Preliminary Results.\u201d In: ISAIM.   \nLee, Junkyu, Radu Marinescu, Alexander Ihler, and Rina Dechter (2020). \u201cA weighted mini-bucket bound for solving influence diagram\u201d. In: Uncertainty in Artificial Intelligence. PMLR, pp. 1159\u2013 1168.   \nLevine, Sergey (2018). \u201cReinforcement learning and control as probabilistic inference: Tutorial and review\u201d. In: arXiv preprint arXiv:1805.00909.   \nLiu, Qiang and Alexander Ihler (2013). \u201cVariational algorithms for marginal MAP\u201d. In: Journal of Machine Learning Research 14, pp. 3165\u20133200.   \nMalek, Alan, Yasin Abbasi-Yadkori, and Peter Bartlett (2014). \u201cLinear programming for large-scale Markov decision problems\u201d. In: International conference on machine learning. PMLR, pp. 496\u2013 504.   \nMarthe, Alexandre, Aur\u00e9lien Garivier, and Claire Vernade (2023). \u201cBeyond Average Reward in Markov Decision Processes\u201d. In: Sixteenth European Workshop on Reinforcement Learning.   \nMartins, Andr\u00e9 FT, M\u00e1rio AT Figueiredo, Pedro MQ Aguiar, Noah A Smith, and Eric P Xing (2015). \u201cAd3: Alternating directions dual decomposition for map inference in graphical models\u201d. In: The Journal of Machine Learning Research 16.1, pp. 495\u2013545.   \nMatheson, James E (2005). \u201cDecision analysi $\\cdot=$ decision engineering\u201d. In: Emerging Theory, Methods, and Applications. INFORMS, pp. 195\u2013212.   \nPalmieri, Francesco AN, Krishna R Pattipati, Giovanni Di Gennaro, Giovanni Fioretti, Francesco Verolla, and Amedeo Buonanno (2022). \u201cA unifying view of estimation and control using belief propagation with application to path planning\u201d. In: IEEE Access 10, pp. 15193\u201315216.   \nPearl, Judea (2012). \u201cThe causal foundations of structural equation modeling\u201d. In: Handbook of structural equation modeling, pp. 68\u201391.   \nPerron, Laurent and Vincent Furnon (Mar. 7, 2024). OR-Tools. Version v9.9. Google. URL: https: //developers.google.com/optimization/.   \nPuterman, Martin L (2014). Markov decision processes: discrete stochastic dynamic programming. John Wiley & Sons.   \nRubenstein, Paul K, Sebastian Weichwald, Stephan Bongers, Joris M Mooij, Dominik Janzing, Moritz Grosse-Wentrup, and Bernhard Sch\u00f6lkopf (2017). \u201cCausal consistency of structural equation models\u201d. In: arXiv preprint arXiv:1707.00819.   \nSanner, Scott (2011). ICAPS 2011 international probabilistic planning competition. URL: https: //users.cecs.anu.edu.au/\\~ssanner/IPPC_2011/.   \nShachter, Ross D (2007). \u201cModel Building with Belief Networks and Influence Diagrams\u201d. In: Advances in decision analysis: from foundations to applications. Cambridge University Press, 177\u2013\u2013201.   \nShen, Yun, Michael J Tobia, Tobias Sommer, and Klaus Obermayer (2014). \u201cRisk-sensitive reinforcement learning\u201d. In: Neural computation 26.7, pp. 1298\u20131328.   \nSontag, David, Amir Globerson, and Tommi Jaakkola (2011). \u201cIntroduction to dual decomposition for inference\u201d. In.   \nSutton, Richard S (2018). \u201cReinforcement learning: An introduction\u201d. In: A Bradford Book.   \nToussaint, Marc and Amos Storkey (2006). \u201cProbabilistic inference for solving discrete and continuous state Markov Decision Processes\u201d. In: Proceedings of the 23rd international conference on Machine learning, pp. 945\u2013952.   \nWainwright, Martin J, Tommi S Jaakkola, and Alan S Willsky (2005). \u201cMAP estimation via agreement on trees: message-passing and linear programming\u201d. In: IEEE transactions on information theory 51.11, pp. 3697\u20133717.   \nWeiss, Yair, Chen Yanover, and Talya Meltzer (2012). \u201cMAP estimation, linear programming and belief propagation with convex free energies\u201d. In: arXiv preprint arXiv:1206.5286.   \nWeller, Adrian, Kui Tang, Tony Jebara, and David A Sontag (2014). \u201cUnderstanding the Bethe approximation: When and how can it go wrong?\u201d In: UAI, pp. 868\u2013877.   \nWu, Zhennan and Roni Khardon (2022). \u201cApproximate Inference for Stochastic Planning in Factored Spaces\u201d. In: International Conference on Probabilistic Graphical Models. PMLR, pp. 433\u2013444.   \nYedidia, Jonathan S, William T Freeman, and Yair Weiss (2005). \u201cConstructing free-energy approximations and generalized belief propagation algorithms\u201d. In: IEEE Transactions on information theory 51.7, pp. 2282\u20132312.   \nYoon, Sung Wook, Alan Fern, Robert Givan, and Subbarao Kambhampati (2008). \u201cProbabilistic planning via determinization in hindsight.\u201d In: AAAI, pp. 1010\u20131016. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Proof of the variational formulation of planning ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In this proof we will use two identities, the first is the variational identity (Jordan et al., 1999): ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\log\\sum_{x,a}f(x,a)=\\operatorname*{max}_{q(x,a)}\\langle\\log f(x,a)\\rangle_{q(x,a)}+H(q(x,a))\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "and the second is ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pi}\\sum_{a}q(a)\\log\\pi(a)=\\operatorname*{max}_{\\pi(a)}-H(q(a))-\\mathrm{KL}(q(a)||\\pi(a))=\\sum_{a}q(a)\\log q(a),\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "which follows because the term $\\pi(a)$ is an arbitrary distribution that can make the KL divergence exactly zero (by choosing $\\pi(\\pmb{a})=q(\\pmb{a}))$ ). With this we can proceed to the main proof: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathrm{\\mathop{sup}}_{\\mathbf{x}\\in\\mathbf{B}_{\\delta}}\\mathrm{low}_{\\mathbf{x}\\in\\mathbf{B}_{\\delta}}\\sum_{\\alpha,\\alpha}\\mathrm{exp}\\left(\\lambda_{\\mathbf{x}\\in\\mathbf{B}_{\\delta}}^{-1}\\mathbb{R}_{(\\alpha),\\alpha_{1},\\alpha_{1}+1}\\right)P(x_{1})\\prod_{t=1}^{m}P(x_{t+1}|\\alpha_{t},x_{t})\\mathrm{\\mathop{sup}}_{(\\alpha(\\alpha),\\gamma)}}\\\\ &{}\\\\ {\\mathrm{\\mathop{sup}}_{\\mathbf{x}\\in\\mathbf{B}_{\\delta}}^{[1,m]}\\mathrm{low}\\left\\{\\mathrm{log}\\left(P(x)\\prod_{t=1}^{T}\\right)\\mathrm{exp}(\\mathrm{A}_{t}(x_{t},\\alpha_{t},x_{t+1}))P(x_{t+1}|\\alpha_{t},x_{t})\\mathrm{\\mathop{sup}}_{(\\alpha(\\alpha),\\gamma)}+H(\\eta(\\mathbf{x},\\alpha)|\\alpha_{t},\\alpha_{1})\\right\\}}\\\\ &{=\\frac{1}{\\lambda}\\mathrm{\\mathop{sup}}_{\\mathbf{x}\\in\\mathbf{B}_{\\delta}}\\left(-E_{\\delta}(\\alpha)+H(\\alpha(\\alpha,\\beta))+\\sum_{t=1}^{m}\\mathrm{log}\\left(\\mathrm{a}_{t}(x_{t})\\mathrm{\\mathop{sp}}_{t}\\right)\\mathrm{e}(\\mathrm{a}_{t},x_{t})\\right)}\\\\ &{=\\frac{1}{\\lambda}\\mathrm{\\mathop{sup}}_{\\mathbf{x}\\in\\mathbf{B}_{\\delta}}\\left(-E_{\\delta}(\\alpha)+H_{\\alpha}(\\alpha_{1})+\\sum_{t=1}^{m}H_{t}(x_{t+1},\\alpha_{1}|\\alpha_{1})+\\mathrm{ing}_{\\mathbf{x}\\in\\mathbf{B}_{\\delta}}[\\mathrm{or}\\left(\\alpha_{1}|x_{t}|\\right)_{t}]\\mathrm{exp}_{(\\alpha(\\alpha),\\gamma)}\\mathrm{e}_{(\\alpha(\\gamma))}\\right)}\\\\ &{\\mathrm{\\mathop{sup}}_{\\mathbf{x}\\in\\mathbf{B}_{\\delta}}\\mathrm{low}\\left\\{-E_{\\delta}(\\alpha)+H_{\\alpha}(\\gamma)+\\sum_{t=1}^{T}H_{t}(x_{t+1},\\alpha_{1}|\\alpha_{1})+\\left(\\mathrm{log}\\left( \n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where we additionally see that we have chosen $\\pi_{t}(a_{t}|x_{t})=q(a_{t}|x_{t})$ , therefore, the optimal policy will be $\\pi_{t}(a_{t}|x_{t})=q^{*}(a_{t}|x_{t})$ where $q^{*}$ is the value of $q$ that maximizes $F_{\\lambda}^{\\mathrm{planning}}(\\mathfrak{q})$ . ", "page_idx": 12}, {"type": "text", "text": "B Proof that for $\\lambda\\to0^{+}$ the variational bound turns into an LP ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Let us rewrite the concave optimization functional Eq. (2) (derived in Appendix A) by combining the energy and entropy terms into KL divergences: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\vec{z}_{\\lambda}^{\\mathrm{plaming}}(q)=\\displaystyle\\frac{1}{\\lambda}\\Big(-E_{\\lambda}(q)+H^{\\mathrm{plaming}}(q)\\Big)=\\frac{1}{\\lambda}\\Big(-\\mathrm{KL}(q(x_{1})||P(x_{1}))}\\\\ {\\quad\\quad\\quad\\quad-\\displaystyle\\sum_{t=1}^{T-1}\\langle\\mathrm{KL}(q(x_{t+1}|x_{t},a_{t})||P(x_{t+1}|x_{t},a_{t}))\\rangle_{q(x_{t},a_{t})}+\\langle\\lambda R_{t}(x_{t},a_{t},x_{t+1})\\rangle_{q(x_{t+1},x_{t},a_{t})}\\Big)}\\\\ {\\quad\\quad\\quad\\quad=\\displaystyle\\sum_{t=1}^{T-1}\\langle R_{t}(x_{t},a_{t},x_{t+1})\\rangle_{q(x_{t+1},x_{t},a_{t})}}\\\\ {\\quad\\quad\\quad-\\displaystyle\\frac{\\mathrm{KL}(q(x_{1})||P(x_{1}))+\\sum_{t=1}^{T-1}\\langle\\mathrm{KL}(q(x_{t+1}|x_{t},a_{t})||P(x_{t+1}|x_{t},a_{t}))\\rangle_{q(x_{t},a_{t})}}{\\lambda}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "It is clear that if any of the KL terms are larger than 0, then F \u03bbplanning(q) = \u2212\u221e(for bounded rewards), whereas if all KL terms are 0, the limit is finite. That means that to maximize the bound wrt $\\pmb q$ in the $\\lambda\\to0^{+}$ limit, we will choose $q(x_{1})\\,=\\,P(x_{1})$ and $q(x_{t+1}|x_{t},a_{t})\\,=\\,P(x_{t+1}|x_{t},a_{t})$ , which allows to remove the KL terms and results in the (constrained) LP optimization of Corollary 1.1 (which also explicitly includes the marginalization constraints $\\mathbf{\\boldsymbol{q}}\\in\\mathcal{M}_{\\mathbf{\\boldsymbol{\\Lambda}}}$ ). ", "page_idx": 12}, {"type": "text", "text": "C Derivation of the planning Bethe entropy for factored MDPs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The standard Bethe entropy of a factored MDP (Yedidia et al., 2005), such as the one in Fig. 4, is: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{H_{\\mathrm{Bethe}}^{\\mathrm{marginal}}(\\tilde{q})=\\displaystyle\\sum_{i=1}^{N_{c}}H_{q}(\\boldsymbol{x}_{1}^{(i)})+\\displaystyle\\sum_{t=1}^{T-1}\\Big(H_{\\mathrm{Bethe}}(\\tilde{q}_{x_{t},a_{t}})-\\displaystyle\\sum_{i=1}^{N_{e}}H_{q}(\\boldsymbol{x}_{t}^{(i)})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\displaystyle\\sum_{i=1}^{N_{c}}H_{q}(\\boldsymbol{x}_{t+1}^{(i)},\\boldsymbol{x}_{t}^{\\mathrm{pa}(i)},a_{t})-H_{q}(\\boldsymbol{x}_{t}^{\\mathrm{pa}(i)},a_{t})\\Big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where we use $\\boldsymbol{x}_{t}^{\\mathrm{pa}(i)}$ to refer to the \u201cparent\u201d variables. To simplify notation, when $i\\in{1,...,N_{e}}$ , $\\boldsymbol x_{t}^{\\mathrm{pa}(i)}$ is the collection variables on which the distribution of $x_{t+1}^{(i)}$ depends according to the dynamics model, but when $i\\in N_{e}+1,\\dots,N_{e}+N_{r},\\,x_{t}^{\\mathrm{pa}}$ , xtpa(i)is the collection of variables on which the (i \u2212Ne)-th reward depends. See also Fig. 4 for clarification on the notation of parents of dynamics and rewards. The Bethe entropy above was defined, for conciseness, in terms of the Bethe entropy of a subset of the variables in a single time slice (current state and action, but not next state): ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{H_{\\mathrm{Bethe}}(\\widetilde{q}_{x_{t},a_{t}})=\\bigr(1-N_{e})H_{q}(a_{t})+\\displaystyle\\sum_{i=1}^{N_{e}}H_{q}(x_{t}^{\\mathrm{pa}(i)},a_{t})+\\displaystyle\\sum_{i=N_{e}+1}^{N_{e}+N_{r}}H_{q}(x_{t}^{\\mathrm{pa}(i)})}\\\\ &{\\qquad\\qquad\\qquad-\\displaystyle\\sum_{i=1}^{N_{e}+N_{r}}\\sum_{k\\in\\mathrm{pa}(i)}H_{q}(x_{t}^{(k)})+\\displaystyle\\sum_{i=1}^{N_{e}}H_{q}(x_{t}^{(i)}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Finally, the Bethe entropy of the states of factored MDP is ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{H_{\\mathrm{Bethe}}(\\widetilde{q}_{x_{t}})=\\displaystyle\\sum_{i=1}^{N_{e}}H_{q}(x_{t}^{(i)})+\\sum_{i=1}^{N_{e}+N_{r}}\\left(H_{q}(x_{t}^{\\mathrm{pa}(i)})-\\displaystyle\\sum_{k\\in\\mathrm{pa}(i)}H_{q}(x_{t}^{(k)})\\right)}\\\\ &{\\qquad\\qquad=\\displaystyle\\sum_{i=1}^{N_{e}}H_{q}(x_{t}^{(i)})-\\sum_{i=1}^{N_{e}+N_{r}}I_{q}(x_{t}^{\\mathrm{pa}(i)})}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where Iq(xtpa(i)) is the mutual information among the parents of variable xt(i+)1. ", "page_idx": 13}, {"type": "text", "text": "Note that all the Bethe entropy definitions are linear combinations of standard Shannon entropies defined over subsets of variables in the factor graph. The subsets are defined by the factor graph, as groups of variables connected by the same factor. The idea of the Bethe entropy approximation is to sum the entropies of all such subsets and then discount the \u201covercounted\u201d entropy corresponding to variables that appear in multiple subsets. The pseudo-marginals $\\tilde{\\pmb q}\\equiv\\{q(x_{t+1}^{(i)},\\bar{x}_{t}^{\\mathrm{ja}(i)},a_{t})\\}_{t=1,i=1}^{t=T-1,i=N_{e}}\\cup\\{q(x_{t}^{\\mathrm{pa}(r)})\\}_{t=1,r=N_{e}+1}^{t=T,r=N_{e}+N_{r}}$ are all the local distributions that correspond to each factor. The pseudo-marginals are locally consistent at the variables, i.e., two pseudo-marginals that contain the same variable should provide the same marginal for that variable. However, there is no further need for consistency, and in particular, they do not need to correspond to the marginals of global joint distribution. The (convex) domain of pseudo-marginals contains the (also convex) domain of the marginals, but it is larger, so switching from marginals to pseudo-marginals in any optimization problem relaxes it and provides an upper bound on the original optimization problem. See (Weller et al., 2014) for more details. For convenience, we have also defined some subsets of the pseudo-marginals: $\\tilde{q}_{x_{t},a_{t}}\\equiv\\{q(x_{t}^{\\mathtt{p a}(i)},a_{t})\\}_{i=1}^{i=N_{e}}\\cup\\{q(x_{t}^{\\mathtt{p a}(r)})\\}_{r=N_{e}+1}^{r=N_{e}+N_{r}}$ and $\\tilde{\\pmb q}_{x_{t}}\\equiv\\{q(\\boldsymbol x_{t}^{\\mathrm{pa}(i)})\\}_{i=1}^{i=N_{e}+N_{r}}$ . ", "page_idx": 13}, {"type": "text", "text": "Since the planning entropy is a linear combination of the standard entropy of the factor graph $H^{\\mathrm{marginal}}(\\dot{\\boldsymbol{q}})$ and two local entropies per time step, we can simply approximate each Shannon entropy by its corresponding Bethe entropy to get the Bethe planning entropy ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle H^{\\mathrm{planing}}(q)=H^{\\mathrm{marginal}}(q)+\\sum_{t=1}^{T-1}H_{q}(x_{t})-H_{q}(x_{t},a_{t})}\\ ~}\\\\ {{\\displaystyle~~~~~~~~\\approx H_{\\mathrm{Bethe}}^{\\mathrm{marginal}}(\\tilde{q})+\\sum_{t=1}^{T-1}H_{\\mathrm{Bethe}}(\\tilde{q}_{x_{t}})-H_{\\mathrm{Bethe}}(\\tilde{q}_{x_{t},a_{t}})}\\ ~}\\\\ {{\\displaystyle~~~~~~~~=\\sum_{i=1}^{N_{c}}H_{q}(x_{1}^{(i)})+\\sum_{t=1}^{T-1}\\left(\\sum_{i=1}^{N_{c}}H_{q}(x_{t+1}^{(i)}|x_{t}^{\\mathrm{pa}(i)},a_{t})-\\sum_{i=1}^{N_{c}+N_{r}}I_{q}(x_{t}^{\\mathrm{pa}(i)})\\right)={\\cal H}_{\\mathrm{Bethe}}^{\\mathrm{planing}}(\\tilde{q})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Note that we sometimes use the superscript marginal for consistency with the main text, but the marginal entropy is simply the standard entropy so that superscript can be safely dropped. ", "page_idx": 14}, {"type": "text", "text": "D Value BP message updates ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We are interested in optimizing the cost function ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{1}{\\lambda}\\operatorname*{max}_{\\tilde{\\pmb q}}\\Big(-E_{\\lambda}(\\tilde{\\pmb q})+\\epsilon H_{\\mathrm{Bethe}}^{\\mathrm{marginal}}(\\tilde{\\pmb q})+(1-\\epsilon)H_{\\mathrm{Bethe}}^{\\mathrm{planning}}(\\tilde{\\pmb q})\\Big)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "As we saw in Appendix C, HBmeatrhgeinal( and $H_{\\mathrm{Bethe}}^{\\mathrm{planning}}(\\tilde{q})$ are linear combinations of local entropies over small subsets of variables defined by the factor graph, therefore so is their linear combination. This score function can be seen as a standard Bethe free energy with non-standard entropy weightings. We can directly derive LBP-like message updates by using these modified weights instead (Hazan and Shashua, 2010), resulting in the following VBP message updates ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\{u_{k}^{(n)},u_{k}\\}=\\sum_{\\substack{x\\in\\{1,2,3\\},\\atop\\theta\\in\\mathbb{Z}}}\\Big(u_{k}^{(n)}\\Big)_{i}[u_{k}^{(n)},u_{k}^{(n)}]}\\\\ &{:=\\Big(\\frac{\\eta\\sin(\\theta^{\\prime})}{\\theta\\le1}\\Big)\\times\\Big(\\frac{\\eta\\sin(\\theta^{\\prime})}{2}u_{k}^{(n)}\\Big)_{i}[u_{k}^{(n)},~]}\\\\ &{:=\\Big(\\frac{\\eta\\sin(\\theta^{\\prime})}{2}\\Big(u_{k}^{(n)}-u_{k}^{(n)}\\Big)^{\\frac{1}{2}}\\Big)\\times\\Big(u_{k}^{(n)}\\Big)_{i}[u_{k}^{(n)},~]}\\\\ &{:=\\Big(u_{k}^{(n)}\\Big)_{i}-\\sum_{\\substack{x\\in\\{1,2,3\\},\\atop\\theta\\in\\mathbb{Z}}}\\Big(\\frac{\\eta\\sin(\\theta^{\\prime})}{2}u_{k}^{(n)}\\Big)_{i}[u_{k}^{(n)},~]}\\\\ &{:=\\Big(u_{k}^{(n)}\\Big)_{i}-\\sum_{\\substack{x\\in\\{1,2,3\\},\\ldots,\\,x}}\\Big(\\frac{\\eta\\sin(\\theta^{\\prime})}{2}u_{k}^{(n)}\\Big)_{i}[u_{k}^{(n)},~]}\\\\ &{:=\\Big(u_{k}^{(n)}\\Big)_{i}-\\frac{\\eta\\sin(\\theta^{\\prime})}{2}[u_{k}^{(n)},~]}\\\\ &{:=\\frac{\\eta\\sin(\\theta^{\\prime})}{2}\\Big(u_{k}^{(n)}\\Big)_{i}\\frac{u_{k}^{(n)}}{[1+\\theta]}[u_{k}^{(n)},~]}\\\\ &{:=\\frac{\\eta\\sin(\\theta^{\\prime})}{2}\\Big(u_{k}^{(n)}\\Big)_{i}[u_{k}^{(n)},~]}\\\\ &{:=\\frac{\\eta\\sin(\\theta^{\\prime})}{2}[u_{k}^{(n)},~]}\\\\ &{:=\\sqrt{\\frac{\\eta\\sin(\\theta^{\\prime})}{2}}[u_{k}^{(n)},~]\\Big[\\frac{u_{k}^{(n)}}{[1+\\theta]}[u_ \n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The following messages should be held constant ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{m_{\\mathsf{b}}(x_{t}^{\\mathsf{p a}(i)})=\\exp(\\lambda R(x_{t}^{\\mathsf{p a}(i)}))\\;\\;\\forall i>N_{e}}\\\\ {m_{\\mathsf{f}}(x_{1}^{(i)})=P(x_{1}^{(i)})\\qquad\\qquad\\qquad\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "See Fig. 4 to track the correspondence between the above messages and the factor graph of the factored MDP. Note that most updates correspond exactly with standard loopy BP, and only a few (the ones involving $\\epsilon$ ) are specific to VBP. These messages updates should be iterated until convergence or for a fixed number of iterations. Two tricks to improve convergence are (a) damping the message updates in log space; (b) as the iterations progress, anneal between LBP $\\left.\\epsilon=1\\right.$ ) and VBP (very small \u03f5). We typically use both, with a damping of 0.5 (i.e., the mean of the old and the new message in log space) and anneal by using as $\\epsilon(\\mathrm{iter})=\\operatorname*{max}\\{0.01,1.0/\\mathrm{iter}\\}$ , where \u201citer\u201d is the iteration number. Scheduling also plays a role in convergence. We propagate the messages by alternating backward and forward schedules in an outer loop, and solving each time slice to convergence in an inner loop. We did observe a correlation between the quality of the solutions and VBP converging. ", "page_idx": 15}, {"type": "image", "img_path": "TXsRGrzICz/tmp/d009b152ba00bc75ed56fb70ff21b9d5d61bbb2311ee4b608afb6a0615f8706d.jpg", "img_caption": ["Figure 4: Correspondence between the message passing updates and the factorized MDP. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "E Entropy terms for other inference types ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The entropy terms for inference types other than the \u201cplanning inference\u201d were introduced in prior work and are compiled here for convenience. ", "page_idx": 15}, {"type": "text", "text": "First we will derive a general variational expression that depends on an inverse temperature $\\beta$ . The function $f({\\boldsymbol{x}},{\\boldsymbol{a}})$ will be an (unnormalized) factor graph, with the structure of Fig. 1[Left]. ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\frac{1}{\\beta}}\\log\\sum_{x,a}f(x,a)^{\\beta}={\\frac{1}{\\beta}}\\log\\sum_{x,a}q(x,a){\\frac{f(x,a)^{\\beta}}{q(x,a)}}{\\overset{{\\mathrm{tightlensen}}}{=}}\\operatorname*{max}_{q(x,a)}{\\frac{1}{\\beta}}\\sum_{x,a}q(x,a)\\log{\\frac{f(x,a)^{\\beta}}{q(x,a)}}}\\\\ {={\\underset{q(x,a)}{\\operatorname*{max}}}\\langle\\log f(x,a)\\rangle_{q(x,a)}+{\\frac{1}{\\beta}}H_{q}(x,a)\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (1)}\\end{array}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Marginal inference This is the standard VI problem, see e.g., (Jordan et al., 1999), and can be recovered from Eq. (11) by setting $\\beta=1$ . Then we get ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\log\\sum_{x,a}f(x,a)=\\operatorname*{max}_{q(x,a)}\\left\\langle\\log f(x,a)\\right\\rangle_{q(x,a)}+H_{q}(x,a).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since $f({\\boldsymbol{x}},{\\boldsymbol{a}})$ forms a chain in our application of interest (see Fig. 1[Left]), we know that the optimal posterior will as well. We can thus decompose $H_{q}(x,a)$ using the chain rule to obtain the expression from Table $\\begin{array}{r}{1,H^{\\mathrm{marginal}}(\\pmb{q})=H_{q}(x_{1})+\\sum_{t=1}^{T-1}\\Bar{H}_{q}(x_{t+1},a_{t}|x_{t})}\\end{array}$ . ", "page_idx": 15}, {"type": "text", "text": "Maximum-a-posterior (MAP) inference MAP inference can be recovered from Eq. (11) by setting $\\beta\\to\\infty$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{x,a}\\log f(\\pmb{x},\\pmb{a})=\\operatorname*{lim}_{\\beta\\rightarrow\\infty}\\frac{1}{\\beta}\\log\\sum_{\\pmb{x},a}f(\\pmb{x},\\pmb{a})^{\\beta}=\\operatorname*{max}_{q(\\pmb{x},\\pmb{a})}\\langle\\log f(\\pmb{x},\\pmb{a})\\rangle_{q(\\pmb{x},\\pmb{a})}+\\operatorname*{lim}_{\\beta\\rightarrow\\infty}\\frac{1}{\\beta}H_{q}(\\pmb{x},\\pmb{a}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where lim\u03b2 \u03b21 $\\begin{array}{r}{\\operatorname*{lim}_{\\beta\\to\\infty}\\frac{1}{\\beta}H_{q}(\\pmb{x},\\pmb{a})}\\end{array}$ produces the term $H^{\\mathrm{marginal}}(q)=0$ from Table 1. The maximization is an LP problem, see (Weiss et al., 2012). This problem can be relaxed into a local polytope, giving rise to most well-known methods for approximate MAP inference, such as dual decomposition. See e.g., (Sontag et al., 2011). An optimal distribution solving this problem is a Dirac delta centered at one of the MAP solutions of the problem. ", "page_idx": 16}, {"type": "text", "text": "Marginal maximum-a-posterior (MMAP) inference MMAP combines the previous two types of inference, see (Liu and Ihler, 2013) for detailed (approximate) solution methods. We can reuse the previous results to obtain the variational expression ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\operatorname*{max}_{\\boldsymbol{a}}\\log\\sum_{\\boldsymbol{x}}f(\\pmb{x},\\pmb{a})^{\\mathrm{{Eq}}}\\frac{\\mathrm{tq}_{\\cdot}(12)}{\\pi}\\operatorname*{max}_{\\boldsymbol{a}}\\left\\langle\\log f(\\pmb{x},\\pmb{a})\\right\\rangle_{\\boldsymbol{q}(\\pmb{x}|\\pmb{a})}+H(\\boldsymbol{q}(\\pmb{x}|\\pmb{a}))}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\stackrel{\\mathrm{Eq}_{\\cdot}(13)}{=}\\operatorname*{max}_{\\boldsymbol{q}(\\pmb{x},\\pmb{a})}\\left\\langle\\log f(\\pmb{x},\\pmb{a})\\right\\rangle_{\\boldsymbol{q}(\\pmb{x},\\pmb{a})}+H_{\\boldsymbol{q}}(\\pmb{x},\\pmb{a})-H_{\\boldsymbol{q}}(\\pmb{a})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\displaystyle\\operatorname*{max}_{\\boldsymbol{q}(\\pmb{x},\\pmb{a})}\\left\\langle\\log f(\\pmb{x},\\pmb{a})\\right\\rangle_{\\boldsymbol{q}(\\pmb{x},\\pmb{a})}+H_{\\boldsymbol{q}}(\\pmb{x},\\pmb{a})-\\sum_{t=1}^{T-1}H_{\\boldsymbol{q}}(\\pmb{a}_{t})}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "eTnot ruonpdye. rTsthains dm tehaen lsa tsht aet qtuhael iltays,t  feirxsptr ensostieo tnh iast $\\begin{array}{r}{\\sum_{t=1}^{T-1}H_{q}(a_{t})\\geq H_{q}(\\pmb{a})}\\end{array}$ ,e qsiunacl et ot hteh el aptrteerv iios utsh eo njeo.i nItt is not yet clear why equality is always achievable. To see this note that, analogously to the previous section, the optimal $q(a)$ will be a Dirac delta centered at an optimal sequence $\\textbf{\\em a}$ . That means that the optimal variational distribution of MMAP factorizes as $\\begin{array}{r}{q(\\pmb{x},\\pmb{a})\\,=\\,q(\\pmb{x})\\prod_{t=1}^{T-1}q(a_{t})}\\end{array}$ . This in taulrwna yms emanast cthh atth,e  fporre tvhieo uosp toinme,a la ndids tgriivbeunt itohna $\\begin{array}{r}{\\sum_{t=1}^{T-1}H_{q}(a_{t})=H_{q}(\\pmb{a})}\\end{array}$ . mSuos tt hh ea vlea stth ee xsparmeses ioopnti cmaanl distribution and value. ", "page_idx": 16}, {"type": "text", "text": "Just like we did for marginal inference, we can now decompose $H_{q}(x,a)$ using the chain rule to obtain the expression from Table 1, $\\begin{array}{r}{,\\,H^{\\mathrm{MMAP}}(\\pmb{q})=H_{\\boldsymbol{q}}(x_{1})+\\sum_{t=1}^{T-1}H_{\\boldsymbol{q}}(x_{t+1},a_{t}|x_{t})-H_{\\boldsymbol{q}}(a_{t}).}\\end{array}$ . ", "page_idx": 16}, {"type": "text", "text": "F Proof of the bounding relationships among different inference types ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We want to prove that for an arbitrary variational distribution $\\pmb q$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F_{\\lambda}^{\\mathrm{MAP}}(\\pmb{q})}\\\\ &{F_{\\lambda}^{\\mathrm{marginal}^{\\mathrm{U}}}(\\pmb{q})\\overset{\\leq}{\\longrightarrow}F_{\\lambda}^{\\mathrm{MMAP}}(\\pmb{q})\\leq F_{\\lambda}^{\\mathrm{planning}}(\\pmb{q})\\leq F_{\\lambda}^{\\mathrm{marginal}}(\\pmb{q})}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and that, when we assume that dynamics are deterministic (which we also take to imply that the first state follows a deterministic distribution, i.e., it is known), the optimal values of the above bounds wrt $\\pmb q$ satisfy the following relationships ", "page_idx": 16}, {"type": "equation", "text": "$$\nF_{\\lambda}^{\\mathrm{marginal}^{\\mathrm{U}}}\\le F_{\\lambda}^{\\mathrm{MAP}}=F_{\\lambda}^{\\mathrm{MMAP}}=F_{\\lambda}^{\\mathrm{planning}}\\le F_{\\lambda}^{\\mathrm{marginal}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since the energy terms are identical for all these bounds, we will simply have to prove the relationship between the corresponding entropies, as given in Table 1. ", "page_idx": 16}, {"type": "text", "text": "Proof that $F_{\\lambda}^{\\mathbf{planning}}(q)\\leq F_{\\lambda}^{\\mathrm{marginal}}(q)$ The marginal entropy term ", "page_idx": 16}, {"type": "equation", "text": "$$\nH^{\\mathrm{marginal}}(\\pmb{q})=H_{q}(x_{1})+\\sum_{t=1}^{T-1}H_{q}(x_{t+1},a_{t}|x_{t})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "clearly upper bounds the \u201cplanning inference\u201d entropy term ", "page_idx": 17}, {"type": "equation", "text": "$$\nH^{\\mathrm{planning}}(q)=H_{q}(x_{1})+\\sum_{t=1}^{T-1}H_{q}(x_{t+1}|a_{t},x_{t})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "since ", "page_idx": 17}, {"type": "equation", "text": "$$\nH_{q}(x_{t+1},a_{t}|x_{t})=H_{q}(x_{t+1}|a_{t},x_{t})+H_{q}(a_{t}|x_{t}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and entropies are non-negative. ", "page_idx": 17}, {"type": "text", "text": "Proof that $F_{\\lambda}^{\\mathrm{MMAP}}(\\pmb q)\\leq F_{\\lambda}^{\\mathrm{planning}}(q)$ (and $F_{\\lambda}^{\\mathrm{MMAP}}=F_{\\lambda}^{\\mathbf{planning}}$ for deterministic dynamics) ", "page_idx": 17}, {"type": "text", "text": "The \u201cplanning inference\u201d entropy term ", "page_idx": 17}, {"type": "equation", "text": "$$\nH^{\\mathrm{planning}}(q)=H_{q}(x_{1})+\\sum_{t=1}^{T-1}H_{q}(x_{t+1}|a_{t},x_{t})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "clearly upper bounds the MMAP entropy term ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{H^{\\mathrm{MMAP}}(q)=H_{q}(x_{1})+\\displaystyle\\sum_{t=1}^{T-1}H_{q}(x_{t+1},a_{t}|x_{t})-H_{q}(a_{t})}\\\\ &{\\qquad\\qquad=H_{q}(x_{1})+\\displaystyle\\sum_{t=1}^{T-1}H_{q}(x_{t+1}|x_{t},a_{t})+\\left(H_{q}(a_{t}|x_{t})-H_{q}(a_{t})\\right)}\\\\ &{\\qquad\\qquad=H_{q}(x_{1})+\\displaystyle\\sum_{t=1}^{T-1}H_{q}(x_{t+1}|x_{t},a_{t})-I_{q}(x_{t};a_{t})}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "since the mutual information $I_{q}(x_{t};a_{t})$ is non-negative. ", "page_idx": 17}, {"type": "text", "text": "In exact MMAP variational inference (Liu and Ihler, 2013), at the optimal solution, the variational distribution factorizes as $\\begin{array}{r}{q(\\mathbf{x},\\pmb{a})\\,=\\,q(\\pmb{x})\\prod_{t=1}^{T-1}q(a_{t})}\\end{array}$ , as we mentioned in Appendix E. This is because MMAP finds a single, deterministi c, optimal sequence of actions. Therefore $I_{q}(x_{t};a_{t})=0$ when evaluated at the $\\pmb q$ that maximizes $F_{\\lambda}^{\\mathrm{MMAP}}(\\pmb q)$ . Setting that term to zero in the MMAP entropy results in the same expression as the \u201cplanning inference\u201d entropy. However, in \u201cplanning inference\u201d, the optimal variational distribution does not need to factorize in the way it does for MMAP, so a richer variational distribution is possible, and $F_{\\lambda}^{\\mathbf{planning}}$ can be strictly larger than $F_{\\lambda}^{\\mathrm{MMAP}}$ for some problems. ", "page_idx": 17}, {"type": "text", "text": "However, when dynamics are deterministic and the first state $x_{1}$ is known, the optimal plan is a deterministic sequence of states and actions, and the optimal \u201cplanning\u201d variational distribution is a Dirac delta at those states and actions. Therefore, the optimal distribution for planning also factorizes as $\\begin{array}{r}{q(\\pmb{x},\\pmb{a})\\,=\\,q(\\pmb{x})\\prod_{t=1}^{T-1}q(a_{t})}\\end{array}$ when the dynamics are deterministic. Therefore, in that case the optimal values coincide F \u03bbplanning= F \u03bbMMAP(and so do the optimal variational distributions). ", "page_idx": 17}, {"type": "text", "text": "We can also show that at the optimal $\\pmb q$ value the entropies of both bounds are zero. $P(x_{t+1}|a_{t},x_{t})$ (and $P(x_{1}))$ are deterministic, $q(x_{t+1}|a_{t},x_{t})=P(x_{t+1}|a_{t},x_{t})$ (and $q(x_{1})=P(x_{1}){\\dot{)}}$ for both of these bounds to be larger than $-\\infty$ (see Section 4.2). Since these are deterministic distributions, $H_{q}(x_{t+1}|a_{t},x_{t})=0$ and $H_{q}(x_{1})=0$ , which in turn makes $H^{\\mathbf{planning}}(q)=0$ at the optimal $\\pmb q$ . I.e., $H^{\\mathbf{planning}}=0$ . Since $I_{q}(x_{t};a_{t})=0$ , also $H^{\\mathrm{MMAP}}=0$ . ", "page_idx": 17}, {"type": "text", "text": "Proof that $F_{\\lambda}^{\\mathrm{MAP}}(q)\\leq F_{\\lambda}^{\\mathrm{MMAP}}$ (and $F_{\\lambda}^{\\mathrm{MAP}}=F_{\\lambda}^{\\mathrm{MMAP}}(\\pmb{q})$ for deterministic dynamics) ", "page_idx": 17}, {"type": "text", "text": "The MMAP entropy can be rearranged in the following form ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{H^{\\mathrm{MMAP}}(q)=H_{q}(x_{1})+\\sum_{t=1}^{T-1}H_{q}(x_{t+1},a_{t}|x_{t})-H_{q}(a_{t})}}\\\\ &{}&{\\quad=H_{q}(x_{1},a_{1},\\ldots,x_{T-1},a_{T-1},x_{T})-\\displaystyle\\sum_{t=1}^{T-1}H_{q}(a_{t})}\\\\ &{}&{\\quad=I_{q}(x_{1};a_{1};\\ldots,x_{T-1};a_{T-1};x_{T})+\\displaystyle\\sum_{t=1}^{T}H_{q}(x_{t})\\geq0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the last equality follows because the mutual information and entropy are non-negative. This clearly upper bounds the MAP entropy term $H^{\\mathrm{MAP}}(\\pmb q)=0$ . ", "page_idx": 18}, {"type": "text", "text": "When dynamics are deterministic and the first state $x_{1}$ is known, we can reuse the results of the previous proof. We know that in that case, the optimal variational distribution for MMAP and planning is a Dirac delta at the optimal sequence of states and actions, and that the bound only contains the energy terms (since the entropy terms for any Dirac delta distribution will be zero). The problem of finding such distribution is exactly the MAP energy minimization problem. Thus, if dynamics are deterministic the optimal values coincide $F_{\\lambda}^{\\mathrm{MMAP}}\\overset{\\underset{\\smile}{=}}F_{\\lambda}^{\\mathrm{MAP}}$ (and so do the optimal variational distributions). ", "page_idx": 18}, {"type": "text", "text": "Proof that F marginalU $F_{\\lambda}^{\\mathrm{marginal}^{\\mathrm{U}}}(q)\\leq F_{\\lambda}^{\\mathrm{MMAP}}(q)$ ", "page_idx": 18}, {"type": "text", "text": "The MMAP entropy term ", "page_idx": 18}, {"type": "equation", "text": "$$\nH^{\\mathrm{MMAP}}(q)=H_{q}(x_{1})+\\sum_{t=1}^{T-1}(H_{q}(x_{t+1},a_{t}|x_{t})-H_{q}(a_{t}))\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "clearly upper bounds $H^{\\mathrm{Marginal}^{\\mathrm{U}}}(\\pmb q)$ , i.e., ", "page_idx": 18}, {"type": "equation", "text": "$$\nH^{\\mathrm{Marginal}^{\\mathrm{U}}}(q)=H_{q}(x_{1})+\\sum_{t=1}^{T-1}(H_{q}(x_{t+1},a_{t}|x_{t})-\\log N_{a})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "since $H_{q}(a_{t})\\leq\\log N_{a}$ , with equality being attained when $q(a_{t})$ is uniform. ", "page_idx": 18}, {"type": "text", "text": "Proof that F \u03bbmarginalU $F_{\\lambda}^{\\mathrm{marginal}^{\\mathrm{U}}}\\le F_{\\lambda}^{\\mathrm{MAP}}$ for deterministic dynamics ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We can rewrite the entropy corresponding to the marginal bound with uniform prior as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{H^{\\mathrm{Marginal}^{\\mathrm{U}}}(q)=H_{q}(x_{1})+\\displaystyle\\sum_{t=1}^{T-1}(H_{q}(x_{t+1},a_{t}|x_{t})-\\log N_{a})}\\\\ &{\\qquad\\qquad\\qquad=H_{q}(x_{1})+\\displaystyle\\sum_{t=1}^{T-1}(H_{q}(x_{t+1}|a_{t},x_{t})+H_{q}(a_{t}|x_{t})-\\log N_{a}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "When dynamics are deterministic and the first state $x_{1}$ is known, we know from the previous proofs that at the optimal value q of F \u03bbmarginal ( we have $H_{q}(x_{1})\\,=\\,0$ and $H_{q}(x_{t+1}|a_{t},x_{t})\\,=\\,0$ . The remaining terms, of the form $H_{q}(\\dot{(a_{t}}|x_{t})-\\log N_{a}$ are trivially non-positive (the conditional entropy over the actions cannot be larger than the log of the cardinality of the action space). This means that at the maximum value of F \u03bbmarginalU( wrt $\\pmb q$ we have $H^{\\mathrm{Marginal}^{\\mathrm{U}}}(\\pmb q)\\leq0$ . Since $H^{\\mathrm{MAP}}(\\pmb q)=0$ , we know that with deterministic dynamics F \u03bbmarginalU $F_{\\lambda}^{\\mathrm{marginal}^{\\mathrm{U}}}\\leq F_{\\lambda}^{\\mathrm{MAP}}$ ", "page_idx": 18}, {"type": "text", "text": "G An application of VI for planning: determinization in hindsight ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In the planning literature, many algorithms make the assumption of a deterministic environment (Geffner and Bonet, 2022; Hoffmann and Nebel, 2001; Helmert, 2006; etc). In order to extend these algorithms to the case of stochastic dynamics, the idea of determinization in hindsight has been developed (Yoon et al., 2008). It starts by factoring out all the stochasticity from the transition probability into the random variables $\\begin{array}{r}{\\gamma=\\{\\gamma_{t}\\}_{t=1}^{t=T-1}}\\end{array}$ ", "page_idx": 18}, {"type": "equation", "text": "$$\nP(x_{t+1}|x_{t},a_{t})=\\sum_{\\gamma_{t}}P_{\\mathrm{det}}(x_{t+1}|x_{t},a_{t},\\gamma_{t})P(\\gamma_{t})=\\langle P_{\\mathrm{det}}(x_{t+1}|x_{t},a_{t},\\gamma_{t})\\rangle_{P(\\gamma_{t})}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This factorization is always possible, and $\\gamma$ is known as the collection of exogenous variables in structural equation modeling (Pearl, 2012; Bareinboim and Pearl, 2016; Rubenstein et al., 2017). We will define $\\begin{array}{r}{P_{\\operatorname*{det}}(\\pmb{x}|\\pmb{a},\\gamma)\\equiv P_{\\operatorname*{det}}(x_{1}|\\gamma_{0})\\prod_{t=1}^{T-1}P_{\\operatorname*{det}}(x_{t+1}|x_{t},a_{t},\\gamma_{t})}\\end{array}$ . Then we can use it to obtain an upper bound on the exact utility ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F_{\\lambda=1}^{\\mathrm{plaming}}=\\log\\underset{\\pi}{\\operatorname*{max}}\\langle\\langle\\exp(R(x,a))\\rangle_{P_{\\mathrm{det}}(\\pi|a,\\gamma)\\pi(a|x)}\\rangle_{P(\\gamma)}}\\\\ &{\\qquad\\qquad\\leq\\log\\langle\\underset{\\pi}{\\operatorname*{max}}\\langle\\exp(R(x,a))\\rangle_{P_{\\mathrm{det}}(\\pi|a,\\gamma)\\pi(a|x)}\\rangle_{P(\\gamma)}=F_{\\lambda=1}^{\\mathrm{det.plaming}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "that has a natural interpretation: the dynamics of a stochastic environment depends on random variables $\\gamma$ that get drawn at each time step, if we knew their value ahead of time, we could treat the environment as deterministic, find an optimal plan and estimate its utility. Because we have hindsight, i.e., the deterministic planner can see the value of future $\\gamma_{t}$ ahead of time, it can make better choices than if these were revealed online, so the utility estimation, if exact, upper bounds the original quantity. See (Yoon et al., 2008) for more details. ", "page_idx": 19}, {"type": "text", "text": "G.1 Closed form determinization for standard MDPs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The above process uses a sample average: it alternates between sampling $\\gamma\\sim P(\\gamma)$ and running a fdoertemr emxinpirsetsisci opnl afnonr $F_{\\lambda=1}^{\\mathrm{det.\\,planning}}$ .e variational framework for planning it is possible to find a closed ", "page_idx": 19}, {"type": "text", "text": "First, let us expand the determinization bound ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F_{\\lambda=1}^{\\mathrm{det.\\,planing\\,}}\\overset{\\mathrm{Eq.}(9)}{=}\\underset{q(\\gamma)}{\\operatorname*{max}}\\langle\\log\\underset{\\pi}{\\operatorname*{max}}\\langle\\exp(R(x,a))\\rangle_{P_{\\mathrm{det}}(x|a,\\gamma)\\pi(a|x)}+\\log P(\\gamma)\\rangle_{q(\\gamma)}+H_{q}(\\gamma)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=\\underset{q(\\gamma)}{\\operatorname*{max}}\\langle F_{\\lambda=1,\\gamma}^{\\mathrm{planing}}\\rangle_{q(\\gamma)}+\\langle\\log P(\\gamma)\\rangle_{q(\\gamma)}+H_{q}(\\gamma)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "in terms of our exact planning utility for a given future $\\gamma$ , F \u03bbpl=an1n,i\u03b3ng . This quantity and the corresponding bound are defined as the usual ones, but conditioned on $\\gamma$ ", "page_idx": 19}, {"type": "equation", "text": "$$\nF_{\\lambda=1,\\gamma}^{\\mathrm{planing}}=\\operatorname*{max}_{q}F_{\\lambda=1}^{\\mathrm{planing}}(q|\\gamma);\\qquad F_{\\lambda=1}^{\\mathrm{planing}}(q|\\gamma)=-E_{\\lambda=1}(q|\\gamma)+H^{\\mathrm{planing}}(q|\\gamma).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "To be precise, the corresponding energy $E_{\\lambda=1}(\\pmb q|\\gamma)$ and entropy $H^{\\mathrm{planning}}(q|\\gamma)$ terms are ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle E_{\\lambda=1}(\\pmb{q}|\\gamma)=-\\left\\langle\\log P_{\\operatorname*{det}}(x_{1}|\\gamma_{0})\\right\\rangle_{\\pmb{q}(x_{1}|\\gamma_{0})}}\\\\ {\\displaystyle\\qquad\\qquad-\\sum_{t=1}^{T-1}\\left\\langle\\log P_{\\operatorname*{det}}(x_{t+1}|x_{t},a_{t},\\gamma_{t})+R_{t}(x_{t},a_{t},x_{t+1})\\right\\rangle_{\\pmb{q}(x_{t+1},x_{t},a_{t}|\\gamma_{t})}}\\\\ {\\displaystyle H^{\\mathrm{plaming}}(\\pmb{q}|\\gamma)=\\!H_{q}(x_{1}|\\gamma_{1})+\\sum_{t=1}^{T-1}H_{q}(x_{t+1}|x_{t},a_{t},\\gamma_{t})}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since the conditional dynamics are now deterministic, $\\log P_{\\mathrm{det}}(x_{t+1}|x_{t},a_{t},\\gamma_{t})$ can only take the values 0 and $-\\infty$ . When maximizing over $\\pmb q$ , it is obvious we must choose $q(x_{t+1}|x_{t},a_{t},\\gamma_{t})=$ $P_{\\mathrm{det}}(x_{t+1}|x_{t},a_{t},\\gamma_{t})\\,\\forall_{t}$ and $q(x_{1}|\\gamma_{0})=P_{\\mathrm{det}}(x_{1}|\\gamma_{0})$ , to avoid putting any mass on the $-\\infty$ value of the previous term, which would result in the whole score function becoming $-\\infty$ . For such choice, $H^{\\mathrm{det.\\,planning}}(q|\\gamma)=0$ and the mentioned energy term also cancels, leaving ", "page_idx": 19}, {"type": "equation", "text": "$$\nF_{\\lambda=1}^{\\mathrm{planing}}(q|\\gamma)=\\sum_{t=1}^{T-1}\\langle R_{t}(x_{t},a_{t},x_{t+1})\\rangle_{P_{\\operatorname*{det}}(x_{t+1}|x_{t},a_{t},\\gamma_{t})q(x_{t},a_{t}|\\gamma_{t})}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, determinization for multiplicative exponentiated rewards can be exactly expressed as a concave variational bound of the form ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F_{\\lambda=1}^{\\mathrm{det,\\,plaming}}=\\underset{q_{\\gamma}}{\\mathrm{max}}\\langle F_{\\lambda=1}^{\\mathrm{plaming}}(q|\\gamma)\\rangle_{q(\\gamma)}+\\underset{t=0}{\\overset{T-1}{\\sum}}\\langle\\log P(\\gamma_{t})\\rangle_{q(\\gamma_{t})}+H_{q}(\\gamma_{t})}\\\\ &{\\mathrm{\\s.t.}\\ \\ \\underset{a_{1},\\gamma_{1}}{\\sum}q(x_{1},a_{1},\\gamma_{1})=\\underset{\\gamma_{0}}{\\sum}P_{\\mathrm{det}}(x_{1}|\\gamma_{0})q(\\gamma_{0})}\\\\ &{\\underset{a_{t+1},\\gamma_{t+1}}{\\sum}q(x_{t+1},a_{t+1},\\gamma_{t+1})=\\underset{x_{t},a_{t},\\gamma_{t}}{\\sum}P_{\\mathrm{det}}(x_{t+1}|x_{t},a_{t},\\gamma_{t})q(x_{t},a_{t},\\gamma_{t})\\ \\forall t;}\\\\ &{q(x_{t},\\gamma_{t})=\\underset{a_{t}}{\\sum}q(x_{t},a_{t},\\gamma_{t})\\ \\forall t;\\quad q(x_{t},a_{t},\\gamma_{t})\\geq0\\ \\forall t,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where we have defined $\\pmb q_{\\gamma}\\equiv\\{q(x_{t},a_{t},\\gamma_{t})\\}_{t=1}^{T-1}\\cup q(\\gamma_{0})$ ", "page_idx": 19}, {"type": "text", "text": "G.2 Factored MDP ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In the case of a standard MDP, it is trivial to see that determinization, which provides an upper bound on the exact utility, cannot improve on our VI bound $F_{\\lambda=1}^{\\mathrm{planning}}$ , since the latter is exact. In this section we will show that this is also the case for factored MDPs in which the deterministic MAP problem is solved using an LP MAP relaxation. ", "page_idx": 20}, {"type": "text", "text": "First, we will show that $F_{\\lambda=1}^{\\mathrm{planning}}$ can be rewritten in a more convenient way to compare with determinization, in the standard, non-factored case. We expand ", "page_idx": 20}, {"type": "equation", "text": "$$\nF_{\\lambda=1}^{\\mathrm{planning}}=\\operatorname*{max}_{q}F_{\\lambda=1}^{\\mathrm{planning}}(q)=\\operatorname*{max}_{q}-E_{\\lambda=1}(q)+H^{\\mathrm{planning}}(q)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and substitute ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\log P(x_{t+1}|x_{t},a_{t})=\\underset{q(\\gamma_{t}|x_{t+1},x_{t},a_{t})}{\\operatorname*{max}}\\left(\\langle\\log P_{\\operatorname*{det}}(x_{t+1}|x_{t},a_{t},\\gamma_{t})\\rangle_{q(\\gamma_{t}|x_{t+1},x_{t},a_{t})}\\right.}\\\\ {\\left.-\\sum_{t=1}^{T}\\mathrm{KL}(q(\\gamma_{t}|x_{t+1},x_{t},a_{t})||P(\\gamma_{t}))\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "inside $\\scriptstyle{E_{\\lambda=1}(q)}$ to get ", "page_idx": 20}, {"type": "equation", "text": "$$\nF_{\\lambda=1}^{\\mathrm{planing}}=\\operatorname*{max}_{q}\\langle F_{\\lambda=1}^{\\mathrm{planing}}(q|\\gamma)\\rangle_{q(\\gamma)}+\\sum_{t=0}^{T-1}\\langle\\log P(\\gamma_{t})\\rangle_{q(\\gamma_{t})}+H_{q}(x_{t+1},\\gamma_{t}|a_{t},x_{t})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $q(x_{t+1}|x_{t},a_{t},\\gamma_{t})\\;=\\;P_{\\operatorname*{det}}(x_{t+1}|x_{t},a_{t},\\gamma_{t})\\;\\forall_{t}$ for the same reasons as above. To compact notation, we use the convention that $\\begin{array}{r}{H_{q}(x_{1},\\gamma_{0}|x_{0},a_{0})=H_{q}(x_{1},\\gamma_{0})}\\end{array}$ (since $x_{0},a_{0}$ are not defined), and similarly $H_{q}(\\gamma_{0}|x_{0},a_{0})\\,=\\,H_{q}(\\gamma_{0}\\hat{)}$ . Noting that $H_{q}(x_{t+1}^{\\ \\_},\\gamma_{t}|a_{t},x_{t})\\,=\\,H_{q}(x_{t+1}|x_{t},a_{t},\\gamma_{t})\\,+$ $H_{q}(\\gamma_{t}|a_{t},x_{t})$ and that $H_{q}(x_{t+1}|x_{t},a_{t},\\gamma_{t})=0$ because $q\\big(x_{t+1}|x_{t},a_{t},\\gamma_{t}\\big)$ is deterministic, we get ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{L}=\\displaystyle\\operatorname*{max}_{q_{\\gamma}}\\langle F_{\\lambda=1}^{\\mathrm{plaming}}(q|\\gamma)\\rangle_{q(\\gamma)}+\\displaystyle\\sum_{t=0}^{T-1}\\langle\\log P(\\gamma_{t})\\rangle_{q(\\gamma_{t})}+H_{q}(\\gamma_{t}|a_{t},x_{t})}\\\\ &{\\;\\mathrm{s.t.}\\displaystyle\\sum_{a_{1},\\ldots_{n}}q(x_{1},a_{1},\\gamma_{1})=\\displaystyle\\sum_{\\gamma_{0}}P_{\\mathrm{det}}(x_{1}|\\gamma_{0})q(\\gamma_{0})}\\\\ &{\\;\\displaystyle\\sum_{a_{t+1},\\gamma_{t+1}}q(x_{t+1},a_{t+1},\\gamma_{t+1})=\\displaystyle\\sum_{x_{t},a_{t},\\gamma_{t}}P_{\\mathrm{det}}(x_{t+1}|x_{t},a_{t},\\gamma_{t})q(x_{t},a_{t},\\gamma_{t})\\;\\;\\forall t;}\\\\ &{q(x_{t},\\gamma_{t})=\\displaystyle\\sum_{a_{t}}q(x_{t},a_{t},\\gamma_{t})\\;\\;\\forall x_{t},\\gamma_{t},t;\\;\\;\\;\\;q(x_{t},a_{t},\\gamma_{t})\\geq0\\,\\forall t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Compare Eqs. (17) and (18). Although we already knew this, with this particular formulation it is very easy to see that $F_{\\lambda=1}^{\\mathrm{planning}}\\;\\leq\\;F_{\\lambda=1}^{\\mathrm{det.\\,planning}}$ F \u03bbde=t. 1planning, since they are identical except for the terms $H_{q}(\\gamma_{t}|\\dot{a_{t}},x_{t})\\dot{\\leq}\\;H_{q}(\\gamma_{t})\\;\\forall t$ . This holds for the same $\\pmb q$ , but also establishes a relationship between both upper bounds at their maximum wrt $\\pmb q$ . ", "page_idx": 20}, {"type": "text", "text": "With this result, we can return to the factored MDP case. As in the main text, here we will consider $R$ to be only a function of $x_{t}$ . We can take Eqs. (17) and (18) and relax ${\\pmb q}_{\\gamma}$ from the marginal polytope $\\mathcal{M}$ to the local polytope $\\mathcal{L}$ , using the pseudo-marginals $\\tilde{\\pmb q}_{\\gamma}\\,\\equiv\\,\\{q(x_{t}^{\\mathrm{ja}(i)},a_{t},\\gamma_{t}^{(i)})\\}_{t=1,i=1}^{t=T-1,i=N_{e}}\\,\\cup$ {q(\u03b30(i ))}iN=e1 \u222a{q(xtpa(i))}tt==1T,,ii==NNee++1Nr. For Eq. (17), and substituting the value from Eq. (16), this ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\hat{F}_{\\lambda=1}^{\\mathrm{det,plaming}}=\\operatorname*{max}_{\\hat{q}_{\\gamma}}\\sum_{t=1}^{T}\\Big(\\sum_{i=N_{e}+1}^{N_{e}+N_{r}}\\langle R({\\pmb x}_{t}^{\\mathbf{pa}(i)})\\rangle_{q({\\pmb x}_{t}^{\\mathbf{pa}(i)})}+\\sum_{i=1}^{N_{e}}\\langle\\log P(\\gamma_{t-1}^{(i)})\\rangle_{q(\\gamma_{t-1}^{(i)})}+H_{q}(\\gamma_{t-1}^{(i)})\\Big)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "s.t. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{a_{1},\\gamma_{1}^{(i)}}q(x_{1}^{(i)},a_{1},\\gamma_{1}^{(i)})=\\sum_{\\gamma_{0}^{(i)}}P_{\\mathrm{det}}(x_{1}^{(i)}|\\gamma_{0}^{(i)})q(\\gamma_{0}^{(i)})\\,\\forall x_{1},i}\\\\ &{\\displaystyle\\sum_{t+1,\\gamma_{t+1}^{(i)}}q(x_{t+1}^{(i)},a_{t+1},\\gamma_{t+1}^{(i)})=\\sum_{x_{t}^{\\mathrm{pat}},a_{t},\\gamma_{t}^{(i)}}P_{\\mathrm{det}}(x_{t+1}^{(i)}|x_{t}^{\\mathrm{pa}(i)},a_{t},\\gamma_{t}^{(i)})q(x_{t}^{\\mathrm{pa}(i)},a_{t},\\gamma_{t}^{(i)})\\,\\,\\,\\forall x_{t+1}^{(i)},t,i\\in1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and pseudo-marginal constraints, ", "page_idx": 20}, {"type": "text", "text": "which can be interpreted as an outer maximization over $q(\\gamma)$ and an inner MAP optimization of the (conditional on $\\gamma$ ) deterministic dynamics problem, solved with an LP relaxation (Sontag et al., 2011). For Eq. (18), and also substituting the value from Eq. (16), this results in a different formulation of the problem Eq. (7) ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\hat{\\frac{\\gamma}{\\lambda}}\\mathrm{{laning}}=\\operatorname*{max}_{\\hat{\\mathcal{q}}_{\\gamma}}\\sum_{t=1}^{T}\\bigg(\\sum_{i=N_{e}+1}^{N_{e}+N_{r}}\\langle R({x}_{t}^{\\mathrm{pa}(i)})\\rangle_{q({x}_{t}^{\\mathrm{pa}(i)})}+\\sum_{i=1}^{N_{e}}\\langle\\log P(\\gamma_{t-1}^{(i)})\\rangle_{q(\\gamma_{t-1}^{(i)})}+H_{q}(\\gamma_{t-1}^{(i)}|x_{t-1},a_{t-1})\\bigg)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{a_{1},\\gamma_{1}^{(i)}}q(x_{1}^{(i)},a_{1},\\gamma_{1}^{(i)})=\\sum_{\\gamma_{0}^{(i)}}P_{\\operatorname*{det}}(x_{1}^{(i)}|\\gamma_{0}^{(i)})q(\\gamma_{0}^{(i)})\\,\\forall x_{1},i}\\\\ &{\\displaystyle\\sum_{a_{t+1},\\gamma_{t+1}^{(i)}}q(x_{t+1}^{(i)},a_{t+1},\\gamma_{t+1}^{(i)})=\\sum_{x_{t}^{\\operatorname*{min}},a_{t},\\gamma_{t}^{(i)}}P_{\\operatorname*{det}}(x_{t+1}^{(i)}|x_{t}^{\\operatorname*{ma}(i)},a_{t},\\gamma_{t}^{(i)})q(x_{t}^{\\operatorname*{pa}(i)},a_{t},\\gamma_{t}^{(i)})\\,\\,\\,\\forall x_{t+1}^{(i)},t,i\\in1}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and pseudo-marginal constraints, ", "page_idx": 21}, {"type": "text", "text": "In the same way as for the non-factorized MDP, it is easy to see that $\\hat{F}_{\\lambda=1}^{\\mathrm{planning}}\\le\\hat{F}^{\\mathrm{c}}$ det. planning, while both upper bound the exact $F_{\\lambda=1}^{\\mathrm{planning}}$ . This means that in the case of (exponentiated) multiplicative rewards ( $\\lambda=1$ ) our proposed bound should be no worse than determinization. Also, $\\hat{F}_{\\lambda=1}^{\\mathrm{det.}}$ planning allows us to compute the determinization bound as a single, solvable, concave optimization, rather than having to resort to sampling and solving a theoretically infinite number of deterministic problems (this holds when each problem would be solved using an LP relaxation of the MAP problem). ", "page_idx": 21}, {"type": "text", "text": "H Empirical validation: details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "H.1 Synthetic MDPs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The synthetic MDPs use binary state and actionss, $T=4$ time steps and follow the connectivity scheme of Fig. 1[Right]. The stochasticity level of the MDPs is controlled by generating their dynamics according to $P(\\overline{{x}}_{t+1}|x_{t},a_{t})=\\bar{P}_{x_{t+1},x_{t},a_{t}}^{s}/Z_{x_{t},a_{t}}$ where $\\bar{P}_{x_{t+1},x_{t},a_{t}}\\sim\\dot{U}[0,1]$ and choosing the exponent $s$ and the divisor $Z_{x_{t},a_{t}}$ to obtain normalized distributions of the desired total normalized entropy $H_{\\mathrm{MDP}}$ . Each entity has two parent entities and we provide a single reward of 1 for reaching the state 0 of entity 1 at the last step $T$ . Having a single reward allows comparing methods with arbitrary $\\lambda$ and create a pure planning problem (see Section 4.2). ", "page_idx": 21}, {"type": "text", "text": "H.2 Reactivity avoidance of MAP and MMAP inference ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "As discussed in Section 4.2, a common flaw among all inference types (other than planning inference) is that they cannot anticipate reacting to the environment. Even the tightest one, MMAP, makes plans assuming that it will only be able to take a predefined sequence of actions. This can severely underestimate the value of an action if, to extract future reward, reacting to the environment state is necessary. In other words, MMAP is optimal only when the best policy $\\pi_{t}(a_{t}|x_{t})$ ignores the state and can be represented as $\\pi_{t}(a_{t})$ . MAP inference shares that limitation, and additionally lacks path integration. I.e., MAP inference makes decisions based on the score of a single action-state trajectory, rather than a combination of these. ", "page_idx": 21}, {"type": "text", "text": "In order to analyze the behavior of different inference types of inference with reactivity, we create an MDP with two entities, each with categorical states $0,\\ldots,5$ . We use a horizon of $T=7$ time steps, placing reward only on the last time step. There are a total of 8 actions. ", "page_idx": 21}, {"type": "text", "text": "The first entity describes the location of the agent, and has a special goal state $\"0\"$ that needs to be reached at the last time step. The second entity describes the dynamics of the first entity and the reward achieved at the goal. Actions $0,\\ldots,5$ allow the agent to move to any location in a single step. Actions 6, 7 allow the agent to modify the dynamics of the first entity, respectively decreasing or increasing the needed level of reactivity of a planner. ", "page_idx": 21}, {"type": "text", "text": "In more detail, the second entity has deterministic dynamics and acts as a knob that the agent can freely turn up or down at each time step: ", "page_idx": 22}, {"type": "equation", "text": "$$\nx_{t+1}^{(2)}=\\left\\{\\!\\!\\begin{array}{l l}{x_{t}^{(2)}-1,}&{\\mathrm{if}\\;a=6\\;\\mathrm{and}\\;0<x_{t}^{(2)}}\\\\ {x_{t}^{(2)}+1,}&{\\mathrm{if}\\;a=7\\;\\mathrm{and}\\;x_{t}^{(2)}<5}\\\\ {x_{t}^{(2)},}&{\\mathrm{otherwise}.}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The values of that knob $x_{t}^{(2)}$ alter the dynamics of the first entity, requiring more or less reactivity: ", "page_idx": 22}, {"type": "equation", "text": "$$\nx_{t+1}^{(1)}=\\left\\{\\begin{array}{l l}{\\mathrm{if~}x_{t}^{(1)}=0\\mathrm{~or~}6\\leq a}&{\\sim U[1,5]}\\\\ {\\mathrm{if~}x_{t}^{(1)}\\neq0\\mathrm{~and~}0\\leq a<6}&{\\left\\{x_{t}^{(1)}+a\\mod6\\quad\\mathrm{~with~probability~}x_{t}^{(2)}/5\\right.}\\\\ {\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\mathrm{~with~probability~}1-x_{t}^{(2)}/5}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "In words, if the agent is at the goal state 0 or the reactivity of the environment is modified, it will jump to a random non-goal state. If the agent is not at the goal state and xt(2)= 5, it can use the actions $0,\\ldots,5$ to jump to any desired state with absolute certainty. As the value of the knob $x_{t}^{(2)}$ is reduced, the agent starts losing control of where it is going, and it is instead more likely to jump to the goal. Now, the final ingredient, the reward, is: ", "page_idx": 22}, {"type": "equation", "text": "$$\nR_{t}(x_{t}^{(1)},x_{t}^{(2)})=\\left\\{\\begin{array}{l l}{1.0,}&{\\mathrm{if~}x_{t}^{(1)}=0\\mathrm{~and~}x_{t}^{(2)}=5\\mathrm{~and~}t=T}\\\\ {0.33,}&{\\mathrm{if~}x_{t}^{(1)}=0\\mathrm{~and~}x_{t}^{(2)}<5\\mathrm{~and~}t=T}\\\\ {0.0,}&{\\mathrm{otherwise}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "When we put all these pieces together, we have the following situation: to achieve the maximum reward, we want to keep the the knob $x_{t}^{(2)}$ at its \u201cmaximum reactivity\u201d value of 5. But when we do that, the action that will take the agent to the goal depends on the state it is located at. Therefore the agent needs to react to the location that it is at. And when planning ahead, to properly choose the best first action to take, the agent needs to score the options according to this future ability to react to actions. By turning the knob $\\bar{x}_{t}^{(2)}$ all the way down to 0, we can jump to the goal by taking any action $0,\\ldots,6$ , regardless of where we are, i.e., we do not need any reactivity. ", "page_idx": 22}, {"type": "text", "text": "We present this factored MDP, initialized at $x_{0}^{(1)}=0,x_{0}^{(2)}=5$ to both VBP and SOGBOFA-LC\\*. Their behavior is very different. VBP is aware that it can jump to the goal state $x_{T}^{(1)}=0$ at any time as long as it is not there yet. So it just idles in the other states, keeping the reactivity at the maximum level, until the last time step, in which it jumps to the goal, capturing a reward of 1.0. SOGBOFA-LC\\*, even though it is replanning at each state, can only evaluate the possible actions by considering a single best sequence of non-reactive actions at a time. Since the reactive plans that VBP prefers are not \u201cvisible\u201d to SOGBOFA-LC\\*, it decides to use the first 5 actions to reduce the reactivity of the environment all the way down to $x_{T-1}^{(2)}=0$ . In that way, it is guaranteed to be able to jump to the goal in the last instant without having to be reactive, even if only capturing a reward 0.33. ", "page_idx": 22}, {"type": "text", "text": "This example highlights how SOGBOFA-LC\\* (and MMAP planning in general) struggle with reactive environments in which the best future actions cannot be known at the current time step. Essentially, the agent is myopic to the fact that it will be able to replan and chooses to be conservative and reactivity avoidant, proposing sequences of actions that will be guaranteed to work in any scenario, even if this reduces the obtained reward. We can make the difference between VBP and SOGBOFA-LC\\*\u2019s performance arbitrarily large simply by increasing the number of states and planning horizon. ", "page_idx": 22}, {"type": "text", "text": "MAP inference presents the same non-reactivity problems as MMAP, combined with the lack of integration across multiple paths. On this problem, the average reward achieved by a perfect MAP inference agent is $\\sim0.13$ . ", "page_idx": 22}, {"type": "text", "text": "H.3 Experimental Details on ICAPS 2011 International Probabilistic Planning Competition Problems ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "H.3.1 Experimental Settings ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Six different inference approaches, MFVI-Bwd, CSVI-Bwd, ARollout, SOGBOFA-LC, VI LP, VBP, and a random agent are evaluated on 6 different domains (Crossing traffic, Elevators, Game of life, ", "page_idx": 22}, {"type": "text", "text": "Skill teaching, Sysadmin, and Traffic) used in the ICAPS 2011 International Probabilistic Planning Competition (Sanner, 2011). Figure 3 shows the average cumulative reward of all domains. Table 2 shows the corresponding objective and reference for each approach. There are 10 instances for each domain. All instances have a horizon of 40 and a discount factor of 1. The cumulative reward is averaged over 30 simulations for each instance and the plotted error bar shows its standard error of the mean. Given the current state $x_{t}$ , the transition probability $P(x_{t+1}|a_{t},x_{t})$ , and the reward function $R$ , each approach infers the best next action $a_{t}$ at each time step. The reward function for all 6 domains only depend on the state $x$ . The cumulative reward is the sum of the rewards of the initial state and the following 39 states. ", "page_idx": 23}, {"type": "text", "text": "For all inference approaches, we run with a look ahead horizon of both 4 and 9. We follow the settings in (Wu and Khardon, 2022) where the look ahead horizon is truncated if it extends beyond the remaining time steps. The horizon with the higher average cumulative reward is then selected for each instance. The experiment evaluates over two look ahead horizons due to the observation that for some domain instances a longer horizon may lead to worse estimates. For each simulated run, the same set of random numbers are applied to the simulated environment to reduce noise when comparing different inference approaches. All experiments were ran on CPU machines in the cloud. ", "page_idx": 23}, {"type": "table", "img_path": "TXsRGrzICz/tmp/e469c44b44f8e9bc403225a616d73c071b22bd8a19065c24f7f4c3e885972f97.jpg", "table_caption": ["Table 2: Six different inference approaches used for comparison in the ICAPS 2011 International Probabilistic Planning Competition problems and their corresponding objective and reference. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "H.3.2 MFVI-Bwd ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The backward mean field variational inference approach (MFVI-Bwd) is based on the implementation in $\\mathrm{W}\\mathbf{u}$ and Khardon, $2022-\\mathrm{url}$ : https://github.com/Zhennan-Wu/AISPFS), in which the mean field approximation $q_{\\phi}$ is the product of independent factors. $q_{\\phi}$ is first obtained by maximizing the ELBO of the true posterior under the condition that the maximum accumulated reward is reached using the EM algorithm. The action is then selected based on the marginal $q_{\\phi}(a)$ . In the experiment, the maximum number of iterations is set to 100 and the convergence threshold is set to 0.1 for the EM algorithm. MFVI-Bwd experiments were ran on a single CPU machine with 32 virtual cores. All 30 simulations were ran in parallel for each task instance. ", "page_idx": 23}, {"type": "text", "text": "H.3.3 CSVI-Bwd ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The backward collapsed variational inference approach (CSVI-Bwd) is proposed in (Wu and Khardon, 2022). It uses collapsed variational inference to effectively marginalizes out variables other than the actions to achieve a tighter ELBO. The authors have shown that this approach out performs MFVI-Bwd. CSVI-Bwd experiments were ran with the same hardware setup as MFVI-Bwd. ", "page_idx": 23}, {"type": "text", "text": "H.3.4 ARollout ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The Algebraic Rollout Algorithm (ARollout) introduced in Cui et al., 2015 is equivalent to belief propagation when conditioned on actions as shown in Cui et al., 2018. In our experimental setting which the search depth is fixed with no computation time limit, ARollout is equivalent to the original SOGBOFA (symbolic online gradient based optimization for factored actions) approach introduced in (Cui and Khardon, 2016). While ARollout performs approximate aggregate rollout simulations to evaluate each action, SOGBOFA uses the gradient of the accumulated reward to update actions for exploration and can be advantageous when the action space is large given a computation time constraint. In this experiment, we use the results of forward belief propagation to represent ARollout. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "For each time step, the action with the highest estimated accumulated reward is selected. The estimated accumulated reward is calculated by running a forward pass on the factored MDP representing the problem conditioned on the next action. ARollout experiments were ran on CPU machines with 2 virtual cores. All simulations were ran in parallel. ", "page_idx": 24}, {"type": "text", "text": "H.3.5 SOGBOFA-LC ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "SOGBOFA-LC is based on the Lifted Conformant SOGBOFA implemented in (Cui et al., $2019-\\mathrm{url}$ : https://github.com/hcui01/SOGBOFA). SOGBOFA-LC has two differences over the original SOGBOFA. First, it uses a lifted graph that saves computation by identifying same operations. This improvement in computation speed doesn\u2019t have an effect in our experiment result since we provide enough computation time for the given maximum search depth. Second, it uses conformant solutions, which the evaluation of the next action is based on a linear rollout plan that best supports the action. This is achieved by calculating the gradient with respect to all actions within the search depth. ", "page_idx": 24}, {"type": "text", "text": "In our experiment, the search depth is set to 9 or 4 based on the look ahead horizon. The number of gradient updates is set to 500 following the experimental setting in Wu and Khardon, 2022. The allowed time is set to 50000 per iteration, which is sufficient for 500 updates given the assigned depth. SOGBOFA-LC experiments were ran on CPU machines with 4 virtual cores sequentially. ", "page_idx": 24}, {"type": "text", "text": "Note that the RDDL flies in the repository (https://github.com/hcui01/SOGBOFA or https:// github.com/Zhennan-Wu/AISPFS/tree/master/SOGBOFA) have different initial states from the original competition for some instances in the elevator and skill teaching domains. These differences were corrected to match the original competition settings in our experiments. Modifications were also made to use the same random numbers in the environment as other experiments and to measure the standard error of the mean instead of the standard deviation. ", "page_idx": 24}, {"type": "text", "text": "H.3.6 VI LP ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The Variational Inference Linear Programming (VI LP) approach uses the GLOP solver in Google\u2019s OR-Tools (Perron and Furnon, 2024) to solve the linear programming (LP) problem derived from each task instance with the target of maximizing the expected accumulated reward. Constraints on states that are specified in the original RDDL problem is added to the LP problem. Among the six domains only the elevator and crossing traffic domains have such constraints. These constraints specifies that the elevator/robot can\u2019t be at different floors/locations at the same time step. ", "page_idx": 24}, {"type": "text", "text": "The solver is ran for each next action at each time step. The next action that has the highest estimated expected accumulated reward based on the solver is selected. See Section 3.1 for more details of this approach. VI LP experiments were ran on CPU machines with 16 virtual cores. All simulations were ran in parallel. For each iteration, LP solvers for each next action were ran concurrently with multiprocessing. ", "page_idx": 24}, {"type": "text", "text": "H.3.7 VBP ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Note that setting a value for $\\lambda$ is meaningless if the reward can have arbitrary scaling (and for the case of IPPC in which rewards are additive, the scaling is indeed arbitrary). Therefore, we first normalize all rewards to have a maximum point-to-point variation of 1.0. $\\lambda$ is chosen as a reward multiplier on top of that normalization. The closer to 0 that we set $\\lambda$ then, the closer we are to the additive limit. Although outside the scope of this work, it is actually possible to set $\\lambda$ to exactly zero in VBP (the problem remains non-convex, but the message updates within each time step become an LP problem). However, at least in a straighforward implementation, we observed such message updates to not result in a good optimization of our score function, and often not converging. We attribute this to the degeneracy of the solutions in the limit. Using instead a small value of $\\lambda=0.3$ had a favorable effect on convergence, while remaining close enough to the additive limit. Empirically, we found that most problems were not very sensitive to the choice of $\\lambda$ . ", "page_idx": 24}, {"type": "text", "text": "For each time step, VBP messages are propagated concurrently for a maximum of 150K iterations with 0.1 damping. The $\\epsilon$ value is annealed every 300 iterations from a value of 1 to 0.01 based on the formula described in Appendix D. The next action with the highest expected accumulated reward is then selected. See Section 3.1 and Appendix D for more details of this approach. VBP experiments were ran on CPU machines with 2 virtual cores. All simulations were ran in parallel. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We provided proofs and experimental results to justify the claims. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 26}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: In section 4.2, we mention settings in which other approaches may outperform our proposed approach. In Figure 2 we show when our proposal perform worse than existing methods. This work very much focuses on discussing and comparing the limitations of different approaches to inference. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Complete proofs can be found in Appendix A, B, C, D, refapp:bounding, and G. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The experimental details can be found in Appendix H. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We plan to release the code for our proposed approaches, VI LP and VBP, with the camera-ready version of the paper. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: All experimental detail and parameters are specified in Appendix H. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We reported standard errors of the mean over multiple evaluation seeds. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [No] ", "page_idx": 29}, {"type": "text", "text": "Justification: We provided detail information of the type of compute worker used for the experiment but did not record the time of execution. The main contribution of this work is to provide a theoretical framework for comparing different planning algorithm. The different approaches ran in the experiment uses different hardware and programming language, therefore the measured time of computation may not be directly comparable. Our experimental setting on the IPPC 2011 competition is based on experiments in (Wu and Khardon, 2022), which there are no limit to the run time and the goal is to understand the quality of decisions. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Our experiments do not include human subjects and all experiment are based on simulated environments. Our main contribution is providing a theoretical framework for analyzing different planning approaches and we do not expect to have immediate social impacts. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [No] ", "page_idx": 30}, {"type": "text", "text": "Justification: Our main contribution in this paper is providing a theoretical framework for analyzing different planning approaches. We do not expect social impacts at this stage of development. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: No trained model or dataset for training is used in this work ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: All approaches used for comparison in experiments are cited properly. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. ", "page_idx": 30}, {"type": "text", "text": "\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: No new assets are released with this submission. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}]