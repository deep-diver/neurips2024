[{"type": "text", "text": "Langevin Unlearning: A New Perspective of Noisy Gradient Descent for Machine Unlearning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Eli Chien Department of Electrical and Computer Engineering Georgia Institute of Technology Georgia, U.S.A. ichien6@gatech.edu ", "page_idx": 0}, {"type": "text", "text": "Haoyu Wang Department of Electrical and Computer Engineering Georgia Institute of Technology Georgia, U.S.A. haoyu.wang@gatech.edu ", "page_idx": 0}, {"type": "text", "text": "Ziang Chen Department of Mathematics Massachusetts Institute of Technology Massachusetts, U.S.A. ziang@mit.edu ", "page_idx": 0}, {"type": "text", "text": "Pan Li Department of Electrical and Computer Engineering Georgia Institute of Technology Georgia, U.S.A. panli@gatech.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Machine unlearning has raised significant interest with the adoption of laws ensuring the \u201cright to be forgotten\u201d. Researchers have provided a probabilistic notion of approximate unlearning under a similar definition of Differential Privacy (DP), where privacy is defined as statistical indistinguishability to retraining from scratch. We propose Langevin unlearning, an unlearning framework based on noisy gradient descent with privacy guarantees for approximate unlearning problems. Langevin unlearning unifies the DP learning process and the privacy-certified unlearning process with many algorithmic benefits. These include approximate certified unlearning for non-convex problems, complexity saving compared to retraining, sequential and batch unlearning for multiple unlearning requests. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "With recent demands for increased data privacy, owners of these machine learning models are responsible for fulfilling data removal requests from users. Certain laws are already in place guaranteeing the users\u2019 \u201cRight to be Forgotten\u201d, including the European Union\u2019s General Data Protection Regulation (GDPR), the California Consumer Privacy Act (CCPA), and the Canadian Consumer Privacy Protection Act (CPPA) [1]. Merely removing user data from the training data set is insufficient, as machine learning models are known to memorize training data information [2]. It is critical to also remove the information of user data subject to removal requests from the machine learning models. This consideration gave rise to an important research direction, referred to as machine unlearning [3]. ", "page_idx": 0}, {"type": "text", "text": "Naively, one may retrain the model from scratch after every data removal request to ensure a \u201cperfect\u201d privacy guarantee. However, it is prohibitively expensive in practice when accommodating frequent removal requests. To avoid complete retraining, various machine unlearning methods have been proposed, including exact [4\u20136] as well as approximate approaches [1, 7\u201310]. Exact approaches ensure that the unlearned model would be identical to the retraining one in distribution. Approximate approaches, on the other hand, allow for slight misalignment between the unlearned model and the retraining one in distribution under a similar definition to Differential Privacy (DP) [11]. ", "page_idx": 0}, {"type": "image", "img_path": "3LKuC8rbyV/tmp/f24a47b2191963b745ef5d8a4ec5d1efaf7f491284f164d9a33a00eeb27cb512.jpg", "img_caption": ["Figure 1: The geometric interpretation of relations between learning and unlearning. (Left) RDP guarantee of the learning process induces a regular polyhedron. Smaller $\\varepsilon_{0}$ implies an \u201ceasier\u201d unlearning problem. (Right) Learning and unlearning processes on adjacent datasets. It illustrates our main idea and results. More learning iteration gives worse privacy (privacy erosion [12]) while more unlearning iteration gives better privacy, which we termed this phenomenon as privacy recuperation. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "1.1 Our Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Learning with noisy gradient methods, such as DP-SGD [13], is widely adopted for privatizing machine learning models with DP guarantee. Intuitively, a learning process with a stronger DP guarantee implies an \u201ceasier\u201d unlearning problem as depicted in Figure 1. However, it is unclear if fine-tuning with it on the updated dataset subject to the unlearning request provides an approximate unlearning guarantee, how the DP learning guarantee affects unlearning, and computational benefit compared to retraining. In this work, we provide an affirmative answer for the empirical risk minimization problems with smooth objectives. We propose Langevin unlearning, an approximate unlearning framework based on projected noisy gradient descent (PNGD). Our core idea can be interpreted via a novel unified geometric view of the learning and unlearning processes in Figure 1, which naturally bridges DP and unlearning. Given sufficient learning iterations via the learning process $\\mathcal{M}$ , we first show that PNGD converges to a unique stationary distribution $\\nu_{D}$ for any dataset $\\mathcal{D}$ (Theorem 3.1). Comparing $\\nu_{\\mathscr D}$ with the stationary distribution $\\nu_{\\mathcal{D}^{\\prime}}$ for any of its adjacent dataset $\\mathcal{D}^{\\prime}$ , the learning process shows R\u00e9nyi DP with privacy $\\mathrm{loss}^{1}\\ \\varepsilon_{0}$ . Given a particular unlearning request $\\mathcal{D}\\rightarrow\\mathcal{D}^{\\prime}$ , the unlearning process $\\boldsymbol{\\mathcal{U}}$ can be interpreted as moving from $\\nu_{D}$ to $\\nu_{\\mathcal{D}^{\\prime}}$ from $\\varepsilon_{0}$ -close to $\\varepsilon$ -close. In practice, due to the unlearning process, the unlearning privacy loss $\\varepsilon$ can be set much smaller than $\\varepsilon_{0}$ , while on the other hand, a stronger initial RDP guarantee, i.e., smaller $\\varepsilon_{0}$ , allows for less unlearning iterations to achieve the desired $\\varepsilon$ . Besides the above DP-unlearning bridge, this framework also brings many beneftis including (1) a capability of dealing with non-convex problems in theory, which to the best of our knowledge, no previous approximate unlearning framework can tackle, (2) better privacy-utility trade-off in practice compared to state-of-the-art approximate unlearning approach [8] in strongly convex settings, (3) a provably computational benefti compared with model retraining, and (4) a friendly extension to sequential and batch settings with multiple unlearning requests. ", "page_idx": 1}, {"type": "text", "text": "We prove the intuition in Fig. 1 formally in Theorem 3.2. We show that $K$ unlearning iterations lead to an exponentially fast privacy loss decay $\\begin{array}{r}{\\varepsilon\\leq\\exp(-\\frac{1}{\\alpha}\\sum_{k=0}^{K-1}R_{k})\\varepsilon_{0}}\\end{array}$ , where $\\alpha$ is the order of R\u00e9nyi divergence and $R_{k}$ is the strict privacy improving rate depends on the problem settings with an iteration independent strictly positive lower bound $\\mathbf{\\bar{\\bar{\\calR}}}>0$ . Our result is based on convergence analysis of Langevin dynamics [15]. The sampling essence of PNGD allows for a provable unlearning guarantee for non-convex problems [16,17]. Our characterization of $\\varepsilon_{0}$ allows an extension of the recent results that PNGD learning satisfies R\u00e9nyi DP for convex problems [12,18,19] to non-convex problems as summarized in Theorem 3.3. Our key technique is to carefully track the constant of log-Sobolev inequality [20] (LSI) along the learning and unlearning processes and leverage the boundedness property of the projection step via results of [21]. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Regarding the computational benefit compared to model retraining, we show iteration complexity saving by comparing two R\u00e9nyi differences, the one between initialization $\\nu_{0}$ and $\\nu_{\\mathcal{D}^{\\prime}}$ , which is at least $\\Omega(1)$ in the worst case versus the other one between the learning convergent distribution $\\nu_{\\mathscr{D}}$ and $\\nu_{\\mathcal{D}^{\\prime}}$ , i.e., $\\varepsilon_{0}$ which is shown to be $O(1/n^{2})$ for a dataset of size $n$ . Such a gap demonstrates that Langevin unlearning is more efficient than retraining, especially for the dataset with large $n$ . ", "page_idx": 2}, {"type": "text", "text": "For sequential unlearning with multiple unlearning requests, we composite the privacy loss bound for single-step requests via the weak triangle inequality of R\u00e9nyi divergence [22], which yields a sequential unlearning procedure that achieves privacy loss $\\varepsilon$ for each request (Corollary 3.4). For batch unlearning, $\\varepsilon_{0}$ is changed to incorporate the batch size (Theorem 3.3). ", "page_idx": 2}, {"type": "text", "text": "Beyond theoretical contributions, we also conduct empirical evaluation. Despite the provable orderwise improvement in $n$ compared to re-training, our current theory has a limitation by relying on some constants that are undetermined or can be only loosely determined in the non-convex setting. Therefore, we focus on logistic regression tasks for empirical evaluation. Compared with the state-ofthe-art gradient-based certified approximate unlearning solution [8] that requires strong convexity, we achieve a superior privacy-utility-complexity trade-off. Although success in the convex case may not directly imply success in non-convex settings, we leave tightening these constants as a future study. For this, we discuss potential alleviation and future direction in Appendix A and 5, respectively. ", "page_idx": 2}, {"type": "text", "text": "1.2 Related Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Unlearning with privacy guarantees. Prior approximate unlearning works require (strong) convexity of the objective function [1,7,8]. Their analysis is based on the sensitivity analysis of the optimal parameter. Since the optimal parameter is not even unique in the non-convex setting, it is unclear how their analysis can be generalized beyond convexity. In contrast, we show that the law of our PNGD learning process admits a unique stationary distribution even for non-convex problems.Authors of [1,7] leverage a second-order update which requires computing Hessian inverse and thus is not scalable for high-dimensional problems. While they only require one unlearning iteration, we show in our experiment that one PNGD unlearning iteration is sufficient for strongly convex loss to achieve satisfied privacy with comparable utility to retraining as well. Neel et al. [8] leverage PGD for learning and unlearning, and achieve the privacy guarantee via publishing the final parameters with additive Gaussian noise. We show in our experiment that our Langevin unlearning strategy provides a better privacy-utility-complexity trade-off compared to this approach. Ullah et al. [5] focus on exact unlearning by leveraging variants of noisy (S)GD. Their analysis is based on total variation stability which is different from our analysis based on R\u00e9nyi divergence. Also, their analysis does not directly generalize to approximate unlearning. Several works focus on extending the unlearning problems for adaptive unlearning requests [6,9,23]. While we focus on the non-adaptive setting, it is possible to show that Langevin unlearning is also capable of adaptive unlearning requests as we do not keep any non-private internal state. We left a rigorous discussion on this as future work. Chourasia et al. [23] also leverage Langevin dynamic analysis in their work. However, their unlearning definition is different from the standard literature as ours2. ", "page_idx": 2}, {"type": "text", "text": "Differential privacy of noisy gradient methods. A pioneer work [24] studied the DP properties of Langevin Monte Carlo methods. Yet, they do not propose using noisy GD for general machine learning problems. A recent line of work [12, 18, 25] shows that projected noisy (S)GD training exhibits DP guarantees based on the analysis of Langevin dynamics [15, 26] under the strong convexity assumption. In the meanwhile, Altschuler et al. [19] also provided the DP guarantees for projected noisy SGD training but with analysis based on Privacy Amplification by Iteration [27] under the convexity assumption. None of these works study how PNGD can be leveraged for machine unlearning or DP guarantees for non-convex problems. ", "page_idx": 2}, {"type": "text", "text": "Sampling literature. Non-asymptotic convergence analysis for Langevin Monte Carlo has a long history [28,29]. The seminal works [15,24] proved non-asymptotic convergence analysis in R\u00e9nyi divergence under strong convexity. Many works improve upon them by either working with weaker isoperimetric inequalities or different notions of convergence [30,31]. See [26] for a more thorough review along this direction. While these works mainly focus on convergence to the unbiased limit (i.e., the limiting distribution for an infinitesimal step size), we have biased limits (i.e., the limiting distribution for a constant step size, such as our $\\nu_{D}$ ) in machine unlearning problems. Recently Altschuler et al. [32] initiated the question of studying the properties and convergence to the biased limit. Our work provides a new important application, machine unlearning, for these astonishing theoretical results in the sampling literature. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "The rest of the paper is organized as follows. In Section 2, we provide preliminaries and problem setup. The theoretical results of Langevin unlearning are in Section 3. We conclude with experiments in Section 4. Due to the space limit, all proofs and future directions are deferred to Appendices. ", "page_idx": 3}, {"type": "text", "text": "2 Preliminaries and Problem Setup ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We consider the empirical risk minimization (ERM) problem. Let $\\mathbf{\\mathcal{D}}=\\{\\mathbf{d}_{i}\\}_{i=1}^{n}$ be a training dataset with $n$ data points ${\\bf d}_{i}$ taken from the universe $\\mathcal{X}$ . Let $\\begin{array}{r}{f_{\\mathcal{D}}(x)=\\frac{1}{n}\\sum_{i=1}^{n}f(x;\\mathbf{d}_{i})}\\end{array}$ be the objective function. We aim to minimize with learnable parameter $\\boldsymbol{x}\\in\\mathcal{C}_{R}$ , where $\\mathcal{C}_{R}=\\{x\\in\\mathbb{R}^{d}\\ |\\ \\|x\\|\\leq R\\}$ is a closed ball of radius $R$ . We denote $\\Pi_{{\\mathcal{C}}_{R}}:\\mathbb{R}^{d}\\mapsto{\\mathcal{C}}_{R}$ to be an orthogonal projection to $\\displaystyle\\mathcal{C}_{R}$ . The norm $\\|\\cdot\\|$ is standard Euclidean $\\ell_{2}$ norm if not specified. $\\mathscr{P}(\\mathscr{C})$ is denoted as the set of all probability measures over a closed convex set $\\mathcal{C}$ . Standard definitions such as convexity can be found in Appendix C. Finally, we use $x\\sim\\nu$ to denote that a random variable $x$ follows the probability distribution $\\nu$ . To control the convergence behavior of (P)NGD, it is standard to check an isoperimetric condition known as log-Sobolev inequality [20], described as follows. ", "page_idx": 3}, {"type": "text", "text": "Definition 2.1 (Log-Sobolev Inequality $(C_{\\mathrm{LSI}}.\\mathrm{LSI})$ ). A probability measure $\\nu\\in\\mathcal{P}(\\mathbb{R}^{d})$ is said to satisfy Logarithmic Sobolev Inequality with constant $C_{\\mathrm{LSI}}$ if ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\forall\\,\\rho\\in\\mathcal{P}(\\mathbb{R}^{d}),\\;\\;D_{1}(\\rho||\\nu)\\leq\\frac{C_{\\mathrm{LSI}}}{2}\\mathbb{E}_{x\\sim\\rho}\\left\\|\\nabla\\log\\frac{\\rho(x)}{\\nu(x)}\\right\\|^{2},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $D_{1}(\\rho||\\nu)$ is the Kullback\u2013Leibler divergence. ", "page_idx": 3}, {"type": "text", "text": "2.1 Privacy Definition for Learning and Unlearning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We say two datasets $\\mathbf{\\mathcal{D}}=\\{\\mathbf{d}_{i}\\}_{i=1}^{n}$ and $\\mathbf{\\mathcal{D}^{\\prime}}=\\{\\mathbf{d}_{i}^{\\prime}\\}_{i=1}^{n}$ are adjacent if they \u201cdiffer\u201d only in one index $i_{0}\\in[\\dot{n}]$ so that $\\mathbf{d}_{i}=\\mathbf{d}_{i}^{\\prime}$ for all $i\\neq i_{0}$ unless otherwise specified. Furthermore, we say two datasets $\\mathcal{D}$ and $\\mathcal{D}^{\\prime}$ are adjacent with a group size of $S\\geq1$ if they differ in at most $S$ indices. We next introduce a useful idea termed R\u00e9nyi difference. ", "page_idx": 3}, {"type": "text", "text": "Definition 2.2 (R\u00e9nyi difference). Let $\\alpha>1$ . For a pair of probability measures $\\nu,\\nu^{\\prime}$ with the same support, the $\\alpha$ R\u00e9nyi difference $d_{\\alpha}(\\nu,\\nu^{\\prime})$ is defined as $d_{\\alpha}\\bar{(\\nu,\\nu^{\\prime})}=\\operatorname*{imax}\\left(D_{\\alpha}(\\nu||\\nu^{\\prime}),D_{\\alpha}(\\nu^{\\prime}||\\nu)\\right)$ , where $D_{\\alpha}(\\nu||\\nu^{\\prime})$ is the $\\alpha$ R\u00e9nyi divergence $D_{\\alpha}(\\nu||\\nu^{\\prime})$ defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\nD_{\\alpha}(\\nu||\\nu^{\\prime})=\\frac{1}{\\alpha-1}\\log\\left(\\mathbb{E}_{x\\sim\\nu^{\\prime}}\\left[\\frac{\\nu(x)}{\\nu^{\\prime}(x)}\\right]^{\\alpha}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We are ready to introduce the formal definition of differential privacy and unlearning. ", "page_idx": 3}, {"type": "text", "text": "Definition 2.3 (R\u00e9nyi Differential Privacy (RDP) [22]). Let $\\alpha\\ >\\ 1$ . A randomized algorithm $\\mathcal{M}\\,:\\,\\mathcal{X}^{n}\\,\\mapsto\\,\\mathbb{R}^{d}$ satisfies $(\\alpha,\\varepsilon)$ -RDP if for any adjacent dataset pair ${\\mathcal D},{\\mathcal D}^{\\prime}\\,\\in\\,{\\mathcal X}^{n}$ , the $\\alpha$ R\u00e9nyi difference $d_{\\alpha}(\\nu,\\nu^{\\prime})\\leq\\varepsilon$ , where $\\mathcal{M}(\\mathcal{D})\\sim\\nu$ and $\\mathcal{M}(\\mathbf{\\bar{\\boldsymbol{D}}^{\\prime}})\\,\\sim\\nu^{\\prime}$ . ", "page_idx": 3}, {"type": "text", "text": "It is known to the literature that an $(\\alpha,\\varepsilon)$ -RDP guarantee can be converted to the popular $(\\epsilon,\\delta)$ -DP guarantee [11] relatively tight [22]. As a result, we will focus on establishing results with respect to $\\alpha$ R\u00e9nyi difference (and equivalently $\\alpha$ R\u00e9nyi divergence). Next, we introduce our formal privacy definition of unlearning. ", "page_idx": 3}, {"type": "text", "text": "Definition 2.4 (R\u00e9nyi Unlearning (RU)). Consider a randomized learning algorithm $\\mathcal{M}:\\mathcal{X}^{n}\\mapsto\\mathbb{R}^{d}$ and a randomized unlearning algorithm $\\mathcal{U}\\,:\\,\\mathbb{R}^{d}\\,\\times\\,\\mathcal{X}^{n}\\,\\times\\,\\mathcal{X}^{n}\\,\\mapsto\\,\\mathbb{R}^{d}$ . We say $(\\mathcal{M},\\mathcal{U})$ achieves $(\\alpha,\\varepsilon)$ -RU if for any $\\alpha>1$ and any adjacent datasets $\\mathcal{D},\\mathcal{D}^{\\prime}$ , the $\\alpha$ R\u00e9nyi difference $d_{\\alpha}(\\dot{\\rho},\\nu^{\\prime})\\leq\\varepsilon$ , where $\\mathcal{U}(\\mathcal{M}(\\mathcal{D}),\\mathcal{\\bar{D}},\\mathcal{D}^{\\prime})\\sim\\rho$ and $\\dot{M}(\\dot{\\mathcal{D}}^{\\prime})\\sim\\nu^{\\prime}$ . ", "page_idx": 3}, {"type": "text", "text": "Notably, our Definition 2.4 can be converted to the standard $(\\epsilon,\\delta)$ -unlearning definition [1, 7, 8], similar to RDP-DP conversion [22]. Since we work with the replacement definition of dataset adjacency, to \u201cerase\u201d a data point ${\\bf d}_{i}$ we can simply replace it with any data point $\\mathbf{d}_{i}^{\\prime}\\in\\mathcal{X}$ for the updated dataset $\\mathcal{D}^{\\prime}$ in practice. ", "page_idx": 4}, {"type": "text", "text": "3 Langevin Unlearning: Main Results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We propose to leverage projected noisy gradient descent for our learning and unlearning algorithm $\\mathcal{M}$ and $\\boldsymbol{\\mathcal{U}}$ . For $\\mathcal{M}$ , we propose to optimize the objective $f_{\\cal D}(x)$ with PNGD: ", "page_idx": 4}, {"type": "equation", "text": "$$\nx_{t+1}=\\Pi_{{\\mathcal{C}}_{R}}\\left(x_{t}-\\eta\\nabla f_{\\mathcal{D}}(x_{t})+\\sqrt{2\\eta\\sigma^{2}}W_{t}\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $W_{t}\\overset{\\mathrm{iid}}{\\sim}\\mathcal{N}(0,I_{d})$ and $\\eta,\\sigma^{2}>0$ are hyperparameters of step size and noise variance respectively. The initialization $x_{0}$ can be chosen arbitrarily in $\\displaystyle\\mathcal{C}_{R}$ unless specified. We assume the learning procedure will train the model until convergence $x_{\\infty}\\,=\\,\\mathcal{M}(D)$ for simplicity, where we prove in Theorem 3.1 that the law of this learning process (1) indeed converges to a unique stationary distribution when $\\nabla f_{\\mathcal{D}}$ is continuous. A similar \u201cwell-trained\u201d assumption has been also used in prior unlearning literature [1,7] and we will discuss the case of insufficient training later. After we obtain a learned parameter $\\mathcal{M}(\\mathcal{D})$ , an unlearning request arrives so that the training dataset changes from $\\mathcal{D}$ to $\\mathcal{D}^{\\prime}$ . For the unlearning algorithm $\\boldsymbol{\\mathcal{U}}$ , we propose to fine-tune the model parameters on the new objective $f_{D^{\\prime}}(y)$ with $K$ iterations of the same PNGD. ", "page_idx": 4}, {"type": "equation", "text": "$$\ny_{k+1}=\\Pi_{\\mathcal{C}_{R}}\\left(y_{k}-\\eta\\nabla f_{\\mathcal{D^{\\prime}}}(y_{k})+\\sqrt{2\\eta\\sigma^{2}}\\bar{W}_{k}\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\bar{W}_{k}\\ \\overset{\\mathrm{iid}}{\\sim}\\ \\mathcal{N}(0,I_{d})$ and $y_{0}~=~x_{\\infty}$ , which starts from the convergent point of the learning procedure. Throughout our work, we assume $f(x;\\mathbf{d})$ is $M$ -Lipschitz and $L$ -smooth in $x$ for any $\\mathbf{d}\\in\\mathcal{X}$ . Nevertheless, one can apply per-sample gradient clipping in (1) and (2) so that the $M$ - Lipschitz assumption can be dropped. In this case, our learning and unlearning processes admit the popular DP-SGD [13] without mini-batching. For the rest of the paper, we denote $\\nu_{t},\\rho_{k}$ as the laws of the processes $x_{t},y_{k}$ respectively. Recall that we also denote the limiting distribution of the learning process (1) as $\\nu_{D}$ for training dataset $\\mathcal{D}$ . ", "page_idx": 4}, {"type": "text", "text": "3.1 Limiting Distribution and General Idea ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "A key component of the Langevin unlearning is the existence, uniqueness, and stationarity of the limiting distribution $\\nu_{D}$ of the training process. We start with proving that $\\nu_{D}$ exists, is unique, and is a stationary distribution. Our proof is relegated to Appendix F, which is based on showing the ergodicity of the process (1) by leveraging results in [33]. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.1. Suppose that the closed convex set $\\mathcal{C}_{R}\\subset\\mathbb{R}^{d}$ is bounded with $\\displaystyle\\mathcal{C}_{R}$ having a positive Lebesgue measure and that $\\nabla f_{\\mathcal{D}}:\\mathcal{C}_{R}\\to\\mathbb{R}^{d}$ is continuous. The Markov chain $\\left\\{x_{t}\\right\\}$ in (1) admits $a$ unique invariant probability measure $\\nu_{D}$ on the Borel $\\sigma$ -algebra of $\\displaystyle\\mathcal{C}_{R}$ . Furthermore, for any $\\boldsymbol{x}\\in\\mathcal{C}_{R}$ , the distribution of $x_{t}$ conditioned on $x_{0}=x$ converges weakly to $\\nu_{D}$ as $t\\to\\infty$ . ", "page_idx": 4}, {"type": "text", "text": "If $\\mathcal{M}$ is known to be $(\\alpha,\\varepsilon_{0})$ -RDP for a $\\alpha>1$ , by definition we know that for all adjacent dataset $\\mathcal{D},\\mathcal{D}^{\\prime}$ , $d_{\\alpha}(\\nu_{\\mathcal{D}},\\nu_{\\mathcal{D}^{\\prime}})\\leq\\varepsilon_{0}$ . In the space of $\\mathcal{P}(\\mathcal{C}_{R})$ , this RDP guarantee gives a \u201cregular polyhedron\u201d, where vertices are $\\nu_{D},\\nu_{D^{\\prime}}$ and all adjacent vertices are of \u201clengths\u201d $\\varepsilon_{0}$ at most in R\u00e9nyi difference. We caveat that R\u00e9nyi difference is not a metric but the idea of the regular polyhedron is useful conceptually. As a result, the RDP guarantee of the learning process controls the \u201cdistance\u201d between distribution induced from adjacent dataset $\\mathcal{D}$ and $\\mathcal{D}^{\\prime}$ . Once we finish the learning process, we receive an unlearning request so that our dataset changes from $\\mathcal{D}$ to an adjacent dataset $\\mathcal{D}^{\\prime}$ . We need to move from $\\nu_{\\mathscr{D}}$ to $\\nu_{\\mathcal{D}^{\\prime}}$ at least $\\varepsilon$ close for a $(\\alpha,\\varepsilon)$ -RU guarantee. Intuitively, if the initial RDP guarantee is stronger (i.e., $\\varepsilon_{0}$ is smaller), unlearning becomes \u201ceasier\u201d at the cost of larger noise. When $\\varepsilon_{0}=\\varepsilon$ , we automatically achieve $(\\alpha,\\varepsilon)$ -RU without any unlearning update. One of our main contributions is to characterize how many PNGD unlearning iteration is needed to reduce $d_{\\alpha}(\\rho_{k},\\nu_{D^{\\prime}})$ from $\\varepsilon_{0}$ to $\\varepsilon$ , where $\\rho_{0}=\\nu_{\\mathcal{D}}$ . ", "page_idx": 4}, {"type": "text", "text": "For the unlearning process, note that the initial R\u00e9nyi difference between $\\rho_{0},\\nu_{\\mathcal{D}^{\\prime}}$ is provided by the RDP guarantees of the learning process. As a result, we are left to characterize the convergence of the process $y_{k}$ to its stationary distribution $\\nu_{\\mathcal{D}^{\\prime}}$ in R\u00e9nyi difference (Theorem 3.2). Since the privacy loss $\\varepsilon$ gradually decays with respect to unlearning iterations, we refer to this phenomenon as privacy recuperation. This is in contrast to the learning process, where prior work [12] has shown the worse privacy loss $\\varepsilon_{0}$ with respect to learning iterations and refers to that phenomenon as privacy erosion. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "3.2 Unlearning Guarantees ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Our first Theorem shows that $(\\mathcal{M},\\mathcal{U})$ achieves $(\\alpha,\\varepsilon)$ -RU, where $\\varepsilon$ decays monotonically in $K$ unlearning iterations starting from $\\varepsilon_{0}$ , condition on $\\mathcal{M}$ being $\\left(\\alpha,\\varepsilon_{0}\\right)$ -RDP. We provide the proof sketch in Appendix G.1 and formal proofs are deferred to Appendix G.2. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.2 (RU guarantee of PNGD unlearning). Assume for all $\\mathcal{D}\\,\\in\\,\\mathcal{X}^{n}$ , $f_{\\mathcal{D}}$ is $L$ -smooth, $M$ -Lipschitz and $\\nu_{\\mathscr{D}}$ satisfies $\\mathrm{\\partial^{\\prime}}\\;C_{L S I}-L S I.$ Let the learning process follow the PNGD update (1). Given $\\mathcal{M}$ is $(\\alpha,\\varepsilon_{0})$ -RDP and $y_{0}=x_{\\infty}=\\mathcal{M}(D),$ , for $\\alpha>1$ , the output of the $K^{t h}$ unlearning iteration along (2) (i.e., $y_{K}$ ) achieves $(\\alpha,\\varepsilon)$ -RU, where $\\begin{array}{r}{\\varepsilon\\leq\\exp\\left(-\\frac{1}{\\alpha}\\sum_{k=0}^{K-1}R_{k}\\right)\\varepsilon_{0}}\\end{array}$ and $R_{k}>0$ depends on the problem settings specified as follows: ", "page_idx": 5}, {"type": "text", "text": "1) For a general non-convex $f_{\\mathcal{D}}$ , we have $\\begin{array}{r}{R_{k}\\,=\\,\\frac{1}{2}\\left(\\frac{1}{((1+\\eta L)^{2}C_{k})^{2}}-\\frac{1}{((1+\\eta L)^{2}C_{k}+2\\eta\\sigma^{2})^{2}}\\right)}\\end{array}$ , where $C_{k+1}\\,\\leq\\,\\operatorname*{min}((1+\\eta L)^{2}C_{k}+2\\eta\\sigma^{2},\\tilde{C}),$ , $\\begin{array}{r}{\\tilde{C}\\,=\\,6(4(R+\\eta M)^{2}+2\\eta\\sigma^{2})\\exp(\\frac{4(R+\\eta M)^{2}}{2\\eta\\sigma^{2}})}\\end{array}$ , where $C_{0}=C_{L S I}$ and $R$ is the radius of the projected set $\\displaystyle\\mathcal{C}_{R}$ . ", "page_idx": 5}, {"type": "text", "text": "2) Suppose $f_{\\mathcal{D}}$ is convex. By choosing $\\begin{array}{r}{\\eta\\le\\frac{2}{L}}\\end{array}$ , we have $\\begin{array}{r}{R_{k}\\,=\\,\\frac{1}{2}\\left(\\frac{1}{(C_{k})^{2}}-\\frac{1}{(C_{k}+2\\eta\\sigma^{2})^{2}}\\right)}\\end{array}$ , where $C_{k+1}\\leq\\operatorname*{min}(C_{k}+2\\eta\\sigma^{2},\\tilde{C}).$ . ", "page_idx": 5}, {"type": "text", "text": "3) Suppose $f_{\\mathcal{D}}$ is $m$ -strongly convex. Let $\\frac{\\sigma^{2}}{m}<C_{L S I}$ and choosing $\\begin{array}{r}{\\eta\\le\\operatorname*{min}(\\frac{2}{m}(1-\\frac{\\sigma^{2}}{m C_{L S I}}),\\frac{1}{L})}\\end{array}$ . Then, $\\begin{array}{r}{R_{k}=\\frac{2\\sigma^{2}\\eta}{C_{L S I}}}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "Note that $R_{k}$ can be interpreted as the strict privacy improving rate at step $k$ and $C_{k}$ is the LSI constant upper bound of distribution of $y_{k}$ . The above theorem states that fine-tuning with PNGD can decrease the privacy loss $d_{\\alpha}(\\rho_{K},\\nu_{D^{\\prime}})$ exponentially fast with the unlearning iteration $K$ . This is because $R_{k}$ is lower bounded away from 0 by a constant, thanks to the iteration independent upper bound on $C_{k}$ . Stronger assumptions on the objective function $f_{\\mathcal{D}}$ lead to a better rate, which implies fewer unlearning iterations are needed to achieve the same RU guarantee. There are several remarks for our Theorem 3.2. First, note that the result is dimension-free, which is favorable for problems with many parameters to be learned. Second, note that the $M$ -Lipschitzness assumption can be dropped by clipping the gradient to norm $M$ in the PNGD update (1) and (2) instead. As a result, our Theorem 3.2 applies to neural networks with smooth activation functions in theory. Finally, our result gives an upper bound on the LSI constants along the unlearning process (i.e., $C_{k}$ ) which may be improved with more advanced analysis. We note that the exponential dependence in $R$ for the bound of $C_{k}$ can be loose. It is possible to have a better constant with either more structural assumptions or working with different isoperimetric inequalities such as (weak) Poicar\u00e9 inequality [31]. A more detailed discussion is in Appendix 5. ", "page_idx": 5}, {"type": "text", "text": "Initial RDP guarantees and LSI constant. Since Theorem 3.2 relies on $\\mathcal{M}$ being $\\left(\\alpha,\\varepsilon_{0}\\right)$ -RDP and the $\\nu_{\\mathscr D}$ satisfies LSI, the theorem below provides such results for the learning process, where the formal proof is relegated to Appendix $\\mathrm{H}$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.3 (RDP guarantee of PNGD learning). Assume $f(\\cdot;\\mathbf{d})$ be $L$ -smooth and $M$ -Lipschitz for all $\\mathbf{d}\\in\\mathcal{X}$ . Also assume that the initialization of PNGD (1) satisfies $C_{0}$ -LSI. Then the learning process (1) is $(\\alpha,\\varepsilon_{0}^{(S)})$ -RDP of group size $S\\geq1$ at $T^{t h}$ iteration with ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\varepsilon_{0}^{(S)}\\leq\\frac{2\\alpha\\eta S^{2}M^{2}}{\\sigma^{2}n^{2}}\\sum_{t=1}^{T}\\prod_{t^{\\prime}=0}^{t-1}(1+\\frac{\\eta\\sigma^{2}}{C_{t^{\\prime},1}})^{-1},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\begin{array}{r}{C_{t,1}\\,\\le\\,\\operatorname*{min}\\big((1+\\eta L)^{2}C_{t}+\\eta\\sigma^{2},\\bar{C}\\big),\\,\\bar{C}\\,=\\,6(4(R+\\eta M)^{2}+\\eta\\sigma^{2})\\exp(\\frac{4(R+\\eta M)^{2}}{\\eta\\sigma^{2}})}\\end{array}$ and $C_{t+1}\\leq\\operatorname*{min}\\left(C_{t,1}+\\eta\\sigma^{2},\\bar{C}\\right)$ . Furthermore, $\\nu_{t}$ satisfies $C_{t}$ -LSI. ", "page_idx": 5}, {"type": "text", "text": "When we additionally assume $f(\\cdot;\\mathbf{d})$ is convex, by choosing $\\begin{array}{r}{\\eta\\,\\le\\,\\frac{2}{L}}\\end{array}$ the same result hold with $C_{t,1}\\leq\\operatorname*{min}\\left(C_{t}+\\eta\\sigma^{2},\\bar{C}\\right)$ . ", "page_idx": 5}, {"type": "text", "text": "When we additionally assume $f(\\cdot;\\mathbf{d})$ is $m$ -strongly convex, by choosing $\\begin{array}{r}{0\\,<\\,\\eta\\,\\leq\\,\\operatorname*{min}(\\frac{2}{m}(1\\,-}\\end{array}$ $\\begin{array}{r}{\\frac{\\sigma^{2}}{m C_{0}}),\\frac{1}{L})}\\end{array}$ with a constant $\\begin{array}{r}{C_{0}>\\frac{\\sigma^{2}}{m}}\\end{array}$ , we have $\\begin{array}{r}{\\varepsilon_{0}^{(S)}\\leq\\frac{4\\alpha S^{2}M^{2}}{m\\sigma^{2}n^{2}}(1-\\exp(-m\\eta T))}\\end{array}$ . Furthermore, $\\nu_{t}$ satisfies $C_{0}$ -LSI for all $t\\geq0$ . ", "page_idx": 6}, {"type": "text", "text": "Note that any initialization $\\scriptstyle x_{0}\\;\\in\\;{\\mathcal{C}}_{R}$ can be viewed as sampling from $\\mathcal{N}(x_{0},c I_{d})$ with $c\\to0$ , which corresponds to $C_{0}$ -LSI for any $C_{0}>0$ . By taking $T\\rightarrow\\infty$ , Theorem 3.3 provides the initial $\\left(\\alpha,\\varepsilon_{0}\\right)$ - RDP guarantee and the LSI constant needed in Theorem 3.2. Since there is an iteration-independent upper bound for $C_{t,1}$ , one can show that $\\begin{array}{r}{\\varepsilon_{0}\\leq\\frac{2\\alpha\\eta S^{2}M^{2}}{\\sigma^{2}n^{2}c}}\\end{array}$ for some $T$ -independent constant $c\\in(0,1)$ due to the finiteness of geometric series. Similar to our discussion for Theorem 3.2, the bound of $C_{t}$ may be loose and it is possible to further improve the LSI constant analysis. The goal of our results is to demonstrate that it is possible to derive (finite) RDP and (arbitrarily small) RU guarantees even for general non-convex problems. ", "page_idx": 6}, {"type": "text", "text": "Nevertheless, for the $m$ -strongly convex case we have $\\begin{array}{r}{\\varepsilon_{0}^{(S)}\\,\\leq\\,\\frac{4\\alpha S^{2}M^{2}}{m\\sigma^{2}n^{2}}}\\end{array}$ for all $T\\,>\\,0$ , including $T\\to\\infty$ . It shows that indeed the current learned distribution $\\nu_{D}$ is close to the retraining distribution $\\nu_{\\mathcal{D}^{\\prime}}$ for $n$ sufficiently large. This also leads to the computational benefit of Langevin unlearning compared to retraining from scratch, which we discuss below. On the other hand, we show by experiments in Section 4 that our results provide a superior privacy-utility-complexity trade-off for the strongly convex case compared to existing approximate unlearning approaches. ", "page_idx": 6}, {"type": "text", "text": "The computational benefit compared to retraining. While our Theorems 3.2 and 3.3 together provide the privacy guarantee of Langevin unlearning, it is critical to check if our approach provides a computational benefti compared to retraining from scratch as well. Let $\\nu_{0}$ be the (data-independent) initialization distribution of the learning process. Intuitively, starting with $\\nu_{D}$ instead of $\\nu_{0}$ (i.e., retraining) should converge faster to $\\nu_{\\mathcal{D}^{\\prime}}$ , since $d_{\\alpha}(\\nu_{\\mathscr D},\\nu_{\\mathscr D^{\\prime}})\\leq\\varepsilon_{0}$ is likely to be much smaller than $d_{\\alpha}(\\nu_{0},\\nu_{\\bar{D^{\\prime}}})$ . Thus, our Langevin unlearning needs less iterations than retraining for most cases, except for a corner case when $\\nu_{0}$ is already close to $\\nu_{D}$ . From Theorem 3.2 we know that the number of PNGD iterations we need to approach $\\varepsilon$ -close in $d_{\\alpha}$ to the target distribution $\\nu_{\\mathcal{D}^{\\prime}}$ is roughly $\\begin{array}{r}{O(\\log(\\frac{\\varepsilon_{I}}{\\varepsilon}))}\\end{array}$ , where $\\varepsilon_{I}$ is the R\u00e9nyi difference between the initial distribution and the target distribution $\\nu_{\\mathcal{D}^{\\prime}}$ . From Theorem 3.3, we know that the initial R\u00e9nyi difference of Langevin unlearning is at most $\\varepsilon_{0}\\,=\\,O(1/n^{2})$ for any datasets $\\mathcal{D},\\mathcal{D}^{\\prime}$ and any smooth Lipchitz loss. In contrast, even if both the target distribution $\\nu_{\\mathcal{D}^{\\prime}}$ and the initialization of retraining $\\nu_{0}$ are Gaussian distributions with the same variance but mean difference $\\Omega(1)$ , their R\u00e9nyi difference is $\\Omega(1)$ [22]. As a result, computational saving offered by Langevin unlearning is significant for sufficiently large $n$ . A more thorough discussion is in Appendix E. ", "page_idx": 6}, {"type": "text", "text": "3.3 Empirical Aspects of Langevin Unlearning ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Insufficient training. While our theorem assumes the learning process runs until convergence, this assumption can be relaxed by the geometric view of Langevin unlearning. Assume the learning process $\\mathbf{\\mathcal{M}}(\\mathbf{\\mathcal{D}})\\,\\sim\\,\\nu_{T}$ terminate with finite step $T$ instead and we only have ${d_{\\alpha}(\\nu_{T},\\nu_{\\mathcal{D}})\\leq\\varepsilon_{T}(\\alpha)}$ for all possible $\\mathcal{D}\\in\\mathcal{X}^{n}$ . One can still apply the weak triangle inequality of R\u00e9nyi divergence [22] twice to bound $d_{\\alpha}\\bar{(\\rho_{k},\\nu_{T}^{\\prime})}$ with $d_{4\\alpha}(\\rho_{k},\\nu_{D^{\\prime}})$ , $\\varepsilon_{T}(2\\alpha)$ , and $\\varepsilon_{T}(4\\alpha)$ with additional factors $(\\alpha\\!-\\!0.5)/(\\alpha\\!-\\!1)$ and $(2\\alpha\\!-\\!0.5)/(2\\alpha\\!-\\!1)$ . In practice, it is reasonable to require the model parameters to be sufficiently trained so that $\\varepsilon_{T}$ is negligible and a tighter weak triangle inequality can be employed. ", "page_idx": 6}, {"type": "image", "img_path": "3LKuC8rbyV/tmp/e30b009243f819e19b225e6b94adb03ecbcea775d5cfefc2cc1d7f57d46afd2c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 2: Illustration of (a) sequential unlearning and (b) batch unlearning. For sequential unlearning, we can leverage the weak triangle inequality of R\u00e9nyi divergence to connect all the error terms. For batch unlearning, only the initial RDP guarantee changes with a general group size. Notably, unlearning more samples at once implies $\\varepsilon_{0}$ being larger (Theorem 3.3), and thus we need more unlearning iteration to recuperate the privacy loss to a desired $\\varepsilon$ . ", "page_idx": 6}, {"type": "text", "text": "Sequential and batch unlearning. Langevin unlearning naturally supports sequential and batch unlearning for unlearning multiple data points thanks to our geometric view of the unlearning problem, see Figure 2 for a pictorial example. For sequential unlearning, we show that fine-tuning the current model parameters on the updated datasets for sequential $S\\geq1$ unlearning requests can achieve $(\\alpha,\\varepsilon)$ -RU simultaneously. The formal proof is deferred to Appendix I. ", "page_idx": 6}, {"type": "image", "img_path": "3LKuC8rbyV/tmp/ea36e54fdb54842093c40cbcbeae47c1f6022c4ec551c48f265e45447c8ce211.jpg", "img_caption": ["Figure 3: Main experiments, where the top and bottom rows are for MNIST and CIFAR10 respectively. (a) Compare to D2D for unlearning one point using limited unlearning iteration. This demonstrates the privacy-utility $\\epsilon$ -accuracy) tradeoff under the fixed unlearning complexity (K). For Langevin unlearning, we use only $K=1$ unlearning iterations. For D2D, we allow it not only to use $K=1,2,5$ unlearning iterations but also to keep the non-private internal state information. (b) Compare to D2D for unlearning 100 points, where all methods achieve $(\\epsilon,1/n)$ -unlearning guarantee with $\\epsilon=1$ . For Langevin unlearning, we vary different unlearning batch sizes $S$ and combine them with the sequential unlearning result. For D2D, we do not allow it to keep the non-private internal state information in this experiment so that there is an inherent lower bound on the unlearning iterations per unlearning request. (c) A detailed investigation of the utility-complexity trade-off of Langevin unlearning with unlearning $S=100$ points at once under the fixed privacy constraint $\\epsilon=1$ . For each $\\sigma$ , we report the corresponding $\\epsilon_{0}$ (black dash line) for the initial $\\left(\\epsilon_{0},1/n\\right)$ -DP guarantee and the utility after unlearning to $\\epsilon=1$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Corollary 3.4 (Sequential unlearning). Assume the unlearning requests arrive sequentially such that our dataset changes from $D_{0}\\to D_{1}\\to...\\to D_{S}$ , where $\\mathcal{D}_{s},\\mathcal{D}_{s+1}$ are adjacent. Let $y_{k}^{(s)}$ be the unlearned parameters for the $s^{t h}$ unlearning request with $k$ unlearning update following (2) on Ds and y(0s $\\bar{y_{0}^{(s+1)}}\\,=\\,\\bar{y_{K_{s}}^{(s)}}\\,\\sim\\,\\bar{\\nu}_{\\ensuremath{\\mathcal{D}_{s}}}$ , where $y_{0}^{(1)}\\,=\\,x_{\\infty}$ and $K_{s}$ is the unlearning steps for the $s^{t h}$ unlearning request. Suppose we have achieved $(\\alpha,\\varepsilon^{(s)}(\\alpha))$ -RU for the $s^{t h}$ unlearning request, the learning process (1) is $\\left(\\alpha,\\varepsilon_{0}(\\alpha)\\right)$ -RDP and $\\bar{\\nu}_{\\mathcal{D}_{s}}$ satisfies $C_{L S I}$ -LSI, we achieve $(\\alpha,\\varepsilon^{(s+1)}(\\alpha))$ -RU for the $(s+1)^{t h}$ unlearning request as well, where ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\varepsilon^{(s+1)}(\\alpha)\\leq\\exp(-\\frac{1}{\\alpha}\\sum_{k=0}^{K_{s+1}-1}R_{k})\\times\\frac{\\alpha-1/2}{\\alpha-1}\\left(\\varepsilon_{0}(2\\alpha)+\\varepsilon^{(s)}(2\\alpha)\\right),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "$\\varepsilon^{(0)}(\\alpha)=0\\,\\forall\\alpha>1$ and $R_{k}$ are defined in Theorem 3.2. ", "page_idx": 7}, {"type": "text", "text": "As a result, one can leverage Corollary 3.4 to recursively determine needed unlearning iterations for each sequential unlearning request. For the batch unlearning setting, it only affects the initial R\u00e9nyi difference in Theorem 3.2. We can simply adopt Theorem 3.3 with a group size of $S\\geq1$ for the RDP guarantees of the learning process $\\bar{\\varepsilon}_{0}^{(\\bar{S})}$ . ", "page_idx": 7}, {"type": "text", "text": "Utility-privacy-efficiency trade-off. An interesting aspect of the Langevin unlearning is its strong connection to the initial RDP guarantee. From Theorem 3.3, we know that increasing $\\sigma$ leads to smaller R\u00e9nyi difference $\\varepsilon_{0}$ and thus better unlearning efficiency. However, this intuitively is at the cost of the utility of $\\nu_{D}$ , see for example the discussion in Section 5 of [12] under the strong convexity assumption. To achieve the same $(\\alpha,\\varepsilon)$ -RU guarantee, one can either ensure smaller $\\varepsilon_{0}$ at the cost of worst utility or run more unlearning iterations at the cost of unlearning efficiency. We investigate how utility trade-off with privacy and unlearning complexity empirically in Section 4. ", "page_idx": 7}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Benchmark datasets. We consider logistic regression with $\\ell_{2}$ regularization. We focus on this strongly convex setting since the non-convex unlearning bound in Theorem 3.2 currently is not tight enough to be applied in practice due to its exponential dependence on various hyperparameters. However, we emphasize its significant theoretical implication due to the lack of a certified non-convex approximate unlearning framework in previous studies. Meanwhile, the existing baseline approach [8] also only applies to strongly convex problems. We conduct experiments on MNIST [34] and CIFAR10 [35], which contain 11,982 and 50,000 training instances respectively. We follow the setting of [7] to distinguish digits 3 and 8 for MNIST so that the problem is a binary classification. For the CIFAR10 dataset, we use all classes and leverage the last layer of the public ResNet18 [36] embedding as the data features, which follows the public feature extractor setting of [7]. Experiments on additional datasets [37] are deferred to Appendix M and our code is publicly available3. ", "page_idx": 8}, {"type": "text", "text": "Baseline methods. Our baseline methods include Delete-to-Descent (D2D) [8], the state-of-the-art gradient-based approximate unlearning method, and retraining from scratch using PNGD. For D2D, we leverage Theorem 9 and 28 in [8] for privacy accounting depending on whether we allow D2D to have an internal non-private state. Note that allowing an internal non-private state provides a weaker notion of privacy guarantee [8] and our Langevin unlearning by default does not require it. We include those theorems for D2D and a detailed explanation of its possible non-privacy internal state in Appendix N for completeness. All experimental details can be found in Appendix M, including how to convert $(\\alpha,\\varepsilon)$ -RU to the standard $(\\epsilon,\\delta)$ -unlearning guarantee. Throughout this section, we choose $\\delta=1/n$ for each dataset and require all tested unlearning approaches to achieve $(\\epsilon,\\delta)$ -unlearning with different $\\epsilon$ . We report test accuracy for all experiments as the utility metric. For the initialization, we sample from Gaussian distribution with mean 1000. This simulates the case that the initial distribution is in a reasonable distance away from the convergent distribution $\\nu_{D}$ . We set the learning iteration $T\\,=\\,10,000$ to ensure all approaches converge. For Langevin unlearning, we leverage Theorems 3.2, 3.3 and Corollay 3.4 for privacy accounting under different settings. All results are averaged over 100 independent trials with standard deviation reported as shades in all figures. ", "page_idx": 8}, {"type": "text", "text": "Unlearning one data point with $K=1$ iteration. We first consider the setting of unlearning one data point using only $K=1$ unlearning iteration for both Langevin unlearning and D2D (Figure 3a). Since D2D cannot achieve a privacy guarantee with only 1 unlearning iteration without a non-private internal state, we allow D2D to have it in this experiment. Even in this case, our Langevin unlearning significantly outperforms D2D in utility for $\\epsilon$ from 0.1 to 5 under the same unlearning complexity $[K=1]$ ), but also achieves similar accuracy to retraining from scratch. Since retraining requires $T=10,000$ PNGD iterations, Langevin unlearning is indeed much more efficient. We also show that D2D can achieve better utility at the cost of a larger unlearning iteration $K=2,5$ . Our Langevin unlearning exhibits both smaller unlearning complexity and better utility compared to D2D. ", "page_idx": 8}, {"type": "text", "text": "Unlearning multiple data points. We now consider the scenario of unlearning 100 data points, where the results are in Figure 3b. We let all methods achieve the same $(1,1/n)$ - unlearning guarantee for a fair comparison. Since D2D only supports sequential unlearning, we directly apply its sequential unlearning results [8]. Also, we do not allow D2D to have an internal non-private state in this experiment for a fair comparison. On the other hand, since Langevin unlearning supports both sequential and batch unlearning, we vary the number of points per unlearning request $S=5,10,20$ and report the accumulated unlearning iterations for $\\sigma=0.03$ . All methods achieve a similar utility, with an accuracy of roughly 0.9 and 0.98 for MNIST and CIFAR10 respectively. Langevine unlearning can achieve a significantly better unlearning complexity compared to D2D if one allows for a larger unlearning batch size. For instance, when we are allowed to unlearn $S\\,=\\,20$ points at once, Langevine unlearning saves $40\\%$ unlearning iteration compared to D2D. Nevertheless, we note that due to the use of weak triangle inequality of R\u00e9nyi divergence in our analysis, Langevin unlearning can be more expensive in complexity compared to D2D when one only allows for unlearning a small batch of points (i.e, $S=5]$ ). We leave the improvement in this direction as the future work. ", "page_idx": 8}, {"type": "image", "img_path": "3LKuC8rbyV/tmp/249e668d5a7047e4a52c78ae0ba6492c46c003bfec14e71dd0884c8b4a264a44.jpg", "img_caption": ["Figure 4: Trade-off between privacy (\u03f5), unlearning complexity $(K)$ , and the number of points to be unlearned $(S)$ in the batch unlearning setting for MNIST. We fix $\\sigma=0.03$ so that $K$ can be determined given $(\\epsilon,S)$ . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Privacy-utility-complexity trade-off. We further examine the inherent privacy-utility-complexity trade-off provided by our Langevin unlearning with two experiments. In the first experiment, we aim to achieve $(\\epsilon,1/n)$ -unlearning guarantee with $\\epsilon=1$ for batch unlearning of 100 points. We vary the choice of $\\sigma$ from 0.01 to 1.0. A smaller $\\sigma$ leads to a worse initial $\\epsilon_{0}$ and thus requires more unlearning iteration $K$ to recuperate it to $\\epsilon=1$ . It is interesting to see that even if we choose a small $\\sigma$ so that the initial $(\\epsilon_{0},\\bar{1}/n)$ -DP guarantee is extremely weak (i.e, $\\epsilon_{0}\\approx100$ for $\\sigma=0.05)$ ), our unlearning iteration can recuperate $\\epsilon_{0}$ to $\\epsilon=1.0$ efficiently. On the other hand, a larger $\\sigma$ leads to a worse utility which is the inherent privacy-utility-complexity trade-off of Langevin unlearning. The results are illustrated in Figure 3c. Compared to retraining until convergence $(T=10,000)$ , we achieve a similar utility but with much lower unlearning complexity with $K$ roughly up to 2500. ", "page_idx": 9}, {"type": "text", "text": "In the second experiment, we investigate the effect of the number of points to be unlearned $S$ in the batch unlearning setting. In Figure 4, we can see that both larger $S$ and smaller $\\epsilon$ will require more unlearning iterations $K$ . It is worth noting that the resulting utility does not change significantly, whereas Langevin unlearning always archives a similar utility compared to retraining (see Figure 5a in Appendix M). Retraining requires $T=10$ , 000 PNGD iterations which is significantly larger than the required unlearning iteration $K$ even for $\\epsilon=0.5$ . We have shown that Langevin unlearning is a promising unlearning solution. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusions and Future Directions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We propose Langevin unlearning based on noisy gradient descent with privacy guarantees for approximate unlearning problems. It unifies the DP learning process and the privacy-certified unlearning process with many algorithmic beneftis such as applicability to non-convex problems and multiple unlearning requests. Below we discuss several future directions for Langevin unlearning. ", "page_idx": 9}, {"type": "text", "text": "Extension to projected noisy stochastic gradient descent. It is straightforward to extend our analysis to the projected noisy SGD case. There are two possibilities for the SGD setting: 1) randomly partition the indices $[n]$ into a sequence of mini-batches, then fix this sequence for all the learning and unlearning process [18]; 2) randomly draw a mini-batch for each update [19,25]. The analysis of [18] can be combined with our LSI constant analysis for RU guarantees, similar to the proof of our Theorem 3.2. Unfortunately, the analysis [25] may lead to an extra large LSI constant in the intermediate step even if $R$ is small. We refer interested readers to Appendix C of [18] for a detailed discussion. The technical difficulty here is to provide a tight analysis of the LSI constant for a mixture of distributions, where each of them corresponds to a possible choice of mini-batch. The analysis of [19] is based on privacy amplification by iteration, which does not directly generalize to the non-convex cases. In our companion work [38], we exploit it not only to establish a better unlearning result but also to enable the mini-batch setting under the convexity assumption. It is currently an open problem whether a matching result can be established via Langevin dynamic analysis as well. ", "page_idx": 9}, {"type": "text", "text": "Better convergence rate. While it is already exciting that Langevin dynamic analysis leads to formal unlearning algorithms and guarantees even for general non-convex problems in theory, the potential of this direction for a practical plug-and-play unlearning solution is even more interesting. Several promising future directions can significantly improve the convergence rate and the unlearning efficiency. Developing a better LSI constant bound under additional structural assumptions for the non-convex problems is the most straightforward one. Another direction is to work with (weak) Poincar\u00e9 inequality instead. While a weaker tail assumption leads to slower convergence [31], the corresponding (weak) Poincar\u00e9 constant may be more tightly tracked. Finally, while we only discuss the noisy GD which corresponds to Langevin Monte Carlo, some other advanced samplers are off-the-shelf including the Metropolis-Hastings filter [39] and Hamiltonian Monte Carlo [40]. We hope our work motivates further collaborations among the sampling and privacy communities and pushes the boundaries of learning and unlearning with privacy guarantees. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The authors thank Sinho Chewi, Wei-Ning Chen, and Ayush Sekhari for the helpful discussions. E.   \nChien, H. Wang and P. Li are supported by NSF awards OAC-2117997 and JPMC faculty award. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] A. Sekhari, J. Acharya, G. Kamath, and A. T. Suresh, \u201cRemember what you want to forget: Algorithms for machine unlearning,\u201d Advances in Neural Information Processing Systems, vol. 34, pp. 18075\u201318086, 2021.   \n[2] N. Carlini, C. Liu, \u00da. Erlingsson, J. Kos, and D. Song, \u201cThe secret sharer: Evaluating and testing unintended memorization in neural networks,\u201d in 28th USENIX Security Symposium (USENIX Security 19), pp. 267\u2013284, 2019.   \n[3] Y. Cao and J. Yang, \u201cTowards making systems forget with machine unlearning,\u201d in 2015 IEEE symposium on security and privacy, pp. 463\u2013480, IEEE, 2015.   \n[4] L. Bourtoule, V. Chandrasekaran, C. A. Choquette-Choo, H. Jia, A. Travers, B. Zhang, D. Lie, and N. Papernot, \u201cMachine unlearning,\u201d in 2021 IEEE Symposium on Security and Privacy $(S P)$ , pp. 141\u2013159, IEEE, 2021.   \n[5] E. Ullah, T. Mai, A. Rao, R. A. Rossi, and R. Arora, \u201cMachine unlearning via algorithmic stability,\u201d in Conference on Learning Theory, pp. 4126\u20134142, PMLR, 2021.   \n[6] E. Ullah and R. Arora, \u201cFrom adaptive query release to machine unlearning,\u201d in International Conference on Machine Learning, pp. 34642\u201334667, PMLR, 2023.   \n[7] C. Guo, T. Goldstein, A. Hannun, and L. Van Der Maaten, \u201cCertified data removal from machine learning models,\u201d in International Conference on Machine Learning, pp. 3832\u20133842, PMLR, 2020.   \n[8] S. Neel, A. Roth, and S. Sharifi-Malvajerdi, \u201cDescent-to-delete: Gradient-based methods for machine unlearning,\u201d in Algorithmic Learning Theory, pp. 931\u2013962, PMLR, 2021.   \n[9] V. Gupta, C. Jung, S. Neel, A. Roth, S. Sharifi-Malvajerdi, and C. Waites, \u201cAdaptive machine unlearning,\u201d Advances in Neural Information Processing Systems, vol. 34, pp. 16319\u201316330, 2021.   \n[10] E. Chien, C. Pan, and O. Milenkovic, \u201cEfficient model updates for approximate unlearning of graph-structured data,\u201d in The Eleventh International Conference on Learning Representations, 2022.   \n[11] C. Dwork, F. McSherry, K. Nissim, and A. Smith, \u201cCalibrating noise to sensitivity in private data analysis,\u201d in Theory of Cryptography: Third Theory of Cryptography Conference, TCC 2006, New York, NY, USA, March 4-7, 2006. Proceedings 3, pp. 265\u2013284, Springer, 2006.   \n[12] R. Chourasia, J. Ye, and R. Shokri, \u201cDifferential privacy dynamics of langevin diffusion and noisy gradient descent,\u201d Advances in Neural Information Processing Systems, vol. 34, pp. 14771\u2013 14781, 2021.   \n[13] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and L. Zhang, \u201cDeep learning with differential privacy,\u201d in Proceedings of the 2016 ACM SIGSAC conference on computer and communications security, pp. 308\u2013318, 2016.   \n[14] P. Kairouz, S. Oh, and P. Viswanath, \u201cThe composition theorem for differential privacy,\u201d in International conference on machine learning, pp. 1376\u20131385, PMLR, 2015.   \n[15] S. Vempala and A. Wibisono, \u201cRapid convergence of the unadjusted langevin algorithm: Isoperimetry suffices,\u201d Advances in neural information processing systems, vol. 32, 2019.   \n[16] Y.-A. Ma, Y. Chen, C. Jin, N. Flammarion, and M. I. Jordan, \u201cSampling can be faster than optimization,\u201d Proceedings of the National Academy of Sciences, vol. 116, no. 42, pp. 20881\u2013 20885, 2019.   \n[17] A. Lamperski, \u201cProjected stochastic gradient langevin algorithms for constrained sampling and non-convex learning,\u201d in Conference on Learning Theory, pp. 2891\u20132937, PMLR, 2021.   \n[18] J. Ye and R. Shokri, \u201cDifferentially private learning needs hidden state (or much faster convergence),\u201d Advances in Neural Information Processing Systems, vol. 35, pp. 703\u2013715, 2022.   \n[19] J. Altschuler and K. Talwar, \u201cPrivacy of noisy stochastic gradient descent: More iterations without more privacy loss,\u201d Advances in Neural Information Processing Systems, vol. 35, pp. 3788\u20133800, 2022.   \n[20] L. Gross, \u201cLogarithmic sobolev inequalities,\u201d American Journal of Mathematics, vol. 97, no. 4, pp. 1061\u20131083, 1975.   \n[21] H.-B. Chen, S. Chewi, and J. Niles-Weed, \u201cDimension-free log-sobolev inequalities for mixture distributions,\u201d Journal of Functional Analysis, vol. 281, no. 11, p. 109236, 2021.   \n[22] I. Mironov, \u201cR\u00e9nyi differential privacy,\u201d in 2017 IEEE 30th computer security foundations symposium (CSF), pp. 263\u2013275, IEEE, 2017.   \n[23] R. Chourasia and N. Shah, \u201cForget unlearning: Towards true data-deletion in machine learning,\u201d in International Conference on Machine Learning, pp. 6028\u20136073, PMLR, 2023.   \n[24] A. Ganesh and K. Talwar, \u201cFaster differentially private samplers via r\u00e9nyi divergence analysis of discretized langevin mcmc,\u201d Advances in Neural Information Processing Systems, vol. 33, pp. 7222\u20137233, 2020.   \n[25] T. Ryffel, F. Bach, and D. Pointcheval, \u201cDifferential privacy guarantees for stochastic gradient langevin dynamics,\u201d arXiv preprint arXiv:2201.11980, 2022.   \n[26] Chewi, Sinho, \u201cLog-Concave Sampling.\u201d https://chewisinho.github.io/main.pdf, 2023. Online; accessed September 29, 2023.   \n[27] V. Feldman, I. Mironov, K. Talwar, and A. Thakurta, \u201cPrivacy amplification by iteration,\u201d in 2018 IEEE 59th Annual Symposium on Foundations of Computer Science (FOCS), pp. 521\u2013532, IEEE, 2018.   \n[28] A. S. Dalalyan and A. B. Tsybakov, \u201cSparse regression learning by aggregation and langevin monte-carlo,\u201d Journal of Computer and System Sciences, vol. 78, no. 5, pp. 1423\u20131443, 2012.   \n[29] A. Durmus and E. Moulines, \u201cNonasymptotic convergence analysis for the unadjusted langevin algorithm,\u201d 2017.   \n[30] M. A. Erdogdu, R. Hosseinzadeh, and S. Zhang, \u201cConvergence of langevin monte carlo in chi-squared and r\u00e9nyi divergence,\u201d in International Conference on Artificial Intelligence and Statistics, pp. 8151\u20138175, PMLR, 2022.   \n[31] A. Mousavi-Hosseini, T. Farghly, Y. He, K. Balasubramanian, and M. A. Erdogdu, \u201cTowards a complete analysis of langevin monte carlo: Beyond poincar\\\u2019e inequality,\u201d arXiv preprint arXiv:2303.03589, 2023.   \n[32] J. M. Altschuler and K. Talwar, \u201cResolving the mixing time of the langevin algorithm to its stationary distribution for log-concave sampling,\u201d arXiv preprint arXiv:2210.08448, 2022.   \n[33] S. P. Meyn and R. L. Tweedie, Markov chains and stochastic stability. Springer Science & Business Media, 2012.   \n[34] L. Deng, \u201cThe mnist database of handwritten digit images for machine learning research,\u201d IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 141\u2013142, 2012.   \n[35] A. Krizhevsky et al., \u201cLearning multiple layers of features from tiny images,\u201d 2009.   \n[36] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for image recognition,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016.   \n[37] B. Becker and R. Kohavi, \u201cAdult.\u201d UCI Machine Learning Repository, 1996. DOI: https://doi.org/10.24432/C5XW20.   \n[38] E. Chien, H. Wang, Z. Chen, and P. Li, \u201cStochastic gradient langevin unlearning,\u201d Advances in Neural Information Processing Systems, 2024.   \n[39] W. K. Hastings, \u201cMonte carlo sampling methods using markov chains and their applications,\u201d 1970.   \n[40] R. M. Neal et al., \u201cMcmc using hamiltonian dynamics,\u201d Handbook of markov chain monte carlo, vol. 2, no. 11, p. 2, 2011.   \n[41] M. Aerni, J. Zhang, and F. Tram\u00e8r, \u201cEvaluations of machine learning privacy defenses are misleading,\u201d arXiv preprint arXiv:2404.17399, 2024.   \n[42] Q. P. Nguyen, B. K. H. Low, and P. Jaillet, \u201cVariational bayesian unlearning,\u201d Advances in Neural Information Processing Systems, vol. 33, pp. 16025\u201316036, 2020.   \n[43] Q. P. Nguyen, R. Oikawa, D. M. Divakaran, M. C. Chan, and B. K. H. Low, \u201cMarkov chain monte carlo-based machine unlearning: Unlearning what needs to be forgotten,\u201d in Proceedings of the 2022 ACM on Asia Conference on Computer and Communications Security, pp. 351\u2013363, 2022.   \n[44] A. Rawat, J. Requeima, W. Bruinsma, and R. Turner, \u201cChallenges and pitfalls of bayesian unlearning,\u201d arXiv preprint arXiv:2207.03227, 2022.   \n[45] S. Fu, F. He, Y. Xu, and D. Tao, \u201cBayesian inference forgetting,\u201d arXiv preprint arXiv:2101.06417, 2021.   \n[46] T. H. Gronwall, \u201cNote on the derivatives with respect to a parameter of the solutions of a system of differential equations,\u201d Annals of Mathematics, pp. 292\u2013296, 1919.   \n[47] M. Hardt, B. Recht, and Y. Singer, \u201cTrain faster, generalize better: Stability of stochastic gradient descent,\u201d in International conference on machine learning, pp. 1225\u20131234, PMLR, 2016.   \n[48] T. maintainers and contributors, \u201cTorchvision: Pytorch\u2019s computer vision library.\u201d https: //github.com/pytorch/vision, 2016.   \n[49] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala, \u201cPytorch: An imperative style, high-performance deep learning library,\u201d in Advances in Neural Information Processing Systems 32, pp. 8024\u20138035, Curran Associates, Inc., 2019.   \n[50] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer, \u201cAutomatic differentiation in pytorch,\u201d 2017.   \n[51] C. R. Harris, K. J. Millman, S. J. van der Walt, R. Gommers, P. Virtanen, D. Cournapeau, E. Wieser, J. Taylor, S. Berg, N. J. Smith, R. Kern, M. Picus, S. Hoyer, M. H. van Kerkwijk, M. Brett, A. Haldane, J. F. del R\u00edo, M. Wiebe, P. Peterson, P. G\u00e9rard-Marchant, K. Sheppard, T. Reddy, W. Weckesser, H. Abbasi, C. Gohlke, and T. E. Oliphant, \u201cArray programming with NumPy,\u201d 2020.   \n[52] P. Kairouz, B. McMahan, S. Song, O. Thakkar, A. Thakurta, and Z. Xu, \u201cPractical and private (deep) learning without sampling or shuffling,\u201d in International Conference on Machine Learning, pp. 5213\u20135225, PMLR, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Limitations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "While our Langevin unlearning provides the first approximate unlearning solution for non-convex problems, it is still not practical enough as we have stated in the main text. One limitation of our unlearning guarantees (Theorem 3.2) is that the privacy bound for non-convex problems is not tight and can potentially improved via advanced analysis, where some possible directions are discussed in the future directions below. Currently, it is still only applicable in practice for strongly convex problems as we have demonstrated in our experiment section if a privacy guarantee is required. Nevertheless, recent studies have shown that a $(\\epsilon,\\delta)$ -DP model provides strong empirical privacy against popular attacks such as membership inference attacks even if $\\epsilon\\approx10^{8}$ [41]. We conjecture a similar phenomenon exists for Langevin unlearning, which allows Langevin unlearning to defend against membership inference attacks for non-convex models as well in practice with a similar $\\epsilon$ scale. We leave this empirical study as our important future work. ", "page_idx": 13}, {"type": "text", "text": "B Broader Impact ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Our work study the theoretical unlearning guarantees of projected noisy gradient descent algorithm for convex problems. We believe our work is a foundational research and does not have a direct path to any negative applications. ", "page_idx": 13}, {"type": "text", "text": "C Standard Definitions ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Let $f:\\mathbb{R}^{d}\\mapsto\\mathbb{R}$ be a mapping. We define smoothness, Lipschitzsness, and strong convexity as follows: ", "page_idx": 13}, {"type": "equation", "text": "$$\nL{\\mathrm{-smooth}}\\colon\\forall\\,x,y\\in\\mathbb{R}^{d},\\ \\|\\nabla f(x)-\\nabla f(y)\\|\\leq L\\|x-y\\|\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$m{\\mathrm{-strongly~convex}}\\colon\\forall\\,x,y\\in\\mathbb{R}^{d},\\ \\langle x-y,\\nabla f(x)-\\nabla f(y)\\rangle\\geq m\\|x-y\\|^{2}$ ", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\nM{\\mathrm{-Lipschitzs:}}\\;\\forall\\;x,y\\in\\mathbb{R}^{d},\\;\\|f(x)-f(y)\\|\\leq M\\|x-y\\|.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Furthermore, we say $f$ is convex means it is 0-strongly convex. ", "page_idx": 13}, {"type": "text", "text": "D Additional Related Works ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Bayesian Unlearning. Due to the relation between Langevin Monte Carlo and Bayes learning approaches, our Langevin unlearning is also loosely related to the Bayesian unlearning literature. See [42\u201344] for a series of empirical results. Along this line of work, [45] is the only one that provides a certain unlearning guarantee in terms of KL divergence. However, they only provide a bound for one direction of KL (similar to $D_{1}\\big(\\rho_{k}||\\nu_{\\mathcal{D}}^{\\prime}\\big))$ which makes it fail to be directly connected to the differential privacy. Note that it is crucial to ensure the bidirectional bound for KL or R\u00e9nyi divergence for the purpose of privacy. Otherwise, we cannot ensure the sufficiently large type I and type II errors of the best possible attacker in membership inference attack [14]. Also, it is essential to have a (relatively) tight conversion to DP, where the general $\\alpha$ order in R\u00e9nyi divergence is crucial. ", "page_idx": 13}, {"type": "text", "text": "E Detailed Discussion on Computational Benefit Against Retraining ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we provide a more detailed discussion of the computational benefit of Langevin unlearning against retraining from scratch. We start our discussion under strongly convex assumption and then explain the non-convex case. Let us consider the case $f(x;\\mathbf{d})$ is $m$ -strongly convex, $L$ - smooth and $M$ -Lipschitz in $x$ for all $\\mathbf{d}\\in\\mathcal{X}$ . Also, assume the initialization distribution $\\nu_{0}~=$ $\\textstyle{N(\\tilde{x}_{0},\\frac{2\\sigma^{2}}{m}I_{d})}$ i foonr  (s1o), mwe $\\tilde{x}_{0}\\in\\mathcal{C}_{R}$ . In this case, from Theorem 3.2 we know that running $T$ PNGD ", "page_idx": 13}, {"type": "equation", "text": "$$\nd_{\\alpha}(\\nu_{T},\\nu_{\\mathcal{D^{\\prime}}})\\leq\\exp(-\\frac{2\\sigma^{2}\\eta T}{\\alpha C_{\\mathrm{LSI}}})d_{\\alpha}(\\nu_{0},\\nu_{\\mathcal{D^{\\prime}}}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Note that by Theorem 3.3, we know that $\\begin{array}{r}{C_{\\mathrm{LSI}}=\\frac{2\\sigma^{2}}{m}}\\end{array}$ by our choice of $\\nu_{0}$ for an appropriate step size $\\begin{array}{r}{\\eta\\leq\\operatorname*{min}(\\frac{1}{m},\\frac{1}{L})}\\end{array}$ . As a result, in order to be $\\varepsilon$ close to $\\nu_{\\mathcal{D}^{\\prime}}$ , we need $\\begin{array}{r}{\\frac{\\alpha}{m\\eta}\\log\\bigl(\\frac{d_{\\alpha}\\left(\\nu_{0},\\nu_{\\mathcal{D}^{\\prime}}\\right)}{\\varepsilon}\\bigr)}\\end{array}$ retraining ", "page_idx": 13}, {"type": "text", "text": "iteration. On the other hand, for Langevin unlearning we need $\\begin{array}{r}{\\frac{\\alpha}{m\\eta}\\log\\bigl(\\frac{\\varepsilon_{0}}{\\varepsilon}\\bigr)}\\end{array}$ , where $\\begin{array}{r}{\\varepsilon_{0}\\leq\\frac{4\\alpha M^{2}}{m\\sigma^{2}n^{2}}}\\end{array}$ m\u03c32n2 . As a result, Langevin unlearning the computational saving for Langevin unlearning against retraining is ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{\\alpha}{n\\eta}\\log(\\frac{d_{\\alpha}(\\nu_{0},\\nu_{{D^{\\prime}}})}{\\varepsilon})-\\frac{\\alpha}{m\\eta}\\log(\\frac{\\varepsilon_{0}}{\\varepsilon})=\\frac{\\alpha}{m\\eta}\\log(\\frac{d_{\\alpha}(\\nu_{0},\\nu_{{D^{\\prime}}})}{\\varepsilon_{0}})\\geq\\frac{\\alpha}{m\\eta}\\log(\\frac{m\\sigma^{2}n^{2}\\times d_{\\alpha}(\\nu_{0},\\nu_{{D^{\\prime}}})}{4\\alpha M^{2}}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Clearly, this saving depends on $\\nu_{\\mathcal{D}^{\\prime}}$ . In some rare cases, $\\nu_{0}$ might accidentally be close to $\\nu_{\\mathcal{D}^{\\prime}}$ so that retraining is more efficient. However, even if $\\begin{array}{r}{\\nu_{\\mathscr D^{\\prime}}\\;=\\;\\bar{\\mathcal N}({x}^{\\star}({\\mathcal D^{\\prime}}),\\frac{2\\sigma^{2}}{m}I_{d})}\\end{array}$ , we have $\\begin{array}{r}{d_{\\alpha}(\\nu_{0},\\nu_{\\mathcal{D}^{\\prime}})\\,=\\,\\frac{\\alpha m\\|\\tilde{x}_{0}-x^{\\star}(\\mathcal{D}^{\\prime})\\|}{4\\sigma^{2}}}\\end{array}$ . That is, even if we know the target distribution is Gaussian and choose the initialization to have the same variance, the corresponding R\u00e9nyi difference is $\\Omega(1)$ for $\\|\\tilde{x}_{0}\\,-\\,x^{\\star}(D^{\\prime})\\|\\,=\\,\\Omega(1)$ . As a result, if we uniformly at random sample $\\scriptstyle{\\tilde{x}}_{0}$ from $\\displaystyle\\mathcal{C}_{R}$ , we have $\\|\\tilde{x}_{0}-x^{\\star}(D^{\\prime})\\|\\geq1$ with probability at least $\\textstyle1-{\\frac{1}{R^{d}}}$ R1d . Plug this into the lower bound above we obtain a data-independent lower bound on the computational savings with probability at least $\\textstyle1-{\\frac{1}{R^{d}}}$ as follows ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\alpha}{m\\eta}\\log(\\frac{d_{\\alpha}(\\nu_{0},\\nu_{{\\mathcal D}^{\\prime}})}{\\varepsilon})-\\frac{\\alpha}{m\\eta}\\log(\\frac{\\varepsilon_{0}}{\\varepsilon})\\geq\\frac{\\alpha}{m\\eta}\\log(\\frac{m\\sigma^{2}n^{2}\\times d_{\\alpha}(\\nu_{0},\\nu_{{\\mathcal D}^{\\prime}})}{4\\alpha M^{2}})}\\\\ &{=\\frac{\\alpha}{m\\eta}\\log(\\frac{m^{2}n^{2}\\|\\tilde{x}_{0}-x^{\\star}({\\mathcal D}^{\\prime})\\|}{16M^{2}})\\geq\\frac{\\alpha}{m\\eta}\\log(\\frac{m^{2}n^{2}}{16M^{2}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Here we can see that for larger problem size $n$ , our computational benefit is more significant. ", "page_idx": 14}, {"type": "text", "text": "For the non-convex case, note that the convergence rate $R_{k}$ in Theorem 3.2 will vary and depend on the LSI constant of $\\nu_{0}$ and $\\nu_{D}$ in general. This makes it hard to have a direct characterization of the computational benefit against retraining. To simplify the situation, we assume the convergence rate $R_{k}$ is a constant $\\bar{R}>0$ that is independent of $n,k$ . In this case, the computational saving can be characterized as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{\\alpha}{\\bar{R}}\\log(\\frac{d_{\\alpha}(\\nu_{0},\\nu_{\\mathcal{D}^{\\prime}})}{\\varepsilon_{0}}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In this case, one can still leverage Theorem 3.3 to provide an upper bound on $\\varepsilon_{0}$ , yet the obtained bound can be weak due to the inaccurate estimate of LSI constants for the non-convex case. Instead, we propose to use unbiased limits $\\tilde{\\nu}_{\\mathcal{D}}$ to approximate the biased limit $\\nu_{D}$ for a rough estimate instead, since $\\bar{D}_{\\alpha}(\\nu_{\\mathscr D}||\\tilde{\\nu}_{\\mathscr D})\\,\\rightarrow\\,0$ as $\\eta\\,\\rightarrow\\,0$ [26]. From standard sampling literature [15], we know that $\\tilde{\\nu}_{\\mathcal{D}}\\propto\\exp(-\\frac{f_{\\mathcal{D}}}{\\sigma^{2}})$ . We provide the following result for bounding $d_{\\alpha}\\big(\\tilde{\\nu}_{D},\\tilde{\\nu}_{D^{\\prime}}\\big)$ . ", "page_idx": 14}, {"type": "text", "text": "Proposition E.1. Let $\\tilde{\\nu}_{D}\\,\\propto\\,\\exp(-f_{D})$ . Assume $|f(x;\\mathbf{d})-f(x;\\mathbf{d}^{\\prime})|\\,\\leq\\,F$ for all $x\\,\\in\\,\\mathbb{R}^{d}$ and $\\mathbf{d},\\mathbf{d}^{\\prime}\\in\\mathcal{X}$ . Then $\\begin{array}{r}{d_{\\alpha}\\big(\\tilde{\\nu}_{\\mathscr D},\\tilde{\\nu}_{\\mathscr D^{\\prime}}\\big)\\leq\\frac{2F}{n}}\\end{array}$ for any adjacent dataset $\\mathcal{D},\\mathcal{D}^{\\prime}$ and $\\alpha>1$ . ", "page_idx": 14}, {"type": "text", "text": "As a result, we know that $\\varepsilon_{0}$ is roughly at most $\\textstyle{\\frac{2F}{\\sigma^{2}n}}$ when the step size $\\eta$ is sufficiently small. Thus when $d_{\\alpha}(\\nu_{0},\\nu_{\\mathcal{D^{\\prime}}})=\\Omega(1)$ , we Langevin unlearning save $\\Omega(\\log(n))$ PNGD iterations. ", "page_idx": 14}, {"type": "text", "text": "F Proof of Theorem 3.1: Convergence of PNGD ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Theorem. Suppose that the closed convex set $\\mathcal{C}_{R}\\subset\\mathbb{R}^{d}$ is bounded with $L e b({\\mathcal{C}}_{R})>0$ where Leb denotes the Lebesgue measure and that $\\nabla f_{\\mathcal{D}}:\\mathcal{C}_{R}\\to\\mathbb{R}^{d}$ is continuous. The Markov chain $\\left\\{x_{t}\\right\\}$ in (1) admits a unique invariant probability measure $\\nu_{D}$ on $\\boldsymbol{{B}}(\\mathcal{C}_{R})$ that is the Borel $\\sigma$ -algebra of $\\displaystyle\\mathcal{C}_{R}$ . Furthermore, for any $x\\in{\\mathcal{C}}_{R},$ the distribution of $x_{t}$ conditioned on $x_{0}=x$ converges weakly to $\\nu_{D}$ as $t\\to\\infty$ . ", "page_idx": 14}, {"type": "text", "text": "In this section, we prove that the learning process (1) with general closed convex set $\\mathcal{C}$ that is restated as follows for the reader\u2019s convenience, ", "page_idx": 14}, {"type": "equation", "text": "$$\nx_{t+1}=\\Pi_{\\mathcal{C}}\\left(x_{t}-\\eta\\nabla f_{\\mathcal{D}}(x_{t})+\\sqrt{2\\eta\\sigma^{2}}W_{t}\\right),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "will converge to an invariant probability measure. One observation is that (11) is a Markov chain and some ergodicity results can be applied. ", "page_idx": 14}, {"type": "text", "text": "Proposition F.1. Suppose that the closed convex set $\\mathcal{C}\\subset\\mathbb{R}^{d}$ is bounded with $L e b(\\mathcal{C})>0$ where Leb denotes the Lebesgue measure and that $\\nabla f_{\\mathcal{D}}:\\mathcal{C}\\to\\mathbb{R}^{d}$ is continuous. Then the Markov chain $\\left\\{x_{t}\\right\\}$ defined by (11) admits a unique invariant measure (up to constant multiples) on $B(\\mathcal{C})$ that is the Borel $\\sigma$ -algebra of $\\mathcal{C}$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. This proposition is a direct application of results from [33]. According to Proposition 10.4.2 in [33], it suffices to verify that $\\left\\{x_{t}\\right\\}$ is recurrent and strongly aperiodic. ", "page_idx": 15}, {"type": "text", "text": ". Recurrency. Thanks to the Gaussian noise $W_{t}$ , $\\left\\{x_{t}\\right\\}$ is Leb-irreducible, i.e., it holds for any $x\\in{\\mathcal{C}}$ and any $A\\in B({\\mathcal{C}})$ with $\\operatorname{Leb}(A)>0$ that ", "page_idx": 15}, {"type": "equation", "text": "$$\nL(x,A):=\\mathbb{P}(\\tau_{A}<+\\infty\\mid x_{0}=x)>0,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\tau_{A}\\,=\\,\\operatorname*{inf}\\{t\\,\\geq\\,0:\\,x_{t}\\,\\in\\,A\\}$ is the stopping time. Therefore, there exists a Borel probability measure $\\psi$ such that that $\\left\\{x_{t}\\right\\}$ is $\\psi$ -irreducible and $\\psi$ is maximal in the sense of Proposition 4.2.2 in [33]. Consider any $A\\,\\in\\,B({\\mathcal{C}})$ with $\\psi(A)\\,>\\,0$ . Since $\\left\\{x_{t}\\right\\}$ is $\\psi$ -irreducible, one has $L(x,A)\\;=\\;\\mathbb{P}(\\tau_{A}\\;<\\;+\\infty\\;|\\;\\dot{x}_{0}\\;=\\;x)\\;>\\;0$ for all $x\\ \\in\\ {\\mathcal{C}}$ . This implies that there exists $T\\;\\geq\\;0,\\;\\delta\\;>\\;0$ , and $\\textit{B}\\in\\,\\mathcal{B}(\\mathcal{C})$ with $\\operatorname{Leb}(B)\\;>\\;0$ , such that $\\mathbb{P}({\\bar{x}}_{T}\\in A\\mid x_{0}=x)\\geq\\delta,\\,\\forall\\,x\\in B.$ . Therefore, one can conclude for any $x\\in{\\mathcal{C}}$ that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{U(x,A):=\\displaystyle\\sum_{t=0}^{\\infty}\\mathbb{P}(x_{t}\\in A\\mid x_{0}=x)}\\\\ {\\displaystyle\\geq\\sum_{t=1}^{\\infty}\\mathbb{P}(x_{t+T}\\in A\\mid x_{t}\\in B,x_{0}=x)\\cdot\\mathbb{P}(x_{t}\\in B\\mid x_{0}=x)}\\\\ {\\displaystyle\\geq\\sum_{t=1}^{\\infty}\\delta\\cdot\\operatorname*{inf}_{y\\in\\mathcal{C}}\\mathbb{P}(x_{t}\\in B\\mid x_{t-1}=y)}\\\\ {\\displaystyle=+\\infty,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where we used the fact that $\\operatorname*{inf}_{y\\in{\\mathcal{C}}}\\mathbb{P}(x_{t}\\in B\\mid x_{t-1}=y)=\\operatorname*{inf}_{y\\in{\\mathcal{C}}}\\mathbb{P}(x_{1}\\in B\\mid x_{t-1}=y)$ \u2208B | x0 = y) > 0 that is implies by $\\mathsf{L e b}(B)>0$ and the boundedness of $\\mathcal{C}$ and $\\nabla f_{\\mathcal{D}}(\\mathcal{C})$ . Let us remark that we actually have compact $\\nabla f_{\\mathcal{D}}(\\mathcal{C})$ since $\\mathcal{C}$ is compact and $\\nabla f_{\\mathcal{D}}$ is continuous. The arguments above verify that $\\left\\{x_{t}\\right\\}$ is recurrent (see Section 8.2.3 in [33] for definition). ", "page_idx": 15}, {"type": "text", "text": "2. Strong aperiodicity. Since $\\mathcal{C}$ and $\\nabla f_{\\mathcal{D}}(\\mathcal{C})$ are bounded and the density of $W_{t}$ has a uniform positive lower bound on any bounded domain, there exists a non-zero multiple of the Lebesgue measure, say $\\nu_{1}$ , satisfying that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}(x_{1}\\in A\\;|\\;x_{0}=x)\\geq\\nu_{1}(A),\\quad\\forall\\;x\\in\\mathcal{C},\\;A\\in\\mathcal{B}(\\mathcal{C}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then $\\left\\{x_{t}\\right\\}$ is strongly aperiodic by the equation above and $\\nu_{1}(\\mathcal{C})>0$ (see Section 5.4.3 in [33] for definition). ", "page_idx": 15}, {"type": "text", "text": "The proof is hence completed. ", "page_idx": 15}, {"type": "text", "text": "Theorem F.2. Under the same assumptions as in Proposition $F.l$ , the Markov chain $\\left\\{x_{t}\\right\\}$ admits $a$ unique invariant probability measure $\\nu_{D}$ on $B(\\mathcal{C})$ . Furthermore, for any $x\\in{\\mathcal{C}}$ , the distribution of $x_{t}$ generated by (11) conditioned on $x_{0}=x$ converges weakly to $\\nu_{D}$ as $t\\to\\infty$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. It has been proved in Proposition F.1 that $\\left\\{x_{t}\\right\\}$ is strongly aperiodic and recurrent with an invariant measure. Consider any $A\\in B({\\mathcal{C}})$ with $\\psi(A)>0$ and use the same settings and notations as in the proof of Proposition F.1. There exists $T\\geq0$ , $\\delta>0$ , and $B\\in B(\\mathcal{C})$ with $\\mathsf{L e b}(B)>0$ , such that $\\mathbb{P}(x_{T}^{-}\\in A\\mid x_{0}\\stackrel{-}{=}x)\\geq\\delta,\\;\\forall$ $\\forall\\,x\\in B$ . This implies that for any $t\\geq0$ and any $x\\in{\\mathcal{C}}$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}(x_{t+T+1}\\in A\\,|\\,x_{t}=x)=\\mathbb{P}(x_{T+1}\\in A\\,|\\,x_{0}=x)\\geq\\mathbb{P}(x_{T+1}\\in A\\,|\\,x_{1}\\in B,x_{0}=x).\\mathbb{P}(x_{1}\\in B\\,|\\,x_{0}=x)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\epsilon=\\delta\\cdot\\operatorname*{inf}_{y\\in{\\mathcal{C}}}\\mathbb{P}(x_{1}\\in B\\mid x_{0}=y)>0,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which then leads to ", "page_idx": 15}, {"type": "equation", "text": "$$\nQ(x,A):=\\mathbb{P}(x_{t}\\in A,\\;{\\mathrm{infinitely~often}})=+\\infty.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This verifies that the chain $\\left\\{x_{t}\\right\\}$ is Harris recurrent (see Section 9 in [33] for definition). It can be further derived that for any $x\\in\\mathcal{C}$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}(\\tau_{A}\\mid x_{0}=x)=\\sum_{t=1}^{\\infty}\\mathbb{P}(\\tau_{A}\\geq t\\mid x_{0}=x)\\leq(T+1)\\sum_{k=0}^{\\infty}\\mathbb{P}(\\tau_{A}>(T+1)k\\mid x_{0}=x)}\\\\ {\\leq\\displaystyle(T+1)\\sum_{k=1}^{\\infty}(1-\\epsilon)^{k}<+\\infty.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The bound above is uniform for all $x\\in\\mathcal{C}$ and this implies that $\\mathcal{C}$ is a regular set of $\\left\\{x_{t}\\right\\}$ (see Section 11 in [33] for definition). Finally, one can apply Theorem 13.0.1 in [33] to conclude that there exists a unique invariant probability measure $\\nu_{D}$ on $B(\\mathcal{C})$ and that the distribution of $x_{t}$ converges weakly to $\\nu_{D}$ conditioned on $x_{0}=x$ for any $x\\in\\mathcal{C}$ . \u53e3 ", "page_idx": 16}, {"type": "text", "text": "G Proof of Theorem 3.2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "G.1 Proof Sketch of Theorem 3.2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Given $\\mathcal{M}$ is $(\\alpha,\\varepsilon_{0})$ -RDP, we aim to show the upper bound of $d_{\\alpha}(\\rho_{k},\\nu_{D^{\\prime}})$ decays in $k$ starting from $\\varepsilon_{0}$ at $k=0$ . As a warm-up, we start with the strongly convex case. The analysis is inspired by [15] and [24] and formal proof can be found in Appendix G.2. Roughly speaking, we characterize how both $\\alpha$ R\u00e9nyi divergence $D_{\\alpha}(\\rho_{k}||\\nu_{\\mathcal{D}^{\\prime}})$ and $D_{\\alpha}(\\nu_{D^{\\prime}}||\\rho_{k})$ decay, given $\\nu_{\\mathcal{D}^{\\prime}}$ and $\\rho_{k}$ satisfy LSI condition for some constants. Standard sampling literature only focuses on the part $D_{\\alpha}(\\rho_{k}||\\nu_{\\mathcal{D}^{\\prime}})$ (i.e., Lemma 8 in [15]), where $\\nu_{\\mathcal{D}^{\\prime}}$ satisfies LSI implies exponential decay in R\u00e9nyi divergence. The other direction is necessary for meaningful privacy guarantee but more challenging as one to carefully track the LSI constant of $\\rho_{k}$ for all $k\\geq0$ . We prove the following lemma for such LSI constant characterization along the unlearning process, which specializes results of [26] to the PNGD update. ", "page_idx": 16}, {"type": "text", "text": "Lemma G.1 (LSI constant characterization). Consider the following PNGD update for a closed convex set $\\mathcal{C}$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\nx_{k,1}=h(x_{k}),\\;x_{k,2}=x_{k,1}+\\sigma W_{k},\\;x_{k+1}=\\Pi_{\\mathcal{C}}(x_{k,2}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $h$ is any $M$ -Lipschitz map $\\mathbb{R}^{d}\\mapsto\\mathbb{R}^{d}$ , $W_{k}\\sim\\mathcal{N}(0,I_{d})$ independent of anything before step $k$ , and $\\Pi_{\\mathcal{C}}$ is the projection onto a closed convex set $\\mathcal{C}$ . Let $\\mu_{k,1},\\mu_{k,2}$ and $\\mu_{k}$ be the distribution of $x_{k,1}$ , $x_{k,2}$ and $x_{k}$ respectively. Then we have the following LSI constant characterization of this process. $I)\\,I f\\mu_{k}$ satisfies $c$ -LSI, $\\mu_{k,1}$ satisfies $M^{2}c{-}L S I.~2)~I f\\mu_{k,1}$ satisfies $c$ -LSI, $\\mu_{k,2}$ satisfies $(c+\\sigma^{2})$ -LSI. $3)\\,I f\\mu_{k,2}$ satisfies $c$ -LSI, $\\mu_{k+1}$ satisfies $c$ -LSI. ", "page_idx": 16}, {"type": "text", "text": "By leveraging Lemma G.1, we can characterize the LSI constant for all $\\rho_{k}$ . One key step is to characterize the Lipschitz constant of the gradient update $h(x)=x-\\eta\\nabla f(x)$ . From Lemma 2.2 in [32] we know if $f$ is $m$ -strongly convex, $L$ -smooth and $\\begin{array}{r}{\\eta\\le\\frac{1}{L}}\\end{array}$ , then $h$ is $(1-\\eta m)$ -Lipschitz. Let $\\rho_{k}$ satisfy $C_{k}$ -LSI, Lemma G.1 leads to the recursion expression $\\bar{C}_{k+1}\\leq(1\\!-\\!\\eta m)^{2}C_{k}\\!+\\!2\\eta\\sigma^{2}$ , $C_{0}=$ $C_{\\mathrm{LSI}}$ . By choosing $\\eta$ satisfying $\\begin{array}{r}{0<\\eta\\le\\operatorname*{min}(\\frac{2}{m}\\big(1-\\frac{\\sigma^{2}}{m C_{\\mathrm{LSI}}}\\big),\\frac{1}{L})}\\end{array}$ \u2212m\u03c3C2LSI ), L1) and the assumption \u03c3m2 < CLSI, Ck is non-increasing and thus $\\rho_{k}$ is $C_{\\mathrm{LSI}}$ -LSI for all $k\\geq0$ . As a result, the decay of both $D_{\\alpha}(\\rho_{k}||\\nu_{\\mathcal{D}^{\\prime}})$ and $D_{\\alpha}(\\nu_{\\mathcal{D^{\\prime}}}||\\rho_{k})$ can be shown. ", "page_idx": 16}, {"type": "text", "text": "Beyond strong convexity. To extend beyond strong convexity, one may naively apply Lemma G.1 for convex and non-convex settings. Unfortunately, both cases lead to monotonically increasing LSI constant $C_{k}$ . As a result, given an $\\varepsilon_{0}$ , proving to achieve an arbitrarily small $\\varepsilon$ is challenging even with $K\\rightarrow\\infty$ since the LSI constant may be unbounded. More specifically, if $f$ is convex and $\\begin{array}{r}{\\bar{\\eta}\\le\\frac{2}{L}}\\end{array}$ , then $h(x)=x-\\eta\\nabla f(x)$ is 1-Lipschitz. If $f$ is $L$ -smooth only, the map $h$ is $(1\\!+\\!\\eta L)$ -Lipschitz. Applying Lemma G.1 leads to the recursions on $C_{k}$ . For the convex case, we have $\\dot{C}_{k+1}\\leq C_{k}+2\\eta\\bar{\\sigma}^{2}$ . For the non-convex case, we have $C_{k+1}\\leq(1+\\eta L)^{2}C_{k}+2\\eta\\sigma^{2}$ . ", "page_idx": 16}, {"type": "text", "text": "One of our contributions is to demonstrate that $C_{k}$ has a universal upper bound which is independent of the number of iterations. Hence, the exponential decay in R\u00e9nyi difference still holds. The key is to leverage the geometry of $\\displaystyle\\mathcal{C}_{R}$ to establish an LSI upper bound that is independent of $k$ using the result of [21], which has not been explored in the prior privacy literature [12,18,25]. ", "page_idx": 16}, {"type": "text", "text": "Lemma G.2 (Corollary 1 in [21]). Let $\\mu$ be a probability measure supported on $\\displaystyle\\mathcal{C}_{R}$ for some $R\\geq0$ .   \nThen, for each $\\xi\\geq0,$ , $\\mu*\\mathcal{N}(0,\\xi I_{d})$ satisfy $C$ -LSI with constant $\\begin{array}{r}{C\\le6(4R^{2}+\\xi)\\exp(\\frac{4R^{2}}\\xi)}\\end{array}$ . ", "page_idx": 16}, {"type": "text", "text": "[19] also leverage the geometry of $\\displaystyle\\mathcal{C}_{R}$ for the DP guarantee of learning with projected noisy (S)GD, but their analysis follows privacy amplification by iteration [27] and still require convexity. Our result demonstrates the potential of Langevin dynamic analysis for unlearning guarantees of non-convex problems. ", "page_idx": 17}, {"type": "text", "text": "G.2 Formal Proof ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We will start with the proof for the strongly convex case and then extend it for convex and non-convex cases. As indicated in our sketch of proof, there are two main parts of our proof. The first is to characterize the decay in R\u00e9nyi divergence between two processes $y_{k},y_{k}^{\\prime}$ under LSI conditions. The second is to track the LSI constant of $y_{k},y_{k}^{\\prime}$ throughout the unlearning process. The analysis is a modification of the proof of Lemma 8 in [15]. ", "page_idx": 17}, {"type": "text", "text": "We first define some useful quantities and list all technical lemmas that we need to proof. For $\\alpha>0$ and any two probability distribution $\\rho,\\nu$ with the same support, define ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F_{\\alpha}(\\rho;\\nu)=\\mathbb{E}_{\\nu}[(\\frac{\\rho}{\\nu})^{\\alpha}]=\\displaystyle\\int\\nu(x)(\\frac{\\rho}{\\nu})^{\\alpha}(x)\\,d x.}\\\\ &{G_{\\alpha}(\\rho;\\nu)=\\mathbb{E}_{\\nu}[(\\frac{\\rho}{\\nu})^{\\alpha}\\|\\nabla\\log\\frac{\\rho}{\\nu}\\|^{2}]=\\mathbb{E}_{\\nu}[(\\frac{\\rho}{\\nu})^{\\alpha-2}\\|\\nabla\\frac{\\rho}{\\nu}\\|^{2}]=\\frac{4}{\\alpha^{2}}\\mathbb{E}_{\\nu}[\\|\\nabla(\\frac{\\rho}{\\nu})^{\\alpha/2}\\|^{2}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note that $\\begin{array}{r}{D_{\\alpha}(\\rho||\\nu)=\\frac{1}{\\alpha-1}\\log F_{\\alpha}(\\rho;\\nu)}\\end{array}$ by definition and $G_{\\alpha}(\\rho;\\nu)$ is known as the R\u00e9nyi Information, where the limit $\\alpha=1$ recovers the relative Fisher information [15]. Now we introduce all the technical lemmas we need. The first is data-processing inequality for R\u00e9nyi divergence, which is the Lemma 2.6 in [32]. The second and third lemmas are based on results in [15]. We note again that we use the definition of LSI in [26], where the LSI constant is reciprocal to those defined in [15]. ", "page_idx": 17}, {"type": "text", "text": "Lemma G.3 (Data-processing inequality for R\u00e9nyi divergence [32]). For any $\\alpha\\geq1$ , any function $h:\\mathbb{R}^{d}\\mapsto\\mathbb{R}^{d}$ and any distribution $\\mu,\\nu$ with support on $\\mathbb{R}^{\\breve{d}}$ ", "page_idx": 17}, {"type": "equation", "text": "$$\nD_{\\alpha}(h_{\\#}\\mu||h_{\\#}\\nu)\\leq D_{\\alpha}(\\mu||\\nu).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Lemma G.4 (Lemma 18 in [15], with customized variance). For any probability distribution $\\rho_{0},\\,\\nu_{0}$ and for any $t\\geq0$ , let $\\rho_{t}=\\bar{\\rho}_{0}\\ast\\mathcal{N}(0,2t\\sigma^{2}I_{d})$ and $\\nu_{t}=\\nu_{0}*\\mathcal{N}(0,\\bar{2}\\bar{t}\\sigma^{2}I_{d})$ . Then for all $\\alpha>0\\,w e$ have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{d}{d t}D_{\\alpha}(\\rho_{t}||\\nu_{t})=-\\alpha\\sigma^{2}\\frac{G_{\\alpha}(\\rho_{t};\\nu_{t})}{F_{\\alpha}(\\rho_{t};\\nu_{t})}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Lemma G.5 (Low bound of G-F ratio, Lemma 5 [15]). Suppose $\\nu$ satisfy $C_{L S I}$ -LSI. Let $\\alpha\\geq1$ . For all probability distribution $\\rho$ we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{G_{\\alpha}(\\rho;\\nu)}{F_{\\alpha}(\\rho;\\nu)}\\geq\\frac{2}{\\alpha^{2}C_{L S I}}D_{\\alpha}(\\rho||\\nu).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Now we are ready to prove Theorem 3.2 under strong convexity assumption. ", "page_idx": 17}, {"type": "text", "text": "Proof. For brevity and to make our proof succinct, we will only prove the harder direction $D_{\\alpha}(\\dot{\\nu}_{\\!\\ensuremath{\\mathcal \u1e0a D \u1e0c }^{\\prime}}||\\rho_{k})$ . The proof of the other direction is not only simpler (due to $\\nu_{\\mathcal{D}^{\\prime}}$ being the stationary distribution), but also the same analysis applies. ", "page_idx": 17}, {"type": "text", "text": "First, let us consider two processes: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{y_{k+1}=\\Pi_{\\mathcal{C}}\\left(y_{k}-\\eta\\nabla f_{\\mathcal{D}^{\\prime}}(y_{k})+\\sqrt{2\\eta\\sigma^{2}}W_{k}\\right),\\mathrm{where~}y_{0}\\sim\\rho_{0}=\\nu_{\\mathcal{D}}}\\\\ &{y_{k+1}^{\\prime}=\\Pi_{\\mathcal{C}}\\left(y_{k}^{\\prime}-\\eta\\nabla f_{\\mathcal{D}^{\\prime}}(y_{k}^{\\prime})+\\sqrt{2\\eta\\sigma^{2}}W_{k}\\right),\\mathrm{where~}y_{0}^{\\prime}\\sim\\nu_{\\mathcal{D}^{\\prime}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note that $y_{k}$ is the process we would have during the unlearning process and $y_{k}^{\\prime}$ is an auxiliary process. Let $\\rho_{k,1},\\rho_{k,2},\\rho_{k}$ be the probability distribution of $y_{k,1},y_{k,2},y_{k}$ respectively, where ", "page_idx": 17}, {"type": "equation", "text": "$$\ny_{k,1}=y_{k}-\\eta\\nabla f_{\\mathcal{D}^{\\prime}}(y_{k}),\\;y_{k,2}=y_{k,1}+\\sqrt{2\\eta\\sigma^{2}}W_{k},\\;y_{k+1}=\\Pi_{\\mathcal{C}}\\left(y_{k,2}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Similarly, let $\\rho_{k,1}^{\\prime},\\rho_{k,2}^{\\prime},\\rho_{k}^{\\prime}$ be the probability distribution of $y_{k,1}^{\\prime},y_{k,2}^{\\prime},y_{k}^{\\prime}$ respectively. By definition $\\nu_{\\mathcal{D}^{\\prime}}$ is the stationary distribution of this process (in fact, both), we know that $\\rho_{k}^{\\prime}=\\nu_{\\mathcal{D}^{\\prime}}$ for all $k\\geq0$ . ", "page_idx": 17}, {"type": "text", "text": "Also, without loss of generality, we assume $\\rho_{k}$ satisfies $C_{k}$ -LSI for some value $C_{k}$ to be determined.   \nNotably by assumption we have $C_{0}=C_{\\mathrm{LSI}}$ . ", "page_idx": 18}, {"type": "text", "text": "Observe that the gradient update $h(\\boldsymbol{y})=y-\\eta\\nabla f_{\\mathcal{D^{\\prime}}}(\\boldsymbol{y})$ is a $(1-\\eta m)$ -Lipschitz map for $f_{\\mathcal{D^{\\prime}}}$ being $L$ -smooth and $m$ -strongly convex due to Lemma 2.2 in [32] when $\\begin{array}{r}{\\dot{\\eta}\\le\\frac{1}{L}}\\end{array}$ . By Lemma G.1 we know that $\\rho_{k,1}$ satisfies $((1-\\eta m)^{2}C_{k})$ -LSI. Next, by Lemma G.3 we have ", "page_idx": 18}, {"type": "equation", "text": "$$\nD_{\\alpha}(\\rho_{k,1}^{\\prime}||\\rho_{k,1})=D_{\\alpha}(h_{\\#}\\rho_{k}^{\\prime}||h_{\\#}\\rho_{k})\\leq D_{\\alpha}(\\rho_{k}^{\\prime}||\\rho_{k})=D_{\\alpha}(\\nu_{\\mathcal{D}^{\\prime}}||\\rho_{k}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Next, consider $\\rho_{k,1,t}=\\rho_{k,1}*\\mathcal{N}(0,2t\\sigma^{2}I_{d})$ and $\\rho_{k,1,t}^{\\prime}=\\rho_{k,1}*\\mathcal{N}(0,2t\\sigma^{2}I_{d})$ for $t\\in[0,\\eta]$ . Clearly, $\\rho_{k,1,\\eta}=\\rho_{k,2}$ and $\\rho_{k,1,\\eta}^{\\prime}=\\rho_{k,2}^{\\prime}$ . By Lemma G.4 we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{d}{d t}D_{\\alpha}(\\rho_{k,1,t}^{\\prime}||\\rho_{k,1,t})=-\\sigma^{2}\\alpha\\frac{G_{\\alpha}(\\rho_{k,1,t}^{\\prime};\\rho_{k,1,t})}{F_{\\alpha}(\\rho_{k,1,t}^{\\prime};\\rho_{k,1,t})}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By Lemma G.1, we know that $\\rho_{k,1,t}$ satisfies $((1-\\eta m)^{2}C_{k}+2\\eta\\sigma^{2})$ -LSI for all $t\\leq\\eta$ . By the choice $\\begin{array}{r}{\\eta\\le\\frac{2}{m}\\big(1-\\frac{\\sigma^{2}}{m C_{k}}\\big)}\\end{array}$ , we know that ", "page_idx": 18}, {"type": "equation", "text": "$$\n(1-\\eta m)^{2}C_{k}+2\\eta\\sigma^{2}\\le C_{k}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Clearly, this would require $\\frac{\\sigma^{2}}{m}<C_{k}$ for $\\eta>0$ . Then by Lemma G.5, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{G_{\\alpha}(\\rho_{k,1,t}^{\\prime};\\rho_{k,1,t})}{F_{\\alpha}(\\rho_{k,1,t}^{\\prime};\\rho_{k,1,t})}\\geq\\frac{2}{\\alpha^{2}C_{k}}D_{\\alpha}(\\rho_{k,1,t}^{\\prime}||\\rho_{k,1,t}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This would imply ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{d}{d t}D_{\\alpha}(\\rho_{k,1,t}^{\\prime}||\\rho_{k,1,t})\\leq-\\frac{2\\sigma^{2}}{\\alpha C_{k}}D_{\\alpha}(\\rho_{k,1,t}^{\\prime}||\\rho_{k,1,t}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By Gronwall\u2019s inequality [46], integrating over $t\\in[0,\\eta]$ gives ", "page_idx": 18}, {"type": "equation", "text": "$$\nD_{\\alpha}(\\rho_{k,2}^{\\prime}||\\rho_{k,2})\\leq\\exp(-\\frac{2\\sigma^{2}\\eta}{\\alpha C_{k}})D_{\\alpha}(\\rho_{k,1}^{\\prime}||\\rho_{k,1}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Apply Lemma G.3 for the mapping $\\Pi_{\\mathcal{C}}$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\nD_{\\alpha}(\\rho_{k+1}^{\\prime}||\\rho_{k+1})\\leq D_{\\alpha}(\\rho_{k,2}^{\\prime}||\\rho_{k,2}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note that by Lemma G.1, we have also shown that $\\rho_{k+1}$ is $C_{k}$ -LSI. This implies $\\rho_{k}$ is $C_{0}$ -LSI, where $C_{0}=C_{\\mathrm{LSI}}$ by our assumption. Combining all results and the fact that $\\nu_{\\mathcal{D}^{\\prime}}$ is the stationary distribution, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\nD_{\\alpha}(\\nu_{\\mathcal{D^{\\prime}}}||\\rho_{k+1})\\leq\\exp(-\\frac{2\\sigma^{2}\\eta}{\\alpha C_{\\mathrm{LSI}}})D_{\\alpha}(\\nu_{\\mathcal{D^{\\prime}}}||\\rho_{k}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Iterating this over $k$ we complete the proof. ", "page_idx": 18}, {"type": "text", "text": "The proof beyond strong convexity is similar, except the characterization of $C_{k}$ is different. As we mentioned in the main text, without strong convexity we can only prove an upper bound of the LSI constant that grows monotonically with respect to the iterations. To prevent a diverging LSI constant, we leverage the boundedness of the projected set $\\displaystyle\\mathcal{C}_{R}$ to establish an iteration-independent bound for the LSI constant. Below we give the proof of Theorem 3.2 without the strong convexity assumption. ", "page_idx": 18}, {"type": "text", "text": "Proof. As before, we will only prove the decay of the direction $D_{\\alpha}(\\nu_{D^{\\prime}}||\\rho_{k})$ , since it is more challenging. We again assume $\\rho_{k}$ is $C_{k}$ -LSI, where $C_{0}=C_{\\mathrm{LSI}}$ by our assumption. First, due to [47] we know that the map $h(y)=y-\\eta\\nabla f_{\\mathcal{D^{\\prime}}}(y)$ is $(1+\\eta L)$ -Lipschitz for $f_{\\cal D^{\\prime}}$ being $L$ -smooth. By Lemma G.1 we know that $\\rho_{k,1}$ satisfies $(1+\\eta L)^{2}C_{k})$ )-LSI. Next, by Lemma G.3 we have ", "page_idx": 18}, {"type": "equation", "text": "$$\nD_{\\alpha}(\\rho_{k,1}^{\\prime}||\\rho_{k,1})\\leq D_{\\alpha}(\\rho_{k}^{\\prime}||\\rho_{k})=D_{\\alpha}(\\mathcal{D}^{\\prime}||\\rho_{k}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Next, by Lemma G.4 we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{d}{d t}D_{\\alpha}(\\rho_{k,1,t}^{\\prime}||\\rho_{k,1,t})=-\\sigma^{2}\\alpha\\frac{G_{\\alpha}(\\rho_{k,1,t}^{\\prime};\\rho_{k,1,t})}{F_{\\alpha}(\\rho_{k,1,t}^{\\prime};\\rho_{k,1,t})}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note that by Lemma G.1, $\\rho_{k,1,t}$ satisfies $((1+\\eta L)^{2}C_{k}+2t\\sigma^{2})$ -LSI. Then by Lemma G.5, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{d}{d t}D_{\\alpha}(\\rho_{k,1,t}^{\\prime}||\\rho_{k,1,t})\\leq-\\frac{2\\sigma^{2}}{\\alpha((1+\\eta L)^{2}C_{k}+2t\\sigma^{2})}D_{\\alpha}(\\rho_{k,1,t}^{\\prime}||\\rho_{k,1,t}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By Gronwall\u2019s inequality [46], integrating over $t\\in[0,\\eta]$ gives ", "page_idx": 19}, {"type": "equation", "text": "$$\nD_{\\alpha}(\\rho_{k,2}^{\\prime}||\\rho_{k,2})\\leq\\exp(-\\int_{t=0}^{\\eta}\\frac{2\\sigma^{2}}{\\alpha((1+\\eta L)^{2}C_{k}+2t\\sigma^{2})}d t)D_{\\alpha}(\\rho_{k,1}^{\\prime}||\\rho_{k,1}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note the calculation ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\int_{t=0}^{\\eta}\\frac{2\\sigma^{2}}{\\alpha((1+\\eta L)^{2}C_{k}+2t\\sigma^{2})}d t=\\int_{t=0}^{\\eta}\\frac{d(2\\sigma^{2}t)}{\\alpha((1+\\eta L)^{2}C_{k}+2t\\sigma^{2})}}\\\\ {\\displaystyle=\\frac{1}{2\\alpha}\\left(\\frac{1}{((1+\\eta L)^{2}C_{k})^{2}}-\\frac{1}{((1+\\eta L)^{2}C_{k}+2\\eta\\sigma^{2})^{2}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By applying Lemma G.3 for the projection operator and combining all results we have ", "page_idx": 19}, {"type": "equation", "text": "$$\nD_{\\alpha}(\\rho_{k+1}^{\\prime}||\\rho_{k+1})\\le\\exp(-\\frac{1}{2\\alpha}\\left(\\frac{1}{((1+\\eta L)^{2}C_{k})^{2}}-\\frac{1}{((1+\\eta L)^{2}C_{k}+2\\eta\\sigma^{2})^{2}}\\right))D_{\\alpha}(\\rho_{k}^{\\prime}||\\rho_{k}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Iterate this over $K$ steps, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\supset_{\\alpha}(\\nu_{\\mathcal{D}^{\\prime}}||\\rho_{K})\\leq\\exp(-\\frac{1}{2\\alpha}\\sum_{k=0}^{K-1}\\frac{1}{2\\alpha}\\left(\\frac{1}{((1+\\eta L)^{2}C_{k})^{2}}-\\frac{1}{((1+\\eta L)^{2}C_{k}+2\\eta\\sigma^{2})^{2}}\\right))D_{\\alpha}(\\nu_{\\mathcal{D}^{\\prime}}||\\rho_{0})\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n=\\exp(-\\frac{1}{2\\alpha}\\sum_{k=0}^{K-1}\\frac{1}{2\\alpha}\\left(\\frac{1}{((1+\\eta L)^{2}C_{k})^{2}}-\\frac{1}{((1+\\eta L)^{2}C_{k}+2\\eta\\sigma^{2})^{2}}\\right))D_{\\alpha}(\\nu_{{D}^{\\prime}}||\\rho_{0}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "To complete the proof, we establish the recursion relation of $C_{k}$ . If $f$ is convex and $\\begin{array}{r}{\\eta\\le\\frac{2}{L}}\\end{array}$ , then $h(x)=\\dot{x}-\\eta\\nabla f(x)$ is 1-Lipschitz. If $f$ is $L$ -smooth only, the map $h$ is $(1\\!+\\!\\eta L)$ -Lipschitz. Applying Lemma G.1 leads to the following recursions on $C_{k}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{Convex:}\\;C_{k+1}\\leq C_{k}+2\\eta\\sigma^{2}\\quad\\mathrm{Non-convex:}\\;C_{k+1}\\leq(1+\\eta L)^{2}C_{k}+2\\eta\\sigma^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "On the other hand, the Corollary 1 in [21] states the following result. ", "page_idx": 19}, {"type": "text", "text": "Lemma G.6 (Corollary 1 in [21]). Let $\\mu$ be a probability measure on $\\mathbb{R}^{d}$ supported on $\\mathcal{C}_{R}$ for some $R\\geq0$ . Then, for each $t\\geq0$ , $\\mu*\\mathcal{N}(0,t I_{d})$ satisfy $C$ -LSI with constant ", "page_idx": 19}, {"type": "equation", "text": "$$\nC\\leq6(4R^{2}+t)\\exp(\\frac{4R^{2}}{t}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Now, consider the following PNGD process similar to Lemma G.1 ", "page_idx": 19}, {"type": "equation", "text": "$$\nx_{k,1}=h(x_{k}),\\;x_{k,2}=x_{k,1}+2\\eta\\sigma^{2}W_{k},\\;x_{k+1}=\\Pi_{{\\mathcal{C}}_{R}}(x_{k,2}),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $h(x)=x-\\eta\\nabla f_{\\mathcal{D}}(x)$ and $W_{k}\\sim\\mathcal{N}(0,I_{d})$ as before. Clearly, due to the projection $\\Pi_{{\\mathcal{C}}_{R}}$ we know that $\\mu_{k}$ is supported on $\\displaystyle\\mathcal{C}_{R}$ . By assumption that $f_{\\mathcal{D}}$ is $M$ -Lipschitz, we know that $\\|f_{\\cal D}(x)\\|\\le M$ and thus $\\mu_{k,1}$ is supported on $\\mathcal{C}_{R+\\eta M}$ . By applying Lemma G.6 we know that $\\mu_{k,2}$ satisfies LSI with constant upper bounded by ", "page_idx": 19}, {"type": "equation", "text": "$$\n6(4(R+\\eta M)^{2}+2\\eta\\sigma^{2})\\exp(\\frac{4(R+\\eta M)^{2}}{2\\eta\\sigma^{2}}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Finally, by Lemma G.1 we know that the projection $\\Pi_{{\\mathcal{C}}_{R}}$ does not increase the LSI constant so that the same LSI constant upper bound holds for all $\\mu_{k}$ . Combining with our previous recursive result we complete the proof. ", "page_idx": 19}, {"type": "text", "text": "If we further have that $f_{\\mathcal{D^{\\prime}}}$ being convex, then by Lemma 3.7 in [47] we know that when $\\begin{array}{r}{\\eta\\le\\frac{2}{L}}\\end{array}$ the gradient map is 1-Lipchitz. As a result, the factor $(1+\\eta L)^{2}$ can be reduced to 1. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "H Proof of Theorem 3.3 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The proof is mainly modified from the analysis of [18] with our LSI constant analysis. First, we list the all needed notations and technical lemmas adopted from [18]. Let us start with the PNGD process with training dataset $\\mathcal{D}$ and $\\mathcal{D}^{\\prime}$ as before ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x_{t+1}=\\Pi_{\\mathcal{C}_{R}}\\left(x_{t}-\\eta\\nabla f_{\\mathcal{D}}(x_{t})+\\sqrt{2\\eta\\sigma^{2}}W_{t}\\right),}\\\\ &{x_{t+1}^{\\prime}=\\Pi_{\\mathcal{C}_{R}}\\left(x_{t}^{\\prime}-\\eta\\nabla f_{\\mathcal{D}^{\\prime}}(x_{t}^{\\prime})+\\sqrt{2\\eta\\sigma^{2}}W_{t}\\right),\\;W_{t}\\overset{\\mathrm{iid}}{\\sim}\\mathcal{N}(0,I_{d}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For each iteration, the above update is equivalent to the following two steps: ", "page_idx": 20}, {"type": "equation", "text": "$$\nx_{t,1}=x_{t}-\\eta\\nabla f_{\\mathcal{D}}(x_{t})+\\sqrt{\\eta\\sigma^{2}}W_{t},\\;x_{t+1}=\\Pi_{\\mathcal{C}_{R}}\\left(x_{t,1}+\\sqrt{\\eta\\sigma^{2}}W_{t}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "That is, it can be decomposed into another noisy GD update followed by a small additive noise with projection. Let $\\nu_{t},\\nu_{t,1},\\bar{\\nu}_{t}^{\\prime},\\nu_{t,1}^{\\prime}$ be the law of $x_{t},x_{t,1},x_{t}^{\\prime},x_{t,1}^{\\prime}$ respectively. Finally, we introduce the following technical lemma from [18] specialized to PNGD case. ", "page_idx": 20}, {"type": "text", "text": "Lemma H.1 (Simplification of Lemma 3.2 in [18]). For any $\\xi_{t},\\xi_{t}^{\\prime}\\in\\mathcal{P}(\\mathbb{R}^{d})$ that both satisfy $C_{t,1}$ -LSI, then we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{D_{\\alpha}(\\xi_{t}\\ast\\mathcal{N}(0,\\eta\\sigma^{2}I_{d})||\\xi_{t}^{\\prime}\\ast\\mathcal{N}(0,\\eta\\sigma^{2}I_{d}))}{\\alpha}\\leq\\frac{D_{\\alpha^{\\prime}}(\\xi_{t}||\\xi_{t}^{\\prime})}{\\alpha^{\\prime}}(1+\\frac{\\eta\\sigma^{2}}{C_{t,1}})^{-1},\\;\\alpha^{\\prime}=\\frac{\\alpha-1}{1+\\frac{\\eta\\sigma^{2}}{C_{t,1}}}+1.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The proof is an application of Lemma G.5 but with the integral involving time-dependent LSI constant.   \nNow we are ready to prove our Theorem 3.3. ", "page_idx": 20}, {"type": "text", "text": "Proof. We first provide a full characterization of the LSI constant of $\\nu_{t},\\nu_{t,1}$ for all $k\\geq0$ , assuming $\\nu_{0}$ is $C_{0}$ -LSI to be chosen later. Let us denote the LSI constant of $\\nu_{t},\\nu_{t,1}$ to be $C_{t},C_{t,1}$ respectively. By Lemma G.1, when $f_{\\mathcal{D}}$ is $L$ -smooth we have that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{C_{t,1}\\leq(1+\\eta L)^{2}C_{t}+\\eta\\sigma^{2},\\quad C_{t+1}\\leq C_{t,1}+\\eta\\sigma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Similarly, by leveraging the same analysis in the proof of Theorem 3.2, using Lemma G.6 with the assumption that $f_{\\mathcal{D}}$ is $M$ -Lipschitz gives the following $k$ independent bound ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C_{t,1}\\leq6(4(R+\\eta M)^{2}+\\eta\\sigma^{2})\\exp(\\frac{4(R+\\eta M)^{2}}{\\eta\\sigma^{2}}),}\\\\ &{C_{t+1}\\leq6(4(R+\\eta M)^{2}+2\\eta\\sigma^{2})\\exp(\\frac{4(R+\\eta M)^{2}}{2\\eta\\sigma^{2}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now we establish the one iteration bound on the R\u00e9nyi divergence. By composition theorem for RDP (and equivalently R\u00e9nyi divergence) [22] and the assumption that $f_{\\mathcal{D}},f_{\\mathcal{D}^{\\prime}}$ are $M$ -Lipschitz, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{D_{\\alpha}(\\nu_{t,1}||\\nu_{t,1}^{\\prime})}{\\alpha}\\leq\\frac{D_{\\alpha}(\\nu_{t}||\\nu_{t}^{\\prime})}{\\alpha}+\\frac{2\\eta S^{2}M^{2}}{\\sigma^{2}n^{2}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This is because the sensitivity of $\\begin{array}{r}{\\|\\nabla f_{\\mathcal{D}}(x)-\\nabla f_{\\mathcal{D}^{\\prime}}(x)\\|^{2}\\,\\le\\,\\frac{S^{2}}{n^{2}}\\,\\times\\,(2\\eta M)^{2}}\\end{array}$ for group size $S\\geq1$ More specifically, there are at most $S$ different pairs of $\\nabla f(x;\\mathbf{d}_{i})-\\nabla f(x;\\mathbf{d}_{i}^{\\prime})$ , and for each pair we have $\\|\\bar{\\eta}\\nabla f(x;\\bar{\\mathbf{d}}_{i})-\\eta\\nabla f(x;\\mathbf{d}_{i}^{\\prime})\\|\\leq2\\eta M$ by triangle inequality and $M$ -Lipschitzness. By triangle inequality again, we have $\\begin{array}{r}{\\|\\nabla f_{\\mathcal{D}}(x)-\\nabla f_{\\mathcal{D}^{\\prime}}(x)\\|^{2}\\leq(\\frac{2\\eta S M}{n})^{2}}\\end{array}$ . On the other hand, the variance of the added Gaussian noise in this step (from $x_{t}$ to $x_{t,1}$ ) is $\\eta\\sigma^{2}$ . Leveraging the standard result of Gaussian mechanism [22] gives the $\\alpha$ -R\u00e9nyi divergence $\\begin{array}{r}{\\frac{4\\alpha\\eta^{2}S^{2}M^{2}/n^{2}}{2\\left(\\sigma^{2}\\eta\\right)}=\\frac{2\\alpha\\eta S^{2}M^{2}}{\\sigma^{2}n^{2}}}\\end{array}$ . Dividing it by $\\alpha$ gives the second term in (47). ", "page_idx": 20}, {"type": "text", "text": "Then by applying Lemma H.1, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{D_{\\alpha}(\\nu_{t+1}||\\nu_{t+1}^{\\prime})}{\\alpha}\\leq\\frac{D_{\\alpha^{\\prime}}(\\nu_{t,1}||\\nu_{t,1}^{\\prime})}{\\alpha^{\\prime}}(1+\\frac{\\eta\\sigma^{2}}{C_{t,1}})^{-1},\\;\\alpha^{\\prime}=\\frac{\\alpha-1}{1+\\frac{\\eta\\sigma^{2}}{C_{t,1}}}+1.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Combining these two bounds we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{D_{\\alpha}(\\nu_{t+1}||\\nu_{t+1}^{\\prime})}{\\alpha}\\leq\\left(\\frac{D_{\\alpha^{\\prime}}(\\nu_{t}||\\nu_{t}^{\\prime})}{\\alpha^{\\prime}}+\\frac{2\\eta S^{2}M^{2}}{\\sigma^{2}n^{2}}\\right)(1+\\frac{\\eta\\sigma^{2}}{C_{t,1}})^{-1},\\;\\alpha^{\\prime}=\\frac{\\alpha-1}{1+\\frac{\\eta\\sigma^{2}}{C_{t,1}}}+1.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Now, iterate this bound for all $t$ and note that $D_{\\alpha}(\\nu_{0}||\\nu_{0}^{\\prime})\\,=\\,0$ for any $\\alpha\\,>\\,1$ due to the same initialization, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{D_{\\alpha}(\\nu_{T}||\\nu_{T}^{\\prime})}{\\alpha}\\leq\\frac{2\\eta S^{2}M^{2}}{\\sigma^{2}n^{2}}\\sum_{t=1}^{T}\\prod_{t^{\\prime}=0}^{t-1}(1+\\frac{\\eta\\sigma^{2}}{C_{t^{\\prime},1}})^{-1}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The same analysis applies to the other direction D\u03b1(\u03bdt+\u03b11||\u03bdt+1). Together we complete the proof for convex and non-convex cases. For the $m$ -strongly convex case, it is a direct result of Theorem D.6 in [18], where the LSI constant analysis of $C_{t}$ is exactly the same to those of Theorem 3.2. Together we complete the proof. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "I Proof of Corollary 3.4 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Corollary I.1 (Sequential unlearning). Assume the unlearning requests arrive sequentially such that our dataset changes from $D=\\mathcal{D}_{0}\\to\\mathcal{D}_{1}\\to...\\to\\mathcal{D}_{S}$ , where $\\mathcal{D}_{s},\\mathcal{D}_{s+1}$ are adjacent. Let $y_{k}^{(s)}$ be the unlearned parameters for the $s^{t h}$ unlearning request with $k$ unlearning update following (2) on $\\mathcal{D}_{s}$ and $y_{0}^{(s+1)}\\,=\\,y_{K_{s}}^{(s)}\\,\\sim\\,\\bar{\\nu}_{\\ensuremath{\\mathcal{D}_{s}}}$ , where $y_{0}^{(1)}=\\bar{x}_{\\infty}$ and $K_{s}$ is the unlearning steps for the $s^{t h}$ unlearning request. Suppose we have achieved $(\\alpha,\\varepsilon^{(s)}(\\alpha))$ -RU for the $s^{t h}$ unlearning request, the learning process (1) is $\\left(\\alpha,\\varepsilon_{0}(\\alpha)\\right)$ -RDP and $\\bar{\\nu}_{\\mathcal{D}_{s}}$ satisfies $C_{L S I}{-}L S I;$ , we achieve $(\\alpha,\\varepsilon^{(s+1)}(\\alpha))$ -RU for the $(s+1)^{t h}$ unlearning request as well, where ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\varepsilon^{(s+1)}(\\alpha)\\leq\\exp(-\\frac{1}{\\alpha}\\sum_{k=0}^{K_{s+1}-1}R_{k})\\frac{\\alpha-1/2}{\\alpha-1}\\left(\\varepsilon_{0}(2\\alpha)+\\varepsilon^{(s)}(2\\alpha)\\right),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "$\\varepsilon^{(0)}(\\alpha)=0\\,\\forall\\alpha>1$ and $R_{k}$ are defined in Theorem 3.2. ", "page_idx": 21}, {"type": "text", "text": "While our main theorems only discuss one unlearning request, we can generalize it to address multiple unlearning requests. Consider the case where our learning process is trained with dataset $\\mathcal{D}$ . At the unlearning phase, we receive a sequence of unlearning requests so that our dataset becomes $\\mathcal{D}_{1},\\mathcal{D}_{2},\\ldots,\\mathcal{D}_{S}$ , where each consecutive dataset $\\mathcal{D}_{s},\\mathcal{D}_{s+1}$ are adjacent (i.e., each unlearning request ask for unlearning one data point). Let us denote $\\nu_{D_{s}}$ the output probability distribution of $\\bar{\\mathcal{M}}(\\bar{\\mathcal{D}}_{s})$ for $s~\\geq~0$ , where we set $\\mathcal{D}_{0}\\,=\\,\\mathcal{D}$ . Sequential unlearning can be viewed as transferring along $\\nu_{\\mathcal{D}_{0}}\\rightarrow\\nu_{\\mathcal{D}_{1}}\\cdot\\cdot\\cdot\\rightarrow\\nu_{\\mathcal{D}_{S}}$ , where for each request we will stop when we are \u201c $\\varepsilon$ \u201d away from the target distribution in terms of R\u00e9nyi difference. As a result, our actual path is $\\nu_{\\mathcal{D}_{0}}\\,\\rightarrow\\,\\bar{\\nu}_{\\mathcal{D}_{1}}\\cdot\\cdot\\cdot\\,\\rightarrow\\,\\bar{\\nu}_{\\mathcal{D}_{S}}$ for some sequence of distribution $\\{\\bar{\\nu}_{D_{s}}\\}_{s=1}^{S}$ such that the $\\alpha$ R\u00e9nyi difference $d_{\\alpha}(\\nu_{\\mathcal{D}_{s}},\\bar{\\nu}_{\\mathcal{D}_{s}})\\,\\leq\\,\\varepsilon$ See Figure 2 for a pictorial example of the case $S=2$ . While we are unable to characterize the convergence along $\\bar{\\nu}_{\\bar{D}_{s}}\\rightarrow\\bar{\\nu}_{\\bar{D}_{s+1}}$ directly, we can leverage the weak triangle inequality of R\u00e9nyi divergence to provide an upper bound of it. ", "page_idx": 21}, {"type": "text", "text": "Proposition I.2 (Weak Triangle Inequality of R\u00e9nyi divergence, Corollary 4 in [22]). For any $\\alpha>1$ , $p,q>1$ satisfying $1/p+1/\\bar{q}=1$ and distributions $P,Q,R$ with the same support: ", "page_idx": 21}, {"type": "equation", "text": "$$\nD_{\\alpha}(P||R)\\leq{\\frac{\\alpha-{\\frac{1}{p}}}{\\alpha-1}}D_{p\\alpha}(P||Q)+D_{q(\\alpha-1/p)}(Q||R).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Note that by choosing $p\\,=\\,q\\,=\\,2$ , we can also establish the weak triangle inequality for R\u00e9nyi difference $d_{\\alpha}$ as follows ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{\\alpha}(P||R)\\le\\displaystyle\\frac{\\alpha-\\frac{1}{2}}{\\alpha-1}D_{2\\alpha}(P||Q)+D_{2\\alpha-1}(Q||R)}\\\\ &{\\overset{(a)}{\\le}\\displaystyle\\frac{\\alpha-\\frac{1}{2}}{\\alpha-1}d_{2\\alpha}(P,Q)+d_{2\\alpha-1}(Q,R)}\\\\ &{\\overset{(b)}{\\le}\\displaystyle\\frac{\\alpha-\\frac{1}{2}}{\\alpha-1}d_{2\\alpha}(P,Q)+d_{2\\alpha}(Q,R)}\\\\ &{\\overset{(c)}{\\le}\\displaystyle\\frac{\\alpha-\\frac{1}{2}}{\\alpha-1}\\left(d_{2\\alpha}(P,Q)+d_{2\\alpha}(Q,R)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where (a) is due to the definition of R\u00e9nyi difference, (b) is due to the monotonicity of R\u00e9nyi divergence in $\\alpha$ and (c) is due to the fact that for all $\\alpha>1$ , ${\\frac{\\alpha\\!-\\!{\\frac{1}{2}}}{\\alpha\\!-\\!1}}\\geq1$ . Repeat the same analysis for $D_{\\alpha}(R||P)$ and combine with the bound above, one can show that ", "page_idx": 22}, {"type": "equation", "text": "$$\nd_{\\alpha}(P,R)\\leq\\frac{\\alpha-\\frac{1}{2}}{\\alpha-1}\\left(d_{2\\alpha}(P,Q)+d_{2\\alpha}(Q,R)\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The main idea is illustrated in Figure 2 (a). We first leverage Theorem 3.2 to upper bound the R\u00e9nyi difference $d_{\\alpha}(\\bar{\\nu}_{D_{2}},\\nu_{D_{2}})$ in terms of the R\u00e9nyi difference between $d_{\\alpha}(\\bar{\\nu}_{\\mathcal{D}_{1}},\\nu_{\\mathcal{D}_{2}})$ (dash line) with a decaying factor. Then by weak triangle inequality of R\u00e9nyi difference we derived above, we can further bound it with $\\varepsilon^{(1)}(2\\alpha)$ (black line) and $\\varepsilon_{0}(2\\alpha)$ (red line). ", "page_idx": 22}, {"type": "text", "text": "Proof. The proof is a direct combination of Theorem 3.2 and Proposition I.2. To achieve $(\\alpha,\\varepsilon^{(s+1)}(\\alpha))$ -RU for the $(s+1)^{t h}$ unlearning request, we need to bound $d_{\\alpha}(\\tilde{\\nu}_{D_{s+1}},\\nu_{D_{s+1}})$ . Assume we run $K_{s+1}$ unlearning iteration, from Theorem 3.2 we have ", "page_idx": 22}, {"type": "equation", "text": "$$\nd_{\\alpha}(\\tilde{\\nu}_{{\\cal D}_{s+1}},\\nu_{{\\cal D}_{s+1}})\\leq\\exp(-\\frac{1}{\\alpha}\\sum_{k=0}^{K_{s+1}-1}R_{k})d_{\\alpha}(\\tilde{\\nu}_{{\\cal D}_{s}},\\nu_{{\\cal D}_{s+1}}),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $R_{k}$ is defined in Theorem 3.2. On the other hand, by weak triangle inequality of R\u00e9nyi difference, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\nd_{\\alpha}(\\tilde{\\nu}_{\\mathscr{D}_{s}},\\nu_{\\mathscr{D}_{s+1}})\\leq\\frac{\\alpha-1/2}{\\alpha-1}\\left(d_{2\\alpha}(\\tilde{\\nu}_{\\mathscr{D}_{s}},\\nu_{\\mathscr{D}_{s}})+d_{2\\alpha}(\\nu_{\\mathscr{D}_{s}},\\nu_{\\mathscr{D}_{s+1}})\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By the initial RDP condition, we know that $d_{2\\alpha}(\\nu_{\\mathcal{D}_{s}},\\nu_{\\mathcal{D}_{s+1}})\\leq\\varepsilon_{0}(2\\alpha)$ . On the other hand, by the RU guarantee of the $s^{t h}$ unlearning request, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\nd_{2\\alpha}(\\tilde{\\nu}_{\\mathcal{D}_{s}},\\nu_{\\mathcal{D}_{s}})\\leq\\varepsilon^{(s)}(2\\alpha).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Together we have ", "page_idx": 22}, {"type": "equation", "text": "$$\nd_{\\alpha}(\\tilde{\\nu}_{\\mathscr{D}_{s}},\\nu_{\\mathscr{D}_{s+1}})\\leq\\frac{\\alpha-1/2}{\\alpha-1}\\left(\\varepsilon_{0}(2\\alpha)+\\varepsilon^{(s)}(2\\alpha)\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Hence we complete the proof. ", "page_idx": 22}, {"type": "text", "text": "J Proof of Lemma G.1 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Lemma (LSI constant characterization). Consider the following PNGD update for a closed convex set $\\mathcal{C}$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\nx_{k,1}=h(x_{k}),\\;x_{k,2}=x_{k,1}+\\sigma W_{k},\\;x_{k+1}=\\Pi_{\\mathcal{C}}(x_{k,2}),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $h$ is any $M$ -Lipschitz map $\\mathbb{R}^{d}\\mapsto\\mathbb{R}^{d}$ , $W_{k}\\sim\\mathcal{N}(0,I_{d})$ independent of anything before step $k$ , and $\\Pi_{\\mathcal{C}}$ is the projection onto $\\mathcal{C}$ . Let $\\mu_{k,1},\\mu_{k,2}$ and $\\mu_{k}$ be the probability distribution of $x_{k,1},\\,x_{k,2}$ and $x_{k}$ respectively. Then we have the following $L S I$ constant characterization of this process. 1) If $\\mu_{k}$ satisfies $c$ -LSI, $\\mu_{k,1}$ satisfies $M^{2}c$ -LSI. 2) If $\\mu_{k,1}$ satisfies $c$ -LSI, $\\mu_{k,2}$ satisfies $(c+\\sigma^{2}){-}L S I.\\;3)\\;I\\!f$ $\\mu_{k,2}$ satisfies $c$ -LSI, $\\mu_{k+1}$ satisfies $c$ -LSI. ", "page_idx": 22}, {"type": "text", "text": "Proof. The first statement is the direct result of Proposition 2.3.3. in [26]. See also Lemma 16 in [15] but additionally require $h$ being differentiable. The second statement is the direct result of Lemma 17 in [15]. The third statement is because $\\Pi_{\\mathcal{C}}$ is a 1-Lipchitz map. Together we complete the proof. ", "page_idx": 23}, {"type": "text", "text": "K Proof of Lemma G.4 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Lemma (Lemma 18 in [15], with customized variance). For any probability distribution $\\rho_{0},\\,\\nu_{0}$ and for any $t\\geq0$ , let $\\rho_{t}=\\rho_{0}*\\mathcal{N}(0,2t\\sigma^{2}I_{d})$ and $\\nu_{t}=\\nu_{0}*\\mathcal{N}(0,2t\\bar{\\sigma}^{2}I_{d})$ . Then for all $\\alpha>0$ we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{d}{d t}D_{\\alpha}(\\rho_{t}||\\nu_{t})=-\\alpha\\sigma^{2}\\frac{G_{\\alpha}(\\rho_{t};\\nu_{t})}{F_{\\alpha}(\\rho_{t};\\nu_{t})}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. The proof is nearly identical to that in [15]. Let $X_{t}\\ \\sim\\ \\rho_{t}$ , then we have the following stochastic differential equation. ", "page_idx": 23}, {"type": "equation", "text": "$$\nd X_{t}=\\sqrt{2}\\sigma d W_{t}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Thus $\\rho_{t}$ evolves following the Fokker-Planck equation: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{\\partial\\rho_{t}}{\\partial t}=\\sigma^{2}\\Delta\\rho_{t}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Same for $\\nu_{t}$ and just plug this into the first step in the proof of Lemma 18 in [15], which gives the result. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "L Proof of Proposition E.1 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The proof is a direct manipulation of the R\u00e9nyi divergence. Due to symmetry, we will only show that $\\begin{array}{r}{D_{\\alpha}(\\dot{\\tilde{\\nu}}_{\\mathcal{D}},\\tilde{\\nu}_{\\mathcal{D}^{\\prime}})\\leq\\frac{2\\alpha F}{(\\alpha-1)n}}\\end{array}$ , as the proof for the bound of $D_{\\alpha}(\\tilde{\\nu}_{D^{\\prime}},\\tilde{\\nu}_{D})$ is identical. ", "page_idx": 23}, {"type": "text", "text": "Define $\\begin{array}{r}{Z_{\\mathcal{D}}=\\int\\exp(-f_{\\mathcal{D}}(x))d x}\\end{array}$ be the normalizing constant. Then we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{\\alpha}(\\tilde{\\nu}_{\\mathcal{D}},\\tilde{\\nu}_{\\mathcal{D^{\\prime}}})=\\frac{1}{\\alpha-1}\\log\\mathbb{E}_{x\\sim\\tilde{\\nu}_{\\mathcal{D^{\\prime}}}}\\left(\\frac{\\tilde{\\nu}_{\\mathcal{D}}(x)}{\\tilde{\\nu}_{\\mathcal{D^{\\prime}}}(x)}\\right)^{\\alpha}=\\frac{1}{\\alpha-1}\\log\\mathbb{E}_{x\\sim\\tilde{\\nu}_{\\mathcal{D}}}\\left(\\frac{\\tilde{\\nu}_{\\mathcal{D}}(x)}{\\tilde{\\nu}_{\\mathcal{D^{\\prime}}}(x)}\\right)^{\\alpha-1}}\\\\ &{\\,=\\frac{1}{\\alpha-1}\\log\\left(\\left(\\frac{Z_{\\mathcal{D^{\\prime}}}}{Z_{\\mathcal{D}}}\\right)^{\\alpha-1}\\mathbb{E}_{x\\sim\\tilde{\\nu}_{\\mathcal{D}}}\\left(\\frac{\\exp\\left(-f_{\\mathcal{D}}(x)\\right)}{\\exp\\left(-f_{\\mathcal{D^{\\prime}}}(x)\\right)}\\right)^{\\alpha-1}\\right)}\\\\ &{\\,=\\log(\\frac{Z_{\\mathcal{D^{\\prime}}}}{Z_{\\mathcal{D}}})+\\frac{1}{\\alpha-1}\\log(\\mathbb{E}_{x\\sim\\tilde{\\nu}_{\\mathcal{D}}}\\left(\\frac{\\exp\\left(-f_{\\mathcal{D}}(x)\\right)}{\\exp\\left(-f_{\\mathcal{D^{\\prime}}}(x)\\right)}\\right)^{\\alpha-1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Recall that $\\mathcal{D}$ and $\\mathcal{D}^{\\prime}$ are adjacent, thus they only differ in one index. Without loss of generality, assume the index is $n$ so that $\\mathbf{d}_{i}=\\mathbf{d}_{i}^{\\prime}$ for all $i<n$ . By definition, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{f_{{\\mathcal{D}}^{\\prime}}(x)=\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n-1}f(x;\\mathbf{d}_{i}^{\\prime})+\\displaystyle\\frac{1}{n}f(x;\\mathbf{d}_{n}^{\\prime})}\\\\ {\\displaystyle=\\frac{1}{n}\\sum_{i=1}^{n-1}f(x;\\mathbf{d}_{i}^{\\prime})+\\displaystyle\\frac{1}{n}f(x;\\mathbf{d}_{n})+\\frac{1}{n}f(x;\\mathbf{d}_{n}^{\\prime})-\\frac{1}{n}f(x;\\mathbf{d}_{n})}\\\\ {\\displaystyle=f_{{\\mathcal{D}}}(x)+\\frac{1}{n}(f(x;\\mathbf{d}_{n}^{\\prime})-f(x;\\mathbf{d}_{n})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "As a result, the ratio of the normalizing constant can be bounded as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{Z_{\\mathcal{D}^{\\prime}}}{Z_{\\mathcal{D}}}=\\frac{\\int\\exp(-f_{\\mathcal{D}^{\\prime}}(x))d x}{Z_{\\mathcal{D}}}=\\frac{\\int\\exp(-f_{\\mathcal{D}^{\\prime}}(x))d x}{Z_{\\mathcal{D}}}=\\frac{\\int\\exp(-f_{\\mathcal{D}}(x)+\\frac{f(x;\\mathbf{d}_{n}^{\\prime})-f(x;\\mathbf{d}_{n})}{n})d x}{Z_{\\mathcal{D}}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\frac{\\int\\exp(-f_{\\mathcal{D}}(x)+\\frac{|f(x;\\mathbf{d}_{n}^{\\prime})-f(x;\\mathbf{d}_{n})|}{n})d x}{Z_{\\mathcal{D}}}}\\\\ &{\\leq\\frac{\\int\\exp(-f_{\\mathcal{D}}(x)+\\frac{F}{n})d x}{Z_{\\mathcal{D}}}=\\frac{\\exp(\\frac{F}{n})\\int\\exp(-f_{\\mathcal{D}}(x))d x}{Z_{\\mathcal{D}}}=\\frac{\\exp(\\frac{F}{n})Z_{\\mathcal{D}}}{Z_{\\mathcal{D}}}=\\exp(\\frac{F}{n}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "On the other hand, for the second term we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{x\\sim\\tilde{\\nu}_{\\mathcal{D}}}\\left(\\frac{\\exp(-f_{\\mathcal{D}}(x))}{\\exp(-f_{\\mathcal{D}^{\\prime}}(x))}\\right)^{\\alpha-1}=\\mathbb{E}_{x\\sim\\tilde{\\nu}_{\\mathcal{D}}}\\exp(-(\\alpha-1)(f_{\\mathcal{D}}(x)-f_{\\mathcal{D}^{\\prime}}(x)))}\\\\ &{\\le\\mathbb{E}_{x\\sim\\tilde{\\nu}_{\\mathcal{D}}}\\exp((\\alpha-1)(\\frac{F}{n}))=\\exp((\\alpha-1)(\\frac{F}{n})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "As a result, we can further simplify (64) as follows ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{\\alpha}(\\tilde{\\nu}_{\\mathcal{D}},\\tilde{\\nu}_{\\mathcal{D}^{\\prime}})=\\log(\\frac{Z_{\\mathcal{D}^{\\prime}}}{Z_{\\mathcal{D}}})+\\frac{1}{\\alpha-1}\\log(\\mathbb{E}_{x\\sim\\tilde{\\nu}_{\\mathcal{D}}}\\left(\\frac{\\exp(-f_{\\mathcal{D}}(x))}{\\exp(-f_{\\mathcal{D}^{\\prime}}(x))}\\right)^{\\alpha-1})}\\\\ &{\\,\\le\\frac{F}{n}+\\frac{(\\alpha-1)F}{(\\alpha-1)n}=\\frac{2F}{n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Together we complete the proof. ", "page_idx": 24}, {"type": "text", "text": "M Experiment Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "M.1 $(\\alpha,\\varepsilon)$ -RU to $(\\epsilon,\\delta)$ -Unlearning Conversion ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Let us first state the definition of $(\\epsilon,\\delta)$ -unlearning from prior literature [1,7,8]. ", "page_idx": 24}, {"type": "text", "text": "Definition M.1. Consider a randomized learning algorithm $\\mathcal{M}\\,:\\,\\mathcal{X}^{n}\\,\\mapsto\\,\\mathbb{R}^{d}$ and a randomized unlearning algorithm $\\mathcal{U}:\\mathbb{R}^{d}\\times\\mathcal{X}^{n}\\times\\mathcal{X}^{n}\\mapsto\\mathbb{R}^{d}$ . We say $(\\mathcal{M},\\mathcal{U})$ achieves $(\\epsilon,\\delta)$ -unlearning if for any adjacent datasets $\\mathcal{D},\\mathcal{D}^{\\prime}$ and any event $E$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(\\mathcal{U}(\\mathcal{M}(\\mathcal{D}),\\mathcal{D},\\mathcal{D}^{\\prime})\\subseteq E\\right)\\leq\\exp(\\epsilon)\\mathbb{P}\\left(\\mathcal{M}(\\mathcal{D}^{\\prime})\\subseteq E\\right)+\\delta,}\\\\ {\\mathbb{P}\\left(\\mathcal{M}(\\mathcal{D}^{\\prime})\\subseteq E\\right)\\leq\\exp(\\epsilon)\\mathbb{P}\\left(\\mathcal{U}(\\mathcal{M}(\\mathcal{D}),\\mathcal{D},\\mathcal{D}^{\\prime})\\subseteq E\\right)+\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Following the same proof of RDP-DP conversion (Proposition 3 in [22]), we have the following $(\\alpha,\\varepsilon)$ -RU to $(\\epsilon,\\delta)$ -unlearning conversion as well. ", "page_idx": 24}, {"type": "text", "text": "Proposition M.2. I $f(\\mathcal{M},\\mathcal{U})$ achieves $(\\alpha,\\varepsilon)$ -RU, it satisfies $(\\epsilon,\\delta)$ -unlearning as well, where ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\epsilon=\\varepsilon+\\frac{\\log(1/\\delta)}{\\alpha-1}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "M.2 Datasets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "MNIST [34] contains the grey-scale image of number 0 to number 9, each with $28\\times28$ pixels. We follow [8] to take the images with the label 3 and 8 as the two classes for logistic regression. The training data contains 11982 instances in total and the testing data contains 1984 samples. We spread the image into an $x\\in\\mathbb{R}^{d},d=724$ feature as the input of logistic regression. ", "page_idx": 24}, {"type": "text", "text": "CIFAR-10 [35] contains the RGB-scale image of ten classes for image classification, each with $32\\times32$ pixels. For CIFAR-10-binary, we also select class #3 (cat) and class #8 (ship) as the two classes for logistic regression. The training data contains 10000 instances and the testing data contains 2000 samples. As to CIFAR-10-multi-class, we include all the classes for multi-class logistic regression. The training data contains 50000 instances and the testing data contains 10000 samples. We apply data pre-processing on CIFAR-10 by extracting the compact feature encoding from the last layer before pooling of an off-the-shelf pre-trained ResNet18 model [36] from Torchvision library [48, 49] as the input of our logistic regression. The compact feature encoding is $x\\in\\mathbb{R}^{d},d=\\overline{{5}}\\mathrm{i}2$ . ", "page_idx": 24}, {"type": "text", "text": "All the inputs from the datasets are normalized with the $\\ell_{2}$ norm of 1. ", "page_idx": 24}, {"type": "text", "text": "M.3 Experiment Settings ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Hardware and Frameworks All the experiments run with PyTorch=2.1.2 [50] and numpy $=1.24.3$ [51]. The codes run on a server with a single NVIDIA RTX 6000 GPU with AMD EPYC 7763 64-Core Processor. ", "page_idx": 24}, {"type": "text", "text": "Problem Formulation Given a binary classification task $\\mathcal{D}=\\{\\mathbf{x}_{i}\\in\\mathbb{R}^{d},y_{i}\\in\\{-1,+1\\}\\}_{i=1}^{n}$ , our goal is to obtain a set of parameters $\\mathbf{w}$ that optimizes the objective below: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\mathbf{w};\\mathcal{D})=\\frac{1}{n}\\sum_{i=1}^{n}l(\\mathbf{w}^{\\top}\\mathbf{x}_{i},y_{i})+\\frac{\\lambda}{2}||\\mathbf{w}||_{2}^{2},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the objective consists of a standard logistic regression loss $l(\\mathbf{w}^{\\top}x_{i},y_{i})=-\\log\\sigma(y_{i}\\mathbf{w}^{\\top}\\mathbf{x}_{i})$ , where $\\begin{array}{r}{\\sigma(t)\\dot{=}\\;\\frac{1}{1+\\exp(-t)}}\\end{array}$ is the sigmoid function; and a $\\ell_{2}$ regularization term where $\\lambda$ is a hyperparameter to control the regularization, and we set $\\lambda$ as $10^{-6}\\times n$ across all the experiments. By simple algebra one can show that [7] ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla l(\\mathbf w^{\\top}\\mathbf x_{i},y_{i})=(\\sigma(y_{i}\\mathbf w^{\\top}\\mathbf x_{i})-1)y_{i}\\mathbf x_{i}+\\lambda\\mathbf w,}\\\\ &{\\nabla^{2}l(\\mathbf w^{\\top}\\mathbf x_{i},y_{i})=\\sigma(y_{i}\\mathbf w^{\\top}\\mathbf x_{i})(1-\\sigma(y_{i}\\mathbf w^{\\top}\\mathbf x_{i}))\\mathbf x_{i}\\mathbf x_{i}^{T}+\\lambda I_{d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Due to $\\sigma(y_{i}\\mathbf{w}^{\\top}\\mathbf{x}_{i})\\in[0,1]$ , it is not hard to see that we have smoothness $L=1/4+\\lambda$ and strong convexity $\\lambda$ . ", "page_idx": 25}, {"type": "text", "text": "The per-sample gradient with clipping w.r.t. the weights $\\mathbf{w}$ of the logistic regression loss function is given as: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\nabla_{c l i p}l(\\mathbf{w}^{\\top}\\mathbf{x}_{i},y_{i})=\\Pi_{{\\mathcal{C}}_{M}}\\left((\\sigma(y_{i}\\mathbf{w}^{\\top}\\mathbf{x}_{i})-1)y_{i}\\mathbf{x}_{i}\\right)+\\lambda\\mathbf{w},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\Pi_{{\\mathcal{C}}_{M}}$ denotes the gradient clipping projection into the Euclidean ball with the radius of $M$ to satisfy the Lipschitz constant bound. According to Proposition 5.2 of [18], the per-sampling clipping operation still results in a $L$ -smooth, $m$ -strongly convex objective. The resulting Langevin learning/unlearning update on the full dataset is as follows: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{i=1}^{n}\\nabla_{c l i p}l(\\mathbf{w}^{T}\\mathbf{x}_{i},y_{i}),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Finally, we remark that in our specific case since we have normalized the features of all data points (i.e., $\\|x\\|=1)$ , by the explicit gradient formula we know that $\\|(\\sigma(y_{i}\\mathbf{w}^{\\top}\\mathbf{x}_{i})-1)y_{i}\\mathbf{x}_{i}\\|\\leq1$ . ", "page_idx": 25}, {"type": "text", "text": "As to multi-class classification task $\\mathcal{D}=\\{\\mathbf{x}_{i}\\in\\mathbb{R}^{d},y_{i}\\in\\{-1,+1\\}^{c}\\}_{i=1}^{n}$ , the loss function is denoted as follows [18]: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\mathbf{w};\\mathcal{D})=\\frac{1}{n}\\sum_{i=1}^{n}l(\\mathbf{w}^{\\top}\\mathbf{x}_{i},y_{i})+\\frac{\\lambda}{2}||\\mathbf{w}||_{2}^{2},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where ", "page_idx": 25}, {"type": "equation", "text": "$$\nl(\\mathbf{w}^{\\top}\\mathbf{x}_{i},y_{i})=-y^{1}\\log(\\frac{e^{w_{1}^{\\top}x_{i}}}{e^{w_{1}^{\\top}x_{i}}+\\ldots+e^{w_{c}^{\\top}x_{i}}})-\\ldots-y^{c}\\log(\\frac{e^{w_{c}^{\\top}x_{i}}}{e^{w_{1}^{\\top}x_{i}}+\\ldots+e^{w_{c}^{\\top}x_{i}}}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "After similar derivations, the aforementioned objective function can also yield an explicit expression for the gradient, as well as bounds for the constants. ", "page_idx": 25}, {"type": "text", "text": "The constant meta-data of the loss function in equation (80) and (85) above for the datasets is shown in the table below: ", "page_idx": 25}, {"type": "table", "img_path": "3LKuC8rbyV/tmp/d77393973d04c3d16d365a98007e15de47490b49d4c0646273d0280ada86719b.jpg", "table_caption": ["Table 1: The constants for the loss function and other calculation on MNIST and CIFAR-10. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "Learning from scratch set-up For the baselines and our Langevin unlearning framework, we all sample the initial weight w randomly sampled from i.i.d Gaussian distribution $\\bar{\\mathcal{N}}(\\mu_{0},C_{\\mathrm{LSI}})$ , where $\\mu_{0}$ is a hyper-parameter denoting the initialization mean and we set as 1000 to simulate the situation where the initial $w$ has a long distance towards the optimum alike most situations in real-world applications. For the learning methods $\\mathcal{M}$ , we set $T=10,000$ for all the methods to converge. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "Unlearning request implementation. In our experiment, for an unlearning request of removing data point $i$ , we replace its feature with random features drawn from $\\mathcal{N}(0,\\bar{I_{d}})$ and its label with a random label drawn uniformly at random drawn from all possible classes. This is similar to the DP replacement definition defined in [52], where they replace a point with a special null point $\\bot$ . ", "page_idx": 26}, {"type": "text", "text": "General implementation of baseline D2D [8] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "\u2022 Across all of our experiments involved with D2D, we follow the original paper to set the step size as $2/(L+m)$ . ", "page_idx": 26}, {"type": "text", "text": "\u2022 For the experiments in Fig. 3a, we calculate the noise to add after gradient descent with the nonprivate bound as illustrated in Theorem. N.1 (Theorem 9 in [8]); For experiments with sequential unlearning requests in Fig. 3b, we calculate the least step number and corresponding noise with the bound in Theorem. N.2(Theorem 28 in [8]). ", "page_idx": 26}, {"type": "text", "text": "\u2022 The implementation of D2D follows the pseudo code shown in Algorithm 1,2 in [8] as follows: ", "page_idx": 26}, {"type": "table", "img_path": "3LKuC8rbyV/tmp/5c133510ab741e208cebbeb3d0eaf58e158a93b96c7fd246bca4b192f2c06526.jpg", "table_caption": [], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "3LKuC8rbyV/tmp/877889f840388803c44b452e953a4dfdcf3918d6636af4d6541180b97a35db80.jpg", "table_caption": [], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "The settings and the calculation of $I,\\sigma$ in Algorithm. 2 are discussed in the later part of this section and could be found in Section. N. ", "page_idx": 26}, {"type": "text", "text": "General Implementation of Langevin Unlearning ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "\u2022 We set the step size $\\eta$ for Langevin unlearning framework across all the experiments as $1/L$ . ", "page_idx": 27}, {"type": "text", "text": "\u2022 The pseudo code for Langevin unlearning framework is as follows: ", "page_idx": 27}, {"type": "table", "img_path": "3LKuC8rbyV/tmp/387599d34dac0a1aed2c5ec506d9f1c778bbde31f279dfeb5c5df90699674f56.jpg", "table_caption": [], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "M.4 Implementation Details for Fig. 3a ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this experiment, we first train the methods on the original dataset $\\mathcal{D}$ from scratch to obtain the initial weights $\\mathbf{w}_{\\mathrm{0}}$ . Then we randomly remove a single data point $(S=1)$ ) from the dataset to get the new dataset $\\mathcal{D}^{\\prime}$ , and unlearn the methods from the initial weights $\\hat{\\bf w}$ and test the accuracy on the testing set. ", "page_idx": 27}, {"type": "text", "text": "we set the target $\\hat{\\epsilon}$ with 6 different values as $[0.05,0.1,0.5,1,2,5]$ . For each target $\\hat{\\epsilon}$ : ", "page_idx": 27}, {"type": "text", "text": "\u2022 For D2D, we set three different unlearning gradient descent step budgets as $I\\,=\\,1,2,5$ , and calculate the corresponding noise to be added to the weight after gradient descent on $\\mathcal{D}$ according to Theorem. N.1, where the detailed noise information is shown in the table below: ", "page_idx": 27}, {"type": "table", "img_path": "3LKuC8rbyV/tmp/b06f84a34fde6fc6589d6faaa8899acef44f29b4befed9b4dd2f2fe586f83d66.jpg", "table_caption": ["Table 2: Baseline $\\sigma$ details in Fig. 3a "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "\u2022 For the Langevin unlearning framework, we set the unlearning fine-tune step budget as K\u02c6 = 1 only, and calculate the smallest $\\sigma$ that could satisfy the fine-tune step budget and target $\\hat{\\epsilon}$ at the same time. The calculation follows the binary search algorithm as follows: ", "page_idx": 27}, {"type": "text", "text": "Algorithm 4 Langevin Unlearning: binary search $\\sigma$ that satisfy K\u02c6 and target budget   \n1: Input:target $\\hat{\\epsilon}$ , unlearn step budget $K$ , lower bound $\\sigma_{\\mathrm{low}}$ , upper bound $\\sigma_{\\mathrm{high}}$   \n2: while $\\sigma_{\\mathrm{low}}\\le\\sigma_{\\mathrm{high}}$ do   \n3: $\\sigma_{\\mathrm{mid}}=(\\sigma_{\\mathrm{low}}+\\sigma_{\\mathrm{high}})/2$   \n4: call Alg. 5 to find the least $K$ that satisfies $\\hat{\\epsilon}$ with $\\sigma=\\sigma_{\\mathrm{mid}}$   \n5: if $K==\\hat{K}$ then   \n6: Return $K$   \n7: else if $K\\leq{\\hat{K}}$ then   \n8: $\\sigma_{\\mathrm{high}}=\\sigma_{\\mathrm{mid}}$   \n9: else   \n10: $\\sigma_{\\mathrm{low}}=\\sigma_{\\mathrm{mid}}$   \n11: end if   \n12: end while   \nAlgorithm 5 Langevin Unlearning: find the least unlearn step $K$ that satisfies the target \u03f5\u02c6   \n1: Input:target \u03f5\u02c6, $\\sigma$   \n2: Initialize $K=1,\\epsilon>\\hat{\\epsilon}$   \n3: while $\\epsilon>\\hat{\\epsilon}$ do   \n4: \u03f5 = min\u03b1>1[exp(\u22122\u03b1KC\u03c32\u03b7 ) 4m\u03b1S\u03c322nM2 2 + lo\u03b1g\u2212( \u03b411 ) ]   \n5: K = K + 1   \n6: end while   \n7: Return K ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "The $\\sigma$ found is reported in the table below: ", "text_level": 1, "page_idx": 28}, {"type": "table", "img_path": "3LKuC8rbyV/tmp/bac824f91c97bcfcb35807d3d8fece5a56ea1f331b4e3801cacd32e509b7874c.jpg", "table_caption": ["Table 3: The $\\sigma$ found with different target \u03f5\u02c6 "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "M.5 Implementation Details for Fig. 3b ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In this experiment, we fix the target $\\hat{\\epsilon}=1$ , we set the total number of data removal as 100. We show the accumulated unlearning steps w.r.t. the number of data removed. We first train the methods from scratch to get the initial weight $\\mathbf{w}_{\\mathrm{0}}$ , and sequentially remove data step by step until all the data points are removed. We count the accumulated unlearning steps $K$ needed in the process. ", "page_idx": 28}, {"type": "text", "text": "\u2022 For D2D, According to the original paper, only one data point could be removed a time. We calculate the least required steps and the noise to be added according to Theorem. N.2. ", "page_idx": 28}, {"type": "text", "text": "\u2022 For Langevin unlearning, we fix the $\\sigma=0.03$ , and we let the model unlearn [5, 10, 20] per time thanks to our theory. We obtain the least required unlearning steps for each removal operation $K_{\\mathrm{list}}$ following corollary. 3.4. The pseudo code is shown in Algorithm. 6. ", "page_idx": 28}, {"type": "text", "text": "Algorithm 6 Langevin Unlearning: find the least unlearn step $K$ in sequential settings ", "page_idx": 29}, {"type": "text", "text": "1: Input:target $\\hat{\\epsilon}$ , $\\sigma$ , total removal $S$ , removal batch size $b$ per time   \n2: $\\bar{K_{\\mathrm{list}}}=\\mathrm{I}$   \n3: for i in range $(S/b)$ do   \n4: Initialize $K_{\\mathrm{list}}[i-1]=1,\\epsilon>$   \n5: while $\\epsilon>\\hat{\\epsilon}$ do   \n6: $\\epsilon=\\mathrm{min}_{\\alpha>1}[\\varepsilon(\\alpha,\\sigma,b,i,K_{\\mathrm{list}})+\\mathrm{log}(1/\\delta)/(\\alpha-1)]$   \n7: $K_{\\mathrm{list}}[i-1]=K_{\\mathrm{list}}[i-1]+1$   \n8: end while   \n9: end for   \n10: Return $K_{\\mathrm{list}}$   \nAlgorithm 7 \u03b5(\u03b1, \u03c3, b, i, Klist)   \n1: Input:target $\\alpha,\\,\\sigma$ , removal batch size $b$ per time, $i$ -th removal in the sequence   \n23:: if $\\scriptstyle{\\mathrm{i}}==1$ trhn $\\begin{array}{r l}{\\mathrm{exp}(-\\frac{\\eta m K_{\\mathrm{list}}[0]}{\\alpha})\\times\\varepsilon_{0}(\\alpha,b,\\sigma)}&{{}}\\end{array}$   \n45:: elseReturn $\\begin{array}{r}{\\exp(-\\frac{\\eta m K_{\\mathrm{list}}(i-1)}{\\alpha})\\times\\frac{\\alpha-0.5}{\\alpha-1}(\\varepsilon_{0}(2\\alpha,b,\\sigma)+\\varepsilon(2\\alpha,\\sigma,b,i-1,K_{\\mathrm{list}}))}\\end{array}$   \n6: end if ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "table", "img_path": "3LKuC8rbyV/tmp/93e68b87cf152524729e660d7445bba817a281342075548c7f9cce430832427c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "M.6 Implementation Details for Fig. 3c ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In this study, we set the $\\sigma$ of the Langevin unlearning framework as $[0.05,0.1,0.2,0.5,1]$ . For each $\\sigma$ , we calculate the corresponding $\\epsilon_{0}$ . We train the Langevin unlearning framework from scratch to get the initial weight $\\mathbf{w}_{0}$ . Then we remove 100 data points from the dataset and unlearn the model. We here also call Algorithm. 5 to obtain the least required unlearning steps $K$ . ", "page_idx": 29}, {"type": "text", "text": "M.7 Implementation Details for Fig. 4 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In this study, we set different target $\\hat{\\epsilon}$ as $[0.5,1,2,5]$ and set different number of data to remove $S=[1,50,\\dot{1}00]$ . We train the Langevin unlearning framework from scratch to get the initial weight, then remove some data, unlearn the model and report the accuracy. We calculate the least required unlearning steps $K$ by again calling Algorithm. 5. ", "page_idx": 29}, {"type": "text", "text": "M.8 Additional experiments ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "N Unlearning Guarantee of Delete-to-Descent [8] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Theorem N.1 (Theorem 9 in [8], with internal non-private state). Assume for all $\\mathbf{d}\\in\\mathcal{X}$ , $f(x;\\mathbf{d})$ is $m$ -strongly convex, $M$ -Lipschitz and $L$ -smooth in $x$ . Define $\\begin{array}{r}{\\gamma=\\frac{L-m}{L+m}}\\end{array}$ and $\\begin{array}{r}{\\eta=\\frac{2}{L+m}}\\end{array}$ . Let the learning iteration $\\begin{array}{r}{T\\geq I+\\log(\\frac{2R m n}{2M})/\\log(1/\\gamma)}\\end{array}$ for PGD (Algorithm $^{\\,l}$ in $[8J]$ ) and the unlearning algorithm (Algorithm 2 in [8], PGD fine-tuning on learned parameters before adding Gaussian noise) run with $I$ iterations. Assume $\\epsilon=O(\\log(1/\\delta))$ , let the standard deviation of the output perturbation gaussian noise $\\sigma$ to be ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\sigma=\\frac{4\\sqrt{2}M\\gamma^{I}}{m n(1-\\gamma^{I})(\\sqrt{\\log(1/\\delta)+\\epsilon}-\\sqrt{\\log(1/\\delta))}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Then it achieves $(\\epsilon,\\delta)$ -unlearning for add/remove dataset adjacency. ", "page_idx": 29}, {"type": "image", "img_path": "3LKuC8rbyV/tmp/58060a9c3348a63f2b5d0ff7e12074bbd3c1be8ddc3b739bcb1fd638660ec4a4.jpg", "img_caption": ["Figure 5: (a) The utility results that correspond to Figure 4. Since $\\sigma$ is fixed the utility is roughly the same. (b) The privacy-utility tradeoff for unlearning one point restricting to one (or $K$ ) unlearning update on the Adult dataset. ", "(a) $S$ v.s. Accuracy ", "(b) Unlearn one point on the Adult dataset "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "Theorem N.2 (Theorem 28 in [8], without internal non-private state). Assume for all $\\mathbf{d}\\in\\mathcal{X}$ , $f(x;\\mathbf{d})$ is $m$ -strongly convex, $M$ -Lipschitz and $L$ -smooth in $x$ . Define $\\begin{array}{r}{\\gamma=\\frac{L-m}{L+m}}\\end{array}$ and $\\begin{array}{r}{\\eta=\\frac{2}{L+m}}\\end{array}$ . Let the learning iteration $\\begin{array}{r}{T\\geq I+\\log(\\frac{2R m n}{2M})/\\log(1/\\gamma)}\\end{array}$ for $P G D$ (Algorithm 1 in [8]) and the unlearning algorithm (Algorithm 2 in [8], PGD fine-tuning on learned parameters after adding Gaussian noise) run with $I+\\log(\\log(4d i/\\delta))/\\log(1/\\gamma)$ iterations for the $i^{\\bar{t}h}$ sequential unlearning request, where $I$ satisfies ", "page_idx": 30}, {"type": "equation", "text": "$$\nI\\geq\\frac{\\log\\left(\\frac{\\sqrt{2d}(1-\\gamma)^{-1}}{\\sqrt{2\\log(2/\\delta)+\\epsilon}-\\sqrt{2\\log(2/\\delta)}}\\right)}{\\log(1/\\gamma)}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Assume $\\epsilon=O(\\log(1/\\delta))$ , let the standard deviation of the output perturbation gaussian noise $\\sigma$ to be ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\sigma=\\frac{8M\\gamma^{I}}{m n(1-\\gamma^{I})(\\sqrt{2\\log(2/\\delta)+3\\epsilon}-\\sqrt{2\\log(2/\\delta)+2\\epsilon})}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Then it achieves $(\\epsilon,\\delta)$ -unlearning for add/remove dataset adjacency. ", "page_idx": 30}, {"type": "text", "text": "Note that the privacy guarantee of D2D [8] is with respect to add/remove dataset adjacency and ours is the replacement dataset adjacency. However, by a slight modification of the proof of Theorem N.1 and N.2, one can show that a similar (but slightly worse) bound of the theorem above also holds for D2D [8]. For simplicity and fair comparison, we directly use the bound in Theorem N.1 and N.2 in our experiment. Note that [52] also compares a special replacement DP with standard add/remove DP, where a data point can only be replaced with a null element in their definition. In contrast, our replacement data adjacency allows arbitrary replacement which intuitively provides a stronger privacy notion. ", "page_idx": 30}, {"type": "text", "text": "The non-private internal state of D2D. There are two different versions of the D2D algorithm depending on whether one allows the server (model holder) to save and leverage the model parameter before adding Gaussian noise. The main difference between Theorem N.1 and N.2 is whether their unlearning process starts with the \u201cclean\u201d model parameter (Theorem N.1) or the noisy model parameter (Theorem N.2). Clearly, allowing the server to keep and leverage the non-private internal state provides a weaker notion of privacy [8]. In contrast, our Langevin unlearning approach by default only keeps the noisy parameter so that we do not save any non-private internal state. As a result, one should compare Langevin unlearning to D2D with Theorem N.2 for a fair comparison. ", "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: All claims are provided with corresponding theorems (i.e., Theorem 3.2 for unlearning guarantees) and we demonstrate superior privacy-utility-complexity tradeoff via experiments in Section 4. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: We provide a detailed discussion on the limitation of our work in Section A. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 31}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: All assumptions are clearly stated in our theorems, see Theorem 3.2 and 3.3 for instance. The proofs for every theoretical statements are provided in Appendix F to L. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We provide the experiment code in our supplementary materials and details about our settings in Section M. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 32}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We provide the experiment code in our supplementary materials and all data used in our experiments are cited with public access. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 33}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: All experimental details are included in Appendix M. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 33}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: All results are provided with 1 standard deviation error bar, which is gathered over 100 independent trials. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The compute resources used for our experiments are detailed in Appendix M.3. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute worker CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 34}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The authors have read the NeurIPS Code of Ethics and affirm that this work conforms with it in every respect. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 34}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We briefly discuss why our work does not have any negative societal impacts in B. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 34}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 35}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 35}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: All datasets used in our experiments are properly cited with proper open access license, see Section 4. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 36}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 37}]