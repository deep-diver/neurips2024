[{"figure_path": "6ZBHIEtdP4/figures/figures_1_1.jpg", "caption": "Figure 1: The comparison among Full Fine-tuning, training with LoRA, and PiSSA. In this visualization, blue modules represent parts of the model whose parameters are frozen during training, while orange modules indicate components that require updates. QLoRA quantizes the pretrained matrix in LoRA to 4-bit, whereas QPiSSA quantizes the residual matrix in PiSSA.", "description": "This figure compares three different fine-tuning methods for large language models: full fine-tuning, LoRA, and PiSSA.  Full fine-tuning updates all model parameters. LoRA updates a low-rank approximation of the weight changes, freezing the original weights. PiSSA, similar in architecture to LoRA, initializes its update matrices with the principal components of the original weight matrix and freezes the residual components. The figure visually represents the different parameter update schemes, highlighting the frozen and updated parts of the model in each method.  It also shows how quantization affects LoRA and PiSSA (QLoRA and QPiSSA).", "section": "1 Introduction"}, {"figure_path": "6ZBHIEtdP4/figures/figures_2_1.jpg", "caption": "Figure 2: We illustrate the two key advantages of PiSSA: converging faster and better, and reducing quantization error. In the left figure, we use a toy example to show PiSSA's faster convergence, where we first train a two-layer MLP classifying odd numbers of MNIST, and then fine-tune the model on even numbers. PiSSA finds the right direction more quickly and achieves a lower loss with the same number of steps. In the right figure, PiSSA reduces quantization error more effectively than LoftQ [14], with an optional 5-iteration SVD for further error reduction, as detailed in Appendix E.", "description": "This figure demonstrates two key advantages of PiSSA over other methods. The left plot shows PiSSA's faster convergence to a lower loss compared to LoRA in a toy example. The right plot illustrates that PiSSA reduces quantization errors significantly better than LoftQ, especially when combined with a 5-iteration SVD.", "section": "1 Introduction"}, {"figure_path": "6ZBHIEtdP4/figures/figures_2_2.jpg", "caption": "Figure 2: We illustrate the two key advantages of PiSSA: converging faster and better, and reducing quantization error. In the left figure, we use a toy example to show PiSSA's faster convergence, where we first train a two-layer MLP classifying odd numbers of MNIST, and then fine-tune the model on even numbers. PiSSA finds the right direction more quickly and achieves a lower loss with the same number of steps. In the right figure, PiSSA reduces quantization error more effectively than LoftQ [14], with an optional 5-iteration SVD for further error reduction, as detailed in Appendix E.", "description": "This figure demonstrates two main advantages of PiSSA over other methods. The left subplot shows PiSSA's faster convergence speed by comparing the loss curves of PiSSA and LoRA in a simple classification task. The right subplot showcases PiSSA's superior performance in reducing quantization error compared to LoftQ, especially when using a 5-iteration SVD.", "section": "Experiments"}, {"figure_path": "6ZBHIEtdP4/figures/figures_5_1.jpg", "caption": "Figure 3: Visualizations of LLaMA 2-7B's \"layers[0].self_attn.q_proj\" matrix, with distributions for the full model shown in Appendix G. Figures (a), (b), (d), and (e) display the singular values of W, Wres, W - nf4(W), and Wres \u2013 nf4(Wres), respectively. Figures (c) and (f) show the data distributions of W and Wres.", "description": "This figure visualizes the singular value decomposition of the query projection matrix (W) from the first self-attention layer of LLaMA 2-7B and its components after applying PiSSA.  It shows the singular values of the original matrix (W), the residual matrix after PiSSA (Wres), the quantization error matrices for QLoRA and QPiSSA, and the data distributions for W and Wres. This visualization is used to illustrate the impact of PiSSA on reducing quantization error by demonstrating that the residual matrix (Wres) has a narrower distribution than the original matrix (W), making it more suitable for quantization.", "section": "Experiments"}, {"figure_path": "6ZBHIEtdP4/figures/figures_6_1.jpg", "caption": "Figure 4: The loss, grad norm, and evaluation accuracy over the training steps of LoRA (indicated in blue), PiSSA (in orange), and full parameter fine-tuning (in red).", "description": "This figure compares the training performance of LoRA, PiSSA, and full fine-tuning methods.  The plots show the training loss, gradient norm, and accuracy on the GSM8K benchmark over training steps.  It visually demonstrates that PiSSA converges faster and achieves better accuracy compared to LoRA and is closer to the performance of full fine-tuning, which uses significantly more parameters.", "section": "5.2 Experiments using Full Data and More Epochs"}, {"figure_path": "6ZBHIEtdP4/figures/figures_7_1.jpg", "caption": "Figure 5: The loss, grad norm, and evaluation accuracy over the training steps of (Q)LoRA, (Q)PiSSA, LoftQ and full parameter fine-tuning.", "description": "This figure compares the training performance of QLoRA, QPiSSA, LoftQ, and full fine-tuning methods across different metrics (loss, gradient norm, and GSM8K accuracy).  The plots show how these different methods converge over training steps, highlighting the relative speed and performance of each approach.  The comparison includes both quantized (Q) and unquantized versions to illustrate the impact of quantization on the fine-tuning process.", "section": "5.3 Conducting 4-bit Quantization Experiments"}, {"figure_path": "6ZBHIEtdP4/figures/figures_8_1.jpg", "caption": "Figure 6: Comparison of (Q)PiSSA and (Q)LoRA across models from 7B to 70B.", "description": "This figure compares the performance of PiSSA and LoRA, as well as their quantized versions QPiSSA and QLoRA, across nine different large language models ranging in size from 7 billion to 70 billion parameters.  The models were fine-tuned on the MetaMathQA-100K and CodeFeedback-100K datasets and then evaluated on the GSM8K and HumanEval benchmarks.  The bar chart visually represents the accuracy achieved by each method on each model.  The results demonstrate a consistent advantage for PiSSA/QPiSSA across various model sizes and types.", "section": "5.4 Experiments Across Various Sizes and Types of Models"}, {"figure_path": "6ZBHIEtdP4/figures/figures_9_1.jpg", "caption": "Figure 7: The comparison among (Q)LoRA, (Q)PiSSA, LoftQ, and full fine-tuning across ranks.", "description": "This figure compares the performance of QLoRA, QPiSSA, LoftQ, and full fine-tuning across different ranks.  Subfigures (a) to (d) show the quantization error reduction ratio, training loss, GSM8K accuracy, and MATH accuracy respectively.  The results show that PiSSA and QPiSSA generally outperform other methods, especially at lower ranks. However, at higher ranks, PiSSA's performance might decrease slightly, suggesting potential over-parameterization.", "section": "5.5 Experiments on Various Ranks"}, {"figure_path": "6ZBHIEtdP4/figures/figures_20_1.jpg", "caption": "Figure 8: Initializing with principal, medium, and minor singular values and vectors, the training loss on the MetaMathQA and the accuracy on the GSM8K and MATH validation sets are reported, respectively, for three models.", "description": "This figure shows the results of initializing the adapters in three different large language models (LLaMA-2-7B, Mistral-7B, Gemma-7B) with principal, middle and minor singular values and vectors. The results are evaluated on three different benchmarks: MetaMathQA (training loss), GSM8K (accuracy), and MATH (accuracy).  It demonstrates that using principal singular values and vectors leads to the best performance across all three models.", "section": "5.4 Experiments Across Various Sizes and Types of Models"}, {"figure_path": "6ZBHIEtdP4/figures/figures_20_2.jpg", "caption": "Figure 3: Visualizations of LLaMA 2-7B's \u201clayers[0].self_attn.q_proj\u201d matrix, with distributions for the full model shown in Appendix G. Figures (a), (b), (d), and (e) display the singular values of W, Wres, W - nf4(W), and Wres \u2013 nf4(Wres), respectively. Figures (c) and (f) show the data distributions of W and Wres.", "description": "This figure visualizes the singular value decomposition of the query projection matrix (W) from the first self-attention layer of the LLaMA 2-7B model and its decomposition into principal and residual components. It compares the singular value distributions of the original matrix (W), the residual matrix (Wres), and the quantization errors using QLORA and QPISSA methods.  The figure shows that QPiSSA results in a smaller quantization error because the residual matrix (Wres) has a narrower distribution.", "section": "Experiments"}, {"figure_path": "6ZBHIEtdP4/figures/figures_21_1.jpg", "caption": "Figure 3: Visualizations of LLaMA 2-7B's \"layers[0].self_attn.q_proj\" matrix, with distributions for the full model shown in Appendix G. Figures (a), (b), (d), and (e) display the singular values of W, Wres, W - nf4(W), and Wres \u2013 nf4(Wres), respectively. Figures (c) and (f) show the data distributions of W and Wres.", "description": "This figure visualizes the singular value decompositions and data distributions of different matrices related to the LLaMA 2-7B model's self-attention query projection layer.  It compares the original weight matrix (W), its quantized version (nf4(W)), the residual matrix after applying PiSSA (Wres), and the resulting error matrices for QLoRA and QPiSSA, providing a visual demonstration of how PiSSA reduces quantization errors by focusing on the principal singular values and vectors.", "section": "5 Experiments"}, {"figure_path": "6ZBHIEtdP4/figures/figures_22_1.jpg", "caption": "Figure 3: Visualizations of LLaMA 2-7B's \"layers[0].self_attn.q_proj\" matrix, with distributions for the full model shown in Appendix G. Figures (a), (b), (d), and (e) display the singular values of W, Wres, W - nf4(W), and Wres \u2013 nf4(Wres), respectively. Figures (c) and (f) show the data distributions of W and Wres.", "description": "This figure visualizes the singular value decompositions of the original weight matrix (W) and the residual matrix (Wres) from a LLaMA 2-7B model's self-attention layer.  It also shows the distribution of values for these matrices and the quantization errors from QLORA and QPISSA methods. The figure shows that the residual matrix has a narrower value distribution than the original matrix and exhibits a smaller quantization error when using PiSSA.", "section": "5 Experiments"}, {"figure_path": "6ZBHIEtdP4/figures/figures_23_1.jpg", "caption": "Figure 4: The loss, grad norm, and evaluation accuracy over the training steps of LoRA (indicated in blue), PiSSA (in orange), and full parameter fine-tuning (in red).", "description": "This figure compares the training performance of LoRA, PiSSA, and full fine-tuning methods over training steps.  Three subplots are shown: training loss, gradient norm, and accuracy on the GSM8K benchmark.  PiSSA demonstrates faster convergence and higher accuracy than LoRA, while full fine-tuning shows signs of overfitting due to its use of many more trainable parameters.", "section": "5.2 Experiments using Full Data and More Epochs"}, {"figure_path": "6ZBHIEtdP4/figures/figures_23_2.jpg", "caption": "Figure 4: The loss, grad norm, and evaluation accuracy over the training steps of LoRA (indicated in blue), PiSSA (in orange), and full parameter fine-tuning (in red).", "description": "This figure shows a comparison of the training loss, gradient norm, and accuracy on the GSM8K benchmark across three different fine-tuning methods: LoRA, PiSSA, and full fine-tuning.  It illustrates that PiSSA converges faster and achieves a lower loss compared to LoRA, while maintaining performance comparable to full fine-tuning. The gradient norm for PiSSA shows a trend similar to full fine-tuning, unlike the behavior of LoRA which starts with near-zero gradient and slowly increases, suggesting that PiSSA is more effectively utilizing the gradient information during training.", "section": "5.2 Experiments using Full Data and More Epochs"}, {"figure_path": "6ZBHIEtdP4/figures/figures_24_1.jpg", "caption": "Figure 14: Comparison of quantization errors in QLoRA, LoftQ, and PiSSA across k_proj, v_proj, o_proj and gate_proj, up_proj, down_proj layers.", "description": "This figure compares the quantization error reduction ratios achieved by QLoRA, LoftQ, and PiSSA across different types of linear layers within a transformer model.  It displays the error reduction for six different types of layers (\"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", and \"down_proj\") at varying ranks (1, 2, 4, 8, 16, 32, 64, 128).  The results visually show PiSSA's superior performance in reducing quantization error compared to QLoRA and LoftQ across all layer types and ranks.", "section": "K.1 Quantization Error for More Type of Layers"}, {"figure_path": "6ZBHIEtdP4/figures/figures_25_1.jpg", "caption": "Figure 3: Visualizations of LLaMA 2-7B's \u201clayers[0].self_attn.q_proj\u201d matrix, with distributions for the full model shown in Appendix G. Figures (a), (b), (d), and (e) display the singular values of W, Wres, W - nf4(W), and Wres \u2013 nf4(Wres), respectively. Figures (c) and (f) show the data distributions of W and Wres.", "description": "This figure visualizes the singular values and data distributions of the original weight matrix (W), the residual matrix (Wres), and the quantization errors for LLaMA 2-7B's self-attention query projection layer.  It highlights the narrower distribution and reduced magnitude of singular values in the residual matrix Wres after applying singular value decomposition, which is a key component of the PiSSA method. The reduced magnitude explains why quantizing Wres (QPiSSA) leads to lower quantization errors than quantizing the full matrix W (QLoRA).", "section": "Experiments"}, {"figure_path": "6ZBHIEtdP4/figures/figures_26_1.jpg", "caption": "Figure 7: The comparison among (Q)LoRA, (Q)PiSSA, LoftQ, and full fine-tuning across ranks.", "description": "This figure compares the performance of four different fine-tuning methods\u2014(Q)LoRA, (Q)PiSSA, LoftQ, and full fine-tuning\u2014across various ranks.  The four subfigures show the quantization error reduction ratio, training loss, GSM8K accuracy, and MATH accuracy, respectively, for each method and rank.  The results illustrate the performance advantages of PiSSA and QPiSSA, especially at lower ranks, and demonstrate their ability to match or exceed the performance of full fine-tuning in certain scenarios.", "section": "5.5 Experiments on Various Ranks"}, {"figure_path": "6ZBHIEtdP4/figures/figures_26_2.jpg", "caption": "Figure 7: The comparison among (Q)LoRA, (Q)PiSSA, LoftQ, and full fine-tuning across ranks.", "description": "This figure compares the performance of QLoRA, QPiSSA, LoftQ, and full fine-tuning across different ranks.  It visualizes the quantization error reduction ratio, the training loss, and the accuracy on GSM8K and MATH datasets.  The results demonstrate the effectiveness of PiSSA and QPiSSA in reducing quantization error and achieving higher accuracy compared to other methods, especially at lower ranks.", "section": "5.5 Experiments on Various Ranks"}, {"figure_path": "6ZBHIEtdP4/figures/figures_28_1.jpg", "caption": "Figure 18: Comparison of Loss and Ratio to the target A and target B for LoRA and PiSSA across the initial 5 steps.", "description": "This figure compares the performance of LoRA and PiSSA over the first five training steps.  The leftmost panel shows the training loss for each method. The remaining panels show the progress towards the final parameter values for matrices A and B (after 50 steps) as a percentage of the total distance from the initial parameter values. PiSSA demonstrates faster convergence towards the target parameters.", "section": "M Comparison of Initial Gradient Subspaces"}]