[{"figure_path": "uCZI8gSfD4/tables/tables_2_1.jpg", "caption": "Table 1: The Pre-training data, aggregates various public sources and specifies sampling proportions for a single epoch of training on 194 billion unique amino acids.", "description": "This table presents the composition of the pre-training dataset UniMeta200B. It lists four datasets used: Uniref50/S, Uniref90/50, ColabFoldDBC, and ColabFoldDBm.  For each dataset, the number of protein sequences (\"Prot. Seq.\"), the number of amino acid tokens (\"Tokens (AAs)\"), and the sampling proportion (\"Samp. Prop.\") within the dataset are provided. The total number of protein sequences and tokens are also given at the end.", "section": "2 Expanding Diversified Metagenomic Data"}, {"figure_path": "uCZI8gSfD4/tables/tables_3_1.jpg", "caption": "Table 2: Coefficient of Equation 1.", "description": "This table presents the coefficients obtained from fitting the power-law scaling equations for both Causal Language Model (CLM) and Masked Language Model (MLM).  The parameters \u03b1 and \u03b2 represent the exponents of the power-laws describing the relationship between compute budget (C) and model size (N) and data size (D), respectively. A and B are scaling constants for the model size and data size equations. These coefficients show the different scaling behaviors of CLM and MLM in terms of the growth of model size and data size with respect to increasing compute budget.", "section": "3 Parameters and Datasize Optimal Allocation"}, {"figure_path": "uCZI8gSfD4/tables/tables_4_1.jpg", "caption": "Table 3: Coefficient of Equation 2", "description": "This table presents the coefficients obtained from fitting the power-law relation defined in Equation 2, which describes the scaling relationship between loss (L(x)), model size (N), compute budget (C), and training dataset tokens (D). The table shows separate coefficients for CLM and MLM objectives.", "section": "3.1 Scaling laws for CLM and MLM"}, {"figure_path": "uCZI8gSfD4/tables/tables_6_1.jpg", "caption": "Table 4: Coefficients for L(Cs) and L(Ct)", "description": "This table presents the coefficients obtained from fitting the power-law equations for the loss in transfer learning (L(Ct)) and training from scratch (L(Cs)) for both MLM and CLM objectives.  The coefficients (As, \u03b1s, Bt, \u03b1t) are used in the equations L(Cs) = As \u00d7 Cs^\u03b1s and L(Ct) = Bt \u00d7 Ct^\u03b1t, which quantify how the loss changes with compute budget (C) for each objective. These coefficients help in understanding the relative effectiveness of training from scratch versus transfer learning for different model sizes and objectives.", "section": "4.1 Transferability"}, {"figure_path": "uCZI8gSfD4/tables/tables_8_1.jpg", "caption": "Table 5: Model architecture details. We compare popular models PROGEN2 and ESM-2 using similar FLOPs with our models estimated by proposed scaling law.", "description": "This table presents the architectural details of several protein language models, including PROGEN2-xlarge, ESM-2, and models developed by the authors.  It compares models with similar FLOPs (floating point operations) to highlight the trade-offs between model size, training tokens, and computational efficiency achieved by the authors' proposed scaling laws.  The table lists the number of parameters, the objective function (CLM or MLM), the number of attention heads, the dimension of the hidden layer, the number of layers, the number of training tokens, and the total FLOPs for each model.", "section": "5 Experimental Validation"}, {"figure_path": "uCZI8gSfD4/tables/tables_9_1.jpg", "caption": "Table 6: Tasks performance of MLM Model on the test dataset with LoRA fine-tuning.", "description": "This table presents the performance of Masked Language Models (MLMs) on various downstream tasks after fine-tuning with Low-Rank Adaptation (LoRA).  It compares the performance of a 3B parameter ESM-2 model and a 10.7B parameter model trained using the methods described in the paper.  It also includes results for smaller 470M parameter models, one trained from scratch and one using transfer learning from a pre-trained Causal Language Model (CLM), highlighting the impact of model size and transfer learning techniques on performance. The tasks evaluated include Contact Prediction, Fold Classification, and Fluorescence prediction.", "section": "5 Experimental Validation"}, {"figure_path": "uCZI8gSfD4/tables/tables_15_1.jpg", "caption": "Table 6: Tasks performance of MLM Model on the test dataset with LoRA fine-tuning.", "description": "This table presents the performance of the MLM model (both the 10.7B parameter model trained with the proposed scaling laws and a 3B parameter ESM-2 model) on various downstream tasks after fine-tuning with LoRA.  The tasks include contact prediction (P@L/5), fold classification (1195 classes), and fluorescence (regression).  The results show how the proposed method compares to a well-established model in protein language modeling.", "section": "5.2 Protein understanding tasks: 10.7B MLM vs. 3B ESM2"}, {"figure_path": "uCZI8gSfD4/tables/tables_19_1.jpg", "caption": "Table 5: Model architecture details. We compare popular models PROGEN2 and ESM-2 using similar FLOPs with our models estimated by proposed scaling law.", "description": "This table provides a comparison of the architecture details for several protein language models, including PROGEN2-xlarge, ESM-2, and the models developed by the authors of the paper.  The comparison is based on similar FLOPS (floating point operations) counts, which reflects computational costs. The table shows the number of parameters, the objective function used during training (CLM or MLM), the number of attention heads, the embedding dimension, the number of layers, the number of training tokens, and the total FLOPS.  The authors' models were sized based on the scaling laws developed and described in their paper.  This comparison allows the reader to understand the relative sizes and computational costs of these various models.", "section": "5 Experimental Validation"}, {"figure_path": "uCZI8gSfD4/tables/tables_23_1.jpg", "caption": "Table A9: Coefficient of Equation 8", "description": "This table presents the coefficients derived from fitting Equation 8, a combined power-law model, to the data for both CLM and MLM objectives.  The equation aims to capture the relationship between model size (N), training data size (D), and loss (L). Coefficients A, B, \u03b1, and \u03b2 represent parameters in the power-law model, providing insights into the scaling behavior of protein language models with different objectives.  The table quantifies the relative contributions of model size and data size to the overall loss, which is crucial for optimizing the training process given limited computational resources.", "section": "J Combined Power-law"}, {"figure_path": "uCZI8gSfD4/tables/tables_25_1.jpg", "caption": "Table 5: Model architecture details. We compare popular models PROGEN2 and ESM-2 using similar FLOPs with our models estimated by proposed scaling law.", "description": "This table presents the architecture details of various protein language models, including the number of parameters, hidden dimension, number of layers, number of attention heads, and FLOPs. It compares the popular models PROGEN2 and ESM-2 with the models proposed in the paper. The FLOPs of the proposed models are estimated based on the scaling laws proposed in the paper.", "section": "5 Experimental Validation"}]