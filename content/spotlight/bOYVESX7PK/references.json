{"references": [{"fullname_first_author": "Arthur Jacot", "paper_title": "Neural tangent kernel: Convergence and generalization in neural networks", "publication_date": "2018-00-00", "reason": "This paper is foundational for understanding the relationship between neural network training dynamics and their generalization performance."}, {"fullname_first_author": "Jaehoon Lee", "paper_title": "Wide neural networks of any depth evolve as linear models under gradient descent", "publication_date": "2019-00-00", "reason": "This paper provides insights into the dynamics of wide neural networks and how they can be analyzed using linear models."}, {"fullname_first_author": "Guy Gur-Ari", "paper_title": "Gradient descent happens in a tiny subspace", "publication_date": "2018-00-00", "reason": "This paper introduces the idea that gradient descent optimization occurs in a low-dimensional subspace of the parameter space, influencing training dynamics."}, {"fullname_first_author": "Lenaic Chizat", "paper_title": "On lazy training in differentiable programming", "publication_date": "2019-00-00", "reason": "This paper explores the concept of lazy training in neural networks, which highlights the relationship between model parameters and training dynamics."}, {"fullname_first_author": "Bernard O Koopman", "paper_title": "Hamiltonian systems and transformation in Hilbert space", "publication_date": "1931-00-00", "reason": "This foundational paper introduces the Koopman operator, which is central to the methods used in the current paper for analyzing dynamical systems."}]}