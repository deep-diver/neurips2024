[{"figure_path": "seAuMedrm5/tables/tables_5_1.jpg", "caption": "Table 1: Settings used with each dataset: LibriSpeech, Voice Search, and YouTube.", "description": "This table shows the settings used for each dataset in the experiments. It includes the number of log-mel features, 2D convolutional layers, encoder dimension, number of encoder layers, number of encoder parameters, LSTM size, and vocabulary size for LibriSpeech, Voice Search, and YouTube datasets.", "section": "4.2 Neural Networks Specifications"}, {"figure_path": "seAuMedrm5/tables/tables_5_2.jpg", "caption": "Table 2: WER (%) on the Voice Search test sets. VS: Main Test, and rare-words RM: Maps, RN: News, RQ: Search Queries", "description": "This table presents the Word Error Rate (WER) results for different models on the Voice Search dataset, broken down by four subsets: Main Test, and rare-word sets for Maps, News, and Search Queries.  The models compared are RNN-T, Aligner, CTC, and Non-AR Aligner.  The WER is a metric for measuring the accuracy of Automatic Speech Recognition (ASR) systems, with lower values indicating better performance.  This table highlights the performance of the Aligner model in comparison to state-of-the-art baselines on a real-world dataset of voice search queries.", "section": "4.3 Base Model Results"}, {"figure_path": "seAuMedrm5/tables/tables_5_3.jpg", "caption": "Table 3: WER (%) on LibriSpeech.", "description": "This table presents the Word Error Rate (WER) achieved by different models on the LibriSpeech dataset.  The models compared are CTC, RNN-T, AED, and the proposed Aligner model.  WER is shown for three subsets of the LibriSpeech test set: DEV, TEST-CLEAN, and TEST-OTHER, representing different levels of difficulty. Lower WER values indicate better performance.", "section": "4.3 Base Model Results"}, {"figure_path": "seAuMedrm5/tables/tables_6_1.jpg", "caption": "Table 4: WER (%) on YouTube long-form test set.", "description": "This table presents the Word Error Rate (WER) results for RNN-T and Aligner models on the YouTube long-form test set.  It compares the performance of both models using a 15-second segmented approach and an unsegmented approach. The unsegmented approach tests the models' ability to handle long audio sequences without dividing them into segments.  The results show comparable performance (7.6% WER) for both models when using the 15-second segmented approach, but RNN-T shows an improvement in performance with the unsegmented approach (6.8% WER), whereas Aligner still shows acceptable performance with 7.3% WER in the unsegmented setting.", "section": "4.4 Long-Form Recognition"}, {"figure_path": "seAuMedrm5/tables/tables_9_1.jpg", "caption": "Table 5: Example measured compute times for our LibriSpeech models (lower is better).", "description": "This table compares the training and inference time of three different models (AED, RNN-T, and Aligner) on the LibriSpeech dataset.  It shows a breakdown of the computation time during training (including encoder and decoder+loss) and during inference (including encoding and decoding). The Aligner model demonstrates significantly faster inference time compared to the other two models, showcasing its computational efficiency.", "section": "4.6 Computational Efficiency"}, {"figure_path": "seAuMedrm5/tables/tables_14_1.jpg", "caption": "Table 6: LibriSpeech common training settings.", "description": "This table lists the common hyperparameter settings used for training the different models (Aligner, RNN-T, AED) on the LibriSpeech dataset.  It includes parameters such as learning rate, optimizer, regularization, batch size, and other training details specific to the Conformer encoder architecture used in the experiments.  The table helps to clarify the consistency and comparability of the experimental setup across the models.", "section": "A.1 LibriSpeech - Expanded Settings and Results"}, {"figure_path": "seAuMedrm5/tables/tables_14_2.jpg", "caption": "Table 7: WER (%) on LibriSpeech Test-Clean set by utterance duration, including models trained with concatenated training examples covering up to the maximum test length of 36s. The number of test utterances in each category is 2466, 89, and 65, in order of length.", "description": "This table presents the Word Error Rate (WER) results on the LibriSpeech Test-Clean dataset, broken down by utterance length.  It compares different ASR models (CTC, RNN-T, AED, Aligner, and their concatenated versions) across three utterance length categories: <17 seconds, 17-21 seconds, and >21 seconds. The table shows that the Aligner model struggles significantly with longer utterances, but this issue is alleviated by concatenating training examples.", "section": "4.3 Base Model Results"}, {"figure_path": "seAuMedrm5/tables/tables_14_3.jpg", "caption": "Table 8: WER (%) on LibriSpeech Test-Other set by utterance duration, including models trained with concatenated training examples covering up to the maximum test length of 36s. The number of test utterances in each category is 2834, 70, and 35, in order of length.", "description": "This table presents the Word Error Rate (WER) results for the LibriSpeech Test-Other dataset, broken down by utterance length categories (<17s, 17-21s, >21s).  It compares the performance of various models (CTC, RNN-T, AED, Aligner, and their concatenated versions) showing the WER for each category and the overall performance. The concatenation methods aim to improve the performance on longer utterances.", "section": "4.3 Base Model Results"}]