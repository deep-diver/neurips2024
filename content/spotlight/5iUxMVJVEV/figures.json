[{"figure_path": "5iUxMVJVEV/figures/figures_1_1.jpg", "caption": "Figure 1: Multi-periodicity, inclusion of periodic components, and Periodic Pyramid.", "description": "This figure illustrates the concept of multi-periodicity in time series, where variations occur at multiple periodic levels (e.g., yearly, monthly, weekly, daily).  It shows how these variations are not independent but rather exhibit inclusion relationships, with longer periods encompassing shorter ones. This is represented as a periodic pyramid structure, where the original time series is at the top level, and subsequent levels consist of periodic components with gradually shorter periods. This pyramid-like structure helps to explicitly represent the complex relationships between periodic variations within the time series.", "section": "1 Introduction"}, {"figure_path": "5iUxMVJVEV/figures/figures_3_1.jpg", "caption": "Figure 2: Model architecture. PPAM denotes Periodic Pyramid Attention Mechanism.", "description": "This figure illustrates the architecture of the Peri-midFormer model. The model starts by taking the original time series as input and then decomposing it using FFT into multiple periodic components of varying lengths. These components are organized into a periodic pyramid structure, with longer periods encompassing shorter ones.  The periodic pyramid is then fed into a Peri-midFormer module which uses a Periodic Pyramid Attention Mechanism (PPAM) to capture complex temporal variations by computing self-attention between periodic components. Finally, depending on the downstream task (classification or reconstruction tasks), different strategies are employed to aggregate the features from the pyramid and produce the final output. ", "section": "3 Methodology"}, {"figure_path": "5iUxMVJVEV/figures/figures_4_1.jpg", "caption": "Figure 1: Multi-periodicity, inclusion of periodic components, and Periodic Pyramid.", "description": "This figure illustrates the concept of multi-periodicity in time series, where multiple periodic components with different periods exist and overlap with each other.  The inclusion relationships between components are shown, where longer periods contain shorter periods. The figure introduces the \"Periodic Pyramid\" structure, where the top level represents the original time series, and lower levels represent periodic components with gradually shorter periods.  This pyramid-like structure explicitly shows the implicit multi-period relationships within a time series.", "section": "1 Introduction"}, {"figure_path": "5iUxMVJVEV/figures/figures_5_1.jpg", "caption": "Figure 2: Model architecture. PPAM denotes Periodic Pyramid Attention Mechanism.", "description": "This figure illustrates the overall architecture of the Peri-midFormer model. It starts with time embedding of the original time series.  Then, it uses the FFT to decompose it into multiple periodic components of varying lengths across different levels, forming the Periodic Pyramid. Each component is treated as an independent token and receives positional embedding.  Next, the Periodic Pyramid is fed into Peri-midFormer, consisting of multiple layers for computing Periodic Pyramid Attention.  Finally, depending on the task, two strategies are employed. For classification, components are directly concatenated and projected into the category space; for reconstruction tasks, features from different pyramid branches are integrated through Periodic Feature Flows Aggregation to generate the final output.", "section": "3 Methodology"}, {"figure_path": "5iUxMVJVEV/figures/figures_6_1.jpg", "caption": "Figure 5: Model performance comparison.", "description": "This radar chart compares the performance of Peri-midFormer against other state-of-the-art time series analysis methods across five common tasks: long-term forecasting, short-term forecasting, imputation, classification, and anomaly detection.  Each axis represents a task, and the distance from the center to a point on a line indicates the performance (lower MSE for forecasting and imputation, higher accuracy for classification, and higher F1-score for anomaly detection) on that specific task. Peri-midFormer demonstrates superior performance across all five tasks, consistently outperforming other methods.", "section": "4.1 Main Results"}, {"figure_path": "5iUxMVJVEV/figures/figures_7_1.jpg", "caption": "Figure 6: Model comparison in classification. The results are averaged from 10 subsets of UEA.", "description": "This figure shows a bar chart comparing the average classification accuracy of Peri-midFormer and various baseline models across 10 subsets of the UEA benchmark dataset.  The chart visually represents the superior performance of Peri-midFormer compared to other methods, highlighting its effectiveness in time series classification tasks.", "section": "4.4 Time Series Classification"}, {"figure_path": "5iUxMVJVEV/figures/figures_9_1.jpg", "caption": "Figure 5: Model performance comparison.", "description": "This figure compares the performance of Peri-midFormer against other state-of-the-art time series analysis models across five benchmark tasks: long-term forecasting, short-term forecasting, classification, imputation, and anomaly detection.  Each task's performance is represented on a separate axis of a radar chart, enabling a visual comparison of the models' overall capabilities.  Peri-midFormer demonstrates superior performance in most cases, indicating its robustness and effectiveness across different tasks. The models' names are given as labels on the radar chart, and specific metrics (e.g., MSE, accuracy, F1-score) are specified for each axis to represent quantitative performance measurements.", "section": "4 Main Results"}, {"figure_path": "5iUxMVJVEV/figures/figures_13_1.jpg", "caption": "Figure 2: Model architecture. PPAM denotes Periodic Pyramid Attention Mechanism.", "description": "This figure illustrates the architecture of the Peri-midFormer model.  It shows the input time series undergoing normalization and decomposition into trend and seasonal components. The seasonal component is then processed using FFT to extract multiple periodic components at different levels, creating a Periodic Pyramid structure. These components are passed into the Peri-midFormer which uses a Periodic Pyramid Attention Mechanism (PPAM) to capture relationships between components at different levels. Finally, the processed features are used for downstream tasks using one of two strategies: direct concatenation and projection for classification tasks; or reconstruction tasks which incorporate features from multiple flows via a Periodic Feature Flows Aggregation.", "section": "3 Methodology"}, {"figure_path": "5iUxMVJVEV/figures/figures_14_1.jpg", "caption": "Figure 2: Model architecture. PPAM denotes Periodic Pyramid Attention Mechanism.", "description": "This figure shows the overall architecture of the proposed Peri-midFormer model. It starts with the time embedding of the original time series, followed by a Fast Fourier Transform (FFT) to decompose the series into multiple periodic components. These components are then organized into a Periodic Pyramid structure, which is further processed by the Periodic Pyramid Attention Mechanism (PPAM). Finally, depending on the downstream task (classification or reconstruction), different strategies are used to generate the final output.  The figure clearly illustrates the inclusion relationships between different levels of periodic components and the overall flow of information through the model.", "section": "3 Methodology"}, {"figure_path": "5iUxMVJVEV/figures/figures_14_2.jpg", "caption": "Figure 9: Visualization of imputation.", "description": "This figure visualizes the imputation results of six different models (Peri-midFormer, GPT4TS, PatchTST, TimesNet, FEDformer, and DLinear) on two datasets (Weather and Electricity) with a 50% mask ratio.  For each dataset and model, it shows the ground truth time series and the corresponding predictions. The visualizations allow for a comparison of the different models' ability to accurately impute missing values in time series data, highlighting the strengths and weaknesses of each approach. ", "section": "4 Experiments"}, {"figure_path": "5iUxMVJVEV/figures/figures_15_1.jpg", "caption": "Figure 9: Visualization of imputation.", "description": "This figure visualizes the imputation results on the Weather and Electricity datasets with a 50% mask ratio. It compares the performance of Peri-midFormer against GPT4TS, PatchTST, TimesNet, FEDformer, and DLinear.  Each sub-figure shows the ground truth (blue) and the predicted values (orange) for a specific time series segment.", "section": "4 Experiments"}, {"figure_path": "5iUxMVJVEV/figures/figures_15_2.jpg", "caption": "Figure 2: Model architecture. PPAM denotes Periodic Pyramid Attention Mechanism.", "description": "This figure shows the overall flowchart of the proposed Peri-midformer model.  It starts with time embedding of the original time series.  Then, it uses the Fast Fourier Transform (FFT) to decompose the time series into multiple periodic components of varying lengths across different levels, forming the Periodic Pyramid.  Each component is treated as an independent token and receives positional embedding. The Periodic Pyramid is then fed into the Peri-midFormer, which consists of multiple layers for computing Periodic Pyramid Attention (PPAM). Finally, there are two strategies for downstream tasks:  For classification tasks, components are directly concatenated and projected into the category space. For other reconstruction tasks (forecasting, imputation, and anomaly detection), features from different pyramid branches are integrated through Periodic Feature Flows Aggregation to generate the final output.", "section": "3 Methodology"}, {"figure_path": "5iUxMVJVEV/figures/figures_16_1.jpg", "caption": "Figure 9: Visualization of imputation.", "description": "This figure visualizes the imputation results of different models on the Weather and Electricity datasets. For each dataset, it shows the original data, data with 50% missing values, and the imputation results of Peri-midFormer, GPT4TS, PatchTST, TimesNet, FEDformer, and DLinear. The visualization helps to understand the performance of each model in terms of capturing the underlying patterns of the time series and its ability to reconstruct the missing values.", "section": "4 Experiments"}, {"figure_path": "5iUxMVJVEV/figures/figures_16_2.jpg", "caption": "Figure 9: Visualization of imputation.", "description": "This figure visualizes the imputation results on the Weather and Electricity datasets with a 50% mask ratio.  It shows the original ground truth, the imputation results from Peri-midFormer, GPT4TS, PatchTST, TimesNet, FEDformer, and DLinear.  The visualizations help to illustrate the relative performance of each method for imputing missing values in time series data. ", "section": "4 Experiments"}, {"figure_path": "5iUxMVJVEV/figures/figures_19_1.jpg", "caption": "Figure 2: Model architecture. PPAM denotes Periodic Pyramid Attention Mechanism.", "description": "This figure shows the architecture of the Peri-midFormer model. The input is the original time series, which is first decomposed into multiple periodic components using FFT. These components are organized into a Periodic Pyramid structure, which is then fed into the Peri-midFormer. The Peri-midFormer consists of multiple layers of Periodic Pyramid Attention Mechanism (PPAM), which computes self-attention among periodic components to capture complex temporal variations. Finally, depending on the downstream task, two different strategies are employed: for classification, components are directly concatenated and projected into the category space; for reconstruction tasks (forecasting, imputation, and anomaly detection), features from different pyramid branches are integrated through Periodic Feature Flows Aggregation to generate the final output.", "section": "3 Methodology"}, {"figure_path": "5iUxMVJVEV/figures/figures_19_2.jpg", "caption": "Figure 2: Model architecture. PPAM denotes Periodic Pyramid Attention Mechanism.", "description": "This figure illustrates the architecture of the Peri-midFormer model.  The model takes as input the original time series, which is then decomposed into multiple periodic components via FFT. These components are organized into a Periodic Pyramid structure, reflecting their inclusion relationships. Each level in the pyramid contains components with the same period. Positional embeddings are added. The Periodic Pyramid is then fed into the Peri-midFormer which utilizes a Periodic Pyramid Attention Mechanism (PPAM) to capture complex temporal variations among the periodic components across different levels. Finally, depending on the downstream task, two strategies are used: for classification tasks, components are directly concatenated; for reconstruction tasks (such as forecasting and imputation), features are aggregated via Periodic Feature Flows Aggregation before generating the output.", "section": "3 Methodology"}, {"figure_path": "5iUxMVJVEV/figures/figures_20_1.jpg", "caption": "Figure 3: Inclusion relationships of periodic components (left) and Periodic Pyramid Attention Mechanism (right).", "description": "The figure illustrates the inclusion relationships between periodic components at different levels of the Periodic Pyramid (left).  It also shows the Periodic Pyramid Attention Mechanism (PPAM) (right) that captures these inclusion and overlap relationships in the attention computation.  The left side visually depicts how shorter periods are nested within longer ones.  The right side illustrates how PPAM calculates attention not only between components across different pyramid levels but also within the same level, effectively modeling complex temporal relationships within the time series. ", "section": "3.3 Periodic Pyramid Transformer (Peri-midFormer)"}, {"figure_path": "5iUxMVJVEV/figures/figures_20_2.jpg", "caption": "Figure 3: Inclusion relationships of periodic components (left) and Periodic Pyramid Attention Mechanism (right).", "description": "The figure shows the inclusion relationships between periodic components. The left panel illustrates how components of different periods overlap and are included within each other, forming a pyramid-like structure.  The right panel illustrates the Periodic Pyramid Attention Mechanism (PPAM), which depicts how attention is calculated between periodic components across different levels in the pyramid structure. The arrows indicate the inclusion relationships, where attention is computed among all components within the same level and between components across levels.", "section": "3.2 Periodic Pyramid Construction"}, {"figure_path": "5iUxMVJVEV/figures/figures_21_1.jpg", "caption": "Figure 16: Visualization of the pyramid form of Periodic Feature Flows (left) and the waveform of Periodic Feature Flows (right).", "description": "The figure visualizes the pyramid structure of Periodic Feature Flows, where each branch represents a sequence of periodic components from the top to the bottom level of the pyramid.  The left panel displays a heatmap showing the number of flows and their dimension (vertical axis), illustrating the structure of the flows and how they are composed of periodic components.  The right panel displays the waveform of each feature flow, revealing how the individual feature flows vary in terms of periodic characteristics and their contribution to the overall signal.", "section": "3.4 Periodic Feature Flows Aggregation"}, {"figure_path": "5iUxMVJVEV/figures/figures_22_1.jpg", "caption": "Figure 1: Multi-periodicity, inclusion of periodic components, and Periodic Pyramid.", "description": "This figure illustrates the concept of multi-periodicity in time series, where multiple periodic variations with different periods (e.g., yearly, monthly, weekly, daily) coexist.  It shows how these periodic components can be organized hierarchically, with longer periods encompassing shorter periods, forming a pyramid structure.  The original time series is at the top of the pyramid, and lower levels represent periodic components with gradually shorter periods. This pyramid structure explicitly represents the inclusion relationships among different levels of periodic components in time series.", "section": "1 Introduction"}, {"figure_path": "5iUxMVJVEV/figures/figures_23_1.jpg", "caption": "Figure 5: Model performance comparison.", "description": "This radar chart compares the performance of Peri-midFormer against other state-of-the-art models across five main time series analysis tasks: long-term forecasting, short-term forecasting, imputation, classification, and anomaly detection.  Each axis represents a specific task, and the distance from the center indicates the performance (lower is better for MSE, higher is better for accuracy and F1-score).  The chart visually demonstrates Peri-midFormer's consistent superiority across all five tasks.", "section": "4.1 Main Results"}, {"figure_path": "5iUxMVJVEV/figures/figures_24_1.jpg", "caption": "Figure 9: Visualization of imputation.", "description": "This figure visualizes the imputation results on the Weather and Electricity datasets with a 50% mask ratio.  It compares the imputation performance of Peri-midFormer against several other methods (GPT4TS, PatchTST, TimesNet, FEDformer, and DLinear) by showing the ground truth and predicted values across multiple time series segments. The plots demonstrate how well each model reconstructs the missing data points.", "section": "4 Experiments"}]