[{"figure_path": "Pf7kdIjHRf/figures/figures_1_1.jpg", "caption": "Figure 1: The Heterogeneous Pre-training concept. It maps different embodiments, each with its own proprioception and vision sensors, onto a shared latent space by embodiment-specific tokenizers (\"stems\"). This aligns the heterogeneous data from different embodiments into a joint representation space. This allows us to train a shared Transformer trunk on the union of all heterogeneous datasets. The pre-trained Transformer can be transferred to a new embodiment, with a small, new tokenizer learned at transferring time.", "description": "This figure illustrates the core concept of heterogeneous pre-training. Different robot embodiments, each having unique proprioceptive (e.g., joint angles) and visual sensors, are processed by separate 'stem' modules. These stems transform the sensor data into a shared representation space, enabling a common 'trunk' transformer to learn a general policy representation from data across diverse robots and tasks. This pre-trained trunk can then be easily adapted to new robot embodiments simply by adding a new stem module.", "section": "1 Introduction"}, {"figure_path": "Pf7kdIjHRf/figures/figures_2_1.jpg", "caption": "Figure 2: HPT architecture. HPT is modularized into stems, trunk, and heads. The stem, consisting of a proprioception tokenizer and a vision tokenizer, maps the vision and proprioception observations of different embodiments to a fixed number (e.g., 16) of tokens. The shared trunk, which is a Transformer, maps the concatenated tokens into shared representations. The head then maps the processed tokens to actions in different downstream tasks. For a specific embodiment, one stem/head pair is activated (denoted by the switch). The trunk is shared and pre-trained on action-labeled data with supervised learning and then transferred to new embodiments. This procedure scales up to 52 datasets and 1B parameters.", "description": "The figure illustrates the architecture of Heterogeneous Pre-trained Transformers (HPT).  It shows a modular design with three main components: Embodiment-specific stems that process proprioceptive and visual inputs, a shared Transformer trunk that learns a task-agnostic representation, and task-specific heads that map the representation to actions.  The figure highlights the flexibility and scalability of HPT, showing how it can handle data from multiple embodiments and tasks.", "section": "3 Heterogeneous Pre-trained Transformers (HPT)"}, {"figure_path": "Pf7kdIjHRf/figures/figures_3_1.jpg", "caption": "Figure 3: Stem Architecture in HPT. In the HPT stem, the proprioceptive tokenizer uses an MLP to map proprioceptive information to a feature which is then attended by 16 learnable tokens. The vision tokenizer uses pre-trained encoders and similarly uses an attention mechanism to map vision features into 16 fixed tokens. The architecture flexibly handles sequences of inputs without increasing the size of tokens.", "description": "This figure illustrates the stem architecture of the Heterogeneous Pre-trained Transformers (HPT) model.  The stem is responsible for processing proprioceptive and visual inputs from different robot embodiments.  The proprioceptive tokenizer uses a Multilayer Perceptron (MLP) to transform proprioceptive data into a feature vector, which is then processed using cross-attention with 16 learnable tokens.  Similarly, the vision tokenizer uses a pre-trained encoder (like ResNet) to process visual information, also employing cross-attention with 16 fixed tokens.  The output is a fixed-length sequence of tokens, regardless of the input sequence length, which allows the model to handle variable-length inputs from diverse sensors.", "section": "3 Heterogeneous Pre-trained Transformers (HPT)"}, {"figure_path": "Pf7kdIjHRf/figures/figures_4_1.jpg", "caption": "Figure 4: Dataset Heterogeneity in Robotics. We show illustrations of dataset mixtures (each color is a distinct embodiment) from different domains including real robot teleop [14], deployed robots [39], simulations, and human videos [15]. See Appendix Section A for dataset mixture details.", "description": "This figure shows a pie chart visualizing the distribution of datasets used in the HPT pre-training process across different domains and embodiments. Each slice represents a specific dataset or group of datasets, categorized by their source (real-world teleoperation, simulation, deployed robots, and human videos). The visual representation highlights the diversity and heterogeneity of the data used for pre-training, emphasizing the paper's focus on scaling learning across different robotics environments and sensor modalities.", "section": "3 Heterogenoues Pre-trained Transformers (HPT)"}, {"figure_path": "Pf7kdIjHRf/figures/figures_6_1.jpg", "caption": "Figure 5: Data Scaling. We run scaling HPT experiments along dataset sizes and the number of datasets. Each point is the validation loss of a full training run. (a) We evaluate the losses on 27 datasets with the number of total trajectories ranging from a maximum of 10 trajectories per dataset (270 in total) to a maximum of 100000 trajectories per dataset (170k in total). We compare two model sizes, HPT-S/L, where HPT-L is a bigger model trained with 4 times more tokens than HPT-S. (b) We compute the validation losses for a fixed subset of 10 datasets with a fixed number of epochs (2). We compute mean and standard deviations for 4 runs across model sizes from HPT-S to HPT-XL and across dataset counts from 10 to 52.", "description": "This figure shows the scaling behavior of the Heterogeneous Pre-trained Transformers (HPT) model with respect to the amount of data used during pre-training.  Subfigure (a) demonstrates the relationship between the number of trajectories and the validation loss for two different model sizes (HPT-S and HPT-L). Subfigure (b) shows how the validation loss changes as the number of datasets increases, keeping the number of epochs constant.", "section": "4 Experiments on Pre-training"}, {"figure_path": "Pf7kdIjHRf/figures/figures_6_2.jpg", "caption": "Figure 5: Data Scaling. We run scaling HPT experiments along dataset sizes and the number of datasets. Each point is the validation loss of a full training run. (a) We evaluate the losses on 27 datasets with the number of total trajectories ranging from a maximum of 10 trajectories per dataset (270 in total) to a maximum of 100000 trajectories per dataset (170k in total). We compare two model sizes, HPT-S/L, where HPT-L is a bigger model trained with 4 times more tokens than HPT-S. (b) We compute the validation losses for a fixed subset of 10 datasets with a fixed number of epochs (2). We compute mean and standard deviations for 4 runs across model sizes from HPT-S to HPT-XL and across dataset counts from 10 to 52.", "description": "This figure shows the scaling behavior of the Heterogeneous Pre-trained Transformers (HPT) model with respect to the amount of data and model size.  The left subplot (a) demonstrates how validation loss decreases as the number of training trajectories increases, comparing two different model sizes. The right subplot (b) shows how validation loss changes as the number of datasets increases, again for different model sizes. The results show that HPT scales well with increasing data and model capacity.", "section": "4 Experiments on Pre-training"}, {"figure_path": "Pf7kdIjHRf/figures/figures_7_1.jpg", "caption": "Figure 5: Data Scaling. We run scaling HPT experiments along dataset sizes and the number of datasets. Each point is the validation loss of a full training run. (a) We evaluate the losses on 27 datasets with the number of total trajectories ranging from a maximum of 10 trajectories per dataset (270 in total) to a maximum of 100000 trajectories per dataset (170k in total). We compare two model sizes, HPT-S/L, where HPT-L is a bigger model trained with 4 times more tokens than HPT-S. (b) We compute the validation losses for a fixed subset of 10 datasets with a fixed number of epochs (2). We compute mean and standard deviations for 4 runs across model sizes from HPT-S to HPT-XL and across dataset counts from 10 to 52.", "description": "This figure shows the scaling behavior of the Heterogeneous Pre-trained Transformers (HPT) model with respect to both the size of the dataset and the number of datasets used in training.  The left subplot (a) demonstrates the impact of increasing the number of trajectories per dataset on the validation loss for two different model sizes (HPT-S and HPT-L). The right subplot (b) shows how the validation loss changes as the number of datasets increases while keeping the number of epochs constant and comparing different model sizes. Overall, the figure highlights the scaling properties of HPT as the amount of training data increases and suggests a beneficial effect of model size and increased diversity of training datasets in improving generalization.", "section": "4 Experiments on Pre-training"}, {"figure_path": "Pf7kdIjHRf/figures/figures_8_1.jpg", "caption": "Figure 9: Simulation Evaluation Tasks. We evaluate HPT across several simulation benchmarks and show policy rollout visualizations of the experiments. Experiment details can be found in Section 5.1 and A.4.", "description": "This figure shows several example tasks from different simulation environments used to evaluate the HPT model.  The figure visually demonstrates the robot performing different manipulation tasks in various simulated settings. The specific details of the experiments are described in sections 5.1 and A.4 of the paper.", "section": "5.1 Transfer to Embodiments in Simulations"}, {"figure_path": "Pf7kdIjHRf/figures/figures_8_2.jpg", "caption": "Figure 10: Success Rates in Simulation Experiments. (a) We evaluate transfer learning performance of models from HPT-B to HPT-XL on tasks across 4 different simulator benchmarks. (b) We compare with several generalist models in the recent Simpler [43] benchmark with Google GDR embodiment. The pre-trained trunks are trained from the Scaled Settings. The success rates are computed over 150 rollouts per approach.", "description": "This figure presents the success rates achieved by different models on robot manipulation tasks across various simulation benchmarks.  Specifically, it compares the transfer learning performance of models with pre-trained trunks (HPT-B to HPT-XL) against other generalist models (RT-1X, RT-2X, Octo) in the Simpler benchmark. The pre-trained trunks used the 'Scaled Settings' from the pre-training phase, highlighting the impact of this pre-training on downstream task performance.", "section": "5.1 Transfer to Embodiments in Simulations"}, {"figure_path": "Pf7kdIjHRf/figures/figures_9_1.jpg", "caption": "Figure 11: Real World Qualitative Results. Pre-trained HPT policies can perform dynamic and long-horizon contact-rich precision tasks in pet care and assembly. The policies show robust and generalized behaviors under scene changes and disturbances.", "description": "This figure shows the qualitative results of applying pre-trained HPT policies to four real-world tasks: Sweep Leftover, Fill Water, Scoop Food, and Switch Insertion.  The image sequence for each task demonstrates the robot's ability to successfully complete the task despite variations in the environment and object placement. The caption highlights that the policies exhibit robustness and generalization.", "section": "5.2 Transfer to Embodiments in the Real World"}, {"figure_path": "Pf7kdIjHRf/figures/figures_9_2.jpg", "caption": "Figure 12: Transfer Learning in the Real World. We evaluate the pre-trained HPTs on four tasks / two embodiments. The average success rate with standard deviations is computed for 45 trials per approach. We use the default pre-training setup with HPT-Base for this experiment. See Section 5.2 for detailed descriptions.", "description": "This bar chart displays the success rates of four different methods (No Trunk, From Scratch, Pretrained Frozen, Pretrained Finetuned) across four real-world robotic tasks (Sweep Leftover, Fill Water, Scoop Food, Switch Insertion).  Each bar represents the average success rate across 45 trials, with error bars indicating the standard deviation.  The chart compares the performance of models that use a pre-trained transformer trunk (Pretrained Frozen, Pretrained Finetuned) against models trained from scratch (From Scratch) and models without a trunk (No Trunk).  The results demonstrate the effectiveness of pre-training the HPT model for improving real-world robotic task performance.", "section": "5.2 Transfer to Embodiments in the Real World"}, {"figure_path": "Pf7kdIjHRf/figures/figures_9_3.jpg", "caption": "Figure 10: Success Rates in Simulation Experiments. (a) We evaluate transfer learning performance of models from HPT-B to HPT-XL on tasks across 4 different simulator benchmarks. (b) We compare with several generalist models in the recent Simpler [43] benchmark with Google GDR embodiment. The pre-trained trunks are trained from the Scaled Settings. The success rates are computed over 150 rollouts per approach.", "description": "This figure shows the success rates achieved by different models on various robotic manipulation tasks in simulation.  Part (a) compares the performance of HPT models of varying sizes (HPT-B to HPT-XL) across four different simulation benchmarks (Fleet-Tools, Hammer, Metaworld, RoboMimic). Part (b) benchmarks HPT-XL against other state-of-the-art generalist models on the Simpler benchmark using the Google GDR robot embodiment.  Higher success rates indicate better performance in completing the tasks.", "section": "5.1 Transfer to Embodiments in Simulations"}, {"figure_path": "Pf7kdIjHRf/figures/figures_17_1.jpg", "caption": "Figure 13: Large-scale Dataset Heterogeneity in Robotics. We show different dataset mixtures at increasing scales (top row) across trajectory counts, dataset sample counts, and sampling weights (bottom row). We also show illustrations of the different embodiments including real robots, simulations, and human videos. By default, during training, we use a uniform distribution to sample from each of the embodiment datasets.", "description": "This figure illustrates the heterogeneity of the datasets used in the HPT pre-training.  It shows how the datasets are composed of various sources (real robots, simulations, human videos) and how they differ in terms of the number of trajectories, episode steps, sample weights, and grouped sample weights. The pie charts visually represent the proportion of each data source in the overall dataset, highlighting the diversity and scale of the pre-training data.", "section": "3 Heterogenoues Pre-trained Transformers (HPT)"}, {"figure_path": "Pf7kdIjHRf/figures/figures_21_1.jpg", "caption": "Figure 10: Success Rates in Simulation Experiments. (a) We evaluate transfer learning performance of models from HPT-B to HPT-XL on tasks across 4 different simulator benchmarks. (b) We compare with several generalist models in the recent Simpler [43] benchmark with Google GDR embodiment. The pre-trained trunks are trained from the Scaled Settings. The success rates are computed over 150 rollouts per approach.", "description": "This figure presents the results of transfer learning experiments using the Heterogeneous Pre-trained Transformers (HPT) model on various robotic manipulation simulation benchmarks. Part (a) shows the success rates achieved by different sized HPT models (HPT-B to HPT-XL) across four different simulation environments.  Part (b) compares the performance of HPT-XL against other generalist robotic models on a more recent benchmark, using the Google GDR embodiment.  All pre-trained trunks utilized data from the 'Scaled Settings' as described in the paper and success rates are based on 150 rollouts for each method.", "section": "5.1 Transfer to Embodiments in Simulations"}, {"figure_path": "Pf7kdIjHRf/figures/figures_21_2.jpg", "caption": "Figure 10: Success Rates in Simulation Experiments. (a) We evaluate transfer learning performance of models from HPT-B to HPT-XL on tasks across 4 different simulator benchmarks. (b) We compare with several generalist models in the recent Simpler [43] benchmark with Google GDR embodiment. The pre-trained trunks are trained from the Scaled Settings. The success rates are computed over 150 rollouts per approach.", "description": "This figure displays the results of transfer learning experiments using pre-trained Heterogeneous Pre-trained Transformers (HPT) models on various robot manipulation simulation benchmarks.  Part (a) shows the success rates of HPT models of different sizes (HPT-B, HPT-XL) on several benchmarks, comparing them against baselines of training from scratch. Part (b) shows a comparison with other state-of-the-art generalist models on the Simpler benchmark using the Google GDR robot embodiment.", "section": "5.1 Transfer to Embodiments in Simulations"}, {"figure_path": "Pf7kdIjHRf/figures/figures_22_1.jpg", "caption": "Figure 18: Ablation Study on HPT Stem. We ablate the pre-training performance for (a) proprioception, (b) vision stems, and (c) vision encoders. Setting: HPT-S, batch 256, iterations 80000, 27 datasets with a maximum of 1000 trajectories for each dataset.", "description": "This figure presents an ablation study on the HPT stem, investigating the impact of removing proprioception, vision stems, and vision encoders from the model.  The results, presented in validation loss, show that removing either proprioception or vision data significantly impairs performance, highlighting the importance of both modalities for effective pre-training.", "section": "B.2 Ablation Study on the Stem"}, {"figure_path": "Pf7kdIjHRf/figures/figures_22_2.jpg", "caption": "Figure 11: Real World Qualitative Results. Pre-trained HPT policies can perform dynamic and long-horizon contact-rich precision tasks in pet care and assembly. The policies show robust and generalized behaviors under scene changes and disturbances.", "description": "This figure shows real-world qualitative results of the pre-trained HPT policies on four different tasks: Sweep Leftover, Fill Water, Scoop Food, and Switch Insertion.  The results demonstrate the policies' ability to perform dynamic and long-horizon contact-rich precision tasks despite scene changes and disturbances, highlighting the robustness and generalization capabilities of the pre-trained HPT models.", "section": "5.2 Transfer to Embodiments in the Real World"}]