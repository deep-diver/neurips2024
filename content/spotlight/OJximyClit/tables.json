[{"figure_path": "OJximyClit/tables/tables_4_1.jpg", "caption": "Table 1: Comparison of accuracy (%) on 10 datasets for CLIP ViT-B/16 and ViT-L/14.", "description": "This table presents a comparison of the accuracy achieved by different zero-shot vision models on 10 benchmark datasets.  The models are evaluated using two different CLIP backbones: ViT-B/16 and ViT-L/14.  The table allows for a direct comparison of performance across various methods and backbones, showing the relative effectiveness of each in enhancing zero-shot capabilities.", "section": "4.2 Main Results"}, {"figure_path": "OJximyClit/tables/tables_6_1.jpg", "caption": "Table 1: Comparison of accuracy (%) on 10 datasets for CLIP ViT-B/16 and ViT-L/14.", "description": "This table presents a comparison of the accuracy achieved by various zero-shot vision models on ten image classification datasets.  The models are tested using two different versions of the CLIP architecture: ViT-B/16 and ViT-L/14.  The table allows for a direct comparison of model performance across different datasets and CLIP versions. The table includes both baseline methods and the proposed 'Frolic' method, enabling an assessment of the performance improvement.", "section": "4.2 Main Results"}, {"figure_path": "OJximyClit/tables/tables_7_1.jpg", "caption": "Table 1: Comparison of accuracy (%) on 10 datasets for CLIP ViT-B/16 and ViT-L/14.", "description": "This table presents a comparison of the accuracy achieved by different zero-shot vision models on ten image classification datasets.  The models compared include various state-of-the-art methods and the proposed Frolic method.  Results are shown for two different CLIP backbones, ViT-B/16 and ViT-L/14, highlighting the performance variations across different model architectures. The table demonstrates the relative improvement achieved by Frolic compared to existing methods.", "section": "4.2 Main Results"}, {"figure_path": "OJximyClit/tables/tables_7_2.jpg", "caption": "Table 3: Accuracy (%) of different models on 10-datasets, ImageNet and its five variant datasets.", "description": "This table presents the accuracy achieved by various models (including the proposed Frolic model and its variants) across different datasets.  The datasets include a set of 10 commonly used image classification benchmarks (10-datasets), the ImageNet dataset, and five variants of ImageNet representing different image distribution shifts (IN-Variants). Each model's performance is evaluated using two different backbone architectures: ViT-B/16 and ViT-L/14. The rows represent different versions of the model, showing the impact of each component (prompt distribution learning, bias correction, fusion technique).", "section": "4.3 Ablation Studies and Further Analysis"}, {"figure_path": "OJximyClit/tables/tables_8_1.jpg", "caption": "Table 1: Comparison of accuracy (%) on 10 datasets for CLIP ViT-B/16 and ViT-L/14.", "description": "This table compares the accuracy of different zero-shot vision models on ten benchmark datasets using two different CLIP backbones: ViT-B/16 and ViT-L/14.  The models compared include the baseline CLIP, several prompt engineering and bias correction methods, and the proposed Frolic method.  The table highlights Frolic's superior performance across various datasets compared to other state-of-the-art techniques, showcasing the effectiveness of its label-free prompt distribution learning and bias correction approach.", "section": "4.2 Main Results"}, {"figure_path": "OJximyClit/tables/tables_8_2.jpg", "caption": "Table 5: Comparison of accuracy (%) between our Frolic and prompt-based methods for CLIP ViT-B/16. * denotes our method built upon InMaP [28]", "description": "This table compares the accuracy of the proposed method, Frolic, against other prompt-based methods (CoOp and CoCoOp) using CLIP ViT-B/16.  The accuracy is reported for various datasets, showing that Frolic outperforms other methods. The asterisk indicates that Frolic uses InMaP.", "section": "4.2 Main Results"}, {"figure_path": "OJximyClit/tables/tables_9_1.jpg", "caption": "Table 6: Comparison of accuracy (%) between our Frolic and adapter-based distribution methods for CLIP ViT-B/16. * denotes our method built upon InMaP [28]", "description": "This table compares the accuracy of Frolic against two other adapter-based distribution methods (LFA and Tip-Adapter) using CLIP ViT-B/16 on ImageNet and its variants.  It shows the accuracy (%) achieved by each method on the ImageNet dataset and four of its distribution shifts (IN-A, IN-V2, IN-R, IN-Sketch). The average accuracy across these five datasets is also provided.", "section": "4.2 Main Results"}, {"figure_path": "OJximyClit/tables/tables_9_2.jpg", "caption": "Table 7: Comparison of running time on ImageNet with ViT-B/16.", "description": "This table compares the running time and accuracy of different models on the ImageNet dataset using the ViT-B/16 architecture. The models compared are CLIP, TPT, TDA, and Frolic.  Frolic shows a significant improvement in accuracy over other methods while maintaining a reasonable runtime compared to TDA. CLIP has the fastest runtime, but also the lowest accuracy.", "section": "Experiments"}]