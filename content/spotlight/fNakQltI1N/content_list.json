[{"type": "text", "text": "Trajectory Flow Matching with Applications to Clinical Time Series Modeling ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xi Zhang1,2 \u2217 Yuan Pu3 \u2217 Yuki Kawamura4 Andrew Loza3 Yoshua Bengio2,5,6 Dennis L. Shung3 \u2020 Alexander Tong2,5 \u2020 ", "page_idx": 0}, {"type": "text", "text": "1McGill University, 2Mila - Quebec AI Institute, 3Yale School of Medicine 4School of Clinical Medicine, University of Cambridge, 5Universit\u00e9 de Montr\u00e9al, 6CIFAR Fellow ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Modeling stochastic and irregularly sampled time series is a challenging problem found in a wide range of applications, especially in medicine. Neural stochastic differential equations (Neural SDEs) are an attractive modeling technique for this problem, which parameterize the drift and diffusion terms of an SDE with neural networks. However, current algorithms for training Neural SDEs require backpropagation through the SDE dynamics, greatly limiting their scalability and stability. To address this, we propose Trajectory Flow Matching (TFM), which trains a Neural SDE in a simulation-free manner, bypassing backpropagation through the dynamics. TFM leverages the flow matching technique from generative modeling to model time series. In this work we first establish necessary conditions for TFM to learn time series data. Next, we present a reparameterization trick which improves training stability. Finally, we adapt TFM to the clinical time series setting, demonstrating improved performance on three clinical time series datasets both in terms of absolute performance and uncertainty prediction, a crucial parameter in this setting. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Real world problems often involve systems that evolve continuously over time, yet these systems are usually noisy and irregularly sampled. In addition, real-world time series often relate to other covariates, leading to complex patterns such as intersecting trajectories. For instance, in the context of clinical trajectories in healthcare, patients\u2019 vital sign evolution can follow drastically different, crossing paths even if the initial measurements are similar, due to the influence of the covariates such as medication intervention and underlying health conditions. These covariates can be time-varying or static, and often sparse. ", "page_idx": 0}, {"type": "text", "text": "Differential equation-based dynamical models are proficient at learning continuous variables without imputations [Chen et al., 2018, Rubanova et al., 2019, Kidger et al., 2021b]. Nevertheless, systems governed by ordinary differential equations (ODEs) or stochastic differential equations (SDEs) are unable to accommodate intersecting trajectories, and thus requires modifications such as augmentation or modelling higher-order derivatives [Dupont et al., 2019]. While ODEs model deterministic systems, SDEs contain a diffusion term and can better represent the inherent uncertainty and fluctuations present in many real world systems. However, ftiting stochastic equations to real life data is challenging because they have thus far required time-consuming backpropagation through an SDE integration. ", "page_idx": 0}, {"type": "image", "img_path": "fNakQltI1N/tmp/17c16fe502ddf9adb2257731fcd09a9deeab3d2e0672ccc97f1eeef89cc258ca.jpg", "img_caption": ["Figure 1: Trajectory Flow Matching trains both an estimator of the next timepoint $(\\hat{x}_{\\theta}(t,x))$ and an estimation of the uncertainty $(\\sigma_{\\theta}(t,x_{t}))$ . Using the conditional flow matching framework, these can be used to predict the instantaneous velocity $v_{\\theta}(t,x_{t})$ and future observations. Both flows are conditioned on past data $x_{[t-h,t-1]}$ and conditional variables $c$ . "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In the domain of generative models, diffusion models [Ho et al., 2020, Nichol and Dhariwal, 2021, Song et al., 2021] and more recently flow matching models [Lipman et al., 2023, Albergo et al., 2023, Li et al., 2020] have had enormous success by training dynamical models in a simulation-free framework. The simulation-free framework facilitates the training of much larger models with significantly improved speed and stability. In this work we generalize simulation-free training for fitting stochastic differential equations to time-series data, to learn population trajectories while preserving individual characteristics with conditionals. We present this method as Trajectory Flow Matching. We demonstrate that our method outperforms current state of the art time series modelling architecture including RNN, ODE based and flow matching methods. We empirically demonstrate the utility of our method in clinical applications where hemodynamic trajectories are critical for ongoing dynamic monitoring and care. We applied our method to the following longitudinal electronic health record datasets from multiple clinical settings: medical intensive care unit (MICU) data of patients with sepsis, Emergency Department (ED) data of patients with acute gastrointestinal bleeding, and MICU data of patients with acute gastrointestinal bleeding. ", "page_idx": 1}, {"type": "text", "text": "Our main contributions are: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We prove the conditions under which continuous time dynamics can be trained simulationfree using matching techniques.   \n\u2022 We extend the approach to irregularly sampled trajectories with a time predictive loss and to estimate uncertainty using an uncertainty prediction loss.   \n\u2022 We empirically demonstrate that our approach reduces the error by $15\u201383\\%$ when applied to the real world clinical data modelling. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Notation ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We consider the setting of a distribution of trajectories over $\\mathbb{R}^{d}$ denoted ${\\mathcal X}:=\\{x^{1},x^{2},\\cdot\\cdot\\cdot,x^{n}\\}$ where each $x^{i}$ consists of a set of trajectories of length $T$ i.e. $x^{i}:=\\{x_{1}^{i},x_{2}^{i},\\dots,x_{T}^{i}\\}$ with associated times $t^{i}:=\\{t_{1}^{i},t_{2}^{i},\\dots,t_{T}^{i}\\}$ . Let $x_{[t-h,t-1]}^{i}$ denote a vector of the last $h$ observed time points. We denote a (Lipschitz smooth) time dependent vector field conditioned on arbitrary conditions $c\\in\\mathbb{R}^{e}$ $\\begin{array}{r}{v(t,x_{t},x_{[t-h,t-1]},c)\\to\\frac{d x}{d t}:([0,1],\\mathbb{R}^{d},\\mathbb{R}^{h\\times d},\\mathbb{R}^{e})\\to\\mathbb{R}^{d}}\\end{array}$ with flow $\\phi_{t}(v)$ which induces the timedependent density $p_{t}\\,=\\,\\phi_{t}(v)_{\\#}(p_{0})$ for any density $p_{0}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}_{+}$ with $\\begin{array}{r}{\\int_{\\mathbb{R}^{d}}p_{0}\\,=\\,1}\\end{array}$ . We also consider the coupling $\\pi(x_{0},x_{1})$ which operates on the product space of marginal distributions $p_{0},p_{1}$ . ", "page_idx": 1}, {"type": "text", "text": "2.2 Neural Stochastic Differential Equations ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "A stochastic differential equation (SDE) can be expressed in terms of a smooth drift $f:[0,T]\\times\\mathbb{R}^{d}\\rightarrow$ $\\mathbb{R}^{d}$ and diffusion $g:[0,T]\\times\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d^{2}}$ in the Ito sense as: ", "page_idx": 2}, {"type": "equation", "text": "$$\nd x_{t}=f\\,d t+g\\,d W_{t}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $W_{t}:[0,T]\\rightarrow\\mathbb{R}^{d}$ is the $d$ -dimensional Wiener process. A density $p_{0}(x_{0})$ evolved according to an SDE induces a collection of marginal distributions $p_{t}(\\boldsymbol{x}_{t})$ viewed as a function $p:[0,T]\\times\\mathbb{R}^{d}\\rightarrow$ $\\mathbb{R}_{+}$ . In a Neural SDE [Li et al., 2020, Kidger et al., 2021a,b] the drift and diffusion terms are parameterized with neural networks $f_{\\theta}(t,x_{t})$ and $g_{\\theta}(t,x_{t})$ . ", "page_idx": 2}, {"type": "equation", "text": "$$\nd x_{t}=f_{\\theta}(t,x_{t})d t+g_{\\theta}(t,x_{t})d W_{t}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the goal is to select $\\theta$ to enforce $x_{T}\\sim X_{t r u e}$ for some distributional notion of similarity such as the Wasserstein distance [Kidger et al., 2021b] or Kullback-Leibler divergence [Li et al., 2020]. However, these objectives are simulation-based, requiring a backpropagation through an SDE solver, which suffers from severe speed and stability issues. While some issues such as memory and numerical truncation can be ameliorated using the adjoint state method and advanced numerical solvers [Kidger et al., 2021b], optimization of Neural SDEs is still a significant issue. ", "page_idx": 2}, {"type": "text", "text": "We note that in the special case of zero-diffusion (i.e. $g_{\\theta}(t,x_{t})=0$ ) this reduces to a neural ordinary differential equation (Neural ODE) [Chen et al., 2018], which is easier to optimize than SDEs, but still presents challenges to scalability. ", "page_idx": 2}, {"type": "text", "text": "2.3 Matching algorithms ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Matching algorithms are a simulation-free class of training algorithms which are able to bypass backpropagation through the solver during training by constructing the marginal distribution as a mixture of tractable conditional probability paths. ", "page_idx": 2}, {"type": "text", "text": "The marginal density $p_{t}$ induced by eq. 1 evolves according to the Fokker-Plank equation (FPE): ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\partial_{t}p_{t}=-\\nabla\\cdot(p_{t}f_{t})+\\frac{g^{2}}{2}\\Delta p_{t}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\Delta p_{t}=\\nabla\\cdot(\\nabla p_{t})$ denotes the Laplacian of $p_{t}$ and gradients are taken with respect to $x_{t}$ ", "page_idx": 2}, {"type": "text", "text": "Matching algorithms first construct a factorization of $p_{t}$ into conditional densities $p_{t}(x_{t}|z)$ such that $p_{t}=\\mathbb{E}_{q(z)}\\left[p_{t}(x_{t}|z)\\right]$ and where $p_{t}(x_{t}|z)$ is generated by an SDE $d x_{t}=v_{t}(x_{t}|\\boldsymbol{z})d t+\\sigma_{t}(x_{t}|\\boldsymbol{z})d W_{t}$ . Given this construction it can be shown that the minimizer of ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{match}}(\\theta):=\\mathbb{E}_{t,q(z),p_{t}(x|z)}\\left[\\left\\|f_{\\theta}(t,x_{t})-v_{t}(x_{t}|z)\\right\\|^{2}+\\lambda_{t}^{2}\\left\\|g_{\\theta}(t,x_{t})-\\sigma_{t}(x_{t}|z)\\right\\|^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "satisfies the FPE of the marginal $p_{t}$ . This is especially useful in the generative modeling setting where $q_{0}$ is samplable noise (e.g. ${\\mathcal{N}}(0,1))$ and $q_{1}$ is the data distribution. Then we can define ${\\boldsymbol{z}}:=(x_{0},x_{1})$ as a tuple of noise and data with $q(z):=q_{0}(x_{0})\\otimes q_{1}(x_{1})$ . This makes eq. 3 optimize a model which will draw new samples according to the data distribution $q_{1}(x_{1})$ using ", "page_idx": 2}, {"type": "equation", "text": "$$\nx_{0}\\sim q_{0};\\quad x_{1}=\\int_{0}^{1}f_{\\theta}(t,x_{t})d t+g_{\\theta}(t,x_{t})d W_{t}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "with the integration computed numerically using any off-the-shelf SDE solver. While this is guaranteed to preserve the distribution over time, it is not guaranteed to preserve the coupling of $q_{0}$ and $q_{1}$ (if given). ", "page_idx": 2}, {"type": "text", "text": "Paired bridge matching In generative modeling random pairings [Liu et al., 2023c, Albergo and Vanden-Eijnden, 2023, Albergo et al., 2023] or optimal transport [Tong et al., 2024, Pooladian et al., 2023] pairings are constructed for the conditional distribution $q(z)$ . However, in some problems we would like to match pairs of points as is the case in image-to-image translation [Isola et al., 2017, Liu et al., 2023a, Somnath et al., 2023]. In this case, training data comes as pairs $(x_{0},x_{1})$ . In this case we set $q(z):=q(x_{0},x_{1})$ to be samples from these known pairs, and optimize eq. 3. While empirically, these models perform well, there are no guarantees that the coupling will be preserved outside of the special case when data comes from the (entropic) optimal transport coupling $\\pi_{\\varepsilon}^{*}(q_{0},q_{1})$ and defined as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pi_{\\varepsilon}^{*}(q_{0},q_{1})=\\underset{\\pi\\in{U}(q_{0},q_{1})}{\\arg\\operatorname*{min}}\\int d(x_{0},x_{1})^{2}\\,d\\pi(x_{0},x_{1})+\\varepsilon\\,\\mathrm{KL}(\\pi\\|q_{0}\\otimes q_{1}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Input: Trajectories $\\mathcal{X}$ , noise $\\sigma$ , initial network $v_{\\theta}$ . while Training do $\\begin{array}{r l}&{z\\sim\\mathcal{U}(\\mathcal{X}),\\quad k\\sim\\mathcal{U}\\{1,T-1\\},\\quad t\\sim\\mathcal{U}(0,1)}\\\\ &{\\mu_{t}\\leftarrow t x_{k}+(1-t)x_{k+1}}\\\\ &{x_{t}\\sim\\mathcal{N}(\\mu_{t},\\sigma^{2}t(1-t)I)}\\\\ &{\\mathcal{L}_{\\mathrm{TFM}}(\\theta)\\leftarrow\\left\\|v_{\\theta}(k+t,x_{t})-\\frac{x_{k+1}-x_{t}}{1-t}\\right\\|^{2}}\\\\ &{\\mathcal{L}_{\\sigma_{t}}(\\theta)\\leftarrow\\left\\|\\sigma_{\\theta}(k+t,x_{t})-\\mathcal{L}_{\\mathrm{TFM}}\\right\\|^{2}}\\\\ &{\\theta\\leftarrow\\mathrm{Update}(\\theta,\\nabla_{\\theta}\\mathcal{L}_{\\mathrm{TFM}}(\\theta),\\nabla_{\\theta}\\mathcal{L}_{\\sigma_{t}}(\\theta))}\\\\ &{\\mathrm{urn}\\;v_{\\theta},\\sigma_{\\theta}}\\end{array}$ ", "page_idx": 3}, {"type": "text", "text": "where $U(q_{0},q_{1})$ is the set of admissible transport plans (i.e. joint distributions over $x_{0}$ and $x_{1}$ whose marginals are equal to $q_{0}$ and $q_{1}$ ) as shown in [Shi et al., 2023] for some regularization parameter $\\varepsilon\\in\\mathbb{R}_{\\geq0}$ . ", "page_idx": 3}, {"type": "text", "text": "3 Trajectory Flow Matching ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We now describe our simulation-free method to learn SDEs on time-series data using trajectory flow matching as summarized in Alg. 1. In the case of time series we need to ensure that trajectory couplings are preserved. We first set out a general algorithm for flow matching on vector fields in $\\S3.1$ then present a numerical reparameterization which we find stabilizes training in $\\S3.2$ , a next observation prediction for irregularly sampled time series in $\\S3.3$ , and finally present how to learn the noise in $\\S3.4$ . ", "page_idx": 3}, {"type": "text", "text": "3.1 Preserving Couplings ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we assume access to fully observed and evenly spaced trajectories $\\mathcal{X}\\,=$ $(x^{1},x^{2},\\ldots,x^{n})$ siwliyt he $x^{i}\\,:=\\,(x_{1}^{i},x_{2}^{i},\\ldots,x_{T}^{i})$ efroarl  sceltatriintgy  oafn idrr engoutlaatirloyn asla msipmlepdl itcrtayj.e ctWorei ens.o tIen  tthhiast simplified case we let ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~~~~z:=(x_{1},x_{2},\\ldots,x_{T})}\\\\ &{~~~~q(z):=\\mathcal{U}(\\mathcal{X})}\\\\ &{p_{t}(x|z):=\\mathcal{N}(([t]-t)x_{\\lfloor t\\rfloor}+(t-\\lfloor t\\rfloor)x_{\\lceil t\\rceil},\\sigma^{2}(\\lceil t\\rceil-t)(t-\\lfloor t\\rfloor)\\mathbf{I})}\\\\ &{u_{t}(x|z):=\\frac{x_{\\lceil t\\rceil}-x_{t}}{\\lceil t\\rceil-t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathcal{U}(\\mathcal{X})$ is the uniform empirical distribution over $\\mathcal{X},\\lceil\\cdot\\rceil,\\lfloor\\cdot\\rfloor$ are the ceiling and floor functions, and $\\mathcal{N}(\\cdot,\\cdot)$ is the multivariate normal distribution. This is a valid regression in the sense that a function minimized with Alg. 1 will return a stochastic process that will match the observed marginal distributions over time as shown in the following lemma. ", "page_idx": 3}, {"type": "text", "text": "Lemma 3.1. The SDE $d x_{t}=u_{t}(x|z)d t+\\sigma^{2}d W_{t}$ where $u_{t}$ is defined in eq. 9 generates $p_{t}(x|z)$ in eq. 8 with initial condition $p_{0}:=\\delta_{x_{1}}$ where $\\delta$ is the Dirac delta function. ", "page_idx": 3}, {"type": "text", "text": "however, while useful, this is still insufficient for time series modeling, as it does not ensure coupling preservation. For intuition why this is an issue see figure 2. ", "page_idx": 3}, {"type": "text", "text": "In TFM we ensure that the couplings are preserved for history lengths $\\textit{h}\\:>\\:\\:0$ . i.e. $\\hat{\\pi}(x_{T-h},x_{T-h+1},\\ldots,x_{T})\\,=\\,\\pi(x_{T-h},x_{T-h+1},\\ldots,x_{T})$ . We first establish a method to ensure that these couplings are preserved allowing us to use simulation-free flow matching training for the time-series modeling task. Specifically, as long as the model takes as input $(x_{T-h},x_{T-h+1},\\ldots,x_{T})$ in predicting the flow from $T\\,\\rightarrow\\,T+1$ , then there exists a function $f_{\\theta}(X_{T-h:T})$ such that the coupling is preserved. ", "page_idx": 3}, {"type": "text", "text": "Proposition 3.2 (Coupling Preservation). Under mild regulatory criteria on $u_{t}(\\cdot|z),\\,p_{t}$ , and $q_{:}$ , if ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{E}_{t\\sim\\mathcal{U}(0,T),z\\sim q(z),c\\sim q(c|z),x_{t}\\sim p_{t}(x_{t}|z)}\\|u_{t}(x_{t}|z,c)-u_{t}(x_{t}|c)\\|_{2}^{2}=0\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and $z,q(z),p_{t}(x|z)$ and $u_{t}(x|z)$ are as defined in eqs. 6-9 then $\\Pi(u)^{\\star}=\\Pi^{\\star}(x_{1:T})$ . ", "page_idx": 3}, {"type": "text", "text": "Where $\\Pi(u)^{\\star}$ represents the coupling of a model which attains minimal loss according to eq. 3 and $\\Pi^{\\star}(x_{1:T})$ is the coupling of the data distribution. Intuitively, as long as no two paths cross given conditionals $c$ , then the coupling is preserved. In prior work $c=\\emptyset$ , and the coupling is only preserved in special cases such as eq. 5. ", "page_idx": 4}, {"type": "text", "text": "We next enumerate three assumptions under which the coupling is guaranteed to be preserved at the optima. We note that these are ", "page_idx": 4}, {"type": "text", "text": "(A1) When $c=x_{0}$ and there exists $T:\\mathcal{X}\\rightarrow\\mathcal{X}$ such that $T(x_{0})=x_{1}$ iff $\\Pi^{\\star}(x_{0},x_{1})$ . We note that this is equivalent to asserting the existence of a Monge map $T^{\\star}$ for the coupling $\\Pi^{\\star}$ . ", "page_idx": 4}, {"type": "text", "text": "(A2) There exist no two trajectories $x^{i}$ , $x^{j}$ such that $\\boldsymbol{x}_{t}^{i}=\\boldsymbol{x}_{t}^{j}$ for $h$ consecutive observations and $g=0$ . (A3) Trajectories are associated with unique conditional vectors $c$ independent of $t$ . ", "page_idx": 4}, {"type": "text", "text": "Even in cases when (A1)-(A3) may not hold exactly, TFM is a useful model and can often still learn useful models of the data. In some sense uniqueness up to some history length is enough as it shows TFM is as powerful as discrete-time autoregressive models. Proofs and further examples are available in $\\S\\mathrm{A.l}$ . ", "page_idx": 4}, {"type": "text", "text": "3.2 Target prediction reparameterization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "While flow matching generally predicts the flow, there is a target predicting equivalent namely given $\\begin{array}{r}{v_{\\theta}(t,x)\\;:=\\;\\frac{\\hat{x}_{\\theta}^{\\lceil t\\rceil}(t,x_{t})-x_{t}}{\\lceil t\\rceil-t}}\\end{array}$ x\u02c6\u03b8 (\u2308tt,\u2309x\u2212tt)\u2212xt and ut(x|z) := $\\begin{array}{r}{u_{t}(x|z)\\;:=\\;\\frac{x^{\\lceil t\\rceil}-x_{t}}{\\lceil t\\rceil-t}}\\end{array}$ which is equivalent to $x_{1}-x_{0}$ when $x_{t}:t x_{1}+(1-t)x_{0}$ then it is easy to show that the target predicting loss is equivalent to a timeweighted flow-matching loss. Specifically let the target predicting loss be ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{target}}(\\theta)=\\mathbb{E}_{t,q(z),p_{t}(x|z)}\\|\\hat{x}_{\\theta}^{\\lceil t\\rceil}(t,x)-x^{\\lceil t\\rceil}\\|^{2}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "then it is easy to show that ", "page_idx": 4}, {"type": "text", "text": "Proposition 3.3. There exists a scaling function $c(t):\\mathbb{R}_{+}\\to\\mathbb{R}$ such that $\\mathcal{L}_{t a r g e t}(\\theta)=c(t)\\mathcal{L}_{m a t c h}(\\theta)$ . ", "page_idx": 4}, {"type": "text", "text": "3.3 Irregularly sampled trajectories ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We next consider irregularly sampled time series of the form $x^{i}:=\\big((x_{1}^{i},t_{1}^{i}),(x_{2}^{i},t_{2}^{i}),\\dots,(x_{T}^{i},t_{T}^{i})\\big)$ with $t_{1}^{i}<t_{2}^{i}<\\cdot\\cdot<t_{T}^{i}$ with $t_{n e x t}$ denoting the next timepoint observed after time $t$ . In this case, when combined with the target predicting reparameterization in $\\S3.2$ , we can predict the time till next observation. We therefore parameterize an auxiliary model $h_{\\theta}(t,x_{t}):[0,\\bar{T]}\\times\\mathbb{R}^{d}\\rightarrow[0,T]$ which predicts the next observation time. This is useful numerically, but also, perhaps more importantly, is useful in a clinical setting, where the spacing between measurements can be as informative as the measurements themselves [Allam et al., 2021]. $h_{\\theta}$ is trained to predict the time till the next observation with the time predictive loss: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathfrak{t p}}(\\theta)=\\sum_{t\\in\\mathcal{T}^{i}}\\|h_{\\theta}(t,x_{t})-(t_{\\mathrm{next}}-t)\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $t_{\\mathrm{next}}$ is the time of the next measurement. This can be used in conjunction with the $x_{\\mathrm{next}}$ predictor to calculate the flow at time $t$ as ", "page_idx": 4}, {"type": "equation", "text": "$$\nv_{\\theta}(t,x_{t}):=\\frac{\\hat{x}_{\\theta}^{1}(t,x_{t})-x_{t}}{h_{\\theta}(t,x_{t})-t}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "which can be used for inference on new trajectories. ", "page_idx": 4}, {"type": "text", "text": "3.4 Uncertainty prediction ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Finally, we consider uncertainty prediction. till now we have defined conditional probability paths using a fixed noise parameter $\\sigma$ . However, this does not have to be fixed. Instead, we consider a learned $\\sigma_{\\theta}(t,x_{t})$ which can be learned iteratively with the loss: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{uncertainty}}(\\theta,x)=\\sum_{t\\in\\mathcal{T}}\\left\\|\\sigma_{\\theta}(t,x_{t})-\\|\\hat{x}_{\\theta}(t,x_{t})-x_{\\mathrm{next}}\\|_{2}^{2}\\right\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "which learns to predict the error in the estimate of $x_{t}$ . This loss can be interpreted as training an epistemic uncertainty predictor which is similar to that proposed in direct epistemic uncertainty prediction (DEUP) [Lahlou et al., 2023]. ", "page_idx": 4}, {"type": "image", "img_path": "fNakQltI1N/tmp/06f7fdcf3e6b81b978dc465a159ec5800c134f9a237dd76622dbf06b4ec08de0.jpg", "img_caption": ["Figure 2: 1D harmonic oscillator overftiting experiment results. Left: TFM-ODE (ours) with memory $=3$ . Middle: TFM-ODE (ours) without memory. Right: Aligned FM [Liu et al., 2023a, Somnath et al., 2023]. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "4 Experimental Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section we empirically evaluate the performance of the trajectory flow matching objective in terms of time series modeling error, but also uncertainty quantification. We also evaluate a variety of simulation-based and simulation-free methods including both stochastic and deterministic methods. Stochastic methods are in general more difficult to fit, but can be used to better model uncertainty and variance. Further experimental details can be found in $\\S B$ . Experiments were run on a computing cluster with a heterogenous cluster of NVIDIA RTX8000, V100, A40, and A100 GPUs for approximately 24,000 GPU hours. Individual training runs require approximately one gpu day. ", "page_idx": 5}, {"type": "text", "text": "Baselines In addition to different ablations of trajectory flow matching, we also evaluate NeuralODE [Chen et al., 2018], NeuralSDE [Li et al., 2020, Kidger et al., 2021b, Kidger, 2022], Latent NeuralODE [Rubanova et al., 2019], and an aligned flow matching method (Aligned FM) [Liu et al., 2023a, Somnath et al., 2023] where the couplings are sampled according to the ground truth coupling during training. ", "page_idx": 5}, {"type": "text", "text": "Metrics We primarily make use of two metrics. The average mean-squared-error (Mean MSE) over left out time series to measure the time series modeling error defined as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{MSE}(\\hat{x},x)=\\frac{1}{T-1}\\sum_{t\\in[2,T]}\\|\\hat{x}_{t}-x_{t}\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\hat{x}$ and $x$ are the predicted and true trajectories respectively. We also use the maximum mean discrepancy with a radial basis function kernel (RBF MMD) which measures how well the distribution over next observation is modelled by comparing the predicted distribution to the distribution over next states in the ground truth trajectory. Specifically we compute: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{RBF-MMD}(\\theta,\\hat{x},x):=\\frac{1}{T-1}\\sum_{t\\in[2,T]}\\mathrm{MMD}(\\hat{\\Delta}_{t},\\Delta_{t})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\hat{\\Delta}_{t}=\\hat{x}_{t}-x_{t-1}$ , $\\Delta_{t}=x_{t}-x_{t-1}$ , and $\\begin{array}{r}{\\hat{x}_{t}:=\\int_{s=t-1}^{t}f_{\\theta}(s,x_{s})d s+g_{\\theta}(t,x_{s})d W_{s}}\\end{array}$ is a set of samples from the model prediction at time $t$ . ", "page_idx": 5}, {"type": "text", "text": "4.1 Exploring coupling preservation with 1D harmonic oscillators ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We begin by evaluating how trajectory flow matching performs in a simple one dimensional setting of harmonic oscillators. We show that vanilla conditional flow and bridge matching [Liu et al., 2023c,b, Albergo and Vanden-Eijnden, 2023], specifically aligned approaches [Somnath et al., 2023, Liu et al., 2023a] are unable to preserve the coupling even in a simple one dimensional setting. However, augmented with our trajectory flow matching approach, and specifically using (A2), which includes information on previous observations, the model is able to fit the harmonic oscillator dataset well. ", "page_idx": 5}, {"type": "text", "text": "The harmonic oscillator dataset consists of one-dimensional oscillatory trajectories from a damped harmonic oscillator, with each trajectory distinguished by a unique damping coefficient $c$ . Specifically we sample trajectories $x$ from: ", "page_idx": 5}, {"type": "equation", "text": "$$\nx_{i}=x_{i-1}+v_{i-1}(t_{i}-t_{i-1});\\quad x_{0}=1\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $v$ is the velocity of the oscillator updated by ", "page_idx": 6}, {"type": "equation", "text": "$$\nv_{i}=v_{i-1}+\\left(-{\\frac{c}{m}}v_{i-1}-{\\frac{k}{m}}x_{i-1}\\right)(t_{i}-t_{i-1});\\quad v_{0}=0\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "with $t_{i}=0.1\\cdot i$ for $i=0,1,2,\\ldots,99$ , spring constant $k=1$ , and mass $m=1$ ", "page_idx": 6}, {"type": "text", "text": "As $c$ increases, the trajectories evolve from underdamped scenarios with prolonged oscillations to critically and overdamped states where the oscillator quickly stabilizes. This leads to intersecting trajectories due to frequency and phase differences, despite their shared starting point. We perform overfitting experiments on three trajectories generated by varying $c$ . ", "page_idx": 6}, {"type": "text", "text": "As shown in Figure 2, models without history information are unable to distinguish between the three crossing trajectories that share the same starting point, resulting in overlapping predictions. In contrast, TFM-ODE that incorporates three previous observations is able to fti the crossing trajectories with high accuracy, with the predicted trajectories almost completely overlapping the ground truth. This is because the dataset with satisfies (A2) with $h=4$ (TFM-ODE), but not $h=0$ (TFM-ODE no memory and Aligned FM). ", "page_idx": 6}, {"type": "text", "text": "4.2 Experiments on clinical datasets ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Next we compared the performance of TFM and TFM-ODE with the current SDE and ODE baselines, respectively, for modeling real-world patient trajectories formed with heart rate and mean arterial blood pressure measurements within the first 24 hours of admission across three different datasets. These are clinical measurements that are taken most frequently and used to evaluate the hemodynamic status of patients, a key indicator of disease severity. Additionally, we evaluated our models against flow matching on these datasets, each with distinct characteristics, to assess their ability to generalize across different distributions. A full description of the datasets are available in Appendix B.2 with the publicly available datasets used under The PhysioNet Credentialed Health Data License Version 1.5.0 and the EHR dataset with local institutional IRB approval: ", "page_idx": 6}, {"type": "text", "text": "\u2022 ICU Sepsis: a subset of the eICU Collaborative Research Database v2.0 [Pollard et al., 2019] of patients admitted with sepsis as the primary diagnosis   \n\u2022 ICU Cardiac Arrest: a subset of the eICU Collaborative Research Database v2.0 [Pollard et al., 2019] of patients at risk for cardiac arrest   \n\u2022 ICU GIB: a subset of the Medical Information Mart for Intensive Care III [Johnson et al., 2016] of patients with gastrointestinal bleeding as the primary diagnosis   \n\u2022 ED GIB: patients presenting with signs and symptoms of acute gastrointestinal bleeding to the emergency department of a large tertiary care academic health system ", "page_idx": 6}, {"type": "text", "text": "4.2.1 Prediction accuracy and precision: TFM and TFM-ODE ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "TFM-ODE yields more accurate trajectory prediction Across the three datasets TFM-ODE outperformed the baseline models by $15\\%$ to $20\\%$ , as seen in table 1. We noticed that TFM has a similar performance as TFM-ODE. In one case TFM outperformed the non-stochastic TFM-ODE, as seen in the ICU GIB dataset. For ICU sepsis, the performance improvement from the baseline is the most significant, around $83\\%$ . This coincides with the ICU sepsis dataset having the most amount of measurement per trajectory. The improvement is seen in both TFM and TFM-ODE, possibly indicating they are able to learn better given more data, resulting in a more precise flow. Not formally measured, we noted that given the same time constraint, FM based models were significantly faster and often finished training before the time limit. ", "page_idx": 6}, {"type": "text", "text": "TFM yields better uncertainty prediction Though TFM-ODE had lower test MSE for two out of three times, TFM yielded better uncertainty prediction overall, as seen in table 2. Notably, TFM also had less variance in the uncertainty prediction than TFM-ODE. A plausible explanation in this case is a sacrifice in bias that subsequently decreases the variance for the stochastic implementation, reflecting the bias-variance trade off. Sampled graphs of TFM can be seen in figure 3. It is notable that the model is able to detect the measurement uncertainty at certain timepoints, matching the increase in amplitude of oscillation in patient trajectories. ", "page_idx": 6}, {"type": "text", "text": "4.2.2 Trajectory Variance Distribution Comparison ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "TFM trajectories accurately match the noise distribution in the data TFM is able to match the noise distribution in addition to the overall trajectory shape, which is useful in settings where ", "page_idx": 6}, {"type": "text", "text": "Table 1: Mean \u00b1 Std. deviation MSE $(\\times10^{-3})$ by models and datasets. Split into deterministic (top) and stochastic models (bottom). Top performing model for each setting and dataset in bold. ", "page_idx": 7}, {"type": "table", "img_path": "fNakQltI1N/tmp/75a5d6503258d1a9daf3a81e6c675e35ecca4bb9c85904bedd42ba234e18692f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "fNakQltI1N/tmp/63d00f090c8ed38fbee9d16512afc9046c76072e2c68a5268b9977bcfa23e7ee.jpg", "table_caption": ["Table 2: Uncertainty test MSE loss for TFM-ODE and TFM with two different ICU datasets. "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "fNakQltI1N/tmp/e0e2254e718bf1bf2d1cbb49866d7c3cf0dcf38b6e296701bdc29a432f50be7e.jpg", "img_caption": ["Figure 3: Three samples from predicted trajectory and uncertainty on ICU GIB test set. Top: Predicted (orange) and the ground truth (blue) mean arterial pressure (MAP). Bottom: The absolute value of the uncertainty predicted by TFM. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "data has high stochasticity. We compared our models to NeuralODE and NeuralSDE in matching the variance in neighboring data points, seen in table 3. We verify that between the baseline NeuralSDE and NeuralODE, NeuralSDE has a lower MMD and is better able to match data points. We find in ICU GIB and ED GIB datasets, TFM outperforms both in matching the variance in data. Notably, the performance pattern is reversed for the MMD metrics and mean MSE metrics with respect to TFM and TFM-ODE where better MSE leads to worse MMD and vice versa. As such, this further confirms the bias-variance trade-off for both TFM and TFM-ODE implementation. ", "page_idx": 7}, {"type": "table", "img_path": "fNakQltI1N/tmp/57cca174fc18b6783af96b79ed96beb77f2c0b7407abcf01eb5b9b57d6add958.jpg", "table_caption": ["Table 3: Data variance MMD for by models and datasets. Split into deterministic models (top) and stochastic models (bottom). Top performing model for each setting and dataset in bold. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.2.3 Ablation Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We performed ablation studies on TFM and TFM-ODE to attribute importance of various model components contributing to the performance, as seen in table 4. We examined three aspects of the model, two of which were part of our main contributions: uncertainty prediction and memory. We also ablate the model hidden dimension width to infer its potential in scaling effect. ", "page_idx": 7}, {"type": "text", "text": "TFM and TFM-ODE performance scales with model size In contrast to Neural DE based models, TFM and TFM-ODE exhibit scaling effect, in which the model performance becomes better with a larger hidden dimension. This has been observed in previous flow matching models in image generation [Tong et al., 2024]. This may pave the way for further improvements from larger models. ", "page_idx": 7}, {"type": "table", "img_path": "fNakQltI1N/tmp/72c694de2d911b608ae80fce9414e842ee2aff054b81561994067ca131e557ba.jpg", "table_caption": ["Table 4: Mean MSE $(\\times10^{-3})$ ) by ablated versions of TFM, TFM-ODE, and datasets. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Uncertainty improves performance of trajectory prediction For TFM and TFM-ODE, the flow network used to learn the uncertainty $\\sigma_{x_{t}}$ is separate from the flow network learning $x_{t}$ . The loss function of the network learning $x_{t}$ is independent of uncertainty flow network. Therefore, it was unexpected that taking away the uncertainty prediction would result in increased MSE test loss for learning $x_{t}$ . This implies further a process in the synergistic effects between $x_{t}$ flow and $\\sigma_{x_{t}}$ flow. ", "page_idx": 8}, {"type": "text", "text": "Trajectory memory may improve performance in high frequency measurement settings We conditioned the model based on a sliding window of trajectory history to disentangle data points that otherwise look indistinguishable to FM models. This improved the interpolation performance in the ICU Sepsis and ICU GIB dataset. Notably, this modification did not improve performance the ED GIB dataset, which could be due to shorter trajectories for patients and lower measurement frequency in the defined time period. This may also be explained by the decreased severity of disease in the ED compared to the ICU. Adding memory as a condition may be more suitable for patients whose clinical trajectories have a higher frequency of measurements. ", "page_idx": 8}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Continuous-time neural network architectures have outperformed traditional RNN methods in modeling irregularly sampled clinical time series to optimize interpolation and extrapolation. Neural ODE with latent representations of trajectories [Rubanova et al., 2019] outperformed RNN-based approaches [Lipton et al., 2016, Che et al., 2018, Cao et al., 2018, Rajkomar et al., 2018] for interpolation while providing explicit uncertainty estimates about latent states. More recently, Neural SDEs appear to outperform LSTM [Hochreiter and Schmidhuber, 1997], Neural ODE [Chen et al., 2018, De Brouwer et al., 2019, Dupont et al., 2019, Lechner and Hasani, 2020], and attention-based [Shukla and Marlin, 2021, Lee et al., 2022] approaches in interpolation performance while natively handling uncertainty using drift and diffusion terms [Oh et al., 2024]. ", "page_idx": 8}, {"type": "text", "text": "Discrete-time approaches offer an alternative to our continuous-time model model transformers utilize a discrete-time representation with a sequential processing [Gao et al., 2024, Nie et al., 2023, Woo et al., 2024, Ansari et al., 2024, Dong et al., 2024, Garza and Mergenthaler-Canseco, 2023, Das et al., 2024, Liu et al., 2024, Kuvshinova et al., 2024] models for traditional time series modeling. Adaptations to the baseline transformer includes structuring observations into text with finetuning [Zhang et al., 2023, Zhou et al., 2023], without finetuning [Xue and Salim, 2024, Gruver et al., 2023], or using autoregressive model vision transformers to model unevenly spaced time series data by converting time series into images [Li et al., 2023]. ", "page_idx": 8}, {"type": "text", "text": "Continuous-time systems are of great interest for learning causal representations using assumptions by using observations to directly modify the system state [De Brouwer et al., 2022, Jia and Benson, 2019]. Variations include intervention modeling with separate ODEs for interventions and outcome processes [Gwak et al., 2020], using liquid time-constant networks [Hasani et al., 2021, Vorbach et al., 2021], or modeling treatment effects with either one [Bellot and van der Schaar, 2021] or multiple interventions [Seedat et al., 2022]. The importance of accounting for external interventions is a particular challenge in clinical data, where external interventions (change in environment due to treatment decisions or clinical context such as ED or ICU) are common in clinical data trajectories. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this work we present Trajectory Flow Matching, a simulation-free training algorithm for neural differential equation models. We show when trajectory flow matching is valid theoretically, then demonstrate its usefulness empirically in a clinical setting. The ability to model the underlying continuous physiologic processes during critical illness using irregular, sparsely sampled, and noisy data has the potential for broad impacts in care settings such as the emergency department or ICU. These models could be used to improve clinical decision making, inform monitoring strategies, and optimize resource allocation by identifying which patients are likely to deteriorate or recover. These use cases will require thorough prospective validation and calibration for specific clinical outcomes, for example using the likelihood of a patient crossing a specific heart rate or blood pressure threshold for decisions on level of care (ICU versus inpatient floors) or specific interventions such as transfusions. In these applications, it will be important to assess and control for bias that may be present due to which patient subpopulations are present in training data. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Limitations Limitations of the method includes the selective utility of integrating memory in clinical settings with high measurement frequency and no current capacity for estimating causal representations, though this will be an important future research direction. Potential harms include the following: erroneous predictions that either results in delayed care or overutilization of the health system. Accurate trajectory predictions have the potential to inform clinical decision-making regarding the appropriate level of care, leading to more timely and appropriate interventions. ", "page_idx": 9}, {"type": "text", "text": "Future work We hope to extend our method to cover other types of time series that have periodicity in the components, potentially incorporating Fourier transform [Li et al., 2021] and Physics-Inspired Neural Networks (PINN). Since interpretability is an important factor for clinical reliability, we are developing methods to further elucidate key components affecting the prediction. As well, we hope to incorporate functional flow matching for fully continuous setting [Kerrigan et al., 2024]. ", "page_idx": 9}, {"type": "text", "text": "7 Broader Impact ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our work extends flow matching into the domain of time series modeling, demonstrating a specific instance of clinical time series prediction. In contrast to the large transformer-based models, our method has fewer in parameters and less training time needed. Notably, it scales well with parameters. As well, our parameterization on Stochastic Differential Equations (SDE) allow faster training time than traditional SDE integration. ", "page_idx": 9}, {"type": "text", "text": "Accurate timeseries modeling in healthcare has the potential for significant benefits, but also introduces risks. Benefits that could be derived from more accurate prediction of clinical courses include improved treatment decisions, resource allocation, as well as more informative discussions of prognosis with patients or family members. Risks may come from inaccuracies in predictions which could lead to harms by biasing decision making of clinical teams. In the general case of false negative prediction (prediction of trajectories with falsely favorable outcomes) this may lead to undertreatment and in the case of false positive prediction (prediction of trajectories with incorrect detrimental outcomes) or overtreating patients. These inaccuracies may also propagate biases in training data. ", "page_idx": 9}, {"type": "text", "text": "To move towards broad impact in the clinical domain, this work will require validation and bias estimates. Furthermore, models deployed in domains with high-stakes prediction require interpretability, which can help identify biases, miscalibration, discordance with domain knowledge, as well as build trust with teams using predictions from the model. At this time, flow-based methods have limited tools for interpretability, and we recognize this as a gap in need of future work. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The authors would like to thank Mathieu Blanchette for useful comments on early versions of this manuscript. We are also grateful to the anonymous reviewers for suggesting numerous improvements. The authors acknowledge funding from the National Institutes of Health, UNIQUE, CIFAR, NSERC, Intel, and Samsung. The research was enabled in part by computational resources provided by the Digital Research Alliance of Canada (https://alliancecan.ca), Mila (https://mila.quebec), Yale School of Medicine and NVIDIA. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "M. S. Albergo and E. Vanden-Eijnden. Building normalizing flows with stochastic interpolants. In The Eleventh International Conference on Learning Representations, 2023. URL https: //openreview.net/forum?id=li7qeBbCR1t. ", "page_idx": 9}, {"type": "text", "text": "M. S. Albergo, N. M. Boff,i and E. Vanden-Eijnden. Stochastic interpolants: A unifying framework for flows and diffusions. CoRR, abs/2303.08797, 2023. URL https://doi.org/10.48550/ arXiv.2303.08797.   \nA. Allam, S. Feuerriegel, M. Rebhan, and M. Krauthammer. Analyzing patient trajectories with artificial intelligence. J Med Internet Res, 23(12):e29812, Dec 2021. ISSN 1438-8871. doi: 10.2196/29812. URL https://www.jmir.org/2021/12/e29812.   \nA. F. Ansari, L. Stella, C. Turkmen, X. Zhang, P. Mercado, H. Shen, O. Shchur, S. S. Rangapuram, S. Pineda Arango, S. Kapoor, J. Zschiegner, D. C. Maddix, H. Wang, M. W. Mahoney, K. Torkkola, A. Gordon Wilson, M. Bohlke-Schneider, and Y. Wang. Chronos: Learning the language of time series. arXiv preprint arXiv:2403.07815, 2024.   \nA. Bellot and M. van der Schaar. Policy analysis using synthetic controls in continuous-time. In M. Meila and T. Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 759\u2013768. PMLR, 18\u201324 Jul 2021. URL https://proceedings.mlr.press/v139/bellot21a.html.   \nW. Cao, D. Wang, J. Li, H. Zhou, L. Li, and Y. Li. Brits: Bidirectional recurrent imputation for time series. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper_files/paper/2018/ file/734e6bfcd358e25ac1db0a4241b95651-Paper.pdf.   \nZ. Che, S. Purushotham, K. Cho, D. Sontag, and Y. Liu. Recurrent neural networks for multivariate time series with missing values. Scientific Reports, 8(1), 2018. doi: 10.1038/s41598-018-24271-9.   \nR. T. Q. Chen, Y. Rubanova, J. Bettencourt, and D. K. Duvenaud. Neural ordinary differential equations. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper_files/paper/2018/ file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf.   \nT. Chen and C. Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining, pages 785\u2013794, 2016.   \nM. M. Churpek, T. C. Yuen, S. Y. Park, D. O. Meltzer, J. B. Hall, and D. P. Edelson. Derivation of a cardiac arrest prediction model using ward vital signs. Critical Care Medicine, 2012.   \nA. Das, W. Kong, R. Sen, and Y. Zhou. A decoder-only foundation model for time-series forecasting. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview. net/forum?id=jn2iTJas6h.   \nE. De Brouwer, J. Simm, A. Arany, and Y. Moreau. Gru-ode-bayes: Continuous modeling of sporadically-observed time series. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9- Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_ files/paper/2019/file/455cb2657aaa59e32fad80cb0b65b9dc-Paper.pdf.   \nE. De Brouwer, J. Gonzalez, and S. Hyland. Predicting the impact of treatments over time with uncertainty aware neural differential equations. In G. Camps-Valls, F. J. R. Ruiz, and I. Valera, editors, Proceedings of The 25th International Conference on Artificial Intelligence and Statistics, volume 151 of Proceedings of Machine Learning Research, pages 4705\u20134722. PMLR, 28\u201330 Mar 2022. URL https://proceedings.mlr.press/v151/de-brouwer22a.html.   \nJ. Dong, H. Wu, Y. Wang, Y.-Z. Qiu, L. Zhang, J. Wang, and M. Long. Timesiam: A pre-training framework for siamese time-series modeling. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id $\\equiv$ wrTzLoqbCg.   \nE. Dupont, A. Doucet, and Y. W. Teh. Augmented neural odes. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/file/ 21be9a4bd4f81549a9d1d241981cec3c-Paper.pdf. ", "page_idx": 10}, {"type": "text", "text": "S. Gao, T. Koker, O. Queen, T. Hartvigsen, T. Tsiligkaridis, and M. Zitnik. Units: Building a unified time series model. arXiv, 2024. URL https://arxiv.org/pdf/2403.00131.pdf. ", "page_idx": 11}, {"type": "text", "text": "A. Garza and M. Mergenthaler-Canseco. Timegpt-1, 2023.   \nN. Gruver, M. Finzi, S. Qiu, and A. G. Wilson. Large language models are zero-shot time series forecasters. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 19622\u201319635. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/ file/3eb7ca52e8207697361b2c0fb3926511-Paper-Conference.pdf.   \nD. Gwak, G. Sim, M. Poli, S. Massaroli, J. Choo, and E. Choi. Neural Ordinary Differential Equations for Intervention Modeling. arXiv e-prints, art. arXiv:2010.08304, Oct. 2020. doi: 10.48550/arXiv.2010.08304.   \nR. Hasani, M. Lechner, A. Amini, D. Rus, and R. Grosu. Liquid time-constant networks. Proceedings of the AAAI Conference on Artificial Intelligence, 35(9):7657\u20137666, May 2021. doi: 10.1609/aaai. v35i9.16936. URL https://ojs.aaai.org/index.php/AAAI/article/view/16936.   \nJ. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840\u20136851, 2020.   \nS. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735\u20131780, 1997. ISSN 1530-888X. doi: 10.1162/neco.1997.9.8.1735. URL http://dx.doi.org/10. 1162/neco.1997.9.8.1735.   \nP. Isola, J. Zhu, T. Zhou, and A. A. Efros. Image-to-image translation with conditional adversarial networks. CVPR, 2017.   \nJ. Jia and A. R. Benson. Neural jump stochastic differential equations. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/file/ 59b1deff341edb0b76ace57820cef237-Paper.pdf.   \nA. E. Johnson, T. J. Pollard, L. Shen, L.-w. H. Lehman, M. Feng, M. Ghassemi, B. Moody, P. Szolovits, L. Anthony Celi, and R. G. Mark. Mimic-iii, a freely accessible critical care database. Scientific data, 3(1):1\u20139, 2016.   \nG. Kerrigan, G. Migliorini, and P. Smyth. Functional flow matching. In S. Dasgupta, S. Mandt, and Y. Li, editors, Proceedings of The 27th International Conference on Artificial Intelligence and Statistics, volume 238 of Proceedings of Machine Learning Research, pages 3934\u20133942. PMLR, 02\u201304 May 2024. URL https://proceedings.mlr.press/v238/kerrigan24a.html.   \nP. Kidger. On neural differential equations, 2022. URL https://arxiv.org/abs/2202.02435.   \nP. Kidger, J. Foster, X. Li, H. Oberhauser, and T. Lyons. Neural sdes as infinite-dimensional gans. In International conference on machine learning. PMLR, 2021a.   \nP. Kidger, J. Foster, X. C. Li, and T. Lyons. Efficient and accurate gradients for neural sdes. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 18747\u201318761. Curran Associates, Inc., 2021b. URL https://proceedings.neurips.cc/paper_files/paper/2021/file/ 9ba196c7a6e89eafd0954de80fc1b224-Paper.pdf.   \nK. Kuvshinova, O. Tsymboi, A. Kostromina, D. Simakov, and E. Kovtun. Towards foundation time series model: To synthesize or not to synthesize? arXiv preprint arXiv:2403.02534, 2024.   \nS. Lahlou, M. Jain, H. Nekoei, V. I. Butoi, P. Bertin, J. Rector-Brooks, M. Korablyov, and Y. Bengio. DEUP: Direct epistemic uncertainty prediction. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/forum?id=eGLdVRvvfQ. Expert Certification.   \nM. Lechner and R. Hasani. Learning long-term dependencies in irregularly-sampled time series. arXiv preprint arXiv:2006.04418, 2020.   \nY. Lee, E. Jun, J. Choi, and H.-I. Suk. Multi-view integrative attention-based deep representation learning for irregular clinical time-series data. IEEE Journal of Biomedical and Health Informatics, 26(8):4270\u20134280, 2022. doi: 10.1109/JBHI.2022.3172549.   \nX. Li, T.-K. L. Wong, R. T. Q. Chen, and D. K. Duvenaud. Scalable gradients and variational inference for stochastic differential equations. In C. Zhang, F. Ruiz, T. Bui, A. B. Dieng, and D. Liang, editors, Proceedings of The 2nd Symposium on Advances in Approximate Bayesian Inference, volume 118 of Proceedings of Machine Learning Research, pages 1\u201328. PMLR, 08 Dec 2020. URL https://proceedings.mlr.press/v118/li20a.html.   \nZ. Li, N. B. Kovachki, K. Azizzadenesheli, B. liu, K. Bhattacharya, A. Stuart, and A. Anandkumar. Fourier neural operator for parametric partial differential equations. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id $\\cdot$ c8P9NQVtmnO.   \nZ. Li, S. Li, and X. Yan. Time series as images: Vision transformer for irregularly sampled time series. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 49187\u201349204. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/ file/9a17c1eb808cf012065e9db47b7ca80d-Paper-Conference.pdf.   \nY. Lipman, R. T. Q. Chen, H. Ben-Hamu, M. Nickel, and M. Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id $\\equiv$ PqvMRDCJT9t.   \nZ. C. Lipton, D. Kale, and R. Wetzel. Directly modeling missing data in sequences with rnns: Improved classification of clinical time series. In F. Doshi-Velez, J. Fackler, D. Kale, B. Wallace, and J. Wiens, editors, Proceedings of the 1st Machine Learning for Healthcare Conference, volume 56 of Proceedings of Machine Learning Research, pages 253\u2013270, Northeastern University, Boston, MA, USA, 18\u201319 Aug 2016. PMLR. URL https://proceedings.mlr.press/v56/ Lipton16.html.   \nG.-H. Liu, A. Vahdat, D.-A. Huang, E. A. Theodorou, W. Nie, and A. Anandkumar. I2sb: imageto-image schr\u00f6dinger bridge. In Proceedings of the 40th International Conference on Machine Learning, ICML\u201923. JMLR.org, 2023a.   \nX. Liu, C. Gong, and Q. Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. The Eleventh International Conference on Learning Representations (ICLR), 2023b. URL https://par.nsf.gov/biblio/10445517.   \nX. Liu, L. Wu, M. Ye, and qiang liu. Learning diffusion bridges on constrained domains. In The Eleventh International Conference on Learning Representations, 2023c. URL https:// openreview.net/forum?id=WH1yCa0TbB.   \nY. Liu, H. Zhang, C. Li, X. Huang, J. Wang, and M. Long. Timer: Transformers for Time Series Analysis at Scale. arXiv e-prints, art. arXiv:2402.02368, Feb. 2024. doi: 10.48550/arXiv.2402. 02368.   \nA. Q. Nichol and P. Dhariwal. Improved denoising diffusion probabilistic models. In M. Meila and T. Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 8162\u20138171. PMLR, 18\u201324 Jul 2021. URL https://proceedings.mlr.press/v139/nichol21a.html.   \nY. Nie, N. H. Nguyen, P. Sinthong, and J. Kalagnanam. A time series is worth 64 words: Longterm forecasting with transformers. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id $\\equiv$ Jbdc0vTOcol.   \nY. Oh, D. Lim, and S. Kim. Stable neural stochastic differential equations in analyzing irregular time series data. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=4VIgNuQ1pY. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "T. Pollard, A. Johnson, J. Raffa, L. A. Celi, O. Badawi, and R. Mark. eicu collaborative research database (version 2.0), 2019. URL https://doi.org/10.13026/C2WM1R. ", "page_idx": 13}, {"type": "text", "text": "A.-A. Pooladian, H. Ben-Hamu, C. Domingo-Enrich, B. Amos, Y. Lipman, and R. T. Q. Chen. Multisample flow matching: straightening flows with minibatch couplings. In Proceedings of the 40th International Conference on Machine Learning, ICML\u201923. JMLR.org, 2023.   \nA. Rajkomar, E. Oren, K. Chen, A. M. Dai, N. Hajaj, M. Hardt, P. J. Liu, X. Liu, J. Marcus, M. Sun, P. Sundberg, H. Yee, K. Zhang, Y. Zhang, G. Flores, G. E. Duggan, J. Irvine, Q. Le, K. Litsch, A. Mossin, J. Tansuwan, D. Wang, J. Wexler, J. Wilson, D. Ludwig, S. L. Volchenboum, K. Chou, M. Pearson, S. Madabushi, N. H. Shah, A. J. Butte, M. D. Howell, C. Cui, G. S. Corrado, and J. Dean. Scalable and accurate deep learning with electronic health records. npj Digital Medicine, 1(1):18, 2018. doi: 10.1038/s41746-018-0029-1.   \nY. Rubanova, R. T. Q. Chen, and D. K. Duvenaud. Latent ordinary differential equations for irregularly-sampled time series. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9- Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_ files/paper/2019/file/42a6845a557bef704ad8ac9cb4461d43-Paper.pdf.   \nN. Seedat, F. Imrie, A. Bellot, Z. Qian, and M. van der Schaar. Continuous-time modeling of counterfactual outcomes using neural controlled differential equations. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 19497\u201319521. PMLR, 17\u201323 Jul 2022. URL https://proceedings.mlr.press/v162/ seedat22b.html.   \nY. Shi, V. De Bortoli, A. Campbell, and A. Doucet. Diffusion schr\u00f6dinger bridge matching. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 62183\u201362223. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/ c428adf74782c2092d254329b6b02482-Paper-Conference.pdf.   \nS. N. Shukla and B. Marlin. Multi-time attention networks for irregularly sampled time series. In International Conference on Learning Representations, 2021. URL https://openreview.net/ forum?id=4c0J6lwQ4_.   \nV. R. Somnath, M. Pariset, Y.-P. Hsieh, M. R. Martinez, A. Krause, and C. Bunne. Aligned diffusion Schr\u00f6dinger bridges. In R. J. Evans and I. Shpitser, editors, Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence, volume 216 of Proceedings of Machine Learning Research, pages 1985\u20131995. PMLR, 31 Jul\u201304 Aug 2023. URL https://proceedings. mlr.press/v216/somnath23a.html.   \nY. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id $\\equiv$ PxTIG12RRHS.   \nA. Tong, K. FATRAS, N. Malkin, G. Huguet, Y. Zhang, J. Rector-Brooks, G. Wolf, and Y. Bengio. Improving and generalizing flow-based generative models with minibatch optimal transport. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL https: //openreview.net/forum?id $\\cdot$ CD9Snc73AW. Expert Certification.   \nC. Vorbach, R. Hasani, A. Amini, M. Lechner, and D. Rus. Causal navigation by continuous-time neural networks. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 12425\u201312440. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/ file/67ba02d73c54f0b83c05507b7fb7267f-Paper.pdf.   \nG. Woo, C. Liu, A. Kumar, C. Xiong, S. Savarese, and D. Sahoo. Unified training of universal time series forecasting transformers. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=Yd8eHMY1wz. ", "page_idx": 13}, {"type": "text", "text": "H. Xue and F. D. Salim. PromptCast: A New Prompt-Based Learning Paradigm for Time Series Forecasting . IEEE Transactions on Knowledge & Data Engineering, 36(11):6851\u2013 6864, Nov. 2024. ISSN 1558-2191. doi: 10.1109/TKDE.2023.3342137. URL https: //doi.ieeecomputersociety.org/10.1109/TKDE.2023.3342137. Y. Zhang, K. Gong, K. Zhang, H. Li, Y. Qiao, W. Ouyang, and X. Yue. Meta-transformer: A unified framework for multimodal learning. arXiv preprint arXiv:2307.10802, 2023. T. Zhou, P. Niu, x. wang, L. Sun, and R. Jin. One fits all: Power general time series analysis by pretrained lm. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 43322\u201343355. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/ file/86c17de05579cde52025f9984e6e2ebb-Paper-Conference.pdf. J. E. Zimmerman, A. A. Kramer, D. S. McNair, and F. M. Malila. Acute physiology and chronic health evaluation (apache) iv: hospital mortality assessment for today\u2019s critically ill patients. Critical care medicine, 34(5):1297\u20131310, 2006. ", "page_idx": 14}, {"type": "text", "text": "A Proof of theorems ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We first prove a Lemma which shows TFM learns valid flows between distributions with the target prediction reparameterization trick. ", "page_idx": 15}, {"type": "text", "text": "Lemma A.1. If $p_{t}(x)>0,$ , $\\delta_{d a t a}$ is Lipschitz continuous for all $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ and $t\\in[0,1]$ , ${\\mathcal{L}}_{F M}$ and ${\\mathcal{L}}_{T F M}$ are equal, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}_{F M}(\\boldsymbol{\\theta})=\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}_{T F M}(\\boldsymbol{\\theta})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. This proof is a simple extension of Lipman et al. [2023], Tong et al. [2024] which proved ${\\mathcal{L}}_{C F M}$ and ${\\mathcal{L}}_{F M}$ are equal under similar constraint. ", "page_idx": 15}, {"type": "text", "text": "Given $\\delta_{d a t a}=t_{1}-t_{0}$ , we have $\\begin{array}{r}{u_{t}(x)=\\frac{x_{1}-x_{0}}{\\delta_{d a t a}}}\\end{array}$ where $t_{0}$ is the previous time in the time series, and $t_{1}$ is the current time for inference. For the time series data, we are assuming it to be Lipschitz continuous there exist $L\\geq0$ such that for all $x,y\\in\\mathbb{R}^{n}$ , $|f(x)-f(y)|\\leq L\\|x-y\\|$ . ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}\\mathbb{E}_{p_{t}(x)}\\|v_{\\theta}(t,x)-u_{t}(x)\\|^{2}=\\mathbb{E}_{t,q(z),p_{t}(x|z)}\\frac{1}{(1-t)^{2}}\\|\\hat{x}_{\\theta}^{1}(t,x)-x_{1}\\|^{2}~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~(18\\theta)^{2}}\\\\ {=\\nabla_{\\theta}\\mathbb{E}_{t,q(z),p_{t}(x|z)}\\frac{1}{(1-t)^{2}}\\left(\\ \\|\\hat{x}_{\\theta}^{1}(t,x)\\|^{2}-2\\left\\langle\\hat{x}_{\\theta}^{1}(t,x),x_{1}\\right\\rangle+x_{1}^{2}\\right)}\\\\ {(19\\theta)}\\\\ {=\\nabla_{\\theta}\\mathbb{E}_{t,q(z),p_{t}(x|z)}\\left(\\ \\frac{1}{(1-t)^{2}}\\|\\hat{x}_{\\theta}^{1}(t,x)\\|^{2}-2\\left\\langle\\hat{x}_{\\theta}^{1}(t,x),x_{1}\\right\\rangle\\right)~~~~~~~~~~~}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By bilinearity of the 2-norm and since $x_{1}$ is independent of $\\theta$ . Next, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}_{p_{t}(x)}\\frac{1}{(1-t)^{2}}\\|\\hat{x}_{\\theta}^{1}(t,x)\\|^{2}=\\displaystyle\\int\\|\\hat{x}_{\\theta}^{1}(t,x)\\|^{2}p_{t}(x)d x}\\\\ {\\displaystyle=\\iint\\|\\hat{x}_{\\theta}^{1}(t,x)\\|^{2}p_{t}(x|z)q(z)d z d x}\\\\ {\\displaystyle=\\mathbb{E}_{q(z),p_{t}(x|z)}\\|\\hat{x}_{\\theta}^{1}(t,x)\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Finally, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{p_{t}(x)}\\left\\langle\\hat{x}_{\\theta}^{1}(t,x),x_{1}\\right\\rangle=\\displaystyle\\int\\left\\langle\\hat{x}_{\\theta}^{1}(t,x),\\frac{\\int x_{1}p_{t}(x|z)q(z)d z}{p_{t}(x)}\\right\\rangle p_{t}(x)d x}\\\\ &{\\phantom{x x}=\\displaystyle\\int\\left\\langle\\hat{x}_{\\theta}^{1}(t,x),\\int x_{1}p_{t}(x|z)q(z)d z\\right\\rangle d x}\\\\ &{\\phantom{x x}=\\displaystyle\\iint\\left\\langle\\hat{x}_{\\theta}^{1}(t,x),x_{1}\\right\\rangle p_{t}(x|z)q(z)d z d x}\\\\ &{\\phantom{x x}=\\mathbb{E}_{q(z),p_{t}(x|z)}\\left\\langle\\hat{x}_{\\theta}^{1}(t,x),x_{1}\\right\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Where we first substitute then change the order of integration for the final equality. Since at all times $t$ the gradients of ${\\mathcal{L}}_{\\mathrm{FM}}$ and ${\\mathcal{L}}_{T F M}$ are equal, $\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}_{\\mathrm{FM}}\\bar{(\\boldsymbol{\\theta})}=\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}_{T F M}$ ", "page_idx": 15}, {"type": "text", "text": "by substitution. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{L}\\mathbb{E}_{t,q(z),p_{t}(x|z)}\\|v_{\\theta}(t,x)-u_{t}(x|z)\\|^{2}=\\mathbb{E}_{t,q(z),p_{t}(x|z)}\\frac{1}{(\\lceil t\\rceil-t)^{2}}\\|\\hat{x}_{\\theta}^{\\lceil t\\rceil}(t,x)-x^{\\lceil t\\rceil}\\|^{2}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbb{E}_{t,q(z),p_{t}(x|z)}\\|v_{\\theta}(t,x)-u_{t}(x|z)\\|^{2}=\\mathbb{E}_{t,q(z),p_{t}(x|z)}\\left\\|\\frac{\\hat{x}_{\\theta}^{\\lceil t\\rceil}(t,x)-x}{\\lceil t\\rceil-t}-\\frac{x^{\\lceil t\\rceil}-x}{\\lceil t\\rceil-t}\\right\\|^{2}}\\\\ &{}&{=\\mathbb{E}_{t,q(z),p_{t}(x|z)}\\frac{1}{(\\lceil t\\rceil-t)^{2}}\\|\\hat{x}_{\\theta}^{\\lceil t\\rceil}(t,x)-x^{\\lceil t\\rceil}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Lemma 3.1 The SDE $d x_{t}=u_{t}(x|z)d t+\\sigma^{2}d W_{t}$ where $u_{t}$ is defined in eq. 9 generates $p_{t}(x|z)$ in eq. 8 with initial condition $p_{0}:=\\delta_{x_{1}}$ where $\\delta$ is the Dirac delta function. ", "page_idx": 16}, {"type": "text", "text": "Proof. For simplicity of notation we first show the case where $\\lceil t\\rceil=1$ . ", "page_idx": 16}, {"type": "equation", "text": "$$\nd x_{t}=u_{t}(x|z)d t+\\sigma^{2}d W_{t}=\\frac{1-x_{t}}{1-t}d t+\\sigma^{2}d W_{t}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which is equivalent to the $d$ dimensional Brownian bridge which has marginal ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\mathcal{N}}((1-t)x_{0}+t x_{1},\\sigma^{2}t(1-t))\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which completes the proof for $\\lceil t\\rceil=1$ . ", "page_idx": 16}, {"type": "text", "text": "Proposition 3.2 (Coupling Preservation) Under mild regulatory criteria on $u_{t}(\\cdot|z),\\,p_{t}$ , and $q,\\,i f$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}_{t\\sim\\mathcal{U}(0,T),z\\sim q(z),c\\sim q(c|z),x_{t}\\sim p_{t}(x_{t}|z)}\\|u_{t}(x_{t}|z,c)-u_{t}(x_{t}|c)\\|_{2}^{2}=0\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and $z,q(z),p_{t}(x|z)$ , and $u_{t}(x|z)$ are as defined in eqs. 6-9 then $\\Pi(u)^{\\star}=\\Pi^{\\star}(x_{1:T})$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. We prove the deterministic case with $T=1$ . The extensions to stochastic and $T>1$ are evident. The couplings are equal if the marginal vector field $u_{t}(x_{t}|c)=u_{t}(x_{t}|z,c)$ everywhere as the coupling is governed by the push forward flows $\\begin{array}{r}{\\phi(x_{0},c)=\\int_{0}^{1}u_{t}(x_{t}|c)d t}\\end{array}$ , and $\\phi(x_{0},c,z)=$ $\\textstyle\\int_{0}^{1}(u_{t}(x_{t}|\\boldsymbol{z},\\boldsymbol{c})$ . If ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}_{t\\sim\\mathcal{U}(0,T),z\\sim q(z),c\\sim q(c|z),x_{t}\\sim p_{t}(x_{t}|z)}\\|u_{t}(x_{t}|z,c)-u_{t}(x_{t}|c)\\|_{2}^{2}=0\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "then $\\phi(x_{0},c,z)=\\phi(x_{0},c)$ for all $x_{0}$ and therefore the couplings of the optimal map are equivalent. We note that this requires exchange of integrals under the same conditions as Lemma A.1. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "Next we show how (A1)-(A3) satisfy Prop. 3.2. ", "page_idx": 16}, {"type": "text", "text": "(A1) When $c=x_{0}$ and there exists $T:\\mathcal{X}\\rightarrow\\mathcal{X}$ such that $T(x_{0})=x_{1}$ if and only if $\\Pi^{\\star}(x_{0},x_{1})$ . We note that this is equivalent to asserting the existence of a Monge map $T^{\\star}$ for the coupling $\\Pi^{\\star}$ . In the two timepoint case, $c=x_{0}$ is sufficient as long as there aren\u2019t two trajectories that have the same $x_{0}$ but different $x_{1}\\mathbf{s}$ . Conditioning on this way ensures the conditions of of Prop. 3.2 as the uniqueness property ensures the uniqueness of $\\boldsymbol{u}_{t}(\\boldsymbol{x}_{t}|\\boldsymbol{c})$ .   \n(A2) There exist no two trajectories $x^{i}$ , $x^{j}$ such that $\\boldsymbol x_{t}^{i}=\\boldsymbol x_{t}^{j}$ for $h+1$ consecutive observations. In this case notice that this is simply a multi-timepoint extension of A1 to $c=x_{t-h-1:t-1}$ , i.e. conditioned on a history of length $h$ . If this is the case then the same reasoning as A1 applies.   \n(A3) Trajectories are associated with unique conditional vectors $c$ independent of $t$ . This satisfies Prop 3.2 by definition. ", "page_idx": 16}, {"type": "text", "text": "Proposition 3.3 There exists a scaling function $c(t):\\mathbb{R}_{+}\\to\\mathbb{R}$ such that ${\\mathcal{L}}_{\\mathrm{target}}(\\theta)=c(t){\\mathcal{L}}_{\\mathrm{match}}(\\theta)$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. We start with the matching loss. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}_{t,q(z),p_{t}(x|z)}\\|v_{\\theta}(t,x)-u_{t}(x|z)\\|^{2}=\\mathbb{E}_{t,q(z),p_{t}(x|z)}\\frac{1}{(\\lceil t\\rceil-t)^{2}}\\|\\hat{x}_{\\theta}^{\\lceil t\\rceil}(t,x)-x^{\\lceil t\\rceil}\\|^{2}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "by substitution, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbb{E}_{t,q(z),p_{t}(x|z)}\\|v_{\\theta}(t,x)-u_{t}(x|z)\\|^{2}=\\mathbb{E}_{t,q(z),p_{t}(x|z)}\\left\\|\\frac{\\hat{x}_{\\theta}^{[t]}(t,x)-x}{\\lceil t\\rceil-t}-\\frac{x^{[t]}-x}{\\lceil t\\rceil-t}\\right\\|^{2}}\\\\ &{}&{=\\mathbb{E}_{t,q(z),p_{t}(x|z)}\\frac{1}{(\\lceil t\\rceil-t)^{2}}\\|\\hat{x}_{\\theta}^{[t]}(t,x)-x^{[t]}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "completing the proof. ", "page_idx": 16}, {"type": "image", "img_path": "fNakQltI1N/tmp/595794f7ba754226435accc203563f196be0372deda995794af8841657434aa1.jpg", "img_caption": ["Figure 4: Left: Distribution of number of complete vital measurements per patient trajectory within the first 24 hours of admission in each clinical dataset. Right: Distribution of raw heart rate values in each clinical dataset. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "B Experimental Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "B.1 1D Oscillators ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The three oscillation trajectories correspond to $c=0.25$ (the red trajectory in Figure 2), $c=2$ (blue), and $c=3.75$ (green). Before used as an input, $t$ was scaled to between 0 and 1 by dividing by 10. ", "page_idx": 17}, {"type": "text", "text": "B.2 Clinical Datasets ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "B.2.1 Clinical Data Characteristics ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In order to accurately model the perturbations in the physiologic signals (mean arterial pressure and heart rate) of the underlying patient states, we need to learn beyond the general trend of the data. ", "page_idx": 17}, {"type": "text", "text": "While the physiologic measurements themselves reflect patient status and drive clinical decision making, the degree of variation holds information that goes beyond the snapshot at a single time point. Our approach models the data distribution and stochasticity rather than just ftiting the average trajectory. Other time-varying such as treatment conditions and non-time-varying covariates such as underlying disease states may also hold information that may impact the underlying state generating the physiologic signals. Our approach also incorporates this information to inform the trajectory modeling. ", "page_idx": 17}, {"type": "text", "text": "The data distribution in the ICU datasets reflect its status as the most resource-intensive clinical setting with increased measurement frequency and data distribution shift towards more abnormal physiologic values (Figure 4). The ED dataset reflects its status as the clinical setting focused on triaging patients, with sparser and physiologic measurements that fall in the normal range. ", "page_idx": 17}, {"type": "text", "text": "B.2.2 Clinical Data Preprocessing ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "For each clinical dataset, we modeled patient trajectories formed with heart rate and blood pressure measurements during the first 24 hours following admission. The timeline for each trajectory, originally in minutes, was scaled to a range between 0 and 1 by dividing by 1440. Additionally, heart rate and blood pressure values were z-score normalized to standardize the data. ", "page_idx": 17}, {"type": "text", "text": "Intensive Care Unit Sepsis (ICU Sepsis) Dataset The eICU Collaborative Research Database v2.0 [Pollard et al., 2019] is a database including deidentified information collected from over 200,000 patients in multiple intensive care units (ICUs) in the United States from 2014 to 2015. The ICU Sepsis Dataset was created by subsetting the eICU Database for 3362 patients with sepsis as the primary admission diagnosis (2689 patients in training set, 336 in validation set, and 337 in test set). The following data fields were extracted: patient sex, age, heart rate, mean arterial pressure, norepinephrine dose and infusion rate, and a validated ICU score (APACHE-IV). Each patient\u2019s complete pair measurements of heart rate and mean arterial pressure over time form one trajectory to be modeled. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "Norepinephrine infusion rates were calculated by converting drug doses or infusion rates to $\\mu\\mathrm{g/kg/min}$ , and where drug doses were not explicitly available, the dose was inferred from the free text given in the drug name. Start and end times for norepinephrine infusion were calculated by dividing the dose by the infusion rate. Where there appeared to be multiple infusions at the same time, the maximum infusion rate was taken as the infusion rate. As a conditional input to the models, the norepinephrine infusion doses are then scaled to between 0 and 1 by dividing by the maximum norepinephrine value in the dataset. ", "page_idx": 18}, {"type": "text", "text": "The APACHE-IV score, a validated critical care risk score, predicts individual patient mortality risk [Zimmerman et al., 2006]. In data preprocessing, we uses logistic regression of the score against binary hospital mortality data to generate a probability for each patient, serving as an additional input condition for models. ", "page_idx": 18}, {"type": "text", "text": "Intensive Care Unit Cardiac Arrest (ICU Cardiac Arrest) Dataset This dataset was extracted from the to eICU Collaborative Research Database v2.0 [Pollard et al., 2019] described above to reflect ICU patients at risk for cardiac arrest. This dataset excludes patients who presented with myocardial infarction (MI) and includes variables used in the Cardiac Arrest Risk Triage (CART) score [Churpek et al., 2012]: respiratory rate, heart rate, diastolic blood pressure, and age at the time of ICU admission. As an input to the model, the age was z-score normalized. 51671 patients were included in the training set, with 6459 patients each in the validation and test sets. ", "page_idx": 18}, {"type": "text", "text": "Intensive Care Unit Acute Gastrointestinal Bleeding (ICU GIB) Dataset The Medical Information Mart for Intensive Care III (MIMIC-III) critical care database contains data for over 40,000 patients in the Beth Israel Deaconess Medical Center from 2001 to 2012 requiring an ICU stay [Johnson et al., 2016]. We selected a cohort of 2602 ICU patients with the primary diagnosis of gastrointestinal bleeding to form the ICU GIB dataset, split into a training set of 2082 patients, and a validation set and a test set of 260 patients each. We extracted the following variables: age, sex, heart rate, systolic blood pressure, diastolic blood pressure, usage of vasopressor, usage of blood product, usage of packed red blood cells, and liver disease. Since the vasopressor and blood product usage are encoded as a binary value and may not represent actual infusion amount that are most likely decaying, we experimented with adding a Gaussian decay to them to use as conditional inputs. Likewise, trajectories to model consist of complete pairs of heart rate and mean arterial pressure (calculated from systolic blood pressure and diastolic blood pressure) measurements. ", "page_idx": 18}, {"type": "text", "text": "Emergency Department Acute Gastrointestinal Bleeding (ED GIB) Dataset This dataset reflects 3348 patients presenting with signs and symptoms of acute gastrointestinal bleeding to two hospital campuses in Yale New Haven Hospital between 2014 and 2018. The patients were split into a training set, a validation set, and a test set of 2636, 352, and 360 patients. Variables extracted include patient sex, age, heart rate, mean arterial pressure, initial measurements of 24 lab tests, and 17 pre-existing medical conditions as determined by ICD-10 codes. Like ICU Sepsis data, the trajectoires consist of complete pairs of heart rate and mean arterial pressure measurements. ", "page_idx": 18}, {"type": "text", "text": "Age, initial lab test measurements (three labs omitted due to missing data), and pre-existing medical conditions were used to train an XGBoost model [Chen and Guestrin, 2016] to predict the binary outcome variable indicating the need for hospital-based care. The resulting probabilities of requiring hospital-based care (outcome of 1) for each patient were then calculated using the trained model and used as conditional input to conditional models in experiments on this dataset. ", "page_idx": 18}, {"type": "text", "text": "Of note, the outcome variable was defined as 1 if a patient (1) requires red blood cell transfusion, (2) requires urgent intervention (endoscopic, interventional radiologic, or surgical) to stop bleeding or (3) all-cause 30-day mortality. Labs and medical conditions included in this dataset are listed below. Labs in bold were excluded from the XGBoost risk score calculation due to missing data. ", "page_idx": 18}, {"type": "text", "text": "\u2022 Labs: Sodium, Potassium, Chloride, Carbon Dioxide, Blood Urea Nitrogen, Creatinine, International Normalized Ratio, Partial Thromboplastin Time, White Blood Cell Count, Hemoglobin, Platelet Count, Hematocrit, Mean Corpuscular Volume, Mean Corpuscular Hemoglobin, Mean Corpuscular Hemoglobin Concentration, Red Cell Distribution Width, ", "page_idx": 18}, {"type": "image", "img_path": "fNakQltI1N/tmp/2702c00ef3a508bd378eb0645879377b182c82d4ed93ceaf082273bb9734f93c.jpg", "img_caption": ["Figure 5: Sigma mean MSE comparison "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Red Blood Cell Count, Aspartate Aminotransferase, Alanine Aminotransferase, Alkaline Phosphatase, Total Bilirubin, Direct Bilirubin, Albumin, Lactate. ", "page_idx": 19}, {"type": "text", "text": "\u2022 Previous Medical Histories: Charlson Comorbidity Index, Cerebrovascular Accident, Deep Vein Thrombosis, Pulmonary Embolism, Atrial Fibrillation, Upper Gastrointestinal Bleeding, Lower Gastrointestinal Bleeding, Unspecified Gastrointestinal Bleeding, Peptic Ulcer Disease, Helicobacter Pylori Infection, Coronary Artery Disease, Heart Failure, Hypertension, Type 2 Diabetes Mellitus, Chronic Kidney Disease, Alcohol Use Disorder, Cirrhosis. ", "page_idx": 19}, {"type": "text", "text": "B.3 Training ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "B.3.1 1D Oscillators ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Since the trajectories in this dataset are deterministic and regularly sampled, we deployed only TFM-ODE and applied solely the ${\\mathcal{L}}_{\\mathrm{match}}$ loss (i.e, no uncertainty or time predictive loss), as these methods sufficiently address the structured nature of the data to generate proof-of-concept results. The three models presented in Figure 2 all have hidden size of 256, $\\sigma$ of 0.1, trained under seed $=\\!0$ with Adam optimizer with learning rate $1\\times10^{-3}$ for a maximum of 1000 epochs with early stopping (patience $=\\!3$ ) monitoring validation loss. ", "page_idx": 19}, {"type": "text", "text": "B.3.2 Clinical Data ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "All the models for clinical data experiments are trained with Adam optimizer. A maximum training time and epochs are set to 48 hours and 300, with early stopping (patience $_{=3}$ ) monitoring validation loss. All metrics reported were ran with 5 seeds (0,1,2,3,4) to ensure it is reproducible. ", "page_idx": 19}, {"type": "text", "text": "TFM, TFM-ODE, and ablations The TFM models were trained with learning rate $1\\times10^{-6}$ and had $\\sigma$ of 0.1. The complete models have hidden size of 256 and memory of 3, while ablation study with a hidden size of 64 and/or no memory was performed (Table 4). The noise parameter for the SDE implementation was set to 0.1 for ablations without Luncertainty. The hyperparameters $\\sigma=0.1$ and memory $=\\!3$ for full models were selected through experiments with different values of $\\sigma$ and memory (Figure 5 and 6). ", "page_idx": 19}, {"type": "text", "text": "FM The FM baseline models were trained with learning rate $1\\times10^{-6}$ . All models had a hidden size of 64 with $\\sigma$ of 0.1. ", "page_idx": 19}, {"type": "text", "text": "Latent Neural ODE The latent Neural ODE models were trained with a learning rate of $1\\times10^{-3}$ .   \n100 GRU units were used for the encoder model and the number of latent dimensions was 2. ", "page_idx": 19}, {"type": "text", "text": "Baseline Neural SDE and Neural ODE Both baseline models were trained with learning rate $1\\times10^{-5}$ and had a hidden size of 64. ", "page_idx": 19}, {"type": "image", "img_path": "fNakQltI1N/tmp/f3eed19141d6308adf25cd0e72c608a07a59b02f4c0e5cd214015ae592e0fda1.jpg", "img_caption": ["Figure 6: Memory Mean MSE comparison "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Claims ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The abstract and introduction clearly state the claim. The theoretical portions of the claim are supported in sections 2 and 3 of the main text and in appendix A; the claims on its performance are supported by section 4 of the main text. We discuss the limitations of our models context of the ablation study and in the related works section. See the next section for more details on the specifics on the discussion of limitations. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We discuss the limits of our current model depending on characteristics of the dataset. In the ablation study, we discuss the variable effect memory had across datasets of different data distributions and measurement frequency as a limitation to the current framework. We also provide context in the related work section that describes the limitation of our current framework in not being able to provide causal explanations. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper. \u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors ", "page_idx": 20}, {"type": "text", "text": "should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: Proof of theorems are included in section 3 and appendix A ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Reproducibility is accomplished by providing a full description of the model in the main text in addition to providing publically available code at https://github. com/nZhangx/TrajectoryFlowMatching. The experimental data are available publicly online (ICU Sepsis, ICU GIB) or upon reasonable request (ED GIB) subject to institutional approval and HIPAA regulations. Data preprocessing steps are extensively outlined in Appendix B.2. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. ", "page_idx": 21}, {"type": "text", "text": "\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Anonymised code is available as supplementary material. The ICU Sepsis dataset is available publicly online via the eICU database and the ICU GIB dataset is available via the MIMIC-III database, whereas the ED GIB datasets are datasets which include identifiable information. A deidentified dataset may be made available upon reasonable request subject to guidelines set by the institutional review board and in accordance with HIPAA policy. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 22}, {"type": "text", "text": "\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We provide an overview with the significance of each clinical dataset in the body of the paper. In the Appendix we have written detailed descriptions of each dataset, including the methodology for identifying the cohorts of patients, input variables extracted, number of patient encounters, summary statistics, as well as frequency distributions. We also detail the process of training and testing for all methods presented in the paper, from baseline to our novel method. We have also provided anonymized link to all code used for the experiments. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper includes a standard error of the mean for all the results reported in a numerical format. There are no figures with error bars for which the nature of error bar calculation would be relevant. There are no statistical tests that were perform which would make discussion of the validity of statistical tests relevant. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 23}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We provide the description of the compute resources utilized with available computing clusters detailed in the Experimental Results section. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We have all reviewed the NeurIPS Code of Ethics and striven to maintain and preserve anonymity. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: In the conclusion section we discuss how improved modeling of Emergency Department and ICU physiology can lead to the positive societal impact of improving disease risk prediction which can enable medical teams to make better clinical decisions, provide patients and family members with increased information on the likely course of the illness, and improve resource allocation by identifying which patients may require costly or rare resources such a blood transfusions. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. ", "page_idx": 24}, {"type": "text", "text": "\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The current study proposes a predictive model for time-varying datasets and as such does not pose risks that are significantly higher in comparison to conventional models that fit data trends. There is no generative component in our model, and little risk that the results could be misused in a way that misleads or proves otherwise detrimental to the broader public. We therefore believe this item does not apply for this submission. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The publicly available datasets from MIMIC-III and eICU databases are properly credited, respected, mentioned and used under the PhysioNet Credentialed Health Data License Version 1.5.0. The ED GIB EHR data is owned by the authors. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: [No] ", "page_idx": 26}, {"type": "text", "text": "Justification: We will not be releasing new datasets. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: While the data involves patient data, it does not include any experiments performed on humans but instead based on the use of retrospectively collected healthcare records generated as part of routine clinical care. Publicly available datasets were used in accordance in the regulations set out by the hosting institution, whereas collection of data from patients in non-publicly available datasets were performed in accordance with institutional review board guidance and HIPAA data protection regulations. As the data collection does not involve experiments on human subjects or crowdsourcing experiments, this item does not apply to our current study. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: We have obtained institutional IRB consent for the use of the ED GIB EHR data under an approved protocol that we are happy to provide upon request. MIMIC-III and eICU databases are pre-approved, de-identified, publicly available clinical data sources. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}]