[{"type": "text", "text": "Functional Bilevel Optimization for Machine Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ieva Petrulionyte, Julien Mairal, Michael Arbel Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, 38000 Grenoble, France firstname.lastname@inria.fr ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this paper, we introduce a new functional point of view on bilevel optimization problems for machine learning, where the inner objective is minimized over a function space. These types of problems are most often solved by using methods developed in the parametric setting, where the inner objective is strongly convex with respect to the parameters of the prediction function. The functional point of view does not rely on this assumption and notably allows using over-parameterized neural networks as the inner prediction function. We propose scalable and efficient algorithms for the functional bilevel optimization problem and illustrate the beneftis of our approach on instrumental regression and reinforcement learning tasks. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Bilevel optimization methods solve problems with hierarchical structures, optimizing two interdependent objectives: an inner-level objective and an outer-level one. Initially used in machine learning for model selection [Bennett et al., 2006] and sparse feature learning [Mairal et al., 2012], these methods gained popularity as efficient alternatives to grid search for hyper-parameter tuning [Feurer and Hutter, 2019, Lorraine et al., 2019, Franceschi et al., 2017]. Applications of bilevel optimization include meta-learning [Bertinetto et al., 2019], auxiliary task learning [Navon et al., 2021], reinforcement learning [Hong et al., 2023, Liu et al., 2021a, Nikishin et al., 2022], inverse problems [Holler et al., 2018] and invariant risk minimization [Arjovsky et al., 2019, Ahuja et al., 2020]. ", "page_idx": 0}, {"type": "text", "text": "Bilevel problems are challenging to solve, even in the well-defined bilevel setting with a unique inner-level solution. This difficulty stems from approximating both the inner-level solution and its sensitivity to the outer-level variable during gradient-based optimization. Methods like Iterative Differentiation (ITD, Baydin et al., 2017) and Approximate Implicit Differentiation (AID, Ghadimi and Wang, 2018) were designed to address these challenges in the well-defined setting, resulting in scalable algorithms with strong convergence guarantees [Domke, 2012, Gould et al., 2016, Ablin et al., 2020, Arbel and Mairal, 2022a, Blondel et al., 2022, Liao et al., 2018, Liu et al., 2022b, Shaban et al., 2019]. These guarantees usually require the inner-level objective to be strongly convex. However, when the inner-level variables are neural network parameters, the lower-level problem becomes non-convex and may have multiple solutions due to over-parameterization. While nonconvexity is considered \"benign\" in this setting [Allen-Zhu et al., 2019, Liu et al., 2022a], multiplicity of inner-level solutions makes their dependence on the outer-level variable ambiguous [Liu et al., 2021b], posing a major challenge in bilevel optimization for modern machine learning applications. ", "page_idx": 0}, {"type": "text", "text": "We identify a common functional structure in bilevel machine learning problems to address the ambiguity challenge that arises with flexible models like neural networks. Specifically, we consider a prediction function $h$ optimized by the inner-level problem over a Hilbert space $\\mathcal{H}$ . This space consists of functions defined over an input space $\\mathcal{X}$ and taking values in a finite dimensional vector space $\\mathcal{V}$ . The optimal prediction function is then evaluated in the outer-level to optimize an outer-level parameter $\\omega$ in a finite dimensional space $\\Omega=\\mathbb{R}^{d}$ , resulting in a functional bilevel problem: ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\omega\\in\\Omega}\\,\\mathcal{F}(\\omega):=L_{o u t}\\left(\\omega,h_{\\omega}^{\\star}\\right)\\quad\\mathrm{s.t.}\\quad h_{\\omega}^{\\star}=\\underset{h\\in\\mathcal{H}}{\\arg\\operatorname*{min}}\\,\\,L_{i n}\\left(\\omega,h\\right).\n$$", "text_format": "latex", "page_idx": 0}, {"type": "image", "img_path": "enlxHLwwFf/tmp/72827a322d401425b23fb7672c92d68b2d1e53a1217b969dc75f3786c4550d68.jpg", "img_caption": ["Figure 1: Parametric vs functional approaches for solving FBO by implicit differentiation. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "In contrast to classical bilevel formulations involving neural networks, where the inner objective is non-convex with respect to the network parameters, the inner objective in (FBO) defines an optimization problem over a prediction function $h$ in a functional vector space $\\mathcal{H}$ . ", "page_idx": 1}, {"type": "text", "text": "A crucial consequence of adopting this new viewpoint is that it renders the strong convexity of the inner objective with respect to $h$ a mild assumption, which ensures the uniqueness of the solution $h_{\\omega}^{\\star}$ for any outer parameter value $\\omega$ . Strong convexity with respect to the prediction function is indeed much weaker than strong convexity with respect to model parameters and often holds in practice. For instance, a supervised prediction task with pairs of features/labels $(x,y)$ drawn from some training data distribution is formulated as a regularized empirical minimization problem: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{h\\in\\mathcal{H}}L_{i n}(\\omega,h):=\\mathbb{E}_{x,y}\\left[\\|y-h(x)\\|_{2}^{2}\\right]+\\frac{\\omega}{2}\\|h\\|_{\\mathcal{H}}^{2},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\mathcal{H}$ is the $L_{2}$ space of square integrable functions w.r.t. the distribution of $x$ , and $\\omega>0$ controls the amount of regularization. The inner objective is strongly convex in $h$ , even though the optimal prediction function $h_{\\omega}^{\\star}$ can be highly nonlinear in $x$ . The function $h_{\\omega}^{\\star}$ may then be approximated, e.g., by an overparameterized deep neural network, used here as a function approximation tool. ", "page_idx": 1}, {"type": "text", "text": "Although appealing, the (FBO) formulation necessitates the development of corresponding theory and algorithms, which is the aim of our paper. To the best of our knowledge, this is the first work to propose a functional bilevel point of view that can leverage deep networks for function approximation. The closest works are either restricted to kernel methods [Rosset, 2008, Kunapuli et al., 2008] and thus cannot be used for deep learning models, or propose abstract algorithms that can only be implemented for finite Hilbert spaces [Suonper\u00e4 and Valkonen, 2024]. ", "page_idx": 1}, {"type": "text", "text": "We introduce in Section 2 a theoretical framework for functional implicit differentiation in an abstract Hilbert space $\\mathcal{H}$ that allows computing the total gradient $\\nabla{\\mathcal{F}}(\\omega)$ using a functional version of the implicit function theorem [Ioffe and Tihomirov, 1979] and the adjoint sensitivity method [Pontryagin, 2018]. This involves solving a well-conditioned functional linear system, equivalently formulated as a regression problem in $\\mathcal{H}$ , to find an adjoint function $a_{\\omega}^{\\star}$ used for computing the total gradient $\\nabla\\bar{\\mathcal{F}}(\\omega)$ . We then specialize this framework to the common scenario where $\\mathcal{H}$ is an $L_{2}$ space and objectives are expectations of point-wise losses. This setting covers many machine learning problems (see Sections 4.1, 4.2, and Appendix A). In Section 3, we propose an efficient algorithm where the prediction and adjoint functions can be approximated using parametric models, like neural networks, learned with standard stochastic optimization tools. We further study its convergence using analysis for biased stochastic gradient descent [Demidovich et al., 2024]. ", "page_idx": 1}, {"type": "text", "text": "The proposed method, called functional implicit differentiation (FuncID), adopts a novel \"differentiate implicitly, then parameterize\" approach (left Figure 1): functional strong convexity is first exploited to derive an unambiguous implicit gradient in function space using a well-defined adjoint function. Then, both the lower-level solution and adjoint function are approximated using neural networks. This contrasts with traditional AID/ITD approaches (right Figure 1), which parameterize the inner-level solution as a neural network, leading to a non-convex \u2018parametric\u2019 bilevel problem in the network\u2019s parameters. An ambiguous implicit gradient is then computed by approximately solving an unstable or ill-posed linear system [Arbel and Mairal, 2022b]. Consequently, FuncID addresses the ambiguity challenge by exploiting the functional perspective and results in a stable algorithm with reduced time and memory costs. In Section 4, we demonstrate the benefits of our approach in instrumental regression and reinforcement learning tasks, which admit a natural functional bilevel structure. ", "page_idx": 1}, {"type": "text", "text": "Related work on bilevel optimization with non-convex inner objectives. In principle, considering amended versions of the bilevel problem can resolve the ambiguity arising from non-convex inner objectives. This is the case of optimistic/pessimistic versions of the problem, often considered in the literature on mathematical optimization, where the outer-level objective is optimized over both outer and inner variables, under the optimality constraint of the inner-level variable [Dempe et al., 2007, Ye and Ye, 1997, Ye and Zhu, 1995, Ye et al., 1997]. While tractable methods were recently proposed to solve them [Liu et al., 2021a,b, 2023, Kwon et al., 2024, Shen and Chen, 2023], it is unclear how well the resulting solutions behave on unseen data in the context of machine learning. For instance, when using over-parameterized models for the inner-level problem, their parameters must be further optimized for the outer-level objective, possibly resulting in over-fitting [Vicol et al., 2022]. More recently, Arbel and Mairal [2022b] proposed a game formulation involving a selection map to deal with multiple inner-level solutions. Such a formulation justifies the use of ITD/AID outside the well-defined bilevel setting, by viewing those methods as approximating the Jacobian of the selection map. However, the justification only hold under rather strong geometric assumptions. Additional related work is discussed in Appendix B on bilevel optimization with strongly-convex inner objectives, the adjoint sensitivity method that is often used in the context of ordinary differential equations, and amortization techniques [Amos et al., 2023] that have been also exploited for approximately solving bilevel optimization problems [MacKay et al., 2019, Bae and Grosse, 2020]. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2 A Theoretical Framework for Functional Bilevel Optimization ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The functional bilevel problem (FBO) involves an optimal prediction function $h_{\\omega}^{\\star}$ for each value of the outer-level parameter $\\omega$ . Solving (FBO) by using a first-order method then requires characterizing the implicit dependence of $h_{\\omega}^{\\star}$ on the outer-level parameter $\\omega$ to evaluate the total gradient $\\nabla{\\mathcal{F}}(\\omega)$ in $\\mathbb{R}^{d}$ . Indeed, assuming that $h_{\\omega}^{\\star}$ and $L_{o u t}$ are Fr\u00e9chet differentiable (this assumption will be discussed later), the gradient $\\nabla{\\mathcal{F}}(\\omega)$ may be obtained by an application of the chain rule: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\nabla\\mathcal{F}(\\omega)=g_{\\omega}+\\partial_{\\omega}h_{\\omega}^{\\star}d_{\\omega},\\quad\\mathrm{with}\\quad g_{\\omega}:=\\partial_{\\omega}L_{o u t}(\\omega,h_{\\omega}^{\\star})\\quad\\mathrm{and}\\quad d_{\\omega}:=\\partial_{h}L_{o u t}\\left(\\omega,h_{\\omega}^{\\star}\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The Fr\u00e9chet derivative $\\partial_{\\omega}h_{\\omega}^{\\star}:\\mathcal{H}\\rightarrow\\mathbb{R}^{d}$ is a linear operator acting on functions in $\\mathcal{H}$ and measures the sensitivity of the optimal solution on the outer variable. We will refer to this quantity as the \u201cJacobian\u201d in the rest of the paper. While the expression of the gradient in Equation (2) might seem intractable in general, we will see in Section 3 a class of practical algorithms to estimate it. ", "page_idx": 2}, {"type": "text", "text": "2.1 Functional implicit differentiation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our starting point is to characterize the dependence of $h_{\\omega}^{\\star}$ on the outer variable. To this end, we rely on the following implicit differentiation theorem (proven in Appendix C) which can be seen as a functional version of the one used in AID [Domke, 2012, Pedregosa, 2016], albeit, under a much weaker strong convexity assumption that holds in most practical cases of interest. ", "page_idx": 2}, {"type": "text", "text": "Theorem 2.1 (Functional implicit differentiation). Consider problem (FBO) and assume that: ", "page_idx": 2}, {"type": "text", "text": "\u2022 For any $\\omega\\in\\Omega$ , there exists $\\mu\\!>\\!0$ such that $h\\mapsto L_{i n}(\\omega^{\\prime},h)$ is $\\mu$ -strongly convex for any $\\omega^{\\prime}$ near $\\omega$ . \u2022 $h\\mapsto L_{i n}(\\omega,h)$ has finite values and is Fr\u00e9chet differentiable on $\\mathcal{H}$ for all $\\omega\\in\\Omega$ . \u2022 $\\partial_{h}L_{i n}$ is Hadamard differentiable on $\\Omega\\times\\mathcal{H}$ (in the sense of Definition C.1 in Appendix $C_{.}$ ). Then, $\\omega\\mapsto h_{\\omega}^{\\star}$ is uniquely defined and is Fr\u00e9chet differentiable with a Jacobian $\\partial_{\\omega}h_{\\omega}^{\\star}$ given by: $B_{\\omega}+\\partial_{\\omega}h_{\\omega}^{\\star}C_{\\omega}=0,\\qquad w i t h\\quad B_{\\omega}:=\\partial_{\\omega,h}L_{i n}(\\omega,h_{\\omega}^{\\star}),\\quad a n d\\quad C_{\\omega}:=\\partial_{h}^{2}L_{i n}(\\omega,h_{\\omega}^{\\star}).$ (3) ", "page_idx": 2}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The strong convexity assumption on the inner-level objective ensures the existence and uniqueness of the solution $h_{\\omega}^{\\star}$ , while differentiability assumptions on $L_{i n}$ and $\\partial_{h}L_{i n}$ ensure Fr\u00e9chet differentiability of the map $\\omega\\mapsto h_{\\omega}^{\\star}$ . Though the implicit function theorem for Banach spaces [Ioffe and Tihomirov, 1979] could yield similar conclusions, it demands the stronger assumption that $\\partial_{h}L_{i n}$ is continuously Fr\u00e9chet differentiable, which is quite restrictive in our setting of interest: for instance, when $\\mathcal{H}$ is an $L_{2}$ -space and $L_{i n}$ is an integral functional of the form $\\begin{array}{r}{\\tilde{L_{i n}}(\\omega,h)=\\int\\ell_{i n}(w,h(x))\\,\\mathrm{d}x}\\end{array}$ , with $\\ell_{i n}$ defined on $\\Omega\\times\\nu$ and satisfying mild smoothness and growth assumptions on $\\ell_{i n}$ , then $h\\mapsto$ $\\partial_{h}L_{i n}(\\omega,h)$ cannot be Fr\u00e9chet differentiable with uniformly continuous differential on bounded sets, unless $v\\mapsto\\ell_{i n}(\\omega,v)$ is a polynomial of degree at most 2 (see [Nemirovski and Semenov, 1973, Corollary 2, p 276] and discussions in [Noll, 1993, Goodman, 1971]). Instead, Theorem 2.1 employs the weaker notion of Hadamard differentiability for $\\partial_{h}L_{i n}$ , widely used in statistics, particularly for deriving the delta-method [van der Vaart and Wellner, 1996, Chapter 3.9]. Consequently, Theorem 2.1 allows us to cover a broader class of functional bilevel problems, as we see in Section 2.2. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Similarly to AID, only a Jacobian-vector product is needed when computing the total gradient $\\nabla{\\mathcal{F}}(\\omega)$ . The result in Proposition 2.2 below, relies on the adjoint sensitivity method [Pontryagin, 2018] to provide a more convenient expression for $\\nabla{\\mathcal{F}}(\\omega)$ and is proven in Appendix C.2. ", "page_idx": 3}, {"type": "text", "text": "Proposition 2.2 (Functional adjoint sensitivity). Under the same assumption on $L_{i n}$ as in Theorem 2.1 and further assuming that $L_{o u t}$ is jointly differentiable in $\\omega$ and $h$ , the total objective $\\mathcal{F}$ is differentiable with $\\nabla{\\mathcal{F}}(\\omega)$ given by: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla\\mathcal{F}(\\omega)=g_{\\omega}+B_{\\omega}a_{\\omega}^{\\star},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the adjoint function $a_{\\omega}^{\\star}:=-C_{\\omega}^{-1}d_{\\omega}$ is an element of $\\mathcal{H}$ that minimizes the quadratic objective: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{a_{\\omega}^{\\star}=\\arg\\underset{a\\in\\mathcal{H}}{\\operatorname*{min}}\\,L_{a d j}(\\omega,a):=\\frac{1}{2}\\,\\,\\langle a,C_{\\omega}a\\rangle_{\\mathcal{H}}+\\langle a,d_{\\omega}\\rangle_{\\mathcal{H}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Equation (4) indicates that computing the total gradient requires optimizing the quadratic objective (5) to find the adjoint function $a_{\\omega}^{\\star}$ . The strong convexity of the adjoint objective $L_{a d j}$ ensures the existence of a unique minimizer, and stems from the positive definiteness of its Hessian operator $C_{\\omega}$ due to the inner-objective\u2019s strong convexity. Both adjoint and inner-level problems occur in the same function space $\\mathcal{H}$ and are equivalent in terms of conditioning, as the adjoint Hessian operator equals the inner-level Hessian at optimum. The strong convexity in $\\mathcal{H}$ of the adjoint objective guarantees well-defined solutions and holds in many practical cases, as opposed to classical parametric bilevel formulations which require the more restrictive strong convexity condition in the model\u2019s parameters, and without which instabilities may arise due to ill-conditioned linear systems (see Appendix F.1). ", "page_idx": 3}, {"type": "text", "text": "2.2 Functional bilevel optimization in $L_{2}$ spaces ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Specializing the abstract results from Section 2.1 to a common scenario in machine learning, we consider both inner and outer level objectives of (FBO) as expectations of point-wise functions over observed data. Specifically, we have two data distributions $\\mathbb{P}$ and $\\mathbb{Q}$ defined over a product space $\\mathcal{X}\\times\\mathcal{Y}\\subset\\mathbb{R}^{d_{\\boldsymbol{x}}}\\times\\mathbf{\\bar{R}}^{d_{\\boldsymbol{y}}}$ , and denote by $\\mathcal{H}$ the Hilbert space of functions $h:\\mathcal{X}\\rightarrow\\mathcal{V}$ , where $\\mathcal{V}=\\mathbb{R}^{d_{v}}$ . Given an outer parameter space $\\Omega$ , we address the following functional bilevel problem: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\omega\\in\\Omega}{\\operatorname*{min}}\\,\\,L_{o u t}\\left(\\omega,h_{\\omega}^{\\star}\\right):=\\mathbb{E}_{\\mathbb{Q}}\\left[\\ell_{o u t}\\left(\\omega,h_{\\omega}^{\\star}(x),x,y\\right)\\right]}\\\\ &{\\qquad\\qquad\\mathrm{~s.t.~}h_{\\omega}^{\\star}=\\underset{h\\in\\mathcal{H}}{\\operatorname{arg\\,min}}\\,\\,L_{i n}\\left(\\omega,h\\right):=\\mathbb{E}_{\\mathbb{P}}\\left[\\ell_{i n}\\left(\\omega,h(x),x,y\\right)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\ell_{o u t}$ , $\\ell_{i n}$ are point-wise loss functions defined on $\\Omega\\times\\mathcal{V}\\times\\mathcal{X}\\times\\mathcal{Y}$ . This setting encompasses various deep learning problems discussed in Sections 4.1 and 4.2, and in Appendix A, representing a specific instance of FBO. The Hilbert space $\\mathcal{H}$ of square-integrable functions not only models a broad range of prediction functions but also facilitates obtaining concrete expressions for the total gradient $\\nabla{\\mathcal{F}}(\\omega)$ , enabling the derivation of practical algorithms in Section 3. ", "page_idx": 3}, {"type": "text", "text": "The following proposition, proved in Appendix D, makes mild technical assumptions on probability distributions $\\mathbb{P}$ , $\\mathbb{Q}$ and the point-wise losses $\\ell_{i n}$ , $\\ell_{o u t}$ . It gives an expression for the total gradient in the form of expectations under $\\mathbb{P}$ and $\\mathbb{Q}$ . ", "page_idx": 3}, {"type": "text", "text": "Proposition 2.3 (Functional Adjoint sensitivity in $L_{2}$ spaces.). Under the technical Assumptions (A) to $(L)$ stated in Appendix $D.I$ , the conditions on $L_{i n}$ and $L_{o u t}$ in Proposition 2.2 hold and the total gradient $\\nabla{\\mathcal{F}}(\\omega)$ of $\\mathcal{F}$ is expressed as $\\nabla\\mathcal{F}(\\omega)=g_{\\omega}+B_{\\omega}a_{\\omega}^{\\star}$ , with $a_{\\omega}^{\\star}\\in\\mathcal{H}$ being the minimizer of the objective $L_{a d j}$ in Equation (5). Moreover, $L_{a d j}$ , $g_{\\omega}$ and $B_{\\omega}a_{\\omega}^{\\star}$ are given by: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{L_{a d j}(\\omega,a)=\\frac{1}{2}\\,\\mathbb{E}_{\\mathbb{P}}\\left[a(x)^{\\top}\\partial_{v}^{2}\\ell_{i n}\\left(\\omega,h_{\\omega}^{\\star}(x),x,y\\right)a(x)\\right]}\\\\ {+\\,\\mathbb{E}_{\\mathbb{Q}}\\left[a(x)^{\\top}\\partial_{v}\\ell_{o u t}\\left(\\omega,h_{\\omega}^{\\star}(x),x,y\\right)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{g_{\\omega}=\\mathbb{E}_{\\mathbb{Q}}\\left[\\partial_{\\omega}\\ell_{o u t}\\left(\\omega,h_{\\omega}^{\\star}(x),x,y\\right)\\right],\\quad B_{\\omega}a_{\\omega}^{\\star}=\\mathbb{E}_{\\mathbb{P}}\\left[\\partial_{\\omega,v}\\ell_{i n}\\left(\\omega,h_{\\omega}^{\\star}(x),x,y\\right)a_{\\omega}^{\\star}(x)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\partial_{\\omega}\\ell_{o u t},\\,\\partial_{v}\\ell_{o u t},\\,\\partial_{\\omega,v}\\ell_{i n}$ , and $\\partial_{v}^{2}\\ell_{i n}$ are partial first and second order derivatives of $\\ell_{o u t}$ and $\\ell_{i n}$ with respect to their first and second arguments $\\omega$ and $\\boldsymbol{v}$ . ", "page_idx": 3}, {"type": "text", "text": "Assumptions (A) and $({\\bf{B}})$ on $\\mathbb{P}$ and $\\mathbb{Q}$ ensure finite second moments and bounded Radon-Nikodym derivatives, maintaining square integrability under both distributions in Equation (6). Assumptions (C) to $\\mathbf{(L)}$ on $\\ell_{i n}$ and $\\ell_{o u t}$ primarily involve integrability, differentiability, Lipschitz continuity, and strong convexity of $\\ell_{i n}$ in its second argument, typically satisfied by objectives like mean squared error or cross entropy (see Proposition D.1 in Appendix D.1). Next, by leveraging Proposition 2.3, we derive practical algorithms for solving FBO using function approximation tools like neural networks. ", "page_idx": 4}, {"type": "text", "text": "3 Methods for Functional Bilevel Optimization in $L_{2}$ Spaces ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We propose Functional Implicit Differentiation (FuncID), a flexible class of algorithms for solving the functional bilevel problem in $L_{2}$ spaces described in Section 2.2 when samples from distributions $\\mathbb{P}$ and $\\mathbb{Q}$ are available. ", "page_idx": 4}, {"type": "text", "text": "FuncID relies on three main components detailed in the next subsections: ", "page_idx": 4}, {"type": "text", "text": "1. Empirical objectives. These approximate the objectives $L_{o u t}$ , $L_{i n}$ and $L_{a d j}$ as empirical expectations over samples from inner and outer datasets $\\mathcal{D}_{i n}$ and $\\mathcal{D}_{o u t}$ . 2. Function approximation. The search space for both the prediction and adjoint functions is restricted to parametric spaces with finite-dimensional parameters $\\theta$ and $\\xi$ . Approximate solutions $\\hat{h}_{\\omega}$ and $\\hat{a}_{\\omega}$ to the optimal functions $h_{\\omega}^{\\star}$ and $a_{\\omega}^{\\star}$ are obtained by minimizing the empirical objectives. 3. Total gradient approximation. FuncID estimates the total gradient $\\nabla{\\mathcal{F}}(\\omega)$ using the empirical objectives, and the approximations $\\hat{h}_{\\omega}$ and $\\hat{a}_{\\omega}$ of the prediction and adjoint functions. ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 provides an outlines of FuncID which has a nested structure similar to AID: (1) innerlevel optimizations (InnerOpt and AdjointOpt) to update the prediction and adjoint models using scalable algorithms such as stochastic gradient descent [Robbins and Monro, 1951], and (2) an outer-level optimization to update the parameter $\\omega$ using a total gradient approximation TotalGrad. An optional warm-start allows initializing the parameters of both the prediction and adjoint models for the current outer-level iteration with those obtained from the previous one. ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 FuncID Input: initial outer, inner, and adjoint parameter $\\omega_{0}$ , $\\theta_{0},\\xi_{0}$ ; warm-start option WS. for $n=0,\\ldots,N-1$ do # Optional warm-start if $\\mathrm{W}\\mathbf{S}\\mathrm{=}^{\\prime}$ True then $(\\theta_{0},\\xi_{0})\\gets(\\theta_{n},\\xi_{n})$ end if # Inner-level optimization $\\hat{h}_{\\omega_{n}},\\theta_{n+1}\\gets\\mathrm{Inner0pt}(\\omega_{n},\\theta_{0},\\mathcal{D}_{i n})$ # Adjoint optimization $\\boldsymbol{i}_{\\omega_{n}},\\xi_{n+1}\\gets\\mathtt{A d j o i n t0p t}(\\omega_{n},\\xi_{0},\\hat{h}_{\\omega_{n}},\\mathcal{D})$ # Outer gradient estimation Sample a mini-batch $B=(B_{o u t},B_{i n})$ from $\\mathcal{D}=(\\mathcal{D}_{o u t},\\mathcal{D}_{i n})$ $g_{o u t}\\gets\\mathtt{T o t a l G r a d}(\\omega_{n},\\hat{h}_{\\omega_{n}},\\hat{a}_{\\omega_{n}},\\mathcal{B})$ $\\omega_{n+1}\\leftarrow$ update $\\omega_{n}$ using $g_{o u t}$ ; end for ", "page_idx": 4}, {"type": "text", "text": "", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "3.1 From population losses to empirical objectives ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We assume access to two datasets $\\mathcal{D}_{i n}$ and $\\mathcal{D}_{o u t}$ , comprising i.i.d. samples from $\\mathbb{P}$ and $\\mathbb{Q}$ , respectively. This assumption can be relaxed, such as when using samples from a Markov chain or a Markov Decision Process to approximate population objectives. For scalability, we operate in a minibatch setting, where batches $B\\,=\\,(B_{o u t},B_{i n})$ are sub-sampled from datasets $\\bar{D}:=(D_{o u t},D_{i n})$ . Approximating both inner and outer level objectives in (6) can be achieved using empirical versions: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{L}_{o u t}\\left(\\omega,h,\\mathcal{B}_{o u t}\\right):=\\frac{1}{\\left|\\mathcal{B}_{o u t}\\right|}\\sum_{\\atop{(\\tilde{x},\\tilde{y})\\in\\mathcal{B}_{o u t}}}\\ell_{o u t}\\left(\\omega,h(\\tilde{x}),\\tilde{x},\\tilde{y}\\right),}\\\\ &{\\quad\\hat{L}_{i n}\\left(\\omega,h,\\mathcal{B}_{i n}\\right):=\\frac{1}{\\left|\\mathcal{B}_{i n}\\right|}\\sum_{\\atop{(x,y)\\in\\mathcal{B}_{i n}}}\\ell_{i n}\\left(\\omega,h(x),x,y\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Adjoint objective. Using the expression of $L_{a d j}$ from Proposition 2.3, we derive a finite-sample approximation of the adjoint loss by replacing the population expectations by their empirical counterparts. More precisely, assuming we have access to an approximation $\\hat{h}_{\\omega}$ to the inner-level prediction function, we consider the following empirical version of the adjoint objective: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{l}{\\hat{L}_{a d j}\\left(\\omega,a,\\hat{h}_{\\omega},\\mathcal{B}\\right):=\\frac{1}{2}\\frac{1}{\\left|\\mathcal{B}_{i n}\\right|}\\displaystyle\\sum_{(x,y)\\in\\mathcal{B}_{i n}}a(x)^{\\top}\\partial_{v}^{2}\\ell_{i n}(\\omega,\\hat{h}_{\\omega}(x),x,y)\\;a(x)}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\;\\frac{1}{\\left|\\mathcal{B}_{o u t}\\right|}\\displaystyle\\sum_{(x,y)\\in\\mathcal{B}_{o u t}}a(x)^{\\top}\\partial_{v}\\ell_{o u t}\\left(\\omega,\\hat{h}_{\\omega}(x),x,y\\right).}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The adjoint objective in Equation (9) requires computing a Hessian-vector product with respect to the output $v$ in $\\mathbb{R}^{d_{v}}$ of the prediction function $\\hat{h}_{\\omega}$ , which is typically of reasonably small dimension, unlike traditional AID methods that necessitate a Hessian-vector product with respect to some model parameters. Importantly, compared to AID, FuncID does not requires differentiating twice w.r.t the model\u2019s parameters $\\tau(\\theta)$ which results in memory and time savings as discussed in Appendix F.2. ", "page_idx": 5}, {"type": "text", "text": "3.2 Approximate prediction and adjoint functions ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To find approximate solutions to the prediction and adjoint functions we rely on three steps: 1) specifying parametric search spaces for both functions, 2) introducing optional regularization to prevent overfitting and, 3) defining a gradient-based optimization procedure on the empirical objectives. ", "page_idx": 5}, {"type": "text", "text": "Parametric search spaces. We approximate both prediction and adjoint functions using parametric search spaces. A parametric family of functions defined by a map $\\tau:\\Theta\\to\\mathcal{H}$ over parameters $\\Theta\\subseteq\\mathbb{R}^{p_{i n}}$ constrains the prediction function $h$ as $h(x)=\\tau(\\theta)\\dot{(x)}$ . We only require $\\tau$ to be continuous and differentiable almost everywhere such that back-propagation can be applied [Bolte et al., 2021]. Notably, unlike AID, we do not need $\\tau$ to be twice differentiable, as functional implicit differentiation computes the Hessian w.r.t. the output of $\\tau$ , not w.r.t. its parameters $\\theta$ . For flexibility, we can consider a different parameterized model $\\nu:\\Xi\\to\\mathcal{H}$ for approximating the adjoint function, defined over parameters $\\Xi\\subseteq\\mathbb{R}^{p_{a d j}}$ , constraining the adjoint similarly to $\\tau$ . In practice, we often use the same parameterization, typically a neural network, for both the inner-level and the adjoint models. ", "page_idx": 5}, {"type": "text", "text": "Regularization. With empirical objectives and parametric search spaces, we can directly optimize parameters of both the inner-level model $\\tau$ and the adjoint model $\\nu$ . However, to address finite sample issues, regularization may be introduced to these empirical objectives for better generalization. The method allows flexibility in regularization choice, accommodating functions $\\bar{\\theta_{\\vdash}}\\bar{\\cal R}_{i n}(\\theta)$ and $\\xi\\mapsto R_{a d j}(\\xi)$ , such as ridge penalty or other commonly used regularization techniques ", "page_idx": 5}, {"type": "text", "text": "Optimization. The function InnerOpt (defined in Algorithm 2) optimizes inner model parameters for a given $\\omega$ , initialization $\\theta_{0}$ , and data $\\mathcal{D}_{i n}$ , using $M$ gradient updates. It returns optimized parameters $\\theta_{M}$ and the corresponding inner model $\\hat{h}_{\\omega}\\,=\\,\\tau(\\theta_{M})$ , approximating the inner-level solution. Similarly, AdjointOpt (defined in Algorithm 3) optimizes adjoint model parameters with $K$ gradient updates, producing the approximate adjoint function $\\hat{a}_{\\omega}=\\nu(\\xi_{K})$ . Other optimization procedures may also be used, especially when closed-form solutions are available, as exploited in some experiments in Section 4. Operations requiring differentiation can be implemented using standard optimization procedures with automatic differentiation packages like PyTorch [Paszke et al., 2019] or Jax [Bradbury et al., 2018]. ", "page_idx": 5}, {"type": "table", "img_path": "enlxHLwwFf/tmp/732aadcd3ed50ee77c364fc662b9a0b913481b9e9f0081ccd00b8699553e849f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "3.3 Total gradient estimation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We exploit Proposition 2.3 to derive Algorithm 4, which allows us to approximate the total gradient $\\nabla{\\mathcal{F}}(\\omega)$ after computing the approximate solutions $\\hat{h}_{\\omega}$ and $\\hat{a}_{\\omega}$ . There, we decompose the gradient into two terms: $g_{E x p}$ , an empirical approximation of $g_{\\omega}$ in Equation (8) representing the explicit dependence of $L_{o u t}$ on the outer variable $\\omega$ , and $g_{I m p}$ , an approximation to the implicit gradient term $B_{\\omega}a_{\\omega}^{\\star}$ in Equation (8). Both terms are obtained by replacing the expectations by empirical averages batches $B_{i n}$ and $B_{o u t}$ , and using the approximations $\\hat{h}_{\\omega}$ and $\\hat{a}_{\\omega}$ instead of the exact solutions. ", "page_idx": 6}, {"type": "table", "img_path": "enlxHLwwFf/tmp/bf0e344d5bbcd59aee7ea0550effa5495326aaab51ecce7cbf634fdd50b4c7e7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "3.4 Convergence Guarantees ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Convergence of Algorithm 1 to stationary points of $\\mathcal{F}$ depends on approximation errors, which result from sub-optimal inner and adjoint solutions, as shown by the convergence result below. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.1. Assume that $\\mathcal{F}$ is $\\mathcal{L}$ -smooth and admits a finite lower bound $\\mathcal{F}^{\\star}$ . Use an update rule $\\omega_{n+1}=\\omega_{n}-\\eta g_{o u t}$ with step size $\\begin{array}{r}{0\\,<\\,\\eta\\,\\leq\\,\\frac{1}{4\\mathcal{L}_{\\omega}}}\\end{array}$ in Algorithm $^{\\,l}$ . Under Assumption (a) on sub-optimality of $\\hat{h}_{\\omega}$ and $\\hat{a}_{\\omega}$ , Assumptions $(b)$ to $(h)$ on the smoothness of $\\ell_{i n}$ and $\\ell_{o u t}$ , all stated in Appendix $E.I$ , and the assumptions in Proposition 2.3, the iterates $\\{\\omega_{n}\\}_{n\\ge0}$ of Algorithm $^{\\,l}$ satisfy: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{0\\leq n\\leq N-1}\\mathbb{E}\\left[\\left\\|\\nabla\\mathcal{F}(\\omega_{n})\\right\\|^{2}\\right]\\leq\\frac{4\\left(\\mathcal{F}(\\omega_{0})-\\mathcal{F}^{\\star}\\right)}{\\eta N}+2\\eta\\mathcal{L}\\sigma_{e f f}^{2}+{\\left(c_{1}\\epsilon_{i n}+c_{2}\\epsilon_{a d j}\\right)},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $c_{1},\\,c_{2},\\sigma_{e f f}^{2}$ are positive constants, and $\\epsilon_{i n},\\epsilon_{a d j}$ are sub-optimality errors that result from the inner and adjoint optimization procedures. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.1 is proven in Appendix E and relies on the general convergence result in Demidovich et al. [2024, Theorem 3] for stochastic biased gradient methods. The key idea is to control both bias and variance of the gradient estimator in terms of generalization errors $\\epsilon_{i n}$ and $\\epsilon_{a d j}$ when approximating the inner and adjoint solutions. These generalization errors can, in turn, be made smaller in the case of over-parameterized networks, by increasing network capacity, number of steps and sample size [Allen-Zhu et al., 2019, Du et al., 2019, Zou et al., 2020]. ", "page_idx": 6}, {"type": "text", "text": "4 Applications ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We consider two applications of the functional bilevel optimization problem: Two-stage least squares regression (2SLS) and Model-based reinforcement learning. To illustrate its effectiveness we compare it with other bilevel optimization approaches like AID or ITD, as well as state-of-the-art methods for each application. We provide a versatile implementation of FuncID (https://github.com/inria-thoth/funcBO) in PyTorch [Paszke et al., 2019], compatible with standard optimizers (e.g., Adam [Kingma and Ba, 2015]), and supports common regularization techniques. For the reinforcement learning application, we extend an existing JAX [Bradbury et al., 2018] implementation of model-based RL from Nikishin et al. [2022] to apply FuncID. To ensure fairness, experiments are conducted with comparable computational budgets for hyperparameter tuning using the MLXP experiment management tool [Arbel and Zouaoui, 2024]. Additionally, we maintain consistency by employing identical neural network architectures across all methods and repeating experiments multiple times with different random seeds. ", "page_idx": 6}, {"type": "text", "text": "4.1 Two-stage least squares regression (2SLS) ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Two-stage least squares regression (2SLS) is commonly used in causal representation learning, including instrumental regression or proxy causal learning [Stock and Trebbi, 2003]. Recent studies have applied bilevel optimization approaches to address 2SLS, yielding promising results [Xu et al., ", "page_idx": 6}, {"type": "image", "img_path": "enlxHLwwFf/tmp/42958e2f36b021322fe964c15ac10068386ad334bcbc92320c5569902b870fee.jpg", "img_caption": ["Figure 2: Performance metrics for Instrumental Variable (IV) regression. All results are averaged over 20 runs with 5000 training samples and 588 test samples. (Left) box plot of the test loss, with the dashed black line indicating the mean test error. (Middle) outer loss vs training iterations, (Right) inner loss vs training iterations. The bold lines in the middle and right plots indicate the mean loss, the shaded area corresponds to standard deviation. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "2021b,a, Hu et al., 2023]. We particularly focus on 2SLS for Instrumental Variable (IV) regression, a widely-used statistical framework for mitigating endogeneity in econometrics [Blundell et al., 2007, 2012], medical economics [Cawley and Meyerhoefer, 2012], sociology [Bollen, 2012], and more recently, for handling confounders in off-line reinforcement learning [Fu et al., 2022]. ", "page_idx": 7}, {"type": "text", "text": "Problem formulation. In an IV problem, the objective is to model $f_{\\omega}:t\\mapsto o$ that approximates the structural function $f_{s t r u c t}$ using independent samples $(o,t,x)$ from a data distribution $\\mathbb{P}$ , where $x$ is an instrumental variable. The structural function $f_{s t r u c t}$ delineates the true effect of a treatment $t$ on an outcome $o$ . A significant challenge in IV is the presence of an unobserved confounder $\\epsilon$ , which influences both $t$ and $o$ additively, rendering standard regression ineffective for recovering $f_{\\omega}$ . However, if the instrumental variable $x$ solely impacts the outcome $o$ through the treatment $t$ and is independent from the confounder $\\epsilon$ , it can be employed to elucidate the direct relationship between the treatment $t$ and the outcome $o$ using the 2SLS framework, under mild assumptions on the confounder [Singh et al., 2019]. This adaptation replaces the regression problem with a variant that averages the effect of the treatment $t$ conditionally on $x$ ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\underset{\\omega\\in\\Omega}{\\operatorname*{min}}\\mathbb{E}_{\\mathbb{P}}\\left[\\left\\|o-\\mathbb{E}_{\\mathbb{P}}\\left[f_{\\omega}(t)|x\\right]\\right\\|^{2}\\right].\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Directly estimating the conditional expectation $\\mathbb{E}_{\\mathbb{P}}\\left[f_{\\omega}(t)|x\\right]$ is hard in general. Instead, it is easier to express it, equivalently, as the solution of another regression problem predicting $f_{\\omega}(t)$ from $x$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\nh_{\\omega}^{\\star}:=\\mathbb{E}_{\\mathbb{P}}\\left[f_{\\omega}(t)|x\\right]=\\arg\\operatorname*{min}_{h\\in\\mathcal{H}}\\mathbb{E}_{\\mathbb{P}}\\left[\\left\\|f_{\\omega}(t)-h(x)\\right\\|^{2}\\right].\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Both equations result in the bilevel formulation in Equation (6) with $\\textit{y}=\\textit{(t,o)}$ , $\\mathbb{Q}\\;=\\;\\mathbb{P}$ and the point-wise losses $\\ell_{i n}$ and $\\ell_{o u t}$ given by $\\ell_{i n}(\\omega,v,x,y)\\,=\\,\\ell_{i n}(\\omega,v,x,(t,o))\\,=\\,\\left\\|f_{\\omega}(t)-v\\right\\|^{2}$ and $\\ell_{o u t}(\\omega,v,x,y)\\,=\\,\\ell_{o u t}(\\omega,v,x,(t,o))\\,=\\,\\|o-v\\|^{2}$ . It is, therefore, possible to directly apply Algorithm 1 to learn $f_{\\omega}$ as we illustrate below. ", "page_idx": 7}, {"type": "text", "text": "Experimental setup. We study the IV problem using the dsprites dataset [Matthey et al., 2017], comprising synthetic images representing single objects generated from five latent parameters: shape, scale, rotation, and posX, posY positions on image coordinates. Here, the treatment variable $t$ is the images, the hidden confounder $\\epsilon$ is the posY coordinate, and the other four latent variables form the instrumental variable $x$ . The outcome $o$ is an unknown structural function $f_{s t r u c t}$ of $t$ , contaminated by confounder $\\epsilon$ as detailed in Appendix G.1. We follow the setup of the Deep Feature Instrumental Variable Regression (DFIV) dsprites experiment by $\\mathrm{\\DeltaXu}$ et al. [2021a, Section 4.2], which achieves state-of-the-art performance. In this setup, neural networks serve as the prediction function and structural model, optimized to solve the bilevel problem in Equations (10) and (11). We explore two versions of our method: FuncID, which optimizes all adjoint network parameters, and FuncID linear, which learns only the last layer in closed-form while inheriting hidden layer parameters from the inner prediction function. We compare our method with DFIV, AID, ITD, and Penalty-based methods: gradient penalty (GD penal.) and value function penalty (Val penal.) [Shen and Chen, 2023], using identical network architectures and computational budgets for hyperparameter selection. Full details on network architectures, hyperparameters, and training settings are provided in Appendix G.2. ", "page_idx": 7}, {"type": "text", "text": "Results. Figure 2 compares structural models learned by different methods using 5K training samples (refer to Figure 6 in Appendix G.3 for 10K sample results). The left subplot illustrates out-of-sample mean squared error of learned structural models compared to ground truth outcomes (uncontaminated by noise $\\epsilon$ ), while the middle and right subplots show the evolution of outer and inner objectives over iterations. For the 5K dataset, FuncID outperforms DFIV ( $p$ -valu $\\scriptstyle=0.003$ , one-sided paired t-test), while showing comparable performance on the 10K dataset (Figure 6). AID and ITD perform notably worse, indicating their parametric approach fails to fully leverage the functional structure. FuncID outperforms the gradient penalty method and performs either better or comparably to the value function penalty method, though the latter shows higher variance with some particularly bad outliers. While all methods achieve similar outer losses, this criterion alone is only reliable as an indicator of convergence when evaluated near the \u2018exact\u2019 inner-level solution corresponding to the lowest inner-loss values. Interestingly, FuncID obtains the lowest inner-loss values, suggesting its outer-loss is a more reliable indicator of convergence. ", "page_idx": 8}, {"type": "text", "text": "4.2 Model-based reinforcement learning ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Model-based reinforcement learning (RL) naturally yields bilevel optimization formulations, since several components of an RL agent need to be learned using different objectives. Recently, Nikishin et al. [2022] showed that casting model-based RL as a bilevel problem can result in better tolerance to model-misspecification. Our experiments show that the functional bilevel framework yields improved results even when the model is well-specified, suggesting a broader use of the bilevel formulation. ", "page_idx": 8}, {"type": "text", "text": "Problem formulation. In model-based RL, the Markov Decision Process (MDP) is approximated by a probabilistic model $q_{\\omega}$ with parameters $\\omega$ that can predict the next state $s_{\\omega}(x)$ and reward $r_{\\omega}(x)$ , given a pair $\\boldsymbol{x}:=(s,a)$ where $s$ is the current environment state and $a$ is the action of an agent. A second model $h$ can be used to approximate the action-value function $h(x)$ that computes the expected cumulative reward given the current state-action pair. Traditionally, the action-value function is learned using the current MDP model, while the latter is learned independently from the action-value function using Maximum Likelihood Estimation (MLE) [Sutton, 1991]. ", "page_idx": 8}, {"type": "text", "text": "In the bilevel formulation of model-based RL by Nikishin et al. [2022], the inner-level problem involves learning the optimal action-value function $h_{\\omega}^{\\star}$ with the current MDP model $q_{\\omega}$ and minimizing the Bellman error. The inner-level objective can be expressed as an expectation of a point-wise loss $f$ with samples $(x,r^{\\prime},s^{\\prime})\\sim\\mathbb{P}$ , derived from the agent-environment interaction: ", "page_idx": 8}, {"type": "equation", "text": "$$\nh_{\\omega}^{\\star}=\\arg\\operatorname*{min}_{h\\in\\mathcal{H}}\\mathbb{E}_{\\mathbb{P}}\\left[f(h(x),r_{\\omega}(x),s_{\\omega}(x))\\right].\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Here, the future state and reward $(r^{\\prime},s^{\\prime})$ are replaced by the MDP model predictions $r_{\\omega}(x)$ and $s_{\\omega}(x)$ . In practice, samples from $\\mathbb{P}$ are obtained using a replay buffer. The buffer accumulates data over several episodes of interactions with the environment, and can therefore be considered independent of the agent\u2019s policy. The point-wise loss function $f$ represents the error between the action-value function prediction and the expected cumulative reward given the current state-action pair: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f(v,r^{\\prime},s^{\\prime}):=\\displaystyle\\frac{1}{2}\\left\\|v-r^{\\prime}-\\gamma\\log\\sum_{a^{\\prime}}e^{\\bar{h}(s^{\\prime},a^{\\prime})}\\right\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "with $\\bar{h}$ a lagged version of $h$ (exponentially averaged network) and $\\gamma$ a discount factor. The MDP model is learned implicitly using the optimal function $h_{\\omega}^{\\star}$ , by minimizing the Bellman error w.r.t. $\\omega$ : ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\omega\\in\\Omega}\\mathbb{E}_{\\mathbb{P}}\\left[f(h_{\\omega}^{\\star}(x),r^{\\prime},s^{\\prime})\\right].\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Equations (12) and (13) define a bilevel problem as in Equation (6), where $\\mathbb{Q}=\\mathbb{P}$ , $\\boldsymbol{y}\\,=\\,(\\boldsymbol{r}^{\\prime},\\boldsymbol{s}^{\\prime})$ , and the point-wise losses $\\ell_{i n}$ and $\\ell_{o u t}$ are given by: $\\ell_{i n}\\,(\\omega,v,x,y)\\;=\\;f\\,(v,r_{\\omega}(x),s_{\\omega}(x))$ and $\\ell_{o u t}\\left(\\omega,\\bar{\\upsilon},x,y\\right)=f\\left(\\upsilon,r^{\\prime},s^{\\prime}\\right)$ . Therefore, we can directly apply Algorithm 1 to learn both the MDP model $q_{\\omega}$ and the optimal action-value function $h_{\\omega}^{\\star}$ . ", "page_idx": 8}, {"type": "text", "text": "Experimental setup. We apply FuncID to the CartPole control problem, a classic benchmark in reinforcement learning [Brockman et al., 2016, Nagendra et al., 2017]. The goal is to balance a pole attached to a cart by moving the cart horizontally. Following Nikishin et al. [2022], we use a modelbased approach and consider two scenarios: one with a well-specified network accurately representing the MDP, and another with a misspecified model having fewer hidden layer units, limiting its capacity. ", "page_idx": 8}, {"type": "image", "img_path": "enlxHLwwFf/tmp/3c063481acd5e01f9c1fc6b9f2d6fecd7566a4a94fe3f1c706ce6fc0ea089672.jpg", "img_caption": ["Figure 3: Average reward on an evaluation environment vs. training iterations on the CartPole task. (Left) Well-specified model with 32 hidden units. (Right) Misspecified model with 3 hidden units. Both plots show mean reward over 10 runs where the shaded region is the $95\\%$ confidence interval. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Using the bilevel formulation in Equations (12) and (13), we compare FuncID with the Optimal Model Design (OMD) algorithm [Nikishin et al., 2022], a variant of AID. Additionally, we compare against a standard single-level model-based RL formulation using Maximum Likelihood Estimation (MLE) [Sutton, 1991]. For the adjoint function in FuncID, we derive a simple closed-form expression based on the structure of the adjoint objective (see Appendix H.1). We follow the experimental setup of Nikishin et al. [2022], providing full details and hyperparameters in Appendix H.2. ", "page_idx": 9}, {"type": "text", "text": "Results. Figure 3 illustrates the training reward evolution for FuncID, OMD, and MLE in both well-specified and misspecified scenarios. FuncID consistently performs well across settings. In the well-specified case, where OMD achieves a reward of 4, FuncID reaches the maximum reward of 5, matching MLE (left Figure 3). In the misspecified scenario, FuncID performs comparably to OMD and significantly outperforms MLE (right Figure 3). Moreover, FuncID tends to converge faster than MLE (see Figure 7 in Appendix H.3) and yields consistently better prediction error than OMD (see Figure 8 in Appendix H.3). These findings align with Nikishin et al. [2022], suggesting that MLE may prioritize minimizing prediction errors, potentially leading to overftiting irrelevant features. In contrast, OMD and FuncID focus on maximizing expected returns, especially in the presence of model misspecification. Our results highlight the effectiveness of (FBO) even in well-specified settings, suggesting, for future work, further investigations for more general RL tasks. ", "page_idx": 9}, {"type": "text", "text": "5 Discussion and concluding remarks ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper introduced a functional paradigm for bilevel optimization in machine learning, shifting focus from parameter space to function space. The proposed approach specifically addresses the ambiguity challenge arising from using deep networks in bilevel optimization. The paper establishes the validity of the functional framework by developing a theory of functional implicit differentiation, proving convergence for the proposed FuncID method, and numerically comparing it with other bilevel optimization methods. ", "page_idx": 9}, {"type": "text", "text": "The theoretical foundations of our work rely on several key assumptions worth examining. While our convergence guarantees assume both inner and adjoint optimization problems are solved to some optimality, this assumption is supported by recent results on global convergence in over-parameterized networks [Allen-Zhu et al., 2019, Liu et al., 2022a]. However, quantifying these optimality errors more precisely and understanding their relationship to optimization procedures remains an open challenge. Additionally, like other bilevel methods, our approach requires careful hyperparameter selection, which can impact practical implementation. ", "page_idx": 9}, {"type": "text", "text": "Several promising directions emerge for future research. While we focus on $L_{2}$ spaces, exploring alternative function spaces (such as Reproducing Kernel Hilbert Spaces or Sobolev spaces) could reveal additional advantages for specific applications. Furthermore, extending our framework to non-smooth objectives or constrained optimization problems, potentially building on existing work in non-smooth implicit differentiation [Bolte et al., 2022], would broaden its applicability. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported by the ERC grant number 101087696 (APHELAIA project) and by ANR 3IA MIAI $@$ Grenoble Alpes (ANR-19-P3IA-0003) and the ANR project BONSAI (grant ANR-23- CE23-0012-01). We thank Edouard Pauwels and Samuel Vaiter for their insightful discussions. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "P. Ablin, G. Peyr\u00e9, and T. Moreau. Super-efficiency of automatic differentiation for functions defined as a minimum. International Conference on Machine Learning (ICML), 2020.   \nK. Ahuja, K. Shanmugam, K. Varshney, and A. Dhurandhar. Invariant risk minimization games. International Conference on Machine Learning (ICML), 2020.   \nZ. Allen-Zhu, Y. Li, and Z. Song. A convergence theory for deep learning via over-parameterization. In International Conference on Machine Learning (ICML), 2019.   \nB. Amos et al. Tutorial on amortized optimization. Foundations and Trends\u00ae in Machine Learning, 16(5):592\u2013732, 2023.   \nM. Arbel and J. Mairal. Amortized implicit differentiation for stochastic bilevel optimization. International Conference on Learning Representations (ICLR), 2022a.   \nM. Arbel and J. Mairal. Non-convex bilevel games with critical point selection maps. Advances in Neural Information Processing Systems (NeurIPS), 2022b.   \nM. Arbel and A. Zouaoui. Mlxp: A framework for conducting replicable machine learning experiments in python. arXiv preprint arXiv:2402.13831, 2024. URL https://github.com/ inria-thoth/mlxp.   \nM. Arjovsky, L. Bottou, I. Gulrajani, and D. Lopez-Paz. Invariant risk minimization. arXiv preprint 1907.02893, 2019.   \nJ. Bae and R. B. Grosse. Delta-stn: Efficient bilevel optimization for neural networks using structured response jacobians. Advances in Neural Information Processing Systems (NeurIPS), 2020.   \nD. Bansal, R. T. Chen, M. Mukadam, and B. Amos. Taskmet: Task-driven metric learning for model learning. Advances in Neural Information Processing Systems (NeurIPS), 2023.   \nH. H. Bauschke and P. L. Combettes. Convex Analysis and Monotone Operator Theory in Hilbert Spaces. Springer Publishing Company, Incorporated, 1st edition, 2011. ISBN 1441994661.   \nA. G. Baydin, B. A. Pearlmutter, A. A. Radul, and J. M. Siskind. Automatic differentiation in machine learning: A survey. Journal of Machine Learning Research (JMLR), 18(153):1\u201343, 2017.   \nY. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is difficult. IEEE Transactions on Neural Networks, 5(2):157\u2013166, 1994.   \nK. P. Bennett, J. Hu, X. Ji, G. Kunapuli, and J.-S. Pang. Model selection via bilevel optimization. IEEE International Joint Conference on Neural Network Proceedings, 2006.   \nL. Bertinetto, J. F. Henriques, P. H. Torr, and A. Vedaldi. Meta-learning with differentiable closedform solvers. International Conference on Learning Representations (ICLR), 2019.   \nM. Bin\u00b4kowski, D. J. Sutherland, M. Arbel, and A. Gretton. Demystifying mmd gans. arXiv preprint arXiv:1801.01401, 2018.   \nM. Blondel, Q. Berthet, M. Cuturi, R. Frostig, S. Hoyer, F. Llinares-L\u00f3pez, F. Pedregosa, and J.-P. Vert. Efficient and modular implicit differentiation. Advances in Neural Information Processing Systems (NeurIPS), 2022.   \nR. Blundell, X. Chen, and D. Kristensen. Semi-nonparametric iv estimation of shape-invariant engel curves. Econometrica, 75(6):1613\u20131669, 2007.   \nR. Blundell, J. L. Horowitz, and M. Parey. Measuring the price responsiveness of gasoline demand: Economic shape restrictions and nonparametric demand estimation. Quantitative Economics, 3(1): 29\u201351, 2012.   \nK. A. Bollen. Instrumental variables in sociology and the social sciences. Annual Review of Sociology, 38:37\u201372, 2012.   \nJ. Bolte, T. Le, E. Pauwels, and T. Silveti-Falls. Nonsmooth implicit differentiation for machinelearning and optimization. Advances in Neural Information Processing Systems (NeurIPS), 2021.   \nJ. Bolte, E. Pauwels, and S. Vaiter. Automatic differentiation of nonsmooth iterative algorithms. Advances in Neural Information Processing Systems (NeurIPS), 2022.   \nJ. Bolte, E. Pauwels, and S. Vaiter. One-step differentiation of iterative algorithms. Advances in Neural Information Processing Systems (NeurIPS), 2024.   \nJ. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, and Q. Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.   \nA. Brock, T. Lim, J. Ritchie, and N. Weston. SMASH: One-shot model architecture search through hypernetworks. International Conference on Learning Representations (ICLR), 2018.   \nG. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. Openai gym. arXiv preprint 1606.01540, 2016.   \nJ. Cawley and C. Meyerhoefer. The medical care costs of obesity: an instrumental variables approach. Journal of Health Economics, 31(1):219\u2013230, 2012.   \nR. T. Chen, Y. Rubanova, J. Bettencourt, and D. K. Duvenaud. Neural ordinary differential equations. Advances in Neural Information Processing Systems (NeurIPS), 2018.   \nL. Debnath and P. Mikusinski. Introduction to Hilbert spaces with applications. Academic press, 2005.   \nY. Demidovich, G. Malinovsky, I. Sokolov, and P. Richt\u00e1rik. A guide through the zoo of biased sgd. Advances in Neural Information Processing Systems (NeurIPS), 2024.   \nS. Dempe, J. Dutta, and B. Mordukhovich. New necessary optimality conditions in optimistic bilevel programming. Optimization, 56(5-6):577\u2013604, 2007.   \nJ. Domke. Generic methods for optimization-based modeling. International Conference on Artificial Intelligence and Statistics (AISTATS), 2012.   \nS. Du, J. Lee, H. Li, L. Wang, and X. Zhai. Gradient descent finds global minima of deep neural networks. In International Conference on Machine Learning (ICML), 2019.   \nZ. Fang and A. Santos. Inference on Directionally Differentiable Functions. The Review of Economic Studies, 86(1):377\u2013412, 2018.   \nM. Feurer and F. Hutter. Hyperparameter optimization. Springer International Publishing, 2019.   \nL. Franceschi, M. Donini, P. Frasconi, and M. Pontil. Forward and reverse gradient-based hyperparameter optimization. International Conference on Machine Learning (ICML), 2017.   \nZ. Fu, Z. Qi, Z. Wang, Z. Yang, Y. Xu, and M. R. Kosorok. Offline reinforcement learning with instrumental variables in confounded markov decision processes. arXiv preprint 2209.08666, 2022.   \nS. Ghadimi and M. Wang. Approximation methods for bilevel programming. Optimization and Control, 2018.   \nV. Goodman. Quasi-differentiable functions of banach spaces. Proceedings of the American Mathematical Society, 30(2):367\u2013370, 1971.   \nS. Gould, B. Fernando, A. Cherian, P. Anderson, R. S. Cruz, and E. Guo. On differentiating parameterized argmin and argmax problems with application to bi-level optimization. arXiv preprint 1607.05447, 2016.   \nR. Grazzi, L. Franceschi, M. Pontil, and S. Salzo. On the iteration complexity of hypergradient computation. International Conference on Machine Learning (ICML), 2020.   \nD. Ha, A. M. Dai, and Q. V. Le. Hypernetworks. International Conference on Learning Representations (ICLR), 2017.   \nG. Holler, K. Kunisch, and R. C. Barnard. A bilevel approach for parameter learning in inverse problems. Inverse Problems, 34(11):115012, 2018.   \nM. Hong, H. Wai, Z. Wang, and Z. Yang. A two-timescale stochastic algorithm framework for bilevel optimization: Complexity analysis and application to actor-critic. SIAM Journal on Optimization, 33(1):147\u2013180, 2023.   \nY. Hu, J. Wang, Y. Xie, A. Krause, and D. Kuhn. Contextual stochastic bilevel optimization. Advances in Neural Information Processing Systems (NeurIPS), 2023.   \nA. D. Ioffe and V. M. Tihomirov. Theory of Extremal Problems. Series: Studies in Mathematics and its Applications 6. Elsevier, 1979.   \nK. Ji, J. Yang, and Y. Liang. Bilevel optimization: Convergence analysis and enhanced design. International Conference on Machine Learning (ICML), 2021.   \nJ. Jia and A. R. Benson. Neural jump stochastic differential equations. Advances in Neural Information Processing Systems (NeurIPS), 2019.   \nD. P. Kingma and J. Ba. Adam: A method for stochastic optimization. International Conference on Learning Representations (ICLR), 2015.   \nD. P. Kingma and M. Welling. Auto-encoding variational bayes. International Conference on Learning Representations (ICLR), 2014.   \nG. Kunapuli, K. Bennett, J. Hu, and J.-S. Pang. Bilevel model selection for support vector machines. CRM Proceedings and Lecture Notes, 45:129\u2013158, 2008.   \nJ. Kwon, D. Kwon, S. Wright, and R. D. Nowak. On penalty methods for nonconvex bilevel optimization and first-order stochastic approximation. In International Conference on Learning Representations (ICLR), 2024.   \nS. Li, Z. Wang, A. Narayan, R. Kirby, and S. Zhe. Meta-learning with adjoint methods. International Conference on Artificial Intelligence and Statistics (AISTATS), 2023.   \nX. Li, T.-K. L. Wong, R. T. Chen, and D. Duvenaud. Scalable gradients for stochastic differential equations. International Conference on Artificial Intelligence and Statistics (AISTATS), 2020.   \nR. Liao, Y. Xiong, E. Fetaya, L. Zhang, K. Yoon, X. Pitkow, R. Urtasun, and R. Zemel. Reviving and improving recurrent back-propagation. International Conference on Machine Learning (ICML), 2018.   \nC. Liu, L. Zhu, and M. Belkin. Loss landscapes and optimization in over-parameterized non-linear systems and neural networks. Applied and Computational Harmonic Analysis, 59:85\u2013116, 2022a.   \nR. Liu, X. Liu, S. Zeng, J. Zhang, and Y. Zhang. Value-function-based sequential minimization for bi-level optimization. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 45:15930\u201315948, 2021a.   \nR. Liu, Y. Liu, S. Zeng, and J. Zhang. Towards gradient-based bilevel optimization with non-convex followers and beyond. Advances in Neural Information Processing Systems (NeurIPS), 2021b.   \nR. Liu, J. Gao, J. Zhang, D. Meng, and Z. Lin. Investigating bi-level optimization for learning and vision from a unified perspective: a survey and beyond. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 44:10045\u201310067, 2022b.   \nR. Liu, Y. Liu, W. Yao, S. Zeng, and J. Zhang. Averaged method of multipliers for bi-level optimization without lower-level strong convexity. In International Conference on Machine Learning (ICML), pages 21839\u201321866. PMLR, 2023.   \nJ. Lorraine, P. Vicol, and D. K. Duvenaud. Optimizing millions of hyperparameters by implicit differentiation. International Conference on Artificial Intelligence and Statistics (AISTATS), 2019.   \nM. MacKay, P. Vicol, J. Lorraine, D. Duvenaud, and R. B. Grosse. Self-tuning networks: Bilevel optimization of hyperparameters using structured best-response functions. International Conference on Learning Representations (ICLR), 2019.   \nJ. Mairal, F. Bach, and J. Ponce. Task-driven dictionary learning. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 34(4):791\u2013804, 2012.   \nC. C. Margossian and M. Betancourt. Efficient automatic differentiation of implicit functions. arXiv preprint 2112.14217, 2021.   \nP. Marion, A. Korba, P. Bartlett, M. Blondel, V. De Bortoli, A. Doucet, F. Llinares-L\u00f3pez, C. Paquette, and Q. Berthet. Implicit diffusion: Efficient optimization through stochastic sampling. arXiv preprint arXiv:2402.05468, 2024.   \nL. Matthey, I. Higgins, D. Hassabis, and A. Lerchner. dsprites: Disentanglement testing sprites dataset, 2017.   \nS. Nagendra, N. Podila, R. Ugarakhod, and K. George. Comparison of reinforcement learning algorithms applied to the cart-pole problem. International Conference on Advances in Computing, Communications and Informatics (ICACCI), 2017.   \nA. Navon, I. Achituve, H. Maron, G. Chechik, and E. Fetaya. Auxiliary learning by implicit differentiation. International Conference on Learning Representations (ICLR), 2021.   \nA. Nemirovski and S. Semenov. On polynomial approximation of functions on hilbert space. Mathematics of the USSR-Sbornik, 21(2):255, 1973.   \nE. Nikishin, R. Abachi, R. Agarwal, and P.-L. Bacon. Control-oriented model-based reinforcement learning with implicit differentiation. AAAI Conference on Artificial Intelligence, 2022.   \nD. Noll. Second order differentiability of integral functionals on Sobolev spaces and L2-spaces. Walter de Gruyter, Berlin/New York Berlin, New York, 1993.   \nR. Pascanu, T. Mikolov, and Y. Bengio. On the difficulty of training recurrent neural networks. International Conference on Machine Learning (ICML), 2013.   \nA. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. Pytorch: An imperative style, high-performance deep learning library. Advances in Neural Information Processing Systems (NeurIPS), 2019.   \nF. Pedregosa. Hyperparameter optimization with approximate gradient. International Conference on Machine Learning (ICML), 2016.   \nL. S. Pontryagin. Mathematical Theory of Optimal Processes. Routledge, 2018.   \nD. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference in deep generative models. International Conference on Machine Learning (ICML), 2014.   \nH. Robbins and S. Monro. A stochastic approximation method. The Annals of Mathematical Statistics, pages 400\u2013407, 1951.   \nS. Rosset. Bi-level path following for cross validated solution of kernel quantile regression. International Conference on Machine Learning (ICML), 2008.   \nA. Shaban, C.-A. Cheng, N. Hatch, and B. Boots. Truncated back-propagation for bilevel optimization. International Conference on Artificial Intelligence and Statistics (AISTATS), 2019.   \nH. Shen and T. Chen. On penalty-based bilevel gradient descent method. In International Conference on Machine Learning (ICML), 2023.   \nR. Singh, M. Sahani, and A. Gretton. Kernel instrumental variable regression. Advances in Neural Information Processing Systems (NIPS), 2019.   \nJ. H. Stock and F. Trebbi. Retrospectives: Who invented instrumental variable regression? Journal of Economic Perspectives, 17(3):177\u2013194, 2003.   \nE. Suonper\u00e4 and T. Valkonen. Linearly convergent bilevel optimization with single-step inner methods. Computational Optimization and Applications, 87(2):571\u2013610, 2024.   \nR. S. Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM Sigart Bulletin, 2(4):160\u2013163, 1991.   \nA. van der Vaart. Efficiency and hadamard differentiability. Scandinavian Journal of Statistics, 18(1): 63\u201375, 1991.   \nA. W. van der Vaart and J. A. Wellner. Weak Convergence and Empirical Processes with Applications to Statistics, pages 16\u201328. Springer New York, 1996.   \nP. Vicol, J. Lorraine, D. Duvenaud, and R. Grosse. Implicit regularization in overparameterized bilevel optimization. International Conference on Machine Learning (ICML), 2022.   \nC. Williams and M. Seeger. Using the nystr\u00f6m method to speed up kernel machines. Advances in Neural Information Processing Systems (NIPS), 2000.   \nL. Xu, Y. Chen, S. Srinivasan, N. de Freitas, A. Doucet, and A. Gretton. Learning deep features in instrumental variable regression. International Conference on Learning Representations (ICLR), 2021a.   \nL. Xu, H. Kanagawa, and A. Gretton. Deep proxy causal learning and its application to confounded bandit policy evaluation. Advances in Neural Information Processing Systems (NeurIPS), 2021b.   \nJ. Ye and X. Ye. Necessary optimality conditions for optimization problems with variational inequality constraints. Mathematics of Operations Research, 22(4):977\u2013997, 1997.   \nJ. Ye and D. Zhu. Optimality conditions for bilevel programming problems. Optimization, 33(1): 9\u201327, 1995.   \nJ. Ye, D. Zhu, and Q. J. Zhu. Exact penalization and necessary optimality conditions for generalized bilevel programming problems. SIAM Journal on Optimization, 7(2):481\u2013507, 1997.   \nC. Zhang, M. Ren, and R. Urtasun. Graph hypernetworks for neural architecture search. International Conference on Learning Representations (ICLR), 2019.   \nY. D. Zhong, B. Dey, and A. Chakraborty. Symplectic ode-net: Learning hamiltonian dynamics with control. arXiv preprint 1909.12077, 2019.   \nD. Zou, Y. Cao, D. Zhou, and Q. Gu. Gradient descent optimizes over-parameterized deep relu networks. Machine learning, 109:467\u2013492, 2020. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Examples of FBO formulations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The functional bilevel setting applies to various practical bilevel problems where objectives depend solely on model predictions, not their parameterization. Below, we discuss a few examples. ", "page_idx": 15}, {"type": "text", "text": "Auxiliary task learning. As in Equation (1), consider a main prediction task with features $x$ and labels $y$ , equipped with a loss function $f(y,h(x))$ . The goal of auxiliary task learning is to learn how a set of auxiliary tasks represented by a vector $f_{a u x}(y,h(x))$ could help solve the main task. This problem is formulated by Navon et al. [2021] as a bilevel problem, which can be written as (FBO) with ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{L_{o u t}(\\omega,h)=\\mathbb{E}_{(y,x)\\sim\\mathcal{D}_{v a l}}\\left[f(y,h(x))\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the loss is evaluated over a validation dataset $\\mathcal{D}_{v a l}$ , and ", "page_idx": 15}, {"type": "equation", "text": "$$\nL_{i n}(\\omega,h)=\\mathbb{E}_{(y,x)\\sim\\mathcal{D}_{t r a i n}}\\left[f(y,h(x))+g_{\\omega}(f_{a u x}(y,h(x)))\\right],\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where an independent training dataset $\\mathcal{D}_{t r a i n}$ is used, and $g_{\\omega}$ is a function that combines the auxiliary losses into a scalar value. ", "page_idx": 15}, {"type": "text", "text": "Task-driven metric learning. Considering now a regression problem with features $x$ and labels $y$ , the goal of task-driven metric learning formulated by Bansal et al. [2023] is to learn a metric parameterized by $\\omega$ for the regression task such that the corresponding predictor $h_{\\omega}^{\\star}$ performs well on a downstream task $L_{t a s k}$ . This can be formulated as (FBO) with $L_{o u t}(\\omega,h)=L_{t a s k}(h)$ and ", "page_idx": 15}, {"type": "equation", "text": "$$\nL_{i n}(\\omega,h)=\\mathbb{E}_{(y,x)}\\left[\\|y-h(x)\\|_{A_{\\omega}(x)}^{2}\\right],\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\|\\cdot\\|_{\\omega}^{2}$ is the squared Mahalanobis norm with parameters $\\omega$ and $A_{\\omega}(x)$ is a data-dependent metric that allows emphasizing features that are more important for the downstream task. ", "page_idx": 15}, {"type": "text", "text": "B Additional Related Work ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Bilevel optimization in machine learning. Two families of bilevel methods are prevalent in machine learning due to their scalability: iterative (or \u2019unrolled\u2019) differentiation (ITD, Baydin et al., 2017) and Approximate Implicit Differentiation (AID, Ghadimi and Wang, 2018). ITD approximates the optimal inner-level solution using an \u2019unrolled\u2019 function from a sequence of differentiable optimization steps, optimizing the outer variable via back-propagation [Shaban et al., 2019, Bolte et al., 2024]. The gradient approximation error decreases linearly with the number of steps when the inner-level is strongly convex, though at increased computational and memory costs [Grazzi et al., 2020, Theorem 2.1]. ITD is popular for its simplicity and availability in major deep learning libraries [Bradbury et al., 2018], but can be unstable, especially with non-convex inner objectives [Pascanu et al., 2013, Bengio et al., 1994, Arbel and Mairal, 2022b]. AID uses the Implicit Function Theorem (IFT) to derive the Jacobian of the inner-level solution with respect to the outer variable [Lorraine et al., 2019, Pedregosa, 2016], solving a finite-dimensional linear system for an adjoint vector representing optimality constraints. AID offers strong convergence guarantees for smooth, strongly convex inner objectives [Ji et al., 2021, Arbel and Mairal, 2022a]. However, without strong convexity, the linear system can become ill-posed due to a degenerate Hessian, leading to instabilities, especially with overparameterized deep neural networks. Our proposed approach avoids this issue, even when using deep networks for function approximation. ", "page_idx": 15}, {"type": "text", "text": "Adjoint sensitivity method. The adjoint sensitivity method [Pontryagin, 2018] efficiently differentiates a controlled variable with respect to a control parameter. In bilevel optimization, AID applies a finite-dimensional version of this method [Margossian and Betancourt, 2021, Section 2]. Infinite-dimensional versions have differentiated solutions of ordinary differential equations (ODEs) with respect to defining parameters [Margossian and Betancourt, 2021, Section 3], and have been used in machine learning to optimize parameters of a vector field describing an ODE [Chen et al., 2018]. Here, the ODE\u2019s vector field, parameterized by a neural network, is optimized to match observations. The adjoint sensitivity method offers an efficient alternative to the unstable process of back-propagation through ODE solvers, requiring only the solution of an adjoint ODE to compute gradient updates, improving performance [Jia and Benson, 2019, Zhong et al., 2019, Li et al., 2020]. This method has been adapted to meta-learning [Li et al., 2023], viewing the inner optimization as an ", "page_idx": 15}, {"type": "text", "text": "ODE evolution with gradients obtained via the adjoint ODE. Recently, Marion et al. [2024] employ the adjoint sensitivity method for optimizing diffusion models, where an adjoint SDE is solved to compute the total gradient. Unlike these works, which use the adjoint method for ODE/SDE solutions as functions of time, our work applies an infinite-dimensional version of the adjoint sensitivity method to general learning problems, where solutions are functions of input data. ", "page_idx": 16}, {"type": "text", "text": "Amortization. Recently, several methods have used amortization to approximately solve bilevel problems [MacKay et al., 2019, Bae and Grosse, 2020]. These methods employ a parametric model called a hypernetwork [Ha et al., 2017, Brock et al., 2018, Zhang et al., 2019], optimized to directly predict the inner-level solution given the outer-level parameter $\\omega$ . Amortized methods treat the two levels as independent optimization problems: (1) learning the hypernetwork for a range of $\\omega$ , and (2) performing first-order descent on $\\omega$ using the hypernetwork as a proxy for the inner-level solution. Unlike ITD, AID, or our functional implicit differentiation method, amortized methods do not fully exploit the implicit dependence between the two levels. They are similar to amortized variational inference [Kingma and Welling, 2014, Rezende et al., 2014], where a parametric model produces approximate samples from a posterior distribution. Amortization methods perform well when the inner solution\u2019s dependence on $\\omega$ is simple but may fail otherwise [Amos et al., 2023, pages 71-72]. In contrast, our functional implicit differentiation framework adapts to complex implicit dependencies between the inner solution and $\\omega$ . ", "page_idx": 16}, {"type": "text", "text": "C Theory for Functional Implicit Differentiation ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "C.1 Preliminary results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We recall the definition of Hadamard differentiability and provide in Proposition C.2 a general property for Hadamard differentiable maps that we will exploit to prove Theorem 2.1 in Appendix C.2. ", "page_idx": 16}, {"type": "text", "text": "Definition C.1. Hadamard differentiability. Let $A$ and $B$ be two separable Banach spaces. A function $L:A\\rightarrow B$ is said to be Hadamard differentiable [van der Vaart, 1991, Fang and Santos, 2018] if for any $a\\in A$ , there exist a continuous linear map $d_{a}L(a):A\\to B$ so that for any sequence $(u_{n})_{n\\geq1}$ in $A$ converging to an element $u\\in A$ , and any real valued and non-vanishing sequence $(t_{n})_{n\\geq1}$ converging to $0$ , it holds that: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left\\|{\\frac{1}{t_{n}}}\\left(L(a+t_{n}u_{n})-L(a)\\right)-d_{a}L(a)u\\right\\|{\\frac{}{\\mathbf{\\eta}_{n\\to+\\infty}}}\\to0.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proposition C.2. Let $A$ and $B$ be two separable Banach spaces. Let $L:A\\rightarrow B$ be a Hadamard differentiable map with differential $d_{a}L$ at point $a$ . Consider a bounded linear map defined over a euclidean space $\\mathbb{R}^{n}$ of finite dimension $n$ and taking values in $A$ , i.e $J:\\mathbb{R}^{n}\\,\\rightarrow\\,A$ . Then, the following holds: ", "page_idx": 16}, {"type": "equation", "text": "$$\nL(a+J u)=L(a)+d_{a}L(a)J u+o(\\|u\\|).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. Consider a sequence $(u_{k})_{k\\geq1}$ in $\\mathbb{R}^{n}$ so that $u_{k}$ converges to 0 with $\\left\\Vert u_{k}\\right\\Vert>0$ for all $k\\geq1$ and define the first order error $E_{k}$ as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\nE_{k}=\\frac{1}{\\|u_{k}\\|}\\left\\|L(a+J u_{k})-L(a)-d_{a}L(a)u_{k}\\right\\|.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The goal is to show that $E_{k}$ converges to 0. We can write $u_{k}$ as $u_{k}\\,=\\,t_{k}\\tilde{u}_{k}$ with $t_{k}\\,=\\,\\|u_{k}\\|$ and $\\|\\tilde{u}_{k}\\|=1$ , so that: ", "page_idx": 16}, {"type": "equation", "text": "$$\n{E}_{k}=\\left\\|\\frac{1}{\\|t_{k}\\|}\\left(L(a+t_{k}J\\tilde{u}_{k})-L(a)\\right)-d_{a}L(a)\\tilde{u}_{k}\\right\\|.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "If $E_{k}$ were unbounded, then, by contradiction, there must exist a subsequence $\\big(E_{\\phi(k)}\\big)_{k\\geq1}$ converging to $+\\infty$ , with $\\phi(k)$ increasing and $\\phi(k)\\to+\\infty$ . Moreover, since $\\tilde{u}_{k}$ is bounded, one can further choose the subsequence $E_{\\phi(k)}$ so that $\\tilde{u}_{\\phi(k)}$ converges to some element $\\tilde{u}$ . We can use the following upper-bound: ", "page_idx": 16}, {"type": "equation", "text": "$$\nE_{k}\\leq\\left\\|\\frac{1}{\\|t_{k}\\|}\\left(L(a+t_{k}J\\tilde{u}_{k})-L(a)\\right)-d_{a}L(a)\\tilde{u}\\right\\|+\\left\\|d_{a}L(a)\\right\\|\\,\\|\\tilde{u}_{k}-\\tilde{u}\\|\\,,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where we used that $d_{a}L(a)$ is bounded. Since $L$ is Hadamard differentiable, $\\tilde{E}_{\\phi(k)}$ converges to 0. Moreover, $\\left|\\left|\\tilde{u}_{\\phi(k)}-\\tilde{u}\\right|\\right|$ also converges to 0. Hence, $E_{\\phi(k)}$ converges to 0 which contradicts $E_{\\phi(k)}{\\rightarrow}{+}\\infty$ . Therefore, $\\ddot{E}_{k}$ is bounded. ", "page_idx": 17}, {"type": "text", "text": "Consider now any convergent subsequence of $(E_{k})_{k\\geq1}$ . Then, it can be written as $\\big(E_{\\phi(k)}\\big)_{k\\geq1}$ with $\\phi(k)$ increasing and $\\phi(k)\\to+\\infty$ . We then have $E_{\\phi(k)}\\to e<+\\infty$ by construction. Since $\\tilde{u}_{k}$ is bounded, one can further choose the subsequence $E_{\\phi(k)}$ so that $\\tilde{u}_{\\phi(k)}$ converges to some element $\\tilde{u}$ . Using again Equation (15) and the fact that $L$ is Hadamard differentiable, we deduce that $\\tilde{E}_{\\phi(k)}$ must converge to 0, and by definition of $\\tilde{u}_{\\phi(k)}$ , that $\\left|\\left|\\tilde{u}_{\\phi(k)}-\\tilde{u}\\right|\\right|$ converges to 0. Therefore, it follows that $E_{\\phi(k)}\\to0$ , so that $e=0$ . We then have shown that $(E_{k})_{k\\geq1}$ is a bounded sequence and every subsequence of it converges to 0. Therefore, $E_{k}$ must converge to 0, which concludes the proof. ", "page_idx": 17}, {"type": "text", "text": "C.2 Proof of the Functional implicit differentiation theorem ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof of Theorem 2.1. The proof strategy consists in establishing the existence and uniqueness of the solution map $\\omega\\mapsto h_{\\omega}^{\\star}$ , deriving a candidate Jacobian for it, then proving that $\\omega\\mapsto h_{\\omega}^{\\star}$ is differentiable. ", "page_idx": 17}, {"type": "text", "text": "Existence and uniqueness of a solution map $\\omega\\mapsto h_{\\omega}^{\\star}$ . Let $\\omega$ in $\\Omega$ be fixed. The map $h\\mapsto L_{i n}(\\omega,h)$ is lower semi-continuous since it is Fr\u00e9chet differentiable by assumption. It is also strongly convex. Therefore, it admits a unique minimier $h_{\\omega}^{\\star}$ [Bauschke and Combettes, 2011, Corollary 11.17]. We then conclude that the map $\\omega\\mapsto h_{\\omega}^{\\star}$ is well-defined on $\\Omega$ . ", "page_idx": 17}, {"type": "text", "text": "Strong convexity inequalities. We provide two inequalities that will be used for proving differentiability of the map $\\omega\\mapsto h_{\\omega}^{\\star}$ . The map $h\\;\\mapsto\\;\\bar{L_{i n}}(\\omega,h)$ is Fr\u00e9chet differentiable on $\\mathcal{H}$ and $\\mu_{-}$ -strongly convex (with $\\mu$ positive by assumption). Hence, for all $h_{1},h_{2}$ in $\\mathcal{H}$ the following quadratic lower-bound holds: ", "page_idx": 17}, {"type": "equation", "text": "$$\nL_{i n}(\\omega,h_{2})\\geq L_{i n}(\\omega,h_{1})+\\langle\\partial_{h}L_{i n}(\\omega,h_{1}),(h_{2}-h_{1})\\rangle_{\\mathcal{H}}+\\frac{\\mu}{2}\\left\\|h_{2}-h_{1}\\right\\|_{\\mathcal{H}}^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "From the inequality above, we can also deduce that $h\\mapsto\\partial_{h}L_{i n}(\\omega,h)$ is a $\\mu$ -strongly monotone operator: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\langle\\partial_{h}L_{i n}(\\omega,h_{1})-\\partial_{h}L_{i n}(\\omega,h_{2}),h_{1}-h_{2}\\rangle_{\\mathcal{H}}\\geq\\mu\\left\\|h_{1}-h_{2}\\right\\|_{\\mathcal{H}}^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Finally, note that, since $h\\mapsto L_{i n}(\\omega,h)$ is Fr\u00e9chet differentiable, its gradient must vanish at the optimum $h_{\\omega}^{\\star}$ , i.e : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\partial_{h}L_{i n}(\\omega,h_{\\omega}^{\\star})=0.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Candidate Jacobian for $\\omega\\mapsto h_{\\omega}^{\\star}$ . Let $\\omega$ be in $\\Omega$ . Using Equation (17) with $h_{1}=h+t v$ and $h_{2}=h$ for some $h,v\\in\\mathcal{H}$ , and a non-zeros real number $t$ we get: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{1}{t}\\langle\\partial_{h}L_{i n}(\\omega,h+t v)-\\partial_{h}L_{i n}(\\omega,h),v\\rangle_{\\mathcal{H}}\\geq\\mu\\left\\|v\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By assumption, $h\\mapsto\\partial_{h}L_{i n}(\\omega,h)$ is Hadamard differentiable and, a fortiori, directionally differentiable. Thus, by taking the limit when $t\\rightarrow0$ , it follows that: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\langle\\partial_{h}^{2}L_{i n}(\\omega,h)v,v\\rangle_{\\mathcal{H}}\\geq\\mu\\left\\|v\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Hence, $\\partial_{h}^{2}L_{i n}(\\omega,h)\\,:\\,\\mathcal{H}\\,\\rightarrow\\,\\mathcal{H}$ defines a coercive quadratic form. By definition of Hadamard differentiability, it is also bounded. Therefore, it follows from Lax-Milgram\u2019s theorem [Debnath and Mikusinski, 2005, Theorem 4.3.16], that $\\partial_{h}^{2}L_{i n}(\\omega,h)$ is invertible with a bounded inverse. Moreover, recalling that $B_{\\omega}\\,=\\,\\partial_{\\omega,h}L_{i n}(\\omega,h_{\\omega}^{\\star})$ is a bounded operator, its adjoint $(B_{\\omega})^{\\star}$ is also a bounded operator from $\\Omega$ to $\\mathcal{H}$ . Therefore, we can define $J=-C_{\\omega}^{-1}(B_{\\omega})^{\\star}$ which is a bounded linear map from $\\Omega$ to $\\mathcal{H}$ and will be our candidate Jacobian. ", "page_idx": 17}, {"type": "text", "text": "Differentiability of $\\omega\\mapsto h_{\\omega}^{\\star}$ . By the strong convexity assumption (locally in $\\omega$ ), there exists an open ball $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ centered at the origin 0 that is small enough so that we can ensure the existence of $\\mu>0$ for which $h\\mapsto L_{i n}(\\omega+\\epsilon,h)$ is $\\mu$ -strongly convex for all $\\epsilon\\in\\mathcal{B}$ . For a given $\\epsilon\\in\\mathcal{B}$ , we use the $\\mu$ -strong monotonicity of $h\\mapsto\\partial_{h}L_{i n}(\\omega+\\epsilon,h)$ (17) at points $h_{\\omega}^{\\star}+J\\epsilon$ and $h_{\\omega+\\epsilon}^{\\star}$ to get: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\left|h_{\\omega+\\epsilon}^{\\star}-h_{\\omega}^{\\star}-J\\epsilon\\right|\\right|^{2}\\leq\\langle\\left(\\partial_{h}L_{i n}\\left(\\omega+\\epsilon,h_{\\omega+\\epsilon}^{\\star}\\right)-\\partial_{h}L_{i n}\\left(\\omega+\\epsilon,h_{\\omega}^{\\star}+J\\epsilon\\right)\\right),\\left(h_{\\omega+\\epsilon}^{\\star}-h_{\\omega}^{\\star}-J\\epsilon\\right)\\rangle_{\\mathcal{H}}}\\\\ &{\\quad=\\langle-\\partial_{h}L_{i n}\\left(\\omega+\\epsilon,h_{\\omega}^{\\star}+J\\epsilon\\right),\\left(h_{\\omega+\\epsilon}^{\\star}-h_{\\omega}^{\\star}-J\\epsilon\\right)\\rangle_{\\mathcal{H}}}\\\\ &{\\quad\\leq\\left\\|\\partial_{h}L_{i n}\\left(\\omega+\\epsilon,h_{\\omega}^{\\star}+J\\epsilon\\right)\\right\\|_{\\mathcal{H}}\\left\\|h_{\\omega+\\epsilon}^{\\star}-h_{\\omega}^{\\star}-J\\epsilon\\right\\|_{\\mathcal{H}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the second line follows from optimality of $h_{\\omega+\\epsilon}^{\\star}$ (Equation (18)), and the last line uses Cauchy-Schwarz\u2019s inequality. The above inequality allows us to deduce that: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\|h_{\\omega+\\epsilon}^{\\star}-h_{\\omega}^{\\star}-J\\epsilon\\right\\|\\leq\\frac{1}{\\mu}\\left\\|\\partial_{h}L_{i n}\\left(\\omega+\\epsilon,h_{\\omega}^{\\star}+J\\epsilon\\right)\\right\\|_{\\mathcal{H}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Moreover, since $\\partial_{h}L_{i n}$ is Hadamard differentiable, by Proposition C.2 it follows that: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\partial_{h}{\\cal L}_{i n}\\left(\\omega+\\epsilon,h_{\\omega}^{\\star}+J\\epsilon\\right)=\\underbrace{\\partial_{h}{\\cal L}_{i n}\\left(\\omega,h_{\\omega}^{\\star}\\right)}_{=0}+d_{\\left(\\omega,h\\right)}\\partial_{h}{\\cal L}_{i n}(\\omega,h_{\\omega}^{\\star})(\\epsilon,J\\epsilon)+o(\\|\\epsilon\\|),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the first term vanishes as a consequence of Equation (18), since $h_{\\omega}^{\\star}$ is a minimizer of $h\\mapsto$ $L_{i n}(\\omega,h)$ . Additionally, note that the differential $d_{(\\omega,h)}\\partial_{h}L_{i n}(\\omega,h):\\Omega\\times\\ddot{\\mathcal{H}}\\rightarrow\\mathcal{H}$ acts on elements $(\\epsilon,g)\\in\\Omega\\times\\mathcal{H}$ as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\nd_{(\\omega,h)}\\partial_{h}L_{i n}(\\omega,h)(\\epsilon,g)=\\partial_{h}^{2}L_{i n}(\\omega,h)g+(\\partial_{\\omega,h}L_{i n}(\\omega,h))^{\\star}\\,\\epsilon,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\partial_{h}^{2}L_{i n}(\\omega,h)~:~\\mathcal{H}~\\to~\\mathcal{H}$ and $\\partial_{\\omega,h}L_{i n}(\\omega,h)~:~\\mathcal{H}~\\to~\\Omega$ are bounded operators and $(\\partial_{\\omega,h}L_{i n}(\\omega,h))^{\\star}$ denotes the adjoint of $\\partial_{\\omega,h}L_{i n}(\\omega,h)$ . By definition of $J$ , and using Equation (23), it follows that: ", "page_idx": 18}, {"type": "equation", "text": "$$\nd_{(\\omega,h)}\\partial_{h}{\\cal L}_{i n}(\\omega,h)(\\epsilon,J\\epsilon)=C_{\\omega}J\\epsilon+B_{\\omega}\\epsilon=0.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, combining Equation (22) with the above equality yields: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\partial_{h}{\\cal L}_{i n}\\left(\\omega+\\epsilon,h_{\\omega}^{\\star}+J\\epsilon\\right)=o(\\|\\epsilon\\|).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Finally, combining Equation (21) with the above equality directly shows that $\\left\\|h_{\\omega+\\epsilon}^{\\star}-h_{\\omega}^{\\star}-J\\epsilon\\right\\|\\leq$ ${\\frac{1}{\\mu}}o\\bigl(\\|\\epsilon\\|\\bigr)$ . We have shown that $\\omega\\mapsto h_{\\omega}^{\\star}$ is differentiable with a Jacobian map $\\partial_{\\omega}h_{\\omega}^{\\star}$ given by $J^{\\star}=-B_{\\omega}C_{\\omega}^{-1}$ . \u53e3 ", "page_idx": 18}, {"type": "text", "text": "C.3 Proof of the functional adjoint sensitivity in Proposition 2.2 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof of Proposition 2.2. We use the assumptions and definitions from Proposition 2.2 and express the gradient $\\nabla{\\mathcal{F}}(\\omega)$ using the chain rule: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\nabla\\mathcal{F}(\\omega)=\\partial_{\\omega}L_{o u t}(\\omega,h_{\\omega}^{\\star})+\\left[\\partial_{\\omega}h_{\\omega}^{\\star}\\right]\\partial_{h}L_{o u t}(\\omega,h_{\\omega}^{\\star}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The Jacobian $\\partial_{\\omega}h_{\\omega}^{\\star}$ is the solution of a linear system obtained by applying Theorem 2.1 : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\partial_{\\omega}h_{\\omega}^{\\star}=-B_{\\omega}C_{\\omega}^{-1}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We note $g_{\\omega}=\\partial_{\\omega}L_{o u t}(\\omega,h_{\\omega}^{\\star})$ and $d_{\\omega}=\\partial_{h}L_{o u t}(\\omega,h_{\\omega}^{\\star})$ . It follows that the gradient $\\nabla{\\mathcal{F}}(\\omega)$ can be expressed as: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla{\\mathcal F}(\\omega)=g_{\\omega}+[\\partial_{\\omega}h_{\\omega}^{\\star}]\\,d_{\\omega}=g_{\\omega}+B_{\\omega}a_{\\omega}^{\\star}}\\\\ &{a_{\\omega}^{\\star}:=-C_{\\omega}^{-1}d_{\\omega}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In other words, the implicit gradient $\\nabla{\\mathcal{F}}(\\omega)$ can be expressed using the adjoint function $a_{\\omega}^{\\star}$ , which is an element of $\\mathcal{H}$ and can be defined as the solution of the following functional regression problem: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{a_{\\omega}^{\\star}=\\arg\\underset{a\\in\\mathcal{H}}{\\operatorname*{min}}\\,L_{a d j}(\\omega,a):=\\frac{1}{2}\\,\\,\\langle a,C_{\\omega}a\\rangle_{\\mathcal{H}}+\\langle a,d_{\\omega}\\rangle_{\\mathcal{H}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "D Functional Adjoint Sensitivity Results in $L_{2}$ Spaces ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section we provides full proofs of Proposition 2.3. We start by stating the assumptions needed on the data distributions and the point-wise losses in Appendix D.1, then provide some differentiation results in Appendix D.2 and conclude with the main proofs in Appendix D.3. ", "page_idx": 18}, {"type": "text", "text": "D.1 Assumptions ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Assumption on $\\mathbb{P}$ and $\\mathbb{Q}$ . ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "(A) $\\mathbb{P}$ and $\\mathbb{Q}$ admit finite second moments. ", "page_idx": 19}, {"type": "text", "text": "$({\\bf{B}})$ The marginal of $X$ w.r.t. $\\mathbb{Q}$ admits a Radon-Nikodym derivative $r(x)$ w.r.t. the marginal of $X$ w.r.t. $\\mathbb{P}$ , i.e. $\\mathrm{d}\\mathbb{Q}(x,\\mathcal{Y})=r(x)\\,\\mathrm{d}\\mathbb{P}(x,\\mathcal{Y})$ . Additionally, $r(x)$ is upper-bounded by a positive constant $M$ . ", "page_idx": 19}, {"type": "text", "text": "Assumptions on $\\ell_{i n}$ . ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "(C) For any $\\omega\\in\\Omega$ , there exists a positive constant $\\mu$ and a neighborhood $B$ of $\\omega$ for which $\\ell_{i n}$ is $\\mu$ -strongly convex in its second argument for all $(\\omega^{\\prime},x,\\bar{y})\\in B\\times\\mathcal{X}\\times\\mathcal{Y}$ .   \n$({\\bf D})$ For any $\\omega\\in\\Omega,\\mathbb{E}_{\\mathbb{P}}\\left[|\\ell_{i n}(\\omega,0,x,y)|+||\\partial_{v}\\ell_{i n}\\left(\\omega,0,x,y\\right)||^{2}\\right]<+\\infty.$   \n$\\mathbf{\\tau}(\\mathbf{E})$ $v\\mapsto\\ell_{i n}(\\omega,v,x,y)$ is continuously differentiable for all $(\\omega,x,y)\\in\\Omega\\times\\mathcal{X}\\times\\mathcal{Y}$ .   \n$(\\mathbf{F})$ For any fixed $\\omega\\,\\in\\,\\Omega$ , there exists a constant $L$ and a neighborhood $B$ of $\\omega$ s.t. $v\\mapsto$ $\\ell_{i n}(\\omega^{\\prime},v,x,y)$ is $L$ -smooth for all $\\omega^{\\prime},x,y\\in B\\times\\mathcal{X}\\times\\mathcal{Y}$ .   \n(G) $(\\omega,v)\\mapsto\\partial_{v}\\ell_{i n}(\\omega,v,x,y)$ is continuously differentiable on $\\Omega\\times\\mathcal{V}$ for all $x,y\\in\\mathcal{X}\\times\\mathcal{Y}$ ,   \n$(\\mathbf{H})$ For any $\\omega\\in\\Omega$ , there exists a positive constant $C$ and a neighborhood $B$ of $\\omega$ s.t. for all $(\\omega^{\\prime},x,y)\\in B\\times\\mathcal{X}\\times\\mathcal{Y}$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Vert\\partial_{\\omega,v}\\ell_{i n}\\left(\\omega^{\\prime},0,x,y\\right)\\Vert\\leq C\\left(1+\\Vert x\\Vert+\\Vert y\\Vert\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "(I) For any $\\omega\\in\\Omega$ , there exists a positive constant $C$ and a neighborhood $B$ of $\\omega$ s.t. for all $(\\omega^{\\prime},v_{1},v_{2},x,y)\\in B\\times\\mathcal{V}\\times\\mathcal{V}\\times\\mathcal{X}\\times\\mathcal{Y}$ we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\partial_{\\omega,v}\\ell_{i n}\\left(\\omega^{\\prime},v_{1},x,y\\right)-\\partial_{\\omega,v}\\ell_{i n}\\left(\\omega^{\\prime},v_{2},x,y\\right)\\right\\|\\leq C\\left\\|v_{1}-v_{2}\\right\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Assumptions on $\\ell_{o u t}$ . ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "$(\\mathbf{J})$ For any $\\mathbf{\\eta}\\in\\Omega,\\mathbb{E}_{\\mathbb{Q}}\\left[|\\ell_{o u t}(\\omega,0,x,y)|\\right]<+\\infty.$   \n$(\\mathbf{K})$ $(\\omega,v)\\mapsto\\ell_{o u t}(\\omega,v,x,y)$ is jointly continuously differentiable on $\\Omega\\times\\nu$ for all $(x,y)\\in$ $\\mathcal X\\times\\mathcal X$ .   \n$\\mathbf{\\rho}(\\mathbf{L})$ For any $\\omega\\in\\Omega$ , there exits a neighborhood $B$ of $\\omega$ and a positive constant $C$ s.t. for all $(\\omega^{\\prime},v,v^{\\prime},x,y)\\in B\\times\\mathcal{V}\\times\\mathcal{V}\\times\\mathcal{X}\\times\\mathcal{Y}$ we have: $\\begin{array}{r l}&{\\left\\|\\partial_{\\omega}\\ell_{o u t}\\left(\\omega^{\\prime},v,x,y\\right)-\\partial_{\\omega}\\ell_{o u t}\\left(\\omega^{\\prime},v^{\\prime},x,y\\right)\\right\\|\\leq C\\left(1+\\left\\|v\\right\\|+\\left\\|v^{\\prime}\\right\\|+\\left\\|x\\right\\|+\\left\\|y\\right\\|\\right)\\left\\|v-v^{\\prime}\\right\\|,}\\\\ &{\\left\\|\\partial_{v}\\ell_{o u t}\\left(\\omega^{\\prime},v,x,y\\right)-\\partial_{v}\\ell_{o u t}\\left(\\omega^{\\prime},v^{\\prime},x,y\\right)\\right\\|\\leq C\\left\\|v-v^{\\prime}\\right\\|,}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\|\\partial_{v}\\ell_{o u t}\\left(\\omega^{\\prime},v,x,y\\right)\\right\\|\\leq C\\left(1+\\left\\|v\\right\\|+\\left\\|x\\right\\|+\\left\\|y\\right\\|\\right),}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\|\\partial_{\\omega}\\ell_{o u t}\\left(\\omega^{\\prime},v,x,y\\right)\\right\\|\\leq C\\left(1+\\left\\|v\\right\\|^{2}+\\left\\|x\\right\\|^{2}+\\left\\|y\\right\\|^{2}\\right).}\\end{array}$ ", "page_idx": 19}, {"type": "text", "text": "Example. Here we consider the squared error between two vectors $v,z$ in $\\mathcal{V}$ . Given a map $(\\omega,x,y)\\mapsto$ $f_{\\omega}(x,\\bar{y})$ defined over $\\Omega\\times\\mathcal X\\times\\mathcal Y$ and taking values in $\\nu$ , we define the following point-wise objective: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\ell(\\omega,v,x,y):=\\frac{1}{2}\\left\\|v-z\\right\\|^{2},\\quad z=f_{\\omega}(x,y).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We assume that for any $\\omega\\in{\\Omega}$ , there exists a constant $C>0$ such that for all $\\omega^{\\prime}$ in a neighborhood of $\\omega$ and all $x,y\\in\\mathcal{X}\\times\\mathcal{Y}$ , the following growth assumption holds: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|f_{\\omega^{\\prime}}(x,y)\\|+\\|\\partial_{\\omega}f_{\\omega^{\\prime}}(x,y)\\|\\le C(1+\\|x\\|+\\|y\\|).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This growth assumption is weak in the context of neural networks with smooth activations as discussed by Bin\u00b4kowski et al. [2018, Appendix C.4]. ", "page_idx": 19}, {"type": "text", "text": "Proposition D.1. Assume that the map $\\omega\\mapsto f_{\\omega}(x,y)$ is continuously differentiable for any $x,y\\in$ $\\mathcal X\\times\\mathcal Y$ , and that Equation (28) holds. Additionally, assume that $\\mathbb{P}$ and $\\mathbb{Q}$ admit finite second order moments. Then the point-wise objective $\\ell$ in Equation (27) satisfies Assumptions $(C)$ to $(L)$ . ", "page_idx": 19}, {"type": "text", "text": "\u2022 Assumption (C): the squared error is 1-strongly convex in $v$ , since $\\partial_{v}^{2}\\ell\\succeq I$ . Hence, the strong convexity assumption holds with $\\mu=1$ . ", "page_idx": 20}, {"type": "text", "text": "\u2022 Assumption (D): For any $\\omega\\in{\\Omega}$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\mathbb{P}}\\left[|\\ell(\\omega,0,x,y)|+\\|\\partial_{v}\\ell\\left(\\omega,0,x,y\\right)\\|^{2}\\right]=\\mathbb{E}_{\\mathbb{P}}\\left[\\frac{1}{2}\\left\\|f_{\\omega}(x,y)\\right\\|^{2}+\\|f_{\\omega}(x,y)\\|^{2}\\right]<+\\infty,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which holds by the growth assumption on $f_{\\omega}(x,y)$ , and $\\mathbb{P}$ having finite second moments. ", "page_idx": 20}, {"type": "text", "text": "\u2022 Assumption (E): With a perturbation $u\\in\\mathcal{V}$ we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\ell(\\omega,v+u,x,y)=\\frac{1}{2}\\|v-z\\|^{2}+\\langle v-z,u\\rangle+o(\\|u\\|^{2}),\\quad z=f_{\\omega}(x,y),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "with $\\begin{array}{r}{o(\\|u\\|^{2})=\\frac{1}{2}\\|u\\|^{2}}\\end{array}$ . The mapping $v\\mapsto v-z$ is continuous, thus the assumption holds. ", "page_idx": 20}, {"type": "text", "text": "\u2022 Assumption (F): For any two points $v_{1},v_{2}\\in\\mathcal{V}$ using the expression of $\\partial_{v}\\ell(\\omega,v,x,y)=$ $v-z$ with $z=f_{\\omega}(x,y)$ we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|\\partial_{v}\\ell(\\omega,v_{1},x,y)\\:-\\:\\partial_{v}\\ell(\\omega,v_{2},x,y)\\|\\;=\\;\\|(v_{1}\\,-\\,z)\\,-\\,(v_{2}\\,-\\,z)\\|\\;=\\;\\|v_{1}\\,-\\,v_{2}\\|,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We see that $\\ell$ is $L$ -smooth with $L=1$ and the assumption holds. ", "page_idx": 20}, {"type": "text", "text": "\u2022 Assumption $(\\mathbf{K})$ : By the differentiation assumption on $f_{\\omega}(x,y)$ , with a perturbation $\\epsilon\\in\\Omega$ we can write: ", "page_idx": 20}, {"type": "equation", "text": "$$\nf_{\\omega+\\epsilon}(x,y)=f_{\\omega}(x,y)+\\partial_{\\omega}f_{\\omega}(x,y)\\epsilon+o(\\epsilon).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "With a perturbation $\\epsilon\\times u\\in\\Omega\\times\\mathcal{V}$ and substituting $f_{\\omega+\\epsilon}(x,y)$ with the expression above we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\ell(\\omega+\\epsilon,v+u,x,y)=\\displaystyle\\frac{1}{2}\\left\\|(v+u)-(f_{\\omega}(x,y)+\\partial_{\\omega}f_{\\omega}(x,y)\\epsilon+o(\\epsilon))\\right\\|^{2}}}\\\\ {{\\displaystyle=\\frac{1}{2}\\left\\|v-f_{\\omega}(x,y)\\right\\|^{2}+\\left\\langle\\epsilon,\\partial_{\\omega}f_{\\omega}(x,y)^{\\top}\\left(f_{\\omega}(x,y)-v\\right)\\right\\rangle}}\\\\ {{\\displaystyle\\quad\\,\\,+\\left\\langle u,v-f_{\\omega}(x,y)\\right\\rangle+o(\\left\\|\\epsilon\\right\\|+\\left\\|u\\right\\|),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which allows us to conclude that $(\\omega,v)\\mapsto\\ell(\\omega,v,x,y)$ is continuously differentiable on $\\Omega\\times\\mathcal{V}$ for all $x,y\\in\\mathcal{X}\\times\\mathcal{Y}$ and the assumption holds. ", "page_idx": 20}, {"type": "text", "text": "\u2022 Assumption (G): With a perturbation $\\epsilon\\times u\\in\\Omega\\times\\nu$ using the expression of $\\partial_{v}\\ell(\\omega,v,x,y)$ we can write: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\partial_{v}\\ell(\\omega+\\epsilon,v+u,x,y)=(v+u)-f_{\\omega+\\epsilon}(x,y)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=(v+u)-\\big(f_{\\omega}(x,y)+\\partial_{\\omega}f_{\\omega}(x,y)\\epsilon+o(\\epsilon)\\big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=(v-f_{\\omega}(x,y))+u-\\partial_{\\omega}f_{\\omega}(x,y)\\epsilon+o(\\epsilon),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "by continuously differentiable $f_{\\omega}(x,y)$ , we have that the assumption holds. ", "page_idx": 20}, {"type": "text", "text": "\u2022 Assumptions $(\\mathbf{H})$ and (I): From the expression of $\\partial_{v}\\ell(\\omega,v,x,y)$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\partial_{\\omega,v}\\ell\\left(\\omega,v,x,y\\right)=\\partial_{\\omega}\\left(v-f_{\\omega}(x,y)\\right)=\\partial_{\\omega}f_{\\omega}(x,y),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "then using the expression above and the growth assumption on $f_{\\omega^{\\prime}}(x,y)$ we have that the two assumptions hold. ", "page_idx": 20}, {"type": "text", "text": "\u2022 Assumption (J): For any $\\omega\\in\\Omega$ we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbb{Q}}\\left[\\left|\\ell(\\omega,0,x,y)\\right|\\right]=\\mathbb{E}_{\\mathbb{Q}}\\left[\\frac{1}{2}\\left\\|f_{\\omega}(x,y)\\right\\|^{2}\\right]<+\\infty,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "by the growth assumption on $f_{\\omega}(x,y)$ , and $\\mathbb{P}$ having finite second moments, thus the assumption is verified. ", "page_idx": 20}, {"type": "text", "text": "\u2022 Assumption $\\mathbf{(L)}$ : Using the growth assumption on $f_{\\omega^{\\prime}}(x,y)$ , we have the following inequalities: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Vert\\partial_{\\alpha^{\\ell}}(w^{\\prime},v,x,y)-\\partial_{\\alpha^{\\ell}}(w^{\\prime},v^{\\prime},x,y)\\Vert=\\Vert f_{\\alpha^{\\ell}}(x,y)^{\\top}(v^{\\prime}-v)\\Vert}&{}\\\\ {\\qquad\\qquad\\qquad\\leq\\Vert f_{\\alpha^{\\ell}}(x,y)^{\\top}\\Vert+\\Vert v^{\\prime}-v\\Vert}&{}\\\\ {\\qquad\\qquad\\qquad\\leq C(1+\\Vert v\\Vert+\\Vert v^{\\prime}\\Vert+\\Vert x\\Vert+\\Vert y\\Vert)\\Vert v-v^{\\prime}\\Vert\\,,}\\\\ {\\Vert\\partial_{v^{\\ell}}(w^{\\prime},v,x,y)\\Vert=\\Vert v-f_{\\alpha^{\\ell}}(x,y)\\Vert}&{}\\\\ {\\qquad\\qquad\\qquad\\leq\\Vert v\\Vert+\\Vert f_{\\alpha^{\\ell}}(x,y)\\Vert}&{}\\\\ {\\qquad\\qquad\\leq\\Vert v\\Vert+C(1+\\Vert x\\Vert+\\Vert y\\Vert)}&{}\\\\ {\\qquad\\qquad\\qquad\\leq C(1+\\Vert v\\Vert+\\Vert y\\Vert)}&{}\\\\ {\\Vert\\partial_{\\alpha^{\\ell}}(w^{\\prime},v,x,y)\\Vert=\\Vert\\partial_{\\alpha^{\\ell}}(w,y)^{\\top}(f_{\\alpha^{\\ell}}(x,y)-v)\\Vert}&{}\\\\ {\\qquad\\qquad\\qquad\\leq\\Vert\\partial_{\\alpha^{\\ell}}(x,y)\\Vert\\Vert(f_{\\alpha^{\\ell}}(x,y)-v)\\Vert}&{}\\\\ {\\qquad\\qquad\\leq\\Vert\\partial_{\\alpha^{\\ell}}(x,v)\\Vert\\Vert(\\Vert f_{\\alpha^{\\ell}}(x,y)\\Vert+\\Vert v\\Vert)}&{}\\\\ {\\qquad\\qquad\\leq C\\left(1+\\Vert v\\Vert^{2}+\\Vert x\\Vert^{2}+\\Vert y\\Vert^{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "combining the above with $L$ -smoothness of $\\ell$ we can conclude that the assumption holds. ", "page_idx": 21}, {"type": "text", "text": "D.2 Differentiability results ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The next lemmas show differentiability of $L_{o u t}$ , $L_{i n}$ and $\\partial_{h}L_{i n}$ and will be used to prove Proposition 2.3. ", "page_idx": 21}, {"type": "text", "text": "Lemma D.2 (Differentiability of $L_{i n}$ in its second argument). Under Assumptions $({\\cal D})$ to $(F,$ , the function $h\\mapsto L_{i n}(\\omega,h)$ is differentiable in $\\mathcal{H}$ with partial derivative vector $\\partial_{h}L_{i n}(\\omega,h)\\in\\mathcal{H}$ given by: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\partial_{h}L_{i n}(\\omega,h):\\mathcal{X}\\to\\mathcal{V}\\qquad\\qquad}\\\\ {x\\mapsto\\mathbb{E}_{\\mathbb{P}}\\left[\\partial_{v}\\ell_{i n}\\left(\\omega,h(x),x,y\\right)|x\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. We decompose the proof into three parts: verifying that $L_{i n}$ is well-defined, identifying a bounded map as candidate for the differential and showing that it is the Fr\u00e9chet differential of $L_{i n}$ . ", "page_idx": 21}, {"type": "text", "text": "Well-defined objective. Consider $(\\omega,h)$ in $\\Omega\\times\\mathcal{H}$ . To show that $L_{i n}(\\omega,h)$ is well-defined, we need to prove that $\\ell_{i n}\\!\\left(\\omega,h(x),x,y\\right)$ is integrable under $\\mathbb{P}$ . We use the following inequalities to control $\\ell_{i n}(\\omega,h(x),x,y)$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\vert\\ell_{i n}(\\omega,h(x),x,y)\\vert\\le\\vert\\ell_{i n}(\\omega,h(x),x,y)-\\ell_{i n}(\\omega,0,x,y)\\vert+\\vert\\ell_{i n}(\\omega,0,x,y)\\vert}&{}\\\\ {=\\left\\vert\\int_{0}^{1}\\mathrm{d}t\\left(h(x)^{\\top}\\partial_{v}\\ell_{i n}(\\omega,t h(x),x,y)\\right)\\right\\vert+\\vert\\ell_{i n}(\\omega,0,x,y)\\vert}&{}\\\\ {\\ \\le\\vert h(x)\\vert\\left\\vert\\int_{0}^{1}\\mathrm{d}t\\left\\Vert\\partial_{v}\\ell_{i n}(\\omega,t h(x),x,y)-\\partial_{v}\\ell_{i n}(\\omega,0,x,y)\\right\\vert\\right.}&{}\\\\ {\\ \\ \\ +\\left\\Vert h(x)\\right\\Vert\\left\\Vert\\partial_{v}\\ell_{i n}(\\omega,0,x,y)\\right\\Vert+\\vert\\ell_{i n}(\\omega,0,x,y)\\vert}&{}\\\\ {\\ \\ \\le\\frac{L}{2}\\left\\Vert h(x)\\right\\Vert^{2}+\\frac{1}{2}\\left(\\left\\Vert h(x)\\right\\Vert^{2}+\\left\\Vert\\partial_{v}\\ell_{i n}(\\omega,0,x,y)\\right\\Vert^{2}\\right)+\\vert\\ell_{i n}(\\omega,0,x,y)\\vert\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the first line follows by triangular inequality, the second follows by application of the fundamental theorem of calculus since $\\ell_{i n}$ is differentiable by Assumption $\\mathbf{\\tau}(\\mathbf{E})$ . The third uses Cauchy-Schwarz inequality along with a triangular inequality. Finally, the last line follows using that $\\ell_{i n}$ is $L$ -smooth in its second argument, locally in $\\omega$ and uniformly in $x$ and $y$ by Assumption (F). Taking the expectation under $\\mathbb{P}$ yields: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|L_{i n}(\\omega,h)\\right|\\leq\\!{\\mathbb E}_{\\mathbb{P}}\\left[\\left|\\ell_{i n}\\left(\\omega,h(x),x,y\\right)\\right|\\right]}\\\\ &{\\qquad\\qquad\\leq\\!\\frac{L+1}{2}\\left\\|h\\right\\|_{\\mathcal{H}}^{2}+{\\mathbb E}_{\\mathbb{P}}\\left[\\left\\|\\partial_{v}\\ell_{i n}(\\omega,0,x,y)\\right\\|^{2}+\\left|\\ell_{i n}(\\omega,0,x,y)\\right|\\right]<+\\infty,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\left\\|h\\right\\|_{\\mathcal{H}}$ is finite since $\\begin{array}{r l r}{h}&{{}\\in}&{\\mathcal{H}}\\end{array}$ and expectations under $\\mathbb{P}$ of $\\left\\|\\partial_{v}\\ell_{i n}(\\omega,0,x,y)\\right\\|^{2}$ and $|\\ell_{i n}(\\omega,0,x,\\overset{..}{y})|$ are finite by Assumption (D). This shows that $L_{i n}(\\omega,h)$ is well defined on $\\Omega\\times\\mathcal{H}$ . ", "page_idx": 22}, {"type": "text", "text": "Candidate differential. Fix $(\\omega,h)$ in $\\Omega\\times\\mathcal{H}$ and consider the following linear form $d_{i n}$ in $\\mathcal{H}$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\nd_{i n}g:=\\mathbb{E}_{\\mathbb{P}}\\left[g(x)^{\\top}\\partial_{v}\\ell_{i n}(\\omega,h(x),x,y)\\right],\\qquad\\forall g\\in\\mathcal{H}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We need to show that it is a bounded form. To this end, we will show that $d_{i n}$ is a scalar product with some vector $D_{i n}$ in $\\mathcal{H}$ . The following equalities hold: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d_{i n}g=\\mathbb{E}_{\\mathbb{P}}\\left[g(x)^{\\top}\\partial_{v}\\ell_{i n}(\\omega,h(x),x,y)\\right]}\\\\ &{\\qquad=\\mathbb{E}_{\\mathbb{P}}\\left[g(x)^{\\top}{\\mathbb{E}}_{\\mathbb{P}}\\left[\\partial_{v}\\ell_{i n}(\\omega,h(x),x,y)|x\\right]\\right]}\\\\ &{\\qquad=\\mathbb{E}_{\\mathbb{P}}\\left[g(x)^{\\top}D_{i n}(x)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the second line follows by the \u201ctower\u201d property for conditional expectations and where we define $D_{i n}(x):=\\mathbb{E}_{\\mathbb{P}}\\left[\\partial_{v}\\ell_{i n}(\\omega,\\dot{h}(x),x,y)|x\\right]$ in the last line. $D_{i n}$ is a the candidate representation of $d_{i n}$ in $\\mathcal{H}$ . We simply need to check that $D_{i n}$ is an element of $\\mathcal{H}$ . To see this, we use the following upper-bounds: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{\\xi}_{\\mathbb{P}}\\left[\\left\\|D_{i n}(x)\\right\\|^{2}\\right]\\leq\\mathbb{E}_{\\mathbb{P}}\\left[\\mathbb{E}_{\\mathbb{P}}\\left[\\left\\|\\partial_{v}\\ell_{i n}(\\omega,h(x),x,y)\\right\\|^{2}\\middle|x\\right]\\right]}\\\\ &{\\qquad\\qquad\\qquad=\\mathbb{E}_{\\mathbb{P}}\\left[\\|\\partial_{v}\\ell_{i n}(\\omega,h(x),x,y)\\|^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leq2\\mathbb{E}_{\\mathbb{P}}\\left[\\|\\partial_{v}\\ell_{i n}(\\omega,h(x),x,y)-\\partial_{v}\\ell_{i n}(\\omega,0,x,y)\\|^{2}\\right]+2\\mathbb{E}_{\\mathbb{P}}\\left[\\|\\partial_{v}\\ell_{i n}(\\omega,0,x,y)\\|^{2}\\right]}\\\\ &{\\qquad\\qquad\\leq2L^{2}\\mathbb{E}_{\\mathbb{P}}\\left[\\|h(x)\\|^{2}\\right]+2\\mathbb{E}_{\\mathbb{P}}\\left[\\|\\partial_{v}\\ell_{i n}(\\omega,0,x,y)\\|^{2}\\right]<+\\infty.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The first inequality is an application of Jensen\u2019s inequality by convexity of the squared norm. The second line follows by the \u201ctower\u201d property for conditional probability distributions while the third follows by triangular inequality and Jensen\u2019s inequality applied to the square function. The last line uses that $\\ell_{i n}$ is $L$ -smooth in its second argument, locally in $\\omega$ and uniformly in $x,y$ by Assumption (F). Since $h$ is square integrable under $\\mathbb{P}$ by construction and $\\|\\partial_{v}\\ell_{i n}(\\omega,0,x,\\dot{y})\\|$ is also square integrable by Assumption $({\\bf D})$ , we deduce from the above upper-bounds that $D_{i n}(x)$ must also be square integrable and thus an element of $\\mathcal{H}$ . Therefore, we have shown that $d_{i n}$ is a continuous linear form admitting the following representation: ", "page_idx": 22}, {"type": "equation", "text": "$$\nd_{i n}g=\\langle D_{i n},g\\rangle_{\\mathcal{H}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Differentiability of $h\\mapsto L_{i n}(\\omega,h)$ . To prove differentiability, we simply control the first order error $E(g)$ defined as: ", "page_idx": 22}, {"type": "equation", "text": "$$\n{\\cal E}(g):=|L_{i n}(\\omega,h+g)-L_{i n}(\\omega,h)-d_{i n}g|\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For a given $g\\in\\mathcal H$ , the following inequalities hold: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E(g)=\\left|\\mathbb{E}_{\\mathbb{P}}\\left[\\int_{0}^{1}\\mathrm{d}t\\left(g(x)^{\\top}\\left(\\partial_{v}\\ell_{i n}(\\omega,h(x)+t g(x),x,y)-\\partial_{v}\\ell_{i n}(\\omega,h(x),x,y)\\right)\\right)\\right]\\right|}\\\\ &{\\qquad\\le\\mathbb{E}_{\\mathbb{P}}\\left[\\int_{0}^{1}\\vert g(x)^{\\top}\\left(\\partial_{v}\\ell_{i n}(\\omega,h(x)+t g(x),x,y)-\\partial_{v}\\ell_{i n}(\\omega,h(x),x,y)\\right)\\vert\\,\\mathrm{d}t\\right]}\\\\ &{\\qquad\\le\\frac{L}{2}\\mathbb{E}_{\\mathbb{P}}\\left[\\|g(x)\\|^{2}\\right]=\\frac{L}{2}\\left\\|g\\right\\|_{\\mathcal{H}}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the first inequality follows by application of the fundamental theorem of calculus since $\\ell_{i n}$ is differentiable in its second argument by Assumption $\\mathbf{\\tau}(\\mathbf{E})$ . The second line follows by Jensen\u2019s inequality while the last line uses that $v\\mapsto\\^{\\cdot}\\partial\\ell_{i n}(\\omega,\\bar{v},x,y)$ is $L$ -Lipschitz locally in $\\omega$ and uniformly in $x$ and $y$ by Assumption (F). Therefore, we have shown that $\\bar{E}(g)\\,=\\,o(\\|g\\|_{\\mathcal{H}})$ which precisely means that $h\\mapsto L_{i n}(\\omega,h)$ is differentiable with differential $d_{i n}$ . Moreover, $D_{i n}$ is the partial gradient of $L_{i n}(\\omega,h)$ in the second variable: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\partial_{h}L_{i n}(\\omega,h)=D_{i n}=x\\mapsto\\mathbb{E}_{\\mathbb{P}}\\left[\\partial_{v}\\ell_{i n}(\\omega,h(x),x,y)|x\\right].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Lemma D.3 (Differentiability of $L_{o u t}$ ). Under Assumptions (A), $(B)$ and $(J)$ to $(L)$ , $L_{o u t}$ is jointly differentiable in $\\omega$ and $h$ . Moreover, its partial derivatives $\\partial_{\\omega}L_{o u t}(\\omega,h)$ and $\\partial_{h}L_{o u t}(\\omega,h)$ are elements in $\\Omega$ and $\\mathcal{H}$ given by: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\partial_{\\omega}L_{o u t}(\\omega,h)=\\mathbb{E}_{\\mathbb{Q}}\\left[\\partial_{\\omega}\\ell_{o u t}\\left(\\omega,h(x),x,y\\right)\\right]}\\\\ &{\\partial_{h}L_{o u t}(\\omega,h)=\\!x\\mapsto r(x)\\mathbb{E}_{\\mathbb{Q}}\\left[\\partial_{v}\\ell_{o u t}(\\omega,h(x),x,y)|x\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. We follow a similar procedure as in Lemma D.2, where we decompose the proof into three steps: verifying that the objective $L_{o u t}$ is well-defined, identifying a candidate for the differential and proving that it is the differential of $L_{o u t}$ . ", "page_idx": 23}, {"type": "text", "text": "Well-definiteness of the objective. Let $(\\omega,h)$ be in $\\Omega\\times\\mathcal{H}$ . First, note that by Assumption $({\\bf{B}})$ , we have that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\mathbb{Q}}\\left[\\left\\|h(x)\\right\\|^{2}\\right]=\\mathbb{E}_{\\mathbb{P}}\\left[\\left\\|h(x)\\right\\|^{2}r(x)\\right]\\leq M\\left\\|h\\right\\|_{\\mathcal{H}}^{2}<+\\infty.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The next inequalities control the growth of $\\ell_{o u t}$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\ell_{o u t}(\\omega,h(x),x,y)|\\leq|\\ell_{o u t}(\\omega,0,x,y)|+|\\ell_{o u t}(\\omega,h(x),x,y)-\\ell_{o u t}(\\omega,0,x,y)|}}\\\\ &{}&{\\leq|\\ell_{o u t}(\\omega,0,x,y)|+\\int_{0}^{1}\\mathrm{d}t\\,\\big|h(x)^{\\top}\\partial_{v}\\ell_{o u t}(\\omega,t h(x),x,y)\\big|}\\\\ &{}&{\\leq|\\ell_{o u t}(\\omega,0,x,y)|+\\|h(x)\\|\\int_{0}^{1}\\mathrm{d}t\\,\\|\\partial_{v}\\ell_{o u t}(\\omega,t h(x),x,y)|}\\\\ &{}&{\\leq|\\ell_{o u t}(\\omega,0,x,y)|+C\\,\\|h(x)\\|\\,\\big(1+\\|h(x)\\|+\\|x\\|+\\|y\\|\\big)}\\\\ &{}&{\\leq|\\ell_{o u t}(\\omega,0,x,y)|+C\\,\\Big(1+3\\,\\|h(x)\\|^{2}+\\|x\\|^{2}+\\|y\\|^{2}\\Big)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The first line is due to the triangular inequality while the second line follows by differentiability of $\\partial_{v}\\ell_{o u t}$ in its second argument (Assumption $(\\mathbf{K})$ ). The third line follows by Cauchy-Scwharz inequality wile the fourth line uses that $\\ell_{o u t}$ has at most a linear growth in its last three arguments by Assumption (L). Using the above inequalities, we get the following upper-bound on $L_{o u t}$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|L_{o u t}(\\omega,h)\\right|\\leq\\mathbb{E}_{\\mathbb{Q}}\\left[\\left|\\ell_{o u t}\\left(\\omega,0,x,y\\right)\\right|\\right]+C\\left(1+3\\mathbb{E}_{\\mathbb{Q}}\\left[\\left|{h}(x)\\right|\\right|^{2}\\right]+\\mathbb{E}_{\\mathbb{Q}}\\left[\\left|{x}\\right|\\right|^{2}+\\left|{y}\\right|^{2}\\right]\\right)<+\\infty.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In the above upper-bound, $\\mathbb{E}_{\\mathbb{Q}}\\left[\\left\\|h(x)\\right\\|^{2}\\right]$ is finite by Equation (32). Additionally, $\\mathbb{E}_{\\mathbb{Q}}\\left[\\left\\|x\\right\\|^{2}+\\left\\|y\\right\\|^{2}\\right]$ is finite since $\\mathbb{Q}$ has finite second moments by Assumption (A) while $\\mathbb{E}_{\\mathbb{Q}}\\left[|\\ell_{o u t}\\left(\\omega,\\bar{0},x,y\\right)|\\right]$ is also finite by Assumption (J). Therefore, $L_{o u t}$ is well defined over $\\Omega\\times\\mathcal{H}$ . ", "page_idx": 23}, {"type": "text", "text": "Candidate differential. Fix $(\\omega,h)$ in $\\Omega\\times\\mathcal{H}$ and define the following linear form: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{d_{o u t}(\\epsilon,g):=\\epsilon^{\\top}\\mathbb{E}_{\\mathbb{Q}}\\left[\\partial_{\\omega}\\ell_{o u t}\\left(\\omega,h(x),x,y\\right)\\right]+\\mathbb{E}_{\\mathbb{Q}}\\left[g(x)^{\\top}\\partial_{v}\\ell_{o u t}(\\omega,h(x),x,y)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Define $D_{o u t}=(D_{\\omega},D_{h})$ to be: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{\\omega}:=\\mathbb{E}_{\\mathbb{Q}}\\left[\\partial_{\\omega}\\ell_{o u t}\\left(\\omega,h(x),x,y\\right)\\right]}\\\\ &{D_{h}:=x\\mapsto r(x)\\mathbb{E}_{\\mathbb{Q}}\\left[\\partial_{v}\\ell_{o u t}(\\omega,h(x),x,y)|x\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By an argument similar to the one in Lemma D.2, we see that $d_{o u t}(\\epsilon,g)=\\langle g,D_{h}\\rangle_{\\mathcal{H}}+\\epsilon^{\\top}D_{\\omega}$ . We now need to show that $D_{\\omega}$ and $D_{h}$ are well defined elements of $\\Omega$ and $\\mathcal{H}$ . ", "page_idx": 23}, {"type": "text", "text": "Square integrability of $D_{h}$ . We use the following upper-bounds: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathbb{P}}\\left[\\left\\|D_{h}(x)\\right\\|^{2}\\right]\\leq\\mathbb{E}_{\\mathbb{P}}\\left[r(x)^{2}\\mathbb{E}_{\\mathbb{Q}}\\left[\\|\\partial_{v}\\ell_{o u t}(\\omega,h(x),x,y)\\||x|^{2}\\right]\\right.}\\\\ &{\\qquad\\qquad\\qquad\\left.\\leq\\mathbb{E}_{\\mathbb{P}}\\left[r(x)^{2}\\mathbb{E}_{\\mathbb{Q}}\\left[\\|\\partial_{v}\\ell_{o u t}(\\omega,h(x),x,y)\\|^{2}\\middle|x\\right]\\right]}\\\\ &{\\qquad\\qquad\\leq M\\mathbb{E}_{\\mathbb{P}}\\left[r(x)\\mathbb{E}_{\\mathbb{Q}}\\left[\\|\\partial_{v}\\ell_{o u t}(\\omega,h(x),x,y)\\|^{2}\\middle|x\\right]\\right]}\\\\ &{\\qquad=M\\mathbb{E}_{\\mathbb{Q}}\\left[\\|\\partial_{v}\\ell_{o u t}(\\omega,h(x),x,y)\\|^{2}\\right]}\\\\ &{\\qquad\\qquad\\leq4M C\\left(1+\\mathbb{E}_{\\mathbb{Q}}\\left[\\left\\|h(x)\\right\\|^{2}\\right]+\\mathbb{E}_{\\mathbb{Q}}\\left[\\left\\|x\\right\\|^{2}+\\left\\|y\\right\\|^{2}\\right]\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The first inequality is an application of Jensen\u2019s inequality by convexity of the norm, while the second one is an application of Cauchy-Schwarz inequality. The third line uses that $r(x)$ is upper-bounded by a constant $M$ by Assumption $({\\bf{B}})$ , and the fourth line follows from the \u201ctower\u201d property for conditional probability distributions. Finally, the last line follows by Assumption $(\\mathbf{L})$ which ensures that $\\partial_{v}\\ell_{o u t}$ has at most a linear growth in its last three arguments. By Equation (32), we have that $\\mathbb{E}_{\\mathbb{Q}}\\left[\\left\\|h(x)\\right\\|^{2}\\right]<+\\infty$ . Moreover, since $\\mathbb{Q}$ has finite second order moment by Assumption (A), we also have that $\\mathbb{E}_{\\mathbb{Q}}\\left[\\left\\|x\\right\\|^{2}+\\left\\|y\\right\\|^{2}\\right]<+\\infty$ . We therefore conclude that $\\mathbb{E}_{\\mathbb{P}}\\left[\\left\\|D_{h}(x)\\right\\|^{2}\\right]$ is finite which ensure that $D_{h}$ belongs to $\\mathcal{H}$ . ", "page_idx": 24}, {"type": "text", "text": "Well-definiteness of $D_{\\omega}$ . To show that $D_{\\omega}$ is well defined, we need to prove that $(x,y)\\;\\mapsto\\;$ $\\partial_{\\omega}\\ell_{o u t}\\left(\\omega,h(x),x,y\\right)$ is integrable under $\\mathbb{Q}$ . By Assumption $(\\mathbf{L})$ , we know that $\\partial_{\\omega}\\ell_{o u t}$ has at most a quadratic growth in it last three arguments so that the following inequality holds. ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\partial_{\\omega}\\ell_{o u t}\\left(\\omega,h(x),x,y\\right)\\|\\leq C\\left\\|1+\\|h(x)\\|^{2}+\\|x\\|^{2}+\\|y\\|^{2}\\right\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We can directly conclude by taking the expectation under $\\mathbb{Q}$ in the above inequality and recalling that $\\mathbb{E}_{\\mathbb{Q}}\\left[\\left\\|h(x)\\right\\|^{2}\\right]$ is finite by Equation (32), and that $\\mathbb{Q}$ has finite second-order moments by Assumption (A). ", "page_idx": 24}, {"type": "text", "text": "Differentiability of $L_{o u t}$ . Since differentiability is a local notion, we may assume without loss of generality that $\\|\\epsilon\\|^{2}\\!+\\!\\|g\\|_{\\mathcal{H}}^{2}\\leq1$ . Introduce the functions $\\Delta_{1}$ and $\\Delta_{2}$ defined over $\\Omega\\!\\times\\!\\mathcal{H},\\mathcal{X}\\!\\times\\!\\mathcal{Y}\\!\\times\\![0,1]$ as follows: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta_{1}(\\epsilon,g,x,y,t):=\\partial_{v}\\ell_{o u t}\\,(\\omega+t\\epsilon,h(x)+t g(x),x,y)-\\partial_{v}\\ell_{o u t}\\,(\\omega+t\\epsilon,h(x),x,y)}\\\\ &{\\Delta_{1}^{\\prime}(\\epsilon,g,x,y,t):=\\partial_{v}\\ell_{o u t}\\,(\\omega+t\\epsilon,h(x),x,y)-\\partial_{v}\\ell_{o u t}\\,(\\omega,h(x),x,y)}\\\\ &{\\Delta_{2}(\\epsilon,g,x,y,t):=\\partial_{\\omega}\\ell_{o u t}\\,(\\omega+t\\epsilon,h(x)+t g(x),x,y)-\\partial_{\\omega}\\ell_{o u t}\\,(\\omega+t\\epsilon,h(x),x,y)}\\\\ &{\\Delta_{2}^{\\prime}(\\epsilon,g,x,y,t):=\\partial_{\\omega}\\ell_{o u t}\\,(\\omega+t\\epsilon,h(x),x,y)-\\partial_{\\omega}\\ell_{o u t}\\,(\\omega,h(x),x,y)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We consider the first-order error $E(\\epsilon,g)$ which admits the following upper-bounds: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{E(\\epsilon,\\phi)=\\left|L_{{\\infty t}}(a+\\epsilon,h+g)-L_{{\\infty t}}(a,h)-d_{\\infty}(\\epsilon,g)\\right|}\\\\ &{=\\left|\\overline{{\\mathbf{g}_{{\\phi}}}}\\left[\\int_{0}^{1}\\mathrm{d}t\\left(g\\tau^{\\top}(h+\\Delta_{\\lambda})(\\epsilon,g,x,y,t)+\\epsilon^{\\top}(\\Delta_{\\lambda}+\\Delta_{\\lambda}^{-\\epsilon})(\\epsilon,g,x,y,t)\\right)\\right]\\right|}\\\\ &{\\leq\\mathrm{Eq}\\left[\\int_{0}^{1}\\mathrm{d}t\\left\\|g(x)\\right\\|(\\|\\Delta_{\\lambda}(\\epsilon,g,x,y,t)\\|+\\|\\Delta_{\\lambda}^{-\\epsilon}(\\epsilon,g,x,y,t)\\|)\\right]}\\\\ &{\\quad+\\mathrm{Eq}\\left[\\int_{0}^{1}\\mathrm{d}t\\left\\|\\mathrm{d}\\Omega_{\\alpha}(\\epsilon,g,x,y,t)\\right\\|+\\|\\Delta_{\\lambda}^{-\\epsilon}(\\epsilon,g,x,y,t)\\|\\right]}\\\\ &{\\leq\\mathrm{Eq}\\left[\\|g(x)\\|^{2}\\right]^{\\frac{1}{2}}\\left\\{\\frac{\\mathrm{Eq}\\left[\\int_{0}^{1}\\mathrm{d}|\\Omega_{\\lambda}(\\epsilon,g,x,y,t)|^{2}\\right]^{\\frac{1}{2}}}{\\mathrm{Aq}(\\epsilon,g)}\\leq\\frac{\\mathrm{Eq}\\left[\\int_{0}^{1}\\mathrm{d}|\\Omega_{\\lambda}^{\\prime}(\\epsilon,g,x,y,t)|^{2}\\right]^{\\frac{1}{2}}}{\\mathrm{Aq}(\\epsilon,g)}\\right\\}}\\\\ &{\\quad+\\left\\|\\mathrm{e}\\left[\\frac{\\mathrm{g}}{\\int_{0}}\\int_{0}^{1}\\mathrm{d}|\\Omega_{\\lambda}(\\epsilon,g,x,y,t)|\\right]+\\frac{\\mathrm{Eq}\\left[\\int_{0}^{1}\\mathrm{d}|\\Omega_{\\lambda}^{\\prime}(\\epsilon,g,x,y,t)|^{2}\\right]}{\\mathrm{Eq}\\left[\\int_{0}^{1}\\mathrm{d}|\\Omega_{\\lambda}^{\\prime}(\\epsilon,g,x,y,t)|^{2}\\right]}\\right\\}}\\\\ &{\\leq M\\left\\|g_{{\\phi}}(A(\\epsilon,g)+\\Delta_{\\lambda}^{\\epsilon}(\\epsilon,g))+\\mathrm{Eq}\\left[\\left(A_{\\lambda}(\\epsilon,g)+\\Delta_{\\lambda}(\\epsilon,g)\\right)\\right]\\right\\|}\\end{array},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The second line uses differentiability of $\\ell_{o u t}$ (Assumption $(\\mathbf{K})$ ). The third uses the triangular inequality, while the fourth line uses Cauchy-Schwarz inequality. Finally, the last line uses Equation (32). ", "page_idx": 24}, {"type": "text", "text": "We simply need to show that each of the terms $A_{1},A_{2},A_{3}$ and $A_{4}$ converge to 0 as $\\epsilon$ and $g$ converge to 0. We treat each term separately. ", "page_idx": 24}, {"type": "text", "text": "Controlling $A_{1}$ and $A_{3}$ . For $\\epsilon$ small enough so that Assumption (L) holds, the following upperbounds on $A_{1}$ and $A_{2}$ hold: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{1}(\\epsilon,g)\\leq C\\mathbb{E}_{\\mathbb Q}\\left[\\left\\|g(x)\\right\\|^{2}\\right]^{\\frac{1}{2}}}\\\\ &{\\qquad\\qquad\\leq C M^{\\frac{1}{2}}\\left\\|g\\right\\|_{\\mathcal H}}\\\\ &{A_{3}(\\epsilon,g)\\leq C\\mathbb R_{0}\\left[\\left(1+\\left\\|h(x)+t g(x)\\right\\|+\\left\\|h(x)\\right\\|+\\left\\|x\\right\\|+\\left\\|y\\right\\|\\right)\\left\\|g(x)\\right\\|\\right]}\\\\ &{\\qquad\\qquad\\leq C\\mathbb E_{0}\\left[\\left\\|g(x)\\right\\|^{2}\\right]^{\\frac{1}{2}}\\left(1+2\\mathbb{E}_{\\mathbb Q}\\left[\\left\\|h(x)\\right\\|^{2}\\right]^{\\frac{1}{2}}+\\mathbb{E}_{\\mathbb Q}\\left[\\left\\|g(x)\\right\\|^{2}\\right]^{\\frac{1}{2}}+\\mathbb{E}_{\\mathbb Q}\\left[\\left\\|x\\right\\|^{2}+\\left\\|y\\right\\|^{2}\\right]^{\\frac{1}{2}}\\right)}\\\\ &{\\qquad\\qquad\\leq C M^{\\frac{1}{2}}\\left\\|g\\right\\|_{\\mathcal H}\\left(1+M^{\\frac{1}{2}}+2\\mathbb{E}_{\\mathbb Q}\\left[\\left\\|h(x)\\right\\|^{2}\\right]^{\\frac{1}{2}}+\\mathbb{E}_{\\mathbb Q}\\left[\\left\\|x\\right\\|^{2}+\\left\\|y\\right\\|^{2}\\right]^{\\frac{1}{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "For $A_{1}$ , we used that $\\partial_{v}\\ell_{o}u t$ has is Lipschitz continuous in its second argument for any $x,y\\in\\mathcal{X}\\times\\mathcal{Y}$ and locally in $\\omega$ by Assumption (L). The second upper-bound on $A_{1}$ uses Equation (32). For $A_{2}$ , we used the locally Lipschitz property of $\\partial_{\\omega}\\ell_{o u t}$ from Assumption $(\\mathbf{L})$ , followed by Cauchy-Schwarz inequality and Equation (32). For the last line, we also used that $\\|g\\|_{\\mathcal H}\\leq1$ by assumption. The above upper-bounds on $A_{1}$ and $A_{3}$ ensure that these quantities converge to 0 as $\\epsilon$ and $g$ approach 0. ", "page_idx": 25}, {"type": "text", "text": "Controlling $A_{2}$ and $A_{4}$ . To show that $A_{2}$ and $A_{4}$ converge to 0, we will use the dominated convergence theorem. It is easy to see that $\\Delta_{1}^{\\prime}\\left(\\epsilon,g,x,y,t\\right)$ and $\\Delta_{2}^{\\prime}\\left(\\epsilon,g,x,y,t\\right)$ converge point-wise to 0 when $\\epsilon$ and $g$ converge to 0 since $(\\omega,v)\\mapsto\\partial_{v}\\ell_{o u t}(\\omega,v,x,y)$ and $(\\omega,v)\\overset{.}{\\mapsto}\\partial_{\\omega}\\ell_{o u t}\\bar{(}\\omega,v,x,y)$ are continuous by Assumption $(\\mathbf{K})$ . It remains to dominate these functions. For $\\epsilon$ small enough so that Assumption $(\\mathbf{L})$ holds, we have that: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta_{1}^{\\prime}\\left(\\epsilon,g,x,y,t\\right)^{2}\\leq\\!16C^{2}\\left(1+\\left\\Vert h(x)\\right\\Vert^{2}+\\left\\Vert x\\right\\Vert^{2}+\\left\\Vert y\\right\\Vert^{2}\\right)}\\\\ &{\\Delta_{2}^{\\prime}\\left(\\epsilon,g,x,y,t\\right)\\leq\\!2C\\left(1+\\left\\Vert h(x)\\right\\Vert^{2}+\\left\\Vert x\\right\\Vert^{2}+\\left\\Vert y\\right\\Vert^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Both upper-bounds are integrable under $\\mathbb{Q}$ since $\\mathbb{E}_{\\mathbb{Q}}\\left[\\left\\|h(x)\\right\\|^{2}\\right]<+\\infty$ by Equation (32) and $\\mathbb{Q}$ has finite second-order moment by Assumption (A). Therefore, by the dominated convergence theorem, we deduce that $A_{2}$ and $A_{4}$ converge to 0 as $\\epsilon$ and $g$ approach 0. ", "page_idx": 25}, {"type": "text", "text": "Finally, we have shown that $E(\\epsilon,g)\\;=\\;o\\left(\\|\\epsilon\\|+\\|g\\|_{\\mathcal{H}}\\right)$ which allows to conclude that $L_{o u t}$ is differentiable with the partial derivatives given by Equation (31). \u53e3 ", "page_idx": 25}, {"type": "text", "text": "Lemma D.4 (Differentiability of $\\partial_{h}L_{i n}$ ). Under Assumptions (A) and $(E)$ to $(I)$ , the differential map $(\\omega,h)\\mapsto\\partial_{h}L_{i n}(\\omega,h)$ defined in Lemma $D.2$ is differentiable on $\\Omega\\times\\mathcal{H}$ in the sense of Definition C.1. Its differential $d_{(\\omega,h)}\\partial_{h}L_{i n}(\\omega,h):\\Omega\\times\\mathcal{H}\\rightarrow\\mathcal{H}$ acts on elements $(\\epsilon,g)\\in\\Omega\\times\\mathcal{H}$ as follows: ", "page_idx": 25}, {"type": "equation", "text": "$$\nd_{(\\omega,h)}\\partial_{h}L_{i n}(\\omega,h)(\\epsilon,g)=\\partial_{h}^{2}L_{i n}(\\omega,h)g+(\\partial_{\\omega,h}L_{i n}(\\omega,h))^{\\star}\\,\\epsilon,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\partial_{h}^{2}L_{i n}(\\omega,h):\\mathcal{H}\\rightarrow\\mathcal{H}$ is a linear symmetric operator representing the partial derivative of $\\partial_{h}L_{i n}(\\omega,h)$ w.r.t $h$ and $(\\partial_{\\omega,h}L_{i n}(\\omega,h))^{\\star}$ is the adjoint of $\\partial_{\\omega,h}L_{i n}(\\omega,h):\\mathcal{H}\\rightarrow\\Omega$ which represents the partial derivative of $\\partial_{h}L_{i n}(\\omega,h)\\ w.r.t\\omega$ . Moreover, $\\partial_{h}^{2}L_{i n}(\\omega,h)$ and $\\partial_{\\omega,h}L_{i n}(\\omega,h)$ are given by: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\partial_{h}^{2}L_{i n}(\\omega,h)g=\\!x\\mapsto\\mathbb{E}_{\\mathbb{P}}\\left[\\partial_{v}^{2}\\ell_{i n}(\\omega,h(x),x,y)\\big|x\\right]g(x)}\\\\ &{}&{\\partial_{\\omega,h}L_{i n}(\\omega,h)g=\\!\\!\\mathbb{E}_{\\mathbb{P}}\\left[\\partial_{\\omega,v}\\ell_{i n}(\\omega,h(x),x,y)g(x)\\right],\\quad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. Let $(\\omega,h)$ be in $\\Omega\\times\\mathcal{H}$ . To show that $\\partial_{\\omega}L_{i n}$ is Hadamard differentiable, we proceed in two steps: we first identify a candidate differential and show that it is a bounded operator, then we prove Hadamard differentiability. ", "page_idx": 25}, {"type": "text", "text": "Candidate differential. For a given $(\\omega,h)\\in\\Omega\\times\\mathcal{H}$ , we consider the following linear operators $C_{w,h}:\\mathcal{H}\\rightarrow\\mathcal{H}$ and $B_{w,h}:\\mathcal{H}\\rightarrow\\Omega$ : ", "page_idx": 25}, {"type": "equation", "text": "$$\nC_{\\omega,h}g=\\mathbb{E}_{\\mathbb{P}}\\left[\\partial_{v}^{2}\\ell_{i n}(\\omega,h(x),x,y)\\middle|x\\right]g(x),\\qquad B_{\\omega,h}g=\\mathbb{E}_{\\mathbb{P}}\\left[\\partial_{\\omega,v}\\ell_{i n}(\\omega,h(x),x,y)g(x)\\right],\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the expectations are over $y$ conditionally on $x$ . Next, we show that $C_{\\omega,h}$ and $B_{\\omega,h}$ are well-defined and bounded. ", "page_idx": 25}, {"type": "text", "text": "Well-definiteness of the operator $C_{\\omega,h}$ . The first step is to show that the image $C_{\\omega,h}g$ of any element $g\\in\\mathcal H$ by $C_{\\omega,h}$ is also an element in $\\mathcal{H}$ . To this end, we simply need to find a finite upper-bound on $\\|C_{\\omega,h}g\\|_{\\mathcal{H}}$ for a given $g\\in\\mathcal H$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|C_{\\omega,h}g\\right\\|_{\\mathcal{H}}^{2}=\\mathbb{E}_{\\mathbb{P}}\\left[\\left\\|\\mathbb{E}_{\\mathbb{P}}\\left[\\partial_{v}^{2}\\ell_{i n}(\\omega,h(x),x,y)\\middle|x\\right]g(x)\\right\\|^{2}\\right]}\\\\ &{\\qquad\\qquad\\leq\\mathbb{E}_{\\mathbb{P}}\\left[\\left\\|\\mathbb{E}_{\\mathbb{P}}\\left[\\partial_{v}^{2}\\ell_{i n}(\\omega,h(x),x,y)\\middle|x\\right]\\right\\|_{o p}^{2}\\left\\|g(x)\\right\\|^{2}\\right]}\\\\ &{\\qquad\\qquad\\leq\\mathbb{E}_{\\mathbb{P}}\\left[\\mathbb{E}_{\\mathbb{P}}\\left[\\left\\|\\partial_{v}^{2}\\ell_{i n}(\\omega,h(x),x,y)\\right\\|_{o p}\\middle|x\\right]^{2}\\left\\|g(x)\\right\\|^{2}\\right]}\\\\ &{\\qquad\\qquad\\leq\\mathbb{E}_{\\mathbb{P}}\\left[\\left\\|\\partial_{v}^{2}\\ell_{i n}(\\omega,h(x),x,y)\\right\\|_{o p}^{2}\\left\\|g(x)\\right\\|^{2}\\right]}\\\\ &{\\qquad\\qquad\\leq L^{2}\\left\\|g\\right\\|_{\\mathcal{H}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The second line follows using the operator norm inequality, the third line follows by Jensen\u2019s inequality applied to the norm, while the fourth uses the \u201ctower\u201d property for conditional distributions. Finally, the last line uses that $\\partial_{v}^{2}\\ell_{i n}$ is upper-bounded uniformly in $x$ and $y$ by Assumption $(\\mathbf{F})$ . Therefore, we conclude that $C_{\\omega,h}g$ belongs to $\\mathcal{H}$ . Moreover, the inequality $\\|C_{\\omega,h}g\\|_{\\mathcal{H}}\\leq L\\,\\|g\\|_{\\mathcal{H}}$ also establishes the continuity of the operator $C_{\\omega,h}$ . ", "page_idx": 26}, {"type": "text", "text": "Well-definiteness of the operator $B_{\\omega,h}$ . We first show that the image $B_{\\omega,h}$ is bounded. For a given $g$ in $\\mathcal{H}$ , we write: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\|B_{\\alpha,h}(g)\\|=\\|\\mathbb{E}_{\\mathcal{F}}\\left[\\left\\|\\partial_{\\alpha,v}\\ell_{i n}(\\omega,h(x),x,y)g(x)\\right\\|\\right]}}\\\\ &{\\leq\\mathbb{E}_{\\mathcal{F}}\\left[\\left\\|\\partial_{\\alpha,v}\\ell_{i n}(\\omega,h(x),x,y)g(x)\\right\\|\\right]}\\\\ &{\\leq\\mathbb{E}_{\\mathcal{F}}\\left[\\left\\|\\partial_{\\alpha,v}\\ell_{i n}(\\omega,h(x),x,y)\\right\\|_{\\partial,p}\\left\\|g(x)\\right\\|\\right]}\\\\ &{\\leq\\|g\\|_{\\mathcal{H}}\\mathbb{E}_{\\mathcal{F}}\\left[\\left\\|\\partial_{\\alpha,v}\\ell_{i n}(\\omega,h(x),x,y)\\right\\|_{\\partial,p}^{2}\\right]^{\\frac{1}{2}}}\\\\ &{\\leq\\|g\\|_{\\mathcal{H}}\\mathbb{E}_{\\mathcal{F}}\\left[\\left\\|\\partial_{\\alpha,v}\\ell_{i n}(\\omega,h(x),x,y)-\\partial_{\\alpha,v}\\ell_{i n}(\\omega,0,x,y)\\right\\|_{\\partial,p}^{2}\\right]^{\\frac{1}{2}}}\\\\ &{\\quad+\\left\\|g\\right\\|_{\\mathcal{H}}\\mathbb{E}_{\\mathcal{F}}\\left[\\left\\|\\partial_{\\alpha,v}\\ell_{i n}(\\omega,0,x,y)\\right\\|_{\\partial,p}^{2}\\right]^{\\frac{1}{2}}}\\\\ &{\\leq C\\left\\|g\\right\\|_{\\mathcal{H}}\\left(\\mathbb{E}_{\\mathcal{F}}\\left[\\left\\|h(x)\\right\\|^{2}\\right]^{\\frac{1}{2}}+\\mathbb{E}_{\\mathcal{F}}\\left[\\left(1+\\|x\\|^{2}\\right\\|^{2}\\right)^{\\frac{1}{2}}\\right)}\\\\ &{\\leq C\\|g\\|_{\\mathcal{H}}\\left(\\|h\\|_{\\mathcal{H}}+2\\mathbb{E}_{\\mathcal{F}}\\left[1+\\|x\\|^{2}+\\|y\\|^{2}\\right]\\right)<+\\infty.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "In the above expression, the second line is due to Jensen\u2019s inequality applied to the norm function, the third line follows from the operator norm inequality, while the fourth follows by Cauchy-Schwarz. The fifth line is due to the triangular inequality. Finally, the sixth line relies on two facts: 1) that $v\\mapsto\\partial_{\\omega,v}\\ell_{i n}(\\omega,v,x,y)$ is Lipschitz uniformly in $x$ and $y$ and locally in $\\omega$ by Assumption (I), and, 2) that $\\|\\partial_{\\omega,v}\\ell_{i n}(\\omega,0,x,y)\\|$ has at most a linear growth in $x$ and $y$ locally in $\\omega$ by Assumption $(\\mathbf{H})$ . Since $\\mathbb{P}$ has finite second order moments by Assumption (A) and both $h$ and $g$ are square integrable, we conclude that the constant $\\|B_{\\omega,h}\\|$ is finite. Moreover, the last inequality establishes that $B_{\\omega,h}$ is a continuous linear operator from $\\mathcal{H}$ to $\\Omega$ . One can then see that the adjoint of $B_{\\omega,h}$ admits a representation of the form: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(B_{\\omega,h})^{\\star}\\epsilon:=(\\partial_{\\omega,h}L_{i n}(\\omega,h))^{\\star}\\,\\epsilon=x\\mapsto\\mathbb{E}_{\\mathbb{P}}\\left[(\\partial_{\\omega,v}\\ell_{i n}(\\omega,h(x),x,y))^{\\top}\\Big|x\\right]\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Therefore, we can consider the following candidate operator $d_{i n}^{2}$ for the differential of $\\partial_{h}L_{i n}$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\nd_{i n}^{2}(\\epsilon,g):=C_{\\omega,h}g+(B_{\\omega,h})^{\\star}\\epsilon.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Differentiablity of $\\partial_{h}L_{i n}$ . We will show that $\\partial_{h}L_{i n}$ is jointly Hadamard differentiable at $(\\omega,h)$ with differential operator given by: ", "page_idx": 26}, {"type": "equation", "text": "$$\nd_{(\\omega,h)}\\partial_{h}L_{i n}(\\omega,h)(\\epsilon,g)=C_{\\omega,h}g+(B_{\\omega,h})^{\\star}\\epsilon.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "To this end, we consider a sequence $\\left(\\epsilon_{k},g_{k}\\right)_{k\\geq1}$ converging in $\\Omega\\times\\mathcal{H}$ towards an element $(\\epsilon,g)\\in$ $\\Omega\\times\\mathcal{H}$ and a non-vanishing real valued sequence $t_{k}$ converging to 0. Define the first-order error $E_{k}$ as follows: ", "page_idx": 27}, {"type": "equation", "text": "$$\nE_{k}:=\\left\\|\\frac{1}{t_{k}}\\left(\\partial_{h}L_{i n}(\\omega+t_{k}\\epsilon_{k},h+t_{k}g_{k})-\\partial_{h}L_{i n}(\\omega,h)\\right)-C_{\\omega,h}g-(B_{\\omega,h})^{\\star}\\epsilon\\right\\|_{\\mathcal{H}}^{2}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Introduce the functions $P_{1},P_{2},\\Delta_{1}$ and $\\Delta_{2}$ defined over $\\mathbb{N}^{\\star},\\mathcal{X}\\times\\mathcal{Y}\\times[0,1]$ as follows: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P_{1}(k,x,y,s)=\\left\\{\\partial_{v}^{2}\\ell_{i n}(\\omega+s t_{k}\\epsilon_{k},h(x)+s t_{k}g_{k}(x),x,y),\\qquad k\\geq1\\right.}\\\\ &{\\left.\\partial_{v}^{2}\\ell_{i n}(\\omega,h(x),x,y),\\right.}&{k=0}\\\\ &{P_{2}(k,x,y,s)=\\left\\{(\\partial_{\\omega,v}\\ell_{i n}(\\omega+s t_{k}\\epsilon_{k},h(x)+s t_{k}g_{k}(x),x,y))^{\\top}\\qquad k\\geq1\\right.}\\\\ &{\\left.\\left.\\vphantom{\\int^{0}}\\Delta_{v,v}\\ell_{i n}(\\omega,h(x),x,y)\\right\\}^{\\top},\\qquad k=0.1}\\\\ &{\\Delta_{1}(k,x,y,s)=P_{1}(k,x,y,s)-P_{1}(0,x,y,s),}\\\\ &{\\Delta_{2}(k,x,y,s)=P_{2}(k,x,y,s)-P_{2}(0,x,y,s).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "By joint differentiability of $(\\omega,v)\\mapsto\\partial_{v}\\ell_{i n}(\\omega,v,x,y)$ (Assumption $\\mathbf{\\Psi}(\\mathbf{E})$ ), we use the fundamental theorem of calculus to express $E_{k}$ in terms of $\\Delta_{1}$ and $\\Delta_{2}$ : ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Sigma_{k}=\\mathbb{E}_{\\mathcal{T}}\\left[\\left\\|\\mathbb{E}_{\\mathcal{F}}\\left[\\int_{0}^{1}\\mathrm{d}t(P_{1}(k,x,y,s)g_{k}(x)\\!-\\!P_{1}(0,x,y,s)g(x)+P_{2}(k,x,y,s)\\epsilon_{k}\\!-\\!P_{2}(0,x,y,s)\\epsilon)\\right|x\\right]\\right\\|}\\\\ &{\\quad\\leq\\mathbb{E}_{\\mathcal{F}}\\left[\\mathbb{E}_{\\mathcal{F}}\\left[\\int_{0}^{1}\\mathrm{d}t\\left|P_{1}(k,x,y,s)g_{k}(x)\\!-\\!P_{1}(0,x,y,s)g(x)+P_{2}(k,x,y,s)\\epsilon_{k}\\!-\\!P_{2}(0,x,y,s)\\epsilon\\right|^{2}\\right]x\\right]}\\\\ &{\\quad=\\mathbb{E}_{\\mathcal{F}}\\left[\\int_{0}^{1}\\mathrm{d}t\\left|P_{1}(k,x,y,s)g_{k}(x)-P_{1}(0,x,y,s)g(x)+P_{2}(k,x,y,s)\\epsilon_{k}-P_{2}(0,x,y,s)\\epsilon_{k}\\right|^{2}\\right]}\\\\ &{\\quad\\leq4\\underbrace{\\mathbb{E}_{\\mathcal{F}}\\left[\\int_{0}^{1}\\mathrm{d}t\\left|\\Delta_{1}(k,x,y,t)\\right|_{\\mathcal{F}_{\\sigma}}^{2}\\left\\|g(x)\\right\\|^{2}\\right]}_{A_{k}^{(\\prime)}}+4\\left\\|e\\right\\|^{2}\\underbrace{\\mathbb{E}_{\\mathcal{F}}\\left[\\int_{0}^{1}\\mathrm{d}t\\left|\\Delta_{2}(k,x,y,t)\\right|_{\\mathcal{F}_{\\sigma}}^{2}\\right]}_{B_{k}^{(\\prime)}}}\\\\ &{\\quad\\quad+\\underbrace{4\\mathbb{E}_{\\mathcal{F}}\\left[\\int_{0}^{1}\\mathrm{d}t\\left\\|P_{1}(k,x,y,t)\\right\\|_{\\mathcal{F}_{\\sigma}}^{2}\\left\\|g(x)-g_{k}(x)\\right\\|^{2}\\right]}_{A_{k}^{(\\prime)}}+4\\left\\|e-\\epsilon_{k}\\right\\|^{2}\\underbrace{\\mathbb{E}_{\\mathcal{F}}\\left[\\int_{0}^{1}\\mathrm{d}t\\left\\|P_{2}(k,x,y,t)\\right\\|_{\\mathcal{F}_{\\sigma}}^{2}\\right]}_{B_{\\mathcal{F}}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The second line uses Jensen\u2019s inequality applied to the squared norm, the fourth line results from the \u201ctower\u201d property of conditional distributions. The fifth line uses Jensen\u2019s inequality for the square function followed by the operator norm inequality. It remains to show that $\\bar{A_{k}^{(1)}},\\bar{B_{k}^{(1)}}$ and $\\bar{A}_{k}^{(2)}$ converge to 0 and that $B_{k}^{(2)}$ is bounded. ", "page_idx": 27}, {"type": "text", "text": "Upper-bound on $A_{k}^{(1)}$ . We will use the dominated convergence theorem. Assumption $(\\mathbf{F})$ ensures the existence of a positive constant $L$ and a neighborhood $B$ of $\\omega$ so that $v\\mapsto\\left\\lVert\\partial_{v}^{2}\\ell_{i n}(\\omega^{\\prime},v,x,y)\\right\\rVert_{o p}$ is bounded by $L$ for any $\\omega^{\\prime},x,y\\in B\\times\\mathcal{X}\\times\\mathcal{Y}$ . Since $\\omega+t_{k}\\epsilon_{k}\\rightarrow\\omega$ , then there exists some $K_{0}$ so that, for any $k\\geq K_{0}$ , we can ensure that $\\omega+t_{k}\\epsilon_{k}\\in B$ . This allows us to deduce that: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\Delta_{1}(k,x,y,t)\\right\\|_{o p}^{2}\\left\\|g(x)\\right\\|^{2}\\leq4L^{2}\\left\\|g(x)\\right\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "for any $k\\geq K_{0}$ and any $x,y\\in\\mathcal{X}\\times\\mathcal{Y}$ , with $\\|g(x)\\|^{2}$ being integrable under $\\mathbb{P}$ . ", "page_idx": 27}, {"type": "text", "text": "Moreover, we also have the following point-wise convergence for $\\mathbb{P}$ -almost all $x\\in\\mathscr{X}$ : ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\Delta_{1}(k,x,y,t)\\|_{o p}^{2}\\,\\|g(x)\\|^{2}\\to0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Equation (39) follows by noting that $\\omega+t_{k}\\epsilon_{k}\\rightarrow\\omega$ and that $h(x)+t_{k}g_{k}(x)\\rightarrow h(x)$ for $\\mathbb{P}.$ -almost all $x\\in\\mathscr{X}$ , since $t_{k}$ converges to $0$ , $\\epsilon_{k}$ converges to $\\epsilon$ and $g_{k}$ converges to $g$ in $\\mathcal{H}$ (a fortiori converges point-wise for $\\mathbb{P}$ -almost all $x\\ \\in\\ \\mathcal{X}$ ). Additionally, the map $(\\bar{\\omega^{\\prime}},v)\\;\\overset{\\bar{\\mathbf{\\mu}}}{\\mapsto}\\;\\|\\partial_{v}^{2}\\ell_{i n}(\\omega,v,x,y)\\|_{o p}$ is ", "page_idx": 27}, {"type": "text", "text": "continuous by Assumption (G), which allows to establish Equation (39). From Equations (38) and (39) we can apply the dominated convergence theorem which allows to deduce that $A_{k}^{(1)}\\rightarrow0$ . ", "page_idx": 28}, {"type": "text", "text": "Upper-bound on $A_{k}^{(2)}$ . By a similar argument as for $A_{k}^{(1)}$ and using Assumption (F), we know that there exists $K_{0}>0$ so that for any $k\\geq K_{0}$ : ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left\\|P_{1}(k,x,y,t)\\right\\|_{o p}^{2}\\leq L^{2}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Therefore, we directly get that: ", "page_idx": 28}, {"type": "equation", "text": "$$\nA_{k}^{(2)}\\leq L^{2}\\left\\|g(x)-g_{k}(x)\\right\\|_{\\mathcal{H}}^{2}\\to0,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where we used that $g_{k}\\rightarrow g$ by construction. ", "page_idx": 28}, {"type": "text", "text": "Upper-bound on $B_{k}^{(2)}$ . We will show that $\\|P_{2}\\left(k,x,y,t\\right)\\|_{o p}$ is upper-bounded by a square integrable function under $\\mathbb{P}$ . By Assumptions $(\\mathbf{H})$ and (I), there exists a neighborhood $B$ and a positive constant $C$ such that, for all $\\omega^{\\prime},v_{1},v_{2},x,y\\in B\\times\\mathcal{V}\\times\\mathcal{V}\\times\\mathcal{X}\\times\\mathcal{Y}$ : ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\|\\partial_{\\omega,v_{1}}\\ell_{i n}\\,(\\omega^{\\prime},0,x,y)\\|\\le C\\,(1+\\|x\\|+\\|y\\|)}\\\\ {\\|\\partial_{\\omega,v}\\ell_{i n}\\,(\\omega^{\\prime},v_{1},x,y)-\\partial_{\\omega,v}\\ell_{i n}\\,(\\omega^{\\prime},v_{2},x,y)\\|\\le C\\,\\|v_{1}-v_{2}\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By a similar argument as for $A_{k}^{(1)}$ , there exists $K_{0}$ so that for any $k\\ge K_{0}$ , the above inequalities hold when choosing $\\omega^{\\prime}\\,=\\,\\omega\\,+\\,t_{k}\\epsilon_{k}$ . Using this fact, we obtain the following upper-bound on $\\|P_{2}\\left(k,x,y,t\\right)\\|_{o p}$ for $k\\geq K_{0}$ : ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|P_{2}\\left(k,x,y,t\\right)\\right|_{o p}\\leq\\left\\|\\partial_{\\omega,v}\\ell_{i n}(\\omega+s t_{k}\\epsilon_{k},h(x)+s t_{k}g_{k}(x),x,y)-\\partial_{\\omega,v}\\ell_{i n}(\\omega+s t_{k}\\epsilon_{k},0,x,y)\\right\\|_{o p}}\\\\ &{\\phantom{\\leq}\\;+\\left\\|\\partial_{\\omega,v}\\ell_{i n}(\\omega+s t_{k}\\epsilon_{k},0,x,y)\\right\\|_{o p}}\\\\ &{\\qquad\\qquad\\leq C\\left(1+\\left\\|h(x)+s t_{k}g_{k}(x)\\right\\|+\\left\\|x\\right\\|+\\left\\|y\\right\\|\\right)}\\\\ &{\\qquad\\qquad\\leq C\\left(1+\\left\\|h(x)\\right\\|+t_{k}\\left\\|g_{k}(x)\\right\\|+\\left\\|x\\right\\|+\\left\\|y\\right\\|\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Therefore, by taking expectations and integrating over $t$ , it follows: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B_{k}^{(2)}\\leq C^{2}\\mathbb{E}_{\\mathbb{P}}\\left[\\left(1+\\|h(x)\\|+t_{k}\\left\\|g_{k}(x)\\right\\|+\\left\\|x\\right\\|+\\left\\|y\\right\\|\\right)^{2}\\right]}\\\\ &{\\qquad\\leq4C^{2}\\mathbb{E}_{\\mathbb{P}}\\left[\\left(1+\\left\\|h(x)\\right\\|^{2}+t_{k}^{2}\\left\\|g_{k}(x)\\right\\|^{2}+\\left\\|x\\right\\|^{2}+\\left\\|y\\right\\|^{2}\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By construction $t_{k}^{2}\\left\\|g_{k}(x)\\right\\|^{2}\\rightarrow0$ and is therefore a bounded sequence. Moreover, $\\mathbb{E}_{\\mathbb{P}}\\left[\\left\\|h(x)\\right\\|^{2}\\right]$ is finite since $h$ belongs to $\\mathcal{H}$ . Finally, $\\mathbb{E}_{\\mathbb{P}}\\left[\\left\\|x\\right\\|^{2}+\\left\\|y\\right\\|^{2}\\right]<+\\infty$ by Assumption (A). Therefore, we have shown that $B_{k}^{(2)}$ is bounded. ", "page_idx": 28}, {"type": "text", "text": "Upper-bound on $B_{k}^{(1)}$ . By a similar argument as for $B_{k}^{(2)}$ and using again Assumptions $(\\mathbf{H})$ and (I), there exists $K_{0}$ so that for any $k\\geq K_{0}$ : ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta_{2}(k,x,y,t)\\|_{o p}\\leq\\|\\partial_{\\omega,v}\\ell_{i n}(\\omega+s t_{k}\\epsilon_{k},h(x)+s t_{k}g_{k}(x),x,y)-\\partial_{\\omega,v}\\ell_{i n}(\\omega+s t_{k}\\epsilon_{k},h(x),x,y)\\|_{o p}}\\\\ &{\\qquad\\qquad\\qquad+\\|\\partial_{\\omega,v}\\ell_{i n}(\\omega+s t_{k}\\epsilon_{k},h(x),x,y)-\\partial_{\\omega,v}\\ell_{i n}(\\omega,h(x),x,y)\\|_{o p}}\\\\ &{\\qquad\\qquad\\qquad\\leq C t_{k}\\,\\|g_{k}(x)\\|+\\|\\partial_{\\omega,v}\\ell_{i n}(\\omega+s t_{k}\\epsilon_{k},h(x),x,y)-\\partial_{\\omega,v}\\ell_{i n}(\\omega,h(x),x,y)\\|_{o p}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where we used Equation (43) to get an upper-bound on the first terms. By squaring the above inequality and taking the expectation under $\\mathbb{P}$ we get: ", "page_idx": 28}, {"type": "equation", "text": "$$\nB_{k}^{(1)}\\leq2C t_{k}\\left\\|g_{k}\\right\\|_{\\mathcal{H}}^{2}+2\\mathbb{E}_{\\mathbb{P}}\\left[\\underbrace{\\|\\partial_{\\omega,v}\\ell_{i n}(\\omega+s t_{k}\\epsilon_{k},h(x),x,y)-\\partial_{\\omega,v}\\ell_{i n}(\\omega,h(x),x,y)\\|_{\\sigma_{p}}^{2}}_{e_{k}(x,y)}\\right].\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We only need to show that $\\mathbb{E}_{\\mathbb{P}}\\left[e_{k}(x,y)\\right]$ converges to 0 since the first term $2C t_{k}\\left\\|g_{k}\\right\\|_{\\mathcal{H}}^{2}$ already converges to 0 by construction of $t_{k}$ and $g_{k}$ . To achieve this, we will use the dominated convergence theorem. It is easy to see that $\\boldsymbol{e}_{k}(\\boldsymbol{x},y)$ converges to 0 point-wise by continuity of $\\omega\\mapsto\\partial_{\\omega,v}\\ell_{i n}\\bar{(}\\omega,v,x,y)$ ", "page_idx": 28}, {"type": "text", "text": "(Assumption (G)). Therefore, we only need to show that $\\boldsymbol{\\mathscr{e}}_{k}(\\boldsymbol{x},\\boldsymbol{y})$ is dominated by an integrable function. Provided that $k\\geq K_{0}$ , we can use Equations (42) and (43) to get the following upper-bounds: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{4}e_{k}(x,y)\\leq\\left\\|\\partial_{\\omega,v}\\ell_{i n}(\\omega+s t_{k}\\epsilon_{k},h(x),x,y)-\\partial_{\\omega,v}\\ell_{i n}(\\omega+s t_{k}\\epsilon_{k},0,x,y)\\right\\|_{o p}^{2}}\\\\ &{\\qquad\\qquad+\\left\\|\\partial_{\\omega,v}\\ell_{i n}(\\omega,h(x),x,y)-\\partial_{\\omega,v}\\ell_{i n}(\\omega,0,x,y)\\right\\|_{o p}^{2}}\\\\ &{\\qquad\\qquad+\\left\\|\\partial_{\\omega,v}\\ell_{i n}(\\omega,0,x,y)\\right\\|_{o p}^{2}+\\left\\|\\partial_{\\omega,v}\\ell_{i n}(\\omega+s t_{k}\\epsilon_{k},0,x,y)\\right\\|_{o p}^{2}}\\\\ &{\\qquad\\qquad\\leq2C^{2}\\left(1+\\left\\|h(x)\\right\\|^{2}+\\left\\|x\\right\\|^{2}+\\left\\|y\\right\\|^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "The l.h.s. of the last line is an integrable function that is independent of $k$ , since $h$ is square integrable by definition and $\\left\\|x\\right\\|^{2}+\\left\\|y\\right\\|^{2}$ are integrable by Assumption (A). Therefore, by application of the dominated convergence theorem, it follows that $\\mathbb{E}_{\\mathbb{P}}\\left[e_{k}(x,y)\\right]\\rightarrow0$ , we have shown that $B_{k}^{(1)}\\rightarrow0$ . ", "page_idx": 29}, {"type": "text", "text": "To conclude, we have shown that the first-order error $E_{k}$ converges to 0 which means that $(\\omega,h)\\mapsto$ $\\partial_{h}L_{i n}(\\omega,h)$ is jointly differentiable on $\\Omega\\times\\mathcal{H}$ , with differential given by Equations (34) and (35). ", "page_idx": 29}, {"type": "text", "text": "D.3 Proof of Proposition 2.3 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Proof. The strategy is to show that the conditions on $L_{i n}$ and $L_{o u t}$ stated in Proposition 2.2 hold. By Assumption (C), for any $\\omega\\in{\\Omega}$ , there exists a positive constant $\\mu$ and a neighborhood $B$ of $\\omega$ on which the function $\\ell_{i n}(\\omega^{\\prime},v,x,y)$ is $\\mu$ -strongly convex in $v$ for any $(\\omega^{\\prime},x,y)\\in B\\times\\mathcal{X}\\times\\mathcal{Y}$ . Therefore, by integration, we directly deduce that $\\dot{h}\\mapsto L_{i n}(\\omega^{\\prime},h)$ is $\\mu$ strongly convex in $h$ for any $\\omega^{\\prime}\\in B$ . By Lemmas D.2 and D.4, $h\\mapsto L_{i n}(\\omega,h)$ is differentiable on $\\mathcal{H}$ for all $\\omega\\in\\Omega$ and $\\partial_{h}L_{i n}$ is Hadamard differentiable on $\\Omega\\times\\mathcal{H}$ . Additionally, $L_{o u t}$ is jointly differentiable in $\\omega$ and $h$ by Lemma D.3. Therefore, the conditions on $L_{i n}$ and $L_{o u t}$ for applying Proposition 2.2 hold. Using the notations from Proposition 2.2, we have that the total gradient $\\nabla{\\mathcal{F}}(\\omega)$ can be expressed as: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\nabla\\mathcal{F}(\\omega)=g_{\\omega}+B_{\\omega}a_{\\omega}^{\\star}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $g_{\\omega}=\\partial_{\\omega}L_{o u t}(\\omega,h_{\\omega}^{\\star})$ , $B_{\\omega}=\\partial_{\\omega,h}L_{i n}(\\omega,h_{\\omega}^{\\star})$ and where $a_{\\omega}^{\\star}$ is the minimizer of the adjoint objective $L_{a d j}$ : ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{L_{a d j}(\\omega,a):=\\frac{1}{2}\\;a^{\\top}C_{\\omega}a+a^{\\top}d_{\\omega},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "with $C_{\\omega}=\\partial_{h}^{2}L_{i n}(\\omega,h_{\\omega}^{\\star})$ and $d_{\\omega}=\\partial_{h}L_{o u t}(\\omega,h_{\\omega}^{\\star})$ . Recalling the expressions of the first and second order differential operators from Lemmas D.2 and D.4, we deduce the expression of the adjoint objective as a sum of two expectations under $\\mathbb{P}$ and $\\mathbb{Q}$ given the optimal prediction function ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{L_{a d j}(\\omega,a)=\\frac{1}{2}\\,\\mathbb{E}_{(x,y)\\sim\\mathbb{P}}\\left[a(x)^{\\top}\\partial_{v}^{2}\\ell_{i n}\\left(\\omega,h_{\\omega}^{\\star}(x),x,y\\right)a(x)\\right]\\,}\\\\ {+\\,\\mathbb{E}_{(x,y)\\sim\\mathbb{Q}}\\left[a(x)^{\\top}\\partial_{v}\\ell_{o u t}\\left(\\omega,h_{\\omega}^{\\star}(x),x,y\\right)\\right].\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Furthermore, the vectors $g_{\\omega}$ and $B_{\\omega}a_{\\omega}^{\\star}$ appearing in Equation (45) can also be expressed as expectations: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{c}{g_{\\omega}=\\mathbb{E}_{(x,y)\\sim\\mathbb{Q}}\\left[\\partial_{\\omega}\\ell_{o u t}\\left(\\omega,h_{\\omega}^{\\star}(x),x,y\\right)\\right]}\\\\ {B_{\\omega}a_{\\omega}^{\\star}=\\mathbb{E}_{(x,y)\\sim\\mathbb{P}}\\left[\\partial_{\\omega,v}\\ell_{i n}\\left(\\omega,h_{\\omega}^{\\star}(x),x,y\\right)a_{\\omega}^{\\star}(x)\\right].}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "E Convergence Analysis ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We provide a convergence result of Algorithm 1 to stationary points of $\\mathcal{F}$ . Our analysis uses the framework of biased stochastic gradient descent (Biased SGD) [Demidovich et al., 2024] where the bias arises from suboptimality errors when solving the inner-level and adjoint problems. ", "page_idx": 29}, {"type": "text", "text": "E.1 Setup and assumptions ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Gradient estimators. Recall that the total gradient $\\nabla{\\mathcal{F}}(\\omega)$ admits the following expression under Assumptions (A) to (L): ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\nabla\\mathcal{F}(\\omega)=\\mathbb{E}_{\\mathbb{Q}}\\left[\\partial_{\\omega}\\ell_{o u t}\\left(\\omega,h_{\\omega}^{\\star}(x),x,y\\right)\\right]+\\mathbb{E}_{\\mathbb{P}}\\left[\\partial_{\\omega,v}\\ell_{i n}\\left(\\omega,h_{\\omega}^{\\star}(x),x,y\\right)a_{\\omega}^{\\star}(x)\\right],\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We denote by $\\hat{g}(\\omega)$ the gradient estimator, i.e., the mapping $\\hat{g}:\\Omega\\to\\Omega$ computed by Algorithm 4 and which admits the following expression: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\dot{\\gamma}(\\omega)=\\frac{1}{|B_{o u t}|}\\sum_{(\\tilde{x},\\tilde{y})\\in B_{o u t}}\\partial_{\\omega}\\ell_{o u t}\\left(\\omega,\\hat{h}_{\\omega}(\\tilde{x}),\\tilde{x},\\tilde{y}\\right)+\\frac{1}{|B_{i n}|}\\sum_{(x,y)\\in B_{i n}}\\partial_{\\omega,v}\\ell_{i n}\\left(\\omega,\\hat{h}_{\\omega}(x),x,y\\right)\\hat{a}_{\\omega}(x),\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $B_{i n}$ and $\\boldsymbol{B_{o u t}}$ are samples from $\\mathbb{P}$ and $\\mathbb{Q}$ independent from $\\hat{h}_{\\omega}$ and $\\hat{a}_{\\omega}$ and independent from each other (i.e. $\\boldsymbol{B}_{i n}\\,\\perp\\,\\boldsymbol{B}_{o u t})$ . Here, there are three independent sources of randomness when computing $\\hat{g}(\\omega)$ : estimation of $\\hat{h}_{\\omega}$ and $\\hat{a}_{\\omega}$ in Algorithms 2 and 3, as well as random batches $\\boldsymbol{B}_{i n}$ and $\\boldsymbol{B_{o u t}}$ . We denote by $\\mathbb{E}[\\cdot]$ the expectation with respect to all random variables appearing in the expression of $\\hat{g}(\\omega)$ , and by $\\mathbb{E}[\\cdot|\\hat{h}_{\\omega}]$ and $\\mathbb{E}[\\cdot|\\hat{h}_{\\omega},\\hat{a}_{\\omega}]$ the conditional expectations knowing $\\hat{h}_{\\omega}$ only and both $\\hat{h}_{\\omega}$ and $\\hat{a}_{\\omega}$ . $\\hat{g}(\\omega)$ is a biased estimator of $\\nabla{\\mathcal{F}}(\\omega)$ (i.e., $\\mathbb{E}[{\\hat{g}}(\\omega)]$ is not equal to $\\nabla\\mathcal{F}(\\omega))$ , as the bias is due to using sub-optimal solutions $\\hat{h}_{\\omega}$ and $\\hat{a}_{\\omega}$ instead of $h_{\\omega}^{\\star}$ and $a_{\\omega}^{\\star}$ in the expression of $\\hat{g}(\\omega)$ . Furthermore, we define $G(\\omega):=\\mathbb{E}\\left[\\hat{g}(\\omega)|\\hat{h}_{\\omega},\\hat{a}_{\\omega}\\right]$ to be the conditional expectation of $\\hat{g}(\\omega)$ at $\\omega$ given estimates $\\hat{h}_{\\omega}$ and $\\hat{a}_{\\omega}$ of $h_{\\omega}^{\\star}$ and $a_{\\omega}^{\\star}$ . By independence of $B_{i n}$ and $\\boldsymbol{B_{o u t}}$ from $\\hat{h}_{\\omega}$ and $\\hat{a}_{\\omega}$ , $G(\\omega)$ admits the following expression: ", "page_idx": 30}, {"type": "equation", "text": "$$\nG(\\omega)=\\mathbb{E}_{\\mathbb{Q}}\\left[\\partial_{\\omega}\\ell_{o u t}\\left(\\omega,\\hat{h}_{\\omega}(x),x,y\\right)\\right]+\\mathbb{E}_{\\mathbb{P}}\\left[\\partial_{\\omega,v}\\ell_{i n}\\left(\\omega,\\hat{h}_{\\omega}(x),x,y\\right)\\hat{a}_{\\omega}(x)\\right],\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the expectation is taken w.r.t. $(x,y)\\sim\\mathbb{Q}$ . ", "page_idx": 30}, {"type": "text", "text": "Approximate adjoint objective. ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "We introduce the approximate adjoint objective $\\tilde{L}_{a d j}(\\omega,a)$ where $h_{\\omega}^{\\star}$ is replaced by $\\hat{h}_{\\omega}$ : ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\tilde{L}_{a d j}(\\omega,a)=\\frac{1}{2}a^{\\top}\\partial_{h}^{2}L_{i n}(\\omega,\\hat{h}_{\\omega})a+a^{T}\\partial_{h}L_{o u t}(\\omega,\\hat{h}_{\\omega}).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "By independence of the estimator $\\hat{h}$ and the samples $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ used for computing $\\hat{L}_{a d j}(\\omega,a,\\hat{h}_{\\omega},B)$ it is easy to see that $\\mathbb{E}\\left[\\hat{L}_{a d j}(\\omega,a,\\hat{h}_{\\omega},\\mathcal{B})\\Big|\\hat{h}_{\\omega}\\right]=\\tilde{L}_{a d j}(\\omega,a)$ . Hence, it is natural to think of $\\hat{a}_{\\omega}$ as an approximation to the minimizer $\\tilde{a}_{\\omega}$ of $\\tilde{L}_{a d j}(\\omega,a)$ in $\\mathcal{H}$ . ", "page_idx": 30}, {"type": "text", "text": "Sub-optimality assumption. ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "The following assumption quantifies the sub-optimality errors made by $\\hat{h}_{\\omega}$ and $\\hat{a}_{\\omega}$ . ", "page_idx": 30}, {"type": "text", "text": "(a) For some positive constants $\\epsilon_{i n}$ and $\\epsilon_{a d j}$ and for all $\\omega\\in\\Omega,\\hat{h}_{\\omega}$ and $\\hat{a}_{\\omega}$ satisfy: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[L_{i n}(\\omega,\\hat{h}_{\\omega})-L_{i n}(\\omega,h_{\\omega}^{\\star})\\right]\\leq\\epsilon_{i n},\\qquad\\mathbb{E}\\left[\\tilde{L}_{a d j}(\\omega,\\hat{a}_{\\omega})-\\tilde{L}_{a d j}(\\omega,\\tilde{a}_{w})\\right]\\leq\\epsilon_{a d j}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Assumptions on $\\ell_{i n}$ . ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "(b) (Strong convexity) $\\ell_{i n}(\\omega,v,x,y)$ is $\\mu$ -strongly convex in $v\\in\\mathcal{V}$ . ", "page_idx": 30}, {"type": "text", "text": "(c) $v\\mapsto\\partial_{v}^{2}\\ell_{i n}(\\omega,v,x,y)$ is $C_{1}$ -Lipschitz on $\\Omega\\times\\mathcal{X}\\times\\mathcal{Y}$ .   \n(d) $v\\mapsto\\partial_{\\omega,v}\\ell_{i n}(\\omega,v,x,y)$ is differentiable and $C_{2}$ -Lipschitz and bounded by $B_{2}\\in\\mathbb{R}$ . ", "page_idx": 30}, {"type": "text", "text": "Assumptions on $\\ell_{o u t}$ . ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "(e) $\\|\\partial_{h}L_{o u t}\\left(\\omega,h_{\\omega}^{\\star}\\right)\\|_{\\mathcal{H}}$ is bounded by a positive constant $B_{3}$ .   \n(f) $v\\mapsto\\partial_{v}\\ell_{o u t}(\\omega,v,x,y)$ is $C_{4}$ -Lipschitz for all $(\\omega,x,y)\\in\\Omega\\times\\mathcal{X}\\times\\mathcal{Y}$ .   \n(g) $v\\mapsto\\partial_{\\omega}\\ell_{o u t}(\\omega,v,x,y)$ is differentiable and $C_{3}$ -Lipschitz for all $(\\omega,x,y)\\in\\Omega\\times\\mathcal{X}\\times\\mathcal{Y}$ . ", "page_idx": 30}, {"type": "text", "text": "(h) $(x,y)\\mapsto\\partial_{\\omega}\\ell_{o u t}(\\omega,h_{\\omega}^{\\star}(x),x,y)$ has a variance bounded by $\\sigma_{o u t}^{2}$ for all $\\omega\\in{\\Omega}$ ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Smoothness of the total objective. ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Remark E.1. Assumption (a) reflects the generalization errors made when optimizing $L_{i n}$ and $\\tilde{L}_{a d j}$ using Algorithms 2 and 3. In the case of over-parameterized networks, these errors can be made smaller by increasing network capacity, number of steps and the number of samples [Allen-Zhu et al., 2019, Du et al., 2019, Zou et al., 2020]. ", "page_idx": 31}, {"type": "text", "text": "Remark E.2. Assumptions (b) to $(\\mathbf{h})$ are similar in spirit to those used for analyzing bi-level optimization algorithms (ex: [Arbel and Mairal, 2022a, Assumptions 1 to 5]). In particular, Assumption (e) is even weaker than [Arbel and Mairal, 2022a, Assumptions 2] where $\\partial_{h}L_{o u t}(\\omega,h)$ needs to be bounded for all $\\omega,h$ in $\\Omega\\times\\mathcal{H}$ . For instance, Assumption (e) trivially holds when $\\partial_{h}L_{o u t}(\\omega,h)$ is a linear transformation of $\\partial_{h}L_{i n}(\\omega,h)$ , as is the case for min-max problems. There $\\partial_{h}L_{i n}(\\omega,h_{\\omega}^{\\star})=0$ so that $\\partial_{h}{\\cal L}_{o u t}(\\omega,h_{\\omega}^{\\star})=0$ is bounded, while $\\partial_{h}L_{o u t}(\\omega,h)$ might not be bounded in general. ", "page_idx": 31}, {"type": "text", "text": "E.2 Proof of the main result ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "The general strategy of the proof is to first show that the conditions for applying the general convergence result for biased SGD of Demidovich et al. [2024, Theorem 3] hold. We start with Proposition E.3 which shows that the biased gradient estimate $\\hat{g}(\\omega)$ satisfies the conditions of Demidovich et al. [2024, Assumption 9]. ", "page_idx": 31}, {"type": "text", "text": "Proposition E.3. Let Assumptions (a) to $(h)$ ) and Assumptions (A) to $(L)$ hold. ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla\\mathcal{F}(\\omega)^{\\top}\\mathbb{E}[\\hat{g}(\\omega)]\\geq\\displaystyle\\frac{1}{2}\\left\\|\\nabla\\mathcal{F}(\\omega)\\right\\|^{2}-\\displaystyle\\frac{1}{2}\\left(c_{1}\\epsilon_{i n}+c_{2}\\epsilon_{o u t}\\right)}\\\\ &{\\qquad\\mathbb{E}\\left[\\left\\|\\hat{g}(\\omega)\\right\\|^{2}\\right]\\leq\\underbrace{\\sigma_{0}^{2}+2\\left(c_{1}\\epsilon_{i n}+c_{2}\\epsilon_{a d j}\\right)}_{\\sigma_{e f f}^{2}}+2\\left\\|\\nabla\\mathcal{F}(\\omega)\\right\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $c_{1}$ and $c_{2}$ are constants defined in Equation (50) and $\\sigma_{0}$ is a positive constant defined in Equation (51). ", "page_idx": 31}, {"type": "text", "text": "Proof. We prove each bound separately. ", "page_idx": 31}, {"type": "text", "text": "Lower-bound on $\\nabla\\mathcal{F}(\\omega)^{\\top}\\mathbb{E}[\\hat{g}(\\omega)]$ . Fist note that $\\mathbb{E}[\\hat{g}(\\omega)]=\\mathbb{E}[G(\\omega)]$ . Hence, by direct calculation, we have that: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla\\mathcal{F}(\\omega)^{\\top}{\\mathbb{E}}[\\hat{g}(\\omega)]=\\nabla\\mathcal{F}(\\omega)^{\\top}{\\mathbb{E}}[G(\\omega)]}\\\\ &{\\qquad\\qquad\\qquad=\\frac{1}{2}\\left(\\|\\nabla\\mathcal{F}(\\omega)\\|^{2}+{\\mathbb{E}}\\left[\\left\\|G(\\omega)\\right\\|^{2}\\right]\\right)-\\frac{1}{2}{\\mathbb{E}}\\left[\\left\\|G(\\omega)-\\nabla\\mathcal{F}(\\omega)\\right\\|^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\geq\\frac{1}{2}\\left\\|\\nabla\\mathcal{F}(\\omega)\\right\\|^{2}-\\frac{1}{2}\\left(c_{1}\\epsilon_{i n}+c_{2}\\epsilon_{o u t}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where we use Lemma E.4 to get the last lower-bound. ", "page_idx": 31}, {"type": "text", "text": "Upper-bound on $\\mathbb{E}[\\|\\hat{g}(\\omega)\\|^{2}]$ . ", "text_level": 1, "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\Vert\\hat{g}(\\omega)\\right\\Vert^{2}\\right]=\\mathbb{E}\\left[\\left\\Vert\\hat{g}(\\omega)-G(\\omega)\\right\\Vert^{2}\\right]+\\mathbb{E}\\left[\\left\\Vert G(\\omega)-\\nabla\\mathcal{F}(\\omega)+\\nabla\\mathcal{F}(\\omega)\\right\\Vert^{2}\\right]}\\\\ &{\\qquad\\qquad\\leq\\mathbb{E}\\left[\\left\\Vert\\hat{g}(\\omega)-G(\\omega)\\right\\Vert^{2}\\right]+2\\mathbb{E}\\left[\\left\\Vert G(\\omega)-\\nabla\\mathcal{F}(\\omega)\\right\\Vert^{2}\\right]+2\\left\\Vert\\nabla\\mathcal{F}(\\omega)\\right\\Vert^{2}}\\\\ &{\\qquad\\qquad\\leq\\sigma_{0}^{2}+2\\left(c_{1}\\epsilon_{i n}+c_{2}\\epsilon_{a d j}\\right)+2\\left\\Vert\\nabla\\mathcal{F}(\\omega)\\right\\Vert^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where the first line uses that $\\mathbb{E}[\\hat{g}(\\omega)]=\\mathbb{E}[G(\\omega)]$ and the last line uses Lemmas E.4 and E.5. ", "page_idx": 31}, {"type": "text", "text": "We can now directly use Proposition E.3 and Assumption (i) on $\\mathcal{F}$ to prove the Theorem 3.1 using the biased SGD convergence result in [Demidovich et al., 2024, Theorem 3]. ", "page_idx": 31}, {"type": "text", "text": "Proof of Theorem 3.1. The proof is a direct application of [Demidovich et al., 2024, Theorem 3] given that the variance and bias conditions on the estimator $\\hat{g}(\\omega)$ are satisfied by Proposition E.3 and that $\\mathcal{F}$ is $\\mathcal{L}$ -smooth and has a finite lower-bound $\\mathcal{F}^{\\star}\\in\\mathbb{R}$ by Assumption (i). \u53e3 ", "page_idx": 31}, {"type": "text", "text": "E.3 Bias-variance decomposition. ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Lemma E.4 (Bias control). Let Assumptions (a) to $({\\pmb g})$ and Assumptions (A) to $(L)$ hold. ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\left\\|G(\\omega)-\\nabla\\mathcal{F}(\\omega)\\right\\|^{2}\\right]\\leq c_{1}\\epsilon_{i n}+c_{2}\\epsilon_{a d j},}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $c_{1}$ and $c_{2}$ are non-negative constants defined in Equation (50). ", "page_idx": 32}, {"type": "text", "text": "Proof. ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{G(\\omega)-\\nabla\\mathcal{F}(\\omega)=\\underbrace{\\mathbb{E}_{\\mathbb{Q}}\\left[\\partial_{\\omega}\\ell_{o u t}\\left(\\omega,\\hat{h}_{\\omega}(x),x,y\\right)-\\partial_{\\omega}\\ell_{o u t}\\left(\\omega,h_{\\omega}^{\\star}(x),x,y\\right)\\right]}_{A_{1}}}\\\\ &{\\quad+\\underbrace{\\mathbb{E}_{\\mathbb{P}}\\left[\\partial_{\\omega,v}\\ell_{i n}\\left(\\omega,\\hat{h}_{\\omega}(x),x,y\\right)\\hat{a}_{\\omega}(x)-\\partial_{\\omega,v}\\ell_{i n}\\left(\\omega,h_{\\omega}^{\\star}(x),x,y\\right)a_{\\omega}^{\\star}(x)\\right]}_{A_{1}}}\\\\ &{=A_{1}+\\underbrace{\\mathbb{E}_{\\mathbb{P}}\\left[\\left(\\partial_{\\omega,v}\\ell_{i n}\\left(\\omega,\\hat{h}_{\\omega}(x),x,y\\right)-\\partial_{\\omega,v}\\ell_{i n}\\left(\\omega,h_{\\omega}^{\\star}(x),x,y\\right)\\right)a_{\\omega}^{\\star}(x)\\right]}_{A_{2}}}\\\\ &{\\quad+\\underbrace{\\mathbb{E}_{\\mathbb{P}}\\left[\\partial_{\\omega,v}\\ell_{i n}\\left(\\omega,\\hat{h}_{\\omega}(x),x,y\\right)\\left(\\hat{a}_{\\omega}(x)-a_{\\omega}^{\\star}(x)\\right)\\right]}_{A_{3}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We have the following: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|A_{1}\\|\\leq\\!M C_{3}\\left\\|\\hat{h}_{\\omega}-h_{\\omega}^{\\star}\\right\\|_{\\mathcal{H}}}\\\\ &{\\|A_{2}\\|\\leq\\!C_{2}\\left\\|a_{\\omega}^{\\star}\\right\\|_{\\mathcal{H}}\\left\\|\\hat{h}_{\\omega}-h_{\\omega}^{\\star}\\right\\|_{\\mathcal{H}}\\leq C_{2}B_{3}\\mu^{-1}\\left\\|\\hat{h}_{\\omega}-h_{\\omega}^{\\star}\\right\\|_{\\mathcal{H}}}\\\\ &{\\|A_{3}\\|\\leq\\!B_{2}\\mathbb{E}_{\\mathbb{P}}\\left[\\|\\hat{a}_{\\omega}(x)-a_{\\omega}^{\\star}(x)\\|\\right]}\\\\ &{\\qquad\\leq\\!B_{2}\\left(\\mathbb{E}_{\\mathbb{P}}\\left[\\|\\hat{a}_{\\omega}(x)-\\tilde{a}_{\\omega}(x)\\|\\right]+\\mathbb{E}_{\\mathbb{P}}\\left[\\|a_{\\omega}^{\\star}(x)-\\tilde{a}_{\\omega}(x)\\|\\right]\\right)}\\\\ &{\\qquad\\leq\\!B_{2}\\left(\\left\\|\\hat{a}_{\\omega}-\\tilde{a}_{\\omega}\\right\\|_{\\mathcal{H}}+\\left\\|\\tilde{a}_{\\omega}-a_{\\omega}^{\\star}\\right\\|_{L_{1}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "The first inequality holds since $v\\,\\mapsto\\,\\partial_{\\omega}\\ell_{o u t}(\\omega,v,x,y)$ is $C_{3}$ -Lipschitz by Assumption $\\bf(g)$ and $\\mathrm{d}\\mathbb{Q}(\\cdot,\\mathcal{V})$ admits a density w.r.t $\\Phi(\\cdot,\\mathcal{V})$ bounded by a positive constant $M$ by Assumption (B). The second inequality holds since $v\\mapsto\\partial_{\\omega,v}\\ell_{i n}(\\omega,v,x,y)$ is $C_{2}$ -Lipschitz by Assumption (d) and $\\|a_{\\omega}^{\\star}\\|_{\\mathcal{H}}$ is upper-bounded by $\\mu^{-1}B_{3}$ as a consequence of Lemma E.6. Finally, the inequality on $\\left\\Vert A_{3}\\right\\Vert$ holds since $\\partial_{\\omega,v}\\ell_{i n}(\\omega,v,x,y)$ is bounded by a constant $B_{2}$ by Assumption (d). Therefore, it holds that the difference between $G(\\omega)$ and $\\nabla{\\mathcal{F}}$ satisfies: ", "page_idx": 32}, {"type": "equation", "text": "$$\nG(\\omega)-\\nabla\\mathcal{F}(\\omega)\\|^{2}\\leq3\\left(M C_{3}+\\frac{C_{2}B_{3}}{\\mu}\\right)^{2}\\left\\|\\hat{h}_{\\omega}-h_{\\omega}^{\\star}\\right\\|_{\\mathcal{H}}^{2}+3B_{2}^{2}\\left\\|\\hat{a}_{\\omega}-\\tilde{a}_{\\omega}\\right\\|_{\\mathcal{H}}^{2}+3B_{2}^{2}\\left\\|\\tilde{a}_{\\omega}-a_{\\omega}^{\\star}\\right\\|_{L_{1}}^{2}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Taking the expectation over $\\hat{h}_{\\omega}$ and $\\hat{a}_{\\omega}$ and using the bounds in Lemma E.6 yields: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|G(\\omega)-\\nabla\\mathcal{F}(\\omega)\\right\\|^{2}\\right]\\leq6\\left(M C_{3}+C_{2}B_{3}\\mu^{-1}\\right)^{2}\\mu^{-1}\\epsilon_{i n}+6B_{2}^{2}\\mu^{-1}\\epsilon_{a d j}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,6B_{2}^{2}\\mu^{-1}\\left(\\mu^{-1}C_{4}M+\\mu^{-2}C_{1}B_{3}\\right)^{2}\\epsilon_{i n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Finally, the upper bound on the bias holds with $c_{1}$ and $c_{2}$ defined as: ", "page_idx": 32}, {"type": "equation", "text": "$$\nc_{1}:=6\\left(M C_{3}+C_{2}B_{3}\\mu^{-1}\\right)^{2}\\mu^{-1}+6B_{2}^{2}\\mu^{-1}\\left(\\mu^{-1}C_{4}M+\\mu^{-2}B_{3}C_{1}\\right)^{2},\\qquad c_{2}=6B_{2}^{2}\\mu^{-1}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Lemma E.5 (Variance control). Let Assumptions (a) to $(h)$ and Assumptions (A) to $(L)$ hold, then the variance is upper-bounded as follows: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\lVert\\hat{g}(\\omega)-G(\\omega)\\right\\rVert^{2}\\right]\\leq\\sigma_{0}^{2},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $\\sigma_{0}$ is a positive constant given by: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\sigma_{0}^{2}:=\\frac{2}{\\vert\\mathcal{B}_{o u t}\\vert}\\left(2C_{3}^{2}\\mu^{-1}\\epsilon_{i n}+\\sigma_{o u t}^{2}\\right)+\\frac{4B_{2}^{2}}{\\vert\\mathcal{B}_{i n}\\vert}\\left(\\mu^{-1}\\epsilon_{a d j}+2\\mu^{-3}C_{4}^{2}M^{2}\\epsilon_{i n}+\\mu^{-2}B_{3}^{2}\\right)\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof. By definition of $\\hat{g}(\\omega)$ and $G(\\omega)$ we have that: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left[\\left\\|\\hat{g}(\\omega)-G(\\omega)\\right\\|^{2}\\right]=\\frac{1}{|B_{o u t}|}\\underbrace{\\mathbb{E}\\left[\\left\\|\\partial_{\\omega}\\ell_{o u t}\\left(\\omega,\\hat{h}_{\\omega}(x),x,y\\right)-\\mathbb{E}_{\\mathbb{Q}}\\left[\\partial_{\\omega}\\ell_{o u t}\\left(\\omega,\\hat{h}_{\\omega}(x^{\\prime}),x^{\\prime},y^{\\prime}\\right)\\right|\\hat{h}_{\\omega}\\right]\\right.}_{V_{1}}}\\\\ &{\\left.+\\left.\\frac{1}{|B_{i n}|}\\underbrace{\\mathbb{E}\\left[\\left\\|\\partial_{\\omega,v}\\ell_{i n}\\left(\\omega,\\hat{h}_{\\omega}(x),x,y\\right)\\hat{a}_{\\omega}(x)\\right\\|^{2}\\right]}_{V_{2}}\\right.}\\\\ &{\\left.\\,-\\left.\\frac{1}{|B_{i n}|}\\underbrace{\\mathbb{E}\\left[\\left\\|\\mathbb{E}_{\\mathbb{P}}\\left[\\partial_{\\omega,v}\\ell_{i n}\\left(\\omega,\\hat{h}_{\\omega}(x),x,y\\right)\\hat{a}_{\\omega}(x)\\right|\\hat{h}_{\\omega},\\hat{a}_{\\omega}\\right]\\right\\|^{2}\\right]}_{V_{3}}}\\\\ &{\\leq\\frac{V_{1}}{|B_{o u t}|}+\\frac{V_{2}}{|B_{i n}|}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Where the first line is a direct consequence of the independence of $B_{i n}$ and $\\boldsymbol{B_{o u t}}$ . Moreover, we can bound $V_{1}$ and $V_{2}$ as follows: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{1}\\leq\\!2\\mathbb{E}\\left[\\left\\|\\partial_{\\omega}\\ell_{o u t}\\left(\\omega,\\hat{h}_{\\omega}(x),x,y\\right)-\\partial_{\\omega}\\ell_{o u t}\\left(\\omega,h_{\\omega}^{\\star}(x),x,y\\right)\\right\\|^{2}\\right]}\\\\ &{\\qquad+2\\mathbb{E}_{\\mathbb{Q}}\\left[\\|\\partial_{\\omega}\\ell_{o u t}\\left(\\omega,h_{\\omega}^{\\star}(x),x,y\\right)-\\mathbb{E}_{\\mathbb{Q}}\\left[\\partial_{\\omega}\\ell_{o u t}\\left(\\omega,h_{\\omega}^{\\star}(x^{\\prime}),x^{\\prime},y^{\\prime}\\right)\\right]\\|^{2}\\right]}\\\\ &{\\qquad\\leq\\!2C_{3}^{2}\\mathbb{E}\\left[\\left\\|\\hat{h}_{\\omega}-h_{\\omega}^{\\star}\\right\\|^{2}\\right]+2\\sigma_{o u t}^{2}\\leq4C_{3}^{2}\\mu^{-1}\\epsilon_{i n}+2\\sigma_{o u t}^{2}}\\\\ &{V_{2}\\leq\\!B_{2}^{2}\\mathbb{E}\\left[\\left\\|\\hat{a}_{\\omega}\\right\\|_{\\mathcal{H}}^{2}\\right]\\leq2B_{2}^{2}\\left(\\mathbb{E}\\left[\\left\\|\\hat{a}_{\\omega}-\\tilde{a}_{\\omega}\\right\\|_{\\mathcal{H}}^{2}\\right]+\\mathbb{E}\\left[\\left\\|\\tilde{a}_{\\omega}\\right\\|_{\\mathcal{H}}^{2}\\right]\\right)}\\\\ &{\\qquad\\leq\\!4B_{2}^{2}\\left(\\mu^{-1}\\epsilon_{a d j}+2\\mu^{-3}C_{4}^{2}M^{2}\\epsilon_{i n}+\\mu^{-2}B_{3}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "For $V_{1}$ , we used that $v\\mapsto\\partial_{\\omega}\\ell_{o u t}(\\omega,v,x,y)$ is $C_{3}$ -Lipschitz uniformly in $\\omega,\\,x$ and $y$ by Assumption $\\bf(g)$ , and that $(x,y)\\mapsto\\partial_{\\omega}\\ell_{o u t}(\\omega,h_{\\omega}^{\\star}(x),x,y)$ has a variance uniformly bounded by $\\sigma_{o u t}^{2}$ as a consequence of Assumption (h). For $V_{2}$ , we use Assumption (d) where we have that $\\partial_{\\omega,v}\\ell_{i n}$ is uniformly bounded by a constant $B_{2}$ and apply the bounds on $\\mathbb{E}\\left[\\left\\|\\tilde{a}_{\\omega}\\right\\|_{\\mathcal{H}}^{2}\\right]$ and $\\mathbb{E}\\left[\\|\\hat{a}_{\\omega}-\\tilde{a}_{\\omega}\\|_{\\mathcal{H}}^{2}\\right]$ from Lemma E.6. \u53e3 ", "page_idx": 33}, {"type": "text", "text": "Lemma E.6. For the optimal prediction and adjoint functions $h_{\\omega}^{\\star},a_{\\omega}^{\\star}$ defined in Equations (6) and (7), as well as their estimated versions $\\hat{h}_{\\omega},\\hat{a}_{\\omega}$ given in Algorithm 1. Under Assumptions (a) to $(f)$ and Assumptions (A) to $(L)$ , the following holds: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|\\hat{h}_{\\omega}-h_{\\omega}^{\\star}\\right\\|_{\\mathcal{H}}^{2}\\right]\\leq2\\mu^{-1}\\epsilon_{i n},\\qquad\\mathbb{E}\\left[\\left\\|\\hat{a}_{\\omega}-\\tilde{a}_{\\omega}\\right\\|_{\\mathcal{H}}^{2}\\right]\\leq2\\mu^{-1}\\epsilon_{a d j},}\\\\ &{\\qquad\\mathbb{E}\\left[\\left\\|\\tilde{a}_{\\omega}-a_{\\omega}^{\\star}\\right\\|_{L_{1}}^{2}\\right]\\leq2\\left(\\mu^{-2}C_{1}B_{3}+\\mu^{-1}C_{4}M\\right)^{2}\\mu^{-1}\\epsilon_{i n},}\\\\ &{\\mathbb{E}\\left[\\left\\|\\tilde{a}_{\\omega}\\right\\|_{\\mathcal{H}}^{2}\\right]\\leq4\\mu^{-3}C_{4}^{2}M^{2}\\epsilon_{i n}+2\\mu^{-2}B_{3}^{2},\\qquad\\|a_{\\omega}^{\\star}\\|_{\\mathcal{H}}\\leq\\mu^{-1}B_{3}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proof. We show each of the upper bounds separately. ", "page_idx": 33}, {"type": "text", "text": "Upper-bound on $\\mathbb{E}\\left[\\left\\|\\hat{h}_{\\omega}-h_{\\omega}^{\\star}\\right\\|_{\\mathcal{H}}^{2}\\right]$ . We use the strong-convexity Assumption (b) and Assumption (a) to show the first bound: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\left\\|\\hat{h}_{\\omega}-h_{\\omega}^{\\star}\\right\\|_{\\mathcal{H}}^{2}\\right]\\leq\\mathbb{E}\\Big[\\frac{2}{\\mu}\\left(L_{i n}(\\omega,\\hat{h}_{\\omega})-L_{i n}(\\omega,h_{\\omega}^{\\star})\\right)\\Big]\\leq\\frac{2\\epsilon_{i n}}{\\mu}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Upper-bound on $\\mathbb{E}\\left[\\|\\hat{a}_{\\omega}-\\tilde{a}_{\\omega}\\|_{\\mathcal{H}}^{2}\\right]$ . The second bound can be proven in the same way as the first, using that, by definition, $\\tilde{L}_{a d j}$ is continuous and $\\mu-$ strongly convex in $a$ together with Assumption (a). ", "page_idx": 33}, {"type": "text", "text": "Upper-bound on $\\mathbb{E}\\left[\\left\\lVert\\tilde{a}_{w}-a_{\\omega}^{\\star}\\right\\rVert_{L_{1}}^{2}\\right]$ . We exploit the closed form expressions of $\\tilde{a}_{\\omega}$ and $a_{\\omega}^{\\star}$ : ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}{\\mathrm{i}_{\\omega}=-\\left(\\underbrace{\\partial_{h}^{2}L_{i n}\\left(\\omega,\\hat{h}_{\\omega}\\right)}_{H_{\\omega}}\\right)^{-1}\\underbrace{\\partial_{h}L_{o u t}\\left(\\omega,\\hat{h}_{\\omega}\\right)}_{\\hat{b}_{\\omega}},}&{}&{a_{\\omega}^{\\star}}&{=-\\left(\\underbrace{\\partial_{h}^{2}L_{i n}\\left(\\omega,h_{\\omega}^{\\star}\\right)}_{H_{\\omega}}\\right)^{-1}\\underbrace{\\partial_{h}L_{o u t}\\left(\\omega,h_{\\omega}^{\\star}\\right)}_{b_{\\omega}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "By standard linear algebra, the difference $\\tilde{a}_{\\omega}-a_{\\omega}^{\\star}$ can be expressed as: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\boldsymbol{\\tilde{a}}\\omega-\\boldsymbol{a}_{\\omega}^{\\star}=H_{\\omega}^{-1}\\left(\\boldsymbol{\\hat{H}}_{\\omega}-H_{\\omega}\\right)\\boldsymbol{\\hat{H}}_{\\omega}^{-1}\\boldsymbol{b}_{\\omega}+\\boldsymbol{\\hat{H}}_{\\omega}^{-1}\\left(\\boldsymbol{b}_{\\omega}-\\boldsymbol{\\hat{b}}_{\\omega}\\right).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "By taking the $L_{1}(\\mathbb{P})$ norm of the above and using the upper-bounds from Lemma E.7, we can write: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\|{\\tilde{a}}_{\\omega}-{a}_{\\omega}^{\\star}\\right\\|_{L_{1}}\\leq\\left\\|{H}_{\\omega}^{-1}\\left({\\hat{H}}_{\\omega}-{H}_{\\omega}\\right){\\hat{H}}_{\\omega}^{-1}{b}_{\\omega}\\right\\|_{L_{1}}+\\left\\|{\\hat{H}}_{\\omega}^{-1}\\left({b}_{\\omega}-{\\hat{b}}_{\\omega}\\right)\\right\\|_{L_{1}}}&{}\\\\ {\\leq\\mu^{-1}\\left\\|\\left({\\hat{H}}_{\\omega}-{H}_{\\omega}\\right){\\hat{H}}_{\\omega}^{-1}{b}_{\\omega}\\right\\|_{L_{1}}+\\mu^{-1}\\left\\|{\\hat{b}}_{\\omega}-{\\hat{b}}_{\\omega}\\right\\|_{L_{1}}}&{}\\\\ {\\leq\\mu^{-1}C_{1}\\left\\|{\\hat{h}}_{\\omega}-{h}_{\\omega}^{\\star}\\right\\|_{\\mathcal{H}}\\left\\|{\\hat{H}}_{\\omega}^{-1}{b}_{\\omega}\\right\\|_{\\mathcal{H}}+\\mu^{-1}\\left\\|{b}_{\\omega}-{\\hat{b}}_{\\omega}\\right\\|_{\\mathcal{H}}}&{}\\\\ {\\leq\\mu^{-2}C_{1}\\left\\|{\\hat{h}}_{\\omega}-{h}_{\\omega}^{\\star}\\right\\|_{\\mathcal{H}}\\left\\|{b}_{\\omega}\\right\\|_{\\mathcal{H}}+\\mu^{-1}C_{4}M\\left\\|{\\hat{h}}_{\\omega}-{h}_{\\omega}^{\\star}\\right\\|_{\\mathcal{H}}}&{}\\\\ {\\leq\\left(\\mu^{-2}C_{1}{B}_{3}+\\mu^{-1}C_{4}M\\right)\\left\\|{\\hat{h}}_{\\omega}-{h}_{\\omega}^{\\star}\\right\\|_{\\mathcal{H}}.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "The first line we used the triangular inequality, the second line follows from Equation (53) from Lemma E.7. The third line applies Equation (54) from Lemma E.7 to the first term and uses that $\\left\\|b_{\\omega}-\\hat{b}_{\\omega}\\right\\|_{L_{1}}\\;\\leq\\;\\left\\|b_{\\omega}-\\hat{b_{\\omega}}\\right\\|_{\\mathcal{H}}$ by Cauchy-Schwarz inequality. The fourth line uses that $\\left\\|\\hat{H}_{\\omega}^{-1}b_{\\omega}\\right\\|_{\\mathcal{H}}\\leq\\mu^{-1}\\left\\|b_{\\omega}\\right\\|_{\\mathcal{H}}$ for the first term since $\\left\\|\\hat{H}_{\\omega}^{-1}\\right\\|_{o p}\\leq\\mu^{-1}$ by Assumption (b) and uses Equation (55) from Lemma E.7 for the second term. The final bound is obtained using Assumption (e) to upper-bound $\\|b_{\\omega}\\|_{\\mathcal{H}}$ by $B_{3}$ . By taking the expectation w.r.t. $\\hat{h}_{\\omega}$ and using Equation (52), we get: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|\\tilde{a}_{\\omega}-a_{\\omega}^{\\star}\\right\\|_{L_{1}}^{2}\\right]\\leq\\left(\\mu^{-2}C_{1}B_{3}+\\mu^{-1}C_{4}M\\right)^{2}\\mathbb{E}\\left[\\left\\|\\hat{h}_{\\omega}-h_{\\omega}^{\\star}\\right\\|_{\\mathcal{H}}^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leq2\\left(\\mu^{-2}C_{1}B_{3}+\\mu^{-1}C_{4}M\\right)^{2}\\mu^{-1}\\epsilon_{i n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Upper-bound on $\\mathbb{E}\\left[\\left\\|\\tilde{a}_{\\omega}\\right\\|_{\\mathcal{H}}^{2}\\right]$ . We use the closed-form expression of $\\tilde{a}_{\\omega}$ : ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\tilde{a}_{\\omega}\\right\\|_{\\mathcal{H}}=\\left\\|\\hat{H}_{\\omega}^{-1}\\hat{b}_{\\omega}\\right\\|_{\\mathcal{H}}\\leq\\mu^{-1}\\left\\|\\hat{b}_{\\omega}\\right\\|_{\\mathcal{H}}}\\\\ &{\\qquad\\qquad\\leq\\mu^{-1}\\left(\\left\\|\\hat{b}_{\\omega}-b_{\\omega}\\right\\|_{\\mathcal{H}}+\\left\\|b_{\\omega}\\right\\|_{\\mathcal{H}}\\right)}\\\\ &{\\qquad\\qquad\\leq\\mu^{-1}\\left(C_{4}M\\left\\|\\hat{h}_{\\omega}-h_{\\omega}^{\\star}\\right\\|_{\\mathcal{H}}+B_{3}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where the first line uses that $\\left\\|\\hat{H}_{\\omega}^{-1}\\right\\|_{o p}\\,\\leq\\,\\mu^{-1}$ by Assumption ${(\\bf b)}$ , the second line follows by triangular inequality while the last line uses Equation (55) from Lemma E.7 for the first term and Assumption (e) for the second terms. By squaring the above bound and taking expectation w.r.t. $\\hat{h}_{\\omega}$ , we get: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\Vert\\tilde{a}_{\\omega}\\right\\Vert_{\\mathcal{H}}^{2}\\right]\\leq\\mathbb{E}\\left[\\left(\\mu^{-1}C_{4}M\\left\\Vert\\hat{h}_{\\omega}-h_{\\omega}^{\\star}\\right\\Vert+\\mu^{-1}B_{3}\\right)^{2}\\right]}\\\\ &{\\qquad\\qquad\\leq2\\mu^{-2}C_{4}^{2}M^{2}\\mathbb{E}\\left[\\left\\Vert\\hat{h}_{\\omega}-h_{\\omega}^{\\star}\\right\\Vert^{2}\\right]+2\\mu^{-2}B_{3}^{2}}\\\\ &{\\qquad\\qquad\\leq4\\mu^{-3}C_{4}^{2}M^{2}\\epsilon_{i n}+2\\mu^{-2}B_{3}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where the last line uses Equation (52). ", "page_idx": 35}, {"type": "text", "text": "Upper-bound on $\\|a_{\\omega}^{\\star}\\|_{\\mathcal{H}}$ . Using the closed-form expression of $a^{\\star}$ , it holds that: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|a_{\\omega}^{\\star}\\|_{\\mathcal{H}}=\\big\\|H_{\\omega}^{-1}b_{\\omega}\\big\\|_{\\mathcal{H}}\\leq\\mu^{-1}B_{3},}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where we used that $\\left\\|H_{\\omega}^{-1}\\right\\|_{o p}\\leq\\mu^{-1}$ by Assumption ${\\bf(b)}$ and that $\\|b_{\\omega}\\|_{\\mathcal{H}}\\leq B_{2}$ by Assumption (d). ", "page_idx": 35}, {"type": "text", "text": "Lemma E.7. Consider the operators $H_{\\omega},\\hat{H}_{\\omega},b_{\\omega},\\hat{b}_{\\omega}$ defined in Lemma E.6. Under Assumptions $(b)$ , (c) and $(f)$ and Assumptions (A) to $(L)$ , the following holds for any $(\\omega,s)\\in\\Omega\\times\\mathcal{H}$ : ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|{H_{\\omega}^{-1}s}\\right\\|_{L_{1}}\\leq\\mu^{-1}\\left\\|{s}\\right\\|_{L_{1}},\\qquad\\left\\|{\\hat{H}_{\\omega}^{-1}s}\\right\\|_{L_{1}}\\leq\\mu^{-1}\\left\\|{s}\\right\\|_{L_{1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\left(\\hat{H}_{\\omega}-H_{\\omega}\\right)s\\right\\|_{L_{1}}\\le C_{1}\\left\\|\\hat{h}_{\\omega}-h_{\\omega}^{\\star}\\right\\|_{\\mathcal{H}}\\|s\\|_{\\mathcal{H}}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "equation", "text": "$$\n\\left\\|b_{\\omega}-\\hat{b}_{\\omega}\\right\\|_{\\mathcal{H}}\\le C_{4}M\\left\\|\\hat{h}_{\\omega}-h_{\\omega}^{\\star}\\right\\|_{\\mathcal{H}}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof. We show each of the upper bounds separately. ", "page_idx": 35}, {"type": "text", "text": "Upper-bound on $\\left\\lVert H_{\\omega}^{-1}s\\right\\rVert_{L_{1}}$ and $\\left\\|\\hat{H}_{\\omega}^{-1}s\\right\\|_{L_{1}}^{\\quad}$ . Using strong convexity Assumption (b), we have the following inequality for the Hessian operator $H_{\\omega}$ acting on a function $c\\in\\mathcal{H}\\subset L_{1}(\\mathbb{P})$ : ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\left\\|H_{\\omega}c\\right\\|_{L_{1}}=\\mathbb{E}_{\\mathbb{P}}\\left[\\left\\|\\mathbb{E}_{\\mathbb{P}}\\left[\\partial_{v}^{2}\\ell_{i n}(\\omega,h_{\\omega}^{\\star}(x),x,y)\\middle|x\\right]c(x)\\right\\|\\right]\\geq\\mu\\left\\|c\\right\\|_{L_{1}}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "by positive-definiteness of $H_{\\omega}$ , we take $c=H_{\\omega}^{-1}s$ for some $s\\in{\\mathcal{H}}$ : ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\left\\|H_{\\omega}^{-1}s\\right\\|_{L_{1}}\\leq\\mu^{-1}\\left\\|s\\right\\|_{L_{1}}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "By the same arguments, the above bound applies to $\\left\\|\\hat{H}_{\\omega}^{-1}s\\right\\|_{L_{1}}$ ", "page_idx": 35}, {"type": "text", "text": "Upper-bound on $\\left\\|\\left(\\hat{H}_{\\omega}-H_{\\omega}\\right)c\\right\\|_{L_{1}}$ . We can express the operator $\\left(\\hat{H}_{\\omega}-H_{\\omega}\\right)$ acting on some $s\\in{\\mathcal{H}}$ as follows: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(\\hat{H}_{\\omega}-H_{\\omega}\\right)s=\\mathbb{E}_{\\mathbb{P}}\\left[\\left(\\mathbb{E}_{\\mathbb{P}}\\left[\\partial_{v}^{2}\\ell_{i n}(\\omega,\\hat{h}_{\\omega}(x),x,y)-\\partial_{v}^{2}\\ell_{i n}(\\omega,h_{\\omega}^{\\star}(x),x,y)\\middle|x\\right]\\right)s(x)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Using Assumption (c) we upper-bound the $L_{1}$ norm of the above quantity as follows: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\left(\\hat{H}_{\\omega}-H_{\\omega}\\right)s\\right\\|_{L_{1}}\\le C_{1}\\mathbb{E}_{\\mathbb{P}}\\left[\\left\\|\\hat{h}_{\\omega}(x))-h_{\\omega}^{\\star}(x)\\right\\|\\left\\|s(x)\\right\\|\\right]\\le C_{1}\\left\\|\\hat{h}_{\\omega}-h_{\\omega}^{\\star}\\right\\|_{\\mathcal{H}}\\left\\|s\\right\\|_{\\mathcal{H}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where we used Cauchy-Schwarz inequality to get the last inequality. ", "page_idx": 35}, {"type": "text", "text": "Upper-bound on $\\left\\|b_{\\omega}-\\hat{b}_{\\omega}\\right\\|_{\\mathcal{H}}$ . Using Lemma D.3 to get an expression of $b_{\\omega}$ and $\\hat{b}_{\\omega}$ , we obtain the following upper-bound: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|b_{\\omega}-\\hat{b}_{\\omega}\\right\\|_{\\mathcal{H}}^{2}=\\mathbb{E}_{\\mathbb{P}}\\left[\\left\\|\\mathbb{E}_{\\mathbb{Q}}\\left[\\left(\\partial_{v}\\ell_{o u t}(\\omega,h_{\\omega}^{\\star}(x),x,y)-\\partial_{v}\\ell_{o u t}(\\omega,\\hat{h}_{\\omega}(x),x,y)\\right)\\vert x\\right]r(x)\\right\\|^{2}\\right]}\\\\ &{\\qquad\\qquad\\leq C_{4}^{2}M^{2}\\left\\|\\hat{h}_{\\omega}-h_{\\omega}^{\\star}\\right\\|_{\\mathcal{H}}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where we used that the density $r(x)$ is upper-bounded by a positive constant $M$ by Assumption (B) and that $\\partial_{v}\\ell_{o u t}$ is $C_{4}$ Lipschitz in its second argument by Assumption (f). \u53e3 ", "page_idx": 35}, {"type": "text", "text": "F Connection with Parametric Implicit Differentiation ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "F.1 Parametric approximation of the functional bilevel problem ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "In this section, we approximate the functional problem in Equation (FBO) with a parametric bilevel problem where inner-level functions are parametrized as $h(x)=\\tau(\\theta)(x)$ with parameters $\\theta$ . Here, the inner-level variable is $\\theta$ instead of the function $h$ . Standard bilevel optimization algorithms like AID can be applied, which involve differentiating twice with respect to the parametric model. However, for models like deep neural networks, the inner objective may not be strongly convex in $\\theta$ , leading to a non-positive or degenerate Hessian (Proposition F.1). This can cause numerical instabilities and divergence from the gradient in Equation (4) (Proposition F.2), especially when using AID, which relies on solving a quadratic problem defined by the Hessian. ", "page_idx": 36}, {"type": "text", "text": "If the model has multiple solutions, the Hessian may be degenerate, making the implicit function theorem inapplicable. In contrast, functional implicit differentiation requires solving a positive definite quadratic problem in $\\mathcal{H}$ to find an adjoint function $a_{\\omega}^{\\star}$ , ensuring a solution even when $h_{\\omega}^{\\star}$ is sub-optimal, due to the strong convexity of $L_{i n}(\\omega,h)$ . This stability with sub-optimal solutions is crucial for practical algorithms like the one in Section 3, where the optimal prediction function is approximated within a parametric family, such as neural networks. ", "page_idx": 36}, {"type": "text", "text": "Formally, to establish a connection with parametric implicit differentiation, let us consider $\\tau:\\Theta\\mapsto\\mathcal{H}$ to be a map from a finite dimensional set of parameters $\\Theta$ to the functional Hilbert space $\\mathcal{H}$ and define a parametric version of the outer and inner objectives in Equation (FBO) restricted to functions in ${\\mathcal{H}}_{\\Theta}:=\\{\\tau(\\theta)\\mid\\theta\\in\\Theta\\}$ : ", "page_idx": 36}, {"type": "equation", "text": "$$\nG_{o u t}(\\omega,\\theta):=L_{o u t}\\left(\\omega,\\tau(\\theta)\\right)\\qquad G_{i n}(\\omega,\\theta):=L_{i n}\\left(\\omega,\\tau(\\theta)\\right).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "The map $\\tau$ can typically be a neural network parameterization and allows to obtain a \u201cmore tractable\u201d approximation to the abstract solution $h_{\\omega}^{\\star}$ in $\\mathcal{H}$ where the function space $\\mathcal{H}$ is often too large to perform optimization. This is typically the case when $\\mathcal{H}$ is an $L_{2}$ -space of functions as we discuss in more details in Section 3. When $\\mathcal{H}$ is a Reproducing Kernel Hilbert Space (RKHS), $\\tau$ may also correspond to the Nystr\u00f6m approximation [Williams and Seeger, 2000], which performs the optimization on a finite-dimensional subspace of an RKHS spanned by a few data points. ", "page_idx": 36}, {"type": "text", "text": "The corresponding parametric version of the problem (FBO) is then formally defined as: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\omega\\in\\Omega}{\\operatorname*{min}}\\ G_{t o t}(\\omega):=G_{o u t}(\\omega,\\theta_{\\omega}^{\\star})}\\\\ &{\\mathrm{~s.t.~}\\theta_{\\omega}^{\\star}\\in\\underset{\\theta\\in\\Theta}{\\arg\\operatorname*{min}}\\ G_{i n}(\\omega,\\theta).}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "The resulting bilevel problem in Equation (PBO) often arises in machine learning but is generally ambiguously defined without further assumptions on the map $\\tau$ as the inner-level problem might admit multiple solutions [Arbel and Mairal, 2022b]. Under the assumption that $\\tau$ is twice continuously differentiable and the rather strong assumption that the parametric Hessian $\\partial_{\\theta}^{2}G_{i n}(\\omega,\\theta_{\\omega}^{\\star})$ is invertible for a given $\\omega$ , the expression for the total gradient $\\nabla_{\\omega}G_{t o t}(\\omega)$ follows by direct application of the parametric implicit function theorem [Pedregosa, 2016]: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\nabla_{\\omega}G_{t o t}(\\omega)=\\partial_{\\omega}G_{o u t}(\\omega,\\theta_{\\omega}^{\\star})+\\partial_{\\omega,\\theta}G_{i n}(\\omega,\\theta_{\\omega}^{\\star})u_{\\omega}^{\\star}}\\\\ &{}&{u_{\\omega}^{\\star}=-\\partial_{\\theta}^{2}G_{i n}(\\omega,\\theta_{\\omega}^{\\star})^{-1}\\partial_{\\theta}G_{o u t}(\\omega,\\theta_{\\omega}^{\\star}),\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $u_{\\omega}^{\\star}$ is the adjoint vector in $\\Theta$ . Without further assumptions, the expression of the gradient in Equation (57) is generally different from the one obtained in Proposition 2.2 using the functional point of view. Nevertheless, a precise connection between the functional and parametric implicit gradients can be obtained under expressiveness assumptions on the parameterization $\\tau$ , as discussed in the next two propositions. ", "page_idx": 36}, {"type": "text", "text": "Proposition F.1. Under the same assumptions as in Proposition 2.2 and assuming that $\\tau$ is twice continuously differentiable, the following expression holds for any $(\\omega,\\theta)\\in\\Omega\\times\\Theta$ : ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\partial_{\\theta}^{2}G_{i n}(\\omega,\\theta):=\\partial_{\\theta}\\tau(\\theta)\\partial_{h}^{2}L_{i n}(\\omega,\\tau(\\theta))\\partial_{\\theta}\\tau(\\theta)^{\\top}+\\partial_{\\theta}^{2}\\tau(\\theta)\\left[\\partial_{h}L_{i n}(\\omega,\\tau(\\theta))\\right],\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $\\partial_{\\theta}^{2}\\tau(\\theta)$ is a linear operator measuring the distortion induced by the parameterization and acts on functions in $\\mathcal{H}$ by mapping them to a matrix $p\\times p$ where $p$ is the dimension of the parameter space $\\Theta$ . If, in addition, $\\tau$ is expressive enough so that $\\tau(\\theta_{\\omega}^{\\star})=h_{\\omega}^{\\star}$ , then the above expression simplifies to: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\partial_{\\theta}^{2}G_{i n}(\\omega,\\theta_{\\omega}^{\\star}):=\\!\\partial_{\\theta}\\tau(\\theta_{\\omega}^{\\star})C_{\\omega}\\partial_{\\theta}\\tau(\\theta_{\\omega}^{\\star})^{\\top}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Proposition F.1 follows by direct application of the chain rule, noting that the distortion term on the right of (58) vanishes when $\\theta=\\theta_{\\omega}^{\\star}$ since $\\partial_{h}{\\cal L}_{i n}(\\omega,\\tau(\\theta_{\\omega}^{\\star}))=\\partial_{h}{\\cal L}_{i n}(\\stackrel{\\_}{\\omega},h_{\\omega}^{\\star})=0$ by optimality of $h_{\\omega}^{\\star}$ . A consequence is that, for an optimal parameter $\\theta_{\\omega}^{\\star}$ , the parametric Hessian is necessarily symmetric positive semi-definite. However, for an arbitrary parameter $\\theta$ , the distortion does not vanish in general, making the Hessian possibly non-positive. This can result in numerical instability when using algorithms such as AID for which an adjoint vector is obtained by solving a quadratic problem defined by the Hessian matrix $\\partial_{\\theta}^{2}G_{i n}$ evaluated on approximate minimizers of the inner-level problem. Moreover, if the model admits multiple solutions $\\theta_{\\omega}^{\\star}$ , the Hessian is likely to be degenerate making the implicit function theorem inapplicable and the bilevel problem in Equation (PBO) ambiguously defined1. On the other hand, the functional implicit differentiation requires finding an adjoint function $a_{\\omega}^{\\star}$ by solving a positive definite quadratic problem in $\\mathcal{H}$ which is always guaranteed to have a solution even when the inner-level prediction function is only approximately optimal. ", "page_idx": 37}, {"type": "text", "text": "Proposition F.2. Assuming that $\\tau$ is twice continuously differentiable and that for $a$ fixed $\\omega\\in\\Omega$ we have $\\tau(\\theta_{\\omega}^{\\star})=h_{\\omega}^{\\star}$ , and $\\bar{J}_{\\omega}:=\\partial_{\\theta}\\tau(\\theta_{\\omega}^{\\star})$ has a full rank, then, under the same assumptions as in Proposition 2.2, $\\nabla_{\\omega}G_{t o t}(\\omega)$ is given by: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\nabla_{\\omega}G_{t o t}(\\omega)=g_{\\omega}+B_{\\omega}P_{\\omega}a_{\\omega}^{\\star},\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $P_{\\omega}:\\mathcal{H}\\rightarrow\\mathcal{H}$ is a projection operator of rank $\\dim(\\Theta)$ . If, in addition, the equality $\\tau(\\theta_{\\omega^{\\prime}}^{\\star})=$ $h_{\\omega^{\\prime}}^{\\star}$ holds for all $\\omega^{\\prime}$ in a neighborhood of $\\omega$ , then $\\nabla_{\\omega}G_{t o t}(\\omega):=\\nabla\\mathcal{F}(\\omega)=g_{\\omega}+B_{\\omega}a_{\\omega}^{\\star}$ . ", "page_idx": 37}, {"type": "text", "text": "Proposition F.2, which is proven below, shows that, even when the parametric family is expressive enough to recover the optimal prediction function $h_{\\omega}^{\\star}$ at a single value $\\omega$ , the expression of the total gradient in Equation (60) using parametric implicit differentiation might generally differ from the one obtained using its functional counterpart. Indeed the projector $P_{\\omega}$ , which has a rank equal to $\\dim(\\Theta)$ , biases the adjoint function by projecting it into a finite dimensional space before applying the cross derivative operator. Only under a much stronger assumption on $\\tau$ , requiring it to recover the optimal prediction function $h_{\\omega}^{\\star}$ in a neighborhood of the outer-level variable $\\omega$ , both parametric and functional implicit differentiation recover the same expression for the total gradient. In this case, the projector operator aligns with the cross-derivative operator so that $B_{\\omega}P_{\\omega}=B_{\\omega}$ . Finally, note that the expressiveness assumptions on $\\tau$ made in Propositions F.1 and F.2 are only used here to discuss the connection with the parametric implicit gradient and are not required by the method we introduce in Section 3. ", "page_idx": 37}, {"type": "text", "text": "Proof of Proposition $F.2$ . Here we want to show the connection between the parametric gradient of the outer variable $\\nabla_{\\omega}G_{t o t}(\\omega)$ usually used in approximate differentiation methods and the functional gradient of the outer variable $\\nabla{\\mathcal{F}}(\\omega)$ derived from the functional bilevel problem definition in Equation (FBO). Recall the definition of the parametric inner objective $G_{i n}(\\omega,\\theta):=L_{i n}\\left(\\omega,\\tau(\\theta)\\right)$ . According to Proposition F.1, we have the following relation ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\partial_{\\theta}^{2}G_{i n}(\\omega,\\theta_{\\omega}^{\\star}):=\\!J_{\\omega}C_{\\omega}J_{\\omega}^{\\top}\\ \\ \\mathrm{with}\\ \\ J_{\\omega}:=\\partial_{\\theta}\\tau(\\theta_{\\omega}^{\\star}).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "By assumption, $J_{\\omega}$ has a full rank which matches the dimension of the parameter space $\\Theta$ . Recall from the assumptions of Theorem 2.1 that the Hessian operator $C_{\\omega}$ is positive definite by the strong convexity of the inner-objective $L_{i n}$ in the second argument. We deduce that $\\partial_{\\theta}^{2}G_{i n}(\\dot{\\omega},\\theta_{\\omega}^{\\star})$ must be invertible, since, by construction, the dimension of $\\Theta$ is smaller than that of the Hilbert space $\\mathcal{H}$ which has possibly infinite dimension. Recall from Theorem 2.1, $B_{\\omega}:=\\partial_{\\omega,h}L_{i n}(\\omega,h_{\\omega}^{\\star})$ and the assumption that $\\tau(\\theta_{\\omega}^{\\star})=h_{\\omega}^{\\star}$ . We apply the parametric implicit function theorem to get the following expression of the Jacobian $\\partial_{\\omega}\\theta_{\\omega}^{\\star}$ : ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\partial_{\\omega}\\theta_{\\omega}^{\\star}:=-B_{\\omega}J_{\\omega}^{\\top}\\left(J_{\\omega}C_{\\omega}J_{\\omega}^{\\top}\\right)^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Hence, differentiating the total objective $G_{t o t}(\\omega):=G_{o u t}(\\omega,\\theta_{\\omega}^{\\star})=L_{o u t}\\left(\\omega,\\tau(\\theta_{\\omega}^{\\star})\\right)$ and applying the chain rule directly results in the following expression: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\nabla_{\\omega}G_{t o t}(\\omega)=g_{\\omega}-B_{\\omega}J_{\\omega}^{\\top}\\left(J_{\\omega}C_{\\omega}J_{\\omega}^{\\top}\\right)^{-1}J_{\\omega}d_{\\omega},\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "with previously defined $g_{\\omega}:=\\partial_{\\omega}L_{o u t}(\\omega,h_{\\omega}^{\\star})$ and $d_{\\omega}:=\\partial_{h}L_{o u t}\\left(\\omega,h_{\\omega}^{\\star}\\right)$ . ", "page_idx": 37}, {"type": "text", "text": "We now introduce the operator $P_{\\omega}:=J_{\\omega}^{\\top}\\left(J_{\\omega}C_{\\omega}J_{\\omega}^{\\top}\\right)^{-1}J_{\\omega}C_{\\omega}$ . The operator $P_{\\omega}$ is a projector as it satisfies $P_{\\omega}^{2}=P_{\\omega}$ . Hence, using the fact that the Hessian operator is invertible, and recalling that the adjoint function is given by $\\bar{a_{\\omega}^{\\star}}\\,{=}\\,-C_{\\omega}^{-1}d_{\\omega}$ , we directly get form Equation (61) that: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla_{\\omega}G_{t o t}(\\omega):=g_{\\omega}+B_{\\omega}P_{\\omega}a_{\\omega}^{\\star}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "If we further assume that $\\tau(\\theta_{\\omega^{\\prime}}^{\\star})=h_{\\omega^{\\prime}}^{\\star}$ holds for all $\\omega^{\\prime}$ in a neighborhood of $\\omega$ , then differentiating with respect to $\\omega$ results in the following identity: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\partial_{\\omega}\\theta_{\\omega}^{\\star}J_{\\omega}=\\partial_{\\omega}h_{\\omega}^{\\star}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Using the expression of $\\partial_{\\omega}h_{\\omega}^{\\star}$ from Equation (3), we have the following identity: ", "page_idx": 38}, {"type": "equation", "text": "$$\n-\\partial_{\\omega}\\theta_{\\omega}^{\\star}J_{\\omega}C_{\\omega}=B_{\\omega}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "In other words, $B_{\\omega}$ is of the form $B_{\\omega}:=D J_{\\omega}C_{\\omega}$ for some finite dimensional matrix $D$ of size $\\dim(\\Omega)\\times\\dim(\\Theta)$ . Recalling the expression of the total gradient, we can deduce the equality between parametric and functional gradients: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\omega}G_{t o t}(\\omega)=g_{\\omega}-B_{\\omega}J_{\\omega}^{\\top}\\left(J_{\\omega}C_{\\omega}J_{\\omega}^{\\top}\\right)^{-1}J_{\\omega}d_{\\omega}}\\\\ &{\\qquad\\qquad\\quad=g_{\\omega}-D J_{\\omega}C_{\\omega}J_{\\omega}^{\\top}\\left(J_{\\omega}C_{\\omega}J_{\\omega}^{\\top}\\right)^{-1}J_{\\omega}d_{\\omega}}\\\\ &{\\qquad\\quad=g_{\\omega}-D J_{\\omega}d_{\\omega}}\\\\ &{\\qquad\\quad=g_{\\omega}-D J_{\\omega}C_{\\omega}C_{\\omega}^{-1}d_{\\omega}}\\\\ &{\\qquad\\quad=g_{\\omega}+B_{\\omega}a_{\\omega}^{\\star}=\\nabla\\mathcal{F}(\\omega).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "The first equality follows from the general expression of the total gradient $\\nabla_{\\omega}G_{t o t}(\\omega)$ . In the second line we use the expression of $B_{\\omega}$ which then allows to simplify the expression in the third line. Then, recalling that the Hessian operator $C_{\\omega}$ is invertible, we get the fourth line. Finally, the result follows by using again the expression of $B_{\\omega}$ and recalling the definition of the adjoint function $a_{\\omega}^{\\star}$ . \u53e3 ", "page_idx": 38}, {"type": "text", "text": "F.2 Computational Cost and Scalability ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "The optimization of the prediction function $\\hat{h}_{\\omega}$ in the inner-level optimization loop is similar to AID, although the total gradient computation differs significantly. Unlike AID, Algorithm 1 does not require differentiating through the parameters of the prediction model when estimating the total gradient $\\nabla{\\mathcal{F}}(\\omega)$ . This property results in an improved cost in time and memory in most practical cases as shown in Table 1 and Figure 4. More precisely, AID requires computing Hessian-vector products of size $p_{i n}$ , which corresponds to the number of hidden layer weights of the neural network $\\bar{\\hat{h}}_{\\omega}$ . While FuncID only requires Hessian-vector products of size $d_{v}$ , i.e. the output dimension of $\\hat{h}_{\\omega}$ . In many practical cases, the network\u2019s parameter dimension $p_{i n}$ is much larger than its output size $d_{v}$ , which results in considerable benefits in terms of memory when using FuncID rather than AID, as shown in Figure 4 (left). Furthermore, unlike AID, the overhead of evaluating Hessian-vector products in FuncID is not affected by the time cost for evaluating the prediction network. When $\\hat{h}_{\\omega}$ is a deep network, such an overhead increases significantly with the network size, making AID significantly slower (Figure 4 (right)). ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\frac{\\mathrm{Method}}{\\mathrm{AID}}\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\mathrm{Time~cost}}&{\\mathrm{Memory~cost}}\\\\ &{F u n c I D}&{\\gamma(T_{L_{i n}}+T_{h})}&{\\beta p_{i n}+M_{h}}\\\\ &{\\gamma T_{L_{i n}}+(2+\\delta)T_{a}+T_{h}}&{\\beta d_{v}+M_{a}}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Table 1: Cost in time and memory for performing a single total gradient estimation using either AID or FuncID and assuming the prediction model is learned. Time cost: $T_{h}$ and $T_{a}$ represent the time cost of evaluating both prediction and adjoint models $h$ and $a$ , while $T_{i n}$ is the time cost for evaluating the inner objective once the outputs of $h$ are computed. The factors $\\gamma$ and $\\delta$ are multiplicative overheads for evaluating hessian-vector products and gradient. Memory cost: $M_{h}$ and $M_{a}$ represent the memory cost of storing the intermediate outputs of $h$ and $a$ , $p_{i n}$ and $d_{v}$ are the memory costs of storing the Hessian-vector product for AID and FuncID respectively and $\\beta$ is a multiplicative constant that depends on a particular implementation. ", "page_idx": 38}, {"type": "image", "img_path": "enlxHLwwFf/tmp/a50c5441da9570a2101590283c0b5bd5b924be098b29cde6cd2d26ec671496d1.jpg", "img_caption": ["Figure 4: Memory and time comparison of a single total gradient approximation using FuncID vs AID. (Left) Memory usage ratio of FuncID over AID vs inner model parameter dimension $p_{i n}$ , for various values of the output dimension $d_{v}$ . (Right) Time ratio of FuncID over AID vs inner model parameter dimension $p_{i n}$ averaged over several values of $d_{v}$ and $10^{4}$ evaluations. The continuous lines are experimental results obtained using a JAX implementation [Bradbury et al., 2018] running on a GPU. The dashed lines correspond to theoretical estimates obtained using the algorithmic costs given in Table 1 with $\\gamma=12,\\delta=2$ for time, and the constant factors in the memory cost ftited to the data. "], "img_footnote": [], "page_idx": 39}, {"type": "text", "text": "G Additional Details about 2SLS Experiments ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "We closely follow the experimental setting of the state-of-the-art method DFIV [Xu et al., 2021a]. The goal of this experiment is to learn a model $f_{\\omega}$ approximating the structural function $f_{s t r u c t}$ that accurately describes the effect of the treatment $t$ on the outcome $o$ with the help of an instrument $x$ , as illustrated in Figure 5. ", "page_idx": 39}, {"type": "image", "img_path": "enlxHLwwFf/tmp/7f93aeee17ae7ddc69fc8a00d175a39cdb54996978bae3f1993064f8987c44b3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 39}, {"type": "text", "text": "Figure 5: The causal relationships between all variables in an Instrumental Variable (IV) causal graph, where $t$ is the treatment variable (dsprites image), $o$ is the outcome (label in $\\mathbb{R}$ ), $x$ is the instrument and $\\epsilon$ is the unobserved confounder ", "page_idx": 39}, {"type": "text", "text": "G.1 Dsprites data. ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "We follow the exact same data generation procedure as in $\\mathrm{Xu}$ et al. [2021a, Appendix E.3]. From the dsprites dataset [Matthey et al., 2017], we generate the treatment $t$ and outcome $o$ as follows: ", "page_idx": 39}, {"type": "text", "text": "1. Uniformly sample latent parameters scale, rotation, posX, posY from dsprites. ", "page_idx": 39}, {"type": "text", "text": "2. Generate treatment variable $t$ as ", "page_idx": 39}, {"type": "equation", "text": "$$\nt=F i g(s c a l e,\\;r o t a t i o n,\\;p o s X,\\;p o s Y)+\\lambda.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "3. Generate outcome variable $o$ as ", "page_idx": 39}, {"type": "equation", "text": "$$\no=\\frac{||A t||_{2}^{2}-5000}{1000}+32(p o s Y\\ -0.5)+\\varepsilon.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Here, function $F i g$ returns the corresponding image to the latent parameters, and $\\lambda,\\varepsilon$ are noise variables generated from $\\lambda\\,\\sim\\,{\\mathcal{N}}(0.{\\bar{0}},0.1I)$ and $\\bar{\\varepsilon}\\,\\sim\\,{\\mathcal N}(0.0,0.5)$ . Each element of the matrix $A\\in\\mathbb{R}^{10\\times\\overline{{4}}096}$ is generated from $\\mathrm{Unif}(0.0,1.0)$ and fixed throughout the experiment. From the data generation process, we can see that $t$ and $o$ are confounded by posY. We use the instrumental variable $x=\\!(s c a l e,$ , rotation, $p o s X)\\in\\mathbb{R}^{3}$ , and figures with random noise as treatment variable $t$ . The variable posY is not revealed to the model, and there is no observable confounder. The structural function for this setting is ", "page_idx": 39}, {"type": "text", "text": "", "page_idx": 40}, {"type": "equation", "text": "$$\nf_{s t r u c t}(t)={\\frac{\\|A t\\|_{2}^{2}-5000}{1000}}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Test data points are generated from grid points of latent variables. The grid consist of 7 evenly spaced values for posX, posY, 3 evenly spaced values for scale, and 4 evenly spaced values for orientation. ", "page_idx": 40}, {"type": "text", "text": "G.2 Experimental details ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "All results are reported over an average of 20 runs with different seeds on 24GB NVIDIA RTX A5000 GPUs. ", "page_idx": 40}, {"type": "text", "text": "Feature maps. As in the DFIV setting, we approximate the true structural function $f_{s t r u c t}$ with $f_{\\omega}\\;=\\;u^{\\top}\\psi_{\\chi}\\overline{{(t)}}$ where $\\psi_{\\chi}$ is a feature map of the treatment $t$ , $u$ is a vector in $\\mathbb{R}^{d_{2}}$ , and $f_{\\omega}$ is parameterized by $\\boldsymbol{\\omega}=(u,\\chi)$ . To solve the inner-problem of the bilevel formulation in Section 4.1, the inner prediction function $h_{\\omega}$ is optimized over functions of the form $h(x)=V\\phi(x)$ where we denote $\\phi$ the feature map of the instrument $x$ and $V$ is a matrix in $\\mathbb{R}^{d_{1}\\times d_{1}}$ . The feature maps $\\psi_{\\chi}$ and $\\phi$ are neural networks (Table 2) that are optimized using empirical objectives from Section 3.1 and synthetic dsprites data, the linear weights $V$ and $u$ are fitted exactly at each iteration. ", "page_idx": 40}, {"type": "text", "text": "Choice of the adjoint function in FuncID. In the dsprites experiment, we call linear FuncID the functional implicit diff. method with a linear choice of the adjoint function. Linear FuncID uses an adjoint function of the form $a_{\\omega}^{\\star}(x)\\,=\\,W\\phi(x)$ with $W\\,\\in\\,\\breve{\\mathbb{R}}^{d_{1}\\times d_{1}}$ . In other words, to find $a_{\\omega}^{\\star}$ , the features $\\phi$ are fixed and only the optimal linear weight $W$ is computed in closed-form. In the FuncID method, the adjoint function lives in the same function space as $h_{\\omega}$ . This is achieved by approximating $a_{\\omega}^{\\star}$ with a separate neural network with the same architecture as $h_{\\omega}$ . ", "page_idx": 40}, {"type": "table", "img_path": "enlxHLwwFf/tmp/061ecd5f2e4e58170785fa1294bde4f6e6771abbc26558fa1d895ccd71fef43a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 40}, {"type": "text", "text": "Table 2: Neural network architectures used in the dsprites experiment for all models. The FuncID model has an extra fully-connected layer $\\mathrm{FC}(32,1)$ in both networks. LN corresponds to LayerNorm and SN to SpectralNorm. ", "page_idx": 40}, {"type": "text", "text": "Hyper-parameter tuning. As in the setup of DFIV, for training all methods, we use 100 outer iterations $N$ in Algorithm 1), and 20 inner iterations ( $M$ in Algorithm 1) per outer iteration with full-batch. We select the hyper-parameters based on the best validation loss, which we obtain using a validation set with instances of all three variables $(t,o,x)$ [Xu et al., 2021a, Appendix A]. Because of the number of linear solvers, the grid search performed for AID is very large, so we only run it with one seed. For other methods, we run the grid search on 4 different seeds and take the ones with the highest average validation loss. Additionally, for the hyper-parameters that are not tuned, we take the ones reported in $\\mathrm{Xu}$ et al. [2021a]. ", "page_idx": 40}, {"type": "text", "text": "\u2022 Deep Feature Instrumental Variable Regression: All DFIV hyper-parameters are set based on the best ones reported in $\\mathrm{Xu}$ et al. [2021a]. \u2022 Approximate Implicit Differentiation: We perform a grid search over 5 linear solvers (two variants of gradient descent, two variants of conjugate gradient and an identity heuristic solver), linear solver learning rate $10^{-n}$ with $n\\in\\{3,4,5\\}$ , linear solver number of iterations $\\{2,10,20\\}$ , inner optimizer learning rate $10^{-n}$ with $n\\in\\{2,3,4\\}$ , inner optimizer weight decay $10^{-n}$ with $\\bar{n^{\\star}}\\in\\{1,2,3\\}$ and outer optimizer learning rate $\\mathrm{i}0^{-n}$ with $n\\in\\{2,3,\\bar{4}\\}$ . \u2022 Iterative Differentiation: We perform a grid search over number of \u201cunrolled\u201d inner iterations $\\{2,5\\}$ (this is chosen because of memory constraints since \u201cunrolling\u201d an iteration is memory-heavy), number of warm-start inner iterations $\\{18,15\\}$ , inner optimizer learning rate $10^{-n}$ with $n\\in\\{2,3,4\\}$ , inner optimizer weight decay $10^{-n}$ with $n\\in\\{1,2,3\\}$ and outer optimizer learning rate $10^{-n}$ with $n\\in\\{2,3,{\\bar{4}}\\}$ . ", "page_idx": 40}, {"type": "text", "text": "", "page_idx": 40}, {"type": "text", "text": "", "page_idx": 41}, {"type": "text", "text": "\u2022 Gradient Penalty: The method is based on Eq. 5.1 in Shen and Chen [2023], for this singlelevel method we perform a grid search on the learning rate $10^{-n}$ with $n\\in\\{2,3,4,5,6\\},$ weight decay $10^{-n}$ with $n\\in\\{1,2,3\\}$ , and the penalty weight $10^{-n}$ with $n\\in\\{0,1,2,3\\}$ . Since the method has only a single optimization loop, we increase the number of total iterations to 2000 compared to the other methods (100 outer-iterations and 20 inner iterations).   \n\u2022 Value Function Penalty: The method is based on Eq. 3.2 in Shen and Chen [2023], for this method we perform the same grid search as for the Gradient Penalty method. However, since this method has an inner loop, we perform 100 outer iterations and perform a grid search on the number of inner iterations with 10 and 20.   \n\u2022 FuncID: We perform a grid search over the number of iterations for learning the adjoint network $\\{10,20\\}$ , adjoint optimizer learning rate $10^{-n}$ with $n\\in\\{2,3,4,5,6\\}$ and adjoint optimizer weight decay $10^{-n}$ with $n\\in\\{1,2,3\\}$ . The rest of the parameters are the same as for DFIV since the inner and outer models are almost equivalent to the treatment and instrumental networks used in their experiments. ", "page_idx": 41}, {"type": "text", "text": "G.3 Additional results ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "We run an additional experiment with $10k$ training points using the same setting described above to illustrate the effect of the sample size on the methods. Figure 6 shows that a similar conclusion can be drawn when increasing the training sample size from $5k$ to $10k$ , thus illustrating the robustness of the obtained results. ", "page_idx": 41}, {"type": "image", "img_path": "enlxHLwwFf/tmp/6d310ffdbe3e5fb2630bdd681c237e4adf733a2a508b62a38b0ffaa608cd1fec.jpg", "img_caption": [], "img_footnote": [], "page_idx": 41}, {"type": "text", "text": "Figure 6: Performance metrics for Instrumental Variable (IV) regression. (Left) final test loss. (Middle) outer loss vs training iterations, (Right) inner loss vs training iterations. All results are averaged over 20 runs with 10000 training samples and 588 test samples. ", "page_idx": 41}, {"type": "text", "text": "H Additional Details about Model-Based RL Experiments ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "H.1 Closed-form expression for the adjoint function ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "For the FuncID method, we exploit the structure of the adjoint objective to obtain a closed-form expression of the adjoint function $a_{\\omega}^{\\star}$ . In the model-based RL setting, the unregularized adjoint objective has a simple expression of the form: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{L}_{a d j}(\\omega,a,\\hat{h}_{\\omega},\\mathcal{B})=\\frac{1}{2\\,\\vert\\mathcal{B}_{i n}\\vert}\\sum_{(x,y)\\in\\mathcal{B}_{i n}}\\left\\Vert a(x)\\right\\Vert^{2}}\\\\ &{\\qquad\\qquad\\qquad+\\left.\\frac{1}{\\vert\\mathcal{B}_{o u t}\\vert}a(x)^{\\top}\\partial_{v}f(\\hat{h}_{\\omega}(x),y).}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "The key observation here is that the same batches of data are used for both the inner and outer problems, i.e. $B_{i n}=B_{o u t}$ . Therefore, we only need to evaluate the function $a$ on a finite set of points $x$ where $(x,y)\\in B_{i n}$ . Without restricting the solution set of $a$ or adding regularization to $\\bar{L}_{a d j}$ , the optimal solution $a_{\\omega}^{\\star}$ simply matches $-\\partial_{v}f(\\hat{h}_{\\omega}(x),y)$ on the set of points $x$ s.t. $(x,y)\\in B_{i n}$ . Our implementation directly exploits this observation and uses the following expression for the total ", "page_idx": 41}, {"type": "text", "text": "gradient estimation: ", "page_idx": 42}, {"type": "equation", "text": "$$\ng_{o u t}=-\\sum_{(x,y)\\in B_{i n}}\\partial_{\\omega,v}f(\\hat{h}_{\\omega}(x),r_{\\omega}(x),s_{\\omega}(x))\\partial_{v}f(\\hat{h}_{\\omega}(x),y).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "H.2 Experimental details ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "As in the experiments of Nikishin et al. [2022], we use the CartPole environment with 2 actions, 4-dimensional continuous state space, and optimal returns of 500. For evaluation, we use a separate copy of the environment. The reported return is an average of 10 runs with different seeds. ", "page_idx": 42}, {"type": "text", "text": "Networks. We us the same neural network architectures that are used in the CartPole experiment of Nikishin et al. [2022, Appendix D]. All networks have two hidden layers and ReLU activations. Both hidden layers in all networks have dimension 32. In the misspecified setting with the limited model class capacity, we set the hidden layer dimension to 3 for the dynamics and reward networks. ", "page_idx": 42}, {"type": "text", "text": "Hyper-parameters. We perform 200000 environment steps (outer-level steps) and set the number of inner-level iterations to $M=1$ for both OMD and FuncID. for MLE, we perform a single update to the state-value function for each update to the model. For training, we use a replay buffer with a batch size of 256, and set the discount factor $\\gamma$ to 0.99. When sampling actions, we use a temperature parameter $\\alpha=0.01$ as in Nikishin et al. [2022]. The learning rate for outer parameters $\\omega$ is set to $\\mathrm{\\dot{1}0^{-3}}$ . For the learning rate of the inner neural network and the moving average coefficient $\\tau$ , we perform a grid search over $\\left\\{10^{-4},10^{-3},3\\cdot10^{-3}\\right\\}$ and $\\{5\\cdot10^{-3},10^{-2}\\}$ as in Nikishin et al. [2022]. ", "page_idx": 42}, {"type": "text", "text": "H.3 Additional results ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Time comparison. Figure 7 shows the average reward on the evaluation environment as a function of training time in seconds. We observe that our model is the fastest to reach best performance both in the well-specified and misspecified settings. ", "page_idx": 42}, {"type": "image", "img_path": "enlxHLwwFf/tmp/8c1346b5c6036270186cb1ff6f46394ee0c94df41ed700dc4dc8d1a7495fd1a8.jpg", "img_caption": ["Figure 7: Average Reward on an evaluation environment vs. time in seconds on the CartPole task. (Left) Well-specified predictive model with 32 hidden units to capture the variability in the states dynamics. (Right) misspecified predictive model with only 3 hidden states. "], "img_footnote": [], "page_idx": 42}, {"type": "text", "text": "MDP model comparison. Figure 8 shows the average prediction error of different methods during training. The differences in average prediction error between the bilevel approaches (OMD, FuncID) and MLE reflect their distinct optimization objectives and trade-offs. OMD and FuncID focus on maximizing performance in the task environment, while MLE emphasizes accurate representation of all aspects of the environment, which can lead to smaller prediction errors but may not necessarily correlate with superior evaluation performance. We also observe that FuncID has a stable prediction error in both settings meanwhile OMD and MLE exhibit some instability. ", "page_idx": 42}, {"type": "image", "img_path": "enlxHLwwFf/tmp/c9cf98bbff8ee33040ea01f16ef3722818815d637d5526b4e8daa0248117b6d0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 43}, {"type": "text", "text": "Figure 8: Average MDP model prediction error in the training environment vs. inner optimization steps on the CartPole task. (Left) Well-specified predictive model with 32 hidden units to capture the variability in the states dynamics. (Right) misspecified predictive model with only 3 hidden states. ", "page_idx": 43}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: All the claims made in the paper are either rigorously proven, shown experimentally or describe well-established facts in the literature. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 44}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Justification: All strong assumptions used in the theoretical results are clearly stated, discussed, and put into perspective with other published work. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 44}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: All theoretical claims are rigorously proven. We provide a structured list of all assumptions used before the statement of a result and refer to a specific assumption whenever it is employed in the proof. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 45}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: We provide the code to reproduce our experiments, which includes a README file with instructions. We also include the information about tuning, datasets and other experimental detail in the appendix. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 45}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: We provide an anonymized repository with extensively commented code and instructions on how to reproduce our main experiment. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 46}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: Yes, we specify and discuss all experimental details in the appendix. Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 46}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: In all of our experiments we run multiple times and report the error bars. Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 46}, {"type": "text", "text": "", "page_idx": 47}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: We report which GPUs we use for larger experiments and show how our method compares to similar methods in the section about computational cost. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 47}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: We reviewed the NeurIPS Code of Ethics and attest that our work conforms with it. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 47}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: We present a new bilevel optimization method, which does not have any immediate societal impacts. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 47}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 48}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 48}, {"type": "text", "text": "Justification: Our new optimization method does not have an identifiable risk for misuse. Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 48}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Justification: Our code includes an open access licence in the README file. Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 48}, {"type": "text", "text": "", "page_idx": 49}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 49}, {"type": "text", "text": "Justification: We do not present new assets in this work. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 49}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 49}, {"type": "text", "text": "Justification: We do not have crowdsourcing nor human subjects in our work. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 49}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 49}, {"type": "text", "text": "Justification: We do not have human subjects in our work. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 49}]