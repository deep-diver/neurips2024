[{"figure_path": "h1iMVi2iEM/figures/figures_1_1.jpg", "caption": "Figure 1: \u201cDual drift\u201d issue of the federated primal dual method under different participation ratios. When the participation ratio is low, dual drift introduces a very large variance, yielding divergence.", "description": "This figure shows how the performance of federated primal dual methods changes with different participation ratios in federated learning. The left graph shows that as the participation ratio decreases, the training loss increases significantly. The right graph shows that the test accuracy also decreases as the participation ratio decreases. This indicates a serious problem called \"dual drift\", which causes instability and divergence in federated primal dual methods under partial participation.", "section": "3 Methodology"}, {"figure_path": "h1iMVi2iEM/figures/figures_5_1.jpg", "caption": "Figure 2: Test of the proposed A-FedPD method on setups of different participation ratios, different local intervals, and different rounds. In these experiments, we fix the total training data samples and total training iterations and then learn their variation trends.", "description": "This figure shows the results of experiments conducted to evaluate the performance of the proposed A-FedPD method under various conditions.  Specifically, it illustrates how the method performs with different client participation ratios (a), varying local training iteration numbers (b), and changing the total number of communication rounds (c).  The consistent performance across these variations highlights the robustness and efficiency of A-FedPD.", "section": "Experiments"}, {"figure_path": "h1iMVi2iEM/figures/figures_8_1.jpg", "caption": "Figure 2: Test of the proposed A-FedPD method on setups of different participation ratios, different local intervals, and different rounds. In these experiments, we fix the total training data samples and total training iterations and then learn their variation trends.", "description": "The figure displays three graphs showing the performance of the A-FedPD method under varying experimental conditions. The first graph demonstrates the effects of different participation ratios on the model's accuracy. The second graph displays the results under different local training intervals. Finally, the third graph illustrates the performance across various communication rounds. In all three cases, the A-FedPD method is compared to other relevant baselines to show the performance improvements.", "section": "5 Experiments"}, {"figure_path": "h1iMVi2iEM/figures/figures_9_1.jpg", "caption": "Figure 2: Test of the proposed A-FedPD method on setups of different participation ratios, different local intervals, and different rounds. In these experiments, we fix the total training data samples and total training iterations and then learn their variation trends.", "description": "This figure displays the performance of the A-FedPD model under varying experimental conditions.  Three subfigures show how accuracy changes with (a) different participation ratios (percentage of clients participating each round), (b) different local intervals (number of local training iterations per client), and (c) different communication rounds (total number of communication rounds). The consistent trend shows that A-FedPD maintains strong performance under various parameters, with some optimal configurations.", "section": "5 Experiments"}, {"figure_path": "h1iMVi2iEM/figures/figures_15_1.jpg", "caption": "Figure 2: Test of the proposed A-FedPD method on setups of different participation ratios, different local intervals, and different rounds. In these experiments, we fix the total training data samples and total training iterations and then learn their variation trends.", "description": "The figure shows the impact of different participation ratios, local intervals, and communication rounds on the performance of the A-FedPD method.  It illustrates how the model's accuracy and training loss change with variations in these key hyperparameters, while keeping the total number of training samples and iterations consistent.  This helps in understanding the trade-offs between these parameters and their effect on training efficiency and generalization.", "section": "Experiments"}, {"figure_path": "h1iMVi2iEM/figures/figures_16_1.jpg", "caption": "Figure 5: Introducing the brightness biases to different clients. We calculate the average brightness to control each sample to a proper state. Each client will randomly sample a Gaussian noise to perturb the local samples.", "description": "This figure illustrates how brightness biases are introduced to different clients in a federated learning setting.  The average brightness of each client's dataset is calculated. Then, Gaussian noise is added to each sample, randomly altering its brightness and one of its color channels. This simulates the real-world scenario of data being collected from various sources with differences in lighting and color balance. The goal is to introduce a level of heterogeneity among clients' datasets, mimicking real-world conditions.", "section": "A.3 Dataset and Splitting"}, {"figure_path": "h1iMVi2iEM/figures/figures_17_1.jpg", "caption": "Figure 2: Test of the proposed A-FedPD method on setups of different participation ratios, different local intervals, and different rounds. In these experiments, we fix the total training data samples and total training iterations and then learn their variation trends.", "description": "The figure shows three graphs, each illustrating the performance of the A-FedPD model under different settings. The first graph shows the test accuracy at different participation ratios (percentage of clients participating in each training round). The second graph illustrates the impact of varying the local training intervals (number of local updates performed by each client before model aggregation). The third graph compares the model's performance over a range of communication rounds (number of communication rounds between server and clients).\nOverall, these graphs highlight the robustness and efficiency of A-FedPD in handling different training scenarios and demonstrate its ability to achieve high accuracy with fewer communication rounds.", "section": "5 Experiments"}, {"figure_path": "h1iMVi2iEM/figures/figures_18_1.jpg", "caption": "Figure 2: Test of the proposed A-FedPD method on setups of different participation ratios, different local intervals, and different rounds. In these experiments, we fix the total training data samples and total training iterations and then learn their variation trends.", "description": "This figure displays the results of experiments evaluating the performance of the A-FedPD method under varying conditions. Three sets of experiments are shown, each with a different parameter fixed:\n(a) Different Participation Ratios:  Shows how A-FedPD's accuracy changes with different percentages of clients participating in each round.\n(b) Different Local Intervals: Shows how A-FedPD's accuracy changes with different numbers of local training iterations.\n(c) Different Rounds: Shows how A-FedPD's accuracy changes over different communication rounds.", "section": "5 Experiments"}, {"figure_path": "h1iMVi2iEM/figures/figures_18_2.jpg", "caption": "Figure 2: Test of the proposed A-FedPD method on setups of different participation ratios, different local intervals, and different rounds. In these experiments, we fix the total training data samples and total training iterations and then learn their variation trends.", "description": "This figure displays the results of experiments conducted to evaluate the performance of the A-FedPD method under varying conditions.  The three subplots show how the method performs with different participation ratios (percentage of clients participating in each round), different local intervals (number of local training iterations), and different communication rounds (number of communication rounds between server and clients).  In all experiments, the total number of training samples and iterations were kept constant to isolate the effect of the varied parameter. The graphs show that A-FedPD demonstrates consistent and stable performance across different settings.", "section": "5 Experiments"}]