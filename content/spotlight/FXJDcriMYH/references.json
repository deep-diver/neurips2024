{"references": [{"fullname_first_author": "Tom B. Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-XX-XX", "reason": "This paper is foundational to the field of large language models (LLMs) by demonstrating the few-shot learning capabilities of LLMs, directly impacting the direction of LLM research and development."}, {"fullname_first_author": "Jared Kaplan", "paper_title": "Scaling laws for neural language models", "publication_date": "2020-XX-XX", "reason": "This paper introduces scaling laws for LLMs which provides empirical guidelines for efficient LLM pre-training, crucial for the efficient training methods discussed in this paper."}, {"fullname_first_author": "Jordan Hoffmann", "paper_title": "Training compute-optimal large language models", "publication_date": "2022-XX-XX", "reason": "This paper introduces the concept of compute-optimal LLMs, directly informing the focus of this paper on efficient LLM pre-training techniques."}, {"fullname_first_author": "Tianqi Chen", "paper_title": "Net2net: Accelerating learning via knowledge transfer", "publication_date": "2015-XX-XX", "reason": "This paper introduces the Net2Net model growth technique which is a foundational method that directly inspires model growth methods used in this paper for efficient LLM pre-training."}, {"fullname_first_author": "Yiqun Yao", "paper_title": "Masked structural growth for 2x faster language model pre-training", "publication_date": "2024-XX-XX", "reason": "This paper is among the most recent works on model growth techniques for LLMs and is closely related to the work presented in this paper, thus representing the state-of-the-art in this specific subfield."}]}