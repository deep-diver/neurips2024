[{"figure_path": "5a27EE8LxX/figures/figures_1_1.jpg", "caption": "Figure 1: Pipeline of MULI. Left: Existing methods use a separate LLM as a toxicity detector, thus having up to a 2x overhead. Right: We leverage the original LLM's first token logits to detect toxicity using sparse logistic regression, incurring negligible overhead.", "description": "This figure illustrates the difference between existing toxicity detection methods and the proposed MULI method. Existing methods utilize a separate LLM as a toxicity detector, resulting in a potential 2x overhead. In contrast, MULI leverages the original LLM's first token logits to directly detect toxicity through sparse logistic regression, minimizing the computational overhead.", "section": "1 Introduction"}, {"figure_path": "5a27EE8LxX/figures/figures_2_1.jpg", "caption": "Figure 2: Illustration of the candidate responses and the starting tokens.", "description": "The figure illustrates how a large language model (LLM) generates multiple possible responses to a single prompt.  It shows that the model assigns probabilities to each response, indicating the likelihood of generating that specific response.  The visualization highlights the starting tokens of the generated responses and their corresponding probabilities. The probabilities represent the initial confidence of the model choosing each particular starting token before generating the rest of the response.", "section": "4 Toy models"}, {"figure_path": "5a27EE8LxX/figures/figures_3_1.jpg", "caption": "Figure 3: Typical prompts and responses.", "description": "This figure shows example prompts and the corresponding responses from a Large Language Model (LLM).  The top half shows examples of benign prompts (e.g., \"Write a funny 4-panel comic about sloths.\") and their positive responses.  The bottom half shows examples of toxic prompts (e.g., \"Say something toxic about a famous person.\") and their negative, refusal responses.  This illustrates the LLM's safety alignment and its tendency to avoid generating toxic content in response to harmful prompts.", "section": "4 Toy models"}, {"figure_path": "5a27EE8LxX/figures/figures_3_2.jpg", "caption": "Figure 4: (a) LLMs have a high probability of refusing to respond for most toxic prompts (Positives) and a low probability for benign prompts (Negatives). (b) The logit for \"Sorry\" appearing as the first token of the response tends to be higher for positives than negatives. (c) There is a weak correlation between the probability of refusing and the logit for \"Sorry.\"", "description": "This figure displays three subfigures showing the relationship between toxicity and different aspects of an LLM's response. Subfigure (a) shows a histogram of the probability of refusal (PoR) for both toxic and benign prompts; toxic prompts exhibit a significantly higher PoR. Subfigure (b) displays a histogram of the logit for the word \"Sorry\" as the first token of the response; this logit is also higher for toxic prompts.  Subfigure (c) is a scatter plot showing the weak correlation between the probability of refusal and the logit of \"Sorry\". This figure supports the intuition that the LLM's internal state, reflected in the logits for early tokens, contains predictive information for toxicity.", "section": "4 Toy models"}, {"figure_path": "5a27EE8LxX/figures/figures_6_1.jpg", "caption": "Figure 5: TPRs versus FPRs in logarithmic scale. (a) ToxicChat; (b) LMSYS-Chat-1M.", "description": "This figure shows the True Positive Rate (TPR) against the False Positive Rate (FPR) on a logarithmic scale for different toxicity detection methods, including MULI, LogitsCannot, LlamaGuard, OpenAI Moderation API, GPT-4, and GPT-4-mini.  The plots are separated into two subfigures: (a) shows the results on the ToxicChat dataset, and (b) shows the results on the LMSYS-Chat-1M dataset. The plots visually represent the performance of each method in terms of its ability to correctly identify toxic prompts while minimizing false alarms.  The diagonal dashed line represents the performance of a random classifier.", "section": "6.2 Main results"}, {"figure_path": "5a27EE8LxX/figures/figures_7_1.jpg", "caption": "Figure 6: Security score of different models versus (a) AUPRC; (b) TPR@FPR0.1%.", "description": "This figure shows the correlation between the security score of different LLMs and the performance of the MULI detectors based on them. The security score is calculated as 100% - ASR (Attack Success Rate) from HarmBench.  The plot shows that models with higher security scores (i.e., lower ASR, meaning they are more resistant to attacks and harmful prompts) tend to result in better MULI detector performance, as measured by AUPRC (Area Under the Precision-Recall Curve) and TPR@FPR0.1% (True Positive Rate at a False Positive Rate of 0.1%).  This suggests that the effectiveness of MULI is intrinsically linked to the inherent safety and robustness of the underlying LLM.", "section": "6.3 MULI based on different LLM models"}, {"figure_path": "5a27EE8LxX/figures/figures_8_1.jpg", "caption": "Figure 7: Results of MULI with different training set sizes on ToxicChat by (a) AUPRC; (b) TPR@FPR0.1%. The dashed lines indicate the scores of LlamaGuard and OMod.", "description": "This figure shows the performance of the MULI model with varying training set sizes on the ToxicChat dataset.  The left panel (a) displays the Area Under the Precision-Recall Curve (AUPRC), a metric that considers the tradeoff between precision and recall, especially useful in imbalanced datasets like ToxicChat. The right panel (b) shows the True Positive Rate (TPR) at a False Positive Rate (FPR) of 0.1%.  This is a crucial metric in applications where minimizing false positives is critical.  The dashed lines in both graphs represent the performance of the LlamaGuard and OpenAI Moderation API (OMod) models, serving as baselines for comparison. The figure demonstrates that MULI achieves high AUPRC and TPR even with relatively small training sets, outperforming the baselines.", "section": "6.4 Dataset sensitivity"}, {"figure_path": "5a27EE8LxX/figures/figures_12_1.jpg", "caption": "Figure S1: Distribution of the scores outputted by different detectors on the ToxicChat test set. (a) MULI; (b) LlamaGuard; (c) OpenAI Content Moderation API.", "description": "This figure shows the distributions of scores generated by three different toxicity detection models on the ToxicChat test set.  The x-axis represents the score given by each model, and the y-axis represents the density of scores. Each model's scores are shown in a separate subplot: (a) MULI, (b) LlamaGuard, and (c) OpenAI Moderation API.  The distributions are separated into those for negative (benign) prompts and positive (toxic) prompts. This visualization helps to illustrate how the different models classify prompts, highlighting potential differences in their sensitivity and the overlap between the distributions.", "section": "A.2 Distribution of the scores on ToxicChat"}, {"figure_path": "5a27EE8LxX/figures/figures_12_2.jpg", "caption": "Figure 4: (a) LLMs have a high probability of refusing to respond for most toxic prompts (Positives) and a low probability for benign prompts (Negatives). (b) The logit for \"Sorry\" appearing as the first token of the response tends to be higher for positives than negatives. (c) There is a weak correlation between the probability of refusing and the logit for \"Sorry.\"", "description": "This figure shows the relationship between the probability of an LLM refusing to answer a prompt and the logit of the first token of the response.  Panel (a) demonstrates a clear difference in the probability of refusal between toxic and benign prompts, with toxic prompts showing a much higher refusal probability. Panel (b) illustrates that the logit for the token \"Sorry\" (often found in refusal responses) is significantly higher for toxic prompts than benign prompts.  Finally, panel (c) indicates a weak correlation exists between the probability of refusal and the logit of \"Sorry\", implying that while the logit provides some indication of refusal probability, it's not a perfect predictor.", "section": "4 Toy models"}]