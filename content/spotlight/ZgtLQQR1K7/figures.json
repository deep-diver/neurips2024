[{"figure_path": "ZgtLQQR1K7/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison of the establishment of correlations between image patches through (a) self-attention and (b) the proposed 2D-Selective-Scan (SS2D). The red boxes indicate the query image patch, with its opacity representing the degree of information loss.", "description": "The figure compares self-attention and the proposed 2D-Selective-Scan (SS2D) mechanisms for establishing correlations between image patches.  In (a), self-attention shows the query patch (red box) interacting with all other patches, indicated by the many yellow lines.  Opacity of patches represents information loss. In (b), SS2D shows how the query patch interacts with patches only along four specific scanning paths, shown by the colored lines. The compressed hidden state computed along each path is used to acquire contextual knowledge. This method is less computationally expensive than self-attention, showing its linear time complexity.", "section": "1 Introduction"}, {"figure_path": "ZgtLQQR1K7/figures/figures_3_1.jpg", "caption": "Figure 2: Illustration of 2D-Selective-Scan (SS2D). Input patches are traversed along four different scanning paths (Cross-Scan), with each sequence independently processed by separate S6 blocks. The results are then merged to construct a 2D feature map as the final output (Cross-Merge).", "description": "The figure illustrates the 2D-Selective-Scan (SS2D) mechanism.  Input image patches are scanned in four directions (Cross-Scan). Each scan produces a sequence which is then processed by independent S6 blocks.  The results from the four S6 blocks are then merged (Cross-Merge) to create a final 2D feature map.  This approach contrasts with self-attention, which uses a computationally expensive process involving all patches.", "section": "4.2 2D-Selective-Scan for Vision Data (SS2D)"}, {"figure_path": "ZgtLQQR1K7/figures/figures_4_1.jpg", "caption": "Figure 3: Left: Illustration of (a) the overall architecture of VMamba, and (b) - (d) the structure of Mamba and VSS blocks. Right: Comparison of VMamba variants and benchmark methods in terms of classification accuracy and computational efficiency.", "description": "This figure shows the architecture of VMamba, comparing it to Mamba and VSS blocks.  The left side illustrates the overall architecture (a) and the structure of the blocks (b-d).  The right side provides a performance comparison table showing ImageNet Top-1 accuracy, GFLOPs, and throughput for different VMamba variants and benchmark models (ConvNeXt-T, Swin-T). It highlights the improvements achieved through a series of architectural and implementation enhancements.", "section": "4 VMamba: Visual State Space Model"}, {"figure_path": "ZgtLQQR1K7/figures/figures_7_1.jpg", "caption": "Figure 4: Illustration of VMamba's adaptability to (a) downstream tasks and (b) input images with progressively increasing resolutions. Swin-T* denotes Swin-T tested with scaled window sizes.", "description": "This figure demonstrates VMamba's performance and scalability across various tasks and input resolutions. Subfigure (a) shows that VMamba outperforms other models (ConvNeXt and Swin) on object detection (APb and APm on COCO), and semantic segmentation (mIoU on ADE20K), maintaining its advantage with an increase in ImageNet-1K classification accuracy.  Subfigure (b) highlights VMamba's superior input scaling efficiency by showing a much smaller performance drop than other models (DeiT-S, ConvNeXt-T, XCIT-S12/16, Swin-T*, Vim-S) as the input resolution increases from 224x224 to 768x768, even without fine-tuning.", "section": "5 Experiments"}, {"figure_path": "ZgtLQQR1K7/figures/figures_7_2.jpg", "caption": "Figure 5: Illustration of VMamba's resource consumption with progressively increasing resolutions. Swin-T* denotes Swin-T tested with scaled window sizes.", "description": "This figure presents a comparison of FLOPs, throughput, and memory consumption for different vision backbones (VMamba-T, Swin-T*, XCIT-S12/16, DeiT-S, ConvNeXt-T, Vim-S, and S4ND-ConvNeXt-T) across various input image resolutions (224x224, 384x384, 512x512, 640x640, and 768x768).  It demonstrates VMamba-T's linear scaling behavior in terms of FLOPs and memory usage while maintaining relatively high throughput compared to other models, particularly as resolution increases.  The performance of Swin-T* is shown with scaled window sizes for a more relevant comparison. ", "section": "5.3 Analysis"}, {"figure_path": "ZgtLQQR1K7/figures/figures_7_3.jpg", "caption": "Figure 6: Illustration of the activation map for query patches indicated by red stars. The visualization results in (b) and (c) are obtained by combining the activation maps from each scanning path in SS2D.", "description": "The figure shows the activation maps for query patches.  (a) shows the input image with query patches marked by red stars. (b) displays the activation map generated using the standard self-attention mechanism (QKT), showcasing the activation of all previously scanned foreground tokens. (c) illustrates the activation map generated by the proposed SS2D mechanism ((Q\u03c9)(K/\u03c9)\u1d40), demonstrating a more focused activation on the neighborhood of the query patches. (d) shows the activation maps for each scanning path, highlighting how SS2D accumulates information during traversal.", "section": "5.3 Analysis"}, {"figure_path": "ZgtLQQR1K7/figures/figures_8_1.jpg", "caption": "Figure 7: Comparison of Effective Receptive Fields (ERF) [40] between VMamba and other benchmark models. Pixels with higher intensity indicate larger responses related to the central pixel.", "description": "This figure compares the effective receptive fields (ERF) of VMamba and several other benchmark models (ResNet-50, ConvNeXt-T, Swin-T, DeiT-S, HiViT-T, Vim-S) before and after training.  The ERF shows the region of the input image that influences the activation of a specific output unit.  The heatmaps show that VMamba, along with DeiT, HiViT and Vim, demonstrates global receptive fields.  This means that the activation is influenced by the entire input image.  ResNet, ConvNeXt, and Swin show local receptive fields, with activation primarily centered around the central pixel.  VMamba's global receptive field indicates its ability to capture long-range dependencies in image data.", "section": "5.3 Analysis"}, {"figure_path": "ZgtLQQR1K7/figures/figures_9_1.jpg", "caption": "Figure 8: Performance comparison of different scanning patterns. The proposed Cross-Scan achieves superior performance in speed while maintaining the same number of parameters and FLOPs.", "description": "This figure compares the performance of four different scanning patterns for the 2D-Selective-Scan (SS2D) module within the VMamba architecture.  The patterns are Unidi-Scan, Bidi-Scan, Cascade-Scan, and Cross-Scan.  The graph shows that Cross-Scan offers the highest throughput (images/s), both with and without depthwise convolutions (DWConv), while maintaining similar performance (Top-1 accuracy) to other methods.  This highlights Cross-Scan's effectiveness in capturing 2D contextual information efficiently.", "section": "5.2 Downstream Tasks"}, {"figure_path": "ZgtLQQR1K7/figures/figures_16_1.jpg", "caption": "Figure 9: Illustration of the attention maps obtained by SS2D.", "description": "This figure visualizes the attention maps generated by the 2D-Selective-Scan (SS2D) module.  It shows the attention maps from four different scanning routes (Cross-Scan) within the SS2D. The top row displays the attention maps using the QKT calculation, illustrating the relationship between the current patch and all previously scanned patches. The bottom row presents attention maps using (Q\u2299w)(K/w)T, showcasing a more focused attention around the neighborhood of the query patches.  The visualizations demonstrate how SS2D captures and retains information from all previously scanned tokens, especially those in the foreground.", "section": "4.2 2D-Selective-Scan for Vision Data (SS2D)"}, {"figure_path": "ZgtLQQR1K7/figures/figures_16_2.jpg", "caption": "Figure 1: Comparison of the establishment of correlations between image patches through (a) self-attention and (b) the proposed 2D-Selective-Scan (SS2D). The red boxes indicate the query image patch, with its opacity representing the degree of information loss.", "description": "This figure compares self-attention and the proposed 2D Selective Scan (SS2D) mechanisms for establishing correlations between image patches. The left panel (a) shows that self-attention computes correlations between all image patches, while the right panel (b) illustrates that SS2D computes correlations only along its scanning paths.  The opacity of the red boxes indicates how much information is lost with each method.", "section": "1 Introduction"}, {"figure_path": "ZgtLQQR1K7/figures/figures_21_1.jpg", "caption": "Figure 1: Comparison of the establishment of correlations between image patches through (a) self-attention and (b) the proposed 2D-Selective-Scan (SS2D). The red boxes indicate the query image patch, with its opacity representing the degree of information loss.", "description": "This figure compares how self-attention and the proposed 2D-Selective-Scan (SS2D) method establish correlations between image patches. The opacity of the red boxes, which represent the query patch, illustrates the degree of information loss in each method. Self-attention considers all patches simultaneously, while SS2D traverses patches along specific paths, capturing context in a computationally efficient manner.", "section": "1 Introduction"}, {"figure_path": "ZgtLQQR1K7/figures/figures_22_1.jpg", "caption": "Figure 7: Comparison of Effective Receptive Fields (ERF) [40] between VMamba and other benchmark models. Pixels with higher intensity indicate larger responses related to the central pixel.", "description": "This figure compares the effective receptive fields (ERFs) of VMamba and other benchmark models (ResNet-50, ConvNeXt-T, Swin-T, DeiT-S, HiViT-T, and Vim-S) before and after training. The ERF is a measure of the region in the input image that influences the activation of a specific output unit.  Higher intensity pixels in the heatmaps indicate stronger responses from the central pixel, which is representative of the region\u2019s influence on activation. The figure shows that VMamba and some other models (DeiT, HiViT, and Vim) achieve global ERFs after training, meaning that the receptive field spans a significant part of the image. In contrast, other models, such as ResNet and ConvNeXt-T, largely maintain local receptive fields even after training. VMamba's global ERF indicates its capacity to capture long-range contextual information.", "section": "5.3 Analysis"}, {"figure_path": "ZgtLQQR1K7/figures/figures_23_1.jpg", "caption": "Figure 14: The visualization of ERF of VMamba with different initialization.", "description": "This figure visualizes the Effective Receptive Fields (ERF) of VMamba before and after training, comparing three different initialization methods: Mamba-Init, Rand-Init, and Zero-Init.  The ERFs are represented as heatmaps, showing the intensity of response for the central pixel in relation to the surrounding pixels.  The comparison allows for an assessment of how the different initialization strategies impact the receptive field of the model, indicating the influence of initialization on the model's ability to capture global contextual information.", "section": "H.2 Influence of the Initialization Approach"}]