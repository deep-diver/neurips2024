[{"type": "text", "text": "VMamba: Visual State Space Model ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yue Liu 1 Yunjie Tian1 Yuzhong Zhao1 Hongtian $\\mathbf{Y}\\mathbf{u}^{1}$ Lingxi Xie2 Yaowei Wang3 Qixiang Ye1 Jianbin Jiao1 Yunfan Liu1\u2021 ", "page_idx": 0}, {"type": "text", "text": "1 UCAS 2 Huawei Inc. 3 Pengcheng Lab. {liuyue171,tianyunjie19,zhaoyuzhong20,yuhongtian17}@mails.ucas.ac.cn 198808xc@gmail.com, wangyw@pcl.ac.cn, {qxye,jiaojb,liuyunfan}@ucas.ac.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Designing computationally efficient network architectures remains an ongoing necessity in computer vision. In this paper, we adapt Mamba, a state-space language model, into VMamba, a vision backbone with linear time complexity. At the core of VMamba is a stack of Visual State-Space (VSS) blocks with the 2D Selective Scan (SS2D) module. By traversing along four scanning routes, SS2D bridges the gap between the ordered nature of 1D selective scan and the non-sequential structure of 2D vision data, which facilitates the collection of contextual information from various sources and perspectives. Based on the VSS blocks, we develop a family of VMamba architectures and accelerate them through a succession of architectural and implementation enhancements. Extensive experiments demonstrate VMamba\u2019s promising performance across diverse visual perception tasks, highlighting its superior input scaling efficiency compared to existing benchmark models. Source code is available at https://github.com/MzeroMiko/VMamba ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Visual representation learning remains as a fundamental research area in computer vision that has witnessed remarkable progress in the era of deep learning. To represent complex patterns in vision data, two primary categories of backbone networks, i.e., Convolutional Neural Networks (CNNs) [49, 27, 29, 53, 37] and Vision Transformers (ViTs) [13, 36, 57, 66], have been proposed and extensively utilized in a variety of visual tasks. Compared to CNNs, ViTs generally demonstrate superior learning capabilities on large-scale data due to their integration of the self-attention mechanism [58, 13]. However, the quadratic complexity of self-attention w.r.t. the number of tokens imposes substantial computational overhead in downstream tasks involving large spatial resolutions. ", "page_idx": 0}, {"type": "text", "text": "To address this challenge, significant efforts have been made to improve the efficiency of attention computation [54, 36, 12]. However, existing approaches either restrict the size of the effective receptive field [36] or suffer from notable performance degradation across various tasks [30, 60]. This motivates us to develop a novel architecture for vision data, while maintaining the inherent advantages of the vanilla self-attention mechanism, i.e., global receptive fields and dynamic weighting parameters [23]. ", "page_idx": 0}, {"type": "text", "text": "Recently, Mamba [17], a innovative State Space Model (SSM) [17, 43, 59, 71, 48], in the field of natural language processing (NLP), has emerged as a promising approach for long-sequence modeling with linear complexity. Drawing inspiration from this advancement, we introduce VMamba, a vision backbone that integrates SSM-based blocks to enable efficient visual representation learning. However, the core algorithm of Mamba, i.e., the parallelized selective scan operation, is essentially designed for processing one-dimensional sequential data. This presents a challenge when adapting it for processing vision data, which lacks an inherent sequential arrangement of visual components. To address this issue, we propose 2D Selective Scan (SS2D), a four-way scanning mechanism designed for spatial domain traversal. In contrast to the self-attention mechanism (Figure 1 (a)), SS2D ensures that each image patch acquires contextual knowledge exclusively through a compressed hidden state computed along its corresponding scanning path (Figure 1 (b)), thereby reducing the computational complexity from quadratic to linear. ", "page_idx": 0}, {"type": "image", "img_path": "ZgtLQQR1K7/tmp/cf61859bee7d85e21e237c4d9121e1da69c99fc3a411d21fb305107f94ea3241.jpg", "img_caption": ["Figure 1: Comparison of the establishment of correlations between image patches through (a) selfattention and (b) the proposed 2D-Selective-Scan (SS2D). The red boxes indicate the query image patch, with its opacity representing the degree of information loss. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Building on the VSS blocks, we develop a family of VMamba architectures (i.e., VMambaTiny/Small/Base) and enhance their performance through architectural improvements and implementation optimizations. Compared to benchmark vision models built on CNNs (ConvNeXt [37]), ViTs (Swin [36], HiViT [66]), and SSMs (S4ND [44], Vim [69]), VMamba consistently achieves higher image classification accuracy on ImageNet-1K [9] across various model scales. Specifically, VMamba-Base achieves a top-1 accuracy of $\\bar{83}.9\\%$ , surpassing Swin by $+0.4\\%$ , with a throughput exceeding Swin\u2019s by a substantial margin over $40\\%$ $646\\,\\nu s.$ . 458). VMamba\u2019s superiority extends across multiple downstream tasks, with VMamba-Tiny/Small/Base achieving $47.3\\%/48.7\\%/49.2\\%$ mAP in object detection on COCO [33] $\\mathrm{1\\times}$ training schedule). This outperforms Swin by $4.6\\%/3.9\\%/2.3\\%$ and ConvNeXt by $3.1\\%/3.3\\%/2.2\\%$ , respectively. As for single-scale semantic segmentation on ADE20K [68], VMamba-Tiny/Small/Base achieves $47.9\\%/50.6\\%/51.0\\%$ mIoU, which surpasses Swin by $3.4\\%/3.0\\%/2.9\\%$ and ConvNeXt by $1.9\\%/1.9\\%/1.9\\%$ , respectively. Furthermore, unlike ViT-based models, which experience quadratic growth in computational complexity with the number of input tokens, VMamba exhibits linear growth in FLOPs while maintaining comparable performance. This demonstrates its state-of-the-art input scalability. ", "page_idx": 1}, {"type": "text", "text": "The contributions of this study are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose VMamba, an SSM-based vision backbone for visual representation learning with linear time complexity. A series of architectural and implementation improvements are adopted to enhance the inference speed of VMamba.   \n\u2022 We introduce 2D Selective Scan (SS2D) to bridge 1D array scanning and 2D plane traversal, enabling the extension of selective SSMs to process vision data.   \n\u2022 VMamba achieves promising performance across various visual tasks, including image classification, object detection, and semantic segmentation. It also exhibits remarkable adaptability w.r.t. the length of the input sequence, showcasing linear growth in computational complexity. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Convolutional Neural Networks (CNNs). Since AlexNet [31], considerable efforts have been devoted to enhancing the modeling capabilities [49, 52, 27, 29] and computational efficiency [28, 53, 64, 46] of CNN-based models across various visual tasks. Sophisticated operators like depth-wise convolution [28] and deformable convolution [5, 70] have been introduced to increase the flexibility and efficacy of CNNs. Recently, inspired by the success of Transformers [58], modern CNNs [37] have shown promising performance by integrating long-range dependencies [11, 47, 34] and dynamic weights [23] into their designs. ", "page_idx": 1}, {"type": "text", "text": "Vision Transformers (ViTs). As a pioneering work, ViT [13] explores the effectiveness of vision models based on vanilla Transformer architecture, highlighting the importance of large-scale pre-training for image classification performance. To reduce ViT\u2019s dependence on large datasets, DeiT [57] introduces a teacher-student distillation strategy, transferring knowledge from CNNs to ViTs and emphasizing the importance of inductive bias in visual perception. Following this approach, subsequent studies propose hierarchical ViTs [36, 12, 61, 39, 66, 55, 6, 10, 67, 1]. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Another research direction focuses on improving the computational efficiency of self-attention, which serves as the cornerstone of ViTs. Linear Attention [30] reformulates self-attention as a linear dot-product of kernel feature maps, using the associativity property of matrix products to reduce computational complexity from quadratic to linear. GLA [65] introduces a hardware-efficient variant of linear attention that balances memory movement with parallelizability. RWKV [45] also leverages the linear attention mechanism to combine parallelizable transformer training with the efficient inference of recurrent neural networks (RNNs). RetNet [51] adds a gating mechanism to enable a parallelizable computation path, offering an alternative to recurrence. RMT [15] further extends this for visual representation learning by applying the temporal decay mechanism to the spatial domain. ", "page_idx": 2}, {"type": "text", "text": "State Space Models (SSMs). Despite their widespread adoption in vision tasks, ViT architectures face significant challenges due to the quadratic complexity of self-attention, especially when handling long input sequences (e.g., high-resolution images). In efforts to improve scaling efficiency [8, 7, 45, 51, 41], SSMs have emerged as compelling alternatives to Transformers, attracting significant research attention. Gu et al. [21] demonstrate the potential of SSM-based models in handling the long-range dependencies using the HiPPO initialization [18]. To improve practical feasibility, S4 [20] proposes normalizing the parameter matrices into a diagonal structure. Various structured SSM models have since emerged, each offering distinct architectural enhancements, such as complexdiagonal structures [22, 19], support for multiple-input multiple-output [50], diagonal plus low-rank decomposition [24], and selection mechanisms [17]. These advancements have also been integrated into larger representation models [43, 41, 16], further highlighting the versatility and scalability of structured state space models in various applications. While these models primarily target long-range and sequential data such as text and speech, limited research has explored applying SSMs to vision data with two-dimensional structures. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Formulation of SSMs. Originating from the Kalman filter [32], SSMs are linear time-invariant (LTI) systems that map the input signal $u(t)\\in\\mathbb{R}$ to the output response $y(t)\\in\\mathbb{R}$ via the hidden state $\\mathbf{h}(t)\\in\\mathbb{R}^{N}$ . Specifically, continuous-time SSMs can be expressed as linear ordinary differential equations (ODEs) as follows, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{h}^{\\prime}(t)=\\mathbf{A}\\mathbf{h}(t)+\\mathbf{B}u(t),}\\\\ {y(t)=\\mathbf{C}\\mathbf{h}(t)+D u(t),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathbf{A}\\in\\mathbb{R}^{N\\times N},\\mathbf{B}\\in\\mathbb{R}^{N\\times1},\\mathbf{C}\\in\\mathbb{R}^{1\\times N}$ , and $D\\in\\mathbb{R}^{1}$ are the weighting parameters. ", "page_idx": 2}, {"type": "text", "text": "Discretization of SSM. To be integrated into deep models, continuous-time SSMs must undergo discretization in advance. Concretely, for the time interval $[t_{a},t_{b}]$ , the analytic solution of the hidden state variable $\\mathbf{h}(t)$ at $t=t_{b}$ can be expressed as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{h}(t_{b})=e^{\\mathbf{A}(t_{b}-t_{a})}\\mathbf{h}(t_{a})+e^{\\mathbf{A}(t_{b}-t_{a})}\\int_{t_{a}}^{t_{b}}\\mathbf{B}(\\tau)u(\\tau)e^{-\\mathbf{A}(\\tau-t_{a})}\\,d\\tau.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "By sampling with the time-scale parameter $\\Delta$ (i.e., $d\\tau|_{t_{i}}^{t_{i+1}}=\\Delta_{i}^{\\phantom{\\dagger}}$ ), $h(t_{b})$ can be discretized by ", "page_idx": 2}, {"type": "equation", "text": "$$\n{\\bf h}_{b}=e^{{\\bf A}(\\Delta_{a}+...+\\Delta_{b-1})}\\left({\\bf h}_{a}+\\sum_{i=a}^{b-1}{\\bf B}_{i}u_{i}e^{-{\\bf A}(\\Delta_{a}+...+\\Delta_{i})}\\Delta_{i}\\right),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $[a,b]$ is the corresponding discrete step interval. Notably, this formulation approximates the result obtained by the zero-order hold (ZOH) method, which is frequently utilized in the literature of SSM-based models (please refer to Appendix A for detailed proof). ", "page_idx": 2}, {"type": "image", "img_path": "ZgtLQQR1K7/tmp/debe23eb916a8e6c76b474911bef34cbb12449b56647795140eb4cfc2616243d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: Illustration of 2D-Selective-Scan (SS2D). Input patches are traversed along four different scanning paths (Cross-Scan), with each sequence independently processed by separate S6 blocks. The results are then merged to construct a 2D feature map as the final output (Cross-Merge). ", "page_idx": 3}, {"type": "text", "text": "Selective Scan Mechanism. To address the limitation of LTI SSMs (Eq. 1) in capturing the contextual information, Gu et al. [17] propose a novel parameterization method for SSMs, which incorporates an input-dependent selection mechanism (referred to as S6). However, for selective SSMs, the time-varying weighting parameters pose a challenge for efficient computation of hidden states, as convolutions cannot accommodate dynamic weights, making them inapplicable. Nevertheless, since the recurrence relation of $h_{b}$ in Eq. 3 can be derived, the response $y_{b}$ can still be efficiently computed using associative scan algorithms [2, 42, 50], which has linear complexity (see Appendix B for a detailed explanation). ", "page_idx": 3}, {"type": "text", "text": "4 VMamba: Visual State Space Model ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "4.1 Network Architecture ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We develop VMamba in three scales: Tiny, Small, and Base (referred to as VMamba-T, VMamba-S, and VMamba-B, respectively). An overview of the architecture of VMamba-T is illustrated in Figure 3 (a), and detailed configurations are provided in Appendix E. The input image $\\mathbf{I}\\in\\mathbb{R}^{H\\times W\\times3}$ is first partitioned into patches by a stem module, resulting in a 2D feature map with spatial dimension of $H/\\bar{4}\\times W/4$ . Without incorporating additional positional embeddings, multiple network stages are employed to create hierarchical representations with resolutions of $H/8\\times W/8,H/16\\times W/16$ , and $\\bar{H7}3\\dot{2}\\times W/32$ . Specifically, each stage comprises a down-sampling layer (except for the first stage), followed by a stack of Visual State Space (VSS) blocks. ", "page_idx": 3}, {"type": "text", "text": "The VSS blocks serve as the visual counterparts to Mamba blocks [17] (Figure 3 (b)) for representation learning. The initial architecture of VSS blocks (referred to as the \u2018vanilla VSS Block\u2019 in Figure 3 (c)) is formulated by replacing the S6 module. S6 is the core of Mamba and achieves global receptive fields, dynamic weights (i.e., selectivity), and linear complexity. We substitute it with the newly proposed 2D-Selective-Scan (SS2D) module, and more details will be introduced in the following subsection. To further enhance computational efficiency, we remove the entire multiplicative branch (highlighted by the red box in Figure 3 (c)), as the effect of the gating mechanism has already been achieved by the selectivity of SS2D. As a result, the improved VSS block (shown in Figure 3 (d)) consists of a single network branch with two residual modules, mimicking the architecture of a vanilla Transformer block [58]. All results in this paper are obtained using VMamba models built with VSS blocks in this architecture. ", "page_idx": 3}, {"type": "text", "text": "4.2 2D-Selective-Scan for Vision Data (SS2D) ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "While the sequential nature of the scanning operation in S6 aligns well with NLP tasks involving temporal data, it poses a significant challenge when applied to vision data, which is inherently nonsequential and encompasses spatial information (e.g., local texture and global structure). To address this issue, S4ND [44] reformulates SSM with convolutional operations, directly extending the kernel from 1D to 2D through the outer-product. However, such modification restricts the weights from being input-dependent, resulting in a limited capacity for capturing contextual information. Therefore, we adhere to the selective scan approach [17] for input processing and propose the 2D-Selective-Scan (SS2D) module to adapt S6 to vision data without compromising its advantages. ", "page_idx": 3}, {"type": "image", "img_path": "ZgtLQQR1K7/tmp/07b571c2475523a29712593495cbf617ee4267a759f2c49f56e346b84c794b52.jpg", "img_caption": ["Figure 3: Left: Illustration of (a) the overall architecture of VMamba, and (b) - (d) the structure of Mamba and VSS blocks. Right: Comparison of VMamba variants and benchmark methods in terms of classification accuracy and computational efficiency. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 2 illustrates that data forwarding in SS2D consists of three steps: cross-scan, selective scanning with S6 blocks, and cross-merge. Specifically, SS2D first unfolds the input patches into sequences along four distinct traversal paths (i.e., Cross-Scan). Each patch sequence is then processed in parallel using a separate S6 block, and the resultant sequences are reshaped and merged to form the output map (i.e., Cross-Merge). Through the use of complementary 1D traversal paths, SS2D allows each pixel in the image to integrate information from all other pixels across different directions. This integration facilitates the establishment of global receptive fields in the 2D space. ", "page_idx": 4}, {"type": "text", "text": "4.3 Accelerating VMamba ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "As shown in Figure 3 (e), the VMamba-T model with vanilla VSS blocks (referred to as \u2018Vanilla VMamba\u2019) achieves a throughput of 426 images/s and contains 22.9M parameters with 5.6G FLOPs. Despite achieving a state-of-the-art classification accuracy of $82.2\\%$ (outperforming Swin-T [36] by $0.9\\%$ at the tiny level), the low throughput and high memory overhead present significant challenges for the practical deployment of VMamba. ", "page_idx": 4}, {"type": "text", "text": "In this subsection, we outline our efforts to enhance its inference speed, primarily focusing on improvements in both implementation details and architectural design. We evaluate the models with image classification on ImageNet-1K. The impact of each progressive improvement is summarized as follows, where $\\%$ , img/s) denote the gains in top-1 accuracy on ImageNet-1K and inference throughput, respectively. Further discussion is provided in Appendix E. ", "page_idx": 4}, {"type": "text", "text": "Step (a) $(+0.0\\%,+41\\,\\mathrm{img/s})$ by re-implementing Cross-Scan and Cross-Merge in Triton. ", "page_idx": 4}, {"type": "text", "text": "Step (b) $(+0.0\\%$ , $-3\\,\\mathrm{img/s})$ by adjusting the CUDA implementation of selective scan to accommodate float16 input and float32 output. This remarkably enhances the training efficiency (throughput from 165 to 184), despite slight speed fluctuation at test time.   \nStep (c) $(+0.0\\%$ , $+174\\;\\mathrm{img/s})$ by substituting the relatively slow einsum in selective scan with a linear transformation (i.e., torch.nn.functional.linear). We also adopt the tensor layout of (B, C, H, W) to eliminate unnecessary data permutations. ", "page_idx": 4}, {"type": "text", "text": "Table 1: Performance comparison on ImageNet-1K. Throughput values are measured with an A100 GPU and an AMD EPYC 7542 CPU, using the toolkit released by [62], following the protocol proposed in [36]. All images are of size $224\\times224$ . ", "page_idx": 5}, {"type": "table", "img_path": "ZgtLQQR1K7/tmp/2e7a04480a0033390bfef3a9f4d4875fbbbee50cb212af9e28d450c2bc7b7f85.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Step (d) $(-0.6\\%$ , $+175\\;\\mathrm{img/s})$ ) by introducing MLP into VMamba due to its computational efficiency. We also discard the DWConv (depth-wise convolutional [23]) layers and change the layer configuration from [2,2,9,2] to [2,2,2,2] to lower FLOPs. ", "page_idx": 5}, {"type": "text", "text": "Step (e) $(+0.6\\%$ , $+366\\;\\mathrm{img/s})$ ) by reducing the parameter ssm-ratio (the feature expansion factor) from 2.0 to 1.0 (also referred to as Step (d.1)), raising the layer numbers to [2,2,5,2] (also referred to as Step (d.2)), and discarding the entire multiplicative branch as illustrated in Figure 3 (c). ", "page_idx": 5}, {"type": "text", "text": "Step (f) $(+0.3\\%,+161\\,\\mathrm{img/s})$ by introducing the DWConv layers (also referred to as Step (e.1)) and reducing the parameter d_state (the SSM state dimension) from 16.0 to 1.0 (also referred to as Step (e.2)), together with raising ssm-ratio back to 2.0. ", "page_idx": 5}, {"type": "text", "text": "Step (g) $(+0.1\\%$ , $+346\\;\\mathrm{img/s})$ ) by reducing the ssm-ratio to 1.0 while changing the layer configuration from [2,2,5,2] to [2,2,8,2]. ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we present a series of experiments to evaluate the performance of VMamba and compare it to popular benchmark models across various visual tasks. We also validate the effectiveness of the proposed 2D feature map traversal method by comparing it with alternative approaches. Additionally, we analyze the characteristics of VMamba by visualizing its effective receptive field (ERF) and activation map, and examining its scalability with longer input sequences. We primarily follow the hyperparameter settings and experimental configurations used in Swin [36]. For detailed experiment settings, please refer to Appendix E and F, and for additional ablations, see Appendix H. All experiments were conducted on a server with $8\\times\\mathrm{{NVIDIA}}$ Tesla-A100 GPUs. ", "page_idx": 5}, {"type": "text", "text": "5.1 Image Classification ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We evaluate VMamba\u2019s performance in image classification on ImageNet-1K [9], with comparison results against benchmark methods summarized in Table 1. With similar FLOPs, VMamba-T achieves a top-1 accuracy of $82.6\\%$ , outperforming DeiT-S by $2.8\\%$ and Swin-T by $1.3\\%$ . Notably, VMamba maintains its performance advantage at both Small and Base scales. For example, VMamba-B achieves a top-1 accuracy of $83.9\\%$ , surpassing DeiT-B by $2.1\\%$ and Swin-B by $0.4\\%$ . ", "page_idx": 5}, {"type": "text", "text": "In terms of computational efficiency, VMamba-T achieves a throughput of 1,686 images/s, which is either superior or comparable to state-of-the-art methods. This advantage continues with VMamba-S and VMamba-B, achieving throughputs of 877 images/s and 646 images/s, respectively. Compared to SSM-based models, the throughput of VMamba-T is $1.47\\times$ higher than S4ND-Conv-T [44] and $1.08\\times$ higher than Vim-S [69], while maintaining a clear performance lead of $0.4\\%$ and $2.1\\%$ over these models, respectively. ", "page_idx": 5}, {"type": "text", "text": "Table 2: Left: Results for object detection and instance segmentation on MSCOCO. $A P^{b}$ and $A P^{m}$ denote box AP and mask AP, respectively. FLOPs are calculated with an input size of $1280\\times800$ . The notation $\"1\\times\"$ indicates models fine-tuned for 12 epochs, while $\\mathbf{\\nabla}^{*}3\\!\\times\\!\\mathbf{M}\\mathbf{S}^{\\star}$ denotes multi-scale training for 36 epochs. Right: Results for semantic segmentation on ADE20K. FLOPs are calculated with an input size of $512\\times2048$ . \u2018SS\u2019 and \u2018MS\u2019 denote single-scale and multi-scale testing, respectively. ", "page_idx": 6}, {"type": "table", "img_path": "ZgtLQQR1K7/tmp/c6df94985bd70fe543d6b923109abb24f1f709d69f00c6fe362567a48b66c4ea.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "5.2 Downstream Tasks ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this sub-section, we evaluate the performance of VMamba on downstream tasks, including object detection and instance segmentation on MSCOCO2017 [33], and semantic segmentation on ADE20K [68]. The training framework is based on the MMDetection [3] and MMSegmenation [4] libraries, following [35] in utilizing Mask R-CNN [26] and UperNet [63] as the detection and segmentation networks, respectively. ", "page_idx": 6}, {"type": "text", "text": "Object Detection and Instance Segmentation. The results on MSCOCO are presented in Table 2. VMamba demonstrates superior performance in both box and mask Average Precision $\\mathrm{\\bfAP}^{\\mathrm{b}}$ and $\\mathbf{A}\\mathbf{P}^{\\mathrm{m}},$ ) across different training schedules. Under the 12-epoch fine-tuning schedule, VMambaT/S/B achieves object detection mAPs of $47.3\\%/48.7\\%/49.2\\%$ , outperforming Swin-T/S/B by $4.6\\%/3.9\\%/2.3\\%$ mAP and ConvNeXt-T/S/B by $3.1\\%/3.3\\%/2.2\\%$ mAP, respectively. VMambaT/S/B achieves instance segmentation mAPs that exceed Swin-T/S/B by $3.4\\%/2.8\\%/1.8\\%$ mAP and ConvNeXt-T/S/B by $2.\\bar{6}\\%/1.9\\%/1.4\\%$ mAP, respectively. Furthermore, VMamba\u2019s advantages persist with the 36-epoch fine-tuning schedule using multi-scale training, highlighting its strong potential in downstream tasks requiring dense predictions. ", "page_idx": 6}, {"type": "text", "text": "Semantic Segmentation. Consistent with previous experiments, VMamba demonstrates superior performance in semantic segmentation on ADE20K with a comparable amount of parameters. As shown in Table 2, VMamba-T achieves $3.4\\%$ higher mIoU than Swin-T and $1.\\bar{9}\\%$ higher than ConvNeXt-T in the Single-Scale (SS) setting, and the advantage persists with Multi-Scale (MS) input. For models at the Small and Base levels, VMamba-S/B outperforms NAT-S/B [25] by $2.6\\%/2.\\bar{5}\\%$ mIoU in the SS setting, and $1.7\\%/1.9\\%$ mIoU in the MS setting. ", "page_idx": 6}, {"type": "text", "text": "Discussion The experimental results in this subsection demonstrate VMamba\u2019s adaptability to object detection, instance segmentation, and semantic segmentation. In Figure 4 (a), we compare VMamba\u2019s performance with Swin and ConvNeXt, highlighting its advantages in handling downstream tasks with comparable classification accuracy on ImageNet-1K. This result aligns with Figure 4 (b), where VMamba shows the most stable performance (i.e., modest performance drop) across different input image sizes, achieving a top-1 classification accuracy of $74.7\\%$ without fine-tuning $(79.2\\%$ with linear tuning) at an input resolution of $768\\times768$ . While exhibiting greater tolerance to changes ", "page_idx": 6}, {"type": "image", "img_path": "ZgtLQQR1K7/tmp/fae6a9eb6c514982dac552bdc7c12ecc9986a7f5d1150dc926e709898ed92210.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 4: Illustration of VMamba\u2019s adaptability to (a) downstream tasks and (b) input images with progressively increasing resolutions. Swin- $\\cdot\\Gamma^{*}$ denotes Swin-T tested with scaled window sizes. ", "page_idx": 7}, {"type": "image", "img_path": "ZgtLQQR1K7/tmp/18c9f3320fa316a84ac5148d05ce1729295a4872bd5c048ffb7ae54496f6fbf9.jpg", "img_caption": ["Figure 5: Illustration of VMamba\u2019s resource consumption with progressively increasing resolutions. Swin- $\\cdot\\Gamma^{*}$ denotes Swin-T tested with scaled window sizes. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "in input resolution, VMamba also maintains linear growth in FLOPs and memory-consumption (see Figure 5 (a) and (c)) and maintains high throughput ( Figure 5 (b)), making it more effective and efficient compared to ViT-based methods when adapting to downstream tasks with inputs of larger spatial resolutions. This aligns with Mamba\u2019s advanced capability in efficient long sequence modeling [17]. ", "page_idx": 7}, {"type": "text", "text": "5.3 Analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Relationship between SS2D and Self-Attention. To formulate the response $\\mathbf{Y}$ within the time interval $[a,b]$ of length $T$ , we denote the corresponding SSM-related variables $\\mathbf{u}_{i}\\odot\\Delta_{i}\\in\\mathbb{R}^{1\\times D_{v}}$ , $\\mathbf{B}_{i}\\,\\in\\,\\mathbb{R}^{1\\,\\times\\,D_{k}^{\\star}}$ , and $\\mathbf{\\bar{C}}_{i}\\,\\in\\,\\mathbb{R}^{1\\times D_{k}}$ as $\\mathbf{V}\\,\\in\\,\\mathbb{R}^{T\\times\\dot{\\boldsymbol{D}}_{v}}$ , $\\bar{\\mathbf{K}}\\,\\,\\breve{\\in}\\,\\,\\mathbb{R}^{T\\times D_{k}}$ , and $\\mathbf{Q}\\in\\mathbb{R}^{T\\times D_{k}}$ , respectively. Therefore, the $j$ -th slice along dimension $D_{v}$ of $\\mathbf{y_{b}}$ , denoted as $\\mathbf{y_{b}}^{(j)}\\in\\mathbb{R}$ can be written as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbf{y}_{\\mathbf{b}}^{(j)}=\\left(\\mathbf{Q}_{\\mathbf{T}}\\odot\\mathbf{w}_{\\mathbf{T}}^{(j)}\\right)\\mathbf{h}_{\\mathbf{a}}^{(j)}+\\mathbf{Q}_{\\mathbf{T}}\\sum_{i=1}^{T}\\left(\\frac{\\mathbf{w}_{\\mathbf{T}}^{(j)}}{\\mathbf{w}_{\\mathbf{i}}^{(j)}}\\odot\\mathbf{K}_{\\mathbf{i}}\\right)^{\\top}\\odot\\left(\\mathbf{V}_{\\mathbf{i}}^{(j)}\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "image", "img_path": "ZgtLQQR1K7/tmp/66762962a349d8739e7215977650cc147c0c93e4b0878c84fe5f1aba72fe978e.jpg", "img_caption": ["Figure 6: Illustration of the activation map for query patches indicated by red stars. The visualization results in (b) and (c) are obtained by combining the activation maps from each scanning path in SS2D. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "ZgtLQQR1K7/tmp/2d0bb96bf3faf554e08b078f0113a09c886f97e96aa4908af70ca0e22b2860ee.jpg", "img_caption": ["Figure 7: Comparison of Effective Receptive Fields (ERF) [40] between VMamba and other benchmark models. Pixels with higher intensity indicate larger responses related to the central pixel. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "where $\\mathbf{h_{a}}\\,\\in\\,\\mathbb{R}^{D_{k}}$ is the hidden state at step $a$ , $\\odot$ denotes element-wise product. Particularly, $\\mathbf{V_{i}}^{(j)}$ is only a scalar. The formulation of each element in $\\mathbf{w}:=[\\mathbf{w}_{1};...\\;;\\mathbf{w}_{T}]\\in\\mathbb{R}^{T\\times D_{k}\\times D_{v}}$ , i.e., $\\mathbf{w}_{i}\\,\\in\\,\\mathbb{R}^{D_{k}\\times D_{v}}$ , can be written as $\\begin{array}{r}{\\mathbf{w}_{i}\\,=\\,\\prod_{j=1}^{i}e^{\\mathbf{A}\\Delta_{a-1+j}^{\\top}}}\\end{array}$ , representing the cumulative attention weight at step $i$ computed along the scanning path. ", "page_idx": 8}, {"type": "text", "text": "Consequently, the $j$ -th dimension of $\\mathbf{Y}$ , i.e., $\\mathbf{Y}^{(j)}\\in\\mathbb{R}^{T\\times1}$ , can be expressed as ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbf{Y}^{(j)}=\\left[\\mathbf{Q}\\odot\\mathbf{w}^{(j)}\\right]\\mathbf{h}_{\\mathbf{a}}^{(j)}+\\left[\\left(\\mathbf{Q}\\odot\\mathbf{w}^{(j)}\\right)\\left(\\frac{\\mathbf{K}}{\\mathbf{w}^{(j)}}\\right)^{\\top}\\odot\\mathbf{M}\\right]\\mathbf{V}^{(j)},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\mathbf{M}$ denotes the temporal mask matrix of size $T\\times T$ with the lower triangular part set to 1 and elsewhere 0. Please refer to Appendix $\\mathbf{C}$ for more detailed derivations. ", "page_idx": 8}, {"type": "text", "text": "In Eq. 5, the matrix multiplication process involving $\\mathbf{Q},\\mathbf{K}$ , and $\\mathbf{V}$ closely resembles the self-attention mechanism, despite the inclusion of w. ", "page_idx": 8}, {"type": "text", "text": "Visualization of Activation Maps. To gain an intuitive and in-depth understanding of SS2D, we further visualize the attention values in $\\mathbf{Q}\\mathbf{K}^{\\top}$ and $\\left(\\mathbf{Q}\\odot\\mathbf{w}\\right)\\left(\\mathbf{K}/\\mathbf{w}\\right)^{\\top}$ corresponding to a specific query patch within foreground objects (referred to as the \u2018activation map\u2019). As shown in Figure 6 (b), the activation map of $\\mathbf{Q}\\mathbf{K}^{\\top}$ demonstrates the effectiveness of SS2D in capturing and retaining traversed information, with all previously scanned tokens in the foreground region being activated. Furthermore, the inclusion of w results in activation maps that are more focused on the neighborhood of query patches (Figure 6 (c)), which is consistent with the temporal weighting effect inherent in the formulation of w. Nevertheless, the selective scan mechanism allows VMamba to accumulate history along the scanning path, facilitating the establishment of long-term dependencies across image patches. This is evident in the sub-figure encircled by a red box (Figure 6 (d)), where patches of the sheep far to the left (scanned in earlier steps) remain activated. For more visualizations and further discussion, please refer to Appendix D. ", "page_idx": 8}, {"type": "text", "text": "Visualization of Effective Receptive Fields. The Effective Receptive Field (ERF) [40, 11] refers to the region in the input space that contributes to the activation of a specific output unit. We conduct a comparative analysis of the central pixel\u2019s ERF across various visual backbones, both before and after training. The results presented in Figure 7 illustrate that among the models examined, only DeiT, HiViT, Vim and VMamba demonstrate global ERFs, while the others exhibit local ERFs despite their theoretical potential for global coverage. Moreover, VMamba\u2019s linear time complexity enhances its computational efficiency compared to DeiT and HiViT, which incur quadratic costs w.r.t. the number of input patches. While both VMamba and Vim are based on the Mamba architecture, VMamba\u2019s ERF is more uniform and 2D-aware than that of Vim, which may intuitively explain its superior performance. ", "page_idx": 8}, {"type": "text", "text": "Diagnostic Study on Selective Scan Patterns. We compare the proposed scanning pattern (i.e. Cross-Scan) to three benchmark patterns: unidirectional scanning (Unidi-Scan), bidirectional scanning (Bidi-Scan), and cascade scanning (Cascade-Scan, scanning the data row-wise and column-wise successively). Feature dimensions are adjusted to maintain similar architectural parameters and ", "page_idx": 8}, {"type": "image", "img_path": "ZgtLQQR1K7/tmp/749e85f5fd607244dbf991f68e5d2c8485955bae628a7ff133afdf5cb5adecf0.jpg", "img_caption": ["Figure 8: Performance comparison of different scanning patterns. The proposed Cross-Scan achieves superior performance in speed while maintaining the same number of parameters and FLOPs. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "FLOPs for a fair comparison. As illustrated in Figure 8, Cross-Scan outperforms the other scanning patterns in both computational efficiency and classification accuracy, highlighting its effectiveness in achieving 2D-Selective-Scan. Removing the DWConv layer, which has been shown to aid the model in learning 2D spatial information, further enhances this advantage. This underscores the inherent strength of Cross-Scan in capturing 2D contextual information through its adoption of four-way scanning. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper presents VMamba, an efficient vision backbone model built with State Space Models (SSMs). VMamba integrates the advantages of selective SSMs from NLP tasks into visual data processing, bridging the gap between ordered 1D scanning and non-sequential 2D traversal through the novel SS2D module. Furthermore, we have significantly improved the inference speed of VMamba through a series of architectural and implementation refinements. The effectiveness of the VMamba family has been demonstrated through extensive experiments, and its linear time complexity makes VMamba advantageous for downstream tasks with large-resolution inputs. ", "page_idx": 9}, {"type": "text", "text": "Limitations. While VMamba demonstrates promising experimental results, there is still room for improvement in this study. Previous research has validated the efficacy of unsupervised pre-training on large-scale datasets (e.g., ImageNet-21K). However, the compatibility of existing pre-training methods with SSM-based architectures like VMamba, as well as the identification of pre-training techniques specifically tailored for such models, remain unexplored. Investigating these aspects could serve as a promising avenue for future research in architectural design. Additionally, limited computational resources have prevented us from exploring VMamba\u2019s architecture at the Large scale and conducting a fine-grained hyperparameter search to further enhance experimental performance. Although SS2D, the core component of VMamba, does not make specific assumptions about the layout or modality of the input data, allowing it to generalize across various tasks, the potential of VMamba for integration into more generalized tasks remains unexplored. Bridging the gap between SS2D and these tasks, along with proposing a more generalized scanning pattern for vision tasks, represents a promising research direction. ", "page_idx": 9}, {"type": "text", "text": "7 Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by National Natural Science Foundation of China (NSFC) under Grant No.62225208 and 62406304, CAS Project for Young Scientists in Basic Research under Grant No.YSBR-117, China Postdoctoral Science Foundation under Grant No.2023M743442, and Postdoctoral Fellowship Program of CPSF under Grant No.GZB20240730. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Alaaeldin Ali, Hugo Touvron, Mathilde Caron, Piotr Bojanowski, Matthijs Douze, Armand Joulin, Ivan Laptev, Natalia Neverova, Gabriel Synnaeve, Jakob Verbeek, et al. Xcit: Cross-covariance image transformers. NeurIPS, 34:20014\u201320027, 2021. [2] Guy E Blelloch. Prefix sums and their applications. 1990. [3] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tianheng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue Wu, Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang, Chen Change Loy, and Dahua Lin. Mmdetection: Open mmlab detection toolbox and benchmark. arXiv preprint arXiv:1906.07155, 2019.   \n[4] MMSegmentation Contributors. MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark. https://github.com/open-mmlab/mmsegmentation, 2020. [5] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei. Deformable convolutional networks. In ICCV, pages 764\u2013773, 2017.   \n[6] Zihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan. Coatnet: Marrying convolution and attention for all data sizes. NeurIPS, 34:3965\u20133977, 2021.   \n[7] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. In ICLR, 2023.   \n[8] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memoryefficient exact attention with io-awareness. NeurIPS, 35:16344\u201316359, 2022. [9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. Imagenet: A large-scale hierarchical image database. In CVPR, pages 248\u2013255, 2009.   \n[10] Mingyu Ding, Bin Xiao, Noel Codella, Ping Luo, Jingdong Wang, and Lu Yuan. Davit: Dual attention vision transformers. In ECCV, pages 74\u201392, 2022.   \n[11] Xiaohan Ding, Xiangyu Zhang, Jungong Han, and Guiguang Ding. Scaling up your kernels to 31x31: Revisiting large kernel design in cnns. In CVPR, pages 11963\u201311975, 2022.   \n[12] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan, Dong Chen, and Baining Guo. Cswin transformer: A general vision transformer backbone with cross-shaped windows. In CVPR, pages 12124\u201312134, 2022.   \n[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021.   \n[14] Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. Neural Networks, 107:3\u201311, 2018.   \n[15] Qihang Fan, Huaibo Huang, Mingrui Chen, Hongmin Liu, and Ran He. Rmt: Retentive networks meet vision transformers. In CVPR, 2024.   \n[16] Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher Re. Hungry hungry hippos: Towards language modeling with state space models. In ICLR, 2022.   \n[17] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023.   \n[18] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Hippo: Recurrent memory with optimal polynomial projections. NeurIPS, 33:1474\u20131487, 2020.   \n[19] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. NeurIPS, 35:35971\u201335983, 2022.   \n[20] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. In ICLR, 2021.   \n[21] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. NeurIPS, 34:572\u2013585, 2021.   \n[22] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. NeurIPS, 35:22982\u201322994, 2022.   \n[23] Qi Han, Zejia Fan, Qi Dai, Lei Sun, Ming-Ming Cheng, Jiaying Liu, and Jingdong Wang. On the connection between local attention and dynamic depth-wise convolution. In ICLR, 2021.   \n[24] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. In ICLR, 2022.   \n[25] Ali Hassani, Steven Walton, Jiachen Li, Shen Li, and Humphrey Shi. Neighborhood attention transformer. In CVPR, pages 6185\u20136194, 2023.   \n[26] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask r-cnn. In ICCV, pages 2961\u2013s2969, 2017.   \n[27] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pages 770\u2013778, 2016.   \n[28] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.   \n[29] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In CVPR, pages 4700\u20134708, 2017.   \n[30] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In ICML, pages 5156\u20135165, 2020.   \n[31] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. NeurIPS, pages 1106\u20131114, 2012.   \n[32] Rudolf Emil K\u00e1lm\u00e1n. A new approach to linear filtering and prediction problems. Journal of Basic Engineering, 82(1):35\u201345, 1960.   \n[33] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. In ECCV, pages 740\u2013755, 2014.   \n[34] Shiwei Liu, Tianlong Chen, Xiaohan Chen, Xuxi Chen, Qiao Xiao, Boqian Wu, Tommi K\u00e4rkk\u00e4inen, Mykola Pechenizkiy, Decebal Constantin Mocanu, and Zhangyang Wang. More convnets in the 2020s: Scaling up kernels beyond 51x51 using sparsity. In ICLR, 2023.   \n[35] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al. Swin transformer v2: Scaling up capacity and resolution. In CVPR, pages 12009\u201312019, 2022.   \n[36] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV, pages 10012\u201310022, 2021.   \n[37] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In CVPR, pages 11976\u201311986, 2022.   \n[38] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.   \n[39] Jiasen Lu, Roozbeh Mottaghi, Aniruddha Kembhavi, et al. Container: Context aggregation networks. NeurIPS, 34:19160\u201319171, 2021.   \n[40] Wenjie Luo, Yujia Li, Raquel Urtasun, and Richard Zemel. Understanding the effective receptive field in deep convolutional neural networks. NeurIPS, 29:4898\u20134906, 2016.   \n[41] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer. Mega: Moving average equipped gated attention. In ICLR, 2022.   \n[42] Eric Martin and Chris Cundy. Parallelizing linear recurrent neural nets over sequence length. In ICLR, 2018.   \n[43] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. In ICLR, 2023.   \n[44] Eric Nguyen, Karan Goel, Albert Gu, Gordon Downs, Preey Shah, Tri Dao, Stephen Baccus, and Christopher R\u00e9. S4nd: Modeling images and videos as multidimensional signals with state spaces. NeurIPS, 35:2846\u20132861, 2022.   \n[45] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. RWKV: reinventing rnns for the transformer era. In EMNLP, pages 14048\u201314077, 2023.   \n[46] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Designing network design spaces. In CVPR, pages 10428\u201310436, 2020.   \n[47] Yongming Rao, Wenliang Zhao, Yansong Tang, Jie Zhou, Ser Nam Lim, and Jiwen Lu. Hornet: Efficient high-order spatial interactions with recursive gated convolutions. NeurIPS, 35:10353\u201310366, 2022.   \n[48] Mark Sch\u00f6ne, Neeraj Mohan Sushma, Jingyue Zhuge, Christian Mayr, Anand Subramoney, and David Kappel. Scalable event-by-event processing of neuromorphic sensory signals with deep state-space models. arXiv preprint arXiv:2404.18508, 2024.   \n[49] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. 2015.   \n[50] Jimmy TH Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In ICLR, 2022.   \n[51] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: A successor to transformer for large language models. arXiv preprint arXiv:2307.08621, 2023.   \n[52] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In CVPR, pages 1\u20139, 2015.   \n[53] Mingxing Tan and Quoc V. Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In ICML, pages 6105\u20136114, 2019.   \n[54] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Computing Surveys, 55(6), 2022.   \n[55] Yunjie Tian, Lingxi Xie, Zhaozhi Wang, Longhui Wei, Xiaopeng Zhang, Jianbin Jiao, Yaowei Wang, Qi Tian, and Qixiang Ye. Integrally pre-trained transformer pyramid networks. In CVPR, pages 18610\u2013 18620, 2023.   \n[56] Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al. Mlp-mixer: An all-mlp architecture for vision. Advances in neural information processing systems, 34:24261\u201324272, 2021.   \n[57] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. In ICML, pages 10347\u201310357, 2021.   \n[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 30:5998\u20136008, 2017.   \n[59] Jue Wang, Wentao Zhu, Pichao Wang, Xiang Yu, Linda Liu, Mohamed Omar, and Raffay Hamid. Selective structured state-spaces for long-form video understanding. In CVPR, pages 6387\u20136397, 2023.   \n[60] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020.   \n[61] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In ICCV, pages 568\u2013578, 2021.   \n[62] Ross Wightman. Pytorch image models. https://github.com/rwightman/pytorch-image-models, 2019.   \n[63] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for scene understanding. In ECCV, pages 418\u2013434, 2018.   \n[64] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, and Jianfeng Gao. Focal self-attention for local-global interactions in vision transformers. arXiv preprint arXiv:2107.00641, 2021.   \n[65] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023.   \n[66] Xiaosong Zhang, Yunjie Tian, Lingxi Xie, Wei Huang, Qi Dai, Qixiang Ye, and Qi Tian. Hivit: A simpler and more efficient design of hierarchical vision transformer. In ICLR, 2023.   \n[67] Weixi Zhao, Weiqiang Wang, and Yunjie Tian. Graformer: Graph-oriented transformer for 3d pose estimation. In CVPR, pages 20438\u201320447, 2022.   \n[68] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In CVPR, pages 5122\u20135130, 2017.   \n[69] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang. Vision mamba: Efficient visual representation learning with bidirectional state space model. In ICML, 2024.   \n[70] Xizhou Zhu, Han Hu, Stephen Lin, and Jifeng Dai. Deformable convnets v2: More deformable, better results. In CVPR, pages 9308\u20139316, 2019.   \n[71] Nikola Zubic, Mathias Gehrig, and Davide Scaramuzza. State space models for event cameras. In CVPR, pages 5819\u20135828, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Discretization of State Space Models (SSMs) ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we explore the correlation between the discretized formulations of State Space Models (SSMs) obtained in Sec. 3 and those derived from the zero-order hold (ZOH) method [17], which is frequently used in studies related to SSMs. ", "page_idx": 14}, {"type": "text", "text": "Recall the discretized formulation of SSMs derived in Sec. 3 as follows, ", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\bf h}_{b}=e^{{\\bf A}(\\Delta_{a}+...+\\Delta_{b-1})}\\left({\\bf h}_{a}+\\sum_{i=a}^{b-1}{\\bf B}_{i}u_{i}e^{-{\\bf A}(\\Delta_{a}+...+\\Delta_{i})}\\Delta_{i}\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Let $b=a+1$ , then the above equation can be re-written as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{h}_{a+1}=e^{\\mathbf{A}\\Delta_{a}}\\mathbf{h}_{a}+\\mathbf{B}_{a}\\Delta_{a}u_{a},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\overline{{\\mathbf{A}_{\\mathbf{a}}}}:=e^{\\mathbf{A}\\Delta_{a}}$ is the exact discretized form of the evolution matrix A obtained by ZOH, and $\\overline{{\\mathbf{B}_{\\mathbf{a}}}}:=\\mathbf{B}_{a}\\Delta_{a}$ represents the first-order Taylor expansion of the discretized $\\mathbf{B}$ acquired through ZOH. ", "page_idx": 14}, {"type": "text", "text": "B Derivation of the Recurrence Relation of Selective SSMs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we derive the recurrence relation of the hidden state in selective SSMs. Given the expression of $h_{b}$ shown in Eq. 6, let us denote $e^{\\mathbf{A}(\\Delta_{a}+\\ldots+\\Delta_{i-1})}$ as $\\mathbf{p_{A,a}^{i}}$ . Then, its recurrence relation can be directly written as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{p_{A,a}^{i}}=e^{\\mathbf{A}\\Delta_{i-1}}\\mathbf{p_{A,a}^{i-1}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "For the second term of Eq. 6, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\mathbf{p_{B,a}^{b}}=e^{\\mathbf{A}(\\Delta_{a}+...+\\Delta_{b-1})}\\displaystyle\\sum_{i=a}^{b-1}\\mathbf{B_{i}}u_{i}e^{-\\mathbf{A}(\\Delta_{a}+...+\\Delta_{i})}\\Delta_{i}}}\\\\ {{=e^{\\mathbf{A}\\Delta_{b-1}}\\mathbf{p_{B,a}^{b-1}}+\\mathbf{B_{b-1}}u_{b-1}\\Delta_{b-1}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore, with the associations derived in Eq. 8 and Eq. 9, hb = pbA,a $\\mathbf{h}_{b}=\\mathbf{p}_{\\mathbf{A},\\mathbf{a}}^{\\mathbf{b}}\\mathbf{h}_{a}+\\mathbf{p}_{\\mathbf{B},\\mathbf{a}}^{\\mathbf{b}}$ can be efficiently computed in parallel using associative scan algorithms [2, 42, 50], which are supported by numerous modern programming libraries. This approach effectively reduces the overall computational complexity to linear, and VMamba further accelerates the computation by adopting a hardware-aware implementation [17]. ", "page_idx": 14}, {"type": "text", "text": "C Details of the relationship between SS2D and Self-attention ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we clarify the relationship between SS2D and the self-attention mechanism commonly employed in existing vision backbone models. Subsequently, visualization results are provided to substantiate our explanation. ", "page_idx": 14}, {"type": "text", "text": "Let $T$ denote the length of the sequence with indices from $a$ to $b$ , we define the following variables ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\bf K}:=[{\\bf K}_{1};\\ldots;{\\bf K}_{\\bf T}]\\in\\mathbb{R}^{T\\times D_{k}},\\mathrm{~where~}{\\bf K}_{1}:={\\bf B}_{{\\bf a}+{\\bf i}-1}\\in\\mathbb{R}^{1\\times D_{k}}}\\ ~}\\\\ {{\\displaystyle{\\bf Q}:=[{\\bf Q}_{1};\\ldots;{\\bf Q}_{\\bf T}]\\in\\mathbb{R}^{T\\times D_{k}},\\mathrm{~where~}{\\bf Q}_{1}:={\\bf C}_{{\\bf a}+{\\bf i}-1}\\in\\mathbb{R}^{1\\times D_{k}}}\\ ~}\\\\ {{\\displaystyle{\\bf w}:=[{\\bf w}_{1};\\ldots;{\\bf w}_{\\bf T}]\\in\\mathbb{R}^{T\\times D_{k}\\times D_{v}},\\mathrm{~where~}{\\bf w}_{1}:=\\prod_{j=1}^{i}e^{{\\bf A}\\Delta_{a-1+j}^{\\top}}\\in\\mathbb{R}^{D_{k}\\times D_{v}}}\\ ~}\\\\ {{\\displaystyle{\\bf H}:=[{\\bf h}_{{\\bf a}+1};\\ldots;{\\bf h}_{{\\bf b}}]\\in\\mathbb{R}^{T\\times D_{k}\\times D_{v}},\\mathrm{~where~}{\\bf h}_{\\bf i}\\in\\mathbb{R}^{D_{k}\\times D_{v}}}\\ ~}\\\\ {{\\displaystyle{\\bf Y}:=[{\\bf y}_{{\\bf a}+1};\\ldots;{\\bf y}_{{\\bf b}}]\\in\\mathbb{R}^{T\\times D_{v}},\\mathrm{~where~}{\\bf y}_{\\bf i}\\in\\mathbb{R}^{D_{v}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Note that in practice, the parameter $A$ in Eq. 1 is simplified to $\\mathbb{R}^{1\\times D_{k}}$ . Consequently, $\\mathbf{h}^{\\prime}(t)\\,=$ $\\mathbf{A}\\mathbf{h}(t)+\\mathbf{B}u(t)$ is simplified to $\\mathbf{h}^{\\prime}(t)=\\mathbf{A}\\odot\\mathbf{h}(t)+\\mathbf{B}u(t)$ , which is the reason why $\\mathbf{w_{i}}\\in\\mathbb{R}^{D_{k}\\times D_{v}}$ . ", "page_idx": 14}, {"type": "text", "text": "Based on these notations, the discretized solution of time-varying SSMs (Eq. 6) can be written as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{h}_{\\mathbf{b}}=\\mathbf{w_{T}}\\odot\\mathbf{h}_{\\mathbf{a}}+\\sum_{i=1}^{T}\\frac{\\mathbf{w_{T}}}{\\mathbf{w_{i}}}\\odot\\left(\\mathbf{K_{i}}^{\\top}\\mathbf{V_{i}}\\right),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "denotes the element-wise product between matrices, and the division is also elements-wise. ", "page_idx": 15}, {"type": "text", "text": "Based on the expression of the hidden state $\\mathbf{h_{b}}$ , the first term of the output of SSM, i.e., $\\mathbf{y_{b}}$ , can be computed by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{y_{b}}=\\mathbf{Q_{T}}\\mathbf{h_{b}}}\\\\ &{\\quad=\\mathbf{Q_{T}}\\left(\\mathbf{w_{T}}\\odot\\mathbf{h_{a}}\\right)+\\mathbf{Q_{T}}\\sum_{i=1}^{T}\\frac{\\mathbf{w_{T}}}{\\mathbf{w_{i}}}\\odot\\left(\\mathbf{K_{i}}^{\\top}\\mathbf{V_{i}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Here, we drop the skip connection between the input and the response for simplicity. Particularly, the $j$ -th slice along dimension $D_{v}$ of $\\mathbf{y_{b}}$ , denoted as $\\mathbf{\\bar{y}_{b}}^{(j)}\\in\\mathbb{R}$ can be written as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{y}_{\\mathbf{b}}^{(j)}=\\left(\\mathbf{Q}_{\\mathbf{T}}\\odot\\mathbf{w}_{\\mathbf{T}}^{(j)}\\right)\\mathbf{h}_{\\mathbf{a}}^{(j)}+\\sum_{i=1}^{T}\\left(\\frac{\\mathbf{Q}_{\\mathbf{T}}\\odot\\mathbf{w}_{\\mathbf{T}}^{(j)}}{\\mathbf{w}_{\\mathbf{i}}^{(j)}}\\mathbf{K}_{\\mathbf{i}}^{\\top}\\right)\\odot\\mathbf{V}_{\\mathbf{i}}^{(j)}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Similarly, the $j$ -th slice along dimension $D_{v}$ of the overall response $\\mathbf{Y}$ , denoted as $\\mathbf{Y}^{(j)}\\in\\mathbb{R}^{T\\times1}$ can be expressed as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{Y}^{(j)}=\\left(\\mathbf{Q}\\odot\\mathbf{w}^{(j)}\\right)\\mathbf{h}_{\\mathbf{a}}^{\\mathbf{(}j)}+\\left[\\left(\\mathbf{Q}\\odot\\mathbf{w}^{(j)}\\right)\\left(\\frac{\\mathbf{K}}{\\mathbf{w}^{(j)}}\\right)^{\\top}\\odot\\mathbf{M}\\right]\\mathbf{V}^{(j)},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\mathbf{M}:=\\mathbf{tri}1(T,T)\\in\\{0,1\\}^{T\\times T}$ denotes the temporal mask matrix with the lower triangular portion of a $T\\times T$ matrix set to 1 and elsewhere 0. It is evident that how matrices $\\mathbf{Q},\\mathbf{K}$ , and $\\mathbf{V}$ are multiplied in Eq. 21 closely resembles the process in the self-attention module of Vision Transformers. Moreover, if w is in shape $(T,D_{k})$ rather than $(T,D_{k},D_{v})$ , then Eq. 18 and Eq. 21 reduce to the form of Gated Linear Attention (GLA) [65], indicating that GLA is also a special case of Mamba. ", "page_idx": 15}, {"type": "text", "text": "D Visualization of Attention and Activation Maps ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In the preceding subsection, we illustrated how the computational process of selective SSMs shares similarities with self-attention mechanisms, allowing us to delve into the internal mechanism of SS2D through the visualization of its weight matrices. ", "page_idx": 15}, {"type": "text", "text": "Given the input image shown in Figure 9 (a), illustrations of four scanning paths in SS2D are presented in Figure 9 (d). The visualizations of the corresponding attention maps, calculated using $\\dot{\\mathbf{Q}}\\mathbf{K}^{\\top}$ and $\\left(\\mathbf{Q}\\odot\\mathbf{w}\\right)\\left(\\mathbf{K}/\\mathbf{w}\\right)^{\\top}$ are shown in Figure 9 (e) and Figure 9 (g) respectively. These results underscore the effectiveness of the proposed scanning approach (i.e., Cross-Scan) in capturing and retaining the traversed information, as each row in a single attention map corresponds to the attention between the current patch and all previously scanned foreground tokens impartially. Additionally, in Figure 9 (f), we showcase the transformed activation maps, where the pixel order corresponds to that of the first route, traversing the image row-wise from the upper-left to the bottom-right. ", "page_idx": 15}, {"type": "text", "text": "By rearranging the diagonal elements of the obtained attention map in the image space, we derive the visualization results shown in Figure 9 (b) and Figure 9 (c) corresponding to $\\mathbf{Q}\\mathbf{K}^{\\top}$ and $\\left(\\mathbf{Q}\\odot\\mathbf{w}\\right)\\left(\\mathbf{K}/\\mathbf{w}\\right)^{\\top}$ respectively. These maps illustrate the effectiveness of VMamba in accurately distinguishing between foreground and background pixels within an image. ", "page_idx": 15}, {"type": "text", "text": "Moreover, given a selected patch as the query, we visualize the corresponding activation map by reshaping the associated row in the attention map (computed by $\\mathbf{Q}\\mathbf{K}^{\\top}$ or $(\\mathbf{Q}\\odot\\mathbf{w})\\,(\\mathbf{K}/\\mathbf{w})^{\\top};$ This reflects the attention score between the query patch and all previously scanned patches. To obtain the complete visualization for a query patch, we collect and combine the activation maps from all four scanning paths in SS2D. The visualization results of the activation map for both $\\mathbf{\\Psi}_{\\mathbf{Q}\\mathbf{K}}^{\\top}\\,.$ and $\\left(\\mathbf{Q}\\odot\\mathbf{w}\\right)\\left(\\mathbf{K}/\\mathbf{w}\\right)^{\\top}$ are shown in Figure 10. We also visualize the diagonal elements of attention maps computed by $\\left(\\mathbf{Q}\\odot\\mathbf{w}\\right)\\left(\\mathbf{K}/\\mathbf{w}\\right)^{\\top}$ , where all foreground objects are effectively highlighted and separated from the background. ", "page_idx": 15}, {"type": "image", "img_path": "ZgtLQQR1K7/tmp/442c36ebc868953e3ca5ef4e065bf37f180178eb78c4b99f67a6592a6ce066fd.jpg", "img_caption": ["Figure 9: Illustration of the attention maps obtained by SS2D. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "ZgtLQQR1K7/tmp/acd7ba3c74ddb21410c22524e72aba4e9d86079f1cb74be68c867507dd0d5daa.jpg", "img_caption": ["Figure 10: Illustration of activation maps for the query patch (marked with a red star). "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "E Detailed Experiment Settings ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Network Architecture. The architectural specifications of Vanilla-VMamba are outlined in Table 3, while detailed configurations of the VMamba series are provided in Table 4. The Vanilla-VMamba series is constructed using the vanilla VSS Block, which includes a multiplicative branch and does not have feed-forward network (FFN) layers. In contrast, the VSS Block in the VMamba series removes the multiplicative branch and introduces FFN layers. Additionally, we provide alternative architectures for VMamba at Small and Base scales, referred to as VMamba-S[s1l20] and VMambaB[s1l20], respectively. The notation \u2018sxly\u2019 indicates that the ssm-ratio is set to $x$ and the number of layers in stage 3 is set to $y$ . Consequently, the versions presented in Table 1 can also be referred to as VMamba-S[s2l15] and VMamba-B[s2l15]. ", "page_idx": 17}, {"type": "text", "text": "Experiment Setting. The hyper-parameters for training VMamba on ImageNet are inherited from Swin [36], except for the parameters related to drop_path_rate and the exponential moving average (EMA) technique. Specifically, VMamba-T/S/B models are trained from scratch for 300 epochs, with a 20-epoch warm-up period, using a batch size of 1024. The training process utilizes the AdamW optimizer [38] with betas set to (0.9, 0.999), an initial learning rate of $\\dot{1}\\times10^{-3}$ , a weight decay of 0.05, and a cosine decay learning rate scheduler. It is noteworthy that this is not the optimal setting for VMamba. With a learning rate of $2\\times10^{-3}$ , the Top-1 accuracy of VMamba-T can reach $80.7\\%$ . ", "page_idx": 17}, {"type": "text", "text": "Additional techniques such as label smoothing (0.1) and EMA (decay ratio of 0.9999) are also applied. The drop_path_ratio is set to 0.2 for Vanilla-VMamba-T and VMamba-T, 0.3 for Vanilla-VMamba-S, VMamba-S[s2l15] and VMamba-S[s1l20], 0.6 for Vanilla-VMamba-B and VMamba-B[s2l15], and 0.5 for VMamba-B[s1l20]. No additional training techniques are employed. ", "page_idx": 17}, {"type": "text", "text": "Throughput Evaluation. Detailed performance comparisons with various models are presented in Table 6. Throughput (referred to as TP.) was assessed on an A100 GPU paired with an AMD EPYC 7542 CPU, utilizing the toolkit provided by [62]. Following the protocol outlined in [36], we set the batch size to 128. The training throughput (referred to as Train TP.) is tested on the same device with mix-resolution, excluding the time consumption of optimizers. The batch size for measuring the training throughput is also set to 128. ", "page_idx": 17}, {"type": "text", "text": "Accelerating VMamba. Table 5 provides detailed configurations of the intermediate variants in the acceleration process from Vanilla-VMamba-T to VMamba-T. ", "page_idx": 17}, {"type": "text", "text": "Evolution of ERF. We further generate the effective receptive field (ERF) maps throughout the training process for Vanilla-VMamba-T. These maps intuitively illustrate how VMamba\u2019s pattern of ERF evolves from being predominantly local to predominantly global, epoch by epoch. ", "page_idx": 17}, {"type": "text", "text": "F Performance of the VMamba Family on Downstream Tasks ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we present the experimental results of Vanilla-VMamba and VMamba on the MSCOCO and ADE20k datasets. The results are summarized in Table 7 and Table 8, respectively. ", "page_idx": 17}, {"type": "text", "text": "For object detection and instance segmentation, we adhere to the protocol outlined by Swin [36] and construct our models using the mmdetection framework [3]. Specifically, we utilize the AdamW optimizer [38] and fine-tune the classification models pre-trained on ImageNet-1K for both 12 and 36 epochs. The learning rate is initialized at $1\\times10^{-\\bar{4}}$ and decreased by a factor of 10 at the 9-th ", "page_idx": 17}, {"type": "text", "text": "Table 3: Architectural overview of the Vanilla-VMamba series. Down-sampling is performed through patch merging [36] operations in stages 1, 2, and 3. The term Linear refers to a linear layer, while DWConv denotes a depth-wise convolution [23] operation. The proposed 2D-selective-scan is labeled as SS2D. ", "page_idx": 18}, {"type": "table", "img_path": "ZgtLQQR1K7/tmp/f984ffdce1318525eef3269dbfe7d6343c95fb5b761da328339c51b9ea69a8cf.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "and $11{-}t h$ epoch. We incorporate multi-scale training and random flipping with a batch size of 16, following established practices for object detection evaluations. ", "page_idx": 18}, {"type": "text", "text": "For semantic segmentation, we follow Swin [36] and construct a UperHead [63] network on top of the pre-trained model using the MMSegmentation library [4]. We employ the AdamW optimizer [38] and set the learning rate to $6\\times10^{-5}$ . The fine-tuning process spans a total of $160k$ iterations with a batch size of 16. The default input resolution is $512\\times512$ . ", "page_idx": 18}, {"type": "text", "text": "G Details of VMamba\u2019s Scale-Up Experiments ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Given Mamba\u2019s exceptional ability in efficient long sequence modeling, we conduct experiments to assess whether VMamba inherits this characteristic. We evaluate the computational efficiency and classification accuracy of VMamba with progressively larger input spatial resolutions. Specifically, following the protocol in XCiT [1], we apply VMamba, trained on $224\\times224$ inputs, to images with resolutions ranging from $288\\times288$ to $768\\times768$ . We measure the generalization performance in terms of the number of parameters, FLOPs, throughput during both training and inference, and the top-1 classification accuracy on ImageNet-1K. We also conduct experiments under the \u2018linear tuning\u2019 setting, where only the header network, consisting of a single linear module, is fine-tuned from random initialization using features extracted by the backbone models. ", "page_idx": 18}, {"type": "table", "img_path": "ZgtLQQR1K7/tmp/39d713b9a4c2811d1222d2efcd66b4de15bd10766128599ef950f681ff16834a.jpg", "table_caption": ["Table 4: Architectural overview of the VMamba series. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "ZgtLQQR1K7/tmp/d9b8d7e1f92d0af09d8e80f567c1f59591d56e2707ba5cfe344b1f160e4f7a72.jpg", "table_caption": ["Table 5: Details of accelerating VMamba. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "According to the results summarized in Table 9, VMamba demonstrates the most stable performance across (i.e., modest performance drop) different input image sizes, achieving a top-1 classification accuracy of $74.7\\%$ without fine-tuning $(79.2\\%$ with linear tuning), while maintaining a relatively high throughput of 149 images per second at an input resolution of $768\\times768$ . In comparison, Swin [36] achieves the second-highest performance with a top-1 accuracy of $73.1\\%$ without finetuning $(77.5\\%$ under linear tuning) at the same input size, using scaled window sizes (set as the resolution divided by 32). However, its throughput significantly drops to 53 images per second. Furthermore, ConvNeXt [37] maintains a relatively high inference speed (i.e., a throughput of 103 images per second) at the largest input resolution. However, its classification accuracy drops to $69.5\\%$ when directly tested on images of size $768\\times768$ , indicating its limited adaptability to images with large spatial resolutions. Deit-S also shows a dramatic performance drop, primarily due to the interpolation used in the absolute positional embedding. ", "page_idx": 19}, {"type": "text", "text": "Table 6: Performance comparison on ImageNet-1K with an image size of 224. \u2020 indicates that Vim is trained solely in float32 in practice, with a training throughput of 232. [69]. ", "page_idx": 20}, {"type": "table", "img_path": "ZgtLQQR1K7/tmp/98862036e40c0fe8797bcdddcfe32f6b66e5f994be11d0bf2000f937f7813fdd.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Notably, VMamba displays a linear increase in computational complexity, as measured by FLOPs, which is comparable to CNN-based architectures. This finding aligns with the theoretical conclusions drawn from selective SSMs [17]. ", "page_idx": 20}, {"type": "text", "text": "H Ablation Study ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "H.1 Influence of the Scanning Pattern ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In the main submission, we validate the effectiveness of the proposed scanning pattern (referred to as Cross-Scan) in SS2D by comparing it to three alternative image traversal approaches, i.e., Unidi-Scan, Bidi-Scan, and Cascade-Scan (Figure 12). Notably, since Unidi-Scan, Bidi-Scan, and Cross-Scan are all implemented in Triton, they exhibit minimal differences in throughput. The results in Table 10 indicate that Cross-Scan demonstrates superior data modeling capacity, as reflected by its higher classification accuracy. This advantage likely stems from the two-dimensional prior introduced by the four-way scanning design. Nevertheless, the practical implementation of Cascade-Scan is significantly constrained by its relatively slow computational pace, primarily due to the inadequate compatibility between selective scanning and high-dimensional data, which is further affected by the multi-step scanning procedure. ", "page_idx": 20}, {"type": "text", "text": "Figure 13 indirectly demonstrates that among the analyzed scanning methods, only Bidi-Scan, Cascade-Scan, and Cross-Scan showcase global ERFs. Moreover, only Cross-Scan and Cascade-Scan exhibit two-dimensional (2D) priors. It is also worth noting that DWConv [23] plays a critical role in establishing 2D priors, thereby contributing to the formation of global ERFs. ", "page_idx": 20}, {"type": "text", "text": "H.2 Influence of the Initialization Approach ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In our study, we adopted the initialization scheme originally proposed for the SS2D block in S4D [19]. Therefore, it is necessary to investigate the contribution of this initialization method to the effective", "page_idx": 20}, {"type": "text", "text": "Table 7: Object detection and instance segmentation results on COCO dataset. FLOPs are calculated using inputs of size $1280\\times800$ . Here, $\\bar{A}P^{b}$ and $A P^{m}$ denote box AP and mask AP, respectively. $\"1\\times\"$ indicates models fine-tuned for 12 epochs, while $\\mathrm{^{\\prime}3\\!\\times\\!M S\"}$ signifies the utilization of multi-scale training for 36 epochs. ", "page_idx": 21}, {"type": "table", "img_path": "ZgtLQQR1K7/tmp/44ffc684c1bae26645813a111e0faff384a297d9f61a3de1ccec9843ef792a1a.jpg", "table_caption": ["Mask R-CNN $\\mathbf{1}\\times$ schedule "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "ZgtLQQR1K7/tmp/8925270290f626d7757a2d43a573017639ad5ad5a2d18d8ff9d8e04a373487b9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "ZgtLQQR1K7/tmp/75d5479120ae8405300ef8abd6a54ef507ed85f5cf6f44111bce0179fee8bbfc.jpg", "img_caption": ["Figure 12: Illustration of different scanning patterns for selective scan. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "ness of VMamba. To explore this further, we replaced the default initialization with two alternative methods: random initialization and zero initialization. ", "page_idx": 21}, {"type": "text", "text": "For both initialization methods, we set the parameter $\\mathbf{D}$ in equation 1 to a vector of all ones, mimicking a basic skip connection (thus we have $\\mathbf{y}=\\mathbf{C}\\mathbf{h}\\mathbf{+}\\mathbf{D}\\mathbf{u})$ . Additionally, the weights and biases associated with the transformation to the dimension $D_{v}$ (which matches the input size), are initialized as random vectors. In contrast, Mamba [17] employs a more sophisticated initialization. ", "page_idx": 21}, {"type": "text", "text": "Table 8: Semantic segmentation results on ADE20K using UperNet [63]. We evaluate the performance of semantic segmentation on the ADE20K dataset with UperNet [63]. FLOPs are calculated with input sizes of $512\\times2048$ . \"SS\" and \"MS\" denote single-scale and multi-scale testing, respectively. ", "page_idx": 22}, {"type": "table", "img_path": "ZgtLQQR1K7/tmp/fd2a2679c0717148262ae86d400b738ce78a27276b63d7ef5e087cac9d7827b4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "ZgtLQQR1K7/tmp/b44c372ed61918bc1548f27f3730978779490e685f21e26917da192a9632162e.jpg", "img_caption": ["Figure 13: The visualization of ERF for models with different scanning patterns. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "The main distinction between random and zero initialization lies in the parameter $A$ in equation 6, which is typically initialized as a HiPPO matrix in both Mamba [17, 19] and our implementation of VMamba. Given that we selected the hyper-parameter d_state to be 1, the Mamba initialization for $l o g(A)$ can be simplified to all zeros, which aligns with zero initialization. In contrast, random initialization assigns a random vector to $l o g(A)$ . We choose to initialize $l o g(A)$ rather than $A$ directly to keep $A$ near the all-ones matrix when the network parameters are close to zero, which empirically enhances the training stability. ", "page_idx": 22}, {"type": "text", "text": "The experimental results in Table 11 indicate that, at least for image classification with SS2D blocks, the model\u2019s performance is not significantly affected by the initialization method. Therefore, within this context, the sophisticated initialization method employed in Mamba [17] can be substituted with a simpler, more straightforward approach. We also visualize the ERF maps of models trained with different initialization methods (see Figure 14), which intuitively reflect SS2D\u2019s robustness across various initialization schemes. ", "page_idx": 22}, {"type": "text", "text": "H.3 Influence of the d_state Parameter ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Throughout this study, we primarily set the value of d_state to 1 to optimize VMamba\u2019s computational speed. To further explore the impact of d_state on the model\u2019s performance, we conduct a series of experiments. ", "page_idx": 22}, {"type": "text", "text": "As shown in Table 12, with all other hyper-parameters fixed, we increase d_state from 1 to 4. This results in a slight improvement in performance but a substantial decrease in throughput, indicating a significant negative impact on the VMamba\u2019s computational efficiency. However, increasing d_state to 8, while reducing ssm-ratio to maintain computational complexity, leads to improved accuracy. Moreover, when d_state is further increased to 16, with ssm-ratio set to 1, performance declines. These findings suggest that modest increases in d_state may not necessarily lead to better performance. Instead, selecting the optimal combination of d_state and ssm-ratio is crucial for achieving a good trade-off between inference speed and performance. ", "page_idx": 22}, {"type": "image", "img_path": "ZgtLQQR1K7/tmp/b54fbd0b218332c6911d1df72c823f0b5c993395480e443525dc7bdc6db824ea.jpg", "img_caption": ["Figure 14: The visualization of ERF of VMamba with different initialization. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "H.4 Influence of ssm-ratio, mlp-ratio, and layer numbers ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section, we investigate the trade-offs among ssm-ratio, layer numbers, and mlp-ratio. ", "page_idx": 23}, {"type": "text", "text": "Experimental results shown in Table 13 indicate that reducing ssm-ratio significantly decreases performance but substantially improves inference speed. Conversely, increasing layer numbers enhances the performance while slowing down the model. ", "page_idx": 23}, {"type": "text", "text": "As the hyper-parameter ssm-ratio represents the dimension used by the SS2D module, the trade-off between ssm-ratio and layer numbers can be interpreted as a balance between channel-mixing and token-mixing [56]. Furthermore, we reduce mlp-ratio from 4.0 to 2.0 and progressively increase ssm-ratio to maintain constant FLOPs, as shown in Table 14. The results presented in Tables 13 and 14 highlight the importance of an optimal combination of ssm-ratio, mlp-ratio, and layer numbers for constructing a model that balances effectiveness and efficiency. ", "page_idx": 23}, {"type": "text", "text": "H.5 Influence of the Activation Function ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In VMamba, the SiLU [14] activation function is utilized to build the SS2D block. However, experimental results in Table 15 show that VMamba maintains robustness across different activation functions. This implies that the choice of activation function does not substantially affect the model\u2019s performance. Therefore, there is flexibility to choose an appropriate activation function based on computational constraints or other preferences. ", "page_idx": 23}, {"type": "table", "img_path": "ZgtLQQR1K7/tmp/8186e5d44f76892ee312d30fee2277a8ea5f266245f02fc0c5cff4badc53df28.jpg", "table_caption": ["Table 9: Comparison of generalizability to inputs with increased spatial resolutions. The throughput and training throughput are measured with a batch size of 32 using PyTorch 2.0 on an A100 GPU paired with an AMD EPYC 7542 CPU. Unlike throughput, the model\u2019s forward pass, loss calculation, and backward pass are included in calculating the training throughput, with mixed precision. We re-implemented the HiViT-T, as the checkpoint for HiViT-T has not been released. $\\dagger$ denotes that the batch size $\\leq16$ due to out-of-memory (OOM) issues. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "Table 10: The performance of VMamba-T with different scanning patterns. ", "page_idx": 25}, {"type": "table", "img_path": "ZgtLQQR1K7/tmp/1d525efdd9a0445c29ec50a28bf797f0663def687101654bc67b4e582a59b230.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "Table 11: The performance of VMamba-T with different initialization. ", "page_idx": 25}, {"type": "table", "img_path": "ZgtLQQR1K7/tmp/b3efd160a8b19fa9defd3da8fedafd0bcde830e36a227b5b4c7df340ff7b9aff.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "ZgtLQQR1K7/tmp/499a78027437d5e7a1ea6c91460b3d7019791b5ee7cbdb2023f925f14692a796.jpg", "table_caption": ["Table 12: The performance of VMamba-T with different d_state. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "Table 13: The performance of VMamba-T under different combination of ssm-ratio and layer numbers. ", "page_idx": 25}, {"type": "table", "img_path": "ZgtLQQR1K7/tmp/e73e3369e54ff14f38b11773e7cced4b09346a6492d3b4b9e8083000156ce7da.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "Table 14: The performance of VMamba under different combination of ssm-ratio and mlp-ratio. ", "page_idx": 25}, {"type": "table", "img_path": "ZgtLQQR1K7/tmp/a141d6d4f4b85886de567df3fa62a8d06a7a92efde6b6d3e6cecca80fc210e94.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "Table 15: The performance of VMamba-T with different activation functions in SS2D. ", "page_idx": 25}, {"type": "table", "img_path": "ZgtLQQR1K7/tmp/a5c261d827dd7dce62adab73f85aa41cad6fb228a009359a8b3173531830ddd5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The abstract and introduction provide a comprehensive overview of the background and motivation of this study, effectively outlining its main contributions pointby-point, thus accurately reflecting the paper\u2019s scope and significance. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 26}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We primarily focused on discussing the limitations associated with this study in section 6. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper includes the full set of assumptions and correct proofs for each theoretical result, primarily presented in the appendix. Notably, it covers the formulation of State Space Models, the discretization process, the derivation of recurrence relationships, and explanations concerning self-attention computations, ensuring completeness and accuracy in theoretical presentation. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: All information regarding the key contribution of this paper, i.e., the introduction of selective SSMs (or S6) to processing vision data, as well as architectural and experimental configurations, have been fully disclosed (to the extent that it affects the main claims and/or conclusions of the paper). Furthermore, the implementation of other components within the proposed VMamba framework, such as Vision Transformers and the parallelized Selective Scan operation, is facilitated by the plenty of support available from existing open-source resources within the community. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce ", "page_idx": 27}, {"type": "text", "text": "the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The supplementary material submitted with the manuscript includes open access to all source code and scripts necessary to faithfully reproduce the main experimental results. Instructions for running the code are also provided within the scripts. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: The paper specifies detailed experimental configurations in Section E in Appendix, providing readers with essential information to comprehend the results. Following established conventions in the field of vision backbone models, the evaluation protocol encompasses standard practices commonly found in the relevant literature, ensuring readers can refer to established methodologies. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [No] ", "page_idx": 29}, {"type": "text", "text": "Justification: We did not include an analysis of the statistical significance of the experiments mainly due to the prohibitively expensive training cost of vision backbone models and our limited computing resources. However, we have provided the code, hyperparameters, and random seeds used in our experiments to facilitate the reproducibility of our findings. We would like to point out that, due to the extensive amount of training data, the statistical patterns of the experiment results are likely to remain consistent across different trials. Consequently, reporting error bars or other information about statistical significance is not a common practice in studies developing deep vision backbones. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: All experiments were carried out on an $8\\times\\mathrm{Al00}$ GPU server, as detailed at the beginning of the experiment section (Section 5). ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: After carefully reviewing the referenced document, we certify that the research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 30}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper primarily focuses on vision backbones trained using publicly available datasets that have undergone thorough validation. While the vision backbone itself is not directly applicable to everyday scenarios, it serves as a neutral and valuable toolkit for further development and research. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The proposed models are vision backbone networks trained on benchmark datasets such as ImageNet-1K, MSCOCO, and ADE20K. These datasets have been extensively used in the computer vision community and have undergone comprehensive safety risk assessments. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: In the paper, we specified the datasets and code sources used (e.g., mmdet), and provided appropriate citations in the reference section. Additionally, we ensured transparency by including the sources of any modified code files, making the changes traceable. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: We have included the code, along with detailed usage instructions, in the supplementary materials. After the review process is completed, we will make the code publicly available to the community. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: This study does not involve any crowdsourcing experiments or research with human subjects. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 32}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: No crowdsourcing experiments or research with human subjects were involved in this study. All experiments were conducted using code and GPU servers. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}]