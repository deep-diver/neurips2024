[{"type": "text", "text": "Statistical Multicriteria Benchmarking via the GSD-Front ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Christoph Jansen1,\u2217 Georg Schollmeyer2,\u2217 c.jansen@lancaster.ac.uk georg.schollmeyer@stat.uni-muenchen.de ", "page_idx": 0}, {"type": "text", "text": "Julian Rodemann2,\u2217 Hannah Blocher2,\u2217 julian@stat.uni-muenchen.de hannah.blocher@stat.uni-muenchen.de ", "page_idx": 0}, {"type": "text", "text": "Thomas Augustin2 thomas.augustin@stat.uni-muenchen.de ", "page_idx": 0}, {"type": "text", "text": "1School of Computing & Communications   \nLancaster University Leipzig   \nLeipzig, Germany   \n2Department of Statistics   \nLudwig-Maximilians-Universit\u00e4t M\u00fcnchen   \nMunich, Germany ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Given the vast number of classifiers that have been (and continue to be) proposed, reliable methods for comparing them are becoming increasingly important. The desire for reliability is broken down into three main aspects: (1) Comparisons should allow for different quality metrics simultaneously. (2) Comparisons should take into account the statistical uncertainty induced by the choice of benchmark suite. (3) The robustness of the comparisons under small deviations in the underlying assumptions should be verifiable. To address (1), we propose to compare classifiers using a generalized stochastic dominance ordering (GSD) and present the GSD-front as an information-efficient alternative to the classical Pareto-front. For (2), we propose a consistent statistical estimator for the GSD-front and construct a statistical test for whether a (potentially new) classifier lies in the GSD-front of a set of state-of-the-art classifiers. For (3), we relax our proposed test using techniques from robust statistics and imprecise probabilities. We illustrate our concepts on the benchmark suite PMLB and on the platform OpenML. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The comparison of classifiers in machine learning is usually carried out using quality metrics $\\phi:{\\mathcal{C}}\\times{\\bar{D}}\\to[0,1]$ , i.e., bounded functions assigning a real number to every pair $(C,D)$ of classifier and data set from a suitable domain ${\\mathcal{C}}\\times{\\mathcal{D}}$ , where, by construction, higher numbers indicate better quality. However, in many applications, the choice of a unique quality metric used for the comparison is not self-evident. Instead, competing quality metrics are available, each of which can be wellmotivated but may lead to a different ranking of the analyzed classifiers. One attempt to safeguard against this effect is to use multidimensional quality metrics: instead of a single metric, one chooses a set of metrics $\\Phi:=(\\phi_{1},\\dots,\\phi_{n}):{\\mathcal{C}}\\times{\\mathcal{D}}^{*}\\to[0,1]^{n}$ that \u2013 taken together \u2013 provide a balanced foundation for assessing the quality of classifiers. Generally, we distinguish two (related but) different motivations for choosing multidimensional quality metrics: ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Performance is a latent construct: The application at hand suggests a very clear evaluation concept, which, however, is too complex to be expressed in terms of a single metric. In this case, the latent construct to evaluate is operationalized with a set of quality metrics (that serve as an approximation). For example, the latent construct of robust accuracy can be operationalized by taking together the following three quality metrics: Accuracy of a classifier (i.e., the proportion of correctly predicted labels), and robustness of this proportion under weak perturbations of the data in either the features or the target variable. This will be exemplified in Section 5.2 using the PMLB benchmark suite. ", "page_idx": 1}, {"type": "text", "text": "Quality is a multidimensional concept: Even if the application at hand suggests evaluation criteria that can be perfectly expressed using quality metrics, it can still be desirable to compare the classifiers under consideration in terms of various contentual dimensions. For example, one can be interested in how well a classifier performs in the trade-off between accuracy and computation time in the training and the test phase: Clearly distinguishable contentual dimensions are included and the analysis aims at investigating how the different classifiers under consideration trade-off between these dimensions. This will be exemplified in Section 5.1 using one of OpenML\u2019s benchmark suites. ", "page_idx": 1}, {"type": "text", "text": "Regardless of the motivation for considering multidimensional quality metrics, their interpretative advantage naturally comes at a price: Without further assumptions, classifiers will often be incomparable, as the quality metrics in the different dimensions contradict each other in their ranking.2 Already on one data set, a multidimensional quality metric only induces a (natural yet potentially incomplete) preorder: a classifier is rated at least as good as a competitor if (and only if) it receives at least the same metric value in each dimension. The problem of incomparability becomes even more severe for multiple data sets (as considered here). In this case, one of the following analysis paths is often chosen: (I) An expected weighted sum (for example, weighted by importance) of the individual quality metrics is considered and the problem is then analyzed on this new pooled quantity.3 (II) The problem is analyzed based on the Pareto-front $\\operatorname{par}(\\Phi)$ , i.e., the set of all classifiers that are not component-wise (strictly) dominated by any competitor, whose definition followed by an illustrative example are included for reference. ", "page_idx": 1}, {"type": "text", "text": "Definition 1. Let $\\tilde{\\mathcal{D}}\\subseteq\\mathcal{D}$ be some set of data sets. The $\\tilde{\\mathcal{D}}$ -Pareto front par $\\cdot(\\Phi,\\tilde{\\mathcal{D}})$ of $\\Phi$ is given by ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\big\\{C\\in\\mathcal{C}|\\nexists C^{\\prime}\\in\\mathcal{C}\\,\\forall D\\in\\tilde{\\mathcal{D}}:\\;\\Phi(C^{\\prime},D)>\\Phi(C,D)\\big\\},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\gtrdot$ is the strict part of the component-wise $\\geq$ -relation on $\\mathbb{R}^{n}$ . Se $\\iota\\,p a r(\\Phi):=\\!p a r(\\Phi,{\\cal D})$ . ", "page_idx": 1}, {"type": "text", "text": "Example 1. Consider the following schematic example of three classifiers $\\mathcal{C}\\;=\\;\\{C_{1},C_{2},C_{3}\\}$ evaluated for a fictitious population of four data sets $\\mathcal{D}=\\{D_{1},D_{2},D_{3},D_{4}\\}$ . Every entry gives the two-dimensional evaluation $\\Phi(C,D)$ of a classifier on a data set w.r.t. predictive accuracy and the computation time for training in three ordinal categories $f a s t$ , medium and $s\\,\\!\\ell\\,o\\,\\!\\psi$ ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\overbrace{\\phantom{(0.51d e r,0.51d e)}_{C_{1}}\\!\\!\\!C_{1}}^{\\substack{d a t a\\,s e t}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!D_{1}\\qquad\\qquad\\qquad\\qquad\\!\\!\\!D_{2}\\qquad\\qquad\\qquad\\qquad\\!\\!D_{3}\\qquad\\qquad\\qquad\\quad D_{4}}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Here, it holds that $\\Phi(C_{2},D_{i})>\\Phi(C_{1},D_{i})$ for all $i=1,2,3,4,$ , i.e., $C_{1}$ is component-wise (strictly) dominated by $C_{2}$ . Classifiers $C_{2}$ and $C_{3}$ are not component-wise (strictly) dominated. Thus, the Pareto-front is given by $p a r(\\Phi)=\\{C_{2},C_{3}\\}$ . ", "page_idx": 1}, {"type": "text", "text": "Both approaches are extreme in a certain sense: (I) reduces the multidimensional information structure of the problem to one single real-valued score. Any selection of classifier based on this score will heavily depend on the choice of the weights in the sum score and, therefore, becomes dubious once this choice is not perfectly justified. This seems even more severe for problems where some of the involved quality metrics might only allow for an ordinal interpretation, e.g., feature sparseness as a proxy for interpretability [75], risk levels in the EU AI act [50] or other regulatory frameworks [73], robustness (see experiments in Section 5.2) or runtime levels (Section 5.1). Opposed to this, (II) seems to be very conservative: By considering classifiers that are in the Pareto-front $\\operatorname{par}(\\Phi)$ , one (potentially) completely ignores both information encoded in the cardinally interpretable dimensions and information about the distribution of the data sets. As a trade-off between these two extremes, which utilizes the complete available information but avoids the choice of weights, it has recently been proposed to compare classifiers under multidimensional quality metrics using generalized stochastic dominance (GSD) [45]. The rough idea of this approach is to first embed the range of the multivariate performance measure in a special type of relational structure, a so-called preference system, which then allows for also formalizing the entire information originating from the cardinal dimensions of the quality metric. A classifier is then judged at least as good as a competitor (similar to classic stochastic dominance), if its expected utility is at least as high with respect to every utility function representing (both the ordinal and the cardinal parts of) the preference system (also see Definition 5). Although GSD also induces only a preorder, the set of not strictly dominated classifiers will generally be considerably smaller than under the classical Pareto analysis. Furthermore, it avoids potentially difficult to justify assumptions about the weighting of the different quality metrics. Therefore, working with the GSD-front, as introduced below, will prove to be a very promising analysis option; it combines the advantages of the conservative Pareto analysis with those of the liberal comparison of weighted sums. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "1.1 Our contribution ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "GSD-Front: We introduce the concept of the GSD-front (see, after some preparations, Definition 6) and characterize it in Theorem 2 as more discriminative than the Pareto-front. In this sense, the GSD-front is an information-efficient way to handle the multiplicity/implicit multidimensionality of quality criteria, powerfully exploiting their ordinal and quantitative components. ", "page_idx": 2}, {"type": "text", "text": "Proper handling of statistical uncertainty; estimating and testing: Since typically the available data sets are just a sample of the corresponding universe, empirical counterparts of the major concepts are needed to do justice to the underlying statistical uncertainty. In particular, we give a sound inference framework: Firstly, we propose a set-valued estimator for the GSD-front and provide sufficient conditions for its consistency (see Theorem 1 and Remark 3). Secondly, we develop static and dynamic statistical permutation-tests if a classifier is in the GSD-front and prove their level- $\\alpha-$ validity and their consistency (see Theorem 3). ", "page_idx": 2}, {"type": "text", "text": "Robustification: Additionally, we recognize the fact that the underlying assumption of identically and independently distributed (i.i.d.) sampling is questionable in many benchmarking studies. Thus, in Section 4.2 we quantify how robust the test decisions are under such deviations. ", "page_idx": 2}, {"type": "text", "text": "Experiments with benchmark suites and implementation: We illustrate the concepts and corroborate their relevance with experiments run over two benchmark suites (PMLB and OpenML, see Section 5), based on an implementation that is freely available and easily adaptable to comparable problems.4. We consider experiments with mixed-scaled (ordinal and cardinal) multidimensional quality metrics, also incorporating (potentially) ordinal criteria. ", "page_idx": 2}, {"type": "text", "text": "1.2 Related work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Benchmarks are the foundations of applied machine learning research [27, 90, 78, 65, 91]. Specifically, benchmarking classifiers over multiple data sets is a much-studied problem in machine learning, as it enables practitioners to make informed choices about which methods to consider for a given data set. Furthermore, also proposals for novel classifiers must often first demonstrate their potential for improvement in benchmark studies. Examples include [58, 40, 31, 57, 12]. In recent years, in recognition of the fact that the benchmark suite under consideration is only a sample of data sets, especially focusing on statistically significant differences between classifiers has received great interest (see, e.g., [24, 35, 34, 19, 45] or, e.g., [9, 22, 8] for Bayesian variants). An R implementation of some of these tests is described in [15], whereas use-cases in the context of time series and neural networks for regression are discussed in [44, 36]. The diversity and the associated problem of selecting quality metrics (e.g., [51]) is currently attracting a great deal of interest (e.g., [89]). Consequently, finding ways for comparing classifiers in terms of multidimensional quality metrics is intensively studied, ranging from multidimensional interpretability measures (e.g., [59]) over classical Pareto-analyses (e.g., [31]) to embeddings in the theory of data depth (e.g., [13, 71]). While utilizing variants of stochastic dominance in statistics is quite common (e.g., [56, 61, 6, 76, 67]), the same seems not to hold for machine learning. Exceptions include [23] in an optimization context, [47, 48], who investigate special types of stochastic orders, and [45], utilizing already GSD-relations for classifier comparisons without the GSD-front. Finally, relying on imprecise probabilities (e.g., [85, 86, 3]) to robustify statistical hypotheses follows the tradition of [66, 42, 41, 2], see also, e.g., [5, 25, 4, 60, 48]. For application to Bayesian networks, see, e.g, [55, 14, 54], and [81, 69, 18, 80, 1, 70, 52, 30, 16, 17], among others, for robustified machine learning in this spirit. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "2 Decision-theoretic preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The relevant basic concepts in order theory are collected in Appendix A.1. Based on these we can make the following definition, originating from the decision-theoretic context discussed in [46]. ", "page_idx": 3}, {"type": "text", "text": "Definition 2. Let $A$ be a non-empty set, $R_{1}\\subseteq A\\times A$ a preorder on $A$ , and $R_{2}\\subseteq R_{1}\\times R_{1}$ a preorder on $R_{1}$ . The triplet $A=[A,R_{1},R_{2}]$ is then called a preference system on $A$ . The preference system $\\mathcal{A}^{\\prime}=[A^{\\prime},R_{1}^{\\prime},R_{2}^{\\prime}]$ is called subsystem of $\\boldsymbol{\\mathcal{A}}$ if $A^{\\prime}\\subseteq A$ , $R_{1}^{\\prime}\\subseteq R_{1},$ , and $R_{2}^{\\prime}\\subseteq R_{2}$ . ", "page_idx": 3}, {"type": "text", "text": "In our context, $R_{1}$ formalizes the ordinal information, i.e., the information about the ranking of the objects in $A$ , whereas $R_{2}$ describes the cardinal information, i.e., the information about the intensity of certain rankings. To ensure that $R_{1}$ and $R_{2}$ are compatible, we use a consistency criterion relying on the idea of simultaneous representability of both relations. For this, for a preorder $R$ , we denote by $I_{R}$ its indifference and by $P_{R}$ its strict part (see A.1). ", "page_idx": 3}, {"type": "text", "text": "Definition 3. The preference system $A=[A,R_{1},R_{2}]$ is consistent if there exists a representation $u:A\\rightarrow\\mathbb{R}$ such that for all $a,b,c,d\\in A$ we have: ", "page_idx": 3}, {"type": "text", "text": "The set of all representations of $\\boldsymbol{\\mathcal{A}}$ is denoted by $\\mathcal{U}_{A}$ . ", "page_idx": 3}, {"type": "text", "text": "Finally, we need to recall the concept of generalized stochastic dominance $(G S D)$ (see, e.g., [48]), which is crucial for the concepts presented in this paper: For a probability space $(\\Omega,{\\cal S},\\pi)$ and a consistent preference system $\\boldsymbol{\\mathcal{A}}$ , we define by ${\\mathcal{F}}_{({\\mathcal{A}},\\pi)}$ the set of all $X\\,\\in\\,\\dot{A}^{\\dot{\\Omega}}$ such that $u\\circ X\\in$ $\\mathcal{L}^{1}(\\Omega,\\mathcal{S},\\pi)$ for all $u\\in\\mathcal{U}_{A}$ . We then can define the GSD-preorder on ${\\mathcal{F}}_{({\\mathcal{A}},\\pi)}$ as follows. ", "page_idx": 3}, {"type": "text", "text": "Definition 4. Let $\\mathcal{A}=[A,R_{1},R_{2}]$ be consistent. For $X,Y\\in{\\mathcal{F}}_{({\\mathcal{A}},\\pi)}$ , say $X\\left({\\mathcal{A}},\\pi\\right)$ -dominates $Y$ if $\\mathbb{E}_{\\pi}(u\\circ X)\\geq\\mathbb{E}_{\\pi}(u\\circ Y)$ for all $u\\in\\mathcal{U}_{A}$ . Denote the induced GSD-preorder on ${\\mathcal{F}}_{({\\mathcal{A}},\\pi)}$ by $R_{(,A,\\pi)}$ . ", "page_idx": 3}, {"type": "text", "text": "3 GSD for classifier comparison ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We return to the initial problem: Assume we are given a finite set $\\mathcal{C}$ of classifiers, an arbitrary set $\\mathcal{D}$ of data sets and $n$ quality metrics $\\phi_{1},\\dots,\\phi_{n}:{\\mathcal{C}}\\times{\\mathcal{D}}\\rightarrow[0,1]$ , combined to the multidimensional quality metric $\\Phi:=(\\phi_{1},\\ldots,\\phi_{n}):\\mathcal{C}\\times\\mathcal{D}\\to[0,1]^{n}$ . As we also want to allow ordinal quality metrics, we assume that, for $0\\leq z\\leq n$ , the metrics $\\phi_{1},\\ldots,\\phi_{z}$ are of cardinal scale (differences may be interpreted), while the remaining ones are purely ordinal (differences are meaningless apart from sign). We embed the range $\\Phi({\\mathcal{C}}\\times{\\mathcal{D}})$ of $\\Phi$ in the following preference system: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathscr{P}=[[0,1]^{n},R_{1}^{*},R_{2}^{*}]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{\\lambda}_{1}^{*}=\\Big\\{(x,y):x_{j}\\geq y_{j}\\ \\forall j\\leq n\\Big\\},\\ \\mathrm{and}\\ R_{2}^{*}=\\Bigg\\{((x,y),(x^{\\prime},y^{\\prime})):\\ \\begin{array}{l}{x_{j}-y_{j}\\geq x_{j}^{\\prime}-y_{j}^{\\prime}\\ \\forall j\\leq z}\\\\ {x_{j}\\geq x_{j}^{\\prime}\\geq y_{j}^{\\prime}\\geq y_{j}\\ \\forall j>z}\\end{array}\\Bigg\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "$R_{1}^{*}$ is the usual component-wise $\\geq$ -relation. For $R_{2}^{\\ast}$ , one pair of consequences is preferred to another if, in the ordinal dimensions, the exchange associated with the first pair is not a deterioration to the exchange associated with the second pair and, in addition, there is component-wise dominance of the differences of the cardinal dimensions. In order to transfer the GSD-relation from Definition 4 to the case of comparing classifiers under multidimensional performance metrics, we interpret the data sets in $\\mathcal{D}$ as realizations of a random variable $T:\\Omega\\to{\\mathcal{D}}$ on some probability space $(\\Omega,{\\cal S},\\pi)$ . We then associate each classifier $C\\in{\\mathcal{C}}$ with the random variable $\\Phi_{C}:=\\Phi(C,T(\\cdot))$ on $\\Omega$ and compare classifiers by comparing the associated random variables by means of GSD. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Definition 5. Denote by $\\mathcal{P}_{\\Phi}$ the preference system obtained by restricting $\\mathcal{P}$ to $\\Phi({\\mathcal{C}}\\times{\\mathcal{D}})$ . Further, let $\\mathcal{C}$ be such that $\\{\\Phi_{C}:C\\in\\mathcal{C}\\}\\subseteq\\mathcal{F}_{(\\mathcal{P}_{\\Phi},\\pi)}$ . For $C,C^{\\prime}\\in{\\mathcal{C}}$ , say that $C$ dominates $C^{\\prime}$ , abbreviated with $C\\succeq C^{\\prime}$ , whenever $\\left(\\Phi_{C},\\Phi_{C^{\\prime}}\\right)\\in R_{(\\mathcal{P}_{\\Phi},\\pi)}$ . ", "page_idx": 4}, {"type": "text", "text": "In the application situation, instead of the true GSD-order $\\succsim$ among classifiers, we will often have to get along with its empirical analogue, i.e., the GSD-relation where a sample of data sets is treated like the underlying population and the true probability measure is replaced by the corresponding empirical ones. More precisely, we assume that we have sampled i.i.d. copies $T_{1},\\mathbf{\\Pi}\\cdot\\mathbf{\\Pi}\\cdot,T_{s}$ of $T$ and then define the set $Z_{s}:=\\{\\Phi(\\dot{C},T_{i}):i\\leq s\\wedge C\\in\\mathcal{C}\\}$ , of (random) observations under the different classifiers. We then use $\\mathcal{W}$ to denote the (random) subsystem of $\\mathcal{P}$ that arises when $\\mathcal{P}$ is restricted to the (random) set $Z_{s}$ . For $C,C^{\\prime}\\in{\\mathcal{C}}$ we define the random variable ", "page_idx": 4}, {"type": "equation", "text": "$$\nd_{s}(C,C^{\\prime}):=\\operatorname*{inf}_{u\\in\\mathcal{U}_{w}}\\sum_{z\\in Z_{s}}u(z)(\\hat{\\pi}_{C}(\\{z\\})-\\hat{\\pi}_{C^{\\prime}}(\\{z\\})),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where, for $M\\subseteq[0,1]^{n}$ , we set $\\textstyle{\\hat{\\pi}}_{C}(M):={\\frac{1}{s}}|\\{i:i\\leq s\\wedge\\Phi(C,T_{i})\\in M\\}|$ . For a concrete sample associated to $\\omega_{0}\\,\\in\\,\\Omega$ , we then say that $C$ empirically GSD-dominates $C^{\\prime}$ , if $d_{s}(C,C^{\\prime})(\\omega_{0})\\geq0$ Intuitively, $d_{s}$ can thus be used to check whether the classifier $C$ empirically dominates the classifier $C^{\\prime}$ with respect to GSD in the samples at hand (i.e., in the benchmark suite under investigation). ", "page_idx": 4}, {"type": "text", "text": "Based on these concepts, we can now define the sets of (empirically) GSD-undominated classifiers. ", "page_idx": 4}, {"type": "text", "text": "Definition 6. Let $\\mathcal{C}$ be such that $\\{\\Phi_{C}:C\\in\\mathcal{C}\\}\\subseteq\\mathcal{F}_{(\\mathcal{P}_{\\Phi},\\pi)}$ . Let denote $T_{1},\\ldots,T_{s}$ i.i.d. copies of $T$ . ", "page_idx": 4}, {"type": "text", "text": "i) The GSD-front is the set ", "page_idx": 4}, {"type": "equation", "text": "$$\ng s d(\\mathcal{C}):=\\{C\\in\\mathcal{C}:\\vec{\\nexists}C^{\\prime}\\in\\mathcal{C}\\mathrm{~s.t.~}C^{\\prime}\\succ C\\},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\succ$ denotes the strict part of \u227f. ", "page_idx": 4}, {"type": "text", "text": "ii) Let $\\varepsilon\\in[0,1]$ . The $\\varepsilon$ -empirical GSD-front is the (random) subset of $\\mathcal{C}$ defined by ", "page_idx": 4}, {"type": "equation", "text": "$$\ne g s d_{s}^{\\varepsilon}(\\mathcal{C})=\\Big\\{C:\\nexists C^{\\prime}\\in\\mathcal{C}\\mathrm{~s.t.~}d_{s}(C^{\\prime},C)\\geq-\\varepsilon\\enspace\\Big\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Remark 1. $e g s d_{s}^{0}(\\mathcal{C})$ is always non-empty. In contrast, $e g s d_{s}^{\\varepsilon}(\\mathcal{C})$ may very well be empty if $\\varepsilon>0$ .   \nNote that choosing values of $\\varepsilon>0$ is intended to make $e g s d_{s}^{\\varepsilon}(\\mathcal{C})$ less prone to sampling noise. ", "page_idx": 4}, {"type": "text", "text": "Remark 2. Some words on the semantics of the GSD-front: From a decision-theoretic point of view, classifier $C$ strictly GSD-dominates classifier $C^{\\prime}$ iff $C$ has at least as high expected utility as $C$ regarding any compatible utility representation of all the metrics considered, and stricly higher for at least one such utility. The GSD-front then simply collects all classifiers from $\\mathcal{C}$ which are not strictly GSD-dominated by any competitor, i.e., which potentially can be optimal in expectation. ", "page_idx": 4}, {"type": "text", "text": "Example 2. Consider again the situation of Example 1 and recall that $p a r(\\Phi)=\\{C_{2},C_{3}\\}$ leaves $C_{2}$ and $C_{3}$ incomparable. However, if considering only the distribution of the (multivariate) performance of the classifiers (while assuming a uniform distribution over $\\mathcal{D}$ ), $C_{3}$ is clearly dominating $C_{2}$ w.r.t. GSD: Matching dataset $D_{i}$ with dataset $D_{5-i}$ creates a (strict) pointwise dominance of $C_{3}$ over $C_{2}$ (where the strict dominance is due to $D_{1}$ and $D_{4}$ ). Thus, $g s d(\\mathcal{C})=\\{C_{3}\\}\\subsetneq p a r(\\Phi)=\\{C_{2},C_{3}\\}$ . ", "page_idx": 4}, {"type": "text", "text": "The following two theorems show that the $\\varepsilon$ -empirical GSD-front fulfills two very natural requirements: First, under some regularity conditions, it is a consistent statistical estimator for the true GSD-front (Theorem 1). This is important because in practical benchmarking we almost never have access to the GSD-front of the whole population, i.e., the benchmarking results on all possible datasets from a specific problem class $\\mathcal{D}$ . Second, it is ensured that neither the $\\varepsilon$ -empirical nor the true GSD-front can ever become larger than the respective Pareto-front, irrespective of the choice of $\\varepsilon$ (Theorem 2). This is important as it guarantees our analysis does never conflict with, but is potentially more information-efficient than a Pareto-type analysis. Proofs are given in B.1 and B.2. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. Denote by $\\mathcal{T}_{\\Phi}$ the set of all sets $\\{a:u(a)\\geq c\\}$ , where $c\\in[0,1]$ and $u\\in\\mathcal{U}_{\\mathcal{P}_{\\Phi}}$ . Assume that $\\succsim$ is antisymmetric. If the VC-dimension5 of $\\mathcal{T}_{\\Phi}$ is finite and i $f\\varepsilon:\\mathbb{N}\\rightarrow[0,1]$ converges to 0 with rate at most $\\Theta(1/\\sqrt[4]{s})$ , then $(\\mathrm{egsd}_{s}^{\\varepsilon(s)}(\\mathcal{C}))_{s\\in\\mathbb{N}}$ is a consistent statistical estimator, i.e., ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\pi\\Big(\\Big\\{\\omega\\in\\Omega:\\operatorname*{lim}_{s\\to\\infty}\\mathrm{egsd}_{s}^{\\varepsilon(s)}(\\mathcal C)=\\mathrm{gsd}(\\mathcal C)\\Big\\}\\Big)=1,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where set convergence is defined via the trivial metric. ", "page_idx": 5}, {"type": "text", "text": "Remark 3. The assumption of a finite VC dimension is only necessary to ensure that the $\\varepsilon$ -empirical GSD front does not become too large. In particular, the following does hold without this assumption: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\pi\\Big(\\Big\\{\\omega\\in\\Omega:\\operatorname*{lim}_{s\\rightarrow\\infty}e g s d_{s}^{\\varepsilon(s)}(\\mathcal{C})\\geq g s d(\\mathcal{C})\\Big\\}\\Big)=1.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Thus, the $\\varepsilon$ -empirical GSD-front almost surely converges to a superset of the true GSD-front. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2. Assume $\\mathcal{C}$ with $\\{\\Phi_{C}:C\\in\\mathcal{C}\\}\\subseteq\\mathcal{F}_{(\\mathcal{P},\\pi)}$ . Let further denote $T_{1},\\mathbf{\\Pi}\\cdot\\mathbf{\\Pi}\\cdot,T_{s}$ i.i.d. copies of $T$ and let $\\varepsilon_{1}\\,\\leq\\,\\varepsilon_{2}\\,\\in\\,[0,1]$ . It then holds that $i$ ) $\\operatorname{gsd}(\\mathcal{C})\\subseteq\\operatorname{par}(\\Phi)$ . Moreover, it holds that ii) $\\operatorname{egsd}_{s}^{\\varepsilon_{2}}(\\mathcal{C})\\subseteq\\operatorname{egsd}_{s}^{\\varepsilon_{1}}(\\mathcal{C})\\subseteq\\operatorname{par}(\\Phi,\\{T_{1},\\ldots,T_{s}\\})$ . ", "page_idx": 5}, {"type": "text", "text": "4 Statistical testing ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We saw the $\\varepsilon$ -empirical GSD-front can be a consistent statistical estimator and that both the empirical and the true GSD-front are compatible with the Pareto-front. We now address statistical testing. ", "page_idx": 5}, {"type": "text", "text": "4.1 A test for the GSD-front ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "From now on, we make the (technical) assumption that the order $\\succsim$ among the classifiers from $\\mathcal{C}$ is additionally antisymmetric, transforming it from a preorder into a partial order.6 Equipped with this assumption, we want to address the question how to statistically test if a given classifier $C\\in{\\mathcal{C}}$ is an element of the true GSD-front $\\gcd(C)$ . To achieve this, we formulate the question of actual interest as the alternative hypothesis of the test, i.e., we obtain the hypothesis pair: ", "page_idx": 5}, {"type": "equation", "text": "$$\nH_{0}:C\\not\\in\\operatorname{gsd}(\\mathcal{C})\\;\\;\\mathbf{vs}.\\;\\;H_{1}:C\\in\\operatorname{gsd}(\\mathcal{C})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "A possible motivation for developing tests on the hypothesis pair $(H_{0},\\neg H_{0})$ is the following: One would like to compare the quality of a newly developed classifier $C$ for a problem class $\\mathcal{D}$ with the classifiers in $\\mathcal{C}\\,\\bar{\\backslash}\\,\\{C\\}$ that are considered state-of-the-art for this problem class, see application in Section 5.2. If a suitable statistical test would allow the above null hypothesis to be rejected, then one could draw the conclusion (subject to statistical uncertainty) that the new classifier $C$ on the problem class $\\mathcal{D}$ could potentially improve the state-of-the-art. As first step, note that (under asymmetry) the null hypothesis $H_{0}$ can be equivalently rewritten as $H_{0}:\\exists C^{\\prime}\\in\\dot{\\mathcal{C}}\\setminus\\{C\\}:C^{\\prime}\\succeq C$ . This reformulation makes obvious that $H_{0}$ is false if and only if for every $C^{\\prime}\\in\\mathcal{C}\\setminus\\{C\\}$ the auxiliary hypothesis $H_{0}^{C^{\\prime}}:C^{\\prime}\\succeq C$ is false. Statistical tests for hypothesis pairs of the form $(H_{0}^{C^{\\prime}},\\neg H_{0}^{C^{\\prime}})$ were proposed (in the context of statistical inequality analysis) in [48]: The authors there showed how exact statistical tests under i.i.d. sampling can be constructed by using a (non-parametric) permutation test based on a regularized version $d_{s}^{\\delta}(C^{\\prime},C)$ of $d_{s}(C^{\\prime},\\stackrel{\\cdot}{C})$ as a test statistic. The strength of regularization of the test statistic is there controlled by a parameter $\\delta\\in[0,1]$ , whose increase reduces the number of representation functions over which the infimum in the test statistic is formed, while equally attenuating all quality metrics.7 Due to space limitations, we omit to recall an exact description of the testing scheme in the main text and instead refer to Appendix A.2. ", "page_idx": 5}, {"type": "text", "text": "The idea is then to replace the global test for $(H_{0},\\neg H_{0})$ with $c:=|\\mathcal{C}|-1$ tests of hypotheses $(H_{0}^{C^{\\prime}},\\neg H_{0}^{C^{\\prime}})$ and to reject the null hypothesis at significance level $\\alpha$ if all tests reject their individual null hypotheses $H_{0}^{C^{\\prime}}$ at the same significance level $\\alpha$ . Call this the static GSD-test. Clearly, this test tends to be conservative, as it ignores potential correlations of the test statistics for different pairs of classifiers. Moreover, a slightly modified test in the context of the GSD-front is directly derivable: If one is rather interested in identifying the maximal subset $S_{\\mathrm{max}}$ of $\\mathcal{C}$ for which $C$ significantly lies in the GSD-front, i.e., in testing $\\tilde{H}_{0}^{S}:C\\notin\\operatorname{gsd}(S)$ vs. $\\tilde{H}_{1}^{S}:C\\in\\operatorname{gsd}(S)$ for all ${\\mathcal{S}}\\subseteq{\\mathcal{C}}$ with $C\\in S$ simultaneously, the following alternative test is a statistically valid level- $\\alpha$ test: First, perform all individual tests for $(H_{0}^{C^{\\prime}},\\neg H_{0}^{C^{\\prime}})$ with level $\\frac{\\alpha}{c}$ . Then identify $S_{\\mathrm{max}}$ as the set of all classifiers from $\\mathcal{C}$ for which the individual hypotheses are rejected. The (random) alternative hypothesis $\\tilde{H}_{1}^{S_{\\mathrm{max}}}:C\\in\\operatorname{gsd}(S_{\\mathrm{max}})$ is then statistically valid in the sense of being false only with a probability bounded by $\\alpha$ . Call this the dynamic GSD-test. We have the following theorem, demonstrating that the proposed tests are indeed reasonable statistical tests (see B.3 for the proof). ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Theorem 3. Let the assumptions of Theorem 1 hold. Then, both the static and dynamic GSD-test are valid level- $\\alpha$ tests. Additionally, both tests are consistent in the sense that under the corresponding alternative hypothesis, i.e., $H_{1}:C\\in g s d({\\mathcal{C}})$ resp. $\\tilde{H}_{1}:\\exists S\\subseteq\\mathcal{C}:C\\in S,|S|\\geq2,C\\in g s d(S),$ , the probability of rejecting the corresponding null hypothesis converges to 1 as $s\\to\\infty$ . ", "page_idx": 6}, {"type": "text", "text": "4.2 Checking robustness under non-i.i.d.-scenarios ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We argue that meaningful benchmark studies should abstain from treating the sample of data sets in the suite as a complete survey. That is, benchmark analyses should aim at statements about a well-defined population and regard the benchmark suite as a non-degenerate sample thereof. A major practical problem in this context is that often little is known about the inclusion criteria for data sets or test problems in the respective benchmark suite (see, e.g., the discussions in [83, 53, 37]). For instance, the popular platform OpenML [82] allows users to upload benchmark results for machine learning models with varying hyperparameters, harming representativity, see Section 5.1 and Appendix C.1. The absence of methods to randomly sample from the set of all problems or data sets is identified as an unsolved issue in [57, Section 2]. This calls the common i.i.d. sampling assumption into question, which our (and most other) tests are based upon, and raises the issue as to what extent statistically significant results depend on this assumption. We now address precisely this question. ", "page_idx": 6}, {"type": "text", "text": "In [48] it was shown how the binary tests on the hypothesis pairs $(H_{0}^{C^{\\prime}},\\neg H_{0}^{C^{\\prime}})$ discussed in Section 4.1 can be checked for robustness against deviations from the underlying i.i.d.-assumption. The idea here is to deliberately perturb the empirical distributions of the performances for the different classifiers and to analyze the permutation test used under the most extreme yet compatible worst-case. The perturbation of the empirical distribution is carried out here using a $\\gamma$ -contamination model (see, e.g., [85, p. 147]), which is widely used in robust statistics. We now want to adapt a similar robustness check for the global hypothesis pair $(H_{0},\\neg H_{0})$ discussed here. For this, suppose we have a sample $T_{1},\\mathbf{\\Pi}\\cdot\\mathbf{\\Pi}\\cdot,T_{s}$ of data sets (i.e., the benchmark suite). We further assume that $k\\leq s$ of these variables (where it is not known which ones) are not sampled i.i.d., but come from an arbitrary distribution about which nothing else is known. We then know, for every fixed $C\\in{\\mathcal{C}}$ , that its associated true empirical measure $\\hat{\\pi}_{C}^{t r u e}$ based on the true (uncontaminated) sample would have to be contained in ", "page_idx": 6}, {"type": "equation", "text": "$$\n{\\mathcal{M}}_{C}=\\textstyle\\left\\{(1-{\\frac{k}{s}}){\\hat{\\pi}}_{C}^{c o n t}+{\\frac{k}{s}}\\mu:\\mu{\\mathrm{~probability~measure}}\\right\\},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\hat{\\pi}_{C}^{c o n t}$ denotes the empirical measure based on the contaminated sample $T_{1},\\mathbf{\\Pi}\\cdot\\mathbf{\\Pi}\\cdot,T_{s}$ . Note that $\\mathcal{M}_{C}$ is by definition a $\\gamma$ -contamination model with central distribution $\\hat{\\pi}_{C}^{c o n t}$ and contamination degree $\\begin{array}{r}{\\gamma:=\\frac{k}{s}}\\end{array}$ . In this setting, [48] show that to ensure that their permutation tests used for hypothesis pairs $(H_{0}^{C^{\\prime}},\\neg H_{0}^{C^{\\prime}})$ only advise rejection of the null hypothesis if this is justifiable for any empirical distribution compatible with the contaminated sample, i.e., for every combination of measures $(\\pi_{1},\\pi_{2})\\,\\in\\,\\mathcal{M}_{C}\\,\\times\\,\\mathcal{M}_{C^{\\prime}}$ , one has to compare the most pessimistic value of the test statistic for the concrete sample at hand with the most optimistic value of the test in each of the resamples. Moreover, they show that the (approximate) observed $p$ -values for a concrete contaminated sample $T_{1}(\\omega_{0}),\\ldots,\\bar{T_{s}}(\\omega_{0})\\,\\in\\,\\mathcal{D}$ associated with $\\omega_{0}\\;\\in\\;\\Omega$ of this robustified test can be expressed by a function in the number of contaminations $k$ , given by ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f_{(C^{\\prime},C)}(k):=1-\\frac{1}{N}\\cdot\\sum_{I\\in\\mathbb{Z}_{N}}{\\mathbb1}_{\\left\\{d_{I}^{\\delta}-d_{s}^{\\delta}(C^{\\prime},C)(\\omega_{0})>\\frac{2k}{(s-k)}\\right\\}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $N$ denotes the number of resamples, $\\mathcal{T}_{N}$ is the corresponding set of resamples, and $d_{I}^{\\delta}$ is the test statistic evaluated for the resample associated to $I$ . Due to space limitations, we omit an exact description of the robustness check for the test on the hypothesis pairs $(H_{0}^{C^{\\prime}},\\neg H_{0}^{C^{\\prime}})$ as well as a derivation of the function $f_{(C^{\\prime},C)}$ in the main text and instead refer to Appendix A.3. ", "page_idx": 6}, {"type": "text", "text": "Similar as shown in Section 4.1, it is straightforward to calculate an (approximate) observed $p$ -value for the static GSD-test for $(H_{0},\\neg H_{0})$ : We calculate the maximal observed $p$ -value among all $C^{\\prime}\\in$ ${\\mathcal{C}}\\backslash\\{C\\}$ , i.e. set $F_{C}(k):=\\operatorname*{max}\\{f_{(C^{\\prime},C)}(k):C^{\\prime}\\in\\mathcal{C}\\backslash\\{C\\}\\}$ . The robustified static GSD-test for the degree of contamination $k$ can be carried out as follows: Calculate $F_{C}(k)$ and reject $H_{0}$ if $F_{C}(k)\\leq\\alpha$ . This indeed gives us a valid level- $\\cdot\\alpha$ -test for the desired global hypothesis $H_{0}:C\\not\\in\\operatorname{gsd}(\\mathcal{C})$ under the additional freedom that up to $k$ of the variables in the sample might be contaminated. Note, however, that also this test tends to be conservative as both performing the individual tests at level $\\alpha$ as well as the adapted resampling scheme of the permutation test are worst-case analyses. Finally, also the robustified dynamic GSD-test can be obtained straightforwardly: Under up to $k$ contaminations, the (random) alternative hypothesis $\\tilde{H}_{1}^{S_{\\mathrm{max}}}:C\\in\\operatorname{gsd}(S_{\\mathrm{max}})$ is statistically valid with level $\\alpha$ if all individual robustified tests reject $H_{0}^{C^{\\prime}}$ at level $\\frac{\\alpha}{c}$ , i.e., if $\\begin{array}{r}{F_{C}(k)\\leq\\frac{\\alpha}{c}}\\end{array}$ . ", "page_idx": 7}, {"type": "text", "text": "We end the section with a short comment on computation: The test statistics for the permutation test and the robustified variant can be calculated using linear programming. We are guided here by the linear programs proposed in [48, Propositions 4 and 5]. There are two computational bottlenecks in the actual evaluation: (1) the creation and storage of the constraint matrices of the linear programs and (2) the repeated need to solve large linear programs. An efficient, well-commented implementation that can be quickly transferred to similar applications is made available on GitHub (see Footnote 4). ", "page_idx": 7}, {"type": "text", "text": "5 Benchmarking experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We demonstrate our concepts on two well-established benchmark suites: OpenML [82, 11] and PMLB [64]. While for PMLB we compare classifiers w.r.t. the latent quality metric robust accuracy (see the first motivation in Section 1), for OpenML we use a multidimensional metric that includes accuracy and computation time as unidimensional metrics (see the second motivation in Section 1). The analysis of PMLB is kept short in the main text and detailed in Appendix C. Since the metrics in both applications are composed of one continuous and two (finitely) discrete metrics, we have (see B.4): ", "page_idx": 7}, {"type": "text", "text": "Corollary 1. In both applications, the $\\varepsilon$ -empirical GSD-front is a consistent estimator for the true GSD-front (provided \u03b5 is chosen as in Theorem 1). ", "page_idx": 7}, {"type": "text", "text": "5.1 Experiments on OpenML ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We select 80 binary classification datasets (according to criteria detailed in Appendix C.1) from OpenML [82] to compare the performance of Support Vector Machine (SVM) with Random Forest (RF), Decision Tree (CART), Logistic Regression (LR), Generalized Linear Model with Elastic net (GLMNet), Extreme Gradient Boosting (xGBoost), and $k$ -Nearest Neighbors (kNN).8Our multidimensional quality metric is composed of predictive accuracy, computation time on the test data, and computation time on the training data. Since the computation time depends strongly on the used computing environment (e.g. number of cores or free memory), we discretize the timerelated metrics and treat them as ordinal. Accuracy is not affected by this and is therefore treated as cardinal. For details, see Appendix C.1. To gain a purely descriptive impression, we computed the empirical GSD relation. For this, we calculated $d_{80}(C,C^{\\prime})$ for $C\\neq C^{\\prime}\\in\\mathcal{C}\\ensuremath{:}=\\ensuremath{\\left\\{\\mathrm{SVM},\\right.}$ RF, CART, LR, GLMNet, xGBoost, $\\mathrm{{kNN}\\}$ (see Hasse graph in Figure 2 in Appendix C.1). We see that CART (strictly) empirically GSD-dominates xGBoost, SVM, LR, and GLMNet. All other classifiers are pairwise incomparable. Three classifiers are not strictly empirically GSD-dominated by any other, namely RF, CART, and kNN. Thus, the 0-empirical GSD-front is formed by these. While at first glance this result might seem rather unexpected, a closer look on the performance evaluations provided by OpenML indeed confirms the dominance structure found, see Appendix C.1 for details. ", "page_idx": 7}, {"type": "text", "text": "To move to reliable inferential statements that take into account the statistical uncertainty, we exemplarily test (at level $\\alpha=0.05)$ ) if SVM significantly lies in the GSD-front of some subset of $\\mathcal{C}$ . As described in Section 4.1, we therefore perform six pairwise permutation tests for the hypothesis pairs $(H_{0}^{C^{\\prime}},\\neg H_{0}^{C^{\\prime}})$ (where $C:=\\mathrm{SVM}$ and $C^{\\prime}\\in\\mathcal{C}\\backslash\\{\\mathrm{SVM}\\})$ at level $\\alpha$ in case of the static GSD-test or at level $\\frac{\\alpha}{6}$ in case of the dynamic GSD-test.9 That is, we test six auxiliary null hypotheses each stating that SVM is GSD-dominated by kNN, xGBoost, RF, CART, LR, and GLMNet, respectively. ", "page_idx": 7}, {"type": "image", "img_path": "jXxvSkb9HD/tmp/0a0044075a059d9667f532ff42ae987d378d1893141ae8bc2dc576bdeeb28409.jpg", "img_caption": ["Figure 1: Left: Densities of resampled test statistics for pairwise permutation tests of SVM vs. six other classifiers on 80 datasets from OpenML. Big (small) vertical lines depict observed (resampled) test statistics. Rejection regions for the static (dynamic) GSD-test are highlighted red (dark red). Right: Effect of Contamination: $p$ -values for pairwise tests of SVM versus GLMNet, kNN, RF and xGBoost.Red lines mark significance levels of $\\alpha~=~0.05$ (dark red: $\\alpha~=~{\\frac{0.05}{6}})$ 0.605 ). Significance of SVM being in the GSD-front remains stable under contamination of up to 7 of 80 datasets. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "The distributions of the test statistics are visualized on the left of Figure 1 (densities) and Figure 3 (CDFs) in C.1. They show that the pairwise tests of SVM versus kNN, xGBoost, RF, and GLMNet reject at level $\\frac{\\alpha}{6}$ and, thus, that SVM significantly (at level $\\alpha$ ) lies in the GSD-front of the subset of $\\mathcal{C}$ composed of SVM and these four classifiers. In other words, we conclude that SVM is significantly $\\left[\\alpha\\;=\\;0.05\\right]$ ) not outperformed by kNN, xGBoost, RF, and GLMNet regarding all compatible utility representation of accuracy, training and test runtime. Finally, as discussed in Section 4.2, we turn to the third aspect of reliability (besides multiple criteria and statistical uncertainty): We analyze how robust this test decision is under contamination of the benchmark suite, i.e., deviations from i.i.d.. The results are visualized on the right of Figure 1. It can be seen that the tests at level $\\frac{0.5}{6}$ of SVM against GLMNet, kNN, RF and xGBoost cease to be significant from a contamination of (approximately) 7, 8, 11, and 11 of 80 data sets, respectively. That is, the results on up to 7, 8, 11, and 11 datasets could be arbitrarily redistributed, while maintaining significance of rejection. Since the significance of the dynamic GSD-test\u2019s decision depends on all pairwise tests being significant at level 0.5 , we can conclude that SVM would still have been significantly in the GSD-front of $\\{{\\bf{S V M}},$ kNN, xGBoost, RF, GLMNet}, even if 7 out of 80 data sets had been contaminated. Summing up, our proposed testing scheme not only allowed for meaningful statistical benchmarking of SVM versus competitors regarding accuracy, test time, and train time; it also enabled us to quantify as to what degree our conclusions remained stable under contamination of the benchmark suite. ", "page_idx": 8}, {"type": "text", "text": "Method comparison: The results highlight the advantages of the GSD-front over existing approaches. Applying first-order stochastic dominance (a special case of GSD where $R_{2}^{*}$ is the trivial preorder) on the same set-up, yields that no classifier is significantly larger than (or incomparable to) any other classifier, based on a $5\\%$ significance level. This illustrates that the GSD-approach accounts for accuracy being a cardinal measure. In contrast, the Pareto-front here contains all considered classifiers. Thus, the Pareto front is much less informative than the GSD-front, which is also reflected in Theorem 2. Unlike the Pareto-front, the GSD-front is based on the distribution of the multidimensional quality metric and not only on the pairwise comparisons, and can use this knowledge to define the front. Thus, the GSD front is a balance between the conservative Pareto analysis and the liberal weighted sum comparison. Finally, we want to compare our method with an approach based on extending the test for single quality metrics proposed in [24] to the multiple metric setting. We therefore perform all possible single-metric tests as in [24] and define the marginal front as those classifiers that are not statistically significantly worse than another classifier on all metrics. However, this procedure can not be used to define a hypothesis test. Therefore, only a comparison with the empirical GSD-front is meaningful. For OpenML, this marginal front consists of all classifiers and is less exploratory than the empirical GSD-front. More details on the results of these other approaches and how these compare to the GSD front can be found in Appendix C.1. ", "page_idx": 8}, {"type": "text", "text": "5.2 Experiments on PMLB ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We select 62 datasets from the Penn Machine Learning Benchmark (PMLB) suite [64] according to criteria explained in Appendix C.2. The following analysis shall exemplify how our proposed statistical tests can aid researchers in benchmarking newly developed classifiers against state-of-theart ones. To this end, we compare a recently proposed classifier based on compressed rule ensembles of trees (CRE) [62] w.r.t. robust accuracy against five well-established classifiers, namely CART, RF, SVM with radial kernel, kNN and GLMNet. We operationalize the latent quality criterion of robust accuracy through i) classical accuracy (metric), ii) robustness of accuracy w.r.t. noisy features (ordinal), and iii) robustness of accuracy w.r.t. noisy classes (ordinal). Computation of i) is straightforward; in order to retrieve ii) and iii), we follow [92, 93] by randomly perturbing a share (here: $20\\;\\%$ ) of both classes and features and computing the accuracy subsequently, as detailed in Appendix C.2. Since there exist competing definitions of robustness [43, 10, 72] and due to the share\u2019s arbitrary size, we treat ii) and iii) as ordinal and discretize the perturbated accuracy in the same way as for the runtimes in the openML experiments. Detailed results and visualization thereof can be found in Appendix C.2. In a nutshell, we find no evidence to reject the null of both the static and the dynamic GSD-test at significance level $\\alpha=0.05$ . In particular, we do not reject any of the pairwise auxiliary tests for hypothesis pairs $(H_{0}^{C^{\\prime}},\\neg H_{0}^{C^{\\prime}})$ with $C:=\\mathrm{CRE}$ and $C^{\\prime}\\in{\\mathcal{C}}\\setminus\\{{\\mathrm{CRE}}\\})$ for neither $\\alpha$ nor $\\frac{\\alpha}{5}$ . Our analysis hence concludes that we cannot rule out at significance level $\\alpha\\:=\\:0.05$ that the newly proposed classifier CRE is dominated by the five state-of-the-art classifiers w.r.t. all compatible utility representation of the latent criterion robust accuracy. ", "page_idx": 9}, {"type": "text", "text": "5.3 Additional recommendations for the end-user ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We end the section with a few brief general notes for end-users of our benchmark methodology. This should make it easy to decide whether a GSD-based analysis is appropriate in a given use-case. ", "page_idx": 9}, {"type": "text", "text": "1. GSD-based studies do not primarily aim to identify the best algorithm for a given benchmark suite. Often, the GSD front contains more than one element. They are rather intended for checking whether a newly proposed classifier for a certain problem class can potentially improve on the state-of-the-art classifiers, or whether it disqualifies itself from the outset.   \n2. GSD-based studies allow statements with inferential guarantees by providing appropriate statistical tests: Assuming an i.i.d. benchmark suite, a judgment about an algorithm represents a statement about an underlying population and not just this specific suite.   \n3. GSD-based studies enable the robustness of the results to be quantified under the deviation from the i.i.d. assumption: It can be checked which share of the benchmark suite may be contaminated without affecting the obtained inferential statements.   \n4. GSD-based studies allow algorithms to be compared w.r.t. multiple metrics simultaneously. They enable the full exploitation of the information contained in differently scaled metrics. ", "page_idx": 9}, {"type": "text", "text": "6 Concluding remarks ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Summary: We introduced the GSD-front for multicriteria comparisons of classifiers, gave conditions for its consistent estimability and proposed a statistical test for checking if a classifier belongs to it. We illustrated our concepts using two well-established benchmark suites. The results came with threefold reliability: They included several quality metrics, representation of statistical uncertainty, and a quantification of robustness under deviations from the assumptions. ", "page_idx": 9}, {"type": "text", "text": "Limitations and future research: Two specific limitations open promising avenues: 1.) Comparing other types of algorithms: We restricted ourselves to comparing classifiers. However, any situation in which objects are to be compared on the basis of different (potentially differently scaled) metrics over a random selection of instances can be analyzed using these ideas. For instance, applications of our framework to the multicriteria deep learning benchmark suite DAWNBench [21] or the bi-criteria optimization benchmark suite DeepOBS [74] appear straighforward. 2.) Extension to regression-type analysis: Analyses based on the GSD-front do not account for meta properties of the data sets. A straightforward extension to the case of additional covariates for the data sets is to stratify by these for the GSD-comparison. This would allow for a situation-specific GSD-analysis, presumably yielding more informative results. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank the anonymous reviewers and the area chair for providing valuable feedback. HB sincerely thanks the Evangelisches Studienwerk Villigst e.V. for the funding and support of her doctoral studies. Support by the Federal Statistical Office of Germany within the co-operation project \u201cMachine Learning in Official Statistics\u201d (JR and TA), by the Bavarian Academy of Sciences (BAS) through the Bavarian Institute for Digital Transformation (bidt, JR) and by the LMU Mentoring Program (JR and HB) is gratefully acknowledged. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] J. Abellan, C. Mantas, J. Castellano, and S. Moral-Garcia. \u201cIncreasing diversity in random forest learning algorithm via imprecise probabilities\u201d. In: Expert Systems with Applications 97 (2018), pp. 228\u2013243.   \n[2] T. Augustin. \u201cNeyman-Pearson testing under interval probability by globally least favorable pairs: Reviewing Huber-Strassen theory and extending it to general interval probability\u201d. In: Journal of Statistical Planning and Inference 105 (2002), pp. 149\u2013173.   \n[3] T. Augustin, F. Coolen, G. de Cooman, and M. Troffaes, eds. Introduction to Imprecise Probabilities. Wiley, 2014.   \n[4] T. Augustin and G. Schollmeyer. \u201cComment: On focusing, soft and strong revision of Choquet capacities and their role in statistics\u201d. In: Statistical Science 36.2 (2021), pp. 205\u2013209.   \n[5] T. Augustin, G. Walter, and F. Coolen. \u201cStatistical Inference\u201d. In: Introduction to Imprecise Probabilities. Ed. by T. Augustin, F. Coolen, G. de Cooman, and M. Troffaes. Wiley, 2014, pp. 135\u2013189.   \n[6] G. Barrett and S. Donald. \u201cConsistent tests for stochastic dominance\u201d. In: Econometrica 71.1 (2003), pp. 71\u2013104.   \n[7] D. Bates and M. Maechler. Package \u2019Matrix\u2019. [Accessed: 13.05.2024]. 2010. URL: http: //cran.%20r-project.%20org/package $=$ %20Matrix.   \n[8] A. Benavoli, G. Corani, J. Dem\u0161ar, and M. Zaffalon. \u201cTime for a change: a tutorial for comparing multiple classifiers through Bayesian analysis\u201d. In: Journal of Machine Learning Research 18.77 (2017), pp. 1\u201336.   \n[9] A. Benavoli, G. Corani, and F. Mangili. \u201cShould we really use post-hoc tests based on meanranks?\u201d In: Journal of Machine Learning Research 17.1 (2016), pp. 152\u2013161.   \n[10] D. Bertsimas, J. Dunn, C. Pawlowski, and Y. Zhuo. \u201cRobust classification\u201d. In: INFORMS Journal on Optimization 1.1 (2019), pp. 2\u201334.   \n[11] B. Bischl, G. Casalicchio, M. Feurer, P. Gijsbers, F. Hutter, M. Lang, R. Mantovani, J. van Rijn, and J. Vanschoren. \u201cOpenML: A benchmarking layer on top of OpenML to quickly create, download, and share systematic benchmarks\u201d. In: NeurIPS \u2013 Track on Datasets and Benchmarks (2021).   \n[12] B. Bischl, P. Kerschke, L. Kotthoff, M. Lindauer, Y. Malitsky, A. Fr\u00e9chette, H. Hoos, F. Hutter, K. Leyton-Brown, K. Tierney, and J. Vanschoren. \u201cASlib: A benchmark library for algorithm selection\u201d. In: Artificial Intelligence 237 (2016), pp. 41\u201358.   \n[13] H. Blocher, G. Schollmeyer, C. Jansen, and M. Nalenz. \u201cDepth functions for partial orders with a descriptive analysis of machine learning algorithms\u201d. In: Proceedings of the Thirteenth International Symposium on Imprecise Probability: Theories and Applications. Vol. 215. Proceedings of Machine Learning Research. PMLR, 2023, pp. 59\u201371.   \n[14] R. Cabanas, A. Antonucci, D. Huber, and M. Zaffalon. \u201cCREDICI: A Java library for causal inference by credal networks\u201d. In: International Conference on Probabilistic Graphical Models. Ed. by M. Jaeger and T. Nielsen. Vol. 138. PMLR. 2020, pp. 597\u2013600.   \n[15] B. Calvo and G. Santaf\u00e9. \u201cscmamp: Statistical comparison of multiple algorithms in multiple problems\u201d. In: The R Journal 8.1 (2016), pp. 248\u2013256.   \n[16] M. Caprio, Y. Sale, E. H\u00fcllermeier, and I. Lee. \u201cA Novel Bayes\u2019 Theorem for Upper Probabilities\u201d. In: Epistemic Uncertainty in Artificial Intelligence \u2013 First International Workshop, Epi UAI 2023, Pittsburgh, PA, USA, August 4, 2023, Revised Selected Papers. Ed. by F. Cuzzolin and M. Sultana. Vol. 14523. Lecture Notes in Computer Science. Springer, 2024, pp. 1\u201312.   \n[17] M. Caprio, M. Sultana, E. Elia, and F. Cuzzolin. Credal Learning Theory. 2024. arXiv: 2402.00957. URL: https://arxiv.org/abs/2402.00957.   \n[18] Y. Carranza and S. Destercke. \u201cImprecise Gaussian discriminant classification\u201d. In: Pattern Recognition 112 (2021), p. 107739.   \n[19] L. Chang. \u201cPartial order relations for classification comparisons\u201d. In: Canadian Journal of Statistics 48.2 (2020), pp. 152\u2013166.   \n[20] T. Chen, T. He, M. Benesty, V. Khotilovich, Y. Tang, H. Cho, K. Chen, R. Mitchell, I. Cano, T. Zhou, M. Li, J. Xie, M. Lin, Y. Geng, Y. Li, and J. Yuan. Package \u2018xgboost\u2019. [Accessed: 13.05.2024]. 2023. URL: https://cran.r-project.org/web/packages/xgboost/ xgboost.pdf.   \n[21] C. Coleman, D. Narayanan, D. Kang, T. Zhao, J. Zhang, L. Nardi, P. Bailis, K. Olukotun, C. R\u00e9, and M. Zaharia. \u201cDawnbench: An end-to-end deep learning benchmark and competition\u201d. In: Training 100.101 (2017), p. 102.   \n[22] G. Corani, A. Benavoli, J. Dem\u0161ar, F. Mangili, and M. Zaffalon. \u201cStatistical comparison of classifiers through Bayesian hierarchical modelling\u201d. In: Machine Learning 106.11 (2017), pp. 1817\u20131837.   \n[23] H. Dai, Y. Xue, N. He, Y. Wang, N. Li, D. Schuurmans, and B. Dai. \u201cLearning to optimize for stochastic dominance constraints\u201d. In: International Conference on Artificial Intelligence and Statistics. Ed. by F. Ruiz, J. Dy, and J. van de Meent. Vol. 206. PMLR. 2023, pp. 8991\u20139009.   \n[24] J. Dem\u0161ar. \u201cStatistical comparisons of classifiers over multiple data sets\u201d. In: Journal of Machine Learning Research 7 (2006), pp. 1\u201330.   \n[25] S. Destercke, I. Montes, and E. Miranda. \u201cProcessing distortion models: A comparative study\u201d. In: International Journal of Approximate Reasoning 145 (2022), pp. 91\u2013120.   \n[26] E. Dimitriadou, K. Hornik, F. Leisch, D. Meyer, and A. Weingessel. Package \u2018e1071\u2019. [Accessed: 13.05.2024]. 2010. URL: https://cran.r-project.org/web/packages/e1071/ e1071.pdf.   \n[27] D. Donoho. \u201cData Science at the Singularity\u201d. In: 2023 IMS International Conference on Statistics and Data Science (ICSDS). Ed. by R. Liu and A. Qu. 2023, p. 3.   \n[28] R. Dudley. \u201cCentral limit theorems for empirical measures\u201d. In: The Annals of Probability 6.6 (1978), pp. 899\u2013929.   \n[29] R. Durrett. Probability: Theory And Examples. Vol. 49. Cambridge University Press, 2019.   \n[30] S. Dutta, M. Caprio, V. Lin, M. Cleaveland, K.J. Jang, I. Ruchkin, O. Sokolsky, and I. Lee. Distributionally Robust Statistical Verification with Imprecise Neural Networks. 2023. arXiv: 2308.14815 [cs.AI]. URL: https://arxiv.org/abs/2308.14815.   \n[31] M. Eugster, T. Hothorn, and F. Leisch. \u201cDomain-based benchmark experiments: Exploratory and inferential analysis\u201d. In: Austrian Journal of Statistics 41.1 (2012), pp. 5\u201326.   \n[32] J. Friedman, T. Hastie, R. Tibshirani, B. Narasimhan, K. Tay, N. Simon, and J. Qian. Package \u2018glmnet\u2019. [Accessed: 13.05.2024]. 2021. URL: https://cran.r- project.org/web/ packages/glmnet/glmnet.pdf.   \n[33] M. Friedman. \u201cThe use of ranks to avoid the assumption of normality implicit in the analysis of variance\u201d. In: Journal of the American Statistical Association 32.200 (1937), pp. 675\u2013701.   \n[34] S. Garc\u00eda, A. Fern\u00e1ndez, J. Luengo, and F. Herrera. \u201cAdvanced nonparametric tests for multiple comparisons in the design of experiments in computational intelligence and data mining: Experimental analysis of power\u201d. In: Information Sciences 180.10 (2010), pp. 2044\u20132064.   \n[35] S. Garc\u00eda and F. Herrera. \u201cAn extension on \u201cStatistical comparisons of classifiers over multiple cata sets\u201d for all pairwise comparisons\u201d. In: Journal of Machine Learning Research 9 (2008), pp. 2677\u20132694.   \n[36] M. Graczyk, T. Lasota, Z. Telec, and B. Trawi\u00b4nski. \u201cNonparametric statistical analysis of machine learning algorithms for regression problems\u201d. In: International Conference on Knowledge-Based and Intelligent Information and Engineering Systems. Ed. by R. Setchi, I. Jordanov, R. Howlett, and L. Jain. Springer. 2010, pp. 111\u2013120.   \n[37] N. Hansen, A. Auger, D. Brockhoff, and T. Tu\u0161ar. \u201cAnytime performance assessment in blackbox optimization benchmarking\u201d. In: IEEE Transactions on Evolutionary Computation 26.6 (2022), pp. 1293\u20131305.   \n[38] K. Hechenbichler and K. Schliep. Weighted k-Nearest-Neighbor Techniques and Ordinal Classification. Technical Report, LMU. 2004. URL: http://nbn-resolving.de/urn/ resolver.pl?urn $\\cdot$ nbn:de:bvb:19-epub-1769-9.   \n[39] K. Hornik, C. Buchta, T. Hothorn, A. Karatzoglou, D. Meyer, and A. Zeileis. Package \u2019rweka\u2019. [Accessed: 13.05.2023]. 2007. URL: https://cran.r-project.org/web/packages/ RWeka/index.html.   \n[40] T. Hothorn, F. Leisch, A. Zeileis, and K. Hornik. \u201cThe design and analysis of benchmark experiments\u201d. In: Journal of Computational and Graphical Statistics 14.3 (2005), pp. 675\u2013699.   \n[41] P. Huber. Robust Statistics. New York: Wiley, 1981.   \n[42] P. Huber. \u201cThe use of Choquet capacities in statistics\u201d. In: Proceedings of the 39th Session of the International Statistical Institute 45 (1973), pp. 181\u2013191.   \n[43] S. Ishii and D. Ljunggren. \u201cA comparative analysis of robustness to noise in machine learning classifiers\u201d. PhD thesis. KTH Royal Institute of Technology, 2021.   \n[44] H. Ismail Fawaz, G. Forestier, J. Weber, L. Idoumghar, and P.-A. Muller. \u201cDeep learning for time series classification: a review\u201d. In: Data Mining and Knowledge Discovery 33.4 (2019), pp. 917\u2013963.   \n[45] C. Jansen, M. Nalenz, G. Schollmeyer, and T. Augustin. \u201cStatistical comparisons of classifiers by generalized stochastic dominance\u201d. In: Journal of Machine Learning Research 24 (2023), pp. 1\u201337.   \n[46] C. Jansen, G. Schollmeyer, and T. Augustin. \u201cConcepts for decision making under severe uncertainty with partial ordinal and partial cardinal preferences\u201d. In: International Journal of Approximate Reasoning 98 (2018), pp. 112\u2013131.   \n[47] C. Jansen, G. Schollmeyer, and T. Augustin. \u201cMulti-target decision making under conditions of severe uncertainty\u201d. In: Modeling Decisions for Artificial Intelligence. Ed. by V. Torra and Y. Narukawa. Springer, 2023, pp. 45\u201357.   \n[48] C. Jansen, G. Schollmeyer, H. Blocher, J. Rodemann, and T. Augustin. \u201cRobust statistical comparison of random variables with locally varying scale of measurement\u201d. In: Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence. Ed. by R. Evans and I. Shpitser. Vol. 216. Proceedings of Machine Learning Research. PMLR, 2023, pp. 941\u2013952.   \n[49] M. Kuhn. Package \u2019caret\u2019. [Accessed: 13.05.2023]. 2015. URL: https://cran.r-project. org/web/packages/caret/index.html.   \n[50] J. Laux, S. Wachter, and B. Mittelstadt. \u201cTrustworthy artificial intelligence and the European Union AI act: On the conflation of trustworthiness and acceptability of risk\u201d. In: Regulation & Governance 18.1 (2024), pp. 3\u201332.   \n[51] N. Lavesson and P. Davidsson. \u201cEvaluating learning algorithms and classifiers\u201d. In: International Journal of Intelligent Information and Database Systems 1 (2007), pp. 37\u201352.   \n[52] J. Lienen and E. H\u00fcllermeier. \u201cCredal Self-Supervised Learning\u201d. In: Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual. Ed. by MA. Ranzato, A. Beygelzimer, Y.N. Dauphin, P. Liang, and J.W. Vaughan. 2021, pp. 14370\u201314382.   \n[53] D. Mattos, L. Ruud, J. Bosch, and H. Holmstr\u00f6m Olsson. On the assessment of benchmark suites for algorithm comparison. 2021. arXiv: 2104.07381 [cs.NE].   \n[54] D. Maua and F. Cozman. \u201cThirty years of credal networks: Specification, algorithms and complexity\u201d. In: International Journal of Approximate Reasoning 126 (2020), pp. 133\u2013157.   \n[55] D. Maua and C. de Campos. \u201cEditorial to: Special issue on robustness in probabilistic graphical models\u201d. In: International Journal of Approximate Reasoning 137 (2021), p. 113.   \n[56] D. McFadden. \u201cTesting for stochastic dominance\u201d. In: Studies in the Economics of Uncertainty. Ed. by T. Fomby and T. Seo. Springer, 1989, pp. 113\u2013134.   \n[57] O. Mersmann, M. Preuss, H. Trautmann, B. Bischl, and C. Weihs. \u201cAnalyzing the BBOB results by means of benchmarking concepts\u201d. In: Evolutionary Computation 23 (2015), pp. 161\u2013185.   \n[58] D. Meyer, F. Leisch, and K. Hornik. \u201cThe support vector machine under test\u201d. In: Neurocomputing 55.1 (2003), pp. 169\u2013186.   \n[59] C. Molnar, G. Casalicchio, and B. Bischl. \u201cQuantifying model complexity via functional decomposition for better post-hoc interpretability\u201d. In: Machine Learning and Knowledge Discovery in Databases. Ed. by P. Cellier and K. Driessens. Springer International Publishing, 2020, pp. 193\u2013204.   \n[60] I. Montes, E. Miranda, and S. Destercke. \u201cUnifying neighbourhood and distortion models: Part II \u2013 new models and synthesis\u201d. In: International Journal of General Systems 49 (2020), pp. 636\u2013674.   \n[61] K. Mosler. \u201cTesting whether two distributions are stochastically ordered or not\u201d. In: Grundlagen der Statistik und ihre Anwendungen: Festschrift f\u00fcr Kurt Weichselberger. Ed. by H. Rinne, B. R\u00fcger, and H. Strecker. Physica-Verlag, 1995, pp. 149\u2013155.   \n[62] M. Nalenz and T. Augustin. \u201cCompressed rule ensemble learning\u201d. In: Proceedings of The 25th International Conference on Artificial Intelligence and Statistics. Ed. by G. Camps-Valls, F. Ruiz, and I. Valera. Vol. 151. Proceedings of Machine Learning Research. PMLR, 2022, pp. 9998\u201310014.   \n[63] P. Nemenyi. \u201cDistribution-free Multiple Comparisons\u201d. PhD thesis. Princeton University, 1963.   \n[64] R. Olson, W. La Cava, P. Orzechowski, R. Urbanowicz, and J. Moore. \u201cPMLB: a large benchmark suite for machine learning evaluation and comparison\u201d. In: BioData Mining 10 (2017), p. 36.   \n[65] S. Ott, A. Barbosa-Silva, K. Blagec, J. Brauner, and M. Samwald. \u201cMapping global dynamics of benchmark creation and saturation in artificial intelligence\u201d. In: Nature Communications 13.1 (2022), p. 6793.   \n[66] Huber P. and V. Strassen. \u201cMinimax tests and the Neyman-Pearson lemma for capacities\u201d. In: The Annals of Statistics 1 (1973), pp. 251\u2013263.   \n[67] T. Range and L. \u00d8sterdal. \u201cFirst-order dominance: stronger characterization and a bivariate checking algorithm\u201d. In: Mathematical Programming 173 (2019), pp. 193\u2013219.   \n[68] B. Ripley and W. Venables. Package \u2018nnet\u2019. [Accessed: 13.05.2024]. 2016. URL: https: //staff.fmi.uvt.ro/\\~daniela.zaharie/dm2019/RO/lab/lab3/biblio/nnet.pdf.   \n[69] J. Rodemann and T. Augustin. \u201cAccounting for Gaussian process imprecision in Bayesian optimization\u201d. In: International Symposium on Integrated Uncertainty in Knowledge Modelling and Decision Making (IUKM). Springer. 2022, pp. 92\u2013104.   \n[70] J. Rodemann, C. Jansen, G. Schollmeyer, and T. Augustin. \u201cIn all likelihoods: Robust selection of pseudo-labeled data\u201d. In: Proceedings of the Thirteenth International Symposium on Imprecise Probability: Theories and Applications. Ed. by E. Miranda, I. Montes, E. Quaeghebeur, and B. Vantaggi. Vol. 215. Proceedings of Machine Learning Research. PMLR, 2023, pp. 412\u2013425.   \n[71] Julian Rodemann and Hannah Blocher. \u201cPartial Rankings of Optimizers\u201d. In: International Conference on Learning Representations (ICLR), Tiny Papers Track. 2024.   \n[72] J. S\u00e1ez, J. Luengo, and F. Herrera. \u201cEvaluating the classifier behavior with noisy data considering performance and robustness: The equalized loss of accuracy measure\u201d. In: Neurocomputing 176 (2016), pp. 26\u201335.   \n[73] L. Schmitt. \u201cMapping global AI governance: a nascent regime in a fragmented landscape\u201d. In: AI and Ethics 2.2 (2022), pp. 303\u2013314.   \n[74] F. Schneider, L. Balles, and P. Hennig. \u201cDeepOBS: A deep learning optimizer benchmark suite\u201d. In: International Conference on Learning Representations. 2018.   \n[75] L. Schneider, B. Bischl, and J. Thomas. \u201cMulti-ojective optimization of performance and interpretability of tabular supervised machine learning Mmodels\u201d. In: Proceedings of the Genetic and Evolutionary Computation Conference. 2023, pp. 538\u2013547.   \n[76] G. Schollmeyer, C. Jansen, and T. Augustin. Detecting stochastic dominance for poset-valued random variables as an example of linear programming on closure systems. [Accessed: 13.05.2024]. 2017. URL: https://epub.ub.uni-muenchen.de/40416/13/TR_209.pdf.   \n[77] M. Shaked and G. Shanthikumar. Stochastic orders. Springer, 2007.   \n[78] A. Shirali, R. Abebe, and M. Hardt. \u201cA theory of dynamic benchmarks\u201d. In: The Eleventh International Conference on Learning Representations. 2023.   \n[79] T. Therneau, B. Atkinson, and B. Ripley. Package \u2018rpart\u2019. [Accessed: 15.02.2023]. 2015. URL: http://cran.ma.ic.ac.uk/web/packages/rpart/rpart.pdf.   \n[80] L. Utkin. \u201cAn imprecise deep forest for classification\u201d. In: Expert Systems with Applications 141 (2020), p. 112978.   \n[81] L. Utkin and A. Konstantinov. \u201cAttention-based random forest and contamination model\u201d. In: Neural Networks 154 (2022), pp. 346\u2013359.   \n[82] J. Van Rijn, B. Bischl, L. Torgo, B. Gao, V. Umaashankar, S. Fischer, P. Winter, B. Wiswedel, M. Berthold, and J. Vanschoren. \u201cOpenML: A collaborative science platform\u201d. In: Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2013, Prague, Czech Republic, September 23-27, 2013, Proceedings, Part III 13. Springer. 2013, pp. 645\u2013649.   \n[83] H. Vandierendonck and K. De Bosschere. \u201cExperiments with subsetting benchmark suites\u201d. In: IEEE International Workshop on Workload Characterization, 2004. WWC-7. 2004. 2004, pp. 55\u201362.   \n[84] V. Vapnik. The Nature Of Statistical Learning Theory. Springer, 1999.   \n[85] P. Walley. Statistical Reasoning with Imprecise Probabilities. London: Chapman and Hall, 1991.   \n[86] K. Weichselberger. Elementare Grundbegriffe einer allgemeineren Wahrscheinlichkeitsrechnung I: Intervallwahrscheinlichkeit als umfassendes Konzept [Elementary Foundations of a More General Calculus of Probability I: Interval Probability as a Comprehensive Concept]. Physica, Heidelberg, 2001.   \n[87] H. Wickham, R. Fran\u00e7ois, L. Henry, and K. M\u00fcller. Package \u2018dplyr\u2019. [Accessed: 13.05.2024]. 2019. URL: https://cran.r-project.org/web/packages/dplyr/index.html.   \n[88] M. Wright and A. Ziegler. \u201cranger: A fast implementation of random gorests for high dimensional data in $C++$ and R\u201d. In: Journal of Statistical Software 77.1 (2017), pp. 1\u201317.   \n[89] B. Yu and K. Kumbier. \u201cVeridical data science\u201d. In: Proceedings of the National Academy of Science 117.8 (2020), pp. 3920\u20133929.   \n[90] G. Zhang and M. Hardt. Inherent Trade-Offs between Diversity and Stability in Multi-Task Benchmark. 2024. arXiv: 2405.01719 [cs.LG].   \n[91] J. Zhang, M. Harman, L. Ma, and Y. Liu. \u201cMachine learning testing: Survey, landscapes and horizons\u201d. In: IEEE Transactions on Software Engineering 48.1 (2020), pp. 1\u201336.   \n[92] X. Zhu and X. Wu. \u201cClass noise vs. attribute noise: A quantitative study\u201d. In: Artificial Intelligence Review 22 (2004), pp. 177\u2013210.   \n[93] X. Zhu, X. Wu, and Y. Yang. \u201cError detection and impact-sensitive instance ranking in noisy datasets\u201d. In: Proceedings of the National Conference on Artificial Intelligence. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999. 2004, pp. 378\u2013384. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Mathematical background ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Basic definitions from order theory ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A binary relation $R$ on a set $M$ is a subset of the Cartesian product of $M$ with itself, i.e., $R\\subseteq M\\times M$ . $R$ is called reflexive, if $(a,a)\\in R_{*}$ , transitive, if $(a,b),\\,\\bar{(b,c)}\\,\\in\\,R\\Rightarrow(a,c)\\,\\in\\,R,$ , antisymmetric, if $(a,b),(b,a)\\,\\in\\,R\\,\\Rightarrow\\,\\stackrel{.}{a}\\,=\\,\\stackrel{.}{b},$ , and complete, if $(a,b)\\,\\in\\,R$ or $(b,a)\\,\\in\\,R$ (or both) for arbitrary elements $a,b,c\\in M$ . A preference relation is a binary relation that is complete and transitive; a preorder is a binary relation that is reflexive and transitive; a linear order is a preference relation that is antisymmetric; a partial order is a preorder that is antisymmetric. If $R$ is a preorder, we denote by $P_{R}\\subseteq M\\times M$ its strict part and by $I_{R}\\subseteq M\\times M$ its indifference part, defined by $(a,b)\\in P_{R}\\Leftrightarrow(a,b)\\in R\\land(b,a)\\notin R$ , and $(a,b)\\in I_{R}\\Leftrightarrow(a,b)\\in R\\land(b,a)\\in R$ . ", "page_idx": 15}, {"type": "text", "text": "A.2 Detailed description of the permutation test from Section 4.1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section we describe in detail the statistical test for the hypothesis pair $(H_{0}^{C^{\\prime}},\\neg H_{0}^{C^{\\prime}})$ discussed in Section 4.1 and first introduced in [48]. Moreover, we give further details on our proposed extension of this test to the global hypothesis pair $(H_{0},\\neg H_{0})$ in both the static and the dynamic variant. ", "page_idx": 15}, {"type": "text", "text": "A.2.1 Preliminaries ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Before we can describe the test from Section 4.1 in detail, we first need to recall two more definitions. Definition 7. Let $\\pmb{\\mathscr{A}}\\,=\\,[A,R_{1},R_{2}]$ be a preference system. We call $\\boldsymbol{\\mathcal{A}}$ bounded, if there exist $a_{*},a^{*}\\in A$ such that $(a^{*},a)\\in R_{1}$ , and $(a,a_{*})\\in R_{1}$ for all $a\\in A$ , and $(a^{\\ast},a_{\\ast})\\in P_{R_{1}}$ . ", "page_idx": 15}, {"type": "text", "text": "Definition 8. Let $\\boldsymbol{A}=[A,R_{1},R_{2}]$ be a consistent and bounded preference system with $a_{*},a^{*}$ as before. Define ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{N}_{A}:=\\{u\\in\\mathcal{U}_{A}:u(a_{*})=0\\,\\land\\,u(a^{*})=1\\}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For $\\delta\\in[0,1)$ , denote by $\\mathcal{N}_{A}^{\\delta}$ the set of all $u\\in\\mathcal{N}_{A}$ with ", "page_idx": 15}, {"type": "equation", "text": "$$\nu(a)-u(b)\\geq\\delta\\quad\\land\\quad u(c)-u(d)-u(e)+u(f)\\geq\\delta\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "for all $(a,b)\\in P_{R_{1}}$ and for all $((c,d),(e,f))\\in P_{R_{2}}$ . ", "page_idx": 15}, {"type": "text", "text": "We now start by describing an adapted version of the permutation test for the hypothesis pairs $(H_{0}^{C^{\\prime}},\\neg H_{0}^{C^{\\prime}})$ proposed in [48]. For a concrete realization of the i.i.d.-sample of data sets ${\\cal D}_{1}:=$ $T_{1}(\\omega_{0}),\\ldots,D_{s}:=T_{s}(\\omega_{0})\\in\\mathcal{D}$ with $s\\in\\mathbb{N}$ associated with $\\omega_{0}\\in\\Omega$ , we define the set ", "page_idx": 15}, {"type": "equation", "text": "$$\n(C,C^{\\prime})_{\\omega_{0}}=\\{\\Phi(C,D_{i}):i\\leq s\\}\\cup\\{\\Phi(C^{\\prime},D_{i}):i\\leq s\\}\\cup\\{\\mathbf{0},\\mathbf{1}\\},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where 0 is the vector containing $n$ zeros and 1 is the vector containing $n$ ones. Denote by $\\mathcal{P}_{\\omega_{0}}$ the restriction of $\\mathcal{P}$ to $(C,C^{\\prime})_{\\omega_{0}}$ . It is then easy to verify that $\\mathcal{P}_{\\omega_{0}}$ is a consistent and bounded preference system with $a_{*}:=\\mathbf{0}$ and $a^{*}:=\\mathbf{1}$ . For testing the hypothesis pair $(H_{0}^{C^{\\prime}},\\neg H_{0}^{C^{\\prime}})$ defined and discussed in Section 4.1 of the main text, we then use the following regularized test statistic for the specific sample induced by $\\omega_{0}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\nd_{s}^{\\delta}(C^{\\prime},C)(\\omega_{0}):=\\operatorname*{inf}_{u\\in\\mathcal{N}_{\\mathcal{P}_{\\omega_{0}}}^{\\mu_{\\delta}}}\\sum_{z\\in(C,C^{\\prime})_{\\omega_{0}}}u(z)\\cdot\\big(\\hat{\\pi}_{C^{\\prime}}^{\\omega_{0}}(\\{z\\})-\\hat{\\pi}_{C}^{\\omega_{0}}(\\{z\\})\\big)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "with $\\delta\\in[0,1]$ and $\\mu_{\\delta}:=\\delta\\cdot\\operatorname*{sup}\\{\\xi:\\mathcal{N}_{A_{\\omega}}^{\\xi}\\neq\\emptyset\\}$ , and $\\hat{\\pi}_{C}^{\\omega_{0}}$ resp. $\\hat{\\pi}_{C^{\\prime}}^{\\omega_{0}}$ are the emiprical probability measures of the performances of $C$ resp. $C^{\\prime}$ for the specific sample induced by $\\omega_{0}$ . ", "page_idx": 15}, {"type": "text", "text": "A.2.2 Testing scheme for $(H_{0}^{C^{\\prime}},\\neg H_{0}^{C^{\\prime}})$ ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We denote our samples as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathbf{x}}&{:=}&{(x_{1},\\ldots,x_{s}):=(\\Phi(C,D_{1}),\\ldots,\\Phi(C,D_{s}))}\\\\ {\\mathbf{y}}&{:=}&{(y_{1},\\ldots,y_{s}):=(\\Phi(C^{\\prime},D_{1}),\\ldots,\\Phi(C^{\\prime},D_{s}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The concrete testing scheme for the permutation test for hypothesis pair $(H_{0}^{C^{\\prime}},\\neg H_{0}^{C^{\\prime}})$ then looks as follows: ", "page_idx": 15}, {"type": "text", "text": "Step 1: Take the pooled data sample: $\\mathbf{w}:=(w_{1},\\ldots,w_{2s}):=(x_{1},\\ldots,x_{s},y_{1},\\ldots,y_{s})$ ) ", "page_idx": 16}, {"type": "text", "text": "Step 2: Take all $r:=(_{s}^{2s})$ index sets $I\\subseteq\\{1,\\ldots,2s\\}$ of size $s$ . Evaluate $d_{s}^{\\delta}(C^{\\prime},C)$ for $(w_{i})_{i\\in I}$ and $(w_{i})_{i\\in\\{1,\\dots,n+m\\}\\backslash I}$ instead of $\\mathbf{x}$ and $\\mathbf{y}$ . Denote the evaluations by $d_{I}^{\\delta}$ . ", "page_idx": 16}, {"type": "text", "text": "Step 3: Sort all $d_{I}^{\\delta}$ in increasing order to get $d_{(1)}^{\\delta},\\ldots,d_{(r)}^{\\delta}$ ", "page_idx": 16}, {"type": "text", "text": "Step 4: Reject $H_{0}^{C^{\\prime}}$ if $d_{s}^{\\delta}(C^{\\prime},C)(\\omega_{0})$ is strictly smaller than $d_{(\\ell)}^{\\delta}$ , with $\\ell:=\\;\\lfloor\\alpha\\cdot r\\rfloor$ and $\\alpha$ the significance level. ", "page_idx": 16}, {"type": "text", "text": "Note that, for large $\\binom{2s}{s}$ , we can approximate the above resampling scheme by computing $d_{I}^{\\delta}$ only for a large number $N$ of randomly drawn $I$ . Moreover, note that only the i.i.d. assumption is needed for the above test to be valid. ", "page_idx": 16}, {"type": "text", "text": "A.2.3 Static GSD-test ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "As argued in the Section 4.1 of the main part of the paper, if we want to obtain a valid statistical test at the significance level $\\alpha\\in[0,1]$ for hypothesis pair $(H_{0},\\neg H_{0})$ , we can simply perform all pairwise tests of hypothesis pairs $(H_{0}^{C^{\\prime}},\\neg H_{0}^{C^{\\prime}})$ at this same significance level $\\alpha$ . We can then reject the hypothesis $H_{0}$ at level $\\alpha$ if we can reject each hypothesis $H_{0}^{C^{\\prime}}$ at level $\\alpha$ or, in other words, if ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{min}\\Biggl\\{\\frac{1}{N}\\cdot\\sum_{I\\in\\mathbb{Z}_{N}}{\\mathbb{1}}_{\\left\\{d_{s}^{\\delta}(C^{\\prime},C)(\\omega_{0})<d_{I}^{\\delta}\\right\\}}:C^{\\prime}\\in\\mathcal{C}\\setminus\\{C\\}\\Biggr\\}\\geq1-\\alpha.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We call this the static GSD-test. ", "page_idx": 16}, {"type": "text", "text": "To see that this procedure indeed gives a valid level- $\\cdot\\alpha$ test for the global hypothesis pair $(H_{0},\\neg H_{0})$ , observe that \u2013 assuming $H_{0}$ to be true \u2013 the probability of $H_{0}$ being rejected equals the probability of all hypothesis $H_{0}^{C^{\\prime}}$ being rejected simultaneously. The latter probability \u2013 still assuming $H_{0}$ to be true \u2013 is obviously bounded from above by the probability that one specific hypothesis $H_{0}^{C^{*}}$ is rejected, which itself is bounded from above by the significance level $\\alpha$ by construction. ", "page_idx": 16}, {"type": "text", "text": "A.2.4 Dynamic GSD-test ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "As discussed in the main text and reprinted here again for convenience of the reader, a slightly modified test in the context of the GSD-front is directly derivable: If one is rather interested in identifying the maximal subset $S_{\\mathrm{max}}$ of $\\mathcal{C}$ for which $C$ significantly lies in the GSD-front, i.e., in testing the hypothesis pairs $(\\tilde{H}_{0}^{S},\\neg\\tilde{H}_{0}^{S})$ for all ${\\mathcal{S}}\\subseteq{\\mathcal{C}}$ simultaneously, the following alternative test would be a statistically valid level- $\\alpha$ test: First, perform all individual tests for $(H_{0}^{C^{\\prime}},\\neg H_{0}^{C^{\\prime}})$ with level $\\frac{\\alpha}{c}$ . Then identify $S_{\\mathrm{max}}$ as the set of all classifiers from $\\mathcal{C}$ for which the individual hypotheses are rejected. The (random) alternative hypothesis $\\tilde{H}_{1}^{S_{\\mathrm{max}}}:C\\in\\operatorname*{gsd}(S_{\\mathrm{max}})$ is then statistically valid in the sense of being false only with a probability bounded by $\\alpha$ . We call this the dynamic GSD-test. ", "page_idx": 16}, {"type": "text", "text": "To see that this procedure indeed gives a valid level- $\\alpha$ test for the (random) hypothesis pair $(\\tilde{H}_{0}^{S_{\\mathrm{max}}},\\neg\\tilde{H}_{0}^{S_{\\mathrm{max}}})$ , observe that \u2013 under the null hypothesis \u2013 the probability of $C$ lying in the GSDfront of some random subset $\\boldsymbol{S}$ of $\\mathcal{C}$ is bounded from above by the sum of probabilities of $C$ lying in the GSD-front of $\\{C,S\\}$ , where summation is over all $S\\in S$ . As each of these probabilities is bounded from above by $\\frac{\\alpha}{c}$ by construction, the corresponding sum is bounded from above by $|S|\\cdot{\\frac{\\alpha}{c}}$ . Finally, as $\\vert S\\vert\\le c$ , this gives the desired upper bound of $\\alpha$ . ", "page_idx": 16}, {"type": "text", "text": "A.2.5 Computation and regularization ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Note that the test statistic $d_{s}^{\\delta}(C^{\\prime},C)(\\omega_{0})$ can be computed by solving a linear optimization problem (see [48, Proposition 4]) and, hence, the test just described is computationally tractable. ", "page_idx": 16}, {"type": "text", "text": "Moreover, note that in both applications in Section 5 the tests are based on the unregularized statistics $d_{s}^{0}(C^{\\prime},C)$ , as the regularization performed in [48] aims at reaching a goal which is not primarily relevant for the present paper: The authors there are primarily interested in significantly detecting GSD of one variable over the other. Consequently, their regularization aims at making the test more sensitive for exactly this purpose. In contrast, in our study we are primarily interested in significantly detecting incomparabilities between variables, making the regularization by far less natural. ", "page_idx": 16}, {"type": "text", "text": "A.3 Detailed description of the robustness check from Section 4.2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section we describe in detail the robustification of the statistical test for the hypothesis pair $(H_{0}^{C^{\\prime}},\\neg H_{0}^{C^{\\prime}})$ discussed in Section 4.2 and first introduced in [48]. Moreover, we give further details on our proposed extension of this robustified statistical test to the global hypothesis pair $(H_{0},\\neg H_{0})$ in both the static and the dynamic variant. ", "page_idx": 17}, {"type": "text", "text": "A.3.1 Preliminiaries ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "If we assume, as done in Section 4.2, that up to $k\\leq s$ of the observations in our sample $T_{1},\\mathbf{\\Pi}\\cdot\\mathbf{\\Pi}\\cdot,T_{s}$ might be contaminated and, accordingly, follow any arbitrary distribution, then we have to base the permutation test for hypothesis pair $(H_{0}^{C^{\\prime}},\\neg H_{0}^{C^{\\prime}})$ on a worst-case analysis of between the measures contained in $\\mathcal{M}_{C}$ and $\\mathbf{\\mathcal{M}}_{C^{\\prime}}$ defined instead of the true empirical measures of the two samples induced by the classifiers $C$ and $C^{\\prime}$ . If again $D_{1}:=T_{1}(\\omega_{0}),\\dots,D_{s}:=T_{s}(\\omega_{0})\\in\\mathcal{D}$ is a concrete (now potentially contaminated) sample associated with $\\omega_{0}\\in\\Omega$ , and we again define $\\mathbf{x}$ and $\\mathbf{y}$ as in Section A.2, then the observed contamination models of $C$ and $C^{\\prime}$ look as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{M}_{C}(\\omega_{0})=\\Big\\{(1-\\frac{k}{s})\\hat{\\pi}_{C}^{c o n t,\\omega_{0}}+\\frac{k}{s}\\mu:\\mu\\mathrm{~probability~measure}\\Big\\},}\\\\ &{\\mathcal{M}_{C^{\\prime}}(\\omega_{0})=\\Big\\{(1-\\frac{k}{s})\\hat{\\pi}_{C^{\\prime}}^{c o n t,\\omega_{0}}+\\frac{k}{s}\\mu:\\mu\\mathrm{~probability~measure}\\Big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "A.3.2 Testing scheme for robustified test on $(H_{0}^{C^{\\prime}},\\neg H_{0}^{C^{\\prime}})$ ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "If we set ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overline{{d_{s}^{\\delta}}}(C^{\\prime},C)(\\omega_{0}):=\\operatorname*{sup}_{\\pi_{1}\\in M_{C^{\\prime}}(\\omega_{0}),\\pi_{2}\\in M_{C}(\\omega_{0})}\\left(\\operatorname*{inf}_{u\\in N_{\\mathcal{P}_{\\omega_{0}}}^{\\mu,\\delta}}\\sum_{\\substack{z\\in(C,C^{\\prime})_{\\omega_{0}}}}u(z)\\cdot(\\pi_{1}(\\{z\\})-\\pi_{2}(\\{z\\}))\\right),}\\\\ &{\\frac{d_{s}^{\\delta}}{c}(C^{\\prime},C)(\\omega_{0}):=\\operatorname*{inf}_{\\pi_{1}\\in M_{C^{\\prime}}(\\omega_{0}),\\pi_{2}\\in M_{C}(\\omega_{0})}\\left(\\operatorname*{inf}_{u\\in N_{\\mathcal{P}_{\\omega_{0}}}^{\\mu,\\delta}}\\sum_{\\substack{z\\in(C,C^{\\prime})_{\\omega_{0}}}}u(z)\\cdot(\\pi_{1}(\\{z\\})-\\pi_{2}(\\{z\\}))\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "then the concrete testing scheme for the permutation test for hypothesis pair $(H_{0}^{C^{\\prime}},\\neg H_{0}^{C^{\\prime}})$ under at most $k$ contaminated sample members looks as follows: ", "page_idx": 17}, {"type": "text", "text": "Step 1: Take the pooled data sample: $\\mathbf{w}:=(w_{1},\\ldots,w_{2s}):=(x_{1},\\ldots,x_{s},y_{1},\\ldots,y_{s})$ ", "page_idx": 17}, {"type": "text", "text": "Step 2: Take all $r:=(_{s}^{2s})$ index sets $I\\subseteq\\{1,\\ldots,2s\\}$ of size $s$ . Evaluate $d_{s}^{\\delta}(C^{\\prime},C)$ for $(w_{i})_{i\\in I}$ and $(w_{i})_{i\\in\\{1,\\dots,n+m\\}\\backslash I}$ instead of $\\mathbf{x}$ and $\\mathbf{y}$ . Denote the evaluations by $\\underline{{d_{I}^{\\delta}}}$ . ", "page_idx": 17}, {"type": "text", "text": "Step 3: Sort all $\\underline{{d_{I}^{\\delta}}}$ in increasing order to get $\\underline{{d}}^{\\delta}(1)\\cdot\\cdot\\cdot,\\underline{{d}}^{\\delta}(r)$ ", "page_idx": 17}, {"type": "text", "text": "Step 4: Reject $H_{0}^{C^{\\prime}}$ if $\\overline{{d_{s}^{\\delta}}}(C^{\\prime},C)(\\omega_{0})$ is strictly smaller than $\\underline{{d}}^{\\delta}{}_{(\\ell)}$ , with $\\ell:=\\;\\lfloor\\alpha\\cdot r\\rfloor$ and $\\alpha$ the significance level. ", "page_idx": 17}, {"type": "text", "text": "The adapted testing scheme just described gives a valid (yet conservative) level- $\\alpha$ -test for the hypothesis pair $(H_{0}^{C^{\\prime}},\\neg H_{0}^{C^{\\prime}})$ under at most $k$ contaminated sample members. ", "page_idx": 17}, {"type": "text", "text": "Moreover, it directly follows from the discussions in Part $\\mathbf{C}$ of the supplementary material to [48] that the (approximate) observed $\\mathbf{p}$ -value of this test is given by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f_{(C^{\\prime},C)}(k):=1-\\frac{1}{N}\\cdot\\displaystyle\\sum_{I\\in\\mathbb{Z}_{N}}\\mathbb{1}_{\\left\\{d_{I}^{\\delta}-d_{s}^{\\delta}(C^{\\prime},C)(\\omega_{0})>\\frac{2k}{(s-k)}\\right\\}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where again $N$ denotes the number of resamples, $\\mathcal{T}_{N}$ is the corresponding set of resamples, and $d_{I}^{\\delta}$ is the test statistic evaluated for the resample associated to $I$ . ", "page_idx": 17}, {"type": "text", "text": "A.3.3 Robustified static GSD-test ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "As already argued in the main text, it is now easy to calculate an (approximate) observed p-value for our global hypothesis pair $(H_{0},\\neg H_{0})$ : We simply calculate the maximal observed $\\mathbf{p}$ -value among all $C^{\\prime}\\in\\mathcal{C}\\setminus\\{C\\}$ , i.e. set ", "page_idx": 17}, {"type": "equation", "text": "$$\nF_{C}(k):=\\operatorname*{max}\\{f_{(C^{\\prime},C)}(k):C^{\\prime}\\in\\mathcal{C}\\setminus\\{C\\}\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The robustified test for the degree of contamination $k$ can be carried out as follows: Calculate $F_{C}(k)$ and reject $H_{0}$ if $F_{C}(k)\\leq\\alpha$ , i.e., if the maximal (approximate) $p$ -value of the pairwise tests is still lower or equal than the significance level. ", "page_idx": 18}, {"type": "text", "text": "The argument that the testing procedure just described indeed produces a valid level- $\\alpha$ test of the global hypothesis pair $(H_{0},\\neg H_{0})$ under up to $k$ contaminated data sets in the sample, can be carried out completely analogous as done in Appendix A.2.3. ", "page_idx": 18}, {"type": "text", "text": "A.3.4 Robustified dynamic GSD-test ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Finally, as discussed in the main text and reprinted here again for convenience of the reader, also the robustified dynamic GSD-test can be obtained in a straightforward manner: Under up to $k$ contaminated data sets in the sample, the (random) alternative hypothesis $\\tilde{H}_{1}^{S_{\\mathrm{max}}}:C\\in\\operatorname{gsd}(S_{\\mathrm{max}})$ from before is statistically valid with level $\\alpha$ if all individual robustified tests reject $H_{0}^{C^{\\prime}}$ at level $\\frac{\\alpha}{c}$ , i.e., if $\\begin{array}{r}{F_{C}(k)\\leq\\frac{\\alpha}{c}}\\end{array}$ . ", "page_idx": 18}, {"type": "text", "text": "The argument that the testing procedure just described indeed produces a valid level- $\\alpha$ test for the (random) hypothesis pair $(\\tilde{H}_{0}^{\\bar{S}_{\\mathrm{max}}},\\neg\\tilde{H}_{0}^{S_{\\mathrm{max}}})$ under up to $k$ contaminated data sets in the sample, can be carried out completely analogous as done in Appendix A.2.4. ", "page_idx": 18}, {"type": "text", "text": "A.3.5 Computation and regularization ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Note that also the robustified test statistic $\\overline{{d_{s}^{\\delta}}}(C^{\\prime},C)(\\omega_{0})$ can be computed by solving a linear optimization problem (see [48, Proposition 6]) and, hence, the test just described is computationally tractable. ", "page_idx": 18}, {"type": "text", "text": "Again, note that the tests in Section 5 are based on the unregularized test statistics with $\\delta=0$ . The reason for this is the same as discussed at the end of Appendix A.2. ", "page_idx": 18}, {"type": "text", "text": "B Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "B.1 Proof of Theorem 1 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof. First, note that for $C,C^{\\prime}\\in{\\mathcal{C}}$ , we have that $C\\succeq C^{\\prime}$ if and only if ", "page_idx": 18}, {"type": "equation", "text": "$$\nD(C,C^{\\prime}):=\\operatorname*{inf}_{u\\in\\mathcal{U}_{\\mathcal{P}_{\\Phi}}}\\left(\\mathbb{E}_{\\pi}(u\\circ\\Phi_{C})-\\mathbb{E}_{\\pi}(u\\circ\\Phi_{C^{\\prime}})\\right)\\geq0.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus, the GSD-front can equivalently be rewritten as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname{gsd}(\\mathcal{C})=\\left\\{C\\in\\mathcal{C}:\\vec{\\mathtt{j}}C^{\\prime}\\in\\mathcal{C}\\mathrm{~s.t.~}\\,D(C^{\\prime},C)\\geq0\\ \\right\\}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now, let $\\varepsilon:\\mathbb{N}\\rightarrow[0,1]:s\\mapsto1/\\sqrt[4]{s}$ . We show that: ", "page_idx": 18}, {"type": "equation", "text": "$$\nC\\in\\operatorname{gsd}(\\mathcal{C})\\Rightarrow C\\in\\operatorname*{lim}_{s\\to\\infty}\\operatorname{egsd}_{s}^{\\varepsilon(s)}(\\mathcal{C})\\quad\\pi\\mathrm{-a.s.\\,and}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\nC\\not\\in\\operatorname{gsd}(\\mathcal{C})\\Rightarrow C\\not\\in\\operatorname*{lim}_{s\\to\\infty}\\operatorname{egsd}_{s}^{\\varepsilon(s)}(\\mathcal{C})\\quad\\pi-\\mathbf{a}.\\mathbf{s}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note that the proof immediately translates to the more general case of $\\varepsilon(s)\\in\\Theta(1/\\sqrt[4]{s})$ as stated in Theorem 1. Denote with $\\hat{\\mathbb E}$ the expectation w.r.t. the empirical measure associated with the i.i.d. sample10 $(T_{1},\\dots,T_{s})$ . For Implication (3), assume that $C\\in\\operatorname{gsd}(\\mathcal{C})$ . Then for every other classifier $C^{\\prime}$ there exists an utility function $u\\in\\mathcal{U}_{\\mathcal{P}_{\\Phi}}$ with $\\mathbb{E}_{\\pi}(u\\circ\\Phi_{C})>\\mathbb{E}_{\\pi}(u\\circ\\Phi_{C^{\\prime}})$ (Otherwise we would have $D(C^{\\prime},C)\\ge0$ and $D(C,C^{\\prime})<0$ , where the second statement is due to antisymmetry). For these corresponding utility functions, because of the strong law of large numbers, we would get $d_{s}\\big(C^{\\prime},C\\big)\\leq\\hat{\\mathbb{E}}\\big(u\\circ\\Phi_{C^{\\prime}}\\big)-\\hat{\\mathbb{E}}\\big(u\\circ\\Phi_{C}\\big)\\stackrel{a.s.}{\\longrightarrow}c<0$ . Since $\\mathcal{C}$ consists only of finitely many classifiers, $\\mathrm{egsd}_{s}^{\\varepsilon(s)}(\\mathcal{C})$ will almost surely not contain $C$ asymptotically. Note that for Implication (3) to hold, it is on\u221aly necessary that $\\varepsilon(s)$ converges to zero as $s$ goes to infinity. The order of convergency as $\\Theta(1/{\\sqrt[4]{s}})$ is only needed for Implication (4). ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "For Implication (4) assume that $C\\not\\in\\operatorname{gsd}(\\mathcal{C})$ . Then there exists a classifier $C^{\\prime}$ with $D(C^{\\prime},C)\\ge0$ and $D\\bar{(}C,C^{\\prime})\\,<\\,0$ . An analog argumentation like above shows that $d_{s}(C,C^{\\prime})$ converges almost surely to a value smaller than zero. It remains to analyze $D(C^{\\prime},C)$ . For this, we have to show that $d_{s}(C^{\\prime},C)+\\varepsilon(s)\\ {\\overset{a.s_{\\cdot}}{\\longrightarrow}}\\ c\\geq0$ . We utilize uniform convergence: For arbitrary $\\xi>0$ , [84, p. 192 Theorem 5.1] gives us ", "page_idx": 19}, {"type": "equation", "text": "$$\nP\\left(\\operatorname*{sup}_{u\\in\\mathcal{U}_{\\mathcal{P}_{\\Phi}}}\\left\\vert\\mathbb{E}(u\\circ\\Phi_{C})-\\hat{\\mathbb{E}}(u\\circ\\Phi_{C})\\right\\vert>\\xi\\right)\\leq8\\left(\\frac{e\\cdot2s}{h}\\right)^{h}\\cdot\\exp\\left\\{-\\xi_{*}^{2}s\\right\\},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\xi_{*}\\,=\\,\\xi\\,-\\,1/s$ and $h$ is the VC dimension of $\\mathcal{T}_{\\Phi}$ . The same holds for $\\Phi_{C^{\\prime}}$ . The triangle inequality then gives ", "page_idx": 19}, {"type": "equation", "text": "$$\nP\\left(\\operatorname*{sup}_{u\\in\\mathcal{U}_{P_{\\Phi}}}\\left|\\hat{\\mathbb{E}}(u\\circ\\Phi_{C})-\\hat{\\mathbb{E}}(u\\circ\\Phi_{C^{\\prime}})\\right|>2\\xi\\right)\\leq8\\left(\\frac{e\\cdot2s}{h}\\right)^{h}\\cdot\\exp\\left\\{-\\xi_{*}^{2}s\\right\\}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For $\\varepsilon(s)=1/\\sqrt[4]{(1/s)}$ and $s$ large enough, we have $\\varepsilon_{*}(s)=\\varepsilon(s)-1/s\\geq\\varepsilon(s)/2$ and therefore ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{P\\left(\\underset{u\\in\\mathcal U_{P_{\\Phi}}}{\\operatorname*{sup}}\\left\\vert\\widehat{\\mathbb{E}}(u\\circ\\Phi_{C})-\\widehat{\\mathbb{E}}(u\\circ\\Phi_{C^{\\prime}})\\right\\vert>2\\varepsilon(s)\\right)\\leq8\\left(\\frac{e\\cdot2s}{h}\\right)^{h}\\cdot\\exp\\left\\{-\\varepsilon_{*}(s)^{2}s\\right\\}}\\\\ &{}&{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq8\\left(\\frac{e\\cdot2s}{h}\\right)^{h}\\exp\\left\\{-\\varepsilon(s)^{2}s/4\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This implies ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P\\left(\\underset{u\\in\\mathcal{U}_{P_{\\Phi}}}{\\operatorname*{sup}}\\left|\\widehat{\\mathbb{E}}(u\\circ\\Phi_{C})-\\widehat{\\mathbb{E}}(u\\circ\\Phi_{C^{\\prime}})\\right|>\\varepsilon(s)/2\\right)\\leq8\\left(\\frac{e\\cdot2s}{h}\\right)^{h}\\exp\\left\\{-\\varepsilon(s)^{2}s/64\\right\\}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=8\\left(\\frac{e\\cdot2s}{h}\\right)^{h}\\exp\\left\\{-\\sqrt{s}/64\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "If the VC dimens\u221aion $h$ is finite, the term $\\begin{array}{r}{8\\left(\\frac{e\\cdot2s}{h}\\right)^{h}}\\end{array}$ is po\u221alynomially growing in $\\sqrt{s}$ (or $s$ ), whereas the term exp $\\{-\\sqrt{s}/64\\}$ is exponentially decreasing in $\\sqrt{s}$ (or $s$ ). Therefore, the right hand side of Inequality (5) converges to zero, which shows that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{u\\in\\mathcal{U}_{\\mathcal{P}_{\\Phi}}}\\left|\\mathbb{\\hat{E}}(u\\circ\\Phi_{C})-\\mathbb{\\hat{E}}(u\\circ\\Phi_{C^{\\prime}})\\right|-\\varepsilon(s)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "converges in probability to a value $c\\leq0$ or equivalently, that $d_{s}(C^{\\prime},C)+\\varepsilon(s)$ converges to a value $c\\geq0$ . Since the right hand side of Inequality (5) converges exponentially in $s$ , the Borel-Cantelli theorem (cf., e.g., [29, p.67ff]) gives also strong co\u221anvergency, which completes the proof. Note that it is not necessar\u221ay to specify $\\varepsilon(s)$ concretely as $\\ln/\\sqrt[4]{s}$ . It would be sufficient to define $\\varepsilon(s)$ as of the order of $\\Theta(1/{\\sqrt[{4}]{s}})$ . \u25a1 ", "page_idx": 19}, {"type": "text", "text": "B.2 Proof of Theorem 2 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proof. i) Assume that $C\\not\\in\\mathsf{p a r}(\\Phi)$ . Then, by definition of $\\operatorname{par}(\\Phi)$ , there exists $C^{\\prime}\\in{\\mathcal{C}}$ such that for all $D\\,\\in\\,{\\mathcal{D}}$ it holds that $\\bar{\\Phi}(\\dot{C}^{\\prime},\\stackrel{\\cdot}{D})\\gg\\Phi(\\dot{C},D)$ . This implies that for all $D\\,\\in\\,{\\mathcal{D}}$ it holds that $(\\Phi(C^{\\prime},D),\\Phi(C,D))\\in P_{R_{1}^{*}}$ . Now, choose $u\\in\\mathcal{U}_{\\mathcal{P}_{\\Phi}}$ . Since $u$ then, by definition, is strictly isotone with respect to $P_{R_{1}^{*}}$ , this allows us to conclude that the function $u(\\Phi(C^{\\prime},\\cdot))-u(\\Phi(C,\\cdot))$ is strictly positive, i.e., we have $u(\\Phi(C^{\\prime},D))-u(\\Phi(C,D))>0$ for arbitrary $D\\in\\mathcal{D}$ . ", "page_idx": 19}, {"type": "text", "text": "We compute: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\mathbb{E}_{\\boldsymbol\\pi}\\big(\\boldsymbol{u}\\circ\\Phi_{C^{\\prime}}\\big)-\\mathbb{E}_{\\boldsymbol\\pi}\\big(\\boldsymbol{u}\\circ\\Phi_{C}\\big)}&{=}&{\\displaystyle\\int_{\\Omega}\\boldsymbol{u}\\big(\\Phi(C^{\\prime},T(\\boldsymbol\\omega))\\big)d\\boldsymbol{\\pi}(\\boldsymbol\\omega)-\\int_{\\Omega}\\boldsymbol{u}\\big(\\Phi(C,T(\\boldsymbol\\omega))\\big)d\\boldsymbol{\\pi}(\\boldsymbol\\omega)}\\\\ &{=}&{\\displaystyle\\int_{\\Omega}\\underbrace{\\boldsymbol{u}\\big(\\Phi(C^{\\prime},T(\\boldsymbol\\omega))\\big)-\\boldsymbol{u}\\big(\\Phi(C,T(\\boldsymbol\\omega))\\big)}_{>0\\mathrm{\\~for~all~}\\omega\\in\\Omega,\\mathrm{~sinc~}T(\\boldsymbol\\omega)\\in\\mathcal{D}}d\\boldsymbol\\pi(\\boldsymbol\\omega)>0}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This gives $\\mathbb{E}_{\\pi}\\big(u\\circ\\Phi_{C^{\\prime}}\\big)>\\mathbb{E}_{\\pi}\\big(u\\circ\\Phi_{C}\\big)$ . As $u$ was chosen arbitrarily, this implies that $C^{\\prime}\\succ C$ . Hence, by definition of the GSD-front, we have $C\\not\\in\\operatorname{gsd}(\\mathcal{C})$ . ", "page_idx": 20}, {"type": "text", "text": "ii) First, note that both postulates are statements involving random sets (i.e., sets dependent on the realizations of the variables $T_{1},\\allowbreak\\cdot\\cdot,T_{s})$ . Thus, we have to prove both statements for arbitrary realizations of these variables. So let $D_{1}\\;:=\\;T_{1}(\\omega_{0}),\\ldots,D_{s}^{\\overline{{\\mathbf{\\alpha}}}}\\;:=\\;T_{s}(\\omega_{0})\\;\\in\\;\\mathcal{D}$ be an arbitrary realisation. For this concrete realization of the sample, the first statement is immediate, since if there is no $C^{\\prime}$ such that $d_{s}(C^{\\prime},C)(\\omega_{0})\\geq-\\varepsilon_{2}$ there is also no $C^{\\prime}$ such that $d_{s}(C^{\\prime},C)(\\omega_{0})\\geq-\\varepsilon_{1}$ (as the latter is harder to satisfy due to $\\varepsilon_{1}\\leq\\varepsilon_{2}$ ). ", "page_idx": 20}, {"type": "text", "text": "Again for the chosen concrete realization of the sample, the second postulate is an immediate consequence of statement i) from above. As in both situations the realization of the variables was chosen arbitrarily, this implies the statement. \u25a1 ", "page_idx": 20}, {"type": "text", "text": "B.3 Proof of Theorem 3 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "To see that the static test is a valid level- $\\alpha$ test for the global hypothesis pair $(H_{0},\\neg H_{0})$ , observe that \u2013 assuming $H_{0}$ to be true \u2013 the probability of $H_{0}$ being rejected equals the probability of all hypothesis $H_{0}^{\\bar{C^{\\prime}}}$ being rejected simultaneously. The latter probability \u2013 still assuming $H_{0}$ to be true \u2013 is obviously bounded from above by the probability that one specific hypothesis $H_{0}^{C^{*}}$ is rejected, which itself is bounded from above by the significance level $\\alpha$ by construction. ", "page_idx": 20}, {"type": "text", "text": "Furthermore, the reason for the consistency of the static test is the following: First, note that under the assumption of Theorem 1 (because of the finite VC dimension), we have that $d_{s}(C^{\\prime},C)$ converges to $D(C^{\\prime},C)$ in probability for every abrbitrary classifier $C^{\\prime}\\neq C$ . Therefore, for fixed $C^{\\prime}$ and under the null hypothesis $H_{0}^{C^{\\prime}}$ , we have $d_{s}(C^{\\prime},C)$ converges in probability to a value larger than or equal to zero. This implies that under this null hypothesis the implicit critical values of the permutation test become arbirarily close to a values larger than or equal to zero. ", "page_idx": 20}, {"type": "text", "text": "Now, let $C$ be in the GSD-front. Then, due to antisymmetry of $\\succsim$ , for every other classifier $C^{\\prime}$ , there exists a utility for which the expectation of $u\\circ\\Phi_{C}$ is larger than the expectation of $u\\circ\\Phi_{C^{\\prime}}$ . Because of the weak law of large numbers, this translates to the empirical expectations with an arbitrarily high probability if only the sample size is large enough. Therefore, all-together, as $s$ converges to infinity, the test rejects the null hypothesis in this situation with arbitrary high probability. Finally, since we have only a finite number of hypothesis of the static test, this also translates to the static test itself. Therefore the static test is indeed a consistent level- $\\alpha$ test. ", "page_idx": 20}, {"type": "text", "text": "To see that also the dynamic test is a valid level- $\\alpha$ test for the (random) hypothesis pair $(\\tilde{H}_{0}^{S_{\\mathrm{max}}},\\neg\\tilde{H}_{0}^{S_{\\mathrm{max}}})$ , observe that \u2013 under the null hypothesis \u2013 the probability of $C$ lying in the GSDfront of some random subset $\\boldsymbol{S}$ of $\\mathcal{C}$ is bounded from above by the sum of probabilities of $C$ lying in the GSD-front of $\\{C,S\\}$ , where summation is over all $S\\in S$ . As each of these probabilities is bounded from above by $\\frac{\\alpha}{c}$ by construction, the corresponding sum is bounded from above by $|S|\\cdot{\\frac{\\alpha}{c}}$ Finally, as $\\vert S\\vert\\le c$ , this gives the desired upper bound of $\\alpha$ . ", "page_idx": 20}, {"type": "text", "text": "Finally, also the consistency of the dynamic test follows from the fact that it is constructed from a finite set of consistent tests for every possible set ${\\mathcal{S}}\\subseteq{\\mathcal{C}}$ . \u25a1 ", "page_idx": 20}, {"type": "text", "text": "B.4 Proof of Corollary 1 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Assume that $\\Phi({\\mathcal{C}}\\times{\\mathcal{D}})\\subseteq M\\times S_{1}\\times S_{2}$ , where $S_{1},S_{2}\\subset[0,1]$ are finite, and $M\\subseteq[0,1]$ is arbitrary. This is possible since by definition of $\\Phi$ we have $M\\,\\subseteq\\,\\phi_{1}\\!({\\mathcal{C}}\\times{\\mathcal{D}})$ and $S_{1}\\subseteq\\,\\bar{\\phi}_{2}(\\bar{\\mathcal{C}}\\times\\mathcal{D})$ and $S_{2}\\subseteq\\,\\bar{\\phi_{3}}(\\mathcal{C}\\times\\mathcal{D})$ , and the metrics $\\phi_{2}$ and $\\phi_{3}$ are assumed to be finitely discrete. We show that the width11 of the restriction of $R_{1}^{*}$ to $\\Phi({\\mathcal{C}}\\times{\\mathcal{D}})$ is finite. It then follows directly from e.g. [76, Proposition 2] that the VC-dimension of ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{Z}_{\\Phi}:=\\left\\{\\{a:u(a)\\geq c\\}:c\\in[0,1]\\land u\\in\\mathcal{U}_{\\mathcal{P}_{\\Phi}}\\right\\}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "is also finite. The claim then follows from Theorem 1 ", "page_idx": 20}, {"type": "text", "text": "To show the finiteness of the width, assume - wlog - that $|S_{1}|\\,=\\,g\\,<\\,\\infty$ and $|S_{2}|\\,=\\,h\\,<\\,\\infty$ . Assume, for contradiction, that there exists an antichain12 $Q\\subseteq\\Phi(\\mathcal{C}\\times\\mathcal{D})$ within the restriction of $R_{1}^{*}$ to $\\Phi({\\mathcal{C}}\\times{\\mathcal{D}})$ of cardinality strictly greater than $g\\cdot h$ . Then there exist $x=(x_{1},x_{2},x_{3}),y=$ $(y_{1},y_{2},y_{3})\\in Q$ such that $x_{2}=y_{2}$ and $x_{3}=y_{3}$ (as there are only $g\\cdot h$ different combinations of the second and the third component). However, since the first component is completely ordered by $\\geq$ , this implies either $(x,y)$ or $(y,x)$ (or both) is contained in the restriction of $R_{1}^{*}$ to $\\Phi({\\mathcal{C}}\\times{\\mathcal{D}})$ . This is a contradiction to $x$ and $y$ being elements of the same antichain $Q$ , completing the argument. \u25a1 ", "page_idx": 21}, {"type": "text", "text": "C Further results on the applications ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "This section provides further information on the benchmarking examples in Section 5. ", "page_idx": 21}, {"type": "text", "text": "C.1 Experiments with OpenML ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "This sections gives further insight to the example on the OpenML data analysed in Section 5.1. We start with giving more details on the data set with all the computation settings of the classifier algorithms. Afterwards, we provide more graphics and explanations of the analysis. ", "page_idx": 21}, {"type": "text", "text": "C.1.1 Data ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Overall, we are comparing the performance of Support Vector Machine (SVM) to further 6 classifier algorithms on 80 data sets. The data sets as well as the performance evaluation is given by the OpenML library [82].13 The analysis is restricted to binary classification problems. We selected those data sets of OpenML that evaluated the predictive accuracy, train data time computation and test data time computation (both measured in milliseconds) for all of the 7 algorithms. Since the computation times depend on the environment, i.e. the number of cores used or the free memory, we discretized the computation times and considered them as ordinal. Therefore, we divided each computation time into ten categories, where category one contains the $10\\%$ highest times, and so on. Moreover, we restricted our analysis on data sets with more than 450 and less than 10000 observations. This gives us in total 80 data sets. ", "page_idx": 21}, {"type": "text", "text": "The algorithms discussed are: ", "page_idx": 21}, {"type": "text", "text": "\u2022 Support Vector Machine (SVM) algorithm is implemented in the e1071 library [26] \u2022 Random Forests (RF) algorithm is implemented in the ranger library [88], \u2022 Decision Tree (CART) algorithm is implemented via the rpart library [79], \u2022 Logistic regression (LR) algorithm is implemented via the nnet library [68], \u2022 eXtreme Gradient Boosting (xGBoost) algorithm is implemented in the xgboost library [20], \u2022 Elastic net (GLMNet) algorithm is implemented through the glmnet library [32], and \u2022 $k$ -nearest neighbors (kNN) algorithm is implemebted via the kknn library [38]. ", "page_idx": 21}, {"type": "text", "text": "C.1.2 Detailed results of the GSD-based analysis ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We started our analysis in Section 5.1 by computing the empirical GSD-front. This gives the Hasse graph 2, where a top-down edge from $C$ to $C^{\\prime}$ states that $d_{80}(C,C^{\\prime})\\geq0$ holds. ", "page_idx": 21}, {"type": "text", "text": "In addition to the left of Figures 1 (densities of resampled test statistics) and the right of Figure 1 (effect of contamination on ${\\bf p}$ -values) in the main paper, we include the cumulative distribution functions (CDFs) in Figure 3. Since we do not include the values of the observed test statistics here, the differences in distributions are visible to a greater extent. We observe the resampled test statistics distributions for SVM vs. xGBoost and GLMNet to be left-shifted compared to SVM vs. CART, xGBoost, and LR. A visual analysis of the test decision, however, is not possible in the absence of the observed test statistics. This is why we include their values in the caption of Figure 3. ", "page_idx": 21}, {"type": "image", "img_path": "jXxvSkb9HD/tmp/9a597c07846f365226baa3888d2e52b87a6c0b9a86eb256cc955b3a4f9825c27.jpg", "img_caption": ["Figure 2: The blue shaded region symbolizes the 0-empirical GSD-front for the OpenML data sets. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "jXxvSkb9HD/tmp/7a866fb551dc310ffd6fde2f42a8b025c3e2c1c16a63507d311d8aebcd696461.jpg", "img_caption": ["Figure 3: Cumulative Distribution Functions (CDFs) of resampled test statistics for hypothesis tests of SVM vs. LR, RF, kNN, GLMNet, xGBoost, and CART, respectively, on OpenML\u2019s benchmarking suite. As opposed to Figure 1 in the main paper, values of observed test statistics are not included. They are: 0.0125 (CART), $-0.3875$ (kNN), $-0.4375$ (xGBoost), $-0.41875$ (RF), $-0.3375$ (GLMNet), and \u22120.04897227 (LR). It becomes evident that the resampled test statistics\u2019 distributions for SVM vs. xGBoost and GLMNet are left-shifted compared to SVM vs. CART, xGBoost, and LR. This is also visible in Figure 1 in the main paper, albeit less clearly. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "C.1.3 Detailed results of state-of-the-art analyses and comparison to GSD-front ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "This section provides the detailed computation of the state-of-the-art approaches and the comparison with the GSD approach. Here, we go step by step through all the methods touched in Section C.1. ", "page_idx": 22}, {"type": "text", "text": "First-order stochastic dominance Analogously to the GSD-front, one can define the front based on (multivariate) first-order stochastic dominance (see, e.g., [77]). Note that classical first-order stochastic dominance is a special case of our generalized stochastic dominance (GSD) in the case that all quality metrics are (treated as) of ordinal scale of measurement. Given the test logic followed by, for example, [6], for the OpenML data it turns out that no classifier is significantly stochastically larger than (or incomparable to) any other classifier (based on a significance level of $5\\;\\%$ ). Compared to the results we obtain from our GSD-front analysis, this indiscriminative result is much less informative. ", "page_idx": 22}, {"type": "text", "text": "Pareto-front Both for the PMLB benchmark suite and the OpenML setup the Pareto-front contains all considered classifiers and is therefore not very informative. This shows the advantage of our approach because our approach is generally more informative (see Theorem 2). In particular, by using generalized stochastic dominance one refrains from solely relying on pointwise comparisons of classifiers over datasets. Instead one only looks at the distribution of the multidimensional quality metric. Beyond this, compared to both a Pareto analysis and a classical first order stochastic dominance analysis (see above), the GSD approach does justice to the fact that the dimension accuracy is cardinal and, at the same time, the fact that the other dimensions are of ordinal scale of measurement. Additionally, compared to an approach that only looks at the marginal distributions of every single quality metric separately, the GSD approach takes also the dependence structure between the different quality metrics into account. This is of particular interest if one has different performance dimensions that are anticorrelated. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "Weighted sum approach The GSD-based approach has advantages over an approach of weighted summation of the various quality metrics especially when it is not clear how specifically the weights are to be chosen. Specifically, each weighting leads to a total ordering among the classifiers under consideration. A clear best classifier can, therefore, be identified for each weighting. Thus, different weightings generally lead to different best classifiers. As a consequence, if one chooses a specific weighting, one should really be convinced that domain knowledge thoroughly justifies it, as even small changes in the weighting can completely change the resulting ranking. In contrast, the GSD-based approach can be used if no weighting of the involved metrics is available, but still more information (e.g., from the cardinal metrics) is available than required for a Pareto-type analysis. In summary, we emphasize that our method and the weighted summation should be used under different conditions and, therefore, complement rather than compete with each other. ", "page_idx": 23}, {"type": "text", "text": "Marginal-front A highly popular testing scheme for benchmark analysis is the one proposed by [24]. We compare our approach against using a marginal front that directly results from following this scheme. This marginal front is defined as a function of (a) statistical test(s), i.e. classifiers are in it depending on the test results. We emphasize that this front is not directly comparable to the GSD-front, since the GSD-front is a theoretical object (like the Pareto front) that can be used to formulate hypotheses that can then be tested by statistical tests like the ones proposed in this paper. Thus, we compare the marginal front to the empirical GSD front as reported in Fig. 1 of the paper for the application on OpenML. ", "page_idx": 23}, {"type": "text", "text": "We run multiple single-objective evaluations and include in the marginal-front the classifiers that are not statistically significantly worse than another classifier on all metrics. For the single-objective tests, we follow the well-established procedure of [24]. That is, we first run a global Friedman test (see [33]) for the null hypothesis that all classifiers have no differences with respect to the quality metric under investigation. In case we reject this null hypothesis, we can run post hoc Nemenyi pairwise tests (see [63]), comparing the performance of algorithms pairwise, with the null hypothesis being that there is no difference between their performances w.r.t. the multidimensional quality metric considered. We would like to emphasize that such an approach does not take into account the dependence structure among the quality metrics. In other words, it only considers the marginal distribution (hence the term marginal-front) of the classifiers w.r.t. the individual quality metrics separately, not their joint distribution. In the following, we conduct the suggested marginal analysis for OpenML w.r.t. the three-dimensional quality metric considered (accuracy, computation time on training data, computation time on test data): ", "page_idx": 23}, {"type": "text", "text": "Accuracy ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "\u2022 Global Friedman Test: Friedman rank sum test [33] rejects global null of no differences (p-value $=3.986\\mathrm{e}{-14};$ ). This means we can conduct (two-sided) pairwise post hoc tests $\\alpha=0.05)$ with no difference as the null hypothesis.   \n\u2022 Post Hoc Nemenyi Test: Table 1 below shows the pairwise comparisons of algorithm performance with the Nemenyi test [63]. P-values below 0.05 are highlighted and indicate statistically significant differences in performance. ", "page_idx": 23}, {"type": "table", "img_path": "jXxvSkb9HD/tmp/3d63e3579b80b92bd6a7a68672990eb971022c790b756a86995ffb20633f3bf9.jpg", "table_caption": ["Table 1: Pairwise comparisons of algorithm performance with the Nemenyi test based on accuracy. Underlined values indicate differences significant at $\\alpha=0.05$ level. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Computation time on training data ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 Global Friedman Test: Friedman rank sum test [33] rejects the global null hypothesis of no differences (p-value $<2.2\\mathrm{e}.16$ ). This means we can conduct pairwise post hoc tests $\\alpha=0.05)$ with the null hypothesis of no difference.   \n\u2022 Post Hoc Nemenyi Test [63], see Table 2. ", "page_idx": 24}, {"type": "text", "text": "Table 2: Pairwise comparisons of algorithm performance with the Nemenyi test based on computation time on the training data. Underlined values indicate differences significant at $\\alpha=0.05$ level. ", "page_idx": 24}, {"type": "table", "img_path": "jXxvSkb9HD/tmp/6dbd6610862833b1e6be50627fb235c7afd1c49adb45bd8d5d7f242f6e4159f5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "Computation time on test data ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 Global Friedman Test Friedman rank sum test [33] rejects the global null hypothesis of no differences ( ${\\bf p}$ -value $<2.2\\mathrm{e}{-16})$ ). This means we can conduct pairwise post hoc tests $\\alpha=0.05)$ with the null hypothesis of no difference.   \n\u2022 Post Hoc Nemenyi Test [63], see Table 3. ", "page_idx": 24}, {"type": "text", "text": "Table 3: Pairwise comparisons of algorithm performance with the Nemenyi test based on computing time on testing data. Underlined values indicate differences significant at $\\alpha=0.05$ level. ", "page_idx": 24}, {"type": "table", "img_path": "jXxvSkb9HD/tmp/e9660d42c53a2b8ce267a5533faffedcf5ffe19c59b9df285abb9b823ab8aed5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "Table 4 provides the mean results of the classifier comparisons. (Recall that for train/test time: the lower, the better) ", "page_idx": 24}, {"type": "table", "img_path": "jXxvSkb9HD/tmp/c964ec2398b6504ce8acae611e3fc72cac5a1f12022655d5a78896a760164988.jpg", "table_caption": ["Table 4: Mean results of the classifier comparisons. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "As becomes evident from the mean values of the three quality metrics and the single-criterion test results presented above, there is no classifier that is significantly dominated by another classifer w.r.t. all three quality metrics. Hence, the marginal-front would contain all classifiers and would be rather indiscriminative compared to the empirical GSD-front that we present in the paper, see Figure 2 Appendix C.1, which contains random forest (RF), trees (CART), and $\\mathbf{k}$ -nearest neighbor (kNN). This is in line with our explanation of OpenML results above. Since the quality metrics accuracy, train time, and test time are only weakly (if at all) correlated due to a trade-off between speed and accuracy, the marginal-front based on single-criterion comparisons does not facilitate practitioners\u2019 decision-making, while our empirical GSD-front provides valuable insights. ", "page_idx": 24}, {"type": "text", "text": "For the sake of completeness, we also report the results of these multiple single-objective evaluations on the PMLB benchmark suite in tables 5, 6, 7, and 8. The interpretation is completely analogous to the interpretation of the results on OpenML above. Note that the Friedman rank sum test rejects global null of no differences for all three criteria. This means we can conduct pairwise post hoc tests $\\alpha=0.05)$ ) with (two-sided) null of no difference. ", "page_idx": 24}, {"type": "table", "img_path": "jXxvSkb9HD/tmp/9cfbbdbf9a94711bd8dddd494bbaa8b0f3b156b6c176ac55556a87c87db95435.jpg", "table_caption": ["Table 5: Post Hoc Nemenyi Test (Accuracy) on PMLB. "], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "jXxvSkb9HD/tmp/3f125d7df8e4bf31596013bdb557955a294581004337003cc22d63ed9ee3f57e.jpg", "table_caption": ["Table 6: Post Hoc Nemenyi Test Summary (Accuracy with Noisy X) on PMLB. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "C.1.4 Discussion of the unexpected results ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Recall the discussion in Section 5.1 about the unexpected results. We want to emphasize that these have a high degree of originality and should be of particular interest to practitioners. This shows that experience and intuition with a method can also be misleading if only the evaluation framework is slightly modified: A multidimensional quality metric that seeks the optimal trade-off between different, potentially conflicting metrics will generally rank differently than a unidimensional one. In the following, we show that the dominance of CART over xGBoost, SVM, LR, and GLMNet is indeed consistent with the quality metrics provided by the OpenML repository. ", "page_idx": 25}, {"type": "text", "text": "First of all, here, we are interested in the trade-off between accuracy and computation time, (e.g., the better the accuracy, the higher/worse the computation time). We now look at the comparison between SVM and CART to demonstrate that the results are indeed in line with the data. We obtain: ", "page_idx": 25}, {"type": "text", "text": "\u2022 For 27 datasets, CART outperforms SVM on all dimensions (e.g., prediction accuracy, computation time on test data, and computation time on training data) at once.   \n\u2022 For 9 datasets, CART dominates SVM for at least one quality metric and for all other quality metrics the performance of CART is not worse.   \n\u2022 For 41 datasets, SVM\u2019s prediction accuracy is better than CART\u2019s. At the same time, CART outperforms SVM for at least one of the two computation times. The two classifiers are therefore incomparable for these datasets.   \n\u2022 For 3 datasets CART outperforms SVM based on accuracy, but at least one of the computation times of SVM is below the one of CART. ", "page_idx": 25}, {"type": "text", "text": "Overall, there exists no dataset where SVM dominates CART in all dimensions at once. Either the two classifiers are incomparable, or CART dominates SVM. Furthermore, CART dominates SVM for nearly half of the datasets $27+9=36$ of 80). Thus, the dominance structure provided by our method is in line with the performance evaluation values provided by OpenML. ", "page_idx": 25}, {"type": "table", "img_path": "jXxvSkb9HD/tmp/ec5049eb0c965e1be51592825412c9801afc9aaf53e2b136047f4b8281fefdc4.jpg", "table_caption": ["Table 7: Post Hoc Nemenyi Test Summary (Accuracy with Noisy Y) on PMLB. "], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "jXxvSkb9HD/tmp/9a3ee1c56bde92e9de8d439d02c8ca979adce64488fca8a6b144725486c65656.jpg", "table_caption": ["Table 8: Mean Results (Accuracy and Noisy Data) on PMLB "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "A second issue that may have influenced the unexpected performance structure obtained in the paper is the way performance is evaluated by OpenML. OpenML is based on the uploads of its users. Each user is free to decide which hyperparameter settings to use. Thus, as there might be a different goal on the hyperparameter setting in each dataset, the results are not representative for the best performance of each algorithm. This aspect should be included in any further discussion. Especially since some algorithms are more dependent on hyperparameter settings/tuning than others. For an example involving hyperparameter tuning that is fixed in advance, see Section 5.2. ", "page_idx": 26}, {"type": "text", "text": "C.2 Experiments with PMLB ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "This sections give further insight to the exemplary benchmarking analysis on the Penn Machine Learning Benchmarks (PMLB) in Section 5.2 in the main paper. We start by giving more details on the data sets with all the computation settings of the classifier algorithms. Afterwards, we provide more figures and explanations of the analysis. ", "page_idx": 26}, {"type": "text", "text": "C.2.1 Data ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Penn Machine Learning Benchmarks (PMLB) is a collection of curated benchmark datasets for evaluating and comparing supervised machine learning algorithms [64]. We select all datasets from PMLB for binary classification tasks with 40 to 1000 observations14 and less than 100 features. On these 62 datasets15, a recently proposed classifier based on compressed rule ensemble learning [62] is compared w.r.t. robust accuracy against five well-established classifiers, namely classification tree (CART), random forest (RF), support vector machine with radial kernel (SVM), $\\mathrm{k\\cdot}$ -nearest neighbour (kNN), and generalized linear model with elastic net (GLMNet). In detail, we deploy ", "page_idx": 26}, {"type": "text", "text": "\u2022 Support Vector Machine (SVM) algorithm as implemented in the e1071 library [26]   \n\u2022 Random Forests (RF) algorithm as implemented in the ranger library [88], requiring e1071 library [26] and dplyr [87]   \n\u2022 Decision Tree (CART) algorithm (C4.5-like trees) as implemented in the RWeka library [39],   \n\u2022 Elastic net (GLMNet) algorithm is implemented through the glmnet library [32] requiring library Matrix [7], and   \n\u2022 $k$ -nearest neighbors (kNN) algorithm as implemented in the kknn library [38]. ", "page_idx": 26}, {"type": "text", "text": "Note that we used the respective methods in the caret library [49] for hyperparameter tuning and cross-validation to retrieve i) through iii), as detailed below. ", "page_idx": 26}, {"type": "text", "text": "We operationalize the latent quality criterion of robust accuracy through i) classical accuracy (metric), ii) robustness w.r.t. noisy features (ordinal), and iii) robustness w.r.t. noisy classes (ordinal). Computation of i) is straightforward; in order to retrieve ii) and iii), we follow [92, 93] by randomly perturbing a share (here: $20\\;\\%$ ) of the data points. We randomly selected data points with a selection probability of $20\\%$ and replaced the values by a random draw from the marginal distribution of the corresponding variable. (This is a slight difference to [92, 93] who replaced the data points by a random draw from a uniform distribution of the corresponding support of the marginal distribution.) ", "page_idx": 26}, {"type": "text", "text": "We then tune the six classifiers\u2019 hyperparameters on a (multivariate) grid of size 10 following [49] for each of the 62 datasets and eventually compute i) to iii) through 10-fold cross validation. ", "page_idx": 26}, {"type": "image", "img_path": "jXxvSkb9HD/tmp/c5513f6fc5c69744203c315fad4dd7dc55b57e503baaf69f89379ad5d88ef88c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 4: Hasse graph of the empirical GSD-relation for the PMLB data sets. The blue shaded region symbolizes the 0-empirical GSD-front, see Definition 6 ii). ", "page_idx": 27}, {"type": "text", "text": "C.2.2 Detailed results of the GSD-based analysis ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "To initially obtain a purely descriptive overview, we construct the Hasse graph illustrating the empirical GSD relation. In this process, we calculate the value $d_{62}(C,C^{\\prime})$ for $C\\neq C^{\\prime}\\in\\mathcal{C}:=$ $\\{{\\bf C}{\\bf R}{\\bf E}$ , SVM, RF, CART, GLMNet, $\\mathrm{kNN}\\}$ and connect $C$ to $C^{\\prime}$ with a top-down edge whenever $\\bar{d}_{62}(C,C^{\\prime})\\geq0$ . The resulting graph is portrayed in Figure 4. It is evident from the graph that RF (strictly) empirically GSD-dominates the classifier CRE. All other classifiers are pairwise incomparable. Five classifiers, namely RF, CART, kNN, GLMNet, and SVM are not strictly empirically GSD-dominated by any other considered classifier and, thus, form the 0-empirical GSD-front. ", "page_idx": 27}, {"type": "text", "text": "This latter purely descriptive analysis already hints at the CRE not belonging to the GSD-Front. In order to transition to inferential statements, we aim to statistically test (at level $\\alpha=0.05)$ ) whether CRE significantly lies in the GSD-front of some subset of $\\mathcal{C}$ . As detailed in Section 4.1, we conduct five pairwise tests for the hypothesis pairs $(H_{0}^{C^{\\prime}},\\neg H_{0}^{C^{\\prime}})$ (where $C:=\\mathrm{CRE}$ and $C^{\\prime}\\in\\mathcal{C}\\setminus\\{\\mathrm{CRE}\\})$ at a level of $\\frac{\\alpha}{5}$ , as explained in Section 4.16 In other words, we test five auxiliary null hypotheses, each asserting that CRE is GSD-dominated by SVM, RF, CART, GLMNet, and kNN, respectively. ", "page_idx": 27}, {"type": "text", "text": "The results of these tests are visualized in Figure 5 (densities) and Figure 6 (cumulative distribution functions).17 They indicate that the pairwise tests of CRE versus SVM, RF, CART, GLMNet, and kNN do not reject at a level of $\\frac{\\alpha}{5}$ nor at level $\\alpha$ . Hence, we conclude that based on the observed benchmark results we cannot conclude at significance level $\\alpha=0.05$ that CRE lies in the GSD-front of any subset of $\\mathcal{C}$ . In other words, we have no evidence to rule out that CRE is in the GSD-front, i.e., we cannot confirm based on the data that CRE is not outperformed by SVM, RF, CART, GLMNet, and kNN with respect to all compatible utility representation of robust accuracy. As can be seen in Figure 5, testing CRE vs. CART results in the smallest p-value of all pairwise tests, which appears plausible, since CRE is a CART-based method. On the other hand, the observed test statistic of CRE vs. RF is far away from the critical value and the test cleary does not reject, even though RF is also a tree-based method. ", "page_idx": 27}, {"type": "text", "text": "Finally, as discussed in Section 4.2, we further analyze the robustness of this test decision under contamination of the benchmark suite, i.e., deviations from the i.i.d.-assumption. As opposed to our OpenML analysis in Section 5.1, see also Appendix C.1, contamination does not affect the test decisions here, since none of the tests rejects already for uncontaminated samples. Increasing contamination only drives $p$ -values further. The results are visualized in Figure 7. It is observed that the tests are neither significant at a level of 0.505 nor at 0.05 and this clearly does not change with growing size of contaminated benchmark data sets. ", "page_idx": 27}, {"type": "text", "text": "In summary, the PMLB experiments demonstrated how to apply our benchmarking framework to the problem of comparing a newly proposed classifier to a set of state-of-the-art ones. Furthermore, it illustrated our tests\u2019 applications to multiple criteria of mixed scales (ordinal and cardinal) that operationalize a latent performance measure, namely robust accuracy. It became evident that our framework allows to statistically assess whether the novel classifier CRE can compete with existing ones - that is, whether CRE lies in the GSD-front of some state-of-the-art classification algorithms. In this case, the test decisions of both static and dynamic GSD-tests was not to reject the null hypothesis of CRE being outperformed by RF, CART, SVM, GLMNet, and kNN w.r.t. to robust accuracy. ", "page_idx": 27}, {"type": "image", "img_path": "jXxvSkb9HD/tmp/b37f82feaaf3c3026de1b2d94c830227d7944dec5ab4215fe1eaf30d5ee6879c.jpg", "img_caption": ["Figure 5: Densities of resampled test statistics for pairwise tests of CRE vs. six other classifiers on 62 datasets from PMLB. Big (small) vertical lines depict observed (resampled) test statistics. Rejection regions for the static (dynamic) GSD-test are highlighted red (dark red). As becomes evident, we cannot reject any of the pairwise tests for neither significance level. "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "jXxvSkb9HD/tmp/4bca246109c3a7a2f332725d1324ae114f9532f2fe0002db8c4bd1dfbf9abee4.jpg", "img_caption": ["Figure 6: Cumulative Distribution Functions (CDFs) of resampled test statistics for hypothesis tests on PMLB benchmark suite of CRE vs. SVM, GLMNet, RF, kNN, and CART, respectively. As opposed to Figure 5 above, values of observed test statistics are not included. They are: $-0.1031746$ (CART), \u22120.08730159 (kNN), 0.02380952 (RF), \u22120.05555556 (GLMNet), \u22120.07936508 (SVM). It becomes evident that the resampled test statistics\u2019 distributions are more similar to each other than in the case of testing SVM vs. competitors in the OpenML benchmark suite. "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "jXxvSkb9HD/tmp/688eda6e9df4c603c31e3ef7a9f448fa66620463b45eba46c19c6930d1db80fb.jpg", "img_caption": ["Figure 7: Effect of Contamination: $p$ -values for pairwise tests of CRE versus the five competitors ilen vPelMs LofB $\\alpha~=~0.05$ k( sduairtke  raepdp:l $\\begin{array}{r l r}{\\alpha}&{{}=}&{\\frac{0.05}{6},}\\end{array}$ a.l oSginocues  tnoo nFei goufr et h5e,  tdeostttse rde jreecdt  lfionre $\\alpha=0.05$ nuinfdicear nncoe contamination, this obviously does not change with contaminated samples. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Abstract and introduction of the paper reflect the paper\u2019s contribution and scope, covering both theoretical and experimental results. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 30}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: The limitations of the work are discussed in Sections 4.2 and 6 of the main paper. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 30}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The main theoretical results of the paper are stated in Theorems 1, 2 and 3 as well as Corollary 1. For each of these theorems, full sets of assumptions are provided. Moreover, complete proofs for all these statements are provided in the paper\u2019s appendix. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 31}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper fully discloses all information needed to reproduce the experimental results in Section 5 of the main paper and in Section C of the paper\u2019s appendix. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: All data and code needed to reproduce the paper\u2019s experiments are openly accessible via a GitHub repository. A link to this repository is provided in Footnote 3 of the main paper. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 32}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: All necessary background information to understand the papers experimental results can be found in Section 5 of the main paper and Section C of the paper\u2019s appendix. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 32}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: All experimental results of the paper are statistically tested for significance. Moreover, the test decisions are checked with respect to their robustness towards the assumptions underlying the respective tests. Compare Section 4 for theoretical considerations on testing and Sections 5 and Appendix C for statistical test results of the applications. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 32}, {"type": "text", "text": "\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 33}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Information on computer resources is provided in the GitHub repository referenced to in Footnote 3 of the main paper. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 33}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The research conducted in the paper is, in every aspect, conform with the NeurIPS Code of Ethics. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 33}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We do not foresee direct negative societal impact from the current work. Positive societal impact in form of making benchmarking results more robust to unjustified assumptions are discussed in Section 4.2 of the paper. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 34}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 34}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 35}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 35}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 36}]