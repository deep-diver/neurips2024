[{"figure_path": "kEQFjKqiqM/tables/tables_3_1.jpg", "caption": "Table 1: Comparison of MSE for the Maxwell, Zener, and Kelvin-Voigt models using FROND-NN and DRAGON-NN frameworks.", "description": "This table compares the Mean Squared Error (MSE) achieved by two different neural network models (FROND-NN and DRAGON-NN) when fitting three different viscoelastic models (Maxwell, Zener, and Kelvin-Voigt).  The results show that the DRAGON-NN model consistently achieves a lower MSE than the FROND-NN model, indicating improved accuracy in capturing the dynamics of these viscoelastic systems.", "section": "2.4 Motivation: Advanced Dynamics Modeling Capability"}, {"figure_path": "kEQFjKqiqM/tables/tables_7_1.jpg", "caption": "Table 2: Numerical results for various methods on LRGB tests.", "description": "This table presents the numerical results obtained for various graph neural network methods on the Long Range Graph Benchmark (LRGB) tests, specifically focusing on the Peptides dataset.  It compares the performance of several methods, including GCN, GCNII, GINE, GatedGCN, Transformer+LapPE, SAN+LapPE, SAN+RWSE, GCN+DRew, PathNN, DRGNN, GRAND-1, F-GRAND-1, and D-GRAND-1, across two tasks: peptides-func (graph classification using Average Precision (AP) as the metric) and peptides-struct (graph regression using Mean Absolute Error (MAE) as the metric). The results highlight the superior performance of the DRAGON-based methods (GRAND-1, F-GRAND-1, and D-GRAND-1) in capturing long-range dependencies within the graph data, demonstrating significant improvements in both AP and MAE compared to the baseline methods.", "section": "4.2 Long Range Graph Benchmark"}, {"figure_path": "kEQFjKqiqM/tables/tables_8_1.jpg", "caption": "Table 3: Node classification results(%) for random train-val-test splits. The best result of each continuous GNN family is highlighted in red.", "description": "This table presents the performance of various continuous Graph Neural Network (GNN) models on node classification tasks across multiple datasets.  The datasets represent different graph types, including citation networks (Cora, Citeseer, Pubmed), tree-structured datasets (Disease, Airport), and co-authorship and co-purchasing networks (CoauthorCS, Computer, Photo, CoauthorPhy).  The results show the classification accuracy (in percentage) achieved by each model on each dataset, with the best performance within each GNN family (GRAND, F-GRAND, D-GRAND, GraphCON, F-GraphCON, D-GraphCON) highlighted in red. This allows for a direct comparison of the effectiveness of different continuous GNN models and the impact of the DRAGON framework on their performance.", "section": "4.3 Node Classification"}, {"figure_path": "kEQFjKqiqM/tables/tables_8_2.jpg", "caption": "Table 4: Node classification results(%). The best and the second-best result for each criterion are highlighted in red and blue, respectively.", "description": "This table presents the results of node classification experiments conducted on six heterophilic graph datasets.  The datasets are characterized by a low adjusted homophily, indicating a higher degree of heterophily.  The results show the performance of various methods, including the proposed D-CDE model (a continuous GNN enhanced with the DRAGON framework), and compare them to other continuous GNN models (e.g., GRAND, GraphCON, CDE, F-CDE) and integer-order GNN models (e.g., GCN, GAT). The best and second-best performing models for each dataset and metric are highlighted for easier comparison and interpretation.", "section": "4.3 Heterophilic Graph Datasets"}, {"figure_path": "kEQFjKqiqM/tables/tables_21_1.jpg", "caption": "Table 5: Dataset statistics used in Table 3", "description": "This table provides details on the datasets used in the node classification experiments reported in Table 3 of the paper.  For each dataset, it lists the type of data (citation, co-author, co-purchase, or tree-like), the number of classes, the number of features, the number of nodes, and the number of edges.  These statistics help to characterize the size and complexity of the datasets used in the evaluation of the proposed DRAGON framework and its comparison to existing continuous GNN models.", "section": "4.3 Node Classification"}, {"figure_path": "kEQFjKqiqM/tables/tables_21_2.jpg", "caption": "Table 4: Node classification results(%). The best and the second-best result for each criterion are highlighted in red and blue, respectively.", "description": "This table presents the results of node classification experiments conducted on six heterophilic graph datasets.  The performance of various methods is compared, with the best and second-best results for each dataset highlighted.  The datasets include Roman-empire, Wiki-cooc, Minesweeper, Questions, Workers, and Amazon-ratings, each characterized by a low adjusted homophily (hadj), indicating a high degree of heterophily.", "section": "4.3 Node Classification"}, {"figure_path": "kEQFjKqiqM/tables/tables_21_3.jpg", "caption": "Table 7: Dataset and graph statistics used in Table 10", "description": "This table provides a detailed breakdown of the datasets used for the experiments presented in Table 10 of the paper.  It includes the number of graphs (total and fake), the total number of nodes and edges, and the average number of nodes per graph for both the Politifact (POL) and Gossipcop (GOS) datasets. This information is crucial for understanding the scale and complexity of the datasets used in the long-range graph benchmark experiments. ", "section": "4.2 Long Range Graph Benchmark"}, {"figure_path": "kEQFjKqiqM/tables/tables_22_1.jpg", "caption": "Table 8: Inference time of models on the Cora dataset: integral time T = 10 and step size of 1", "description": "This table shows the inference time in milliseconds (ms) for different continuous GNN models on the Cora dataset. The integral time T is set to 10 and the step size is 1. Three groups of models are presented: DRAGON models, FROND models, and the original continuous GNN models (GRAND, GraphCON, and CDE). For each model, two versions are tested: linear and non-linear. The table allows for a comparison of the computational efficiency of different models in terms of inference time.", "section": "4.1 Implementation Details"}, {"figure_path": "kEQFjKqiqM/tables/tables_22_2.jpg", "caption": "Table 9: Training time per epoch on the Cora dataset: integral time T = 10 and step size of 1", "description": "This table presents the training time per epoch for various continuous Graph Neural Network (GNN) models on the Cora dataset.  The models are categorized into three groups: DRAGON, FROND, and baseline models.  Each group includes linear and nonlinear versions of several models (GRAND, GraphCON, CDE).  The integral time (T) is set to 10, and the step size is 1.  The results show the training time in milliseconds (ms) for each model.", "section": "4.1 Implementation Details"}, {"figure_path": "kEQFjKqiqM/tables/tables_22_3.jpg", "caption": "Table 10: Graph classification results", "description": "This table presents the results of graph classification experiments conducted on the FakeNews-Net dataset using various methods, including GraphSage, GCN, GAT, GRAND-1, F-GRAND-1, and D-GRAND-1.  The dataset includes profile, word2vec, and BERT features. The table showcases the performance (Average Precision) of each method on both the POL and GOS subsets of the FakeNews-Net dataset, highlighting the superior performance of the DRAGON-enhanced models (F-GRAND-1 and D-GRAND-1).", "section": "I More Experiment Results"}, {"figure_path": "kEQFjKqiqM/tables/tables_23_1.jpg", "caption": "Table 11: Oversmoothing mitigation under fixed data splitting without using largest connected component (LCC). '-' indicates the numerical solvers failed.", "description": "This table presents the results of an experiment designed to evaluate the oversmoothing mitigation capabilities of different models across various graph datasets.  The experiment used fixed data splitting (without using the largest connected component) and varied the number of layers (represented by integration time). The models compared include GCN, GAT, GRAND-1, F-GRAND-1, and D-GRAND-1. The results are shown as the accuracy with standard deviation. The '-' indicates cases where the numerical solver failed to converge.", "section": "1.2 Oversmoothing Mitigation"}, {"figure_path": "kEQFjKqiqM/tables/tables_23_2.jpg", "caption": "Table 12: Node classification results (%) of heterophilic graph under fixed data splits [76]", "description": "This table presents the results of node classification experiments conducted on three heterophilic graph datasets (Texas, Wisconsin, Cornell).  The results compare the performance of various continuous graph neural network (GNN) models, including several baselines and the proposed D-GRAND and D-GREAD models. The table highlights the improved performance of the DRAGON framework integrated GNNs, showcasing their effectiveness in handling heterophilic datasets, particularly D-GREAD which achieves the best performance on two out of three datasets.", "section": "4.3 Node Classification"}, {"figure_path": "kEQFjKqiqM/tables/tables_24_1.jpg", "caption": "Table 13: Node classification results (%) under limited-label scenarios", "description": "This table presents the node classification results achieved by different GNN models under limited-label conditions.  It compares the performance of the standard GRAND++, the FROND-enhanced F-GRAND++, and the DRAGON-enhanced D-GRAND++ across various datasets (Cora, Citeseer, Pubmed, CoauthorCS, Computer, Photo) and different numbers of pre-training classes (1, 2, 5, 10, 20). The results highlight the superior performance of the DRAGON-enhanced models, demonstrating their effectiveness in handling scenarios with limited labeled data. ", "section": "Experiments"}, {"figure_path": "kEQFjKqiqM/tables/tables_24_2.jpg", "caption": "Table 3: Node classification results(%) for random train-val-test splits. The best result of each continuous GNN family is highlighted in red.", "description": "This table presents the node classification accuracy results for several continuous Graph Neural Networks (GNNs) on various datasets.  The results are shown as percentages, and the best performance within each family of GNNs is highlighted in red. The datasets include citation networks (Cora, Citeseer, Pubmed), tree-structured datasets (Disease, Airport), and co-authorship/co-purchasing networks (CoauthorCS, Computer, Photo, CoauthorPhy). The table provides a comparison of the performance of different continuous GNN models, including those enhanced by the DRAGON framework, under random train-validation-test splits.", "section": "4.3 Node Classification"}, {"figure_path": "kEQFjKqiqM/tables/tables_25_1.jpg", "caption": "Table 15: Learned  wj of Airport dataset", "description": "This table displays the learned weights (wj) for each fractional order (\u03b1j) in the DRAGON model applied to the Airport dataset.  The weights were learned using the distributed-order fractional derivative approach. Each row represents a different set of learned weights, demonstrating the model's ability to find optimal weights for the combination of fractional orders.  The final accuracy achieved for each set of weights is also shown. This table illustrates the robustness of the DRAGON framework's parameter selection.", "section": "4.3 Node Classification"}, {"figure_path": "kEQFjKqiqM/tables/tables_25_2.jpg", "caption": "Table 16: Learned wj of Roman-empire dataset.", "description": "This table presents the learned weights (wj) for each fractional order (aj) in the DRAGON framework when applied to the Roman-empire dataset.  The weights are learned parameters that determine the contribution of each fractional derivative order to the overall feature update dynamics. Each row represents a different set of learned weights, resulting in a slightly different model accuracy. The 'X' indicates that the weight for that specific fractional order was not learned or is zero. The final column shows the corresponding accuracy achieved by the model with that set of weights.", "section": "4.3 Node Classification"}, {"figure_path": "kEQFjKqiqM/tables/tables_25_3.jpg", "caption": "Table 17: Node classification accuracy(%) on Ogb-products dataset", "description": "This table presents the results of node classification experiments conducted on the Ogb-products dataset.  The results compare the performance of various graph neural network (GNN) models, including a Multilayer Perceptron (MLP), Node2vec, a full-batch GCN, GraphSAGE, GRAND-1, F-GRAND-1 (FROND-enhanced GRAND), and D-GRAND-1 (DRAGON-enhanced GRAND). The accuracy (Acc) is reported with standard deviation for each model, demonstrating the effectiveness of the DRAGON framework in enhancing the performance of GNNs on large-scale graph datasets.", "section": "Large Scale Ogb-Products dataset"}, {"figure_path": "kEQFjKqiqM/tables/tables_26_1.jpg", "caption": "Table 18: Hyper-parameters used in Table 4", "description": "This table lists the hyperparameters used for the D-CDE model in the node classification experiments reported in Table 4 of the paper.  The hyperparameters include learning rate (lr), weight decay, input dropout rate, dropout rate, hidden dimension size, integration time, and step size.  Each row represents a different dataset and its corresponding hyperparameter settings.", "section": "4.3 Node Classification"}]