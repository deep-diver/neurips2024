[{"type": "text", "text": "Learning Linear Causal Representations from General Environments: Identifiability and Intrinsic Ambiguity ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jikai Jin Institute for Computational and Mathematical Engineering Stanford University Stanford, CA 94305 jkjin@stanford.edu ", "page_idx": 0}, {"type": "text", "text": "Vasilis Syrgkanis Management Science and Engineering Stanford University Stanford, CA 94305 vsyrgk@stanford.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study causal representation learning, the task of recovering high-level latent variables and their causal relationships in the form of a causal graph from lowlevel observed data (such as text and images), assuming access to observations generated from multiple environments. Prior results on the identifiability of causal representations typically assume access to single-node interventions which is rather unrealistic in practice, since the latent variables are unknown in the first place. In this work, we consider the task of learning causal representation learning with data collected from general environments. We show that even when the causal model and the mixing function are both linear, there exists a surrounded-node ambiguity (SNA) [46] which is basically unavoidable in our setting. On the other hand, in the same linear case, we show that identification up to SNA is possible under mild conditions, and propose an algorithm, LiNGCReL which provably achieves such identifiability guarantee. We conduct extensive experiments on synthetic data and demonstrate the effectiveness of LiNGCReL in the finite-sample regime. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Artificial intelligence (AI) has achieved tremendous success in various domains in the past decade [4, 40, 6]. However, current approaches are largely based on learning the statistical structures and relationships in the data that we observe. As a result, it is not surprising that these approaches often capture spurious statistical dependencies between different features, resulting in poor performance in the presence of test distribution shift [30, 22] or adversarial attacks [3, 50]. ", "page_idx": 0}, {"type": "text", "text": "In view of these pitfalls, a recent line of work has explored the problem of causal representation learning (CRL) [34], the task of learning the causal relationships between high-level latent variables underlying our low-level observations. Notably, it is widely believed in cognitive psychology that humans take a causal approach to distill information from the world and make decisions to achieve their goals [37, 12, 19]. As a result, there is reason to believe that learning causal representations has the potential to significantly improve the power of AI, especially on tasks where performance lags far behind human level [17]. ", "page_idx": 0}, {"type": "text", "text": "Despite such promise, a crucial challenge in CRL is the identifiability of the data generating process; in other words, given the data that we observe, can we uniquely identify the underlying causal model. It has been shown that given observational data (i.e., i.i.d. data generated from a single environment), the model is already non-identifiable in strictly simpler settings where the latent variables are known to be independent [25, 26], or where there is no mixing function and one directly observes the latent variables [39]. As a result, existing algorithms for CRL with observational data [52, 53, 11] typically require additional assumptions on the structure of the underlying causal graph. A natural question that arises is what types of data do we need to acquire to make identification possible in the general case. ", "page_idx": 1}, {"type": "text", "text": "One line of works assumes access to counterfactual data [27, 48, 5], where some form of weak supervision is typically required. A common assumption here is that one observes data in pairs, where each pair of data is related via sharing part of the latent representation. However, such data is hard to acquire since it requires direct control on the latent representation. ", "page_idx": 1}, {"type": "text", "text": "Another line of works [1, 49, 7, 47] instead considers an interventional setting, where the learner observes data generated from multiple different environments. This is arguably a much more realistic setup and reflects common practices in robotics [24] and genomics [28, 43] applications. However, a vast majority of identifiability guarantees assume that each environment corresponds to single-node, hard interventions, which is defined as interventions that isolate a single latent variable from its causal parents. Again, this is quite a restrictive assumption because of two reasons. First, since the latent variables are unknown and need to be learned from data, it is unclear how to perform interventions that only affect one variable. Second, even if one can perform single-node interventions, it may not be feasible to artificially remove causal effects in the data generating processes. This issue is ubiquitous in real-world applications as pointed out in Campbell [8], Eberhardt [14], Eronen [15]. Motivated by these challenges, we make the following contributions in this paper: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Assuming access to data collected from multiple environments, but not necessarily from single-node, hard interventions, we identify an intrinsic surrounded-node ambiguity (SNA) in learning the underlying causal representations. We show in Theorem 3 that SNA is unavoidable even if (1) both the mixing function and the causal model are known to be linear and (2) one has access to single-node, soft interventions. This highlights a remarkable difference with existing literature which shows that perfect identification can be achieved with hard interventions.   \n\u2022 When the causal model and the mixing function are both linear, we prove in Theorem 1 that identification up to SNA is achievable with $O(d)$ diverse environments (Assumption 4), where $d$ is the size of the latent causal graph. To the best of our knowledge, this is the first identification guarantee that applies to fully general environments and makes no assumption on their relationship or similarity. Interestingly, we also show in Theorem 2 that one would require $\\Omega(d^{2})$ single-node soft interventions to achieve the same identification guarantee, indicating the benefit of learning from diverse environments.   \n\u2022 We propose an algorithm, LiNGCReL, in Section 5 that provably recovers the ground-truth model up to SNA (Theorem 4) in the setting of Theorem 1 when perfect information of the observation distributions is available. To demonstrate the effectiveness of LiNGCReL in finite-sample regime, we conduct extensive experiments on synthetic data, and our results reported in Section 6 show that LiNGCReL is capable of recovering the true causal model up to SNA with high accuracy. ", "page_idx": 1}, {"type": "text", "text": "Due to space limit, proofs of all our statements and additional theoretical results are given in the appendix. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We consider the standard setup of CRL from multiple environments $E\\in{\\mathfrak{E}}$ . Let $\\mathcal{G}=(\\mathcal{V},\\mathcal{E})$ be the ground-truth causal graph which is directed and acylic (DAG), where $\\mathcal{V}=[d]$ and $\\mathcal{E}$ describes the causal relationship between different nodes. Each node corresponds to a latent variable $z_{i}\\in\\mathbb{R}$ . ", "page_idx": 1}, {"type": "text", "text": "For any node $i$ , we let $\\mathrm{pa}_{\\mathcal{G}}(i)$ , $\\mathrm{ch}_{\\mathcal{G}}(i)$ , $\\operatorname{ans}_{\\mathcal{G}}(i)$ and $\\mathrm{nd}_{\\mathcal{G}}(i)$ to be the set of all parents, children, ancestors and non-descendants of $i$ in $\\mathcal{G}$ respectively. We also define $\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}(i)=\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}(i)\\cup\\{i\\}$ and similarly for $\\overline{{\\mathrm{ch}}}_{\\mathcal{G}}(i),\\overline{{\\mathrm{ans}}}_{\\mathcal{G}}(i)$ and $\\overline{{\\mathrm{nd}}}_{\\mathcal{G}}(i)$ . Assuming that all probability distributions have continuous ", "page_idx": 1}, {"type": "text", "text": "densities, the joint density of the latent variables $_{\\textit{z}}$ can then be written as ", "page_idx": 2}, {"type": "equation", "text": "$$\np_{E}(z)=\\prod_{i=1}^{d}p_{i}^{E}\\left(z_{i}\\mid z_{\\mathrm{pa}_{G}(i)}\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $p_{i}^{E}$ is the (unknown) latent generating distribution from environment $E$ at node $i$ . Here for a given vector $\\pmb{v}$ , we write $\\pmb{v}_{i}=\\pmb{e}_{i}^{\\top}\\pmb{v}$ , and let $\\pmb{v}_{S}=(\\pmb{v}_{i}:i\\in S)\\in\\mathbb{R}^{|S|}$ . ", "page_idx": 2}, {"type": "text", "text": "The causal graph model with density given by (1) necessarily enjoys the following property: ", "page_idx": 2}, {"type": "text", "text": "Definition 1 (Causal Markov Condition). For any node $i,$ , conditioned on $z_{\\mathrm{pa}_{\\mathcal{G}}(i)}$ , $z_{i}$ is independent of $z_{\\mathrm{nd}_{\\mathcal{G}}(i)}$ . As a consequence, for any node $i,j\\in[d]$ and $S\\subseteq[d],$ , if $S$ $d$ -separates $i$ from $j$ (cf. Definition 7), then $z_{i}\\perp z_{j}\\mid z_{S}$ . ", "page_idx": 2}, {"type": "text", "text": "The latent variables $_{z}$ are unknown to the learner. Instead, the learner has access to observations $\\pmb{x}\\in\\mathbb{R}^{n}\\,(n\\geqslant d)$ from all environments $E\\in{\\mathfrak{E}}$ that are related to the latent $_{\\textit{z}}$ via an injective mixing function $\\textbf{\\textit{g}}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{x}=\\pmb{g}(\\pmb{z}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The main assumption here that the mixing function is the same across all environments: ", "page_idx": 2}, {"type": "text", "text": "Assumption 1. All environments $E\\in{\\mathfrak{E}}$ share the same diffeomorphic mixing function $g:\\mathbb{R}^{d}\\mapsto\\mathbb{R}^{n}$ . ", "page_idx": 2}, {"type": "text", "text": "In CRL, the goal of the learner is to 1) recover the inverse of the mixing function $h=g^{-1}$ (often called the unmixing function) which allows recovering the latent variables given any observations, and, 2) recover the underlying causal graph $\\mathcal{G}$ . In the remaining part of this paper, we refer to $(h,\\mathcal{G})$ as the causal model to be learned. Obviously, there would be some ambiguities in learning $(h,\\mathcal{G})$ . For example, choosing a different permutation of the nodes in the causal graph would lead to a different model, and so does element-wise transformations on each component $h_{i}$ of $^h$ . ", "page_idx": 2}, {"type": "text", "text": "A line of recent works show that the ground-truth model can be identified up to these ambiguities in various settings, assuming access to single-node hard interventions [36, 49, 47]. On the other hand, some weaker notions of identifiability have also been proposed and studied in the literature [36, 46, 23] for single-node soft interventions. Here, we provide a generic definition of single-node soft interventions that we will rely on in this paper. ", "page_idx": 2}, {"type": "text", "text": "Definition 2. We say that a collection of environments \u02c6E is a set of (soft) interventions on a subset of latent variables $\\{z_{j},j\\in S\\}$ if for any $i\\in[d]$ and any $E_{1},E_{2}\\in\\hat{\\mathfrak{E}},E_{1}\\neq E_{2},$ , we have piE1= piE2if and only if $i\\not\\in S$ (the notation $p_{i}^{E}$ comes from (1)). Equivalently, we write $\\mathcal{T}_{z}^{\\hat{\\mathfrak{E}}}=S$ . ", "page_idx": 2}, {"type": "text", "text": "We note that soft interventions are very different from hard interventions, since they do not remove causal relationships between latent variables. The goal of this paper is to address the following question: ", "page_idx": 2}, {"type": "text", "text": "What is the best-achievable identification guarantee when hard interventions are not available, and what are the intrinsic ambiguities? ", "page_idx": 2}, {"type": "text", "text": "3 The surrounding set and a notion of identifiability ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "One may expect that identifiability with soft interventions is not much different from hard interventions, since soft interventions can approximate hard interventions with arbitrary accuracy. However, we will show that this is not the case. At a high level, hard intervention is more powerful than soft intervention because it is capable of isolating a latent variable from its direct cause while soft interventions is not, so soft interventions can sometimes fail to identify the true causal relationship from a mixture of causal effects. ", "page_idx": 2}, {"type": "text", "text": "To quantify what kind of ambiguities may arise, we can define the surrounding set for each node in a causal graph $\\mathcal{G}$ as follows: ", "page_idx": 2}, {"type": "text", "text": "Definition 3. (46, Definition 3) For two nodes $i,j\\in[d]$ in $\\mathcal{G}$ , we say that $j$ is surrounded by i, or $i\\in\\operatorname{sur}_{\\mathcal{G}}(j)\\;i f i\\in\\operatorname{pa}_{\\mathcal{G}}(j)$ , and $\\mathrm{ch}_{\\mathcal{G}}(j)\\subseteq\\mathrm{ch}_{\\mathcal{G}}(i)$ . Moreover, we define $\\overline{{\\operatorname{sur}}}_{\\mathcal{G}}(j)=\\operatorname{sur}_{\\mathcal{G}}(j)\\cup\\{j\\}$ . ", "page_idx": 2}, {"type": "text", "text": "Intuitively, if there exists some $i\\in\\operatorname{sur}_{\\mathcal{G}}(j)$ , then ambiguities may arise for the causal variable at node $j$ , since any effect of $j$ on any of its child $k$ can also be interpreted as a mixture of the effect of $i$ and $j$ . In Appendix E we discuss an example with three causal variables to further illustrate such ambiguities. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "image", "img_path": "dB99jjwx3h/tmp/bb8241cef7190fd2e8341a126b28f85e630de858d7b947047dd7350ed10251fd.jpg", "img_caption": ["Figure 1: An illustration of Definition 3; here $i\\in\\operatorname{sur}_{\\mathcal{G}}(j)$ . "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Definition 3 naturally induces the following relationship between causal models: ", "page_idx": 3}, {"type": "text", "text": "Definition 4. Using the notations in Definition $I O$ , we write $(h,\\mathcal{G})\\sim_{\\mathrm{sur}}\\ (\\hat{h},\\hat{\\mathcal{G}})$ if there exists a permutations $\\pi$ on $[d]$ , and a diffeomorphism $\\psi:\\mathbb{R}^{d}\\mapsto\\mathbb{R}^{d}$ where the $j$ -th component of $\\psi$ , denoted by $\\psi_{j}(z)$ , is a function of $z_{\\overline{{\\mathrm{sur}}}_{\\mathcal{G}}(j)}\\,f o r\\,\\forall j\\in[d]$ , such that the following holds: ", "page_idx": 3}, {"type": "text", "text": "In other words, $\\sim_{\\mathrm{sur}}$ requires that the causal graph to be exactly the same up to some permutation of nodes, but allows each latent variable $\\pmb{v}_{i}$ to be a mixture of $z_{\\mathrm{sur}_{\\mathcal{G}}(i)}$ . Although not obvious from definition, one can actually check that $\\sim_{\\mathrm{sur}}$ defines an equivalence relation (see Lemma 11). Moreover, we will show in the following section that $\\sim_{\\mathrm{sur}}$ is in general the best that we can hope for in our problem setting. ", "page_idx": 3}, {"type": "text", "text": "4 Identifiability theory for linear CRL with general environments ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we consider learning causal models from general environments. Specifically, we assume that the environments $E_{k},k\\in[K]$ share the same causal graph, but the dependencies between connected nodes (latent variables) are completely unknown, and, in contrast with existing literature on single-node interventions, we impose no similarity constraints on the environments. We begin our investigation of identifiability in this setting in the context of linear causal models with a linear mixing function. ", "page_idx": 3}, {"type": "text", "text": "4.1 Problem setup ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Formally, we assume the following generative model in $K$ distinct environments ${\\mathfrak{E}}=\\{E_{k}:k\\in[K]\\}$ with data generating process ", "page_idx": 3}, {"type": "equation", "text": "$$\nz=A_{k}z+\\Omega_{k}^{\\frac{1}{2}}\\epsilon,\\quad x=G z\\quad k\\in[K],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the matrix $A_{k}$ satisfies $(A_{k})_{i j}\\neq0$ if and only if $j\\rightarrow i$ in $\\mathcal{G}$ . We refer to $(A_{k},\\Omega_{k})$ as the weight matrices of latent variables $_{z}$ in the environment $E_{k}$ . It is easy to see that Assumption 1 in our general setup translates into the following assumption: ", "page_idx": 3}, {"type": "text", "text": "Assumption 2. The mixing matrix $G\\in\\mathbb{R}^{n\\times d}$ has full column rank. Equivalently, the unmixing matrix $\\bar{\\pmb{H}}=\\pmb{G}^{\\dagger}$ has full row rank. ", "page_idx": 3}, {"type": "text", "text": "Let $B_{k}\\,=\\,\\Omega_{k}^{-\\,\\frac{1}{2}}(I-A_{k}),k\\,\\in\\,[K].$ , then we have $\\epsilon=B_{k}z=B_{k}H x$ . Since in the linear case, there is an easy to see one-to-one correspondence between the matrix $\\pmb{H}$ and the un-mixing function $x\\mapsto H x$ , we abuse the notation and write $(H,{\\mathcal{G}})$ to represent the model instead of $(h,\\bar{\\mathcal{G}})$ . Using $h_{i}$ to denote the $i$ -th row of $\\pmb{H}$ , the following lemma translates Definition 4 the the linear setting: ", "page_idx": 3}, {"type": "text", "text": "Lemma 1. According to Definition 4, $(H,\\mathcal{G})\\sim_{\\mathrm{sur}}\\left(\\hat{H},\\hat{\\mathcal{G}}\\right)$ if and only if there exists a permutation $\\pi$ on $[d]$ , such that the following statements hold: ", "page_idx": 3}, {"type": "text", "text": "1. For all $i,j\\in[d],\\,i\\in\\mathrm{pa}_{\\mathcal{G}}(j)$ if and only $i f\\pi(i)\\in\\mathrm{pa}_{\\hat{\\mathcal{G}}}(\\pi(j)).$ , and ", "page_idx": 4}, {"type": "text", "text": "We also need to make the following assumption on noise. ", "page_idx": 4}, {"type": "text", "text": "Assumption 3. The noise vector $\\epsilon\\in\\mathbb{R}^{d}$ has independent components, at most one component is Gaussian distributed, and any two components have different distribution. ", "page_idx": 4}, {"type": "text", "text": "The non-gaussianity of the noise vectors is a typical assumption in causal discovery within linear models [9, 39] and is always assumed in the LinGAM setting [38]. The assumption that all components have a different distribution is not so standard, but is quite natural in real-world scenarios. ", "page_idx": 4}, {"type": "text", "text": "4.2 Identifiability guarantee ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "For each node $i\\in[d]$ of $\\mathcal{G}$ , we use $w_{k}(i)$ to be the weight vector of environment $E_{k}$ at node $i$ , i.e., $\\pmb{w}_{k}(i)=\\left((\\pmb{A}_{k})_{i j}:j\\in\\mathrm{pa}_{\\mathcal{G}}(i)\\right)\\in\\mathbb{R}^{|\\mathrm{pa}_{\\mathcal{G}}(i)|}$ . In other words, the structural equation for node $i$ in environment $k$ is of the form: ", "page_idx": 4}, {"type": "equation", "text": "$$\nz_{i}=w_{k}(i)^{\\top}z_{\\mathrm{pa}_{\\mathcal{G}}(i)}+\\sqrt{\\omega_{k,i,i}}\\epsilon_{i}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "To obtain our identifiability result, the main assumption we need to make is the non-degeneracy of the weights at each node: ", "page_idx": 4}, {"type": "text", "text": "Assumption 4. For each node $i~\\in~[d]$ of $\\mathcal{G}$ , we have aff $(\\pmb{w}_{k}(i):k\\in[K])\\\\,=\\,\\mathbb{R}^{|\\mathrm{pa}_{\\mathcal{G}}(i)|}$ where aff $(\\cdot)$ denotes the affine hull. Equivalently, the weights ${\\pmb w}_{k}(i),k\\,=\\,1,2,\\cdots\\,,K$ do not lie in $a$ paG(i)  \u22121 -dimensional hyperplane of R|paG(i)|. ", "page_idx": 4}, {"type": "text", "text": "This assumption is quite mild since it only requires the weight vectors to be in general positions, and it holds with probability 1 if the weights at each node are sampled from continuous distributions. Moreover, as shown in Lemma 5, it is equivalent to the following assumption. ", "page_idx": 4}, {"type": "text", "text": "Assumption 5 (Node-level non-degeneracy). We say that the matrices $\\{B_{k}\\}_{k=1}^{K}$ are node-level non-degenerate if for all node $i\\in[d]$ , we have dim span $\\langle(B_{k})_{i}:k\\in[K]\\rangle=|{\\mathrm{pa}}_{\\mathcal{G}}(i)|+1,$ , where $(\\boldsymbol{B}_{k})_{i}$ is the $i$ -th row of $\\scriptstyle B_{k}$ . ", "page_idx": 4}, {"type": "text", "text": "In the following, we state our main result in this section, which shows that $K=d$ non-degenerate environments suffices for the model to be identifiable up to $\\sim_{\\mathrm{sur}}$ . ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. Suppose that $K\\geqslant d$ and we have access to observations generated from the linear causal model $(H,{\\mathcal{G}})$ across multiple environments ${\\mathfrak{E}}\\,=\\,\\{E_{k}:k\\in[K]\\}$ with observation distributions $\\{\\mathbb{P}_{\\mathbf{x}}^{E}\\}_{E\\in\\mathfrak{E}}$ , and the data generating processes are given by (3). Let $(\\hat{H},\\hat{\\mathcal{G}})$ be any candidate solution with the hypothetical data generating process ", "page_idx": 4}, {"type": "equation", "text": "$$\nv=\\hat{A}_{k}v+\\hat{\\Omega}_{k}^{\\frac{1}{2}}\\hat{\\epsilon},\\quad x=\\hat{H}^{\\dagger}v\\quad i n\\;t h e\\;e n\\nu i r o n m e n t\\;E_{k}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\hat{H}$ has full row rank, such that ", "page_idx": 4}, {"type": "text", "text": "(i) the observation distribution that this hypothetical model generates in $E_{k}$ is exactly $\\mathbb{P}_{\\mathbf{x}}^{E_{k}}$ ; (ii) all environments share the same causal graph: $\\forall k\\in[K]$ and $i,j\\in[d],\\,({\\bf{{A}}}_{k})_{i j}\\ne0\\Leftrightarrow j\\in$ $\\mathrm{pa}_{\\mathcal{G}}(i),\\,(\\hat{A}_{k})_{i j}\\neq0\\Leftrightarrow j\\in\\mathrm{pa}_{\\hat{\\mathcal{G}}}(i)$ and $\\Omega_{k},\\hat{\\Omega}_{k}$ are diagonal matrices with positive entries; (iii) {Bk}kK=1 and B\u02c6k = \u02c6\u2126k\u221212(I \u2212A\u02c6k) kK=1 are non-degenerate in the sense of Assump(iv) the noise variables $\\epsilon$ and $\\hat{\\epsilon}$ satisfy Assumption 3. ", "page_idx": 4}, {"type": "text", "text": "Then we must have $(H,\\mathcal{G})\\sim_{\\mathrm{sur}}\\left(\\hat{H},\\hat{\\mathcal{G}}\\right)$ . ", "page_idx": 4}, {"type": "text", "text": "The proof of Theorem 1 is given in Appendix H.1. In the next section, we will introduce an algorithm, LiNGCReL, that provably recovers the ground-truth up to $\\sim_{\\mathrm{sur}}$ . ", "page_idx": 4}, {"type": "text", "text": "To the best of our knowledge, this is the first identifiability guarantee in the literature for CRL from general environments, even for the linear case. Our result is closely related but fundamentally different from Xie et al. [52, 53], Dong et al. [11] that consider the task of linear CRL using observational data. As discussed before, with observational data the causal graph can at best be identified up to Markov equivalence. As a result, one typically requires additional assumptions on the structure of the causal graph to obtain stronger guarantees. In contrast, we show that with data from multiple environments, exact recovery of the causal graph is possible without any structural assumptions. ", "page_idx": 5}, {"type": "text", "text": "Interestingly, while the fact that existing works focus on single-node interventions seem to suggest that learning from diverse environments is hard, it turns out that such diversity is actually helpful. Specifically, we show that in the worst case, $\\Theta(d^{2})$ interventions are required for identifying the ground-truth model under $\\sim_{\\mathrm{sur}}$ : ", "page_idx": 5}, {"type": "text", "text": "Theorem 2 (informal version of Theorem 6). There exists a causal graph $\\mathcal{G}$ with $\\Theta(d^{2})$ edges, such that for any unmixing matrix $H\\in\\mathbb{R}^{d\\times n}$ with full row rank, any independent noise variables \u03f5, and any $\\dot{0}<s_{i}\\gets\\left|\\mathrm{pa}_{\\mathcal{G}}(\\breve{i})\\right|,i\\in[d]$ , the ground-truth model $(H,{\\mathcal{G}})$ is non-identifiable up to $\\sim_{\\mathrm{sur}}$ with $s_{i}$ soft interventions for node $i$ , unless the (ground-truth and intervened) weights of the causal model lie in a null set (w.r.t the Lebesgue measure). ", "page_idx": 5}, {"type": "text", "text": "A formal version and the proof of Theorem 2 can be found in Appendix H.2. On the other hand, by having $d$ single-node interventions per node, Assumption 5 can be satisfied as long as the weights are in general positions, so in this case we have $(H,\\mathcal{G})\\sim_{\\mathrm{sur}}\\ (\\hat{H},\\hat{\\mathcal{G}})$ by Theorem 1. Therefore, Theorems 1 and 6 together imply that $\\Theta(d^{2})$ single-node interventions are necessary and sufficient for identification up to \u223csur. ", "page_idx": 5}, {"type": "text", "text": "Given that Theorem 1 only guarantees identification up to $\\sim_{\\mathrm{sur}}$ that is strictly weaker than full identification, one might naturally ask whether Theorem 1 can be further improved. Our last theorem in this section indicates that $\\sim_{\\mathrm{sur}}$ is indeed a fundamental barrier that exists even when we access to single node, soft interventions. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3 (Counterpart to Theorem 1, informal version of Theorem 9). For any linear causal model $(H,{\\mathcal{G}})$ and any set of environments ${\\mathfrak{E}}=\\{E_{k}:k\\in[K]\\}$ such that all conditions in Theorem 1 are satisfied, there must exists a candidate solution $(\\hat{H},\\mathcal{G})$ and a hypothetical data generating process that satisfy the same set of conditions, but ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{\\partial{\\pmb v}_{i}}{\\partial z_{j}}\\neq0,\\quad\\forall j\\in\\overline{{\\mathrm{sur}}}_{\\mathcal{G}}(i).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Moreover, if we additionally assume that the environments are groups of single-node soft interventions, then we can guarantee the existence of $(\\hat{H},\\mathcal{G})$ and weight matrices which, besides the properties listed above, are also groups of single-node soft interventions. ", "page_idx": 5}, {"type": "text", "text": "5 LinGCReL: Algorithm for linear non-Gaussian causal representation learning ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we introduce Linear Non-Gaussian Causal Representation Learning (LiNGCReL), an algorithm that provably recovers the underlying causal graph and latent variables up to $\\sim_{\\mathrm{sur}}$ in the infinite-sample limit. At this point, it is instructive to recall the celebrated LiNGAM algorithm [38] for linear causal graph discovery. Different from their setting, we only observe some unknown linear mixture of the latent variables. Hence, running linear ICA as in LiNGAM only gives us $M_{k}=B_{k}H$ rather than the weight matrix $B_{k}$ itself. ", "page_idx": 5}, {"type": "text", "text": "The key idea in our approach is an effect cancellation scheme that allows us to determine the \u201cremaining degree of freedom\u201d (RDF) of any node (a.k.a. latent variable) given any subset of its ancestors. This scheme allows us to not only find a topological order of the nodes, but also figure out direct causes by tracking the changes of the RDF. In the following, we present the main steps of LiNGCReL in more details. ", "page_idx": 5}, {"type": "text", "text": "${\\pmb X}^{(k)}=\\left\\{{\\pmb x}_{i}^{(k)}\\right\\}_{i=1}^{N},k\\in[K]$ where (k)is the $i$ -th sample from the $k$ -th environment. ", "page_idx": 5}, {"type": "text", "text": "Step 1. Recover the matrices $M_{k}=B_{k}H$ Since $\\epsilon=B_{k}z=B_{k}H x$ in the $k$ -th environment, so we can use any identification algorithm for linear ICA to recover the matrix $M_{k}$ . Then we properly rearrange the rows of $M_{k}$ so that all $M_{k}{\\boldsymbol{x}},k=1,2,\\cdots,K$ correspond to the same permutation of noise variables. This step is quite standard and details can be found in Appendix B.1. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Step 2. CRL based on $M_{k}$ Now we have obtained $M_{k}\\,=\\,B_{k}H$ , but the unmixing matrix $\\pmb{H}$ is still unknown. We propose Algorithm 3 to learn $\\pmb{H}$ and the causal graph $\\mathcal{G}$ . The main part of Algorithm 3 contains a loop that maintains a node set $S$ which, we will show later, is ancestral, i.e., $i\\in S\\Rightarrow\\operatorname{ans}_{\\mathcal{G}}(i)\\subseteq S.$ . In each round the algorithm finds a new node $i\\not\\in S$ such that $\\operatorname{ans}_{\\mathcal{G}}(i)\\subseteq S$ , and a subroutine Identify-Parents (Algorithm 2) is used to find all parents of $i$ . After that, we append $i$ into $S$ and continue until $S$ contains all nodes in $\\mathcal{G}$ . Finally, the rows of the mixing matrix $\\pmb{H}$ is obtained by intersections of properly-chosen row spaces of $M_{k}$ . ", "page_idx": 6}, {"type": "text", "text": "Both Algorithm 2 and Algorithm 3 include a crucial step, which we call it orthogonal projection, as described in Algorithm 1. At a high level, it helps determine the minimal RDF for $z_{i}$ after fixing the latent variables $z_{S}$ , and this exactly corresponds to the number of parents of $z_{i}$ that are not in $z_{S}$ . We provide a simple example in Appendix E.2 to illustrate why this approach works. ", "page_idx": 6}, {"type": "text", "text": "The following result states that Algorithm 3 can recover the ground-truth causal model up to $\\sim_{\\mathrm{sur}}$ : ", "page_idx": 6}, {"type": "text", "text": "Theorem 4. Suppose that $M_{k},k\\in[K]$ are perfectly identified in Step 1. Let $(\\hat{H},\\hat{\\mathcal{G}})$ be the solution returned by Algorithm 3, then we must have $(H,\\mathcal{G})\\sim_{\\mathrm{sur}}\\left(\\hat{H},\\hat{\\mathcal{G}}\\right)$ . ", "page_idx": 6}, {"type": "text", "text": "The full proof of Theorem 4 is given in Appendix H.3. It crucially relies on the following two propositions that reveal how Algorithm 3 and the subroutine Algorithm 2 work. ", "page_idx": 6}, {"type": "table", "img_path": "dB99jjwx3h/tmp/2e5a3784c1beb2bf343da950ead013085f2d687d88ff20cee9eb4680ada32a8e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Proposition 1. The following two propositions hold for Algorithm 3: ", "page_idx": 6}, {"type": "text", "text": "\u2022 $\\mathrm{ans}_{\\mathcal{G}}(i)\\subseteq S\\Leftrightarrow t h e$ if condition in line 8 of Algorithm 3 is fulfilled; \u2022 the set $S$ maintained in Algorithm 3 is always an ancestral set, in the sense that $j\\in S\\Rightarrow$ $\\operatorname{ans}_{\\mathcal{G}}(j)\\subseteq S$ . ", "page_idx": 6}, {"type": "text", "text": "Proposition 2. Given any ordered ancestral set $S$ that contains $\\mathrm{pa}_{\\mathcal{G}}(i)$ for some $i\\not\\in S$ , Algorithm 2 returns a set $P_{i}\\subseteq S$ that is exactly $\\mathrm{pa}_{\\mathcal{G}}(i)$ . ", "page_idx": 6}, {"type": "table", "img_path": "dB99jjwx3h/tmp/f92be21bcbe3790ceb4b014a8aa1ca4f7b25c93b1b54a1416d659f05fc2537ed.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we present our experimental setup and results for LiNGCReL. Note that LiNGCReL as described in the previous section only works in the population regime. When the number of samples is limited, two main challenges in implementing LiNGCReL are to accurately compute the dimension ", "page_idx": 6}, {"type": "text", "text": "1: Input: Matrices $M_{k},k\\in[K]$   \n2: Output: The edge set $\\mathcal{E}$ on the vertex set $[d]$ and the mixing matrix $\\hat{H}$   \n3: $S\\gets\\emptyset$ ; \u25b7 $S$ is an ordered set of nodes   \n4: $\\mathcal{E}\\gets\\emptyset$ ; $\\vartriangleright\\mathscr{E}$ is the edge set   \n5: while $|S|<d$ do   \n6: for $i\\not\\in S$ do   \n7: $\\{p_{k}\\}_{k=1}^{K}\\gets0$ rthogonal-projections $\\left(S,i,\\{M_{k}\\}_{k\\in[K]}\\right)$   \n8: if dim span $\\langle\\pmb{q}_{k}:k\\in[K]\\rangle=1$ then   \n9: break \u25b7Proposition 1 guarantees that such an $i$ must exist   \n10: end if   \n11: end for   \n12: $P_{i}\\leftarrow$ Identify-Parents(S, i)   \n13: $S\\gets S\\cup\\{i\\}$   \n14: $\\mathcal{E}\\leftarrow\\mathcal{E}\\cup\\{(j,i):j\\in P_{i}\\}$   \n15: end while   \n16: for $i=1$ to $d$ do   \n17: $E_{i}\\gets\\mathrm{span}\\left\\langle(M_{k})_{i}:k\\in[K]\\right\\rangle$   \n18: end for   \n19: for $i=1$ to $d$ do   \n20: $\\hat{h}_{i}\\gets$ any non-zero vector in $\\left(\\cap_{j:(i,j)\\in\\varepsilon}E_{j}\\right)\\cap E_{i}$   \n21: end for   \n22: $\\hat{H}\\gets\\left[\\hat{h}_{1}^{\\top},\\hat{h}_{2}^{\\top},\\cdots,\\hat{h}_{d}^{\\top}\\right]^{\\top}$ ", "page_idx": 7}, {"type": "text", "text": "of a subspace (line 6 of Algorithm 2 and line 8 of Algorithm 3), and to find a vector in the intersection of multiple subspaces (line 20, Algorithm 3). Due to space limit, the implementation details are described in Appendix B.2. ", "page_idx": 7}, {"type": "text", "text": "Experimental setup. We generate the independent noise variables from generalized Gaussian distributions $p_{\\beta}(x)\\ {\\stackrel{-}{\\propto}}\\ \\exp\\left(-\\left|x\\right|^{\\beta}\\right)$ with parameters $\\beta_{k}=0.2k^{2},k=1,2,\\cdots,d,$ , multiplied by normalization constants to make their variances equal to 1. The ground-truth causal graph is generated by first fixing a total order of the vertices, say $1,2,\\cdots\\,,d_{!}$ , then add directed edges $i\\rightarrow j(i<j)$ according to i.i.d. Bernoull $(p)$ distributions, where $p\\in(0,1)$ . The non-zero entries of matrices $\\scriptstyle B_{k}$ and $\\pmb{H}$ are all generated independently from Gaussian distributions. For simplicity, we focus on the case $n=d$ since recovery of the latent graphs only requires information from $d$ components of $\\textbf{\\em x}$ . ", "page_idx": 7}, {"type": "text", "text": "Metrics of estimation error. Since CRL seeks to learn both the causal graphs and the latent variables, for each output of our algorithm we first check if it exactly recovers the ground-truth causal graph. Then, recall that the latent variables and the observations are related by $z=H x$ , given any output unmixing matrix $\\hat{H}$ from Algorithm 3, we define the relative estimation error $\\Delta_{i}$ for $z_{i}$ as the solution of the following optimization problem: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\left\\|\\Delta\\right\\|_{\\infty}\\quad s.t.\\Delta_{i}=\\frac{\\left\\|\\mathrm{proj}_{\\mathrm{span}\\langle h_{j}:j\\in\\overline{{\\mathrm{sur}_{\\mathcal{G}}(i)\\rangle}}}(\\hat{\\hat{h}}_{i})\\right\\|_{2}}{\\left\\|\\hat{\\hat{h}}_{i}\\right\\|_{2}},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where signed permutation is allowed here since the noise distribution in our experiments is symmetric and the order of latent variables $z_{i},i=1,2,\\cdots,d$ does not matter. We refer to the errors $\\Delta_{i}$ defined in (5) as the SNA error. The SNA error measures how much of the row $\\hat{\\hat{h}}_{i}$ that we learn is contained in the span of the ground-truth rows $h_{j},j\\in\\overline{{\\mathrm{sur}_{\\mathcal{G}}}}(i)$ . Indeed, recall that given any observation $\\textbf{\\em x}$ , the ground-truth latent variable is $z=H x$ while our algorithm outputs $\\hat{\\pmb{v}}_{i}=\\hat{\\hat{h}}_{i}^{\\top}\\pmb{x}$ , so the SNA error essentially captures whether the recovered latent variable is close to some linear mixture of latent variables in the effect-dominating set of $i$ . When the SNA error is zero for some node $i$ , we know that the recovered latent variable at node $i$ is exactly a linear mixture of the ground-truth latent variables in $\\overline{{\\mathrm{sur}g}}(i)$ , according to Lemma 1. ", "page_idx": 7}, {"type": "image", "img_path": "dB99jjwx3h/tmp/a32bb2dd52b19f800f2704cf6626cf53c94cfedca57417919df52e95cc31ab74.jpg", "img_caption": ["(e) An example causal graph in our experiment ", "LiNGCReL "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 2: First two rows: plots of SNA Error and graph recovery accuracy achieved by LiNGCReL as functions of sample size (per environment) for different choices of graph size $d$ and number of environments $K$ . Third row: an example of causal graph generated in our experiments, and the estimation error of LiNGCReL for each node. ", "page_idx": 8}, {"type": "text", "text": "We also define the true error for estimating each latent variable. Formally, let $\\hat{\\hat{H}}$ be the unmixing matrix that corresponds to the solution of (5), then we define the true estimation error $\\tilde{\\Delta}_{i}$ of $z_{i}$ as ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\tilde{\\Delta}_{i}=\\left\\|\\left(\\boldsymbol{I}-h_{i}h_{i}^{\\top}\\right)\\hat{\\hat{\\boldsymbol{h}}}_{i}\\right\\|_{2}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Results. We randomly sample 100 causal models with size $d=5$ , 30 causal models with size $d=8$ ad 30 causal models of size $d=10$ . In light of Theorem 1, for each $d\\in\\{5,8,10\\}$ , we sample data from $K\\,=\\,d$ randomly chosen environments; for $d=5$ we also consider $K=20$ to study how different choices of $K$ can affect the result. We run LiNGCReL for each model with different sample sizes, compute the SNA error and true error of the obtained solution from (5) and (6) respectively for each latent variable, and check whether the ground-truth causal graph is exactly recovered. ", "page_idx": 8}, {"type": "text", "text": "Figure 2 shows how the average SNA error (over all latent variables) and the accuracy of graph recovery changes when sample size grows. We can see LiNGCReL successfully recovers about $80\\%$ of all models within each category, and the median of the average SNA error is smaller than $1\\%$ . Moreover, by comparing Figure 2a with Figure 2b, one can observe that if we fix the total number of samples but choose a larger $K$ (i.e., fewer samples per environment), LiNGCReL can still achieve the same level of performance compared with the choice $K=d$ . Intuitively, this is because $K\\gg d$ vectors sampled from an $r(r\\leqslant d)$ dimensional subspace are unlikely to approximately lie in an $(r\\!-\\!1)$ -dimensional subspace, so that the calculation of line 6 of Algorithm 2 and line 8 of Algorithm 3 can be more accurate. We leave a better and quantitative understanding of the trade-off between $d$ and $K$ to future work. ", "page_idx": 8}, {"type": "text", "text": "SNA error v.s. true error. To understand the implication of our theory, we dive deeper by looking into the learning outcome of LiNGCReL on a specific model, of which the causal graph is shown in ", "page_idx": 8}, {"type": "text", "text": "Figure 2e. In Figure 2f, we list the surrounding set of each node and the corresponding SNA error and true error. We can see that if $\\operatorname{sur}_{\\mathcal{G}}(i)=\\emptyset$ , the two errors equal and both are small, but if $\\operatorname{sur}_{\\mathcal{G}}(i)\\neq\\emptyset$ , the true error is much larger than the SNA error. This indicates that LiNGCReL indeed learns the ground-truth model up to $\\sim_{\\mathrm{sur}}$ , as Theorem 1 predicts. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper studies the limit of learning identifiable causal representations using data from multiple environments. When hard interventions are not available, we provide theory and algorithm for identification up to SNA, and also show that SNA is an intrinsic ambiguity in our setting. ", "page_idx": 9}, {"type": "text", "text": "It is interesting to further investigate the setting where we do not assume that the causal model is linear. Moreover, it is important to understand the concrete form of available interventions in real-world applications. For instance, it is suggested that for single-cell genomics, the intervention is sometimes a \"mixture\" of hard and soft interventions, and sometimes can even reverse the direction of an edge [43]. Modelling such more complicated interventions appears to be crucial to reveal the underlying causal mechanisms in real-world problems. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "VS is supported by NSF Award IIS-2337916 and a 2023 Google Research Scholar Award. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Kartik Ahuja, Divyat Mahajan, Yixin Wang, and Yoshua Bengio. Interventional causal representation learning. In International Conference on Machine Learning, pages 372\u2013407. PMLR, 2023.   \n[2] Kartik Ahuja, Amin Mansouri, and Yixin Wang. Multi-domain causal representation learning via weak distributional invariances. arXiv preprint arXiv:2310.02854, 2023.   \n[3] Naveed Akhtar and Ajmal Mian. Threat of adversarial attacks on deep learning in computer vision: A survey. Ieee Access, 6:14410\u201314430, 2018.   \n[4] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8): 1798\u20131828, 2013.   \n[5] Johann Brehmer, Pim De Haan, Phillip Lippe, and Taco S Cohen. Weakly supervised causal representation learning. Advances in Neural Information Processing Systems, 35:38319\u201338331, 2022.   \n[6] S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.   \n[7] Simon Buchholz, Goutham Rajendran, Elan Rosenfeld, Bryon Aragam, Bernhard Sch\u00f6lkopf, and Pradeep Ravikumar. Learning linear causal representations from interventions under general nonlinear mixing. arXiv preprint arXiv:2306.02235, 2023.   \n[8] John Campbell. An interventionist approach to causation in psychology. Causal learning: Psychology, philosophy, and computation, pages 58\u201366, 2007.   \n[9] Pierre Comon. Independent component analysis, a new concept? Signal processing, 36(3): 287\u2013314, 1994.   \n[10] Gregory F Cooper and Changwon Yoo. Causal discovery from a mixture of experimental and observational data. In Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence, pages 116\u2013125, 1999.   \n[11] Xinshuai Dong, Biwei Huang, Ignavier Ng, Xiangchen Song, Yujia Zheng, Songyao Jin, Roberto Legaspi, Peter Spirtes, and Kun Zhang. A versatile causal discovery framework to allow causally-related hidden variables. In The Twelfth International Conference on Learning Representations, 2023.   \n[12] Kevin N Dunbar and Jonathan A Fugelsang. Causal thinking in science: How scientists and students interpret the unexpected. In Scientific and technological thinking, pages 57\u201379. Psychology Press, 2004.   \n[13] Frederick Eberhardt. Almost optimal intervention sets for causal discovery. In Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence, pages 161\u2013168, 2008.   \n[14] Frederick Eberhardt. Direct causes and the trouble with soft interventions. Erkenntnis, 79: 755\u2013777, 2014.   \n[15] Markus I Eronen. Causal discovery and the problem of psychological interventions. New Ideas in Psychology, 59:100785, 2020.   \n[16] Ronald Aylmer Fisher et al. The design of experiments. The design of experiments., (7th Ed), 1960.   \n[17] Robert Geirhos, J\u00f6rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks. Nature Machine Intelligence, 2(11):665\u2013673, 2020.   \n[18] Alain Hauser and Peter B\u00fchlmann. Two optimal strategies for active learning of causal models from interventional data. International Journal of Approximate Reasoning, 55(4):926\u2013939, 2014.   \n[19] Keith J Holyoak and Patricia W Cheng. Causal learning and inference as a rational process: The new synthesis. Annual review of psychology, 62:135\u2013163, 2011.   \n[20] Antti Hyttinen, Frederick Eberhardt, and Patrik O Hoyer. Experiment selection for causal discovery. Journal of Machine Learning Research, 14:3041\u20133071, 2013.   \n[21] Ilyes Khemakhem, Diederik Kingma, Ricardo Monti, and Aapo Hyvarinen. Variational autoencoders and nonlinear ica: A unifying framework. In International Conference on Artificial Intelligence and Statistics, pages 2207\u20132217. PMLR, 2020.   \n[22] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of in-the-wild distribution shifts. In International Conference on Machine Learning, pages 5637\u20135664. PMLR, 2021.   \n[23] Wendong Liang, Armin Kekic\u00b4, Julius von K\u00fcgelgen, Simon Buchholz, Michel Besserve, Luigi Gresele, and Bernhard Sch\u00f6lkopf. Causal component analysis. arXiv preprint arXiv:2305.17225, 2023.   \n[24] Phillip Lippe, Sara Magliacane, Sindy L\u00f6we, Yuki M Asano, Taco Cohen, and Efstratios Gavves. Biscuit: Causal representation learning from binary interactions. arXiv preprint arXiv:2306.09643, 2023.   \n[25] Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard Sch\u00f6lkopf, and Olivier Bachem. Challenging common assumptions in the unsupervised learning of disentangled representations. In international conference on machine learning, pages 4114\u2013 4124. PMLR, 2019.   \n[26] Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar R\u00e4tsch, Sylvain Gelly, Bernhard Sch\u00f6lkopf, and Olivier Bachem. A sober look at the unsupervised learning of disentangled representations and their evaluation. The Journal of Machine Learning Research, 21(1):8629\u2013 8690, 2020.   \n[27] Francesco Locatello, Ben Poole, Gunnar R\u00e4tsch, Bernhard Sch\u00f6lkopf, Olivier Bachem, and Michael Tschannen. Weakly-supervised disentanglement without compromises. In International Conference on Machine Learning, pages 6348\u20136359. PMLR, 2020.   \n[28] Romain Lopez, Natasa Tagasovska, Stephen Ra, Kyunghyun Cho, Jonathan Pritchard, and Aviv Regev. Learning causal representations of single cells via sparse mechanism shift modeling. In Conference on Causal Learning and Reasoning, pages 662\u2013691. PMLR, 2023.   \n[29] Chaochao Lu, Yuhuai Wu, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, and Bernhard Sch\u00f6lkopf. Invariant causal representation learning for out-of-distribution generalization. In International Conference on Learning Representations, 2021.   \n[30] Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model\u2019s uncertainty? evaluating predictive uncertainty under dataset shift. Advances in neural information processing systems, 32, 2019.   \n[31] Judea Pearl. Causality. Cambridge university press, 2009.   \n[32] Geoffrey Roeder, Luke Metz, and Durk Kingma. On linear identifiability of learned representations. In International Conference on Machine Learning, pages 9030\u20139039. PMLR, 2021.   \n[33] Mark Rudelson and Roman Vershynin. Smallest singular value of a random rectangular matrix. Communications on Pure and Applied Mathematics: A Journal Issued by the Courant Institute of Mathematical Sciences, 62(12):1707\u20131739, 2009.   \n[34] Bernhard Sch\u00f6lkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner, Anirudh Goyal, and Yoshua Bengio. Toward causal representation learning. Proceedings of the IEEE, 109(5):612\u2013634, 2021.   \n[35] J Schwartz. The formula for change in variables in a multiple integral. The American Mathematical Monthly, 61(2):81\u201385, 1954.   \n[36] Anna Seigal, Chandler Squires, and Caroline Uhler. Linear causal disentanglement via interventions. arXiv preprint arXiv:2211.16467, 2022.   \n[37] David R Shanks and Anthony Dickinson. Associative accounts of causality judgment. In Psychology of learning and motivation, volume 21, pages 229\u2013261. Elsevier, 1988.   \n[38] Shohei Shimizu, Patrik O Hoyer, Aapo Hyv\u00e4rinen, Antti Kerminen, and Michael Jordan. A linear non-gaussian acyclic model for causal discovery. Journal of Machine Learning Research, 7(10), 2006.   \n[39] Ricardo Silva, Richard Scheines, Clark Glymour, Peter Spirtes, and David Maxwell Chickering. Learning the structure of linear latent variable models. Journal of Machine Learning Research, 7(2), 2006.   \n[40] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484\u2013489, 2016.   \n[41] Peter Spirtes, Clark N Glymour, and Richard Scheines. Causation, prediction, and search. 2000.   \n[42] Michael Strevens. Review of woodward,\" making things happen\", 2007.   \n[43] Alejandro Tejada-Lapuerta, Paul Bertin, Stefan Bauer, Hananeh Aliee, Yoshua Bengio, and Fabian J Theis. Causal machine learning for single-cell genomics. arXiv preprint arXiv:2310.14935, 2023.   \n[44] Robert E Tillman and Frederick Eberhardt. Learning causal structure from multiple datasets with similar variable sets. Behaviormetrika, 41(1):41\u201364, 2014.   \n[45] Simon Tong and Daphne Koller. Active learning for structure in bayesian networks. In International joint conference on artificial intelligence, volume 17, pages 863\u2013869. Citeseer, 2001.   \n[46] Burak Varici, Emre Acarturk, Karthikeyan Shanmugam, Abhishek Kumar, and Ali Tajer. Scorebased causal representation learning with interventions. arXiv preprint arXiv:2301.08230, 2023.   \n[47] Burak Var\u0131c\u0131, Emre Acart\u00fcrk, Karthikeyan Shanmugam, and Ali Tajer. General identifiability and achievability for causal representation learning. arXiv preprint arXiv:2310.15450, 2023.   \n[48] Julius Von K\u00fcgelgen, Yash Sharma, Luigi Gresele, Wieland Brendel, Bernhard Sch\u00f6lkopf, Michel Besserve, and Francesco Locatello. Self-supervised learning with data augmentations provably isolates content from style. Advances in neural information processing systems, 34: 16451\u201316467, 2021.   \n[49] Julius von K\u00fcgelgen, Michel Besserve, Wendong Liang, Luigi Gresele, Armin Keki\u00b4c, Elias Bareinboim, David M Blei, and Bernhard Sch\u00f6lkopf. Nonparametric identifiability of causal representations from unknown interventions. arXiv preprint arXiv:2306.00542, 2023.   \n[50] Tony Tong Wang, Adam Gleave, Tom Tseng, Kellin Pelrine, Nora Belrose, Joseph Miller, Michael D Dennis, Yawen Duan, Viktor Pogrebniak, Sergey Levine, et al. Adversarial policies beat superhuman go ais. 2023.   \n[51] James Woodward. Making things happen: A theory of causal explanation. Oxford university press, 2005.   \n[52] Feng Xie, Ruichu Cai, Biwei Huang, Clark Glymour, Zhifeng Hao, and Kun Zhang. Generalized independent noise condition for estimating latent variable causal graphs. Advances in neural information processing systems, 33:14891\u201314902, 2020.   \n[53] Feng Xie, Biwei Huang, Zhengming Chen, Yangbo He, Zhi Geng, and Kun Zhang. Identification of linear non-gaussian latent hierarchical structure. In International Conference on Machine Learning, pages 24370\u201324387. PMLR, 2022.   \n[54] Jiaqi Zhang, Chandler Squires, Kristjan Greenewald, Akash Srivastava, Karthikeyan Shanmugam, and Caroline Uhler. Identifiability guarantees for causal disentanglement from soft interventions. arXiv preprint arXiv:2307.06250, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Related works ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The interventionist approach to causation For the problem of causal graph discovery, it is wellknown that the underlying causal structure is non-identifiable given only \u201cpassively observed\u201d (equivalently, i.i.d.) data alone. As a result, randomized controlled experiments [16] is often used to infer causality. These experiments typically take the form of interventions [41, 31], i.e., manipulations on the \u201cnatural state\u201d of the system of interest. Early works [51, 42] define the \u201chard\u201d (also called \u201csurgical\u201d or \u201carrow-breaking\u201d) interventions in which the value of the intervened variable is entirely determined by the experimenter, thereby removing the dependence of this variable on its direct causes. This type of intervention is arguably the most natural one to consider, and following this definition, a line of works explore sufficient conditions for designing experiments that guarantee identifiability of the causal model in various settings [10, 45, 13, 20, 18]. ", "page_idx": 13}, {"type": "text", "text": "Intervention v.s. passive observation While extensive works demonstrate the success of the interventionist approach, it faces several key challenges that significantly limit its applicability. First, Eberhardt [14] finds that in the presence of unobserved variables, certain causal structures are indistinguishable if we only perform hard interventions. This issue can be resolved by performing soft interventions i.e., interventions that do not remove the dependency on direct causes but only changes the conditional distribution. Second, as pointed out in [44], interventions \u2014 whether hard or soft \u2014 are often expensive or even infeasible to perform in practice. For example, a psychological intervention is likely to affect multiple psychological variables simultaneously Eronen [15]. As a result, [44] returns to the \u201cpassive observation\u201d setting but with multiple datasets with overlapping latent variables. ", "page_idx": 13}, {"type": "text", "text": "Interventional causal representation learning Motivated by the interventionist literature in causal graph discovery, a recent line of works [1, 36, 46, 49, 7, 54, 47] consider performing interventions to resolve the non-identifiability issue in causal representation learning [25]. Roughly speaking, these result indicate that identification (possibly with some ambiguities) is possible if one can perform intervention on every latent variable. However, it is unclear how to perform such interventions in practice, given that the underlying latent variables are unknown. Khemakhem et al. [21], Lu et al. [29], Roeder et al. [32] do not require single-node interventions to achieve identifiability, but assumes that the joint distribution of latent variables in each environment lie in a certain exponential family. This assumption can be understood as a prior on the latent variables, but it is unclear when or why it is reasonable to make in reality. Recently, Ahuja et al. [2] considers learning causal representations from multiple domains that relate to each other via an invariance constraint on the subset $\\boldsymbol{S}$ of stable latent variables, and they prove identification up to affine mixtures within $\\boldsymbol{S}$ . ", "page_idx": 13}, {"type": "text", "text": "B Experiment details for Section 6 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1 Details for step 1 in Section 5 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Since $\\epsilon=B_{k}z=B_{k}H x$ in the $k$ -th environment, so we can use any identification algorithm for linear ICA to recover the matrix $M_{k}$ . Note that while standard linear ICA algorithms only apply to the case where $n=d$ , for $n>d$ we can arbitrarily choose $d$ principal components of $\\textbf{\\em x}$ to reduce it to the $n=d$ case. This is without loss of generality, since when $n>d$ there is redundant information in $\\textbf{\\em x}$ . ", "page_idx": 13}, {"type": "text", "text": "After recovering $M_{k}$ for each $k$ by running linear ICA, we still do not know whether each ${\\cal M}_{k}x$ corresponds to the same permutation of the ground-truth noise variables $\\epsilon$ . To resolve this issue, we choose test function $\\Psi$ mapping any distribution on $\\mathbb{R}$ to a deterministic real value, which we expect to take different values for different $\\epsilon_{i}$ \u2019s. We choose $\\Psi(\\mathbb{P})=\\mathbb{P}\\left[|X|\\leqslant1\\right]$ in our experiments. For all $k\\geqslant2$ , we calculate the $\\Psi$ value of each component of the $d$ -dimensional empirical distribution $\\begin{array}{r}{\\hat{\\mathbb P}_{k}=\\frac{1}{N}\\sum_{i=1}^{N}\\mathbb{1}_{M_{k}{\\pmb x}_{i}^{(k)}}}\\end{array}$ , and choose a permutation $\\pi_{k}$ to rearrange them in increasing order. Then, we rearrange the columns of $M_{k}$ using the same permutation $\\pi_{k}$ . This procedure would asymptotically produce correct alignments as long as $\\Psi(\\epsilon_{i}),i\\in[d]$ are different, and we find that it empirically works well. ", "page_idx": 13}, {"type": "text", "text": "Alternatively, this alignment step can be done as follows: for each pair of environments $(E_{1},E_{t})$ , and for each pair of nodes $(i,j)$ , we calculate the distribution distance between $\\epsilon_{i}$ in environment $E_{1}$ and $\\epsilon_{j}$ in environment $E_{t}$ , based on some notion of distribution distance (e.g. kernel maximum mean discrepancy). Then we find the min-cost perfect matching, where the cost of an edge is the distribution distance. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "B.2 Details for the implementation of LiNGCReL in the finite-sample regime ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Although LiNGCReL provably works in the population regime, it faces several challenges when there is only a finite number of samples: ", "page_idx": 14}, {"type": "text", "text": "\u2022 First, since rank is not a continuous function, it is sensitive to finite-sample estimation errors. In our implementation of Algorithm 3, in each iteration we instead choose $i\\not\\in S$ that has the largest ratio between the first and second singular values of $[q_{1},q_{2},\\cdots\\,,q_{K}]$ . And in line 6 of Algorithm 2, we introduce a hyper-parameter $\\tt t1$ such that the matrix $[\\pmb q_{1},\\pmb q_{2},\\cdots,\\pmb q_{K}]$ is considered to have rank $r_{m^{\\prime}-1}$ if its $r_{m^{\\prime}}$ -th singular value is smaller than $\\tt t1$ . Since \u221athe smallest singular value of a random matrix $\\pmb{A}\\in\\mathbb{R}^{K\\times m}(K\\geqslant m)$ is at the order of $\\sqrt{K}-\\sqrt{m-1}$ with high probability [33], when $K=d$ one shall choose $\\mathrm{tl}\\sim{\\sqrt{d}}-{\\sqrt{d-1}}={\\mathcal{O}}\\left({\\frac{1}{\\sqrt{d}}}\\right)$ . On the other hand, for larger $K$ we can correspondingly choose a larger tl. Note that a small $\\tt t1$ potentially has the risk of being dominated the noise in the estimation, which means that we need more samples per environment to reduce the noise. In contrast, for larger $\\tt t1$ the estimation is more robust to noise and we can use fewer samples.   \n\u2022 Second, finite-sample estimation errors of $M_{k}$ make it harder to obtain $h_{i}$ in Algorithm 3 of Algorithm 3. We implement this step in the following way: first let $Q_{j}$ be the orthogonal projection matrix onto $E_{j}^{\\perp}$ i.e., $Q_{j}^{\\top}\\pmb{x}=\\mathrm{proj}_{E_{j}^{\\bot}}(\\pmb{x})$ , then choose $h_{i}$ to be the singular vector of $\\sum_{j:(j,i)\\in\\mathcal{E}}$ or $\\it{{_{j=i}}\\,}\\it{{Q}_{j}^{\\top}}\\it{{Q}_{j}}$ that corresponds to the smallest singular value (including zero). Indeed, in the noiseless case we would have $\\begin{array}{r}{\\left(\\sum_{j:(j,i)\\in\\mathcal{E}\\mathrm{\\:or\\:}j=i}Q_{j}^{\\intercal}Q_{j}\\right)h_{i}=0}\\end{array}$ if and only if $h_{i}\\in\\left(\\cap_{j:(i,j)\\in\\mathcal{E}}E_{j}\\right)\\cap E_{i}$ . ", "page_idx": 14}, {"type": "text", "text": "C Further experiment results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "SNA error v.s. true error We plot the SNA error v.s. true error achieved by LiNGCReL in Figure 3. We observe that ", "page_idx": 14}, {"type": "text", "text": "\u2022 For most nodes, SNA error is exactly equal to the true error and both errors are small, indicating that the corresponding latent variables have been successfully learned by LiNGCReL. \u2022 The remaining nodes typically have true error much larger than SNA error. This indicates that there exists some ambiguities at these nodes in the sense that $\\operatorname{sur}_{\\mathcal{G}}(i)\\neq\\emptyset$ . Note that the true error for many nodes are close to 1; one possible reason is that one selects the wrong singular vector in the second part of Appendix B.2, so that it is orthogonal to the ground-truth vector. ", "page_idx": 14}, {"type": "text", "text": "Sensitivity of LiNGCReL to the hyperparameter tl We examine how different choices of $\\tt t1$ would affect the performance of LiNGCReL. Specifically, we run LiNGCReL on the 100 models with size $d=5$ and number of environments $K=5$ sampled in Section 6 with $\\mathsf{t1}\\in\\{0.1,0.15,0.2,0.25,0.3\\}$ and the results are reported in Figure 4. We can see that the permance is actually quite sensitive to tl. ", "page_idx": 14}, {"type": "image", "img_path": "dB99jjwx3h/tmp/a74ca88c127ae046cb10e22505008bd6a4c3412bd93520dc149e9906fae36aea.jpg", "img_caption": ["Figure 3: Comparing SNA error with true error for the 500 latent variables in the 100 graphs of size $d=5$ that we sample in Section 6. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "dB99jjwx3h/tmp/d9b8c46af1d87336d849769f20b773a5cb3666edf6e6e8ffe5a5f366a97cdd04.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 4: Performance of LiNGCReL as a function of tl. $\\mathtt{t1=0.15}$ achieves the best performance in terms of both SNA error and graph recovery accuracy. ", "page_idx": 15}, {"type": "text", "text": "D Background on causal representation learning ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "It is common to assume some axioms on what kind of (conditional) dependency information is encoded in a causal graph (see 41, Section 3.4 for a detailed discussion). The most natural one is the Causal Markov Condition introduced in Definition 1 that gives sufficient conditions for conditional independence via $d$ -separation. We introduce the formal definition of $d$ -separation below: ", "page_idx": 15}, {"type": "text", "text": "Definition 5 (paths and colliders). Let $i,j$ be two nodes of a DAG $\\mathcal{G}$ , a path is a sequence of nodes $i_{0}\\,=\\,i,i_{1},\\cdot\\cdot\\cdot\\,,i_{k}\\,=\\,j$ such that there is an edge (in either direction) between $i_{j}$ and $i_{j+1},j\\ =$ $0,1,\\cdot\\cdot\\cdot,k-1$ . A node $i_{j}$ is called a collider on this path if $i_{j}\\in\\mathrm{ch}_{\\mathcal{G}}(i_{j-1})\\cap\\mathrm{ch}_{\\mathcal{G}}(i_{j+1})$ . ", "page_idx": 15}, {"type": "text", "text": "Definition 6 (blocked path). A path in a DAG $\\mathcal{G}$ between node i and node $j$ is said to be blocked by $a$ node set $S$ if either of the following holds: ", "page_idx": 15}, {"type": "text", "text": "\u2022 there exists a node v on the path that is in $S$ but not a collider, or   \n\u2022 there exists a node v on the path that is a collider, but none of its descendants (including itself) are in $S$ . ", "page_idx": 15}, {"type": "text", "text": "Definition 7 (d-separation). For a DAG $\\mathcal{G}$ with node set $[d]$ , any two nodes $i\\neq j$ are said to be $d$ -separated by a set $S\\subset[d]\\setminus\\{i,j\\}$ if all paths from i to $j$ are blocked by $S$ . ", "page_idx": 15}, {"type": "text", "text": "The minimality condition states that there is no redundant edges in the causal graph, and is a natural consequence of the Occam\u2019s Razor Principle. ", "page_idx": 15}, {"type": "text", "text": "Assumption 6 (Causal minimality, 41, Section 3.4.2). For latent variables $_{z}$ , removing any edge from $\\mathcal{G}$ would render violation of the causal Markov condition Definition 1. In other words, let $\\mathcal{G}_{1}$ be the graph obtained by removing any single edge from $\\mathcal{G}$ , then there must exist $i\\in[d]$ such that $z_{i}$ \u0338\u22a5\u22a5 $z_{\\mathrm{nd}_{\\mathcal{G}_{1}}(i)}\\mid z_{\\mathrm{pa}_{\\mathcal{G}_{1}}(i)}$ . ", "page_idx": 15}, {"type": "text", "text": "The faithfulness condition states that the Causal Markov Condition actually entails all (conditional) independence in the latent variables. ", "page_idx": 16}, {"type": "text", "text": "Assumption 7 (Faithfulness, 41, Section 3.4.3). Every (conditional) independence in the latent variables $_{\\textit{z}}$ is entailed by the Causal Markov Condition applied to G. In other words, $z_{i}\\perp z_{j}\\mid$ $z_{S}\\Leftrightarrow i,j$ are $d$ -separated by $S$ . ", "page_idx": 16}, {"type": "text", "text": "Existing works have explored different notions of identifiability. For observational data, it is well known that Markov equivalence of graphs is an intrinsic ambiguity that one cannot resolve: ", "page_idx": 16}, {"type": "text", "text": "Definition 8 (Markov equivalence/Faithful Indistinguishability, 41, Section 4.2). If two DAGs encodes the same set of dependency relations, we say that they are Markov equivalent. ", "page_idx": 16}, {"type": "text", "text": "Any DAG $\\mathcal{G}$ induces a partial order on its nodes which we denote by $\\prec g$ . In the special case when for all $i\\neq j$ , either $i\\prec_{\\mathcal{G}}j$ or $j\\prec\\!g\\ i$ holds, we say that $\\prec g$ is a total order. This partial order is equivalent to the transitional closure of the graph, as defined below: ", "page_idx": 16}, {"type": "text", "text": "Definition 9 (Transitional closure). Given any $D A G\\,\\mathcal{G}$ , its transitional closure $\\bar{\\mathcal G}$ is defined to be the graph obtained by connecting all edges $i\\rightarrow j$ where i is an ancestor of $j$ in $\\mathcal{G}$ . ", "page_idx": 16}, {"type": "text", "text": "When $\\prec g$ is a total order, each pair of nodes are connected by a directed edge in its transitive closure $\\bar{\\mathcal G}$ . Such $\\bar{\\mathcal{G}}$ is often called a tournament in graph theory. ", "page_idx": 16}, {"type": "text", "text": "In the following, we list different forms of identifiability that appear in the literature: ", "page_idx": 16}, {"type": "text", "text": "Definition 10 (different notions of identifiability). Let $\\mathcal{H}\\,:\\,\\mathbb{R}^{n}\\,\\,\\supseteq\\,\\,\\mathcal{X}\\,\\,\\mapsto\\,\\,\\mathbb{R}^{d}$ be the space of diffeomorphic mappings from observation to latent, and G be the space of all DAGs with d nodes, then for $h,\\hat{h}\\in\\mathcal{H}$ and $\\mathcal{G},\\hat{\\mathcal{G}}\\in\\mathfrak{G}$ , we write ", "page_idx": 16}, {"type": "text", "text": "(i) [36, $23J\\left(h,\\mathcal{G}\\right)\\stackrel{T}{\\sim}_{G}\\left(\\hat{h},\\hat{\\mathcal{G}}\\right)$ if there exists a permutation $\\pi$ on $[d]$ such that $\\pi(\\mathcal{G})$ and $\\hat{\\mathcal G}$ have the same transitional closure; (ii) [49, 47] (h, G) \u223cCRL $(\\hat{h},\\hat{\\mathcal{G}})$ if we actually have $\\mathcal G=\\hat{\\mathcal G}$ for the $\\phi$ defined above. ", "page_idx": 16}, {"type": "text", "text": "Given an equivalence relation $\\sim$ on $\\mathcal{H}\\times\\mathfrak{G}$ , we say that a causal model $(h,\\mathcal{G})$ is identifiable under $\\sim$ if any candidate solution $(\\hat{h},\\hat{\\mathcal{G}})$ satisfies $(\\hat{h},\\hat{\\mathcal{G}})\\sim(h,\\mathcal{G})$ . The notion of identification up to $\\stackrel{T}{\\sim}_{G}$ , as shown in Seigal et al. [36] with single-node soft interventions on linear causal models, is highly related to this paper. Compared with their result, our $\\sim_{\\mathrm{sur}}$ guarantee is must stronger, since not only the causal graph can be fully recovered, but the latent variables can be identified up to mixtures of the effect-dominating sets as well. ", "page_idx": 16}, {"type": "text", "text": "E Illustrating examples for our theory and algorithm ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "E.1 An example for understanding the SNA ambiguity ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We provide a simple example below to illustrate the SNA ambiguity discussed in Section 3. ", "page_idx": 16}, {"type": "text", "text": "Example 1. Let $G$ be a causal graph with $d=3$ nodes and edges $1\\rightarrow2$ and $2\\rightarrow3,$ . We have access to observations from a set of environments E. It turns out that there is no way to distinguish between the following two structural equation models: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{z_{1}=\\epsilon_{1}^{E}}\\\\ &{z_{2}=f_{2}^{E}(z_{1},\\epsilon_{2}^{E})}\\\\ &{z_{3}=f_{3}^{E}(z_{2},\\epsilon_{3}^{E})}\\\\ &{x=z=(z_{1},z_{2},z_{3})^{\\top}}\\end{array}\\qquad\\qquad\\qquad\\begin{array}{r l}&{v_{1}=\\epsilon_{1}^{E}}\\\\ &{v_{2}=f_{2}^{E}(v_{1},\\epsilon_{2}^{E})}\\\\ &{v_{3}=v_{2}+f_{3}^{E}(v_{2},\\epsilon_{3}^{E})}\\\\ &{x=(v_{1},v_{2},v_{3}-v_{2})^{\\top}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\epsilon_{i}^{E},i=1,2,3$ are independent noise variables, if we do not change the causal graph $\\mathcal{G}$ , no matter what environment $E$ that we have. ", "page_idx": 16}, {"type": "text", "text": "This issue does not exist when we assume access to hard interventions on node 3, which effectively removes the edge $2\\rightarrow3$ . Specifically, with hard intervention on $z_{3}$ , the variables $z_{2}$ and $z_{3}$ become independent. But by definition, $v_{2}=z_{2}$ and ${\\pmb v}_{3}={\\pmb z}_{2}+{\\pmb z}_{3}$ must be dependent, so this intervention cannot be realized by any hard intervention on $\\pmb{v}_{3}$ , thereby providing a way to distinguish between the above models. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "Without node 3, the same ambiguity would arise on node 2. However, node 3 can help us to overcome this ambiguity, thanks to the fact that node 2 is the only causal parent of node 3. Suppose for example that $\\pmb{v}_{2}=m(z_{1},z_{2})$ is some mixture of $z_{1}$ and $z_{2}$ , then ${v_{3}}\\bar{=}\\hat{f}_{3}^{E}\\left(\\bar{v_{2}},\\epsilon_{3}^{E}\\right)=\\hat{f}_{3}^{\\bar{E}}\\left(m(z_{1},z_{2}),\\bar{\\epsilon_{3}^{E}}\\right)$ . Since all environments share the same mixing function, $\\pmb{v}_{3}$ must be some deterministic function $\\psi_{3}(z)$ of $_{z}$ , where $\\psi_{3}$ is the same across all environment $E$ . Hence, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\hat{f}_{3}^{E}\\left(m(z_{1},z_{2}),\\epsilon_{3}^{E}\\right)=\\psi_{3}\\left(z_{1},z_{2},f_{3}^{E}(z_{2},\\epsilon_{3}^{E})\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Now we note that the dependencies of $L H S$ on $\\boldsymbol{z}_{1}$ and $z_{2}$ are through a single scalar-valued function $m$ , but since we would have different $f_{3}^{E}$ \u2019s in different environments, this in general does not hold for the RHS. Therefore, any causal model with latent variable $\\pmb{v}_{2}$ as a mixture of $z_{1}$ and $z_{2}$ cannot be equivalent to the ground-truth model. ", "page_idx": 17}, {"type": "text", "text": "According to Definition 3, in Example 1 we have $\\mathrm{sur}_{\\mathcal{G}}(1)=\\mathrm{sur}_{\\mathcal{G}}(2)=\\varnothing$ but $\\operatorname{sur}_{\\mathcal{G}}(3)=\\{2\\}$ . ", "page_idx": 17}, {"type": "text", "text": "E.2 An example for the main idea behind LiNGCReL ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "To illustrate our main algorithm on how we can recover the graph $\\mathcal{G}$ and the matrix $\\pmb{H}$ , we first provide some intuition using a simple three-node example: ", "page_idx": 17}, {"type": "text", "text": "Example 2. Let $\\mathcal{G}$ be the graph with $d=3$ nodes and edges $1\\rightarrow2,1\\rightarrow3$ and $2\\rightarrow3$ , so that each $B_{k}$ is of form ", "page_idx": 17}, {"type": "equation", "text": "$$\nB_{k}=\\left(\\begin{array}{c c c c}{{\\times}}&{{0}}&{{}}&{{0}}\\\\ {{\\times}}&{{\\times}}&{{}}&{{0}}\\\\ {{\\times}}&{{\\times}}&{{}}&{{\\times}}\\end{array}\\right)\\begin{array}{c}{{\\leftrightarrow b_{k1}}}\\\\ {{\\leftrightarrow b_{k2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We can identify the graph as follows: first, for $i\\in\\{1,2,3\\}$ , look at the space $W_{i}$ spanned by the rows $(M_{k})_{i},k\\in[K]$ . If dim $W_{i}=1$ , we know that $i$ is a source node (i.e., $\\mathrm{pa}_{\\mathcal{G}}(i)=\\varnothing,$ in $\\mathcal{G}$ . Otherwise $i t$ is not, due to Assumption 5. Hence we can know that node 1 is a source node. ", "page_idx": 17}, {"type": "text", "text": "In our example, there is no other node that satisfies this requirement. We then proceed to search for some $i\\neq1$ such that the projection of $W_{i}$ onto $W_{1}^{\\perp}$ has dimension 1. If this holds, then one can show that $\\mathrm{pa}_{\\mathcal{G}}(i)=\\{1\\}$ . Otherwise, $i$ must have parents other than 1. ", "page_idx": 17}, {"type": "text", "text": "It turns this requirement is satisfied for node 2 since $\\mathrm{dim}\\left(\\mathrm{proj}_{h_{1}}\\mathrm{span}\\left\\langle h_{1},h_{2}\\right\\rangle\\right)\\,=\\,1$ , but is not satisfied for node 3 since $\\dim\\left(\\mathrm{proj}_{h_{1}}\\mathrm{span}\\,\\langle h_{1},h_{2},h_{3}\\rangle\\right)\\geqslant2$ (by Lemma 4). Hence we know that $\\mathrm{{pa}}_{\\mathcal{G}}(2)=\\{1\\}$ . ", "page_idx": 17}, {"type": "text", "text": "Finally, it remains to determine $\\mathrm{pa}_{\\mathcal{G}}(3)$ . To do this, we first note that dim $W_{3}=3$ . Then we project $W_{3}$ onto $W_{1}^{\\perp}$ and $W_{2}^{\\perp}$ respectively, and the resulting dimensions are 2 and 1. As we rigorously show in Proposition 2, a decrease of the dimension exactly indicates finding a new parent. Thus we have $\\mathrm{pa}_{\\mathcal{G}}(3)=\\{1,2\\}$ , completing the recovery of the graph. ", "page_idx": 17}, {"type": "text", "text": "Finally, we recover the unmixing matrix $\\pmb{H}$ (and thus the latent variables) by noticing that $h_{1}\\in W_{1}$ , $h_{2}\\in W_{2}\\cap W_{3}$ and $h_{3}\\in W_{3}$ . Ambiguities would arise at nodes 2 and 3, which are exactly the nodes that have non-empty effect-dominating sets. ", "page_idx": 17}, {"type": "text", "text": "F Auxiliary lemmas ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Lemma 2. For any family of $m$ -dimensional vectors $\\{{\\pmb v}_{k}\\}_{k=1}^{K}$ and $\\{z_{k}\\}_{k=1}^{K}\\ i f\\,{\\pmb v}_{k}\\,=\\,z_{k}T$ and $\\pmb{T}\\in\\mathbb{R}^{m\\times m}$ is invertible, then ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\dim\\operatorname{span}\\left\\langle v_{k}:k\\in[K]\\right\\rangle=\\dim\\operatorname{span}\\left\\langle z_{k}:k\\in[K]\\right\\rangle\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Theorem 5 (Darmois-Skitovic Theorem). Let $\\epsilon_{i},i\\in[d]$ be independent random variables and $\\begin{array}{r}{X\\,=\\,\\sum_{i=1}^{d}\\alpha_{i}\\epsilon_{i},Y\\,=\\,\\sum_{i=1}^{d}\\beta_{i}\\epsilon_{i}}\\end{array}$ . If $X\\,\\perp Y\\,$ , then for $\\forall i\\in[d]$ , $\\alpha_{i}\\beta_{i}\\,\\neq\\,0\\,\\Rightarrow\\,\\epsilon_{i}$ is Gaussian ", "page_idx": 17}, {"type": "text", "text": "Lemma 3. Suppose that $\\boldsymbol{\\epsilon}\\,=\\,(\\epsilon_{1},\\cdots\\,,\\epsilon_{d})$ is a $d$ -dimensional random vector with independent components such that $\\mathrm{Var}(\\epsilon_{i})=1,\\forall i\\in[d]$ , and there exists an invertible and non-diagonal matrix $_M$ such that $M\\epsilon\\stackrel{d}{=}\\epsilon$ , then at least one of the following statements must hold: ", "page_idx": 17}, {"type": "text", "text": "(1) there exists at least two Gaussian variables in $\\epsilon_{1},\\cdot\\cdot\\cdot\\,,\\epsilon_{d}.$ ; ", "page_idx": 18}, {"type": "text", "text": "(2) $_M$ is a permutation matrix and there exists $1\\leqslant i<j\\leqslant d$ such that $\\epsilon_{i}\\overset{d}{=}\\epsilon_{j}$ . ", "page_idx": 18}, {"type": "text", "text": "Proof. Suppose that (1) does not hold, then there is at most one Gaussian variable in $\\epsilon_{1},\\cdot\\cdot\\cdot\\,,\\epsilon_{d}$ . We assume WLOG that $\\epsilon_{1},\\cdot\\cdot\\cdot\\,,\\epsilon_{d-1}$ are all non-Gaussian. Then by the Darmois-Skitovic Theorem, we know that for $\\forall1\\leqslant j<k\\leqslant[d]$ and $i\\in[d-1]$ , $M_{j i}\\cdot M_{k i}=0\\Rightarrow$ there is at most one non-zero entry in each of the first $d-1$ columns of $_M$ . ", "page_idx": 18}, {"type": "text", "text": "Assume that $M_{k_{i},i}\\ \\neq\\ 0,\\ i\\ \\in\\ [d\\mathrm{~-~}1]$ . Since $_M$ is invertible, we know that $k_{i},i~\\in~[d\\mathrm{~-~}1]$ must be different. Let $k_{d}$ be the remaining element in $[d]$ that does not appear in $k_{i},i<d$ , then $(M\\epsilon)_{k_{d}}\\;=\\;M_{k_{d},d}\\epsilon_{d}$ , while $(M\\epsilon)_{k_{i}}\\:=\\:M_{k_{i},i}\\epsilon_{i}\\:+\\:M_{k_{i},d}^{\\bar{}}\\epsilon_{d}$ . Since the components of $M\\epsilon$ are independent, it is easy to see that $M_{i d}\\neq0,\\forall i\\neq k_{d}$ . In other words, $_M$ only has non-zero entries at $(k_{i},i),i\\in[d]$ . ", "page_idx": 18}, {"type": "text", "text": "Since $\\mathrm{Var}(\\epsilon_{i})\\;=\\;1$ , we know that $_M$ must be a signed permutation matrix. Finally, let $\\pi$ be the permutation on $[d]$ such that $M_{i,\\pi(i)}\\,\\ne\\,0$ . Since $_M$ is not diagonal, $\\pi$ must have a cycle $(i_{1},i_{2},\\cdot\\cdot\\cdot\\,,i_{k})$ with length $k\\geqslant2$ , so that $\\epsilon_{i_{1}},\\cdot\\cdot\\cdot\\mathrm{~,~}\\epsilon_{i_{k}}$ all have the same distribution, which implies that (2) holds, as desired. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "Lemma 4. Let $V_{1},V_{2}$ be two subspaces of $\\mathbb{R}^{d}$ such that $V_{1}\\cap V_{2}=\\{\\mathbf{0}\\}$ , and $P_{V_{1}^{\\perp}}$ be the orthogonal projection onto $V_{1}^{\\perp}$ , then we have that $\\dim(V_{2})=\\dim\\left(P_{V_{1}^{\\bot}}V_{2}\\right)$ . ", "page_idx": 18}, {"type": "text", "text": "Proof. Obviously we have $\\dim(V_{2})\\,\\geqslant\\,\\dim\\left(P_{V_{1}^{\\bot}}V_{2}\\right)$ . On the other hand, let ${\\pmb u}_{1},{\\pmb u}_{2},\\cdot\\cdot\\cdot\\mathrm{~,~}{\\pmb u}_{m}$ be a basis of $V_{2}$ , then $\\pmb{w}_{i}\\;=\\;P_{V_{1}^{\\bot}}\\pmb{u}_{i},i\\;=\\;\\stackrel{\\cdot}{1},2,\\cdots\\;,\\stackrel{\\cdot}{m}$ are also independent. Indeed, suppose that $\\lambda_{i},i\\,\\,=\\,\\,1,2,\\cdots\\,,m$ satisfy $\\dot{\\sum}_{i=1}^{m}\\,\\lambda_{i}{\\pmb w}_{i}\\;=\\;0$ , then $\\begin{array}{r}{P_{V_{1}^{\\perp}}\\left(\\sum_{i=1}^{m}\\lambda_{i}\\pmb{u}_{i}\\right)\\;=\\;0}\\end{array}$ , implying that $\\textstyle\\sum_{i=1}^{m}\\lambda_{i}\\mathbf{u}_{i}\\in V_{1}$ . However, we know that $V_{1}\\cap V_{2}=\\{\\mathbf{0}\\}$ , so $\\lambda_{1}=\\cdot\\cdot\\cdot=\\lambda_{m}=0$ . This concludes the proof. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "Lemma 5. Assumption 4 is equivalent to Assumption 5. ", "page_idx": 18}, {"type": "text", "text": "Proof. The main observation is that for each $k\\in[K]$ , $(\\boldsymbol{B}_{k})_{i}$ only has non-zero entries at the $j$ -th coordinate where $j\\in\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}(i)$ . Moreover, let $\\tilde{w}_{k}(i)$ be the vector consisting of these entries, then $\\tilde{\\pmb{w}}_{k}(i)=(\\Omega_{k})_{i i}^{-\\frac{1}{2}}\\left(-\\pmb{w}_{k}(\\dot{i}),1\\right)$ . Hence, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{dim\\,span}\\left\\langle(B_{k})_{i}:k\\in[K]\\right\\rangle=\\mathrm{dim\\,span}\\left\\langle(-w_{k}(i),1):k\\in[K]\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Suppose that Assumption 4 holds, then for $\\forall\\pmb{x}\\in\\mathbb{R}^{\\left|\\mathrm{pa}_{\\mathcal{G}}(i)\\right|}$ , there exists $\\lambda_{k}\\in\\mathbb{R},1\\leqslant k\\leqslant\\left|\\mathrm{pa}_{\\mathcal{G}}(i)\\right|$ such that $\\textstyle\\sum_{k}\\lambda_{k}=1$ and $\\begin{array}{r}{\\sum_{k}\\lambda_{k}\\pmb{w}_{k}(i)=\\pmb{x}}\\end{array}$ . Hence, ", "page_idx": 18}, {"type": "equation", "text": "$$\n(x,1)=\\sum_{k}\\lambda_{k}{\\tilde{w}}_{k}(i)\\in\\mathrm{span}\\left\\langle(B_{k})_{i}:k\\in[K]\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This immediately implies that span $\\langle(B_{k})_{i}:k\\in[K]\\rangle=\\mathbb{R}^{\\left|\\mathrm{pa}_{\\mathcal{G}}(i)\\right|+1}$ , so that Assumption 5 holds. ", "page_idx": 18}, {"type": "text", "text": "Conversely, suppose that Assumption 5 holds, then for $\\forall\\pmb{x}\\in\\mathbb{R}^{\\left|\\mathrm{pa}_{\\mathcal{G}}(i)\\right|}$ , there exists $\\lambda_{k}\\in\\mathbb{R},1\\leqslant$ $k\\leqslant\\left|\\mathrm{pa}_{\\mathcal{G}}(i)\\right|$ such that $\\textstyle\\sum_{k}\\lambda_{k}{\\tilde{\\pmb w_{k}}}(i)=(\\pmb{x},1)$ . Hence we have $\\begin{array}{r}{\\sum_{k}\\lambda_{k}\\pmb{w}_{k}(i)=\\pmb{x}}\\end{array}$ and $\\textstyle\\sum_{k}\\lambda_{k}=1$ implying Assumption 4 . \u53e3 ", "page_idx": 18}, {"type": "text", "text": "G Properties of effect-domination sets ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Lemma 6. \u2022 $j\\in\\mathrm{sur}_{\\mathscr{G}}(i)$ if and only i $f\\overline{{\\mathrm{ch}}}_{\\mathcal{G}}(i)\\subseteq\\mathrm{ch}_{\\mathcal{G}}(j),$ ; ", "page_idx": 18}, {"type": "text", "text": "\u2022 when $i\\neq j$ , $j\\in\\operatorname{sur}_{\\mathcal{G}}(i)$ if and only if ${\\overline{{\\operatorname{ch}}}}_{\\mathcal{G}}(i)\\subseteq{\\overline{{\\operatorname{ch}}}}_{\\mathcal{G}}(j).$ ", "page_idx": 18}, {"type": "text", "text": "Proof. If $j\\,\\in\\,\\mathrm{sur}_{\\mathcal{G}}(i)$ , by definition $i\\,\\in\\,\\mathrm{ch}_{\\mathcal{G}}(j)$ and $\\mathrm{ch}_{\\mathcal{G}}(i)\\,\\subseteq\\,\\mathrm{ch}_{\\mathcal{G}}(j)$ , so that ${\\overline{{\\operatorname{ch}}}}_{\\mathcal{G}}(i)\\subseteq\\,\\operatorname{ch}_{\\mathcal{G}}(j)$ . Conversely, $\\overline{{\\mathrm{ch}}}_{\\mathcal{G}}(i)\\subseteq\\mathrm{ch}_{\\mathcal{G}}(j)$ implies that $i\\in\\mathrm{ch}_{\\mathscr{G}}(j)$ and $\\mathrm{ch}_{\\mathcal{G}}(i)\\subseteq\\mathrm{ch}_{\\mathcal{G}}(j)$ , so $j\\,\\in\\,\\mathrm{sur}_{\\mathcal{G}}(i)$ . This proves the first claim. ", "page_idx": 18}, {"type": "text", "text": "To prove the second claim, assume that $\\overline{{\\mathrm{ch}}}_{\\mathcal{G}}(i)\\subseteq\\overline{{\\mathrm{ch}}}_{\\mathcal{G}}(j)$ holds but $\\overline{{\\mathrm{ch}}}_{\\mathcal{G}}(i)\\subseteq\\mathrm{ch}_{\\mathcal{G}}(j)$ does not hold, then we must have $j\\in\\overline{{\\mathrm{ch}}}_{\\mathcal{G}}(i)$ . since $j\\neq i$ , we have $j\\in\\mathrm{ch}_{\\mathscr{G}}(i)$ , but then $i\\not\\in\\overline{{\\cosh}}_{\\mathcal{G}}(j)$ , which is a contradiction. Hence $\\overline{{\\mathrm{ch}}}_{\\mathcal{G}}(i)\\subseteq\\mathrm{ch}_{\\mathcal{G}}(j)$ and the conclusion follows from the first claim. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "Lemma 7. Let $\\mathcal{G}$ be a DAG and i be its node, then for $\\forall j\\in\\mathrm{pa}_{\\mathcal{G}}(i)$ , we have $\\operatorname{sur}_{\\mathcal{G}}(j)\\subseteq\\operatorname{pa}_{\\mathcal{G}}(i).$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. Let $k\\;\\in\\;\\mathrm{sur}_{\\mathcal{G}}(j)$ , then by definition we have $\\mathrm{ch}_{\\mathcal{G}}(j)\\,\\subseteq\\,\\mathrm{ch}_{\\mathcal{G}}(k)$ . In particular, we have $i\\in\\mathrm{ch}_{\\mathcal{G}}(k)\\Rightarrow k\\in\\mathrm{pa}_{\\mathcal{G}}(i)$ . \u53e3 ", "page_idx": 19}, {"type": "text", "text": "Lemma 8. Let $\\mathcal{G}$ be a DAG and i be its node, then for $\\forall j\\in\\operatorname{sur}_{\\mathcal{G}}(i)$ , we have $\\operatorname{sur}_{\\mathcal{G}}(j)\\subseteq\\operatorname{sur}_{\\mathcal{G}}(i)$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. Let $k\\in\\mathrm{sur}_{\\mathcal{G}}(j)$ , then by definition we have $\\overline{{\\operatorname{ch}}}_{\\mathcal{G}}(j)\\subset\\overline{{\\operatorname{ch}}}_{\\mathcal{G}}(k)$ . We also know that $\\overline{{\\mathrm{ch}}}_{\\mathcal{G}}(i)\\subset$ $\\overline{{\\operatorname{ch}}}_{\\mathcal{G}}(j)$ , so $\\overline{{\\operatorname{ch}}}_{\\mathcal{G}}(i)\\subset\\overline{{\\operatorname{ch}}}_{\\mathcal{G}}(k)$ , implying that $k\\in\\mathrm{sur}_{\\mathcal{G}}(i)$ . \u53e3 ", "page_idx": 19}, {"type": "text", "text": "Lemma 9. If $M\\in\\mathcal{M}_{\\mathrm{sur}}^{0}(\\mathcal{G})$ , then $M^{-1}\\in\\mathcal{M}_{\\operatorname{sur}}^{0}(\\mathcal{G})$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. Assume WLOG that the nodes of $\\mathcal{G}$ satisfy $i\\in\\mathrm{{pa}}_{\\mathcal{G}}(j)\\Rightarrow i<j$ (otherwise we can choose a different index of the nodes and correspondingly swap some rows and columns of $_M$ ). Since $i\\in\\operatorname{sur}_{\\mathcal{G}}(j)\\Rightarrow i\\in\\operatorname{pa}_{\\mathcal{G}}(j)$ , it follows that $_M$ must be lower triangular and the diagonal entries are nonzero. ", "page_idx": 19}, {"type": "text", "text": "Let $N=M^{-1}$ , then for $\\forall i\\in[d]$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{j=1}^{d}N_{i j}M_{j\\ell}=0,\\quad\\forall\\ell\\notin\\overline{{\\operatorname{sur}}}_{\\mathcal{G}}(i).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since $M\\in\\mathcal{M}_{\\mathrm{sur}}^{0}(\\mathcal{G})$ , we have $M_{j\\ell}=0$ for $\\forall j$ such that $\\ell\\not\\in\\overline{{\\mathrm{sur}}}_{\\mathcal{G}}(j)$ . By Lemma 8, if $j\\in\\overline{{\\mathrm{sur}}}_{\\mathcal{G}}(i)$ , then $\\ell\\not\\in\\overline{{\\mathrm{sur}}}_{\\mathcal{G}}(i)$ necessarily implies that $\\ell\\not\\in\\overline{{\\operatorname{sur}}}_{\\mathcal{G}}(j)$ . Hence the left hand side of (9) is essentially a sum over $j\\not\\in\\dot{\\mathrm{sur}}_{\\mathcal{G}}(i)$ , i.e., ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{j\\notin\\overline{{\\operatorname{sur}}}_{\\mathcal{G}}(i)}N_{i j}M_{j\\ell}=0,\\quad\\forall\\ell\\notin\\overline{{\\operatorname{sur}}}_{\\mathcal{G}}(i).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Viewing the above as a system of linear equations in $N_{i j},j\\ \\notin\\ \\overline{{\\operatorname{sur}}}_{\\mathcal{G}}(i)$ , the coefficient matrix $\\left(M_{j\\ell}\\right)_{j,\\ell\\in\\notin\\overline{{\\operatorname{sur}}}_{\\mathcal{G}}(i)}$ must be invertible since it is a sub-matrix of the invertible lower-triangular matrix $_M$ . As a result, we necessary have $\\mathbf{}N_{i j}=0,\\forall j\\notin\\overline{{\\mathrm{sur}}}_{\\mathcal{G}}(i)$ . Finally, $N=M^{-1}$ must be invertible, so $N\\in\\mathcal{M}_{\\mathrm{sur}}^{0}(\\mathcal{G})$ as desired. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "Lemma 10. Suppose that $\\psi:\\mathbb{R}^{d}\\mapsto\\mathbb{R}^{d}$ is a diffeomorphism and $\\mathcal{G}$ be a DAG, such that for $\\forall i\\in[d]$ , $\\psi_{i}(z)$ is a function of $z_{\\overline{{\\mathrm{sur}}}_{\\mathcal{G}}(i)}$ . Then for $\\forall j\\in[d]$ , $\\bar{(\\psi^{-1})}_{j}(\\pmb{v})$ is a function of $\\pmb{v}_{\\mathrm{sur}_{\\mathcal{G}}(j)}$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. Let $\\pmb{J}_{z}=\\pmb{J}_{\\psi}(z)$ be the Jacobian matrix of $\\psi$ . Since $\\psi$ is a diffeomorphism, $J_{z}$ is invertible for any $z\\in\\mathbb{R}^{d}$ . Moreover, our assumption implies that $(J_{z})_{i j}=0,\\forall j\\notin\\overline{{\\mathrm{sur}}}_{\\mathcal{G}}(i)$ , so $J_{z}\\in\\mathcal{M}_{\\mathrm{sur}}^{0}(\\mathcal{G})$ . By Lemma 9, $J_{z}^{-1}\\in\\mathcal{M}_{\\mathrm{sur}}^{0}(\\mathcal{G})$ . But $J_{z}^{-1}$ is exactly the Jacobian matrix of $\\psi^{-1}$ at ${\\pmb v}=\\psi({\\pmb z})$ , hence it follows that $(\\psi^{-1})_{j}(\\pmb{v})$ is only a function of $\\pmb{v}_{\\mathrm{sur}_{\\mathcal{G}}(j)}$ , as desired. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "Lemma 11. The binary relation $\\sim_{\\mathrm{sur}}$ defined in Definition $^{4}$ is an equivalence relation. ", "page_idx": 19}, {"type": "text", "text": "Proof. It is obvious that $(h,\\mathcal{G})\\sim_{\\mathrm{sur}}\\left(h,\\mathcal{G}\\right)$ holds for any model $(h,\\mathcal{G})$ . ", "page_idx": 19}, {"type": "text", "text": "Suppose that $(h_{1},\\mathcal{G}_{1})\\sim_{\\mathrm{sur}}\\left(h_{2},\\mathcal{G}_{2}\\right)$ , then there exists a permutation $\\pi$ on $[d]$ and a diffeomorphism $\\psi:\\mathbb{R}^{d}\\mapsto\\mathbb{R}^{d}$ where $\\psi_{i}(z)$ is a function of $z_{\\overline{{\\mathrm{sur}}}_{\\mathcal{G}_{1}}(i)}$ , such that $i\\in\\mathrm{{pa}}_{\\mathcal{G}_{1}}(j)\\Leftrightarrow\\pi(i)\\in\\mathrm{{pa}}_{\\mathcal{G}_{2}}(\\pi(j))$ and $P_{\\pi}\\circ h_{2}\\,=\\,\\psi\\circ h_{1}$ . Then we can write $P_{\\pi}^{\\tilde{-1}}\\circ h_{1}\\,=\\,\\hat{\\psi}\\circ h_{2}$ where $\\hat{\\psi}\\,=\\,P_{\\pi}^{-1}\\circ\\psi^{-1}\\circ P_{\\pi}$ . By Lemma 10, we know that $\\left(\\psi^{-1}\\right)_{j}(\\boldsymbol{v})$ is a function of $\\pmb{v}_{\\overline{{\\mathrm{sur}}}_{\\mathcal{G}_{1}}(j)}$ , so $(\\hat{\\psi})_{j}$ is a function of $\\pmb{v}_{\\pi(\\overline{{\\operatorname{sur}}}\\mathscr{G}_{1}(j))}=\\pmb{v}_{\\overline{{\\operatorname{sur}}}\\mathscr{G}_{2}(j)}$ , implying that $\\stackrel{\\cdot}{(h_{2},\\mathcal{G}_{2})}\\sim_{\\mathrm{sur}}\\left(h_{1},\\mathcal{G}_{1}\\right)$ . ", "page_idx": 19}, {"type": "text", "text": "Finally, let $(h_{1},\\mathcal{G}_{1})\\sim_{\\mathrm{sur}}\\left(h_{2},\\mathcal{G}_{2}\\right)$ and $(h_{2},\\mathcal{G}_{2})\\sim_{\\mathrm{sur}}\\left(h_{3},\\mathcal{G}_{3}\\right)$ , then we can write ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\pmb{{\\cal P}}_{\\pi}\\circ\\pmb{{h}}_{2}=\\psi\\circ\\pmb{{h}}_{1}\\quad\\mathrm{and}\\quad\\pmb{{\\cal P}}_{\\hat{\\pi}}\\circ\\pmb{{h}}_{3}=\\hat{\\psi}\\circ\\pmb{{h}}_{2}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where: for $\\forall i\\in[d],\\psi_{i}(z)$ is a function of $z_{\\overline{{\\mathrm{sur}}}\\varsigma_{1}}(i)$ , $\\hat{\\psi}_{i}(z)$ is a function of $z_{\\overline{{\\mathrm{sur}}}\\mathcal{G}_{2}}(i)$ , $i\\in\\mathrm{pa}_{\\mathcal{G}_{1}}(j)\\Leftrightarrow$ $\\pi(i)\\in\\mathrm{pa}_{\\mathcal{G}_{2}}(\\pi(j))$ and $i\\in\\mathrm{{pa}}_{\\mathcal{G}_{2}}(j)\\Leftrightarrow\\hat{\\pi}(i)\\in\\bar{\\mathrm{{pa}}}_{\\mathcal{G}_{2}}(\\hat{\\pi}(j))$ . Then, we can write ", "page_idx": 20}, {"type": "equation", "text": "$$\nP_{\\pi}\\circ P_{\\hat{\\pi}}\\circ h_{3}=P_{\\pi}\\circ\\hat{\\psi}\\circ P_{\\pi}^{-1}\\circ\\psi\\circ h_{1}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since $\\hat{\\psi}_{i}(z)$ is a function of $z_{\\overline{{\\mathrm{sur}}}_{\\mathcal{G}_{2}}(i)}$ , we deduce that $\\left(P_{\\pi}\\circ\\hat{\\psi}\\circ P_{\\pi}^{-1}\\right)_{i}(z)$ is a function of $z_{\\overline{{\\mathrm{sur}}}_{\\mathcal{G}_{1}}(i)}$ . Hence, $\\left(\\boldsymbol{P}_{\\pi}\\circ\\hat{\\psi}\\circ\\boldsymbol{P}_{\\pi}^{-1}\\circ\\boldsymbol{\\psi}\\right)_{i}(z)=\\left(\\boldsymbol{P}_{\\pi}\\circ\\hat{\\psi}\\circ\\boldsymbol{P}_{\\pi}^{-1}\\right)_{i}(\\boldsymbol{\\psi}(z))$ is a function of $\\psi_{\\overline{{\\mathrm{sur}}}_{\\mathcal{G}_{1}}(i)}(z)$ . The definition of $\\psi$ implies that for each $j\\in\\overline{{\\operatorname{sur}}}_{\\mathcal{G}_{1}}(i),\\,\\psi_{j}(\\overline{{\\boldsymbol{z}}})$ is a function of $z_{\\overline{{\\mathrm{sur}}}_{\\mathcal{G}_{1}}(j)}$ . By Lemma 8, we have $\\cup_{j\\in\\overline{{\\operatorname{sur}}}\\mathscr{G}_{1}}(i)\\overline{{\\operatorname{sur}}}_{\\mathscr{G}_{1}}(j)\\ \\subseteq\\ \\overline{{\\operatorname{sur}}}_{\\mathscr{G}_{1}}(i)$ . Hence $\\left({\\cal P}_{\\pi}\\circ\\hat{\\psi}\\circ{\\cal P}_{\\pi}^{-1}\\circ\\psi\\right)_{i}(z)$ is still a function of $z_{\\overline{{\\mathrm{sur}}}\\mathcal{G}_{1}}(i)$ . Moreover, we also have $i\\in\\mathrm{pa}_{\\mathcal{G}_{1}}(j)\\Leftrightarrow\\pi(i)\\in\\mathrm{pa}_{\\mathcal{G}_{2}}(\\pi(j))\\Leftrightarrow\\dot{\\pi}\\circ\\pi(i)\\in\\mathrm{pa}_{\\mathcal{G}_{2}}(\\hat{\\pi}\\circ\\pi(j))$ , so by definition, $(h_{1},\\mathcal{G}_{1})\\sim_{\\mathrm{sur}}\\left(h_{3},\\mathcal{G}_{3}\\right)$ , as desired. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "H Omitted proofs from Section 4 and Section 5 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "H.1 Proof of Theorem 1 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "According to the assumption, we have that $\\epsilon\\;\\;=\\;\\;B_{k}H x$ and $\\hat{\\epsilon}\\;\\;=\\;\\;\\hat{B}_{k}\\hat{H}x$ , so that $\\epsilon=$ $B_{k}H(\\hat{B}_{k}\\hat{H})^{\\dagger}\\hat{\\epsilon},\\forall k\\;\\in\\;[K]$ . By Lemma 3, we know that for each $k$ $,\\,P_{k}\\,:=\\,B_{k}H(\\hat{B}_{k}\\hat{H})^{\\dag}$ is $^d$ a signed permutation matrix, so that $\\epsilon\\:=\\:P_{k}\\hat{\\epsilon}$ . Since for any $i\\;\\neq\\;j,\\;\\hat{\\epsilon}_{i}\\;\\neq\\;\\hat{\\epsilon}_{j}$ , we must have $|P|_{1}^{-}=|\\bar{P}|_{2}=\\cdot\\cdot\\cdot=|P|_{K}=:P$ and $\\epsilon=P\\hat{\\epsilon}$ , where $|M|$ denotes the resulting matrix by taking the absolute value of all entries in $_M$ . Thus, we can WLOG assume that $\\epsilon=\\hat{\\epsilon}$ , since otherwise we can permute the noise variables $\\hat{\\epsilon}$ , and also permute the rows of $B_{k}$ correspondingly. In other words, suppose that the permutation matrix $|P|$ has $|P|_{k_{i},i}=1,i\\in[d]$ , then we can assign to each node $i$ in $\\hat{\\mathcal G}$ a new index $k_{i}$ and work with the new indices. ", "page_idx": 20}, {"type": "text", "text": "In this case, by Lemma 3 we have $B_{k}H=\\Sigma_{k}\\hat{B}_{k}\\hat{H},\\forall k\\in[K]$ or equivalently $\\Sigma_{k}\\hat{B}_{k}=B_{k}{\\cal T}$ , where $\\pmb{T}=\\pmb{H}\\hat{\\pmb{H}}^{\\dagger}\\in\\mathbb{R}^{d\\times d}$ , and $\\Sigma_{k}$ is a diagonal matrix with diagonal entries in $\\{+1,-1\\}$ . Let $\\hat{\\hat{B}}_{k}=\\Sigma_{k}\\hat{B}_{k}$ , then the rows of $\\hat{\\hat{B}}_{k}$ equals (up to sign) to the rows of $\\hat{B}_{k}$ . ", "page_idx": 20}, {"type": "text", "text": "To summarize, we now know that i) $\\hat{\\hat{\\boldsymbol B}}_{k}\\,=\\,B_{k}{\\boldsymbol T},k\\,\\in\\,[K]$ , ii) $(B_{k})_{i j}\\neq0\\Leftrightarrow j\\in\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}(i)$ , and similarly, $(\\hat{B}_{k})_{i j}\\neq0\\Leftrightarrow j\\in\\overline{{\\mathrm{pa}}}_{\\hat{\\mathcal{G}}}(i)$ , and iii) Both $\\{B_{k}\\}$ and $\\left\\{\\hat{\\hat{B}}_{k}\\right\\}$ satisfy the node-level nondegeneracy assumption Assumption 5. For any two such matrices that satisfy such a set of conditions, it must necessarily be true that $\\mathcal G=\\hat{\\mathcal G}$ . ", "page_idx": 20}, {"type": "text", "text": "Lemma 12 (Graph Identifiability). Consider any two sets matrices $\\{\\hat{\\hat{B}}_{k}\\}_{k\\in[K]}$ and $\\{B_{k}\\}_{k\\in[K]}$ and associated graphs $\\mathcal{G},\\hat{\\mathcal{G}}.$ . If these sets and graphs satisfy that: ", "page_idx": 20}, {"type": "text", "text": "then it must hold that $\\mathcal G=\\hat{\\mathcal G}$ . ", "page_idx": 20}, {"type": "text", "text": "Proof. We prove this via induction on the size of the graph $d$ . Note that here $\\mathcal G=\\hat{\\mathcal G}$ is not up to permutation and our statement is equivalent to $\\mathrm{pa}_{\\mathcal{G}}(i)\\overset{-}{=}\\mathrm{pa}_{\\hat{\\mathcal{G}}}(i),\\forall i\\in[d]$ . ", "page_idx": 20}, {"type": "text", "text": "If $d=1$ , i.e., $\\mathcal G=\\hat{\\mathcal G}$ obviously holds since both are graphs with only 1 node. ", "page_idx": 20}, {"type": "text", "text": "Suppose that for all graphs $\\mathcal{G}$ of size $d-1$ , the graph $\\hat{\\mathcal G}$ satisfying all given assumptions must necessarily be equal to $\\mathcal{G}$ . Now, we consider the case that $\\mathcal{G}$ has $d$ nodes. WLOG we can assume that the nodes of $\\mathcal{G}$ are properly indexed such that $i\\;\\in\\;\\mathrm{pa}_{\\mathcal{G}}(j)\\;\\Rightarrow\\;i\\;<\\;j$ , so $\\boldsymbol{B}_{k},\\boldsymbol{k}\\;\\in\\;[K]$ are lower-triangular matrices. (However, it is currently unknown whether $\\hat{\\hat{B}}_{k}$ are also lower-triangular.) By our assumption that $i\\in\\mathrm{pa}_{\\mathcal{G}}(j)\\Rightarrow i<j$ , the node $d$ in $\\mathcal{G}$ has no child. Thus we can write ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{B_{k}=\\left(\\begin{array}{c c}{B_{k}^{-}}&{\\mathbf{0}}\\\\ {b_{k}}&{c_{k}}\\end{array}\\right),T=\\left(\\begin{array}{c c}{T^{-}}&{\\times}\\\\ {\\times}&{\\times}\\end{array}\\right)\\,\\mathrm{and}\\,\\hat{B}_{k}=B_{k}T=\\left(\\begin{array}{c c}{\\hat{B}_{k}^{-}}&{\\times}\\\\ {\\times}&{\\times}\\end{array}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $B_{k}^{-},T,\\hat{\\pmb{B}}_{k}^{-}=B_{k}^{-}T^{-}\\in\\mathbb{R}^{(d-1)\\times(d-1)},b_{k}\\in\\mathbb{R}^{d-1},c_{k}\\in\\mathbb{R}$ and $\\times$ denotes irrelevant entries. Let $A_{k}^{-},\\hat{A}_{k}^{-},\\Omega_{k}^{-}$ and $\\hat{\\Omega}_{k}^{-}$ be the top-left $(d-1)\\times(d-1)$ sub-matrices of $A_{k},\\hat{A},\\Omega_{k}$ and $\\hat{\\Omega}_{k}$ respectively, and $g^{-}$ and ${\\hat{\\mathcal{G}}}^{-}$ are graphs obtained by deleting node $d$ and all related edges from $\\mathcal{G}$ and $\\hat{\\mathcal G}$ . Then it is easy to see that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left(A_{k}^{-}\\right)_{i j}\\neq0\\Leftrightarrow j\\in\\mathrm{pa}_{{\\mathscr G}^{-}}(i)\\quad\\mathrm{and}\\quad\\left(\\hat{A}_{k}^{-}\\right)_{i j}\\neq0\\Leftrightarrow j\\in\\mathrm{pa}_{{\\mathscr G}^{-}}(i).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Moreover, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left(\\begin{array}{c c}{B_{k}^{-}}&{\\mathbf{0}}\\\\ {b_{k}}&{c_{k}}\\end{array}\\right)=B_{k}=\\Omega_{k}^{-\\frac{1}{2}}\\left(I-A_{k}\\right)=\\left(\\begin{array}{c c}{\\left(\\Omega_{k}^{-}\\right)^{-\\frac{1}{2}}}&{\\mathbf{0}}\\\\ {\\mathbf{0}}&{\\times}\\end{array}\\right)\\left(\\begin{array}{c c}{I-A_{k}^{-}}&{\\mathbf{\\Delta\\times}}\\\\ {\\mathbf{\\Delta\\times}}&{\\mathbf{\\Delta\\times}}\\end{array}\\right)=\\left(\\begin{array}{c c}{\\left(\\Omega_{k}^{-}\\right)^{-\\frac{1}{2}}(I-A_{k})}&{\\mathbf{\\Delta\\times}}\\\\ {\\mathbf{\\Delta\\times}}&{\\mathbf{\\Delta\\times}}\\end{array}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "so that $B_{k}^{-}=\\left(\\Omega_{k}^{-}\\right)^{-\\frac{1}{2}}(I-A_{k}^{-})$ . Similarly, we have $\\hat{\\b{B}}_{k}^{-}=\\left(\\hat{\\Omega}_{k}^{-}\\right)^{-\\frac{1}{2}}({\\pmb{I}}-\\hat{\\pmb{A}}_{k}^{-})$ . ", "page_idx": 21}, {"type": "text", "text": "We can also verify that $\\left\\{B_{k}^{-}\\right\\}_{k=1}^{K}$ and $\\left\\{\\hat{\\hat{B}}_{k}^{-}\\right\\}_{k=1}^{K}$ are node-level independent in the sense of Assumption 5. We only prove this for $\\left\\{\\hat{\\hat{B}}_{k}\\right\\}_{k=1}^{K}$ ; the arguments used for $\\{B_{k}\\}_{k=1}^{K}$ are exactly the same as the first case considered below. Now for each $i\\in[d-1]$ , let ${\\pmb R}_{i}\\in\\mathbb R^{K\\times d}$ be the matrix whose $k$ -th row is the $i$ -th row of $\\hat{\\hat{B}}_{k}$ , and $\\pmb{R}_{i}^{-}\\in\\mathbb{R}^{K\\times(d-1)}$ be the matrix whose $k$ -th row is the $i$ -th row of $\\hat{\\hat{B}}_{k}^{-}$ , then obviously $R_{i}$ is of form $[R_{i}^{-},r_{i}]$ . We consider two cases: ", "page_idx": 21}, {"type": "text", "text": "\u2022 Case $\\mathbf{1.}d\\not\\in\\mathrm{\\mathrm{pa}}_{\\hat{\\mathcal{G}}}(i)$ This means that the last entry of the $i$ -th row of B\u02c6k is zero. Thus ${\\boldsymbol r}_{i}=\\mathbf{0}$ , and rank $\\left(R_{i}^{-}\\right)=\\mathrm{{rank}}\\left(R_{i}\\right)=\\left|{\\overline{{\\mathrm{pa}}}}_{\\hat{\\mathcal{G}}}(i)\\right|=\\left|{\\overline{{\\mathrm{pa}}}}_{\\hat{\\mathcal{G}}^{-}}(i)\\right|$ , where the second equality follows from Assumption 5. \u2022 Case $2.d\\in\\mathrm{pa}_{\\hat{\\mathcal{G}}}(i)$ In this case we have rank $\\left(R_{i}^{-}\\right)\\geqslant\\operatorname{rank}\\left(R_{i}\\right)-1=\\left|{\\overline{{\\mathrm{pa}}}}_{\\hat{\\mathcal{G}}}(i)\\right|-1=$ $\\left|\\overline{{\\mathrm{pa}}}_{\\hat{\\mathcal{G}}-}(i)\\right|$ . Due to our assumption on $\\hat{A}_{k}$ and the relationship $\\hat{\\b{B}}_{k}^{-}=\\left(\\hat{\\b{\\Omega}}_{k}^{-}\\right)^{-\\frac{1}{2}}({\\pmb I}-\\hat{\\pmb A}_{k}^{-})$ , we know that each row of $R_{i}^{-}$ , namely the $i$ -th row of some $\\hat{\\hat{B}}_{k}$ only has $\\left|\\overline{{\\mathrm{pa}}}_{\\hat{\\mathcal{G}}}(i)\\right|-1=$ $\\left|\\overline{{\\mathrm{pa}}}_{\\hat{\\mathcal{G}}-}(i)\\right|$ non-zero entries, so that rank $\\left(R_{i}^{-}\\right)=\\left|\\overline{{\\mathrm{pa}}}_{\\hat{\\mathcal{G}}^{-}}(i)\\right|$ holds. ", "page_idx": 21}, {"type": "text", "text": "Since we have shown that the matrices $B_{k}^{-}$ and $\\hat{\\hat{B}}_{k}^{-}$ satisfy the three properties that we assume for induction with $_T$ replaced by ${\\mathbf{}}^{T-}$ and $\\mathcal{G},\\hat{\\mathcal{G}}$ replaced by $\\mathcal{G}^{-},\\hat{\\mathcal{G}}^{-}$ respectively, by induction hypothesis, we can thus deduce that $\\mathcal{G}^{-}=\\hat{\\mathcal{G}}^{-}$ . To prove $\\mathcal G=\\hat{\\mathcal G}$ it remains to show that the dependency of node $d$ on the remaining nodes are the same in $\\mathcal{G}$ and $\\hat{\\mathcal G}$ . ", "page_idx": 21}, {"type": "text", "text": "First, we show that $\\operatorname{ch}_{\\hat{\\mathcal{G}}}(d)=\\varnothing$ . Suppose in contrary that there is some $i\\in\\mathrm{ch}_{\\hat{\\mathcal{G}}}(d)$ , then $\\left|\\mathrm{pa}_{\\mathcal{G}}(i)\\right|=$ $\\left|\\mathrm{pa}_{\\mathcal{G}^{-}}(i)\\right|=\\left|\\mathrm{pa}_{\\hat{\\mathcal{G}}^{-}}(i)\\right|=\\left|\\mathrm{pa}_{\\hat{\\mathcal{G}}}(i)\\right|-1$ . Recalling that $(\\boldsymbol{B})_{i}$ denotes the $i$ -th row of matrix $_B$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\dim\\Big(\\mathrm{span}\\left\\langle\\left(\\hat{\\hat{B}}_{k}\\right)_{i}:1\\leqslant k\\leqslant K\\right\\rangle\\!\\Big)=\\dim\\left(\\mathrm{span}\\left\\langle(B_{k})_{i}:1\\leqslant k\\leqslant K\\right\\rangle\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leqslant\\left\\vert\\mathrm{pa}_{\\mathcal{G}}(i)\\right\\vert+1<\\left\\vert\\mathrm{pa}_{\\hat{\\mathcal{G}}}(i)\\right\\vert+1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the first inequality follows from $\\left(\\hat{\\hat{B}}_{k}\\right)_{i}=(B_{k})_{i}\\,{\\cal T}$ and Lemma 2, the second holds since each $(B_{k})_{i}$ has nonzero elements only at coordinates in $j\\in\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}(i)$ , and the last one holds since ", "page_idx": 21}, {"type": "text", "text": "$\\left|\\mathrm{pa}_{\\mathcal{G}}(i)\\right|=\\left|\\mathrm{pa}_{\\hat{\\mathcal{G}}}(i)\\right|-1$ . However, (11) contradicts the non-degeneracy condition Assumption 5 that we assume for matrices $\\hat{B}_{k},k\\,\\in\\,[K]$ in the statement of the theorem. Therefore we have $\\mathrm{ch}_{\\hat{\\mathcal{G}}}(d)=\\varnothing=\\mathrm{ch}_{\\mathcal{G}}(d)$ . ", "page_idx": 22}, {"type": "text", "text": "Second, by a similar argument comparing the number of nonzero elements in the last row of $B_{k}$ and $\\hat{\\hat{B}}_{k}$ , we can also deduce that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left|\\mathrm{pa}_{\\mathcal{G}}(d)\\right|=\\left|\\mathrm{pa}_{\\hat{\\mathcal{G}}}(d)\\right|.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Indeed, since $\\left(\\hat{\\hat{B}}_{k}\\right)_{d}=(B_{k})_{d}\\,{\\cal T}$ , by Lemma 2 we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\dim\\Big(\\mathrm{span}\\,\\Big\\langle\\Big(\\hat{\\hat{B}}_{{k}}\\Big)_{d}:1\\leqslant k\\leqslant K\\Big\\rangle\\Big)=\\dim\\big(\\mathrm{span}\\,\\langle(B_{k})_{d}:1\\leqslant k\\leqslant K\\rangle\\big)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "However, since we assume that Assumption 5 is satisfied for $\\{B_{k}\\}_{k=1}^{K}$ and $\\left\\{\\hat{B}_{k}\\right\\}_{k=1}^{K}$ , we know that the LHS and RHS of the above equation are equal to $\\left|\\operatorname{pa}_{\\mathcal{G}}(d)\\right|+1$ and $\\left|\\operatorname{pa}_{\\hat{\\mathcal{G}}}(d)\\right|+1$ respectively, implying (12). ", "page_idx": 22}, {"type": "text", "text": "Third, we show that $\\mathrm{pa}_{\\mathcal{G}}(d)\\,=\\,\\mathrm{pa}_{\\hat{\\mathcal{G}}}(d)$ . Suppose the contrary, let $\\ell$ be the smallest element in $\\mathrm{pa}_{\\mathcal{G}}(d)\\Delta\\mathrm{pa}_{\\hat{\\mathcal{G}}}(d)$ , where $A\\Delta B:=(A\\setminus B)\\cup(B\\setminus A)$ . Recall that while $\\mathcal{G}$ and $\\hat{\\mathcal G}$ are originally not symmetric as nodes are topologically sorted according to $\\mathcal{G}$ , now we have shown that $\\mathcal{G}^{-}\\equiv\\hat{\\mathcal{G}}^{-}$ and that $\\operatorname{ch}_{\\mathcal{G}}(d)=\\operatorname{ch}_{\\hat{\\mathcal{G}}}(d)={\\mathit{\\bar{\\Phi}}}\\varnothing$ , so we can assume WLOG that $\\ell\\,\\in\\,\\mathrm{pa}_{\\mathcal{G}}(d)$ and $\\ell\\not\\in\\mathrm{pa}_{\\hat{\\mathcal{G}}}(d)$ , and the other case can be handled symmetrically. Since $B_{k}$ is lower triangular and $\\left(B_{k}\\right)_{j j}=\\left(\\Omega_{k}\\right)_{j j}^{-{\\frac{1}{2}}}\\neq$ $0,\\forall j\\in[d]$ , the top-left $\\ell\\times\\ell$ sub-matrix of $B_{k}$ , which we denote by $[B_{k}]_{\\ell,\\ell}$ , must be invertible. This implies that $\\left\\{[B_{k}]_{\\ell,\\ell}^{\\top}\\,\\pmb{\\lambda}:\\pmb{\\lambda}\\in\\mathbb{R}^{\\ell}\\right\\}=\\mathbb{R}^{\\ell}$ , so we can always find coefficients $\\lambda_{k j},j\\in[\\ell]$ such that the first $\\ell$ entries of the vector $\\begin{array}{r}{(B_{k})_{d}-\\sum_{i=1}^{\\ell}\\lambda_{k i}(B_{k})_{i}\\in{\\mathbb R}^{d}}\\end{array}$ are all zero. Since $\\hat{\\hat{B}}_{k}=B_{k}\\pmb{T}$ and $\\textbf{\\emph{T}}$ is invertible, we have $\\begin{array}{r}{\\left(\\hat{\\pmb{B}}_{k}\\right)_{d}-\\sum_{j=1}^{\\ell}\\lambda_{k j}\\left(\\hat{\\pmb{B}}_{k}\\right)_{j}=\\left(\\left(\\pmb{B}_{k}\\right)_{d}-\\sum_{j=1}^{\\ell}\\lambda_{k j}\\left(\\pmb{B}_{k}\\right)_{j}\\right)\\pmb{T},\\forall k\\in[K]}\\end{array}$ and ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{lim}\\left(\\mathrm{span}\\left\\langle\\left(\\hat{\\hat{B}}_{k}\\right)_{d}-\\sum_{j=1}^{\\ell}\\lambda_{k j}\\left(\\hat{\\hat{B}}_{k}\\right)_{j}:k\\in[K]\\right\\rangle\\right)=\\dim\\left(\\mathrm{span}\\left\\langle(B_{k})_{d}-\\sum_{j=1}^{\\ell}\\lambda_{k j}\\left(B_{k}\\right)_{j}:k\\in[K]\\right)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leqslant\\left\\vert\\mathbf{pa}_{\\mathcal{G}}(d)\\setminus[\\ell]\\right\\vert+1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Here, the inequality holds because for any coordinate $t\\in[d]$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left(\\left(B_{k}\\right)_{d}-\\sum_{j=1}^{\\ell}\\lambda_{k j}\\left(B_{k}\\right)_{j}\\right)_{t}={\\left\\{\\begin{array}{l l}{\\qquad0}&{{\\mathrm{if~}}t\\leqslant\\ell}\\\\ {\\qquad\\left(B_{k}\\right)_{d,t}}&{{\\mathrm{otherwise}}}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where we note that $\\scriptstyle B_{k}$ is lower-triangular and thus $(\\boldsymbol{B}_{k})_{j,t}=0,\\forall j\\leqslant\\ell,t>\\ell$ . This implies that $\\begin{array}{r}{\\left(\\left(\\boldsymbol{B}_{k}\\right)_{d}-\\sum_{j=1}^{\\ell}\\lambda_{k j}\\left(\\boldsymbol{B}_{k}\\right)_{j}\\right)_{t}}\\end{array}$ is nonzero only if $t>\\ell$ and $t\\in\\mathrm{pa}_{\\mathcal{G}}(d)$ . ", "page_idx": 22}, {"type": "text", "text": "On the other hand, let $S=\\left(\\mathrm{{pa}}_{\\hat{\\mathcal{G}}}(d)\\cap[\\ell]^{c}\\right)\\cup\\{d\\}$ , then ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname{pan}\\left\\langle\\left(\\hat{\\hat{B}}_{k}\\right)_{d}-\\displaystyle\\sum_{j=1}^{\\ell}\\lambda_{k j}\\left(\\hat{\\hat{B}}_{k}\\right)_{j}:k\\in[K]\\right\\rangle\\right)\\geqslant\\dim\\left(\\operatorname{span}\\left\\langle\\left(\\left(\\hat{\\hat{B}}_{k}\\right)_{d}-\\displaystyle\\sum_{j=1}^{\\ell}\\lambda_{k j}\\left(\\hat{\\hat{B}}_{k}\\right)_{j}\\right)_{s}:k\\in[K]\\right\\rangle\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\dim\\left(\\operatorname{span}\\left\\langle\\left(\\left(\\hat{\\hat{B}}_{k}\\right)_{d}\\right)_{s}:k\\in[K]\\right\\rangle\\right)=|S|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where we recall that ${\\pmb u}_{S}$ denotes the vector $(u_{i}:i\\in S)\\in\\mathbb{R}^{|S|}$ . Here the first equality holds due to the same reason as (12), and the second follows from Assumption 5. To see why this is the case, note that Assumption 5 implies that the $K\\times\\left(\\left|\\operatorname{pa}_{\\mathcal{G}}(d)\\right|+1\\right)$ having $((B_{k})_{d})_{\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}(d)}$ as the $k$ -th row has full column rank, so that the sub-matrix obtained by extracting columns corresponding to the node set $S$ also has full column rank. ", "page_idx": 22}, {"type": "text", "text": "We have shown that $\\left|\\overline{{\\mathrm{pa}}}_{\\hat{\\mathcal{G}}}(d)\\cap[\\ell]^{c}\\right|\\;=\\;|S|\\;\\leqslant\\;\\left|\\mathrm{pa}_{\\mathcal{G}}(d)\\cap[\\ell]^{c}\\right|\\;+\\;1\\;=\\;\\left|\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}(d)\\cap[\\ell]^{c}\\right|$ . On the other hand, recall that by our choice of $\\ell$ , we have $\\left|\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}(d)\\cap[\\ell-1]\\right|\\,=\\,\\left|\\overline{{\\mathrm{pa}}}_{\\mathcal{\\hat{G}}}(d)\\cap[\\ell-1]\\right|$ and $\\ell\\in\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}(d)\\setminus\\overline{{\\mathrm{pa}}}_{\\hat{\\mathcal{G}}}(d)$ . Putting these together, we have $\\left|{\\overline{{\\operatorname{pa}}}}_{\\mathcal{G}}(d)\\right|>\\left|{\\overline{{\\operatorname{pa}}}}_{\\hat{\\mathcal{G}}}(d)\\right|$ . However, we know from (12) that $\\bar{|\\mathrm{pa}_{\\mathcal{G}}(d)|}=\\left|\\mathrm{pa}_{\\hat{\\mathcal{G}}}(d)\\right|$ , leading to a contradiction. Hence, such $\\ell$ shouldn\u2019t exist and we must have $\\mathrm{pa}_{\\mathcal{G}}(d)=\\mathrm{pa}_{\\hat{\\mathcal{G}}}(d)$ , completing the induction step for graphs of size $d$ . ", "page_idx": 23}, {"type": "text", "text": "By the principle of induction, we have shown that $\\mathcal G=\\hat{\\mathcal G}$ holds for any graphs under given assumptions. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "Now that we have established that $\\mathcal G=\\hat{\\mathcal G}$ , we prove the remaining part of the theorem. Note that for any $i,j\\in[d]$ such that $i\\not\\in\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}(j)$ , we have $\\overline{{(B_{k})_{j i}}}=(\\hat{\\hat{B}}_{k})_{j i}=\\overline{{0}},\\forall k\\in[K]$ . Since $\\hat{\\hat{B}}_{k}=B_{k}\\boldsymbol{T}$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{\\ell\\in\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}(j)}(B_{k})_{j\\ell}\\pmb{T}_{\\ell i}=0.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By Assumption 5, the above implies that $T_{\\ell i}=0$ for $\\forall\\ell\\in\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}(j)$ . In short, we have argued that if there exists $j$ such that $i\\notin\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}(j)$ and $\\ell\\in\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}(j)$ , then $T_{\\ell i}=0$ . ", "page_idx": 23}, {"type": "text", "text": "This implies that $T_{\\ell i}$ is non-zero only if ${\\bar{\\mathrm{ch}}}_{\\mathcal{G}}(\\ell)\\ \\subseteq\\ {\\bar{\\mathrm{ch}}}_{\\mathcal{G}}(i)$ . Since $\\pmb{v}~=~T z$ , we have $\\mathbf{\\nabla}v_{\\ell}\\ =$ $\\begin{array}{r}{\\sum_{i=1}^{d}T_{\\ell i}z_{i}=\\sum_{i\\in[d]:\\bar{\\mathrm{ch}}_{\\mathcal G}(\\ell)\\subseteq\\bar{\\mathrm{ch}}_{\\mathcal G}(i)}T_{\\ell i}z_{i}}\\end{array}$ . Note that when $i\\neq\\ell,\\deg(\\ell)\\subseteq\\deg(i)$ is equivalent to $i\\in\\operatorname{sur}_{\\mathcal{G}}(\\ell)$ , so $\\pmb{v}_{\\ell}$ only depends on $z_{\\overline{{\\mathrm{sur}}}\\mathcal{G}}(\\ell)$ by Lemma 6, as desired. ", "page_idx": 23}, {"type": "text", "text": "H.2 Formal version and proof of Theorem 2 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In previous works [36, 54], it is common to consider single-node soft interventions in the following sense: ", "page_idx": 23}, {"type": "text", "text": "Assumption 8. For $\\forall2\\:\\leqslant\\:k\\:\\leqslant\\:K$ , there exists $i_{k}~\\in~[d]$ , such that the structural equation in environment $k$ satisfies (4) satisfies $\\pmb{w}_{k}(i)=\\pmb{w}_{1}(i)$ and $\\omega_{k,i,i}=\\omega_{1,i,i}\\,f o r\\,\\forall i\\neq i_{k}$ . ", "page_idx": 23}, {"type": "text", "text": "Let $S_{i}=\\left\\{k:2\\leqslant k\\leqslant K,i_{k}=i\\right\\},i\\in\\left[d\\right]$ and $s_{i}=|S_{i}|$ . Suppose that $\\mathcal{G}$ has $\\begin{array}{r}{e=\\sum_{i=1}^{d}\\left|\\mathrm{pa}_{\\mathcal{G}}(i)\\right|}\\end{array}$ edges, then we can view the weight vectors $\\{(\\pmb{w}_{k}(i),\\omega_{k,i,i}):k=1$ or $i=i_{k}\\}$ as elements of the Euclidean space $\\mathbb{R}^{e+\\sum_{k=2}^{K}\\left|\\mathrm{pa}_{\\mathcal{G}}\\left(i_{k}\\right)\\right|}\\;\\times\\;\\mathbb{R}_{+}^{d+K-1}$ . Under Assumption 8, the models can be fully determined by these weight vectors. The following result states that if we restrict ourselves to single-node interventions, then in the worst case, $\\Theta(\\bar{d}^{2})$ interventions are required. ", "page_idx": 23}, {"type": "text", "text": "Theorem 6. There exists a causal graph $\\mathcal{G}$ with $\\Theta(d^{2})$ edges, such that for any unmixing matrix $H\\in\\mathbb{R}^{d\\times n}$ with full row rank, any independent noise variables \u03f5, and any $s_{i}\\,>\\,0,i\\,\\in\\,[d]$ such that $s_{i}\\;\\leqslant\\;\\left|\\mathrm{pa}_{\\mathcal{G}}(i)\\right|$ for some $i$ , the following holds: except from a null set of the weight space Re+ kK=2|paG(ik)| \u00d7 Rd++K\u22121(w.r.t the Lebesgue measure), there must exist a candidate solution $(\\hat{H},\\hat{\\mathcal{G}})$ and a hypothetical data generating process ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\forall k\\in[K],\\quad v=\\hat{A}_{k}v+\\hat{\\Omega}_{k}^{\\frac{1}{2}}\\epsilon,\\quad x=\\hat{H}^{\\dagger}v\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "such that ", "page_idx": 23}, {"type": "text", "text": "$(i^{\\prime})$ the unmixing matrix $\\hat{H}\\in\\mathbb{R}^{d\\times n}$ has full row rank; ", "page_idx": 23}, {"type": "text", "text": "$(i i^{\\prime})\\ \\forall k\\in[K]$ and $i,j\\,\\in\\,[d],\\,(\\hat{A}_{k})_{i j}\\,\\neq\\,0\\,\\Leftrightarrow\\,j\\,\\in\\,\\mathrm{pa}_{\\hat{\\mathcal{G}}}(i)$ and $\\hat{\\Omega}_{k}$ is a diagonal matrix with positive entries; ", "page_idx": 23}, {"type": "text", "text": "$(i i i^{\\prime})$ for $\\forall2\\leqslant k\\leqslant K$ , the weight matrices $\\hat{A}_{k},\\hat{\\Omega}_{k}$ of environment $E_{k}$ are from a single-node soft intervention on $E_{1}$ on node $i_{k}$ , in the sense of Assumption 8, ", "page_idx": 23}, {"type": "text", "text": "but $\\mathcal{G}$ is non-isomorphic to $\\hat{\\mathcal G}$ . ", "page_idx": 23}, {"type": "text", "text": "In this subsection we give the full proof of Theorem 6. We say that $S\\subseteq\\mathbb{R}^{m}$ is a null set if it has zero Lebesgue measure. Obviously, any hyperplanes in $\\mathbb{R}^{m}$ are null sets. We will also need the following simple lemma: ", "page_idx": 23}, {"type": "text", "text": "Lemma 13. Suppose that $m\\in\\mathbb{Z}_{+}$ and $V$ is a subspace of $\\mathbb{R}^{m}$ . Then for any set of vectors ${\\pmb u}_{i}\\in$ $\\mathbb{R}^{m}$ $n_{,\\,i}=1,2,\\cdots{},n$ that does not lie in $V$ , there must exists $\\pmb{v}\\in\\mathbb{R}^{m}$ such that $\\pmb{u}_{i}^{\\top}\\bar{\\pmb{v}}\\neq0,\\forall i\\in[n]$ but $\\pmb{v}\\in V^{\\bot}$ , where $V^{\\perp}$ is the orthogonal space of $V$ . ", "page_idx": 24}, {"type": "text", "text": "Proof. Let $\\pmb{w}_{i}$ be the orthogonal projection of $\\pmb{u}_{i}$ onto $V^{\\perp}$ . Since $u_{i}\\notin V$ , we know that $w_{i}\\neq{\\bf0}$ . The solution space of each equation $w_{i}^{\\top}v\\,=\\,0$ in $V^{\\perp}$ must then be a proper subspace of $V^{\\perp}$ . Equipped with the Lebesgue measure, all these spaces are null sets in $V^{\\perp}$ , so one can always choose a $\\dot{\\pmb{v}}\\in\\dot{\\pmb{V}}^{\\perp}$ that does not lie in any of these solution spaces. Such $\\pmb{v}$ satisfies all the requirements. ", "page_idx": 24}, {"type": "text", "text": "We choose $\\mathcal{G}$ to be the graph with $i\\rightarrow j$ for $\\forall1\\leqslant i<j\\leqslant d$ , so that $\\mathcal{G}$ has $\\textstyle{\\frac{d(d-1)}{2}}$ edges. Suppose that $i_{0}\\in[d]$ satisfies $s_{i}\\leqslant\\left|\\mathrm{pa}_{\\mathcal{G}}(i)\\right|-1$ , then we must have $i_{0}\\geqslant2$ , so there is an edge $1\\rightarrow i_{0}$ in $\\mathcal{G}$ , Let $\\hat{\\mathcal G}$ be the resulting graph obtained via removing the edge $1\\rightarrow i_{0}$ in $\\mathcal{G}$ , then $\\mathcal{G}$ and $\\hat{\\mathcal G}$ are clearly non-isomorphic. ", "page_idx": 24}, {"type": "text", "text": "Note that the $i$ -th row of $B_{k}$ can be written as $\\omega_{k,i,i}^{-\\frac{1}{2}}\\left(\\pmb{e}_{i}-(\\pmb{A}_{k})_{i}\\right)$ . Let\u2019s choose an lower-triangular matrix $\\pmb{T}=(t_{i j})_{i,j=1}^{d}\\in\\mathbb{R}^{d\\times d}$ with columns $\\pmb{t}_{i},i\\in[d]$ such that the following holds: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(e_{i}-(A_{k})_{i})^{\\top}t_{j}=\\left\\{\\begin{array}{l l}{=0,}&{\\forall k\\in\\{1\\}\\cup S_{i0},j=1\\mathrm{~and~}i=i_{0}}\\\\ {>0,}&{\\forall i=j\\mathrm{~and~}k\\in\\{1\\}\\cup S_{i}}\\\\ {\\neq0,}&{\\forall\\mathrm{~remaining~}(i,j,k)\\in\\{k=1,j<i\\}\\cup\\{k\\geqslant2,i=i_{k},j<i\\}}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and ", "page_idx": 24}, {"type": "equation", "text": "$$\nt_{i i}\\neq0,\\quad\\forall i\\in[d].\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We now show that: except from a null set in the weight space, such $\\textbf{\\emph{T}}$ can always be chosen. To see why this is the case, we first consider all the constraints on $\\pmb{t}_{1}$ : ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left(e_{i}-(A_{k})_{i}\\right)^{\\top}t_{1}=\\left\\{\\begin{array}{l l}{=0,}&{\\forall k\\in\\{1\\}\\cup S_{i_{0}}\\mathrm{~and~}i=i_{0}}\\\\ {>0,}&{\\forall i=1\\mathrm{~and~}k\\in\\{1\\}\\cup S_{i}}\\\\ {\\neq\\,0,}&{\\forall\\,\\mathrm{remaining~}(i,k)\\in\\{k=1,i>1\\}\\cup\\{k\\geqslant2,i=i_{k}>1\\}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Now let $V=\\operatorname{span}\\langle e_{i}-(A_{k})_{i}:k\\in\\{1\\}\\cup S_{i_{0}}$ and $i=i_{0}\\rangle$ and $R$ be the set of pairs $(i,k)$ specified in the second and third row of (15). For $\\forall(i,k)$ , let $w_{k}(i)$ be the weight vector of node $i$ in the environment $k$ , i.e., the vector of nonzero entries in $(A_{k})_{i}$ . Then for $\\forall(i,k)\\in R$ , the following set (as a subset of the weight space) ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\bigcup_{k^{*}\\in\\{1\\}\\cup S_{i_{0}}}\\left\\{e_{i_{0}}-{\\pmb w}_{k^{*}}(i_{0})\\in\\mathrm{span}\\,\\langle e_{i}-{\\pmb w}_{k}(i),e_{i_{0}}-{\\pmb w}_{k^{\\prime}}(i_{0}):k^{\\prime}\\in\\{1\\}\\cup S_{i_{0}}\\setminus\\{k^{*}\\}\\rangle\\right\\}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "must be a null set. Thus ", "page_idx": 24}, {"type": "equation", "text": "$$\nE=\\bigcup_{(i,k)\\in R}\\bigcup_{k^{\\ast}\\in\\{1\\}\\cup S_{i_{0}}}\\{e_{i_{0}}-w_{k^{\\ast}}(i_{0})\\in\\mathrm{span}\\,\\langle e_{i}-w_{k^{\\ast}}(i),e_{i_{0}}-w_{k^{\\prime}}(i_{0}):k^{\\prime}\\in\\{1\\}\\cup S_{i_{0}}\\setminus\\{k^{\\ast}\\}|\\mathrm{s}_{i_{0}}\\}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "is also a null set. For any weights that are not in $\\pmb{E}$ , we necessarily have ", "page_idx": 24}, {"type": "equation", "text": "$$\ne_{i}-w_{k}(i)\\notin\\operatorname{span}\\left\\langle e_{i}-(A_{k})_{i}:k\\in\\{1\\}\\cup S_{i_{0}}\\mathrm{~and~}i=i_{0}\\right\\rangle=V,\\quad(i,k)\\in R.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Let $U=\\{e_{i}-{\\pmb w}_{k}(i):(i,k)\\in R\\}$ , then we can apply Lemma 13 to deduce that there exists $\\pmb{t}_{1}$ such that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left(e_{i}-(A_{k})_{i}\\right)^{\\top}t_{1}=\\left\\{\\frac{=0,}{\\forall}\\ \\ \\forall k\\in\\{1\\}\\cup S_{i_{0}}\\mathrm{~and~}i=i_{0}\\right.}\\\\ {\\left.\\forall\\mathrm{~}0,\\ \\ \\forall\\mathrm{~remaining~}(i,k)\\in\\{k=1\\}\\cup\\{k\\geqslant2,i=i_{k}\\}\\right.}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Note that the only difference between (18) and (15) is that the latter one further requires that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left(e_{1}-(A_{k})_{1}\\right)^{\\top}t_{1}>0,\\quad\\forall k\\in\\{1\\}\\cup S_{i}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "while the former only guarantees that these terms are nonzero. However, recall that $(A_{k})_{i j}\\neq0\\Rightarrow$ $j\\in\\mathrm{{pa}}_{\\mathcal{G}}(i)\\Rightarrow j<i$ , so the above essentially says that $t_{11}>0$ . This can be easily guaranteed by replacing the solution $\\pmb{t}_{1}$ we obtained satisfying (18) with $-t_{1}$ if needed. ", "page_idx": 24}, {"type": "text", "text": "Assuming that the weights do not lie in the null set $\\pmb{E}$ we have shown that $\\pmb{t}_{1}$ can always be chosen to satisfy all constraints imposed on it. We now proceed to choose the remaining entries of $\\textbf{\\emph{T}}$ . The remaining entries in $\\pmb{t}_{1}$ can be chosen arbitrarily. For $\\pmb{t}_{j},j>1$ , we note that the remaining constraints in (13) that need to be satisfied consist of the \"nonzero\" part and the \"positivity\" part. The positivity constrains can always be satisfied by choosing a sufficiently large $t_{j j}$ for $j>1$ . ", "page_idx": 25}, {"type": "text", "text": "After choosing the $\\pmb{t}_{j}$ \u2019s satisfying the positivity constraints, the nonzero constraints along with (14) are easy to fulflil by slightly perturbing $\\pmb{t}_{j}$ if they are violated; since each of these constraints are only violated in a zero-measure set of the weight space. Hence, we have shown that except a null set $\\boldsymbol{E}$ in the weight space, there always exists some $\\textbf{\\emph{T}}$ satisfying (13). Such $_T$ must be invertible since it is lower-triangular and its diagonal entries are nonzero. Now let $\\hat{H}=T^{-1}H$ and $\\hat{\\Omega}_{k}$ be the diagonal matrix with entries $\\hat{\\omega}_{k,i,i}=t_{i i}^{-2}\\cdot\\omega_{k,i,i},i\\in[d]$ and ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\hat{A}_{k}=I-\\hat{\\Omega}_{k}^{\\frac{1}{2}}\\Omega_{k}^{-\\frac{1}{2}}(I-A_{k}){\\bf T}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "First since $_T$ is invertible and $\\pmb{H}$ has full rank, $\\hat{H}$ must also have full row rank. Second, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(\\hat{A}_{k})_{i j}=\\left\\{\\begin{array}{r l}{1-\\hat{\\omega}_{k,i,i}^{\\frac{1}{2}}\\omega_{k,i,i}^{-\\frac{1}{2}}t_{i i}=0}&{\\mathrm{if~}j=i}\\\\ {-\\hat{\\omega}_{k,i,i}^{\\frac{1}{2}}\\omega_{k,i,i}^{-\\frac{1}{2}}\\left(e_{i}-(A_{k})_{i}\\right)^{\\top}t_{j}=0}&{\\mathrm{if~}j>i}\\\\ {-\\hat{\\omega}_{k,i,i}^{\\frac{1}{2}}\\omega_{k,i,i}^{-\\frac{1}{2}}\\left(e_{i}-(A_{k})_{i}\\right)^{\\top}t_{j}}&{\\mathrm{if~}j<i.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where we again recall that both $A_{k}$ and $\\textbf{\\emph{T}}$ are lower-triangular. From (13) we can see that ", "page_idx": 25}, {"type": "text", "text": "\u2022 When $i=i_{0}$ and $j=1$ , we have \u2013 $(\\hat{A}_{k})_{i_{0},1}=0$ if $k\\in\\{1\\}\\cup S_{i_{0}}$ , and \u2013 $\\mathbf{-\\partial}(\\hat{A}_{k})_{i_{0},1}=(\\hat{A}_{1})_{i_{0},1}=0$ if $k\\notin\\{1\\}\\cup S_{i_{0}}$ , by definition of $S_{i_{0}}$ and Assumption 8.   \n\u2022 When $i>j$ and $(i,j)\\neq(i_{0},1)$ , we have \u2013 $(\\hat{A}_{k})_{i j}\\neq0$ if $k=1$ or $i=i_{k}$ , which directly follows from (13), and $-\\ (\\hat{A}_{k})_{i j}=(\\hat{A}_{1})_{i j}\\neq0$ , by Assumption 8. ", "page_idx": 25}, {"type": "text", "text": "To summarize, for each $k$ , $(A_{k})_{i j}\\neq0\\Leftrightarrow j\\in\\mathrm{pa}_{\\mathcal{G}}(i)$ and $(i,j)\\neq(i_{0},1)$ . ", "page_idx": 25}, {"type": "text", "text": "Finally, let $\\hat{w}_{k}(i)$ be the weight vector of node $i$ in environment $k$ in the hypothetical model i.e., the vector of nonzero entries in $(A_{k})_{i}$ , and ${\\mathbf{}}T_{S}$ be the submatrix of $_T$ by selecting the rows and columns in the index set $S$ , then by (19) we have that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\omega}_{k,i,i}=t_{i i}^{-2}\\cdot\\omega_{k,i,i},\\quad\\hat{\\omega}_{k,i,i}^{\\frac{1}{2}}\\omega_{k,i,i}^{-\\frac{1}{2}}w_{k}(i)T_{\\mathrm{pa}_{\\mathcal{G}}(i)}=\\left\\{\\begin{array}{r l}{\\hat{w}_{k}(i)}&{{}\\mathrm{if}\\ i\\neq i_{0}}\\\\ {[0,\\hat{w}_{k}(i)]}&{\\mathrm{if}\\ i=i_{0}}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By our assumption, for $\\forall k\\geqslant2$ , $i\\neq i_{k}\\Rightarrow{\\pmb w}_{k}(i)={\\pmb w}_{1}(i)$ and $\\omega_{k,i,i}=\\omega_{1,i,i}$ . Thus (20) imply that $\\forall k\\geqslant2$ $\\geqslant2,i\\neq i_{k}\\Rightarrow\\hat{\\pmb{w}}_{k}(i)=\\hat{\\pmb{w}}_{1}(i)$ and $\\hat{\\omega}_{k,i,i}=\\hat{\\omega}_{1,i,i}$ . In other words, a single-node intervention on node $i_{k}$ in environment $k$ in the ground-truth model corresponds to a single-node intervention on node $i_{k}$ in environment $k$ in the hypothetical model, thereby completing the proof. ", "page_idx": 25}, {"type": "text", "text": "H.3 Proof of Theorem 4 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We first prove two lemmas. ", "page_idx": 25}, {"type": "text", "text": "Lemma 14. $\\forall i\\in[d]$ , we have span $\\langle(M_{k})_{i}:k\\in[K]\\rangle=\\operatorname{span}\\big\\langle h_{j}:j\\in\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}(i)\\big\\rangle.$ ", "page_idx": 25}, {"type": "text", "text": "Proof. Since $(M_{k})_{i}\\;=\\;(B_{k})_{i}{H}$ , and $(B_{k})_{i j}\\;\\neq\\;0\\;\\Leftrightarrow\\;j\\;\\in\\;\\overline{{{\\mathrm{pa}}}}_{\\mathcal{G}}(i)$ , we can see that $(M_{k})_{i}\\ \\in$ span $\\left\\langle h_{j}:j\\in{\\overline{{\\mathrm{pa}}}}_{\\mathcal{G}}(i)\\right\\rangle$ . On the other hand, since $\\pmb{H}$ is invertible, by Assumption 5 we have dim span $\\langle(M_{k})_{i}:k\\in[K]\\rangle\\ =\\ \\dim\\operatorname{span}\\left\\langle(B_{k})_{i}:k\\in[K]\\right\\rangle\\ =\\ \\left|{\\overline{{\\operatorname{pa}}}}_{\\mathcal{G}}(i)\\right|$ . Thus we must have span $\\langle(M_{k})_{i}:k\\in[K]\\rangle=\\operatorname{span}\\left\\langle h_{j}:j\\in{\\overline{{\\operatorname{pa}}}}_{\\mathcal{G}}(i)\\right\\rangle$ . \u53e3 ", "page_idx": 25}, {"type": "text", "text": "Lemma 15. Let $\\hat{S}$ be an ancestral set of graph $\\mathcal{G}$ and $\\hat{V}_{k}=\\operatorname{span}\\left\\langle(M_{k})_{s}:s\\in\\hat{S}\\right\\rangle,k\\in[K].$ . Then we have $V_{1}=V_{2}=\\cdots=V_{K}=\\operatorname{span}{\\Big\\langle}h_{s}:s\\in{\\hat{S}}{\\Big\\rangle}.$ . ", "page_idx": 25}, {"type": "text", "text": "Proof. Recall that $M_{k}=B_{k}H$ , so for $\\forall s\\in{\\hat{S}}$ , the $s$ -th row of $M_{k}$ can be written as ", "page_idx": 26}, {"type": "equation", "text": "$$\n(M_{k})_{s}=\\sum_{t=1}^{d}(B_{k})_{s t}h_{t}=\\sum_{t\\in\\overline{{\\mathbb{P a}}}_{\\mathcal{G}}(s)}(B_{k})_{s t}h_{t}\\in\\mathrm{span}\\left\\langle h_{s}:s\\in\\hat{S}\\right\\rangle\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the last equation is because $\\hat{S}$ is ancestral $\\Rightarrow\\ \\overline{{\\mathrm{pa}}}_{\\mathcal{G}}(s)\\ \\subseteq\\ \\hat{S}$ . Thus, for $\\forall k\\ \\in\\ [K]$ , $\\hat{V}_{k}\\ =\\ \\operatorname{span}\\left\\langle(M_{k})_{s}:s\\in\\hat{S}\\right\\rangle\\ \\subseteq\\ \\operatorname{span}\\left\\langle h_{s}:s\\in\\hat{S}\\right\\rangle$ . On the other hand, recall that both $\\scriptstyle B_{k}$ and $\\pmb{H}$ have full rank, so $M_{k}$ has full row rank as well, which implies that $\\mathrm{dim}\\,V_{k}\\,=\\,|S|\\,=$ dim span $\\left<h_{s}:s\\in\\hat{S}\\right>$ . Hence, $V_{k}=\\operatorname{span}\\left\\langle h_{s}:s\\in{\\hat{S}}\\right\\rangle,\\forall k\\in[K]$ . \u53e3 ", "page_idx": 26}, {"type": "text", "text": "The following two propositions show that our algorithm always maintain an ancestral set, recursively adds a new node into the set and correctly identifies its parents. ", "page_idx": 26}, {"type": "text", "text": "Proposition 3 (Proposition 1 restated). The following two propositions hold for Algorithm 3: ", "page_idx": 26}, {"type": "text", "text": "\u2022 $\\operatorname{ans}_{\\mathcal{G}}(i)\\subseteq S\\Leftrightarrow t h e\\ i f$ condition in line 8 of Algorithm 3 is fulfilled; \u2022 the set $S$ maintained in Algorithm 3 is always an ancestral set, in the sense that $j\\in S\\Rightarrow$ $\\operatorname{ans}_{\\mathcal{G}}(j)\\subseteq S$ . ", "page_idx": 26}, {"type": "text", "text": "Proof. At the starting point, we have $S=\\emptyset$ which is obviously an ancestral set. Now suppose that after the $\\ell$ -th iteration, $S\\,=\\,\\{s_{1},s_{2},\\cdot\\cdot\\cdot\\,,s_{\\ell}\\}$ is an ancestral set. In the following, we show that $\\operatorname{ans}_{\\mathcal{G}}(i)\\,\\subseteq\\,S\\,\\Leftrightarrow$ the if condition in line 8 is fulfilled. This would immediately imply that there always exists a node $i$ that can be added into $S$ in the $(\\ell+1)$ -th iteration, and that after adding $i,{\\cal S}$ is still an ancestral set. ", "page_idx": 26}, {"type": "text", "text": "Suppose that $\\operatorname{ans}_{\\mathcal{G}}(i)\\subseteq\\ S$ for some $i\\ \\not\\in\\ S$ , by Lemma 14 we know that $(M_{k})_{i}\\quad\\in$ span $\\left\\langle h_{j}:j\\in{\\overline{{\\mathrm{pa}}}}_{\\mathcal{G}}(i)\\right\\rangle$ , so there exists $\\alpha_{k}\\in\\mathbb{R}$ such that $(M_{k})_{i}-\\alpha_{k}\\pmb{h}_{i}\\in\\operatorname{span}\\left\\langle\\pmb{h}_{j}:j\\in\\operatorname{pa}_{\\mathcal{G}}(i)\\right\\rangle$ . Moreover, since $\\begin{array}{r}{(M_{k})_{i}=\\sum_{j\\in\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}(i)}(B_{k})_{j j}h_{j}}\\end{array}$ , $(B_{k})_{i i}=\\omega_{k,i,i}^{-\\frac{1}{2}}\\neq0$ and $\\pmb{H}$ has full row rank by assumption, we must have $(M_{k})_{i}\\notin\\operatorname{span}\\left\\langle h_{j}:j\\in\\mathrm{pa}_{\\mathcal{G}}(i)\\right\\rangle$ and so $\\alpha_{k}\\neq0$ . Thus, we have by the linearity of the projection operator ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{q_{k}:=\\mathrm{proj}_{V_{k}^{\\perp}}\\left((M_{k})_{i}\\right)=\\mathrm{proj}_{V_{k}^{\\perp}}\\left((M_{k})_{i}-\\alpha_{k}h_{i}\\right)+\\mathrm{proj}_{V_{k}^{\\perp}}\\left(\\alpha_{k}h_{i}\\right)=\\alpha_{k}\\mathrm{proj}_{V_{k}^{\\perp}}\\left(h_{i}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Recall that all the $V_{k}$ \u2019s are the same and equal span $\\langle h_{s}:s\\in S\\rangle$ by Lemma 15. So dim span $\\langle\\pmb{q}_{k}:k\\in[K]\\rangle\\leqslant1$ . Since $\\pmb{H}$ has full row rank, we have $\\boldsymbol{h}_{i}\\notin$ span $\\langle h_{s}:s\\in S\\rangle=V_{k}$ , so that dim span $\\langle\\pmb{q}_{k}:k\\in[K]\\rangle=1$ holds, which is exactly the if condition in line 8. ", "page_idx": 26}, {"type": "text", "text": "Conversely, suppose that there is an $i\\not\\in S$ such that $\\operatorname{ans}_{\\mathcal{G}}(i)\\not\\subseteq S$ but dim span $\\langle\\pmb{q}_{k}:k\\in[K]\\rangle=1$ holds. Since $S$ is ancestral, we know that there must be some $j~\\in~\\mathrm{pa}_{\\mathcal{G}}(i)$ such that $j\\ \\notin\\ S$ . Since $e_{i}$ and $e_{j}$ both have support on the coordinates in $\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}(i)$ , by Assumption 5 we know that $\\operatorname{span}\\langle e_{i},e_{j}\\rangle\\,\\subseteq\\,\\operatorname{span}\\langle(B_{k})_{i}:{\\bar{k}}\\in[K]\\rangle$ , so that $\\mathrm{span}\\langle h_{i},h_{j}\\rangle\\dot{=}\\,\\mathrm{span}\\langle e_{i},e_{j}\\rangle H\\subseteq\\mathrm{span}\\langle(B_{k})_{i}\\,:$ $k\\in[K]\\rangle\\dot{H}=\\operatorname{span}\\langle(M_{k})_{i}:k\\in[K]\\rangle$ . Since dim span $\\langle\\pmb{q}_{k}:k\\in[K]\\rangle=1$ , there must exist some vector $\\pmb{u}\\in\\mathbb{R}^{n}$ and $\\alpha_{i},\\alpha_{j}\\,\\in\\mathbb{R}$ such that $\\pmb{h}_{i}-\\alpha_{i}\\pmb{u},\\pmb{h}_{j}-\\alpha_{j}\\pmb{u}\\in V_{k}=\\mathrm{span}\\langle\\pmb{h}_{s}:s\\in S\\rangle$ . Since $i,j\\not\\in S$ and $\\pmb{H}$ has full row rank, we can deduce that $h_{i},h_{j}\\notin\\operatorname{span}\\langle h_{s}:s\\in S\\rangle$ , and so both of $\\alpha_{i}$ and $\\alpha_{j}$ are non-zero. Hence $\\alpha_{j}\\pmb{h}_{i}-\\alpha_{i}\\pmb{h}_{j}\\in\\mathrm{span}\\langle\\pmb{h}_{s}:s\\in\\dot{S}\\rangle$ , which is impossible since we know that $\\pmb{H}$ has full row-rank. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "Proposition 4 (Proposition 2 restated). Given any ordered ancestral set $S$ that contains $\\mathrm{pa}_{\\mathcal{G}}(i)$ for some $i\\not\\in S$ , Algorithm 2 returns a set $P_{i}\\subseteq S$ that is exactly $\\mathrm{pa}_{\\mathcal{G}}(i)$ . ", "page_idx": 26}, {"type": "text", "text": "Proof. As we have shown in Proposition 1, for each possible input $(S,i)$ to Algorithm 2, both $S$ and $S\\cup\\{i\\}$ are ancestral sets, so that $\\operatorname{ans}_{\\mathcal{G}}(i)\\subseteq S$ . Similarly one can see that inside the set $S:=\\{s_{1},s_{2},\\cdot\\cdot\\cdot,s_{m}\\}$ , all the ancestors of $s_{j}$ are contained in $\\{s_{1},s_{2},\\cdot\\cdot\\cdot\\,,s_{j-1}\\}$ . In the following, we show that $\\forall m^{\\prime}\\in\\{0,\\ldots,m\\}$ , $r_{m^{\\prime}}=\\left|{\\overline{{\\mathrm{pa}}}}_{\\mathcal{G}}(i)-\\left\\{s_{j}:j\\leqslant m^{\\prime}\\right\\}\\right|$ (\\*). ", "page_idx": 26}, {"type": "text", "text": "By Lemma 15 we have $W_{1}\\,=\\,W_{2}\\,=\\,\\cdot\\,\\cdot\\,=\\,W_{K}\\,=\\,\\mathrm{span}\\left\\langle h_{s_{j}}:j\\leqslant m^{\\prime}\\right\\rangle$ . Let $t_{1},t_{2},\\cdots\\,,t_{\\ell}$ be elements of $\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}(i)$ that are not in $\\{s_{j}:j\\leqslant m^{\\prime}\\}$ , then ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{r_{m^{\\prime}}=\\dim\\operatorname{span}\\left\\langle p_{k}:k\\in[K]\\right\\rangle=\\dim\\left(\\operatorname{proj}_{\\mathrm{span}\\left\\langle h_{s_{j}}:j\\leqslant m^{\\prime}\\right\\rangle^{\\perp}}\\operatorname{span}\\left\\langle(M_{k})_{i}:k\\in[K]\\right\\rangle\\right)}\\\\ &{\\quad=\\dim\\left(\\operatorname{proj}_{\\mathrm{span}\\left\\langle h_{s_{j}}:j\\leqslant m^{\\prime}\\right\\rangle^{\\perp}}\\operatorname{span}\\left\\langle h_{j}:j\\in\\overline{{\\operatorname{pa}}}_{\\mathcal{G}}(i)\\right\\rangle\\right)\\quad\\mathrm{(by~Lemma~l4)}}\\\\ &{\\quad=\\dim\\left(\\operatorname{proj}_{\\mathrm{span}\\left\\langle h_{s_{j}}:j\\leqslant m^{\\prime}\\right\\rangle^{\\perp}}\\operatorname{span}\\left\\langle h_{t_{1}},h_{t_{2}},\\cdots,h_{t_{\\ell}}\\right\\rangle\\right)}\\\\ &{\\quad=\\ell\\quad\\mathrm{(by~Lemma~4~and~non{\\mathrm{-}}d e g e n e r a c y~o f~}H)}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which proves $(^{*})$ . From $({^*})$ it is easy to see that $m^{\\prime}\\in\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}(i)$ (and thus in $\\mathrm{pa}_{\\mathcal{G}}(i)$ since $i\\not\\in S;$ ) if and only if $r_{m^{\\prime}}=r_{m^{\\prime}-1}-1$ . ", "page_idx": 27}, {"type": "text", "text": "Now we conclude the proof of Theorem 4. Propositions 1 and 2 directly imply that Algorithm 3 is able to exactly recover the ground-truth causal graph $\\mathcal{G}$ . It remains to show that Line 20 in Algorithm 3 produces the correct $\\hat{\\pmb h}_{i}$ \u2019s. By Lemma 14 we know that $E_{j}=\\mathrm{span}\\left\\langle h_{\\ell}:\\ell\\in\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}(j)\\right\\rangle$ , so ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\cap_{j\\in\\overline{{\\mathrm{ch}}}_{\\mathcal{G}}(i)}E_{j}=\\cap_{j\\in\\overline{{\\mathrm{ch}}}_{\\mathcal{G}}(i)}\\mathrm{span}\\left\\langle h_{\\ell}:\\ell\\in\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}(j)\\right\\rangle=\\mathrm{span}\\left\\langle h_{\\ell}:\\ell\\in\\overline{{\\mathrm{sur}}}_{\\mathcal{G}}(i)\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the last step holds because $\\pmb{H}$ has full row rank and $\\cap_{j\\in\\overline{{\\mathrm{ch}}}_{\\mathcal{G}}(i)}\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}(j)=\\overline{{\\mathrm{sur}}}_{\\mathcal{G}}(i)$ by definition.   \nHence, each $\\hat{\\pmb h}_{i}$ is a linear combination of $h_{\\ell},\\ell\\in\\overline{{\\mathrm{sur}}}_{\\mathcal{G}}(i)$ , completing the proof. ", "page_idx": 27}, {"type": "text", "text": "I Identification limit of general causal models with soft interventions ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "While Theorem 1 guarantees identifiability with general environments, it only applies to linear causal models. In this section, we show that if we have access to single-node soft interventions, then we can identify general non-parametric causal models up to $\\sim_{\\mathrm{sur}}$ . To obtain our identifiability result, we also require that the environments are non-degenerate in the following sense: ", "page_idx": 27}, {"type": "text", "text": "Definition 11 (Non-degeneracy set of interventions). Let ${\\hat{\\operatorname{\\boldmath~\\mu~}}}(z_{i}\\mid z_{\\mathrm{pa}_{\\mathcal{G}}(i)})\\;,k\\in[K_{i}]$ be conditional probability densities at node $i$ , then $\\{\\hat{p}_{k}\\}_{k=1}^{K_{i}}$ is said to be non-degenerate on node $i$ at point $\\hat{z}\\in\\mathbb{R}^{d}$ if all these conditional densities are well-defined and positive at , and the matrix ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left[\\frac{\\partial\\left(\\hat{p}_{1}/\\hat{p}_{k}\\right)}{\\partial z_{j}}\\right]_{2\\leqslant k\\leqslant K_{i},j\\in\\bar{\\mathtt{p a}}_{\\mathcal{G}}(i)}\\left.\\right|_{z=\\hat{z}}\\in\\mathbb{R}^{\\left(K_{i}-1\\right)\\times\\left(\\left|\\mathtt{p a}_{\\mathcal{G}}(i)\\right|+1\\right)}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "has full row rank. Moreover, we say that $\\{\\hat{p}_{k}\\}_{k=1}^{K_{i}}$ is non-degenerate in a point set $o$ if for all $\\hat{z}\\in O$ , it is non-degenrate at $\\hat{z}$ . ", "page_idx": 27}, {"type": "text", "text": "The following lemma shows how Definition 11 is related to Assumption 5 in the linear setting: ", "page_idx": 27}, {"type": "text", "text": "Lemma 16. Suppose that $\\begin{array}{r}{\\hat{p}_{k}(z)\\;=\\;\\prod_{i=1}^{d}\\hat{p}_{k}\\left(z_{i}\\mid z_{\\mathrm{pa}_{\\mathcal{G}}(i)}\\right),k\\;\\in\\;[K]}\\end{array}$ be probability distributions of latent variables $_{z}$ generated from the linear causal models (3), such that for $\\forall i\\in[d]$ , ${\\hat{p}}_{k}\\left(z_{i}\\mid z_{\\mathrm{pa}_{\\mathcal{G}}(i)}\\right),k\\,\\in\\,[K]$ are non-degenerate on node $i$ in the sense of Definition $_{l l}$ . Then the corresponding matrices $B_{k},k\\in[K]$ satisfy Assumption 5. ", "page_idx": 27}, {"type": "text", "text": "Now we are ready to state our main result in this section: ", "page_idx": 27}, {"type": "text", "text": "Theorem 7. Suppose that we have access to observations generated from multiple environments $\\{P_{X}^{E}\\}_{E\\in\\mathfrak{E}}$ . Let $\\left(\\hat{h},\\hat{\\mathcal{G}}\\right)$ be any candidate solution with data generated according to Assumption $I$ with latent variables ${\\pmb v}=\\hat{\\pmb h}({\\pmb x})$ and joint distribution $q_{E}$ with factors $q_{i}^{E}$ . Assuming that ", "page_idx": 27}, {"type": "text", "text": "(i) the joint densities $\\{p_{E}(z)\\}_{E\\in\\mathfrak{E}}$ are continuous differentiable on $\\mathbb{R}^{d}$ with common support $O_{z}$ , and $\\{q_{E}(\\boldsymbol{v})\\}_{E\\in\\mathfrak{E}}$ are continuous differentiable on $\\mathbb{R}^{d}$ with common support $O_{v}$ ;   \n(ii) we have access to multiple single-node soft interventions on each node with unknown targets: there exists a partition $\\mathfrak{E}=\\bar{\\cup}_{i=1}^{d}\\mathfrak{E}_{i}$ such that $\\mathcal{T}_{z}^{\\mathfrak{E}_{i}}=\\{\\pi(i)\\},\\mathcal{T}_{v}^{\\mathfrak{E}_{i}}=\\{\\pi^{\\prime}(i)\\},\\forall i\\in[d]$ for some unknown permutations $\\pi$ and $\\pi^{\\prime}$ on $[d]$ ; ", "page_idx": 27}, {"type": "text", "text": "(iii) the intervention distributions on each node are non-degenerate in the sense of Definition $_{l l}$ : there exists $N_{z}\\ \\subseteq\\ O_{z}$ and $N_{v}\\,\\subseteq\\,O_{v}$ satisfying $N_{z}^{\\mathrm{o}}\\,=\\,N_{v}^{\\mathrm{o}}\\,=\\,\\emptyset$ where $S^{\\mathrm{o}}$ denotes the interior of a set $S$ , such that for all $i~\\in~[d]$ , $\\left\\{p_{i}^{E}(\\cdot):E\\in\\mathfrak{E}_{\\pi^{-1}(i)}\\right\\}$ (resp. $\\left\\{q_{i}^{E}(\\cdot):E\\in\\mathfrak{E}_{\\pi^{\\prime-1}(i)}\\right\\})$ is non-degenerate on node i in $O_{z}\\setminus N_{z}$ (resp. $O_{v}\\setminus N_{v})$ . ", "page_idx": 28}, {"type": "text", "text": "Then we must have $(h,\\mathcal{G})\\sim_{\\mathrm{sur}}\\ (\\hat{h},\\hat{\\mathcal{G}}).$ . ", "page_idx": 28}, {"type": "text", "text": "Previous works on the identifiability of non-parametric causal models typically require that all the joint distributions are supported on the whole space $\\mathbb{R}^{d}$ [49, 23, 47]. In contrast, we only assume that the densities have common and unknown support across all interventions. ", "page_idx": 28}, {"type": "text", "text": "Theorem 7 can be regarded as a soft-intervention version of 49, Theorem 4.3, which assumes access to hard interventions and only need two paired interventions per node. While they are able to show full identifiability, we show in the following that identifiability up to $\\sim_{\\mathrm{sur}}$ is the best we can hope for with soft interventions. ", "page_idx": 28}, {"type": "text", "text": "Theorem 8 (Counterpart to Theorem 7, informal version of Theorem 10). For any causal model $(h,\\mathcal{G})$ and any set of environments ${\\mathfrak{E}}=\\{E_{k}:k\\in[K]\\}$ such that all conditions in Theorem 7 are satisfied, there must exists a candidate solution $(\\hat{h},\\mathcal{G})$ and a hypothetical data generating process that satisfy the same set of conditions, but ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{\\partial{\\pmb v}_{i}}{\\partial z_{j}}\\neq0,\\quad\\forall j\\in\\overline{{\\mathrm{sur}}}_{\\mathcal{G}}(i).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Finally, the ambiguity still exists $i f$ we additionally assume standard axioms such as causal minimality (Assumption 6) and faithfulness (Assumption 7) on the causal model. ", "page_idx": 28}, {"type": "text", "text": "I.1 Proof of Lemma 16 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Let $\\pmb{w}_{k}(i)\\ \\in\\ \\mathbb{R}^{|\\mathrm{pa}_{g}(i)|}$ be the vector obtained by removing all zero entries in the $i$ -th row of $A_{k}$ and $\\omega_{k,i,i}$ be the $i$ -th diagonal entry in $\\Omega_{k}$ , then for the $k$ -th environment we have $z_{i}=$ $\\pmb{w}_{k}(i)^{\\top}\\pmb{z}_{\\mathrm{pa}_{\\mathcal{G}}(i)}+\\omega_{k,i,i}^{\\frac{1}{2}}\\epsilon_{i}$ , so that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{p}_{k}\\left(z_{i}\\mid z_{\\mathrm{pa}_{\\mathcal{G}}(i)}\\right)=\\omega_{k,i,i}^{-\\frac{1}{2}}p_{\\epsilon_{i}}\\left(\\omega_{k,i,i}^{-\\frac{1}{2}}(z_{i}-\\langle w_{k}(i),z_{\\mathrm{pa}_{\\mathcal{G}}(i)}\\rangle)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $p_{\\epsilon_{i}}(\\cdot)$ is the density of $\\epsilon_{i}$ . As a result, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla\\frac{\\hat{p}_{1}}{\\hat{p}_{k}}\\left(z_{i}\\mid z_{\\mathsf{p a}_{\\mathcal{G}}(i)}\\right)=\\frac{\\hat{p}_{1}}{\\hat{p}_{k}}\\left(z_{i}\\mid z_{\\mathsf{p a}_{\\mathcal{G}}(i)}\\right)\\cdot\\nabla\\log\\frac{\\hat{p}_{1}}{\\hat{p}_{k}}\\left(z_{i}\\mid z_{\\mathsf{p a}_{\\mathcal{G}}(i)}\\right)\\qquad\\qquad\\qquad\\qquad\\quad}\\\\ {=\\frac{\\hat{p}_{1}}{\\hat{p}_{k}}\\left(z_{i}\\mid z_{\\mathsf{p a}_{\\mathcal{G}}(i)}\\right)\\cdot\\left[c_{i1}(1,-\\boldsymbol{w}_{1}(i))-c_{i k}(1,-\\boldsymbol{w}_{k}(i))\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where for convenience we use $\\nabla$ to denote the gradient with respect to all variables $z_{\\overline{{\\mathrm{pa}}}_{G}(i)}$ , and $\\begin{array}{r}{c_{i k}=\\omega_{k,i,i}^{-\\frac{1}{2}}\\cdot\\frac{p_{\\epsilon_{i}}^{\\prime}}{p_{\\epsilon_{i}}}\\left(\\omega_{k,i,i}^{-\\frac{1}{2}}(z_{i}-\\langle w_{k}(i),z_{\\mathrm{pa}_{G}(i)}\\rangle\\right)}\\end{array}$ (we omit the dependency on $_{z}$ for simplicity). Definition 11 implies that span $\\langle c_{i1}(1,-{\\pmb w}_{1}(i))-c_{i k}(1,-{\\pmb w}_{k}(i)):2\\leqslant k\\leqslant K\\rangle\\;=\\;\\mathbb{R}^{\\left|\\mathrm{pa}_{\\mathcal{G}}(i)\\right|+1},$ thus it holds that $\\mathrm{span}\\langle(1,-{\\pmb w}_{k}(i)):k\\in[K]\\rangle=\\mathbb{R}^{|{\\mathrm{pa}}_{\\mathcal{G}}(i)|+1}$ as well. By definition of $B_{k}$ , this immediately implies that dim (sp $\\operatorname{an}\\left\\langle(B_{k})_{i}:k\\in[K]\\right\\rangle)=\\left|\\operatorname{pa}_{\\mathcal{G}}(i)\\right|+1$ as desired. ", "page_idx": 28}, {"type": "text", "text": "I.2 Proof of Theorem 7 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Define $\\tau:=\\hat{\\pmb{h}}\\circ\\pmb{h}^{-1}:\\mathbb{R}^{d}\\mapsto\\mathbb{R}^{d}$ , then we have that $\\pmb{v}=\\pmb{\\tau}(\\pmb{z})$ . Since both $^h$ and $\\hat{h}$ are diffeomorphisms by assumption, so is $\\tau$ . To avoid confusion, in this section we use $_{z}$ (resp. $\\pmb{v}$ ) to denote random variables while using $\\hat{z}$ (resp. $\\hat{\\pmb v}$ ) to denote (deterministic) vectors. ", "page_idx": 28}, {"type": "text", "text": "Let $\\mathfrak{E}_{j}\\,=\\,\\Bigl\\{E_{k}^{(j)}:k\\in[K_{j}]\\Bigr\\}$ be the $j$ -th collection of environments according to our assumption. We first prove the following lemma: ", "page_idx": 28}, {"type": "text", "text": "Lemma 17. $O_{v}=\\tau(O_{z})$ . ", "page_idx": 28}, {"type": "text", "text": "Proof. By the change of variable formula [35], for $\\forall\\hat{z}\\ \\in\\ \\mathbb{R}^{d}$ and $\\forall E\\,\\in\\,{\\mathfrak{E}}$ we have $p_{E}(\\hat{z})\\;=\\;$ $q_{E}(\\hat{\\pmb{v}})\\,|\\mathrm{det}\\,J_{\\pmb{\\tau}}(\\hat{z})|$ , where $\\hat{\\pmb{v}}=\\pmb{\\tau}(\\hat{z})$ . Since $\\tau$ is a diffeomorphism, we must have $\\vert\\operatorname*{det}J_{\\tau}(\\hat{z})\\vert\\neq0$ , so $\\dot{z}\\in O_{z}\\Leftrightarrow\\dot{v}=\\tau(\\hat{z})\\in O_{v}$ , concluding the proof. \u53e3 ", "page_idx": 29}, {"type": "text", "text": "Lemma 18. Let $\\hat{z}\\in O_{z}$ . For $\\forall j\\in[d]$ and $2\\leqslant k\\leqslant K_{j}$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{p_{j}^{E_{k}^{(j)}}}{p_{j}^{E_{1}^{(j)}}}\\left(\\hat{z}_{j}\\mid\\hat{z}_{\\mathrm{pa}_{\\mathcal{G}}(j)}\\right)=\\frac{q_{j}^{E_{k}^{(j)}}}{q_{j}^{E_{1}^{(j)}}}\\left(\\hat{v}_{j}\\mid\\hat{v}_{\\mathrm{pa}_{\\hat{\\mathcal{G}}}(j)}\\right),\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\hat{v}=\\tau(\\hat{z})\\in O_{v}$ . ", "page_idx": 29}, {"type": "text", "text": "Proof. Since $\\pmb{v}=\\pmb{\\tau}(\\pmb{z})$ , by the change-of-measure formula [35] we have that for $\\forall\\hat{z}\\in O_{z}$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\prod_{i=1}^{d}p_{i}^{E}\\left(\\hat{z}_{i}\\mid\\hat{z}_{\\mathsf{p a}_{\\mathcal{G}}(i)}\\right)=p_{E}(\\hat{z})=q_{E}(\\hat{v})\\left|\\operatorname*{det}J_{\\tau}(\\hat{z})\\right|=\\prod_{i=1}^{d}q_{i}^{E}\\left(\\tau_{i}(\\hat{z})\\mid\\tau_{\\mathsf{p a}_{\\mathcal{G}}(i)}(\\hat{z})\\right)\\left|\\operatorname*{det}J_{\\tau}(\\hat{z})\\right|.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "for all $E\\in\\mathfrak{E}_{j}$ , where $\\hat{\\pmb v}=\\pmb\\tau(\\hat{z})$ . By Assumption $(i i)$ and Definition 2, we know that $p_{i}^{E_{k}^{1}}=p_{i}^{E_{1}^{(1)}}\\Leftrightarrow$ $i\\neq1$ and $q_{i}^{E_{k}^{\\mathrm{i}}}=q_{i}^{E_{1}^{(1)}}\\Leftrightarrow i\\neq1$ for all $k>1$ . Thus, we have that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\prod_{i=1}^{d}\\frac{p_{i}^{E_{k}^{(j)}}\\left(\\hat{z}_{i}\\mid\\hat{z}_{\\mathrm{pa}_{\\mathcal G}(i)}\\right)}{p_{i}^{E_{1}^{(j)}}\\left(\\hat{z}_{i}\\mid\\hat{z}_{\\mathrm{pa}_{\\mathcal G}(i)}\\right)}=\\frac{p_{j}^{E_{k}^{(j)}}}{p_{j}^{E_{1}^{(j)}}}(\\hat{z}_{j}\\mid\\hat{z}_{\\overline{{\\mathrm{pa}}}_{\\mathcal G}(j)})\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\prod_{i=1}^{d}\\frac{q_{i}^{E_{k}^{(j)}}\\left(\\hat{v}_{i}\\mid\\hat{v}_{\\mathrm{pa}_{\\hat{g}}(i)}\\right)}{q_{i}^{E_{1}^{(j)}}\\left(\\hat{v}_{i}\\mid\\hat{v}_{\\mathrm{pa}_{\\hat{g}}(i)}\\right)}=\\frac{q_{j}^{E_{k}^{(j)}}}{q_{j}^{E_{1}^{(j)}}}(\\hat{v}_{j}\\mid\\hat{v}_{\\overline{{{\\mathrm{pa}}}}_{\\mathcal{G}}(j)}).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Since the LHS of the above two equations are the same by (23), the RHS must also be the same, concluding the proof. \u53e3 ", "page_idx": 29}, {"type": "text", "text": "We assume WLOG that the vertices of $\\mathcal{G}$ are labelled such that $i\\rightarrow j\\Rightarrow i<j$ , and that $\\pi(i)=$ $i,\\forall i\\in[d]$ . Also we can assume the nodes are fixed and only consider how they are connected, $i.e.$ , $\\pi^{\\prime}(i)=i,\\forall i\\in[d]$ . 1 ", "page_idx": 29}, {"type": "text", "text": "Lemma 19. We have $(\\tau(N_{z}))^{\\circ}=\\left(\\tau^{-1}(N_{v})\\right)^{\\circ}=\\varnothing.$ ", "page_idx": 29}, {"type": "text", "text": "Proof. The result immediately follows from the assumption that $N_{z}^{\\mathrm{o}}\\,=\\,N_{v}^{\\mathrm{o}}\\,=\\,\\emptyset$ and that $\\tau$ is a diffeomorphism. \u53e3 ", "page_idx": 29}, {"type": "text", "text": "For any vertex set $V$ , we use ${\\mathcal{G}}_{V}$ to denote its corresponding induced subgraph of $\\mathcal{G}$ . We first prove the following statements by induction on $j$ : ", "page_idx": 29}, {"type": "text", "text": "(1) $\\forall i\\neq j,i\\in\\mathrm{pa}_{\\mathcal{G}}(j)\\Leftrightarrow i\\in\\mathrm{pa}_{\\mathcal{G}^{\\prime}}\\left(j\\right);$   \n(2) $\\forall j\\in[d]$ , there exists a continuously differentiable function $\\phi_{i}$ such that $\\begin{array}{r}{\\pmb{v}_{j}=\\phi_{j}\\left(z_{\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}(j)}\\right)}\\end{array}$ . Moreover, \u2202\u2202\u03d5zjj \u0338\u22610 (i.e., not always zero).   \n(3) $\\forall j\\ \\in\\ [d]$ , there exists a continuously differentiable function $\\Upsilon_{j}$ such that $\\pmb{v}_{\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}(j)}~=$ $\\Upsilon_{j}(z_{\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}(j)})$ . ", "page_idx": 29}, {"type": "text", "text": "For $j=1$ , by assumption $\\mathrm{pa}_{\\mathcal{G}}(j)=\\emptyset$ . Lemma 18 implies that for any $\\hat{z}\\in O_{z}$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{p_{1}^{E_{k}^{(1)}}}{p_{1}^{E_{1}^{(1)}}}(\\hat{z}_{1})=\\frac{q_{1}^{E_{k}^{(1)}}}{q_{1}^{E_{1}^{(1)}}}\\left(\\hat{v}_{1}\\mid\\hat{v}_{\\mathrm{pa}_{\\hat{\\mathcal{G}}}(1)}\\right),\\forall2\\leqslant k\\leqslant K_{1}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "1This is also WLOG because we now have groups of soft interventions where each group corresponds to a single node, so we can just relabel the node in $\\hat{\\mathcal G}$ that corresponds to the $i$ -th group as node $i$ . ", "page_idx": 29}, {"type": "text", "text": "Then for $\\forall i\\in{\\overline{{\\operatorname{pa}}}}_{\\hat{\\mathcal{G}}}$ (1), taking the partial derivative w.r.t $\\pmb{v}_{j}$ gives ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{\\partial}{\\partial{\\hat{v}}_{i}}\\frac{q_{1}^{E_{k}^{(1)}}}{q_{1}^{E_{1}^{(1)}}}\\left({\\hat{v}}_{1}\\mid{\\hat{v}}_{\\mathsf{p a}_{\\hat{\\phi}}(1)}\\right)=\\left(\\frac{p_{1}^{E_{k}^{(1)}}}{p_{1}^{E_{1}^{(1)}}}\\right)^{\\prime}({\\hat{z}}_{1})\\cdot\\frac{\\partial{\\hat{z}}_{1}}{\\partial{\\hat{v}}_{i}}\\Rightarrow\\nabla_{\\mathsf{v}_{\\perp\\hat{\\phi}}(1)}\\frac{q_{1}^{E_{k}^{(1)}}}{q_{1}^{E_{1}^{(1)}}}\\left({\\hat{v}}_{1}\\mid{\\hat{v}}_{\\mathsf{p a}_{\\hat{\\phi}}(1)}\\right)=\\left(\\frac{p_{1}^{E_{k}^{(1)}}}{p_{1}^{E_{1}^{(1)}}}\\right)^{\\prime}({\\hat{z}}_{1})\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Thus, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathrm{rank}\\left[\\nabla_{v_{\\overline{{\\mathrm{pa}}}_{\\hat{Q}}(^{(1)})}}\\frac{q_{1}^{E_{k}^{(1)}}}{q_{1}^{E_{1}^{(1)}}}\\left(\\hat{v}_{1}\\mid\\hat{v}_{\\mathrm{pa}_{\\hat{Q}}(1)}\\right):2\\leqslant k\\leqslant K_{1}\\right]\\leqslant1.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Note that the above inequality holds for $\\forall\\hat{\\pmb{v}}\\in O_{v}$ . If $\\mathrm{pa}_{\\hat{\\mathcal{G}}}(1)\\neq\\emptyset$ , then this would contradict the non-degeneracy assumption $(i i i)$ which implies that the above matrix should have rank $\\geqslant2$ at some point $\\hat{\\pmb{v}}\\in{\\cal O}_{v}$ . Hence we must have $\\mathrm{pa}_{\\hat{\\mathcal{G}}}(1)=\\emptyset$ , implying that (1) holds for $j=1$ . ", "page_idx": 30}, {"type": "text", "text": "Taking the derivative of both sides of (24) w.r.t $z_{i},i\\geqslant2$ implies that $\\begin{array}{r}{\\left(\\frac{g_{k}^{(1)}}{q_{1}^{E_{1}^{(1)}}}\\right)^{\\prime}(\\hat{v}_{1})\\cdot\\frac{\\partial\\hat{v}_{1}}{\\partial\\hat{z}_{i}}=0.}\\\\ {q_{1}^{E_{1}^{(1)}}}\\end{array}$ . By our assumption $(i i i)$ , for $\\forall\\hat{\\pmb{v}}\\in O_{v}\\setminus N_{v}$ , there exists $2\\leqslant k\\leqslant K_{1}$ such that $\\left(\\frac{q_{k}^{(1)}}{q_{1}^{E_{1}^{(1)}}}\\right)^{\\prime}\\,(\\hat{\\pmb{v}}_{1})\\neq0,$ and thus we have \u2202\u2202vz\u02c6\u02c6i1 $\\begin{array}{r}{\\frac{\\partial\\hat{v}_{1}}{\\partial\\hat{z}_{i}}=0,\\forall\\hat{z}\\in\\tau^{-1}\\left(O_{v}\\setminus N_{v}\\right)}\\end{array}$ . Since $\\tau$ is a diffeomorphism, we can deduce that $\\tau^{-1}\\left(O_{v}\\setminus N_{v}\\right)=O_{z}\\setminus\\tau^{-1}\\left(N_{v}\\right)$ and $\\left(\\tau^{-1}\\left(N_{v}\\right)\\right)^{\\mathrm{o}}=\\varnothing$ by Lemma 19. As a result, we actually have $\\begin{array}{r}{\\frac{\\partial\\hat{\\pmb{v}}_{1}}{\\partial\\hat{z}_{i}}=0,\\forall\\hat{z}\\in O_{z}}\\end{array}$ . Hence in $O_{z}$ there exists a continuous differentiable function $\\phi_{1}$ such that $\\pmb{v}_{1}=\\phi_{1}(z_{1})$ , proving (2). Finally, (3) directly follows from (2) since $\\mathrm{pa}_{\\mathcal{G}}(1)=\\emptyset$ , concluding the proof for $j=1$ . ", "page_idx": 30}, {"type": "text", "text": "Now suppose that the statement holds up to $j-1$ , and we need to prove it for $j$ . Again by Lemma 18 we have for $\\forall\\hat{z}\\in O_{z}$ that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{p_{j}^{E_{k}^{(j)}}}{p_{j}^{E_{1}^{(j)}}}\\left(\\hat{z}_{j}\\mid\\hat{z}_{\\mathrm{pa}_{\\mathcal{G}}(j)}\\right)=\\frac{q_{j}^{E_{k}^{(j)}}}{q_{j}^{E_{1}^{(j)}}}\\left(\\hat{v}_{j}\\mid\\hat{v}_{\\mathrm{pa}_{\\hat{\\mathcal{G}}}(j)}\\right),\\quad\\forall2\\leqslant k\\leqslant K_{j}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "For all $i\\not\\in\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}(j)$ , taking partial derivative w.r.t. $z_{i}$ gives ", "page_idx": 30}, {"type": "equation", "text": "$$\n0=\\sum_{\\ell\\in\\overline{{\\mathrm{pa}}}_{\\hat{\\mathcal{G}}}(j)}\\frac{\\partial}{\\partial\\hat{w}_{\\ell}}\\frac{q_{j}^{E_{k}^{(j)}}}{q_{j}^{E_{1}^{(j)}}}\\left(\\hat{v}_{j}\\mid\\hat{v}_{\\mathrm{pa}_{\\hat{\\mathcal{G}}}(j)}\\right)\\cdot\\frac{\\partial\\hat{v}_{\\ell}}{\\partial\\hat{z}_{i}},\\quad\\forall2\\leqslant k\\leqslant K_{j},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "i.e. ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\left[\\nabla_{v_{\\overline{{\\mathrm{pa}}}_{\\hat{\\mathcal{G}}}(j)}}\\frac{q_{j}^{E_{k}^{(j)}}}{q_{j}^{E_{1}^{(j)}}}\\left(\\hat{v}_{j}\\ |\\ \\hat{v}_{\\mathrm{pa}_{\\hat{\\mathcal{G}}}(j)}\\right):2\\leqslant k\\leqslant K_{j}\\right]^{\\top}\\frac{\\partial\\hat{v}_{\\overline{{\\mathrm{pa}}}_{\\hat{\\mathcal{G}}}(j)}}{\\partial\\hat{z}_{i}}=0.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Similar to the $j=1$ case, by assumption $(i i i)$ , we know that the above corfficient matrix has full row rank for $\\forall\\hat{\\pmb{v}}\\in O_{v}\\setminus N_{v}$ , so for $\\forall z\\in\\tau^{-1}\\left(O_{v}\\setminus N_{v}\\right)=O_{z}\\setminus\\tau^{-1}\\left(N_{v}\\right)$ , we have $\\frac{\\partial\\hat{\\pmb{v}}_{\\overline{{{\\mathrm{pa}}}}_{\\hat{\\mathcal{G}}}(j)}}{\\partial\\hat{\\pmb{z}}_{i}}\\,=\\,0$ . Since $\\left(\\tau^{-1}(N_{v})\\right)^{\\mathrm{o}}\\;=\\;\\emptyset$ by Lemma 19, for all $\\hat{z}~\\in~N_{z}$ we can choose a sequence of points $\\hat{z}^{(i)},i\\,=\\,1,2,\\cdot\\cdot\\cdot$ in $O_{z}$ such that $\\hat{z}^{(i)}~\\rightarrow~\\hat{z}$ . Since $\\tau$ is a diffeomorphism, its derivatives are continuous and we can deduce that $\\begin{array}{r}{\\frac{\\partial\\hat{\\pmb{v}}_{\\overline{{\\mathbb{P}^{\\mathtt{a}}}}_{\\hat{\\mathcal{G}}}}(j)}{\\partial\\hat{z}_{i}}=\\operatorname*{lim}_{\\ell\\rightarrow+\\infty}\\frac{\\partial\\hat{\\pmb{v}}_{\\overline{{\\mathbb{P}^{\\mathtt{a}}}}_{\\hat{\\mathcal{G}}}(j)}^{(\\ell)}}{\\partial\\hat{\\pmb{z}}_{i}^{(\\ell)}}=0}\\end{array}$ = 0. As a result, p\u2202az\u02c6 G\u02c6i ( $\\frac{\\partial\\hat{\\pmb{v}}_{\\overline{{{\\mathtt{p a}}}}_{\\hat{\\mathcal{G}}}}(j)}{\\partial\\hat{\\pmb{z}}_{i}}\\,=\\,0$ actually holds for all $z\\in O_{z}$ . Hence, there exists a continuous differentiable function $\\Upsilon_{j}$ such that $\\begin{array}{r}{\\pmb{v}_{\\overline{{\\mathrm{pa}}}_{\\hat{\\mathcal{G}}}(j)}=\\Upsilon_{j}\\left(z_{\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}(j)}\\right)}\\end{array}$ . ", "page_idx": 30}, {"type": "text", "text": "By our assumption, $\\mathrm{pa}_{\\mathcal{G}}(j)\\subseteq[j\\!-\\!1]$ . Suppose that $\\mathrm{pa}_{\\hat{\\mathcal{G}}}(j)\\nsubseteq\\{i:i<j\\}$ , let $\\ell\\in\\mathrm{{pa}}_{\\hat{\\mathcal{G}}}(j)\\backslash\\{i:i<j\\}$ , then by induction hypothesis, $\\hat{\\pmb{v}}_{t}=\\pmb{\\tau}_{t}(\\hat{\\pmb{z}}),\\hat{\\pmb{z}}\\in\\pmb{O}_{z}$ , $t=1,2,\\cdot\\cdot\\cdot,j,\\ell$ are all functions of $\\hat{z}_{1},\\cdot\\cdot\\cdot,\\hat{z}_{j}$ . Since $\\tau$ is a diffeomorphism and $O_{z}$ is the support of the distributions $p_{E},E\\in{\\mathfrak{E}}$ , we can deduce that the support of the latent variables $(\\pmb{v}_{t}:t=1,2,\\cdots,j,\\ell)$ lie on a submanifold with dimension $\\leqslant j$ , which is impossible since $\\pmb{v}$ is supported on the open set $O_{v}\\subseteq\\mathbb{R}^{d}$ by assumption $(i)$ . ", "page_idx": 30}, {"type": "text", "text": "Hence, we must have $\\mathrm{pa}_{\\hat{\\mathcal{G}}}(j)\\,\\subseteq\\,\\{i:i<j\\}$ . Furthermore, if there exists $i\\,\\in\\,\\mathrm{pa}_{\\hat{\\mathcal{G}}}(j)$ such that $i\\not\\in\\mathrm{pa}_{\\mathcal{G}}(j)$ , then the induction hypothesis implies that $\\begin{array}{r}{\\frac{\\partial\\pmb{v}_{i}}{\\partial\\pmb{z}_{i}}\\equiv0}\\end{array}$ , but $\\pmb{v}_{i}$ is a function of $z_{\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}(j)}$ as previously derived, which is also a contradiction. Thus we actually have $\\mathrm{pa}_{\\hat{\\mathcal{G}}}(j)\\subseteq\\mathrm{pa}_{\\mathcal{G}}(j)$ . ", "page_idx": 31}, {"type": "text", "text": "In a completely symmetric manner, we can take the derivatives of (25) w.r.t. $\\pmb{v}_{i},\\forall i\\in\\overline{{\\mathrm{pa}}}_{\\hat{\\mathcal{G}}}(j)$ and obtain that $\\mathrm{pa}_{\\mathcal{G}}(j)\\subseteq\\mathrm{pa}_{\\hat{\\mathcal{G}}}(j)$ . Hence, $\\mathrm{pa}_{\\hat{\\mathcal{G}}}(j)=\\mathrm{pa}_{\\mathcal{G}}(j)$ , completing the proof of (1) and (3) for the $j$ case. ", "page_idx": 31}, {"type": "text", "text": "Finally, if \u2202\u2202vzjj $\\begin{array}{r}{\\frac{\\partial{\\pmb v}_{j}}{\\partial{\\pmb z}_{j}}\\equiv0}\\end{array}$ , then by (3) and the induction hypothesis, $\\pmb{v}_{1},\\cdots,\\pmb{v}_{j}$ are all functions of $z_{[j-1]}$ , awshsiucmh pitmiopnl .t hTaht $(\\pmb{v}_{1},\\cdot\\cdot\\cdot\\mathrm{~,~}\\pmb{v}_{j})$ T lhiiess  coon ma psleutbesm tahneif porldo owf iotfh  odiurm ienndsuioctni $\\leqslant j-1$ , again contradicting $(i)$ $\\begin{array}{r}{\\frac{\\partial\\pmb{v}_{j}}{\\partial\\pmb{z}_{j}}\\not\\equiv0}\\end{array}$ ", "page_idx": 31}, {"type": "text", "text": "To recap, we now know that ", "page_idx": 31}, {"type": "text", "text": "\u2022 $\\mathcal G=\\hat{\\mathcal G}$ , and \u2022 For $\\forall i\\in[d]$ , there exists a function $\\Upsilon_{i}$ such that $\\begin{array}{r}{{\\pmb v}_{\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}(i)}=\\Upsilon_{i}\\left(z_{\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}(i)}\\right)}\\end{array}$ . ", "page_idx": 31}, {"type": "text", "text": "It remains to show that for $\\forall k\\in\\mathrm{pa}_{\\mathcal{G}}(i)\\setminus\\mathrm{sur}_{\\mathcal{G}}(i),\\Upsilon_{i}$ doesn\u2019t depend on $z_{k}$ ", "page_idx": 31}, {"type": "text", "text": "By definition, if $k\\in\\mathrm{pa}_{\\mathcal{G}}(i)\\setminus\\mathrm{sur}_{\\mathcal{G}}(i)$ , we know that there exists $j\\in\\mathrm{ch}_{\\mathscr{G}}(i)$ such that $j\\not\\in\\mathrm{ch}_{\\mathscr{G}}(k)$ . We have shown that $\\pmb{v}_{i}$ , as a component of ${\\pmb v}_{\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}}(j)$ , is a function of $z_{\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}(j)}$ . By the choice of $k$ , we have $k\\not\\in\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}(j)$ , so that $\\pmb{v}_{i}$ does not depend on $z_{k}$ . The conclusion follows. ", "page_idx": 31}, {"type": "text", "text": "J Omitted Proofs for Theorem 3 and Theorem 8 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In this section we provide detailed proofs of main ambiguity results. ", "page_idx": 31}, {"type": "text", "text": "Definition 12. We say that a matrix $M\\,\\in\\,\\mathbb{R}^{d\\times d}$ is effect-respecting for a causal graph $\\mathcal{G}$ , or $M\\in\\mathcal{M}_{\\mathrm{sur}}(\\mathcal{G}),$ , if $M_{i j}\\neq0\\Leftrightarrow j\\in\\overline{{\\mathrm{sur}}}_{\\mathcal{G}}(i)$ . We also write $M\\in\\mathcal{M}_{\\mathrm{sur}}^{0}(\\mathcal{G})$ if $_M$ is invertible and $M_{i j}\\neq0\\Rightarrow j\\in\\overline{{\\mathrm{sur}}}_{\\mathcal{G}}(i)$ . Finally, we write $M\\in\\overline{{\\mathcal{M}}}_{\\mathrm{sur}}(\\mathcal{G})$ if $M_{i j}\\neq0\\Rightarrow j\\in\\overline{{\\mathrm{sur}}}_{\\mathcal{G}}(i)$ . ", "page_idx": 31}, {"type": "text", "text": "Remark 1. By definition $\\mathcal{M}_{\\mathrm{sur}}^{0}(\\mathcal{G})$ is the set of all matrices $_M$ where $M_{i j}\\neq0,\\forall j\\notin\\overline{{\\operatorname{sur}}}_{\\mathcal{G}}(i),$ , so it can be identified as $\\mathbb{R}^{d+d_{\\mathcal{G}}}$ where $\\begin{array}{r}{d\\mathscr{G}=\\sum_{i=1}^{d}\\left|\\mathrm{sur}_{\\mathscr{G}}(i)\\right|}\\end{array}$ . Equipped with the Lebesgue measure, we have $\\mathcal{M}_{\\mathrm{sur}}(\\mathcal{G})\\subset\\mathcal{M}_{\\mathrm{sur}}^{0}(\\mathcal{G})\\subset\\overline{{\\mathcal{M}}}_{\\mathrm{sur}}(\\mathcal{G})$ and $\\overline{{\\mathcal{M}}}_{\\mathrm{sur}}(\\mathcal{G})\\setminus\\mathcal{M}_{\\mathrm{sur}}(\\mathcal{G})$ is a null set. In the remaining part of this section, we will use measure-theoretic statement for $M\\in\\mathcal{M}_{\\mathrm{sur}}(\\mathcal{G})$ in the above sense. ", "page_idx": 31}, {"type": "text", "text": "We first present a result that serves as a good starting point to understand why this is the case. It states that latent representations that are equivalent under $\\sim_{\\mathrm{sur}}$ are essentially generated from the same causal graph. ", "page_idx": 31}, {"type": "text", "text": "Proposition 5. Let $_M$ be an invertible matrix such that $M_{i j}\\neq0\\Rightarrow j\\in\\overline{{\\operatorname{sur}}}_{\\mathcal{G}}(i)$ . Suppose that the latent variables $\\boldsymbol{z}\\,\\in\\,\\mathbb{R}^{d}$ are generated from any distributions $p_{i}\\left(z_{i}\\mid z_{\\mathrm{pa}_{\\mathcal{G}}(i)}\\right),i\\in\\left[d\\right]$ with joint density $\\begin{array}{r}{p(z)=\\prod_{i=1}^{d}p_{i}\\left(z_{i}\\mid z_{\\mathrm{pa}_{\\mathcal{G}}(i)}\\right)}\\end{array}$ , then the joint density of $v=M z$ can be written as $\\begin{array}{r}{q(\\boldsymbol{v})=\\prod_{i=1}^{d}q_{i}\\left(\\pmb{v}_{i}\\mid\\pmb{v}_{\\mathrm{pa}_{\\mathcal{G}}(i)}\\right)}\\end{array}$ for some density functions $q_{i},i\\in[d]$ . ", "page_idx": 31}, {"type": "text", "text": "J.1 Proof of Proposition 5 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "We first prove the following lemma: ", "page_idx": 31}, {"type": "text", "text": "Lemma 20. Let $M\\,\\in\\,\\mathcal{M}_{\\mathrm{sur}}^{0}(\\mathcal{G})$ and latent variables $\\pmb{v}=M\\pmb{z}$ , then for $\\forall i\\in[d]$ , there exists invertible matrices $M_{i}$ and $M_{i}^{-}$ such that $\\begin{array}{r}{\\pmb{v}_{\\mathrm{pa}_{\\mathcal{G}}(i)}={M}_{i}^{-}\\pmb{z}_{\\mathrm{pa}_{\\mathcal{G}}(i)}}\\end{array}$ and $v_{\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}(i)}=M_{i}z_{\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}(i)}$ . ", "page_idx": 31}, {"type": "text", "text": "Proof. $\\forall j\\in\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}(i)$ , we know that $\\pmb{v}_{j}$ is a linear function of $z_{\\ell},\\ell\\in\\overline{{\\mathrm{sur}}}_{\\mathcal{G}}(j)$ . By Lemma 7, we know that $\\overline{{\\mathrm{sur}}}_{\\mathcal{G}}(j)\\subseteq\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}(i)$ , so each $\\bar{\\mathbf{v}_{j}},\\dot{j}\\in\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}(i)$ is a linear function of $z_{\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}(i)}$ . Thus we can write $v_{\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}(i)}=M_{i}z_{\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}(i)}$ . In the following we argue that $M_{i}$ is invertible. Let $\\pi$ be a permutation on $\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}(i)$ such that $k\\in\\mathrm{{pa}}_{\\mathcal{G}}(\\ell)\\Rightarrow\\pi(k)<\\pi(\\ell)$ (such $\\pi$ can always be chosen since $\\mathcal{G}$ is acyclic), then we can write ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\left(\\hat{\\pmb{v}}_{\\pi(j)}:j\\in\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}(i)\\right)^{\\top}=\\tilde{M}_{i}\\left(\\hat{z}_{\\pi(j)}:j\\in\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}(i)\\right)^{\\top}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $\\tilde{M}_{i}$ is an upper triangular matrix with non-zero diagonal entries by our choice of $_M$ . Since $M_{i}$ can be obtained from $\\tilde{M}_{i}$ be exchanging a few rows and columns, $M_{i}$ is invertible as well. ", "page_idx": 32}, {"type": "text", "text": "Similarly, using the fact that $\\forall j\\,\\in\\,\\mathrm{pa}_{\\mathcal{G}}(i)$ , ${\\overline{{\\operatorname{sur}}}}_{\\mathcal{G}}(j)\\,\\subseteq\\,\\operatorname{pa}_{\\mathcal{G}}(i)$ , we can prove the existence of an invertible matrix M i\u2212 such that vpaG(i) = M i\u2212 zpaG(i). \u53e3 ", "page_idx": 32}, {"type": "text", "text": "Returning to the proof of Proposition 5. Assume WLOG that the nodes of $\\mathcal{G}$ are ordered in a way such that $\\dot{i}\\in\\mathrm{pa}_{\\bar{\\mathcal{G}}}(j)\\Rightarrow i<j$ , so that $_M$ is a lower-triangular matrix. The joint density of $\\pmb{v}$ can be written as ", "page_idx": 32}, {"type": "equation", "text": "$$\nq(\\pmb{v})=\\prod_{i=1}^{d}q\\left(\\pmb{v}_{i}\\mid\\pmb{v}_{1},\\cdot\\cdot\\cdot,\\pmb{v}_{i-1}\\right).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Since $\\pmb{v}=M z$ and $_M$ is lower triangular and invertible (hence, with non-zero diagonals), we know that $(\\pmb{v}_{1},\\pmb{v}_{2},\\cdot\\cdot\\cdot\\mid,\\pmb{v}_{i-1})$ is an invertible linear function of $(z_{1},z_{2},\\cdots,z_{i-1})$ and $(\\pmb{v}_{1},\\pmb{v}_{2},\\cdot\\cdot\\cdot\\mathrm{~,~}\\pmb{v}_{i})$ is an invertible linear function of $(z_{1},z_{2},\\cdots,z_{i})$ . Let $\\pmb{\\hat{v}}=M\\pmb{\\hat{z}}\\in\\mathbb{R}^{d}$ , then we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q\\left(\\hat{v}_{i}\\mid\\hat{v}_{1},\\cdots,\\hat{v}_{i-1}\\right)=\\frac{q\\left(\\hat{v}_{1},\\hat{v}_{2},\\cdots,\\hat{v}_{i}\\right)}{q\\left(\\hat{v}_{1},\\hat{v}_{2},\\cdots,\\hat{v}_{i-1}\\right)}=\\frac{p\\left(\\hat{z}_{1},\\hat{z}_{2},\\cdots,\\hat{z}_{i}\\right)\\operatorname*{det}\\hat{M}_{1:i,1:i}}{p\\left(\\hat{z}_{1},\\hat{z}_{2},\\cdots,\\hat{z}_{i-1}\\right)\\operatorname*{det}\\hat{M}_{1:i-1,1:i-1}}}\\\\ &{\\qquad\\qquad\\qquad\\propto\\frac{p\\left(\\hat{z}_{1},\\hat{z}_{2},\\cdots,\\hat{z}_{i}\\right)}{p\\left(\\hat{z}_{1},\\hat{z}_{2},\\cdots,\\hat{z}_{i-1}\\right)}=p\\left(\\hat{z}_{i}\\mid\\hat{z}_{1},\\cdots,\\hat{z}_{i-1}\\right)=p_{i}\\left(\\hat{z}_{i}\\mid\\hat{z}_{\\mathtt{p a}_{\\mathcal{O}}(i)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $\\hat{M}_{1:i,i:i}$ denotes that top-left submatrix of $\\hat{M}$ of size $i\\times i$ , and the last step follows from the causal Markov condition (Definition 1). On the other hand, let $q_{i}\\left(\\hat{\\pmb{v}}_{i}\\mid\\hat{\\pmb{v}}_{\\mathrm{pa}_{\\mathcal{G}}(i)}\\right)$ be the conditional density of $\\pmb{v}_{i}$ on its parents at $\\hat{\\pmb{v}}\\in\\mathbb{R}^{d}$ . For $\\forall j\\in\\mathrm{pa}_{\\mathcal{G}}(i)$ , from $v=M z$ we know that $\\pmb{v}_{j}$ is a linear function of $z_{\\overline{{\\mathrm{sur}}}_{\\mathcal{G}}(j)}$ . By Lemma 20 we know that $\\hat{v}_{\\mathrm{pa}_{\\mathcal{G}}(i)}$ is a linear function of $\\hat{z}_{\\mathrm{pa}_{\\mathcal{G}}(i)}$ and $\\hat{\\pmb{v}}_{\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}(i)}$ is a linear function of $\\hat{z}_{\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}(i)}$ , so that ", "page_idx": 32}, {"type": "equation", "text": "$$\nq\\left(\\widehat{v}_{\\mathrm{pa}_{\\mathcal{G}}\\left(i\\right)}\\right)\\propto p\\left(\\widehat{z}_{\\mathrm{pa}_{\\mathcal{G}}\\left(i\\right)}\\right)\\quad\\mathrm{~and~}\\quad q\\left(\\widehat{v}_{\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}\\left(i\\right)}\\right)\\propto p\\left(\\widehat{z}_{\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}\\left(i\\right)}\\right)\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "and ", "page_idx": 32}, {"type": "equation", "text": "$$\nq_{i}\\left(\\hat{v}_{i}\\mid\\hat{v}_{\\mathrm{pa}_{\\mathcal{G}}(i)}\\right)\\propto\\frac{p\\left(\\hat{z}_{\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}(i)}\\right)}{p\\left(\\hat{z}_{\\mathrm{pa}_{\\mathcal{G}}(i)}\\right)}=p_{i}\\left(\\hat{z}_{i}\\mid\\hat{z}_{\\mathrm{pa}_{\\mathcal{G}}(i)}\\right).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Hence, we have $q_{i}\\left(\\hat{\\pmb{v}}_{i}\\mid\\hat{\\pmb{v}}_{\\mathrm{pa}_{\\mathcal{G}}(i)}\\right)\\propto q\\left(\\hat{\\pmb{v}}_{i}\\mid\\hat{\\pmb{v}}_{1},\\cdots,\\hat{\\pmb{v}}_{i-1}\\right)$ , so that ", "page_idx": 32}, {"type": "equation", "text": "$$\nq(\\boldsymbol{\\hat{v}})=\\prod_{i=1}^{d}q_{i}\\left(\\hat{v}_{i}\\mid\\hat{v}_{\\mathrm{pa}_{\\mathcal{G}}(i)}\\right)\\propto\\prod_{i=1}^{d}q_{i}\\left(\\hat{v}_{i}\\mid\\hat{v}_{\\mathrm{pa}_{\\mathcal{G}}(i)}\\right).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Since both sides integrate to 1, it turns out that they are equal, as desired. ", "page_idx": 32}, {"type": "text", "text": "J.2 Formal version and proof of Theorem 3: the linear case ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Theorem 9 (Counterpart to Theorem 1). For any causal model ${\\mathfrak{E}}=\\{E_{k}:k\\in[K]\\}$ , suppose that we have observations $\\left\\{P_{\\mathbf{X}}^{E}\\right\\}_{E\\in\\mathfrak{E}}$ $(H,{\\mathcal{G}})$ satisfying Assumption and any set of environments $^{\\,l}$ : ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\forall k\\in[K],\\quad z=A_{k}z+\\Omega_{k}^{\\frac{1}{2}}\\epsilon,\\quad x=H^{\\dagger}z\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "such that ", "page_idx": 32}, {"type": "text", "text": "(i) the unmixing matrix $H\\in\\mathbb{R}^{d\\times n}$ has full row rank; ", "page_idx": 32}, {"type": "text", "text": "(ii) $\\forall k\\in[K]$ and $i,j\\;\\in\\;[d],\\;({\\cal A}_{k})_{i j}\\;\\ne0\\,\\Leftrightarrow\\,j\\;\\in\\,\\mathrm{pa}_{\\mathcal{G}}(i)$ and $\\Omega_{k}$ is a diagonal matrix with positive entries; ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\Big\\{B_{k}=\\Omega_{k}^{-\\frac{1}{2}}(I-A_{k})\\Big\\}_{k=1}^{K}\\,a r e\\,n o d e\\,l e\\nu e l\\,n o n{\\cdot}d e g e n e r a t e\\,i n\\,t h e\\,s e n s e\\,o f A s s u m p t i o n\\,5,\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "then there must exist a candidate solution $(\\hat{H},\\mathcal{G})$ and a hypothetical data generating process ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\forall k\\in[K],\\quad v=\\hat{A}_{k}v+\\hat{\\Omega}_{k}^{\\frac{1}{2}}\\epsilon,\\quad x=\\hat{H}^{\\dagger}v\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "such that ", "page_idx": 32}, {"type": "text", "text": "$(i^{\\prime})$ the unmixing matrix $\\hat{H}\\in\\mathbb{R}^{d\\times n}$ has full row rank; ", "page_idx": 33}, {"type": "text", "text": "$(i i^{\\prime})\\ \\forall k\\in[K]$ and $i,j\\,\\in\\,[d],\\,(\\hat{A}_{k})_{i j}\\,\\neq\\,0\\,\\Leftrightarrow\\,j\\,\\in\\,\\mathrm{pa}_{\\mathcal{G}}(i)$ and $\\hat{\\Omega}_{k}$ is a diagonal matrix with positive entries; ", "page_idx": 33}, {"type": "text", "text": "$(i i i^{\\prime})\\;\\left\\{\\hat{B}_{k}=\\hat{\\Omega}_{k}^{-\\frac12}(I-\\hat{A}_{k})\\right\\}_{k=1}^{K}$ are node level non-degenerate in the sense of Assumption 5, ", "page_idx": 33}, {"type": "text", "text": "but ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\frac{\\partial\\pmb{v}_{i}}{\\partial z_{j}}\\neq0,\\quad\\forall j\\in\\overline{{\\mathrm{sur}}}_{\\mathcal{G}}(i).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Finally, if we additionally assume that ", "page_idx": 33}, {"type": "text", "text": "(iii) the environments are groups of single-node interventions: there exists a partition ${\\mathfrak{E}}=$ $\\cup_{i=1}^{d}\\mathfrak{E}_{i}$ such that $\\mathcal{T}_{z}^{\\mathfrak{E}_{i}}=\\{i\\}$ (see Definition 2), ", "page_idx": 33}, {"type": "text", "text": "then we can guarantee the existence of $(\\hat{H},\\mathcal{G})$ and weight matrices which, besides the properties listed above, also satisfy ", "page_idx": 33}, {"type": "text", "text": "$(i i i^{\\prime})$ for the same partition $\\mathfrak{E}=\\cup_{i=1}^{d}\\mathfrak{E}_{i}$ , we have $\\mathcal{T}_{v}^{\\mathfrak{E}_{i}}=\\{i\\}$ . ", "page_idx": 33}, {"type": "text", "text": "In other words, additionally assuming that the environments are from single-node interventions does not resolve the ambiguity. ", "page_idx": 33}, {"type": "text", "text": "Remark 2. Compared with our identifiability guarantee Theorem 1, Theorem 9 actually demonstrates a stronger form of impossibility. Specifically, it states that the SNA cannot be resolved even if both the ground-truth causal graph and the noise variables are known. ", "page_idx": 33}, {"type": "text", "text": "We define ", "page_idx": 33}, {"type": "equation", "text": "$$\nv=M z\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $_M$ is an effect-respecting matrix. At this point we do not make any other restrictions on $_M$ , but we will specify the appropriate choise of $_M$ later. ", "page_idx": 33}, {"type": "text", "text": "By assumption, the latent variables in the $k$ -th environment are generated by ", "page_idx": 33}, {"type": "equation", "text": "$$\nz=A_{k}z+\\Omega_{k}^{\\frac{1}{2}}\\epsilon,\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "then $\\pmb{v}=M(\\pmb{I}-\\pmb{A}_{k})^{-1}\\pmb{\\Omega}_{k}^{\\frac{1}{2}}\\epsilon$ . Let $\\hat{\\Omega}_{k}$ be the diagonal matrix with entries $M_{i i}^{2}\\cdot(\\Omega_{k})_{i i},i\\in[d]$ and $\\hat{A}_{k}=I-\\hat{\\Omega}_{k}^{\\frac{1}{2}}\\Omega_{k}^{-\\frac{1}{2}}(I-A_{k})M^{-1}$ , then $\\pmb{v}=\\hat{A}_{k}\\pmb{v}+\\hat{\\Omega}_{k}^{\\frac{1}{2}}\\epsilon$ . Note that the choice of $\\hat{\\Omega}_{k}$ here is to that the diagonal entries of $\\hat{A}_{k}$ are zero, as we show below. It remains to show that: for almost all $M\\in\\mathcal{M}_{\\mathrm{sur}}^{0}(\\mathcal{G})$ , it holds for $\\forall k\\in[K]$ that $(\\hat{A}_{k})_{i j}=0\\Leftrightarrow j\\notin\\mathrm{pa}_{\\mathcal{G}}(i)$ . ", "page_idx": 33}, {"type": "text", "text": "For the $\\Leftarrow$ direction, since $M\\in\\mathcal{M}_{\\mathrm{sur}}^{0}(\\mathcal{G}),M^{-1}\\in\\mathcal{M}_{\\mathrm{sur}}^{0}(\\mathcal{G})$ as well. Thus, $\\forall j\\notin\\mathrm{pa}_{\\mathcal{G}}(i)$ we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left[(I-A_{k})M^{-1}\\right]_{i j}=\\displaystyle\\sum_{\\ell=1}^{d}(I-A_{k})_{i\\ell}\\cdot(M^{-1})_{\\ell j}=\\displaystyle\\sum_{\\ell\\in\\overline{{\\mathbb{P a}}}_{\\ell}(i)\\cap\\{\\ell^{\\prime}:j\\in\\mathtt{s u r}_{\\ell}(\\ell^{\\prime})\\}}(I-A_{k})_{i\\ell}\\cdot(M^{-1})_{\\ell j}}&\\\\ {=\\left\\{\\begin{array}{r l}{0}&{\\mathrm{if~}j\\notin\\overline{{\\mathbb{P a}}}_{\\mathcal{G}}(i)}\\\\ {(M^{-1})_{i i}}&{\\mathrm{if~}j=i}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where the last step holds because $\\forall\\ell\\in[d],\\ell\\in\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}(i),j\\in\\overline{{\\mathrm{sur}}}_{\\mathcal{G}}(\\ell)\\Rightarrow j\\in\\overline{{\\mathrm{pa}}}_{i}$ , and when $j=i$ , the only such $\\ell$ is $\\ell=i$ . Hence, we can see that our choice of $\\hat{A}_{k}$ satisfies ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\left(\\hat{A}_{k}\\right)_{i j}=\\left\\{{\\begin{array}{c c}{0-0=0}&{{\\mathrm{if~}}j\\notin\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}(i)}\\\\ {1-{\\hat{\\omega}}_{k,i,i}^{\\frac{1}{2}}\\omega_{k,i,i}^{-\\frac{1}{2}}(M^{-1})_{i i}=0}&{{\\mathrm{if~}}j=i,}\\end{array}}\\right.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "so $\\left(\\hat{A}_{k}\\right)_{i j}\\neq0\\Rightarrow j\\in\\mathrm{pa}_{\\mathcal{G}}(i)$ . ", "page_idx": 33}, {"type": "text", "text": "Conversely, for $\\forall j\\in\\mathrm{pa}_{\\mathcal{G}}(i)$ , ", "page_idx": 33}, {"type": "equation", "text": "$$\n(\\hat{A}_{k})_{i j}=0\\Leftrightarrow\\sum_{s\\in\\overline{{\\mathbb{P a}}}_{\\mathcal{G}}(i)}(I-A_{k})_{i s}(M^{-1})_{s j}=0\\Leftrightarrow\\sum_{s\\in\\overline{{\\mathbb{P a}}}_{\\mathcal{G}}(i)}(-1)^{s}(I-A_{k})_{i s}\\operatorname*{det}M_{s j}^{-}=0\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $M_{s j}^{-}$ is the $(d-1)\\times(d-1)$ matrix obtained by removing the $s$ -th row and $j$ -th column of $_M$ , and the second step in the equation above follows from the fact that $M^{-1}=\\operatorname*{det}(M)^{-1}\\mathrm{adj}(M)$ , where adj $(M)$ denotes the adjugate matrix of $_M$ whose $(i,j)$ -th entry is $(-1)^{i+j}\\operatorname*{det}M_{i j}^{-}$ . ", "page_idx": 34}, {"type": "text", "text": "(28) holds if only if $_M$ takes values on a lower-dimensional algebraic manifold of its embedded space $\\mathbb{R}^{d+d\\varrho}$ (see Remark 1). As a result, for almost every $M\\bar{\\in}\\,\\mathcal{M}_{\\mathrm{sur}}^{0}(\\mathcal{G})$ , $\\pmb{v}$ is generated from a linear causal model with graph $\\mathcal{G}$ as defined in (3). Moreover, let $\\hat{B}_{k}=B_{k}M^{-1},k\\in[K]$ , so that $\\boldsymbol{\\epsilon}=\\hat{B}_{k}\\boldsymbol{v}$ in the $k$ -th environment. Then for all nodes $i\\in[d]$ and $S\\subseteq\\mathrm{{pa}}(i)\\cup\\{i\\}$ , we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\dim\\operatorname{span}\\left\\langle\\left(\\hat{B_{k}}^{\\top}e_{i}\\right)_{S}:k\\in[K]\\right\\rangle=\\mathrm{~dim~span}\\left\\langle M^{-\\top}\\left(\\left(B_{k}^{\\top}e_{i}\\right)_{S}:k\\in[K]\\right)\\right\\rangle}\\\\ &{=\\mathrm{~dim~span}\\left\\langle\\left(B_{k}^{\\top}e_{i}\\right)_{S}:k\\in[K]\\right\\rangle=\\left|{\\mathrm{pa}}_{\\mathcal{G}}(i)\\right|+1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "implying that $\\hat{B}_{k},k\\in[K]$ satisfy Assumption 5. ", "page_idx": 34}, {"type": "text", "text": "Now we have shown that for almost every $M\\in\\mathcal{M}_{\\mathrm{sur}}^{0}(\\mathcal{G})$ , we can construct a hypothetical data generating process with latent variables $v=M z$ that satisfies all requirements in Theorem 9. Choose an arbitrary $_M$ that is in $\\mathcal{M}_{\\mathrm{sur}}(\\mathcal{G})$ , then we have that ", "page_idx": 34}, {"type": "equation", "text": "$$\n{\\frac{\\partial\\pmb{v}_{i}}{\\partial z_{j}}}\\neq0,\\quad j\\notin\\overline{{\\mathrm{sur}}}_{\\mathcal{G}}(i).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Finally, if we additionally assume single-node interventions, $\\forall k,\\ell\\,\\in\\,\\mathfrak{E}_{i}$ , we have that $(B_{k})_{j}\\neq$ $(\\boldsymbol{B}_{\\ell})_{j}\\Leftrightarrow j=i$ . For any $M\\in\\mathcal{M}_{\\mathrm{sur}}^{0}(\\mathcal{G})$ (and specifically the $_M$ that we have already chosen above), we have $(\\hat{B}_{k})_{j}=(B_{k})_{j}M^{-1}$ and $(\\hat{B}_{\\ell})_{j}=(B_{\\ell})_{j}M^{-1},\\forall j\\in[d]$ . Thus, $(\\hat{B}_{k})_{j}\\neq(\\hat{B}_{\\ell})_{j}\\Leftrightarrow j=i$ as well, implying that $\\mathfrak{E}_{i}$ is also a group of single-node interventions on $\\pmb{v}$ , concluding the proof. ", "page_idx": 34}, {"type": "text", "text": "J.3 Formal statement and proof of Theorem 10: the non-parametric case ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Theorem 10 (Counterpart to Theorem 7). For any causal model $(h,\\mathcal{G})$ and any set of environments E, suppose that we have observations P XE E\u2208E satisfying Assumption 1: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\forall E\\in\\mathfrak{E},z\\sim p_{E}(\\hat{z})=\\prod_{i=1}^{d}p_{i}^{E}\\left(\\hat{z}_{i}\\mid\\hat{z}_{\\mathtt{p a}_{\\mathcal{G}}(i)}\\right),x=h^{-1}(z)\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "such that ", "page_idx": 34}, {"type": "text", "text": "(i) all densities $p_{i}^{E}$ are continuously differentiable and the joint density $p_{E}$ is positive everywhere;   \n(ii) the environments are groups of single-node interventions: there exists a partition ${\\mathfrak{E}}=$ $\\cup_{i=1}^{d}\\mathfrak{E}_{i}$ such that $\\mathcal{T}_{z}^{\\mathfrak{E}_{i}}=\\{i\\}$ ;   \n(iii) the intervention distributions on each node are non-degenerate: $\\forall i\\in[d].$ , the set of distributions $\\left\\{p_{i}^{E}:E\\in\\mathfrak{E}_{i}\\right\\}$ satisfy Definition $_{l l}$ at any point $\\hat{z}\\in\\mathbb R^{d}$ , ", "page_idx": 34}, {"type": "text", "text": "then there must exist a candidate solution $(\\hat{h},\\mathcal{G})$ and a hypothetical data generating process ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\forall E\\in\\mathfrak{E},v\\sim q_{E}(\\hat{v})=\\prod_{i=1}^{d}q_{i}^{E}\\left(\\hat{v}_{i}\\mid\\hat{v}_{\\mathrm{pa}_{\\mathcal{G}}(i)}\\right),x=\\hat{h}^{-1}(v)\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "such that ", "page_idx": 34}, {"type": "text", "text": "$(i^{\\prime})$ all densities $q_{i}^{E}$ are continuously differentiable and the joint density $q_{E}$ is positive everywhere;   \n$(i i^{\\prime})$ for the same partition $\\mathfrak{E}=\\cup_{i=1}^{d}\\mathfrak{E}_{i}$ , we have $\\mathcal{T}_{v}^{\\mathfrak{E}_{i}}=\\{i\\}$ ;   \n(iii\u2032) $)\\ \\forall i\\in[d]$ , the set of distributions $\\left\\{q_{i}^{E}:E\\in\\mathfrak{E}_{i}\\right\\}$ satisfy Definition $_{l l}$ at any point $\\hat{\\pmb{v}}\\in\\mathbb{R}^{d}$ , ", "page_idx": 34}, {"type": "text", "text": "but ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\frac{\\partial{\\pmb v}_{i}}{\\partial z_{j}}\\neq0,\\quad\\forall j\\in\\overline{{\\mathrm{sur}}}_{\\mathcal{G}}(i).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Remark 3. Similar to the case of Theorem 9, Appendix $J.3$ also establishes a stronger form of identifiability. First, it is assumed that the causal graph $\\mathcal{G}$ is known. Second, we only focus on a special case of the setting of Theorem 7 by assuming that the support is the whole space, and the non-degeneracy condition Definition $_{l l}$ holds at any point. Even in this case, we show that our identification guarantee up to SNA cannot be improved. ", "page_idx": 35}, {"type": "text", "text": "We state and prove a stronger version of Theorem 10: ", "page_idx": 35}, {"type": "text", "text": "Theorem 11. For any causal model $(h,\\mathcal{G})$ and any set of environments $\\mathfrak{E}$ , suppose that we have observations $\\big\\{P_{X}^{E}\\big\\}_{E\\in\\mathfrak{E}}$ satisfying Assumption $^{\\,l}$ : ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\forall E\\in\\mathfrak{E},\\quad z\\sim p_{E}(z)=\\prod_{i=1}^{d}p_{i}^{E}\\left(z_{i}\\mid z_{\\mathtt{p a}_{\\mathcal{G}}(i)}\\right),\\quad x=h^{-1}(z)\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "such that ", "page_idx": 35}, {"type": "text", "text": "(i) all densities $p_{i}^{E}$ are continuously differentiable and the joint density $p_{E}$ is positive everywhere;   \n(ii) the environments are groups of single-node interventions: there exists a partition ${\\mathfrak{E}}=$ $\\cup_{i=1}^{d}\\mathfrak{E}_{i}$ such that $\\mathcal{T}_{z}^{\\mathfrak{E}_{i}}=\\{i\\}$ ;   \n(iii) the intervention distributions on each node are non-degenerate: $\\forall i\\in[d]$ , the set of distributions $\\left\\{p_{i}^{E}:E\\in\\mathfrak{E}_{i}\\right\\}$ satisfy Definition $_{l l}$ , ", "page_idx": 35}, {"type": "text", "text": "then there must exist a candidate solution $(\\hat{h},\\mathcal{G})$ and a hypothetical data generating process ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\forall E\\in\\mathfrak{E},\\quad v\\sim q_{E}(v)=\\prod_{i=1}^{d}q_{i}^{E}\\left(v_{i}\\mid v_{\\mathrm{pa}_{\\mathcal{G}}(i)}\\right),\\quad x=\\hat{h}^{-1}(v)\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "such that ", "page_idx": 35}, {"type": "text", "text": "$(i^{\\prime})$ all densities $q_{i}^{E}$ are continuously differentiable and the joint density $q_{E}$ is positive everywhere; ", "page_idx": 35}, {"type": "text", "text": "$(i i^{\\prime})$ for the same partition $\\mathfrak{E}=\\cup_{i=1}^{d}\\mathfrak{E}_{i}$ , we have $\\mathcal{T}_{v}^{\\mathfrak{E}_{i}}=\\{i\\}$ ; ", "page_idx": 35}, {"type": "text", "text": "$(i i i^{\\prime})\\ \\forall i\\in[d]$ , the set of distributions $\\left\\{q_{i}^{E}:E\\in\\mathfrak{E}_{i}\\right\\}$ satisfy Definition $I I$ , ", "page_idx": 35}, {"type": "text", "text": "but ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\frac{\\partial\\pmb{v}_{i}}{\\partial z_{j}}\\neq0,\\quad\\forall j\\in\\overline{{\\mathrm{sur}}}_{\\mathcal{G}}(i).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Finally, if we additionally assume minimality (Assumption $^{6}$ ) and/or faithfulness (Assumption 7) of all $p_{E}$ \u2019s, we can guarantee the existence of $(\\hat{h},\\mathcal{G})$ and $q_{E}$ \u2019s satisfying minimality and/or faithfulness in addition to the properties listed above. In other words, assuming minimality and/or faithfulness does not resolve the ambiguity. ", "page_idx": 35}, {"type": "text", "text": "Proof. We define ", "page_idx": 35}, {"type": "equation", "text": "$$\nv=M z\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where $_M$ is an effect-respecting matrix. At this point we do not make any other restrictions on $_M$ , and we will choose appropriate $_M$ later. By Lemma 20, there exists invertible matrices $M_{i}$ and $M_{i}^{-}$ such that $\\begin{array}{r}{\\pmb{v}_{\\mathrm{pa}_{\\mathcal{G}}(i)}={M}_{i}^{-}z_{\\mathrm{pa}_{\\mathcal{G}}(i)}}\\end{array}$ and $v_{\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}(i)}=M_{i}z_{\\overline{{\\mathrm{pa}}}_{\\mathcal{G}}(i)}$ , so for all environment $E\\in{\\mathfrak{E}}$ we have ", "page_idx": 35}, {"type": "equation", "text": "$$\nq_{i}^{E}(v_{\\mathsf{p a}_{\\mathcal{G}}(i)})=p_{i}^{E}(z_{\\mathsf{p a}_{\\mathcal{G}}(i)})\\cdot\\left|\\operatorname*{det}(M_{i}^{-})^{-1}\\right|,\\quad q_{i}^{E}(v_{\\overline{{\\mathbb{p a}}}_{\\mathcal{G}}(i)})=p_{i}^{E}(z_{\\overline{{\\mathbb{p a}}}_{\\mathcal{G}}(i)})\\cdot\\left|\\operatorname*{det}(M_{i})^{-1}\\right|\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "so that ", "page_idx": 35}, {"type": "equation", "text": "$$\nq_{i}^{E}\\left(v_{i}\\mid\\boldsymbol{v}_{\\mathrm{pa}_{\\mathcal{G}}(i)}\\right)=p_{i}^{E}\\left(z_{i}\\mid\\boldsymbol{z}_{\\mathrm{pa}_{\\mathcal{G}}(i)}\\right)\\frac{\\left|\\operatorname*{det}M_{i}^{-1}\\right|}{\\left|\\operatorname*{det}(M_{i}^{-})^{-1}\\right|},\\quad\\forall i\\in[d].\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "In the following, assuming that $\\left(p_{i}^{E}:E\\in\\mathfrak{E}\\right)$ satisfies any of the listed assumptions, we show that $\\left(q_{i}^{E}:E\\in\\mathfrak{E}\\right)$ satisfies the same assumption as well. ", "page_idx": 36}, {"type": "text", "text": "Firstly, (30) immediately implies that the density of $\\pmb{v}$ is continuous differentiable and positive everywhere. Secondly, $\\forall k,\\ell\\in\\mathfrak{E}_{i}$ , we have that ", "page_idx": 36}, {"type": "equation", "text": "$$\np_{j}^{E_{k}}\\left(z_{j}\\mid z_{\\mathrm{pa}_{\\mathcal G}(j)}\\right)=p_{j}^{E_{\\ell}}\\left(z_{j}\\mid z_{\\mathrm{pa}_{\\mathcal G}(j)}\\right)\\Leftrightarrow j=i.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "By (30) it is easy to see that ", "page_idx": 36}, {"type": "equation", "text": "$$\nq_{j}^{E_{k}}\\left(v_{j}\\mid v_{\\mathrm{pa}_{\\mathcal{G}}(j)}\\right)=q_{j}^{E_{\\ell}}\\left(v_{j}\\mid v_{\\mathrm{pa}_{\\mathcal{G}}(j)}\\right)\\Leftrightarrow j=i\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "as well, i.e., $q^{k},k\\in\\mathfrak{E}_{i}$ are single-node interventions on $\\pmb{v}_{i}$ according to Definition 2. ", "page_idx": 36}, {"type": "text", "text": "Thirdly, we verify the non-degeneracy condition for $q_{i}^{E}$ \u2019s. Indeed we have for $\\forall k\\geqslant2$ that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathsf{\\mathcal{T}}_{v_{\\mathtt{p r a}_{\\mathcal{G}}(i)}}\\frac{q_{i}^{E_{1}}}{q_{i}^{E_{k}}}\\left(v_{i}\\mid\\boldsymbol{v}_{\\mathtt{p a}_{\\mathcal{G}}(i)}\\right)=\\frac{\\partial\\mathscr{z}_{\\mathtt{p a}_{\\mathcal{G}}(i)}}{\\partial\\boldsymbol{v}_{\\mathtt{p a}_{\\mathcal{G}}(i)}}\\nabla_{z_{\\mathtt{p a}_{\\mathcal{G}}(i)}}\\frac{q_{i}^{E_{1}}}{q_{i}^{E_{k}}}\\left(z_{i}\\mid\\boldsymbol{z}_{\\mathtt{p a}_{\\mathcal{G}}(i)}\\right)=M_{i}^{-1}\\nabla_{z_{\\mathtt{p a}_{\\mathcal{G}}(i)}}\\frac{q_{i}^{E_{1}}}{q_{i}^{E_{k}}}\\left(z_{i}\\mid\\boldsymbol{z}_{\\mathtt{p a}_{\\mathcal{G}}(i)}\\right)\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Since $M_{i}$ is invertible, the above equation and the non-degeneracy of $p^{E_{k}},k\\in[K]$ immediately implies that non-degeneracy of $q^{E_{k}},\\dot{k}\\in[K]$ . ", "page_idx": 36}, {"type": "text", "text": "Thus, for arbitrary $M\\in\\mathcal{M}_{\\mathrm{sur}}(\\mathcal{G})$ , we have constructed a hypothetical data generating process with latent variable $v=M z$ that satisfies all given conditions. It remains to show that such construction is still possible under additional minimality and faithfulness conditions. ", "page_idx": 36}, {"type": "text", "text": "Claim 1. There exists a neighbourhood $O$ of the identity matrix $\\boldsymbol{\\mathit{I}}$ in $\\overline{{\\mathcal{M}}}_{\\mathrm{sur}}(\\mathcal{G})$ (in the sense of Remark 1) such that for $\\forall M\\in O\\cap\\mathcal{M}_{\\operatorname{sur}}^{0}(\\mathcal{G}),p^{E_{k}},k\\in[\\dot{K}]$ satisfy Assumption $\\bar{7}\\Rightarrow q^{E_{k}},k\\in[K]$ satisfy Assumption 7. ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "For $\\forall i,j$ not $d$ -separated by $\\begin{array}{l l l}{S}&{\\subseteq}&{[d]}\\end{array}$ , for all $\\textit{k}\\in\\textit{}[K]$ there exists $\\hat{z}~~\\in~\\mathbb{R}^{d}$ such that $\\Delta_{k}^{(i,j,S)}\\;\\;=\\;\\;p^{E_{k}}\\left(\\hat{z}_{i},\\hat{z}_{j}\\mid\\hat{z}_{S}\\right)\\;-\\;p^{E_{k}}\\left(\\hat{z}_{i}\\mid\\hat{z}_{S}\\right)p^{E_{k}}\\left(\\hat{z}_{j}\\mid\\hat{z}_{S}\\right)\\;\\;\\neq\\;\\;0$ . By continuous differentiability of $p^{E_{k}}$ , we know that there exists $\\delta_{k}^{(i,j,S)}~>~0$ such that for all $M\\ \\in\\ \\overline{{\\mathcal{M}}}_{\\mathrm{sur}}(\\mathcal{G})$ such that $\\begin{array}{r l r}{\\|\\boldsymbol{M}-\\boldsymbol{I}\\|_{F}}&{{}\\leqslant}&{\\delta_{k}^{(i,j,S)}}\\end{array}$ \u03b4(ki,j,S), the density of the variable v = Mz satisfies $q^{E_{k}}\\left(\\hat{v}_{i},\\hat{v}_{j}~|~\\hat{v}_{S}\\right)~\\neq~q^{E_{k}}\\left(\\hat{v}_{i}~|~\\hat{v}_{S}\\right)q^{k}\\left(\\hat{v}_{j}~|~\\hat{v}_{S}\\right)$ for $\\hat{\\pmb{v}}~=~M\\hat{z}$ , which implies that $\\pmb{v}_{i}$ and $\\pmb{v}_{j}$ are dependent given $\\pmb{v}_{S}$ . Now choose $\\delta=\\mathrm{min}_{k,i,j,S}\\,\\delta_{k}^{(i,j,S)}>0$ , then for all $M\\in\\overline{{\\mathcal{M}}}_{\\mathrm{sur}}(\\mathcal{G})$ such that $\\|M-I\\|_{F}\\leqslant\\delta$ , the resulting distributions $q^{E_{k}},k\\in[K]$ satisfy assumption Assumption 6. ", "page_idx": 36}, {"type": "text", "text": "Claim 2. There exists a neighbourhood $O$ of $\\boldsymbol{\\mathit{I}}$ in $\\overline{{\\mathcal{M}}}_{\\mathrm{sur}}(\\mathcal{G})$ (in the sense of Remark 1) such that for almost all $M\\in O\\cap\\mathcal{M}_{\\mathrm{sur}}^{0}(\\mathcal{G}),p^{E_{k}},k\\in[K]$ satisfies Assumption $\\mathbf{6}\\Rightarrow p^{E_{k}},k\\in[K]$ satisfies Assumption 6. ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "The proof is similar to the previous statement. Since Assumption 6 causal minimality is satisfied for $_{\\textit{z}}$ , for $\\forall k\\in[K],i\\in[d]$ , let $\\mathcal{G}_{i j}$ be the resulting graph obtained by removing the edge $j\\rightarrow i$ from $\\mathcal{G}$ , then there must exists some $\\bar{\\alpha_{i j k}}\\in[d]$ such that $z_{\\alpha_{i j k}}\\mathrm{~\\mathcal{1}~}z_{\\mathrm{nd}_{\\mathcal{G}_{i j}}(\\alpha_{i j k})}\\mid z_{\\mathrm{pa}_{\\mathcal{G}_{i j}}(\\alpha_{i j k})}$ . Hence, there exists $\\hat{z}^{i j k}\\in\\mathbb{R}^{d}$ such that p $\\begin{array}{r}{\\frac{E_{k}}{\\varepsilon}\\left(\\hat{z}_{\\alpha_{i j k}}^{i j k}\\mid\\hat{z}_{\\mathrm{pa}_{\\mathcal{G}_{i j}}(\\alpha_{i j k})}^{i j k}\\right)p^{E_{k}}\\left(\\hat{z}_{\\mathrm{nd}_{\\mathcal{G}_{i j}}(\\alpha_{i j k})}^{i j k}\\mid\\hat{z}_{\\mathrm{pa}_{\\mathcal{G}_{i j}}(\\alpha_{i j k})}^{i j k}\\right)\\neq p^{E_{k}}\\left(\\hat{z}_{\\mathrm{nd}_{\\mathcal{G}_{i j}}(\\alpha_{i j k})}^{i j k}\\mid\\hat{z}_{\\mathrm{pa}_{\\mathcal{G}_{i j}}(\\alpha_{i j k})}^{i j k}\\right).}\\end{array}$ By continuous differentiability of $p^{E_{k}}$ , there exists $\\delta_{k}^{(i,j)}>0$ such that for all $M\\in\\bar{\\mathcal{M}}_{\\mathrm{sur}}(\\mathcal{G})$ such that $\\|M-I\\|_{F}\\leqslant\\delta_{k}^{(i,j)}$ (ki,j), the density qiEjk of the variable $\\hat{\\pmb{v}}^{i j k}=M\\hat{z}^{i j k}$ satisfies qEk $\\begin{array}{r}{_{L^{E}}\\left(\\hat{v}_{\\alpha_{i j k}}^{i j k}\\mid\\hat{v}_{\\mathrm{pa}_{\\mathcal{G}_{i j}}(\\alpha_{i j k})}^{i j k}\\right)q^{E_{k}}\\left(\\hat{v}_{\\mathrm{nd}_{\\mathcal{G}_{i j}}(\\alpha_{i j k})}^{i j k}\\mid\\hat{v}_{\\mathrm{pa}_{\\mathcal{G}_{i j}}(\\alpha_{i j k})}^{i j k}\\right)\\neq q^{E_{k}}\\left(\\hat{v}_{\\mathrm{nd}_{\\mathcal{G}_{i j}}(\\alpha_{i j k})}^{i j k}\\mid\\hat{v}_{\\mathrm{pa}_{\\mathcal{G}_{i j}}(\\alpha_{i j k})}^{i j k}\\right).}\\end{array}$ for $\\hat{\\pmb{v}}^{i j k}\\;=\\;M\\hat{z}^{i j k}$ . This implies that removing the edge $j~\\rightarrow~i$ in $\\mathcal{G}$ would break the causal Markov condition for $q^{E_{k}}$ . Now let $\\delta=\\operatorname*{min}_{k,i,j}\\delta_{k}^{(i,j)}>0.$ \u03b4(ki,j)> 0, then for all M \u2208M\u00afsur(G) such that $\\|M-I\\|_{F}\\leqslant\\delta$ , the resulting distributions $q^{E_{k}},k\\in[K]$ satisfy assumption Assumption 1. ", "page_idx": 36}, {"type": "text", "text": "Combining the above two statements and what we have proven before, it is straightfoward to see that one can choose some $M\\in\\mathcal{M}_{\\mathrm{sur}}(\\mathcal{G})$ in a small neighbourhood of $\\boldsymbol{\\mathit{I}}$ that satisfies all the requirements, completing the proof. \u53e3 ", "page_idx": 36}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 37}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 37}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 37}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 37}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 37}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 37}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 37}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: Our abstract and introduction provide the readers a sense of our main results. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 37}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We compare with existing works in the introduction and the related work sections. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 38}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Justification: Rigorous proofs are provided in the appendix. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 38}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We introduce our experimental setup in details. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 38}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 39}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 39}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Justification: Code will be released after review. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 39}, {"type": "text", "text": "\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 40}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: Details are provided. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 40}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: we run experiments on 100 random causal graphs and report the overall accuracy. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 40}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 40}, {"type": "text", "text": "Answer: [No] ", "page_idx": 40}, {"type": "text", "text": "Justification: The experiments do not require huge computational resources and can be run on a local computer. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 40}, {"type": "text", "text": "\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 41}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: The paper conforms the code of ethics. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 41}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 41}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 42}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 42}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 42}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 43}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 43}]