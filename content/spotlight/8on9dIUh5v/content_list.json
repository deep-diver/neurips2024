[{"type": "text", "text": "Provable Benefit of Cutout and CutMix for Feature Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Junsoo Oh Chulhee Yun KAIST AI KAIST AI junsoo.oh@kaist.ac.kr chulhee.yun@kaist.ac.kr ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Patch-level data augmentation techniques such as Cutout and CutMix have demonstrated significant efficacy in enhancing the performance of vision tasks. However, a comprehensive theoretical understanding of these methods remains elusive. In this paper, we study two-layer neural networks trained using three distinct methods: vanilla training without augmentation, Cutout training, and CutMix training. Our analysis focuses on a feature-noise data model, which consists of several label-dependent features of varying rarity and label-independent noises of differing strengths. Our theorems demonstrate that Cutout training can learn low-frequency features that vanilla training cannot, while CutMix training can learn even rarer features that Cutout cannot capture. From this, we establish that CutMix yields the highest test accuracy among the three. Our novel analysis reveals that CutMix training makes the network learn all features and noise vectors \u201cevenly\u201d regardless of the rarity and strength, which provides an interesting insight into understanding patch-level augmentation. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Data augmentation is a crucial technique in deep learning, particularly in the image domain. It involves creating additional training examples by applying various transformations to the original data, thereby enhancing the generalization performance and robustness of deep learning models. Traditional data augmentation techniques typically focus on geometric transformations such as random rotations, horizontal and vertical flips, and cropping (Krizhevsky et al., 2012), or color-based adjustments such as color jittering (Simonyan and Zisserman, 2014). ", "page_idx": 0}, {"type": "text", "text": "In recent years, several new data augmentation techniques have appeared. Among them, patch-level data augmentation techniques like Cutout (DeVries and Taylor, 2017) and CutMix (Yun et al., 2019) have received considerable attention for their effectiveness in improving generalization. Cutout is a straightforward method where random rectangular regions of an image are removed during training. In comparison, CutMix adopts a more complex strategy by cutting and pasting sections from different images and using mixed labels, encouraging the model to learn from blended contexts. The success of Cutout and CutMix has triggered the development of numerous variants including Random Erasing (Zhong et al., 2020), GridMask (Chen et al., 2020a), CutBlur (Yo0 et al., 2020), Puzzle Mix (Kim et al., 2020), and Co-Mixup (Kim et al., 2021). However, despite the empirical success of these patch-level data augmentation techniques in various image-related tasks, a lack of comprehensive theoretical understanding persists: why and how do they work? ", "page_idx": 0}, {"type": "text", "text": "In this paper, we aim to address this gap by offering a theoretical analysis of two important patch-level data augmentation techniques: Cutout and CutMix. Our theoretical framework draws inspiration from a study by Shen et al. (2022), which explores a data model comprising multiple label-dependent feature vectors and label-independent noises of varying frequencies and intensities. The key idea of this work is that learning features with low frequency can be challenging due to strong noises (i.e., low signal-to-noise ratio). We focus on how Cutout and CutMix can aid in learning such rare features. ", "page_idx": 0}, {"type": "text", "text": "1.1 Our Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this paper, we consider a patch-wise data model consisting of features and noises, and use two-layer convolutional neural networks as learner networks. We focus on three different training methods: vanilla training without any augmentation, Cutout training, and CutMix training. We refer to these training methods in our problem setting as ERM, Cutout, and CutMix. We investigate how these methods affect the network's ability to learn features. We summarize our contributions below: ", "page_idx": 1}, {"type": "text", "text": "\u00b7 We analyze ERM, Cutout, and CutMix, revealing that Cutout outperforms ERM since it enables the learning of rarer features compared to ERM (Theorem 3.1 and Theorem 3.2). Furthermore, CutMix demonstrates almost perfect performance (Theorem 3.3) by learning all features. \u00b7 Our main intuition behind the negative result for ERM is that ERM learns to classify training samples by memorizing noise vectors instead of learning meaningful features if the features do not appear frequently enough. Hence, ERM suffers low test accuracy because it cannot learn rare features. However, Cutout alleviates this challenge by removing some of the strong noise patches, allowing it to learn rare features to some extent. \u00b7 We prove the near-perfect performance of CutMix based on a novel technique that views the non-convex loss as a composition of a convex function and reparameterization. This enables us to characterize the global minimum of the loss and show that CutMix forces the model to activate almost uniformly across every patch of inputs, allowing it to learn all features. ", "page_idx": 1}, {"type": "text", "text": "1.2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Feature Learning Theory.  Our work aligns with a recent line of studies investigating how training methods and neural network architectures infuence feature learning. These studies focus on a specific data distribution composed of two components: label-dependent features and label-independent noise. The key contribution of this body of work is the exploration of which training methods or neural networks are most effective at learning meaningful features and achieving good generalization performance. Allen-Zhu and Li (2020) demonstrate that an ensemble model can achieve near-perfect performance by learning diverse features, while a single model tends to learn only certain parts of the feature space, leading to lower test accuracy. In other works, Cao et al. (2022); Kou et al. (2023a) explore the phenomenon of benign overfitting when training a two-layer convolutional neural network. The authors identify the specific conditions under which benign overfitting occurs, providing valuable insights into how these networks behave during training. Several other studies seek to understand various aspects of deep learning through the lens of feature learning (Zou et al., 2021; Jelassi and Li, 2022; Chen et al., 2022, 2023; Li and Li, 2023; Huang et al., 2023a,b). ", "page_idx": 1}, {"type": "text", "text": "Theoretical Analysis of Data Augmentation. Several works aim to analyze traditional data augmentation from different perspectives, including kernel theory (Dao et al., 2019), margin-based approach (Rajput et al., 2019), regularization effects (Wu et al., 2020), group invariance (Chen et al., 2020b), and impact on optimization (Hanin and Sun, 2021). Moreover, many papers have explored various aspects of a recent technique called Mixup (Zhang et al., 2017). For example, studies have explored its regularization effects (Carratino et al., 2020; Zhang et al., 2020), its role in improving calibration (Zhang et al., 2022), its ability to find optimal decision boundaries (Oh and Yun, 2023) and its potential negative effects (Chidambaram et al., 2021; Chidambaram and Ge, 2024). Some works investigate the broader framework of Mixup, including CutMix, which aligns with the scope of our work. Park et al. (2022) study the regularization effect of mixed-sample data augmentation within a unified framework that contains both Mixup and CutMix. In Oh and Yun (2023), the authors analyze masking-based Mixup, which is a class of Mixup variants that also includes CutMix. In their context, they show that masking-based Mixup can deviate from the Bayes optimal classifier but require less training sample complexity. However, neither work provides a rigorous explanation for why CutMix has been successful. The studies most closely related to our work include Shen et al. (2022); Chidambaram et al. (2023); Zou et al. (2023). Shen et al. (2022) regard traditional data augmentation as a form of feature manipulation and investigate its advantages from a feature learning perspective. Both Chidambaram et al. (2023) and Zou et al. (2023) analyze Mixup within a feature learning framework. However, patch-level data augmentation such as Cutout and CutMix, which are the focus of our work, have not yet been explored within this context. ", "page_idx": 1}, {"type": "text", "text": "2 Problem Setting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we introduce the data distribution and neural network architecture, and formally describe the three training methods considered in this paper. ", "page_idx": 2}, {"type": "text", "text": "2.1 Data Distribution ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We consider a binary classification problem on structured data, consisting of patches of labeldependent vectors (referred to as features) and label-independent vectors (referred to as noise). ", "page_idx": 2}, {"type": "text", "text": "Definition 2.1 (Feature Noise Patch Data). We defne a data distribution $\\mathcal{D}$ on $\\mathbb{R}^{d\\times P}\\times\\{-1,1\\}$ such that $(X,y)\\sim\\mathcal{D}$ where $\\pmb{X}=\\left(\\pmb{x}^{(1)},\\dots,\\pmb{x}^{(P)}\\right)\\in\\mathbb{R}^{d\\times P}$ and $y\\in\\{\\pm1\\}$ is constructed as follows. ", "page_idx": 2}, {"type": "text", "text": "1. Choose the label $y\\in\\{\\pm1\\}$ uniformly at random. ", "page_idx": 2}, {"type": "text", "text": "2. Let $\\{\\pmb{v}_{s,k}\\}_{s\\in\\{\\pm1\\},k\\in[K]}\\subset\\mathbb{R}^{d}$ be a set of orthonormal feature vectors. Choose the feature vector $\\pmb{v}\\in\\mathbb{R}^{d}$ for data point $\\mathbf{\\deltaX}$ as $\\pmb{v}=\\pmb{v}_{y,k}$ with probability $\\rho_{k}$ from $\\{\\pmb{v}_{y,k}\\}_{k\\in[K]}\\,\\subset\\,\\mathbb{R}^{d}$ where $\\rho_{1}+\\cdots+\\rho_{K}\\,=\\,1$ and $\\rho_{1}~\\geq~\\cdot\\cdot~\\geq~\\rho_{K}$ . In our setting, there are three types of features with significantly different frequencies: common features, rare features, and extremely rare features, ordered from most to least frequent. The indices of these features partition $[K]$ into $(K_{C},K_{R},K_{E})$ ", "page_idx": 2}, {"type": "text", "text": "3. We construct $P$ patches of $\\mathbf{\\deltaX}$ as follows. ", "page_idx": 2}, {"type": "text", "text": "\u00b7 Feature Patch: Choose $p^{*}$ uniformly from $[P]$ and we set $\\boldsymbol{\\mathbf{\\mathit{x}}}^{(p^{*})}=\\boldsymbol{\\mathbf{\\mathit{v}}}$ ", "page_idx": 2}, {"type": "text", "text": "\u00b7 Dominant Noise Patch: Choose $\\tilde{p}$ uniformly from $[P]\\setminus\\{p^{*}\\}$ . We construct $\\pmb{x}^{(\\widetilde{p})}=\\alpha\\pmb{u}+\\xi^{(\\widetilde{p})}$   \nwhere $_{\\alpha u}$ is feature noise drawn uniformly from $\\{\\alpha\\pmb{v}_{1,1},\\alpha\\pmb{v}_{-1,1}\\}$ with $0<\\alpha<1$ and $\\xi^{(\\tilde{p})}$ is Gaussian dominant noise drawn from $N({\\bf0},\\sigma_{\\mathrm{d}}^{2}{\\bf A})$   \n\u00b7 Background Noise Patch: The remaining patches $p\\,\\in\\,[P]\\,\\setminus\\,\\{p^{*},\\tilde{p}\\}$ consist of Gaussian background noise, i., we set ${\\pmb x}^{(p)}=\\xi^{(p)}$ where $\\boldsymbol\\xi^{(p)}\\sim N(\\mathbf0,\\sigma_{\\mathrm{b}}^{2}\\mathbf{A})$ ", "page_idx": 2}, {"type": "text", "text": "Here, the noise covariance matrix is defined as $\\begin{array}{r}{\\pmb{\\Lambda}:=\\pmb{I}-\\sum_{s,k}\\pmb{v}_{s,k}\\pmb{v}_{s,k}^{\\top}}\\end{array}$ which ensures that Gaussian noises are orthogonal to all features. We assume that the dominant noise is stronger than the background noise, i.e., $\\sigma_{\\mathrm{b}}<\\sigma_{\\mathrm{d}}$ ", "page_idx": 2}, {"type": "text", "text": "Our data distribution captures characteristics of image data, where the input consists of several patches. Some patches contain information relevant to the image labels, such as cat faces for the label \u201ccat, while other patches contain information irrelevant to the labels, such as the background. Intuitively, there are two ways to fit the given data: learning features or memorizing noise. If a model fits the data by learning features, it can correctly classify test data having the same features. However, if a model fits the data by memorizing noise, it cannot generalize to unseen data because noise patches are not relevant to labels. Thus, learning more features is crucial for achieving better generalization. ", "page_idx": 2}, {"type": "text", "text": "In real-world scenarios, different features may appear with varying frequencies. For instance, the occurrences of cat's faces and cat's tails in a dataset might differ significantly, although both are relevant to the \u201ccat' label. Our data distribution reflects these characteristics by considering features with varying frequencies. To emphasize the distinctions between the three training methods we analyze, we categorize features into three groups: common, rare, and extremely rare. We refer to data points containing these features as common data, rare data, and extremely rare data, respectively. We emphasize that these terminologies are chosen merely to distinguish the three different levels of rarity, and even \u201cextremely rare\u201d features appear in a nontrivial fraction of the training data with high probability (see our assumptions in Section 2.4). ", "page_idx": 2}, {"type": "text", "text": "Comparison to Previous Work.  Our data distribution is similar to those considered in Shen et al. (2022) and Zou et al. (2023), which investigate the benefits of standard data augmentation methods and Mixup by comparing them to vanilla training without any augmentation. These results consider two types of features\u2014common and rare\u2014-with different levels of rarity, along with two types of noise: feature noise and Gaussian noise. However, we consider three types of features: common, rare, and extremely rare, and three types of noise: feature noise, dominant noise, and background noise. This distinction allows us to compare three distinct methods and demonstrate the differences between them, whereas Shen et al. (2022) and Zou et al. (2023) compared only two methods. ", "page_idx": 2}, {"type": "text", "text": "2.2 Neural Network Architecture ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "For the prediction model, we focus on the following two-layer convolutional neural network where the weights in the second layer are fixed at 1 and $-1$ , with only the first layer being trainable. Several works including Shen et al. (2022) and Zou et al. (2023) also focus on similar two-layer convolutional neuralnetworks. ", "page_idx": 3}, {"type": "text", "text": "Definition 2.2 (2-Layer CNN). We define 2-layer CNN $f_{W}\\;:\\;\\mathbb{R}^{d\\times P}\\;\\rightarrow\\;\\mathbb{R}$ parameterized by $W=\\left\\{{\\pmb w}_{1},{\\pmb w}_{-1}\\right\\}\\in\\mathbb{R}^{d\\times2}$ .For each input $\\pmb{X}=\\left(\\pmb{x}^{(1)},\\dots,\\pmb{x}^{(P)}\\right)\\in\\mathbb{R}^{d\\times P}$ ,wedefine ", "page_idx": 3}, {"type": "equation", "text": "$$\nf_{W}(X):=\\sum_{p\\in[P]}\\phi\\left(\\left\\langle w_{1},x^{(p)}\\right\\rangle\\right)-\\sum_{p\\in[P]}\\phi\\left(\\left\\langle w_{-1},x^{(p)}\\right\\rangle\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\phi(\\cdot)$ is a smoothed version of leaky ReLU activation, defined as follows. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\phi(z):=\\left\\{\\frac{z-\\frac{(1-\\beta)r}{2}}{\\frac{1-\\beta}{2r}z^{2}+\\beta z}\\right.\\ \\ \\varepsilon\\geq z\\leq r\\ ,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $0<\\beta\\leq1$ and $r>0$ ", "page_idx": 3}, {"type": "text", "text": "Previous works on the theory of feature learning often consider neural networks with (smoothed) ReLU or polynomial activation functions. However, we adopt a smoothed leaky ReLU activation, which always has a positive slope, to exclude the possibility of neurons \u201cdying\u201d during the complex optimization trajectory. Using smoothed leaky ReLU to analyze the learning dynamics of neural networks is not entirely new; there is a body of work that studies phenomena such as benign overfitting (Frei et al., 2022a) and implicit bias (Frei et al., 2022b; Kou et al., 2023b) by analyzing neural networks with (smoothed) leaky ReLU activation. ", "page_idx": 3}, {"type": "text", "text": "A key difference between ReLU and leaky ReLU lies in the possibility of ReLU neurons \u201cdying\" in the negative region, where some negatively initialized neurons remain unchanged throughout training. As a result, using ReLU activation requires multiple neurons to ensure the survival of neurons at initialization, which becomes increasingly probable as the number of neurons increases. In contrast, the derivative of leaky ReLU is always positive, ensuring that a single neuron is often sufficient. Therefore, for mathematical simplicity, we consider the case where the network has a single neuron for each positive and negative output. We believe that our analysis can be extended to the multi-neuron case as we validate numerically in Appendix A.2. ", "page_idx": 3}, {"type": "text", "text": "2.3  Training Methods ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Using a training set sampled from the distribution $\\mathcal{D}$ , we would like to train our network $f_{W}$ to learn to correctly classify unseen data points from $\\mathcal{D}$ . We consider three learning methods: vanilla training without any augmentation, Cutout, and CutMix. We first introduce necessary notation for our data and parameters, and then formalize training methods within our framework. ", "page_idx": 3}, {"type": "text", "text": "Training Data. We consider a training set $\\mathcal{Z}=\\{(X_{i},y_{i})\\}_{i\\in[n]}$ comprising $n$ data points, each independently drawn from $\\mathcal{D}$ For each $i\\in[n]$ , we denote $\\pmb{X}_{i}=\\bar{(\\pmb{x}_{i}^{(1)},\\dots,\\pmb{x}_{i}^{(P)})}$ ", "page_idx": 3}, {"type": "text", "text": "Initialization.We initialize the model parameters in our neural network using random initialization, Specifialy w initaie the mode arameter $W^{(0)}=\\{{\\pmb w}_{1}^{(0)},{\\pmb w}_{-1}^{(0)}\\}$ where ${\\pmb w}_{1}^{(0)},{\\pmb w}_{-1}^{(0)}$ $N(\\mathbf{0},\\sigma_{0}^{2}I_{d})$ . Let us denote updatd modl parameters at teration $t$ $W^{(t)}=\\{\\pmb{w}_{1}^{(t)},\\pmb{w}_{-1}^{(t)}\\}$ ", "page_idx": 3}, {"type": "text", "text": "2.3.1  Vanilla Training ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The vanilla approach to training a model $f_{W}$ is solving the empirical risk minimization problem using gradient descent. We refer to this method as ERM. Then, ERM updates parameters $\\mathbf{\\boldsymbol{W}}^{(t)}$ ofa model using the following rule. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\boldsymbol{W}^{(t+1)}=\\boldsymbol{W}^{(t)}-\\eta\\nabla_{\\boldsymbol{W}}\\mathcal{L}_{\\mathrm{ERM}}\\left(\\boldsymbol{W}^{(t)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\eta$ is a learning rate and $\\mathcal{L}_{\\mathrm{ERM}}(\\cdot)$ is the ERM training loss defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{ERM}}(W):=\\frac{1}{n}\\sum_{i\\in[n]}\\ell(y_{i}f_{W}(X_{i})),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\ell(\\cdot)$ is the logistic loss $\\ell(z)=\\log(1+e^{-z})$ ", "page_idx": 4}, {"type": "text", "text": "2.3.2 Cutout Training. ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Cutout (DeVries and Taylor, 2017) is a data augmentation technique that randomly cuts out rectangular regions of image inputs. In our patch-wise data, we regard Cutout training as using inputs with masked patches from the original data. For each subset $\\mathcal{C}$ of $[P]$ and $i\\in[n]$ ,we define augmented data $X_{i,c}\\in\\mathbb{R}^{d\\times P}$ as a data point generated by cuting the patches with indices in $\\mathcal{C}$ out of $X_{i}$ We canrepresent $X_{i,c}$ as ", "page_idx": 4}, {"type": "equation", "text": "$$\nX_{i,\\mathcal{C}}=\\left(x_{i,\\mathcal{C}}^{(1)},\\ldots,x_{i,\\mathcal{C}}^{(P)}\\right),\\;\\mathrm{where}\\;\\;x_{i,\\mathcal{C}}^{(p)}=\\left\\{\\!x_{i}^{(p)}\\!\\quad\\mathrm{if}\\;p\\notin\\mathcal{C},\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Note that the output of the model $f_{W}(\\cdot)$ on this augmented data point $X_{i,c}$ is ", "page_idx": 4}, {"type": "equation", "text": "$$\nf_{W}(X_{i,\\mathcal{C}})=\\sum_{p\\not\\in\\mathcal{C}}\\phi\\left(\\left\\langle w_{1},x_{i}^{(p)}\\right\\rangle\\right)-\\sum_{p\\not\\in\\mathcal{C}}\\phi\\left(\\left\\langle w_{-1},x_{i}^{(p)}\\right\\rangle\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Then, the objective function for Cutout training can be defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{Cutout}}(W):=\\frac{1}{n}\\sum_{i\\in[n]}\\mathbb{E}_{\\mathcal{C}\\sim\\mathcal{D}_{\\mathcal{C}}}[\\ell(y_{i}f_{W}(X_{i,\\mathcal{C}}))],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathcal{D}_{\\mathcal{C}}$ is a uniform distribution on the collection of subsets of $[P]$ with cardinality $C$ ,where $C$ is a hyperparameter satisfying $\\begin{array}{r}{1\\leq C<\\frac{P}{2}}\\end{array}$ 1 We refer to the process of training our model using gradient descent on Cutout loss $\\mathcal{L}_{\\mathrm{Cutout}}(\\bar{\\pmb{W}})$ as Cutout, and its update rule is ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{W}^{(t+1)}=\\pmb{W}^{(t)}-\\eta\\nabla_{\\pmb{W}}\\mathcal{L}_{\\mathrm{Cutout}}\\left(\\pmb{W}^{(t)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\eta$ is a learning rate. ", "page_idx": 4}, {"type": "text", "text": "2.3.3  CutMix Training. ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "CutMix (Yun et al., 2019) involves not only cutting parts of images, but also pasting them into different images as well as assigning them mixed labels. For each subset $\\boldsymbol{S}$ of $[P]$ and $i,j\\in[n]$ ,we define the augmented data point $\\pmb{X}_{i,j,S}\\in\\mathbb{R}^{d\\times P}$ as the data obtained by cutting patches with indices in $\\boldsymbol{S}$ from data $X_{i}$ and pasting them into $X_{j}$ at the same indices $\\boldsymbol{S}$ Wecanwrite $X_{i,j,S}$ as ", "page_idx": 4}, {"type": "equation", "text": "$$\nX_{i,j,S}=\\left(\\pmb{x}_{i,j,S}^{(1)},\\allowbreak\\cdot,\\allowbreak\\cdot,\\allowbreak\\pmb{x}_{i,j,S}^{(P)}\\right)\\mathrm{,~where~}\\pmb{x}_{i,j,S}^{(p)}=\\left\\{\\b{x}_{i}^{(p)}\\qquad\\mathrm{if}\\;p\\in\\mathcal{S},\\allowbreak\\mathscr{\\right.}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The one-hot encoding of the labels $y_{i}$ and $y_{j}$ are also mixed with proportions $\\frac{|\\boldsymbol{S}|}{P}$ and $1\\,-\\,{\\frac{|S|}{P}}$ respectively. This mixed label results in the loss of the form ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{|S|}{P}\\ell(y_{i}f_{W}(\\boldsymbol{X}_{i,j,S}))+\\left(1-\\frac{|S|}{P}\\right)\\ell(y_{j}f_{W}(\\boldsymbol{X}_{i,j,S})).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "From this, the CutMix training loss $\\mathcal{L}_{\\mathrm{CutMix}}(W)$ can be defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{CutMix}}(\\pmb{W}):=\\frac{1}{n^{2}}\\sum_{i,j\\in[n]}\\mathbb{E}_{\\pmb{S}\\sim\\mathcal{D}_{S}}\\left[\\frac{|\\pmb{S}|}{P}\\ell(y_{i}f_{\\pmb{W}}(\\pmb{X}_{i,j,\\mathcal{S}}))+\\left(1-\\frac{|\\pmb{S}|}{P}\\right)\\ell(y_{j}f_{\\pmb{W}}(\\pmb{X}_{i,j,\\mathcal{S}}))\\right],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathcal{D}_{S}$ is a probability distribution on the set of subsets of $[P]$ which samples $S\\sim\\mathcal{D}_{S}$ as follows.2 ", "page_idx": 4}, {"type": "text", "text": "1. Choose the cardinality $s$ of $\\boldsymbol{S}$ uniformly at random from $\\{0,1,\\ldots,P\\}$ , and ", "page_idx": 5}, {"type": "text", "text": "2.Choose $\\boldsymbol{S}$ uniformly at random from the collection of subsets of $\\big[P\\big]$ with cardinality $s$ ", "page_idx": 5}, {"type": "text", "text": "We refer to the process of training our network using gradient descent on CutMix loss $\\mathcal{L}_{\\mathrm{CutMix}}(W)$ as CutMix, and its update rule is ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{W}^{(t+1)}=\\pmb{W}^{(t)}-\\eta\\nabla_{\\pmb{W}}\\mathcal{L}_{\\mathrm{CutMix}}\\left(\\pmb{W}^{(t)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\eta$ is a learning rate. ", "page_idx": 5}, {"type": "text", "text": "2.4  Assumptions on the Choice of Problem Parameters ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To control the quantities that appear in the analysis of training dynamics, we make assumptions on several quantities in our problem setting. For simplicity, we use choices of problem parameters as a function of the dimension of patches $d$ and consider sufficiently large $d$ ", "page_idx": 5}, {"type": "text", "text": "We use the standard asymptotic notation $\\mathcal{O}(\\cdot),\\Omega(\\cdot),\\Theta(\\cdot),o(\\cdot),\\omega(\\cdot)$ to express the dependency on $d$ We also use $\\widetilde{\\mathcal{O}}(\\cdot),\\widetilde{\\Omega}(\\cdot),\\widetilde{\\Theta}(\\cdot)$ to hide logarithmic factors of $d$ . Additionally, $\\mathrm{poly}(d)$ (or p $\\mathrm{olylog}(d))$ represents quantities that increase faster than $d^{c_{1}}$ (or $(\\log d)^{c_{1}})$ and slower than $d^{c_{2}}$ (or $(\\log d)^{c_{2}},$ for some constant $0\\ <\\ c_{1}\\ <\\ c_{2}$ . Similarly, $o(1/\\mathrm{poly}(d))$ (or $o(1/\\mathrm{polylog}(d)))$ denotes some quantities that decrease faster than $1/d^{c}$ (or $1/(\\log d)^{c})$ for any constant $c$ . Finally, we use $f(d)=$ $o(g(d)/\\mathrm{polylog}(d))$ when $f(d)/g(d)=o(1/\\mathrm{polylog}(d))$ for some function $f$ and $g$ of $d$ ", "page_idx": 5}, {"type": "text", "text": "Assumptions. We assume that $P=\\Theta(1)$ and $P\\geq8$ for simplicity. Additionally, we consider a high-dimensional regime where the number of data points is much smaller than the dimension $d$ which is expressed as $n=o\\left(\\alpha\\beta\\sigma_{\\mathrm{d}}^{-1}\\sigma_{\\mathrm{b}}d^{\\frac{1}{2}}/\\mathrm{polylog}(\\bar{d})\\right)$ . We also assume that $\\rho_{k}n=\\omega\\left(n^{\\frac{1}{2}}\\log d\\right)$ for all $k\\in[K]$ , which ensures the sufficiency of data points with each feature. ", "page_idx": 5}, {"type": "text", "text": "In addition, as we will describe in Section 4, the relative scales between the frequencies of features and the strengths of noises play crucial roles in our analysis, as they serve as a proxy for the \u201clearning speed\" in the initial phase. For common features $k\\in\\kappa_{C}$ , we assume $\\rho_{k}=\\Theta(1)$ and the learning speed of common features is much faster than that of dominant noise, which translates into the assumption $\\sigma_{\\mathrm{d}}^{2}d=o(\\beta n)$ .For rare features $k\\in\\kappa_{R}$ , we assume $\\rho_{k}=\\Theta(\\rho_{R})$ for some $\\rho_{R}$ , and we consider the case where the learning speed of rare features is much slower than that of dominant noise but faster than background noise, which is expressed as $\\rho_{R}n=o\\left(\\alpha^{2}\\sigma_{\\mathrm{d}}^{2}d/\\mathrm{polylog}(d)\\right)$ and $\\sigma_{\\mathrm{b}}^{2}d\\,=\\,o(\\beta\\rho_{R}n)$ .Finally, for extremely rare features $k\\ \\in\\ K_{E}$ , we say $\\rho_{k}\\,=\\,\\Theta(\\rho_{E})$ for some $\\rho_{E}$ and their learning is even slower than that of background noises, which can be expressed as $\\dot{\\rho}_{E}n=o\\left(\\alpha^{2}\\sigma_{\\mathrm{b}}^{2}d/\\mathrm{polylog}(d)\\right)$ ", "page_idx": 5}, {"type": "text", "text": "Lastly, we assume the strength of feature noise satisfies $\\alpha\\;=\\;o\\left(n^{-1}\\beta\\sigma_{\\mathrm{d}}^{2}d/\\mathrm{polylog}(d)\\right)$ ,and $r,\\sigma_{0},\\eta>0$ are sufficiently small so that $\\sigma_{0},r=o\\left(\\alpha/\\mathrm{polylog}(d)\\right)$ \uff0c $\\eta=o\\left(r\\sigma_{\\mathrm{d}}^{-2}d^{-1}/\\mathrm{polylog}(d)\\right)$ We list our assumptions in Assumption B.1 and there are many choices of parameters satisfying the set of assumptions, including: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{P=8,C=2,n=\\Theta\\left(d^{0.4}\\right),\\alpha=\\Theta\\left(d^{-0.02}\\right),\\beta=\\frac{1}{\\mathrm{polylog}(d)},\\sigma_{0}=\\Theta(d^{-0.2}),r=\\Theta(d^{-0.2}),}\\\\ &{}&{\\sigma_{\\mathrm{d}}=\\Theta\\left(d^{-0.305}\\right),\\sigma_{\\mathrm{b}}=\\Theta\\left(d^{-0.375}\\right),\\rho_{R}=\\Theta\\left(d^{-0.1}\\right),\\rho_{E}=\\Theta\\left(d^{-0.195}\\right),\\eta=\\Theta(d^{-1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "3 Main Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we provide a characterization of the high probability guarantees for the behavior of models trained using three distinct methods we have introduced. We denote by $T^{*}$ themaximum admissibletraining iterates and we assume $\\begin{array}{r}{T^{*}=\\frac{\\mathrm{poly}(d)}{\\eta}}\\end{array}$ With a sficientlylarge plynomial in $d$ . In all of our theorem statements, the randomness is over the sampling of training data and the initialization of models and all results hold under the condition that $d$ is sufficiently large. ", "page_idx": 5}, {"type": "text", "text": "The following theorem characterizes training accuracy and test accuracy achieved by ERM. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3. Let $\\mathbf{\\nabla}W^{(t)}$ beratfwi $\\textstyle1-o\\left({\\frac{1}{\\operatorname{poly}(d)}}\\right)$ there exists $T_{\\mathrm{ERM}}$ such that any $T\\in[T_{\\mathrm{ERM}},T^{*}]$ satisfies the following: ", "page_idx": 6}, {"type": "text", "text": "\u00b7 (Perfectly fits training set): For all $i\\in[n],y_{i}f_{W^{(T)}}(\\pmb{X}_{i})>0.$ \u00b7R $a n d o m\\;o n\\;(e x t r e m e l y)\\;r a r e\\;d a t a)\\colon\\mathbb{P}_{(X,y)\\sim\\mathcal{D}}\\left[y f_{W^{(T)}}(X)>0\\right]\\!=\\!1\\!-\\!\\frac{1}{2}\\!\\sum_{k\\in\\mathcal{K}_{R}\\cup K_{E}}\\rho_{k}\\!\\pmb{\\pmb{\\pmb{\\omega}}}\\left(\\frac{1}{\\mathrm{poly}(d)}\\right).$ ", "page_idx": 6}, {"type": "text", "text": "The proof is provided in Appendix C.2. Theorem 3.1 demonstrates that ERM achieves perfect training accuracy; however, it performs almost like random guessing on unseen data points with rare and extremely rare features. This is because ERM can only learn common features and overfit rare or extremely rare data in the training set by memorizing noises to achieve perfect training accuracy. ", "page_idx": 6}, {"type": "text", "text": "In comparison, we show that Cutout can perfectly fit both augmented training data and original training data, and it can also learn rare features that ERM cannot. However, Cutout still makes random guesses on test data with extremely rare features. We state these in the following theorem with the proof provided in Appendix D.2: ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.2. Let $\\mathbf{\\boldsymbol{W}}^{(t)}$ be iteratesof Cutout raining. Then with probabiliry a least $\\textstyle1-o\\left({\\frac{1}{\\operatorname{poly}(d)}}\\right)$ thereexists $T_{\\mathrm{{Cutout}}}$ suchthat any $T\\in[T_{\\mathrm{Cutout}},T^{*}]$ satisfies the following: ", "page_idx": 6}, {"type": "text", "text": "\u00b7 (Perfectly fits augmented data): For all $i\\in[n]$ and $\\mathcal{C}\\subset[P]$ with $|\\mathcal{C}|=C,\\,y_{i}f_{W^{(T)}}(X_{i,\\mathcal{C}})>0.$ \u00b7 (Perfectly fits original training data): For all $i\\in[n],y_{i}f_{{\\pmb{W}}^{(T)}}({\\pmb{X}}_{i})>0$ $\\mathbb{P}_{({\\pmb X},{\\pmb y})\\sim{\\mathscr D}}\\left[y f_{W^{(T)}}({\\pmb X})>0\\right]=1-\\frac{1}{2}\\sum_{k\\in\\mathcal K_{E}}\\rho_{k}\\pmb{\\mathscr\\rho}\\left(\\frac{1}{\\mathrm{poly}(d)}\\right).$ ", "page_idx": 6}, {"type": "text", "text": "In the case of CutMix, it is challenging to discuss training accuracy directly because the augmented data have soft labels generated by mixing pairs of labels. Instead, we prove that CutMix achieves a sufficiently small gradient of the loss, and the training accuracy on the original training data is perfect. We also demonstrate that CutMix achieves almost perfect test accuracy, as it learns all types of features regardless of rarity. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.3. Let $\\mathbf{\\boldsymbol{W}}^{(t)}$ b iteratesof CutMixtraining Then with probabiliy a least $\\textstyle1\\!-\\!o\\left({\\frac{1}{\\operatorname{poly}(d)}}\\right)$ there existssome $T_{\\mathrm{CutMix}}\\in[0,T^{*}]$ that satisfies the following: ", "page_idx": 6}, {"type": "text", "text": "\u00b7(Finds anear statonary pont:wuix (W(Tux)= poly@ \u00b7 (Perfectly fits original training data): For all $i\\in[n],\\,y_{i}f_{\\mathbf{W}^{(T_{\\mathrm{CutMix}})}}(\\mathbf{X}_{i})>0.$ $\\begin{array}{r}{\\mathbb{P}_{(\\boldsymbol{X},\\boldsymbol{y})\\sim\\mathcal{D}}\\left[y f_{\\boldsymbol{W}^{(T_{\\mathrm{CutMix}})}}(\\boldsymbol{X})>0\\right]=1-o\\left(\\frac{1}{\\mathrm{poly}(d)}\\right).}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "To prove Theorem 3.3, we characterize the global minimum of objective loss of CutMix. Surprisingly, at the global minimum, the model has the same outputs for all patches of the input data. In other words, the contributions of all feature vectors and noise vectors to the final outcome of the network are identical, regardless of their frequency and strength (see Section 4.2 for more details). Moreover, this uniform \u201ccontribution\u201d is large enough, which allows the model to learn all types of features by reaching the global minimum. We provide the detailed proof in Appendix E.2. ", "page_idx": 6}, {"type": "text", "text": "Our three main theorems elucidate the benefits of Cutout and CutMix. Cutout enables a model to learn rarer features than ERM, while CutMix can even outperform Cutout. These advantages in learning rarer features lead to improvements in generalization performance. ", "page_idx": 6}, {"type": "text", "text": "4  Overview of Analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we discuss key proof ideas and the main challenges in our analysis. For ease of presentation, we consider the case $\\alpha=0$ . Although our assumptions do not allow the choice $\\alpha=0$ the choice of nonzero $\\alpha$ is to show guarantees on the test accuracy and does not significantly affect the feature learning aspect. ", "page_idx": 6}, {"type": "text", "text": "$i\\in[n]$ $\\pmb{X}_{i}=(\\pmb{x}_{i}^{(1)},\\dots,\\pmb{x}_{i}^{(P)})$ $p_{i}^{*}$ $\\tilde{p}_{i}$ $\\pmb{v}_{s,k}$ where $s\\in\\{\\pm1\\}$ and $k\\in[K]$ , let $\\mathcal{V}_{s,k}\\subset[n]$ represent the set of indices of data points having the feature vector $\\pmb{v}_{s,k}$ , and $\\begin{array}{r}{\\mathcal{V}_{s}=\\bigcup_{k=1}^{K}\\mathcal{V}_{s,k}}\\end{array}$ denotes the set of indices of data with label $s$ For each data point $i\\in[n]$ and dominant or background noise patch $p\\in[P]\\setminus\\{p_{i}^{*}\\}$ , we refer to the Gaussian noise inside $\\pmb{x}_{i}^{(p)}$ $\\xi_{i}^{(p)}$ ", "page_idx": 7}, {"type": "text", "text": "4.1 Vanilla Training and Cutout Training ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We now explain why ERM fails to learn (extremely) rare features, while Cutout can learn rare features but not extremely rare features. Let us consider ERM. From (1), for $s,s^{\\prime}\\in\\{\\pm1\\},k\\in[K],i\\in[n]$ and $p\\in[P]\\setminus\\{p_{i}^{*}\\}$ ,thecomponent of $\\pmb{w}_{s}$ in the feature vector $\\pmb{v}_{s^{\\prime},k}$ 's direction is updated as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\left\\langle w_{s}^{(t+1)},v_{s^{\\prime},k}\\right\\rangle=\\left\\langle w_{s}^{(t)},v_{s^{\\prime},k}\\right\\rangle-\\frac{s s^{\\prime}\\eta}{n}\\sum_{j\\in\\mathcal{V}_{s^{\\prime},k}}\\ell^{\\prime}(y_{j}f_{W^{(t)}}(X_{j}))\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(t)},v_{s^{\\prime},k}\\right\\rangle\\right),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "and similarly,the \u201cupdate\u201d of inner product of $\\pmb{w}_{s}$ with a noise patch $\\xi_{i}^{(p)}$ can be written as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\left\\langle w_{s}^{(t+1)},\\xi_{i}^{(p)}\\right\\rangle\\approx\\left\\langle w_{s}^{(t)},\\xi_{i}^{(p)}\\right\\rangle-\\frac{s y_{i}\\eta}{n}\\ell^{\\prime}(y_{i}f_{W^{(t)}}(X_{i}))\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(t)},\\xi_{i}^{(p)}\\right\\rangle\\right)\\left\\|\\xi_{i}^{(p)}\\right\\|^{2},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where the approximation is due to the near-orthogonality of Gaussian random vectors in the highdimensional regime.This approximation showsthat $\\langle\\pmb{w}_{s}^{(t+1)},\\pmb{v}_{s^{\\prime},k}\\rangle$ 'sand $\\langle w_{s}^{(t)},\\xi_{i}^{(p)}\\rangle$ 'sarealmost monotonically increasing or decreasing. We address the approximation errors using a variant of the technique introduced by Cao et al. (2022), as detailed in Appendix B.3. ", "page_idx": 7}, {"type": "text", "text": "From (4) and (5), we can observe that in the early phase of training satisfying $-\\ell^{\\prime}\\bigl(y_{i}\\,f_{{\\pmb W}^{(t)}}\\bigl({\\pmb X}_{i}\\bigr)\\bigr)=$ $\\Theta(1)$ , the main factor for the speed of learning features and noises are the number of feature ocurrence $|\\mathcal{V}_{s^{\\prime},k}|$ and thestrength of noise $\\|\\xi_{i}^{(p)}\\|^{2}$ . From our assumptions introduced in Section 2.4, if we compare the learning speed of different components, we have ", "page_idx": 7}, {"type": "text", "text": "common features $\\gg$ dominant noises $\\gg$ rare features $\\gg$ background noises $\\gg$ extremely rare features, in terms of \u201clearning speed? Based on this observation, we conduct a three-phase analysis for ERM. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "\u00b7 Phase 1: Learning common features quickly.   \n\u00b7 Phase 2: Fitting (extremely) rare data by memorizing dominant noises instead of learning features.   \n\u00b7 Phase 3: A model cannot learn (extremely) rare features since gradients of all data are small. ", "page_idx": 7}, {"type": "text", "text": "The main intuition behind why ERM cannot learn (extremely) rare features is that the gradients of all data containing these features become small after quickly memorizing dominant noise patches. In contrast, since Cutout randomly cuts some patches out, there exist augmented data points that do not contain dominant noises and have only features and background noises. This allows Cutout to learn rare features, thanks to these augmented data. However, extremely rare features cannot be learned since the learning speed of background noise is much faster and there are too many background noise patches to cut them all out. ", "page_idx": 7}, {"type": "text", "text": "Remark 4.1. Shen et al. (2022) conduct analysis on vanilla training and training using standard data augmentation, sharing the same intuition in similar but different data models and neural networks. Also, we emphasize that we proved the model cannot learn (extremely) rare features even if we run poly(d) iterations of GD, whereas Shen et a. (2022) only consider the frst iteration that achieves perfect training accuracy. ", "page_idx": 7}, {"type": "text", "text": "Practical Insights. In practice, images contain features and noise across several patches. A larger cutting size can be more effective in removing noise but may also remove important features that the model needs to learn. Thus, there is a trade-off in choosing the optimal cutting size, a trend also observed in DeVries and Taylor (2017). One limitation of Cutout is that it may not effectively remove dominant noise. Thus, dominant noise can persist in the augmented data, leading to potential noise memorization. We believe that developing strategies that can more precisely detect and remove these noise components from the image input could enhance the effectiveness of these methods. ", "page_idx": 7}, {"type": "text", "text": "4.2  CutMix Training ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In learning dynamics of ERM and Cutout, inner products between weight and data patches evolve (approximately) monotonically, which makes the analysis much more feasible. However, analyzing the learning dynamics of CutMix involves non-monotone change of inner products, which is inevitable since CutMix uses mixed labels; this is also demonstrated in our experimental results (Section 5,especially the leftmost plot in Figure 1). Non-monotonicity and non-convexity of the problem necessitates novel proof strategies. ", "page_idx": 8}, {"type": "equation", "text": "$$\nz_{i}^{(p)}:=\\phi\\left(\\left\\langle w_{1},\\xi_{i}^{(p)}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-1},\\xi_{i}^{(p)}\\right\\rangle\\right),\\quad z_{s,k}:=\\phi(\\left\\langle w_{1},v_{s,k}\\right\\rangle)-\\phi(\\left\\langle w_{-1},v_{s,k}\\right\\rangle).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Then, $_{z}$ represents the contribution of each noise patch and feature vector to the neural network output, and the nonconvex function $\\mathcal{L}_{\\mathrm{CutMix}}(W)$ can be viewed as the composition of $Z(W)$ and a convex function $h(Z)$ . By using the convexity of $h(Z)$ , we can characterize the global minimum of $\\mathcal{L}_{\\mathrm{CutMix}}(W)$ . Surprisingly, we show that any global minimizer $W^{*}=\\lbrace{\\pmb w}_{1}^{*},{\\pmb w}_{-1}^{*}\\rbrace$ satisfies ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\phi\\left(\\left\\langle w_{s}^{*},\\pmb{x}_{i}^{(p)}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-s}^{*},\\pmb{x}_{i}^{(p)}\\right\\rangle\\right)=C_{s},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "for all $s\\in\\{\\pm1\\},i\\in\\mathcal{V}_{s}$ , and $p\\in[P]$ , with some constants $C_{1},C_{-1}=\\Theta(1)$ . In other words, at the global minimum, the output of model on each patch of the training data is uniform across the set of data with the same labels. We also prove that CutMix can achieve a point close to the global minimum within $\\frac{\\mathrm{poly}(d)}{\\eta}$ iterations. As a result,the model rained by CutMix ca Iearn ll features including extremely rare features. The complete proof of Theorem 3.3 appears in Appendix E.2. Remark 4.2. Zou et al. (2023) investigate Mixup in a similar feature-noise model and show that Mixup can learn rarer features than vanilla training, with its benefits emerging from the early dynamics of training. However, our characterization of the global minimum of $\\mathcal{L}_{\\mathrm{CutMix}}(W)$ and experimental results in our setting (Section 5, Figure 1) suggest that the benefits of CutMix, especially for learning extremely rare features, arise from the later stages of training. This suggests that Mixup and CutMix have different underlying mechanisms for promoting feature learning. ", "page_idx": 8}, {"type": "text", "text": "Practical Insights.  The main underlying mechanism of CutMix is that it learns information almost uniformly from all patches in the training data. However, this approach also involves memorizing noise, which can potentially degrade performance in real-world scenarios. We believe that a more sophisticated strategy such as considering the positional information of patches as used in Puzzle Mix (Kim et al., 2020) or Co-Mixup (Kim et al., 2021) could improve the ability to learn more from patches containing features and reduce the impact of noise. ", "page_idx": 8}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We conduct experiments both in our setting and real-world data CIFAR-10 to support our theoretical findings and intuition. We defer CIFAR-10 experiment results to Appendix A.1. ", "page_idx": 8}, {"type": "text", "text": "For the numerical experiments on our setting, we set the number of patches $P\\,=\\,3$ , dimension $d=2000$ , number of data points $n=300$ , dominant noise strength $\\sigma_{\\mathrm{d}}=0.25$ , background noise strength $\\sigma_{\\mathrm{b}}\\,=\\,0.15$ , and feature noise strength $\\alpha\\,=\\,0.005$ . The feature vectors are given as the standard basis $e_{1},e_{2},e_{3},e_{4},e_{5},e_{6}\\in\\mathbb{R}^{d}$ ,where $e_{1},e_{2},e_{3}$ are features for the positive label $y=1$ and $e_{4},e_{5},e_{6}$ are features for the negative label $y=-1$ . We categorize $e_{1}$ and $e_{4}$ as common features with a frequency of 0.8, $e_{2}$ and $e_{5}$ as rare features with a frequency of 0.15, and lastly, $e_{3}$ and $e_{6}$ as extremely rare features with a frequency of 0.05. For the learner network, we set the slope of negative regime $\\beta=0.1$ and the length of the smoothed interval $r=1$ . We train models using three methods: ERM, Cutout, and CutMix with a learning rate $\\eta=1$ . For Cutout, we cut a single patch of data ( $C=1$ 0. We apply full-batch gradient descent for all methods; for Cutout and CutMix, we utilize all possible augmented data points.3 We note that this choice of problem parameters does not exactly match the technical assumptions in Section 2.4. However, we empirically observe the same conclusions, which suggests that our analysis could be extended beyond our assumptions. ", "page_idx": 8}, {"type": "text", "text": "For eachfeaturevector $\\pmb{v}$ of the positive label, we plot the output of the learned filters for the feature vector $\\phi\\big(\\langle\\pmb{w}_{1}^{(t)},\\pmb{v}\\rangle\\big)-\\phi\\big(\\langle\\pmb{w}_{-1}^{(t)},\\bar{\\pmb{v}}\\rangle\\big)$ throughout training in Figure 1. Our numerical findings confirm that ERM can only learn common features, Cutout can learn common and rare features but cannot learn extremely rare features, and CutMix can learn all types of features. Especially, CutMix learn common features, rare features, and extremely rare features almost evenly. Also, we observed nonmonotone behavior of the output in the case of CutMix, which motivated our novel proof technique. The same trends are observed with different architectures, such as a smoothed (leaky) ReLU network with multiple neurons, as detailed in Appendix A.2. ", "page_idx": 9}, {"type": "image", "img_path": "8on9dIUh5v/tmp/b373b4b1b4eda1abbbf656f0cddf0c0bca84c8e619a829cf75c78026a630ec28.jpg", "img_caption": ["Figure 1: Numerical results on our problem setting. We validate our findings on the trends of ERM, Cutout, and CutMix in learning common feature (Left), rare feature (Center), and extremely rare feature (Right). The output of the common feature trained by CutMix shows non-monotone behavior. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We studied how Cutout and CutMix influence the ability to learn features in a patch-wise featurenoise data model learning with two-layer convolutional neural networks by comparing them with vanilla training. We showed that Cutout enables the learning of rare features that cannot be learned through vanilla training by mitigating the problem of memorizing label-independent noises instead of learning label-dependent features. Surprisingly, we further proved that CutMix can learn extremely rare features that Cutout cannot learn. We also present our theoretical insights on the underlying mechanism of these methods and provide experimental support. ", "page_idx": 9}, {"type": "text", "text": "Limitation and Future Work.  Our work has some limitations related to the neural network architecture, specifically, the use of a 2-layer two-neuron smoothed leaky ReLU network. Extending our results to neural networks with deeper, wider, and more general activation functions is a direction for future work. Another future direction is to develop patch-level data augmentation based on our theoretical findings. Also, it would be interesting to perform theoretical analysis on state-of-the-art patch-level data augmentation such as Puzzle Mix (Kim et al., 2020) or Co-Mixup (Kim et al., 2021). These methods utilize patch location information, thus it may require the development of a theoretical framework capturing more complex characteristics of image data. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by three Institute of Information & communications Technology Planning & Evaluation (IITP) grants (No. RS-2019-IH190075, Artificial Intelligence Graduate School Program (KAIST); No. RS-2022-1I220184, Development and Study of AI Technologies to Inexpensively Conform to Evolving Policy on Ethics; No. RS-2024-00457882, AI Research Hub Project) funded by the Korean government (MSIT), and a National Research Foundation of Korea (NRF) grant (No. RS-2019-NR040050) funded by the Korean government (MSIT). CY acknowledges support from a grant funded by Samsung Electronics Co., Ltd. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Zeyuan Allen-Zhu and Yuanzhi Li. Towards understanding ensemble, knowledge distillation and self-distillation in deep learning. arXiv preprint arXiv:2012.09816, 2020.   \nSebastien Bubeck et al. Convex optimization: Algorithms and complexity. Foundations and Trends@ in Machine Learning, 8(3-4):231-357, 2015.   \nYuan Cao, Zixiang Chen, Misha Belkin, and Quanquan Gu. Benign overitting in two-layer convolutional neural networks. Advances in neural information processing systems, 35:25237-25250, 2022.   \nLuigi Carratino, Moustapha Cisse, Rodolphe Jenatton, and Jean-Philippe Vt. On mixup regularzation. arXiv preprint arXiv:2006.06049, 2020.   \nPengguang Chen, Shu Liu, Hengshuang Zhao, and Jiaya Jia. Gridmask data augmentation. arXiv preprint arXiv:2001.04086, 2020a.   \nShuxiao Chen, Edgar Dobriban, and Jane H Lee. A group-theoretic framework for data augmentation. In Proceedings of the 34th International Conference on Neural Information Processing Systems, pages 21321-21333, 2020b.   \nZixiang Chen, Yihe Deng, Yue Wu, Quanquan Gu, and Yuanzhi Li. Towards understanding the mixture-of-experts layer in deep learning. Advances in neural information processing systems, 35: 23049-23062, 2022.   \nZixiang Chen, Junkai Zhang, Yiwen Kou, Xiangning Chen, Cho-Jui Hsieh, and Quanquan Gu. Why does sharpness-aware minimization generalize better than sgd? In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \nMuthu Chidambaram and Rong Ge. For better or for worse? learning minimum variance features with label augmentation. arXiv preprint arXiv:2402.06855, 2024.   \nMuthu Chidambaram, Xiang Wang, Yuzheng Hu, Chenwei Wu, and Rong Ge. Towards understanding the data dependency of mixup-style training. arXiv preprint arXiv:2110.07647, 2021.   \nMuthu Chidambaram, Xiang Wang, Chenwei Wu, and Rong Ge. Provably learning diverse features inmulti-view data with midpoint mixup. In International Conference on Machine Learning, pages 5563-5599. PMLR, 2023.   \nTri Dao, Albert Gu, Alexander Ratner, Virginia Smith, Chris De Sa, and Christopher R\u00e9. A kernel theory of modern data augmentation. In International conference on machine learning, pages 1528-1537. PMLR, 2019.   \nTerrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017.   \nSpencer Frei, Niladri S Chatterji, and Peter Bartlett. Benign overfitting without linearity: Neural network classifers trainedby gradient descent for noisy linear data. In Conference on Learning Theory, pages 2668-2703. PMLR, 2022a.   \nSpencer Frei, Gal Vardi, Peter L Bartlett, Nathan Srebro, and Wei Hu. Implicit bias in leaky relu networks trained on high-dimensional data. arXiv preprint arXiv:2210.07082, 2022b.   \nBoris Hanin and Yi Sun. How data augmentation affects optimization for linear regression. Advances in Neural Information Processing Systems, 34:8095-8105, 2021.   \nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for mage recognition. In Proceedings of the iEEE conference on computervision and pattern recognition, pages 770-778, 2016.   \nWei Huang, Yuan Cao, Haonan Wang, Xin Cao, and Taiji Suzuki. Graph neural networks provably beneft from structural information: A feature learning perspective. arXiv preprint arXiv:2306.13926, 2023a.   \nWei Huang, Ye Shi, Zhongyi Cai, and Taiji Suzuki. Understanding convergence and generalization in federated learning through feature learning theory. In The Twelfth International Conference on Learning Representations, 2023b.   \nSamy Jelassi and Yuanzhi Li. Towards understanding how momentum improves generalization in deep learning. In International Conference on Machine Learning, pages 9965-10040. PMLR, 2022.   \nZiheng Jiang, Chiyuan Zhang, Kunal Talwar, and Michael C Mozer. Characterizing structural regularities of labeled data in overparameterized models. In International Conference on Machine Learning, pages 5034-5044. PMLR, 2021.   \nJang-Hyun Kim, Wonho Choo, and Hyun Oh Song. Puzzle mix: Exploiting saliency and local statistics for optimal mixup. In International Conference on Machine Learning, pages 5275-5285. PMLR, 2020.   \nJang-Hyun Kim, Wonho Choo, Hosan Jeong, and Hyun Oh Song. Co-mixup: Saliency guided joint mixup with supermodular diversity. arXiv preprint arXiv:2102.03065, 2021.   \nYiwen Kou, Zixiang Chen, Yuanzhou Chen, and Quanquan Gu. Benign overfitting in two-layer reluconvoltional neuraletworks. In International Conference onMachine Learning, pages 17615-17659. PMLR, 2023a.   \nYiwen Kou, Zixiang Chen, and Quanquan Gu. Implicit bias of gradient descent for two-layer relu and leaky relu networks on nearly-orthogonal data. In Thirty-seventh Conference on Neural Information Processing Systems, 2023b.   \nAlex Krizhevsky, lya Sutskever, and Geoffrey E Hinton. Imagenet classfication with deep convolutional neural networks. Advances in neural information processing systems, 25, 2012.   \nBinghui Li and Yuanzhi Li. Why clan generalization and robust overfitting both happen in adversarial training. arXiv preprint arXiv:2306.01271, 2023.   \nJunsoo Oh and Chulhee Yun. Provable benefit of mixup for fnding optimal decision boundaries. In International Conference on Machine Learning, pages 26403-26450. PMLR, 2023.   \nChanwoo Park, Sangdoo Yun, and Sanghyuk Chun. A unifed analysis of mixed sample data augmentation: A loss function perspective. Advances in Neural Information Processing Systems, 35:35504-35518, 2022.   \nShashank Rajput, Zhili Feng, Zachary Charles, Po-Ling Loh, and Dimitris Papailiopoulos. Does data augmentation lead to positive margin? In International Conference on Machine Learning, pages 5321-5330. PMLR, 2019.   \nRuoqi Shen, Sebastien Bubeck, and Suriya Gunasekar. Data augmentation as feature manipulation. In International conference on machine learning, pages 19773-19808. PMLR, 2022.   \nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.   \nRoman Vershynin. High-dimensional probability: An introduction with applications in data science, volume 47. Cambridge university press, 2018.   \nSen Wu, Hongyang Zhang, Gregory Valiant, and Christopher Re. On the generalization effects of linear transformations in data augmentation. In International Conference on Machine Learning, pages 10410-10420. PMLR, 2020.   \nJaejun Yoo, Namhyuk Ahn, and Kyung-Ah Sohn. Rethinking data augmentation for image superresolution: A comprehensive analysis and a new strategy. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8375-8384, 2020.   \nSangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6023-6032, 2019.   \nHongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017.   \nLinjun Zhang, Zhun Deng, Kenji Kawaguchi, Amirata Ghorbani, and James Zou. How does mixup help with robustness and generalization? arXiv preprint arXiv:2010.04819, 2020.   \nLinjun Zhang, Zhun Deng, Kenji Kawaguchi, and James Zou. When and how mixup improves calibration. In International Conference on Machine Learning, pages 26135-26160. PMLR, 2022.   \nZhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 13001-13008, 2020.   \nDifan Zou, Yuan Cao, Yuanzhi Li, and Quanquan Gu. Understanding the generalization of adam in learning neural networks with proper regularization. arXiv preprint arXiv:2108.11371, 2021.   \nDifan Zou, Yuan Cao, Yuanzhi Li, and Quanquan Gu. The benefits of mixup for feature learning. In International Conference on Machine Learning, pages 43423-43479. PMLR, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "1.1 Our Contributions 2   \n1.2  Related Works . 2 ", "page_idx": 13}, {"type": "text", "text": "2  Problem Setting ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "2.1 Data Distribution 3   \n2.2 Neural Network Architecture 4   \n2.3 Training Methods 4   \n2.4  Assumptions on the Choice of Problem Parameters 6 ", "page_idx": 13}, {"type": "text", "text": "3Main Results 6 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "4  Overview of Analysis ", "page_idx": 13}, {"type": "text", "text": "4.1 Vanilla Training and Cutout Training 8   \n4.2 CutMix Training 9 ", "page_idx": 13}, {"type": "text", "text": "5 Experiments 9 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "6Conclusion 10 ", "page_idx": 13}, {"type": "text", "text": "A Additional Experimental Results 15 ", "page_idx": 13}, {"type": "text", "text": "A.1 Experiments on CIFAR-10 Dataset 15   \nA.2 Additional Experimental Results on Our Data Distribution 19 ", "page_idx": 13}, {"type": "text", "text": "B Proof Preliminaries 20 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1 Properties of the Choice of Problem Parameters 20   \nB.2 Quantities at the Beginning . . 21   \nB.3 Feature Noise Decomposition 23 ", "page_idx": 13}, {"type": "text", "text": "C Proof for ERM 29 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "C.1 Proof of Lemma B.3 for ERM 29   \nC.2 Proof of Theorem 3.1 30   \nD Proof for Cutout 45   \nD.1 Proof of Lemma B.3 for Cutout 45   \nD.2 Proof of Theorem 3.2 46 ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "E Proof for CutMix 62 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "E.1 Proof of Lemma B.3 for CutMix 62   \nE.2Proof of Theorem 3.3 62 ", "page_idx": 13}, {"type": "text", "text": "F Technical Lemmas 75 ", "page_idx": 13}, {"type": "text", "text": "A Additional Experimental Results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "For all experiments described in this section and in Section 5, we use NVIDIA RTX A6000 GPUs. ", "page_idx": 14}, {"type": "text", "text": "A.1 Experiments on CIFAR-10 Dataset ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1.1 Experimental Support for Our Intuition ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We compare three methods, ERM training, Cutout training, and CutMix training on CIFAR-10 classification. For ERM training, we apply only random cropping and random horizontal flipping on train dataset. In comparison, for Cutout training and CutMix training, we additionally apply Cutout and CutMix, respectively, on training data. For Cutout training, we randomly cut $16\\times16$ pixels of input images, and for CutMix training, we sample the mixing ratio from a beta distribution Beta(0.5, 0.5). We train ResNet-18 (He et al., 2016) for 200 epochs with a batch size of 128 using SGD with a learning rate 0.1, momentum 0.9, and weight decay $5\\times10^{-4}$ . Trained models using ERM, Cutout, and CutMix achieve test accuracy $95.16\\bar{\\%}$ $96.05\\%$ ,and $96.29\\%$ ,respectively. ", "page_idx": 14}, {"type": "text", "text": "We randomly generate augmented data using CutMix from pairs of cat images and dog images in CIFAR-10 with varying mixing ratios $\\lambda=1,0.8,0.6$ $\\left(\\mathrm{Dog};\\mathrm{Cat}=\\lambda:1-\\lambda\\right)$ . We randomly make 5, 000 (cat, dot)-pairs in CIFAR-10 training set and apply CutMix randomly 10 times. By repeating this procedure 10 times, we generate total ! $5,000\\times10\\times10=500,$ 000 augmented samples for each mixing ratio $\\lambda$ . We plot a histogram of dog prediction output subtracted by cat prediction output (before applying the softmax function), evaluated on 500, 000 augmented data in Figure 2. ", "page_idx": 14}, {"type": "image", "img_path": "8on9dIUh5v/tmp/3c049464ae31aeef2e4a40a156695aa3ee13902ba3eee0c1a604e51914f977b2.jpg", "img_caption": ["Figure 2: Histogram of dog prediction output subtracted by cat prediction output evaluated on data points augmented by CutMix data using cat data and dog data with varying mixing ratio $\\lambda$ $(\\mathrm{Dog}:\\mathrm{Cat}=\\lambda:1-\\lambda)$ (Left) $\\lambda=1$ ,(Center) $\\lambda=0.8$ (Right) $\\lambda=0.6$ "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "The leftmost plot represents the evaluation results for original dog images, as it uses a mixing ratio of $\\lambda=1$ . We can observe that the output of the model trained using Cutout is skewed toward higher values compared to the output of the model trained using other methods. We believe this aligns with the theoretical intuition that Cutout learns more information from the original image using augmented data. ", "page_idx": 14}, {"type": "text", "text": "The remaining two plots show the output for randomly augmented data using CutMix. We observe that the models trained with CutMix exhibit a shorter tail, supporing our intuition from the CutMix analysis that the models learn uniformly across all patches. ", "page_idx": 14}, {"type": "text", "text": "A.1.2 Experimental Support for Our Findings ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We train ResNet-18 using ERM training, Cutout training, and CutMix training following the same experimental details described in Appendix A.1.1, except using only $10\\%$ of the training set. This data-hungry setting is intended to highlight the benefits of Cutout and CutMix. We then evaluated the trained models on the remaining $90\\%$ of the CIFAR-10 training dataset. The reason for evaluating the remaining training dataset is to analyze the misclassified data using C-score (Jiang et al., 2021), which is publicly available only for the training dataset. ", "page_idx": 14}, {"type": "text", "text": "C-score measures the structural regularity of data, with lower values indicating examples that are more difficult to classify correctly. In our framework, data with harder-to-learn features (corresponding to rarer features) would likely have lower C-scores. Since directly extracting and quantitatively evaluating features learned by the models is challenging, we use the C-score as a proxy to evaluate the misclassified data across models trained by ERM, Cutout, and CutMix. ", "page_idx": 15}, {"type": "text", "text": "Table 1 illustrates that Cutout tends to misclassify data with lower C-scores compared to ERM, indicating that Cutout learns more hard-to-learn features than vanilla training. Furthermore, the data misclassified by CutMix has even lower C-scores than those misclassified by Cutout, suggesting that CutMix is effective at learning features that are the most challenging to classify. This observation aligns with our theoretical findings, demonstrating that CutMix captures even more difficult features compared to both ERM and Cutout. ", "page_idx": 15}, {"type": "text", "text": "Table 1: Mean and quantiles of the C-score on misclassified data across models trained with ERM, Cutout, and CutMix. The results indicate that Cutout tends to misclassify data with lower C-scores compared to ERM, while CutMix exhibits even lower C-scores. ", "page_idx": 15}, {"type": "table", "img_path": "8on9dIUh5v/tmp/0579580e521460865c6a0241f95968217102ee1a7e260455d7e227106a4516ac.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Since directly visualizing features learned by a model is challenging, we present data that were misclassified by the model trained with ERM but correctly classified by the model trained with Cutout instead. In Figure 3, we show 7 samples per class with the lowest C-scores, which are considered to have rare features. Similarly, we also visualize data misclassified by the model trained with Cutout but correctly classified by the model trained with CutMix to represent extremely rare data in Figure 4. This approach allows us to interpret some (extremely) rare features in CIFAR-10, such as frogs with unusual colors. ", "page_idx": 15}, {"type": "image", "img_path": "8on9dIUh5v/tmp/ed9b334a904f8a326199231cf39c59231cc1d75705897a47c3c0436ba2518ac9.jpg", "img_caption": ["Figure 3: Examples of rare data in CIFAR-10 "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "8on9dIUh5v/tmp/f5c0ad084d855c2e1648930e8ecb540b86c38751a78ecc160d72c96a52ba3325.jpg", "img_caption": ["Figure 4: Examples of extreme data in CIFAR-10 "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "A.2 Additional Experimental Results on Our Data Distribution ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In addition to the results described in Section 5, we further conducted numerical experiments on our data distribution by applying two variations to our architecture: increasing the number of neurons, and increasing the number of neurons with a smoothed ReLU activation (instead of smoothed leaky ReLU). We observed the same trends as predicted by our theoretical findings and shown in Figure 1. ", "page_idx": 18}, {"type": "text", "text": "Let us describe the setting of our experiments in detail. In both cases, We set the number of patches $P=3$ , dimension $d=2000$ , and the number of data $n=300$ . The feature vectors are given by the standard basis $e_{1},e_{2},e_{3},e_{4},e_{5},e_{6}\\in\\mathbb{R}^{d}$ where $e_{1},e_{2},e_{3}$ are features for the positive label $y=1$ and $e_{4},e_{5},e_{6}$ are features for the negative label $y=-1$ . We categorize $e_{1}$ and $e_{4}$ as common features, $e_{2}$ and $e_{5}$ as rare features, and lastly, $e_{3}$ and $e_{6}$ as extremely rare features. We apply full-batch gradient descent with learning rate $\\eta\\,=\\,1$ and for Cutout and CutMix, we utilize all possible augmented data. ", "page_idx": 18}, {"type": "text", "text": "For the multi-neuron with smoothed Leaky ReLU case (Figure 5), we use 10 neurons for each positive/negative output with the slope of negative regime $\\beta=0.1$ and the length of polynomial regime $r=1$ . We set the strength of dominant noise $\\sigma_{\\mathrm{d}}=0.25$ , the strength of background noise $\\sigma_{\\mathrm{b}}=0.12$ , and the strength of feature noise $\\alpha=0.05$ . In addition, frequencies of common features, rare features, and extremely rare features are set to 0.72, 0.15, and 0.03, respectively. ", "page_idx": 18}, {"type": "text", "text": "For the multi-neuron with smoothed ReLU case i.e., $\\beta\\,=\\,0$ (Figure 6), we set the length of the polynomial regime as $r=1$ , and we use 10 neurons for each positive/negative output. We set the remaining problem parameters as follows: the strength of dominant noise $\\sigma_{\\mathrm{d}}=0.25$ , the strength of background noise $\\sigma_{\\mathrm{b}}=0.12$ , and the strength of feature noise $\\alpha=0.05$ . In addition, frequencies of common features, rare features, and extremely rare features are set to 0.75, 0.2, and 0.05, respectively. ", "page_idx": 18}, {"type": "image", "img_path": "8on9dIUh5v/tmp/23799db8b2b49ad4c056d4a57c633c3d1786beb1a381f9e64b620e7c5cf2ef4f.jpg", "img_caption": ["Figure 5: Multi-neuron with a smoothed leaky ReLU actiation "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "8on9dIUh5v/tmp/4a4a9d652d6baa4dc72a44927f4e565f350e2baba37ebcadce25c99bd1bee3a2.jpg", "img_caption": ["Figure 6: Multi-neuron with a smoothed ReLU "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "B Proof Preliminaries ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "B.1 Properties of the Choice of Problem Parameters ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In our analysis, we consider the choice of problem parameters as a function of the dimension of patches $d$ and consider sufficiently large $d$ . Let us summarize the assumptions on the parameters for the problem setting and assume they hold. ", "page_idx": 19}, {"type": "text", "text": "Assumption B.1. The following conditions hold. ", "page_idx": 19}, {"type": "text", "text": "A1. (The number of patches) $P=\\Theta(1)$ and $P\\geq8$ ", "page_idx": 19}, {"type": "text", "text": "A2. (Overparameterized regime): $n=o\\left(\\alpha\\beta\\sigma_{\\mathrm{d}}^{-1}\\sigma_{\\mathrm{b}}d^{\\frac{1}{2}}/\\mathrm{polylog}(d)\\right)\\mathrm{.}$ ", "page_idx": 19}, {"type": "text", "text": "A3. (Sufficent fature data): For all $k\\in[K]$ $\\rho_{k}n=\\omega\\left(n^{\\frac{1}{2}}\\log d\\right)$ ", "page_idx": 19}, {"type": "text", "text": "A4. (Common feature vs dominant noise): For all $k\\in\\mathcal{K}_{C},\\rho_{k}=\\Theta(1)$ and $\\sigma_{\\mathrm{d}}^{2}d=o(\\beta n)$ ", "page_idx": 19}, {"type": "text", "text": "A5. (Rare feature vs noise): For all $k\\in K_{R},\\rho_{k}=\\Theta(\\rho_{R})$ with $\\rho_{R}n=o\\left(\\alpha^{2}\\sigma_{\\mathrm{d}}^{2}d/\\mathrm{polylog}(d)\\right)$ and $\\sigma_{\\mathrm{b}}^{2}d=o(\\beta\\rho_{R}n)$ ", "page_idx": 19}, {"type": "text", "text": "A6. (Extremely rare feature vs background noise) For all $k\\,\\in\\,\\mathcal{K}_{E},\\rho_{k}\\,=\\,\\Theta(\\rho_{E})$ With $\\rho_{E}n\\;=\\;$ $o\\left(\\alpha^{2}\\sigma_{\\mathrm{b}}^{2}d/\\mathrm{polylog}(d)\\right)$ ", "page_idx": 19}, {"type": "text", "text": "A7. (Strength of feature noise) $\\alpha=o\\left(n^{-1}\\beta\\sigma_{\\mathrm{d}}^{2}d/\\mathrm{polylog}(d)\\right)$ ", "page_idx": 19}, {"type": "equation", "text": "$\\sigma_{0}\\sigma_{\\mathrm{d}}^{2}d,r=o\\left(\\alpha/\\mathrm{polylog}(d)\\right),\\eta=o\\left(r\\sigma_{\\mathrm{d}}^{-2}d^{-1}/\\mathrm{polylog}(d)\\right)$ ", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We now present some properties derived from Assumption B.1, which are frequently used throughout our proof. ", "page_idx": 19}, {"type": "text", "text": "From (A3), for all $k\\in[K]$ , we have the following inequality: ", "page_idx": 19}, {"type": "equation", "text": "$$\nn\\geq\\rho_{1}n\\geq\\rho_{k}^{2}n=\\omega\\left(\\log^{2}d\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "From (A1) and (A2), and given that $\\beta<1,\\sigma_{\\mathrm{b}}<\\sigma_{\\mathrm{d}}$ ,we have ", "page_idx": 19}, {"type": "equation", "text": "$$\nd>(\\beta\\sigma_{\\mathrm{d}}^{-1}\\sigma_{\\mathrm{b}}d^{\\frac{1}{2}})^{2}>n^{2}P>n P.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "From (A2), (A3), and (A6), and given that $\\alpha,\\beta<1$ ,wehave ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sigma_{\\mathrm{d}}^{2}d>\\sigma_{\\mathrm{b}}^{2}d=\\omega(\\rho_{E}n)=\\omega(1).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "From (A1), (A2) and the fact that $0<\\alpha<1$ ,we have ", "page_idx": 19}, {"type": "equation", "text": "$$\nn P\\beta^{-1}\\sigma_{\\mathrm{d}}\\sigma_{\\mathrm{b}}^{-1}d^{-\\frac{1}{2}}=o\\left(\\frac{\\alpha}{\\mathrm{polylog}(d)}\\right)=o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "From (A7) and (A4), we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\alpha\\beta^{-1}<\\alpha\\beta^{-2}=o\\left(\\frac{n^{-1}\\beta^{-1}\\sigma_{\\mathrm{d}}^{2}d}{\\mathrm{polylog}(d)}\\right)=o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "From (8) and (A8), $\\eta=o(1)$ and then we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\eta\\leq\\frac{\\log(\\eta T^{*})}{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "From (A2), (A3), (A4), and (A5) we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\alpha^{-2}=o\\left(\\frac{\\sigma_{\\mathrm{d}}^{2}d}{\\rho_{R}n}\\right)=o\\left(\\rho_{R}^{-1}\\right)=o\\left(n^{\\frac{1}{2}}\\right)=o\\left(d^{\\frac{1}{4}}\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "B.2 Quantities at the Beginning ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We characterize some quantities at the beginning of training. Lemma B.2. Let $E_{\\mathrm{init}}$ theeventsuch that all thefollowingholds: ", "page_idx": 20}, {"type": "text", "text": "\u00b7 $\\begin{array}{r}{{\\frac{25}{52}}n\\leq|\\mathcal{V}_{1}|,|\\mathcal{V}_{-1}|\\leq{\\frac{27}{52}}n}\\end{array}$   \n\u00b7 For each $s\\in\\{\\pm1\\}$ and $\\begin{array}{r}{k\\in[K],\\,\\frac{\\rho_{k}n}{4}\\leq|\\mathcal{V}_{s,k}|\\leq\\frac{3\\rho_{k}n}{4}}\\end{array}$   \n$\\cup_{i\\in\\mathcal{V}_{1,1}}\\{p_{i}^{*}\\}=[P]$   \n\u00b7 For any $s,s^{\\prime}\\in\\{\\pm1\\}$ and $k\\in[K],\\left|\\left\\langle\\pmb{w}_{s}^{(0)},\\pmb{v}_{s^{\\prime},k}\\right\\rangle\\right|\\leq\\sigma_{0}\\log d.$   \n\u00b7For any $s\\in\\{\\pm1\\}$ andl $i\\in[n],\\left\\lvert\\left\\langle\\pmb{w}_{s}^{(0)},\\xi_{i}^{(\\tilde{p}_{i})}\\right\\rangle\\right\\rvert\\leq\\sigma_{0}\\sigma_{\\mathrm{d}}d^{\\frac{1}{2}}\\log d.$   \n\u00b7 For any $s\\in\\{\\pm1\\}$ $_{2}1\\},i\\in[n]\\ a n d\\,p\\in[P]\\setminus\\{p_{i}^{*},\\tilde{p}_{i}\\},\\left|\\left\\langle{\\pmb w}_{s}^{(0)},{\\xi}_{i}^{(p)}\\right\\rangle\\right|\\leq\\sigma_{0}\\sigma_{\\mathrm{b}}d^{\\frac{1}{2}}\\log d.$ \u00b7 For any $\\begin{array}{r}{i,j\\in[n]\\ w i t h\\ i\\not=j,\\,\\frac{1}{2}\\sigma_{\\mathrm{d}}^{2}d\\leq\\left\\|\\xi_{i}^{(\\tilde{p}_{i})}\\right\\|^{2}\\leq\\frac{3}{2}\\sigma_{\\mathrm{d}}^{2}d\\,a n d\\left|\\left\\langle\\xi_{i}^{(\\tilde{p}_{i})},\\xi_{j}^{(\\tilde{p}_{j})}\\right\\rangle\\right|\\leq\\sigma_{\\mathrm{d}}^{2}d^{\\frac{1}{2}}\\log d.}\\end{array}$ \u00b7 For any $i,j\\in[n]$ and $p\\in[P]\\setminus\\{p_{j}^{*},\\tilde{p}_{j}\\},\\left|\\left\\langle\\xi_{i}^{(\\tilde{p}_{i})},\\xi_{j}^{(p)}\\right\\rangle\\right|\\leq\\sigma_{\\mathrm{d}}\\sigma_{\\mathrm{b}}d^{\\frac{1}{2}}\\log d.$   \n\u00b7 For any $i,j\\in[n]$ and $p\\in[P]\\setminus\\{p_{i}^{*},\\tilde{p}_{i}\\},q\\in[P]\\setminus\\{p_{j}^{*},\\tilde{p}_{j}\\}\\ w i t h\\ (i,p)\\neq(j,q),$   \n$\\begin{array}{r}{\\frac{1}{2}\\sigma_{\\mathrm{b}}^{2}d\\leq\\left\\|\\xi_{i}^{(p)}\\right\\|^{2}\\leq\\frac{3}{2}\\sigma_{\\mathrm{b}}^{2}d}\\end{array}$ and $\\left|\\left\\langle\\xi_{i}^{(p)},\\xi_{j}^{(q)}\\right\\rangle\\right|\\leq\\sigma_{\\mathrm{b}}^{2}d^{\\frac{1}{2}}\\log d.$   \n\u00b7 $\\{v_{s,k}\\}_{s\\in\\{\\pm1\\},k\\in[K]}\\cup\\{{\\pmb x}_{i}^{(p)}\\}_{i\\in[n],p\\in[P]\\backslash\\{p_{i}^{*}\\}}$ is linearly independent. ", "page_idx": 20}, {"type": "text", "text": "Then, the event $E_{\\mathrm{init}}$ occurs with probabiliy at least $\\begin{array}{r}{1-o\\left(\\frac{1}{\\mathrm{poly}(d)}\\right)}\\end{array}$ .Also, $i f\\xi\\ \\sim N(\\mathbf{0},\\sigma^{2}\\Lambda)$ .3 independentofw,w and {(Xi,)e[m), we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\left\\langle w_{1}^{(0)},\\xi\\right\\rangle\\right|,\\left|\\left\\langle w_{-1}^{(0)},\\xi\\right\\rangle\\right|\\leq\\sigma_{0}\\sigma d^{\\frac{1}{2}}\\log d,\\;a n d\\;\\left|\\left\\langle\\xi,\\xi_{i}^{(p)}\\right\\rangle\\right|\\leq\\sigma\\sigma_{\\mathrm{d}}d^{\\frac{1}{2}}\\log d,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for all $i\\in[n]$ and $p\\in[P]\\setminus\\{p_{i}^{*}\\}$ with probability at least $\\textstyle1-o\\left({\\frac{1}{\\operatorname{poly}(d)}}\\right)$ ", "page_idx": 20}, {"type": "text", "text": "Profof Lema B.2. Le us prove th frst the pins hold with probability at least $\\textstyle1-o\\left({\\frac{1}{\\operatorname{poly}(d)}}\\right)$ By Hoeffding's inequality, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left[\\left|\\left|\\mathcal{V}_{1}\\right|-\\frac{n}{2}\\right|>\\frac{n}{52}\\right]=\\mathbb{P}\\left[\\left|\\displaystyle\\sum_{i\\in[n]}\\left(\\mathbb{1}_{y_{i}=1}-\\mathbb{E}[\\mathbb{1}_{y_{i}=1}]\\right)\\right|>\\frac{n}{52}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leq2\\exp\\left(-\\frac{2}{52^{2}}n\\right)=o\\left(\\frac{1}{\\mathrm{poly}(d)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the last equality is due to (6). In addition, for each $s\\,\\in\\,\\{\\pm1\\},k\\,\\in\\,[K]$ , by Hoeffding's inequality ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left[\\left|\\vert\\mathcal{V}_{s,k}\\vert-\\frac{\\rho_{k}}{2}n\\right|>\\frac{\\rho_{k}}{4}n\\right]=\\mathbb{P}\\left[\\left|\\sum_{i\\in[n]}\\left(\\mathbb{1}_{i\\in\\mathcal{V}_{s,k}}-\\mathbb{E}[\\mathbb{1}_{i\\in\\mathcal{V}_{s,k}}]\\right)\\right|>\\frac{\\rho_{k}}{4}n\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq2\\exp\\left(-\\frac{\\rho_{k}^{2}}{8}n\\right)=o\\left(\\frac{1}{\\mathrm{poly}(d)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the last equality is due to (6). Also, for each $i\\in[n]$ and $p\\in[P]$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{P}[\\{i\\in\\mathcal{V}_{1,1}\\}\\cap\\{p_{i}^{*}=p\\}]=\\frac{\\rho_{1}}{P}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Hence, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left[\\cup_{i\\in\\mathcal{V}_{1,1}}\\{p_{i}^{*}\\}\\neq[P]\\right]\\leq\\displaystyle\\sum_{p\\in[P]}\\mathbb{P}\\left[\\bigcap_{i\\in[n]}\\left(\\left(\\{i\\in\\mathcal{V}_{1,1}\\}\\cap\\{p_{i}^{*}=p\\}\\right)^{\\complement}\\right)\\right]}\\\\ &{\\qquad\\qquad\\qquad=P\\left(1-\\frac{\\rho_{1}}{P}\\right)^{n}\\leq P\\exp\\left(-\\frac{\\rho_{1}}{P}n\\right)}\\\\ &{\\qquad\\qquad\\qquad=o\\left(\\frac{1}{\\mathrm{poly}(d)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Next, we will prove the remaining. Let us refer to the standard deviation of the Gaussian noise vector in $p$ -th patch of $i$ -th data as $\\sigma_{i,p}$ . In other words, for each $i\\in[n]$ and $p\\in[P]\\setminus\\{p_{i}^{*}\\}$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sigma_{i,p}=\\left\\{\\!\\!\\begin{array}{l l}{\\sigma_{\\mathrm{d}}}&{\\mathrm{if}\\ p=\\tilde{p}_{i},}\\\\ {\\sigma_{\\mathrm{b}}}&{\\mathrm{otherwise}.}\\end{array}\\!\\right.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For each $s,s^{\\prime}\\in\\{\\pm1\\}$ and $k\\in[K],\\left\\langle\\pmb{w}_{s}^{(0)},\\pmb{v}_{s^{\\prime},k}\\right\\rangle\\sim N(0,\\sigma_{0})$ Hence, by Heffdings inequalty we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\left|\\left\\langle w_{s}^{(0)},v_{s^{\\prime},k}\\right\\rangle\\right|>\\sigma_{0}\\log d\\right]\\le2\\exp\\left(-\\frac{\\left(\\sigma_{0}\\log d\\right)^{2}}{2\\sigma_{0}^{2}}\\right)=o\\left(\\frac{1}{\\mathrm{poly}(d)}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Let $\\{u_{l}\\}_{l\\in[d-2K]}$ be   an   orthonormal basis  of  the  orthogonal complement   of $\\operatorname{Span}(\\{v_{s,k}\\}_{s\\in\\{\\pm1\\},k\\in[K]})$ .Note that for each $s\\ \\in\\ \\{\\pm1\\},i\\ \\in\\ [n]$ and $p~\\in~[P]~\\backslash~\\{p_{i}^{*}\\}$ \uff0cwe can write $\\xi_{i}^{(p)}$ and $\\xi$ as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\pmb{w}_{s}(0)=\\sigma_{0}\\sum_{l\\in[d-2K]}\\pmb{\\mathbf{z}}_{s,l}\\pmb{u}_{l},\\quad\\xi_{i}^{(p)}=\\sigma_{i,p}\\sum_{l\\in[d-2K]}\\pmb{\\mathbf{z}}_{i,l}^{(p)}\\pmb{u}_{l},\\quad\\xi=\\sigma\\sum_{l\\in[d-2K]}\\pmb{\\mathbf{z}}_{l}\\pmb{u}_{l}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\mathbf{z}_{s,l},\\mathbf{z}_{i,l}^{(p)},\\mathbf{z}_{l}\\overset{i.i.d.}{\\sim}N(0,1)$ The sub-gaussan norm o standard normal distribution $N(0,1)$ .s V. Then $\\left(\\mathbf{z}_{i,l}^{\\left(p\\right)}\\right)^{2}-1$ 's are mean zero sub-exponential with sub-exponential norm $\\frac83$ (Lemma 2.7.6 in Vrshynin (2018) In addition, $\\mathbf{z}_{s,l}\\mathbf{z}_{i,l}^{(p)}$ $\\mathbf{z}_{i,l}^{(p)}\\mathbf{z}_{j,l}^{(q)}$ 's and $\\mathbf{z}_{i,l}^{(p)}\\mathbf{z}_{l}$ 's are mean zero sub-xponential with sub-exponential norm less than or equal to $\\frac{8}{3}$ (Lemma 2.7.7 in Vershynin (2018)). We use Bernstein's inequality (Theorem 2.8.1 in Vershynin (2018), with $c$ being the absolute constant stated therein. We then have the following: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{1-\\mathbb{P}\\left[\\cfrac{1}{2}\\sigma_{i,p}^{2}d\\leq\\left\\|\\xi_{i}^{(p)}\\right\\|^{2}\\leq\\cfrac{3}{2}\\sigma_{i,p}^{2}d\\right]\\leq\\mathbb{P}\\left[\\cfrac{\\left\\|\\xi_{i}^{(p)}\\right\\|^{2}-\\sigma_{i,p}^{2}(d-2K)}{6}\\right]\\geq\\sigma_{i,p}^{2}d^{\\frac{1}{2}}\\log d\\right]}\\\\ {=\\mathbb{P}\\left[\\cfrac{\\sum_{\\substack{i\\in[d-2K]}}\\left(\\left[\\mathbf{z}_{i,l}^{(p)}\\right]^{2}-1\\right)\\bigg|\\geq d^{\\frac{1}{2}}\\log d\\right]}\\\\ {\\leq2\\exp\\left(-\\frac{9c d\\log^{2}d}{64(d-2K)}\\right)}\\\\ {\\leq2\\exp\\left(-\\frac{9c\\log^{2}d}{64}\\right)=o\\left(\\frac{1}{\\mathrm{poly}(d)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "in addition, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left[\\left|\\left\\langle\\xi_{i}^{(p)},\\xi_{j}^{(q)}\\right\\rangle\\right|\\geq\\sigma_{i,p}\\sigma_{j,q}d^{\\frac{1}{2}}\\log d\\right]=\\mathbb{P}\\left[\\left|\\displaystyle\\sum_{l\\in[d-2K]}\\displaystyle\\mathbf{z}_{i,l}^{(p)}\\mathbf{z}_{j,l}^{(q)}\\right|\\geq d^{\\frac{1}{2}}\\log d\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq2\\exp\\left(-\\frac{9c d\\log^{2}d}{64(d-2K)}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq2\\exp\\left(-\\frac{9c\\log^{2}d}{64}\\right)=o\\left(\\frac{1}{\\mathrm{poly}(d)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Similarly, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\left|\\left\\langle w_{s}^{(0)},\\xi_{i}^{(p)}\\right\\rangle\\right|\\geq\\sigma_{0}\\sigma_{i,p}d^{\\frac{1}{2}}\\log d\\right]\\leq2\\exp\\left(-\\frac{9c\\log^{2}d}{64}\\right)=o\\left(\\frac{1}{\\mathrm{poly}(d)}\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Lastly, the last result holds almost surely due to (7). Applying the union bound to all events, each of which is at most $\\mathrm{poly}(d)$ due to (7), leads us to our first conclusion. ", "page_idx": 22}, {"type": "text", "text": "In addition, for each $s\\in\\{\\pm1\\},i\\in[n]$ and $p\\in[P]\\setminus\\{p_{i}^{*}\\}$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\left|\\left\\langle w_{s}^{(0)},\\xi\\right\\rangle\\right|\\geq\\sigma_{0}\\sigma d^{\\frac{1}{2}}\\log d\\right]\\leq2\\exp\\left(-\\frac{9c\\log^{2}d}{64}\\right)=o\\left(\\frac{1}{\\mathrm{poly}(d)}\\right),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\left|\\left\\langle\\xi_{i}^{(p)},\\xi\\right\\rangle\\right|\\geq\\sigma_{i,p}\\sigma d^{\\frac{1}{2}}\\log d\\right]\\leq2\\exp\\left(-\\frac{9c\\log^{2}d}{64}\\right)=o\\left(\\frac{1}{\\mathrm{poly}(d)}\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Applying the union bound to all events, each of which is at most $\\mathrm{poly}(d)$ due to (7), leads us to our second conclusion. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "B.3 Feature Noise Decomposition ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In our analysis, we use a technique that analyzes the coefficients of linear combinations of feature and noise vectors. A similar technique in a different data and network setting is introduced by Cao et al. (2022). ", "page_idx": 22}, {"type": "text", "text": "Lemma B.3. If we run one of ERM, Cutout, and CutMix training to update parameters $\\mathbf{\\boldsymbol{W}}^{(t)}$ ofa model $f_{\\pmb{W}^{(t)}}$ , then there exist coeffcients (corresponding to each method) $\\gamma_{s}^{(t)}(s^{\\prime},k)^{\\,*}$ and $\\rho_{s}^{(t)}(i,p)^{\\,\\prime}s$ so that we can write $W^{(t)}=\\{\\pmb{w}_{1}^{(t)},\\pmb{w}_{-1}^{(t)}\\}$ as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w_{s}^{(t)}=w_{s}^{(0)}+\\displaystyle\\sum_{k\\in[K]}\\gamma_{s}^{(t)}(s,k)v_{s,k}-\\displaystyle\\sum_{k\\in[K]}\\gamma_{s}^{(t)}(-s,k)v_{-s,k}}\\\\ &{\\quad\\quad+\\displaystyle\\sum_{i\\in\\mathcal{V}_{s},p\\in[P]\\backslash\\{p_{i}^{*}\\}}\\rho_{s}^{(t)}(i,p)\\displaystyle\\frac{\\xi_{i}^{(p)}}{\\left\\|\\xi_{i}^{(p)}\\right\\|^{2}}-\\displaystyle\\sum_{i\\in\\mathcal{V}_{-s},p\\in[P]\\backslash\\{p_{i}^{*}\\}}\\rho_{s}^{(t)}(i,p)\\displaystyle\\frac{\\xi_{i}^{(p)}}{\\left\\|\\xi_{i}^{(p)}\\right\\|^{2}}}\\\\ &{\\quad\\quad+\\alpha\\left(\\displaystyle\\sum_{i\\in\\mathcal{F}_{s}}s y_{i}\\rho_{s}^{(t)}(i,\\tilde{p}_{i})\\displaystyle\\frac{v_{s,1}}{\\left\\|\\xi_{i}^{(\\tilde{p}_{i})}\\right\\|^{2}}+\\displaystyle\\sum_{i\\in\\mathcal{F}_{-s}}s y_{i}\\rho_{s}^{(t)}(i,\\tilde{p}_{i})\\displaystyle\\frac{v_{-s,1}}{\\left\\|\\xi_{i}^{(\\tilde{p}_{i})}\\right\\|^{2}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\mathcal{F}_{s}$ denotes the set of indices of data with feature noise $\\pmb{v}_{s,1}$ . Furthermore, if we run one of ERM and Cutout, the coeficients $\\gamma_{s}^{(t)}(s^{\\prime},k)^{\\prime}s$ and $\\rho_{s}^{(t)}(i,p)$ 's are monotone increasing. ", "page_idx": 22}, {"type": "text", "text": "We provide proof of Lemma B.3 for ERM in Appendix C.1, for Cutout in Appendix D.1 and for CutMix in Appendix E.1. ", "page_idx": 22}, {"type": "text", "text": "Since Gaussian vectors in a high-dimensional regime are nearly orthogonal, we can use the coeffcients to approximate the inner products or outputs of neurons. The following lemma quantifies the approximationerror. ", "page_idx": 22}, {"type": "text", "text": "Lemma B.4. Suppose the event $E_{\\mathrm{init}}$ occurs and $0\\leq\\gamma_{s}^{(t)}(s^{\\prime},k),\\rho_{s}^{(t)}(i,p)\\leq\\widetilde{\\mathcal{O}}(\\beta^{-1})$ for all $s,s^{\\prime}\\in$ $\\{\\pm1\\},k\\in[K],i\\in[n]$ and $p\\in[P]\\setminus\\{p_{i}^{*}\\}$ at iteration $t$ Then, for each $s\\in\\{\\pm1\\},k\\in[K],i\\in[n]$ \uff0c and $\\dot{p}\\in[P]\\setminus\\{p_{i}^{*}\\}$ , the following holds: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bullet\\ \\left|\\left\\langle w_{s}^{(t)},v_{s,k}\\right\\rangle-\\gamma_{s}^{(t)}(s,k)\\right|,\\left|\\phi\\left(\\left\\langle w_{s}^{(t)},v_{s,k}\\right\\rangle\\right)-\\gamma_{s}^{(t)}(s,k)\\right|=o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right)}\\\\ &{\\bullet\\ \\left|\\left\\langle w_{s}^{(t)},v_{-s,k}\\right\\rangle+\\gamma_{s}^{(t)}(-s,k)\\right|,\\left|\\phi\\left(\\left\\langle w_{s}^{(t)},v_{-s,k}\\right\\rangle\\right)+\\beta\\gamma_{s}^{(t)}(-s,k)\\right|=o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right.}\\\\ &{\\bullet\\ \\left|\\left\\langle w_{y_{i}}^{(t)},\\xi_{i}^{(p)}\\right\\rangle-\\rho_{y_{i}}^{(t)}(i,p)\\right|,\\left|\\phi\\left(\\left\\langle w_{y_{i}}^{(t)},\\xi_{i}^{(p)}\\right\\rangle\\right)-\\rho_{y_{i}}^{(t)}(i,p)\\right|=o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right)}\\\\ &{\\bullet\\ \\left|\\left\\langle w_{-y_{i}}^{(t)},\\xi_{i}^{(p)}\\right\\rangle+\\rho_{-y_{i}}^{(t)}(i,p)\\right|,\\left|\\phi\\left(\\left\\langle w_{-y_{i}}^{(t)},\\xi_{i}^{(p)}\\right\\rangle\\right)+\\beta\\rho_{-y_{i}}^{(t)}(i,p)\\right|=o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bullet\\ \\left|\\phi\\left(\\left\\langle w_{y_{i}}^{(t)},x_{i}^{(\\tilde{p}_{i})}\\right\\rangle\\right)-\\rho_{y_{i}}^{(t)}(i,\\tilde{p}_{i})\\right|,\\left|\\phi\\left(\\left\\langle w_{-y_{i}}^{(t)},x_{i}^{(\\tilde{p}_{i})}\\right\\rangle\\right)+\\beta\\rho_{-y_{i}}^{(t)}(i,\\tilde{p}_{i})\\right|=o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof of Lemma B.4. For each $s\\in\\{\\pm1\\},k\\in[K]\\setminus\\{1\\}$ , by (A8) and (8), we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left|\\left\\langle w_{s}^{(t)},v_{s,k}\\right\\rangle-\\gamma_{s}^{(t)}(s,k)\\right|=\\left|\\left\\langle w_{s}^{(0)},v_{s,k}\\right\\rangle\\right|=\\widetilde{\\mathcal{O}}(\\sigma_{0})=o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Similarly, by (A8) and (8), ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left|\\left\\langle w_{s}^{(t)},v_{-s,k}\\right\\rangle+\\gamma_{s}^{(t)}(-s,k)\\right|=\\left|\\left\\langle w_{s}^{(0)},v_{-s,k}\\right\\rangle\\right|=\\widetilde{\\mathcal{O}}(\\sigma_{0})=o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Next, we will consider the case of $\\pmb{v}_{1,1}$ and $\\pmb{v}_{-1,1}$ . For each $s\\in\\{\\pm1\\}$ ,wehave ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\vert\\left\\langle w_{s}^{(t)},v_{s,1}\\right\\rangle-\\gamma_{s}^{(t)}(s,1)\\right\\vert}\\\\ &{\\le\\left\\vert\\left\\langle w_{s}^{(0)},v_{s,1}\\right\\rangle\\right\\vert+\\alpha\\displaystyle\\sum_{i\\in[n]}\\rho_{s}^{(t)}(i,\\tilde{p}_{i})\\left\\vert\\left\\vert\\xi_{i}^{(\\tilde{p}_{i})}\\right\\vert\\right\\vert^{-2}}\\\\ &{\\le\\widetilde{\\mathcal{O}}(\\sigma_{0})+\\widetilde{\\mathcal{O}}\\left(\\alpha n\\beta^{-1}\\sigma_{\\mathrm{d}}^{-2}d^{-1}\\right)}\\\\ &{=o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the last equality is due to (8) and (A7). Similarly, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left|\\left\\langle w_{s}^{(t)},v_{-s,1}\\right\\rangle+\\gamma_{s}^{(t)}(-s,1)\\right|}\\\\ &{\\le\\left|\\left\\langle w_{s}^{(0)},v_{-s,1}\\right\\rangle\\right|+\\alpha\\displaystyle\\sum_{i\\in[n]}\\rho_{s}^{(t)}(i,\\tilde{p}_{i})\\left\\|\\xi_{i}^{(\\tilde{p}_{i})}\\right\\|^{-2}}\\\\ &{\\le\\tilde{\\mathcal{O}}(\\sigma_{0})+\\tilde{\\mathcal{O}}\\left(\\alpha n\\beta^{-1}\\sigma_{\\mathrm{d}}^{-2}d^{-1}\\right)}\\\\ &{=o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Hence, from (A8) and the fact that $\\begin{array}{r}{|\\phi(z)-z|\\leq\\frac{(1-\\beta)r}{2}}\\end{array}$ for any $z\\geq0$ we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left|\\phi\\left(\\left\\langle w_{s}^{(t)},v_{s,k}\\right\\rangle\\right)-\\gamma_{s}^{(t)}(s,k)\\right|}\\\\ &{\\le\\left|\\phi\\left(\\left\\langle w_{s}^{(t)},v_{s,k}\\right\\rangle\\right)-\\phi\\left(\\gamma_{s}^{(t)}(s,k)\\right)\\right|+\\left|\\phi\\left(\\gamma_{s}^{(t)}(s,k)\\right)-\\gamma_{s}^{(t)}(s,k)\\right|}\\\\ &{\\le\\left|\\left\\langle w_{s}^{(t)},v_{s,k}\\right\\rangle-\\gamma_{s}^{(t)}(s,k)\\right|+\\displaystyle\\frac{(1-\\beta)r}{2}}\\\\ &{=o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\phi\\left(\\left\\langle w_{s}^{(t)},v_{-s,k}\\right\\rangle\\right)+\\beta\\gamma_{s}^{(t)}(-s,k)\\right|=\\left|\\phi\\left(\\left\\langle w_{s}^{(t)},v_{-s,k}\\right\\rangle\\right)-\\phi\\left(-\\gamma_{s}^{(t)}(-s,k)\\right)\\right|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\left|\\left\\langle w_{s}^{(t)},v_{-s,k}\\right\\rangle+\\gamma_{s}^{(t)}(-s,k)\\right|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\sigma\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For each $i\\in[n]$ , and $p\\in[P]\\setminus\\{p_{i}^{*}\\}$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left\\vert\\left\\langle w_{y_{i}}^{(t)},\\xi_{i}^{(p)}\\right\\rangle-\\rho_{y_{i}}^{(t)}(i,p)\\right\\vert\\leq\\left\\vert\\left\\langle w_{y_{i}}^{(0)},\\xi_{i}^{(p)}\\right\\rangle\\right\\vert+\\sum_{\\substack{j\\in[n],q\\in[P]\\backslash\\left\\{p_{i}^{*}\\right\\}}}\\rho_{y_{i}}^{(t)}(j,q)\\frac{\\left\\vert\\left\\langle\\xi_{i}^{(p)},\\xi_{j}^{(q)}\\right\\rangle\\right\\vert}{\\left\\Vert\\xi_{j}^{(q)}\\right\\Vert^{2}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\le\\widetilde{\\mathcal{O}}\\left(\\sigma_{0}\\sigma_{\\mathrm{d}}d^{\\frac{1}{2}}\\right)+\\widetilde{\\mathcal{O}}\\left(n P\\beta^{-1}\\sigma_{\\mathrm{d}}\\sigma_{\\mathrm{b}}^{-1}d^{-\\frac{1}{2}}\\right)}\\\\ &{=o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the last equality is due to (A8) and (9). By triangular inequality, (A8), and the fact that $\\phi^{\\prime}\\leq1$ and $\\begin{array}{r}{|\\phi(z)-z|\\leq\\frac{(1-\\beta)r}{2}}\\end{array}$ (13)r for any 2 \u2265O, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left|\\phi\\left(\\left\\langle w_{y_{i}}^{(t)},\\xi_{i}^{(p)}\\right\\rangle\\right)-\\rho_{y_{i}}^{(t)}(i,p)\\right|}\\\\ &{\\leq\\left|\\phi\\left(\\left\\langle w_{y_{i}}^{(t)},\\xi_{i}^{(p)}\\right\\rangle\\right)-\\phi\\left(\\rho_{y_{i}}^{(t)}(i,p)\\right)\\right|+\\left|\\phi\\left(\\rho_{y_{i}}^{(t)}(i,p)\\right)-\\rho_{y_{i}}^{(t)}(i,p)\\right|}\\\\ &{\\leq\\left|\\left\\langle w_{y_{i}}^{(t)},\\xi_{i}^{(p)}\\right\\rangle-\\rho_{y_{i}}^{(t)}(i,p)\\right|+\\frac{(1-\\beta)r}{2}}\\\\ &{=o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Also, if $i\\in\\mathcal{F}_{s}$ for some $s\\in\\{\\pm1\\}$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\phi\\left(\\left\\langle w_{y_{i}}^{(t)},x_{i}^{(\\bar{p}_{i})}\\right\\rangle\\right)-\\rho_{y_{i}}^{(t)}(i,\\bar{p}_{i})\\right|}\\\\ &{\\leq\\left|\\phi\\left(\\left\\langle w_{y_{i}}^{(t)},\\xi_{i}^{(\\bar{p}_{i})}\\right\\rangle\\right)-\\rho_{y_{i}}^{(t)}(i,\\bar{p}_{i})\\right|+\\left|\\phi\\left(\\left\\langle w_{y_{i}}^{(t)},x_{i}^{(\\bar{p}_{i})}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{y_{i}}^{(t)},\\xi_{i}^{(\\bar{p}_{i})}\\right\\rangle\\right)\\right|}\\\\ &{\\leq\\left|\\phi\\left(\\left\\langle w_{y_{i}}^{(t)},\\xi_{i}^{(\\bar{p}_{i})}\\right\\rangle\\right)-\\rho_{y_{i}}^{(t)}(i,\\bar{p}_{i})\\right|+\\alpha\\left|\\left\\langle w_{y_{i}}^{(t)},v_{s,1}\\right\\rangle\\right|}\\\\ &{\\leq\\left|\\phi\\left(\\left\\langle w_{y_{i}}^{(t)},\\xi_{i}^{(\\bar{p}_{i})}\\right\\rangle\\right)-\\rho_{y_{i}}^{(t)}(i,\\bar{p}_{i})\\right|+\\alpha\\gamma_{y_{i}}^{(t)}(s,1)+\\alpha\\cdot\\ o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right)}\\\\ &{\\leq\\tilde{\\mathcal{O}}\\left(\\alpha\\beta^{-1}\\right)+o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right)}\\\\ &{=o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where we apply the triangular inequality, the fact that $\\phi^{\\prime}\\leq\\,1$ , the triangular inequality again, $\\rho_{y_{i}}^{(t)}(s,1)=\\bar{\\tilde{O}}(\\vec{\\beta}^{-1})$ and (10) equentially. ", "page_idx": 24}, {"type": "text", "text": "Similarly, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\left\\langle w_{-y_{i}}^{(t)},\\xi_{i}^{(p)}\\right\\rangle+\\rho_{-y_{i}}^{(t)}(i,p)\\right|\\leq\\left|\\left\\langle w_{-y_{i}}^{(0)},\\xi_{i}^{(p)}\\right\\rangle\\right|+\\displaystyle\\sum_{\\substack{j\\in[n],\\,q\\in[P]\\backslash\\{p_{i}^{*}\\}}}\\rho_{-y_{i}}^{(t)}(j,q)\\frac{\\left|\\left\\langle\\xi_{i}^{(p)},\\xi_{j}^{(q)}\\right\\rangle\\right|}{\\left\\|\\xi_{j}^{(q)}\\right\\|^{2}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\widetilde{\\mathcal{O}}(\\sigma_{0}\\sigma_{\\mathrm{d}}d^{\\frac{1}{2}})+\\widetilde{\\mathcal{O}}\\left(n P\\beta^{-1}\\sigma_{\\mathrm{d}}\\sigma_{\\mathrm{b}}^{-1}d^{-\\frac{1}{2}}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\vert\\phi\\left(\\left\\langle w_{-y_{i}}^{(t)},\\xi_{i}^{(p)}\\right\\rangle\\right)+\\beta\\rho_{-y_{i}}^{(t)}(i,p)\\right\\vert=\\left\\vert\\phi\\left(\\left\\langle w_{-y_{i}}^{(t)},\\xi_{i}^{(p)}\\right\\rangle\\right)-\\phi\\left(-\\rho_{-y_{i}}^{(t)}(i,p)\\right)\\right\\vert}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\left\\vert\\left\\langle w_{-y_{i}}^{(t)},\\xi_{i}^{(p)}\\right\\rangle+\\rho_{-y_{i}}^{(t)}(i,p)\\right\\vert}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Also, if $i\\in\\mathcal{F}_{s}$ for some $s\\in\\{\\pm1\\}$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left|\\phi\\left(\\left\\langle w_{-y_{i}}^{(t)},x_{i}^{(\\tilde{p}_{i})}\\right\\rangle\\right)+\\beta\\rho_{-y_{i}}^{(t)}(i,\\tilde{p}_{i})\\right|}\\\\ &{=\\left|\\phi\\left(\\left\\langle w_{-y_{i}}^{(t)},\\xi_{i}^{(\\tilde{p}_{i})}\\right\\rangle\\right)+\\beta\\rho_{-y_{i}}^{(t)}(i,\\tilde{p}_{i})\\right|+\\left|\\phi\\left(\\left\\langle w_{-y_{i}}^{(t)},x_{i}^{(\\tilde{p}_{i})}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-y_{i}}^{(t)},\\xi_{i}^{(\\tilde{p}_{i})}\\right\\rangle\\right)\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\left|\\phi\\left(\\left\\langle w_{-y_{i}}^{(t)},\\xi_{i}^{(\\bar{p}_{i})}\\right\\rangle\\right)+\\beta\\rho_{-y_{i}}^{(t)}(i,\\tilde{p}_{i})\\right|+\\alpha\\left|\\left\\langle w_{-y_{i}}^{(t)},v_{s,1}\\right\\rangle\\right|}\\\\ &{\\leq\\left|\\phi\\left(\\left\\langle w_{-y_{i}}^{(t)},\\xi_{i}^{(\\bar{p}_{i})}\\right\\rangle\\right)+\\beta\\rho_{-y_{i}}^{(t)}(i,\\tilde{p}_{i})\\right|+\\alpha\\gamma_{-y_{i}}^{(t)}(s,1)+\\alpha\\cdot o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right)}\\\\ &{\\leq\\widetilde{\\mathcal{O}}\\left(\\alpha\\beta^{-1}\\right)+o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right)}\\\\ &{=o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We define the set $\\mathcal{W}$ as the collection of ${\\cal W}\\,=\\,\\{{\\pmb w}_{1},{\\pmb w}_{-1}\\}$ ,Where ${\\pmb w}_{1}-{\\pmb w}_{1}^{(0)},{\\pmb w}_{-1}-{\\pmb w}_{-1}^{(0)}$ a are elements of the subspace spanned by $\\{v_{s,k}\\}_{s\\in\\{\\pm1\\},k\\in[K]}\\cup\\left\\{x_{i}^{(p)}\\right\\}_{i\\in[n],p\\in[P]\\setminus\\{p_{i}^{*}\\}}$ .The following lemma guarantees the unique expression of any $W\\,\\in\\,{\\mathcal W}$ in the form of the feature noise decomposition. ", "page_idx": 25}, {"type": "text", "text": "Lemma B.5. Suppose the event $E_{\\mathrm{init}}$ occurs. For each element $\\pmb{W}=\\{\\pmb{w}_{1},\\pmb{w}_{-1}\\}\\in\\mathcal{W}$ , there exist unique coefficients $\\gamma_{s}(s^{\\prime},k)$ 's and $\\rho_{s}(i,p)$ 's such that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w_{s}=w_{s}^{(0)}+\\displaystyle\\sum_{k\\in[K]}\\gamma_{s}(s,k)v_{s,k}-\\displaystyle\\sum_{k\\in[K]}\\gamma_{s}(-s,k)v_{-s,k}}\\\\ &{\\quad\\,+\\displaystyle\\sum_{j\\in[P]\\backslash\\{p_{i}^{*}\\}}\\rho_{s}(i,p)\\frac{\\xi_{i}^{(p)}}{\\left\\|\\xi_{i}^{(p)}\\right\\|^{2}}-\\displaystyle\\sum_{\\substack{p\\in[P]\\backslash\\{p_{i}^{*}\\}}}\\rho_{s}(i,p)\\frac{\\xi_{i}^{(p)}}{\\left\\|\\xi_{i}^{(p)}\\right\\|^{2}}}\\\\ &{\\quad\\,+\\displaystyle\\alpha\\left(\\sum_{i\\in\\mathcal{F}_{s}}s y_{i}\\rho_{s}(i,\\tilde{p}_{i})\\frac{v_{s,1}}{\\left\\|\\xi_{i}^{(\\tilde{p}_{i})}\\right\\|^{2}}+\\displaystyle\\sum_{i\\in\\mathcal{F}_{s-s}}s y_{i}\\rho_{s}(i,\\tilde{p}_{i})\\frac{v_{-s,1}}{\\left\\|\\xi_{i}^{(\\tilde{p}_{i})}\\right\\|^{2}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "for each $s\\in\\{\\pm1\\}$ . Using this fact, for each $s^{*}\\in\\{\\pm1\\}$ and $k^{*}\\in[K]$ we can introduce a function $Q^{(s^{*},k^{*})}:\\mathcal{W}\\to\\mathbb{R}^{d\\times2}$ such that for each $\\pmb{W}=\\{\\pmb{w}_{1},\\pmb{w}_{-1}\\}\\in\\mathcal{W}$ ", "page_idx": 25}, {"type": "equation", "text": "$$\nQ^{(s^{*},k^{*})}(W)=\\left\\{Q_{1}^{(s^{*},k^{*})}({\\pmb w}_{1}),Q_{-1}^{(s^{*},k^{*})}({\\pmb w}_{-1})\\right\\}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "is given by: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{Q_{s}^{(s^{*},k^{*})}(w_{s})=s s^{*}\\gamma_{s}(s^{*},k^{*})v_{s^{*},k^{*}}+s s^{*}\\displaystyle\\sum_{i\\in\\mathcal{V}_{s^{*},k^{*}},p\\in[P]\\backslash\\{p_{i}^{*}\\}}\\rho_{s}(i,p)\\displaystyle\\frac{\\xi_{i}^{(p)}}{\\left\\|\\xi_{i}^{(p)}\\right\\|^{2}}}}\\\\ {{+\\alpha\\left(\\displaystyle\\sum_{i\\in\\mathcal{F}_{s}\\cap\\mathcal{V}_{s^{*},k^{*}}}s s^{*}\\rho_{s}(i,\\tilde{p}_{i})\\displaystyle\\frac{v_{s,1}}{\\left\\|\\xi_{i}^{(\\tilde{p}_{i})}\\right\\|^{2}}+\\displaystyle\\sum_{i\\in\\mathcal{F}_{-s}\\cap\\mathcal{V}_{s^{*},k^{*}}}s s^{*}\\rho_{s}(i,\\tilde{p}_{i})\\displaystyle\\frac{v_{-s,1}}{\\left\\|\\xi_{i}^{(\\tilde{p}_{i})}\\right\\|^{2}}\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The function $Q^{(s^{*},k^{*})}$ plays a crucial role in Section C.2.4 and Section D.2.4. The key intuition behind our defnition of $\\bar{Q}^{(s^{*},k^{*})}$ is that $Q^{(s^{*},k^{*})}(W^{(t)})$ represents the term updated by the data having the feature vector $\\pmb{v}_{s^{*},k^{*}}$ , where $\\mathbf{\\boldsymbol{W}}^{(t)}$ are the iterates of either ERM or Cutout. As expected from this iton f e sll $Q_{1}^{(s^{*},k^{*})}(w_{1})$ and $Q_{-1}^{(s^{*},k^{*})}({\\pmb w}_{-1})$ over ll $s^{*}\\in\\{\\pm1\\}$ and $k^{*}\\in[K]$ the result will be equal to ${\\pmb w}_{1}-{\\pmb w}_{1}^{(0)}$ and ${\\pmb w}_{-1}-{\\pmb w}_{-1}^{(0)}$ , respectively. ", "page_idx": 25}, {"type": "text", "text": "Proof Fromlinear independency of $\\{v_{s,k}\\}_{s\\in\\{\\pm1\\},k\\in[K]}\\cup\\left\\{\\pmb{x}_{i}^{(p)}\\right\\}_{i\\in[n],p\\in[P]\\backslash\\{p_{i}^{*}\\}};$ .we can express any element $\\pmb{W}=\\{\\pmb{w}_{1},\\pmb{w}_{-1}\\}\\in\\mathcal{W}$ as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\pmb{w}_{s}=\\pmb{w}_{s}^{(0)}+\\sum_{k\\in[K]}\\tilde{\\gamma}_{s}(s,k)\\pmb{v}_{s,k}-\\sum_{k\\in[K]}\\tilde{\\gamma}_{s}(-s,k)\\pmb{v}_{-s,k}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n+\\sum_{i\\in\\mathcal{V}_{s},\\atop p\\in[P]\\setminus\\{p_{i}^{*}\\}}\\rho_{s}(i,p)\\frac{\\xi_{i}^{(p)}}{\\Big\\|\\xi_{i}^{(p)}\\Big\\|}-\\sum_{i\\in\\mathcal{V}-s,\\atop p\\in[P]\\setminus\\{p_{i}^{*}\\}}\\rho_{s}(i,p)\\frac{\\xi_{i}^{(p)}}{\\Big\\|\\xi_{i}^{(p)}\\Big\\|}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "with unique $\\{\\tilde{\\gamma}_{s}(s,k),\\tilde{\\gamma}_{s}(-s,k)\\}_{s\\in\\{\\pm1\\},k\\in[K]}$ and $\\{\\rho_{s}(i,p)\\}_{s\\in\\{\\pm1\\},i\\in[n],p\\in[P]\\backslash\\{i^{*}\\}}$ .If we define $\\gamma_{s}(s,k)$ and $\\gamma_{s}(-s,k)$ as $\\gamma_{s}(s,k)=\\widetilde{\\gamma}_{s}(s,k),\\gamma_{s}(-s,k)=\\widetilde{\\gamma}_{s}(-s,k)$ for $k\\neq1$ , and ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\gamma_{s}(s,1)=\\tilde{\\gamma}_{s}(s,1)-\\alpha\\sum_{i\\in\\mathcal{F}_{s}}s y_{i}\\rho_{s}(i,\\tilde{p}_{i})\\left\\Vert\\xi_{i}^{(\\tilde{p}_{i})}\\right\\Vert^{-2},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\gamma_{s}(-s,1)=\\widetilde{\\gamma}_{s}(-s,1)+\\alpha\\sum_{i\\in\\mathcal{F}_{-s}}s y_{i}\\rho_{s}(i,\\widetilde{p}_{i})\\left\\Vert\\xi_{i}^{(\\widetilde{p}_{i})}\\right\\Vert^{-2},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "then we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w_{s}=w_{s}^{(0)}+\\displaystyle\\sum_{k\\in[K]}\\gamma_{s}(s,k)v_{s,k}-\\displaystyle\\sum_{k\\in[K]}\\gamma_{s}(-s,k)v_{-s,k}}\\\\ &{\\quad\\,+\\displaystyle\\sum_{\\substack{p\\in[P]\\backslash\\left\\{p_{i}^{*}\\right\\}}}\\rho_{s}(i,p)\\frac{\\xi_{i}^{(p)}}{\\left\\|\\xi_{i}^{(p)}\\right\\|^{2}}-\\displaystyle\\sum_{\\substack{p\\in[P]\\backslash\\left\\{p_{i}^{*}\\right\\}}}\\rho_{s}(i,p)\\frac{\\xi_{i}^{(p)}}{\\left\\|\\xi_{i}^{(p)}\\right\\|^{2}}}\\\\ &{\\quad\\,+\\alpha\\left(\\displaystyle\\sum_{i\\in\\mathcal{F}_{s}}s y_{i}\\rho_{s}(i,\\tilde{p}_{i})\\frac{v_{s,1}}{\\left\\|\\xi_{i}^{(\\tilde{p}_{i})}\\right\\|^{2}}+\\displaystyle\\sum_{i\\in\\mathcal{F}_{-s}}s y_{i}\\rho_{s}(i,\\tilde{p}_{i})\\frac{v_{-s,1}}{\\left\\|\\xi_{i}^{(\\tilde{p}_{i})}\\right\\|^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Next, we want to show the uniqueness part. Suppose $\\{\\hat{\\gamma}_{s}(s,k),\\hat{\\gamma}_{s}(-s,k)\\}_{s\\in\\{\\pm1\\},k\\in[K]}$ and $\\{\\hat{\\rho}_{s}(i,p)\\}_{s\\in\\{\\pm1\\},i\\in[n],p\\in[P]\\backslash\\{i^{*}\\}}$ satisfies ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w_{s}=w_{s}^{(0)}+\\displaystyle\\sum_{k\\in[K]}\\hat{\\gamma}_{s}(s,k)v_{s,k}-\\displaystyle\\sum_{k\\in[K]}\\hat{\\gamma}_{s}(-s,k)v_{-s,k}}\\\\ &{\\quad\\quad+\\displaystyle\\sum_{i\\in[P]\\backslash\\{\\hat{\\ell}_{i}^{(p)}\\}}\\hat{\\rho}_{s}(i,p)\\frac{\\xi_{i}^{(p)}}{\\left\\|\\xi_{i}^{(p)}\\right\\|^{2}}-\\displaystyle\\sum_{\\stackrel{i\\in[P]\\backslash\\{\\hat{\\ell}_{i}^{(p)}\\}}{p\\in[P]\\backslash\\{p_{i}^{*}\\}}}\\hat{\\rho}_{s}(i,p)\\frac{\\xi_{i}^{(p)}}{\\left\\|\\xi_{i}^{(p)}\\right\\|^{2}}}\\\\ &{\\quad\\quad+\\alpha\\left(\\displaystyle\\sum_{i\\in\\mathcal{F}_{s}}s y_{i}\\hat{\\rho}_{s}(i,\\tilde{p}_{i})\\frac{v_{s,1}}{\\left\\|\\xi_{i}^{(\\tilde{p}_{i})}\\right\\|^{2}}+\\displaystyle\\sum_{i\\in\\mathcal{F}_{-s}}s y_{i}\\hat{\\rho}_{s}(i,\\tilde{p}_{i})\\frac{v_{-s,1}}{\\left\\|\\xi_{i}^{(\\tilde{p}_{i})}\\right\\|^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w_{s}=w_{s}^{(0)}+\\displaystyle\\sum_{k\\in[K]\\backslash\\{1\\}}\\hat{\\gamma}_{s}(s,k)v_{s,k}-\\displaystyle\\sum_{k\\in[K]\\backslash\\{1\\}}\\hat{\\gamma}_{s}(-s,k)v_{-s,k}}\\\\ &{\\qquad+\\left(\\hat{\\gamma}_{s}(s,1)+\\alpha\\displaystyle\\sum_{i\\in\\mathcal{F}_{s}}s y_{i}\\hat{\\rho}_{s}(i,\\hat{p}_{i})\\left\\|\\xi_{i}^{(\\hat{\\pi}_{i})}\\right\\|^{-2}\\right)v_{s,1}}\\\\ &{\\qquad-\\left(\\hat{\\gamma}_{s}(-s,1)-\\alpha\\displaystyle\\sum_{i\\in\\mathcal{F}_{s-s}}s y_{i}\\hat{\\rho}_{s}(i,\\hat{p}_{i})\\left\\|\\xi_{i}^{(\\hat{\\pi}_{i})}\\right\\|^{-2}\\right)v_{-s,1}}\\\\ &{\\qquad+\\displaystyle\\sum_{p\\in\\mathcal{F}_{s}}\\hat{\\rho}_{s}(i,p)\\displaystyle\\frac{\\xi_{i}^{(p)}}{\\left\\|\\xi_{i}^{(p)}\\right\\|^{2}}-\\displaystyle\\sum_{i\\in\\mathcal{V}_{s-s}}\\hat{\\rho}_{s}(i,p)\\displaystyle\\frac{\\xi_{i}^{(p)}}{\\left\\|\\xi_{i}^{(p)}\\right\\|^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "From the uniqueness of (13), we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\hat{\\gamma}_{s}(s,k)=\\tilde{\\gamma}_{s}(s,k)=\\gamma_{s}(s,k),\\quad\\hat{\\gamma}_{s}(-s,k)=\\tilde{\\gamma}_{s}(-s,k)=\\gamma_{s}(-s,k),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "for each $s\\;\\in\\;\\{\\pm1\\},k\\;\\in\\;[K]\\;\\backslash\\;\\{1\\}$ , and $\\hat{\\rho}_{s}(i,p)\\,=\\,\\rho_{s}(i,p)$ for each $i\\;\\in\\;[n],p\\;\\in\\;[P]\\;\\backslash\\;\\{p_{i}^{*}\\}$ Furthermore, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\hat{\\gamma}_{s}(s,1)+\\alpha\\sum_{i\\in\\mathcal{F}_{s}}s y_{i}\\hat{\\rho}_{s}(i,\\tilde{p}_{i})\\left\\lVert\\xi_{i}^{(\\tilde{p}_{i})}\\right\\rVert^{-2}=\\tilde{\\gamma}_{s}(s,1)=\\gamma_{s}(s,1)+\\alpha\\sum_{i\\in\\mathcal{F}_{s}}s y_{i}\\rho_{s}(i,\\tilde{p}_{i})\\left\\lVert\\xi_{i}^{(\\tilde{p}_{i})}\\right\\rVert^{-2},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\prime_{s}(-s,1)-\\alpha\\sum_{i\\in\\mathcal{F}_{-s}}s y_{i}\\hat{\\rho}_{s}(i,\\tilde{p}_{i})\\left\\|\\xi_{i}^{(\\bar{p}_{i})}\\right\\|^{-2}=\\tilde{\\gamma}_{s}(-s,1)=\\gamma_{s}(-s,1)-\\alpha\\sum_{i\\in\\mathcal{F}_{-s}}s y_{i}\\rho_{s}(i,\\tilde{p}_{i})\\left\\|\\xi_{i}^{(\\bar{p}_{i})}\\right\\|^{-2}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Hence, we obtain the uniqueness of the expression and $Q^{(s^{*},k^{*})}$ is well defined for each $s^{*}\\in\\{\\pm1\\}$ and $k^{*}\\in[K]$ \u53e3 ", "page_idx": 27}, {"type": "text", "text": "C Proof for ERM ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In this section, we use $\\begin{array}{r}{g_{i}^{(t)}:=\\frac{1}{1+\\exp\\left(y_{i}f_{W^{(t)}}(\\pmb{X}_{i})\\right)}}\\end{array}$ for each data $i$ and iteration $t$ , for simplicity. ", "page_idx": 28}, {"type": "text", "text": "C.1 Proof of Lemma B.3 for ERM ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "For $s\\in\\{\\pm1\\}$ and iterate $t$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad w_{s}^{(t+1)}-w_{s}^{(t)}}\\\\ &{=-\\eta\\nabla_{w_{s}}\\mathcal{L}_{\\mathrm{ERM}}\\left(W^{(t)}\\right)}\\\\ &{=\\frac{\\eta}{n}\\displaystyle\\sum_{i\\in[n]}s y_{i}g_{i}^{(t)}\\sum_{p\\in[P]}\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)x_{i}^{(p)}}\\\\ &{=\\frac{\\eta}{n}\\left(\\displaystyle\\sum_{i\\in\\mathcal{V}_{s}}g_{i}^{(t)}\\sum_{p\\in[P]}\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)x_{i}^{(p)}-\\displaystyle\\sum_{i\\in\\mathcal{V}_{-s}}g_{i}^{(t)}\\sum_{p\\in[P]}\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)x_{i}^{(p)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i\\in V_{s}}g_{i}^{(t)}\\sum_{p\\in[P]}\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)x_{i}^{(p)}}\\\\ &{=\\displaystyle\\sum_{k\\in[K]}\\sum_{i\\in V_{s},k}g_{i}^{(t)}\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(t)},v_{s,k}\\right\\rangle\\right)v_{s,k}+\\displaystyle\\sum_{i\\in V_{s}}g_{i}^{(t)}\\sum_{p\\in[P]\\backslash\\{p_{i}^{*},\\tilde{p}_{i}\\}}\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(t)},\\xi_{i}^{(p)}\\right\\rangle\\right)\\xi_{i}^{(p)}}\\\\ &{\\quad+\\displaystyle\\sum_{i\\in V_{s}\\cap\\mathcal{F}_{s}}g_{i}^{(t)}\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(t)},\\alpha v_{s,1}+\\xi_{i}^{(\\tilde{p}_{i})}\\right\\rangle\\right)\\left(\\alpha v_{s,1}+\\xi_{i}^{(\\tilde{p}_{i})}\\right)}\\\\ &{\\quad+\\displaystyle\\sum_{i\\in V_{s}\\cap\\mathcal{F}_{s-s}}g_{i}^{(t)}\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(t)},\\alpha v_{-s,1}+\\xi_{i}^{(\\tilde{p}_{i})}\\right\\rangle\\right)\\left(\\alpha v_{-s,1}+\\xi_{i}^{(\\tilde{p}_{i})}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i\\in V-s}g_{i}^{(t)}\\sum_{p\\in[P]}\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(t)},\\mathbf{\\boldsymbol{x}}_{i}^{(p)}\\right\\rangle\\right)\\boldsymbol{x}_{i}^{(p)}}\\\\ &{=\\displaystyle\\sum_{k\\in[K]}\\sum_{i\\in V-s,k}g_{i}^{(t)}\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(t)},\\boldsymbol{v}_{-s,k}\\right\\rangle\\right)\\boldsymbol{v}_{-s,k}+\\displaystyle\\sum_{i\\in V-s}g_{i}^{(t)}\\sum_{p\\in[P]\\backslash\\{p_{i}^{*},\\bar{p}_{i}\\}}\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(t)},\\xi_{i}^{(p)}\\right\\rangle\\right)\\xi_{i}^{(p)}}\\\\ &{\\displaystyle\\quad+\\sum_{i\\in V-s\\cap\\mathcal{F}_{s}}g_{i}^{(t)}\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(t)},\\alpha\\boldsymbol{v}_{s,1}+\\xi_{i}^{(\\bar{p}_{i})}\\right\\rangle\\right)\\left(\\alpha\\boldsymbol{v}_{s,1}+\\xi_{i}^{(\\bar{p}_{i})}\\right)}\\\\ &{\\displaystyle\\ +\\sum_{i\\in V-s\\cap\\mathcal{F}_{s}}g_{i}^{(t)}\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(t)},\\alpha\\boldsymbol{v}_{-s,1}+\\xi_{i}^{(\\bar{p}_{i})}\\right\\rangle\\right)\\left(\\alpha\\boldsymbol{v}_{-s,1}+\\xi_{i}^{(\\bar{p}_{i})}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Hence, if we define $\\gamma_{s}^{(t)}(s^{\\prime},k)$ 's and $\\rho_{s}^{(t)}(i,p)$ 's recursively by using the rule ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma_{s}^{(t+1)}(s^{\\prime},k)=\\gamma_{s}^{(t)}(s^{\\prime},k)+\\frac{\\eta}{n}\\displaystyle\\sum_{i\\in\\mathcal{V}_{s^{\\prime},k}}g_{i}^{(t)}\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(t)},v_{s^{\\prime},k}\\right\\rangle\\right),}\\\\ &{\\rho_{s}^{(t+1)}(i,p)=\\rho_{s}^{(t)}(i,p)+\\frac{\\eta}{n}g_{i}^{(t)}\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)\\left\\|\\xi_{i}^{(p)}\\right\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "starting from $\\gamma_{s}^{(0)}(s^{\\prime},k)=\\rho_{s}^{(0)}(i,p)=0$ for each $s,s^{\\prime}\\in\\{\\pm1\\},k\\in[K],i\\in[n]$ and $p\\in[P]\\setminus\\{p_{i}^{*}\\}$ then we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle w_{s}^{(t)}=w_{s}^{(0)}+\\sum_{k\\in[K]}\\gamma_{s}^{(t)}(s,k)v_{s,k}-\\sum_{k\\in[K]}\\gamma_{s}^{(t)}(-s,k)v_{-s,k}}}\\\\ {{\\displaystyle\\qquad+\\sum_{i\\in\\mathcal{V}_{s},p\\in[P]\\setminus\\{p_{i}^{*}\\}}\\rho_{s}^{(t)}(i,p)\\frac{\\xi_{i}^{(p)}}{\\left\\|\\xi_{i}^{(p)}\\right\\|^{2}}-\\sum_{i\\in\\mathcal{V}_{-s},p\\in[P]\\setminus\\{p_{i}^{*}\\}}\\rho_{s}^{(t)}(i,p)\\frac{\\xi_{i}^{(p)}}{\\left\\|\\xi_{i}^{(p)}\\right\\|^{2}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$$\n+\\,\\alpha\\left(\\sum_{i\\in\\mathcal{F}_{s}}s y_{i}\\rho_{s}^{(t)}(i,\\tilde{p}_{i})\\frac{v_{s,1}}{\\left\\|\\xi_{i}^{(\\tilde{p}_{i})}\\right\\|^{2}}+\\sum_{i\\in\\mathcal{F}_{-s}}s y_{i}\\rho_{s}^{(t)}(i,\\tilde{p}_{i})\\frac{v_{-s,1}}{\\left\\|\\xi_{i}^{(\\tilde{p}_{i})}\\right\\|^{2}}\\right),\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "for each $s\\in\\{\\pm1\\}$ . Furthermore, $\\gamma_{s}^{(t)}(s^{\\prime},k)$ 's and $\\rho_{s}^{(t)}(i,p)$ 's are monotone increasing. ", "page_idx": 29}, {"type": "text", "text": "C.2 Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "To show Theorem 3.1, we present a structured proof comprising the following five steps: ", "page_idx": 29}, {"type": "text", "text": "1. Establish upper bounds on $\\gamma_{s}^{(t)}(s^{\\prime},k)$ 's and $\\rho_{s}^{(t)}(i,p)$ 's to apply Lemma B.4 (Section C.2.1).   \n2. Demonstrate that the model learns common features quickly (Section C.2.2).   \n3. Show that the model overfits dominant noise in (extremely) rare data instead of learning its feature   \n(Section C.2.3).   \n4. Confirm the persistence of this tendency until $T^{*}$ iterates (Section C.2.4).   \n5. Characterize train accuracy and test accuracy (Section C.2.5). ", "page_idx": 29}, {"type": "text", "text": "C.2.1 Bounds on the Coefficients in Feature Noise Decomposition ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "The following lemma provides upper bounds on Lemma B.3 during $T^{*}$ iterations. Lemma C.1. Suppose the event $E_{\\mathrm{init}}$ occurs.Forany $t\\in[0,T^{*}].$ wehave ", "page_idx": 29}, {"type": "equation", "text": "$$\n0\\leq\\gamma_{s}^{(t)}(s,k)+\\beta\\gamma_{-s}^{(t)}(s,k)\\leq4\\log(\\eta T^{\\ast}),\\quad0\\leq\\rho_{y_{i}}^{(t)}(i,p)+\\beta\\rho_{-y_{i}}^{(t)}(i,p)\\leq4\\log\\left(\\eta T^{\\ast}\\right),\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "for all $s\\in\\{\\pm1\\}$ \uff0c $k\\in[K],i\\in[n]$ and $p\\in[P]\\setminus\\{p_{i}^{*}\\}$ Consequently $\\gamma_{s}^{(t)}(s^{\\prime},k),\\rho_{s}^{(t)}(i,p)=\\widetilde{\\mathcal{O}}(\\beta^{-1})$ for all $s,s^{\\prime}\\in\\{\\pm1\\},k\\in[K],i\\in[n]$ and $p\\in[P]\\setminus\\{p_{i}^{*}\\}$ ", "page_idx": 29}, {"type": "text", "text": "Proof of Lemma C.1. The first argument implies the second argument since $\\log(\\eta T^{*})=\\mathrm{polylog}(d)$ and ", "page_idx": 29}, {"type": "text", "text": "$\\begin{array}{r}{\\gamma_{s}^{(t)}(s^{\\prime},k)\\le\\beta^{-1}\\left(\\gamma_{s^{\\prime}}^{(t)}(s^{\\prime},k)+\\beta\\gamma_{s^{\\prime}}^{(t)}(s^{\\prime},k)\\right),\\quad\\rho_{s}^{(t)}(i,p)\\le\\beta^{-1}\\left(\\rho_{y_{i}}^{(t)}(i,p)+\\beta\\rho_{-y_{i}}^{(t)}(i,p)\\right),}\\end{array}$ for all $s,s^{\\prime}\\in\\{\\pm1\\},k\\in[K],i\\in[n]$ and $p\\in[P]\\setminus\\{p_{i}^{*}\\}$ ", "page_idx": 29}, {"type": "text", "text": "We will prove this by using induction on $t$ . The initial case $t\\,=\\,0$ is trivial. Suppose the given statement holds at $t=T$ and consider the case $t=T+1$ ", "page_idx": 29}, {"type": "text", "text": "Let $\\tilde{T}_{s,k}\\leq T$ denote the smallest iteration where $\\gamma_{s}^{(\\tilde{T}_{s,k}+1)}(s,k)+\\beta\\gamma_{-s}^{(\\tilde{T}_{s,k}+1)}(s,k)>2\\log(\\eta T^{*})$ We assume the existence of $\\tilde{T}_{s,k}$ , as its absence would directly lead to our desired conclusion; to see why, note that the following holds, due to (14) and (11): ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\gamma_{s}^{(T+1)}(s,k)+\\beta\\gamma_{-s}^{(T+1)}(s,k)}\\\\ &{=\\gamma_{s}^{(T)}(s,k)+\\beta\\gamma_{-s}^{(T)}(s,k)+\\frac{\\eta}{n}\\sum_{i\\in\\mathcal{V}_{s,k}}g_{i}^{(T)}\\bigg(\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(T)},v_{s,k}\\right\\rangle\\right)+\\beta\\phi^{\\prime}\\left(\\left\\langle w_{-s}^{(T)},v_{s,k}\\right\\rangle\\right)\\bigg)}\\\\ &{\\leq2\\log(\\eta T^{*})+2\\eta\\leq4\\log(\\eta T^{*})}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Now suppose there exists such $\\tilde{T}_{s,k}\\leq T$ . By (14), we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma_{s}^{(T+1)}(s,k)+\\beta\\gamma_{-s}^{(T+1)}(s,k)}\\\\ &{=\\gamma_{s}^{(\\widehat{T}_{s,k})}(s,k)+\\beta\\gamma_{-s}^{(\\widehat{T}_{s,k})}(s,k)}\\\\ &{\\quad+\\displaystyle\\sum_{t=\\widehat{T}_{s,k}}^{T}\\Big(\\gamma_{s}^{(t+1)}(s,k)+\\beta\\gamma_{-s}^{(t+1)}(s,k)-\\gamma_{s}^{(t)}(s,k)-\\beta\\gamma_{-s}^{(t)}(s,k)\\Big)}\\\\ &{\\leq2\\log(\\eta T^{*})+\\log(\\eta T^{*})+\\displaystyle\\frac{\\eta}{n}\\sum_{t=\\widehat{T}_{s,k}+1}^{T}\\sum_{i\\in\\mathcal{V}_{s,k}}g_{i}^{(t)}\\bigg(\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(t)},v_{s,k}\\right\\rangle\\right)+\\beta\\phi^{\\prime}\\left(\\left\\langle w_{-s}^{(t)},v_{s,k}\\right\\rangle\\right)\\bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "The inequalit is due o $\\gamma_{s}^{(\\tilde{T}_{s,k})}(s,k)+\\beta\\gamma_{-s}^{(\\tilde{T}_{s,k})}(s,k)\\le2\\log(\\eta T^{*})$ fromour choice of $\\tilde{T}_{s,k}$ and ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{\\eta}{n}\\sum_{i\\in\\mathcal{V}_{s,k}}g_{i}^{(\\tilde{T}_{s,k})}\\bigg(\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(\\tilde{T}_{s,k})},v_{s,k}\\right\\rangle\\right)+\\beta\\phi^{\\prime}\\left(\\left\\langle w_{-s}^{(\\tilde{T}_{s,k})},v_{s,k}\\right\\rangle\\right)\\bigg)\\leq2\\eta\\leq\\log(\\eta T^{*}),\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "from (11). ", "page_idx": 30}, {"type": "text", "text": "For each $t=\\tilde{T}_{s,k}+1,\\ldots T$ , and $i\\in\\mathcal{V}_{s,k}$ , we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phantom{\\geq}y_{i}f_{W^{(t)}}(X_{i})}\\\\ &{=\\phi\\left(\\left\\langle w_{s}^{(t)},v_{s,k}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-s}^{(t)},v_{s,k}\\right\\rangle\\right)+\\displaystyle\\sum_{p\\in[P]\\backslash\\{p_{i}^{*}\\}}\\left(\\phi\\left(\\left\\langle w_{s}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-s}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)\\right)}\\\\ &{\\geq\\gamma_{s}^{(t)}(s,k)+\\beta\\gamma_{-s}^{(t)}(s,k)+\\displaystyle\\sum_{p\\in[P]\\backslash\\{p_{i}^{*}\\}}\\left(\\rho_{s}^{(t)}(i,p)+\\beta\\rho_{-s}^{(t)}(i,p)\\right)-2P\\cdot o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right)}\\\\ &{\\geq\\displaystyle\\frac{3}{2}\\log(\\eta T^{*})}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "The first inequality is due to Lemma B.4 and the second inequality holds due to (A7), (8), and our choice of $t$ $\\bar{\\gamma_{s}^{(t)}}(s,k)+\\beta\\gamma_{-s}^{(t)}(s,k)\\geq2\\log(\\eta T^{*})$ ", "page_idx": 30}, {"type": "text", "text": "Hence, we obtain ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\eta}{n}\\displaystyle\\sum_{t=\\bar{T}_{s,k}}^{T}\\sum_{i\\in\\mathcal{V}_{s,k}}g_{i}^{(t)}\\bigg(\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(t)},v_{s,k}\\right\\rangle\\right)+\\beta\\phi^{\\prime}\\left(\\left\\langle w_{-s}^{(t)},v_{s,k}\\right\\rangle\\right)\\bigg)}\\\\ &{\\leq\\displaystyle\\frac{2\\eta}{n}\\displaystyle\\sum_{t=\\bar{T}_{s,k}}^{T}\\sum_{i\\in\\mathcal{V}_{s,k}}\\exp\\left(-y_{i}f_{W^{(t)}}(X_{i})\\right)}\\\\ &{\\leq\\displaystyle\\frac{2|\\mathcal{V}_{s,k}|}{n}\\big(\\eta T^{*}\\big)\\exp\\left(-\\frac{3}{2}\\log(\\eta T^{*})\\right)}\\\\ &{\\leq\\displaystyle\\frac{2}{\\sqrt{\\eta T^{*}}}\\leq\\log(\\eta T^{*}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the last inequality holds for any reasonably large $T^{*}$ . Merging all inequalities together, we haveys $\\gamma_{s}^{(T+1)}(s,k)+\\bar{\\beta}\\gamma_{-s}^{(T+1)}(s,k)\\leq4\\log(\\eta T^{*}).$ ", "page_idx": 30}, {"type": "text", "text": "Next, we will follow similar arguments to show that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\rho_{y_{i}}^{(T+1)}(i,p)+\\beta\\rho_{-y_{i}}^{(T+1)}(i,p)\\leq4\\log(\\eta T^{*})\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "for each $i\\in[n]$ and $p\\in[P]\\setminus\\{p_{i}^{*}\\}$ ", "page_idx": 30}, {"type": "text", "text": "Let T(P)< T be the smallet iteration such that pgi\\* $\\rho_{y_{i}}^{(\\tilde{T}_{i}^{(p)}+1)}(i,p)+\\beta\\rho_{-y_{i}}^{(\\tilde{T}_{i}^{(p)}+1)}(i,p)>2\\log(\\eta T^{*})$ We assume the existence of $\\tilde{T}_{i}^{(p)}$ : as its absence would directly lead to our desired conclusion; to see why, note that the following holds, due to (15) and (11): ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\rho_{y_{i}}^{(T+1)}(i,p)+\\beta\\rho_{-y_{i}}^{(T+1)}(i,p)}\\\\ &{=\\rho_{y_{i}}^{(T)}(i,p)+\\beta\\rho_{-y_{i}}^{(T)}(i,p)+\\frac{\\eta}{n}g_{i}^{(T)}\\bigg(\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)+\\beta\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)\\bigg)\\left\\|\\xi_{i}^{(p)}\\right\\|^{2}}\\\\ &{\\leq2\\log(\\eta T^{*})+2\\eta\\leq4\\log(\\eta T^{*}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the fist inequality is due to $\\begin{array}{r}{\\left\\|\\xi_{i}^{(p)}\\right\\|\\leq\\frac{3}{2}\\sigma_{\\mathrm{d}}^{2}d}\\end{array}$ and(A4), adthlastiqualty isde t(1). Now suppose there exists such $\\tilde{T}_{i}^{(p)}\\le T$ By (15), we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\rho_{y_{i}}^{(T+1)}(i,p)+\\beta\\rho_{-y_{i}}^{(T+1)}(i,p)\n$$", "text_format": "latex", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\rho_{y_{i}}^{(\\bar{T}_{i}^{(p)})}(i,p)+\\beta\\rho_{-y_{i}}^{(\\bar{T}_{i}^{(p)})}(i,p)}\\\\ &{\\phantom{=}+\\sum_{t=\\bar{T}_{i}^{(p)}}^{T}\\left(\\rho_{y_{i}}^{(t+1)}(i,p)+\\beta\\rho_{-y_{i}}^{(t+1)}(i,p)-\\rho_{y_{i}}^{(t)}(i,p)-\\beta\\rho_{-y_{i}}^{(t)}(i,p)\\right)}\\\\ &{\\le2\\log(\\eta T^{*})+\\log(\\eta T^{*})}\\\\ &{\\phantom{=}+\\frac{\\eta}{n}\\sum_{t=\\bar{T}_{i}^{(p)}+1}^{T}g_{i}^{(t)}\\biggl(\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)+\\beta\\phi^{\\prime}\\left(\\left\\langle w_{-s}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)\\biggr)\\left\\|\\xi_{i}^{(p)}\\right\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "The inequality is due to $\\rho_{y_{i}}^{(\\tilde{T}_{i}^{(p)})}(i,p)+\\beta\\rho_{-y_{i}}^{(\\tilde{T}_{i}^{(p)})}(i,p)\\leq2\\log(\\eta T^{*})$ from our choiceof $\\tilde{T}_{i}^{(p)}$ and ", "page_idx": 31}, {"type": "text", "text": "$\\frac{\\eta}{n}g_{i}^{(\\widetilde{T}_{i}^{(p)})}\\left[\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(\\widetilde{T}_{i}^{(p)})},x_{i}^{(p)}\\right\\rangle\\right)+\\beta\\phi^{\\prime}\\left(\\left\\langle w_{-s}^{(\\widetilde{T}_{i}^{(p)})},x_{i}^{(p)}\\right\\rangle\\right)\\right]\\left\\Vert\\xi_{i}^{(p)}\\right\\Vert^{2}\\leq2\\eta\\leq\\log(\\eta T^{*}),$ ", "page_idx": 31}, {"type": "text", "text": "from $\\begin{array}{r}{\\left\\|\\xi_{i}^{(p)}\\right\\|^{2}\\leq\\frac{3}{2}\\sigma_{\\mathrm{d}}^{2}d,}\\end{array}$ (A4), an (1). ", "page_idx": 31}, {"type": "text", "text": "For each $t=\\tilde{T}_{i}^{(p)}+1,\\ldots,T$ $i\\in\\mathcal{V}_{s,k}$ , then we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad y_{i}J_{W^{(t)}}(\\mathbf{\\boldsymbol{A}}_{i})}\\\\ &{=\\phi\\left(\\left\\langle w_{y_{i}}^{(t)},\\boldsymbol{x}_{i}^{(p)}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-y_{i}}^{(t)},\\boldsymbol{x}_{i}^{(p)}\\right\\rangle\\right)+\\displaystyle\\sum_{q\\in[P]\\backslash\\{p\\}}\\left(\\phi\\left(\\left\\langle w_{y_{i}}^{(t)},\\boldsymbol{x}_{i}^{(p)}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-y_{i}}^{(t)},\\boldsymbol{x}_{i}^{(p)}\\right\\rangle\\right)\\right)}\\\\ &{\\geq\\rho_{y_{i}}^{(t)}(i,p)+\\beta\\rho_{-y_{i}}^{(t)}(i,p)+\\gamma_{y_{i}}^{(t)}(s,k)+\\beta\\gamma_{-y_{i}}^{(t)}(s,k)}\\\\ &{\\quad+\\displaystyle\\sum_{q\\in[P]\\backslash\\{p,p_{i}^{*}\\}}\\left(\\rho_{y_{i}}^{(t)}(i,q)+\\beta\\rho_{-y_{i}}^{(t)}(i,q)\\right)-2P\\cdot o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right)}\\\\ &{\\geq\\frac{3}{1-\\alpha\\alpha^{(\\prime)}}\\frac{\\left\\langle v\\right\\rangle}{1-\\beta}}\\end{array}\n$$1og(nT\\*). ", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "The first inequality is due to Lemma B.4 and the second inequality holds because from our choice of   \n$t$ $,\\rho_{y_{i}}^{(t)}(i,p)\\stackrel{{\\textstyle\\cdot}}{+}\\beta\\rho_{-y_{i}}^{(t)}(i,p)\\geq2\\log(\\eta T^{*})$   \nTherefore, we have ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\eta}{n}\\displaystyle\\sum_{t=\\tilde{T}_{i}^{(p)}+1}^{T}g_{i}^{(t)}\\biggr(\\phi^{\\prime}\\left(\\left\\langle w_{y_{i}}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)+\\beta\\phi^{\\prime}\\left(\\left\\langle w_{-y_{i}}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)\\bigg)\\left\\|\\xi_{i}^{(p)}\\right\\|^{2}}\\\\ &{\\leq\\eta\\displaystyle\\sum_{t=\\tilde{T}_{i}^{(p)}+1}^{T}\\exp\\left(-y_{i}f_{W^{(t)}}(X_{i})\\right)\\leq(\\eta T^{*})\\exp\\left(-\\frac{3}{2}\\log(\\eta T^{*})\\right)}\\\\ &{\\leq\\displaystyle\\frac{1}{\\sqrt{\\eta T^{*}}}\\leq\\log(\\eta T^{*}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where the frst iequality is due to $\\begin{array}{r}{\\left\\lVert\\xi_{i}^{(p)}\\right\\rVert^{2}\\ \\leq\\ \\frac{3}{2}\\sigma_{\\mathrm{d}}^{2}}\\end{array}$ A4 ad tlast equality ls fy reasonably large $T^{*}$ Merngalqlw $\\rho_{y_{i}}^{(T+1)}(i,p)+\\beta\\rho_{-y_{i}}^{(T+1)}(i,p)\\leq$ $4\\log(\\eta T^{*})$ \u53e3 ", "page_idx": 31}, {"type": "text", "text": "C.2.2 Learning Common Features ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In the initial stages of training, the model quickly learns common features while exhibiting minimal overfitting to Gaussian noise. ", "page_idx": 31}, {"type": "text", "text": "First, we establish lower bounds on the number of iterations ensuring that noise coeffcients $\\rho_{s}^{(t)}(i,p)$ remain small,up to the order of $\\textstyle{\\frac{1}{P}}$ ", "page_idx": 31}, {"type": "text", "text": "Lemma C.2. Suppose the event $E_{\\mathrm{init}}$ occurs.There exists $\\begin{array}{r}{\\tilde{T}>\\frac{n}{6\\eta P\\sigma_{\\mathrm{d}}^{2}d}}\\end{array}$ such that $\\begin{array}{r}{\\rho_{s}^{(t)}(i,p)\\leq\\frac{1}{4P}}\\end{array}$ for all $0\\leq t<\\tilde{T},s\\in\\{\\pm1\\},i\\in[n]$ and $p\\in[P]\\setminus\\{p_{i}^{*}\\}$ ", "page_idx": 31}, {"type": "text", "text": "Proof of Lemma C.2. Let $\\tilde{T}$ be the smallst iteation such that $\\begin{array}{r}{\\rho_{s}^{(\\tilde{T})}(i,p)\\ \\geq\\ \\frac{1}{4P}}\\end{array}$ for some $s\\ \\in$ $\\{\\pm1\\},i\\in[n]$ and $p\\in[P]\\setminus\\{p_{i}^{*}\\}$ . We assume the existence of $\\tilde{T}$ as its absence would directly lead to our conclusion.Then, for any $0\\leq t<\\tilde{T}$ , we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\rho_{s}^{(t+1)}(i,p)=\\rho_{s}^{(t)}(i,p)+\\frac{\\eta}{n}g_{i}^{(t)}\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)\\left\\|\\xi_{i}^{(p)}\\right\\|^{2}\\leq\\rho_{s}^{(t)}(i,p)+\\frac{3\\eta\\sigma_{\\mathrm{d}}^{2}d}{2n},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where the inequalityis due to $g_{i}^{(t)}<1,\\phi^{\\prime}\\leq1$ and $\\begin{array}{r}{\\left\\|\\xi_{i}^{(p)}\\right\\|^{2}\\leq\\frac{3}{2}\\sigma_{\\mathrm{d}}^{2}d}\\end{array}$ Hence, we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\frac{1}{4P}\\leq\\rho_{s}^{(\\tilde{T})}(i,p)=\\sum_{t=0}^{\\tilde{T}-1}\\left(\\rho_{s}^{(t+1)}(i,p)-\\rho_{s}^{(t)}(i,p)\\right)<\\frac{3\\eta\\sigma_{\\mathrm{d}}^{2}d}{2n}\\tilde{T},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "and we conclude $\\begin{array}{r}{\\tilde{T}>\\frac{n}{6\\eta P\\sigma_{\\mathrm{d}}^{2}d}}\\end{array}$ which is the desired result. ", "page_idx": 32}, {"type": "text", "text": "Next, we will show that the model learns common features in at least constant order within $\\tilde{T}$ iterates. Lemma C.3 Suppse tevent $E_{\\mathrm{init}}$ occurs end $\\begin{array}{r}{\\rho_{k}=\\omega\\left(\\frac{\\sigma_{\\mathrm{d}}^{2}d}{\\beta n}\\right)}\\end{array}$ for some $k\\in[K]$ Then,for ach $s\\in\\{\\pm1\\}$ therexists $\\begin{array}{r}{T_{s,k}\\le\\frac{9n}{\\eta\\beta|\\mathcal{V}_{s,k}|}}\\end{array}$ such that $\\gamma_{s}^{(t)}(s,k)+\\beta\\gamma_{-s}^{(t)}(s,k)\\geq1$ for any $t>T_{s,k}$ ", "page_idx": 32}, {"type": "text", "text": "Proofof Lema C.3. Supose $\\gamma_{s}^{(t)}(s,k)+\\beta\\gamma_{-s}^{(t)}(s,k)\\,<\\,1$ for al $\\begin{array}{r}{0\\,\\leq\\,t\\,\\leq\\,\\frac{n}{6\\eta P\\sigma_{\\mathrm{d}}^{2}d}}\\end{array}$ For cach $i\\in\\mathcal{V}_{s,k}$ ,we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad y_{i}f_{W^{(t)}}(X_{i})}\\\\ &{=\\phi\\left(\\left\\langle w_{s}^{(t)},v_{s,k}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-s}^{(t)},v_{s,k}\\right\\rangle\\right)+\\displaystyle\\sum_{p\\in[P]\\backslash\\{p_{i}^{*}\\}}\\left(\\phi\\left(\\left\\langle w_{s}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-s}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)\\right)}\\\\ &{\\leq\\gamma_{s}^{(t)}(s,k)+\\beta\\gamma_{-s}^{(t)}(s,k)+\\displaystyle\\sum_{p\\in[P]\\backslash\\{p_{i}^{*}\\}}\\left(\\rho_{s}^{(t)}(i,p)+\\beta\\rho_{-s}^{(t)}(i,p)\\right)+2P\\cdot o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right)}\\\\ &{\\leq1+2P\\cdot\\frac{1}{4P}+2P\\cdot o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "The first inequality is due to Lemma B.4, the second inequality holds since we can apply Lemma C.2. and the last inequality is due to (A1). Thus, I+xp(u:Sw(e(xi) and we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\gamma_{s}^{(t+1)}(s,k)+\\beta\\gamma_{-s}^{(t+1)}(s,k)}\\\\ &{=\\gamma_{s}^{(t)}(s,k)+\\beta\\gamma_{-s}^{(t)}(s,k)+\\frac{\\eta}{n}\\sum_{i\\in\\mathcal{V}_{s,k}}g_{i}^{(t)}\\bigg(\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(t)},v_{s,k}\\right\\rangle\\right)+\\beta\\phi^{\\prime}\\left(\\left\\langle w_{-s}^{(t)},v_{s,k}\\right\\rangle\\right)\\bigg)}\\\\ &{\\geq\\gamma_{s}^{(t)}(s,k)+\\beta\\gamma_{-s}^{(t)}(s,k)+\\frac{\\eta\\beta|\\mathcal{V}_{s,k}|}{9n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Notice that $|\\mathcal{V}_{s,k}|~=~\\rho_{k}n$ . From the condition in the lemma statement, we have $\\frac{9n}{\\eta\\beta|{\\mathscr V}_{s,k}|}\\;=$ $\\scriptstyle O\\,\\left({\\frac{n}{6\\eta P\\sigma_{\\mathrm{d}}^{2}d}}\\right)$ .If we choose $\\begin{array}{r}{t_{0}\\in\\left[\\frac{9n}{\\eta\\beta|\\mathcal{V}_{s,k}|},\\frac{n}{6\\eta P\\sigma_{\\mathrm{d}}^{2}d}\\right]}\\end{array}$ ,then ", "page_idx": 32}, {"type": "equation", "text": "$$\n1>\\gamma_{s}^{(t_{0})}(s,k)+\\beta\\gamma_{-s}^{(t_{0})}(s,k)\\geq\\frac{\\eta\\beta|\\mathcal{V}_{s,k}|}{9n}t_{0}\\geq1,\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "and this saitry tit $\\gamma_{s}^{(t)}(s,k)+\\beta\\gamma_{-s}^{(t)}(s,k)<1$ for al $0\\leq t\\leq$ $\\frac{n}{6\\eta P\\sigma_{\\mathrm{d}}^{2}d}$ $\\begin{array}{r}{0\\,\\le\\,T_{s,k}\\,<\\,\\frac{n}{6\\eta P\\sigma_{\\mathrm{d}}^{2}d}}\\end{array}$ suchthat $\\gamma_{s}^{(T_{s,k}+1)}(s,k)+\\beta\\gamma_{-s}^{(T_{s,k}+1)}(s,k)\\geq1$ and choose the smallest one. Then we obtain ", "page_idx": 32}, {"type": "equation", "text": "$$\n1>\\gamma_{s}^{(T_{s,k})}(s,k)+\\beta\\gamma_{-s}^{(T_{s,k})}(s,k)\\geq\\frac{\\eta\\beta|\\mathcal{V}_{s,k}|}{9n}T_{s,k}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Therefore, Ts,k < $\\begin{array}{r}{T_{s,k}<\\frac{9n}{\\eta\\beta|\\mathcal{V}_{s,k}|}}\\end{array}$ and this is what we desired. ", "page_idx": 32}, {"type": "text", "text": "What We Have So Far. For any common feature $\\pmb{v}_{s,k}$ with $s\\in\\{\\pm1\\}$ and $k\\in\\kappa_{C}$ , it satisfies $\\rho_{k}=$ $\\begin{array}{r}{w\\left(\\frac{\\sigma_{\\mathrm{d}}^{2}d}{\\beta n}\\right)}\\end{array}$ due to (A4) By Lema C3, a any iterae $t\\in[\\bar{T}_{1},T^{*}]$ Wwith $\\bar{T}_{1}:=\\operatorname*{max}_{s\\in\\{\\pm1\\},k\\in K_{C}}T_{s,k}$ \uff0c the following properties hold if the event $E_{\\mathrm{init}}$ occurs: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 (Learn common features): For any $s\\in\\{\\pm1\\}$ and $k\\in\\kappa_{C}$ \uff0c ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\gamma_{s}^{(t)}(s,k)+\\beta\\gamma_{-s}^{(t)}(s,k)=\\Omega(1).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "\u00b7 For any $s\\in\\{\\pm1\\},i\\in[n]$ , and $p\\in[P]\\backslash\\{p_{i}^{*}\\},\\rho_{s}^{(t)}(i,p)=\\widetilde{\\mathcal{O}}\\left(\\beta^{-1}\\right).$ ", "page_idx": 33}, {"type": "text", "text": "C.2.3 Overfitting (extremely) Rare Data ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "In the previous step, we have shown that common data can be well-classified by learning common features. In this step, we will show that the model correctly classifies (extremely) rare data by overfitting dominant noise instead of learning its features. ", "page_idx": 33}, {"type": "text", "text": "We frst introduce lower bounds on the number o iterates such that feature coeficients $\\gamma_{s}^{(t)}(s^{\\prime},k)$ remain small, up to the order of $\\alpha^{2}\\beta^{-1}$ . This lemma holds for any kind of features, but we will focus on (extremely) rare features. This does not contradict the results from Section C.2.2 for common features since the upper bound on the number of iterations in Lemma C.3 is larger than the lower bound on the number of iterations in this lemma. ", "page_idx": 33}, {"type": "text", "text": "Lemma C.4. Suppose the event $E_{\\mathrm{init}}$ occurs. For each $s~\\in~\\{\\pm1\\}$ and $\\textit{k}\\in\\ [K]$ , there exists $\\begin{array}{r}{\\tilde{T}_{s,k}>\\frac{n\\alpha^{2}}{\\eta\\beta|\\mathcal{V}_{s,k}|}}\\end{array}$ such har $\\gamma_{s^{\\prime}}^{(t)}(s,k)\\le\\alpha^{2}\\beta^{-1}$ forary $0\\leq t<\\tilde{T}_{s,k}$ $s^{\\prime}\\in\\{\\pm1\\}$ ", "page_idx": 33}, {"type": "text", "text": "Proof of Lemma C.4. Let $\\tilde{T}_{s,k}$ be the smallest iterate such that (Ta,k)(s,k) > \u03b1\u00b2\u03b2-1 for some $s^{\\prime}\\in\\{\\pm1\\}$ . We assume the existence of $\\tilde{T}_{s,k}$ , as its absence would directly lead to our conclusion. For any $0\\leq t<\\tilde{T}_{s,k}$ \uff0c ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\gamma_{s^{\\prime}}^{(t+1)}(s,k)=\\gamma_{s^{\\prime}}^{(t)}(s,k)+\\frac{\\eta}{n}\\sum_{i\\in\\mathcal{V}_{s,k}}g_{i}^{(t)}\\phi^{\\prime}\\left(\\left\\langle w_{s^{\\prime}}^{(t)},v_{s,k}\\right\\rangle\\right)\\leq\\gamma_{s^{\\prime}}^{(t)}(s,k)+\\frac{\\eta|\\mathcal{V}_{s,k}|}{n},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\alpha^{2}\\beta^{-1}<\\gamma_{s^{\\prime}}^{(\\tilde{T}_{s,k})}(s,k)=\\sum_{t=0}^{\\tilde{T}_{s,k}-1}\\left(\\gamma_{s^{\\prime}}^{(t+1)}(s,k)-\\gamma_{s^{\\prime}}^{(t)}(s,k)\\right)\\leq\\frac{\\eta|\\mathcal{V}_{s,k}|}{n}\\tilde{T}_{s,k}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We conclude $\\begin{array}{r}{\\tilde{T}_{s,k}>\\frac{n\\alpha^{2}}{\\eta\\beta|\\mathcal{V}_{s,k}|}}\\end{array}$ which is the desired result. ", "page_idx": 33}, {"type": "text", "text": "Next, we will show that the model overfits (extremely) rare data by memorizing dominant noise patches in at least constant order within $\\tilde{T}_{s,k}$ iterates. ", "page_idx": 33}, {"type": "text", "text": "Lemma C.5. Suppose the event $E_{\\mathrm{init}}$ occurs and $\\rho_{k}=o\\left(\\frac{\\alpha^{2}\\sigma_{\\mathrm{d}}^{2}d}{n}\\right)$ Then, for each $i\\in\\mathcal{V}_{s,k}$ there exiss $\\begin{array}{r}{T_{i}\\in\\left[\\bar{T}_{1},\\frac{18n}{\\eta\\beta\\sigma_{\\mathrm{d}}^{2}d}\\right]}\\end{array}$ ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\sum_{p\\in[P]\\backslash\\{p_{i}^{*}\\}}\\Big(\\rho_{s}^{(t)}(i,p)+\\beta\\rho_{-s}^{(t)}(i,p)\\Big)\\geq1,\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "for any $t>T_{i}$ ", "page_idx": 33}, {"type": "text", "text": "Proof of Lemma C.5. Suppose $\\begin{array}{r}{\\begin{array}{r}{\\Im\\sum_{p\\in[P]\\backslash\\{p_{i}^{*}\\}}\\left(\\rho_{s}^{(t)}(i,p)+\\beta\\rho_{-s}^{(t)}(i,p)\\right)<1\\mathrm{~for~}0\\leq t\\leq\\frac{n\\alpha^{2}}{\\eta\\beta|\\mathcal{V}_{s,k}|},}\\end{array}}\\end{array}$ From Lemma B.4 and Lemma C.4, we have ", "page_idx": 33}, {"type": "equation", "text": "$$\ny_{i}f_{\\mathbf{W}^{(t)}}\\left(\\mathbf{X}_{i}\\right)\n$$", "text_format": "latex", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\phi\\left(\\left\\langle w_{s}^{(t)},v_{s,k}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-s}^{(t)},v_{s,k}\\right\\rangle\\right)+\\displaystyle\\sum_{p\\in[P]\\backslash\\{p_{i}^{*}\\}}\\left(\\phi\\left(\\left\\langle w_{s}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-s}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)\\right)}\\\\ &{\\leq\\gamma_{s}^{(t)}(s,k)+\\beta\\gamma_{-s}^{(t)}(s,k)+\\displaystyle\\sum_{p\\in[P]\\backslash\\{p_{i}^{*}\\}}\\left(\\rho_{s}^{(t)}(i,p)+\\beta\\rho_{-s}^{(t)}(i,p)\\right)+2P\\cdot o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right)}\\\\ &{\\leq(1+\\beta)\\alpha^{2}\\beta^{-1}+1+2P\\cdot o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "wherehelasl $\\begin{array}{r}{g_{i}^{(t)}=\\frac{1}{1+\\exp\\left(y_{i}f_{W^{(t)}}(\\pmb{X}_{i})\\right)}\\geq\\frac{1}{9}}\\end{array}$ Als, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\rho_{s}^{(t+1)}(i,\\tilde{p}_{i})+\\beta\\rho_{-s}^{(t+1)}(i,\\tilde{p}_{i})}\\\\ &{=\\rho_{s}^{(t)}(i,\\tilde{p}_{i})+\\beta\\rho_{-s}^{(t)}(i,\\tilde{p}_{i})+\\frac{\\eta}{n}g_{i}^{(t)}\\bigg(\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(t)},x_{i}^{(\\tilde{p}_{i})}\\right\\rangle\\right)+\\beta\\phi^{\\prime}\\left(\\left\\langle w_{-s}^{(t)},x_{i}^{(\\tilde{p}_{i})}\\right\\rangle\\right)\\bigg)\\left\\|\\xi_{i}^{(\\tilde{p}_{i})}\\right\\|^{2}}\\\\ &{\\geq\\rho_{s}^{(t)}(i,\\tilde{p}_{i})+\\beta\\rho_{-s}^{(t)}(i,\\tilde{p}_{i})+\\frac{\\eta\\beta\\sigma_{d}^{2}d}{18n},}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where the last inequality is d $\\begin{array}{r}{\\left\\lVert\\xi_{i}^{(\\tilde{p}_{i})}\\right\\rVert^{2}\\geq\\frac12\\sigma_{\\mathrm{d}}^{2}d}\\end{array}$ and $\\phi^{\\prime}\\geq\\beta$ ", "page_idx": 34}, {"type": "text", "text": "Notice that $|\\mathcal{V}_{s,k}|\\,=\\,\\rho_{k}n$ . From the given condition in the lemma statement, we have $\\begin{array}{r l}{\\frac{18n}{\\eta\\beta\\sigma_{\\mathrm{d}}^{2}d}\\,=\\,}&{{}}\\end{array}$ $o\\left(\\frac{n\\alpha^{2}}{\\eta\\beta|\\mathcal{V}_{s,k}|}\\right)$ If wechoose $\\begin{array}{r}{t_{0}\\in\\left[\\frac{18n}{\\eta\\beta\\sigma_{\\mathrm{d}}^{2}d},\\frac{n\\alpha^{2}}{\\eta\\beta|\\mathcal{V}_{s,k}|}\\right]}\\end{array}$ , then we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n1>\\sum_{p\\in[P]\\backslash\\{p_{i}^{*}\\}}\\left(\\rho_{s}^{(t_{0})}(i,p)+\\beta\\rho_{-s}^{(t_{0})}(i,p)\\right)\\geq\\rho_{s}^{(t_{0})}(i,\\tilde{p}_{i})+\\beta\\rho_{-s}^{(t_{0})}(i,\\tilde{p}_{i})\\geq\\frac{\\eta\\beta\\sigma_{\\mathrm{d}}^{2}d}{18n}t_{0}\\geq1.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "This is a contradiction therefore it cannot hold that $\\begin{array}{r}{\\sum_{p\\in[P]\\backslash\\{p_{i}^{*}\\}}\\left(\\rho_{s}^{(t)}(i,p)+\\beta\\rho_{-s}^{(t)}(i,p)\\right)\\,<\\,1}\\end{array}$ for all $\\begin{array}{r}{0\\ \\leq\\ t\\ \\leq\\ \\frac{n\\alpha^{2}}{\\eta\\beta|\\mathcal{V}_{s,k}|}}\\end{array}$ Hence, we can choose the smallest $\\begin{array}{r}{0\\ \\leq\\ T_{i}\\ <\\ \\frac{n\\alpha^{2}}{\\eta\\beta|\\mathcal{V}_{s,k}|}}\\end{array}$ such that $\\begin{array}{r}{\\sum_{p\\in[P]\\backslash\\{p_{i}^{*}\\}}\\left(\\rho_{s}^{(T_{i}+1)}(i,p)+\\beta\\rho_{-s}^{(T_{i}+1)}(i,p)\\right)\\geq1.}\\end{array}$   \nFor any $0\\leq t<T_{i}$ ", "page_idx": 34}, {"type": "equation", "text": "$$\n1\\geq\\sum_{p\\in[P]\\backslash\\{p_{i}^{*}\\}}\\left(\\rho_{s}^{(T_{i})}(i,p)+\\beta\\rho_{-s}^{(T_{i})}(i,p)\\right)\\geq\\rho_{s}^{(T_{i})}(i,\\tilde{p}_{i})+\\beta\\rho_{-s}^{(T_{i})}(i,\\tilde{p}_{i})\\geq\\frac{\\eta\\beta\\sigma_{\\mathrm{d}}^{2}d}{18n}T_{i},\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and we conclude that $\\begin{array}{r}{T_{i}\\leq\\frac{18n}{\\eta\\beta\\sigma_{\\mathrm{d}}^{2}d}}\\end{array}$ ", "page_idx": 34}, {"type": "text", "text": "Lastly, we move on to prove $T_{i}>\\bar{T}_{1}$ . Combining Lemma C.2 and Lemma C.3 leads to ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\sum_{p\\in[P]\\backslash\\{p_{i}^{*}\\}}\\left(\\rho_{s}^{(\\bar{T}_{1})}(i,p)+\\beta\\rho_{-s}^{(\\bar{T}_{1})}(i,p)\\right)\\leq\\frac{1}{2}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Thus, we have $T_{i}>\\bar{T}_{1}$ and this is what we desired. ", "page_idx": 34}, {"type": "text", "text": "What We Have So Far.  For any $k\\,\\in\\,\\mathcal{K}_{R}\\cup K_{E}$ , it satisfies $\\rho_{k}\\,=\\,o\\left(\\frac{\\alpha^{2}\\sigma_{\\mathrm{d}}^{2}d}{n}\\right)$ due to (A5). By Lemma C.5 at iterate $t\\in[T_{\\mathrm{ERM}},T^{*}]$ with ", "page_idx": 34}, {"type": "equation", "text": "$$\nT_{\\mathrm{ERM}}:=\\operatorname*{max}_{s\\in\\{\\pm1\\}\\atop{k\\in K_{R}\\cup K_{E}}}\\operatorname*{max}_{i\\in\\mathcal{V}_{s,k}}T_{i}\\quad\\in\\left[\\bar{T}_{1},T^{*}\\right]\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "the following properties hold if the event $E_{\\mathrm{init}}$ occurs: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 (Learn common features): For $s\\in\\{\\pm1\\}$ and $k\\in\\kappa_{C}$ ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\gamma_{s}^{(t)}(s,k)+\\beta\\gamma_{-s}^{(t)}(s,k)=\\Omega(1),\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "\u00b7 (Overfit (extremely) rare data): For any $s\\in\\{\\pm1\\}$ \uff0c $k\\in K_{R}\\cup K_{E}$ , and $i\\in\\mathcal{V}_{s,k}$ ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\sum_{p\\in[P]\\backslash\\{p_{i}^{*}\\}}\\left(\\rho_{s}^{(t)}(i,p)+\\beta\\rho_{-s}^{(t)}(i,p)\\right)=\\Omega(1),\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "\u00b7 (Do not learn (extremely) rare features at $T_{\\mathrm{ERM}}$ 0: For any $s,s^{\\prime}\\in\\{\\pm1\\}$ and $k\\,\\in\\,K_{R}\\cup K_{E}$ $\\gamma_{s^{\\prime}}^{(T_{\\mathrm{ERM}})}(s,k)\\leq\\alpha^{2}\\beta^{-1}$ ", "page_idx": 35}, {"type": "text", "text": "\u00b7 For any $s\\in\\{\\pm1\\},i\\in[n]$ , and $p\\in[P]\\backslash\\{p_{i}^{*}\\},\\rho_{s}^{(t)}(i,p)=\\widetilde{\\mathcal{O}}\\left(\\beta^{-1}\\right)\\!.$ ", "page_idx": 35}, {"type": "text", "text": "C.2.4 ERM cannot Learn (extremely) Rare Features Within Polynomial Times ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "In this step, we will show that ERM cannot learn (extremely) rare features within the maximum admissible iterations T\\* = poly(d) . ", "page_idx": 35}, {"type": "text", "text": "From now on, we fix any $s^{*}\\in\\{\\pm1\\}$ and $k^{*}\\in K_{R}\\cup K_{E}$ . Recall that we defined the set $\\mathcal{W}$ and the function $Q^{(s^{*},k^{*})}\\,:\\,\\mathcal{W}\\to\\,\\bar{\\mathbb{R}^{d\\times2}}$ in Lemma B.5. Let us omit superscripts for simplicity. For each iteration $t$ $Q(\\pmb{W}^{(t)})$ represents the cumulative updates contributed by data points with feature vector $\\pmb{v}_{s^{*},k^{*}}$ until $t$ -th iteration. We will sequentially introduce several technical lemmas and by combining these lemmas, quantify update by data with feature vector $\\pmb{v}_{s^{*},k^{*}}$ after $T_{\\mathrm{ERM}}$ and derive our conclusion. ", "page_idx": 35}, {"type": "text", "text": "Let us define $W^{*}=\\left\\{\\pmb{w}_{1}^{*},\\pmb{w}_{-1}^{*}\\right\\}$ , where ", "page_idx": 35}, {"type": "equation", "text": "$$\nw_{s}^{*}=w_{s}^{(T_{\\mathrm{ERM}})}+M\\sum_{i\\in\\mathcal{V}_{s^{*},k^{*}}}\\frac{\\xi_{i}^{(\\tilde{p}_{i})}}{\\left\\|\\xi_{i}^{(\\tilde{p}_{i})}\\right\\|^{2}},\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "for each $s\\in\\{\\pm1\\}$ with $\\begin{array}{r}{M=4\\beta^{-1}\\log\\left(\\frac{2\\eta\\beta^{2}T^{*}}{\\alpha^{2}}\\right)}\\end{array}$ $\\beta<1$ and $\\begin{array}{r}{T^{*}=\\frac{\\mathrm{poly}(d)}{\\eta}}\\end{array}$ together imply $M=\\widetilde{\\mathcal{O}}\\left(\\beta^{-1}\\right)$ . Note that $\\mathbf{\\nabla}W^{(t)}$ \uff0c $W^{*}\\in\\mathcal{W}$ for any $t\\geq0$ ", "page_idx": 35}, {"type": "text", "text": "Lemma C.6. Suppose the event $E_{\\mathrm{init}}$ occurs. Then, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\left\\|Q\\left(W^{(T_{\\mathrm{ERM}})}\\right)-Q(W^{*})\\right\\|^{2}\\leq12M^{2}|\\mathcal{V}_{s^{*},k^{*}}|\\sigma_{\\mathrm{d}}^{-2}d^{-1},\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where $\\lVert\\cdot\\rVert$ denotes the Frobenius norm. ", "page_idx": 35}, {"type": "text", "text": "Proof of Lemma C.6. For each $s\\in\\{\\pm1\\}$ \uff0c", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad s s^{*}\\left(Q_{s}\\left(w_{s}^{*}\\right)-Q_{s}\\left(w_{s}^{(T_{\\mathrm{ERM}})}\\right)\\right)}\\\\ &{=Q_{s}\\left(s s^{*}M\\displaystyle\\sum_{i\\in\\mathcal{V}_{s^{*},k^{*}}}\\frac{\\xi_{i}^{(\\tilde{p}_{i})}}{\\left\\|\\xi_{i}^{(\\tilde{p}_{i})}\\right\\|}\\right)}\\\\ &{=M\\displaystyle\\sum_{i\\in\\mathcal{V}_{s^{*},k^{*}}}\\frac{\\xi_{i}^{(\\tilde{p}_{i})}}{\\left\\|\\xi_{i}^{(\\tilde{p}_{i})}\\right\\|^{2}}+\\alpha M\\left(\\sum_{i\\in\\mathcal{F}_{s}\\cap\\mathcal{V}_{s^{*},k^{*}}}\\frac{v_{s,1}}{\\left\\|\\xi_{i}^{(\\tilde{p}_{i})}\\right\\|^{2}}+\\sum_{i\\in\\mathcal{F}_{-s}\\cap\\mathcal{V}_{s^{*},k^{*}}}\\frac{v_{-s,1}}{\\left\\|\\xi_{i}^{(\\tilde{p}_{i})}\\right\\|^{2}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "and we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\left\\|Q\\left(W^{(T_{\\mathrm{ERM}})}\\right)-Q(W^{*})\\right\\|^{2}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\left\\|Q_{1}(\\pmb{w}_{1}^{*})-Q_{1}\\left(\\pmb{w}_{1}^{(T_{\\mathrm{ERM}})}\\right)\\right\\|^{2}+\\left\\|Q_{-1}(\\pmb{w}_{-1}^{*})-Q_{-1}\\left(\\pmb{w}_{-1}^{(T_{\\mathrm{ERM}})}\\right)\\right\\|^{2}}\\\\ &{\\leq2M^{2}\\left(\\displaystyle\\sum_{i\\in\\mathcal{V}_{s^{*},k^{*}}}\\left\\|\\xi_{i}^{(\\tilde{p}_{i})}\\right\\|^{-2}+\\displaystyle\\sum_{i,j\\in\\mathcal{V}_{s^{*},k^{*}},i\\neq j}\\displaystyle\\frac{\\left|\\left\\langle\\xi_{i}^{(\\tilde{p}_{i})},\\xi_{j}^{(\\tilde{p}_{j})}\\right\\rangle\\right|}{\\left\\|\\xi_{i}^{(\\tilde{p}_{i})}\\right\\|^{2}\\left\\|\\xi_{j}^{(\\tilde{p}_{j})}\\right\\|^{2}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "equation", "text": "$$\n+\\,2M^{2}\\left(\\alpha^{2}\\left(\\sum_{i\\in\\mathcal{F}_{s}\\cap\\mathcal{V}_{s^{*},k^{*}}}\\left\\|\\xi_{i}^{(\\tilde{p}_{i})}\\right\\|^{-2}\\right)^{2}+\\alpha^{2}\\left(\\sum_{i\\in\\mathcal{F}_{-s}\\cap\\mathcal{V}_{s^{*},k^{*}}}\\left\\|\\xi_{i}^{(\\tilde{p}_{i})}\\right\\|^{-2}\\right)^{2}\\right).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "From the event $E_{\\mathrm{init}}$ defined in Lemma B.2 and (A2), we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{\\substack{i,j\\in\\mathcal{V}_{s^{*},k^{*}},i\\neq j}}\\displaystyle\\frac{\\left|\\left\\langle\\xi_{i}^{\\left(\\widetilde{\\rho}_{i}\\right)},\\xi_{j}^{\\left(\\widetilde{\\rho}_{j}\\right)}\\right\\rangle\\right|}{\\left|\\left\\{\\xi_{i}^{\\left(\\widetilde{\\rho}_{i}\\right)}\\right\\}\\right|^{2}\\left\\|\\xi_{j}^{\\left(\\widetilde{\\rho}_{j}\\right)}\\right\\|^{2}}\\leq\\displaystyle\\sum_{\\substack{i\\in\\mathcal{V}_{s^{*},k^{*}}}}\\displaystyle\\sum_{j\\in\\mathcal{V}_{s^{*},k^{*}}}\\left\\|\\xi_{i}^{\\left(\\widetilde{\\rho}_{i}\\right)}\\right\\|^{-2}\\widetilde{\\mathcal{O}}\\left(d^{-\\frac{1}{2}}\\right)}\\\\ {\\leq\\displaystyle\\sum_{\\substack{i\\in\\mathcal{V}_{s^{*},k^{*}}}}\\left\\|\\xi_{i}^{\\left(\\widetilde{\\rho}_{i}\\right)}\\right\\|^{-2}\\widetilde{\\mathcal{O}}\\left(n d^{-\\frac{1}{2}}\\right)}\\\\ &{\\leq\\displaystyle\\sum_{\\substack{i\\in\\mathcal{V}_{s^{*},k^{*}}}}\\left\\|\\xi_{i}^{\\left(\\widetilde{\\rho}_{i}\\right)}\\right\\|^{-2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "In addition, we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha^{2}\\left(\\displaystyle\\sum_{i\\in\\mathcal{F}_{s}\\cap\\mathcal{V}_{s},\\,i\\in\\mathcal{I}}\\left\\|\\xi_{i}^{(\\bar{\\rho}_{i})}\\right\\|^{-2}\\right)^{2}+\\alpha^{2}\\left(\\displaystyle\\sum_{i\\in\\mathcal{F}_{s-s}\\cap\\mathcal{V}_{s},\\,i\\in\\mathcal{I}}\\left\\|\\xi_{i}^{(\\bar{\\rho}_{i})}\\right\\|^{-2}\\right)^{2}}\\\\ &{\\leq\\left(\\displaystyle\\sum_{i\\in\\mathcal{F}_{s}\\cap\\mathcal{V}_{s},\\,i\\in\\mathcal{I}}\\left\\|\\xi_{i}^{(\\bar{\\rho}_{i})}\\right\\|^{-2}\\right)^{2}+\\left(\\displaystyle\\sum_{i\\in\\mathcal{F}_{s-s}\\cap\\mathcal{V}_{s},\\,i\\in\\mathcal{I}}\\left\\|\\xi_{i}^{(\\bar{\\rho}_{i})}\\right\\|^{-2}\\right)^{2}}\\\\ &{\\leq\\displaystyle\\sum_{i\\in\\mathcal{F}_{s}\\cap\\mathcal{V}_{s},\\,i\\in\\mathcal{I}}\\left\\|\\xi_{i}^{(\\bar{\\rho}_{i})}\\right\\|^{-2}+\\displaystyle\\sum_{i\\in\\mathcal{F}_{s-s}\\cap\\mathcal{V}_{s},\\,i\\in\\mathcal{I}}\\left\\|\\xi_{i}^{(\\bar{\\rho}_{i})}\\right\\|^{-2}}\\\\ &{=\\displaystyle\\sum_{i\\in\\mathcal{F}_{s}\\cap\\mathcal{V}_{s},\\,i\\in\\mathcal{I}}\\left\\|\\xi_{i}^{(\\bar{\\rho}_{i})}\\right\\|^{-2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where the frs inequlity is due to $\\alpha<1$ and the second inequaliy is due to $\\begin{array}{r}{\\sum_{i\\in\\mathcal{V}_{s^{*},k^{*}}}\\left\\|\\xi_{i}^{(\\tilde{p}_{i})}\\right\\|^{-2}\\leq}\\end{array}$ $2|\\mathcal{V}_{s^{*},k^{*}}|\\sigma_{\\mathrm{d}}^{-2}d^{-1}<1$ from (A5). Hence, from $E_{\\mathrm{init}}$ weobtain ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\Big\\|Q\\left(W^{(T_{\\mathrm{ERM}})}\\right)-Q(W^{*})\\Big\\|^{2}\\leq6M^{2}\\sum_{i\\in\\mathcal{V}_{s^{*},k^{*}}}\\left\\|\\xi_{i}^{(\\tilde{p}_{i})}\\right\\|^{-2}\\leq12M^{2}|\\mathcal{V}_{s^{*},k^{*}}|\\sigma_{\\mathrm{d}}^{-2}d^{-1}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Lemma C.7. Suppose the $E_{\\mathrm{init}}$ occurs. For any $t\\geq T_{\\mathrm{ERM}}$ and $i\\in\\mathcal{V}_{s^{*},k^{*}}$ , it holds that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\langle y_{i}\\nabla_{W}f_{W^{(t)}}(X_{i}),Q(W^{*})\\rangle\\geq\\frac{M\\beta}{2}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Proof of Lemma C.7. We have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle y_{i}\\nabla_{W}f_{W^{(t)}}(X_{i}),Q(W^{*})\\rangle}\\\\ &{=\\displaystyle\\sum_{p\\in[P]}\\left(\\phi^{\\prime}\\left(\\left\\langle w_{s^{*}}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)\\left\\langle Q_{s^{*}}(w_{s^{*}}^{*}),x_{i}^{(p)}\\right\\rangle-\\phi^{\\prime}\\left(\\left\\langle w_{-s^{*}}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)\\left\\langle Q_{-s^{*}}(w_{-s^{*}}^{*}),x_{i}^{(p)}\\right\\rangle\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "For any $s\\in\\{\\pm1\\}$ and $p\\in[P]\\setminus\\{p_{i}^{*},\\tilde{p}_{i}\\}$ ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad s s^{*}\\left\\langle Q_{s}(w_{s}^{*}),\\xi_{i}^{(p)}\\right\\rangle}\\\\ &{=\\rho_{s}^{(T_{\\mathrm{ERM}})}(i,p)+\\underset{j\\in V_{s^{*},k^{*},q}\\in[P]\\backslash\\{p_{j}^{*}\\}}{\\sum}\\rho_{s}^{(T_{\\mathrm{ERM}})}(j,q)\\frac{\\left\\langle\\xi_{i}^{(p)},\\xi_{j}^{(q)}\\right\\rangle}{\\left\\|\\xi_{j}^{(q)}\\right\\|^{2}}+\\underset{j\\in V_{s^{*},k^{*}}}{\\sum}M\\frac{\\left\\langle\\xi_{i}^{(p)},\\xi_{j}^{(\\tilde{p}_{j})}\\right\\rangle}{\\left\\|\\xi_{j}^{(\\tilde{p}_{j})}\\right\\|^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ge-\\widetilde{\\mathcal{O}}\\left(n P\\beta^{-1}\\sigma_{\\mathrm{d}}\\sigma_{\\mathrm{b}}^{-1}d^{-\\frac{1}{2}}\\right)-\\widetilde{\\mathcal{O}}\\left(n M\\sigma_{\\mathrm{b}}\\sigma_{\\mathrm{d}}^{-1}d^{-\\frac{1}{2}}\\right)}\\\\ &{=-o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where the last equality is due to (9) and $M\\ \\ =\\ \\ \\widetilde{\\mathcal{O}}\\left(\\beta^{-1}\\right)$ .Also, for any $\\begin{array}{l l l}{{s}}&{{\\in}}&{{\\{\\pm1\\}}}\\end{array}$ $s s^{\\ast}\\left\\langle Q_{s}(\\pmb{w}_{s}^{\\ast}),\\pmb{v}_{s^{\\ast},k^{\\ast}}\\right\\rangle=\\gamma_{s}^{(T_{\\mathrm{ERM}})}(s^{\\ast},k^{\\ast})\\geq0$ In adion, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad_{S^{\\star}}\\left\\langle Q_{0}(\\mathbf{r}^{\\star}),x_{i}^{(k)}\\right\\rangle}\\\\ &{=s^{\\star}\\left\\langle Q_{s}(\\mathbf{r}^{\\star}),\\xi_{i}^{(k)}\\right\\rangle+s^{\\star}\\left\\langle Q_{s}(\\mathbf{r}_{i}^{\\star}),x_{i}^{(k)}-\\xi_{i}^{(k)}\\right\\rangle}\\\\ &{\\geq s^{\\star}\\left\\langle Q_{s}(\\mathbf{r}_{i}^{\\star}),\\xi_{i}^{(k)}\\right\\rangle-\\bar{\\mathcal{O}}\\left(\\sigma_{j}^{2\\star}\\mathbf{\\cdot}p_{i},n_{i}\\sigma_{i}^{2\\star}\\right)}\\\\ &{=M+p_{s}^{2\\lceil\\operatorname*{max}\\{\\bar{q}_{i},\\bar{q}_{j}\\}}+\\frac{\\sum_{\\mathbf{r}\\in\\partial\\Omega}\\left(p_{s}^{\\star}\\right)}{\\sum_{\\mathbf{r}\\in\\partial\\Omega}\\left(p_{s}^{\\star}\\right)}p_{i}^{\\prime\\operatorname*{max}\\{\\bar{q}_{i},\\bar{q}_{j}\\}}\\frac{\\zeta_{i}^{(k)}\\cdot\\xi_{i}^{(q)}}{\\left\\|\\xi_{i}^{(q)}\\right\\|^{2}}}\\\\ &{\\quad+\\underbrace{\\sum_{\\mathbf{r}\\in\\partial\\Omega}\\left(\\frac{\\zeta_{i}^{(k)}\\cdot\\xi_{i}^{(k)}}{\\left\\|\\xi_{i}^{(k)}\\right\\|^{2}}\\right)^{\\!\\!1/2}}_{\\geq\\delta\\cdot\\delta\\cdot\\delta\\cdot\\delta\\cdot\\delta}\\left(\\sigma_{j}^{2\\star-1}p_{i},n_{i}\\sigma_{i}^{2\\star-1}\\right)}\\\\ &{\\geq M-\\bar{\\mathcal{O}}\\left(p^{\\star}\\right)^{-1}\\sigma_{i}\\sigma_{i}\\frac{\\left\\langle\\overline{{q}}^{\\star}\\right\\rangle-\\left\\langle\\overline{{\\mathbf{\\xi}}}\\right\\rangle}{2}-\\bar{\\mathcal{O}}\\left(\\sigma_{j}^{2\\star-1}p_{i},n_{i}\\sigma_{i}^{2\\star-1}\\right)}\\\\ &{=M-\\sigma\\left(\\frac{1}{p\\sqrt{\\operatorname*{min}\\left(q\\right)}}\\right)}\\\\ &{\\geq\\frac{M}{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where the first inequality is due to the definition of $Q$ and the second-to-last line is due to (9) and (A7). ", "page_idx": 37}, {"type": "text", "text": "Hence, applying (16) and (17) for $s=s^{*},-s^{*}$ and combining with $\\phi^{\\prime}\\geq\\beta$ we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\left\\langle y_{i}\\nabla_{W}f_{W^{(t)}}(\\mathbf{X}_{i}),Q(W^{*})\\right\\rangle\\geq M\\beta-o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right)\\geq\\frac{M\\beta}{2}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "By combining Lemma C.6 and Lemma C.7, we can obtain the following result. ", "page_idx": 37}, {"type": "text", "text": "Lemma C.8. Suppose the event $E_{\\mathrm{init}}$ occurs. ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\frac{\\eta}{n}\\sum_{t=T_{\\mathrm{ERM}}}^{T^{*}}\\sum_{i\\in\\mathcal{V}_{s^{*},k^{*}}}\\ell\\left(y_{i}f_{W^{(t)}}(X_{i})\\right)\\leq\\left\\Vert Q\\left(W^{(T_{\\mathrm{ERM}})}\\right)-Q(W^{*})\\right\\Vert^{2}+2\\eta T^{*}e^{-\\frac{M\\beta}{4}},\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $\\lVert\\cdot\\rVert$ denotes the Frobenius norm. ", "page_idx": 37}, {"type": "text", "text": "Proof of Lemma C.8. Note that for any $T_{\\mathrm{ERM}}\\leq t<T^{*}$ \uff0c ", "page_idx": 37}, {"type": "equation", "text": "$$\nQ\\left(\\pmb{W}^{(t+1)}\\right)=Q\\left(\\pmb{W}^{(t)}\\right)-\\frac{\\eta}{n}\\nabla_{\\pmb{W}}\\sum_{i\\in\\mathcal{V}_{s^{*},k^{*}}}\\ell\\left(y_{i}f_{\\pmb{W}^{(t)}}(\\pmb{X}_{i})\\right),\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "and thus ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\left\\|Q\\left({\\pmb W}^{(t)}\\right)-Q\\left({\\pmb W}^{*}\\right)\\right\\|^{2}-\\left\\|Q\\left({\\pmb W}^{(t+1)}\\right)-Q\\left({\\pmb W}^{*}\\right)\\right\\|^{2}}}\\\\ {{\\displaystyle=\\frac{2\\eta}{n}\\left\\langle\\nabla_{{\\pmb W}}\\sum_{i\\in V_{s^{*},k^{*}}}\\ell\\left(y_{i}f_{{\\pmb W}^{(t)}}({\\pmb X}_{i})\\right),Q\\left({\\pmb W}^{(t)}\\right)-Q\\left({\\pmb W}^{*}\\right)\\right\\rangle-\\frac{\\eta^{2}}{n^{2}}\\left\\|\\nabla_{{\\pmb W}}\\sum_{i\\in V_{s^{*},k^{*}}}\\ell\\left(y_{i}f_{{\\pmb W}^{(t)}}({\\pmb X}_{i})\\right)\\right\\|^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\displaystyle\\frac{2\\eta}{n}\\left\\langle\\nabla_{W}\\sum_{i\\in V_{s^{\\star},k^{\\star}}}\\ell(y_{i}f_{W^{(i)}}(X_{i})),Q\\left(W^{(i)}\\right)\\right\\rangle}\\\\ &{\\phantom{=}\\displaystyle-\\frac{2\\eta}{n}\\sum_{i\\in V_{s^{\\star},k^{\\star}}}\\ell^{\\prime}(y_{i}f_{W^{(i)}}(X_{i}))\\left\\langle\\nabla_{W^{y_{i}}}f_{W^{(i)}}(X_{i}),Q\\left(W^{*}\\right)\\right\\rangle-\\displaystyle\\frac{\\eta^{2}}{n^{2}}\\left\\|\\nabla_{W}\\sum_{i\\in V_{s^{\\star},k^{\\star}}}\\ell(y_{i}f_{W^{(i)}}(X_{i}))\\right\\|^{2}}\\\\ &{\\geq\\displaystyle\\frac{2\\eta}{n}\\left\\langle\\nabla_{W}\\sum_{i\\in V_{s^{\\star},k^{\\star}}}\\ell(y_{i}f_{W^{(i)}}(X_{i})),Q\\left(W^{(i)}\\right)\\right\\rangle}\\\\ &{\\phantom{=}\\displaystyle-\\frac{M\\beta\\eta}{n}\\sum_{i\\in V_{s^{\\star},k^{\\star}}}\\ell^{\\prime}(y_{i}f_{W^{(i)}}(X_{i}))-\\displaystyle\\frac{\\eta^{2}}{n^{2}}\\left\\|\\nabla_{W}\\sum_{i\\in V_{s^{\\star},k^{\\star}}}\\ell\\big(y_{i}f_{W^{(i)}}(X_{i})\\big)\\right\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where the last inequality is due to Lemma C.7. By the chain rule, we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\Biggl\\langle\\nabla_{W}\\displaystyle\\sum_{i\\in V_{s^{*},k^{*}}}\\ell(y_{i}f_{W^{(t)}}(X_{i})),Q\\left(W^{(t)}\\right)\\Biggr\\rangle}\\\\ &{=\\displaystyle\\sum_{i\\in V_{s^{*},k^{*}}}\\Bigg[\\ell^{\\prime}(y_{i}f_{W^{(t)}}(X_{i}))}\\\\ &{\\quad\\times\\displaystyle\\sum_{p\\in[P]}\\left(\\phi^{\\prime}\\left(\\left\\langle w_{s^{*}}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)\\left\\langle Q_{s^{*}}\\left(w_{s^{*}}^{(t)}\\right),x_{i}^{(p)}\\right\\rangle-\\phi^{\\prime}\\left(\\left\\langle w_{-s^{*}}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)\\left\\langle Q_{-s^{*}}\\left(w_{-s^{*}}^{(t)}\\right),x_{i}^{(p)}\\right\\rangle\\right)\\Bigg].}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "For each $s\\in\\{\\pm1\\}$ \uff0c $i\\in\\mathcal{V}_{s^{*},k^{*}}$ , and $p\\in[P]$ ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\left\\langle w_{s}^{(i)},x_{i}^{(j)}\\right\\rangle-\\left\\langle Q_{s}\\left(w_{s}^{(i)}\\right),x_{i}^{(j)}\\right\\rangle\\right|}\\\\ &{=\\left|\\left\\langle w_{s}^{(i)}-Q_{s}\\left(w_{s}^{(i)}\\right),x_{i}^{(j)}\\right\\rangle\\right|}\\\\ &{\\leq\\qquad\\qquad\\sum_{j\\in[N]}\\left|\\Biggl\\langle\\Biggl\\langle\\rho_{s}^{(i)}(j,q)\\frac{\\xi_{j}^{(i)}}{\\left\\|\\xi_{j}\\right\\|^{(j)}},x_{i}^{(j)}\\Biggr\\rangle\\right|}\\\\ &{\\qquad+\\alpha_{s}\\frac{1}{j\\left(1\\right)\\left(\\eta_{s}^{(i)}\\left\\|\\xi_{j}\\right\\|^{(j)}\\right)}\\left|\\left\\langle\\rho_{s}^{(j)}\\right\\|^{2}\\right\\rangle\\right|\\cdot}\\\\ &{\\qquad+\\alpha_{s}\\frac{\\rho_{s}^{(i)}\\left\\langle\\rho_{s}^{(i)}(j,\\widehat{p}_{j})\\right\\|\\xi_{j}^{(j)}\\right\\|^{2}-\\left|\\left\\langle\\eta_{s},x_{i}^{(j)}\\right\\rangle\\right|}{j\\left(1\\right)\\left(\\eta_{s}^{(i)}\\left\\|\\xi_{j}\\right\\|^{(j)}\\right)}}\\\\ &{\\qquad+\\alpha_{s}\\frac{1}{j\\left(\\eta_{s}^{(i)}\\left\\|\\xi_{j}\\right\\|^{(j)}\\right)}\\left|\\left\\langle\\xi_{j}^{(i)}\\right\\rangle\\right|^{2}\\left|\\left\\langle\\eta_{s-1,1},x_{i}^{(j)}\\right\\rangle\\right|}\\\\ &{\\leq\\tilde{\\mathcal{O}}\\left(\\eta\\mathcal{P}^{2-1}w_{s}\\sigma_{\\mathbb{P}}^{-1/2}+\\tilde{\\mathcal{O}}\\left(\\alpha^{2}\\beta^{-1}w_{\\sigma}^{-2}\\alpha_{s}^{-1}\\right)}\\\\ &{=\\sigma\\left(\\frac{1}{\\sqrt{\\eta_{s}\\}\\log(\\widehat{q})}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where the last inequality is due to Lemma C.1 and the event $E_{\\mathrm{init}}$ . By Lemma F.1, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{p\\in[P]}\\left(\\phi^{\\prime}\\left(\\left\\langle w_{s^{*}}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)\\left\\langle Q_{s^{*}}\\left(w_{s^{*}}^{(t)}\\right),x_{i}^{(p)}\\right\\rangle-\\phi^{\\prime}\\left(\\left\\langle w_{-s^{*}}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)\\left\\langle Q_{-s^{*}}\\left(w_{s^{*}}^{(t)}\\right),x_{i}^{(p)}\\right\\rangle\\right)}\\\\ &{\\displaystyle\\leq\\sum_{p\\in[P]}\\left(\\phi\\left(\\left\\langle w_{s^{*}}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-s^{*}}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)\\right)+r P+o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right)}\\\\ &{\\displaystyle=y_{i}f_{W^{(t)}}(X_{i})+o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where the last equality is due to $\\begin{array}{r}{r=o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right)}\\end{array}$ ( polyog(a) . Therefore, we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\left\\|Q\\left(\\mathbf{W}^{(t)}\\right)-Q\\left(\\mathbf{W}^{*}\\right)\\right\\|^{2}-\\left\\|Q\\left(\\mathbf{W}^{(t+1)}\\right)-Q(\\mathbf{W}^{*})\\right\\|^{2}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\geq\\displaystyle\\frac{2\\eta}{n}\\sum_{i\\in\\mathcal{V}_{s^{\\star},\\star}}\\ell^{\\prime}\\left(y_{i}f_{W^{(i)}}(X_{i})\\right)\\left(y_{i}f_{W^{(i)}}(X_{i})+o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right)-\\frac{M\\beta}{2}\\right)}\\\\ &{\\quad-\\displaystyle\\frac{\\eta^{2}}{n^{2}}\\left\\|\\nabla_{W}\\sum_{i\\in\\mathcal{V}_{s^{\\star},\\star}}\\ell(y_{i}f_{W^{(i)}}(X_{i}))\\right\\|^{2}}\\\\ &{\\geq\\displaystyle\\frac{2\\eta}{n}\\sum_{i\\in\\mathcal{V}_{s^{\\star},\\star}}\\ell^{\\prime}(y_{i}f_{W^{(i)}}(X_{i}))\\left(y_{i}f_{W^{(i)}}(X_{i})-\\frac{M\\beta}{4}\\right)}\\\\ &{\\quad-\\displaystyle\\frac{\\eta^{2}}{n^{2}}\\left\\|\\nabla_{W}\\sum_{i\\in\\mathcal{V}_{s^{\\star},\\star}}\\ell(y_{i}f_{W^{(i)}}(X_{i}))\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "From the convexity of $\\ell(\\cdot)$ ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i\\in\\mathcal{V}_{s^{*},k^{*}}}\\ell^{\\prime}(y_{i}f_{W^{(t)}}(X_{i}))\\left(y_{i}f_{W^{(t)}}(X_{i})-\\frac{M\\beta}{4}\\right)\\geq\\displaystyle\\sum_{i\\in\\mathcal{V}_{s^{*},k^{*}}}\\left(\\ell(y_{i}f_{W^{(t)}}(X_{i}))-\\ell\\left(\\frac{M\\beta}{4}\\right)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\geq\\displaystyle\\sum_{i\\in\\mathcal{V}_{s^{*},k^{*}}}\\ell\\big(y_{i}f_{W^{(t)}}(X_{i})\\big)-n e^{-\\frac{M\\beta}{4}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "In addition, by Lemma F.2, ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\frac{\\eta^{2}}{n^{2}}\\left\\|\\nabla\\sum_{i\\in\\mathcal{V}_{s^{*},k^{*}}}\\ell\\left(y_{i}f_{W^{(t)}}(X_{i})\\right)\\right\\|^{2}\\leq\\frac{8\\eta^{2}P^{2}\\sigma_{\\mathrm{d}}^{2}d|\\mathcal{V}_{s^{*},k^{*}}|}{n^{2}}\\sum_{i\\in\\mathcal{V}_{s^{*},k^{*}}}\\ell\\big(y_{i}f_{W^{(t)}}(X_{i})\\big)}\\\\ &{\\leq\\displaystyle\\frac{\\eta}{n}\\sum_{i\\in\\mathcal{V}_{s^{*},k^{*}}}\\ell\\big(y_{i}f_{W^{(t)}}(X_{i})\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where the last inequality is due to (A8), and we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|Q\\left({{W^{(t)}}}\\right)-Q({{W^{*}}})\\right\\|^{2}-\\left\\|Q\\left({{W^{(t+1)}}}\\right)-Q({{W^{*}}})\\right\\|^{2}}\\\\ &{\\geq\\displaystyle\\frac{\\eta}{n}\\sum_{i\\in\\mathcal{V}_{s^{*},k^{*}}}\\ell(y_{i}f_{{W^{(t)}}}(X_{i}))-2\\eta e^{-\\frac{M\\beta}{4}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "From telescoping summation, we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\frac{\\eta}{n}\\sum_{t=T_{\\mathrm{ERM}}}^{T^{*}}\\sum_{i\\in\\mathcal{V}_{s^{*},k^{*}}}\\ell\\left(y_{i}f_{W^{(t)}}(X_{i})\\right)\\leq\\left\\Vert Q\\left(W^{(T_{\\mathrm{ERM}})}\\right)-Q\\left(W^{*}\\right)\\right\\Vert^{2}+2\\eta T^{*}e^{-\\frac{M\\beta}{4}}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Finally, we can prove that the model cannot learn (extremely) rare features within $T^{*}$ iterations. ", "page_idx": 39}, {"type": "text", "text": "Lemma C9. Suppose the event $E_{\\mathrm{init}}$ occurs. For any $T\\in[T_{\\mathrm{ERM}},T^{*}]$ we have $\\gamma_{s}^{(T)}(s^{*},k^{*})=$ $\\widetilde{\\mathcal{O}}\\left(\\alpha^{2}\\beta^{-2}\\right)$ for each $s\\in\\{\\pm1\\}$ ", "page_idx": 39}, {"type": "text", "text": "Proof of Lemma C.9. For any $T\\in[T_{\\mathrm{ERM}},T^{*}]$ , we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma_{s}^{(T)}(s,k)=\\gamma_{s}^{(T_{\\mathrm{ERM}})}(s^{*},k^{*})+\\frac{\\eta}{n}\\displaystyle\\sum_{t=T_{\\mathrm{ERM}}}^{T-1}\\sum_{i\\in\\mathcal{V}_{s^{*},k^{*}}}g_{i}^{(t)}\\phi^{\\prime}\\left(\\left\\langle\\mathbf{w}_{s}^{(t)},\\boldsymbol{v}_{s^{*},k^{*}}\\right\\rangle\\right)}\\\\ &{\\qquad\\qquad\\leq\\gamma_{s}^{(T_{\\mathrm{ERM}})}(s^{*},k^{*})+\\frac{\\eta}{n}\\displaystyle\\sum_{t=T_{\\mathrm{ERM}}}^{T-1}\\sum_{i\\in\\mathcal{V}_{s^{*},k^{*}}}g_{i}^{(t)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "equation", "text": "$$\n\\leq\\gamma_{s}^{(T_{\\mathrm{ERM}})}\\big(s^{*},k^{*}\\big)+\\frac{\\eta}{n}\\sum_{t=T_{\\mathrm{ERM}}}^{T-1}\\sum_{i\\in\\mathcal{V}_{s^{*},k^{*}}}\\ell\\left(y_{i}f_{W^{(t)}}\\big(\\boldsymbol{X}_{i}\\big)\\right),\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where the first inequality is due to $\\phi^{\\prime}\\leq1$ and the second inequality is due to $-\\ell^{\\prime}\\leq\\ell$ . From the result of Section C.2.3 we know $\\gamma_{s}^{(T_{\\mathrm{ERM}})}(s^{*},k^{*})\\leq\\alpha^{2}\\beta^{-1}$ . Additionally, by Lemma C.8 and Lemma C.6, we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{\\eta}{n}\\sum_{t=T_{\\mathrm{ERM}}}\\sum_{i\\in\\mathcal{V}_{s^{*},k^{*}}}\\ell\\left(y_{i}f_{W^{(t)}}(X_{i})\\right)\\leq\\frac{\\eta}{n}\\sum_{t=T_{\\mathrm{ERM}}}^{(T^{*})}i\\!\\sum_{i\\in\\mathcal{V}_{s^{*},k^{*}}}\\ell\\left(y_{i}f_{W^{(t)}}(X_{i})\\right)}}\\\\ &{\\leq\\left\\Vert Q\\left(W^{(T_{\\mathrm{ERM}})}\\right)-Q(W^{*})\\right\\Vert^{2}+2\\eta T^{*}e^{-\\frac{M\\beta}{4}}}\\\\ &{\\leq12M^{2}|\\gamma_{s^{*},k^{*}}|\\sigma_{\\mathrm{d}}^{-2}d^{-1}+2\\eta T^{*}e^{-\\frac{M\\beta}{4}}}\\\\ &{=\\widetilde{\\mathcal{O}}\\left(\\alpha^{2}\\beta^{-2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "The last line is due to (A5) and our choice $\\begin{array}{r}{M=4\\beta^{-1}\\log\\left(\\frac{2\\eta\\beta^{2}T^{*}}{\\alpha^{2}}\\right)}\\end{array}$ . Thus, we have our conclusion. ", "page_idx": 40}, {"type": "text", "text": "What We Have So Far. Suppose the event $E_{\\mathrm{init}}$ occurs. For any $t\\in[T_{\\mathrm{ERM}},T^{*}]$ ,we have ", "page_idx": 40}, {"type": "text", "text": "\u00b7 (Learn common features): For each $s\\in\\{\\pm1\\}$ and $k\\in\\kappa_{C}$ \uff0c ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\gamma_{s}^{(t)}(s,k)+\\beta\\gamma_{-s}^{(t)}(s,k)=\\Omega(1).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "\u00b7 (Overfit (extremely) rare data): For each $s\\in\\{\\pm1\\},k\\in\\mathcal{K}_{R}\\cup\\mathcal{K}_{E}$ and $i\\in\\mathcal{V}_{s,k}$ \uff0c", "page_idx": 40}, {"type": "equation", "text": "$$\n\\sum_{p\\in[P]\\backslash\\{p_{i}^{*}\\}}\\left(\\rho_{s}^{(t)}(i,p)+\\beta\\rho_{-s}^{(t)}(i,p)\\right)=\\Omega(1).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "\u00b7 (Cannot learn (extremely) rare features): For each $s\\in\\{\\pm1\\}$ and $k\\in K_{R}\\cup K_{E}$ ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\gamma_{s}^{(t)}(s,k),\\gamma_{-s}^{(t)}(s,k)=\\mathcal{O}\\left(\\alpha^{2}\\beta^{-2}\\right).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "\u00b7 For any $s\\in\\{\\pm1\\},i\\in[n]$ , and $p\\in[P]\\backslash\\{p_{i}^{*}\\},\\rho_{s}^{(t)}(i,p)=\\widetilde{\\mathcal{O}}\\left(\\beta^{-1}\\right)\\!,$ ", "page_idx": 40}, {"type": "text", "text": "C.2.5  Train and Test Accuracy ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "In this step, we will prove that the model trained by ERM has perfect training accuracy but has near-random guesses on (extremely) rare data. ", "page_idx": 40}, {"type": "text", "text": "For any $i\\in\\mathcal{V}_{s,k}$ with $s\\in\\{\\pm1\\}$ and $k\\in\\kappa_{C}$ , by Lemma B.4, we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{p\\in[P]}{\\operatorname{\\mathbb{\\sum}}}\\left(\\phi\\left(\\left\\langle w_{s}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-s}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)\\right)}\\\\ &{=\\underset{p\\in[P]}{\\sum}\\left(\\phi\\left(\\left\\langle w_{s}^{(t)},x_{i}^{(p)}(s,k)+\\underset{p\\in[P]\\backslash\\{r_{i}^{(t)}\\}}{\\sum}\\left(\\rho_{s}^{(t)}(i,p)+\\beta\\rho_{-s}^{(t)}(i,p)\\right)-2P\\cdot o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right)\\right.\\right.}\\\\ &{\\left.\\left.\\geq\\gamma_{s}^{(t)}(s,k)+\\beta\\gamma_{-s}^{(t)}(s,k)-o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right)\\right.\\right.}\\\\ &{=\\Omega(1)-o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right)}\\\\ &{>0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "for any $t\\in[T_{\\mathrm{ERM}},T^{*}]$ . In addition, for any $i\\in\\mathcal{V}_{s,k}$ With $s\\in\\{\\pm1\\}$ and $k\\in K_{R}\\cup K_{E}$ ,wehave $y_{i}f_{\\mathbf{W}^{(t)}}\\left(\\mathbf{X}_{i}\\right)$ ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\displaystyle\\sum_{p\\in[P]}\\left(\\phi\\left(\\left\\langle w_{s}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-s}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)\\right)}\\\\ &{=\\displaystyle\\gamma_{s}^{(t)}(s,k)+\\beta\\gamma_{-s}^{(t)}(s,k)+\\displaystyle\\sum_{p\\in[P]\\backslash\\{p_{i}^{*}\\}}\\left(\\rho_{s}^{(t)}(i,p)+\\beta\\rho_{-s}^{(t)}(i,p)\\right)-2P\\cdot o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right)}\\\\ &{\\geq\\displaystyle\\sum_{p\\in[P]\\backslash\\{p_{i}^{*}\\}}\\left(\\rho_{s}^{(t)}(i,p)+\\beta\\rho_{-s}^{(t)}(i,p)\\right)-o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right)}\\\\ &{=\\Omega(1)-o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "for any $t\\in[T_{\\mathrm{ERM}},T^{*}]$ . We can conclude that ERM with $t\\in[T_{\\mathrm{ERM}},T^{*}]$ iterates achieve perfect training accuracy. ", "page_idx": 41}, {"type": "text", "text": "Next, let us move on to the test accuracy part. Let $(X,y)\\;\\sim\\;{\\mathcal{D}}$ be a test data with $\\textbf{\\em X}=$ $\\left(\\pmb{x}^{(1)},\\ldots,\\pmb{x}^{(P)}\\right)\\in\\mathbb{R}^{d\\times P}$ having feature patch index $p^{*}$ , dominant noise patch index $\\tilde{p}$ and feature vector $\\pmb{v}_{y,k}$ We have $\\mathbf{x}^{(p)}\\sim N(\\mathbf{0},\\sigma_{\\mathrm{b}}^{2}\\mathbf{A})$ for each $p\\in[P]\\setminus\\{p^{*},\\tilde{p}\\}$ and $\\pmb{x}^{(\\tilde{p})}-\\alpha\\pmb{v}_{s,1}\\sim N(\\mathbf{0},\\sigma_{\\mathrm{d}}^{2}\\pmb{\\Lambda})$ for some $s\\in\\{\\pm1\\}$ . Therefore, for all $t\\in[T_{\\mathrm{ERM}},T^{*}]$ and $p^{'}\\in[P]\\setminus\\{p^{*},\\tilde{p}\\}$ \uff0c ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left|\\phi\\left(\\left\\langle w_{1}^{(t)},x^{(p)}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-1}^{(t)},x^{(p)}\\right\\rangle\\right)\\right|}\\\\ &{\\leq\\left|\\left\\langle w_{1}^{(t)}-w_{-1}^{(t)},x^{(p)}\\right\\rangle\\right|}\\\\ &{\\leq\\left|\\left\\langle w_{1}^{(0)}-w_{-1}^{(0)},x^{(p)}\\right\\rangle\\right|+\\displaystyle\\sum_{\\substack{i\\in[n],q\\in[P]\\setminus\\{p_{i}^{*}\\}}}\\left|\\rho_{1}^{(t)}(i,q)-\\rho_{-1}^{(t)}(i,q)\\right|\\frac{\\left|\\left\\langle\\xi_{i}^{(q)},x^{(p)}\\right\\rangle\\right|}{\\left\\|\\xi_{i}^{(q)}\\right\\|^{2}}}\\\\ &{\\leq\\tilde{\\mathcal{O}}\\left(\\sigma_{0}\\sigma_{\\mathrm{b}}d^{\\frac{1}{2}}\\right)+\\tilde{\\mathcal{O}}\\left(n P\\beta^{-1}\\sigma_{\\mathrm{d}}\\sigma_{\\mathrm{b}}^{-1}d^{-\\frac{1}{2}}\\right)}\\\\ &{=o\\left(\\frac{\\alpha}{\\mathrm{polylog}(d)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Wwith probability t east $\\begin{array}{r}{1-o\\left(\\frac{1}{\\mathrm{poly}(d)}\\right)}\\end{array}$ due to Lma2, (A8), (8), and (9) Iaddition,for any $s^{\\prime}\\in\\{\\pm1\\}$ , we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left|\\left\\langle w_{s^{\\prime}}^{(t)},x^{(\\bar{p})}-\\alpha v_{s,1}\\right\\rangle\\right|}\\\\ &{\\le\\left|\\left\\langle w_{s^{\\prime}}^{(0)},x^{(\\bar{p})}-\\alpha v_{s,1}\\right\\rangle\\right|+\\displaystyle\\sum_{i\\in[n],q\\in[P]\\backslash\\{p_{i}^{*}\\}}\\rho_{s^{\\prime}}^{(t)}(i,q)\\displaystyle\\frac{\\left|\\left\\langle\\xi_{i}^{(q)},x^{(\\bar{p})}-\\alpha v_{s,1}\\right\\rangle\\right|}{\\left\\|\\xi_{i}^{(q)}\\right\\|^{2}}}\\\\ &{=\\widetilde{O}\\left(\\sigma_{0}\\sigma_{\\mathrm{d}}d^{\\frac{1}{2}}\\right)+\\widetilde{O}\\left(n P\\beta^{-1}\\sigma_{\\mathrm{d}}\\sigma_{\\mathrm{b}}^{-1}d^{-\\frac{1}{2}}\\right)}\\\\ &{=o\\left(\\displaystyle\\frac{\\alpha}{\\mathrm{polylog}(d)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "with probability at least $\\textstyle1-o\\left({\\frac{1}{\\operatorname{poly}(d)}}\\right)$ due to Lemma B.2, (A8), (8), and (9). ", "page_idx": 41}, {"type": "text", "text": "Case 1: $k\\in\\kappa_{C}$ ", "page_idx": 41}, {"type": "text", "text": "By Lemma B.2, (A8), and (10), ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left|\\phi\\left(\\left\\langle w_{1}^{(t)},x^{(\\tilde{p})}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-1}^{(t)},w^{(\\tilde{p})}\\right\\rangle\\right)\\right|}\\\\ &{\\leq\\left|\\left\\langle w_{1}^{(t)}-w_{-1}^{(t)},x^{(\\tilde{p})}\\right\\rangle\\right|}\\\\ &{\\leq\\alpha\\left|\\left\\langle w_{1}^{(t)}-w_{-1}^{(t)},v_{s,1}\\right\\rangle\\right|+\\left|\\left\\langle w_{1}^{(t)}-w_{-1}^{(t)},x^{(p)}-\\alpha v_{s,1}\\right\\rangle\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\le\\alpha\\left(\\gamma_{1}^{(t)}(s,1)+\\gamma_{-1}^{(t)}(s,1)\\right)+\\alpha\\left|\\left\\langle w_{1}^{(0)},v_{s,1}\\right\\rangle\\right|+\\alpha\\left|\\left\\langle w_{-1}^{(0)},v_{s,1}\\right\\rangle\\right|+o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right)}\\\\ &{\\le\\widetilde{O}\\left(\\alpha\\beta^{-1}\\right)+\\widetilde{O}\\left(\\alpha\\sigma_{0}\\right)+o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right)}\\\\ &{=o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "with probability at least $\\textstyle1-o\\left({\\frac{1}{\\operatorname{poly}(d)}}\\right)$ . Suppose (18) and (20) holds. By Lemma B.4, we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad y f_{W^{(t)}}(X)}\\\\ &{=\\left(\\phi\\left(\\left\\langle w_{y}^{(t)},v_{y,k}\\right\\rangle\\right)\\ -\\phi\\left(\\left\\langle w_{-y}^{(t)},v_{y,k}\\right\\rangle\\right)\\right)}\\\\ &{\\quad+\\displaystyle\\sum_{p\\in[P]\\backslash\\{p^{*}\\}}\\left(\\phi\\left(\\left\\langle w_{y}^{(t)},x^{(p)}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-y}^{(t)},x^{(p)}\\right\\rangle\\right)\\right)}\\\\ &{=\\displaystyle\\gamma_{y}^{(t)}(y,k)+\\beta\\gamma_{-y}^{(t)}(y,k)-o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right)}\\\\ &{=\\Omega(1)-o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right)}\\\\ &{>0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Therefore, we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{P}_{(X,y)\\sim\\mathcal{D}}\\left[y f_{W^{(t)}}(X)>0\\mid x^{(p^{*})}=v_{y,k},k\\in K_{C}\\right]\\geq1-o\\left(\\frac{1}{\\mathrm{poly}(d)}\\right).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Case 2: $k\\in K_{R}\\cup K_{E}$ By triangular inequality and $\\phi^{\\prime}\\leq1$ ,wehave ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\phi\\left(\\left\\langle w_{s}^{(t)},x^{(\\tilde{p})}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-s}^{(t)},x^{(\\tilde{p})}\\right\\rangle\\right)}\\\\ &{=\\phi\\left(\\left\\langle w_{s}^{(t)},\\alpha v_{s,1}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-s}^{(t)},\\alpha v_{s,1}\\right\\rangle\\right)}\\\\ &{\\quad+\\left(\\phi\\left(\\left\\langle w_{s}^{(t)},x^{(\\tilde{p})}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{s}^{(t)},\\alpha v_{s,1}\\right\\rangle\\right)\\right)-\\left(\\phi\\left(\\left\\langle w_{-s}^{(t)},x^{(\\tilde{p})}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-s}^{(t)},\\alpha v_{s,1}\\right\\rangle\\right)\\right)}\\\\ &{\\geq\\phi\\left(\\left\\langle w_{s}^{(t)},\\alpha v_{s,1}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-s}^{(t)},\\alpha v_{s,1}\\right\\rangle\\right)}\\\\ &{\\quad-\\left|\\left\\langle w_{s}^{(t)},x^{(\\tilde{p})}-\\alpha v_{s,1}\\right\\rangle\\right|-\\left|\\left\\langle w_{-s}^{(t)},x^{(\\tilde{p})}-\\alpha v_{s,1}\\right\\rangle\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "In addition, ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phi\\left(\\left\\langle w_{s}^{(t)},\\alpha v_{s,1}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-s}^{(t)},\\alpha v_{s,1}\\right\\rangle\\right)}\\\\ &{=\\left(\\phi\\left(x_{s}^{(t)}(s,1)\\right)-\\phi\\left(-x_{1}^{(t)}(s,1)\\right)\\right)}\\\\ &{\\quad+\\left(\\phi\\left(\\left\\langle w_{s}^{(t)},\\alpha v_{s,1}\\right\\rangle\\right)-\\phi\\left(x_{1}^{(t)}(s,1)\\right)\\right)}\\\\ &{\\quad-\\left(\\phi\\left(\\left\\langle w_{-s}^{(t)},\\alpha v_{s,1}\\right\\rangle\\right)-\\phi\\left(-x_{1}^{(t)}(s,1)\\right)\\right)}\\\\ &{\\geq\\left(\\phi\\left(x_{1}^{(t)}(s,1)\\right)-\\phi\\left(-x_{1}^{(t)}(s,1)\\right)\\right)}\\\\ &{\\quad-\\alpha\\left|\\left\\langle w_{s}^{(t)},v_{s,1}\\right\\rangle-\\gamma_{s}^{(t)}(s,1)\\right|-\\alpha\\left|\\left\\langle w_{s}^{(t)},v_{s,1}\\right\\rangle+\\gamma_{s}^{(t)}(s,1)\\right|}\\\\ &{=\\alpha\\left(\\gamma_{s}^{(t)}(s,1)+\\beta\\gamma_{s}^{(t)}(s,1)\\right)-\\alpha\\cdot\\left(\\frac{1}{\\Theta N}\\right\\Vert_{\\mathbf{S}^{(t)}}(d)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where the second equality is due to Lemma B.4 and (A8). If (19) holds, we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\phi\\left(\\left\\langle w_{s}^{(t)},\\boldsymbol{x}^{(\\tilde{p})}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-s}^{(t)},\\boldsymbol{x}^{(\\tilde{p})}\\right\\rangle\\right)=\\Omega(\\alpha)-o\\left(\\frac{\\alpha}{\\mathrm{polylog}(d)}\\right)=\\Omega(\\alpha).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Note that ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad y f_{W^{(t)}}(X)}\\\\ &{=\\phi\\left(\\left\\langle w_{y}^{(t)},v_{y,k}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-y}^{(t)},v_{y,k}\\right\\rangle\\right)+\\phi\\left(\\left\\langle w_{y}^{(t)},x^{(\\tilde{p})}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-y}^{(t)},x^{(\\tilde{p})}\\right\\rangle\\right)}\\\\ &{\\quad+\\displaystyle\\sum_{p\\in[P]\\backslash\\{p^{*},\\tilde{p}\\}}\\left(\\phi\\left(\\left\\langle w_{y}^{(t)},x^{(p)}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-y}^{(t)},x^{(p)}\\right\\rangle\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "and ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\phi\\left(\\left\\langle w_{y}^{(t)},v_{y,k}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-y}^{(t)},v_{y,k}\\right\\rangle\\right)\\right|}\\\\ &{\\quad+\\left|\\displaystyle{\\sum_{p\\in[p]\\backslash[p]\\in\\mathcal{P}_{r}^{(t)}}}\\left(\\phi\\left(\\left\\langle w_{y}^{(t)},x^{(p)}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-y}^{(t)},x^{(p)}\\right\\rangle\\right)\\right)\\right|}\\\\ &{\\leq\\left|\\left\\langle w_{y}^{(t)}-w_{-y}^{(t)},v_{y,k}\\right\\rangle\\right|+o\\left(\\displaystyle{\\sum_{p\\in\\mathcal{P}_{r}\\in\\mathcal{P}_{r}^{(t)}}}\\right)}\\\\ &{\\leq\\gamma_{1}^{(t)}(y,k)+\\gamma_{-1}^{(t)}(y,k)+\\left|\\left\\langle w_{0}^{(t)}-w_{-y}^{(t)},v_{y,k}\\right\\rangle\\right|+o\\left(\\displaystyle{\\frac{\\alpha}{\\mathrm{polylog}(d)}}\\right)}\\\\ &{\\leq O(\\alpha^{2}\\beta^{-2})+\\bar{O}(\\sigma_{0})+o\\left(\\displaystyle{\\frac{\\alpha}{\\mathrm{polylog}(d)}}\\right)}\\\\ &{=o\\left(\\displaystyle{\\frac{\\alpha}{\\mathrm{polylog}(d)}}\\right)}\\\\ &{\\leq\\phi\\left(\\left\\langle w_{0}^{(t)},w^{(p)}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-x}^{(t)},w^{(p)}\\right\\rangle\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where the first inequality is due to (18), second-to-last line is due to (A8), (8) and (10), and the last inequality is due to (22). Therefore, we have $y f_{W^{(t)}}(\\pmb{X})>0$ if $y=s$ .Otherwise, $y f_{W^{(t)}}(\\pmb{X})<0$ Therefore,wehave ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathbb{P}_{(X,y)\\sim\\mathcal{D}}\\left[y f_{W^{(t)}}(X)>0\\mid x^{(p^{*})}=v_{y,k},k\\in K_{R}\\cup K_{E}\\right]=\\frac{1}{2}\\pm o\\left(\\frac{1}{\\mathrm{poly}(d)}\\right).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Hence, combining (21) and (23) implies ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbb{P}_{({\\pmb X},{\\pmb y})\\sim{\\pmb\\mathcal{D}}}\\left[y f_{{\\pmb W}^{(t)}}({\\pmb X})>0\\right]=\\displaystyle\\sum_{k\\in\\mathcal{K}_{C}}\\rho_{k}+\\frac{1}{2}\\left(1-\\displaystyle\\sum_{k\\in\\mathcal{K}_{C}}\\rho_{k}\\right)\\pmb{\\pmb\\pmb{\\pmb\\alpha}}\\left(\\frac{1}{\\mathrm{poly}(d)}\\right)}\\\\ &{}&{=1-\\displaystyle\\frac{1}{2}\\sum_{k\\in\\mathcal{K}_{R}\\cup\\mathcal{K}_{E}}\\rho_{k}\\pmb{\\pmb{\\alpha}}\\left(\\frac{1}{\\mathrm{poly}(d)}\\right).\\quad\\quad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "D Proof for Cutout ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "In this section, we use gc $\\begin{array}{r}{g_{i,\\mathcal{C}}^{(t)}\\,:=\\,\\frac{1}{1+\\exp\\left(y_{i}f_{{\\pmb{W}}^{(t)}}({\\pmb{X}}_{i,\\mathcal{C}})\\right)}}\\end{array}$ for each data $i$ ${\\mathcal{C}}\\subset[P]$ with $|{\\mathcal{C}}|\\,=\\,C$ and iteration $t$ , for simplicity. ", "page_idx": 44}, {"type": "text", "text": "D.1Proof of Lemma B.3 for Cutout ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "For $s\\in\\{\\pm1\\}$ and iterate $t$ ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w_{s}^{(t+1)}-w_{s}^{(t)}}\\\\ &{=-\\eta\\nabla_{w_{s}}\\mathcal{L}_{\\mathrm{cutont}}\\left(W^{(t)}\\right)}\\\\ &{=\\frac{\\eta}{n}\\displaystyle\\sum_{i\\in[n]}s_{i}y_{i}\\mathbb{E}_{\\circ\\sim\\mathcal{N}_{C}}\\left[g_{i,c}^{(t)}\\!\\sum_{p\\notin C}\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)x_{i}^{(p)}\\right]}\\\\ &{=\\displaystyle\\frac{\\eta}{n}\\left(\\sum_{i\\in\\mathcal{V}_{s}}\\mathbb{E}_{\\circ\\sim\\mathcal{N}_{C}}\\left[g_{i,c}^{(t)}\\!\\sum_{p\\notin C}\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)x_{i}^{(p)}\\right]\\right.}\\\\ &{\\phantom{=\\displaystyle}\\left.-\\sum_{i\\in\\mathcal{V}_{s}}\\mathbb{E}_{\\circ\\sim\\mathcal{N}_{C}}\\left[g_{i,c}^{(t)}\\!\\sum_{p\\notin C}\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)x_{i}^{(p)}\\right]\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "and we have ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\sum_{i\\in\\mathcal{V}_{s}}\\mathbb{E}_{C\\sim\\mathcal{D}_{C}}\\left[g_{i,c}^{(i)}\\sum_{p\\neq c}\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)x_{i}^{(p)}\\right]}\\\\ &{=\\displaystyle\\sum_{k\\in\\mathbb{K}}\\sum_{i\\in\\mathcal{V}_{s},k}\\mathbb{E}_{C\\sim\\mathcal{D}_{C}}\\left[g_{i,c}^{(i)}\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(t)},w_{s,k}\\right\\rangle\\right)\\cdot\\mathbf{1}_{p_{i}^{*}\\notin\\mathcal{C}}\\right]v_{s,k}}\\\\ &{\\quad+\\displaystyle\\sum_{i\\in\\mathcal{V}_{s}}\\sum_{p\\in\\mathcal{D}_{C}}\\mathbb{E}_{C\\sim\\mathcal{D}_{C}}\\left[g_{i,c}^{(t)}\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(t)},\\xi_{i}^{(p)}\\right\\rangle\\right)\\cdot\\mathbf{1}_{p\\notin\\mathcal{C}}\\right]\\xi_{i}^{(p)}}\\\\ &{\\quad+\\displaystyle\\sum_{i\\in\\mathcal{V}_{s}\\cap\\mathcal{F}_{s}}\\mathbb{E}_{C\\sim\\mathcal{D}_{C}}\\left[g_{i,c}^{(t)}\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(t)},\\alpha v_{s,1}+\\xi_{i}^{(\\hat{p}_{i})}\\right\\rangle\\right)\\cdot\\mathbf{1}_{\\hat{p}_{i}\\notin\\mathcal{C}}\\right]\\left(\\alpha v_{s,1}+\\xi_{i}^{(\\hat{p}_{i})}\\right)}\\\\ &{\\quad+\\displaystyle\\sum_{i\\in\\mathcal{V}_{s}\\cap\\mathcal{F}_{s}}\\mathbb{E}_{C\\sim\\mathcal{D}_{C}}\\left[g_{i,c}^{(t)}\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(t)},\\alpha v_{s,1}+\\xi_{i}^{(\\hat{p}_{i})}\\right\\rangle\\right)\\cdot\\mathbf{1}_{\\hat{p}_{i}\\notin\\mathcal{C}}\\right]\\left(\\alpha v_{s,1}+\\xi_{i}^{(\\hat{p}_{i})}\\right)}\\\\ &{\\quad+\\displaystyle\\sum_{i\\in\\mathcal{V}_{s}\\cap\\mathcal{F}_{s}}\\mathbb{E}_{C\\sim\\mathcal{D}_{C}}\\left[g_{i,c}^{(t)}\\phi^{\\prime}\\left(\\left\\langle w_{s} \n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "and ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{i\\in V_{s}}{\\sum}\\underset{s\\in\\mathcal{N}_{C}}{\\mathbb{E}}\\left[\\underset{g_{i,c}^{(i)}\\sim\\mathcal{P}_{C}}{\\sum}\\left(\\varphi_{s}^{(i)}\\right)\\underset{x_{i}^{(i)}}{\\sum}\\varphi^{\\prime}\\left(\\left\\langle w_{s}^{(i)},x_{i}^{(p)}\\right\\rangle\\right)x_{i}^{(i)}\\right]}\\\\ &{=\\underset{k\\in\\mathbb{K}}{\\sum}\\underset{i\\in\\mathcal{N}_{A}}{\\sum}\\underset{s\\in\\mathcal{N}_{C}}{\\sum}\\left[g_{i,c}^{(i)}\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(i)},v_{-s,k}\\right\\rangle\\right)\\cdot\\mathbb{1}_{p_{i}^{*}\\notin\\mathcal{C}}\\right]v_{-s,k}}\\\\ &{\\quad+\\underset{i\\in V_{s}\\to\\mathcal{P}_{C}}{\\sum}\\underset{p\\in\\mathcal{N}_{C}}{\\sum}\\frac{\\mathbb{E}_{C\\sim\\mathcal{P}_{C}}\\left[g_{i,c}^{(i)}\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(i)},\\xi_{i}^{(p)}\\right\\rangle\\right)\\cdot\\mathbb{1}_{p\\notin\\mathcal{C}}\\right]\\xi_{i}^{(p)}}{\\sum}}\\\\ &{\\quad+\\underset{i\\in V_{s}\\to\\mathcal{P}_{C}}{\\sum}\\frac{\\mathbb{E}_{C\\sim\\mathcal{P}_{C}}\\left[g_{i,c}^{(i)}\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(i)},\\alpha v_{s,1}+\\xi_{i}^{(p)}\\right\\rangle\\right)\\cdot\\mathbb{1}_{\\Tilde{p}_{i}\\notin\\mathcal{C}}\\right]\\left(\\alpha v_{s,1}+\\xi_{i}^{(\\Tilde{p}_{i})}\\right)}{\\sum}}\\\\ &{\\quad+\\underset{i\\in V_{s}\\to\\mathcal{P}_{C}}{\\sum}\\frac{\\mathbb{E}_{C\\sim\\mathcal{P}_{C}}\\left[g_{i,c}^{(i)}\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(i)},\\alpha v_{-s,1}+\\xi_{i}^{(\\Tilde{p}_{i})}\\right\\rangle\\right)\\cdot\\mathbb{1}_{\\Tilde{p}_{i}\\notin\\mathcal{C}}\\right]\\left(\\alpha v_{s,1}+\\xi_{i}^{(\\Tilde{p}_{i})}\\right)}{\\sum}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Hence, if we define $\\gamma_{s}^{(t)}(s^{\\prime},k)$ 's and $\\rho_{s}^{(t)}(i,p)$ 's recursively by using the rule ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\gamma_{s}^{(t+1)}(s^{\\prime},k)=\\gamma_{s}^{(t)}(s^{\\prime},k)+\\frac{\\eta}{n}\\sum_{i\\in\\mathcal{V}_{s^{\\prime},k}}\\mathbb{E}_{\\mathcal{C}\\sim\\mathcal{D}_{\\mathcal{C}}}\\left[g_{i,\\mathcal{C}}^{(t)}\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(t)},v_{s^{\\prime},k}\\right\\rangle\\right)\\cdot\\mathbb{I}_{p_{i}^{*}\\notin\\mathcal{C}}\\right],\n$$", "text_format": "latex", "page_idx": 44}, {"type": "equation", "text": "$$\n\\rho_{s}^{(t+1)}(i,p)=\\rho_{s}^{(t)}(i,p)+\\frac{\\eta}{n}\\mathbb{E}_{\\mathcal{C}\\sim\\mathcal{D}_{c}}\\left[g_{i,\\mathcal{C}}^{(t)}\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)\\cdot\\mathbb{1}_{p\\notin\\mathcal{C}}\\right]\\left\\|\\xi_{i}^{(p)}\\right\\|^{2},\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "starting from $\\gamma_{s}^{(0)}(s^{\\prime},k)=\\rho_{s}^{(0)}(i,p)=0$ for each $s,s^{\\prime}\\in\\{\\pm1\\},k\\in[K],i\\in[n]$ and $p\\in[P]\\setminus\\{p_{i}^{*}\\}$ \uff0c then we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w_{s}^{(t)}=w_{s}^{(0)}+\\displaystyle\\sum_{k\\in[K]}\\gamma_{s}^{(t)}(s,k)v_{s,k}-\\displaystyle\\sum_{k\\in[K]}\\gamma_{s}^{(t)}(-s,k)v_{-s,k}}\\\\ &{\\quad\\quad+\\displaystyle\\sum_{i\\in\\mathcal{V}_{s},p\\in[P]\\backslash\\{p_{i}^{*}\\}}\\rho_{s}^{(t)}(i,p)\\displaystyle\\frac{\\xi_{i}^{(p)}}{\\left\\|\\xi_{i}^{(p)}\\right\\|^{2}}-\\displaystyle\\sum_{i\\in\\mathcal{V}_{-s},p\\in[P]\\backslash\\{p_{i}^{*}\\}}\\rho_{s}^{(t)}(i,p)\\displaystyle\\frac{\\xi_{i}^{(p)}}{\\left\\|\\xi_{i}^{(p)}\\right\\|^{2}}}\\\\ &{\\quad\\quad+\\alpha\\left(\\displaystyle\\sum_{i\\in\\mathcal{F}_{s}}s y_{i}\\rho_{s}^{(t)}(i,\\tilde{p}_{i})\\displaystyle\\frac{v_{s,1}}{\\left\\|\\xi_{i}^{(\\tilde{p}_{i})}\\right\\|^{2}}+\\displaystyle\\sum_{i\\in\\mathcal{F}_{-s}}s y_{i}\\rho_{s}^{(t)}(i,\\tilde{p}_{i})\\displaystyle\\frac{v_{-s,1}}{\\left\\|\\xi_{i}^{(\\tilde{p}_{i})}\\right\\|^{2}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "for each $s\\in\\{\\pm1\\}$ . Furthermore, $\\gamma_{s}^{(t)}(s^{\\prime},k)$ 's and $\\rho_{s}^{(t)}(i,p)$ 's are monotone increasing. ", "page_idx": 45}, {"type": "text", "text": "D.2  Proof of Theorem 3.2 ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "To show Theorem 3.2, we present a structured proof comprising the following five steps: ", "page_idx": 45}, {"type": "text", "text": "1. Establish upper bounds on $\\gamma_{s}^{(t)}(s^{\\prime},k)$ 's and $\\rho_{s}^{(t)}(i,p)$ 's to apply Lemma B.4 (Section D.2.1).   \n2. Demonstrate that the model quickly learns common and rare features (Section D.2.2).   \n3. Show that the model overfits augmented data if it does not contain common or rare features   \n(Section D.2.3).   \n4. Confirm the persistence of this tendency until $T^{*}$ iterates (Section D.2.4).   \n5. Characterize train accuracy and test accuracy (Section D.2.5). ", "page_idx": 45}, {"type": "text", "text": "D.2.1 Bounds on the Coefficients in Feature Noise Decomposition ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "The following lemma provides upper bounds on Lemma B.3 during $T^{*}$ iterations. ", "page_idx": 45}, {"type": "text", "text": "Lemma D.1. Suppose the event $E_{\\mathrm{init}}$ occurs. For any $0\\leq t\\leq T^{*}$ , we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n0\\leq\\gamma_{s}^{(t)}(s,k)+\\beta\\gamma_{-s}^{(t)}(s,k)\\leq4\\log(\\eta T^{\\ast}),\\quad0\\leq\\rho_{y_{i}}^{(t)}(i,p)+\\beta\\rho_{-y_{i}}^{(t)}(i,p)\\leq4\\log\\left(\\eta T^{\\ast}\\right),\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "for all $s\\in\\{\\pm1\\},k\\in[K],i\\in[n]$ and $p\\in[P]\\backslash\\{p_{i}^{*}\\}$ : Consequently, $\\gamma_{s}^{(t)}(s^{\\prime},k),\\rho_{s}^{(t)}(i,p)=\\widetilde{\\mathcal{O}}(\\beta^{-1})$ for all $s,s^{\\prime}\\in\\{\\pm1\\},k\\in[K],i\\in[n]$ and $p\\in[P]\\,\\backslash\\,\\{p_{i}^{*}\\}$ ", "page_idx": 45}, {"type": "text", "text": "Proof of Lemma $D.I$ . The first argument implies the second argument since $\\log(\\eta T^{*})=\\mathrm{polylog}(d)$ and ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\gamma_{s}^{(t)}(s^{\\prime},k)\\le\\beta^{-1}\\left(\\gamma_{s^{\\prime}}^{(t)}(s^{\\prime},k)+\\beta\\gamma_{s^{\\prime}}^{(t)}(s^{\\prime},k)\\right),\\quad\\rho_{s}^{(t)}(i,p)\\le\\beta^{-1}\\left(\\rho_{y_{i}}^{(t)}(i,p)+\\beta\\rho_{-y_{i}}^{(t)}(i,p)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "for all $s,s^{\\prime}\\in\\{\\pm1\\},k\\in[K],i\\in[n]$ and $p\\in[P]\\setminus\\{p_{i}^{*}\\}$ ", "page_idx": 45}, {"type": "text", "text": "We will prove the first argument by using induction on $t$ . The initial case $t=0$ is trivial. Suppose the statement holds at $t=T$ and consider the case $t=T+1$ ", "page_idx": 45}, {"type": "text", "text": "Let $\\tilde{T}_{s,k}\\leq T$ denote the smallest iteration where $\\gamma_{s}^{(\\tilde{T}_{s,k}+1)}(s,k)+\\beta\\gamma_{-s}^{(\\tilde{T}_{s,k}+1)}(s,k)>2\\log(\\eta T^{*})$ We assume the existence of $\\tilde{T}_{s,k}$ , as its absence would directly lead to our desired conclusion; to see why, note that the following holds, due to (24) and (11): ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{c l}{\\displaystyle\\gamma_{s}^{(T+1)}(s,k)+\\beta\\gamma_{-s}^{(T+1)}(s,k)}\\\\ {\\displaystyle=\\gamma_{s}^{(T)}(s,k)+\\beta\\gamma_{-s}^{(T)}(s,k)}\\\\ {\\displaystyle\\;\\;+\\frac{\\eta}{n}\\sum_{i\\in\\mathcal{V}_{s,k}}\\mathbb{E}_{\\mathcal{C}\\sim\\mathcal{D}_{\\mathcal{C}}}\\left[g_{i,\\mathcal{C}}^{(T)}\\cdot\\mathbb{1}_{p_{i}^{*}\\notin\\mathcal{C}}\\right]\\left(\\phi^{\\prime}\\left(\\left<w_{s}^{(T)},v_{s,k}^{}\\right>\\right)+\\beta\\phi^{\\prime}\\left(\\left<w_{-s}^{(T)},v_{s,k}^{}\\right>\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "equation", "text": "$$\n\\leq2\\log(\\eta T^{*})+2\\eta\\leq4\\log(\\eta T^{*})\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "By (24), we have ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\gamma_{s}^{(T+1)}(s,k)+\\beta\\gamma_{-s}^{(T+1)}(s,k)}\\\\ &{=\\gamma_{s}^{(\\Tilde{T}_{s,k})}(s,k)+\\beta\\gamma_{-s}^{(\\Tilde{T}_{s,k})}(s,k)}\\\\ &{\\quad+\\displaystyle\\sum_{t=\\Tilde{T}_{s,k}}^{T}\\left(\\gamma_{s}^{(t+1)}(s,k)+\\beta\\gamma_{-s}^{(t+1)}(s,k)-\\gamma_{s}^{(t)}(s,k)-\\beta\\gamma_{-s}^{(t)}(s,k)\\right)}\\\\ &{\\le2\\log(\\eta T^{*})+\\log(\\eta T^{*})}\\\\ &{\\quad+\\displaystyle\\frac{\\eta}{n}\\sum_{t=\\Tilde{T}_{s,k}+1}^{T}\\underbrace{\\mathbb{E}_{C\\sim\\mathcal{D}_{C}}\\left[g_{i,C}^{(t)}\\Big(\\phi^{\\prime}\\left(\\left<w_{s}^{(t)},v_{s,k}^{}\\right>\\right)+\\beta\\phi^{\\prime}\\left(\\left<w_{-s}^{(t)},v_{s,k}^{}\\right>\\right)\\Big)\\right.\\cdot\\mathbb{I}_{p_{i}^{*}\\notin\\mathcal{C}}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Th nequality is due to $\\gamma_{s}^{(\\tilde{T}_{s,k})}(s,k)+\\beta\\gamma_{-s}^{(\\tilde{T}_{s,k})}(s,k)\\le2\\log(\\eta T^{*})$ and ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\eta}{n}\\displaystyle\\sum_{i\\in\\mathcal{V}_{s,k}}\\mathbb{E}_{\\boldsymbol{C}\\sim\\mathcal{D}_{c}}\\left[g_{i,\\boldsymbol{C}}^{(\\tilde{T}_{s,k})}\\bigg(\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(\\tilde{T}_{s,k})},v_{s,k}\\right\\rangle\\right)+\\beta\\phi^{\\prime}\\left(\\left\\langle w_{-s}^{(\\tilde{T}_{s,k})},v_{s,k}\\right\\rangle\\right)\\bigg)\\cdot\\mathbb{1}_{p_{i}^{*}\\notin c}\\right]}\\\\ &{\\leq2\\eta\\leq\\log(\\eta T^{*}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "from our choice of $\\tilde{T}_{s,k}$ and $\\eta$ ", "page_idx": 46}, {"type": "text", "text": "For each $t=\\tilde{T}_{s,k}+1,\\ldots T,i\\in\\mathcal{V}_{s,k}$ and $\\mathcal{C}\\subset[P]$ such that $|{\\mathcal{C}}|=C$ and $p_{i}^{*}\\notin\\mathcal{C}$ , we have ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phantom{\\geq}\\displaystyle y_{i}J W^{(t)}\\lfloor\\mathbf{A}_{i,C}\\rfloor}\\\\ &{=\\phi\\left(\\left\\langle w_{s}^{(t)},v_{s,k}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-s}^{(t)},v_{s,k}\\right\\rangle\\right)+\\displaystyle\\sum_{p\\not\\in\\mathcal{C}\\cup\\{p_{i}^{*}\\}}\\left(\\phi\\left(\\left\\langle w_{s}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-s}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)\\right)}\\\\ &{\\geq\\displaystyle\\gamma_{s}^{(t)}(s,k)+\\beta\\gamma_{-s}^{(t)}(s,k)+\\displaystyle\\sum_{p\\not\\in\\mathcal{C}\\cup\\{p_{i}^{*}\\}}\\left(\\rho_{s}^{(t)}(i,p)+\\beta\\rho_{-s}^{(t)}(i,p)\\right)-2P\\cdot o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right)}\\\\ &{\\geq\\displaystyle\\frac{3}{2}\\log(\\eta T^{*})}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "The first inequality is due to Lemma B.4 and the second inequality holds due to (A7), (8), and our choice of $t$ $\\bar{\\gamma}_{s}^{(t)}(s,k)+\\beta\\gamma_{-s}^{(t)}(s,k)\\geq2\\log(\\eta T^{*})$ ", "page_idx": 46}, {"type": "text", "text": "Hence, we obtain ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\eta}{n}\\displaystyle\\sum_{t=\\bar{T}_{s,k}}^{T}\\sum_{i\\in\\mathcal{V}_{s,k}}\\mathbb{E}_{c\\sim\\mathcal{D}_{c}}\\left[g_{i,c}^{(t)}\\bigg(\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(t)},v_{s,k}\\right\\rangle\\right)+\\beta\\phi^{\\prime}\\left(\\left\\langle w_{-s}^{(t)},v_{s,k}\\right\\rangle\\right)\\bigg)\\cdot\\mathbb{1}_{p_{i}^{*}\\notin C}\\right]}\\\\ &{\\le\\frac{2\\eta}{n}\\displaystyle\\sum_{t=\\bar{T}_{s,k}}^{T}\\sum_{i\\in\\mathcal{V}_{s,k}}\\mathbb{E}_{c\\sim\\mathcal{D}_{c}}\\left[\\exp\\left(-y_{i}f_{W^{(t)}}(X_{i,c})\\right)\\cdot\\mathbb{1}_{p_{i}^{*}\\notin C}\\right]}\\\\ &{\\le\\frac{2|\\mathcal{V}_{s,k}|}{n}(\\eta T^{*})\\exp\\left(-\\frac{3}{2}\\log(\\eta T^{*})\\right)}\\\\ &{\\le\\displaystyle\\frac{2}{\\sqrt{\\eta T^{*}}}\\le\\log(\\eta T^{*}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where the last inequality holds for any reasonably large $T^{*}$ . Merging all inequalities together, we have $\\gamma_{s}^{(T+1)}(s,k)+\\beta\\gamma_{-s}^{(T+1)}(s,k)\\dot{\\leq}4\\log(\\eta T^{*})$ ", "page_idx": 46}, {"type": "text", "text": "Next, we will follow similar arguments to show that ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\rho_{y_{i}}^{(T+1)}(i,p)+\\beta\\rho_{-y_{i}}^{(T+1)}(i,p)\\leq4\\log(\\eta T^{*})\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "for each $i\\in[n]$ and $p\\in[P]\\setminus\\{p_{i}^{*}\\}$ ", "page_idx": 46}, {"type": "text", "text": "Let T(p)< T be the smallest iteration such that pyi\\* $\\rho_{y_{i}}^{(\\tilde{T}_{i}^{(p)}+1)}(i,p)+\\beta\\rho_{-y_{i}}^{(\\tilde{T}_{i}^{(p)}+1)}(i,p)>2\\log(\\eta T^{*}).$ We assume the existence of $\\tilde{T}_{i}^{(p)}$ , , as its absence would directly lead to our desired conclusion; to see why, note that the following holds, due to (25) and (11): ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\rho_{y_{i}}^{(T+1)}(i,p)+\\beta\\rho_{-y_{i}}^{(T+1)}(i,p)}\\\\ &{=\\rho_{y_{i}}^{(T)}(i,p)+\\beta\\rho_{-y_{i}}^{(T)}(i,p)}\\\\ &{\\quad+\\frac{\\eta}{n}\\mathbb{E}_{\\mathcal{C}\\sim\\mathcal{D}_{c}}\\left[g_{i,\\mathcal{C}}^{(T)}\\cdot\\mathbb{1}_{p\\not\\in\\mathcal{C}}\\right]\\left(\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)+\\beta\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)\\right)\\left\\Vert\\xi_{i}^{(p)}\\right\\Vert^{2}}\\\\ &{\\leq2\\log(\\eta T^{*})+2\\eta\\leq4\\log(\\eta T^{*}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "where the first inequality is due to $\\begin{array}{r}{\\left\\|\\xi_{i}^{(p)}\\right\\|\\leq\\frac{3}{2}\\sigma_{\\mathrm{d}}^{2}d}\\end{array}$ and (A4), and the last inequalityis due to (11). Now we suppose there exists such $\\tilde{T}_{i}\\leq T$ . By (25), we have ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~\\rho_{y_{i}}^{(T+1)}(i,p)+\\beta\\rho_{-y_{i}}^{(T+1)}(i,p)}\\\\ &{=\\rho_{y_{i}}^{(T(p))}(i,p)+\\beta\\rho_{-y_{i}}^{(T(p))}(i,p)}\\\\ &{~~+\\displaystyle\\sum_{t=\\bar{T}_{i}^{(p)}}^{T}\\left(\\rho_{y_{i}}^{(t+1)}(i,p)+\\beta\\rho_{-y_{i}}^{(t+1)}(i,p)-\\rho_{y_{i}}^{(t)}(i,p)-\\beta\\rho_{-y_{i}}^{(t)}(i,p)\\right)}\\\\ &{\\le2\\log(\\eta T^{*})+\\log(\\eta T^{*})}\\\\ &{~~+\\displaystyle\\frac{\\eta}{n}\\sum_{t=\\bar{T}_{i}^{(p)}+1}^{T}\\mathbb{E}_{C\\sim\\mathcal{D}_{c}}\\left[g_{i,C}^{(t)}\\cdot\\mathbb{I}_{p\\notin\\mathcal{C}}\\right]\\left(\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)+\\beta\\phi^{\\prime}\\left(\\left\\langle w_{-s}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)\\right)\\left\\Vert\\xi_{i}^{(p)}\\right\\Vert^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "The inequality is due to $\\rho_{y_{i}}^{(t)}(i,p)+\\beta\\rho_{-y_{i}}^{(t)}(i,p)\\leq2\\log(\\eta T^{*})$ our choice of ${\\tilde{T}}_{i}^{\\left(p\\right)}$ and ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\eta}{n}\\mathbb{E}_{\\mathcal{C}\\sim\\mathcal{D}_{c}}\\left[g_{i,\\mathcal{C}}^{(\\tilde{T}_{i}^{(p)})}\\cdot\\mathbb{1}_{p\\notin\\mathcal{C}}\\right]\\left(\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(\\tilde{T}_{i}^{(p)})},x_{i}^{(p)}\\right\\rangle\\right)+\\beta\\phi^{\\prime}\\left(\\left\\langle w_{-s}^{(\\tilde{T}_{i}^{(p)})},x_{i}^{(p)}\\right\\rangle\\right)\\right)\\left\\Vert\\xi_{i}^{(p)}\\right\\Vert^{2}}\\\\ &{\\leq2\\eta\\leq\\log(\\eta T^{*}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "from $\\begin{array}{r}{\\left\\|\\xi_{i}^{(p)}\\right\\|^{2}\\leq\\frac{3}{2}\\sigma_{\\mathrm{d}}^{2}d,}\\end{array}$ (A4), and (11). ", "page_idx": 47}, {"type": "text", "text": "For each $t=\\tilde{T}_{i}^{(p)}+1,\\ldots,T$ and $\\mathcal{C}\\subset[P]$ such that $|{\\mathcal{C}}|=C$ and $p\\not\\in{\\mathcal{C}}$ we have ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad y_{i}f_{W^{(t)}}\\bigl(X_{i,C}\\bigr)}\\\\ &{=\\phi\\left(\\left\\langle w_{y_{i}}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-y_{i}}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)+\\displaystyle\\sum_{q\\notin C\\cup\\{p\\}}\\left(\\phi\\left(\\left\\langle w_{y_{i}}^{(t)},x_{i}^{(q)}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-y_{i}}^{(t)},x_{i}^{(q)}\\right\\rangle\\right)\\right)}\\\\ &{\\geq\\rho_{y_{i}}^{(t)}(i,p)+\\beta\\rho_{-y_{i}}^{(t)}(i,p)-2P\\cdot o\\left(\\displaystyle\\frac{1}{\\mathrm{polylog}(d)}\\right)}\\\\ &{\\geq\\frac{3}{2}\\log(\\eta T^{*}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "The first inequality is due to Lemma B.4 and the second inequality holds since from our choice of $t$   \n$\\rho_{y_{i}}^{(t)}(i,p)+\\bar{\\beta}\\rho_{-y_{i}}^{(t)}(i,p)\\geq2\\log(\\eta T^{*})$   \nTharaforawe hova ", "page_idx": 47}, {"type": "text", "text": "", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\eta}{n}\\displaystyle\\sum_{t=\\tilde{T}_{i}^{(p)}+1}^{T}\\mathbb{E}_{\\mathcal{C}\\sim\\mathcal{D}_{\\mathcal{C}}}\\left[g_{i,\\mathcal{C}}^{(t)}\\cdot\\mathbb{1}_{p\\not\\in\\mathcal{C}}\\right]\\left(\\phi^{\\prime}\\left(\\left\\langle w_{y_{i}}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)+\\beta\\phi^{\\prime}\\left(\\left\\langle w_{-y_{i}}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)\\right)\\Big\\Vert\\xi_{i}^{(p)}\\Big\\Vert^{2}}\\\\ &{\\leq\\eta\\displaystyle\\sum_{t=\\tilde{T}_{i}^{(p)}+1}^{T}\\mathbb{E}_{\\mathcal{C}\\sim\\mathcal{D}_{\\mathcal{C}}}\\left[\\exp\\left(-y_{i}f_{W^{(t)}}(X_{i,\\mathcal{C}})\\right)\\mathbb{1}_{p\\not\\in\\mathcal{C}}\\right]\\leq(\\eta T^{*})\\exp\\left(-\\frac{3}{2}\\log(\\eta T^{*})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "equation", "text": "$$\n\\leq\\frac{1}{\\sqrt{\\eta T^{*}}}\\leq\\log(\\eta T^{*}),\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Wwherethe frst inequality is dueto $\\begin{array}{r}{\\left\\|\\xi_{i}^{(p)}\\right\\|^{2}\\leq\\frac{3}{2}\\sigma_{\\mathrm{d}}^{2}d}\\end{array}$ and (A4).Hence,weconclude $\\rho_{y_{i}}^{(T+1)}(i,p)+$ $\\beta\\rho_{-y_{i}}^{(T+1)}(i,p)\\leq4\\log(\\eta T^{*}).$ ", "page_idx": 48}, {"type": "text", "text": "D.2.2Learning Common Features and Rare Features ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "In the initial stages of training, the model quickly learns common features while exhibiting minimal Overfitting to Gaussian noise. ", "page_idx": 48}, {"type": "text", "text": "First, we establish lower bounds on the number of iterations, ensuring that background noise coeffcients $\\rho_{s}^{(t)}(i,p)$ for $p\\neq p_{i}^{*},\\tilde{p}_{i}$ remain small, up to the order of $\\textstyle{\\frac{1}{P}}$ ", "page_idx": 48}, {"type": "text", "text": "Lema D.2. suppse he vent $E_{\\mathrm{init}}$ occurs There exiss $\\begin{array}{r}{\\tilde{T}>\\frac{n}{6\\eta P\\sigma_{\\mathrm{b}}^{2}d}}\\end{array}$ such hat $\\begin{array}{r}{\\rho_{s}^{(t)}(i,p)\\leq\\frac{1}{4P}}\\end{array}$ for all $0\\leq t<\\tilde{T},s\\in\\{\\pm1\\},i\\in[n]$ and $p\\in[P]\\setminus\\{p_{i}^{*},\\tilde{p}_{i}\\}$ ", "page_idx": 48}, {"type": "text", "text": "Proof of Lemma $D.2$ Let $\\tilde{T}$ be the smallest iteration such that $\\begin{array}{r}{\\rho_{s}^{(\\tilde{T})}(i,p)\\ \\geq\\ \\frac{1}{4P}}\\end{array}$ for some $s\\ \\in$ $\\{\\pm1\\},i\\in[n]$ and $p\\in[P]\\setminus\\{p_{i}^{*}\\}$ . We assume the existence of $\\tilde{T}$ , as its absence would directly lead to our conclusion. Then, for any $0\\leq t<\\tilde{T}$ , we have ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathfrak{z}_{s}^{(t+1)}(i,p)=\\rho_{s}^{(t)}(i,p)+\\frac{\\eta}{n}\\mathbb{E}_{\\mathcal{C}\\sim\\mathcal{D}_{c}}\\left[g_{i,c}^{(t)}\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)\\cdot\\mathbb{1}_{p\\notin c}\\right]\\left\\Vert\\xi_{i}^{(p)}\\right\\Vert^{2}<\\rho_{s}^{(t)}(i,p)+\\frac{3\\eta\\sigma_{\\mathbf{b}}^{2}d}{2n},}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where the inequality is due to $g_{i,\\mathcal{C}}^{(t)}<1,\\phi^{\\prime}\\leq1$ and $\\begin{array}{r}{\\left\\|\\xi_{i}^{(p)}\\right\\|^{2}\\leq\\frac{3}{2}\\sigma_{\\mathrm{b}}^{2}d}\\end{array}$ Hence, we have ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\frac{1}{4P}\\leq\\rho_{s}^{(\\tilde{T})}(i,p)=\\sum_{t=0}^{\\tilde{T}-1}\\left(\\rho_{s}^{(t+1)}(i,p)-\\rho_{s}^{(t)}(i,p)\\right)<\\frac{3\\eta\\sigma_{\\mathrm{b}}^{2}d}{2n}\\tilde{T},\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "and we conclude $\\begin{array}{r}{\\tilde{T}>\\frac{n}{6\\eta P\\sigma_{\\mathrm{b}}^{2}d}}\\end{array}$ which is the desired result. ", "page_idx": 48}, {"type": "text", "text": "Next, we will show that the model learns common features in at least constant order within $\\tilde{T}$ iterates. ", "page_idx": 48}, {"type": "text", "text": "Lemma D3. Suppose he event $E_{\\mathrm{init}}$ occursand $\\begin{array}{r}{\\rho_{k}=\\omega\\left(\\frac{\\sigma_{\\mathrm{b}}^{2}d}{\\beta n}\\right)}\\end{array}$ for some $k\\in[K]$ Then,fo ach $s\\in\\{\\pm1\\}$ . there exists $\\begin{array}{r}{T_{s,k}\\leq\\frac{9n P}{\\eta\\beta|\\mathcal{V}_{s,k}|}}\\end{array}$ such that $\\gamma_{s}^{(t)}(s,k)+\\beta\\gamma_{-s}^{(t)}(s,k)\\geq1$ for any $t>T_{s,k}$ ", "page_idx": 48}, {"type": "text", "text": "Proof of Lemma D.3. Suppose $\\gamma_{s}^{(t)}(s,k)+\\beta\\gamma_{-s}^{(t)}(s,k)\\,<\\,1$ for all $\\begin{array}{r}{0\\;\\leq\\;t\\;\\leq\\;\\frac{n}{6\\eta P\\sigma_{\\mathrm{b}}^{2}d}}\\end{array}$ 6nPo?d. For each $i\\in\\mathcal{V}_{s,k}$ and $\\mathcal{C}\\subset[P]$ with $|{\\mathcal{C}}|=C$ such that $p_{i}^{*}\\notin\\mathcal{C}$ and $\\tilde{p}_{i}\\in\\mathcal{C}$ , we have ", "page_idx": 48}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 48}, {"type": "equation", "text": "$$\ny_{i}f_{\\mathbf{W}^{(t)}}(\\mathbf{X}_{i,c})\n$$$$\n\\begin{array}{r l}&{\\quad y_{i}\\jmath_{W^{(t)}}(\\mathbf{A}_{i,\\mathcal{C}})}\\\\ &{=\\phi\\left(\\left\\langle w_{s}^{(t)},v_{s,k}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-s}^{(t)},v_{s,k}\\right\\rangle\\right)+\\displaystyle\\sum_{p\\notin\\mathcal{C}\\cup\\{p_{i}^{*}\\}}\\left(\\phi\\left(\\left\\langle w_{s}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-s}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)\\right)}\\\\ &{\\leq\\gamma_{s}^{(t)}(s,k)+\\beta\\gamma_{-s}^{(t)}(s,k)+\\displaystyle\\sum_{p\\notin\\mathcal{C}\\cup\\{p_{i}^{*}\\}}\\left(\\rho_{s}^{(t)}(i,p)+\\beta\\rho_{-s}^{(t)}(i,p)\\right)+2P\\cdot o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right)}\\\\ &{\\leq1+2P\\cdot\\frac{1}{4P}+2P\\cdot o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "The first inequality is due to Lemma B.4, the second inequality holds since we can apply Lemma D.2, and the last inequality is due to A1). Thus, gi.e = $\\begin{array}{r}{g_{i,{\\mathcal C}}^{(t)}=\\frac{1}{1+\\exp\\left(y_{i}f_{W^{(t)}}({\\pmb X}_{i,{\\mathcal C}})\\right)}>\\frac{1}{9}}\\end{array}$ >andwe have $\\gamma_{s}^{(t+1)}(s,k)+\\beta\\gamma_{-s}^{(t+1)}(s,k)$ ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\gamma_{s}^{(t)}(s,k)+\\beta\\gamma_{-s}^{(t)}(s,k)}\\\\ &{\\quad+\\displaystyle\\frac{\\eta}{n}\\sum_{i\\in\\mathcal{V}_{s,k}}\\mathbb{E}_{\\sigma\\sim\\mathcal{D}_{\\sigma}}\\left[g_{i,\\ell}^{\\prime}\\bigg(\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(t)},v_{s,k}\\right\\rangle\\right)+\\beta\\phi^{\\prime}\\left(\\left\\langle w_{-s}^{(t)},v_{s,k}\\right\\rangle\\right)\\bigg)\\cdot\\mathbb{I}_{p_{s}^{\\ast}}\\phi c\\right]}\\\\ &{\\ge\\gamma_{s}^{(t)}(s,k)+\\beta\\gamma_{-s}^{(t)}(s,k)+\\displaystyle\\frac{\\eta\\beta}{9n}\\sum_{i\\in\\mathcal{V}_{s,k}}\\mathbb{E}_{\\sigma\\sim\\mathcal{D}_{c}}\\big[\\mathbb{I}_{p_{i}^{\\ast}}\\phi c\\hat{\\boldsymbol{x}}_{\\bar{p}_{i}\\in\\mathcal{C}}\\big]}\\\\ &{=\\gamma_{s}^{(t)}(s,k)+\\beta\\gamma_{-s}^{(t)}(s,k)+\\displaystyle\\frac{\\eta\\beta\\big|\\mathcal{V}_{s,k}\\big|C(P-C)}{9n P(P-1)}}\\\\ &{\\ge\\gamma_{s}^{(t)}(s,k)+\\beta\\gamma_{-s}^{(t)}(s,k)+\\displaystyle\\frac{\\eta\\beta\\big|\\mathcal{V}_{s,k}\\big|}{9n P}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "From the given condition in the lemma statement, we have $\\begin{array}{r}{\\frac{9n P}{\\eta\\beta|\\mathcal{V}_{s,k}|}=o\\left(\\frac{n}{6\\eta P\\sigma_{\\mathrm{b}}^{2}d}\\right)}\\end{array}$ .Ifwechoose $\\begin{array}{r}{t_{0}\\in\\left[\\frac{9n P}{\\eta\\beta|\\mathcal{V}_{s,k}|},\\frac{n}{6\\eta P\\sigma_{\\mathrm{b}}^{2}d}\\right]}\\end{array}$ then ", "page_idx": 49}, {"type": "equation", "text": "$$\n1>\\gamma_{s}^{(t_{0})}(s,k)+\\beta\\gamma_{-s}^{(t_{0})}(s,k)\\geq\\frac{\\eta\\beta|\\mathcal{V}_{s,k}|}{9n P}t_{0}\\geq1,\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "and this is contradictory therfore, it cannt hld that $\\gamma_{s}^{(t)}(s,k)+\\beta\\gamma_{-s}^{(t)}(s,k)<1$ for all $0\\leq t\\leq$ $\\frac{n}{6\\eta P\\sigma_{\\mathrm{b}}^{2}d}$ $\\begin{array}{r}{0\\,\\le\\,T_{s,k}\\,<\\,\\frac{n}{6\\eta P\\sigma_{\\mathrm{b}}^{2}d}}\\end{array}$ suchthat $\\gamma_{s}^{(T_{s,k}+1)}(s,k)+\\beta\\gamma_{-s}^{(T_{s,k}+1)}(s,k)\\geq1$ and choose the smallest one. Then we obtain ", "page_idx": 49}, {"type": "equation", "text": "$$\n1\\geq\\gamma_{s}^{(t)}(s,k)+\\beta\\gamma_{-s}^{(t)}(s,k)\\geq\\frac{\\eta\\beta|\\mathcal{V}_{s,k}|}{9n P}T_{s,k}.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Therefore, $\\begin{array}{r}{T_{s,k}\\le\\frac{9n P}{\\eta\\beta|\\mathcal{V}_{s,k}|}}\\end{array}$ and this is what we desired. ", "page_idx": 49}, {"type": "text", "text": "What We Have So Far. For any common feature or rare feature $\\pmb{v}_{s,k}$ with $s\\in\\{\\pm1\\}$ and $k\\in$ $\\ K_{C}\\cup K_{R}$ , it satisfies $\\begin{array}{r}{\\rho_{k}=\\omega\\left(\\frac{\\sigma_{\\mathrm{b}}^{2}d}{\\beta n}\\right)}\\end{array}$ due to (A5). By Lemma D.3, at any iterate $t\\in[\\bar{T}_{1},T^{*}]$ with $\\bar{T}_{1}:=\\operatorname*{max}_{s\\in\\{\\pm1\\},k\\in\\mathcal{C}}T_{s,k}$ the following properties hold if the event $E_{\\mathrm{init}}$ occurs: ", "page_idx": 49}, {"type": "text", "text": "\u00b7 (Learn common/rare features): For $s\\in\\{\\pm1\\}$ and $k\\in\\mathcal{K}_{C}\\cup\\mathcal{K}_{R},\\gamma_{s}^{(t)}(s,k)+\\beta\\gamma_{-s}^{(t)}(s,k)=\\Omega(1),$ \u00b7For any $s\\in\\{\\pm1\\},i\\in[n]$ ,and $p\\in[P]\\setminus\\{p_{i}^{*}\\},\\rho_{s}^{(t)}(i,p)=\\widetilde{\\mathcal{O}}\\left(\\beta^{-1}\\right).$ ", "page_idx": 49}, {"type": "text", "text": "D.2.3 Overfitting Augmented Data ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "In the previous step, we have shown that data containing common or rare features can be wellclassified by learning common and rare features. In this step, we will show that the model correctly classifies the remaining training data by overfitting background noise instead of learning its features. ", "page_idx": 49}, {"type": "text", "text": "We frs inroduce lower bounds on the number of iterates such that fature coefficients $\\gamma_{s}^{(t)}(s^{\\prime},k)$ remain small, up to the order of $\\alpha^{2}\\beta^{-1}$ . This lemma holds to any kind of features, but we will focus on extremely rare features. This does not contradict the results from Section D.2.2 for common features and rare features since the upper bound on the number of iterations in Lemma D.3 is larger than the lower bound on the number of iterations in this lemma. ", "page_idx": 49}, {"type": "text", "text": "Lemma D.4. Suppose the event $E_{\\mathrm{init}}$ occurs. For each $s~\\in~\\{\\pm1\\}$ and $\\textit{k}\\in\\ [K]$ , there exists $\\begin{array}{r}{\\tilde{T}_{s,k}\\geq\\frac{n\\alpha^{2}}{\\eta\\beta|\\mathcal{V}_{s,k}|}}\\end{array}$ sucha $\\gamma_{s^{\\prime}}^{(t)}(s,k)\\le\\alpha^{2}\\beta^{-1}$ forany $0\\leq t<\\tilde{T}_{s,k}$ $s^{\\prime}\\in\\{\\pm1\\}$ L. ", "page_idx": 49}, {"type": "text", "text": "Proo of Lemma $D.4.$ Let $\\tilde{T}_{s,k}$ be the smallet iterate such that $\\gamma_{s^{\\prime}}^{(t)}(s,k)>\\alpha^{2}\\beta^{-1}$ for some $s^{\\prime}\\in$ $\\{\\pm1\\}$ . We assume the existence of $\\tilde{T}_{s,k}$ , as its absence would directly lead to our conclusion. For any $0\\leq t<\\tilde{T}_{s,k}$ \uff0c $\\mathbf{\\Phi}_{s^{\\prime}}^{(t+1)}(s,k)=\\gamma_{s^{\\prime}}^{(t)}(s,k)+\\frac{\\eta}{n}\\sum_{i\\in\\mathcal{V}_{s,k}}\\mathbb{E}_{C\\sim\\mathcal{D}_{C}}\\left[g_{i,C}^{(t)}\\phi^{\\prime}\\left(\\left\\langle w_{s^{\\prime}}^{(t)},v_{s,k}\\right\\rangle\\right)\\cdot\\mathbb{1}_{p_{i}^{*}\\notin\\mathcal{C}}\\right]\\leq\\gamma_{s^{\\prime}}^{(t)}(s,k)+\\frac{\\eta|\\mathcal{V}_{s,k}|}{n},$ ", "page_idx": 49}, {"type": "text", "text": "and we have ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\alpha^{2}\\beta^{-1}\\le\\gamma_{s^{\\prime}}^{\\left(\\widetilde{T}_{s,k}\\right)}(s,k)=\\sum_{t=0}^{\\widetilde{T}_{s,k}-1}\\left(\\gamma_{s^{\\prime}}^{(t+1)}(s,k)-\\gamma_{s^{\\prime}}^{(t)}(s,k)\\right)\\le\\frac{\\eta|\\mathcal{V}_{s,k}|}{n}\\widetilde{T}_{s,k}.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "We conclude $\\begin{array}{r}{\\tilde{T}_{s,k}\\geq\\frac{n\\alpha^{2}}{\\eta\\beta|\\mathcal{V}_{s,k}|}}\\end{array}$ which is the desired result. ", "page_idx": 50}, {"type": "text", "text": "Next, we will show that the model overfits data augmented not containing common or rare features in at least constant order within $\\tilde{T}_{s,k}$ iterates. ", "page_idx": 50}, {"type": "text", "text": "Lemma D.5. Suppose the event $E_{\\mathrm{init}}$ occurs and $\\rho_{k}=o\\left(\\frac{\\alpha^{2}\\sigma_{\\mathrm{b}}^{2}d}{n}\\right)$ . For each $i\\in[n]$ and $\\mathcal{C}\\subset[P]$ with $|{\\mathcal{C}}|=C,\\,i f(l)\\,i\\in\\mathcal{V}_{y_{i},k}$ $p_{i}^{*}\\notin\\mathcal{C}$ $(2)\\;i\\in[n]$ $p_{i}^{*}\\in\\mathcal{C}$ $\\begin{array}{r}{T_{i,c}\\in\\left[\\bar{T}_{1},\\frac{18n\\left(\\underset{C}{P}\\right)}{\\eta\\beta\\sigma_{\\mathrm{b}}^{2}d}\\right]}\\end{array}$ ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\sum_{p\\notin\\mathcal{C}\\cup\\{p_{i}^{*}\\}}\\left(\\rho_{y_{i}}^{(t)}(i,p)+\\beta\\rho_{-y_{i}}^{(t)}(i,p)\\right)\\geq1,\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "for any $t>T_{i,\\mathcal{C}}$ ", "page_idx": 50}, {"type": "text", "text": "Proof of Lemma D.5. We can address both cases in the statement simultaneously. Suppose $\\begin{array}{r}{\\sum_{p\\notin{\\mathcal C}\\cup\\{p_{i}^{*}\\}}\\left(\\rho_{y_{i}}^{(t)}(i,p)+\\beta\\rho_{-y_{i}}^{(t)}(i,p)\\right)<1}\\end{array}$ forall $\\begin{array}{r}{0\\leq t\\leq\\frac{n\\alpha^{2}}{\\eta\\beta|\\mathcal{V}_{y_{i},k}|}}\\end{array}$ ", "page_idx": 50}, {"type": "text", "text": "From Lemma B.4 and Lemma D.4, we have ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad y_{i}f_{W^{(t)}}(X_{i,\\mathcal{C}})}\\\\ &{=\\displaystyle\\sum_{p\\notin\\mathcal{C}}\\left(\\phi\\left(\\left\\langle w_{y_{i}}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-y_{i}}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)\\right)}\\\\ &{\\leq\\displaystyle\\gamma_{y_{i}}^{(t)}(y_{i},k)+\\beta\\gamma_{-y_{i}}^{(t)}(y_{i},k)+\\displaystyle\\sum_{p\\notin\\mathcal{C}\\cup\\{p_{i}^{*}\\}}\\left(\\rho_{y_{i}}^{(t)}(i,p)+\\beta\\rho_{-y_{i}}^{(t)}(i,p)\\right)+2P\\cdot o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right)}\\\\ &{\\leq\\displaystyle(1+\\beta)\\alpha^{2}\\beta^{-1}+1+2P\\cdot o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "\u22642, ", "page_idx": 50}, {"type": "text", "text": "and gi.c =1+ex(:fwt)(X,c) \u2265. Also, for eachp CU{p},we have ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\rho_{s}^{(t+1)}(i,p)+\\beta\\rho_{-s}^{(t+1)}(i,p)}\\\\ &{\\geq\\rho_{s}^{(t)}(i,p)+\\beta\\rho_{-s}^{(t)}(i,p)}\\\\ &{\\quad+\\cfrac{\\eta}{n}\\mathbb{P}_{c^{\\prime}\\sim\\mathcal{D}_{c}}[c^{\\prime}=\\mathcal{C}]g_{i,c}^{(t)}\\bigg(\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)+\\beta\\phi^{\\prime}\\left(\\left\\langle w_{-s}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)\\bigg)\\left\\|\\xi_{i}^{(p)}\\right\\|^{2}}\\\\ &{\\geq\\rho_{s}^{(t)}(i,p)+\\beta\\rho_{-s}^{(t)}(i,p)+\\cfrac{\\eta\\beta\\sigma_{\\mathrm{b}}^{2}d}{18n\\binom{P}{C}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "where the last inequality is due to $\\begin{array}{r}{\\left\\|\\xi_{i}^{(p)}\\right\\|^{2}\\geq\\frac{1}{2}\\sigma_{\\mathrm{b}}^{2}d}\\end{array}$ and $\\phi^{\\prime}\\geq\\beta$ .We also have ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\sum_{p\\notin C\\cup\\{p_{i}^{*}\\}}\\left(\\rho_{s}^{(t+1)}(i,p)+\\beta\\rho_{-s}^{(t+1)}(i,p)\\right)\\geq\\sum_{p\\notin C\\cup\\{p_{i}^{*}\\}}\\left(\\rho_{s}^{(t)}(i,p)+\\beta\\rho_{-s}^{(t)}(i,p)\\right)+\\frac{\\eta\\beta\\sigma_{\\mathbf{b}}^{2}d}{18n\\binom{P}{C}}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "From the ivenconditn in thelmastatment, wehae $\\begin{array}{r}{\\frac{18n\\binom{P}{C}}{\\eta\\beta\\sigma_{\\mathrm{b}}^{2}d}=o\\left(\\frac{n\\alpha^{2}}{\\eta\\beta\\vert\\mathcal{V}_{s,k}\\vert}\\right)}\\end{array}$ .If we choose $\\begin{array}{r}{t_{0}\\in\\Bigg[\\frac{18n\\binom{P}{C}}{\\eta\\beta\\sigma_{\\mathrm{b}}^{2}d},\\frac{n\\alpha^{2}}{\\eta\\beta|\\mathcal{V}_{s,k}|}\\Bigg],}\\end{array}$ then we have ", "page_idx": 50}, {"type": "equation", "text": "$$\n1>\\sum_{p\\notin C\\cup\\{p_{i}^{*}\\}}\\left(\\rho_{s}^{(t_{0})}(i,p)+\\beta\\rho_{-s}^{(t_{0})}(i,p)\\right)\\geq\\frac{\\eta\\beta\\sigma_{\\mathrm{b}}^{2}d}{18n{\\binom{P}{C}}}t_{0}\\geq1,\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "and thisis acnaditin; tore, it cat t $\\begin{array}{r l}{\\sum_{p\\notin{\\mathcal{C}}\\cup\\{p_{i}^{*}\\}}\\left(\\rho_{y_{i}}^{(t)}(i,p)+\\beta\\rho_{-y_{i}}^{(t)}(i,p)\\right)<}&{{}}\\end{array}$ 1 for all $\\begin{array}{r c l c l}{0}&{\\le}&{t}&{\\le}&{\\frac{n\\alpha^{2}}{\\eta\\beta|\\mathcal{V}_{y_{i},k}|}}\\end{array}$ Thus, there exists $\\begin{array}{l l l l l}{0}&{\\le}&{T_{i,{\\cal C}}}&{<}&{\\frac{n\\alpha^{2}}{\\eta\\beta|\\mathcal{V}_{s,k}|}}\\end{array}$ satisfying $\\begin{array}{r}{\\sum_{p\\notin\\mathcal{C}\\cup\\{p_{i}^{*}\\}}\\left(\\rho_{s}^{(T_{i,c}+1)}(i,p)+\\beta\\rho_{-s}^{(T_{i,c}+1)}(i,p)\\right)\\geq1}\\end{array}$   \nFor any $0\\leq t<T_{i,\\mathcal{C}}$ , we have ", "page_idx": 51}, {"type": "equation", "text": "$$\n1\\geq\\sum_{p\\notin C\\cup\\{p_{i}^{*}\\}}\\left(\\rho_{s}^{(T_{i,c})}(i,p)+\\beta\\rho_{-s}^{(T_{i,c})}(i,p)\\right)\\geq\\frac{\\eta\\sigma_{\\mathrm{b}}^{2}d}{18n{\\binom{P}{C}}}T_{i},\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "and we conclude that $\\begin{array}{r}{T_{i,\\mathcal{C}}\\leq\\frac{18n\\binom{P}{C}}{\\eta\\beta\\sigma_{\\mathrm{b}}^{2}d}}\\end{array}$ ", "page_idx": 51}, {"type": "text", "text": "Lastly, we move on to prove $T_{i,c}>\\bar{T}_{1}$ . Combining Lemma D.2 and Lemma D.3 leads to ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\sum_{p\\notin\\mathcal{C}\\cup\\{p_{i}^{*}\\}\\setminus\\{p_{i}^{*}\\}}\\left(\\rho_{s}^{(\\bar{T}_{1})}(i,p)+\\beta\\rho_{-s}^{(\\bar{T}_{1})}(i,p)\\right)\\leq\\frac{1}{2}.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Thus, we have $T_{i,c}>\\bar{T}_{1}$ and this is what we desired. ", "page_idx": 51}, {"type": "text", "text": "What We Have So Far.  For any $k\\in\\kappa_{E}$ , it satisfies $\\begin{array}{r}{\\rho_{k}=o\\left(\\frac{\\alpha^{2}n}{\\sigma_{\\mathrm{b}}^{2}d}\\right)}\\end{array}$ due to (A6). By Lemma D.5 at iterate $t\\in[T_{\\mathrm{Cutout}},T^{*}]$ with ", "page_idx": 51}, {"type": "equation", "text": "$$\nT_{\\mathrm{Cutout}}:=\\operatorname*{max}{\\left\\{\\operatorname*{max}_{k\\in\\mathcal{K}_{E_{i},k},p_{i}^{*}\\notin\\mathcal{C}}{T_{i,\\mathcal{C}}},\\operatorname*{max}_{i\\in[n],p_{i}^{*}\\in\\mathcal{C}}{T_{i,\\mathcal{C}}}\\right\\}}\\in\\left[\\bar{T}_{1},T^{*}\\right]\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "the following properties hold if the event $E_{\\mathrm{init}}$ occurs: ", "page_idx": 51}, {"type": "text", "text": "\u00b7 (Learn common/rare features): For any $s\\in\\{\\pm1\\}$ and $k\\in K_{C}\\cup K_{R}$ ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\gamma_{s}^{(t)}(s,k)+\\beta\\gamma_{-s}^{(t)}(s,k)=\\Omega(1),\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "\u00b7 (Overfit augmented data with extremely rare features or no feature): For each $i\\in[n],k\\in K_{E}$ $\\mathcal{C}\\subset[P]$ with $|{\\mathcal{C}}|=C$ such that (1) $i\\in\\mathcal{V}_{y_{i},k}$ and $p_{i}^{*}\\notin\\mathcal{C}$ or (2) $i\\in[n]$ and $p_{i}^{*}\\in\\mathcal{C}$ ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\sum_{p\\notin\\mathcal{C}\\cup\\{p_{i}^{*}\\}}\\Big(\\rho_{y_{i}}^{(t)}(i,p)+\\beta\\rho_{-y_{i}}^{(t)}(i,p)\\Big)=\\Omega(1).\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "\u00b7 (Do not learn extremely rare features at $T_{\\mathrm{Cutout}})$ : For any $s,s^{\\prime}\\in\\{\\pm1\\}$ and $k\\in\\kappa_{E}$ \uff0c ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\gamma_{s^{\\prime}}^{(T_{\\mathrm{Cutout}})}(s,k)\\le\\alpha^{2}\\beta^{-1}.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "\u00b7 For any $s\\in\\{\\pm1\\},i\\in[n]$ , and $p\\in[P]\\setminus\\{p_{i}^{*}\\},\\rho_{s}^{(t)}(i,p)=\\widetilde{\\mathcal{O}}\\left(\\beta^{-1}\\right)\\!.$ ", "page_idx": 51}, {"type": "text", "text": "D.2.4 Cutout cannot Learn Extremely Rare Features Within Polynomial Times ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "In this step, We will show that Cutout cannot learn extremely rare features within the maximum admissible iterate T\\* = poly(d). ", "page_idx": 51}, {"type": "text", "text": "we fix any $s^{*}~\\in~\\{\\pm1\\}$ and $k^{*}\\;\\in\\;\\kappa_{E}$ . Recall the function $Q^{(s^{*},k^{*})}\\,:\\,\\mathcal{W}\\,\\to\\,\\mathbb{R}^{d\\times2}$ , defined in Lemma B.5 and omit superscripts for simplicity. For each iteration $t$ $Q(\\pmb{W}^{(t)})$ represents quantities updates by data with feature vector $\\pmb{v}_{s^{*},k^{*}}$ until $t$ -th iteration. We will sequentially introduce several technical iemmas and by combining these lemmas, quantify update by data with feature vector $\\pmb{v}_{s^{*},k^{*}}$ after $T_{\\mathrm{{Cutout}}}$ and derive our conclusion. ", "page_idx": 51}, {"type": "text", "text": "Let us define $W^{*}=\\left\\{\\pmb{w}_{1}^{*},\\pmb{w}_{-1}^{*}\\right\\}$ , where ", "page_idx": 51}, {"type": "equation", "text": "$$\nw_{s}^{*}=w_{s}^{(T_{\\mathrm{Cutout}})}+M\\sum_{i\\in\\mathcal{V}_{s^{*},k^{*}}}\\sum_{p\\in[P]\\setminus\\{p_{i}^{*},\\tilde{p}_{i}\\}}\\frac{\\xi_{i}^{(p)}}{\\left\\|\\xi_{i}^{(p)}\\right\\|^{2}},\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "where $\\begin{array}{r}{M=4\\beta^{-1}\\log\\left(\\frac{2\\eta\\beta^{2}T^{*}}{\\alpha^{2}}\\right)}\\end{array}$ . Note that (12), $\\beta<1$ , and $\\begin{array}{r}{T^{*}=\\frac{\\mathrm{poly}(d)}{\\eta}}\\end{array}$ together imply $M=$ $\\widetilde{\\mathcal{O}}\\left(\\beta^{-1}\\right)$ . Note that $\\boldsymbol{W}^{(t)},\\boldsymbol{W}^{*}\\in\\mathcal{W}$ for any $t\\geq0$ ", "page_idx": 51}, {"type": "text", "text": "Lemma D.6. Suppose the event $E_{\\mathrm{init}}$ occurs. Then, ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|Q\\left(W^{(T_{\\mathrm{Cutout}})}\\right)-Q(W^{*})\\right\\|^{2}\\leq8M^{2}P|\\mathcal V_{s^{*},k^{*}}|\\sigma_{\\mathrm{b}}^{-2}d^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "where $\\lVert\\cdot\\rVert$ denotes the Frobenius norm. ", "page_idx": 52}, {"type": "text", "text": "Proof of Lemma D.6. For each $s\\in\\{\\pm1\\}$ ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad s s^{*}\\left(Q_{s}\\left(\\pmb{w_{s}^{*}}\\right)-Q_{s}\\left(\\pmb{w_{s}^{(T_{\\mathrm{Cutout}})}}\\right)\\right)}\\\\ &{=Q_{s}\\left(\\pmb{s}s^{*}\\pmb{M}\\displaystyle\\sum_{i\\in\\mathcal{V}_{s^{*},k^{*}}}\\sum_{p\\in[P]\\backslash\\{p_{i}^{*},\\bar{p}_{i}\\}}\\frac{\\xi_{i}^{(p)}}{\\left\\|\\xi_{i}^{(p)}\\right\\|}\\right)}\\\\ &{=\\displaystyle M\\sum_{i\\in\\mathcal{V}_{s^{*},k^{*}}}\\sum_{p\\in[P]\\backslash\\{p_{i}^{*},\\bar{p}_{i}\\}}\\frac{\\xi_{i}^{(p)}}{\\left\\|\\xi_{i}^{(p)}\\right\\|^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "and we have ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|Q\\left(W^{(T_{\\mathrm{Cutout}})}\\right)-Q(W^{*})\\right\\|^{2}}\\\\ &{=\\left\\|Q_{1}(w_{1}^{*})-Q_{1}\\left(w_{1}^{(T_{\\mathrm{Cutout}})}\\right)\\right\\|^{2}+\\left\\|Q_{-1}(w_{-1}^{*})-Q_{-1}\\left(w_{-1}^{(T_{\\mathrm{Cutout}})}\\right)\\right\\|^{2}}\\\\ &{\\leq2M^{2}\\left(\\sum_{\\stackrel{i\\in\\mathcal{V}^{*}}{i\\in\\mathcal{V}^{*},k^{*},p\\in[P]\\setminus\\{p_{i}^{*},\\tilde{p}_{i}\\}}}\\left\\|\\xi_{i}^{(p)}\\right\\|^{-2}+\\sum_{\\stackrel{i,j\\in\\mathcal{V}^{*},k^{*}}{p\\in[P]\\setminus\\{p_{i}^{*},\\tilde{p}_{i}\\},q\\in[P]}}\\frac{\\left|\\left\\langle\\xi_{i}^{(p)},\\xi_{j}^{(q)}\\right\\rangle\\right|}{\\left|\\xi_{i}^{(p)}\\right|^{2}\\left\\|\\xi_{j}^{(q)}\\right\\|^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "From $E_{\\mathrm{init}}$ and (A2), we have ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\substack{p\\in[P]\\setminus\\{p_{*}^{*},\\bar{p}\\}\\in[P]\\setminus\\{p_{*}^{*},\\bar{p}_{*}\\}}}{\\sum}\\ \\frac{\\left|\\left\\langle\\xi_{i}^{(p)},\\xi_{j}^{(q)}\\right\\rangle\\right|}{\\left|\\left\\langle\\xi_{i}^{(p)}\\right\\|^{2}\\left\\|\\xi_{j}^{(q)}\\right\\|^{2}\\right|}\\leq\\underset{\\substack{p\\in[P]\\setminus\\{p_{*}^{*},\\bar{p}_{*}\\}}}{\\sum}\\ \\underset{\\substack{p\\in[P]\\setminus\\{p_{*}^{*},\\bar{p}_{*}\\}}}{\\sum}\\ \\left\\|\\xi_{i}^{(\\bar{p})}\\right\\|^{-2}\\widetilde{\\mathcal{O}}\\left(d^{-\\frac{1}{2}}\\right)}\\\\ {p\\in[P]\\backslash\\{p_{*}^{*},\\bar{p}_{*}\\}\\#\\{P\\}\\backslash\\{p_{*}^{*},\\bar{p}_{*}\\}}\\\\ {\\underset{(i,p)\\not\\in[Q,\\eta]}{\\sum}\\ }&{\\leq\\underset{\\substack{p\\in[P]\\setminus\\{p_{*}^{*},\\bar{p}_{*}\\}}}{\\sum}\\ \\Big\\|\\xi_{i}^{(\\bar{p})}\\Big\\|^{-2}\\ \\widetilde{\\mathcal{O}}\\left(n P d^{-\\frac{1}{2}}\\right)}\\\\ &{\\leq\\underset{\\substack{p\\in[P]\\setminus\\{p_{*}^{*},\\bar{p}_{*}\\}}}{\\sum}\\Big\\|\\xi_{i}^{(p)}\\Big\\|^{-2}}\\\\ &{\\leq\\underset{\\substack{i\\in[P]\\setminus\\{p_{*}^{*},\\bar{p}_{*}\\}}}{\\sum}\\Big\\|\\xi_{i}^{(p)}\\Big\\|^{-2}}\\\\ &{\\underset{p\\in[P]\\setminus\\{p_{*}^{*},\\bar{p}_{*}\\}}{\\sum}}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "From the event $E_{\\mathrm{init}}$ defined in Lemma B.2, we have ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\sum_{\\stackrel{i\\in\\mathcal{V}_{s^{*},k^{*}}}{p\\in[P]\\backslash\\{p_{i}^{*},\\tilde{p}_{i}\\}}}\\left\\|\\xi_{i}^{(p)}\\right\\|^{-2}\\leq2P|\\mathcal{V}_{s^{*},k^{*}}|\\sigma_{\\mathrm{d}}^{-2}d^{-1},\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "and we obtain ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\left\\|Q\\left(W^{(T_{\\mathrm{Cutout}})}\\right)-Q(W^{*})\\right\\|^{2}\\leq4M^{2}\\sum_{i\\in\\mathcal{V}_{s^{*},k^{*}},p\\in[P]\\backslash\\{p_{i}^{*}\\}}\\left\\|\\xi_{i}^{(p)}\\right\\|^{-2}\\leq8M^{2}P|\\mathcal{V}_{s^{*},k^{*}}|\\sigma_{\\mathrm{b}}^{-2}d^{-1}.\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Lemma D.7. Suppose the $E_{\\mathrm{init}}$ occurs. For any $t\\,\\geq\\,T_{\\mathrm{Cutout}},\\;i\\,\\in\\,\\mathcal{V}_{s^{\\ast},k^{\\ast}}$ and any $\\mathcal{C}\\subset[P]$ with $|{\\mathcal{C}}|=C$ it holds that ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\langle y_{i}\\nabla_{W}f_{W^{(t)}}(\\mathbf{X}_{i,c}),Q(W^{*})\\rangle\\geq\\frac{M\\beta}{2}.\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Proof of Lemma D.7. We have ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\langle y_{i}\\nabla_{W}f_{W^{(i)}}(X_{i,c}),Q(W^{*})\\rangle}\\\\ &{=\\displaystyle\\sum_{p\\notin C}\\Bigg(\\phi^{\\prime}\\left(\\left\\langle w_{s^{*}}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)\\left\\langle Q_{s^{*}}(w_{s^{*}}^{*}),x_{i}^{(p)}\\right\\rangle-\\phi^{\\prime}\\left(\\left\\langle w_{-s^{*}}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)\\left\\langle Q_{-s^{*}}(w_{-s^{*}}^{*}),x_{i}^{(p)}\\right\\rangle\\Bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "For any $s\\in\\{\\pm1\\}$ and $p\\in[P]\\setminus\\{p_{i}^{*},\\tilde{p}_{i}\\}$ ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad s^{*}\\left\\langle Q_{s}(\\mathbf{w}_{s}^{*}),\\xi_{i}^{(p)}\\right\\rangle}\\\\ &{\\geq M+\\rho_{s}^{(T_{\\mathrm{cuton}})}(i,p)-\\displaystyle\\sum_{\\substack{j\\in[n],q\\in[p]\\setminus[p]}}\\rho_{s}^{(T_{\\mathrm{cuton}})}(j,q)\\frac{\\left|\\left\\langle\\xi_{i}^{(p)},\\xi_{j}^{(q)}\\right\\rangle\\right|}{\\left\\|\\xi_{j}^{(q)}\\right\\|^{2}}}\\\\ &{\\quad-\\displaystyle\\sum_{\\substack{j\\in[p_{s},\\ldots,q\\in[p]]\\setminus\\{s,j\\}}}\\frac{\\left|\\left\\langle\\xi_{i}^{(p)},\\xi_{j}^{(q)}\\right\\rangle\\right|}{\\left\\|\\xi_{j}^{(q)}\\right\\|^{2}}}\\\\ &{\\geq M-\\displaystyle\\tilde{\\sigma}\\left(n P\\beta^{-1}\\sigma_{i}\\sigma_{s}^{-1}d-\\frac{1}{2}\\right)=M-\\sigma\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right)}\\\\ &{\\geq\\frac{M}{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "where the last equality is due to (9). Also, for any $s\\in\\{\\pm1\\}$ $\\mathbf{\\Pi}_{s s^{\\ast}}\\langle Q_{s}(\\mathbf{w}_{s}^{\\ast}),\\mathbf{v}_{s^{\\ast},k^{\\ast}}\\rangle\\;\\;=$ Toutout)(s\\*, k\\*) \u2265 0. In addition, ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad s^{*}\\left\\langle Q_{s}(w_{s}^{*}),x_{i}^{(\\bar{p}_{i})}\\right\\rangle}\\\\ &{=s^{*}\\left\\langle Q_{s}(w_{s}^{*}),\\xi_{i}^{(\\bar{p}_{i})}\\right\\rangle+s^{*}\\left\\langle Q_{s}(w_{s}^{*}),x_{i}^{(\\bar{p}_{i})}-\\xi_{i}^{(\\bar{p}_{i})}\\right\\rangle}\\\\ &{=s^{*}\\left\\langle Q_{s}(w_{s}^{*}),\\xi_{i}^{(\\bar{p}_{i})}\\right\\rangle-\\widetilde{O}\\left(\\alpha^{2}\\beta^{-1}\\rho_{k}\\cdot n\\sigma_{\\mathrm{d}}^{-2}d^{-1}\\right)}\\\\ &{=\\rho_{s}^{(T_{\\mathrm{cuss}})}(i,\\bar{p}_{i})+\\underset{\\substack{j\\in[n],q\\in[n]\\,\\{i\\}}}{\\sum}\\rho_{s}^{(T_{\\mathrm{cuss}})}(j,q)\\frac{\\left\\langle\\xi_{i}^{(\\bar{p}_{i})},\\xi_{j}^{(\\bar{q})}\\right\\rangle}{\\left\\|\\xi_{j}^{(\\bar{q})}\\right\\|^{2}}-\\widetilde{O}\\left(\\alpha^{2}\\beta^{-1}\\rho_{k}\\cdot n\\sigma_{\\mathrm{d}}^{-2}d^{-1}\\right)}\\\\ &{\\geq-\\widetilde{O}\\left(n P\\beta^{-1}\\sigma_{\\mathrm{d}}\\sigma_{\\mathrm{b}}^{-1}d^{-\\frac{1}{2}}\\right)-\\widetilde{O}\\left(\\alpha^{2}\\beta^{-1}\\rho_{k}\\cdot n\\sigma_{\\mathrm{d}}^{-2}d^{-1}\\right)}\\\\ &{=-o\\left(\\frac{1}{\\mathrm{polyspo}(i)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "where the last equality is due to (9) and (A7). ", "page_idx": 53}, {"type": "text", "text": "For any $\\mathcal{C}\\subset[P]$ with $|{\\mathcal{C}}|=C$ , there exists $p\\in[P]\\setminus\\{p_{i}^{*},\\tilde{p}_{i}\\}$ such that $p\\neq\\mathcal{C}$ since $\\begin{array}{r}{C<\\frac{P}{2}}\\end{array}$ .By applying (26) and (27) for $s=s^{*},-s^{*}$ and combining with $\\phi^{\\prime}\\geq\\beta$ , we have ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\langle y_{i}\\nabla_{W}f_{W^{(t)}}(X_{i,c}),Q(W^{*})\\rangle}\\\\ &{\\geq\\left(\\phi^{\\prime}\\left(\\left\\langle w_{s^{*}}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)\\left\\langle Q_{s^{*}}(w_{s^{*}}^{*}),x_{i}^{(p)}\\right\\rangle-\\phi^{\\prime}\\left(\\left\\langle w_{-s^{*}}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)\\left\\langle Q_{-s^{*}}(w_{-s^{*}}^{*}),x_{i}^{(p)}\\right\\rangle\\right)}\\\\ &{\\quad+\\displaystyle\\sum_{q\\notin\\mathrm{Ccup}\\{p\\}}\\left(\\phi^{\\prime}\\left(\\left\\langle w_{s^{*}}^{(t)},x_{i}^{(q)}\\right\\rangle\\right)\\left\\langle Q_{s^{*}}(w_{s^{*}}^{*}),x_{i}^{(q)}\\right\\rangle-\\phi^{\\prime}\\left(\\left\\langle w_{-s^{*}}^{(t)},x_{i}^{(q)}\\right\\rangle\\right)\\left\\langle Q_{-s^{*}}(w_{-s^{*}}^{*}),x_{i}^{(q)}\\right\\rangle\\right.}\\\\ &{\\geq M\\beta-o\\left(\\displaystyle\\frac{1}{\\mathrm{polylog}(d)}\\right)}\\\\ &{\\geq\\frac{M\\beta}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "By combining Lemma D.6 and Lemma D.7, we can obtain the following result. ", "page_idx": 54}, {"type": "text", "text": "Lemma D.8. Suppose the event $E_{\\mathrm{init}}$ occurs. ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\frac{\\eta}{n}\\sum_{t=T_{\\mathrm{Cutout}}}^{T^{*}}\\sum_{i\\in V_{s^{*},k^{*}}}\\mathbb{E}_{\\mathcal{C}\\sim\\mathcal{D}_{C}}\\big[\\ell\\left(y_{i}f_{W^{(t)}}(X_{i,{\\mathcal C}})\\right)\\big]\\le\\Big\\|Q\\left(W^{(T_{\\mathrm{Cutout}})}\\right)-Q(W^{*})\\Big\\|^{2}+2\\eta T^{*}e^{-\\frac{M\\beta}{4}},\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "where $\\lVert\\cdot\\rVert$ denotes the Frobenius norm. ", "page_idx": 54}, {"type": "text", "text": "Proof of Lemma $D.8$ .Note that for any $T_{\\mathrm{Cutout}}\\leq t<T^{*}$ ", "page_idx": 54}, {"type": "equation", "text": "$$\nQ\\left(\\pmb{W}^{(t+1)}\\right)=Q\\left(\\pmb{W}^{(t)}\\right)-\\frac{\\eta}{n}\\nabla_{\\pmb{W}}\\sum_{i\\in\\mathcal{V}_{s^{\\star},k^{\\star}}}\\mathbb{E}_{\\mathcal{C}\\sim\\mathcal{D}_{C}}\\left[\\ell\\left(y_{i}\\pmb{f}_{\\pmb{W}^{(t)}}(\\pmb{X}_{i,\\mathcal{C}})\\right)\\right].\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Therefore, we have ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\lVert\\bar{Q}\\left(W^{(n)}\\right)-\\bar{Q}(W^{(\\varepsilon)})\\right\\rVert^{2}=\\left\\lVert\\bar{Q}\\left(W^{(n+1)}\\right)-\\bar{Q}(W^{(\\varepsilon)})\\right\\rVert^{2}}\\\\ &{=\\frac{2\\eta}{n}\\Bigg\\langle\\nabla\\mathfrak{w}\\displaystyle\\sum_{s\\in\\mathcal{N}_{n}\\setminus\\mathcal{N}_{n}}\\mathbb{E}_{\\Phi^{(\\varepsilon)}\\cap\\mathcal{N}_{n}}[(\\delta_{t}\\mathrm{foren}(X_{\\varepsilon})),Q(W^{(\\varepsilon)})-Q(W^{(\\varepsilon)})\\Bigg)}\\\\ &{\\qquad-\\frac{\\eta^{2}}{n}\\Bigg\\rangle\\Bigg[\\mathfrak{w}_{\\varepsilon\\in\\mathcal{N}_{n}\\setminus\\mathcal{N}_{n}}^{(\\varepsilon)}\\underbrace{\\mathbb{E}_{\\Phi^{(\\varepsilon)}\\cap\\mathcal{N}_{n}}[(\\delta_{t}\\mathrm{foren}(X_{\\varepsilon}))]}_{\\textnormal{D=g(b)\\to g(b)}}\\Bigg]^{2}}\\\\ &{=\\frac{2\\eta}{n}\\Bigg\\langle\\mathfrak{w}\\displaystyle\\sum_{s\\in\\mathcal{N}_{n}\\setminus\\mathcal{N}_{n}}\\mathbb{E}_{\\Phi^{(\\varepsilon)}\\cap\\mathcal{N}_{n}}[(\\delta_{t}\\mathrm{foren}(X_{\\varepsilon})),Q(W^{(\\varepsilon)})\\Bigg)\\Bigg\\rangle}\\\\ &{\\qquad-\\frac{2\\eta}{n}\\sum_{i_{1}\\in\\mathcal{N}_{n}\\setminus\\mathcal{N}_{n}}\\mathbb{E}_{\\Phi^{(\\varepsilon)}\\cap\\mathcal{N}_{n}}[(\\delta_{i}\\mathrm{foren}(X_{\\varepsilon}))\\nabla\\mathfrak{w}_{\\varepsilon\\in\\mathcal{N}_{n}}(X_{\\varepsilon})],Q(W^{(\\varepsilon)})}\\\\ &{\\qquad-\\frac{\\eta^{2}}{n}\\Bigg]\\nabla\\mathfrak{w}_{\\varepsilon\\in\\mathcal{N}_{n}}\\underbrace{\\mathbb{E}_{\\Phi^{(\\varepsilon)}\\cap\\mathcal{N}_{n}}[(\\delta_{t}\\mathrm{foren}(X_{\\varepsilon}))]}_{\\textnormal{D=g(b)\\to g(b)}}\\Bigg[\\mathfrak{w}_{\\varepsilon\\in\\mathcal{N}_{n}}^{(\\varepsilon)}}\\\\ &{\\qquad\\frac{\\eta^{2}}{n}\\Bigg]\\nabla\\mathfrak{w}_{\\varepsilon\\in\\mathcal{N}_{n}}\\underbrace{\\mathbb{E}_{\\Phi^{(\\varepsilon)}\\in\\mathcal{N}_{n}}[(\\delta_{t}\\mathrm{foren}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "where the last inequality is due to Lemma D.7. By the chain rule, for each $\\mathcal{C}\\subset[P]$ With $|{\\mathcal{C}}|=C$ we have ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\Biggl\\langle\\nabla_{W}\\displaystyle\\sum_{i\\in\\mathcal{V}_{s^{*}},k^{*}}\\ell(y_{i}f_{W^{(t)}}(X_{i,c})),Q\\left(W^{(t)}\\right)\\Biggr\\rangle}\\\\ &{=\\displaystyle\\sum_{i\\in\\mathcal{V}_{s^{*}},k^{*}}\\left[\\ell^{\\prime}(y_{i}f_{W^{(t)}}(X_{i,c}))\\right.}\\\\ &{\\quad\\times\\displaystyle\\sum_{p\\notin\\mathcal{C}}\\left(\\phi^{\\prime}\\left(\\left\\langle w_{s^{*}}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)\\left\\langle Q_{s^{*}}\\left(w_{s^{*}}^{(t)}\\right),x_{i}^{(p)}\\right\\rangle-\\phi^{\\prime}\\left(\\left\\langle w_{-s^{*}}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)\\left\\langle Q_{-s^{*}}\\left(w_{-s^{*}}^{(t)}\\right),x_{i}^{(p)}\\right\\rangle\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "For each $s\\in\\{\\pm1\\}$ \uff0c $i\\in\\mathcal{V}_{s^{*},k^{*}}$ , and $p\\in[P]$ ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\,\\,\\left|\\left<\\boldsymbol{w}_{s}^{(t)},\\boldsymbol{x}_{i}^{(p)}\\right>-\\left<Q_{s}\\left(\\boldsymbol{w}_{s}^{(t)}\\right),\\boldsymbol{x}_{i}^{(p)}\\right>\\right|}\\\\ &{=\\left|\\left<\\boldsymbol{w}_{s}^{(t)}-Q_{s}\\left(\\boldsymbol{w}_{s}^{(t)}\\right),\\boldsymbol{x}_{i}^{(p)}\\right>\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\le\\displaystyle\\sum_{j\\in[n]\\backslash\\mathcal{V}_{s^{\\star}},\\ldots,\\mathcal{\\ell}\\in[P]\\backslash\\{r_{j}\\}}\\left\\langle\\left\\langle\\rho_{s}^{(t)}(j,q)\\frac{\\xi_{j}^{(t)}}{\\left\\|\\xi_{j}^{(t)}\\right\\|^{2}},x_{i}^{(p)}\\right\\rangle\\right|}\\\\ &{\\quad+\\alpha\\displaystyle\\sum_{j\\in\\mathcal{F}_{1}\\backslash\\mathcal{V}_{s^{\\star}},\\ldots,\\mathcal{\\ell}}\\rho_{s}^{(t)}(j,\\widehat{p}_{j})\\left\\|\\xi_{j}^{(t)}\\right\\|^{2}\\left\\langle v_{1,1},x_{i}^{(p)}\\right\\rangle\\Big|}\\\\ &{\\quad+\\alpha\\displaystyle\\sum_{j\\in\\mathcal{F}_{-1}\\backslash\\mathcal{V}_{s^{\\star}},\\ldots,*}\\rho_{s}^{(t)}(j,\\widehat{p}_{j})\\left\\|\\xi_{j}^{(\\widehat{p}_{j})}\\right\\|^{2-2}\\left\\langle\\left\\langle v_{-1,1},x_{i}^{(p)}\\right\\rangle\\right\\rangle}\\\\ &{\\le\\tilde{\\mathcal{O}}\\left(n P\\beta^{-1}\\sigma_{d}\\sigma_{s}^{-1}d^{-\\frac{1}{2}}\\right)+\\tilde{\\mathcal{O}}\\left(\\alpha^{2}\\beta^{-1}n\\sigma_{d}^{-2}d^{-1}\\right)}\\\\ &{=o\\left(\\frac{1}{\\operatorname{Polylog}(d)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "where the last inequality is due to Lemma D.1 and the event $E_{\\mathrm{init}}$ . By Lemma F.1, ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{p\\notin\\mathcal{C}}\\left(\\phi^{\\prime}\\left(\\left\\langle w_{s^{*}}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)\\left\\langle Q_{s^{*}}\\left(w_{s^{*}}^{(t)}\\right),x_{i}^{(p)}\\right\\rangle-\\phi^{\\prime}\\left(\\left\\langle w_{-s^{*}}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)\\left\\langle Q_{-s^{*}}\\left(w_{s^{*}}^{(t)}\\right),x_{i}^{(p)}\\right\\rangle\\right)}\\\\ &{\\displaystyle\\leq\\sum_{p\\notin\\mathcal{C}}\\left(\\phi\\left(\\left\\langle w_{s^{*}}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-s^{*}}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)\\right)+r P+o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right)}\\\\ &{\\displaystyle=y_{i}f_{W^{(t)}}(X_{i,\\mathcal{C}})+o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "where the last equality is due to $\\begin{array}{r}{r=o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right)}\\end{array}$ . Therefore, we have ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|Q\\left(W^{(t)}\\right)-Q\\left(W^{*}\\right)\\right\\|^{2}-\\left\\|Q\\left(W^{(t+1)}\\right)-Q(W^{*})\\right\\|^{2}}\\\\ &{\\geq\\frac{2\\eta}{n}\\underset{i\\in\\mathbb{R}_{+}^{n},k=}{\\sum}\\mathbb{E}_{C\\sim\\mathcal{N}_{c}}\\left[\\ell^{\\prime}\\left(\\eta_{i}f_{W^{(t)}}(X_{i,c})\\right)\\left(y_{i}f_{W^{(t)}}(X_{i,c})+o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right)-\\frac{M\\beta}{2}\\right)\\right]}\\\\ &{\\quad-\\frac{\\eta^{2}}{n^{2}}\\left\\|\\nabla_{W}\\underset{i\\in\\mathbb{R}_{+}^{n},k=}{\\sum}\\mathbb{E}_{C\\sim\\mathcal{N}_{c}}[\\ell(y_{i}f_{W^{(t)}}(X_{i,c}))]\\right\\|^{2}}\\\\ &{\\geq\\frac{2\\eta}{n}\\underset{i\\in\\mathbb{R}_{+}^{n},k=}{\\sum}\\mathbb{E}_{C\\sim\\mathcal{N}_{c}}\\left[\\ell^{\\prime}(y_{i}f_{W^{(t)}}(X_{i,c}))\\left(y_{i}f_{W^{(t)}}(X_{i,c})-\\frac{M\\beta}{4}\\right)\\right]}\\\\ &{\\quad-\\frac{\\eta^{2}}{n^{2}}\\left\\|\\nabla_{W}\\underset{i\\in\\mathbb{R}_{+}^{n},k=}{\\sum}\\mathbb{E}_{C\\sim\\mathcal{N}_{c}}[\\ell(y_{i}f_{W^{(t)}}(X_{i,c}))]\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "From the convexity of $\\ell(\\cdot)$ ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i\\in\\mathcal{V}_{s^{*},k^{*}}}\\mathbb{E}_{\\mathcal{C}\\sim\\mathcal{D}_{\\mathcal{C}}}\\left[\\ell^{\\prime}\\big(y_{i}f_{W^{(t)}}(X_{i,\\mathcal{C}})\\big)\\left(y_{i}f_{W^{(t)}}(X_{i,\\mathcal{C}})-\\frac{M\\beta}{4}\\right)\\right]}\\\\ &{\\ge\\displaystyle\\sum_{i\\in\\mathcal{V}_{s^{*},k^{*}}}\\mathbb{E}_{\\mathcal{C}\\sim\\mathcal{D}_{\\mathcal{C}}}\\left[\\left(\\ell\\big(y_{i}f_{W^{(t)}}(X_{i,\\mathcal{C}})\\big)-\\ell\\left(\\frac{M\\beta}{4}\\right)\\right)\\right]}\\\\ &{\\ge\\displaystyle\\sum_{i\\in\\mathcal{V}_{s^{*},k^{*}}}\\mathbb{E}_{\\mathcal{C}\\sim\\mathcal{D}_{\\mathcal{C}}}\\left[\\ell\\big(y_{i}f_{W^{(t)}}(X_{i,\\mathcal{C}})\\big)\\right]-n e^{-\\frac{M\\beta}{4}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "In addition, by Lemma F.3, ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\frac{\\eta^{2}}{n^{2}}\\left\\|\\nabla\\sum_{i\\in\\mathcal{V}_{s^{\\ast},k^{\\ast}}}\\mathbb{E}_{\\mathcal{C}\\sim\\mathcal{D}_{\\mathcal{C}}}\\big[\\ell\\left(y_{i}f_{W^{(t)}}\\!\\left(\\pmb{X}_{i,\\mathcal{C}}\\right)\\right)\\big]\\right\\|^{2}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\frac{8\\eta^{2}P^{2}\\sigma_{\\mathrm{d}}^{2}d\\left|\\mathcal{V}_{s^{*},k^{*}}\\right|}{n^{2}}\\displaystyle\\sum_{i\\in\\mathcal{V}_{s^{*},k^{*}}}\\mathbb{E}_{\\mathcal{C}\\sim\\mathcal{D}_{\\mathcal{C}}}[\\ell(y_{i}f_{W^{(t)}}(\\boldsymbol{X}_{i,\\mathcal{C}}))]}\\\\ &{\\leq\\displaystyle\\frac{\\eta}{n}\\sum_{i\\in\\mathcal{V}_{s^{*},k^{*}}}\\mathbb{E}_{\\mathcal{C}\\sim\\mathcal{D}_{\\mathcal{C}}}[\\ell(y_{i}f_{W^{(t)}}(\\boldsymbol{X}_{i,\\mathcal{C}}))],}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "where the last inequality is due to (A8), and we have ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|Q\\left(\\pmb{W}^{(t)}\\right)-Q(\\pmb{W}^{\\ast})\\right\\|^{2}-\\left\\|Q\\left(\\pmb{W}^{(t+1)}\\right)-Q(\\pmb{W}^{\\ast})\\right\\|^{2}}\\\\ &{\\geq\\frac{\\eta}{n}\\displaystyle\\sum_{i\\in\\mathcal{V}_{s^{\\ast},k^{\\ast}}}\\mathbb{E}_{\\mathcal{C}\\sim\\mathcal{D}_{c}}[\\ell(y_{i}f_{\\pmb{W}^{(t)}}(\\pmb{X}_{i,c}))]-2\\eta e^{-\\frac{M\\beta}{4}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "From telescoping summation, we have ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\frac{\\eta}{n}\\sum_{t=T_{\\mathrm{Cutout}}}^{T^{*}}\\sum_{i\\in V_{s^{*},k^{*}}}\\mathbb{E}_{\\mathcal{C}\\sim\\mathcal{D}_{c}}\\big[\\ell\\left(y_{i}f_{W^{(t)}}\\left(X_{i,c}\\right)\\right)\\big]\\leq\\Big\\|Q\\left(W^{(T_{\\mathrm{Cutout}})}\\right)-Q\\left(W^{*}\\right)\\Big\\|^{2}+2\\eta T^{*}e^{-\\frac{M\\beta}{4}}.\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Finally, we can prove that the model cannot learn extremely rare features within $T^{*}$ iterations. ", "page_idx": 56}, {"type": "text", "text": "Lemma D.9. Suppose the event $E_{\\mathrm{init}}$ occurs. For any $T\\in[T_{\\mathrm{Cutout}},T^{*}].$ we have $\\gamma_{s}^{(T)}(s^{*},k^{*})=$ $\\widetilde{\\mathcal{O}}(\\alpha^{2}\\beta^{-2})$ for each $s\\in\\{\\pm1\\}$ ", "page_idx": 56}, {"type": "text", "text": "Proof of Lemma D.9. For any $T\\in[T_{\\mathrm{Cutout}},T^{*}]$ we have ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma_{s}^{(T)}(s^{*},k^{*})=\\gamma_{s}^{(T_{\\mathrm{Cutout}})}(s^{*},k^{*})+\\frac{\\eta}{n}\\displaystyle\\sum_{t=T_{\\mathrm{Cutout}}}^{T-1}\\sum_{i\\in\\mathcal{V}_{s^{*},k^{*}}}\\mathbb{E}_{\\mathcal{C}\\sim\\mathcal{D}_{C}}\\left[g_{i,\\mathcal{C}}^{(t)}\\cdot\\mathbb{I}_{p\\not\\in\\mathcal{C}}\\right]\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(t)},v_{s^{*},k^{*}}\\right\\rangle\\right)}\\\\ &{\\qquad\\le\\gamma_{s}^{(T_{\\mathrm{Cutout}})}(s^{*},k^{*})+\\frac{\\eta}{n}\\displaystyle\\sum_{t=T_{\\mathrm{Cutout}}}^{T-1}\\sum_{i\\in\\mathcal{V}_{s^{*},k^{*}}}\\mathbb{E}_{\\mathcal{C}\\sim\\mathcal{D}_{C}}\\left[g_{i,\\mathcal{C}}^{(t)}\\right]}\\\\ &{\\qquad\\le\\gamma_{s}^{(T_{\\mathrm{Cutout}})}(s^{*},k^{*})+\\frac{\\eta}{n}\\displaystyle\\sum_{t=T_{\\mathrm{Cutout}}}^{T-1}\\sum_{i\\in\\mathcal{V}_{s^{*},k^{*}}}\\mathbb{E}_{\\mathcal{C}\\sim\\mathcal{D}_{C}}\\left[\\ell\\left(y_{i}f_{W^{(t)}}(X_{i,\\mathcal{C}})\\right)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "where the first inequality is due to $\\phi^{\\prime}\\leq1$ and the second inequality is due to $-\\ell^{\\prime}\\leq\\ell$ . From the result of Section D.2.3,. $\\gamma_{s}^{(T_{\\mathrm{Cutout}})}(s^{*},k^{*})\\le\\alpha^{2}\\beta^{-1}$ and by Lemma D.8 and Lemma D.6, we have ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\frac{\\eta}{n}\\sum_{t=T_{\\mathrm{Cutout}}}\\sum_{i\\in\\mathcal{V}_{s^{*},k^{*}}}\\mathbb{E}_{\\mathcal{C}\\sim\\mathcal{D}_{c}}\\big[\\ell\\left(y_{i}f_{W^{(t)}}(X_{i,c})\\right)\\big]\\leq\\displaystyle\\frac{\\eta}{n}\\sum_{t=T_{\\mathrm{Cutout}}}^{(T^{*})}\\sum_{i\\in\\mathcal{V}_{s^{*},k^{*}}}\\mathbb{E}_{\\mathcal{C}\\sim\\mathcal{D}_{c}}\\big[\\ell\\left(y_{i}f_{W^{(t)}}(X_{i,c})\\right)\\big]}\\\\ {\\displaystyle}&{\\leq\\left\\lVert Q\\left(W^{(T_{\\mathrm{Cutout}})}\\right)-Q(W^{*})\\right\\rVert^{2}+2\\eta T^{*}e^{-\\frac{M\\beta}{2}}}\\\\ &{\\leq8M^{2}P|\\mathcal{V}_{s^{*},k^{*}}|\\sigma_{\\mathrm{b}}^{-2}d^{-1}+2\\eta T^{*}e^{-\\frac{M\\beta}{4}}}\\\\ &{=\\widetilde{O}\\left(\\alpha^{2}\\beta^{-2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "The last line is due to (A6) and $\\begin{array}{r}{M=4\\beta^{-1}\\log\\left(\\frac{2\\eta\\beta^{2}T^{*}}{\\alpha^{2}}\\right)}\\end{array}$ This finishes the proof. ", "page_idx": 56}, {"type": "text", "text": "What We Have So Far.  Suppose the event $E_{\\mathrm{init}}$ occurs. For any $t\\in[T_{\\mathrm{Cutout}},T^{*}]$ wehave ", "page_idx": 56}, {"type": "text", "text": "\u00b7 (Lean common/rare features): $\\gamma_{s}^{(t)}(s,k)+\\beta\\gamma_{-s}^{(t)}(s,k)\\,=\\,\\Omega(1)$ foreach $s~\\in~\\{\\pm1\\}$ and $k\\ \\in$ $\\ K_{C}\\cup K_{R}$ ", "page_idx": 56}, {"type": "text", "text": "\u00b7 (Overfit augmented data with extremely rare features or no feature): For each $i\\in[n],k\\in K_{E},\\mathcal{C}\\subset$ $\\big[P\\big]$ with $|\\bar{\\mathcal{C}}|=C$ such that (1) $i\\in\\mathcal{V}_{y_{i},k}$ and $p_{i}^{*}\\notin\\mathcal{C}$ or (2) $i\\in[n]$ and $p_{i}^{*}\\in\\mathcal{C}$ ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\sum_{p\\notin\\mathcal{C}\\cup\\{p_{i}^{*}\\}}\\left(\\rho_{y_{i}}^{(t)}(i,p)+\\beta\\rho_{-y_{i}}^{(t)}(i,p)\\right)=\\Omega(1).\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "\u00b7 (Cannot learn extreme features): $\\gamma_{s}^{(t)}(s,k),\\gamma_{-s}^{(t)}(s,k)\\,=\\,\\mathcal{O}\\left(\\alpha^{2}\\beta^{-2}\\right)$ for each $s~\\in~\\{\\pm1\\}$ and $k\\in\\kappa_{E}$ ", "page_idx": 57}, {"type": "text", "text": "\u00b7 For any $s\\in\\{\\pm1\\},i\\in[n]$ , and $p\\in[P]\\backslash\\{p_{i}^{*}\\},\\rho_{s}^{(t)}(i,p)=\\widetilde{\\mathcal{O}}\\left(\\beta^{-1}\\right)\\!,$ ", "page_idx": 57}, {"type": "text", "text": "D.2.5 Train and Test Accuracy ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "In this step, we will prove that the model trained by Cutout has perfect training accuracy on both augmented data and original data but has near-random guesses on test data with extremely rare data. ", "page_idx": 57}, {"type": "text", "text": "For any $i\\in\\mathcal{V}_{s,k}$ with $s\\in\\{\\pm1\\}$ \uff0c $k\\in K_{C}\\cup K_{\\mathcal{R}}$ and $\\mathcal{C}\\subset[P]$ with $|{\\mathcal{C}}|=C$ and $p_{i}^{*}\\notin\\mathcal{C}$ ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\displaystyle\\sum_{p\\not=C}\\left(\\phi\\left(\\left\\langle w_{s}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-s}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)\\right)}\\\\ &{=\\displaystyle\\gamma_{s}^{(t)}(s,k)+\\beta\\gamma_{-s}^{(t)}(s,k)+\\sum_{p\\not=C\\cup\\{p_{i}^{*}\\}}\\left(\\rho_{s}^{(t)}(i,p)+\\beta\\rho_{-s}^{(t)}(i,p)\\right)-2(P-C)\\cdot o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right)}\\\\ &{\\ge\\displaystyle\\gamma_{s}^{(t)}(s,k)+\\beta\\gamma_{-s}^{(t)}(s,k)-2(P-C)\\cdot o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right)}\\\\ &{=\\Omega(1)-o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "for any $t\\,\\in\\,[T_{\\mathrm{Cutout}},T^{*}]$ . In addition, for any $i\\,\\in\\,[n]$ and ${\\mathcal{C}}\\subset[P]$ With $|{\\mathcal{C}}|\\,=\\,C$ that does not correspond to the case above, by Lemma D.5 and Lemma B.4, we have ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad y_{i}f_{W^{(t)}}(X_{i,\\cdot})}\\\\ &{=\\displaystyle\\sum_{p\\notin C}\\left(\\phi\\left(\\left\\langle w_{y_{i}}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-y_{i}}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)\\right)}\\\\ &{\\ge\\displaystyle\\sum_{p\\notin C\\cup\\{p_{i}^{*}\\}}\\left(\\rho_{y_{i}}^{(t)}(i,p)+\\beta\\rho_{-y_{i}}^{(t)}(i,p)\\right)-2(P-C)\\cdot o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right)}\\\\ &{=\\Omega(1)-o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right)}\\\\ &{=\\Omega(1),}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "for any $t\\,\\in\\,[T_{\\mathrm{Cutout}},T^{*}]$ . We can conclude that Cutout with $t\\,\\in\\,[T_{\\mathrm{Cutout}},T^{*}]$ iterates achieve perfect training accuracy on augmented data. ", "page_idx": 57}, {"type": "text", "text": "Next, we will show that Cutout achieves perfect training accuracy on the original data. For any $i\\in[n]$ ,let us choose ${\\mathcal{C}}\\subset[P]$ Wwith $|{\\mathcal{C}}|=C$ such that $p_{i}^{*}\\in\\mathcal{C}$ . Then, from the result above, we have ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{y_{i}f_{W^{(t)}}(X_{i})=y_{i}f_{W^{(t)}}(X_{i,{\\mathcal{C}}})+\\displaystyle\\sum_{p\\in{\\mathcal{C}}}\\left(\\phi\\left(\\left\\langle w_{y_{i}}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-y_{i}}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\geq y_{i}f_{W^{(t)}}(X_{i,{\\mathcal{C}}})+\\displaystyle\\sum_{p\\in{\\mathcal{C}}\\backslash\\{p_{i}^{*}\\}}\\left(\\rho_{y_{i}}^{(t)}(i,p)+\\beta\\rho_{-y_{i}}^{(t)}(i,p)\\right)-C\\cdot o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right)}\\\\ &{\\qquad\\qquad\\geq\\Omega(1),}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "for any $t\\in[T_{\\mathrm{Cutout}},T^{*}]$ and we conclude that Cutout with $t\\in[T_{\\mathrm{Cutout}},T^{*}]$ iterates achieve perfect training accuracy on original data. ", "page_idx": 57}, {"type": "text", "text": "Lastly, let us move on to the test accuracy part. Let $(X,y)\\;\\sim\\;{\\mathcal{D}}$ be a test data with $\\textbf{\\em X}=$ $\\left(\\pmb{x}^{(1)},\\cdot\\,.\\,.\\,,\\pmb{x}^{(P)}\\right)\\ \\in\\ \\mathbb{R}^{d\\times P}$ having feature patch $p^{*}$ , dominant noise patch $\\tilde{p}$ , and feature vector $\\pmb{v}_{y,k}$ . We have $\\pmb{x}^{(p)}\\sim N(\\mathbf{0},\\sigma_{\\mathrm{b}}^{2}\\pmb{\\Lambda})$ for each $p\\in[P]\\setminus\\{p^{*},\\tilde{p}\\}$ and $\\pmb{x}^{(\\tilde{p})}-\\alpha\\pmb{v}_{s,1}\\sim N(\\mathbf{0},\\sigma_{\\mathrm{d}}^{2}\\pmb{\\Lambda})$ for some $s\\in\\{\\pm1\\}$ . Therefore, for ali $t\\in[T_{\\mathrm{Cutout}},T^{*}]$ and $p\\in[P]\\setminus\\{p^{*},\\tilde{p}\\}$ \uff0c ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\left|\\phi\\left(\\left\\langle\\pmb{w}_{1}^{(t)},\\pmb{x}^{(p)}\\right\\rangle\\right)-\\phi\\left(\\left\\langle\\pmb{w}_{-1}^{(t)},\\pmb{x}^{(p)}\\right\\rangle\\right)\\right|\n$$", "text_format": "latex", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\left|\\left\\langle w_{1}^{(t)}-w_{-1}^{(t)},x^{(p)}\\right\\rangle\\right|}\\\\ &{\\leq\\left|\\left\\langle w_{1}^{(0)}-w_{-1}^{(0)},x^{(p)}\\right\\rangle\\right|+\\displaystyle\\sum_{\\substack{i\\in[n],\\,q\\in[P]\\backslash\\{p_{i}^{*}\\}}}\\left|\\rho_{1}^{(t)}(i,q)-\\rho_{-1}^{(t)}(i,q)\\right|\\frac{\\left|\\left\\langle\\xi_{i}^{(q)},x^{(p)}\\right\\rangle\\right|}{\\left\\|\\xi_{i}^{(q)}\\right\\|^{2}}}\\\\ &{\\leq\\widetilde{\\mathcal{O}}\\left(\\sigma_{0}\\sigma_{\\mathrm{b}}d^{\\frac{1}{2}}\\right)+\\widetilde{\\mathcal{O}}\\left(n P\\beta^{-1}\\sigma_{\\mathrm{d}}\\sigma_{\\mathrm{b}}^{-1}d^{-\\frac{1}{2}}\\right)}\\\\ &{=o\\left(\\frac{\\alpha}{\\mathrm{polylog}(d)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "with probability at least $\\textstyle1-o\\left({\\frac{1}{\\operatorname{poly}(d)}}\\right)$ due to Lemma B.2, (A8), (8), and (9). In addition, for any $s^{\\prime}\\in\\{\\pm1\\}$ ,wehave ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left|\\left\\langle w_{s^{\\prime}}^{(t)},x^{(\\widetilde{p})}-\\alpha v_{s,1}\\right\\rangle\\right|}\\\\ &{\\le\\left|\\left\\langle w_{s^{\\prime}}^{(0)},x^{(\\widetilde{p})}-\\alpha v_{s,1}\\right\\rangle\\right|+\\displaystyle\\sum_{i\\in[n],q\\in[P]\\backslash\\{p_{i}^{*}\\}}\\rho_{s^{\\prime}}^{(t)}(i,q)\\displaystyle\\frac{\\left|\\left\\langle\\xi_{i}^{(q)},x^{(\\widetilde{p})}-\\alpha v_{s,1}\\right\\rangle\\right|}{\\left\\|\\xi_{i}^{(q)}\\right\\|^{2}}}\\\\ &{=\\widetilde{O}\\left(\\sigma_{0}\\sigma_{\\mathrm{d}}d^{\\frac{1}{2}}\\right)+\\widetilde{O}\\left(n P\\beta^{-1}\\sigma_{\\mathrm{d}}\\sigma_{\\mathrm{b}}^{-1}d^{-\\frac{1}{2}}\\right)}\\\\ &{=o\\left(\\displaystyle\\frac{\\alpha}{\\mathrm{polylog}(d)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "with probability at least $\\textstyle1-o\\left({\\frac{1}{\\operatorname{poly}(d)}}\\right)$ due to Lemma B.2, (A8), (8), and (9). ", "page_idx": 58}, {"type": "text", "text": "Case 1: $k\\in K_{C}\\cup K_{R}$ ", "page_idx": 58}, {"type": "text", "text": "By Lemma B.2, (A7), and (10), ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left|\\phi\\left(\\left\\langle w_{1}^{(t)},x^{(\\bar{p})}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-1}^{(t)},x^{(\\bar{p})}\\right\\rangle\\right)\\right|}\\\\ &{\\leq\\left|\\left\\langle w_{1}^{(t)}-w_{-1}^{(t)},x^{(\\bar{p})}\\right\\rangle\\right|}\\\\ &{\\leq\\alpha\\left|\\left\\langle w_{1}^{(t)}-w_{-1}^{(t)},v_{s,1}\\right\\rangle\\right|+\\left|\\left\\langle w_{1}^{(t)}-w_{-1}^{(t)},x^{(p)}-\\alpha v_{s,1}\\right\\rangle\\right|}\\\\ &{\\leq\\alpha\\left(\\gamma_{1}^{(t)}(s,1)+\\gamma_{-1}^{(t)}(s,1)\\right)+\\alpha\\left|\\left\\langle w_{1}^{(0)},v_{s,1}\\right\\rangle\\right|+\\alpha\\left|\\left\\langle w_{-1}^{(0)},v_{s,1}\\right\\rangle\\right|+o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right)}\\\\ &{\\leq\\tilde{O}\\left(\\alpha\\beta^{-1}\\right)+\\tilde{O}\\left(\\alpha\\sigma_{0}\\right)+o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right)}\\\\ &{=o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "with probability at least $\\textstyle1-o\\left({\\frac{1}{\\operatorname{poly}(d)}}\\right)$ . Suppose (28) and (30) holds. By Lemma B.4, we have ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{y f_{W^{(t)}}(X)}\\\\ &{=\\left(\\phi\\left(\\left\\langle w_{y}^{(t)},v_{y,k}\\right\\rangle\\right)\\ -\\phi\\left(\\left\\langle w_{-y}^{(t)},v_{y,k}\\right\\rangle\\right)\\right)}\\\\ &{\\ \\ \\ +\\displaystyle\\sum_{p\\in[P]\\backslash\\{p^{*}\\}}\\left(\\phi\\left(\\left\\langle w_{y}^{(t)},x^{(p)}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-y}^{(t)},x^{(p)}\\right\\rangle\\right)\\right)}\\\\ &{=\\gamma_{y}^{(t)}(y,k)+\\beta\\gamma_{-y}^{(t)}(y,k)-o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right)}\\\\ &{=\\Omega(1)-o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "equation", "text": "$$\n>0.\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Therefore, we have ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\mathbb{P}_{(X,y)\\sim\\mathcal{D}}\\left[y f_{W^{(t)}}(X)>0\\mid x^{(p^{*})}=v_{y,k},k\\in K_{C}\\cup K_{R}\\right]\\geq1-o\\left(\\frac{1}{\\mathrm{poly}(d)}\\right).\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Case 2: $k\\in\\kappa_{E}$ ", "page_idx": 59}, {"type": "text", "text": "By triangular inequality and $\\phi^{\\prime}\\leq1$ , we have ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\phi\\left(\\left\\langle w_{s}^{(t)},x^{(\\tilde{p})}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-s}^{(t)},x^{(\\tilde{p})}\\right\\rangle\\right)}\\\\ &{=\\phi\\left(\\left\\langle w_{s}^{(t)},\\alpha v_{s,1}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-s}^{(t)},\\alpha v_{s,1}\\right\\rangle\\right)}\\\\ &{\\quad+\\left(\\phi\\left(\\left\\langle w_{s}^{(t)},x^{(\\tilde{p})}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{s}^{(t)},\\alpha v_{s,1}\\right\\rangle\\right)\\right)-\\left(\\phi\\left(\\left\\langle w_{-s}^{(t)},x^{(\\tilde{p})}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-s}^{(t)},\\alpha v_{s,1}\\right\\rangle\\right)\\right)}\\\\ &{\\geq\\phi\\left(\\left\\langle w_{s}^{(t)},\\alpha v_{s,1}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-s}^{(t)},\\alpha v_{s,1}\\right\\rangle\\right)}\\\\ &{\\quad-\\left|\\left\\langle w_{s}^{(t)},x^{(\\tilde{p})}-\\alpha v_{s,1}\\right\\rangle\\right|-\\left|\\left\\langle w_{-s}^{(t)},x^{(\\tilde{p})}-\\alpha v_{s,1}\\right\\rangle\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "In addition, ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phi\\left(\\left\\langle w_{s}^{(t)},\\alpha v_{s,1}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-s}^{(t)},\\alpha v_{s,1}\\right\\rangle\\right)}\\\\ &{=\\left(\\phi\\left(x_{s}^{(t)}(s,1)\\right)-\\phi\\left(-\\alpha v_{-s}^{(t)}(s,1)\\right)\\right)}\\\\ &{\\quad+\\left(\\phi\\left(\\left\\langle w_{s}^{(t)},\\alpha v_{s,1}\\right\\rangle\\right)-\\phi\\left(x_{s}^{(t)}(s,1)\\right)\\right)}\\\\ &{\\quad-\\left(\\phi\\left(\\left\\langle w_{-s}^{(t)},\\alpha v_{s,1}\\right\\rangle\\right)-\\phi\\left(-\\alpha\\gamma_{-s}^{(t)}(s,1)\\right)\\right)}\\\\ &{\\geq\\left(\\phi\\left(x_{s}^{(t)}(s,1)\\right)-\\phi\\left(-\\alpha\\gamma_{-s}^{(t)}(s,1)\\right)\\right)}\\\\ &{\\quad-\\alpha\\left|\\left\\langle w_{s}^{(t)},v_{s,1}\\right\\rangle-\\gamma_{s}^{(t)}(s,1)\\right|-\\alpha\\left|\\left\\langle w_{-s}^{(t)},v_{s,1}\\right\\rangle+\\gamma_{-s}^{(t)}(s,1)\\right|}\\\\ &{=\\alpha\\left(\\gamma_{s}^{(t)}(s,1)+\\beta\\gamma_{-s}^{(t)}(s,1)\\right)-\\alpha\\cdot o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right)}\\\\ &{=\\Omega(\\alpha),}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "where the second equality is due to Lemma B.4 and (A8). If (29) holds, we have ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\phi\\left(\\left\\langle\\mathbf{w}_{s}^{(t)},\\mathbf{x}^{(\\tilde{p})}\\right\\rangle\\right)-\\phi\\left(\\left\\langle\\mathbf{w}_{-s}^{(t)},\\mathbf{x}^{(\\tilde{p})}\\right\\rangle\\right)=\\Omega(\\alpha)-o\\left(\\frac{\\alpha}{\\mathrm{polylog}(d)}\\right)=\\Omega(\\alpha).\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Note that ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad y f_{W^{(t)}}(X)}\\\\ &{=\\phi\\left(\\left\\langle w_{y}^{(t)},v_{y,k}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-y}^{(t)},v_{y,k}\\right\\rangle\\right)+\\phi\\left(\\left\\langle w_{y}^{(t)},x^{(\\tilde{p})}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-y}^{(t)},x^{(\\tilde{p})}\\right\\rangle\\right)}\\\\ &{\\quad+\\displaystyle\\sum_{p\\in[P]\\backslash\\{p^{*},\\tilde{p}\\}}\\left(\\phi\\left(\\left\\langle w_{y}^{(t)},x^{(p)}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-y}^{(t)},x^{(p)}\\right\\rangle\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "and ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\phi\\left(\\left\\langle w_{y}^{(t)},v_{y,k}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-y}^{(t)},v_{y,k}\\right\\rangle\\right)\\right|}\\\\ &{\\ +\\left|\\displaystyle\\sum_{p\\in[P]\\backslash\\{p^{*},\\tilde{p}\\}}\\left(\\phi\\left(\\left\\langle w_{y}^{(t)},x^{(p)}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-y}^{(t)},x^{(p)}\\right\\rangle\\right)\\right)\\right|}\\\\ &{\\leq\\left|\\left\\langle w_{y}^{(t)}-w_{-y}^{(t)},v_{y,k}\\right\\rangle\\right|+o\\left(\\displaystyle\\frac{\\alpha}{\\mathrm{polylog}(d)}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\le\\gamma_{1}^{(t)}(y,k)+\\gamma_{-1}^{(t)}(y,k)+\\left|\\left\\langle w_{y}^{(0)}-w_{-y}^{(0)},v_{y,k}\\right\\rangle\\right|+o\\left(\\frac{\\alpha}{\\mathrm{polylog}(d)}\\right)}\\\\ &{\\le\\mathcal{O}(\\alpha^{2}\\beta^{-2})+\\widetilde{\\mathcal{O}}(\\sigma_{0})+o\\left(\\frac{\\alpha}{\\mathrm{polylog}(d)}\\right)}\\\\ &{=o\\left(\\frac{\\alpha}{\\mathrm{polylog}(d)}\\right)}\\\\ &{<\\phi\\left(\\left\\langle w_{s}^{(t)},x^{(\\widetilde{p})}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-s}^{(t)},x^{(\\widetilde{p})}\\right\\rangle\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "where the first inequality is due to (28), the second-to-last line is due to (A8), (8), and (10) , and the last inequality is due to (32). Therefore, we have $y f_{W^{(t)}}(\\pmb{X})>0$ if $y=s$ .Otherwise, $y f_{W^{(t)}}(\\boldsymbol{X})<0$ ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\mathbb{P}_{(X,y)\\sim\\mathcal{D}}\\left[y f_{W^{(t)}}(X)>0\\mid x^{(p^{*})}=v_{y,k},k\\in K_{E}\\right]=\\frac{1}{2}\\pm o\\left(\\frac{1}{\\mathrm{poly}(d)}\\right).\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Hence, combining (31) and (33) implies ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\mathbb{P}_{(X,y)\\sim\\mathcal{D}}\\left[y f_{W^{(t)}}(X)>0\\right]=\\sum_{k\\in K_{C}\\cup K_{R}}\\rho_{k}+\\frac{1}{2}\\left(1-\\sum_{k\\in K_{C}\\cup K_{R}}\\rho_{k}\\right)\\pm o\\left(\\frac{1}{\\mathrm{poly}(d)}\\right)\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "E Proof for CutMix ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "E.1Proof of Lemma B.3 for CutMix ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "For each $i,j\\in[n]$ and $\\mathcal S\\subset[P]$ , let ", "page_idx": 61}, {"type": "equation", "text": "$$\ng_{i,j,S}^{(t)}:=-\\frac{\\left|S\\right|}{P}y_{i}\\ell^{\\prime}\\!\\left(y_{i}f_{W^{(t)}}(X_{i,j,S})\\right)-\\left(1-\\frac{\\left|S\\right|}{P}\\right)y_{j}\\ell^{\\prime}\\!\\left(y_{j}f_{W^{(t)}}(X_{i,j,S})\\right)\\!.\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "For $s\\in\\{\\pm1\\}$ and iterate $t$ ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w_{s}^{(t+1)}-w_{s}^{(t)}}\\\\ &{=-\\eta\\nabla_{w_{s}}\\mathcal{L}_{\\mathrm{culMix}}\\left(W^{(t)}\\right)}\\\\ &{=\\frac{\\eta}{n^{2}}\\displaystyle\\sum_{i,j\\in[n]}\\mathbb{E}_{s\\sim\\mathcal{D}_{s}}\\left[s g_{i,j,s}^{(t)}\\left(\\sum_{p\\in\\mathcal{S}}\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)x_{i}^{(p)}+\\sum_{p\\notin\\mathcal{S}}\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)x_{j}^{(p)}\\right)\\right]}\\\\ &{=\\frac{s\\eta}{n^{2}}\\displaystyle\\sum_{s^{\\prime}\\in\\{\\pm1\\},k\\in[K]}\\sum_{k\\in[K]}\\mathbb{E}_{s\\sim\\mathcal{D}_{s}}\\left[\\mathcal{G}_{i,j,s}^{(t)}\\mathbb{I}_{p_{i}^{*}\\in\\mathcal{S}}+g_{j,i,s}^{(t)}\\mathbb{I}_{p_{i}^{*}\\notin\\mathcal{S}}\\right]\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(t)},v_{s^{\\prime},k}^{(t)}\\right\\rangle\\right)v_{s^{\\prime},k}}\\\\ &{\\quad+\\frac{s\\eta}{n^{2}}\\displaystyle\\sum_{i,j\\in[n],p\\in[P]\\backslash\\{p\\}\\setminus\\{s\\}}\\mathbb{E}_{s\\sim\\mathcal{D}_{s}}\\left[\\mathcal{G}_{i,j,s}^{(t)}\\mathbb{I}_{p\\in\\mathcal{S}}+g_{j,i,s}^{(t)}\\mathbb{I}_{p\\notin\\mathcal{S}}\\right]\\phi^{\\prime}\\left(\\left\\langle w_{s}^{(t)},x_{i}^{(p)}\\right\\rangle\\right)x_{i}^{(p)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "Hence, if we define $\\gamma_{s}^{(t)}(s^{\\prime},k)$ 's and $\\rho_{s}^{(t)}(i,p)$ 's recursively by using the rule ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "starting from $\\gamma_{s}^{(0)}(s^{\\prime}k)=\\rho_{s}^{(0)}(i,p)=0$ for each $s,s^{\\prime}\\in\\{\\pm1\\},k\\in[K],i\\in[n]$ and $p\\in[P]\\setminus\\{p_{i}^{*}\\}$ then we have ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w_{s}^{(t)}=w_{s}^{(0)}+\\displaystyle\\sum_{k\\in[K]}\\gamma_{s}^{(t)}(s,k)v_{s,k}-\\displaystyle\\sum_{k\\in[K]}\\gamma_{s}^{(t)}(-s,k)v_{-s,k}}\\\\ &{\\qquad+\\displaystyle\\sum_{\\substack{p\\in[P]\\setminus\\{\\tilde{\\ell}_{i}\\}}}\\rho_{s}^{(t)}(i,p)\\frac{\\xi_{i}^{(p)}}{\\left\\|\\xi_{i}^{(p)}\\right\\|^{2}}-\\displaystyle\\sum_{\\substack{p\\in[P]\\setminus\\{\\tilde{\\ell}_{i}\\}}}\\rho_{s}^{(t)}(i,p)\\frac{\\xi_{i}^{(p)}}{\\left\\|\\xi_{i}^{(p)}\\right\\|^{2}}}\\\\ &{\\qquad+\\alpha\\left(\\displaystyle\\sum_{i\\in\\mathcal{F}_{s}}s y_{i}\\rho_{s}^{(t)}(i,\\tilde{p}_{i})\\frac{v_{s,1}}{\\left\\|\\xi_{i}^{(\\tilde{p}_{i})}\\right\\|^{2}}+\\displaystyle\\sum_{i\\in\\mathcal{F}_{s-s}}s y_{i}\\rho_{s}^{(t)}(i,\\tilde{p}_{i})\\frac{v_{-s,1}}{\\left\\|\\xi_{i}^{(\\tilde{p}_{i})}\\right\\|^{2}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "for each $s\\in\\{\\pm1\\}$ ", "page_idx": 61}, {"type": "text", "text": "E.2Proof of Theorem 3.3 ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "We will prove that the conclusion of Theorem 3.3 holds when the event $E_{\\mathrm{init}}$ occurs. The proof of Theorem 3.3 is structured into the following six steps: ", "page_idx": 61}, {"type": "text", "text": "1. Introduce a reparametrization of the CutMix loss $\\mathcal{L}_{\\mathrm{CutMix}}(W)$ to a convex function $h(Z)$ for ease of analysis (Section E.2.1).   \n2. Characterize a global minimum of $h(Z)$ (Section E.2.2).   \n3. Evaluate strong convexity constant in the region near the global minimum of $h(Z)$ (Section E.2.3).   \n4. Show that near stationary point of $h(Z)$ is close to a global minimum (Section E.2.4).   \n5. Prove that gradient descent on the CutMix loss $\\mathcal{L}_{\\mathrm{CutMix}}(W)$ achieves a near-stationary point of the reparametrized function $h(Z)$ and perfect accuracy on original training data (Section E.2.5).   \n6. Evaluate the test accuracy of a model in near-stationary point (Section E.2.6). ", "page_idx": 61}, {"type": "text", "text": "E.2.1 Reparametrization of CutMix Loss ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "It is complicated to characterize the stationary points of CutMix loss $\\mathcal{L}_{\\mathrm{CutMix}}(W)$ due to its nonconvexity. We will overcome this problem by introducing reparameterization of the objective function. Let us define ", "page_idx": 62}, {"type": "equation", "text": "$$\nz_{i}^{(p)}:=\\phi\\left(\\left\\langle\\pmb{w}_{1},\\pmb{x}_{i}^{(p)}\\right\\rangle\\right)-\\phi\\left(\\left\\langle\\pmb{w}_{-1},\\pmb{x}_{i}^{(p)}\\right\\rangle\\right),\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "for $i\\in[n],p\\in[P]$ and ", "page_idx": 62}, {"type": "equation", "text": "$$\nz_{s,k}:=\\phi(\\langle\\pmb{w}_{1},\\pmb{v}_{s,k}\\rangle)-\\phi(\\langle\\pmb{w}_{-1},\\pmb{v}_{s,k}\\rangle),\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "for each $s\\in\\{\\pm1\\},k\\in[K]$ . We can rewrite CutMix loss $\\mathcal{L}_{\\mathrm{CutMix}}(W)$ as a function $h(Z)$ of the defned variables $Z:=\\{z_{s,k}\\}_{s\\in\\{\\pm1\\},k\\in[K]}\\cup\\{z_{i}^{(p)}\\}_{i\\in[n],p\\in[P]\\backslash\\{p_{i}^{*}\\}}$ as folws. ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h(\\pmb{Z}):=\\frac{1}{n^{2}}\\displaystyle\\sum_{i,j\\in[n]}\\mathbb{E}_{\\pmb{S}\\sim\\mathcal{D}_{\\pmb{S}}}\\left[\\frac{|\\pmb{S}|}{P}\\ell\\left(y_{i}\\left(\\displaystyle\\sum_{p\\in\\mathcal{S}}z_{i}^{(p)}+\\displaystyle\\sum_{p\\not\\in\\mathcal{S}}z_{j}^{(p)}\\right)\\right)\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\left.+\\left(1-\\frac{|\\pmb{S}|}{P}\\right)\\ell\\left(y_{j}\\left(\\displaystyle\\sum_{p\\in\\mathcal{S}}z_{i}^{(p)}+\\displaystyle\\sum_{p\\not\\in\\mathcal{S}}z_{j}^{(p)}\\right)\\right)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "where we wrie (gt $z_{i}^{(p_{i}^{*})}\\,=\\,z_{s,k}$ if $i\\in\\mathcal{V}_{s,k}$ . For notational simplicity, let us consider $_{z}$ as vectors .n $\\mathbb{R}^{2K+n(P-1)}$ with the standard orthonormal basis $\\{e_{s,k}\\}_{s\\in\\{\\pm1\\},k\\in[K]}\\cup\\left\\{e_{i}^{(p)}\\right\\}_{i\\in[n],p\\in[P]\\backslash\\{p_{i}^{*}\\}}$ which means ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\displaystyle{\\cal Z}=\\{z_{s,k}\\}_{s\\in\\{\\pm1\\},k\\in[K]}\\cup\\Big\\{z_{i}^{(p)}\\Big\\}_{i\\in[n],p\\in[P]\\backslash\\{p_{i}^{*}\\}}}\\\\ &{\\quad=\\displaystyle\\sum_{s\\in\\{\\pm1\\},k\\in[K]}z_{s,k}e_{s,k}+\\displaystyle\\sum_{i\\in[n],p\\in[P]\\backslash\\{p_{i}^{*}\\}}z_{i}^{(p)}e_{i}^{(p)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "If there is no confusion, we willuse e e(pi) to represent es,k, for i E Vs,k. ", "page_idx": 62}, {"type": "text", "text": "By the chain rule, ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\nabla_{W}\\mathcal{L}_{\\mathrm{CutMix}}(W)=J(W)\\nabla_{Z}h(Z),\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "where each column of Jacobian matrix $J(W)\\in\\mathbb{R}^{2d\\times(n(P-1)+2K)}$ is ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla_{W}z_{s,k}=\\left(\\begin{array}{l}{\\phi^{\\prime}(\\langle w_{1},v_{s,k}\\rangle)v_{s,k}}\\\\ {-\\phi^{\\prime}(\\langle w_{-1},v_{s,k}\\rangle)v_{s,k}}\\end{array}\\right)\\in\\mathbb{R}^{2d},\\nabla_{W}z_{i}^{(p)}=\\left(\\begin{array}{l}{\\phi^{\\prime}\\left(\\left\\langle w_{1},x_{i}^{(p)}\\right\\rangle\\right)x_{i}^{(p)}}\\\\ {-\\phi^{\\prime}\\left(\\left\\langle w_{-1},x_{i}^{(p)}\\right\\rangle\\right)x_{i}^{(p)}}\\end{array}\\right)\\in\\mathbb{R}^{2d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "Let us characterize the smallest singular value $\\sigma_{\\mathrm{min}}(J(W))$ of the Jacobian matrix $J(W)$ . For any unit vector $c=\\{c_{s,k}\\}_{s\\in\\{\\pm1\\},k\\in[K]}\\cup\\left\\{c_{i}^{(p)}\\right\\}_{i\\in[n],p\\in[P]\\backslash\\{p_{i}^{*}\\}}\\in\\mathbb{R}^{2K+n(P-1)},$ wehave ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|J(W)c\\|^{2}=}&{\\displaystyle\\sum_{\\substack{s\\in\\{\\pm1\\},\\,k\\in\\{K\\}}}c_{s,k}^{2}\\left\\|\\nabla_{W}z_{s,k}\\right\\|^{2}+\\displaystyle\\sum_{\\substack{i\\in[n],\\,\\rho\\in[P]\\backslash\\{s_{i}^{\\prime}\\}}}\\left(c_{i}^{(p)}\\right)^{2}\\left\\|\\nabla_{W}z_{i}^{(p)}\\right\\|^{2}}\\\\ &{+\\displaystyle\\sum_{\\substack{s\\in\\{\\pm1\\},\\,k\\in\\{K\\}}}c_{s,1,k}^{(p)}c_{s,2,k}\\langle\\nabla_{W}z_{s,1,k_{1}},\\nabla_{W}z_{s,2,k_{2}}\\rangle}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad c_{s,1,k}c_{s,2,k}\\langle\\mathbf{k}\\rangle}\\\\ &{+2\\,\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad c_{s,k}c_{i}^{(p)}\\left\\langle\\nabla_{W}z_{s,k},\\nabla_{W}z_{i}^{(p)}\\right\\rangle}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,}\\\\ &{+\\displaystyle\\sum_{i\\in[n],\\,\\rho\\in[P]\\backslash\\{p\\}\\setminus\\{\\phi\\}}}\\\\ &{+\\displaystyle\\sum_{\\{i:[n],\\,\\phi\\in[P]\\}\\setminus\\{\\phi\\}}c_{i}^{(p)}\\left\\langle\\nabla_{W}z_{i}^{(p)}\\right\\rangle\\langle\\nabla_{W}z_{i}^{(p)},\\nabla_{W}z_{j}^{(q)}\\rangle\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "For each $s_{1},s_{2}\\in\\{\\pm1\\},k_{1},k_{2}\\in[K]$ such that $(s_{1},k_{1})\\neq(s_{2},k_{2})$ , and $i\\in[n],p\\in[P]\\setminus\\{p_{i}^{*},\\tilde{p}_{i}\\}$ ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\left\\langle\\nabla_{W}z_{s_{1},k_{1}},\\nabla_{W}z_{s_{2},k_{2}}\\right\\rangle=\\left\\langle\\nabla_{W}z_{s_{1},k_{1}},\\nabla_{W}z_{i}^{(p)}\\right\\rangle=0,\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "and if $k_{1}>1$ ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\left\\langle\\nabla_{W}z_{s_{1},k_{1}},\\nabla_{W}z_{i}^{(p)}\\right\\rangle=\\left\\langle\\nabla_{W}z_{s_{1},k_{1}},\\nabla_{W}z_{i}^{(\\widetilde{p}_{i})}\\right\\rangle=0,\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "since $\\left\\langle v_{s_{1},k_{1}},v_{s_{2},k_{2}}\\right\\rangle=\\left\\langle v_{s_{1},k_{1}},\\xi_{i}^{(p)}\\right\\rangle=\\left\\langle v_{s_{1},k_{1}},\\xi_{i}^{(p_{i})}\\right\\rangle=0$ Also, for each $s\\in\\{\\pm1\\}$ and $i\\in\\mathcal{F}_{s}$ \uff0c then ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad2\\left|c_{s,1}c_{i}^{(\\bar{\\beta}_{1})}\\left\\langle\\nabla_{W}z_{s,1},\\nabla_{W}z_{i}^{(\\bar{\\beta}_{1})}\\right\\rangle\\right|}\\\\ &{=2\\left|c_{s,1}c_{i}^{(\\bar{\\beta}_{1})}\\left|\\left(\\phi^{\\prime}(\\langle w_{1},v_{s,1}\\rangle)\\phi^{\\prime}\\left(\\left\\langle w_{1},x_{i}^{(\\bar{\\beta}_{1})}\\right\\rangle\\right)+\\phi^{\\prime}(\\langle w_{-1},v_{s,1}\\rangle)\\phi^{\\prime}\\left(\\left\\langle w_{-1},x_{i}^{(\\bar{\\beta}_{1})}\\right\\rangle\\right)\\right)\\alpha\\right|}\\\\ &{\\leq4c_{s,1}^{2}\\left(\\phi^{\\prime}(\\langle w_{1},v_{s,1}\\rangle)^{2}+\\phi^{\\prime}(\\langle w_{-1},v_{s,1}\\rangle)^{2}\\right)\\frac{\\alpha^{2}}{\\left\\|\\mathbf{x}_{i}^{(\\bar{\\beta}_{1})}\\right\\|^{2}}}\\\\ &{\\quad+\\frac{1}{4}\\left(c_{i}^{(\\bar{\\beta}_{1})}\\right)^{2}\\left(\\phi^{\\prime}\\left(\\left\\langle w_{1},x_{i}^{(\\bar{\\beta}_{1})}\\right\\rangle\\right)^{2}+\\phi^{\\prime}\\left(\\left\\langle w_{-1},x_{i}^{(\\bar{\\beta}_{1})}\\right\\rangle\\right)^{2}\\right)\\left\\|\\mathbf{x}_{i}^{(\\bar{\\beta}_{1})}\\right\\|^{2}}\\\\ &{<\\frac{1}{2\\alpha}c_{s,1}^{2}\\left(\\phi^{\\prime}(\\langle w_{1},v_{s,1}\\rangle)^{2}+\\phi^{\\prime}(\\langle w_{-1},v_{s,1}\\rangle)^{2}\\right)}\\\\ &{\\quad+\\frac{1}{4}\\left(c_{i}^{(\\bar{\\beta}_{1})}\\right)^{2}\\left(\\phi^{\\prime}\\left(\\left\\langle w_{1},x_{i}^{(\\bar{\\beta}_{1})}\\right\\rangle\\right)^{2}+\\phi^{\\prime}\\left(\\left\\langle w_{-1},x_{i}^{(\\bar{\\beta}_{1})}\\right\\rangle\\right)^{2}\\right)\\left\\|\\mathbf{x}_{i}^{(\\bar{\\beta}_{1})}\\right\\|^{2 \n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "where the last inequality holds since ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\left\\|\\pmb{x}_{i}^{(\\widetilde{p}_{i})}\\right\\|^{2}=\\alpha^{2}+\\left\\|\\xi_{i}^{(\\widetilde{p}_{i})}\\right\\|^{2}\\ge\\frac{1}{2}\\sigma_{\\mathrm{d}}^{2}d=\\omega(n\\alpha^{2}),\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "where we apply the fact from the event $E_{\\mathrm{init}}$ defined in Lemma B.2 and (A7). Also, ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\left\\langle\\nabla_{W}z_{-s,1},\\nabla_{W}z_{i}^{(\\tilde{p}_{i})}\\right\\rangle=0.\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "Furthermore, for each $i,j\\in[n],p\\in[P]\\setminus\\{p_{i}^{*}\\},q\\in[P]\\setminus\\{p_{j}^{*}\\}$ with $(i,p)\\neq(j,q)$ satisfies ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|c_{i}^{(p)}c_{j}^{(q)}\\left\\langle\\nabla_{W}z_{i}^{(p)},\\nabla_{W}z_{j}^{(q)}\\right\\rangle\\right|}\\\\ &{=\\left|c_{i}^{(p)}c_{j}^{(q)}\\right|\\left(\\phi^{\\prime}\\left(\\left\\langle w_{1},x_{i}^{(p)}\\right\\rangle\\right)\\phi^{\\prime}\\left(\\left\\langle w_{1},x_{j}^{(q)}\\right\\rangle\\right)+\\phi^{\\prime}\\left(\\left\\langle w-1,x_{i}^{(p)}\\right\\rangle\\right)\\phi^{\\prime}\\left(\\left\\langle w-1,x_{j}^{(q)}\\right\\rangle\\right)\\right)\\left|\\left\\langle x_{i}^{(p)},x_{j}^{(q)}\\right\\rangle\\right|}\\\\ &{\\leq\\frac{1}{4P n}\\left(c_{i}^{(p)}\\right)^{2}\\left(\\phi^{\\prime}\\left(\\left\\langle w_{1},x_{i}^{(p)}\\right\\rangle\\right)^{2}+\\phi^{\\prime}\\left(\\left\\langle w-1,x_{i}^{(p)}\\right\\rangle\\right)^{2}\\right)\\left\\|\\mathbf{x}_{i}^{(p)}\\right\\|^{2}}\\\\ &{\\quad+\\frac{1}{4P n}\\left(c_{j}^{(q)}\\right)^{2}\\left(\\phi^{\\prime}\\left(\\left\\langle w_{1},x_{j}^{(q)}\\right\\rangle\\right)^{2}+\\phi^{\\prime}\\left(\\left\\langle w-1,x_{j}^{(q)}\\right\\rangle\\right)^{2}\\right)\\left\\|\\mathbf{x}_{j}^{(q)}\\right\\|^{2}}\\\\ &{=\\frac{1}{4P n}\\left(\\left(c_{i}^{(p)}\\right)^{2}\\left\\|\\nabla w z_{i}^{(p)}\\right\\|^{2}+\\left(c_{j}^{(q)}\\right)^{2}\\left\\|\\nabla w z_{j}^{(q)}\\right\\|^{2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "where the last inequality is due to AM-GM inequality and ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\left\\|\\pmb{x}_{i}^{(p)}\\right\\|\\cdot\\left\\|\\pmb{x}_{j}^{(q)}\\right\\|\\geq2n P\\left|\\left\\langle\\pmb{x}_{i}^{(p)},\\pmb{x}_{j}^{(q)}\\right\\rangle\\right|,\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "which we show through a case analysis. For the case $p=\\tilde{p}_{i}$ and $q=\\tilde{p}_{j}$ , this inequality holds since ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|{\\boldsymbol x}_{i}^{(p)}\\right\\|\\cdot\\left\\|{\\boldsymbol x}_{j}^{(q)}\\right\\|\\geq\\left\\|\\xi_{i}^{(p)}\\right\\|\\cdot\\left\\|\\xi_{j}^{(q)}\\right\\|\\geq2n P\\left(\\left|\\left\\langle\\xi_{i}^{(p)},\\xi_{j}^{(q)}\\right\\rangle\\right|+\\alpha^{2}\\right)\\geq2n P\\left|\\left\\langle{\\boldsymbol x}_{i}^{(p)},{\\boldsymbol x}_{j}^{(q)}\\right\\rangle\\right|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "where the second inequality is due to ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\left\\|\\xi_{i}^{(p)}\\right\\|\\cdot\\left\\|\\xi_{j}^{(q)}\\right\\|\\geq2n P\\left|\\left\\langle\\xi_{i}^{(p)},\\xi_{j}^{(q)}\\right\\rangle\\right|,\\quad\\frac{1}{2}\\left\\|\\xi_{i}^{(p)}\\right\\|\\cdot\\left\\|\\xi_{j}^{(q)}\\right\\|\\geq2n P\\alpha^{2}.\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "which is implied by the fact from the event $E_{\\mathrm{init}}$ defined in Lemma B.2, (A1), (A2), and (A7). In the remaining case, ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|{\\boldsymbol x}_{i}^{(p)}\\right\\|\\cdot\\left\\|{\\boldsymbol x}_{j}^{(q)}\\right\\|\\geq\\left\\|\\xi_{i}^{(p)}\\right\\|\\cdot\\left\\|\\xi_{j}^{(q)}\\right\\|\\geq2n P\\left|\\left\\langle\\xi_{i}^{(p)},\\xi_{j}^{(q)}\\right\\rangle\\right|=2n P\\left|\\left\\langle{\\boldsymbol x}_{i}^{(p)},{\\boldsymbol x}_{j}^{(q)}\\right\\rangle\\right|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "where the second inequality is due to the fact from event $E_{\\mathrm{init}}$ defined in Lemma B.2, (A1), and (A2). For $s\\in\\{\\pm1\\},k\\in[\\bar{K}]$ and $i\\in[n],p\\in[P]\\setminus\\{p_{i}^{*}\\}$ ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\|\\nabla_{W}z_{s,k}\\|^{2}=\\phi^{\\prime}(\\langle w_{1},v_{s,k}\\rangle)^{2}+\\phi^{\\prime}(\\langle w_{-1},v_{s,k}\\rangle)^{2}\\geq2\\beta^{2},\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "and ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\nabla_{W}z_{i}^{(p)}\\right\\|^{2}=\\left(\\phi^{\\prime}\\left(\\left\\langle w_{1},x_{i}^{(p)}\\right\\rangle\\right)^{2}+\\phi^{\\prime}\\left(\\left\\langle w_{-1},x_{i}^{(p)}\\right\\rangle\\right)^{2}\\right)\\left\\|x_{i}^{(p)}\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\geq\\beta^{2}\\sigma_{i,p}^{2}d}\\\\ &{\\qquad\\qquad\\geq\\beta^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "where the last inequality is due to (8). By merging all inequalities together, we have ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\underset{s\\in\\{1,2,4\\}}{\\sum_{{\\mathbf{i}}\\in\\{1,1\\},\\{\\mathbf{i}\\}\\cap\\{\\mathbf{r}^{2},\\mathbf{i}\\}}}\\left(\\underset{\\bar{\\tau}\\in\\{1,3\\}}{\\sum_{{\\mathbf{i}}\\in\\{1,2,4\\}}}\\underset{\\bar{\\tau}\\in\\{1,3\\}}{\\sum_{{\\mathbf{i}}\\in\\{1,2,3\\}}}\\left(\\binom{\\ell\\beta}{\\mathbf{i}}\\right)^{2}\\left|\\nabla{\\mathbf{w}}_{\\bar{\\tau}^{2}}\\bar{\\mathbf{i}}\\right|\\right)^{2}}\\\\ &{\\quad+\\underset{s\\in\\{1,4\\}}{\\sum_{{\\mathbf{i}}\\in\\{1,2,5\\}}}c_{s,1}c_{\\mu}^{(2)}\\left\\langle\\nabla{\\mathbf{w}}_{\\bar{\\tau}^{2}\\times\\mathbf{i}}\\nabla\\cdot\\mathbf{w}^{\\top}\\mathfrak{d}^{(\\mu)}\\right\\rangle+\\underset{\\mu_{1}\\in\\{1,4\\}}{\\sum_{\\mathbf{i}\\in\\{1,2,4\\}}}c_{s,2}^{(\\mu)}\\psi_{s,\\mu_{2}^{(2)}}\\left\\langle\\nabla{\\mathbf{w}}_{\\bar{\\tau}^{2}\\cup\\cdot\\nabla}\\mathfrak{d}^{(\\mu)}\\right\\rangle}\\\\ &{\\quad+\\underset{s\\in\\{1,4\\}}{\\sum_{{\\mathbf{i}}\\in\\{1,3\\},\\{\\mathbf{i}\\}\\cap\\{\\mathbf{r}^{2},\\mathbf{i}\\}}}\\mathrm{~}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "and we conclude $\\begin{array}{r}{\\sigma_{\\operatorname*{min}}(J(W))\\geq\\frac{\\beta}{2}}\\end{array}$ for any $W$ ", "page_idx": 64}, {"type": "text", "text": "E.2.2 Characterization of a Global Minimum of CutMix Loss ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "In this section, we will check that $h(Z)$ is strictly convex and it has a global minimum. ", "page_idx": 64}, {"type": "text", "text": "For each $i,j\\in[n]$ and $\\mathcal S\\subset[P]$ let us define $\\mathbf{a}_{i,j,S}\\in\\mathbb{R}^{2K+n(P-1)}$ as ", "page_idx": 64}, {"type": "equation", "text": "$$\n{a}_{i,j,S}=\\sum_{p\\in\\cal{S}}{e}_{i}^{(p)}+\\sum_{p\\notin\\cal{S}}{e}_{j}^{(p)},\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "and then ", "page_idx": 64}, {"type": "equation", "text": "$$\nh(\\pmb{Z})=\\frac{1}{n^{2}}\\sum_{i,j\\in[n]}\\mathbb{E}_{S\\sim\\mathcal{D}_{S}}\\left[\\frac{|\\pmb{S}|}{P}\\ell\\left(y_{i}\\langle\\pmb{a}_{i,j,S},\\pmb{Z}\\rangle\\right)+\\left(1-\\frac{|\\pmb{S}|}{P}\\right)\\ell\\left(y_{j}\\langle\\pmb{a}_{i,j,S},\\pmb{Z}\\rangle\\right)\\right].\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "Since $\\ell(\\cdot)$ is convex, $h(Z)$ is also convex. Note that ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\7h(Z)=\\frac{1}{n^{2}}\\sum_{i,j\\in[n]}\\mathbb{E}_{S\\sim\\mathcal{D}_{S}}\\left[\\left(\\frac{|S|}{P}y_{i}\\ell^{\\prime}(y_{i}\\langle a_{i,j,S},Z\\rangle)+\\left(1-\\frac{|S|}{P}\\right)y_{j}\\ell^{\\prime}(y_{j}\\langle a_{i,j,S},Z\\rangle)\\right)a_{i,j,S}\\right],\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "and ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\nabla^{2}h(Z)\n$$", "text_format": "latex", "page_idx": 64}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\displaystyle\\frac{1}{n^{2}}\\sum_{i,j\\in[n]}\\mathbb{E}_{\\boldsymbol{S}\\sim\\mathcal{D}_{\\boldsymbol{S}}}\\left[\\left(\\frac{|\\boldsymbol{S}|}{P}\\ell^{\\prime\\prime}(y_{i}\\langle a_{i,j,\\boldsymbol{S}},\\boldsymbol{Z}\\rangle)+\\left(1-\\frac{|\\boldsymbol{S}|}{P}\\right)\\ell^{\\prime\\prime}(y_{j}\\langle a_{i,j,\\boldsymbol{S}},\\boldsymbol{Z}\\rangle)\\right)a_{i,j,\\boldsymbol{S}}a_{i,j,\\boldsymbol{S}}^{\\top}\\right]}\\\\ &{\\displaystyle=\\frac{1}{n^{2}}\\sum_{i,j\\in[n]}\\mathbb{E}_{\\boldsymbol{S}\\sim\\mathcal{D}_{\\boldsymbol{S}}}\\left[\\ell^{\\prime\\prime}(\\langle a_{i,j,\\boldsymbol{S}},\\boldsymbol{Z}\\rangle)a_{i,j,\\boldsymbol{S}}a_{i,j,\\boldsymbol{S}}^{\\top}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "where the last equality holds since $\\ell^{\\prime\\prime}(z)=\\ell^{\\prime\\prime}(-z)$ for any $z\\,\\in\\,\\mathbb{R}$ . From the equation above, it sufices to show that $\\{\\pmb{a}_{i,j,S}\\}_{i,j\\in[n],S\\subset[P]}$ spans $\\!\\!\\!\\!\\mathbb{R}^{2K+n(P-1)}$ to show strict convexity of $h(Z)$ ", "page_idx": 65}, {"type": "text", "text": "We define a functio [P \u2192 n) such that for each P],P() = p with a(g) ${\\pmb x}_{I(p)}^{(p)}={\\pmb v}_{1,1}$ ,where the existence is guaranteed by Lemma B.2 (but not necessarily unique). Then for any $i\\in[n]$ and $p\\in[p]$ ,wehave ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\begin{array}{r l}&{\\mathbf{a}_{i,i,\\emptyset}+\\displaystyle\\sum_{q\\in[P]\\backslash\\{p\\}}\\mathbf{a}_{I(q),i,\\{q\\}}-(P-1)a_{I(p),i,\\{p\\}}}\\\\ &{=\\displaystyle\\sum_{p^{\\prime}\\in[P]}e_{i}^{(p^{\\prime})}+\\sum_{q\\in[P]\\backslash\\{p\\}}\\left(e_{1,1}+\\sum_{p^{\\prime}\\in[P]\\backslash\\{q\\}}e_{i}^{(p^{\\prime})}\\right)-(P-1)\\left(e_{1,1}+\\sum_{p^{\\prime}\\in[P]\\backslash\\{p\\}}e_{i}^{(p^{\\prime})}\\right)}\\end{array}}\\\\ &{=\\displaystyle\\sum_{p^{\\prime}\\in[P]}e_{i}^{(p^{\\prime})}+\\left((P-1)e_{i}^{(p)}+(P-2)\\sum_{p^{\\prime}\\in[P]\\backslash\\{p\\}}e_{i}^{(p^{\\prime})}\\right)-(P-1)\\sum_{p^{\\prime}\\in[P]\\backslash\\{p\\}}e_{i}^{(p^{\\prime})}}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "Hence, $\\{a_{i,j,S}\\}_{i,j\\in[n],S\\subset[P]}$ Spans $\\mathbb{R}^{2K+n(P-1)}$ and $h(Z)$ is strictly convex. Thus, it can have at most one global minimum. We want to show the existence of the global minimum and characterize it. ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad n^{2}\\nabla h(Z)}\\\\ &{=\\displaystyle\\sum_{i,j\\in[n]}\\mathbb{E}_{\\mathcal{S}\\sim\\mathcal{D}_{s}}\\left[\\left(\\frac{|\\mathcal{S}|}{P}y_{i}\\ell^{\\prime}(y_{i}\\langle a_{i,j,s},Z\\rangle)+\\left(1-\\frac{|\\mathcal{S}|}{P}\\right)y_{j}\\ell^{\\prime}(y_{j}\\langle a_{i,j,s},Z\\rangle)\\right)a_{i,j,s}\\right]}\\\\ &{=2\\displaystyle\\sum_{i,j\\in[n]}\\mathbb{E}_{\\mathcal{S}\\sim\\mathcal{D}_{s}}\\left[\\left(\\frac{|\\mathcal{S}|}{P}y_{i}\\ell^{\\prime}(y_{i}\\langle a_{i,j,s},Z\\rangle)+\\left(1-\\frac{|\\mathcal{S}|}{P}\\right)y_{j}\\ell^{\\prime}(y_{j}\\langle a_{i,j,s},Z\\rangle)\\right)\\mathbb{1}_{p\\in\\mathcal{S}}\\right]e_{i}^{(p)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "We can simplify terms as ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{j\\in[n]}\\mathbb{E}_{y\\sim\\mathcal{P}_{x}}\\left[\\left(\\frac{|\\mathcal{S}|}{\\mathcal{P}}\\right)_{j}\\ell^{\\prime}(y_{i}(a_{j,s},z,z))+\\left(1-\\frac{|\\mathcal{S}|}{P}\\right)y_{j}\\ell^{\\prime}(y_{j}(a_{i,j,s},z))\\right)\\mathbb{1}_{y\\in\\mathcal{S}}\\right]}\\\\ &{=\\displaystyle\\sum_{j\\in\\mathcal{Y}_{n}}\\mathbb{E}_{z\\sim\\mathcal{P}_{x}}\\left[\\left(\\frac{|\\mathcal{S}|}{P}y_{j}\\ell^{\\prime}(y_{i}(a_{i,j,s},z))+\\left(1-\\frac{|\\mathcal{S}|}{P}\\right)y_{j}\\ell^{\\prime}(y_{i}(a_{i,j,s},z))\\right)\\mathbb{1}_{y\\in\\mathcal{S}}\\right]}\\\\ &{\\displaystyle~+\\sum_{j\\in\\mathcal{Y}_{n}}\\mathbb{E}_{z\\sim\\mathcal{P}_{x}}\\left[\\left(\\frac{|\\mathcal{S}|}{P}y_{j}\\ell^{\\prime}(y_{i}(a_{i,j,s},z))+\\left(1-\\frac{|\\mathcal{S}|}{P}\\right)y_{j}\\ell^{\\prime}(y_{i}(a_{i,j,s},z))\\right)\\mathbb{1}_{y\\in\\mathcal{S}}\\right]}\\\\ &{=\\displaystyle\\sum_{j\\in\\mathcal{Y}_{n}}\\mathbb{E}_{z\\sim\\mathcal{P}_{x}}[\\ell^{\\prime}(y_{i}(a_{i,j,s},z))\\mathbb{1}_{y\\in\\mathcal{S}}]}\\\\ &{\\displaystyle~~+\\operatorname*{\\mathcal{H}}_{j\\in\\mathcal{Y}_{n}}\\mathbb{E}_{z\\sim\\mathcal{P}_{x}}\\left[\\left(\\ell^{\\prime}(y_{i}(a_{i,j,s},z))+\\left(1-\\frac{|\\mathcal{S}|}{P}\\right)\\right)\\mathbb{1}_{y\\in\\mathcal{S}}\\right]}\\\\ &{=y_{i}|\\mathcal{Y}_{n}|\\mathcal{Z}_{s\\sim\\mathcal{P}_{x}}\\left[\\left(1-\\frac{|\\mathcal{S}|}{P}\\right)\\mathbb{1}_{y\\in\\mathcal{S}}\\right]+y_{i}\\sum_{j\\in[n]}\\mathbb{E}_{z\\sim\\mathcal{P}_{x}}[\\ell^{\\prime}(y_{i}(a_{i,j,s},z))\\mathbb{1}_{y\\in\\mathcal{S}}],}\\end{array}\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "where the second equality holds since $\\ell^{\\prime}(z)+\\ell^{\\prime}(-z)=-1$ .Also, for any $p\\in[P]$ ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\mathbb{E}_{{\\mathcal{S}}\\sim{\\mathcal{D}}_{\\mathcal{S}}}\\left[\\left(1-\\frac{|{\\mathcal{S}}|}{P}\\right)\\mathbb{1}_{p\\in{\\mathcal{S}}}\\right]=\\frac{1}{P}\\sum_{q\\in[P]}\\mathbb{E}_{{\\mathcal{S}}\\sim{\\mathcal{D}}_{\\mathcal{S}}}\\left[\\left(1-\\frac{|{\\mathcal{S}}|}{P}\\right)\\mathbb{1}_{q\\in{\\mathcal{S}}}\\right]\n$$", "text_format": "latex", "page_idx": 65}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle=\\frac1P\\mathbb{E}_{S\\sim\\mathcal{D}_{S}}\\left[\\left(1-\\frac{|S|}P\\right)\\sum_{q\\in\\mathcal{S}}\\mathbb{1}_{q\\in\\mathcal{S}}\\right]}\\\\ {\\displaystyle=\\frac1P\\mathbb{E}_{S\\sim\\mathcal{D}_{S}}\\left[\\left(1-\\frac{|S|}P\\right)|S|\\right]=\\frac{P-1}{6P}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "Hence, if ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\sum_{j\\in[n]}\\mathbb{E}_{S\\sim\\mathcal{D}_{S}}[\\ell^{\\prime}(y_{i}\\langle a_{i,j,S},Z\\rangle)\\mathbb{1}_{p\\in S}]+\\frac{P-1}{6P}|\\mathcal{V}_{-y_{i}}|=0,\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "for all $i\\in[n]$ and $p\\in[P]$ , then we have $\\nabla h(\\textbf Z)=0$ . Let us consider a specific $_{z}$ parameterized by $z_{1},z_{-1}$ , of the form $z_{i}^{(p)}=y_{i}z_{y_{i}}$ for all $i\\in[n]$ and $p\\in[P]$ . We will find a stationary point with this specific form and then it should be the unique global minimum in the entire domain. Then for each $\\bar{i^{}}\\in[n]$ and $p\\in[P]$ , we have ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\sum_{j\\in\\mathcal{N}_{D}}\\mathbb{E}_{x\\sim\\mathcal{P}_{D}}\\{\\ell(y_{j}(u_{\\delta,j},\\xi),\\mathbb{I}_{P}|\\phi_{C})}\\\\ &{=\\displaystyle\\sum_{j\\in\\mathcal{N}_{D}}\\mathbb{E}_{x\\sim\\mathcal{P}_{D}}\\{\\ell(y_{j}(u_{\\delta,j},\\xi),\\mathbb{I}_{P}|\\phi_{C})\\}_{I\\in\\mathcal{N}_{D}}\\Big)\\sum_{\\beta\\in\\mathcal{N}_{D}}\\mathbb{E}_{x\\sim\\mathcal{P}_{D}}[\\ell(\\beta_{\\beta}(u_{\\delta,j},\\xi))]_{I\\in\\mathcal{N}}\\Big|}\\\\ &{=\\displaystyle\\sum_{j\\in\\mathcal{N}_{D}}\\mathbb{E}_{x\\sim\\mathcal{P}_{D}}\\{\\ell(y_{j}(u_{\\delta,j},\\mathbb{I}_{P}|\\phi_{C})+|\\mathcal{V}_{\\sim\\mathcal{P}_{D}}|\\mathcal{V}_{\\sim\\mathcal{P}_{D}}|\\mathcal{V}_{\\delta}(\\lambda_{\\beta},-(P-|\\delta)|_{\\mathcal{H}})\\}_{I\\in\\mathcal{N}}]}\\\\ &{=\\displaystyle\\frac{1}{P}\\sum_{j\\in\\mathcal{N}_{D}}\\Big(|\\mathcal{V}_{\\nu_{\\delta}}|\\cdot\\mathbb{E}_{x\\sim\\mathcal{P}_{D}}\\{\\ell(P_{\\mathcal{H}_{D}})_{\\mathrm{H}\\in\\mathcal{N}}\\}+|\\mathcal{V}_{\\nu_{\\delta}}|\\cdot\\mathbb{E}_{x\\sim\\mathcal{P}_{D}}|\\ell(\\mathcal{N}|\\hat{v}_{\\delta}-(P-|\\delta)|_{\\mathcal{H}})\\Big)\\psi(x)\\Big)}\\\\ &{=\\displaystyle\\frac{1}{P}\\left(|\\mathcal{V}_{\\nu_{\\delta}}|\\cdot\\mathbb{E}_{\\delta\\sim\\mathcal{P}_{D}}\\Big[\\ell(P_{\\mathcal{H}_{D}})_{\\mathrm{Y}}\\sum_{1\\in\\mathcal{N}_{\\delta}}\\Big]}\\\\ &{\\qquad+|\\mathcal{V}_{\\nu_{\\delta}}|\\cdot\\mathbb{E}_{\\delta\\sim\\mathcal{P}_{D}}\\Big[\\ell(\\mathcal{V}_{\\nu_{\\delta}}|\\sum_{j\\in\\mathcal{N}_{\\delta}}\\Big(P\\bigg)}\\\\ &{\\qquad+|\\mathcal{V}_{\\nu_{\\delta}}|\\sum_{j\\in\\mathcal{N}_{\\delta}=\\mathcal{P}_{D}}\\bigg[\\ell \n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "$\\hat{\\pmb{Z}}=\\{\\hat{z}_{s,k}\\}_{s\\in\\{\\pm1\\},k\\in[K]}\\cup\\left\\{\\hat{z}_{i}^{(p)}\\right\\}_{i\\in[n],p\\in[P]\\backslash}$ of $h(Z)$ and it satisfies $s\\hat{z}_{s,k}=z_{s}^{*}=\\Theta(1)$ for all $k\\in[K]$ and $y_{i}\\hat{z}_{i}^{(p)}=z_{y_{i}}^{*}=\\Theta(1)$ for all $i\\in[n]$ and $p\\in[P]\\setminus\\{p_{i}^{*}\\}$ due to (A1). ", "page_idx": 66}, {"type": "text", "text": "E.2.3 Strong Convexity Near Global Minimum ", "text_level": 1, "page_idx": 66}, {"type": "text", "text": "We will show that $h(Z)$ is strongly convex in a set $\\mathcal{G}$ containing a global minimum $\\hat{Z}$ where $\\mathcal{G}$ is defined as follows. ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{G}:=\\left\\{Z\\in\\mathbb{R}^{2K+n(P-1)}:\\|Z-\\hat{Z}\\|_{\\infty}<\\|\\hat{Z}\\|_{\\infty}\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "here $\\|\\cdot\\|_{\\infty}$ is $\\ell_{\\infty}$ norm. For any $\\textbf{Z}\\in\\textbf{\\textit{G}}$ and a unit vector $\\textbf{\\textit{c}}\\in\\mathbb{R}^{2K+n(P-1)}$ with $\\c=$ $\\begin{array}{r}{\\sum_{s\\in\\{\\pm1\\},k\\in[K]}c_{s,k}e_{s,k}+\\sum_{i\\in[n],p\\in[P]\\backslash\\{p_{i}^{*}\\}}c_{i}^{(p)}e_{i}^{(p)}}\\end{array}$ we have ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{c^{\\top}\\nabla^{2}h(Z)c=\\displaystyle\\frac{1}{n^{2}}\\displaystyle\\sum_{i,j\\in[n]}\\mathbb{E}_{{\\cal S}\\sim{\\cal D}_{{\\cal S}}}\\left[\\ell^{\\prime\\prime}(\\langle a_{i,j,{\\cal S}},Z\\rangle)\\langle a_{i,j,{\\cal S}},c\\rangle^{2}\\right]}\\\\ &{\\qquad\\qquad\\ge\\displaystyle\\frac{\\ell^{\\prime\\prime}(2{\\cal P}\\|\\hat{\\cal Z}\\|_{\\infty})}{n^{2}}\\displaystyle\\sum_{i,j\\in[n]}\\mathbb{E}_{{\\cal S}\\sim{\\cal D}_{{\\cal S}}}[\\langle a_{i,j,{\\cal S}},c\\rangle^{2}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "Note that for each $i\\in[n],p\\in[P]$ , from (34), we have ", "page_idx": 67}, {"type": "equation", "text": "$$\nc_{i}^{(p)}=\\left\\langle c,e_{i}^{(p)}\\right\\rangle=\\frac{1}{P}\\langle c,a_{i,i,\\theta}\\rangle+\\frac{1}{P}\\sum_{q\\in[P]\\backslash\\{p\\}}\\langle c,a_{I(q),i,\\{q\\}}\\rangle-\\frac{P-1}{P}\\left\\langle c,a_{I(p),i,\\{p\\}}\\right\\rangle,\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "wherewe use the notational convention $c_{i}^{(p_{i}^{*})}\\,=\\,c_{s,k}$ for $s\\,\\in\\,\\{\\pm1\\},k\\,\\in\\,[K]$ and $i\\in\\mathcal{V}_{s,k}$ By Cauchy-Schwartz equalityand the fact that $\\begin{array}{r}{\\mathbb{P}_{{S}\\sim\\mathcal{D}_{S}}[S=\\emptyset],\\mathbb{P}_{{S}\\sim\\mathcal{D}_{S}}[S=\\{q\\}]\\ge\\frac{1}{P(P+1)}}\\end{array}$ for all $q\\in[P]$ ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(c_{i}^{(p)}\\right)^{2}}\\\\ &{=\\left(\\displaystyle\\frac{1}{P}\\left\\langle c,a_{i,i,\\theta}\\right\\rangle+\\displaystyle\\frac{1}{P}\\sum_{q\\in[P]\\backslash\\{p\\}}\\left\\langle c,a_{t(q),i,\\{q\\}}\\right\\rangle-\\displaystyle\\frac{P-1}{P}\\left\\langle c,a_{I(p),i,\\{p\\}}\\right\\rangle\\right)^{2}}\\\\ &{\\leq\\left(\\displaystyle\\frac{1}{P^{2}}+\\frac{P-1}{P^{2}}+\\left(-\\frac{P-1}{P}\\right)^{2}\\right)\\left(\\left\\langle c,a_{i,i,\\theta}\\right\\rangle^{2}+\\displaystyle\\sum_{q\\in[P]\\backslash\\{p\\}}\\left\\langle c,a_{I(q),i,\\{q\\}}\\right\\rangle^{2}+\\left\\langle c,a_{I(p),i,\\{p\\}}\\right\\rangle^{2}\\right)}\\\\ &{\\leq\\left(\\displaystyle\\frac{1}{P^{2}}+\\frac{P-1}{P^{2}}+\\left(-\\frac{P-1}{P}\\right)^{2}\\right)P(P+1)\\displaystyle\\sum_{i,j\\in[n]}\\mathbb{E}_{\\delta\\sim P,S}[\\left\\langle c,a_{i,j,\\delta}\\right\\rangle^{2}]}\\\\ &{\\leq2P^{2}\\displaystyle\\sum_{i,j\\in[n]}^{2}\\mathbb{E}_{\\delta\\sim\\mathcal{P}_{\\delta}}\\left[\\left\\langle c,a_{i,i,\\delta}\\right\\rangle^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "Hence, we have ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{c^{\\top}\\nabla^{2}h(Z)c\\geq\\displaystyle\\frac{\\ell^{\\prime\\prime}(2P\\|\\hat{Z}\\|_{\\infty})}{(4K+2n(P-1))P^{2}n^{2}}(4K+2n(P-1))P^{2}\\displaystyle\\sum_{i,j\\in[n]}\\mathbb{E}_{S\\sim\\mathcal{D}_{s}}\\left[\\langle c,a_{i,j,s}\\rangle^{2}\\right]}\\\\ &{\\qquad\\qquad\\geq\\displaystyle\\frac{\\ell^{\\prime\\prime}(2P\\|\\hat{Z}\\|_{\\infty})}{(4K+2n(P-1))P^{2}n^{2}}\\left(\\displaystyle\\sum_{s\\in\\{\\pm1\\},k\\in[K]}c_{s,k}^{2}+\\displaystyle\\sum_{i\\in[n],q\\in[P]\\setminus\\{p_{i}^{*}\\}}\\left(c_{i}^{(q)}\\right)^{2}\\right)}\\\\ &{\\qquad\\qquad=\\displaystyle\\frac{\\ell^{\\prime\\prime}(2P\\|\\hat{Z}\\|_{\\infty})}{(4K+2n(P-1))P^{2}n^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "and we conelde $h(Z)$ $\\mu$ strongly convex in $\\mathcal{G}$ Wwhere $\\begin{array}{r}{\\mu:=\\frac{\\ell^{\\prime\\prime}(2P\\|\\hat{Z}\\|_{\\infty})}{(4K+2n(P-1))P^{2}n^{2}}}\\end{array}$ Due to(A1,(A2), and the fact tha /Zll = (1), we have \u03bc \u2265 poly(d)- ", "page_idx": 67}, {"type": "text", "text": "E.2.4 Near Stationary Points are Close to Global Minimum ", "text_level": 1, "page_idx": 67}, {"type": "text", "text": "In this step, we want to show that near stationary points of $h(Z)$ are close to a global minimum $\\hat{Z}$ Lemma E.1. Suppose $Z\\in\\mathbb{R}^{2K+n(P-1)}$ satisfies $\\|\\nabla h(Z)\\|<\\mu\\epsilon$ with some $0<\\epsilon<\\frac{\\|\\hat{\\boldsymbol{z}}\\|_{\\infty}}{2}$ l/ll\u3002 . Then, we have $\\left\\|Z-{\\hat{Z}}\\right\\|<\\epsilon$ ", "page_idx": 67}, {"type": "text", "text": "Proof of Lemma E.1. If $Z={\\hat{Z}}$ , we immediately have our conclusion. We may assume $z\\neq{\\hat{Z}}$ Let us define a function $g:\\mathbb{R}\\rightarrow\\mathbb{R}$ as $g(t)=h\\left({\\hat{Z}}+t(Z-{\\hat{Z}})\\right)$ Then $g$ is convex and ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g^{\\prime}(t)=\\left\\langle\\nabla h\\left(\\hat{\\pmb{Z}}+t(\\pmb{Z}-\\hat{\\pmb{Z}})\\right),\\pmb{Z}-\\hat{\\pmb{Z}}\\right\\rangle,}\\\\ &{g^{\\prime\\prime}(t)=\\left(\\pmb{Z}-\\hat{\\pmb{Z}}\\right)^{\\top}\\nabla^{2}h\\left(\\hat{\\pmb{Z}}+t(\\pmb{Z}-\\hat{\\pmb{Z}})\\right)\\left(\\pmb{Z}-\\hat{\\pmb{Z}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "Furthermore, for O \u2264t\u2264to where to := 2- ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\hat{\\pmb Z}+t(\\pmb Z-\\hat{\\pmb Z})\\in\\mathcal G,\\qquad\\therefore g^{\\prime\\prime}(t)\\geq\\mu\\left\\|\\pmb Z-\\hat{\\pmb Z}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "We can conclude $g$ is $\\mu\\left\\Vert Z-\\hat{Z}\\right\\Vert^{2}$ -strongly convex in $[0,t_{0}]$ . From strong convexity in $[0,t_{0}]$ and convexity in $\\mathbb{R}$ , we have ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(g^{\\prime}(t_{0})-g^{\\prime}(0))t_{0}=g^{\\prime}(t_{0})t_{0}\\geq\\mu\\left\\|Z-\\hat{Z}\\right\\|^{2}t_{0}^{2},\\quad(g^{\\prime}(1)-g^{\\prime}(t_{0}))(1-t_{0})\\geq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "If $t_{0}<1$ , we have ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla h(\\pmb{Z})\\|\\left\\|\\pmb{Z}-\\hat{\\pmb{Z}}\\right\\|\\geq\\left\\langle\\nabla h(\\pmb{Z}),\\pmb{Z}-\\hat{\\pmb{Z}}\\right\\rangle=g^{\\prime}(1)\\geq g^{\\prime}(t_{0})\\geq\\mu\\left\\|\\pmb{Z}-\\hat{\\pmb{Z}}\\right\\|^{2}t_{0},}\\end{array}\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "and ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\left\\|\\nabla h(\\pmb{Z})\\right\\|\\geq\\mu\\left\\|\\pmb{Z}-\\hat{\\pmb{Z}}\\right\\|t_{0}=\\frac{\\mu\\left\\|\\pmb{Z}-\\hat{\\pmb{Z}}\\right\\|\\left\\|\\pmb{\\hat{Z}}\\right\\|_{\\infty}}{2\\left\\|\\pmb{Z}-\\hat{\\pmb{Z}}\\right\\|_{\\infty}}\\geq\\frac{\\mu\\left\\|\\pmb{\\hat{Z}}\\right\\|_{\\infty}}{2},\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "this is contradictory. Thus, we have $t_{0}\\geq1$ and $Z\\in{\\mathcal{G}}$ . From the strong convexity of $h(Z)$ in $\\mathcal{G}$ we have ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\mu\\left\\|Z-\\hat{Z}\\right\\|\\leq\\left\\|\\nabla h(Z)-\\nabla h(\\hat{Z})\\right\\|=\\|\\nabla h(Z)\\|<\\mu\\epsilon,\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "and we have our conclusion $\\left\\|Z-{\\hat{Z}}\\right\\|<\\epsilon$ ", "page_idx": 68}, {"type": "text", "text": "E.2.5 Gradient Descent Achieves a Near Stationary Point ", "text_level": 1, "page_idx": 68}, {"type": "text", "text": "We will show that $\\mathcal{L}_{\\mathrm{CutMix}}(W)$ is a smooth function. ", "page_idx": 68}, {"type": "text", "text": "LemmaE.2.Supposetheevent $E_{\\mathrm{init}}$ Occurs.CutMixLoss $\\mathcal{L}_{\\mathrm{CutMix}}(W)$ is $L$ -Smoothwith $L=$ $9r^{-1}P\\sigma_{\\mathrm{d}}^{2}d$ ", "page_idx": 68}, {"type": "text", "text": "Proof of Lemma $E.2$ .Note that ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\nabla_{w_{1}}\\mathcal{L}_{\\mathrm{CutMix}}(W)}\\\\ &{=\\displaystyle\\frac{1}{n^{2}}\\sum_{i,j\\in[n]}\\mathbb{E}_{\\mathcal{S}\\sim\\mathcal{D}_{S}}\\Bigg[\\left(\\frac{|\\mathcal{S}|}{P}y_{i}\\ell^{\\prime}(y_{i}f_{W}(\\mathbf{X}_{i,j,\\mathcal{S}}))+\\left(1-\\displaystyle\\frac{|\\mathcal{S}|}{P}\\right)y_{j}\\ell^{\\prime}(y_{j}f_{W}(\\mathbf{X}_{i,j,\\mathcal{S}}))\\right)}\\\\ &{\\quad\\times\\displaystyle\\left(\\sum_{p\\in\\mathcal{S}}\\phi^{\\prime}\\left(\\left\\langle w_{1},x_{i}^{(p)}\\right\\rangle\\right)x_{i}^{(p)}+\\displaystyle\\sum_{p\\notin\\mathcal{S}}\\phi^{\\prime}\\left(\\left\\langle w_{1},x_{j}^{(p)}\\right\\rangle\\right)x_{j}^{(p)}\\right)\\Bigg].}\\end{array}\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "Let $\\widetilde{\\pmb{W}}=\\{\\widetilde{\\pmb{w}}_{1},\\widetilde{\\pmb{w}}_{-1}\\}$ and $\\overline{{\\pmb{W}}}=\\{\\overline{{\\pmb{w}}}_{1},\\overline{{\\pmb{w}}}_{-1}\\}$ be any parameters of the neural network $f_{W}$ . For any $i,j\\in[n]$ and $\\cal S\\subset[P]$ \uff0c ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\frac{|\\mathcal{S}|}{P}\\nu^{\\ell}(\\mathfrak{p}_{f_{\\ell}}(X_{t,\\theta}))+\\left(1-\\frac{|\\mathcal{S}|}{P}\\right)\\mathfrak{p}_{f}(\\mathfrak{p}_{f_{\\ell}}(X_{t,\\theta}))\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\times\\left(\\frac{|\\mathcal{S}|}{P}\\psi^{\\ell}\\left(\\mathfrak{p}(\\mathfrak{p}(X_{t,\\theta}))\\right)\\mathfrak{x}^{\\ell}+\\frac{|\\mathcal{S}|}{P}\\right)}\\\\ &{\\qquad-\\left(\\frac{|\\mathcal{S}|}{P}\\nu^{\\ell}(\\mathfrak{p}(\\mathfrak{p}(X_{t,\\theta})))+\\left(1-\\frac{|\\mathcal{S}|}{P}\\right)\\mathfrak{p}_{f}(\\mathfrak{p}(X_{t,\\theta}))\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\times\\left(\\frac{|\\mathcal{S}|}{P}\\psi^{\\ell}\\left(\\mathfrak{p}(\\mathfrak{p}(X_{t,\\theta}))\\right)\\right)=\\frac{1}{P}\\nu^{\\ell}\\left(\\left(\\mathfrak{p}(\\mathfrak{p}(X_{t,\\theta}))\\right)\\right)}\\\\ {=}&{\\left(\\frac{|\\mathcal{S}|}{P}\\mathfrak{p}^{\\ell}(\\mathfrak{p}(\\mathfrak{p}(X_{t,\\theta})))+\\left(1-\\frac{|\\mathcal{S}|}{P}\\right)\\mathfrak{p}_{f}(\\mathfrak{p}(\\mathfrak{p}(X_{t,\\theta})))\\right)}\\\\ &{\\qquad\\qquad\\qquad\\times\\left(\\frac{|\\mathcal{S}|}{P}\\mathfrak{p}^{\\ell}(\\mathfrak{p}(\\mathfrak{p}(X_{t,\\theta})))\\right)=\\frac{1}{P}\\nu^{\\ell}\\left(\\left(\\mathfrak{p}(\\mathfrak{p}_{1},\\mathfrak{p}(\\mathfrak{p}(X_{t,\\theta}))\\right)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\times\\left(\\frac{|\\mathcal{S}|}{P}\\mathfrak{p}^{\\ell}(\\mathfrak{p}(X_{t,\\theta}))\\right)=0,}\\\\ &{\\qquad-\\left(\\frac{|\\mathcal{S}|}{P}\\mathfrak{p}^{\\ell}(\\mathfrak{p}(X_{t,\\theta}))\\right)+\\left(1-\\frac{|\\mathcal{S}|}{P}\\right)\\mathfrak{p}_{f}(\\mathfrak{p}(X_{t,\\theta}))\\sum_{\\vec{\\theta}\\in\\mathcal{S}}\\left(\\frac{|\\mathcal{S}|}{\\mathfrak{p}(X_{t,\\theta})}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\times \n$$", "text_format": "latex", "page_idx": 68}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{+\\left(\\frac{|S|}{P}y_{i}\\ell^{\\prime}(y_{i}f_{\\widetilde W}({X}_{i,j,s}))+\\left(1-\\frac{|S|}{P}\\right)y_{j}\\ell^{\\prime}(y_{j}f_{\\widetilde W}({X}_{i,j,s}))\\right)}}\\\\ &{}&{\\times\\left(\\sum_{p\\in\\mathcal{S}}\\phi^{\\prime}\\left(\\left\\langle\\overline{{w}}_{1},x_{i}^{(p)}\\right\\rangle\\right)x_{i}^{(p)}+\\sum_{p\\notin\\mathcal{S}}\\phi^{\\prime}\\left(\\left\\langle\\overline{{w}}_{1},x_{j}^{(p)}\\right\\rangle\\right)x_{j}^{(p)}\\right)}\\\\ &{}&{-\\left(\\frac{|S|}{P}y_{i}\\ell^{\\prime}(y_{i}f_{\\overline{{W}}}({X}_{i,j,s}))+\\left(1-\\frac{|S|}{P}\\right)y_{j}\\ell^{\\prime}(y_{j}f_{\\overline{{W}}}({X}_{i,j,s}))\\right)}\\\\ &{}&{\\times\\left(\\sum_{p\\in\\mathcal{S}}\\phi^{\\prime}\\left(\\left\\langle\\overline{{w}}_{1},x_{i}^{(p)}\\right\\rangle\\right)x_{i}^{(p)}+\\sum_{p\\notin\\mathcal{S}}\\phi^{\\prime}\\left(\\left\\langle\\overline{{w}}_{1},x_{j}^{(p)}\\right\\rangle\\right)x_{j}^{(p)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "Since $|\\ell^{\\prime}|\\leq1$ \uff0c", "page_idx": 69}, {"type": "equation", "text": "$$\n\\left|\\frac{|S|}{P}y_{i}\\ell^{\\prime}\\left(y_{i}f_{\\widetilde W}({\\pmb X}_{i,j,S})\\right)+\\left(1-\\frac{|S|}{P}\\right)y_{j}\\ell^{\\prime}\\left(y_{j}f_{\\widetilde W}({\\pmb X}_{i,j,S})\\right)\\right|\\le1,\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "and since $\\vert\\phi^{\\prime}\\vert\\le1$ \uff0c", "page_idx": 69}, {"type": "equation", "text": "$$\n\\left\\|\\sum_{p\\in S}\\phi^{\\prime}\\left(\\left\\langle\\overline{{w}}_{1},x_{i}^{(p)}\\right\\rangle\\right)x_{i}^{(p)}+\\sum_{p\\notin S}\\phi^{\\prime}\\left(\\left\\langle\\overline{{w}}_{1},x_{j}^{(p)}\\right\\rangle\\right)x_{j}^{(p)}\\right\\|\\leq P\\operatorname*{max}_{i\\in[n],p\\in[P]}\\left\\|x_{i}^{(p)}\\right\\|.\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "In addition, since $\\phi$ is $r^{-1}$ -smooth, ", "page_idx": 69}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\left(\\displaystyle\\sum_{p\\in\\mathcal{S}}\\phi^{\\prime}\\left(\\left\\langle\\tilde{w}_{1},\\boldsymbol{x}_{i}^{(p)}\\right\\rangle\\right)x_{i}^{(p)}+\\displaystyle\\sum_{p\\in\\mathcal{S}}\\phi^{\\prime}\\left(\\left\\langle\\tilde{w}_{1},\\boldsymbol{x}_{j}^{(p)}\\right\\rangle\\right)x_{j}^{(p)}\\right)\\right.}\\\\ &{\\qquad-\\left.\\left(\\displaystyle\\sum_{p\\in\\mathcal{S}}\\phi^{\\prime}\\left(\\left\\langle\\overline{{w}}_{1},\\boldsymbol{x}_{i}^{(p)}\\right\\rangle\\right)x_{i}^{(p)}+\\displaystyle\\sum_{p\\in\\mathcal{S}}\\phi^{\\prime}\\left(\\left\\langle\\overline{{w}}_{1},\\boldsymbol{x}_{j}^{(p)}\\right\\rangle\\right)x_{j}^{(p)}\\right)\\right\\|}\\\\ &{\\leq\\displaystyle\\sum_{p\\in\\mathcal{S}}\\left\\|\\phi^{\\prime}\\left(\\left\\langle\\tilde{w}_{1},\\boldsymbol{x}_{i}^{(p)}\\right\\rangle\\right)-\\phi^{\\prime}\\left(\\left\\langle\\overline{{w}}_{1},\\boldsymbol{x}_{i}^{(p)}\\right\\rangle\\right)\\right\\|\\left\\|x_{i}^{(p)}\\right\\|}\\\\ &{\\quad+\\displaystyle\\sum_{p\\in\\mathcal{S}}\\left\\|\\phi^{\\prime}\\left(\\left\\langle\\tilde{w}_{1},\\boldsymbol{x}_{i}^{(p)}\\right\\rangle\\right)-\\phi^{\\prime}\\left(\\left\\langle\\overline{{w}}_{1},\\boldsymbol{x}_{j}^{(p)}\\right\\rangle\\right)\\right\\|\\left\\|x_{j}^{(p)}\\right\\|}\\\\ &{\\leq\\displaystyle r^{-1}\\sum_{p\\in\\mathcal{S}}\\left\\langle\\left\\langle\\overline{{w}}_{1}-\\overline{{w}}_{1},\\boldsymbol{x}_{i}^{(p)}\\right\\rangle\\right\\|\\left\\|\\boldsymbol{x}_{i}^{(p)}\\right\\|+r^{-1}\\displaystyle\\sum_{p\\in\\mathcal{S}}\\left|\\left\\langle\\tilde{w}_{1}-\\overline{{w}}_{1},\\boldsymbol{x}_{j}^{(p)}\\right\\rangle\\right\\|\\left\\|\\boldsymbol{x}_{j}^{(p)}\\right\\|}\\\\ &{\\leq r^{-1}P\\left(\\epsilon_{\\mathrm{eff},\\mathrm{eff},\\mathrm{eff}}^{(p)}\\right)\\left\\|x_{i}^{(p)}\\right\\|\\left\\|\\boldsymbol{\\tilde{x}}_{i}^{(p)}-\\overline{{w}}_{1}\\right\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "and since $\\ell^{\\prime}$ and $\\phi$ are 1-Lipschitz, we have ", "page_idx": 69}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bigg|\\bigg(\\displaystyle\\prod_{j=1}^{|S|}\\mu_{j}\\ell(y_{i}f_{W}(X_{i,j,s}))+\\left(1-\\frac{|S|}{P}\\right)y_{j}\\ell^{\\prime}(y_{j}f_{W}(X_{i,j,s}))\\bigg)}\\\\ &{\\quad-\\left(\\displaystyle\\frac{|S|}{P}y_{i}\\ell^{\\prime}(y_{i}f_{W}(X_{i,j,s}))+\\left(1-\\frac{|S|}{P}\\right)y_{j}\\ell^{\\prime}(y_{j}f_{W}(X_{i,j,s}))\\right)\\bigg|}\\\\ &{\\le\\left|f_{\\overline{{W}}}(X_{i,j,s})-f_{\\overline{{W}}}(X_{i,j,s})\\right|}\\\\ &{\\le\\displaystyle\\sum_{p\\in\\mathcal{S}}\\left(\\left|\\left\\langle\\tilde{w}_{1}-\\overline{{w}}_{1},x_{i}^{(p)}\\right\\rangle\\right|+\\left|\\left\\langle\\tilde{w}_{-1}-\\overline{{w}}_{-1},x_{i}^{(p)}\\right\\rangle\\right|\\right)}\\\\ &{\\quad+\\displaystyle\\sum_{p\\in\\mathcal{S}}\\left(\\left|\\left\\langle\\tilde{w}_{1}-\\overline{{w}}_{1},x_{j}^{(p)}\\right\\rangle\\right|+\\left|\\left\\langle\\tilde{w}_{-1}-\\overline{{w}}_{-1},x_{j}^{(p)}\\right\\rangle\\right|\\right)}\\\\ &{\\le P_{\\mathrm{+trmize}}\\left\\|\\mathbf{x}^{(p)}\\right\\|\\left(\\|\\overline{{w}}_{1}-\\overline{{w}}_{1}\\|+\\|\\tilde{w}_{-1}-\\overline{{w}}_{-1}\\|\\right)}\\\\ &{\\le\\sqrt{p}\\displaystyle\\sum_{i\\in[m,n]\\in\\mathbb{Z}}\\left\\|\\mathbf{x}^{(p)}\\right\\|\\left\\|\\overline{{W}}-\\overline{{w}}\\right\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "Therefore, ", "page_idx": 70}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|\\nabla_{w_{1}}\\mathcal{L}_{\\mathrm{CutMix}}(\\widetilde{W})-\\nabla_{w_{1}}\\mathcal{L}_{\\mathrm{CutMix}}(\\overline{{W}})\\right\\|}\\\\ &{\\leq r^{-1}P\\left(\\underset{i\\in[n],p\\in[P]}{\\operatorname*{max}}\\left\\|x_{i}^{(p)}\\right\\|\\right)^{2}\\|\\widetilde{w}_{1}-\\overline{{w}}_{1}\\|+\\sqrt{2}P^{2}\\left(\\underset{i\\in[n],p\\in[P]}{\\operatorname*{max}}\\left\\|x_{i}^{(p)}\\right\\|\\right)^{2}\\left\\|\\widetilde{W}-\\overline{{W}}\\right\\|}\\\\ &{\\leq2r^{-1}P\\left(\\underset{i\\in[n],p\\in[P]}{\\operatorname*{max}}\\left\\|x_{i}^{(p)}\\right\\|\\right)^{2}\\left\\|\\widetilde{W}-\\overline{{W}}\\right\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "where the last equality is due to (A1) and (A8). In the same way, we can obtain ", "page_idx": 70}, {"type": "equation", "text": "$$\n\\Big\\|\\nabla_{w_{-1}}\\mathcal L_{\\mathrm{CutMix}}(\\widetilde W)-\\nabla_{w_{-1}}\\mathcal L_{\\mathrm{CutMix}}(\\overline{W})\\Big\\|\\leq2r^{-1}P\\left(\\operatorname*{max}_{i\\in[n],p\\in[P]}\\left\\|x_{i}^{(p)}\\right\\|\\right)^{2}\\left\\|\\widetilde W-\\overline{W}\\right\\|,\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "and ", "page_idx": 70}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\nabla{\\mathcal{L}}_{\\mathrm{CutMix}}(\\widetilde W)-\\nabla{\\mathcal{L}}_{\\mathrm{CutMix}}(\\overline{W})\\right\\|\\leq4r^{-1}P\\left(\\underset{i\\in[n],p\\in[P]}{\\operatorname*{max}}\\left\\|x_{i}^{(p)}\\right\\|\\right)^{2}\\left\\|\\widetilde W-\\overline{W}\\right\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq9r^{-1}P\\sigma_{\\mathrm{d}}^{2}d\\left\\|\\widetilde W-\\overline{W}\\right\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "where the last inequality holds since $\\left\\|\\xi_{i}^{(p)}\\right\\|^{2}\\ <\\ \\frac{3}{2}\\sigma_{\\mathrm{d}}^{2}d$ and $\\textstyle\\alpha^{2}\\ \\leq\\ {\\frac{3}{4}}\\sigma_{\\mathrm{d}}^{2}d$ due to (A7). Hence, $\\mathcal{L}_{\\mathrm{CutMix}}(W)$ is $L$ -smooth with $L:=9r^{-1}P\\sigma_{\\mathrm{d}}^{2}d$ \u53e3 ", "page_idx": 70}, {"type": "text", "text": "Since our objective function $\\mathcal{L}_{\\mathrm{CutMix}}(W)$ is $L$ -smooth and $\\begin{array}{r}{\\eta\\le\\frac{1}{L}}\\end{array}$ due to (A8), descent lemma (see Lemma 3.4 in Bubeck et al. (2015)) implies ", "page_idx": 70}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{CutMix}}\\left(\\pmb{W}^{(t+1)}\\right)-\\mathcal{L}_{\\mathrm{CutMix}}\\left(\\pmb{W}^{(t)}\\right)\\leq-\\frac{\\eta}{2}\\left\\|\\nabla\\mathcal{L}_{\\mathrm{CutMix}}\\left(\\pmb{W}^{(t)}\\right)\\right\\|^{2},\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "and by telescoping sum, we have ", "page_idx": 70}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}\\left\\|\\nabla\\mathcal{L}_{\\mathrm{CutMix}}\\left(\\boldsymbol{W}^{(t)}\\right)\\right\\|^{2}\\leq\\frac{2\\mathcal{L}_{\\mathrm{CutMix}}\\left(\\boldsymbol{W}^{(0)}\\right)}{\\eta T}=\\frac{\\Theta(1)}{\\eta T},\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "for any $T>0$ ", "page_idx": 70}, {"type": "text", "text": "Choose $\\begin{array}{r}{\\epsilon=\\frac{\\mu\\beta\\lVert\\hat{\\boldsymbol{Z}}\\rVert_{\\infty}}{\\mathrm{polylog}(d)}}\\end{array}$ . Then from (35), there exists $\\begin{array}{r}{T_{\\mathrm{CutMix}}\\leq\\frac{\\mathrm{poly}(d)}{\\eta}}\\end{array}$ such that ", "page_idx": 70}, {"type": "equation", "text": "$$\n\\left\\|\\nabla\\mathcal{L}_{\\mathrm{CutMix}}\\left(W^{(T_{\\mathrm{CutMix}})}\\right)\\right\\|\\leq\\epsilon.\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "From characterization of $\\sigma_{\\mathrm{min}}(J(W))$ in Section E.2.1, ", "page_idx": 70}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\epsilon\\geq\\left\\|\\nabla\\mathcal{L}_{\\mathrm{CutMix}}\\left(W^{(T_{\\mathrm{CutMix}})}\\right)\\right\\|\\geq\\sigma_{\\operatorname*{min}}\\left(J\\left(W^{(T_{\\mathrm{CutMix}})}\\right)\\right)\\left\\|\\nabla h\\left(Z^{(T_{\\mathrm{CutMix}})}\\right)\\right\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\geq\\frac{\\beta}{2}\\left\\|\\nabla h\\left(Z^{(T_{\\mathrm{CutMix}})}\\right)\\right\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "and thus ", "page_idx": 70}, {"type": "equation", "text": "$$\n\\left\\|\\nabla h\\left(\\pmb{Z}^{(T_{\\mathrm{CutMix}})}\\right)\\right\\|\\leq2\\beta^{-1}\\epsilon=\\mu\\cdot\\frac{2\\|\\hat{\\pmb{Z}}\\|_{\\infty}}{\\mathrm{polylog}(d)}.\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "For suffcientlylarge $d$ the RHS becomes smaller than $\\mu\\cdot\\frac{\\|\\hat{\\mathbf{Z}}\\|_{\\infty}}{4}$ .Then, by Lemma E.1 we have seen in Section E.2.4, ", "page_idx": 70}, {"type": "equation", "text": "$$\n\\left\\|Z^{(T_{\\mathrm{CutMix}})}-\\hat{Z}\\right\\|\\leq\\frac{\\|\\hat{Z}\\|_{\\infty}}{4},\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "and thus ", "page_idx": 70}, {"type": "equation", "text": "$$\n\\phi\\left(\\left\\langle w_{y_{i}}^{(T_{\\mathrm{CutMix}})},\\pmb{x}_{i}^{(p)}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-y_{i}}^{(T_{\\mathrm{CutMix}})},\\pmb{x}_{i}^{(p)}\\right\\rangle\\right)=\\Theta(1),\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "for all $i\\in[n]$ and $p\\in[P]$ , and therefore it reaches perfect training accuracy. ", "page_idx": 70}, {"type": "text", "text": "E.2.6 Test Accuracy of Solution Found by Gradient Descent ", "text_level": 1, "page_idx": 71}, {"type": "text", "text": "The final step is showing that $W^{(T_{\\mathrm{CutMix}})}$ reaches almost perfect test accuracy. From the results of Section E.2.5, we have ", "page_idx": 71}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phi\\left(\\left\\langle w_{s}^{(T_{\\mathrm{CutMix}})},v_{s,k}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-s}^{(T_{\\mathrm{CutMix}})},v_{s,k}\\right\\rangle\\right)=\\Theta(1),}\\\\ &{\\phi\\left(\\left\\langle w_{y_{i}}^{(T_{\\mathrm{CutMix}})},\\xi_{i}^{(p)}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-y_{i}}^{(T_{\\mathrm{CutMix}})},\\xi_{i}^{(p)}\\right\\rangle\\right)=\\Theta(1),}\\end{array}\n$$", "text_format": "latex", "page_idx": 71}, {"type": "text", "text": "for each $s\\in\\{\\pm1\\},k\\in[K],i\\in[n]$ and $p\\in[P]\\setminus\\{p_{i}^{*}\\}$ ", "page_idx": 71}, {"type": "text", "text": "For any $u>v$ , by the mean value theorem, we have ", "page_idx": 71}, {"type": "equation", "text": "$$\n\\beta(u-v)\\le\\phi(u)-\\phi(v)=(u-v)\\frac{\\phi(u)-\\phi(v)}{u-v}\\le(u-v).\n$$", "text_format": "latex", "page_idx": 71}, {"type": "text", "text": "Hence, we have ", "page_idx": 71}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\phi\\left(\\left\\langle w_{s}^{(T_{\\mathrm{CutMix}})},v_{s,k}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-s}^{(T_{\\mathrm{CutMix}})},v_{s,k}\\right\\rangle\\right)\\leq\\left\\langle w_{s}^{(T_{\\mathrm{CutMix}})}-w_{-s}^{(T_{\\mathrm{CutMix}})},v_{s,k}\\right\\rangle,}\\\\ {\\left\\langle w_{s}^{(T_{\\mathrm{CutMix}})}-w_{-s}^{(T_{\\mathrm{CutMix}})},v_{s,k}\\right\\rangle\\leq\\beta^{-1}\\left(\\phi\\left(\\left\\langle w_{s}^{(T_{\\mathrm{CutMix}})},v_{s,k}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-s}^{(T_{\\mathrm{CutMix}})},v_{s,k}\\right\\rangle\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 71}, {"type": "text", "text": "and ", "page_idx": 71}, {"type": "equation", "text": "$$\n\\Omega(1)\\leq\\left\\langle\\pmb{w}_{s}^{(T_{\\mathrm{CutMix}})}-\\pmb{w}_{-s}^{(T_{\\mathrm{CutMix}})},\\pmb{v}_{s,k}\\right\\rangle\\leq\\mathcal{O}(\\beta^{-1}),\n$$", "text_format": "latex", "page_idx": 71}, {"type": "text", "text": "for each $s\\in\\{\\pm1\\}$ and $k\\in[K]$ . Similarly, for all $i\\in[n]$ and $p\\in[P]\\setminus\\{p_{i}^{*}\\}$ \uff0c ", "page_idx": 71}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\phi\\left(\\left\\langle w_{y_{i}}^{(T_{\\mathrm{CutMix}})},\\xi_{i}^{(p)}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-y_{i}}^{(T_{\\mathrm{CutMix}})},\\xi_{i}^{(p)}\\right\\rangle\\right)\\leq\\left\\langle w_{y_{i}}^{(T_{\\mathrm{CutMix}})}-w_{-y_{i}}^{(T_{\\mathrm{CutMix}})},\\xi_{i}^{(p)}\\right\\rangle,}\\\\ {\\left\\langle w_{y_{i}}^{(T_{\\mathrm{CutMix}})}-w_{-y_{i}}^{(T_{\\mathrm{CutMix}})},\\xi_{i}^{(p)}\\right\\rangle\\leq\\beta^{-1}\\left(\\phi\\left(\\left\\langle w_{y_{i}}^{(T_{\\mathrm{CutMix}})},\\xi_{i}^{(p)}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-y_{i}}^{(T_{\\mathrm{CutMix}})},\\xi_{i}^{(p)}\\right\\rangle\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 71}, {"type": "text", "text": "and ", "page_idx": 71}, {"type": "equation", "text": "$$\n\\Omega(1)\\leq\\left\\langle\\mathbf{w}_{y_{i}}^{(T_{\\mathrm{CutMix}})}-\\mathbf{w}_{-y_{i}}^{(T_{\\mathrm{CutMix}})},\\xi_{i}^{(p)}\\right\\rangle\\leq\\mathcal{O}(\\beta^{-1}).\n$$", "text_format": "latex", "page_idx": 71}, {"type": "text", "text": "By Lemma B.3, ", "page_idx": 71}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle w_{1}^{(T_{\\mathrm{CutMix}})}-\\pmb{w}_{-1}^{(T_{\\mathrm{CutMix}})}}\\\\ &{\\displaystyle=w_{1}^{(0)}-\\pmb{w}_{-1}^{(0)}+\\sum_{\\substack{s\\in\\{\\pm1\\},k\\in[K]}}s\\gamma(s,k)\\pmb{v}_{s,k}+\\sum_{\\substack{i\\in[n],p\\in[P]\\backslash\\{p_{i}^{*}\\}}}y_{i}\\rho(i,p)\\frac{\\xi_{i}^{(p)}}{\\left\\|\\xi_{i}^{(p)}\\right\\|^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 71}, {"type": "text", "text": "where for each $s\\in\\{\\pm1\\}$ ", "page_idx": 71}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma(s,1)=\\gamma_{1}^{(T_{\\mathrm{CutMix}})}(s,1)+\\gamma_{-1}^{(T_{\\mathrm{CutMix}})}(s,1)}\\\\ &{\\qquad\\qquad+\\alpha\\displaystyle\\sum_{i\\in\\mathcal{F}_{s}}y_{i}\\left(\\rho_{1}^{(T_{\\mathrm{CutMix}})}(i,\\tilde{p}_{i})+\\rho_{-1}^{(T_{\\mathrm{CutMix}})}(i,\\tilde{p}_{i})\\right)\\left\\|\\xi_{i}^{(\\tilde{p}_{i})}\\right\\|^{-2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 71}, {"type": "text", "text": "and ", "page_idx": 71}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\gamma(s,k)=\\gamma_{1}^{(T_{\\mathrm{CutMix}})}(s,k)+\\gamma_{-1}^{(T_{\\mathrm{CutMix}})}(s,k),}\\\\ {\\rho(i,p)=\\rho_{1}^{(T_{\\mathrm{CutMix}})}(i,p)+\\rho_{-1}^{(T_{\\mathrm{CutMix}})}(i,p),}\\end{array}\n$$", "text_format": "latex", "page_idx": 71}, {"type": "text", "text": "for each $s\\in\\{\\pm1\\},k\\in[K]\\setminus\\{1\\},i\\in[n]$ and $p\\in[P]\\setminus\\{p_{i}^{*}\\}$ . If we choose $j\\in[n],q\\in[P]\\setminus\\{p_{j}^{*}\\}$ such that $\\rho(j,q)={\\mathrm{max}}_{i\\in[n],p\\in[P]\\backslash\\{p_{i}^{*}\\}}\\,\\rho(i,p)$ , then we have ", "page_idx": 71}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left<w_{y_{j}}^{(T_{\\mathrm{CutMix}})}-w_{-y_{j}}^{(T_{\\mathrm{CutMix}})},\\xi_{j}^{(q)}\\right>}\\\\ &{=\\left<w_{y_{j}}^{(0)}-w_{-y_{j}}^{(0)},\\xi_{j}^{(q)}\\right>+\\rho(j,q)+y_{j}\\underset{i\\in[n],p\\in[P]\\backslash\\{\\xi_{i}^{*}\\}}{\\sum}\\!y_{i}\\rho(i,p)\\frac{\\left<\\xi_{i}^{(p)},\\xi_{j}^{(q)}\\right>}{\\left\\|\\xi_{i}^{(p)}\\right\\|^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 71}, {"type": "text", "text": "From the event $E_{\\mathrm{init}}$ defined in Lemma B.2, (A8), and (8), ", "page_idx": 72}, {"type": "equation", "text": "$$\n\\left|\\left\\langle w_{y_{j}}^{(0)}-w_{-y_{j}}^{(0)},\\xi_{j}^{(q)}\\right\\rangle\\right|=o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right)\\leq\\frac{1}{2}\\left\\langle w_{y_{j}}^{(T_{\\mathrm{CutMix}})}-w_{-y_{j}}^{(T_{\\mathrm{CutMix}})},\\xi_{j}^{(q)}\\right\\rangle,\n$$", "text_format": "latex", "page_idx": 72}, {"type": "text", "text": "where the inequality holds since $\\left\\langle w_{y_{j}}^{(T_{\\mathrm{CutMix}})}-w_{-y_{j}}^{(T_{\\mathrm{CutMix}})},\\xi_{j}^{(q)}\\right\\rangle=\\Omega(1)$ (TutMix), ) =2(1). In addition,by triangular inequality, we have ", "page_idx": 72}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\underbrace{\\sum_{i\\in[n],p\\in[P]\\backslash\\{p_{i}^{*}\\}}y_{i}\\rho(i,p)\\frac{\\left\\langle\\xi_{i}^{(p)},\\xi_{j}^{(q)}\\right\\rangle}{\\left\\|\\xi_{i}^{(p)}\\right\\|^{2}}\\right|\\leq\\displaystyle\\sum_{\\begin{array}{c}{(i,p)\\neq(j,q)}\\\\ {(i,p)\\neq(j,q)}\\end{array}}\\rho(i,p)\\frac{\\left|\\left\\langle\\xi_{i}^{(p)},\\xi_{j}^{(q)}\\right\\rangle\\right|}{\\left\\|\\xi_{i}^{(p)}\\right\\|^{2}}}\\\\ {\\leq\\rho(j,q)\\widetilde{\\mathcal{O}}\\left(n P\\sigma_{\\mathrm{d}}\\sigma_{\\mathrm{b}}^{-1}d^{-\\frac{1}{2}}\\right)\\leq\\frac{\\rho(j,q)}{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 72}, {"type": "text", "text": "where the last inequality is due to (9). Hence, ", "page_idx": 72}, {"type": "equation", "text": "$$\n\\frac{1}{3}\\rho(j,q)\\leq\\left|\\left\\langle\\pmb{w}_{y_{j}}^{(T_{\\mathrm{CutMix}})}-\\pmb{w}_{-y_{j}}^{(T_{\\mathrm{CutMix}})},\\xi_{j}^{(q)}\\right\\rangle\\right|\\leq3\\rho(j,q)\n$$", "text_format": "latex", "page_idx": 72}, {"type": "text", "text": "and we have $\\rho(j,q)=\\widetilde{\\mathcal{O}}(\\beta^{-1})$ ", "page_idx": 72}, {"type": "text", "text": "Let $(X,y)\\,\\sim\\,{\\mathcal{D}}$ be a test data with $\\pmb{X}\\;=\\;\\left(\\pmb{x}^{(1)},\\pmb{\\mathscr{s}}...\\,,\\pmb{x}^{(P)}\\right)\\;\\in\\;\\mathbb{R}^{d\\times P}$ having feature patch $p^{*}$ \uff0c dominant noise patch $\\tilde{p}$ and feature vector $\\pmb{v}_{y,k}$ . We have $\\pmb{x}^{(p)}\\sim N(\\mathbf{0},\\sigma_{\\mathrm{b}}^{2}\\pmb{\\Lambda})$ for each $p\\in[P]\\;\\backslash$ $\\{p^{*},\\tilde{p}\\}$ and $\\pmb{x}^{(\\widetilde{p})}-\\alpha\\pmb{v}_{s,1}\\sim N(\\mathbf{0},\\sigma_{\\mathrm{d}}^{2}\\pmb{\\Lambda})$ for some $s\\in\\{\\pm1\\}$ . Therefore, for all $p\\in[P]\\setminus\\{p^{*},\\tilde{p}\\}$ ", "page_idx": 72}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left|\\phi\\left(\\left\\langle w_{1}^{(T_{\\mathrm{CuuMis}})},\\mathbf{\\boldsymbol{x}}^{(p)}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-1}^{(T_{\\mathrm{CuMis}})},\\mathbf{\\boldsymbol{x}}^{(p)}\\right\\rangle\\right)\\right|}\\\\ &{\\leq\\left|\\left\\langle w_{1}^{(T_{\\mathrm{CuMis}})}-w_{-1}^{(T_{\\mathrm{CuMis}})},\\mathbf{\\boldsymbol{x}}^{(p)}\\right\\rangle\\right|}\\\\ &{=\\left|\\left\\langle w_{1}^{(0)}-w_{-1}^{(0)},\\boldsymbol{x}^{(p)}\\right\\rangle\\right|+\\displaystyle\\sum_{i\\in[n],q\\in[P]\\backslash\\{\\boldsymbol{y}_{i}^{+}\\}}\\rho(i,q)\\frac{\\left|\\left\\langle\\xi_{i}^{(q)},\\mathbf{\\boldsymbol{x}}^{(p)}\\right\\rangle\\right|}{\\left\\|\\xi_{i}^{(q)}\\right\\|^{2}}}\\\\ &{\\leq\\tilde{\\mathcal{O}}\\left(\\sigma_{0}\\sigma_{\\mathrm{b}}d^{\\frac{1}{2}}\\right)+\\tilde{\\mathcal{O}}\\left(n P\\beta^{-1}\\sigma_{\\mathrm{d}}\\sigma_{\\mathrm{b}}^{-1}d^{-\\frac{1}{2}}\\right)}\\\\ &{=o\\left(\\frac{1}{\\mathrm{polylog}(d)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 72}, {"type": "text", "text": "with probability at least $\\textstyle1-o\\left({\\frac{1}{\\operatorname{poly}(d)}}\\right)$ due to Lemma B.2. In addition, ", "page_idx": 72}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left|\\phi\\left(\\left\\langle w_{1}^{(T_{\\mathrm{Cautin}})},x^{(j)}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-1}^{(T_{\\mathrm{Cautin}})},x^{(j)}\\right\\rangle\\right)\\right|}\\\\ &{\\leq\\left|\\left\\langle w_{1}^{(T_{\\mathrm{Cautin}})}-w_{-1}^{(T_{\\mathrm{Cautin}})},x^{(j)}\\right\\rangle\\right|}\\\\ &{\\leq\\alpha\\left|\\left\\langle w_{1}^{(T_{\\mathrm{Cautin}})}-w_{-1}^{(T_{\\mathrm{Cavin}})},w_{s,1}\\right\\rangle\\right|+\\left|\\left\\langle w_{1}^{(T_{\\mathrm{Cautin}})}-w_{-1}^{(T_{\\mathrm{Cautin}})},x^{(j)}-\\alpha v_{s,1}\\right\\rangle\\right|}\\\\ &{\\leq\\alpha\\beta^{-1}\\left|\\phi\\left(\\left\\langle w_{1}^{(T_{\\mathrm{Cautin}})},w_{s,1}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-1}^{(T_{\\mathrm{Cautin}})},w_{s,1}\\right\\rangle\\right)\\right|}\\\\ &{\\quad+\\left|\\left\\langle w_{1}^{(0)}-w_{-1}^{(0)},x^{(j)}-\\alpha v_{s,1}\\right\\rangle\\right|+\\underbrace{\\left|\\left\\langle w_{-1}^{(T_{\\mathrm{Cavin}})},w_{1}\\right\\rangle\\right|}_{\\mathrm{tet~fluation}\\:\\forall\\left\\langle t_{\\mathrm{Cav}}^{(j)},w_{1}\\right\\rangle}\\right|}\\\\ &{\\leq\\tilde{\\mathcal{O}}\\left(\\alpha\\beta^{-1}\\right)+\\tilde{\\mathcal{O}}\\left(\\sigma_{0}\\sigma_{d}\\mathtt{d}^{2}\\right)+\\tilde{\\mathcal{O}}\\left(n{p}\\beta^{-1}\\sigma_{d}\\sigma_{b}^{-1}d^{-\\frac{1}{2}}\\right)}\\\\ &{=o\\left(\\frac{1}{\\mathrm{polylog}\\left(d\\right)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 72}, {"type": "text", "text": "with probabilityat least $\\textstyle1-o\\left({\\frac{1}{\\operatorname{poly}(d)}}\\right)$ where the last quality is de to (8),(9),(10), and (A). ", "page_idx": 72}, {"type": "text", "text": "Suppose (36) and (37) holds. Then, ", "page_idx": 73}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad y f_{W^{(T_{\\mathrm{CutMix}})}}(X)}\\\\ &{=\\left(\\phi\\left(\\left\\langle w_{y}^{(T_{\\mathrm{CutMix}})},v_{y,k}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-y}^{(T_{\\mathrm{CutMix}})},v_{y,k}\\right\\rangle\\right)\\right)}\\\\ &{\\quad+\\displaystyle\\sum_{p\\in[P]\\backslash\\{p^{*}\\}}\\left(\\phi\\left(\\left\\langle w_{y}^{(T_{\\mathrm{CutMix}})},x^{(p)}\\right\\rangle\\right)-\\phi\\left(\\left\\langle w_{-y}^{(T_{\\mathrm{CutMix}})},x^{(p)}\\right\\rangle\\right)\\right)}\\\\ &{=\\Omega(1)-o\\left(\\displaystyle\\frac{1}{\\mathrm{polylog}(d)}\\right)}\\\\ &{>0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 73}, {"type": "text", "text": "Hence, we have our conclusion. ", "page_idx": 73}, {"type": "text", "text": "F Technical Lemmas ", "text_level": 1, "page_idx": 74}, {"type": "text", "text": "In this section, we introduce technical lemmas that are used for proving the main theorems. We present their proofs here for better readability. ", "page_idx": 74}, {"type": "text", "text": "The following lemma is used in Section C.2.4 and Section D.2.4: ", "page_idx": 74}, {"type": "text", "text": "Lemma F.1.For any $z,\\delta\\in\\mathbb{R},$ ", "page_idx": 74}, {"type": "equation", "text": "$$\n|\\phi(z)-(z+\\delta)\\phi^{\\prime}(z)|\\leq r+|\\delta|.\n$$", "text_format": "latex", "page_idx": 74}, {"type": "text", "text": "Proof of Lemma $F.l$ ", "page_idx": 74}, {"type": "equation", "text": "$$\n\\phi(z)-z\\phi^{\\prime}(z)=\\left\\{\\begin{array}{l l}{z-\\frac{1-\\beta}{2}r-z=-\\frac{1-\\beta}{2}r=-\\frac{1-\\beta}{2}r}&{\\mathrm{if~}z\\geq r}\\\\ {\\frac{1-\\beta}{2r}z^{2}+\\beta z-\\left(\\frac{1-\\beta}{r}z+\\beta\\right)z=\\frac{1-\\beta}{2r}z^{2}}&{\\mathrm{if~}0\\leq z\\leq r\\;,}\\\\ {\\beta z-\\beta z=0}&{\\mathrm{if~}z<0}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 74}, {"type": "text", "text": "and we obtain ", "page_idx": 74}, {"type": "equation", "text": "$$\n|\\phi(z)-(z+\\delta)\\phi^{\\prime}(z)|\\leq|\\phi(z)-z\\phi^{\\prime}(z)|+|\\delta|\\phi^{\\prime}(z)\\leq\\frac{1-\\beta}{2}r+|\\delta|\\leq r+|\\delta|.\n$$", "text_format": "latex", "page_idx": 74}, {"type": "text", "text": "The following lemma is used in Section C.2.4. ", "page_idx": 74}, {"type": "text", "text": "Lemma F.2. Suppose $E_{\\mathrm{init}}$ occurs. Then, for any model parameter $W=\\{\\pmb{w}_{1},\\pmb{w}_{-1}\\}$ we have ", "page_idx": 74}, {"type": "equation", "text": "$$\n\\left\\|\\nabla_{W}\\sum_{i\\in\\mathcal{V}_{s,k}}\\ell\\left(y_{i}f_{W}(\\pmb{X}_{i})\\right)\\right\\|^{2}\\leq8P^{2}\\sigma_{\\mathrm{d}}^{2}d|\\mathcal{V}_{s,k}|\\sum_{i\\in\\mathcal{V}_{s,k}}\\ell(y_{i}f_{W}(\\pmb{X}_{i})),\n$$", "text_format": "latex", "page_idx": 74}, {"type": "text", "text": "for each $s\\in\\{\\pm1\\}$ and $k\\in[K]$ ", "page_idx": 74}, {"type": "text", "text": "Proof of Lemma $F.2$ .For each $s\\in\\{\\pm1\\}$ and $i\\in[n]$ , we have ", "page_idx": 74}, {"type": "equation", "text": "$$\n\\|\\nabla_{w_{s}}f_{W}(X_{i})\\|=\\left\\|\\sum_{p\\in[P]}\\phi^{\\prime}\\left(\\left\\langle w_{s},x_{i}^{(p)}\\right\\rangle\\right)x_{i}^{(p)}\\right\\|\\leq P\\operatorname*{max}_{p\\in[P]}\\left\\|x_{i}^{(p)}\\right\\|\\leq2P\\sigma_{\\mathrm{d}}d^{\\frac{1}{2}},\n$$", "text_format": "latex", "page_idx": 74}, {"type": "text", "text": "where the inequality is due to the condition from the event $E_{\\mathrm{init}}$ defined in Lemma B.2 and (A7).. Therefore, for each $s\\in\\{\\pm1\\}$ ,wehave ", "page_idx": 74}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left|\\nabla_{\\mathbf{w}_{i}}\\displaystyle\\sum_{i\\in V_{s,k}}\\ell\\left(y_{i}f_{W}(X_{i})\\right)\\right|^{2}=\\left|\\displaystyle\\prod_{i\\in V_{s,k}}\\ell\\left(y_{i}f_{W}(X_{i})\\right)\\nabla_{\\mathbf{w}_{i}}f_{W}(X_{i})\\right|^{2}}\\\\ &{\\leq\\left(\\displaystyle\\sum_{i\\in V_{s,k}}\\ell\\left(y_{i}f_{W}(X_{i})\\right)\\left\\|\\nabla_{\\mathbf{w}_{i}}f_{W}(X_{i})\\right\\|\\right)^{2}}\\\\ &{\\leq4D^{2}\\sigma_{\\mathbf{d}}^{2}d\\left(\\displaystyle\\sum_{i\\in V_{s,k}}\\ell^{\\prime}\\left(y_{i}f_{W}(X_{i})\\right)\\right)^{2}}\\\\ &{\\leq4D^{2}\\sigma_{\\mathbf{d}}^{2}d|V_{s,k}|\\displaystyle\\sum_{i\\in V_{s,k}}\\ell\\left(\\ell\\left(y_{i}f_{W}(X_{i})\\right)\\right)^{2}}\\\\ &{\\leq4D^{2}\\sigma_{\\mathbf{d}}^{2}d|V_{s,k}|\\displaystyle\\sum_{i\\in V_{s,k}}\\ell\\left(\\ell\\left(y_{i}f_{W}(X_{i})\\right)\\right)^{2}}\\\\ &{\\leq4D^{2}\\sigma_{\\mathbf{d}}^{2}d|V_{s,k}|\\displaystyle\\sum_{i\\in V_{s,k}}\\ell\\left(\\ell\\left(y_{i}f_{W}(X_{i})\\right).\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 74}, {"type": "text", "text": "The first inequality is due to triangular inequality, the third inequality is due to Cauchy-Schwartz inequality and the last inequality is due to $0\\bar{\\leq}-\\bar{\\ell^{\\prime}}\\leq1$ , which can be used to show $(\\ell^{\\prime})^{2^{\\prime}}\\leq-\\ell^{\\prime}\\leq\\ell$ As a result, we have our conclusion: ", "page_idx": 74}, {"type": "equation", "text": "$$\n\\left\\|\\nabla_{W}\\sum_{i\\in\\mathcal{V}_{s,k}}\\ell\\left(y_{i}f_{W}(X_{i})\\right)\\right\\|^{2}=\\left\\|\\nabla_{w_{1}}\\sum_{i\\in\\mathcal{V}_{s,k}}\\ell\\left(y_{i}f_{W}(X_{i})\\right)\\right\\|^{2}+\\left\\|\\nabla_{w_{-1}}\\sum_{i\\in\\mathcal{V}_{s,k}}\\ell\\left(y_{i}f_{W}(X_{i})\\right)\\right\\|^{2}\n$$", "text_format": "latex", "page_idx": 74}, {"type": "equation", "text": "$$\n\\leq8P^{2}\\sigma_{\\mathrm{d}}^{2}d|\\mathcal{V}_{s,k}|\\sum_{i\\in\\mathcal{V}_{s,k}}\\ell(y_{i}f_{W}(\\boldsymbol{X}_{i})).\n$$", "text_format": "latex", "page_idx": 75}, {"type": "text", "text": "The following lemma is used in Section D.2.4. ", "page_idx": 75}, {"type": "text", "text": "Lemma F.3. Suppose $E_{\\mathrm{init}}$ occurs. Then, for any model parameter $W=\\{\\pmb{w}_{1},\\pmb{w}_{-1}\\}$ we have ", "page_idx": 75}, {"type": "equation", "text": "$$\n\\left\\|\\nabla\\sum_{i\\in\\mathcal{V}_{s,k}}\\mathbb{E}_{\\mathcal{C}\\sim\\mathcal{D}_{C}}[\\ell\\left(y_{i}f_{W^{(t)}}(X_{i,c})\\right)]\\right\\|^{2}\\leq8P^{2}\\sigma_{\\mathrm{d}}^{2}d|\\mathcal{V}_{s,k}|\\sum_{i\\in\\mathcal{V}_{s,k}}\\mathbb{E}_{\\mathcal{C}\\sim\\mathcal{D}_{C}}[\\ell(y_{i}f_{W^{(t)}}(X_{i,c}))]\n$$", "text_format": "latex", "page_idx": 75}, {"type": "text", "text": "for each $s\\in\\{\\pm1\\}$ and $k\\in[K]$ ", "page_idx": 75}, {"type": "text", "text": "Proof of Lemma $F.3$ .For each $s\\in\\{\\pm1\\}$ \uff0c $i\\in[n]$ and $\\mathcal{C}\\subset[P]$ with $|{\\mathcal{C}}|=C$ , we have ", "page_idx": 75}, {"type": "equation", "text": "$$\n\\|\\nabla_{w_{s}}f_{W}(X_{i,c})\\|=\\left\\|\\sum_{p\\notin C}\\phi^{\\prime}\\left(\\left\\langle w_{s},x_{i}^{(p)}\\right\\rangle\\right)x_{i}^{(p)}\\right\\|\\leq P\\operatorname*{max}_{p\\in[P]}\\left\\|x_{i}^{(p)}\\right\\|\\leq2P\\sigma_{\\mathrm{d}}d^{\\frac{1}{2}},\n$$", "text_format": "latex", "page_idx": 75}, {"type": "text", "text": "where the inequality is due to the condition from the event $E_{\\mathrm{init}}$ defined in Lemma B.2 and (A7). Therefore, for any $s\\in\\{\\pm1\\}$ ,wehave ", "page_idx": 75}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\nabla_{w_{k}}\\sum_{i=0}^{K}\\mathbb{E}_{\\sigma\\sim\\mathcal{P}_{k}}[\\ell\\left(y_{k}/w_{k}(X_{i})\\right)]\\right\\|^{2}}\\\\ &{=\\left\\|\\sum_{i=0}^{K}\\mathbb{E}_{\\sigma\\sim\\mathcal{P}_{k}}[\\ell\\left(y_{k}/w_{k}\\left(X_{i}\\right)\\right)\\nabla_{w_{k}}\\int\\mathcal{W}(X_{i},c)]\\right\\|^{2}}\\\\ &{\\leq\\left(\\sqrt{\\sum_{i=0}^{K}\\mathbb{E}_{\\sigma\\sim\\mathcal{P}_{k}}\\{\\left(y_{k}/w_{k}\\left(X_{i}\\right)\\right)\\mathbb{I}[\\mathcal{Z}_{w},\\int\\mathcal{W}(X_{i},c)]\\}}\\right)^{2}}\\\\ &{\\leq4I^{2}\\sigma_{\\mathcal{Q}}^{2}\\left(\\sqrt{\\sum_{i=0}^{K}\\mathbb{E}_{\\sigma\\sim\\mathcal{P}_{k}}\\{\\ell\\left(y_{k}/w_{k}\\left(X_{i}\\right)\\right)\\}}\\right)^{2}}\\\\ &{\\leq4I^{2}\\sigma_{\\mathcal{Q}}^{2}d[\\nu_{k,i}]\\sum_{i=0}^{K}\\mathbb{E}_{\\sigma\\sim\\mathcal{P}_{k}}\\Big[\\ell\\left(\\ell\\left(y_{k}/w_{k}\\left(X_{i}\\right)\\right)\\right)^{2}\\Big]}\\\\ &{\\leq4I^{2}\\sigma_{\\mathcal{Q}}^{2}d[\\nu_{k,i}]\\sum_{i=0}^{K}\\mathbb{E}_{\\sigma\\sim\\mathcal{P}_{k}}[\\ell\\left(y_{k}/w_{k}\\left(X_{i}\\right)\\right)]^{2}\\Big]}\\\\ &{\\leq4I^{2}\\sigma_{\\mathcal{Q}}^{2}d[\\nu_{k,i}]\\sum_{i=0}^{K}\\mathbb{E}_{\\sigma\\sim\\mathcal{P}_{k}}[\\ell\\left(y_{k}/w_{k}\\left(X_{i}\\right)\\right)].}\\end{array}\n$$", "text_format": "latex", "page_idx": 75}, {"type": "text", "text": "The first inequality is due to triangular inequality, the third inequality is due to Cauchy-Schwartz inequality and the last inequality is due to $0\\bar{\\leq}-\\bar{\\ell^{\\prime}}\\leq1$ ,which can be used to show $(\\ell^{\\prime})^{2^{\\prime}}\\leq-\\ell^{\\prime}\\leq\\ell.$ As a result, we have our conclusion: ", "page_idx": 75}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\nabla_{W}\\displaystyle\\sum_{i\\in\\mathcal{V}_{s,k}}\\mathbb{E}_{\\mathcal{C}\\sim\\mathcal{D}_{c}}\\left[\\ell\\left(y_{i}f_{W}(X_{i,c})\\right)\\right]\\right\\|^{2}}\\\\ &{=\\left\\|\\nabla_{w_{1}}\\displaystyle\\sum_{i\\in\\mathcal{V}_{s,k}}\\mathbb{E}_{\\mathcal{C}\\sim\\mathcal{D}_{c}}\\left[\\ell\\left(y_{i}f_{W}(X_{i,c})\\right)\\right]\\right\\|^{2}+\\left\\|\\nabla_{w_{-1}}\\displaystyle\\sum_{i\\in\\mathcal{V}_{s,k}}\\mathbb{E}_{\\mathcal{C}\\sim\\mathcal{D}_{c}}\\left[\\ell\\left(y_{i}f_{W}(X_{i,c})\\right)\\right]\\right\\|^{2}}\\\\ &{\\leq8P^{2}\\sigma_{{\\sf d}}^{2}d|\\mathcal{V}_{s,k}|\\displaystyle\\sum_{i\\in\\mathcal{V}_{s,k}}\\mathbb{E}_{\\mathcal{C}\\sim\\mathcal{D}_{c}}\\left[\\ell(y_{i}f_{W}(X_{i,c}))\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 75}, {"type": "text", "text": "The following lemma guarantees the existence and characterizes the minimum of the CutMix loss in SectionE.2.2. ", "page_idx": 76}, {"type": "text", "text": "Lemma F.4. Suppose the event $E_{\\mathrm{init}}$ occurs. Let $g_{1},g_{-1}:\\mathbb{R}\\times\\mathbb{R}\\to\\mathbb{R}$ be defined as ", "page_idx": 76}, {"type": "equation", "text": "$$\ng_{s}(z_{1},z_{-1}):=\\frac{|\\mathcal{V}_{s}|}{|\\mathcal{V}_{-s}|}\\ell^{\\prime}(P z_{s})+\\frac{2}{P}\\mathbb{E}_{S\\sim\\mathcal{D}_{S}}\\left[|S|\\ell^{\\prime}(|S|z_{s}-(P-|S|)z_{-s})\\right]+\\frac{P-1}{3P},\n$$", "text_format": "latex", "page_idx": 76}, {"type": "text", "text": "for each $s\\in\\{\\pm1\\}$ There exist unique $z_{1}^{*},z_{-1}^{*}>0$ such that $g_{1}\\bigl(z_{1}^{*},z_{-1}^{*}\\bigr)\\,=\\,g_{-1}\\bigl(z_{1}^{*},z_{-1}^{*}\\bigr)\\,=\\,0$ Furthermore, we have $z_{1}^{*},z_{-1}^{*}=\\Theta(1)$ ", "page_idx": 76}, {"type": "text", "text": "Proof of Lemma $F.4.$ For each $z_{1}>0$ ", "page_idx": 76}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g_{-1}(z_{1},0)=\\left(\\displaystyle\\frac{|\\mathcal{V}_{-1}|}{|\\mathcal{V}_{1}|}+1\\right)\\cdot\\left(-\\frac{1}{2}\\right)+\\displaystyle\\frac{2}{P}\\mathbb{E}_{\\mathcal{S}\\sim\\mathcal{D}_{s}}[|\\mathcal{S}|\\ell^{\\prime}(-(P-|\\mathcal{S}|)z_{1})]+\\frac{P-1}{3P}}\\\\ &{\\hphantom{\\times\\left(\\frac{|\\mathcal{V}_{-1}|}{|\\mathcal{V}_{1}|}+1\\right)\\cdot}<\\left(\\displaystyle\\frac{1}{2}\\right)+\\frac{P-1}{3P}<0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 76}, {"type": "text", "text": "since $\\ell^{\\prime}(z)\\leq-\\frac{1}{2}$ for any $z\\leq0$ and we use $\\begin{array}{r}{\\frac{25}{52}n\\leq|\\mathcal{V}_{1}|,|\\mathcal{V}_{-1}|\\leq\\frac{27}{52}n}\\end{array}$ from the event $E_{\\mathrm{init}}$ defined in Lemma B.2. In addition, ", "page_idx": 76}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad g_{-1}(z_{1},P z_{1}+\\log9)}\\\\ &{=\\displaystyle\\frac{|\\mathcal{V}_{-1}|}{|\\mathcal{V}_{1}|}\\ell^{\\prime}(P^{2}z_{1}+P\\log9)+\\frac{2}{P}\\mathbb{E}_{S\\sim\\mathcal{D}_{s}}[|S|\\ell^{\\prime}(|S|P z_{1}+|S|\\log9-(P-|S|)z_{1})]+\\frac{P-1}{3P}}\\\\ &{\\geq\\left(\\frac{|\\mathcal{V}_{-1}|}{|\\mathcal{V}_{1}|}+1\\right)\\ell^{\\prime}(\\log9)+\\frac{P-1}{3P}}\\end{array}\n$$", "text_format": "latex", "page_idx": 76}, {"type": "text", "text": "Where we use $\\begin{array}{r}{{\\frac{25}{52}}n\\leq|\\mathcal{V}_{1}|,|\\mathcal{V}_{-1}|\\leq{\\frac{27}{52}}n}\\end{array}$ from the event $E_{\\mathrm{init}}$ defined in Lemma B.2 and (A1) for the last inequality. ", "page_idx": 76}, {"type": "text", "text": "Since $z\\,\\mapsto\\,g_{-1}(z_{1},z)$ is strictly increasing and by intermediate value theorem, there exists $S:$ $(0,\\infty)\\ \\rightarrow\\ (0,\\infty)$ such that $z\\ =\\ S(z_{1})$ is a unique solution of $g_{-1}(z_{1},z)~=~0$ and $S(z_{1})~<$ $P z_{1}+\\log9$ . Note that $S$ is strictly increasing since $g_{-1}(z_{1},z_{-1})$ is strictly decreasing with respect to $z_{1}$ and strictly increasing with respect to $z_{-1}$ . Also, if $S(z)$ is bounded above, i.e., there exists some $U>0$ such that $S(z)\\leq U$ for any $z>0$ ", "page_idx": 76}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{z\\rightarrow\\infty}{\\operatorname*{lim}}g_{-1}(z,S(z))}\\\\ &{=\\underset{z\\rightarrow\\infty}{\\operatorname*{lim}}\\left(\\frac{\\left|\\mathcal{V}_{-1}\\right|}{\\left|\\mathcal{V}_{1}\\right|}\\ell^{\\prime}(P S(z))+\\frac{2}{P}\\mathbb{E}_{S\\sim\\mathcal{D}_{S}}\\left[|S|\\ell^{\\prime}\\big(|S|S(z)-(P-|S|)z\\big)\\right]+\\frac{P-1}{3P}\\right)}\\\\ &{\\leq\\underset{z\\rightarrow\\infty}{\\operatorname*{lim}}\\left(\\frac{\\left|\\mathcal{V}_{-1}\\right|}{\\left|\\mathcal{V}_{1}\\right|}\\ell^{\\prime}(P U)+\\frac{2}{P}\\mathbb{E}_{S\\sim\\mathcal{D}_{S}}\\left[|S|\\ell^{\\prime}\\big(|S|U-(P-|S|)z\\big)\\right]+\\frac{P-1}{3P}\\right)}\\\\ &{\\leq-\\frac{2}{P}\\mathbb{E}_{S\\sim\\mathcal{D}_{S}}\\left[|S|\\cdot\\mathbb{I}_{|S|\\neq P}\\right]+\\frac{P-1}{3P}=-\\frac{P-1}{P+1}+\\frac{P-1}{3P}}\\end{array}\n$$", "text_format": "latex", "page_idx": 76}, {"type": "text", "text": "and it is contradictory. Hence, we have $\\mathrm{lim}_{z\\rightarrow\\infty}\\,S(z)=\\infty$ ", "page_idx": 76}, {"type": "text", "text": "Let us choose $\\underline{{z}}>0$ such that ", "page_idx": 76}, {"type": "equation", "text": "$$\n\\underline{{z}}=\\frac{1}{P}\\log\\left(\\frac{3P\\left(1+\\frac{|\\mathcal{V}_{1}|}{|\\mathcal{V}_{-1}|}\\right)}{P-1}-1\\right),\n$$", "text_format": "latex", "page_idx": 76}, {"type": "text", "text": "and thus ", "page_idx": 76}, {"type": "equation", "text": "$$\n\\ell^{\\prime}(P\\underline{{z}})=-\\frac{P-1}{3P\\left(1+\\frac{|\\mathcal{V}_{1}|}{|\\mathcal{V}_{-1}|}\\right)}.\n$$", "text_format": "latex", "page_idx": 76}, {"type": "text", "text": "We have ", "page_idx": 76}, {"type": "equation", "text": "$$\ng_{1}(\\underline{{{z}}},S(\\underline{{{z}}}))=\\frac{|\\mathcal{V}_{1}|}{|\\mathcal{V}_{-1}|}\\ell^{\\prime}(P\\underline{{{z}}})+\\frac{2}{P}\\mathbb{E}_{S\\sim\\mathcal{D}_{S}}\\left[|S|\\ell^{\\prime}\\big(|S|\\underline{{{z}}}-(P-|S|)S(\\underline{{{z}}})\\big)\\right]+\\frac{P-1}{3P}\n$$", "text_format": "latex", "page_idx": 76}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\leq\\left(\\frac{|\\mathcal{V}_{1}|}{|\\mathcal{V}_{-1}|}+1\\right)\\ell^{\\prime}(P\\underline{{z}})+\\frac{P-1}{3P}}\\\\ {\\displaystyle=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 77}, {"type": "text", "text": "Next, we will prove the existence of $z^{*}>0$ such that $g_{1}\\bigl(z^{*},S(z^{*})\\bigr)>0$ . Let us choose $\\epsilon>0$ such that ", "page_idx": 77}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\epsilon^{-1}=\\operatorname*{max}\\Bigg\\{\\frac{3P(P+1)|\\mathcal{V}_{-1}|}{(P-2)(P+2)|\\mathcal{V}_{1}|}+\\frac{3(P-1)}{P-2},\\frac{3}{2}\\left(1+\\frac{P(P+1)|\\mathcal{V}_{-1}|}{(P-1)(P-2)|\\mathcal{V}_{1}|}\\right),\\;\\Bigg.}\\\\ {\\frac{12P}{P-7}\\left(1+\\frac{|\\mathcal{V}_{-1}|}{|\\mathcal{V}_{1}|}\\right),\\frac{12P(P+1)}{(P-2)(P+2)}\\left(1+\\frac{|\\mathcal{V}_{1}|}{|\\mathcal{V}_{-1}|}\\right)\\Bigg\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 77}, {"type": "text", "text": "and note that $\\begin{array}{r l r}{\\epsilon}&{{}=}&{\\Theta(1)}\\end{array}$ .Since $\\begin{array}{l l l}{\\operatorname*{lim}_{z\\to\\infty}S(z)}&{=}&{\\infty}\\end{array}$ \uff0cwe can choose $z^{*}$ such that $\\begin{array}{r}{\\ell^{\\prime}\\left(\\frac{1}{2}\\operatorname*{min}\\left\\{z^{*},S(z^{*})\\right\\}\\right)=-\\frac{\\epsilon}{2}}\\end{array}$ Then, for any $\\begin{array}{r}{t\\geq\\frac{z^{*}}{2}}\\end{array}$ , we have ", "page_idx": 77}, {"type": "equation", "text": "$$\n-\\epsilon<\\ell^{\\prime}(t)<0\\quad\\mathrm{and}\\quad-1<\\ell^{\\prime}(-t)<-1+\\epsilon.\n$$", "text_format": "latex", "page_idx": 77}, {"type": "text", "text": "From the definition of $S$ and (39) with $\\begin{array}{r}{t=P S(z^{*})>\\frac{1}{2}\\operatorname*{min}\\{z^{*},S(z^{*})\\}}\\end{array}$ , we have ", "page_idx": 77}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathcal{S}\\sim\\mathcal{D}_{\\mathcal{S}}}\\left[|S|\\ell^{\\prime}\\big(|S|S(z^{*})-(P-|S|)z^{*}\\big)\\right]=-\\displaystyle\\frac{P}{2}\\left(\\frac{|\\mathcal{V}_{-1}|}{|\\mathcal{V}_{1}|}\\ell^{\\prime}\\big(P S(z^{*})\\big)+\\frac{P-1}{3P}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad<-\\displaystyle\\frac{P-1}{6}+\\frac{P|\\mathcal{V}_{-1}|}{2|\\mathcal{V}_{1}|}\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 77}, {"type": "text", "text": "If $S(z^{*})-(P-1)z^{*}\\geq0$ , then ", "page_idx": 77}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{P S(z^{*})>(P-1)S(z^{*})-z^{*}>...>2S(z^{*})-(P-2)z^{*}}&{}\\\\ {=z^{*}+S(z^{*})+S(z^{*})-(P-1)z^{*}}\\\\ {\\geq z^{*}+S(z^{*})\\geq\\displaystyle\\frac{1}{2}\\operatorname*{min}\\{z^{*},S(z^{*})\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 77}, {"type": "text", "text": "and we have ", "page_idx": 77}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle-\\frac{P-1}{6}+\\frac{P|\\mathcal{V}_{-1}|}{2|\\mathcal{V}_{1}|}\\epsilon>\\mathbb{E}_{\\mathcal{S}\\sim\\mathcal{D}_{S}}\\left[|\\mathcal{S}|\\ell^{\\prime}\\big(|\\mathcal{S}|S(z^{*})-(P-|\\mathcal{S}|)z^{*}\\big)\\right]}&{}\\\\ {\\displaystyle=\\frac{1}{P+1}\\left(\\ell^{\\prime}\\big(S(z^{*})-(P-1)z^{*}\\big)+\\displaystyle\\sum_{m=2}^{P}m\\ell^{\\prime}\\big(m S(z^{*})+(P-m)z^{*}\\big)\\right)}&{}\\\\ {\\displaystyle}&{\\geq\\frac{1}{P+1}\\left(-\\frac{1}{2}-\\left(\\frac{P(P+1)}{2}-1\\right)\\epsilon\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 77}, {"type": "text", "text": "where the last inequality is due to (39). This is contradictory to (38), especially the first term inside the maximum, and we have $S(z^{\\ast})-(P-1)z^{\\ast}<0$ In addition, if $(P-1)S(z^{*})-z^{*}\\leq0$ ,then ", "page_idx": 77}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P z^{*}>(P-1)z^{*}-S(z^{*})>...>2z^{*}-(P-2)S(z^{*})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=z^{*}+S(z^{*})+z^{*}-(P-1)S(z^{*})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\geq z^{*}+S(z^{*})\\geq\\frac{1}{2}\\operatorname*{min}\\{z^{*},S(z^{*})\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 77}, {"type": "text", "text": "and we have ", "page_idx": 77}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad-\\cfrac{P-1}{6}-\\cfrac{P|\\mathcal{V}_{-1}|}{2|\\mathcal{V}_{1}|}\\epsilon}\\\\ &{<\\mathbb{E}_{S\\sim\\mathcal{D}_{s}}\\left[|S|\\ell^{\\prime}\\big(|S|S(z^{*})-(P-|S|)z^{*}\\big)\\right]}\\\\ &{=\\cfrac{1}{P+1}\\left(P\\ell^{\\prime}\\big(P S(z^{*})\\big)+(P-1)\\ell^{\\prime}\\big((P-1)S(z^{*})-z^{*}\\big)+\\displaystyle\\sum_{m=0}^{P-2}m\\ell^{\\prime}\\big(m S(z^{*})-(P-m)z^{*}\\big)\\right)}\\\\ &{<\\cfrac{1}{P+1}\\left(-\\cfrac{(P-1)(P-2)}{2}(1-\\epsilon)-\\cfrac{P-1}{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 77}, {"type": "text", "text": "where the last inequality is due to (39). This is contradictory to (38), especially the second term inside the maximum, and we have $(P-1)S(z^{*})-z^{*}>0$ .Notethatwehave ", "page_idx": 78}, {"type": "equation", "text": "$$\n-\\frac{\\epsilon}{2}=\\ell^{\\prime}\\left(\\frac{1}{2}\\operatorname*{min}\\{z^{*},S(z^{*})\\}\\right)\\geq\\ell^{\\prime}\\left(\\frac{z^{*}}{2P}\\right),\n$$", "text_format": "latex", "page_idx": 78}, {"type": "text", "text": "and since $\\epsilon=\\Theta(1)$ in (38), we have $\\begin{array}{r}{z^{*}\\leq2P\\log\\left(\\frac{2}{\\epsilon}-1\\right)=\\mathcal{O}(1).}\\end{array}$ ", "page_idx": 78}, {"type": "text", "text": "Thus, we have $S(z^{*})-(P-1)z^{*}<0<(P-1)S(z^{*})-z^{*}<P S(z^{*})$ One can consider dividing the interval $[S(z^{*})-(P-1)z^{*},P S(z^{*})]$ into a grid of length $z^{*}+S(z^{*})$ . Then, the interval is equally divided into $P-1$ sub-intervals and O belongs to one of them. In other words, there exists $k\\in[P-2]$ such that ", "page_idx": 78}, {"type": "equation", "text": "$$\nk S(z^{*})-(P-k)z^{*}\\leq0<(k+1)S(z^{*})-(P-k-1)z^{*},\n$$", "text_format": "latex", "page_idx": 78}, {"type": "text", "text": "and note that if $P=3$ , then $k=1$ . The rest of the proof is divided into two cases: $(k+1)S(z^{\\ast})-$ $\\begin{array}{r}{(P-k-1)z^{*}\\geq\\frac{1}{2}(z^{*}+S(z^{*}))}\\end{array}$ or $\\begin{array}{r}{(k+1)S(z^{*})-\\bar{(P-k-1)}z^{*}<\\frac{1}{2}(z^{*}+S(z^{*}))}\\end{array}$ . In both cases, we show that $g_{1}(z^{\\overline{{*}}},S(z^{*}))>0$ ", "page_idx": 78}, {"type": "equation", "text": "$(k+1)S(z^{*})-(P-k-1)z^{*}\\geq\\frac{1}{2}(z^{*}+S(z^{*}))$ ", "text_format": "latex", "page_idx": 78}, {"type": "text", "text": "From (39), we have ", "page_idx": 78}, {"type": "equation", "text": "$$\n-1<\\ell^{\\prime}(-P z^{*})<\\cdot\\cdot\\cdot<\\cdot\\cdot<\\ell^{\\prime}\\big((k-1)S(z^{*})-(P-k+1)z^{*}\\big)<-1+\\epsilon,\n$$", "text_format": "latex", "page_idx": 78}, {"type": "text", "text": "and ", "page_idx": 78}, {"type": "equation", "text": "$$\n-\\epsilon<\\ell^{\\prime}\\big((k+1)S(z^{*})-(P-k-1)z^{*}\\big)<\\dots<\\ell^{\\prime}\\big(P S(z^{*})\\big)<0.\n$$", "text_format": "latex", "page_idx": 78}, {"type": "text", "text": "Thus, we have ", "page_idx": 78}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{\\mathcal{S}\\sim\\mathcal{D}_{S}}\\left[|S|\\ell^{\\prime}\\big(|S|S(z^{*})-(P-|S|)z^{*}\\big)\\right]]}\\\\ &{>\\displaystyle\\frac{1}{P+1}\\left(k\\ell^{\\prime}\\big(k S(z^{*})-(P-k)z^{*}\\big)-\\frac{k(k-1)}{2}\\right)-\\frac{P}{2}\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 78}, {"type": "text", "text": "and we obtain $\\begin{array}{r}{k>\\frac{P-1}{2}}\\end{array}$ since ", "page_idx": 78}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\frac{k(k+1)}{2}}\\\\ &{=\\frac{k(k-1)}{2}+k}\\\\ &{>-\\frac{P(P+1)}{2}\\epsilon-(P+1)\\mathbb{E}_{\\mathcal{S}\\sim\\mathcal{D}_{s}}[|\\mathcal{S}|^{\\ell}(|\\mathcal{S}|S(z^{*})-(P-|\\mathcal{S}|)z^{*})]}\\\\ &{\\quad+k\\ell^{\\prime}(k S(z^{*})-(P-k)z^{*})+k}\\\\ &{>-\\frac{P(P+1)}{2}\\left(1+\\frac{|\\mathcal{V}_{-1}|}{|\\mathcal{V}_{1}|}\\right)\\epsilon+\\frac{(P-1)(P+1)}{6}}\\\\ &{\\geq\\frac{\\frac{P-1}{2}\\left(\\frac{P-1}{2}+1\\right)}{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 78}, {"type": "text", "text": "where the second inequality is due to (40) and the fact that $\\ell^{\\prime}\\geq-1$ , and the last inequality is due to (38), especially the third term inside the maximum. Note that since $k\\in\\mathbb{N}$ $\\begin{array}{r}{k\\geq\\frac{P}{2}}\\end{array}$ ", "page_idx": 78}, {"type": "text", "text": "Note that from (39), we have ", "page_idx": 78}, {"type": "equation", "text": "$$\n-1<\\ell^{\\prime}\\big(-P S(z^{*})\\big)<\\cdot\\cdot<\\ell^{\\prime}\\big((P-k-1)z^{*}-(k+1)S(z^{*})\\big)<-1+\\epsilon,\n$$", "text_format": "latex", "page_idx": 78}, {"type": "text", "text": "and ", "page_idx": 78}, {"type": "equation", "text": "$$\n-\\epsilon<\\ell^{\\prime}\\big((P-k+1)z^{*}-(k-1)z^{*}\\big)<\\cdots<\\ell^{\\prime}(P z^{*})<0.\n$$", "text_format": "latex", "page_idx": 78}, {"type": "text", "text": "Hence, we obtain ", "page_idx": 78}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~\\mathbb{E}_{S\\sim\\mathcal{D}_{S}}\\left[|S|\\ell^{\\prime}\\big(|S|z^{*}-(P-|S|)S(z^{*})\\big)\\right]}\\\\ &{\\ge\\displaystyle\\frac{1}{P+1}\\left(-\\frac{(P-k-1)(P-k)}{2}-\\frac{1}{2}(P-k)-\\big((P-k+1)+\\cdot\\cdot\\cdot+P\\big)\\,\\epsilon\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 78}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\geq-\\frac{(P-k)^{2}}{2(P+1)}-\\frac{1}{P+1}\\cdot\\frac{P(P+1)}{2}\\epsilon}\\\\ {\\displaystyle\\geq-\\frac{P^{2}}{8(P+1)}-\\frac{P}{2}\\epsilon,}\\end{array}\n$$", "text_format": "latex", "page_idx": 79}, {"type": "text", "text": "where we use $\\begin{array}{r}{k\\geq\\frac{P}{2}}\\end{array}$ for the last inequality. Therefore, we have ", "page_idx": 79}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g_{1}(z^{*},S(z^{*}))=\\displaystyle\\frac{|\\mathcal{V}_{1}|}{|\\mathcal{V}_{-1}|}\\ell^{\\prime}(P z^{*})+\\displaystyle\\frac{2}{P}\\mathbb{E}_{S\\sim\\mathcal{D}_{S}}[|S|\\ell^{\\prime}(|S|z^{*}-(P-|S|)S(z^{*}))]+\\displaystyle\\frac{P-1}{3P}}\\\\ &{\\phantom{\\sum_{\\theta_{1}}\\bigl(z^{*},S(z^{*})\\bigr)}\\ge-\\left(\\displaystyle\\frac{|\\mathcal{V}_{1}|}{|\\mathcal{V}_{-1}|}+1\\right)\\epsilon-\\displaystyle\\frac{P}{4(P+1)}+\\frac{P-1}{3P}}\\\\ &{\\phantom{\\sum_{\\theta_{1}}\\bigl(z^{*},S(z^{*})\\bigr)}>0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 79}, {"type": "text", "text": "where the last inequality is due to (38), especially the fourth term inside the maximum. ", "page_idx": 79}, {"type": "equation", "text": "$(k+1)S(z^{*})-(P-k-1)z^{*}<\\textstyle{\\frac{1}{2}}(z^{*}+S(z^{*}))$ ", "text_format": "latex", "page_idx": 79}, {"type": "text", "text": "In this case, we have $\\begin{array}{r}{k S(z^{*})-(P-k)z^{*}\\leq-\\frac{1}{2}(z^{*}+S(z^{*}))}\\end{array}$ . From (39), we have ", "page_idx": 79}, {"type": "equation", "text": "$$\n-1<\\ell^{\\prime}(-P z^{*})<\\cdot\\cdot\\cdot<\\ell^{\\prime}\\big(k S(z^{*})-(P-k)z^{*}\\big)<-1+\\epsilon,\n$$", "text_format": "latex", "page_idx": 79}, {"type": "text", "text": "and ", "page_idx": 79}, {"type": "equation", "text": "$$\n-\\epsilon<\\ell^{\\prime}\\big((k+2)S(z^{*})-(P-k-2)z^{*}\\big)<\\dots<\\ell^{\\prime}\\big(P S(z^{*})\\big)<0.\n$$", "text_format": "latex", "page_idx": 79}, {"type": "text", "text": "Thus, we have ", "page_idx": 79}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~\\mathbb{E}_{S\\sim\\mathcal{D}_{S}}[|S|\\ell^{\\prime}(|S|S(z^{*})-(P-|S|)z^{*}))|]}\\\\ &{>\\displaystyle\\frac{1}{P+1}\\left((k+1)\\ell^{\\prime}\\big((k+1)S(z^{*})-(P-k-1)z^{*}\\big)-\\frac{k(k+1)}{2}\\right)-\\frac{P}{2}\\epsilon,}\\end{array}\n$$", "text_format": "latex", "page_idx": 79}, {"type": "text", "text": "and we obtain $\\begin{array}{r}{k>\\frac{P-1}{2}}\\end{array}$ since ", "page_idx": 79}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{(k+1)^{2}}{2}}\\\\ &{=\\frac{k(k+1)}{2}+\\frac{k+1}{2}}\\\\ &{>-\\frac{P(P+1)}{2}\\epsilon-(P+1)\\mathbb{E}_{\\mathcal{S}\\sim\\mathcal{P}_{x}}[|\\mathcal{S}|^{\\ell}(|\\mathcal{S}|S(z^{*})-(P-|\\mathcal{S}|)z^{*})]}\\\\ &{\\quad+(k+1)\\ell^{\\prime}((k+1)S(z^{*})-(P-k-1)z^{*})+\\frac{k+1}{2}}\\\\ &{>-\\frac{P(P+1)}{2}\\left(1+\\frac{|\\mathcal{V}_{-1}|}{|\\mathcal{V}_{1}|}\\right)\\epsilon+\\frac{(P-1)(P+1)}{6}}\\\\ &{>\\frac{(\\frac{P-1}{2}+1)^{2}}{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 79}, {"type": "text", "text": "where the second inequality is due to (40) and the fact that $\\ell^{\\prime}(z)\\,\\geq\\,-\\frac{1}{2}\\quad\\forall z\\,\\geq\\,0$ ,and the last inequality is due to our (38), especially the third term inside the maximum. Note that since $k\\in\\mathbb{N}$ we have $\\begin{array}{r}{\\dot{k}\\geq\\frac{P}{2}}\\end{array}$ ", "page_idx": 79}, {"type": "text", "text": "Note that from (39), we have ", "page_idx": 79}, {"type": "equation", "text": "$$\n-1<\\ell^{\\prime}\\big(-P S(z^{*})\\big)<\\dots<\\ell^{\\prime}\\big((P-k-2)z^{*}-(k+2)S(z^{*})\\big)<-1+\\epsilon,\n$$", "text_format": "latex", "page_idx": 79}, {"type": "text", "text": "and ", "page_idx": 79}, {"type": "equation", "text": "$$\n-\\epsilon<\\ell^{\\prime}\\big((P-k)z^{*}-k z^{*}\\big)<\\cdots<\\ell^{\\prime}(P z^{*})<0.\n$$", "text_format": "latex", "page_idx": 79}, {"type": "text", "text": "Hence, we obtain ", "page_idx": 79}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ \\ \\mathbb{E}_{S\\sim\\mathcal{D}_{S}}[|S|\\ell^{\\prime}(|S|z^{*}-(P-|S|)S(z^{*})]}\\\\ &{\\ge\\displaystyle\\frac{1}{P+1}\\left(-\\frac{(P-k-1)(P-k)}{2}-((P-k)+\\cdot\\cdot\\cdot+P)\\,\\epsilon\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 79}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\geq-\\frac{(P-k)(P-k-1)}{2(P+1)}-\\frac{1}{P+1}\\cdot\\frac{P(P+1)}{2}\\epsilon}\\\\ {\\displaystyle\\geq-\\frac{(P-k)^{2}}{2(P+1)}-\\frac{1}{P+1}\\cdot\\frac{P(P+1)}{2}\\epsilon}\\\\ {\\displaystyle\\geq-\\frac{P^{2}}{8(P+1)}-\\frac{P}{2}\\epsilon,}\\end{array}\n$$", "text_format": "latex", "page_idx": 80}, {"type": "text", "text": "where we use $\\begin{array}{r}{k\\geq\\frac{P}{2}}\\end{array}$ for the last inequality. Therefore, we have ", "page_idx": 80}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g_{1}(z^{*},S(z^{*}))=\\displaystyle\\frac{|\\mathcal{V}_{1}|}{|\\mathcal{V}_{-1}|}\\ell^{\\prime}(P z^{*})+\\displaystyle\\frac{2}{P}\\mathbb{E}_{S\\sim\\mathcal{D}_{S}}[|S|\\ell^{\\prime}(|S|z^{*}-(P-|S|)S(z^{*}))]+\\displaystyle\\frac{P-1}{3P}}\\\\ &{\\phantom{\\sum_{\\theta_{1}}\\bigl(z^{*},S(z^{*})\\bigr)}\\ge-\\left(\\displaystyle\\frac{|\\mathcal{V}_{1}|}{|\\mathcal{V}_{-1}|}+1\\right)\\epsilon-\\displaystyle\\frac{P}{4(P+1)}+\\frac{P-1}{3P}}\\\\ &{\\phantom{\\sum_{\\theta_{1}}\\bigl(z^{*},S(z^{*})\\bigr)}>0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 80}, {"type": "text", "text": "where the last inequality is due to (38), especially the fourth term inside the maximum. ", "page_idx": 80}, {"type": "text", "text": "In both cases, we have $g_{1}(z^{*},S(z^{*}))\\;>\\;0$ By intermediate value theorem, there exist unique $z_{1}^{*},z_{-1}^{*}>0$ such that $g_{1}(z_{1}^{*},z_{-1}^{*})=g_{-1}(z_{1}^{*},z_{-1}^{*})=0$ . In addition, $\\underline{{z}}\\,\\leq\\,z_{1}^{*}\\,\\leq\\,z^{*}$ and we have $z_{1}\\,=\\,\\Theta(1)$ since $\\underline{{z}}\\;=\\;\\Omega(1)$ and $z^{*}\\,=\\,\\mathcal{O}(1)$ . By using a similar argument, we can show that $z_{-1}^{*}=\\Theta(1)$ , and we have our conclusion. \u53e3 ", "page_idx": 80}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 81}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 81}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 81}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 81}, {"type": "text", "text": "Justification: The abstract and introduction accurately reflect the paper's contributions. Guidelines: ", "page_idx": 81}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 81}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 81}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 81}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 81}, {"type": "text", "text": "Justification: We discuss the limitation on our problem setting and theoretical framework in Section6. ", "page_idx": 81}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 81}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational effciency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 81}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 81}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 81}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 81}, {"type": "text", "text": "Justification: We provide the full set of assumptions and a complete proof in Appendix. Guidelines: ", "page_idx": 82}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 82}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 82}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 82}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 82}, {"type": "text", "text": "Justification: We provide detailed descriptions of the experimental setting in Section 5 and AppendixA. ", "page_idx": 82}, {"type": "text", "text": "Guidelines: ", "page_idx": 82}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might sufice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. () If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 82}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 82}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 82}, {"type": "text", "text": "Answer: [No] ", "page_idx": 83}, {"type": "text", "text": "Justification: We do not provide open access to the data and code since our main focus is theory. ", "page_idx": 83}, {"type": "text", "text": "Guidelines: The main focus of this paper is theory. ", "page_idx": 83}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : / /nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 83}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 83}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 83}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 83}, {"type": "text", "text": "Justification: We provide detailed descriptions of the experimental setting in Section 5 and AppendixA. ", "page_idx": 83}, {"type": "text", "text": "Guidelines: ", "page_idx": 83}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 83}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 83}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 83}, {"type": "text", "text": "Answer: [No] ", "page_idx": 83}, {"type": "text", "text": "Justification: The main focus of this paper is theory. ", "page_idx": 83}, {"type": "text", "text": "Guidelines: ", "page_idx": 83}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative errorrates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 83}, {"type": "text", "text": "", "page_idx": 84}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 84}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce theexperiments? ", "page_idx": 84}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 84}, {"type": "text", "text": "Justification: We provide the type of compute workers used (NVIDIA RTX A6000) in AppendixA. ", "page_idx": 84}, {"type": "text", "text": "Guidelines: ", "page_idx": 84}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 84}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 84}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 84}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 84}, {"type": "text", "text": "Justification: The research address to the NeurIPS Code of Ethics, ensuring ethical conduct throughout the study. ", "page_idx": 84}, {"type": "text", "text": "Guidelines: ", "page_idx": 84}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 84}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 84}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 84}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 84}, {"type": "text", "text": "Justification: The paper mainly focuses on establishing a theoretical understanding of existing data augmentation techniques. Thus, there are no direct societal implications arising from the research. ", "page_idx": 84}, {"type": "text", "text": "Guidelines: ", "page_idx": 84}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed. \u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 84}, {"type": "text", "text": "\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 85}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 85}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 85}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 85}, {"type": "text", "text": "Justification: The paper does not involve the release of data or models that pose a high risk for misuse. ", "page_idx": 85}, {"type": "text", "text": "Guidelines: ", "page_idx": 85}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 85}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 85}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 85}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 85}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 85}, {"type": "text", "text": "Guidelines: ", "page_idx": 85}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 85}, {"type": "text", "text": "", "page_idx": 86}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 86}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 86}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 86}, {"type": "text", "text": "Justification: The paper does not introduce any new assets. ", "page_idx": 86}, {"type": "text", "text": "Guidelines: ", "page_idx": 86}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 86}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 86}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 86}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 86}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 86}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 86}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 86}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 86}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 86}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects Guidelines: ", "page_idx": 86}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 86}, {"type": "text", "text": "", "page_idx": 87}]