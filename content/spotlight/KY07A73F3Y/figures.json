[{"figure_path": "KY07A73F3Y/figures/figures_1_1.jpg", "caption": "Figure 1: Left: Our paper proposes Stable Control Representations, which uses pre-trained text-to-image diffusion models as a source of language-guided visual representations for downstream policy learning. Right: Stable Control Representations enable learning control policies that achieve all-round competitive performance on a wide range of embodied control tasks, including in domains that require open-vocabulary generalization. Empirical results are provided in Section 4.", "description": "This figure demonstrates the Stable Control Representations (SCR) method, which leverages pre-trained text-to-image diffusion models to generate language-guided visual representations for robotic control tasks. The left panel illustrates the architecture of SCR, showing how it integrates a CLIP language encoder, a Stable Diffusion model, and a VAE to produce visual representations.  The right panel shows a comparative performance analysis of SCR against other state-of-the-art methods across a variety of benchmarks. SCR consistently achieves competitive results, showcasing its effectiveness in complex, open-vocabulary environments.", "section": "1 Introduction"}, {"figure_path": "KY07A73F3Y/figures/figures_3_1.jpg", "caption": "Figure 2: Extraction of Stable Control Representations from Stable Diffusion. Given an image-text prompt, s = {$image, Stext}, we encode and noise the image and feed it into the U-Net together with the language prompt. We then aggregate feature maps from multiple layers within the U-Net, as described in Section 3. Shown here are features from the mid and downsampling blocks of the U-Net.", "description": "This figure illustrates the process of extracting Stable Control Representations (SCR) from the Stable Diffusion model.  An image and text prompt are input. The image is encoded using a VAE, added noise at a specified level (\u03c3t), and fed into the U-Net along with the language embedding from a CLIP language encoder. Feature maps from multiple layers (mid and downsampling blocks) of the U-Net are then aggregated to form the final SCR representation.", "section": "3 Stable Control Representations"}, {"figure_path": "KY07A73F3Y/figures/figures_4_1.jpg", "caption": "Figure 3: Sample scenes from the Habitat environments for the ImageNav (left) and OVMM (center) tasks. Instances from training and validation datasets of the OVMM object set are shown on the right.", "description": "This figure shows example scenes from the Habitat simulator used in the ImageNav and OVMM tasks. The left image displays a photorealistic rendering of an indoor environment used for ImageNav, while the center image shows a top-down view of a simplified environment for OVMM. The right side showcases example objects from the training and validation sets of OVMM, highlighting the variety of objects used in the benchmark.  The figure illustrates the different visual complexities of the tasks and the diversity of objects involved in OVMM.", "section": "3 Stable Control Representations"}, {"figure_path": "KY07A73F3Y/figures/figures_8_1.jpg", "caption": "Figure 2: Extraction of Stable Control Representations from Stable Diffusion. Given an image-text prompt, s = {\\$image, Stext}, we encode and noise the image and feed it into the U-Net together with the language prompt. We then aggregate feature maps from multiple layers within the U-Net, as described in Section 3. Shown here are features from the mid and downsampling blocks of the U-Net.", "description": "This figure illustrates the architecture for extracting Stable Control Representations (SCR) from a pre-trained Stable Diffusion model.  An image and text prompt are input. The image is encoded and noised using a VAE, and then fed into a U-Net along with the text embedding. Feature maps from multiple layers within the U-Net (specifically, mid and downsampling blocks) are concatenated and then passed through an interpolation and compression layer to create the final SCR representation.", "section": "3 Stable Control Representations"}, {"figure_path": "KY07A73F3Y/figures/figures_18_1.jpg", "caption": "Figure 5: Sample scenes from the Habitat environments for the ImageNav (left) and OVMM (center) tasks. Instances from training and validation datasets of the OVMM object set are shown on the right.", "description": "This figure shows example scenes from the ImageNav and OVMM benchmark datasets used in the paper.  The left image shows a scene from ImageNav, an indoor visual navigation task. The center image displays a scene from OVMM, an open vocabulary mobile manipulation task.  The right side of the figure shows various objects used in the OVMM object set, illustrating the diversity of objects present in the dataset's training and validation sets.", "section": "3 Stable Control Representations"}, {"figure_path": "KY07A73F3Y/figures/figures_20_1.jpg", "caption": "Figure 1: Left: Our paper proposes Stable Control Representations, which uses pre-trained text-to-image diffusion models as a source of language-guided visual representations for downstream policy learning. Right: Stable Control Representations enable learning control policies that achieve all-round competitive performance on a wide range of embodied control tasks, including in domains that require open-vocabulary generalization. Empirical results are provided in Section 4.", "description": "This figure shows the overall approach of the paper (left) and summarizes the performance of the proposed method compared to other state-of-the-art methods on various tasks (right).  The left panel depicts the architecture for extracting Stable Control Representations (SCR) from a pre-trained text-to-image diffusion model, using a language encoder and a Stable Diffusion model. The right panel shows that SCR achieves competitive results across different robotic control tasks, demonstrating its versatility and improved performance, especially in open-vocabulary settings.", "section": "1 Introduction"}, {"figure_path": "KY07A73F3Y/figures/figures_20_2.jpg", "caption": "Figure 1: Left: Our paper proposes Stable Control Representations, which uses pre-trained text-to-image diffusion models as a source of language-guided visual representations for downstream policy learning. Right: Stable Control Representations enable learning control policies that achieve all-round competitive performance on a wide range of embodied control tasks, including in domains that require open-vocabulary generalization. Empirical results are provided in Section 4.", "description": "This figure shows the overall approach of the paper and the results. The left panel illustrates how Stable Control Representations (SCR) are generated using pre-trained text-to-image diffusion models, while the right panel presents a comparison of the performance of SCR against other state-of-the-art methods across multiple embodied control tasks.  The results demonstrate that SCR achieves competitive performance on diverse tasks, including those requiring open-vocabulary generalization.", "section": "1 Introduction"}, {"figure_path": "KY07A73F3Y/figures/figures_24_1.jpg", "caption": "Figure 8: Noising and denoising plots for images from 3 of our tasks using the fine-tuned Stable Diffusion model. For each image, we first add noise up to timestep t, where t \u2208 {100, 200, 300}, and then denoise the image back to timestep 0. We observe that inputs from different tasks are differently sensitive to the noising ranges, based on the amount of information the images contain. On Meta-World, SD is able to reconstruct the image correctly even at t = 200, while for the referring expressions grounding task, noising leads to information loss even at t = 100 with several small objects reconstructed differently to the original. This affects the range of the noise we could add to the ended latents at the time of representation extraction. It should be noted however, that using un-noised inputs (t = 0) always worked well in our experiments, and this hyper-parameter could be ignored in practice.", "description": "This figure shows the effect of adding different levels of noise to images from three different tasks (Meta-World, OVMM, and Refer Expression) and then denoising them back to their original state. It demonstrates that the sensitivity to noise varies across tasks, with some tasks being more robust to noise than others. This finding has implications for selecting the appropriate noise level during representation extraction.", "section": "5 Deconstructing Stable Control Representations"}, {"figure_path": "KY07A73F3Y/figures/figures_25_1.jpg", "caption": "Figure 9: The Stable Diffusion model allows us to extract word-level cross-attention maps for any given text prompt. We visualize these maps in a robotic manipulation environment and observe that they are accurate at localizing objects in a scene. Since these maps are category agnostic, downstream policies should become robust to unseen objects at test time.", "description": "This figure shows that Stable Diffusion can extract word-level cross-attention maps that accurately localize objects in a scene.  Because these maps are category-agnostic, they make downstream policies robust to unseen objects at test time.", "section": "3 Stable Control Representations"}, {"figure_path": "KY07A73F3Y/figures/figures_25_2.jpg", "caption": "Figure 10: Images from OVMM benchmark with their corresponding attention maps obtained from the fine-tuned Stable Diffusion (SD) model. The first 5 pairs of images correspond to failed episodes, with the bottom right pair corresponding to a successful episode. The attention maps help us interpret the cause of failure: (1) Tomato - SD wrongly attends strongly to an apple. (2) Gaming Console - visible at the top of the image; however, SD attends to multiple objects due to low visual quality. (3) Plant Container - SD instead focuses on the two glasses it sees in the image. (4) Spray Bottle - SD completely misses the spray bottles in the image and attends to the lava lamp. (5) Plant Container - SD wrongly attends to the apple. (6) Candle Holder - SD correctly attends.", "description": "This figure shows images from the Open Vocabulary Mobile Manipulation (OVMM) benchmark along with their corresponding attention maps generated by the fine-tuned Stable Diffusion model. The attention maps highlight the model's focus during object localization.  Five examples demonstrate failures due to various factors like visual ambiguity and misidentification of objects, while one example demonstrates a successful localization.", "section": "3 Stable Control Representations"}]