[{"figure_path": "cOuLbPhOT1/figures/figures_4_1.jpg", "caption": "Figure 1: Thm. 1: A flatter minimum has smaller gradient and Hessian norms, yielding better generalization. Thm. 2: Large gradient norms indicate large differences among perturbations. PACE minimizes these differences, reducing gradient norms. Thm. 3: Minimizing all pairs of distances between f(\u03b8\u2080+z\u2081\u2299\u2206\u03b8) and f(\u03b8\u2080+z\u2082\u2299\u2206\u03b8) where z\u2081, z\u2082~N(1, \u03c3\u00b2I) also reduces FP-distance (between fine-tuned f(\u03b8\u2080+\u2206\u03b8) and pre-trained f(\u03b8\u2080)), especially when z\u2081=1, z\u2082=0 or vice versa.", "description": "This figure illustrates three theorems of the paper using graphical representations of loss functions. Theorem 1 shows the relationship between flat minima and small gradient and Hessian norms, implying better generalization. Theorem 2 demonstrates how PACE (the proposed method) reduces large differences between perturbed outputs, leading to smaller gradient norms. Theorem 3 shows that PACE implicitly aligns the fine-tuned model with the pre-trained model by minimizing distances between outputs perturbed with different noise, resulting in a reduced FP-distance (Fine-tuned vs Pre-trained distance).", "section": "3.2 Generalization of deep neural networks"}, {"figure_path": "cOuLbPhOT1/figures/figures_5_1.jpg", "caption": "Figure 2: Our pipeline. Adapter Ah and ho from pre-trained model form the linear layer h of Multi-Head Attention and MLP in fine-tuned model. We perturb Ah with multiplicative noise and ensure the network remains consistent to same inputs under varying perturbations.", "description": "This figure illustrates the PACE pipeline.  The pre-trained transformer block (ho) and an adapter (\u0394h) are combined to form the linear layer (h) in the fine-tuned model.  Multiplicative noise (z) is added to the adapter's output.  A consistency regularization loss is applied by comparing the model's output (f1(x)) with a second model's output (f2(x)) that uses the same weights but different noise.  This forces the model to maintain consistent output across different noise perturbations, thus improving generalization.", "section": "3.5 Efficient implementation of PACE"}, {"figure_path": "cOuLbPhOT1/figures/figures_8_1.jpg", "caption": "Figure 3: Analysis for PACE. (a) gradient norm, (b) FP-Distance and (c) train & val accuracy, are evaluated on validation set of CIFAR-100 (VTAB-1K) with baseline LoRAmul+VPTadd on ViT-B/16.", "description": "This figure presents the results of an analysis comparing the baseline LoRAmul+VPTadd model with the PACE model.  The analysis focuses on the validation set of CIFAR-100 from the VTAB-1K benchmark using the ViT-B/16 architecture.  Three key metrics are plotted: gradient norm (a), FP-distance (b), and train and validation accuracy (c).  Each metric is plotted against training epochs.  The figure demonstrates that PACE outperforms the baseline, resulting in lower gradient norms and FP-distances, which correlates to improved generalization performance as evidenced by higher validation accuracy.", "section": "4.2 Analyses"}, {"figure_path": "cOuLbPhOT1/figures/figures_8_2.jpg", "caption": "Figure 3: Analysis for PACE. (a) gradient norm, (b) FP-Distance and (c) train & val accuracy, are evaluated on validation set of CIFAR-100 (VTAB-1K) with baseline LoRAmul+VPTadd on ViT-B/16.", "description": "This figure shows the results of an experiment comparing the performance of the proposed PACE method against a baseline LoRAmul+VPTadd method. The experiment was conducted on the CIFAR-100 dataset from the VTAB-1K benchmark using a ViT-B/16 model.  The figure contains three subplots:\n\n(a) shows the gradient norms of both methods during training.\n(b) shows the FP-distance (output distance between fine-tuned and pre-trained models) for both methods.\n(c) shows the training and validation accuracy for both methods.\n\nThe results demonstrate that PACE achieves lower gradient norms and FP-distance, leading to better generalization performance as indicated by higher validation accuracy, compared to the baseline.", "section": "4.2 Analyses"}, {"figure_path": "cOuLbPhOT1/figures/figures_8_3.jpg", "caption": "Figure 5: Gradient norms of models across wide range of regularization strengths \u03bb on CIFAR-100 (VTAB-1K) w/ ViT-B/16. Line and shadow represent mean and std across training epochs.", "description": "The figure shows the gradient norms of different models trained with various regularization strengths (\u03bb) on the CIFAR-100 dataset using the ViT-B/16 model.  It demonstrates how the proposed PACE method effectively controls gradient norms across a wide range of \u03bb values, unlike the naive alignment approach (FPA) which exhibits unpredictable behavior and may even lead to gradient explosion. The plot highlights PACE's robust gradient regularization capability, essential for improved generalization.", "section": "4.2 Analyses"}, {"figure_path": "cOuLbPhOT1/figures/figures_20_1.jpg", "caption": "Figure 3: Analysis for PACE. (a) gradient norm, (b) FP-Distance and (c) train & val accuracy, are evaluated on validation set of CIFAR-100 (VTAB-1K) with baseline LoRAmul+VPTadd on ViT-B/16.", "description": "This figure shows the experimental results of applying PACE on the CIFAR-100 dataset of the VTAB-1K benchmark using the ViT-B/16 model. Three subplots are presented. Subplot (a) displays the gradient norm over training epochs for both the baseline LoRAmul+VPTadd and PACE methods. Subplot (b) shows the FP-distance (output distance between fine-tuned and pre-trained models) over epochs.  Subplot (c) illustrates the training and validation accuracy for both methods over epochs.  The results demonstrate that PACE reduces the gradient norm and maintains a lower FP-distance than the baseline while achieving higher validation accuracy.", "section": "4.2 Analyses"}, {"figure_path": "cOuLbPhOT1/figures/figures_20_2.jpg", "caption": "Figure 5: Gradient norms of models across wide range of regularization strengths \u03bb on CIFAR-100 (VTAB-1K) w/ ViT-B/16. Line and shadow represent mean and std across training epochs.", "description": "This figure shows the gradient norms of different models trained with varying regularization strengths (\u03bb) on the CIFAR-100 dataset using the ViT-B/16 architecture.  The baseline model is compared against models using Fine-tuned Pre-trained model Alignment (FPA) and PACE. The plot demonstrates how PACE consistently reduces gradient norms across a wide range of \u03bb values, while FPA shows unpredictable behavior and even gradient explosion in certain regions.  The shaded areas represent the standard deviations of the gradient norms across different training epochs. The results visually support the theoretical findings of the paper, highlighting the effectiveness of PACE in gradient regularization.", "section": "4.2 Analyses"}]