[{"figure_path": "E7fZOoiEKl/figures/figures_2_1.jpg", "caption": "Figure 1: Structure Equation Model [109] of FL.", "description": "This figure is a causal graph that illustrates the data generation process in federated learning (FL).  It highlights how spurious features (R<sup>spu</sup>) and invariant features (R<sup>inv</sup>) contribute to the data heterogeneity across different clients. Panel (a) shows the isolated training scenario in typical ensemble FL or one-shot FL, where models are trained independently and easily fit to spurious correlations, leading to poor performance. Panel (b) shows the federated fusion approach, where augmenting intermediate features from other clients helps to mitigate the spurious correlations and improve generalization.", "section": "Federated Learning: A Causal View"}, {"figure_path": "E7fZOoiEKl/figures/figures_4_1.jpg", "caption": "Figure 2: Estimated MI and separability of trained models with non-IID datasets.", "description": "This figure presents the empirical estimation of mutual information (MI) between features and inputs (I(Hk; X)) and between features and labels (I(Hk; Y)) at different layers (modules) of a model trained on non-IID data.  It shows that locally trained models tend to fit more on spurious correlations (higher I(Hk; X), lower I(Hk; Y)), while FuseFL's progressive fusion helps to learn more invariant features (lower I(Hk; X), higher I(Hk; Y)). The separability of features at each layer is also compared, with FuseFL showing improved separability, indicating better generalization ability.", "section": "3 Federated Learning: A Causal View"}, {"figure_path": "E7fZOoiEKl/figures/figures_4_2.jpg", "caption": "Figure 2: Estimated MI and separability of trained models with non-IID datasets.", "description": "This figure shows the estimated mutual information (MI) between features and input (I(Hk; X)), features and labels (I(Hk; Y)), and the linear separability of layers in a model trained on non-IID datasets.  The different lines represent different non-IID degrees (\u03b1 = 0.1, 0.3, 0.5) and whether features are obtained from isolated local training or from FuseFL (feature fusion).  The results indicate that FuseFL's feature fusion method helps to improve the MI between features and labels and the separability of layers, reducing overfitting to spurious correlations.", "section": "3 Federated Learning: A Causal View"}, {"figure_path": "E7fZOoiEKl/figures/figures_5_1.jpg", "caption": "Figure 4: (a) Initially, all layers are isolated training. Note that the layer here does not only mean one or Conv layer, but generally refers to a neural network block that can consist of multiple layers. (b) Then, all first blocks (L1) of different clients are communicated, shared and frozen among clients. Then, the adaptors are added behind the fused block, to fuse features outputted from the concatenated local blocks. (c) Train the third blocks (L3) follow the similar process in (b). (d) inference process of FuseFL. The larger squares represent the original training block in local models. The smaller squares are adaptors that fuse features from previous modules together, which are 1 \u00d7 1 Conv kernels or simple average operations with little or no memory costs. Note that (a) also represents local training in ensemble FL, where different clients train models on local datasets.", "description": "This figure illustrates the FuseFL training process. (a) shows the isolated training of each client's model. (b), (c) show the progressive fusion of blocks from different clients, with adaptors used to integrate the fused features. (d) shows the final inference process.", "section": "4 FuseFL: Progressive FL Model Fusion"}, {"figure_path": "E7fZOoiEKl/figures/figures_26_1.jpg", "caption": "Figure 5: Each row is a class of original (upper) and backdoored (lower) images of CIFAR-10. The shapes are added on images according to label indexes.", "description": "This figure shows example images from the CIFAR-10 dataset that have been modified to include backdoor triggers.  The top row shows the original images, while the bottom row shows the same images with added shapes (squares, circles, triangles, etc.) overlaid on them. The shapes are color-coded according to the image's label, creating spurious correlations that a model might learn during training if it is not robust to such adversarial examples. This is used to test the models' resilience to backdoors in the experiments.", "section": "D Details of Experiment Configuration"}]