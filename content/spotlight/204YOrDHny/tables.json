[{"figure_path": "204YOrDHny/tables/tables_7_1.jpg", "caption": "Table 1: In-distribution performance across methods trained on MNIST, FMNIST and CIFAR-10.", "description": "This table presents the in-distribution performance of different Bayesian methods trained on three different datasets: MNIST, FMNIST, and CIFAR-10.  The performance is measured using several metrics: Confidence (Conf.), Negative Log-Likelihood (NLL), Accuracy (Acc.), Brier Score (Brier), Expected Calibration Error (ECE), and Maximum Calibration Error (MCE).  The table allows for a comparison of the effectiveness of different approaches (Laplace Diffusion, Sampled Laplace, and Linearized Laplace) in terms of their ability to accurately estimate the uncertainty and produce well-calibrated predictions.", "section": "7 Experiments"}, {"figure_path": "204YOrDHny/tables/tables_7_2.jpg", "caption": "Table 2: Out-of-distribution AUROC (\u2191) performance for MNIST, FMNIST and CIFAR-10.", "description": "This table presents the out-of-distribution Area Under the Receiver Operating Characteristic curve (AUROC) for several Bayesian deep learning methods.  The models were trained on MNIST, FMNIST, and CIFAR-10 datasets and tested on a variety of out-of-distribution datasets to evaluate their ability to generalize to unseen data.  The AUROC scores indicate the performance of each method in distinguishing between in-distribution and out-of-distribution samples.", "section": "7 Experiments"}, {"figure_path": "204YOrDHny/tables/tables_22_1.jpg", "caption": "Table 1: In-distribution performance across methods trained on MNIST, FMNIST and CIFAR-10.", "description": "This table presents the in-distribution performance of different Bayesian methods on three benchmark datasets: MNIST, Fashion-MNIST, and CIFAR-10.  The performance is evaluated using several metrics: Confidence, Accuracy, Negative Log-Likelihood (NLL), Brier Score, Expected Calibration Error (ECE), and Maximum Calibration Error (MCE).  The results show how different methods compare in terms of calibration, accuracy, and predictive uncertainty within the training distribution.", "section": "7 Experiments"}, {"figure_path": "204YOrDHny/tables/tables_22_2.jpg", "caption": "Table 1: In-distribution performance across methods trained on MNIST, FMNIST and CIFAR-10.", "description": "This table presents a comparison of different Bayesian methods (Laplace diffusion, Sampled Laplace, Linearised Laplace, SWAG, Last-Layer Laplace, Diagonal Laplace, and MAP) in terms of their in-distribution performance.  The metrics used to evaluate performance include Confidence, Negative Log-Likelihood (NLL), Accuracy (Acc), Brier Score (Brier), Expected Calibration Error (ECE), and Maximum Calibration Error (MCE). The results are shown for three different datasets: MNIST, FMNIST, and CIFAR-10, allowing for a comparison of method performance across various datasets.", "section": "7 Experiments"}, {"figure_path": "204YOrDHny/tables/tables_22_3.jpg", "caption": "Table 1: In-distribution performance across methods trained on MNIST, FMNIST and CIFAR-10.", "description": "This table presents the in-distribution performance of different Bayesian methods trained on three datasets: MNIST, FMNIST, and CIFAR-10.  The performance is evaluated using several metrics: Confidence (Conf.), Negative Log-Likelihood (NLL), Accuracy (Acc.), Brier score (Brier), Expected Calibration Error (ECE), and Maximum Calibration Error (MCE).  Lower values are better for NLL, Brier, ECE, and MCE, while higher values are better for Conf. and Acc.  The table allows for a comparison of the performance of Laplace Diffusion (the proposed method) against other approaches like Sampled Laplace, Linearised Laplace, etc.", "section": "7 Experiments"}, {"figure_path": "204YOrDHny/tables/tables_23_1.jpg", "caption": "Table 2: Out-of-distribution AUROC (\u2191) performance for MNIST, FMNIST and CIFAR-10.", "description": "This table presents the out-of-distribution Area Under the Receiver Operating Characteristic curve (AUROC) for different Bayesian methods tested on MNIST, FMNIST, and CIFAR-10 datasets.  AUROC is a metric that measures the ability of a classifier to distinguish between in-distribution and out-of-distribution data. Higher AUROC values indicate better performance. The table shows that the proposed Laplace diffusion method generally outperforms other methods in this out-of-distribution setting.", "section": "Experiments"}, {"figure_path": "204YOrDHny/tables/tables_24_1.jpg", "caption": "Table 2: Out-of-distribution AUROC (\u2191) performance for MNIST, FMNIST and CIFAR-10.", "description": "This table presents the out-of-distribution Area Under the Receiver Operating Characteristic curve (AUROC) performance for three different datasets: MNIST, Fashion-MNIST (FMNIST), and CIFAR-10.  The AUROC metric is used to evaluate the performance of different methods in distinguishing between in-distribution and out-of-distribution samples.  Higher AUROC values are indicative of better performance.  The table compares the Laplace diffusion method with other methods, such as sampled Laplace and linearized Laplace.", "section": "Experiments"}, {"figure_path": "204YOrDHny/tables/tables_24_2.jpg", "caption": "Table 2: Out-of-distribution AUROC (\u2191) performance for MNIST, FMNIST and CIFAR-10.", "description": "This table presents the results of out-of-distribution AUROC performance using different methods.  The AUROC (Area Under the Receiver Operating Characteristic curve) is a metric to evaluate the performance of binary classification.  Higher values indicate better performance. The table compares the Laplace diffusion method with sampled Laplace, linearized Laplace, SWAG, last-layer Laplace, diagonal Laplace, and MAP (Maximum a Posteriori) across various out-of-distribution datasets (MNIST, FMNIST, EMNIST, KMNIST, CIFAR-10, CIFAR-100, SVHN).  This allows for comparison of the different methods' ability to generalize beyond the training distribution.", "section": "Experiments"}]