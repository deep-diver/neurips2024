{"references": [{"fullname_first_author": "Edward J. Hu", "paper_title": "LoRA: Low-rank adaptation of large language models", "publication_date": "2022-00-00", "reason": "This paper introduces LoRA, a highly efficient parameter-efficient fine-tuning method that is a key comparison point for the ReFT method proposed in this paper."}, {"fullname_first_author": "Atticus Geiger", "paper_title": "Finding alignments between interpretable causal variables and distributed neural representations", "publication_date": "2023-03-00", "reason": "This paper introduces the theoretical framework of causal abstraction and distributed interchange interventions which motivates and underpins the ReFT methods."}, {"fullname_first_author": "Zhengxuan Wu", "paper_title": "Interpretability at scale: Identifying causal mechanisms in Alpaca", "publication_date": "2023-00-00", "reason": "This paper demonstrates the efficacy of subspace interventions in improving the interpretability of large language models, providing empirical support for the ReFT approach."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "LLaMA: Open and efficient foundation language models", "publication_date": "2023-02-13", "reason": "This paper introduces the LLaMA family of language models, which are the primary models used in the experimental evaluation of ReFT."}, {"fullname_first_author": "Zhengxuan Wu", "paper_title": "Enhancing chat language models by scaling high-quality instructional conversations", "publication_date": "2023-12-00", "reason": "This paper provides a strong benchmark for instruction-following tasks, allowing for a robust evaluation of the ReFT approach in this important domain."}]}