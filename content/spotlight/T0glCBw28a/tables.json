[{"figure_path": "T0glCBw28a/tables/tables_5_1.jpg", "caption": "Table 1: Testing performance of the distilled model is reported for each combination of method and dataset. The estimated cost is obtained by calculating the number of input and output tokens associated with GPT-3.5's pricing table [49]. Other models may be even more expensive.", "description": "This table presents the results of using a distilled model trained on labels generated by Alchemist and compares its performance with labels generated using zero-shot prompting.  It shows the accuracy and estimated cost (based on GPT-3.5 pricing) for various datasets across different tasks, highlighting the cost-effectiveness of the Alchemist approach.  The cost reflects the number of tokens processed by the language model.", "section": "4.1 Cost Reduction and Improved Performance"}, {"figure_path": "T0glCBw28a/tables/tables_6_1.jpg", "caption": "Table 2: Alchemist on non-text modalities. We experiment with standard Alchemist (top), our proposed extension with two CLIP-based local models as feature extractors, and CLIP prompting baselines. Alchemist achieves comparable performance on average accuracy while improving robustness to spurious correlations.", "description": "This table presents the results of experiments conducted to evaluate Alchemist's performance on non-text modalities (images).  Three different feature extractors were used:  none, CLIP VIT-B/32, and CLIP VIT-L/14.  For each feature extractor, three methods were compared: standard Alchemist, a group prompting approach, and CLIP prompting baselines. The table shows the average accuracy, worst group accuracy, and the difference between them (gap). The results demonstrate that Alchemist's extension, utilizing local multimodal models as feature extractors, improves robustness to spurious correlations, achieving comparable average accuracy with better worst-group accuracy.", "section": "4.2 Extending Alchemist to Other Modalities"}, {"figure_path": "T0glCBw28a/tables/tables_7_1.jpg", "caption": "Table 3: Testing performance of the label model is reported for each combination of prompting strategy and dataset. We observe that GPT-4 and Claude 3 (that may possess better comprehension capabilities) exhibit greater enhancements when provided with supplementary information.", "description": "This table presents the results of an experiment evaluating the performance of a label model under different prompting strategies and datasets.  The prompting strategies involved incorporating various types of supplementary information (dataset description, data exemplars, keywords, and labeling rules) to enhance the language model's ability to generate effective programs for labeling data. The table shows the performance (F1-score or Accuracy) achieved by different language models (GPT-3.5, GPT-4, Claude 3) under each prompting strategy and for each dataset. The results suggest that GPT-4 and Claude 3, potentially due to their superior comprehension capabilities, benefit most from the inclusion of supplementary information.", "section": "4.3 Use of Supplementary Information"}, {"figure_path": "T0glCBw28a/tables/tables_8_1.jpg", "caption": "Table 4: Analysis showing that Alchemist can achieve comparable or better accuracy and higher coverage while using fewer programs to label the data.", "description": "This table compares the performance of Alchemist's synthesized programs against human-crafted programs in terms of accuracy and coverage for four different datasets (YouTube, SMS, Yelp, and IMDb).  It highlights that Alchemist, using a significantly smaller number of generated programs (10 compared to 73 for human-crafted in the SMS dataset), achieves comparable or better results.  The 'Coverage' metric indicates the proportion of the dataset that can be labeled, while 'Performance' likely represents accuracy (F1-score or similar). The table demonstrates Alchemist's cost-effectiveness and efficiency in data annotation.", "section": "4.5 Comparing to Human-crafted Programs"}, {"figure_path": "T0glCBw28a/tables/tables_13_1.jpg", "caption": "Table 1: Testing performance of the distilled model is reported for each combination of method and dataset. The estimated cost is obtained by calculating the number of input and output tokens associated with GPT-3.5's pricing table [49]. Other models may be even more expensive.", "description": "This table presents the accuracy and estimated cost of using both zero-shot prompting and Alchemist with GPT-3.5 for different datasets.  The cost reflects the number of tokens processed, highlighting Alchemist's significant cost reduction.  The accuracy demonstrates Alchemist's comparable or superior performance to zero-shot prompting.", "section": "4.1 Cost Reduction and Improved Performance"}, {"figure_path": "T0glCBw28a/tables/tables_13_2.jpg", "caption": "Table 1: Testing performance of the distilled model is reported for each combination of method and dataset. The estimated cost is obtained by calculating the number of input and output tokens associated with GPT-3.5's pricing table [49]. Other models may be even more expensive.", "description": "This table presents the results of the experiments conducted to evaluate the performance of the distilled model on various datasets.  For each dataset and method (zero-shot prompting and Alchemist with GPT-3.5), the estimated cost and accuracy (or F1-score) are shown. The cost is estimated based on the number of tokens processed by GPT-3.5, highlighting the cost savings achieved by the Alchemist method.", "section": "4.1 Cost Reduction and Improved Performance"}, {"figure_path": "T0glCBw28a/tables/tables_13_3.jpg", "caption": "Table 1: Testing performance of the distilled model is reported for each combination of method and dataset. The estimated cost is obtained by calculating the number of input and output tokens associated with GPT-3.5's pricing table [49]. Other models may be even more expensive.", "description": "This table presents the results of the experiments comparing the performance of the distilled model (using Alchemist) against the zero-shot prompting baseline across eight different datasets.  It shows the estimated cost (based on GPT-3.5 pricing) and accuracy (or F1-score for SMS) achieved by each method on each dataset. The table highlights the significant cost reduction achieved by Alchemist while maintaining comparable or even improved performance compared to the baseline.", "section": "4.1 Cost Reduction and Improved Performance"}, {"figure_path": "T0glCBw28a/tables/tables_14_1.jpg", "caption": "Table 1: Testing performance of the distilled model is reported for each combination of method and dataset. The estimated cost is obtained by calculating the number of input and output tokens associated with GPT-3.5's pricing table [49]. Other models may be even more expensive.", "description": "This table presents the results of the distilled model's performance on eight different datasets using two different methods: zero-shot prompting and Alchemist with GPT-3.5.  The accuracy, F1-score (for relevant datasets), and estimated cost are shown for each combination.  The cost estimates are based on the GPT-3.5 pricing, indicating that Alchemist offers significant cost savings compared to zero-shot prompting, especially when using more expensive models.", "section": "4.1 Cost Reduction and Improved Performance"}, {"figure_path": "T0glCBw28a/tables/tables_15_1.jpg", "caption": "Table 1: Testing performance of the distilled model is reported for each combination of method and dataset. The estimated cost is obtained by calculating the number of input and output tokens associated with GPT-3.5's pricing table [49]. Other models may be even more expensive.", "description": "This table presents the results of the distilled model's performance on eight different datasets using two methods: zero-shot prompting and Alchemist with GPT-3.5.  For each dataset and method, the estimated cost and accuracy are shown.  The estimated cost is calculated based on the number of tokens used in GPT-3.5 API calls. The results highlight Alchemist's cost-effectiveness and improved accuracy in several datasets compared to the zero-shot prompting approach.", "section": "4.1 Cost Reduction and Improved Performance"}, {"figure_path": "T0glCBw28a/tables/tables_15_2.jpg", "caption": "Table 1: Testing performance of the distilled model is reported for each combination of method and dataset. The estimated cost is obtained by calculating the number of input and output tokens associated with GPT-3.5's pricing table [49]. Other models may be even more expensive.", "description": "This table presents the results of the experiments comparing the performance of the distilled model using Alchemist with the baseline method of zero-shot prompting for eight different datasets across four language tasks.  For each dataset and method, the estimated cost and accuracy (or F1-score) are shown.  The cost is calculated based on the number of tokens processed by GPT-3.5, highlighting the significant cost reduction achieved by Alchemist.", "section": "4.1 Cost Reduction and Improved Performance"}, {"figure_path": "T0glCBw28a/tables/tables_16_1.jpg", "caption": "Table 1: Testing performance of the distilled model is reported for each combination of method and dataset. The estimated cost is obtained by calculating the number of input and output tokens associated with GPT-3.5's pricing table [49]. Other models may be even more expensive.", "description": "This table shows the accuracy and estimated cost of using Alchemist with GPT-3.5 compared to zero-shot prompting for various text classification tasks.  The cost reflects the API calls needed; Alchemist significantly reduces the cost by generating programs to label data instead of directly querying models for each data point.  Accuracies are reported for a distilled model trained on pseudolabels created using the generated programs.", "section": "4.1 Cost Reduction and Improved Performance"}, {"figure_path": "T0glCBw28a/tables/tables_16_2.jpg", "caption": "Table 2: Alchemist on non-text modalities. We experiment with standard Alchemist (top), our proposed extension with two CLIP-based local models as feature extractors, and CLIP prompting baselines. Alchemist achieves comparable performance on average accuracy while improving robustness to spurious correlations.", "description": "This table presents the results of experiments evaluating Alchemist's performance on non-text modalities, specifically using image data. Three different methods are compared: the standard Alchemist approach, an extension of Alchemist incorporating CLIP-based local feature extractors, and CLIP prompting baselines.  The table shows the average accuracy, worst-group accuracy (accuracy on the group with the lowest accuracy), and the gap between these two metrics for each method. The results demonstrate that while Alchemist's average accuracy is comparable to the baselines, its extension significantly improves robustness by reducing the gap between average and worst-group accuracy, indicating better handling of potentially problematic data.", "section": "4.2 Extending Alchemist to Other Modalities"}]