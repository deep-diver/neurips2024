[{"figure_path": "fu0xdh4aEJ/tables/tables_6_1.jpg", "caption": "Table 1: Comparison of BroNet, Spectral (Bjorck et al., 2021), and Vanilla MLP architectures in notriously hard Dog environments. All metrics except return are averaged over time steps. All architectures are combined with BRO.", "description": "This table compares the performance of three different critic network architectures (BroNet, Spectral, and Vanilla MLP) when combined with the BRO algorithm on challenging Dog environments.  It shows that BroNet significantly outperforms the others in terms of final return, gradient norm, mean Q-values, and TD-error, indicating its superior stability and performance in these complex tasks.", "section": "4 Related Work"}, {"figure_path": "fu0xdh4aEJ/tables/tables_17_1.jpg", "caption": "Table 2: Approaches examined during BRO development. Methods incorporated into BRO are highlighted in bold.", "description": "This table summarizes various techniques explored during the development of the BRO algorithm.  It categorizes them into groups (Exploration, Value Regularization, Network Regularization, Scheduling, Distributional RL, Plasticity Regularization, Learning) and lists specific methods used within each group, along with their source.  The methods highlighted in bold are the ones that were ultimately incorporated into the final BRO algorithm.", "section": "B BRO additional details"}, {"figure_path": "fu0xdh4aEJ/tables/tables_17_2.jpg", "caption": "Table 3: List of tasks from DeepMind Control and MetaWorld on which the agents were ablated. The table also contains the dimensions of action and observation space.", "description": "This table lists the 40 tasks used in the paper's experiments, broken down by benchmark suite (DeepMind Control, MetaWorld, MyoSuite). For each task, the number of dimensions in the observation and action spaces are provided.  This information is important for understanding the complexity of the tasks and the computational resources required to solve them.", "section": "2.1 Experimental setup"}, {"figure_path": "fu0xdh4aEJ/tables/tables_18_1.jpg", "caption": "Table 4: List of tasks from DeepMind Control, MetaWorld, and MyoSuite on which the agents were tested. The table also contains the dimensions of action and observation space.", "description": "This table lists the 40 continuous control tasks used in the paper's experiments.  These tasks are drawn from three benchmark suites: DeepMind Control, MetaWorld, and MyoSuite. For each task, the table provides the dimensionality of the observation space (number of state variables) and the dimensionality of the action space (number of control signals).  This information is crucial for understanding the complexity of each task and for comparing the performance of different reinforcement learning algorithms.", "section": "2 Bigger, Regularized, Optimistic (BRO) algorithm"}, {"figure_path": "fu0xdh4aEJ/tables/tables_19_1.jpg", "caption": "Table 5: Hyperparameter values for actor-critic agents used in the experiments.", "description": "This table lists the hyperparameter settings used for the BRO algorithm and its comparison algorithms (SAC, TD3, SR-SAC, and CrossQ).  It details the batch size, replay ratio, critic network architecture and dimensions, actor network dimensions, number of quantiles for quantile regression, KL target for the KL divergence penalty, initial optimism value for optimistic exploration, standard deviation multiplier, learning rates for both actor and critic networks, temperature learning rate, optimizer used, discount factor, initial temperature, number of exploratory steps, target entropy, and Polyak averaging weight.  These settings are crucial for replicating the experimental results reported in the paper.", "section": "D Hyperparameters"}, {"figure_path": "fu0xdh4aEJ/tables/tables_19_2.jpg", "caption": "Table 6: Links to the repositories of the used baselines. All are distributed under MIT License.", "description": "This table provides links to the source code repositories for the various baseline algorithms used in the paper's experiments.  This allows for reproducibility and verification of results. Each algorithm is listed alongside the link to its corresponding repository. All the repositories mentioned are under MIT License.", "section": "Related Work"}, {"figure_path": "fu0xdh4aEJ/tables/tables_20_1.jpg", "caption": "Table 7: Description of the considered model sizes.", "description": "This table describes the different model sizes used in the experiments. Each size is defined by the number of BroNet blocks and the hidden size of each block.  The sizes range from 0.55 million parameters to 26.31 million parameters.", "section": "E Additional Experiments"}, {"figure_path": "fu0xdh4aEJ/tables/tables_23_1.jpg", "caption": "Table 8: We replace the Q-network in SR-SPR with BroNet, while keeping the standard convolutional encoder. We test two values of reset interval (RI) and shrink-and-perturb (SP) and find that these hyperparameters impact the performance of the BroNet agent. 5 seeds.", "description": "This table presents the results of an experiment where the Q-network of the SR-SPR algorithm (a sample-efficient SAC implementation) was replaced with the BroNet architecture.  The experiment tested two different reset interval (RI) values and shrink-and-perturb (SP) values to investigate their impact on performance. The results are presented for three Atari games: Pong, Seaquest, and Breakout.", "section": "E Additional Experiments"}, {"figure_path": "fu0xdh4aEJ/tables/tables_25_1.jpg", "caption": "Table 9: Summary of IQM results of BRO and other agents evaluated on 40 tasks from DeepMind Control Suite, Metaworld and MyoSuite achieved at 100k, 200k, 500k and 1M steps. BRO achieves better results than other state-of-the-art agents (both model-based and model-free) while featuring great sample efficiency.", "description": "This table presents a summary of the Interquartile Mean (IQM) performance results for BRO and several other baseline algorithms across 40 continuous control tasks from three benchmark suites (DeepMind Control, MetaWorld, and MyoSuite).  The results are shown at four different stages of training: 100k, 200k, 500k, and 1M environment steps.  The table is organized to show the performance across the aggregated tasks as well as broken down by benchmark suite, providing a comprehensive view of the BRO algorithm's performance in various scenarios and in comparison to other leading methods.", "section": "3 Analysis"}]