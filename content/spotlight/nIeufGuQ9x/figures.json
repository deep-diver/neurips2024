[{"figure_path": "nIeufGuQ9x/figures/figures_1_1.jpg", "caption": "Figure 1: Diffusion process. In the forward process, we start from a ground truth scene flow vector field Vo and gradually add noise to it until we reach VT, which is completely Gaussian noise. In the reverse process, we recover the scene flow vector field Vo from the randomly sampled noisy vector field VT conditioned on the source point cloud Psource and the target point cloud Ptarget.", "description": "This figure illustrates the diffusion process used in the DiffSF model. The forward diffusion process starts with the ground truth scene flow (Vo) and iteratively adds Gaussian noise until a completely noisy state (VT) is reached.  The reverse process, used during training and inference, learns to reconstruct the original scene flow (Vo) from the noisy state (VT), conditioned on the source and target point clouds. This process allows the model to handle noisy data and estimate uncertainty.", "section": "3.2 Scene Flow Estimation as Diffusion Process"}, {"figure_path": "nIeufGuQ9x/figures/figures_4_1.jpg", "caption": "Figure 2: The reverse process with detailed denoising block for scene flow estimation. The denoising block takes the current noisy input Vt, the source point cloud Psource, and the target point cloud Ptarget as input. The output Vpred is the denoised scene flow prediction. Shared weights for the feature extraction are indicated in the same color.", "description": "This figure shows the detailed architecture of the denoising block used in the reverse diffusion process for scene flow estimation.  It illustrates how the noisy input (Vt), source point cloud (Psource), and target point cloud (Ptarget) are processed to produce the final scene flow prediction (Vpred). The process involves feature extraction, global correlation, and local-global-cross transformers to refine the prediction iteratively.  The use of shared weights for feature extraction is also highlighted.", "section": "3.3 Architecture"}, {"figure_path": "nIeufGuQ9x/figures/figures_7_1.jpg", "caption": "Figure 3: Analysis of uncertainty estimation on F3D\u3002 dataset. Left: Uncertainty-error correspondence. The horizontal axis is an interval of EPE. The vertical axis is the estimated uncertainty averaged over all the points that fall in the interval and the indication of the scaled uncertainty standard deviation. Right: Recall (red) and precision curve (blue) of outliers prediction. The horizontal axis is the threshold of the estimated uncertainty to determine the outliers.", "description": "This figure analyzes the uncertainty estimation of the proposed method. The left panel shows the correlation between the endpoint error (EPE) and the estimated uncertainty, demonstrating an almost linear relationship, indicating that higher uncertainty values correspond to higher errors. The right panel presents a precision-recall curve for outlier prediction (EPE > 0.30 meters), showing the effectiveness of uncertainty as a measure for identifying outliers.", "section": "4.5 Uncertainty-error Correspondence"}, {"figure_path": "nIeufGuQ9x/figures/figures_8_1.jpg", "caption": "Figure 4: Visualization of outlier prediction on F3D, dataset. Black: Accurate prediction. Red: Outliers. Top row: Outliers defined as EPE > 0.30. Bottom row: Outliers predicted by Uncertainty.", "description": "This figure visualizes the outlier prediction results on the F3D dataset. The top row shows the actual outliers, identified as having an Endpoint Error (EPE) greater than 0.30. The bottom row displays the outliers predicted by the model's uncertainty estimation.  Black points represent accurate predictions, while red points indicate outliers. The comparison between the two rows demonstrates the model's capability to identify inaccurate predictions based on its estimated uncertainty.", "section": "4.5 Uncertainty-error Correspondence"}, {"figure_path": "nIeufGuQ9x/figures/figures_13_1.jpg", "caption": "Figure 5: The detailed architecture of the feature extraction backbone DGCNN. The upper figure shows the overall architecture of DGCNN. The bottom figure shows the detailed architecture of each layer.", "description": "This figure shows the detailed architecture of the feature extraction backbone DGCNN used in the DiffSF model. The upper part illustrates the overall structure, showing how four layers of processing (Layer 1 to Layer 4) are connected sequentially. Each layer increases the feature dimension from the input point cloud, eventually culminating in an output feature with a dimension of 128. The lower part shows the inner workings of a single layer, detailing how the k-nearest neighbors of each point are identified, feature concatenation, multi-layer perceptron (MLP) application, and max pooling are performed to achieve the final features.", "section": "3.3 Architecture"}, {"figure_path": "nIeufGuQ9x/figures/figures_13_2.jpg", "caption": "Figure 6: The detailed architecture of the local transformer. The left figure shows the overall architecture of the local transformer. The right figure shows the detailed architecture for the positional embedding.", "description": "This figure shows the detailed architecture of the local transformer used in the DiffSF model for scene flow estimation.  The left side illustrates the overall process, showing how the input features are processed through linear layers to generate queries, keys, and values for an attention mechanism. The keys and values are derived from k-nearest-neighbors of the input points. Positional embeddings are incorporated to improve the model's ability to handle spatial relationships.  The right side provides a detailed breakdown of how these positional embeddings are calculated, using a multi-layer perceptron (MLP) on the input coordinates and their k-nearest neighbors.", "section": "3.3 Architecture"}, {"figure_path": "nIeufGuQ9x/figures/figures_13_3.jpg", "caption": "Figure 2: The reverse process with detailed denoising block for scene flow estimation. The denoising block takes the current noisy input Vt, the source point cloud Psource, and the target point cloud Ptarget as input. The output Vpred is the denoised scene flow prediction. Shared weights for the feature extraction are indicated in the same color.", "description": "This figure illustrates the architecture of the denoising block used in the reverse diffusion process of DiffSF. The denoising block takes as input the noisy scene flow vector (Vt), the source point cloud (Psource), and the target point cloud (Ptarget). It processes these inputs through several components: Feature Extraction (to extract higher-dimensional features), Local-Global-Cross Transformer (to capture local, global, and cross-point relationships), and Global Correlation (to generate an initial scene flow estimation). Finally, a Denoising Block refines the initial estimation and outputs the denoised scene flow prediction (Vpred).  The color-coding highlights the shared weights used within the Feature Extraction blocks. ", "section": "3.3 Architecture"}, {"figure_path": "nIeufGuQ9x/figures/figures_14_1.jpg", "caption": "Figure 8: Visualization of the reverse diffusion process on the KITTI dataset. The orange points denote the source point cloud warped by the prediction of the current timestep. The green points denote the target point cloud.", "description": "This figure visualizes the denoising process of the diffusion model on the KITTI dataset.  It shows four different steps in the reverse diffusion process, where the model iteratively refines its prediction of the scene flow starting from pure noise.  Orange points represent the source point cloud warped according to the current prediction. Green points represent the target point cloud. The progression shows how the model gradually recovers the accurate scene flow from noise, indicating the denoising capability of the diffusion model.", "section": "B Additional visualizations"}, {"figure_path": "nIeufGuQ9x/figures/figures_14_2.jpg", "caption": "Figure 9: Visualization comparison of GMSF and DiffSF on the FlyingThings3D dataset. The blue points represent the target point cloud. The green points represent the warped source points with an EPE3D smaller than a certain threshold. The orange points represent the warped source points with an EPE3D larger than a certain threshold.", "description": "This figure compares the performance of GMSF and DiffSF on the FlyingThings3D dataset by visualizing the warped source point clouds.  Points are colored based on their endpoint error (EPE3D): green for low error and orange for high error.  Blue points represent the target point cloud.", "section": "Additional visualizations"}, {"figure_path": "nIeufGuQ9x/figures/figures_14_3.jpg", "caption": "Figure 9: Visualization comparison of GMSF and DiffSF on the FlyingThings3D dataset. The blue points represent the target point cloud. The green points represent the warped source points with an EPE3D smaller than a certain threshold. The orange points represent the warped source points with an EPE3D larger than a certain threshold.", "description": "This figure compares the performance of GMSF and DiffSF on the FlyingThings3D dataset by visualizing the warped source points. Blue points represent the target point cloud, green points represent accurately warped source points (low EPE3D), and orange points represent inaccurately warped points (high EPE3D).  It highlights the improved accuracy of DiffSF in handling challenging cases.", "section": "Additional visualizations"}]