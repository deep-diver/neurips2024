[{"figure_path": "QpKWFLtZKi/figures/figures_1_1.jpg", "caption": "Figure 1: Trajectories of policies trained with different exploration algorithms in the real-life indoor environment. (a) episodic count-based method under Lp norm (b) bisimulation metric-based method (c) our method.", "description": "This figure compares the trajectories of reinforcement learning agents trained with three different exploration methods in a real-world indoor environment.  (a) shows the trajectory of an agent using an episodic count-based method with Lp norm for evaluating state discrepancy. (b) shows the trajectory of an agent using a bisimulation metric-based method. (c) shows the trajectory of an agent using the proposed Effective Metric-based Exploration-bonus (EME) method. The figure highlights that EME allows the agent to explore a significantly larger area compared to the other two methods.", "section": "1 Introduction"}, {"figure_path": "QpKWFLtZKi/figures/figures_6_1.jpg", "caption": "Figure 2: Results for various hard exploration tasks from Robosuite. The x-axis represents the number of steps (1e7) in training. The y-axis represents the mean success rate(standard deviations in shade).", "description": "The figure shows the results of different exploration methods on three Robosuite tasks: Door Opening, Table Wiping, and Pick-and-Place.  The x-axis represents the number of training steps (in millions), and the y-axis shows the mean success rate achieved by each method. Shaded regions represent standard deviations.  The plot visually compares the performance of EME against several baselines, illustrating its superior performance across all three tasks.", "section": "5.1 Continuous Control"}, {"figure_path": "QpKWFLtZKi/figures/figures_7_1.jpg", "caption": "Figure 3: Comparison results for different hard exploration Atari games.", "description": "This figure compares the performance of EME against other baseline methods across various challenging Atari games known for their difficulty in exploration.  The x-axis represents the number of frames (training steps), and the y-axis shows the average game score per episode. The shaded areas likely represent the standard deviation across multiple runs.  The figure aims to visually demonstrate EME's superior learning efficiency and ability to achieve higher average scores compared to other methods.", "section": "5.2 Discrete-action Games"}, {"figure_path": "QpKWFLtZKi/figures/figures_8_1.jpg", "caption": "Figure 4: Results of exploration tasks on Habitat. Error bars represent std, deviations over 5 seeds.", "description": "This figure presents the results of the exploration tasks conducted in the Habitat environment.  It compares the performance of EME against several baselines (ICM, RND, RIDE, NovelD, E3B, LIBERTY) across three different tasks: CloseCab, OpenFridge, and Rearrange. The y-axis shows the mean success rate, and the error bars represent the standard deviation over five seeds.  The figure visually demonstrates the superior performance of EME compared to other methods in achieving higher success rates across the three tasks.", "section": "5.3 Real-life Habitat Environment"}, {"figure_path": "QpKWFLtZKi/figures/figures_8_2.jpg", "caption": "Figure 5: Visual observations in Habitat (RGB, semantic, and depth)", "description": "This figure shows example visual observations from the Habitat environment.  It showcases the type of data used by the RL agents to learn navigation: RGB images, semantic segmentation maps (identifying object types), and depth maps (distance to objects).  These multi-modal observations make the Habitat environment particularly challenging for exploration, as subtle changes in the scene are significant.", "section": "5.3 Real-life Habitat Environment"}, {"figure_path": "QpKWFLtZKi/figures/figures_18_1.jpg", "caption": "Figure 6: Ablation study on the max reward scaling M (M = 1, 5, 10, 20, 40).", "description": "This ablation study shows the effect of the maximum reward scaling factor M on the performance of the EME algorithm across three Robosuite tasks: Door Opening, Table Wiping, and Pick-and-Place.  Different values of M (1, 5, 10, 20, 40) were tested, with M=10 used as the default. The results demonstrate the optimal range of M and how this hyperparameter affects exploration and learning efficiency.  The shaded regions represent standard deviations across multiple runs.", "section": "5.1 Continuous Control"}, {"figure_path": "QpKWFLtZKi/figures/figures_18_2.jpg", "caption": "Figure 7: Ablation study on the ensemble size (ES = 3, 6, 9, 12) and the other baseline with best performance.", "description": "This figure shows the ablation study on the ensemble size of reward models used in the EME method.  The x-axis represents training timesteps, and the y-axis represents the mean success rate for three different Robosuite tasks: Door Opening, Table Wiping, and Pick-and-Place. Different lines represent the results using different ensemble sizes (ES = 3, 6, 9, 12).  The shaded area around each line represents the standard deviation. The red line shows the best performing baseline for comparison. The results indicate that increasing the ensemble size improves performance up to a certain point (ES=9), after which the performance starts to decrease slightly. This demonstrates a trade-off between accuracy of reward variance prediction and exploration. ", "section": "5.1 Continuous Control"}, {"figure_path": "QpKWFLtZKi/figures/figures_18_3.jpg", "caption": "Figure 2: Results for various hard exploration tasks from Robosuite. The x-axis represents the number of steps (1e7) in training. The y-axis represents the mean success rate(standard deviations in shade).", "description": "This figure shows the results of different exploration methods on three manipulation tasks in Robosuite. The x-axis shows the number of training steps (in millions), and the y-axis shows the mean success rate achieved by each method. Shaded areas represent standard deviations.  EME consistently outperforms other methods across all three tasks, demonstrating its effectiveness in continuous control environments.", "section": "5.1 Continuous Control"}, {"figure_path": "QpKWFLtZKi/figures/figures_18_4.jpg", "caption": "Figure 9: Results for EME and its variants combined with different feature encoders on more Atari games.", "description": "This figure compares the performance of the proposed EME method and its variants (using different feature encoders) across multiple challenging Atari games.  It shows the average return achieved by each method over time, illustrating the impact of the feature encoder on exploration effectiveness. The results demonstrate how the choice of feature encoder affects the learning and exploration performance in diverse Atari game environments.", "section": "5.2 Discrete-action Games"}, {"figure_path": "QpKWFLtZKi/figures/figures_19_1.jpg", "caption": "Figure 10: Trajectories of policies trained with different exploration algorithms in the Habitat environment. Our method EME reveals the largest portion of the map than other methods.", "description": "This figure shows the trajectories of different reinforcement learning agents trained with various exploration methods in a Habitat environment.  The different colors represent the paths taken by agents using EME, EME-Static (EME without dynamic scaling), EME-EP (EME with episodic count scaling), ICM, RND, RIDE, NovelD, E3B, and LIBERTY. The figure visually demonstrates that the EME algorithm explores a significantly larger area of the environment compared to the other methods.", "section": "5.3 Real-life Habitat Environment"}, {"figure_path": "QpKWFLtZKi/figures/figures_19_2.jpg", "caption": "Figure 4: Results of exploration tasks on Habitat. Error bars represent std, deviations over 5 seeds.", "description": "This figure compares the performance of EME against several baselines on three different exploration tasks within the Habitat environment.  The tasks are CloseCab, OpenFridge, and Rearrange. The y-axis represents the mean success rate, showing the percentage of successful task completions. Error bars indicate standard deviation across multiple trials. The results demonstrate EME's superior performance and scalability in comparison to other methods. ", "section": "5.3 Real-life Habitat Environment"}, {"figure_path": "QpKWFLtZKi/figures/figures_21_1.jpg", "caption": "Figure 2: Results for various hard exploration tasks from Robosuite. The x-axis represents the number of steps (1e7) in training. The y-axis represents the mean success rate (standard deviations in shade).", "description": "The figure shows the results of different reinforcement learning algorithms on three challenging Robosuite tasks: Door Opening, Table Wiping, and Pick-and-Place.  The x-axis represents the number of training steps (in millions), and the y-axis shows the mean success rate achieved by each algorithm. Error bars indicate standard deviations.  The graph visually compares the performance of EME against several baselines, illustrating its superior performance in successfully completing these challenging robotic manipulation tasks.", "section": "5.1 Continuous Control"}, {"figure_path": "QpKWFLtZKi/figures/figures_21_2.jpg", "caption": "Figure 13: Results of EME, EME with a static scaling factor, and EME with an episodic count from Robosuite. The x-axis represents the number of steps (1e7) in training. The y-axis represents the mean success rate(standard deviations in shade).", "description": "This figure compares the performance of three variations of the EME algorithm on three Robosuite tasks: Door Opening, Table Wiping, and Pick-and-Place.  The variations are the full EME algorithm, EME without the dynamic scaling factor, and EME using an episodic count as the scaling factor.  The x-axis shows the number of training steps (in millions), and the y-axis displays the mean success rate, with shaded areas representing standard deviations, indicating performance variability.  The results illustrate how the dynamic scaling factor in the full EME method contributes to improved performance over simpler alternatives.", "section": "5.1 Continuous Control"}, {"figure_path": "QpKWFLtZKi/figures/figures_22_1.jpg", "caption": "Figure 2: Results for various hard exploration tasks from Robosuite. The x-axis represents the number of steps (1e7) in training. The y-axis represents the mean success rate(standard deviations in shade).", "description": "The figure shows the mean success rates for three challenging Robosuite manipulation tasks (Door Opening, Table Wiping, Pick and Place) across different exploration methods.  The x-axis represents the number of training steps (in millions), and the y-axis shows the mean success rate, with shaded regions indicating standard deviation. The figure demonstrates the superior performance of EME (Effective Metric-based Exploration Bonus) compared to other baseline methods over time.", "section": "5.1 Continuous Control"}, {"figure_path": "QpKWFLtZKi/figures/figures_23_1.jpg", "caption": "Figure 1: Trajectories of policies trained with different exploration algorithms in the real-life indoor environment. (a) episodic count-based method under Lp norm (b) bisimulation metric-based method (c) our method.", "description": "This figure compares the trajectories of three different reinforcement learning agents trained with different exploration methods in a real indoor environment.  The agents are trained using (a) a count-based method with Lp norm, (b) a bisimulation metric-based method, and (c) the proposed Effective Metric-based Exploration-bonus (EME) method. The figure visually demonstrates how the EME method explores a much larger portion of the environment compared to the other two methods.", "section": "1 Introduction"}, {"figure_path": "QpKWFLtZKi/figures/figures_23_2.jpg", "caption": "Figure 15: Visual observations in Habitat (RGB, semantic, and depth)", "description": "The figure displays three different visual observation modalities provided by the Habitat simulator: RGB image, semantic segmentation, and depth map.  The RGB image shows a realistic view of an indoor scene. The semantic segmentation depicts the scene with different color-coded regions corresponding to object classes. The depth map provides a visual representation of the scene's distances from the viewpoint, often represented by a color gradient.", "section": "5.3 Real-life Habitat Environment"}, {"figure_path": "QpKWFLtZKi/figures/figures_23_3.jpg", "caption": "Figure 10: Trajectories of policies trained with different exploration algorithms in the real-life indoor environment. (a) episodic count-based method under Lp norm (b) bisimulation metric-based method (c) our method.", "description": "This figure compares the trajectories of three different exploration methods in a real-world indoor environment.  Method (a) uses an episodic count-based approach with Lp norms, (b) uses a bisimulation metric-based approach, and (c) uses the proposed EME method. The figure visually demonstrates the superior exploration coverage achieved by the EME method compared to the other two approaches.", "section": "4 Effective Metric-based Exploration Bonus"}]