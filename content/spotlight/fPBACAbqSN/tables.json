[{"figure_path": "fPBACAbqSN/tables/tables_2_1.jpg", "caption": "Table 1: Comparison of different sparse patterns.", "description": "This table compares four different sparse attention patterns: A-shape, Vertical-Slash, Block-Sparse, and Top-K.  It contrasts them based on their spatial distribution (how the sparse elements are arranged in the attention matrix), latency on GPU (how long it takes to compute attention using these patterns), and the time required to build the index for sparse computation.  The table highlights that A-shape has low latency and zero index build time, while Vertical-Slash has medium latency and small index build time. Block-Sparse is similar to Vertical-Slash, and Top-K exhibits high latency and index build time. This illustrates the trade-offs involved when choosing a sparse attention pattern for long-context LLMs.", "section": "2.2 Attention Sparsity Exhibits Patterns"}, {"figure_path": "fPBACAbqSN/tables/tables_4_1.jpg", "caption": "Table 2: Performance of different methods with different base models on InfiniteBench [ZCH+24].", "description": "This table presents the performance comparison of various methods, including MInference and its variants, along with several baselines, on the InfiniteBench benchmark.  The results are broken down by specific tasks within the benchmark (e.g., English summarization, English question answering, etc.) and show the average performance across all tasks.  The table helps demonstrate the effectiveness of MInference in improving performance across a diverse set of long-context tasks while using different base LLMs.", "section": "4 Experiments"}, {"figure_path": "fPBACAbqSN/tables/tables_6_1.jpg", "caption": "Table 2: Performance of different methods with different base models on InfiniteBench [ZCH+24].", "description": "This table presents the performance comparison of different methods (StreamingLLM, StreamingLLM with dilated/strided windows, InfLLM, Ours with static masks, and Ours (MInference)) on the InfiniteBench benchmark.  It shows the average performance across ten tasks, categorized into English summarization, English question answering, English multiple choice questions, English dialogue, Chinese question answering, code debugging, math finding, and retrieval tasks (Passkey, Number, and KV retrieval).  The results are broken down by the three different base LLMs used: LLaMA-3-8B-262K, Yi-9B-200K, and GLM-4-9B-1M.  It illustrates the impact of different sparse attention methods on the accuracy of various downstream tasks for long-context LLMs.", "section": "4 Experiments"}, {"figure_path": "fPBACAbqSN/tables/tables_6_2.jpg", "caption": "Table 3: Performance (%) of different models and different methods on RULER [HSK+24] evaluated at lengths from 4k to 128k.", "description": "This table presents the performance of different methods (StreamingLLM variants, InfLLM, and the proposed MInference method) on the RULER benchmark, which evaluates long-context reasoning capabilities.  The performance is evaluated at different context lengths (4K, 8K, 16K, 32K, 64K, and 128K tokens). The \"Claimed Effective\" column indicates the claimed effective context window size reported by each model.", "section": "4 Experiments"}, {"figure_path": "fPBACAbqSN/tables/tables_8_1.jpg", "caption": "Table 2: Performance of different methods with different base models on InfiniteBench [ZCH+24].", "description": "This table presents a comparison of the performance of different methods (StreamingLLM, StreamingLLM w/ dilated, StreamingLLM w/ strided, InfLLM, Ours w/ static, Ours) on the InfiniteBench benchmark across multiple tasks (En.Sum, En.QA, En.MC, En.Dia, Zh.QA, Code.Debug, Math.Find, Retr.PassKey, Retr.Num, Retr.KV).  The results are shown for three different base LLMs: LLaMA-3-8B-262K, Yi-9B-200K, and GLM-4-9B-1M. The table allows readers to evaluate the effectiveness of MInference in comparison to other existing methods for long-context LLM inference, and across different model architectures. The average performance across all tasks is also provided for each method and model.", "section": "4 Experiments"}, {"figure_path": "fPBACAbqSN/tables/tables_8_2.jpg", "caption": "Table 5: Performance of different methods on InfiniteBench [ZCH+24] using SnapKV [LHY+24] in the decoding stage.", "description": "This table presents the performance comparison of different methods on the InfiniteBench benchmark when using SnapKV for KV cache compression in the decoding stage. It shows the average performance across various tasks (e.g., summarization, question answering, code debugging) using LLaMA-3 with and without MInference. The results demonstrate the compatibility and potential performance gains of combining MInference with KV cache compression techniques for efficient long-context LLM inference.", "section": "Integrate with KV cache compression methods"}, {"figure_path": "fPBACAbqSN/tables/tables_8_3.jpg", "caption": "Table 2: Performance of different methods with different base models on InfiniteBench [ZCH+24].", "description": "This table presents the performance comparison of different methods (StreamingLLM with variations, InfLLM, and the proposed method 'Ours' with and without static sparse indices) across various tasks within the InfiniteBench benchmark.  The results are broken down by specific task (e.g., English summarization, English question answering, etc.) and model (LLaMA-3-8B-262K, Yi-9B-200K, GLM-4-9B-1M) showing performance scores for each method.  The average performance across all tasks is provided for each model and method.", "section": "4 Experiments"}, {"figure_path": "fPBACAbqSN/tables/tables_18_1.jpg", "caption": "Table 2: Performance of different methods with different base models on InfiniteBench [ZCH+24].", "description": "This table presents the performance comparison of different methods (StreamingLLM with variations, InfLLM, and the proposed method 'Ours' with and without static sparse attention) across various tasks within the InfiniteBench benchmark.  The results are broken down by model (LLaMA-3-8B-262K, Yi-9B-200K, and GLM-4-9B-1M) and specific tasks (e.g., summarization, question answering, dialogue, code debugging, etc.). The average performance across all tasks is also provided.  The table highlights how MInference performs compared to baselines in terms of accuracy on long-context tasks.", "section": "4 Experiments"}, {"figure_path": "fPBACAbqSN/tables/tables_21_1.jpg", "caption": "Table 4: Performance of different ablation methods using LLaMA-3-8B-Instruct-262K on In-finiteBench [ZCH+24].", "description": "This table presents the results of an ablation study conducted on the InfiniteBench benchmark using the LLaMA-3-8B-Instruct-262K model. The study evaluates the effectiveness of different components of the proposed MInference method by removing or modifying specific parts of the algorithm. The table shows the average performance across various tasks within the InfiniteBench benchmark for the following models: the full MInference model ('Ours'), MInference without Block-Sparse patterns ('Ours w/ only vertical'), and MInference without Vertical-Slash patterns ('Ours w/ only slash'). The results demonstrate the impact of each component on the overall performance, highlighting the importance of both Block-Sparse and Vertical-Slash patterns for achieving optimal accuracy.", "section": "4 Experiments"}, {"figure_path": "fPBACAbqSN/tables/tables_24_1.jpg", "caption": "Table 2: Performance of different methods with different base models on InfiniteBench [ZCH+24].", "description": "This table presents a comprehensive comparison of various methods' performance on the InfiniteBench benchmark, which consists of 10 diverse tasks.  The results are broken down by specific tasks (e.g., English summarization, English question answering, Chinese question answering, etc.) and across three different base LLMs (LLaMA-3-8B-262K, Yi-9B-200K, and GLM-4-9B-1M).  The table compares the performance of MInference against several baselines, including StreamingLLM with different windowing strategies, and InfLLM, providing a detailed analysis of the effectiveness and efficiency of the proposed approach across different models and tasks.", "section": "4 Experiments"}, {"figure_path": "fPBACAbqSN/tables/tables_26_1.jpg", "caption": "Table 2: Performance of different methods with different base models on InfiniteBench [ZCH+24].", "description": "This table presents the performance comparison of various methods (StreamingLLM, StreamingLLM with dilated and strided attention, InfLLM, Ours w/ static, and Ours) on different base models (LLaMA-3-8B-262K, Yi-9B-200K, and GLM-4-9B-1M) across multiple tasks within the InfiniteBench benchmark.  The tasks evaluate the models' performance in various Natural Language Processing (NLP) and code-related tasks, showing the average performance across all tasks for each method.  The results highlight how different methods handle different types of long-context scenarios.", "section": "4 Experiments"}]