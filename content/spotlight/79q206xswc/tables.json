[{"figure_path": "79q206xswc/tables/tables_6_1.jpg", "caption": "Table 1: Optimized M-SOG Metrics. We report the MSOG scores for both detection and segmentation tasks in our pipeline. The scores are calculated based on the car class for detection and all semantic classes for segmentation. Line-roll and Pyramid-roll are abbreviated as L-roll and P-roll.", "description": "This table presents the M-SOG (Surrogate Metric of Semantic Occupancy Grids) scores for different LiDAR placements.  M-SOG is used to evaluate the quality of LiDAR placements for 3D object detection and semantic segmentation tasks. Lower scores indicate better performance, implying more effective LiDAR placement. The table compares several baseline LiDAR configurations (Center, Line, Pyramid, Square, Trapezoid, Line-roll, Pyramid-roll) with the optimized configuration ('Ours').  The scores are separated for 3D object detection and 3D semantic segmentation tasks.", "section": "4 Experiments"}, {"figure_path": "79q206xswc/tables/tables_7_1.jpg", "caption": "Table 2: Performance evaluations of LiDAR semantic segmentation under clean and adverse conditions. For each LiDAR placement strategy, we report the mIoU scores (\u2191), represented in percentage (%). The average scores only include adverse scenarios.", "description": "This table presents the mean Intersection over Union (mIoU) scores for LiDAR semantic segmentation using different LiDAR placement strategies.  The mIoU is a metric evaluating the accuracy of semantic segmentation. Results are shown for both clean and adverse (corrupted) conditions. The average mIoU across various adverse conditions is also provided for each placement strategy.", "section": "4 Experiments"}, {"figure_path": "79q206xswc/tables/tables_7_2.jpg", "caption": "Table 2: Performance evaluations of LiDAR semantic segmentation under clean and adverse conditions. For each LiDAR placement strategy, we report the mIoU scores (\u2191), represented in percentage (%). The average scores only include adverse scenarios.", "description": "This table presents the mean Intersection over Union (mIoU) scores for LiDAR semantic segmentation using different LiDAR placement strategies.  The performance is evaluated under both clean and six different adverse conditions (fog, wet ground, snow, motion blur, crosstalk, incomplete echo). The table highlights the mIoU for each condition and provides an average mIoU across all adverse conditions for comparison.", "section": "4 Experiments"}, {"figure_path": "79q206xswc/tables/tables_18_1.jpg", "caption": "Table 4: Comparisons between the established benchmark and popular driving perception datasets with support for both the LiDAR semantic segmentation and 3D object detection tasks.", "description": "This table compares the Place3D dataset with three other popular autonomous driving datasets: nuScenes, Waymo, and SemanticKITTI.  The comparison highlights key differences in the number of detection and segmentation classes, the number of 3D bounding boxes and points per frame, the number of LiDAR channels, the vertical field of view of the LiDAR sensors, the LiDAR placement strategy (single vs. multiple), and the presence or absence of adverse weather conditions in the data.", "section": "4.1 Benchmark Setups"}, {"figure_path": "79q206xswc/tables/tables_18_2.jpg", "caption": "Table 2: Performance evaluations of LiDAR semantic segmentation under clean and adverse conditions. For each LiDAR placement strategy, we report the mIoU scores (\u2191), represented in percentage (%). The average scores only include adverse scenarios.", "description": "This table presents the performance of LiDAR semantic segmentation using different placement strategies.  The mIoU (mean Intersection over Union) scores are reported for both clean and adverse (fog, wet ground, snow, motion blur, crosstalk, incomplete echo) conditions.  Higher mIoU indicates better performance. The average mIoU across adverse conditions is also provided for each strategy.", "section": "4 Experiments"}, {"figure_path": "79q206xswc/tables/tables_20_1.jpg", "caption": "Table 11: The per-class LiDAR semantic segmentation results of MinkUNet [18], SPVCNN [86], PolarNet [111], and Cylinder3D [117], under different LiDAR placement strategies. All mIoU (\u2191) and class-wise IoU (\u2191) scores are given in percentage (%).", "description": "This table presents a detailed breakdown of the performance of four different LiDAR semantic segmentation models (MinkUNet, SPVCNN, PolarNet, and Cylinder3D) across various LiDAR placement strategies.  The performance is measured using the mean Intersection over Union (mIoU) and class-wise IoU, both expressed as percentages.  The table allows for a comparison of model performance across different LiDAR configurations and highlights strengths and weaknesses of each model in specific semantic categories.", "section": "C Additional Quantitative Result"}, {"figure_path": "79q206xswc/tables/tables_20_2.jpg", "caption": "Table 11: The per-class LiDAR semantic segmentation results of MinkUNet [18], SPVCNN [86], PolarNet [111], and Cylinder3D [117], under different LiDAR placement strategies. All mIoU (\u2191) and class-wise IoU (\u2191) scores are given in percentage (%).", "description": "This table presents a detailed breakdown of the performance of four different LiDAR semantic segmentation models (MinkUNet, SPVCNN, PolarNet, and Cylinder3D) across various LiDAR placement strategies.  The results are shown as mIoU (mean Intersection over Union) scores and class-wise IoU scores, both expressed as percentages. This allows for a granular analysis of model performance on specific semantic classes under different LiDAR configurations.", "section": "C Additional Quantitative Result"}, {"figure_path": "79q206xswc/tables/tables_23_1.jpg", "caption": "Table 8: The training and optimization configurations of the four LiDAR semantic segmentation models [18, 86, 111, 117] used in our Place3D benchmark.", "description": "This table lists the hyperparameters used for training the four LiDAR semantic segmentation models (MinkUNet, SPVCNN, PolarNet, and Cylinder3D) within the Place3D benchmark.  It specifies the batch size, number of epochs, optimizer (AdamW), learning rate, weight decay, and epsilon values for each model. These settings are crucial for understanding and reproducing the experimental results.", "section": "B Additional Implementation Detail"}, {"figure_path": "79q206xswc/tables/tables_23_2.jpg", "caption": "Table 9: The training and optimization configurations of the four 3D object detection models [51, 107, 63, 109] used in our Place3D benchmark.", "description": "This table presents the hyperparameters used for training the four 3D object detection models (PointPillars, CenterPoint, BEVFusion-L, and FSTR) within the Place3D benchmark.  It details the batch size, number of epochs, optimizer (AdamW), learning rate, weight decay, and epsilon values used for each model.", "section": "B Additional Implementation Detail"}, {"figure_path": "79q206xswc/tables/tables_24_1.jpg", "caption": "Table 10: Comparisons of the key differences among existing sensor placement approaches.", "description": "This table compares four different methods for optimizing LiDAR sensor placement for autonomous driving, highlighting key differences in venue, deployment type (vehicle-mounted vs roadside), sensor type, prior information used, the tasks addressed (detection or segmentation), and whether the method includes optimization.  Place3D is shown to be unique in optimizing for both semantic segmentation and detection, and utilizing semantic occupancy as prior information. ", "section": "4 Experiments"}, {"figure_path": "79q206xswc/tables/tables_28_1.jpg", "caption": "Table 2: Performance evaluations of LiDAR semantic segmentation under clean and adverse conditions. For each LiDAR placement strategy, we report the mIoU scores (\u2191), represented in percentage (%). The average scores only include adverse scenarios.", "description": "This table presents the mean Intersection over Union (mIoU) scores for LiDAR semantic segmentation using different LiDAR placement strategies.  It compares performance across various models (MinkUNet, SPVCNN, PolarNet, Cylinder3D) under both clean and adverse weather conditions (fog, wet ground, snow, motion blur, crosstalk, incomplete echo). The mIoU, a measure of semantic segmentation accuracy, is reported as a percentage.  Average mIoU scores across the adverse conditions are also provided for each strategy.", "section": "4 Experiments"}, {"figure_path": "79q206xswc/tables/tables_29_1.jpg", "caption": "Table 12: Benchmark results of LiDAR semantic segmentation under clean and adverse conditions. For each placement, we report the mIoU (\u2191), mAcc (\u2191), and ECE (\u2193) scores for models under the clean condition, and mIoU (\u2191) scores for models under adverse conditions. The mIoU and mAcc scores are given in percentage (%).", "description": "This table presents the performance evaluation results of LiDAR semantic segmentation using different LiDAR placement strategies under both clean and adverse conditions.  The metrics used are mean Intersection over Union (mIoU), mean Accuracy (mAcc), Expected Calibration Error (ECE), and average mIoU across adverse conditions.  Higher mIoU and mAcc values indicate better performance, while a lower ECE value signifies better calibration. The table allows comparison of various LiDAR placement strategies across different adverse weather and sensor failure conditions.", "section": "C Additional Quantitative Result"}, {"figure_path": "79q206xswc/tables/tables_30_1.jpg", "caption": "Table 2: Performance evaluations of LiDAR semantic segmentation under clean and adverse conditions. For each LiDAR placement strategy, we report the mIoU scores (\u2191), represented in percentage (%). The average scores only include adverse scenarios.", "description": "This table presents the mean Intersection over Union (mIoU) scores for LiDAR semantic segmentation using different LiDAR placement strategies.  The mIoU is a metric evaluating the accuracy of semantic segmentation. The results are shown for both clean conditions (no added noise or weather effects) and for several adverse conditions (fog, wet ground, snow, motion blur, crosstalk, incomplete echo).  The average mIoU score across the adverse conditions is also provided for each strategy, allowing for a comparison of robustness.", "section": "4 Experiments"}, {"figure_path": "79q206xswc/tables/tables_31_1.jpg", "caption": "Table 12: Benchmark results of LiDAR semantic segmentation under clean and adverse conditions. For each placement, we report the mIoU (\u2191), mAcc (\u2191), and ECE (\u2193) scores for models under the clean condition, and mIoU (\u2191) scores for models under adverse conditions. The mIoU and mAcc scores are given in percentage (%).", "description": "This table presents the performance of four different LiDAR semantic segmentation models (MinkUNet, SPVCNN, PolarNet, and Cylinder3D) under various LiDAR placement strategies.  The results are broken down by performance metric (mIoU, mAcc, ECE) for both clean and adverse conditions (fog, wet ground, snow, motion blur, crosstalk, and incomplete echo).  The table allows for a comparison of model performance under various conditions and LiDAR placements.", "section": "C Additional Quantitative Result"}]