[{"figure_path": "On5WIN7xyD/tables/tables_22_1.jpg", "caption": "Table D.1: Collected metadata and base evaluation metrics for base pretrained models used in Sec. 4.1, Sec. 4.3, and Sec. 5. Model names follow the HuggingFace naming. See data collection details in Appx. D.1.1. For the most up-to-date results, please refer to https://github.com/ryoungj/ObsScaling/blob/main/eval_results/base_llm_benchmark_eval.csv.", "description": "This table presents the metadata and evaluation results for the pretrained base language models used in sections 4.1, 4.3, and 5 of the paper.  It includes model family, model name, number of parameters (in billions), training data size (in trillions of tokens), total training FLOPs (in 1e21), and scores on several benchmark tasks: MMLU, ARC-C, HellaSwag, Winograd, TruthfulQA, XWinograd, and HumanEval. The table provides a comprehensive overview of the models' characteristics and their performance across different evaluation metrics.  Note that the most up-to-date results can be found at the provided GitHub link.", "section": "D.1 Model Collection & Evaluation"}, {"figure_path": "On5WIN7xyD/tables/tables_23_1.jpg", "caption": "Table D.1: Collected metadata and base evaluation metrics for base pretrained models used in Sec. 4.1, Sec. 4.3, and Sec. 5. Model names follow the HuggingFace naming. See data collection details in Appx. D.1.1. For the most up-to-date results, please refer to https://github.com/ryoungj/ObsScaling/blob/main/eval_results/base_llm_benchmark_eval.csv.", "description": "This table presents the metadata and base evaluation metrics for the pretrained base models used in sections 4.1, 4.3, and 5 of the paper.  The metadata includes parameters, data size, and FLOPs (floating point operations).  The evaluation metrics cover several benchmarks: MMLU (Massive Multitask Language Understanding), ARC-C (AI2 Reasoning Challenge), HellaSwag (Commonsense Reasoning), Winograd Schema Challenge, TruthfulQA (Truthfulness), XWinograd (Multilingual Commonsense), and HumanEval (Programming).  Model names follow the HuggingFace naming convention. For the most current results, consult the provided GitHub link.", "section": "D.1 Model Collection & Evaluation"}, {"figure_path": "On5WIN7xyD/tables/tables_40_1.jpg", "caption": "Table D.1: Collected metadata and base evaluation metrics for base pretrained models used in Sec. 4.1, Sec. 4.3, and Sec. 5. Model names follow the HuggingFace naming. See data collection details in Appx. D.1.1. For the most up-to-date results, please refer to https://github.com/ryoungj/ObsScaling/blob/main/eval_results/base_llm_benchmark_eval.csv.", "description": "This table presents the metadata and evaluation results for 77 pretrained base language models used in sections 4.1, 4.3, and 5 of the paper.  The metadata includes the number of parameters, the amount of training data, and the estimated training FLOPs. The evaluation metrics include scores from several standard benchmarks assessing general capabilities, reasoning, and programming skills.  The table also specifies the model family and model name following the HuggingFace naming convention.  A link is provided for the most up-to-date results.", "section": "D.1 Model Collection & Evaluation"}, {"figure_path": "On5WIN7xyD/tables/tables_43_1.jpg", "caption": "Table D.1: Collected metadata and base evaluation metrics for base pretrained models used in Sec. 4.1, Sec. 4.3, and Sec. 5. Model names follow the HuggingFace naming. See data collection details in Appx. D.1.1. For the most up-to-date results, please refer to https://github.com/ryoungj/ObsScaling/blob/main/eval_results/base_llm_benchmark_eval.csv.", "description": "This table presents the metadata and evaluation results for the pretrained base language models used in sections 4.1, 4.3, and 5 of the paper.  It includes information such as the model family, model name, number of parameters, data size, training FLOPs, and performance scores on various standard benchmarks (MMLU, ARC-C, HellaSwag, Winograd, TruthfulQA, XWinograd, HumanEval). The data collection process is detailed in Appendix D.1.1, and a link to the most up-to-date results is provided.", "section": "D.1 Model Collection & Evaluation"}, {"figure_path": "On5WIN7xyD/tables/tables_44_1.jpg", "caption": "Table D.1: Collected metadata and base evaluation metrics for base pretrained models used in Sec. 4.1, Sec. 4.3, and Sec. 5. Model names follow the HuggingFace naming. See data collection details in Appx. D.1.1. For the most up-to-date results, please refer to https://github.com/ryoungj/ObsScaling/blob/main/eval_results/base_llm_benchmark_eval.csv.", "description": "This table presents a comprehensive overview of the metadata and baseline evaluation metrics for various pretrained language models used in different sections of the research paper.  It includes information such as model family, model name, number of parameters, data size, training FLOPs, and performance scores on several standard benchmarks (MMLU, ARC-C, HellaSwag, Winogrande, TruthfulQA, XWinograd, HumanEval). The table facilitates a detailed comparison of various models across different scales and capabilities.", "section": "D.1 Model Collection & Evaluation"}]