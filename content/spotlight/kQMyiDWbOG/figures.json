[{"figure_path": "kQMyiDWbOG/figures/figures_2_1.jpg", "caption": "Figure 1: (a) Positional encoding (PE) in ANN Transformers. (b) Relative PE in Spike Transformers [4\u20136]. (c) Our Proposed CPG-PE method. (d) CPG-PE consistently improves learning performance across various tasks. CPG-PE is an ideal PE method tailored for SNNs, detailed in Section 3.", "description": "This figure compares three different positional encoding (PE) methods for neural networks: (a) Traditional PE in ANN Transformers using sine and cosine functions; (b) Existing PE methods in spike-based Transformers, which have limitations in terms of uniqueness and spike-form representation; (c) The proposed CPG-PE method, which leverages central pattern generators (CPGs) for a biologically-inspired and more effective PE tailored for spiking neural networks (SNNs). Finally, (d) shows the improved performance of CPG-PE compared to existing methods across different tasks, including time-series forecasting, text classification, and image classification.", "section": "3 Methods"}, {"figure_path": "kQMyiDWbOG/figures/figures_4_1.jpg", "caption": "Figure 2: (a) Illustration of a pair of CPG neurons demonstrating mutual inhibition through spiking activity. The spikes represent neural spikes that inhibit each other, exemplifying the coordination mechanism in CPG networks. (b) Spike trains of the first 4 CPG neurons. The curve represents the membrane potential, while the vertical lines represent spikes.", "description": "This figure illustrates the concept of Central Pattern Generators (CPGs). Panel (a) shows a schematic diagram of two CPG neurons mutually inhibiting each other through spiking activity. Panel (b) presents spike trains of four CPG neurons, depicting the rhythmic and coordinated spiking pattern produced by a CPG network.  The curves represent the neurons' membrane potential over time, with vertical lines indicating spike events.", "section": "3.2 CPG-based Positional Encoding"}, {"figure_path": "kQMyiDWbOG/figures/figures_5_1.jpg", "caption": "Figure 3: Illustration of applying CPG-PE to SNNs. X, X', and Xoutput are all spike matrices. Initially, CPG-PE encodes the positional information of the input spike matrix X, resulting in X'. Then, to maintain binary values and avoid introducing non-binary elements, we opt to concatenate X and X' along the feature dimension. Lastly, a linear layer is employed to map the feature dimension from D + 2N back to D, where D is the feature dimension of X, and N is the number of CPG pairs. This effectively neutralizes the dimensional increase caused by concatenation. The whole process can be formalized as follows: X' = CPG-PE(X), X\u2081 = X + X', Xoutput = SN (BN (Linear (X1))), X \u2208 {0,1}T\u00d7B\u00d7L\u00d7D, X' \u2208 {0, 1}T\u00d7B\u00d7L\u00d72N, X\u2081 \u2208 {0,1}T\u00d7B\u00d7L\u00d7(D+2N), Xoutput \u2208 {0,1}T\u00d7B\u00d7L\u00d7D where BN represents batch normalization and SN is a spike neuron layer. Furthermore, CPG-PE necessitates that input samples be sequential data, making it directly applicable to time series data and natural language. For image data, however, an adaptation is required: images must be segmented into patches similar to the approach used in the Vision Transformer [23]. Considering the compatibility with neuromorphic hardware, we also (1) implement CPG-PE with LIF neurons, and (2) integrate CPG-PE into a classic linear layer. Please refer to Appendices C and D for details.", "description": "This figure illustrates the implementation of the CPG-PE method in SNNs.  It shows how the positional encoding (CPG-PE) is integrated into the network, maintaining binary spike signals.  A linear layer adjusts the dimensionality after concatenation of the input and encoded signals, preparing the data for a spiking neuron layer.", "section": "3 Methods"}, {"figure_path": "kQMyiDWbOG/figures/figures_8_1.jpg", "caption": "Figure 1: (a) Positional encoding (PE) in ANN Transformers. (b) Relative PE in Spike Transformers [4\u20136]. (c) Our Proposed CPG-PE method. (d) CPG-PE consistently improves learning performance across various tasks. CPG-PE is an ideal PE method tailored for SNNs, detailed in Section 3.", "description": "This figure compares different positional encoding methods for neural networks, including traditional methods used in ANN transformers and spike transformers, and the novel CPG-PE method proposed in the paper.  Panel (a) shows the sinusoidal positional encoding in ANN Transformers. Panel (b) illustrates the relative positional encoding previously used in spike-based transformers. Panel (c) presents the proposed CPG-PE (Central Pattern Generator Positional Encoding) method. Finally, Panel (d) demonstrates the superior performance of CPG-PE across various tasks like time-series forecasting, text classification, and image classification, highlighting its effectiveness as a positional encoding technique for spiking neural networks (SNNs).", "section": "3 Methods"}, {"figure_path": "kQMyiDWbOG/figures/figures_17_1.jpg", "caption": "Figure 3: Illustration of applying CPG-PE to SNNs. X, X', and Xoutput are all spike matrices.", "description": "This figure illustrates how the proposed CPG-PE method is integrated into spiking neural networks (SNNs). It shows the process of positional encoding using CPG-PE, followed by concatenation with the input spike matrix and a linear layer to map the feature dimension back to the original size.  The figure highlights that the entire process maintains the spike format for hardware-friendly compatibility.", "section": "3 Methods"}]