[{"figure_path": "dIktpSgK4F/figures/figures_0_1.jpg", "caption": "Figure 1: We propose a new way to study query-key interactions via the singular value decomposition of the query-key interaction matrix. Many of the modes (i.e. pairs of singular vectors corresponding to the query and the key respectively), are semantically interpretable. Two example modes are shown. Top row: ViT layer 8 head 7 mode 2. Bottom row: DINO layer 8 head 9 mode 2. The red channel indicates the projection value of embedding onto the left singular vector which corresponds to the query; the cyan channel indicates the projection value of embedding onto the right singular vector which corresponds to the key.", "description": "This figure shows two examples of semantically interpretable modes obtained from the singular value decomposition of the query-key interaction matrix. Each example shows a pair of singular vectors (query and key) visualized as image channels. The red channel represents the query vector, while the cyan channel represents the key vector.  The images illustrate how the model attends to semantically similar features (e.g., parts of an object or related objects).  The top row shows an example from a Vision Transformer (ViT), and the bottom row shows an example from a self-supervised vision transformer (DINO).", "section": "Abstract"}, {"figure_path": "dIktpSgK4F/figures/figures_3_1.jpg", "caption": "Figure 2: Attention preference in the Odd-One-Out (O3) dataset [24]. A. An example from the O3 dataset. Two tokens are chosen to correspond to the target and distractor in the image. Attention maps using two tokens as queries are computed. We examine the overlap between the attention map of the target, and each of the mask labels of the target, distractor, and background masks. Similarly, we examine the overlap between the attention map of the distractor, and each of the mask labels of the distractor, target, and background. B. Ratio of attention on the same objects (target-target and distractor-distractor attention). The x-axis is normalized layer numbers, from early layers (left) to late layers (right). C. Ratio of attention on the different objects (target-distractor and distractor-target attention). D. Ratio of attention on the background (target-to-background and distractor-background attention)", "description": "This figure empirically studies whether an image token attends to tokens belonging to the same objects, different objects, or background using the Odd-One-Out dataset.  It analyzes attention preference across different ViT models, showing the ratio of attention on the same objects, different objects, and the background for both target and distractor tokens across various layers.  This helps visualize whether self-attention focuses more on grouping similar features or contextualizing with dissimilar features at different network depths.", "section": "3 Grouping or contextualizing"}, {"figure_path": "dIktpSgK4F/figures/figures_5_1.jpg", "caption": "Figure 3: Cosine similarity between left and right singular vectors. The cosine similarity is computed per head and singular mode. The weighted average value of cosine similarity is computed with weights of corresponding singular values.", "description": "This figure shows the weighted average cosine similarity between left and right singular vectors across different layers of various vision transformer models.  The cosine similarity is a measure of how similar the left and right singular vectors are for each singular mode (a pair of singular vectors representing a feature interaction). A high cosine similarity indicates that tokens attend to similar tokens (perceptual grouping), while a low cosine similarity indicates attention to dissimilar tokens (contextualization). The plot shows the trend of cosine similarity across different layers for several ViT models. This visualization is used to analyze whether self-attention in the models prioritizes grouping (similar tokens) or contextualization (dissimilar tokens) at different network depths.", "section": "4.2 Similarity between left and right singular vectors"}, {"figure_path": "dIktpSgK4F/figures/figures_6_1.jpg", "caption": "Figure 4: Examples of optimal attention images of singular modes and query and key map in dino-vitb16. Optimal attention images are found from the Imagenet validation set that induce the largest attention score (sorted by the product of the maximum of query map and maximum of key map). The red and cyan (i.e. green and blue) channels are the projection values of embedding onto the left and right singular vectors of a singular mode. They correspond to query and key. The white area is where the query map and key map overlap. The name code we assign to singular modes specifies the layer, head, and mode numbers. For example, \"L1 H5 M1\" means layer 1, head 5, and mode 1. The value below indicates the cosine similarity between the left and right singular vectors.", "description": "This figure shows examples of semantically interpretable singular modes from the DINO-vitb16 model. Each mode is represented by a pair of singular vectors (query and key), visualized in red and cyan channels respectively. The images shown are those that maximize the attention score for each mode. The caption also explains the naming convention for the singular modes and provides the cosine similarity between the left and right singular vectors for each mode.", "section": "Semanticity of singular modes"}, {"figure_path": "dIktpSgK4F/figures/figures_7_1.jpg", "caption": "Figure 5: Visualization of a single image with multiple modes. We pick an example dog image from the ImageNet dataset and use the dino-vitb16 model. Top 6 modes (ordered by the contribution to the attention score) for example layers and heads are shown. See Supplementary Figure S17 for extended mode visualizations of this image.", "description": "This figure visualizes how different singular modes of the dino-vitb16 model process a single dog image from the ImageNet dataset. It shows the top 6 modes (ranked by their contribution to the attention score) for selected layers and heads. Each mode is represented by a set of images, showcasing how the model attends to different features within the image based on these modes. To see a more complete visualization of the modes, refer to Supplementary Figure S17.", "section": "5 Semanticity of singular modes"}, {"figure_path": "dIktpSgK4F/figures/figures_8_1.jpg", "caption": "Figure 2: Attention preference in the Odd-One-Out (O3) dataset [24]. A. An example from the O3 dataset. Two tokens are chosen to correspond to the target and distractor in the image. Attention maps using two tokens as queries are computed. We examine the overlap between the attention map of the target, and each of the mask labels of the target, distractor, and background masks. Similarly, we examine the overlap between the attention map of the distractor, and each of the mask labels of the distractor, target, and background. B. Ratio of attention on the same objects (target-target and distractor-distractor attention). The x-axis is normalized layer numbers, from early layers (left) to late layers (right). C. Ratio of attention on the different objects (target-distractor and distractor-target attention). D. Ratio of attention on the background (target-to-background and distractor-background attention)", "description": "This figure shows the attention preference of different ViT models on the Odd-One-Out dataset.  It examines how attention is distributed between same-object tokens, different-object tokens, and background tokens across various layers of the model.  Subplots (B), (C), and (D) present the ratio of attention for each of these categories across different layers, illustrating the change in attention preference from early layers to later layers.", "section": "3 Grouping or contextualizing"}, {"figure_path": "dIktpSgK4F/figures/figures_13_1.jpg", "caption": "Figure 3: Cosine similarity between left and right singular vectors. The cosine similarity is computed per head and singular mode. The weighted average value of cosine similarity is computed with weights of corresponding singular values.", "description": "This figure shows the cosine similarity between left and right singular vectors in various ViT models across different layers.  The cosine similarity is calculated for each singular mode and then averaged across heads, weighting the average by the corresponding singular values.  High cosine similarity indicates that tokens mostly attend to other similar tokens (grouping), while low cosine similarity suggests tokens attend more to dissimilar tokens (contextualization). The plot shows the trend in different ViT models. In many models, there is a decrease in cosine similarity from earlier layers to later layers. Some self-supervised models show a different trend, having higher cosine similarity in the last few layers.", "section": "4.2 Similarity between left and right singular vectors"}, {"figure_path": "dIktpSgK4F/figures/figures_13_2.jpg", "caption": "Figure 2: Attention preference in the Odd-One-Out (O3) dataset [24]. A. An example from the O3 dataset. Two tokens are chosen to correspond to the target and distractor in the image. Attention maps using two tokens as queries are computed. We examine the overlap between the attention map of the target, and each of the mask labels of the target, distractor, and background masks. Similarly, we examine the overlap between the attention map of the distractor, and each of the mask labels of the distractor, target, and background. B. Ratio of attention on the same objects (target-target and distractor-distractor attention). The x-axis is normalized layer numbers, from early layers (left) to late layers (right). C. Ratio of attention on the different objects (target-distractor and distractor-target attention). D. Ratio of attention on the background (target-to-background and distractor-background attention)", "description": "This figure empirically studies whether an image token attends to tokens belonging to the same objects, different objects, or background.  It uses the Odd-One-Out (O3) dataset, which contains images with a group of similar objects (distractors) and a distinct singleton object (target). The figure shows attention maps for target and distractor tokens, analyzing the overlap between attention maps and mask labels for each category (target, distractor, background).  Subplots then show the ratios of attention on same objects, different objects, and background across different layers in various ViT models.", "section": "3 Grouping or contextualizing"}, {"figure_path": "dIktpSgK4F/figures/figures_14_1.jpg", "caption": "Figure 3: Cosine similarity between left and right singular vectors. The cosine similarity is computed per head and singular mode. The weighted average value of cosine similarity is computed with weights of corresponding singular values.", "description": "This figure shows the weighted average cosine similarity between the left and right singular vectors across different layers of various Vision Transformer (ViT) models.  The x-axis represents the layer number, and the y-axis represents the weighted average cosine similarity. Each line represents a different ViT model.  High cosine similarity indicates that tokens attend to similar tokens (perceptual grouping), while low cosine similarity suggests attention to dissimilar tokens (contextualization). The figure helps to visualize the trend of self-attention shifting from grouping in early layers to contextualization in later layers, although variations exist across different models and training objectives.", "section": "4.2 Similarity between left and right singular vectors"}, {"figure_path": "dIktpSgK4F/figures/figures_14_2.jpg", "caption": "Figure 2: Attention preference in the Odd-One-Out (O3) dataset [24]. A. An example from the O3 dataset. Two tokens are chosen to correspond to the target and distractor in the image. Attention maps using two tokens as queries are computed. We examine the overlap between the attention map of the target, and each of the mask labels of the target, distractor, and background masks. Similarly, we examine the overlap between the attention map of the distractor, and each of the mask labels of the distractor, target, and background. B. Ratio of attention on the same objects (target-target and distractor-distractor attention). The x-axis is normalized layer numbers, from early layers (left) to late layers (right). C. Ratio of attention on the different objects (target-distractor and distractor-target attention). D. Ratio of attention on the background (target-to-background and distractor-background attention)", "description": "This figure empirically studies whether an image token attends to tokens belonging to the same object, different objects, or background.  It uses the Odd-One-Out (O3) dataset, showing example images (A) and then plotting the ratio of attention on the same objects (B), different objects (C), and background (D) across different ViT layers for multiple model types.  The x-axis represents normalized layer number, progressing from early to late layers.", "section": "3 Grouping or contextualizing"}, {"figure_path": "dIktpSgK4F/figures/figures_15_1.jpg", "caption": "Figure 1: We propose a new way to study query-key interactions via the singular value decomposition of the query-key interaction matrix. Many of the modes (i.e. pairs of singular vectors corresponding to the query and the key respectively), are semantically interpretable. Two example modes are shown. Top row: ViT layer 8 head 7 mode 2. Bottom row: DINO layer 8 head 9 mode 2. The red channel indicates the projection value of embedding onto the left singular vector which corresponds to the query; the cyan channel indicates the projection value of embedding onto the right singular vector which corresponds to the key.", "description": "This figure shows two examples of semantically interpretable modes obtained from the singular value decomposition of the query-key interaction matrix. Each mode consists of a pair of singular vectors (one for the query and one for the key). The images depict the projection values of the embeddings onto these singular vectors. The red channel represents the query projection, while the cyan channel represents the key projection. The top row shows an example from a ViT model, while the bottom row shows an example from a DINO model, both illustrating how these singular vector pairs reveal interpretable semantic relationships between image features.", "section": "Abstract"}, {"figure_path": "dIktpSgK4F/figures/figures_16_1.jpg", "caption": "Figure 5: Visualization of a single image with multiple modes. We pick an example dog image from the ImageNet dataset and use the dino-vitb16 model. Top 6 modes (ordered by the contribution to the attention score) for example layers and heads are shown. See Supplementary Figure S17 for extended mode visualizations of this image.", "description": "This figure visualizes the top 6 singular modes (ranked by their contribution to the attention score) from different layers and heads of the dino-vitb16 model applied to a single dog image from ImageNet.  For each mode, the figure shows the query and key maps, highlighting the interaction between features represented by left and right singular vectors. This provides insights into how the model attends to features at various levels (low-level in early layers, higher-level in later layers).  Supplementary Figure S17 provides a more detailed visualization for this example.", "section": "5 Semanticity of singular modes"}, {"figure_path": "dIktpSgK4F/figures/figures_17_1.jpg", "caption": "Figure 2: Attention preference in the Odd-One-Out (O3) dataset [24]. A. An example from the O3 dataset. Two tokens are chosen to correspond to the target and distractor in the image. Attention maps using two tokens as queries are computed. We examine the overlap between the attention map of the target, and each of the mask labels of the target, distractor, and background masks. Similarly, we examine the overlap between the attention map of the distractor, and each of the mask labels of the distractor, target, and background. B. Ratio of attention on the same objects (target-target and distractor-distractor attention). The x-axis is normalized layer numbers, from early layers (left) to late layers (right). C. Ratio of attention on the different objects (target-distractor and distractor-target attention). D. Ratio of attention on the background (target-to-background and distractor-background attention)", "description": "This figure shows the attention preference of ViT models in the Odd-One-Out dataset. Subfigure A shows an example image from the dataset. Subfigures B, C, and D show the ratio of attention on the same objects, different objects, and background, respectively, across different layers of the ViT models.", "section": "3 Grouping or contextualizing"}, {"figure_path": "dIktpSgK4F/figures/figures_18_1.jpg", "caption": "Figure 4: Examples of optimal attention images of singular modes and query and key map in dino-vitb16. Optimal attention images are found from the Imagenet validation set that induce the largest attention score (sorted by the product of the maximum of query map and maximum of key map). The red and cyan (i.e. green and blue) channels are the projection values of embedding onto the left and right singular vectors of a singular mode. They correspond to query and key. The white area is where the query map and key map overlap. The name code we assign to singular modes specifies the layer, head, and mode numbers. For example, \"L1 H5 M1\" means layer 1, head 5, and mode 1. The value below indicates the cosine similarity between the left and right singular vectors.", "description": "This figure showcases examples of optimal attention images from the ImageNet validation set for singular modes in the dino-vitb16 model.  Each example highlights the interaction between query and key maps (represented by red and cyan channels) projected onto the left and right singular vectors of a specific singular mode. The white area shows overlap between the query and key maps.  The figure provides a visual representation of how these singular modes relate semantically to image features and the cosine similarity between the left and right singular vectors of each mode is included.", "section": "Semanticity of singular modes"}, {"figure_path": "dIktpSgK4F/figures/figures_19_1.jpg", "caption": "Figure 5: Visualization of a single image with multiple modes. We pick an example dog image from the ImageNet dataset and use the dino-vitb16 model. Top 6 modes (ordered by the contribution to the attention score) for example layers and heads are shown. See Supplementary Figure S17 for extended mode visualizations of this image.", "description": "This figure visualizes the results of applying singular value decomposition to a single dog image using the dino-vitb16 model. It displays the top 6 singular modes (ranked by contribution to the attention score) across different layers and heads within the model. Each mode's visualization helps understand the interactions between features within the model. Supplementary Figure S17 provides more visualizations.", "section": "5 Semanticity of singular modes"}, {"figure_path": "dIktpSgK4F/figures/figures_20_1.jpg", "caption": "Figure 5: Visualization of a single image with multiple modes. We pick an example dog image from the ImageNet dataset and use the dino-vitb16 model. Top 6 modes (ordered by the contribution to the attention score) for example layers and heads are shown. See Supplementary Figure S17 for extended mode visualizations of this image.", "description": "This figure visualizes how different singular modes of the dino-vitb16 model attend to different parts of an example dog image from the ImageNet dataset.  It shows the top 6 modes from various layers and heads, ordered by their contribution to the total attention score. Each mode highlights specific interactions between features, visualized through red and cyan channels representing query and key maps.  The supplementary material (Figure S17) offers more detailed visualizations.", "section": "5 Semanticity of singular modes"}, {"figure_path": "dIktpSgK4F/figures/figures_21_1.jpg", "caption": "Figure S11: Examples of semantic singular modes in deit-base-distilled-patch16-224 (part 1).", "description": "This figure shows examples of semantic singular modes in the deit-base-distilled-patch16-224 model.  Each row represents a specific singular mode, identified by layer, head, and mode number (e.g., L0 H0 M3). The leftmost column shows the cosine similarity between the left and right singular vectors for that mode. The remaining columns display the top 8 images from the ImageNet validation set that maximize the attention score for that singular mode. The red and cyan channels in these images indicate the projection of the image embeddings onto the left and right singular vectors respectively. This visualization helps to understand the semantic information captured by each singular mode, revealing how specific features in the query and key maps interact to produce attention.", "section": "Supplemental material"}, {"figure_path": "dIktpSgK4F/figures/figures_22_1.jpg", "caption": "Figure S12: Examples of semantic singular modes in deit-base-distilled-patch16-224 (part 2).", "description": "This figure displays examples of semantically interpretable singular modes from the DeiT-base-distilled-patch16-224 model.  Each row represents a singular mode, identified by its layer, head, and mode number. For each mode, the figure shows the top 8 images from the ImageNet validation set that maximize the attention score for that mode.  The red and cyan channels in each image visualization represent the projection values of the embedding onto the left and right singular vectors of the mode, respectively.  These visualizations help in understanding how different parts of the image are attended to in various layers of the model.", "section": "Supplemental material"}, {"figure_path": "dIktpSgK4F/figures/figures_23_1.jpg", "caption": "Figure S13: Examples of semantic singular modes in deit-base-distilled-patch16-224 (part 3).", "description": "This figure shows examples of semantic singular modes in the DeiT-base-distilled-patch16-224 model, specifically focusing on part 3 of the examples.  Each mode is represented visually, highlighting the interactions between query and key embeddings. The color channels (red and cyan) represent projections onto left and right singular vectors, showing how different parts of an image (or even different images) relate within the model's attention mechanism.", "section": "Supplemental material"}, {"figure_path": "dIktpSgK4F/figures/figures_24_1.jpg", "caption": "Figure 4: Examples of optimal attention images of singular modes and query and key map in dino-vitb16. Optimal attention images are found from the Imagenet validation set that induce the largest attention score (sorted by the product of the maximum of query map and maximum of key map). The red and cyan (i.e. green and blue) channels are the projection values of embedding onto the left and right singular vectors of a singular mode. They correspond to query and key. The white area is where the query map and key map overlap. The name code we assign to singular modes specifies the layer, head, and mode numbers. For example, \"L1 H5 M1\" means layer 1, head 5, and mode 1. The value below indicates the cosine similarity between the left and right singular vectors.", "description": "This figure shows examples of optimal attention images for several singular modes in the dino-vitb16 model.  Each mode is represented by a pair of singular vectors (left and right), visualized in red and cyan channels respectively in the images shown. These channels show the projection of image embeddings onto those vectors. The white areas highlight overlap between query and key maps.  The caption also provides a naming scheme to identify the layer, head, and mode of each example and its cosine similarity score.", "section": "Semanticity of singular modes"}, {"figure_path": "dIktpSgK4F/figures/figures_25_1.jpg", "caption": "Figure 4: Examples of optimal attention images of singular modes and query and key map in dino-vitb16. Optimal attention images are found from the Imagenet validation set that induce the largest attention score (sorted by the product of the maximum of query map and maximum of key map). The red and cyan (i.e. green and blue) channels are the projection values of embedding onto the left and right singular vectors of a singular mode. They correspond to query and key. The white area is where the query map and key map overlap. The name code we assign to singular modes specifies the layer, head, and mode numbers. For example, \"L1 H5 M1\" means layer 1, head 5, and mode 1. The value below indicates the cosine similarity between the left and right singular vectors.", "description": "This figure visualizes examples of optimal attention images from the ImageNet validation set for singular modes in the dino-vitb16 model.  Each example shows the query and key maps (red and cyan channels, respectively) corresponding to a specific singular mode (layer, head, and mode number indicated). The white area represents the overlap between the query and key maps.  The value below each example represents the cosine similarity between the left and right singular vectors of that mode, indicating the alignment between the query and key.", "section": "Semanticity of singular modes"}, {"figure_path": "dIktpSgK4F/figures/figures_26_1.jpg", "caption": "Figure S16: Examples of semantic singular modes in clip-vit-base-patch16 (part 3).", "description": "This figure shows examples of semantic singular modes in the clip-vit-base-patch16 model.  Each row represents a singular mode and contains multiple images. The red and cyan channels highlight the projection of the image embeddings onto the left and right singular vectors, respectively. The arrangement illustrates the interactions between query and key feature vectors, particularly highlighting the semantic relationships between image regions captured by each mode. The overall figure showcases various types of feature interactions present within the model, ranging from low-level features to higher-level object relationships, thereby demonstrating the varying semantic properties encoded within the different singular modes across various layers and heads.", "section": "Supplemental material"}, {"figure_path": "dIktpSgK4F/figures/figures_27_1.jpg", "caption": "Figure 5: Visualization of a single image with multiple modes. We pick an example dog image from the ImageNet dataset and use the dino-vitb16 model. Top 6 modes (ordered by the contribution to the attention score) for example layers and heads are shown. See Supplementary Figure S17 for extended mode visualizations of this image.", "description": "This figure visualizes the results of applying singular value decomposition to self-attention in a vision transformer model. It shows how different singular modes (combinations of left and right singular vectors) capture different aspects of an image. In this case, an example dog image is used and the top six modes are displayed to show how different parts of the image and its relation with other parts are represented in the model. Early layers capture low level properties, and deeper layers capture higher level semantics.  Supplementary Figure S17 offers more examples.", "section": "5 Semanticity of singular modes"}, {"figure_path": "dIktpSgK4F/figures/figures_28_1.jpg", "caption": "Figure 2: Attention preference in the Odd-One-Out (O3) dataset [24]. A. An example from the O3 dataset. Two tokens are chosen to correspond to the target and distractor in the image. Attention maps using two tokens as queries are computed. We examine the overlap between the attention map of the target, and each of the mask labels of the target, distractor, and background masks. Similarly, we examine the overlap between the attention map of the distractor, and each of the mask labels of the distractor, target, and background. B. Ratio of attention on the same objects (target-target and distractor-distractor attention). The x-axis is normalized layer numbers, from early layers (left) to late layers (right). C. Ratio of attention on the different objects (target-distractor and distractor-target attention). D. Ratio of attention on the background (target-to-background and distractor-background attention)", "description": "Figure 2 empirically studies whether an image token attends to tokens belonging to the same object, different objects, or the background. It uses the Odd-One-Out (O3) dataset, which contains images with a group of similar objects (distractors) and a distinct singleton object (target). The figure shows the attention preference in different ViT models by computing the attention score (overlap) between attention maps of target/distractor tokens and mask labels of target, distractor and background.  Subplots B, C, and D show the ratios of attention on the same objects, different objects, and background respectively, across different layers of the model.", "section": "3 Grouping or contextualizing"}, {"figure_path": "dIktpSgK4F/figures/figures_29_1.jpg", "caption": "Figure 1: We propose a new way to study query-key interactions via the singular value decomposition of the query-key interaction matrix. Many of the modes (i.e. pairs of singular vectors corresponding to the query and the key respectively), are semantically interpretable. Two example modes are shown. Top row: ViT layer 8 head 7 mode 2. Bottom row: DINO layer 8 head 9 mode 2. The red channel indicates the projection value of embedding onto the left singular vector which corresponds to the query; the cyan channel indicates the projection value of embedding onto the right singular vector which corresponds to the key.", "description": "This figure shows two examples of semantically interpretable modes obtained from the singular value decomposition of the query-key interaction matrix. Each mode is represented by a pair of singular vectors (one for query and one for key).  The images are color-coded to show the projection of image embeddings onto these singular vectors, revealing how different image features interact through the query-key mechanism in Vision Transformers (ViTs). The top row shows an example from a ViT model, and the bottom row shows an example from a DINO model, highlighting the versatility of the proposed method across different ViT architectures.", "section": "Abstract"}]