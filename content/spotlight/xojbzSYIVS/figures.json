[{"figure_path": "xojbzSYIVS/figures/figures_1_1.jpg", "caption": "Figure 1: The preliminary experiments of SASRec on Beauty dataset.", "description": "This figure shows the performance of SASRec (Self-Attentive Sequential Recommendation) on the Amazon Beauty dataset, highlighting the long-tail user and long-tail item challenges.  The left subplot (a) illustrates the long-tail user challenge, showing that the majority of users interact with a small number of items, leading to subpar performance for these users.  The right subplot (b) demonstrates the long-tail item challenge, where most items are seldom consumed, resulting in significantly better performance for popular items compared to less popular ones.  The histograms visually represent the distribution of users and items, indicating the scarcity of interactions for long-tail users and items.", "section": "1 Introduction"}, {"figure_path": "xojbzSYIVS/figures/figures_2_1.jpg", "caption": "Figure 2: The overview of the proposed LLM-ESR framework.", "description": "This figure illustrates the architecture of the LLM-ESR framework, which is designed to enhance sequential recommendation models by incorporating semantic information from large language models (LLMs).  It shows two main modules: Dual-view Modeling and Retrieval Augmented Self-Distillation.  The Dual-view Modeling module combines semantic embeddings from LLMs with collaborative signals from traditional SRS to address the long-tail item challenge.  The Retrieval Augmented Self-Distillation module leverages semantic user representations from LLMs to retrieve similar users and improve user preference representation for long-tail users.  The framework is model-agnostic, meaning it can be adapted to various sequential recommendation models.", "section": "3 LLM-ESR"}, {"figure_path": "xojbzSYIVS/figures/figures_8_1.jpg", "caption": "Figure 3: The hyper-parameter experiments on the weight of self-distillation loss \u03b1 and the number of retrieved similar users N. The results are based on the Yelp dataset with the SASRec model.", "description": "This figure shows the results of hyperparameter tuning for the LLM-ESR model using the Yelp dataset and SASRec as the base model.  The left two subfigures illustrate how the performance (HR@10 and NDCG@10) changes with different values of \u03b1 (weight of self-distillation loss). The right two subfigures show how performance changes with different numbers of retrieved similar users (N).  The results help determine optimal values for \u03b1 and N to maximize the model's performance.", "section": "4.4 Hyper-parameter Analysis"}, {"figure_path": "xojbzSYIVS/figures/figures_8_2.jpg", "caption": "Figure 4: The results of the proposed LLM-ESR and competing baselines in meticulous user and item groups. The results are based on the Beauty dataset with the SASRec model.", "description": "This figure shows the performance of LLM-ESR and other baseline models on the Beauty dataset using the SASRec model.  It is broken down into two sub-figures.  Subfigure (a) shows the performance across different user groups, categorized by the number of items each user interacted with (long-tail users having interacted with fewer items).  Subfigure (b) shows performance across different item groups, categorized by the number of times each item was interacted with (long-tail items having been interacted with fewer times).  The results illustrate LLM-ESR's superior performance, especially for the long-tail users and items, compared to baselines like SASRec, MELT, and LLMInit.", "section": "4.5 Group Analysis"}, {"figure_path": "xojbzSYIVS/figures/figures_18_1.jpg", "caption": "Figure 5: The visualization of the item embeddings by t-SNE. The dataset used in the experiments is Yelp. \u201cCITIES\u201d, \u201cMELT\u201d and \u201cLLM-ESR\u201d are all based on the SASRec backbone model. \u201cLLM\u201d represents the embeddings derived from LLM, which encodes the semantics of textual item prompts. Different colors of circles shown in the figures mean different popularity groups of the item.", "description": "This figure visualizes item embeddings using t-SNE, comparing SASRec, CITIES, MELT, LLM-ESR, and LLM embeddings. It shows how different models represent item semantics, particularly focusing on the impact of LLMs and the proposed LLM-ESR framework.  The visualization helps to understand how well different methods separate items based on popularity.", "section": "C.2 Visualization"}, {"figure_path": "xojbzSYIVS/figures/figures_19_1.jpg", "caption": "Figure 4: The results of the proposed LLM-ESR and competing baselines in meticulous user and item groups. The results are based on the Beauty dataset with the SASRec model.", "description": "This figure shows a detailed comparison of the performance of LLM-ESR against other baselines (SASRec, MELT, LLMInit) on the Beauty dataset.  The results are broken down by user group (based on interaction history length) and item group (based on item popularity). The graphs display HR@10 (Hit Rate at 10) for each group, allowing for a precise understanding of how well each model performs for both long-tail users/items and head users/items.  LLM-ESR consistently shows improved performance across all user and item groups.", "section": "4.5 Group Analysis"}, {"figure_path": "xojbzSYIVS/figures/figures_19_2.jpg", "caption": "Figure 4: The results of the proposed LLM-ESR and competing baselines in meticulous user and item groups. The results are based on the Beauty dataset with the SASRec model.", "description": "This figure shows a detailed performance comparison between the proposed LLM-ESR method and several baseline methods across different user and item groups.  The performance is measured by Hit Rate@10 (HR@10), and broken down by user group (categorized by number of interactions) and item group (categorized by popularity).  The results demonstrate the effectiveness of LLM-ESR, particularly for long-tail users and items, highlighting its ability to improve recommendation accuracy in scenarios with limited user-item interaction data. The dataset used is Amazon Beauty, and the underlying recommendation model is SASRec.", "section": "4.5 Group Analysis"}]