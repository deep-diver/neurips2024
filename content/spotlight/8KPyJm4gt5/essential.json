{"importance": "This paper is crucial for researchers in imitation learning.  It **challenges the prevailing assumptions** about the superiority of online methods over offline methods by providing **novel theoretical analysis** and **empirical validation**. The findings **open new avenues for improving offline algorithms** and **rethinking the design of online approaches**, significantly advancing the field.", "summary": "Offline imitation learning achieves surprisingly strong performance, matching online methods' efficiency under certain conditions, contradicting prior assumptions.", "takeaways": ["Offline imitation learning (behavior cloning with log-loss) can achieve horizon-independent sample complexity under certain conditions.", "The performance gap between offline and online imitation learning is smaller than previously thought, with offline methods performing comparably well under specific conditions.", "For stochastic policies, the sample complexity of offline imitation learning can depend on variance of the expert policy."], "tldr": "Imitation learning (IL) aims to train agents by mimicking expert behavior. Behavior cloning (BC), a simple offline IL method, suffers from a supposedly unfavorable quadratic dependence on the problem horizon (the length of the sequence of decisions).  This has motivated the development of various online IL algorithms which require the ability to repeatedly query the expert for guidance and achieve better (linear) scaling with horizon. This paper revisits this apparent gap between online and offline IL.  \nThe paper uses learning-theoretic analysis to show that the performance of offline BC, when using logarithmic loss, is often horizon-independent, even for deep neural networks as policy classes, when appropriate conditions on cumulative payoff range and supervised learning complexity are satisfied.  It then shows that for deterministic, stationary policies, offline BC achieves linear dependence on horizon under certain conditions, contradicting previous findings. This challenges the prevalent notion that online methods are fundamentally better than offline ones. The paper also shows that even when dealing with stochastic policies, online methods do not always improve over offline ones.", "affiliation": "Microsoft Research", "categories": {"main_category": "Machine Learning", "sub_category": "Reinforcement Learning"}, "podcast_path": "8KPyJm4gt5/podcast.wav"}