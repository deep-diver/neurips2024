[{"figure_path": "2TktDpGqNM/figures/figures_2_1.jpg", "caption": "Figure 1: The AUGRC metric based on Generalized Risk overcomes common flaws in current evaluation of Selective Classification (SC). a) Refined task definition for SC. Analogously to standard classification, we distinguish between holistic evaluation for method development and benchmarking using multi-threshold metrics versus evaluation of specific application scenarios at pre-determined working points. The current most prevalent multi-threshold metric in SC, AURC, is based on Selective Risk, a concept for working point evaluation that is not suitable for aggregation over rejection thresholds (red arrow). To fill this gap, we formulate the new concept of Generalized Risk and a corresponding metric, AUGRC (green arrow). b) We formalize our perspective on SC evaluation by identifying five key requirements for multi-threshold metrics and analyze how previous metrics fail to fulfill them. Abbreviations, CSF: Confidence Scoring Function, AU(G)RC: Area Under the (Generalized) Risk Coverage curve.", "description": "This figure illustrates the proposed AUGRC metric and compares it to existing metrics for selective classification.  Panel (a) shows the different evaluation approaches for selective classification: working-point evaluation versus multi-threshold evaluation.  It highlights how the existing Area Under the Risk Coverage (AURC) curve uses the \"Selective Risk\", which is not suitable for multi-threshold evaluation.  Instead, the authors propose using the \"Generalized Risk\" which leads to the new Area Under the Generalized Risk Coverage Curve (AUGRC) metric.  Panel (b) summarizes five key requirements for multi-threshold metrics in selective classification.  It then shows how the AUGRC metric satisfies these requirements, while existing metrics (AURC, e-AURC, AUROC, and Brier Score) do not.", "section": "2.3 Requirements for Selective Classification multi-threshold metrics"}, {"figure_path": "2TktDpGqNM/figures/figures_5_1.jpg", "caption": "Figure 2: The proposed AUGRC metric resolves shortcomings of AURC. All figures are based on rankings of predictions according to descending associated confidence scores induced by a CSF. All AURC, e-AURC, and AUGRC values are scaled by \u00d71000. a) shows the contribution of an individual failure case on the AURC and AUGRC metrics depending on its ranking position (for technical details, see Section A.1.1). While AUGRC reflects the intuitive behavior of weighing the failure cases proportional to their ranking position, the AURC puts excessive weight on high-confidence failures. b-d) Toy example of three CSFs ranking 20 predictions to show how AUGRC resolves the broken monotonicity requirement (R2) of AURC. Despite equal AUROCf and equal acc in CSF-1 and CSF-2, the AURC improves. And AURC even improves in CSF-3, which features lower AUROCf and lower acc compared to CSF-1. e-f) The corresponding risk-coverage curves reveal that the non-intuitive behavior of AURC is due to the excessive effect of the high-confidence failure of CSF-1 on the Selective Risk, which is resolved in the Generalized Risk.", "description": "This figure demonstrates the advantages of the proposed AUGRC metric over the existing AURC metric for evaluating selective classification systems.  It highlights how AUGRC addresses the shortcomings of AURC by providing a more intuitive and robust measure of system performance. The figure uses a combination of visualizations (graphs and diagrams) to illustrate the different weighting schemes of AUGRC and AURC for failure cases across various confidence levels, showing how AUGRC avoids the excessive weighting of high-confidence failures that is present in AURC. This results in a more consistent and meaningful evaluation, especially when assessing the general performance of the system across multiple thresholds, addressing the monotonicity problems of AURC.", "section": "3 Area under the Generalized Risk Coverage Curve"}, {"figure_path": "2TktDpGqNM/figures/figures_7_1.jpg", "caption": "Figure 3: Substantial differences in method rankings for AUGRC and AURC. On 5 out of\n6 datasets, the top-3 CSFs (out of 13 compared methods) change when employing the proposed\nAUGRC instead of AURC. This demonstrates the practical relevance of the AUGRC metric for Se-\nlective Classification evaluation. CSFs are color-coded and sorted from top (best) to bottom (worst)\nby average rank based on 500 bootstrap samples from the test dataset to ensure ranking stability.\nRanking changes are reflected in changes in the color sequence and highlighted by red arrows. We\nassess the stability of the method rankings for each metric individually using one-sided Wilcoxon\nsigned-rank tests based on the bootstrap samples at 5% significance level with adjustment for multi-\nple testing according to Holm. Adjacent to each ranking, we present the resulting significance maps\nfor the pairwise CSF comparisons. These maps can be interpreted as follows: At each grid position\n(x, y), filled entries indicate that metric values of CSF y are ranked significantly better than those\nfrom CSF x (across bootstrap samples), cross-marks indicate no significant superiority. An ideal\nranking exhibits only filled entries above the diagonal.", "description": "This figure compares the ranking of 13 confidence scoring functions (CSFs) for selective classification using two different metrics: AUROC and AUGRC.  It shows that the AUGRC metric leads to substantially different rankings of CSFs compared to the AURC, especially in the top-performing CSFs. The visualization helps to understand the impact of metric choice on CSF evaluation.", "section": "4.1 Comparing Method Rankings of AUGRC and AURC"}, {"figure_path": "2TktDpGqNM/figures/figures_8_1.jpg", "caption": "Figure 2: The proposed AUGRC metric resolves shortcomings of AURC. All figures are based on rankings of predictions according to descending associated confidence scores induced by a CSF. All AURC, e-AURC, and AUGRC values are scaled by \u00d71000. a) shows the contribution of an individual failure case on the AURC and AUGRC metrics depending on its ranking position (for technical details, see Section A.1.1). While AUGRC reflects the intuitive behavior of weighing the failure cases proportional to their ranking position, the AURC puts excessive weight on high-confidence failures. b-d) Toy example of three CSFs ranking 20 predictions to show how AUGRC resolves the broken monotonicity requirement (R2) of AURC. Despite equal AUROCf and equal acc in CSF-1 and CSF-2, the AURC improves. And AURC even improves in CSF-3, which features lower AUROCf and lower acc compared to CSF-1. e-f) The corresponding risk-coverage curves reveal that the non-intuitive behavior of AURC is due to the excessive effect of the high-confidence failure of CSF-1 on the Selective Risk, which is resolved in the Generalized Risk.", "description": "This figure demonstrates how the proposed AUGRC metric addresses the shortcomings of the existing AURC metric in evaluating selective classification systems. It highlights the differences in how AUGRC and AURC weigh the contribution of individual failure cases based on their confidence scores, showing that AUGRC provides a more intuitive and accurate assessment.  The figure uses a toy example and risk-coverage curves to illustrate how AUGRC overcomes the issues of monotonicity and ranking interpretability present in AURC.", "section": "3 Area under the Generalized Risk Coverage Curve"}, {"figure_path": "2TktDpGqNM/figures/figures_13_1.jpg", "caption": "Figure 5: Visualization of the relationship between AUGRC and AUROCf. (a) The Selective Risk curve can be transformed into the Generalized Risk curve via multiplication by the respective coverages. The resulting curve is monotonically increasing and bounded by the diagonal; decreasing Selective Risk corresponds to a plateau in Generalized Risk. The AUGRC corresponds to the AUGRC of an optimal CSF (shaded red) plus the re-scaled AUROC (shaded in green). The AUROCf corresponds to the fraction of the area (parallelogram) enclosed by the green dashed line that lies above the Generalized Risk curve. (b) AUGRC (color-coded) and its negative gradients (arrows) in the Accuracy-AUROCf space.", "description": "This figure visualizes the relationship between AUGRC and AUROCf. Panel (a) shows how the Selective Risk curve can be transformed into the Generalized Risk curve by multiplying by the respective coverages. The AUGRC is shown to be composed of the AUGRC of an optimal CSF (shaded red) plus the rescaled AUROC (shaded green). The AUROCf is visualized as the fraction of the area above the Generalized Risk curve. Panel (b) shows a heatmap of the AUGRC values (color-coded) and the negative gradients (arrows) plotted in the Accuracy-AUROCf space.", "section": "3 Area under the Generalized Risk Coverage Curve"}, {"figure_path": "2TktDpGqNM/figures/figures_15_1.jpg", "caption": "Figure 3: Substantial differences in method rankings for AUGRC and AURC. On 5 out of 6 datasets, the top-3 CSFs (out of 13 compared methods) change when employing the proposed AUGRC instead of AURC. This demonstrates the practical relevance of the AUGRC metric for Selective Classification evaluation. CSFs are color-coded and sorted from top (best) to bottom (worst) by average rank based on 500 bootstrap samples from the test dataset to ensure ranking stability. Ranking changes are reflected in changes in the color sequence and highlighted by red arrows. We assess the stability of the method rankings for each metric individually using one-sided Wilcoxon signed-rank tests based on the bootstrap samples at 5% significance level with adjustment for multiple testing according to Holm. Adjacent to each ranking, we present the resulting significance maps for the pairwise CSF comparisons. These maps can be interpreted as follows: At each grid position (x, y), filled entries indicate that metric values of CSF y are ranked significantly better than those from CSF x (across bootstrap samples), cross-marks indicate no significant superiority. An ideal ranking exhibits only filled entries above the diagonal.", "description": "This figure compares the ranking of 13 confidence scoring functions (CSFs) for selective classification using two different metrics: AUROC and AUGRC.  The results show that the AUGRC metric leads to substantially different rankings of the CSFs compared to AURC, highlighting the importance of AUGRC for a more accurate evaluation of selective classification systems. The figure uses color-coding and statistical significance testing to illustrate the differences and the stability of the rankings.", "section": "4.1 Comparing Method Rankings of AUGRC and AURC"}, {"figure_path": "2TktDpGqNM/figures/figures_16_1.jpg", "caption": "Figure 3: Substantial differences in method rankings for AUGRC and AURC. On 5 out of 6 datasets, the top-3 CSFs (out of 13 compared methods) change when employing the proposed AUGRC instead of AURC. This demonstrates the practical relevance of the AUGRC metric for Selective Classification evaluation. CSFs are color-coded and sorted from top (best) to bottom (worst) by average rank based on 500 bootstrap samples from the test dataset to ensure ranking stability. Ranking changes are reflected in changes in the color sequence and highlighted by red arrows. We assess the stability of the method rankings for each metric individually using one-sided Wilcoxon signed-rank tests based on the bootstrap samples at 5% significance level with adjustment for multiple testing according to Holm. Adjacent to each ranking, we present the resulting significance maps for the pairwise CSF comparisons. These maps can be interpreted as follows: At each grid position (x, y), filled entries indicate that metric values of CSF y are ranked significantly better than those from CSF x (across bootstrap samples), cross-marks indicate no significant superiority. An ideal ranking exhibits only filled entries above the diagonal.", "description": "Figure 3 shows that using AUGRC instead of AURC changes the ranking of the confidence scoring functions (CSFs) significantly.  It highlights the practical importance of AUGRC as a superior evaluation metric for selective classification. The figure visually compares the rankings of 13 CSFs across six datasets using both AUGRC and AURC metrics, demonstrating the considerable differences that arise when adopting the proposed AUGRC metric. The figure also includes statistical significance testing to validate the robustness of the observed ranking differences.", "section": "4.1 Comparing Method Rankings of AUGRC and AURC"}, {"figure_path": "2TktDpGqNM/figures/figures_17_1.jpg", "caption": "Figure 3: Substantial differences in method rankings for AUGRC and AURC. On 5 out of 6 datasets, the top-3 CSFs (out of 13 compared methods) change when employing the proposed AUGRC instead of AURC. This demonstrates the practical relevance of the AUGRC metric for Selective Classification evaluation. CSFs are color-coded and sorted from top (best) to bottom (worst) by average rank based on 500 bootstrap samples from the test dataset to ensure ranking stability. Ranking changes are reflected in changes in the color sequence and highlighted by red arrows. We assess the stability of the method rankings for each metric individually using one-sided Wilcoxon signed-rank tests based on the bootstrap samples at 5% significance level with adjustment for multiple testing according to Holm. Adjacent to each ranking, we present the resulting significance maps for the pairwise CSF comparisons. These maps can be interpreted as follows: At each grid position (x, y), filled entries indicate that metric values of CSF y are ranked significantly better than those from CSF x (across bootstrap samples), cross-marks indicate no significant superiority. An ideal ranking exhibits only filled entries above the diagonal.", "description": "This figure compares the ranking of 13 confidence scoring functions (CSFs) for selective classification using two different metrics: AURC and AUGRC.  The key finding is that AUGRC leads to substantially different rankings compared to AURC, highlighting the importance of using the AUGRC metric. The visualization uses color-coding and significance maps to show the stability and differences in rankings.", "section": "4.1 Comparing Method Rankings of AUGRC and AURC"}, {"figure_path": "2TktDpGqNM/figures/figures_18_1.jpg", "caption": "Figure 3: Substantial differences in method rankings for AUGRC and AURC. On 5 out of 6 datasets, the top-3 CSFs (out of 13 compared methods) change when employing the proposed AUGRC instead of AURC. This demonstrates the practical relevance of the AUGRC metric for Selective Classification evaluation. CSFs are color-coded and sorted from top (best) to bottom (worst) by average rank based on 500 bootstrap samples from the test dataset to ensure ranking stability. Ranking changes are reflected in changes in the color sequence and highlighted by red arrows. We assess the stability of the method rankings for each metric individually using one-sided Wilcoxon signed-rank tests based on the bootstrap samples at 5% significance level with adjustment for multiple testing according to Holm. Adjacent to each ranking, we present the resulting significance maps for the pairwise CSF comparisons. These maps can be interpreted as follows: At each grid position (x, y), filled entries indicate that metric values of CSF y are ranked significantly better than those from CSF x (across bootstrap samples), cross-marks indicate no significant superiority. An ideal ranking exhibits only filled entries above the diagonal.", "description": "This figure compares the ranking of 13 confidence scoring functions (CSFs) using two different metrics: AUROC and AUGRC.  It shows that the rankings significantly differ between the two metrics, especially for the top 3 performing CSFs.  The differences highlight the AUGRC's ability to provide a more reliable and practical evaluation of selective classification systems.", "section": "4.1 Comparing Method Rankings of AUGRC and AURC"}]