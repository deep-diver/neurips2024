[{"figure_path": "bhSfbjS6j9/figures/figures_1_1.jpg", "caption": "Figure 1: Many shapes (left) can explain the same image (middle) under different lighting, including flattened and tilted versions and convex/concave flips. The concave/convex flip in this example is also perceived by humans, often aided by rotating the image clockwise by 90 degrees. Previous methods for inferring either surface normals (SIRFS [4], Derender3D [53], Wonder3D [33]) or depth (Marigold [27], Depth Anything [56]) produce a single shape estimate or a unimodal distribution. Ours produces a multimodal distribution that matches the perceived flip. (Image adapted from [31]).", "description": "This figure demonstrates the multistable nature of shape from shading.  Multiple 3D shapes can produce the same 2D shading image, depending on the lighting conditions.  The figure shows an example of a shape that can be interpreted as either convex or concave, illustrating the ambiguity humans experience.  It also compares the results of the proposed model with other state-of-the-art methods that generally produce a single shape estimate (unimodal distribution), highlighting the advantage of the proposed model in capturing the multimodal distribution of shape interpretations.", "section": "1 Introduction"}, {"figure_path": "bhSfbjS6j9/figures/figures_2_1.jpg", "caption": "Figure 2: Training patches are cropped from synthetic images of ordinary diffuse objects, and during training, a small diffusion model learns to denoise the normal field x for patch u from a random sample x conditioned on the patch intensities c\u201c. During inference, the model is applied in parallel to non-overlapping patches, with guidance from inter-patch shape-consistency constraints to minimize the curvature smoothness loss Ls and integrability loss L1.", "description": "This figure shows a schematic of the proposed multiscale patch diffusion model. (a) Training: The model is trained using cropped patches from synthetic images of everyday objects and their corresponding normal fields.  A small diffusion model learns to denoise the normal field given the patch's intensity and a random sample. (b) Inference: During inference, the model is applied to non-overlapping patches in parallel. Inter-patch consistency constraints are used to guide the diffusion process to minimize curvature smoothness and integrability losses. This multi-scale approach allows the model to capture global ambiguities in the shading image.", "section": "3 Multiscale Patch Diffusion with Guidance"}, {"figure_path": "bhSfbjS6j9/figures/figures_4_1.jpg", "caption": "Figure 3: Top: Illustration of multiscale sampling across two scales in a fine-coarse-fine \u201cV-cycle\u201d, with conditional images omitted for simplicity. In practice, our V-cycle covers more than two scales. Left: The N&R subroutine injects noise to an earlier timestep 0 < t < T and then resumes guided sampling (Fig. 2b) at that scale. Right: Optional intermediate guidance comes from lighting consistency (LCG), where each patch nominates a dominant light direction and then some patches flip in response to those nominations. Pseudocode is in the appendix.", "description": "This figure illustrates the multiscale sampling process used in the model.  A V-cycle approach is used, iteratively refining the predictions at different scales (fine-to-coarse-to-fine). The \"Inject Noise & Resume Sampling (N&R)\" step injects noise into an earlier timestep and then resumes the sampling process at that scale.  Additionally, the \"Lighting Consistency Guidance (LCG)\" step uses a global constraint on lighting to help coordinate the predictions across patches.", "section": "3 Multiscale Patch Diffusion with Guidance"}, {"figure_path": "bhSfbjS6j9/figures/figures_6_1.jpg", "caption": "Figure 4: Ablations, and comparison to human subjects using image and psychophysics data from [37]. Left: Ablations demonstrate the importance of each component. Right: Depth cross-sections extracted from four (integrated) samples from the convex mode of our full model exhibit relief-like variations similar to those reported across human subjects. (The dashed line is the depth that was used to render the input image.)", "description": "This figure shows ablation studies and a comparison to human perception. The left part demonstrates the importance of each component (random sampling, single-scale spatial consistency, multi-scale spatial consistency, and lighting consistency) in achieving multistable perception. The right part compares depth cross-sections from the model's convex mode with those reported from human subjects, showing a qualitative similarity.", "section": "4 Experimental Results"}, {"figure_path": "bhSfbjS6j9/figures/figures_6_2.jpg", "caption": "Figure 5: Normals produced by our model for various synthetic test surfaces rendered with directional light sources. For depth maps, brighter is closer. \u201cReference\u201d depicts the shapes\u2014each with a convex/concave counterpart that were used to render the input images. We find that our reconstructions are more accurate and diverse than other methods.", "description": "This figure compares the performance of the proposed model with several state-of-the-art methods on various synthetic test surfaces with ambiguous shapes (convex/concave). The results demonstrate that the proposed model produces more accurate and diverse reconstructions compared to existing methods, highlighting its ability to capture the multistability of shape perception.", "section": "4.2 Ambiguous Images"}, {"figure_path": "bhSfbjS6j9/figures/figures_7_1.jpg", "caption": "Figure 6: t-SNE visualizations of normal field samples produced by our model and by Wonder3D. Plots depict 100 samples from each model, along with the two mathematical possibilities (under directional light) and the normals of a trivial frontal plane. For each model we report the Wasserstein distance (smaller is better) between its samples and the reference distribution, which is uniform over two possibilities. Our model is more accurate and in all cases covers both possibilities.", "description": "This figure compares the performance of the proposed model and Wonder3D in generating normal field samples for ambiguous shapes.  t-SNE plots visualize the distribution of 100 samples from each model, alongside the two theoretically possible normal distributions (under directional lighting) and a flat surface.  The Wasserstein distance, a measure of the difference between the sample distributions and the reference distribution, is reported for each model. The results show that the proposed model is more accurate and encompasses both possible interpretations, while Wonder3D's samples are less diverse and less accurate.", "section": "4 Experimental Results"}, {"figure_path": "bhSfbjS6j9/figures/figures_7_2.jpg", "caption": "Figure 7: Sampled reconstructions for real images. (a) For the \u2018plates\u2019 image from [57], regions such as the indicated box can exhibit independent convex/concave flips when lighting consistency is not used; but when lighting consistency is enforced, only two global modes emerge. (b) Sampled reconstructions for some multistable images we captured with illumination from a point or area light. (Rotate them by 180\u00b0 to enhance the alternative experience.) Note that half of the object in the first row was painted matte, and its other half was left glossy. Despite being trained entirely on synthetic data under idealized lighting, the model exhibits some generalization by producing plausible multistable outputs for these captured scenes.", "description": "This figure shows the results of applying the model to real-world ambiguous images.  (a) demonstrates the impact of enforcing lighting consistency on the multistability of the 'plates' image. (b) showcases the model's generalization ability to various lighting conditions and surface properties (matte vs. glossy) by producing plausible multistable interpretations for real-world images.", "section": "4.2 Ambiguous Images"}, {"figure_path": "bhSfbjS6j9/figures/figures_8_1.jpg", "caption": "Figure 8: Left: Reconstructed normals and integrated depth for an image taken from the web. Right: Reconstructions for images in the SfS dataset of [54] with median angular error from the ground truth normals (lower is better). Our model's accuracy is on par with the best existing methods. Additional quantitative results are in the appendix.", "description": "The figure shows qualitative and quantitative results of the proposed model on real images from the web and from a shape from shading dataset [54]. The left part shows reconstructed normals and depth maps for a real image with comparisons against ground truth. The right part shows the median angular error between model predictions and ground truth normals for multiple images and compares this error against existing methods.", "section": "4 Experimental Results"}, {"figure_path": "bhSfbjS6j9/figures/figures_13_1.jpg", "caption": "Figure 4: Ablations, and comparison to human subjects using image and psychophysics data from [37]. Left: Ablations demonstrate the importance of each component. Right: Depth cross-sections extracted from four (integrated) samples from the convex mode of our full model exhibit relief-like variations similar to those reported across human subjects. (The dashed line is the depth that was used to render the input image.)", "description": "This figure presents ablation studies and a comparison of the model's output to human perception. The left side shows the effect of removing individual components of the model (multi-scale sampling, spatial consistency, lighting consistency). The right side shows depth cross-sections from the model's output compared to cross-sections from human perception studies, highlighting the similarity of relief-like variations.", "section": "4 Experimental Results"}, {"figure_path": "bhSfbjS6j9/figures/figures_14_1.jpg", "caption": "Figure 5: Normals produced by our model for various synthetic test surfaces rendered with directional light sources. For depth maps, brighter is closer. \u201cReference\u201d depicts the shapes\u2014each with a convex/concave counterpart that were used to render the input images. We find that our reconstructions are more accurate and diverse than other methods.", "description": "This figure compares the performance of the proposed model against other state-of-the-art methods on synthetic test images. Each row shows a different test image and its corresponding depth map generated by each method.  The \"Reference\" column shows the ground truth shapes used to render the input images.  The results highlight the superior accuracy and diversity of the proposed model in capturing various reconstructions compared to existing methods.", "section": "4.2 Ambiguous Images"}, {"figure_path": "bhSfbjS6j9/figures/figures_15_1.jpg", "caption": "Figure 10: Output normal maps and their t-SNE visualizations when our model is applied to a 16 \u00d7 16 image of an exactly quadratic surface under directional lighting. When our model is trained using images of spline surfaces (b), the outputs cluster around the four mathematical interpretations from [54] (convex, concave, and two saddles). When it is trained using images of everyday objects (a), the outputs exhibit more diversity. In both scenarios, samples are drawn independently without guidance.", "description": "This figure compares the results of applying the model to a small image of a perfectly quadratic surface, once trained on images of everyday objects and once trained on images of cubic spline surfaces.  When trained on the spline surfaces, the results tightly cluster around the four theoretically possible interpretations of such an image (convex, concave, and two saddle shapes). When trained on everyday objects, the model produces more diverse results, showing the influence of the training data on the model's ability to interpret ambiguous shading.", "section": "A.4 Relation to the four-way convex/concave/saddle ambiguity"}, {"figure_path": "bhSfbjS6j9/figures/figures_16_1.jpg", "caption": "Figure 11: Inferred normal map samples from our model on ridge images taken from [31] and 'cobble' test image from [37].", "description": "This figure compares the results of different shape-from-shading models on two ambiguous images. The first row shows images from Kunsberg and Zucker (2021) that can be interpreted as either convex or concave shapes.  The second row displays an image from Nartker et al. (2017) of small bumps, which also has multiple possible interpretations. For each image, the figure shows the normal maps produced by the authors' model (Ours), Wonder3D, SIRFS, Derender3D, Marigold, and Depth Anything.  The different models produce varied interpretations, showcasing the ambiguity inherent in shape-from-shading and demonstrating the authors' model's ability to capture multiple interpretations.", "section": "4.2 Ambiguous Images"}, {"figure_path": "bhSfbjS6j9/figures/figures_16_2.jpg", "caption": "Figure 5: Normals produced by our model for various synthetic test surfaces rendered with directional light sources. For depth maps, brighter is closer. \u201cReference\u201d depicts the shapes\u2014each with a convex/concave counterpart that were used to render the input images. We find that our reconstructions are more accurate and diverse than other methods.", "description": "This figure compares the results of different shape reconstruction models on various synthetic images with directional lighting. Each image has an ambiguous interpretation (convex or concave). The \"Reference\" column shows the ground truth shapes used to render the images, illustrating the ambiguity. The \"Ours\" column showcases the model's reconstructions, highlighting its ability to generate multiple interpretations, accurately representing the multimodal nature of human perception in ambiguous situations. In contrast, other models produce less accurate and less diverse output, often failing to capture the full range of possibilities.", "section": "4.2 Ambiguous Images"}, {"figure_path": "bhSfbjS6j9/figures/figures_17_1.jpg", "caption": "Figure 10: Output normal maps and their t-SNE visualizations when our model is applied to a 16 \u00d7 16 image of an exactly quadratic surface under directional lighting. When our model is trained using images of spline surfaces (b), the outputs cluster around the four mathematical interpretations from [54] (convex, concave, and two saddles). When it is trained using images of everyday objects (a), the outputs exhibit more diversity. In both scenarios, samples are drawn independently without guidance.", "description": "This figure compares the results of applying the proposed model to a 16x16 image of an exactly quadratic surface under directional lighting.  Two training scenarios are shown: one using images of spline surfaces, and another using images of everyday objects. The t-SNE visualizations show that when trained on spline surfaces, the model's outputs cluster around four distinct mathematical interpretations (convex, concave, and two saddle shapes), aligning with theoretical predictions.  However, when trained on everyday objects, the outputs exhibit greater diversity, suggesting that the model's learned representation is more nuanced and less constrained by this specific mathematical case.", "section": "A.4 Relation to the four-way convex/concave/saddle ambiguity"}, {"figure_path": "bhSfbjS6j9/figures/figures_18_1.jpg", "caption": "Figure 14: Effect of lighting bias during training. Model A is trained using synthetic images with lighting directions that are uniformly sampled within a 60\u00b0 cone around the view direction. Model B is trained using the same shapes but with lighting that is distributed non-uniformly within the cone, where 80% of images are lit from above. After training, we draw 50 samples from each model for a test image, using the same schedule and guidance hyperparameters. We project the results using t-SNE (dots are randomly colored for visual clarity) and show representative samples. Model A produces a balanced distribution across convex and concave explanations, whereas Model B produces concave predictions more often. (To humans, the test image usually appears concave, and it usually appears convex when rotated. These are both physically consistent with lighting from above.)", "description": "This figure shows the ablation study on lighting distribution in the training set. Two models are trained with different lighting distributions: Model A with uniform lighting and Model B with 80% of images lit from above.  The t-SNE plots and samples demonstrate how the lighting bias in the training data affects the model's ability to generate both convex and concave interpretations of ambiguous shapes. Model A produces a more balanced distribution, while Model B shows a bias towards concave interpretations, highlighting the influence of training data on model behavior.", "section": "A.9 Ablation on lighting distribution in training set"}]