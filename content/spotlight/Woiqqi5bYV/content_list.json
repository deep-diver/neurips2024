[{"type": "text", "text": "Interpret Your Decision: Logical Reasoning Regularization for Generalization in Visual Classification ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zhaorui Tan1,2, Xi Yang1\u2217, Qiufeng Wang1, Anh Nguyen2, Kaizhu Huang3\u2217 1 Xi\u2019an-Jiaotong Liverpool University 2 University of Liverpool 3Duke Kunshan University ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Vision models excel in image classification but struggle to generalize to unseen data, such as classifying images from unseen domains or discovering novel categories. In this paper, we explore the relationship between logical reasoning and deep learning generalization in visual classification. A logical regularization termed L-Reg is derived which bridges a logical analysis framework to image classification. Our work reveals that L-Reg reduces the complexity of the model in terms of the feature distribution and classifier weights. Specifically, we unveil the interpretability brought by L-Reg, as it enables the model to extract the salient features, such as faces to persons, for classification. Theoretical analysis and experiments demonstrate that L-Reg enhances generalization across various scenarios, including multi-domain generalization and generalized category discovery. In complex real-world scenarios where images span unknown classes and unseen domains, L-Reg consistently improves generalization, highlighting its practical efficacy. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "One critical challenge in visual classification models is their ability to generalize effectively to unseen samples or unknown classes. For instance, a model trained on real images of various animals should ideally classify animal sketches accurately (referred to as multi-domain generalization classification [20, 35, 34, 23, 25, 37, 50]) or discover novel categories not present in the training set (referred to as generalized category discovery [54, 16]). These problems are prevalent in realworld scenarios, where training data", "page_idx": 0}, {"type": "image", "img_path": "Woiqqi5bYV/tmp/917c85ff48a210d5896ce662526cd4bf501813bcd03d16361401fa2aa611d382.jpg", "img_caption": [], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "With L-Reg ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Figure 1: GradCAM [45] visualizations for the unknown class \u2018person\u2019 across seen and unseen domains of the GMDG baseline with $L_{2}$ regularization that is trained without and with L-Reg, respectively. Both experiments share the same hyper-parameters, except the latter uses the L-Reg. ", "page_idx": 0}, {"type": "text", "text": "target pairs are usually insufficient, and labeling is time-consuming so that not every data is paired with a label. Meanwhile, test data is likely to contain shifts in both data and targets, making it essential to propose methods that generalize to border scenarios. ", "page_idx": 0}, {"type": "text", "text": "Regularization terms, such as $L_{2}$ regularization leading to weight decay, are commonly employed during training to improve a model\u2019s generalization capabilities. However, the $L_{2}$ regularization is parametric-based rather than sample-based, which may lead to ambiguous interpretability [58]. As illustrated in Fig. 1, the model trained solely with $L_{2}$ regularization exhibits low interpretability. Other regularization terms [57, 58, 59] attempt to improve the interpretability of deep learning models for sequential signals rather than vision, whereas [39] proposes a regularization term to enhance interpretability for robustness in visual classification models rather than generalization. Drawing inspiration from logical reasoning has shown promise for better generalization and interpretability in various tasks. Current work unveils the effectiveness of logical reasoning in generalization tasks, such as boosting performance in length generalization [1, 3, 2, 60] and abstract symbol relational reasoning [10, 36] (e.g., mathematical solving and psychological tests). Several efforts, such as [6], explore the explicit entropy-based logical explanations of neural networks for image classification, confirming the presence and interpretability of logical reasoning within visual tasks. Yet, there are limited studies tackling the generalization of visual classification tasks through the lens of logical reasoning. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "This paper studies two pivotal questions corresponding to the above: 1) How does logical reasoning relate to visual tasks such as image classification? 2) How can we derive a logical reasoning-based regularization term to benefti generalization? To achieve these, we correlate the image classification procedure in computer vision with the framework of logic studies [4], positing that training an image classifier involves learning a good general logical relationship between images and labels via an encoder. This good general logic is attained when the semantics generated by the encoder and classifier can be combined to form atomic formulas. Our exploration leads to the introduction of a sample-based Logical regularization term named L-Reg. We reveal that L-Reg efficiently reduces the complexity of the model from two aspects: 1) L-Reg leads to a balanced feature distribution in the semantic space; 2) L-Reg reduces the number of weights with extreme values in the classifier. ", "page_idx": 1}, {"type": "text", "text": "Intuitively, the complexity reduction achieved by L-Reg stems from its ability to fliter out redundant features or semantics, focusing instead on the minimal yet sufficient semantics for classification - defined as semantic support in Definition 3.2, where the interpretability also emerges. This filtering feature beneftis the generalization when there is a domain shift in data where the domain-dependent features are ignored for classification. Moreover, it further promotes generalization when unlabeled data from the unknown classes is present. If such data lacks the semantic support associated with known classes, it is then classified as belonging to an unknown class, and its corresponding semantic supports are extracted. These capabilities equip L-Reg with explicit interpretability. As Fig. 1 shows, with L-Reg, the model can identify the unknown class \u2018person\u2019, and pinpoint faces which are the crucial features for classifying this category. In contrast, the model trained solely with $L_{2}$ (without L-Reg) focuses on the ambiguous features for classification. ", "page_idx": 1}, {"type": "text", "text": "Rigorous theoretical analysis and experimental results validate that L-Reg yields better generalization across diverse scenarios. Specifically, L-Reg facilitates better performance under the aforementioned multi-domain generalization and generalized category discovery tasks, whose settings are presented in Fig. 2 (a)(b). Furthermore, to evaluate L-Reg\u2019s robustness, we introduce a more complex real-world scenario, as shown in Fig. 2 (c), where unlabeled images may not only belong to unknown classes, but also originate from unseen domains. Even in this challenging context, L-Reg is still able to consistently demonstrate notable improvements in generalization, underscoring its practical utility and effectiveness. Our code is available at https://github.com/zhaorui-tan/L-Reg_NeurIPS24. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries and generalization settings for visual classification ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Consider paired $\\mathbf{\\Psi}(X,Y)~\\sim~(\\mathcal{X},\\mathcal{Y})$ , $(X_{s},Y_{s})\\,\\,\\sim$ $(\\mathcal{X}_{s},\\mathcal{Y}_{s})$ , and $(X_{u},Y_{u})\\sim(\\mathcal{X}_{u},\\mathcal{Y}_{u})$ denote all sets of inputs and labels, seen paired subsets of $(X,Y)$ , and unseen paired subsets of $(X,Y)$ , respectively. Note that $X_{u},Y_{u}$ may be accessible for the model separately, but their pairing relationships are not accessible. Let $D$ denote the possible domains, with $D_{s},D_{u}\\subset D$ representing the seen and unseen domains. In classification tasks, an encoding function $g(x)\\rightarrow Z\\in\\mathbb{R}^{M}$ is commonly introduced to map $X$ into the latent feature set $Z$ , where each latent feature has $M$ dimensions. A predictor $\\boldsymbol{h}(\\boldsymbol{Z})\\,\\rightarrow\\,\\hat{\\boldsymbol{Y}}\\,\\in\\,\\mathbb{R}^{K}$ ", "page_idx": 1}, {"type": "image", "img_path": "Woiqqi5bYV/tmp/d47426ad208bd54c18a77f3cdf4f10f17ccc982c74b784540ba1f283dba0840c.jpg", "img_caption": [], "img_footnote": ["maps $Z$ to predictions $\\hat{Y}$ , where $K$ denotes "], "page_idx": 1}, {"type": "text", "text": "Figure 2: Diagrams of different generalization settings in visual classification tasks. ", "page_idx": 1}, {"type": "image", "img_path": "Woiqqi5bYV/tmp/7b09761e4bcc692f588764162739997fb16c5c732bb9c9d1037918743bdcec9d.jpg", "img_caption": ["Figure 3: Visualizations of classifiers\u2019 weights form models trained using GMDG on PACS dataset without and with L-Reg under $\\scriptstyle\\mathrm{mDG+GCD}$ setting, respectively. Both experiments share the same hyper-parameters using Regnety-16g backbone, except the latter uses additional L-Reg. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "the number of classes and the dimensions of predictions. $P(\\cdot)$ and $H(\\cdot)$ symbolize probability and entropy, respectively. This paper discusses two typical cases for generalization in image classification tasks: (1) Data-shift generalization: $X_{s}$ and $X_{u}$ have distribution shifts, such as multi-domain generalization (mDG); and (2) Target-shift generalization: $Y_{s}$ and $Y_{u}$ have distribution shifts, which stands for tasks like generalized category discovery (GCD). We additionally explore a challenging scenario called All-shift generalization: both $X_{s}$ and $X_{u}$ , $Y_{s}$ and $Y_{u}$ have distribution shifts, which is a combination of mDG and GCD tasks $\\mathrm{(mDG+GCD)}$ . The following lists the detailed settings for generalization. Please refer to Fig. 2 for brief diagrams. ", "page_idx": 2}, {"type": "text", "text": "Data-shift generalization: Problem setting for mDG. Illustrated in Fig. 2 (a), mDG [9] intends to generalize well to unseen domains having the objective of min $\\imath\\,H(X_{s},Y_{s}\\mid D_{s})$ and expecting the model to be generalized to $X_{u}$ when predicting $Y_{u}$ from the unseen domain $D_{u}$ . In such cases, $Y_{u}$ is fully accessible to the model since $Y_{s}$ and $Y_{u}$ share the same domain: $y_{s}=y_{u}$ but there are shifts in $X$ where $\\mathcal{X}_{s}\\neq\\mathcal{X}_{u}$ . ", "page_idx": 2}, {"type": "text", "text": "Target-shift generalization: Problem setting for GCD. GCD [54] (Fig. 2 (b)) aims to discover possible unseen labels among unlabeled datasets $X_{u}$ . The challenge is that the samples in $X_{u}$ may belong to known classes or unknown classes: $\\mathcal{V}_{s}\\neq\\mathcal{V}_{u}$ and probably $\\mathcal{V}_{s}\\cap\\mathcal{V}_{u}\\neq\\emptyset$ . The model should be able to distinguish the samples from the known classes and cluster the samples for unknown classes simultaneously. Note that $X_{u}$ is used for model training, but the relationship between $X_{u}$ and $Y_{u}$ is unseen for the model. In summary, shifts exist between $Y_{s}$ and $Y_{u}$ but not between $X_{s}$ and $X_{u}$ . ", "page_idx": 2}, {"type": "text", "text": "All-shift generalization: Problem setting for $\\mathbf{mDG}+\\mathbf{GCD}.$ . To explore the generalization problem further, we introduce a setting that is the combination of mDG and GCD as shown in Fig. 2 (c). Specifically, the model is trained on the labeled pairs $(X_{s},Y_{s})$ and unlabeled set $X_{u}$ from the seen domains $D_{s}$ ; $X_{u}$ may belong to known and unknown classes. Furthermore, the model is tested on $X_{u}$ from the unseen domain $D_{u}$ , where $X_{u}$ may also come from the known and unknown classes. In this setting, the model is expected to 1) classify samples to the seen classes and discover the unseen classes among unlabeled samples from seen domains and 2) generalize this ability to the samples from the unseen domain. In this scenario, $X_{s}$ and $X_{u}$ have shifts, and so do $Y_{s}$ and $Y_{u}$ . ", "page_idx": 2}, {"type": "text", "text": "For all aforementioned generalization settings, the objective can be summarized as minimizing the generalization loss: ", "page_idx": 2}, {"type": "text", "text": "Definition 2.1 (Generalization loss). Let the target model $f^{*}:f^{*}(X,Y):X\\to Y$ , can generalize across both seen and unseen sets $X,Y$ . Denote its trainable $f$ , which is only trained on the seen sets. The generalization loss for the unseen sets is defined as: ", "page_idx": 2}, {"type": "equation", "text": "$$\nG L(f,f^{*},(X_{u},Y_{u}))=\\mathbb{E}_{(x,y)\\in(X_{u},Y_{u})}||f(x,y)-f^{*}(x,y)||_{2}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "3 Logical regularization for generalization in image classification ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Under the problem settings defined in Section 2, we introduce Logic regularization (L-Reg) targeting the objective: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{h,g}\\mathbb{E}_{z_{i}\\in z,z\\in Z}[H(\\hat{Y}|z_{i},D)]\\!-\\!\\mathbb{E}_{z\\in Z}[H(\\hat{Y}|Z,D)],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\hat{Y}\\,\\in\\,\\mathbb{R}^{K}\\,=\\,h\\circ g(X)$ is the prediction set. The corresponding Logic regularization loss (L-Reg) is defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nL_{L-R e g}\\!\\!=\\!\\!\\frac{1}{M}\\sum_{i=1}^{M}\\left[\\sum_{j=1}^{K}[\\sigma(\\hat{Y}_{j}^{T}Z_{i})\\log\\sigma(\\hat{Y}_{j}^{T}Z_{i})]-[\\frac{1}{K}\\sum_{j=1}^{K}\\sigma(\\hat{Y}_{j}^{T}Z_{i})\\log(\\frac{1}{K}\\sum_{j=1}^{K}\\sigma(\\hat{Y}_{j}^{T}Z_{i})]\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\sigma(\\hat{Y}_{j}^{T}Z_{i})$ denotes the value at the $i,j$ position of $s o f t m a x(\\hat{Y}^{T}Z)$ and the soft-max function is applied at the last dimension. By incorporating other existing methods\u2019 losses denoted by $L_{m a i n}$ , the overall loss is formulated as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nL_{a l l}=L_{m a i n}+\\alpha L_{L-R e g},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "with a weight $\\alpha$ applied to balance two losses. As depicted in Fig. 1, L-Reg plays a pivotal role in extracting crucial features for image classification, thus enhancing generalization capabilities. This beneficial outcome can be attributed to two primary factors: ", "page_idx": 3}, {"type": "text", "text": "Reducing classifier complexity: L-Reg streamlines the complexity of the classifier itself, as depicted in Fig. 3 (a). Notably, the heat map of the model with L-Reg displays fewer extremely valued weights, evidenced by the diminished presence of intense blue and red colors. This reduction implies that the classifier focuses on leveraging semantically rich and relevant features for decision-making (classification), sidelining the less relevant ones. Additionally, Fig. 3 (b) reveals a reduction in the number of semantic features used to classify each class. ", "page_idx": 3}, {"type": "text", "text": "Balancing feature complexity: L-Reg results in a more balanced distribution of features compared to the baseline, as illustrated in Fig. 4. This balanced distribution suggests the elimination of certain extracted semantics characterized by dominant frequencies across all samples. Semantics that occur frequently across samples often lack decisiveness for classification. Hence, reducing their prominence contributes to more expressive feature space and less complex feature distributions. Coupled with the reduced classifier complexity, a simplified classifier achieved through L-Reg facilitates improved generalization across various settings. Specifically, the top row also indicates the distance between the feature distributions of the known and unknown classes, which is enlarged; thus, they are more dividable, leading to classification improvements. ", "page_idx": 3}, {"type": "image", "img_path": "Woiqqi5bYV/tmp/cebfc14db7bc632850b2b5656eb6d5991ac28107baaf3936e24034d0641dd59e.jpg", "img_caption": ["Figure 4: Visualizations of latent features form models trained using GMDG on PACS dataset without and with LReg under $\\scriptstyle\\mathrm{mDD}+\\mathrm{GCD}$ setting using RegNetY-16G backbone, respectively. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "We present a logical-based theoretical analysis in Section 3.1 and provide the derivation details of L-Reg in Section 3.2. In addition, we discuss the efficacy of L-Reg under various generalization settings in Section 4. Furthermore, L-Reg serves as a plug-and-play loss function that is compatible with most existing frameworks. We conduct experiments applying L-Reg to various established approaches across different generalization settings, as outlined in Section 5. ", "page_idx": 3}, {"type": "text", "text": "3.1 Logical framework for visual classification ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "This part provides the connections between logical reasoning and visual classification tasks. We would like to remind readers of the framework for studying logics and link it with our practical scenarios. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.1. Following [4], a logic $\\mathcal{L}$ is defined as a five-tuple in the form: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\langle F_{\\mathcal{L}},M_{\\mathcal{L}},\\vert\\vert\\boldsymbol{z}_{\\mathcal{L}},m n g_{\\mathcal{L}},\\vert\\boldsymbol{\\cdot}_{\\mathcal{L}},\\rangle\\,,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where 1) $F_{\\mathcal{L}}$ denotes the set of formulas formed by images and labels $(X,Y)$ ; 2) $M_{\\mathcal{L}}$ represents different domains $D$ of $X;3)\\vDash_{\\mathcal{L}}$ is a binary relation relating the truth of whether the formulas are true or false, which has $\\begin{array}{r}{\\left|=\\!c\\!\\subseteq M\\!_{\\mathcal{L}}\\times F\\!_{\\mathcal{L}}\\right\\rangle}\\end{array}$ ; 4) $m n g_{\\mathcal{L}}:F_{\\mathcal{L}}\\times M_{\\mathcal{L}}\\longrightarrow$ Sets defines the meaning of $X$ as determined by classifiers, where Sets indicate the class of all sets. $(5)\\vdash_{\\mathcal{L}}$ symbolizes the provability relation of $\\mathcal{L}$ , evaluating formulas formed by $m n g_{\\cal G}$ is true or false in one possible world, such as the estimation criteria. More details of $\\mathcal{L}$ can be seen in Appendix B. ", "page_idx": 3}, {"type": "text", "text": "For clarity, we specify $\\mathcal{L}_{(X_{s},Y_{s})}\\,=\\,\\left\\langle F_{(X_{s},Y_{s})},D,\\middle|\\lbrack\\boldsymbol{x}_{(X_{s},Y_{s})},h,\\middle|\\lbrack\\boldsymbol{\\mathrm{h}}_{(X),Y})\\right\\rangle$ as the logic formed on the given $X,Y$ sets. With the goal for logic to generalize across a broader scenario and provide extrapolation across all possible formulas in $\\mathcal{L}$ , a good general logic $\\mathcal{L}^{\\ast}$ should be derived from $\\mathcal{L}$ through the feature extractor $g$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}^{*}=\\left\\langle F_{(g(X_{s}),Y_{s})},D,\\middle|\\overline{{\\Omega}}_{(g(X_{s}),Y_{s})},h,\\middle|\\Gamma_{(h o g(X),Y)}\\right\\rangle,s.t.,\\vdash_{(h o g(X),Y)}=\\middle|\\overline{{\\Omega}}_{(g(X_{s}),Y_{s})}\\ .\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Importantly, as a good general logic, $F_{(g(X_{s}),Y_{s})}$ and $h$ in $\\mathcal{L}^{\\ast}$ should form the atomic formulas, i.e., the tuple of terms with a predicate: $h\\circ g(x)$ belongs/not belongs to class $y$ in domain $d\\rightarrow$ $T u r e/F a\\bar{l}s e$ , where $x,y,d\\in X,Y,D$ , which makes that $\\vdash_{(h\\circ g(X_{u}),Y_{u})}{=}|{=}_{(g(X_{s}),Y_{s})}$ still holds. We simply denote one atomic formula in the form of $h(g(x),y,d)$ mapping to binary values. Additionally, $\\vdash_{\\left(h\\circ g(X),Y\\right)}{=}\\middle|{=}_{\\left(g(X_{s}),Y_{s}\\right)}$ in Eq. (6) can be safely omitted in the rest of the paper. Please see more details about the conditions of the good general logic in Appendix B. ", "page_idx": 4}, {"type": "text", "text": "An additional tool is necessary to convert the logic problem into a continuous form, enabling the application of machine learning algorithms. The conditional entropy-based method enables a logically sound derivation of knowledge from the provided dataset with constraints [43]. Specifically, the probabilistic inference process adheres to a probabilistic version of Modus Ponens: $A\\to B,A\\vdash B$ (if $A$ then $B$ ; not $A$ therefore not $B$ ). It is important to note that the logical propositions in probabilistic Modus Ponens are uncertain, with the conditional probability replacing the material implication $A\\,\\rightarrow\\,B$ . This framework allows us to interpret logical deduction through the lens of entropy. Therefore, for Eq. (6) which implies ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\exists h\\circ g,\\,\\forall(x,y)\\in(X,Y),\\,\\forall d\\in D,\\,\\,h\\circ g(x)\\to y,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "finding $h\\circ g$ through optimization is equivalent to ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{h,g}\\mathbb{E}_{(x,y)\\in(X,Y),d\\in D}P(y|g(x),d)-\\mathcal{R}\\Longleftrightarrow\\operatorname*{min}_{h,g}\\mathbb{E}_{(x,y)\\in(X,Y),d\\in D}H(y|g(x),d)+\\mathcal{R},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathcal{R}$ denotes any other possible regularization. ", "page_idx": 4}, {"type": "text", "text": "As the logical framework for image classification takes shape, it becomes evident that the unresolved question of identifying an appropriate function $g$ to generate suitable atomic formulas emerges as a critical factor in ensuring the effectiveness of the overarching logic $\\mathcal{L}^{\\ast}$ . This paper proposes L-Reg as the regularization to ensure $F_{(g(X_{s}),Y_{s})}$ are formed by atomic formulas in Section 3.2. ", "page_idx": 4}, {"type": "text", "text": "3.2 Constructing atomic formulas using L-Reg ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this part, we show the derivation details of L-Reg the aims to ensure the formation of suitable atomic formulas, as depicted in Eq. (6). As highlighted in [1], current algorithms may induce implicit biases towards unseen data, resulting in varied solutions for such data. However, expecting an algorithm to generalize effectively to unseen data domains without appropriate incentivization, such as specifically designed regularization, is unreasonable. Therefore, we aim to enhance the generalization capability of models by employing a logic-based regularization approach. To this end, we introduce the concept of semantic support for image classification. ", "page_idx": 4}, {"type": "text", "text": "Definition 3.2 (Semantic support). We denote $z=g(x)$ , where $z\\in Z$ , as a set of compositions of tahlle ssee mseamntaicnsti icns: $z:=\\{z^{i}\\}_{i=1}^{\\bar{M}^{\\prime}}$ u, l wfohre rdee $M$ citsi otnh eo rn iunmfebreern ocfe .d iWmee ndseifoinnes  tohre  sseumbsaentt so. f ,o teaxbtlrya,c tneodt $z$ $\\gamma$ $z$ from the sample $x\\sim\\chi$ , as the semantic support of $x$ if $\\gamma$ is sufficient for deducing the relationship between $x$ and a $y\\sim\\mathcal{V}$ . ", "page_idx": 4}, {"type": "text", "text": "sFeorm ainnstitcasn $\\{z^{i}\\}_{i=3}^{M}$ e wsiullb sneott $\\{z^{1},z^{2}\\}\\ \\subseteq\\ z$ f iesr esnucfef icpireoncte fsos.r  aWcchuerna $\\{z^{1},z^{2}\\}$ nccoen, sttihteu tveasl tuhees  omfi noitmhearl combination of semantics required for inference, it is termed the semantic support. We denote as the set of semantic supports of $X$ for deducing each individual class. ", "page_idx": 4}, {"type": "text", "text": "Derivation of L-Reg. Regarding Eq. (6), if the semantic supports and their relationship with $Y$ form atomic formulas, Eq. (6) holds as a good general logic, and the generalization would be improved. Thus, we aim to learn the latent features $Z$ , which contain sufficient semantic supports for the deduction of $Y$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\exists\\gamma\\in\\Gamma,\\gamma\\subseteq z,\\;\\forall(z,y)\\in(Z,Y),\\forall d\\in D,\\;h(\\gamma|d)\\to y.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Specifically, $g(\\cdot)$ should meet the following: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\forall(\\Gamma_{i},y_{i}),(\\Gamma_{j},y_{j})\\in(Z,Y),\\forall d\\in D,\\ y_{i}\\neq y_{j}\\Longleftrightarrow\\Gamma_{i}\\neq\\Gamma_{j},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "i.e., the semantic support set for each class should be distinct. The multiple-class classification task has that $\\forall\\Gamma$ , $|\\Gamma|\\leq\\bar{M}$ . Under the constraints demonstrated in Eq. (9) and Eq. (10), we need to achieve the following through optimization: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{h,g}H(Y|g(\\Gamma),D),\\operatorname*{max}_{h,g}H(Y|g(\\bar{\\Gamma}),D)\\Longleftrightarrow\\operatorname*{min}_{h,g}H(Y|g(\\Gamma),D)-H(Y|g(\\bar{\\Gamma}),D),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\bar{\\Gamma}$ denotes the negation of $\\Gamma$ , i.e., the set of semantics which does not include semantic support. ", "page_idx": 5}, {"type": "text", "text": "Intuitively, Eq. (11) regularizes that the model should be able to judge whether a sample belongs to a class by using a minimal set of semantic supports; simultaneously, the semantic support sets are also implicitly disentangled for each class, not only for maintaining rich and useful semantics but also for enhancing the independence of deduction of each class. The actual collection of $\\Gamma$ appears to be intractable during optimization. Hence, we resort to deriving its bounds. Regarding Eq. (11), its former term can be elaborated as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\nH(Y|g(\\Gamma),D)\\leq H(Y|h(z_{i}),D)\\leq\\mathbb{E}_{z_{i}\\sim z}[H(Y|g(z_{i}),D)],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $z_{i}$ is minimal semantics form $z$ , and $\\mathbb{E}_{i=1}^{M}H(Y|g(z_{i}),D)$ is the upper-bound for $\\mathrm{min}_{h,g}\\,H(Y|g(\\Gamma),D)$ . Therefore, minimizing $\\mathbb{E}_{i=1}^{M}H(Y|g(z_{i}),D)$ is equivalent to minimizing $H(Y|\\bar{g}(\\Gamma),D)$ . Meanwhile, for the latter in Eq. (11), we have: ", "page_idx": 5}, {"type": "equation", "text": "$$\nH(Y|g(\\bar{\\Gamma}),D)\\geq H(Y|g(z),D),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $H(Y|h(z)),D)$ is the lower-bound for $\\operatorname*{max}_{h,g}H(Y|g(\\bar{\\Gamma}),D)$ . Combining the aforementioned bounds, we have the L-Reg objective as Eq. (2). ", "page_idx": 5}, {"type": "text", "text": "Interpretability of semantic supports roots in forming atomic formulas. The atomic formula $\\mathcal{A}^{y}$ is of the form $\\dot{h}(g(x),y,d)$ . Our aim is to find the good (most) general $\\mathcal{A}^{y*}\\in\\mathcal{A}^{y}$ for $y$ class from which the interpretability of L-Reg is derived. Consider ${\\cal A}_{1}^{y}$ , $\\bar{A_{2}^{y}}\\in\\bar{A}^{y}$ , if ${\\mathcal{A}}_{1}^{y}$ is more general than ${\\mathcal{A}}_{2}^{y}$ , there will be a substitution $\\psi$ such that $A_{1}^{y}\\psi=A_{2}^{y}$ [52]. $A^{y*}$ should meet $A^{y*}\\bar{\\psi}\\bar{=}A_{i}^{y}\\in\\mathcal{A}^{y}$ , which infers that $\\gamma^{y}\\psi=z^{y}$ (cf. Eq. (9)) for predication of $y$ where $\\gamma^{y}$ is the semantic support. Note here that the form of $\\mathcal{A}^{y}$ is constructed for $y\\in Y$ , i.e., predicate whether the sample belongs to the $y$ class. Considering multiple classes $y_{i},y_{j}\\in Y,i\\neq j$ , it has $A^{y_{i}*}\\neq A^{y_{j}*}$ thus $\\gamma^{y_{i}}\\neq\\gamma^{y_{j}}$ (cf. Eq. (10)), which constrains that different minimal semantic supports should be used for predicting different classes. The interpretability of $\\mathbf{L}{-}\\mathbf{R}\\mathrm{eg}$ is based on $A^{y*}$ , compelling the model to use distinct minimal semantic supports for each class. These minimal semantic supports can be interpreted as the most critical features for efficient prediction. For example, as shown in Fig. 1, the model with L-Reg has learned the facial features of the person class (see more examples in Appendix Figs. 7 to 12), forming the (informal) atomic formula $h$ (has a human face, is person, $d\\in D)\\to$ True. Similarly, it also leads to $h$ (not has a human face, is person, $d\\in D)\\to{\\mathrm{Fals}}$ e. ", "page_idx": 5}, {"type": "text", "text": "4 L-Reg under different generalization settings ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "L-Reg under data-shift generalization. The task mDG endeavors to facilitate a model\u2019s ability to generalize to unseen domains by fostering invariance across seen domains [50]. In the context of mDG, the term $|D|\\ge2$ in Eq. (8) typically denotes multiple domains. Traditionally, existing methods focus on minimizing domain gaps, leading to remarkable results [25, 50]. However, it is noteworthy that even when the domain gap is effectively minimized, and $|D|=1$ for the latent features can be considered, L-Reg still demonstrates its efficacy in promoting the generalization of $X_{u}$ from $D_{u}$ . ", "page_idx": 5}, {"type": "text", "text": "Proposition 4.1 (Effectiveness of L-Reg in enhancing data-shift generalization.). Assume the gap farcormo stsh ae llu ndsoemena idnso ims aiwne lwl itmhi ntihme ilzoewd.e sLt ect $f^{*}$ pdleexnitoyt.e  tFhoer  taa rmgoetd eml $f_{(X_{s},Y_{s})}^{R},\\bar{f}_{(X_{s},Y_{s})}$ z ters ation ethd eu dnadtear $X_{u}$ data-shift generalization setting (i.e., $(X_{s},Y_{s})$ is accessible and $y_{s}=y_{u}.$ ). We have: ", "page_idx": 5}, {"type": "equation", "text": "$$\nG L(f_{(X_{s},Y_{s})}^{R},f^{*},X_{u})\\leq G L(f_{(X_{s},Y_{s})},f^{*},X_{u}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Please see proof details in Proposition C.1. To illustrate Proposition 4.1, consider the following intuitive example: In the seen domains, all cats are either black or white, while all dogs are brown. Now, imagine encountering a sample labeled \u2018a brown cat\u2019 from an unseen domain. Without the application of L-Reg, the model might erroneously classify it as a dog. However, with L-Reg in place, the model is compelled to rely on minimal semantics for classification. This means filtering out irrelevant features such as color terms, thus enabling more accurate deductions. ", "page_idx": 5}, {"type": "table", "img_path": "Woiqqi5bYV/tmp/6ac9258fbf6e7a11b94792b9cbd72469af70526c887c94adde9644ef53f6fb10.jpg", "table_caption": ["Table 1: MDG results: Comparison between the proposed and previous non-ensemble and ensemble mDG methods. The best results for each group are highlighted in bold. Improvement and degradation in our approach from GMDG are highlighted in red. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "L-Reg under target-shift generalization. We demonstrate how L-Reg enhances generalized discovery in scenarios where only a subset of classes $(Y_{s})$ is available for training, and there may exist an overlap between the unseen classes $(Y_{u})$ and the seen classes $(Y_{s})$ , denoted as $\\mathcal{V}_{u}\\cap\\mathcal{V}_{s}\\neq\\emptyset$ . We define $Y_{u}/Y_{s}^{\\bar{}}$ as the novel classes not included in $Y_{s}$ , and $Y_{u}\\sim Y_{s}$ as the seen classes for $X_{u}$ classification, where $|D|=1$ . Building upon Proposition 4.1, L-Reg further enhances GCG by improving the generalization performance on $Y_{u}$ . ", "page_idx": 6}, {"type": "text", "text": "Proposition 4.2 (L-Reg improves target-shift generalization). When $|D|=1$ , $L$ -Reg promotes generalization performance on $Y_{u}$ under the target-shift scenario. ", "page_idx": 6}, {"type": "text", "text": "Proof. When $|D|=1$ , since all $Y$ belongs to a close set, minimizing $-H(Y_{s}|g(\\bar{\\Gamma}),D)$ is equivalent to the following: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{h,g}-H(Y_{s}|g(\\bar{\\Gamma}))\\Longleftrightarrow\\operatorname*{min}_{h,g}H(\\bar{Y_{s}}|g(\\bar{\\Gamma})),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where ${\\bar{Y_{s}}}$ is the negation of $Y_{s}$ , i.e., $Y_{u}/Y_{s}$ . In this situation, if one sample does not contain sufficient semantic support to be classified under $Y_{s}$ , it otherwise will be assigned under $Y/Y_{s}$ , promoting performance for both $Y_{u}/Y_{s}$ and $Y_{u}\\sim Y_{s}$ . Therefore, the generalization performance on the unseen classes will be improved by L-Reg. \u53e3 ", "page_idx": 6}, {"type": "text", "text": "L-Reg under all-shift generalization. When the domain gap is sufficiently minimized and $|D|=1$ can be considered, the combination of Proposition 4.1 and Proposition 4.2 demonstrates that L-Reg enhances generalization performance on both novel classes $\\bar{(Y_{u}/Y_{s})}$ and seen classes $(Y_{u}\\sim Y_{s})$ for $X_{u}$ from other domains. Our experiments validate that L-Reg, when applied in scenarios with well-minimized domain gaps, consistently improves generalization across all shifts. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To validate $\\mathbf{L}{-}\\mathbf{Reg}$ , three groups of experiments under the three kinds of settings are conducted. Notably, all baselines we used already incorporate the $L_{2}$ regulation in the form of weight decay. We also compare other commonly used regularization terms, such as independence or sparsity regularization on $Z$ . More results in Appendix Findicate that our L-Reg also surpasses them. ", "page_idx": 6}, {"type": "text", "text": "5.1 Experiments on mDG ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Experimental settings. We operate on the DomainBed suite [21] and leverage standard leaveone-out cross-validation as the evaluation protocol. We test L-Reg with GMDG [50] on 5 realworld benchmark datasets: PACS [32], VLCS [18], OfficeHome [55], TerraIncognita [7], and ", "page_idx": 6}, {"type": "text", "text": "DomainNet [42]. Following MIRO [25] and GMDG [50], the RegNetY-16GF backbone with SWAG pre-training [47]) is used. Specifically, we train the backbone using GMDG with L-Reg. Accuracy is adopted as the evaluation metric, and the results of the averages from three trials of each experiment, with standard deviations, are presented. See Supplementary H for more experimental details. ", "page_idx": 7}, {"type": "text", "text": "Results. The experimental results presented in Table 1 demonstrate the efficacy of L-Reg in improving the performance of GMDG across all datasets in mDG classification tasks. Notably, more substantial improvements are observed when the GMDG baseline achieves relatively low accuracy. These observed enhancements provide empirical support for Proposition 4.1. Please see using L-Reg with basic ERM in Appendix E. For detailed insights into each domain within each dataset, please refer to Appendix H.1. ", "page_idx": 7}, {"type": "text", "text": "5.2 Experiments on GCD ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Experimental settings. We validate our approach through training PIM additionally with L-Reg. Six image datasets are adopted to validate the feasibility of our proposed RPIM compared to other competitors, including three generic object recognition datasets, CIFAR10 [29], CIFAR100 [29] and ImageNet-100 [17]; two fine-grained datasets CUB [56] and Stanford Cars [28]; and the longtail dataset Herbarium19 [49]. Following prior works [54, 16], we use the proposed accuracy metric from [54] of all classes, known classes, and unknown classes for evaluation. Please see a detailed description of the experimental setup in Appendix H.2. ", "page_idx": 7}, {"type": "text", "text": "Results. The average results across all datasets for utilizing L-Reg with PIM are presented in Table 2, while detailed dataset-specific information is available in Appendix H Table 17. The results highlight that L-Reg consistently increases the accuracy of all unknown classes across all datasets, thus confirming the validity of Proposition 4.2. However, it is notable that L-Reg may marginally compromise the performance of known classes, as it reduces the size of semantic support for deducing $Y$ , thereby reducing the information available for known classification. Nevertheless, this compromise is deemed acceptable given the significant improvements observed for the unknown classes. ", "page_idx": 7}, {"type": "table", "img_path": "Woiqqi5bYV/tmp/b2b5a361a20939b58289533036a4c928d2acfb42c89f69b9deb80334a5db9ec1.jpg", "table_caption": ["Table 2: GCD results: Average results across all datasets of PIM with L-Reg. Improvements and degradation are highlighted in red and blue, respectively. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "Woiqqi5bYV/tmp/20885d69239ef63f20824f4a389c30e0c3958d1204d919def4215c505fe65846.jpg", "table_caption": ["Table $3\\colon\\mathrm{MDG+GCD}$ results: Averaged accuracy scores for all, known and unknown classes across all five datasets. Improvements and degradation are highlighted in red and blue respectively. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.3 Experiments on mDG $^+$ GCD ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Experimental settings. We utilize datasets designed for mDG tasks to conduct $\\mathrm{mDG}+\\mathrm{GCD}$ experiments. During the training stage, only samples from seen domains are available, with half of the classes masked as unknown, and only their unlabeled data are utilized. Notably, even though all the unlabeled data originates from unknown classes during training, this prior knowledge is not assumed or constrained, aligning the setting with GCD. Similar to mDG, we adopt the leave-one-out cross-validation method. This entails testing each domain in each dataset as the unseen domain. The performance is tested on unseen domains by employing GCD metrics. To validate L-Reg\u2019s efficacy comprehensively, we re-implement four methods under the $\\mathrm{mDG}+\\mathrm{GCD}$ setting, testing them both with and without L-Reg. The four methods include ERM, PIM, MIRO, and GMDG. ERM serves as the baseline approach without additional regularization, while PIM maximizes information without minimizing domain gaps. MIRO and GMDG focus on minimizing domain gaps, with GMDG offering a comprehensive approach in this regard. It is worth noting that PIM has been re-implemented. For further experimental details, please refer to Appendix H.3. ", "page_idx": 7}, {"type": "text", "text": "Results. The averaged results across all unseen domains of all datasets are summarized in Table 3. For a detailed breakdown of results for each domain in each dataset, please refer to Appendix H.3. As discussed in Proposition 4.1 and Proposition 4.2, a noticeable trend is observed wherein, as the domain gap is gradually minimized, the improvements for unknown classes increase, with the best results achieved using GMDG with L-Reg. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "L-Reg forms atomic formulas and improves interpretability. Furthermore, Fig. 5 provides visual insights into the behavior of models trained with LReg. Evidently, these models tend to focus on minimal semantics sufficient for class distinctions. For the known classes, the efficacy of L-Reg can be intuitively understood as extracting the minimal semantic supports for a given class label. For instance, the presence of a guitar\u2019s fingerboard, even in unseen domains, helps classify a sample as belonging to the guitar category, whose informal forms can be denoted as $h$ (has fingerboard, is guitar, $d\\in D$ ) $\\rightarrow$ True and $h$ (not has fingerboard, is guitar, $d\\ \\in\\ D$ ) $\\rightarrow$ False. For all known classes, samples with these minimal semantic supports are recognized accordingly. In contrast, if a sample lacks these minimal supports for any known class, it is very likely categorized as an unknown class. This behavior stems from Paper Eq.10 which ensures $A^{y_{i}*}\\ne A^{y_{j}*}$ through constraining $\\gamma^{y_{i}}\\neq\\gamma^{y_{j}}$ . L-Reg further enhances the model\u2019s ability to identify minimal supports for unknown classes by filtering out co-covariant features associated with other classes and thus generalizing to unseen domains. Therefore, the very interpretable features for unknown classes from unseen domains can be extracted using L-Reg. Fig. 5 (right side) demonstrates that the model with L-Reg can even extract facial features for the unknown person class and can generalize this to the unseen domain. Similarly, here we obtain (informal) atomic formulas as $h$ (has a face, is person, $d\\in D)\\to\\operatorname{Tr}$ ue, $h$ (not has a face, is person, $d\\in D)\\to{\\mathrm{False}}$ . ", "page_idx": 8}, {"type": "image", "img_path": "Woiqqi5bYV/tmp/19b378ebe11fdbe84d8284d33f7ddf07a9189fc41d512bef434ce04ddf3366d8.jpg", "img_caption": ["Figure 5: GradCAM visualizations of GMDG trained without and with L-Reg. The seen, unseen domains and known, unknown classes are denoted. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "However, as shown in Row 3, significant domain shifts, such as those between the sketch domain and other domains, pose challenges. Specifically, the differences between the stick-figure style of sketches of persons and figures from other domains can hinder the model\u2019s ability to cluster sketches with other domains\u2019 figures when the class label is unknown. Thus, under this circumstance, the model may fail to extract meaningful features from those sketches. We acknowledge this limitation and will explore solutions in future work. ", "page_idx": 8}, {"type": "text", "text": "L-Reg should be applied to features from deep layers. One crucial precondition highlighted in the theoretical analysis is that L-Reg operates effectively with a representation $Z$ , where each dimension represents independent semantics. The semantic features usually come from the deeper layers of the model architecture [51]. However, Table 4 shows that applying L-Reg to features from earlier layers, which may not necessarily represent semantics, leads to a degradation in performance for known classes, albeit improving performance for unknown classes. This phenomenon arises due to the potential interdependence among features from earlier layers, resulting in penalization that may hinder the capture of semantic supports essential for known classes. To ensure generalization improvements without significant compromise to the performance of known classes, we advocate for applying L-Reg specifically to features extracted from deeper layers, such as the bottleneck layer. These suggest that the compromised results observed in Table 2 could be attributed to the less depth of the model structure, which fails to provide the expected semantic features. ", "page_idx": 8}, {"type": "table", "img_path": "Woiqqi5bYV/tmp/dd6a3206b63d4913d6754e7a028101a21af052bc4c59adbcdf7b50f40c89d34a.jpg", "table_caption": ["Table 4: Averaged results of applying L-Reg to different layers across domains in PACS. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "5.4 Apply L-Reg to congestion prediction for circuit design. ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Experimental settings. We also test L-Reg in Congestion prediction on the CircuitNet [15] dataset by using CircuitFormer [63] backbone. The congestion prediction is for circuit design and benefits from logical reasoning-based approaches. All parameters, except for L-Reg, remain consistent with CircuitFormer, and we follow its metrics. ", "page_idx": 8}, {"type": "text", "text": "Results. Table 5 shows the results of prediction results on the CircuitNet dataset. We also include the results of Gpdl with $\\mathrm{UNet++}$ and CircuitFormer for better comparison. Notably, the improvements brought by CircuitFormer with LReg across all metrics, especially for the pearson metric can be observed. The consistent improvement with L-Reg across all metrics indicates L-Reg\u2019s feasibility. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "table", "img_path": "Woiqqi5bYV/tmp/7ec3b055bbc83e876dbe7366d3d2054b949a0cec0ae6b9ec196e00ca87313901.jpg", "table_caption": ["Table 5: Results of Congestion prediction: Congestion prediction is proposed for circuit design. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Related work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Logical reasoning for deep learning. Current studies focus on length generalization or symbolic reasoning in the logic-based scope. For length generalization, [1] proposes the generalization to the unseen setting, theoretically verifying that commonly used models can generalize to the unseen and degree curriculum promotes the generalization ability of the transformer, followed by [3, 2, 60]. Another branch is to improve the logical reasoning ability for abstract symbols, such as learning the logical-based temples and expecting the model to generalize to unseen samples [10, 36]. These studies are closely related to languages, such as generating longer answering sequences or solving mathematical problems in large language models, lacking explicit connections to visual tasks. [6] delves into the logical explanations in image classification by explicitly extracting logical relationships. While this logical-based approach sheds light on the interpretability of image classification models, its specific benefits for visual generalization remain relatively unexplored. ", "page_idx": 9}, {"type": "text", "text": "Multi-domain generalization. Current approaches for mDG in image classification focus on learning invariant representation across domains. Previous approaches like DANN [20] minimize feature divergences between source domains. CDANN [35], CIDG [34], and MDA [23] consider conditions for learning conditionally invariant features. MIRO [25] and GMDG [50] take advantage of pretrained models to improve generalization. Specifically, in comparison to MIRO, GMDG proposes a general entropy-based learning objective for mDG and sufficiently minimizes the domain gaps, yielding better generalization results. ", "page_idx": 9}, {"type": "text", "text": "Generalized category discovery. Generalized category discovery, pioneered by [54], addresses unlabeled samples with both known and unknown classes. Furthermore, PIM [16] integrates InfoMax into generalized category discovery, effectively handling imbalanced datasets and surpassing GCD on both short- and long-tailed datasets. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper presents L-Reg, a logical regularization approach tailored for image classification tasks using logic analysis frameworks. L-Reg yields better generalization across different settings by fostering balanced feature distributions and streamlining the classification model\u2019s complexity. Rigorous theoretical analyses and empirical validations underscore its efficacy, as L-reg consistently improves generalization performance with different frameworks under various scenarios. ", "page_idx": 9}, {"type": "text", "text": "Limitation. L-Reg narrows the extent of semantic supports, potentially diminishing the amount of information available for classification and leading to certain trade-offs in the performance of seen datasets. This effect is evidenced by the slight decline in the accuracy of known classes when L-Reg is applied, as shown in Table 2. A similar phenomenon is observed in Fig. 5, where the model fails to recognize a person in the sketch domain lacking facial features. Analysis from Table 4 suggests that these compromises may result from improper $Z$ . Future work should focus on mitigating potential compromises on seen datasets by exploring strategies for better capturing $Z$ through improved model architecture design. We offer more experimental results of possible solutions to this limitation in Appendix G, such as further constraining the independence of each dimension in $Z$ . Those results may suggest a direction for future work. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The work was partially supported by the following: National Natural Science Foundation of China under No. 92370119, No. 62376113, No. 62206225, and No. 62276258. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Emmanuel Abbe, Samy Bengio, Aryo Lotf,i and Kevin Rizk. Generalization on the unseen, logic reasoning and degree curriculum. In International Conference on Machine Learning, pages 31\u201360. PMLR, 2023.   \n[2] Emmanuel Abbe, Elisabetta Cornacchia, and Aryo Lotf.i Provable advantage of curriculum learning on parity targets with mixed inputs. Advances in Neural Information Processing Systems, 36, 2024.   \n[3] Kartik Ahuja and Amin Mansouri. On provable length and compositional generalization. arXiv preprint arXiv:2402.04875, 2024.   \n[4] Hajnal Andr\u00e9ka, Istv\u00e1n N\u00e9meti, and Ildik\u00f3 Sain. Universal algebraic logic. Studies in Logic, Springer, due to, 2017.   \n[5] Martin Arjovsky, L\u00e9on Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. arXiv preprint arXiv:1907.02893, 2019.   \n[6] Pietro Barbiero, Gabriele Ciravegna, Francesco Giannini, Pietro Li\u00f3, Marco Gori, and Stefano Melacci. Entropy-based logic explanations of neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 6046\u20136054, 2022.   \n[7] Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In Proceedings of the European conference on computer vision (ECCV), pages 456\u2013473, 2018.   \n[8] Gilles Blanchard, Aniket Anand Deshmukh, \u00dcrun Dogan, Gyemin Lee, and Clayton Scott. Domain generalization by marginal transfer learning. The Journal of Machine Learning Research, 22(1):46\u2013100, 2021.   \n[9] Gilles Blanchard, Gyemin Lee, and Clayton Scott. Generalizing from several related classification tasks to a new unlabeled sample. Advances in neural information processing systems, 24, 2011.   \n[10] Enric Boix-Adsera, Omid Saremi, Emmanuel Abbe, Samy Bengio, Etai Littwin, and Joshua Susskind. When can transformers reason with abstract symbols? arXiv preprint arXiv:2310.09753, 2023.   \n[11] Malik Boudiaf, Imtiaz Ziko, J\u00e9r\u00f4me Rony, Jos\u00e9 Dolz, Pablo Piantanida, and Ismail Ben Ayed. Information maximization for few-shot learning. Advances in Neural Information Processing Systems, 33:2445\u20132457, 2020.   \n[12] Manh-Ha Bui, Toan Tran, Anh Tran, and Dinh Phung. Exploiting domain-specific features to enhance domain generalization. Advances in Neural Information Processing Systems, 34:21189\u201321201, 2021.   \n[13] Kaidi Cao, Maria Brbic, and Jure Leskovec. Open-world semi-supervised learning. In International Conference on Learning Representations, 2022.   \n[14] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 9650\u20139660, 2021.   \n[15] Zhuomin Chai, Yuxiang Zhao, Wei Liu, Yibo Lin, Runsheng Wang, and Ru Huang. Circuitnet: An open-source dataset for machine learning in vlsi cad applications with improved domain-specific evaluation metric and learning strategies. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 42(12):5034\u20135047, 2023.   \n[16] Florent Chiaroni, Jose Dolz, Ziko Imtiaz Masud, Amar Mitiche, and Ismail Ben Ayed. Parametric information maximization for generalized category discovery. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1729\u20131739, 2023.   \n[17] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.   \n[18] Chen Fang, Ye Xu, and Daniel N Rockmore. Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias. In Proceedings of the IEEE International Conference on Computer Vision, pages 1657\u20131664, 2013.   \n[19] Enrico Fini, Enver Sangineto, St\u00e9phane Lathuili\u00e8re, Zhun Zhong, Moin Nabi, and Elisa Ricci. A unified objective for novel class discovery. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9284\u20139292, 2021.   \n[20] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Fran\u00e7ois Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. The journal of machine learning research, 17(1):2096\u20132030, 2016.   \n[21] Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. arXiv preprint arXiv:2007.01434, 2020.   \n[22] Kai Han, Sylvestre-Alvise Rebuff,i Sebastien Ehrhardt, Andrea Vedaldi, and Andrew Zisserman. Autonovel: Automatically discovering and learning novel visual categories. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.   \n[23] Shoubo Hu, Kun Zhang, Zhitang Chen, and Laiwan Chan. Domain generalization via multidomain discriminant analysis. In Uncertainty in Artificial Intelligence, pages 292\u2013302. PMLR, 2020.   \n[24] Zeyi Huang, Haohan Wang, Eric P Xing, and Dong Huang. Self-challenging improves cross-domain generalization. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part II 16, pages 124\u2013140. Springer, 2020.   \n[25] Cha Junbum, Lee Kyungjae, Park Sungrae, and Chun Sanghyuk. Domain generalization by mutualinformation regularization with pre-trained models. European Conference on Computer Vision (ECCV), 2022.   \n[26] Daehee Kim, Youngjun Yoo, Seunghyun Park, Jinkyu Kim, and Jaekoo Lee. Selfreg: Self-supervised contrastive regularization for domain generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9619\u20139628, 2021.   \n[27] Andreas Krause, Pietro Perona, and Ryan Gomes. Discriminative clustering by regularized information maximization. Advances in neural information processing systems, 23, 2010.   \n[28] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In Proceedings of the IEEE international conference on computer vision workshops, pages 554\u2013561, 2013.   \n[29] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[30] David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrapolation (rex). In International Conference on Machine Learning, pages 5815\u20135826. PMLR, 2021.   \n[31] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy Hospedales. Learning to generalize: Meta-learning for domain generalization. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018.   \n[32] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier domain generalization. In Proceedings of the IEEE international conference on computer vision, pages 5542\u20135550, 2017.   \n[33] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C Kot. Domain generalization with adversarial feature learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5400\u20135409, 2018.   \n[34] Ya Li, Mingming Gong, Xinmei Tian, Tongliang Liu, and Dacheng Tao. Domain generalization via conditional invariant representations. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018.   \n[35] Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao. Deep domain generalization via conditional invariant adversarial networks. In Proceedings of the European conference on computer vision (ECCV), pages 624\u2013639, 2018.   \n[36] Zenan Li, Yunpeng Huang, Zhaoyu Li, Yuan Yao, Jingwei Xu, Taolue Chen, Xiaoxing Ma, and Jian Lu. Neuro-symbolic learning yielding logical constraints. Advances in Neural Information Processing Systems, 36, 2024.   \n[37] Ziyue Li, Kan Ren, Xinyang Jiang, Yifei Shen, Haipeng Zhang, and Dongsheng Li. Simple: Specialized model-sample matching for domain generalization. In The Eleventh International Conference on Learning Representations, 2022.   \n[38] J MacQueen. Classification and analysis of multivariate observations. In Proceedings of the 5th Berkeley Symposium on Mathematical Statistics and Probability, pages 281\u2013297, 1967.   \n[39] Ofir Moshe, Gil Fidel, Ron Bitton, and Asaf Shabtai. Improving interpretability via regularization of neural activation sensitivity. arXiv preprint arXiv:2211.08686, 2022.   \n[40] Hyeonseob Nam, HyunJae Lee, Jongchan Park, Wonjun Yoon, and Donggeun Yoo. Reducing domain gap by reducing style bias. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8690\u20138699, 2021.   \n[41] Konstantinos Panagiotis Panousis, Dino Ienco, and Diego Marcos. Sparse linear concept discovery models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2767\u20132771, 2023.   \n[42] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1406\u20131415, 2019.   \n[43] Wilhelm R\u00f6dder. Conditional logic and the principle of entropy. Artificial Intelligence, 117(1):83\u2013106, 2000.   \n[44] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. arXiv preprint arXiv:1911.08731, 2019.   \n[45] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE international conference on computer vision, pages 618\u2013626, 2017.   \n[46] Yuge Shi, Jeffrey Seely, Philip HS Torr, N Siddharth, Awni Hannun, Nicolas Usunier, and Gabriel Synnaeve. Gradient matching for domain generalization. arXiv preprint arXiv:2104.09937, 2021.   \n[47] Mannat Singh, Laura Gustafson, Aaron Adcock, Vinicius de Freitas Reis, Bugra Gedik, Raj Prateek Kosaraju, Dhruv Mahajan, Ross Girshick, Piotr Doll\u00e1r, and Laurens Van Der Maaten. Revisiting weakly supervised pre-training of visual perception models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 804\u2013814, 2022.   \n[48] Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In Computer Vision\u2013ECCV 2016 Workshops: Amsterdam, The Netherlands, October 8-10 and 15-16, 2016, Proceedings, Part III 14, pages 443\u2013450. Springer, 2016.   \n[49] Kiat Chuan Tan, Yulong Liu, Barbara Ambrose, Melissa Tulig, and Serge Belongie. The herbarium challenge 2019 dataset. arXiv preprint arXiv:1906.05372, 2019.   \n[50] Zhaorui Tan, Xi Yang, and Kaizhu Huang. Rethinking multi-domain generalization with a general learning objective. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 23512\u201323522, June 2024.   \n[51] Zhaorui Tan, Xi Yang, and Kaizhu Huang. Semantic-aware data augmentation for text-to-image synthesis. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 5098\u20135107, 2024.   \n[52] Irene Tsapara and Gy\u00f6rgy Tur\u00e1n. Learning atomic formulas with prescribed properties. In Proceedings of the eleventh annual conference on Computational learning theory, pages 166\u2013174, 1998.   \n[53] Vladimir N. Vapnik. Statistical Learning Theory. Wiley-Interscience, 1998.   \n[54] Sagar Vaze, Kai Han, Andrea Vedaldi, and Andrew Zisserman. Generalized category discovery. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7492\u20137501, 2022.   \n[55] Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing network for unsupervised domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5018\u20135027, 2017.   \n[56] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset. 2011.   \n[57] Chunyang Wu, Mark JF Gales, Anton Ragni, Penny Karanasou, and Khe Chai Sim. Improving interpretability and regularization in deep learning. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 26(2):256\u2013265, 2017.   \n[58] Mike Wu, Michael Hughes, Sonali Parbhoo, Maurizio Zazzi, Volker Roth, and Finale Doshi-Velez. Beyond sparsity: Tree regularization of deep models for interpretability. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018.   \n[59] Mike Wu, Sonali Parbhoo, Michael Hughes, Ryan Kindle, Leo Celi, Maurizio Zazzi, Volker Roth, and Finale Doshi-Velez. Regional tree regularization for interpretability in deep neural networks. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 6413\u20136421, 2020.   \n[60] Changnan Xiao and Bing Liu. A theory for length generalization in learning to reason. arXiv preprint arXiv:2404.00560, 2024.   \n[61] Marvin Zhang, Henrik Marklund, Nikita Dhawan, Abhishek Gupta, Sergey Levine, and Chelsea Finn. Adaptive risk minimization: Learning to adapt to domain shift. Advances in Neural Information Processing Systems, 34:23664\u201323678, 2021.   \n[62] Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Domain generalization with mixstyle. arXiv preprint arXiv:2104.02008, 2021.   \n[63] Jialv Zou, Xinggang Wang, Jiahao Guo, Wenyu Liu, Qian Zhang, and Chang Huang. Circuit as set of points. Advances in Neural Information Processing Systems, 36, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Broader impact ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Our regularization term based on logic for image classification offers significant potential beyond academia. By integrating logical constraints, our approach enhances model robustness, interpretability, and ethical alignment. This translates into improved performance on real-world tasks such as disease diagnosis in healthcare and mitigating biases in decision-making systems. Our work fosters interdisciplinary collaboration and contributes to the responsible deployment of AI technologies, ultimately benefiting society through enhanced efficiency, fairness, and transparency in machine learning applications. ", "page_idx": 14}, {"type": "text", "text": "B Details of the logical framework for visual classification task ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We provide more details of the connections between logical reasoning and visual classification tasks. Definition B.1. Following [4], a logic $\\mathcal{L}$ is a five-tuple defined in the form: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\langle F_{\\mathcal{L}},M_{\\mathcal{L}},\\vert\\vert\\boldsymbol{\\omega}_{\\mathcal{L}},m n g_{\\mathcal{L}},\\vert\\boldsymbol{\\omega}_{\\mathcal{L}}\\rangle\\,,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where ", "page_idx": 14}, {"type": "text", "text": "\u2022 $F_{\\mathcal{L}}$ is a set of all formulas of $\\mathcal{L}$ . $F_{\\mathcal{L}}$ arbitrarily refers to any collections that can be \u2018expressed\u2019 by language $\\mathcal{L}$ . Therefore, $F_{\\mathcal{L}}$ could be not only a collection of languages but also images and labels $(X,Y)$ for computer vision cases.   \n\u2022 $M_{\\mathcal{L}}$ is a class called the class of all models (or possible worlds) of $\\mathcal{L}$ ; intuitively, this can be considered as different domains $D$ of $X$ .   \n\u2022 $\\vdash_{\\mathcal{L}}$ is a binary relation, $\\begin{array}{r}{\\|{=}c\\subseteq M_{\\mathcal{L}}\\times F_{\\mathcal{L}}}\\end{array}$ , called the validity relation of $\\mathcal{L}$ . For example, in the known set, the ground truth label of the image is given as truth, which is the validity relation.   \n\u2022 $m n g_{\\cal{L}}:F_{{\\cal{L}}}\\times M_{{\\cal{L}}}\\longrightarrow\\;\\mathrm{S}$ ets where Sets is the class of all sets. $m n g_{\\cal G}$ is a function with domain $F_{\\mathcal{L}}\\times M_{\\mathcal{L}}$ , called the meaning function of $\\mathcal{L}$ : Intuitively, mngL extracts the meaning of the expressions can be understood as the classifiers.   \n\u2022 $\\vdash_{\\mathcal{L}}$ represents the provability relation of $\\mathcal{L}$ , telling us which formulas are \u2018true\u2019 in which possible world and usually is definable from $m n g_{\\cal{L}}$ , such as the estimation criteria in the machine learning system. ", "page_idx": 14}, {"type": "text", "text": "Accordingly and still following [4], a good general logic is defined as: ", "page_idx": 14}, {"type": "text", "text": "Definition B.2 (General logic). : A general logic is a class: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{L}^{*}:=\\left\\langle\\mathcal{L}^{P}:P\\in S i g\\right\\rangle,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $S i g$ is a class of sets; $\\mathcal{L}^{P}=\\left\\langle F_{\\mathcal{L}}^{P},M_{\\mathcal{L}}^{P},\\vert\\vert_{\\mathcal{L}}^{P},m n g_{\\mathcal{L}}^{P},\\vert\\vert_{\\mathcal{L}}^{P},\\right\\rangle$ is a compositional logic in the sense of Definition B.1 for $P\\in S i g$ , and for any sets $P,Q\\in S i g$ satisfies the following conditions: ", "page_idx": 14}, {"type": "text", "text": "1. $P$ is the set of atomic formulas of $\\textstyle{\\mathcal{L}}^{P}$ .   \n2. $C n(\\mathcal{L}^{P})=C n(\\mathcal{L}^{Q}):=C n(\\mathcal{L}^{*})$ where $C n(\\cdot)$ is called the set of logical connectives of the given logic (these are operation symbols with finite or infinite ranks).   \n3. Any bijection $f:P\\mathbb{1}Q$ that extends to a bijection between the tautological formula algebras of $\\dot{\\boldsymbol{\\mathcal{L}}}^{P}$ and $\\boldsymbol{\\mathcal{L}}^{Q}$ induces an isomorphism between $\\textstyle{\\mathcal{L}}^{P}$ and $\\textstyle{\\mathcal{L}}^{Q}$ .   \n4. If $P\\subseteq Q$ , then $\\textstyle{\\mathcal{L}}^{P}$ is a sublogic of $\\textstyle{\\mathcal{L}}^{Q}$ .   \n5. For any $P\\in S i g$ and set $H$ , there is a $P^{\\prime}\\in S i g$ such that $P^{\\prime}$ is disjoint from $H$ and ${\\mathcal{L}}^{P^{\\prime}}$ is an isomorphic copy of $\\textstyle{\\mathcal{L}}^{P}$ .   \n6. The union of a system $P_{i},i\\ \\in\\ I$ of pairwise disjoint sets $P_{i}$ from Sig belongs to $S i g$ , whenever $I$ is not empty. Let $\\mathfrak{F r}(\\cdot)$ denotes free algebra, $\\mathrm{Alg}_{m}({\\mathcal{L}})$ represents $\\{m n g_{\\mathfrak{M}}(\\mathfrak{F}):\\mathfrak{M}\\in M\\}$ where $\\mathfrak{F}$ denotes the term algebra. Further, the tautological congruence of the logic belonging to the disjoint union $P$ is generated in $\\mathfrak{F}\\left(\\mathrm{Alg}_{m}\\left(\\bar{\\mathcal{L}}^{P}\\right),P\\right)$ as a ", "page_idx": 14}, {"type": "text", "text": "congruence by the union of the tautological congruence relations of the logics belonging to $P_{i},i\\in I$ . ", "page_idx": 15}, {"type": "text", "text": "7. Sig contains at least one non-empty set. ", "page_idx": 15}, {"type": "text", "text": "Our L-Reg aims to regularize the semantics extracted by $g$ and the classifier to satisfy condition 1. ", "page_idx": 15}, {"type": "text", "text": "$\\vdash_{(h\\circ g(X),Y)}{=}|{=}_{(g(X_{s}),Y_{s})}$ in Eq. (6) can be safely omitted in the rest of the paper. Consider the logic formed on $X,Y\\colon\\mathcal{L}_{(X_{s},Y_{s})}\\,=\\,\\left\\langle F_{(X_{s},Y_{s})},D,\\middle|\\overline{{\\Omega}}_{(X_{s},Y_{s})},h,\\vdash_{(h(X),Y)}\\right\\rangle$ . Assume we want to study the logic of $\\vdash$ which can be defined in the form of ${\\mathcal{L}}_{\\vdash}$ : $\\ensuremath{\\underline{{\\underline{{\\mathrm{def}}}}}}^{\\mathrm{~\\underline{{def}}}}\\left\\langle F_{X_{s},Y_{s}},D_{\\vdash},h_{\\vdash},\\vert\\alpha_{\\vdash}\\right\\rangle$ , where $D_{\\vdash},h_{\\vdash},\\left\\vert=_{\\vdash}\\right.$ are pseudo-components associated with $\\vdash$ . Particularly, $D_{\\vdash}$ is a subset of all possible world/domains from $F_{(X_{s},Y_{s})}$ : $D_{\\vdash}\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\{T\\ \\subseteq\\ F_{(X_{s},Y_{s})}\\,:\\,T$ is closed under $\\vdash_{(h(X),Y)}\\}$ . For any $T\\in D_{\\mathsf{vdash}}$ and $a\\in F_{(X_{s},Y_{s})}$ , it has $h_{\\vdash}(a,T)\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\{b\\in F:T\\vdash(a\\leftrightarrow b)\\}.$ . Further, $\\leftrightharpoons$ in $T\\in D_{\\mathsf{vdash}}$ is defined as $T\\models_{\\mathsf{{\\boldsymbol{\\mathsf{\\Pi}}}}}a$ d\u21d4ef $a\\in T$ . [4] points out that the following condition is almost always satisfied: (Cond) $\\forall a,b\\in F_{\\vdash},d\\in D_{\\vdash}$ , we have $(h_{\\vdash}(a,d)=h_{\\vdash}(b,d))$ and $d\\left\\vert=_{\\mathsf{vdash}}a\\Rightarrow d\\left\\vert=_{\\mathsf{\\vdash}}b$ . Therefore, the semantical consequence relation induced by $\\leftrightharpoons$ coincides with the original syntactical $\\vdash_{(h\\circ g(X),Y)}$ while Cond holds. Due to that $D_{\\vdash}\\subseteq D$ , it infers that $\\left|=_{\\left(g\\left(X_{s}\\right),Y_{s}\\right)}\\right.$ coincides with $\\leftrightharpoons$ . Therefore, $\\vdash_{(h\\circ g(X),Y)}{=}|{=}_{(g(X_{s}),Y_{s})}$ can be safely omitted in the rest of the paper. ", "page_idx": 15}, {"type": "text", "text": "C Details of proofs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proposition C.1 (L-Reg reduces the complexity of the model, promoting data-shift generalization performance.). Assume the domain gap is well minimized. Consider a $f^{*}$ is the target model that generalizes to the unseen with the lowest complexity. There are f (RXs,Ys), f(Xs,Ys) trained under the setting of data-shift generalization (i.e., $(X_{s},Y_{s})$ is accessible and $y_{s}=y_{u,}$ ), it has that: ", "page_idx": 15}, {"type": "equation", "text": "$$\nG L(f_{(X_{s},Y_{s})}^{R},f^{*},X_{u})\\leq G L(f_{(X_{s},Y_{s})},f^{*},X_{u}),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. We assume the loss is achieved for the tractable form by minimizing the mean squared error. In that case, we have $f_{(X_{s},Y_{s})}^{*}$ for the given training set as: ", "page_idx": 15}, {"type": "equation", "text": "$$\nf_{(X,Y)}^{*}=(g(X)^{T}g(X))^{-1}h\\circ g(X)^{T}Y_{s}=(Z^{T}Z)^{-1}h(Z^{T})Y,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "In comparison to $f^{*},\\,f_{(X_{s},Y_{s})}$ for the given seen sets is as: ", "page_idx": 15}, {"type": "equation", "text": "$$\nf_{(X_{s},Y_{s})}=(Z_{s}^{T}Z_{s})^{-1}h(Z_{s}^{T})Y_{s},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and $f_{(X_{s},Y_{s})}^{R}$ is derived from $f_{(X_{s},Y_{s})}$ , where $Z_{s}$ is constrained additionally by L-Reg and the constrained $Z_{s}$ is denoted as $Z_{s}^{R}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\nf_{(X_{s},Y_{s})}^{R}=(Z_{s}^{R\\;T}Z_{s})^{-1}h(Z_{s}^{R\\;T})Y_{s}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For simplification, we denote $(Z^{T}Z)^{-1}Z^{T}$ , $(Z_{s}^{T}Z_{s})^{-1}Z_{s}^{T}$ , and $(Z_{s}^{R\\;T}Z_{s})^{-1}Z_{s}^{R\\;T}$ as $\\mathcal{N}^{*},\\mathcal{N}$ , and ${\\mathcal N}^{R}$ , respectively. ", "page_idx": 15}, {"type": "text", "text": "The form of $\\mathcal{N}$ . For multi-domain generalization, the model is tested on the unseen domain, referring that $X_{u}$ contains some unseen semantics besides the seen: $Z_{s}\\sim\\mathcal{Z}_{s},Z_{u}\\sim\\mathcal{Z}_{u},\\mathcal{Z}_{s}\\ne\\mathcal{Z}_{u},\\mathcal{Z}_{s}\\cap\\mathcal{Z}_{u}\\ne$ $\\varnothing$ . Considering each dimension of $Z$ represents a specific semantics, we denote $\\Gamma$ as the dimensions of $Z$ that contain the seen semantics support in $X_{s}$ and $\\bar{\\Gamma}$ for the unseen, we can decompose $\\mathcal{N}$ as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\boldsymbol{\\mathcal{N}}=\\left[\\bar{\\Gamma}^{T}\\bar{\\Gamma}\\quad\\Gamma^{T}\\bar{\\Gamma}\\right]^{-1}[h(\\Gamma)\\;h(\\bar{\\Gamma})]^{T}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The form of $\\mathcal{N}^{*}$ . Assume $\\Gamma$ already contains semantic support for deducting $Y$ ; thus, $\\bar{\\Gamma}$ would not affect the deduction of $Y$ . In such case, it has that $\\Gamma^{T}\\bar{\\Gamma}=\\mathbf{\\dot{0}}$ and $\\bar{\\Gamma}^{T}\\Gamma=\\mathbf{0}$ and $\\bar{\\Gamma}^{T}\\bar{\\Gamma}={\\bf1}$ where ${\\bf0},{\\bf1}$ ", "page_idx": 15}, {"type": "text", "text": "denote zero matrix and identity matrix: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{N}^{*}=\\left[\\!\\!\\begin{array}{c c}{\\Gamma^{T}\\Gamma}&{\\mathbf{0}}\\\\ {\\mathbf{0}}&{\\mathbf{1}}\\end{array}\\!\\!\\right]^{-1}[h(\\Gamma)\\ h(\\bar{\\Gamma})]^{T}}\\\\ &{\\quad=\\left[\\!\\!\\begin{array}{c c}{(\\Gamma^{T}\\Gamma)}&{\\mathbf{0}}\\\\ {\\mathbf{0}}&{\\mathbf{1}}\\end{array}\\!\\!\\right]^{-1}[h(\\Gamma)\\ h(\\bar{\\Gamma})]^{T}}\\\\ &{\\quad=\\left[\\!\\!\\begin{array}{c c}{(\\Gamma^{T}\\Gamma)^{-1}h(\\Gamma)}\\\\ {h(\\bar{\\Gamma})}\\end{array}\\!\\!\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where we also expect $h(\\bar{\\Gamma})=\\mathbf{0}$ so that $z_{u}$ does not influence the deduction. We now have $\\mathcal{N}^{*}$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{N}^{*}=\\left[\\overset{\\Gamma^{T}\\Gamma}{\\mathbf{0}}\\quad\\mathbf{1}\\right]^{-1}[h(\\Gamma)\\;h(\\bar{\\Gamma})]^{T},\\;\\mathrm{s.t.},h(\\bar{\\Gamma})=\\mathbf{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Note that for $\\mathcal{N}$ in $f_{(X_{s},Y_{s})}$ , $\\Gamma^{T}\\bar{\\Gamma}$ and $\\bar{\\Gamma}^{T}\\Gamma$ are not constrained. Please refer to Lemma C.2. Furthermore, $h(\\bar{\\Gamma})$ is also not constrained. ", "page_idx": 16}, {"type": "text", "text": "The form of ${\\mathcal N}^{R}$ . Now we discuss the trainable $\\mathcal{N}^{R}$ obtained with the application of L-Reg. The form of $\\mathcal{N}^{R}$ is similar to $\\mathcal{N}^{*}$ . However, Eq. (11) indicates that L-Reg minimizes $||\\Gamma^{T}\\bar{\\Gamma}||_{2}$ and $||\\bar{\\Gamma}^{T}\\Gamma||_{2}$ through $-H(Y|g(\\bar{\\Gamma})),D)$ and also minimizing $||h(\\bar{\\Gamma})||_{2}$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{N}^{R}=\\left[\\!\\!\\begin{array}{l l}{\\Gamma^{T}\\bar{\\Gamma}}&{\\Gamma^{T}\\bar{\\Gamma}}\\end{array}\\!\\!\\right]^{-1}[h(\\Gamma)\\ h(\\bar{\\Gamma})]^{T},\\ \\mathrm{s.t.},\\operatorname*{min}||\\Gamma^{T}\\bar{\\Gamma}||_{2}+||\\bar{\\Gamma}^{T}\\Gamma||_{2}+|h(\\bar{\\Gamma})||_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Compare $G L(f_{(X_{s},Y_{s})}^{R},f^{*},X_{u})$ with $G L(f_{(X_{s},Y_{s})},f^{*},X_{u})$ . By comparing the forms of $\\mathcal{N}^{R},\\mathcal{N}$ and $\\mathcal{N}^{*}$ , it is obvious that $||\\mathcal{N}^{R}\\,-\\,\\mathcal{N}^{*}||_{2}~~\\le~~||\\mathcal{N}\\,-\\,\\mathcal{N}^{*}||_{2}$ . Therefore, we have that: $G L(f_{(X_{s},Y_{s})}^{R},f^{*},X_{u})\\leq G L(f_{(X_{s},Y_{s})},f^{*},X_{u})$ . \u53e3 ", "page_idx": 16}, {"type": "text", "text": "Lemma C.2 (Minimizing $H(Y|g(X),D)+\\mathcal{R}$ solely may cause generalization degradation). Minimizing $H(Y|g(X),D)+\\mathcal{R}$ solely without $L$ -Reg may confilct with $\\begin{array}{r}{\\operatorname*{max}_{h,g}H(Y|\\check{g}(\\bar{\\Gamma}),D).}\\end{array}$ , causing invalid semantics for decision process and degrading the generalization. ", "page_idx": 16}, {"type": "text", "text": "Proof. We have the following relationship for $H(Y|g(z),D)$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{c}{H(Y|g(z),D)=H(Y|g(\\bar{\\Gamma}),g(\\Gamma),D)}\\\\ {H(Y,g(\\Gamma)|g(\\bar{\\Gamma}),D)-H(g(\\Gamma)|g(\\bar{\\Gamma}),D)=H(Y|g(\\bar{\\Gamma}),g(\\Gamma),D)+H(g(\\bar{\\Gamma})|g(\\Gamma),D).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since the independence between $\\{z_{i}\\}_{i=1}^{M}$ is unconstrained, $H(Y,g(\\Gamma)|g(\\bar{\\Gamma}),D)$ may cause that $Y$ can be deducted from $\\bar{\\Gamma}$ . Therefore, $\\Gamma^{T}\\bar{\\Gamma}$ and $\\bar{\\Gamma}^{T}\\Gamma$ are not constrained even when the domain gap is minimized where $|D|=1$ , causing the sub-optimal generalization. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "D One toy example ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We present a simplified informal illustrative example to compare the efficacy of our proposed L-Reg against conventional L1 and L2 regularization methods. As depicted in Fig. 6, the ground truth (GT) image represents the underlying data, generated according to $f^{*}(x_{1},x_{2})=\\sin(2\\pi x_{1})\\cdot\\sin(2\\pi x_{2})$ , where $x_{1}$ and $x_{2}$ denote the horizontal and vertical coordinates respectively, and the pixel color corresponds to the value of $f^{*}(x_{1},x_{2})$ . The training domain is delineated by the black box, while the testing domain encompasses the area outside of this boundary. ", "page_idx": 16}, {"type": "text", "text": "For our experiments, we use a 6-linear-layer size-110 ReLU model network. Mean squared error serves as the loss function. ", "page_idx": 16}, {"type": "text", "text": "Our experimental results reveal that L-Reg enhances the model\u2019s ability to extrapolate beyond the training domain. Notably, our proposed L-Reg demonstrates superior extrapolative capabilities compared to traditional $L_{1}$ and $L_{2}$ regularization methods. This observation highlights the efficacy of L-Reg in fostering improved generalization. ", "page_idx": 16}, {"type": "image", "img_path": "Woiqqi5bYV/tmp/6612a5dfaef6ffba03800098536748f2ad0b9df90f51f2982a4a9f4682d4df15.jpg", "img_caption": ["Figure 6: Prediction visualizations of MLP with different regularization terms. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "E Apply L-Reg to ERM Baseline for mDG ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "To further validate L-Reg\u2019s efficacy for mDG, we use ERM as the baseline on the TerraIncognita dataset. For a fair comparison, all experiments share the same hyperparameter settings and use the Regnety-16gf backbone. Original ERM results are also included alongside our reproduced results. The results in Table 8 reveal that ERM with L-Reg significantly improves mDG performance (from $49.9\\%$ to $52.9\\%$ ). ", "page_idx": 17}, {"type": "text", "text": "F Compare L-Reg with more regularization terms ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We also compare L-Reg with other regularization terms: The Ortho-Reg - the orthogonality regularization that constrains the independence of each dimension of the semantic feature $z$ ; and Sparsity - implemented as Bernoulli Sample of the latent features from the sparse linear concept discovery models [41] on our used PIM backbone. To investigate this fairly, we re-implemented the Bernoulli Sample of the latent features from the Sparse Linear Concept Discovery Models [41] on the same PIM backbone that we used, to achieve the sparsity. Table 6&Table 7 demonstrate that L-Reg outperforms Ortho-Reg and Sparsity. ", "page_idx": 17}, {"type": "text", "text": "Especially, while a common sparse concept model may be able to achieve $\\gamma^{y}\\psi\\,=\\,z^{y}$ by filtering irrelevant features through the sparsity, it may not ensure $\\gamma^{y_{i}}\\neq\\gamma^{y_{j}}$ , which is crucial for disentangling features used for predicting different classes. This limitation can potentially lead to degradation in generalization performance for common sparse concept models. 6&7 indicate that while L-Reg consistently achieves overall improvement, the sparse concept-based approach does not consistently improve generalization, validating the aforementioned difference. ", "page_idx": 17}, {"type": "text", "text": "G Limitation of L-Reg and possible solutions ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "As analyzed and discussed in the paper, L-Reg is based on the precondition that each dimension of the latent features represents an independent semantic. ", "page_idx": 17}, {"type": "text", "text": "We hypothesize this is due to the fact that our L-Reg is derived based on the precondition that $z^{i},z^{j}\\in z,I\\neq j$ is independent of each other. This condition holds for most deep-layer features but may not apply to shallow layers. Thus, applying L-Reg to the semantic features from the deep layers may improve the performance for unknown classes without negatively impacting known classes. ", "page_idx": 17}, {"type": "text", "text": "Derived from this hypothesis, another possible solution is further regularizing the independence, which may lead to further improvements. To validate this hypothesis, we test L-Reg by reinforcing independence with Ortho-Reg. MDG results in Table 8 and GCD results in Table 6&Table 7 show that combining L-Reg with Ortho-Reg leads to further improvements, whereas Ortho-Reg alone may not guarantee improvements. These findings support our hypothesis and suggest that L-Reg, particularly ", "page_idx": 17}, {"type": "text", "text": "Table 6: Results of GCD: Averaged results across all datasets of PIM with different regularization applied to the latent features: Sparsity: achieved through Bernoulli Sample; Ortho-Reg: orthogonality regularization. $+\\mathrm{L}{\\mathrm{-}}\\mathrm{Reg}$ outperforms other regularization terms when they are applied solely; $+\\mathrm{L}_{}$ - Reg+Ortho-Reg achieves the best performance and alleviates the performance degradation of unknown classes, validating our hypothesis in the paper that the improper $Z$ may result in compromises and constraining the independence of each $z^{i}\\in z,z\\in Z$ may be helpful. ", "page_idx": 18}, {"type": "table", "img_path": "Woiqqi5bYV/tmp/caaed16ac255dd44cc30414c367258ff49fa6c4f5b739fddb0242929fe7e4cf9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "Woiqqi5bYV/tmp/02687e9e91bbf2be2d73abc730968ffed316062a8c3bb5ead013df5f3470ed09.jpg", "table_caption": ["Table 7: Results of GCD: Detailed results across all datasets of PIM with different regularization applied to the latent features: Sparsity: achieved through Bernoulli Sample; Ortho-Reg: orthogonality regularization. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "when applied to deep layers or in conjunction with Ortho-Reg, is beneficial. This suggests a direction for future work. ", "page_idx": 18}, {"type": "text", "text": "H More experimental details and results ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "All experiments can be conducted on one NVIDIA GeForce RTX 3090 GPU. ", "page_idx": 18}, {"type": "text", "text": "H.1 Multi-domain generalization ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Competitors. We listed results from previous important work in the mDG field for better validation. They are: MMD [33], Mixstyle [62], GroupDRO [44], IRM [5], ARM [61], VREx [30], CDANN [35], DANN [20], RSC [24], MTL [8], MLDG [31], Fish [46], ERM [53], SagNet [40], SelfReg [26], CORAL [48], mDSDI [12], MIRO [25], and GMDG [50]. Among them, GMDG is treated as our baseline since it sufficiently minimizes the domain gaps. ", "page_idx": 18}, {"type": "text", "text": "Datasets. We use PACS (4 domains, 9,991 samples, 7 classes) [32], VLCS (4 domains, 10, 729 samples, 5 classes) [18], OfficeHome (4 domains, 15,588 samples, 65 classes) [55], TerraIncognita ", "page_idx": 18}, {"type": "text", "text": "Table 8: Results of mDG: Results of using ERM as the baseline. We use the ERM method as the baseline to test L-Reg\u2019s efficacy. Ortho-Reg: orthogonality regularization. This table includes results: (1) The improved performance of L-Reg on ERM baseline. (2) Comparison between L-Reg and the Ortho-Reg on ERM baseline. (3) Using L-Reg and Ortho-Reg together yields further promotion, validating our \u2018improper $z'$ hypothesis in the Paper limitation part. The used dataset is TerraIncognita. All experiments share the same hyperparameters except the added regularization term. Each group of experiments is run with seeds [0,1,2], and the averaged results for each domain and additionally with the standard deviation (Std) are reported. ", "page_idx": 19}, {"type": "table", "img_path": "Woiqqi5bYV/tmp/be0c62544bcb5d1151af3366f9dc9d0ea174ff6073ab20105c321ec87642505d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "Woiqqi5bYV/tmp/eafdbd1cef8d5a742f62a396d1f233d6122b8738e5d81cd4556192d8d69182c1.jpg", "table_caption": ["Table 9: Parameters for mDG task "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "(TerraIncognita, 4 domains, 24, 778 samples, 10 classes) [7], and DomainNet (6 domains, 586,575 samples, 345 classes) [42]. ", "page_idx": 19}, {"type": "text", "text": "Training details. We use GMDG [50] as our baseline. Especially, we use all loss terms proposed in GMDG as $L_{m a i n}$ . The training procedure is the same as MIRO [25] and GMDG. We use seeds $0,1,2$ for all three trails training. ", "page_idx": 19}, {"type": "text", "text": "Parameters. We adhere to the parameters proposed by GMDG, particularly focusing on its recommended loss terms. Furthermore, we provide a detailed listing of the hyper-parameters pertaining to L-Reg, along with the tuned \u2018lr mult\u2019, as outlined in Table 9, to facilitate the reproducibility of our results. ", "page_idx": 19}, {"type": "text", "text": "Evaluation metric. The models undergo training on known domains and subsequent testing on unseen domains. For each trial, a distinct domain within the datasets is designated as the unseen domain. The evaluation metric reports the prediction accuracy achieved on these unseen domains. The aggregated results across all unseen domains within the datasets provide a comprehensive assessment of the algorithm\u2019s performance in domain generalization for the given datasets. ", "page_idx": 19}, {"type": "text", "text": "More results. Results of each domain for each dataset are presented in Tables 10 to 14. ", "page_idx": 19}, {"type": "text", "text": "H.2 Generalized category discovery ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Competitors. We compare our proposed method with existing generalized category discovery methods: GCD [54], and PIM [16]. In particular, PIM based on information maximization is the current state-of-the-art (SOTA) generalized category discovery method. Additionally, the traditional machine learning method, k-means [38]; three novel category discovery methods: RankStats $^+$ [22], $\\mathrm{UNO+}$ [19], ORCA [13]; and several information maximization methods: RIM [27], and TIM [11] are adapted for generalized category discovery as competitors. The results of the modified novel category discovery methods are reported in [54], and the modified information maximization methods are reported in [16]. ", "page_idx": 19}, {"type": "text", "text": "Usage details of datasets for GCD. Following the protocols of GCD and PIM [54, 16], the initial training set of each dataset is divided into labeled and unlabeled subsets; samples from half of the classes are assigned as unlabeled, and their labels are not used for training. Specifically, half of the image samples from known classes are allocated to the labeled subset, while the remaining half are assigned to the unlabeled subset. Additionally, the unlabeled subset includes all image samples from the novel classes in the original dataset. As a result, the unlabeled subset consists of instances from $K$ different classes. The detailed statistics of datasets are listed in Table 15. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "Training details. Consistent with PIM, we utilize latent features extracted by the feature encoder DINO (VIT-B/16) [14] that is pre-trained on ImageNet [17] through self-supervised learning. The losses proposed in PIM are treated as $L_{m a i n}$ . The original PIM freezes the feature extractor during the training, directly using the pre-saved extracted features as the model input. For a fair comparison, we only added one linear layer as $g$ on the extracted features, which is the minimal modification. ", "page_idx": 20}, {"type": "text", "text": "$L_{2}$ (weight decay) value searching. For a more fair comparison, we conduct weight decay value searching to ensure that the weight of $L_{2}$ is the best. To address this, we devised a methodology for weight decay searching involving the construction of smaller labeled and unlabeled subsets derived solely from the labeled data. To conduct parameter searching, we split the labeled samples to construct a \u2019smaller\u2019 sub-labeled and sub-unlabeled set. Specifically, we take $50\\%$ of the samples from known classes as sub-unlabeled samples from unknown classes. Additionally, we take $25\\%$ of the samples from the remaining $50\\%$ of known classes as sub-unlabeled samples from known classes. The remaining samples are treated as sub-labeled samples. Hyper-parameters are then searched on these sub-labeled and sub-unlabeled sets. ", "page_idx": 20}, {"type": "text", "text": "Parameters of L-Reg. The hyper-parameters of L-Reg values are shown in Table 16. ", "page_idx": 20}, {"type": "text", "text": "Evaluation metric. Following prior works [54, 16], we use the proposed accuracy metric from [54] of all classes, known classes, and unknown classes for evaluation. ", "page_idx": 20}, {"type": "text", "text": "More results. The results for each dataset are presented in Table 17. It is evident that L-Reg yields enhanced performance across half of the datasets for both known and unknown classes. On the remaining datasets, while L-Reg may slightly compromise the performance of known classes, it demonstrates significant improvements in the unknown classes, resulting in an overall enhancement in the performance across all classes. ", "page_idx": 20}, {"type": "text", "text": "More ablation results. Due to the introduction of tuned weight decay and the additional $g$ component, we have conducted ablation studies to assess their impact. The results are summarized in Table 18. It is observed that the baseline model utilizing the tuned weight decays performs slightly better than the original weight decay settings. Notably, the tuned weight decays contribute to improvements in unknown classes while often leading to slight decreases in known classes across most datasets. Inclusion of the proposed extra component $g$ results in marginal improvements in both known and unknown classes compared to the tuned baseline. Our proposed L-Reg demonstrates significant improvements specifically in the unknown classes, thereby corroborating Proposition 4.2. However, as discussed in the main paper, it is acknowledged that L-Reg may entail compromises in the performance of known classes. ", "page_idx": 20}, {"type": "text", "text": "H.3 Combination of multi-domain generalization and generalized category discovery ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Datasets. We leverage the datasets utilized in mDG tasks to construct the $\\scriptstyle\\mathrm{mDG+GCD}$ datasets. Specifically, during the seen domains of training, labels from approximately half of the classes are masked. For instance, in the PCAS dataset comprising 7 classes, classes labeled within the range $[0,1,2,3]$ are retained, while classes in [4, 5, 6] are masked. It is noteworthy that data categorized as unknown classes in our setup are from unknown classes. However, we acknowledge that this prior is not explicitly known. To align with the GCD setting, we operate under the assumption that the unlabeled set may potentially include samples from known classes. Consequently, we refrain from constraining the model by mandating that unlabeled data be classified solely as unknown classes. This adjustment introduces a more challenging generalization scenario. ", "page_idx": 20}, {"type": "text", "text": "Training details. For all experiments, the implementation directly adds L-Reg to their previously proposed loss sets. The models are trained with the aforementioned labeled and unlabeled sets from the seen domains and tested on the samples from the unseen domain. ", "page_idx": 20}, {"type": "text", "text": "Parameters. We include all the parameters for reproducing our experiments in the code. Please refer to the code for details. ", "page_idx": 20}, {"type": "text", "text": "Evaluation metric. We use the same metric from the GCD task for the mDG $^{\\cdot+}$ GCD task. Similarly, the metrics include the accuracy for all, known and unknown classes. ", "page_idx": 20}, {"type": "text", "text": "More results. The averaged results of each dataset are exhibited in Table 19, while the detailed results of each dataset are presented in Tables 20 to 24. ", "page_idx": 21}, {"type": "text", "text": "H.4 More GradCAM visualizations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We provide more visualized examples of L-Reg. Examples of known classes can be seen in Figs. 7 to 10 and unknown classes in Figs. 11 and 12. Compromises in known sets, as discussed in the limitations, can be seen in Figs. 8 and 12. ", "page_idx": 21}, {"type": "table", "img_path": "Woiqqi5bYV/tmp/d8efdfbcabdfd0f0f87c2ff5115525f711a526eac8d1c80e8bfa1606e2fafb5b.jpg", "table_caption": ["Table 10: MDG experiments on TerraIncognita: More results of full GMDG $+\\mathrm{L}$ -Reg for each category. "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "Woiqqi5bYV/tmp/91b4f3214dcad5ed96a27ffd0fc45f632cfdd71801011fcfa3ccda9034f9e311.jpg", "table_caption": ["Table 11: MDG experiments on OfficeHome: More results of full GMDG $+\\mathrm{L}$ -Reg for each category. "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "Woiqqi5bYV/tmp/636b15a749cf3fc1721e4f209f0f88667a667c0e469cef73e427621b445f4055.jpg", "table_caption": ["Table 12: MDG experiments on VLCS: More results of full GMDG+L-Reg for each category. "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "Woiqqi5bYV/tmp/8edf816feeacd93d6d48f193f9dc70fb6d26eebf1a502dae8923bc81779152e6.jpg", "table_caption": ["Table 13: MDG experiments on PACS: More results of full GMDG+L-Reg for each category. "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "Woiqqi5bYV/tmp/57c6c9accf16d438e352739d374420dc6d085c61b4cb2b3cda31227e4b0a0b7a.jpg", "table_caption": ["Table 14: MDG experiments on DomainNet: More results of full GMDG $+\\mathrm{L}$ -Reg for each category. "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "Woiqqi5bYV/tmp/641a6c65eaeea7c9ad230b6451f7c772ad8754403998c92bd79dce97eb217af7.jpg", "table_caption": ["Table 15: Statistics of datasets. "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "Woiqqi5bYV/tmp/3fb5e550a8d50d43c53a1c3cb51caf010203fd520e2b0024712aec73912e9857.jpg", "table_caption": ["Table 16: Tuned weight decay values for each dataset. "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "Woiqqi5bYV/tmp/3d0d1886b507a91e11129bd0b5119f70ce87e0958cc086fc9c76f84596fceda0.jpg", "table_caption": ["Table 17: GCD results: Accuracy scores across fine-grained and generic PIM datasets with our L-Reg and other competitors. The best results of each group are highlighted in bold. Improvement and degradation in our approach from PIM are highlighted in red and blue, respectively. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Table 18: GCD results: Accuracy scores across fine-grained and generic datasets of each setting. The best results are highlighted in bold. To eliminate the impact of hyper-parameters on performance, we also present the results of PIM with tuned hyper-parameters (termed baseline tuned). $L_{m a i n}$ denotes the losses used in PIM. $g$ denotes the transformation applied to the input features. ", "page_idx": 23}, {"type": "table", "img_path": "Woiqqi5bYV/tmp/dd5f60e10990e1d449f7b2de5c8f0b3df46d58ed92e1ee4c63243cd08fa01485.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "Woiqqi5bYV/tmp/69bd375a2dfe72c5373a37afa1b17e6707f0793f0521b8830ab9e60f8124f996.jpg", "table_caption": ["Table 19: MDG $^+$ GCD results: accuracy scores of each dataset. Improvements are highlighted in red. "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "Woiqqi5bYV/tmp/da7e8b15d625aafbba57048347342f69044945df55a558b0342df395e613ab9a.jpg", "table_caption": ["Table 20: MDG $^+$ GCD results: accuracy scores of each domain in PACS dataset. "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "Woiqqi5bYV/tmp/714889a24f13e3c545bf2bd9f1377d6646d0ce9870ed5aeba9409c68dc4f97b1.jpg", "table_caption": ["Table 21: MDG $^{+}$ GCD results: accuracy scores of each domain in HomeOffice dataset. "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "Woiqqi5bYV/tmp/42bc26327e23524f743d030b83bfdf0cecc447af24b66c2a5a2675ececcc140d.jpg", "table_caption": ["Table 22: MDG+GCD results: accuracy scores of each domain in VLCS dataset. "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "Woiqqi5bYV/tmp/76503676c99b83a909f33d6ee9c7bc3cbd47a9f72a32b646356067920327eb1f.jpg", "table_caption": ["Table 23: MDG $^+$ GCD results: accuracy scores of each domain in TerraIncognita dataset. "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "Woiqqi5bYV/tmp/fb5c8aa337c078ce51e65e07eb207440c70f32e3ab0532e364936c77bc1ef5f1.jpg", "table_caption": ["Table 24: MDG+GCD results: accuracy scores of each domain in DomainNet dataset. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "Unseen domain: Art painting ", "text_level": 1, "page_idx": 26}, {"type": "image", "img_path": "Woiqqi5bYV/tmp/b12d088c759524a04fdebbc2890823d871491fb8e5eafd2f3f64e7556c7f7c32.jpg", "img_caption": ["Figure 7: GradCAM visualizations: Baseline is GMDG. The used dataset is PACS. The model is trained under $\\mathbf{u}\\mathbf{D}\\mathbf{G}\\mathbf{+}\\mathbf{G}\\mathbf{C}\\mathbf{D}$ setting with and without L-Reg, respectively. It can be seen that for the known class \u2018dog,\u2019 the model trained with L-Reg extracts the area around the nose area for classification across all seen and unseen domains. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "Woiqqi5bYV/tmp/8680786f3806eaaafae343808a057d71f2e481c28b4d0274323b60531402b86d.jpg", "img_caption": ["Figure 8: GradCAM visualizations: Baseline is GMDG. The used dataset is PACS. The model is trained under $\\mathbf{u}\\mathbf{D}\\mathbf{G}\\mathbf{+}\\mathbf{G}\\mathbf{C}\\mathbf{D}$ setting with and without L-Reg, respectively. It can be seen that for the known class \u2018elephant,\u2019 the model trained with L-Reg extracts the shape of long noses, teeth, and big ears for classification across all seen and unseen domains. The compromise of the known sets can be seen in the sketch domain, where those features are not significant. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "", "text_level": 1, "page_idx": 28}, {"type": "image", "img_path": "Woiqqi5bYV/tmp/3bca25f68dd9f4e2afcaf47916f4d8321b8793acb52147d2920591a803e5f593.jpg", "img_caption": ["Figure 9: GradCAM visualizations: Baseline is GMDG. The used dataset is PACS. The model is trained under $\\mathbf{u}\\mathbf{D}\\mathbf{G}\\mathbf{+}\\mathbf{G}\\mathbf{C}\\mathbf{D}$ setting with and without L-Reg, respectively. It can be seen that for the known class \u2018giraffe,\u2019 the model trained with L-Reg extracts the feature of the long necks for classifying across all seen and unseen domains. "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "Woiqqi5bYV/tmp/9dc597aab4bbc4d882864036ec373ad56eda3289943abd88b2a5d14b78612436.jpg", "img_caption": ["Figure 10: GradCAM visualizations: Baseline is GMDG. The used dataset is PACS. The model is trained under $\\mathbf{u}\\mathbf{D}\\mathbf{G}\\mathbf{+}\\mathbf{G}\\mathbf{C}\\mathbf{D}$ setting with and without L-Reg, respectively. It can be seen that for the known class \u2018guitar,\u2019 the model trained with L-Reg extracts the features of the necks and the strings of the guitar for classification across all seen and unseen domains. "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "Woiqqi5bYV/tmp/409937d44d20032e97c950f39b47981e44af8a6116cb494c150602f29d8368f2.jpg", "img_caption": ["Figure 11: GradCAM visualizations: Baseline is GMDG. The used dataset is PACS. The model is trained under $\\mathbf{u}\\mathbf{D}\\mathbf{G}\\mathbf{+}\\mathbf{G}\\mathbf{C}\\mathbf{D}$ setting with and without L-Reg, respectively. It can be seen that for the unknown class \u2018horse,\u2019 the model trained with L-Reg extracts the features of the overall outline shapes of horses for classification across all seen and unseen domains. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "", "text_level": 1, "page_idx": 31}, {"type": "image", "img_path": "Woiqqi5bYV/tmp/ee3fc2fd0e6b617351ca0cbc129f72f7471b9e0aba49729536b57512b62f33da.jpg", "img_caption": ["Figure 12: GradCAM visualizations: Baseline is GMDG. The used dataset is PACS. The model is trained under $\\mathbf{u}\\mathbf{D}\\mathbf{G}\\mathbf{+}\\mathbf{G}\\mathbf{C}\\mathbf{D}$ setting with and without L-Reg, respectively. It can be seen that for the unknown class \u2018person,\u2019 the model trained with L-Reg extracts the features of human faces for classification across all seen and unseen domains. The compromise of the known sets can be seen in the sketch domain, where those faces are not drawn. "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Yes, the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 32}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: Yes, the paper discusses the limitations of the work in the Conclusion Section. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 32}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Yes, the paper provides the full set of assumptions and a complete proof in the main paper and appendix. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 33}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: This paper provides all the information needed to reproduce the main experimental results of the paper as much as possible in the appendix and supplementary materials. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 33}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper provides uses open-source data and open access to the code. The code is especially attached as supplementary material. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 34}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Yes, this paper specifies all the training and test details in the Experiments Section and the appendix. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 34}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [No] ", "page_idx": 34}, {"type": "text", "text": "Justification: Following previous work, this paper does not report error bars. However, this paper reports some results in the form of mean with standard deviation, as shown in Table 1. However, ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 34}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 35}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper provides information on the computer resources in the appendix. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 35}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: Yes, the research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 35}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: We think that there is no societal impact of the work performed. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 35}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 36}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: This paper seems to pose no such risks ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 36}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: All assets (e.g., code, data, models) used in the paper are properly credited, and the license and terms of use are explicitly mentioned and properly respected. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 37}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: New assets introduced in the paper are documented, and the documentation is provided alongside the assets. The model training code is provided in the supplementary materials. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 37}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 37}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 37}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 38}]