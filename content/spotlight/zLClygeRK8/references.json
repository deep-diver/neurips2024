{"references": [{"fullname_first_author": "Miroslav Dud\u00edk", "paper_title": "Doubly robust policy evaluation and learning", "publication_date": "2011", "reason": "This paper introduces the foundational concept of doubly robust policy evaluation, a key method used in off-policy evaluation and a basis for many subsequent works in the field."}, {"fullname_first_author": "Adith Swaminathan", "paper_title": "Batch learning from logged bandit feedback through counterfactual risk minimization", "publication_date": "2015", "reason": "This paper introduces the crucial concept of counterfactual risk minimization for offline policy learning, which is widely adopted and extended in many other offline RL works."}, {"fullname_first_author": "Ilja Kuzborskij", "paper_title": "Confident off-policy evaluation and selection through self-normalized importance weighting", "publication_date": "2021", "reason": "This paper provides a confident off-policy evaluation method that is robust against high variance in importance weights and serves as a direct competitor of the proposed method."}, {"fullname_first_author": "Germano Gabbianelli", "paper_title": "Importance-weighted offline learning done right", "publication_date": "2024", "reason": "This paper provides a state-of-the-art pessimistic off-policy learning method that is directly compared with the proposed method in the experimental section."}, {"fullname_first_author": "Otmane Sakhi", "paper_title": "PAC-Bayesian Offline Contextual Bandits with Guarantees", "publication_date": "2023", "reason": "This paper, also authored by some of the current authors, introduces a previous PAC-Bayesian based approach to offline contextual bandits that is directly extended in the current work."}]}