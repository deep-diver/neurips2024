[{"figure_path": "zLClygeRK8/figures/figures_4_1.jpg", "caption": "Figure 1: LS with different \u03bbs.", "description": "This figure shows the Logarithmic Smoothing (LS) estimator with different \u03bb values.  The x-axis represents the importance weight \u03c9\u03c0, and the y-axis represents the value of the LS estimator for various values of \u03bb.  The figure also includes lines for the standard IPS estimator and the Clipping estimator for comparison, illustrating the bias-variance trade-off of the LS estimator as \u03bb changes.  As \u03bb increases, the variance of the LS estimator decreases but its bias increases.", "section": "3.3 Logarithmic smoothing"}, {"figure_path": "zLClygeRK8/figures/figures_9_1.jpg", "caption": "Figure 2: Results for OPE and OPS experiments.", "description": "This figure shows the results of experiments comparing different methods for off-policy evaluation (OPE) and off-policy selection (OPS).  The left panel shows the cumulative distribution of the relative radius of different upper bounds on the risk, demonstrating that the Logarithmic Smoothing (LS) bound is generally tighter than competing bounds. The right panel shows the performance of various selection strategies on the task of choosing better performing policies than a logging policy, illustrating the advantages of using pessimistic strategies based on tighter upper bounds. ", "section": "5.1 Off-policy evaluation and selection experiments"}, {"figure_path": "zLClygeRK8/figures/figures_19_1.jpg", "caption": "Figure 3: Proposition 1 for different values of L and with different regularized IPS h.", "description": "This figure shows the results of evaluating the empirical moments bound (Proposition 1) for different values of L (moment order) and different regularized IPS functions (IPS, Clipped IPS, Implicit Exploration, Exponential Smoothing).  It demonstrates how the tightness of the upper bound varies with the number of moments used and different regularizers. The experiment was performed on the 'balance-scale' dataset, with parameters \u03bb = \u221a1/n and a fixed policy with R(\u03c0) = -0.93.", "section": "3.1 Preliminaries and unified risk bounds"}, {"figure_path": "zLClygeRK8/figures/figures_19_2.jpg", "caption": "Figure 1: LS with different \u03bbs.", "description": "The figure shows the impact of the logarithmic smoothing parameter \u03bb on the Logarithmic Smoothing estimator.  When \u03bb = 0, the estimator is equivalent to the standard IPS estimator. As \u03bb increases, the importance weights are smoothed, leading to a bias-variance tradeoff. The plot shows how the LS estimator behaves for different values of \u03bb, demonstrating its ability to control bias and variance.", "section": "3.3 Logarithmic smoothing"}, {"figure_path": "zLClygeRK8/figures/figures_42_1.jpg", "caption": "Figure 2: Results for OPE and OPS experiments.", "description": "This figure shows the results of experiments conducted for off-policy evaluation (OPE) and off-policy selection (OPS). The left panel compares the tightness of different risk upper bounds in OPE by plotting the cumulative distribution of their relative radiuses. The right panel shows the performance of different policy selection strategies (IPS, SN, SN-ES, CIPS-EB, IX, CIPS-L=1, and LS) by displaying the percentage of times each strategy selected the best, better, or worse policies than the behavior policy.", "section": "5.1 Off-policy evaluation and selection experiments"}, {"figure_path": "zLClygeRK8/figures/figures_42_2.jpg", "caption": "Figure 2: Results for OPE and OPS experiments.", "description": "This figure presents the results of experiments conducted for off-policy evaluation (OPE) and off-policy selection (OPS). The left subplot shows a comparison of the tightness of different upper bounds used for OPE, illustrating how the Logarithmic Smoothing (LS) bound provides tighter estimates than existing methods.  The right subplot illustrates the performance of various policy selection strategies, highlighting the effectiveness of pessimistic selection approaches using tighter bounds in selecting better-performing policies and avoiding worse ones.", "section": "5 Experiments"}]