[{"figure_path": "thUf6ZBlPp/figures/figures_3_1.jpg", "caption": "Figure 1: Target probability distributions (black dashed curves) on the interval [-1,1] (left), the unit circle (middle), and the real line (right), and their approximations by orthogonal function expansions from different families and of different orders; see Eq. 2 and Table 1.", "description": "This figure shows how various probability distributions can be approximated using orthogonal function expansions.  It demonstrates the flexibility of this approach by showing examples on three different types of support: the interval [-1,1], the unit circle, and the real line. Each example shows a target distribution (black dashed curve) and its approximations using different numbers of basis functions from different families (Legendre polynomials, Fourier series, and Hermite polynomials). The approximations become more accurate as the number of basis functions increases.", "section": "Score-based variational inference with orthogonal function expansions"}, {"figure_path": "thUf6ZBlPp/figures/figures_6_1.jpg", "caption": "Figure 1: Target probability distributions (black dashed curves) on the interval [-1,1] (left), the unit circle (middle), and the real line (right), and their approximations by orthogonal function expansions from different families and of different orders; see Eq. 2 and Table 1.", "description": "This figure demonstrates how orthogonal function expansions of various orders and from different families (Legendre polynomials, Fourier series, and Hermite polynomials) can approximate various target probability distributions.  Each row presents a different target distribution and shows how the approximation improves as more terms are included in the expansion.  The black dashed curves show the target distributions, while the colored curves illustrate approximations of increasing order (i.e., number of basis functions). This visualizes the expressiveness of the proposed method for approximating various types of distributions.", "section": "Score-based variational inference with orthogonal function expansions"}, {"figure_path": "thUf6ZBlPp/figures/figures_7_1.jpg", "caption": "Figure 3: 2D target functions (column 1): a 3-component Gaussian mixture distribution (row 1), a funnel distribution (row 2), and a cross distribution (row 3). We report the KL(p; q) for the resulting optimal variational distributions obtained using score-based VI with a Gaussian variational family (column 2) and the EigenVI variational family (columns 3\u20135), where K = K1K2.", "description": "This figure compares the performance of EigenVI against a standard score-based VI method using Gaussian variational family for approximating three different 2D distributions: a 3-component Gaussian mixture, a funnel distribution, and a cross distribution. Each row represents a different target distribution. The first column shows the target distribution. The second column shows the approximation using score-based VI with a Gaussian variational family. The remaining columns (3-5) show the approximations obtained by EigenVI with increasing number of basis functions (K = K1K2). KL(p; q) values are provided for each approximation, quantifying the difference between the approximation and the target distribution.  EigenVI demonstrates improved accuracy with a higher number of basis functions.", "section": "4.1 2D synthetic targets"}, {"figure_path": "thUf6ZBlPp/figures/figures_8_1.jpg", "caption": "Figure 4: Sinh-arcsinh normal distribution synthetic target. Panel (a) shows the three targets we consider in 2D, and their resulting EigenVI fit. Panel (b) shows measures KL(p; q) for D = 2, and panel (c) shows KL(p; q) for D = 5; the x-axis shows the number of basis functions, K=\u03a0\u0105 Ka.", "description": "This figure presents a study on the performance of EigenVI on synthetic data generated using the sinh-arcsinh distribution, which allows for control over skewness and tail heaviness.  Panel (a) visually displays three 2D target distributions and their corresponding EigenVI approximations. Panels (b) and (c) show the KL divergence (a measure of the difference between the true and approximated distributions) for different numbers of basis functions (K) used in EigenVI, comparing its performance to ADVI and BaM (both Gaussian-based methods), for dimensions 2 and 5 respectively.  The results demonstrate EigenVI's ability to accurately model distributions with varying degrees of non-Gaussianity.", "section": "4.2 Non-Gaussianity: varying skew and tails in the sinh-arcsinh distribution"}, {"figure_path": "thUf6ZBlPp/figures/figures_8_2.jpg", "caption": "Figure 3: 2D target functions (column 1): a 3-component Gaussian mixture distribution (row 1), a funnel distribution (row 2), and a cross distribution (row 3). We report the KL(p; q) for the resulting optimal variational distributions obtained using score-based VI with a Gaussian variational family (column 2) and the EigenVI variational family (columns 3\u20135), where K = K1K2.", "description": "This figure compares the performance of EigenVI and a Gaussian variational family for approximating various 2D probability distributions.  The three rows show different target distributions: a three-component Gaussian mixture, a funnel shape, and a cross shape. The first column displays the target distributions, while the following columns present their approximations using a Gaussian variational family (score-based VI) and EigenVI with increasing numbers of basis functions (K = K1*K2). The KL divergence (KL(p;q)) is provided for each approximation to measure the closeness to the target distribution.", "section": "4.1 2D synthetic targets"}, {"figure_path": "thUf6ZBlPp/figures/figures_9_1.jpg", "caption": "Figure 3: 2D target functions (column 1): a 3-component Gaussian mixture distribution (row 1), a funnel distribution (row 2), and a cross distribution (row 3). We report the KL(p; q) for the resulting optimal variational distributions obtained using score-based VI with a Gaussian variational family (column 2) and the EigenVI variational family (columns 3\u20135), where K = K1K2.", "description": "This figure compares the performance of EigenVI and a Gaussian variational family for approximating three different 2D target distributions: a 3-component Gaussian mixture, a funnel distribution, and a cross distribution.  Each row represents one target distribution with its approximation by Gaussian score-based VI and EigenVI using increasing numbers of basis functions (K). KL divergence (KL(p; q)) is shown for each approximation, illustrating EigenVI's ability to achieve lower KL values with more basis functions, indicating better approximation accuracy.", "section": "4.1 2D synthetic targets"}, {"figure_path": "thUf6ZBlPp/figures/figures_9_2.jpg", "caption": "Figure 3: 2D target functions (column 1): a 3-component Gaussian mixture distribution (row 1), a funnel distribution (row 2), and a cross distribution (row 3). We report the KL(p; q) for the resulting optimal variational distributions obtained using score-based VI with a Gaussian variational family (column 2) and the EigenVI variational family (columns 3\u20135), where K = K1K2.", "description": "This figure demonstrates the performance of EigenVI on three 2D synthetic target distributions: a 3-component Gaussian mixture, a funnel distribution, and a cross distribution.  Each row shows a different target distribution, with the first column displaying the true distribution. Subsequent columns illustrate the results of different variational inference methods, using both a Gaussian variational family and the EigenVI family with increasing numbers of basis functions (K). The KL divergence is reported for each approximation to quantify the closeness between the approximation and the true distribution.  The results show that EigenVI, with sufficient basis functions, produces more accurate approximations compared to the Gaussian variational family.", "section": "4.1 2D synthetic targets"}, {"figure_path": "thUf6ZBlPp/figures/figures_19_1.jpg", "caption": "Figure 3: 2D target functions (column 1): a 3-component Gaussian mixture distribution (row 1), a funnel distribution (row 2), and a cross distribution (row 3). We report the KL(p; q) for the resulting optimal variational distributions obtained using score-based VI with a Gaussian variational family (column 2) and the EigenVI variational family (columns 3\u20135), where K = K1K2.", "description": "This figure compares the performance of EigenVI and a Gaussian variational family on three different 2D target distributions: a Gaussian mixture, a funnel, and a cross.  Each row shows the target distribution and then four different approximations: one from a Gaussian variational approximation using score-based VI and then three using EigenVI with an increasing number of basis functions (K). KL divergence is used to measure the closeness of approximation, with lower values indicating better performance. The results show that EigenVI yields better approximation compared to the Gaussian approach.", "section": "4.1 2D synthetic targets"}, {"figure_path": "thUf6ZBlPp/figures/figures_21_1.jpg", "caption": "Figure E.2: Targets (top) for the 5D sinh-arcsinh normal distribution example and EigenVI fits (bottom) with the KL divergence in the figure title.", "description": "This figure visualizes the results of applying EigenVI to 5D sinh-arcsinh normal distributions with varying levels of skew and tail weight.  The top row shows the true target distributions, demonstrating different degrees of non-Gaussianity. The bottom row displays the corresponding approximations generated by EigenVI, along with the Kullback-Leibler (KL) divergence values quantifying the difference between the true and approximated distributions. The figure illustrates EigenVI's ability to model complex, non-Gaussian distributions in higher dimensions.", "section": "E.3 Sinh-arcsinh targets"}, {"figure_path": "thUf6ZBlPp/figures/figures_22_1.jpg", "caption": "Figure 3: 2D target functions (column 1): a 3-component Gaussian mixture distribution (row 1), a funnel distribution (row 2), and a cross distribution (row 3). We report the KL(p; q) for the resulting optimal variational distributions obtained using score-based VI with a Gaussian variational family (column 2) and the EigenVI variational family (columns 3\u20135), where K = K1K2.", "description": "This figure compares the performance of EigenVI and score-based VI with a Gaussian variational family on three different 2D target distributions: a 3-component Gaussian mixture, a funnel distribution, and a cross distribution.  For each target distribution and method, the figure shows the target distribution and several approximations from EigenVI at increasing values of K (K1 and K2 being the number of basis functions used in each dimension, such that K = K1K2). The Kullback-Leibler divergence (KL(p;q)) between the true distribution and the approximation is also given for each.  The results show that EigenVI produces better approximations, particularly with increasing K.", "section": "4.1 2D synthetic targets"}, {"figure_path": "thUf6ZBlPp/figures/figures_23_1.jpg", "caption": "Figure 3: 2D target functions (column 1): a 3-component Gaussian mixture distribution (row 1), a funnel distribution (row 2), and a cross distribution (row 3). We report the KL(p; q) for the resulting optimal variational distributions obtained using score-based VI with a Gaussian variational family (column 2) and the EigenVI variational family (columns 3\u20135), where K = K1K2.", "description": "This figure compares the performance of EigenVI and score-based VI with a Gaussian variational family on three different 2D target distributions: a 3-component Gaussian mixture, a funnel distribution, and a cross distribution.  It demonstrates how increasing the number of basis functions (K) in EigenVI leads to progressively better approximations of the target distribution, as measured by the Kullback-Leibler (KL) divergence.  The Gaussian variational family serves as a baseline, showing the advantage of EigenVI's flexibility in handling complex, non-Gaussian distributions.", "section": "4.1 2D synthetic targets"}]