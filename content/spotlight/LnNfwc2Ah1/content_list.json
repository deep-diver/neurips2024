[{"type": "text", "text": "Tolerant Algorithms for Learning with Arbitrary Covariate Shift ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Surbhi Goel Department of Computer Science University of Pennsylvania surbhig@seas.upenn.edu ", "page_idx": 0}, {"type": "text", "text": "Abhishek Shetty\u2217 Department of EECS UC Berkeley shetty@berkeley.edu ", "page_idx": 0}, {"type": "text", "text": "Konstantinos Stavropoulos\u2020 Department of Computer Science UT Austin kstavrop@utexas.edu ", "page_idx": 0}, {"type": "text", "text": "Arsen Vasilyan\u2021 Department of EECS UC Berkeley arsenvasilyan@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study the problem of learning under arbitrary distribution shift, where the learner is trained on a labeled set from one distribution but evaluated on a different, potentially adversarially generated test distribution. We focus on two frameworks: $P Q$ learning [GKKM20], allowing abstention on adversarially generated parts of the test distribution, and TDS learning [KSV24b], permitting abstention on the entire test distribution if distribution shift is detected. All prior known algorithms either rely on learning primitives that are computationally hard even for simple function classes, or end up abstaining entirely even in the presence of a tiny amount of distribution shift. ", "page_idx": 0}, {"type": "text", "text": "We address both these challenges for natural function classes, including intersections of halfspaces and decision trees, and standard training distributions, including Gaussians. For PQ learning, we give efficient learning algorithms, while for TDS learning, our algorithms can tolerate moderate amounts of distribution shift. At the core of our approach is an improved analysis of spectral outlier-removal techniques from learning with nasty noise. Our analysis can (1) handle arbitrarily large fraction of outliers, which is crucial for handling arbitrary distribution shifts, and (2) obtain stronger bounds on polynomial moments of the distribution after outlier removal, yielding new insights into polynomial regression under distribution shifts. Lastly, our techniques lead to novel results for tolerant testable learning [RV23], and learning with nasty noise. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Despite the tremendous progress of machine learning, real-world deployment and use of machine learning models has proven challenging. A major reason for this is distribution shift, which occurs when the model is trained on one distribution $\\mathcal{D}^{\\mathrm{train}}$ over $\\mathcal{X}\\times\\{\\pm1\\}$ , while the data during deployment comes from a different distribution $\\mathcal{D}^{\\mathrm{test}}$ . In such scenarios, a model can unexpectedly make incorrect predictions, leading to loss of reliability, as well as erosion of trust in the machine learning system itself. Among many other critical applications, distribution shift continues to be a major challenge in healthcare applications $[Z\\mathrm{BL}^{+}18\\$ , SS20, $\\mathrm{WOD}^{+}21$ , $\\mathrm{TCK}^{+}22\\$ ]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Handling distribution shift when $\\mathcal{D}^{\\mathrm{train}}$ and $\\mathcal{D}^{\\mathrm{test}}$ are allowed to be arbitrary is known to be impossible [DLLP10]. To circumvent this impossibility, recent works [GKKM20, KK21, KSV24b, GHMS24, KSV24a] allow the machine learning model to additionally abstain (not make a prediction) on some or all of the inputs. These frameworks generalize standard PAC learning, requiring the algorithm to abstain from making predictions rather than giving incorrect predictions. In this work, we focus on two such frameworks for binary classification: ", "page_idx": 1}, {"type": "text", "text": "PQ learning [GKKM20, KK21], requiring the learning algorithm to output a selective classifier ${\\widehat{f}}.$ which is allowed to abstain on some inputs and simultaneously satisfy: (i) $\\epsilon$ -accuracy: the probabili ty that $\\widehat{f}$ does not abstain and incorrectly classifies an input $\\mathbf{x}$ from the test distribution $\\mathcal{D}^{\\mathrm{test}}$ is at most $\\epsilon$ , and (ii) $\\epsilon$ -rejection rate: the probability that $\\bar{\\widehat{f}}$ abstains on an input $\\mathbf{x}$ from the original distribution $\\mathcal{D}^{\\mathrm{train}}$ is at most $\\epsilon$ . In particular, this implies that $\\widehat{f}$ abstains on $\\mathcal{D}^{\\mathrm{test}}$ with probability at most $\\mathrm{d}_{\\mathrm{TV}}(\\mathcal{D}^{\\mathrm{train}},\\mathcal{D}^{\\mathrm{test}})+\\epsilon,$ , i.e. the probability of abstentio n deteriorates only in proportion to the amount of distribution shift. ", "page_idx": 1}, {"type": "text", "text": "Testable Distribution Shift (TDS) [KSV24b], allowing the classifier to abstain on the entire distribution $\\mathcal{D}^{\\mathrm{test}}$ if any distribution shift is detected. If there is no distribution shift, then the classifier is \u03f5-accurate on Dtest. ", "page_idx": 1}, {"type": "text", "text": "Prior known algorithms for both these settings have strong inherent limitations, making them impractical for real-world scenarios. For PQ learning, all known algorithms require access to oracles that are computationally inefficient even for the most basic concept classes and training distributions. For example, even for the most basic class of halfspaces (linear separators) over $\\mathbb{R}^{d}$ under the Gaussian training distribution, no PQ learning algorithm has run-time better than 2d\u2126(1). On the other hand, TDS learning algorithms, while being computationally efficient, reject entire test sets even when the test set has a tiny amount of distribution shift. For example, the algorithms of [KSV24b], use the low-degree moment-matching approach, which can reject distributions $D^{\\mathrm{test}}\\ne D^{\\mathrm{train}}$ even when $\\mathrm{d}_{\\mathrm{TV}}(\\bar{D^{\\mathrm{train}}},\\bar{D^{\\mathrm{test}}})=o(\\epsilon)$ . ", "page_idx": 1}, {"type": "text", "text": "1.1 Our results ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this work, we overcome both these limitations using a unified approach: spectral outlier removal [DKS18] in tandem with strong polynomial approximation results in terms of $\\mathcal{L}_{2}$ -sandwiching [KSV24b]. For PQ learning, we give the first dimension-efficient learning algorithms. For TDS learning, we give the first tolerant TDS learners that accept test sets with moderate amount of distribution shift in TV distance, ${\\mathrm{d}}_{\\mathrm{TV}}(D^{\\mathrm{train}},D^{\\mathrm{test}})=O(\\epsilon)$ . We summarize our results in Table 1. ", "page_idx": 1}, {"type": "table", "img_path": "LnNfwc2Ah1/tmp/6c9da31069b5043fdf74ef4d761142571a424111bd3c760bc22c99d77d2ea0dc.jpg", "table_caption": [], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "Table 1: Summary of our results for PQ learning and tolerant TDS learning. Except for the first row, all results are for the agnostic noise model. ", "page_idx": 1}, {"type": "text", "text": "Application: Testable Agnostic Learning. Our techniques give new learning algorithms in the testable agnostic learning framework of [RV23]. Testable learning does not address distribution shift, as it assumes that the training and testing distributions are the same. Similarly to the TDS learning algorithms of [KSV24b], all known testable agnostic learning algorithms are based either entirely [RV23, GKK23] or partially [GKSV23, $\\mathrm{DKK}^{+}\\bar{2}3$ , GKSV24] on low-degree moment matching4, and are subsequently not tolerant to small amounts of violations of the testing assumption in TV distance. We give the first tolerant testable learning algorithms for a number of function classes, including Halfspaces and low-depth formulas (see Table 2 in Appendix D for details). ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Application: Learning with Nasty Noise. As a corollary of our tolerant agnostic learning algorithms we obtain algorithms that withstand an $\\Omega(\\epsilon)$ amount of nasty noise corruption, and produce classifiers with an error at most $\\epsilon$ . (In this setting $\\Omega(\\epsilon)$ fraction of both labels and examples given to the algorithm are corrupted). The error bound of $\\epsilon$ compares favorably with the bound of [KKM18] under $\\Omega(\\epsilon)$ nasty noise, which is $\\sqrt{\\epsilon}$ . Compared with the results of [DKS18] in the nasty noise setting, our results are incomparable (see relevant discussion in Section 3, Appendix D.2 for more information). ", "page_idx": 2}, {"type": "text", "text": "1.2 Our Techniques ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To explain our technical approach, we focus of the PQ learning setting (in TDS learning setting and testable agnostic learning setting, our approach is analogous). If the TV distance between $\\mathcal{D}^{\\mathrm{test}}$ and the training distribution $\\mathcal{D}^{\\mathrm{train}}$ is at most $\\mathrm{d}_{\\mathrm{TV}}(\\ensuremath{\\mathcal{D}}^{\\mathrm{train}},\\ensuremath{\\mathcal{D}}^{\\mathrm{test}})$ , then we think of the dataset as consisting of $1\\,{\\stackrel{\\cdot}{-}}\\,\\mathrm{d}_{\\mathrm{TV}}\\!\\left({\\cal D}^{\\mathrm{train}},{\\cal D}^{\\mathrm{test}}\\right)$ fraction of inliers and an $\\mathrm{d}_{\\mathrm{TV}}(\\ensuremath{\\mathcal{D}}^{\\mathrm{train}},\\ensuremath{\\mathcal{D}}^{\\mathrm{test}})$ fraction of outliers. In order to accomplish PQ learning, we aim to remove a portion of the test set while (i) ensuring that a learning algorithm based on low-degree polynomial regression [KOS08] works on the remaining data (ii) not removing more than $\\epsilon$ fraction of the inliers5. ", "page_idx": 2}, {"type": "text", "text": "It was known from [KSV24b] that the degree- $k$ polynomial regression performs correctly if the dataset satisfies the degree- $k$ moment-matching test. Despite its power, the low-degree moment test can reject distributions even $\\epsilon/d^{O(k)}$ -close to the reference distribution. However (i) it is not clear how to efficiently prune the dataset, so the remaining datapoints satisfy the moment-matching condition (ii) even if one could do this efficiently, this can require one remove a constant fraction of inliers. To overcome this issue, we introduce the notion of low-degree spectral boundedness, which requires that for every degree- $\\cdot k$ polynomial $p$ the expectation $\\bar{\\mathbb{E}_{{\\mathbf{x}}\\sim\\mathcal{D}}}\\bar{[p({\\mathbf{x}})^{2}]}$ does not exceed the analogous expectation with respect to the reference distribution by more than a desired factor. Our first key insight is that by using the notion of $\\mathcal{L}_{2}$ -sandwiching polynomials [KSV24b], for many settings the low-degree moment matching test can be replaced by this low-degree spectral boundedness test. ", "page_idx": 2}, {"type": "text", "text": "If our dataset does not satisfy low-degree spectral boundedness, our second key insight is to make it do so by removing outliers. As in many other algorithms based on outlier removal6 (see e.g. $[{\\mathrm{DKK}}^{+}19]$ , LRV16, HLZ20, Ste18, DK19, DV04] and references therein), our outlier-removal algorithm repeatedly finds regions in $\\mathbb{R}^{d}$ , such that at least $1-\\epsilon$ fraction of points in them are outliers. This way, as we remove all the points in such outlier-rich regions, we will not remove too many inliers. Finally, we find such outlier-rich regions efficiently using a spectral approach. Specifically, if the dataset $S$ does not satisfy the low-degree spectral boundedness, then there is some polynomial $p$ for which $\\mathbb{E}_{x\\sim S}[p(\\mathbf{x})^{2}]$ is much greater than the corresponding expectation over the reference distribution. We infer that, for an appropriate value of $\\tau$ , at least $1-\\epsilon$ fraction of points in the region $\\{\\mathbf{x}:\\,p(\\mathbf{x})^{2}>\\tau\\}$ are outliers. ", "page_idx": 2}, {"type": "text", "text": "1.3 Related work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Domain Adaptation. During the last two decades, there has been a long line of works in domain adaptation literature (see, e.g., [BDBCP06, $\\mathrm{BDBC^{+}}10$ , MMR09, ${\\tt B C K^{\\bar{+}}07}$ , DLLP10, $\\mathrm{RMH}^{+}20\\$ , KZZ24] and references therein), aiming to provide generalization bounds for the error on the test distribution, after training using only labeled examples from the training distribution. However, the generalization bounds provided involve distances between the training and test marginals that typically involve enumerations over the whole concept class and no efficient algorithms for estimating or even testing such distances directy are available. ", "page_idx": 2}, {"type": "text", "text": "PQ Learning. The PQ learning framework was defined by [GKKM20], which showed that a PQ learner can be efficiently implemented using an oracle to a distribution-free agnostic learner. In follow-up work by [KK21], it was shown that distribution-free PQ learning is actually equivalent to distribution-free agnostic reliable learning, which is a learning primitive known to be hard even for the fundamental class of halfspaces $\\langle\\exp(\\Omega({\\sqrt{d}}))$ time is believed to be necessary). Here, we show how to take advantage of standard assumptions on the training marginal (e.g., Gaussianity) in order to obtain the first dimension-efficient results for PQ learning of several fundamental concept classes. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "TDS Learning. Testable learning with distribution shift was defined recently by [KSV24b], where dimension-efficient algorithms for several concept classes including halfspaces, halfspace intersections, decision trees and boolean formulas were provided. In this work, we give similar results for each of these classes in the tolerant TDS learning framework. Further work by [KSV24a] provided improved guarantees for TDS learning halfspace intersections in the realizable case. We believe that our techniques can likely be used to provide similar improvements for tolerant TDS learning, but, for ease of exposition, we do not include such results in this work. ", "page_idx": 3}, {"type": "text", "text": "Tolerant Distribution Testing: The notion of tolerance in property testing was introduced in [PRR06] and has been the focus of many works including [FF05, VV11, ${\\tt B C E}^{\\bar{+}}19$ , RV20, CJKL22, $\\mathrm{CFG}^{+}22$ , BH18, CP23]. However, over $\\mathbf{\\bar{R}}^{d}$ all existing tolerant distribution testing algorithms (such as [VV11]) have run-times and sample complexities of $2^{\\Omega(d)}$ , which greatly exceeds our run-times. ", "page_idx": 3}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Notation. For details on the notation, see Appendix A. We denote with $\\mathbf{x}^{\\otimes k}$ the vector of monomials of degree $k$ of $\\mathbf{x}\\in\\mathbb{R}^{d}$ , i.e., $\\mathbf{x}^{\\otimes k}$ is a vector of length $d^{k}$ with elements of the form $\\begin{array}{r}{\\mathbf{x}^{r}=\\prod_{i=1}^{d}\\mathbf{x}^{r_{i}}}\\end{array}$ , where $\\begin{array}{r}{\\sum_{i\\in[d]}r_{i}\\le k,r_{i}\\in\\mathbb{N},r=(r_{1},\\ldots,r_{d})}\\end{array}$ and $k$ is the degree of $\\mathbf{x}^{r}$ . A polynomial $p$ over $\\mathbb{R}^{d}$ is a function $\\begin{array}{r}{\\dot{p}({\\mathbf x})=\\sum_{r\\in\\mathbb{N}^{d}}p_{r}{\\mathbf x}^{r}=p^{\\top}{\\mathbf x}^{\\otimes d}}\\end{array}$ , where we abuse the notation to denote with $p$ the vector of coefficients of the corresponding polynomial. A polynomial $p$ over $\\{\\pm1\\}^{d}$ is defined similarly, but all of the coefficients corresponding to monomials $\\mathbf{x}^{r}$ where $r_{i}>1$ for some $i$ are zero. ", "page_idx": 3}, {"type": "text", "text": "Learning Setting. We consider distribution $\\mathcal{D}$ over $\\mathcal{X}$ and $\\mathcal{D}^{\\mathrm{train}}$ , $\\mathcal{D}^{\\mathrm{test}}$ distributions over $\\mathcal{X}\\times\\{\\pm1\\}$ such that the marginal on $\\mathcal{X}$ of $\\mathcal{D}^{\\mathrm{train}}$ is $\\mathcal{D}$ and the marginal of $\\mathcal{D}^{\\mathrm{test}}$ is $\\mathcal{D}_{\\mathcal{X}}^{\\mathrm{test}}$ . We also consider some concept class $\\mathcal{F}\\subseteq\\{\\mathcal{X}\\to\\{\\pm1\\}\\}$ . The learner is given access to labeled examples from $\\mathcal{D}^{\\mathrm{train}}$ as well as unlabeled examples from $\\mathcal{D}_{\\mathcal{X}}^{\\mathrm{test}}$ and the goal is to produce some hypothesis with low error on $\\mathcal{D}^{\\mathrm{test}}$ , but is also allowed to abstain from predicting either on specific points (for PQ learning, Def. 4.1) or even the entire distribution (for TDS learning, Def. 5.1) if distribution shift is detected. ", "page_idx": 3}, {"type": "text", "text": "In the realizable setting, the labels of both the training distribution and the test distribution are generated according to some concept $f^{*}\\in\\mathcal{F}$ and the training examples are of the form $(\\mathbf{x},f^{*}(\\mathbf{x}))$ , where $\\mathbf{x}\\sim\\mathcal{D}$ . The target test error is $\\epsilon$ for some arbitrarily chosen $\\epsilon\\in(0,1)$ . In the agnostic setting, the distributions $\\mathcal{D}^{\\mathrm{train}}$ and $\\mathcal{D}^{\\mathrm{test}}$ can be arbitrary, except from the assumption that the marginal of $\\mathcal{D}^{\\mathrm{train}}$ is $\\mathcal{D}_{\\mathcal{X}}^{\\mathrm{train}}=\\mathcal{D}$ . To quantify the target error, we use parameter $\\bar{\\lambda}\\bar{=\\lambda}(\\mathcal{F};\\mathcal{D}^{\\mathrm{train}},\\bar{D}^{\\mathrm{test}})\\,=$ $\\begin{array}{r}{\\operatorname*{min}_{f\\in\\mathcal{F}}(\\mathrm{err}(f;\\mathcal{D}^{\\mathrm{train}})+\\mathrm{err}(f;\\mathcal{D}^{\\mathrm{test}}))}\\end{array}$ , where $\\operatorname{err}(f;\\mathcal{D}^{\\operatorname{train}})\\;=\\;\\mathbb{P}_{(\\mathbf{x},y)\\sim\\mathcal{D}^{\\operatorname{train}}}[y\\;\\neq\\;f(\\mathbf{x})]$ (and similarly for $\\operatorname{err}(f;{\\mathcal{D}}^{\\mathrm{test}}))$ . The error guarantee we can hope for is some function of $\\lambda$ , because $\\lambda$ encodes the (unknown) relationship between the training and test distributions, in that $\\lambda$ is small when there is a concept in the class $\\mathcal{F}$ that has low error on both training and test distributions. Error bounds in terms of $\\lambda$ are standard (and necessary) in the domain adaptation literature (see, e.g., [BDBCP06, $\\mathbf{BDBC}^{+}10]$ ) as well as TDS learning (see [KSV24b]). ", "page_idx": 3}, {"type": "text", "text": "Properties of Distributions. We make standard assumptions about training marginal $\\mathcal{D}$ . We denote with $\\mathcal{N}_{d}$ the standard Gaussian distribution over $\\mathbb{R}^{d}$ and with $\\operatorname{Unif}(\\{\\pm1\\}^{d})$ the uniform distribution over the hypercube $\\{\\pm1\\}^{d}$ . A distribution $\\mathcal{D}$ over $\\mathcal{X}$ is $k$ -tame if for every degree- $k$ polynomial $p$ over $\\mathcal{X}$ with $\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}[(p(\\mathbf{x}))^{2}]\\le1$ and every $B$ greater than $e^{k}$ we have $\\bar{\\mathbb{P}_{\\mathbf{x}\\sim\\mathcal{D}}}\\left[(p(\\mathbf{x}))^{2}>B\\right]\\le$ $e^{-\\Omega(B^{1/(2k)})}$ . And note that the Gaussian distribution, all isotropic log-concave distributions over $\\mathbb{R}^{d}$ , as well as the uniform distribution over $\\{\\pm1\\}^{d}$ are $k$ -tame for all $k\\in\\mathbb{N}$ (see Appendix A.4). ", "page_idx": 3}, {"type": "text", "text": "For a concept class $\\mathcal{F}$ , a distribution $\\mathcal{D}$ over $\\mathcal{X}$ , $\\epsilon\\,\\in\\,(0,1)$ , we say that $\\mathcal{F}$ has $\\epsilon{-}\\mathcal{L}_{2}$ sandwiching degree $k$ with respect to $\\mathcal{D}$ if for any $f\\in\\mathcal F$ , there exist polynomials $p_{\\mathrm{up}},p_{\\mathrm{low}}$ over $\\mathcal{X}$ with degree at most $k$ such that (1) $p_{\\mathrm{low}}(\\mathbf{x})\\leq f(\\mathbf{x})\\leq p_{\\mathrm{up}}(\\mathbf{x})$ for all $\\mathbf{x}\\in\\mathcal{X}$ and (2) $\\begin{array}{r}{\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}[(p_{\\mathrm{up}}(\\mathbf{x})\\!-\\!p_{\\mathrm{low}}(\\mathbf{x}))^{2}]\\le\\epsilon}\\end{array}$ . If the coefficients of $p_{\\mathrm{up}},p_{\\mathrm{low}}$ are all absolutely bounded by $B$ , we say that $\\mathcal{F}$ has $\\epsilon{-}\\mathcal{L}_{2}$ sandwiching coefficient bound $B$ . ", "page_idx": 3}, {"type": "text", "text": "3 Outlier Removal Procedure ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The key ingredient of our approach is an outlier removal procedure which is closely related to the corresponding procedure proposed by [DKS18] in the context of learning with nasty noise, but ours enjoys stronger error guarantees and works even when the fraction of outliers is arbitrarily large. The last property is important because we aim to handle arbitrary covariate shifts. Our outlier removal procedure outputs a selector $g:\\mathcal{X}\\to\\{0,1\\}$ that satisfies two main guarantees, provided examples drawn independently from some arbitrary, unknown distribution $\\mathcal{D}^{\\prime}$ : (1) for any low-degree polynomial $p$ , the part of the expectation of $p^{2}(\\dot{\\mathbf{x}})$ under $\\mathcal{D}^{\\prime}$ within the selected subset of $\\mathcal{X}$ (i.e., $\\bar{\\mathbb{E}}_{{\\mathbf{x}}\\sim\\mathcal{D}^{\\prime}}[p^{2}({\\mathbf{x}})g({\\mathbf{x}})])$ , is a bounded multiple of the expectation of $p^{2}(\\mathbf{x})$ under the reference distribution $\\mathcal{D}$ and (2) the probability of rejecting a fresh sample drawn from $\\mathcal{D}$ (i.e., $\\mathbb{P}_{\\mathbf{x}\\sim\\mathcal{D}}[g(\\mathbf{x})=0];$ ) is bounded by a multiple of the statistical distance between $\\mathcal{D}$ and $\\mathcal{D}^{\\prime}$ . Formally, we prove the following theorem. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.1 (Outlier Removal, see Appendix E). There exists an algorithm (Algorithm 1) that, given sample access to an arbitrary distribution $\\mathcal{D}^{\\prime}$ over $\\mathcal{X}\\subseteq\\mathbb{R}^{d}$ , sample access to a $k$ -tame probability distribution $\\mathcal{D}$ over $\\mathcal{X}$ , parameters $\\epsilon,\\alpha,\\delta\\in(0,1)$ and $k\\in\\mathbb{N},$ , runs in time $\\mathrm{poly}(\\textstyle{\\frac{1}{\\epsilon}}(k d)^{k}\\log{\\frac{1}{\\delta}})$ and outputs a succinct poly $\\bigl(\\frac{1}{\\epsilon}(k d)^{k}\\log\\frac{1}{\\delta}\\bigr)$ -time-computable description of a function $g:\\mathcal{X}\\to\\{0,1\\}$ that satisfies the following properties with probability at least $1-\\delta$ . ", "page_idx": 4}, {"type": "equation", "text": "$\\begin{array}{r}{(a)\\ \\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}^{\\prime}}\\left[(p(\\mathbf{x}))^{2}g(\\mathbf{x})\\right]\\le\\frac{200}{\\alpha}\\,\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}[(p(\\mathbf{x}))^{2}],}\\end{array}$ ", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$\\begin{array}{r}{\\mathbb{P}_{\\mathbf{x}\\sim\\mathcal{D}}[g(\\mathbf{x})=0]\\le\\alpha\\,\\mathrm{d}_{\\mathrm{TV}}(\\mathcal{D},\\mathcal{D}^{\\prime})+\\frac{\\epsilon}{2}.}\\end{array}$ ", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Remark 3.2. In Theorem 3.1, Condition (b) also implies some bound on the rejection rate over the distribution $\\mathcal{D}^{\\prime}$ and, in particular, $\\mathbb{P}_{\\mathbf{x}\\sim\\mathcal{D}^{\\prime}}[g(\\mathbf{x})=0]\\stackrel{*}{\\leq}(1+\\alpha)\\mathrm{d}_{\\mathrm{TV}}(\\mathcal{D},\\mathcal{D}^{\\prime})+\\epsilon\\bar{/}2$ . ", "page_idx": 4}, {"type": "text", "text": "Remark 3.3. Our algorithm further satisfies a strengthened form of Condition (b) (with probability at least $1-\\delta)$ . For $\\sigma\\,>\\,\\alpha/2$ and any distribution $\\mathcal{D}^{\\prime\\prime}$ that is $1/\\sigma$ -smooth w.r.t. $\\mathcal{D}$ , (i.e. for any measurable set $T\\subset\\mathbb{R}^{d}$ we have $\\begin{array}{r}{\\mathbb{P}_{\\mathbf{x}\\sim\\mathcal{D}^{\\prime\\prime}}[\\mathbf{x}\\in T]\\le\\frac{1}{\\sigma}\\mathbb{P}_{\\mathbf{x}\\sim\\mathcal{D}}[\\mathbf{x}\\in\\dot{T}])}\\end{array}$ it is the case that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\underset{\\mathbf{x}\\sim\\mathcal{D}}{\\mathbb{P}}[g(\\mathbf{x})=0]\\le\\frac{\\alpha}{\\sigma}\\,\\mathrm{d_{TV}}\\big(\\mathcal{D}^{\\prime\\prime},\\mathcal{D}^{\\prime}\\big)+\\frac{\\epsilon}{2},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "which in particular implies that $\\mathbb{P}_{\\mathbf{x}\\sim\\mathcal{D}}[g(\\mathbf{x})=0]\\le\\epsilon/2$ if $\\mathcal{D}^{\\prime}$ itself is $2/\\alpha$ -smooth w.r.t. $\\mathcal{D}$ . ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1: Outlier Removal Procedure ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Input: Sets $S_{D},S_{D^{\\prime}}$ , each of size $N$ , containing points in $\\mathcal{X}\\subseteq\\mathbb{R}^{d}$ and parameters $k,\\epsilon,\\delta,\\alpha$ Output: A succinct description of a selector function $g:\\mathcal{X}\\to\\{0,1\\}$ ", "page_idx": 4}, {"type": "text", "text": "Let $\\begin{array}{r}{t=\\binom{d+k-1}{k}}\\end{array}$ , $\\begin{array}{r}{B=\\frac{4}{\\epsilon}d^{3k}}\\end{array}$ and $\\begin{array}{r}{\\Delta=200B d^{k}(\\frac{\\log N}{N}\\log(1/\\delta))^{\\bar{1}/2}}\\end{array}$ ", "page_idx": 4}, {"type": "text", "text": "Compute monomial correlations estimate $\\widehat{M}$ by running Algorithm 2 on inputs $S_{D}$ , $k$ and $\\delta/10$ . $S^{0}\\leftarrow S_{\\mathcal{D^{\\prime}}}\\backslash\\{\\mathbf{x}\\in S_{\\mathcal{D^{\\prime}}}$ : there is $p\\in\\mathbb{R}^{t}$ with $(\\boldsymbol{p}^{\\intercal}\\mathbf{x}^{\\otimes k})^{2}>B$ and $p^{\\top}\\widehat{M}p\\leq1\\}$ ", "page_idx": 4}, {"type": "text", "text": "for $i=1,2,\\dots,N$ do ", "page_idx": 4}, {"type": "text", "text": "Let $p_{i}\\in\\mathbb{R}^{t}$ be the solution and $\\mu_{i}$ and the value of the following quadratic program. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{p\\in\\mathbb{R}^{t}}\\frac{1}{N}\\sum_{\\mathbf{x}\\in S^{i-1}}(p^{\\top}\\mathbf{x}^{\\otimes k})^{2}\\quad\\mathrm{{s.t.}:}\\;p^{\\top}\\widehat{M}p\\leq1\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "if $\\begin{array}{r}{\\mu_{i}\\leq\\frac{50}{\\alpha}(1+\\Delta)}\\end{array}$ then set $i_{\\mathrm{max}}=i-1$ and exit the loop; else let $\\tau_{i}$ be the minimum non-negative real number such that the following is true ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\frac{1}{N}\\Big|\\{\\mathbf{x}\\in S^{i-1}:(p_{i}^{\\top}\\mathbf{x}^{\\otimes k})^{2}>\\tau_{i}\\}\\Big|\\geq\\frac{10}{\\alpha}\\Big(\\displaystyle\\frac{\\mathbb{P}}{\\mathbf{x}\\sim S_{D}}[B\\geq(p_{i}^{\\top}\\mathbf{x}^{\\otimes k})^{2}>\\tau_{i}]+\\Delta\\Big)}\\\\ {\\displaystyle\\operatorname{Set}S^{i}\\gets S^{i-1}\\setminus\\{\\mathbf{x}\\in S^{i-1}:(p_{i}^{\\top}\\mathbf{x}^{\\otimes k})^{2}>\\tau_{i}\\}\\,;}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Set $g\\mathbf{(x)}$ to be 0 if and only if either there is $p\\in\\mathbb{R}^{t}$ with $(\\boldsymbol{p}^{\\intercal}\\mathbf{x}^{\\otimes k})^{2}>B$ and $p^{\\top}\\widehat{M}p\\leq1$ , or $(p_{i}^{\\top}\\bar{\\mathbf{x}}^{\\otimes k})^{2}>\\tau_{i}$ for some $i\\in[i_{\\mathrm{max}}]$ . Otherwise, set $g(\\mathbf{x})=1$ . ", "page_idx": 4}, {"type": "text", "text": "The outlier removal procedure of Theorem 3.1 iteratively solves a quadratic program with quadratic constraints (which can be solved efficiently, see Appendix E.1.1) and increases the rejection region by setting $g(\\mathbf{x})=0$ on each point $\\mathbf{x}$ where the corresponding (maximum second moment) polynomial takes large values. The procedure halts when the solution of the quadratic program has value bounded by $O(1/\\alpha)$ (which implies condition (a)). ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Proof overview. The main idea for the analysis is that whenever the stopping criterion does not hold, then there is a polynomial with unreasonably large second moment over the remaining part of $\\mathcal{D}^{\\prime}$ (after the rejections). When such a polynomial $p$ exists, there must be a threshold $\\tau$ for the squared values of $p$ such that $\\mathcal{D}^{\\prime}$ assigns $\\bar{\\Omega}(1/\\alpha)$ times more mass on non-rejected points $\\mathbf{x}$ with $p^{\\bar{2}}(\\mathbf{x})>\\tau$ compared to the reference distribution $\\mathcal{D}$ . Such points can be safely rejected, because, in that case, the mass of points under $\\mathcal{D}$ rejected is multiplicatively smaller (by a factor of $O(\\alpha);$ than the corresponding mass under $\\mathcal{D}^{\\prime}$ (which implies condition (b)). Note that the procedure will have to end eventually, because in each iteration, at least one example is removed. ", "page_idx": 5}, {"type": "text", "text": "In order to account for errors incurred by sampling (from $\\mathcal{D}$ and $\\mathcal{D}^{\\prime}$ ), it is important to provide a bound on the number of iterations that is independent from the number of examples drawn, because the complexity of the selector $g$ depends on the number of iterations and we need the desired properties of $g$ to generalize to the actual distributions $\\mathcal{D}$ and $\\mathcal{D}^{\\prime}$ . To this end, we consider the trace of the matrix $\\begin{array}{r}{M_{i}^{\\smile}=\\frac{\\daleth}{N}\\sum_{{\\bf x}\\in S^{i}}({\\bf x}_{.}^{\\otimes k})({\\bf x}^{\\otimes k})^{\\top}}\\end{array}$ as a potential function and we show that it reduces by a multiplicative factor in each iteration (see Claim 6 in the Appendix). ", "page_idx": 5}, {"type": "text", "text": "Comparison with [DKS18]. Among all outlier removal algorithms, ours is most related to the algorithm of [DKS18], which also removes elements in regions of the form $\\{\\mathbf{x}:\\;(p(\\mathbf{x}))^{2}>\\tau\\}$ However, there are two differences. First, [DKS18] assume that the fraction of outliers is bounded, while ours provides meaningful guarantees even in the presence of arbitrary fraction of outliers. In particular, we can maintain low rejection rates even in the presence of large fractions of outliers by relaxing the bound on the polynomial moments after outlier removal. This is important for PQ learning, because we need low error guarantees even when the amount of distribution shift is arbitrarily large. Second, even when the fraction of outliers is small, our bound on the second moments of polynomials does not depend on the degree and the degree dependence only appears in the runtime of the outlier removal process. This gives new insights on polynomial regression in the presence of outliers (due to distribution shift or noise). In contrast, the moment bound of [DKS18] scales with the degree of the corresponding polynomial and when the degree bound scales with the target learning error, their results become vacuous. This enables us to combine the outlier removal process with $\\mathcal{L}_{2}$ sandwiching results from TDS learning to obtain, for example, the first dimension-efficient robust learners with nasty noise of rate $\\Omega(\\epsilon)$ that achieve error $\\epsilon$ for the class of intersections of halfspaces. While [DKS18] also provide robust learners for this class, they only achieve error guarantees that scale as $\\tilde{O}(k^{1/12}\\epsilon^{1/1\\bar{1}})$ , for intersections of $k$ halfspaces. The key difference between our analysis and that of [DKS18] is that, to bound the number of iterations, we use an appropriate potential function, while [DKS18] ensure that the number of iterations is bounded by making sure to remove at least some fraction of points in each step. As a result, their stopping criterion scales with the target polynomial degree. ", "page_idx": 5}, {"type": "text", "text": "4 Selective Classification with Arbitrary Covariate Shift ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In order to provide provable learning guarantees in the presence of distribution shift, when no test labels are available, one reasonable approach is to enable the model to abstain on certain regions for which the training samples do not provide sufficient information. The model should not be able to abstain frequently on samples from the training distribution, since, otherwise the provided guarantees would be vacuous (e.g., when the model abstains always). A formal definition of this framework was given by [GKKM20] and, in this section, we provide the first end-to-end, dimension-efficient algorithms for learning various fundamental classes (e.g., halfspaces) in this setting. ", "page_idx": 5}, {"type": "text", "text": "PQ Setting. We first consider the case where the test samples are independently drawn from some (potentially adversarial) distribution $\\mathcal{D}^{\\mathrm{test}}$ and the goal of the learner is to achieve low error under $\\bar{\\mathcal{D}}^{\\mathrm{test}}$ (on points where the learner does not abstain), without abstaining frequently on fresh training samples, as described formally in the following definition of agnostic PQ learning. ", "page_idx": 5}, {"type": "text", "text": "Definition 4.1 (PQ Learning [GKKM20]). Let $\\mathcal{F}$ be a concept class over $\\mathcal{X}\\subseteq\\mathbb{R}^{d}$ and $\\mathcal{D}$ a distribution over $\\mathcal{X}$ . The algorithm $\\boldsymbol{\\mathcal{A}}$ is a PQ-learner for $\\mathcal{F}$ with respect to $\\mathcal{D}$ up to error $\\gamma$ , rejection rate $\\eta$ and probability of failure $\\delta$ if, upon receiving $m_{\\mathrm{train}}$ labeled samples from a training distribution $\\mathcal{D}^{\\mathrm{train}}$ with $\\mathcal{X}$ -marginal $\\mathcal{D}$ and $m_{\\mathrm{test}}$ unlabeled samples from a test distribution $\\mathcal{D}^{\\mathrm{test}}$ , algorithm $\\boldsymbol{\\mathcal{A}}$ outputs, w.p. at least $1-\\delta$ , a hypothesis $h:\\mathcal{X}\\rightarrow\\{\\pm1\\}$ and a selector $g:\\mathcal{X}\\rightarrow\\{0,1\\}$ such that: ", "page_idx": 5}, {"type": "text", "text": "(b) (rejection rate) The probability of rejection is bounded as $\\mathbb{P}_{\\mathbf{x}\\sim\\mathcal{D}}[g(\\mathbf{x})=0]\\le\\eta$ . ", "page_idx": 6}, {"type": "text", "text": "The error $\\gamma$ and the rejection rate $\\eta$ are, in general, functions of the parameter $\\lambda=\\lambda(\\mathcal{F};\\mathcal{D}^{\\mathrm{train}},\\mathcal{D}^{\\mathrm{test}})$ . ", "page_idx": 6}, {"type": "text", "text": "Adversarial Setting. Another reasonable scenario from [GKKM20] corresponds to the case where the test examples are not independent, but are chosen adversarially as follows. The adversary receives $N$ independent samples $S_{\\mathrm{iid}}$ from $\\mathcal{D}$ and substitutes any number of them adversarially, forming a new unlabeled dataset $S_{\\mathrm{test}}$ which is given to the learner along with a fresh set of independent samples from $\\mathcal{D}$ , labeled according to some hypothesis $f^{*}\\in\\mathcal{F}$ (realizable setting). The goal is to learn a hypothesis $h:\\mathcal{X}\\to\\{\\pm1\\}$ and a set $S_{g}\\subseteq S_{\\mathrm{test}}$ such that $\\mathbb{P}_{\\mathbf{x}\\in S_{\\mathrm{test}}}[h(\\mathbf{x})\\bar{\\neq}f^{*}(\\mathbf{x})$ and $\\mathbf{x}\\in S_{g}]\\le\\gamma$ and $|S_{\\mathrm{iid}}\\cap(S_{\\mathrm{test}}\\setminus S_{g})|\\stackrel{\\cdot}{\\leq}\\eta N$ (only a small fraction of i.i.d. points can be rejected). Note that the adversarial setting is primarily interesting in the realizable case, since there is no underlying test distribution and for any meaningful notion learning to be possible, there needs to be some relationship between the training and test labels. In the rest of this section, we focus on positive results for PQ learning, but, as we argue in Appendix C.2, all of our positive results on (realizable) PQ learning also work analogously in the adversarial setting. This is because our outlier removal process (Theorem 3.1) also works when the examples from the distribution $\\mathcal{D}^{\\prime}$ are in fact generated adversarially (see Theorem E.1). ", "page_idx": 6}, {"type": "text", "text": "4.1 PQ Learning of Halfspaces ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now give the first dimension-efficient PQ learning algorithms for the fundamental concept class of halfspaces, in the realizable setting and with respect to the Gaussian distribution, i.e., when both the training and the test labels are generated by some unknown halfspace and the training marginal $\\mathcal{D}$ is the standard Gaussian distribution $\\mathcal{N}_{d}$ . ", "page_idx": 6}, {"type": "text", "text": "Warm-Up: Homogeneous Halfspaces. We first focus on the class $\\mathcal{F}$ of homogeneous halfspaces, i.e., functions $f:\\check{\\mathbb{R}^{d}}\\rightarrow\\{\\pm1\\}$ with $f(\\mathbf{x})=\\mathrm{sign}(\\mathbf{w}\\cdot\\mathbf{x})$ for $\\mathbf{w}\\in\\mathbb{S}^{d-1}$ . Recent work by [KSV24b] showed that there is a simple fully polynomial-time TDS learner for this problem. In fact, their approach readily implies a PQ learner as well. ", "page_idx": 6}, {"type": "text", "text": "Proposition 4.2 (Implicit in [KSV24b]). For any $\\epsilon,\\delta\\in(0,1)$ , there is an algorithm that $P Q$ learns the class of homogeneous halfspaces with respect to $\\mathcal{N}_{d}$ in the realizable setting, up to error and rejection rate \u03f5 and probability of failure $\\delta$ that runs in time $\\mathrm{poly}(d,{\\textstyle\\frac{1}{\\epsilon}})\\log(1/\\delta)$ . ", "page_idx": 6}, {"type": "text", "text": "The algorithm of [KSV24b, Proposition 5.1] rejects when the probability that a randomly chosen example $\\mathbf{x}$ from the test marginal falls in some particular region $\\mathbf{D}$ in $\\dot{\\mathbb{R}}^{d}$ (for which there is an efficient membership oracle) is greater than $\\Omega(\\epsilon)$ . Since the training marginal is Gaussian, the ERM, run on sufficiently many labeled training examples, outputs a hypothesis $h(\\mathbf{x})=\\mathrm{sign}(\\widehat{\\mathbf{w}}\\cdot\\mathbf{x})$ such that $\\|\\widehat{\\mathbf{w}}-\\mathbf{w}^{*}\\|_{2}\\stackrel{\\cdot}{\\leq}\\epsilon^{\\prime}$ , where $\\mathbf{w}^{*}$ is the ground truth. Region $\\mathbf{D}$ consists precisely of the points $\\mathbf{x}$ for which  the ERM hypothesis $h$ is not confident: there are two (potential ground truth) unit vectors $\\mathbf{v}_{1}$ and $\\mathbf{v}_{2}$ that are both $\\epsilon^{\\prime}$ -close to $\\widehat{\\bf w}$ and $\\mathrm{sign}(\\mathbf{v}_{1}\\cdot\\mathbf{x})\\neq\\mathrm{sign}(\\mathbf{v}_{2}\\cdot\\mathbf{x})$ . Crucially, the Gaussian mass of $\\mathbf{D}$ is known to be $\\mathrm{poly}(\\epsilon^{\\prime})\\cdot\\sqrt{d}$ (see, e.g., [Han14]). Therefore, for PQ learning, we may return the classifier $h$ along with the selector $g(\\mathbf{x})=\\mathbb{1}\\{\\mathbf{x}\\notin\\mathbf{D}\\}$ and note that access to unlabeled test examples is not neeeded to form $h$ and $g$ . ", "page_idx": 6}, {"type": "text", "text": "General Halfspaces. For the class of general halfspaces (i.e., functions of the form $f(\\mathbf{x})\\;=$ $\\mathrm{sign}(\\mathbf{w}\\cdot\\mathbf{x}+\\tau)$ where w $\\in\\mathbb{S}^{d-1}$ and $\\tau\\in\\mathbb{R}$ ), the labeled training samples do not always provide sufficient information to recover the unknown parameters. This is because the bias $\\tau^{*}$ of the ground truth could take arbitrarily large positive or negative values, in which case all of the training examples will likely have the same label and (almost) no information about the ground truth $\\mathbf{w}^{*}$ is revealed. The concern in that case is that the test marginal $\\mathcal{D}^{\\prime}$ assigns a lot of mass far from the origin in the direction of $\\mathbf{w}^{*}$ . By appropriately applying Theorem 3.1 to select a part of the test marginal $\\mathcal{D}^{\\prime}$ that is sufficiently concentrated in every direction (hence even in the direction of $\\mathbf{w}^{*}$ ), we obtain the following PQ learning result. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.3 (PQ Learning of Halfspaces). For any $\\epsilon,\\delta\\,\\in\\,(0,1)$ , there is an algorithm that $P Q$ learns the class of general halfspaces with respect to $\\mathcal{N}_{d}$ in the realizable setting, up to error and rejection rate \u03f5 and probability of failure $\\delta$ that runs in time $\\mathrm{ooly}(d^{\\log(\\frac{1}{\\epsilon})},\\log(1/\\delta))$ . ", "page_idx": 6}, {"type": "text", "text": "The first ingredient for Theorem 4.3 is a result from [KSV24b] regarding recovering the parameters of an unknown general halfspace provided labeled examples from the Gaussian distribution, which was previously used for TDS learning. ", "page_idx": 7}, {"type": "text", "text": "Proposition 4.4 (Halfspace Parameter Recovery, Proposition 5.5 in [KSV24b]). For $\\epsilon,\\delta\\in(0,1)$ and $\\tau\\in\\mathbb{R},$ , suppose that $S$ consists of at least $m=\\mathrm{poly}(d,1/\\epsilon)e^{O(\\tau^{2})}\\log(1/\\delta)$ i.i.d. samples from $\\mathcal{N}_{d}$ , labeled by some halfspace of the form $f^{*}(\\mathbf{x})=\\mathrm{sign}(\\mathbf{w}^{*}\\cdot\\mathbf{x}+\\boldsymbol{\\tau}^{*}),$ , for some $\\mathbf{w}^{\\ast}\\in\\mathbb{S}^{d-1}$ . Then, with probability at least $1-\\delta,$ , for $\\begin{array}{r}{\\widehat{\\mathbf{w}}=\\sum_{(\\mathbf{x},y)\\in S}\\mathbf{x}y/\\|\\sum_{(\\mathbf{x},y)\\in S}\\mathbf{x}y\\|_{2}}\\end{array}$ and $\\widehat{\\boldsymbol{\\tau}}=\\widehat{\\mathbf{w}}\\cdot\\mathbf{x}$ for some x from $S$ such that $\\mathbb{P}_{(\\mathbf{x},y)\\in S}[y\\neq\\mathrm{sign}(\\widehat{\\mathbf{w}}\\cdot\\mathbf{x}+\\widehat{\\tau})]$ is minimized, we have $\\lVert\\widehat{\\mathbf{w}}-\\mathbf{w}^{*}\\rVert_{2}\\leq\\epsilon$ and $|\\widehat{\\tau}-\\tau^{*}|\\leq\\epsilon$ . ", "page_idx": 7}, {"type": "text", "text": "Therefore, in the case when the bias $\\tau^{*}$ of the unknown ground truth halfspace is not too large in absolute value, the selector can reject all points $\\mathbf{x}$ for which there exist two halfspaces with parameters close to $\\widehat{\\bf w}$ and $\\widehat{\\tau}$ accordingly that disagree on $\\mathbf{x}$ (similarly to the case of homogeneous halfspaces). Once more, such a selector can be implemented efficiently via a convex program. ", "page_idx": 7}, {"type": "text", "text": "When the bias is large, in TDS learning, checking whether the first ${\\cal O}(\\log(1/\\epsilon))$ moments of the test marginal $\\mathcal{D}^{\\prime}$ match the corresponding Gaussian moments is sufficient to ensure that the distribution is concentrated in every direction and, therefore, even in the unknown direction of $\\mathbf{w}^{*}$ . In order to obtain a selective classifier for this case, we instead use Theorem 3.1 with $k=O(\\log(1/\\epsilon))$ and ensure that the selected part of the test marginal is indeed sufficiently concentrated in every direction as required. For more details, see Appendix C.1.1. ", "page_idx": 7}, {"type": "text", "text": "4.2 PQ for Classes with Low Sandwiching Degree ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The outlier removal process of Theorem 3.1 enables one to fully control the ratios between the second moment of any low-degree polynomial under the selected part of the test marginal $\\mathcal{D}^{\\prime}$ and its second moment under the reference distribution $\\mathcal{D}$ , since the provided bound (see condition (a)) does not depend on the degree of the polynomial, but only on the target rejection rate. Combining our outlier removal process with ideas from TDS learning, we provide a general result on PQ learning classes with low ${\\mathcal{L}}_{2}$ sandwiching degree. In particular, we require the following properties for the hypothesis class $\\mathcal{F}$ and the training marginal $\\mathcal{D}$ . ", "page_idx": 7}, {"type": "text", "text": "Definition 4.5 (Reasonable Pairs of Classes and Distributions). We say that the pair $(\\mathcal{D},\\mathcal{F})$ , where $\\mathcal{D}$ is a distribution over $\\mathcal{X}\\subseteq\\mathbb{R}^{d}$ and $\\mathcal{F}\\subseteq\\{\\mathcal{X}\\to\\{\\pm1\\}\\}$ is $(\\epsilon,\\delta,k,m)$ -reasonable if the following properties hold: (1) the $\\epsilon\\!-\\!\\mathcal{L}_{2}$ sandwiching degree of $\\mathcal{F}$ under $\\mathcal{D}$ is at most $k$ with coefficient bound $B$ , (2) the distribution $\\mathcal{D}$ is $k$ -tame and (3) if $S$ consists of $m^{\\prime}$ i.i.d. samples from some distribution $\\mathcal{D}$ over $\\mathcal{X}\\times\\{\\pm1\\}$ with marginal $\\mathcal{D}$ and $m^{\\prime}\\geq m$ then, with probability at least $1-\\delta$ we have that for any degree- $k$ polynomial $p$ with coefficient bound $B$ it holds $|\\mathbb{E}_{\\mathcal{D}_{\\mathcal{X}\\mathcal{Y}}}[(\\dot{y}-p(\\mathbf{x}))^{2}]-\\mathbb{E}_{S}[(y-p(\\mathbf{x}))^{2}]|\\le\\dot{\\epsilon}$ . ", "page_idx": 7}, {"type": "text", "text": "Property (1) corresponds to the existence of sandwiching approximators, which is known to be important for learning in the presence of distribution shift by prior work on TDS learning [KSV24b]. Property (2) is the tameness condition Definition A.8, which is important for the outlier removal procedure and was used in the work of [DKS18] for similar purposes. Finally, property (3) ensures generalization for polynomial regression. ", "page_idx": 7}, {"type": "text", "text": "We obtain the following theorem which gives the first dimension-efficient results on PQ learning several fundamental concept classes with respect to standard training marginals, including intersections of halfspaces, decision trees and boolean formulas. The results work even in the agnostic setting. The algorithm runs the outlier removal process once to form the selector and runs polynomial regression on the training distribution to form the output hypothesis. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.6 (PQ Learning via Sandwiching). For $\\epsilon,\\eta,\\delta\\in(0,1)$ , let $\\mathcal{X}\\subseteq\\mathbb{R}^{d}$ and $(\\mathcal{D},\\mathcal{F})$ be an $\\bigl(\\frac{\\epsilon\\eta}{C},\\frac{\\delta}{C},k,m\\bigr)$ -reasonable pair (Definition 4.5) for some sufficiently large universal constant $C>0$ . Then, there is an algorithm that $P Q$ learns $\\mathcal{F}$ with respect to $\\mathcal{D}$ up to error $\\begin{array}{r}{O(\\frac{\\lambda}{\\eta})+\\epsilon,}\\end{array}$ , rejection rate $\\eta$ and probability of failure $\\delta$ with sample complexity $\\begin{array}{r}{m+\\mathrm{poly}(\\frac{1}{\\eta}(k d)^{k}\\log(1/\\delta))}\\end{array}$ and time complexity $\\mathrm{poly}({\\frac{m}{\\eta}}(k d)^{k}\\log(1/\\delta))$ . ", "page_idx": 7}, {"type": "text", "text": "Proof of Theorem 4.6. The algorithm forms the selector $g$ by applying the outlier removal process of Theorem 3.1 with parameters $\\alpha,\\epsilon\\,\\leftarrow\\,\\textstyle{\\frac{\\eta}{2}},\\,\\delta\\,\\leftarrow\\,\\delta/C$ and $k\\leftarrow k$ . Then, we run the following box-constrained least squares problem, using at least $m$ labeled examples $S_{\\mathrm{train}}$ from $\\mathcal{D}^{\\mathrm{train}}$ , where $t=d^{k}$ and $B$ is the value specified in Definition 4.5. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "$\\operatorname*{min}_{p}\\mathbb{E}_{(\\mathbf{x},y)\\sim S_{\\mathrm{train}}}[(y-p(\\mathbf{x}))^{2}]$ s.t. $p$ has degree at most $k$ and coefficient bound $B$ ", "page_idx": 8}, {"type": "text", "text": "Let $\\widehat{p}$ be the solution. The algorithm returns the selector $g$ and the classifier $h(\\mathbf{x})\\,=\\,\\mathrm{sign}(\\widehat{p}(\\mathbf{x}))$ . The rejection rate is bounded due to condition (b) in Theorem 3.1 while the accuracy can be shown by applying condition (a) as follows, where $f^{*}\\ \\in\\ {\\mathcal{F}}$ is the concept achieving $\\lambda\\ =$ $\\begin{array}{r}{\\operatorname*{min}_{f\\in\\mathcal{F}}\\(\\mathrm{err}(\\bar{f};\\mathcal{D}^{\\mathrm{train}})+\\mathrm{err}(f;\\mathcal{D}^{\\mathrm{test}}))}\\end{array}$ and $p_{\\mathrm{up}},p_{\\mathrm{low}}$ are the corresponding sandwiching polynomials for $f^{*}$ (as per Definition 4.5). We show that $\\begin{array}{r}{\\mathbb{P}_{(\\mathbf{x},y)\\sim\\mathcal{D}^{\\mathrm{test}}}[y\\neq h(\\mathbf{x}),g(\\mathbf{x})=1]\\leq O(\\frac{\\lambda}{\\eta})+\\epsilon.}\\end{array}$ $\\begin{array}{r l}&{\\underset{\\mathbf{x},y)\\sim\\mathcal{D}^{\\mathrm{test}}}{\\mathbb{P}}[y\\neq h(\\mathbf{x}),g(\\mathbf{x})=1]\\leq\\underset{(\\mathbf{x},y)\\sim\\mathcal{D}^{\\mathrm{test}}}{\\mathbb{P}}[y\\neq f^{*}(\\mathbf{x})]+\\underset{\\mathbf{x}\\sim\\mathcal{D}^{\\prime}}{\\mathbb{P}}[f^{*}(\\mathbf{x})\\neq\\mathrm{sign}(\\widehat{p}(\\mathbf{x})),g(\\mathbf{x})=1]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\lambda+\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}^{\\prime}}[(f^{*}(\\mathbf{x})-\\widehat{p}(\\mathbf{x}))^{2}g(x)]}\\end{array}$ The term $\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}^{\\prime}}[(f^{*}(\\mathbf{x})\\,-\\,\\widehat{p}(\\mathbf{x}))^{2}g(x)]$ can be bounded as $\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}^{\\prime}}[(f^{*}(\\mathbf{x})\\,-\\,\\widehat{p}(\\mathbf{x}))^{2}g(x)]\\ \\le$ $2\\,\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}^{\\prime}}[(f^{*}(\\mathbf{x})-p_{\\mathrm{low}}(\\mathbf{x}))^{2}g(x)]+2\\,\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}^{\\prime}}[(p_{\\mathrm{low}}(\\mathbf{x})-\\widehat{p}(\\mathbf{x}))^{2}g(x)]$ and since $p_{\\mathrm{up}},p_{\\mathrm{low}}$ sandwich $f^{*}$ , we bound $\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}^{\\prime}}[(f^{*}(\\mathbf{x})-p_{\\mathrm{low}}(\\mathbf{x}))^{2}g(x)]$ by $\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}^{\\prime}}[(p_{\\mathrm{up}}(\\mathbf{x})-p_{\\mathrm{low}}(\\mathbf{x}))^{2}g(x)]$ . By applying condition (a) from Theorem 3.1, since $(p_{\\mathrm{low}}(\\mathbf{x})-\\widehat{p}(\\mathbf{x}))^{2}$ and $(p_{\\mathrm{up}}(\\mathbf{x})-p_{\\mathrm{low}}(\\mathbf{x}))^{2}$ are squares of polynomials of degree $k$ , we have the following for s ome sufficiently large constant $C^{\\prime}$ . $\\underset{\\mathcal{D}^{\\mathrm{test}}}{\\mathbb{P}}[y\\neq h(\\mathbf{x}),g(\\mathbf{x})=1]\\leq\\lambda+\\frac{C^{\\prime}}{\\eta}(\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}[(p_{\\mathrm{low}}(\\mathbf{x})-\\widehat{p}(\\mathbf{x}))^{2}]+\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}[(p_{\\mathrm{up}}(\\mathbf{x})-p_{\\mathrm{low}}(\\mathbf{x}))^{2}])$ The term $\\begin{array}{r}{\\frac{C^{\\prime}}{\\eta}\\,\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}\\big[(p_{\\mathrm{up}}(\\mathbf{x})-p_{\\mathrm{low}}(\\mathbf{x}))^{2}\\big]}\\end{array}$ is at most $\\frac{\\epsilon}{3}$ and $\\begin{array}{r}{\\frac{C^{\\prime}}{\\eta}\\,\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}\\big[(p_{\\mathrm{low}}(\\mathbf{x})-\\widehat{p}(\\mathbf{x}))^{2}\\big]}\\end{array}$ is bounded by $O(\\frac{\\lambda}{\\eta})+2\\epsilon/3$ $\\mathrm{;~as}\\,\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}[(p_{\\mathrm{low}}(\\mathbf{x})-\\widehat{p}(\\mathbf{x}))^{2}]\\le2\\mathbb{E}_{\\mathcal{D}^{\\mathrm{train}}}[(p_{\\mathrm{low}}(\\mathbf{x})-y)^{2}]+2\\,\\mathbb{E}_{\\mathcal{D}^{\\mathrm{train}}}[(y-\\widehat{p}(\\mathbf{x}))^{2}].$ We have that $\\begin{array}{r}{\\mathbb{E}_{\\mathcal{D}^{\\mathrm{train}}}\\big[(p_{\\mathrm{low}}(\\mathbf{x})-y)^{2}\\big]\\leq2\\,\\mathbb{E}_{\\mathcal{D}^{\\mathrm{train}}}\\big[(p_{\\mathrm{low}}(\\mathbf{x})-f^{*}(\\mathbf{x}))^{2}\\big]+2\\,\\mathbb{E}_{\\mathcal{D}^{\\mathrm{train}}}\\big[(y-f^{*}(\\mathbf{x}))^{2}\\big]\\leq}\\end{array}$ $\\begin{array}{r}{\\frac{\\epsilon\\eta}{C}+O(\\lambda)}\\end{array}$ , due to sandwiching and the definition of $\\lambda$ . The term $\\mathbb{E}_{\\mathcal{D}^{\\mathrm{train}}}[(y-\\widehat{p}(\\mathbf{x}))^{2}]$ is $\\frac{\\epsilon\\eta}{C}$ -close to $\\check{\\mathbb{E}}_{S_{\\mathrm{train}}}[(y-\\widehat{p}(\\mathbf{x}))^{2}]$ (due to Definition 4.5) and, since $\\widehat{p}$ is the solution of the least squares program, $\\begin{array}{r}{\\bar{\\mathscr{z}}_{S_{\\mathrm{train}}}[(\\bar{y}-\\bar{p}(\\mathbf{x}))^{2}]^{*}\\!\\le\\!\\mathbb{E}_{S_{\\mathrm{train}}}[(y-p_{\\mathrm{low}}(\\mathbf{x}))^{2}]\\le\\mathbb{E}_{\\mathcal{D}^{\\mathrm{train}}}[(y-p_{\\mathrm{low}}(\\mathbf{x}))^{2}]\\!+\\!\\frac{\\epsilon\\eta}{C}=O(\\frac{\\epsilon\\bar{\\eta}}{C})\\!+\\!\\mathcal{O}(\\lambda)}\\end{array}$ . $\\lambda$ ", "page_idx": 8}, {"type": "text", "text": "Remark 4.7. In a semi-agnostic setting where is known, t\u221ahen $\\eta$ can be chosen to balance the error and rejection rates in Theorem 4.6, obtaining bounds of $O(\\sqrt\\lambda)$ , which is known to be best-possible in the PQ setting, even for contrived concept classes (see [GKKM20]). ", "page_idx": 8}, {"type": "text", "text": "By combining Theorem 4.6 with bounds on the ${\\mathcal{L}}_{2}$ sandwiching degree of fundamental concept classes by [KSV24b] (see Appendix A.5), we obtain the results of Table 2 for PQ learning. ", "page_idx": 8}, {"type": "text", "text": "5 Tolerant TDS Learning ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Another approach to provide provable learning guarantees with distribution shift is to enable rejecting the whole test distribution. The formal definition of this setting was given by [KSV24b], but the proposed algorithms were allowed to reject even if a miniscule amount of distribution shift was detected. We provide the first TDS learners that are guaranteed to accept whenever the test marginal $\\mathcal{D}^{\\prime}$ is close to the training marginal $\\mathcal{D}$ in total variation distance (see Definition A.7). ", "page_idx": 8}, {"type": "text", "text": "Definition 5.1 (Tolerant TDS Learning, extension of [KSV24b]). Let $\\mathcal{F}$ be a concept class over $\\mathcal{X}\\subseteq\\mathbb{R}^{d}$ and $\\mathcal{D}$ a distribution over $\\mathcal{X}$ . The algorithm $\\boldsymbol{\\mathcal{A}}$ is a TDS-learner for $\\mathcal{F}$ with respect to $\\mathcal{D}$ up to error $\\gamma$ (which is, in general a function of parameter $\\lambda$ ), tolerance $\\theta$ and probability of failure $\\delta$ if, upon receiving $m_{\\mathrm{train}}$ labeled samples from a training distribution $\\mathcal{D}^{\\mathrm{train}}$ with $\\mathcal{X}$ -marginal $\\mathcal{D}$ and $m_{\\mathrm{test}}$ unlabeled samples from a test distribution $\\mathcal{D}^{\\mathrm{test}}$ , algorithm $\\boldsymbol{\\mathcal{A}}$ either rejects or accepts and outputs a hypothesis $h:\\dot{\\boldsymbol{\\mathcal{X}}}\\rightarrow\\{\\pm1\\}$ such that, with probability at least $1-\\delta$ , the following hold: ", "page_idx": 8}, {"type": "text", "text": "(a) (soundness) Upon acceptance, the test error is bounded as $\\mathbb{P}_{(\\mathbf{x},y)\\sim\\mathcal{D}^{\\mathrm{test}}}[y\\neq h(\\mathbf{x})]\\le\\gamma.$ .   \n(b) (completeness) If $\\mathrm{d}_{\\mathrm{TV}}(\\mathcal{D},\\mathcal{D}_{\\mathcal{X}}^{\\mathrm{test}})\\leq\\theta$ , then the algorithm accepts. ", "page_idx": 8}, {"type": "text", "text": "We first observe that tolerant TDS learning is implied by PQ learning. ", "page_idx": 8}, {"type": "text", "text": "Proposition 5.2 (PQ implies Tolerant TDS Learning, modification of Proposition 56 in [KSV24b]). Suppose that there is an algorithm $\\boldsymbol{\\mathcal{A}}$ that $P Q$ learns the class $\\mathcal{F}$ with respect to $\\mathcal{D}$ up to error $\\gamma$ , rejection rate $\\eta$ and failure probability $\\delta$ . Then, for any $\\epsilon,\\theta\\in(0,1)$ there is an algorithm that $T D S$ learns $\\mathcal{F}$ with respect to $\\mathcal{D}$ up to error $\\gamma+\\eta+\\theta+\\epsilon,$ with tolerance $\\theta$ and failure probability $\\delta$ that calls $\\boldsymbol{\\mathcal{A}}$ once and uses $\\begin{array}{r}{O\\big(\\frac{1}{\\epsilon^{2}}\\log(1/\\delta)\\big)}\\end{array}$ additional samples and evaluations of the selector given by $\\mathcal{A}$ . ", "page_idx": 8}, {"type": "text", "text": "The above result readily follows by two simple observations about the selector $g$ and the hypothesis $h$ in the output of a PQ learner. In particular, we have $\\mathbb{P}_{\\mathbf{x}\\sim\\mathcal{D}_{\\mathcal{X}}^{\\mathrm{test}}}[g(\\mathbf{x})=0]\\le\\mathbb{P}_{\\mathbf{x}\\sim\\mathcal{D}}[g(\\Breve{\\mathbf{x}})=0]+$ $\\mathrm{d}_{\\mathrm{TV}}(\\mathcal{D},\\mathcal{D}_{\\chi}^{\\mathrm{test}})\\leq\\eta+\\mathrm{d}_{\\mathrm{TV}}(\\mathcal{D},\\mathcal{D}_{\\chi}^{\\mathrm{test}})$ and $\\mathrm{err}(h;\\mathcal{D}^{\\mathrm{test}})\\leq\\mathbb{P}_{\\mathbf{x}\\sim\\mathcal{D}_{\\mathcal{X}}^{\\mathrm{test}}}[g(\\mathbf{x})=0]+\\mathbb{P}_{(\\mathbf{x},y)\\sim\\mathcal{D}^{\\mathrm{test}}}[y\\neq$ $h(\\mathbf{x}),g(\\mathbf{x})\\,=1\\]$ . The TDS learner will reject if the empirical estimate of $\\mathbb{P}_{\\mathbf{x}\\sim\\mathcal{D}_{\\mathcal{X}}^{\\mathrm{test}}}[g(\\mathbf{x})=0]$ is larger than $\\eta+\\theta+\\Omega(\\epsilon)$ ; otherwise it will output $h$ . ", "page_idx": 9}, {"type": "text", "text": "Proposition 5.2 allows us to conclude that the realizable tolerant TDS learning algorithm in Table 1 follows from the PQ learning algorithm of Theorem 4.3. ", "page_idx": 9}, {"type": "text", "text": "In t\u221ahe agnostic setting, however, the error rate of PQ learning is known to be necessarily high (i.e., $\\Omega({\\sqrt{\\lambda}}))$ even for very simple classes (see Remark 4.7). Therefore, the corresponding TDS learning results implied by Proposition 5.2 do not achieve the optimum error rate for the case of TDS learning (i.e. $\\Theta(\\lambda),$ ). Nevertheless, we are able to use, once more, our outlier removal process directly and obtain the following analogue of Theorem 4.6 for tolerant TDS learning. ", "page_idx": 9}, {"type": "text", "text": "Theorem 5.3 (Tolerant TDS Learning via Sandwiching). For $\\epsilon,\\theta,\\delta\\in(0,1)$ , let $\\mathcal{X}\\subseteq\\mathbb{R}^{d}$ and $(\\mathcal{D},\\mathcal{F})$ be an $\\left(\\frac{\\epsilon}{C},\\frac{\\delta}{C},k,m\\right)$ -reasonable pair (Definition 4.5) for some sufficiently large universal constant $C>0$ . Then, there is an TDS learner $\\mathcal{F}$ with respect to $\\mathcal{D}$ up to error $O(\\lambda)+2\\theta+\\epsilon,$ , tolerance $\\theta$ and probability of failure $\\delta$ with sample and time complexity poly $\\mathit{\\Omega}_{\\cdot}^{\\prime}\\frac{m}{\\epsilon}(k d)^{k}\\log(1/\\delta))$ . ", "page_idx": 9}, {"type": "text", "text": "Furthermore, (via Remark 3.3) we show that our tolerant TDS learning algorithm will with high probability be guaranteed to accept a distribution $\\mathcal{D}^{\\prime}$ that is $1/2$ -smooth with respect to $\\mathcal{D}$ . ", "page_idx": 9}, {"type": "text", "text": "For the full proof, see Appendix C.3. As a corollary of Theorem 5.3, we obtain the results of Table 2 for tolerant TDS learning. ", "page_idx": 9}, {"type": "text", "text": "Limitations, Broader Impacts, and Future Work. Our current results hold only for a limited class of training marginal distributions (e.g., standard Gaussian or uniform over the hypercube). We leave it as an interesting open question to relax these distributional assumptions, as well as expand the completeness criterion for our tolerant TDS learning algorithms to accept when the test marginal is close to any distribution among the members of some wide class of well-behaved distributions (i.e., satisfy the universality condition as defined by [GKSV23]). Additionally, we would like to point out that rejecting to predict on certain distributions may lead to unfair or biased predictions if the distributions overlap significantly with the minority groups. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements. We thank the anonymous reviewers of NeurIPS 2024 for their constructive feedback. We thank Adam Klivans for insightful conversations and helpful references. K.S. thanks Aravind Gollakota for useful discussions regarding PQ learning of homogeneous halfspaces and the observation that no unlabeled test examples are needed in this case. A.V. thanks Shyam Narayanan for helpful conversations about robust mean estimation and outlier removal. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "$[\\mathbf{BCE^{+}}19]$ Eric Blais, Cl\u00e9ment L Canonne, Talya Eden, Amit Levi, and Dana Ron. Tolerant junta testing and the connection to submodular optimization and function isomorphism. ACM Transactions on Computation Theory (TOCT), 11(4):1\u201333, 2019. $[{\\bf B C K^{+}07}]$ John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman. Learning bounds for domain adaptation. Advances in neural information processing systems, 20, 2007.   \n$[\\mathbf{BDBC}^{+}10]$ Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A theory of learning from different domains. Machine learning, 79:151\u2013175, 2010.   \n[BDBCP06] Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations for domain adaptation. Advances in neural information processing systems, 19, 2006. [BEK02] Nader H Bshouty, Nadav Eiron, and Eyal Kushilevitz. Pac learning with nasty noise. Theoretical Computer Science, 288(2):255\u2013275, 2002. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "[BH18] Avrim Blum and Lunjia Hu. Active tolerant testing. In Conference On Learning Theory, pages 474\u2013497. PMLR, 2018.   \n$[\\mathrm{CFG}^{+}22]$ Sourav Chakraborty, Eldar Fischer, Arijit Ghosh, Gopinath Mishra, and Sayantan Sen. Exploring the gap between tolerant and non-tolerant distribution testing. Proceedings of the Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques (APPROX/RANDOM 2022), 245:1\u201323, 2022. [CGR16] Cl\u00e9ment L Canonne, Themis Gouleakis, and Ronitt Rubinfeld. Sampling correctors. In Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science, pages 93\u2013102, 2016.   \n[CJKL22] Cl\u00e9ment L Canonne, Ayush Jain, Gautam Kamath, and Jerry Li. The price of tolerance in distribution testing. In Conference on Learning Theory, pages 573\u2013624. PMLR, 2022. [CP23] Xi Chen and Shyamal Patel. New lower bounds for adaptive tolerant junta testing. In 2023 IEEE 64th Annual Symposium on Foundations of Computer Science (FOCS), pages 1778\u20131786. IEEE, 2023. [DK19] Ilias Diakonikolas and Daniel M. Kane. Recent advances in algorithmic highdimensional robust statistics, 2019.   \n$[\\mathrm{DKK}^{+}19]$ Ilias Diakonikolas, Gautam Kamath, Daniel Kane, Jerry Li, Ankur Moitra, and Alistair Stewart. Robust estimators in high-dimensions without the computational intractability. SIAM Journal on Computing, 48(2):742\u2013864, 2019.   \n$[\\mathrm{DKK}^{+}23]$ Ilias Diakonikolas, Daniel Kane, Vasilis Kontonis, Sihan Liu, and Nikos Zarifis. Efficient testable learning of halfspaces with adversarial label noise. Advances in Neural Information Processing Systems, 36, 2023. [DKS18] Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. Learning geometric concepts with nasty noise. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, pages 1061\u20131073, 2018.   \n[DLLP10] Shai Ben David, Tyler Lu, Teresa Luu, and D\u00e1vid P\u00e1l. Impossibility theorems for domain adaptation. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pages 129\u2013136. JMLR Workshop and Conference Proceedings, 2010. [DV04] John Dunagan and Santosh Vempala. Optimal outlier removal in high-dimensional spaces. Journal of Computer and System Sciences, 68(2):335\u2013373, 2004. Special Issue on STOC 2001. [FF05] Eldar Fischer and Lance Fortnow. Tolerant versus intolerant testing for boolean properties. In 20th Annual IEEE Conference on Computational Complexity (CCC\u201905), pages 135\u2013140. IEEE, 2005.   \n[GHMS24] Surbhi Goel, Steve Hanneke, Shay Moran, and Abhishek Shetty. Adversarial resilience in sequential prediction via abstention. Advances in Neural Information Processing Systems, 36, 2024.   \n[GKK23] Aravind Gollakota, Adam R Klivans, and Pravesh K Kothari. A moment-matching approach to testable learning and a new characterization of rademacher complexity. Proceedings of the fifty-fifth annual ACM Symposium on Theory of Computing, 2023.   \n[GKKM20] Shafi Goldwasser, Adam Tauman Kalai, Yael Kalai, and Omar Montasser. Beyond perturbations: Learning guarantees with arbitrary adversarial test examples. Advances in Neural Information Processing Systems, 33:15859\u201315870, 2020.   \n[GKSV23] Aravind Gollakota, Adam Klivans, Konstantinos Stavropoulos, and Arsen Vasilyan. Tester-learners for halfspaces: Universal algorithms. Advances in Neural Information Processing Systems, 36, 2023.   \n[GKSV24] Aravind Gollakota, Adam R Klivans, Konstantinos Stavropoulos, and Arsen Vasilyan. An efficient tester-learner for halfspaces. The Twelfth International Conference on Learning Representations, 2024. [Han14] Steve Hanneke. Theory of disagreement-based active learning. Foundations and Trends\u00ae in Machine Learning, 7(2-3):131\u2013309, 2014. [HLZ20] Samuel B. Hopkins, Jerry Li, and Fred Zhang. Robust and heavy-tailed mean estimation made simple, via regret minimization. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS \u201920, Red Hook, NY, USA, 2020. Curran Associates Inc. [KK21] Adam Tauman Kalai and Varun Kanade. Efficient learning with arbitrary covariate shift. In Algorithmic Learning Theory, pages 850\u2013864. PMLR, 2021.   \n[KKM18] Adam Klivans, Pravesh K Kothari, and Raghu Meka. Efficient algorithms for outlierrobust regression. In Conference On Learning Theory, pages 1420\u20131430. PMLR, 2018.   \n[KKMS08] Adam Tauman Kalai, Adam R Klivans, Yishay Mansour, and Rocco A Servedio. Agnostically learning halfspaces. SIAM Journal on Computing, 37(6):1777\u20131805, 2008. [KOS08] Adam R Klivans, Ryan O\u2019Donnell, and Rocco A Servedio. Learning geometric concepts via gaussian surface area. In 2008 49th Annual IEEE Symposium on Foundations of Computer Science, pages 541\u2013550. IEEE, 2008. [KS17] Pravesh K Kothari and Jacob Steinhardt. Better agnostic clustering via relaxed tensor norms. arXiv preprint arXiv:1711.07465, 2017.   \n[KSV24a] Adam Klivans, Konstantinos Stavropoulos, and Arsen Vasilyan. Learning intersections of halfspaces with distribution shift: Improved algorithms and sq lower bounds. In Shipra Agrawal and Aaron Roth, editors, Proceedings of Thirty Seventh Conference on Learning Theory, volume 247 of Proceedings of Machine Learning Research, pages 2944\u20132978. PMLR, 30 Jun\u201303 Jul 2024.   \n[KSV24b] Adam Klivans, Konstantinos Stavropoulos, and Arsen Vasilyan. Testable learning with distribution shift. In Shipra Agrawal and Aaron Roth, editors, Proceedings of Thirty Seventh Conference on Learning Theory, volume 247 of Proceedings of Machine Learning Research, pages 2887\u20132943. PMLR, 30 Jun\u201303 Jul 2024. [KZZ24] Alkis Kalavasis, Ilias Zadik, and Manolis Zampetakis. Transfer learning beyond bounded density ratios. arXiv preprint arXiv:2403.11963, 2024. [LRV16] Kevin A. Lai, Anup B. Rao, and Santosh Vempala. Agnostic estimation of mean and covariance. In 2016 IEEE 57th Annual Symposium on Foundations of Computer Science (FOCS), pages 665\u2013674, 2016.   \n[MMR09] Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds and algorithms. In Proceedings of The 22nd Annual Conference on Learning Theory (COLT 2009), Montr\u00e9al, Canada, 2009. [OS03] Ryan O\u2019Donnell and Rocco A Servedio. New degree bounds for polynomial threshold functions. In Proceedings of the thirty-fifth annual ACM symposium on Theory of computing, pages 325\u2013334, 2003. [PRR06] Michal Parnas, Dana Ron, and Ronitt Rubinfeld. Tolerant property testing and distance approximation. Journal of Computer and System Sciences, 72(6):1012\u20131042, 2006.   \n$[\\mathrm{RMH}^{+}20]$ Ievgen Redko, Emilie Morvant, Amaury Habrard, Marc Sebban, and Youn\u00e8s Bennani. A survey on domain adaptation theory: learning bounds and theoretical guarantees. arXiv preprint arXiv:2004.11829, 2020. [RV20] Ronitt Rubinfeld and Arsen Vasilyan. Monotone probability distributions over the boolean cube can be learned with sublinear samples. arXiv preprint arXiv:2002.03415, 2020. [RV23] Ronitt Rubinfeld and Arsen Vasilyan. Testing distributional assumptions of learning algorithms. Proceedings of the fifty-fifth annual ACM Symposium on Theory of Computing, 2023. [SS20] Adarsh Subbaswamy and Suchi Saria. From development to deployment: dataset shift, causality, and shift-stable models in health ai. Biostatistics, 21(2):345\u2013352, 2020. [Ste18] Jacob Steinhardt. Robust learning: Information theory and algorithms. Stanford University, 2018.   \n$[\\mathrm{TCK}^{+}22]$ Niels K Ternov, Anders N Christensen, Peter JT Kampen, Gustav Als, Tine Vestergaard, Lars Konge, Martin Tolsgaard, Lisbet R H\u00f6lmich, Pascale Guitera, Annette H Chakera, et al. Generalizability and usefulness of artificial intelligence for skin cancer diagnostics: An algorithm validation study. JEADV Clinical Practice, 1(4):344\u2013354, 2022.   \n[VDVW09] Aad Van Der Vaart and Jon A Wellner. A note on bounds for vc dimensions. Institute of Mathematical Statistics collections, 5:103, 2009. [VV11] Gregory Valiant and Paul Valiant. Estimating the unseen: an n/log (n)-sample estimator for entropy and support size, shown optimal via new clts. In Proceedings of the forty-third annual ACM symposium on Theory of computing, pages 685\u2013694, 2011.   \n$[\\mathbf{WOD}^{+}21]$ Andrew Wong, Erkin Otles, John P Donnelly, Andrew Krumm, Jeffrey McCullough, Olivia DeTroyer-Cooley, Justin Pestrue, Marie Phillips, Judy Konye, Carleen Penoza, et al. External validation of a widely implemented proprietary sepsis prediction model in hospitalized patients. JAMA Internal Medicine, 181(8):1065\u20131070, 2021.   \n$[Z\\mathbf{B}\\mathbf{L}^{+}18]$ John R Zech, Marcus A Badgeley, Manway Liu, Anthony B Costa, Joseph J Titano, and Eric Karl Oermann. Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: a cross-sectional study. PLoS medicine, 15(11):e1002683, 2018. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Extended Preliminaries ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Notation ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We use $\\mathbf{x},\\mathbf{w},\\mathbf{v}$ to denote vectors in the $d_{\\cdot}$ -dimensional Euclidean space $\\mathbb{R}^{d}$ . For some distribution $\\mathcal{D}$ and a function $f$ , we denote with $\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}[f(\\mathbf{x})]$ the expectation of the random variable $f(\\mathbf{x})$ when $\\mathbf{x}$ is drawn from $\\mathcal{D}$ . For a set of points $X$ , we use a similar notation for the empirical expectations over $X$ , i.e., $\\begin{array}{r}{\\mathbb{E}_{\\mathbf{x}\\sim X}[f(\\mathbf{x})]=\\frac{1}{|X|}\\sum_{\\mathbf{x}\\in X}f(\\mathbf{x})}\\end{array}$ . In our paper, $\\mathcal{X}\\subseteq\\mathbb{R}^{d}$ will either be $\\mathbb{R}^{d}$ or the hypercube $\\{\\pm1\\}^{d}$ . We denote with $\\mathbb{N}$ the set of natural numbers $\\mathbb{N}=\\{0,1,2,\\dots\\}$ . The expression $\\mathbf{x}\\cdot\\mathbf{w}$ or $\\mathbf{x}^{\\top}\\mathbf{w}$ denotes the inner product between two vectors, i.e., $\\begin{array}{r}{\\mathbf{x}\\cdot\\mathbf{w}=\\sum_{i=1}^{d}x_{i}w_{i}}\\end{array}$ (where $x_{i}$ is the value of the $i$ -th coordinate of $\\mathbf{x}$ ). ", "page_idx": 13}, {"type": "text", "text": "A.2 Polynomials ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Throughout this work, we will refer to polynomials whose degree is at most $k$ as \u201cdegree- $k$ polynomials\u201d for brevity. We will identify every degree- $k$ polynomial $p$ with the vector of its coefficients. Furthermore, for a vector $\\mathbf{x}$ in $\\mathbb{R}^{d}$ we will denote $\\mathbf{\\dot{x}}^{\\otimes k}$ the vector whose entries correspond to the values of all monomials of degree at most $k$ evaluated on $\\mathbf{x}$ . Both the vector corresponding to a degree- ${\\cdot k}$ polynomial and the vector $\\mathbf{x}^{\\otimes k}$ have dimension $m$ , where $m$ is the number of distinct monomials on $\\mathbb{R}^{d}$ of degree at most $k$ . Note that $m\\leq d^{k}$ and that with this notation in hand we have $p(\\mathbf{x})=p\\cdot(\\mathbf{x}^{\\otimes k})=p^{\\top}\\mathbf{\\check{x}}^{\\otimes k}$ . ", "page_idx": 13}, {"type": "text", "text": "A.3 Learning theory ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We will usually consider function classes over $\\mathbb{R}^{d}$ taking values in $\\{\\pm1\\}$ or in $\\{0,1\\}$ . Consider the following definitions: ", "page_idx": 13}, {"type": "text", "text": "Definition A.1. A halfspace over $\\mathbb{R}^{d}$ is a function mapping $\\mathbf{x}$ in $\\mathbb{R}^{d}$ to $\\mathrm{sign}(\\mathbf{w}\\cdot\\mathbf{x}+\\theta)$ for some w in $\\mathbb{S}^{d-1}$ and $\\theta$ in $\\mathbb{R}$ . ", "page_idx": 13}, {"type": "text", "text": "Definition A.2. A degree- $k$ polynomial threshold function (PTF) over $\\mathbb{R}^{d}$ is a function mapping $\\mathbf{x}$ in $\\mathbb{R}^{d}$ to $\\mathrm{sign}(p(\\mathbf{x}))$ for some degree- $k$ polynomial $p$ . ", "page_idx": 13}, {"type": "text", "text": "Definition A.3. The OR of a collection of function classes ${\\mathcal{F}}_{1}\\lor\\cdot\\cdot\\cdot\\lor{\\mathcal{F}}_{m}$ , is defined as the collection of functions $f$ defined as $f(\\mathbf{x})\\,=\\,f_{1}(\\mathbf{x})\\;\\vee\\cdots\\;\\vee\\;f_{m}(\\mathbf{x})$ for each $f_{i}$ belonging to ${\\mathcal{F}}_{i}$ respectively. Analogously, the AND of the collection of function classes ${\\mathcal{F}}_{1}\\wedge\\cdot\\cdot\\cdot\\wedge{\\mathcal{F}}_{m}$ , is defined as the collection of functions $f$ defined as $f(\\mathbf{x})=f_{1}(\\mathbf{x})\\wedge\\dots\\wedge f_{m}(\\mathbf{x})$ for each $f_{i}$ belonging to ${\\mathcal{F}}_{i}$ respectively. ", "page_idx": 13}, {"type": "text", "text": "The following facts about VC dimensions of various classes are standard: ", "page_idx": 13}, {"type": "text", "text": "Fact A.4. The VC dimension of halfspaces over $\\mathbb{R}^{d}$ is at most $d+1$ , and the VC dimensions of degree-k polynomial threshold functions is at most $d^{k}+1$ . ", "page_idx": 13}, {"type": "text", "text": "Fact A.5 (e.g. [VDVW09] and references therein.). Let $\\{{\\mathcal{F}}_{1},\\cdot\\cdot\\cdot,{\\mathcal{F}}_{m}\\}$ be a collection of function classes each of which has a VC dimension of $V$ . Then, the VC dimension of ${\\mathcal{F}}_{1}\\lor\\cdots\\lor{\\mathcal{F}}_{m}$ and ${\\mathcal{F}}_{1}\\wedge\\cdot\\cdot\\cdot\\wedge{\\mathcal{F}}_{m}$ are at most $O(V m\\log m)$ . ", "page_idx": 13}, {"type": "text", "text": "We will also need the standard uniform convergence bound for function classes of bounded VC dimension. ", "page_idx": 13}, {"type": "text", "text": "Fact A.6. Let $\\mathcal{F}$ be a function class over $\\mathbb{R}^{d}$ taking values in $\\{\\pm1\\}$ with VC dimension at most $V$ , let $D$ be a distribution over $\\mathbb{R}^{d}\\times\\{\\pm1\\}$ , and let $S\\subset\\mathbb{R}^{d}\\times\\{\\pm\\mathrm{i}\\}$ be composed of $N$ i.i.d. examples from $D$ . Then, with probability at least $1-\\delta$ we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{f\\in\\mathcal{F}}\\left[\\left|\\frac{1}{N}\\sum_{(\\mathbf{x},y)\\in S}[|f(\\mathbf{x})-y|]-\\frac{\\mathbb{E}}{(\\mathbf{x},y)\\sim D}[|f(\\mathbf{x})-y|]\\right|\\right]\\leq\\sqrt{\\frac{V\\log N}{N}\\log\\frac{1}{\\delta}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The same statement also true if $\\mathcal{F}$ is taking values in $\\{0,1\\}$ and $D$ be a distribution over $\\mathbb{R}^{d}\\times\\{0,1\\}$ . ", "page_idx": 13}, {"type": "text", "text": "A.4 Properties of Distributions ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We denote with $\\mathcal{D},\\mathcal{D}^{\\prime}$ distributions over $\\mathcal{X}\\subseteq\\mathbb{R}^{d}$ . For two distributions $\\mathcal{D},\\mathcal{D}^{\\prime}$ , the total variation distance between them is defined as follows. ", "page_idx": 13}, {"type": "text", "text": "Definition A.7. Let $\\mathcal{D},\\mathcal{D}^{\\prime}$ be distributions over $\\mathcal{X}\\subseteq\\mathbb{R}^{d}$ (for some $\\sigma$ -algebra $B\\subseteq\\operatorname{Pow}(\\mathbb{R}^{d}))$ . Then, the total variation distance between $\\mathcal{D}$ and $\\mathcal{D}^{\\prime}$ is ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{d}_{\\mathrm{TV}}(\\mathcal{D},\\mathcal{D}^{\\prime})=\\operatorname*{sup}_{A\\in\\mathcal{B}}\\Big|\\underset{\\mathbf{x}\\sim\\mathcal{D}}{\\mathbb{P}}[\\mathbf{x}\\in A]-\\underset{\\mathbf{x}\\sim\\mathcal{D}^{\\prime}}{\\mathbb{P}}[\\mathbf{x}\\in A]\\Big|\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We define the family of tame distributions as follows. ", "page_idx": 14}, {"type": "text", "text": "Definition A.8. A distribution $\\mathcal{D}$ over $\\mathcal{X}$ is $k$ -tame if for every degree- $k$ polynomial $p$ over $\\mathcal{X}$ with $\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}[(p(\\mathbf{x}))^{2}]\\le1$ and every $B$ greater than $e^{2k}$ we have $\\mathbb{P}_{\\mathbf{x}\\sim\\mathcal{D}}[(p(\\mathbf{x}))^{2}>B]\\le e^{-\\Omega(B^{\\frac{1}{2k}})}$ . ", "page_idx": 14}, {"type": "text", "text": "It is known that the standard Gaussian distribution, any log-concave distribution, as well as the uniform distribution over the hypercube $\\{\\pm1\\}^{d}$ are tame (see [DKS18] and references therein). ", "page_idx": 14}, {"type": "text", "text": "Fact A.9. Let $\\mathcal{D}$ be either the standard Gaussian distribution $\\mathcal{N}$ or the uniform distribution over $\\{\\pm1\\}^{d}$ , then, then $\\mathcal{D}$ is $k$ -tame for any $k\\in\\mathbb{N}$ . ", "page_idx": 14}, {"type": "text", "text": "Fact A.10. Let $\\mathcal{D}$ be a log-concave distribution over $\\mathbb{R}^{d}$ , then $\\mathcal{D}$ is $k$ -tame for any $k\\in\\mathbb{N}$ . ", "page_idx": 14}, {"type": "text", "text": "A.5 Sandwiching Polynomials ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We provide a formal definition of the $\\mathcal{L}_{2}$ sandwiching property which is key in order to be able to apply our outlier removal process many of for our main learning applications. ", "page_idx": 14}, {"type": "text", "text": "Definition A.11 ( ${\\mathcal{L}}_{2}$ Sandwiching). For a concept class $\\mathcal{F}$ , a distribution $\\mathcal{D}$ over $\\mathcal{X}$ , $\\epsilon\\in(0,1)$ , we say that $\\mathcal{F}$ has $\\epsilon{-}\\mathcal{L}_{2}$ sandwiching degree $k$ with respect to $\\mathcal{D}$ if for any $f\\in\\mathcal F$ , there exist polynomials $p_{\\mathrm{up}},p_{\\mathrm{low}}$ over $\\mathcal{X}$ with degree at most $k$ such that (1) $p_{\\mathrm{low}}(\\mathbf{x})\\leq f(\\mathbf{x})\\leq p_{\\mathrm{up}}(\\mathbf{x})$ for all $\\mathbf{x}\\in\\mathcal{X}$ and (2) $\\begin{array}{r}{\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}[(p_{\\mathrm{up}}(\\mathbf{x})-p_{\\mathrm{low}}(\\mathbf{x}))^{2}]\\leq\\epsilon.}\\end{array}$ . If the coefficients of $p_{\\mathrm{up}},p_{\\mathrm{low}}$ are all absolutely bounded by $B$ , we say that $\\mathcal{F}$ has $\\epsilon{-}\\mathcal{L}_{2}$ sandwiching coefficient bound $B$ . ", "page_idx": 14}, {"type": "text", "text": "In order to obtain our learning results, in addition to ${\\mathcal{L}}_{2}$ sandwiching, we use some further properties of the marginal distribution (see Definition 4.5). The following proposition from [KSV24b] shows that these properties are true for the Gaussian distribution as well as the uniform distribution over the hypercube $\\{\\pm1\\}^{d}$ . ", "page_idx": 14}, {"type": "text", "text": "Proposition A.12 (Appendix D in [KSV24b]). Let $\\mathcal{D}$ be either $\\mathcal{N}_{d}$ or $\\operatorname{Unif}(\\{\\pm1\\}^{d})$ and let $\\mathcal{F}$ be some concept class with $\\epsilon\\!-\\!\\mathcal{L}_{2}$ sandwiching degree $k$ with respect to $\\mathcal{D}$ . Then, $\\mathcal{F}\\,\\epsilon{-}\\mathcal{L}_{2}$ sandwiching coefficient bound $B=d^{O(k)}$ and $(\\mathcal{D},\\mathcal{F})$ is $(\\epsilon,\\delta,k,m)$ -reasonable, where $\\begin{array}{r}{m=O(\\frac{1}{\\delta})(d k)^{O(k)}}\\end{array}$ . ", "page_idx": 14}, {"type": "text", "text": "Finally, we list a number of fundamental concept classes that are known to admit low-degree $\\mathcal{L}_{2}$ sandwiching approximators. ", "page_idx": 14}, {"type": "text", "text": "Lemma A.13 (Decision Trees, Lemma 34 in [KSV24b]). Let $\\mathcal{D}$ be the uniform distribution over the hypercube $\\mathcal{X}=\\left\\{\\pm1\\right\\}^{d}$ . For $s\\in\\mathbb{N}$ , let $\\mathcal{F}$ be the class of Decision Trees of size s. Then, for any $\\epsilon>0$ the $\\epsilon{-}\\mathcal{L}_{2}$ sandwiching degree of $\\mathcal{F}$ is at most $k=O(\\log(s/\\epsilon))$ . ", "page_idx": 14}, {"type": "text", "text": "Lemma A.14 (Boolean Formulas, Theorem 6 in [OS03] and Lemma 35 in [KSV24b]). Let $\\mathcal{D}$ be the uniform distribution over the hypercube $\\mathcal{X}=\\left\\{\\pm1\\right\\}^{d}$ . For $s,\\ell\\in\\mathbb{N}$ , let $\\mathcal{F}$ be the class of Boolean formulas of size at most s, dept\u221ah at most $\\ell$ . Then, for any $\\epsilon>0$ the $\\epsilon{-}\\mathcal{L}_{2}$ sandwiching degree of $\\mathcal{F}$ is at most $k=(C\\log(s/\\epsilon))^{5\\ell/2}\\sqrt{s}$ , for some sufficiently large universal constant $C>0$ . ", "page_idx": 14}, {"type": "text", "text": "Lemma A.15 (Intersections and Decision Trees of Halfspaces, Lemma 37 in [KSV24b]). Let $\\mathcal{D}$ be either the uniform distribution over the hypercube $\\mathcal{X}=\\left\\{\\pm1\\right\\}^{d}$ or the Gaussian $\\mathcal{N}_{d}$ over $\\mathcal{X}=\\mathbb{R}^{d}$ . For $\\ell\\in\\mathbb{N},$ , let also $\\mathcal{F}$ be the class of intersections of $\\ell$ halfspaces on $\\mathcal{X}$ . Then, for any $\\epsilon>0$ the $\\epsilon{-}\\mathcal{L}_{2}$ sandwiching degree of $\\mathcal{F}$ is at most $\\begin{array}{r}{k=\\widetilde O(\\frac{\\ell^{6}}{\\epsilon^{2}})}\\end{array}$ . For Decision Trees of halfspaces of size s and depth \u2113, the bound is $\\begin{array}{r}{k=\\widetilde{O}(\\frac{s^{2}\\ell^{6}}{\\epsilon^{2}})}\\end{array}$ . ", "page_idx": 14}, {"type": "text", "text": "A.6 Other Learning Settings ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Our approach provides new results even for standard learning scenarios (without distribution shift). In particular, we provide the first tolerant testable learning algorithms. In the testable learning setting, there is no distribution shift, but rather, the learner receives labeled examples from some distribution $\\mathcal{D}_{\\mathcal{X}\\mathcal{Y}}$ over $\\mathcal{X}\\times\\{\\pm1\\}$ and is asked to either reject, or accept and output a hypothesis with low error on $\\mathcal{D}_{\\mathcal{X}\\mathfrak{X}}$ . The target error is opt $+\\,\\epsilon$ , where opt $\\begin{array}{r}{\\mathbf{\\Phi}=\\operatorname*{min}_{f\\in\\mathcal{F}}\\operatorname{err}(f;\\mathcal{D}_{\\mathcal{X}\\mathcal{Y}})}\\end{array}$ . In order to obtain efficient learners, we allow for the algorithm to reject if it detects that the marginal distribution $\\mathcal{D}^{\\prime}$ of $\\mathcal{D}_{\\mathcal{X}\\mathcal{X}}$ on $\\mathcal{X}$ is not equal to some given target distribution $\\mathcal{D}$ which is known to be well-behaved. The tester is, once more, allowed to reject even when $\\mathcal{D}^{\\prime}$ and $\\mathcal{D}$ differ by a tiny amount. We are interested in tolerant testable learning, where the tester-learner is required to accept when $\\mathcal{D}^{\\prime}$ is moderately close to $\\mathcal{D}$ and provide the first upper bounds for the problem. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Definition A.16 (Tolerant Testable Learning, extension of [RV23]). Let $\\mathcal{F}$ be a concept class over $\\mathcal{X}\\subseteq\\mathbb{R}^{d}$ and $\\mathcal{D}$ a distribution over $\\mathcal{X}$ . The algorithm $\\boldsymbol{\\mathcal{A}}$ is a tester-learner for $\\mathcal{F}$ with respect to $\\mathcal{D}$ up to error $\\gamma$ , tolerance $\\theta$ and probability of failure $\\delta$ if, upon receiving $m$ labeled samples from a distribution $\\mathcal{D}_{\\mathcal{X}3}$ with $\\mathcal{X}$ -marginal $\\mathcal{D}^{\\prime}$ , algorithm $\\boldsymbol{\\mathcal{A}}$ either rejects or accepts and outputs a hypothesis $h:\\mathcal{X}\\to\\{\\pm1\\}$ such that, w.p. at least $1-\\delta$ , the following hold: ", "page_idx": 15}, {"type": "text", "text": "(a) (soundness) Upon acceptance, the error is bounded as $\\mathbb{P}_{(\\mathbf{x},y)\\sim\\mathcal{D}_{x y}}[y\\neq h(\\mathbf{x})]\\leq\\mathsf{o p t}+\\gamma.$ . ", "page_idx": 15}, {"type": "text", "text": "(b) (completeness) If $\\mathrm{d}_{\\mathrm{TV}}(D,D^{\\prime})\\leq\\theta$ , then the algorithm accepts. ", "page_idx": 15}, {"type": "text", "text": "The optimum error opt is defined as $\\begin{array}{r}{\\mathsf{o p t}=\\operatorname*{min}_{f\\in\\mathcal{F}}\\mathbb{P}_{(\\mathbf{x},y)\\sim\\mathcal{D}_{x y}}[y\\neq f(\\mathbf{x})].}\\end{array}$ ", "page_idx": 15}, {"type": "text", "text": "Finally, we give a definition for the model of learning with nasty noise (proposed by [BEK02]). ", "page_idx": 15}, {"type": "text", "text": "Definition A.17 (Learning with Nasty Noise [BEK02]). Let $\\mathcal{F}$ be a concept class over $\\mathcal{X}\\subseteq\\mathbb{R}^{d}$ and $\\mathcal{D}$ a distribution over $\\mathcal{X}$ . The algorithm $\\boldsymbol{\\mathcal{A}}$ is a learner for $\\mathcal{F}$ with respect to $\\mathcal{D}$ , robust under nasty noise with rate $\\eta\\in\\left(0,1\\right)$ , up to error $\\gamma$ and probability of failure $\\delta$ if the following hold. If the algorithm $\\boldsymbol{\\mathcal{A}}$ receives a set of $N$ labeled samples $S$ that are formed by some adversary who first draws $N$ i.i.d. labeled samples $S_{\\mathrm{iid}}$ from $\\mathcal{D}$ , labeled by some concept $f^{*}\\in\\mathcal{F}$ and then corrupts at most $\\eta N$ (arbitrarily chosen) elements of $S_{\\mathrm{iid}}$ and substitutes them by $\\eta N$ arbitrary points of $\\mathcal{X}\\times\\{\\pm1\\}$ , then $\\boldsymbol{\\mathcal{A}}$ outputs w.p. at least $1-\\delta$ some hypothesis $h:\\mathcal{X}\\to\\{\\pm1\\}$ such that $\\mathbb{P}_{\\mathbf{x}\\sim\\mathcal{D}}[f^{*}(\\mathbf{x})\\neq h(\\mathbf{x})]\\le\\gamma$ . ", "page_idx": 15}, {"type": "text", "text": "The error $\\gamma$ is a function of the noise rate $\\eta$ . ", "page_idx": 15}, {"type": "text", "text": "B Additional Tools ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Miscellaneous lemmas ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section we present two technical lemmas used for the design and analysis of our flitering algorithm. The following lemma allows one to efficiently estimate the moments of a $k$ -tame distribution and is used for the algorithms of Theorem E.1 and Theorem E.2. ", "page_idx": 15}, {"type": "text", "text": "Algorithm 2: Monomial Correlations Matrix Estimation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Input: Set $S$ of size $N$ , containing points in $\\mathcal{X}\\subseteq\\mathbb{R}^{d}$ , parameter $\\delta$ and parameter $k\\in\\mathbb N$ Output: A matrix $\\widehat{M}$ ", "page_idx": 15}, {"type": "text", "text": "Partition $S_{D}$ into $\\sqrt{N}$ parts $(S_{\\mathcal{D}}^{(i)})_{i\\in[\\sqrt{N}]}$ , each of size $\\sqrt{N}$ ", "page_idx": 15}, {"type": "text", "text": "Compute the matrix $\\widehat{M_{i}}=\\mathbb{E}_{\\mathbf{x}\\sim S_{\\mathcal{D}}^{(i)}}[\\left(\\mathbf{x}^{\\otimes k}\\right)(\\mathbf{x}^{\\otimes k})^{\\top}]$ for each $i\\in[\\sqrt{N}]$ . ", "page_idx": 15}, {"type": "text", "text": "Let $\\widehat{M}=\\widehat{M}_{i}$ for some $i$ such that the number of indices $j\\in[\\sqrt{N}]$ for which the following condition holds ", "page_idx": 15}, {"type": "text", "text": "is at least $0.8\\sqrt{N}$ , if such an index $i\\in[\\sqrt{N}]$ exists.   \nOtherwise, let $\\widehat{M}=\\widehat{M}_{1}$ . ", "page_idx": 15}, {"type": "text", "text": "Lemma B.1. For some sufficiently large absolute constant $C$ , the following holds. There is an algorithm (Algorithm 2) that takes a parameter $\\delta$ in $(0,1)$ , a positive integer $k$ and $N\\ \\geq$ $C\\left((k d)^{k}\\log1/\\delta\\right)^{C}$ i.i.d. examples from a $k$ -tame distribution $\\mathcal{D}$ over $\\mathbb{R}^{d}$ . The algorithm runs in time $\\mathrm{poly}(N)$ and with probability at least $1\\!-\\!\\delta$ the outputs an $m\\times m$ symmetric positive-semidefinite matrixM  that for every degree- $k$ polynomial $p$ satisfies ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{9}{10}\\underset{\\mathbf{x}\\sim\\mathcal{D}}{\\mathbb{E}}[(p(\\mathbf{x}))^{2}]\\leq p^{\\top}\\widehat{M}p\\leq\\frac{11}{10}\\underset{\\mathbf{x}\\sim\\mathcal{D}}{\\mathbb{E}}[(p(\\mathbf{x}))^{2}].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. The run-time bound of $\\mathrm{poly}(N)$ is immediate. We now argue that the algorithm succeeds with probability at least $1-\\delta$ when the absolute constant $C$ is large enough. The matrixM is symmetric positive-semidefinite because each matrix $M_{i}=\\mathbb{E}_{\\mathbf{x}\\sim S_{i}}\\left[(\\mathbf{x}^{\\otimes k})(\\breve{\\mathbf{x}}^{\\otimes k})^{\\top}\\right]$ is symm etric positive-semidefinite by construction. ", "page_idx": 16}, {"type": "text", "text": "Let $M$ denote the matrix $\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}\\left[(\\mathbf{x}^{\\otimes k})(\\mathbf{x}^{\\otimes k})^{\\top}\\right]$ . Clearly $\\mathbb{E}_{S_{i}}[M_{i}]\\;=\\;M$ , and we will use the second moment method to bound the deviation, but first we observe that with probability 1 for every polynomial $p$ in the nullspace of $M$ we also have $p^{\\top}\\mathbb{E}_{S_{i}}[M_{i}]p=0$ . Indeed, this is the case because $p^{\\top}M p=\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}(p(\\mathbf{x}))^{2}$ means that $p(\\mathbf{x})^{2}=0$ almost surely for $\\mathbf{x}\\sim\\mathcal{D}$ . Thus, with probability 1 this holds for a collection of basis elements for the nullspace of $M$ , and consequently for the entire nullspace of $M$ . ", "page_idx": 16}, {"type": "text", "text": "Now, we bound the deviation between $M$ and $M_{i}$ for polynomials $p$ for which $p^{\\top}M p>0$ . Let $m^{\\prime}$ be such that $m-m^{\\prime}$ is the dimension of the nullspace of $M$ . Then there is a collection $\\{r_{1},\\cdot\\cdot\\cdot,r_{m^{\\prime}}\\}$ of degree- $\\cdot k$ polynomials that satisfy ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\underset{\\mathbf{x}\\sim\\mathcal{D}}{\\mathbb{E}}[r_{j}(\\mathbf{x})r_{j^{\\prime}}(\\mathbf{x})]=\\left\\{1\\begin{array}{l l}{\\mathrm{\\quadif\\;}j=j^{\\prime}}\\\\ {\\mathrm{\\quadotherwise.}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "(Such collection necessarily exists via the Gram-Schmidt process.) Overall, for any polynomial $p$ ", "page_idx": 16}, {"type": "equation", "text": "$$\np^{\\top}M p=\\underset{{\\mathbf{x}}\\sim{\\mathcal{D}}}{\\mathbb{E}}[(p({\\mathbf{x}}))^{2}]=\\sum_{j}\\left(\\underset{{\\mathbf{x}}\\sim{\\mathcal{D}}}{\\mathbb{E}}[p({\\mathbf{x}})r_{j}({\\mathbf{x}})]\\right)^{2}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, denoting $\\{e_{1},\\cdot\\cdot\\cdot,e_{m}\\}$ the $m$ basis vectors in $\\mathbb{R}^{m}$ we have ", "page_idx": 16}, {"type": "equation", "text": "$$\nM^{1/2}p=\\left[\\begin{array}{c}{{\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}[p(\\mathbf{x})r_{1}(\\mathbf{x})]}}\\\\ {{\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}[p(\\mathbf{x})r_{2}(\\mathbf{x})]}}\\\\ {{\\vdots}}\\\\ {{\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}[p(\\mathbf{x})r_{m^{\\prime}}(\\mathbf{x})]}}\\\\ {{0}}\\\\ {{\\cdots\\cdots}}\\\\ {{0}}\\end{array}\\right]\\qquad\\qquad M^{-1/2}\\left(\\sum_{i=1}^{m^{\\prime}}c_{i}e_{i}\\right)=\\sum_{i=1}^{m^{\\prime}}c_{i}r_{i}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "(Where $M^{-1/2}p$ is defined to be the Moore-Penrose pseudo-inverse of $M^{1/2}$ if $M$ is singular). We now bound the expected Frobenius norm: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathbb{E}_{s}\\left[\\left\\|I-M^{-1/2}M_{i}M^{-1/2}\\right\\|_{F}^{2}\\right]=\\mathbb{E}_{s}\\left[\\sum_{j,j^{\\prime}\\in\\{1,\\cdots,m\\}}\\left(e_{j}^{\\top}\\left(I-M^{-1/2}M_{i}M^{-1/2}\\right)e_{j^{\\prime}}\\right)^{2}\\right]=}\\\\ &{=\\displaystyle\\mathbb{E}_{s}\\left[\\sum_{j,j^{\\prime}\\in\\{1,\\cdots,m\\}}\\left(\\mathbf{1}_{j=j^{\\prime}}-r_{j}^{\\top}M_{i}r_{j^{\\prime}}\\right)^{2}\\right]=\\mathbb{E}_{s}\\left[\\sum_{j,j^{\\prime}\\in\\{1,\\cdots,m\\}}\\left(\\sum_{\\mathbf{x}\\sim\\mathcal{D}}\\left[r_{j}(\\mathbf{x})r_{j^{\\prime}}(\\mathbf{x})\\right]-r_{j}^{\\top}M_{i}r_{j^{\\prime}}\\right)^{2}\\right]=}\\\\ &{=\\displaystyle\\sum_{j,j^{\\prime}}\\mathbb{E}_{i}\\left(\\underbrace{\\mathbb{E}_{\\mathbf{\\Theta}}}_{\\mathbf{x}\\sim\\mathcal{D}}\\left[r_{j}(\\mathbf{x})r_{j^{\\prime}}(\\mathbf{x})\\right]-\\underbrace{\\mathbb{E}_{i}\\left[r_{j}(\\mathbf{x})r_{j^{\\prime}}(\\mathbf{x})\\right]}_{\\mathbf{x}\\sim S_{i}}\\right)^{2}=\\frac{1}{\\sqrt{N}}\\sum_{j_{1},j_{2}\\in\\{1,\\cdots,m\\}}\\mathrm{Var}_{\\mathbf{x}\\sim\\mathcal{D}}(r_{j}(\\mathbf{x})r_{j^{\\prime}}(\\mathbf{x}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "From the $k$ -tameness of distribution $\\mathcal{D}$ , and the fact that $\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}[(r_{j}(\\mathbf{x}))^{2}]=1$ we see that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathrm{Var}_{\\mathbf{x}\\sim\\mathcal{D}}(r_{j}(\\mathbf{x})r_{j^{\\prime}}(\\mathbf{x}))\\le\\underset{\\mathbf{x}\\sim\\mathcal{D}}{\\mathbb{E}}\\left[(r_{j}(\\mathbf{x})r_{j^{\\prime}}(\\mathbf{x}))^{2}\\right]=\\int_{0}^{\\infty}\\mathbb{P}\\left[r_{j}(\\mathbf{x})r_{j^{\\prime}}(\\mathbf{x}))^{2}>B^{2}\\right]2B\\;d B\\le}\\\\ &{}&{2e^{4k}+\\int_{e^{2k}}^{\\infty}e^{-\\Omega(B^{1/(2k)})}2B\\;d B=O(k^{O(k)}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus, combining Equation B.2 and Equation B.3 and recalling that $M$ is an $m\\times m$ matrix with $m\\leq d^{k}$ we get a bound for the expected spectral norm of $M-M_{i}$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\mathbb{E}}{S_{i}}\\left[\\left\\Vert I-M^{-1/2}M_{i}M^{-1/2}\\right\\Vert_{2}\\right]\\leq\\frac{\\mathbb{E}}{S_{i}}\\left[\\left\\Vert I-M^{-1/2}M_{i}M^{-1/2}\\right\\Vert_{F}\\right]\\leq}\\\\ &{\\qquad\\qquad\\qquad\\sqrt{\\frac{\\mathbb{E}}{S_{i}}\\left[\\left\\Vert I-M^{-1/2}M_{i}M^{-1/2}\\right\\Vert_{F}^{2}\\right]}\\leq\\frac{1}{N^{1/4}}m O(k^{O(k)})\\leq\\frac{1}{N^{1/4}}(d k)^{O(k)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Recalling that $N\\,\\geq\\,C\\left((k d)^{k}\\log1/\\delta\\right)^{C}$ , we see that for a sufficiently large absolute constant $C$ we can use the Chebyshev\u2019s inequality to conclude that with probability at least 0.9 we have $\\left\\Vert I-M^{-1/2}M_{i}M^{-1/2}\\right\\Vert_{2}\\leq10^{-3}$ , which implies that for any $p$ of degree at most $k$ we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{p^{\\top}M_{i}p=p^{\\top}M p+p^{\\top}(M_{i}-M)p=p^{\\top}M p+p^{\\top}M^{1/2}(M^{-1/2}M_{i}M^{-1/2}-I)M^{1/2}p}&{}\\\\ {\\in[(1-10^{-3})p^{\\top}M p,(1+10^{-3})p^{\\top}M p]}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Recalling that $i$ is in $\\{1,\\cdot\\cdot\\cdot,\\sqrt{N}\\}$ and using the standard Hoeffding\u2019s inequality, we see that when $C$ is sufficiently large, with probability at least $1-\\delta$ , the above holds for at least 0.95 fraction of indices $i$ . Call such indices good. For any pair of values $i_{1},\\,i_{2}$ of good indices we hence have ", "page_idx": 17}, {"type": "equation", "text": "$$\n(1-10^{-2})p^{\\top}M_{i_{2}}p\\leq p^{\\top}M_{i_{1}}p\\leq(1+10^{-2})p^{\\top}M_{i_{2}}p\\,,{\\mathrm{~for~all~}}p\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "For any given indices, the above property can be checked by computing the maximum singular value of the matrix $M_{i_{2}}^{-1/2}M_{i_{1}}M_{i_{2}}^{-1/2}$ (where we take the Moore-Penrose pseudoinverse) and comparing the nullspaces of $\\overline{{M_{i_{1}}}}$ and ${\\dot{M_{i_{2}}}}$ . Therefore, the output $i_{1}=i^{*}$ satisfies the above property for at least 0.8 fraction of the values of $i_{2}$ , according to Algorithm 2. Moreover, $i^{*}$ satisfies the above property for at least one good index $i_{2}=i_{g}$ (the fraction of good indices is 0.95 and the property is satisfied for at least a 0.8 fraction). Overall, we have that for all polynomials $p$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n0.9p^{\\mathsf{T}}M p\\leq0.99p^{\\mathsf{T}}M_{i_{g}}p\\leq p^{\\mathsf{T}}M_{i^{*}}p\\leq1.01p^{\\mathsf{T}}M_{i_{g}}p\\leq1.1p^{\\mathsf{T}}M p\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which implies the correctness of the algorithm. ", "page_idx": 17}, {"type": "text", "text": "The following lemma allows one to show that as long as a distribution $\\mathcal{D}^{\\prime}$ is flitered using a low-VCdimension function $f$ , the moments of the resulting flitered dataset approximate well the moments of the distribution one obtains by filtering the distribution $\\mathcal{D}^{\\prime}$ using $f$ . ", "page_idx": 17}, {"type": "text", "text": "Lemma B.2. Let $\\mathcal{D}$ be a probability distribution over $\\mathbb{R}^{d}$ and let $\\mathcal{F}$ be a function class over $\\mathbb{R}^{d}$ taking values in $\\{0,1\\}$ with VC dimension $V$ , such that for every $f$ in $\\mathcal{F}$ we have $f(\\mathbf{x})=0$ for all x such that $\\operatorname*{max}_{p\\colon\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}(p(\\mathbf{x}))^{2}\\leq1}(p(\\mathbf{x}))^{2}>B$ . Let $\\mathcal{D}^{\\prime}$ be a probability distribution over $\\mathbb{R}^{d}$ and $\\textit{S b e}$ collection of $N$ i.i.d. samples from $\\mathcal{D}^{\\prime}$ , then with probability at least $1-\\delta$ we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{f\\in\\mathcal{F},\\;p\\;s;t\\mathrm{:}\\;\\deg(p)\\leq k,}\\Big|\\frac{1}{N}\\sum_{\\mathbf{x}\\sim S}\\Big[f(\\mathbf{x})(p(\\mathbf{x}))^{2}\\Big]-\\underset{\\mathbf{x}\\sim\\mathcal{D}^{\\prime}}{\\mathbb{E}}\\big[f(\\mathbf{x})(p(\\mathbf{x}))^{2}\\big]\\Big|\\leq O\\Big(B^{\\frac{1}{2}}\\Big(V d^{2k}\\frac{\\log N}{N}\\log\\frac{1}{\\delta}\\Big)^{\\frac{1}{4}}\\Big).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. Let $p$ be a polynomial of degree $k$ s.t: $\\begin{array}{r}{\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}(p(\\mathbf{x}))^{2}\\leq1}\\end{array}$ and let $f$ be a function in $\\mathcal{F}$ . We   \nrecall that whenever $j^{\\boldsymbol{\\mathsf{\\Delta}}}(\\mathbf{x})\\neq0$ we have $\\operatorname*{max}_{\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}(p(\\mathbf{x}))^{2}\\leq1}(p(\\mathbf{x}))^{2}\\leq B$ and we let $\\Delta$ be a positive real p:   \nnumber, to be chosen later. We then have via the triangle inequality ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\mathbb{E}_{\\delta}}{\\mu(x)}\\underset{x\\sim\\delta}{\\left[f(\\mathbf{x})(p(\\mathbf{x}))^{2}\\right]}-\\frac{\\mathbb{E}_{\\delta}}{\\mathbf{x}\\cdot\\delta}\\left[f(\\mathbf{x})(p(\\mathbf{x}))^{2}\\right]\\Bigg|\\leq}\\\\ &{\\frac{\\mathbb{E}_{\\delta}}{\\mu(x)}\\Big[\\underset{y\\sim\\delta}{\\sum}\\left\\{f(\\mathbf{x})(p(\\mathbf{x}))^{2}1_{\\#\\leq\\delta(\\eta(x))^{2}<(j+1),\\Delta}\\right\\}-\\frac{\\mathbb{E}_{\\delta}}{\\mathbf{x}\\sim\\mathcal{P}}\\left[f(\\mathbf{x})(p(\\mathbf{x}))^{2}\\mathbf{1}_{j\\delta\\leq(p(\\mathbf{x}))^{2}<(j+1),\\Delta}\\right]\\Big|\\leq}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\frac{\\mathbb{E}_{\\delta}}{\\mu(x)}\\left[\\underset{y\\sim\\delta}{\\sum}\\left\\{f(\\mathbf{x})(j\\Delta{\\texttt A}_{j\\delta\\leq(\\eta(x))^{2}<(j+1),\\Delta})\\right\\}-\\frac{\\mathbb{E}_{\\delta}}{\\mathbf{x}\\sim\\mathcal{P}}\\left[f(\\mathbf{x})j\\Delta{\\texttt A}_{j\\delta\\leq(\\eta(x))^{2}<(j+1),\\Delta}\\right]\\right|+}\\\\ &{+\\triangleq\\left(\\underset{y\\sim\\delta}{\\sum}\\frac{\\mu(\\lambda)}{\\mathbf{E}_{\\delta}\\sim\\mathcal{S}}\\left[f(\\mathbf{x})\\mathbf{1}_{j\\delta\\leq(p(\\mathbf{x}))^{2}<(j+1),\\Delta}\\right]+\\frac{\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{P}}}{\\mathbf{x}\\sim\\mathcal{P}}\\left[f(\\mathbf{x})\\mathbf{1}_{j\\delta\\leq(p(\\mathbf{x}))^{2}<(j+1),\\Delta}\\right]\\right)\\leq}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ 2\\Delta+B\\underset{y\\sim\\delta}{\\sum}\\left[\\underset{x\\sim\\delta}{\\sum}\\left[f(\\mathbf{x})-1\\right)\\wedge\\left(p(\\mathbf{x})^{2}\\in\\left(j\\Delta,(j+1)\\Delta\\right)\\right]\\right)-}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\cdot\\underset{\\mathbf{x}\\sim\\delta}{\\sum}\\left[f(\\mathbf{x})=1\\right]\\wedge\\left(p(\\mathbf{x})^{2}\\in\\left(j\\Delta,(j+1)\\Delta\\right)\\right)\\right]\\Big|~.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The function that maps $\\mathbf{x}$ to 1 if and only if $f(\\mathbf{x})=1$ and $p(\\mathbf{x})^{2}\\in(j\\Delta,(j+1)\\Delta]$ is a logical AND of a function in $\\mathcal{F}$ and two polynomial threshold functions of degree at most $2k$ . Thus, by Fact A.5 the VC dimension of these functions is at most $O(d^{2k}+V)\\leq\\breve{O}(d^{2k}\\cdot V)$ . Therefore, we can use Fact A.6 together with the inequality above to conclude that with probability at least $1-\\delta$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\mathbf{\\mu}\\in\\mathcal{F},\\ p{\\mathrm{~of~degree}}\\ k_{\\mathbf{\\mu}\\in\\mathbf{\\hat{X}}},\\mathbf{\\mu}\\left[\\mathbf{\\underline{{X}}}_{\\mathbf{\\hat{X}}\\sim\\mathcal{X}}\\left[f(\\mathbf{x})(p(\\mathbf{x}))^{2}\\right]-\\operatorname*{\\mathbb{E}}_{\\mathbf{x}\\sim\\mathcal{D}^{\\prime}}\\left[f(\\mathbf{x})(p(\\mathbf{x}))^{2}\\right]\\right]\\leq2\\Delta+O\\Big(\\frac{B}{\\Delta}\\sqrt{\\frac{V d^{2k}\\log N}{N}\\log\\frac{1}{\\delta}}\\Big).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Finally, taking $\\Delta$ to minimize the expression above, we recover our proposition. ", "page_idx": 18}, {"type": "text", "text": "C Certified Learning with Distribution Shift, Omitted Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "C.1 PQ Setting ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "C.1.1 General Halfspaces ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We now prove Theorem 4.3, which is restated here for convenience. ", "page_idx": 18}, {"type": "text", "text": "Theorem C.1 (PQ Learning of Halfspaces). For any $\\epsilon,\\delta\\in(0,1)$ , there is an algorithm that $P Q$ learns the class of general halfspaces with respect to $\\mathcal{N}_{d}$ in the realizable setting, up to error and rejection rate \u03f5 and probability of failure $\\delta$ that runs in time $\\mathrm{poly}(d^{\\log(\\frac{1}{\\epsilon})},\\log(1/\\bar{\\delta}))$ . ", "page_idx": 18}, {"type": "text", "text": "Proof. The algorithm does the following for sufficiently large universal constants $C_{1},C_{2},C_{3}\\geq1$ . ", "page_idx": 18}, {"type": "text", "text": "2. If either of these values is at most $\\epsilon^{C_{2}}/C_{1}$ , then let $g$ be the selector of Theorem 3.1 with inputs $\\epsilon$ $\\,k=C_{3}\\log(1/\\epsilon),\\alpha=\\epsilon/2$ and access to samples from $\\mathcal{D}$ and $\\mathcal{D}^{\\prime}$ and $h$ the constant hypothesis for the value in $\\{-1,1\\}$ with which the labels are most frequently consistent. ", "page_idx": 18}, {"type": "text", "text": "3. Otherwise, let $\\widehat{\\bf w}$ and $\\widehat{\\tau}$ be as in Proposition 4.4 from some sufficiently large labeled sample from th e traini n g distribution. Let $h(\\mathbf{x})=\\mathrm{sign}(\\widehat{\\mathbf{w}}\\cdot\\mathbf{x}+\\widehat{\\tau})$ and for $\\boldsymbol{\\mathcal{W}}=\\{(\\mathbf{w},\\boldsymbol{\\tau}):$ $\\|\\mathbf{w}\\!-\\!\\widehat{\\mathbf{w}}\\|_{2}\\le(\\epsilon/d)^{C_{2}}/C_{1},|\\tau\\!-\\!\\widehat{\\tau}|\\le(\\epsilon/d)^{C_{2}}/C_{1}\\}$ , let $g\\mathbf{(x)}$ (where $g:\\mathbb{R}^{d}\\rightarrow\\{0,1\\})$ ) be 0 if and on ly if there are $(\\mathbf{w}_{1},\\tau_{1}),(\\mathbf{w}_{2},\\tau_{2})\\in\\mathcal{W}$ such that $\\mathrm{sign}(\\mathbf{w}_{1}\\!\\cdot\\!\\mathbf{x}\\!+\\!\\tau_{1})\\neq\\mathrm{sign}(\\mathbf{w}_{2}\\!\\cdot\\!\\mathbf{x}\\!+\\!\\tau_{2})$ (which can be implemented via a linear program with quadratic constraints). ", "page_idx": 18}, {"type": "text", "text": "4. Return $(g,h)$ . ", "page_idx": 18}, {"type": "text", "text": "Note that when step 3 is activated, then, with high probability, we have that the bias $\\tau^{*}$ of the ground truth is $\\tau^{*}=O(\\sqrt{\\log(1/\\epsilon)})$ and, therefore, the samples required to apply Proposition 4.4 is polynomial in $1/\\epsilon$ . From Lemma 5.7 in [KSV24b], we then have that the selector $g$ has Gaussian rejection rate $\\epsilon$ , as desired. The accuracy guarantee for the case of step 3 is given by the guarantee of Proposition 4.4 combined with the fact that, with high probability, $h$ agrees with the ground truth anywhere outside the disagreement region (i.e., for all $\\mathbf{x}$ such that $g(\\mathbf{x})=1$ ). ", "page_idx": 18}, {"type": "text", "text": "When step 2 is activated, then the rejection rate is bounded by Theorem 3.1 and the accuracy guarantee is implied by the fact that $\\tau^{*}\\,\\geq\\,\\sqrt{C_{2}\\log(1/\\epsilon)/C}$ (for some universal constant $C\\geq1$ ) and the following reasoning, where we suppose, without loss of generality that $h(\\mathbf{x})=1$ . ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\mathbf{x}\\sim\\mathcal{D}_{x}^{\\mathrm{test}}}{\\mathbb{P}}[h(\\mathbf{x})\\neq\\mathrm{sign}(\\mathbf{w}^{*}\\cdot\\mathbf{x}+\\tau^{*}),g(\\mathbf{x})=1]\\leq\\underset{\\mathbf{x}\\sim\\mathcal{D}_{x}^{\\mathrm{test}}}{\\mathbb{P}}[1\\neq\\mathrm{sign}(\\mathbf{w}^{*}\\cdot\\mathbf{x}+\\tau^{*}),g(\\mathbf{x})=1]}\\\\ &{\\qquad\\qquad\\leq\\underset{\\mathbf{x}\\sim\\mathcal{D}_{x}^{\\mathrm{test}}}{\\mathbb{P}}[|\\mathbf{w}^{*}\\cdot\\mathbf{x}|>|\\tau^{*}|,g(\\mathbf{x})=1]}\\\\ &{\\qquad\\qquad\\leq\\frac{\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}_{x}^{\\mathrm{test}}}[(\\mathbf{w}^{*}\\cdot\\mathbf{x})^{2k}g(\\mathbf{x})]}{(\\tau^{*})^{2k}}}\\\\ &{\\qquad\\qquad\\leq\\frac{400}{\\epsilon}\\frac{(2C_{3}\\log(1/\\epsilon))^{k}}{(C_{2}\\log(1/\\epsilon)/C)^{k}}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where we used Markov\u2019s inequality and the guarantee from Theorem 3.1. Suppose that $C_{2}$ is sufficiently larger than $C_{3}$ and $C_{3}$ is sufficiently large. Then we have that $\\frac{(2C_{3}\\log(1/\\epsilon))^{k}}{(C_{2}\\log(1/\\epsilon)/C)^{k}}\\ \\ \\leq$ $(1/2)^{k}\\le\\epsilon^{C_{3}}$ , which gives that $\\mathbb{P}_{\\mathbf{x}\\sim\\mathcal{D}_{x}^{\\mathrm{test}}}[h(\\mathbf{x})\\neq\\mathrm{sign}(\\mathbf{w}^{*}\\cdot\\mathbf{x}+\\tau^{*}),g(\\mathbf{x})=1]\\leq400\\epsilon^{C_{3}}/\\epsilon\\leq\\epsilon$ for sufficiently large $C_{3}$ . \u53e3 ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "In the proof of Theorem 4.3, we use Proposition 4.4. However, the original version of Proposition 4.4 worked for constant probability of failure. We show the following general lemma which can be used to amplify the probability of success in logarithmic number of rounds. ", "page_idx": 19}, {"type": "text", "text": "Lemma C.2 (Parameter Recovery Success Probability Amplification). Let $\\mathcal{X}$ be a vector space and $\\|\\cdot\\|$ some norm. For some $\\mathbf{w}^{*}\\in\\mathcal{X}$ and $\\epsilon\\in(0,1)$ , suppose that an algorithm $\\boldsymbol{\\mathcal{A}}$ outputs with probability at least 0.9 some w $\\in\\mathcal{X}$ with $\\|\\mathbf{w}^{*}-\\mathbf{w}\\|\\leq\\epsilon$ . Then, for any $\\delta\\,\\in\\,(0,1)$ , if we run $\\boldsymbol{\\mathcal{A}}$ for $T\\,=\\,O(\\log(1/\\delta))$ independent rounds receiving outputs $W\\,=\\,\\bigl\\{\\mathbf{w}^{1},\\mathbf{w}^{2},\\colon\\cdot\\cdot,\\mathbf{w}^{\\top}\\bigr\\}$ , and take $\\begin{array}{r}{\\widehat{\\mathbf{w}}=\\arg\\operatorname*{min}_{\\mathbf{w}\\in W}\\sum_{\\mathbf{w}^{\\prime}\\in W}\\|\\mathbf{w}-\\mathbf{w}^{\\prime}\\|,}\\end{array}$ , then we have $\\|\\widehat{\\mathbf{w}}-\\mathbf{w}^{*}\\|\\leq5\\epsilon$ , with probability at least $1-\\delta$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. Let $W_{G}$ be the subset of $W$ corresponding to w such that $\\|\\mathbf{w}-\\mathbf{w}^{*}\\|\\leq\\epsilon$ , let $W_{B}=W\\backslash W_{G}$ and note that due to a standard Hoeffding bound, $\\textstyle|W_{G}|\\geq{\\frac{3T}{4}}$ , with probability at least $1-\\delta$ , for $T=C\\log(1/\\delta)$ , where $C$ is a sufficiently large universal constant. ", "page_idx": 19}, {"type": "text", "text": "Say that $\\|\\widehat{\\mathbf{w}}\\!-\\!\\mathbf{w}^{*}\\|=\\alpha$ . Then, by the triangle inequality, $\\|\\widehat{\\mathbf{w}}-\\mathbf{w}^{\\prime}\\|\\geq\\|\\widehat{\\mathbf{w}}-\\mathbf{w}^{*}\\|-\\|\\mathbf{w}^{*}-\\mathbf{w}^{\\prime}\\|\\geq\\alpha-\\epsilon$ for any $\\mathbf{w}^{\\prime}\\in W_{G}$ and we have the following ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{A}:=\\displaystyle\\sum_{\\mathbf{w}^{\\prime}\\in W}\\|\\widehat{\\mathbf{w}}-\\mathbf{w}^{\\prime}\\|=\\displaystyle\\sum_{\\mathbf{w}^{\\prime}\\in W_{G}}\\|\\widehat{\\mathbf{w}}-\\mathbf{w}^{\\prime}\\|+\\displaystyle\\sum_{\\mathbf{w}^{\\prime}\\in W_{B}}\\|\\widehat{\\mathbf{w}}-\\mathbf{w}^{\\prime}\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\geq|W_{G}|(\\alpha-\\epsilon)+\\displaystyle\\sum_{\\mathbf{w}^{\\prime}\\in W_{B}}\\|\\widehat{\\mathbf{w}}-\\mathbf{w}^{\\prime}\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Let w $\\in W_{G}$ . We have $\\|\\mathbf{w}-\\mathbf{w}^{\\prime}\\|\\leq\\|\\mathbf{w}-\\widehat{\\mathbf{w}}\\|+\\|\\widehat{\\mathbf{w}}-\\mathbf{w}^{\\prime}\\|\\leq\\|\\mathbf{w}-\\mathbf{w}^{*}\\|+\\|\\mathbf{w}^{*}-\\widehat{\\mathbf{w}}\\|+\\|\\widehat{\\mathbf{w}}-\\mathbf{w}^{\\prime}\\|\\leq$ $\\epsilon+\\alpha+\\|\\widehat{\\mathbf{w}}-\\mathbf{w}^{\\prime}\\|$ , for any $\\mathbf{w}\\in W$ . Therefore, in total, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A:=\\displaystyle\\sum_{\\mathbf w^{\\prime}\\in W}\\|\\mathbf w-\\mathbf w^{\\prime}\\|=\\displaystyle\\sum_{\\mathbf w^{\\prime}\\in W_{G}}\\|\\mathbf w-\\mathbf w^{\\prime}\\|+\\displaystyle\\sum_{\\mathbf w^{\\prime}\\in W_{B}}\\|\\mathbf w-\\mathbf w^{\\prime}\\|}\\\\ {\\leq2\\epsilon|W_{G}|+(\\epsilon+\\alpha)|W_{B}|+\\displaystyle\\sum_{\\mathbf w^{\\prime}\\in W_{B}}\\|\\widehat{\\mathbf w}-\\mathbf w^{\\prime}\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By the definition of $\\widehat{\\bf w}$ , we have that $\\widehat{A}-A\\leq0$ . With probability at least $1-\\delta$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{0\\geq\\widehat{A}-A\\geq|W_{G}|(\\alpha-3\\epsilon)-(\\alpha+\\epsilon)(T-|W_{G}|)}\\\\ &{\\qquad\\qquad\\geq\\displaystyle\\frac{3T}{4}(\\alpha-3\\epsilon)-(\\alpha+\\epsilon)\\frac{T}{4}}\\\\ &{\\qquad\\qquad\\geq T(\\frac{\\alpha}{2}-\\frac{5\\epsilon}{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, $\\alpha\\leq5\\epsilon$ , which concludes the proof. ", "page_idx": 19}, {"type": "text", "text": "C.2 Adversarial Setting ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Our results on realizable PQ learning extend to the following related model, where the evaluation set is formed by some adversary. In the following definition, we consider each element of the considered sets to be a separate object (even if the corresponding value is the same with some other element of the set). ", "page_idx": 19}, {"type": "text", "text": "Definition C.3 (Tranductive Learning [GKKM20]). Let $\\mathcal{F}$ be a concept class over $\\mathcal{X}\\subseteq\\mathbb{R}^{d}$ and $\\mathcal{D}$ a distribution over $\\mathcal{X}$ . The algorithm $\\boldsymbol{\\mathcal{A}}$ is a transductive learner for $\\mathcal{F}$ with respect to $\\mathcal{D}$ , up to error $\\gamma$ , rejection rate $\\eta$ and probability of failure $\\delta$ if the following hold. If the algorithm $\\boldsymbol{\\mathcal{A}}$ has access to labeled examples from the distribution $\\mathcal{D}$ labeled by some concept $f^{*}\\in\\mathcal{F}$ and receives $N$ unlabeled samples $S$ that are formed by some adversary who first draws $N$ i.i.d. unlabeled samples $S_{\\mathrm{iid}}$ from $\\mathcal{D}$ and then corrupts any number of elements of $S_{\\mathrm{iid}}$ and substitutes them by the same number of arbitrary points of $\\mathcal{X}$ , then $\\boldsymbol{\\mathcal{A}}$ outputs w.p. at least $1-\\delta$ some set $S_{\\mathrm{{filt}}}$ and $h:\\mathcal{X}\\to\\{\\pm1\\}$ such that: ", "page_idx": 19}, {"type": "text", "text": "(a) (accuracy) The error after filtering is bounded as $\\begin{array}{r}{\\sum_{(\\mathbf{x},y)\\in S_{\\mathrm{filt}}}\\mathbb{1}\\{y\\ne h(\\mathbf{x})\\}\\le\\gamma N.}\\end{array}$ ", "page_idx": 19}, {"type": "text", "text": "In particular, we have $\\#\\{\\mathbf{x}:\\mathbf{x}\\in S\\setminus S_{\\mathrm{filt}}\\}\\leq2\\eta N.$ ", "page_idx": 20}, {"type": "text", "text": "Our proofs of Theorems 4.3 and 4.6 generalize in this setting exactly analogously, with the only difference being the use of Theorem E.1 in place of Theorem 3.1 for the outlier removal process. The learning phase is not different, since the learner has sample access to clean examples from the labeled training distribution. ", "page_idx": 20}, {"type": "text", "text": "For the proof of Theorem 4.3, we either run the outlier removal process to fliter the evaluation dataset in order to ensure that it is concentrated in every direction (in the case when almost all the training examples have the same label) or, if the training examples are indeed informative, we reject only the examples that fall inside the disagreement region. The arguments hold analogously. ", "page_idx": 20}, {"type": "text", "text": "For the proof of Theorem 4.6, we fliter the evaluation dataset by using degree $k$ outlier removal (see Theorem E.1) and run polynomial regression on the training distribution to find a hypothesis that has low error on the remaining points of the evaluation dataset. Once more, the analysis is analogous to the one for PQ learning. ", "page_idx": 20}, {"type": "text", "text": "C.3 Tolerant TDS Learning ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We now prove Theorem 5.3, which we restate for convenience. ", "page_idx": 20}, {"type": "text", "text": "Theorem C.4 (Tolerant TDS Learning via Sandwiching). For $\\epsilon,\\theta,\\delta\\in(0,1)$ , let $\\mathcal{X}\\subseteq\\mathbb{R}^{d}$ and $(\\mathcal{D},\\mathcal{F})$ be an $\\left(\\frac{\\epsilon}{C},\\frac{\\delta}{C},k,m\\right)$ -reasonable pair (Definition 4.5) for some sufficiently large universal constant $C>0$ . Then, there is an TDS learner $\\mathcal{F}$ with respect to $\\mathcal{D}$ up to error $O(\\lambda)+2\\theta+\\epsilon$ , tolerance $\\theta$ and probability of failure $\\delta$ with sample complexity $m+\\mathrm{poly}(\\frac{1}{\\epsilon}(k d)^{k}\\log(1/\\delta))$ and time complexity $\\mathrm{poly}(\\frac{m}{\\epsilon}(k d)^{k}\\log(1/\\delta))$ . ", "page_idx": 20}, {"type": "text", "text": "Note that the notion of tolerance in property testing was introduced in [PRR06] and has been the focus of many works including [FF05, VV11, ${\\tt B C E}^{+}19$ , RV20, CJKL22, $\\mathrm{CFG}^{+}22$ , BH18, CP23]. However, over $\\mathbb{R}^{d}$ all existing tolerant distribution testing algorithms (such as [VV11]) have run-times and sample complexities of $\\bar{2}^{\\Omega(d)}$ , which greatly exceeds our run-times. ", "page_idx": 20}, {"type": "text", "text": "Proof of Theorem 5.3. The algorithm first runs the outlier removal process of Theorem 3.1 with parameters $\\alpha\\,\\leftarrow\\,1,\\,\\epsilon\\,\\leftarrow\\,\\epsilon/{\\bar{C}},\\,\\delta\\,\\leftarrow\\,\\delta/C$ and $k\\,\\leftarrow\\,k$ , to receive the selector $g\\,:\\,\\mathcal{X}\\,\\rightarrow\\,\\{0,1\\}$ . Using a large enough sample from the test marginal $\\mathcal{D}_{\\mathcal{X}}^{\\mathrm{test}}$ , the algorithm estimates the quantity $\\mathbb{P}_{\\mathbf{x}\\sim\\mathcal{D}_{\\mathcal{X}}^{\\mathrm{test}}}[g(\\mathbf{x})=0]$ and rejects if the estimated value is larger than $2\\theta+{\\frac{2\\epsilon}{C}}$ . Otherwise, it runs the following box-constrained least squares problem, using at least $m$ labeled examples $S_{\\mathrm{train}}$ from $\\mathcal{D}^{\\mathrm{train}}$ , where $t=d^{k}$ and $B$ is the value specified in Definition 4.5. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{p}\\mathbb{E}_{(\\mathbf{x},y)\\sim S_{\\mathrm{train}}}[(y-p(\\mathbf{x}))^{2}]\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "s.t. $p$ has degree at most $k$ and coefficient bound $B$ ", "page_idx": 20}, {"type": "text", "text": "Let $\\widehat{p}$ be the minimizer of the above program. The algorithm accepts and returns classifier $h(\\mathbf{x})=$ $\\mathrm{sign}(\\widehat{p}(\\mathbf{x}))$ . ", "page_idx": 20}, {"type": "text", "text": "Soundness follows from the observation that $\\mathbb{P}_{(\\mathbf{x},y)\\sim\\mathcal{D}^{\\mathrm{test}}}[y~\\neq~h(\\mathbf{x})]\\,\\leq\\,\\mathbb{P}_{\\mathbf{x}\\sim\\mathcal{D}_{\\mathcal{X}}^{\\mathrm{test}}}[g(\\mathbf{x})~=~0]\\,+$ $\\mathbb{P}_{(\\mathbf{x},y)\\sim\\mathcal{D}^{\\mathrm{test}}}[y\\neq h(\\mathbf{x}),g(\\mathbf{x})=1]$ and the properties of $g$ according to Theorem 3.1, via an analysis which is analogous to the one used for Theorem 4.6, but with the difference that, since the parameter $\\alpha$ of the outlier removal process was chosen to be 1, the value $\\mathbb{P}_{(\\mathbf{x},y)\\sim\\mathcal{D}^{\\mathrm{test}}}[y\\neq h(\\mathbf{x}),g(\\bar{\\mathbf{x}})=1]$ is bounded by $O(\\lambda+{\\frac{\\epsilon}{C}})$ (instead of $O(\\frac{\\lambda}{\\eta}))$ . The term $\\mathbb{P}_{\\mathbf{x}\\sim\\mathcal{D}_{\\mathcal{X}}^{\\mathrm{test}}}[g(\\mathbf{x})=0]$ , when the test has accepted, is bounded by $2\\theta+\\epsilon/2$ . ", "page_idx": 20}, {"type": "text", "text": "For completeness, we assume that $\\mathrm{d}_{\\mathrm{TV}}({\\cal D},{\\cal D}_{\\chi}^{\\mathrm{test}})\\;\\leq\\;\\theta$ and observe that due to condition (b), $^3\\mathbf{\\hat{x}}_{\\times\\sim\\mathcal{D}_{x}^{\\mathrm{test}}}[g(\\mathbf{x})\\,=\\,0]\\,\\leq\\,\\mathbb{P}_{\\mathbf{x}\\sim\\mathcal{D}}[g(\\mathbf{x})\\,=\\,0]+\\mathrm{d}_{\\mathrm{TV}}^{\\sim}(\\mathcal{D},\\mathcal{D}_{x}^{\\mathrm{test}})\\,\\leq\\,2\\mathrm{d}_{\\mathrm{TV}}(\\mathcal{D},\\mathcal{D}_{x}^{\\mathrm{test}})\\,+\\,\\frac{\\epsilon}{C}\\,\\leq\\,2\\theta+\\,\\frac{\\epsilon}{C}.$ Therefore, the tester will accept with high probability. Furthermore, via Remark 3.3 our tolerant TDS learning algorithm will also with high probability accept any distribution $\\mathcal{D}^{\\prime}$ that is $1/2$ -smooth with respect to $\\mathcal{D}$ . \u53e3 ", "page_idx": 20}, {"type": "text", "text": "D Applications to Other Learning Settings ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Our techniques provide new results in other classical settings as well. In particular, we discuss applications on tolerant testable learning as well as robust learning. In total, our results for polynomial regression can be summarized in Table 2. ", "page_idx": 21}, {"type": "table", "img_path": "LnNfwc2Ah1/tmp/9844e9f9cb67733828a434acc77d3208eb808627f206873d9b6eec62b799062a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Table 2: Our learning results parameterized by $\\sigma$ , which captures the required precision of the ${\\mathcal{L}}_{2}$ -sandwiching approximators in each of the settings: (1) agnostic PQ learning with error $O(\\frac{\\lambda}{\\eta})+\\epsilon$ and rejection rate $\\eta$ , where $\\sigma=\\epsilon\\eta$ , (2) agnostic $\\theta$ -tolerant TDS learning with error $O(\\lambda)+2\\dot{\\theta}+\\epsilon$ , where $\\sigma=\\epsilon$ , (3) $\\theta$ -tolerant testable learning with excess error $2\\theta+\\epsilon$ , where $\\sigma=\\epsilon^{2}$ and (4) robust learning with nasty noise of rate $\\eta$ up to error $4\\eta+\\epsilon$ , where $\\sigma=\\epsilon^{2}$ . The probability of failure in each of the cases is some considered constant and $\\eta,\\theta,\\epsilon\\in(0,1)$ . ", "page_idx": 21}, {"type": "text", "text": "D.1 Classical Testable learning ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The following theorem gives the first dimension-efficient algorithms for tolerant testable learning (see Definition A.16) for various important concept classes. ", "page_idx": 21}, {"type": "text", "text": "Theorem D.1 (Tolerant Testable Learning via Sandwiching). For $\\epsilon,\\theta,\\delta\\in(0,1)$ , let $\\mathcal{X}\\subseteq\\mathbb{R}^{d}$ and $(\\mathcal{D},\\mathcal{F})$ be an $\\left(\\frac{\\epsilon^{2}}{C},\\frac{\\delta}{C},k,m\\right)$ -reasonable pair (Definition 4.5) for some sufficiently large universal constant $C>0$ . Then, there is a tester-learner for with respect to up to error opt $+~2\\theta+\\epsilon_{:}$ , tolerance $\\theta$ and probability of failure $\\delta$ with sample complexity $\\begin{array}{r}{\\hat{m}+\\mathrm{poly}(\\frac{1}{\\epsilon}(\\hat{k}d)^{k}\\log(1/\\dot{\\delta}))}\\end{array}$ and time complexity poly $\\mathit{\\check{\\Psi}}_{\\epsilon}^{m}(k d)^{k}\\log(1/\\delta))$ , where $\\mathsf{o p t}=\\operatorname*{min}_{f\\in\\mathcal{F}}\\mathrm{err}(f)$ . ", "page_idx": 21}, {"type": "text", "text": "Our plan is to once more make use of the outlier removal Theorem 3.1. In this case, is suffices to run tests on the marginal distribution that certify the existence of a low-degree polynomial approximator for the unknown ground truth concept (achieving optimum error), due to the following classical result from [KKMS08] (which has been used for non-tolerant testable learning in [GKK23]). ", "page_idx": 21}, {"type": "text", "text": "Proposition D.2 $\\mathcal{L}_{1}$ regression guarantee, [KKMS08]). Let $\\mathcal{F}$ be a concept class over $\\mathcal{X}$ where $\\mathcal{X}\\:\\bar{\\subseteq}\\:\\mathbb{R}^{d}$ and $\\mathcal{D}_{\\mathcal{X}\\mathfrak{X}}$ be any distribution over $x\\times\\{\\pm1\\}$ where the $\\mathcal{X}$ -marginal of $\\mathcal{D}_{\\mathcal{X}\\mathfrak{X}}$ is $\\mathcal{D}^{\\prime}$ . For $\\epsilon\\,\\in\\,(0,1)$ and $k\\,\\in\\,\\mathbb{N}$ , suppose that for any $f\\,\\in\\,{\\mathcal{F}}$ there is some polynomial $p$ over $\\mathcal{X}$ of degree at most $k$ such that $\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}^{\\prime}}[|f(\\mathbf{x})-p(\\mathbf{x})|]\\le\\epsilon$ . Then, there is an algorithm ( ${\\mathcal{L}}_{1}$ polynomial regression) that, upon receiving a number of i.i.d. samples from $\\mathcal{D}_{\\mathcal{X}\\mathcal{X}}$ , outputs with probability at least $1-\\delta$ some hypothesis $h:\\mathcal{X}\\rightarrow\\{\\pm1\\}$ with $\\mathbb{P}_{(\\mathbf{x},y)\\sim\\mathcal{D}_{x y}}[y\\neq h(\\mathbf{x})]\\,\\le\\,\\mathsf{o p t}+O(\\epsilon),$ , where $\\mathsf{o p t}=\\operatorname*{min}_{f\\in\\mathcal{F}}\\operatorname{err}(f;\\mathcal{D}_{\\mathcal{X}\\mathcal{Y}})$ . The algorithm uses p $\\operatorname{oly}(d^{k},{\\frac{1}{\\epsilon}})\\log(1/\\delta)$ time and samples. ", "page_idx": 21}, {"type": "text", "text": "The tester of Theorem D.1 does the following for some sufficiently large universal constant $C\\geq1$ . ", "page_idx": 21}, {"type": "text", "text": "1. Runs the outlier removal of Theorem 3.1 with parameters $\\alpha\\gets1$ , $\\epsilon\\leftarrow\\epsilon/C,\\delta\\leftarrow\\delta/C$ to receive a selector $g$ with the guarantees specified in Theorem 3.1.   \n2. Estimates, using unlabeled samples form $\\mathcal{D}_{\\mathcal{X}\\mathcal{Y}}$ , the value of $\\mathbb{P}_{\\mathbf{x}\\sim\\mathcal{D}^{\\prime}}[g(\\mathbf{x})=0]$ and rejects if the estimated value is greater than $2\\theta+2\\epsilon/C$ .   \n3. Otherwise, the tester accepts and runs the algorithm of Proposition D.2 with fresh samples from the distribution $\\tilde{\\mathcal{D}}_{\\mathcal{X}\\mathcal{Y}}$ that corresponds to the conditioning of $\\mathcal{D}_{\\mathcal{X}\\mathcal{Y}}$ to $g(\\mathbf{x})=1$ , with parameters $\\epsilon\\leftarrow\\epsilon/C$ and $k\\leftarrow k$ . ", "page_idx": 21}, {"type": "text", "text": "Without loss of generality, we have that $2\\theta\\!+\\!\\epsilon\\leq1/2$ (otherwise, we may output a random hypothesis). This implies that the runtime does not change asymptotically by conditioning on $g(\\mathbf{x})=1$ (which can be done through rejection sampling). ", "page_idx": 22}, {"type": "text", "text": "For completeness, we observe that, by condition (b), $\\mathbb{P}_{\\mathbf{x}\\sim\\mathcal{D}}[g(\\mathbf{x})=0]\\le\\mathrm{d}_{\\mathrm{TV}}(\\mathcal{D},\\mathcal{D}^{\\prime})+\\frac{\\epsilon}{C}$ and hence $\\mathbb{P}_{\\mathbf{x}\\sim\\mathcal{D}^{\\prime}}[g(\\mathbf{x})=0]\\le2\\mathrm{d}_{\\mathrm{TV}}(\\mathcal{D},\\mathcal{D}^{\\prime})+\\frac{\\epsilon}{C}$ . By a standard Hoeffding bound, we have that the estimated value for $\\mathbb{P}_{\\mathbf{x}\\sim\\mathcal{D}^{\\prime}}[g(\\mathbf{x})=0]$ (obtained using unlabeled samples from the marginal $\\mathcal{D}^{\\prime}$ of $\\mathcal{D}_{\\mathcal{X}\\mathcal{Y}}$ ) is at most $\\begin{array}{r}{2\\mathrm{d}_{\\mathrm{TV}}(\\mathcal{D},\\dot{\\mathcal{D}^{\\prime}})+\\frac{2\\epsilon}{C}}\\end{array}$ and the tester will, with high probability accept if $\\mathrm{d}_{\\mathrm{TV}}(D,D^{\\prime})\\leq\\theta$ . ", "page_idx": 22}, {"type": "text", "text": "For soundness, we want to show that, upon acceptance, for any $f\\,\\in\\,{\\mathcal{F}}$ , there is a polynomial $p$ of degree $k$ such that $\\begin{array}{r}{\\mathbb{E}_{\\mathbf{x}\\sim\\tilde{\\mathcal{D}}}[|f(\\mathbf{x})-\\bar{p}(\\mathbf{x})|]\\;\\le\\;O(\\frac{\\epsilon}{C})}\\end{array}$ . Then, by Proposition D.2, we have that the output $h$ satisfies $\\begin{array}{r}{\\operatorname{srr}(h;\\tilde{\\cal D}_{\\cal X\\mathcal{Y}})\\leq\\operatorname*{min}_{f\\in\\mathcal{F}}\\operatorname{err}(f;\\tilde{\\cal D}_{\\cal X\\mathcal{Y}})+{\\cal O}(\\epsilon/C)}\\end{array}$ . Moreover we would also have $\\mathrm{err}(h;{\\mathcal D}_{\\mathcal{X}\\mathcal{Y}})\\,\\leq\\,\\mathbb{P}_{{\\mathbf x}\\sim{\\mathcal D^{\\prime}}}[g({\\mathbf x})\\,=\\,0]\\,+\\,\\mathbb{P}_{{\\mathbf x}\\sim{\\mathcal D^{\\prime}}}[g({\\mathbf x})\\,=\\,1]\\mathrm{err}(h;\\bar{{\\mathcal D}}_{\\mathcal{X}\\mathcal{Y}})$ ), by the law of total probability. The second term of the sum can be bounded as $\\begin{array}{r}{\\bar{\\mathbb{P}}_{\\mathbf{x}\\sim\\mathcal{D^{\\prime}}}[g(\\mathbf{x})=1]\\mathrm{err}(h;\\tilde{\\mathcal{D}}_{\\mathcal{X}\\mathcal{Y}})\\le}\\end{array}$ $\\begin{array}{r}{\\operatorname*{min}_{f\\in\\mathcal{F}}\\mathbb{P}_{\\mathbf{x}\\sim\\mathcal{D}^{\\prime}}[g(\\mathbf{x})=1]\\mathbb{P}_{(\\mathbf{x},y)\\sim\\mathcal{D}_{\\mathcal{X}^{\\mathcal{Y}}}}[y\\neq h(\\mathbf{x})|g(\\mathbf{x})=1]+O(\\epsilon/C)\\stackrel{\\cdot}{=}\\mathrm{opt}+\\dot{O}(\\epsilon/C)}\\end{array}$ . Overall, the bound on $\\mathrm{err}(h;\\mathcal{D}_{\\mathcal{X}}\\boldsymbol{y})$ would then be o $\\mathfrak{t}+2\\theta+\\epsilon$ , because after acceptance, $\\mathbb{P}_{\\mathbf{x}\\sim\\mathcal{D}^{\\prime}}[g(\\mathbf{x})=0]$ is bounded, with high probability, by $2\\theta+O(\\epsilon/C)$ . ", "page_idx": 22}, {"type": "text", "text": "It remains to show the polynomial approximation bound. Let $f$ be some element of $\\mathcal{F}$ and $p_{\\mathrm{up}},p_{\\mathrm{low}}$ the corresponding $\\scriptstyle{\\frac{\\epsilon^{2}}{C}}-{\\mathcal{L}}_{2}$ sandwiching polynomials. If $\\tilde{\\mathcal{D}}$ is the $\\mathcal{X}$ -marginal of $\\tilde{\\mathcal{D}}_{\\mathcal{X}\\mathcal{Y}}$ , we have the following, by applying the sandwiching property, Jensen\u2019s inequality and the definition of $\\tilde{\\mathcal{D}}$ . ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\mathbb{E}_{\\mathbf{x}\\sim\\tilde{\\mathcal{D}}}[|f(\\mathbf{x})-p_{\\mathrm{low}}(\\mathbf{x})|])^{2}\\le(\\mathbb{E}_{\\mathbf{x}\\sim\\tilde{\\mathcal{D}}}[p_{\\mathrm{up}}(\\mathbf{x})-p_{\\mathrm{low}}(\\mathbf{x})])^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\le\\mathbb{E}_{\\mathbf{x}\\sim\\tilde{\\mathcal{D}}}[(p_{\\mathrm{up}}(\\mathbf{x})-p_{\\mathrm{low}}(\\mathbf{x}))^{2}]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\le\\frac{\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D^{\\prime}}}[(p_{\\mathrm{up}}(\\mathbf{x})-p_{\\mathrm{low}}(\\mathbf{x}))^{2}g(\\mathbf{x})]}{\\mathbb{P}_{\\mathbf{x}\\sim\\mathcal{D^{\\prime}}}[g(\\mathbf{x})=1]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By applying condition (a), as well as the fact that $\\mathbb{P}_{\\mathbf{x}\\sim\\mathcal{D}^{\\prime}}[g(\\mathbf{x})=1]\\ge\\Omega(1)$ (since $\\mathbb{P}_{\\mathbf{x}\\sim\\mathcal{D}^{\\prime}}[g(\\mathbf{x})=0]$ ), we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(\\mathbb{E}_{\\mathbf{x}\\sim\\tilde{\\mathcal{D}}}[|f(\\mathbf{x})-p_{\\mathrm{low}}(\\mathbf{x})|])^{2}\\le O(1)\\cdot\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}[(p_{\\mathrm{up}}(\\mathbf{x})-p_{\\mathrm{low}}(\\mathbf{x}))^{2}]\\le O(\\epsilon^{2}/C)}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, indeed, $\\mathbb{E}_{\\mathbf{x}\\sim\\tilde{D}}[|f(\\mathbf{x})-p_{\\mathrm{low}}(\\mathbf{x})|]\\le\\epsilon$ , which concludes the proof of Theorem D.1. ", "page_idx": 22}, {"type": "text", "text": "D.2 Robust Learning ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We also provide the following result for learning with nasty noise (see Definition A.17). While there are algorithms for learning in the nasty noise model that are more efficient than the one we analyze here (see [DKS18]), we achieve an error bound that is close to the optimal: we only incur a multiplicative factor of 2 from the information theoretically optimal bound of $2\\eta$ , where $\\eta$ is the noise rate (see Definition A.17). For intersections of halfspaces, for instance, to the best of our knowledge, all prior known dimension-efficient algorithms incurred an error of $\\Omega({\\sqrt{\\eta}})$ for learning under nasty noise of rate $\\eta$ (see [KKM18, DKS18]). ", "page_idx": 22}, {"type": "text", "text": "Theorem D.3 (Learning with Nasty Noise via Sandwiching). For \u03f5, \u03b7 $\\r,\\delta\\in(0,1)$ , let $\\mathcal{X}\\subseteq\\mathbb{R}^{d}$ and $(\\mathcal{D},\\mathcal{F})$ be an $\\left(\\frac{\\epsilon^{2}}{C},\\frac{\\delta}{C},k,m\\right)$ -reasonable pair (Definition 4.5) for some sufficiently large universal constant $C>0$ . Then, there is a robust learner for $\\mathcal{F}$ under nasty noise of rate $\\eta$ with respect to $\\mathcal{D}$ up to error $4\\eta+\\epsilon$ and probability of failure $\\delta$ with sample complexity $\\begin{array}{r}{m+\\mathrm{poly}(\\frac{1}{\\epsilon}(k d)^{k}\\log(1/\\delta))}\\end{array}$ and time complexity poly $\\textstyle({\\frac{m}{\\epsilon}}(k d)^{k}\\log(1/\\delta))$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. We follow a very similar approach as the one for Theorem D.1. The main differences are two. First, instead of the outlier removal of Theorem 3.1, we apply the outlier removal of Theorem E.1, which works in the adversarial setting. Second, in the nasty noise setting, we assume that the noise rate is bounded and hence we do not need to run any tests in order to obtain the desired guarantees. ", "page_idx": 22}, {"type": "text", "text": "Recall that in this setting, the learner receives a sample $S$ of size $N$ with $S=S_{\\mathrm{iid}}\\cup S_{\\mathrm{adv}}\\backslash S_{\\mathrm{rem}}$ , where $S_{\\mathrm{iid}}$ is an i.i.d. labeled sample drawn from $\\mathcal{D}$ and labeled by some $f^{*}\\in\\mathcal{F}$ , $|S_{\\mathrm{adv}}|=|S_{\\mathrm{rem}}|\\le\\eta N$ , where $S_{\\mathrm{rem}}$ is an arbitrary subset of $S_{\\mathrm{iid}}$ and $S_{\\mathrm{adv}}$ is an arbitrary sample of size $S_{\\mathrm{rem}}$ (i.e., the adversary removes the samples in $S_{\\mathrm{rem}}$ and substitutes them with adversarial samples $S_{\\mathrm{adv}}$ ). ", "page_idx": 22}, {"type": "text", "text": "The algorithm runs the outlier removal of Theorem E.1 on $S$ with parameters $\\alpha\\gets1$ , $\\epsilon\\leftarrow\\epsilon/C$ , $\\delta\\leftarrow\\delta{\\bar{/}}C$ and $k\\leftarrow k$ to receive a flitered set of samples $S_{\\mathrm{{filt}}}$ such that $|\\bar{S}_{\\mathrm{iid}}\\backslash S_{\\mathrm{filt}}|\\leq|S_{\\mathrm{adv}}|\\!+\\!\\epsilon N/\\dot{C}\\leq$ $\\eta N+\\epsilon N/C$ and also $\\begin{array}{r}{\\frac{1}{N}\\sum_{\\mathbf{x}\\in S_{\\mathrm{filt}}}(p(\\mathbf{x}))^{2}\\,\\le\\,200\\,\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}[(p(\\mathbf{x}))^{2}]}\\end{array}$ for any polynomial $p$ of degree at most $k$ . Then, it runs polynomial regression of degree $k$ with coefficient bound $B$ (given by Definition 4.5) over the set $S_{\\mathrm{{filt}}}$ and outputs $h(\\mathbf{x})\\ =\\ \\mathrm{sign}(\\widehat{p}(\\mathbf{x}))$ where $\\widehat{p}$ is the output of the polynomial regression routine (of Proposition D.2). ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "We aim to bound $\\mathbb{P}_{\\mathbf{x}\\sim\\mathcal{D}}[f^{*}(\\mathbf{x})\\neq h(\\mathbf{x})]$ . By uniform convergence, we have a bound of the form P $\\\"\\mathbf{x}\\sim\\!D[f^{*}(\\mathbf{x})\\,\\neq\\,h(\\mathbf{x})]\\,\\doteq\\,\\mathbb{P}_{(\\mathbf{x},y)\\sim S_{\\mathrm{iid}}}[y\\,\\neq\\,h(\\mathbf{x})]+\\epsilon/C\\,\\le\\,\\mathbb{P}_{(\\mathbf{x},y)\\sim S}[y\\,\\neq\\,h(\\mathbf{x})]+\\eta+\\epsilon/C$ . We further bound the quantity $\\begin{array}{r}{\\mathbb{P}_{(\\mathbf{x},y)\\sim S}[y\\neq h(\\mathbf{x})]\\le\\mathbb{P}_{(\\mathbf{x},y)\\sim S}[y\\neq h(\\mathbf{x}),(\\mathbf{x},y)\\in S_{\\mathrm{filt}}]+\\frac{|S\\setminus S_{\\mathrm{filt}}|}{N}}\\end{array}$ \u2264 $\\mathbb{P}_{(\\mathbf{x},y)\\sim S}[y\\neq h(\\mathbf{x}),(\\mathbf{x},y)\\in S_{\\mathrm{filt}}]+2\\eta$ . ", "page_idx": 23}, {"type": "text", "text": "We may apply Proposition D.2 to show that $\\mathbb{P}_{(\\mathbf{x},y)\\sim S}[y\\neq h(\\mathbf{x}),(\\mathbf{x},y)\\in S_{\\mathrm{filt}}]\\le\\eta+O(\\epsilon/C)$ , as long as the following is true for some polynomial of degree at most $k$ . ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{1}{N}\\sum_{(\\mathbf{x},y)\\in S_{\\mathrm{filt}}}|f^{*}(\\mathbf{x})-p(\\mathbf{x})|\\leq O(\\epsilon/C)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Due to the sandwiching property, this is true for the sandwiching polynomial $p_{\\mathrm{low}}$ for $f^{*}$ (which exists since $f^{*}\\in\\mathcal{F}-$ see Definition 4.5). To show this, we may follow an analogous approach as the one for Theorem D.1. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "E Outlier Removal Procedure ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We now give the proofs of our outlier removal theorem in the adversarial, as well as the PQ setting. ", "page_idx": 23}, {"type": "text", "text": "E.1 Outlier Removal in the Adversarial Setting ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We present our outlier removal result in the adversarial setting: ", "page_idx": 23}, {"type": "text", "text": "Theorem E.1. There exists an algorithm that satisfies the following specifications for some sufficiently large absolute constant $C$ . The algorithm $\\boldsymbol{\\mathcal{A}}$ is given parameters $\\epsilon,\\alpha,\\delta$ in $(0,1]$ , a positive integer $k$ , and a pair of size- $\\mathcal{N}$ sets $S_{D}$ and $S_{D^{\\prime}}$ of points in $\\mathbb{R}^{d}$ , where $\\begin{array}{r}{N\\ge C\\left(\\frac{(k d)^{k}}{\\epsilon}\\log\\frac{1}{\\delta}\\right)^{C}}\\end{array}$ . The algorithm $\\boldsymbol{\\mathcal{A}}$ then accepts a subset $S_{a c c e p t}\\subseteq S_{\\mathcal{D^{\\prime}}}$ , rejects a subset $S_{r e j e c t}\\,=\\,\\dot{S}_{\\mathcal{D^{\\prime}}}\\,\\backslash\\,\\,S_{a c c e p t}$ and runs in time $\\mathrm{poly}(N)$ . ", "page_idx": 23}, {"type": "text", "text": "Let the set $S_{D}$ in $\\mathbb{R}^{d}$ of size $N$ be sampled i.i.d. from a $k$ -tame probability distribution $\\mathcal{D}$ , and let $S_{D^{\\prime}}$ be generated by: ", "page_idx": 23}, {"type": "text", "text": "1. Sampling a size- $N$ i.i.d. set $S_{c l e a n}$ from $\\mathcal{D}$ . ", "page_idx": 23}, {"type": "text", "text": "2. Adversary corrupting an arbitrary subset of elements in $S_{c l e a n}$ . Formally, $S_{\\mathcal{D^{\\prime}}}=S_{u n c o r r u p t e d}\\cup$ Sadversarial, where $S_{\\iota}$ uncorrupted is an adversarially chosen subset of $S_{c l e a n}$ and Sadversarial is $a$ set of adversarially chosen points in $\\mathbb{R}^{d}$ of size $N-|S_{u n c o r r u p t e d}|.$ . ", "page_idx": 23}, {"type": "text", "text": "Then, with probability at least $1-\\delta$ , the algorithm $\\boldsymbol{\\mathcal{A}}$ given the sets $S_{D}$ and $S_{D^{\\prime}}$ will accept a set $S_{a c c e p t}\\subseteq S_{\\mathcal{D^{\\prime}}}$ satisfying the following two properties: ", "page_idx": 23}, {"type": "text", "text": "\u2022 Degree- $k$ spectral $\\frac{200}{\\alpha}$ -boundedness: For every polynomial $p$ of degree at most $k$ satisfying ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\underset{\\mathbf{x}\\sim\\mathcal{D}}{\\mathbb{E}}[(p(\\mathbf{x}))^{2}]\\leq1,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "it is the case that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{1}{N}\\sum_{\\mathbf{x}\\in S_{a c c e p t}}p(\\mathbf{x})^{2}\\leq\\frac{200}{\\alpha}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "\u2022 $(\\alpha,\\epsilon/2)$ -validity: The set $S_{r e j e c t}\\cap S_{u n c o r r u p t e d}$ has a size of at most $\\begin{array}{r}{\\alpha|S_{a d v e r s a r i a l}|+\\frac{\\epsilon}{2}N}\\end{array}$ . ", "page_idx": 23}, {"type": "text", "text": "We describe our algorithm for Theorem E.1 (restating Algorithm 1): ", "page_idx": 23}, {"type": "text", "text": "1. Input Sets $S_{D^{\\prime}}$ and $S_{D}$ of size $N$ in $\\mathbb{R}^{d}$ , parameters $\\epsilon,\\delta$ in $(0,1)$ .   \n2. $\\widehat{M}\\gets$ ESTIMATE-MOMENTS $(S_{D},k,\\delta/10)$ . (See Lemma B.1 for further info). ", "page_idx": 23}, {"type": "equation", "text": "$B_{0}\\gets\\frac{4d^{3k}}{\\epsilon}$ $\\begin{array}{r}{\\Delta_{0}\\leftarrow200\\sqrt{d^{2k}\\frac{\\log N}{N}\\log\\frac{1}{\\delta}}}\\end{array}$ $$\n\\begin{array}{r l}&{B_{0}\\gets\\frac{\\infty}{\\epsilon}\\ \\mathrm{and}\\,\\Delta_{0}\\gets200\\sqrt{\\,d^{\\alpha}\\frac{\\mathrm{s}_{0}}{N}\\frac{\\mathrm{s}_{0}}{N}}\\mathrm{~log}\\,\\frac{\\dot{s}}{\\delta}}\\\\ &{S_{\\mathrm{filtered}}^{0}\\gets S_{\\mathcal{D}^{\\prime}}\\setminus\\Big\\{\\mathbf{x}:\\operatorname*{max}_{p:\\ p^{\\top}\\widehat{M}p\\leq1}(p(\\mathbf{x}))^{2}>B_{0}\\Big\\}.}\\\\ &{\\mathrm{While~}\\operatorname*{max}_{p:\\ p^{\\top}\\widehat{M}p\\leq1}\\Big(\\frac{1}{N}\\sum_{\\mathbf{x}\\in S_{\\mathrm{filtered}}^{i}}(p(\\mathbf{x}))^{2}\\Big)>\\frac{50}{\\alpha}\\,(1+\\Delta_{0}\\cdot B_{0}).}\\\\ &{\\mathrm{~(a)~}\\,p_{i}\\gets\\arg\\operatorname*{max}_{p:\\ p^{\\top}\\widehat{M}p\\leq1}\\Big(\\frac{1}{N}\\sum_{\\mathbf{x}\\in S_{\\mathrm{filtered}}^{i}}(p(\\mathbf{x}))^{2}\\Big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\frac{1}{N}\\Big|\\{\\mathbf{x}\\in S_{\\mathrm{filtered}}^{i}:\\ (p_{i}(\\mathbf{x}))^{2}>\\tau\\}\\Big|\\geq\\frac{10}{\\alpha}\\left(\\underset{\\mathbf{x}\\sim S_{D}}{\\mathbb{P}}[B_{0}\\geq(p_{i}(\\mathbf{x}))^{2}>\\tau]+\\Delta_{0}\\right)}\\\\ &{S_{\\mathrm{filtered}}^{i+1}\\leftarrow S_{\\mathrm{filtered}}^{i}\\setminus\\{\\mathbf{x}:(p_{i}(\\mathbf{x}))^{2}>\\tau_{i}\\}}\\\\ &{i\\leftarrow i+1}\\\\ &{\\mathrm{ut}\\,\\big(S_{\\mathrm{accept}},S_{\\mathrm{reject}}\\big)=(S_{\\mathrm{filtered}}^{i},S_{D^{\\prime}}\\setminus S_{\\mathrm{filtered}}^{i}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Note that the procedure ESTIMATE-MOMENTS produces a good spectral approximation for the degree- $k$ moment matrix of $\\mathcal{D}$ . Formally, Lemma B.1 says that the matrixM is symmetric positivesemidefinite and with probability at least $1-\\delta/10$ it is the case that every degree- $k$ polynomial $p$ satisfies. ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{9}{10}\\underset{\\mathbf{x}\\sim\\mathcal{D}}{\\mathbb{E}}[(p(\\mathbf{x}))^{2}]\\leq p^{\\top}\\widehat{M}p\\leq\\frac{11}{10}\\underset{\\mathbf{x}\\sim\\mathcal{D}}{\\mathbb{E}}[(p(\\mathbf{x}))^{2}].\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "E.1.1 Efficient implementation ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We now explain how to execute certain steps of our algorithm in polynomial time: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The quantity $\\operatorname*{max}_{p}$ : $_{p}\\top\\widehat{M}_{p\\leq1}(p(\\mathbf{x}))^{2}$ equals to the largest eigenvalue of the matrix ", "page_idx": 24}, {"type": "equation", "text": "$$\n(\\widehat{M})^{-1/2}\\left((\\mathbf{x}^{\\otimes d})(\\mathbf{x}^{\\otimes d})^{\\top}\\right)(\\widehat{M})^{-1/2},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which can be computed in polynomial time in the dimension $m$ of the matrix. (Note that if the matrix $\\widehat{M}$ is not full-rank, then the above is still true if long as $(\\widehat{M})^{-1/2}$ is replaced by the Moore\u2013Penrose pseudo-inverse of $(\\widehat{M})^{1/2}$ , which again can be computed efficiently. Also note that we used the fact that the matrixM is symmetric.) ", "page_idx": 24}, {"type": "text", "text": "\u2022 The quantity $\\operatorname*{max}_{p}$ : $\\begin{array}{r}{p^{\\top}\\widehat{M}p{\\leq}1\\left(\\frac{1}{N}\\sum_{\\mathbf{x}\\in S_{\\mathrm{filtered}}^{i}}(p(\\mathbf{x}))^{2}\\right)}\\end{array}$ equals to the largest eigenvalue of the matrix ", "page_idx": 24}, {"type": "equation", "text": "$$\n(\\widehat{M})^{-1/2}\\left(\\frac{1}{N}\\sum_{\\mathbf{x}\\in S_{\\mathrm{filtered}}^{i}}(\\mathbf{x}^{\\otimes d})(\\mathbf{x}^{\\otimes d})^{\\top}\\right)(\\widehat{M})^{-1/2},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which can be computed in polynomial time in the dimension $m$ of the matrix. (Again, if the matrix $\\widehat{M}$ is not full-rank, then the above is still true if long as $(\\widehat{M})^{-1/2}$ is replaced by the Moore\u2013Penrose pseudo-inverse of $(\\widehat{M})^{1/2}$ . Also note that we again used the fact that the matrixM is symmetric.)   \n\u2022 The polynomial $p_{i}\\leftarrow\\arg\\operatorname*{max}_{p}$ : $\\begin{array}{r}{p^{\\top}\\widehat{M}p{\\leq}1\\left(\\frac{1}{N}\\sum_{\\mathbf{x}\\in S_{\\mathrm{filtered}}^{i}}(p(\\mathbf{x}))^{2}\\right)}\\end{array}$ can be computed by taking the leading eigenvector of 2  N1 x\u2208Sfiiltered(x\u2297d)(x\u2297d)\u22a4 (M )\u22121/2 and multiplying this vector by $(\\widehat{M})^{-1/2}$ (again, one takes the Moore\u2013Penrose pseudo-inverse if $\\widehat{M}$ is not full-rank).   \n\u2022 The value of $\\tau_{i}$ can be computed in time $\\mathrm{poly}(N)$ by considering all the candidate values $\\tau$ of the form $(p_{i}(\\mathbf{x}))^{2}$ for all elements $\\mathbf{x}$ in $S_{\\mathrm{filtered}}^{i}$ , and setting $\\tau_{i}$ to be the smallest candidate that satisfies the condition in the algorithm. ", "page_idx": 24}, {"type": "text", "text": "Note that to prove the run-time bound of $\\mathrm{poly}(N)$ for the algorithm as a whole we will need to bound the total number of iterations in the main while loop, which is done in Section E.1.3. ", "page_idx": 24}, {"type": "text", "text": "E.1.2 Correctness analysis ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We now proceed to proving first the correctness of the algorithm in Section E.1.2. Then, in Section E.1.3 we show the required run-time bound. ", "page_idx": 25}, {"type": "text", "text": "In this section we show that with probability at least $1-\\delta$ the algorithm $\\boldsymbol{\\mathcal{A}}$ satisfies the two correctness guarantees in Theorem E.1. We begin by arguing that with probability at least $1-\\delta$ the sets $S_{D}$ and $S_{\\mathrm{clean}}$ are well-behaved. ", "page_idx": 25}, {"type": "text", "text": "Claim 1. Let the set $S$ be formed of $N$ i.i.d. samples from a $k$ -tame distribution $\\mathcal{D}$ , where $N\\geq$ $\\begin{array}{r}{C\\left(\\frac{(k d)^{k}}{\\epsilon}\\log\\frac{1}{\\delta}\\right)^{C}}\\end{array}$ and $C$ is a sufficiently large absolute constant. Also, let $\\begin{array}{r}{B_{0}=\\frac{4d^{3k}}{\\epsilon}}\\end{array}$ 4d\u03f5 . Then with probability at least $1-\\delta/10$ the set $S$ satisfies the following properties for any polynomial $p$ over $\\mathbb{R}^{d}$ of degree at most $k$ and any pair of values of $\\tau_{1},\\tau_{2}$ in $\\mathbb{R}$ : ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left|\\frac{|\\{\\mathbf{x}\\in S:\\;\\tau_{2}\\geq(p(\\mathbf{x}))^{2}>\\tau_{1}\\}|}{N}-\\mathbb{P}_{\\mathbf{x}\\sim\\mathcal{D}}[\\tau_{2}\\geq(p(\\mathbf{x}))^{2}>\\tau_{1}]\\right|\\leq100\\sqrt{\\frac{d^{2k}\\log N}{N}\\log\\frac{1}{\\delta}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{1}{N}\\left|\\left\\{\\mathbf{x}\\in S:\\operatorname*{max}_{\\substack{p^{\\prime}\\,o f\\,d e g r e e\\;k:\\;\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}[(p^{\\prime}(\\mathbf{x}))^{2}]\\leq1}}(p^{\\prime}(\\mathbf{x}))^{2}>\\frac{2d^{3k}}{\\epsilon}\\right\\}\\right|\\leq\\frac{3\\epsilon}{4},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and if the polynomial $p$ further satisfies $\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}[(p(\\mathbf{x}))^{2}]\\le1$ then also ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\underset{\\mathbf{x}\\sim S_{\\mathcal{D}}}{\\mathbb{E}}\\left[(p(\\mathbf{x}))^{2}\\cdot\\mathbf{1}_{(p(\\mathbf{x}))^{2}\\le B_{0}}\\right]\\le\\underset{\\mathbf{x}\\sim\\mathcal{D}}{\\mathbb{E}}[(p(\\mathbf{x}))^{2}]+0.01\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. Since $(p(\\mathbf{x}))^{2}$ is a polynomial of degree at most $2k$ , the every function of the form $\\{\\mathbf{1}_{\\tau_{2}\\geq(p(\\mathbf{x}))^{2}>\\tau}\\}$ is an AND of two degree- $2k$ polynomial threshold functions. Since degree- $2k$ polynomial threshold functions have a VC dimension of at most $d^{2k}+1$ , we can use Fact A.5 and Fact A.6 to conclude that property (1) holds with probability at least $1-\\delta/30$ . ", "page_idx": 25}, {"type": "text", "text": "Now, we show that property (2) is likely to be satisfied. Then there is a collection $\\{r_{1},\\cdot\\cdot\\cdot\\,,r_{m^{\\prime}}\\}$ of degree- $k$ polynomials that satisfy ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\underset{\\mathbf{x}\\sim\\mathcal{D}}{\\mathbb{E}}[r_{j}(\\mathbf{x})r_{j^{\\prime}}(\\mathbf{x})]=\\left\\{1\\begin{array}{l l}{\\phantom{-}\\mathrm{if}\\ j=j^{\\prime}}\\\\ {\\phantom{-}\\mathrm{otherwise.}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "(Such collection necessarily exists via the Gram-Schmidt process.) We let $M$ denote the matrix $\\check{\\mathbb{E}}_{\\mathbf{x}\\sim\\mathcal{D}}(\\mathbf{x}^{\\otimes d})(\\mathbf{x}^{\\otimes d})^{\\top}$ . Additionally, we consider a basis $\\{g_{1},\\cdot\\cdot\\cdot,g_{m-m^{\\prime}}\\}$ for the nullspace of $M$ Now, for $\\mathbf{x}$ sampled from $\\mathcal{D}$ we have: ", "page_idx": 25}, {"type": "text", "text": "\u2022 For a specific index $j$ , we have $\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}[(r_{j}(\\mathbf{x})^{2}]=1$ and therefore by Markov\u2019s inequality we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\underset{\\mathbf{x}\\sim\\mathcal{D}}{\\mathbb{P}}\\left[(r_{j}(\\mathbf{x}))^{2}\\leq\\frac{2d^{k}}{\\epsilon}\\right]\\geq1-\\frac{\\epsilon}{2d^{k}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "\u2022 Each $g_{j}$ has $\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}[(g_{j}(\\mathbf{x})^{2}]=0$ , and therefore $\\mathbb{P}_{\\mathbf{x}\\sim\\mathcal{D}}[g_{j}(\\mathbf{x})=0]=1$ . ", "page_idx": 25}, {"type": "text", "text": "By union bound, all the events above take place for $\\mathbf{x}$ sampled from $\\mathcal{D}$ with probability at least $1-m{\\frac{\\epsilon}{2d^{k}}}\\geq1-{\\frac{\\epsilon}{2}}$ . Via the standard Hoeffding bound, with probability at least $1-\\delta$ the events above take place for at least $\\textstyle{\\frac{\\epsilon}{2}}+{\\sqrt{{\\frac{20}{N}}\\log{\\frac{20}{\\delta}}}}$ fraction of elements $\\mathbf{x}$ in $S$ . Since $\\{r_{j}\\}$ are orthonormal with respect to $M$ , every degree- $k$ polynomial $p^{\\prime}$ satisfying $\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}[(p^{\\prime}(\\mathbf{x}))^{2}]\\leq1$ can be decomposed as $\\begin{array}{r}{p^{\\prime}=\\sum_{i=0}^{m^{\\prime}}\\alpha_{i}r_{i}+\\sum_{i=0}^{m-m^{\\prime}}\\beta_{i}g_{i}}\\end{array}$ , where each $\\alpha_{i}$ is in $[-1,1]$ . Therefore if the events above take ", "page_idx": 25}, {"type": "text", "text": "place for a point $\\mathbf{x}$ , then ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{p^{\\prime}\\circ\\,\\mathbf{\\Delta}}{\\mathrm{max}}}&{(p^{\\prime}(\\mathbf{x}))^{2}=\\underset{\\alpha_{1},\\ldots,\\alpha_{m^{\\prime}}\\in[-1,1]}{\\mathrm{max}}\\left(\\underset{i=0}{\\overset{m^{\\prime}}{\\sum}}\\underset{\\alpha_{i}r_{i}(\\mathbf{x})}{\\overbrace{\\sum_{i=0}^{m-m^{\\prime}}}}\\right)^{=0}\\leq}\\\\ &{\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}[(p^{\\prime}(\\mathbf{x}))^{2}]\\leq1}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\left(\\underset{i=0}{\\overset{m^{\\prime}}{\\sum}}\\underset{\\lceil r_{i}(\\mathbf{x})\\rceil}{\\overset{\\leq}{\\prod}}\\right)^{2}\\leq\\frac{2d^{k}/\\epsilon}{\\epsilon}\\overset{2}{\\leq}\\frac{2d^{3k}}{\\epsilon},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "from which Property (2) follows. ", "page_idx": 26}, {"type": "text", "text": "Finally, we remark that Property (3) holds with probability at least $1-\\delta/30$ due to an argument analogous to the proof of Lemma B.2 (where in place of $f$ , we have the function $\\mathbf{1}_{p^{2}(\\mathbf{x})\\leq B_{0}})$ . ", "page_idx": 26}, {"type": "text", "text": "We now argue that in each iteration $i$ there is a value of $\\tau_{i}$ satisfying the condition in step (5b) ", "page_idx": 26}, {"type": "text", "text": "Claim 2. Suppose the set $S_{D}$ is such that it satisfies property (3) of claim $^{\\,l}$ , i.e. for every degree- $k$ polynomial $p$ satisfying $\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}(p(\\mathbf{x}))^{2}\\,\\leq\\,\\mathrm{~\\r~{~1~}~}w e$ have $\\ddot{\\mathbb{E}}_{\\mathbf{x}\\sim S_{\\mathcal{D}}}\\left[(p(\\mathbf{x}))^{2}\\cdot\\mathbf{1}_{(p(\\mathbf{x}))^{2}\\le B_{0}}\\right]\\ \\le$ $\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}[(p(\\mathbf{x}))^{2}]+0.01$ and the matrix $\\widehat{M}$ satisfies Equation E.1, i.e. for every degree- $k$ polynomial $p$ we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\frac{9}{10}\\underset{\\mathbf{x}\\sim\\mathcal{D}}{\\mathbb{E}}[(p(\\mathbf{x}))^{2}]\\leq p^{\\top}\\widehat{M}p\\leq\\frac{11}{10}\\underset{\\mathbf{x}\\sim\\mathcal{D}}{\\mathbb{E}}[(p(\\mathbf{x}))^{2}].\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Suppose it is the case that $\\begin{array}{r}{\\frac{1}{N}\\sum_{\\mathbf{x}\\in S_{\\beta l t e r e d}^{i}}(p_{i}(\\mathbf{x}))^{2}>\\frac{50}{\\alpha}\\left(1+\\Delta_{0}\\cdot B_{0}\\right)}\\end{array}$ (i.e. the while loop does not terminate at step $i$ ). The there exists some $\\tau$ for which ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\frac{1}{N}\\bigg\\vert\\{\\mathbf{x}\\in S_{\\beta l t e r e d}^{i}:\\;(p_{i}(\\mathbf{x}))^{2}>\\tau\\}\\bigg\\vert\\geq\\frac{10}{\\alpha}\\left(\\underset{\\mathbf{x}\\sim S_{D}}{\\mathbb{P}}[B_{0}\\geq(p_{i}(\\mathbf{x}))^{2}>\\tau]+\\Delta_{0}\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof of Claim 2. For the sake of contradiction, suppose that for every $\\tau\\geq0$ it is the case that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\frac{1}{N}\\bigg\\vert\\{\\mathbf{x}\\in S_{\\mathrm{filered}}^{i}:\\ (p_{i}(\\mathbf{x}))^{2}>\\tau\\}\\bigg\\vert\\leq\\frac{10}{\\alpha}\\left(\\underset{\\mathbf{x}\\sim S_{D}}{\\mathbb{P}}[B_{0}\\geq(p_{i}(\\mathbf{x}))^{2}>\\tau]+\\Delta_{0}\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Since every element $\\mathbf{x}$ of $S_{\\mathrm{filtered}}^{i}$ satisfies $(p_{i}(\\mathbf{x}))^{2}\\leq B_{0}$ we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l l}{\\displaystyle\\frac{1}{N}\\sum_{\\mathbf{x}\\in S_{\\mathrm{ilterd}}^{i}}(p_{i}(\\mathbf{x}))^{2}=\\displaystyle\\int_{\\tau=0}^{\\infty}\\displaystyle\\frac{1}{N}\\Big|\\{\\mathbf{x}\\in S_{\\mathrm{filterd}}^{i}:(p_{i}(\\mathbf{x}))^{2}>\\tau\\}\\Big|\\,d\\tau=}&{}&\\\\ {\\displaystyle\\int_{\\tau=0}^{B_{0}}\\displaystyle\\frac{1}{N}\\Big|\\{\\mathbf{x}\\in S_{\\mathrm{filterd}}^{i}:(p_{i}(\\mathbf{x}))^{2}>\\tau\\}\\Big|\\,d\\tau.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "which combined with Equation E.3 implies ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{1}{N}\\sum_{\\mathbf{x}\\in S_{\\mathrm{siterd}}^{i}}(p_{i}(\\mathbf{x}))^{2}\\leq\\frac{10}{\\alpha}\\left(\\Delta_{0}B_{0}+\\int_{\\tau=0}^{\\infty}\\underset{\\mathbf{x}\\sim S_{D}}{\\mathbb{P}}[B_{0}\\geq(p_{i}(\\mathbf{x}))^{2}>\\tau]\\,d\\tau\\right)=}}\\\\ &{}&{\\frac{10}{\\alpha}\\left(\\Delta_{0}B_{0}+\\underset{\\mathbf{x}\\sim S_{D}}{\\mathbb{E}}\\left[p_{i}(\\mathbf{x})\\right)^{2}\\cdot\\mathbf{1}_{\\left(p_{i}(\\mathbf{x})\\right)^{2}\\leq B_{0}}\\right]\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Additionally, since $S_{D}$ is assumed to satisfy property (3) in Claim 1, andM is assumed to satisfy Equation E.1, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\underset{\\mathbf{x}\\sim S_{D}}{\\mathbb{E}}\\left[(p_{i}(\\mathbf{x}))^{2}\\cdot\\mathbf{1}_{(p_{i}(\\mathbf{x}))^{2}\\le B_{0}}\\right]\\le\\underset{\\mathbf{x}\\sim\\mathcal{D}}{\\mathbb{E}}[(p_{i}(\\mathbf{x}))^{2}]+0.01\\le\\frac{11}{10}(p_{i})^{\\top}\\widehat{M}(p_{i})+0.01\\le\\frac{111}{100}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Combining Equation E.5 and Equation E.6 we get ", "page_idx": 27}, {"type": "equation", "text": "$$\n{\\frac{1}{N}}\\sum_{\\mathbf{x}\\in S_{\\mathrm{filtered}}^{i}}(p_{i}(\\mathbf{x}))^{2}\\leq{\\frac{10}{\\alpha}}\\left(\\Delta_{0}B_{0}+{\\frac{111}{100}}\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "This contradicts the premise that $\\begin{array}{r}{\\frac{1}{N}\\sum_{\\mathbf{x}\\in S_{\\mathrm{filtered}}^{i}}(p_{i}(\\mathbf{x}))^{2}>\\frac{50}{\\alpha}\\left(1+\\Delta_{0}\\cdot B_{0}\\right)}\\end{array}$ , finishing the proof. ", "page_idx": 27}, {"type": "text", "text": "Now, we proceed to argue that if all the properties in Claim 1 and Equation E.1 hold then the algorithm $\\boldsymbol{\\mathcal{A}}$ will satisfy the $(\\alpha,\\epsilon/2)$ -validity property. In other words, we show that the set $S_{\\mathrm{accept}}\\cap S_{\\mathrm{unco}}$ rrupted has a size of at most $\\alpha|S_{\\mathrm{adversarial}}|+\\frac{\\epsilon}{2}N$ . The set $S_{\\mathrm{accept}}\\cap S_{\\mathrm{uncorrupted}}$ consists of two components: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The elements in $\\left\\{\\mathbf{x}\\in S_{\\mathrm{uncorrupted}}:\\operatorname*{max}_{p\\;\\mathrm{of}\\;\\deg\\mathrm{ree}\\;k}(p(\\mathbf{x}))^{2}>B_{0}\\right\\},$ , whose number is upperbounded by $2\\epsilon N/3$ for the following reason. Equation E.1 implies that whenever $p^{\\top}\\widehat{M}p$ holds, we also have $\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}[(p(\\mathbf{x}))^{2}]\\stackrel{=}{\\leq}10/9$ and since $S_{\\mathrm{uncorrupted}}$ is assumed to satisfy Claim 1w, hfiocrh  aits  lleesass tt $1-2\\epsilon/3$ fraction of elements $\\mathbf{x}$ in $S_{\\mathrm{uncorrupted}}$ we have $\\begin{array}{r}{(p(\\mathbf{x}))^{2}\\,\\leq\\,\\frac{10}{9}\\frac{2d^{3k}}{\\epsilon}}\\end{array}$ $B_{0}$   \n\u2022 The elements in i (Sfiiltered \\ $\\bigcup_{i}\\big((S_{\\mathrm{filtered}}^{i}\\setminus S_{\\mathrm{filtered}}^{i+1})\\cap S_{\\mathrm{uncorrupted}}\\big)$ , the number of which is bounded by $\\frac{2\\alpha}{5}\\big|S_{\\mathrm{adversarial}}\\big|$ by the following claim. ", "page_idx": 27}, {"type": "text", "text": "Claim 3. Suppose the sets $S_{D}$ and $S_{c l e a n}$ satisfy the properties in Claim $^{\\,I}$ . Then, for each iteration $i$ of the main loop of the algorithm, it is the case that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left|(S_{f i l t e r e d}^{i}\\setminus S_{f i l t e r e d}^{i+1})\\cap S_{u n c o r n u p t e d}\\right|\\le\\frac{2\\alpha}{5}\\left|(S_{f i l t e r e d}^{i}\\setminus S_{f i l t e r e d}^{i+1})\\cap S_{a d v e r s a r i a l}\\right|.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. Since the set $S_{\\mathrm{uncorrupted}}$ is a subset of the set $S_{\\mathrm{clean}}$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\big|\\big(S_{\\mathrm{filtered}}^{i}\\,\\big\\setminus S_{\\mathrm{filtered}}^{i+1}\\big)\\cap S_{\\mathrm{uncorrupted}}\\big|\\,\\le\\,\\big|\\big(S_{\\mathrm{filtered}}^{i}\\,\\big\\setminus S_{\\mathrm{filtered}}^{i+1}\\big)\\cap S_{\\mathrm{clean}}\\big|\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Also, based on how the algorithm chooses the set $S_{\\mathrm{filtered}}^{i+1}$ and the parameter $\\tau_{i}$ , we have: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{|S_{\\mathrm{filterd}}^{i}\\setminus S_{\\mathrm{filterd}}^{i+1}|}{N}=\\frac{1}{N}\\bigg|\\{\\mathbf{x\\inS}_{\\mathrm{filterd}}^{i}:(p_{i}(\\mathbf{x}))^{2}>\\tau_{i}\\}\\bigg|\\geq}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\frac{10}{\\alpha}\\left(\\underset{\\mathbf{x}\\sim S_{D}}{\\mathbb{P}}[B_{0}\\geq(p_{i}(\\mathbf{x}))^{2}>\\tau_{i}]+200\\sqrt{d^{2k}\\frac{\\log N}{N}\\log\\frac{1}{\\delta}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "but since $S_{D}$ and $S_{\\mathrm{clean}}$ satisfy Property 1 in Claim 1, we also have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{1}{N}\\Big\\vert\\{\\mathbf{x}\\in S_{\\mathrm{clean}}:\\ B_{0}\\geq(p_{i}(\\mathbf{x}))^{2}>\\tau_{i}\\}\\Big\\vert\\leq\\underbrace{\\mathbb{P}}_{\\mathbf{x}\\sim S_{D}}[B_{0}\\geq(p_{i}(\\mathbf{x}))^{2}>\\tau_{i}]+200\\sqrt{d^{2k}\\frac{\\log N}{N}\\log\\frac{1}{\\delta}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Combining the preceding two inequalities yields ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left|S_{\\mathrm{filtered}}^{i}\\setminus S_{\\mathrm{filtered}}^{i+1}\\right|\\geq\\frac{10}{\\alpha}\\Big|\\{\\mathbf{x}\\in S_{\\mathrm{clean}}:\\;B_{0}\\geq(p_{i}(\\mathbf{x}))^{2}>\\tau_{i}\\}\\Big|.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We argue that every x in (Sfiiltered \\ $\\big(S_{\\mathrm{filtered}}^{i}\\setminus S_{\\mathrm{filtered}}^{i+1}\\big)\\cap S_{\\mathrm{clean}}$ satisfies $B_{0}\\geq(p_{i}(\\mathbf{x}))^{2}>\\tau_{i}$ . Indeed, if $\\mathbf{x}$ belongs to $S_{\\mathrm{filtered}}^{i}$ , it also belongs to $S_{\\mathrm{\\hbarltered}}^{0}$ and therefore $(p_{i}(\\mathbf{x}))^{2}\\leq B_{0}((p_{i})^{\\top}\\widehat{M}(p_{i}))\\leq B_{0}$ . It also has to be that $(p_{i}(\\mathbf{x}))^{2}>\\tau_{i}$ because of how $S_{\\mathrm{filtered}}^{i+1}$ is defined inside the algorithm. Thus, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left|S_{\\mathrm{filtered}}^{i}\\setminus S_{\\mathrm{filtered}}^{i+1}\\right|\\geq\\frac{10}{\\alpha}\\bigg|(S_{\\mathrm{filtered}}^{i}\\setminus S_{\\mathrm{filtered}}^{i+1})\\cap S_{\\mathrm{clean}}\\bigg|.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Since Sfiiltered\\ $S_{\\mathrm{filtered}}^{i}\\backslash S_{\\mathrm{filtered}}^{i+1}$ is the disjoint union of $\\big(S_{\\mathrm{filtered}}^{i}\\backslash S_{\\mathrm{filtered}}^{i+1}\\big)\\cap S_{\\mathrm{clean}}$ and $S_{\\mathrm{filtered}}^{i}\\backslash S_{\\mathrm{filtered}}^{i+1})\\cap S_{\\mathrm{adversaria}}$ we further conclude that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\biggl|\\bigl(S_{\\mathrm{filtered}}^{i}\\setminus S_{\\mathrm{filtered}}^{i+1}\\bigr)\\cap S_{\\mathrm{adversarial}}\\biggr|\\ge\\left(\\frac{10}{\\alpha}+1\\right)\\biggl|\\bigl(S_{\\mathrm{filtered}}^{i}\\setminus S_{\\mathrm{filtered}}^{i+1}\\bigr)\\cap S_{\\mathrm{clean}}\\biggr|.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Finally, recalling that $S_{\\mathrm{uncorrupted}}$ is contained in $S_{\\mathrm{clean}}$ , we conclude the proposition. ", "page_idx": 27}, {"type": "text", "text": "Overall, the above claim concludes the proof of $(\\alpha,\\epsilon)$ -validity. Now we proceed to proving the spectral $\\frac{100}{\\alpha}$ -boundedness. ", "page_idx": 28}, {"type": "text", "text": "Claim 4. Suppose the algorithm terminates and produces a partition $(S_{a c c e p t},S_{r e j e c t})$ and the matrix $\\widehat{M}$ satisfies Equation E.1. Also, suppose that $C$ exceeds a certain absolute constant. Then, for every polynomial $p$ of degree at most $k$ satisfying ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\underset{\\mathbf{x}\\sim\\mathcal{D}}{\\mathbb{E}}[(p(\\mathbf{x}))^{2}]\\leq1,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "it is the case that ", "page_idx": 28}, {"type": "equation", "text": "$$\n{\\frac{1}{N}}\\sum_{\\mathbf{x}\\in S_{a c c e p t}}p(\\mathbf{x})^{2}\\leq{\\frac{100}{\\alpha}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. Since the matrixM satisfies Equation E.1, we have $p^{\\top}\\widehat{M}p\\leq11/10$ and since the main while loop of the algorithm ha s terminated, for the final value $i_{\\mathrm{max}}$ of $i$ it is the case that ", "page_idx": 28}, {"type": "equation", "text": "$$\n{\\frac{1}{N}}\\sum_{\\mathbf{x}\\in S_{\\mathrm{filerd}}^{i}}(p(\\mathbf{x}))^{2}\\leq{\\frac{11}{10}}\\cdot{\\frac{50}{\\alpha}}\\cdot\\left(1+200{\\sqrt{d^{2k}{\\frac{\\log N}{N}}\\log{\\frac{1}{\\delta}}}}\\cdot B_{0}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Substituting B0 = 4d\u03f53k and $\\begin{array}{r}{N\\ge C\\left(\\frac{(k d)^{k}}{\\epsilon}\\log\\frac{1}{\\delta}\\right)^{C}}\\end{array}$ , we see that the inequality above yields Equation E.8 when $C$ exceeds a large enough absolute constant. \u53e3 ", "page_idx": 28}, {"type": "text", "text": "E.1.3 Run-time analysis ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We now prove the run-time bound of $\\mathrm{poly}(N)$ . Since each step of the algorithm takes time $\\mathrm{poly}(N d^{k}/\\epsilon)\\,=\\,\\mathrm{poly}(N)$ (see Section E.1.1), in order to obtain a required run-time bound, it is enough to show that the number of iterations of the main while loop is at most $N$ . We argue this via the following claim: ", "page_idx": 28}, {"type": "text", "text": "Claim 5. Suppose the sets $S_{D}$ and $S_{c l e a n}$ satisfy the properties in Claim $^{\\,l}$ and the matrix $\\widehat{M}$ satisfies Equation E.1. Then, for every $i\\mathrm{~\\ensuremath~{~<~}~}i_{\\mathrm{max}}$ , we have $\\begin{array}{r l}{\\frac{1}{N}\\sum_{\\mathbf{x}\\in S_{\\hbar l l t e r e d}^{i+1}}(p_{i}(\\mathbf{x}))^{2}}&{\\le}\\end{array}$ $\\begin{array}{r}{\\frac{50}{\\alpha}\\left(1+200\\sqrt{d^{2k}\\frac{\\log N}{N}\\log\\frac{1}{\\delta}}\\cdot\\frac{4d^{3k}}{\\epsilon}\\right)}\\end{array}$ ", "page_idx": 28}, {"type": "text", "text": "Indeed, since in $i$ -th loop of the algorithm we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n{\\frac{1}{N}}\\sum_{\\mathbf{x}\\in S_{\\mathrm{filered}}^{i}}(p_{i}(\\mathbf{x}))^{2}>{\\frac{50}{\\alpha}}\\left(1+200{\\sqrt{d^{2k}{\\frac{\\log N}{N}}\\log{\\frac{1}{\\delta}}}}\\cdot{\\frac{4d^{3k}}{\\epsilon}}\\right),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "the claim above implies that necessarily $S_{\\mathrm{filtered}}^{i+1}\\neq S_{\\mathrm{filtered}}^{i}$ , which means that $|S_{\\mathrm{flltered}}^{i+1}|\\le|S_{\\mathrm{flltered}}^{i}|-1$ Therefore the total number of iterations $i_{\\mathrm{max}}$ is upper-bounded by . Now, we prove Claim 5. ", "page_idx": 28}, {"type": "text", "text": "Proof of Claim 5. Since every element $\\mathbf{x}$ of $S_{\\mathrm{filtered}}^{i+1}$ satisfies $(p_{i}(\\mathbf{x}))^{2}\\,\\leq\\,\\tau_{i}$ and the set $S_{\\mathrm{filtered}}^{i+1}$ is a subset of $S_{\\mathrm{filtered}}^{i}$ we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{1}{N}\\sum_{\\mathbf{x}\\in S_{\\mathrm{ihterd}}^{i+1}}(p_{i}(\\mathbf{x}))^{2}=\\int_{\\tau=0}^{\\infty}\\frac{1}{N}\\Big|\\{\\mathbf{x}\\in S_{\\mathrm{filterd}}^{i+1}:(p_{i}(\\mathbf{x}))^{2}>\\tau\\}\\Big|\\,d\\tau=}\\\\ {\\displaystyle\\int_{\\tau=0}^{\\tau_{i}}\\frac{1}{N}\\Big|\\{\\mathbf{x}\\in S_{\\mathrm{filterd}}^{i+1}:(p_{i}(\\mathbf{x}))^{2}>\\tau\\}\\Big|\\,d\\tau\\leq\\int_{\\tau=0}^{\\tau_{i}}\\frac{1}{N}\\Big|\\{\\mathbf{x}\\in S_{\\mathrm{filterd}}^{i}:(p_{i}(\\mathbf{x}))^{2}>\\tau\\}\\Big|\\,d\\tau.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Now, recall that $\\tau_{i}$ is the smallest value of $\\tau$ subject to: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{1}{N}\\bigg\\vert\\{\\mathbf{x}\\in S_{\\mathrm{filered}}^{i}:(p_{i}(\\mathbf{x}))^{2}>\\tau\\}\\bigg\\vert\\geq\\frac{10}{\\alpha}\\left(\\underset{\\mathbf{x}\\sim S_{D}}{\\mathbb{P}}[B_{0}\\geq(p_{i}(\\mathbf{x}))^{2}>\\tau]+200\\sqrt{d^{2k}\\frac{\\log N}{N}\\log\\frac{1}{\\delta}}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Therefore, for all values of $\\tau$ smaller than $\\tau_{i}$ we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{1}{N}\\bigg\\vert\\{\\mathbf{x}\\in S_{\\mathrm{filered}}^{i}:\\mathbf{\\beta}(p_{i}(\\mathbf{x}))^{2}>\\tau\\}\\bigg\\vert<\\frac{10}{\\alpha}\\left(\\underset{\\mathbf{x}\\sim S_{D}}{\\mathbb{P}}[B_{0}\\geq(p_{i}(\\mathbf{x}))^{2}>\\tau]+200\\sqrt{d^{2k}\\frac{\\log N}{N}\\log\\frac{1}{\\delta}}\\right),\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "which combined with Equation E.9 implies ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{1}{N}\\sum_{\\mathbf{x}\\in S_{\\mathrm{titerel}}^{i+1}}(p_{i}(\\mathbf{x}))^{2}\\leq}}\\\\ &{}&{\\frac{10}{\\alpha}\\left(\\left(200\\sqrt{d^{2k}\\frac{\\log N}{N}\\log\\frac{1}{\\delta}}\\right)\\tau_{i}+\\int_{\\tau=0}^{\\infty}\\underset{\\mathbf{x}\\sim S_{D}}{\\mathbb{P}}[B_{0}\\geq(p_{i}(\\mathbf{x}))^{2}>\\tau]\\,d\\tau\\right)=}\\\\ &{}&{\\frac{10}{\\alpha}\\left(\\left(200\\sqrt{d^{2k}\\frac{\\log N}{N}\\log\\frac{1}{\\delta}}\\right)\\tau_{i}+\\underset{\\mathbf{x}\\sim S_{D}}{\\mathbb{E}}\\left[p_{i}(\\mathbf{x})\\right)^{2}\\cdot1_{\\mathbf{x}\\leq B_{0}}\\right]\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Additionally, since $S_{D}$ is assumed to satisfy property (3) in Claim 1, andM  is assumed to satisfy Equation E.1, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\underset{\\mathbf{x}\\sim S_{D}}{\\mathbb{E}}\\left[(p_{i}(\\mathbf{x}))^{2}\\cdot\\mathbf{1}_{(p(\\mathbf{x}))^{2}\\le B_{0}}\\right]\\le\\underset{\\mathbf{x}\\sim\\mathcal{D}}{\\mathbb{E}}[(p_{i}(\\mathbf{x}))^{2}]+0.01\\le\\frac{11}{10}(p_{i})^{\\top}\\widehat{M}(p_{i})+0.01\\le\\frac{111}{100}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Combining Equation E.10 and Equation E.11 we get ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{1}{N}\\sum_{\\mathbf{x}\\in S_{\\mathrm{titerd}}^{i+1}}(p_{i}(\\mathbf{x}))^{2}\\leq\\frac{10}{\\alpha}\\left(\\left(200\\sqrt{d^{2k}\\frac{\\log N}{N}\\log\\frac{1}{\\delta}}\\right)\\tau_{i}+\\frac{111}{100}\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Recall that $S_{\\mathrm{filtered}}^{i+1}$ is a subset of $S_{\\mathrm{\\hbarltered}}^{0}$ and therefore for every element $\\mathbf{x}$ of $S_{\\mathrm{filtered}}^{i+1}$ it is the case that $(p_{i}(\\mathbf{x}))^{2}\\,\\leq\\,B\\cdot(p_{i})^{\\top}{\\widehat M}(p_{i})\\,\\leq\\,B$ , which combinded with the definition of $\\tau_{i}$ implies that $\\begin{array}{r}{\\tau_{i}\\le B=\\frac{4d^{3k}}\\epsilon}\\end{array}$ . Substituting this above allows us to conclude the claim. \u53e3 ", "page_idx": 29}, {"type": "text", "text": "E.2 Outlier Removal in the PQ setting ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We restate Theorem 3.1: ", "page_idx": 29}, {"type": "text", "text": "Theorem E.2. There exists an algorithm that, given sample access to an arbitrary distribution $\\mathcal{D}^{\\prime}$ over $\\mathbb{R}^{d}$ , sample access to a $k$ -tame probability distribution $\\mathcal{D}$ over $\\mathbb{R}^{d}$ , parameters $\\epsilon,\\alpha,\\delta$ in $(0,1)$ , and a positive integer $k$ , runs in time poly $\\left(\\frac{(k d)^{k}}{\\epsilon}\\log\\frac{1}{\\delta}\\right)$ and outputs a succinct poly $\\left(\\frac{(k d)^{k}}{\\epsilon}\\log\\frac{1}{\\delta}\\right)$ - time-computable description of a function $g:\\mathbb{R}^{d}\\rightarrow\\{0,1\\}$ that satisfies the following properties with probability at least $1-\\delta$ : ", "page_idx": 29}, {"type": "text", "text": "\u2022 Degree- $k$ spectral $\\frac{200}{\\alpha}$ -boundedness: For every polynomial $p$ of degree at most $k$ it is the case that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\underset{\\mathbf{x}\\sim\\mathcal{D}^{\\prime}}{\\mathbb{E}}\\left[(p(\\mathbf{x}))^{2}g(\\mathbf{x})\\right]\\le\\frac{200}{\\alpha}\\underset{\\mathbf{x}\\sim\\mathcal{D}}{\\mathbb{E}}[(p(\\mathbf{x}))^{2}].\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "\u2022 $(\\alpha,\\epsilon)$ -validity: we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\underset{{\\bf x}\\sim\\mathcal{D}}{\\mathbb{P}}[g({\\bf x})=0]\\le\\alpha\\,d i s t_{T V}(\\mathcal{D}^{\\prime},\\mathcal{D})+\\frac{\\epsilon}{2},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "which in particular implies that $\\begin{array}{r}{\\mathbb{P}_{\\mathbf{x}\\sim\\mathcal{D}^{\\prime}}[g(\\mathbf{x})=0]\\le(1+\\alpha)d i s t_{T V}(\\mathcal{D}^{\\prime},\\mathcal{D})+\\epsilon/2.}\\end{array}$ ", "page_idx": 29}, {"type": "text", "text": "We also restate Algorithm 1 as follows: ", "page_idx": 29}, {"type": "text", "text": "1. Draw sets $S_{D}$ and $S_{D^{\\prime}}$ of $\\begin{array}{r}{N=C\\left(\\frac{(k d)^{k}B}{\\epsilon}\\log\\frac{1}{\\delta}\\right)^{C}}\\end{array}$ samples from distributions $\\mathcal{D}$ and $\\mathcal{D}^{\\prime}$ respectively, where $C$ is a sufficiently large absolute constant. ", "page_idx": 29}, {"type": "text", "text": "2. Run the algorithm of Theorem E.1 on the input $S_{D^{\\prime}}$ . Set $i_{\\mathrm{max}}$ to be the number of iterations of the main loop in the algorithm of Theorem E.1, and store the polynomials $\\{p_{i}\\}$ , values $\\{\\tau_{i}\\}$ computed at each iteration of the main loop, as well as the matrix $\\widehat{M}$ .   \n3. Output the function $g:\\mathbb{R}^{d}\\rightarrow\\{0,1\\}$ that does the following given an input $\\mathbf{x}$ in $\\mathbb{R}^{d}$ : (a) If $\\mathrm{max}_{p}$ of degree $_k(p(\\mathbf{x}))^{2}>B_{0}$ , then $g\\mathbf{(x)}$ is defined to be 0. (See Section E.1.1 to see $p^{\\top}\\overleftarrow{M}p{\\leq}1$ how to compute this quantity in time $\\mathrm{poly}(N)$ .) (b) If for some $i$ it is the case that $(p_{i}(\\mathbf{x}))^{2}$ is greater than $\\tau_{i}$ , then $g\\mathbf{(x)}$ is defined to be 0. (c) Otherwise, $g\\mathbf{(x)}$ is defined to be 1. ", "page_idx": 30}, {"type": "text", "text": "It is immediate from Theorem E.1 that the algorithm above runs in time $\\mathrm{poly}\\,\\left(\\frac{(k d)^{k}B}{\\epsilon}\\log\\frac{1}{\\delta}\\right)$ with probability at least $1-\\delta$ . Furthermore, we also see that the function $g$ can be described using poly (kd\u03f5)kBlog \u03b41 bits and can be computed using this description on a given input x in time poly $\\left({\\frac{(k d)^{k}B}{\\epsilon}}\\log{\\frac{1}{\\delta}}\\right)$ ", "page_idx": 30}, {"type": "text", "text": "We need the following claim bounding the number of iterations in the algorithm of Theorem E.1, proof of which is deferred until the end of this section. We remark that for Theorem E.1 we bounded the total number of iterations by $N$ , but in this section we will need a bound that depends only on $d,k$ and $\\epsilon$ and not on $N$ . ", "page_idx": 30}, {"type": "text", "text": "Claim 6. If the set $S_{D}$ satisfies the condition of Claim 1, then the number of iterations $i_{\\mathrm{max}}$ of the main while loop in the algorithm of Theorem E.1 satisfies $i_{\\mathrm{max}}=O\\left(k d^{k}\\log(B_{0}d)\\right)$ . ", "page_idx": 30}, {"type": "text", "text": "fWires t nsotewp ,p rwoec esehdo two  tuhsee f oClllaoiwmi n6g t:o argue the spectral $\\frac{200}{\\alpha}$ -boundedness and $(\\alpha,\\epsilon/2)$ -validity. As the ", "page_idx": 30}, {"type": "text", "text": "Observation E.3. There exists a function class $\\mathcal{G}$ with a VC dimension of at most $O(i_{\\mathrm{max}}~d^{2k}\\log(i_{\\mathrm{max}}~))$ , such that all possible values of the function $g$ belong to $\\mathcal{G}$ . ", "page_idx": 30}, {"type": "text", "text": "Proof. The function $g$ is necessarily a logical AND of at most $i_{\\operatorname*{max}}+1$ functions, one of which is the function indicator of a ball in $\\mathbb{R}^{d}$ and the other $i_{\\mathrm{max}}$ are logical OR-s of pairs of degree- $.2k$ polynomial threshold functions. Combining this with Fact A.5 and Fact A.4 yields the observation. \u53e3 ", "page_idx": 30}, {"type": "text", "text": "We start with arguing the $(\\alpha,\\epsilon/2)$ -validity, as well as the stronger condition of Remark 3.3 (implied by Equation E.13): ", "page_idx": 30}, {"type": "text", "text": "Claim 7. With probability at least $1-\\delta/2$ over the choice of the sets $S_{D}$ and $S_{D^{\\prime}}$ , we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\underset{{\\bf x}\\sim\\mathcal{D}}{\\mathbb{P}}[g({\\bf x})=0]\\le\\alpha\\,d i s t_{T V}(\\mathcal{D}^{\\prime},\\mathcal{D})+\\frac{\\epsilon}{2}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Furthermore, for $\\sigma\\ >\\ \\alpha/2$ and any distribution $\\mathcal{D}^{\\prime\\prime}$ that is $1/\\sigma$ -smooth w.r.t. $\\mathcal{D}$ , (i.e. for any measurable set $T\\subset\\mathbb{R}^{d}$ we have $\\begin{array}{r}{\\mathbb{P}_{\\mathbf{x}\\sim\\mathcal{D}^{\\prime\\prime}}[\\mathbf{x}\\in T]\\le\\frac{1}{\\sigma}\\mathbb{P}_{\\mathbf{x}\\sim\\mathcal{D}}[\\mathbf{x}\\in T])}\\end{array}$ it is the case that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\underset{\\mathbf{x}\\sim\\mathcal{D}}{\\mathbb{P}}[g(\\mathbf{x})=0]\\le\\frac{\\alpha}{\\sigma}\\,\\mathrm{d_{TV}}\\big(\\mathcal{D}^{\\prime\\prime},\\mathcal{D}^{\\prime}\\big)+\\frac{\\epsilon}{2},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. With probability at least $1-\\delta/4$ the set $S_{D}$ satisfies the condition of Claim 1. Assuming this, we have: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{\\mathbf{x}\\sim\\mathcal{D}}{\\mathbb{P}}[g(\\mathbf{x})=0]\\leq\\displaystyle\\sum_{i=0}^{i_{\\operatorname*{max}}}\\underset{\\mathbf{x}\\sim\\mathcal{D}}{\\mathbb{P}}\\left[(p_{i}(\\mathbf{x}))^{2}\\geq\\tau_{i}\\right]\\leq\\medskip}\\\\ {\\underset{i=0}{\\overset{i_{\\operatorname*{max}}}{\\sum}}\\underset{\\mathbf{x}\\sim S_{D}}{\\mathbb{P}}\\left[(p_{i}(\\mathbf{x}))^{2}\\geq\\tau_{i}\\right]+i_{\\operatorname*{max}}\\left(200\\sqrt{d^{2k}\\frac{\\log N}{N}\\log\\frac{1}{\\delta}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the last step used the premise that $S_{D}$ satisfies the condition of Claim 1. Recalling that by Observation E.3 the function $g$ belongs to a function class with VC dimension of $O(i_{\\mathrm{max}}\\,d^{O(k)}\\log(i_{\\mathrm{max}}))$ , and combining this with Fact A.6, we see that with probability at least $1-\\delta/4$ ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathop{\\mathbb{P}}_{\\mathbf{x}\\sim\\mathcal{D}^{\\prime}}[g(\\mathbf{x})=0]\\ge\\mathop{\\mathbb{P}}_{\\mathbf{x}\\sim S_{\\mathcal{D}^{\\prime}}}[g(\\mathbf{x})=0]-O\\left(\\sqrt{\\frac{i_{\\operatorname*{max}}\\,d^{O(k)}\\log(i_{\\operatorname*{max}})\\log N}{N}\\log\\frac{1}{\\delta}}\\right)\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Recalling the definition of $g$ , we see that for $\\mathbf{x}$ in $S_{D^{\\prime}}$ , we have $g(\\mathbf{x})=0$ when for some iteration $i$ , the point $\\mathbf{x}$ is in the set $\\{\\mathbf{x}\\in S_{\\mathrm{filtered}}^{i}:\\;(p_{i}(\\mathbf{x}))^{2}>\\tau_{i}\\}$ , we conclude that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\mathbf{x}\\sim\\mathcal{D}^{\\prime}}{\\mathbb{P}}[g(\\mathbf{x})=0]\\ge}\\\\ &{\\quad\\quad\\overset{i_{\\mathrm{max}}}{\\underset{i=0}{\\sum}}\\frac{\\lvert\\{\\mathbf{x}\\in S_{\\mathrm{filterd}}^{i}:(p_{i}(\\mathbf{x}))^{2}>\\tau_{i}\\}\\rvert}{N}-O\\left(\\sqrt{\\frac{i_{\\mathrm{max}}\\,d^{O(k)}\\log(i_{\\mathrm{max}})\\log N}{N}\\log\\frac{1}{\\delta}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Now, we recall that for every iteration $i$ we have: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\frac{1}{N}\\Big\\vert\\{\\mathbf{x}\\in S_{\\mathrm{filered}}^{i}:\\;(p_{i}(\\mathbf{x}))^{2}>\\tau_{i}\\}\\Big\\vert\\geq\\frac{10}{\\alpha}\\left(\\underset{\\mathbf{x}\\sim S_{D}}{\\mathbb{P}}[(p_{i}(\\mathbf{x}))^{2}>\\tau_{i}]+200\\sqrt{d^{2k}\\frac{\\log N}{N}\\log\\frac{1}{\\delta}}\\right),\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and therefore: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{i=0}^{i_{\\mathrm{max}}}\\underline{{\\mathbb{P}}}_{\\boldsymbol{x}}[(p_{i}(\\mathbf{x}))^{2}>\\tau_{i}]\\leq}\\\\ {\\displaystyle\\frac{\\alpha}{10}\\left(\\sum_{i=0}^{i_{\\mathrm{max}}}\\frac{1}{N}\\Big|\\{\\mathbf{x}\\in S_{\\mathrm{filered}}^{i}:\\;(p_{i}(\\mathbf{x}))^{2}>\\tau_{i}\\}\\Big|\\right)+200i_{\\mathrm{max}}\\sqrt{\\frac{d^{2k}\\log N}{N}\\log\\frac{1}{\\delta}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Thus, combining Equation E.14, Equation E.15 and Equation E.16 we get: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\operatorname{\\mathbb{P}}_{\\mathbf{x}\\sim\\mathcal{D}}[g(\\mathbf{x})=0]\\le\\frac{\\alpha}{10}\\underbrace{\\Big(\\underset{\\mathbf{x}\\sim\\mathcal{D}^{\\prime}}{\\mathbb{P}}[g(\\mathbf{x})=0]\\Big)}_{\\le\\mathrm{distrv}(\\mathcal{D}^{\\prime},\\mathcal{D})+\\mathbb{P}_{\\mathbf{x}\\sim\\mathcal{D}}[g(\\mathbf{x})=0]}+O\\left(i\\operatorname*{max}\\sqrt{\\frac{d^{O(k)}\\log N}{N}\\log\\frac{1}{\\delta}}\\right).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We now the bound on $i_{\\mathrm{max}}$ from Claim 6, and recall that $\\begin{array}{r}{N=C\\left(\\frac{(k d)^{k}B}{\\epsilon}\\log\\frac{1}{\\delta}\\right)^{C}}\\end{array}$ . Overall, we see that for sufficiently large absolute constant $C$ the error term above is upper-bounded by $\\epsilon/10$ , so ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\underset{\\mathbf{x}\\sim\\mathcal{D}}{\\mathbb{P}}[g(\\mathbf{x})=0]\\le\\frac{\\alpha}{10}\\bigg(\\mathrm{dist}_{\\mathrm{TV}}(\\mathcal{D}^{\\prime},\\mathcal{D})+\\underset{\\mathbf{x}\\sim\\mathcal{D}}{\\mathbb{P}}[g(\\mathbf{x})=0]\\bigg)+\\frac{\\epsilon}{10}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Rearranging the inequality above and recalling that $\\alpha<1$ , we conclude that Equation E.12 holds. Finally, we prodeed to argue Equation E.13. For $\\sigma>\\alpha/2$ suppose that the distribution $\\mathcal{D}^{\\prime\\prime}$ is $1/\\sigma\\cdot$ - smooth w.r.t. $\\mathcal{D}$ , (i.e. for any measurable set $T\\subset\\mathbb{R}^{d}$ we have $\\begin{array}{r}{\\mathbb{P}_{\\mathbf{x}\\sim\\mathcal{D}^{\\prime\\prime}}[\\mathbf{x}\\in T]\\le\\frac{1}{\\sigma}\\mathbb{P}_{\\mathbf{x}\\sim\\mathcal{D}}[\\mathbf{x}\\in\\dot{T}])}\\end{array}$ . Then, from Equation E.17 we have ", "page_idx": 31}, {"type": "image", "img_path": "LnNfwc2Ah1/tmp/5c3a3b8375bcc9d32d31d0432e41b9c1f25a1c7d2ebb48fa9f2b1aa0eba75425.jpg", "img_caption": [], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "Rearranging the inequality above and recalling that $\\alpha<1$ and $\\sigma>\\alpha/2$ , we conclude that Equation E.13 holds. ", "page_idx": 31}, {"type": "text", "text": "Now, we argue the spectral $\\frac{200}{\\alpha}$ -boundedness. Recall that with probability at least $\\delta/20$ the matrix $\\widehat{M}$ satisfies Equation E.1, which we will henceforth assume. Also recall that Claim 4 says that for every polynomial $p$ of degree at most $k$ satisfying $\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}[(p(\\mathbf{x}))^{2}]\\le1$ , the set $S_{\\mathrm{accept}}$ given by the algorithm in Theorem E.1 satisfies $\\begin{array}{r}{\\frac{1}{N}\\sum_{\\mathbf{x}\\in S_{\\mathrm{accept}}}p(\\mathbf{x})^{2}\\leq\\frac{100}{\\alpha}}\\end{array}$ . By inspecting the definition of the function $g$ , we see that for $\\mathbf{x}$ in $S_{D^{\\prime}}$ we have $g(\\mathbf{x})=1$ if an only if $\\mathbf{x}$ is in $S_{\\mathrm{accept}}$ . Therefore, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\substack{p\\,\\mathrm{{of}\\,d e g r e e\\;}k\\mathrm{\\;s.t:\\atop{\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}}[(p(\\mathbf{x}))^{2}]\\leq1}}}\\left[\\!\\!\\!\\begin{array}{c}{\\mathbb{E}}\\\\ {\\mathbf{x}{\\sim}S_{\\mathcal{D^{\\prime}}}}[g(\\mathbf{x})p(\\mathbf{x})^{2}]\\!\\!\\right]\\leq\\frac{100}{\\alpha}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "In order to conclude the spectral $\\textstyle{\\frac{200}{\\alpha}}$ -boundedness condition we need to be able to conclude that the equation above is likely to generalize, i.e. it approximately holds when one replaces the expectation w.r.t. $S_{D^{\\prime}}$ with the expectation w.r.t. the distribution $\\mathcal{D}^{\\prime}$ . To show this, we first recall that via Observation E.3 the function $g$ belongs to a function class $\\mathcal{G}$ with a VC dimension of at most $O(i_{\\mathrm{max}}\\,d^{2k}\\log(i_{\\mathrm{max}}\\,))$ . We also see that $g(\\mathbf{x})=0$ for all $\\mathbf{x}$ satisfying $\\operatorname*{max}_{\\substack{{}_{2}\\colon\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}\\,[(p(\\mathbf{x}))^{2})]\\le1}}(p(\\mathbf{x}))^{2}>$ $10B_{0}$ , because the matrix $\\widehat{M}$ \u221asatisfies Eq\u221auation E.1 and th\u221aerefore if $\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}[(p(\\mathbf{x}))^{2})]\\ \\le\\ 1$ and $(p(\\mathbf{x}))^{2}\\;>\\;10B_{0}$ , then also $(\\sqrt{0.9}p)^{\\top}M(\\sqrt{0.9}p)\\,\\leq\\,1$ and $\\sqrt{0.9}p(\\mathbf{x})\\,>\\,B_{0}$ , which implies that $g(\\mathbf{x})=0$ by the definition of $g$ . We show in Lemma B.2 that with probability at least $1-\\delta/20$ such function classes satisfy ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{g\\in\\mathcal{G}}{\\operatorname*{sup}}\\quad\\quad\\underset{\\mathbf{x},\\tau\\in\\mathcal{D}(p(\\mathbf{x}))^{2}\\leq1}{\\operatorname*{sup}}\\left|\\frac{1}{N}\\sum_{\\mathbf{x}\\sim S_{D^{\\prime}}}\\left[f(\\mathbf{x})(p(\\mathbf{x}))^{2}\\right]-\\underset{\\mathbf{x}\\sim\\mathcal{D}^{\\prime}}{\\mathbb{E}}\\left[f(\\mathbf{x})(p(\\mathbf{x}))^{2}\\right]\\right|\\leq}\\\\ &{\\quad\\quad\\quad\\quad O\\left(\\sqrt{B_{0}}\\left(i_{\\operatorname*{max}}\\,d^{2k}\\log(i_{\\operatorname*{max}})\\,d^{2k}\\frac{\\log N}{N}\\log\\frac{1}{\\delta}\\right)^{1/4}\\right)\\leq1\\leq\\frac{1}{\\alpha},}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where the penultimate inequality above is achieved by substituting the bound $\\mathrm{\\Delta}i_{\\mathrm{max}}\\quad=$ $O\\left(k d^{k}\\log B_{0}d\\right)$ Calnadi mta k6i inngt thoe  beex pa rseusfsifoicine antbloy vlea,r gseu basbtsitoultuitneg $\\begin{array}{r}{\\bar{B_{0}}=\\frac{4d^{3k}}{\\epsilon}}\\end{array}$ , mrebicnaillnign gE tqhuaat$\\begin{array}{r}{N=C\\left(\\frac{(k d)^{k}}{\\epsilon}\\log\\frac{1}{\\delta}\\right)^{C}}\\end{array}$ $C$ constant. Co tion E.21 with Equation E.20 we conclude that with probability at least $1-\\delta/20$ it is the case that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\substack{p\\;\\mathrm{of}\\;\\mathrm{degree}\\;k\\;\\mathrm{s.t.}\\quad\\mathbf{x}\\sim\\mathcal{D}^{\\prime}}}\\big[(p(\\mathbf{x}))^{2}g(\\mathbf{x})\\big]\\le\\frac{101}{\\alpha},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Overall, with probability at least $1\\!-\\!\\delta$ the function $g$ satisfies spectral $\\frac{200}{\\alpha}$ -boundedness, $(\\alpha,\\epsilon)$ -validity, as well as the required run-time bound. ", "page_idx": 32}, {"type": "text", "text": "Finally, we come back to Claim 6, proving which concludes this section. ", "page_idx": 32}, {"type": "text", "text": "Proof of Claim $6$ . Let $i$ be an iteration such that $i<i_{\\mathrm{max}}$ . Since the while loop did not terminate on step $i$ , we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\sum_{\\mathbf{x}\\in S_{\\mathrm{filtered}}^{i}}(p_{i}(\\mathbf{x}))^{2}>\\frac{100}{\\alpha}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "At the same time, Claim 5 implies that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\frac{1}{N}\\sum_{\\mathbf{x}\\in S_{\\mathrm{filtered}}^{i+1}}(p_{i}(\\mathbf{x}))^{2}\\leq\\frac{20}{\\alpha}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Let $m\\leq d^{k}$ denote the dimension of the vector space of degree- ${\\cdot k}$ polynomials. For values of $i$ between 0 and $i_{\\mathrm{max}}$ and for values of $j$ between 1 and $m$ ,let the collection of polynomials $\\{R_{j}^{i}\\}$ and non-negative real values $\\{\\lambda_{j}^{i}\\}$ be defined as ", "page_idx": 32}, {"type": "equation", "text": "$$\nR_{j}^{i}=\\operatorname*{arg\\,max}_{\\stackrel{R\\,\\mathrm{of\\,degree\\,}k\\,.}{\\forall j^{\\prime}<j:\\,(R_{j^{\\prime}}^{i})^{\\top}\\,\\widehat{M}R=0}}\\frac{1}{N}\\sum_{\\substack{\\mathbf{x}\\in S_{\\mathrm{ihtered}}^{i}}}\\big[(R(\\mathbf{x}))^{2}\\big]\\qquad\\lambda_{j}^{i}=\\frac{1}{N}\\sum_{\\substack{\\mathbf{x}\\in S_{\\mathrm{ihtered}}^{i}}}\\big[(R_{j}^{i}(\\mathbf{x}))^{2}\\big]\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "In particular7, we have $p_{i}=R_{1}^{i}$ . We will use the quantity $\\varphi_{i}:=\\sum_{j=1}^{m}\\lambda_{j}^{i}$ as a potential function, for which we have: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\frac{\\varphi_{i}-\\varphi_{i+1}}{N}\\sum_{\\substack{j=1}}^{m}\\sum_{\\mathbf{x}\\in S_{\\mathrm{ihecd}}^{i}\\backslash S_{\\mathrm{ihecel}}^{i+1}}(R_{j}^{i}(\\mathbf{x}))^{2}\\geq\\frac{1}{N}\\sum_{\\substack{\\mathbf{x}\\in S_{\\mathrm{ihered}}^{i}\\backslash S_{\\mathrm{ihered}}^{i+1}}}(R_{1}^{i}(\\mathbf{x}))^{2}=\\frac{1}{N}\\sum_{\\substack{\\mathbf{x}\\in S_{\\mathrm{ihecd}}^{i}\\backslash S_{\\mathrm{ihered}}^{i+1}}}(p_{i}(\\mathbf{x}))^{2}=\\frac{\\varphi_{i}}{N}\\sum_{\\substack{\\mathbf{x}\\in S_{\\mathrm{ihered}}^{i}\\backslash S_{\\mathrm{ihered}}^{i+1}}}(p_{i}(\\mathbf{x}))^{2}=\\frac{\\varphi_{i}}{N}\\sum_{\\substack{\\mathbf{x}\\in S_{\\mathrm{ihered}}^{i+1}\\backslash S_{\\mathrm{ihered}}^{i+1}}}(p_{i}(\\mathbf{x}))^{2}=\\frac{\\varphi_{i}}{N}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Where in the end we substituted Equation E.23. Since $\\lambda_{1}^{i}$ equals to $\\frac{1}{N}\\sum_{\\mathbf{x}\\in S_{\\mathrm{filtered}}^{i}}\\left[(p_{i}(\\mathbf{x}))^{2}\\right]$ and has a value of at least $100/\\alpha$ by Equation E.22, the inequality above allows us to conclude ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\varphi_{i+1}\\leq\\sum_{j=1}^{m}\\lambda_{j}^{i}-0.8\\lambda_{1}^{i}\\leq\\sum_{j=1}^{m}\\lambda_{j}^{i}-\\frac{0.8}{m}\\sum_{j}\\lambda_{j}^{i}\\leq\\left(1-\\frac{0.9}{d^{k}}\\right)\\varphi_{i}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We now combine the inequality above with the following two observations: ", "page_idx": 33}, {"type": "text", "text": "\u2022 We have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\varphi_{i_{\\mathrm{max}}-1}>\\frac{100}{\\alpha}>1,\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "because the algorithm did not terminate in the $(i_{\\mathrm{max}}-1)$ -th iteration, and therefore Equation E.22 holds. ", "page_idx": 33}, {"type": "text", "text": "\u2022 We have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\varphi_{0}=\\frac{1}{N}\\sum_{j=1}^{m}\\sum_{\\mathbf{x}\\in S_{\\mathrm{filtered}}^{0}}(R_{j}^{i}(\\mathbf{x}))^{2}\\leq B_{0}m,\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where the last inequality follows from the fact that every element $\\mathbf{x}$ in Sf0iltered satisfies ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{p{\\mathrm{~of~degree~}}k\\colon p^{\\top}\\widehat{M}p\\le1}(p(\\mathbf{x}))^{2}\\le B_{0}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Overall, the two bounds above, together with Equation E.26 allow us to conclude that: ", "page_idx": 33}, {"type": "equation", "text": "$$\ni_{\\mathrm{max}}\\leq O\\left(d^{k}\\log(B_{0}m)\\right)=O\\left(k d^{k}\\log(B_{0}d)\\right),\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where the last step follows by substituting the definitions of $m$ and $\\epsilon$ . ", "page_idx": 33}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We provide theorems with complete proofs or relevant citations for all claims made in the abstract and introduction. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 34}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: We include a paragraph on the limitations of our work in the end of the main paper. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 34}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: Our theorem statements clearly state the assumptions and we provide full proofs in the main paper or appendix. Wherever possible, we provide proof sketches in the main paper. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 35}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: Our paper does not have any experiments. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 35}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: Our paper does not have any experiments. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 36}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: Our paper does not have any experiments. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 36}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: Our paper does not have any experiments. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 37}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: Our paper does not have any experiments. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 37}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We read the code of ethics in its entirety and strongly believe that our research abides by the stated code. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 37}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We include a discussion on the broader impacts of our work at the end of the main paper. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 38}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: We do not release any data and models. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 38}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: Our paper does not use any existing code, data, or models. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 38}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 39}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: Our paper does not release any new assets. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 39}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 39}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Justification: Our paper does not involve crowd-sourcing nor research with human subjects. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 39}]