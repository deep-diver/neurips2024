[{"type": "text", "text": "Continual learning with the neural tangent ensemble ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ari S. Benjamin Christian Pehle Kyle Daruwalla Cold Spring Harbor Laboratory Cold Spring Harbor, NY 11724 {benjami,pehle,daruwal}@cshl.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "A natural strategy for continual learning is to weigh a Bayesian ensemble of fixed functions. This suggests that if a (single) neural network could be interpreted as an ensemble, one could design effective algorithms that learn without forgetting. To realize this possibility, we observe that a neural network classifier with $_\\mathrm{N}$ parameters can be interpreted as a weighted ensemble of $_\\mathrm{N}$ classifiers, and that in the lazy regime limit these classifiers are fixed throughout learning. We call these classifiers the neural tangent experts and show they output valid probability distributions over the labels. We then derive the likelihood and posterior probability of each expert given past data. Surprisingly, the posterior updates for these experts are equivalent to a scaled and projected form of stochastic gradient descent (SGD) over the network weights. Away from the lazy regime, networks can be seen as ensembles of adaptive experts which improve over time. These results offer a new interpretation of neural networks as Bayesian ensembles of experts, providing a principled framework for understanding and mitigating catastrophic forgetting in continual learning settings. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Neural networks often forget previous knowledge when trained with gradient descent. In contrast, animals learn from sequential experiences, suggesting that true \u2018lifelong learners\u2019 use different strategies for learning [25]. ", "page_idx": 0}, {"type": "text", "text": "One strategy to learns without forgetting is to update the posterior distribution over a set of fixed probabilistic models [8]. This includes any fully Bayesian model, such as Bayesian linear regression. The fundamental reason why these algorithms do not forget information is because the posterior over models is invariant to data sequence. Given two permutations of the data, the posterior will be the same. This property of posteriors has inspired many strategies to reduce forgetting by approximating the posterior distribution over neural networks [22, 24, 11, 28, 26, 41, 37]. However, these approximations introduce many new parameters and considerable memory overhead. In general, estimating the full posterior distribution over high-dimensional networks is prohibitive. ", "page_idx": 0}, {"type": "text", "text": "Here, we shift our perspective and instead interpret a single neural network as an ensemble of many experts. This allows tracking a posterior (over experts, instead of networks) without introducing any memory overhead besides the network itself. ", "page_idx": 0}, {"type": "text", "text": "This motivates our main result, which we note is generally applicable outside of continual learning. More specifically, we show that neural network classifiers perturbed by a small vector in parameter space can be described as a weighted ensemble of valid classifiers outputting a probability distribution over labels. We call this the Neural Tangent Ensemble (NTE). Inspired by the Neural Tangent Kernel, this result depends on a first-order Taylor expansion around a seed point [19]. As a consequence, it operates as an ensemble of fixed classifiers in the NTK limit of infinite width. ", "page_idx": 0}, {"type": "text", "text": "In this framework, learning is framed as Bayesian posterior updating rather than optimization. These two approaches might be expected to be quite different, as a posterior update is multiplicative whereas gradient-based optimization is additive. Surprisingly, however, we find that the NTE posterior update rule is approximately stochastic gradient descent (SGD) on the network with batch size 1, thus shedding new light on the dynamics of neural network optimization. ", "page_idx": 1}, {"type": "text", "text": "Our primary contributions are: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We introduce the Neural Tangent Ensemble (NTE), a novel formulation that interprets networks as ensembles of classifiers, with each parameter contributing one classifier.   \n\u2022 We derive the posterior update rule for the NTE for networks in the lazy regime, in which experts are fixed, and show that it is equivalent to single-example stochastic gradient descent (SGD) without momentum, projected to the probability simplex.   \n\u2022 This justifies the empirical finding that SGD with no momentum forgets much less than standard optimizer settings.   \n\u2022 We demonstrate that catastrophic forgetting in neural networks is associated with the transition from the lazy to the rich regime. ", "page_idx": 1}, {"type": "text", "text": "2 Motivation: Ensembles are natural continual learners ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "To demonstrate why Bayesian ensembles make good continual learners, consider a function $f(x)$ that is an ensemble of many experts $f_{i}(x)$ (Fig. 1). We will consider what it takes to modify this ensemble so that it performs well on two tasks $\\boldsymbol{\\mathcal{A}}$ and $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ . ", "page_idx": 1}, {"type": "text", "text": "A simple strategy for continual learning is to prune away experts. Let $\\mathcal{S}_{A}$ be the set of functions that are good (and equally good) for task $\\boldsymbol{\\mathcal{A}}$ . A good ensemble can be constructed by sampling from $\\mathcal{S}_{A}$ : ", "page_idx": 1}, {"type": "image", "img_path": "qOSFiJdVkZ/tmp/b155d727f832cb3b3ae3348879b07de903a839776c91bc25ecb0f09bc76f54a3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "equation", "text": "$$\nf_{A}(x)=\\frac{1}{N}\\sum_{f_{i}\\in S_{A}}f_{i}(x).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Given a subsequent task $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ , a new ensemble can be constructed on the fly by continuing to prune away the experts in $f_{A}(\\boldsymbol{x})$ that perform poorly on task $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ . The remaining ensemble is still composed of experts from $\\mathcal{S}_{A}$ (assuming that the set intersection is not zero). ", "page_idx": 1}, {"type": "text", "text": "In contrast to many continual learning strategies for neural networks, this does not require replay, task boundaries, or any additional memory dedicated to preserving old task performance. ", "page_idx": 1}, {"type": "text", "text": "Figure 1: High-level intuition for model averaging and continual learning. Pruning the set of functions $f_{i}$ to those good for task $\\boldsymbol{\\mathcal{A}}$ , followed by further pruning for tasks $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ and $\\mathcal{C}$ , will result in a set of $f_{i}$ still good on $\\boldsymbol{\\mathcal{A}}$ . ", "page_idx": 1}, {"type": "text", "text": "2.1 Belief updating generalizes set intersections ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In real ensembles, experts do not perform equally well. This justifies weighing each expert in the ensemble with weights $p_{i}$ which are chosen such that $\\textstyle\\sum_{i}^{N}p_{i}=1$ : ", "page_idx": 1}, {"type": "equation", "text": "$$\nf_{A}(x)=\\sum_{f_{i}\\in\\mathcal{F}}p_{i}f_{i}(x).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "This is particularly convenient if the experts encode the probability or belief about an event, $f_{i}(x)=$ $p(y|x,f_{i})$ . In this case, one can weigh each function by its posterior probability given previous data: ", "page_idx": 1}, {"type": "equation", "text": "$$\np(y|x,\\mathcal{D})=\\sum_{f_{i}\\in\\mathcal{F}}p(f_{i}|\\mathcal{D})\\;p(y|x,f_{i}).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "This is the optimal weighing strategy when the experts can be assumed to be independent [45]. ", "page_idx": 1}, {"type": "text", "text": "It is useful to contrast the ensemble in Eq. 2 with linear regression using a feature map, $f(x)=$ $\\textstyle\\sum_{i}w_{i}\\phi_{i}(x)$ , as might be observed in kernel regression. In an ensemble the weights $w_{i}$ are strictly positive, whereas weights in regression may switch sign arbitrarily. ", "page_idx": 1}, {"type": "text", "text": "2.2 The posterior is invariant to data ordering ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The property of Bayesian ensembles that motivates this paper is that the posterior probability of each expert is invariant to the order in which data in seen. This is because, like set intersections, single-task posteriors multiply to form the multi-task posterior: ", "page_idx": 2}, {"type": "equation", "text": "$$\np(f_{i}|A\\cap B)\\propto p(f_{i}|A)p(f_{i}|B).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "This property is restated more formally in the following Lemma: ", "page_idx": 2}, {"type": "text", "text": "Lemma 1. Invariance to data ordering in Bayesian Ensembles. Let $\\mathcal{F}=\\,f_{1},...,f_{N}$ be a set of fixed experts, $\\mathcal{W}=\\,w_{1},...,w_{N}$ be their weights, and $\\mathcal{D}=D_{1},...,D_{T}$ be a sequence of datasets from $T$ tasks. Then, for any permutation $\\pi$ of the indices $I_{\\mathrm{:}}$ , ..., T, $p(f_{i}|\\mathcal{D})=p(f_{i}|D_{1},...,D_{T})=$ $p(f_{i}|D_{\\pi(1)},...,D_{\\pi(T)})$ ", "page_idx": 2}, {"type": "text", "text": "Proof. By Bayes\u2019 rule, $\\begin{array}{r}{p(f_{i}|\\mathcal{D})\\propto p(f_{i})\\prod_{t=1}^{T}p(D_{t}|f_{i})}\\end{array}$ . The right-hand side is a product of terms, one for each dataset. Since multiplication is commutative, $\\begin{array}{r}{\\prod_{t=1}^{T}p(D_{t}|f_{i})=\\prod_{t=1}^{T}p(D_{\\pi(t)}|f_{i})}\\end{array}$ for any permutation $\\pi$ . Therefore, $p(f_{i}|D_{1},...,D_{T})=p(f_{i}|D_{\\pi(1)},...,D_{\\pi(T)})$ . \u53e3 ", "page_idx": 2}, {"type": "text", "text": "Thus, there is no catastrophic forgetting problem for models which are ensembles of fixed, independent probabilistic classifiers. This motivates assessing under what conditions neural networks approach this setting. ", "page_idx": 2}, {"type": "text", "text": "3 The Neural Tangent Ensemble ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "How might a neural network be described as an ensemble? One simple strategy would be to take the last network layer as a set of functions, and then to choose the output weights according to their relative performance. However, this is is an expensive strategy to construct a relatively small set of classifiers, and it does not specify how earlier weights might change. ", "page_idx": 2}, {"type": "text", "text": "Here, we employ a first-order Taylor expansion to show that neural networks are (approximately) large ensembles over $N$ component functions, one for each edge in the network. We will examine a neural network $p(y|x,W^{(t)})$ with parameters $W^{(t)}$ whose output represents the probability or confidence of a label $y$ given input $x$ . We can describe this output with a linearization around a very nearby seed point $W^{(0)}$ . Note that we use the notation $W^{(0)}$ and $W^{(t)}$ for consistency and in general $W^{(0)}$ need not be the initialization or on the optimization trajectory at all. ", "page_idx": 2}, {"type": "equation", "text": "$$\np(y|x,W^{(t)})\\approx p(y|x,W^{(0)})+\\sum_{i}^{N}\\Delta w_{i}\\frac{\\partial p(y|x,W^{(0)})}{\\partial w_{i}^{(0)}}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "At first glance it does not appear that this Taylor expansion is an ensemble. There seem to be no true classifiers: the gradients are not probabilities over classes $y$ , being neither nonnegative, bounded, nor normalized to 1 across the output labels. Nor are there true weights, as $\\Delta w_{i}$ is also not nonnegative. However, both of these criteria can be met with some rearrangements and with the assumption that the loss is sufficiently smooth with respect to its parameters. This leads to our main result: ", "page_idx": 2}, {"type": "text", "text": "Theorem 2. Suppose $p(y|x,W^{(0)})$ is a neural network for which the log-likelihood is $L$ \u2212Lipschitz continuous in its parameters, i.e. all gradients of the loss are bounded by a constant $L$ . Let $W^{(0)}$ then be perturbed by a $\\Delta W$ with $\\|\\Delta W\\|_{1}=z$ . If the perturbation is sufficiently small (with $z L<1,$ ), then the network can be described as an ensemble of a set of $N$ classifiers $\\{p(y|x,f_{i})\\}_{i}^{N}$ , each with weight $\\frac{|\\Delta w_{i}|}{z}$ , plus higher-order contributions which vanish for small $z$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\np(\\boldsymbol{y}|\\boldsymbol{x},W^{(t)})=\\sum_{\\boldsymbol{w e i g h t s}}^{N}\\frac{|\\Delta w_{i}|}{z}\\,p(\\boldsymbol{y}|\\boldsymbol{x},f_{i})\\,+\\,\\mathcal{O}(\\|\\Delta W\\|_{2}^{2})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Each classifier $p(\\boldsymbol{y}|\\boldsymbol{x},f_{i})$ , which we call the neural tangent expert, outputs a probability distribution over labels $y$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\np(y|x,f_{i})=p(y|x,W^{(0)})\\left(1+z\\,s i g n(\\Delta w_{i})\\frac{\\partial}{\\partial w_{i}^{(0)}}\\log p(y|x,W^{(0)})\\right)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The proof is postponed to Appendix 8.1. Informally, it relies two simple rearrangements: splitting the weights into sign and magnitude $\\Delta w_{i}=|\\Delta w_{i}|\\mathrm{sign}(\\Delta w_{i})$ , and bringing the zeroth order term inside the first-order sum. This results in a sum over a term which, surprisingly, sums to 1 over the output labels and is weighted by a term that sums to 1 over experts, meeting the criteria of an ensemble. ", "page_idx": 3}, {"type": "text", "text": "This simple reformulation invites a change in perspective about the role of each parameter in a deep neural network. Each parameter contributes a separate classifier. The distributed architecture and connected paths of the network matter, but they explicitly contribute through the gradients alone. ", "page_idx": 3}, {"type": "text", "text": "In the literature on ensembles, a common focus is to examine the quality and diversity of the experts separately. By the bias/variance decomposition, both aspects enter in the generalization error [38, 47]. Here, it is clear that all experts share a factor that is the overall quality of the center of the Taylor expansion, $p(y|x,W^{(0)})$ . What distinguishes experts from one another is the diversity of network gradients. ", "page_idx": 3}, {"type": "text", "text": "3.1 Experts are fixed in the lazy regime ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "This paper is motivated by the fact that Bayesian ensembles of fixed experts do not forget past data when learning by posterior updating. Under what conditions is the Neural Tangent Ensemble composed of fixed functions? ", "page_idx": 3}, {"type": "text", "text": "The answer to this question follows directly from the literature on the Neural Tangent Kernel (NTK) and lazy regime networks [19, 7]. If the network is in the \u2018lazy\u2019 regime, then the Jacobian of the network does not change during gradient descent learning and the linearization remains valid. This occurs in the limit of infinite width for MLPs for certain initializations [19]. (Output scaling also controls laziness [7], and is a necessary when using softmax nonlinearities even in the infinite width [29].) As a consequence, the experts in the NTE interpretation are fixed functions in the lazy regime. ", "page_idx": 3}, {"type": "text", "text": "3.2 Learning ensemble weights ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "If a network is secretly an ensemble, how should it learn from new data? The natural next step is to convert the NTE into a Bayesian ensemble. In a Bayesian ensemble, the weight of each function is its posterior probability given past data: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{|\\Delta w_{i}|}{z}\\gets p(f_{i}|\\mathcal{D})=\\frac{p(\\mathcal{D}|f_{i})\\,p(f_{i})}{\\sum_{i}p(\\mathcal{D}|f_{i})\\,p(f_{i})}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This can be seen as the $\\boldsymbol{\\mathrm E}$ step in a generalized EM algorithm [45]. In the following section, we will describe how to calculate this posterior probability with an online learning algorithm. For the moment, we assume the experts are fixed functions (i.e. the network is lazy). ", "page_idx": 3}, {"type": "text", "text": "3.2.1 The data likelihood ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Lemma 3. For IID data $\\begin{array}{r}{\\mathcal{D}=\\left\\{x_{k},y_{k}\\right\\}_{k=1}^{P}}\\end{array}$ , the likelihood of the data under an expert can be written in terms of a log-likelihood loss function $\\ell_{k}^{(0)}=-\\log p(y_{k}|x_{k},W^{(0)})$ of the network at initialization: ", "page_idx": 3}, {"type": "equation", "text": "$$\np(\\mathcal{D}|f_{i})=\\prod_{e x a m p l e s\\;k}e^{-\\ell_{k}^{(0)}}\\left(1-z\\,s i g n(\\Delta w_{i})\\frac{\\partial}{\\partial w_{i}^{(0)}}\\ell_{k}^{(0)}\\right)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Proof. Starting with the data likelihood, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p(\\mathcal{D}|f_{i})=\\displaystyle\\prod_{\\mathrm{examples}\\;k}p(y_{k}|x_{k},f_{i})}\\\\ &{\\qquad=\\displaystyle\\prod_{\\mathrm{examples}\\;k}\\left(p(y_{k}|x_{k},W^{(0)})+z\\,\\mathrm{sign}(\\Delta w_{i})\\frac\\partial{\\partial w_{i}^{(0)}}p(y_{k}|x_{k},W^{(0)})\\right)}\\\\ &{\\qquad=\\displaystyle\\prod_{\\mathrm{examples}\\;k}p(y_{k}|x_{k},W^{(0)})\\left(1+z\\,\\mathrm{sign}(\\Delta w_{i})\\frac\\partial{\\partial w_{i}^{(0)}}\\log p(y_{k}|x_{k},W^{(0)})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Plugging in the definition of $\\ell_{k}^{(0)}$ yields the above expression. ", "page_idx": 4}, {"type": "text", "text": "3.2.2 The posterior probability: renormalization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "After the data likelihoods are computed for each neural tangent expert, they must be renormalized to obtain the posterior probabilities. In our case, we naturally have access to a very large number of tangent experts and their likelihoods. Indeed, if the width is indeed taken to infinity, this there are infinitely many neural tangent experts in a single network. We propose to use the distribution of likelihoods in the current network as a Monte Carlo estimate of the normalizing constant. ", "page_idx": 4}, {"type": "equation", "text": "$$\np(f_{i}|\\mathcal{D})=\\frac{\\prod_{\\mathrm{examples}\\;k}e^{-\\ell_{k}^{(0)}}\\left(1-z\\,\\mathrm{sign}(\\Delta w_{i})\\frac{\\partial}{\\partial w_{i}^{(0)}}\\ell_{k}^{(0)}\\right)p(f_{i})}{\\sum_{i}\\prod_{\\mathrm{examples}\\;k}e^{-\\ell_{k}^{(0)}}\\left(1-z\\,\\mathrm{sign}(\\Delta w_{i})\\frac{\\partial}{\\partial w_{i}^{(0)}}\\ell_{k}^{(0)}\\right)p(f_{i})}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This can be simplified by noting that each $e^{-\\ell_{k}^{(0)}}$ term will cancel; the product $\\prod_{k}e^{-\\ell_{k}^{(0)}}$ appears in every additive term in the denominator. Assuming a uniform prior $\\bar{p(f_{i})}$ , we then have: ", "page_idx": 4}, {"type": "equation", "text": "$$\np(f_{i}|T)=\\frac{\\prod_{k}\\left(1-z\\,\\mathrm{sign}(\\Delta w_{i})\\frac{\\partial}{\\partial w_{i}^{(0)}}\\ell_{k}^{(0)}\\right)}{\\sum_{i}\\prod_{k}\\left(1-z\\,\\mathrm{sign}(\\Delta w_{i})\\frac{\\partial}{\\partial w_{i}^{(0)}}\\ell_{k}^{(0)}\\right)}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3.3 The posterior update is (almost) stochastic gradient descent ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We will now link this posterior expression with a neural network update rule. Recall that in Theorem 2, the normalized magnitude of each perturbation is interpreted as the posterior probability of the corresponding neural tangent expert. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{|\\Delta w_{i}|}{z}\\gets p(f_{i}|D)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This means the parameters can act as a running cache of the posterior as new data is encountered. As in standard belief updating, this involves a likelihood update followed by renormalization. Surprisingly, this multiplicative belief update rule yields an update which is very close to SGD. ", "page_idx": 4}, {"type": "text", "text": "Lemma 4. For any network that is well-described as a first-order Taylor expansion around around $W^{(0)}$ with perturbation $\\|\\Delta W\\|_{1}=z$ , the posterior belief update given a new example is equivalent to single-example stochastic gradient descent under a cross-entropy loss objective, subject to the constraint that $\\|\\Delta W\\|_{1}=z$ , and using a per-parameter learning rate of $z|\\Delta w_{i}|$ . ", "page_idx": 4}, {"type": "text", "text": "Proof. The proof is a matter of writing out how the posterior changes with a single example. Multiplying by the likelihood of a new example, the unnormalized posterior updates as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{|\\Delta w_{i}^{\\prime(t)}|}{z}=\\frac{|\\Delta w_{i}^{(t-1)}|}{z}\\left(1-z\\,\\mathrm{sign}(\\Delta w_{i}^{(t-1)})\\frac{\\partial}{\\partial w_{i}^{(0)}}\\ell_{k}^{(0)}\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This multiplicative update for the unnormalized weights can also be written an additive rule. Multiplying by $z$ and by $\\mathrm{sign}(\\Delta w_{i})$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Delta w_{i}^{\\prime(t)}=\\Delta w_{i}^{(t-1)}-z|\\Delta w_{i}^{(t-1)}|\\frac{\\partial}{\\partial w_{i}^{(0)}}\\ell_{k}^{(0)}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "One can add the initial parameters to either side to yield a rule in the space of network parameters: ", "page_idx": 5}, {"type": "equation", "text": "$$\nw_{i}^{\\prime(t)}=w_{i}^{(t-1)}-z|\\Delta w_{i}^{(t-1)}|\\frac{\\partial}{\\partial w_{i}^{(0)}}\\ell_{k}^{(0)}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This is true (single-example) stochastic gradient descent projected in the L1 diamond with a learning rate $z|\\Delta w_{i}|$ . Note that this does not allow averaging gradients across examples (a \u201cbatch size of $1^{\\circ}$ update) and that it uses the gradients at initialization (though see section 4.1). ", "page_idx": 5}, {"type": "text", "text": "To complete the update, the parameters should then be renormalized such that $\\begin{array}{r}{\\sum_{i}|\\Delta w_{i}^{(t)}|=z}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "An alternative normalization scheme is to use a gradient projection algorithm. Adding a Langrage multiplier $\\gamma$ to Eq. 13 and solving for the $\\gamma$ that ensures $\\bar{\\sum_{i}}\\,\\bar{|}\\Delta w_{i}|=z$ yields a update which keeps $\\|\\Delta W\\|_{1}=z$ even without renormalization: ", "page_idx": 5}, {"type": "equation", "text": "$$\nw_{i}^{(t)}=w_{i}^{(t-1)}-z\\left(|\\Delta w_{i}^{(t-1)}|\\frac{\\partial}{\\partial w_{i}^{(0)}}\\ell_{k}^{(0)}-\\mathrm{avg}_{j}\\left(|\\Delta w_{j}^{(t-1)}|\\frac{\\partial}{\\partial w_{j}^{(0)}}\\ell_{k}^{(0)}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Not only is the posterior update tractable, then, but it is sufficiently close to gradient descent that it can be interpreted in a standard optimization framework. ", "page_idx": 5}, {"type": "text", "text": "Although it may seem that our result would depend on the idiosyncratic likelihood function of the NTE, this result is nevertheless similar to previous algorithms that have been proposed as ways to weigh many experts. At high level, our result appears similar to the Multiplicative Weights algorithm described in [1]. Another interpretation of this algorithm is as the approximated exponential gradient descent with positive and negative weights algorithm from [23] but applied to the change in weights $\\Delta W$ . There, it is derived by minimizing an arbitrary loss function under a constrained change in the relative entropy over ensemble weights to obtain the exponentiated gradient descent algorithm, which is then linearized with a Taylor expansion in the approximated version. ", "page_idx": 5}, {"type": "text", "text": "3.4 Summary of the NTE theory ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The Neural Tangent Ensemble is an interpretation of networks as ensembles of neural tangent experts. Updating the NTE of lazy networks as a Bayesian ensemble creates a perfect continual learner, in the sense that the multitask solution is guaranteed to be the same as the sequential task solution. ", "page_idx": 5}, {"type": "text", "text": "The posterior probability of each expert in the NTE is surprisingly tractable. Given a new example, the update rule is a simple additive rule in the space of network parameters which can be interpreted as projected gradient descent scaled by the change in parameters since initialization. ", "page_idx": 5}, {"type": "text", "text": "4 Networks away from the lazy regime ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In real finite-width networks, gradients change throughout learning. Since each weight\u2019s corresponding neural tangent expert changes, there is no guarantee that weights at time $t$ still reflect the cumulative likelihood of past data under that expert. ", "page_idx": 5}, {"type": "text", "text": "This phenomenon is clearly observed empirically by measuring how much experts change under the NTE update rule as a function of network width. In Fig. 2, we measure the average change in expert\u2019s Jacobian from initialization after training on MNIST as a function of network width with the NTE rule described above. Experts change less in wider networks than in narrow networks. ", "page_idx": 5}, {"type": "image", "img_path": "qOSFiJdVkZ/tmp/3057355dfd39fc306fd9308a3fc8885db64ca9dd7d8ef7473b1b12886f1aa121.jpg", "img_caption": ["Figure 2: The average squared difference between experts\u2019 columns of the Jacobian measured at initialization and the end of training on MNIST with an 2-layer ReLU MLP and the NTE rule. Error bands indicate the standard deviation over 10 random seeds. As the width of the network increases, the average distance decreases, indicating the larger networks remain closer to the original linearization. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Another way this can be measured is by verifying that, in finite-width networks, the posterior update rule using the gradients around initialization does not lead to effective training. In Figure 3, we confirm that as the gradients lose correlation with the gradient at initialization, performance begins to rapidly degrade. This echoes the findings of [7] that linearized CNNs do not learn as effectively as their non-lazy counterparts. Thus, the NTE posterior update rule as written above is only effective when the Jacobian is truly static. ", "page_idx": 6}, {"type": "image", "img_path": "qOSFiJdVkZ/tmp/e57dd778a73ab72acaedd18567067b1099594be699ad3eacf1c243e59c60ef77.jpg", "img_caption": ["4.1 Rich-regime networks are ensembles of adaptive experts ", "Figure 3: a) Gradients of an MLP at time $(t)$ rapidly lose correlation with the gradients at initialization. b) Training a network with the NTE posterior update rule fails when gradients diverge. Hyperparameters are reported in the Appendix. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "To ensure the NTE formulation remains valid, one can allow the seed point of the Taylor expansion (the \u2018initialization\u2019) to change throughout learning. This has an interesting interpretation. Namely, it allows us to view finite-width neural networks as ensembles of changing neural tangent experts. ", "page_idx": 6}, {"type": "text", "text": "Lemma 5. (informal) Let $W^{(t)}$ be the parameters of a (finite-width) neural network. Choose a nearby seed point $\\tilde{W}^{(t)}\\;a s\\;W^{(t)}+\\epsilon,$ , with $\\epsilon$ fixed and $\\|\\epsilon\\|_{2}$ sufficiently small relative to the curvature such that the Jacobians of the log output probabilities of the perturbed and unperturbed networks are identical, $J(\\tilde{W}^{(t)})=J(\\dot{W}^{(t)})$ . The network can then be written as an ensemble of adaptive experts: ", "page_idx": 6}, {"type": "equation", "text": "$$\np(y|x,f_{i}^{(t)})=p(y|x,\\tilde{W}^{(t)})\\left(1+\\|\\epsilon\\|_{1}s i g n(\\epsilon_{i})\\frac{\\partial}{\\partial w_{i}^{(t)}}\\log p(y|x,W^{(t)})\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "If \u03f5 is set as the uniform vector with values $\\epsilon_{i}=\\sqrt{\\eta/N}$ , the learning rate in the posterior update rule reduces to $\\lVert\\epsilon\\rVert_{1}|\\epsilon_{i}|=\\eta$ and we recover stochastic gradient descent with mean-centered gradients and learning rate \u03b7: ", "page_idx": 6}, {"type": "equation", "text": "$$\nw_{i}^{(t+1)}=w_{i}^{(t)}-\\eta\\left(\\frac{\\partial}{\\partial w_{i}^{(t)}}\\ell_{K}^{(t)}-a\\nu g_{j}\\left(\\frac{\\partial}{\\partial w_{j}^{(t)}}\\ell_{K}^{(t)}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Rich-regime learning is thus akin to a particle filter; each expert changes individually, but the prediction is the ensemble vote. ", "page_idx": 6}, {"type": "text", "text": "A interesting feature of this lemma is the equivalence between the rule that improves each expert (gradient descent on $w$ ) and the rule that decides how to weigh the experts in the ensemble (also gradient descent on $w$ ). This need not have been the case. As a result, one can perform belief updating assuming a fixed ensemble and end up improving each expert within it. ", "page_idx": 6}, {"type": "text", "text": "4.2 The NTE rule with current gradients ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Motivated by this result, we evaluated how well the NTE posterior update rule works when the gradients evaluated at initialization ,\u2202w\u2202(0)\u2113(K0) , are replaced with the gradients of the current network $\\frac{\\partial}{\\partial w_{i}^{(t)}}\\ell_{K}^{(t)}$ . These converge in the infinite-width limit. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "To obtain a practical algorithm, we additionally modify the NTE update rule with two hyperparameters that control the learning rate. First, noting that $z$ in Eq. 14 acts as a learning rate, we replace it with a tunable parameter $\\eta$ . Secondly, we introduce a regularization parameter $\\beta$ which keeps the network close to initialization as measured by the relative entropy of the change in parameters (see Appendix 8.2 for derivation). This constrains the amount of information contained in the weights [17]. ", "page_idx": 7}, {"type": "text", "text": "Pseudocode for the resulting algorithm is in the Appendix 1. We also display the result of sweeps over $\\beta$ and $\\eta$ on the Permuted MNIST task in Fig. 7. ", "page_idx": 7}, {"type": "text", "text": "5 Predictions and results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Our findings suggest several ways to reduce forgetting in finite networks. First, networks closer to the lazy regime will better remember old tasks as long as the update rule is sufficiently similar to the NTE posterior update rule. Second, one should be able to reduce forgetting by ablating standard optimization methods like momentum and moving towards the NTE posterior update rule. ", "page_idx": 7}, {"type": "text", "text": "Below, we verify these predictions on the Permuted MNIST task with MLPs and on the taskincremental CIFAR100 with modern CNN architectures. In the Permuted MNIST task, an MLP with 10 output units is tasked with repeatedly classifying MNIST, but in each task the pixels are shuffled with a new static permutation. In task-incremental CIFAR100, a convolutional net with 100 output units sees only 10 classes each task. In the terminology of van de Ven et al, this is a task-incremental task, whereas Permuted MNIST is a domain-incremental task [48]. ", "page_idx": 7}, {"type": "text", "text": "5.1 Momentum causes forgetting ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Momentum is not appropriate in a posterior update framework because it over-counts the likelihood of past data. Furthermore, it is a history-dependent factor. By contrast, posterior update rules are multiplicative and give identical results regardless of the order of data presentation. ", "page_idx": 7}, {"type": "text", "text": "Here, we report that any amount of momentum with SGD is harmful for remembering past tasks. To our knowledge, this has not been noted by previous empirical studies on catastrophic forgetting [13, 36, 35, 2]. As can been seen in Fig. 4, increasing momentum monotonically increases forgetting a first task on Permuted MNIST. Similar trends exist for ResNet18 and ConvNeXtTiny on the CIFAR100 task (see Fig. 8) [30]. Note that the momentum buffers were not reset between tasks; when they are reset, the momentum curve is nonmonotonic (see Fig. 9). Although momentum assists single-task performance, any amount of momentum will lead to forgetting previous knowledge. ", "page_idx": 7}, {"type": "image", "img_path": "qOSFiJdVkZ/tmp/393a147f069159945d0694bca5cb6f0c62547435814e15371ecc3fec1cf8e82e.jpg", "img_caption": ["Figure 4: Effect of momentum in SGD on the Permuted MNIST task for an MLP with 2 layers and 1,000 hidden units. (middle) Test accuracy on the first task at the end of training 5 sequential tasks. (right) Final test accuracy on the first task before seeing the other tasks. Error bars represent standard deviations over seeds. See Appendix for further parameters. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.2 Width improves remembering \u2014 but only for certain optimizers ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "As networks grow wider and (slowly) approach their infinite-width limit, they should remember better if one uses the appropriate posterior update rule over the Neural Tangent Ensemble. ", "page_idx": 8}, {"type": "text", "text": "Previous literature confirms that this is generally the case. In [40], the authors report the benefits of scale are robust across architectures, tasks, and pretraining strategies, although they largely use SGD with momentum $\\beta=0.9$ . In [35], the authors report similar results and investigate other continual learning benchmark algorithms such as EWC ([22]). Forgetting seems to be largely solved by scale. ", "page_idx": 8}, {"type": "text", "text": "The reason for this in our framework differs from the reason cited by both [35, 40], which state that the gradients on different tasks will be more orthogonal in high dimensions, which reduces interference. Our interpretation is somewhat different and instead depends on the Jacobian of the network changes. We place no condition on gradient orthogonality between tasks. If the neural tangent experts are indeed fixed, the NTE update rule will find the multitask solution. ", "page_idx": 8}, {"type": "text", "text": "If this is the case, then wide networks should better remember only if the optimizer can be interpreted as a posterior update. In Fig. 5, we report that Adam\u2019s amnesia is not helped with increasing scale for the Permuted MNIST task. Although this could be for multiple reasons, we argue it stems from a divergence from a valid interpretation as a posterior update. ", "page_idx": 8}, {"type": "text", "text": "5.3 The NTE posterior update rule using current gradients improves with scale ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In Section 4.2, we introduced a modified version of NTE posterior update rule in which the Jacobian at initialization replaced with the current Jacobian. As networks get wider, this algorithm will converges to the proper update rule due to the fact that the network Jacobian does not change in the lazy regime. This predicts that this rule will improve with scale. To test this, we trained an ML on Permuted MNIST and ConvNeXtTiny on task-incremental CIFAR100 with this approximate rule. We find that both single-task and multitask learning are greatly improved with width (Fig. 5 and Fig. 10). We take this as empirical evidence that the proper NTE posterior update (with a static Jacobian) would work well in the infinite-width limit. ", "page_idx": 8}, {"type": "image", "img_path": "qOSFiJdVkZ/tmp/3ed5ddc99b4055a887cf028bcf16455e1352927f64158a6353f0ed52b12b7cbb.jpg", "img_caption": ["Figure 5: Wider networks forget less, unless trained with Adam. See Alg. 1. All networks are 2-layer MLPs with ReLU nonlinearities trained on 5 Permuted MNIST tasks. Loss curves and further parameters in Appendix. Error bars represent standard deviations. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "6 Related work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "There is a long history of interpreting networks as ensembles. Networks with dropout, for example, allow this interpretation [12, 14]. This is also closely related to Mixture of Experts models in classic [18, 20] and recent [43, 54] work. These approaches explicitly encode the experts within the network, and unlike our work do not use a Taylor expansion to establish the ensemble experts. ", "page_idx": 8}, {"type": "text", "text": "The idea of a Bayesian ensemble over networks is also well-studied. Such ensembles can either be assembled empirically through sampling [51, 50], built via a Laplace approximation [32], or optimized [3]. Bayesian posteriors are also common players in theoretical works using methods from statistical physics and PAC-Bayes [44, 27]. Some treatments of infinite-width limits study the ensemble of lazy learners [16]. While similar in spirit, these methods study groups of many networks rather than view a single network as an ensemble. ", "page_idx": 8}, {"type": "text", "text": "Finally, there is related work that uses ideas from ensembles for continual learning. Many of these are in the category of methods that continually learn by training new modules for each task [49, 5, 42, 52, 39, 21]. Most directly related to this current work are papers that take a Bayesian approach and track statistics about the approximated posterior over networks [22, 10, 11, 28, 41, 37]. Many of these works in both categories require task boundaries. Furthermore, by introducing new modules or tracking statistics, these methods require additional memory to prevent forgetting. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6.1 Moving in directions of low curvature to forget less ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our framework justifies the strategy of encouraging parameters to change mostly in directions of low curvature. Such regularization methods are already well-established and proven to reduce forgetting [24, 31, 41]. Although not directly equivalent, this is also similar to Elastic Weight Consolidation, which penalizes by the Fisher Information matrix (an expected second-order derivative of the loglikelihood, rather than the likelihood) [22]. Another proximal method is Synaptic Intelligence, which penalizes parameter changes proportional to their integrated gradients along the path, which in the special case of diagonal, quadratic loss functions, is equivalent the Hessian [53]. Thus, a second interpretation of why these methods work well (and improve with scale [35]) is that they ensure the tangent experts in the NTE do not change much while learning. ", "page_idx": 9}, {"type": "text", "text": "7 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Here, we described how networks in the lazy regime can be seen as ensembles of fixed classifiers. With this perspective, we proposed weighing each expert by its posterior probability to form a Bayesian ensemble, and derived the update rule. This strategy of learning by posterior updating has the benefti that the order of data presentation does not matter \u2013 sequential experience and interleaved experience lead to identical ensembles. ", "page_idx": 9}, {"type": "text", "text": "The posterior update rule to the Tangent Ensemble is surprisingly similar to SGD on the model weights. However, it is interesting to note that this update rule is suboptimal. Posterior probabilities are the optimal ensemble weights only when the experts are independent [47, 34, 38] and wellspecified [33, 6]. This assumption is violated by the use of shared data, as well as the fact that neural network architectures introduce dependencies between gradients. Although this does not affect the equivalence between the interleaved and sequential task performance (i.e. forgetting), this will reduce the performance of networks trained with the NTE posterior update. This suggests avenues for improving SGD. ", "page_idx": 9}, {"type": "text", "text": "This suboptimality could be addressed in multiple ways. In the ensemble literature, there are many strategies to diversify the expert pool [4] such as repulsion [9]. Different experts might also be trained on different data [46], and one might even take a boosting approach [15]. It is very likely that these approaches would yield neural networks that outperform standard networks trained by updating the posterior distribution over tangent experts. ", "page_idx": 9}, {"type": "text", "text": "The ability in interpret single networks as ensembles opens many avenues for future research. These extend beyond continual learning; for example, one might be able to obtain a measure of uncertainty of the network output via the variance of the experts [12]. We are hopeful that this insight will lead to deep learning systems that are better understood as their use expands within society. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The authors thank Peter Koo and Ben Cowley for helpful early conversations and Tony Zador for providing a collaborative research environment. Additionally we would like to thank a grant from Schmidt Futures to CSHL for funding. ", "page_idx": 9}, {"type": "text", "text": "Code availability ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The code for all figures in this paper were written in Jax and are available at https://github.com/ZadorLaboratory/NeuralTangentEnsemble. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Sanjeev Arora, Elad Hazan, and Satyen Kale. The Multiplicative Weights Update Method: a Meta-Algorithm and Applications. Theory of Computing, 8(6):121\u2013164, 2012. Publisher: Theory of Computing.   \n[2] Dylan R. Ashley, Sina Ghiassian, and Richard S. Sutton. Does the Adam Optimizer Exacerbate Catastrophic Forgetting?, June 2021. arXiv:2102.07686.   \n[3] Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural network. In International conference on machine learning, pages 1613\u20131622. PMLR, 2015.   \n[4] Gavin Brown, Jeremy Wyatt, Rachel Harris, and Xin Yao. Diversity creation methods: a survey and categorisation. Information fusion, 6(1):5\u201320, 2005. Publisher: Elsevier.   \n[5] Wuyang Chen, Yanqi Zhou, Nan Du, Yanping Huang, James Laudon, Zhifeng Chen, and Claire Cui. Lifelong language pretraining with distribution-specialized experts. In International Conference on Machine Learning, pages 5383\u20135395. PMLR, 2023.   \n[6] Badr-Eddine Cherief-Abdellatif and Pierre Alquier. MMD-Bayes: Robust Bayesian Estimation via Maximum Mean Discrepancy. In Cheng Zhang, Francisco Ruiz, Thang Bui, Adji Bousso Dieng, and Dawen Liang, editors, Proceedings of The 2nd Symposium on Advances in Approximate Bayesian Inference, volume 118 of Proceedings of Machine Learning Research, pages 1\u201321. PMLR, December 2020.   \n[7] L\u00e9na\u00efc Chizat, Edouard Oyallon, and Francis Bach. On Lazy Training in Differentiable Programming. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d\u2019 Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.   \n[8] Martin W Cripps. Divisible updating. Manuscript, UCL, 2018.   \n[9] Francesco D\u2019 Angelo and Vincent Fortuin. Repulsive Deep Ensembles are Bayesian. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 3451\u20133465. Curran Associates, Inc., 2021.   \n[10] Sayna Ebrahimi, Mohamed Elhoseiny, Trevor Darrell, and Marcus Rohrbach. Uncertaintyguided Continual Learning with Bayesian Neural Networks. In International Conference on Learning Representations, 2020.   \n[11] Sebastian Farquhar and Yarin Gal. A unifying bayesian view of continual learning. arXiv preprint arXiv:1902.06494, 2019.   \n[12] Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In international conference on machine learning, pages 1050\u20131059. PMLR, 2016.   \n[13] Ian J. Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio. An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks, March 2015. arXiv:1312.6211 [cs, stat].   \n[14] Kazuyuki Hara, Daisuke Saitoh, and Hayaru Shouno. Analysis of dropout learning regarded as ensemble learning. In Artificial Neural Networks and Machine Learning\u2013ICANN 2016: 25th International Conference on Artificial Neural Networks, Barcelona, Spain, September 6-9, 2016, Proceedings, Part II 25, pages 72\u201379. Springer, 2016.   \n[15] Trevor Hastie, Saharon Rosset, Ji Zhu, and Hui Zou. Multi-class adaboost. Statistics and its Interface, 2(3):349\u2013360, 2009. Publisher: International Press of Boston.   \n[16] Bobby He, Balaji Lakshminarayanan, and Yee Whye Teh. Bayesian Deep Ensembles via the Neural Tangent Kernel. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 1010\u20131022. Curran Associates, Inc., 2020.   \n[17] Geoffrey E Hinton and Drew Van Camp. Keeping the neural networks simple by minimizing the description length of the weights. In Proceedings of the sixth annual conference on Computational learning theory, pages 5\u201313, 1993.   \n[18] Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of local experts. Neural computation, 3(1):79\u201387, 1991. Publisher: MIT Press.   \n[19] Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural Tangent Kernel: Convergence and Generalization in Neural Networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.   \n[20] Michael I Jordan and Robert A Jacobs. Hierarchical mixtures of experts and the EM algorithm. Neural computation, 6(2):181\u2013214, 1994. Publisher: MIT Press.   \n[21] Haeyong Kang, Rusty John Lloyd Mina, Sultan Rizky Hikmawan Madjid, Jaehong Yoon, Mark Hasegawa-Johnson, Sung Ju Hwang, and Chang D. Yoo. Forget-free Continual Learning with Winning Subnetworks. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 10734\u2013 10750. PMLR, July 2022.   \n[22] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114(13):3521\u20133526, March 2017. Publisher: Proceedings of the National Academy of Sciences.   \n[23] Jyrki Kivinen and Manfred K. Warmuth. Exponentiated Gradient versus Gradient Descent for Linear Predictors. Information and Computation, 132(1):1\u201363, January 1997.   \n[24] Yajing Kong, Liu Liu, Huanhuan Chen, Janusz Kacprzyk, and Dacheng Tao. Overcoming Catastrophic Forgetting in Continual Learning by Exploring Eigenvalues of Hessian Matrix. IEEE Transactions on Neural Networks and Learning Systems, pages 1\u201315, 2023. Conference Name: IEEE Transactions on Neural Networks and Learning Systems.   \n[25] Dhireesha Kudithipudi, Mario Aguilar-Simon, Jonathan Babb, Maxim Bazhenov, Douglas Blackiston, Josh Bongard, Andrew P. Brna, Suraj Chakravarthi Raja, Nick Cheney, Jeff Clune, Anurag Daram, Stefano Fusi, Peter Helfer, Leslie Kay, Nicholas Ketz, Zsolt Kira, Soheil Kolouri, Jeffrey L. Krichmar, Sam Kriegman, Michael Levin, Sandeep Madireddy, Santosh Manicka, Ali Marjaninejad, Bruce McNaughton, Risto Miikkulainen, Zaneta Navratilova, Tej Pandit, Alice Parker, Praveen K. Pilly, Sebastian Risi, Terrence J. Sejnowski, Andrea Soltoggio, Nicholas Soures, Andreas S. Tolias, Dar\u00edo Urbina-Mel\u00e9ndez, Francisco J. Valero-Cuevas, Gido M. van de Ven, Joshua T. Vogelstein, Felix Wang, Ron Weiss, Angel Yanguas-Gil, Xinyun Zou, and Hava Siegelmann. Biological underpinnings for lifelong learning machines. Nature Machine Intelligence, 4(3):196\u2013210, March 2022. Publisher: Nature Publishing Group.   \n[26] Richard Kurle, Botond Cseke, Alexej Klushyn, Patrick van der Smagt, and Stephan G\u00fcnnemann. Continual Learning with Bayesian Neural Networks for Non-Stationary Data. September 2019.   \n[27] Esther Levin, Naftali Tishby, and Sara A Solla. A statistical approach to learning and generalization in layered neural networks. Proceedings of the IEEE, 78(10):1568\u20131574, 1990. Publisher: IEEE.   \n[28] Honglin Li, Payam Barnaghi, Shirin Enshaeifar, and Frieder Ganz. Continual learning using bayesian neural networks. IEEE transactions on neural networks and learning systems, 32(9):4243\u20134252, 2020. Publisher: IEEE.   \n[29] Chaoyue Liu, Libin Zhu, and Misha Belkin. On the linearity of large non-linear models: when and why the tangent kernel is constant. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 15954\u201315964. Curran Associates, Inc., 2020.   \n[30] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A ConvNet for the 2020s. pages 11976\u201311986, 2022.   \n[31] Ekdeep Singh Lubana, Puja Trivedi, Danai Koutra, and Robert Dick. How do Quadratic Regularizers Prevent Catastrophic Forgetting: The Role of Interpolation. In Sarath Chandar, Razvan Pascanu, and Doina Precup, editors, Proceedings of The 1st Conference on Lifelong Learning Agents, volume 199 of Proceedings of Machine Learning Research, pages 819\u2013837. PMLR, August 2022.   \n[32] David JC MacKay. A practical Bayesian framework for backpropagation networks. Neural computation, 4(3):448\u2013472, 1992. Publisher: MIT Press.   \n[33] Andres Masegosa. Learning under Model Misspecification: Applications to Variational and Ensemble methods. In Advances in Neural Information Processing Systems, volume 33, pages 5479\u20135491. Curran Associates, Inc., 2020.   \n[34] Ron Meir and Tong Zhang. Generalization error bounds for Bayesian mixture algorithms. Journal of Machine Learning Research, 4(Oct):839\u2013860, 2003.   \n[35] Seyed Iman Mirzadeh, Arslan Chaudhry, Dong Yin, Huiyi Hu, Razvan Pascanu, Dilan Gorur, and Mehrdad Farajtabar. Wide Neural Networks Forget Less Catastrophically. In Proceedings of the 39th International Conference on Machine Learning, pages 15699\u201315717. PMLR, June 2022. ISSN: 2640-3498.   \n[36] Seyed Iman Mirzadeh, Mehrdad Farajtabar, Razvan Pascanu, and Hassan Ghasemzadeh. Understanding the Role of Training Regimes in Continual Learning. In Advances in Neural Information Processing Systems, volume 33, pages 7308\u20137320. Curran Associates, Inc., 2020.   \n[37] Cuong V. Nguyen, Yingzhen Li, Thang D. Bui, and Richard E. Turner. Variational Continual Learning. In International Conference on Learning Representations, 2018.   \n[38] Luis A. Ortega, Rafael Caba\u00f1as, and Andres Masegosa. Diversity and Generalization in Neural Network Ensembles. In Proceedings of The 25th International Conference on Artificial Intelligence and Statistics, pages 11720\u201311743. PMLR, May 2022. ISSN: 2640-3498.   \n[39] Sam Powers, Eliot Xing, and Abhinav Gupta. Self-activating neural ensembles for continual reinforcement learning. In Conference on Lifelong Learning Agents, pages 683\u2013704. PMLR, 2022.   \n[40] Vinay Venkatesh Ramasesh, Aitor Lewkowycz, and Ethan Dyer. Effect of scale on catastrophic forgetting in neural networks. In International Conference on Learning Representations, 2022.   \n[41] Hippolyt Ritter, Aleksandar Botev, and David Barber. Online Structured Laplace Approximations for Overcoming Catastrophic Forgetting. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.   \n[42] Murray Shanahan, Christos Kaplanis, and Jovana Mitrovi\u00b4c. Encoders and ensembles for task-free continual learning. arXiv preprint arXiv:2105.13327, 2021.   \n[43] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.   \n[44] Samuel L Smith and Quoc V Le. A Bayesian Perspective on Generalization and Stochastic Gradient Descent. In International Conference on Learning Representations, 2018.   \n[45] Volker Tresp and Michiaki Taniguchi. Combining Estimators Using Non-Constant Weighting Functions. In Advances in Neural Information Processing Systems, volume 7. MIT Press, 1994.   \n[46] Alexey Tsymbal, Seppo Puuronen, and David W Patterson. Ensemble feature selection with the simple Bayesian classification. Information fusion, 4(2):87\u2013100, 2003. Publisher: Elsevier.   \n[47] N. Ueda and R. Nakano. Generalization error of ensemble estimators. In Proceedings of International Conference on Neural Networks (ICNN\u201996), volume 1, pages 90\u201395 vol.1, June 1996.   \n[48] Gido M. van de Ven, Tinne Tuytelaars, and Andreas S. Tolias. Three types of incremental learning. Nature Machine Intelligence, 4(12):1185\u20131197, December 2022. Publisher: Nature Publishing Group.   \n[49] Joshua T. Vogelstein, Jayanta Dey, Hayden S. Helm, Will LeVine, Ronak D. Mehta, Tyler M. Tomita, Haoyin Xu, Ali Geisa, Qingyang Wang, Gido M. van de Ven, Chenyu Gao, Weiwei Yang, Bryan Tower, Jonathan Larson, Christopher M. White, and Carey E. Priebe. Representation Ensembling for Synergistic Lifelong Learning with Quasilinear Complexity, February 2024. arXiv:2004.12908 [cs, stat].   \n[50] Max Welling and Yee W Teh. Bayesian learning via stochastic gradient Langevin dynamics. In Proceedings of the 28th international conference on machine learning (ICML-11), pages 681\u2013688. Citeseer, 2011.   \n[51] Florian Wenzel, Kevin Roth, Bastiaan S. Veeling, Jakub \u00b4Swi a\u02dbtkowski, Linh Tran, Stephan Mandt, Jasper Snoek, Tim Salimans, Rodolphe Jenatton, and Sebastian Nowozin. How Good is the Bayes Posterior in Deep Neural Networks Really?, July 2020. arXiv:2002.02405 [cs, stat].   \n[52] Mateusz Andrzej W\u00f3jcik, Witold Kos\u00b4ciukiewicz, Adam Gonczarek, and Tomasz Jan Kajdanowicz. Neural Architecture for Online Ensemble Continual Learning. In Sixth Workshop on Meta-Learning at the Conference on Neural Information Processing Systems, 2022.   \n[53] Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual Learning Through Synaptic Intelligence. Proceedings of machine learning research, 70:3987\u20133995, 2017.   \n[54] Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M Dai, Quoc V Le, James Laudon, and others. Mixture-of-experts with expert choice routing. Advances in Neural Information Processing Systems, 35:7103\u20137114, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "8 Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "8.1 Proof of Theorem 2 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof. We begin by noting that the change in weights we can be split up the sign and magnitude, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Delta w_{i}=|\\Delta w_{i}|\\mathrm{sign}(\\Delta w_{i}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We will then interpret $|\\Delta w_{i}|$ as the unnormalized component weight. The remaining terms must be the component functions. ", "page_idx": 14}, {"type": "text", "text": "To identify these functions, and show that they satisfy the properties of a probability distribution, we will rearrange terms. First, noting that $\\begin{array}{r}{\\sum_{i}|\\dot{\\Delta w_{i}}|=\\dot{z}}\\end{array}$ for some constant $z$ (potentially $z=1$ ), ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\langle y|x,W^{(t)}\\rangle=p(y|x,W^{(0)})+\\sum_{i}^{N}|\\Delta w_{i}|\\mathrm{sign}(\\Delta w_{i})\\frac{\\partial}{\\partial w_{i}^{(0)}}p(y|x,W^{(0)})+\\mathcal{O}(\\|\\Delta W\\|^{2})\\qquad\\mathrm{(17)}}\\\\ {\\displaystyle=\\sum_{i}^{N}\\frac{|\\Delta w_{i}|}{z}\\underbrace{\\left(p(y|x,W^{(0)})+z\\,\\mathrm{sign}(\\Delta w_{i})\\frac{\\partial}{\\partial w_{i}^{(0)}}p(y|x,W^{(0)})\\right)}_{p(y|x,f_{i})}+\\mathcal{O}(\\|\\Delta W\\|^{2})}\\\\ {\\displaystyle=\\sum_{i}^{N}\\frac{|\\Delta w_{i}|}{z}p(y|x,W^{(0)})\\left(1+z\\,\\mathrm{sign}(\\Delta w_{i})\\frac{\\partial}{\\partial w_{i}^{(0)}}\\log p(y|x,W^{(0)})\\right)+\\mathcal{O}(\\|\\Delta W\\|^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We call the term $p(y|x,f_{i})$ the neural tangent expert. ", "page_idx": 14}, {"type": "text", "text": "The neural tangent expert provides a valid probability distribution for small $\\Delta W$ . First, see that it satisfies $\\begin{array}{r}{\\sum_{j}p(y_{j}|x,f_{i})=1}\\end{array}$ . This can be seen from the fact that the right term inside $p(y|x,f_{i})$ (the parentheses in the middle line) sums to 0 over the output label: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{j}p(y_{j}|x,f_{i})=\\sum_{j}z\\,\\mathrm{sign}(\\Delta w_{i})\\frac{\\partial p(y_{j}|x,W^{(0)})}{\\partial w_{i}^{(0)}}}}\\\\ &{}&{=z\\,\\mathrm{sign}(\\Delta w_{i})\\sum_{j}p(y_{j}|x,W^{(0)})\\frac{\\partial\\log p(y_{j}|x,W^{(0)})}{\\partial w_{i}^{(0)}}}\\\\ &{}&{=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Here we have used the identity that the expectation of a score function is zero. ", "page_idx": 14}, {"type": "text", "text": "Next, we will show that $1\\,\\geq\\,p(y_{j}|x,f_{i})\\,\\geq\\,0$ . First, since each $p(y_{j}|x,f_{i})$ sum to 1 over $j$ , no component can be greater than 1 if all components are nonnegative. Thus, it only needs to be shown that $p(y|x,f_{i})\\ge0$ . While this cannot be guaranteed in general, by construction we have assumed that $z L<1$ . This Lipschitz continuity bounds the L2 norm of the gradients of the log likelihood, which in turn bounds the L1 norm and implies that any individual gradient has magnitude less than $\\frac{1}{z}$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\nz\\,\\left|\\frac{\\partial\\log p(y_{j}|x,W^{(0)})}{\\partial w_{i}^{(0)}}\\right|<1\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus, whether $\\mathrm{sign}(\\Delta w_{i})=1$ or $\\mathrm{sign}(\\Delta w_{i})=-1$ , the term in parenthesis is nonnegative. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left(1+z\\,\\mathrm{sign}(\\Delta w_{i})\\frac{\\partial}{\\partial w_{i}^{(0)}}\\log p(y|x,W^{(0)})\\right)>0.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "8.2 Preventing component functions from changing by keeping the network close to initialization ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The continual learning ability of a Bayesian ensemble derives from learning to weight a set of fixed functions. If these functions change over time, then there is no guarantee that the likelihood of each function at time $t$ still reflects the cumulative likelihood of past data under the current function. ", "page_idx": 15}, {"type": "text", "text": "One good way to ensure this is does not occur is to ensure that the parameters change as little as possible from initialization. Although it is typical to measure this distance with $\\|\\Delta W\\|_{2}$ , we instead measure distance as the relative entropy of the change in parameters from the uniform perturbation, due to the simplicity of its result. These have the same minimum; remembering that $\\|\\Delta\\bar{W}\\|_{1}=1$ , by normalization, the smallest Euclidean distance $\\lVert\\Delta W\\rVert_{2}$ will occur when all $\\Delta w_{i}$ are equal. ", "page_idx": 15}, {"type": "text", "text": "To derive the maximum-entropy vector $|\\Delta W|$ that is as similar as possible to $p(f_{i}|\\mathcal{D})$ , we will follow the steps of [23]. A first step is to set the notion of similarity $L$ between $|\\Delta W|$ and $\\dot{p}(f_{i}|\\mathcal{D})$ . We will then find the value that minimizes: ", "page_idx": 15}, {"type": "equation", "text": "$$\nU(|\\Delta W|)=-H[|\\Delta W|]+\\beta L(|\\Delta W|,\\{p(f_{i}|\\mathcal{D})\\})+\\gamma(\\|\\Delta W\\|_{1}-1)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Here $\\beta$ is a parameter that trades off between entropy and matching $p(f_{i}|\\mathcal{D})$ , and $\\gamma$ is a Langrange multiplier that ensures the parameters remain normalized. ", "page_idx": 15}, {"type": "text", "text": "If one chooses to maximize the dot product $|\\Delta W|^{T}p(f_{i}|\\mathcal{D})$ , one obtains the following relation: ", "page_idx": 15}, {"type": "equation", "text": "$$\nw_{i}=\\frac{e^{\\beta\\,p(f_{i}|\\mathcal{D})}}{\\sum_{i}e^{\\beta\\,p(f_{i}|\\mathcal{D})}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Alternatively, if one chooses to minimize the relative entropy $\\mathrm{KL}(|\\Delta W|,p(f_{i}|D))$ , then one obtains ", "page_idx": 15}, {"type": "equation", "text": "$$\nw_{i}=\\frac{p(f_{i}|\\mathcal{D})^{\\beta}}{\\sum_{i}p(f_{i}|\\mathcal{D})^{\\beta}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We implement this second term. If the posterior likelihoods are maintained in log space, $\\beta$ acts as a multiplicative scale upon the log data likelihood. ", "page_idx": 15}, {"type": "table", "img_path": "qOSFiJdVkZ/tmp/974173aca663b73d3e7145775ca16001f40dcdda6b45b04ae34e84afe480c2f0.jpg", "table_caption": ["8.3 Pseudocode for the NTE update rule using current gradients "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "8.4 Experiment details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "All MNIST experiments were completed on two NVIDIA RTX 6000 cards, and all CIFAR100 experiments were conducted on NVIDIA H100 cards. Over 1,500 individual models were trained across all seeds and conditions, amounting to roughly 440 GPU-hours of compute time. ", "page_idx": 15}, {"type": "text", "text": "8.4.1 Figure 2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A single MLP was trained with 1,000 hidden units per layer and 2 hidden layers using ReLU nonlinearities. The model perturbed from initialization with a random normal vector with scale 0.001, and then was trained with the NTE update rule (Algorithm 1) but using the Jacobian of the model at initialization. The batch size was 24 and the parameters of the NTE algorithm were $\\eta=0.01$ and $\\beta=1$ . ", "page_idx": 16}, {"type": "text", "text": "8.4.2 Figure 3 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We first created a Permuted MNIST task and code to measure the test accuracy on all tasks after training on each task sequentially. All reported results use 5 tasks. ", "page_idx": 16}, {"type": "text", "text": "We trained an MLP on this task with ReLU nonlinearities and 1,000 hidden units in 2 hidden layers. We used SGD with batch size 128, learning rate 0.01, and momentum swept from 0 to 1. The momentum buffer was not reset between tasks. We report the standard deviation of 10 random seeds. ", "page_idx": 16}, {"type": "image", "img_path": "qOSFiJdVkZ/tmp/1b1fc3a5c1258d6c56e40cfd475ee307ca46377a99a50b54e43276378f172af6.jpg", "img_caption": ["Figure 6: Loss curves for the task in Fig. 4. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "8.4.3 Figure 4 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Here, we used the same continual learning task as Figure 3, but swept the width of the two hidden layers from 10 to 10,000. All batch sizes were 128. ", "page_idx": 16}, {"type": "image", "img_path": "qOSFiJdVkZ/tmp/029a34c59de88fa87bebb72f02c42aa163887963ebafaee5e134c8de15d33c3a.jpg", "img_caption": ["Figure 7: For the same task and architecture as the other figures (Permuted MNIST for 5 tasks with a ReLU MLP with two hidden layers and 1,000 hidden units each), we swept the parameters $\\beta$ and $\\eta$ in the Algorithm above. The accuracies (top) and losses (bottom) are shown for the first task after 5 total tasks (left), after immediately finishing that task before task switching (middle) and the difference between the two (right). Error bars show the standard deviation across seeds. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "qOSFiJdVkZ/tmp/54fe47340cd8cf37c58175f0d54f897f1b3070a9f5d66f9d77e6fb75ad0d7e03.jpg", "img_caption": ["Figure 8: Effect of momentum in SGD for modern CNN architectures trained on the CIFAR-100 task-incremental task. In this task, models are trained on 10/100 classes at a time, and the softmax output layer is masked to only the active classes. Each model is trained for 100 epochs per task, and evaluated on all previous tasks. The two models shown are a ResNet18 and a ConvNeXtTiny. (left) The test set accuracy on the immediately previous task after learning the final task. (middle). The test set accuracy on the first task. (right) The difference between the two plots to the left. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "qOSFiJdVkZ/tmp/6c2bf473a673d18c2f52215ffebbefc88215b9a8c5f8410b51bcd0c5ec7f614a.jpg", "img_caption": ["Figure 9: Identically to Fig. 4, we trained a 2L MLP with 1,000 hidden units on the Permuted MNIST task and varied the momentum of SGD. This time, we reset the momemtum buffer between tasks. Interestingly, this introduces a nonmonotonic behavior and one can attain good performance with momentum near 0.99. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "qOSFiJdVkZ/tmp/a20cbdf1a05893273fd4a07a3261bbca2cd5b8a3ba8e7dccc4e3c5b447e299de.jpg", "img_caption": ["Figure 10: Performance with width for CIFAR-100. We scaled the number of convolutional filters in all layers of a ConvNeXtTiny by a constant factor, and then trained on the CIFAR-100 taskincremental task for each network. Subpanels represent identical information as Fig. 8. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Claims ", "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The abstract and introduction accurately describe the contents of the paper. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper. \u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. \u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. ", "page_idx": 18}, {"type": "text", "text": "\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The main limitations of this work are 1) Theorem 1 is valid only in the limit of small parameter changes, and with the assumption of low network curvature. This assumption is stated clearly in the Theorem statement. Then, in the discussion, we mention that weighing experts by their posterior probability is only optimal when the experts are independent, which is not the case. We do not claim this is the optimal weighing strategy. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: All assumptions are listed upfront, and the proofs are complete. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The experiment details are listed in full. Code is also provided that implements all models and optimizers. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: Code is provided in supplementary information. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: All experimental details are provided. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: Error bars where present are clearly described as representing the standard deviation over seeds. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: In the Appendix, we describe the type of GPU used and approximate number of GPU hours used for experiments. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: All authors have reviewed and agreed to the Code of Ethics, and affirm that this submission adheres. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: Although this theoretical work has little direct impact, we mention in the discussion the impacts of continual learning devices upon society. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. ", "page_idx": 22}, {"type": "text", "text": "\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: This model describes no data or models that have a high risk of misuse. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: All re-used code and libraries are used according to their terms of use. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: No new assets are released. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not release new assets. ", "page_idx": 23}, {"type": "text", "text": "\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: No crowdsourcing or research with human subjects is reported here. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This paper does not involve human subjects. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]