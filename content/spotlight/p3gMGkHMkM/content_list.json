[{"type": "text", "text": "Particle Semi-Implicit Variational Inference ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jen Ning Lim ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Adam M. Johansen ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "University of Warwick Coventry, United Kingdom Jen-Ning.Lim@warwick.ac.uk ", "page_idx": 0}, {"type": "text", "text": "University of Warwick Coventry, United Kingdom a.m.johansen@warwick.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Semi-implicit variational inference (SIVI) enriches the expressiveness of variational families by utilizing a kernel and a mixing distribution to hierarchically define the variational distribution. Existing SIVI methods parameterize the mixing distribution using implicit distributions, leading to intractable variational densities. As a result, directly maximizing the evidence lower bound (ELBO) is not possible, so they resort to one of the following: optimizing bounds on the ELBO, employing costly inner-loop Markov chain Monte Carlo runs, or solving minimax objectives. In this paper, we propose a novel method for SIVI called Particle Variational Inference (PVI) which employs empirical measures to approximate the optimal mixing distributions characterized as the minimizer of a free energy functional. PVI arises naturally as a particle approximation of a Euclidean\u2013Wasserstein gradient flow and, unlike prior works, it directly optimizes the ELBO whilst making no parametric assumption about the mixing distribution. Our empirical results demonstrate that PVI performs favourably compared to other SIVI methods across various tasks. Moreover, we provide a theoretical analysis of the behaviour of the gradient flow of a related free energy functional: establishing the existence and uniqueness of solutions as well as propagation of chaos results. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In Bayesian inference, a quantity of vital importance is the posterior $p(x|y)=p(x,y)/\\int p(x,y)\\mathrm{d}x$ , where $p(x,y)$ is a probabilistic model; $y$ denotes the observed data, and $x$ the latent variable. An ever-present issue in Bayesian inference is that the posterior is often intractable. This is because the normalizing constant is available only in the form of an integral, and approximation methods are required. One popular method is variational inference (VI) (Jordan, 1999; Wainwright et al., 2008; Blei et al., 2017). The essence of VI is to approximate the posterior with a member from a variational family $\\mathcal{Q}$ where each element of $\\mathcal{Q}$ is a distribution $q_{\\theta}$ (called \u201cvariational distribution\u201d) parameterized by $\\theta$ . These parameters $\\theta$ are obtained via minimizing a distance or discrepancy (or an approximation of it) between the posterior $p(\\cdot|y)$ and the variational distribution $q_{\\theta}$ . ", "page_idx": 0}, {"type": "text", "text": "Here, we focus on semi-implicit variational inference (SIVI) (Yin and Zhou, 2018). It enables a rich variational family by utilizing variational distributions, which we refer to as semi-implicit distributions (SIDs), defined as ", "page_idx": 0}, {"type": "equation", "text": "$$\nq_{k,r}(x):=\\int k(x|z)r(z)\\,\\mathrm{d}z,\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $k:\\mathbb{R}^{d_{x}}\\,\\times\\mathbb{R}^{d_{z}}\\,\\to\\,\\mathbb{R}_{+}$ is a kernel satisfying $\\begin{array}{r}{\\int k(x|z)\\mathrm{d}x\\,=\\,1;\\,r\\,\\in\\,\\mathcal{P}(\\mathbb{R}^{d_{z}})}\\end{array}$ is the mixing distribution and $\\mathcal{P}(\\mathbb{R}^{d_{z}})$ denotes the space of distributions with support $\\mathbb{R}^{d_{z}}$ , with its usual Borel $\\sigma$ -field, with finite second moments: $\\mathbb{E}_{r}[\\|z\\|^{2}]<\\infty$ . Here, and throughout, we assume that the distributions and kernels of interest admit densities. SIDs are very flexible (Yin and Zhou, 2018) and can express complex properties, such as skewness, multimodality, and kurtosis. These properties might be present in the posterior but typical variational families may fail to capture them. There are various approaches to parameterizing these variational distributions: current techniques utilize neural networks built on top of existing kernels (e.g., Gaussian kernels) to define more complex kernels (Titsias and Ruiz, 2019), and/or utilize pushforward distributions (a.k.a., implicit distributions (Husza\u00b4r, 2017)) (Yin and Zhou, 2018). On choosing a parameterization, an approximation to the posterior is obtained by minimizing the exclusive Kullback-Leibler (KL) divergence. This optimization has the same solution as minimizing the free energy (or the negative evidence lower bound) defined as ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathcal{E}(k,r):=\\int\\log\\frac{q_{k,r}(x)}{p(x,y)}q_{k,r}(\\mathrm{d}x).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "However, since the integral in $q_{k,r}$ is typically intractable, directly optimizing $\\mathcal{E}$ is not feasible. As a result, SIVI algorithms focus on designing tractable objectives by using upper bounds of $\\mathcal{E}$ (Yin and Zhou, 2018); expensive Markov Chain Monte Carlo (MCMC) chains to estimate the gradient of $\\mathcal{E}$ (Titsias and Ruiz, 2019); and optimizing different objectives such as score matching which results in min-max objectives (Yu and Zhang, 2023). ", "page_idx": 1}, {"type": "text", "text": "In our work, we propose an alternative parameterization for SIDs: kernels are constructed as before (with parameter space denoted by $\\Theta$ ) whereas the mixing distribution $r$ is obtained by optimizing over the whole space $\\mathcal{P}(\\mathbb{R}^{d_{z}})$ . We motivate the case for minimizing a regularized version of the free energy $\\mathcal{E}$ denoted by $\\mathcal{E}_{\\lambda}$ (see Eq. (4)); thus, SIVI can be posed as the following optimization problem: arg $\\operatorname*{min}_{(\\theta,r)\\in\\Theta\\times\\mathcal{P}(\\mathbb{R}^{d_{z}})}\\mathcal{E}_{\\lambda}(\\theta,r)$ . As a means to solving the SIVI problem, we construct a gradient flow that minimizes $\\mathcal{E}_{\\lambda}$ where the space $\\Theta\\times\\mathcal{P}(\\mathbb{R}^{d_{z}})$ is equipped with the Euclidean\u2013Wasserstein geometry (Jordan et al., 1998; Kuntz et al., 2023). Via discretization, we obtain a practical algorithm for SIVI called Particle Variational Inference (PVI) that does not rely upon upper bounds of $\\mathcal{E}$ , MCMC chains, or solving minimax objectives. ", "page_idx": 1}, {"type": "text", "text": "Our main contributions are as follows: (1) we introduce a Euclidean\u2013Wasserstein gradient flow minimizing $\\mathcal{E}_{\\lambda}$ as means to perform SIVI; (2) we develop a practical algorithm, PVI, which arises as a discretization of the gradient flow that allows for general mixing distributions; (3) we empirically compare PVI compared with other SIVI approaches across toy and real-world experiments and find that it compares favourably; (4) we study the behaviour of the gradient flow of a related free energy functional to establish existence and uniqueness of solutions (Prop. 8) as well as propagation of chaos results (Prop. 9). ", "page_idx": 1}, {"type": "text", "text": "The structure of this paper is as follows: in Section 2, we begin with a discussion of previous approaches to parameterizing SIDs and their relationship with one another. Then, in Section 3, we show how PVI is developed: beginning with designing a well-defined loss functional, the construction of the gradient flow, and how to obtain a practical algorithm. In Section 4, we study properties of a related gradient flow; and, in Section 5, we conclude with experiments to demonstrate the efficacy of our proposal. For sake of brevity, we defer our discussion of related works to App. A. ", "page_idx": 1}, {"type": "text", "text": "2 On implicit mixing distributions in SID ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "This section outlines existing approaches to parameterizing SIDs with implicit distributions and how these choices affect the resulting variational family. Before we begin, we shall summarize the key assumptions of SIVI. The kernel $k$ is assumed to be a reparametrized distribution in the sense of Salimans and Knowles (2013); Kingma and Welling (2014); Ruiz et al. (2016). In other words, the kernel $k$ is defined by the pair $\\left(\\phi,p_{k}\\right)$ where $\\phi:\\bar{\\mathbb{R}}^{d_{z}}\\times\\mathbb{R}^{d_{x}}\\rightarrow\\mathbb{R}^{d_{x}}$ and $p_{k}\\,\\in\\,\\mathcal{P}(\\mathbb{R}^{d_{x}})$ such that $k(\\cdot|z)=\\phi(z,\\cdot)_{\\#}p_{k}$ Furthermore, to ensure that it admits a tractable density, the map $\\dot{\\epsilon}\\mapsto\\phi(z,\\epsilon)$ is assumed to be a diffeomorphism for all $z\\in\\mathbb{R}^{d_{z}}$ with its inverse map written as $\\phi^{-1}(z,\\cdot)$ . From the change-of-variable formula, its density is given as $k(\\cdot|z)=p_{k}(\\phi^{-1}(\\bar{z},\\cdot))\\,|\\operatorname*{det}\\nabla_{x}\\phi^{-1}(z,\\cdot)|$ . We sometimes write $k_{\\phi,p_{k}}$ to denote the underlying $\\phi$ and $p_{k}$ explicitly. Furthermore, the kernel $k$ is assumed to be computable and differentiable with respect to both arguments. ", "page_idx": 1}, {"type": "text", "text": "Several approaches to the parameterization of SID have been explored in the literature. One can define the variational family by choosing the kernel and mixing distribution from sets $\\kappa$ and $\\mathcal{R}$ respectively, i.e, the variational family is $\\mathcal{Q}(\\mathcal{K},\\mathcal{R})\\;:=\\;\\{q_{k,r}\\,:\\,\\stackrel{\\cdot}{k}\\,\\in\\,\\mathcal{K},r\\,\\in\\,\\mathcal{R}\\}$ . Yin and Zhou (2018) focused on a fixed kernel $k$ with $r$ being a pushforward (or \u201cimplicit\u201d) distribution, i.e., $r\\in\\{g_{\\#}p_{r}:g\\in\\mathcal{G}\\}=:\\mathcal{R}_{\\mathcal{G};p_{r}}$ where $\\mathcal{G}$ is a subset of measurable mappings from the sample space of $p_{r}$ to $\\mathbb{R}^{d_{z}}$ . Thus, the $\\mathcal{Q}_{\\mathrm{YiZ}}$ -variational family is $\\mathcal{Q}(\\{k\\},\\mathcal{R}_{\\mathcal{G};p_{r}})$ . On the other hand, Titsias and Ruiz (2019) considered a fixed mixing distribution $r$ with $k$ belonging to some parameterized class $\\kappa$ . The typical example is one in which each kernel is defined by composing an existing kernel $k_{\\phi,p_{k}}$ with a function $f\\in\\mathcal F$ , the result is $k_{f;\\phi,p_{k}}(\\cdot|z):=k_{\\phi(f(\\cdot),\\cdot),p_{k}}\\bar{(}\\cdot|z)=\\phi(f\\bar{(}z),\\cdot)_{\\#}p_{k}$ which clearly satisfies the reparameterization assumption. We denoted this kernel class as $K_{\\mathcal{F};\\phi,p_{k}}:=\\{k_{f;\\phi,p_{k}}:f\\in\\mathcal{F}\\}$ and its respective $\\mathcal{Q}_{\\tt T R}$ -variational family is ${\\mathcal{Q}}(K_{{\\mathcal{G}};\\phi,p_{k}},\\{r\\})$ . In $\\mathrm{Yu}$ and Zhang (2023), they combine both parameterization for $\\kappa$ and $\\mathcal{R}$ , i.e., the $\\mathcal{Q}_{\\mathrm{YuZ}}$ -variational family is $\\mathcal{Q}(\\mathcal{K}_{\\mathcal{F};\\phi,p_{k}},\\mathcal{R}_{\\mathcal{G};p_{r}})$ . We note that this is how the variational family is presented in $\\mathrm{Yu}$ and Zhang (2023, see Sec. 2) but the authors used $\\mathcal{Q}_{\\mathtt{T R}}$ -variational family in experiments, i.e., $r$ was fixed. While $\\mathcal{Q}_{\\mathtt{Y u Z}}$ might seem like it defines a larger variational family than the other approaches, under these common parameterization practices, we show that they define the same variational family. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Proposition 1 $(\\mathcal{Q}_{\\mathtt{Y u Z}}\\;=\\;\\mathcal{Q}_{\\mathtt{Y i Z}}\\;=\\;\\mathcal{Q}_{\\mathtt{T R}})$ . Given a $\\mathcal{Q}_{\\mathtt{Y u Z}}$ -variational family of the form $\\mathcal{Q}_{\\mathtt{Y u Z}}\\ :=$ $\\mathcal{Q}(K_{\\mathcal{F};\\phi,p_{k}},\\mathcal{R}_{\\mathcal{G};p_{r}}),$ , then there is a $\\mathcal{Q}_{\\mathtt{Y i Z}}$ -variational family and $\\mathcal{Q}_{\\mathtt{T R}}$ -variational family (i.e., $\\mathcal{Q}_{\\mathtt{T R}}:=$ $\\mathcal{Q}(K_{\\mathcal{F}\\circ\\mathcal{G};\\phi,p_{k}},\\left\\{p_{r}\\right\\})$ and $\\mathcal{Q}_{\\mathtt{Y i Z}}:=\\mathcal{Q}(\\{k_{\\phi,p_{k}}\\},\\mathcal{R}_{\\mathcal{F}\\circ\\mathcal{G};p_{r}});$ such that $\\mathcal{Q}_{\\mathtt{Y u Z}}=\\mathcal{Q}_{\\mathtt{Y i Z}}=\\mathcal{Q}_{\\mathtt{T R}}$ . ", "page_idx": 2}, {"type": "text", "text": "The proof can be found in App. D. This proposition shows that $\\mathcal{Q}_{\\mathrm{YuZ}}$ -parameterization defines the \u201csame\u201d variational family as $\\mathcal{Q}_{\\mathtt{Y i Z}}$ and $\\mathcal{Q}_{\\mathtt{T R}}$ when we parametrize $\\mathcal{R}$ with push-forward distributions. In practice, $\\mathcal{F}$ and $\\mathcal{G}$ are parameterized by neural networks hence $\\mathcal{Q}_{\\mathtt{Y u Z}}$ can be viewed as $\\mathcal{Q}_{\\mathrm{YiZ}}$ or $\\mathcal{Q}_{\\mathtt{T R}}$ with a deeper neural networks $\\mathcal{F}\\circ\\mathcal{G}$ . This simplification is a direct result of using push-forward distributions. Although this parametrization has shown promise e.g., Goodfellow et al. (2020)), they have issues with expressivity particularly when distributions are disconnected (Salmona et al., 2022). In our work, we follow in $\\mathcal{Q}_{\\mathrm{YuZ}}$ -variational families, but, we avoid the use of push-forward distributions. Instead, we propose to directly optimize over $\\mathcal{P}(\\mathbb{R}^{d_{z}})$ and so, our variational family does not simply reduce to $\\mathcal{Q}_{\\mathrm{YiZ}}$ or $\\mathcal{Q}_{\\tt T R}$ . ", "page_idx": 2}, {"type": "text", "text": "3 Particle Variational Inference ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we present our proposed method for SIVI, called particle variational inference (PVI). Similar to prior SIVI methods, the algorithm utilizes kernels (denoted by $k_{\\theta}$ ) with parameters $\\Theta$ which satisfy the assumptions listed in Section 2. One example is $k_{\\theta}\\in\\mathcal{K}_{\\Theta;\\phi,p_{k}}$ where $\\Theta$ is a function space induced by a neural network. We slightly abuse the notation $\\Theta$ to also indicate its corresponding weight space $\\mathbb{R}^{\\dot{d}_{\\theta}}$ . The novelty of this algorithm is that, for the mixing distribution, we directly optimize over the space $\\mathcal{P}(\\mathbb{R}^{d_{z}})$ which loosens the requirement for the neural network in the kernel to learn complex mappings. The result is a \u201csimpler\u201d optimization procedure and increases expressivity over existing methods. Thus, the variational parameters of PVI are $(\\theta,r)\\in\\Theta\\times\\mathcal{P}(\\mathbb{R}^{d_{z}})=\\dot{:}\\mathcal{M}$ with its corresponding variational distribution defined as $\\begin{array}{r}{q_{\\theta,r}:=\\int k_{\\theta}(\\cdot|\\dot{z})r(\\dot{z})\\mathrm{d}z}\\end{array}$ . PVI arises naturally as a discretization of a gradient flow minimizing a suitably defined free energy on $\\Theta\\times\\mathcal{P}(\\mathbb{R}^{d_{z}})$ endowed with the Euclidean\u2013Wasserstein geometry (Jordan et al., 1998; Ambrosio et al., 2005; Kuntz et al., 2023). In Section 3.1, we begin by constructing a suitably defined free energy functional; then, in Section 3.2, we formulate its gradient flow; finally, in Section 3.3, we construct PVI from its gradient flow. ", "page_idx": 2}, {"type": "text", "text": "3.1 Free energy functional ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "As with other VI algorithms, we are interested in finding variational parameters that minimize $(\\theta,r)\\mapsto\\mathsf{K L}(q_{\\theta,r},p(\\cdot|y))$ . This optimization problem can be cast equivalently as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{arg\\,min}_{(\\theta,r)\\in\\mathcal{M}}\\mathcal{E}(\\theta,r),\\quad\\mathrm{where}~\\,\\mathcal{E}:\\mathcal{M}\\rightarrow\\mathbb{R}:(\\theta,r)\\mapsto\\int q_{\\theta,r}(x)\\log\\frac{q_{\\theta,r}(x)}{p(x,y)}\\,\\mathrm{d}x.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Before we can solve this problem, we must ensure that it is well-posed. In other words, it must admit minimizers in $\\mathcal{M}$ . In the following proposition, we outline various properties of $\\mathcal{E}$ : ", "page_idx": 2}, {"type": "text", "text": "Proposition 2. Assume that the evidence is bounded $\\log p(y)<\\infty$ and $k$ is bounded; then we have that $\\mathcal{E}$ is $(i)$ lower bounded, (ii) lower semi-continuous (l.s.c.), and $(i i i)$ non-coercive. ", "page_idx": 2}, {"type": "text", "text": "The proof can be found in App. E.1. Prop. 2 tells us that even though $\\mathcal{E}$ possesses many of the properties one looks for in a meaningful minimization functional, it lacks coercivity (in the sense of Dal Maso (2012, Definition 1.12)): a sufficient property to establish the existence of solutions. The key to showing non-coercivity is that we can construct a kernel $k_{\\theta}(x|z)$ that does not depend on $z$ . ", "page_idx": 2}, {"type": "text", "text": "At first glance, this issue might seem contrived but we note that this problem is closely related to the problem of posterior collapse (Lucas et al., 2019; Wang et al., 2021). To address non-coercivity, we propose to utilize regularization and define the regularized free energy as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{E}_{\\lambda}(\\theta,r):=\\mathbb{E}_{q_{\\theta,r}(x)}\\left[\\log\\frac{q_{\\theta,r}(x)}{p(x,y)}\\right]+\\mathsf{R}_{\\lambda}(\\theta,r),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathsf{R}_{\\lambda}$ is a regularizer with parameters $\\lambda$ . In Prop. 3, we show that if $\\mathsf{R}_{\\lambda}$ is sufficiently regular, then the $\\mathcal{E}_{\\lambda}$ enjoys better properties than its unregularized counterpart $\\mathcal{E}$ . ", "page_idx": 3}, {"type": "text", "text": "Proposition 3. Under the assumptions of Prop. 2, $i f\\mathsf{R}_{\\lambda}$ is coercive and l.s.c., then the regularized free energy $\\mathcal{E}_{\\lambda}$ is $(i)$ lower bounded, (ii) l.s.c., (iii) coercive. Hence it admits at least one minimizer in $\\mathcal{M}$ . ", "page_idx": 3}, {"type": "text", "text": "The proof can be found in App. E.2. From here forward, we shall focus on regularizers of the form $\\mathsf{R}_{\\lambda}^{\\mathrm{E}}:(\\theta,r)\\mapsto\\lambda_{r}\\mathsf{K L}(r,p_{0})+\\dot{\\lambda}_{\\theta}\\mathsf{R}_{\\theta}(\\theta)$ where $\\lambda=\\{\\lambda_{r},\\lambda_{\\theta}\\}$ are the regularization parameters and $p_{0}$ is a predefined reference distribution. As long as $\\mathsf{R}_{\\theta}$ is l.s.c., coercive and $\\lambda_{\\theta},\\lambda_{r}>0$ , the resulting regularizer $\\mathsf{R}_{\\lambda}^{\\mathrm{E}}$ will also be l.s.c. and coercive. There are many possible choices for $p_{0}$ and $\\mathsf{R}_{\\theta}$ . For $p_{0}$ , this regularizes solutions of the gradient flow toward it; as such, in settings where there is some knowledge or preference about $r$ at hand, we can set $p_{0}$ to reflect that. In our experiments, we utilize $p_{0}=\\mathcal{N}(0,M)$ where $M$ is a positive definite (p.d.) matrix. As for $\\mathsf{R}_{\\theta}$ , there are also many choices. In the context of neural networks, one natural choice is Tikhonov regularization ${\\frac{1}{2}}\\parallel\\cdot\\parallel^{2}$ , resulting in weight decay (Hanson and Pratt, 1988) for gradient descent (Loshchilov and Hutter, 2019) which is a popular method for regularizing neural networks. In our experiments, we either use Tikhonov regularization or its simple variant $\\|\\theta\\|_{M}^{2}:=\\langle\\theta,M\\theta\\rangle$ . ", "page_idx": 3}, {"type": "text", "text": "3.2 Gradient flow ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To solve the problem in Eq. (3), we construct a gradient flow that minimizes $\\mathcal{E}_{\\lambda}$ . To this end, we endow the space $\\mathcal{M}$ with a suitable notion of gradient $\\nabla_{\\boldsymbol{\\mathcal{M}}}\\mathcal{E}_{\\lambda}(\\boldsymbol{\\theta},\\boldsymbol{r}):=(\\nabla_{\\boldsymbol{\\theta}}\\mathcal{E}_{\\lambda},\\nabla_{\\boldsymbol{r}}\\mathcal{E}_{\\lambda})$ where $\\nabla_{\\theta}$ and $\\nabla_{r}$ denotes the Euclidean gradient and Wasserstein-2 gradient (Jordan et al., 1998), respectively. The latter gradient is given by $\\nabla_{r}\\mathcal{E}_{\\lambda}(\\theta,r)\\;:=\\;-\\nabla_{z}\\,\\cdot\\,\\bar{(r\\nabla_{z}\\delta_{r}\\mathcal{E}_{\\lambda}[\\theta,r])}$ , where $\\nabla_{z}$ \u00b7 denotes the standard divergence operator and $\\delta_{r}$ denotes the first variation which is characterized in the following proposition. ", "page_idx": 3}, {"type": "text", "text": "P $\\mathcal{M}$ $\\begin{array}{r}{\\mathbf{roposition\\,4}\\,(\\mathrm{First\\,Variation\\,of}\\,\\mathcal{E}_{\\lambda}\\mathrm{\\,and}\\,\\mathsf{R}_{\\lambda}^{\\mathrm{E}})_{\\star}\\,A s s u m e\\,t h a t\\,\\mathbb{E}_{k_{\\theta}(X|\\cdot)}\\left|\\log\\frac{q_{\\theta,r}(X)}{p(X,y)}\\right|<\\infty f o r\\,a l l\\,(\\theta,r)\\in\\mathbb{C}_{\\phi}\\mathrm{\\,~}}\\\\ {\\textit{d};t h e n\\,t h e\\,f i r s t\\,v a r i a t i o n\\,o f\\mathcal{E}_{\\lambda}\\mathrm{\\,}i s\\,\\delta_{r}\\mathcal{E}_{\\lambda}=\\delta_{r}\\mathcal{E}+\\delta_{r}\\mathsf{R}_{\\lambda}\\,w h e r e\\,\\delta_{r}\\mathcal{E}[\\theta,r](z)=\\mathbb{E}_{k_{\\theta}(X|z)}\\left[\\log\\frac{q_{\\theta,r}(X)}{p(X,y)}\\right],}\\end{array}$ and $\\delta_{r}\\mathsf{R}_{\\lambda}^{\\mathrm{E}}[\\theta,r]=\\lambda_{r}\\log r/p_{0}$ . ", "page_idx": 3}, {"type": "text", "text": "The proof can be found in App. E.3. Thus, the (Euclidean\u2013Wasserstein) gradient flow of $\\mathcal{E}_{\\lambda}$ is ", "page_idx": 3}, {"type": "equation", "text": "$$\n(\\dot{\\theta}_{t},\\dot{r}_{t})=-\\nabla_{M}\\mathcal{E}_{\\lambda}(\\theta_{t},r_{t})\\iff\\begin{array}{l}{\\dot{\\theta}_{t}=-\\nabla_{\\theta}\\mathcal{E}_{\\lambda}(\\theta_{t},r_{t})}\\\\ {\\dot{r}_{t}=-\\nabla_{r}\\mathcal{E}_{\\lambda}(\\theta_{t},r_{t})=\\nabla_{z}\\cdot\\big(r_{t}\\nabla_{z}\\delta_{r}\\mathcal{E}_{\\lambda}[\\theta_{t},r_{t}]\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We now establish that the above gradient flow dynamic is contractive and that if a log-Sobolev inequality Eq. (7) holds, one can also establish exponential convergence. The log-Sobolev inequality is closely related to Polyak\u2013\u0141ojasiewicz inequality (or gradient dominance condition) and is commonly assumed in gradient-based systems to obtain convergence (for instance, see Kim et al. (2024)). This is formally stated in the following proposition and proved in App. E.3. ", "page_idx": 3}, {"type": "text", "text": "Proposition 5 (Contracting Gradient Dynamics). The free energy $\\mathcal{E}_{\\lambda}$ along the flow Eq. (5) is non-increasing and satisfies ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\mathcal{E}_{\\lambda}(\\theta_{t},r_{t})=-\\|\\nabla_{\\mathcal{M}}\\mathcal{E}_{\\lambda}(\\theta_{t},r_{t})\\|^{2}\\leq0,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\begin{array}{r}{\\|\\nabla_{\\mathcal{M}}\\mathcal{E}_{\\lambda}(\\theta,r)\\|^{2}:=\\|\\nabla_{\\theta}\\mathcal{E}_{\\lambda}(\\theta,r)\\|^{2}+\\|\\nabla_{z}\\delta_{r}\\mathcal{E}_{\\lambda}[\\theta,r]\\|_{r}^{2}.}\\end{array}$ . Moreover, if a log-Sobolev Inequality holds for a constant $\\tau\\in\\mathbb{R}_{>0}$ , i.e., for all $(\\theta,r)\\in\\mathcal{M}$ , we have ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{\\lambda}(\\theta,r)-\\mathcal{E}_{\\lambda}^{*}\\le\\tau\\|\\nabla_{\\mathcal{M}}\\mathcal{E}_{\\lambda}(\\theta,r)\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\displaystyle\\mathcal{E}_{\\lambda}^{*}:=\\operatorname*{inf}_{(\\theta,r)\\in\\mathcal{M}}\\mathcal{E}_{\\lambda}(\\theta,r),$ ; then we have exponential convergence ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{\\lambda}(\\theta_{t},r_{t})-\\mathcal{E}_{\\lambda}^{*}\\le\\exp(-t/\\tau)(\\mathcal{E}_{\\lambda}(\\theta_{0},r_{0})-\\mathcal{E}_{\\lambda}^{*}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Typically direct simulation of the gradient flow Eq. (5) is intractable as the derivative of the first variation of $\\mathsf{R}_{\\lambda}^{\\mathrm{E}}$ involves $\\nabla_{z}\\log{r_{t}}$ ; instead, it is useful to identify the gradient flow with a McKean\u2013 Vlasov SDE, for which they share the same Fokker\u2013Planck equation. The key distinction is that the SDE can be simulated without access to this quantity $\\nabla_{z}\\log{r_{t}}$ . This SDE, which we term the PVI flow, is given by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{d}\\theta_{t}=-\\nabla_{\\theta}\\mathcal{E}_{\\lambda}(\\theta_{t},r_{t})\\,\\mathrm{d}t,\\ \\mathrm{d}Z_{t}=b(\\theta_{t},r_{t},Z_{t})\\,\\mathrm{d}t+\\sqrt{2\\lambda_{r}}\\,\\mathrm{d}W_{t},\\ \\mathrm{where~}r_{t}=\\mathrm{Law}(Z_{t}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the drift is $b(\\theta,r,\\cdot):=-\\nabla_{z}\\delta_{r}\\mathcal{E}[\\theta,r]+\\lambda_{r}\\nabla_{z}\\log p_{0}$ (with the first variation given in Prop. 4) and $W_{t}$ is a $d_{z}$ -dimensional \u221aWiener process. A connection between the Langevin diffusion, i.e., $\\mathrm{d}Z_{t}=\\nabla_{z}\\log p(Z_{t},y)\\,\\mathrm{d}t+\\sqrt{2}\\mathrm{d}W_{t}$ , and PVI flow can be observed with the fixed kernel $k_{\\theta}(\\mathrm{d}x|z)=$ $\\delta_{z}(\\mathrm{d}x)$ and $\\lambda_{r}=0$ , namely, they both satisfy the same Fokker\u2013Planck equation. ", "page_idx": 4}, {"type": "text", "text": "3.3 A practical algorithm ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Algorithm 1 Particle Variational Inference (PVI) ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Input: initialization $(\\theta_{0},\\{Z_{0}\\}_{i=1}^{M})$ ; regularization parameters $\\{\\lambda_{\\theta},\\lambda_{r}\\}$ ; step-sizes $h_{\\theta}$ and $h_{r}$ ;   \nnumber of Monte Carlo samples $L$ (for Eqs. (11) and (12)); and preconditioner $\\Psi=(\\Psi^{\\theta},\\Psi^{r})$ .   \nfor $k=1$ to $K$ do $\\begin{array}{r l r}&{r_{k-1}^{M}\\leftarrow\\frac{1}{M}\\sum_{m=1}^{M}\\delta_{Z_{k-1,m}}}\\\\ &{\\theta_{k}\\leftarrow\\theta_{k-1}-h_{\\theta}\\Psi^{\\theta}\\widehat{\\nabla}_{\\theta}\\mathcal{E}_{\\lambda}(\\theta_{k-1},r_{k-1}^{M})}&{\\qquad\\qquad\\qquad\\qquad\\triangleright\\mathrm{~See~Eq.~}(11)}\\\\ &{\\hat{b}_{k}\\leftarrow Z\\mapsto-\\widehat{\\nabla}_{z}\\mathcal{S}_{r}\\mathcal{E}[\\theta_{k},r_{k-1}^{M}](Z)+\\lambda_{r}\\nabla_{z}\\log p_{0}(Z)}&{\\qquad\\qquad\\qquad\\quad\\triangleright\\mathrm{~See~Eq.~}(12)}\\\\ &{\\mathsf{f o r~}m=1\\;\\mathbf{t0}\\;M\\;\\mathbf{d0}}&\\\\ &{\\quad Z_{k,m}\\leftarrow Z_{k-1,m}+h_{r}\\Psi^{r}\\widehat{b}(Z_{k-1,m})+\\sqrt{\\lambda_{r}h_{r}\\Psi^{r}}\\eta_{k,m}}&{\\qquad\\qquad\\qquad\\quad\\forall\\;\\eta_{k,m}\\sim\\mathcal{N}(0,I_{d_{z}})}\\end{array}$ end for   \nend for   \nreturn $(\\theta_{K},\\{Z_{K,m}\\}_{m=1}^{M})$ ", "page_idx": 4}, {"type": "text", "text": "To produce a practical algorithm, we are faced with several practical issues. The first issue we tackle is the computation of gradients of expectations for which using standard automatic differentiation is insufficient. The second problem is that these gradients are often ill-conditioned and have different scales in each dimension. This is tackled using preconditioning resulting in adaptive stepsize. Finally, to produce computationally feasible algorithms, we show how to discretize the PVI flow in both space and time. PVI is summarised in Algorithm 1. ", "page_idx": 4}, {"type": "text", "text": "Computing the gradients. In the PVI flow, both the drift of the ODE and SDE include a gradient of an expectation with respect to parameters that define the distribution that is being integrated. Specifically, the terms that contain these gradients are $\\nabla_{\\boldsymbol{\\theta}}\\mathcal{E}_{\\lambda}$ and $\\nabla_{z}\\delta_{r}\\mathcal{E}_{\\lambda}$ . Fortunately, these gradients can be rewritten as an expectation (as described in Prop. 6) for which the parameters being differentiated w.r.t. is only found in the integrand (see derivation in App. G.1). ", "page_idx": 4}, {"type": "text", "text": "Proposition 6. If $\\phi$ and $k$ are differentiable, then we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla_{\\theta}\\mathcal{E}(\\theta,r)=\\!\\!\\mathbb{E}_{p_{k}(\\epsilon)r(z)}\\left[\\big(\\nabla_{\\theta}\\phi_{\\theta}\\cdot[s_{\\theta,r}-s_{p}]\\big)(z,\\epsilon)\\right],}\\\\ {\\nabla_{z}\\delta_{r}\\mathcal{E}[\\theta,r](z)=\\!\\!\\mathbb{E}_{p_{k}(\\epsilon)}\\left[\\big(\\nabla_{z}\\phi_{\\theta}\\cdot[s_{\\theta,r}-s_{p}]\\big)(z,\\epsilon)\\right].\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\nabla_{\\theta}\\phi\\in\\mathbb{R}^{d_{\\theta}\\times d_{x}}$ denotes the Jacobian $(\\nabla_{\\theta}\\phi)_{i j}=\\partial_{\\theta_{i}}\\phi_{j}$ (and similarly for $\\nabla_{z}\\phi)$ ; scores are $s_{\\theta,r}(z,\\epsilon):=\\nabla_{x}\\log q_{\\theta,r}(\\phi_{\\theta}(z,\\epsilon))$ (and similarly $s_{p}(z,\\epsilon),$ ) ; and $\\cdot$ denotes the usual matrix-vector multiplication in the sense of $M\\cdot v:(z,\\epsilon)\\mapsto M(\\bar{z_{,}}\\epsilon)v(z,\\epsilon)$ . ", "page_idx": 4}, {"type": "text", "text": "From Eqs. (9) and (10), we can produce Monte Carlo estimators for the gradients, i.e., ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\widehat{\\nabla}_{\\theta}\\mathcal{E}(\\theta,r):=\\frac{1}{L}\\sum_{l=1}^{L}\\mathbb{E}_{z\\sim r}[(\\nabla_{z}{\\phi}_{\\theta}\\cdot[s_{\\theta,r}-s_{p}])(z,\\epsilon_{l})],}\\\\ {\\displaystyle\\widehat{\\nabla}_{z}{\\delta}_{r}\\mathcal{E}[\\theta,r]:=\\frac{1}{L}\\sum_{l=1}^{L}(\\nabla_{z}{\\phi}_{\\theta}\\cdot[s_{\\theta,r}-s_{p}])(\\cdot,\\epsilon_{l}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "$\\{\\epsilon_{l}\\}_{l=1}^{L}\\stackrel{i.i.d.}{\\sim}\\ p_{k}$ performant estimator that has been shown empirically to exhibit lower variance than other standard estimators (Kingma and Welling, 2014; Roeder et al., 2017; Mohamed et al., 2020). ", "page_idx": 5}, {"type": "text", "text": "Adaptive Stepsizes. One of the complexities of training neural networks is that their gradient is often poorly conditioned. As a result, for certain problems, the gradients computed from Eq. (9) and Eq. (10) can often produce unstable algorithms without careful tuning of the step sizes. When this occurs, we utilize preconditioners (Staib et al., 2019) to avoid this issue. Let $\\Psi^{\\theta}:\\stackrel{\\cdot}{\\Theta}\\mapsto\\mathbb{R}^{d_{\\theta}\\times d_{\\theta}}$ and $\\Psi^{r}:\\vec{\\mathbb{R}}^{d_{z}}\\mapsto\\mathbb{R}^{d_{z}\\,\\dot{\\times}\\,d_{z}}$ be the precondition for components $\\theta$ and $r$ respectively, then the resulting preconditioned gradient flow is given by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\theta_{t}=-\\Psi^{\\theta}\\nabla_{\\theta}\\mathcal{E}_{\\lambda}(\\theta_{t},r_{t})\\,\\mathrm{d}t,\\;\\;\\partial_{t}r_{t}=\\nabla_{z}\\cdot\\bigl(r_{t}\\Psi^{r}\\nabla_{z}\\delta_{r}\\mathcal{E}_{\\lambda}[\\theta_{t},r_{t}]\\bigr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "If $\\Psi^{\\theta}$ and $\\Psi^{r}$ are positive definite, then $\\mathcal{E}_{\\lambda}(\\theta_{t},r_{t})$ remains non-increasing, i.e., Eq. (6) holds. As before, this Fokker\u2013Planck equation is satisfied by the following Mckean\u2013Vlasov SDE: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{d}\\theta_{t}=-\\Psi^{\\theta}(\\theta_{t})\\nabla_{\\theta}\\mathcal{E}_{\\lambda}(\\theta_{t},r_{t})\\,\\mathrm{d}t,}\\\\ &{\\mathrm{d}Z_{t}=[\\Psi^{r}(Z_{t})b(\\theta_{t},r_{t},Z_{t})+\\nabla_{z}\\cdot\\Psi^{r}(Z_{t})]\\,\\mathrm{d}t+\\sqrt{2\\lambda\\Psi^{r}(Z_{t})}\\mathrm{d}W_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\begin{array}{r}{(\\nabla_{z}\\cdot\\Psi^{r})_{i}=\\sum_{j=1}^{d_{z}}\\partial_{z_{j}}[(\\Psi^{r})_{i j}]}\\end{array}$ and $r_{t}=\\mathrm{Law}(Z_{t})$ . The equivalence between Eq. (13) and Eqs. (14) and (15) is shown in App. G.2. A simple example for the preconditioner allows the $\\theta_{t}$ and $Z_{t}$ to have different time scales; ultimately, this results in different step sizes. Another more complex example of preconditioner $\\Psi^{\\theta}$ is the RMSProp (Tieleman and Hinton, 2012), and $\\Psi^{r}$ we utilize a preconditioner inspired by RMSProp (see App. G.2). As with other related works (e.g., see Li et al. (2016)), we found that the additional term $\\nabla_{z}\\cdot\\Psi^{r}$ can be omitted in practice: it has little effect but incurs a large computational cost. ", "page_idx": 5}, {"type": "text", "text": "Discretization in both space and time. To obtain an actionable algorithm, we need to discretize the PVI flow in both space and time. For the space discretization, we propose to use a particle approximation for $r_{t}$ , i.e., for a set of particles $\\{Z_{t,m}^{\\bullet}\\}_{m=1}^{M}$ with each satisfying $\\operatorname{Law}(Z_{t,m})\\stackrel{\\cdot}{=}r_{t}$ , we utilize the approximation $\\begin{array}{r}{r_{t}^{M}:=\\frac{1}{M}\\sum_{m=1}^{M}\\delta_{Z_{t,m}}}\\end{array}$ which converges almost surely to $r_{t}$ in the weak topology as $M\\rightarrow\\infty$ by the strong law of large numbers and a countable determining class argument (e.g., see Schmon et al. (2020, Theorem 1.1)). This approximation is key to making the intractable tractable, e.g., Eq. (1) is approximated by $\\begin{array}{r}{q_{\\theta,r_{t}^{M}}=\\bar{\\frac{1}{M}}\\sum_{m=1}^{M}k_{\\theta}(x|Z_{t,m})}\\end{array}$ . One obtains a particle approximation to the PVI flow from the following ODE\u2013SDE: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{d}\\theta_{t}^{M}=-\\nabla_{\\theta}\\mathcal{E}_{\\lambda}(\\theta_{t}^{M},r_{t}^{M})\\,\\mathrm{d}t,\\ \\forall m\\in[M]:\\mathrm{d}Z_{t,m}^{M}=b(\\theta_{t}^{M},r_{t}^{M},Z_{t,m}^{M})\\,\\mathrm{d}t+\\sqrt{2\\lambda_{r}}\\,\\mathrm{d}W_{t,m},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $[M]:=\\{1,\\dots,M\\}$ . As for the time discretization, we employ Euler-Maruyama discretization with step-size $h$ which (using an appropriately defined preconditioner) can be decoupled into different stepsizes for $\\theta_{t}$ and $Z_{t}$ denoted by $h_{\\theta}$ and $h_{r}$ respectively. ", "page_idx": 5}, {"type": "text", "text": "4 Theoretical analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We are interested in the behaviour of the PVI flow (8). However, a key issue in its study is that the drift in PVI flow might lack the necessary continuity properties to analyze using the existing theory. In this section, we instead analyze the related gradient flow of the more regular functional ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{E}_{\\lambda}^{\\gamma}(\\theta,r):=\\mathcal{E}^{\\gamma}(\\theta,r)+\\mathsf{R}_{\\lambda}(\\theta,r),\\quad\\mathrm{where~}\\mathcal{E}^{\\gamma}(\\theta,r)=\\mathbb{E}_{q_{\\theta,r}(x)}\\left[\\log\\frac{q_{\\theta,r}(x)+\\gamma}{p(x,y)}\\right]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for $\\gamma>0$ . A similar modified flow was also explored in Crucinio et al. (2024) for similar reasons; they found empirically that, at least when using a tamed Euler scheme, setting $\\gamma=0$ did not cause problems in practice. Similarly, our experimental results for PVI found $\\gamma=0$ did not have issues. To provide an additional measure of confidence in the reasonableness of this regularization and of the use of this functional as a proxy for $\\mathcal{E}_{\\lambda}$ , we establish that the minima of $\\mathcal{E}_{\\lambda}^{\\gamma}$ converge to those of $\\mathcal{E}_{\\lambda}$ in the $\\gamma\\to0$ limit. ", "page_idx": 5}, {"type": "text", "text": "Proposition 7 ( $\\Gamma$ -convergence and convergence of minima). Under the same assumptions as Prop. 3, we have that $\\mathcal{E}_{\\lambda}^{\\gamma}$ \u0393-converges to $\\mathcal{E}_{\\lambda}$ as $\\gamma\\to0$ (in the sense of Def. B.1). Moreover, we have as an immediate corollary that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{(\\theta,r)\\in\\mathcal{M}}\\mathcal{E}_{\\lambda}(\\theta,r)=\\operatorname*{lim}_{\\gamma\\to0}\\operatorname*{inf}_{(\\theta,r)\\in\\mathcal{M}}\\mathcal{E}_{\\lambda}^{\\gamma}(\\theta,r).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The proof uses techniques from $\\Gamma$ -convergence theory (introduced by De Giorgi, see e.g. De Giorgi and Franzoni (1975); see Dal Maso (2012) for a good modern introduction) and can be found in App. F.1. The gradient flow of $\\mathcal{E}_{\\lambda}^{\\gamma}$ , which we term $\\gamma$ -PVI, is given by ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\,\\mathrm{d}\\theta_{t}^{\\gamma}=-\\nabla_{\\theta}\\mathcal{E}_{\\lambda}^{\\gamma}(\\theta_{t}^{\\gamma},r_{t}^{\\gamma})\\,\\mathrm{d}t,\\ \\mathrm{d}Z_{t}^{\\gamma}=b^{\\gamma}(\\theta_{t}^{\\gamma},r_{t}^{\\gamma},Z_{t}^{\\gamma})\\,\\mathrm{d}t+\\sqrt{2\\lambda_{r}}\\,\\mathrm{d}W_{t},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $r_{t}^{\\gamma}\\,=\\,\\mathrm{Law}(Z_{t}^{\\gamma})$ and $b^{\\gamma}(\\theta,r,\\cdot)\\,=\\,-\\nabla_{z}\\delta_{r}\\mathcal{E}^{\\gamma}[\\theta,r]\\,+\\,\\lambda_{r}\\nabla_{z}\\log p_{0}$ . The derivation follows similarly to that in Section 3.2, and is omitted for brevity. The use of $\\gamma>0$ is crucial for establishing key regularity conditions in our analysis. We proceed by stating our assumptions. ", "page_idx": 6}, {"type": "text", "text": "Assumption 1 (Regularity of the target $p$ , reference distribution $p_{0}$ , and $\\mathsf{R}_{\\theta}$ ). We assume that $\\log{p(y)}$ is bounded; and $p,p_{0}$ and $\\mathsf{R}_{\\theta}$ have Lipschitz gradients with constants $K_{p},K_{p_{0}},K_{\\mathsf{R}_{\\theta}}$ respectively: there exists some $B_{p}\\in\\mathbb{R}_{>0}$ such that $\\log p(y)\\le B_{p}$ ; and for any given $y$ there exists a $K_{p}\\in\\mathbb{R}_{>0}$ such that $\\|\\nabla_{x}\\log p(x,y)-\\nabla_{x}\\log p(x^{\\prime},y)\\|\\leq K_{p}\\|x-x^{\\prime}\\|$ for all $x,x^{\\prime}\\in\\mathbb{R}^{d_{x}}$ (similarly for $p_{0}$ and $\\mathsf{R}_{\\theta}$ ). ", "page_idx": 6}, {"type": "text", "text": "Assumption 2 (Regularity of $k$ ). We assume that the kernel $k$ and its gradient is bounded and has $K_{k}$ -Lipschitz gradient; i.e., there exist constants $B_{k},K_{k}\\;\\in\\;\\mathbb{R}_{>0}$ such that $|k_{\\theta}(x|z)|~+$ $\\|\\nabla_{(\\theta,x,z)}k_{\\theta}(x|z)\\|\\le B_{k}$ , and $\\begin{array}{r}{\\|\\nabla_{x}k_{\\theta}(x|z)-\\nabla_{x}k_{\\theta^{\\prime}}(x^{\\prime}|z^{\\prime})\\|\\le K_{k}(\\|(\\theta,x,z)-(\\theta^{\\prime},x^{\\prime},z^{\\prime})\\|)}\\end{array}$ hold for all $\\theta,\\theta^{\\prime}\\in\\Theta,z,z^{\\prime}\\in\\mathbb{R}^{d_{z}}$ , and $x,x^{\\prime}\\in\\mathbb{R}^{d_{x}}$ , ", "page_idx": 6}, {"type": "text", "text": "Assumption 3 (Regularity of $\\phi$ and $p_{k}$ .). We assume that $\\phi$ has Lipschitz gradient and bounded gradient. In order words, there is $a_{\\phi}\\;\\in\\;\\mathbb{R}_{\\geq0},b_{\\phi}\\;\\in\\;\\mathbb{R}_{>0}$ such that $\\phi$ satisfies $\\|\\nabla_{(\\theta,z)}\\phi(z,\\epsilon)\\textrm{--}$ $\\nabla_{(\\theta,z)}\\phi_{\\theta^{\\prime}}(z^{\\prime},\\epsilon)\\|_{F}\\leq(a_{\\phi}\\|\\epsilon\\|+b_{\\phi})(\\|(\\theta,z)-(\\theta^{\\prime},z^{\\prime})\\|)$ and $\\|\\nabla_{(\\theta,z)}\\phi_{\\theta}(z,\\epsilon)\\|_{F}\\leq(a_{\\phi}\\|\\epsilon\\|+b_{\\phi})$ for all $(\\theta,z)$ $\\prime,z),(\\theta^{\\prime},z^{\\prime})\\in\\Theta\\times\\mathbb{R}^{d_{z}},\\epsilon\\in\\mathbb{R}^{d_{x}}$ , where $\\|\\cdot\\|_{F}$ denotes the Frobenius norm. We also assume that $p_{k}$ has finite second moments. ", "page_idx": 6}, {"type": "text", "text": "Assumptions 2 and 3 are intimately connected; under some regularity conditions, one may imply the other but we shall abstain from this digression for the sake of clarity. Assumptions 2 and 3 are quite mild and hold for popular kernels such as $k_{\\theta}(x|z)=\\mathcal{N}(x;\\mu_{\\theta}(z),\\Sigma)$ under some regularity assumptions on $\\mu_{\\theta}$ and $\\Sigma$ (which we show in App. C). These assumptions are key to establishing that the drift in Eq. (17) is Lipschitz continuous (see Prop. 11 in the App.), from which, we establish the existence and uniqueness of the solutions of Eq. (17). ", "page_idx": 6}, {"type": "text", "text": "Proposition 8 (Existence and Uniqueness). Under Assumptions $^{\\,l}$ to 3, if $\\gamma\\ \\mathrm{~>~}\\ 0$ and $\\mathbb{E}_{p_{k}(\\epsilon)}\\|s_{\\theta,r}^{\\gamma}(z,\\epsilon)-s_{p}(z,\\epsilon)\\|$ is bounded (with $s_{\\theta,r}^{\\gamma}:(z,\\epsilon)\\mapsto\\nabla_{x}\\log(q_{\\theta,r}\\circ\\phi_{\\theta}(z,\\epsilon)+\\gamma))$ ; then, given $(\\theta_{0},r_{0})\\in\\mathcal{M}$ , the solutions to Eq. (17) exists and is unique. ", "page_idx": 6}, {"type": "text", "text": "The proof can be found in App. F.2. Under the same assumptions, we can establish an asymptotic propagation of chaos result that justifies the usage of a particle approximation in place of $\\bar{r}_{t}^{\\gamma}$ in Eq. (17). ", "page_idx": 6}, {"type": "text", "text": "Proposition 9 (Propagation of chaos). Under the same assumptions as Prop. 8; we have for any fixed $T$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{M\\to\\infty}\\mathbb{E}\\operatorname*{sup}_{t\\in[0,T]}\\left\\|\\theta_{t}^{\\gamma}-\\theta_{t}^{\\gamma,M}\\right\\|^{2}+\\mathsf{W}_{2}^{2}\\left((r_{t}^{\\gamma})^{\\otimes M},q_{t}^{\\gamma,M}\\right)=0,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\begin{array}{r}{\\big(r_{t}^{\\gamma}\\big)^{\\otimes M}=\\prod_{i=1}^{M}(r_{t}^{\\gamma});\\,q_{t}^{\\gamma,M}=\\mathrm{Law}(\\{Z_{t,m}^{\\gamma,M}\\}_{m=1}^{M});\\,\\theta_{t}^{\\gamma,M}}\\end{array}$ and $Z_{t,m}^{\\gamma,M}$ are solutions to ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\mathrm{d}\\theta_{t}^{\\gamma,M}=-\\nabla_{\\theta}\\mathcal{E}_{\\lambda}^{\\gamma}(\\theta_{t}^{\\gamma,M},r_{t}^{\\gamma,M})\\,\\mathrm{d}t,\\ \\boldsymbol{w h e r e}\\,r_{t}^{\\gamma,M}=\\frac{1}{M}\\sum_{m=1}^{M}\\delta_{Z_{t,m}^{\\gamma,M}}}\\\\ {\\displaystyle\\forall m\\in[M]:\\mathrm{d}Z_{t,m}^{\\gamma,M}=b^{\\gamma}(\\theta_{t}^{\\gamma,M},r_{t}^{\\gamma,M},Z_{t,m}^{\\gamma,M})\\,\\mathrm{d}t+\\sqrt{2\\lambda_{r}}\\,\\mathrm{d}W_{t,m}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The proof can be found in App. F.3. Having established the existence and uniqueness of the $\\gamma.$ -PVI flow, as well as an asymptotic justification for using particles, we now provide a numerical evaluation to demonstrate the efficacy of our proposal. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we compare PVI against other semi-implicit VI methods. As described in the App. A, these include unbiased semi-implicit variational inference (UVI) of Titsias and Ruiz (2019), semiimplicit variational inference (SVI) of Yin and Zhou (2018), and the score matching approach (SM) ", "page_idx": 6}, {"type": "text", "text": "of $\\mathrm{Yu}$ and Zhang (2023). Through experiments, we show the benefits of optimizing the mixing distribution; we compare the effectiveness of PVI against other SIVI methods on a density estimation problem on toy examples; and, we compare against other SIVI methods on posterior estimation tasks for (Bayesian) logistic regression and (Bayesian) neural network. The details for reproducing experiments as well as computation information can be found in App. H. The code is available at https://github.com/jenninglim/pvi. ", "page_idx": 7}, {"type": "image", "img_path": "p3gMGkHMkM/tmp/66d41768367a8c2c66582919b4e0a4e84d2269a35bef8eec6d8b47b7c8270835.jpg", "img_caption": ["5.1 Impact of the mixing distribution ", "Figure 1: Comparison of PVI and PVIZero on a bimodal mixture of Gaussians for various kernels. The plot shows the density $q_{\\theta,r}$ from PVI and PVIZero as well as the KDE plot of $r$ from PVI described by 100 particles. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "From Prop. 1, it can be said that ultimately current SIVI methods utilize (directly or indirectly) a fixed mixing distribution whilst PVI does not. We are interested in establishing whether there is any benefti to optimizing the mixing distribution. Intuitively, the mixing distribution can be utilized to express complex properties, such as multimodality, which the neural network kernel $k_{\\theta}$ can then exploit. If the mixing distribution is fixed, this means that the neural network must learn to express these complex properties directly\u2014which can be difficult (Salmona et al., 2022). This intuition turns out to hold, but for the kernel to exploit an expressive mixing distribution, it must be designed well. To illustrate this, consider the distributions $\\textstyle{\\frac{1}{2}}{\\dot{\\mathcal{N}}}(\\mu,I)+{\\frac{1}{2}}{\\mathcal{N}}{\\overset{\\underbrace{\\,}{-}}{(-\\mu,I)}}$ for $\\mu=\\{1,2,4\\}$ and following kernels: the \u201cConstant\u201d kernel $\\mathcal{N}(z,I_{2})$ ; \u201cPush\u201d kernel $\\mathcal{N}(f_{\\theta}(z),\\sigma_{\\theta}^{2}I_{2})$ ; \u201cSkip\u201d kernel $\\mathcal{N}(z+f_{\\theta}(z),\\sigma_{\\theta}^{2}I_{2})$ ; and \u201cLSkip\u201d kernel $\\mathcal{N}(W z+f_{\\theta}(z),\\sigma_{\\theta}^{2}I_{2})$ where $W\\in\\mathbb{R}^{2\\times2}$ ;. We compare the results from PVI and PVIZero (PVI with $h_{r}=0$ to result in a fixed $r\\approx\\mathcal{N}(0,I_{2}))$ ) to emulate PVI with a fixed mixing distribution. As $\\mu$ gets larger, the complexity of the kernel (or the mixing distribution) must grow to express this (e.g., see (Salmona et al., 2022, Corollary 2)). ", "page_idx": 7}, {"type": "text", "text": "Fig. 1 shows the resulting densities and the learnt mixing distribution of PVI and PVIZero for different kernels and various $\\mu$ . For the constant kernel, PVI can solve this problem by learning a complex mixing distribution to express the multimodality. However, for the push kernel, it can be seen that as $\\mu$ gets larger PVI and PVIZero suffer from mode collapse which we suspect is due to the mode-seeking behaviour of using reverse KL and why prior SIVI methods utilized annealing methods (see Yu and Zhang (2023, Section 4.1)). As a remedy, we utilize a Skip kernel which can be seen to improve both PVI and PVIZero. In particular, both PVI and PVIZero were able to successfully express the bimodality in $\\mu=2$ ; however, PVIZero falls short when $\\mu=4$ while PVI can express the multimodality by learning a bimodal mixing distribution. Since Skip requires $d_{z}=d_{x}$ , we show that LSkip (which removes the requirement) exhibits a similar behaviour to Skip. ", "page_idx": 7}, {"type": "text", "text": "5.2 Density estimation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We follow prior works (e.g., Yin and Zhou (2018)) and consider three toy examples whose densities are shown in Fig. 2 (they are given explicitly in App. H.2). In this setting, we use the kernel $k_{\\theta}(x|z)\\,=\\,\\mathcal{N}(\\bar{x_{;}}\\,z\\,+\\,f_{\\theta}\\bar{(}z),\\sigma_{\\theta}^{\\bar{2}}I)$ with $d_{z}~=~d_{x}~=~2$ where $f_{\\theta}(z)$ is a neural network whose architecture can be found in App. H.2. As a qualitative measure of performance, Fig. 2 shows the resulting approximating distribution of PVI which can be seen to be a close match to the desired distribution. To compare methods quantitatively, we report the (sliced) Wasserstein distance (computed by POT (Flamary et al., 2021)) and the rejection power of a state-of-the-art two-sample kernel test (Biggs et al., 2023) between the approximating and true distribution in Table 1. The results reported are the average and standard deviation (from ten independent trials of the respective SIVI algorithms). In each trial, the rejection rate $p$ is computed from 100 tests and the sliced Wasserstein distance is computed from 10000 samples with 100 projections. If the variational approximation matches the distribution, the rejection rate will be at the nominal level of 0.05. It can be seen that PVI consistently performs better than SIVI across all problems. PVI can achieve a rejection rate near nominal levels across all problems whilst other algorithms can achieve good performances on one but not the other. The details regarding how the Wasserstein distance is calculated and the hyperparameters used can be found in App. H.2. ", "page_idx": 7}, {"type": "image", "img_path": "p3gMGkHMkM/tmp/ce6c4b5dbd6205d750dc8268d57693288ffb7989f64aa505bd9b639e07cc3eda.jpg", "img_caption": ["Figure 2: Contour plots of the densities $q_{\\theta,r}$ (in blue) against the true densities (in black) for various toy density estimation problems. We also plot the absolute difference in the density of $q_{\\theta,r}$ and the true density, i.e., $|q_{\\theta,r}-p|$ . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "table", "img_path": "p3gMGkHMkM/tmp/8f5c5a7e3d8a2b9f779d124fa6bd3da654147c65aea55e9f107cb75283f8800d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 1: This table shows the rejection rate $p$ and average (sliced) Wasserstein distance $w$ for toy density estimation problems. It is written in the format $p/w$ (lower is better) with the subscripts showing the standard deviation estimated from 10 independent runs. We indicate in bold when the rejection rate minus the standard deviation is lower than the nominal level 0.05, and for the algorithm that achieves the lowest Wasserstein score. ", "page_idx": 8}, {"type": "text", "text": "5.3 Bayesian logistic regression ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "As with others (Yin and Zhou, 2018), we consider a Bayesian logistic regression problem on the waveform dataset (Breiman and Stone, 1984). The model is expressed as $y\\,|\\,x,w\\ \\sim$ Bernoulli(Sigmoid $(\\langle x,{\\overline{{w}}}\\rangle)$ ) with prior $x\\,\\sim\\,\\mathcal{N}(0,0.01^{-1}\\,\\times\\,I_{22})$ where $(y,\\pmb{w})\\;\\in\\;\\{0,1\\}\\,\\times\\mathbb{R}^{21}$ is the response and covariates, and $\\overline{{w}}\\,:=\\,[1,w]$ is the covariates with appended one for the intercept. The \u201cground truth\u201d is composed of posterior samples generated from running Markov chain Monte Carlo (MCMC) samples in Yin and Zhou (2018). We use the kernel $\\bar{k_{\\theta}}(x|z)\\;=$ $\\begin{array}{r}{\\mathcal{N}(W z+f_{\\theta}(z),\\exp(\\frac{1}{2}[M_{\\theta}+M_{\\theta}^{\\top}]))}\\end{array}$ where exp denotes the matrix exponential which ensures positive definiteness. In Fig. 3, we visually compare certain statistics of the distribution obtained from MCMC and distribution obtained from SIVI methods. Fig. 3a shows the pair-wise marginal posterior distributions for three weights $x_{1},x_{2},x_{3}$ chosen at random from MCMC and SIVI approximations; and in Fig. 3b we compare the correlation coefficients obtained from MCMC against SIVI methods. It can be seen that PVI obtains an approximation close to MCMC with most other SIVI methods obtaining similar performance levels (with the exception of SM). See App. H.3 for all the implementation details. ", "page_idx": 8}, {"type": "text", "text": "5.4 Bayesian neural networks ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Following prior works (e.g., Yu and Zhang (2023)), we compare our methods with other baselines on sampling the posterior of the Bayesian neural network for regression problems on a range of real-world datasets. We utilize the LSkip kernel $k_{\\theta}(x|z)=\\mathcal{N}(x;\\bar{W z}+f_{\\theta}(\\hat{z}),\\sigma_{\\theta}^{2}(z)I_{d_{x}})$ . In Table 2, ", "page_idx": 8}, {"type": "image", "img_path": "p3gMGkHMkM/tmp/eb10ae585d93577dfc6760eea35f1101d3c16afa54b12745bd025259b1d98b0e.jpg", "img_caption": ["Figure 3: Comparison between SIVI methods and MCMC on Bayesian logistic regression problem. (a) shows the marginal and pairwise approximations of posterior of the weights $x_{1},x_{2},x_{3}$ , and (b) shows the scatter plot of the correlation coefficient of MCMC ( $y$ -axis) vs PVI ( $x$ -axis). "], "img_footnote": ["Table 2: Root mean square error (lower is better) for Bayesian neural networks on the test set for various datasets. Here, we write the results in the form $\\mu_{\\sigma}$ where $\\mu$ is the average RMS and $\\sigma$ is its standard error computed over 10 independent trials. We indicate in bold the lowest score. "], "page_idx": 9}, {"type": "text", "text": "we show the root mean squared error on the test set. It can be seen that PVI performs well, or at least comparable, with other SIVI methods across all datasets. The details regarding the model and other parameters can be found App. H.4. ", "page_idx": 9}, {"type": "table", "img_path": "p3gMGkHMkM/tmp/2ced314a489523eb738c84b4379f60f6bb2624a4f867e88e774bd94531ad13ce.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Conclusion, Limitations, and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we frame SIVI as a minimization problem of $\\mathcal{E}_{\\lambda}$ , and then, as a solution, we study its gradient flow. Through discretization, we propose a novel algorithm called Particle Variational Inference (PVI). Our experiments found that PVI can outperform current SIVI methods. At a marginal increase in computation cost (see App. H) compared with prior methods, PVI can consistently perform better (or at least comparably in the worst cases considered) which we attribute to not imposing a particular form on the mixing distribution. This is a key advantage of PVI compared to prior methods: by not relying upon push-forward mixing distributions and instead using particles, the mixing distribution can express arbitrary distributions when the number of particles is sufficiently large. Furthermore, it is not necessary to tune the family of mixing distributions to obtain good results in particular problems. Theoretically, we study a related gradient flow of $\\mathcal{E}_{\\lambda}^{\\gamma}$ and establish desirable properties such as the existence and uniqueness of solutions and propagation of chaos results. ", "page_idx": 9}, {"type": "text", "text": "The main limitation of our work is that the theoretical results only apply to the case where $\\gamma>0$ ; yet, our experiments were performed with $\\gamma=0$ as this is when $\\mathcal{E}_{\\lambda}$ corresponds to the (regularized) evidence lower bound. In future work, one can address these limitations by reducing this gap. Furthermore, we found that certain kernels were more amenable than others when exploiting an expressive mixing distribution (e.g., the skip kernel). The question of designing these kernels for PVI (or SIVI more generally) is important for future work. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "JNL gratefully acknowledges the funding of the Feuer International Scholarship in Artificial Intelligence. AMJ acknowledges financial support from the United Kingdom Engineering and Physical Sciences Research Council (EPSRC; grants EP/R034710/1 and EP/T004134/1) and by United Kingdom Research and Innovation (UKRI) via grant EP/Y014650/1, as part of the ERC Synergy project OCEAN. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Luigi Ambrosio, Nicola Gigli, and Giuseppe Savar\u00b4e. Gradient flows: in metric spaces and in the space of probability measures. Springer Science & Business Media, 2005. ", "page_idx": 10}, {"type": "text", "text": "Michael Arbel, Anna Korba, Adil Salim, and Arthur Gretton. Maximum Mean Discrepancy gradient flow. Advances in Neural Information Processing Systems, 32, 2019. ", "page_idx": 10}, {"type": "text", "text": "Felix Biggs, Antonin Schrab, and Arthur Gretton. MMD-Fuse: Learning and combining kernels for two-sample testing without data splitting. In Advances in Neural Information Processing Systems, volume 37, 2023. ", "page_idx": 10}, {"type": "text", "text": "David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians. Journal of the American Statistical Association, 112(518):859\u2013877, 2017. ", "page_idx": 10}, {"type": "text", "text": "James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs. v. 0.3.13, 2018. URL http://github.com/google/jax. ", "page_idx": 10}, {"type": "text", "text": "Andrea Braides. Gamma-convergence for Beginners, volume 22. Clarendon Press, 2002. ", "page_idx": 10}, {"type": "text", "text": "L. Breiman and C.J. Stone. Waveform Database Generator (Version 1). UCI Machine Learning Repository, 1984. DOI: https://doi.org/10.24432/C5CS3C. ", "page_idx": 10}, {"type": "text", "text": "Rocco Caprio, Juan Kuntz, Samuel Power, and Adam M Johansen. Error bounds for particle gradient descent, and extensions of the log-Sobolev and Talagrand inequalities. e-print 2403.02004, ArXiv, 2024. ", "page_idx": 10}, {"type": "text", "text": "Ren\u00b4e Carmona. Lectures on BSDEs, stochastic control, and stochastic differential games with financial applications. SIAM, 2016. ", "page_idx": 10}, {"type": "text", "text": "Ziheng Cheng, Longlin Yu, Tianyu Xie, Shiyue Zhang, and Cheng Zhang. Kernel Semi-Implicit Variational Inference. In International Conference on Machine Learning, volume abs/2405.18997, 2024. ", "page_idx": 10}, {"type": "text", "text": "Francesca R. Crucinio, Valentin De Bortoli, Arnaud Doucet, and Adam M. Johansen. Solving a class of Fredholm integral equations of the first kind via Wasserstein gradient flows. Stochastic Processes and their Applications, 173:104374, 2024. ISSN 0304-4149. URL https://doi.org/ 10.1016/j.spa.2024.104374. ", "page_idx": 10}, {"type": "text", "text": "Gianni Dal Maso. An Introduction to \u0393-convergence, volume 8. Springer Science & Business Media, 2012. ", "page_idx": 10}, {"type": "text", "text": "Ennio De Giorgi and Tullio Franzoni. Su un tipo di convergenza variazionale. Atti Accad. Naz. Lincei Rend. Cl. Sci. Fis. Mat. Nat. (8), 58(6):842\u2013850, 1975. ISSN 0392-7881. ", "page_idx": 10}, {"type": "text", "text": "Re\u00b4mi Flamary, Nicolas Courty, Alexandre Gramfort, Mokhtar Z. Alaya, Aure\u00b4lie Boisbunon, Stanislas Chambon, Laetitia Chapel, Adrien Corenflos, Kilian Fatras, Nemo Fournier, L\u00b4eo Gautheron, Nathalie T.H. Gayraud, Hicham Janati, Alain Rakotomamonjy, Ievgen Redko, Antoine Rolet, Antony Schutz, Vivien Seguy, Danica J. Sutherland, Romain Tavenard, Alexander Tong, and Titouan Vayer. POT: Python Optimal Transport. Journal of Machine Learning Research, 22(78): 1\u20138, 2021. URL http://jmlr.org/papers/v22/20-451.html. ", "page_idx": 10}, {"type": "text", "text": "Nicolas Fournier and Arnaud Guillin. On the rate of convergence in Wasserstein distance of the empirical measure. Probability Theory and Related Fields, 162(3):707\u2013738, 2015. ", "page_idx": 11}, {"type": "text", "text": "J. Gerritsma, R. Onnink, and A. Versluis. Yacht Hydrodynamics. UCI Machine Learning Repository, 2013. DOI: https://doi.org/10.24432/C5XG7R. ", "page_idx": 11}, {"type": "text", "text": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139\u2013144, 2020. ", "page_idx": 11}, {"type": "text", "text": "Alex Graves. Stochastic backpropagation through mixture density distributions. arXiv preprint arXiv:1607.05690, 2016. ", "page_idx": 11}, {"type": "text", "text": "Stephen Hanson and Lorien Pratt. Comparing biases for minimal network construction with backpropagation. Advances in Neural Information Processing Systems, 1, 1988. ", "page_idx": 11}, {"type": "text", "text": "Roger A Horn and Charles R Johnson. Matrix Analysis. Cambridge University Press, 2012. ", "page_idx": 11}, {"type": "text", "text": "Ferenc Husza\u00b4r. Variational inference using implicit distributions. e-print 1702.08235, ArXiv, 2017. ", "page_idx": 11}, {"type": "text", "text": "Michael Irwin Jordan. Learning in Graphical Models. MIT press, 1999. ", "page_idx": 11}, {"type": "text", "text": "Richard Jordan, David Kinderlehrer, and Felix Otto. The variational formulation of the Fokker\u2013Planck equation. SIAM Journal on Mathematical Analysis, 29(1):1\u201317, 1998. ", "page_idx": 11}, {"type": "text", "text": "Juno Kim, Kakei Yamamoto, Kazusato Oko, Zhuoran Yang, and Taiji Suzuki. Symmetric Meanfield Langevin Dynamics for Distributional Minimax Problems. In Proceedings of The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/ forum?id $\\equiv$ YItWKZci78.   \nDiederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In Yoshua Bengio and Yann LeCun, editors, 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014. URL http: //arxiv.org/abs/1312.6114.   \nAnna Korba, Pierre-Cyril Aubin-Frankowski, Szymon Majewski, and Pierre Ablin. Kernel Stein discrepancy descent. In International Conference on Machine Learning, pages 5719\u20135730. PMLR, 2021.   \nAlp Kucukelbir, Dustin Tran, Rajesh Ranganath, Andrew Gelman, and David M Blei. Automatic differentiation variational inference. Journal of Machine Learning Research, 18(14):1\u201345, 2017.   \nJuan Kuntz, Jen Ning Lim, and Adam M. Johansen. Particle algorithms for maximum likelihood training of latent variable models. In Francisco Ruiz, Jennifer Dy, and Jan-Willem van de Meent, editors, Proceedings of The 26th International Conference on Artificial Intelligence and Statistics, volume 206 of Proceedings of Machine Learning Research, pages 5134\u20135180. PMLR, 25\u201327 Apr 2023. URL https://proceedings.mlr.press/v206/kuntz23a.html.   \nMarc Lambert, Sinho Chewi, Francis Bach, Silv\\`ere Bonnabel, and Philippe Rigollet. Variational inference via Wasserstein gradient flows. Advances in Neural Information Processing Systems, 35: 14434\u201314447, 2022.   \nChunyuan Li, Changyou Chen, David Carlson, and Lawrence Carin. Preconditioned stochastic gradient Langevin dynamics for deep neural networks. In Proceedings of the AAAI conference on artificial intelligence, volume 30, 2016.   \nLingxiao Li, Qiang Liu, Anna Korba, Mikhail Yurochkin, and Justin Solomon. Sampling with Mollified Interaction Energy Descent. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id $\\equiv$ zWy7dqOcel.   \nJen Ning Lim, Juan Kuntz, Samuel Power, and Adam Michael Johansen. Momentum particle maximum likelihood. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 29816\u201329871. PMLR, 21\u201327 Jul 2024. URL https://proceedings.mlr.press/v235/ lim24b.html.   \nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id= Bkg6RiCqY7.   \nJames Lucas, George Tucker, Roger Grosse, and Mohammad Norouzi. Understanding posterior collapse in generative latent variable models, 2019. URL https://openreview.net/forum? id $\\cdot$ r1xaVLUYuE.   \nShakir Mohamed, Mihaela Rosca, Michael Figurnov, and Andriy Mnih. Monte Carlo Gradient Estimation in Machine Learning. Journal of Machine Learning Research, 21(132):1\u201362, 2020. URL http://jmlr.org/papers/v21/19-346.html.   \nWarren Morningstar, Sharad Vikram, Cusuh Ham, Andrew Gallagher, and Joshua Dillon. Automatic differentiation variational inference with mixtures. In International Conference on Artificial Intelligence and Statistics, pages 3250\u20133258. PMLR, 2021.   \nPrashant Rana. Physicochemical Properties of Protein Tertiary Structure. UCI Machine Learning Repository, 2013. DOI: https://doi.org/10.24432/C5QW3H.   \nGeoffrey Roeder, Yuhuai Wu, and David K Duvenaud. Sticking the landing: Simple, lower-variance gradient estimators for variational inference. Advances in Neural Information Processing Systems, 30, 2017.   \nFrancisco R Ruiz, Titsias RC AUEB, David Blei, et al. The generalized reparameterization gradient. Advances in Neural Information Processing Systems, 29, 2016.   \nTim Salimans and David A. Knowles. Fixed-Form Variational Posterior Approximation through Stochastic Linear Regression. Bayesian Analysis, 8(4):837 \u2013 882, 2013. doi: 10.1214/13-BA858. URL https://doi.org/10.1214/13-BA858.   \nAntoine Salmona, Valentin De Bortoli, Julie Delon, and Agnes Desolneux. Can push-forward generative models fit multimodal distributions? Advances in Neural Information Processing Systems, 35:10766\u201310779, 2022.   \nFilippo Santambrogio. Optimal transport for applied mathematicians. Birk\u00a8auser, NY, 55(58-63):94, 2015.   \nSebastian M Schmon, George Deligiannidis, Arnaud Doucet, and Michael K Pitt. Large-sample asymptotics of the pseudo-marginal method. Biometrika, 108(1):37\u201351, 07 2020. ISSN 0006-3444. doi: 10.1093/biomet/asaa044. URL https://doi.org/10.1093/biomet/asaa044.   \nAlbert N. Shiryaev. Probability. Number 95 in Graduate Texts in Mathematics. Springer, New York, second edition, 1996.   \nMatthew Staib, Sashank Reddi, Satyen Kale, Sanjiv Kumar, and Suvrit Sra. Escaping saddle points with adaptive gradient methods. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 5956\u20135965. PMLR, 09\u201315 Jun 2019. URL https:// proceedings.mlr.press/v97/staib19a.html.   \nTijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop, coursera: Neural networks for machine learning. University of Toronto, Technical Report, 6, 2012.   \nMichalis K. Titsias and Francisco Ruiz. Unbiased implicit variational inference. In Kamalika Chaudhuri and Masashi Sugiyama, editors, Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics, volume 89 of Proceedings of Machine Learning Research, pages 167\u2013176. PMLR, 16\u201318 Apr 2019. URL https://proceedings.mlr.press/ v89/titsias19a.html.   \nMartin J Wainwright, Michael I Jordan, et al. Graphical models, exponential families, and variational inference. Foundations and Trends\u00ae in Machine Learning, 1(1\u20132):1\u2013305, 2008.   \nYixin Wang, David Blei, and John P Cunningham. Posterior collapse and latent variable nonidentifiability. Advances in Neural Information Processing Systems, 34:5443\u20135455, 2021.   \nI-Cheng Yeh. Concrete Compressive Strength. UCI Machine Learning Repository, 2007. DOI: https://doi.org/10.24432/C5PK67.   \nMingzhang Yin and Mingyuan Zhou. Semi-implicit variational inference. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 5660\u20135669. PMLR, 10\u201315 Jul 2018. URL https://proceedings.mlr.press/v80/yin18b.html.   \nLonglin Yu and Cheng Zhang. Semi-implicit variational inference via score matching. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/ forum?id $\\equiv$ sd90a2ytrt. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Related work ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we outline four areas of related work: semi-implicit variational inference; EuclideanWasserstein gradient flows and Wasserstein-gradient flows in VI; mixture models in VI; and finally, the link between SIVI and solving Fredholm equations of the first kind. ", "page_idx": 14}, {"type": "text", "text": "At the time of writing, there are three algorithms for SIVI proposed: SVI Yin and Zhou (2018), UVI Titsias and Ruiz (2019), SM Yu and Zhang (2023). Concurrently, Cheng et al. (2024) extended the SM variant by solving the inner minimax objective and simplified the optimization problem. Each had their parameterization of SID (as discussed in Section 2), and their proposed optimization method. SVI relies on optimizing a bound of the ELBO which is asymptotically tight. UVI, like our approach, optimizes the ELBO by using gradients-based approaches. However, one of its terms is the score $\\nabla_{x}\\log q_{\\theta,r}(x)$ which is intractable. The authors proposed using expensive MCMC chains to estimate it; in contrast to PVI, this term is readily available to us. For SM, they propose to optimize the Fisher divergence, however, to deal with the intractabilities the resulting objective is a minimax optimization problem which is difficult to optimize compared to standard minimization problems. ", "page_idx": 14}, {"type": "text", "text": "PVI utilizes the Euclidean\u2013Wasserstein geometry. This geometry and associated gradient flows are initially explored in the context of (marginal) maximum likelihood estimation by Kuntz et al. (2023) and their convergence properties are investigated by Caprio et al. (2024). In Lim et al. (2024), the authors investigated accelerated gradient variants of the aforementioned gradient flow in Euclidean\u2013 Wasserstein geometry. The Wasserstein geometry particularly for gradient flows on probability space has received much attention with many works exploring different functionals (for examples, see Arbel et al. (2019); Korba et al. (2021); Li et al. (2023)). In the context of variational inference Lambert et al. (2022) analyzed VI as a Bures\u2013Wasserstein Gradient flow on the space of Gaussian measures. ", "page_idx": 14}, {"type": "text", "text": "PVI is reminiscent of mixture distributions which is a consequence of the particle discretization. Mixture models have been studied in prior works as variational distributions(Graves, 2016; Morningstar et al., 2021). In Graves (2016), the authors extended the parameterization trick to mixture distributions; and Morningstar et al. (2021) proposed to utilize mixture models as variational distributions in the framework of Kucukelbir et al. (2017). Although similar, the mixing distribution assists the kernel in expressing complex properties of the true distribution at hand (see Section 5.1) which is an interpretation that mixture distribution lacks. ", "page_idx": 14}, {"type": "text", "text": "There is an obvious similarity between SIVI and solving Fredholm equations of the first kind. There is considerable literature on solving such problems; see Crucinio et al. (2024), which is closest in spirit to the approach of the present paper, and references therein. In fact, writing $\\begin{array}{r}{p(\\cdot|y)=\\int\\tilde{k}(\\cdot|z,\\theta)r(z)\\mathrm{d}z}\\end{array}$ . with $\\tilde{k}(\\cdot|z,\\theta)\\:\\equiv\\:k_{\\theta}(\\cdot|z)$ makes the connection more explicit: essentially, one seeks to solve a nonstandard Fredholm equation, with the LHS known only up to a normalizing constant, constraining the solution to be in $\\bar{\\mathcal{P}(\\mathcal{Z})}\\times\\{\\delta_{\\theta}:\\theta\\in\\Theta\\}$ . While Crucinio et al. (2024) develop and analyse a simple Wasserstein gradient flow to address a regularised Fredholm equation, neither the method nor analysis can be applied to the SIVI problem because of this non-trivial constraint. ", "page_idx": 14}, {"type": "text", "text": "B $\\Gamma$ -convergence ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The following is one of many essentially equivalent definitions of $\\Gamma$ -convergence (see Dal Maso (2012); Braides (2002) for comprehensive summaries of $\\Gamma$ -convergence). We take as definition the following (see Dal Maso (2012, Proposition 8.1), Braides (2002, Definition 1.5)): ", "page_idx": 14}, {"type": "text", "text": "Definition B.1 ( $\\Gamma$ -convergence). Assume that $\\mathcal{M}$ is a topological space that satisfies the first axiom of countability. Then a sequence $\\mathcal{F}_{\\gamma}:\\mathcal{M}\\rightarrow\\mathbb{R}$ is said to $\\Gamma$ -converge to $\\mathcal{F}$ if: ", "page_idx": 14}, {"type": "text", "text": "\u2022 (lim-inf inequality) for every sequence $(\\theta_{\\gamma},r_{\\gamma})\\in\\mathcal{M}$ converging to $(\\theta,r)\\in\\mathcal{M}$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\gamma\\to0}\\operatorname*{inf}_{}{\\mathcal{F}}_{\\gamma}(\\theta_{\\gamma},r_{\\gamma})\\geq{\\mathcal{F}}(\\theta,r).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "\u2022 (lim-sup inequality) for any $(\\theta,r)\\in\\mathcal{M}$ , there exists a sequence $(\\theta_{\\gamma},r_{\\gamma})\\in\\mathcal{M}$ , known as a recovery sequence, converging to $(\\theta,r)$ which satisfies ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\gamma\\to0}\\mathcal{F}_{\\gamma}(\\theta_{\\gamma},r_{\\gamma})\\leq\\mathcal{F}(\\theta,r).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "$\\Gamma$ -convergence corresponds, roughly speaking, to the convergence of the lower semicontinuous envelope of a sequence of functionals and, under mild further regularity conditions such as equicoercivity, is sufficient to ensure the convergence of the sets of minimisers of those functionals to the set of minimisers of the limit functional. ", "page_idx": 15}, {"type": "text", "text": "C On Assumptions 2 and 3 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We shall show that the Gaussian kernel $k_{\\theta}(x|z)=\\mathcal{N}(x;\\mu_{\\theta}(z),\\Sigma)$ , i.e., ", "page_idx": 15}, {"type": "equation", "text": "$$\nk_{\\theta}(x|z)=(2\\pi)^{-d_{x}/2}\\mathrm{det}(\\Sigma)^{-0.5}\\exp\\left(-\\frac{1}{2}(x-\\mu_{\\theta}(z))^{T}\\Sigma^{-1}(x-\\mu_{\\theta}(z))\\right),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\mu_{\\theta}:\\mathbb{R}^{d_{z}}\\,\\mapsto\\,\\mathbb{R}^{d_{x}}$ ; and $\\Sigma\\,\\in\\,\\mathbb{R}^{d_{x}\\times d_{x}}$ and is positive definite. In this section, we show that Assumptions 2 and 3 are implied by Assumptions 4 and 5. ", "page_idx": 15}, {"type": "text", "text": "Assumption 4. $\\mu_{\\theta}$ is bounded and $\\Sigma$ is positive definite: there exists $B_{\\mu}\\,\\in\\,\\mathbb{R}_{>0}$ such that the following holds for all $(\\theta,z)\\in\\Theta\\times\\mathbb{R}^{d_{z}}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|\\nabla_{(\\theta,z)}\\mu_{\\theta}(z)\\|_{F}\\leq B_{\\mu},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and for any $x\\in\\mathbb{R}^{d_{x}}\\setminus\\{0,x^{T}\\Sigma x>0$ . ", "page_idx": 15}, {"type": "text", "text": "Assumption 5. $\\mu_{\\theta}$ is Lipschitz and has Lipschitz gradient, i.e., there exist constants $K_{\\mu}\\in\\mathbb{R}_{>0}$ such that for all $(\\theta,z),(\\theta^{\\prime},z^{\\prime})\\in\\Theta\\times\\mathbb{R}^{d_{z}}$ the following hold: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mu_{\\theta}(z)-\\mu_{\\theta^{\\prime}}(z^{\\prime})\\|\\le K_{\\mu}\\|(\\theta,z)-(\\theta^{\\prime},z^{\\prime})\\|,}\\\\ {\\|\\nabla_{(\\theta,z)}\\mu_{\\theta}(z)-\\nabla_{(\\theta,z)}\\mu_{\\theta^{\\prime}}(z^{\\prime})\\|_{F}\\le K_{\\mu}\\|(\\theta,z)-(\\theta^{\\prime},z^{\\prime})\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "C.1 $k_{\\theta}$ satisfies Assumption 2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we show that $k_{\\theta}$ satisfies Assumption 2. We first show the boundedness property then the Lipschitz property. ", "page_idx": 15}, {"type": "text", "text": "Boundedness. First, we shall show that $k_{\\theta}$ is bounded. Clearly, we have $k_{\\theta}(x|z)\\ \\ \\in$ $\\left[0,(2\\pi)^{-d_{x}/2}\\mathrm{det}(\\Sigma)^{-0.5}\\right]$ hence $|k_{\\theta}|$ is bounded as a consequence of Assumption 4. Now to show that the gradient is bounded $||\\nabla_{(\\theta,x,z)}k_{\\theta}(x|z)||$ , we have the following ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{x}k_{\\theta}(x|z)=-k_{\\theta}(x|z)\\Sigma^{-1}(x-\\mu_{\\theta}(z)),}\\\\ &{\\nabla_{z}k_{\\theta}(x|z)=\\,\\nabla_{z}\\mu_{\\theta}(z)\\nabla_{\\mu}{\\mathcal{N}(x;\\mu,\\sigma^{2}I_{d_{x}})}\\mid_{\\mu_{\\theta}(z)},}\\\\ &{\\nabla_{\\theta}k_{\\theta}(x|z)=\\,\\nabla_{\\theta}\\mu_{\\theta}(z)\\nabla_{\\mu}{\\mathcal{N}(x;\\mu,\\sigma^{2}I_{d_{x}})}\\big|_{\\mu_{\\theta}(z)}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Hence, we have $\\|\\nabla_{(x,\\mu,\\sigma)}\\mathcal{N}(x;\\mu_{\\theta}(z),\\sigma^{2}I_{d_{x}})\\|<\\infty$ , from Assumption 4 and using the fact the gradient of a Gaussian density of given covariance w.r.t. $\\mu$ is uniformly bounded. Thus, we have shown that $k_{\\theta}$ satisfies the boundedness property in Assumption 2. ", "page_idx": 15}, {"type": "text", "text": "Lipschitz. For $k_{\\theta}$ , one choice of coupling function and noise distribution is $\\phi_{\\theta}(z,\\epsilon)=\\Sigma^{\\frac{1}{2}}\\epsilon+\\mu_{\\theta}(z)$ and $p_{k}=\\mathcal{N}(0,I_{d_{x}})$ where $\\sum{\\textstyle{\\frac{1}{2}}}$ be the unique symmetric and positive definite matrix with $\\textstyle(\\sum^{\\frac{1}{2}})^{2}=\\Sigma$ (Horn and Johnson, 2012, Theorem 7.2.6); and the inverse map is $\\phi_{\\theta}^{-1}(z,x)=\\Sigma^{-\\frac{1}{2}}(x-\\mu_{\\theta}(z))$ . Thus, from the change-of-variables formula, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{x}k_{\\theta}(x|z)=\\nabla_{x}[p_{k}(\\phi_{\\theta}^{-1}(z,x))\\mathrm{det}(\\nabla_{x}\\phi_{\\theta}^{-1}(z,x))]}\\\\ &{\\qquad\\qquad\\quad=\\mathrm{det}(\\nabla_{x}\\phi_{\\theta}^{-1}(z,x))\\nabla_{x}[p_{k}\\left(\\phi_{\\theta}^{-1}(z,x)\\right)]}\\\\ &{\\qquad\\qquad\\quad=\\mathrm{det}(\\Sigma^{-1/2})\\Sigma^{-1/2}\\nabla_{x}p_{k}(\\phi_{\\theta}^{-1}(z,x))}\\\\ &{\\qquad\\qquad\\quad=\\tilde{\\Sigma}^{-\\frac{1}{2}}\\nabla_{x}p_{k}(\\phi_{\\theta}^{-1}(z,x))}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\tilde{\\Sigma}^{-\\frac{1}{2}}:=\\operatorname*{det}(\\Sigma^{-1/2})\\Sigma^{-1/2}$ . Thus, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{x}k_{\\theta}(x|z)-\\nabla_{x}k_{\\theta^{\\prime}}(x^{\\prime}|z^{\\prime})\\|}\\\\ &{\\le\\|\\tilde{\\Sigma}^{-\\frac{1}{2}}\\nabla_{x}p_{k}(\\phi_{\\theta}^{-1}(z,x))-\\tilde{\\Sigma}^{-\\frac{1}{2}}\\nabla_{x}p_{k}(\\phi_{\\theta^{\\prime}}^{-1}(z^{\\prime},x^{\\prime}))\\|}\\\\ &{\\le\\|\\tilde{\\Sigma}^{-\\frac{1}{2}}\\|_{F}\\|\\nabla_{x}p_{k}(\\phi_{\\theta}^{-1}(z,x))-\\nabla_{x}p_{k}(\\phi_{\\theta^{\\prime}}^{-1}(z^{\\prime},x^{\\prime}))\\|}\\\\ &{\\le C\\|\\phi_{\\theta}^{-1}(z,x)-\\phi_{\\theta^{\\prime}}^{-1}(z^{\\prime},x^{\\prime})\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $C$ is a constant and we use the following facts: $\\begin{array}{r}{\\|\\tilde{\\Sigma}^{-\\frac{1}{2}}\\|_{F}\\leq|\\operatorname*{det}(\\Sigma^{-1/2})|\\|\\Sigma^{-1/2}\\|_{F}<\\infty}\\end{array}$ following from the fact $\\Sigma^{-1/2}$ is positive definite; $p_{k}$ is a standard Gaussian density function with Lipschitz gradients; and that the inverse map $\\phi^{-1}$ is Lipschitz from Assumptions 4 and 5: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\phi_{\\theta}^{-1}(z,x)-\\phi_{\\theta^{\\prime}}^{-1}(z^{\\prime},x^{\\prime})\\|\\le\\|\\Sigma^{-\\frac{1}{2}}\\|_{F}\\|(x,\\mu_{\\theta}(z))-(x^{\\prime},\\mu_{\\theta^{\\prime}}(z^{\\prime}))\\|\\le C^{\\prime}\\|(x,\\theta,z)-(x^{\\prime},\\theta^{\\prime},z^{\\prime})\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Hence, we have shown that $k_{\\theta}$ satisfies the Lipschitz property of Assumption 2, and so Assumption 2 holds for $k_{\\theta}$ . ", "page_idx": 16}, {"type": "text", "text": "C.2 $k_{\\theta}$ satisfies Assumption 3 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "One can compute the gradient as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\nabla_{(\\theta,z)}\\phi_{\\theta}(z,\\epsilon)=\\nabla_{(\\theta,z)}\\mu_{\\theta}(z),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and hence $\\|\\nabla_{(\\theta,z)}\\phi_{\\theta}(z,\\epsilon)\\|_{F}$ is bounded from Assumption 4. The Lipschitz gradient property is immediate from Assumption 5. ", "page_idx": 16}, {"type": "text", "text": "$p_{k}$ has finite second moments since it is a Gaussian. ", "page_idx": 16}, {"type": "text", "text": "D Proofs in Section 2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof of Prop. $^{\\,I}$ . We start by showing $\\mathcal{Q}_{\\mathtt{Y u Z}}=\\mathcal{Q}_{\\mathtt{T R}}$ . To this end, we begin by showing the inclusion $\\mathcal{Q}_{\\mathtt{Y u Z}}\\subseteq\\,\\mathcal{Q}_{\\mathtt{T R}}$ , i.e., ${\\mathcal{Q}}(K_{{\\mathcal{F}};\\phi,p_{k}},{\\mathcal{R}}_{{\\mathcal{G}},p_{r}})\\,\\subseteq\\,{\\mathcal{Q}}(K_{{\\mathcal{F}}\\circ{\\mathcal{G}};\\phi,p_{k}},\\{p_{r}\\})$ . Let $q\\,\\in\\,\\mathcal{Q}(K_{\\mathcal{F};\\phi,p_{k}},\\mathcal{R}_{\\mathcal{G},p_{r}})$ , then there is some $f\\in\\mathcal F$ and $g\\in{\\mathcal{G}}$ such that $q=q_{k_{f};\\phi,p_{k}},g_{\\#}p_{r}$ . From straight-forward computation, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{q_{k_{f};\\phi,\\boldsymbol{p}_{k},\\boldsymbol{g}_{\\#}\\boldsymbol{p}_{r}}=\\mathbb{E}_{z\\sim\\boldsymbol{g}_{\\#}\\boldsymbol{p}_{r}}[k_{f;\\phi,\\boldsymbol{p}_{k}}(\\cdot|\\boldsymbol{z})]\\overset{(a)}{=}\\mathbb{E}_{z\\sim\\boldsymbol{p}_{r}}[k_{f;\\phi,\\boldsymbol{p}_{k}}(\\cdot|\\boldsymbol{g}(\\boldsymbol{z}))]\\in\\mathcal{Q}(K_{\\mathcal{F}\\circ\\mathcal{G};\\phi,\\boldsymbol{p}_{k}},\\{\\boldsymbol{p}_{r}\\}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where (a) follows the law of the unconscious statistician, and the last element-of follows from the fact that $k_{f;\\phi,p_{k}}(\\cdot|g(\\epsilon)))=\\phi(f\\circ g(\\epsilon),\\cdot)_{\\#}p_{k}\\in K_{\\mathcal{F}\\circ\\mathcal{G};\\phi,p_{k}}$ . We can follow the argument above in reverse to obtain the reverse inclusion. Hence, we have obtained as desired. ", "page_idx": 16}, {"type": "text", "text": "That $\\mathcal{Q}_{\\mathtt{Y u Z}}\\;=\\;\\mathcal{Q}_{\\mathtt{Y i Z}}$ , follows in a similar manner, which we shall outline for completeness: let $q\\in\\mathcal{Q}(K_{\\mathcal{F};\\phi,p_{k}},\\mathcal{R}_{\\mathcal{G},p_{r}})$ , then ", "page_idx": 16}, {"type": "equation", "text": "$$\nq=q_{k_{f;\\phi,p_{k}},g_{\\#}p_{r}}=\\mathbb{E}_{z\\sim g_{\\#}p_{r}}[k_{f;\\phi,p_{k}}(\\cdot|z)]=\\mathbb{E}_{z\\sim f\\circ g_{\\#}p_{r}}[k_{\\phi,p_{k}}(\\cdot|z)]\\in\\mathcal{Q}_{\\Upsilon\\mathrm{i}2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "One can conclude by applying the same logic in the reverse direction. ", "page_idx": 16}, {"type": "text", "text": "E Proofs in Section 3 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "E.1 Proof of Prop. 2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof of Prop. 2. ( $\\mathcal{E}$ is lower bounded). Clearly, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{E}(\\theta,r)=\\mathsf{K L}(q_{\\theta,r},p(\\cdot|y))-\\log p(y)\\geq-\\log p(y),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Hence, we have $\\mathcal{E}(\\theta,r)\\in[-\\log p(y),\\infty)$ which is lower bounded by our assumption. ", "page_idx": 16}, {"type": "text", "text": "( $\\mathcal{E}$ is lower semi-continuous). Let $(\\theta_{n},r_{n})_{n\\in\\mathbb{N}}$ be such that $\\operatorname*{lim}_{n\\to\\infty}r_{n}=r$ and $\\operatorname*{lim}_{n\\to\\infty}\\theta_{n}=\\theta$ . ", "page_idx": 16}, {"type": "text", "text": "We can split the domain of integration, and write $\\mathcal{E}$ equivalently as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{E}(\\theta,r)=\\int\\underbrace{\\mathbb{1}_{[1,\\infty)}\\left(\\frac{p(x,y)}{q_{\\theta,r}(x)}\\right)\\log\\left(\\frac{q_{\\theta,r}(x)}{p(x,y)}\\right)}_{\\le0}q_{\\theta},r(x)\\,\\mathrm{d}x}\\\\ &{\\qquad+\\int\\underbrace{\\mathbb{1}_{[0,1)}\\left(\\frac{p(x,y)}{q_{\\theta,r}(x)}\\right)\\log\\left(\\frac{q_{\\theta,r}(x)}{p(x,y)}\\right)q_{\\theta,r}(x)\\,\\mathrm{d}x}_{\\ge0}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We shall focus on the RHS of (18). ", "page_idx": 16}, {"type": "text", "text": "Note that we have the following bound ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~\\left|-\\mathbb{1}_{[1,\\infty)}\\left(\\frac{p(x,y)}{q_{\\theta_{n},r_{n}}(x)}\\right)\\log\\left(\\frac{q_{\\theta_{n},r_{n}}(x)}{p(x,y)}\\right)q_{\\theta_{n},r_{n}}(x)\\right|}\\\\ &{\\leq\\operatorname*{max}\\left\\{0,\\log\\left(\\frac{p(x,y)}{q_{\\theta_{n},r_{n}}(x)}\\right)q_{\\theta_{n},r_{n}}(x)\\right\\}\\leq\\operatorname*{max}\\{0,C\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $C$ is some constant. The last inequality follows from the fact that the evidence is bounded from above and the kernel is bounded. We can apply Reverse Fatou\u2019s Lemma to obtain ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\underset{n\\to\\infty}{\\operatorname*{lim}\\operatorname*{sup}}-\\int\\mathbb{1}_{[1,\\infty)}\\left(\\frac{p(x,y)}{q_{\\theta_{n},r_{n}}(x)}\\right)\\log\\left(\\frac{q_{\\theta_{n},r_{n}}(x)}{p(x,y)}\\right)q_{\\theta_{n},r_{n}}(x)\\,\\mathrm{d}x}\\\\ &{}&{\\qquad\\qquad\\qquad\\qquad\\qquad\\le\\int\\underset{n\\to\\infty}{\\operatorname*{lim}\\operatorname*{sup}}\\left(-\\mathbb{1}_{[1,\\infty)}\\left(\\frac{p(x,y)}{q_{\\theta_{n},r_{n}}(x)}\\right)\\log\\left(\\frac{q_{\\theta_{n},r_{n}}(x)}{p(x,y)}\\right)q_{\\theta_{n},r_{n}}(x)\\right)\\,\\mathrm{d}x.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since we have the following relationships ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}}\\,\\mathrm{~l_{[1,\\infty)}~}\\!\\left(\\frac{p(x,y)}{q_{\\theta_{n},r_{n}}(x)}\\right)\\leq\\mathbb{1}_{[1,\\infty)}\\left(\\frac{p(x,y)}{q_{\\theta,r}(x)}\\right),}&\\\\ &{\\quad\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}}-\\log\\left(\\frac{p(x,y)}{q_{\\theta_{n},r_{n}}(x)}\\right)=-\\log\\left(\\frac{p(x,y)}{q_{\\theta,r}(x)}\\right),}&\\\\ &{\\quad\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}}\\,q_{\\theta_{n},r_{n}}=q_{\\theta,r}\\mathrm{~pointwise},}&\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the first line is from u.s.c. of $\\mathbb{1}_{[1,\\infty)}$ ; the second line from the continuity of log; the final line follows from the bounded kernel $k$ assumption and dominated convergence theorem. ", "page_idx": 17}, {"type": "text", "text": "Thus, we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{lim}_{n\\to\\infty}-\\int\\mathbb{1}_{[1,\\infty)}\\left(\\frac{p(x,y)}{q_{\\theta_{n},r_{n}}(x)}\\right)\\log\\left(\\frac{q_{\\theta_{n},r_{n}}(x)}{p(x,y)}\\right)q_{\\theta_{n},r_{n}}(x)\\,\\mathrm{d}x}\\\\ &{\\leq-\\int\\mathbb{1}_{[1,\\infty)}\\left(\\frac{p(x,y)}{q_{\\theta,r}(x)}\\right)\\log\\left(\\frac{q_{\\theta,r}(x)}{p(x,y)}\\right)q_{\\theta,r}(x)\\,\\mathrm{d}x,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Using the fact that l $\\dim\\operatorname*{sup}_{n\\to\\infty}-x_{n}=-\\operatorname*{lim}\\operatorname*{inf}_{n\\to\\infty}x_{n}$ , we have shown that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad-\\operatorname*{liminf}_{n\\to\\infty}\\int\\mathbb{1}_{[1,\\infty)}\\left(\\frac{p(x,y)}{q_{\\theta_{n},r_{n}}(x)}\\right)\\log\\left(\\frac{q_{\\theta_{n},r_{n}}(x)}{p(x,y)}\\right)q_{\\theta_{n},r_{n}}(x)\\,\\mathrm{d}x}\\\\ &{\\leq-\\int\\mathbb{1}_{[1,\\infty)}\\left(\\frac{p(x,y)}{q_{\\theta,r}(x)}\\right)\\log\\left(\\frac{q_{\\theta,r}(x)}{p(x,y)}\\right)q_{\\theta,r}(x)\\,\\mathrm{d}x.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Similarly, for the RHS of (19), using Fatou\u2019s Lemma (with varying measure and the set-wise convergence of $q_{\\theta_{n},r_{n}})$ and using the l.s.c. of $\\mathbb{1}_{[0,1)}$ , we obtain that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{n\\to\\infty}{\\operatorname*{lim}\\operatorname*{inf}}\\int\\mathbb{1}_{[0,1)}\\left(\\frac{p(x,y)}{q_{\\theta_{n},r_{n}}(x)}\\right)\\log\\left(\\frac{q_{\\theta_{n},r_{n}}(x)}{p(x,y)}\\right)q_{\\theta_{n},r_{n}}(x)\\,\\mathrm{d}x}\\\\ &{\\geq\\int\\mathbb{1}_{[0,1)}\\left(\\frac{p(x,y)}{q_{\\theta,r}(x)}\\right)\\log\\left(\\frac{q_{\\theta,r}(x)}{p(x,y)}\\right)q_{\\theta,r}(x)\\,\\mathrm{d}x.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Hence, combining the bounds (20) and (21), we have that shown that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{n\\to\\infty}{\\operatorname*{lim}\\operatorname*{inf}}\\,\\mathcal{E}(\\theta_{n},r_{n})=\\underset{n\\to\\infty}{\\operatorname*{lim}\\operatorname*{inf}}\\int\\log\\left(\\frac{q_{\\theta_{n},r_{n}}(x)}{p(x,y)}\\right)q_{\\theta_{n},r_{n}}(x)\\,\\mathrm{d}x}\\\\ &{\\geq\\int\\log\\left(\\frac{q_{\\theta,r}(x)}{p(x,y)}\\right)q_{\\theta,r}(x)\\,\\mathrm{d}x\\geq\\mathcal{E}(\\theta,r).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In other words, $\\mathcal{E}$ is lower semi-continuous. ", "page_idx": 17}, {"type": "text", "text": "(Non-Coercivity) To show non-coercivity, we will show that there exists some level set $\\{(\\theta,r):$ ${\\mathcal{E}}(\\theta,r)\\leq\\beta\\}$ that is not compact. We do this by finding a sequence contained in the level set that does not contain a (weakly) converging subsequence. ", "page_idx": 17}, {"type": "text", "text": "Consider the sequence $\\Pi\\,:=\\,(\\theta_{n},r_{n})_{n\\in\\mathbb{N}}$ where $\\theta_{n}\\ =\\ \\theta_{0}$ ; $\\|\\theta_{0}\\|\\,<\\,\\infty$ ; $r_{n}~=~\\delta_{n}$ ; $k_{\\theta}(x|z)\\;=\\;$ $\\mathcal{N}(x;\\theta,I_{d_{x}})$ ; and $p(x|y)\\ =\\ \\dot{{\\mathcal N}}(x;0,I_{d_{x}})$ . Clearly, we have $\\dot{q}_{\\theta,r}(x)\\ =\\ \\mathcal{N}(x;\\theta,I_{d_{x}})$ and so $\\begin{array}{r}{{\\sf K L}(q_{\\theta,r},p(\\cdot|y))=\\frac{1}{2}\\|\\theta\\|^{2}}\\end{array}$ . Hence, there is a $\\beta<\\infty$ such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{E}(\\theta_{n},r_{n})=\\mathsf{K L}(q_{\\theta_{n},r_{n}},p(\\cdot|y))-\\log p(y)\\leq\\frac{1}{2}\\|\\theta_{0}\\|^{2}-\\log p(y)\\leq\\beta.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus, we have shown that $\\Pi\\subset\\{(\\theta,r):\\mathcal{E}(\\theta,r)\\leq\\beta\\}$ . However, since the support of the elements of $\\{r_{n}\\in\\mathcal{P}(\\mathbb{R}^{d_{z}})\\}_{n}$ eventually lies outside a ball of radius $R$ for any $R<\\infty$ and hence of any compact set, $\\Pi$ is not tight. Hence, Prokhorov\u2019s theorem (Shiryaev, 1996, p. 318) tells us that, as $\\Pi$ is not tight, it is not relatively compact. We conclude that, as the level set is not relatively compact, the functional is not-coercive. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "E.2 Proof of Prop. 3 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof of Prop. 3. (Coercivity) Consider the level set $\\{(\\theta,r):\\mathcal{E}_{\\lambda}(\\theta,r)\\leq\\beta\\}$ , which is contained in a relatively compact set. To see this, first note that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\{(\\theta,r):\\mathcal{E}_{\\lambda}(\\theta,r)\\leq\\beta\\}\\subseteq\\{(\\theta,r):-\\log p(y)+\\mathsf{R}_{\\lambda}(\\theta,r)\\leq\\beta\\}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\{(\\theta,r):\\mathsf{R}_{\\lambda}(\\theta,r)\\leq\\beta+\\log p(y)\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By coercivity of $\\mathsf{R}_{\\lambda}$ , i.e., the above level set is relatively compact hence $\\mathcal{E}_{\\lambda}$ is coercive. ", "page_idx": 18}, {"type": "text", "text": "(Lower semi-continuity) Lower semi-continuity (l.s.c.) follows immediately from the l.s.c. of $\\mathcal{E}$ and $\\mathsf{R}_{\\lambda}$ . ", "page_idx": 18}, {"type": "text", "text": "(Existence of a minimizer) The existence of a minimizer follows from Dal Maso (2012, Theorem 1.15) utilizing coercivity and l.s.c. of $\\mathcal{E}_{\\lambda}$ . \u53e3 ", "page_idx": 18}, {"type": "text", "text": "E.3 Proof of Prop. 4 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Recall from Santambrogio (2015, Definition 7.12), ", "page_idx": 18}, {"type": "text", "text": "Definition E.1 (First Variation). If $p$ is regular for $F$ , the first variation of $F:\\mathcal{P}(\\mathbb{R}^{d_{z}})\\rightarrow\\mathbb{R}$ , if it exists, is the element that satisfies ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\epsilon\\to0}\\frac{F(p+\\epsilon\\chi)-F(p)}{\\epsilon}=\\int\\delta_{r}F[r](z)\\chi(\\mathrm{d}z),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for any perturbation $\\chi=\\tilde{p}-p$ with $\\widetilde{p}\\in\\mathcal{P}(\\mathbb{R}^{d_{z}})\\cap L_{c}^{\\infty}(\\mathbb{R}^{d_{z}})$ (see Santambrogio (2015, Notation)). $\\mathcal{E}_{\\lambda}^{\\gamma}$ ", "page_idx": 18}, {"type": "text", "text": "One can decompose the first variation of as: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\delta_{r}\\mathcal{E}_{\\lambda}^{\\gamma}[\\theta,r]=\\delta_{r}\\mathcal{E}^{\\gamma}[\\theta,r]+\\delta_{r}\\mathsf{R}_{\\lambda}^{\\mathrm{E}}[\\theta,r].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where E\u03b3 : (\u03b8, r)  \u2192 log q\u03b8,pr((xx,)y)+\u03b3 . Since $\\delta_{r}\\mathsf{R}_{\\lambda}^{\\mathrm{E}}[\\theta,r]=\\lambda_{r}\\delta_{r}\\mathrm{KL}(r|p_{0})$ , its first variation follows immediately from standard calculations (Ambrosio et al., 2005; Santambrogio, 2015). As for $\\delta_{r}\\mathcal{E}^{\\gamma}$ , we have the following proposition: ", "page_idx": 18}, {"type": "text", "text": "Proposition 10 (First Variation of $\\mathcal{E}^{\\gamma}$ ). Assume that for all $(\\theta,r,z)\\in\\mathcal{M}\\times\\mathbb{R}^{d_{z}}$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}_{k_{\\theta}(X|z)}\\left|\\mathrm{log}\\left(\\frac{q_{\\theta,r}(X)+\\gamma}{p(X,y)}\\right)\\right|<\\infty,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "then we obtain ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\delta_{r}\\mathcal{E}^{\\gamma}[\\theta,r](z)=\\mathbb{E}_{k_{\\theta}(X\\mid z)}\\left[\\log\\left(\\frac{q_{\\theta,r}(X)+\\gamma}{p(X,y)}\\right)+\\frac{q_{\\theta,r}(X)}{q_{\\theta,r}(X)+\\gamma}\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. Since $\\begin{array}{r}{q_{\\theta,r+\\epsilon\\chi}=\\int k_{\\theta}(\\cdot|z)(r+\\epsilon\\chi)(z)\\,\\mathrm{d}z=q_{\\theta,r}+\\epsilon q_{\\theta,\\chi}}\\end{array}$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathcal{E}^{\\gamma}(\\theta,r+\\epsilon\\chi)=\\int_{\\mathcal{X}}q_{\\theta,r+\\epsilon\\chi}(x)\\log\\left(\\frac{q_{\\theta,r+\\epsilon\\chi}(x)+\\gamma}{p(y,x)}\\right)\\,\\mathrm{d}x}\\\\ {\\displaystyle=\\int_{X}[q_{\\theta,r}+\\epsilon q_{\\theta,\\chi}](x)\\log([q_{\\theta,r}+\\epsilon q_{\\theta,\\chi}](x)+\\gamma)\\,\\mathrm{d}x}\\\\ {\\displaystyle\\qquad-\\int_{X}[q_{\\theta,r}+\\epsilon q_{\\theta,\\chi}](x)\\log p(y,x)\\,\\mathrm{d}x.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Applying Taylor\u2019s expansion, we obtain $\\begin{array}{r c l}{(x~+~\\epsilon y)\\log(x~+~\\epsilon y~+~\\gamma)}&{=}&{x\\log(x~+~\\gamma)~+}\\end{array}$ $\\begin{array}{r}{\\epsilon\\dot{y}\\left(\\log(\\dot{x}+\\dot{\\gamma})+\\frac{x}{x+\\gamma}\\right)+o(\\epsilon)}\\end{array}$ , we obtain ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathcal{E}^{\\gamma}(\\theta,r+\\epsilon\\chi)=\\int_{\\mathcal{X}}q_{\\theta,r}(x)\\log\\frac{q_{\\theta,r}(x)+\\gamma}{p(y,x)}\\,\\mathrm{d}x}\\\\ {\\displaystyle\\qquad\\qquad\\qquad+\\,\\epsilon\\int_{\\mathcal{X}}q_{\\theta,\\chi}(x)\\left[\\log\\left(\\frac{q_{\\theta,r}(x)+\\gamma}{p(y,x)}\\right)+\\frac{q_{\\theta,r}(x)}{q_{\\theta,r}(x)+\\gamma}\\right]\\,\\mathrm{d}x+o(\\epsilon).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Hence, we obtain ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{\\mathrm{in}}{\\to}\\frac{\\mathcal{E}^{\\gamma}(\\theta,r+\\epsilon\\chi)-\\mathcal{E}^{\\gamma}(\\theta,r)}{\\epsilon}=\\int_{\\mathcal{X}}q_{\\theta,\\chi}(x)\\left[\\log\\frac{q_{\\theta,r}(x)+\\gamma}{p(y,x)}+\\frac{q_{\\theta,r}(x)}{q_{\\theta,r}(x)+\\gamma}\\right]\\,\\mathrm{d}x}}\\\\ &{}&{=\\int_{\\mathcal{X}}\\left[\\int_{\\mathcal{Z}}k_{\\theta}(x|z)\\chi(\\mathrm{d}z)\\right]\\left[\\log\\left(\\frac{q_{\\theta,r}(x)+\\gamma}{p(y,x)}\\right)+\\frac{q_{\\theta,r}(x)}{q_{\\theta,r}(x)+\\gamma}\\right]\\,\\mathrm{d}x}\\\\ &{}&{\\overset{(a)}{=}\\int_{\\mathcal{Z}}\\left(\\int_{X}k_{\\theta}(x|z)\\left[\\log\\left(\\frac{q_{\\theta,r}(x)+\\gamma}{p(y,x)}\\right)+\\frac{q_{\\theta,r}(x)}{q_{\\theta,r}(x)+\\gamma}\\right]\\,\\mathrm{d}x\\right)\\chi(z)\\,\\mathrm{d}z}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "One can then identify the desired result. In (a), we appeal to Fubini\u2019s theorem for the interchange of integrals whose conditions ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\int_{\\mathcal{Z}}\\int_{\\mathcal{X}}\\bigg|k_{\\theta}(x|z)\\left[\\log\\left(\\frac{q_{\\theta,r}(x)+\\gamma}{p(y,x)}\\right)+\\frac{q_{\\theta,r}(x)}{q_{\\theta,r}(x)+\\gamma}\\right]\\chi(z)\\bigg|\\,\\mathrm{d}x\\mathrm{d}z<\\infty,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "are satisfied by our assumptions. This can be seen from ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{LHS~Eq.~}(22)\\leq\\int_{\\mathcal{Z}}\\mathbb{E}_{k_{\\theta}(X|z)}\\left|\\log\\left(\\frac{q_{\\theta,r}(X)+\\gamma}{p(X,y)}\\right)+\\frac{q_{\\theta,r}(X)}{q_{\\theta,r}(X)+\\gamma}\\right||\\chi(z)|\\mathrm{d}z\\leq0,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where we use our assumption and the fact that $\\chi$ is absolutely integrable. ", "page_idx": 19}, {"type": "text", "text": "Proof of Prop. 5. The result can be obtained from direct computation. We begin ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\mathcal{E}_{\\lambda}(\\theta_{t},r_{t})=\\Big\\langle\\nabla_{\\theta}\\mathcal{E}_{\\lambda}(\\theta_{t},r_{t}),\\dot{\\theta}_{t}\\Big\\rangle+\\int\\delta_{r}\\mathcal{E}_{\\lambda}[\\theta_{t},r_{t}]\\,\\partial_{t}r_{t}\\,\\mathrm{d}z\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The second term can be simplified ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\int\\delta_{r}\\mathcal{E}_{\\lambda}[\\theta_{t},r_{t}]\\,\\partial_{t}r_{t}\\,\\mathrm{d}z=\\int\\delta_{r}\\mathcal{E}_{\\lambda}[\\theta_{t},r_{t}](z)\\nabla_{z}\\cdot(r_{t}(z)\\nabla\\delta_{r}\\mathcal{E}_{\\lambda}[\\theta_{t},r_{t}](z))\\,\\mathrm{d}z}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=-\\int r_{t}\\|\\nabla_{z}\\delta_{r}\\mathcal{E}_{\\lambda}[\\theta_{t},r_{t}](z)\\|^{2}\\,\\mathrm{d}z}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the last inequality follows from integration by parts. Hence, the claim holds. If the log-Sobolev inequality holds, then we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\left[\\mathcal{E}_{\\lambda}(\\theta_{t},r_{t})-\\mathcal{E}_{\\lambda}^{*}\\right]=-\\|\\nabla_{\\mathcal{M}}\\mathcal{E}_{\\lambda}[\\theta_{t},r_{t}]\\|\\leq-\\frac{1}{\\tau}\\left[\\mathcal{E}_{\\lambda}(\\theta_{t},r_{t})-\\mathcal{E}_{\\lambda}^{*}\\right].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "From Gro\u00a8nwalls inequality, we obtain the desired result. ", "page_idx": 19}, {"type": "text", "text": "F Proofs in Section 4 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "F.1 Proof of Prop. 7 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proof. We first begin by proving $\\Gamma$ -convergence directly via its definition, i.e., demonstrating that the liminf inequality holds and establishing the existence of a recovery sequence. The latter follows from pointwise convergence: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\gamma\\to0}\\mathcal{E}_{\\lambda}^{\\gamma}(\\theta,r)=\\mathcal{E}_{\\lambda}(\\theta,r),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "upon taking $(\\theta_{\\gamma},r_{\\gamma})=(\\theta,r)$ for all $\\gamma$ . ", "page_idx": 19}, {"type": "text", "text": "The liminf inequality can be seen to follow similarly from the l.s.c. argument in App. E.1. ", "page_idx": 19}, {"type": "text", "text": "To arrive at the convergence of minima, we invoke Dal Maso (2012, Theorem 7.8) by using the fact that $\\mathcal{E}_{\\lambda}^{\\gamma}$ is equi-coercive in the sense of Dal Maso (2012, Definition 7.6). To see that $\\bar{\\mathcal{E}}_{\\lambda}^{\\gamma}$ is equi-coercive, note that we have $\\mathcal{E}_{\\lambda}^{\\gamma}\\geq\\mathcal{E}_{\\lambda}$ and $\\mathcal{E}_{\\lambda}$ is l.s.c. (from Prop. 3), then applying Dal Maso (2012, Proposition 7.7). \u53e3 ", "page_idx": 19}, {"type": "text", "text": "F.2 Proof of Prop. 8 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proof of Prop. 8. We can equivalently write the $\\gamma$ -PVI flow in Eq. (17) as follows ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{d}(\\theta_{t},Z_{t})=\\tilde{b}^{\\gamma}(\\theta_{t},\\mathrm{Law}(Z_{t}),Z_{t})\\,\\mathrm{d}t+\\sigma\\,\\mathrm{d}W_{t},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\sigma=\\left[0\\atop0\\right.\\ \\left.\\sqrt{2\\lambda_{r}}I_{d_{z}}\\right]$ , and ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\tilde{b}^{\\gamma}:\\mathbb{R}^{d_{\\theta}}\\times\\mathcal{P}(\\mathcal{Z})\\times\\mathbb{R}^{d_{z}}\\rightarrow\\mathbb{R}^{d_{\\theta}+d_{z}}:(\\theta,r,Z)\\mapsto\\left[\\!\\!\\begin{array}{c}{-\\nabla_{\\theta}\\mathcal{E}_{\\lambda}^{\\gamma}(\\theta,r)\\!\\!\\right].}\\\\ {b^{\\gamma}(\\theta,r,Z)}\\end{array}\\!\\!\\right]\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In App. F.4, we show that under our assumptions the drift $\\tilde{b}^{\\gamma}$ is Lipschitz. And under Lipschitz regularity conditions, the proof follows similarly to Lim et al. (2024) which we shall outline for completeness. ", "page_idx": 20}, {"type": "text", "text": "We begin endowing the space $\\Theta\\times\\mathcal{P}(\\mathbb{R}^{d_{z}})$ with the metric ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathsf{d}\\bigl((\\theta,r),(\\theta^{\\prime},r^{\\prime})\\bigr)=\\sqrt{\\|\\theta-\\theta^{\\prime}\\|^{2}+\\mathsf{W}_{2}^{2}(q,q^{\\prime})}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Let $\\Upsilon\\in C([0,T],\\Theta\\times\\mathcal{P}(\\mathbb{R}^{d_{z}}))$ and denote $\\Upsilon_{t}=(\\vartheta_{t}^{\\Upsilon},\\nu_{t}^{\\Upsilon})$ for it\u2019s respective components. Consider the process that substitutes $\\Upsilon$ into (23), in place of the $\\operatorname{Law}(Z_{t})$ and $\\theta_{t}$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{d}(\\theta_{t}^{\\Upsilon},Z_{t}^{\\Upsilon})=\\tilde{b}^{\\gamma}(\\vartheta_{t}^{\\Upsilon},\\nu_{t}^{\\Upsilon},Z_{t}^{\\Upsilon})\\,\\mathrm{d}t+\\sigma\\,\\mathrm{d}W_{t}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "whose existence and uniqueness of strong solutions are given by Carmona (2016)[Thereom 1.2]. Define the operator ", "page_idx": 20}, {"type": "equation", "text": "$$\nF_{T}:C([0,T],\\Theta\\times\\mathcal{P}(\\mathbb{R}^{d_{z}}))\\to C([0,T],\\Theta\\times\\mathcal{P}(\\mathbb{R}^{d_{z}})):\\Upsilon\\to(t\\mapsto(\\theta_{t}^{\\Upsilon},\\mathrm{Law}(Z_{t}^{\\Upsilon})).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Let $(\\theta_{t},Z_{t})$ denote a process that is a solution to (23) then the function $t\\mapsto\\left(\\theta_{t},\\operatorname{Law}(Z_{t})\\right)$ is a fixed point of the operator $F_{T}$ . The converse also holds. Thus, it is sufficient to establish the existence and uniqueness of the fixed point of the operator $F_{T}$ . For $\\Upsilon=(\\vartheta,\\nu)$ and $\\Upsilon^{\\prime}=\\left(\\vartheta^{\\prime},\\nu^{\\prime}\\right)$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\theta_{t}^{\\Upsilon}-\\theta_{t}^{\\Upsilon^{\\prime}}\\|^{2}+\\mathbb{E}[\\|Z_{t}^{\\Upsilon}-Z_{t}^{\\Upsilon^{\\prime}}\\|]^{2}=\\mathbb{E}\\left\\|\\int_{0}^{t}\\tilde{b}^{\\gamma}(\\vartheta_{s},\\nu_{s},Z_{s}^{\\Upsilon})-\\tilde{b}^{\\gamma}(\\vartheta_{s}^{\\prime},\\nu_{s}^{\\prime},Z_{s}^{\\Upsilon^{\\prime}})\\,\\mathrm{d}s\\right\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq t C\\displaystyle\\int_{0}^{t}\\left[\\mathbb{E}\\|Z_{s}^{\\Upsilon}-Z_{s}^{\\Upsilon^{\\prime}}\\|^{2}+\\|\\vartheta_{s}-\\vartheta_{s}^{\\prime}\\|^{2}+\\mathbb{W}_{1}^{2}(\\nu_{s},\\nu_{s}^{\\prime})\\right]\\,\\mathrm{d}s}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq C(t)\\displaystyle\\int_{0}^{t}[\\mathbb{W}_{2}^{2}(\\nu_{s},\\nu_{s}^{\\prime})+\\|\\vartheta_{s}-\\vartheta_{s}^{\\prime}\\|^{2}]\\,\\mathrm{d}s,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where we apply Jensen\u2019s inequality; $C_{r}$ -inequality; Lipschitz drift of b\u02dc\u03b3; and Gr\u00a8onwall\u2019s inequality. The constant $\\dot{C}:=3K_{\\tilde{b}}^{2}$ and $\\begin{array}{r}{\\dot{C}(t)\\stackrel{}{:}=t\\dot{C}\\exp{\\left(\\frac{1}{2}t^{2}\\dot{C}\\right)}}\\end{array}$ . Thus, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathsf{d}^{2}(F_{T}(\\Upsilon)_{t},F_{T}(\\Upsilon^{\\prime})_{t})\\le C(t)\\int_{0}^{t}\\mathsf{d}^{2}(\\Upsilon_{s},\\Upsilon_{s}^{\\prime})\\,\\mathrm{d}s.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then, for $F_{T}^{k}$ denoting $k$ successive composition of $F_{T}$ , one can inductively show that it satisfies ", "page_idx": 20}, {"type": "equation", "text": "$$\n{\\mathsf{d}}^{2}(F_{T}^{k}(\\Upsilon)_{t},F_{T}^{k}(\\Upsilon^{\\prime})_{t})\\le\\frac{(t C(t))^{k}}{k!}\\operatorname*{sup}_{s\\in[0,T]}{\\mathsf{d}}^{2}(\\Upsilon_{s},\\Upsilon_{s}^{\\prime}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Taking the supremum, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{s\\in[0,T]}\\mathsf{d}^{2}(F_{T}^{k}(\\Upsilon)_{s},F_{T}^{k}(\\Upsilon^{\\prime})_{s})\\leq\\frac{(T C(T))^{k}}{k!}\\operatorname*{sup}_{s\\in[0,T]}\\mathsf{d}^{2}(\\Upsilon_{s},\\Upsilon_{s}^{\\prime}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus, for a large enough $k$ , we have shown that $F_{T}^{k}$ is a contraction and from Banach Fixed Point Theorem and the completeness of the space $(C([0,T],\\Theta\\times\\mathcal{P}(\\mathbb{R}^{d_{z}})),\\operatorname*{sup}{\\mathsf{d}})$ , we have existence and uniqueness. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "F.3 Proof of Prop. 9 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Recall, the process defined in Prop. 9: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\mathrm{d}\\theta_{t}^{\\gamma,M}=-\\nabla_{\\theta}\\mathcal{E}_{\\lambda}^{\\gamma}(\\theta_{t}^{\\gamma,M},r_{t}^{\\gamma,M})\\,\\mathrm{d}t,\\;\\;\\mathrm{where}\\;r_{t}^{\\gamma,M}=\\frac{1}{M}\\sum_{m=1}^{M}\\delta_{Z_{t,m}^{\\gamma,M}}}\\\\ {\\displaystyle\\forall m\\in[M]:\\mathrm{d}Z_{t,m}^{\\gamma,M}=b^{\\gamma}(\\theta_{t}^{\\gamma,M},r_{t}^{\\gamma,M},Z_{t,m}^{\\gamma,M})\\,\\mathrm{d}t+\\sqrt{2\\lambda_{r}}\\,\\mathrm{d}W_{t,m}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and $\\gamma{\\mathrm{-}}\\mathrm{PVI}$ (defined in Eq. (17)) augmented with extra particles (in the sense that there are $M$ independent copies of the $Z$ -process) to facilitate a synchronous coupling argument ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\mathrm{d}\\theta_{t}^{\\gamma}=-\\nabla_{\\theta}\\mathcal{E}_{\\lambda}^{\\gamma}(\\theta_{t}^{\\gamma},\\mathrm{Law}(Z_{t,1}^{\\gamma}))\\,\\mathrm{d}t,}\\\\ {\\forall m\\in[M]:\\mathrm{d}Z_{t,m}^{\\gamma}=b^{\\gamma}(\\theta_{t}^{\\gamma},\\mathrm{Law}(Z_{t,1}^{\\gamma}),Z_{t,m}^{\\gamma})\\,\\mathrm{d}t+\\sqrt{2\\lambda_{r}}\\,\\mathrm{d}W_{t,m}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof of Prop. 9. This is equivalent to proving that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\underbrace{\\mathbb{E}_{\\mathrm{\\Pi}_{t\\in[0,T]}}\\|\\theta_{t}^{\\gamma}-\\theta_{t}^{\\gamma,M}\\|^{2}}_{(a)}+\\underbrace{\\mathbb{E}_{\\mathrm{\\Pi}_{t\\in[0,T]}}\\left\\{\\frac{1}{M}\\sum_{m=1}^{M}\\|Z_{t,m}^{\\gamma}-Z_{t,m}^{\\gamma,M}\\|^{2}\\right\\}}_{(b)}=o(1).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We shall treat the two terms individually. We begin with (a) in (24), where Jensen\u2019s inequality gives: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle(\\mathbf{a})\\operatorname*{in}{(24)}=\\mathbb{E}_{\\ t\\in[0,T]}\\left\\|\\int_{0}^{t}\\left[\\nabla_{\\theta}\\mathcal{E}_{\\lambda}^{\\gamma}\\big(\\theta_{s}^{\\gamma,M},r_{s}^{\\gamma,M}\\big)-\\nabla_{\\theta}\\mathcal{E}_{\\lambda}^{\\gamma}\\big(\\theta_{s}^{\\gamma},r_{s}^{\\gamma}\\big)\\right]\\,\\mathrm{d}s\\right\\|^{2}}\\\\ {\\displaystyle\\leq T\\mathbb{E}\\int_{0}^{T}\\left\\|\\nabla_{\\theta}\\mathcal{E}_{\\lambda}^{\\gamma}\\big(\\theta_{t}^{\\gamma,M},r_{s}^{\\gamma,M}\\big)-\\nabla_{\\theta}\\mathcal{E}_{\\lambda}^{\\gamma}\\big(\\theta_{s}^{\\gamma},r_{s}^{\\gamma}\\big)\\right\\|^{2}\\,\\mathrm{d}t}\\\\ {\\displaystyle\\leq C_{\\theta}\\int_{0}^{T}\\mathbb{E}\\|\\theta_{s}^{\\gamma}-\\theta_{s}^{\\gamma,M}\\|^{2}+\\mathbb{E}\\mathsf{W}_{2}^{2}\\big(r_{s}^{\\gamma,M},r_{s}^{\\gamma}\\big)\\,\\mathrm{d}t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $C_{\\theta}:=2T K_{\\mathcal{E}_{\\lambda}^{\\gamma}}^{2}$ , we apply Cauchy\u2013Schwarz; and the $C_{r}$ inequality with the Lipschitz continuity of $\\nabla_{\\theta}\\mathcal{E}_{\\lambda}^{\\gamma}$ from Prop. 12. Using the $C_{r}$ inequality again, together with the triangle inequality: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbb{E}\\mathsf{W}_{2}^{2}(r_{s}^{\\gamma,M},r_{s}^{\\gamma})\\leq2\\mathbb{E}\\mathsf{W}_{2}^{2}\\,(r_{s}^{\\gamma},\\hat{r}_{s}^{\\gamma})+2\\mathbb{E}\\mathsf{W}_{2}^{2}(r_{s}^{\\gamma,M},\\hat{r}_{s}^{\\gamma})}\\\\ &{}&{\\qquad\\qquad\\qquad\\qquad\\leq o(1)+\\displaystyle\\frac{2}{M}\\sum_{m=1}^{M}\\mathbb{E}\\|Z_{s,m}^{\\gamma}-Z_{s,m}^{\\gamma,M}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\begin{array}{r}{\\hat{r}_{s}^{\\gamma}=\\frac{1}{M}\\sum_{m=1}^{M}\\delta_{Z_{s,m}^{\\gamma}}}\\end{array}$ and we use Fournier and Guillin (2015). Note that we also have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\|\\theta_{s}^{\\gamma}-\\theta_{s}^{\\gamma,M}\\|^{2}\\leq\\operatorname*{sup}_{s^{\\prime}\\in[0,T]}\\|\\theta_{s^{\\prime}}^{\\gamma}-\\theta_{s^{\\prime}}^{\\gamma,M}\\|^{2},}\\\\ {\\displaystyle\\frac{1}{M}\\sum_{m=1}^{M}\\|Z_{s,m}^{\\gamma}-Z_{s,m}^{\\gamma,M}\\|^{2}\\leq\\displaystyle\\operatorname*{sup}_{s^{\\prime}\\in[0,T]}\\frac{1}{M}\\sum_{m=1}^{M}\\|Z_{s^{\\prime},m}^{\\gamma}-Z_{s^{\\prime},m}^{\\gamma,M}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Applying Eq. (26) in Eq. (25) then Eqs. (27) and (28), we obtain ", "page_idx": 21}, {"type": "equation", "text": "$$\n(a)\\leq2C_{\\theta}\\int_{0}^{T}\\mathbb{E}\\operatorname*{sup}_{s\\in[0,T]}\\|\\theta_{s}^{\\gamma}-\\theta_{s}^{\\gamma,M}\\|^{2}+\\mathbb{E}\\operatorname*{sup}_{s\\in[0,T]}\\frac{1}{M}\\sum_{m=1}^{M}\\|Z_{s,m}^{\\gamma}-Z_{s,m}^{\\gamma,M}\\|^{2}\\,\\mathrm{d}s+o(1).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Similarly, for (b) in (24), we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(b)=\\mathbb{E}\\underset{t\\in[0,T]}{\\operatorname*{sup}}\\frac{1}{M}\\sum_{m=1}^{M}\\left\\|\\int_{0}^{t}b^{\\gamma}(\\theta_{s}^{\\gamma,M},r_{s}^{\\gamma,M},Z_{s,m}^{\\gamma,M})-b^{\\gamma}(\\theta_{s}^{\\gamma},\\mathrm{Law}(Z_{s,1}^{\\gamma}),Z_{s,m}^{\\gamma})\\,\\mathrm{d}s\\right\\|^{2}}\\\\ &{\\quad\\leq C_{z}\\mathbb{E}\\displaystyle\\int_{0}^{T}\\|\\theta_{s}^{\\gamma,M}-\\theta_{s}^{\\gamma}\\|^{2}+\\mathsf{W}_{2}^{2}(r_{s}^{\\gamma,M},\\mathrm{Law}(Z_{s,1}^{\\gamma}))+\\displaystyle\\frac{1}{M}\\sum_{m=1}^{M}\\|Z_{s,m}^{\\gamma}-Z_{s,m}^{\\gamma,M}\\|^{2}\\,\\mathrm{d}s,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $C_{z}:=3K_{b\\gamma}^{2}$ and, as before, we apply Cauchy\u2013Schwarz, Lipschitz and $C_{r}$ inequalities. Then from Eqs. (26) to (28), we obtain ", "page_idx": 22}, {"type": "equation", "text": "$$\n(b)\\leq C\\mathbb{E}\\int_{0}^{T}\\operatorname*{sup}_{s\\in[0,T]}\\|\\theta_{s}^{\\gamma,M}-\\theta_{s}^{\\gamma}\\|^{2}+\\operatorname*{sup}_{s\\in[0,T]}\\frac{1}{M}\\sum_{m=1}^{M}\\|Z_{s,m}^{\\gamma}-Z_{s,m}^{\\gamma,M}\\|^{2}+o(1)\\mathrm{d}s.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Combining Eqs. (29) and (30) and applying Gro\u00a8nwall\u2019s inequality, we obtain ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}\\operatorname*{sup}_{t\\in[0,T]}\\|\\theta_{t}^{\\gamma}-\\theta_{t}^{\\gamma,M}\\|^{2}+\\mathbb{E}\\operatorname*{sup}_{t\\in[0,T]}\\left\\{\\frac{1}{M}\\sum_{m=1}^{M}\\|Z_{t,m}^{\\gamma}-Z_{t,m}^{\\gamma,M}\\|^{2}\\right\\}=o(1).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Taking the limit, we have the desired result. ", "page_idx": 22}, {"type": "text", "text": "F.4 The drift in Eq. (17) is Lipschitz ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we show that the drift in the $\\gamma.$ -PVI flow in Eq. (17) is Lipschitz. ", "page_idx": 22}, {"type": "text", "text": "Proposition 11. Under the same assumptions as Prop. 8; the drift $\\tilde{b}(A,r)$ is Lipschitz, i.e., there exists a constant $K_{\\tilde{b}}\\in\\mathbb{R}_{>0}$ such that: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\tilde{b}^{\\gamma}(\\theta,r,z)-\\tilde{b}^{\\gamma}(\\theta^{\\prime},r^{\\prime},z^{\\prime})\\|\\leq K_{\\tilde{b}}(\\|(\\theta,z)-(\\theta^{\\prime},z^{\\prime})\\|+\\mathsf{W}_{2}(r,r^{\\prime})),\\ \\ \\forall\\theta,\\theta^{\\prime}\\in\\Theta,z,z^{\\prime}\\in\\mathcal{Z},r,r^{\\prime}\\in\\mathcal{P}(\\mathcal{Z}_{\\delta}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "P\u221aroof. From the \u221adefinition and using the concavity of $\\sqrt{\\cdot}$ (which ensures that for any $a,b\\ge0$ , ${\\sqrt{a+b}}\\leq{\\sqrt{a}}+{\\sqrt{b}})$ , we obtain ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\tilde{b}^{\\gamma}(\\theta,r,z)-\\tilde{b}^{\\gamma}(\\theta^{\\prime},r^{\\prime},z^{\\prime})\\|\\leq\\|\\nabla_{\\theta}\\mathcal{E}_{\\lambda}^{\\gamma}(\\theta,r)-\\nabla_{\\theta}\\mathcal{E}_{\\lambda}^{\\gamma}(\\theta^{\\prime},r^{\\prime})\\|+\\|b^{\\gamma}(\\theta,r,z)-b^{\\gamma}(\\theta^{\\prime},r^{\\prime},z^{\\prime})\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "It is established below in Prop. 12 that $\\nabla_{\\theta}\\mathcal{E}_{\\lambda}^{\\gamma}$ satisfies a Lipschitz inequality, i.e., there is some $K_{\\mathcal{E}_{\\lambda}^{\\gamma}}\\in\\mathbb{R}_{>0}$ such that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla_{\\theta}\\mathcal{E}_{\\lambda}^{\\gamma}(\\theta,r)-\\nabla_{\\theta}\\mathcal{E}_{\\lambda}^{\\gamma}(\\theta^{\\prime},r^{\\prime})\\|\\leq K_{\\mathcal{E}_{\\lambda}^{\\gamma}}(\\|\\theta-\\theta^{\\prime}\\|+\\mathsf{W}_{2}(r,r^{\\prime})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "It is established below in Prop. 13 that $b^{\\gamma}$ satisfies a Lipschitz inequality, i.e., there is some $K_{b^{\\gamma}}\\in\\mathbb{R}_{>0}$ such that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|b^{\\gamma}(\\theta,r,z)-b^{\\gamma}(\\theta^{\\prime},r^{\\prime},z^{\\prime})\\|\\leq K_{b^{\\gamma}}(\\|(\\theta,z)-(\\theta^{\\prime},z^{\\prime})\\|+\\mathsf{W}_{2}(r,r^{\\prime})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Hence, we have obtained as desired with $K_{\\tilde{b}}=K_{\\varepsilon_{\\lambda}^{\\gamma}}+K_{b^{\\gamma}}$ . ", "page_idx": 22}, {"type": "text", "text": "Proposition 12. Under the same assumptions as Prop. 8, the function $(\\theta,r)\\,\\mapsto\\,\\nabla_{\\theta}\\mathcal{E}_{\\lambda}^{\\gamma}(\\theta,r)$ is Lipschitz, i.e., there exist some constant $K_{\\mathcal{E}_{\\lambda}^{\\gamma}}\\in\\mathbb{R}_{>0}$ such that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|\\nabla_{\\theta}\\mathcal{E}_{\\lambda}^{\\gamma}(\\theta,r)-\\nabla_{\\theta}\\mathcal{E}_{\\lambda}^{\\gamma}(\\theta^{\\prime},r^{\\prime})\\|\\leq K_{\\mathcal{E}_{\\lambda}^{\\gamma}}(\\|\\theta-\\theta^{\\prime}\\|+\\mathsf{W}_{2}(r,r^{\\prime})),\\ \\forall(\\theta,r),(\\theta^{\\prime},r^{\\prime})\\in\\mathcal{M}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. From the definition, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}\\mathcal{E}_{\\lambda}^{\\gamma}(\\theta,r)=\\nabla_{\\theta}\\mathcal{E}^{\\gamma}(\\theta,r)+\\nabla_{\\theta}\\mathsf{R}_{\\lambda}(\\theta,r).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Thus, if both $\\nabla_{\\theta}\\mathcal{E}^{\\gamma}$ and $\\nabla_{\\theta}\\mathsf{R}_{\\lambda}$ are Lipschitz, then so is $\\nabla_{\\theta}\\mathcal{E}_{\\lambda}^{\\gamma}$ . Since $\\mathsf{R}_{\\lambda}$ has Lipschitz gradient (by Assumption 1), it remains to be shown that $\\nabla_{\\boldsymbol{\\theta}}\\mathcal{E}^{\\gamma}$ is Lipschitz. From Prop. 6, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}\\mathcal{E}^{\\gamma}(\\theta,r)=\\mathbb{E}_{p_{k}(\\epsilon)r(z)}\\left[(\\nabla_{\\theta}\\phi_{\\theta}\\cdot[s_{\\theta,r}^{\\gamma}-s_{p}])(z,\\epsilon)\\right]=\\int\\nabla_{\\theta}\\phi_{\\theta}\\cdot d_{\\theta,r}^{p,\\gamma}(z,\\epsilon)\\,p_{k}(\\mathrm{d}\\epsilon)r(\\mathrm{d}z),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $d_{\\theta,r}^{p,\\gamma}(z,\\epsilon):=s_{\\theta,r}^{\\gamma}(z,\\epsilon)-s_{p}(z,\\epsilon)$ . Then, applying Jensen\u2019s inequality, we obtain ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{\\theta}\\mathcal{E}^{\\gamma}(\\theta,r)-\\nabla_{\\theta}\\mathcal{E}^{\\gamma}(\\theta^{\\prime},r^{\\prime})\\|}\\\\ &{=\\left\\|\\displaystyle\\int p(\\epsilon)\\int\\left[\\nabla_{\\theta}\\phi_{\\theta}\\cdot d_{\\theta,r}^{p,\\gamma}(z,\\epsilon)r(z)-\\nabla_{\\theta}\\phi_{\\theta^{\\prime}}\\cdot d_{\\theta^{\\prime},r^{\\prime}}^{p,\\gamma}(z,\\epsilon)r^{\\prime}(z)\\right]\\mathrm{d}z\\mathrm{d}\\epsilon\\right\\|}\\\\ &{\\le\\displaystyle\\int p(\\epsilon)\\left\\|\\displaystyle\\int\\left[\\nabla_{\\theta}\\phi_{\\theta}\\cdot d_{\\theta,r}^{p,\\gamma}(z,\\epsilon)r(z)-\\nabla_{\\theta}\\phi_{\\theta^{\\prime}}\\cdot d_{\\theta^{\\prime},r^{\\prime}}^{p,\\gamma}(z,\\epsilon)r^{\\prime}(z)\\right]\\mathrm{d}z\\right\\|\\mathrm{d}\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Focusing on the integrand, we can upper-bound it with ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|\\int\\left[\\nabla_{\\theta}\\phi_{\\theta}\\cdot d_{\\theta,r}^{p,\\gamma}(z,\\epsilon)r(z)-\\nabla_{\\theta}\\phi_{\\theta^{\\prime}}\\cdot d_{\\theta^{\\prime},r^{\\prime}}^{p,\\gamma}(z,\\epsilon)r^{\\prime}(z)\\right]\\mathrm{d}z\\right\\|}\\\\ &{\\stackrel{a)}{\\leq}\\left\\|\\int\\nabla_{\\theta}\\phi_{\\theta}\\cdot d_{\\theta,r}^{p,\\gamma}(z,\\epsilon)\\left[r(z)-r^{\\prime}(z)\\right]\\mathrm{d}z\\right\\|+\\left\\|\\int\\left[\\nabla_{\\theta}\\phi_{\\theta}\\cdot d_{\\theta,r}^{p,\\gamma}(z,\\epsilon)-\\nabla_{\\theta}\\phi_{\\theta^{\\prime}}\\cdot d_{\\theta^{\\prime},r^{\\prime}}^{p,\\gamma}(z,\\epsilon)\\right]r^{\\prime}(z)\\mathrm{d}z\\right\\|}\\\\ &{\\stackrel{b)}{\\leq}\\int\\left\\|\\nabla_{\\theta}\\phi_{\\theta}\\cdot d_{\\theta,r}^{p,\\gamma}(z,\\epsilon)\\right\\||r(z)-r^{\\prime}(z)|\\mathrm{d}z}\\\\ &{+\\int\\left\\|\\nabla_{\\theta}\\phi_{\\theta}\\cdot d_{\\theta,r}^{p,\\gamma}(z,\\epsilon)-\\nabla_{\\theta}\\phi_{\\theta^{\\prime}}\\cdot d_{\\theta^{\\prime},r^{\\prime}}^{p,\\gamma}(z,\\epsilon)\\right\\|r^{\\prime}(z)\\mathrm{d}z.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where in (a) we add and subtract the relevant terms and invoke the triangle inequality, and in (b) we apply Jensen\u2019s inequality. Plugging this back into Eq. (31), we obtain ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ \\ \\ \\||\\nabla_{\\theta}\\mathcal{E}^{\\gamma}(\\theta,r)-\\nabla_{\\theta}\\mathcal{E}^{\\gamma}(\\theta^{\\prime},r^{\\prime})||}\\\\ &{\\le\\displaystyle\\int\\mathbb{E}_{p_{k}(\\epsilon)}\\left\\|\\nabla_{\\theta}{\\phi}_{\\theta}\\cdot d_{\\theta,r}^{p,\\gamma}(z,\\epsilon)\\right\\||r(z)-r^{\\prime}(z)|\\mathrm{d}z}\\\\ &{\\quad+\\displaystyle\\int\\mathbb{E}_{p_{k}(\\epsilon)}\\left\\|\\nabla_{\\theta}{\\phi}_{\\theta}\\cdot d_{\\theta,r}^{p,\\gamma}(z,\\epsilon)-\\nabla_{\\theta}{\\phi}_{\\theta^{\\prime}}\\cdot d_{\\theta^{\\prime},r^{\\prime}}^{p,\\gamma}(z,\\epsilon)\\right\\|r^{\\prime}(z)\\mathrm{d}z,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the interchange of integrals is justified from Fubini\u2019s theorem for non-negative functions (also known as Tonelli\u2019s theorem). ", "page_idx": 23}, {"type": "text", "text": "As we shall later show, the two terms have the following upper bounds: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{(32)\\leq K\\mathsf{W}_{1}(r,r^{\\prime}),\\mathrm{~and}}}\\\\ {{(33)\\leq K(\\|\\theta-\\theta^{\\prime}\\|+\\mathsf{W}_{1}(r,r^{\\prime})),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $K$ denotes a generic constant; and, upon noting that $\\mathsf{W}_{1}\\leq\\mathsf{W}_{2}$ , we obtained the desired result. Now, we shall verify Eqs. (34) and (35). For the Eq. (34), we use the fact that the map $z\\mapsto$ $\\mathbb{E}_{p_{k}(\\epsilon)}\\left\\|\\nabla_{\\theta}\\phi_{\\theta}\\cdot\\boldsymbol{d}_{\\theta,r}^{p}(\\boldsymbol{z},\\epsilon)\\right\\|$ is Lipschitz then from the dual representation of $\\mathsf{W}_{1}$ , we obtain the desired result. To see that the aforementioned map is Lipschitz, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{p}\\Big(a_{q}^{*}\\Big\\lVert\\nabla_{\\theta}\\phi_{i}^{(*)}-\\phi_{q^{*}}^{(*)}(z^{*},q)\\Big\\rVert-\\mathbb{E}_{p}\\Big_{q}\\Big(\\Big\\lVert\\nabla_{\\theta}\\phi_{i}^{(*)}-\\theta_{p^{*}}^{(*)}(z^{*},q)\\Big\\rVert\\Big)}\\\\ &{\\stackrel{(a_{q}^{*})}\\le\\mathbb{E}_{p}\\Big_{q}\\Big(a_{q}^{*}\\rVert\\nabla_{\\theta}\\phi_{i}^{(*)}-\\theta_{p^{*}}^{(*)}(z^{*},q)-\\nabla_{\\theta}\\phi_{i}^{(*)}-\\theta_{p^{*}}^{(*)}(z^{*},q)\\Big\\rVert}\\\\ &{\\stackrel{(b_{q}^{*})}\\le\\mathbb{E}_{p+\\theta_{q}}\\Big)\\Big[\\nabla_{\\theta}\\phi_{i}^{(*)}(z^{*},q)\\Big\\langle\\theta_{q}^{(*)}(z^{*},q)-\\theta_{p^{*}}^{(*)}(z^{*},q)\\Big\\rVert\\Big]}\\\\ &{+\\mathbb{E}_{p}\\Big_{q}\\Big[\\Big\\lVert\\nabla_{\\theta}\\phi_{i}^{(*)}(z,z)-\\nabla_{\\theta}\\phi_{i}^{(*)}(z^{*},q)\\Big\\rVert+\\theta_{q}^{(*)}(z^{*},q)\\Big]\\Big]}\\\\ &{\\stackrel{(c_{q}^{*})}\\le\\mathbb{E}_{p+\\theta_{q}}\\Big[\\Big\\lVert\\nabla_{\\theta}\\phi_{i}^{(*)}(z,{z})\\Big\\rVert_{p}\\Big]\\Big|\\phi_{i}^{(*)}(z^{*},q)-\\theta_{p^{*}}^{(*)}(z^{*},q)\\Big]\\Big]}\\\\ &{+\\mathbb{E}_{p}\\Big_{q}\\Big[\\Big\\lVert\\nabla_{\\theta}\\phi_{i}(z,{z})-\\nabla_{\\theta}\\phi_{i}^{(*)}(z^{*},q)\\Big\\rVert_{p}\\Big]\\Big|\\theta_{q}^{(*)}(z^{*},q)\\Big]\\Big]}\\\\ &{\\stackrel{(a_{q}^{*})}\\le\\mathbb{E}_{p}\\Big_{q}\\Big[\\Big\\lVert\\theta_{q}\\Big\\lVert\\phi_{i}^{(*)}\\Big\\rVert+h_{\\theta} \n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where (a) we use the reverse triangle inequality; (b) we add and subtract relevant terms and apply the triangle inequality; (c) we use a property of the matrix norm with $\\|\\cdot\\|_{F}$ denoting the Frobenius norm; (d) we utilize Assumption 3 and the Lipschitz property from Prop. 15; (e) we apply Young\u2019s ", "page_idx": 23}, {"type": "text", "text": "inequality. Then, from the fact that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{p_{k}(\\epsilon)}\\left[(a_{\\phi}\\|\\epsilon\\|+b_{\\phi})(a_{d}\\|\\epsilon\\|+b_{d})\\right]<\\infty,\\ \\mathbb{E}_{p_{k}(\\epsilon)}\\left[(a_{\\phi}\\|\\epsilon\\|+b_{\\phi})^{2}+\\left\\|d_{\\theta,r}^{p,\\gamma}(z^{\\prime},\\epsilon)\\right\\|^{2}\\right]<\\infty,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which holds from the assumption that $p_{k}$ has finite second moments Assumption 3, and from our assumption that $\\mathbb{E}_{p_{k}(\\epsilon)}\\left\\|d_{\\theta,r}^{p,\\gamma}\\bar{(z^{\\prime}},\\epsilon)\\right\\|$ is bounded. Hence, the map is Lipschitz and so Eq. (34) holds. ", "page_idx": 24}, {"type": "text", "text": "As for Eq. (35), we focus on the integrand in Eq. (33) ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{p_{k}(\\epsilon)}\\left\\|\\nabla_{\\theta}\\phi_{\\theta}\\cdot d_{\\theta,r}^{p,\\gamma}(z,\\epsilon)-\\nabla_{\\theta}\\phi_{\\theta^{\\prime}}\\cdot d_{\\theta^{\\prime},r^{\\prime}}^{p,\\gamma}(z,\\epsilon)\\right\\|}\\\\ &{\\le\\!\\mathbb{E}_{p_{k}(\\epsilon)}\\left[\\left\\|\\nabla_{\\theta}\\phi_{\\theta}\\cdot(d_{\\theta,r}^{p,\\gamma}-d_{\\theta^{\\prime},r^{\\prime}}^{p,\\gamma})(z,\\epsilon)\\right\\|+\\left\\|(\\nabla_{\\theta}\\phi_{\\theta}-\\nabla_{\\theta}\\phi_{\\theta^{\\prime}})\\cdot d_{\\theta^{\\prime},r^{\\prime}}^{p}(z,\\epsilon)\\right\\|\\right]}\\\\ &{\\le\\!\\mathbb{E}_{p_{k}(\\epsilon)}\\left[\\left\\|\\nabla_{\\theta}\\phi_{\\theta}(z,\\epsilon)\\right\\|_{F}\\left\\|(d_{\\theta,r}^{p,\\gamma}-d_{\\theta^{\\prime},r^{\\prime}}^{p,\\gamma})(z,\\epsilon)\\right\\|+\\left\\|(\\nabla_{\\theta}\\phi_{\\theta}-\\nabla_{\\theta}\\phi_{\\theta^{\\prime}})(z,\\epsilon)\\right\\|_{F}\\right\\|d_{\\theta^{\\prime},r^{\\prime}}^{p,\\gamma}(z,\\epsilon)\\right\\|\\right]}\\\\ &{\\le\\!\\mathbb{E}_{p_{k}(\\epsilon)}\\left[(a_{\\phi}\\|\\epsilon\\|+b_{\\phi})(a_{d}\\|\\epsilon\\|+b_{d})\\right](\\|\\theta-\\theta^{\\prime}\\|+\\mathbb{W}_{1}(r,r^{\\prime}))}\\\\ &{+\\mathbb{E}_{p_{k}(\\epsilon)}\\left[(a_{\\phi}\\|\\epsilon\\|+b_{\\phi})\\left\\|d_{\\theta^{\\prime},r^{\\prime}}^{p,\\gamma}(z,\\epsilon)\\right\\|\\right]\\|\\theta-\\theta^{\\prime}\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where, for the last line, we apply Prop. 15 and Assumption 3. Applying Young\u2019s inequality and (36), we have the desired result. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "Proposition 13 $b^{\\gamma}$ is Lipschitz). Under the same assumptions as Prop. 8, the map $b^{\\gamma}$ is $K_{b^{\\gamma}}$ - Lipschitz, i.e., there exists a constant $K_{b^{\\gamma}}\\in\\mathbb{R}_{>0}$ such that the following inequality holds for all $(\\dot{\\theta_{*}}z,r),(\\theta^{\\prime},z^{\\prime},r^{\\prime})\\in\\Theta\\times\\mathbb{R}^{d_{z}}\\times\\mathcal{P}(\\mathbb{R}^{d_{z}})$ : ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|b^{\\gamma}(\\theta,r,z)-b^{\\gamma}(\\theta^{\\prime},r^{\\prime},z^{\\prime})\\|\\leq K_{b^{\\gamma}}(\\|(\\theta,z)-(\\theta^{\\prime},z^{\\prime})\\|+\\mathsf{W}_{1}(r,r^{\\prime})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. One can write the drift $b^{\\gamma}$ as follows (can be found in Eq. (43) similarly to Prop. 6), we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{b^{\\gamma}(\\theta,r,z)=-\\mathbb{E}_{p_{k}(\\epsilon)}\\left[\\big(\\nabla_{z}\\phi_{\\theta}\\cdot[s_{\\theta,r}^{\\gamma}-s_{p}+\\Gamma_{\\theta,r}^{\\gamma}]\\big)(z,\\epsilon)\\right]+\\nabla_{x}\\log p_{0}(z),}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\begin{array}{r}{\\Gamma_{\\theta,r}^{\\gamma}(z,\\epsilon):=\\frac{\\gamma\\nabla_{x}q_{\\theta,r}(\\phi_{\\theta}(z,\\epsilon))}{(q_{\\theta,r}(\\phi_{\\theta}(z,\\epsilon))+\\gamma)^{2}}}\\end{array}$ \u03b3\u2207xq\u03b8,r(\u03d5\u03b8(z,\u03f5)) . Hence, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{b^{\\gamma}(\\theta,r,z)-b^{\\gamma}(\\theta^{\\prime},r^{\\prime},z^{\\prime})\\|\\leq\\|\\mathbb{E}_{p_{k}(\\epsilon)}[(\\nabla_{z}\\phi_{\\theta}\\cdot[d_{\\theta,r}^{p,\\gamma}+\\Gamma_{\\theta,r}^{\\gamma}])(z,\\epsilon)-(\\nabla_{z}\\phi_{\\theta^{\\prime}}\\cdot[d_{\\theta^{\\prime},r^{\\prime}}^{p}+\\Gamma_{\\theta^{\\prime},r^{\\prime}}^{\\gamma}])(z^{\\prime},\\epsilon)}\\\\ &{\\phantom{b^{\\gamma}(\\theta,r^{\\prime},z)\\|}+\\|\\nabla_{z}\\log p_{0}(z)-\\nabla_{z}\\log p_{0}(z^{\\prime})\\|}\\\\ &{\\phantom{b^{\\gamma}(\\theta,r^{\\prime},z)\\|}\\leq\\mathbb{E}_{p_{k}(\\epsilon)}\\|(\\nabla_{z}\\phi_{\\theta}\\cdot d_{\\theta,r}^{p,\\gamma}+\\Gamma_{\\theta,r}^{\\gamma})(z,\\epsilon)-(\\nabla_{z}\\phi_{\\theta^{\\prime}}\\cdot[d_{\\theta^{\\prime},r^{\\prime}}^{p,\\gamma}+\\Gamma_{\\theta^{\\prime},r^{\\prime}}^{\\gamma}])(z^{\\prime},\\epsilon)\\|}\\\\ &{\\phantom{b^{\\gamma}(\\theta,r^{\\prime},z)\\|}+K_{p_{0}}\\|z-z^{\\prime}\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where for the last inequality we use Jensen\u2019s inequality and Assumption 1. Since we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{p_{k}(\\epsilon)}\\|(\\nabla_{z}\\phi_{\\theta^{\\star}}[d_{\\theta^{\\star}}^{p,\\gamma}+\\Gamma_{\\theta,r}^{\\gamma}])(z,\\epsilon)-(\\nabla_{z}\\phi_{\\theta^{\\prime}}\\cdot[d_{\\theta^{\\prime},r^{\\prime}}^{p,\\gamma}+\\Gamma_{\\theta^{\\prime},r^{\\prime}}^{\\gamma}])(z^{\\prime},\\epsilon)\\|}\\\\ &{\\stackrel{(a)}{\\leq}\\mathbb{E}_{p_{k}(\\epsilon)}\\|(\\nabla_{z}\\phi_{\\theta^{\\star}}[d_{\\theta^{\\star}}^{p,\\gamma}+\\Gamma_{\\theta,r}^{\\gamma}])(z,\\epsilon)-\\nabla_{z}\\phi_{\\theta}(z,\\epsilon)\\cdot[d_{\\theta^{\\prime},r^{\\prime}}^{p,\\gamma}+\\Gamma_{\\theta^{\\prime},r^{\\prime}}^{\\gamma}](z^{\\prime},\\epsilon)\\|}\\\\ &{\\quad+\\mathbb{E}_{p_{k}(\\epsilon)}\\|\\nabla_{z}\\phi_{\\theta}(z,\\epsilon)\\cdot[d_{\\theta^{\\prime},r^{\\prime}}^{p,\\gamma}+\\Gamma_{\\theta^{\\prime},r^{\\prime}}^{\\gamma}](z^{\\prime},\\epsilon)-(\\nabla_{z}\\phi_{\\theta^{\\star}}\\cdot[d_{\\theta^{\\prime},r^{\\prime}}^{p,\\gamma}+\\Gamma_{\\theta^{\\prime},r^{\\prime}}^{\\gamma}])(z^{\\prime},\\epsilon)\\|}\\\\ &{\\stackrel{(b)}{\\leq}\\mathbb{E}_{p_{k}(\\epsilon)}\\|\\nabla_{z}\\phi_{\\theta}(z,\\epsilon)\\|_{r}\\|(d_{\\theta,r}^{p,\\gamma}+\\Gamma_{\\theta,r}^{\\gamma})(z,\\epsilon)-(d_{\\theta^{\\prime},r^{\\prime}}^{p,\\gamma}+\\Gamma_{\\theta^{\\prime},r^{\\prime}}^{\\gamma})(z^{\\prime},\\epsilon)\\|}\\\\ &{\\quad+\\mathbb{E}_{p_{k}(\\epsilon)}\\|\\nabla_{z}\\phi_{\\theta}(z,\\epsilon)-\\nabla_{z}\\phi_{\\theta^{\\prime}}(z^{\\prime},\\epsilon)\\|_{r}\\|(d_{\\theta^{\\prime},r^{\\prime}}^{p,\\gamma}+\\Gamma_{\\theta^{\\prime},r^{\\prime}}^{\\gamma})(z^{\\prime},\\epsilon)\\|}\\\\ &{\\stackrel{(c)}{\\leq} \n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where, for (a), we add and subtract the relevant terms and invoke the triangle inequality, in (b) we use properties of the matrix norm, and in (c) we use the bounded gradient and Lipschitz gradient in Assumption 3. For Eq. (37); upon using Props. 14 and 15, which are established below, we obtain ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{p_{k}(\\epsilon)}(a_{\\phi}\\|\\epsilon\\|+b_{\\phi})\\big(\\|d_{\\theta,r}^{p,\\gamma}(z,\\epsilon)-d_{\\theta^{\\prime},r^{\\prime}}^{p,\\gamma}(z^{\\prime},\\epsilon)\\|+\\|\\Gamma_{\\theta,r}^{\\gamma}(z,\\epsilon)-\\Gamma_{\\theta^{\\prime},r^{\\prime}}^{\\gamma}(z^{\\prime},\\epsilon)\\|\\big)}\\\\ &{\\leq\\!\\mathbb{E}_{p_{k}(\\epsilon)}(a_{\\phi}\\|\\epsilon\\|+b_{\\phi})[(a_{d}+a_{\\Gamma})\\|\\epsilon\\|+(b_{d}+b_{\\Gamma})](\\|(\\theta,z)-(\\theta^{\\prime},z^{\\prime})\\|+\\mathsf{W}_{1}(r,r^{\\prime})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "As for the second term, Eq. (38), ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{p_{k}(\\epsilon)}\\big(a_{\\phi}\\|\\epsilon\\|+b_{\\phi}\\big)\\|\\big(a_{\\theta^{\\prime},r^{\\prime}}^{p,\\gamma}+\\Gamma_{\\theta^{\\prime},r^{\\prime}}^{\\gamma}\\big)(z^{\\prime},\\epsilon)\\|}\\\\ &{\\stackrel{(a)}{\\leq}\\mathbb{E}_{p_{k}(\\epsilon)}\\big(a_{\\phi}\\|\\epsilon\\|+b_{\\phi}\\big)[\\|a_{\\theta^{\\prime},r^{\\prime}}^{p,\\gamma}(z^{\\prime},\\epsilon)\\|+\\|\\Gamma_{\\theta^{\\prime},r^{\\prime}}^{\\gamma}(z^{\\prime},\\epsilon)\\|]}\\\\ &{\\stackrel{(b)}{\\leq}\\mathbb{E}_{p_{k}(\\epsilon)}\\big(a_{\\phi}\\|\\epsilon\\|+b_{\\phi}\\big)[\\|a_{\\theta^{\\prime},r^{\\prime}}^{p,\\gamma}(z^{\\prime},\\epsilon)\\|+B_{\\Gamma}]}\\\\ &{\\stackrel{(c)}{\\leq}\\mathbb{E}_{p_{k}(\\epsilon)}\\frac{1}{2}\\big(a_{\\phi}\\|\\epsilon\\|+b_{\\phi})^{2}+\\frac{1}{2}\\|a_{\\theta^{\\prime},r^{\\prime}}^{p,\\gamma}(z^{\\prime},\\epsilon)\\|^{2}+B_{\\Gamma}\\big(a_{\\phi}\\|\\epsilon\\|+b_{\\phi}\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where (a) follows from the triangle inequality, (b) we use Prop. 14 boundedness of $\\Gamma$ , (c) we apply Young\u2019s inequality to the first term. Similarly to Eq. (36), from our Assumption 3 and our boundness assumption of the score, we have as desired. Combining Eq. (39) with the result of plugging Eq. (40) into Eq. (38), we obtain the result. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "Proposition 14 $\\Gamma$ is Lipschitz and bounded). Under Assumption 2, the map $(\\theta,r,z)\\mapsto\\Gamma_{\\theta,r}^{\\gamma}(z,\\epsilon)$ is Lipschitz and bounded. (Lipschitz) there is constants $a_{\\Gamma},b_{\\Gamma}\\in\\mathbb{R}_{>0}$ such that following hold: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\Gamma_{\\theta,r}^{\\gamma}(z,\\epsilon)-\\Gamma_{\\theta,r}^{\\gamma}(z,\\epsilon)\\|\\le(a_{\\Gamma}\\|\\epsilon\\|+b_{\\Gamma})(\\|(\\theta,z)-(\\theta^{\\prime},z^{\\prime})\\|+\\mathsf{W}_{1}(r,r^{\\prime})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Furthermore, $i t$ is bounded ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|\\Gamma_{\\theta,r}^{\\gamma}(z,\\epsilon)\\|\\leq B_{\\Gamma}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{\\rho}^{\\theta}(z,e)-\\Gamma_{\\varphi^{\\prime},\\rho^{\\prime}}^{\\theta}(z^{\\prime},\\epsilon)\\|}\\\\ &{\\leq\\gamma\\left\\|\\frac{\\big(\\varphi\\nu_{\\mathrm{\\mathcal{r}^{\\prime}}}(x^{\\prime})+\\gamma\\big)\\nabla_{x}\\log(\\varphi(\\mu_{\\mathrm{\\mathcal{r}}}(x)+\\gamma)-(\\varphi_{\\mathrm{\\mathcal{r}}}(x)+\\gamma)\\big)\\nabla_{x}\\log(\\varphi(\\nu_{\\mathrm{\\mathcal{r}}^{\\prime}}(x^{\\prime})+\\gamma)\\big)}{\\big(\\varphi_{\\mathrm{\\mathcal{r}}}(x)+\\gamma\\big)(\\varphi_{\\mathrm{\\mathcal{r}^{\\prime}}}(x^{\\prime})+\\gamma)(\\varphi_{\\mathrm{\\mathcal{r}}})+\\gamma}\\right\\|}\\\\ &{\\leq\\frac{1}{\\gamma}\\|(\\psi_{\\mathrm{\\mathcal{r}^{\\prime}}}(x^{\\prime})-q_{\\theta,x}(x))\\|\\nabla_{x}\\log(\\varphi_{\\mathrm{\\mathcal{r}}}(x)+\\gamma)\\|}\\\\ &{+\\frac{1}{\\gamma}(q_{\\theta,x}(x)+\\gamma)\\|\\nabla_{x}\\log(\\varphi_{\\mathrm{\\mathcal{r}}}(x)+\\gamma)-\\nabla_{x}\\log(\\varphi_{\\mathrm{\\mathcal{r}}^{\\prime},\\rho}(x)+\\gamma)\\|}\\\\ &{\\leq\\frac{B_{\\mathrm{\\mathcal{r}}}}{\\gamma}\\|\\theta_{\\mathrm{\\mathcal{r}},\\gamma}(x^{\\prime})-q_{\\theta,x}(x)\\|+\\frac{(B_{\\mathcal{r}}+\\gamma)}{\\gamma}\\|s_{\\theta,x}^{\\gamma}(z,\\epsilon)-s_{\\theta,x^{\\prime}}^{\\gamma}(z^{\\prime},\\epsilon)\\|}\\\\ &{\\leq\\frac{B_{\\mathcal{r}}H_{\\sigma}}{\\gamma^{2}}(1+a_{\\phi}\\|\\epsilon\\|+b_{\\phi})(\\|(\\theta,z)-(\\theta^{\\prime},z^{\\prime})\\|+\\mathbb{W}_{1}(r,r))}\\\\ &{+\\frac{B_{\\mathcal{r}}+\\gamma}{\\gamma^{2}}(a_{s}\\|\\epsilon\\|+b_{s})(\\|(\\theta,z)-(\\theta^{\\prime},z^{\\prime})\\|+\\mathbb{W}_{1}(r,r^{\\prime})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the last inequality follows from applying Prop. 18 and Assumption 3 to the first term and Prop. 16 to the last term . Hence, we have as desired. ", "page_idx": 25}, {"type": "text", "text": "Boundedness follows from the fact that $\\begin{array}{r}{\\left\\|\\frac{\\gamma\\nabla_{x}q_{\\theta,r}(\\phi_{\\theta}(z,\\epsilon))}{(q_{\\theta,r}(\\phi_{\\theta}(z,\\epsilon))+\\gamma)^{2}}\\right\\|\\le\\frac{1}{\\gamma}\\|\\nabla_{x}q_{\\theta,r}(\\phi_{\\theta}(z,\\epsilon))\\|\\le\\frac{B_{k}}{\\gamma}.}\\end{array}$ ", "page_idx": 25}, {"type": "text", "text": "Proposition 15. Under Assumptions 1 to 3, the map $(\\theta,r,z)\\mapsto s_{\\theta,r}^{\\gamma}(z,\\epsilon)-s_{p}(z,\\epsilon)=:d_{\\theta,r}^{p,\\gamma}(z,\\epsilon)$ satisfies the following: there exist $a_{d},b_{d}\\in\\mathbb{R}_{>0}$ such that for all $(\\theta,r),(\\theta^{\\prime},r^{\\prime})\\in\\mathcal{M}$ , an $l\\,z,z^{\\prime}\\in\\mathbb{R}^{d_{z}}$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|d_{\\theta,r}^{p,\\gamma}(z,\\epsilon)-d_{\\theta^{\\prime},r^{\\prime}}^{p,\\gamma}(z^{\\prime},\\epsilon)\\|\\le(a_{d}\\|\\epsilon\\|+b_{d})(\\|(\\theta,z)-(\\theta^{\\prime},z^{\\prime})\\|+\\mathsf{W}_{1}(r,r^{\\prime})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. Let $x:=\\phi_{\\theta}(z,\\epsilon)$ , and $x^{\\prime}:=\\phi_{\\theta^{\\prime}}(z^{\\prime},\\epsilon)$ . Then, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|d_{\\theta,r}^{p,\\gamma}(z,\\epsilon)-d_{\\theta^{\\prime},r^{\\prime}}^{p,\\gamma}(z^{\\prime},\\epsilon)\\|\\leq\\|\\nabla_{x}\\log p(x,y)-\\nabla_{x}\\log p(x^{\\prime},y)\\|}&{}\\\\ {+\\|\\nabla_{x}\\log(q_{\\theta,r}(x)+\\gamma)-\\nabla_{x}\\log(q_{\\theta^{\\prime},r^{\\prime}}(x^{\\prime})+\\gamma)\\|}&{}\\\\ {\\leq K_{p}\\|x-x^{\\prime}\\|+\\|\\nabla_{x}\\log(q_{\\theta,r}(x)+\\gamma)-\\nabla_{x}\\log(q_{\\theta^{\\prime},r^{\\prime}}(x^{\\prime})+\\gamma)\\|}&{}\\\\ {\\leq K_{p}(a_{\\phi}\\|\\epsilon\\|+b_{\\phi})\\|(\\theta,z)-(\\theta^{\\prime},z^{\\prime})\\|}&{}\\\\ {+\\left(a_{s}\\|\\epsilon\\|+b_{s})(\\|(\\theta,z)-(\\theta^{\\prime},z^{\\prime})\\|+\\mathbb{W}_{1}(r,r^{\\prime})),}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where Prop. 16 and Assumptions 1 and 3 are used. ", "page_idx": 25}, {"type": "text", "text": "Proposition 16 ( $s$ is Lipschitz). Under Assumptions 2 and 3 and $\\gamma>0$ , the map $(\\theta,r,z)\\mapsto s_{\\theta,r}^{\\gamma}(z,\\epsilon)$ satisfies the following: there exist constants $a_{s},b_{s}\\in\\mathbb{R}_{>0}$ such that the following inequality holds for all $(\\theta,r),(\\dot{\\theta}^{\\prime},r^{\\prime})\\in\\dot{\\mathcal{M}}$ , and $z,z^{\\prime}\\in\\mathbb{R}^{d_{z}}$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|s_{\\theta,r}^{\\gamma}(z,\\epsilon)-s_{\\theta^{\\prime},r^{\\prime}}^{\\gamma}(z^{\\prime},\\epsilon)\\|\\le(a_{s}\\|\\epsilon\\|+b_{s})(\\|(\\theta,z)-(\\theta^{\\prime},z^{\\prime})\\|+\\mathsf{W}_{1}(r,r^{\\prime})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. For brevity, we write $x=\\phi_{\\theta}(z,\\epsilon)$ and $x^{\\prime}=\\phi_{\\theta^{\\prime}}(z^{\\prime},\\epsilon)$ ; from the definition, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{s_{\\theta,r}^{\\gamma}(z,\\epsilon)-s_{\\theta^{\\prime},r^{\\prime}}^{\\gamma}(z^{\\prime},\\epsilon)\\|=\\Big\\|\\displaystyle\\frac{\\nabla_{x}q_{\\theta,r}(x)}{q_{\\theta,r}(x)+\\gamma}-\\displaystyle\\frac{\\nabla_{x}q_{\\theta^{\\prime},r^{\\prime}}(x^{\\prime})}{q_{\\theta^{\\prime},r^{\\prime}}(x^{\\prime})+\\gamma}\\Big\\|}&{}\\\\ {\\overset{(a)}{\\le}\\Big\\|\\displaystyle\\frac{\\nabla_{x}q_{\\theta,r}(x)}{q_{\\theta,r}(x)+\\gamma}-\\displaystyle\\frac{\\nabla_{x}q_{\\theta^{\\prime},r^{\\prime}}(x^{\\prime})}{q_{\\theta,r}(x)+\\gamma}\\Big\\|+\\Big\\|\\displaystyle\\frac{\\nabla_{x}q_{\\theta^{\\prime},r^{\\prime}}(x^{\\prime})}{q_{\\theta,r}(x)+\\gamma}-\\displaystyle\\frac{\\nabla_{x}q_{\\theta^{\\prime},r^{\\prime}}(x^{\\prime})}{q_{\\theta^{\\prime},r^{\\prime}}(x^{\\prime})+\\gamma}\\Big\\|}\\\\ {\\overset{}{\\le}\\displaystyle\\frac{1}{q_{\\theta,r}(x)+\\gamma}\\|\\nabla_{x}q_{\\theta,r}(x)-\\nabla_{x}q_{\\theta^{\\prime},r^{\\prime}}(x^{\\prime})\\|}&{}\\\\ {+\\|\\nabla_{x}q_{\\theta^{\\prime},r^{\\prime}}(x^{\\prime})\\|\\left|\\displaystyle\\frac{1}{q_{\\theta,r}(x)+\\gamma}-\\displaystyle\\frac{1}{q_{\\theta^{\\prime},r^{\\prime}}(x^{\\prime})+\\gamma}\\right|}&{}\\\\ {\\overset{(b)}{\\le}\\displaystyle\\frac{1}{\\gamma}\\|\\nabla_{x}q_{\\theta,r}(x)-\\nabla_{x}q_{\\theta^{\\prime},r^{\\prime}}(x^{\\prime})\\|+\\displaystyle\\frac{B_{k}}{\\gamma^{2}}|q_{\\theta,r}(x)-q_{\\theta^{\\prime},r^{\\prime}}(x^{\\prime})|,\\ }&{(41)}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where (a) we add and subtract the relevant terms and the triangle inequality; (b) we use the fact that $\\begin{array}{r}{\\|\\nabla_{x}q_{\\theta^{\\prime},r^{\\prime}}\\left(x^{\\prime}\\right)\\|=\\|\\int\\nabla_{x}k_{\\theta^{\\prime}}(x^{\\prime}|z)r^{\\prime}(\\mathrm{d}z)\\|\\le B_{k}}\\end{array}$ (from Cauchy-Schwartz and the boundedness part of Assumption 2). Now, we deal with the terms individually. For the first term in Eq. (41), we use the fact that the map $(\\theta,r,z)\\mapsto\\nabla_{x}q_{\\theta,r}(\\phi_{\\theta}(z,\\epsilon))$ is $K_{q}$ -Lipschitz from Prop. 17. As for the second term in Eq. (41), we apply Prop. 18. ", "page_idx": 26}, {"type": "text", "text": "Hence, we obtain ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{s_{\\theta,r}^{\\gamma}(z,\\epsilon)-s_{\\theta^{\\prime},r^{\\prime}}^{\\gamma}(z^{\\prime},\\epsilon)\\|\\leq\\left(\\displaystyle\\frac{K_{g q}}{\\gamma}+\\frac{B_{k}K_{q}}{\\gamma^{2}}\\right)(\\|(\\theta,x)-(\\theta^{\\prime},x^{\\prime})\\|+\\mathsf{W}_{1}(r,r^{\\prime}))}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\left(\\displaystyle\\frac{K_{g q}}{\\gamma}+\\frac{B_{k}K_{q}}{\\gamma^{2}}\\right)(1+a_{\\phi}\\|\\epsilon\\|+b_{\\phi})(\\|(\\theta,z)-(\\theta^{\\prime},z^{\\prime})\\|+\\mathsf{W}_{1}(r,r^{\\prime})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where we use Assumption 3 for the last line. ", "page_idx": 26}, {"type": "text", "text": "Proposition 17. Under Assumption 2, the map $(\\theta,r,x)\\mapsto\\nabla_{x}q_{\\theta,r}(x)$ is Lipschitz, i.e., for all $\\epsilon$ , there exists a $K_{g q}\\in\\mathbb{R}_{>0}$ such that the following inequality holds for all $(\\vec{\\theta^{,}}r),(\\theta^{\\prime},r^{\\prime})\\in\\mathcal{M}$ and $z,z^{\\prime}\\in\\mathbb{R}^{d_{z}}$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla_{x}q_{\\theta,r}(x)-\\nabla_{x}q_{\\theta^{\\prime},r^{\\prime}}(x^{\\prime})\\|\\leq K_{g q}(\\|(\\theta,x)-(\\theta^{\\prime},x^{\\prime})\\|+\\mathsf{W}_{1}(r,r^{\\prime})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. From direct computation, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|\\nabla_{x}q_{\\theta,r}\\left(x\\right)-\\nabla_{x}q_{\\theta^{\\prime},r^{\\prime}}\\left(x^{\\prime}\\right)\\|=\\bigg\\|\\bigg\\int[\\nabla_{x}k_{\\theta}(x|z)r\\left(z\\right)-\\nabla_{x}k_{\\theta^{\\prime}}(x^{\\prime}|z)r^{\\prime}\\left(z\\right)]\\,\\mathrm{d}z\\bigg\\|}&{}\\\\ {\\overset{(a)}{\\leq}\\int\\|\\nabla_{x}[k_{\\theta}(x|z)-k_{\\theta^{\\prime}}(x^{\\prime}|z)]\\|\\,r\\left(z\\right)\\mathrm{d}z}&{}\\\\ {+\\int\\|\\nabla_{x}k_{\\theta^{\\prime}}(x^{\\prime}|z)\\|\\,|r-r^{\\prime}|\\left(z\\right)\\mathrm{d}z}&{}\\\\ {\\overset{(b)}{\\leq}K_{g q}(\\|(\\theta,x)-(\\theta^{\\prime},x^{\\prime})\\|+\\mathsf{W}_{1}(r,r^{\\prime})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where (a) we add and subtract the appropriate terms, apply the triangle inequality and the CauchySchwarz inequality; (b) for the first term, we use the Lipschitz gradient Assumption 2; and for the second term, we use the dual representation of $\\mathsf{W}_{1}$ with the fact map $z\\mapsto\\|\\nabla_{x}\\log k_{\\theta}(x|z)\\|$ is $K_{k}$ -Lipschitz from the reverse triangle inequality and the Lipschitz Assumption 2. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "Proposition 18. Under Assumption 2, the map $(\\theta,r,x)\\mapsto q_{\\theta,r}(x)$ is Lipschitz, i.e., there exists some $K_{q}\\in\\mathbb{R}_{>0}$ such that for all $(\\theta,r,x),(\\theta^{\\prime},r^{\\prime},x^{\\prime})\\in\\Theta\\times\\mathcal{P}(\\mathbb{R}^{d_{z}})\\times\\mathbb{R}^{d_{x}}$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n|q_{\\theta,r}(x)-q_{\\theta^{\\prime},r^{\\prime}}(x^{\\prime})|<K_{q}(||(\\theta,x)-(\\theta^{\\prime},x^{\\prime})||+\\mathsf{W}_{1}(r,r^{\\prime})).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. From direct computation, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{|q_{\\theta,r}(x)-q_{\\theta^{\\prime},r^{\\prime}}(x^{\\prime})|\\le|q_{\\theta,r}(x)-q_{\\theta,r^{\\prime}}(x)|+|q_{\\theta,r^{\\prime}}(x)-q_{\\theta^{\\prime},r^{\\prime}}(x^{\\prime})|}}\\\\ &{}&{\\le\\displaystyle\\int|k_{\\theta}(x|z)||r-r^{\\prime}|(z)\\mathrm{d}z+\\displaystyle\\int|k_{\\theta}(x|z)-k_{\\theta^{\\prime}}(x^{\\prime}|z)|r(z)\\mathrm{d}z}\\\\ &{}&{\\overset{(a)}{\\le}K_{q}(\\mathsf{W}_{1}(r,r^{\\prime})+\\|(\\theta,x)-(\\theta,x^{\\prime})\\|)}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where (a) for the first term, we use the fact that the map $z\\;\\mapsto\\;|k_{\\theta}(x|z)|$ is $B_{k}$ -Lipschitz (from the bounded gradient of Assumption 2), and again the Lipschitz property of $k$ from the same assumption. \u53e3 ", "page_idx": 27}, {"type": "text", "text": "G Algorithmic details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "G.1 Gradient estimator ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Proof of Prop. 6. We show the derivation of the estimators in Eq. (10). Eq. (9) will follow similarly. We have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{z}\\delta_{r}\\mathcal{E}[\\theta,r](z)=\\nabla_{z}\\mathbb{E}_{k_{\\theta}(x|z)}\\left[\\log\\frac{q_{\\theta,r}(x)}{p(y,x)}\\right]}\\\\ &{\\qquad\\qquad\\qquad=\\nabla_{z}\\mathbb{E}_{\\epsilon\\sim p_{k}}\\left[\\log\\frac{q_{\\theta,r}(\\phi_{\\theta}(z,\\epsilon))}{p(y,\\phi_{\\theta}(z,\\epsilon))}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Assuming that $\\phi_{\\theta}$ and $p_{k}$ are sufficiently regular to justify the interchange of differentiation and integration, we obtain ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\nabla_{z}\\delta_{r}\\mathcal{E}[\\theta,r](z)=\\mathbb{E}_{\\epsilon\\sim p_{k}}\\left[\\nabla_{z}\\log\\frac{q_{\\theta,r}(\\phi(z,\\epsilon))}{p(y,\\phi(z,\\epsilon))}\\right].\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "To obtain as desired, one can apply the chain rule. ", "page_idx": 27}, {"type": "text", "text": "Similarly, one can derive a Monte Carlo gradient estimator for $\\nabla_{z}\\delta_{r}\\mathcal{E}^{\\gamma}$ as follows: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{z}\\delta_{r}\\mathcal{E}^{\\gamma}[\\theta,r](z)=\\nabla_{z}\\mathbb{E}_{k_{\\theta}(x\\mid z)}\\left[\\log\\frac{q_{\\theta,r}(x)+\\gamma}{p(y,x)}+\\frac{q_{\\theta,r}(x)}{q_{\\theta,r}(x)+\\gamma}\\right]}\\\\ &{\\qquad\\qquad\\qquad=\\nabla_{z}\\mathbb{E}_{\\epsilon\\sim p_{k}}\\left[\\log\\frac{q_{\\theta,r}(\\phi_{\\theta}(z,\\epsilon))+\\gamma}{p(y,\\phi_{\\theta}(z,\\epsilon))}+\\frac{q_{\\theta,r}(\\phi_{\\theta}(z,\\epsilon))}{q_{\\theta,r}(\\phi_{\\theta}(z,\\epsilon))+\\gamma}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "As before, if $\\phi_{\\theta}$ and $p_{k}$ are sufficiently regular, we obtain ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\nabla_{z}\\delta_{r}\\mathcal{E}^{\\gamma}[\\theta,r](z)=\\mathbb{E}_{\\epsilon\\sim p_{k}}\\left[\\nabla_{z}\\log\\frac{q_{\\theta,r}(\\phi(z,\\epsilon))+\\gamma}{p(y,\\phi(z,\\epsilon))}+\\frac{\\gamma\\nabla q_{\\theta,r}(\\phi_{\\theta}(z,\\epsilon))}{(q_{\\theta,r}(\\phi_{\\theta}(z,\\epsilon))+\\gamma)^{2}}\\right].\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "To obtain as desired, one can apply chain rule. ", "page_idx": 27}, {"type": "text", "text": "G.2 Preconditioners ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Recall that the preconditioned gradient flow is given by ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\theta_{t}=-\\Psi_{t}^{\\theta}\\nabla_{\\theta}\\mathcal{E}_{\\lambda}(\\theta_{t},r_{t})\\,\\mathrm{d}t,\\,\\,\\,\\partial_{t}r_{t}=\\nabla_{z}\\cdot\\bigl(r_{t}\\Psi_{t}^{r}\\nabla_{z}\\delta_{r}\\mathcal{E}_{\\lambda}[\\theta_{t},r_{t}]\\bigr),}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $\\delta_{r}\\mathcal{E}_{\\lambda}[\\theta,r]=\\delta_{r}\\mathcal{E}[\\theta,r]+\\log r/p_{0}$ We can rewrite the dynamics of $r_{t}$ as ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\partial_{t}r_{t}=\\nabla_{z}\\cdot\\big(r_{t}\\Psi_{t}^{r}\\nabla_{z}\\big[\\delta_{r}\\mathcal{E}\\big[\\theta_{t},r_{t}\\big]-\\log p_{0}+\\log r_{t}\\big]\\big),}\\\\ &{\\qquad=\\nabla_{z}\\cdot\\big(r_{t}\\Psi_{t}^{r}\\nabla_{z}\\big[\\delta_{r}\\mathcal{E}\\big[\\theta_{t},r_{t}\\big]-\\log p_{0}\\big]\\big)+\\nabla_{z}\\cdot\\big(r_{t}\\Psi_{t}^{r}\\nabla_{z}\\log r_{t}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The second term can be written as ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\nabla_{z}\\cdot\\left(r_{t}\\Psi_{t}^{r}\\nabla_{z}\\log r_{t}\\right)=\\nabla_{z}\\cdot\\left(\\Psi_{t}^{r}\\nabla_{z}r_{t}\\right)=\\nabla_{z}\\cdot\\left(\\nabla_{z}\\cdot\\left[\\Psi_{t}^{r}r_{t}\\right]\\right)-\\nabla_{z}\\cdot\\left(r_{t}\\nabla_{z}\\cdot\\Psi_{t}^{r}\\right)\n$$", "text_format": "latex", "page_idx": 27}, {"type": "table", "img_path": "p3gMGkHMkM/tmp/b38c8fb274e858a230b5b6e5ab228739b1bcae398fdf75a8053d8cb4eb26c778.jpg", "table_caption": [], "table_footnote": ["Table 3: Neural network architecture defined by $\\overline{{\\mathrm{NN}}}(d_{i n},d_{h},d_{o u t})$ . "], "page_idx": 28}, {"type": "text", "text": "where $\\begin{array}{r}{(\\nabla\\cdot\\Psi_{t}^{r})_{i}=\\sum_{j=1}^{d_{z}}\\partial_{z_{j}}[(\\Psi_{t}^{r})_{i j}]}\\end{array}$ , and last equality holds since ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{d_{z}}\\partial_{z_{i}}\\left\\{\\sum_{j=1}^{d_{z}}(\\Psi_{t}^{r})_{i j}\\partial_{z_{j}}r_{t}\\right\\}=\\sum_{i=1}^{d_{z}}\\partial_{z_{i}}\\left\\{\\sum_{j=1}^{d_{z}}\\left(\\partial_{z_{j}}\\left[(\\Psi_{t}^{r})_{i j}r_{t}\\right]-r_{t}\\partial_{z_{j}}[(\\Psi_{t}^{r})_{i j}]\\right)\\right\\}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Hence, we have the following dynamics of $r_{t}$ : ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\partial_{t}\\boldsymbol{r}_{t}=\\nabla_{z}\\cdot\\left(\\boldsymbol{r}_{t}\\left(\\Psi_{t}^{r}\\nabla_{z}[\\delta_{r}\\mathcal{E}[\\theta_{t},\\boldsymbol{r}_{t}]-\\log p_{0}]-\\nabla_{z}\\cdot\\Psi_{t}^{r}\\right)\\right)+\\nabla_{z}\\cdot\\left(\\nabla_{z}\\cdot[\\Psi_{t}^{r}\\boldsymbol{r}_{t}]\\right)\\right)\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Examples. Following in the essence of RMSProp (Tieleman and Hinton, 2012), we utilize the preconditioner defined as follows: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{B_{k}=\\beta B_{k-1}+(1-\\beta)\\mathrm{Diag}(A(\\{\\nabla_{z}\\delta_{r}\\mathcal{E}[\\theta_{k},r_{k}](Z_{m})^{2}\\}_{m=1}^{M}))}\\\\ {\\Psi_{k}^{r}(Z)=(B_{k})^{-0.5}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $B_{k}\\in\\mathbb{R}^{d_{z}\\times d_{z}}$ and $A$ is some aggregation function such as the mean or max. The idea is to normalize by the aggregated gradient of the first variation across all the particles since this is the dominant component in the drift of PVI. Similarly to RMSProp, it keeps an exponential moving average of the squared gradient which can then be used in the preconditioner. ", "page_idx": 28}, {"type": "text", "text": "H Experimental details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In this section, we highlight additional details for reproducibility and computation. The code was written in JAX (Bradbury et al., 2018) and executed on a NVIDIA GeForce RTX 4090. ", "page_idx": 28}, {"type": "text", "text": "H.1 Section 5.1 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Hyperparameters. For the neural network, we use $f_{\\theta}\\,=\\,\\mathrm{NN}(2,128,2)$ defined in Table 3, the number of particles $M=100$ , $d_{z}=2$ , $K=1000$ , $h_{\\theta}=10^{-4}$ , $h_{z}=10^{-2}$ , $\\lambda_{r}=10^{-8}$ , for $\\Psi^{\\theta}$ we use RMSProp and we set ${\\Psi}^{r}=I_{2}$ . ", "page_idx": 28}, {"type": "text", "text": "Computation Time. Each run took 8 seconds using JIT compilation. ", "page_idx": 28}, {"type": "text", "text": "H.2 Section 5.2 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In this section, we outline all the experimental details regarding Section 5.2. ", "page_idx": 28}, {"type": "text", "text": "Densities. Table 4 shows the densities used in the toy experiments. ", "page_idx": 28}, {"type": "table", "img_path": "p3gMGkHMkM/tmp/1312261e745246c53afbcc7707b79bac6232c8afb316fd56d1bf5782c3d7b43e.jpg", "table_caption": [], "table_footnote": ["Table 4: Densities used in toy experiments (see Section 5.2). "], "page_idx": 28}, {"type": "text", "text": "Hyperparameters. We set the number of parameter updates and particle steps to be $K=15000$ , and $d_{z}=2$ . ", "page_idx": 28}, {"type": "text", "text": "\u2022 $f_{\\theta}$ . We use $f_{\\theta}=\\mathrm{{NN}}(2,512,2)$ .   \n\u2022 PVI. We use $M\\,=\\,100$ , $\\lambda_{\\theta}\\,=\\,0$ , $\\lambda_{r}\\,=\\,10^{-8}$ , $h_{x}\\,=\\,10^{-2}$ , $h_{\\theta}\\,=\\,10^{-4}$ , $\\Psi^{\\theta}$ we use the RMSProp preconditoner, $\\Psi^{r}=I_{d_{z}}$ , and $L=250$ .   \n\u2022 SVI. We use $K=50$ to estimate the objective (Yin and Zhou, 2018, see Eq. 5) which are around the values used in (Yin and Zhou, 2018). We utilize RMSProp with step size $10^{-4}$ , and $r=\\mathcal{N}(0,I_{d_{z}})$ . The implicit distribution is set to $r=\\mathcal{N}(0,I_{d_{z}})$ .   \n\u2022 UVI. For the HMC sampler, we follow in (Titsias and Ruiz, 2019) and use 50 burn-in steps, with step-size $10^{-1}$ and 5 leap-frog steps. We use the RMSProp optimizer with stepsize $10^{-4}$ for $k_{\\theta}$ . The implicit distribution is set to $r=\\mathcal{N}(0,I_{d_{z}})$ .   \n\u2022 SM. For the \u201cdual\u201d function written as $f$ in the original paper (Yu and Zhang, 2023, see Algorithm 1) we use $\\mathrm{NN}(2,512,2)$ . We utilize RMSProp with decaying learning rate from $10^{\\overline{{-4}}}$ to $10^{\\overset{\\cdot}{-5}}$ to optimize the kernel $k_{\\theta}$ , and RMSProp with $10^{-3}$ to $10^{-4}$ for the dual function $f$ . The implicit distribution is set to $r=\\mathcal{N}(0,I_{d_{z}})$ . ", "page_idx": 29}, {"type": "text", "text": "Sliced Wasserstein Distance. We report the average sliced Wasserstein distance using 100 projections computed from 10000 samples from the target and the variational distribution. ", "page_idx": 29}, {"type": "text", "text": "Two-Sample Test. We use the MMD-Fuse implementation found in https://github.com/ antoninschrab/mmdfuse.git. ", "page_idx": 29}, {"type": "text", "text": "Computation Time. An example run on Banana with JIT compilation, PVI took 42 seconds, UVI took 10 minutes 36 seconds, SM took 45 seconds, and SVI took 38 seconds. ", "page_idx": 29}, {"type": "text", "text": "H.3 Section 5.3 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In this section, we outline all the hyperparameters for each method used. ", "page_idx": 29}, {"type": "text", "text": "Hyperparameters. We use $K=20000$ set $d_{z}=10$ . For all kernel parameters, we use RMSProp preconditioner with step size $h_{\\theta}=10^{-3}$ . ", "page_idx": 29}, {"type": "text", "text": "\u2022 $f_{\\theta}$ . We use $f_{\\theta}=\\mathrm{NN}(d_{z},512,22)$ .   \n\u2022 PVI. We use $M\\,=\\,100$ , $\\lambda_{\\theta}\\,=\\,0$ , $\\lambda_{r}\\,=\\,10^{-8}$ , $h_{x}\\,=\\,10^{-2}$ , and for $\\Psi^{r}$ we use the one described in App. G.2 with mean as the aggregate function.   \n\u2022 SVI. We use $K=50$ to estimate the objective (Yin and Zhou, 2018, see Eq. 5) which are around the values used in (Yin and Zhou, 2018).   \n\u2022 UVI. For the HMC sampler, we follow in (Titsias and Ruiz, 2019) and use 50 burn-in steps, with step-size $10^{-1}$ and 5 leap-frog steps.   \n\u2022 SM. We were unable to improve the performance of SM with our chosen kernel and instead used the implementation in https://github.com/longinYu/SIVISM?utm_source= catalyzex.com to obtain posterior samples with implementation details found in the code repository and in the paper (Yu and Zhang, 2023). ", "page_idx": 29}, {"type": "text", "text": "H.4 Section 5.4 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In this section, we outline all the experimental details regarding Section 5.4. ", "page_idx": 29}, {"type": "text", "text": "Model. We consider the neural network $\\mathrm{BNN}(d_{i n}^{\\mathrm{bnn}},d_{h}^{\\mathrm{bnn}})$ defined as $f_{x}(o)=W_{2}^{\\top}\\mathrm{ReLU}(W_{1}^{\\top}o+$ $b_{1})+b_{2}$ where $\\mathrm{~o~}\\in\\mathbb{R}^{d_{i n}^{\\mathrm{bnn}}}$ , $x\\,=\\,[\\mathrm{vec}(W_{2}),b_{2},\\mathrm{vec}(W_{1}),b_{1}]^{\\top}$ , $W_{2}\\,\\in\\,\\mathbb{R}^{d_{h}^{\\mathrm{bnn}}\\times1}$ , $b_{2}\\in\\mathbb{R}$ , $W_{1}~\\in$ $\\mathbb{R}^{d_{i n}^{\\mathrm{bnn}}\\times d_{h}^{\\mathrm{bnn}}}$ , $b_{1}\\in\\mathbb{R}^{d_{h}^{\\mathrm{bnn}}}$ . Given an input-output pair $\\pmb{Y}:=\\{(O_{i},Y_{i})\\}_{i=1}^{B}$ , the model can be defined as $p(Y,x)\\,=\\,p(Y|x)p(x)$ where the likelihood is $\\begin{array}{r}{p(Y|x)=\\prod_{i=1}^{B}\\mathcal{N}(Y_{i};f_{x}(O_{i}),0.01^{2})}\\end{array}$ and the prior is $\\mathcal{N}(x;0,25I)$ . ", "page_idx": 29}, {"type": "text", "text": "Datasets. For all the datasets, we standardize by removing the mean and dividing by the standard deviation. ", "page_idx": 29}, {"type": "text", "text": "\u2022 Protein. For the model, we use $\\mathrm{BNN}(9,30)$ which results in the problem having dimension $d_{x}=331$ . The dataset is composed of 1600 train examples, 401 test examples. ", "page_idx": 29}, {"type": "text", "text": "\u2022 Yacht. For the model, we use $\\mathrm{BNN}(6,10)$ which results in the problem having dimension $d_{x}=81$ . The dataset is composed of 246 train examples and 62 test examples. \u2022 Concrete For the model, we use $\\mathrm{BNN}(8,10)$ which results in the problem having dimension $d_{x}=101$ . The dataset comprises of 824 training examples and 206 test examples. ", "page_idx": 30}, {"type": "text", "text": "Hyperparameters. We use $K=1500$ set $d_{z}=10$ . For all kernel parameters, we use RMSProp preconditioner with step size $h_{\\theta}\\,=\\,10^{-3}$ that decays to $10^{-5}$ following a constant schedule that transitions every 100 parameters steps. ", "page_idx": 30}, {"type": "text", "text": "\u2022 $f_{\\theta},\\sigma_{\\theta}$ . We use $f_{\\theta}=\\mathrm{NN}(d_{z},512,d_{x})$ and $\\sigma_{\\theta}=\\mathrm{Softplus}(\\mathrm{NN}(d_{z},512,d_{x}))+10^{-8}$ and they share parameters except for the last layers.   \n\u2022 PVI. We use $M\\,=\\,100$ , $\\lambda_{\\theta}\\,=\\,0$ , $\\lambda_{r}\\,=\\,10^{-3}$ , $h_{x}\\,=\\,10^{-3}$ , and for $\\Psi^{r}$ we use the one described in App. G.2 with mean as the aggregate function.   \n\u2022 SVI. We use $K\\,=\\,50$ to estimate the objective (Yin and Zhou, 2018, see Eq. 5) which are around the values used in (Yin and Zhou, 2018). The implicit distribution is set to $r=\\mathcal{N}(0,I_{d_{z}})$ .   \n\u2022 UVI. For the HMC sampler, we follow in (Titsias and Ruiz, 2019) and use 50 burn-in steps, with step-size $10^{-1}$ and 5 leap-frog steps. The implicit distribution is set to $r=\\mathcal{N}(0,\\bar{I_{d_{z}}})$ .   \n\u2022 SM. For the \u201cdual\u201d function written as $f$ in the original paper ( $\\mathrm{Yu}$ and Zhang, 2023, see Algorithm 1) we use $\\mathrm{NN}(d_{x},512,d_{x})$ and trained with RMSProp with stepsize $10^{-2}$ . We tried a decaying learning schedule to $10\\mathrm{-4}$ but found that this degraded the performance. We used ReLU activations instead as we found that using leaky ReLUs harmed performance. The implicit distribution is set to $r=\\mathcal{N}(0,I_{d_{z}})$ . ", "page_idx": 30}, {"type": "text", "text": "Computation Time. For each run in the Concrete dataset with JIT compilation, PVI took 37 seconds, UVI took approximately 1 minute 40 seconds, SVI took 30 seconds, and SM took 27 seconds. ", "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We clearly define our paper scope and contributions at the end of the introduction with references to why they are accurate. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: We highlight the limitations in Section 6. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 31}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: For each result, we outline relevant assumptions which can be found in the appendix. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: These can be found in App. H. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 32}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We provide code for reproducibility. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 33}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: These can be found in App. H and explanations can be found in Section 5. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 33}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: In our results, we report the mean and standard deviations from independent trials which can be found in Tables 1 and 2. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We outline the computational resource in App. H. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 34}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We conform. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 34}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: We do not release data or models that have a high risk of misuse. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 35}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: Yes, we reference the dataset used. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 35}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: We do not release new assets. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: No human subjects were used. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 36}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: We did not have human participants. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 36}]