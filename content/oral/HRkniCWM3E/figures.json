[{"figure_path": "HRkniCWM3E/figures/figures_4_1.jpg", "caption": "Fig. 1: Schematic of the Slater determinant (1a) and our NeurPf (1b). Where the Slater formulation requires exactly N<sub>e</sub> orbital functions, the Pfaffian formulation works for any number N<sub>o</sub> \u2265 max{N<sub>\u2191</sub>, N<sub>\u2193</sub>} of orbital functions, indicated by the rectangular orbital blocks.", "description": "This figure illustrates the architectural differences between the traditional Slater determinant approach and the proposed Neural Pfaffian method for constructing electronic wave functions.  The left panel (a) depicts the Slater determinant, which requires a square matrix of exactly N<sub>e</sub> orbitals (where N<sub>e</sub> is the total number of electrons), with separate orbitals for spin-up and spin-down electrons. The right panel (b) shows the Neural Pfaffian, which allows for an over-parameterized approach where the number of orbitals (N<sub>o</sub>) can be greater than or equal to the maximum of N<sub>\u2191</sub> and N<sub>\u2193</sub> (the number of spin-up and spin-down electrons, respectively). This flexibility is achieved by utilizing a Pfaffian instead of a determinant, enabling a more general and potentially more accurate representation of the wave function.", "section": "4 Neural Pfaffian"}, {"figure_path": "HRkniCWM3E/figures/figures_6_1.jpg", "caption": "Fig. 1: Schematic of the Slater determinant (1a) and our NeurPf (1b). Where the Slater formulation requires exactly N\u2191 + N\u2193 orbital functions, the Pfaffian formulation works for any number N\u00b0 \u2265 max{N\u2191, N\u2193} of orbital functions, indicated by the rectangular orbital blocks.", "description": "This figure compares the architecture of the Slater determinant and Neural Pfaffian wave functions.  The Slater determinant requires exactly N\u2191 + N\u2193 orbitals while the Neural Pfaffian uses N\u00b0 \u2265 max{N\u2191, N\u2193} orbitals, offering greater flexibility and allowing for overparametrization. The figure highlights the key differences in their structure, emphasizing the advantage of the Neural Pfaffian's flexibility in handling different numbers of orbitals.", "section": "4 Neural Pfaffian"}, {"figure_path": "HRkniCWM3E/figures/figures_7_1.jpg", "caption": "Fig. 3: Ground state, electron affinity, and ionization potential errors of second-row elements during training. A single NeurPf has been trained on all systems jointly while references (Pfau et al., 2020) were calculated separately for each system. Energies are averaged over the last 10% of steps.", "description": "This figure displays the results of training a single Neural Pfaffian (NeurPf) model on second-row elements.  The plot shows the errors in ground state energy, electron affinity, and ionization potential during the training process.  The key takeaway is that NeurPf achieves chemical accuracy across these properties, even though a single model was trained on all elements simultaneously.  This contrasts with previous methods (Pfau et al., 2020) which trained separate models for each element.", "section": "Atomic systems and spin configurations"}, {"figure_path": "HRkniCWM3E/figures/figures_7_2.jpg", "caption": "Fig. 4: Potential energy surface of nitrogen. Energies are relative to Le Roy et al. (2006).", "description": "This figure displays the potential energy surface of the nitrogen molecule (N2).  It compares the energy errors (in millihartrees, mEh) of different neural network models (NeurPf with and without ethene data augmentation, Globe with and without ethene data, FermiNet, and PESNet) against the experimental data from Le Roy et al. (2006). The x-axis represents the internuclear distance (in units of Bohr radius, a0), and the y-axis represents the energy error. The figure highlights how well the NeurPf model generalizes to different systems even when trained only on the nitrogen dimer, significantly reducing errors compared to other models when incorporating data from additional molecules (ethene) in the training data.", "section": "Effect of uncorrelated data"}, {"figure_path": "HRkniCWM3E/figures/figures_8_1.jpg", "caption": "Fig. 5: Convergence of mean energy difference on the TinyMol dataset from Scherbela et al. (2024). The y-axis is linear < 1 and logarithmic > 1. Due to the variational principle, NeurPf is better than the reference CCSD(T) on the small molecules.", "description": "This figure shows the convergence of the mean energy difference on the TinyMol dataset for different models (NeurPf, TAO, Globe) as a function of training steps. The y-axis represents the mean energy difference compared to the CCSD(T) reference energy. The plot is divided into two subplots, one for small molecules and one for large molecules. The shaded region highlights the improvement achieved by NeurPf over the CCSD(T) reference energy. The figure demonstrates that NeurPf converges to lower energy values than the other models and outperforms CCSD(T) for small molecules.", "section": "Experiments"}, {"figure_path": "HRkniCWM3E/figures/figures_20_1.jpg", "caption": "Fig. 6: Energy per atom of hydrogen chains with different lengths. The energy is computed with a single NeurPf trained on the hydrogen chains with 6 and 10 atoms.", "description": "This figure shows the energy per atom of hydrogen chains with varying lengths.  A single Neural Pfaffian (NeurPf) model, trained on data from hydrogen chains with 6 and 10 atoms, was used to predict the energy per atom for chains of different lengths. The results are compared against other methods (TAO, Globe + Moon, Globe + FermiNet, Hartree-Fock, and AFQMC), highlighting the NeurPf's ability to generalize to longer chains not included in its training data.", "section": "Extensivity on hydrogen chains"}, {"figure_path": "HRkniCWM3E/figures/figures_21_1.jpg", "caption": "Fig. 7: Ionization energies of metal atoms. The ionization energies are computed with a single NeurPf trained on the neutral and ionized atoms. Reference energies are taken from Martin & Musgrove (1998).", "description": "This figure shows the ionization energy errors for several metal atoms (Na, Mg, Al, K, Ca) during the training of a single Neural Pfaffian (NeurPf) model.  The model was trained on both neutral and ionized states of these atoms. The y-axis represents the error in ionization energy, and the x-axis shows the training steps. A horizontal dashed line indicates chemical accuracy.  The results demonstrate that NeurPf can accurately predict the ionization energies of these metal atoms, achieving chemical accuracy.", "section": "Atomic systems and spin configurations"}, {"figure_path": "HRkniCWM3E/figures/figures_21_2.jpg", "caption": "Fig. 8: Energy convergence as a function of time.", "description": "This figure compares the convergence speed of different models on the TinyMol dataset.  The left panel shows results for smaller molecules, and the right panel shows results for larger molecules.  The x-axis represents training time in hours, and the y-axis represents the total energy. Four different models are compared: NeurPf, NeurPf with FermiNet embedding network, NeurPf with PsiFormer embedding network, and Globe. The results show that NeurPf converges faster and achieves lower energy than the Globe method.", "section": "H TinyMol convergence in time"}, {"figure_path": "HRkniCWM3E/figures/figures_22_1.jpg", "caption": "Fig. 9: Ablation study on the small TinyMol dataset. The y-axis shows the sum of all energies in the dataset. The left plot shows the convergence in terms of the number of steps. The right plot shows the convergence in terms of time. our + Full Env. shows a NeurPf with the envelopes from Spencer et al. (2020) and our + Bottleneck Env. uses the bottleneck envelopes from Pfau et al. (2024).", "description": "This figure presents an ablation study on the small TinyMol dataset to compare the performance of different envelope functions used within the Neural Pfaffian model.  The left graph shows the total energy convergence over training steps, and the right graph shows the convergence over training time. Four model variants are compared: the AGP model, the Neural Pfaffian with full envelopes (from Spencer et al., 2020), the Neural Pfaffian with bottleneck envelopes (from Pfau et al., 2024), and the Neural Pfaffian with the authors' efficient envelopes.  The results illustrate the impact of the different envelope choices on the speed and accuracy of the model's convergence.", "section": "J Model ablation studies"}, {"figure_path": "HRkniCWM3E/figures/figures_22_2.jpg", "caption": "Fig. 10: TinyMol ablation with fixed and learnable antisymmetrizer.", "description": "This figure shows the ablation study on the TinyMol dataset with fixed and learnable antisymmetrizers. The results show that using a learnable antisymmetrizer leads to significantly better performance on both the small and large molecules compared to using a fixed antisymmetrizer. The plots show that the mean absolute error decreases significantly faster when using a learnable antisymmetrizer for both small and large datasets, indicating that the model is learning to better approximate the wavefunction.", "section": "J Model ablation studies"}, {"figure_path": "HRkniCWM3E/figures/figures_22_3.jpg", "caption": "Fig. 11: Ablation study on the small TinyMol dataset with different embedding networks.", "description": "This figure displays the ablation study results on the small TinyMol dataset using different embedding networks. It compares the performance of three different embedding networks: Moon, FermiNet, and PsiFormer, within the Neural Pfaffian framework, and contrasts them against the CCSD(T) reference energies. The plot shows the mean absolute error (MAE) in millihartrees (mEh) against training steps for both small and large molecule sets.", "section": "J Model ablation studies"}, {"figure_path": "HRkniCWM3E/figures/figures_23_1.jpg", "caption": "Fig. 12: Boxplot of the energy per molecule on both TinyMol small and large datasets for NeurPf, TAO, and the pretrained TAO from Scherbela et al. (2024). Each boxplot contains results from 10 structures for the given molecule. The line indicates the mean, the box the interquartile range, and the whiskers the 1.5 times the interquartile range.", "description": "This figure presents box plots comparing the energy per molecule calculated by NeurPf, TAO, and a pretrained version of TAO on the TinyMol dataset.  The dataset includes small and large molecule subsets, each containing 10 different molecular structures.  The box plots display the median, interquartile range, and 1.5 times the interquartile range of the energy for each molecule, enabling a visual comparison of the performance differences between the methods.", "section": "J TinyMol results"}, {"figure_path": "HRkniCWM3E/figures/figures_24_1.jpg", "caption": "Fig. 5: Convergence of mean energy difference on the TinyMol dataset from Scherbela et al. (2024). The y-axis is linear < 1 and logarithmic > 1. Due to the variational principle, NeurPf is better than the reference CCSD(T) on the small molecules.", "description": "This figure shows the convergence of the mean energy difference between the calculated energies using Neural Pfaffian (NeurPf) and the reference CCSD(T) energies from the TinyMol dataset, as training progresses. The plot includes data for both small and large molecules. The y-axis uses a logarithmic scale for values above 1, and a linear scale for values below 1.  The results demonstrate that NeurPf achieves lower energies than the reference CCSD(T) for the small molecules and converges towards more accurate results for the large molecules as training continues.  This highlights the efficacy of the Neural Pfaffian approach.", "section": "Experiments"}, {"figure_path": "HRkniCWM3E/figures/figures_24_2.jpg", "caption": "Fig. 14: Comparison of the energy per molecule on the TinyMol dataset for training jointly on all structures vs training a model per structure.", "description": "This figure compares the convergence behavior of total energy on the TinyMol dataset using two different training approaches: joint training (a generalized wave function trained on all molecules simultaneously) and separate training (a separate model trained for each molecule).  The plot shows the energy error relative to the CCSD(T) CBS reference energy as a function of the total training steps.  The results demonstrate the trade-off between training efficiency and accuracy using a generalized model versus a more tailored, but computationally expensive, approach for each molecule.", "section": "L Pretraining on the TinyMol dataset"}, {"figure_path": "HRkniCWM3E/figures/figures_25_1.jpg", "caption": "Fig. 15: Time per training step depending on the number of electrons in two molecules.", "description": "This figure shows a heatmap representing the time taken per training step for various combinations of electron counts in two molecules. The x-axis and y-axis both represent the number of electrons (Ne) in molecule 1 and molecule 2 respectively. Each cell in the heatmap displays the time (in seconds) required per training step for the corresponding combination of electron counts. The color scale indicates the time taken, with darker shades representing shorter times and lighter shades representing longer times. This figure helps in visualizing the impact of the number of electrons on training efficiency. Notably, the diagonal elements (where the number of electrons in both molecules is the same) generally show shorter training times compared to off-diagonal elements, suggesting a potential relationship between computational efficiency and balanced system sizes.", "section": "L Pretraining on the TinyMol dataset"}, {"figure_path": "HRkniCWM3E/figures/figures_25_2.jpg", "caption": "Fig. 16: Time for the forward pass, gradient and Laplacian computation of determinant vs our Pfaffian implementation.", "description": "This figure compares the computation time for the forward pass, gradient, and Laplacian of both Slater determinant and Neural Pfaffian wave functions.  The x-axis represents the number of electrons (Ne), and the y-axis shows the computation time in milliseconds.  It demonstrates that while both have the same complexity O(N\u00b3), Neural Pfaffian is approximately 5 times slower than the Slater determinant. This is likely due to the lack of highly optimized CUDA kernels available for the Pfaffian computation.", "section": "O Pfaffian runtime"}]