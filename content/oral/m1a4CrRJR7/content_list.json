[{"type": "text", "text": "Generalization Error Bounds for Two-stage Recommender Systems with Tree Structure ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jin Zhang1, Ze Liu2, Defu Lian\u22171, 2, 3, Enhong Chen1, 2, 3 ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 School of Artificial Intelligence and Data Science, University of Science and Technology of China 2 School of Computer Science and Technology, University of Science and Technology of China 3 State Key Laboratory of Cognitive Intelligence, Hefei, Anhui, China ", "page_idx": 0}, {"type": "text", "text": "{jinzhang21, lz123}@mail.ustc.edu.cn, {liandefu, cheneh}@ustc.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Two-stage recommender systems play a crucial role in efficiently identifying relevant items and personalizing recommendations from a vast array of options. This paper, based on an error decomposition framework, analyzes the generalization error for two-stage recommender systems with a tree structure, which consist of an efficient tree-based retriever and a more precise yet time-consuming ranker. We use the Rademacher complexity to establish the generalization upper bound for various tree-based retrievers using beam search, as well as for different ranker models under a shifted training distribution. Both theoretical insights and practical experiments on real-world datasets indicate that increasing the branches in tree-based retrievers and harmonizing distributions across stages can enhance the generalization performance of two-stage recommender systems. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recommender systems play a crucial role in many online services, such as e-commerce [29], digital streaming [4], and social media [2], influencing consumer behavior, media consumption, and social interaction. It needs to quickly identify a few relevant items from millions or billions of options, and personalize to the dynamic needs of large numbers of users with low response latency. A widely adopted solution to this problem is the two-stage recommender system. In the first stage, a computationally efficient retriever preselects a small number of candidates from a large pool. In the second stage, a slower but more accurate ranker narrows down and reorders these candidates before presenting them to the user. In this way, a well-balanced trade-off between efficiency and accuracy is achieved that meets the demands of real-world scenarios. ", "page_idx": 0}, {"type": "text", "text": "The retrievers are often heterogeneous, popular choices like matrix factorization [15, 19], two-tower [23], recurrent neural networks [3], and so on. In recent years, the tree-structured retriever model [8, 29, 28, 7], which takes advantage of the tree structure, usually combined with a greedy algorithm to identify relevant items quickly, has demonstrated commendable performance and efficiency. The ranker typically uses enriched features as input, combined with a complex model, to enhance prediction accuracy. The computational costs are generally linear relative to the number of items at deployment [4, 18]. ", "page_idx": 0}, {"type": "text", "text": "Despite the practical success of two-stage models, particularly those based on tree structures, theoretical research in this area remains limited. To fill in the gap, we start from the perspective of generalization error to investigate the upper bounds of generalization error for these models to promote understanding of their generalization capabilities. ", "page_idx": 0}, {"type": "text", "text": "In this paper, we decompose the generalization error of two-stage models across each stage. Using Rademacher complexity as a methodological tool, our analysis encompasses a range of models prevalent in two-stage methods. This includes tree-structured retriever models employing beam search, such as linear model, multilayer perceptron, and target attention model. Besides, we give generalization error bound for ranker models under shifted training distributions. The theoretical results show that tree models with increased branches and rankers trained on harmonized distributions can improve generalization performance, and we validate these findings on real-world datasets. ", "page_idx": 1}, {"type": "text", "text": "To summarize, the main contributions of this work are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We are the first to analyze the learnability of tree-based retriever models in recommender systems and prove the generalization upper bound for various tree-based retrievers using beam search. Both theoretical insights and practical experiments confirm that expanding the number of branches in tree-based retrievers enhances their ability to generalize.   \n\u2022 We establish an error decomposition framework for analyzing generalization errors of twostage recommender systems and theoretically derive the optimized training objectives for the ranker models within this framework.   \n\u2022 We prove the generalization upper bounds for different ranker models under shifted training distribution, highlighting the significant impact of disparities between training and inference data distributions on generalization errors. Theoretical and empirical findings indicate that harmonizing distributions across stages enhances the overall generalization performance of two-stage recommender systems. ", "page_idx": 1}, {"type": "text", "text": "The remainder of this paper is organized as follows: Section 2 provides an overview of related work, Section 3 presents the notation and background, Section 4 presents the main results and analytical techniques, and Section 5 provides the conclusion of the paper. Finally, the missing proofs and experimental settings are provided in the appendix. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Two-stage Recommender Systems ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Two-stage recommender systems with candidate generation followed by ranking have been widely adopted in the industry, including YouTube [4, 23, 26], Linkedin [2], Pinterest [6]. Some works focus more on improving candidate generation, particularly under tree structures. TDM [29] efficiently manages candidate retrieval in large-scale systems using a hierarchical tree-index strategy. JTM [28] improves on TDM by jointly optimizing the tree index structure and the user node preference prediction model. DeFoRec [8] extends the loss function used in TDM from a binary probability to a multi-class softmax loss. Other works, like [18], study off-policy learning for two-stage recommender systems, where the goal is to learn a good recommendation policy from the typically abundant logged data. This approach is possibly most related to our work in the ranker component. The main proposal of [18] is to modify the training objective by adding importance weights based on the ranker\u2019s probability of recommending each item. With adjustments facilitating gradient descent optimization, the authors show empirical improvements compared to a system trained without importance weighting. [13] propose a modification of naive bandit method deployment in two-stage recommenders that improves results by sharing inferred statistics between ranker and nominators with minimal computational overhead. [25] aims to provide an LLM-based two-stage recommender that uses a large language model as a ranker to improve performance. ", "page_idx": 1}, {"type": "text", "text": "2.2 Theoretical Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this subsection, we discuss the theoretical work related to our study, as well as other theories related to two-stage models. [1] propose a multi-class, hierarchical data-dependent bound on the generalization error of classifiers deployed in large-scale taxonomies to explain several empirical results related to the performance of hierarchical classifiers. Our analysis of tree structure models is inspired by this work. We extend it to the search method of beam search and provide a more refined estimate for the tree model. [17] investigates generalization error bounds for extreme multi-class classification with minimal dependence on the class set by using multi-class Gaussian complexity to construct bounds for multi-class problem. [12] theoretically demonstrated that nominator count and training objectives significantly impact two-stage recommender performance and linked twostage recommenders to Mixture-of-Experts models to show performance improvements by allowing nominators to specialize. [14] quantitatively assesses the asymptotic convergence properties of the two-tower model applied in two-stage recommenders toward an optimal recommender system. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Notation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We use the following notational conventions: bold lowercase and uppercase letters for vectors and matrices respectively, such as $\\textbf{\\em a}$ and $\\pmb{A}$ , and non-bold letters for scalars or constants, such as $c$ and $B$ . For vectors, ${\\|\\boldsymbol{a}\\|_{p}}$ denotes the $\\ell_{p}$ norm; we drop the subscript for the $\\ell_{2}$ norm. For matrix, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\|A\\|_{p}=\\operatorname*{sup}_{\\|\\pmb{x}\\|_{p}=1}\\|A\\pmb{x}\\|_{p}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "denotes matrix norms induced by vector $p$ -norms. We denote the set $\\{1,2,\\ldots,\\mathrm{m}\\}$ by $[\\mathbf{m}]$ . In the defined notation system, $m$ represents the number of users or queries, $N$ denotes the total number of items, and $K$ is the number of items retrieved in the first stage. $B$ indicates the number of children nodes per non-leaf node, while $L$ specifies the number of layers in a neural network. The complete set of items is symbolized by $\\boldsymbol{\\wp}$ . For $i\\in[m]$ , $\\mathbf{\\boldsymbol{x}}_{i}\\in\\mathbb{R}^{d}$ is the embedding vector to represent user, if we use sequence embeddings to represent the user, we denote this with a matrix $\\boldsymbol{A}^{(i)}$ , and $y_{i}\\in\\mathcal{V}$ is the corresponding target item. The function $h(\\pmb{x})\\in\\mathcal{Y}$ represents the prediction result for $\\textbf{\\em x}$ . ", "page_idx": 2}, {"type": "text", "text": "Following previous traditional notations, in the case of hierarchical classification, the hierarchy of classes $(V,E)$ is defined in the form of a rooted tree, with a root $\\bot$ and a parent relationship $\\pi:V\\backslash\\{\\bot\\}\\rightarrow V$ where $\\pi(v)$ is the parent of node $\\pmb{v}\\in V\\backslash\\{\\bot\\}$ , and $E$ denotes the set of edges with parent to child orientation. $\\scriptstyle B_{k}(x)$ identifies nodes selected at depth $k$ during a beam search for input $\\textbf{\\em x}$ . For each node $\\pmb{v}\\in V\\backslash\\{\\bot\\}$ , we further define the set of its children $\\begin{array}{r}{\\boldsymbol{\\mathcal{D}}(\\boldsymbol{v})\\stackrel{*}{=}\\{\\boldsymbol{v}^{\\prime}\\in V\\backslash\\{\\bot\\};\\pi\\,(\\dot{\\boldsymbol{v}^{\\prime}})=}\\end{array}$ $v\\}$ . The specialized nodes at the leaf level constitute the set of target items. Finally, for each item $y$ in $\\boldsymbol{\\wp}$ we define the set of its ancestors ${\\mathfrak{P}}(y)$ defined as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathfrak{P}(y)=\\{v_{1},\\ldots,v_{k_{y}}:v_{1}=\\pi(y),\\pi\\left(v_{k_{y}}\\right)=\\bot,v_{l+1}=\\pi\\left(v_{l}\\right),\\forall\\,l\\in\\{1,\\ldots,k_{y}-1\\}\\}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "3.2 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.2.1 Tree Structure Retriever Model ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "During the retrieval stage, a tree is utilized where each leaf node represents an item. Additionally, each node in the tree has a learnable parameter vector that has the same dimensionality as the user vector. The architecture of the tree is generally determined by hierarchical clustering techniques, as shown in the studies by [29, 24]. ", "page_idx": 2}, {"type": "text", "text": "When constructing the tree, we first need to obtain the initial item representations, which can be accomplished through various methods. The item representations can be represented by the instanceitem matrix $\\pmb{Y}\\in\\{0,1\\}^{m\\times N}$ . A strategy to construct item representations is by leveraging indices of positive instances. For any given item $i$ within the item set $\\boldsymbol{\\wp}$ , its corresponding representation vector $\\boldsymbol{z}_{i}$ is defined through normalization as: $z_{i}=\\overline{{\\pmb{y}}}_{i}/||\\overline{{\\pmb{y}}}_{i}||$ , where the vector $\\overline{{\\pmb{y}}}_{i}\\in\\{0,1\\}^{m}$ signifies the $i$ -th column of the instance-to-item matrix $Y$ , encapsulating the relationship between instances and the item $i$ . It is possible to refine the item representation by incorporating additional feature information, an enhanced formulation of $\\overline{{\\pmb{y}}}_{i}$ is employed, expressed as $\\begin{array}{r}{\\overline{{\\pmb{y}}}_{i}=\\sum_{j=1}^{m^{-}}\\pmb{Y}_{j i}\\pmb{f}_{j}}\\end{array}$ , where $\\pmb{f}_{j}$ denotes the feature vector associated with the $j.$ -th instance. Besides, some works, like [29], have employed a technique of starting with a randomly initialized tree structure aligned with item categories. The learned parameters of the leaf nodes are subsequently utilized as new initial representations for the items. ", "page_idx": 2}, {"type": "text", "text": "After acquiring item representations, we repeatedly apply clustering algorithms, such as $k$ -means, to form the complete tree structure. In the initial phase, all items are aggregated at the root. These items are then clustered into $B$ categories, creating the root\u2019s child nodes. This procedure is recursively performed in each child node until each category is reduced to a single item, establishing the leaf nodes. A balanced distribution of items among the categories can lead to a more even tree structure, a practice widely utilized in applications. ", "page_idx": 2}, {"type": "text", "text": "During inference, the user representation $\\textbf{\\em x}$ starts from the root node, and the path is continually extended until a leaf node is reached, using a beam search based on the model score between the user representation and the node parameter vector. Specifically, assuming a beam size of $K$ , we maintain at most $K$ paths during the inference process. Initially, the user representation selects the top $K$ nodes with the highest model scores from the children of the root node, denoted as $\\scriptstyle B_{1}(x)$ . If the number of available nodes for selection is less than $K$ , all available nodes are selected. $B_{i+1}(\\pmb{x})$ denotes the selected nodes at depth $(i+1)$ for an input $\\textbf{\\em x}$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{B}_{i+1}(\\pmb{x})=\\mathcal{T}_{\\pmb{c}\\in\\mathcal{D}(\\mathcal{B}_{i}(\\pmb{x}))}^{K}f(\\pmb{x},\\pmb{c}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where we denote $\\mathcal{T}^{K}$ as the Top- $\\mathbf{\\nabla}\\cdot\\mathbf{K}$ operator, which selects the top $K$ nodes from the children of nodes in $B_{i}({\\pmb x})$ , denoted as $\\mathcal{D}(\\boldsymbol{B}_{i}(\\boldsymbol{x}))$ based on the highest score $f(\\pmb{x},\\pmb{c})$ . To avoid the problem of leaf nodes lacking children during the inference process, we extend the definition of $\\mathcal{D}(v)$ , ", "page_idx": 3}, {"type": "equation", "text": "$\\mathcal{D}(v)=\\{v\\}$ ", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In this extended definition, the children of a leaf node are considered to be the leaf node itself. The set of $K$ leaf nodes ultimately selected by this process is denoted by $\\boldsymbol{B}(\\boldsymbol{x})$ . ", "page_idx": 3}, {"type": "text", "text": "3.2.2 Ranker Model ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The retriever models and the ranker models are often trained independently using logged feedback data (e.g., user clicks or dwell time) generated by previous versions of the recommender system. Compared to retriever models, ranker models may utilize more contextual information to better represent the user, leading to more accurate predictions. For simplicity, in this work, we assume that both the retriever and ranker models have the same input (e.g., user interaction history sequence). The key difference is that the retriever uses a tree-structured greedy model to retrieve a subset from a large item pool, while the ranker predicts scores for each item within this subset $\\boldsymbol{B}(\\boldsymbol{x})$ . Finally, the item with the highest score from $\\boldsymbol{B}(\\boldsymbol{x})$ , as determined by the ranker, is returned as the prediction result, which we denote as $h(x)$ . ", "page_idx": 3}, {"type": "text", "text": "3.2.3 Generalization Error ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In the training of machine learning algorithms, we are constrained to a finite dataset for learning. Nevertheless, the resulting function must generalize effectively beyond the training sample. Thus, ensuring high probability guarantees for the difference between the loss on the training sample and the loss on the test population is of paramount importance. Generalization bounds aim to constrain this loss difference. ", "page_idx": 3}, {"type": "text", "text": "Mathematically, if we have a hypothesis class $\\mathcal{H}$ , sample space $\\mathcal{X}$ , item space $\\boldsymbol{\\wp}$ , loss function $\\ell$ , and distribution over the sample and item space $\\mathcal{D}$ , then our generalization gap for a set of samples and items $S=\\{(\\mathbf{\\alpha}_{\\pmb{i}},y_{i})\\}_{i=1}^{m}\\,\\mathbf{\\dot{\\alpha}}_{}x_{i}\\in\\mathcal{X},y_{i}\\in\\mathcal{Y},$ , on the hypothesis $h\\in\\mathcal H$ is defined to be ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}[\\ell(h(\\mathbf{x}),y)]-\\frac{1}{m}\\sum_{i=1}^{m}\\ell\\left(h\\left(\\pmb{x}_{i}\\right),y_{i}\\right)\\right|.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Notice how if we can have this value go to 0 with high probability over all sets of samples and for all $h\\in\\mathcal H$ , then we can be confident that minimizing the empirical loss will not impact our generalization. ", "page_idx": 3}, {"type": "text", "text": "One tool that can be used to upper bound the generalization gap is the Rademacher complexity. The Rademacher complexity of a hypothesis class $\\mathcal{H}$ is defined to be ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{\\mathcal{R}}_{m}(\\mathcal{H})=\\frac{1}{m}\\mathbb{E}_{\\pmb{\\epsilon}}\\left[\\operatorname*{sup}_{h\\in\\mathcal{H}}\\sum_{i=1}^{m}\\epsilon_{i}h\\left(\\pmb{x}_{i}\\right)\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where each $\\epsilon_{i}$ are i.i.d. and take the value 1 or $-1$ each with half probability and $\\epsilon=(\\epsilon_{1},\\hdots,\\epsilon_{m})$ . It is well known that [21], if the magnitude of our loss function is bounded above by $c$ , with probability greater than $1-\\delta$ for all $h\\in\\mathcal H$ , we have ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}_{({\\pmb x},{\\pmb y})\\sim\\mathcal{D}}[\\ell(h({\\pmb x}),{\\pmb y})]-\\frac{1}{m}\\sum_{i=1}^{m}\\ell\\left(h\\left({\\pmb x}_{i}\\right),y_{i}\\right)\\right|\\leq2\\hat{\\mathcal{R}}_{m}(\\ell\\circ\\mathcal{H})+c\\sqrt{\\frac{2\\log(2/\\delta)}{m}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Therefore, if we have an upper bound on the Rademacher complexity, we can have an upper bound on the generalization gap. ", "page_idx": 3}, {"type": "text", "text": "4 Theory Results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We begin by considering the two-stage error decomposition. Let $P_{\\mathrm{two-stage}}^{\\mathrm{err}}$ denote the two-stage classification error, i.e., $P(h(\\pmb{x})\\neq y)\\mathrm{~.~}P_{\\mathrm{retrieve}}^{\\mathrm{err}}$ and $\\tilde{P}_{\\mathrm{rank}}^{\\mathrm{err}}$ represent the classification errors caused by the retriever and ranker, respectively. We have the following proposition: ", "page_idx": 4}, {"type": "text", "text": "Proposition 4.1. The probability of a classification error of the two-stage method can be decomposed as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nP_{t w o-s t a g e}^{e r r}\\,=P_{r e t r i e v e}^{e r r}\\,+\\tilde{P}_{r a n k}^{e r r}\\,(K)\\,(1-P_{r e t r i e v e}^{e r r}\\,),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In Proposition 4.1, the total generalization error of two stages is decomposed into two critical components, $P_{\\mathrm{retrieve}}^{\\mathrm{err}}$ and $\\tilde{P}_{\\mathrm{rank}}^{\\mathrm{err}}\\cdot P_{\\mathrm{retrieve}}^{\\mathrm{err}}$ captures the error when the target item isn\u2019t included in the retriever model\u2019s results, reflecting the probability that the item $y$ doesn\u2019t appear in the set . $\\tilde{P}_{\\mathrm{rank}}^{\\mathrm{err}}$ refers to the error that occurs when the target item $y$ , although present in the retriever\u2019s results $\\boldsymbol{B}(\\boldsymbol{x})$ , is not correctly ranked by the ranker model. ", "page_idx": 4}, {"type": "text", "text": "Compared to a single ranker model for classification tasks, we use $P_{\\mathrm{rank}}^{\\mathrm{err}}(N)$ to denote the classification error, where $N$ emphasizes that the ranker model is used for an $N$ -class task, we have the following corollary: ", "page_idx": 4}, {"type": "text", "text": "Corollary 4.2. The error P tewror-stage \u2264P rearnrk(N) if and only if the retrieval error P reertrrieve satisfies the following inequality: ", "page_idx": 4}, {"type": "equation", "text": "$$\nP_{r e t r i e\\nu e}^{e r r}\\,\\leq\\frac{P_{r a n k}^{e r r}(N)-\\tilde{P}_{r a n k}^{e r r}\\left(K\\right)}{1-\\tilde{P}_{r a n k}^{e r r}\\left(K\\right)}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Regarding inequality (4), we have two approaches to enhance its validity. One is to reduce $P_{\\mathrm{retrieve}}^{\\mathrm{err}}$ without increasing the number of retriever results. The other is to improve the ranker model while keeping the retriever unchanged, to reduce $\\tilde{P}_{\\mathrm{rank}}^{\\mathrm{err}}$ and thus increasing the threshold on the right side of the inequality. Furthermore, by Property 4.1, both approaches will lead to lower two-stage classification error. In the following subsections, we will analyze these two errors separately from a generalization bound perspective, revealing their relationship with the actual observed empirical errors. ", "page_idx": 4}, {"type": "text", "text": "4.1 Retriever ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "For the model described in Sec.3.2.1, we represent the user using the vector $\\mathbf{\\boldsymbol{x}}_{i}\\in\\mathbb{R}^{d}$ , which can be a vector representation of a text segment, or an embedding derived from a pre-trained model that includes relevant features (If we use sequence embeddings to represent the user, we denote this with a matrix $A^{(i)}$ ). We consider different user and target items to be independently and identically distributed, denoted as $(\\pmb{x_{1}},y_{1}),\\dotsc,(\\pmb{x_{m}},y_{m})\\sim\\mathcal{D}$ . ", "page_idx": 4}, {"type": "text", "text": "Following previous work [1], we consider the following analogous function space induced by the function space $\\mathcal{F}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{G}_{\\mathcal{F}}=\\{g_{f}:(\\pmb{x},y)\\in\\mathcal{X}\\times\\mathcal{Y}\\mapsto\\underset{\\pmb{v}\\in\\mathfrak{P}(y)}{\\operatorname*{min}}\\left(f(\\pmb{x},\\pmb{v})-\\underset{\\operatorname*{max}}{\\operatorname*{min}}\\right)}\\\\ &{\\operatorname*{max}_{\\mathrm{K}}\\{f\\left(\\pmb{x},\\pmb{v}^{\\prime}\\right)|\\pmb{v}^{\\prime}\\in\\mathcal{B}(\\pmb{v},\\pmb{x})\\}\\Big)\\mid f\\in\\mathcal{F}\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where ${\\mathfrak{P}}(y)$ is the ancestors of node $y$ , $B(v,x)$ denotes the set of candidates during beam search at the same level as node $\\pmb{v}$ , in particular, if $d(v)$ represents the depth of node $\\pmb{v}$ within the tree structure, then $\\mathcal{B}(\\pmb{v},\\pmb{x})=\\mathcal{D}(\\mathcal{B}_{d(\\pmb{v})-1}\\bar{(\\pmb{x})})$ , $\\mathrm{\\max_{K}}$ denotes the $K$ -th largest element in a set, $f(\\boldsymbol{x},\\boldsymbol{v})$ represents the score function between node $\\pmb{v}$ and input $\\textbf{\\em x}$ . The specific formulation of the score function will be discussed in Section 4.1.1. ", "page_idx": 4}, {"type": "text", "text": "Compared with previous work, we extend the function space to the top- $\\cdot\\mathbf{k}$ form, as described in equation (5), we can observe the following proposition: ", "page_idx": 4}, {"type": "text", "text": "Proposition 4.3. For any leaf node $y\\in\\mathcal{V}$ and user representation $\\textbf{\\em x}$ , we have ", "page_idx": 4}, {"type": "equation", "text": "$$\ng_{f}(\\pmb{x},y)\\geq0\\Leftrightarrow y\\in\\mathcal{B}(\\pmb{x}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This implies that the probability of classification error is equal to the probability of occurrence of the event $\\bar{g_{f}}(\\pmb{x},y)<0$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{P}(y\\notin\\mathcal{B}(\\pmb{x}))=\\mathbb{P}\\left(g_{f}(\\pmb{x},y)<0\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "For this event, we can formulate the following theorem about the Rademacher complexity of the function space $\\mathcal{G}_{\\mathcal{F}}$ : ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.4. Consider a loss function ${\\mathcal{A}}(x)$ that is monotonically decreasing, satisfies $\\mathbb{I}(x\\leq0)\\leq$ ${\\mathcal{A}}(x)$ , and is a Lipschitz function with Lipschitz constant $c_{\\mathcal{A}}$ and an upper bound $B_{A}$ . The following inequality holds with a probability of at least $1-\\delta$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\nP_{r e t r i e v e}^{e r r}\\,\\leq\\frac{1}{m}\\sum_{i=1}^{m}A\\left(g_{f}\\left(x_{i},y_{i}\\right)\\right)+4c_{A}\\hat{\\mathcal{R}}_{m}(\\mathcal{G}_{\\mathcal{F}})+B_{A}\\sqrt{\\frac{2\\log\\left(2/\\delta\\right)}{m}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Theorem 4.4 presents a general result, considering an abstracted loss function under specific conditions and the Rademacher complexity of the function space. In Section 4.1.1, we will present the upper bounds of Rademacher complexity for various specific function spaces. Regarding ${\\mathcal{A}}(g_{f})$ , it can be related to common loss functions, such as margin-based loss and cross-entropy. We will discuss the relationship between $\\boldsymbol{A}(g_{f})$ and these commonly used loss functions in the Appendix A. ", "page_idx": 5}, {"type": "text", "text": "4.1.1 Effect of Model Architectures ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this part, we discuss several common score models and provide upper bounds on their Rademacher complexity. ", "page_idx": 5}, {"type": "text", "text": "Linear Model. One such model is the linear model, which is widely used in text retrieval tasks [24, 10]. It calculates scores by taking the dot product of user vectors and node vectors, which can be expressed as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\nf_{\\mathrm{lin}}({\\boldsymbol{x}},{\\boldsymbol{v}})=\\langle{\\boldsymbol{x}},{\\boldsymbol{w}}_{v}\\rangle\\,,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\pmb{w}_{\\pmb{v}}$ is a learnable parameter for node $v$ in the tree model. The function space ${\\mathcal{F}}_{\\mathrm{lin}}$ is expressed as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{F}_{\\mathrm{lin}}=\\left\\{f:(\\boldsymbol{x},\\boldsymbol{v})\\mapsto\\langle\\boldsymbol{x},\\boldsymbol{w}_{\\boldsymbol{v}}\\rangle\\ |\\ \\|\\boldsymbol{w}_{\\boldsymbol{v}}\\|_{2}\\leq B_{0},\\forall\\,\\boldsymbol{v}\\in V\\right\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We have the following results: ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.5. Suppose $\\forall i\\in[m],\\|{\\pmb x}_{i}\\|_{2}\\leq B_{x},$ , then the Rademacher complexity of $\\mathcal{G}_{\\mathcal{F}_{l i n}}$ can be bounded by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{\\mathcal{R}}_{m}(\\mathcal{G}_{\\mathcal{F}_{l i n}})\\leq\\frac{4B_{0}B_{x}}{\\sqrt{m}}\\mathcal{T},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathcal{T}=B N/\\sqrt{B^{2}-1}$ . ", "page_idx": 5}, {"type": "text", "text": "MLP. We consider the concatenation of the user vector and the node vector as inputs to a multilayer perceptron (MLP). This architecture is widely used in the network structures of recommender systems [8], which can be expressed as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\nf_{\\mathrm{mlp}}(\\pmb{x},\\pmb{v})=W_{L}\\cdot\\sigma_{L-1}\\circ\\sigma_{L-2}\\circ...\\circ\\sigma_{1}\\left(\\pmb{x};\\pmb{w}_{v}\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $W_{L}\\in\\mathbb{R}^{1\\times d_{L-1}}$ , $(\\pmb{x};\\pmb{w}_{v})\\in\\mathbb{R}^{2d}$ represents the concatenation of the column vectors $\\textbf{\\em x}$ and $\\pmb{w}_{\\pmb{v}}$ , the function $\\sigma_{k}({\\pmb x})$ is defined: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\sigma_{k}(\\pmb{x})=\\sigma\\,(\\pmb{W_{k}}\\pmb{x})\\in\\mathbb{R}^{d_{k}\\times1},\\;\\forall k\\in[L-1].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The function $\\sigma$ is a Lipschitz continuous activation function with a Lipschitz constant $c_{\\sigma}$ , has the property $\\sigma(0)=0$ and $W_{k}\\in\\mathbb{R}^{d_{k}\\times d_{k-1}}$ represents the weight matrix. For the function space: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{F}_{\\mathrm{mlp}}=\\Big\\{f:(\\pmb{x},\\pmb{v})\\mapsto f_{\\mathrm{mlp}}(\\pmb{x},\\pmb{v})\\ |\\ \\|\\pmb{w}_{\\pmb{v}}\\|_{2}\\le B_{0},\\forall\\,\\pmb{v}\\in V;\\|\\pmb{W}_{k}\\|_{1}\\le B_{1},\\forall k\\in[L]\\Big\\},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "we have the following results: ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.6. Suppose $\\forall i\\in[m],\\|\\pmb{x}_{i}\\|_{2}\\leq B_{x}$ , then the Rademacher complexity of $\\mathcal{G}_{\\mathcal{F}_{m l p}}$ can be bounded by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{\\mathcal{R}}_{m}(\\mathcal{G}_{\\mathcal{F}_{m l p}})\\leq\\frac{8c_{\\sigma}^{L-1}B_{1}^{L}\\cdot\\left(B_{0}+B_{x}\\right)}{\\sqrt{m}}\\mathcal{T}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Target Attention. As a deep neural network architecture, target attention has achieved competitive performance in recommender systems and is widely used as a score function in tree-structured recommendations[8, 29]. In contrast to the previous two models, which represent a user as a single embedding vector, the model characterizes the user representation by a history sequence of items they have interacted with. In this context, we denote the matrix of item embedding vectors that the $i$ -th user has interacted with as $\\boldsymbol{A}^{(i)}$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{A}^{(i)}=[\\pmb{a}_{1}^{(i)},\\pmb{a}_{2}^{(i)},...,\\pmb{a}_{T}^{(i)}]\\in\\mathbb{R}^{d\\times T},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where we consider the last $T$ recorded item interaction histories. The model uses a two-layer fully connected network to compute weights for node vectors and user-history item vectors, which can be expressed as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\boldsymbol{w}_{k}^{(i)}=\\sigma\\left(\\boldsymbol{W}_{w}^{(2)}\\sigma\\left(\\boldsymbol{W}_{w}^{(1)}\\left[a_{k}^{(i)};a_{k}^{(i)}\\odot\\boldsymbol{w}_{v};\\boldsymbol{w}_{v}\\right]\\right)\\right)\\in\\mathbb{R},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\pmb{W}_{w}^{(1)}\\,\\in\\,\\mathbb{R}^{h\\times3d},\\pmb{W}_{w}^{(2)}\\,\\in\\,\\mathbb{R}^{1\\times h}$ , and $\\sigma$ is activation function. The score function can be expressed as: ", "page_idx": 6}, {"type": "equation", "text": "$$\nf_{t a}(\\pmb{x}_{i},\\pmb{v})=f_{m l p}\\left(\\pmb{z}_{1}^{(i)};\\pmb{z}_{2}^{(i)};...;\\pmb{z}_{N^{\\prime}}^{(i)};\\pmb{w}_{v}\\right),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\forall j\\in[N^{\\prime}]$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\nz_{j}^{(i)}=\\sum_{k\\in\\mathcal{C}_{j}}w_{k}^{(i)}\\pmb{a}_{k}^{(i)},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "$\\mathcal{C}_{j}$ , corresponding to different time windows, each $\\mathcal{C}_{j}$ being mutually exclusive, satisfies the following conditions: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\bigcup_{j=1}^{N^{\\prime}}\\mathcal{C}_{j}=\\{1,2,\\ldots,T\\}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "For the function space: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathscr F}_{\\mathrm{ta}}=\\Big\\{f:({\\pmb x},{\\pmb v})\\mapsto f_{\\mathrm{ta}}({\\pmb x},{\\pmb v})~|~\\|{\\pmb w}_{\\pmb v}\\|_{2}\\leq B_{0},\\forall~{\\pmb v}\\in V;}\\\\ &{~~~~\\|{\\pmb W}_{k}\\|_{1}\\leq B_{1},\\forall k\\in[L];~\\|{\\pmb W}_{\\pmb w}^{(j)}\\|_{1}\\leq B_{2},\\forall j\\in\\{1,2\\}\\Big\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "we have the following results: ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.7. Suppose $\\forall i\\in[m],\\forall k\\in[T],\\|a_{k}^{(i)}\\|_{2}\\leq B_{a},$ , then the Rademacher complexity of $\\mathcal{G}_{\\mathcal{F}_{t a}}$ can be bounded by ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\hat{\\mathcal{R}}_{m}(\\mathcal{G}_{\\mathcal{F}_{t a}})\\leq\\frac{4c_{\\sigma}^{L-1}B_{1}^{L}\\left(B_{w}T+B_{0}\\right)}{\\sqrt{m}}T,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $B_{w}=c_{\\sigma}^{2}B_{2}^{2}\\left(B_{a}^{2}+B_{a}^{2}B_{0}+B_{0}B_{a}\\right)$ . ", "page_idx": 6}, {"type": "text", "text": "4.1.2 Insights from Generalization Bound ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The theorems 4.5, 4.6, and 4.7, show the effect of three different score models on their generalization. More complex models tend to have higher function space complexity. From a generalization error perspective, this represents a tradeoff between function space complexity and empirical error, as they often result in lower empirical errors. We can see that, similar to most generalization conclusions derived from Rademacher complexity, the order of the number of sample points $m$ is $\\mathcal{O}(m^{-1/2})$ . This implies that as the number of samples increases, the error rate can be effectively controlled by the empirical error, resulting in a performance on the test set that is as satisfactory as on the training set. ", "page_idx": 6}, {"type": "text", "text": "Besides, the theoretical results reveal a relationship between model generalization capabi\u221alities with tree structure retrievers. Specifically, the generalization bound includes a term $\\mathcal{O}(B/\\sqrt{B^{2}-1})$ , where $B$ represents the number of branches, suggesting that a tree with a larger number of child nodes (branches) tends to exhibit enhanced generalization performance. Intuitively, a tree with more branches will have a flatter structure. In an extreme case, when the number of branches equals the number of items, the tree structure becomes ineffective because it requires traversing all items during inference. This leads to the highest computational complexity, as the retriever model degenerates into a ranker model that processes the entire item pool. Thus, the number of branches of a tree structure represents, to some extent, a tradeoff between efficiency and performance. ", "page_idx": 6}, {"type": "image", "img_path": "m1a4CrRJR7/tmp/1ccedfa8a988f8417e03bf7556eee2e153441c7edc2cf4e5d2faec2dd12c67f5.jpg", "img_caption": ["Figure 1: Effect of branch number on Recall $@20$ for Mind (left) and Movie (right) "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "We conduct experiments on real-world datasets to study the effects of increasing the number of tree branches. In our experiments, we use the same datasets as work [8], specifically Mind and Movie. We adopt an improved TDM model [8] as the retriever model architecture and use recall as the evaluation metric, since in the retrieval stage, the focus is on whether the target item is successfully retrieved. A more detailed description of the experimental setup can be found in Appendix D. The results are presented in Figure 1. As we can see, the recall rate increases with the number of branches. Similar phenomena can be observed in other studies related to tree models [7]. ", "page_idx": 7}, {"type": "text", "text": "4.2 Ranker ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In the context of training data sets $S\\,=\\,\\{({\\pmb x_{1}},y_{1}),\\dots,({\\pmb x_{m}},y_{m})\\}$ independently and identically distributed according to distribution $\\mathcal{D}$ , where each $\\mathbf{\\boldsymbol{x}}_{i}\\in\\mathbb{R}^{d}$ and $y_{i}\\in\\{1,\\ldots,N\\}$ , we examine a subset of this data, referred to as flitered training data, given by $S^{\\prime}=\\{(\\pmb{x}_{1}^{\\prime},\\bar{y}_{1}^{\\prime}),\\dots,(\\pmb{x}_{m^{\\prime}}^{\\prime},y_{m^{\\prime}}^{\\prime})\\}\\subset S$ We suppose this subset is independently and identically distributed, following the distribution $\\mathcal{D}^{\\prime}$ , where each $y_{i}^{\\prime}\\in B(x_{i}^{\\prime})$ . The generalization error of ranker $\\tilde{P}_{\\mathrm{rank}}^{\\mathrm{err}}$ , is defined as the expected probability of the ranking error under the distribution $\\mathcal{D}^{\\prime}$ . Specifically, we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\tilde{P}_{\\mathrm{rank}}^{\\mathrm{err}}=\\mathbb{E}_{(\\boldsymbol{x},\\boldsymbol{y})\\sim\\mathcal{D^{\\prime}}}\\left[\\mathbb{I}\\left(f(\\boldsymbol{x},\\boldsymbol{y})-\\operatorname*{max}_{\\boldsymbol{j}\\in\\mathcal{B}(\\boldsymbol{x})}f(\\boldsymbol{x},\\boldsymbol{j})<0\\right)\\right],\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where we use $f(\\pmb{x},j)$ to denote the model score of user $\\textbf{\\em x}$ with respect to item $j$ in this subsection. ", "page_idx": 7}, {"type": "text", "text": "To establish a relationship between the expected generalization error on distribution $\\mathcal{D}^{\\prime}$ and the empirical error measured on training data distribution $\\mathcal{D}$ , we have the following theorem: ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.8. Consider a loss function $\\Phi(x)$ that is monotonically decreasing, satisfies $\\mathbb{I}(x\\leq0)\\leq$ $\\Phi(x)$ , and is a Lipschitz function with Lipschitz constant $c_{\\Phi}$ and an upper bound $B_{\\Phi}$ . The following inequality holds with a probability of at least $1-\\delta$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\tilde{P}_{r a n k}^{e r r}\\leq\\mathbb{E}_{(x,y)\\sim\\mathcal{D}}\\left|1-\\frac{P^{\\prime}(x,y)}{P(x,y)}\\right|+\\tilde{l}_{r a n k}\\,+4c_{\\Phi}N\\left(K+1\\right)\\hat{\\mathcal{R}}_{m}(\\Pi_{1}(\\mathcal{F}))+B_{\\Phi}\\sqrt{\\frac{2\\log{(2/\\delta)}}{m}},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\Pi_{1}({\\mathcal{F}})=\\{x\\mapsto f({\\pmb x},y):y\\in\\mathcal{y},f\\in\\mathcal{F}\\}\\;P$ and $P^{\\prime}$ denote the probability density functions of $\\mathcal{D}$ and $\\mathcal{D}^{\\prime}$ , respectively, and $\\begin{array}{r}{\\tilde{l}_{r a n k}\\,=\\frac{1}{m}\\sum_{i=1}^{m}\\Phi\\bigl(f(\\mathbf{x}_{i},y_{i})-\\operatorname*{max}_{j\\in\\mathcal{B}(x)}f(\\mathbf{x}_{i},j)\\bigr).}\\end{array}$ . ", "page_idx": 7}, {"type": "text", "text": "Similar to Theorem 4.4, Theorem 4.8 presents a general result. As for the abstracted loss function $\\tilde{l}_{\\mathrm{rank}}$ , we can see that $\\begin{array}{r}{\\tilde{\\cal l}_{\\mathrm{rank}}\\,\\le\\,\\hat{\\cal l}_{\\mathrm{rank}}\\,=\\,\\frac{1}{m}\\sum_{i=1}^{m}\\Phi\\bigl(f(x_{i},y_{i})-\\operatorname*{max}_{j\\in\\mathcal{Y}}f(x_{i},j)\\bigr)}\\end{array}$ , where the latter, margin-based loss, is commonly used in training. This can also be extended to other common loss functions, as discussed similarly in the Appendix A. As for the Rademacher complexity of the function space, to maintain consistency with the previous subsection, we introduce the notation ${\\mathcal{F}}^{\\mathcal{N}}$ to denote the restriction of the function space $\\mathcal{F}$ w.r.t. $\\boldsymbol{\\wp}$ , specifically: ", "page_idx": 7}, {"type": "equation", "text": "$$\n{\\mathcal{F}}^{\\mathcal{V}}=\\{f({\\boldsymbol{\\mathbf{\\mathit{x}}}},{\\boldsymbol{\\mathbf{\\mathit{v}}}})\\in{\\mathcal{F}}:v\\in{\\mathcal{V}}\\}\\subset{\\mathcal{F}}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "For the score function described in the subsection 4.1.1, we have the following theorem: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\hat{\\mathcal{R}}_{m}(\\Pi_{1}(\\mathcal{F}_{m o d e l}^{\\mathcal{Y}}))\\leq\\frac{B_{m o d e l}}{\\sqrt{m}},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $B_{l i n}=B_{0}B_{x}$ $B_{m l p}=2c_{\\sigma}^{L-1}B_{1}^{L}\\cdot(B_{0}+B_{x}),\\,B_{t a}=c_{\\sigma}^{L-1}B_{1}^{L}(B_{w}T+B_{0}).$ ", "page_idx": 8}, {"type": "text", "text": "Besides, compared to traditional generalization bounds, Theorem 4.8 includes an additional error term induced by distributional disparities. It shows that the generalization performance of the twostage ranker model degrades due to discrepancies between the inference distribution and training distributions. When the training distribution is aligned with the inference distribution, i.e., using the subset of the training data successfully retrieved by the retriever, the distributional disparities are minimized. This suggests that in practice, aligning the training distribution and inference distribution can enhance the model\u2019s performance. ", "page_idx": 8}, {"type": "text", "text": "We conduct experiments on real-world datasets to verify this. In our experiments, we use the improved TDM [8] as the retriever, and the DIN model [27], which uses the target attention structure, as the ranker. We investigate the effect of the training data distribution on the ranker performance in a fixed retriever two-stage setup. The ranker model is trained in two ways: using the original training data and using a subset of training data successfully retrieved by the retriever model. A more detailed description of the experimental setup can be found in the Appendix D. We evaluate the overall classification accuracy of the two-stage model. The results are presented in Table 1. We compared the top-1 classification accuracy (i.e., Precsion $@1$ ) of rankings produced by the ranker with different numbers of retrieval items for two methods. We can see that the Harmonized Two Stage Model (H-TS) improves performance over the original Two Stage Model (TS) on these datasets. ", "page_idx": 8}, {"type": "table", "img_path": "m1a4CrRJR7/tmp/89803a335e18ea53617fae4604655ad2e2d7979fbb190ca05531227b08aae1c4.jpg", "table_caption": ["Table 1: Comparison of classification accuracy of the two-stage model. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "It is worth noting that while aligning the training distribution to the inference distribution eliminates the bias introduced by the distribution differences, it reduces the number of training samples available $m^{\\prime}$ relative to the original number of samples $m$ . This reduction means that the upper bound of the generalization guarantee is also somewhat weakened. Consequently, the effectiveness of this adjustment method depends on the presence of a high recall retriever model. In our experiments, we found that a recall rate of more than $10\\%$ is typically required to see an improvement effect. It must ensure that there are enough training samples to maintain the generalization performance of the model. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In summary, our study provides a theoretical and empirical investigation into the generalization error bounds of two-stage recommender systems, particularly emphasizing tree-based retriever models. Our study uses Rademacher complexity to analyze the generalization capabilities of several commonly used models in two-stage recommender systems, highlighting how tree models with increased branches and ranker models trained on shifted distributions can affect generalization performance. The theoretical results show that as the number of branches in the tree increases, the model tends to exhibit improved generalization capabilities, effectively balancing efficiency and accuracy. In the presence of a high recall retriever model, using a harmonized distributions to train the ranker will improve performance. Furthermore, our experimental validation on real-world datasets with advanced models for both retriever and ranker stages corroborates the theoretical insights. This study deepens the understanding of generalization in tree-based two-stage models and provides a theoretical foundation for designing more effective models in two-stage recommender systems. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The work was supported by grants from the National Key R&D Program of China (No.   \n2021ZD0111801) and the National Natural Science Foundation of China (No. 62022077). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Rohit Babbar, Ioannis Partalas, Eric Gaussier, Massih-Reza Amini, and C\u00e9cile Amblard. Learning taxonomy adaptation in large-scale classification. The Journal of Machine Learning Research, 17(1):3350\u20133386, 2016. [2] Fedor Borisyuk, Krishnaram Kenthapadi, David Stein, and Bo Zhao. Casmos: A framework for learning candidate selection models over structured queries and documents. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 441\u2013450, 2016. [3] Minmin Chen, Alex Beutel, Paul Covington, Sagar Jain, Francois Belletti, and Ed H Chi. Top-k off-policy correction for a reinforce recommender system. In Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining, pages 456\u2013464, 2019. [4] Paul Covington, Jay Adams, and Emre Sargin. Deep neural networks for youtube recommendations. In Proceedings of the 10th ACM conference on recommender systems, pages 191\u2013198, 2016. [5] Koby Crammer and Yoram Singer. On the algorithmic implementation of multiclass kernelbased vector machines. Journal of machine learning research, 2(Dec):265\u2013292, 2001. [6] Chantat Eksombatchai, Pranav Jindal, Jerry Zitao Liu, Yuchen Liu, Rahul Sharma, Charles Sugnet, Mark Ulrich, and Jure Leskovec. Pixie: A system for recommending $3+$ billion items to ${200+}$ million users in real-time. In Proceedings of the 2018 world wide web conference, pages 1775\u20131784, 2018. [7] Chao Feng, Wuchao Li, Defu Lian, Zheng Liu, and Enhong Chen. Recommender forest for efficient retrieval. Advances in Neural Information Processing Systems, 35:38912\u201338924, 2022. [8] Chao Feng, Defu Lian, Zheng Liu, Xing Xie, Le Wu, and Enhong Chen. Forest-based deep recommender. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 523\u2013532, 2022. [9] Yann Guermeur. Sample complexity of classifiers taking values in r q, application to multi-class svms. Communications in Statistics\u2014Theory and Methods, 39(3):543\u2013557, 2010.   \n[10] Nilesh Gupta, Patrick Chen, Hsiang-Fu Yu, Cho-Jui Hsieh, and Inderjit Dhillon. Elias: Endto-end learning to index and search in large output spaces. Advances in Neural Information Processing Systems, 35:19798\u201319809, 2022.   \n[11] Marjorie G Hahn. Probability in banach spaces: Isoperimetry and processes., 1994.   \n[12] Jiri Hron, Karl Krauth, Michael Jordan, and Niki Kilbertus. On component interactions in twostage recommender systems. Advances in neural information processing systems, 34:2744\u20132757, 2021.   \n[13] Jiri Hron, Karl Krauth, Michael I Jordan, and Niki Kilbertus. Exploration in two-stage recommender systems. arXiv preprint arXiv:2009.08956, 2020.   \n[14] Amit Kumar Jaiswal. Towards a theoretical understanding of two-stage recommender systems. arXiv preprint arXiv:2403.00802, 2024.   \n[15] Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix factorization techniques for recommender systems. Computer, 42(8):30\u201337, 2009.   \n[16] Maksim Lapin, Matthias Hein, and Bernt Schiele. Top-k multiclass svm. Advances in neural information processing systems, 28, 2015.   \n[17] Yunwen Lei, \u00dcr\u00fcn Dogan, D Zhou, and Marius Kloft. Generalization error bounds for extreme multi-class classification. CoRR, abs/1706.09814, 2017.   \n[18] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Ji Yang, Minmin Chen, Jiaxi Tang, Lichan Hong, and Ed H Chi. Off-policy learning in two-stage recommender systems. In Proceedings of The Web Conference 2020, pages 463\u2013473, 2020.   \n[19] Andriy Mnih and Russ R Salakhutdinov. Probabilistic matrix factorization. Advances in neural information processing systems, 20, 2007.   \n[20] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning. MIT press, 2018.   \n[21] Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algorithms. Cambridge university press, 2014.   \n[22] John Shawe-Taylor and Nello Cristianini. Kernel methods for pattern analysis. Cambridge university press, 2004.   \n[23] Xinyang Yi, Ji Yang, Lichan Hong, Derek Zhiyuan Cheng, Lukasz Heldt, Aditee Kumthekar, Zhe Zhao, Li Wei, and Ed Chi. Sampling-bias-corrected neural modeling for large corpus item recommendations. In Proceedings of the 13th ACM Conference on Recommender Systems, pages 269\u2013277, 2019.   \n[24] Hsiang-Fu Yu, Jiong Zhang, Wei-Cheng Chang, Jyun-Yu Jiang, Wei Li, and Cho-Jui Hsieh. Pecos: Prediction for enormous and correlated output spaces. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 4848\u20134849, 2022.   \n[25] Zhenrui Yue, Sara Rabhi, Gabriel de Souza Pereira Moreira, Dong Wang, and Even Oldridge. Llamarec: Two-stage recommendation using large language models for ranking. arXiv preprint arXiv:2311.02089, 2023.   \n[26] Zhe Zhao, Lichan Hong, Li Wei, Jilin Chen, Aniruddh Nath, Shawn Andrews, Aditee Kumthekar, Maheswaran Sathiamoorthy, Xinyang Yi, and Ed Chi. Recommending what video to watch next: a multitask ranking system. In Proceedings of the 13th ACM Conference on Recommender Systems, pages 43\u201351, 2019.   \n[27] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. Deep interest network for click-through rate prediction. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining, pages 1059\u20131068, 2018.   \n[28] Han Zhu, Daqing Chang, Ziru Xu, Pengye Zhang, Xiang Li, Jie He, Han Li, Jian Xu, and Kun Gai. Joint optimization of tree-based index and deep model for recommender systems. Advances in Neural Information Processing Systems, 32, 2019.   \n[29] Han Zhu, Xiang Li, Pengye Zhang, Guozheng Li, Jie He, Han Li, and Kun Gai. Learning tree-based deep model for recommender systems. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 1079\u20131088, 2018. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "A Effect of Loss Functions ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "In the discussion that follows, we examine in detail a variety of widely used loss functions, including margin-based, top- $\\cdot\\mathbf{k}$ , and soft-max losses. The critical focus of this discussion is to establish that these loss functions act either as upper bounds or as consistent multiples of the upper bounds of $\\boldsymbol{A}(g_{f})$ . This perspective is to confirm the easy applicability of our theoretical results to these commonly used loss functions. ", "page_idx": 11}, {"type": "text", "text": "In the Theorem 4.4, for a single data point $(\\boldsymbol{x},\\boldsymbol{y})$ , the loss function we consider is: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\mathcal{A}(g_{f})=\\mathcal{A}\\left(\\operatorname*{min}_{\\boldsymbol v\\in\\mathfrak{P}(\\boldsymbol y)}\\left(f(\\boldsymbol x,\\boldsymbol v)-\\operatorname*{max}_{\\boldsymbol v^{\\prime}\\in\\mathcal{B}(\\boldsymbol v,\\boldsymbol x)}f\\left(\\boldsymbol x,\\boldsymbol v^{\\prime}\\right)\\right)\\right).\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "We show the following functions are upper bounds for $\\boldsymbol{A}(g_{f})$ : ", "page_idx": 11}, {"type": "text", "text": "Example 1 (Classical multi-class margin-based loss [5]). The loss function is defined as ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\ell_{m a r g i n}=\\ell\\left(f(\\mathbf{\\boldsymbol{x}},\\boldsymbol{v})-\\operatorname*{max}_{\\boldsymbol{v}^{\\prime}\\neq\\boldsymbol{v}\\in\\mathcal{B}(\\boldsymbol{v},\\boldsymbol{x})}f\\left(\\boldsymbol{x},\\boldsymbol{v}^{\\prime}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "where $\\ell$ represents a function that is $c_{l}$ -Lipschitz and monotonically decreasing. By defining $\\boldsymbol{\\mathcal{A}}$ as \u2113, the margin loss $\\ell_{m a r g i n}$ acts as an upper bound for ${\\mathcal{A}}(g_{f})$ : ", "page_idx": 11}, {"type": "equation", "text": "$$\nA(g_{f})\\leq\\operatorname*{max}_{v\\in\\mathfrak{P}(y)}\\ell_{m a r g i n}.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Example 2 (Top-K loss [16]). The top- $K$ hinge loss is defined by ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\ell_{t o p k}=\\operatorname*{max}\\left\\{0,\\frac{1}{K}\\sum_{j=1}^{K}\\left(1+f_{[j]}-f_{v}\\right)\\right\\},\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "where for the sake of simplicity, we abbreviate the notation ", "page_idx": 11}, {"type": "equation", "text": "$$\nf_{[j]}:=\\operatorname*{max}_{\\mathbf{v^{\\prime}}\\in B(v,x)}f\\left(x,v^{\\prime}\\right),\\;f_{v}:=f(\\mathbf{x},v).\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "By setting ${\\mathcal A}({\\boldsymbol x})=\\operatorname*{max}(0,1-x).$ , it can be observed that ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\mathcal{A}(g_{f})=\\operatorname*{max}_{v\\in\\mathfrak{P}(y)}(\\operatorname*{max}(0,1+f_{[K]}-f_{v}))\\leq\\operatorname*{max}_{v\\in\\mathfrak{P}(y)}\\ell_{t o p k}.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Example 3 (Cross entropy [8]). This loss is employed in tree-structured recommender systems and is defined as ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\ell_{s o f t m a x}=\\sum_{\\pmb{v}\\in\\mathfrak{P}(y)}-\\log\\frac{\\exp(f(\\pmb{x},\\pmb{v}))}{\\sum_{\\pmb{v^{\\prime}}\\in\\mathcal{B}(\\pmb{v},\\pmb{x})}\\exp\\left(f(\\pmb{x},\\pmb{v^{\\prime}})\\right)}.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Adopting the notation from equation (12), and by setting $A=-\\log_{2}\\sigma(\\cdot)$ , it can be observed that ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{A}(g_{f})=\\underset{v\\in\\mathfrak{P}(y)}{\\operatorname*{max}}\\left\\lbrace-\\log_{2}\\left(\\frac{1}{1+\\exp(f_{[K]}-f_{v})}\\right)\\right\\rbrace}\\\\ &{\\qquad\\leq\\underset{v\\in\\mathfrak{P}(y)}{\\sum}\\left\\lbrace-\\log_{2}\\left(\\frac{\\exp(f_{v})}{\\exp(f_{v})+\\exp(f_{[K]})}\\right)\\right\\rbrace}\\\\ &{\\qquad\\leq\\log_{2}(e)\\cdot\\ell_{s o f t m a x}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "B Proof of Results ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "B.1 Proof of Proposition 4.1 ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Proof of Proposition 4.1. Using the law of total probability, we have ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P(h(x)\\neq y)=P(h(x)\\neq y\\mid y\\notin B(x))P(y\\notin B(x))}\\\\ &{\\qquad\\qquad\\qquad\\quad+\\;P(h(x)\\neq y\\mid y\\in B(x))P(y\\in B(x))}\\\\ &{\\qquad\\qquad\\qquad\\quad={P}(y\\notin B(x))+{P}(h(x)\\neq y\\mid y\\in B(x))\\left(1-P(y\\notin B(x))\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "where $P(h(\\pmb{x})\\neq y\\mid y\\notin B(\\pmb{x}))$ is always 1. ", "page_idx": 11}, {"type": "text", "text": "B.2 Proof of Theorem 4.4 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Proof of Theorem 4.4. Exploiting the fact that $\\boldsymbol{\\mathcal{A}}$ dominates the $0/1$ loss and using the Rademacher data-dependent generalization bound presented in lemma C.3, we have with at least $1-\\delta$ : ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{\\xi}_{(\\boldsymbol{x},\\boldsymbol{y})\\sim\\mathcal{D}}\\left[\\mathbb{I}_{\\boldsymbol{g}_{f}(\\boldsymbol{x},\\boldsymbol{y})\\le0}-1\\right]\\le\\mathbb{E}_{(\\boldsymbol{x},\\boldsymbol{y})\\sim\\mathcal{D}}\\left[A\\circ\\boldsymbol{g}_{f}(\\boldsymbol{x},\\boldsymbol{y})-1\\right]}\\quad}&{{}}\\\\ &{\\le\\displaystyle\\frac{1}{m}\\sum_{i=1}^{m}\\left(A\\left(\\boldsymbol{g}_{f}\\left(\\boldsymbol{x}_{i},\\boldsymbol{y}_{i}\\right)\\right)-1\\right)+2\\hat{\\mathcal{R}}_{m}\\left((A-1)\\circ\\boldsymbol{\\mathcal{G}}_{\\mathcal{F}}\\right)+B_{A}\\sqrt{\\frac{2\\log(2/\\delta)}{m}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $\\hat{\\mathcal{R}}_{m}$ denotes the empirical Rademacher complexity of $(\\mathcal{A}-1)\\circ\\mathcal{G}_{\\mathcal{F}}$ on $\\boldsymbol{S}$ . As ${\\pmb x}\\mapsto{\\pmb A}({\\pmb x})$ is a Lipschitz function with constant $c_{\\mathcal{A}}$ and $(\\mathcal{A}-1)(0)=0$ . By lemma C.2 we further have: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\mathcal{R}}_{m}\\left((A-1)\\circ\\mathcal{G}_{\\mathcal{F}}\\right)\\leq2c_{A}\\hat{\\mathcal{R}}_{m}\\left(\\mathcal{G}_{\\mathcal{F}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Plugging this bound into the first inequality yields the desired result: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(g_{f}(\\pmb{x},y)<0\\right)\\leq\\frac{1}{m}\\sum_{i=1}^{m}\\pmb{\\mathscr{A}}\\left(g_{f}\\left(\\pmb{x}_{i},y_{i}\\right)\\right)+4c_{A}\\hat{\\mathcal{R}}_{m}\\left(\\mathcal{G}_{\\mathcal{F}},S\\right)+B_{A}\\sqrt{\\frac{2\\log\\left(2/\\delta\\right)}{m}}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "B.3 Proof of Theorem 4.5 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "The proof provided below is motivated by [1], requiring the modification for the $\\mathrm{max}_{K}$ operator. In addition, we have made more detailed estimates of the results and generalize of proof technology. ", "page_idx": 12}, {"type": "text", "text": "Lemma B.1. Define the mapping $c$ from $\\mathcal{F}\\times\\mathcal{X}\\times\\mathcal{Y}$ into $V\\times V$ as: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{c(f,\\pmb{x},\\pmb{y})=(\\pmb{v},\\pmb{v}^{\\prime})\\Rightarrow\\left(f\\left(\\pmb{x},\\pmb{v}^{\\prime}\\right)=\\displaystyle\\operatorname*{max}_{\\pmb{v}^{\\prime\\prime}\\in B(\\pmb{v},\\pmb{x})}f\\left(\\pmb{x},\\pmb{v}^{\\prime\\prime}\\right)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\wedge\\left(f(\\pmb{x},\\pmb{v})-f\\left(\\pmb{x},\\pmb{v}^{\\prime}\\right)=\\displaystyle\\operatorname*{min}_{\\pmb{u}\\in\\mathfrak{P}(\\pmb{y})}\\left(f(\\pmb{x},\\pmb{u})-\\displaystyle\\operatorname*{max}_{\\pmb{u}^{\\prime}\\in B(\\pmb{u},\\pmb{x})}f(\\pmb{x},\\pmb{u}^{\\prime})\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "which is similar to the one given by [9] for flat multi-class classification. Then, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\mathcal{R}}_{m}\\left(\\mathcal{G}_{\\mathcal{F}}\\right)\\leq\\displaystyle\\frac{2}{m}\\sum_{\\left(\\pmb{v},\\pmb{v}^{\\prime}\\right)\\in V^{2}}\\left(\\mathbb{E}_{\\epsilon}\\left[\\operatorname*{sup}_{\\substack{f\\in\\mathcal{F}}_{i:c\\left(f,\\pmb{x}_{i},y_{i}\\right)=\\left(\\pmb{v},\\pmb{v}^{\\prime}\\right)}}\\epsilon_{i}\\left(f\\left(\\pmb{x}_{i},\\pmb{v}\\right)\\right)\\right]+\\right.}\\\\ {\\left.\\mathbb{E}_{\\epsilon}\\left[\\operatorname*{sup}_{\\substack{f\\in\\mathcal{F}}_{i:c\\left(f,\\pmb{x}_{i},y_{i}\\right)=\\left(\\pmb{v},\\pmb{v}^{\\prime}\\right)}}\\epsilon_{i}f\\left(\\pmb{x}_{i},\\pmb{v}^{\\prime}\\right)\\right]\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Proof of Lemma B.1. ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\mathcal{R}}_{m}\\left(\\mathcal{G}_{\\mathcal{F}}\\right)=\\mathbb{E}_{\\epsilon}\\left[\\operatorname*{sup}_{{g_{f}\\in\\mathcal{G}_{\\mathcal{F}}}}\\frac{1}{m}\\sum_{i=1}^{m}\\epsilon_{i}{g_{f}}\\left({x}_{i},{y}_{i}\\right)\\right]}\\\\ &{\\quad\\quad\\quad=\\mathbb{E}_{\\epsilon}\\left[\\operatorname*{sup}_{f\\in\\mathcal{F}}\\frac{1}{m}\\sum_{i=1}^{m}\\epsilon_{i}\\operatorname*{min}_{v\\in\\mathfrak{V}(y_{i})}\\left(f\\left({x}_{i},{v}\\right)-\\operatorname*{max}_{v^{\\prime}\\in{B}\\left(v,x\\right)}f\\left({x}_{i},{v^{\\prime}}\\right)\\right)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $\\epsilon_{i}$ s are independent uniform random variables which take value in $\\{-1,+1\\}$ and are known as Rademacher variables. By construction of $c$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\hat{R}_{m}\\left(g_{T}\\right)\\leq\\frac{1}{m}\\sum_{\\stackrel{(\\mathrm{S})}{(\\mathrm{S})}^{\\prime}\\in V^{2}}\\mathbb{E}_{\\varepsilon}\\left[\\frac{\\log}{f\\in\\mathcal{F}_{i^{\\varepsilon}(f,\\pi_{i},y)}}\\sum_{\\stackrel{(\\mathrm{S})}{(\\mathrm{S})}^{\\prime}}\\epsilon_{i}\\left(f\\left(x_{i},v\\right)-f\\left(x_{i},v^{\\prime}\\right)\\right)\\right]}}\\\\ &{\\leq\\frac{1}{m}\\sum_{\\stackrel{(\\mathrm{S})}{(\\mathrm{S})}^{\\prime}\\in V^{2}}\\mathbb{E}_{\\varepsilon}\\left[\\exp\\underset{(\\varepsilon\\in[-1,1),\\varepsilon)}{\\operatorname*{sup}}f\\underset{(\\varepsilon\\in[2,\\pi_{i},y])=(\\mathrm{S})^{\\prime}}{\\sum}\\sum_{\\stackrel{(\\mathrm{S})}{(\\mathrm{S})}^{\\prime}}\\varepsilon_{i}f\\left(x_{i},v\\right)}\\\\ &{\\quad\\quad\\quad\\quad+\\underset{s\\in[-1,1),\\varepsilon}{\\operatorname*{sup}}f\\underset{(\\varepsilon\\in[2,\\pi_{i},y])=(\\mathrm{S})^{\\prime}}{\\sum}\\sum_{\\stackrel{(\\mathrm{S})}{(\\mathrm{S})}^{\\prime}}f\\left(x_{i},v^{\\prime}\\right)\\right]}\\\\ &{=\\frac{2}{m}\\underset{(\\mathrm{v},v^{\\prime})\\in V^{2}}{\\sum}\\left(\\mathbb{E}_{\\varepsilon}\\left[\\underset{(\\varepsilon\\in[2,\\pi_{i},y])=(\\mathrm{S})^{\\prime}}{\\operatorname*{sup}}\\sum_{\\stackrel{(\\mathrm{S})}{(\\mathrm{S})}^{\\prime}}\\epsilon_{i}\\left(f\\left(x_{i},v\\right)\\right)\\right]}\\\\ &{\\quad\\quad\\quad\\quad+\\mathbb{E}_{\\varepsilon}\\left[\\underset{(\\varepsilon\\in[2,\\pi_{i},y])=(\\mathrm{S})^{\\prime}}{\\operatorname*{sup}}\\sum_{\\stackrel{(\\mathrm{S})}{(\\mathrm{S})}^{\\prime}}\\epsilon_{i}f\\left(x_{i},v^{\\prime}\\right)\\right]\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the last equation holds because the fact $\\hat{\\mathcal{R}}_{m}(\\mathcal{F}\\cup-\\mathcal{F})\\le2\\hat{\\mathcal{R}}_{m}(\\mathcal{F})$ . ", "page_idx": 13}, {"type": "text", "text": "Proof of Theorem 4.5. By definition, $f\\left(\\pmb{x}_{i},\\pmb{v}\\right)-f\\left(\\pmb{x}_{i},\\pmb{v}^{\\prime}\\right)=\\left\\langle\\pmb{w}_{\\pmb{v}}-\\pmb{w}_{\\pmb{v}^{\\prime}},\\pmb{x}_{i}\\right\\rangle$ , lemma B.1 and using Cauchy-Schwartz inequality: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\hat{\\mathcal{R}}_{m}\\left(\\boldsymbol{\\mathcal{G}}_{\\mathcal{F}}\\right)\\leq\\frac{2}{m}\\mathbb{E}_{\\epsilon}\\left[\\operatorname*{sup}_{\\left\\vert\\boldsymbol{\\|W\\|_{2}\\leq B}\\ldots\\right\\vert\\in V^{2}\\right]}\\left\\vert\\left\\langle\\boldsymbol{w}_{v}-\\boldsymbol{w}_{v^{\\prime}},\\sum_{\\substack{i:c\\left(f,x_{i},y_{i}\\right)=(\\boldsymbol{\\mathbf{v}},\\boldsymbol{v}^{\\prime})}}\\boldsymbol{\\epsilon}_{i}\\boldsymbol{x}_{i}\\right\\rangle\\right\\vert\\right]}\\\\ {\\displaystyle\\leq\\frac{2}{m}\\mathbb{E}_{\\epsilon}\\left[\\operatorname*{sup}_{\\left\\vert\\boldsymbol{\\|W\\|_{2}\\leq B}\\ldots\\right\\vert\\in V^{2}\\right]}\\left\\|\\boldsymbol{w}_{v}-\\boldsymbol{w}_{v^{\\prime}}\\right\\|_{2}\\left\\|\\sum_{\\substack{i:c\\left(f,x_{i},y_{i}\\right)=(\\boldsymbol{\\mathbf{v}},\\boldsymbol{v}^{\\prime})}}\\boldsymbol{\\epsilon}_{i}\\boldsymbol{x}_{i}\\right\\|_{2}\\right]}\\\\ {\\displaystyle\\leq\\frac{4B_{0}}{m}\\sum_{\\left(\\boldsymbol{\\mathbf{v}},\\boldsymbol{v}^{\\prime}\\right)\\in V^{2}}\\mathbb{E}_{\\epsilon}\\left[\\left\\|\\sum_{\\left\\vert i:c\\left(f,x_{i},y_{i}\\right)=(\\boldsymbol{\\mathbf{v}},\\boldsymbol{v}^{\\prime})\\right\\vert\\right]_{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Using Jensen\u2019s inequality, and as, $\\forall i,j\\in\\{l\\ |\\ c\\left(f,\\mathbf{\\boldsymbol{x}}_{i},y_{i}\\right)=\\left(\\mathbf{\\boldsymbol{v}},\\mathbf{\\boldsymbol{v}}^{\\prime}\\right)\\}^{2}\\,,i\\neq j,\\mathbb{E}_{\\epsilon}\\left[\\epsilon_{i}\\epsilon_{j}\\right]=0$ , we get: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\hat{\\mathcal{R}}_{m}\\,(\\mathcal{G}_{\\mathcal{F}_{\\mathrm{li}}})\\le\\frac{4B_{0}}{m}\\sum_{\\{\\boldsymbol{v},\\boldsymbol{v}^{\\prime}\\}\\in V^{2}}\\left(\\mathbb{E}_{\\epsilon}\\left[\\left\\|\\sum_{i:c(f,\\boldsymbol{x}_{i},y_{i})=(\\boldsymbol{v},\\boldsymbol{v}^{\\prime})}\\epsilon_{i}x_{i}\\right\\|_{2}^{2}\\right]\\right)^{1/2}}}\\\\ &{=\\frac{4B_{0}}{m}\\sum_{\\{\\boldsymbol{v},\\boldsymbol{v}^{\\prime}\\}\\in V^{2}}\\left(\\sum_{i:c(f,\\boldsymbol{x}_{i},y_{i})=(\\boldsymbol{v},\\boldsymbol{v}^{\\prime})}\\|\\boldsymbol{x}_{i}\\|_{2}^{2}\\right)^{1/2}}\\\\ &{\\le\\frac{4B_{0}}{m}\\sum_{\\{\\boldsymbol{v},\\boldsymbol{v}^{\\prime}\\}\\in V^{2}}\\left(n_{(\\boldsymbol{v},\\boldsymbol{v}^{\\prime})}B_{\\boldsymbol{x}}^{2}\\right)^{1/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\sum_{({\\boldsymbol{v}},{\\boldsymbol{v}}^{\\prime})\\in V^{2}}\\left(n_{({\\boldsymbol{v}},{\\boldsymbol{v}}^{\\prime})}\\right)^{1/2}\\leq\\sqrt{\\left(\\displaystyle\\sum_{({\\boldsymbol{v}},{\\boldsymbol{v}}^{\\prime})\\in V^{2}}n_{({\\boldsymbol{v}},{\\boldsymbol{v}}^{\\prime})}\\right)\\cdot\\mid\\{({\\boldsymbol{v}},{\\boldsymbol{v}}^{\\prime})\\in V^{2}:\\exists i,s.t.\\ {\\boldsymbol{v}}^{\\prime}\\in B({\\boldsymbol{v}},{\\boldsymbol{x}}_{i})\\}\\mid}}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad\\leq\\sqrt{m}\\sqrt{\\displaystyle\\sum_{v\\in V\\setminus\\bot}\\left(\\operatorname*{min}\\left(B^{d({\\boldsymbol{v}})-1},N\\right)-1\\right)}}}\\\\ {{\\displaystyle\\qquad\\qquad\\leq\\frac{\\sqrt{m}B N}{\\sqrt{B^{2}-1}},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $n_{(\\pmb{v},\\pmb{v}^{\\prime})}=|\\{i:c\\,(\\pmb{f},\\pmb{x}_{i},y_{i})=(\\pmb{v},\\pmb{v}^{\\prime})\\}|$ is the number of set $\\{i:c\\left(f,\\pmb{x}_{i},y_{i}\\right)=\\left(\\pmb{v},\\pmb{v}^{\\prime}\\right)\\}$ , which satisfies $\\begin{array}{r}{\\sum_{(\\pmb{v},v^{\\prime})}n_{(\\pmb{v},v^{\\prime})}=m}\\end{array}$ , (a) use the Cauchy-Schwartz inequality, (b) holds because for a given node $\\pmb{v}$ , the alternative nodes $v^{\\prime}$ are in the same layer of $v$ ; note that this is based on a $\\mathbf{B}$ -ary tree structure, (c) holds because ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sum_{v\\in V\\setminus\\bot}\\left(\\operatorname*{min}\\left(B^{d(v)-1},N\\right)-1\\right)=\\sum_{d=1}^{h-2}B^{d-1}(B^{d-1}-1)+N(N-1)\\leq\\frac{N^{2}B^{2}}{B^{2}-1},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $h$ is the depth of tree, satisfy $B^{h-2}<N\\leq B^{h-1}$ . ", "page_idx": 14}, {"type": "text", "text": "Combining equations (15) and (16), we obtain the following result: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\hat{\\mathcal{R}}_{m}\\left(\\mathcal{G}_{\\mathcal{F}_{\\mathrm{lin}}}\\right)\\le\\frac{4B_{0}B_{x}}{\\sqrt{m}}\\frac{B N}{\\sqrt{B^{2}-1}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "B.4 Proof of Theorem 4.6 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Before we start the analysis, for any vectors $v,u_{i}\\in\\mathbb{R}^{d},\\|v\\|_{1}\\le B_{v}$ , notice the following inequality: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\boldsymbol{v}}\\sum_{i}\\boldsymbol{v}^{\\top}\\boldsymbol{u}_{i}\\leq B_{v}\\operatorname*{max}_{j\\in[d]}\\left\\vert\\sum_{i}e_{j}\\boldsymbol{u}_{i}\\right\\vert\\leq\\sum_{i}B_{v}\\operatorname*{max}_{\\substack{j\\in[d],\\boldsymbol{s}\\in\\{-1,1\\}}}s e_{j}\\boldsymbol{u}_{i}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof of Theorem 4.6. By lemma B.1, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\hat{\\mathcal{R}}_{m}\\left(\\mathcal{G}_{\\mathcal{F}_{\\mathrm{mp}}}\\right)\\leq\\frac{2}{m}\\sum_{\\left(\\pmb{v},\\pmb{v}^{\\prime}\\right)\\in V^{2}}\\left(\\mathbb{E}_{\\epsilon}\\left[\\operatorname*{sup}_{f\\in\\mathcal{F}_{\\mathrm{mp}}}\\sum_{\\substack{i:c\\left(f,\\pmb{x}_{i},y_{i}\\right)=\\left(\\pmb{v},\\pmb{v}^{\\prime}\\right)}}\\epsilon_{i}\\left(f\\left(\\pmb{x}_{i},\\pmb{v}\\right)\\right)\\right]\\right.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\varepsilon\\Biggl[\\Biggl|\\mathcal{R}_{p}\\Biggr]}\\\\ &{=\\mathbb{E}\\Biggl[\\Biggl|\\sum_{\\ell=0}^{\\infty}\\Biggl|\\sum_{\\ell=1/2\\atop0}^{\\infty}\\mathrm{exp}_{\\ell,j_{\\ell}}\\Biggl(f\\left(\\pi_{\\ell},\\boldsymbol{\\nu}\\right)\\Biggr)\\Biggr|}\\\\ &{\\phantom{=}\\eta_{\\ell}\\Biggl[\\Biggl|\\sum_{\\ell=1/2\\atop0}^{\\infty}\\mathrm{exp}_{\\ell,j_{\\ell}}\\Biggl|\\sum_{\\ell=1/2\\atop0}^{\\infty}\\mathrm{exp}_{\\ell,j_{\\ell}}\\left(\\eta_{\\ell}\\left(\\eta_{\\ell},\\rho_{\\ell-1}-\\pi_{\\ell}\\left(\\pi_{\\ell}\\left(\\pi_{\\ell},\\rho_{\\ell-1}-\\pi_{\\ell}\\right)\\right)\\right)\\right.\\Biggr.}\\\\ &{\\phantom{=}\\left.\\left.\\times\\left|\\mathbf{1}_{\\{\\ell=1/2\\atop0}^{\\infty}}\\right)\\Biggr|\\sum_{\\ell=1/2\\atop0}^{\\infty}\\mathrm{exp}_{\\ell,j_{\\ell}}\\Biggl|\\sum_{\\ell=1/2\\atop0}^{\\infty}\\mathrm{exp}_{\\ell,j_{\\ell}}\\Biggl|\\phi\\left(\\eta_{\\ell}\\left(\\eta_{\\ell},\\rho_{\\ell-1}-\\pi_{\\ell}\\left(\\pi_{\\ell}\\left(\\pi_{\\ell},\\rho_{\\ell-1}\\right)\\right)\\right)\\right.\\Biggr.}\\\\ &{\\phantom{=}\\left.\\left.\\times\\Biggl|\\mathbf{1}_{\\ell=1/2\\atop0}^{\\infty}\\right)\\Biggr|\\sum_{\\ell=1/2\\atop0}^{\\infty}\\mathrm{exp}_{\\ell,j_{\\ell}}\\Biggl|\\sum_{\\ell=1/2\\atop0}^{\\infty}\\mathrm{exp}_{\\ell,j_{\\ell}}\\Biggl|\\sum_{\\ell=1/2\\atop0}^{\\infty}\\mathrm{exp}_{\\ell,j_{\\ell}}\\Biggl|\\phi\\left(\\eta_{\\ell}\\left(\\eta_{\\ell},\\eta_{\\ell-1}-\\pi_{\\ell}\\left(\\pi_{\\ell}\\left(\\pi_{\\ell},\\rho_{\\ell-1}-\\pi_{\\ell}\\right)\\right)\\right)\\right.\\Biggr.}\\\\ &{\\phantom{=}\\left.\\left.\\times\\Biggl|\\mathbf{1}_{\\ell=1/2\\atop0}^{\\infty}\\right)\\Biggr|\\sum_{\\ell=1/2\\atop0}^{\\infty}\\mathrm{exp}_{\\ell,j_{\\ell}}\\Biggl|\\phi\\left(\\eta_{\\ell}\\left(\\eta_{\\\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "(19) where (a) holds since $\\sigma$ is applied element wise, we can bring $e_{j}^{\\top}$ inside the function and the use of contraction inequality [11], (b) use equation (17) again as $e_{j}^{\\bar{\\top}}W_{L-1}$ is a vector, (c) holds as $\\hat{\\mathcal{R}}_{m}(\\mathcal{F}\\cup-\\mathcal{F})\\le2\\hat{\\mathcal{R}}_{m}(\\mathcal{F})$ . ", "page_idx": 15}, {"type": "text", "text": "As term of $I_{1}$ , using Cauchy-Schwartz inequality and Jensen inequality: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I_{1}\\leq\\left(\\mathbb{E}_{\\epsilon}\\left[\\left\\|\\sum_{i:c(f,x_{i},y_{i})=(v,v^{\\prime})}\\epsilon_{i}x_{i}\\right\\|_{2}\\right]\\right)\\leq\\left(\\mathbb{E}_{\\epsilon}\\left[\\left\\|\\sum_{i:c(f,x_{i},y_{i})=(v,v^{\\prime})}\\epsilon_{i}x_{i}\\right\\|_{2}^{2}\\right]\\right)^{1/2}}\\\\ &{\\quad=\\left(\\underset{i:c(f,x_{i},y_{i})=(v,v^{\\prime})}{\\sum}\\|x_{i}\\|_{2}^{2}\\right)^{1/2}\\leq\\left(n_{(v,v^{\\prime})}B_{x}^{2}\\right)^{1/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "As term of $I_{2}$ , we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I_{2}=\\mathbb{E}_{\\epsilon}\\left[\\underset{w_{\\infty},j\\in[2d]}{\\operatorname*{sup}}\\langle e_{j},(0;w_{\\infty})\\rangle\\cdot\\cdot\\left|\\underset{i:c(f,x_{i},y_{i})=(v,v^{\\prime})}{\\sum}\\epsilon_{i}\\right|\\right]}\\\\ &{\\ \\ \\leq\\mathbb{E}_{\\epsilon}\\left[B_{0}\\cdot\\left|\\underset{i:c(f,x_{i},y_{i})=(v,v^{\\prime})}{\\sum}\\epsilon_{i}\\right|\\right]}\\\\ &{\\ \\ \\ \\leq B_{0}\\left(\\mathbb{E}_{\\epsilon}\\left[\\underset{i:c(f,x_{i},y_{i})=(v,v^{\\prime})}{\\sum}\\epsilon_{i}\\frac{1}{|\\epsilon|}\\right]\\right)^{1/2}}\\\\ &{\\ \\ \\ =B_{0}\\sqrt{n}(v,v^{\\prime}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where (a) holds since for any given $(\\epsilon_{1},\\epsilon_{2},...,\\epsilon_{m})$ , the value of $\\langle\\boldsymbol{e}_{j},(0;\\boldsymbol{w}_{v})\\rangle$ is fixed independent of $i$ , (b) use Jensen inequality. ", "page_idx": 16}, {"type": "text", "text": "Note that $\\begin{array}{r}{\\mathbb{E}_{\\epsilon}[\\operatorname*{sup}_{f\\in\\mathcal{F}_{\\operatorname*{mlp}}}\\sum_{i:c(f,\\pmb{x}_{i},y_{i})=(\\pmb{v},\\pmb{v}^{\\prime})}\\epsilon_{i}f\\left(\\pmb{x}_{i},\\pmb{v}^{\\prime}\\right)]}\\end{array}$ and $\\begin{array}{r}{\\mathbb{E}_{\\epsilon}[\\operatorname*{sup}_{f\\in\\mathcal{F}_{\\operatorname*{mlp}}}\\sum_{i:c(f,\\pmb{x}_{i},y_{i})=(\\pmb{v},\\pmb{v}^{\\prime})}\\epsilon_{i}f\\left(\\pmb{x}_{i},\\pmb{v}\\right)]}\\end{array}$ share the same upper bound, combined above equations (18), (19), (20), (21), and (16), we get the desired result. ", "page_idx": 16}, {"type": "text", "text": "B.5 Proof of Theorem 4.7 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof of Theorem 4.7. Using the same analytical procedure as in equation (19), we can get ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathbf{c}}\\left[\\underset{\\ell\\in\\mathbb{R}_{+}(\\ell,T_{\\ell},\\ell,s,\\bar{s})=(\\ell,\\ell)}{\\sum_{\\ell\\in\\mathbb{R}_{+}(\\ell,T_{\\ell},\\ell,s,\\bar{s})=(\\ell,\\ell)}}\\mathcal{\\epsilon}_{\\ell}\\left(f(\\mathbf{x}_{\\ell},\\cdot)\\right)\\right]}\\\\ &{\\quad=\\mathbb{E}_{\\mathbf{c}}\\left[\\underset{\\mathbf{w}_{\\ell\\in\\mathbb{R}_{+}}}{\\operatorname*{sup}}\\underset{\\ell\\in\\mathbb{R}_{+}}{\\sum}\\underset{\\ell\\in(\\ell,T_{\\ell},s,\\bar{s})=(\\ell,T_{\\ell})=(\\ell,T_{\\ell})}{\\sum_{\\ell\\in\\mathbb{R}_{+}(\\ell,T_{\\ell},s,\\bar{s})=(\\ell,T_{\\ell}-1)}}\\mathcal{\\epsilon}_{\\ell}\\left(\\mathbb{W}_{\\ell},\\mathcal{O}_{\\ell-2}\\otimes_{\\ell-2}\\otimes_{\\ell}(\\mathbf{x}_{\\ell};\\mathbf{w}_{\\ell})\\right)\\right]}\\\\ &{\\quad\\le c_{\\sigma}^{L-1}\\Pi_{k=1}^{L}\\left\\lVert\\mathbb{W}_{k}\\right\\rVert\\mathbb{E}_{\\mathbf{c}}\\left[\\underset{s\\in\\mathbb{R}_{+}^{+}}{\\left[\\infty^{\\mathcal{L}}(\\Gamma(\\mathcal{R}^{+}+1)d_{1},\\mathbf{w}_{\\ell\\in\\mathbb{R}})\\right]}\\underset{\\ell\\in(\\ell,T_{\\ell},s,\\bar{s})=(\\ell,T_{\\ell})}{\\sum_{\\ell\\in\\mathbb{R}_{+}(\\ell,T_{\\ell},s,\\bar{s})=(\\ell,T_{\\ell})}}\\mathcal{\\epsilon}_{\\ell}\\left(\\mathcal{O}_{\\ell}_{\\ell};\\ell_{1};\\ell_{2};\\dots;\\mathbb{Z}_{N};\\mathbb{w}_{\\ell}\\right)\\right]\\right]}\\\\ &{\\quad\\le c_{\\sigma}^{L-1}\\Pi_{k=1}^{L}\\left\\lVert\\mathbb{W}_{k}\\right\\rVert\\mathbb{E}_{\\mathbf{c}}\\left[\\underset{s\\in\\mathbb{R}_{+}}{\\sum_{\\ell\\in\\mathbb{R}_{+}}^{\\mathcal{L}}}\\left(\\underset{\\ell\\in\\mathbb{R}_{+}}{\\operatorname*{sup}}\\right)\\underset{\\ell\\in(\\ell,T_{\\ell},s,\\bar{p})=(\\ell,T_{\\ell\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Based on equation (21), we can obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\epsilon}\\left[\\operatorname*{sup}_{\\substack{s,j\\in[d],w_{v}}}\\quad\\sum_{\\substack{i:c(f,x_{i},y_{i})=(v,v^{\\prime})}}{s\\epsilon}_{i}\\left\\langle e_{j},w_{v}\\right\\rangle\\right]\\leq B_{0}\\sqrt{n_{(v,v^{\\prime})}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For other terms, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathbf{x}}\\left[\\begin{array}{c c c}{\\operatorname*{sup}_{\\theta\\in\\Theta}}&&\\\\ &{\\operatorname*{sup}_{i\\in\\Theta}}&\\\\ {...\\operatorname*{sup}_{i\\in\\Theta}}&{...(\\ell_{i}\\mathcal{A}_{i,k,n})\\mathrm{=}(\\nu_{i},\\Psi_{n})}\\end{array}\\right]}\\\\ &{=\\mathbb{E}_{\\mathbf{x}}\\left[\\begin{array}{c c c}{\\operatorname*{sup}_{\\theta\\in\\Theta}}&&{\\mathrm{sut}_{i}\\left(e_{i},\\sum_{k\\in\\Theta}w_{k}^{(i)}a_{k}^{(i)}\\right)}\\\\ {...\\operatorname*{sup}_{i\\in\\Theta}}&&{...\\operatorname*{sup}_{i\\in\\Theta}}\\end{array}\\right]}\\\\ &{\\leq\\sum_{k\\in\\Theta}\\mathbb{E}_{\\mathbf{x}}\\left[\\begin{array}{c c c}{\\operatorname*{sup}_{\\theta\\in\\Theta}}&&{\\mathrm{sut}_{i}\\left(e_{i},\\psi_{n}^{(i)}a_{k}^{(i)}\\right)}\\\\ &{...\\operatorname*{sup}_{i\\in\\Theta}}&\\\\ {...\\operatorname*{sup}_{i\\in\\Theta}}&&{....\\operatorname*{sup}_{i\\in\\Theta}}\\end{array}\\right]}\\\\ &{=\\sum_{k\\in\\Theta}\\mathbb{E}_{\\mathbf{x}}\\left[\\begin{array}{c c c}{\\operatorname*{sup}_{\\theta\\in\\Theta}}&&{\\mathrm{sut}_{i}\\left(\\nu_{i k}^{(2)}\\left(\\pi\\Psi_{k}^{(i)}a_{k}^{(i)}\\otimes w_{k};\\mathrm{sup}_{i}\\right)\\right)\\right)\\left(e_{i},a_{k}^{(i)}\\right.}\\\\ {...\\operatorname*{sup}_{i\\in\\Theta}}&&{...\\operatorname*{sup}_{i\\in\\Theta}}\\end{array}\\right]}\\\\ &{\\leq c_{\\phi}^{2}\\left\\|\\mathbf{W}_{\\mathbf{x}}^{(1)}\\right\\|_{1}\\left\\|\\mathbf{W}_{\\mathbf{x}}^{(2)}\\right\\|_{1}\\sum_{k\\in\\Theta}\\mathbb{E}_{\\mathbf{x}}\\left[\\begin{array}{c c c}{\\operatorname*{sup}_{\\theta\\in\\Theta}}&&{\\mathrm{sup}_{i}}\\\\ &{...\\operatorname*{sup}_{i}}&\\\\ {...\\operatorname*{sup}_{i}}&&{...\\operatorname*{sup}_{i}}\\end{array}\\right]}\\\\ &{~~ \n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Furthermore, we can get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\boldsymbol{\\epsilon}}\\left[\\underset{s\\in\\mathcal{Y}_{\\epsilon}}{\\operatorname*{sup}}\\!\\!\\!\\left[\\phi(\\boldsymbol{\\lambda},\\boldsymbol{\\eta})\\!\\!\\!\\right],w\\!\\!\\!\\!}&{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "As terms of $I_{1}$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{i<\\left(f,A_{i},y_{i}\\right)=\\left(\\mathbf{v},\\tau^{\\prime}\\right)}{\\sum}s\\epsilon_{i}\\left\\langle e_{j},a_{k}^{(i)}\\right\\rangle\\left\\langle e_{j},a_{k}^{(i)}\\right\\rangle=\\underset{i<\\left(f,A_{i},y_{i}\\right)=\\left(\\mathbf{v},\\tau^{\\prime}\\right)}{\\sum}s\\epsilon_{i}e_{j}^{\\top}P_{a}^{(i)}e_{j}}&{}\\\\ &{=\\underset{i<\\left(f,A_{i},y_{i}\\right)=\\left(\\mathbf{v},\\tau^{\\prime}\\right)}{\\sum}s\\epsilon_{i}\\,\\mathrm{Tr}(e_{j}e_{j}^{\\top},P_{a}^{(i)})}\\\\ &{=\\mathrm{Tr}\\left(e_{j}e_{j}^{\\top}\\left(\\underset{i<\\left(f,A_{i},y_{i}\\right)=\\left(\\mathbf{v},\\tau^{\\prime}\\right)}{\\sum}s\\epsilon_{i}P_{a}^{(i)}\\right)\\right)}\\\\ &{=\\left\\langle e_{j}e_{j}^{\\top},\\underset{i<\\left(f,A_{i},y_{i}\\right)=\\left(\\mathbf{v},\\tau^{\\prime}\\right)}{\\sum}s\\epsilon_{i}P_{a}^{(i)}\\right\\rangle_{E},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where P (i) $P_{a}^{(i)}=a_{k}^{(i)}a_{k}^{(i)\\top}$ (i )a(i)\u22a4. Then, we can get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I_{1}=\\mathbb{E}_{\\epsilon}\\left[\\underset{s\\neq\\ell}{\\operatorname*{sup}}_{s\\neq\\ell}|_{\\ell}\\frac{\\sum_{i}}{i}\\underset{i}{\\sum_{i<\\ell}(f_{i},a_{i},y_{i})}{=}(v_{i}\\star_{i})_{i}\\biggr.\\biggr.=\\biggr.}\\\\ &{\\quad=\\mathbb{E}_{\\epsilon}\\left[\\underset{s\\neq\\ell}{\\operatorname*{sup}}_{s\\neq\\ell}|_{\\ell}\\biggr.\\biggr.\\biggr\\langle e_{j}e_{j}^{\\top},\\underset{i<\\ell}{\\sum_{i}}\\underset{j=1}{\\sum_{i<\\ell}(f_{i},a_{i},y_{i})}{\\sum_{i<\\ell}(f_{i},a_{i},y_{i})}\\biggr.\\biggr.\\biggr]}\\\\ &{\\quad\\le\\mathbb{E}_{\\epsilon}\\left[\\left\\|{\\underset{i<\\ell}{\\sum_{i}}{\\sum_{i}}}_{i,\\gamma_{i}}\\underset{=\\ell,\\nu_{i}}{\\sum_{i}}{e_{i}}P_{\\alpha_{i}}^{(i)}\\right\\|_{F}\\right]}\\\\ &{\\quad=\\sqrt{\\underset{i<\\ell}{\\sum_{i}}{\\sum_{i<\\ell}(f_{i},a_{i},y_{i})}{\\sum_{i}}\\left\\|{P_{\\alpha_{i}}^{(i)}}\\right\\|_{F}^{2}}\\le\\sqrt{n_{\\ell}}\\mathrm{e},\\mathrm{e}^{\\gamma})B_{\\alpha}^{4}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "As terms of $I_{2}$ , use the same analysis technique as for $I_{1}$ , we can get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I_{2}=\\mathbb{E}_{\\epsilon}\\left[\\underset{s,j\\in[d],j^{\\prime}\\in[d],w}{\\operatorname*{sup}}\\;\\sum_{\\substack{i\\in(f,A_{i},y_{i})=(\\mathfrak{v},\\mathfrak{v}^{\\prime})}}s\\epsilon_{i}\\left\\langle e_{j^{\\prime}},a_{k}^{(i)}\\odot w_{\\mathfrak{v}}\\right\\rangle\\left\\langle e_{j},a_{k}^{(i)}\\right\\rangle\\right]}\\\\ &{=\\mathbb{E}_{\\epsilon}\\left[\\underset{s,j\\in[d],j^{\\prime}\\in[d],w}{\\operatorname*{sup}}\\;\\sum_{\\substack{i<(f,A_{i},y_{i})=(\\mathfrak{v},\\mathfrak{v}^{\\prime})}}s\\epsilon_{i}\\left\\langle e_{j^{\\prime}}\\odot w_{\\mathfrak{v}},a_{k}^{(i)}\\right\\rangle\\left\\langle e_{j},a_{k}^{(i)}\\right\\rangle\\right]}\\\\ &{=\\mathbb{E}_{\\epsilon}\\left[\\underset{s,j\\in[d],j^{\\prime}\\in[d],w}{\\operatorname*{sup}}\\;\\;\\;\\left\\langle e_{j}e_{j^{\\prime}}^{\\top}\\odot w_{\\mathfrak{v}}^{\\top},\\underset{i\\in(f,A_{i},y_{i})=(\\mathfrak{v},\\mathfrak{v}^{\\prime})}{\\sum}s\\epsilon_{i}P_{\\mathfrak{a}}^{(i)}\\right\\rangle_{F}\\right]}\\\\ &{\\le\\sqrt{n_{(\\mathfrak{v},\\mathfrak{v}^{\\prime})}B_{\\mathfrak{a}}^{4}B_{\\mathfrak{a}}^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "As terms of $I_{3}$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I_{3}=\\mathbb{E}_{\\boldsymbol{\\epsilon}}\\left[\\underset{s,j\\in[d],j^{\\prime}\\in[d],w}{\\operatorname*{sup}}\\quad\\underset{i:c(f,A_{i},y_{i})=(v,v^{\\prime})}{\\sum}s\\epsilon_{i}\\left\\langle e_{j^{\\prime}},w_{v}\\right\\rangle\\left\\langle e_{j},a_{k}^{(i)}\\right\\rangle\\right]}\\\\ &{\\quad=\\mathbb{E}_{\\boldsymbol{\\epsilon}}\\left[\\underset{s,j\\in[d],j^{\\prime}\\in[d],w}{\\operatorname*{sup}}\\quad\\langle e_{j^{\\prime}},w_{v}\\rangle\\left\\langle e_{j},\\underset{i:c(f,A_{i},y_{i})=(v,v^{\\prime})}{\\sum}s\\epsilon_{i}a_{k}^{(i)}\\right\\rangle\\right]}\\\\ &{\\quad\\leq B_{0}\\mathbb{E}_{\\boldsymbol{\\epsilon}}\\left[\\left\\|\\underset{i:c(f,A_{i},y_{i})=(v,v^{\\prime})}{\\sum}\\epsilon_{i}a_{k}^{(i)}\\right\\|_{2}\\right]\\leq B_{0}\\sqrt{n_{(v,v^{\\prime})}B_{a}^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Combined equations (22), (23), (24), (25), and use $\\begin{array}{r}{\\sum_{n=1}^{N^{\\prime}}|{\\mathcal{C}}_{n}|=T}\\end{array}$ we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\epsilon}\\left[\\underset{f\\in\\mathcal{F}_{\\boldsymbol{u}}}{\\operatorname*{sup}}\\sum_{\\substack{i\\in\\left(f,x_{i},y_{i}\\right)=\\left(v,v^{\\prime}\\right)}}{\\epsilon_{i}\\left(f\\left(x_{i},v\\right)\\right)}\\right]}\\\\ &{\\leq c_{\\sigma}^{L-1}\\Pi_{k=1}^{L}\\|W_{k}\\|_{1}\\left(c_{\\sigma}^{2}\\left\\|W_{w}^{(1)}\\right\\|_{1}\\left\\|W_{w}^{(2)}\\right\\|_{1}\\left(B_{a}^{2}+B_{a}^{2}B_{0}+B_{0}B_{a}\\right)T+B_{0}\\right)\\sqrt{n_{\\left(v,v^{\\prime}\\right)}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\epsilon}\\left[\\operatorname*{sup}_{f\\in\\mathcal{F}_{\\mathrm{ta}}}\\sum_{i:c\\left(f,\\pmb{x}_{i},\\pmb{y}_{i}\\right)=\\left(\\pmb{v},\\pmb{v}^{\\prime}\\right)}\\epsilon_{i}f\\left(\\pmb{x}_{i},\\pmb{v}^{\\prime}\\right)\\right]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\epsilon}\\left[\\operatorname*{sup}_{f\\in\\mathcal{F}_{\\mathrm{u}}}\\sum_{i:c\\left(f,\\pmb{x}_{i},\\pmb{y}_{i}\\right)=\\left(\\pmb{v},\\pmb{v}^{\\prime}\\right)}\\epsilon_{i}f\\left(\\pmb{x}_{i},\\pmb{v}\\right)\\right]\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "share the same upper bound, combined lemma B.1, equations (26), and (16), we get the desired result. ", "page_idx": 19}, {"type": "text", "text": "B.6 Proof of Theorem 4.8 ", "text_level": 1, "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{P}_{\\mathrm{rank}}^{\\mathrm{err}}(K)=\\mathbb{E}_{(\\mathbf{x},y)\\sim\\mathcal{D}^{\\prime}}\\left[\\mathbb{I}\\left[f(\\mathbf{x}_{i},y_{i})-\\underset{j\\in B(\\mathbf{x})}{\\operatorname*{max}}\\ f(\\mathbf{x},j)<0\\right]\\right]}\\\\ &{\\qquad\\qquad=\\mathbb{E}_{(\\mathbf{x},y)\\sim\\mathcal{D}}\\left[\\mathbb{I}\\left[f(\\boldsymbol{x}_{i},y_{i})-\\underset{j\\in B(\\mathbf{x})}{\\operatorname*{max}}\\ f(\\mathbf{x},j)<0\\right]\\cdot\\frac{P^{\\prime}(\\mathbf{x},y)}{P(\\mathbf{x},y)}\\right]}\\\\ &{\\qquad\\qquad\\leq\\mathbb{E}_{(\\mathbf{x},y)\\sim\\mathcal{D}}\\left|1-\\frac{P^{\\prime}(\\mathbf{x},y)}{P(\\mathbf{x},y)}\\right|+\\mathbb{E}_{(\\mathbf{x},y)\\sim\\mathcal{D}}\\left[1_{\\rho_{f}(\\mathbf{x},y)<0}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where we define ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\rho_{f}(\\pmb{x},y)=\\operatorname*{min}_{y^{\\prime}\\in\\mathcal{B}(\\pmb{x})}\\left(f(\\pmb{x},y)-f\\left(\\pmb{x},y^{\\prime}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Let $\\widetilde{\\mathcal{F}}=\\{(\\pmb{x},y)\\mapsto\\rho_{f}(\\pmb{x},y):f\\in\\mathcal{F}\\}$ , By lemma C.3, with probability at least $1-\\delta$ , for all $f\\in\\mathcal F$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Phi\\left(\\rho_{f}(\\pmb{x},y)\\right)\\right]\\leq\\frac{1}{m}\\sum_{i=1}^{m}\\Phi\\left(\\rho_{f}\\left(\\pmb{x}_{i},y_{i}\\right)\\right)+2\\hat{\\mathcal{R}}_{m}(\\Phi\\circ\\tilde{\\mathcal{F}})+B_{\\Phi}\\sqrt{\\frac{2\\log\\left(2/\\delta\\right)}{m}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since $\\mathbb{I}(u<0)\\le\\Phi(u)$ for all $u\\in\\mathbb{R}$ , and given the Lipschitz continuity of $\\Phi$ , we can write: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[1_{\\rho_{f}\\left(\\mathbf{x},y\\right)<0}\\right]\\le\\mathbb{E}\\left[\\Phi\\left(\\rho_{f}(\\mathbf{x},y)\\right)\\right]\\le\\frac{1}{m}\\sum_{i=1}^{m}\\Phi\\left(\\rho_{f}\\left(x_{i},y_{i}\\right)\\right)+4c_{\\Phi}\\hat{\\mathcal{R}}_{m}(\\widetilde{\\mathcal{F}})+B_{\\Phi}\\sqrt{\\frac{2\\log\\left(2/\\delta\\right)}{m}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "$\\hat{\\mathcal{R}}_{m}(\\widetilde{\\mathcal{F}})$ can be upper-bounded as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{\\hat{R}}_{m}(\\widetilde{\\mathcal{F}})=\\displaystyle\\frac{1}{m}\\mathbb{E}\\left[\\operatorname*{sup}_{f\\in\\mathcal{F}}\\sum_{i=1}^{m}\\epsilon_{i}\\left(f\\left(x_{i},y_{i}\\right)-\\operatorname*{max}_{y\\in\\mathcal{B}(x_{i})}f\\left(x_{i},y\\right)\\right)\\right]}\\\\ &{\\qquad\\quad\\leq\\displaystyle\\frac{1}{m}\\mathbb{E}\\left[\\operatorname*{sup}_{f\\in\\mathcal{F}}\\sum_{i=1}^{m}\\epsilon_{i}f\\left(x_{i},y_{i}\\right)\\right]+\\displaystyle\\frac{1}{m\\,\\epsilon}\\left[\\operatorname*{sup}_{f\\in\\mathcal{F}}\\sum_{i=1}^{m}\\epsilon_{i}\\operatorname*{max}_{y\\in\\mathcal{B}(x_{i})}\\left(f\\left(x_{i},y\\right)\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Now we bound the first term above. Observe that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{1}{m\\epsilon}\\mathbb{E}\\left[\\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}\\sum_{i=1}^{m}\\epsilon_{i}f\\left(x_{i},y_{i}\\right)\\right]=\\frac{1}{m\\epsilon}\\mathbb{E}\\left[\\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}\\sum_{i=1}^{m}\\epsilon_{i}f\\left(x_{i},y\\right)1_{y_{i}=y}\\right]}}\\\\ &{}&{\\leq\\frac{1}{m}\\sum_{\\psi\\in\\mathcal{Y}}\\mathbb{E}\\left[\\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}\\sum_{i=1}^{m}\\epsilon_{i}f\\left(x_{i},y\\right)1_{y_{i}=y}\\right]}\\\\ &{}&{=\\sum_{\\psi\\in\\mathcal{Y}}\\frac{1}{m}\\mathbb{E}\\left[\\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}\\sum_{i=1}^{m}\\epsilon_{i}f\\left(x_{i},y\\right)\\left(\\frac{s_{i}}{2}+\\frac{1}{2}\\right)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $s_{i}=2\\cdot1_{y_{i}=y}-1$ . Since $\\epsilon_{i}\\in\\{-1,+1\\}$ , we have that $\\epsilon_{i}$ and $\\epsilon_{i}s_{i}$ admit the same distribution and, for any $y\\in\\mathcal{V}$ , each of the terms of the right-hand side can be bounded as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{m\\epsilon}\\underset{\\epsilon}{\\mathbb{E}}\\left[\\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}\\sum_{i=1}^{m}\\epsilon_{i}f\\left(x_{i},y\\right)\\left(\\frac{s_{i}}{2}+\\frac{1}{2}\\right)\\right]}\\\\ &{\\quad\\leq\\frac{1}{2m\\epsilon}\\underset{\\epsilon}{\\mathbb{E}}\\left[\\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}\\sum_{i=1}^{m}\\epsilon_{i}s_{i}f\\left(x_{i},y\\right)\\right]+\\frac{1}{2m\\epsilon}\\underset{\\epsilon}{\\mathbb{E}}\\left[\\underset{f\\in\\mathcal{F}}{\\operatorname*{sup}}\\sum_{i=1}^{m}\\epsilon_{i}f\\left(x_{i},y\\right)\\right]}\\\\ &{\\quad\\leq\\widehat{\\mathfrak{R}}_{m}\\left(\\Pi_{1}(\\mathcal{F})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus, we can write $\\begin{array}{r}{\\frac{1}{m}\\mathbb{E}_{\\epsilon}\\left[\\operatorname*{sup}_{f\\in\\mathcal{F}}\\sum_{i=1}^{m}\\epsilon_{i}f\\left(\\pmb{x}_{i},y_{i}\\right)\\right]\\leq N\\hat{\\mathcal{R}}_{m}\\left(\\Pi_{1}(\\mathcal{F})\\right).}\\end{array}$ To bound the second term, we first apply lemma C.5 which yields that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{1}{m}\\mathbb{E}\\left[\\operatorname*{sup}_{f\\in\\mathcal{F}}\\sum_{i=1}^{m}\\epsilon_{i}\\operatorname*{max}_{y\\in\\mathcal{B}(\\pmb{x}_{i})}f\\left(\\pmb{x}_{i},y\\right)\\right]\\leq\\sum_{j=1}^{K}\\hat{\\mathcal{R}}_{m}\\left(\\mathcal{F}_{j}\\right),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where we use $B_{j}(\\mathbf{x}_{i})$ denote the $j$ -th elements in $B(x_{i})$ and ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\hat{\\mathcal{R}}_{m}\\left(\\mathcal{F}_{j}\\right)=\\frac{1}{m}\\mathbb{E}\\left[\\operatorname*{sup}_{f\\in\\mathcal{F}}\\sum_{i=1}^{m}\\epsilon_{i}f\\left(x_{i},\\mathcal{B}_{j}(x_{i})\\right)\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\displaystyle\\frac{1}{m}\\sum_{y\\in\\mathcal{Y}}\\mathbb{E}\\left[\\operatorname*{sup}_{f\\in\\mathcal{F}}\\sum_{i=1}^{m}\\epsilon_{i}f\\left(x_{i},y\\right)\\mathbb{I}(\\mathcal{B}_{j}(x_{i})=y)\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\displaystyle N\\hat{\\mathcal{R}}_{m}\\left(\\Pi_{1}(\\mathcal{F})\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the last inequality holds due to equations (29) and (30). ", "page_idx": 20}, {"type": "text", "text": "Combined equations (27), (28) and above equations, we get the desired results. ", "page_idx": 20}, {"type": "text", "text": "B.7 Proof of Theorem 4.9 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Using the same analysis applied to equation 15, we can derive the following results: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\epsilon}\\left[\\operatorname*{sup}_{f\\in\\mathcal{F}_{\\mathrm{lin}},v\\in\\mathcal{V}}\\sum_{i=1}^{m}\\epsilon_{i}\\left(f\\left(\\boldsymbol{x}_{i},\\boldsymbol{v}\\right)\\right)\\right]\\leq B_{0}B_{x}\\sqrt{m}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Using the same analysis applied to equation 19, we can derive the following results: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\epsilon}\\left[\\operatorname*{sup}_{f\\in\\mathcal{F}_{\\operatorname*{mip}},v\\in\\mathcal{Y}}\\sum_{i=1}^{m}\\epsilon_{i}\\left(f\\left(x_{i},v\\right)\\right)\\right]\\leq2c_{\\sigma}^{L-1}B_{1}^{L}\\cdot(B_{0}+B_{x})\\sqrt{m}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Using the same analysis applied to equation 26, we can derive the following results: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\epsilon}\\left[\\underset{f\\in\\mathcal{F}_{\\mathrm{t}_{0},v\\in\\mathcal{Y}}}{\\operatorname*{sup}}\\sum_{i=1}^{m}\\epsilon_{i}\\left(f\\left(\\boldsymbol{x}_{i},\\boldsymbol{v}\\right)\\right)\\right]}\\\\ &{\\le c_{\\sigma}^{L-1}\\Pi_{k=1}^{L}\\|\\boldsymbol{W}_{k}\\|_{1}\\left(c_{\\sigma}^{2}\\left\\|\\boldsymbol{W}_{w}^{(1)}\\right\\|_{1}\\left\\|\\boldsymbol{W}_{w}^{(2)}\\right\\|_{1}\\left(B_{a}^{2}+B_{a}^{2}B_{0}+B_{0}B_{a}\\right)T+B_{0}\\right)\\sqrt{m}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "C Auxiliary Lemmas ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Lemma C.1. For matrix $\\pmb{A}$ and $_B$ , vector $\\pmb{v}$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|A B v\\|_{\\infty}\\leq\\|A\\|_{\\infty}\\|B\\|_{\\infty}\\|v\\|_{\\infty},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where we denoted $\\|A\\|_{\\infty}$ as $\\|A\\|_{\\infty,\\infty}=m a x(|a_{i,j}|)$ . ", "page_idx": 20}, {"type": "text", "text": "Proof of lemma $C.I$ . We have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|A B v\\|_{\\infty}=\\left\\|A{\\frac{B v}{\\|B v\\|_{\\infty}}}\\right\\|_{\\infty}\\|B v\\|_{\\infty}\\leq\\|A\\|_{\\infty}\\|B v\\|_{\\infty}\\leq\\|A\\|_{\\infty}\\|B\\|_{\\infty}\\|v\\|_{\\infty}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Lemma C.2 (Theorem 4.15 (iv) in [22]). Let $\\mathcal{F}$ be classes of real functions. If $\\mathcal{A}:\\mathbb{R}\\,\\rightarrow\\,\\mathbb{R}$ is Lipschitz with constant $L$ and satisfies $\\boldsymbol{A}(0)=0$ , then ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\hat{R}_{\\ell}(A\\circ\\mathcal{F})\\leq2L\\hat{R}_{\\ell}(\\mathcal{F}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Lemma C.3 (Theorem 26.5 in [21]). If the magnitude of our loss function is bounded above by $c_{s}$ , with probability greater than $1-\\delta$ for all $h\\in\\mathcal H$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}_{(\\mathbf{x},y)\\sim\\mathcal{D}}[\\ell(h(\\mathbf{x}),y)]-\\frac{1}{n}\\sum_{i=1}^{n}\\ell\\left(h\\left(\\mathbf{x}_{i}\\right),y_{i}\\right)\\right|\\leq2\\hat{\\mathcal{R}}_{n}(\\ell\\circ\\mathcal{H},S)+c\\sqrt{\\frac{2\\log(2/\\delta)}{n}},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\ell\\circ\\mathcal{H}=\\{\\ell(h(\\pmb{x}),y)\\mid(\\pmb{x},y)\\in\\mathcal{X}\\times\\mathcal{Y},h\\in\\mathcal{H}\\}.$ ", "page_idx": 21}, {"type": "text", "text": "Lemma C.4 (Contraction lemma, lemma 26.9 in [21]). For each $i\\in[m]$ , let $\\phi_{i}:\\mathbb{R}\\rightarrow\\mathbb{R}$ be a $\\rho$ Lipschitz function, namely for all $\\alpha,\\beta\\in\\mathbb{R}$ we have $|\\phi_{i}(\\alpha)-\\phi_{i}(\\beta)|\\leq\\rho|\\alpha-\\beta|$ . For $\\pmb{a}\\in\\mathbb{R}^{m}$ let $\\phi(a)$ denote the vector $\\left(\\phi_{1}\\left(a_{1}\\right),\\ldots,\\phi_{m}\\left(y_{m}\\right)\\right)$ . Let $\\phi\\circ A=\\{\\phi(\\mathbf{a}):a\\in A\\}$ . Then, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\hat{\\mathcal{R}}_{m}(\\phi\\circ A)\\leq\\rho\\hat{\\mathcal{R}}_{m}(A).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Lemma C.5 ( Lemma 9.1 in [20]). Let $\\mathcal{F}_{1},\\ldots,\\mathcal{F}_{l}$ be $l$ hypothesis sets, $l~\\geq~1$ , and let $\\mathcal{G}\\,=$ $\\left\\{\\operatorname*{max}\\left\\{h_{1},\\dots,\\ h_{l}\\right\\}:h_{i}\\in\\mathcal{F}_{i},i\\in[l]\\right\\}$ . Then, for any sample $S$ of size $m$ , the empirical Rademacher complexity of $\\mathcal{G}$ can be upper bounded as follows: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\widehat{\\mathfrak{R}}_{m}(\\mathcal{G})\\leq\\sum_{j=1}^{l}\\widehat{\\mathfrak{R}}_{m}\\left(\\mathcal{F}_{j}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "D Experiments ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We conduct experiments on real-world datasets to validate the effectiveness of the proposed method and theoretical insights. All experiments were conducted on a Linux server equipped with a 3.00 GHz Intel CPU, 300 GB of main memory, and NVIDIA 20/30 series GPUs. ", "page_idx": 21}, {"type": "table", "img_path": "m1a4CrRJR7/tmp/3d96c6d441cf23b8b4b3e1a40b24c7c6d6dcc37351bde28ad0f52687f4008580.jpg", "table_caption": ["Table 2: Statistics of Datasets "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "D.1 Datasets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We evaluate the two-stage models with two real-world recommendation datasets, which can be downloaded from the $\\mathrm{url}^{2}$ . The datasets are MovieLens 10M (abbreviated as Movie), Microsoft News Dataset (abbreviated as Mind). Following previous work [8, 7], since some datasets only include rating-based explicit feedback, they should be converted into implicit feedback for inputs. These datasets are pre-processed by flitering the users who interact with no more than 15 items. The overall information of datasets is summarized in Table 2. ", "page_idx": 21}, {"type": "text", "text": "D.2 Settings ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In each dataset, we randomly choose $10\\%$ users as validation users, $10\\%$ users as test users, and all the left users as training users. Following [29, 28], we use a slide window to split user-item interaction histories into slices of length 70 at most. For training users\u2019 data, the first 69 interactions are used for input context and the 70-th item is regarded as the ground truth of prediction. Each of the 10 time windows contains $[1,1,1,2,2,2,10,10,20,20]$ (sum up to 69; if the length of behavior history is less than 69, pad the absence by zero) interactions. For data of both validation users and test users, we regard the first half as context and others as ground truth. ", "page_idx": 21}, {"type": "text", "text": "In the experiment on the effect of branch number, reported in Figure 1, we used a structure similar to the TDM model [29], except that we increased the number of tree branches and replaced the loss used in training with sampled softmax like [8]. We randomly initialize the correspondence between leaf nodes and labels and use a single well-trained tree model as the retriever model. The dimensions of item and node embeddings are both set to 24 across different branch numbers. We use Adam as the optimizer, with a learning rate of $1.0e{-3}$ with exponential decay. For different branch numbers, we conduct a grid search on the hyperparameters, including weight decay and the number of negative samples. Specifically, we explore weight decay values within the range $[1e{-2,1e{-3,1e{-4,1e{-5}}}}]$ , and the number of negative samples ranges from 50 to 200 in increments of 10. For each branch number, we perform a beam search with a beam size of 100 and report the highest recall $@20$ value achieved during the grid search of hyperparameters. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "In the experiment on harmonized distribution reported in Table 1, we analyze the effect of the training data distribution on the performance of the ranker model within the two-stage model. Following the same setup as the previous experiment, we use the same retriever model and the DIN model [27] as the ranker model. In DIN, we replace the original loss function with a sampled softmax loss, sampling 60 negative examples for each loss computation. The embedding dimension of each item is set to 96, the hidden dimensions of the attention units are set to [64, 16], and the hidden dimensions of the fully connected layers are set to [200, 80, 1]. The retriever model remains fixed throughout the experiment, while the ranker model is trained on different datasets: the original training data and a subset containing only items successfully retrieved by the retriever. We use precision as a metric to save the ranker model that performs best on the validation set during the training process and then evaluate the overall classification accuracy on the test set of the two-stage model. ", "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The main claims in the abstract and introduction accurately summarize the key contributions and findings of the paper, and they align with the theoretical and experimental results presented. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We have stated the applicability of the proposed method in the theoretical result discussion part, noting that it relies on a retriever model with a high recall rate. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The assumptions required for our theoretical results are described in the main paper, and proofs are provided in the appendix. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We provide detailed experimental settings in the appendix. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We have provided links to the publicly available data and submitted a ZIP flie containing the experimental code. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We provide detailed experimental settings in the appendix. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [No] ", "page_idx": 25}, {"type": "text", "text": "Justification: We reported the best results within the adjustable parameter range for each method. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We provided a description of the platform and hardware used for the experiments. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We adhere to the NeurIPS Code of Ethics. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [No] ", "page_idx": 26}, {"type": "text", "text": "Justification: This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 26}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The research involves only publicly available datasets and standard models, posing no significant misuse risks, thus no specific safeguards were necessary. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We have cited the relevant papers and provided links to download the data. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not introduce any new assets, thus documentation for such is not applicable. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing or research with human subjects, thus the inclusion of participant instructions, screenshots, and compensation details is not applicable. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: This paper does not involve research with human subjects. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 28}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]