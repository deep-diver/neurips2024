[{"type": "text", "text": "MetaLA: Unified Optimal Linear Approximation to Softmax Attention Map ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yuhong Chou1,2,\u2217 Man Yao2\u2217, Kexin $\\mathbf{Wang^{2}}$ , Yuqi $\\mathbf{Pan^{2}}$ , Ruijie $\\mathbf{Zhu^{3}}$ , Yiran Zhong4, Yu Qiao4, Jibin $\\mathbf{W}\\mathbf{u}^{1}$ , Bo $\\mathbf{Xu^{2}}$ , Guoqi $\\mathbf{L}\\mathbf{i}^{2}\\mathbf{\\Omega}$ \u2020 ", "page_idx": 0}, {"type": "text", "text": "1The Hong Kong Polytechnic University 2Institute of Automation, Chinese Academy of Sciences 3UC Santa Cruz 4Shanghai AI Lab ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Various linear complexity models, such as Linear Transformer (LinFormer), State Space Model (SSM), and Linear RNN (LinRNN), have been proposed to replace the conventional softmax attention in Transformer structures. However, the optimal design of these linear models is still an open question. In this work, we attempt to answer this question by finding the best linear approximation to softmax attention from a theoretical perspective. We start by unifying existing linear complexity models as the linear attention form and then identify three conditions for the optimal linear attention design: i) Dynamic memory ability; ii) Static approximation ability; iii) Least parameter approximation. We find that none of the current linear models meet all three conditions, resulting in suboptimal performance. Instead, we propose Meta Linear Attention (MetaLA) as a solution that satisfies these conditions. Our experiments on Multi-Query Associative Recall (MQAR) task, language modeling, image classification, and Long-Range Arena (LRA) benchmark demonstrate that MetaLA is more effective than the existing linear models. Code: https://github.com/BICLab/MetaLA ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Transformer with softmax attention [1] beneftis from efficient parallel training and exhibits impressive performance on deep learning applications [2, 3, 4, 5, 6, 7], but it suffers from the quadratic growth of computation cost to the input length [8]. Linear recurrent models, such as LinFormer [9], SSM [10], and LinRNN [11], are expected to achieve linear substitution of Transformer. The original intention of LinFormer is to replace softmax attention, which exploits the kernel approach to decompose softmax operation; typical work includes TransNormer [12, 13], RetNet [14], GLA [15]. On the other hand, SSMs, such as S4 [10] and Mamba [16], are models inspired by the classical state-space approach, which enjoys sub-quadratic training and inference like either a recurrence or convolution. In contrast, LinRNN is a revival of traditional RNNs, including RWKV-4 [17], Griffin [18], LRU [19], etc., which solves the training difficulties of traditional RNNs due to nonlinear dependencies between hidden states. It is natural to think that they are different types of models, since these LinFormer/SSM/LinRNN models have different origins and forms. ", "page_idx": 0}, {"type": "text", "text": "This work breaks this perception and abstracts existing LinFormer/SSM/LinRNN models into a unified linear attention form, which has the following significance: i) Facilitates understanding the key designs of existing linear models. Through the unified form, we demonstrate that the main difference between LinFormer/SSM/LinRNN is the hidden state size, how to maintain the hidden state, and how to perform parameter mapping. ii) Links LinFormer/SSM/LinRNN to softmax attention in terms of functionality. The recurrent inference complexity of softmax attention is $O(n)$ , which can also be regarded as the maintenance of a hidden state with infinite size. Linear models with $\\mathcal{O}(1)$ inference complexity are hoping to achieve the same functionality as softmax attention using a fixed hidden state. Since we have unified LinFormer/SSM/LinRNN into linear attention in the form of Query, Key, and Value, we can understand and evaluate existing linear models from the view of \u201cDoes the linear attention map have the function of softmax attention map?\u201d. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To answer this question, we define the necessary conditions for achieving \u201coptimal linear approximation to softmax attention map\". First, linear attention needs to satisfy dynamic memory and static approximation to realize the approximation. The former defines memory ability: linear attention with limited hidden states should be able to store the most important information and forget unimportant ones. The latter defines the modeling ability: a linear attention map should be able to approximate any softmax attention map. According to our theoretical analysis, Query and dynamic decay are necessary conditions for approximation. Thus, linear models such as TransNormer [13], RetNet [14], RWKV-4 [17], LRU [19], HGRN [20], H3 [21], S5 [22], cannot achieve approximation of the softmax attention functions. Second, the Key matrix is not required to achieve approximation, so Mamba [16] and GLA [15] are not optimal parametric approximations. ", "page_idx": 1}, {"type": "text", "text": "We then propose the MetaLA module, which can satisfy the necessary conditions for optimal linear approximation to softmax attention. MetaLA makes three enhancements: i) Removes the unnecessary Key matrices; ii) Employs self-augmentation to enhance the token\u2019s attention to itself, which avoids attention dilution [12]; iii) Exploits short convolutions to enhance local interactions. We then build a MetaLA Transformer based on MetaLA. Our experiments on associative recall, language modeling, long sequence modeling, and image classification show the effectiveness of MetaLA. Furthermore, we conduct ablation studies to validate the effectiveness of each proposed enhancement in MetaLA. Finally, we discuss two open questions: i) How to further improve linear attention based on the approximation theory introduced in this work? ii) Does the approximation of linear attention to softmax attention imply that it has an upper limit on its capacity? ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "For notations in this work, we use bold upper-case letters for matrices (e.g., Q, K), bold lower-case letters for row vectors (e.g., $\\mathbf{q}_{t},\\mathbf{k}_{t})$ , and italic upper-case for learnable parameter matrices $(\\mathbf{e}.\\mathbf{g}.,W_{Q})$ . We generally use the same alphabet to show the rows of a matrix, e.g., $\\mathbf{q}_{t}$ is the $t$ -th row of $\\mathbf{Q}$ . ", "page_idx": 1}, {"type": "text", "text": "Softmax Attention first calculates an attention map SoftAttMap $(\\mathbf{Q},\\mathbf{K})$ through $\\mathbf{Q}$ (Query), $\\mathbf{K}$ (Key), and use the attention map to weight different tokens $\\mathbf{V}$ (Value) later: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbf{O}=\\mathrm{SoftAttMap}\\left(\\mathbf{Q},\\mathbf{K}\\right)\\mathbf{V}=\\mathrm{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^{\\top}}{\\sqrt{d_{k}}}\\odot\\mathbf{M}\\right)\\mathbf{V}}&{{}\\in\\mathcal{R}^{n\\times d_{v}},}\\\\ {\\mathbf{Q},\\mathbf{K}=\\mathbf{X}W_{Q},\\mathbf{X}W_{K}}&{{}\\in\\mathcal{R}^{n\\times d_{k}};\\quad\\mathbf{V}=\\mathbf{X}W_{V}\\quad\\in\\mathcal{R}^{n\\times d_{v}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\pmb{W}_{Q},\\pmb{W}_{K}\\in\\mathcal{R}^{d\\times d_{k}}$ , $W_{V}\\in\\mathcal{R}^{d\\times d_{v}}$ are learnable matrices, $n,d,d_{k},d_{v}$ are sequence length, model dimension, Key/Query and Value dimension, respectively. $\\mathbf{X}\\in\\mathcal{R}^{n\\times d}$ refers to the input. $\\mathbf{M}\\in\\mathcal{R}^{n\\times n}$ is a mask matrix in autoregressive tasks to prevent a token from attending to future tokens. The $t$ -th row of SoftAttMap $(\\mathbf{Q},\\mathbf{K})$ is a probability distribution that represents the attention scores between token $\\mathbf{v}_{t}$ to others. Softmax attention in Eq. (1) enables efficient parallel training, but suffers from $O(n^{2})$ time and memory complexity [9]. It uses the recurrent form during inference: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbf o_{t}=\\frac{\\sum_{s=1}^{t}\\exp\\left(\\mathbf q_{t}\\mathbf k_{s}^{\\top}\\right)\\mathbf v_{s}}{\\sum_{s=1}^{t}\\exp\\left(\\mathbf q_{t}\\mathbf k_{s}^{\\top}\\right)}\\quad\\in\\mathcal{R}^{1\\times d_{v}},}\\\\ &{\\mathbf q_{t},\\mathbf k_{t}=\\mathbf x_{t}W_{Q},\\mathbf x_{t}W_{K}\\quad\\in\\mathcal{R}^{1\\times d_{k}};\\quad\\mathbf v_{t}=\\mathbf x_{t}W_{V}\\quad\\in\\mathcal{R}^{1\\times d_{v}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "At each time $t$ , token mix is computed between query $\\mathbf{q}_{t}$ and all the keys, values before $\\mathbf{k}_{s},\\mathbf{v}_{s}(s\\le t)$ .   \nThis \"KV cache\" results in ${\\mathcal{O}}(n)$ time and memory complexity per token during inference. ", "page_idx": 1}, {"type": "text", "text": "Linear Transformer (LinFormer) is a substitute for softmax attention, which can be expressed as a linear dot-product of kernel feature maps [9]: ", "page_idx": 2}, {"type": "equation", "text": "$$\n{\\bf o}_{t}=\\frac{\\sum_{s=1}^{t}F({\\bf q}_{t},{\\bf k}_{s}){\\bf v}_{s}}{\\sum_{s=1}^{t}F({\\bf q}_{t},{\\bf k}_{s})},\\quad F({\\bf q}_{t},{\\bf k}_{s})=\\phi({\\bf q}_{t})\\phi^{\\top}({\\bf k}_{s}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathbf{q}_{t},\\mathbf{k}_{t}\\in\\mathcal{R}^{1\\times d_{k}}$ and ${\\bf v}_{t}\\in\\mathcal{R}^{1\\times d_{v}}$ are query, key and value at position $t$ , which are obtained in the same manner as softmax attention. $\\bar{F}(\\cdot)$ is the kernel function usually constrained to be non-negative. $\\phi(\\cdot)$ is map function applied row-wise to $\\mathbf{Q}$ and $\\mathbf{K}$ . By removing the nonlinear softmax operation, LinFormer enables inference with $\\mathcal{O}(1)$ time and memory complexity per token. LinFormer can also be formulated in the following parallel form during training ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{O}=\\left(\\phi(\\mathbf{Q})\\phi^{\\top}(\\mathbf{K})\\odot\\mathbf{M}\\right)\\mathbf{V}\\quad\\in\\mathcal{R}^{n\\times d_{v}},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "which has ${\\mathcal{O}}(n)$ time and memory complexity using chunkwise algorithm. Recent advances in LinFormer mainly focus on training acceleration[12, 13, 23] or improving performance[14]. ", "page_idx": 2}, {"type": "text", "text": "State-Space Model (SSM) come from continuous-time system which maps a 1D function $x(t)\\in\\mathcal{R}$ to another function $y(t)\\in\\mathcal{R}$ via a hidden state $\\mathbf{h}(t)\\in\\mathcal{R}^{N}$ . In SSM, the continuous parameters can be discretized using a step size $\\Delta$ and get discrete parameters ${\\overline{{A}}},{\\overline{{B}}},{\\overline{{C}}}$ . The resulting discrete-time system is used to model sequences $\\mathbf{x},\\bar{\\mathbf{y}}\\in\\mathcal{R}^{1\\times n}$ with elements $x_{t},y_{t}\\in\\mathcal{R}$ via the recurrent form: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{h}_{t}=\\overline{{A}}\\mathbf{h}_{t-1}+\\overline{{B}}x_{t},\\quad y_{t}=\\overline{{C}}\\mathbf{h}_{t},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "in autoregressive inference with $\\mathcal{O}(1)$ time and memory complexity per token. The linear timeinvariant SSM above can be unrolled and computed using the long convolution with kernel $\\kappa$ ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{K:=(\\overline{{C B}},\\overline{{C A B}},\\cdots,\\overline{{C A}}^{n-1}\\overline{{B}})}&{{}\\in\\mathcal{R}^{1\\times n},\\quad\\mathbf{y}=K\\ast\\mathbf{x},}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $^*$ represents casual convolution operation [24], which enables parallelizable training utilizing Fast Fourier Transforms, resulting in $\\bar{O(n\\log n)}$ time and $\\mathcal{O}(n)$ memory complexity during training. When handling vector sequences $\\mathbf{X}$ $\\mathbf{\\hat{X}},\\mathbf{Y}\\in\\mathcal{R}^{d\\times n}$ , SSMs are applied individually on the $d$ channels. Typical SSMs (S4D[25], DSS[26], H3[21], S5[22]) employ data-independent structured transition matrix A or special initialization strategies HiPPO[27] to efficiently enhance long-range dependencies. Mamba [16] advances SSMs by introducing data-dependent parameters and designs a hardware-aware parallel algorithm, further improving prior $\\ O(n\\log n)$ into ${\\mathcal{O}}(n)$ time complexity during training. ", "page_idx": 2}, {"type": "text", "text": "Linear RNNs (LinRNN) Traditional RNNs suffer from slow sequential training, limited capability in modeling long-term dependencies, and difficulty in scaling. To address these issues, LinRNNs eliminate the nonlinearity within the recurrence and employ element-wise product instead of matrix multiplication [11, 28]. Typical LinRNN such as Gated Linear Recurrent Unit (GLRU) [20, 29] is ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbf{f}_{t},\\mathbf{i}_{t},\\mathbf{c}_{t}=\\sigma(\\mathbf{x}_{t}W_{f}+b_{f}),\\sigma(\\mathbf{x}_{t}W_{i}+b_{i}),\\phi(\\mathbf{x}_{t}W_{c}+b_{c})}&{{}\\in\\mathcal{R}^{1\\times d},}\\\\ {\\mathbf{h}_{t}=\\mathbf{f}_{t}\\odot\\mathbf{h}_{t-1}+\\mathbf{i}_{t}\\odot\\mathbf{c}_{t},\\in\\mathcal{R}^{1\\times d},}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathbf{x}_{t},\\mathbf{h}_{t}$ denote input and output, $\\mathbf{f}_{t},\\mathbf{i}_{t}$ are forget and input gates as in traditional RNNs, $\\odot$ is element-wise multiplication. Linear RNNs have $\\mathcal{O}(1)$ time and memory complexity per token during inference. Since Eq. (10) removes nonlinearity, it enables parallelized training using parallel scan[11], with only $\\mathcal{O}(n)$ time and memory complexity. Recent works have made effort to explore effective recurrence (LRU [19], RWKV [17]) or gating mechanisms (HGRN [20], Griffin [18]). ", "page_idx": 2}, {"type": "text", "text": "3 General Form of LinFormer/SSM/LinRNN Mechanisms ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Observing Eq. (3), Eq. (5), Eq. (7), and Eq. (10), we find that their recurrent forms during inference can all be understood from the view of maintaining hidden states. Softmax attention maintains an unlimited hidden state (KV cache). By contrast, LinFormer/SSM/LinRNN have limited hidden states: linear attention with $\\dot{\\phi}^{\\top}({\\bf k}_{t}){\\bf v}_{t}\\in\\mathcal{R}^{\\bar{d}_{k}\\times d_{v}}$ , SSM with ${\\mathbf{h}}_{t}\\in\\mathcal{R}^{N\\times d}$ , linear RNNs with $\\mathbf{h}_{t}\\in\\mathcal{R}^{1\\times d}$ , where $d_{k}>N>1$ in usual. Inspired by this fact, we unify LinFormer/SSM/LinRNN mechanisms in the form of linear attention, formally containing Query, Key, and Value matrices (see Fig. 1). ", "page_idx": 2}, {"type": "image", "img_path": "Y8YVCOMEpz/tmp/9b44011d2204e38f08c7640b0f059c718136a68e57dd421f330fccc04da205b1.jpg", "img_caption": ["Figure 1: General Form of LinFormer/SSM/LinRNN Mechanisms. The general form equips with two modes of parallel and recurrent computation which enjoys both training and inference efficiency. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "General Recurrent Form of LinFormer/SSM/LinRNN is: ", "text_level": 1, "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{q}_{t}=\\mathrm{f}_{q}(\\mathbf{x}_{t},\\theta_{q}),\\mathbf{k}_{t}=\\mathrm{f}_{k}\\big(\\mathbf{x}_{t},\\theta_{k}\\big),\\boldsymbol{\\alpha}_{t}=\\mathrm{f}_{\\alpha}\\big(\\mathbf{x}_{t},\\theta_{\\alpha}\\big)\\quad\\in\\mathcal{R}^{1\\times d_{k}},}\\\\ &{\\mathbf{v}_{t}=\\mathrm{f}_{v}\\big(\\mathbf{x}_{t},\\theta_{v}\\big),\\mathbf{g}_{t}=\\mathrm{f}_{g}\\big(\\mathbf{x}_{t},\\theta_{g}\\big)\\quad\\in\\mathcal{R}^{1\\times d_{v}},}\\\\ &{\\mathbf{S}_{t}^{h}=\\mathrm{diag}(\\alpha_{t}^{h})\\mathbf{S}_{t-1}^{h}+(\\mathbf{k}_{t}^{h})^{\\top}\\mathbf{v}_{t}^{h}\\quad\\in\\mathcal{R}^{d_{k}^{\\prime}\\times d_{v}^{\\prime}},}\\\\ &{\\mathbf{o}_{t}=\\mathrm{XNorm}(\\mathrm{concat}[\\mathbf{q}_{t}^{1}\\mathbf{S}_{t}^{1},\\mathbf{q}_{t}^{2}\\mathbf{S}_{t}^{2},\\cdot\\cdot\\cdot,\\mathbf{q}_{t}^{H}\\mathbf{S}_{t}^{H}])\\quad\\in\\mathcal{R}^{1\\times d_{v}},}\\\\ &{\\mathbf{y}_{t}=\\big(\\mathbf{o}_{t}\\odot\\mathbf{g}_{t}\\big)\\mathbf{W}_{\\mathcal{O}}\\quad\\in\\mathcal{R}^{1\\times d},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{x}_{t}\\,\\in\\,\\mathcal{R}^{1\\times d}$ is the $t$ -th input. ${\\bf q}_{t},{\\bf k}_{t},{\\bf v}_{t},\\alpha_{t},{\\bf g}_{t},{\\bf S}_{t}$ are query, key, value, decay, output gate, hidden state respectively. $\\mathrm{f}_{q/k/\\alpha}$ are functions that map $\\mathbf{x}_{t}$ from $\\mathcal{R}^{1\\breve{\\times}d}$ to $\\mathcal{R}^{1\\times d_{k}}$ , $\\theta_{q/k/\\alpha}$ are the corresponding parameters to be trained. Similarly, $\\mathrm{f}_{v/g}$ map $\\mathbf{x}_{t}$ from $\\mathcal{R}^{1\\times d}$ to $\\mathcal{R}^{1\\times d_{v}}$ and $\\theta_{v/g}$ $\\begin{array}{r}{d_{k/v}^{\\prime}={\\frac{d_{k/v}}{H}}}\\end{array}$ $h=1,\\cdots\\,,H$ is the $\\mathbf{q}_{t},\\alpha_{t},\\mathbf{k}_{t},\\mathbf{v}_{t}$ s. Ea head mai $H$ ins a hidden state $\\mathbf{S}_{t}^{h}$ . The $\\mathrm{diag}(\\alpha_{t}^{h})$ $\\mathbf{k}_{t}^{h}$ token $\\mathbf{v}_{t}^{h}$ . The hidden states are 2D matrix once $d_{k}^{\\prime}\\neq1$ . In Eq. (14), to turn back to 1D shape, the $\\mathbf{q}_{t}^{h}$ operation is necessary as a dot-product with $\\mathbf{S}_{t}^{h}$ , then the concat and normalization operations are followed. XNorm denotes any kinds of normalization. In Eq. (15), a gate machanism is optional for $\\mathbf{o}_{t}$ while the dimension should be projected back to $d$ from $d_{v}$ through $W_{O}\\in\\mathcal{R}^{d_{v}\\times d}$ . ", "page_idx": 3}, {"type": "text", "text": "From a functional view, Eq. (13) represents the update process of the hidden state, which contains historical information on keys and values. Eq. (14) represents query and weighted sum operations to derive the attention output. Eq. (15) represents gate and projection operations to get the final output. ", "page_idx": 3}, {"type": "text", "text": "General Parallel Form of LinFormer/SSM/LinRNN can be written as follow: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{O}=\\mathrm{LinAttMap}\\left(\\mathbf{Q},\\mathbf{K}\\right)\\mathbf{V}=\\left(\\left(\\left(\\mathbf{Q}\\odot\\mathbf{A}\\right)\\big(\\frac{\\mathbf{K}}{\\mathbf{A}}\\big)^{\\top}\\right)\\odot\\mathbf{M}\\right)\\mathbf{V},}\\\\ &{(\\mathbf{Q}/\\mathbf{K}/\\mathbf{V})_{t,:}=(\\mathbf{q}/\\mathbf{k}/\\mathbf{v})_{t},\\quad\\mathbf{A}_{t,:}=\\displaystyle\\prod_{j=1}^{t}\\alpha_{j},\\quad\\mathbf{M}_{i,j}=\\left\\{\\begin{array}{l l}{1,i\\leq j.}\\\\ {0,i>j.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "$\\frac{\\mathbf{K}}{\\mathbf{A}}$ denotes element-wise division, $(\\mathbf{Q})_{t,}$ : is the $t$ -th row of $\\mathbf{Q}$ , and LinAttMap $(\\mathbf{Q},\\mathbf{K})$ is the attention map. Each element in the attention map matrix is as follows (heads are omitted for simplicity): ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{LinAttMap}\\left(\\mathbf{Q},\\mathbf{K}\\right)_{t,s}=\\left\\{\\mathbf{q}_{t}\\cdot\\left(\\left(\\prod_{j=s+1}^{t}\\alpha_{j}\\right)\\odot\\mathbf{k}_{s}\\right)^{\\top},s\\leq t.\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "As shown in Tab. 1, the main differences between various linear models are parameter functions $\\mathrm{f}_{q/k/v/\\alpha/g}$ and dimension settings $d_{k},d_{v},H$ . We give details in appendix A1 on how to derive LinFormers/SSMs/linRNNs from our unified form, which is termed as \u201cLinear Attention (LinAtt)\". ", "page_idx": 3}, {"type": "text", "text": "LinFormer/SSM/LinRNN models have different origins, so they have different optimization perspectives and various hidden state sizes: i) LinFormer originate from approximation of vanilla softmax attention. They focus on designing better kernel function $\\phi$ , i.e., to optimize $\\mathrm{f}_{q},\\mathrm{f}_{k}$ ; They have relatively large matrix hidden state whose size $(d_{v}d_{k}/H)$ is mainly correlated to model dimension $d$ . ii) SSM originate from state space equations. So they focus on how to better maintain the hidden state and optimize $\\mathrm{f}_{\\alpha}$ ; They have a matrix hidden state of moderate size $(d_{v}N)$ , which is correlated to the fixed expansion $N$ . iii) LinRNN originate from removing nonlinearity in the recurrence of vanilla RNN. So they focus on designing better forget/input/output gates, i.e., to optimize $\\mathrm{f}_{\\alpha},\\mathrm{f}_{k},\\mathrm{f}_{g}$ ; They have 1D vector hidden state whose size $(d_{v})$ is relatively small. Despite these differences, they all try to design better parameter functions $\\mathrm{f}_{{q}/{k}/{v}/{\\alpha}/{g}}$ and maintain a limited hidden state $\\mathbf{S}_{t}$ . ", "page_idx": 3}, {"type": "table", "img_path": "Y8YVCOMEpz/tmp/452e01e13d255ed60da3c54b68cba8245a24d860bc323d8b470e11fe574a362a.jpg", "table_caption": ["Table 1: From our general form to existing linear models ( $^*$ indicates the bias term is omitted). "], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "4 Optimal Linear Approximation to the Softmax Attention Map ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We here discuss the optimal approximation of LinAttMap to SoftAttMap based on its general form. The function of softmax attention is two-fold: i) Memorizing information, all the current and historical information can be stored in KV cache; ii) Modeling relationships, softmax attention can calculate arbitrary attention scores of stored information. Unfortunately, such a powerfully expressive attention map generated by softmax attention requires infinite hidden states. By contrast, linear attention expects to exploit limited hidden states to achieve the same functionality as softmax attention. ", "page_idx": 4}, {"type": "text", "text": "Some existing linear models, such as Performer[30], RFA[31], etc., optimize the model with the goal of approximating the value of SoftAttMap. In contrast, this work investigates the functional approximation of SoftAttMap, which is the basis for value approximation. Specifically, we here attempt to answer two key questions: i) Can linear attention realize the function of softmax attention? ii) If it can be achieved, what kind of linear attention approximation is better? To achieve this goal, we first give the definition of necessary conditions of optimal linear approximation. Then we categorize the existing linear models based on the conditions of the optimal linear approximation. ", "page_idx": 4}, {"type": "text", "text": "Definition 4.1. Necessary Conditions of Optimal Linear Approximation to Softmax Attention Map. A function $f(\\mathbf{x}_{t},\\mathbf{x}_{s}\\bar{|}\\theta):\\mathcal{R}^{1\\times d}\\times\\mathcal{R}^{1\\times\\bar{d}}\\rightarrow\\mathcal{R}$ , used to compute attention score between any $\\mathbf{x}_{t}$ and $\\mathbf{x}_{s}$ (tokens), with parameters $\\theta$ , is an optimal linear approximation to softmax attention map if it satisfies: i) Linear complexity. Attention map can be computed in linear time, i.e., ${\\mathcal{O}}(n)$ space and time complexity during training and $O(1)$ space and time complexity during inference. ii) Dynamic memory ability. When handling inputs sequentially, $f(\\mathbf{x}_{t},\\mathbf{x}_{s}|\\boldsymbol{\\theta})$ with limited hidden states should be able to store the most important information adaptively while forgetting unimportant ones. iii) Static approximation ability: For an arbitrarily given softmax attention map $\\mathbf{P}$ with scores $p_{t s}$ , there must exists bounded $\\theta$ such that $f(\\mathbf{x}_{t},\\mathbf{x}_{s}\\vert\\theta)=p_{t s},\\forall t,s=1,\\cdots,n$ . iv) Least parameter approximation: On the premise that the first three conditions are met, use as few parameters as possible to achieve approximation to softmax attention map. ", "page_idx": 4}, {"type": "text", "text": "In definition 4.1, Condition 0 (C0) underlines computational and memory efficiency. Conditions 1 (C1) and 2 (C2) consider memory and modeling ability of linear attention. Due to limited state size $d$ , linear attention can only memorize the history of most important $d$ tokens without information loss and precisely model arbitrary attention map of those $d$ tokens. Condition 3 (C3) is our expectation to seek the least parameters on the premise that previous three conditions are met. ", "page_idx": 4}, {"type": "text", "text": "Theoretical Analysis for Optimal Linear Approximation. For the C1 condition, suppose the information about $\\mathbf{v}_{t_{1}},\\dots,\\mathbf{v}_{t_{d_{k}}}$ is successfully stored in $\\mathbf{S}_{t}$ $\\mathfrak{j}_{t}\\left(t_{1},\\ldots,t_{d_{k}}\\leq t\\right)$ , we will check whether the model can substitute unimportant $\\mathbf{v}_{t_{1}}$ when the new important input $\\mathbf{v}_{t+1}$ arrives. ", "page_idx": 4}, {"type": "text", "text": "For the C2 condition, Eq. (18) illustrates the LinAttMap only relate to $\\mathbf{q}_{t}\\;=\\;\\mathrm{f}_{q}\\big(\\mathbf{x}_{t},\\theta_{q}\\big),\\mathbf{k}_{t}\\;=$ $\\mathrm{f}_{k}(\\mathbf{x}_{t},\\theta_{k})$ , $\\alpha_{t}=\\mathrm{f}_{\\alpha}(\\mathbf{x}_{t},\\theta_{\\alpha})$ . Denote decay $\\mathbf{A}_{t}=\\mathrm{diag}(\\alpha_{t})$ . Assuming the inputs are good enough and the functions $(\\mathrm{f}_{q},\\mathrm{f}_{k},\\mathrm{f}_{\\alpha})$ are expressive enough, we can shift from solving $(\\theta_{q},\\theta_{k},\\theta_{\\alpha})$ to solving $\\left(\\mathbf{Q},\\mathbf{K},\\Lambda_{t}\\right)$ . We focus on approximating the attention scores between stored tokens, and the problem is simplified via: i) setting query dimension $d_{k}\\,=\\,1\\$ ; ii) considering only a given time $t$ and its attention distribution $\\mathbf{p}_{t}=[p_{t s},s=1,\\dots,t]\\in\\mathcal{R}^{1\\times t}$ . Then, C2 is proved by the following equations holding with bounded parameters, as a foundation conclusion: ", "page_idx": 4}, {"type": "table", "img_path": "Y8YVCOMEpz/tmp/1956bc5ade1bdefb2e8f8870ab11ecb20c58aab95e79089439ce61e16d4e258b.jpg", "table_caption": ["Table 2: A review of existing linear models. According to definition 4.1, existing linear models all have some deficiencies: i) Models without dynamic decay $\\mathbf{{A}}_{t}$ have no ability to memorize dynamically (not satisfying C1); ii) LinRNNs lack the selection ability brought by $\\mathbf{Q}$ (not satisfying C2), and the approximation ability is poor owing to the small hidden state; iii) Models with $\\mathbf{K}$ have redundant parameters (not satisfying C3), which probably leads to higher learning difficulty. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\nf(\\mathbf{x}_{t},\\mathbf{x}_{s}|\\mathbf{Q},\\mathbf{K},\\mathbf{A}_{t})=q_{t}\\big(\\prod_{j=s+1}^{t}\\alpha_{j}\\big)k_{s}=p_{t s},\\forall s=1,\\ldots,t,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{s.t.~}\\quad|q_{s}|\\leq C_{q},|k_{s}|\\leq C_{k},\\alpha_{s}\\in[0,1],\\forall s=1,\\ldots,t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "For bounded inputs $\\mathbf{X}$ , bounded parameters $(\\theta_{q},\\theta_{k},\\theta_{\\alpha})$ are equivalent to bounded $\\left(\\mathbf{Q},\\mathbf{K},\\Lambda_{t}\\right)$ . Afterwards we will generalize to vector version with $d_{k}>1$ and consider distribution of all time $(\\mathbf{p}_{t},t=1,\\ldots,d_{k})$ . This is done by viewing $\\mathbf{q}_{t}$ as a channel selector. ", "page_idx": 5}, {"type": "text", "text": "For the C3 condition, least parameters mean the fewest parameter groups $({\\bf Q},{\\bf K},{\\bf\\Lambda}_{t})$ when $d,d_{k},d_{v}$ are fixed. Due to space constraints, the detailed analysis in this Section is provided in appendix A2. ", "page_idx": 5}, {"type": "text", "text": "Conclusions of Optimal Linear Approximation Analysis. i) Linear approximation. The necessary conditions (C1 and C2) for LinAttMap to achieve approximation to SoftAttMap is that its implementation must include $\\mathbf{Q}$ and dynamic decay $\\mathbf{{A}}_{t}$ . Both $({\\bf Q},{\\bf K},{\\bfLambda}_{t})$ and $(\\mathbf{Q},\\mathbf{A}_{t})$ can achieve approximation. ii) Least parameter approximation. $(\\mathbf{Q},\\mathbf{A}_{t})$ has fewer parameters (i.e., $\\mathbf{K}$ is not necessary), if the model dimensions are fixed. iii) Function of dynamic decay. $\\mathbf{\\Lambda}_{}\\Lambda_{t}$ is the key to achieve dynamic memory. iv) Function of Query. $\\mathbf{Q}$ can be seen as a channel selector which selects several channels of Hadamard product of $\\mathbf{\\Lambda}_{}\\Lambda_{t}$ and $\\mathbf{K}$ to approximate attention map. ", "page_idx": 5}, {"type": "text", "text": "In Tab. 2, we review some existing linear models and judge whether they meet the necessary conditions for optimal approximation. Linear attentions can be classified into three types based on the parameter groups: i) Using $\\left(\\mathbf{Q},\\mathbf{K},\\mathbf{A}_{t}\\right)$ all together, ii) Exploiting $(\\mathbf{Q},\\mathbf{K})$ , $(\\mathbf{Q},\\mathbf{A}_{t})$ or $(\\mathbf{K},\\boldsymbol{\\Lambda}_{t})$ , iii) Employing only one of $\\mathbf{Q}$ , $\\mathbf{K}$ , $\\mathbf{\\Lambda}_{}\\Lambda_{t}$ . Considering decay can be either dynamic or fixed, here we use subscript $t$ to distinguish, i.e., ${\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}/{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}_{t}$ denote fixed/dynamic decay. According to definition 4.1, they have different degrees of deficiencies: i) Models without dynamic decay such as RetNet[14], TransNormer[12], RFA[31], cannot memorize dynamically; ii) LinRNNs such as RWKV-4[17], HGRN[20], Griffin[18] lack the selection ability brought by $\\mathbf{Q}$ and the approximation ability is poor due to the small hidden state; iii) Models with $\\mathbf{K}$ such as Mamba[16], GLA[15] have redundant parameters, which probably leads to higher learning difficulty. Thus, none of the existing linear models meet all C1/C2/C3 conditions. These analyses also inspire us that ignoring the functional approximation of softmax attention does not enable the approximation of softmax attention values. ", "page_idx": 5}, {"type": "table", "img_path": "Y8YVCOMEpz/tmp/14a14cb6f0d6b6dddc85b6bd8ea6c5988f73dd6fba6a894d65823a6bab0f1211.jpg", "table_caption": ["Table 3: From general recurrent linear form to our MetaLA. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "5 MetaLA Transformer ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Transformer is stacked by a series of Encoder/Decoder blocks. Generally, each block is composed of two modules in sequence: token mixer and channel mixer [34, 35]. Softmax attention plays the role of the token mixer. In this work, we follow the Transformer architecture as a whole but use our proposed MetaLA module as the token mixer. Due to space constraints, the architecture of MetaLA Transformer is given in detail in appendix A3. Here we focus on describing the three enhancements of MetaLA relative to the general linear attention in Sec. 3 (see Fig. 2): i) The Key matrix is not used, which is based on our theoretical analysis. ii) Self-augmentation and iii) Short convolution are two other optional techniques to further enhance the modeling ability of our model. ", "page_idx": 6}, {"type": "text", "text": "i) The Key matrix is not used. We exploit ${\\mathbf1}-\\alpha_{t}$ to replace $\\mathbf{k}_{t}$ , based on theoretical analysis in Sec. 4 and appendix A2, i.e., dynamic decay $\\Lambda_{t}$ is the key mechanism to achieve dynamic memory and static approximation, and $\\mathbf{K}$ is not required. As shown in Tab. 3, compared with Eq. (13), the main improvement is: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbf{S}_{t}^{h}=\\mathrm{diag}(\\alpha_{t}^{h})\\mathbf{S}_{t-1}^{h}+(\\mathbf{1}-\\alpha_{t}^{h})^{\\mathsf{T}}\\mathbf{v}_{t}}&{{}\\in\\mathcal{R}^{d_{k}^{\\prime}\\times d_{v}^{\\prime}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "which can be trained in a parallel form in Eq. (16). The only difference is that $\\mathbf{K}$ is replaced by $\\mathbf{B}$ and $\\mathbf{B}_{t,:}=\\mathbf{1}\\!-\\!\\alpha_{t}$ . With usage of $\\Lambda_{t}$ and Q, MetaLA can achieve linear approximation to SoftAttMap. Without usage of $\\mathbf{K}\\left(\\mathbf{W}_{K}\\right)$ , we can allocate more parameters and utilize full-rank matrix $W_{\\alpha}$ to produce dynamic decay rather than low-rank matrix used by GLA, such that we do not sacrifice expression capacity of $\\mathrm{f}_{\\alpha}$ and approximation performance of MetaLA. ", "page_idx": 6}, {"type": "image", "img_path": "Y8YVCOMEpz/tmp/9dd009852550b2361b31234fdba81dc87e538bf1e371c63e87b8e4082c12a4ed.jpg", "img_caption": ["Figure 2: Recurrent form of MetaLA. We mark all three enhancements in red. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "ii) Self-augmentation can enhance a token\u2019s attention to itself, avoiding attention dilution [12]: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbf{o}_{t}^{h}=\\mathbf{q}_{t}^{h}\\mathbf{S}_{t}^{h}+\\sigma_{\\mathrm{aug}}\\big(\\mathbf{q}_{t}^{h}\\big(\\boldsymbol{w}_{\\mathrm{aug}}^{h}\\odot(\\mathbf{1}-\\boldsymbol{\\alpha}_{t}^{h})\\big)^{\\sf T}\\mathbf{v}_{t}\\big)}&{{}\\in\\mathcal{R}^{1\\times d_{v}^{\\prime}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Without changing the hidden state $\\mathbf{S}_{t}^{h}$ in Eq. (21), the proposed self-augmentation (the second term on the right side of the equation) is only added on the output process, with a learnable parameter $w_{\\mathrm{aug}}\\in\\breve{\\mathcal{R}}^{1\\times d_{k}}$ . The proposed design has two advantages (more analysis in appendix A3.2): First, it maintains parallel computing; Second, it augments the information of current token itself and does not affect future output through inner state. ", "page_idx": 6}, {"type": "text", "text": "iii) Short convolution. An additional short convolution can be inserted before entering the MetaLA layer to enhance local interaction further, motivated by Mamba [16] and Griffin [18]. ", "page_idx": 6}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "image", "img_path": "Y8YVCOMEpz/tmp/e52d4b45f698474cd0d7109c635cc1f08e591f35779059ff2439ec8c932f3472.jpg", "img_caption": ["Figure 3: Accuracy $(\\%)$ on the synthetic MQAR task. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "We conduct a comprehensive evaluation of MetaLA to validate its ", "page_idx": 6}, {"type": "table", "img_path": "Y8YVCOMEpz/tmp/9b9c7a18cad2b6b41d0d675e4c2d7e751fdf7fdcc87bad00dd013f6e10870c55.jpg", "table_caption": ["Table 4: Performance Comparison on SuperGLUE. PS: parameter size (billion). T: tokens (billion). \u2020 means the results reported by [20]. For baselines that need to be compared, if they do not have public checkpoints, we train and test them under identical conditions with MetaLA. MetaLAa: MetaLA with tied embedding trained using 100B tokens. MetaLA $^b$ : MetaLA trained with 300B tokens. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "Y8YVCOMEpz/tmp/6de7b854abea1101be7f41972321555f01236aaf126b9a986fd0959676da1a2e.jpg", "table_caption": ["Table 5: Performance Comparison on Commonsense Reasoning. \u2021 indicates testing using opensource checkpoints. HS: HellaSwag. WG: WinoGrande. OBQA: OpenbookQA. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "capabilities as a foundation model. i) MQAR [36]. Performance on the Multi-Query Associative Recall (MQAR) task is closely linked to language modeling and can also imply the effectiveness of our theory in modeling hidden states and retrieving information. ii) Autoregressive language modeling on the Pile [37] dataset and evaluation on Common-Sense Reasoning and SuperGLUE [38] zero-shot benchmarks are conducted. iii) LRA [39]. We execute experiments on the Long Range Arena (LRA) benchmark [39] to investigate MetaLA\u2019s ability in long sequence modeling. iv) ImageNet [40]. Generalization ability in visual tasks. Due to space constraints, we put some additional experiments in appendix A5, including: v) Scalability. We extend MetaLA to a 3B parameter scale and a 300B data scale for preliminary validation. vi) Retrieval and long context abilities. We evaluated MetaLA\u2019s retrieval performance on the MAD tasks [41], and its effectiveness in handling long contexts on the Needle in a Haystack task [42]. vii) Training efficiency. We provide comparative results on training throughput and GPU memory usage across various models. Detailed experimental setup and further discussion are given in appendix A4. ", "page_idx": 7}, {"type": "text", "text": "Associative Recall. The synthetic MQAR task [36] is exploited to evaluate MetaLA\u2019s memory ability. In the task, given multiple queries, the model must recall the corresponding key-value pairs before. We follow default settings in [36] to generate datasets. Fig. 3 shows that MetaLA outperforms other linear models, which have three parameter groups (Mamba [16], GLA [15], Based [43]) or fixed decay (RWKV-4 [17]), well supporting our theoretical analysis and module design. The attention baseline achieves optimal results $\\left(>99.0\\right)$ under both conditions. The additional experiments in appendix A5 show that MetaLA outperforms Mamba on more challenging settings. ", "page_idx": 7}, {"type": "text", "text": "Language Modeling. We train two scales of MetaLA: 360M/1.4B on the Pile dataset. For baselines of 360M MetaLA, we train them from scratch aligned with our configurations. For the 1.3B MetaLA, we compare it with publicly available models [14, 15, 16, 20, 44]. We implement all the pre-train experiments with GPT-Neox [45]. The zero-shot evaluation results on SuperGLUE and Commensense Reasoning benchmarks are reported in Tab. 4 and Tab. 5. Specifically, compared to the LinRNN model HGRN [20], MetaLA expands hidden state dimensions and uses Query matrix; Compared to LinFormer model RetNet [14] with fixed decay, MetaLA uses dynamic decay; Compared to SSMs like Mamba [16] and LinFormer with dynamic decay GLA [15], MetaLA omits the Key matrix in computation. Results indicate that MetaLA has better performance than these linear models and the Transformer-based Pythia [44]. See appendix A5 for more task results. ", "page_idx": 7}, {"type": "table", "img_path": "Y8YVCOMEpz/tmp/514558418f71b4984cea47eebc693f32f21956775d98ee072dcecafe41212b41.jpg", "table_caption": ["Table 7: Performances Comparison on the Long Range Arena. We cite baselines from HGRN [20]. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "Y8YVCOMEpz/tmp/4a271cafa422621d3487c46cc4ae88f8bc188545fb5223d3af1395354916cf00.jpg", "table_caption": ["Table 8: Ablation studies. Ablation study results on the 360M model trained for 15B tokens. We compared the model variants on zero-shot experiments of the Comparison on Commonsense Reasoning benchmark. HS: HellaSwag. WG: WinoGrande. OBQA: OpenbookQA. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Long Sequence Modeling. LRA is used to evaluate the model\u2019s ability in long sequence modeling. We compare MetaLA with Transformer, linear foundation models, and models specifically designed for long sequence modeling. Tab. 7 shows that MetaLA achieves comparable results with SOTA linear models, demonstrating that our model effectively preserves the ability to model long sequences. ", "page_idx": 8}, {"type": "table", "img_path": "Y8YVCOMEpz/tmp/6a857ab1de3486c6c06f97095e52a324698ae6da43e941b8ce59e18881b1125c.jpg", "table_caption": ["Table 6: Results on ImageNet-1k. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Image Classification. We compare MetaLA with   \nDeit [49] (Transformer), HGRN [20] (LinRNN),   \nGLA [15] (LinFormer) and Mamba [16] (SSM) on ImageNet. As shown in Tab. 6, MetaLA performs better than other typical linear models at both scales of 6M and 23M. ", "page_idx": 8}, {"type": "text", "text": "Ablation Studies. We conduct ablation studies on the 360M model trained with 15B tokens and compare the results in zero-shot experiments. First, restoring the Key matrix in linear attention does not improve performance while increasing parameters, supporting our theoretical result that $\\mathbf{K}$ is not necessary for approximation, and its functional role can be replaced by dynamic decay. Second, the ablations of self-augmentation and short convolution demonstrate the effectiveness of our model design, i.e., enhance tokens\u2019 own attention and local interactions. ", "page_idx": 8}, {"type": "text", "text": "7 Conclusion and Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Conclusion. We unify LinFormer/SSM/LinRNN models into the form of linear attention with Query, Key, Vaule matrices, and then analyze whether they can achieve the optimal approximation to the softmax attention function. Theoretical analysis shows that the existing LinFormer/SSM/LinRNN cannot achieve optimal approximation. Consequently, we propose the MetaLA architecture, which can achieve functional approximation of softmax attention with least parameters. The performance on various types of tasks verifies the effectiveness of MetaLA. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Discussion and Limitation. Here, we discuss two key questions about approximation perspectives. i) How does an optimal approximation to softmax attention inspire linear attention design? In this work, we mainly remove the Key matrix, use dynamic decay, and enhance local interactions and the token\u2019s own attention. This is clearly not the end of linear attention optimization. This work focuses on functional approximation, previous studies about the value approximation [30, 31, 50] can be further investigated on the basis of our functional approximation theory as well as MetaLA architecture. Additional optimization may include improving the recall ability of limited hidden states or designing better parameter functions. ii) Does approximation to the softmax attention imply that linear attention has an upper capacity limit? Taken literally, approximation seems to imply that linear attention cannot exceed softmax attention. However, we found better results for linear attention than softmax attention in some experimental results, such as zero-shot and LRA. Similar findings were also reported in previous work [13, 15, 16]. We argue that this issue deserves further exploration. For the time being, evaluation metrics that do not adequately reflect the model\u2019s capabilities, insufficient training [51], and linear attention that does have advantages in certain abilities [15] are all possibilities. ", "page_idx": 9}, {"type": "text", "text": "8 Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was partially supported by CAS Project for Young Scientists in Basic Research (YSBR116), National Distinguished Young Scholars (62325603), National Natural Science Foundation of China (62236009, U22A20103, 62441606, 62406322), Beijing Science and Technology Plan (Z241100004224011), Beijing Natural Science Foundation for Distinguished Young Scholars (JQ21015), China Postdoctoral Science Foundation (GZB20240824, 2024M753497), and CAAIMindSpore Open Fund, developed on OpenI Community. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pages 5998\u20136008, 2017.   \n[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in Neural Information Processing Systems, 33:1877\u20131901, 2020.   \n[3] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.   \n[4] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[5] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.   \n[6] Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, et al. Neural codec language models are zero-shot text to speech synthesizers. arXiv preprint arXiv:2301.02111, 2023.   \n[7] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in Neural Information Processing Systems, 36, 2024.   \n[8] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Computing Surveys, 55(6):1\u201328, 2022.   \n[9] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, pages 5156\u20135165. PMLR, 2020.   \n[10] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The Tenth International Conference on Learning Representations, 2022.   \n[11] Eric Martin and Chris Cundy. Parallelizing linear recurrent neural nets over sequence length. In International Conference on Learning Representations, 2018.   \n[12] Zhen Qin, Xiaodong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, and Yiran Zhong. The devil in linear transformer. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 7025\u20137041, 2022.   \n[13] Zhen Qin, Dong Li, Weigao Sun, Weixuan Sun, Xuyang Shen, Xiaodong Han, Yunshen Wei, Baohong Lv, Xiao Luo, Yu Qiao, and Yiran Zhong. Transnormerllm: A faster and better large language model with improved transnormer, 2024.   \n[14] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: A successor to transformer for large language models. arXiv preprint arXiv:2307.08621, 2023.   \n[15] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. In Forty-first International Conference on Machine Learning, 2024.   \n[16] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. In First Conference on Language Modeling, 2024.   \n[17] Bo Peng, Eric Alcaide, Quentin Gregory Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, et al. Rwkv: Reinventing rnns for the transformer era. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023.   \n[18] Soham De, Samuel L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et al. Griffin: Mixing gated linear recurrences with local attention for efficient language models. arXiv preprint arXiv:2402.19427, 2024.   \n[19] A. Orvieto, S. L. Smith, A. Gu, A. Fernando, C. Gulcehre, R. Pascanu, and S. De. Resurrecting recurrent neural networks for long sequences. In International Conference on Machine Learning, pages 26670\u201326698, 2023.   \n[20] Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. In Advances in Neural Information Processing Systems, volume 36, pages 33202\u201333221, 2023.   \n[21] Daniel Y. Fu, Tri Dao, Khaled Kamal Saab, Armin W. Thomas, Atri Rudra, and Christopher Re. Hungry hungry hippos: Towards language modeling with state space models. In The Eleventh International Conference on Learning Representations, 2022.   \n[22] Jimmy TH Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations, 2022.   \n[23] Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, and Yiran Zhong. Various lengths, constant speed: Efficient language modeling with lightning attention. In Forty-first International Conference on Machine Learning, 2024.   \n[24] Albert Gu. Modeling Sequences with Structured State Spaces. Stanford University, 2023.   \n[25] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. Advances in Neural Information Processing Systems, 35:35971\u201335983, 2022.   \n[26] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. Advances in Neural Information Processing Systems, 35:22982\u201322994, 2022.   \n[27] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Hippo: Recurrent memory with optimal polynomial projections. In Advances in Neural Information Processing Systems, pages 1474\u20131487, 2020.   \n[28] Tao Lei, Yu Zhang, Sida I. Wang, Hui Dai, and Yoav Artzi. Simple recurrent units for highly parallelizable recurrence. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4470\u20134481, 2018.   \n[29] Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. HGRN2: Gated linear RNNs with state expansion. In First Conference on Language Modeling, 2024.   \n[30] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, et al. Rethinking attention with performers. In International Conference on Learning Representations, 2020.   \n[31] Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah Smith, and Lingpeng Kong. Random feature attention. In International Conference on Learning Representations, 2020.   \n[32] Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, and Yiran Zhong. cosformer: Rethinking softmax in attention. In International Conference on Learning Representations, 2022.   \n[33] David Balduzzi and Muhammad Ghifary. Strongly-typed recurrent neural networks. In International Conference on Machine Learning, pages 1292\u20131300. PMLR, 2016.   \n[34] Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, and Alexey Dosovitskiy. Mlp-mixer: An all-mlp architecture for vision. In Advances in Neural Information Processing Systems, volume 34, pages 24261\u201324272, 2021.   \n[35] Weihao Yu, Chenyang Si, Pan Zhou, Mi Luo, Yichen Zhou, Jiashi Feng, Shuicheng Yan, and Xinchao Wang. Metaformer baselines for vision. IEEE Transactions on Pattern Analysis and Machine Intelligence, 46(2):896\u2013912, 2024.   \n[36] Simran Arora, Sabri Eyuboglu, Aman Timalsina, Isys Johnson, Michael Poli, James Zou, Atri Rudra, and Christopher R\u00e9. Zoology: Measuring and improving recall in efficient language models. arXiv preprint arXiv:2312.04927, 2023.   \n[37] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An $800\\mathrm{gb}$ dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.   \n[38] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. Advances in Neural Information Processing Systems, 32, 2019.   \n[39] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers. In International Conference on Learning Representations, 2020.   \n[40] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 248\u2013255, 2009.   \n[41] Michael Poli, Armin W Thomas, Eric Nguyen, Pragaash Ponnusamy, Bj\u00f6rn Deiseroth, Kristian Kersting, Taiji Suzuki, Brian Hie, Stefano Ermon, Christopher Re, et al. Mechanistic design and scaling of hybrid architectures. In Forty-first International Conference on Machine Learning.   \n[42] Xuyang Shen, Dong Li, Ruitao Leng, Zhen Qin, Weigao Sun, and Yiran Zhong. Scaling laws for linear complexity language models. arXiv preprint arXiv:2406.16690, 2024.   \n[43] Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, Dylan Zinsley, James Zou, Atri Rudra, and Christopher R\u00e9. Simple linear attention language models balance the recall-throughput tradeoff. arXiv preprint arXiv:2402.18668, 2024.   \n[44] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad Aflah Khan, et al. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pages 2397\u20132430. PMLR, 2023.   \n[45] Alex Andonian, Quentin Anthony, Stella Biderman, Sid Black, Preetham Gali, Leo Gao, Eric Hallahan, Josh Levy-Kramer, Connor Leahy, Lucas Nestler, Kip Parker, Michael Pieler, Jason Phang, Shivanshu Purohit, Hailey Schoelkopf, Dashiell Stander, Tri Songz, Curt Tigges, Benjamin Th\u00e9rien, Phil Wang, and Samuel Weinbach. GPT-NeoX: Large Scale Autoregressive Language Modeling in PyTorch. https://www.github.com/eleutherai/gpt-neox, 2023.   \n[46] Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai, Lingpeng Kong, and Yiran Zhong. Toeplitz neural network for sequence modeling. In The Eleventh International Conference on Learning Representations, 2022.   \n[47] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer. Mega: Moving average equipped gated attention. In The Eleventh International Conference on Learning Representations, 2022.   \n[48] Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. What makes convolutional models great on long sequence modeling? In The Eleventh International Conference on Learning Representations, 2022.   \n[49] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers & distillation through attention. In International Conference on Machine Learning, volume 139, pages 10347\u201310357, 2021.   \n[50] Michael Zhang, Kush Bhatia, Hermann Kumbong, and Christopher Re. The hedgehog & the porcupine: Expressive linear attentions with softmax mimicry. In The Twelfth International Conference on Learning Representations, 2023.   \n[51] Ido Amos, Jonathan Berant, and Ankit Gupta. Never train from scratch: Fair comparison of long-sequence models requires data-driven priors. In The Twelfth International Conference on Learning Representations, 2024.   \n[52] Jungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu Chen, and Noah A Smith. Finetuning pretrained transformers into rnns. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10630\u201310643, 2021.   \n[53] Krzysztof Marcin Choromanski, Han Lin, Haoxian Chen, Arijit Sehanobish, Yuanzhe Ma, Deepali Jain, Jake Varley, Andy Zeng, Michael S Ryoo, Valerii Likhosherstov, et al. Hybrid random features. In International Conference on Learning Representations, 2021.   \n[54] Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas Singh. Nystr\u00f6mformer: A nystr\u00f6m-based algorithm for approximating self-attention. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 14138\u201314148, 2021.   \n[55] Imanol Schlag, Kazuki Irie, and J\u00fcrgen Schmidhuber. Linear transformers are secretly fast weight programmers. In International Conference on Machine Learning, pages 9355\u20139366. PMLR, 2021.   \n[56] Wolfgang Maass. Networks of spiking neurons: the third generation of neural network models. Neural Networks, 10(9):1659\u20131671, 1997.   \n[57] Man Yao, Guangshe Zhao, Hengyu Zhang, Yifan Hu, Lei Deng, Yonghong Tian, Bo Xu, and Guoqi Li. Attention spiking neural networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(8):9393\u20139410, 2023.   \n[58] Man Yao, Ole Richter, Guangshe Zhao, Ning Qiao, Yannan Xing, Dingheng Wang, Tianxiang Hu, Wei Fang, Tugba Demirci, Michele De Marchi, et al. Spike-based dynamic computing with asynchronous sensing-computing neuromorphic chip. Nature Communications, 15(1):4464, 2024.   \n[59] Guoqi Li, Lei Deng, Huajin Tang, Gang Pan, Yonghong Tian, Kaushik Roy, and Wolfgang Maass. Brain-inspired computing: A systematic survey and future trends. Proceedings of the IEEE, 112(6):544\u2013584, 2024.   \n[60] Man Yao, Jiakui Hu, Zhaokun Zhou, Li Yuan, Yonghong Tian, Bo Xu, and Guoqi Li. Spikedriven transformer. Advances in Neural Information Processing Systems, 36, 2024.   \n[61] Man Yao, JiaKui Hu, Tianxiang Hu, Yifan Xu, Zhaokun Zhou, Yonghong Tian, Bo XU, and Guoqi Li. Spike-driven transformer v2: Meta spiking neural network architecture inspiring the design of next-generation neuromorphic chips. In The Twelfth International Conference on Learning Representations, 2024.   \n[62] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. Transformer quality in linear time. In International Conference on Machine Learning, pages 9099\u20139117. PMLR, 2022.   \n[63] Huanru Henry Mao. Fine-tuning pre-trained transformers into decaying fast weights. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 10236\u201310242, 2022.   \n[64] Tobias Katsch. Gateloop: Fully data-controlled linear recurrence for sequence modeling. arXiv preprint arXiv:2311.01927, 2023.   \n[65] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34:572\u2013585, 2021.   \n[66] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. In International Conference on Learning Representations, 2023.   \n[67] Junxiong Wang, Jing Yan, Albert Gu, and Alexander M Rush. Pretraining without attention. In Findings of the Association for Computational Linguistics: EMNLP, pages 58\u201369, 2023.   \n[68] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models. In International Conference on Machine Learning, pages 28043\u201328078. PMLR, 2023.   \n[69] Eric Nguyen, Karan Goel, Albert Gu, Gordon Downs, Preey Shah, Tri Dao, Stephen Baccus, and Christopher R\u00e9. S4nd: Modeling images and videos as multidimensional signals with state spaces. Advances in Neural Information Processing Systems, 35:2846\u20132861, 2022.   \n[70] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. In The Eleventh International Conference on Learning Representations, 2022.   \n[71] Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. In NIPS Workshop on Deep Learning, 2014.   \n[72] A. Graves and A. Graves. Long short-term memory. In Supervised Sequence Labelling with Recurrent Neural Networks, pages 37\u201345. Springer, 2012.   \n[73] Felix A Gers, J\u00fcrgen Schmidhuber, and Fred Cummins. Learning to forget: Continual prediction with lstm. Neural Computation, 12(10):2451\u20132471, 2000.   \n[74] Shiva Kaul. Linear dynamical systems as a core computational primitive. Advances in Neural Information Processing Systems, 33:16808\u201316820, 2020.   \n[75] Antonio Orvieto, Soham De, Caglar Gulcehre, Razvan Pascanu, and Samuel L Smith. On the universality of linear recurrences followed by nonlinear projections. arXiv preprint arXiv:2307.11888, 2023.   \n[76] James Bradbury, Stephen Merity, Caiming Xiong, and Richard Socher. Quasi-recurrent neural networks. In International Conference on Learning Representations, 2016.   \n[77] Shuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang, and Josh Susskind. An attention free transformer. arXiv preprint arXiv:2105.14103, 2021.   \n[78] Lin Zheng, Jianbo Yuan, Chong Wang, and Lingpeng Kong. Efficient attention via control variates. In International Conference on Learning Representations, 2023.   \n[79] Songlin Yang and Yu Zhang. Fla: A triton-based library for hardware-efficient implementations of linear attention mechanism. https://github.com/sustcsonglin/ flash-linear-attention, 2024.   \n[80] Denis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern\u00e1ndez. The lambada dataset: Word prediction requiring a broad discourse context. arXiv preprint arXiv:1606.06031, 2016.   \n[81] Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. Logiqa: a challenge dataset for machine reading comprehension with logical reasoning. In Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence, pages 3622\u20133628, 2021.   \n[82] Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning, 2012.   \n[83] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2924\u20132936, 2019.   \n[84] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 7432\u20137439, 2020.   \n[85] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 2019.   \n[86] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99\u2013106, 2021.   \n[87] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.   \n[88] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2381\u20132391, 2018.   \n[89] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPof,i Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, et al. A framework for few-shot language model evaluation. Version v0. 0.1. Sept, page 8, 2021.   \n[90] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344\u201316359, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A1 Related Work ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We overview several architectures of three types of linear models, i.e., LinFormer, SSM, and linRNN, and show how they can be specialized from the general form in Sec. 3. ", "page_idx": 15}, {"type": "text", "text": "A1.1 LinFormer ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "LinFormer abandons the softmax form of vanilla attention, instead leveraging the dot product between Keys and Values to achieve linear complexity [9]. Advances in LinFormer mainly focus on designing better kernel function [30, 32, 52, 53, 54, 55, 50] such as the spiking neurons in a spiking neural network[56, 57, 58, 59] can be understood as efficient kernel functions[60, 61], exploring architectural optimization [13, 14, 62], introducing gating mechanism [15, 31, 63, 64], etc. We show the general form of several LinFormer architectures in Tab. A1. For the general form of LinFormers, $d_{k}=d_{v}=d$ is the most common choice. ", "page_idx": 15}, {"type": "text", "text": "\u2022 Vanilla LinFormer [9]: Vanilla linear Transformers lack explicit decay. Instead, They choose to maintain two hidden states containing a denominator to normalize attention scores. As for their general form, $\\mathbf{q}_{\\underline{{t}}}{\\slash\\!\\!\\!k}_{\\!\\!t}\\,=\\,\\mathrm{f}_{q/k}\\bigl(\\mathbf{x}_{t},{\\theta}_{q/k}\\bigr)\\,=\\,\\phi\\bigl(\\mathbf{x}_{t}{W}_{Q/K}\\bigr)$ where $\\theta_{q/k}\\,=\\,W_{Q/K}$ and $\\phi$ is the nonlinearity. ${\\bf v}_{t}={\\bf x}_{t}{\\bf W}_{V}$ . There is no decay or output gate, i.e., $\\mathbf{g}_{t}=\\mathbf{1}$ and $\\alpha_{t}=1$ . ", "page_idx": 15}, {"type": "text", "text": "\u2022 RetNet [14]: Different from vanilla LinFormer, RetNet uses positional encoding $\\exp\\left(j n\\theta\\right)$ and fixed decay $\\lambda$ to control the hidden state. For its general form, $\\mathbf{q}_{t}/\\mathbf{k}_{t}/\\mathbf{v}_{t}=\\mathrm{f}_{\\mathbf{\\Phi}_{q/k/v}}(\\mathbf{x}_{t},\\theta_{q/k/v})=$ $\\mathbf{x}_{t}\\mathbf{W}_{Q/K/V}$ where $\\theta_{q/k/v}\\,=\\,W_{Q/K/V}$ . $\\alpha_{t}\\,=\\,\\lambda\\exp\\left(j\\theta\\right)$ is the fixed decay vector, where $j$ is imaginary unit. Furthermore, $\\mathbf{y}_{t}=[\\mathbf{o}_{t}\\odot\\mathrm{SiLU}(\\mathbf{x}_{t}W_{G})]W_{O}$ where $\\theta_{g}=W_{G}$ . ", "page_idx": 15}, {"type": "text", "text": "\u2022 TransNormer [12]: TransNormer also uses positional encoding $\\exp\\left(j n\\theta\\right)$ and fixed decay $\\lambda$ to control the hidden state. For its general form, $\\mathbf{q}_{t}/\\mathbf{k}_{t}\\,=\\,\\mathrm{f}_{q/k}\\bigl(\\mathbf{x}_{t},{\\boldsymbol{\\theta}}_{q/k}\\bigr)\\,=\\,\\phi\\bigl(\\mathbf{x}_{t}{\\boldsymbol{W}}_{Q/K}\\bigr)$ where $\\theta_{q/k}\\,=\\,W_{Q/K}$ and $\\phi$ is the nonlinearity. ${\\bf v}_{t}\\,=\\,{\\bf x}_{t}W_{V}$ , and $\\alpha_{t}\\,=\\,\\lambda\\exp\\left(j\\theta\\right)$ is the fixed decay vector, where $j$ is imaginary unit. The normalization layer chosen by the paper is SRMSNorm. Furthermore, $\\dot{\\mathbf{y}}_{t}=[\\mathbf{o}_{t}\\odot(\\dot{\\mathbf{x}_{t}}\\mathbf{W}_{U})]\\mathbf{W}_{O}$ where $\\theta_{g}=W_{U}$ . ", "page_idx": 15}, {"type": "text", "text": "\u2022 GLA [15]: GLA considers a data-dependent gating mechanism for LinFormer. For its general form, $\\mathbf{q}_{t}/\\mathbf{k}_{t}/\\mathbf{v}_{t}\\ =\\ \\mathrm{f}_{{q}/{k}/{v}}\\big(\\mathbf{x}_{t},\\theta_{q/k/v}\\big)\\ =\\ \\dot{\\mathbf{x}_{t}}\\dot{\\mathbf{W}_{Q/K/V}}$ where $\\theta_{q/k/v}\\;=\\;{\\cal W}_{Q/K/V}$ . $\\alpha_{t}=$ sigmoid $\\displaystyle(\\mathbf{x}_{t}W_{\\alpha}^{1}W_{\\alpha}^{2}+b_{\\alpha})^{1/\\tau}$ is a dynamic decay, where $W_{\\alpha}^{1}$ and $W_{\\alpha}^{2}$ are low-rank matrices. Furthermore, $\\mathbf{y}_{t}=[\\mathbf{o}_{t}\\odot\\mathrm{SiLU}(\\mathbf{x}_{t}W_{r}+b_{r})]W_{O}$ where $\\theta_{g}=[\\boldsymbol{W}_{r},\\boldsymbol{b}_{r}]$ . ", "page_idx": 15}, {"type": "table", "img_path": "Y8YVCOMEpz/tmp/ac1472e02557730bd318797a0651e5d1d4cd7af7d0042cb284fbc779b90f7dcd.jpg", "table_caption": ["Table A1: From general form to LinFormers. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "A1.2 SSM ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "SSM represents an alternative linear architecture to Transformer, based on the state space equations [24, 65]. Typical SSMs [25, 26, 66, 67, 68, 69] employ structured transition matrix and special initialization strategies such as HiPPO [27] to efficiently enhance long-range dependencies. Mamba [16] advances SSMs by introducing selection mechanism, i.e. input-dependent parameters [70] and designs a hardware-aware parallel algorithm. We show the general form of several SSM architectures in Table A2. All of them are SISO (Single-Input, Single-Output) except S5 [22]. We omit original S4 [10] model because it focuses on Diagonal Plus Low-Rank (DPLR) transition matrix ", "page_idx": 15}, {"type": "text", "text": "$\\pmb{A}$ rather than fully diagonal one, which is difficult to implement due to computational inefficiency.   \nFor most SSMs, $d_{v}=H$ , which means each channel uses independent parameters $(\\mathbf{q}_{t}^{h},\\alpha_{t}^{h},\\mathbf{k}_{t}^{h})$ . ", "page_idx": 16}, {"type": "text", "text": "\u2022 DSS [26]: DSS is the first research to show the effectiveness of diagonal structured SSM with fixed parameters, which means all parameters are constant through time. For the general form of DSS, $d_{v}=d=H$ , $\\begin{array}{r}{{\\frac{d{_k}}{H}}=N}\\end{array}$ , where $N$ is an expansion. $\\mathbf{Q},\\mathbf{K},\\mathbf{A}$ are independent of $\\mathbf{x}_{t}$ and are determined by learnable parameters $[A,B,C,\\Delta]$ , using ZOH method to discretize. Furthermore, $\\mathbf{x}_{t}\\,=\\,\\mathbf{v}_{t}$ , $\\mathbf{g}_{t}=\\mathbf{1}$ and $W_{O}=\\mathbf{I}$ , where I denotes the identity matrix. ", "page_idx": 16}, {"type": "text", "text": "\u2022 S4D [25]: S4D is also a diagonal structured SSM with fixed parameters. It theoretically explain and expand the effectiveness of DSS and focus on parameterization and initialization of diagonal SSM. It can be discretized using ZOH or Bilinear method, resulting different $\\mathrm{f}_{k/\\alpha}$ compared with DSS. Similarly, $\\mathbf{x}_{t}=\\mathbf{v}_{t}$ , $\\mathbf{g}_{t}=\\mathbf{1}$ and $W_{O}=\\mathbf{I}$ . ", "page_idx": 16}, {"type": "text", "text": "\u2022 H3 [21]: H3 combines S4D with LinFormer and adds some architectural optimization such as local convolution and output gates. The core layer of H3 is S4D, and thus it shares the same general form with S4D. ", "page_idx": 16}, {"type": "text", "text": "\u2022 S5 [22]: S5 uses MIMO (Multi-Input, Multi-Output) SSM with fixed parameters. For its general form, $\\begin{array}{r}{d_{v}=N d=H,\\frac{d_{k}}{H}=1}\\end{array}$ , where $N$ is an expansion much smaller than that of SISO models. $\\alpha_{t}=$ $\\exp\\left(\\Delta A\\right)$ is the fixed decay and $\\mathbf{v}_{t}=\\mathrm{f}_{v}\\big(\\mathbf{x}_{t},\\theta_{v}\\big)=\\mathbf{x}_{t}\\overline{{B}}$ where $\\overline{{B}}=(\\Delta A)^{-1}(\\exp{(\\Delta A)}-\\mathbf{I})\\cdot\\Delta B$ , i.e., it chooses ZOH method. Similar to previous SSM models, all the parameters are independent of $\\mathbf{x}_{t}$ and are determined by learnable parameters $[A,B,C,\\Delta]$ . Furthermore, $\\mathbf{q}_{t}=\\mathbf{k}_{t}=\\mathbf{g}_{t}=\\mathbf{1}$ and $W_{O}=C$ . ", "page_idx": 16}, {"type": "text", "text": "\u2022 Mamba [16]: Mamba advances SSMs with selection, which means all parameters are time-varying and input-dependent. For the general form of Mamba, $d_{v}=d=H$ , $\\begin{array}{r}{\\frac{\\Bar{d_{k}}}{H}=N}\\end{array}$ . $\\mathbf{q}_{t}=C_{t}=\\mathbf{x}_{t}W_{C}$ . Similarly, $\\mathbf{k}_{t}=B_{t}=\\mathbf{x}_{t}W_{B}\\Delta_{t}$ . The decay term can be written as $\\mathbf{\\bar{\\alpha}}_{\\alpha_{t}}=\\exp(\\Delta_{t}\\pmb{A})$ . $\\mathbf{x}_{t}=\\mathbf{v}_{t}$ , $\\mathbf{g}_{t}=\\mathbf{1}$ and $W_{O}=\\mathbf{I}$ . Furthermore, $\\Delta_{t}=\\mathrm{softplus}(\\mathbf{x}_{t}W_{l o r a}+\\pmb{b}_{\\Delta})$ . ", "page_idx": 16}, {"type": "table", "img_path": "Y8YVCOMEpz/tmp/c67f8b00f341b4902d8e4930ae1eeb032de866d00da0fed08d30c8d60f908fd6.jpg", "table_caption": ["Table A2: From general form to SSMs. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "A1.3 LinRNN ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "LinRNN is a variant of the vanilla Recurrent Neural Network (RNN) model [71, 72, 73] that eliminates the nonlinearity within the recurrence and employs element-wise product instead of matrix multiplication [11, 28, 74, 75]. Recent works have made efforts to explore more effective recurrence [17, 19, 76, 77] and design more advanced gating mechanisms [18, 20, 33]. We show the general form of several LinRNN architectures in Tab. A3. LinRNNs generally implement $d_{k}=d_{v}=H=d$ except LRU [19], which means each channel uses independent parameters $(\\mathbf{q}_{t}^{h},\\alpha_{t}^{h},\\mathbf{k}_{t}^{h})$ . They maintain a smaller unexpanded hidden state, which is a 1D vector and can be seen as a special case with head dimension size $d_{k}^{\\prime}=1$ . ", "page_idx": 16}, {"type": "text", "text": "\u2022 RWKV-4 [17]: RWKV-4 is a linear RNN model which includes fixed decay and output gates. Moreover, RWKV-4 treats the present token differently and uses token shift operation (we omit for clear illustration). For its general form, $\\mathbf{k}_{t}=\\exp\\left(\\mathbf{x}_{t}\\pmb{W}_{K}\\right)$ where $\\theta_{k}\\,=\\,W_{K}$ . $\\mathbf{v}_{t}=\\mathbf{x}_{t}\\mathbf{W}_{V}$ while $\\mathbf q_{t}=\\mathbf1$ . $\\alpha_{t}=\\exp\\left(-W\\right)$ is fixed decay with the learnable parameter $\\theta_{\\alpha}=W$ . Furthermore, $\\mathbf{y}_{t}=[\\mathbf{o}_{t}\\odot\\mathrm{sigmoid}(\\mathbf{x}_{t}W_{r})]W_{O}$ where $\\theta_{g}=W_{r}$ . ", "page_idx": 16}, {"type": "text", "text": "\u2022 GLRU [20]: Gated linear recurrent unit (GLRU) mentioned in HGRN is a linear RNN model including input/forget/output gates. For the general form of GLRU, $\\alpha_{t}/\\mathbf{k}_{t}\\,=\\,\\mathrm{f}_{\\alpha/k}\\bigl(\\mathbf{x}_{t},\\theta_{\\alpha/k}\\bigr)\\,=$ $\\sigma(\\mathbf{x}_{t}\\mathbf{W}_{\\alpha/K}\\,+\\,b_{\\alpha/K})$ where $\\theta_{\\alpha/k}~=~[W_{\\alpha/K},b_{\\alpha/K}]$ and $\\sigma$ is the sigmoid function. $\\mathbf{v}_{t}=$ $\\mathrm{SiLU}({\\bf x}_{t}{\\cal W}_{V}+{\\pmb b}_{V})$ while $\\mathbf q_{t}\\ =\\ 1$ . Furthermore, $\\mathbf{y}_{t}\\,=\\,[\\mathbf{o}_{t}\\odot\\mathrm{SiLU}(\\mathbf{x}_{t}W_{g}+b_{g})]W_{O}$ where $\\theta_{g}=[{\\pmb W}_{g},{\\pmb b}_{g}]$ . ", "page_idx": 17}, {"type": "text", "text": "\u2022 Griffin [18]: Griffin is an RNN model with gated linear recurrence. Moreover, it ties dynamic decay and Key as we do. For the general form of Griffin, $\\alpha_{t}\\,=\\,\\mathrm{f}_{\\alpha}(\\mathbf{x}_{t},\\theta_{\\alpha})\\,=\\,A^{c\\cdot\\sigma(\\mathbf{x}_{t}\\dot{\\mathbf{W}}_{a}+\\pmb{b}_{a})}$ where $\\theta_{\\alpha}=[A,W_{a},b_{a}]$ . Then $\\mathbf{k}_{t}=\\sqrt{1-\\alpha_{t}^{2}}$ to replace Key\u2019s role. $\\mathbf{v}_{t}=\\sigma(\\mathbf{x}_{t}W_{x}+b_{x})\\odot\\mathbf{x}_{t}$ which contains an input gate additionally. $\\mathbf q_{t}=\\mathbf1$ and $\\mathbf{y}_{t}=[\\pmb{\\ o}_{t}\\odot\\operatorname{GeLU}(\\mathbf{x}_{t}\\pmb{W}_{g}+\\pmb{b}_{g})]\\pmb{W}_{O}$ where $\\theta_{g}=[{\\pmb W}_{g},{\\pmb b}_{g}]$ . ", "page_idx": 17}, {"type": "text", "text": "\u2022 LRU [19]: LRU is a linear and diagonal RNN model with fixed decay, inspired by deep SSMs\u2019 success. For its general form, $d_{v}=N d=H$ , $\\begin{array}{r}{\\frac{d_{k}}{H}=1}\\end{array}$ , where $N$ is a small expansion. $\\alpha_{t}=A$ is the fixed decay and $\\mathbf{v}_{t}=\\mathrm{f}_{v}\\big(\\mathbf{x}_{t},\\theta_{v}\\big)=\\mathbf{x}_{t}B$ where $\\theta_{v}=B$ . Furthermore, $\\mathbf{q}_{t}=\\mathbf{k}_{t}=\\mathbf{g}_{t}=\\mathbf{1}$ and $W_{O}=C$ . ", "page_idx": 17}, {"type": "table", "img_path": "Y8YVCOMEpz/tmp/2c5e2684c335dba0ec06f8f3f8c994519ab3629e2b2c857b7ec514f4f1cfc60a.jpg", "table_caption": ["Table A3: From general form to LinRNN. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "A1.4 Approximation to Softmax Attention ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Approximation to softmax attention includes two parts: value and functional approximation. Some previous works of LinFormer [31, 30, 53, 78] were devoted to designing better kernel map $\\phi$ to approximate the value of SoftAttMap via randomized features, i.e., $\\phi(\\mathbf{q}_{t}^{\\mathsf{\\Gamma}})\\phi^{\\top}(\\mathbf{k}_{s})\\approx\\exp(\\mathbf{\\dot{q}}_{t}\\mathbf{\\dot{k}}_{s}^{\\top})$ . Recent work [50] further introduces attention weight distillation loss to minimize the cross-entropy loss between the computed linear attention weights and those that would have been computed via softmax attention, in order to achieve strict value approximation. Most of them follow typical designs in [9] and use only $\\mathbf{Q}$ and $\\mathbf{K}$ . However, in this work, we find that only considering value approximation is insufficient. Different from previous studies, we investigate functional approximation of SoftAttMap. Through theoretical analysis, we show that these works cannot achieve the functionality of softmax attention due to lack of dynamic decay $\\mathbf{\\Lambda}_{}\\Lambda_{t}$ . Therefore, their purpose of achieving value approximation cannot be achieved as well. It is worth noting that this is not a denial of the significance of previous works about value approximation. We can also get inspiration from them. Functional approximation is the basis and prerequisite for value approximation. ", "page_idx": 17}, {"type": "text", "text": "A2 Analysis of Optimal Linear Approximation Conditions ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we show that existing linear models are not optimal linear approximation to softmax attention map, according to definition 4.1, and prove that linear models with only Query and dynamic decay can satisfy the optimal linear approximation conditions. In definition 4.1, C0 underlines computational and memory efficiency. C1 and C2 consider memory and modeling ability of linear attention. C3 is our expectation to seek the least parameters on the premise that previous three conditions are met. We first discuss the requirements to satisfy each conditions, based on the general form of linear attention in Sec. 3. All the linear models own linear complexity and satisfy C0, so we analyze C1, C2 and C3 in appendix A2.1,appendix A2.2 and appendix A2.3, respectively. Then we synthesize all the discussions and present the conclusions in appendix A2.4. ", "page_idx": 17}, {"type": "text", "text": "For notation we use $\\mathbf{Q},\\mathbf{K},\\mathbf{V},\\mathbf{A}_{t}$ to denote Query, Key, Value and Decay matrices respectively. Considering decay can be either dynamic or fixed, here we use subscript $t$ to distinguish, i.e., ${\\mathbf{}}{\\mathbf{}}{\\mathbf{}\\Lambda}/{\\mathbf{}}{\\mathbf{}\\Lambda}_{t}$ denote fixed/dynamic decay. Corresponding $\\mathbf{q}_{t},\\mathbf{k}_{t},\\mathbf{v}_{t},\\alpha/\\alpha_{t}$ are query, key, value and fixed/dynamic decay of each timestep. ", "page_idx": 18}, {"type": "text", "text": "A2.1 Analysis of Dynamic Memory Ability (C1) ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proposition A2.1. Only models with dynamic decay can satisfy $c\\pmb{I}$ (Dynamic memory ability). Let $\\mathbf{S}_{t}\\in\\mathcal{R}^{d_{k}\\times d_{v}}$ be hidden state of general linear attention (see Sec. 3). $A t$ a time $t_{;}$ , the information about $\\mathbf{v}_{t_{1}},\\dots,\\mathbf{v}_{t_{d_{k}}}$ is successfully stored in $\\mathfrak{I}_{t}\\left(t_{1},\\ldots,t_{d_{k}}\\leq t\\right)$ , which means $(\\mathbf{S}_{t})_{i,:}=\\mathbf{v}_{t_{i}}$ . When the new important input $\\mathbf{v}_{t+1}$ arrives, only models with dynamic decay can substitute historical unimportant $\\mathbf{v}_{t_{1}}$ by $\\mathbf{v}_{t+1}$ successfully, i.e., obtain $(\\mathbf{S}_{t+1})_{1,:}=\\mathbf{v}_{t+1}$ . ", "page_idx": 18}, {"type": "text", "text": "Proof. Without loss of generality, suppose $t\\,=\\,d_{k}$ and $t_{i}\\,=\\,i(i\\,=\\,1,\\ldots,d_{k})$ , where $d_{k}$ is Key dimension. That is, the information about $\\mathbf{v}_{1},\\ldots,\\mathbf{v}_{d_{k}}$ is successfully stored, which means the $i$ -th row of $\\mathbf{S}_{d_{k}}$ , $(\\mathbf{S}_{d_{k}})_{i,:}=\\mathbf{v}_{i}$ . Suppose $\\mathbf{v}_{1}$ is unimportant information need to be substituted. Now the new and important input $\\mathbf{v}_{d_{k}+1}$ arrives, models with general form Eq. (13) update $\\mathbf{S}_{d_{k}}$ as follows (heads are omitted for simplicity): ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbf{S}_{d_{k}+1}=\\operatorname{diag}(\\alpha_{d_{k}+1})\\mathbf{S}_{d_{k}}+(\\mathbf{k}_{d_{k}+1})^{\\mathsf{T}}\\mathbf{v}_{d_{k}+1}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We will check whether the model can obtain $(\\mathbf{S}_{d_{k}+1})_{1,:}\\,=\\,\\mathbf{v}_{d_{k}+1}$ , with fixed/dynamic decay or without decay. In the following discussion we let $\\mathbf{k}_{d_{k}+1}=\\mathbf{1}$ . ", "page_idx": 18}, {"type": "text", "text": "i) Models with fixed decay, which means $\\alpha_{d_{k}+1}=\\alpha\\in(0,1)$ . The model can update and obtain $(\\mathbf{S}_{d_{k}+1})_{1,:}\\,=\\,\\alpha\\mathbf{v}_{1}+\\mathbf{v}_{d_{k}+1}$ . This means the model with fixed decay can only store most recent several tokens rather than store most important tokens without information loss, i.e., it has ability to forget but no ability to forget and memorize dynamically. ", "page_idx": 18}, {"type": "text", "text": "ii) Models without decay, which means $\\alpha_{d_{k}+1}=1$ . This case results in $(\\mathbf{S}_{d_{k}+1})_{1,:}=\\mathbf{v}_{1}+\\mathbf{v}_{d_{k}+1}$ , which means they have no mechanism to erase old information. So such linear models have no ability to forget and memorize dynamically, and what they can only do is information blending. Thus the attention at time $t$ is distracted by all the information before, making it hard to focus on and approximate most important ones. After normalization, the attention distribution will have a relative high entropy along time dimension, which is often referred as attention dilution problem. ", "page_idx": 18}, {"type": "text", "text": "iii) Models with dynamic decay, which means $\\alpha_{d_{k}+1}\\,\\in\\,[0,1]$ . The model can easily erase and substitute information. Setting $\\alpha_{d_{k}+1}=[0,1,\\cdots\\,,1]$ can obtain $(\\mathbf{S}_{d_{k}+1})_{1,:}=\\mathbf{v}_{d_{k}+1}$ . So such linear models have ability to memorize and forget dynamically, making historical unimportant information have few effect on new one. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "A2.2 Analysis of Static Approximation Ability (C2) ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Eq. (18) illustrates the LinAttMap only relate to $\\mathbf{q}_{t}\\;=\\;\\mathrm{f}_{q}(\\mathbf{x}_{t},\\theta_{q}),\\mathbf{k}_{t}\\;=\\;\\mathrm{f}_{k}(\\mathbf{x}_{t},\\theta_{k})$ and $\\alpha_{t}=$ $\\mathrm{f}_{\\alpha}(\\mathbf{x}_{t},\\theta_{\\alpha})$ . Assuming the inputs are good enough and the functions $(\\mathrm{f}_{q},\\mathrm{f}_{k},\\mathrm{f}_{\\alpha})$ are expressive enough, we can shift from solving $(\\theta_{q},\\theta_{k},\\theta_{\\alpha})$ to solving $\\left(\\mathbf{Q},\\mathbf{K},\\mathbf{A}_{t}\\right)$ . Based on definition 4.1, we focus on approximating the attention scores between stored tokens $\\mathbf{x}_{t_{1}},\\ldots,\\mathbf{x}_{t_{d_{k}}}$ . ", "page_idx": 18}, {"type": "text", "text": "Proposition A2.2. Only models with parameters $({\\bf Q},{\\bf K},{\\bfLambda}_{t}),$ , $(\\mathbf{Q},\\mathbf{K})$ or $(\\mathbf{Q},\\mathbf{A}_{t})$ can satisfy $^{C2}$ (Static approximation ability). For an arbitrarily given softmax attention map $\\mathbf{P}$ with scores $p_{t s}$ , only above models can ensure Eq. (A2) and Eq. (A3) hold with bounded parameters. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{f({\\bf x}_{t},{\\bf x}_{s}|\\mathbf{Q},{\\bf K},{\\bf A}_{t})={\\bf q}_{t}\\cdot\\Big((\\prod_{j=s+1}^{t}\\alpha_{j})\\odot{\\bf k}_{s}\\Big)^{\\top}=p_{t s},\\forall s\\leq t=t_{1},\\cdots,t_{d_{k}},}\\\\ {\\mathrm{~s.t.~}\\ \\ |{\\bf q}_{t}||\\leq C_{q},||{\\bf k}_{t}||\\leq C_{k},||\\alpha_{t}||\\in[0,1],\\forall t=t_{1},\\cdots,t_{d_{k}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $C_{q},C_{k}$ are constant and $||\\cdot||$ denotes vector norm. In our general form in Sec. 3, $f({\\bf x}_{t},{\\bf x}_{s}|\\mathbf{\\dot{Q}},{\\bf K},{\\bf\\Lambda}_{t})\\;=\\;\\mathrm{LinAttMap}\\left({\\bf Q},{\\bf K}\\right)_{t,s}$ (see Eq. (18)). For bounded inputs $\\mathbf{X}$ , bounded parameters $(\\theta_{q},\\theta_{k},\\theta_{\\alpha})$ are equivalent to bounded $\\left(\\mathbf{Q},\\mathbf{K},\\mathbf{\\Lambda}\\right)$ . ", "page_idx": 18}, {"type": "text", "text": "Proof. Without loss of generality, suppose $t\\,=\\,d_{k}$ and $t_{i}\\,=\\,i(i\\,=\\,1,\\ldots,d_{k})$ , where $d_{k}$ is Key dimension. That is, we need to prove following equations hold (heads are omitted for simplicity): ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\mathbf{x}_{t},\\mathbf{x}_{s}|\\mathbf{Q},\\mathbf{K},\\mathbf{A}_{t})=\\mathbf{q}_{t}\\cdot\\Big((\\prod_{j=s+1}^{t}\\alpha_{j})\\odot\\mathbf{k}_{s}\\Big)^{\\top}=p_{t s},\\forall s\\leq t=1,\\cdots,d_{k},}\\\\ &{\\mathrm{s.t.}\\quad||\\mathbf{q}_{t}||\\leq C_{q},||\\mathbf{k}_{t}||\\leq C_{k},||\\alpha_{t}||\\in[0,1],\\forall t=1,\\cdots,d_{k},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Simple case. We first simplify the problem via i) setting dimension size of $\\mathbf{Q}$ , i.e. $d_{k}\\,=\\,1\\$ and ii) considering only a given time $t$ $(1\\ \\leq\\ t\\ \\leq\\ d_{k})$ and its autoregressive attention distribution $\\mathbf{p}_{t}\\,=\\,[p_{t s},s\\,\\,\\bar{=}\\,\\,1,\\dots,\\bar{t}\\,]\\,\\in\\,\\mathcal{R}^{1\\times t}$ . We need to prove the following equations hold with bounded parameters, as a foundation conclusion: ", "page_idx": 19}, {"type": "equation", "text": "$$\nf(\\mathbf{x}_{t},\\mathbf{x}_{s}|\\mathbf{Q},\\mathbf{K},\\mathbf{A}_{t})=q_{t}\\big(\\prod_{j=s+1}^{t}\\alpha_{j}\\big)k_{s}=p_{t s},\\forall s=1,\\ldots,t,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{s.t.~}\\quad|q_{s}|\\leq C_{q},|k_{s}|\\leq C_{k},\\alpha_{s}\\in[0,1],\\forall s=1,\\ldots,t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We discuss models with fixed/dynamic decay or without decay. For cases with dynamic decay, we further consider using dynamic decay to replace key. Query\u2019s role is discussed later. ", "page_idx": 19}, {"type": "text", "text": "i) Models with fixed decay $\\alpha$ result in unbounded parameters. We can solve Eq. (A6) and derive: ", "page_idx": 19}, {"type": "equation", "text": "$$\n|k_{s}|=\\frac{p_{t s}}{\\alpha^{t-s}|q_{t}|}\\geq\\frac{p_{t s}}{C_{q}}\\cdot\\frac{1}{\\alpha^{t-s}},\\forall s=1,\\ldots,t.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Because $\\alpha\\in(0,1)$ , the last term $\\frac{1}{\\alpha^{t-s}}$ is unbounded, which leads to unbounded $k_{s}$ . Hence Eq. (A7) can not be satisfied. The result is intuitive because the fixed exponential decay makes it hard for a token to attend to distant information, leading to an excessively focused attention map rather than an arbitrary attention map. This conclusion about fixed decay is general and is unrelated to the usage of $\\mathbf{Q}$ or $\\mathbf{K}$ . ", "page_idx": 19}, {"type": "text", "text": "ii) Models without decay $\\mathrm{\\Delta}\\alpha=1$ ) can satisfy Eq. (A6). We can solve Eq. (A6) and derive: ", "page_idx": 19}, {"type": "equation", "text": "$$\nk_{s}=\\frac{p_{t s}}{q_{t}},\\forall s=1,\\ldots,t.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "At any time $s$ , fix an appropriate $|q_{t}|\\leq C_{q}$ , and we can obtain bounded solutions for all $k_{s}$ ", "page_idx": 19}, {"type": "text", "text": "iii) Models with both dynamic decay $\\alpha_{t}$ and key $k_{t}$ can satisfy Eq. (A6), but have parameterredundancy. We can solve Eq. (A6) starting from time $t$ to time $1$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{k_{t}=\\frac{p_{t t}}{q_{t}},}\\\\ &{k_{t-1}\\alpha_{t}=\\frac{p_{t t-1}}{q_{t}},}\\\\ &{\\quad\\cdots,}\\\\ &{k_{t-k}\\alpha_{t-k+1}\\cdot\\left(\\alpha_{t-k+2}\\cdot\\cdot\\cdot\\alpha_{t}\\right)=\\frac{p_{t t-k}}{q_{t}},}\\\\ &{\\quad\\cdots,}\\\\ &{k_{1}\\alpha_{2}\\cdot\\left(\\alpha_{3},\\cdot\\cdot\\cdot,\\alpha_{t}\\right)=\\frac{p_{t1}}{q_{t}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "At any time $t-k$ , parameters $\\alpha_{t-k+2},\\cdot\\cdot\\cdot,\\alpha_{t}$ have already been determined. It is equivalent to use two free parameters $k_{t-k},\\alpha_{t-k+1}$ to concurrently approximate just one variable $\\frac{p_{t\\:t-\\bar{k}}}{q_{t}}$ , which is redundant. Dynamic decay and key actually have similar function, i.e., to balance historical memory and new input. Fix a $|q_{t}|\\le C_{q}$ , we can obtain many possible bounded solutions of all $k_{s}$ and $\\alpha_{s}$ . ", "page_idx": 19}, {"type": "text", "text": "iv) Models with dynamic decay $\\alpha_{t}$ and without key $k_{t}$ can satisfy Eq. (A6). Inspired by the parameterredundancy in iii), we further consider a linear model with only query and dynamic decay, i.e., using dynamic decay $1-\\alpha_{t}$ to replace $k_{t}$ \u2019s role to approximate softmax attention map. We further assume $q_{t}=1$ , which is bounded, and define $\\begin{array}{r}{S_{t}=\\sum_{s=1}^{t}p_{t s}}\\end{array}$ , which satisfies $S_{t}\\in[0,1]$ and $S_{t}\\geq S_{t-1}$ . ", "page_idx": 19}, {"type": "text", "text": "Solve Eq. (A6) starting from time $t$ to time 1: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha_{t}=1-p_{t t}=S_{t-1},}\\\\ &{\\alpha_{t-1}=1-\\frac{p_{t t-1}}{\\alpha_{t}}=\\frac{S_{t-2}}{S_{t-1}},}\\\\ &{\\cdots,}\\\\ &{\\alpha_{t-k}=1-\\frac{p_{t t-k}}{\\alpha_{t-k+1}\\cdot\\cdots\\alpha_{t}}=\\frac{S_{t-k-1}}{S_{t-k}},}\\\\ &{\\cdots,}\\\\ &{\\alpha_{1}=1-\\frac{p_{t1}}{\\alpha_{2}\\cdot\\cdot\\cdot\\cdot\\alpha_{t}}=\\frac{S_{0}}{S_{1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "At any time $t-k$ , we can derive a closed-form and bounded solution $\\begin{array}{r}{\\alpha_{t-k}=\\frac{S_{t-k-1}}{S_{t-k}}\\in[0,1]}\\end{array}$ ", "page_idx": 20}, {"type": "text", "text": "General case. Aiming to solve Eq. (A4) and Eq. (A5), we generalize to vector version with $d_{k}>1$ and consider distribution of all time $\\mathbf{p}_{t},t=1,\\ldots,d_{k}$ . In an extreme case, let one dimension of $\\mathbf{q}$ relate to one time, which means $\\mathbf{q}_{t}\\,=\\,[0,\\cdot\\cdot\\cdot\\,,q_{t},\\cdot\\cdot\\cdot\\,,0]$ . Through the analysis above for one dimension (Eq. (A6) and Eq. (A7)), we can ensure: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbf{q}_{t}\\cdot\\Big(\\big(\\prod_{j=s+1}^{t}\\alpha_{j}\\big)\\odot\\mathbf{k}_{s}\\Big)^{\\top}=q_{t}\\big(\\prod_{j=s+1}^{t}\\alpha_{j}\\big)k_{s}=p_{t s},\\forall s=1,\\ldots,t.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Actually, we can further relax the condition and view $\\mathbf{q}_{t}$ as a selector, which selects several channels of Hadamard product of $\\alpha_{j}$ and ${\\bf k}_{s}$ , and uses these channels to approximate its attention distribution. One-hot assumption of $\\mathbf{q}_{t}$ is an extremely hard selector. This means models without $\\mathbf{Q}$ can not approximate softmax attention map, because without selection they can only fti one token\u2019s attention distribution theoretically. ", "page_idx": 20}, {"type": "text", "text": "In summary, through analysis of simple case i) to iv), models with fixed decay $\\pmb{\\Lambda}$ cannot ensure bounded parameters. Through analysis of general case, $\\mathbf{Q}$ is necessary. So we can conclude that: only models with parameters $\\left(\\mathbf{Q},\\mathbf{K},\\mathbf{A}_{t}\\right)$ , $(\\mathbf{Q},\\mathbf{K})$ or $(\\mathbf{Q},\\mathbf{A}_{t})$ can satisfy C2. (For models with only $\\mathbf{Q}$ , without decay and $\\mathbf{k}_{t}=\\mathbf{1}$ for all $t$ , a token\u2019s attentions to other tokens are all the same, which means such models can only approximate one special attention map.) ", "page_idx": 20}, {"type": "text", "text": "A2.3 Analysis of Least Parameter Approximation (C3) ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "When model dimension $d,d_{k},d_{v}$ are fixed, parameter counts of $\\theta_{q},\\theta_{k},\\theta_{\\alpha}$ are fixed too. Thus fewest parameters means fewest parameter groups. Linear attentions with general form can be classified into three types based on the parameter groups: i) using $\\left({\\bf Q},{\\bf K},{\\bfLambda}_{}{\\bf A}/{\\bfLambda}_{t}\\right)$ all together, ii) exploiting $(\\mathbf{Q},\\mathbf{K})$ , $\\left(\\mathbf{Q},\\mathbf{A}/\\mathbf{A}_{t}\\right)$ or $({\\bf K},{\\bf A}/{\\bf A}_{t})$ , iii) employing only one of $\\mathbf{Q}$ , K, ${\\mathbf{}A}/{\\mathbf{}A}_{t}$ . ", "page_idx": 20}, {"type": "text", "text": "Proposition A2.3. Only models with Query and dynamic decay $(\\mathbf{Q},\\mathbf{A}_{t})$ can satisfy C3 (Least parameter approximation). Models with $(\\mathbf{Q},\\mathbf{A}_{t})$ can meet $C O$ , C1, C2 simultaneously with fewest parameters. ", "page_idx": 20}, {"type": "text", "text": "Proof. i) C0: all the linear models with general form (Sec. 3) own linear complexity and satisfy C0. ", "page_idx": 20}, {"type": "text", "text": "ii) C1: according to proposition A2.1, only models with dynamic decay $\\mathbf{{A}}_{t}$ have dynamic memory ability. ", "page_idx": 20}, {"type": "text", "text": "iii) C2: according to proposition A2.2, only models with parameters $\\left(\\mathbf{Q},\\mathbf{K},\\mathbf{A}_{t}\\right)$ , $(\\mathbf{Q},\\mathbf{K})$ or $(\\mathbf{Q},\\mathbf{A}_{t})$ have static approximation ability. ", "page_idx": 20}, {"type": "text", "text": "So we claim that only models using $\\left(\\mathbf{Q},\\mathbf{K},\\mathbf{A}_{t}\\right)$ or $(\\mathbf{Q},\\mathbf{A}_{t})$ can meet C0, C1 and C2 simultaneously. And $(\\mathbf{Q},\\mathbf{A}_{t})$ with two parameter groups is least parameter approximation. That is, dynamic decay can replace Key\u2019s function to balance historical information and new input when approximation is performed. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "A2.4 Conclusions of Optimal Linear Approximation Analysis ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we summarize our conclusions of theoretical analysis. It is worth noting that our theory studies functions of LinAttMap, but not whether these functions will be successfully learned. ", "page_idx": 21}, {"type": "text", "text": "i) Linear approximation. The necessary conditions (C1 and C2) for LinAttMap to achieve approximation to SoftAttMap is that its implementation must include at least two parameter groups: Query $\\mathbf{Q}$ and dynamic decay $\\mathbf{\\Lambda}_{}\\Lambda_{t}$ . Both $\\left(\\mathbf{Q},\\mathbf{K},\\Lambda_{t}\\right)$ and $(\\mathbf{Q},\\mathbf{A}_{t})$ can achieve approximation. ", "page_idx": 21}, {"type": "text", "text": "ii) Least parameter approximation. LinAttMap utilizing $(\\mathbf{Q},\\mathbf{A}_{t})$ can achieve linear approximation to SoftAttMap with theoretically fewest parameters. ", "page_idx": 21}, {"type": "text", "text": "iii) Function of Query. $\\mathbf{Q}$ can be seen as a channel selector, which selects several channels of Hadamard product of $\\alpha_{t}$ and $\\mathbf{k}_{t}$ , and uses these channels to approximate attention map. Thus it is indispensable for approximation and its dimension size (also the model memory size) is the guarantee of approximation ability. ", "page_idx": 21}, {"type": "text", "text": "iv) Function of dynamic decay. Dynamic decay $\\mathbf{{A}}_{t}$ is the key to achieve dynamic memory and precise approximation, and can substitute the role of $\\mathbf{K}$ when approximation is performed. ", "page_idx": 21}, {"type": "text", "text": "A3 MetaLA Architecture ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We here introduce Meta Linear Attention (MetaLA), a linear approximation with least parameters to softmax attention map. We design three enhancements of MetaLA relative to the general linear attention in Sec. 3: i) The Key matrix is not used, which is based on our theoretical analysis. ii) Self-augmentation and iii) Short convolution are two other optional techniques to further enhance approximation ability of our model. appendix A3.1 explicates enhancement i) and design of basic MetaLA layer. appendix A3.2 introduces enhancements ii) and iii), and design of overall MetaLA architecture. ", "page_idx": 21}, {"type": "text", "text": "A3.1 MetaLA Layer ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In MetaLA layer, we exploit $1-\\alpha_{t}$ to replace $\\mathbf{k}_{t}$ based on theoretical analysis in appendix A2, i.e., LinAttMap utilizing $(\\mathbf{Q},\\mathbf{A}_{t})$ is the linear approximation with least parameter redundancy to SoftAttMap and $\\mathbf{K}$ is redundant. ", "page_idx": 21}, {"type": "text", "text": "General Recurrent Form. Using marks in Eq. (11)-Eq. (15), the improvement lies in Eq. (A25): ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{q}_{t}=\\mathbf{x}_{t}W_{Q},\\boldsymbol{\\alpha}_{t}=\\sigma(\\mathbf{x}_{t}W_{\\alpha})\\quad\\in\\mathcal{R}^{1\\times d_{k}},}\\\\ &{\\mathbf{v}_{t}=\\mathbf{x}_{t}W_{V},\\mathbf{g}_{t}=\\phi(\\mathbf{x}_{t}W_{G}+\\pmb{b}_{G})\\quad\\in\\mathcal{R}^{1\\times d_{v}},}\\\\ &{\\mathbf{S}_{t}^{h}=\\mathrm{diag}(\\alpha_{t}^{h})\\mathbf{S}_{t-1}^{h}+(\\mathbf{1}-\\alpha_{t}^{h})^{\\top}\\mathbf{v}_{t}\\quad\\in\\mathcal{R}^{d_{k}^{\\prime}\\times d_{v}^{\\prime}},}\\\\ &{\\mathbf{o}_{t}=\\mathrm{XNorm}(\\mathrm{concat}[\\mathbf{q}_{t}^{1}\\mathbf{S}_{t}^{1},\\mathbf{q}_{t}^{2}\\mathbf{S}_{t}^{2},\\cdot\\cdot\\cdot,\\mathbf{q}_{t}^{H}\\mathbf{S}_{t}^{H}])\\quad\\in\\mathcal{R}^{1\\times d_{v}},}\\\\ &{\\mathbf{y}_{t}=(\\mathbf{g}_{t}\\odot\\mathbf{o}_{t})\\mathbf{W}_{Q}\\quad\\in\\mathcal{R}^{1\\times d},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\mathbf{x}_{t}\\ \\in\\ \\mathcal{R}^{1\\times d}$ denotes the current input. ${\\cal W}_{\\cal Q},{\\cal W}_{\\alpha}\\;\\in\\;{\\mathcal R}^{d\\times d_{k}}$ , $W_{V}$ $V_{V},W_{G}\\ \\in\\ \\mathcal{R}^{d\\times d_{v}}$ and $W_{O}\\,\\in\\,\\mathcal{R}^{d_{v}\\times d}$ are learnable parameters. $\\begin{array}{r}{d_{k/v}^{\\prime}\\,=\\,\\frac{d_{k/v}}{H}}\\end{array}$ and $h\\,=\\,1,\\cdot\\cdot\\cdot\\,,H$ is the index of heads. Here $d_{k}$ represents Query/Decay dimension. LayerNorm is chosen as the normalization operation. Furthermore, we choose $\\sigma=\\mathrm{Sigmoid}(\\cdot)^{1/\\tau}$ following [15], where $\\tau=16$ is used to control the value of dynamic decay and $\\phi=\\mathrm{SiLU}$ . ", "page_idx": 21}, {"type": "text", "text": "As for parameter allocation, we simply set $d_{v}\\,=\\,d$ and $\\begin{array}{r}{d_{k}\\,=\\,\\frac{d}{2}}\\end{array}$ , similar to GLA [15]. Thus, the whole number of parameters used by a MetaLA layer is $4d^{2}$ , the same as the softmax attention layer and smaller than other popular attention-like subquadratic models such as RetNet [14] $(8d^{2})$ and GLA [15] $(4d^{2}+24d)$ , etc. Without usage of $\\mathbf{K}$ , we can allocate more parameters and utilize full-rank matrix $W_{\\alpha}$ to produce dynamic decay rather than low-rank matrix used by GLA, such that do not sacrifice expression capacity of $\\mathrm{f}_{\\alpha}$ and approximation performance of MetaLA. ", "page_idx": 21}, {"type": "text", "text": "General Parallel Form. The recurrent form of MetaLA can be written in a parallel mode as follows: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\bf O}=\\left(\\left(\\left({\\bf Q}\\odot{\\bf A}\\right)\\left(\\frac{{\\bf B}}{{\\bf A}}\\right)^{\\top}\\right)\\odot{\\bf M}\\right){\\bf V}},}\\\\ {{\\displaystyle({\\bf Q}/{\\bf K}/{\\bf V})_{t,:}=({\\bf q}/{\\bf k}/{\\bf v})_{t},\\quad{\\bf A}_{t,:}=\\prod_{j=1}^{t}\\alpha_{j},\\quad{\\bf B}_{t,:}={\\bf1}-\\alpha_{t},\\quad{\\bf M}_{i,j}=\\left\\{\\!1,i\\leq j\\!.}\\end{array}\\right.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "$\\frac{\\mathbf{B}}{\\mathbf{A}}$ in Eq. (A28) denotes element-wise division and $(\\mathbf{Q})_{t,}$ : means the $t$ -th row of matrix $\\mathbf{Q}$ . In practical implementation, we utilize chunkwise form proposed in [15] for hardware-efficient training. ", "page_idx": 22}, {"type": "text", "text": "Attention map. Here the attention map is $\\begin{array}{r}{\\left(\\left(\\left(\\mathbf{Q}\\odot\\mathbf{A}\\right)\\left(\\frac{\\mathbf{B}}{\\mathbf{A}}\\right)^{\\top}\\right)\\odot\\mathbf{M}\\right)}\\end{array}$ . The element in the attention map matrix is as follows (heads are omitted for simplicity): ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname{LinAttMap}_{t,s}=\\left\\{\\mathbf{q}_{t}\\cdot\\Big((\\prod_{j=s+1}^{t}\\alpha_{j})\\odot(\\mathbf{1}-\\alpha_{s})^{\\top}\\Big),s\\leq t.\\right.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "A3.2 MetaLA Transformer ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Improved MetaLA. After deriving the basic MetaLA layer shown above, we consider further optimization when designing complete MetaLA block. One observation is that in most cases, setting $\\alpha_{t}\\,=\\,0$ and completely discarding historical information is catastrophic. Actually, in real-world scenario, the learned dynamic decay $\\alpha_{t}$ always closes to 1 because of many tasks\u2019 strong need to capture long-term dependencies. Hence we propose a technique called self-augmentation to enlarge a token\u2019s attention to itself while do not disrupt the model\u2019s state constitution, in order to better avert attention dilution. Moreover, an additional short convolution can be inserted before entering MetaLA layer to further enhance local interaction, motivated by Mamba [16] and Griffin [18]. Combining these two techniques, the improved MetaLA is shown as follows, the main improvements lie in Eq. (A31) and Eq. (A35): ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{\\mathbf{x}_{t}=\\mathrm{Convld}(\\mathbf{x}_{t})}&{\\in\\mathcal{R}^{1\\times d},}&&{}\\\\ &{\\mathbf{q}_{t}=\\mathbf{x}_{t}W_{Q},\\alpha_{t}=\\sigma(\\mathbf{x}_{t}W_{\\alpha})}&{\\in\\mathcal{R}^{1\\times d_{k}},}&&{}\\\\ &{\\mathbf{v}_{t}=\\mathbf{x}_{t}W_{V},\\mathbf{g}_{t}=\\phi(\\mathbf{x}_{t}W_{G})}&{\\in\\mathcal{R}^{1\\times d_{v}},}&&{}\\\\ &{\\mathbf{S}_{t}^{h}=\\mathrm{diag}(\\alpha_{t}^{h})\\mathbf{S}_{t-1}^{h}+(1-\\alpha_{t}^{h})^{\\top}\\mathbf{v}_{t}}&{\\in\\mathcal{R}^{d_{k}^{\\prime}\\times d_{v}^{\\prime}},}&&{}\\\\ &{\\mathbf{o}_{t}^{h}=\\mathbf{q}_{t}^{h}\\mathbf{S}_{t}^{h}+\\sigma_{\\mathrm{aug}}\\big(\\mathbf{q}_{t}^{h}(w_{\\mathrm{aug}}^{h}\\odot(1-\\alpha_{t}^{h}))^{\\top}\\mathbf{v}_{t}\\big)}&{\\in\\mathcal{R}^{1\\times d_{v}^{\\prime}},}&&{}\\\\ &{\\mathbf{o}_{t}=\\mathrm{XNorm}(\\mathrm{concat}[\\mathbf{o}_{t}^{1},\\mathbf{o}_{t}^{2},\\cdots,\\mathbf{o}_{t}^{H}])}&{\\in\\mathcal{R}^{1\\times d_{v}},}&&{}\\\\ &{\\mathbf{y}_{t}=(\\mathbf{g}_{t}\\odot\\mathbf{o}_{t})W_{Q}\\quad\\in\\mathcal{R}^{1\\times d}.}&&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "As for self-augmentation, without changing the composition of hidden state $\\mathbf{S}_{t}^{h}$ , it is only added on the output process through a learnable parameter $w_{\\mathrm{aug}}\\in\\mathcal{R}^{1\\times d_{k}}$ , which is then divided into heads like other parameters do. $\\sigma_{\\mathrm{aug}}(\\cdot)$ is a nonlinearity to control the magnitude of augmentation term and avoid covering with the original attention. We choose Sigmoid $(\\cdot)$ in this paper. ", "page_idx": 22}, {"type": "text", "text": "The proposed design has two advantages: First, it maintains parallel computing as shown in Eq. (A38); Second, it only enhances current token\u2019s own attention and does not change the attention of future tokens to the current token $(p_{t s},s<t)$ , because we only change $\\mathbf{o}_{t}^{h}$ while maintaining $\\mathbf{S}_{t}^{h}$ unchanged like that in original MetaLA layer Eq. (A25). Thus the separation of output and memory is realized. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbf{O}=\\Big(\\Big(\\big(\\mathbf{Q}\\odot\\mathbf{A}\\big)\\big(\\frac{\\mathbf{B}}{\\mathbf{A}}\\big)^{\\top}\\Big)\\odot\\mathbf{M}\\Big)\\mathbf{V}+\\mathrm{diag}\\,\\big(\\,\\mathrm{sum}\\,\\big((\\mathbf{Q}\\odot\\mathbf{B}\\odot\\mathbf{W}),\\mathrm{dim}=1\\big)\\big)\\mathbf{V},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\mathbf{W}_{t,:}=\\pmb{w}_{\\mathrm{aug}}$ and $\\mathrm{sum}(\\mathbf{Q},\\mathrm{dim}=1)$ means calculate sum for each row of matrix $\\mathbf{Q}$ . Other marks are defined same as Eq. (A29). ", "page_idx": 22}, {"type": "text", "text": "MetaLA block. Regard each layer as a function mapping the input $\\mathbf{X}\\,\\in\\,\\mathcal{R}^{n\\times d}$ to the output $\\mathbf{Y}\\in\\mathcal{R}^{n\\times d}$ where: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbf{X}=\\left[\\begin{array}{c}{\\mathbf{x}_{1}}\\\\ {\\mathbf{x}_{2}}\\\\ {\\vdots}\\\\ {\\mathbf{x}_{n}}\\end{array}\\right],\\mathbf{Y}=\\left[\\begin{array}{c}{\\mathbf{\\bar{y}}_{1}}\\\\ {\\mathbf{\\bar{y}}_{2}}\\\\ {\\vdots}\\\\ {\\mathbf{\\bar{y}}_{n}}\\end{array}\\right],\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "following Transformer structure, now the Token Mix mechanism Eq. (A31)-Eq. (A37) can be integrated as follows: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbf{Y}=\\operatorname{TokenMix}(\\mathbf{X}),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and the Channel Mix part (GLU [3]) can be written as follows: ", "page_idx": 23}, {"type": "equation", "text": "$$\n{\\bf Y}=(\\mathrm{Swish}({\\bf X}W_{1})\\odot{\\bf X}W_{2})W_{3},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which can be integrated as: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\mathbf{X}}^{l+\\frac{1}{2}}=\\mathrm{TokenMix}^{l}(\\mathrm{XNorm}^{l}({\\mathbf{X}}^{l}))+{\\mathbf{X}}^{l},}\\\\ {{\\mathbf{X}}^{l+1}=\\mathrm{ChannelMix}^{l}(\\mathrm{XNorm}^{l+\\frac{1}{2}}({\\mathbf{X}}^{l+\\frac{1}{2}})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "image", "img_path": "Y8YVCOMEpz/tmp/e8d3f4c3aeeb26218c02dfba26778818c525043cbb4e331127cb1587afab7f75.jpg", "img_caption": ["Figure A1: MetaLA Transformer. Stacking $N$ MetaLA blocks, each block is composed of two modules in sequence: token mixer and channel mixer. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "where $\\mathbf{X}^{l}$ and $\\mathbf{X}^{l+1}\\,\\in\\,\\mathcal{R}^{n\\times d}$ refer to the input and output of block $l$ in MetaLA. We choose $\\mathrm{XNorm}=$ LayerNorm. Stacking several MetaLA blocks above, we can derive complete MetaLA Transformer as a Linear Foundation Model, as shown in Fig. A1. ", "page_idx": 23}, {"type": "text", "text": "A4 Experimental Details ", "text_level": 1, "page_idx": 23}, {"type": "table", "img_path": "Y8YVCOMEpz/tmp/50456425827d5c08fa73c13295cecda14679aef2d5d18a0e6ea732420c09e899.jpg", "table_caption": ["Table A4: Hyper-parameters of MetaLA on LRA. d is the dimension of model. d1 is the dimension of $d_{q}$ and $d_{k}$ , d2 is the hidden dimension in GLU. num-warmup and max-step are used for cosine warmup. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Associative Recall (AR). Following [36], we train two layer models with a Transformer backbone that interleaves token mixer and channel mixer. Learning rates are swept by np. logspace $(-4,-2,4)$ for sequence length 256 and additional np. logspace $(-5,-3,4)$ for length 512, and maximum test accuracy is reported. For GLA [15] and MetaLA, we set $H=2$ and $d_{k}=d_{v}=d$ . The kernel size of short convolution of MetaLA is 2. ", "page_idx": 23}, {"type": "text", "text": "Language Modeling. For 360M/1.4B model, we train it from scrach with a total of 15B/300B tokens on 16/32 A100 GPUs at a learning rate of $3\\mathrm{e}{-}4/2\\mathrm{e}{-}4$ with batch size $0.5\\mathrm{M}/2\\mathrm{M}$ . Both models maintain a length of 2048 and are trained using fp16. The training setup of baselines [15, 16, 44] of 360M MetaLA are aligned with MetaLA configurations. For the 1.3B MetaLA, we compare it with publicly available models [14, 15, 16, 20, 44]. To maintain a fair comparison between linear models, we trained Mamba from scratch using the same settings with MetaLA on 100B tokens. For GLA and Retnet, we adopted the open-source checkpoints in FLA [79]. For HGRN and Pythia, we used the official open-source checkpoints. We implement all the pretrain experiments with GPTNeox [45]. We evaluate our models on SuperGLUE benchmark [38] and Common-Sense Reasoning benchmark including LAMBADA [80], LogiQA [81], Winograd Schema Challenge (WSC273) [82], BoolQ [83], PiQA [84], HellaSwag [85], WinoGrande [86], ARC-easy (ARC-e), ARC-challenge (ARC-c) [87], OpenBookQA [88]. We report perplexity (ppl) and accuracy (acc) on LAMBADA, accuracy normalized by length on HellaSwag, ARC-challenge and OpenbookQA, and acc on the other subtasks. For SuperGLUE benchmark, we report F1 score on CB, Exact-Match (EM) score on MultiRC, and accuracy on the other subtasks, following the original work. The LM evaluation harness [89] is used to implement all evaluations. ", "page_idx": 23}, {"type": "text", "text": "Long Range Arena. We evaluate the long sequence modeling capability of our model on LRA task. We use the adamW optimizer and cosine warmup scheduler. We set the head size be 4 for group normalization. In Retrieval, Image, Pathfinder and Path-X we use a bid-model. The hyperparameters for all tasks can be found in Table A4 ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "Image Classification. We evaluate our models on ImageNet [40]. The input size of ImageNet is 224 $\\times~224$ . Following Deit [49], the batch size is set to 2048 during 300 training epochs with a cosinedecay learning rate whose peak value is $2.4\\times10^{-3}$ . We choose AdamW $(\\beta_{1}\\stackrel{-}{=}\\bar{0}.9,\\beta_{2}=0.98)$ with 0.05 weight decay as the optimizer. Note that we do not use cutmix or mixup during the training. ", "page_idx": 24}, {"type": "text", "text": "Table A5: Performance Comparison on Additional subtasks for Common-Sense Reasoning. PS: parameter size (billion). T: tokens (billion). \u2020 means the results reported by [20]. $^\\ddag$ indicates testing using open-source checkpoints. For baselines that need to be compared, if they do not have public checkpoints, we train and test them under identical conditions with MetaLA. LAMB: Lambada. HS: HellaSwag. WG: WinoGrande. OBQA: OpenbookQA. MetaL $\\mathbf{\\nabla}\\mathbf{A}_{a}$ : MetaLA with tied embedding trained using 100B tokens. MetaL $\\mathbf{\\nabla}_{\\cdot}\\mathbf{A}_{b}$ : MetaLA trained with 300B tokens. \"AVG\" refers to the average result on subtasks other than LAMBADA. ", "page_idx": 24}, {"type": "table", "img_path": "Y8YVCOMEpz/tmp/f3d419461406b724a00a1d3e39a53a1de438a66a76a0d664fd609b12b8e4623b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "Y8YVCOMEpz/tmp/c31459eb0b093555b4255d15367bd216c2e87afa5dc51f39723cad55c4ff60d2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "Y8YVCOMEpz/tmp/9a0504ec6d22b25bea68be28f99193efb7da2f4494c09a35a28fffb7b4199d7e.jpg", "table_caption": ["Table A6: Results on MAD tasks. All architectures are tested according to the MAD protocol. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "A5 Additional Experiments ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Additional subtasks for Common-Sense Reasoning. We extend more subtasks of Commonsense Reasoning for 1B4 MetaLA. Additional experimental results are shown in Tab. A5. ", "page_idx": 24}, {"type": "text", "text": "More challenging settings for MQAR. We evaluate some models with sequence length 512 and with more retrieval key-value pairs (80, default is 64). The attention baseline beneftis from global modeling capabilities, achieving optimal results. The results in Tab. A7 show that MetaLA outperforms Mamba (does not converge under the same training conditions), and there is still a significant gap compared to transformers. ", "page_idx": 24}, {"type": "text", "text": "Results on the synthetic MAD task. we evaluate several models on MAD [41], a collection of six synthetic tasks predictive of scaling laws, including recall, memorization, and compression. As shown in Tab. A6, MetaLA achieves the best results across various linear complexity models. ", "page_idx": 24}, {"type": "table", "img_path": "Y8YVCOMEpz/tmp/abd9430e27af076ae94722c17123962667b2aaa2b4d5a0585ef24f7df0c761e8.jpg", "table_caption": ["Table A7: Results on MQAR with sequence length 512 and retrieval key-value pairs 80. "], "table_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "Y8YVCOMEpz/tmp/bedc8d4e343e9c0064260e1a114b8c0b647ac7b34aa49ad8cf7d0f797a5f59da.jpg", "img_caption": ["Figure A2: Training efficiency evaluations. The throughput and memory usage on a single A800 GPU of Transformer and various linear models. Transformer+ $^{\\cdot+}$ is implemented using FlashAttention [90] and SwiGLU. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "Results on the Needle in a Haystack (NIAH) task. We also present experimental results on the NIAH task following [42], which is designed to evaluate the in-context retrieval capabilities of LLMs. Retrieval ability in long texts is a significant challenge for linear models, as all current linear models lack good solutions to this problem. Nonetheless, Tab. A8 shows that MetaLA has achieved satisfactory results in comparisons among linear models. Compared to Transformer models, this performance is still insufficient. This is precisely the issue we hope to address next, following the unification of linear model forms. ", "page_idx": 25}, {"type": "text", "text": "The scalability with respect to model size and training tokens. For preliminary validation, we further evaluate our model ranging in size from 380M to 3B, trained with up to 300B tokens, on the CommonSense Reasoning benchmark. The strong results in Tab. A9 demonstrate the potential of our model when scaling up the parameter scale and training dataset. ", "page_idx": 25}, {"type": "text", "text": "Training efficiency evaluations. The comparative results on training throughput and GPU memory usage across various 1.3B-sized models are shown in Fig. A2. The report indicates that: (1) Our model demonstrates good linearity, maintaining processing speed and memory efficiency with increasing sequence length, unlike the Transformer, which experiences a sharp drop in token processing speed as sequence length increases. (2) Our model matches the computational efficiency of linear models like GLA [2] in both latency and memory, and is significantly faster than Mamba [5], which also has linear complexity. ", "page_idx": 25}, {"type": "text", "text": "Table A8: Results on the Needle in a Haystack (NIAH) task. We introduce accuracy metrics across four context scales and three model scales. The middle columns display accuracies below the 4K and 8K thresholds. The rightmost columns detail both the average accuracy and the weighted average accuracy. All models are trained with sequence length 8K. ", "page_idx": 25}, {"type": "table", "img_path": "Y8YVCOMEpz/tmp/59fbcb79ffefe1c064089c0b129eb79465f53c2d8496d2803ffa24c41d117dc1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "Y8YVCOMEpz/tmp/9cda679a4664644950b65394626024332a916d44ee6d8ac28aa810cd85ecc1ea.jpg", "table_caption": ["Table A9: Scalability tests of MetaLA on the CommonSense Reasoning benchmark. PS: parameter size (billion). T: tokens (billion). \u201cAVG\u201d refers to the average result of all subtasks. "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: In the abstract, we summarized the theory we proposed, our solution MetaLA, and the results obtained from our experiments. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: In Sec. 7, we discussed potential concerns about the approximation of softmax attention and the reasons that may cause these concerns. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 27}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: In Sec. 4 and appendix A2, we provided the definitions and detailed proofs of our theory. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: In Sec. 6, we showcased our experimental setup and the results, and in appendix A4, we further elaborated on the details of all experimental setups. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 29}, {"type": "text", "text": "Answer: [No] ", "page_idx": 29}, {"type": "text", "text": "Justification: We have not open-sourced the code, but we have clearly outlined the details of our experiments in Sec. 6 and appendix A4 to ensure the reproducibility of the results. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 29}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We presented the detailed settings for training and test in Sec. 6 and appendix A4. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: We did not conduct experiments that required reporting significance. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: In Sec. 6 and appendix A4, we provided details about the computer resources used for the experiments. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 30}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The research conducted in the paper complies with the NeurIPS ethical guidelines in all aspects. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 30}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: This paper is foundational research and not tied to particular applications. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 30}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 31}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [No] ", "page_idx": 31}, {"type": "text", "text": "Justification: Impact Statements. This paper presents work whose goal is to advance the field of Language Modeling. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 31}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The data, code, and models used in this paper have all been properly cited. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: In Sec. 5, we provided details of the model we proposed. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 32}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 32}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}]