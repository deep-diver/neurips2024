[{"figure_path": "LEzx6QRkRH/tables/tables_6_1.jpg", "caption": "Table 1: Comparison of several tasks selected from the Minedojo benchmark. Our RL-GPT achieves the highest success rate on all tasks. All values in our tables refer to the actual successful rate.", "description": "This table presents a comparison of the success rates achieved by different methods (MINEAGENT, MINEAGENT (AUTOCRAFT), PLAN4MC, and RL-GPT) on various tasks selected from the MineDojo benchmark.  The tasks involve crafting and harvesting items within the Minecraft environment.  RL-GPT demonstrates the highest success rate across all tasks.", "section": "4.3 Main Results"}, {"figure_path": "LEzx6QRkRH/tables/tables_6_2.jpg", "caption": "Table 2: Main results in the challenging ObtainDiamond task in Minecraft. Existing strong baselines in ObtainDiamond either require expert data (VPT, DEPS), hand-crafted policies (DEPS-Oracle) for subtasks, or take huge number of environment steps to train (DreamerV3, VPT). Our method can automatically decompose and learn subtasks with only a little human prior, achieving ObtainDiamond with great sample efficiency.", "description": "This table presents the main results of the ObtainDiamond challenge in the Minecraft game, comparing the performance of RL-GPT with several existing strong baselines.  It highlights that RL-GPT achieves a high success rate (8%) with significantly fewer samples compared to other methods, demonstrating its efficiency in solving complex, long-horizon tasks. The table also emphasizes the advantages of RL-GPT in addressing the limitations of previous approaches, particularly in terms of data requirements, sample efficiency, and the need for human-designed components.", "section": "4.2 Implementation Details"}, {"figure_path": "LEzx6QRkRH/tables/tables_7_1.jpg", "caption": "Table 3: Ablation on the RL and Code-as-policy components and the iteration mechanism. RL-GPT outperforms its Pure RL and Pure Code counterparts. Given more iterations, RL-GPT gets better results.", "description": "This table presents the results of an ablation study on the RL-GPT model.  It shows the success rates of different model variants on four tasks in the Minecraft environment.  The variants include a model using only reinforcement learning (Pure RL), a model using only code (Pure Code), and different versions of RL-GPT with varying numbers of iterations.  The results demonstrate that integrating both RL and code, as done in RL-GPT, leads to significantly better performance than using either approach alone, and that increasing the number of iterations further improves performance.", "section": "4.4 Ablation Study"}, {"figure_path": "LEzx6QRkRH/tables/tables_7_2.jpg", "caption": "Table 4: Ablation on the agent structure.", "description": "This table presents the ablation study on different agent structures used in the RL-GPT framework.  It shows the success rates for three different configurations:\n\n1. **One Agent:** All tasks are handled by a single agent.\n2. **Slow + Fast:** The tasks are divided between a slow agent (for planning) and a fast agent (for execution).\n3. **Slow + Fast + Critic:**  A critic agent is added to provide feedback and improve the performance of the slow and fast agents.\n\nThe results indicate that using a slow, fast, and critic agent structure leads to the highest success rate.", "section": "4.4 Ablation Study"}, {"figure_path": "LEzx6QRkRH/tables/tables_21_1.jpg", "caption": "Table 17: OpenAI tokens it takes to \"speed up\" RL.", "description": "This table shows the number of OpenAI tokens consumed in each iteration of the RL-GPT framework for enhancing the RL training process.  The number of tokens increases with each iteration, suggesting that more complex instructions or reasoning are needed to improve the RL agent's performance.", "section": "4 Experiments"}, {"figure_path": "LEzx6QRkRH/tables/tables_22_1.jpg", "caption": "Table 18: High-level comparison among different methods.", "description": "This table provides a high-level comparison of several methods in terms of their ability to handle long-horizon tasks, low-level control, sample efficiency and self-improvement.  It shows that RL-GPT outperforms other methods in all these aspects.", "section": "4.3 Main Results"}, {"figure_path": "LEzx6QRkRH/tables/tables_22_2.jpg", "caption": "Table 19: Comparison among different LLMs.", "description": "This table compares the performance of three different Large Language Models (LLMs): Vicuna-13B, Claude, and GPT-4, on a specific task.  The performance is measured by two metrics: Success Rate and Dead Loop rate. The success rate indicates the percentage of times the LLM successfully completed the task, while the Dead Loop rate shows how often the LLM got stuck in an unproductive loop.", "section": "4.3 Main Results"}]