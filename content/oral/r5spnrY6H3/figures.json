[{"figure_path": "r5spnrY6H3/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration with a target object and multiple auxiliary objects, associated with a referring expression. The target marked in green represents the main referred instance, while targets in other colors indicate other mentioned entities. This visual highlights the challenge of effectively completing semantic reasoning in the absence of spatial inference.", "description": "This figure illustrates a scene with a target object (a chair) and several other objects (a desk and a sofa).  The caption highlights the difficulty of accurately segmenting the target chair using only textual information, specifically when the description involves relative spatial relationships (like \"near\" and \"far away\"). This scenario demonstrates the need for a model to understand and utilize spatial relationships between objects to perform accurate 3D referring expression segmentation.", "section": "1 Introduction"}, {"figure_path": "r5spnrY6H3/figures/figures_3_1.jpg", "caption": "Figure 2: An overview of the proposed RG-SAN. This model analyzes a point cloud and a textual description with N1 tokens, extracting superpoints and word-level features. The TLM assigns spatial positions to tokens, facilitating multimodal fusion. The RWS strategy enables the model to learn the positions of all mentioned entities using only the supervision of the target position.", "description": "This figure illustrates the architecture of the Rule-Guided Spatial Awareness Network (RG-SAN) for 3D Referring Expression Segmentation.  It shows the two main components: the Text-driven Localization Module (TLM) and the Rule-guided Weak Supervision (RWS). The TLM processes both point cloud and text features to iteratively refine the spatial positions of all entities mentioned in the text. The RWS leverages dependency tree rules and the target object's location to guide the network's learning, using only the target's position for supervision. This allows for accurate spatial relationship modeling among all entities. The figure also details the feature extraction, multimodal fusion, position refinement, and loss functions used in the model training process.", "section": "3 Method"}, {"figure_path": "r5spnrY6H3/figures/figures_9_1.jpg", "caption": "Figure 3: Visualization of all the nouns in the textual description. Our RG-SAN can segment instances corresponding to different nouns, while 3D-STMN indiscriminately assigns all nouns to the target instance. Zoom in for best view.", "description": "The figure showcases the qualitative results of RG-SAN and 3D-STMN on the ScanRefer validation set. It illustrates RG-SAN's ability to accurately segment multiple instances mentioned in the textual descriptions, unlike 3D-STMN which assigns all nouns to a single target object. This highlights RG-SAN's enhanced referring capability by precisely segmenting distinct entities, demonstrating its robust generalization for complex texts and precise localization for multiple entities.", "section": "4.5 Qualitative Comparison"}, {"figure_path": "r5spnrY6H3/figures/figures_16_1.jpg", "caption": "Figure 4: Statistics of samples in the ScanRefer dataset based on the presence of spatial relation descriptions, where \"spatial\" represents samples with spatially related descriptions, while \"w/o spatial\" denotes spatially unrelated samples.", "description": "This figure shows a pie chart illustrating the distribution of samples in the ScanRefer dataset based on whether their descriptions include spatial relations. The vast majority (92%) of the descriptions contain spatial terms, highlighting the significance of spatial reasoning in this 3D object localization task. Only a small fraction (8%) of the samples lack spatial descriptions. ", "section": "Appendix"}, {"figure_path": "r5spnrY6H3/figures/figures_19_1.jpg", "caption": "Figure 2: An overview of the proposed RG-SAN. This model analyzes a point cloud and a textual description with N tokens, extracting superpoints and word-level features. The TLM assigns spatial positions to tokens, facilitating multimodal fusion. The RWS strategy enables the model to learn the positions of all mentioned entities using only the supervision of the target position.", "description": "This figure presents a detailed architecture overview of the proposed RG-SAN model. The model processes both point cloud data and textual descriptions, extracting relevant features from each modality. It employs a Text-driven Localization Module (TLM) to iteratively refine the spatial positions of all entities mentioned in the text. The Rule-Guided Weak Supervision (RWS) strategy leverages the target object's position to implicitly supervise the positioning of all entities, including auxiliary objects. This enhances the model's ability to accurately capture and reason about spatial relationships between entities.", "section": "3 Method"}, {"figure_path": "r5spnrY6H3/figures/figures_20_1.jpg", "caption": "Figure 3: Visualization of all the nouns in the textual description. Our RG-SAN can segment instances corresponding to different nouns, while 3D-STMN indiscriminately assigns all nouns to the target instance. Zoom in for best view.", "description": "This figure visualizes the results of both RG-SAN and 3D-STMN on a sample from the ScanRefer dataset.  The image shows the original scene, ground truth segmentation, and the segmentation results from each model. RG-SAN successfully segments multiple instances corresponding to the nouns mentioned in the referring expression. In contrast, 3D-STMN fails to discriminate between instances, assigning all mentioned objects to a single target.", "section": "4.5 Qualitative Comparison"}]