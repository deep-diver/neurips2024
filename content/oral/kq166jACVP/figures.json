[{"figure_path": "kq166jACVP/figures/figures_1_1.jpg", "caption": "Figure 1: (Left) Architecture of the Aligner module and illustration of its behavior in semantic space. As a plug-and-play module, Aligner stack upon an upstream LLM. The Aligner redistributes initial answers from the upstream model into more helpful and harmless answers, thus aligning the composed LLM responses with human intentions. (Right) Analogy of Aligner as a residual learning enhancer for LLMs in architecture and capabilities. Like a residual block that adds modifications via a shortcut without altering the base structure, the Aligner employs a copy and correct method to improve the original answer. This analogy highlights the Aligner's dual role in preserving the parameter of the upstream model while enhancing it to align with desired outcomes.", "description": "The figure illustrates the Aligner model architecture and its functionality.  The left side shows the Aligner as a plug-and-play module that takes the output of an upstream large language model (LLM) and modifies it to be more helpful and harmless.  It does this by redistributing the initial answer in semantic space. The right side provides an analogy comparing the Aligner to a residual block in a neural network, highlighting its efficiency in enhancing the original response without significantly altering the base model's structure.", "section": "2 Aligner"}, {"figure_path": "kq166jACVP/figures/figures_4_1.jpg", "caption": "Figure 2: Distribution of helpfulness and harmlessness scores. (a) The distribution shift in preferred and dis-preferred answers in the training dataset; (b) redistribution shift of Aligner-7B, based on upstream models such as GPT-4 (b1), Alpaca-7B (b2) and Llama2-70B-Chat (b3) in the evaluation dataset. Our findings include: (1) Preferred answers in the training dataset surpasses original answers in both helpfulness and harmlessness; (2) The refuse-to-answer pattern of GPT-4 created an area of overcorrected answers where both helpful and harmless scores are low, and Aligner-7B improved these answers by providing additional information and corrections. (3) The Alpaca-7B model, which lacks alignment, had its answers significantly corrected by our Aligner-7B, increasing both scores. (4) The Llama2-70B-Chat model, already aligned with a higher average safety score than the training dataset corrections, benefits from Aligner-7B corrections, significantly enhancing helpfulness while maintaining the harmless score.", "description": "This figure shows the distribution of helpfulness and harmlessness scores before and after applying Aligner-7B to different upstream models.  Panel (a) displays the distribution in the training data, highlighting the difference between preferred and dis-preferred answers. Panels (b1-b3) show how Aligner-7B shifts the distribution for different LLMs, improving helpfulness and harmlessness.  It demonstrates Aligner-7B's ability to correct for various model shortcomings, including refusal to answer and lack of alignment.", "section": "3.2 Experiment Results"}, {"figure_path": "kq166jACVP/figures/figures_5_1.jpg", "caption": "Figure 2: Distribution of helpfulness and harmlessness scores. (a) The distribution shift in preferred and dis-preferred answers in the training dataset; (b) redistribution shift of Aligner-7B, based on upstream models such as GPT-4 (b1), Alpaca-7B (b2) and Llama2-70B-Chat (b3) in the evaluation dataset. Our findings include: (1) Preferred answers in the training dataset surpasses original answers in both helpfulness and harmlessness; (2) The refuse-to-answer pattern of GPT-4 created an area of overcorrected answers where both helpful and harmless scores are low, and Aligner-7B improved these answers by providing additional information and corrections. (3) The Alpaca-7B model, which lacks alignment, had its answers significantly corrected by our Aligner-7B, increasing both scores. (4) The Llama2-70B-Chat model, already aligned with a higher average safety score than the training dataset corrections, benefits from Aligner-7B corrections, significantly enhancing helpfulness while maintaining the harmless score.", "description": "This figure shows the distribution shifts in helpfulness and harmlessness scores before and after applying Aligner-7B to different LLMs.  It illustrates how Aligner-7B improves the scores, especially for models that initially had low scores or exhibited problematic behaviors like refusing to answer.  The plots show the distribution of scores in the training data and then the resulting distribution after Aligner-7B's intervention for several different LLMs.", "section": "3.2 Experiment Results"}, {"figure_path": "kq166jACVP/figures/figures_7_1.jpg", "caption": "Figure 4: Interpretability experiment results on Aligner. (a)(b) The LAT scan graph of Aligner's each layer when generating the first 20 output tokens for two given question-answer pairs. A higher value in the graph indicates a more active correction representation in that layer. Specifically, (a) exhibits raised activity, suggesting an enhanced correction action in the output, whereas (b) displays a tendency towards copying the original response. Moreover, the distinct differences between the two graphs are mainly observed in the early layers. This indicates that the decision regarding the degree of correction is made in the early layers of Aligner. (c) The control experiment shows the effectiveness of the extracted correction representation vector in modulating the Aligner's correction behavior. The relationship between the average levenshtein ratio and representation vector coefficients is approximately linear, with an R2 value of approximately 0.93.", "description": "This figure shows the results of interpretability experiments on the Aligner model.  The LAT scan graphs (a) and (b) visualize the neural activity in different layers of the Aligner while generating responses. Graph (a) shows higher activity for correction, while (b) shows a tendency to copy the original response, indicating that the correction decision is made primarily in the early layers. Graph (c) demonstrates how manipulating the correction representation vector linearly affects the degree of correction applied by the Aligner, confirming its interpretability.", "section": "3.4 Interpretability Experiments"}, {"figure_path": "kq166jACVP/figures/figures_7_2.jpg", "caption": "Figure 5: Illustration of multi-round alignment pipeline with Aligner. As a data augmentation and synthetic tool, Aligner can enhance the upstream model\u2019s response A into an improved response A*, thereby forming a synthetic preference dataset. This dataset can be used to further train the upstream model via RLHF/DPO. Repeating this process allows for multi-round RLHF or DPO.", "description": "This figure illustrates how Aligner, a plug-and-play module, can be integrated into a multi-round RLHF (Reinforcement Learning from Human Feedback) or DPO (Direct Preference Optimization) pipeline to improve alignment.  In each round, the upstream LLM generates a response (A), which is then refined by Aligner to produce a better response (A*).  These improved responses (A*) are used to create a synthetic preference dataset for the next round of RLHF/DPO, iteratively bootstrapping the upstream model's alignment with human preferences and values. This iterative process helps mitigate reward model collapse and over-optimization that can occur in typical multi-round RLHF/DPO training.", "section": "4 Multi-round RLHF training via Aligner"}, {"figure_path": "kq166jACVP/figures/figures_8_1.jpg", "caption": "Figure 6: Multi-round refinement through Aligner.", "description": "The figure shows the results of a multi-round alignment pipeline using Aligner, compared to standard multi-round PPO and DPO.  The x-axis represents helpfulness, and the y-axis represents harmlessness.  Each point represents the model's performance after a round of training.  Aligner consistently improves both helpfulness and harmlessness across multiple rounds, unlike the other methods which mainly focus on helpfulness, often at the cost of increased harmfulness. This demonstrates Aligner's ability to enhance both dimensions simultaneously and its effectiveness in mitigating reward model collapse in multi-round RLHF.", "section": "4 Multi-round RLHF training via Aligner"}, {"figure_path": "kq166jACVP/figures/figures_14_1.jpg", "caption": "Figure 7: An illustration of our methodology. The Superalignment problem focuses on scaling human oversight for supervising increasingly intelligent and complex AI systems. The Weak-to-Strong Generalization [51] analogy emphasizes using weak models to supervise strong models. Our approach composes weak and strong models to offer reliable and scalable supervision.", "description": "This figure illustrates the methodology of using weak models to supervise strong models, specifically focusing on the \"Weak-to-Strong Correction via Aligner\" approach.  It compares three scenarios: Super Alignment (human directly supervising a very strong AI), Weak-to-Strong Generalization (a weaker AI supervising a stronger AI), and the proposed Weak-to-Strong Correction via Aligner (where a lightweight Aligner model acts as the weak supervisor to correct and improve the outputs of a much stronger LLM, such as GPT-4 or Llama2). The figure emphasizes the scalability and reliability of the proposed method compared to direct human supervision, which becomes increasingly difficult as AI models become more powerful.", "section": "A Application: Weak-to-Strong Correction via Aligner"}, {"figure_path": "kq166jACVP/figures/figures_14_2.jpg", "caption": "Figure 7: An illustration of our methodology. The Superalignment problem focuses on scaling human oversight for supervising increasingly intelligent and complex AI systems. The Weak-to-Strong Generalization [51] analogy emphasizes using weak models to supervise strong models. Our approach composes weak and strong models to offer reliable and scalable supervision.", "description": "This figure illustrates the difference between the Weak-to-Strong Generalization and the Weak-to-Strong Correction methodologies.  The former involves a weak model generating labels for training a strong model. The latter uses Aligner, a smaller model, to correct the output of a strong model, creating training labels to further improve the strong model's performance.  This highlights the paper's approach of using a smaller, efficient model (Aligner) to enhance the alignment of larger language models.", "section": "A Application: Weak-to-Strong Correction via Aligner"}, {"figure_path": "kq166jACVP/figures/figures_27_1.jpg", "caption": "Figure 9: The data processing procedure of Aligner's correction data. We compile a training dataset of 50K Q-A-C pairs. This dataset originated from 27K queries based on the Stanford Alpaca [3], user-shared conversations from ShareGPT (https://sharegpt.com), HH-RLHF [62, 5] and others. Using various LLMs, we generated and refined initial Q-A pairs, which were then annotated with corrections by GPT-4, human annotators, and Llama2-70B-Chat, followed by quality filtering.", "description": "This figure illustrates the data processing pipeline for creating the training dataset used in the Aligner model.  It starts with a raw corpus of prompts, which undergoes prompt quality filtering.  Then, multiple language models (LLMs) generate answers.  These are filtered for quality and duplicates, resulting in pairwise data.  Finally, multiple annotators, including human annotators, GPT-4 and Llama2-70B-Chat, provide corrections, leading to a final training dataset of 50K query-answer-correction (Q-A-C) triplets.", "section": "D.1 The Details of Query-Answer Dataset"}, {"figure_path": "kq166jACVP/figures/figures_29_1.jpg", "caption": "Figure 1: (Left) Architecture of the Aligner module and illustration of its behavior in semantic space. As a plug-and-play module, Aligner stack upon an upstream LLM. The Aligner redistributes initial answers from the upstream model into more helpful and harmless answers, thus aligning the composed LLM responses with human intentions. (Right) Analogy of Aligner as a residual learning enhancer for LLMs in architecture and capabilities. Like a residual block that adds modifications via a shortcut without altering the base structure, the Aligner employs a copy and correct method to improve the original answer. This analogy highlights the Aligner's dual role in preserving the parameter of the upstream model while enhancing it to align with desired outcomes.", "description": "The figure shows the architecture of the Aligner module, a plug-and-play module that can be stacked on top of any upstream LLM to improve its alignment with human intentions.  The left side illustrates how Aligner redistributes the initial LLM output to produce more helpful and harmless responses. The right side draws an analogy between Aligner and a residual block in a neural network, highlighting its ability to enhance the upstream model without significantly altering its parameters.", "section": "2 Aligner"}]