[{"figure_path": "uNKlTQ8mBD/figures/figures_1_1.jpg", "caption": "Figure 1: We train mathematical reasoning agents starting only from the axioms of a given formal domain, where they jointly learn to pose challenging but provable conjectures and to find their proofs.", "description": "The figure illustrates the MINIMO framework.  It shows a two-part process: Conjecturing and Theorem Proving. In the Conjecturing phase, a language model, informed by axioms and using constrained decoding, generates novel mathematical conjectures. These conjectures are then fed into the Theorem Proving phase, where a Monte Carlo Tree Search (MCTS) algorithm, guided by a policy and value function (also learned from the language model), attempts to find proofs for the conjectures. The success or failure of proof attempts, along with the difficulty of the proofs, generates new training data, which is then used to improve both the conjecturing and theorem proving components of the model in an iterative self-improvement loop.", "section": "3 MINIMO"}, {"figure_path": "uNKlTQ8mBD/figures/figures_6_1.jpg", "caption": "Figure 2: Difficulty of proved conjectures found in each iteration of MINIMO, evaluate as the log-probability of the proof under the policy checkpoints after each iteration of the training loop (with standard error bands for runs with 3 random seeds).", "description": "This figure shows how the difficulty of proven conjectures changes over training iterations of the MINIMO model.  Difficulty is measured by the negative log-probability of the proof under the policy network at each iteration.  The plot shows that as the model trains, it generates progressively harder (lower log-probability) conjectures that are still provable.  The lines represent the difficulty of conjectures across the different training iterations and the shaded regions represent the standard error for three random seeds.", "section": "4.1 Learning dynamics"}, {"figure_path": "uNKlTQ8mBD/figures/figures_8_1.jpg", "caption": "Figure 3: Difficulty of proved conjectures proposed in each iteration under the current policy at that same iteration, comparing when using and not using hindsight relabeling to generate new proofs and conjectures, with standard error bands for runs with 3 random seeds. Ideal behavior would be a flat line, representing constant relative difficulty. Hindsight significantly helps the agent conjecture propose harder problems.", "description": "This figure shows the effect of hindsight relabeling on the difficulty of conjectures generated by the MINIMO agent across 5 training iterations.  The y-axis represents the negative log-likelihood of the proof under the current policy (lower values mean harder problems).  The x-axis shows the training iteration. Separate lines are shown for experiments with and without hindsight relabeling. The shaded area shows standard error across three runs.  The results indicate that hindsight relabeling is crucial for maintaining the challenge of the generated conjectures.", "section": "3.3 Hindsight Relabeling"}, {"figure_path": "uNKlTQ8mBD/figures/figures_8_2.jpg", "caption": "Figure 4: Success rate of our agents at proving theorems from the \u201cIntroduction to Metamathematics", "description": "This figure shows the success rate of the trained agents in proving theorems from two external sources: Kleene's textbook and the Natural Number Game.  The x-axis represents the checkpoint iteration (training stage), and the y-axis represents the success rate (proportion of theorems proven within 2000 MCTS expansions).  The plot shows separate lines for the Arithmetic and Propositional Logic domains, demonstrating the improvement in solving human-written theorems as the agents trained on self-generated problems progress through the training iterations.", "section": "4.2 Proving human-written theorems (RQ4)"}, {"figure_path": "uNKlTQ8mBD/figures/figures_13_1.jpg", "caption": "Figure 5: Relation between MCTS iterations until the proof is found, vs log-likelihood of the proof that is found. The higher the likelihood, the faster MCTS is in finding the proof.", "description": "This figure shows the relationship between the number of iterations required by Monte Carlo Tree Search (MCTS) to find a proof and the log-likelihood of that proof under the learned policy.  The log-likelihood serves as a measure of difficulty; higher log-likelihood indicates an easier proof to find, hence fewer MCTS iterations are required.  The data points represent individual proofs found by MCTS, showing a clear trend where higher likelihoods correlate with faster proof discovery. A regression line visually reinforces this negative correlation.", "section": "B Proof log-likelihood and MCTS Expansions"}, {"figure_path": "uNKlTQ8mBD/figures/figures_14_1.jpg", "caption": "Figure 6: Ratio of proven conjectures in each batch of 200, at each iteration of training.", "description": "This figure shows the fraction of conjectures that were proven across different training iterations for three different axiomatic domains (Arithmetic, Groups, and Propositional Logic). The results are shown separately for experiments with and without hindsight relabeling.  The graph reveals how the ratio of successfully proven conjectures changes as the model improves over multiple training iterations in each domain.  This indicates whether the model is generating increasingly difficult, yet still solvable, conjectures.", "section": "4 Experiments"}]