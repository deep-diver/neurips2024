[{"figure_path": "4bKEFyUHT4/tables/tables_7_1.jpg", "caption": "Table 1: Main results for the CIFAR-10 experiments. Our LogicTreeNet models reduce the required numbers of logic gates by factors of \u2265 29\u00d7 compared to the state-of-the-art models. Our models are scaled to match accuracies.", "description": "This table presents the main results of the CIFAR-10 experiments.  It compares the accuracy and number of logic gates used by various models, including the authors' LogicTreeNet models and several state-of-the-art baselines.  The LogicTreeNet models show significantly fewer gates while achieving comparable or better accuracy than existing methods.", "section": "5 Experiments"}, {"figure_path": "4bKEFyUHT4/tables/tables_7_2.jpg", "caption": "Table 2: Timing results for CIFAR-10. The time is per image on an FPGA. We use a Xilinx VU13P FPGA. Our times are bottleneck by the data transfer onto the FPGA. 'A' indicates the use of an ASIC.", "description": "This table compares the inference time per image on a Xilinx VU13P FPGA for various methods on the CIFAR-10 dataset.  The time is the bottleneck of data transfer to FPGA.  The methods compared include FINN CNV, RebNet (with one and two residual blocks), Zhao et al., FBNA CNV, FracBNN, TrueNorth, and three different sizes of the LogicTreeNet model (S, M, and B).  Note that TrueNorth uses an ASIC instead of an FPGA.", "section": "5 Experiments"}, {"figure_path": "4bKEFyUHT4/tables/tables_8_1.jpg", "caption": "Table 3: Results of the MNIST experiment. We use a Xilinx XC7Z045 FPGA, the same device as FINN CNV. All other baselines utilize equivalent or more powerful FPGAs.", "description": "This table presents the results of the MNIST experiments, comparing the proposed LogicTreeNet models to various existing state-of-the-art methods.  It shows the accuracy, number of logic gates used, and FPGA inference time for each method. The table highlights the superior efficiency and accuracy of the LogicTreeNet models compared to other approaches in terms of both accuracy and the number of gates used, which is directly proportional to hardware costs.", "section": "5.2 MNIST"}, {"figure_path": "4bKEFyUHT4/tables/tables_8_2.jpg", "caption": "Table 4: Variances between individual models on MNIST.", "description": "This table shows the accuracy variations observed across multiple runs of different MNIST models (S, M, and L).  The variations are presented as mean accuracy \u00b1 standard deviation, highlighting the impact of random initialization and fixed connectivity on model performance.", "section": "5.2 MNIST"}, {"figure_path": "4bKEFyUHT4/tables/tables_8_3.jpg", "caption": "Table 5: Ablation study on CIFAR-10 wrt. architectural choices.", "description": "This ablation study analyzes the impact of different architectural components of the LogicTreeNet model on its performance. The table shows the accuracy achieved with various combinations of architectural elements, including the use of trees, residual initializations, or-pooling, weight decay, and the number of input channels.  The study demonstrates the importance of each element for the model's success.", "section": "5.3 Ablation Study"}, {"figure_path": "4bKEFyUHT4/tables/tables_14_1.jpg", "caption": "Table 6: Hyperparameters for each model and data set: softmax temperatures \u03c4, learning rates \u03b7, weight decays \u03b2, and batch sizes bs. For reference to show the relationship to \u03c4, we include the number of output neurons in the last layer per class nee/c. The range of attainable class scores is [0, nee/c/\u03c4].", "description": "This table shows the hyperparameters used for training different models on CIFAR-10 and MNIST datasets.  It lists the softmax temperature, learning rate, weight decay, batch size, output gate factor, number of input bits, number of outputs per class, and the maximum attainable class score for each model.", "section": "A.2 Training Details"}]