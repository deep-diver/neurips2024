[{"figure_path": "YvA8UF0I37/tables/tables_5_1.jpg", "caption": "Table 1: Comparing different fine-tuning strategies for VQ, GPTQ and AQLM on LLAMA 2 7B in terms of perplexity on WikiText-2, C4 and average zero-shot accuracy on tasks from Section 4.3.", "description": "This table compares the performance of different fine-tuning strategies (calibration only, continuous parameters only, naive linearized PV, stochastic rounding, straight-through estimation, subspace linearized PV, and subspace linearized PV + STE) on three different quantized representations (GPTQ, VQ, and AQLM) of the LLAMA 2 7B model.  The performance is measured using perplexity on the WikiText-2 and C4 datasets, and average zero-shot accuracy on tasks described in Section 4.3.  The table highlights the superior performance of PV-tuning.", "section": "4.2 Evaluating Fine-tuning Algorithms"}, {"figure_path": "YvA8UF0I37/tables/tables_7_1.jpg", "caption": "Table 1: Comparing different fine-tuning strategies for VQ, GPTQ and AQLM on LLAMA 2 7B in terms of perplexity on WikiText-2, C4 and average zero-shot accuracy on tasks from Section 4.3.", "description": "This table compares the performance of different fine-tuning strategies (Calibration only, Continuous params only, Naive Linearized PV, Stochastic Rounding, Straight Through Estimation, Subspace Linearized PV, Subspace Linearized PV+STE) on three different quantized representations (VQ, GPTQ, AQLM) of the LLAMA 2 7B model. The performance is measured using perplexity on WikiText-2 and C4 datasets, as well as average zero-shot accuracy across multiple tasks (as defined in Section 4.3).  The table highlights the relative improvements achieved by each fine-tuning method compared to others.", "section": "4.2 Evaluating Fine-tuning Algorithms"}, {"figure_path": "YvA8UF0I37/tables/tables_8_1.jpg", "caption": "Table 2: Quantized model perplexity on WikiText-2\u2193 [45] & C4\u2193 [54] and the Average\u2191 accuracy on 5 zero-shot tasks [24] for various models and bitwidths. Arrows \u2191 / \u2193 mean higher / lower is better.", "description": "This table presents a comparison of various large language models (LLMs) after quantization using different methods.  It shows the perplexity scores on the WikiText-2 and C4 datasets, along with the average zero-shot accuracy across five tasks. The models are grouped by size (7B, 13B, 70B), and the results are shown for different average bits per parameter, demonstrating the trade-off between model size and accuracy after quantization.", "section": "4.3 Large-scale Evaluation & Discussion"}, {"figure_path": "YvA8UF0I37/tables/tables_18_1.jpg", "caption": "Table 1: Comparing different fine-tuning strategies for VQ, GPTQ and AQLM on LLAMA 2 7B in terms of perplexity on WikiText-2, C4 and average zero-shot accuracy on tasks from Section 4.3.", "description": "This table compares the performance of three different fine-tuning strategies (continuous parameters only, straight-through estimation, and stochastic rounding) across three different quantized representations (VQ, GPTQ, and AQLM) of the Llama 2 7B model.  The performance is measured using perplexity on the WikiText-2 and C4 datasets, as well as average zero-shot accuracy across several tasks described in section 4.3 of the paper.  The table aims to show the effectiveness of each fine-tuning approach on achieving a balance between accuracy and compression.", "section": "Evaluating Fine-tuning Algorithms"}, {"figure_path": "YvA8UF0I37/tables/tables_24_1.jpg", "caption": "Table 3: Estimated L along the GD trajectory for Llama-160m (Schema I).", "description": "This table presents the estimated L-smoothness constant (L) for the Llama-160m model along the gradient descent (GD) trajectory using Schema I.  Different subspace sizes (5%, 10%, 20%, 30%, 40%, 60%, 70%, 85%, 100%) are considered, showing the number of trainable parameters for each subspace and the corresponding estimated L value. The table illustrates the non-increasing property of the L-smooth constant when restricting the subspace of training variables, demonstrating a significant decrease in L as the subspace size reduces.", "section": "F On L-smoothness of LLM Objective in Sparse Subspaces"}, {"figure_path": "YvA8UF0I37/tables/tables_24_2.jpg", "caption": "Table 4: Estimated L along the GD trajectory for TinyLlama-1.1B (Schema I).", "description": "This table presents the estimated L-smoothness constant (L) for the TinyLlama-1.1B language model using two different schemas.  The L-smoothness constant is a measure of how curved the objective function is.  Lower values indicate a smoother, more easily optimized function. The table shows how L changes as the size of the subspace used for gradient descent is varied. The subspace size is represented as a percentage (5%, 10%, etc.), indicating the proportion of trainable parameters updated in each iteration of the optimization process.   The 'Number of Trainable Parameters' column shows the number of parameters being optimized within the specified subspace. The 'Estimated L' column shows the estimated L-smoothness constant for that subspace size, obtained through gradient descent. Schema I refers to a specific method for estimating L that does not fully capture local curvature of the objective function.", "section": "F On L-smoothness of LLM Objective in Sparse Subspaces"}, {"figure_path": "YvA8UF0I37/tables/tables_25_1.jpg", "caption": "Table 3: Estimated L along the GD trajectory for LLama-160m (Schema I).", "description": "This table presents the estimated L-smoothness constant (L) along the gradient descent (GD) trajectory for the LLama-160m model using Schema I.  The L-smoothness constant is calculated for different subspace sizes (5%, 10%, 20%, 30%, 40%, 60%, 70%, 85%, and 100%), which represents the percentage of trainable parameters considered.  The table shows the number of trainable parameters within each subspace and the corresponding estimated L value.  Schema I is an estimation method that doesn't capture local curvature, providing an upper bound estimate of the true L-smoothness constant along the GD trajectory.", "section": "F On L-smoothness of LLM Objective in Sparse Subspaces"}, {"figure_path": "YvA8UF0I37/tables/tables_25_2.jpg", "caption": "Table 4: Estimated \\(\\hat{L}\\) along the GD trajectory for TinyLlama-1.1B (Schema I).", "description": "This table presents the estimated L-smoothness constant (\\(\\hat{L}\\)) for the TinyLlama-1.1B language model using two different schemas (Schema I and II)  along the gradient descent (GD) trajectory.  The table shows how the estimated \\(\\hat{L}\\) changes as the size of the subspace used in GD varies from 5% to 100% of the total trainable parameters.  Schema I estimates \\(\\hat{L}\\) without considering local curvature, while Schema II captures local curvature by utilizing an approximate hessian-vector product.", "section": "F On L-smoothness of LLM Objective in Sparse Subspaces"}, {"figure_path": "YvA8UF0I37/tables/tables_28_1.jpg", "caption": "Table 7: Comparison of WikiText-2 Perplexity for each method with and without fine-tuning for quantizing Llama 2 7B model. For each method, PPL no FT denotes its perplexity without fine-tuning, whereas PPL w/ FT is perplexity with fine-tuning. We use the same setup as in Section 4.1.", "description": "This table compares the WikiText-2 perplexity of different quantization methods applied to the Llama 2 7B model, both with and without fine-tuning.  It shows the average bits per weight used by each method and the resulting perplexity scores before and after fine-tuning. The setup for each method is consistent with Section 4.1 of the paper. The table allows for a comparison of the effectiveness of various quantization techniques and the impact of fine-tuning on model performance.", "section": "4.1 Evaluating quantized representations with finetuning"}, {"figure_path": "YvA8UF0I37/tables/tables_28_2.jpg", "caption": "Table 1: Comparing different fine-tuning strategies for VQ, GPTQ and AQLM on LLAMA 2 7B in terms of perplexity on WikiText-2, C4 and average zero-shot accuracy on tasks from Section 4.3.", "description": "This table compares the performance of different fine-tuning strategies (calibration only, continuous parameters only, naive linearized PV, stochastic rounding, straight-through estimation, subspace linearized PV, and subspace linearized PV+STE) on three different quantized representations (GPTQ, VQ, and AQLM) of the LLAMA 2 7B model.  The evaluation metrics include perplexity on WikiText-2 and C4 datasets, and average zero-shot accuracy across multiple downstream tasks. The table highlights the superior performance of subspace linearized PV and its combination with STE across various representations, showcasing its robustness and effectiveness.", "section": "4.2 Evaluating Fine-tuning Algorithms"}, {"figure_path": "YvA8UF0I37/tables/tables_30_1.jpg", "caption": "Table 9: Evaluation of LLAMA-2 7B quantized using QuIP# with various fine-tuning strategies. We report perplexity on WikiText-2 [45] & C4 [54] and zero-shot accuracy. The Average is the mean accuracy of 5 zero-shot tasks. Primary metrics are Wiki2 (PPL), C4 (PPL) and Average (Accuracy).", "description": "This table presents a comparison of different fine-tuning strategies applied to the Llama-2 7B model quantized using the QuIP# algorithm.  It shows the performance in terms of perplexity (WikiText-2 and C4 datasets) and average zero-shot accuracy across five tasks. The comparison includes QuIP# without fine-tuning, QuIP# with the built-in fine-tuning method, an improved version of the built-in fine-tuning, and finally QuIP# combined with PV-Tuning. The primary metrics considered are WikiText-2 perplexity, C4 perplexity, and average zero-shot accuracy.", "section": "4.3 Large-scale Evaluation & Discussion"}, {"figure_path": "YvA8UF0I37/tables/tables_31_1.jpg", "caption": "Table 10: Evaluation of LLAMA-2 7B quantized using VQ+PV with different group size (GS, also known as vector dimension) and code bits (CB, s.t. codebook size is 2<sup>CB</sup>). We report perplexity on WikiText-2 [45] & C4 [54] and zero-shot accuracy. The Average is the mean accuracy of 5 zero-shot tasks. Primary metrics are Wiki2 (PPL), C4 (PPL) and Average (Accuracy).", "description": "This table presents the results of experiments conducted to evaluate the impact of varying group size and codebook size on the performance of the Vector Quantization with PV-Tuning method.  It shows the WikiText-2 and C4 perplexity scores, as well as the average zero-shot accuracy across five different tasks, for various combinations of group size and codebook size.  The goal is to determine the optimal settings that balance model size and performance.", "section": "4.3 Large-scale Evaluation & Discussion"}, {"figure_path": "YvA8UF0I37/tables/tables_32_1.jpg", "caption": "Table 1: Comparing different fine-tuning strategies for VQ, GPTQ and AQLM on LLAMA 2 7B in terms of perplexity on WikiText-2, C4 and average zero-shot accuracy on tasks from Section 4.3.", "description": "This table compares different fine-tuning strategies (Calibration only, Continuous params only, Naive Linearized PV, Stochastic Rounding, Straight Through Estimation, Subspace Linearized PV, and Subspace Linearized PV+STE) across three different quantized weight representations (GPTQ, VQ, and AQLM) on the Llama 2 7B model.  The performance metrics include perplexity on the WikiText-2 and C4 datasets and average zero-shot accuracy across several tasks. The table helps to analyze the effectiveness of different fine-tuning algorithms and highlight the superior performance of PV-Tuning in achieving optimal compression-accuracy trade-offs.", "section": "4.2 Evaluating Fine-tuning Algorithms"}, {"figure_path": "YvA8UF0I37/tables/tables_32_2.jpg", "caption": "Table 2: Quantized model perplexity on WikiText-2\u2193 [45] & C4\u2193 [54] and the Average\u2191 accuracy on 5 zero-shot tasks [24] for various models and bitwidths. Arrows \u2191 / \u2193 mean higher / lower is better.", "description": "This table presents a comparison of various large language models (LLMs) compressed to different bitwidths using different quantization methods.  It shows the perplexity scores on the WikiText-2 and C4 datasets, as well as the average accuracy across five zero-shot tasks.  The models include Llama 2 and 3, Mistral, and Phi-3 Mini-4k-Instruct.  The table helps to understand the trade-off between model size (bits per parameter) and performance across different models and quantization techniques. Lower perplexity and higher accuracy are better.", "section": "4.3 Large-scale Evaluation & Discussion"}, {"figure_path": "YvA8UF0I37/tables/tables_33_1.jpg", "caption": "Table 2: Quantized model perplexity on WikiText-2\u2193 [45] & C4\u2193 [54] and the Average\u2191 accuracy on 5 zero-shot tasks [24] for various models and bitwidths. Arrows \u2191 / \u2193 mean higher / lower is better.", "description": "This table presents a large-scale evaluation of the PV-Tuning algorithm across different large language models (LLMs) and bitwidths.  It compares the performance of PV-Tuning against existing quantization methods (QuIP, BiLLM, PB-LLM, DB-LLM, AQLM, OneBit, and QuIP#) in terms of perplexity on the WikiText-2 and C4 benchmark datasets, as well as average zero-shot accuracy across five different tasks (WinoGrande, PiQA, HellaSwag, ARC-easy, and ARC-challenge).  The table shows the model size, the quantization method used, the average number of bits per weight parameter, and the resulting perplexity and accuracy scores. The arrows indicate whether a higher or lower value is better for each metric. The results demonstrate that PV-Tuning achieves state-of-the-art performance across various models and bitwidths.", "section": "4.3 Large-scale Evaluation & Discussion"}, {"figure_path": "YvA8UF0I37/tables/tables_33_2.jpg", "caption": "Table 12: Evaluation of quantized LLAMA 2 models for 2-2.3 bits per weight, grouped by bitwidth. We report perplexity on WikiText-2 [45] & C4 [54] and zero-shot accuracy. The Average is the mean accuracy of 5 zero-shot tasks. Primary metrics are Wiki2 (PPL), C4 (PPL) and Average (Accuracy).", "description": "This table presents the results of quantizing Llama 2 models to 2-2.3 bits per weight using different methods. It shows the perplexity scores on WikiText-2 and C4 datasets and the average accuracy across five zero-shot tasks for each method and bit-width. The table allows comparison of different quantization methods in terms of their accuracy-compression trade-off.", "section": "4.2 Evaluating Fine-tuning Algorithms"}, {"figure_path": "YvA8UF0I37/tables/tables_34_1.jpg", "caption": "Table 2: Quantized model perplexity on WikiText-2\u2193 [45] & C4\u2193 [54] and the Average\u2191 accuracy on 5 zero-shot tasks [24] for various models and bitwidths. Arrows \u2191 / \u2193 mean higher / lower is better.", "description": "This table presents a comparison of different LLM quantization methods across various model sizes and bit-widths.  The metrics used for comparison are perplexity scores on the WikiText-2 and C4 datasets, along with average accuracy across five zero-shot tasks (WinoGrande, PiQA, HellaSwag, ARC-easy, ARC-challenge). Lower perplexity and higher accuracy are preferred. The table helps to assess the trade-off between model size, bit-width (bits per parameter), and the performance achieved by each method. It showcases the performance of PV-Tuning in comparison to several state-of-the-art quantization techniques, demonstrating its superior performance particularly at low bit-widths (1-2 bits/parameter).", "section": "4.3 Large-scale Evaluation & Discussion"}, {"figure_path": "YvA8UF0I37/tables/tables_36_1.jpg", "caption": "Table 1: Comparing different fine-tuning strategies for VQ, GPTQ and AQLM on LLAMA 2 7B in terms of perplexity on WikiText-2, C4 and average zero-shot accuracy on tasks from Section 4.3.", "description": "This table compares the performance of three different fine-tuning strategies (continuous parameters only, straight-through estimation, and stochastic rounding) across three different quantized representations (VQ, GPTQ, and AQLM) for a Llama 2 7B model.  The results are measured in terms of perplexity on the WikiText-2 and C4 datasets and average zero-shot accuracy across a set of tasks. This table helps to assess the effectiveness of various fine-tuning approaches in improving the accuracy of quantized LLMs. ", "section": "4.2 Evaluating Fine-tuning Algorithms"}]