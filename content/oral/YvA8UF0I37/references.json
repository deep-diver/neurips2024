{"references": [{"fullname_first_author": "Vladimir Malinovskii", "paper_title": "PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression", "publication_date": "2024-XX-XX", "reason": "This is the main reference as it is the current paper being analyzed."}, {"fullname_first_author": "E. Frantar", "paper_title": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers", "publication_date": "2022-10-17", "reason": "This paper is highly influential due to its introduction of GPTQ, a significant advancement in post-training quantization techniques for LLMs."}, {"fullname_first_author": "V. Egiazarian", "paper_title": "Extreme Compression of Large Language Models via Additive Quantization", "publication_date": "2024-01-06", "reason": "This paper proposes AQLM, a state-of-the-art quantization method that the current paper builds upon and improves."}, {"fullname_first_author": "A. Tseng", "paper_title": "QuIP#: Even better LLM quantization with Hadamard incoherence and lattice codebooks", "publication_date": "2024-02-04", "reason": "This paper introduces QuIP#, another state-of-the-art method that is directly compared against in the current work."}, {"fullname_first_author": "T. Dettmers", "paper_title": "SpQR: A sparse-quantized representation for near-lossless LLM weight compression", "publication_date": "2023-06-03", "reason": "This paper presents SpQR, a method for lossless LLM compression that is analyzed and compared in the current study."}]}