[{"figure_path": "6YIpvnkjUK/tables/tables_2_1.jpg", "caption": "Table 1: Comparison of sample and communication complexity of various single-agent and Federated Q-learning algorithms for learning an \u025b-optimal Q-function under the synchronous setting. We hide logarithmic factors and burn-in costs for all results for simplicity of presentation. In the above table, S and A represent state and action spaces respectively and y denotes the discount factor. We report the communication complexity only in terms of number of rounds as other algorithms assume transmission of real numbers and hence do not report bit level costs. For the lower bound, Azar et al. [2013] and this work establish the bound for sample and communication complexity respectively.", "description": "The table compares the sample and communication complexity of several single-agent and federated Q-learning algorithms.  It shows the number of agents, the sample complexity (number of samples needed to learn an epsilon-optimal Q-function), and the communication complexity (number of communication rounds).  Logarithmic factors and burn-in costs are omitted for simplicity.  The table highlights that the proposed Fed-DVR-Q algorithm achieves optimal sample and communication complexity.", "section": "1.1 Main Results"}, {"figure_path": "6YIpvnkjUK/tables/tables_4_1.jpg", "caption": "Table 1: Comparison of sample and communication complexity of various single-agent and Federated Q-learning algorithms for learning an \u025b-optimal Q-function under the synchronous setting. We hide logarithmic factors and burn-in costs for all results for simplicity of presentation. In the above table, S and A represent state and action spaces respectively and y denotes the discount factor. We report the communication complexity only in terms of number of rounds as other algorithms assume transmission of real numbers and hence do not report bit level costs. For the lower bound, Azar et al. [2013] and this work establish the bound for sample and communication complexity respectively.", "description": "This table compares the sample and communication complexity of several single-agent and federated Q-learning algorithms, highlighting the trade-off between these two factors. It considers the synchronous setting and shows that Fed-DVR-Q, a new algorithm proposed in this paper, achieves optimal order sample and communication complexity.  The table hides logarithmic factors for simplicity.", "section": "Main Results"}, {"figure_path": "6YIpvnkjUK/tables/tables_6_1.jpg", "caption": "Table 1: Comparison of sample and communication complexity of various single-agent and Federated Q-learning algorithms for learning an \u025b-optimal Q-function under the synchronous setting. We hide logarithmic factors and burn-in costs for all results for simplicity of presentation. In the above table, S and A represent state and action spaces respectively and y denotes the discount factor. We report the communication complexity only in terms of number of rounds as other algorithms assume transmission of real numbers and hence do not report bit level costs. For the lower bound, Azar et al. [2013] and this work establish the bound for sample and communication complexity respectively.", "description": "The table compares the sample and communication complexity of several single-agent and federated Q-learning algorithms.  It highlights the trade-off between sample complexity (number of samples needed to achieve a certain accuracy) and communication complexity (communication cost required to achieve the same accuracy). The algorithms are evaluated under a synchronous setting (all agents update simultaneously), and logarithmic factors and burn-in costs are omitted for simplicity.  The communication complexity is expressed in terms of communication rounds, due to variations in how other works report communication cost (some report number of bits).  Finally, the table includes a lower bound for both sample and communication complexity.", "section": "1.1 Main Results"}, {"figure_path": "6YIpvnkjUK/tables/tables_13_1.jpg", "caption": "Table 1: Comparison of sample and communication complexity of various single-agent and Federated Q-learning algorithms for learning an \u025b-optimal Q-function under the synchronous setting. We hide logarithmic factors and burn-in costs for all results for simplicity of presentation. In the above table, S and A represent state and action spaces respectively and \u03b3 denotes the discount factor. We report the communication complexity only in terms of number of rounds as other algorithms assume transmission of real numbers and hence do not report bit level costs. For the lower bound, Azar et al. [2013] and this work establish the bound for sample and communication complexity respectively.", "description": "This table compares the sample and communication complexity of several single-agent and federated Q-learning algorithms.  It shows the number of agents, the sample complexity (number of samples needed to learn an epsilon-optimal Q-function), and the communication complexity (number of communication rounds).  The table highlights the trade-off between sample and communication complexity, illustrating how algorithms with lower communication complexity might have higher sample complexity and vice versa.  A lower bound on sample and communication complexity is also provided.", "section": "Main Results"}, {"figure_path": "6YIpvnkjUK/tables/tables_18_1.jpg", "caption": "Table 1: Comparison of sample and communication complexity of various single-agent and Federated Q-learning algorithms for learning an \u025b-optimal Q-function under the synchronous setting. We hide logarithmic factors and burn-in costs for all results for simplicity of presentation. In the above table, S and A represent state and action spaces respectively and \u03b3 denotes the discount factor. We report the communication complexity only in terms of number of rounds as other algorithms assume transmission of real numbers and hence do not report bit level costs. For the lower bound, Azar et al. [2013] and this work establish the bound for sample and communication complexity respectively.", "description": "This table compares the sample and communication complexity of different single-agent and federated Q-learning algorithms.  It shows the number of agents, sample complexity (number of samples needed to learn an epsilon-optimal Q-function), and communication complexity (number of communication rounds).  Logarithmic factors and burn-in costs are omitted for simplicity. The table highlights that the proposed Fed-DVR-Q algorithm achieves order-optimal sample and communication complexities, outperforming existing algorithms.", "section": "Main Results"}]