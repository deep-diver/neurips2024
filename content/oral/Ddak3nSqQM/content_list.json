[{"type": "text", "text": "Policy Learning from Tutorial Books via Understanding, Rehearsing and Introspecting ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xiong-Hui Chen1,+,\u2217, Ziyan Wang2,\u2217, Yali $\\mathrm{Du}^{2,\\,\\diamond}$ , Shengyi Jiang5, Meng Fang4, Yang $\\mathbf{Y}\\mathbf{u}^{1,\\,\\diamond}$ , Jun Wang3, \u22c4 ", "page_idx": 0}, {"type": "text", "text": "1 National Key Laboratory for Novel Software Technology, Nanjing University, China & School of Artificial Intelligence, Nanjing University, China 2Cooperative AI Lab, Department of Informatics, King\u2019s College London 3 AI Centre, Department of Computer Science, University College London 4 University of Liverpool 5 The University of Hong Kong ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "When humans need to learn a new skill, we can acquire knowledge through written books, including textbooks, tutorials, etc. However, current research for decisionmaking, like reinforcement learning (RL), has primarily required numerous real interactions with the target environment to learn a skill, while failing to utilize the existing knowledge already summarized in the text. The success of Large Language Models (LLMs) sheds light on utilizing such knowledge behind the books. In this paper, we discuss a new policy learning problem called Policy Learning from tutorial Books (PLfB) upon the shoulders of LLMs\u2019 systems, which aims to leverage rich resources such as tutorial books to derive a policy network. Inspired by how humans learn from books, we solve the problem via a three-stage framework: Understanding, Rehearsing, and Introspecting (URI). In particular, it first rehearses decision-making trajectories based on the derived knowledge after understanding the books, then introspects in the imaginary dataset to distill a policy network. We build two benchmarks for PLfB based on Tic-Tac-Toe and Football games. In experiment, URI\u2019s policy achieves at least $44\\%$ net win rate against GPT-based agents without any real data; In Football game, which is a complex scenario, URI\u2019s policy beat the built-in AIs with a $37\\%$ while using GPT-based agent can only achieve a $6\\%$ winning rate. The project page: plfb-football.github.io. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Humans can acquire new skills through various written materials that provide condensed knowledge, without the need for direct interactions with the target environment. In contrast, traditional policy learning paradigms, such as reinforcement learning (RL) [1] primarily rely on trial and error [2\u20134]. Despite recent advances in offline RL [5, 6] showing that policy improvements can be achieved simply by using pre-collected data, in fields such as embodied AI for robotics [7], the process of collecting large amounts of decision-making data remains costly and, at times, impractical or even impossible. Therefore, a question arises: similar to how humans learn, can a policy learn from non-real data, e.g., from tutorial books? ", "page_idx": 0}, {"type": "text", "text": "We argue that the recent successes of Large Language Models (LLMs), such as GPT-4 [8], and LLaMA [9] already demonstrated the potential to learn from textual content. However, current studies focus on using LLM directly for decision making [10, 11] or integrating them as auxiliary modules in other machine learning workflows [12\u201314]. In this study, we introduce a novel topic built upon the shoulders of LLMs\u2019 systems: Policy Learning from Books (PLfB). PLfB aims to derive a policy network directly from natural language texts, bypassing the need for numerous real-world interaction data, as shown in Fig. 1(A-B). This can be viewed as a further step towards enabling more resources for policy learning and also a more generalized form of offilne RL problem, which uses textbooks to learn a policy offilne. The essential challenge of PLfB comes from the inevitable large modality gaps between the text space, which includes the knowledge related to decision-making, and the policy network space, which formulates the parameters of the policy function. ", "page_idx": 0}, {"type": "image", "img_path": "Ddak3nSqQM/tmp/3f07f9b5463b5f8836bb2d3e398db457595b91224edb9c250306baedd2f4aeee.jpg", "img_caption": ["Figure 1: (A-B) A comparison between the problem of policy learning from tutorial books (PLfB) and from data; (C) An illustration of the understanding, rehearsal, and introspection for PLfB. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To realize PLfB, inspired by human learning processes, we propose a three-stage learning methodology: understanding, rehearsal, and introspection (URI), which is shown in Fig. 1(C). For understanding, it first extracts knowledge from books to form a knowledge database; then it rehearses imaginary decision-making trajectories with the help of the knowledge retrieved from the database; finally, it introspects on the imaginary dataset to distill a policy network for decision-making. We present the first practical implementation of URI, employing LLMs to convert book paragraphs into pseudocode for policy, dynamics, and reward functions. This pseudocode forms a code-based knowledge database, which is used to generate an imaginary dataset based on retrieval augmented generation techniques [15]. A policy learning technique inspired by offilne RL [6] is then applied to distill a policy network that addresses inaccuracies in the imagined actions, rewards, and states. ", "page_idx": 1}, {"type": "text", "text": "In the experiments, we build two benchmarks from Tic-Tac-Toe and Football games for PLfB. We first validate URI on Tic-Tac-Toe game, where the tutorials are manually constructed to cover the complete knowledge for decision-makings. The results show that our method achieves at least $44\\%$ net win rate against GPT-based agents without any real game interaction. In addition, we build a testbed based on football tasks, which is popular in recent studies [16] as a difficult decision-making task, and focus on policy learning from football tutorial books, which naturally contain condensed knowledge, especially information closely related to football skill acquisition, the environment and dynamics of football games, and ways to evaluate football behaviors. We collect football tutorial books from RedPajama [17]. The policy is deployed in the Google Football simulator [18] directly. Our agent controlled by the policy network could beat the built-in AI with a $37\\%$ winning rate on average while using GPT as the football agent can only achieve a $6\\%$ winning rate. The experiments demonstrate the scalability of our approach from simple board games to complex scenarios. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "LLMs have demonstrated remarkable potential in achieving human-level ability in various tasks, sparking a surge in studies investigating LLM-based autonomous agents. Based on the different roles of LLM in the agents, there are two types of work. Firstly, LLM can be used as an actionable agent directly [10]. Early works [19, 20] prefer open-loop plans and hard-code search algorithms to improve the long-term planning ability of LLM. Closed-loop planning [21\u201323] leverages environmental feedback, thereby facilitating more adaptive decision-making. Secondly, LLMs can be used to assist or accelerate the learning of agents. The form of assistance can be quite diverse. It can generate high-level plans [24, 25], (surrogate) rewards [26\u201331], transitions [32\u201334], or as an adapter to convert human instructions into structured inputs [35]. In our work, LLMs play a different role compared to all the above-mentioned types. LLMs are to understand the knowledge in the books, rehearsing the decision-making process to derive an imaginary dataset based on the knowledge. The policy to control the agent is a simple neural network distilled from the imaginary data. ", "page_idx": 1}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Markov Decision Process (MDP) is defined as a tuple of $\\mathcal{M}:=(\\mathcal{S},\\mathcal{A},T,R,\\gamma,\\rho_{0})$ [1] of the target environment, where $\\boldsymbol{S}$ is state space, $\\boldsymbol{\\mathcal{A}}$ is action space, $T:S\\times A\\to\\Delta(S)$ is the transition function $\\operatorname{\\Delta}(\\cdot)$ is the probability simplex), $R:S\\times A\\to[0,R_{\\mathrm{max}}]$ is the reward function, $\\gamma\\in[0,1)$ is the discount factor, and $\\rho_{0}$ is the initial distribution over states. A policy $\\pi:S\\to\\Delta(A)$ describes the distribution of actions for each state. ", "page_idx": 2}, {"type": "text", "text": "Retrieval Augmented Generation (RAG) [15] has shown great ability to improve LLM\u2019s generation ability for knowledge-intensive tasks. RAG enables LLMs to query external data sources to obtain relevant information before proceeding to answer questions or generate text. Formally, given an LLM M, the retrieval module ${\\bf R}(\\bar{x},\\{y_{i}\\})$ takes a textual query $x$ as input, finds the most relevant textual segment $y^{\\ast}$ by finding the most similar $\\mathbf{E}(y)$ from the knowledge database $y\\in\\{y_{i}\\}$ compared to ${\\bf E}(x)$ . Augmentation transforms the content into an additional input for the LLM\u2019s generation. In this paper, RAG is used to implement the URI algorithm. We use standard RAG techniques as the retrieval module, i.e., cosine similarity matching [36, 37], within the implementation of URI. In particular, we use GPT embedding to index the texts in the database and also the current query, then use cosine similarity to find the top- ${\\cdot n}$ matching data for downstream tasks, i.e., $\\mathbf{M}(x,[y_{1}^{*},...,y_{n}^{*}])$ . ", "page_idx": 2}, {"type": "text", "text": "Offilne RL [38] addresses the problem of learning policies from a pre-collected dataset $\\mathcal{D}$ . Existing studies can be classified into two categories: model-free and model-based methods. Model-free [5, $39-$ 44] methods learn a policy directly from the dataset $\\mathcal{D}$ through a specially designed conservative policy learning loss which usually aims to avoid policy taking actions unseen in $\\mathcal{D}$ . Model-based offilne algorithms [6, 45\u201351] first estimate the dynamics and reward model $\\hat{T}$ and $\\hat{R}$ from the dataset $\\mathcal{D}$ . The policy is learned by iterating with $\\hat{T}$ and $\\hat{R}$ . In the process, specially designed penalties $\\mathcal{R}$ are adopted to discourage the policy from visiting states where the model predictions are of high uncertainty. ", "page_idx": 2}, {"type": "text", "text": "4 Problem Formulation of Policy Learning from Tutorial Books ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The goal of Policy Learning from Tutorial Books (PLfB) is to use an algorithm ALG to learn a policy $\\hat{\\pi}^{*}=\\mathrm{ALG}(B,|\\mathcal{M}|)$ from textual books $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ , which can maximize the cumulative discounted reward $\\begin{array}{r}{\\eta(\\pi)\\,=\\,\\sum_{t}\\mathbb{E}_{a_{t}\\sim\\pi}\\gamma^{t}R(s_{t},a_{t})}\\end{array}$ , where the book is defined as $N_{b}$ segments $b_{i}$ divided by paragraphs, i.e., $B:=\\,\\{b_{1},...,b_{i},...,b_{N_{b}}\\}$ , and $|{\\mathcal{M}}|$ denotes the brief textual descriptions of the structures of MDP, including the descriptions of state space, action space, the task we faced, and the initial state distribution. $|\\mathcal{M}|$ is inevitable to be used since $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ only gives general knowledge of the target environment while $|{\\mathcal{M}}|$ defines the specific information, e.g., the exact format to interact with the simulator. Unlike standard RL, during the learning process of PLfB, the algorithm has no access to the environment. To ensure the feasibility of extracting a non-trivial policy, we make a mild assumption that $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ contains the descriptions of transition functions $\\{\\hat{T}\\}$ , and reward functions $\\{\\hat{R}\\}$ that can be regarded as the approximations of $T$ and $R$ , respectively. These descriptions could range from simple game rules to complex natural language paragraphs, depending on the books. It also contains descriptions of relatively high-quality behavior policies. ", "page_idx": 2}, {"type": "text", "text": "5 Understanding, Rehearsing, and Introspecting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we first present our motivation for the proposed three-stage framework in Sec. 5.1.   \nAfter that, we explain how we implement those stages in Sec. 5.2, 5.3, and 5.4 respectively. ", "page_idx": 2}, {"type": "text", "text": "5.1 Motivation of the Three-Stage Framework ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "As shown in Fig. 1, the methodology to solve PLfB consists of three major stages: understanding, rehearsing, and introspecting (URI). The motivation of these three modules can be easily understood by reviewing how humans learn from books: as humans, we first extract knowledge from books to extend our knowledge database. Then, to acquire skills, we often rehearse the possible consequences of applying the skill in our mind, integrating knowledge from the books with our prior life knowledge. Finally, we will re-examine the steps we took in our minds and think how we could have done better until we confirm that we know how to execute in the real world. In this paper, we mimic the above three steps via the following modules: Understanding module takes paragraphs of books as input and forms a knowledge database organized in pseudo-code. Rehearsing module iteratively takes the current imagined state as input and outputs the action, next state, and reward with guidance from the database. After gathering this imagined content to form a dataset, Introspecting module distills a policy network, which should consider errors of generations of state, action, and rewards. ", "page_idx": 2}, {"type": "image", "img_path": "Ddak3nSqQM/tmp/8d3fdfb75beee27598411b39e7a75408484abf3cfd6b34a24dc480bed4b0ae4d.jpg", "img_caption": ["Figure 2: The URI pipeline consists of three major stages: (A) Understanding: The knowledge extractor and aggregator modules process paragraphs from books to form a structured knowledge database organized as pseudocode. (B) Rehearsing: Using the knowledge database, the simulator generates and iterates through imagined states, actions, and rewards to create an extensive imaginary dataset. (C) Introspecting: The introspection module refines the policy network by evaluating and correcting errors in the generated states, actions, and rewards to ensure accurate and effective policy implementation. The pseudocode of the pipeline is in Appendix F.8. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "We give the overall architecture of our implementation of URI in Fig. 2. A detailed description of how they realize these functionalities will be discussed in the following. ", "page_idx": 3}, {"type": "text", "text": "5.2 Book Content Understanding ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The understanding module is responsible for extracting knowledge $\\mathcal{K}:=\\{K_{1},...,K_{i},...,K_{N_{K}}\\}$ from the books $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ , where $K_{i}$ denotes one piece of knowledge. For we humans, the knowledge is naturally stored in our brains. For machines, the first question is what should be the appropriate format to represent the knowledge. Since LLM has shown superior performance in code generation and codes themselves as interpretable, compact, and expressive languages [52], we also choose it as the basic format of knowledge representation. Different from previous works [10] which directly asked for runnable code for the downstream tasks using, we do not execute this code. Instead, we just use it as a more flexible and abstract description of knowledge which still maintains a rigorous control flow. As shown in Fig. 2(A), a knowledge extractor module is used as the first step. The knowledge extractor is an LLM-injected model. We iteratively ask ${K_{j}}={\\bf{M}}_{\\mathrm{{ext}}}(b_{i})$ to examine whether a paragraph is related to the decision-making elements, i.e., policy functions, reward functions, dynamics functions, and how to represent them as pseudocode. ", "page_idx": 3}, {"type": "text", "text": "Moreover, humans often learn by repeatedly reading the texts across pages and even books, updating existing knowledge with newly learned ones, and summarizing them into more general and abstract forms. A similar procedure is achieved by the knowledge aggregator module, shown in Fig. 2(A), as the second step of understanding. The knowledge aggregator is also an LLM-injected model. We iteratively ask $\\mathbf{M}_{\\mathrm{agg}}([K_{i},K_{j},\\bar{K_{l}},...])$ to aggregate relevant knowledge among different segments based on a similarity estimation of the embedding model $\\mathbf{E}$ . Formally, let $\\mathcal{K}^{0}:=\\{K_{1}^{0},...,K_{N_{K}^{0}}^{0}\\}$ be the paragraph-wise knowledge extracted by $\\mathbf{M}_{\\mathrm{ext}}$ . For the $j$ -the iteration, $[K_{o}^{j+1},K_{p}^{j+1},\\ldots]^{\\scriptscriptstyle*}{=}$ $\\mathbf{M}_{\\mathrm{agg}}([K_{i}^{j},K_{l}^{j},K_{n}^{j},...])$ , where $[K_{i}^{j},K_{l}^{j},K_{m}^{j},...]$ is from $K^{j}$ by selecting the most similar $N_{\\mathrm{agg}}$ pieces of knowledge by comparing their cosine similarity under the embedding model $\\mathbf{E}(K)$ of GPT and $o,p,i,l,n$ here denote the indexes. $[K_{o}^{j+1},K_{p}^{j+1},\\ldots]$ Kpj+1, ...] are then added to Kj+1. The iteration will stop if $|K^{j+1}|>=|K^{j}|$ , where $|\\mathcal{K}^{j}|$ is the pieces of knowledge at $j$ -th iteration. After iteration stops, the remaining knowledge pieces constitute the knowledge databases by splitting them into dynamics-related knowledge $\\kappa_{T}$ , reward-related knowledge $\\displaystyle\\kappa_{R}$ , and policy-related knowledge $\\kappa_{\\pi}$ for later modules to use. More details are provided in Appendix F.1. ", "page_idx": 3}, {"type": "text", "text": "5.3 Knowledge-based Rehearsing of Decision-Making ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "When humans develop a rough understanding of a new skill $\\pi$ from knowledge $\\kappa$ , they will usually imagine what choice they will make given certain situations and what are the possible consequences in their minds. This rehearsing procedure would help humans correct apparent mistakes and consider better actions for long-term benefits [53, 54]. ", "page_idx": 4}, {"type": "text", "text": "We implement a closed-loop generation process involving the LLM-injected dynamics function ${\\bf M}_{T}$ , reward function $\\mathbf{M}_{R}$ , and policy $\\mathbf{M}_{\\pi}$ , as depicted in Fig. 2(B). This approach resembles the model rollout in traditional model-based RL, where the policy, reward function, and dynamics function are represented by LLMs. Given the current imagined state $\\hat{s}_{t}$ , the LLMs $\\mathbf{M}_{\\pi}$ first predict the most plausible action $\\hat{a}_{t}$ . Subsequently, ${{\\bf{M}}_{T}}$ and $\\mathbf{M}_{R}$ generate the next state $\\hat{s}_{t+1}$ and the associated reward $\\hat{r}_{t}$ based on this action and the current state. In the process, a knowledge retrieval module is involved in selecting relevant knowledge pieces enhancing the LLM input with this information for LLM\u2019s predictions, e.g., $\\hat{a}_{t}=\\mathbf{M}_{\\pi}\\big(\\hat{s_{t}},\\bar{\\mathbf{R}}\\big(s_{t},\\mathcal{K}_{\\pi}\\big)\\big)$ , $\\hat{r}_{t}=\\mathbf{\\bar{M}}_{R}\\big(\\hat{s}_{t},\\hat{a}_{t},\\mathbf{R}\\bar{(}s_{t},\\mathcal{K}_{R})\\big)$ and $\\hat{s}_{t+1}={\\bf M}_{T}\\big(\\hat{s}_{t},\\hat{a}_{t},{\\bf R}(s_{t},K_{T})\\big)$ . The knowledge retrieval module includes the following two steps: ", "page_idx": 4}, {"type": "text", "text": "State-based Knowledge Scope Retrieval: The fundamental problem of standard RAG techniques, i.e., embedding-vector similarity matching, in this scenario, is the modality gap between the query $s_{t}$ and the knowledge $\\kappa$ . Standard RAG approaches aim to identify information closely related to the query, while here we need to ask the most suitable knowledge to be applied as a dynamics/reward/policy function that has the best predictions for the queried state and action. Standard RAG techniques tend to retrieve knowledge pieces that include similar text patterns as the queried states and fail to find the best knowledge for predictions. To address this, we propose a knowledge scope retrieval method that includes a simple yet effective preprocessing step to bridge the modality gap. In particular, we traverse all knowledge pieces $K_{i}\\in\\mathcal{K}$ by iteratively sampling $n_{\\mathrm{scope}}$ knowledge pieces $\\bar{\\{K_{i}\\}}_{n_{\\mathrm{scope}}}$ from the database $\\kappa$ , combining them with simulator information $|\\!\\!\\cdot\\!\\!\\!\\cdot|\\!\\!\\!|_{\\!\\!\\!\\cdot\\!\\!\\!\\cdot\\!\\!\\!\\!\\cdot}|\\!\\!\\!|$ and using LLM $\\{K_{i}^{S}\\}_{n_{\\mathrm{scope}}}=\\mathbf{M}_{\\mathrm{scope}}(\\{K_{i}\\}_{n_{\\mathrm{scope}}},|\\mathcal{M}|)$ to determine the preferred scopes $K_{i}^{S}$ for each piece of knowledge. $K_{i}^{S}$ is defined by the preferred subspace of state to use the knowledge. Then a standard RAG technique is applied to identify the most relevant knowledge scope $K_{j}^{s}$ and its relevant knowledge $K_{j}$ . Formally, $(\\{K_{j}\\},\\{K_{j}^{S}\\})={\\bf R}_{\\mathrm{scope}}(\\hat{s},K^{S})$ , where $\\kappa^{S}$ is the scopes of the knowledge database. This method is effective for embedding models in retrieving the correct knowledge as the texts of keys and queries both are about the descriptions of states. ", "page_idx": 4}, {"type": "text", "text": "Post-Retrieval: Knowledge Instantiation: Predicting based on code knowledge requires the LLM\u2019s robust understanding of code. We refine this process employing an LLM to instantiate the code $K^{I}\\,=\\,\\mathbf{M}_{\\mathrm{Inst}}(\\hat{s},|\\bar{\\mathcal{M}}|,\\{K_{j}\\})$ based on the current state $\\hat{s}$ , the simulator information description $|{\\mathcal{M}}|$ and the knowledge $\\{K_{j}\\}$ retrieved by $\\mathbf{R}_{\\mathrm{scope}}$ . Knowledge instantiation involves generating a domain-specific pseudocode based on the knowledge coded in the retrieved information, tailored to the target environment\u2019s current state and action requirements. ", "page_idx": 4}, {"type": "text", "text": "Finally, three LLMs are involved to generate the imaginary dataset $\\mathcal{D}_{\\mathrm{img}}$ , including $\\hat{a}_{t}\\,=\\,\\mathbf{M}_{\\pi}\\big(\\hat{s}_{t},\\mathbf{M}_{\\mathrm{Inst}}(s_{t},|\\mathcal{M}|,\\mathbf{R}_{\\mathrm{scope}}\\big(\\hat{s},K_{\\pi}^{S}\\big))\\big)$ , $\\hat{r}_{t}\\,=\\,{\\bf M}_{R}\\big(\\hat{s}_{t},\\hat{a}_{t},{\\bf M}_{\\mathrm{Inst}}(s_{t},|\\mathcal{M}|,{\\bf R}_{\\mathrm{scope}}\\big(\\hat{s},\\mathcal{K}_{R}^{S}\\big))\\big)$ and $\\hat{s}_{t+1}=\\mathbf{M}_{T}\\big(\\hat{s}_{t},\\hat{a}_{t},\\mathbf{M}_{\\mathrm{Inst}}(s_{t},|\\mathcal{M}|,\\mathbf{R}_{\\mathrm{scope}}\\big(\\hat{s},\\mathcal{K}_{T}^{S}\\big))\\big)$ , where ${\\boldsymbol{\\kappa}}_{\\pi}^{S}$ , $\\kappa_{R}^{S}$ , and ${\\kappa}_{T}^{S}$ are the scope of the knowledge database $\\kappa_{\\pi}$ , $\\kappa_{R}$ , and $\\kappa_{T}$ respectively. To start the rollout of a trajectory, we need a state as the initial state. It has several choices to achieve, e.g., sampled from $\\boldsymbol{S}$ , generated by another LLM, or pre-collected a small number of real states from the target environments as part of simulator information, i.e., initial state distribution $\\rho_{0}$ . Since the first choice might introduce unrealistic initial states in complex scenarios, in Football tasks, we opt for a more practical approach by pre-collecting a small set of real states from the target environments for our experimental validation. The detailed setup is in the experiment section. Besides, during the rollout over $H$ steps, we reuse instantiated knowledge to reduce the overload of LLM\u2019s callings. More details are provided in Appendix F.2. ", "page_idx": 4}, {"type": "text", "text": "5.4 Introspecting based on the Imaginary Dataset ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The direct output LLM in the rehearsing stage can be sub-optimal or incorrect. We would like to re-examine the collected data and try to distill a policy that can avoid the side effects. We can regard the data $\\mathcal{D}_{\\mathrm{img}}$ collected during the rehearsing stage as an offline dataset and apply offline RL algorithms to train an improved policy from the dataset. However, directly applying existing offilne RL algorithms over-simplifies the problem. Compared with the standard offilne setting where only the behavior policy is sub-optimal, there is an additional misalignment in the data generated during the rehearsal: the transition and the reward function estimated by the LLM are also inaccurate. Overlooking such inaccuracy would result in a policy exploiting the sub-optimal transition and reward function and cause performance degradation or even risky behaviors in the final deployment. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "To solve the problem, in this paper, we adopt the Conservative Q-learning [5] as the base offilne RL algorithm, whose learning objective is as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathcal{O}}\\operatorname*{max}_{\\pi}\\,\\alpha\\,\\left(\\mathbb{E}_{\\delta\\sim\\mathcal{D}_{\\mathrm{img}},a\\sim\\pi(a|s)}\\left[Q(\\hat{s},a)\\right]-\\mathbb{E}_{\\delta,\\hat{a}\\sim\\mathcal{D}_{\\mathrm{img}}}[Q(\\hat{s},\\hat{a})]+\\mathcal{R}(\\pi)\\right)+\\mathbb{E}_{\\delta,\\hat{a}\\sim\\mathcal{D}_{\\mathrm{img}}}[(Q(\\hat{s},\\hat{a})-\\hat{B}^{\\pi}\\hat{Q}(\\hat{s},\\hat{a}))]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\hat{\\beta}^{\\pi}\\hat{Q}$ is the bellman update operator to update the $\\mathrm{{Q}}.$ -value function [1], and the first term is to learn a policy with conservative $\\mathrm{^Q}$ value [5]. As a solution of introspecting from imaginary data, as shown in Fig. 2(C), we add the uncertainty of the reward and transition estimation as the regularization terms, $\\mathcal{R}_{R}$ and $\\mathcal{R}_{T}$ over the original reward $\\hat{r}$ output by the LLM $\\mathbf{M}_{R}$ . In practice, we adopt these regularization terms by applying them when we backup the ${\\hat{Q}}^{k}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{B}_{\\mathrm{I}}^{\\pi}\\hat{Q}(\\hat{s},\\hat{a}):=\\hat{r}-\\eta_{R}\\mathcal{R}_{R}(\\hat{s},\\hat{a})-\\eta_{T}\\mathcal{R}_{T}(\\hat{s},\\hat{a})+\\gamma\\mathbb{E}_{\\hat{s}^{\\prime}\\sim\\mathcal{D}_{\\mathrm{img}},a^{\\prime}\\sim\\pi_{k}(a^{\\prime}|\\hat{s}^{\\prime})}[Q(\\hat{s}^{\\prime},a^{\\prime})],}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\hat{s}^{\\prime}\\sim\\mathcal{D}_{\\mathrm{img}}$ is to sample the next state given $\\hat{s},\\hat{a},\\,\\eta_{R}$ and $\\eta_{T}$ are two hyper-parameters to control the weighting of the uncertainty terms. Inspired by Model-based Offline Policy Optimization (MOPO) [6], the uncertainty is estimated by an ensemble of $N_{\\mathrm{ens}}$ Gaussian models of $\\hat{T}$ and $\\hat{R}$ , which is learned by maximizing logarithmic likelihood from the imaginary dataset. Then the uncertainty is estimated by $\\mathcal{R}_{R}(s,a)=\\operatorname*{max}_{i\\in\\{1,...,N_{\\mathrm{ens}}\\}}\\sigma_{i}^{r}(s,a)$ and $\\bar{\\mathcal{R}_{T}}(s,\\bar{a})=\\mathrm{max}_{i\\in\\{1,...,N_{\\mathrm{ens}}\\}}\\,\\sigma_{i}^{T}(s,a)$ , where $\\boldsymbol{\\sigma}_{i}^{r}$ and $\\sigma_{i}^{T}$ are the $i$ -th reward and dynamics model\u2019s predicted standard deviation for $s,a$ respectively. We call the solution of CQL with $\\hat{B}_{\\mathrm{I}}^{\\pi}$ as the bellman update operator Conservative Imaginary Q-Learning (CIQL). This regularization is easy to implement and can force the policy to generalize better over the regions where the LLM outputs inconsistent next states and rewards. ", "page_idx": 5}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We build two benchmarks with increasing complexity to evaluate PLfB: a classical Tic-Tac-Toe game with discrete states and deterministic rules, and the Google Research Football environment (GRF) [55] featuring continuous states and multi-agent interactions. In Sec. 6.1, we introduce the experimental settings for both environments. In Sec. 6.2, we analyze the performance of URI against various baselines. In Sec. 6.3 and 6.4, we examine the effectiveness of our knowledge extraction and retrieval mechanisms. Finally, in Sec. 6.5-6.8, we provide a detailed analysis of the training process, including data generation, ablation studies, inference efficiency, and dataset visualization. ", "page_idx": 5}, {"type": "text", "text": "6.1 Experiment Setups ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Tic-Tac-Toe Game To build a proof-of-concept benchmark for PLfB, we first construct a Tic-TacToe (TTT) environment where players take turns placing their marks (X or O) on a $3{\\times}3$ grid, aiming to form a line of three marks. This classical game serves as an ideal testbed for two reasons: (1) it has a known optimal solution (minimax algorithm), allowing us to evaluate how close our learned policy is to optimality; (2) due to its discrete and deterministic nature, we can collect a complete set of optimal trajectories covering all possible game states. Based on these trajectories, we can use GPT to generate comprehensive tutorial texts that contain complete knowledge of game mechanics and winning strategies, thereby controlling for the impact of knowledge incompleteness on algorithm performance. In our experiments, each policy plays as \u2018X\u2019 and moves first, with performance measured by win rate, draw rate, loss rate, and net win rate (win rate minus loss rate) across different opponents. For detailed game rules and setup, please refer to Appendix A. ", "page_idx": 5}, {"type": "text", "text": "Google Research Football [55] is a physics-based 3D football simulator that supports the main football rules such as goals, fouls, corners, penalty kicks, and offside. Google Research Football (GRF) includes a built-in AI bot for the opposing team, whose difficulty can be adjusted between 0 and 1. An illustration of the game can be seen in Fig. 7. We define three custom difficulty levels for our experiments on the 11vs11 scenario: easy, medium, and hard. The difficulty levels differ in the bot\u2019s reaction time and decision-making defined in GRF, with higher difficulty corresponding to a stronger opponent. The major metric in our experiment is Goal Difference per Match (GDM), calculated as the average number of goals scored in all league matches minus the average number of goals conceded per match. For URI, for each seed, we selected the average performance of the top 3 checkpoints among all recorded checkpoints as the final performance. For more details about the GRF implementation, please refer to Appendix B. ", "page_idx": 5}, {"type": "table", "img_path": "Ddak3nSqQM/tmp/64536977ede8cb0be4841d617293710276d7415731a398706233a26e8fe0909f.jpg", "table_caption": ["Table 1: Performance of different policies in Tic-Tac-Toe. Each policy plays as $\\mathbf{\\nabla}\\times\\mathbf{\\zeta}$ and moves first, tested across 100 matches (50 for LLM-based methods). "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Datasets We utilize three datasets in our approach. For Tic-Tac-Toe, we generate tutorial texts by having GPT analyze all possible game trajectories collected from an optimal minimax policy (see Appendix A.2). For football, we collect the textbook dataset from the open-source book dataset RedPajama-1T [56], focusing on titles and abstracts related to football or soccer. After filtering, we obtain a curated set of ninety books closely aligned with the domain. For both environments, we need initial states to start our imaginary data generation process. In Tic-Tac-Toe, since we play as \u2019X\u2019 and move first, we always start from an empty board. In GRF, we sample 7,500 states from the rollout of a rule-based policy competing with the hard built-in AI. Due to limited resources, to distill the policy, we imagine 75,000 transitions via URI, which is 10 times compared to the initial states. ", "page_idx": 6}, {"type": "text", "text": "Baselines We compare URI against several baselines across both environments. LLM-as-agent directly uses a large language model (GPT 3.5) to output actions conditioned on the current state description. LLM-RAG enhances LLM-as-agent by retrieving relevant knowledge from the database extracted from the tutorial books, similar to the retrieval step in our rehearsing stage, but directly outputs the action without policy learning. For Tic-Tac-Toe, we additionally include Minimax-noise, which follows the optimal minimax strategy but with $30\\%$ random action selection to serve as a nearoptimal baseline. For GRF, we include Rule-based-AI from the Kaggle Football Competition [57], which is hand-designed and serves as a reference for the performance of hand-coded policies. Random Policy randomly chooses actions in both environments. For the implementation details about the baselines, please refer to Appendix D. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "6.2 Policy Performance Evaluation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "TTT Results We first evaluate URI\u2019s performance in the Tic-Tac-Toe environment. Tab. 1 shows the head-to-head match results between different policies, where each policy plays as $\\mathbf{\\nabla}\\cdot\\mathbf{X}^{\\bullet}$ and goes first. URI demonstrates superior performance across all opponents, achieving net win rates (win rate minus loss rate) of $+66\\%$ , $+44\\%$ , and $+52\\%$ against LLM-as-agent, LLM-RAG, and Random Policy respectively. Moreover, when playing against Minimax-noise, which introduces $30\\%$ randomness to the optimal minimax strategy, URI maintains a positive net win rate of $+26\\%$ , while all other baselines suffer negative net win rates. This indicates URI\u2019s ability to learn effective strategies from tutorial texts even in this classical game setting. ", "page_idx": 6}, {"type": "text", "text": "GRF Results The results in Tab. 2 demonstrate the superiority of the proposed URI approach compared to the baselines in the 11 vs 11 full-game scenarios of the GRF environment. The LLM-based agents, including LLM-as-agent and LLM-RAG, exhibit zero-shot task completion capabilities, outperforming the Random Policy. However, even with the use of RAG techniques, the best-performing LLM agent can only barely match the performance of the Medium-level built-in AI and fails to achieve any wins against the Hard-level built-in AI. In contrast, URI surpasses the performance of the baseline methods on all difficulty levels. Surprisingly, in the Hard task, URI achieves a higher win rate than the Rule-based Policy. We believe this is due to URI\u2019s ability to leverage knowledge from domain textbooks and generate high-quality imaginary data for policy learning, enabling it to learn more adaptable and robust policies compared to the hand-crafted rule-based AI. ", "page_idx": 6}, {"type": "text", "text": "These results across both simple and complex environments highlight the scalability of URI in learning strong policies from well-defined board games to challenging scenarios. ", "page_idx": 6}, {"type": "text", "text": "Table 2: Performance Comparison of Different Policies Against Built-in AI Levels in a GRF 11 vs 11 settings, where the performance of URI is averaged among three different seeds, LLM-as-agent, LLM-RAG is tested with 10 matches, and URI policy and random policy is tested with 40 matches. ", "page_idx": 7}, {"type": "table", "img_path": "Ddak3nSqQM/tmp/2d7f9878a647b009067c36731e0ef3f1cbda8a104f7c998c6845d443ebb0dd59.jpg", "table_caption": ["6.3 Effectiveness of Code-based Knowledge Extraction and Aggregation "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "According to Sec. 5.2, we perform an iterative process of code extraction and aggregation to understand the tutorial books and obtain executable knowledge. As shown in Fig. 3(a), for Tic-Tac-Toe, the initial extraction yields approximately 600 code segments which are effectively consolidated through the aggregation process. Similarly, in the more complex football domain (Fig. 3(b)), the number of code segments for the dynamics, policy, and reward functions decreases significantly over the aggrega", "page_idx": 7}, {"type": "image", "img_path": "Ddak3nSqQM/tmp/f6f1b6fd8c51c956e56d31f41fe1833a85c310b9de1a465e5142d225e377468c.jpg", "img_caption": ["Figure 3: Knowledge Segment Aggregation. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "tion rounds. Through iterative aggregation, the number of code segments, decreases significantly in both environments, which helps condense the extracted knowledge into a more compact and coherent form. This consistent pattern across domains of varying complexity demonstrates the robustness of our knowledge extraction and aggregation approach. ", "page_idx": 7}, {"type": "text", "text": "6.4 Correlation between Knowledge Embedding and Current State Embedding ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To validate the effectiveness of our code embedding method mentioned in Sec. 5.3, we conducted experiments in GRF task comparing it with two baselines: Vector Embedding, which directly compares the state and code embeddings, and Code Summary, which first summarizes the code segments before comparing the embeddings. We evaluate the top-15 hit rate in a hand-crafted test dataset. The observations and actions in the dataset we used are collected by the rule-based policy interacting with the real environment, while the ground-truth codes are labeled by GPT and aggregated using a similar way as in Sec. 5.2. As shown in Fig. 4(a), our method significantly outperforms the other two baselines on both pre-trained language models. The hit rate, which measures the proportion of relevant code segments retrieved, is consistently higher for URI across all three random seeds, demonstrating its robustness and superiority in improving the correlation between the state embedding and the code embedding. These results highlight the importance of learning a dedicated matcher for effective code retrieval in our framework. ", "page_idx": 7}, {"type": "text", "text": "6.5 Tracing the Data Generation Process ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In URI, it is crucial to guarantee that the imaginary data include states, actions, and rewards. The task is non-trivial since it has to involve several transformations to align the gap between textual contents in books and decision-making trajectories in MDP. To demonstrate this process in detail, we use the football environment as a case study. We trace how $\\mathbf{M}_{\\pi}$ outputs an imaginary action $\\hat{a}$ on an imaginary state $\\hat{s}$ , which is actually collected at the start of a football game in GRF. We record all the intermediate outputs to trace the action generation process. The result is shown in Fig. 5. It is clearly shown that the output action \u201cdribble\u201d is fully supported by the logic in\u201chas_space\u201d branch in $K_{I}$ , \u201cassess_forward_decisions\u201d function in $K$ , and the paragraph of \u201cPlayer in possession of the ball\u201d in the raw content of tutorial books. However, we would like to point out that there are also some examples that demonstrate LLMs having hallucinations in the generation, and the retrieved module might also miss the ground-truth piece of knowledge. More results are provided in Appendix H. These results indicate the necessity of introspecting in URI. The relevant ablation studies are in Sec. 6.6. ", "page_idx": 7}, {"type": "image", "img_path": "Ddak3nSqQM/tmp/727721df5d004bc0e6d099e0fab152cc30624f9cada7618fff77d7eee7452a1b.jpg", "img_caption": ["Figure 4: (a). Comparison of different code retrieval methods on two pre-trained language models. (b). Performance comparison of different variants of the URI framework in the GRF. This figure illustrates the average GDM, win, draw, and lose rates among the three levels of built-in AIs. The error bars in the figure indicate the standard deviation from the mean performance for each configuration in three random seeds. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "Ddak3nSqQM/tmp/5e6aca1f196430c231d6ac8d018defe3a2d20fe4391217c1bde7399844fbb991.jpg", "img_caption": ["Figure 5: An example of the URI data generation process in the football game. The imaginary state $\\hat{s}$ is collected from the real environment, which is 12 timesteps of a football game, while the rendered image is the corresponding scenario generated by the simulator. The imaginary action is \u201cdribble\u201d, where the logic is supported by the \u201chas_space\u201d branch in $K_{I}$ . Based on this, we bold the relevant information in the predecessor nodes and skip irrelevant information with the ellipsis \u201c... ...\u201d. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "6.6 Importance of the components in URI ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We validate the effectiveness of the rehearsing technique presented in Sec. 5.3 and the CIQL method introduced in Sec. 5.4 through ablation studies. Specifically, we constructed the following variants of the URI framework in GRF task: (1) URI-w/o-rehearsing, which solves the policy without using the rehearsing dataset and relies on a pre-collected dataset of 7,500 samples for offline RL policy training; (2) URI-w/o-penalty, where penalties $\\eta_{T}$ and $\\eta_{R}$ are zero, same as the standard CQL for policy learning; (3) CQL-real-data, where we collect real data of equivalent scale to the rehearsing-generated data using rule-based AI and apply standard CQL for offilne RL policy training. The results are shown in Fig. 4(b). ", "page_idx": 8}, {"type": "text", "text": "Firstly, URI-w/o-rehearsing demonstrates that without generating a substantial amount of imaginary data through rehearsing, solely relying on offilne RL algorithms to train a policy with our pre-collected ", "page_idx": 8}, {"type": "text", "text": "7,500 samples is ineffective. The results indicate that it cannot even beat the AI on Easy difficulty, though it still performs better than a random strategy. URI-w/o-penalty underscores the importance of penalizing the uncertain aspects of the outcomes generated from the imaginary data. Neglecting this penalty leads to results worse than those of URI-w/o-rehearsing. Finally, our method slightly outperforms CQL-real-data. We attribute this improvement to the strategies inferred from prior knowledge by LLMs, which are partially superior to built-in AI behaviors, or possibly because LLMs generate more diverse data. The successful ablation results in such a challenging domain illustrate the considerable potential of PLfB. ", "page_idx": 9}, {"type": "text", "text": "6.7 Efficiency in inference ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Lower cost of inference is one of the benefits for agents controlled by URI policies. We compare the inference time per action for different methods in the GRF task in Tab. 3. Our approach takes only 0.009 seconds on average to choose an action, which is significantly faster at least 300 times than using LLMs directly as agents (2.84 seconds) or with retrieval-augmented generation (RAG) (4.12 seconds) in complex environments. This makes URI more suitable for real-time decision-making in the football simulator. The efficiency gain comes from the fact that URI distills the knowledge from the LLM into a compact policy network, which can be executed quickly without the need for expensive LLM inference at each step. ", "page_idx": 9}, {"type": "table", "img_path": "Ddak3nSqQM/tmp/20417aa9083be6158f9afa538f88ac4c39f800a69ae5bea1d1e9a92937a3b8ec.jpg", "table_caption": ["Table 3: Comparison of inference time per action for different methods in GRF. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6.8 Imaginary Dataset Visualization ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We visualize the imaginary datasets to analyze the quality of generation and uncertainty estimation. Here we present the results from the GRF, while the TTT results can be found in Appendix G.1. We choose t-SNE [58] as the visualization method and project the imaginary dataset and a real dataset collected by the rule-based policy into 2-d space for comparison. The results are in Fig. 6. The \"real data\" marks the data collected by the rule-based policy, while \"low-unc. data\" and \"high-unc. data\" represent segments of the imaginary dataset categorized by their uncertainty scores $R_{T}$ and $R_{R}$ falling within the lower and upper $50\\%$ percentiles, respectively. The real data and the imaginary data follow a similar data distribution, which indicates the effectiveness of the rehearsing process in URI. Besides, as highlighted by the yellow dashed circles, the uncertainty score also identifies parts of the clusters that are out of the real data distribution, which will be penalized when introspecting via CIQL. ", "page_idx": 9}, {"type": "image", "img_path": "Ddak3nSqQM/tmp/439994a0622fb23f947b2b78b21a35a8c46f711fd6976b86a2931b9ddcb3b45e.jpg", "img_caption": ["Figure 6: Visualization of the projected distributions for real and imaginary datasets in the football environment. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "7 Conclusion and Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Inspired by the learning behavior of humans when they try to acquire a new skill, we propose PLfB that trains an agent from books, instead of numerous interaction data with the real environment. We also implement a practical algorithm of understanding, rehearsing, and introspecting modules to realize such a learning paradigm. The result of deploying our method in Tic-Tac-Toe and football game environments demonstrates a huge improvement in the winning rate over the baseline methods. This success proves the feasibility of utilizing knowledge stored in various written texts for decisionmaking agents, which was neglected by the community for a long time. ", "page_idx": 9}, {"type": "text", "text": "One major limitation of URI is its implicit dependence on the quality of the \"tutorial books\": they should cover sufficiently the dynamics, policy, and rewards of the targeted environment so that relevant knowledge and imaginary data can be extracted to train the policy. It is also important to develop metrics evaluating the textual data quality to decide whether to use URI. ", "page_idx": 9}, {"type": "text", "text": "Besides, we hope that the promising result in current experiments will initiate more research on PLfB. We leave more discussion of several interesting open problems in PLfBthat URI does not address in Appendix I. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work is supported by the Jiangsu Science Foundation (BK20243039). The authors thank anonymous reviewers for their helpful discussions and suggestions for improving the article. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.   \n[2] Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-learning. In Proceedings of the AAAI conference on artificial intelligence, volume 30, 2016. [3] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [4] Jonas Degrave, Federico Felici, Jonas Buchli, Michael Neunert, Brendan Tracey, Francesco Carpanese, Timo Ewalds, Roland Hafner, Abbas Abdolmaleki, Diego de Las Casas, et al. Magnetic control of tokamak plasmas through deep reinforcement learning. Nature, 602(7897): 414\u2013419, 2022.   \n[5] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline reinforcement learning. Advances in Neural Information Processing Systems, 33: 1179\u20131191, 2020.   \n[6] Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea Finn, and Tengyu Ma. Mopo: Model-based offline policy optimization. Advances in Neural Information Processing Systems, 33:14129\u201314142, 2020.   \n[7] Soroush Nasiriany, Abhiram Maddukuri, Lance Zhang, Adeet Parikh, Aaron Lo, Abhishek Joshi, Ajay Mandlekar, and Yuke Zhu. RoboCasa: Large-scale simulation of everyday tasks for generalist robots. arXiv preprint arXiv:2406.02523, 2024.   \n[8] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   \n[9] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.   \n[10] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023.   \n[11] Andrew Szot, Max Schwarzer, Harsh Agrawal, Bogdan Mazoure, Rin Metcalf, Walter Talbott, Natalie Mackraz, R Devon Hjelm, and Alexander T Toshev. Large language models as generalizable policies for embodied tasks. In The Proceedings of the 12th International Conference on Learning Representations, 2023.   \n[12] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents: A survey. arXiv preprint arXiv:2309.07864, 2023.   \n[13] Sihao Hu, Tiansheng Huang, Fatih Ilhan, Selim Tekin, Gaowen Liu, Ramana Kompella, and Ling Liu. A survey on large language model-based game agents. arXiv preprint arXiv:2404.02039, 2024.   \n[14] Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh V. Chawla, Olaf Wiest, and Xiangliang Zhang. Large language model based multi-agents: A survey of progress and challenges. arXiv preprint arXiv:2402.01680, 2024.   \n[15] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. volume 33, pages 9459\u20139474, 2020.   \n[16] Zhe Wang, Petar Velic\u02c7kovic\u00b4, Daniel Hennes, Nenad Toma\u0161ev, Laurel Prince, Michael Kaisers, Yoram Bachrach, Romuald Elie, Li Kevin Wenliang, Federico Piccinini, William Spearman, Ian Graham, Jerome Connor, Yi Yang, Adri\u00e0 Recasens, Mina Khan, Nathalie Beauguerlange, Pablo Sprechmann, Pol Moreno, Nicolas Heess, Michael Bowling, Demis Hassabis, and Karl Tuyls. Tacticai: an ai assistant for football tactics. Nature Communications, 15(1):1906, Mar 2024. ISSN 2041-1723.   \n[17] Together Computer. Redpajama: An open source recipe to reproduce llama training dataset, 2023. URL https://github.com/togethercomputer/RedPajama-Data.   \n[18] Karol Kurach, Anton Raichuk, Piotr Stanczyk, Michal Zajac, Olivier Bachem, Lasse Espeholt, Carlos Riquelme, Damien Vincent, Marcin Michalski, Olivier Bousquet, and Sylvain Gelly. Google research football: A novel reinforcement learning environment. arXiv preprint arXiv:1907.11180.   \n[19] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824\u201324837, 2022.   \n[20] Zirui Zhao, Wee Sun Lee, and David Hsu. Large language models as commonsense knowledge for large-scale task planning. arXiv preprint arXiv:2305.14078, 2023.   \n[21] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022.   \n[22] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through planning with language models. arXiv preprint arXiv:2207.05608, 2022.   \n[23] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. In Proceedings of the 37th Conference on Neural Information Processing Systems, 2023.   \n[24] Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone. Llm+ p: Empowering large language models with optimal planning proficiency. arXiv preprint arXiv:2304.11477, 2023.   \n[25] Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M Sadler, Wei-Lun Chao, and Yu Su. Llm-planner: Few-shot grounded planning for embodied agents with large language models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2998\u20133009, 2023.   \n[26] Meng Cao, Lei Shu, Lei Yu, Yun Zhu, Nevan Wichers, Yinxiao Liu, and Lei Meng. Drlc: Reinforcement learning with dense rewards from llm critic. arXiv preprint arXiv:2401.07382, 2024.   \n[27] Minae Kwon, Sang Michael Xie, Kalesha Bullard, and Dorsa Sadigh. Reward design with language models. arXiv preprint arXiv:2303.00001, 2023.   \n[28] Yue Wu, Yewen Fan, Paul Pu Liang, Amos Azaria, Yuanzhi Li, and Tom M Mitchell. Read and reap the rewards: Learning to play atari with the help of instruction manuals. volume 36, 2024.   \n[29] Jing-Cheng Pang, Pengyuan Wang, Kaiyuan Li, Xiong-Hui Chen, Jiacheng Xu, Zongzhang Zhang, and Yang Yu. Language model self-improvement by reinforcement learning contemplation. In Proceedings of the 12th International Conference on Learning Representations, Vienna, Austria, 2024.   \n[30] Ziyan Wang, Meng Fang, Tristan Tomilin, Fei Fang, and Yali Du. Safe multi-agent reinforcement learning with natural language constraints. arXiv preprint arXiv:2405.20018, 2024.   \n[31] Martin Klissarov, Pierluca D\u2019Oro, Shagun Sodhani, Roberta Raileanu, Pierre-Luc Bacon, Pascal Vincent, Amy Zhang, and Mikael Henaff. Motif: Intrinsic motivation from artificial intelligence feedback. In Proceedings of the 12th International Conference on Learning Representations, 2024.   \n[32] Abhay Zala, Jaemin Cho, Han Lin, Jaehong Yoon, and Mohit Bansal. Envgen: Generating and adapting environments via llms for training embodied agents. arXiv preprint arXiv:2403.12014, 2024.   \n[33] Ruiyang Ren, Peng Qiu, Yingqi Qu, Jing Liu, Wayne Xin Zhao, Hua Wu, Ji-Rong Wen, and Haifeng Wang. Bases: Large-scale web search user simulation with large language model based agents. arXiv preprint arXiv:2402.17505, 2024.   \n[34] Jessy Lin, Yuqing Du, Olivia Watkins, Danijar Hafner, Pieter Abbeel, Dan Klein, and Anca D. Dragan. Learning to model the world with language. In Proceedings of the 41st International Conference on Machine Learning, 2024.   \n[35] Hengyuan Hu and Dorsa Sadigh. Language instructed reinforcement learning for human-ai coordination. In Proceedings of the 40th International Conference on Machine Learning, 2023.   \n[36] Omar Khattab and Matei Zaharia. Colbert: Efficient and effective passage search via contextualized late interaction over bert. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, pages 39\u201348, 2020.   \n[37] Ehsan Kamalloo, Xinyu Zhang, Odunayo Ogundepo, Nandan Thakur, David Alfonso-Hermelo, Mehdi Rezagholizadeh, and Jimmy Lin. Evaluating embedding apis for information retrieval. arXiv preprint arXiv:2305.06300, 2023.   \n[38] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.   \n[39] Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning. arXiv preprint arXiv:1911.11361, 2019.   \n[40] Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to offilne reinforcement learning. Advances in neural information processing systems, 34:20132\u201320145, 2021.   \n[41] Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing offpolicy q-learning via bootstrapping error reduction. Advances in Neural Information Processing Systems, 32, 2019.   \n[42] Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on offline reinforcement learning. In International Conference on Machine Learning, pages 104\u2013114. PMLR, 2020.   \n[43] Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit q-learning. arXiv preprint arXiv:2110.06169, 2021.   \n[44] Yuhang Ran, Yi-Chen Li, Fuxiang Zhang, Zongzhang Zhang, and Yang Yu. Policy regularization with dataset constraint for offline reinforcement learning. In Proceedings of the 40th International Conference on Machine Learning, 2023.   \n[45] Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-based offline reinforcement learning. volume 33, pages 21810\u201321823, 2020.   \n[46] Xiong-Hui Chen, Fan-Ming Luo, Yang Yu, Qingyang Li, Zhiwei Qin, Wenjie Shang, and Jieping Ye. Offilne model-based adaptable policy learning for decision-making in out-of-support regions. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.   \n[47] Yihao Sun, Jiaji Zhang, Chengxing Jia, Haoxin Lin, Junyin Ye, and Yang Yu. Model-bellman inconsistency for model-based offline reinforcement learning. In Proceedings of the 40th International Conference on Machine Learning, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "[48] Fan-Ming Luo, Tian Xu, Xingchen Cao, and Yang Yu. Reward-consistent dynamics models are strongly generalizable for offline reinforcement learning. In Proceedings of the 12th International Conference on Learning Representations, 2024. ", "page_idx": 13}, {"type": "text", "text": "[49] Ruifeng Chen, Chengxing Jia, Zefang Huang, Tian-Shuo Liu, Xu-Hui Liu, and Yang Yu. Offilne transition modeling via contrastive energy learning. In Proceedings of the 41th International Conference on Machine Learning, 2024. ", "page_idx": 13}, {"type": "text", "text": "[50] Ruifeng Chen, Xiong-Hui Chen, Yihao Sun, Siyuan Xiao, Minhui Li, and Yang Yu. Policyconditioned environment models are more generalizable. In Proceedings of the 41st International Conference on Machine Learning, 2024. ", "page_idx": 13}, {"type": "text", "text": "[51] Xiong-Hui Chen, Yang Yu, Zhengmao Zhu, Zhihua Yu, Zhenjun Chen, Chenghe Wang, Yinan Wu, Rong-Jun Qin, Hongqiu Wu, Ruijin Ding, and Fangsheng Huang. Adversarial counterfactual environment model learning. In Advances in Neural Information Processing Systems 36, 2023. ", "page_idx": 13}, {"type": "text", "text": "[52] Ke Yang, Jiateng Liu, John Wu, Chaoqi Yang, Yi R Fung, Sha Li, Zixuan Huang, Xu Cao, Xingyao Wang, Yiquan Wang, et al. If llm is the wizard, then code is the wand: A survey on how code empowers large language models to serve as intelligent agents. arXiv preprint arXiv:2401.00812, 2024. ", "page_idx": 13}, {"type": "text", "text": "[53] Chengxing Jia, Chenxiao Gao, Hao Yin, Fuxiang Zhang, Xiong-Hui Chen, Tian Xu, Lei Yuan, Zongzhang Zhang, Yang Yu, and Zhi-Hua Zhou. Policy rehearsing: Training generalizable policies for reinforcement learning. In Proceedings of the 12th International Conference on Learning Representations, 2024. ", "page_idx": 13}, {"type": "text", "text": "[54] Jing-Cheng Pang, Si-Hang Yang, Kaiyuan Li, Jiaji Zhang, Xiong-Hui Chen, Nan Tang, and Yang Yu. Knowledgeable agents by offline reinforcement learning from large language model rollouts. In The 38th Annual Conference on Neural Information Processing Systems, 2024. ", "page_idx": 13}, {"type": "text", "text": "[55] Karol Kurach, Anton Raichuk, Piotr Stan\u00b4czyk, Micha\u0142 Zaj a\u02dbc, Olivier Bachem, Lasse Espeholt, Carlos Riquelme, Damien Vincent, Marcin Michalski, Olivier Bousquet, et al. Google research football: A novel reinforcement learning environment. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 4501\u20134510, 2020. ", "page_idx": 13}, {"type": "text", "text": "[56] TogetherComputer. Redpajama: an open dataset for training large language models, 2023. URL https://github.com/togethercomputer/RedPajama-Data. ", "page_idx": 13}, {"type": "text", "text": "[57] Sarvar Anvarov. Solution ranked 35th in kaggle football competition. https://github.com/ Sarvar-Anvarov/Google-Research-Football, 2020. ", "page_idx": 13}, {"type": "text", "text": "[58] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine Learning Research, 9(86):2579\u20132605, 2008. ", "page_idx": 13}, {"type": "text", "text": "[59] Jerry Liu. LlamaIndex, 11 2022. URL https://github.com/jerryjliu/llama_index. ", "page_idx": 13}, {"type": "text", "text": "[60] Takuma Seno and Michita Imai. d3rlpy: An offilne deep reinforcement learning library. Journal of Machine Learning Research, 23(315):1\u201320, 2022. ", "page_idx": 13}, {"type": "text", "text": "[61] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-languageaction models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023. ", "page_idx": 13}, {"type": "text", "text": "[62] Leo Gao. An empirical exploration in quality filtering of text data. arXiv preprint arXiv:2109.00698, 2021. ", "page_idx": 13}, {"type": "text", "text": "[63] Alexander Wettig, Aatmik Gupta, Saumya Malik, and Danqi Chen. QuRating: Selecting highquality data for training language models. In Proceedings of the 41st International Conference on Machine Learning, Vienna, Austria, 2024. ", "page_idx": 13}, {"type": "text", "text": "[64] Simin Fan, Matteo Pagliardini, and Martin Jaggi. DOGE: domain reweighting with generalization estimation. In Proceedings of the 41st International Conference on Machine Learning, 2024. ", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Tic-Tac-Toe (TTT) Task Setup 16   \nA.1 Environment . 16   \nA.2 Tutorial Text Generation 16 ", "page_idx": 14}, {"type": "text", "text": "B Google Research Football (GRF) Tasks Setup 17 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 State Space 17   \nB.2 Action Space 21   \nB.3 Transition 22 ", "page_idx": 14}, {"type": "text", "text": "C Additional Related Work 23 ", "page_idx": 14}, {"type": "text", "text": "D Baseline Implementations for TTT 23   \nD.1 LLM-as-agent 23   \nD.2 LLM-RAG 24   \nD.3 Additional Baselines 25   \nE Baseline Implementations for GRF 25   \nE.1 LLM-as-agent 25   \nE.2 LLM-RAG 27   \nF URI Implementation 28   \nF.1 Book Content Understanding 28   \nF.2 Knowledge-based Rehearsing of Decision-Making . . 32   \nF.3 Tic-Tac-Toe Prompt Configurations . . . 37   \nF.4 Football Prompt Configuration 39   \nF.5 Introspecting based on the Imaginary Dataset . . 43   \nF.6 General Recipe for Prompt Design 43   \nF.7 Hyper-parameters 43   \nF.8 URI Algorithm 44   \nG Additional Ablations 45   \nG.1 Data Aggregation Visualization for Tic-Tac-Toe 45   \nG.2 Performance Comparison with Additional Baselines in GRF 45 ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "H Examples of Data Generation and Policy Execution Videos 45 ", "page_idx": 14}, {"type": "text", "text": "I Open Problems and Future Directions 45 ", "page_idx": 14}, {"type": "text", "text": "J Compute Resources 46 ", "page_idx": 14}, {"type": "text", "text": "K Broader Impact Statement 46 ", "page_idx": 14}, {"type": "text", "text": "A Tic-Tac-Toe (TTT) Task Setup ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Environment ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Tic-Tac-Toe is a classical two-player game played on a $3{\\times}3$ grid. Players take turns placing their symbols (X or O) in empty cells, with the goal of forming a line of three marks horizontally, vertically, or diagonally. The game state is represented by a 9-dimensional vector, where each dimension corresponds to a cell that can be empty (0), marked by X (1), or marked by O (2). The action space consists of integers from 0 to 8, representing the cell indices for placement. ", "page_idx": 15}, {"type": "text", "text": "A.2 Tutorial Text Generation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.2.1 Data Collection ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We first collect comprehensive game trajectories by having a minimax player (optimal policy) play against all possible opponent moves. The data collection process: ", "page_idx": 15}, {"type": "text", "text": "1. Start from empty board   \n2. For X\u2019s moves (minimax player): \u2022 Use optimal minimax strategy \u2022 First move randomized for diversity   \n3. For O\u2019s moves (opponent): \u2022 Explore all possible valid move \u2022 Record complete trajectory   \n4. Store trajectories with: \u2022 Complete state sequence \u2022 Actions taken \u2022 Player turns \u2022 Game outcomes \u2022 Winning/losing patterns ", "page_idx": 15}, {"type": "text", "text": "This systematic exploration generates a dataset covering all possible game scenarios under optimal play. ", "page_idx": 15}, {"type": "text", "text": "A.2.2 Knowledge Extraction ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We use GPT to analyze these trajectories and extract strategic knowledge. The prompt are used as follow: ", "page_idx": 15}, {"type": "text", "text": "Prompt 1: Knowledge Extraction Prompt ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Given a Tic-Tac-Toe game trajectory where X is known to be the oracle (optimal player), analyze the game and provide extracted knowledge about various aspects: 1. Observe the entire game trajectory, including moves by both X and O. 2. Formulate concise statements of knowledge that capture insights about: - Provided Trajectory Analysis: Overall assessment of game flow and key decisions - Learned Game Mechanics: Turn-taking, move validity, board state changes - Learned Winning Conditions: Victory achievement and prevention - Learned Strategic Principles: General strategies demonstrated by optimal player Your output should consist of 3-4 concise statements, each focusing on one aspect. These statements should be generalizable and applicable broadly. ", "page_idx": 15}, {"type": "table", "img_path": "Ddak3nSqQM/tmp/58c0bed2c5484b0c58a3d4f93910175aa5774167bca09b64fae1d7fdd3c17c4d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "A.2.3 Generated Knowledge ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The complete knowledge base is available at: https://github.com/ziyan-wang98/URI_video_ NeurIPS/blob/main/tic_tac_toe_knowledge.jsonl ", "page_idx": 16}, {"type": "text", "text": "Representative examples include: ", "page_idx": 16}, {"type": "text", "text": "1. Strategic Principles: ", "page_idx": 16}, {"type": "text", "text": "\"Prioritize creating two potential winning lines simultaneously to force the opponent into a defensive position and increase chances of victory.\" ", "page_idx": 16}, {"type": "text", "text": "2. Position Control: ", "page_idx": 16}, {"type": "text", "text": "\"Control the center and occupy adjacent positions to create multiple threats while forcing opponent into defensive posture.\" ", "page_idx": 16}, {"type": "text", "text": "3. Opening Strategy: ", "page_idx": 16}, {"type": "text", "text": "\"Claim the center early when playing $\\boldsymbol{\\mathrm{X}}$ to maximize board control and create multiple winning opportunities.\" ", "page_idx": 16}, {"type": "text", "text": "4. Tactical Patterns: ", "page_idx": 16}, {"type": "text", "text": "\"When establishing dominance, prioritize claiming center and a row/column to establish multiple winning opportunities while blocking opponent threats.\" ", "page_idx": 16}, {"type": "text", "text": "This knowledge base provides complete coverage of optimal strategies, serving as ground truth for evaluating PLfB\u2019s ability to learn from textual instructions. ", "page_idx": 16}, {"type": "text", "text": "B Google Research Football (GRF) Tasks Setup ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we mainly focus on the GRF implementation2 and the modifications we made under this environment. ", "page_idx": 16}, {"type": "text", "text": "B.1 State Space ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The raw state of each player contains information about the game state, the ball and all players on the pitch. The game state includes scores, game modes indicating free kicks, corner kicks or other game stages, and game time represented as steps. The ball information includes its spatial position, direction, and an indicator for ball ownership (identifying the player and team that possess it). The player information comprises position, direction, roles, yellow card records, and more. For players on the opposite team, positions and directions are mirrored. The details of the raw state are as follows: ", "page_idx": 16}, {"type": "image", "img_path": "Ddak3nSqQM/tmp/044e56dd5ac631a569f63748d05fe494be150639321a56082999426bc612321e.jpg", "img_caption": ["Figure 7: Illustration of the game in football simulator. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "\u2022 Ball Information: ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "\u2013 ball \u2013 $[x,y,z]$ position of the ball.   \n\u2013 ball_direction \u2013 $[x,y,z]$ ball movement vector.   \n\u2013 ball_rotation \u2013 $\\left[x,y,z\\right]$ rotation angles in radians.   \n\u2013 ball_owned_team \u2013 $\\{-1,0,1\\}$ , where $^{-1}$ indicates the ball is not owned, 0 denotes the left team, and 1 the right team.   \n\u2013 ball_owned_player \u2013 {0..N-1} integer denoting the index of the player owning the ball. ", "page_idx": 17}, {"type": "text", "text": "\u2022 Left Team: ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "\u2013 left_team \u2013 N-elements vector with $[x,y]$ positions of players.   \n\u2013 left_team_direction \u2013 N-elements vector with $[x,y]$ movement vectors of players.   \n\u2013 left_team_tired_factor \u2013 N-elements vector of floats in the range {0..1}. 0 means the player is not tired at all.   \n\u2013 left_team_yellow_card \u2013 N-elements vector of integers denoting the number of yellow cards a given player has (0 or 1).   \n\u2013 left_team_active \u2013 N-elements vector of booleans denoting whether a given player is playing the game (False means the player got a red card).   \n\u2013 left_team_roles \u2013 N-elements vector denoting roles of players, where: \\* $0=\\mathfrak{e}$ e_PlayerRole_GK - goalkeeper, $^*$ $1=\\mathsf{e}_{\\cdot}$ _PlayerRole_CB - centre back, \\* $^{2=}$ e_PlayerRole_LB - left back, ", "page_idx": 17}, {"type": "text", "text": "\u2022 Right Team: Same attributes as for the left team. ", "page_idx": 17}, {"type": "text", "text": "\u2022 Controlled Player Information: ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "\u2013 active \u2013 {0..N-1} integer denoting the index of the controlled players. \u2013 designated \u2013 {0..N-1} integer denoting the index of the designated player. \u2013 sticky_actions \u2013 10-elements vectors of 0s or 1s denoting whether a corresponding action is active. ", "page_idx": 17}, {"type": "text", "text": "\u2022 Match State: ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "\u2013 score \u2013 Pair of integers denoting the number of goals for the left and right teams, respectively.   \n\u2013 steps_left \u2013 How many steps are left till the end of the match.   \n\u2013 game_mode \u2013 Current game mode. ", "page_idx": 17}, {"type": "text", "text": "For the imaginary dataset generation, we created our own Imaginary state, which includes the following information: ", "page_idx": 17}, {"type": "text", "text": "\u2022 Game Information: ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "\u2013 Sticky actions: A list of currently active sticky actions.   \n\u2013 Game mode: The current game mode.   \n\u2013 Score: The current score of the game.   \n\u2013 Time: The current game time in minutes and seconds.   \n\u2013 Active player: The index of the currently active player.   \n\u2013 Active player role: The role of the currently active player.   \n\u2013 Ball ownership: The team currently in possession of the ball (none, left team, or right team).   \n\u2013 Ball ownership player: The index of the player currently in possession of the ball.   \n\u2013 Ball zone: The zone where the ball is located.   \n\u2013 Ball direction: The direction in which the ball is moving. ", "page_idx": 18}, {"type": "text", "text": "\u2022 Player Information: ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "\u2013 Team: The team the player belongs to (left team or right team).   \n\u2013 Role: The role of the player.   \n\u2013 Zone: The zone where the player is located.   \n\u2013 Direction: The direction the player is facing. ", "page_idx": 18}, {"type": "text", "text": "The imaginary state is designed to provide a more compact and informative representation of the game state compared to the raw state. It extracts and processes the relevant information from the raw state, making it easier to understand and use for generating imaginary data. ", "page_idx": 18}, {"type": "text", "text": "The sticky actions, game mode, score, time, active player, and active player role provide a high-level overview of the current game state. The ball ownership and ball ownership player indicate which team and player are currently in control of the ball. The ball zone and direction give spatial information about the ball\u2019s location and movement. ", "page_idx": 18}, {"type": "text", "text": "For each player, the Imaginary state includes their team, role, zone, and direction. This information helps in understanding the positioning and orientation of the players on the pitch. ", "page_idx": 18}, {"type": "text", "text": "By including these key pieces of information in the Imaginary state, we aim to capture the essential aspects of the game state that are relevant for generating realistic and diverse imaginary data. The imaginary state serves as a preprocessed and structured representation of the raw state, making it more suitable for the subsequent steps in the imaginary data generation process. ", "page_idx": 18}, {"type": "text", "text": "Here, we show an example of imaginary state, ", "page_idx": 18}, {"type": "image", "img_path": "Ddak3nSqQM/tmp/2938dcb7bfb61a6831fb1b706db0dc75aa8d7ef25da117f26834ef19768bc036.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "To enhance the understanding of player and ball positions beyond mere coordinates, our methodology involves dividing the football field into a grid of $20\\mathrm{~x~}12$ rectangular zones. This granular zoning approach serves a dual purpose: firstly, it abstracts the complex spatial dynamics of the game into a more manageable form, allowing our LLM to process and generate data with increased effectiveness. Secondly, it provides a framework for more accurately simulating the spatial strategies and movements employed in real-world football scenarios. Each zone, defined by specific dimensions, acts as a unique spatial identifier, offering precise reference points for player positioning and ball location. This system not only simplifies the representation of the field but also enriches the strategic depth of the game model, as actions and decisions can be tailored to the distinct characteristics of each zone. By adopting this zoning strategy, we aim to bridge the gap between the high-level strategic understanding required for football and the detailed, positional awareness needed to implement those strategies effectively within the simulated environment. ", "page_idx": 19}, {"type": "text", "text": "The simulator information $|{\\mathcal{M}}|$ includes the description of the state space $\\boldsymbol{S}$ . What we use in this paper is as follows. ", "page_idx": 19}, {"type": "text", "text": "\u2022 First, it provides information such as the time and score of the match. ", "page_idx": 19}, {"type": "text", "text": "\u2022 Second, which side has control of the ball, and the active player in the left team, you need to propose corresponding policies to him. ", "page_idx": 19}, {"type": "text", "text": "\u2022 Next, we present the position and role information of each player: In this text description, the football grass field is divided into 240 zones.   \n\u2022 We use the zone $(\\mathbf{x},\\,\\mathbf{y})$ to express the position of the player. \"x\" is the distance from the left team\u2019s penalty area to the right team\u2019s penalty area, ranging from 1 to 20, and y is the distance from the lower corner to the upper corner flag, ranging from 1 to 12. This means that the center circle position of the field is zone (10, 6), where the game start.   \n\u2022 The lower left corner position of the left team is (1, 1), and the upper right corner position of the right team is (20, 12).   \n\u2022 The venues never interchange or change. The direction of the position information is the direction the player is currently facing and the direction of future actions. ", "page_idx": 20}, {"type": "text", "text": "B.2 Action Space ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The default action set comprises 19 actions, including directional movements, three various ball passing, ball shooting, sliding, sprinting and others. Throughout our experiments, we utilized the default action set. In details, ", "page_idx": 20}, {"type": "text", "text": "\u2022 Idle actions ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "action_idle ${\\mathbf{\\theta}}=\\mathbf{0}$ a no-op action, sticky actions are not affected (player maintains his directional movement etc.). ", "page_idx": 20}, {"type": "text", "text": "\u2022 Movement actions ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "action_left $\\mathbf{\\varepsilon}=\\mathbf{1}$ : run to the left, sticky action.   \naction_top_left $\\mathbf{\\epsilon}=2$ : run to the top-left, sticky action.   \naction_top $=3$ : run to the top, sticky action.   \naction_top_right $=4$ : run to the top-right, sticky action.   \naction_right $=5$ : run to the right, sticky action.   \naction_bottom_right $=6$ : run to the bottom-right, sticky action.   \naction_bottom $=7$ : run to the bottom, sticky action.   \naction_bottom_left $\\mathbf{\\theta}=\\mathbf{8}$ : run to the bottom-left, sticky action. ", "page_idx": 20}, {"type": "text", "text": "\u2022 Passing / Shooting ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "action_long_pass ${\\bf\\delta}={\\bf9}$ : perform a long pass to the player on your team. Player to pass the ball to is auto-determined based on the movement direction.   \naction_high_pass ${\\bf\\delta}=10$ : perform a high pass, similar to action_long_pass.   \naction_short_pass $\\mathbf{\\delta}=\\mathbf{11}$ : perform a short pass, similar to action_long_pass.   \naction_shot $=12$ perform a shot, always in the direction of the opponent\u2019s goal. ", "page_idx": 20}, {"type": "text", "text": "\u2022 Other actions ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "action_sprint $=13$ : start sprinting, sticky action. Player moves faster, but has worse ball handling.   \naction_release_direction $=14$ : reset current movement direction.   \naction_release_sprint $=15$ : stop sprinting.   \naction_sliding $=16$ : perform a slide (effective when not having a ball).   \naction_dribble $=17$ : start dribbling (effective when having a ball), sticky action. Player moves slower, but it is harder to take over the ball from him.   \naction_release_dribble ${\\bf\\delta}=18$ : stop dribbling. ", "page_idx": 20}, {"type": "text", "text": "The simulator information $|{\\mathcal{M}}|$ includes the description of the action space $\\boldsymbol{\\mathcal{A}}$ . What we use in this paper is as follows. ", "page_idx": 20}, {"type": "text", "text": "\u2022 0, # action_idle, a no-op action, sticky actions are not affected (player maintains his directional movement etc.). \u2022 1, # action_left, sticky action and will change the player\u2019s direction, player will continue to move left until another action is taken. Such as, from zone(11,4) to zone(10,4) \u2022 2, # action_top_left, sticky action and will change the player\u2019s direction, player will continue to move top left until another action is taken. Such as, from zone(11,4) to zone(10,5) ", "page_idx": 20}, {"type": "text", "text": "\u2022 3, # action_top, sticky action and will change the player\u2019s direction, player will continue to move top until another action is taken. Such as, from zone(11,4) to zone(11,5)   \n\u2022 4, # action_top_right, sticky action and will change the player\u2019s direction, player will continue to move top right until another action is taken. Such as, from zone(11,4) to zone(12,5)   \n\u2022 5, # action_right, sticky action and will change the player\u2019s direction, player will continue to move right until another action is taken. Such as, from zone(11,4) to zone(12,4)   \n\u2022 6, # action_bottom_right, sticky action and will change the player\u2019s direction, player will continue to move bottom right until another action is taken. Such as, from zone(11,4) to zone(12,3)   \n\u2022 7, # action_bottom, sticky action and will change the player\u2019s direction, player will continue to move bottom until another action is taken.Such as, from zone(11,4) to zone(11,3)   \n\u2022 8, # action_bottom_left, sticky action and will change the player\u2019s direction, player will continue to move bottom left until another action is taken. Such as, from zone(11,4) to zone(10,3)   \n\u2022 9, # action_long_pass, the player will long pass to their teammate in his current direction. Before you pass, you should release the sprint if the agent is doing sprint. A long pass covers a large distance on the field.   \n\u2022 10, # action_high_pass, the player will high pass to their teammate in his current direction. Before you pass, you should release the sprint if the agent is doing sprint. A high pass sends the ball into the air, often over obstacles, to reach a teammate.   \n\u2022 11, # action_short_pass, the player will short pass to their teammate in his current direction. Before you pass, you should tune make sure the diresction is fine, using action 0-8 to chenge the direction. A short pass is a quick and close-range exchange between teammates, commonly used to maintain possession and build an attack.   \n\u2022 12, # action_shot, players will try to shoot.   \n\u2022 13, # action_sprint, when the player will chose this action, the agent will sprint with sticky action\u2019s direction, it will make agent run faster.   \n\u2022 14, # action_release_direction, player will stop moving in the current direction. Choose it when you want this agent to change the direction, after this action, you should choose another action from 0-8 to change it.   \n\u2022 15, # action_release_sprint, player will stop sprinting, only when the agent\u2019s during the sprint.   \n\u2022 16, # action_sliding, the player will try to slide the tackle. If your position is on the opponent\u2019s path with the ball, you can intercept it. However, if the sliding tackle fails, you will be separated by a large distance, allowing the opponent to ignore the defense.   \n\u2022 17, # action_dribble, players will try to dribble. When they have the ball, dribbling will greatly improve the success rate of dribbling, especially in multi-person double-teams and difficult-to-handle situations.   \n\u2022 18, # action_release_dribble, player will stop dribbling. ", "page_idx": 21}, {"type": "text", "text": "B.3 Transition ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The game dynamics in GRF closely resemble realistic football games. Players can move or sprint with or without the ball. They can also execute various passes and shots. Besides different actions, complex interactions such as collisions and trips between players are simulated as well. Additionally, the game engine introduces stochasticity in the dynamics, which can influence the passes and shots randomly. ", "page_idx": 21}, {"type": "text", "text": "In our experiment, to address the challenge of executing fine-grained maneuvers within a game environment, we employ a strategy where actions generated by rule-based policies [57] are utilized to replace those produced by the policies, including URI and all of the baselines. This substitution is particularly enacted when considering the spatial segmentation of the play area into zones. Using zones as a pivotal input, the approach facilitates more nuanced control over in-zone activities. The rationale behind this decision is rooted in the state that precise actions within these designated zones can significantly impact the outcome of play, necessitating a method that allows for detailed operational control. This method ensures that the agents can perform more sophisticated strategies, particularly in scenarios that demand high levels of precision and situational awareness within the confined spaces of each zone. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "The simulator information $|{\\mathcal{M}}|$ includes the description of the transition information and the task information $T$ and $R$ . What we use in this paper is as follows. ", "page_idx": 22}, {"type": "text", "text": "\u2022 Dynamics is to give the dynamics function or related rules of the football game under the football manager policy\u00b4s action, such as after shooting, the ball will be in the goal or not. For example: \"When the direction of shooting is vertical to the goal, the ball will be easy to the goal.   \n\u2022 for each step of transition prediction, you should simulate 1.5 second based on current state and action of players.   \n\u2022 Reward is to give the reward or punishment of the football manager policy. The behavior that is encouraged is when the forwards are restricted, the midfielder can support and take away the defenders. This is a very encouraging behavior because it allows the team to keep possession of the ball and control the game.   \n\u2022 You should only identify 5 types of rewards: A. 2 for optimal behavior; B. 1 for encouraging behavior; C. 0 for borderline behavior; D. 1 for punishing behavior; E: -2 for worst behavior. ", "page_idx": 22}, {"type": "text", "text": "C Additional Related Work ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Offilne RL addresses the problem of learning policies from a pre-collected dataset. Most methods can be classified into two categories: model-free and model-based methods. Model-free [39\u201342, 5, 43, 44] methods learn a conservative policy directly from the dataset. Model-based offilne algorithms [6, 45\u2013 48] first estimate a model from the dataset and perform policy learning or planning based on this learned model. In our work, to achieve introspection from the data generated by the LLMs, we build our policy distillation algorithm based on several existing techniques in offline RL, including the uncertainty penalty in MOPO [6] , which constructs a pessimistic model that discourages the policy from visiting states where the model is inaccurate; and conservative Q-learning loss [5] to obtain a robust value function that does not overestimate unseen state-action pairs too much. ", "page_idx": 22}, {"type": "text", "text": "D Baseline Implementations for TTT ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we detail the implementation of baselines for both Tic-Tac-Toe. ", "page_idx": 22}, {"type": "text", "text": "D.1 LLM-as-agent ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The LLM-as-agent baseline uses GPT-3.5 to directly generate actions for both environments. we use the following prompts: ", "page_idx": 22}, {"type": "image", "img_path": "Ddak3nSqQM/tmp/03b9b77ad068d13efbaffa4b9f19cea0a5f5f37d39eb8cf17c1d477051509d30.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "Ddak3nSqQM/tmp/39879ecace6fe9d47a55189940bdbd471dc14c4b28765b6e49ffcd9b9d43d73b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "D.2 LLM-RAG ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The LLM-RAG baseline enhances LLM-as-agent by retrieving relevant knowledge from tutorial books. ", "page_idx": 23}, {"type": "text", "text": "Prompt 4: LLM-RAG Global Prompt for Tic-Tac-Toe ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "[Same as LLM-as-agent Global Prompt with additional instruction:] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The policy knowledge from tutorials will be provided to guide your decision. ", "page_idx": 24}, {"type": "text", "text": "Prompt 5: LLM-RAG Query Prompt for Tic-Tac-Toe ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The relevant policy knowledge for the current state is: {policy_knowledge} ", "page_idx": 24}, {"type": "text", "text": "## Choose the next move based on the policy knowledge ", "page_idx": 24}, {"type": "text", "text": "[Rest same as LLM-as-agent Query Prompt] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "For the RAG part, we use the following knowledge retrieval approach: ", "page_idx": 24}, {"type": "text", "text": "\u2022 Use GPT embeddings to encode both state descriptions and tutorial knowledge \u2022 Retrieve top-3 most relevant pieces of strategic knowledge using cosine similarity \u2022 Combine retrieved knowledge with state information for move generation ", "page_idx": 24}, {"type": "text", "text": "D.3 Additional Baselines ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 Minimax (Optimal): ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2013 Uses standard minimax algorithm with perfect play \u2013 Evaluates states with $+1$ for win, -1 for loss, 0 for draw \u2013 Uses random first move selection for diversity   \n\u2022 Minimax-noise: \u2013 Based on optimal minimax algorithm \u2013 Makes random moves with $30\\%$ probability \u2013 Uses optimal moves with $70\\%$ probability \u2013 Serves as a strong but imperfect opponent   \n\u2022 Random: \u2013 Selects moves uniformly from available positions \u2013 Serves as a baseline for minimal strategic play ", "page_idx": 24}, {"type": "text", "text": "E Baseline Implementations for GRF ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this section, we will focus on implementing of two relevant baselines. ", "page_idx": 24}, {"type": "text", "text": "E.1 LLM-as-agent ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "As mentioned in the paper, the LLM-as-agent method uses LLM (GPT3.5) directly to generate the action. We provide the prompt as follows, ", "page_idx": 24}, {"type": "text", "text": "Prompt 6: LLM-as-agent Query Prompt 1/2   \nThe texted state for the current state:   \n{text_state}   \n{Simulator Information $|{\\mathcal{M}}|$ . Refer to Appendix F.4.}   \nFor example, if the active player is Player 2 and you want him to be close to the ball and control it in the texted state,   \n- Forward Player 2 is at Zone(9,9).   \n- The ball is at Zone(11,8).   \nGiven these coordinates, the ball is diagonally one zone to the right (east) and one zone down (south) from the player\u2019s current position. The most direct route to the ball would indeed be diagonally towards the bottom right.   \nTherefore, the most appropriate action for Forward Player 2 in this situation would be: $6=$ action_bottom_right: This sticky action will allow the player to move diagonally in the bottom-right direction (southeast), which is the direct path to where the ball is currently located in Zone(11,8).   \nBy choosing action_bottom_right, Forward Player 2 can close the distance to the ball more effectively, aligning their movement directly with the ball\u2019s current location. Once the player reaches the ball, the subsequent action can be decided based on the situation at that moment (e.g., dribbling, passing, or shooting).   \nQuestion: What next action do you want this active player to take?   \nAnswer: ", "page_idx": 25}, {"type": "text", "text": "The global prompt shows as follows, ", "page_idx": 25}, {"type": "text", "text": "Prompt 7: LLM-as-agent Global Prompt ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "I want you to act like a football manager and also an expert in Python coding and Reinforcement Learning, which is about learning a football manager policy in a football simulator. ", "page_idx": 25}, {"type": "text", "text": "I will give you the pseudocode snippets to define the specific policy for the active player in the football game. ", "page_idx": 25}, {"type": "text", "text": "These codes represent absolute correctness and do not add other common logic. Your task is to select the action that best fits and executes the code based on the logic of the given code. ", "page_idx": 25}, {"type": "text", "text": "Requirements: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "About the action choosing: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u2022 Please choose the action that best fits the code logic. ", "page_idx": 25}, {"type": "text", "text": "About the format: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u2022 you should answer in pure JSON format with the key: \u2019action\u2019: an int number from 0 to 18,   \n\u2022 \u2019thought\u2019: why did you choose this action? without any other information or code.   \n\u2022 For example, you should not add the \u201c\u2018JSON\u201c\u2018 tag in the answer.   \nResponse example (you should respond in the following order):   \n{{   \n\"action\": 0,   \n\"thought\": \"Based on your thought, tell me the optimal action you would like to select   \nin the action set.\"   \n}} ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "E.2 LLM-RAG ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "The LLM-RAG enhances LLM-as-agent by retrieving relevant knowledge from the database extracted from the tutorial books, similar to the retrieval step in our rehearsing stage, but directly outputs the action without policy learning. The dataset which the LLM-RAG using is the text-booked dataset. Here we show the prompts of LLM-RAG. ", "page_idx": 26}, {"type": "text", "text": "Prompt 8: LLM-RAG Query Prompt 1/2 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "The relative policy from the book you want this active player to implement is as follows:   \n{Policy_Str}   \nThe texted state for the current state:   \n{text_state}   \n{Simulator Information $|{\\mathcal{M}}|$ . Refer to Appendix F.4.}   \nFor example, if the active player is Player 2.   \nThe most relative policy from books:   \n- When you become a back defender, you aim to get close to the ball and stop the attacker\u2019s progression towards the goal.   \nThe texted state:   \n- Forward Player 2 is at Zone(9,9).   \n- The ball is at Zone(11,8).   \nGiven these coordinates, the ball is diagonally one zone to the right (east) and one zone down (south) from the player\u2019s current position. The most direct route to the ball would indeed be diagonally towards the bottom right.   \nTherefore, the most appropriate action for Forward Player 2 in this situation would be: $6=$ action_bottom_right: This sticky action will allow the player to move diagonally in the bottom-right direction (southeast), which is the direct path to where the ball is currently located in Zone(11,8).   \nBy choosing action_bottom_right, Forward Player 2 can close the distance to the ball more effectively, aligning their movement directly with the ball\u2019s current location. Once the player reaches the ball, the subsequent action can be decided based on the situation at that moment (e.g., dribbling, passing, or shooting). ", "page_idx": 26}, {"type": "table", "img_path": "Ddak3nSqQM/tmp/ed548e0d148f201c37d0f37d22700f457453bba6378556042a9c74eb53e30dd6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "The global prompt shows as follows, ", "page_idx": 27}, {"type": "text", "text": "Prompt 9: LLM-RAG Global Prompt ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "I want you to act like a football manager and also an expert in Python coding and   \nReinforcement Learning, which is about learning a football manager policy in a   \nfootball simulator.   \nI will give you the pseudocode snippets to define the specific policy for the active   \nplayer in the football game.   \nThese codes represent absolute correctness and do not add other common logic.   \nYour task is to select the action that best fits and executes the code based on the   \nlogic of the given code.   \nRequirements:   \nAbout the action choosing: \u2022 Please choose the action that best fits the code logic.   \nAbout the format: \u2022 you should answer in pure JSON format with the key: \u2019action\u2019: an int number from 0 to 18, \u2022 \u2019thought\u2019: why did you choose this action? without any other information or code. \u2022 For example, you should not add the \u201c\u2018JSON\u201c\u2018 tag in the answer.   \nResponse example (you should respond in the following order):   \n{{   \n\"action\": 0,   \n\"thought\": \"Based on your thought, tell me the optimal action you would like to select   \nin the action set.\"   \n}} ", "page_idx": 27}, {"type": "text", "text": "For the RAG part, we use the llama index $[59]^{3}$ for embedding the textbook paragraphs and the state and find the most relevant one as input to the query prompt. ", "page_idx": 27}, {"type": "text", "text": "F URI Implementation ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this section, we will focus on implementing URI. ", "page_idx": 27}, {"type": "text", "text": "F.1 Book Content Understanding ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Book content understanding is achieved via a code-based knowledge extractor and a code-based knowledge aggregator. The implementation is designed to be task-agnostic, with task-specific parameters configured through prompts. ", "page_idx": 27}, {"type": "text", "text": "F.1.1 Knowledge Extractor ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "The knowledge extractor processes book paragraphs to identify and extract relevant knowledge. Algorithm 1 shows the extraction process: ", "page_idx": 28}, {"type": "text", "text": "Algorithm 1 Code Extractor   \nRequire: Book $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ consisting of segments $b_{1},b_{2},\\ldots,b_{N_{b}}$   \n1: Initialize language model $\\mathbf{M}_{\\mathrm{ext}}$   \n2: Define KnowledgeContext $\\leftarrow\\left\\{\\begin{array}{r l}\\end{array}\\right.$ Rewards, Policies, Dynamics}   \n3: $\\kappa\\gets\\emptyset$   \n4: $\\mathcal{T}\\gets\\emptyset$   \n5: for $b_{i}\\in B$ do \u25b7Iterate over each paragraph in the books   \n6: I.add(bi)   \n7: Output \u2190Mext(I, KnowledgeContext)   \n8: if \u2032WithKnowledge\u2032in Output then   \n9: K.add(Output.code)   \n10: else if \u2032WithoutKnowledge\u2032 in Output then   \n11: I \u2190\u2205   \n12: end if   \n13: end for   \n14: return K ", "page_idx": 28}, {"type": "text", "text": "Task-specific configuration can be found at Appendix. F.3 and F.4 ", "page_idx": 28}, {"type": "text", "text": "F.1.2 Knowledge Extraction Prompt ", "text_level": 1, "page_idx": 28}, {"type": "image", "img_path": "Ddak3nSqQM/tmp/6093c1803cc4306bc64da27607b3e5ec3e9bc145ed5a2400c26e5b8b5c6ffeab.jpg", "img_caption": [], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "Ddak3nSqQM/tmp/e7a4d2eb7cbcc3b92e1a154ea0d6e23bf5d94c9ab1a03fd273a716c2c8cbe6f5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "F.1.3 Knowledge Aggregation ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "The aggregation process combines similar pieces of knowledge into more concise representations. Algorithm 2 shows the process: ", "page_idx": 29}, {"type": "text", "text": "Require: Knowledge $\\kappa$ , text inclusion flag $T$ , aggregation number $N$ , similarity threshold $\\tau$   \n1: Initialize language model $\\mathbf{M}_{\\mathrm{agg}}$ and embedding model E   \n2: $K^{\\prime}\\gets\\emptyset$   \n3: for $K$ in $\\kappa$ do $\\triangleright$ Iterate over each knowledge piece   \n4: Define KnowledgeContext \u2190{Rewards, Policies, Dynamics}   \n5: $\\mathcal{K}^{\\mathrm{sim}}\\gets\\{K^{\\prime}\\in\\bar{\\mathcal{K}}|$ calculate-similarities $({\\bf E}(K),{\\bf E}(K^{\\prime}))>\\tau]$   \n6: if $|K^{\\mathrm{sim}}|\\geq N_{\\mathrm{agg}}$ then   \n7:   \n8:   \n9: else   \n10: K\u2032.add(K)   \n11: end if   \n12: end for   \n13: return $K^{\\prime}$ ", "page_idx": 29}, {"type": "image", "img_path": "Ddak3nSqQM/tmp/a09776459ebe79ea2a40cd69150faf247d9d5c64b5cd4bcc3d0e09cf05126312.jpg", "img_caption": [], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "Prompt 12: Text-based Aggregation Prompt ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "I want you to act like {task_info}. ", "page_idx": 30}, {"type": "text", "text": "I will give you several paragraphs and the corresponding summary written by code from several related books. You need to analyze the given paragraph step-by-step from a related context to aggregate the specific theorem, principle, rule, and law of the related elements or concepts: ", "page_idx": 30}, {"type": "image", "img_path": "Ddak3nSqQM/tmp/946589a933d5d99fc8b89c615267c2ab052413312a23da69f15906b8d0e813dc.jpg", "img_caption": [], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "F.2 Knowledge-based Rehearsing of Decision-Making ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Knowledge-based rehearsing of decision making is achieved via a state-space knowledge scope retrieval module and retrieved knowledge instantiation. ", "page_idx": 31}, {"type": "text", "text": "For state-space knowledge scope retrieval, we use GPT-3.5 to identify the knowledge scope represented by state space, then adopt a standard RAG module to retrieve the correct pieces of knowledge. ", "page_idx": 31}, {"type": "text", "text": "Prompt 13: Football Knowledge Code Generation Prompt ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "I want you to act like a football manager and also an expert in python coding and Reinforcement Learning that wants to learn a football manager policy in a football simulator.   \nI will provide an observation space of the football simulator and some policy functions written by others. Your task is to summarize an observation scope, which is a subspace of the observation space and most suitable to use this policy function to win the game.   \nThe observation space: {observation_space_desc}   \nThe action space: {action_space_desc}   \nWe have N codes for you to analyze: {code_list}   \nAbout the definition of the observation scope of each code:   \nFor each code, you should return a description of the best scope of the observation variables to be used as the policy function to win the game, including:   \n\u2022 A text summary of the preferred/better scope   \n\u2022 score is preferred/better to be in [?] ", "page_idx": 31}, {"type": "text", "text": "\u2022 active_player_role is preferred/better to be in [?] \u2022 ball_ownership is preferred/better to be in [?] \u2022 ball_ownership_player is preferred/better to be in [?] \u2022 ball_zone is preferred/better to be in [?] \u2022 ball_direction is preferred/better to be in [?] \u2022 {the preferred zone of all players from 0 to 21} ", "page_idx": 32}, {"type": "text", "text": "NOTE: 1. You should define your observation scope as detailed and tight as possible 2. You should define the observation scope for each code by comparing the differences between the codes   \nAbout the format: \u2022 You should answer in pure JSON format, without any other information or code   \nResponse example:   \n{ \u2019code_idx 1\u2019: { \u2019preferred scope description\u2019: \u2019a text summary of the suitable situations, \u2019score\u2019: \u2019preferred scope of the score\u2019, \u2019active_player_role\u2019: \u2019preferred scope of the active_player_role\u2019 }, ", "page_idx": 32}, {"type": "text", "text": "Prompt 14: Tic-Tac-Toe Knowledge Code Generation Prompt ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "I want you to act like a tic-tac-toe pro player and also an expert in python coding and Reinforcement Learning that wants to learn a tic-tac-toe policy in a tic-tac-toe simulator. ", "page_idx": 32}, {"type": "text", "text": "I will provide an observation space of the tic-tac-toe simulator and some policy functions written by others. Your task is to summarize an observation scope, which is a subspace of the observation space and most suitable to use this policy function to win the game. ", "page_idx": 32}, {"type": "text", "text": "The observation space: {observation_space_desc} ", "page_idx": 32}, {"type": "text", "text": "The action space: {action_space_desc} ", "page_idx": 32}, {"type": "text", "text": "We have N codes for you to analyze: {code_list} ", "page_idx": 32}, {"type": "text", "text": "About the definition of the observation scope of each code: ", "page_idx": 32}, {"type": "text", "text": "For each code, you should return a description of the best scope of the observation variables to be used as the policy function to win the game, including: ", "page_idx": 32}, {"type": "text", "text": "\u2022 A text summary of the preferred/better scope   \n\u2022 Preferred board states: [(?), (?), (?), (?), (?), (?), (?), (?), (?)], where you should replace \"?\" with all possible values or \"any\" if all values are suitable   \n\u2022 Preferred/better player: [?], where you should replace \"?\" with possible values   \nNOTE: 1. You should define your observation scope as detailed and tight as possible 2. You should define the observation scope for each code by comparing the differences between the codes   \nAbout the format: \u2022 You should answer in pure JSON format, without any other information or code   \nResponse example:   \n{ \u2019code_idx 1\u2019: { \u2019preferred scope description\u2019: \u2019a text summary of the suitable situations, \u2019current_player\u2019: \u2019preferred scopes\u2019, \u2019preferred board states\u2019: \u2019preferred scopes\u2019 },   \n} ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "Then a standard RAG technique is applied to identify the most relevant knowledge scope $K_{j}^{s}$ and its relevant knowledge $K_{j}$ . Formally, $(\\{K_{j}\\},\\{K_{j}^{S}\\})={\\bf R}_{\\mathrm{scope}}(\\hat{s},\\mathcal{K}^{S})$ , where $\\kappa^{S}$ is the scope of the knowledge database. ", "page_idx": 33}, {"type": "text", "text": "F.2.1 Code Instantiation ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "After that, we employ an LLM to instantiate the code $K^{I}\\,=\\,{\\bf M}_{\\mathrm{Inst}}(\\hat{s},|\\mathcal{M}|,\\{K_{j}\\})$ based on the current state $\\hat{s}$ , the simulator information description $|{\\mathcal{M}}|$ and the knowledge $\\{\\boldsymbol{\\bar{K}_{j}}\\}$ retrieved by $\\mathbf{R}_{\\mathrm{scope}}$ . The prompt is listed in Prompt 15. Since this step requires a strong understanding of the code, we use GPT-4 instead of GPT-3.5 as the LLM implementation. ", "page_idx": 33}, {"type": "text", "text": "The code instantiation process converts general knowledge into task-specific code through a dedicated module. Here\u2019s the detailed implementation: ", "page_idx": 33}, {"type": "text", "text": "Prompt 15: Code Instantiation Prompt ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "I want you to act {task_info}. ", "page_idx": 33}, {"type": "text", "text": "I will give you an observation which you are facing in the simulator, your task is to instantiate a code that serves as a {knowledge_type} function, which is used for {knowledge_function}. ", "page_idx": 33}, {"type": "text", "text": "For example, {knowledge_function_example} ", "page_idx": 33}, {"type": "text", "text": "Formally, the format of knowledge_type function is {knowledge_format}. ", "page_idx": 33}, {"type": "text", "text": "To help you complete the task, I will provide you: ", "page_idx": 33}, {"type": "text", "text": "1. Pieces of relevant knowledge from the tutorial, written in Python-style pseudocode   \n2. The observation space and action space of the target simulator   \n3. The current observation you are facing ", "page_idx": 33}, {"type": "text", "text": "Python-style relevant knowledge from tutorial: {code_string} ", "page_idx": 33}, {"type": "text", "text": "The observation space: {observation_space_desc} The action space: {action_space_desc} Current observation: {obs} ", "page_idx": 33}, {"type": "text", "text": "About your code-instantiation task: \u2022 Please provide the PYTHON-style pseudo code as detailed as you can \u2022 Your task is to rewrite a code that describes a {knowledge_type} suitable to current observation \u2022 You should make the optimal decision based on analyzing current observation \u2022 After analysis, rewrite the pseudocode to make it most suitable for downstream tasks \u2022 Keep a main function named \"{knowledge_type}\" and add inner functions if necessary   \nImportant Notes: \u2022 You cannot call the pseudocodes provided in the prompt \u2022 Variable assignment can be simplified, but implement logic in detail \u2022 Do not use placeholder implementations   \nAdditional Requirements: \u2022 For one-step use: Keep code short, direct, and focused on main logic \u2022 For multi-step use: Code should generalize to next minute of game, considering: \u2013 Changes in active player \u2013 Ball position changes \u2013 Opponent position changes   \nResponse format:   \n{ \"analyze\": \"Analysis of current observation, focusing on code instantiation approach\", \"code\": \"Rewritten code based on current observation analysis\"   \n} ", "page_idx": 34}, {"type": "text", "text": "F.2.2 Rollout Process ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Finally, three LLMs are involved to generate the imaginary dataset $\\mathcal{D}_{\\mathrm{img}}$ , including $\\hat{a}_{t}\\,=\\,\\mathbf{M}_{\\pi}\\big(\\hat{s}_{t},\\mathbf{M}_{\\mathrm{Inst}}(s_{t},|\\mathcal{M}|,\\mathbf{R}_{\\mathrm{scope}}(\\hat{s},K_{\\pi}^{S}))\\big)$ , $\\hat{r}_{t}\\,=\\,{\\bf M}_{R}\\big(\\hat{s}_{t},\\hat{a}_{t},{\\bf M}_{\\mathrm{Inst}}(s_{t},|\\mathcal{M}|,{\\bf R}_{\\mathrm{scope}}^{-}\\big(\\hat{s},{\\mathcal K}_{R}^{S}\\big))\\big)$ and $\\hat{s}_{t+1}=\\mathbf{M}_{T}\\big(\\hat{s}_{t},\\hat{a}_{t},\\mathbf{M}_{\\mathrm{Inst}}(s_{t},|\\mathcal{M}|,\\mathbf{R}_{\\mathrm{scope}}\\big(\\hat{s},\\mathcal{K}_{T}^{S}\\big))\\big)$ , where ${\\boldsymbol{\\kappa}}_{\\pi}^{S}$ , $\\kappa_{R}^{S}$ , and ${\\kappa}_{T}^{S}$ are the scope of the knowledge database $\\kappa_{\\pi}$ , $\\kappa_{R}$ , and $\\kappa_{T}$ respectively. ", "page_idx": 34}, {"type": "text", "text": "Prompt 16: Policy Inference Prompt ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "I want you to act {task_info}.   \nI will give you an observation which you are facing in a simulator, your task is to response a correct results serving as a {knowledge_type} function, which is used for {knowledge_function}.   \nFor example, {knowledge_function_example}.   \nFormally, the format of {knowledge_type} function is {knowledge_format}. To help you complete the task, I will provide you: ", "page_idx": 34}, {"type": "text", "text": "1. Pieces of relevant knowledge from the tutorial, written in Python-style pseudocode 2. The observation space and action space of the simulator 3. The current observation you are facing Python-style relevant knowledge from tutorial books: {code_string} The observation space: {observation_space_desc} The action space: {action_space_desc} Current observation: {obs} Requirements: \u2022 Choose the action that best fits the code logic \u2022 Answer in pure JSON format with keys: \u2013 \u2019action\u2019: integer from 0 to 18 \u2013 \u2019thought\u2019: reasoning for the choice Hints for inference: {hint} {inference_format_example} ", "page_idx": 35}, {"type": "text", "text": "Prompt 17: Dynamics Inference Prompt ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "I want you to act like {task_info}. I will give you an observation which you are facing in the target simulator, your task is to response a correct results serving as a dynamics function code from the tutorial. Formally, the format of dynamics function is dynamics_format. The function describes the mechanism for updating the position of the ball and players. To help you complete the task, I will provide you:   \n1. The dynamics function from tutorial books: {dynamics_code_string}   \n2. Current observation: {obs}   \n3. Chosen action: {action} Hints for inference: {hint} Response format: {inference_format_example} ", "page_idx": 35}, {"type": "text", "text": "", "text_level": 1, "page_idx": 35}, {"type": "image", "img_path": "Ddak3nSqQM/tmp/13848fc112e84e816a8c7989b6ed1b2223134e267e593212e1f50801ab516df9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "The rollout process follows Algorithm 3, which integrates the three inference components: ", "page_idx": 36}, {"type": "text", "text": "Algorithm 3 Knowledge-based Rollout ", "text_level": 1, "page_idx": 36}, {"type": "table", "img_path": "Ddak3nSqQM/tmp/ccc6c47d3f51fbd0272215c544c9f6c835bfa9eeb77d341951cbaa9d73a78c48.jpg", "table_caption": [], "table_footnote": [], "page_idx": 36}, {"type": "text", "text": "F.3 Tic-Tac-Toe Prompt Configurations ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "The following configurations detail the task-specific parameters used in the previously described prompts for the Tic-Tac-Toe environment. These include the task description, knowledge type definitions, observation space descriptions, and other parameters that are substituted into the corresponding placeholders in the book content understanding prompts (Appendix F.1), knowledge rehearsing prompts (Appendix F.2). ", "page_idx": 36}, {"type": "text", "text": "{task_info} ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "\"An expert Tic-Tac-Toe player and also an expert in python coding and Reinforcement Learning.\" ", "page_idx": 36}, {"type": "text", "text": "{knowledge_type} ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "\u2022 Policy Function: ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "\"The policy in Tic Tac Toe involves strategic placement of X\u2019s or O\u2019s on the game board to block the opponent\u2019s moves and create opportunities to complete a line. For example, \"Always place your mark in the center if it is your first move, as it maximizes potential winning combinations.\" ", "page_idx": 36}, {"type": "text", "text": "\u2022 Environment Dynamics Function: ", "page_idx": 36}, {"type": "text", "text": "\"This function describes the rules and changes in the game environment following player actions. For example, \"If a player completes a row, column, or diagonal with the same symbol, they win the game. Otherwise, the game continues or ends in a draw if all spaces are filled without a winner.\" ", "page_idx": 37}, {"type": "text", "text": "\u2022 Rewards Function: ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "\"This function outlines the incentives or penalties based on the game outcomes in Tic Tac Toe. For example, \"Completing three consecutive symbols horizontally, vertically, or diagonally results in a win and positive reward, while failing to block an opponent\u2019s winning move results in a loss and a negative reward.\" ", "page_idx": 37}, {"type": "text", "text": "{knowledge_function_example} ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "\u2022 Policy Examples: ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "\"A common opening strategy is to place the first mark in the center of an empty board, which provides the most opportunities for setting up wins in subsequent moves. Another strategy involves placing the second mark in a corner if the center is already occupied by the opponent.\" ", "page_idx": 37}, {"type": "text", "text": "\u2022 Dynamics Examples: ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "\"The board state changes with each player\u2019s move, leading to new potential lines for winning. A player wins by placing three of their marks in a horizontal, vertical, or diagonal row. If all nine squares are fliled and no player has three in a row, the game is a draw.\" ", "page_idx": 37}, {"type": "text", "text": "\u2022 Reward Examples: ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "\"You can classify the outcomes as follows: 2: Winning move; 1: Blocking an opponent\u2019s winning move; 0: Neutral move; -1: Missing an opportunity to block an opponent\u2019s winning move; -2: Making a move that leads directly to a loss.\" ", "page_idx": 37}, {"type": "text", "text": "{observation_space_desc} ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "\"The observation space consists of a $3{\\times}3$ grid representing the game board. Each cell can be empty or contain an $\\mathbf{\\nabla}\\mathbf{X}^{\\bullet}$ or \u2019O\u2019 symbol. We use a $1\\!\\times\\!9$ array with number 0, 1, and 2 to represent empty, $\\mathbf{\\nabla}\\mathbf{X}^{\\bullet}$ , and \u2019O\u2019 respectively. The board state is updated after each player\u2019s move, reflecting the current game configuration. We will use another line to tell you whose turn it is, 1 for $\\Chi$ and 2 for O.\" ", "page_idx": 37}, {"type": "text", "text": "{action_space_desc} ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "\"The action space is discrete from 0 to 8, including 9 possible moves corresponding to the 9 cells on the game board. Players can place their mark (\u2019X (1)\u2019 or $\\mathrm{'O}\\left(2\\right)\\mathrm{'}$ ) in any empty cell to make their move.\" ", "page_idx": 37}, {"type": "text", "text": "{inference_format_example} ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "\u2022 Policy Inference: ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "\"For example, if it is X\u2019s turn and you observe the following board state: [1, 2, 0,   \n0, 0, 0, 0, 0, 0]   \nThe strategic thought might be \"Placing X in the center (cell 4) could block O\u2019s   \npotential diagonal completion and creates multiple opportunities for $\\Chi$ to win on   \nsubsequent turns. Therefore, the most appropriate action for X in this situation   \nwould be to place in the center.\"   \nExpected output format: { \"thought\": \"Given the current board state [0,1,2,0,0,0,0,0,0] and that it\u2019s X\u2019s turn...\", \"action\": 4 }\" ", "page_idx": 37}, {"type": "text", "text": "\u2022 Dynamics Inference: ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "\"Important keys in the observation:   \n\u2013 board: Current board configuration as a list of nine integers (0 for empty, 1 for X, 2 for O). Example: [0,1,2,0,0,0,0,0,0]   \n\u2013 player_turn: Indicates current player (1 for X, 2 for O)   \nExpected output format:   \n{ \"thought\": \"Given the board state [0,1,2,0,0,0,0,0,0] and the action to place X in the center, the next state of the board should be [0,1,2,0,1,0,0,0,0] reflecting X\u2019s placement.\", \"board\": [0,1,2,0,1,0,0,0,0], \"player_turn\": 2 # O\u2019s turn next   \n}\" ", "page_idx": 38}, {"type": "text", "text": "\u2022 Reward Inference: ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "\"Based on current state, action, and resulting state, compute rewards as: \u2013 $+2$ : Winning move \u2013 $+1$ : Blocking opponent\u2019s winning move \u2013 0: Neutral move \u2013 -1: Missing opportunity to block \u2013 -2: Move leading to direct loss   \nExpected output format:   \n{ \"thought\": \"Given the board state [0,1,2,0,0,0,0,0,0], the action to place X in the center, and the observed next state [0,1,2,0,1,0,0,0,2]. With X\u2019s placement resulting in no immediate win or loss and the game still in play, the reward for this action should be 0.\", \"dense_reward\": 0   \n}\" ", "page_idx": 38}, {"type": "text", "text": "F.4 Football Prompt Configuration ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Here we detail the configuration for the Football: ", "page_idx": 38}, {"type": "text", "text": "{task_info} ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "\"A football manager and also an expert in python coding and Reinforcement Learning\" ", "page_idx": 38}, {"type": "text", "text": "{knowledge_type} ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "\u2022 Policy Function: ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "The football manager policy is to give the tactics and strategies for all players in the team, such as how players should be used in the frontcourt during a match, or when forwards should take shots. For example: \"When watching defenders you have to assess how they respond to their opponents as well as the ball.\" ", "page_idx": 38}, {"type": "text", "text": "\u2022 Environment Dynamics Function: ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Dynamics is to give the dynamics function or related rules of the football game under the football manager policy\u2019s action, such as after shotting, the ball will be in the goal or not. For example: \"When the direction of shotting is vertical to the goal, the ball will be easy to the goal.\" ", "page_idx": 38}, {"type": "text", "text": "\u2022 Rewards Function: ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Reward is to give the reward or punishment of the football manager policy. For example: \"When the forwards are restricted, the midfielder can support and take away the defenders, which is a very encouraging behavior.\" ", "page_idx": 38}, {"type": "text", "text": "{knowledge_format} ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "\u2022 Policy Function: ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Policy function: observation $->$ action ", "page_idx": 39}, {"type": "text", "text": "\u2022 Environment Dynamics Function: Dynamics function: observation, action $->$ next_observation ", "page_idx": 39}, {"type": "text", "text": "\u2022 Rewards Function: Reward function: observation, action, next_observation $->$ reward ", "page_idx": 39}, {"type": "text", "text": "{knowledge_function_example} ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "\u2022 Policy Function: ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "\"4-4-2 is a good formation for a team with a strong midfield, because it allows the team to control the ball and keep possession. To play this formation, the team should have two central midfielders, two wingers, and two strikers. The central midfielders should be able to pass the ball well and control the game. The wingers should be able to run up and down the wing, and cross the ball into the box. The strikers should be able to score goals\" ", "page_idx": 39}, {"type": "text", "text": "\u2022 Environment Dynamics Function: ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "\"The behavior that is encouraged is when the forwards are restricted, the midfielder can support and take away the defenders. This is a very encouraging behavior because it allows the team to keep possession of the ball and control the game. You should only identify 5 types of rewards: 2: Optimal behavior; 1: Encouraging behavior; 0: Borderline behavior; -1: Punishing behavior; -2: Worest behavior.\" ", "page_idx": 39}, {"type": "text", "text": "\u2022 Rewards Function: ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "\"When the direction of shotting is vertical to the goal, the ball will be easy to the goal.\" ", "page_idx": 39}, {"type": "text", "text": "{observation_space_desc} ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "\"First, it provides information such as the time and score of the match. Second, which side has control of the ball, and the active player in the Left team you need to propose corresponding policies to him. ", "page_idx": 39}, {"type": "text", "text": "Next is the position, direction and role information of each player: ", "page_idx": 39}, {"type": "text", "text": "\u2022 In this text description, the football grass field, which is $120\\mathrm{m}*90\\mathrm{m}$ in the real world, is divided into 240 zones in the simulator, where $\"\\mathbf{x}\"$ ranges from 1 to 20 (left to right from the left team\u2019s perspective) and \"y\" ranges from 1 to 12 (bottom to top).   \n\u2022 Based on this transformation, the concepts in real-world football field can be mapped to the simulator, For instance: \u2013 the center circle is at (10, 6), and the lower left corner from the left team\u2019s view is at (1, 1), and the upper right corner position of the Right team is (20, 12) \u2013 the distance from (10, 5) to (11, 5) in the real-world is $12\\mathrm{m}$ , and the distance from (10, 5) to (10, 6) is $7.5\\mathrm{m}$ \u2013 the left team\u2019s penalty area is zone $(1,4)\u2013(1,8)\u2013(2,8)\u2013(2,4)$ , and the right team\u2019s penalty area is zone (20, 4)-(20, 8)-(18, 8)-(18, 4) \u2013 the center circle position of the field is zone (10, 6), where the game start   \n\u2022 The position of the player is defined by the player\u2019s moving in the corresponding direction, including north, northeast, east, southeast, south, southwest, west, northwest   \n\u2022 Role information: for each team, player roles are fixed, including Goalkeeper, Forward, Forward, Defender, Defender, Defender, Defender, Midfielder, Midfielder, Midfielders, Forward ", "page_idx": 39}, {"type": "text", "text": "NOTE: Venues never interchange or change. Following the position information is direction, meaning the direction the player is currently facing and the direction of future actions.\" ", "page_idx": 40}, {"type": "text", "text": "{action_space_desc} \"The action set includes: ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "1. action_idle: A no-op action, sticky actions are not affected (player maintains his directional movement etc.)   \n2. action_left: Sticky action and will change the player\u2019s direction, player will continue to move left until another action is taken. Such as, from zone(11,4) to zone(10,4)   \n3. action_top_left: Sticky action and will change the player\u2019s direction, player will continue to move top left until another action is taken. Such as, from zone(11,4) to zone(10,5)   \n4. action_top: Sticky action and will change the player\u2019s direction, player will continue to move top until another action is taken. Such as, from zone(11,4) to zone(11,5)   \n5. action_top_right: Sticky action and will change the player\u2019s direction, player will continue to move top right until another action is taken. Such as, from zone(11,4) to zone(12,5) 6. action_right: Sticky action and will change the player\u2019s direction, player will continue to move right until another action is taken. Such as, from zone(11,4) to zone(12,4) 7. action_bottom_right: Sticky action and will change the player\u2019s direction, player will continue to move bottom right until another action is taken. Such as, from zone(11,4) to zone(12,3)   \n8. action_bottom: Sticky action and will change the player\u2019s direction, player will continue to move bottom until another action is taken. Such as, from zone(11,4) to zone(11,3) 9. action_bottom_left: Sticky action and will change the player\u2019s direction, player will continue to move bottom left until another action is taken. Such as, from zone(11,4) to zone(10,3)   \n10. action_long_pass: The player will long pass to their teammate in his current direction. Before you pass, you should release the sprint if the agent is doing sprint. A long pass covers a large distance on the field   \n11. action_high_pass: The player will high pass to their teammate in his current direction. Before you pass, you should release the sprint if the agent is doing sprint. A high pass sends the ball into the air, often over obstacles, to reach a teammate   \n12. action_short_pass: The player will short pass to their teammate in his current direction. Before you pass, you should tune make sure the direction is fine, using action 0-8 to change the direction. A short pass is a quick and close-range exchange between teammates, commonly used to maintain possession and build an attack   \n13. action_shot: Players will try to shoot. When near to the opponent penalty area, such as zone(x, y), $_{\\mathrm{X}>18}$ , $4{<}\\mathrm{y}{<}8$ , try to shoot   \n14. action_sprint: When the player will choose this action, the agent will sprint with sticky action\u2019s direction, it will make agent run faster   \n15. action_release_direction: Player will stop moving in the current direction. Choose it when you want this agent to change the direction, after this action, you should choose another action from 0-8 to change it   \n16. action_release_sprint: Player will stop sprinting, only when the agent\u2019s during the sprint   \n17. action_sliding: The player will try to slide the tackle. If your position is on the opponent\u2019s path with the ball, you can intercept it. However, if the sliding tackle fails, you will be separated by a large distance, allowing the opponent to ignore the defense   \n18. action_dribble: Players will try to dribble. When they have the ball, dribbling will greatly improve the success rate of dribbling, especially in multi-person double-teams and difficult-to-handle situations   \n19. action_release_dribble: Player will stop dribbling\" ", "page_idx": 40}, {"type": "text", "text": "\u2022 Policy hint: ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "For example, if the active player is Player 2 and you want him to close to the ball and control it, in the texted observation:   \n\u2013 Forward Player 2 is at Zone(9,9)   \n\u2013 The ball is at Zone(11,8)   \nYour thought should be: \"Given these coordinates, the ball is diagonally one zone to the right (east) and one zone down (south) from the player\u2019s current position. The most direct route to the ball would indeed be diagonally towards the bottom right. Therefore, the most appropriate action for Forward Player 2 in this situation would be: action_bottom_right\" ", "page_idx": 41}, {"type": "text", "text": "\u2022 Dynamics hint: ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Important keys in the observation:   \n\u2013 score: The current score of the match, should be a list of two integers, such as [0,0] \u2013 step: The current step of the match   \n\u2013 ball_ownership: 1 for left team, 2 for right team, 0 for no team/during passing   \n\u2013 left_active_player_zone: The zone of the active player [x,y]   \n\u2013 ball_zone: Ball position [x,y]   \nNOTE: if the active player still owner the ball, the player zone and the ball zone   \nshould be the same.   \nBased on the current texted observation and the action set, generate the next step   \ntext observation.   \nFor instance, if the current observation shows the Left team player 3 in zone (12,4)   \nand the chosen action is \"Top,\" the next observation should show the player in   \nzone (12,5). ", "page_idx": 41}, {"type": "text", "text": "\u2022 Reward hint: ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Based on the current texted observation and the action set, generate the dense reward of the current step\u2019s action and state, based on the reward code above. The rewards should be one of -2, -1, 0, 1, or 2. ", "page_idx": 41}, {"type": "text", "text": "{inference_format_example} ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "\u2022 Policy Inference: ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "{ \"thought\": \"give your thought to generate the action the current step\u2019s state, based on the policy code above.\", \"action\": 0 ", "page_idx": 41}, {"type": "text", "text": "\u2022 Dynamics Inference: ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "{ ", "page_idx": 41}, {"type": "text", "text": "\"thought\": \"based on the current texted observation and the action set,   \nthe ball and player 3 are in the zone [9,4], and the active player   \nis controlling the ball, and choose the action go top, thus consider   \nthe right have very low chance to intercept the ball, the ball   \nwill be in the zone [9,5] in the next time step.\",   \n\"score\": [0,0],   \n\"step\": 2,   \n\"ball_ownership\": 1,   \n\"ball_zone\": [9,5],   \n\"left_active_player_zone\": [9,5] ", "page_idx": 41}, {"type": "text", "text": "\u2022 Reward Inference: ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "{ \"thought\": \"give your thought to generate the dense reward of the current step\u2019s action and state, based on the reward code above.\", \"dense_reward\": 1   \n} ", "page_idx": 42}, {"type": "text", "text": "F.5 Introspecting based on the Imaginary Dataset ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "We implement CIQL based on the open source codes of CQL in d3rlpy [60], where the uncertainty of rewards and transitions are estimated by an ensemble of 20 Gaussian models based on 4-layer fullconnected neural networks [6, 46], which are learned through standard log-likelihood maximization from the imaginary dataset. ", "page_idx": 42}, {"type": "text", "text": "F.6 General Recipe for Prompt Design ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "The specific prompt design would significantly affect the quality of the LLM outputs and the algorithm built on them. Though URI does not focus on proposing new prompting techniques, we briefly summarize the general design principle as follows in our practice. These principles are adopted in implementations for both URI and all baseline methods in our experiments. ", "page_idx": 42}, {"type": "text", "text": "1. Let LLMs output the \u201cthoughts/analyze\u201d first before outputting the results. The concrete prompts following this principle can be found in the \"Response example\" part in our prompts in Appendix E;   \n2. Make the requirements as explicit as possible. The concrete prompts following this principle can be found in the \"Requirements\" part in our prompts in Appendix E;   \n3. Use pseudo-code instead of using natural language to represent the knowledge. ", "page_idx": 42}, {"type": "text", "text": "(1) and (2) are well-known principles and have been verified in other studies many times. For (3), we would like to highlight its effectiveness based on the following ablation study. We conduct the same experiment setting as Fig. 4(a) by using natural language to represent the knowledge. As shown in the table below, if we switch to the natural language representation, whatever the embedding models and retrieval techniques we used, the hit rate of the retrieval will drop a lot, as demonstrated by Tab. 4. We hope that this could provide insights for people interested in designing similar LLM-based frameworks for other decision-making tasks. ", "page_idx": 42}, {"type": "table", "img_path": "Ddak3nSqQM/tmp/1a81730f70a2f5d8aa2fcdffe56f85cb69ca07a51bbd8f00e1f85f6d8c172bb4.jpg", "table_caption": ["Table 4: Comparison of different knowledge representation "], "table_footnote": [], "page_idx": 42}, {"type": "text", "text": "F.7 Hyper-parameters ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Here, we list the important hyperparameters for URI here. ", "page_idx": 42}, {"type": "table", "img_path": "Ddak3nSqQM/tmp/2de49e8b3a4e5a72c7b4b35ae5021f7bce7f0bfd7db48e614bd09fe7d2cb62c4.jpg", "table_caption": ["Table 5: URI Hyperparameters. "], "table_footnote": [], "page_idx": 43}, {"type": "text", "text": "F.8 URI Algorithm ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "This algorithm implements the URI method for policy learning from books. It consists of three main stages: 1) Understanding (L1-L2): knowledge is extracted and aggregated from the input book using CodeExtractor and CodeAggregator. 2) Rehearsing (L3-L14): An imaginary dataset is generated by iteratively applying knowledge retrieval, instantiation, and language model-based simulation. 3) Introspecting (L15-L16): a policy is learned from the imaginary dataset using Conservative Imaginary Q-Learning (CIQL). The algorithm leverages language models $\\mathbf{M}_{\\pi}$ , $\\mathbf{M}_{R}$ , and ${{\\bf{M}}_{T}}$ for action selection, reward estimation, and state transition, respectively, bridging the gap between textual knowledge and reinforcement learning. ", "page_idx": 43}, {"type": "table", "img_path": "Ddak3nSqQM/tmp/6fa9c97709d1ff0c366980ae7501167b1f6722968ab4995b5e8046f999473b36.jpg", "table_caption": [], "table_footnote": [], "page_idx": 43}, {"type": "text", "text": "G Additional Ablations ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "G.1 Data Aggregation Visualization for Tic-Tac-Toe ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Similar to the visualization analysis in the main text, we also conduct t-SNE [58] visualization for the Tic-Tac-Toe environment to examine the quality of imaginary data generation and uncertainty estimation. As shown in Fig. 8, despite the discrete nature of the state space in Tic-Tac-Toe, we observe similar patterns as in the football environment. The imaginary data (separated into low and high uncertainty regions) largely follows the distribution of real data collected from optimal minimax gameplay. The uncertainty estimator successfully identifies deviant clusters (marked by yellow dashed circles) that lie outside the distribution of optimal play data. This consistency in behavior across both simple discrete environments and complex continuous ones supports the robustness of URI\u2019s data generation and uncertainty estimation mechanisms. ", "page_idx": 44}, {"type": "image", "img_path": "Ddak3nSqQM/tmp/6d300189fd9fc023c9a335b1b293045c4eddb2358683cda103291cf6553ab96b.jpg", "img_caption": ["Figure 8: Visualization of the projected distributions for real and imaginary datasets in the Tic-Tac-Toe environment. "], "img_footnote": [], "page_idx": 44}, {"type": "text", "text": "G.2 Performance Comparison with Additional Baselines in GRF ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "To further validate the effectiveness of our approach, we implement three additional baselines: (1) RT-Policy: a policy trained using LLAMA following the RT-style architecture [61], using the same 7,500 initial states as URI and rule-based actions for training; (2) URI w/o BK: a variant of URI that generates imaginary data without using book knowledge; and (3) CQL: a pure offline RL approach using the Conservative Q-Learning algorithm with real interaction data. ", "page_idx": 44}, {"type": "text", "text": "Table 6: Performance Comparison of RT-policy, URI w/o book knowledge, CQL, Against Built-in AI Levels in a GRF 11 vs 11 settings, where the performance of URI is averaged among three different seeds, all methods are tested with 20 matches except RT-Policy which are tested with 10 matches due to computation resource limitation. ", "page_idx": 44}, {"type": "table", "img_path": "Ddak3nSqQM/tmp/d9ee98878aa2fb713fadce08ffd708b098239002d1bbd8fa51ce36e88b327e03.jpg", "table_caption": [], "table_footnote": [], "page_idx": 44}, {"type": "text", "text": "H Examples of Data Generation and Policy Execution Videos ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "We list more visualization results for each stage of URI in the links: https://plfb-football. github.io/, including examples of knowledge extraction, code aggregation, imaginary data, and videos of policy execution in GRF. ", "page_idx": 44}, {"type": "text", "text": "I Open Problems and Future Directions ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "We hope that this promising result will initiate more research on PLfB. We will first elaborate on the points mentioned in Sec. 7 on applying URI when textual data are insufficient of low-quality. ", "page_idx": 44}, {"type": "text", "text": "\u2022 Multimodal Data Integration: Beyond purely textual data, additional modalities such as tutorial voices, demonstration videos, and replays can be incorporated. By employing advanced multimodal large language models, these diverse forms of data can be processed in a manner akin to the current handling within the URI, thereby augmenting the knowledge base and enhancing the robustness of policy learning. ", "page_idx": 44}, {"type": "text", "text": "\u2022 Utilization of Real Interaction Data: During the introspection phase, incorporating real interaction data with the target environment, rather than relying solely on simulated data generated by large language models, can enhance policy learning. This mixed-data approach can be used to further fine-tune various modules within the URI framework, potentially boosting overall performance. It also mimics human learning behavior better as humans learn from both reading and actual experience. However, one might develop new techniques to utilize both sources of data for better policy learning. ", "page_idx": 44}, {"type": "text", "text": "", "page_idx": 45}, {"type": "text", "text": "\u2022 Injection of Prior Knowledge: The URI framework allows for the integration of human expert knowledge at different stages of the pipeline. Experts can provide specific code knowledge representations/formulations/templates when generating code-based knowledge. We can also provide constraints during the rehearsal process to enhance stability and realism. ", "page_idx": 45}, {"type": "text", "text": "\u2022 Evaluating the Quality of Textual Data: Most current studies in this fields [62\u201364] are still in primitives. These studies suggest frameworks and empirical strategies that can be adapted to evaluate the relevance and quality of textual and multimodal data for training purposes. However, given the significant modality gap between textual data and the neural network parameters in PLfB topics\u2014and the largely unsupervised nature of our learning process\u2014this is not a trivial problem. This complexity requires innovative approaches for assessment. ", "page_idx": 45}, {"type": "text", "text": "We are also compelled to investigate other potential applications of the framework beyond the scope of the current experiment. Domains such as embodied AI for robotics, which require a large amount of data for foundation policy learning and the robotics dataset currently, could benefit significantly from the integration of the paradigm for data augmentation. This exploration opens exciting avenues for future research and application of the method. ", "page_idx": 45}, {"type": "text", "text": "J Compute Resources ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "All experiments were conducted on a high-performance computing (HPC) system featuring 128 Intel Xeon processors running at 2.2 GHz, 5 TB of memory, an Nvidia A100 PCIE-40G GPU, and two Nvidia A30 GPUs. This computational setup ensures efficient processing and reliable performance throughout the experiments. ", "page_idx": 45}, {"type": "text", "text": "K Broader Impact Statement ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Integrating Large Language Models (LLMs) with Reinforcement Learning (RL) in PLfB offers a groundbreaking approach to AI skill acquisition. This innovation has the potential to revolutionize industries reliant on automation, such as manufacturing and robotics, by enabling AI agents to learn complex tasks from textual sources, reducing the need for extensive real-world data. However, ethical considerations regarding bias, privacy, and transparency are paramount. Overall, PLfB promises enhanced efficiency and accessibility but requires careful navigation of its social implications. ", "page_idx": 45}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: We introduce the contribution in the abstract and the introduction section. ", "page_idx": 46}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: The limitation is discussed in Appendix I. ", "page_idx": 46}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 46}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 46}, {"type": "text", "text": "Justification: This paper does not include theoretical results. ", "page_idx": 46}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: We provided all details including prompt, LLM version, and hyper-parameters in Appendix F to guarantee the reproducibility. ", "page_idx": 46}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: We commit to open-source the code that can reproduce all the experiment results after the paper is published. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: We provided the experiment setup in Appendix B and the start of the experiment section. ", "page_idx": 46}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: The results reported are averaged over three random runs except for these with high cost of running, like LLM-as-agent and LLM-RAG ", "page_idx": 46}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: We provide the compute resources in Appendix J. ", "page_idx": 47}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: The research conforms with the NeurIPS Code of Ethics. ", "page_idx": 47}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: We provide the broader impacts statement in Appendix K. ", "page_idx": 47}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: This paper poses no such risks, as it does not release new pre-trained models or datasets. ", "page_idx": 47}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: The assets/environments used in this paper are properly credited: Google Research Football [55], d3rlpy [60], and RedPajama-1T [17]. ", "page_idx": 47}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 47}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 47}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 47}]