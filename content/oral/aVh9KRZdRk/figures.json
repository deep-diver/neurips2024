[{"figure_path": "aVh9KRZdRk/figures/figures_1_1.jpg", "caption": "Figure 1: (a) The dataset. The tasks are labeled by vectors (a, b) \u2208 Z2. Each table contains examples of ax + by mod p. A fraction 1/\u03b1 of the examples is blacked out; while the remaining examples are flattened into a single \"document\" in the batch. Each document is organized as a collection of triples (x, y, ax + by) for x, y from the training set (i.e. not blacked out in the table). Our training is similar to the traditional next-token prediction (autoregressive); with the main difference that we predict every third token, which are marked in red (x and y are uncorrelated). Every task appears exactly the same number of times in each batch. (b) Phase diagram for a six-layer model. We find four different phases. (1) in-distribution memorization: The model only performs well on tasks (a, b) and examples (x, y) from the training set \u2013 it does not generalize on unseen examples or tasks. (2) in-distribution generalization: model generalizes on unseen examples (x, y) but not on unseen tasks (a, b). (3) out-of-distribution memorization: model generalizes on unseen tasks (a, b) but only for examples (x, y) it has seen during training. (4) out-of-distribution generalization: model generalizes on unseen tasks (a, b) for seen as well as unseen examples (x, y). We focus on investigating phase (4) in more detail. (c) In-context sample complexity. Accuracy of the model in phase (4) as a function of the number of few-shot examples. (d) Representations developed by one of the attention heads in the first layer. These are projections of the embedding of a pair of numbers onto the two largest principal components (PCs) of the internal representation formed after passing through the attention layer and projection matrix. (e) First 3 PCs of embeddings separate log27-annotated numbers into even/odd planes, with 0 sandwiched between them.", "description": "This figure shows the dataset used for training and testing the model. The modular arithmetic tasks are represented as tables (a), where a fraction of examples is hidden. The model is trained to predict the output given input x and y. The phase diagram (b) illustrates the four different phases of model generalization, ranging from in-distribution memorization to out-of-distribution generalization. The in-context sample complexity (c) shows the accuracy as a function of the number of few-shot examples. The attention head representation (d) and principal component analysis (e) provide insights into the model's internal representations.", "section": "4 Emergence of In-Context Learning and Task Composition"}, {"figure_path": "aVh9KRZdRk/figures/figures_3_1.jpg", "caption": "Figure 2: Structured selection of pre-training tasks and sequences.", "description": "This figure illustrates the methodology for selecting pre-training tasks and designing sequences.  Panel (a) shows a schematic of the rectangular rule used for task selection.  New tasks are chosen by incrementally adjusting one parameter (a or b) while keeping the other constant.  This ensures a systematic exploration of the task space and facilitates the model's learning process. Panel (b) demonstrates the structure of the pre-training sequences.  Each batch contains an equal number of sequences for each task, and the sequences are structured to ensure that the model learns task vectors in a coherent, step-wise fashion.  The consistent sequence structure throughout all batches contributes to effective learning, reducing confusion and noise.", "section": "4 Emergence of In-Context Learning and Task Composition"}, {"figure_path": "aVh9KRZdRk/figures/figures_4_1.jpg", "caption": "Figure 3: Phase diagram for the depth d = 6 models. (a) Accuracy on all four sets used to plot the 1 phase diagram, with an early stopping applied. Notably, in the regions when models generalize to o.o.d. sets, the pre-training performance degrades; (b, c) a = 0.6 training accuracy and o.o.d. test accuracy (dotted line). For ni.d. = 28, we notice that the o.o.d. generalization ability of the model first improves then degrades as we train longer; (d, e) a = 0.6, loss and accuracy vs context length, measured on Sood. at the end of training, where for ni.d. = 28 case the ICL ability fades away.", "description": "This figure shows a phase diagram for a 6-layer transformer model trained on modular arithmetic tasks.  It illustrates the transition between different generalization phases as the number of pre-training tasks and the number of in-context examples vary. The four phases are: in-distribution memorization, in-distribution generalization, out-of-distribution memorization, and out-of-distribution generalization. Notably, the figure shows that out-of-distribution generalization is a transient phenomenon for deeper models, requiring early stopping to achieve optimal performance. The plots also demonstrate the relationship between loss and accuracy, as a function of the number of training steps and the number of in-context shots.", "section": "Emergence of In-Context Learning and Task Composition"}, {"figure_path": "aVh9KRZdRk/figures/figures_5_1.jpg", "caption": "Figure 3: Phase diagram for the depth d = 6 models. (a) Accuracy on all four sets used to plot the 1 phase diagram, with an early stopping applied. Notably, in the regions when models generalize to o.o.d. sets, the pre-training performance degrades; (b, c) a = 0.6 training accuracy and o.o.d. test accuracy (dotted line). For ni.d. = 28, we notice that the o.o.d. generalization ability of the model first improves then degrades as we train longer; (d, e) a = 0.6, loss and accuracy vs context length, measured on Sood. at the end of training, where for ni.d. = 28 case the ICL ability fades away.", "description": "This figure shows the phase diagram for a six-layer transformer model trained on modular arithmetic tasks.  It illustrates four distinct phases of generalization behavior as a function of the number of training tasks (ni.d.) and the fraction of training data used (a) at inference time. These phases are: in-distribution memorization, in-distribution generalization, out-of-distribution memorization, and out-of-distribution generalization. The figure also presents training and testing accuracy curves, showing how the out-of-distribution generalization ability of the model improves initially and then degrades with more training steps for a specific number of training tasks (ni.d. = 28). Finally, it shows loss and accuracy curves as a function of the context length (number of shots) used at inference time, further illustrating the trade-off between memorization and generalization in the model's behavior. ", "section": "4 Emergence of In-Context Learning and Task Composition"}, {"figure_path": "aVh9KRZdRk/figures/figures_6_1.jpg", "caption": "Figure 5: d = 4 and d = 2 models' performance on k-shot inference, on the grid of inputs (x,y) \u2208 Zp (task vector = (6,6)). row 1: Models' predictions on o.o.d. task of the type (x1 y1 z1 ... xk yk zk x y?). row 2: Analytical plots showing predictions solely based on Modular Regression algorithm. row 3: Subtract row 2 from row 1, by using correct=1 and incorrect=0. The red points correspond to the examples where Ratio Matching does not give the correct predictions but the model predicts correctly. The blue points are examples that the model missed despite Ratio Matching being applicable. This row tells us about the model's ability to implement Modular Regression by combining the in-context examples. Note that d = 4 model readily learns to combine previous examples, while its d = 2 counterpart struggles due to its limited capacity.", "description": "This figure compares the performance of depth 4 and 2 models on a modular arithmetic task with varying numbers of in-context examples (k-shot).  Row 1 shows the models' predictions, Row 2 shows the predictions based on the Modular Regression algorithm, and Row 3 highlights the differences. Red points indicate where the model outperforms Ratio Matching, while blue points show where Ratio Matching outperforms the model. The depth 4 model shows better ability to combine in-context examples.", "section": "Interpreting the Learned Algorithms"}, {"figure_path": "aVh9KRZdRk/figures/figures_7_1.jpg", "caption": "Figure 6: Models that generalize o.o.d. (left) exhibit more structured attention maps. Additionally, the top-2 principal components of the features from the corresponding heads also show more structured patterns. The features are computed for sequences with an o.o.d. task vector (at, bt) = (6,6), loop over (xi, Yi) at a specific shot while the previous parts of the sequence are fixed. (a) The principal components form a circle of circles where the position of the outer circle is controlled by xi. This pattern remains the same for different task vectors or the shot choices; (b) Only plotted pairs with even log27 Xi, with each log27 Xi circled with different colors. The PCA pattern forms a similar double circle as those in (a), with the key difference that those circles depend on task vector choices and the shot choices; (c, d) Models without o.o.d. generalization ability. We pick heads from the first block that corresponds to the first column of (a). Clearly, the structure of attention maps and PCA patterns deteriorate as the task diversity decreases.", "description": "This figure demonstrates that models capable of out-of-distribution generalization exhibit more structured attention maps and principal component analysis (PCA) patterns compared to models lacking this ability.  The structure is visualized through a \"circle of circles\" pattern, where the outer circle's position is determined by one of the input values. This pattern persists across various task vectors and shot choices.  The less structured patterns in models without out-of-distribution generalization are also shown for comparison.", "section": "Interpreting the Learned Algorithms"}, {"figure_path": "aVh9KRZdRk/figures/figures_8_1.jpg", "caption": "Figure 5: d = 4 and d = 2 models' performance on k-shot inference, on the grid of inputs (x,y) \u2208 Zp (task vector = (6,6)). row 1: Models' predictions on o.o.d. task of the type (x1 y1 z1 ... xk yk zk x y?). row 2: Analytical plots showing predictions solely based on Modular Regression algorithm. row 3: Subtract row 2 from row 1, by using correct=1 and incorrect=0. The red points correspond to the examples where Ratio Matching does not give the correct predictions but the model predicts correctly. The blue points are examples that the model missed despite Ratio Matching being applicable. This row tells us about the model's ability to implement Modular Regression by combining the in-context examples. Note that d = 4 model readily learns to combine previous examples, while its d = 2 counterpart struggles due to its limited capacity.", "description": "This figure compares the performance of two models (depth 4 and depth 2) on a modular arithmetic task with varying numbers of in-context examples (k-shot).  It shows that the deeper model (d=4) is able to leverage in-context examples to perform Modular Regression effectively, while the shallower model (d=2) primarily uses Ratio Matching, which is less effective.  The figure highlights the difference in algorithmic capabilities between the models due to their differences in capacity. Red and blue points indicate cases where the models deviate from the expected behavior of the respective algorithms.", "section": "Interpreting the Learned Algorithms"}, {"figure_path": "aVh9KRZdRk/figures/figures_9_1.jpg", "caption": "Figure 7: Cosine-similarities (Equation (2)) between layer outputs h at token z position (cos(h(x, y), h(x', y')), first row) and token y position (cos(h(x, y), h(x', y')), second row) reveal patterns in the models\u2019 internal representations. For clarity, we only show selected x and x\u2019 values, where y and y\u2019 values range from 0 to 28 between each tick. For the d = 4 model, kaleidoscopic patterns in the third layer indicate the generation of all possible yi/xi features for subsequent computations. The last layer shows an algorithmic shift from Ratio Matching to Modular Regression. The d = 2 model also shows the kaleidoscopic pattern in the first layer, while the second layer identifies the relevant y/x features from the in-context examples for matching.", "description": "This figure shows the cosine similarity between layer outputs at different token positions for both d=4 and d=2 models.  The d=4 model exhibits kaleidoscopic patterns in the third layer, indicating the generation of all possible y/x ratios for computation, and an algorithmic shift to Modular Regression in the final layer.  The d=2 model shows a similar kaleidoscopic pattern in the first layer but only uses Ratio Matching in the second layer.", "section": "Interpreting the Learned Algorithms"}, {"figure_path": "aVh9KRZdRk/figures/figures_13_1.jpg", "caption": "Figure 3: Phase diagram for the depth d = 6 models. (a) Accuracy on all four sets used to plot the 1 phase diagram, with an early stopping applied. Notably, in the regions when models generalize to o.o.d. sets, the pre-training performance degrades; (b, c) a = 0.6 training accuracy and o.o.d. test accuracy (dotted line). For ni.d. = 28, we notice that the o.o.d. generalization ability of the model first improves then degrades as we train longer; (d, e) a = 0.6, loss and accuracy vs context length, measured on Sood. at the end of training, where for ni.d. = 28 case the ICL ability fades away.", "description": "This figure shows a phase diagram for a 6-layer transformer model trained on modular arithmetic tasks.  The diagram illustrates four distinct phases of model behavior based on the number of pre-training tasks and the fraction of examples used at inference time. These phases are characterized by different levels of in-distribution and out-of-distribution generalization.  The figure also includes plots demonstrating the training accuracy, out-of-distribution test accuracy, loss, and accuracy as functions of the number of training steps and the number of in-context examples, revealing a trade-off between memorization and generalization in certain scenarios.  The diagram shows how the out-of-distribution generalization ability emerges and then disappears as training progresses for a specific number of tasks.", "section": "Emergence of In-Context Learning and Task Composition"}, {"figure_path": "aVh9KRZdRk/figures/figures_14_1.jpg", "caption": "Figure 3: Phase diagram for the depth d = 6 models. (a) Accuracy on all four sets used to plot the 1 phase diagram, with an early stopping applied. Notably, in the regions when models generalize to o.o.d. sets, the pre-training performance degrades; (b, c) a = 0.6 training accuracy and o.o.d. test accuracy (dotted line). For ni.d. = 28, we notice that the o.o.d. generalization ability of the model first improves then degrades as we train longer; (d, e) a = 0.6, loss and accuracy vs context length, measured on Sood. at the end of training, where for ni.d. = 28 case the ICL ability fades away.", "description": "This figure shows the phase diagram for a 6-layer transformer model trained on modular arithmetic tasks.  The diagram illustrates the model's performance across four phases: in-distribution memorization, in-distribution generalization, out-of-distribution memorization, and out-of-distribution generalization. It highlights a trade-off between in-distribution and out-of-distribution generalization as the number of training tasks increases, and a transient nature of out-of-distribution generalization ability in deeper models.", "section": "4 Emergence of In-Context Learning and Task Composition"}, {"figure_path": "aVh9KRZdRk/figures/figures_15_1.jpg", "caption": "Figure 1: (a) The dataset. The tasks are labeled by vectors (a, b) \u2208 Z2. Each table contains examples of ax + by mod p. A fraction 1 \u03b1 of the examples is blacked out; while the remaining examples are flattened into a single \"document\" in the batch. Each document is organized as a collection of triples (x, y, ax + by) for x, y from the training set (i.e. not blacked out in the table). Our training is similar to the traditional next-token prediction (autoregressive); with the main difference that we predict every third token, which are marked in red (x and y are uncorrelated). Every task appears exactly the same number of times in each batch. (b) Phase diagram for a six-layer model. We find four different phases. (1) in-distribution memorization: The model only performs well on tasks (a, b) and examples (x, y) from the training set \u2013 it does not generalize on unseen examples or tasks. (2) in-distribution generalization: model generalizes on unseen examples (x, y) but not on unseen tasks (a, b). (3) out-of-distribution memorization: model generalizes on unseen tasks (a, b) but only for examples (x, y) it has seen during training. (4) out-of-distribution generalization: model generalizes on unseen tasks (a, b) for seen as well as unseen examples (x, y). We focus on investigating phase (4) in more detail. (c) In-context sample complexity. Accuracy of the model in phase (4) as a function of the number of few-shot examples. (d) Representations developed by one of the attention heads in the first layer. These are projections of the embedding of a pair of numbers onto the two largest principal components (PCs) of the internal representation formed after passing through the attention layer and projection matrix. (e) First 3 PCs of embeddings separate log27-annotated numbers into even/odd planes, with 0 sandwiched between them.", "description": "This figure presents several key aspects of the modular arithmetic task dataset and the model's behavior.  Panel (a) shows the data format, where examples of modular arithmetic functions are presented with some masked values. Panel (b) illustrates a phase diagram for a six-layer transformer model, identifying four phases of generalization and memorization on in-distribution (i.d) and out-of-distribution (o.o.d) tasks. Panel (c) explores in-context sample complexity, showing how accuracy changes with the number of shots. Panels (d) and (e) offer insights into the model's internal representations, visualizing activation patterns in attention heads and the separation of even/odd numbers in the principal component analysis.", "section": "Emergence of In-Context Learning and Task Composition"}, {"figure_path": "aVh9KRZdRk/figures/figures_15_2.jpg", "caption": "Figure 6: Models that generalize o.o.d. (left) exhibit more structured attention maps. Additionally, the top-2 principal components of the features from the corresponding heads also show more structured patterns. The features are computed for sequences with an o.o.d. task vector (at, bt) = (6,6), loop over (xi, Yi) at a specific shot while the previous parts of the sequence are fixed. (a) The principal components form a circle of circles where the position of the outer circle is controlled by xi. This pattern remains the same for different task vectors or the shot choices; (b) Only plotted pairs with even log27 Xi, with each log27 Xi circled with different colors. The PCA pattern forms a similar double circle as those in (a), with the key difference that those circles depend on task vector choices and the shot choices; (c, d) Models without o.o.d. generalization ability. We pick heads from the first block that corresponds to the first column of (a). Clearly, the structure of attention maps and PCA patterns deteriorate as the task diversity decreases.", "description": "This figure shows the visualization of attention maps and principal component analysis (PCA) of attention head features for models that generalize out-of-distribution (o.o.d) and those that don't.  The o.o.d. generalizing models exhibit highly structured attention maps and PCA patterns forming \"circles of circles\", indicating the emergence of structured representations that are crucial for generalization. In contrast, models lacking o.o.d. generalization show less structured patterns, highlighting the relationship between structured representations and the ability to generalize to unseen tasks.", "section": "Interpreting the Learned Algorithms"}, {"figure_path": "aVh9KRZdRk/figures/figures_16_1.jpg", "caption": "Figure 6: Models that generalize o.o.d. (left) exhibit more structured attention maps. Additionally, the top-2 principal components of the features from the corresponding heads also show more structured patterns. The features are computed for sequences with an o.o.d. task vector (at, bt) = (6,6), loop over (xi, Yi) at a specific shot while the previous parts of the sequence are fixed. (a) The principal components form a circle of circles where the position of the outer circle is controlled by xi. This pattern remains the same for different task vectors or the shot choices; (b) Only plotted pairs with even log27 Xi, with each log27 Xi circled with different colors. The PCA pattern forms a similar double circle as those in (a), with the key difference that those circles depend on task vector choices and the shot choices; (c, d) Models without o.o.d. generalization ability. We pick heads from the first block that corresponds to the first column of (a). Clearly, the structure of attention maps and PCA patterns deteriorate as the task diversity decreases.", "description": "This figure shows the attention maps and principal component analysis (PCA) of attention head outputs for models that generalize out-of-distribution (OOD) and those that do not.  The OOD models exhibit highly structured attention patterns and PCA plots, forming \"circles of circles.\" The structure is consistent across different task vectors and shot choices. In contrast, models without OOD generalization show less structured attention maps and PCA plots, demonstrating a correlation between structured representations and OOD generalization ability.", "section": "Interpreting the Learned Algorithms"}, {"figure_path": "aVh9KRZdRk/figures/figures_16_2.jpg", "caption": "Figure 6: Models that generalize o.o.d. (left) exhibit more structured attention maps. Additionally, the top-2 principal components of the features from the corresponding heads also show more structured patterns. The features are computed for sequences with an o.o.d. task vector (at, bt) = (6,6), loop over (xi, Yi) at a specific shot while the previous parts of the sequence are fixed. (a) The principal components form a circle of circles where the position of the outer circle is controlled by xi. This pattern remains the same for different task vectors or the shot choices; (b) Only plotted pairs with even log27 Xi, with each log27 Xi circled with different colors. The PCA pattern forms a similar double circle as those in (a), with the key difference that those circles depend on task vector choices and the shot choices; (c, d) Models without o.o.d. generalization ability. We pick heads from the first block that corresponds to the first column of (a). Clearly, the structure of attention maps and PCA patterns deteriorate as the task diversity decreases.", "description": "This figure shows the analysis of attention heads in models that generalize out-of-distribution (o.o.d.) and those that do not.  The left panels show attention maps which are more structured in the o.o.d. generalizing models. The right panels show less structure.  The bottom panels show principal component analysis (PCA) of the attention features.  The o.o.d. generalizing models show circular patterns, while the non-generalizing models show less structure.  This demonstrates that the structured attention patterns are correlated with the ability to generalize o.o.d.", "section": "Interpreting the Learned Algorithms"}, {"figure_path": "aVh9KRZdRk/figures/figures_17_1.jpg", "caption": "Figure 15: All attention heads in d = 2 model.", "description": "This figure displays the attention patterns of all attention heads in a depth-2 model.  Each subplot shows an attention head's attention weights, visualized as a heatmap. These heatmaps illustrate the connections and dependencies between different tokens in the input sequence, providing insights into how the model processes information within each head. The patterns observed could reveal specific strategies or mechanisms utilized by the model for processing sequential data and achieving its tasks.", "section": "5 Interpreting the Learned Algorithms"}, {"figure_path": "aVh9KRZdRk/figures/figures_17_2.jpg", "caption": "Figure 6: Models that generalize o.o.d. (left) exhibit more structured attention maps. Additionally, the top-2 principal components of the features from the corresponding heads also show more structured patterns. The features are computed for sequences with an o.o.d. task vector (at, bt) = (6,6), loop over (xi, Yi) at a specific shot while the previous parts of the sequence are fixed. (a) The principal components form a circle of circles where the position of the outer circle is controlled by xi. This pattern remains the same for different task vectors or the shot choices; (b) Only plotted pairs with even log27 Xi, with each log27 Xi circled with different colors. The PCA pattern forms a similar double circle as those in (a), with the key difference that those circles depend on task vector choices and the shot choices; (c, d) Models without o.o.d. generalization ability. We pick heads from the first block that corresponds to the first column of (a). Clearly, the structure of attention maps and PCA patterns deteriorate as the task diversity decreases.", "description": "This figure shows the attention maps and principal component analysis (PCA) of the features from attention heads in models with and without out-of-distribution generalization ability.  The models that generalize well exhibit highly structured attention maps and PCA patterns forming circles, indicating structured representations. In contrast, models without o.o.d. generalization show less structure. The PCA analysis highlights how the representation changes with the input and the task, and how this structure degrades when the model does not generalize well.", "section": "Interpreting the Learned Algorithms"}, {"figure_path": "aVh9KRZdRk/figures/figures_18_1.jpg", "caption": "Figure 6: Models that generalize o.o.d. (left) exhibit more structured attention maps. Additionally, the top-2 principal components of the features from the corresponding heads also show more structured patterns. The features are computed for sequences with an o.o.d. task vector (at, bt) = (6,6), loop over (xi, Yi) at a specific shot while the previous parts of the sequence are fixed. (a) The principal components form a circle of circles where the position of the outer circle is controlled by xi. This pattern remains the same for different task vectors or the shot choices; (b) Only plotted pairs with even log27 Xi, with each log27 Xi circled with different colors. The PCA pattern forms a similar double circle as those in (a), with the key difference that those circles depend on task vector choices and the shot choices; (c, d) Models without o.o.d. generalization ability. We pick heads from the first block that corresponds to the first column of (a). Clearly, the structure of attention maps and PCA patterns deteriorate as the task diversity decreases.", "description": "This figure shows the attention maps and PCA analysis of the attention heads in models that generalize out-of-distribution (o.o.d.) versus those that do not.  The left side shows models exhibiting structured attention maps and PCA patterns forming \"circles of circles.\" The structure is consistent across different task vectors and shot choices, indicating a robust, generalized representation. The right side shows models without o.o.d. generalization, exhibiting less structured attention maps and PCA patterns.  The lack of structure suggests a memorization-based approach rather than a generalized algorithm.", "section": "Interpreting the Learned Algorithms"}, {"figure_path": "aVh9KRZdRk/figures/figures_18_2.jpg", "caption": "Figure 6: Models that generalize o.o.d. (left) exhibit more structured attention maps. Additionally, the top-2 principal components of the features from the corresponding heads also show more structured patterns. The features are computed for sequences with an o.o.d. task vector (at, bt) = (6,6), loop over (xi, Yi) at a specific shot while the previous parts of the sequence are fixed. (a) The principal components form a circle of circles where the position of the outer circle is controlled by xi. This pattern remains the same for different task vectors or the shot choices; (b) Only plotted pairs with even log27 Xi, with each log27 Xi circled with different colors. The PCA pattern forms a similar double circle as those in (a), with the key difference that those circles depend on task vector choices and the shot choices; (c, d) Models without o.o.d. generalization ability. We pick heads from the first block that corresponds to the first column of (a). Clearly, the structure of attention maps and PCA patterns deteriorate as the task diversity decreases.", "description": "This figure shows the attention maps and PCA analysis of the attention heads and MLPs for models with and without out-of-distribution generalization ability.  The left panels (a,b) show models with strong o.o.d. generalization, exhibiting highly structured attention maps and PCA patterns forming concentric circles.  These patterns are consistent across different task vectors and shots. The right panels (c,d) display models lacking o.o.d. generalization, showing less structured attention maps and PCA patterns, indicating a relationship between structured representations and the ability to generalize to unseen tasks. This demonstrates that the model's ability to generalize is connected to the structure of its representations.", "section": "Interpreting the Learned Algorithms"}, {"figure_path": "aVh9KRZdRk/figures/figures_18_3.jpg", "caption": "Figure 5: d = 4 and d = 2 models' performance on k-shot inference, on the grid of inputs (x,y) \u2208 Zp (task vector = (6,6)). row 1: Models' predictions on o.o.d. task of the type (x1 y1 z1 ... xk yk zk x y ?). row 2: Analytical plots showing predictions solely based on Modular Regression algorithm. row 3: Subtract row 2 from row 1, by using correct=1 and incorrect=0. The red points correspond to the examples where Ratio Matching does not give the correct predictions but the model predicts correctly. The blue points are examples that the model missed despite Ratio Matching being applicable. This row tells us about the model's ability to implement Modular Regression by combining the in-context examples. Note that d = 4 model readily learns to combine previous examples, while its d = 2 counterpart struggles due to its limited capacity.", "description": "This figure compares the performance of 4-layer and 2-layer transformer models on a modular arithmetic task.  It shows that the 4-layer model is better able to generalize to unseen inputs by combining information from multiple in-context examples (using Modular Regression), while the 2-layer model struggles with this task, relying more heavily on simpler pattern matching (Ratio Matching). The figure uses a grid of inputs to systematically evaluate model performance and highlights the differences in algorithmic strategies employed by the models of different depths.", "section": "Interpreting the Learned Algorithms"}, {"figure_path": "aVh9KRZdRk/figures/figures_19_1.jpg", "caption": "Figure 7: Cosine-similarities (Equation (2)) between layer outputs h at token z position (cos(h(x, y), h(x', y')), first row) and token y position (cos(h(x, y), h(x', y')), second row) reveal patterns in the models' internal representations. For clarity, we only show selected x and x' values, where y and y' values range from 0 to 28 between each tick. For the d = 4 model, kaleidoscopic patterns in the third layer indicate the generation of all possible yi/xi features for subsequent computations. The last layer shows an algorithmic shift from Ratio Matching to Modular Regression. The d = 2 model also shows the kaleidoscopic pattern in the first layer, while the second layer identifies the relevant y/x features from the in-context examples for matching.", "description": "This figure displays cosine similarity matrices for the outputs of different layers in depth-2 and depth-4 models.  The matrices show the cosine similarity between the output vectors for different input pairs (x, y) and (x', y'). The depth-4 model shows a clear transition from Ratio Matching (earlier layers) to Modular Regression (later layers), indicated by the characteristic patterns in the cosine similarity matrices. The depth-2 model shows less structured patterns, suggesting it relies more heavily on Ratio Matching.", "section": "Interpreting the Learned Algorithms"}, {"figure_path": "aVh9KRZdRk/figures/figures_19_2.jpg", "caption": "Figure 7: Cosine-similarities (Equation (2)) between layer outputs h at token z position (cos(h(x, y), h(x', y')), first row) and token y position (cos(h(x, y), h(x', y')), second row) reveal patterns in the models' internal representations. For clarity, we only show selected x and x' values, where y and y' values range from 0 to 28 between each tick. For the d = 4 model, kaleidoscopic patterns in the third layer indicate the generation of all possible yi/xi features for subsequent computations. The last layer shows an algorithmic shift from Ratio Matching to Modular Regression. The d = 2 model also shows the kaleidoscopic pattern in the first layer, while the second layer identifies the relevant y/x features from the in-context examples for matching.", "description": "This figure displays cosine similarity matrices for layer outputs at token positions z and y in both d=2 and d=4 models, illustrating the internal representations and algorithmic shifts.  The d=4 model shows a transition from Ratio Matching to Modular Regression as more in-context examples are provided, reflected in distinctive patterns across layers. The d=2 model exhibits a simpler pattern, mainly showing Ratio Matching.", "section": "Interpreting the Learned Algorithms"}, {"figure_path": "aVh9KRZdRk/figures/figures_19_3.jpg", "caption": "Figure 7: Cosine-similarities (Equation (2)) between layer outputs h at token z position (cos(h(x, y), h(x', y')), first row) and token y position (cos(h(x, y), h(x', y')), second row) reveal patterns in the models' internal representations. For clarity, we only show selected x and x' values, where y and y' values range from 0 to 28 between each tick. For the d = 4 model, kaleidoscopic patterns in the third layer indicate the generation of all possible yi/xi features for subsequent computations. The last layer shows an algorithmic shift from Ratio Matching to Modular Regression. The d = 2 model also shows the kaleidoscopic pattern in the first layer, while the second layer identifies the relevant y/x features from the in-context examples for matching.", "description": "This figure displays cosine similarity matrices for layer outputs at token positions y and z for both d=4 and d=2 models.  The d=4 model shows a distinctive kaleidoscopic pattern in layer 3, indicative of generating all possible y/x ratios for calculations, while transitioning to Modular Regression in the final layer. The d=2 model exhibits a simpler pattern, utilizing Ratio Matching primarily, with layer 2 identifying relevant y/x ratios from given examples.", "section": "Interpreting the Learned Algorithms"}, {"figure_path": "aVh9KRZdRk/figures/figures_20_1.jpg", "caption": "Figure 7: Cosine-similarities (Equation (2)) between layer outputs h at token z position (cos(h(x, y), h(x', y')), first row) and token y position (cos(h(x, y), h(x', y')), second row) reveal patterns in the models\u2019 internal representations. For clarity, we only show selected x and x\u2019 values, where y and y\u2019 values range from 0 to 28 between each tick. For the d = 4 model, kaleidoscopic patterns in the third layer indicate the generation of all possible yi/xi features for subsequent computations. The last layer shows an algorithmic shift from Ratio Matching to Modular Regression. The d = 2 model also shows the kaleidoscopic pattern in the first layer, while the second layer identifies the relevant y/x features from the in-context examples for matching.", "description": "This figure shows cosine similarity matrices for layer outputs at token positions z and y for both d=4 and d=2 models.  The d=4 model exhibits kaleidoscopic patterns in layer 3, suggesting the generation of all possible y/x ratios. In contrast, the d=2 model shows simpler patterns, reflecting the differences in algorithmic complexity between the two models and their transition from Ratio Matching to Modular Regression.", "section": "Interpreting the Learned Algorithms"}, {"figure_path": "aVh9KRZdRk/figures/figures_20_2.jpg", "caption": "Figure 3: Phase diagram for the depth d = 6 models. (a) Accuracy on all four sets used to plot the 1 phase diagram, with an early stopping applied. Notably, in the regions when models generalize to o.o.d. sets, the pre-training performance degrades; (b, c) a = 0.6 training accuracy and o.o.d. test accuracy (dotted line). For ni.d. = 28, we notice that the o.o.d. generalization ability of the model first improves then degrades as we train longer; (d, e) a = 0.6, loss and accuracy vs context length, measured on Sood. at the end of training, where for ni.d. = 28 case the ICL ability fades away.", "description": "This figure shows a phase diagram for a depth-6 transformer model trained on modular arithmetic tasks. The diagram illustrates the transition from in-distribution to out-of-distribution generalization as the number of pre-training tasks increases.  It also shows the effect of training steps and the number of in-context examples on the model's accuracy. The model exhibits a transient phase where out-of-distribution generalization is observed but eventually degrades with prolonged training, particularly noticeable when the number of pre-training tasks is 28. This suggests a trade-off between memorization and generalization in the model's learning dynamics.", "section": "Emergence of In-Context Learning and Task Composition"}, {"figure_path": "aVh9KRZdRk/figures/figures_21_1.jpg", "caption": "Figure 3: Phase diagram for the depth d = 6 models. (a) Accuracy on all four sets used to plot the 1 phase diagram, with an early stopping applied. Notably, in the regions when models generalize to o.o.d. sets, the pre-training performance degrades; (b, c) a = 0.6 training accuracy and o.o.d. test accuracy (dotted line). For ni.d. = 28, we notice that the o.o.d. generalization ability of the model first improves then degrades as we train longer; (d, e) a = 0.6, loss and accuracy vs context length, measured on Sood. at the end of training, where for ni.d. = 28 case the ICL ability fades away.", "description": "This figure shows the phase diagram for a 6-layer transformer model trained on modular arithmetic tasks.  The diagram illustrates four distinct phases of generalization: in-distribution memorization, in-distribution generalization, out-of-distribution memorization, and out-of-distribution generalization. The transition between these phases depends on the number of pre-training tasks and the number of in-context examples.  The plots also show the training accuracy and out-of-distribution test accuracy as a function of the training steps and the number of shots, highlighting the transient nature of out-of-distribution generalization for certain model configurations.", "section": "Emergence of In-Context Learning and Task Composition"}, {"figure_path": "aVh9KRZdRk/figures/figures_21_2.jpg", "caption": "Figure 3: Phase diagram for the depth d = 6 models. (a) Accuracy on all four sets used to plot the 1 phase diagram, with an early stopping applied. Notably, in the regions when models generalize to o.o.d. sets, the pre-training performance degrades; (b, c) a = 0.6 training accuracy and o.o.d. test accuracy (dotted line). For ni.d. = 28, we notice that the o.o.d. generalization ability of the model first improves then degrades as we train longer; (d, e) a = 0.6, loss and accuracy vs context length, measured on Sood. at the end of training, where for ni.d. = 28 case the ICL ability fades away.", "description": "This figure shows the phase diagram for a six-layer model trained on modular arithmetic tasks. The diagram illustrates four distinct phases depending on the number of training tasks and the fraction of training data used. The phases are: in-distribution memorization, in-distribution generalization, out-of-distribution memorization, and out-of-distribution generalization. The figure also shows the training accuracy and out-of-distribution test accuracy as functions of the number of training steps and the number of in-context examples. Finally, it demonstrates how the out-of-distribution generalization ability of the model first improves and then degrades as training progresses.", "section": "Emergence of In-Context Learning and Task Composition"}, {"figure_path": "aVh9KRZdRk/figures/figures_22_1.jpg", "caption": "Figure 3: Phase diagram for the depth d = 6 models. (a) Accuracy on all four sets used to plot the 1 phase diagram, with an early stopping applied. Notably, in the regions when models generalize to o.o.d. sets, the pre-training performance degrades; (b, c) a = 0.6 training accuracy and o.o.d. test accuracy (dotted line). For ni.d. = 28, we notice that the o.o.d. generalization ability of the model first improves then degrades as we train longer; (d, e) a = 0.6, loss and accuracy vs context length, measured on Sood. at the end of training, where for ni.d. = 28 case the ICL ability fades away.", "description": "This figure shows the phase diagram for a 6-layer transformer model trained on modular arithmetic tasks.  The diagrams illustrate the model's performance across four distinct phases as the number of pre-training tasks and the fraction of training data used for few-shot learning vary.  The phases represent different levels of generalization capability, ranging from memorization of training data to out-of-distribution generalization.  Importantly, the figure also highlights a trade-off between in-distribution and out-of-distribution generalization, particularly for a model with 28 pre-training tasks.  Additional plots show the training loss and accuracy as a function of training steps and the number of few-shot examples, emphasizing the transient nature of out-of-distribution generalization in deeper models and the impact of context length.", "section": "Emergence of In-Context Learning and Task Composition"}, {"figure_path": "aVh9KRZdRk/figures/figures_23_1.jpg", "caption": "Figure 28: We compare the best performance for d = 6 models with different  ni.d. values, averaged over three seeds. We use learning rate \u03b7 = 10\u22124 for p = 37 and p = 47, while keeping other hyperparameters the same as p = 29.", "description": "This figure shows the effect of varying task difficulties (controlled by the value of *p*) on the model's ability to generalize out-of-distribution. The x-axis represents the number of pre-training tasks (*ni.d.*), and the y-axis shows both the loss and the accuracy.  Different lines represent different values of *p* (29, 37, 47). The results indicate that as the task difficulty increases (larger *p*), the model requires a greater number of pre-training tasks to achieve out-of-distribution generalization. ", "section": "4.2 Effect of Model Size and Task Difficulty"}]