[{"figure_path": "0XeNkkENuI/figures/figures_1_1.jpg", "caption": "Figure 1: Schedule-Free methods (black) closely track the Pareto frontier of loss v.s. training time in a single run. Both Schedule-Free SGD (left) and AdamW (right) match or exceed the performance of cosine learning rate schedules of varying lengths (red).", "description": "This figure shows the training performance of Schedule-Free SGD and Schedule-Free AdamW compared to traditional cosine learning rate schedules.  The black lines represent the Schedule-Free methods, demonstrating that they closely follow the Pareto frontier (optimal balance between loss and training time).  The red lines represent cosine schedules with different lengths. The results show that Schedule-Free methods perform comparably to or better than the tuned cosine schedules, even without requiring the specification of the optimization stopping time.", "section": "Summary of Results"}, {"figure_path": "0XeNkkENuI/figures/figures_2_1.jpg", "caption": "Figure 1: Schedule-Free methods (black) closely track the Pareto frontier of loss v.s. training time in a single run. Both Schedule-Free SGD (left) and AdamW (right) match or exceed the performance of cosine learning rate schedules of varying lengths (red).", "description": "This figure shows the performance of Schedule-Free SGD and Schedule-Free AdamW compared to cosine learning rate schedules.  Both Schedule-Free methods track the Pareto frontier (optimal balance between training time and loss) closely.  In both the left and right panels, the Schedule-Free method matches or surpasses the performance of the cosine schedules.", "section": "Summary of Results"}, {"figure_path": "0XeNkkENuI/figures/figures_2_2.jpg", "caption": "Figure 2b: Incorporating the momentum parameter \u03b2 allows for convergence despite using larger learning rates \u03b3 on quadratic problems.", "description": "The figure is a heatmap showing the minimal loss achieved as a function of the two parameters \u03b2 (momentum parameter) and \u03b3 (learning rate). The x-axis represents different values of \u03b3, and the y-axis represents different values of \u03b2. The color of each cell in the heatmap indicates the minimal loss achieved for the given values of \u03b2 and \u03b3. The heatmap reveals that when the learning rate \u03b3 is small, the value of \u03b2 has little effect on the convergence of the algorithm. However, when \u03b3 is large, choosing \u03b2 < 1 becomes crucial for achieving convergence.", "section": "Method"}, {"figure_path": "0XeNkkENuI/figures/figures_7_1.jpg", "caption": "Figure 3: Deep Learning Experiments", "description": "This figure presents the results of deep learning experiments comparing the performance of Schedule-Free methods against traditional cosine learning rate schedules and step-wise schedules across various benchmark datasets and architectures.  The results demonstrate that Schedule-Free methods consistently match or exceed the performance of the other methods, highlighting the efficacy of the proposed approach. The datasets include CIFAR-10, CIFAR-100, SVHN, ImageNet, IWSLT14, fastMRI, Criteo Kaggle, and OpenWebText.  The architectures range from relatively simple convolutional neural networks to complex Transformers.", "section": "4 Experiments"}, {"figure_path": "0XeNkkENuI/figures/figures_8_1.jpg", "caption": "Figure 4: Schedule-Free Adam compared to target-setting baseline on the Algoperf competition self-tuning track.", "description": "This figure compares the performance of Schedule-Free AdamW against the NAdamW baseline in the MLCommons AlgoPerf Algorithmic Efficiency Challenge Self-Tuning track.  The figure presents normalized test metrics (y-axis) against normalized time (x-axis) across eight different deep learning tasks: WMT, ViT, fastMRI, Librispeech Conformer, OGBG, Criteo1TB, Librispeech Deepspeech. Each task is presented as a separate subplot. The black lines represent the performance of Schedule-Free AdamW across ten different random seeds. The red dotted line shows the NAdamW baseline. The results indicate that Schedule-Free AdamW generally matches or exceeds the performance of the NAdamW baseline across various tasks.", "section": "4.1 MLCommons Algorithmic Efficiency benchmark"}, {"figure_path": "0XeNkkENuI/figures/figures_9_1.jpg", "caption": "Figure 3: Deep Learning Experiments", "description": "This figure compares the performance of Schedule-Free methods against cosine learning rate schedules and step-wise schedules on various deep learning tasks.  The results show that Schedule-Free methods closely track the Pareto frontier of loss vs. training time, often matching or exceeding the performance of tuned schedules across a range of problems, including image classification, translation, and natural language processing.", "section": "4 Experiments"}, {"figure_path": "0XeNkkENuI/figures/figures_9_2.jpg", "caption": "Figure 5: Sensitivity to momentum values", "description": "The figure shows the impact of different momentum values (\u03b2) on the convergence of the Schedule-Free method. It uses ImageNet ResNet-50 training for 200 epochs with a fixed learning rate of 1.5.  The results indicate that the optimal momentum value (\u03b2=0.9) remains consistent across different training durations, demonstrating the time-horizon independence of this hyperparameter in Schedule-Free learning.", "section": "Parameter Sensitivity"}, {"figure_path": "0XeNkkENuI/figures/figures_29_1.jpg", "caption": "Figure 7: Stochastic logistic regression experiments.", "description": "This figure shows the results of stochastic logistic regression experiments, comparing the performance of Polyak averaging, primal averaging, Schedule-Free, and a linear decay schedule across twelve different datasets.  Each subplot represents a dataset and shows the accuracy over epochs for each method.  The results visually demonstrate the superior performance of the Schedule-Free approach across several datasets.", "section": "H Convex Experiments"}, {"figure_path": "0XeNkkENuI/figures/figures_30_1.jpg", "caption": "Figure 8: Polyak and Primal Averaging Experiments", "description": "This figure compares the performance of Polyak averaging, primal averaging, and the Schedule-Free method on various deep learning tasks.  Each subplot shows the test accuracy or loss over epochs for a specific task. The results demonstrate that the Schedule-Free method generally matches or exceeds the performance of the other averaging methods, indicating its effectiveness across diverse machine learning problems.", "section": "I Polyak and Primal Averaging Runs"}, {"figure_path": "0XeNkkENuI/figures/figures_31_1.jpg", "caption": "Figure 3: Deep Learning Experiments", "description": "The figure shows the performance comparison of Schedule-Free methods against cosine learning rate schedules and step-wise schedules on various deep learning tasks, including CIFAR-10, CIFAR-100, SVHN, ImageNet, IWSLT14, fastMRI, Criteo DLRM, and OpenWebText.  The results demonstrate that Schedule-Free methods closely track the Pareto frontier of loss versus training time and often outperform tuned schedules.", "section": "4 Experiments"}]