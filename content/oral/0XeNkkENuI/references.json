{"references": [{"fullname_first_author": "Bach, F.", "paper_title": "Non-strongly-convex smooth stochastic approximation with convergence rate O(1/n)", "publication_date": "2013", "reason": "This paper provides foundational theory on non-strongly convex optimization, which is highly relevant to the core method developed in the main paper."}, {"fullname_first_author": "Cesa-Bianchi, N.", "paper_title": "On the generalization ability of on-line learning algorithms", "publication_date": "2004", "reason": "This paper offers fundamental insights into the generalization capabilities of online learning algorithms, crucial for analyzing the anytime convergence guarantees of the proposed method."}, {"fullname_first_author": "Defazio, A.", "paper_title": "Momentum via primal averaging: Theoretical insights and learning rate schedules for non-convex optimization", "publication_date": "2020", "reason": "This paper's theoretical analysis of momentum and primal averaging is directly relevant to the method's foundation and its comparison with other approaches."}, {"fullname_first_author": "Defazio, A.", "paper_title": "Adaptivity without compromise: A momentumized, adaptive, dual averaged gradient method for stochastic optimization", "publication_date": "2022", "reason": "This paper offers an improved momentum method compared to Polyak-Ruppert and primal averaging, crucial for understanding and comparing the proposed approach."}, {"fullname_first_author": "Joulani, P.", "paper_title": "A simpler approach to accelerated optimization: iterative averaging meets optimism", "publication_date": "2020", "reason": "This work provides a unification of accelerated optimization methods by combining averaging and optimism, which is directly related to the main paper's theoretical contributions."}]}