[{"figure_path": "C4NbtYnyQg/figures/figures_1_1.jpg", "caption": "Figure 1: Left: Learning effects of traditional Teacher-Student Consistency Model (TSCM, e.g., SimGCD [75]) and our Flipped Classroom Consistency Model (FlipClass) on Stanford Cars [33]. Middle: Model comparison between TSCM and our FlipClass, where Dnew refers to data belonging to new classes. Right: Illustration of the inner feedback mechanism in FlipClass, where teacher attention is adapted to the student, leading to the alignment of attention.", "description": "This figure demonstrates the differences between traditional Teacher-Student Consistency Models (TSCM) and the proposed FlipClass model in handling new classes during learning. The left panel shows the learning curves of TSCM and FlipClass on the Stanford Cars dataset, highlighting the learning gap and unsynchronized learning in TSCM. The middle panel compares the model performances of TSCM and FlipClass, illustrating how FlipClass addresses the issue of inconsistent feature learning across different classes. The right panel illustrates the inner feedback mechanism of FlipClass, demonstrating how teacher attention is dynamically adapted based on student feedback, thereby achieving synchronized learning.", "section": "1 Introduction"}, {"figure_path": "C4NbtYnyQg/figures/figures_2_1.jpg", "caption": "Figure 2: Exploring prior gaps between SSL and GCD on SCars and CUB datasets. Left: Accuracy of sorted pseudo labels for old and new classes. Middle: Consistency loss trends over epochs, illustrating challenges in optimization and slower convergence for new classes. Right: Categorize errors [75], where \u201cTrue Old\u201d refers to predicting an \u2018Old\u2019 class sample to another \u2018Old\u2019 class, while \u2018False Old\u201d indicates predicting an \u2018Old\u2019 class sample as some \u2018New\u2019 class.", "description": "This figure demonstrates the challenges of applying traditional semi-supervised learning (SSL) methods to the task of Generalized Category Discovery (GCD). The left panel shows that the quality of pseudo-labels generated by SSL models is significantly lower for new classes than for old classes.  The middle panel shows that the consistency loss, a common objective function in SSL, converges slower for new classes in GCD. The right panel shows that SSL models tend to misclassify new classes as old classes more frequently than vice versa, indicating a bias towards previously seen categories.", "section": "Background"}, {"figure_path": "C4NbtYnyQg/figures/figures_4_1.jpg", "caption": "Figure 3: Left: Attention heatmaps for teacher and student across attention layers. Right: Energy trend over epochs, with lower energy indicating less discrepancy in pattern recognition between teacher and student.", "description": "The figure demonstrates the attention heatmaps for both teacher and student models across different attention layers (left).  The heatmaps visually represent where the models focus their attention on the input image.  The right side shows the energy trend across training epochs. Lower energy indicates better alignment and less discrepancy between the teacher and student's pattern recognition.  The visualization highlights how the proposed method aligns teacher and student attention, improving learning consistency.", "section": "3 How Consistency Loss Goes Awry: Unraveling the Pitfalls"}, {"figure_path": "C4NbtYnyQg/figures/figures_5_1.jpg", "caption": "Figure 4: Framework of FlipClass demonstrating teacher-student interaction, where teacher's and student's attention is aligned by teacher's updating (Eq. 8). Then Lrep and Lcons are combined for optimization.", "description": "This figure illustrates the FlipClass framework, which dynamically updates the teacher's attention based on student feedback. The teacher's attention is adjusted to align with the student's attention, promoting synchronized learning. The framework consists of a transformer encoder, projectors for the teacher and student, and a consistency loss (Lcons) and a representation learning loss (Lrep). The consistency loss ensures that the teacher and student produce consistent predictions, while the representation learning loss encourages the model to learn effective representations.", "section": "4 FlipClass: Teacher-Student Attention Alignment"}, {"figure_path": "C4NbtYnyQg/figures/figures_7_1.jpg", "caption": "Figure 5: Ablation study results for FlipClass, indicate the critical role of strong augmentations, attention alignment, and regularization in model performance across multiple datasets.", "description": "The figure shows the ablation study results for the FlipClass model. It demonstrates the importance of three key components: strong augmentations, attention alignment, and regularization.  Removing any one of these components significantly reduces the model's performance across multiple datasets (CUB, SCars, Aircraft, CIFAR-10, and CIFAR-100). The results highlight the synergistic effect of these components in achieving high accuracy, especially for new classes.", "section": "5 Experiments"}, {"figure_path": "C4NbtYnyQg/figures/figures_8_1.jpg", "caption": "Figure 6: Accuracy and representation alignment with different strategies: (1) initial state, (2) distribution alignment, (3) FixMatch, and (4) our teacher-attention update. Performance on 'New' and 'Old' classes are shown, alongside alignment of teacher (red) and student (blue) representation.", "description": "This figure shows the comparison of accuracy and representation alignment among different strategies: initial state, logits alignment, consistency loss, and the proposed method (FlipClass).  The visualization uses t-SNE to project high-dimensional feature embeddings into 2D space for both teacher and student models, colored red and blue respectively.  The color intensity represents the density of data points.  The figure demonstrates how the proposed method improves representation alignment and accuracy, especially for 'New' classes, which lack explicit supervision.", "section": "4 FlipClass: Teacher-Student Attention Alignment"}, {"figure_path": "C4NbtYnyQg/figures/figures_8_2.jpg", "caption": "Figure 7: Attention alignment methods comparison and categorize errors with different update rates.", "description": "This figure compares different attention alignment methods and analyzes the categorization errors with various update rates.  Subfigure (a) shows a comparison of attention alignment methods, highlighting the effectiveness of the proposed teacher-attention update strategy. Subfigure (b) displays categorization errors on CIFAR100 and CUB datasets, illustrating how different update rates affect the model's robustness and reduce prediction bias for 'False Old' and 'False New' classes.", "section": "3 How Consistency Loss Goes Awry: Unraveling the Pitfalls"}, {"figure_path": "C4NbtYnyQg/figures/figures_9_1.jpg", "caption": "Figure 8: Attention alignment improves energy dynamic and brings performance gains.", "description": "The figure demonstrates that aligning attention between the teacher and student improves energy dynamics and enhances performance. The left part shows the performance regarding attention-update layers across different epochs, indicating that deeper layers (8-11) generally yield better accuracy. The right part compares the representation quality and class-wise accuracy between InfoSieve and FlipClass, demonstrating that FlipClass offers superior representation learning and accuracy.  Specifically, FlipClass's representations are more compact, resulting in less confusion between classes, and it performs significantly better on tail classes.", "section": "3 How Consistency Loss Goes Awry: Unraveling the Pitfalls"}, {"figure_path": "C4NbtYnyQg/figures/figures_9_2.jpg", "caption": "Figure 9: Zoom-in comparison of InfoSieve and FlipClass on the CUB dataset using t-SNE and PCA. FlipClass shows improved cluster separation and compactness.", "description": "This figure compares the visualization of the CUB dataset's features using t-SNE and PCA dimensionality reduction techniques by InfoSieve and FlipClass.  The visualization highlights that FlipClass produces more distinct and well-separated clusters, indicating improved cluster separation and compactness compared to InfoSieve.", "section": "Analysis and Discussion"}, {"figure_path": "C4NbtYnyQg/figures/figures_21_1.jpg", "caption": "Figure 10: Left: Comparison of representation discrepancy with respect to old and new classes before and after training, showing the misalignment of student (blue) and teacher (red). Right: Learning unsynchronization between teacher and student with trends of learning regression and learning gap for old and new classes.", "description": "This figure shows the comparison of representation discrepancy between old and new classes before and after training. The left part shows the misalignment of student and teacher representations, especially for new classes. The right part shows the learning unsynchronization between teacher and student. The learning gap and learning regression indicate the learning progress is not synchronized, especially for new classes.", "section": "B Extended Experimental Analysis of Attention Alignment"}, {"figure_path": "C4NbtYnyQg/figures/figures_21_2.jpg", "caption": "Figure 1: Left: Learning effects of traditional Teacher-Student Consistency Model (TSCM, e.g., SimGCD [75]) and our Flipped Classroom Consistency Model (FlipClass) on Stanford Cars [33]. Middle: Model comparison between TSCM and our FlipClass, where Dnew refers to data belonging to new classes. Right: Illustration of the inner feedback mechanism in FlipClass, where teacher attention is adapted to the student, leading to the alignment of attention.", "description": "This figure shows a comparison of the learning curves between traditional Teacher-Student Consistency Models (TSCM) and the proposed FlipClass model.  The left panel demonstrates the learning gap and unsynchronized learning in the TSCM approach, particularly for new classes. The middle panel highlights the superior performance of FlipClass in aligning teacher and student learning, resulting in improved consistency.  The right panel illustrates the inner feedback mechanism of FlipClass, where the teacher dynamically adjusts its attention based on student feedback, promoting synchronized learning and consistency between old and new classes.", "section": "1 Introduction"}, {"figure_path": "C4NbtYnyQg/figures/figures_22_1.jpg", "caption": "Figure 12: Learning curves for SCars and CUB datasets. FlipClass achieves better synchronized and stable learning effects compared to the traditional teacher-student model.", "description": "This figure compares the learning curves of the traditional Teacher-Student Consistency Model (TSCM) and the proposed FlipClass model on the Stanford Cars and CUB datasets.  The learning curves show the accuracy of the teacher and student models over epochs for both old and new classes.  FlipClass demonstrates significantly better synchronized and stable learning compared to the TSCM, indicating that its attention alignment strategy leads to more consistent and effective learning across all classes.", "section": "B Extended Experimental Analysis of Attention Alignment"}, {"figure_path": "C4NbtYnyQg/figures/figures_22_2.jpg", "caption": "Figure 13: Attention heatmap accuracy per layer on SCars dataset, with deeper layers focused on local semantic features and earlier layers on general features, indicating better transfer learning for old and new classes with attention alignment in deeper network layers.", "description": "This figure shows the attention heatmaps for different layers in the vision transformer network when processing images from the Stanford Cars dataset.  The heatmaps reveal a pattern where deeper layers focus more on specific, localized features (like a car's headlights or wheels), while shallower layers attend to more general features (like the overall shape or color of the car). This is evidence that attention alignment is effective at improving transfer learning and helping the model recognize both old and new car classes more effectively. The aligned attention improves performance because it addresses the learning gap and discrepancies often encountered with existing semi-supervised learning techniques in open-world scenarios.", "section": "B.4 Attention Specialization in Deep Network Layers"}, {"figure_path": "C4NbtYnyQg/figures/figures_23_1.jpg", "caption": "Figure 1: Left: Learning effects of traditional Teacher-Student Consistency Model (TSCM, e.g., SimGCD [75]) and our Flipped Classroom Consistency Model (FlipClass) on Stanford Cars [33]. Middle: Model comparison between TSCM and our FlipClass, where Dnew refers to data belonging to new classes. Right: Illustration of the inner feedback mechanism in FlipClass, where teacher attention is adapted to the student, leading to the alignment of attention.", "description": "This figure compares the performance of traditional Teacher-Student Consistency Models (TSCM) and the proposed FlipClass model on the Stanford Cars dataset. The left panel shows the learning curves, highlighting the significant learning gap and unsynchronized learning in TSCM compared to FlipClass. The middle panel illustrates the model comparison, emphasizing the difference in how TSCM and FlipClass handle data from new classes (Dnew). The right panel visually explains FlipClass's inner feedback mechanism, demonstrating how it dynamically updates the teacher's attention to align with the student's focus, achieving better learning synchronization.", "section": "1 Introduction"}, {"figure_path": "C4NbtYnyQg/figures/figures_26_1.jpg", "caption": "Figure 15: Comparison of clustering results on Cifar-10 and Cifar-100 datasets using GCD, InfoSieve, and our FlipClass.", "description": "This figure compares the clustering performance of three different methods: GCD, InfoSieve, and FlipClass, on the Cifar-10 and Cifar-100 datasets.  Each method's output is visualized using t-SNE, a dimensionality reduction technique. The plots show how well each method separates the data points into their respective clusters based on class labels.  By visually comparing the cluster distributions, we can gain insights into the effectiveness of each method in clustering data points of similar classes together, and separating clusters of different classes.", "section": "More Experimental Results"}, {"figure_path": "C4NbtYnyQg/figures/figures_26_2.jpg", "caption": "Figure 1: Left: Learning effects of traditional Teacher-Student Consistency Model (TSCM, e.g., SimGCD [75]) and our Flipped Classroom Consistency Model (FlipClass) on Stanford Cars [33]. Middle: Model comparison between TSCM and our FlipClass, where Dnew refers to data belonging to new classes. Right: Illustration of the inner feedback mechanism in FlipClass, where teacher attention is adapted to the student, leading to the alignment of attention.", "description": "This figure compares the learning effects of traditional Teacher-Student Consistency Models (TSCM) and the proposed FlipClass model on the Stanford Cars dataset. The left panel shows the learning curves, highlighting the learning gap and unsynchronized learning of TSCM compared to the synchronized learning of FlipClass. The middle panel visually compares the two models, demonstrating FlipClass's improved consistency in handling new classes. The right panel illustrates the inner feedback mechanism of FlipClass, emphasizing how teacher attention dynamically adapts to student attention, leading to better alignment and learning.", "section": "1 Introduction"}, {"figure_path": "C4NbtYnyQg/figures/figures_27_1.jpg", "caption": "Figure 1: Left: Learning effects of traditional Teacher-Student Consistency Model (TSCM, e.g., SimGCD [75]) and our Flipped Classroom Consistency Model (FlipClass) on Stanford Cars [33]. Middle: Model comparison between TSCM and our FlipClass, where Dnew refers to data belonging to new classes. Right: Illustration of the inner feedback mechanism in FlipClass, where teacher attention is adapted to the student, leading to the alignment of attention.", "description": "This figure compares the performance of traditional teacher-student models and the proposed FlipClass model in Generalized Category Discovery (GCD). The left panel shows the learning curves, demonstrating that FlipClass achieves better learning synchronization between the teacher and student.  The middle panel illustrates how FlipClass addresses the challenges of inconsistent pattern learning. Finally, the right panel details the inner feedback mechanism in FlipClass, showing how teacher attention adapts based on student feedback.", "section": "1 Introduction"}, {"figure_path": "C4NbtYnyQg/figures/figures_28_1.jpg", "caption": "Figure 18: Results with varying the number of old classes |Ce|.", "description": "This figure shows the performance of FlipClass on Cifar-100 and CUB datasets when varying the number of old classes used for training. The results show that the performance is relatively stable across different proportions of old classes, demonstrating the robustness of FlipClass in handling various numbers of known classes.", "section": "C More Experimental Results"}]