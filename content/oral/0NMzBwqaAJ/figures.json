[{"figure_path": "0NMzBwqaAJ/figures/figures_0_1.jpg", "caption": "Figure 1: We continual pretrain 1B and 7B LMs with 15B OpenWebMath tokens. RHO-1 is trained with our proposed Selective Language Modeling (SLM), while baselines are trained using causal language modeling. SLM improves average few-shot accuracy on GSM8k and MATH by over 16%, achieving the baseline performance 5-10x faster.", "description": "This figure compares the performance of the proposed RHO-1 language model with baseline models on two different sizes of language models (1B and 7B parameters). The models were continually pre-trained on 15 billion OpenWebMath tokens.  The left panel shows the results for 1B parameter models, and the right panel shows results for 7B parameter models.  The x-axis represents the number of tokens (in billions) used for pre-training. The y-axis represents the average few-shot accuracy across several mathematical reasoning benchmarks (GSM8k and MATH). The results show that RHO-1, which uses Selective Language Modeling (SLM), significantly outperforms the baseline models in terms of both accuracy and training speed. For instance, RHO-1 achieves a similar level of accuracy to the DeepSeekMath baseline but with 5 to 10 times fewer tokens.", "section": "1 Introduction"}, {"figure_path": "0NMzBwqaAJ/figures/figures_1_1.jpg", "caption": "Figure 2: Upper: Even an extensively filtered pretraining corpus contains token-level noise. Left: Previous Causal Language Modeling (CLM) trains on all tokens. Right: Our proposed Selective Language Modeling (SLM) selectively applies loss on those useful and clean tokens.", "description": "This figure illustrates the difference between traditional causal language modeling (CLM) and the proposed selective language modeling (SLM). CLM trains on all tokens in a corpus, while SLM selectively trains on useful tokens identified by a reference model. The figure uses a sample sentence to show how SLM filters out noisy tokens from the pretraining corpus, resulting in a more focused training process.", "section": "1 Introduction"}, {"figure_path": "0NMzBwqaAJ/figures/figures_2_1.jpg", "caption": "Figure 3: The loss of four categories of tokens during pretraining. (a) shows the loss of H\u2192H, L\u2192H, H\u2192L, and L\u2192L tokens during pretraining. (b) and (c) show three cases of fluctuating tokens' loss in L\u2192L and H\u2192H during pretraining, respectively.", "description": "This figure visualizes the training dynamics of different token categories in a language model. Tokens are categorized into four groups based on their loss trajectory during pretraining: high-to-high (H\u2192H), low-to-high (L\u2192H), high-to-low (H\u2192L), and low-to-low (L\u2192L).  The plots show the average loss for each category across training steps.  Subplots (b) and (c) provide examples of the loss fluctuation patterns for specific tokens within the L\u2192L and H\u2192H categories, respectively, highlighting the variability in individual token learning curves.", "section": "2 Selective Language Modeling"}, {"figure_path": "0NMzBwqaAJ/figures/figures_3_1.jpg", "caption": "Figure 4: The pipeline of Selective Language Modeling (SLM). SLM optimizes language model performance by concentrating on valuable, clean tokens during pre-training. It involves three steps: (Step 1) Initially, train a reference model on high-quality data. (Step 2) Then, score each token's loss in a corpus using the reference model. (Step 3) Finally, selectively train the language model on tokens that have higher scores.", "description": "This figure illustrates the three-step process of Selective Language Modeling (SLM). First, a reference model is trained on high-quality data. This model is then used to score each token in a larger pre-training corpus based on its loss. Finally, a language model is trained using only the tokens with high scores (determined by the reference model). This method focuses training on high-value, clean tokens, improving data efficiency and model performance.", "section": "2 Selective Language Modeling"}, {"figure_path": "0NMzBwqaAJ/figures/figures_6_1.jpg", "caption": "Figure 1: We continual pretrain 1B and 7B LMs with 15B OpenWebMath tokens. RHO-1 is trained with our proposed Selective Language Modeling (SLM), while baselines are trained using causal language modeling. SLM improves average few-shot accuracy on GSM8k and MATH by over 16%, achieving the baseline performance 5\u201310\u00d7 faster.", "description": "This figure shows the results of continual pretraining language models (LMs) of size 1B and 7B parameters on the OpenWebMath dataset.  Two training methods are compared:  a baseline using causal language modeling (CLM) and the proposed Selective Language Modeling (SLM) used to train the RHO-1 model. The graphs plot average few-shot accuracy against the number of training tokens (in billions).  The results demonstrate that SLM significantly improves few-shot accuracy compared to the CLM baseline, achieving similar performance with 5-10 times fewer tokens.", "section": "1 Introduction"}, {"figure_path": "0NMzBwqaAJ/figures/figures_7_1.jpg", "caption": "Figure 6: The dynamics of pretraining loss and downstream loss. (a) and (c) represent the loss of tokens selected/unselected by SLM during pretraining in both SLM and CLM methods, while (b) represents the loss of the SLM and CLM methods on MetaMath [Yu et al., 2024]. We tested the above results through the process of pretraining with a total of 4 billion tokens.", "description": "This figure shows a comparison of the training loss and downstream task loss between the SLM (Selective Language Modeling) and the CLM (Causal Language Modeling) methods. The left panel (a) displays the loss for the tokens selected by SLM during pretraining. The middle panel (b) illustrates the downstream task loss for both methods on the MetaMath dataset. The right panel (c) shows the loss for the tokens not selected by SLM. The results demonstrate that SLM leads to lower training loss and better downstream task performance.  A total of 4 billion tokens were used during the pretraining process.", "section": "3.5 Ablation Study and Analysis"}, {"figure_path": "0NMzBwqaAJ/figures/figures_8_1.jpg", "caption": "Figure 1: We continual pretrain 1B and 7B LMs with 15B OpenWebMath tokens. RHO-1 is trained with our proposed Selective Language Modeling (SLM), while baselines are trained using causal language modeling. SLM improves average few-shot accuracy on GSM8k and MATH by over 16%, achieving the baseline performance 5-10x faster.", "description": "This figure shows the results of continual pre-training language models (LMs) of size 1B and 7B parameters on a 15B token OpenWebMath dataset.  Two pre-training methods are compared:  causal language modeling (baseline) and Selective Language Modeling (SLM), which is the core contribution of the paper. The results demonstrate that the SLM method significantly improves few-shot accuracy on two downstream tasks (GSM8k and MATH) with a speedup of 5 to 10 times compared to the baseline.", "section": "1 Introduction"}, {"figure_path": "0NMzBwqaAJ/figures/figures_8_2.jpg", "caption": "Figure 1: We continual pretrain 1B and 7B LMs with 15B OpenWebMath tokens. RHO-1 is trained with our proposed Selective Language Modeling (SLM), while baselines are trained using causal language modeling. SLM improves average few-shot accuracy on GSM8k and MATH by over 16%, achieving the baseline performance 5-10x faster.", "description": "This figure compares the few-shot accuracy of 1B and 7B language models (LMs) trained with and without Selective Language Modeling (SLM) on the OpenWebMath dataset.  The baseline models use causal language modeling. The results show that SLM significantly improves few-shot accuracy (by over 16%) and achieves comparable performance to baseline models at a 5-10x faster training speed. The x-axis represents the number of tokens (in billions) used for pretraining, and the y-axis represents the average few-shot accuracy.", "section": "Introduction"}, {"figure_path": "0NMzBwqaAJ/figures/figures_8_3.jpg", "caption": "Figure 9: Effect of token select ratio. We train 1B LM with SLM objective on 5B tokens.", "description": "This figure shows the impact of different token selection ratios on the performance of a 1B parameter language model trained using the Selective Language Modeling (SLM) objective.  The x-axis represents the percentage of tokens selected for training, while the y-axis shows the accuracy achieved on the GSM8K and MATH datasets. The results suggest an optimal token selection ratio exists, beyond which performance starts to decrease. ", "section": "3.2 Math Pre-training Results"}, {"figure_path": "0NMzBwqaAJ/figures/figures_19_1.jpg", "caption": "Figure 3: The loss of four categories of tokens during pretraining. (a) shows the loss of H\u2192H, L\u2192H, H\u2192L, and L\u2192L tokens during pretraining. (b) and (c) show three cases of fluctuating tokens' loss in L\u2192L and H\u2192H during pretraining, respectively.", "description": "This figure visualizes the loss curves of four token categories during the language model pretraining.  Panel (a) shows the average loss for each category (H\u2192H, L\u2192H, H\u2192L, L\u2192L) across the entire training process. Panels (b) and (c) provide example loss curves for individual tokens in the L\u2192L and H\u2192H categories, respectively, illustrating the inconsistent patterns observed for some tokens.", "section": "2 Selective Language Modeling"}]