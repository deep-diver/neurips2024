[{"type": "text", "text": "Not All Tokens Are What You Need for Pretraining ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zhenghao Lin\u22c6\u03c7\u03d5 Zhibin Gou\u22c6\u03c0\u03d5 Yeyun Gong\u22c4\u03d5 Xiao Liu\u03d5 Yelong Shen\u03d5 Ruochen $\\mathbf{\\tilde{X}}\\mathbf{u}^{\\phi}$ Chen Lin\u22c4\u03c7 Yujiu Yang\u22c4\u03c0 Jian Jiao\u03d5 Nan Duan\u03d5 Weizhu Chen\u03d5 \u03c7Xiamen University \u03c0Tsinghua University \u03d5Microsoft https://aka.ms/rho ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Previous language model pre-training methods have uniformly applied a next-token prediction loss to all training tokens. Challenging this norm, we posit that \u201cNot all tokens in a corpus are equally important for language model training\u201d. Our initial analysis examines token-level training dynamics of language model, revealing distinct loss patterns for different tokens. Leveraging these insights, we introduce a new language model called RHO-1. Unlike traditional LMs that learn to predict every next token in a corpus, RHO-1 employs Selective Language Modeling (SLM), which selectively trains on useful tokens that aligned with the desired distribution. This approach involves scoring tokens using a reference model, and then training the language model with a focused loss on tokens with higher scores. When continual pretraining on 15B OpenWebMath corpus, RHO-1 yields an absolute improvement in few-shot accuracy of up to $30\\%$ in 9 math tasks. After fine-tuning, RHO-1-1B and 7B achieved state-of-the-art results of $40.6\\%$ and $51.8\\%$ on MATH dataset, respectively \u2014 matching DeepSeekMath with only $3\\%$ of the pretraining tokens. Furthermore, when continual pretraining on 80B general tokens, RHO-1 achieves $6.8\\%$ average enhancement across 15 diverse tasks, increasing both data efficiency and performance of the language model pre-training. ", "page_idx": 0}, {"type": "image", "img_path": "0NMzBwqaAJ/tmp/fe05e6dbbb2627e9d462c138df312f510db4ea97900e0a3ce08c5add588031db.jpg", "img_caption": ["Figure 1: We continual pretrain 1B and 7B LMs with 15B OpenWebMath tokens. RHO-1 is trained with our proposed Selective Language Modeling (SLM), while baselines are trained using causal language modeling. SLM improves average few-shot accuracy on GSM8k and MATH by over $16\\%$ , achieving the baseline performance 5-10x faster. "], "img_footnote": [], "page_idx": 0}, {"type": "image", "img_path": "0NMzBwqaAJ/tmp/06322d4a0ef7d0e67c6496ae837f82c1dd6e79b45b6e7f584af32460becedaaf.jpg", "img_caption": ["Figure 2: Upper: Even an extensively filtered pretraining corpus contains token-level noise. Left: Previous Causal Language Modeling (CLM) trains on all tokens. Right: Our proposed Selective Language Modeling (SLM) selectively applies loss on those useful and clean tokens. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Scaling up model parameters and dataset size has consistently elevated the next-token prediction accuracy in large language models, yielding significant advancements in artificial intelligence [Kaplan et al., 2020, Brown et al., 2020, OpenAI, 2023, Team et al., 2023]. However, training on all available data is not always optimal or feasible. As a result, the practice of data filtering has become crucial, using various heuristics and classifiers [Brown et al., 2020, Wenzek et al., 2019] to select training documents. These techniques significantly improve data quality and boost model performance. ", "page_idx": 1}, {"type": "text", "text": "However, despite thorough document-level filtering, high-quality datasets still contain many noisy tokens that can negatively affect training, as illustrated in Figure 2 (Upper). Removing such tokens might alter the text\u2019s meaning, while overly strict filtering could exclude useful data [Welbl et al., 2021, Muennighoff et al., 2024] and lead to biases [Dodge et al., 2021, Longpre et al., 2023]. Furthermore, research indicates that the distribution of web data does not inherently align with the ideal distribution for downstream applications [Tay et al., 2022, Wettig et al., 2023]. For example, common corpus at the token level may include undesirable content like hallucinations or highly ambiguous tokens that are hard to predict. Applying the same loss to all tokens can lead to inefficient computation on non-essential tokens, potentially restricting LLMs from achieving more advanced levels of intelligence. ", "page_idx": 1}, {"type": "text", "text": "To explore how language models learn at the token level, we initially examined training dynamics, particularly how the token-level loss evolves during usual pretraining. In $\\S2.1$ , we evaluated the model\u2019s token perplexity at different checkpoints and categorized tokens into different types. Our findings reveal that significant loss reduction is limited to a select group of tokens. Many tokens are \u201ceasy tokens\u201d that are already learned, and some are \u201chard tokens\u201d that exhibit variable losses and resist convergence. These tokens can lead to numerous ineffective gradient updates. ", "page_idx": 1}, {"type": "text", "text": "Based on these analyses, we introduce RHO-1 models trained with a novel Selective Language Modeling (SLM) objective. As shown in Figure 2 (Right), this approach inputs the full sequence into the model and selectively removes the loss of undesired tokens. The detailed pipeline is depicted in Figure 4: First, SLM trains a reference language model on high-quality corpora. This model establishes utility metrics to score tokens according to the desired distribution, naturally filtering out unclean and irrelevant tokens. Second, SLM uses the reference model to score each token in a corpus using its loss (\u00a72.2). Finally, we train a language model only on those tokens that exhibit a high excess loss between the reference and the training model, selectively learning the tokens that best benefit downstream applications (\u00a72.2). ", "page_idx": 1}, {"type": "text", "text": "We show through comprehensive experiments that SLM significantly enhances token efficiency during training and improves performance on downstream tasks. Furthermore, our findings indicate that SLM effectively identifies tokens relevant to the target distribution, resulting in improved perplexity scores on benchmarks for models trained with the selected tokens. $\\S3.2$ shows the effectiveness of SLM on math continual pretraining: both 1B and 7B RHO-1 outperform CLM-trained baselines by over $16\\%$ on the GSM8k and MATH datasets. SLM reaches baseline accuracy up to $10\\mathrm{x}$ faster, as shown in Figure 1. Remarkably, RHO-1-7B matches the state-of-the-art performance of DeepSeekMath-7B using only 15B tokens, compared to the 500B tokens required by DeepSeekMath. Upon fine-tuning, RHO-1-1B and 7B achieve $40.6\\%$ and $51.8\\%$ on MATH, respectively. Notably, RHO-1-1B is the first 1B LM to exceed $40\\%$ accuracy, nearing the early GPT-4\u2019s CoT performance of $42.5\\%$ . $\\S3.3$ confirms the efficacy of SLM in general continual pretraining: Training Tinyllama-1B on 80B tokens with SLM improves $6.8\\%$ on average across 15 benchmarks, with gains over $10\\%$ in code and math tasks. In $\\S3.4$ , we demonstrate that in settings without high-quality reference data, we can use SLM for self-referencing, leading to an average improvement of up to $3.3\\%$ in downstream tasks. ", "page_idx": 1}, {"type": "image", "img_path": "0NMzBwqaAJ/tmp/d7c0af0040e4cecb637630c939360ed5724e558f9e2dea7522a714bfa0769370.jpg", "img_caption": ["Figure 3: The loss of four categories of tokens during pretraining. (a) shows the loss of $\\mathrm{H\\mathrm{\\rightarrow}H}$ , $\\mathrm{L}{\\rightarrow}\\mathrm{H}$ , $\\mathrm{H}{\\rightarrow}\\mathrm{L}$ , and $\\mathrm{L}{\\rightarrow}\\mathrm{L}$ tokens during pretraining. (b) and (c) show three cases of fluctuating tokens\u2019 loss in ${\\mathrm{L}}{\\rightarrow}{\\mathrm{L}}$ and $\\mathrm{H\\mathrm{\\rightarrow}H}$ during pretraining, respectively. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2 Selective Language Modeling ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Not All Tokens Are Equal: Training Dynamics of Token Loss ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our investigation begins with a critical look at how individual tokens\u2019 losses evolve during standard pre-training. We continue pre-training Tinyllama-1B with 15B tokens from OpenWebMath, saving checkpoints after every 1B tokens. We then evaluate token-level loss at these intervals using the validation set of approximately 320,000 tokens. Figure 3(a) reveals a striking pattern: tokens fall into four categories based on their loss trajectory\u2014persistent high loss $\\left(\\mathrm{H}{\\rightarrow}\\mathrm{H}\\right)$ , increasing loss $(\\mathrm{L}{\\rightarrow}\\mathrm{H})$ ), decreasing loss $\\scriptstyle(\\mathrm{H}\\to\\mathrm{L})$ ), and consistent low loss $(\\mathrm{L}{\\rightarrow}\\mathrm{L})$ . For further details on these categories, see $\\S D.1$ . Our analysis uncovers that a mere $26\\%$ of tokens show a notable loss reduction $\\scriptstyle(\\mathrm{H}\\to\\mathrm{L})$ , while the majority $(51\\%)$ remain in the ${\\mathrm{L}}{\\rightarrow}{\\mathrm{L}}$ category, indicating they have already been learned. Interestingly, $11\\%$ of the tokens are persistently challenging $\\scriptstyle(\\mathrm{H}\\rightarrow\\mathrm{H})$ ), likely due to high aleatoric uncertainty [H\u00fcllermeier and Waegeman, 2021]. Additionally, $12\\%$ of tokens experience an unexpected loss increase $\\scriptstyle\\mathrm{{L}\\rightarrow\\mathrm{{H}}}$ ) during training. ", "page_idx": 2}, {"type": "text", "text": "Our second observation is that a significant number of token losses exhibit persistent fluctuations, and resist convergence. The loss of many ${\\mathrm{L}}{\\rightarrow}{\\mathrm{L}}$ and $\\mathrm{H\\mathrm{\\rightarrow}H}$ tokens, as depicted in Figure 3 (b) and (c), show high variance during training. In $\\S\\mathrm{D}.2$ , we visualize and analyze the content of these tokens and find that many of them are noisy, which is consistent with our hypothesis. ", "page_idx": 2}, {"type": "text", "text": "Consequently, we learn that the loss associated with each token during training does not decrease smoothly like the overall loss; instead, there is a complex training dynamic among different tokens. If we can select the appropriate tokens for the model to focus on during training, we may be able to stabilize the trajectory of the model\u2019s training and enhance its data efficiency. ", "page_idx": 2}, {"type": "text", "text": "2.2 Selective Language Modeling ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Overview Inspired by the practice of reference model in document-level filtering, we propose a simple pipeline of token-level data selection, termed \u201cSelective Language Modeling (SLM)\u201d. Our method comprises three steps, as depicted in Figure 4. We begin by training a reference model on a curated, high-quality dataset. This model then assesses the loss of each token within the pretraining corpus. In the final phase, we train the language model selectively, focusing on tokens with high excess loss between the training and reference model. The intuition is that tokens with high excess loss are more learnable and better aligned with the desired distribution, naturally excluding tokens that are either irrelevant or of low quality. Below, we provide a detailed description of each step. ", "page_idx": 2}, {"type": "image", "img_path": "0NMzBwqaAJ/tmp/d9b9d020467af5b96353de779e331d0856bbbac531bf6213f51d94d21e39a0eb.jpg", "img_caption": ["Figure 4: The pipeline of Selective Language Modeling (SLM). SLM optimizes language model performance by concentrating on valuable, clean tokens during pre-training. It involves three steps: (Step 1) Initially, train a reference model on high-quality data. (Step 2) Then, score each token\u2019s loss in a corpus using the reference model. (Step 3) Finally, selectively train the language model on tokens that have higher scores. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Reference Modeling We begin by curating a high-quality dataset that reflects the desired data distribution. We train a reference model (RM) using standard cross-entropy loss on the curated data. The resulting RM is then used to assess the token loss within a larger pretraining corpus. We compute the reference loss $(\\mathcal{L}_{\\mathrm{RM}})$ of a token $x_{i}$ based on the probability that the RM assigns to this token. The calculation is formalized as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{RM}}(x_{i})=-\\log P(x_{i}|\\boldsymbol{x}_{<i})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "By evaluating $\\mathcal{L}_{\\mathrm{RM}}$ for each token, we establish the reference loss for selective pretraining, allowing us to focus on the most influential tokens in language modeling. ", "page_idx": 3}, {"type": "text", "text": "Selective Pretraining Note that Causal Language Modeling (CLM) employs the cross-entropy loss: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{CLM}}(\\theta)=-\\frac{1}{N}\\sum_{i=1}^{N}\\log P(x_{i}|\\boldsymbol{x}_{<i};\\theta)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, $\\mathcal{L}_{\\mathrm{CLM}}(\\theta)$ represents the loss function parameterized by model $\\theta$ . $N$ is the length of the sequence, $x_{i}$ is the $i$ -th token in the sequence, and $x_{<i}$ represents all tokens before the $i$ -th token. In contrast, Selective Language Modeling (SLM) trains the language model with a focus on tokens that exhibit a high excess loss when compared to the reference model. The excess loss $(\\mathcal{L}_{\\Delta})$ for a token $x_{i}$ is defined as the difference between the current training model loss $(\\mathcal{L}_{\\theta})$ and the reference loss: ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\mathcal{L}}_{\\Delta}(x_{i})={\\mathcal{L}}_{\\theta}(x_{i})-{\\mathcal{L}}_{\\mathrm{RM}}(x_{i})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We introduce a token selection ratio $k\\%$ , which determines the proportion of tokens to be included based on their excess loss. The cross-entropy loss for the selected tokens is computed as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{SLM}}(\\theta)=-\\frac{1}{N*k\\mathcal{V}_{0}}\\sum_{i=1}^{N}I_{k\\mathcal{V}_{0}}(x_{i})\\cdot\\log P(x_{i}|x_{<i};\\theta)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, $N*k\\%$ defines the number of tokens that fall within the top $k\\%$ of excess loss. The indicator function $I_{k\\%}(x_{i})$ is defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nI_{k\\mathcal{V}_{0}}(x_{i})=\\left\\{1\\begin{array}{l l}{\\mathrm{~if~}x_{i}\\mathrm{~ranks~in~the~top~}k\\mathcal{V}_{0}\\mathrm{~by~}S(x_{i})}\\\\ {\\mathrm{~otherwise}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "table", "img_path": "0NMzBwqaAJ/tmp/7fe3dc1fa028a63c2ceb3405b6941b699c5608d05d4aa09b93cd5a6faafa381e.jpg", "table_caption": ["Table 1: Few-shot CoT reasoning results of math pretraining. All models are tested with few-shot prompting. Previous best results are highlighted in blue, while our best results are in purple. \u2217Only unique math-related tokens are calculated. For RHO-1, we calculate only the selected tokens that are used for training. \u2020We use OpenAI\u2019s MATH subset [Lightman et al., 2023] for evaluation, since some original test samples have been used in public training sets such as PRM800k. \u2021The SAT only has 32 four-choice problems, so we average our results over the last three checkpoints, if available. "], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "By default, we use ${\\mathcal{L}}_{\\Delta}$ as the score function $S$ . This ensures that the loss is applied only to the tokens that are deemed most beneficial for the language model to learn from. In practice, token selection can be implemented by ranking the tokens in a batch according to their excess loss and using only the top $k\\%$ of tokens for training. This process eliminates the loss for undesired tokens without incurring additional costs during pretraining, making our approach both efficient and easily integrated. ", "page_idx": 4}, {"type": "text", "text": "3 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We continually pretrained models in both mathematical and general domain and designed ablation and analysis experiments to understand the effectiveness of SLM. ", "page_idx": 4}, {"type": "text", "text": "3.1 Experimental Setup ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Reference Model Training To train our mathematical reference model, we gathered a dataset of 0.5B high-quality, math-related tokens. This dataset is a blend of synthetic data from GPT [Yu et al., 2024, Huang et al., 2024] and manually curated data [Yue et al., 2024, Ni et al., 2024]. For the general reference model, we compiled a corpus of 1.9B tokens from open-source datasets, such as Tulu-v2 [Ivison et al., 2023] and OpenHermes-2.5 [Teknium, 2023]. We trained the reference models for 3 epochs. The maximum learning rate was set at 5e-5 for 1B models and 1e-5 for 7B models, applying a cosine decay schedule. We set the maximum sequence lengths to 2048 for 1B models and 4096 for 7B models, packing multiple samples into these lengths for model input. In all main experiments, we initialized the continual pretraining model and the reference model with the same base model. ", "page_idx": 4}, {"type": "table", "img_path": "0NMzBwqaAJ/tmp/8422102d03aa9040d166e8d573e5a99e96ef3189b7b9b7000ed7e5bd193fe5f5.jpg", "table_caption": ["Table 2: Tool-integrated reasoning results of math pretraining. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Pretraining Corpus For mathematical reasoning, we utilize the OpenWebMath (OWM) dataset [Paster et al., 2023], which comprises approximately 14B tokens sourced from math-related web pages in the Common Crawl. In the general domain, we combine the SlimPajama [Daria et al., 2023] and StarCoderData [Li et al., 2023a] (both part of the Tinyllama corpus) with OpenWebMath, training on a total of 80 billion tokens with a mix ratio of 6:3:1. ", "page_idx": 5}, {"type": "text", "text": "Pretraining Setting For math pretraining, we continue pretraining on the Tinyllama-1.1B model [Zhang et al., 2024] and the Mistral-7B model [Jiang et al., 2023] with learning rates of 8e-5 and 2e-5, respectively. For the 1.1B model, we conducted our training on $32\\times\\mathrm{H100\\,80G}$ GPUs. This configuration allowed us to train approximately 15 billion tokens in around 3.5 hours and 50 billion tokens in about 12 hours. In the case of the 7B model, training the same 15 billion tokens took approximately 18 hours under similar hardware conditions. For general domain, we set the learning rate for Tinyllama-1.1B model to 1e-4 and train 80B tokens under the same hardware conditions, which takes approximately 19 hours. The batch size is uniformly set to 1M tokens for both domains. Regarding the token selection ratio, we use $60\\%$ for the Tinyllama-1.1B model and $70\\%$ for the Mistral-7B model. ", "page_idx": 5}, {"type": "text", "text": "Baseline Setting We use models that have been continually pretrained (Tinyllama-CT and MistralCT) through regular causal language modeling as baselines. Moreover, we compare RHO-1 with well-known and top-performing baselines, including Gemma [Team et al., 2024], Qwen1.5 [Bai et al., 2023], Phi-1.5 [Li et al., 2023b], DeepSeekLLM [DeepSeek-AI, 2024], DeepSeekMath [Shao et al., 2024], CodeLlama [Roziere et al., 2023], Mistral [Jiang et al., 2023], Minerva [Lewkowycz et al., 2022], Tinyllama [Zhang et al., 2024], LLemma [Azerbayev et al., 2023], and InternLM2-Math [Ying et al., 2024]. For fine-tuning results, we also compare with previous best models MAmmoTH[Yue et al., 2024] and ToRA[Gou et al., 2024]. ", "page_idx": 5}, {"type": "text", "text": "Evaluation Setup To comprehensively evaluate pretrained models, we compare their few-shot capabilities and fine-tuning performance across a variety of tasks. We adopt the lm-eval-harness3 [Gao et al., 2023] for general tasks, and develop math evaluation suite4 for math tasks. We use vllm (v0.3.2) [Kwon et al., 2023] to speed up inference. Further details on our evaluation can be found in Appendix E. ", "page_idx": 5}, {"type": "text", "text": "3.2 Math Pre-training Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Few-shot CoT Reasoning Results We evalute base models prompting with few-shot chain-ofthought (CoT) [Wei et al., 2022a] examples following previous works [Lewkowycz et al., 2022, Azerbayev et al., 2023, Shao et al., 2024]. As results shown in Table 1, in comparison to continue pretraining directly, RHO-1-Math has achieved the average few-shot accuracy improvement of ", "page_idx": 5}, {"type": "text", "text": "$16.5\\%$ on 1B models and $10.4\\%$ on 7B models. Furthermore, after training for multiple epochs on OpenWebMath, we find that RHO-1 could further increase the average few-shot accuracy to $40.9\\%$ . Compared to DeepSeekMath-7B, which pretrained on 500 billion math-related tokens, RHO-1-7B pretrained on only 15 billion tokens (selecting 10.5 billion tokens) achieved comparable results, demonstrating the efficiency of our approach. ", "page_idx": 6}, {"type": "text", "text": "Tool-Integrated Reasoning Results We fine-tune RHO-1 and baseline models on 69k ToRA corpus [Gou et al., 2024], consisting of 16k GPT-4-generated trajectories in a tool-integrated reasoning format, and $53\\mathrm{k}$ answer-augmented samples using LLaMA. As presented in Table 2, RHO-1-1B and RHO-1-7B achieved a state-of-the-art $40.6\\%$ and $51.8\\%$ on MATH dataset, respectively. On some unseen tasks (e.g., TabMWP and GSM-Hard), RHO-1 also demonstrates a certain degree of generalizability, with an average few-shot accuracy improvement of $6.2\\%$ on the RHO-1-Math-1B and $2.7\\%$ on RHO-1-Math-7B. ", "page_idx": 6}, {"type": "image", "img_path": "0NMzBwqaAJ/tmp/95e32c91ba26f5cd38b87cf638e098d1bd1ae4c7635d9fb3c283ea9e4cb2a05d.jpg", "img_caption": ["Figure 5: General pretraining results. We continual pretraining Tinyllama-1B on 80G general tokens. Tinyllama-CT is etrained with CLM, while RHO-1 is trained with our proposed SLM. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "3.3 General Pre-training Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We confirm the efficacy of the SLM in general pretraining by continual training Tinyllama-1.1B on 80 billion tokens. The results depicted in Figure 5 indicate that although Tinyllama has already undergone extensive training on the majority of these tokens, the application of SLM yields an average enhancement of $6.8\\%$ across 15 benchmarks compared to direct continual pretraining. The improvements were especially pronounced in code and math tasks, exceeding $10\\%$ . ", "page_idx": 6}, {"type": "text", "text": "3.4 Self-Reference Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we demonstrate that SLM can enhance the effectiveness of model pre-training using only pre-training corpora, without the need for additional high-quality data. Specifically, we initially trained the reference model on the OpenWebMath (OWM) corpus, a subset of Proof-Pile-2 (PPile). We evaluated OWM and PPile using the trained reference model and selected tokens for training. In this scenario, we assume the absence of downstream task-related data, a common situation in real-world applications. We hypothesize that the key factor is not scoring the desired distribution but filtering out noisy tokens. Therefore, we employed two different scoring functions based on the reference model loss, ${\\mathcal{L}}_{\\mathrm{RM}}$ , and the information entropy of the next token, $\\mathcal{H}_{\\mathrm{RM}}$ , which measures the uncertainty of the next token. Details are provided in Appendix H. ", "page_idx": 6}, {"type": "table", "img_path": "0NMzBwqaAJ/tmp/04842540958ddad03460c0573cfed07a4463b85a5a6d5e19eb4328a9a58db055.jpg", "table_caption": ["Table 3: Self-Reference results. We use OpenWebMath (OWM) to train the reference model. "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "0NMzBwqaAJ/tmp/bfa82dd2917bcf1d20d8c029f2e3130d9fd56c6667c2ebbeaf6ba28f55d05481.jpg", "img_caption": ["Figure 6: The dynamics of pretraining loss and downstream loss. (a) and (c) represent the loss of tokens selected/unselected by SLM during pretraining in both SLM and CLM methods, while (b) represents the loss of the SLM and CLM methods on MetaMath [Yu et al., 2024]. We tested the above results through the process of pretraining with a total of 4 billion tokens. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "The experimental results, as shown in Table 3, indicate that using only the OWM-trained reference model can effectively guide the model in pre-training on the same corpus, improving average downstream performance by $+2.4\\%$ . Using only the information entropy as the score function brought about a similar improvement. Additionally, we considered training on the intersection of tokens selected by the two scoring functions and found better performance, with a $40\\%$ reduction in tokens and $+3.3\\%$ performance. Furthermore, training the SLM on the PPile, despite only using the OWM subset to train the reference model, still achieved a $1.8\\%$ improvement with $30\\%$ fewer tokens used. For more details, please refer to Appendix $\\mathrm{H}$ . ", "page_idx": 7}, {"type": "text", "text": "3.5 Ablation Study and Analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Selected Token Loss Aligns Better with Downstream Performance We utilized the reference model to filter tokens and assess their impact on validation and downstream losses after training. As depicted in Figure 6, we pretrained on 4B tokens and tracked loss variations across methods and validation sets. The RHO-1 showed greater loss reduction on selected tokens than regular pretraining. Cross-referencing figures (a), (b), and (c) reveals that selected-token pretraining substantially lowers downstream loss, while traditional pretraining\u2019s effect on downstream loss is less pronounced despite initial loss reductions. Therefore, we expect that selecting tokens for pretraining is more efficient. ", "page_idx": 7}, {"type": "text", "text": "In Figure 7, we demonstrate that the loss of selected tokens correlates with downstream task performance, following a power law similar to recent findings [Gadre et al., 2024]. Our analysis shows that tokens selected by SLM positively impact performance, while those not selected have a negative impact. Thus, reducing loss across all tokens is not imperative for improved model performance. Refer to Appendix F for further details. ", "page_idx": 7}, {"type": "text", "text": "What Tokens are Selected with SLM? We aim to analyze the tokens selected by the SLM method in pretraining to further explore its working mechanism. To this end, we visualize the token selection process during the training of RHO-1 using the OpenWebMath. In $\\S\\mathrm{G}.1$ , we have highlighted in blue the tokens that were retained during actual pretraining. We observe that the majority of tokens chosen by the SLM method are closely related to mathematics, effectively training the model on the parts of the original corpus that are pertinent to mathematical content. ", "page_idx": 7}, {"type": "image", "img_path": "0NMzBwqaAJ/tmp/822903aba5704767e7918fe37bb3e3482b8b952b3a54095064f6804e783106e4.jpg", "img_caption": ["Figure 7: The relationship between the selected tokens / unselected tokens loss in SLM and downstream task performance. The y-axis represents the average few-shot accuracy on GSM8k and MATH. The x-axis represents the average loss on selected tokens / unselected tokens at corresponding checkpoint (2B, 5B, 8B, 11B, and 14B). "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "0NMzBwqaAJ/tmp/ec7fb76e474a7523bbe44f6101125352f0b079971d014aaf1e471cee0785f82d.jpg", "img_caption": ["Figure 8: The PPL of tokens selected by different checkpoint. We test the PPL of the tokens selected at 2B, 5B, 8B, 11B, and 14B. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "0NMzBwqaAJ/tmp/ad68bd393913192a32bc972c4b1441e3e1947f0ceaa157dd11c3d5718295e7e0.jpg", "img_caption": ["Figure 9: Effect of token select ratio. We train 1B LM with SLM objective on 5B tokens. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Furthermore, we investigated the differences in token filtering across various checkpoints during the training process and tested the perplexity of these tokens on different checkpoints. As illustrated in Figure 8, we found that the tokens selected by later checkpoints tend to have higher perplexity towards the later stages of training and lower perplexity in the earlier stages. This may suggest that the model first optimizes tokens with a larger learnable space, thereby increasing learning efficiency. Moreover, we noticed a sample-wise \u201cdouble descent\u201d [Nakkiran et al., 2021] on the loss of selected tokens, where the select token\u2019s perplexity initially increases before decreases. This might be an effect of selecting tokens based on excess loss, targeting those most in need at each checkpoint. ", "page_idx": 8}, {"type": "text", "text": "Effect of Token Select Ratio We investigate the impact of token selecting ratios of the SLM. Generally, the selecting ratio is defined by heuristic rules, similar to the approach previously employed in the training of Masked Language Models (MLMs) [Devlin et al., 2019, Liu et al., 2019]. As shown in Figure 9, the selected tokens is suitable for accounting for about $60\\%$ of the original tokens. ", "page_idx": 8}, {"type": "text", "text": "4 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this paper, we propose using Selective Language Modeling(SLM) to train RHO-1, which select more suitable tokens for current pretraining stage. We conducted the detailed analysis of the loss of tokens during the pretraining process and found that not all tokens are equal during pretraining. Our experiments and analysis in the fields of mathematics and general have demonstrated the effectiveness of the SLM method, emphasizing the importance of token level in the LLM pretraining process. In the future, how to improve pretraining of LLMs from the perspective of token level worthy of in-depth research. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Zhenghao Lin and Chen Lin were supported by National Key R&D Program of China (No. 2022ZD0160501), the Natural Science Foundation of China (No.62372390,62432011). Zhibin Gou and Yujiu Yang were supported by the Shenzhen Science and Technology Program (JCYJ20220818101001004) and the \u201cGraph Neural Network Project\u201d of Ping An Technology (Shenzhen) Co., Ltd. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. ", "page_idx": 9}, {"type": "text", "text": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020. ", "page_idx": 9}, {"type": "text", "text": "OpenAI. Gpt-4 technical report, 2023.   \nGemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.   \nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzm\u00e1n, Armand Joulin, and Edouard Grave. Ccnet: Extracting high quality monolingual datasets from web crawl data. arXiv preprint arXiv:1911.00359, 2019.   \nJohannes Welbl, Amelia Glaese, Jonathan Uesato, Sumanth Dathathri, John Mellor, Lisa Anne Hendricks, Kirsty Anderson, Pushmeet Kohli, Ben Coppin, and Po-Sen Huang. Challenges in detoxifying language models. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2447\u20132469, 2021.   \nNiklas Muennighoff, Alexander Rush, Boaz Barak, Teven Le Scao, Nouamane Tazi, Aleksandra Piktus, Sampo Pyysalo, Thomas Wolf, and Colin A Raffel. Scaling data-constrained language models. Advances in Neural Information Processing Systems, 36, 2024.   \nJesse Dodge, Maarten Sap, Ana Marasovi\u00b4c, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. Documenting large webtext corpora: A case study on the colossal clean crawled corpus. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1286\u20131305, 2021.   \nShayne Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph, Denny Zhou, Jason Wei, Kevin Robinson, David Mimno, et al. A pretrainer\u2019s guide to training data: Measuring the effects of data age, domain coverage, quality, & toxicity. arXiv preprint arXiv:2305.13169, 2023.   \nYi Tay, Mostafa Dehghani, Samira Abnar, Hyung Won Chung, William Fedus, Jinfeng Rao, Sharan Narang, Vinh Q Tran, Dani Yogatama, and Donald Metzler. Scaling laws vs model architectures: How does inductive bias influence scaling? arXiv preprint arXiv:2207.10551, 2022.   \nAlexander Wettig, Tianyu Gao, Zexuan Zhong, and Danqi Chen. Should you mask $15\\%$ in masked language modeling? In Andreas Vlachos and Isabelle Augenstein, editors, Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 2985\u20133000, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.eacl-main.217. URL https://aclanthology.org/2023.eacl-main.217.   \nEyke H\u00fcllermeier and Willem Waegeman. Aleatoric and epistemic uncertainty in machine learning: An introduction to concepts and methods. Machine learning, 110(3):457\u2013506, 2021.   \nLonghui Yu, Weisen Jiang, Han Shi, YU Jincheng, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. In ICLR, 2024.   \nYiming Huang, Xiao Liu, Yeyun Gong, Zhibin Gou, Yelong Shen, Nan Duan, and Weizhu Chen. Key-pointdriven data synthesis with its enhancement on mathematical reasoning. arXiv preprint arXiv:2403.02333, 2024.   \nXiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning. In ICLR, 2024.   \nXinzhe Ni, Yeyun Gong, Zhibin Gou, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. Exploring the mystery of influential data for mathematical reasoning, 2024.   \nHamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A Smith, Iz Beltagy, et al. Camels in a changing climate: Enhancing lm adaptation with tulu 2. arXiv preprint arXiv:2311.10702, 2023.   \nTeknium. Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants, 2023. URL https://huggingface.co/datasets/teknium/OpenHermes-2.5.   \nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let\u2019s verify step by step. arXiv preprint arXiv:2305.20050, 2023.   \nKeiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. Openwebmath: An open dataset of high-quality mathematical web text, 2023.   \nSoboleva Daria, Al-Khateeb Faisal, Myers Robert Steeves Jacob R, Hestness Joel, and Dey Nolan. SlimPajama: A 627B token cleaned and deduplicated version of RedPajama. https://www.cerebras.net/blog/ slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama, 2023. URL https://huggingface.co/datasets/cerebras/SlimPajama-627B.   \nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, Jo\u00e3o Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy V, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Moustafa-Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mu\u00f1oz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. Starcoder: may the source be with you! CoRR, abs/2305.06161, 2023a.   \nPeiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Tinyllama: An open-source small language model, 2024.   \nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.   \nGemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi\u00e8re, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024.   \nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.   \nYuanzhi Li, S\u00e9bastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. Textbooks are all you need ii: phi-1.5 technical report, 2023b.   \nDeepSeek-AI. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954, 2024. URL https://github.com/deepseek-ai/DeepSeek-LLM.   \nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, YK Li, Y Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.   \nBaptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J\u00e9r\u00e9my Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.   \nAitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:3843\u20133857, 2022.   \nZhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model for mathematics. arXiv preprint arXiv:2310.10631, 2023.   \nHuaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan Shao, Zhaoye Fei, Yichuan Ma, Jiawei Hong, Kuikun Liu, Ziyi Wang, et al. Internlm-math: Open math large language models toward verifiable reasoning. arXiv preprint arXiv:2402.06332, 2024.   \nZhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Minlie Huang, Nan Duan, Weizhu Chen, et al. Tora: A tool-integrated reasoning agent for mathematical problem solving. In ICLR, 2024.   \nLeo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac\u2019h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 12 2023. URL https://zenodo.org/records/10256836.   \nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023.   \nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In NIPS, volume 35, pages 24824\u201324837, 2022a.   \nSamir Yitzhak Gadre, Georgios Smyrnis, Vaishaal Shankar, Suchin Gururangan, Mitchell Wortsman, Rulin Shao, Jean Mercat, Alex Fang, Jeffrey Li, Sedrick Keh, Rui Xin, Marianna Nezhurina, Igor Vasiljevic, Jenia Jitsev, Alexandros G. Dimakis, Gabriel Ilharco, Shuran Song, Thomas Kollar, Yair Carmon, Achal Dave, Reinhard Heckel, Niklas Muennighoff, and Ludwig Schmidt. Language models scale reliably with over-training and on downstream tasks. Preprint, 2024.   \nPreetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep double descent: Where bigger models and more data hurt. Journal of Statistical Mechanics: Theory and Experiment, 2021(12):124003, 2021.   \nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT (1), pages 4171\u20134186. Association for Computational Linguistics, 2019.   \nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692, 2019.   \nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1\u201367, 2020.   \nStanislas Polu and Ilya Sutskever. Generative language modeling for automated theorem proving. arXiv preprint arXiv:2009.03393, 2020.   \nSuriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C\u00e9sar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. Textbooks are all you need. arXiv preprint arXiv:2306.11644, 2023.   \nKatherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. Deduplicating training data makes language models better. arXiv preprint arXiv:2107.06499, 2021.   \nNikhil Kandpal, Eric Wallace, and Colin Raffel. Deduplicating training data mitigates privacy risks in language models. In International Conference on Machine Learning, pages 10697\u201310707. PMLR, 2022.   \nKushal Tirumala, Daniel Simig, Armen Aghajanyan, and Ari Morcos. D4: Improving llm pretraining via document de-duplication and diversification. In NIPS, volume 36, 2023.   \nSang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy S Liang. Data selection for language models via importance resampling. Advances in Neural Information Processing Systems, 36, 2024a.   \nAlon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, Colin Raffel, Shiyu Chang, Tatsunori Hashimoto, and William Yang Wang. A survey on data selection for language models, 2024.   \nSang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy S Liang, Quoc V Le, Tengyu Ma, and Adams Wei Yu. Doremi: Optimizing data mixtures speeds up language model pretraining. Advances in Neural Information Processing Systems, 36, 2024b.   \nMayee Chen, Nicholas Roberts, Kush Bhatia, Jue Wang, Ce Zhang, Frederic Sala, and Christopher R\u00e9. Skill-it! a data-driven skills framework for understanding and training language models. Advances in Neural Information Processing Systems, 36, 2024.   \nYingwei Ma, Yue Liu, Yue Yu, Yuanliang Zhang, Yu Jiang, Changjian Wang, and Shanshan Li. At which training stage does code data help LLMs reasoning? In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=KIPJKST4gw.   \nMing Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng, Jianzong Wang, Tianyi Zhou, and Jing Xiao. From quantity to quality: Boosting llm performance with self-guided data selection for instruction tuning. arXiv preprint arXiv:2308.12032, 2023c.   \nWei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He. What makes good data for alignment? a comprehensive study of automatic data selection in instruction tuning. In ICLR, 2024.   \nYunshui Li, Binyuan Hui, Xiaobo Xia, Jiaxi Yang, Min Yang, Lei Zhang, Shuzheng Si, Junhao Liu, Tongliang Liu, Fei Huang, et al. One shot learning as instruction data prospector for large language models. arXiv preprint arXiv:2312.10302, 2023d.   \nMengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. Less: Selecting influential data for targeted instruction tuning. arXiv preprint arXiv:2402.04333, 2024.   \nFeiyang Kang, Hoang Anh Just, Yifan Sun, Himanshu Jahagirdar, Yuanzhi Zhang, Rongxing Du, Anit Kumar Sahu, and Ruoxi Jia. Get more for less: Principled data selection for warming up fine-tuning in LLMs. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/ forum?id=QmYNBVukex.   \nZiheng Qin, Kai Wang, Zangwei Zheng, Jianyang Gu, Xiangyu Peng, Zhaopan Xu, Daquan Zhou, Lei Shang, Baigui Sun, Xuansong Xie, and Yang You. Infobatch: Lossless training speed up by unbiased dynamic data pruning. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id $=$ C61sk5LsK6.   \nTogether Computer. Redpajama: an open dataset for training large language models, 10 2023. URL https: //github.com/togethercomputer/RedPajama-Data.   \nIlya Loshchilov and Frank Hutter. Online batch selection for faster training of neural networks. arXiv preprint arXiv:1511.06343, 2015.   \nTom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv preprint arXiv:1511.05952, 2015.   \nAngelos Katharopoulos and Fran\u00e7ois Fleuret. Not all samples are created equal: Deep learning with importance sampling. In International conference on machine learning, pages 2525\u20132534. PMLR, 2018.   \nAngela H Jiang, Daniel L-K Wong, Giulio Zhou, David G Andersen, Jeffrey Dean, Gregory R Ganger, Gauri Joshi, Michael Kaminksy, Michael Kozuch, Zachary C Lipton, et al. Accelerating deep learning by focusing on the biggest losers. arXiv preprint arXiv:1910.00762, 2019.   \nS\u00f6ren Mindermann, Jan M Brauner, Muhammed T Razzak, Mrinank Sharma, Andreas Kirsch, Winnie Xu, Benedikt H\u00f6ltgen, Aidan N Gomez, Adrien Morisot, Sebastian Farquhar, and Yarin Gal. Prioritized training on points that are learnable, worth learning, and not yet learnt. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 15630\u201315649. PMLR, 17\u201323 Jul 2022. URL https://proceedings.mlr.press/v162/mindermann22a.html.   \nSimin Fan and Martin Jaggi. Irreducible curriculum for language model pretraining. arXiv preprint arXiv:2310.15389, 2023.   \nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.   \nYuxian Gu, Zhengyan Zhang, Xiaozhi Wang, Zhiyuan Liu, and Maosong Sun. Train no evil: Selective masking for task-guided pre-training. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6966\u20136974, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.566. URL https://aclanthology.org/2020.emnlp-main.566.   \nTanish Lad, Himanshu Maheshwari, Shreyas Kottukkal, and Radhika Mamidi. Using selective masking as a bridge between pre-training and fine-tuning. arXiv preprint arXiv:2211.13815, 2022.   \nQihuang Zhong, Liang Ding, Juhua Liu, Xuebo Liu, Min Zhang, Bo Du, and Dacheng Tao. Revisiting token dropping strategy in efficient BERT pretraining. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 10391\u201310405, Toronto, Canada, July 2023a. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.579. URL https://aclanthology.org/2023.acl-long.579.   \nLe Hou, Richard Yuanzhe Pang, Tianyi Zhou, Yuexin Wu, Xinying Song, Xiaodan Song, and Denny Zhou. Token dropping for efficient BERT pretraining. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3774\u20133784, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.262. URL https://aclanthology.org/2022.acl-long.262.   \nJessica Rumbelow and Matthew Watkins. Solidgoldmagikarp (plus, prompt generation). LessWrong, 2023. URL https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/ solidgoldmagikarp-plus-prompt-generation.   \nSander Land and Max Bartolo. Fishing for magikarp: Automatically detecting under-trained tokens in large language models. arXiv preprint arXiv:2405.05417, 2024.   \nNaomi Saphra and Adam Lopez. Understanding learning dynamics of language models with svcca. arXiv preprint arXiv:1811.00225, 2018.   \nLeshem Choshen, Guy Hacohen, Daphna Weinshall, and Omri Abend. The grammar-learning trajectories of neural language models. arXiv preprint arXiv:2109.06096, 2021.   \nLeo Z Liu, Yizhong Wang, Jungo Kasai, Hannaneh Hajishirzi, and Noah A Smith. Probing across time: What does roberta know and when? arXiv preprint arXiv:2104.07885, 2021.   \nAlethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Generalization beyond overfitting on small algorithmic datasets. arXiv preprint arXiv:2201.02177, 2022.   \nMengzhou Xia, Mikel Artetxe, Chunting Zhou, Xi Victoria Lin, Ramakanth Pasunuru, Danqi Chen, Luke Zettlemoyer, and Ves Stoyanov. Training trajectories of language models across scales. arXiv preprint arXiv:2212.09803, 2022.   \nDanny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. Scaling laws for transfer. arXiv preprint arXiv:2102.01293, 2021.   \nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.   \nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022b.   \nBerivan Isik, Natalia Ponomareva, Hussein Hazimeh, Dimitris Paparas, Sergei Vassilvitskii, and Sanmi Koyejo. Scaling laws for downstream task performance of large language models. arXiv preprint arXiv:2402.04177, 2024.   \nKushal Tirumala, Aram Markosyan, Luke Zettlemoyer, and Armen Aghajanyan. Memorization without overfitting: Analyzing the training dynamics of large language models. Advances in Neural Information Processing Systems, 35:38274\u201338290, 2022.   \nNicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. Quantifying memorization across neural language models. arXiv preprint arXiv:2202.07646, 2022.   \nTom Henighan, Shan Carter, Tristan Hume, Nelson Elhage, Robert Lasenby, Stanislav Fort, Nicholas Schiefer, and Christopher Olah. Superposition, memorization, and double descent. Transformer Circuits Thread, 2023.   \nStella Biderman, USVSN PRASHANTH, Lintang Sutawika, Hailey Schoelkopf, Quentin Anthony, Shivanshu Purohit, and Edward Raff. Emergent and predictable memorization in large language models. Advances in Neural Information Processing Systems, 36, 2024.   \nDanny Hernandez, Tom Brown, Tom Conerly, Nova DasSarma, Dawn Drain, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Tom Henighan, Tristan Hume, et al. Scaling laws and interpretability of learning from repeated data. arXiv preprint arXiv:2205.10487, 2022.   \nFuzhao Xue, Yao Fu, Wangchunshu Zhou, Zangwei Zheng, and Yang You. To repeat or not to repeat: Insights from scaling llm under token-crisis. Advances in Neural Information Processing Systems, 36, 2024.   \nCharles AE Goodhart and CAE Goodhart. Problems of monetary management: the UK experience. Springer, 1984.   \nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730\u201327744, 2022.   \nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021. URL https://arxiv.org/abs/2110.14168.   \nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In NIPS, 2021.   \nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. arXiv preprint arXiv:2211.10435, 2022.   \nArkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2080\u20132094, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.168. URL https://aclanthology.org/2021.naacl-main.168.   \nShen-yun Miao, Chao-Chun Liang, and Keh-Yih Su. A diverse corpus for evaluating and developing English math word problem solvers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 975\u2013984, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/ 2020.acl-main.92. URL https://aclanthology.org/2020.acl-main.92.   \nRik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. MAWPS: A math word problem repository. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1152\u20131157, San Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1136. URL https://aclanthology.org/N16-1136.   \nPan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview. net/forum?id=DHyHRBwJUTN.   \nAida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. Mathqa: Towards interpretable math word problem solving with operation-based formalisms. arXiv preprint arXiv:1905.13319, 2019.   \nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.   \nMirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-ofthought can solve them. arXiv preprint arXiv:2210.09261, 2022.   \nWanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation models. arXiv preprint arXiv:2304.06364, 2023b.   \nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.   \nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019.   \nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 7432\u20137439, 2020.   \nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.   \nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99\u2013106, 2021.   \nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018.   \nQinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Lei Shen, Zihan Wang, Andi Wang, Yang Li, et al. Codegeex: A pre-trained model for code generation with multilingual benchmarking on humaneval-x. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 5673\u20135684, 2023.   \nJonathan H Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and Jennimaria Palomaki. Tydi qa: A benchmark for information-seeking question answering in ty pologically di verse languages. Transactions of the Association for Computational Linguistics, 8:454\u2013470, 2020.   \nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.   \nDaya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y Wu, YK Li, et al. Deepseek-coder: When the large language model meets programming\u2013the rise of code intelligence. arXiv preprint arXiv:2401.14196, 2024.   \nXiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis. Contrastive decoding: Open-ended text generation as optimization. In ACL (1), pages 12286\u201312312. Association for Computational Linguistics, 2023e.   \nFanqi Wan, Xinting Huang, Deng Cai, Xiaojun Quan, Wei Bi, and Shuming Shi. Knowledge fusion of large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=jiDsk12qcz.   \nYao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot. Specializing smaller language models towards multi-step reasoning. In International Conference on Machine Learning, pages 10421\u201310430. PMLR, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A Author Contributions 18 ", "page_idx": 16}, {"type": "text", "text": "B Related Works 18 ", "page_idx": 16}, {"type": "text", "text": "B.1 Pretraining Data Optimization 18   \nB.2 Data Selection . 18   \nB.3 Language Model Training Dynamics 18   \nB.4 Scaling Laws 19 ", "page_idx": 16}, {"type": "text", "text": "C Limitations and Future Work 19 ", "page_idx": 16}, {"type": "text", "text": "D Analysis and Visualization of Tokens in Pretraining 19   \nD.1 More Details of Four Categories Tokens 19   \nD.2 Non-Converging Tokens in Pretrainig 20   \nE Evalution Details 20   \nE.1 Math Evalution 20   \nE.2 General Evalution 21 ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "F Relate the Selected Tokens\u2019 Loss to Downstream Task Performance 21 ", "page_idx": 16}, {"type": "text", "text": "G Examples of Tokens Selected by SLM 21   \nG.1 Token Selected Examples 21   \nG.2 Dynamic Token Selected 21 ", "page_idx": 16}, {"type": "text", "text": "H Self-Reference Setting 21 ", "page_idx": 16}, {"type": "text", "text": "I Weak-to-Strong Generalization 22 ", "page_idx": 16}, {"type": "text", "text": "A Author Contributions ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Zhenghao Lin designed and implemented detailed token selection process, conducted extensive preliminary experiments, developed the pre-training and evaluation pipeline, conducted most of the pre-training experiments and analysis, implemented baselines, and significantly contributed to the writing. Zhibin Gou presented a preliminary proposal, introduced the method of using excess loss for reweighting tokens, compiled high-quality corpora, trained reference models, set up the fine-tuning and evaluation pipelines, designed the experimental analysis, and significantly contributed to the writing. Yeyun Gong proposed the initial project and co-led the project with Weizhu Chen, they offered extensive advice and guidance on experiments and writing, and oversaw team collaboration and resource management. Xiao Liu, Yelong Shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, and Nan Duan offered research mentorship, coordinated the project, and contributed to the writing. ", "page_idx": 17}, {"type": "text", "text": "B Related Works ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "B.1 Pretraining Data Optimization ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The objective of optimizing pre-training corpora is to maximize the performance and efficiency of language model training by improving the quality and scale of the pretrain data mixture. This includes data collecting through crawling [Raffel et al., 2020] or synthesis [Polu and Sutskever, 2020, Gunasekar et al., 2023], de-duplication [Lee et al., 2021, Kandpal et al., 2022, Tirumala et al., 2023], filtering and selection [Xie et al., 2024a, Albalak et al., 2024], as well as data composition [Xie et al., 2024b] and curriculum [Chen et al., 2024, Ma et al., 2024]. ", "page_idx": 17}, {"type": "text", "text": "B.2 Data Selection ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Data selection for fine-tuning has been extensively studied, focusing on improving quality [Li et al., 2023c], diversity [Liu et al., 2024], and distribution matching [Li et al., 2023d, Xia et al., 2024, Ni et al., 2024, Kang et al., 2024]. For pretraining, various lightweight filters are utilized [Albalak et al., 2024], including heuristic-based (e.g., language and item count filtering), classifier-based [Brown et al., 2020], and perplexity or loss-based approaches [Wenzek et al., 2019, Qin et al., 2024]. The massive public RedPajama-Data-v2 dataset [Computer, 2023], for example, leverages over 40 quality indicators for data filtering and reweighting. Nevertheless, strict filtering like blocklist [Raffel et al., 2020] and Safety API filtering [Welbl et al., 2021], have been found to hurt evaluation loss or induce bias [Dodge et al., 2021]. ", "page_idx": 17}, {"type": "text", "text": "Sample-level selection has been extensively studied in previous research, particularly through online batch selection methods [Loshchilov and Hutter, 2015, Schaul et al., 2015, Katharopoulos and Fleuret, 2018, Jiang et al., 2019]. These approaches have been successfully applied to diverse classification tasks [Mindermann et al., 2022] and language modeling [Fan and Jaggi, 2023]. ", "page_idx": 17}, {"type": "text", "text": "Token-level training strategies have also been explored, especially for the pre-training of BERT-like models using Masked Language Modeling (MLM) [Devlin et al., 2018]. Specifically, \u201cselective masking\u201d involves masking important tokens in the input to focus on learning tokens that are more relevant to downstream tasks [Gu et al., 2020, Lad et al., 2022], whereas \u201ctoken dropping\u201d aims to reduce training costs by omitting less important tokens [Zhong et al., 2023a, Hou et al., 2022]. Additionally, some research has approached the analysis and detection of under-trained tokens from a tokenization perspective [Rumbelow and Watkins, 2023, Land and Bartolo, 2024]. To our knowledge, we are the first to explore token-level data selection for large language model training, aimed at enhancing data quality and information density at the most fundamental granularity. ", "page_idx": 17}, {"type": "text", "text": "B.3 Language Model Training Dynamics ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Investigating the training dynamics of language models is essential for understanding their behavior throughout the training process. This research includes studying internal representations [Saphra and Lopez, 2018], the acquisition of linguistic knowledge [Choshen et al., 2021, Liu et al., 2021], and the phenomenon of grokking [Power et al., 2022]. The analysis by Xia et al. [2022] is the most related to ours, which examines token-level training trajectories in models of varying sizes. Our findings, however, diverge from those of Xia et al. [2022], who posit that tokens with little change in perplexity are \u201calready learned\u201d. We identify a spectrum of token patterns, including \u201ceasy tokens\u201d and \u201chard tokens\u201d that resist convergence. Recognizing this, we propose a method of selective language modeling that targets the influential tokens, optimizing the learning process. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "B.4 Scaling Laws ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Scaling laws guide us in discovering the impact of factors such as parameter count, data size, and compute on language model performance and behavior. These studies usually focus on predicable scaling though power law [Kaplan et al., 2020, Hernandez et al., 2021], optimal resource allocation [Hoffmann et al., 2022], downstream tasks [Wei et al., 2022b, Isik et al., 2024, Gadre et al., 2024], architectures [Tay et al., 2022], memorization [Tirumala et al., 2022, Carlini et al., 2022, Henighan et al., 2023, Biderman et al., 2024], and repeating data [Hernandez et al., 2022, Muennighoff et al., 2024, Xue et al., 2024]. Most scaling laws on model performance study cross-entory loss on all training tokens, while we focus on the tokens loss of desired distributions. ", "page_idx": 18}, {"type": "text", "text": "C Limitations and Future Work ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Generalizability In math continual pretraining, as depicted in Figure 6, training exclusively with SLM leads to quickly convergence to the domain focused by the reference model, accompanied by a significant rise in the loss of unselected tokens. Although no adverse effects, like biases, have been observed from the increased loss yet, a general pretraining loss on text and code may prevent overfitting [Goodhart and Goodhart, 1984], as suggested by Ouyang et al. [2022] and Azerbayev et al. [2023]. Furthermore, future efforts could broaden the corpus scope of the reference model, and enlarge the pretraining data size, as exemplified by DeepSpeedMath [Shao et al., 2024]. ", "page_idx": 18}, {"type": "text", "text": "Scalability Due to budget constraints, we have only verified the effectiveness of our method on smaller models $\\scriptstyle\\mathbf{\\omega}=7\\mathbf{B}$ parameters) and smaller datasets $\\mathbf{\\langle<}100\\mathbf{B}$ tokens). Smaller models benefit significantly from removing the loss of irrelevant tokens and focusing on important ones. However, it\u2019s possible that very large models trained on extensive corpora may naturally develop this inductive bias to compress useful data (i.e., compressing everything), although it may sounds inefficient for now. Therefore, future works should study whether this selective language modeling technique can scale to very large models and data [Kaplan et al., 2020]. ", "page_idx": 18}, {"type": "text", "text": "Is training a reference model necessary? To score tokens, we need a high-quality reference model. This could be a base model trained with a small amount of high-quality data, or a performant open-source model. In fact, since we only need input logprobs or perplexity from reference model, we could even utilize more powerful proprietary model APIs. We can input tokens and use the log probabilities of the input returned by the API as reference scores. We leave this for future works. ", "page_idx": 18}, {"type": "text", "text": "How to improve upon SLM? There are many natural extensions of SLM, e.g., reweighting tokens instead of selecting may improve robustness; using a reference model as a reward model to guide pretraining with reinforcement learning; adopting multiple reference models to reduce overfitting; designing token-level curriculum learning and iterative strategies for continuous improvements, etc. ", "page_idx": 18}, {"type": "text", "text": "Expanding the use of SLM SLM may be extended to supervised fine-tuning to address the noise and distribution mismatches in many SFT datasets. Another potential application is alignment, e.g., by training a reference model to emphasize helpfulness, truthfulness, and harmlessness, we may obtain a base model that is natively aligned during the pretraining stage. Meanwhile, we believe that the idea of SLM may find broader applications in multimodal data such as images, videos, and speech, which have a high noise-to-information ratio than text. ", "page_idx": 18}, {"type": "text", "text": "D Analysis and Visualization of Tokens in Pretraining ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "D.1 More Details of Four Categories Tokens ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We categorize tokens into four categories: $\\mathrm{H\\mathrm{\\rightarrow}H}$ , $\\mathrm{L}{\\rightarrow}\\mathrm{H}.$ , $\\mathrm{H}{\\rightarrow}\\mathrm{L}$ , $\\mathrm{L}{\\rightarrow}\\mathrm{L}$ . During the training process, we collected the loss of each token after training on each 1 billion tokens training data. We then used ", "page_idx": 18}, {"type": "image", "img_path": "0NMzBwqaAJ/tmp/86f2df7fe9dc19cb63ff4eaf1e05e9fd078f6c89c6ee24b394a852c6255ef711.jpg", "img_caption": ["Figure 10: The loss of four categories of tokens during Mistral-7B pretraining on OpenWebMath. (a) shows the loss of $\\mathrm{H\\mathrm{\\rightarrow}H}$ , $\\mathrm{L{\\rightarrow}H}$ , $\\mathrm{H}{\\rightarrow}\\mathrm{L}$ , and ${\\mathrm{L}}{\\rightarrow}{\\mathrm{L}}$ tokens during pretraining. (b) and (c) show three cases of fluctuating tokens\u2019 loss in $\\mathrm{L}{\\rightarrow}\\mathrm{L}$ and $\\mathrm{H\\mathrm{\\rightarrow}H}$ during pretraining, respectively. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "linear fitting and took the difference in loss between the first and last points as evidence of whether the loss decreased during the training process. ", "page_idx": 19}, {"type": "text", "text": "Specifically, suppose we have a sequence of token\u2019s loss $(l_{0},l_{1},...,l_{n})$ . Our goal is to minimize the sum of the squares of the differences between each data point and its linear predictive value: ", "page_idx": 19}, {"type": "equation", "text": "$$\nf(a,b)={\\mathrm{minimize}}\\sum_{i=0}^{n}(l_{i}-(a x_{i}+b))^{2},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $x_{0}=0$ is the initial checkpoint and $x_{n}=n$ is the final checkpoint. Substituting these into the fitted equation, we can obtain the Loss values at the start and end after fitting: $\\mathcal{L}_{\\mathrm{start}}=b$ and $\\mathcal{L}_{\\mathrm{end}}=a n+b$ . The change in loss can then be expressed as: $\\Delta\\mathcal{L}=\\mathcal{L}_{\\mathrm{end}}-\\mathcal{L}_{\\mathrm{start}}$ . Meanwhile, we represent the average Loss of the last checkpoint as $\\mathcal{L}_{\\mathrm{mean}}$ . ", "page_idx": 19}, {"type": "text", "text": "Next, we can classify the tokens based on $\\Delta\\mathcal{L}$ and the $\\mathcal{L}_{\\mathrm{mean}}$ . We categorize tokens with $\\Delta\\mathcal{L}<-0.2$ as $\\mathrm{H}{\\rightarrow}\\mathrm{L}$ (loss decreases from high to low) category tokens, and tokens with $\\Delta\\mathcal{L}>0.2$ as $\\mathrm{L}{\\rightarrow}\\mathrm{H}$ (loss increases from low to high) category tokens. If $-0.2\\leq\\Delta\\mathcal{L}\\leq0.2$ and $l_{n}\\leq\\mathcal{L}_{\\mathrm{mean}}$ , then tokens are classified as ${\\mathrm{L}}{\\rightarrow}{\\mathrm{L}}$ (loss remains low); if $l_{n}>\\mathcal{L}_{\\mathrm{mean}}$ , they are classified as $\\mathrm{H\\mathrm{\\rightarrow}H}$ (loss remains high). In Figure 10, we have added the tokens\u2019 loss curves of the 7B model which is consistent with the other experimental settings in $\\S2.1$ , for readers to refer to whether similar phenomena exist on larger models. In Figure 11, we visualize examples of the four categories of tokens in actual text. ", "page_idx": 19}, {"type": "text", "text": "D.2 Non-Converging Tokens in Pretrainig ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In $\\S2.1$ , we mentioned that during the training process, only a minority of tokens belong to the $\\mathrm{H}{\\rightarrow}\\mathrm{L}$ category. Among the remaining categories of $\\mathrm{H\\mathrm{\\rightarrow}H}$ and ${\\mathrm{L}}{\\rightarrow}{\\mathrm{L}}$ tokens, there are tokens that exhibit significant fluctuations during training. Furthermore, there are instances where $\\mathrm{H}{\\rightarrow}\\mathrm{L}$ tokens are not effectively learned. Therefore, in our analysis, we specifically select those tokens from these categories that demonstrate considerable variability and distinct loss. We visualize these tokens that exhibit abnormal behavior during the training process. As illustrated in Figure 12, we find that the majority of these tokens originate from rather chaotic corpora. For instance, the corpora may include a mix of custom symbols, unintelligible gibberish, and information such as timetables and bibliographic references. Within a segment of normal text, there may also be fluctuations in the usage of common conjunctions, word suffixes, and punctuation marks. The latter may not necessarily be disastrous for training; in fact, it could represent a normal occurrence. However, if we can effectively mitigate the losses caused by the former, it might lead to more stable and efficient model training. ", "page_idx": 19}, {"type": "text", "text": "E Evalution Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "E.1 Math Evalution ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We conducted a comprehensive evaluation of the model across various math reasoning benchmarks, encompassing a range of difficulties from elementary to university level, multiple mathematical domains, and diverse question types including multiple-choice and open-ended questions. Our benchmarks include GSM8k [Cobbe et al., 2021], MATH [Hendrycks et al., 2021], GSM-Hard [Gao et al., 2022], SVAMP [Patel et al., 2021], ASDIV [Miao et al., 2020], MAWPS [Koncel-Kedziorski et al., 2016], TabMWP (TAB) [Lu et al., 2023], MathQA (MQA) [Amini et al., 2019], MMLU-STEM [Hendrycks et al., 2020], and SAT [Azerbayev et al., 2023]. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "E.2 General Evalution ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In the evaluation of general domain, we followed the lm-evaluation-harness [Gao et al., 2023] and evalute model on MMLU [Hendrycks et al., 2020], BBH [Suzgun et al., 2022], AGIEval [Zhong et al., 2023b], ARC-Easy and ARC-Challenge [Clark et al., 2018], BoolQ [Clark et al., 2019], PIQA [Bisk et al., 2020], Hellaswag [Zellers et al., 2019], WinoGrande [Sakaguchi et al., 2021], OpenBookQA [Mihaylov et al., 2018]. On HumanEval [Zheng et al., 2023] and TydiQA [Clark et al., 2020], we follow the evaluation pipeline of open-instrcut [Ivison et al., 2023] and report $\\mathrm{Pass}(\\varpi1$ and Pass $@10$ for HumanEval and F1 for TydiQA. For MBPP [Austin et al., 2021] benchmark, we follow the evaluation pipeline of DeepSeek-Coder [Guo et al., 2024], and report Pass $@1$ and Pass $@10$ . ", "page_idx": 20}, {"type": "text", "text": "F Relate the Selected Tokens\u2019 Loss to Downstream Task Performance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we declare the details about correlating the loss of selected tokens with the performance of downstream tasks. Concurrent study has explored similar methods to study the impact of scaling laws with the performance of models in downstream tasks [Gadre et al., 2024]. Our analysis here differs in that it aims to elucidate the relationship between the decrease/increase in loss for selected/unselected tokens and the model\u2019s performance on downstream tasks. ", "page_idx": 20}, {"type": "text", "text": "We use the average accuracy of MATH and GSM8K as the standard for measuring downstream tasks performance of model. Based on the trend of data points in Figure 7, we propose the relationship between the average accuracy of downstream tasks and selected/unselected tokens\u2019 loss, ", "page_idx": 20}, {"type": "equation", "text": "$$\nA c c(\\mathcal{L})=\\log(a*\\mathcal{L}+c)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The parameters $a$ and $c$ are fitted from the data. If the loss of selected tokens $\\mathcal{L}_{s}$ is used for fitting, then $a>0$ . Conversely, if the loss of unselected tokens $\\mathcal{L}_{u s}$ is used for fitting, then $a<0$ . Therefore, we believe that training the model on selected tokens can effectively improve its performance on downstream tasks, while unselected tokens may have a detrimental effect on the model\u2019s performance in downstream tasks. ", "page_idx": 20}, {"type": "text", "text": "G Examples of Tokens Selected by SLM ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "G.1 Token Selected Examples ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In Figure 13, we present several examples of tokens selected by the SLM method, with content marked in blue indicating the tokens actually chosen during the pretraining process. ", "page_idx": 20}, {"type": "text", "text": "G.2 Dynamic Token Selected ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In Figure 14, we display the dynamic changes in token selection tendencies throughout the SLM training process. We chose four checkpoints during the training process ${[0\\%}$ , $33\\%$ , $66\\%$ , and $100\\%$ ) to analyze the current tendencies in token selection. The preferences for token selection are indicated by different colors, ranging from high to low preference, typically represented as deep blue, blue, black, orange, and dark orange, respectively. ", "page_idx": 20}, {"type": "text", "text": "H Self-Reference Setting ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we will provide a detailed introduction to the reference loss score function and information entropy score function in SLM. Reference loss score function is to directly use the loss of the reference model as the basis for selecting tokens. The higher the token\u2019s loss of the reference model, the lower the expectation that the token will be selected. The score $\\mathcal{L}_{\\mathrm{RM}}(x_{i})$ can be directly obtained by referring to Equation 1. Information entropy score function is to select the corresponding token based on the information entropy of the reference model in each token. The information entropy of token $x_{i}$ can be expressed as: ", "page_idx": 20}, {"type": "table", "img_path": "0NMzBwqaAJ/tmp/0b9ee65f9a894095d1c7caecaa85e5aa4e4d619a9362f53a63f820c00e558f6d.jpg", "table_caption": ["Table 4: Full Self-Reference results on Tinyllama-1.1B. "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "0NMzBwqaAJ/tmp/a528ee49caf00fa3d3191bfb5f32d7119120e22649ba423d23aba6c3aed7d828.jpg", "table_caption": ["Table 5: Weak-to-Strong generalization result on math benchmark. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{H}_{\\mathrm{RM}}(x_{i})=-\\sum_{k=1}^{V}P(t_{k}|x_{<i})\\log P(t_{k}|x_{<i}),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $t_{k}$ represents the i-th token in the vocabulary, and $V$ represents the size of the vocabulary. The intuition of this strategy is that the higher the information entropy, the higher the uncertainty of the token in the context. Therefore, we consider that if the language model is still uncertain for certain tokens after pretraining, we do not hope that the language model to learn it during pretraining. In Table 4, we provide more SLM results, including different select ratios and combinations of two score functions, for the convenience of readers to refer to. ", "page_idx": 21}, {"type": "text", "text": "I Weak-to-Strong Generalization ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Apart from the main experiments where we use the same base model for the reference and continual pretraining, we also investigate if a smaller reference model can effectively guide the pretraining of a larger model. We use Tinyllama-1.1B as reference model and continual pretraining Llama-2-7B on 15B OpenWebMath tokens. Results presented in Table 5 indicate that, despite the considerable gap between the small and large models [Li et al., 2023e], employing the small reference model to token selection can still yield benefits to the pre-training of the larger model. If reference and training models have different vocabularies, one can consider performing token alignment [Wan et al., 2024, Fu et al., 2023], which we leave for future work. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: In the abstract and $\\S1$ , we clearly demonstrate the contribution and scope of this paper. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: In Appendix C, we have thoroughly discussed the limitations of our article, hoping to guide more future work. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: In $\\S2.1$ and $\\S2.2$ , we elaborated on the motivation and theoretical derivation of our method, with a complete proof process in place. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We have provided detailed descriptions of the experimental setup in $\\S3.1$ and methods in $\\S2.2$ to ensure that our experiment can be reproduced. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [No] ", "page_idx": 25}, {"type": "text", "text": "Justification: This may be temporary, and we are working hard to promote the process of open source. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: In $\\S3.1$ and Appendix E, we clearly demonstrated various experimental settings, including hyperparameters, model settings, training settings, evaluation settings, etc. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [No] ", "page_idx": 25}, {"type": "text", "text": "Justification: Due to the high cost of pre-training and the significant results obtained across various settings, we do not repeat the same experiments. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 25}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: In $\\S3.1$ , we have provided sufficient information on the computer resources needed to reproduce the experiments. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We guarantee that the research conducted in the paper complies with NeurIPS Code of Ethics in all aspects. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The purpose of this paper is to improve the training process of large language models, without any negative societal impacts. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The creators or original owners of the assets used in the paper, such as code, data, and models, have been appropriately recognized, and the licenses and terms of use have been clearly mentioned and properly respected. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 28}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}, {"type": "text", "text": "GMAT 1: 670 Q49 V31 $\\ln$ GMAT 2: 710 Q50 V35 \\n Followers: $175\\;\\mathrm{{m}}$ \\n Kudos [?]: 890 [0], given: $235\\;\\mathrm{{ln}\\;\\;\\mathrm{{ln}\\;\\mathrm{{Re}}}}$ : Mr. and Mrs Wiley, VIC[#permalink] 13 Feb 2010, 01:03 $\\ln$ Ans A $\\textsf{i n}\\,\\ln$ their first child was born after J years... $\\ln$ $\\ln$ thus 1 child $->$ j years $\\mathrm{~\\psi~}_{\\mathrm{{m}}}\\mathrm{~\\psi~}_{\\mathrm{{n}}}=>$ thus after another J years his ag $\\mathrm{~\\r~{~e~}~=~J~\\backslash~n~\\backslash~n~}$ thus his age is $\\mathrm{J}\\rightarrow$ after 2J years and 2j after 3j years $\\textsf{i n}\\ln$ his present age is $\\mathrm{T}$ which is after $\\mathrm{T}$ years. $\\textsf{i n}\\ln$ thus total time after 2years will be $\\mathrm{T}{+}2\\;\\mathrm{W}$ since after every J year they have a child after T+2 they will have \\fra $\\mathfrak{c}\\{(\\mathrm{T}{+}2)\\}\\{\\mathrm{J}\\}+1$ ( $+1$ is for the oldest) $\\ln$ $\\ln$ thus A $\\ln$ $\\ln$ $\\ln$ Fight for your dreams :For all those who fear from Verbal- lets give it a fight $\\textsf{i n}\\ln$ Money Saved is the Money Earned $\\textsf{i n}\\ln$ Jo Bole So Nihaal , Sat Shri Akaal $\\textsf{i n}\\ln$ Gmat test review : $\\ln670$ -to-710-a-long-journey-without-destination-still-happy-141642.html $\\textsf{i n}\\ln$ Intern $\\ln$ Joined: $06\\;\\mathrm{Apr}\\;2012\\;\\mathrm{{ln}}$ Posts: $28\\;\\mathrm{{un}}$ Followers: $0\\;\\mathrm{{u}\\;\\ln\\;\\Omega_{n}}$ Kudos [?]: 4 [0], given: $37\\;\\mathrm{{ln}\\;\\;\\mathrm{{ln}}}$ Re: Mr. and Mrs Wiley, VIC[#permalink] 21 Nov 2012, 07:46 $\\ln$ jeeteshsingh wrote: $\\ln$ Need the solution using Algebra.... $\\ln\\,{\\mathrm{~w~}}$ Mr. & Mrs Wiley have a child every J years. Their oldest child is now $\\mathrm{T}$ years old. If they have a child 2 years from now, how many children will they have in total? $\\textsf{i n}\\ln$ (A) \\frac{T+2}{J} + 1 \\n \\n (B) $\\mathrm{JT}+1$ $1\\mathrm{\\:\\backslash\\:}\\ln\\mathrm{\\:\\backslash\\:}\\mathrm{\\:frac\\{J\\}\\{T\\}+\\backslash\\:}\\mathrm{\\:frac\\{1\\}\\{T\\}\\:\\:\\backslash n\\:\\:}\\mathrm{\\:\\backslashn\\:\\:}\\mathrm{(D)\\:TJ-1\\:\\:}\\mathrm{\\:\\backslashn\\:\\:}$ $\\ln{(\\mathrm{E})}\\,\\backslash\\!\\operatorname{frac}{\\{\\mathrm{T}\\!+\\!\\mathrm{J}\\}}\\,\\{\\mathrm{J}\\}\\,\\ln\\,\\,\\backslash\\!{\\mathrm{n}}$ [Reveal] Spoiler: OA: $\\ln$ (A) $\\textsf{i n}\\ln$ Source: Manhattan Guide $\\mathsf{i n}\\,\\mathsf{i n}$ Bunuel - would really appreciate you providing your bit on solving the original problem above algebraically. The problem and various explanations remain confusing. Should we think of it as a progression or some other way? Please share your take. Thank you. $\\ln$ Veritas Prep GMAT Instructor $\\ln$ Joined: 16 Oct $2010\\;\\mathrm{{un}}$ Posts: $4566\\;\\mathrm{{u}}\\mathrm{{n}}$ Location: Pune, India $\\ln$ Followers: $1029\\;\\mathrm{{un}}$ $\\ln$ Kudos [?]: 4460 [1] , given: $162\\;\\mathrm{{w}\\;\\mathrm{{w}\\Omega}}$ Re: Mr. and Mrs Wiley, VIC[#permalink] 21 Nov 2012, 09:45 $\\ln{1}\\ln$ KUDOS $\\ln$ Expert\u2019s post $\\ln$ jeeteshsingh wrote: $\\ln$ Need the solution using Algebra.... \\n \\n Mr. & Mrs Wiley have a child every J years. Their oldest child is now T years old. If they have a child 2 years from now, how many children will they have in total? $\\ln\\,{\\mathrm{~w~}}$ (A) \\fra $:\\{{\\mathrm{T}}\\!+\\!2\\}\\{{\\mathrm{J}}\\}+1$ \\n $\\ln$ (B) $\\mathrm{JT}+1$ \\n $\\ln$ (C) \\frac{J}{T} $+\\,\\backslash\\mathrm{frac}\\{1\\}\\{\\mathrm{T}\\}\\mathrm{~\\backslash\\mathrm{~\\bar{n}~}\\,\\backslash\\mathrm{n}\\,\\,(\\mathrm{D})\\,\\mathrm{TJ-1\\,\\backslash\\mathrm{~\\bar{n}~}\\,\\backslash\\mathrm{n}\\,\\,(E)\\,\\backslash\\mathrm{frac}\\{\\mathrm{T}+\\mathrm{J}\\}\\{\\mathrm{J}\\}\\mathrm{~\\backslash\\mathrm{~\\bar{n}~}\\,\\backslash\\mathrm{n}~}}$ [Reveal] Spoiler: OA: $\\ln$ (A) $\\ln\\,{\\mathrm{~w~}}$ Source: Manhattan Guide $\\ln\\,{\\mathrm{~w~}}$ Think of it as an Arithmetic Progression where every subsequent term (child) has a difference of J yrs from the previous term (child). $\\textsf{i n}\\ln$ 1st child, 2nd child, 3rd child, ....... nth child (to be born after 2 yrs) $\\ln\\,{\\mathrm{~w~}}$ What is the difference between first and last terms (children)? $(\\mathrm{T}+2)$ yrs $\\ln\\,{\\mathrm{~w~}}$ What is the common difference (age difference between two consecutive kids)? J yrs $\\textsf{i n}\\ln$ What is the number of terms (children)? $(\\mathrm{T}+2)/\\mathrm{J}+1$ $\\ln$ (Number of terms of an AP is $\\mathrm{n}=\\mathrm{(Last}$ term - First term)/Common Difference $+~1$ . ) $\\ln$ $\\textsf{i n}\\ln$ Karishma $\\ln$ Veritas Prep | GMAT Instructor $\\ln$ My Blog $\\textsf{i n}\\ln$ Save $\\mathbb{S}100$ on Veritas Prep GMAT Courses And Admissions Consulting Enroll now. Pay later. Take advantage of Veritas Prep\u2019s flexible payment plan options. Veritas Prep Reviews Re: Mr. and Mrs Wiley, VIC [#permalink] 21 Nov 2012, 09:45 Similar topics Replies Last post Similar Topics: 1 Mr. and Mrs. O\u2019Leary (SC) 5 08 Jul 2012, 07:15 Mr. INVESTOR invested a total of\\$12,000 for a one-year 4 30 Mar 2007, 09:24 $\\ln2$ Mr. and Mrs. Wiley have a child every J years. Their oldest 7 19 Feb 2007, 11:40 $\\ln$ Mr.kevincan 6 16 Aug 2006, $12{:}26\\;\\mathrm{{m}}$ PS: Mr. & Mrs. Smith 2 06 Dec 2005, $00{:}03\\;\\mathrm{{u}}\\mathrm{}$ Display posts from previous: Sort by Sciencemadness Discussion Board $\\gg$ Fundamentals $\\gg$ Reagents and Apparatus Acquisition $\\gg$ Sulphuric Acid in Australia Select A Forum Fundamentals $\\gg$ Chemistry in General $\\gg$ Organic Chemistry $\\gg$ Reagents and Apparatus Acquisition $\\gg$ Beginnings $\\gg$ Responsible Practices $\\gg$ Miscellaneous $\\gg$ The Wiki Special topics $\\gg$ Technochemistry $\\gg$ Energetic Materials $\\gg$ Biochemistry $\\gg$ Radiochemistry $\\gg$ Computational Models and Techniques $\\gg$ Prepublication Non-chemistry $\\gg$ Forum Matters $\\gg$ Legal and Societal Issues $\\textsf{i n}\\ln$ Pages: $1\\;2\\;\\mathrm{{w}}$ Author: Subject: Sulphuric Acid in Australia $\\ln$ hissingnoise $\\ln$ International Hazard \\n \\n Posts: $3939\\;\\mathrm{{un}}$ Registered: 26-12-2002 $\\ln$ Member Is Offline $\\textsf{i n}\\ln$ Mood: Pulverulescent! $\\textsf{i n}\\ln$ I\u2019ve stated several times on various threads, that SO<sub>3</sub> produces a practically incondensable acid mist when led to water and, BTW, at $700^{\\circ}\\mathrm{C}$ the decomposition rate of SO<sub>3</sub> is ${}^{87\\%}$ . . . $\\ln$ Cracking Na<sub>2</sub>S<sub>2</sub>O<sub>7</sub> proceeds at $\\mathrm{\\sim}466^{\\circ}\\mathrm{C}$ and the issuing gasses are readily absorbed by conc. H<sub>2</sub>SO<sub>4</sub> to form oleum! $\\textsf{i n}\\ln$ Phthalic Acid $\\ln$ Harmless $\\textsf{i n}\\ln$ Posts: $19\\,\\mathrm{{ln}}$ Registered: 7-8-2011 $\\ln$ Location: Australia $\\ln$ Member Is Offline $\\textsf{i n}\\ln$ Mood: No Mood $\\textsf{i n}\\ln$ That\u2019s a good idea Neil, I\u2019ll be sure to try that next time (probably for H2O2). Just went to Tradelink and asked if they sold Moflo drain cleaner. The guy said yeah and I asked for a liter of it. No problems whatsoever, he just said \u201dbe careful with it\u201d. It was $\\mathbb{S}45$ but a liter will last me a while and making it myself would\u2019ve been vastly more expensive I imagine. Success! MeSynth Hazard to Others Posts: 107 Registered: 29-7-2011 Member Is Offline Mood: http://www.youtube.com/watch?v $=:$ 5ZltqlVuDIo Sulfuric acid can be produced in the laboratory by burning sulfur in air and dissolving the gas produced in a hydrogen peroxide solution. $\\mathrm{SO}2+\\mathrm{H}2\\mathrm{O}2\\rightarrow\\mathrm{H}2\\mathrm{SO}4$ this was found on wikipedia... did you not look through the sullfuric acid wiki before boiling down batery acid? anyways... There are some good videos on youtube that demonstrate how to synthesize sulfuric acid using different methods. The drain cleaner you get from the store will be impure and may contain organic matter that discolors the acid. ", "page_idx": 30}, {"type": "text", "text": "Figure 11: Sample text containing four categories of tokens. Among them, blue represents tokens of categorie $\\mathrm{H}{\\rightarrow}\\mathrm{L}$ , green indicates tokens of categorie ${\\mathrm{L}}{\\rightarrow}{\\mathrm{L}}$ , yellow signifies tokens of categorie $\\mathrm{H\\mathrm{\\rightarrow}H}$ , and red denotes tokens of categorie $\\mathrm{L}{\\rightarrow}\\mathrm{H}$ . ", "page_idx": 30}, {"type": "text", "text": "as \\n \\n \\begin{aligned}A \\in \\{\\pm \\begin{bmatrix}\\cos\\theta & - \\sin\\theta \\\\ \\sin\\theta & \\cos\\theta \\\\ \\end{bmatrix}, \\pm \\begin{bmatrix}\\cos\\theta & \\sin\\theta \\\\ \\sin\\theta & - \\cos\\theta \\\\ \\end{bmatrix}, \\pm \\begin{bmatrix}i \\sinh\\theta & -\\cosh\\theta \\\\ \\cosh\\theta & i \\sinh\\theta \\\\ \\end{bmatrix}, \\pm \\begin{bmatrix}i \\sinh\\theta & \\cosh\\theta \\\\ \\cosh\\theta & - i \\sinh\\theta \\\\ \\end{bmatrix}\\}\\end{aligned} \\quad\\quad\\quad(25) $\\textsf{w}\\ln\\textsf{I}$ suspect this class of transformations has a name in the grand group classification scheme, but I don\u2019t know what it is. ### Mathematics Class XI $\\textsf{W}\\textsf{W}$ Unit-I: Sets and Functions $\\mathfrak{m}$ Chapter 1: Sets $\\ln$ Unit-II: Algebra $\\ln$ Chapter 5: Binomial Theorem $\\mathfrak{m}$ Chapter 6: Sequence and Series $\\ln$ Unit-III: Coordinate Geometry $\\ln$ Chapter 1: Straight Lines $\\ln$ Chapter 2: Conic Sections $\\mathfrak{m}$ Unit-IV: Calculus $\\mathfrak{m}$ Unit-V: Mathematical Reasoning $\\ln$ Unit-VI: Statistics and Probability $\\ln$ Chapter 1: Statistics $\\mathfrak{m}$ Chapter 2: Probability $\\textsf{i n}\\ln$ # Graphs of Trigonometric Functions $\\textsf{m}\\ln$ (i) Geometry in any field. Queries are case-independent. Funct\\* Wildcard queries are specified by \\* (e.g. functions, functorial, etc.). Otherwise the search is exact. \u201dTopological group\u201d Phrases (multi-words) should be set in \u201dstraight quotation marks\u201d. au: Bourb aki & ti: Algebra Search for author and title. The and-operator $\\&$ is default and can be omitted. Cheb yshev | Tschebyscheff The or-operator | allows to search for Cheb yshev or Tschebyscheff. \u201dQuasi\\* map\\*\u201d py: 1989 The resulting documents have publication year 1989. so: Eur\\* $\\mathrm{J}^{*}$ Mat\\* Soc\\* cc: 14 Search for publications in a particular source with a Mathematics Subject Classification code (cc) in 14. \u201dPartial diff\\* eq\\*\u201d ! elliptic The not-operator ! eliminates all results containing the word elliptic. dt: b & au: Hilbert The document type is set to books; alternatively: $\\mathrm{j}$ for journal articles, a for book articles. py: 2000-2015 cc: (94A | 11T) Number ranges are accepted. Terms can be grouped within (parentheses). la: chinese Find documents in a given language. ISO 639-1 language codes can also be used. ", "page_idx": 31}, {"type": "text", "text": "Code: Select all \\ $\\overline{{1\\ \\backslash\\ \\ln\\mathrm{x}=64}}$ , $\\overline{{\\mathrm{y}=86}}$ , rule = B3/S23 \\n 13bo\\$3bobo6bo\\$4b2o6b3o\\$4bo\\$54bo\\$54bobo\\$13b2o39b 2o \\$12b2o44b2o\\$3o11bo43b $\\mathfrak{m}$ o3b2o\\$2bo49bo 6bo2bo\\$bo50b 2o6b obo\\$51bob o7bo\\$7bo49bo\\$7 b3o47b3o\\$10bo5b2o $\\ln$ 42bo\\$9b2o4b2o 42b 2o\\$17bo7\\$13bo\\$3b obo6bo\\$4b 2o6b 3o\\$4bo\\$54bo\\$54b obo\\$13b 2o 39b2o\\$12b2o44b2o\\$3o11bo43bo3b2o\\$2bo49bo6bo2bo\\$bo50b 2o6b obo\\$51bobo7bo\\$7bo49bo\\$7b3o47b3o\\$10bo5b2o42bo\\$9bo5b2o42bo\\$9b2o6bo41b2o7\\$13bo\\$3bobo6bo\\$4b 2o6b3o\\$4bo\\$54bo\\$54bobo\\$13b2o39b2o\\$12b2o44b2o\\$3o11bo43bo3b2o\\$2bo49bo   \n6bo2bo\\$bo50b2o6bobo\\$51bobo7bo\\$7bo49bo\\$7b3o47b3o\\$10bo5b2o42bo\\$7b3o5b2o   \n40b3o\\$7bo9bo39bo7\\$13bo\\$3b obo6bo\\$4b 2o6b 3o\\$4bo\\$54bo\\$54b obo\\$13b 2o39b 2o\\$ \\n 12b 2o44b 2o\\$3o11bo43bo3b 2o\\$2bo49bo6bo2bo\\$bo50b 2o6b obo\\$51b obo7bo\\$7bo49bo\\$7b 3o47b 3o\\$10bo5b 2o42bo\\$7b 2obo4b 2o40b 2obo\\$7b obo7bo39b obo! The 16-bitter thus goes down to 9 gliders. It does not reduce any further 17-bitters, though. Princess of Science, Parcly Taxel Kazyan Posts: 867 Joined: February 6th, 2014, 11:02 pm ### Re: 17 in 17: Efficient 17-bit synthesis project Good catch, Sokwe. $\\#61$ in 15G:   \nGround Penetrating Radar for Archaeology $\\overline{{\\mathrm{~\\ln~}\\mathrm{~}\\mathrm{~}\\mathrm{~m~}}}$ Workshop | December1 | 1-5 p.m. | 1012251 College (Archaeological Research Facility) $\\textsf{i n}\\ln$ Scott Byram, Research Associate, Archaeological Research Facility, UC Berkeley $\\textsf{i n}\\ln$ Archaeological Research Facility $\\textsf{i n}\\ln$ At 1pm the workshop will begin at the UC Faculty Club lawn where subsurface features are being mapped. $\\textsf{i n}\\ln$ ### Student Probability/PDE Seminar: Large Deviation Principle for random graphs $\\mathrm{II}\\,\\backslash\\,\\backslash\\,\\backslash\\,\\backslash\\,\\backslash$ Seminar | December1 $1\\,2{:}10{-}3{:}30\\ \\mathrm{p.m.}$ | 891Evans Hall $\\textsf{i n}\\ln$ Fraydoun Rezakhanlou, UC Berkeley $\\textsf{m}\\ln$ Department of Mathematics $\\textsf{i n}\\ln$ ### BLC Fellows Forum $\\textsf{i n}\\ln$ Presentation | December1 | 3-5 p.m. | Dwinelle Hall, B-4 (Classroom side) $\\textsf{m}\\ln$ FAll 2017 BLC Fellows, UC Berkeley $\\textsf{i n}\\ln$ Berkeley Language Center $\\textsf{m}\\ln$ Teaching French Listening Comprehension and Cultural Awareness through Regional Variation $\\mathfrak{m}$ Elyse Ritchey, GSR, French $\\mathfrak{m}$ At the university level, French language instruction in the US traditionally includes a course on phonetics and pronunciation. While the major aim of such courses is to improve students\u2019 speaking and listening competence, they also emphasize speaking \u2018correctly\u2019 using... More $>\\ln$ \\n ### MENA Salon $\\ln$ $\\mathfrak{m}$ Workshop | December1 $\\mid3{-}4\\,\\mathrm{\\,p.m}$ . | 340Stephens Hall $\\textsf{m}\\ln$ Every Friday in the semester, the CMES hosts an informal week ", "page_idx": 31}, {"type": "text", "text": "Token Selected Examples ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "\u2022 Process the student answer as a Math Object Formula, and break down its parse tree by its top-level operators. The idea is to create an array of the student\u2019s primitive factors, so say $3(\\mathrm{x}{+}1)(\\mathrm{x}{+}2)^{\\times}2$ gives $(3,\\mathbf{x}{+}1,\\mathbf{x}{+}2)$ . \u2022 Because we may want factoring over Z, checking the gcd of coefficients within each factor. $\\bullet$ Pass each of these things to SAGE and ask if the nonconstant factors are reducible over $Z$ or Q. Also ask if they are monic. These things at least we learned how to do at the Vancouver code camp. The end goal is to count the following forms as correct, possibly controlled by flags: n \\{}prod (factor)\u02c6power, where each factor is irreducible in Z[X], n in Z r \\{}prod (factor)\u02c6power, where each factor is irreducible and monic in Q[X], r in $Q\\,\\mathrm{I}$ suppose on the last one the monic requirement could be dropped with a flag. I have no plans to check that the form is fully condensed, e.g. forcing $(\\mathrm{x}{+}1)^{{\\wedge}2}$ and rejecting $(\\mathrm{x}{+}1)(\\mathrm{1{+}x})$ ", "page_idx": 32}, {"type": "text", "text": "The equation of the path traversed by a projectile is called equation of trajectory. $\\overline{{\\ln\\,\\,\\ln\\,}}$ Suppose, the body reaches the point $\\mathrm{P}$ after time ( t ) . $\\textsf{i n}\\ln$ Horizontal motion has no acceleration. Thus, using kinematic equation, horizontal distance covered will be $-\\ln$ \\n $\\Chi=\\mathbf{u}$ \\cos \\theta ${\\mathfrak{t}}\\ln\\ {\\mathrm{ln}}$ Or, \\quad $\\mathbf{t}=$ ( \\frac $\\{\\textbf{x}\\}\\{$ { u \\cos \\theta } ) $\\mathfrak{m}$ $\\ln$ Vertical motion has constant acceleration $\\mathrm{~(~g~)~}$ . Thus, distance covered will be $-\\ln$ \\n ${\\mathrm{y}}=($ ( u \\sin \\theta ) t - \\left ( \\frac {1}{2} \\right ) g t\u02c62 \\n $\\ln{=}$ ( u \\sin \\theta ) \\left ( \\frac $\\{\\mathbf{x}\\}$ {u \\cos \\theta} \\right ) - \\left ( \\frac {1}{2} \\right ) g \\left ( \\frac {x}{u \\cos \\theta} \\right $)^{\\wedge}2\\;\\mathrm{\\backslashn}\\;\\;\\mathrm{\\backslashn}=`$ left ( \\tan \\theta \\right ) x - \\left ( \\frac $\\{\\mathbf{g}\\}\\{2\\mathrm{~u~}^{\\star}2\\}\\cos^{\\star}2$ \\theta} \\right ) $\\mathbf{X}^{\\sim}2$ $\\ln\\,{\\mathrm{~w~}}$ In this equation, ( \\theta, \\ u \\ \\text {and} $\\mid\\mathbf{g}$ ) are constants. Thus, $\\mathsf{i n}\\,\\mathsf{\\Omega}\\setminus\\mathsf{i n}\\,1$ . Term \\left ( \\tan \\theta \\right ) is a constant, let it is $({\\textsf{p}})\\ln2$ . Term \\left [ \\left ( \\frac { $\\mathrm{g}\\}\\{2\\,\\mathrm{u}^{\\wedge}2\\,\\backslash\\mathrm{cos}^{\\wedge}2$ \\theta} \\right ) \\right ] is also a constant, let it is ( q ) \\n \\n So, \\quad ${\\bf y}={\\bf p}\\mathrm{~x~}{-}\\;{\\bf q}\\;\\mathrm{x}\\hat{\\bf\\Omega}2\\mathrm{~\\boldmath~\\bar{~}{\\bf n}~}\\mathrm{~\\bar{~}{~n~}~}$ Therefore, $\\big(\\mathrm{\\textbf{y}}\\mathrm{\\,propto\\x}^{\\prime}2\\big)$ ) , which is a required condition of a parabola. The trajectory of the projectile is a parabola. $\\overline{{\\ln\\,\\ln\\,}}$ ### Time of Maximum height $\\overline{{\\ln\\,\\,}}\\,\\mathrm{~\\textbar{n}~}$ As the body is projected it goes up. Vertical component of velocity ( u $\\dot{\\mathrm{{\\backslashsin}}}$ \\theta ) gradually diminishes and becomes zero at the maximum height of flight. After that, body starts moving downwards. $\\textsf{W}\\ln$ Let, $\\textrm{t},\\textrm{m})$ is the time to reach at maximum height ( h_m ) of flight. $\\textsf{i n}\\ln$ Therefore, from kinematic equation, we have $-\\left.\\ln\\,\\right\\backslash{\\bf n}\\ 0={\\bf u}$ \\sin \\theta - $\\mathbf{g}\\mathrm{~t~}\\underline{{\\mathfrak{m}}}\\ln\\mathrm{~\\textbar{~}{\\mathfrak{n}}~}\\mathbf{Or},$ , \\quad $\\mathrm{t\\_m}=\\backslash\\mathrm{left}$ ( \\frac {u \\sin \\theta} $\\{\\mathbf{g}\\}$ \\right ) $\\textsf{i n}\\ln$ ### Time of Flight $\\textsf{i n}\\ln$ Total time taken by the projectile between the instant it is projected and till it reaches at a point in the horizontal plane of its projection is called Time of flight. $\\mathsf{i n}\\,\\mathsf{i n}$ Let, the body reaches at point B on ground after time ( T_f ) of projection. Then $-\\ln\\,\\mathrm{~}\\mathrm{~u~}$ Net vertical displacement covered during the time of flight is zero. Using kinematic equation of motion, we get \u2013 $-\\ln$ \\n $0=$ ( u \\sin \\theta ) T_f - \\left ( \\frac {1}{2} \\right ) $\\mathrm{~g~}\\backslash\\left(\\mathrm{~T~}_{-}\\mathrm{~f~}\\right)\\hat{2}$ \\n $\\ln$ Or, \\quad $\\mathrm{T}\\_{\\mathrm{f}}=1$ \\left ( \\frac {2 u \\sin \\theta}{g} \\right $)=2$ \\left ( \\frac {u \\sin \\theta}{g} \\right ) $\\mathrm{~\\bar{~}n~}\\,\\backslash\\mathrm{n}=2\\mathrm{~t~}\\mathrm{~m~}\\backslash\\mathrm{n}$ \\n Thus, \\quad \\text {Total time of flight} $=$ \\text {Time of ascent} $^{+}$ \\text {Time of descent} $\\mathfrak{m}$ $\\ln=2$ \\times \\text {Time of maximum height.} $\\ln\\,{\\mathrm{~w~}}$ ### Maximum height of Flight $\\textsf{i n}\\ln$ It is the maximum height reached by a projectile. It is denoted by $(\\textrm{h}\\textrm{m}$ ) $\\mathfrak{m}$ $\\ln$ At the highest point of flight, the vertical component of velocity becomes zero. $\\ln\\,{\\mathrm{~w~}}$ From kinematic equation of motion, we have $-\\ln$ $\\ln{\\mathrm{v}}^{\\star}2=\\mathsf{u}^{\\star}2+2$ a s $\\ln$ \\n Therefore, \\quad $0^{\\star_{2}}$ - ( u \\sin \\theta ) $)^{\\star}2=2\\ ({\\bf\\Pi}-{\\bf g}\\ )\\,{\\bf h}\\_\\mathrm{m}\\ \\backslash\\ n\\ {\\bf O r},$ , \\quad $\\mathbf{h}\\_\\mathrm{m}=^{\\prime}$ \\left ( \\frac $\\{\\mathbf{u}^{\\mathrm{{\\hat{\\Delta}}}2}\\,\\mathrm{{lsin}^{\\mathrm{{\\hat{\\Delta}}}2}\\,\\backslash t h e t a}\\}\\{2\\;\\mathrm{g}\\}\\;\\backslash\\mathrm{{right}}\\}$ ) ", "page_idx": 32}, {"type": "text", "text": "We identify two equations having the same solution with the equivalence relation: $\\overline{{\\ln\\,\\,}}\\,\\mathrm{~\\textbar{n}~}$ \\$(a,b) \\sim (c,d) \\mbox{ if and only if } $\\mathsf{a d}=\\mathsf{b c}\\mathbb{S}\\setminus\\mathsf{i n}\\setminus\\mathsf{m}$ To show that this is an equivalence relation: $\\ln\\,\\ln\\,1$ . Reflexivity: $\\mathbb{S}\\mathbb{S}(\\mathrm{a},\\mathrm{b})$ \\sim (a,b)\\$\\$ if and only if $\\mathbb{S}\\mathbb{S}\\mathbf{a}\\mathbf{b}=\\mathbf{b}\\mathbf{a}\\mathbb{S}\\mathbb{S}$ which is true. Hence it is reflexive. $\\ln2$ . Symmetry: $\\mathbb{S}\\mathbb{S}(\\mathrm{a},\\mathrm{b})$ \\sim $(\\mathsf{c},\\mathsf{d})\\mathbb{S}\\mathbb{S}$ if and only if $\\mathbb{S}\\mathbb{S}\\mathbf{a}\\mathbf{d}=\\mathbf{b}\\mathrm{c}\\mathbb{S}\\mathbb{S}$ if and only if $\\mathbb{S}\\mathbb{S}\\mathbf{bc}=\\mathrm{ad}\\mathbb{S}\\mathbb{S}$ if and only if $\\Phi\\mathbb{S}(\\mathrm{c},\\mathrm{d})$ \\sim (a,b)\\$\\$. Hence it is symmetric. $\\ln3$ . Transitivity: \\$\\$(a,b) \\sim (c,d)\\$\\$ and $\\mathbb{S}\\mathbb{S}(\\mathbf{c},\\mathbf{d})$ \\sim (e,f)\\$\\$ if and only if $\\mathbb{S}\\mathbb{S}{\\mathbf{ad}}={\\mathbf{b}}\\mathrm{c}\\mathbb{S}\\mathbb{S}$ and $\\mathbb{S}\\mathbb{S}\\mathrm{cf}=\\mathbf{de}\\mathbb{S}\\mathbb{S}$ . Multiplying these equations together, we get \\$\\$adcf $=$ bcde\\$\\$. We can cancel $\\mathbb{S}\\mathbb{S}\\mathrm{d}\\mathbb{S}\\mathbb{S}$ and $\\mathbb{S}\\mathbb{S}\\mathbf{c}\\mathbb{S}\\mathbb{S}$ from both sides to get $\\mathbb{S}\\mathbb{S}\\mathrm{af}=\\mathrm{be}\\mathbb{S}\\mathbb{S}$ . Hence $\\mathbb{S}\\mathbb{S}(\\mathrm{a},\\mathrm{b})$ \\sim (e,f)\\$\\$. $\\textsf{i n}\\,\\ln$ Hence, we have successfully formed the set of rational numbers when we factor out the equivalence classes! $\\mathfrak{m}$ \\n \\$\\mathbb{Q} $=$ \\frac{\\mathbb $\\{Z\\}$ \\times \\mathbb $\\{Z\\}$ \\backslash\\{0\\}}{\\sim}\\$ \\n $\\ln$ Let\u2019s now take a look at what members of \\$\\$\\mathbb $\\{\\mathrm{Q}\\}\\mathbb{S}\\mathbb{S}$ look like, say for the equation $\\mathbb{S}\\mathbb{S}2\\mathrm{x}=3\\mathbb{S}\\mathbb{S}$ . This equation is represented by the ordered pair ", "page_idx": 32}, {"type": "text", "text": "If the light moves in a purely radial direction, we can describe its path by the coordinate functions $\\mathbb{S}\\mathbb{S}\\mathrm{t}(\\mathrm{\\backslashlambda})\\mathbb{S}\\mathbb{S}$ and $\\mathbb{S}\\mathbb{S}\\mathbf{r}(\\mathrm{llambda})\\mathbb{S}\\mathbb{S}$ . The equation of motion $\\mathbb{S}\\mathbb{S}\\mathrm{d}\\mathrm{s}^{\\wedge}\\mathrm{2}\\,{=}\\mathrm{0}\\mathbb{S}\\mathbb{S}$ then takes the form $\\mathbb{S}\\mathbb{S}_{8-}\\{$ {tt} \\left(\\frac{dt}{d\\lambda}\\right)\u02c62 $+\\mathrm{\\bf~g}_{-}\\{\\mathrm{\\bfr}\\}$ \\left(\\frac{dr}{d\\lambda $\\}\\mathrm{{tright}})\\mathrm{{}^{\\sim}}2\\,=\\,0{,}\\mathbb{S}\\mathbb{S}$ which we can rewrite as \\$\\$\\left(\\frac{ $\\mathrm{{\\small~\\displaystyle{\\hat{\\m}}~}}\\}\\,\\{\\mathrm{{\\scriptsize~d}}\\mathbf{r}\\}\\mathrm{\\backslash\\dot{\\mathrm{right}}})^{\\wedge}2\\,=\\,-$ \\frac $\\{\\mathbf{g}_{-}\\{\\mathrm{rr}\\}\\}\\,\\{\\mathrm{g}_{-}\\{\\mathrm{tt}\\}\\}$ .\\$\\$ $\\textsf{i n}\\ln$ The length of the rod is then $\\mathbb{S}\\mathbb{S}\\mathbb{L}={\\mathbf{c}}\\;\\mathrm{{sint}\\_\\{r\\_1\\}\\uparrow\\_\\{r\\_2\\}\\}\\;\\mathrm{{tfrac}\\{\\,\\mathbf{dt}\\}\\,\\{d r\\}\\;\\mathrm{{text}\\{\\mathbf{\\tau}\\mathbf{d}\\}r=c}}$ $\\mathrm{int\\_}\\{\\mathrm{r\\_1}\\}\\,\\hat{\\mathrm{~}}\\{\\mathrm{r\\_2}\\}$ \\sqrt{-\\frac{g_{rr}}{g_{tt}}} \\text{ d}r,\\$\\$ where I have taken the positive square root because $9\\mathbb{S}\\mathrm{r}\\_2>\\mathrm{r}\\_1\\mathbb{S}\\mathbb{S}$ . $\\mathfrak{w}$ $\\ln$ Notice that the length is independent of the signature of the metric, so whether you work with the $(-+++)$ or $(+-)$ metric is purely conventional and will not change the physics. $\\textsf{i n}\\ln$ For the Schwarzschild metric, we obtain explicitly \\$\\$L = r_2 - r_1 + r_s \\ln\\left(\\frac{r_2 - r_s}{r_1 - r_s}\\right) > r_2 - r_1.\\$\\$ $\\mathsf{i n}\\,\\mathsf{i n}$ Now what happens if you magically, instantaneously increase the mass of the black hole? I think the length $\\mathbb{S}\\mathbb{S}\\mathbb{L}\\mathbb{S}\\mathbb{S}$ of the rod stays the same $\\mathrm{{Tm}}$ here assuming that the rod is infinitely stiff), but that it would now \u201dappear shorter\u201d to the distant observer - i.e. it would no longer occupy the entire space between $\\mathbb{S}\\mathbb{S}\\mathrm{r\\_1}\\mathbb{S}\\mathbb{S}$ and $\\mathbb{S}\\mathbb{S}\\mathrm{r\\_2}\\mathbb{S}\\mathbb{S}$ . ", "page_idx": 32}, {"type": "text", "text": "Figure 13: Specific examples of selecting tokens during the selective pretraining process of the RHO-1. The tokens marked in blue represent the actual tokens trained during the training process, while the remaining black tokens are not trained during the training process. ", "page_idx": 32}, {"type": "text", "text": "After Training 0% Checkpoint ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Item Type: Journal Article Copyright of this article belongs to Elsevier. Division of Mechanical Sciences $>$ Mechanical Engineering 28 May 2007 19 Sep 2010 04:36 http://eprints.iisc.ernet.in/id/eprint/10277 # Question #8de97 $\\ln$ $\\mathfrak{m}$ Dec 10, 2016 $\\ln$ $\\mathfrak{m}$ That is not an identity. $\\lnot{\\mathrm{~n~}}\\ln$ #### Explanation: $\\textsf{i n}\\ln$ Recall that $\\ln\\,\\ln\\mathbb{S}\\{\\cot\\}\\,\\{2\\}$ $\\mathbf{x}+1=\\{\\backslash\\mathrm{csc}\\}^{\\wedge}\\{2\\}\\;\\mathrm{x}\\mathfrak{F}.$ $\\ln$ $\\ln$ So, we can write $\\ln\\,\\ln\\,\\mathrm{SWrac}\\{1-\\{\\mathrm{csc}\\}^{\\sim}\\{2\\}\\,\\mathrm{x}\\}\\{\\mathrm{Vesc}\\}\\sim2\\,\\mathrm{x}=\\mathrm{vac}\\{1-\\mathrm{left}(\\{\\mathrm{lcot}\\}^{\\sim}\\{2\\}\\,\\mathrm{x})\\}$ $+\\ \\mathrm{1\\backslashright)}\\}\\{\\mathrm{\\backslashcsc}\\}\\ \\mathrm{^{\\sim}}\\ 2\\ \\mathrm{x}\\mathfrak{F}\\ \\mathrm{{w}}$ $\\ln\\mathbb{S}{=}\\{\\backslash\\cot\\}{\\hat{2}}\\}$ \\frac{x}{\\csc} $\\mathbf{\\hat{\\mu}}_{2}\\mathbf{\\Phi}_{\\mathbf{X}}\\mathbb{S}\\ln\\mathbf{\\hat{\\mu}}_{\\mathbf{\\hat{\\mu}}}$ $\\ln$ Recall also that $\\mathbb{S}^{\\nu}$ \\cot $\\mathbf{X}=\\backslash\\mathbf{cos}$ \\frac $\\{\\mathbf{x}\\}\\{\\backslash\\mathrm{in}\\}$ $\\times\\mathbb{S}$ and $\\mathbb{S}\\mathrm{lcsc~}\\mathbf{x}=\\mathsf{V r a c}\\{1\\}\\,\\{\\backslash\\mathrm{sin}\\}\\;.$ x\\$. \\n $\\ln$ This allows us to continue \\n $\\ln\\mathbb{S}{=}\\mathrm{Vrac}\\{\\{\\cos\\}\\hat{\\{}}2\\}$ \\frac $\\{\\mathrm{x}\\}\\{\\mathrm{sin}\\}\\ ^{\\wedge}\\ 2$ $\\mathbf{x}\\}\\{{\\mathrm{Wrac}}\\{1\\}\\{{\\mathrm{Wsin}}\\}\\sim2\\,\\mathbf{x}\\}\\mathbb{S}\\,{\\mathrm{~in~}}\\,\\mathbf{\\ln}\\,\\mathbb{S}{=}\\{{\\mathrm{}}\\backslash\\mathbf{cos}\\}{\\mathrm{}}\\{2\\}$ \\frac{x}{\\sin} $\\hat{\\mathbf{\\xi}}_{2}\\mathbf{\\xi}_{\\mathbf{X}}$ \\cdot $(\\sin)\\setminus\\{2\\}\\,\\,{\\mathrm{trac}}\\{\\mathrm{x}\\}\\{1\\}\\mathbb{S}\\,\\mathrm{\\ln}\\,\\,\\mathrm{\\ln}\\,\\mathbb{S}{=}\\{\\cos\\}\\,\\{2\\}$ $\\mathbf{x}\\mathbb{S}\\ln\\ \\ln$ Which is not identically $\\mathbb{S}\\mathrm{|cos\\x\\mathbb{S}}$ $\\mathrm{x}\\mathbb{5}.\\mathrm{~}\\mathrm{~}\\mathrm{~in~}\\ln\\mathbb{N}\\left(\\mathbb{S}\\{\\cos\\}\\hat{\\mathbf{\\Omega}}\\{2\\}\\textbf{x}\\!=\\!\\operatorname{cos}\\mathrm{~}\\!\\mathrm{x}\\mathbb{S}$ only when \\$\\cos $\\mathbf{X}=1\\mathbb{S}$ or $\\mathbb{S}0\\mathbb{S}$ ) $\\ln\\,{\\mathrm{~w~}}$ Dec $10,2016\\ensuremath{\\,\\mathrm{\\backslashn\\,\\mathrm{~}}}\\ensuremath{\\mathrm{~\\:n~}}$ No. It is equal to $\\mathbb{S}\\{\\vert\\mathrm{sin}\\}^{\\wedge}\\{2\\}\\ \\mathbf{x}\\textrm{-}1\\mathbb{S}$ . $\\textsf{i n}\\ln$ #### Explanation: $\\textsf{i n}\\ln$ If we have $\\mathbb{S}\\mathrm{trac}\\{1-\\mathbf{X}\\}\\,\\{\\mathrm{x}\\}\\mathbb{S}$ , we can write it as \\$\\frac{1}{x} - \\frac{x}{x}\\$. $\\ln$ $\\ln$ The same way, $\\Im\\mathrm{trac}\\{\\mathrm{1-\\{\\mathrm{csc}\\}\\hat{\\Omega}\\{2\\}\\ x\\}\\{\\mathrm{\\{csc\\}\\hat{\\Omega}\\hat{\\Omega}}\\}$ can be written a $,\\mathrm{{SWIrac}\\{1\\}\\{\\backslash\\mathrm{{lcsc}\\}\\sim2\\mathrm{\\bf~x-}\\{\\mathrm{frac}\\{\\{\\backslash\\mathrm{{csc}\\}\\sim\\{2\\}\\mathrm{\\bf~x}\\}\\{\\{\\backslash\\mathrm{{csc}\\}\\sim\\{2\\}\\mathrm{\\bf~x}\\}\\}\\}}}}$ . $\\mathsf{i n}\\,\\mathsf{i n}$ This is equal to $\\mathbb{S}\\{\\vert\\mathrm{sin}\\}^{\\wedge}\\{2\\}\\ \\mathrm{x}-1\\mathfrak{F}$ . SMS scnews item created by Hannah Bryant at Wed 25 May 2022 1227 $\\ln$ Type: Seminar $\\ln$ Distribution: World $\\ln$ Expiry: 31 May $2022\\;\\mathrm{{un}}$ Calendar1: 31 May 202 $2\\ 1500\u20131600\\ \\mathrm{{un}}$ CalLoc1: Quad S224 & via $Z_{\\mathrm{OOm}}\\ln\\rightleftharpoons$ CalTitle1: SMRI \u2019What is ...a virtual knot?\u2019 Hans Boden (McMaster University) $\\ln$ Auth: hannahb $@$ w1d4n6z2.staff.sydney.edu.au (hbry8683) ", "page_idx": 33}, {"type": "text", "text": "After Training 33% Checkpoint ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Item Type: Journal Article Copyright of this article belongs to Elsevier. Division of Mechanical Sciences $>$ Mechanical Engineering 28 May 2007 19 Sep 2010 04:36 http://eprints.iisc.ernet.in/id/eprint/10277 # Question #8de97 $\\ln$ $\\mathfrak{m}$ Dec 10, 2016 $\\ln$ $\\mathfrak{m}$ That is not an identity. $\\lnot{\\mathrm{~n~}}\\ln$ #### Explanation: $\\ln$ $\\mathfrak{m}$ Recall that $\\ln\\,\\ln\\mathbb{S}\\{\\vert\\mathrm{cot}\\}^{\\star}\\{2\\}$ $\\mathbf{x}+1=\\{\\backslash\\mathrm{csc}\\}^{\\wedge}\\{2\\}$ x\\$. $\\ln$ $\\ln$ So, we can write $\\mathfrak{w}$ $\\mathsf{i n\\,\\&W r a c\\{1-\\{c s c\\}\\hat{\\!\\!\\sim}\\{2\\}\\,x\\}\\{\\:\\!\\{c s c\\}\\hat{\\!\\!\\sim}2\\;x=\\lvert\\mathrm{frac}\\{1-\\mathsf{l i e f t}(\\{\\ k c o t\\}\\hat{\\!\\!\\sim}\\{2\\}\\;\\mathbf{x})\\,x=0\\}\\,x=\\mathsf{f r a c}\\{1-\\mathsf{l i e f t}(\\{\\ k c o t\\}\\hat{\\!\\!\\sim}\\{2\\}\\;\\mathbf{x})\\}\\,x=0$ $+\\ {\\mathrm{1Wight}})\\}\\{\\mathrm{1csc}\\}\\sim2\\ {\\mathrm{x}}\\mathfrak{H}\\ \\backslash{\\mathrm{n}}\\ \\backslash{\\mathrm{=}}\\ \\{\\mathrm{\\mathrm{1cot}}\\}^{\\star}\\{2\\}$ \\frac{x}{\\csc} \u02c6 2 x\\$ \\n \\n Recall also that \\$\\cot $\\mathbf{\\boldsymbol{x}}=\\mathsf{\\backslash c o s}$ \\frac $\\{\\mathbf{X}\\}\\{\\backslash\\mathrm{in}\\}$ $\\times\\mathbb{S}$ and \\$\\csc $\\mathbf{X}=$ \\frac{1}{\\sin} $\\mathbf{X}\\mathbb{S}$ . $\\ln$ $\\ln$ This allows us to continue $\\ln$ $\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\mathbf{\\sigma}\\!\\!\\!\\!\\!\\!\\!\\setminus\\!\\!\\!\\!\\!\\!\\!\\!\\{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\setminus\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! $ \\frac $\\{\\mathrm{x}\\}\\{\\mathrm{sin}\\}\\ ^{\\wedge}\\ 2$ $\\mathbf{x}\\}\\{{\\mathrm{Wrac}}\\{1\\}{\\{\\mathrm{Wsin}\\}}\\sim2{\\mathrm{~x}}\\}\\mathbb{S}\\,{\\mathrm{~w~}}\\ln\\,\\mathbb{S}{=}\\{\\backslash{\\mathrm{cos}}\\}{\\hat{\\{2\\}}}$ \\frac $\\{{\\mathrm{x}}\\}\\{{\\mathrm{\\backslashsin}}\\}\\,\\hat{\\mathrm{~}}^{\\prime}\\,2{\\mathrm{~x~}}{\\mathrm{\\backslashcdot}}$ $\\backslash\\mathrm{cdot}\\ \\{\\sin\\}^{\\cdot}\\{2\\}\\,\\backslash\\mathrm{{trac}\\{x\\}\\{1\\}\\&\\ i n\\ \\backslash n\\ \\mathbb{S}}{=}\\{\\cos\\}^{\\cdot}\\{2\\}$ $\\mathbf{x}\\mathbb{S}\\ln\\ \\ln$ Which is not identically $\\mathbb{S}|\\mathrm{cos~x}\\mathbb{S}$ . \\n $\\mathfrak{m}$ $\\mathbf{l}\\left(\\mathbb{S}\\{\\cos\\}\\hat{\\mathbf{\\mu}}^{2}\\right\\{2\\}$ $\\mathbf{x}=\\cos\\mathbf{x}\\mathbb{S}$ only when \\$\\cos $\\mathbf{X}=1\\mathbb{S}$ or $\\mathbb{S}0\\mathbb{S})$ ) $\\ln{\\,\\mathrm{\\ln}}$ Dec 10, $2016\\;\\mathrm{{u}}\\mathrm{{n}}$ $\\ln$ No. It is equal to $\\mathbb{S}\\{\\mathrm{\\backslash\\sin}\\}^{\\sim}\\{2\\}\\mathrm{~\\bf{x}~-~}1\\mathbb{S}$ . \\n $\\ln$ #### Explanation: $\\textsf{i n}\\,\\ln$ If we have \\$\\fra $\\operatorname{\\mathcal{C}}\\left\\{\\mathbf{1}\\,-\\,\\mathbf{x}\\right\\}\\left\\{\\operatorname{\\mathbb{X}}\\right\\}\\mathbb{S}$ , we can write it a $;\\,\\Re\\mathrm{Frac}\\{1\\}\\,\\{\\mathrm{x}\\}-\\backslash\\mathrm{frac}\\{\\mathrm{x}\\}\\{\\mathrm{x}\\}\\mathfrak{F}.$ $\\textsf{i n}\\ln$ The same way, $\\mathbb{S}\\backslash\\mathrm{frac}\\{1-\\{\\mathrm{vcsc}\\}\\cap2\\}\\ \\mathrm{x}\\}\\{\\mathrm{vcsc}\\}\\wedge2\\mathbb{S}$ can be written as $\\mathbb{S}\\mathrm{{Hrac}}\\{1\\}\\{\\mathrm{{[scsc\\}\\sim2\\mathrm{\\,x-}\\mathrm{{Hrac}}\\{\\{\\mathrm{{[csc]}^{\\sim}\\{2\\}\\,\\mathrm{{x}}\\}\\{\\{\\mathrm{[csc]}^{\\sim}\\{2\\}\\,\\mathrm{{x}}\\}\\}}}}.$ $\\mathsf{i n}\\,\\mathsf{i n}$ This is equal to $\\mathbb{S}\\{\\vert\\mathrm{sin}\\}^{\\wedge}\\{2\\}\\mathrm{~x~-~}1\\mathfrak{F}$ . SMS scnews item created by Hannah Bryant at Wed 25 May 2022 1227 $\\ln$ Type: Seminar $\\ln$ Distribution: World $\\ln$ Expiry: 31 May $2022\\;\\mathrm{{un}}$ Calendar1: 31 May 2022 1500-1600 \\n CalLoc1: Quad S224 & via $Z_{\\mathrm{OOm}}\\ln\\rightleftharpoons$ CalTitle1: SMRI \u2019What is ...a virtual knot?\u2019 Hans Boden (McMaster University) $\\ln$ Auth: hannahb $@$ w1d4n6z2.staff.sydney.edu.au (hbry8683) ", "page_idx": 33}, {"type": "text", "text": "After Training 66% Checkpoint ", "page_idx": 33}, {"type": "text", "text": "Item Type: Journal Article Copyright of this article belongs to Elsevier. Division of Mechanical Sciences $>$ Mechanical Engineering 28 May 2007 19 Sep 2010 04:36 http://eprints.iisc.ernet.in/id/eprint/10277 # Question #8de97 $\\ln$ \\n Dec 10, 2016 $\\ln$ \\n That is not an identity. \\n $\\ln$ #### Explanation: $\\textsf{i n}\\ln$ Recall that \\ ${\\mathrm{~n~}}\\backslash{\\mathfrak{n}}\\mathbb{S}\\{\\backslash\\cot\\}^{\\star}\\{2\\}$ $\\mathbf{x}+\\mathbf{\\phi}1=\\{\\backslash\\mathrm{csc}\\}^{\\wedge}\\{2\\}\\ x\\mathfrak{F}.$ $\\ln$ $\\ln$ So, we can write $\\ln\\,\\ln\\,\\mathrm{\\SWrac}\\{1-\\{\\mathrm{\\Scsc}\\}^{\\star}\\{2\\}\\,\\,\\mathrm{x}\\}\\{\\mathrm{kcsc}\\}\\sim2\\,\\,\\mathrm{x}\\,=\\operatorname{sfrac}\\{1-\\mathrm{keft}(\\{\\mathrm{\\vec{c}o t}\\}^{\\star}\\{2\\}\\,\\mathrm{x}\\,=\\,0)\\}.$ $+\\ {\\mathrm{1Wight}})\\}\\{\\mathrm{1csc}\\}\\ \\sim2\\ {\\mathrm{x}}\\mathfrak{H}\\ \\backslash{\\mathrm{n}}\\ \\backslash{\\mathrm{n}}\\ \\mathfrak{S}=\\{\\mathrm{1cot}\\}\\,\\{2\\}$ \\frac{x}{\\csc} \u02c6 2 x\\$ \\n \\n Recall also that \\$\\cot $\\mathbf{\\boldsymbol{x}}=\\mathsf{\\backslash c o s}$ \\frac $\\{\\mathbf{X}\\}\\{\\backslash\\mathrm{in}\\}$ $\\times\\mathbb{S}$ and \\$\\csc $\\mathbf{x}={\\mathrm{{\\backslashfrac\\{1\\}\\,\\{\\backslash s i n\\}\\,\\,}x\\mathrm{{\\:}}}}$ . \\n $\\ln$ This allows us to continue $\\backslash{\\bf n}\\;\\;\\backslash{\\bf n}\\;\\mathbb{S}{=}\\backslash\\mathrm{frac}\\{\\{\\cos\\}\\hat{\\bf\\Phi}\\{2\\}$ \\frac $\\{\\mathrm{x}\\}\\{\\mathrm{sin}\\}\\ ^{\\sim}2$ $\\mathbf{x}\\}\\{\\mathrm{Wrac}\\{1\\}\\{\\mathrm{sin}\\}\\sim2\\,\\mathrm{x}\\}\\mathbb{S}\\,\\mathrm{w}\\ \\ln\\,\\mathbb{S}{=}\\{\\mathrm{\\backslashcos}\\}^{\\sim}\\{2\\}$ \\frac $\\{{\\mathrm{x}}\\}\\{{\\mathrm{\\backslashsin}}\\}\\ {\\mathrm{^{\\star}}}2{\\mathrm{\\scriptscriptstylex}}\\,{\\mathrm{\\backslashcdot}}$ $\\{\\dot{\\mathrm{\\sin}}\\}^{\\star}\\{2\\}$ $\\operatorname{\\sf{Wrac}}\\{\\operatorname{x}\\}\\{1\\}\\mathfrak{F}\\ln\\,\\operatorname{\\sf{W}}\\mathfrak{F}{=}\\{\\operatorname{cos}\\}^{\\wedge}\\{2\\}$ $\\mathbf{x}\\mathbb{S}\\ln\\ \\ln$ Which is not identically $\\mathbb{S}\\mathrm{kcos~x}\\mathbb{S}$ . \\n $\\mathfrak{m}$ $\\mathbf{1}\\left(\\mathbb{S}\\{\\cos\\}\\hat{\\mathbf{\\mu}}^{2}\\right\\}$ $\\mathbf{x}=\\cos\\mathbf{x}\\mathbb{S}$ only when \\$\\cos $\\mathbf{X}=1\\mathbb{S}$ or $\\Phi0\\mathbb{S}$ ) $\\ln{\\,\\mathrm{\\ln}}$ Dec 10, 2016 \\n $\\ln$ No. It is equal to $\\mathbb{S}\\{\\vert\\sin\\}^{\\wedge}\\{2\\}\\ X-1\\mathbb{S}$ . $\\ln$ $\\ln$ #### Explanation: $\\textsf{i n}\\,\\ln$ If we have $\\mathbb{S}\\mathrm{{trac}}\\{1-{\\bf X}\\}\\{\\mathrm{{x}}\\}\\mathbb{S},$ we can write it a $\\mathbf{\\hat{s}}\\,\\mathbf{\\hat{W}}\\mathrm{rac}\\{\\mathbf{1}\\}\\,\\{\\mathbf{x}\\}-\\mathrm{vac}\\{\\mathrm{x}\\}\\{\\mathrm{x}\\}\\,\\mathbf{\\hat{\\mathbb{S}}}.\\,\\mathbf{\\hat{w}}\\,\\mathbf{\\hat{\\mathbb{~n}}}\\,\\mathbf{\\hat{\\mathbb{T}}}$ he same way, $\\mathbb{S}$ $\\mathrm{frac}\\{\\mathrm{1-\\{\\backslashcsc\\}^{\\sim}\\{2\\}\\ x\\}\\{\\backslash\\mathrm{csc}\\}\\sim2\\mathbb{S}$ can be written a $\\{{\\}\\}\\{\\mathrm{rac}\\{1\\}\\{\\backslash\\mathrm{csc}\\}\\sim2\\mathrm{\\bf~x}-\\{\\mathrm{frac}\\{\\{\\backslash\\mathrm{csc}\\}^{\\sim}\\{2\\}\\mathrm{\\bf~x}\\}\\{\\{\\backslash\\mathrm{csc}\\}^{\\sim}\\{2\\}\\mathrm{\\bf~x}\\}\\}$ . $\\mathsf{i n}\\,\\mathsf{i n}$ This is equal to $\\mathbb{S}\\{\\vert\\mathrm{sin}\\}^{\\wedge}\\{2\\}\\ \\mathbf{x}-1\\mathbb{S}$ . SMS scnews item created by Hannah Bryant at Wed $25$ May 2022 1227 $\\ln$ Type: Seminar $\\ln$ Distribution: World $\\ln$ Expiry: 31 May $2022\\;\\mathrm{m}$ Calendar1: 31 May 2022 1500-1600 \\n CalLoc1: Quad S224 & via $Z_{\\mathrm{OOm}}\\ln\\rightleftharpoons$ CalTitle1: SMRI \u2019What is ...a virtual knot?\u2019 Hans Boden (McMaster University) $\\ln$ Auth: hannahb@w1d4n6z2.staff.sydney.edu.au (hbry8683) ", "page_idx": 33}, {"type": "text", "text": "After Training 100% Checkpoint ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Item Type: Journal Article Copyright of this article belongs to Elsevier. Division of Mechanical Sciences $>$ Mechanical Engineering 28 May 2007 19 Sep 2010 04:36 http://eprints.iisc.ernet.in/id/eprint/10277 # Question #8de97 $\\ln$ $\\mathfrak{m}$ Dec 10, 2016 $\\lnot{\\mathrm{~n~}}\\ln$ That is not an identity. $\\ln{\\,\\mathrm{\\ln}}$ #### Explanation: $\\textsf{i n}\\ln$ Recall that $\\ln\\,\\ln\\mathbb{S}\\{\\vert\\cot\\}^{\\wedge}\\{2\\}$ $\\mathbf{x}+\\mathbf{\\mathbb{1}}=\\{\\backslash\\mathrm{csc}\\}^{\\star}\\{2\\}~\\mathrm{x}$ \\$. $\\ln$ $\\ln$ So, we can write $\\ln\\ \\ln\\ \\mathrm{{Sifrac}\\{1-\\{\\ x c s c\\}^{\\sim}\\{2\\}\\ x\\}\\{\\ x c s c\\}\\sim2\\ x=\\forall{\\mathrm{frac}}\\{1-\\mathrm{{Sieft}(\\{\\ x c o t\\}^{\\sim}\\{2\\}\\ x)\\}\\}.$ $+\\ {\\mathrm{1wight}})\\}\\{\\mathrm{1csc}\\}\\ \\sim2\\ {\\mathrm{x}}\\mathfrak{P}\\ \\mathfrak{m}\\ \\mathrm{\\wp}=\\{\\mathrm{1cot}\\}\\,\\{2\\}$ \\frac{x}{\\csc} \u02c6 2 x\\$ \\n \\n Recall also that $\\mathbb{S}^{\\nu}$ \\cot x = \\cos \\frac $\\{\\mathbf{X}\\}\\{\\backslash\\mathrm{in}\\}$ $\\times\\mathbb{S}$ and $\\mathbb{S}\\backslash\\mathbf{c}\\mathrm{sc}\\;\\mathbf{x}=\\mathbb{H}\\mathrm{rac}\\{1\\}\\,\\{\\backslash\\mathrm{sin}\\}$ x\\$. \\n \\n This allows us to continue \\n $\\ln\\mathbb{S}{=}\\mathrm{Vrac}\\{\\{\\cos\\}^{\\star}\\{2\\}$ \\frac $\\{\\mathrm{x}\\}\\{\\mathrm{sin}\\}\\ ^{\\sim}2$ $\\mathbf{x}\\}\\{{\\mathrm{Wrac}}\\{1\\}\\{{\\mathrm{(sin)}}\\hat{\\mathbf{\\alpha}}^{\\sim}2{\\mathrm{~x}}\\}{\\mathbb{S}}\\ {\\mathrm{in}}\\ {\\mathrm{~w}}\\mathbb{S}{=}\\{\\backslash{\\mathrm{cos}}\\}\\hat{\\mathbf{\\alpha}}\\{2\\}$ \\frac $:\\{\\mathrm{{x}\\}\\}\\{\\backslash\\mathrm{{sin}}\\}\\ ^{\\wedge}\\ 2\\ \\mathrm{{x}}\\,\\backslash\\mathrm{{cdot}}$ $\\{\\dot{\\mathrm{\\sin}}\\}^{\\star}\\{2\\}$ \\fra $\\{\\,{\\mathrm{x}}\\,\\}\\,\\{1\\}\\mathbb{S}\\mid\\!{\\mathrm{n}}\\,\\,\\backslash\\!{\\mathrm{n}}\\,\\,\\mathbb{S}\\!=\\{\\!\\,\\!\\backslash\\!{\\mathrm{cos}}\\,\\}\\,\\{2\\}$ $\\mathbf{x}\\mathbb{S}\\ln\\ \\ln$ Which is not identically \\$\\cos $\\mathrm{x}\\mathbb{S}$ . \\n $\\ln{(\\mathbb{S}\\{\\cos\\}^{\\wedge}\\{2\\}\\,\\,{\\mathbf x}=\\cos\\,{\\mathbf x}\\mathbb{S}}$ only when \\$\\cos $\\mathbf{X}=1\\mathbb{S}$ or $\\Phi0\\mathbb{S}$ ) \\n \\n Dec 10, 2016 \\n $\\ln$ No. It is equal to $\\mathbb{S}\\{\\mathsf{i n}\\}^{\\star}\\{2\\}$ $\\mathbf{X}\\mathrm{~-~}1\\mathbb{S}$ . $\\ln$ $\\ln$ #### Explanation: $\\textsf{i n}\\,\\ln$ If we have $\\mathbb{S}\\mathrm{{trac}}\\{1-\\mathbf{x}\\}\\left\\{\\mathbf{x}\\right\\}\\mathbb{S},$ we can write it a ${\\bf s}\\,\\Re\\mathrm{frac}\\{1\\}\\,\\{{\\bf x}\\}-\\mathrm{Vrac}\\{{\\bf x}\\}\\,\\{{\\bf x}\\}\\mathbb{S}.\\;\\backslash{\\bf n}$ \\n The same way, $\\{{\\bf r a c}\\{\\mathrm{1-\\{\\langle\\vsc\\rangle\\hat{\\phi}\\{2\\}\\ x}\\}\\{\\langle\\mathrm{\\bar{x}s c}\\}\\hat{\\phi}\\rangle}$ can be written as $\\Im\\mathrm{trac}\\{1\\}\\{\\mathrm{[csc\\}\\sim2\\mathrm{\\bf~x}-\\mathrm{\\brac}\\{\\{\\mathrm{}c\\mathrm{sc}\\}^{\\sim}\\{2\\}\\mathrm{\\bf~x}\\}\\{\\{\\mathrm{\\mathscr{V}}\\mathrm{csc}\\}^{\\sim}\\{2\\}\\mathrm{\\bf~x}\\}\\{\\{\\mathrm{\\mathscr{V}}\\mathrm{csc}\\}^{\\sim}\\{2\\}\\mathrm{\\bf~x}\\}\\S$ . $\\ln$ $\\ln$ This is equal to $\\mathbb{S}\\{\\vert\\mathbf{sin}\\}^{\\wedge}\\{2\\}\\ \\mathbf{x}-1\\mathbb{S}$ . SMS scnews item created by Hannah Bryant at Wed 25 May 2022 1227 $\\ln$ Type: Seminar $\\ln$ Distribution: World $\\ln$ Expiry: 31 May $2022\\;\\mathrm{m}$ Calendar1: 31 May 2022 1500-1600 \\n CalLoc1: Quad S224 & via $Z_{\\mathrm{OOm}}\\ln\\rightleftharpoons$ CalTitle1: SMRI \u2019What is ...a virtual knot?\u2019 Hans Boden (McMaster University) $\\ln$ Auth: hannahb $@$ w1d4n6z2.staff.sydney.edu.au (hbry8683) ", "page_idx": 33}, {"type": "text", "text": "Figure 14: An example of dynamic token selection changes during the training process, which illustrated with five different score levels represented by deep blue, light blue, black, light orange, and dark orange. The bluer the color indicates a higher tendency for the token to be selected, while the more orange the color suggests a lower tendency for the token to be selected. ", "page_idx": 33}]