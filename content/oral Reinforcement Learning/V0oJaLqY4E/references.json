{"references": [{"fullname_first_author": "Chelsea Finn", "paper_title": "A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models", "publication_date": "2016-11-16", "reason": "This paper establishes a crucial theoretical link between generative models, inverse reinforcement learning, and energy-based models, providing a foundation for the current work's approach."}, {"fullname_first_author": "Jonathan Ho", "paper_title": "Denoising diffusion probabilistic models", "publication_date": "2020-00-00", "reason": "This is a seminal paper introducing denoising diffusion probabilistic models, the core generative model that the current work builds upon and improves."}, {"fullname_first_author": "Brian D Ziebart", "paper_title": "Maximum entropy inverse reinforcement learning", "publication_date": "2008-00-00", "reason": "This paper introduces the maximum entropy inverse reinforcement learning framework, a key theoretical basis for the current work's method of learning rewards from expert demonstrations."}, {"fullname_first_author": "Geoffrey E Hinton", "paper_title": "Training products of experts by minimizing contrastive divergence", "publication_date": "2002-00-00", "reason": "This paper introduces contrastive divergence, a fundamental method used in training energy-based models, which are central to the current work's approach."}, {"fullname_first_author": "Andrew Y Ng", "paper_title": "Algorithms for inverse reinforcement learning", "publication_date": "2000-00-00", "reason": "This paper is a foundational work on inverse reinforcement learning, the theoretical basis of the current work's algorithm for learning a reward function from expert trajectories."}]}