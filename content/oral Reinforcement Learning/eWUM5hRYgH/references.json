{"references": [{"fullname_first_author": "M. G. Bellemare", "paper_title": "A distributional perspective on reinforcement learning", "publication_date": "2017-00-00", "reason": "This paper introduces the distributional reinforcement learning framework, which is fundamental to the research presented in this paper."}, {"fullname_first_author": "W. Dabney", "paper_title": "Distributional reinforcement learning with quantile regression", "publication_date": "2018-00-00", "reason": "This paper is cited for its introduction of quantile regression for distributional reinforcement learning, a key technique used in the current work."}, {"fullname_first_author": "M. Rowland", "paper_title": "An analysis of categorical distributional reinforcement learning", "publication_date": "2018-00-00", "reason": "This paper provides the asymptotic convergence analysis of categorical distributional temporal difference learning (CTD), which the current work extends to non-asymptotic analysis."}, {"fullname_first_author": "G. Li", "paper_title": "Is q-learning minimax optimal?", "publication_date": "2024-00-00", "reason": "This paper is referenced for its analysis of the classic temporal difference learning algorithm's statistical efficiency, which this paper aims to match with distributional TD."}, {"fullname_first_author": "M. Rowland", "paper_title": "An analysis of quantile temporal-difference learning", "publication_date": "2024-00-00", "reason": "This paper shows asymptotic convergence of quantile temporal difference learning (QTD), another key algorithm in the field, and provides a comparison point for the current work."}]}