[{"figure_path": "pGEY8JQ3qx/figures/figures_7_1.jpg", "caption": "Figure 1: A general MDP where \u03b3-discounted approximation fails unless \u03b3 = \u03a9(T) \u226b ||h*||span", "description": "This figure presents a general Markov Decision Process (MDP) with three states and one transient state, where the discounted approximation method fails unless the discount factor \u03b3 is greater than or equal to \u03a9(T), which is significantly larger than the span of the optimal bias function (||h*||span).  The MDP includes a transient state (state 1) with two actions. Action 1 leads to an immediate reward of 1 and then transitions to the absorbing state 3 with reward 0, while action 2 transitions to the absorbing state 2 with a reward of 0.5.  The parameter T controls the expected number of steps before leaving the transient state.  The figure illustrates that the long-term average reward depends on this transient behavior, making the discounted approximation (which focuses on short-term rewards) ineffective unless \u03b3 is sufficiently large to account for this long-term behavior.", "section": "4 Main results for general MDPs"}, {"figure_path": "pGEY8JQ3qx/figures/figures_21_1.jpg", "caption": "Figure 1: A general MDP where \u03b3-discounted approximation fails unless \u03b3 = \u03a9(T) \u226b ||h*||span", "description": "This figure shows two Markov Decision Processes (MDPs). In both MDPs, there are four states and the optimal bias function has a span of 0.  The top MDP, M0, has a transition probability of 1/T from state 1 to state 3. The bottom MDP, M1, has a transition probability of (1+2\u03b5)/(2T) from state 1 to state 3.  The parameter T controls the expected time before the MDP reaches either a state that gives a reward of 0 or 0.5.  This parameter illustrates that the discounted approximation approach for average reward MDPs may fail unless the discount factor is set sufficiently close to 1.  This is because transient states (states that are not visited in the long run) can have a significant impact on the long-run performance of the MDP. ", "section": "Main results for general MDPs"}, {"figure_path": "pGEY8JQ3qx/figures/figures_41_1.jpg", "caption": "Figure 3: MDP Instances Used in the Proof of Lower Bound in Theorem 4", "description": "This figure presents the Markov Decision Process (MDP) instances used in the proof of the lower bound presented in Theorem 4.  The MDPs, denoted as M1 and Ma* (where a* is an index from 2 to A), are used to demonstrate the statistical hardness of distinguishing between MDPs with different spans of the optimal bias function.  The key differences lie in the transition probabilities and reward values associated with the actions taken from state 1, with the goal being to highlight the challenges in identifying the optimal policy and achieving optimal sample complexity without full knowledge of the MDP's structural properties, like the span of the optimal bias function. The transient time parameter 'B' also plays a significant role in the complexity analysis illustrated by these instances.", "section": "B.4 Proof of Theorems 4 and 5 (Lower Bounds)"}, {"figure_path": "pGEY8JQ3qx/figures/figures_41_2.jpg", "caption": "Figure 3: MDP Instances Used in the Proof of Lower Bound in Theorem 4", "description": "This figure shows the Markov Decision Processes (MDPs) used to prove the lower bound in Theorem 4 of the paper.  The MDPs, denoted as M1 and Ma* (where a* is an action), illustrate a scenario with a transient state (state 1) and multiple absorbing states (states 2, 3, and 4).  The key difference between M1 and Ma* lies in the transition probabilities and rewards associated with actions from state 1.  This subtle difference in structure is carefully crafted to demonstrate the inherent difficulty in learning near-optimal policies in general average-reward MDPs with limited samples. The complexity of distinguishing between these MDPs based on observed transitions forms the basis of the lower bound.", "section": "B.4 Proof of Theorems 4 and 5 (Lower Bounds)"}]