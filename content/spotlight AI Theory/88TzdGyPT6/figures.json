[{"figure_path": "88TzdGyPT6/figures/figures_40_1.jpg", "caption": "Figure 1: Generalization error of a two-layer leaky ReLU network trained to 0 hinge loss varying n and d. Parameter settings: a = 0.1, \u03b3 = 5/n, m = 64, k = 0.1n, number of trials = 5, size of validation sample = 1000.", "description": "This figure shows the generalization error of a two-layer leaky ReLU network as a function of the ratio of input dimension to sample size (d/n) and sample size (n).  The network was trained using gradient descent on the hinge loss until the training loss reached zero. The color of each cell represents the generalization error, with darker colors indicating lower error. The figure demonstrates that for a fixed d/n ratio, the generalization error decreases as n increases, and for a fixed n, the generalization error decreases as d/n increases. This behavior is consistent with the theoretical findings of benign overfitting described in the paper, showing that with sufficiently large n and d/n, even with noisy training data, a low generalization error can be achieved.", "section": "Appendix F Experiments"}, {"figure_path": "88TzdGyPT6/figures/figures_41_1.jpg", "caption": "Figure 2: Generalization error of a two-layer leaky ReLU network trained to 0 hinge loss varying \u03b3 and n. Parameter settings: \u03b1 = 0.1, d = 2n, m = 64, k = 0.1n, number of trials = 10, size of validation sample = 1000.", "description": "The figure shows the generalization error of a two-layer leaky ReLU network as a function of \u03b3 (signal strength) and n (number of data points).  The network was trained to zero hinge loss, with other parameters held constant (\u03b1 = 0.1, d = 2n, m = 64, k = 0.1n).  Multiple lines represent different values of n.  The plot demonstrates how the generalization error decreases as \u03b3 increases, indicating that better generalization is achieved when the signal-to-noise ratio in the data is higher.  It also shows that the required \u03b3 for good generalization decreases as n increases, showing the effect of increasing data size on the network's robustness to noise.", "section": "Appendix F Experiments"}]