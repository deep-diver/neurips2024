[{"type": "text", "text": "Benign overfitting in leaky ReLU networks with moderate input dimension ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Kedar Karhadkar1\u2217 Erin George1\u2217 Michael Murray1 Guido Mont\u00fafar12 Deanna Needell1 {kedar,egeo,mmurray,montufar,deanna}@math.ucla.edu 1UCLA 2Max Planck Institute for Mathematics in the Sciences \u2217Equal contribution ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The problem of benign overftiting asks whether it is possible for a model to perfectly fti noisy training data and still generalize well. We study benign overftiting in twolayer leaky ReLU networks trained with the hinge loss on a binary classification task. We consider input data that can be decomposed into the sum of a common signal and a random noise component, that lie on subspaces orthogonal to one another. We characterize conditions on the signal to noise ratio (SNR) of the model parameters giving rise to benign versus non-benign (or harmful) overfitting: in particular, if the SNR is high then benign overfitting occurs, conversely if the SNR is low then harmful overfitting occurs. We attribute both benign and nonbenign overftiting to an approximate margin maximization property and show that leaky ReLU networks trained on hinge loss with gradient descent (GD) satisfy this property. In contrast to prior work we do not require the training data to be nearly orthogonal. Notably, for input dimension $d$ and training sample size $n$ , while results in prior work require $d={\\dot{\\Omega}}(n^{2}\\log n)$ , here we require only $d=\\Omega\\left(n\\right)$ . ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Intuition from learning theory suggests that fitting noise during training reduces a model\u2019s performance on test data. However, it has been observed in some settings that machine learning models can interpolate noisy training data with only nominal cost to their generalization performance (Zhang et al., 2017; Belkin et al., 2018, 2019), a phenomenon referred to as benign overftiting. Establishing theory that can explain this phenomenon has attracted much interest in recent years and there is now a rich body of work on this topic particularly in the context of linear models. However, the study of benign overfitting in the context of non-linear models, in particular shallow ReLU or leaky ReLU networks, has additional technical challenges and subsequently is less well advanced. ", "page_idx": 0}, {"type": "text", "text": "Much of the effort in regard to theoretically characterizing benign overfitting focuses on showing, under an appropriate scaling of the dimension of the input domain $d$ , size of the training sample $n$ , number of corruptions $k$ and number of model parameters $p$ that a model can interpolate noisy training data while achieving an arbitrarily small generalization error. Such characterizations of benign overfitting position it as a high dimensional phenomenon1: indeed, the decrease in generalization error is achieved by escaping to higher dimensions at some rate relative to the other aforementioned hyperparameters. However, for these mathematical results to be relevant for explaining benign overftiting as observed in practice, clearly the particular scaling of $d$ with respect to $n,k$ and $p$ needs to reflect the ratios seen in practice. Although a number of works, which we discuss in Section 1.2, establish benign overfitting results for shallow neural networks, a key and significant limitation they share is the requirement that the input features of the training data are at least approximately orthogonal to one another. To study benign overftiting, these prior works typically assume the input features consist of a small, low-rank signal component plus an isotropic noise term. Therefore, for the near orthogonality property to hold with high probability it is required that the input dimension $d$ scales as $d={\\bar{\\Omega}}(n^{2}\\,\\bar{\\log{n}})$ or higher. This assumption highly restricts the applicability of these results for explaining benign overfitting in practice. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this work we assume only $d=\\Omega(n)$ and establish both harmful and benign overftiting results for shallow leaky ReLU networks trained via gradient descent (GD) on the hinge loss. In particular, we consider $n$ data point pairs $(\\pmb{x}_{i},y_{i})\\in\\mathbb{R}^{d}\\stackrel{<}{\\times}\\{\\pm1\\}$ , where, for some vector $\\bar{\\pmb{v}}\\in\\mathbb{S}^{d-1}$ \u221aand scalar $\\gamma\\in$ $[0,1]$ , the input features are drawn from a pair of Gaussian clusters $\\begin{array}{r}{\\pmb{x}_{i}\\sim\\mathcal{N}(\\pmb{\\pmb{\\pmb{\\cdot}}})\\pmb{v},\\sqrt{1-\\gamma}\\frac{1}{d}(\\mathbf{I}_{d}^{\\phantom{\\dagger}}-\\mathbf{\\nabla})\\pmb{\\Psi}_{i}}\\end{array}$ ${\\pmb v}{\\pmb v}^{T})$ ) and $y_{i}=\\mathrm{sign}(\\langle\\mathbb{E}[\\pmb{x}_{i}],\\pmb{v}\\rangle)$ . The training data is noisy in that $k$ of the $n$ points in the training sample have their output label flipped. We assume equal numbers of positive and negative points among clean and corrupt ones. We provide a full description of our setup and assumptions in Section 2. Our proof techniques are novel and identify a new condition allowing for the analysis of benign and harmful overfitting which we term approximate margin maximization, wherein the norm of the network parameters is upper bounded by a constant of the norm of the max-margin linear classifier. ", "page_idx": 1}, {"type": "text", "text": "1.1 Summary of contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Our key results are summarized as follows. ", "page_idx": 1}, {"type": "text", "text": "\u2022 In Theorem 3.1, we prove that a leaky ReLU network trained on linearly separable data with gradient descent and the hinge loss will attain zero training loss in finitely many iterations. Moreover, the network weight matrix $W$ at convergence will be approximately max-margin in the sense that \u2225W \u2225= O \u2225\u03b1w\u221am\u2225 , where $\\alpha$ is the leaky parameter of the activation function, $m$ is the width of the network, and $\\pmb{w}^{*}$ is the max-margin linear classifier. We apply this result to derive generalization bounds for the network on test data.   \n\u2022 In Theorem 3.2, we establish conditions under which benign overfitting occurs for leaky ReLU networks. If the input dimension $d$ , number of training points $n$ , number of corrupt points $k$ , and signal strength $\\gamma$ satisfy $d\\,=\\,\\Omega(n)$ and $\\gamma\\,=\\,\\bar{\\Omega}({\\textstyle{\\frac{1}{k}}})$ , then the network will exhibit benign overfitting. We emphasize that existing works on benign overfitting require $d=\\Omega(n^{2}\\,\\bar{\\log{n}})$ to ensure nearly orthogonal data.   \n\u2022 In Theorem 3.3, we find a complementary lower bound for the generalization error to show that, for gradient descent classifiers, the bound in Theorem 3.2 is tight up to a constant in the exponent that can depend on $\\alpha$ .   \n\u2022 In Theorem 3.4, we find conditions under which non-benign overftiting occurs. If $d=\\Omega(n)$ and $\\gamma\\,=\\,O(\\frac{1}{d})$ , then the network will exhibit non-benign overfitting: in particular its generalization error will be at least $\\frac18$ . ", "page_idx": 1}, {"type": "text", "text": "1.2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "There is now a significant body of literature theoretically characterizing benign overfitting in the context of linear models, including linear regression (Bartlett et al., 2020; Muthukumar et al., 2020; Wu & Xu, 2020; Zou et al., 2021; Hastie et al., 2022; Koehler et al., 2021; Wang et al., 2021a; Chatterji & Long, 2022; Shamir, 2022), logistic regression (Chatterji & Long, 2021; Muthukumar et al., 2021; Wang et al., 2021b), max-margin classification with linear and random feature models (Montanari et al., 2023b,a; Mei & Montanari, 2022; Cao et al., 2021) and kernel regression (Liang & Rakhlin, 2020; Liang et al., 2020; Adlam & Pennington, 2020). However, the study of benign overfitting in non-linear models is more nascent. ", "page_idx": 1}, {"type": "text", "text": "Homogeneous networks trained with gradient descent and an exponentially tailed loss are known to converge in direction to a Karush-Kuhn-Tucker (KKT) point of the associated max-margin problem (Lyu & Li, 2020; Ji & Telgarsky, 2020)2. This property has been widely used in prior works to prove benign overfitting results for shallow neural networks. Frei et al. (2022) consider a shallow, smooth leaky ReLU network trained with an exponentially tailed loss and assume the data is drawn from a mixture of well-separated sub-Gaussian distributions. A key result of this work is, given sufficient iterations of GD, that the network will interpolate noisy training data while also achieving minimax optimal generalization error up to constants in the exponents. Xu & Gu (2023) extend this result to more general activation functions, including ReLU, as well as relax the assumptions on the noise distribution to being centered with bounded logarithmic Sobolev constant, and finally also improve the convergence rate. George et al. (2023) also study ReLU as opposed to leaky ReLU networks but do so in the context of the hinge loss, for which, and unlike exponentially tailed losses, a characterization of the implicit bias is not known. This work also establishes transitions on the margin of the clean data driving harmful, benign and no-overfitting training outcomes. Frei et al. (2023) use the aforementioned implicit bias of GD for linear classifiers and shallow leaky ReLU networks towards solutions that satisfy the KKT conditions of the margin maximization problem to establish settings where the satisfaction of said KKT conditions implies benign overftiting. Kornowski et al. (2023) also use the implicit bias results for exponentially tailed losses to derive similar benign overfitting results for shallow ReLU networks. Cao et al. (2022); Kou et al. (2023) study benign overftiting in two-layer convolutional as opposed to feedforward neural networks: indeed, whereas in most prior works data is modeled as the sum of a signal and noise component, in these two works the signal and noise components are assumed to lie in disjoint patches. The weight vector of each neuron is applied to both patches separately and a non-linearity, such as ReLU, is applied to the resulting pre-activation. In this setting, the authors prove interpolation of the noisy training data and derive conditions on the clean margin under which the network benignly versus harmfully overftis. A follow up work (Chen et al., 2023) considers the impact of Sharpness Aware Minimization (SAM) in the same setting. Finally, and assuming $d=\\Omega(n^{\\bar{5}})$ , Xu et al. (2024) establish benign overfitting results for a data distribution which, instead of being linearly separable, is separated according to an XOR function. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "We emphasize that the prior work on benign overfitting in the context of shallow neural networks requires the input data to be approximately orthogonal. Under standard data models studied this equates to the requirement that the input dimension $d$ versus the size of the training sample $n$ satisfies $\\dot{d}=\\Omega(n^{2}\\log n)$ or higher. Here we require only $d=\\Omega(n)$ . The weaker dimensionality requirement requires substantially different proof techniques. George et al. (2023) study a setting most similar to the one studied here, however, the techniques are very different. In particular, the results presented in this other work are derived by carefully tracking neuron activation patterns. While in high dimensions this is feasible due to the near orthogonality of the noise in low dimensions this is far more challenging as noise vectors can be highly correlated leading to coupling effects. ", "page_idx": 2}, {"type": "text", "text": "Finally, we remark that our proof technique for the convergence of GD to a global minimizer in the context of a shallow leaky ReLU network (Theorem 3.1) is closely related to the proof techniques used by Brutzkus et al. (2018). While this work does establish a generalization bound, the bound assumes that population dataset is linearly separable rather than just the training dataset. Hence, it cannot be applied when the training dataset has label-filpping noise, which is the setting that we are interested in for benign overfitting. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Let $[n]=\\{1,2,\\ldots,n\\}$ denote the set of the first $n$ natural numbers. We remark that when using big- $O$ notation we implicitly assume only positive constants. We use $c,C,C_{1},C_{2},...$ to denote absolute constants with respect to the input dimension $d$ , the training sample size $n$ , and the width of the network $m$ . Note constants may change in value from line to line. Furthermore, when using big- $O$ notation all variables aside from $d,n,k$ and $m$ are considered constants. However, for clarity we will frequently make the constants concerning the confidence $\\delta$ and failure probability $\\epsilon$ explicit. Moreover, for two functions $f,g:\\mathbb{N}\\rightarrow\\mathbb{N}$ , if we say $f=O(g)$ implies property $p$ , what we mean is there exists an $N\\in\\mathbb{N}$ and a constant $C$ such that if $f(n)\\le C g(n)$ for all $n\\geq N$ then property $p$ holds. Likewise, if we say $f=\\Omega(g)$ implies property $p$ , what we mean is there exists an $N\\in\\mathbb{N}$ and a constant $C$ such that if $f(n)\\ge\\dot{C}\\dot{g}(n)$ for all $n\\geq N$ then property $p$ holds. Finally, we use $\\|\\cdot\\|$ to denote the $\\ell^{2}$ norm of the vector argument or $\\ell^{2}\\to\\ell^{2}$ operator norm of the matrix argument. ", "page_idx": 2}, {"type": "text", "text": "2.1 Data model ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We study data generated as per the following data model. ", "page_idx": 2}, {"type": "text", "text": "Definition 2.1. Suppose $d,n,k\\in\\mathbb{N}$ , $\\gamma\\in(0,1)$ and $\\pmb{v}\\in\\mathbb{S}^{d-1}$ . $I f\\left(X,{\\hat{y}},y,x,y\\right)\\sim{\\mathcal{D}}(d,n,k,\\gamma,v)$ then ", "page_idx": 3}, {"type": "text", "text": "1. $\\pmb{X}\\in\\mathbb{R}^{n\\times d}$ is a random matrix whose rows, which we denote $\\pmb{x}_{i}$ , satisfy $\\pmb{x}_{i}=\\sqrt{\\gamma}y_{i}\\pmb{v}+$ $\\sqrt{1-\\gamma}n_{i}$ , where $\\begin{array}{r}{\\pmb{n}_{i}\\sim\\mathcal{N}(\\pmb{\\theta}_{d},\\frac{1}{d}(\\pmb{I}_{d}-\\pmb{v}\\pmb{v}^{T}))}\\end{array}$ are mutually i.i.d..   \n2. $\\pmb{y}\\in\\{\\pm1\\}^{n}$ is a random vector with entries $y_{i}$ that are mutually independent of one another as well as the noise vectors $({\\pmb n}_{i})_{i\\in[n]}$ and are uniformly distributed over $\\{\\pm1\\}$ . This vector holds the true labels of the training set.   \n3. Let $B\\subset[n]$ be any subset chosen independently of $\\textit{\\textbf{y}}$ such that $|\\beta|=k$ . Then $\\hat{\\pmb{y}}\\in\\{\\pm1\\}^{n}$ is a random vector whose entries satisfy $\\hat{y}_{i}\\neq y_{i}$ for all $i\\in{\\cal B}$ and $\\hat{y}_{i}=y_{i}$ for all $i\\in B^{c}=:\\mathcal{G}$ . This vector holds the observed labels of the training set.   \n4. y is a random variable representing a test label which is uniformly distributed over $\\{\\pm1\\}$ .   \n5. $\\textbf{\\em x}\\in\\mathbb{R}^{d}$ is a r\u221aandom vector representing the input feature of a test point and satisfies $\\pmb{x}=\\sqrt{\\gamma}y\\pmb{v}+\\sqrt{1-\\gamma}\\pmb{n},$ , where $\\begin{array}{r}{\\dot{\\boldsymbol{n}}\\sim\\mathcal{N}(\\bar{\\pmb{\\theta}_{d}},\\frac{1}{d}(\\pmb{I}_{d}-\\bar{\\pmb{v}}\\pmb{v}^{T}))}\\end{array}$ is mutually independent of the random vectors (ni)i\u2208[n]. ", "page_idx": 3}, {"type": "text", "text": "We refer to $(X,{\\hat{y}})$ as the training data and $(\\pmb{x},y)$ as the test data. Furthermore, for typographical convenience we define $\\pmb{y}\\odot\\hat{\\pmb{y}}=:\\beta\\in\\{\\pm1\\}^{n}$ . ", "page_idx": 3}, {"type": "text", "text": "To provide some interpretation to Definition 2.1, the training data consists of $n$ points of which $k$ have their observed label flipped relative to the true label. We refer to $\\pmb{v}$ and $\\pmb{n}_{i}$ as the signal and noise co\u221amponents of the $i$ -th data point respectively: indeed, with $\\gamma\\,>\\,0$ then for $i\\ \\in\\ {\\mathcal{G}}$ $y_{i}\\langle\\pmb{x}_{i},\\pmb{v}\\rangle=\\sqrt{\\gamma}>0$ . The test data is drawn from the same distribution and is assumed not to be corrupted. We say that the training data $(X,{\\hat{y}})$ is linearly separable if there exists $\\pmb{w}\\in\\mathbb{R}^{d}$ such that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{y}_{i}\\langle\\pmb{w},\\pmb{x}_{i}\\rangle\\geq1,\\quad\\mathrm{for~all~}i\\in[n].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For finite $n$ , this condition is equivalent to the existence of a $\\mathbf{\\nabla}w$ with $\\hat{y}_{i}\\langle{\\pmb w},{\\pmb x}_{i}\\rangle>0$ for all $i\\in[n]$ . We denote the set of linearly separable datasets as $\\mathcal{X}_{l i n}\\subset\\mathbb{R}^{n\\times d}\\times\\{\\pm1\\}^{n}$ . For a linearly separable dataset $(X,{\\hat{y}})$ , the max-margin linear classifier is the unique solution to the optimization problem ", "page_idx": 3}, {"type": "text", "text": "Observe one may equivalently take a strictly convex objective $\\lVert\\boldsymbol{w}\\rVert^{2}$ and the constraint set is a closed convex polyhedron that is non-empty iff the data is linearly separable. The max-margin linear classifier $\\pmb{w}^{*}$ has a corresponding geometric margin $2/\\|\\pmb{w}^{*}\\|$ . When $d\\geq n$ and $\\gamma>0$ , input feature matrices $\\mathbf{\\deltaX}$ from our data model almost surely have linearly independent rows $\\pmb{x}_{i}$ and thus $(X,{\\hat{y}})$ is almost surely linearly separable for any observed labels $\\hat{\\pmb{y}}\\in\\{\\pm1\\}^{n}$ . ", "page_idx": 3}, {"type": "text", "text": "2.2 Architecture and learning algorithm ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We study shallow leaky ReLU networks with a forward pass function $f:\\mathbb{R}^{2m\\times d}\\times\\mathbb{R}^{d}\\to\\mathbb{R}$ defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\nf(W,\\pmb{x})=\\sum_{j=1}^{2m}(-1)^{j}\\sigma(\\langle\\pmb{w}_{j},\\pmb{x}\\rangle),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $W\\,\\in\\,\\mathbb{R}^{2m\\times d}$ are the parameters of the network, $\\sigma:\\mathbb{R}\\rightarrow\\mathbb{R}$ is the leaky ReLU function, defined as $\\sigma(x)=\\operatorname*{max}(x,\\alpha x)$ , where $\\alpha\\in(0,1]$ is referred to as the leaky parameter. We remark that we only train the weights of the first layer and keep the output weights of each neuron fixed. Although $\\sigma$ is not differentiable at 0, in the context of gradient descent we adopt a subgradient and let $\\dot{\\sigma}(z)=1$ for $z\\geq0$ and let $\\dot{\\sigma}(z)=\\alpha$ otherwise. The hinge loss $\\ell:\\mathbb{R}\\rightarrow\\mathbb{R}_{\\geq0}$ is defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\ell(z)=\\operatorname*{max}\\{0,1-z\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Again, $\\ell$ is not differentiable at zero; adopting a subgradient we define for any $j\\in[2m]$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\nabla_{w_{j}}\\ell(\\hat{y}f(W,x))=\\left\\{\\!\\!\\begin{array}{l l}{(-1)^{j+1}\\hat{y}x\\dot{\\sigma}(\\langle w_{j},x\\rangle)}&{\\hat{y}f(W,x)<1,}\\\\ {0}&{\\hat{y}f(W,x)\\geq1.}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The training loss $L:\\mathbb{R}^{2m\\times d}\\times\\mathbb{R}^{n\\times d}\\times\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ is defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\nL(W,X,{\\hat{\\pmb y}})=\\sum_{i=1}^{n}\\ell({\\hat{y}}_{i}f(W,\\pmb x_{i})).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Let $W^{(0)}\\in\\mathbb{R}^{2m\\times d}$ denote the model parameters at initialization. For each $t\\in\\mathbb{N}$ we define $\\mathbf{\\boldsymbol{W}}^{(t)}$ recursively as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pmb{W}^{(t)}=\\pmb{W}^{(t-1)}-\\eta\\nabla_{\\pmb{W}}L(\\pmb{W}^{(t-1)},\\pmb{X},\\hat{\\pmb{y}}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\eta>0$ is the step size. Let ${\\mathcal{F}}^{(t)}\\subseteq[n]$ denote the set of all $i\\in[n]$ such that $\\hat{y}_{i}f(W^{(t)},\\pmb{x}_{i})<1$ . Then equivalently each neuron is updated according to the following rule: for $j\\in[2m]$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pmb{w}_{j}^{(t)}=G D(\\pmb{W}^{(t-1)},\\eta):=\\pmb{w}_{j}^{(t-1)}+\\eta(-1)^{j}\\sum_{i\\in\\mathcal{F}^{(t-1)}}\\hat{y}_{i}\\pmb{x}_{i}\\dot{\\sigma}(\\langle\\pmb{w}_{j}^{(t-1)},\\pmb{x}_{i}\\rangle).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For ease of reference we now provide the following definition of the learning algorithm described above. ", "page_idx": 4}, {"type": "text", "text": "Definition 2.2. Let $\\mathcal{A}_{G D}:\\mathbb{R}^{n\\times d}\\times\\{\\pm1\\}^{n}\\times\\mathbb{R}\\times\\mathbb{R}^{2m\\times d}\\rightarrow\\mathbb{R}^{2m\\times d}$ return $\\mathcal{A}_{G D}(\\boldsymbol{X},\\hat{\\pmb{y}},\\eta,W^{(0)})=:$ $W$ , where the $j$ -th row ${\\pmb w}_{j}$ of $W$ is defined as follows: let ${\\pmb w}_{j}^{(0)}$ be the $j$ -th row of $W^{(0)}$ and generate the sequence $(\\pmb{w}_{j}^{(t)})_{t\\geq0}$ using the recurrence relation $\\pmb{w}_{j}^{(t)}=G D(\\pmb{W}^{(t-1)},\\eta)$ as defined in equation 4. ", "page_idx": 4}, {"type": "text", "text": "2. Otherwise we say $A_{G D}$ converges and $\\begin{array}{r}{\\pmb{w}_{j}=\\operatorname*{lim}_{t\\rightarrow\\infty}\\pmb{w}_{j}^{(t)}}\\end{array}$ ", "page_idx": 4}, {"type": "text", "text": "3. If there exists a $T\\in\\mathbb N$ such that for all $j\\in[2m]\\;{\\pmb w}_{j}^{(t)}={\\pmb w}_{j}^{(T)}$ for all $t\\geq T$ , then we say $A_{G D}$ converges in finite time. ", "page_idx": 4}, {"type": "text", "text": "We often find that all matrices in the set ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\{\\mathcal{A}_{G D}(X,\\hat{\\pmb{y}},\\eta,\\pmb{W}^{(0)}):\\forall j\\,\\|\\pmb{w}_{j}^{(0)}\\|\\le\\lambda\\}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "agree on all relevant properties. In this case, we abuse notation and say that $\\boldsymbol{\\mathcal{A}_{G D}}(\\boldsymbol{X},\\hat{\\boldsymbol{y}},\\eta,\\lambda)=\\boldsymbol{W}$ where $W$ is a generic element from this set. ", "page_idx": 4}, {"type": "text", "text": "Finally, in order to derive our results we make the following assumptions concerning the step size and initialization of the network. ", "page_idx": 4}, {"type": "text", "text": "Assumption 1. The step size $\\eta$ satisfies $\\eta\\,\\leq\\,1/(m n\\,\\mathrm{max}_{i\\in[n]}\\,\\|\\pmb{x}_{i}\\|^{2})$ and for all $j~\\in~[2m]$ the network at initialization satisfies $\\begin{array}{r}{\\|\\pmb{w}_{j}^{(0)}\\|\\leq\\sqrt{\\alpha}/(m\\operatorname*{min}_{i\\in[n]}\\|\\pmb{x}_{i}\\|)}\\end{array}$ . ", "page_idx": 4}, {"type": "text", "text": "Under our data model the input data points have approximately unit norm; therefore these assumptions reduce to \u03b7 \u2264mCn and $\\begin{array}{r}{\\|\\pmb{w}_{j}^{(0)}\\|\\leq\\frac{\\bar{C}\\sqrt{\\alpha}}{m}}\\end{array}$ ", "page_idx": 4}, {"type": "text", "text": "2.3 Approximate margin maximization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We now introduce the notion of an approximate margin maximizing algorithm, which plays a key role in deriving our results. Although the primary setting we consider in this work is the learning algorithm $A_{G D}$ (see Definition 2.2), we derive benign overfitting guarantees more broadly for any learning algorithm which fits into this category. Recall $\\chi_{l i n}$ denotes the set of linearly separable datasets $(\\check{X},\\hat{\\pmb{y}})\\in\\mathbb{R}^{n\\times d}\\times\\{\\pmb{\\pm}{1}\\}^{n}$ . ", "page_idx": 4}, {"type": "text", "text": "Definition 2.3. Let $f:\\mathbb{R}^{p}\\times\\mathbb{R}^{d}\\to\\mathbb{R}$ denote a predictor function with p parameters. An algorithm $\\mathcal{A}:\\mathbb{R}^{n\\times d}\\times\\mathbb{R}^{n}\\,\\rightarrow\\,\\mathbb{R}^{p}$ is approximately margin maximizing with factor $M\\,>\\,0$ on $f$ if for all $(X,{\\hat{y}})\\in{\\mathcal{X}}_{l i n}$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{y}_{i}\\,f(\\pmb{\\mathscr{A}}(\\pmb{{\\cal X}},\\pmb{\\hat{y}}),\\pmb{x}_{i})\\geq1\\,\\,\\,f o r\\,a l l\\,i\\in[n]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\|\\pmb{\\mathcal{A}}(\\pmb{\\mathcal{X}},\\hat{\\pmb{y}})\\|\\leq M\\|\\pmb{w}^{*}\\|,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\pmb{w}^{*}$ is the max-margin linear classifier of $(X,{\\hat{y}})$ . Moreover, if $\\boldsymbol{\\mathcal{A}}$ is an approximate margin maximizing algorithm we define ", "page_idx": 4}, {"type": "equation", "text": "$$\n|{\\cal A}|=\\operatorname*{inf}\\{M>0:{\\mathcal A}\\;i s\\;a p p r o x i m a t e l y\\;m a r g i n\\;m a x i m i z i n g\\;w i t h\\;f a c t o r\\;M\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In the above definition we take the standard Euclidean norm on $\\mathbb{R}^{p}$ . In particular if $\\mathbb{R}^{p}=\\mathbb{R}^{2m\\times d}$ is a space of matrices we take the Frobenius norm. ", "page_idx": 5}, {"type": "text", "text": "3 Main results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In order to prove benign overftiting it is necessary to show that the learning algorithm outputs a model that correctly classifies all points in the training sample. The following theorem establishes this for $A_{G D}$ and bounds the margin maximizing factor $|\\mathcal{A}_{G D}|$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.1. Let $f:\\mathbb{R}^{p}\\times\\mathbb{R}^{n}\\to\\mathbb{R}$ be a leaky ReLU network with forward pass as defined by equation $^{\\,l}$ . Suppose the step size $\\eta$ and initialization condition $\\lambda$ satisfy Assumption 1. Then for any linearly separable data set $\\left(\\boldsymbol{X},\\hat{\\pmb y}\\right)\\mathcal{A}_{G D}(\\boldsymbol{X},\\hat{\\pmb y},\\eta,\\lambda)$ converges after $T$ iterations, where ", "page_idx": 5}, {"type": "equation", "text": "$$\nT\\leq\\frac{C\\|\\pmb{w}^{*}\\|^{2}}{\\eta\\alpha^{2}m}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Furthermore $A_{G D}$ is approximately margin maximizing on $f$ (Definition 2.3) with ", "page_idx": 5}, {"type": "equation", "text": "$$\n|{\\mathcal{A}}_{G D}|\\leq{\\frac{C}{\\alpha{\\sqrt{m}}}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "A proof of Theorem 3.1 can be found in Appendix D.1. Note also by Definition 2.3 that the solution $W=\\mathcal{A}_{G D}(\\boldsymbol{X},\\hat{\\boldsymbol{y}})$ for $(X,{\\hat{y}})\\in{\\mathcal{X}}_{l i n}$ is a global minimizer of the training loss defined in equation 3 with $L(\\boldsymbol{W},\\boldsymbol{X},\\hat{\\boldsymbol{y}})=0$ . Our approach to proving this result is reminiscent of the proof of convergence of the perceptron algorithm and therefore is also similar to the techniques used by Brutzkus et al. (2018). ", "page_idx": 5}, {"type": "text", "text": "For training and test data as per Definition 2.1 we provide an upper bound on the generalization error for approximately margin maximizing algorithms. For convenience we summarize our setting as follows. ", "page_idx": 5}, {"type": "text", "text": "Assumption 2. Setting for proving generalization results. ", "page_idx": 5}, {"type": "text", "text": "\u2022 $f:\\mathbb{R}^{2m\\times d}\\times\\mathbb{R}^{d}\\to\\mathbb{R}$ is a shallow leaky ReLU network as per equation 1.   \n\u2022 $\\mathcal{A}:\\mathbb{R}^{n\\times d}\\times\\{\\pm1\\}^{n}\\rightarrow\\mathbb{R}^{2m\\times d}$ is a learning algorithm that returns the weights $W\\in\\mathbb{R}^{2m\\times d}$ of the first layer of $f$ .   \n\u2022 We let $\\pmb{v}\\in\\mathbb{S}^{d-1}$ and consider training data $(X,{\\hat{y}})$ and test data $(\\boldsymbol{x},\\boldsymbol{y})$ distributed according to $(X,{\\hat{y}},y,x,y)\\sim{\\mathcal{D}}(d,n,k,\\gamma,v)$ as per $D$ efinition 2.1. ", "page_idx": 5}, {"type": "text", "text": "Under this setting we have the following generalization result for an approximately margin maximizing algorithm $\\boldsymbol{\\mathcal{A}}$ . Note this result requires $\\gamma$ , and hence the signal to noise ratio of the inputs, to be sufficiently large. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.2. Under the setting given in Assumption 2, let $\\delta\\in(0,1)$ and suppose $\\boldsymbol{\\mathcal{A}}$ is approximately margin-maximizing (Definition 2.3). If $n=\\Omega$ $\\left(\\log{\\frac{1}{\\delta}}\\right)$ , $d=\\Omega\\left(n\\right)$ , $\\begin{array}{r}{k=O\\big(\\frac{n}{1+m|A|^{2}}\\big)}\\end{array}$ , and $\\begin{array}{r}{\\gamma=\\Omega\\left(\\frac{1}{k}\\right)}\\end{array}$ then there is a fixed positive constant $C$ such that with probability at least $1-\\delta$ over $(X,{\\hat{y}})$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{P}(y f(W,x)\\leq0\\mid X,\\hat{y})\\leq\\exp\\left(-C\\cdot\\frac{d}{k(1+m|A|^{2})}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "A proof of Theorem 3.2 is provided in Appendix D.2. To comment informally on the relationship between $k$ and $\\gamma$ , we require $\\gamma=\\Omega(k^{-1})$ in order to guarantee that any network which achieves zero hinge loss does so by focusing on the signal component $\\pmb{v}$ rather than the noise components $\\pmb{n}_{i}$ We use the projection of the model weights onto the signal subspace as a measure of the strength of the signal the model has learned and derive our generalization results based on this measure. In Section 4 we provide a proof sketch of this framework in the simpler, linear model setting. Combining Theorems 3.1, and 3.2 we arrive at the following benign overfitting result for shallow leaky ReLU networks trained with GD on hinge loss. ", "page_idx": 5}, {"type": "text", "text": "Corollary 3.2.1. Under the setting given in Assumption 2, let $\\delta\\in(0,1)$ and suppose $\\mathcal{A}=\\mathcal{A}_{G D}$ where $\\eta,\\lambda\\in\\mathbb{R}_{>0}$ satisfy Assumption $^{\\,I}$ . If $n=\\Omega\\left(\\log\\frac{1}{\\delta}\\right)$ , $d=\\Omega\\left(n\\right)$ , $k=O(\\alpha^{2}n)$ , and $\\begin{array}{r}{\\gamma=\\Omega\\left(\\frac{1}{k}\\right)}\\end{array}$ then the following hold. ", "page_idx": 5}, {"type": "text", "text": "1. The algorithm $A_{G D}$ terminates almost surely after a finite number of updates. If $W\\,=$ $\\mathcal{A}_{G D}(X,\\hat{y})$ , then $L(\\boldsymbol{W},\\boldsymbol{X},\\hat{\\boldsymbol{y}})=0$ . ", "page_idx": 6}, {"type": "text", "text": "2. There is a fixed positive constant $C$ such that, with probability at least $1-\\delta$ over the training data $(X,{\\hat{y}})$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{P}(y f(W,\\pmb{x})\\le0\\mid\\pmb{X},\\hat{\\pmb{y}})\\le\\exp\\left(-C\\cdot\\frac{\\alpha^{2}d}{k}\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We remark that the upper bound is at most $\\exp\\left(-C d/n\\right)$ for a different constant $C$ as we assume $k=O(\\alpha^{2}n)$ . ", "page_idx": 6}, {"type": "text", "text": "If $k$ is large enough, this bound is tight up to constants and factors of $\\alpha$ in the exponent. This is given by the following theorem, proven in Appendix D.2. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.3. Under the setting given in Assumption 2, let $\\delta\\in(0,1)$ and suppose $\\mathcal{A}=\\mathcal{A}_{G D}$ where $\\eta,\\lambda\\in\\mathbb{R}_{>0}$ satisfy Assumption $^{\\,l}$ . If $n=\\Omega\\left(k\\right)$ , $d=\\Omega\\left(n\\right)$ , and $\\begin{array}{r}{\\dot{k}=\\Omega(\\log\\frac{1}{\\delta}+\\frac{1}{\\alpha}),}\\end{array}$ , then there is $a$ fixed positive constant $C$ such that with probability at least $1-\\delta$ over $(X,{\\hat{y}})$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{P}(y f(W,x)\\leq0\\mid X,{\\hat{y}})\\geq\\exp\\left(-C\\cdot{\\frac{d}{\\alpha k}}\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In addition to this benign overftiting result we also provide the following non-benign overftiting result for $A_{G D}$ . Note that conversely this result requires $\\gamma$ , and hence the signal to noise ratio of the inputs, to be sufficiently small. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.4. Under the setting given in Assumption 2, let $\\delta\\,\\in\\,(0,1)$ and suppose $A=A_{G D}$ , where $\\eta,\\lambda\\in\\mathbb{R}_{>0}$ satisfy Assumption $^{\\,I}$ . If $\\begin{array}{r}{n=\\Omega(1),d=\\Omega\\left(n+\\log\\frac{1}{\\delta}\\right)}\\end{array}$ and $\\begin{array}{r}{\\gamma=O\\left(\\frac{\\alpha^{3}}{d}\\right)}\\end{array}$ then the following hold. ", "page_idx": 6}, {"type": "text", "text": "1. The algorithm $A_{G D}$ terminates almost surely after finitely many updates. With ${\\textbf{\\em W}}=$ $A_{G D}(X,\\hat{\\pmb y}),\\,L({\\cal W},X,\\hat{\\pmb y})=0.$ . $1-\\delta$ $(X,{\\hat{y}})$ ", "page_idx": 6}, {"type": "text", "text": "2. With probability at least over the training data ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{P}(y f(W,x)<0\\mid X,{\\hat{\\pmb y}})\\geq{\\frac{1}{8}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "A proof of Theorem 3.4 is provided in Appendix D.3. ", "page_idx": 6}, {"type": "text", "text": "4 Approximate margin maximization and generalization: insight from linear models ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section we outline proofs for the analogues of Theorems 3.2 and 3.4 in the context of linear models. The arguments are thematically similar and clearer to present. We provide complete proofs of benign and non-benign overfitting for linear models in Appendix C. ", "page_idx": 6}, {"type": "text", "text": "An important lemma is the following, which bounds the largest and $n$ -th largest singular values ( $\\overline{{\\sigma_{1}}}$ and $\\sigma_{n}$ respectively) of the noise matrix $_{N}$ : ", "page_idx": 6}, {"type": "text", "text": "Lemma 4.1. Let $N\\in\\mathbb{R}^{n\\times d}$ denote a random matrix whose rows are drawn mutually i.i.d. from $\\begin{array}{r}{\\mathcal{N}(\\mathbf{0}_{d},\\frac{1}{d}(\\mathbf{{I}}_{d}-v\\mathbf{{v}}^{T}))}\\end{array}$ . If $\\begin{array}{r}{d=\\Omega\\left(n+\\log\\frac{1}{\\delta}\\right)}\\end{array}$ , then there exists constants $C_{1}$ and $C_{2}$ such that, with probability at least $1-\\delta$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\nC_{1}\\leq\\sigma_{n}(N)\\leq\\sigma_{1}(N)\\leq C_{2}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We prove this lemma in Appendix B using results from Vershynin (2018) and Rudelson & Vershynin (2009). A consequence of this lemma is that with probability at least $1-\\delta$ , the condition number of $_{N}$ restricted to span $_{N}$ can be bounded above independently of all hyperparameters. For this reason, we refer to the noise as being well-conditioned. ", "page_idx": 6}, {"type": "text", "text": "Now let $\\pmb{w}\\,=\\,\\mathcal{A}(\\pmb{X},\\hat{\\pmb{y}})$ be the linear classifier returned by the algorithm. Observe that we can decompose the weight vector into a signal and noise component ", "page_idx": 6}, {"type": "equation", "text": "$$\n{\\pmb w}=a_{v}{\\pmb v}+{\\pmb z},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\textit{z}\\perp\\textit{\\textbf{v}}$ and $a_{v}\\in\\mathbb{R}$ . Based on this decomposition the proof proceeds as follows. ", "page_idx": 6}, {"type": "text", "text": "1. Generalization bounds based on the SNR: For test data as per the data model given in Definition 2.1 we want to bound the probability of misclassification: in particular, we want to bound the probability that ", "page_idx": 7}, {"type": "equation", "text": "$$\nX:=y\\langle{\\pmb w},{\\pmb x}\\rangle=\\sqrt{\\gamma}a_{v}+\\sqrt{1-\\gamma}\\langle{\\pmb n},z\\rangle\\leq0.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "As the noise is normally distributed, $\\begin{array}{r}{X\\,\\sim\\,{\\mathcal{N}}\\left(\\sqrt{\\gamma}a_{v},\\frac{1-\\gamma}{d}\\|z\\|^{2}\\right)}\\end{array}$ and the desired upper bound therefore follows from Hoeffding\u2019s inequality, ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{P}(X\\leq0)\\leq\\exp\\left(-\\frac{\\gamma d a_{v}^{2}}{2(1-\\gamma)\\|z\\|^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Using Gaussian anti-concentration, we also obtain a lower bound for the probability of misclassification: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{P}(y\\langle w,x\\rangle\\leq0)\\geq\\operatorname*{max}\\left\\{\\frac{1}{2}-\\sqrt{\\frac{d\\gamma}{2\\pi(1-\\gamma)}}\\frac{a_{v}}{\\|z\\|},\\frac{1}{4}\\exp\\left(-\\frac{6d}{\\pi}\\frac{\\gamma}{1-\\gamma}\\frac{a_{v}^{2}}{\\|z\\|^{2}}\\right)\\right\\}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "2. Upper bound the norm of the max-margin classifier: In order to use the approximate maxmargin property we require an upper bound on $\\lVert\\pmb{w}^{*}\\rVert$ . As by definition $\\lVert\\pmb{w}^{*}\\rVert\\leq\\lVert\\bar{\\pmb{w}}\\rVert$ , it suffices to construct a vector $\\tilde{w}$ that interpolates the data and has small norm. Using that the noise matrix of the data is well-conditioned with high probability, we achieve this by strategically constructing the signal and noise components of $\\tilde{\\pmb{w}}$ . This yields the bound ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\|w^{*}\\|\\leq\\|\\tilde{w}\\|\\leq C\\operatorname*{min}\\left(\\sqrt{\\frac{n}{1-\\gamma}},\\sqrt{\\frac{1}{\\gamma}+\\frac{k}{1-\\gamma}}\\right),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where the arguments of the min function originate from a small and large $\\gamma$ regime respectively. ", "page_idx": 7}, {"type": "text", "text": "3. Lower bound the SNR using the approximate margin maximization property: Based on step 1 the key quantity of interest from a generalization perspective is the ratio $a_{v}/\\lVert z\\rVert$ , which describes the signal to noise ratio (SNR) of the learned classifier. To lower bound this quantity we first lower bound $a_{v}$ . In particular, if $a_{v}$ is small, then the only way to attain zero loss on the clean data is for $\\|z\\|$ to be large. However, under appropriate assumptions on $d,n,k$ and $\\gamma$ this can be shown to contradict the bound $\\|z\\|\\leq\\|w\\|\\leq|\\bar{A}|\\|w^{*}\\|$ , and thus $a_{v}$ must be bounded from below. A lower bound on $a_{v}/\\lVert z\\rVert$ then follows by again using $\\|z\\|\\leq\\|w^{*}\\|$ . Hence we obtain a lower bound for the SNR and establish benign overfitting. ", "page_idx": 7}, {"type": "text", "text": "4. Upper bound the SNR using the zero loss condition: For the generalization lower bound, we compute an upper bound for the ratio $a_{v}/\\Vert z\\Vert$ rather than a lower bound. Since the model perfectly fits the training data with margin one, ", "page_idx": 7}, {"type": "equation", "text": "$$\n1\\leq\\hat{y}_{i}\\langle{\\pmb w},{\\pmb x}_{i}\\rangle=\\sqrt{\\gamma}\\beta_{i}a_{v}+\\sqrt{1-\\gamma}\\hat{y}_{i}\\langle{\\pmb z},{\\pmb n}_{i}\\rangle\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "for all $i\\in[n]$ . The above inequality implies that $\\sqrt{1-\\gamma}\\hat{y}_{i}\\langle n_{i},z\\rangle$ is at least $\\sqrt{\\gamma}a_{v}$ for all corrupt points. Since the noise is well-conditioned, this gives a lower bound on $\\left\\|z\\right\\|$ in terms of $a_{v}$ and hence an upper bound on the SNR $a_{v}/\\lVert z\\rVert$ . By the second generalization lower bound in step 1, the generalization error is bounded below at a similar exponential rate to the upper bound. ", "page_idx": 7}, {"type": "text", "text": "5. Upper bound the SNR using the zero loss condition and maximum margin property: To prove non-benign overfitting, we again compute an upper bound for the ratio $a_{v}/\\lVert z\\rVert$ . We return to the zero loss condition: ", "page_idx": 7}, {"type": "equation", "text": "$$\n1\\leq\\hat{y}_{i}\\langle{\\pmb w},{\\pmb x}_{i}\\rangle=\\sqrt{\\gamma}\\beta_{i}a_{v}+\\sqrt{1-\\gamma}\\hat{y}_{i}\\langle{\\pmb z},{\\pmb n}_{i}\\rangle\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "for all $i\\in[n]$ . If $\\gamma$ is small and $|\\sqrt{\\gamma}a_{v}|$ is large, then $\\lVert\\pmb{w}\\rVert$ will b\u221ae large, contradicting the approximate margin maximization. Hence the above inequality implies that $\\sqrt{1-\\gamma}\\hat{y}_{i}\\langle n_{i},z\\rangle$ is large for all $i\\in[n]$ . Since the noise is well-conditioned, this can only happen when $\\|z\\|$ is large. This gives us a lower bound on $\\|z\\|$ . As before, we can also upper bound $a_{v}$ by $\\lVert\\pmb{w}\\rVert$ , giving us an upper bound on the SNR $a_{v}/\\lVert z\\rVert$ . By the first generalization lower bound in step 1, the classifier generalizes poorly and exhibits non-benign overfitting. ", "page_idx": 7}, {"type": "text", "text": "4.1 From linear models to leaky ReLU networks ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The proof of benign and non-benign overfitting in the linear case uses the tension between the two properties of approximate margin maximization: ftiting both the clean and corrupt points with margin versus the bound on the norm. To extend this idea to a shallow leaky ReLU network as per equation 1, we consider the same decomposition for each neuron $j\\in[2m]$ , ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\pmb{w}_{j}=a_{j}\\pmb{v}+z_{j},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $a_{j}\\in\\mathbb{R}$ and $z_{j}\\perp v$ . In the linear case $\\pm a_{v}$ can be interpreted as the activation of the linear classifier on $\\pm v$ respectively: in terms of magnitude the signal activation is the same in either case and thus we measure the alignment of the linear model with the signal using $\\left|a_{v}\\right|$ . For leaky ReLU networks we define their activation on $\\pm v$ respectively as $A_{1}=f(\\bar{\\mathbf{W}},\\pmb{v})$ and $\\bar{A_{-1}}=-f(W,-v)$ , and then define the alignment of the network as $A_{\\operatorname*{min}}=\\operatorname*{min}\\{A_{1},A_{-1}\\}$ . Considering the alignment of the network with the noise, then if $Z\\,\\in\\,\\mathbb{R}^{2m\\times d}$ denotes a matrix whose $j$ -th row is $z_{j}$ , then we measure the alignment of the network using $\\|Z\\|_{F}$ . As a result, analogous to $a_{v}/\\Vert z\\Vert$ , the key ratio from a generalization perspective in the context of a leaky ReLU network is $A_{\\mathrm{min}}/\\|Z\\|_{F}$ . The proof Theorems 3.2, 3.3, and 3.4 then follow the same outline as Steps 1-3 above but with additional non-trivial technicalities. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this work we have proven conditions under which leaky ReLU networks trained on binary classification tasks exhibit benign and non-benign overfitting. We have substantially relaxed the necessary assumptions on the input data compared with prior work; instead of requiring nearly orthogonal data with $d=\\Omega(n^{2}\\log n)$ or higher, we only need $d=\\Omega(n)$ . We achieve this by using the distribution of singular values of the noise rather than specific correlations between noise vectors. Our emphasis was on networks trained by gradient descent with the hinge loss, but we establish a new framework that is general enough to accommodate any algorithm that is approximately margin maximizing. ", "page_idx": 8}, {"type": "text", "text": "There are a few limitations of our results which would be natural questions to address in future work. While we improve upon existing results in our dependence on the input dimension of the data, we still require that the training dataset is linearly separable. This leaves open the question of whether an overparameterized network will perfectly fit the training data and generalize well for lower dimensional data, or satisfy a similar margin maximization condition. We also focus mainly on two-layer networks with fixed outer layer weights trained with the hinge loss. It would be interesting to investigate whether analogous results hold for deeper architectures or different loss functions and data models. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "This material is based upon work supported by the National Science Foundation under Grant No. DMS-1928930 and by the Alfred P. Sloan Foundation under grant G-2021-16778, while the authors EG and DN were in residence at the Simons Laufer Mathematical Sciences Institute (formerly MSRI) in Berkeley, California, during the Fall 2023 semester. EG and DN were also partially supported by NSF DMS 2011140. EG was also supported by a NSF Graduate Research Fellowship under grant DGE 2034835. GM and KK were partly supported by NSF CAREER DMS 2145630 and DFG SPP 2298 Theoretical Foundations of Deep Learning grant 464109215. GM was also partly supported by NSF grant CCF 2212520, ERC Starting Grant 757983 (DLT), and BMBF in DAAD project 57616814 (SECAI). ", "page_idx": 8}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Ben Adlam and Jeffrey Pennington. The neural tangent kernel in high dimensions: Triple descent and a multi-scale theory of generalization. In Hal Daum\u00e9 III and Aarti Singh (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 74\u201384. PMLR, 2020. URL https://proceedings.mlr.press/v119/ adlam20a.html.   \nPeter L. Bartlett, Philip M. Long, G\u00e1bor Lugosi, and Alexander Tsigler. Benign overftiting in linear regression. Proceedings of the National Academy of Sciences, 117(48):30063\u201330070, 2020. URL https://www.pnas.org/doi/abs/10.1073/pnas.1907378117.   \nMikhail Belkin, Siyuan Ma, and Soumik Mandal. To understand deep learning we need to understand kernel learning. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 541\u2013549. PMLR, 2018. URL https://proceedings.mlr.press/v80/belkin18a.html.   \nMikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning practice and the classical bias\u2013variance trade-off. Proceedings of the National Academy of Sciences, 116(32):15849\u201315854, 2019. URL https://doi.org/10.1073/pnas.190307011.   \nAlon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. SGD learns overparameterized networks that provably generalize on linearly separable data. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id $=$ rJ33wwxRb.   \nYuan Cao, Quanquan Gu, and Mikhail Belkin. Risk bounds for over-parameterized maximum margin classification on sub-gaussian mixtures. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 8407\u20138418. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/ paper_files/paper/2021/file/46e0eae7d5217c79c3ef6b4c212b8c6f-Paper.pdf.   \nYuan Cao, Zixiang Chen, Misha Belkin, and Quanquan Gu. Benign overftiting in two-layer convolutional neural networks. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 25237\u201325250. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/ 2022/file/a12c999be280372b157294e72a4bbc8b-Paper-Conference.pdf.   \nNiladri S. Chatterji and Philip M. Long. Finite-sample analysis of interpolating linear classifiers in the overparameterized regime. Journal of Machine Learning Research, 22(129):1\u201330, 2021. URL http://jmlr.org/papers/v22/20-974.html.   \nNiladri S. Chatterji and Philip M. Long. Foolish crowds support benign overfitting. Journal of Machine Learning Research, 23(125):1\u201312, 2022. URL http://jmlr.org/papers/v23/ 21-1199.html.   \nZixiang Chen, Junkai Zhang, Yiwen Kou, Xiangning Chen, Cho-Jui Hsieh, and Quanquan Gu. Why does sharpness-aware minimization generalize better than SGD? In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id $=$ 3WAnGWLpSQ.   \nSpencer Frei, Niladri S Chatterji, and Peter Bartlett. Benign overfitting without linearity: Neural network classifiers trained by gradient descent for noisy linear data. In Po-Ling Loh and Maxim Raginsky (eds.), Proceedings of Thirty Fifth Conference on Learning Theory, volume 178 of Proceedings of Machine Learning Research, pp. 2668\u20132703. PMLR, 2022. URL https:// proceedings.mlr.press/v178/frei22a.html.   \nSpencer Frei, Gal Vardi, Peter L. Bartlett, and Nathan Srebro. Benign overftiting in linear classifiers and leaky relu networks from KKT conditions for margin maximization. In Gergely Neu and Lorenzo Rosasco (eds.), The Thirty Sixth Annual Conference on Learning Theory, 12-15 July 2023, Bangalore, India, volume 195 of Proceedings of Machine Learning Research, pp. 3173\u20133228. PMLR, 2023. URL https://proceedings.mlr.press/v195/frei23a.html.   \nErin George, Michael Murray, William Swartworth, and Deanna Needell. Training shallow ReLU networks on noisy data using hinge loss: when do we overfit and is it benign? In A. Oh, T. Neumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 35139\u201335189. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/ 6e73c39cc428c7d264d9820319f31e79-Paper-Conference.pdf.   \nTrevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J. Tibshirani. Surprises in highdimensional ridgeless least squares interpolation. The Annals of Statistics, 50(2):949\u2013986, 2022. URL https://doi.org/10.1214/21-AOS2133.   \nZiwei Ji and Matus Telgarsky. Directional convergence and alignment in deep learning. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 17176\u201317186. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/ c76e4b2fa54f8506719a5c0dc14c2eb9-Paper.pdf.   \nFrederic Koehler, Lijia Zhou, Danica J. Sutherland, and Nathan Srebro. Uniform convergence of interpolators: Gaussian width, norm bounds and benign overftiting. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id $=$ FyOhThdDBM.   \nGuy Kornowski, Gilad Yehudai, and Ohad Shamir. From tempered to benign overfitting in ReLU neural networks. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id $=$ LnZuxp3Tx7.   \nYiwen Kou, Zixiang Chen, Yuanzhou Chen, and Quanquan Gu. Benign overftiting in two-layer ReLU convolutional neural networks. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 17615\u201317659. PMLR, 2023. URL https://proceedings.mlr.press/v202/kou23a.html.   \nTengyuan Liang and Alexander Rakhlin. Just interpolate: Kernel \u201cRidgeless\u201d regression can generalize. The Annals of Statistics, 48(3):1329 \u2013 1347, 2020. URL https://doi.org/10.1214/ 19-AOS1849.   \nTengyuan Liang, Alexander Rakhlin, and Xiyu Zhai. On the multiple descent of minimum-norm interpolants and restricted lower isometry of kernels. In Jacob Abernethy and Shivani Agarwal (eds.), Proceedings of Thirty Third Conference on Learning Theory, volume 125 of Proceedings of Machine Learning Research, pp. 2683\u20132711. PMLR, 2020. URL https://proceedings.mlr. press/v125/liang20a.html.   \nKaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks. In International Conference on Learning Representations, 2020. URL https://openreview. net/forum?id $\\equiv$ SJeLIgBKPS.   \nSong Mei and Andrea Montanari. The generalization error of random features regression: Precise asymptotics and the double descent curve. Communications on Pure and Applied Mathematics, 75(4):667\u2013766, 2022. URL https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa. 22008.   \nAndrea Montanari, Feng Ruan, Basil Saeed, and Youngtak Sohn. Universality of max-margin classifiers. arXiv preprint arXiv:2310.00176, 2023a.   \nAndrea Montanari, Feng Ruan, Youngtak Sohn, and Jun Yan. The generalization error of max-margin linear classifiers: Benign overfitting and high-dimensional asymptotics in the overparametrized regime. arXiv preprint arXiv:1911.01544, 2023b.   \nVidya Muthukumar, Kailas Vodrahalli, Vignesh Subramanian, and Anant Sahai. Harmless interpolation of noisy data in regression. IEEE Journal on Selected Areas in Information Theory, 1(1): 67\u201383, 2020. URL https://doi.org/10.1109/JSAIT.2020.2984716.   \nVidya Muthukumar, Adhyyan Narang, Vignesh Subramanian, Mikhail Belkin, Daniel Hsu, and Anant Sahai. Classification vs regression in overparameterized regimes: Does the loss function matter? J. Mach. Learn. Res., 22(1), 2021. URL http://jmlr.org/papers/v22/20-603.html.   \nGeorge P\u00f3lya. Remarks on computing the probability integral in one and two dimensions. In Proceedings of the Berkeley Symposium on Mathematical Statistics and Probability, number 1, pp. 63. University of California Press Berkeley, 1949.   \nMark Rudelson and Roman Vershynin. Smallest singular value of a random rectangular matrix. Communications on Pure and Applied Mathematics, 62(12):1707\u20131739, 2009. URL https: //doi.org/10.1002/cpa.20294.   \nOhad Shamir. The implicit bias of benign overfitting. In Po-Ling Loh and Maxim Raginsky (eds.), Proceedings of Thirty Fifth Conference on Learning Theory, volume 178 of Proceedings of Machine Learning Research, pp. 448\u2013478. PMLR, 2022. URL https://proceedings.mlr. press/v178/shamir22a.html.   \nRoman Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Science. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2018. URL https://doi.org/10.1017/9781108231596.   \nGuillaume Wang, Konstantin Donhauser, and Fanny Yang. Tight bounds for minimum $\\ell_{1}$ -norm interpolation of noisy data. In International Conference on Artificial Intelligence and Statistics, 2021a. URL https://proceedings.mlr.press/v151/wang22k.html.   \nKe Wang, Vidya Muthukumar, and Christos Thrampoulidis. Benign overftiting in multiclass classification: All roads lead to interpolation. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 24164\u201324179. Curran Associates, Inc., 2021b. URL https://proceedings.neurips.cc/ paper_files/paper/2021/file/caaa29eab72b231b0af62fbdff89bfce-Paper.pdf.   \nDenny Wu and Ji Xu. On the optimal weighted $\\ell_{2}$ regularization in overparameterized linear regression. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 10112\u201310123. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/ 72e6d3238361fe70f22fb0ac624a7072-Paper.pdf.   \nXingyu Xu and Yuantao Gu. Benign overftiting of non-smooth neural networks beyond lazy training. In Francisco Ruiz, Jennifer Dy, and Jan-Willem van de Meent (eds.), Proceedings of The 26th International Conference on Artificial Intelligence and Statistics, volume 206 of Proceedings of Machine Learning Research, pp. 11094\u201311117. PMLR, 2023. URL https://proceedings. mlr.press/v206/xu23k.html.   \nZhiwei Xu, Yutong Wang, Spencer Frei, Gal Vardi, and Wei Hu. Benign overfitting and grokking in ReLU networks for XOR cluster data. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id $\\equiv$ BxHgpC6FNv.   \nChiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In International Conference on Learning Representations, 2017. URL https://openreview.net/forum?id $\\equiv$ Sy8gdB9xx.   \nDifan Zou, Jingfeng Wu, Vladimir Braverman, Quanquan Gu, and Sham Kakade. Benign overftiting of constant-stepsize SGD for linear regression. In Mikhail Belkin and Samory Kpotufe (eds.), Proceedings of Thirty Fourth Conference on Learning Theory, volume 134 of Proceedings of Machine Learning Research, pp. 4633\u20134635. PMLR, 2021. URL https://proceedings.mlr. press/v134/zou21a.html. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Appendix A Preliminaries on random vectors ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Recall that the sub-exponential norm of a random variable $X$ is defined as ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\|X\\|_{\\psi_{1}}:=\\operatorname*{inf}\\{t>0\\colon\\mathbb{E}[\\exp(|X|/t)]\\le2\\}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "(see Vershynin, 2018, Definition 2.7.5) and that the sub-Gaussian norm is defined as ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\|X\\|_{\\psi_{2}}:=\\operatorname*{inf}\\{t>0\\colon\\mathbb{E}[\\exp(X^{2}/t^{2})]\\leq2\\}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "A random variable $X$ is sub-Gaussian if and only if $X^{2}$ is sub-exponential. Furthermore, $\\|X^{2}\\|_{\\psi_{1}}=$ $\\|X\\|_{\\psi_{2}}^{2}$ . ", "page_idx": 12}, {"type": "text", "text": "Lemma A.1. Let $\\boldsymbol{n}\\sim\\mathcal{N}(\\mathbf{0}_{d},d^{-1}(I_{d}-\\pmb{v}\\pmb{v}^{T}))$ and suppose that $\\pmb{Z}\\in\\mathbb{R}^{m\\times d}$ . Then with probability at least $1-\\epsilon,$ ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\|Z n\\|\\leq C\\|Z\\|_{F}\\sqrt{\\frac{1}{d}\\log\\frac{1}{\\epsilon}}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Proof. Let $\\pmb{P}=\\pmb{I}_{d}-\\pmb{v}\\pmb{v}^{T}$ be the orthogonal projection onto $\\operatorname{span}(\\{v\\})^{\\perp}$ , so that $_{z n}$ is identically distributed to $d^{-1/2}Z P n^{\\prime}$ , where $n^{\\prime}$ has distribution $\\mathcal{N}(\\mathbf{0}_{d},\\pmb{I}_{d})$ . Following Vershynin (2018, Theorem 6.3.2), ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Big\\|\\|Z n\\|-\\|d^{-1/2}Z P\\|_{F}\\Big\\|_{\\psi_{2}}=\\Big\\|\\|d^{-1/2}Z P n^{\\prime}\\|-\\|d^{-1/2}Z P\\|_{F}\\Big\\|_{\\psi_{2}}}&{}\\\\ {\\leq C\\|d^{-1/2}Z P\\|}&{}\\\\ {\\leq C d^{-1/2}\\|Z\\|\\|P\\|}&{}\\\\ {=C d^{-1/2}\\|Z\\|}\\\\ {\\leq C d^{-1/2}\\|Z\\|_{F},}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where we used that $_{P}$ is an orthogonal projection in the fourth line and that the operator norm is by bounded above by the Frobenius norm in the fifth line. As a result the sub-Gaussian norm of $||Z n||$ is bounded as ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\|Z\\pmb{n}\\|\\right\\|_{\\psi_{2}}\\leq\\left\\|\\|Z\\pmb{n}\\|-\\|d^{-1/2}Z\\pmb{P}\\|_{F}\\right\\|_{\\psi_{2}}+\\left\\|\\|d^{-1/2}\\pmb{Z}\\pmb{P}\\|_{F}\\right\\|_{\\psi_{2}}}\\\\ &{\\qquad\\qquad\\leq C d^{-1/2}(\\|\\pmb{Z}\\|_{F}+\\|\\pmb{Z}\\pmb{P}\\|_{F})}\\\\ &{\\qquad\\qquad\\leq C d^{-1/2}\\|\\pmb{Z}\\|_{F},}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where the last line follows from the calculation ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|Z P\\|_{F}=\\|P^{T}Z^{T}\\|_{F}}\\\\ &{\\qquad\\quad=\\|P Z^{T}\\|_{F}}\\\\ &{\\qquad\\quad\\leq\\|P\\|\\|Z^{T}\\|_{F}}\\\\ &{\\qquad\\quad=\\|Z^{T}\\|_{F}}\\\\ &{\\qquad\\quad=\\|Z\\|_{F}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "This implies a tail bound (see Vershynin, 2018, Proposition 2.5.2) ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\|Z\\pmb{n}\\|\\ge t)\\le2\\exp\\left(-\\frac{d t^{2}}{C\\|Z\\|_{F}^{2}}\\right),\\quad\\mathrm{for~all~}t\\ge0.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Setting $\\begin{array}{r}{t=C\\|Z\\|_{F}\\sqrt{\\frac{1}{d}\\log\\frac{2}{\\epsilon}}}\\end{array}$ , the result follows. ", "page_idx": 12}, {"type": "text", "text": "Lemma A.2. Let $\\boldsymbol{n}\\sim\\mathcal{N}(\\mathbf{0}_{d},d^{-1}(I_{d}-\\pmb{v}\\pmb{v}^{T}))$ and suppose $z\\in\\mathrm{span}(\\{v\\})^{\\perp}$ . There exists a $C>0$ such that with probability at least $1-\\delta$ ", "page_idx": 12}, {"type": "equation", "text": "$$\n|\\langle n,z\\rangle|\\leq C\\|z\\|{\\sqrt{{\\frac{1}{d}}\\log{\\frac{1}{\\delta}}}}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Furthermore, there exists a $c>0$ such that with probability at least $\\frac{1}{2}$ ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\left|\\langle n,z\\rangle\\right|\\geq{\\frac{c\\|z\\|}{\\sqrt{d}}}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Proof. Let $X=\\langle n,z\\rangle$ . Then $X$ is Gaussian with variance ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[X^{2}]=\\mathbb{E}[z^{T}\\pmb{n}\\pmb{n}^{T}z]}\\\\ &{\\quad\\quad=z^{T}\\mathbb{E}[\\pmb{n}\\pmb{n}^{T}]z}\\\\ &{\\quad\\quad=d^{-1}z^{T}(I_{d}-\\pmb{v}\\pmb{v}^{T})z}\\\\ &{\\quad\\quad=d^{-1}z^{T}z}\\\\ &{\\quad\\quad=d^{-1}\\|z\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Note the third line above follows from the fact that $z\\in\\mathrm{span}(\\{v\\})^{\\perp}$ . Therefore by Hoeffding\u2019s inequality, for all $t\\geq0$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{P}(|\\langle\\pmb{n},z\\rangle|\\geq t)\\leq2\\exp\\left(-\\frac{d t^{2}}{C\\|z\\|^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Setting $\\begin{array}{r}{t=C\\|z\\|^{2}\\sqrt{\\frac{1}{d}\\log\\frac{2}{\\delta}}}\\end{array}$ , we obtain ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{P}(|\\langle n,z\\rangle|\\geq t)\\leq\\delta,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "which establishes the first part of the result. ", "page_idx": 13}, {"type": "text", "text": "Since $d^{1/2}\\|z\\|^{-1}X$ is a standard Gaussian, there exists a constant $c$ such that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{P}(|d^{1/2}\\|z\\|^{-1}X|\\geq c)\\leq\\frac{1}{2}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Rearranging, we obtain ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{P}(|\\langle n,z\\rangle|\\geq c d^{-1/2}\\|z\\|)\\leq\\frac{1}{2}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "This establishes the second part of the result. ", "page_idx": 13}, {"type": "text", "text": "Appendix B Upper bounding the norm of the max-margin classifier of the data ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Here we establish key properties concerning the data model given in Definition 2.1, our main goal being to establish bounds on the norm of the max-margin classifier. To this end we first identify certain useful facts about rectangular Gaussian matrices. In what follows we index the singular values of any given matrix $\\pmb{A}$ in decreasing order as $\\sigma_{1}(A)\\geq\\sigma_{2}(A)\\geq\\cdot\\cdot\\cdot$ . Furthermore, we denote the $i$ -th-row of a matrix $\\pmb{A}$ as $\\pmb{a}_{i}$ . ", "page_idx": 13}, {"type": "text", "text": "Lemma B.1. Let $G\\in\\mathbb{R}^{n\\times d}$ be a Gaussian matrix whose entries are mutually i.i.d. with distribution $\\mathcal{N}(0,1)$ . If $\\begin{array}{r}{d=\\Omega\\left(n+\\log\\frac{1}{\\delta}\\right)}\\end{array}$ , then with probability at least $1-\\delta$ the following inequalities are simultaneously true. ", "page_idx": 13}, {"type": "equation", "text": "$$\nl.\\;\\;\\sigma_{1}(G)\\leq C({\\sqrt{d}}+{\\sqrt{n}}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. We proceed by upper bounding the probability that each individual inequality does not hold. ", "page_idx": 13}, {"type": "text", "text": "$^{\\,l}$ . To derive an upper bound on $\\sigma_{1}(G)$ we use the following fact (see Vershynin, 2018, Theorem 4.4.5). For any $\\epsilon>0$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\sigma_{1}(\\pmb{G})\\geq C_{1}(\\sqrt{n}+\\sqrt{d}+\\epsilon))\\leq2\\exp(-\\epsilon^{2}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "With $\\epsilon={\\sqrt{n}}+{\\sqrt{d}}$ and $d\\geq\\log{\\frac{4}{\\delta}}$ then ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(\\sigma_{1}(G)\\geq2C_{1}(\\sqrt{n}+\\sqrt{d}))\\leq2\\exp(-d)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\displaystyle\\frac{\\delta}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "2. To derive a lower bound on $\\sigma_{n}(G)$ we use the following fact (see Rudelson & Vershynin, 2009, Theorem 1.1). There exist constants $C_{1},C_{2}>0$ such that, for any $\\epsilon>0$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\sigma_{n}(G)\\leq\\epsilon(\\sqrt{d}-\\sqrt{n-1}))\\leq(C_{1}\\epsilon)^{d-n+1}+e^{-C_{2}d}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Let $\\begin{array}{r}{\\epsilon=\\frac{1}{C_{1}e}}\\end{array}$ and let $\\begin{array}{r}{d\\ge2n+\\left(2+\\frac{1}{C_{2}}\\right)\\log\\frac{4}{\\delta}}\\end{array}$ . Then ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(\\sigma_{n}(\\pmb{A})\\leq\\epsilon(\\sqrt{d}-\\sqrt{n}))\\leq\\exp(-d/2)+\\exp(-C_{2}d)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\frac{\\delta}{4}+\\frac{\\delta}{4}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\frac{\\delta}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Hence both bounds hold simultaneously with probability at least $1-\\delta$ . ", "page_idx": 14}, {"type": "text", "text": "The next lemma formulates lower and upper bounds on the smallest and largest singular values of a noise matrix under our data model. ", "page_idx": 14}, {"type": "text", "text": "Lemma 4.1. Let $N\\in\\mathbb{R}^{n\\times d}$ denote a random matrix whose rows are drawn mutually i.i.d. from $\\begin{array}{r}{\\mathcal{N}(\\mathbf{0}_{d},\\frac{1}{d}(\\mathbf{{I}}_{d}-v\\mathbf{{v}}^{T}))}\\end{array}$ . If $\\begin{array}{r}{d=\\Omega\\left(n+\\log\\frac{1}{\\delta}\\right)}\\end{array}$ , then there exists constants $C_{1}$ and $C_{2}$ such that, with probability at least $1-\\delta$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\nC_{1}\\leq\\sigma_{n}(N)\\leq\\sigma_{1}(N)\\leq C_{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. Let $\\mathbb{H}=\\mathrm{span}(\\{\\pmb{v}\\})^{\\perp}\\,\\cong\\,\\mathbb{R}^{d-1}$ . Let $N^{\\prime}:\\mathbb{H}\\to\\mathbb{R}^{n}$ be a random matrix whose rows are drawn mutually i.i.d. from $\\mathcal{N}(\\mathbf{0}_{d},\\mathbf{\\calI}_{\\mathbb{H}})$ . Since $\\begin{array}{r}{d=\\Omega\\left(n+\\log\\frac{1}{\\delta}\\right)}\\end{array}$ , with probability at least $1-\\delta$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\nc(\\sqrt{d}-\\sqrt{n})\\leq\\sigma_{n}(N^{\\prime})\\leq\\sigma_{1}(N^{\\prime})\\leq C(\\sqrt{d}+\\sqrt{n}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We denote the above event by $\\omega$ . Let $\\pmb{J}:\\mathbb{H}\\rightarrow\\mathbb{R}^{d}$ be the inclusion map and let $\\pmb{P}=\\pmb{I}_{d}-\\pmb{v}\\pmb{v}^{T}$ . For any random vector $\\mathbfit{\\Delta}$ with distribution $\\mathcal{N}(\\mathbf{0}_{d},\\mathbf{\\calI}_{\\mathbb{H}})$ , $_{J n}$ is a Gaussian random vector with covariance matrix $J J^{T}=P$ . Therefore, $d^{-1/2}N^{\\prime}J^{T}$ is a random matrix whose rows are drawn mutually i.i.d. from $\\mathcal{N}(\\mathbf{0}_{d},d^{-1}P)$ . That is, $d^{-1/2}N^{\\prime}J^{T}$ is identically distributed to $_{N}$ . For the lower bound on the $n$ -th largest singular value, if $\\textstyle d\\geq{\\frac{4n}{c^{2}}}$ , then conditional on $\\omega$ we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\sigma_{n}(d^{-1/2}N^{\\prime}J^{T})=d^{-1/2}\\sigma_{\\mathrm{min}}(J N^{\\prime T})}&{}\\\\ {\\geq d^{-1/2}\\sigma_{\\mathrm{min}}(J)\\sigma_{\\mathrm{min}}(N^{\\prime T})}&{}\\\\ {=d^{-1/2}\\sigma_{\\mathrm{min}}(N^{\\prime T})}&{}\\\\ {=d^{-1/2}\\sigma_{n}(N^{\\prime})}&{}\\\\ {\\geq c-\\sqrt{\\frac{n}{d}}}&{}\\\\ {\\geq\\frac{c}{2}.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Note here we define $\\sigma_{\\mathrm{min}}$ to be the smallest singular value of a matrix. In the first line we used $J N^{\\prime T}$ is a linear map $\\mathbb{R}^{n}\\to\\mathbb{R}^{d}$ and $d\\geq n$ , and in the third line we used the fact that $_{J}$ is an inclusion map. For the upper bound on the largest singular value, if $d\\geq{\\frac{n}{C^{2}}}$ , then conditional on $\\omega$ we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sigma_{1}(d^{-1/2}N^{\\prime}J^{T})=d^{-1/2}\\sigma_{1}(J N^{\\prime T})}\\\\ &{\\qquad\\qquad\\qquad\\leq d^{-1/2}\\sigma_{1}(N^{\\prime T})}\\\\ &{\\qquad\\qquad\\qquad=d^{-1/2}\\sigma_{1}(N^{\\prime})}\\\\ &{\\qquad\\qquad\\qquad\\leq C+\\sqrt{\\frac{n}{d}}}\\\\ &{\\qquad\\qquad\\qquad\\leq2C.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Note here that again we used the fact that $_{J}$ is an inclusion map in the first line. Therefore, if d = \u2126 n + log \u03b41 ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(\\frac{c}{2}\\leq\\sigma_{n}(N)\\leq\\sigma_{1}(N)\\leq2C\\right)\\geq\\mathbb{P}(\\omega)}\\\\ {\\geq1-\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The following lemma is useful for constructing vectors in the noise subspace with properties suitable for bounding the norm of the max-margin solution. We remark that the same approach could be used in the setting where the noise and signal are not orthogonal by considering the pseudo-inverse $([N,v]^{T})^{\\dag}$ instead of $N^{\\dagger}$ . ", "page_idx": 15}, {"type": "text", "text": "Lemma B.2. Let $\\mathcal{T}\\subseteq[n]$ be an arbitrary subset such that $|{\\mathcal{T}}|\\,=\\,\\ell$ . In the context of the data model given in Definition 2.1, assume $\\begin{array}{r}{d=\\Omega\\left(n+\\log\\frac{1}{\\delta}\\right)}\\end{array}$ . Then there exists $z\\in\\mathbb{R}^{d}$ such that with probability at least $1-\\delta$ the following hold simultaneously. ", "page_idx": 15}, {"type": "text", "text": "Proof. Recall that $N\\in\\mathbb{R}^{n\\times d}$ is a random matrix whose rows are selected i.i.d. from $\\mathcal{N}(\\mathbf{0}_{d},d^{-1}(I_{d}-$ $v v^{T})$ ). By Lemma 4.1, with probability at least $1-\\delta$ we have ", "page_idx": 15}, {"type": "equation", "text": "$$\nc\\leq\\sigma_{n}(N)\\leq\\sigma_{1}(N)\\leq C.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Conditioning on this event, we will construct a vector $_{\\textit{z}}$ which satisfies the desired properties. Let $\\pmb{w}\\in\\mathbb{R}^{n}$ satisfy $w_{i}=\\hat{y}_{i}$ if $i\\in\\mathcal{Z}$ and $w_{i}=0$ otherwise. Let $z=N^{\\dag}w$ , where $N^{\\dag}=\\dot{N}^{T}(N N^{T})^{-1}$ is the right pseudo-inverse of $_{N}$ . Then $N z=w$ . In particular, for $i\\in\\mathcal{T}$ , $\\hat{y}_{i}\\langle n_{i},z\\rangle=\\hat{y}_{i}w_{i}=1$ , and for $\\not\\in\\mathcal{Z},\\,\\hat{y}_{i}\\langle\\pmb{n}_{i},\\pmb{z}\\rangle=\\hat{y}_{i}w_{i}=0$ . This establishes properties 1 and 2. Since $N^{\\dagger}w$ is in the span of the set $\\{n_{i}\\}_{i\\in[n]}$ , it is orthogonal to $\\pmb{v}$ . This establishes property 3. Finally, we can bound ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|z\\|=\\|N^{\\dagger}w\\|}\\\\ &{\\qquad\\le\\|N^{\\dagger}\\|\\|w\\|}\\\\ &{\\qquad=\\frac{\\|w\\|}{\\sigma_{n}(N)}}\\\\ &{\\qquad\\le\\frac{\\|w\\|}{c}}\\\\ &{\\qquad=\\frac{\\sqrt{\\ell}}{c}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\|\\boldsymbol{z}\\|=\\|N^{\\dagger}\\boldsymbol{w}\\|}}\\\\ &{}&{\\geq\\sigma_{n}(N^{\\dagger})\\|\\boldsymbol{w}\\|}\\\\ &{}&{=\\frac{\\|\\boldsymbol{w}\\|}{\\sigma_{1}(N)}}\\\\ &{}&{\\geq\\frac{\\|\\boldsymbol{w}\\|}{C}}\\\\ &{}&{=\\frac{\\sqrt{\\ell}}{C}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which establishes property 4. ", "page_idx": 15}, {"type": "text", "text": "With Lemma B.2 in place we are now able to appropriately bound the max-margin norm. ", "page_idx": 16}, {"type": "text", "text": "Lemma B.3. In the context of the data model given in Definition 2.1, let $\\pmb{w}^{*}$ denote the max-margin classifier of the training data $(X,{\\hat{y}})$ , which exists almost surely. If $\\begin{array}{r}{d=\\Omega\\left(n+\\log\\frac{1}{\\delta}\\right)}\\end{array}$ then with probability at least $1-\\delta$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|w^{*}\\|\\leq C\\sqrt{\\frac{1}{\\gamma}+\\frac{k}{1-\\gamma}},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $C>0$ is a constant. ", "page_idx": 16}, {"type": "text", "text": "Proof. Under the assumptions stated, the conditions of Lemma B.2 hold with probability at least $1-\\delta$ . Conditioning on this let ", "page_idx": 16}, {"type": "equation", "text": "$$\nw=\\frac{1}{\\sqrt{\\gamma}}v+\\frac{2}{\\sqrt{1-\\gamma}}z\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $_{z}$ is the vector constructed in Lemma B.2 with ${\\mathcal{T}}={\\mathcal{B}}$ . For any $i\\in[n]$ we therefore have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\hat{y_{i}}\\langle\\pmb{x}_{i},\\pmb{w}\\rangle=\\beta_{i}+2\\hat{y_{i}}\\langle\\pmb{n}_{i},\\pmb{z}\\rangle.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "As a result, for $i\\in\\mathcal G$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\hat{y_{i}}\\langle\\pmb{x}_{i},\\pmb{w}\\rangle=1+2\\hat{y_{i}}\\langle\\pmb{n}_{i},z\\rangle=1,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "while for $l\\in{\\mathcal{B}}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\hat{y_{i}}\\langle\\pmb{x}_{i},\\pmb{w}\\rangle=-1+2\\hat{y_{i}}\\langle\\pmb{n}_{i},\\pmb{z}\\rangle=1.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "As a result $\\hat{y}_{i}\\langle\\pmb{x}_{i},\\pmb{w}\\rangle=1$ for all $i\\in[n]$ . Furthermore, observe ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|\\pmb{w}\\|^{2}=\\frac{1}{\\gamma}+\\frac{4}{1-\\gamma}\\|\\pmb{z}\\|^{2}}\\\\ {\\displaystyle\\leq C\\left(\\frac{1}{\\gamma}+\\frac{k}{1-\\gamma}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for a universal constant $C$ . To conclude observe $\\|\\pmb{w}^{*}\\|\\leq\\|\\pmb{w}\\|$ by definition of being max-margin. ", "page_idx": 16}, {"type": "text", "text": "Lemma B.3 constructs a classifier with margin one using an appropriate linear combination of the signal vector $\\pmb{v}$ and a vector in the noise subspace which classifies all noise components belonging to bad points correctly. This bound is useful for the benign overftiting setting in which $\\gamma$ is not too small. However, for small $\\gamma$ , as is the case in the non-benign overfitting setting, this bound behaves poorly as the only way the construction can fti all the data points is by making the coefficient in front of the $\\pmb{v}$ component large. For the non-benign overfitting setting we therefore require a different approach and instead fit all data points based on their noise components alone. In particular, the following bound behaves better than that given in Lemma B.3 when $\\gamma$ approaches 0. ", "page_idx": 16}, {"type": "text", "text": "Lemma B.4. In the context of the data model given in Definition 2.1, let $\\pmb{w}^{*}$ denote the max-margin classifier of the training data $(X,{\\hat{y}})$ , which exists almost surely. If $\\begin{array}{r}{d=\\Omega\\left(n+\\log\\frac{1}{\\delta}\\right)}\\end{array}$ then with probability at least $1-\\delta$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\lvert|\\pmb{w}^{*}\\rvert|\\leq C\\sqrt{\\frac{n}{1-\\gamma}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. App\u221alying Lemma B.2 with $\\mathcal{T}=[n]$ then with probability $1-\\delta$ there exists $z\\in\\mathbb{R}^{d}$ such that $\\|z\\|=\\Theta({\\sqrt{n}})$ , $\\hat{y}_{i}\\langle n_{i},z\\rangle=1$ for all $i\\in[n]$ and $z\\perp v$ . Conditioning on this event, let ", "page_idx": 16}, {"type": "equation", "text": "$$\nw=\\frac{1}{\\sqrt{1-\\gamma}}z.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then for all $i\\in[n]$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\hat{y_{i}}\\langle\\pmb{x}_{i},\\pmb{w}\\rangle=\\hat{y_{i}}\\langle\\pmb{n}_{i},\\pmb{z}\\rangle=1.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Furthermore, there exists a constant $C>0$ such that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|w\\|\\leq C{\\sqrt{\\frac{n}{1-\\gamma}}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "To conclude observe $\\|\\pmb{w}^{*}\\|\\leq\\|\\pmb{w}\\|$ by definition of being max-margin. ", "page_idx": 16}, {"type": "text", "text": "Appendix C Linear models ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "C.1 Sufficient conditions for benign and harmful overfitting ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We start by providing a lemma which characterizes the generalization properties of linear classifiers. Lemma C.1. In the context of the data model given in Definition 2.1, consider the linear classifier $\\pmb{w}\\,=\\,a_{v}\\pmb{v}+\\pmb{z}$ , where $a_{v}\\in\\mathbb{R}$ and $\\langle z,v\\rangle\\,=\\,{\\bar{0}}$ . If $a_{v}\\;\\geq\\;0,$ , then the generalization error can be bounded as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}(y\\langle\\pmb{w},\\pmb{x}\\rangle\\le0)\\le\\exp\\left(-\\frac{d}{2}\\frac{\\gamma}{1-\\gamma}\\frac{a_{v}^{2}}{\\|z\\|^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}(y\\langle w,x\\rangle\\leq0)\\geq\\operatorname*{max}\\left\\{\\frac{1}{2}-\\sqrt{\\frac{d\\gamma}{2\\pi(1-\\gamma)}}\\frac{a_{v}}{\\|z\\|},\\frac{1}{4}\\exp\\left(-\\frac{6d}{\\pi}\\frac{\\gamma}{1-\\gamma}\\frac{a_{v}^{2}}{\\|z\\|^{2}}\\right)\\right\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. Recall from Definition 2.1 that a test pair $(\\pmb{x},y)$ satisfies ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\pmb{x}=y(\\sqrt{\\gamma}\\pmb{v}+\\sqrt{1-\\gamma}\\pmb{n}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\boldsymbol{n}\\sim\\mathcal{N}(\\mathbf{0}_{d},d^{-1}(I_{d}-\\pmb{v}\\pmb{v}^{T}))$ is a random vector. Let $X=y\\langle{\\pmb w},{\\pmb x}\\rangle$ , so ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{X=\\langle a_{v}v+z,\\sqrt{\\gamma}v+\\sqrt{1-\\gamma}n\\rangle}}\\\\ {{=\\sqrt{\\gamma}a_{v}+\\sqrt{1-\\gamma}\\langle n,z\\rangle.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then $X$ is a Gaussian random variable with expectation ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[X]=\\sqrt{\\gamma}a_{v}+\\sqrt{1-\\gamma}\\mathbb{E}[\\langle n,z\\rangle]}\\\\ {=\\sqrt{\\gamma}a_{v}\\quad\\quad\\quad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and variance ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Var}(X)=(1-\\gamma)\\mathrm{Var}(\\langle n,z\\rangle)}\\\\ &{\\quad\\quad\\quad=(1-\\gamma)\\mathbb{E}[z^{T}n n^{T}z]}\\\\ &{\\quad\\quad\\quad=\\frac{1-\\gamma}{d}z^{T}(I_{d}-v v^{T})z}\\\\ &{\\quad\\quad\\quad=\\frac{1-\\gamma}{d}z^{T}z}\\\\ &{\\quad\\quad\\quad=\\frac{(1-\\gamma)\\|z\\|^{2}}{d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By Hoeffding\u2019s inequality, for all $t\\geq0$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}(X\\leq\\sqrt{\\gamma}a_{v}-t)\\leq\\exp\\left(-\\frac{t^{2}d}{2(1-\\gamma)\\|z\\|^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Setting $t=\\sqrt{\\gamma}a_{v}$ , we obtain ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}(X\\le0)\\le\\exp\\left(-\\frac{\\gamma d a_{v}^{2}}{2(1-\\gamma)\\|z\\|^{2}}\\right),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which establishes the upper bound on the generalization error. ", "page_idx": 17}, {"type": "text", "text": "To prove the lower bound, we integrate a standard Gaussian pdf: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(X\\leq0)=\\mathbb{P}\\left(\\frac{X-\\sqrt{\\gamma}a_{\\upsilon}}{\\sqrt{1-\\gamma}\\|z\\|/\\sqrt{d}}\\leq-\\sqrt{\\frac{\\gamma d}{1-\\gamma}}\\frac{a_{\\upsilon}}{\\|z\\|}\\right)}\\\\ &{\\qquad\\qquad=\\frac{1}{2}-\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\sqrt{\\frac{\\gamma d}{1-\\gamma}}\\frac{a_{\\upsilon}}{\\|z\\|}}^{0}e^{-t^{2}/2}d t}\\\\ &{\\qquad\\qquad\\geq\\frac{1}{2}-\\frac{1}{\\sqrt{2\\pi}}\\left(\\sqrt{\\frac{\\gamma d}{1-\\gamma}}\\frac{a_{\\upsilon}}{\\|z\\|}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Another bound can be obtained using the following inequality (P\u00f3lya, 1949, (1.5)): ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{1}{\\sqrt{2\\pi}}\\int_{0}^{x}e^{-t^{2}/2}d t\\leq\\sqrt{1-e^{-2x^{2}/\\pi}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We proceed ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(X\\leq0)=\\frac{1}{2}-\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\sqrt{\\frac{x+t}{1-\\gamma}}\\frac{a_{x}}{\\Vert x\\Vert}}^{0}e^{-t^{2}/2}d t}\\\\ &{\\phantom{\\frac{1}{2}-\\frac{1}{\\sqrt{2\\pi}}\\int_{0}^{\\sqrt{\\frac{x+t}{1-\\gamma}}\\frac{a_{x}}{\\Vert x\\Vert}}e^{-t^{2}/2}d t}}\\\\ &{\\phantom{\\frac{1}{2}-\\frac{1}{2}-\\frac{1}{2}\\sqrt{1-\\exp\\left(-\\frac{2\\gamma d a_{x}^{2}}{\\pi(1-\\gamma)\\Vert\\varepsilon\\Vert^{2}}\\right)}}\\\\ &{\\phantom{\\frac{1}{2}-\\frac{1}{2}-\\frac{1}{2}\\left(1-\\frac{1}{2}\\exp\\left(-\\frac{2\\gamma d a_{x}^{2}}{\\pi(1-\\gamma)\\Vert\\varepsilon\\Vert^{2}}\\right)\\right)}}\\\\ &{\\geq\\frac{1}{4}\\exp\\left(-\\frac{2\\gamma d a_{x}^{2}}{\\pi(1-\\gamma)\\Vert\\varepsilon\\Vert^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The following result establishes benign and non-benign overfitting for linear models and data as per Definition 2.1, with a phase transition between these outcomes depending on the signal to noise parameter $\\gamma$ . ", "page_idx": 18}, {"type": "text", "text": "Theorem C.2. In the context of the data model described in Section 2, let $\\pmb{w}^{*}$ be a max-margin linear classifier of the training data. Let $\\mathcal{A}:\\mathbb{R}^{n\\times d}\\times\\{\\pm1\\}^{n}\\rightarrow\\mathbb{R}^{d}$ be a learning algorithm which is approximately margin-maximizing, Definition 2.3. For $\\delta\\in(0,1]$ , let $\\begin{array}{r}{d=\\Omega\\left(n+\\log\\frac{1}{\\delta}\\right)}\\end{array}$ . Then with probability at least $1-\\delta$ over the randomness of the training data $(X,{\\hat{y}})$ , the following hold with $\\epsilon$ denoting the generalization error of $\\mathcal{A}(X,\\hat{\\pmb{y}})$ . ", "page_idx": 18}, {"type": "text", "text": "Proof. For train\u221aing data $(X,{\\hat{y}})$ let $\\pmb{w}\\,=\\,\\mathcal{A}(\\pmb{X},\\hat{\\pmb{y}})$ be the learned linear classifier. First, recall $\\pmb{x}_{i}=\\sqrt{\\gamma}y_{i}\\pmb{v}+\\sqrt{1-\\gamma}\\pmb{n}_{i}$ for all $i\\in[n]$ , $\\|\\pmb{v}\\|=1$ , and observe that we can decompose the vector $\\pmb{w}$ as $\\pmb{w}=a_{v}\\pmb{v}+\\pmb{z}$ , where $a_{v}\\in\\mathbb{R}$ , and $\\textit{z}\\perp\\textit{\\textbf{v}}$ . As a result, for each $i\\in[n]$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\hat{y}_{i}\\langle\\pmb{x}_{i},\\pmb{w}\\rangle=\\sqrt{\\gamma}a_{v}\\beta_{i}+\\sqrt{1-\\gamma}\\hat{y}_{i}\\langle\\pmb{n}_{i},\\pmb{z}\\rangle.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "First we establish $(A)$ . As $\\begin{array}{r}{d=\\Omega\\left(n+\\log\\frac{1}{\\delta}\\right)}\\end{array}$ , Lemmas 4.1 and B.3 show that with probability at least $\\textstyle1-{\\frac{\\delta}{2}}$ , $\\|N\\|^{2},\\|N_{\\mathcal{G}}\\|^{2},\\|N_{\\mathcal{B}}\\|\\le C$ and $\\begin{array}{r}{\\|\\pmb{w}^{*}\\|\\leq C\\sqrt{\\frac{1}{\\gamma}+\\frac{k}{1-\\gamma}}}\\end{array}$ . Here $N_{\\mathcal{G}}$ and $N_{B}$ denote the matrices formed by taking only the rows of $_{N}$ which satisfy $\\beta=1$ and $\\beta=-1$ , respectively. We denote this event $\\omega$ and condition on it in all that follows for the proof of $(A)$ . As $\\boldsymbol{\\mathcal{A}}$ is approximately max margin then given equation 8 we have for all $i\\in\\mathcal G$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n1\\leq\\sqrt{\\gamma}a_{v}+\\sqrt{1-\\gamma}\\hat{y}_{i}\\langle{\\pmb n}_{i},z\\rangle.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Suppose that $\\sqrt{\\gamma}a_{v}\\,<\\,\\frac{1}{2}$ . Then the above inequality implies $\\sqrt{1-\\gamma}\\hat{y}_{i}\\langle{\\pmb n}_{i},z\\rangle\\,\\geq\\,\\frac{1}{2}$ for all $i\\in\\mathcal G$ . Squaring and then summing this expression over all $i\\in\\mathcal G$ it follows that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{n-k}{4}\\le(1-\\gamma)\\sum_{i\\in\\mathcal{G}}|\\langle\\pmb{n}_{i},z\\rangle|^{2}}}\\\\ &{}&{\\le(1-\\gamma)\\|\\ensuremath{N_{\\mathcal{G}}}\\|^{2}}\\\\ &{}&{\\le(1-\\gamma)\\|\\ensuremath{N_{\\mathcal{G}}}\\|^{2}\\|z\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since $\\boldsymbol{\\mathcal{A}}$ is approximately margin-maximizing, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{n-k}{4}\\leq(1-\\gamma)C\\|z\\|^{2}}\\\\ &{\\qquad\\leq(1-\\gamma)C\\|w\\|^{2}}\\\\ &{\\qquad\\leq C|A|^{2}(1-\\gamma)\\|w^{*}\\|^{2}}\\\\ &{\\qquad\\leq C|A|^{2}(1-\\gamma)\\left(\\displaystyle\\frac{1}{\\gamma}+\\displaystyle\\frac{k}{1-\\gamma}\\right)}\\\\ &{\\qquad\\leq\\frac{C|A|^{2}}{\\gamma}+C|A|^{2}k,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which further implies $\\begin{array}{r}{n\\leq\\frac{C|A|^{2}}{\\gamma}+C|A|^{2}k}\\end{array}$ for some other constant $C$ . For this inequality to hold, either C|A|2 $\\begin{array}{r}{\\frac{C|A|^{2}}{\\gamma}\\geq\\frac{n}{2}}\\end{array}$ or $C|A|^{2}k\\geq\\frac{n}{2}$ . With $\\begin{array}{r}{k\\leq\\frac{n}{4C|A|^{2}}}\\end{array}$ and $\\gamma\\geq\\frac{1}{k}$ , neither of these can be true and therefore we conclude that $\\sqrt{\\gamma}a_{v}\\ge\\frac{1}{2}$ . Then ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{a_{\\nu}^{2}}{\\|z\\|^{2}}\\geq\\frac{1}{4\\gamma\\|z\\|^{2}}}\\\\ &{\\qquad\\geq\\frac{1}{4\\gamma\\|w\\|^{2}}}\\\\ &{\\qquad\\geq\\frac{1}{4\\gamma|A|^{2}\\|w^{*}\\|^{2}}}\\\\ &{\\qquad\\geq\\frac{C}{\\|A\\|^{2}\\gamma\\frac{1}{\\gamma}+\\frac{k}{1-\\gamma}}}\\\\ &{\\qquad\\geq\\frac{C}{2|A|^{2}\\gamma}\\frac{1-\\gamma}{k}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for a positive constant $C$ . Letting $(\\pmb{x},y)$ denote a test point pair, then by Lemma C.1 it follows that for a different constant $C$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{P}(y\\langle{\\pmb w},{\\pmb x}\\rangle\\le0\\mid\\omega)\\le\\exp\\left(-\\frac{d}{2}\\frac{\\gamma}{1-\\gamma}\\frac{a_{v}^{2}}{\\|{\\pmb z}\\|^{2}}\\right)}\\\\ {\\le\\exp\\left(-\\frac{C d}{|{\\pmb A}|^{2}k}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Hence the generalization error is at most $\\epsilon$ when $\\omega$ occurs, which happens with probability at least $1-\\delta$ . This establishes the upper bound of $(A)$ . ", "page_idx": 19}, {"type": "text", "text": "For the lower bound of $(A)$ , since $\\pmb{w}$ is a linear classifier, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\langle n_{i},z\\rangle\\geq\\frac{1}{\\sqrt{1-\\gamma}}(1+a_{v}\\sqrt{\\gamma})\\geq a_{v}\\sqrt{\\frac{\\gamma}{1-\\gamma}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for all $i\\in{\\cal B}$ , from which we conclude $|\\langle n_{i},z\\rangle|\\geq a_{v}\\sqrt{\\gamma}$ . This implies ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\lvert|N_{B}z\\rvert|\\geq a_{V}\\sqrt{\\frac{k\\gamma}{1-\\gamma}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Along with $\\|N_{B}z\\|\\leq\\|N_{B}\\|\\|z\\|\\leq C\\|z\\|$ we conclude ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|z\\|\\geq{\\frac{a_{v}}{C}}{\\sqrt{\\frac{k\\gamma}{1-\\gamma}}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "With this bound we then bound ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\frac{a_{v}}{\\|z\\|}}\\leq C\\cdot{\\sqrt{\\frac{1-\\gamma}{k\\gamma}}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By Lemma C.1 we then can bound ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{P}(y\\langle\\pmb{w},\\pmb{x}\\rangle\\leq0\\mid\\omega)\\geq\\frac{1}{4}\\exp\\left(-\\frac{6d}{\\pi}\\frac{\\gamma}{1-\\gamma}\\frac{a_{v}^{2}}{\\|z\\|^{2}}\\right)}\\\\ {\\displaystyle\\geq\\exp\\left(-\\frac{C d}{k}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for a new constant $C$ , provided $a_{v}$ is positive. In the last line we can bound $\\frac{d}{k}$ below as $d\\geq n$ and $k=O(n)$ . If $a_{v}$ is negative then the generalization error is at least $\\frac{1}{2}$ , which is still bounded below by $\\exp(-C d/k)$ . ", "page_idx": 20}, {"type": "text", "text": "We now turn our attention to $(B)$ . As $\\begin{array}{r}{d=\\Omega(n+\\log\\frac{1}{\\delta})}\\end{array}$ , from Lemmas 4.1 and B.4 with probability at least 1 \u2212\u03b4 it holds that \u2225NG\u22252 \u2264C and \u2225w\u2217\u2225\u2264\u221aC1\u2212n\u03b3 . We denote this event $\\omega^{\\prime}$ and condition on it in all that follows for the proof of $(B)$ . In particular, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{a_{v}^{2}+\\|z\\|^{2}=\\|w\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq|A|^{2}\\|w^{*}\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\displaystyle\\frac{C|A|^{2}n}{1-\\gamma}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For all $i\\in[n]$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{1\\leq\\sqrt{\\gamma}\\beta_{i}a_{v}+\\sqrt{1-\\gamma}\\hat{y_{i}}\\langle\\pmb{n}_{i},z\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For this inequality to hold, either $|\\sqrt{\\gamma}a_{v}|\\;\\geq\\;1/2$ or $\\sqrt{1-\\gamma}\\hat{y_{i}}\\langle n_{i},z\\rangle\\,\\geq\\,1/2$ for all $i\\;\\in\\;[n]$ . If $|\\sqrt{\\gamma}a_{v}|\\ge1/2$ , then with $\\begin{array}{r}{\\gamma\\le\\frac{1}{4(C+1)|A|^{2}d}}\\end{array}$ we have ", "page_idx": 20}, {"type": "equation", "text": "$$\na_{v}^{2}\\geq2C|A|^{2}d\\geq2C|A|^{2}n.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "However, from equation 9 we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n2C|A|^{2}n>\\frac{C|A|^{2}n}{1-\\gamma}\\geq a_{v}^{2}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which is a contradictio\u221an. Therefore, under the regime specified there exists a $C>0$ such that with $\\begin{array}{r}{\\gamma\\leq\\frac{1}{C|A|^{2}d}}\\end{array}$ , we have $\\sqrt{1-\\gamma}\\hat{y_{i}}\\langle{\\pmb n}_{i},{\\pmb z}\\rangle\\geq1/2$ for all $i\\in[n]$ . Rearranging, squaring and summing over all $i\\in[n]$ yields ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{n}{4(1-\\gamma)}\\leq\\sum_{i=1}^{n}|\\langle\\pmb{n}_{i},\\pmb{z}\\rangle|^{2}\\leq\\|\\pmb{N}\\pmb{z}\\|^{2}\\leq C\\|\\pmb{z}\\|^{2},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the final inequality follows from conditioning on $\\omega$ . Then ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{a_{v}^{2}}{\\|z\\|^{2}}\\leq\\frac{\\|w\\|^{2}}{\\|z\\|^{2}}}\\\\ &{\\qquad\\leq\\frac{|A|^{2}\\|w^{*}\\|^{2}}{\\|z\\|^{2}}}\\\\ &{\\qquad\\leq\\frac{C|A|^{2}\\frac{n}{1-\\gamma}}{\\frac{n}{1-\\gamma}}}\\\\ &{\\qquad\\leq C|A|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the constant $C>0$ may vary between inequalities. Letting $(\\boldsymbol{x},\\boldsymbol{y})$ denote a test point pair, by Lemma C.1 it follows that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(y\\langle\\pmb{w},\\pmb{x}\\rangle\\leq0\\mid\\omega)\\geq\\displaystyle\\frac{1}{2}-\\sqrt{\\frac{d\\gamma}{2\\pi(1-\\gamma)}}\\frac{a_{v}}{\\|\\pmb{z}\\|}}\\\\ &{\\qquad\\qquad\\qquad\\geq\\displaystyle\\frac{1}{2}-\\sqrt{\\frac{C d\\gamma|\\pmb{A}|^{2}}{1-\\gamma}}}\\\\ &{\\qquad\\qquad\\geq\\displaystyle\\frac{1}{2}-\\sqrt{C d\\gamma|\\pmb{A}|^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Hence the generalization error is at least $\\textstyle{\\frac{1}{2}}\\,-\\,{\\sqrt{C d\\gamma|A|^{2}}}$ when $\\omega^{\\prime}$ occurs, which happens with probability at least $1-\\delta$ . This establishes $(B)$ . \u53e3 ", "page_idx": 20}, {"type": "text", "text": "Appendix D Leaky ReLU Networks ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section we consider a leaky ReLU network $f:\\mathbb{R}^{2m}\\times\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ with forward pass given by ", "page_idx": 21}, {"type": "equation", "text": "$$\nf(W,x)=\\sum_{j=1}^{2m}(-1)^{j}\\sigma(\\langle{\\pmb w}_{j},{\\pmb x}_{i}\\rangle),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\sigma(z)\\,=\\,\\operatorname*{max}(\\alpha z,z)$ for some $\\alpha\\,\\in\\,(0,1)$ . For any such network, we may decompose the neuron weights ${\\pmb w}_{j}$ into a signal component and a noise component, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\pmb{w}_{j}=a_{j}\\pmb{v}+z_{j},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $a_{j}\\in\\mathbb{R}$ and $z_{j}\\in\\mathbb{R}^{d}$ satisfies $z_{j}\\perp v$ . The ratio $a_{j}/\\lVert z_{j}\\rVert$ therefore grows with the alignment of ${\\pmb w}_{j}$ with the signal and shrinks if ${\\pmb w}_{j}$ instead aligns more with the noise. Collecting the noise components of the weight vectors, let $\\bar{Z}\\in\\mathbb{R}^{(2m)\\times d}$ be the matrix whose $j$ -th row is $z_{j}$ . In order to track the alignment of the network as a whole with the signal versus noise subspaces we introduce the following quantities. Let ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{A_{1}=f({\\cal W},{\\pmb v})=\\displaystyle\\sum_{j=1}^{2m}(-1)^{j}\\sigma(a_{j}),}}\\\\ {{A_{-1}=f({\\cal W},-{\\pmb v})=\\displaystyle\\sum_{j=1}^{2m}(-1)^{j+1}\\sigma(-a_{j})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "be referred to as the positive and negative signal activation of the network respectively. Moreover, define ", "page_idx": 21}, {"type": "equation", "text": "$$\nA_{\\mathrm{min}}=\\operatorname*{min}(A_{1},A_{-1})\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "as the worst-case signal activation of the network, and ", "page_idx": 21}, {"type": "equation", "text": "$$\nA_{\\mathrm{lin}}=\\sum_{j=1}^{2m}(-1)^{j}a_{j}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "as the linearized network activation. To measure the amount of noise the network learns we define ", "page_idx": 21}, {"type": "equation", "text": "$$\nz_{\\mathrm{lin}}=\\sum_{j=1}^{2m}(-1)^{j}z_{j}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "D.1 Training dynamics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Theorem 3.1. Let $f:\\mathbb{R}^{p}\\times\\mathbb{R}^{n}\\to\\mathbb{R}$ be a leaky ReLU network with forward pass as defined by equation 1. Suppose the step size \u03b7 and initialization condition $\\lambda$ satisfy Assumption 1. Then for any linearly separable data set $(\\boldsymbol{X},\\hat{\\boldsymbol{y}})\\,\\boldsymbol{A}_{G D}(\\boldsymbol{X},\\hat{\\boldsymbol{y}},\\eta,\\lambda)$ converges after $T$ iterations, where ", "page_idx": 21}, {"type": "equation", "text": "$$\nT\\leq\\frac{C\\|\\pmb{w}^{*}\\|^{2}}{\\eta\\alpha^{2}m}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Furthermore $A_{G D}$ is approximately margin maximizing on $f$ (Definition 2.3) with ", "page_idx": 21}, {"type": "equation", "text": "$$\n|{\\mathcal{A}}_{G D}|\\leq{\\frac{C}{\\alpha{\\sqrt{m}}}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. Our approach is to adapt a classical technique used for the proof of convergence of the Perceptron algorithm for linearly separable data. This is also the approach adopted by Brutzkus et al. (2018). The key idea of the proof is to bound in terms of the number of updates both the norm of the learned vector $\\mathbf{\\nabla}w$ as well as its alignment with any linear separator of the data. From the Cauchy-Schwarz inequality these bounds cannot cross, and this in turn bounds the number of updates that can occur. Analogously, we track the alignment of $\\mathbf{\\nabla}W^{(t)}$ with the max-margin classifier along with the Frobenius norm of the $\\mathbf{\\nabla}W^{(t)}$ . To this end denote ", "page_idx": 21}, {"type": "equation", "text": "$$\nG(t)=\\|W_{j}^{(t)}\\|_{F}^{2}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and ", "page_idx": 22}, {"type": "equation", "text": "$$\nF(t)=\\sum_{j=1}^{2m}(-1)^{j}\\langle{\\pmb w}_{j}^{(t)},{\\pmb w}^{*}\\rangle,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\pmb{w}^{*}$ is a max-margin linear classifier of the dataset. Recall that $\\mathcal{F}^{(t)}\\ =\\ \\{i\\ \\in\\ [n]\\ :$ $\\hat{y}_{i}f(W^{(t)},\\pmb{x}_{i})\\,<\\,1\\}$ denotes the number of active data points at training step $t$ . We also define $\\begin{array}{r}{U(t)=\\sum_{s=0}^{t-1}|\\mathcal{F}^{(s)}|}\\end{array}$ to be the number of data point updates between iterations 0 and $t$ . First, by Cauchy-Schwarz ", "page_idx": 22}, {"type": "equation", "text": "$$\n1\\leq\\langle\\pmb{w}^{*},\\pmb{x}_{i}\\rangle\\leq\\|\\pmb{w}^{*}\\|\\cdot\\|\\pmb{x}_{i}\\|\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for all $i\\in[n]$ . Therefore, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|\\pmb{w}^{*}\\|\\geq\\frac{1}{\\operatorname*{min}_{i\\in[n]}\\|\\pmb{x}_{i}\\|}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By Assumption 1, for all $j\\in[2m]$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|\\pmb{w}_{j}^{(0)}\\|\\leq\\frac{\\sqrt{\\alpha}}{m\\operatorname*{min}_{i\\in[n]}\\|\\pmb{x}_{i}\\|}\\leq\\frac{\\|\\pmb{w}^{*}\\|}{\\alpha m}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For all $t\\geq0$ , the update rule of GD implies ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{Y}(t+1)=\\sum_{j=1}^{2m}\\|w_{j}^{(t+1)}\\|^{2}}\\\\ {\\displaystyle=\\sum_{j=1}^{2m}\\left\\|w_{j}^{(t)}+\\eta(-1)^{j}\\sum_{i\\in\\mathcal{F}^{(t)}}\\hat{\\sigma}(\\langle w_{j}^{(t)},x_{i}\\rangle)\\hat{y}_{i}x_{i}\\right\\|^{2}}\\\\ {\\displaystyle=\\sum_{j=1}^{2m}\\|w_{j}^{(t)}\\|^{2}+2\\eta\\sum_{j=1}^{2m}\\sum_{i\\in\\mathcal{F}^{(t)}}(-1)^{j}\\hat{\\sigma}(\\langle w_{j}^{(t)},x_{i}\\rangle)\\hat{y}_{i}\\langle w_{j}^{(t)},x_{i}\\rangle+\\eta^{2}\\sum_{j=1}^{2m}\\sum_{i,l\\in\\mathcal{F}^{(t)}}\\hat{\\sigma}(\\langle w_{j}^{(t)},x_{i}\\rangle)}\\\\ {\\displaystyle\\le\\sum_{j=1}^{2m}\\|w_{j}^{(t)}\\|^{2}+2\\eta\\sum_{j=1}^{2m}\\sum_{i\\in\\mathcal{F}^{(t)}}(-1)^{j}\\hat{\\sigma}(\\langle w_{j}^{(t)},x_{i}\\rangle)\\hat{y}_{i}\\langle w_{j}^{(t)},x_{i}\\rangle+2m\\eta^{2}|\\mathcal{F}^{(t)}|^{2}\\operatorname*{max}_{i\\in[n]}\\|x_{i}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Observe that for all $z\\in\\mathbb{R}$ , $\\boldsymbol{\\sigma}(s)=\\dot{\\boldsymbol{\\sigma}}(z)z$ , so can rewrite the second term of the above expression as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\eta\\displaystyle\\sum_{j=1}^{2m}\\sum_{i\\in\\mathcal{F}^{(t)}}(-1)^{j}\\dot{\\sigma}(\\langle w_{j}^{(t)},x_{i}\\rangle)\\hat{y}_{i}\\langle w_{j}^{(t)},x_{i}\\rangle=2\\eta\\displaystyle\\sum_{j=1}^{2m}\\sum_{i\\in\\mathcal{F}^{(t)}}(-1)^{j}\\sigma(\\langle w_{j}^{(t)},x_{i}\\rangle)\\hat{y}_{i}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=2\\eta\\displaystyle\\sum_{i\\in\\mathcal{F}^{(t)}}\\hat{y}_{i}f(W^{(t)},x_{i})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad<2\\eta\\displaystyle\\sum_{i\\in\\mathcal{F}^{(t)}}1}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=2\\eta|\\mathcal{F}^{(t)}|}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the inequality in the second-to-last line follows as we are summing over ${\\mathcal F}^{(t)}$ , which by definition consists of the $i\\in[n]$ such that $\\hat{y}_{i}f(W^{(t)},\\pmb{x}_{i})<1$ . As a result we obtain ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{G(t+1)\\le\\sum_{j=1}^{2m}\\|\\pmb{w}_{j}^{(t)}\\|^{2}+2\\eta|\\mathcal{F}^{(t)}|+2m\\eta^{2}|\\mathcal{F}^{(t)}|^{2}\\operatorname*{max}_{i\\in[n]}\\|\\pmb{x}_{i}\\|^{2}}}\\\\ &{}&{=G(t)+2\\eta|\\mathcal{F}^{(t)}|+2m\\eta^{2}|\\mathcal{F}^{(t)}|^{2}\\operatorname*{max}_{i\\in[n]}\\|\\pmb{x}_{i}\\|^{2}}\\\\ &{}&{\\le G(t)+4\\eta|\\mathcal{F}^{(t)}|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the last line follows since ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\eta\\leq\\frac{1}{m n\\operatorname*{max}_{i\\in[n]}\\|\\pmb{x}_{i}\\|^{2}}\\leq\\frac{1}{|\\mathcal{F}^{(t)}|m\\operatorname*{max}_{i\\in[n]}\\|\\pmb{x}_{i}\\|^{2}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By equation 10, the initialization satisfies ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{G(0)=\\sum_{j=1}^{2m}\\|w_{j}^{(0)}\\|^{2}}}\\\\ &{}&{\\leq\\sum_{j=1}^{2m}\\frac{\\|w^{*}\\|^{2}}{\\alpha^{2}m^{2}}}\\\\ &{}&{=\\frac{2\\|w^{*}\\|^{2}}{\\alpha^{2}m}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "So by induction, for all $t\\geq0$ ", "page_idx": 23}, {"type": "equation", "text": "$$\nG(t)\\leq\\frac{2\\|\\pmb{w}^{*}\\|^{2}}{\\alpha^{2}m}+3\\eta\\sum_{s=0}^{t-1}|\\mathcal{F}^{(s)}|=\\frac{2\\|\\pmb{w}^{*}\\|^{2}}{\\alpha^{2}m}+3\\eta U(t).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Next we find a bound for $F(t)$ . For all $t\\geq0$ then by definition of the GD update ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle F(t+1)=\\sum_{j=1}^{2m}(-1)^{j}\\langle w_{j}^{(t+1)},w^{*}\\rangle}}\\\\ {{\\displaystyle\\qquad=\\sum_{j=1}^{2m}(-1)^{j}\\langle w_{j}^{(t)},w^{*}\\rangle+\\eta\\sum_{j=1}^{2m}\\sum_{i\\in\\mathcal{F}^{(t)}}\\dot{\\sigma}(\\langle w_{j}^{(t)},x_{i}\\rangle)\\hat{y}_{i}\\langle w^{*},x_{i}\\rangle}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Since $\\hat{y}_{i}\\langle\\pmb{w}^{*},\\pmb{x}_{i}\\rangle\\geq1$ for all $i\\in[n]$ , the above expression is bounded below by ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{j=1}^{2m}(-1)^{j}\\langle w_{j}^{(t)},w^{*}\\rangle+\\eta\\sum_{j=1}^{2m}\\sum_{i\\in\\mathcal{F}^{(t)}}\\dot{\\sigma}(\\langle w_{j}^{(t)},\\pmb{x}_{i}\\rangle)\\hat{y}_{i}=F(t)+\\eta\\sum_{j=1}^{2m}\\sum_{i\\in\\mathcal{F}^{(t)}}\\dot{\\sigma}(\\langle w_{j}^{(t)},\\pmb{x}_{i}\\rangle)}}\\\\ &{}&{\\geq F(t)+\\eta\\sum_{j=1}^{2m}\\sum_{i\\in\\mathcal{F}^{(t)}}\\alpha}\\\\ &{}&{\\geq F(t)+2\\eta m\\alpha|\\mathcal{F}^{(t)}|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Hence unrolling the update for GD for all $t\\geq0$ it follows that ", "page_idx": 23}, {"type": "equation", "text": "$$\nF(t+1)\\geq F(0)+2\\eta m\\alpha\\sum_{s=0}^{t-1}|\\mathcal{F}^{(s)}|.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "At initialization, by equation 10 then ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle F(0)=\\sum_{j=1}^{2m}(-1)^{j}\\langle w_{j}^{(0)},w^{*}\\rangle}\\\\ {\\displaystyle\\ge-\\sum_{j=1}^{2m}\\|w_{j}^{(0)}\\|\\cdot\\|w^{*}\\|}\\\\ {\\displaystyle\\ge-\\sum_{j=1}^{2m}\\frac{\\|w^{*}\\|^{2}}{\\alpha m}}\\\\ {\\displaystyle=-\\frac{2\\|w^{*}\\|^{2}}{\\alpha}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore by induction, for all $t\\geq0$ we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{F(t)\\geq-\\frac{2\\|\\pmb{w}^{*}\\|^{2}}{\\alpha}+2\\eta m\\alpha\\displaystyle\\sum_{s=0}^{t-1}|\\mathcal{F}^{(s)}|}}\\\\ &{}&{=-\\frac{2\\|\\pmb{w}^{*}\\|^{2}}{\\alpha}+2\\eta m\\alpha U(t).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Combining our bounds for $F(t)$ and $G(t)$ , we obtain ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{-\\frac{2\\|w^{*}\\|^{2}}{\\alpha}+2\\eta m a U(t)\\leq F(t)}&{}\\\\ &{=\\frac{2m}{\\sum_{j=1}^{m}(-1)^{j}\\langle w_{j}^{(t)},w^{*}\\rangle}}\\\\ &{\\leq\\|w^{*}\\|\\frac{2m}{j-1}\\|w_{j}^{(t)}\\|}\\\\ &{\\leq\\|w^{*}\\|\\left(2\\frac{2m}{\\int_{-1}^{\\infty}\\|w_{j}^{(t)}\\|^{2}}\\right)^{1/2}}\\\\ &{=\\|w^{*}\\|\\left(2m\\tilde{G}\\right)^{1/2}}\\\\ &{\\leq\\|w^{*}\\|\\left(\\frac{4\\|w^{*}\\|^{2}}{\\alpha^{2}}+6m\\eta{\\bar{U}}(t)\\right)^{1/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "This implies that either ", "page_idx": 24}, {"type": "equation", "text": "$$\n-\\frac{2\\|\\pmb{w}^{*}\\|^{2}}{\\alpha}+2\\eta m\\alpha U(t)\\leq0\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "or ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left(-\\frac{2\\|\\pmb{w}^{*}\\|^{2}}{\\alpha}+2\\eta m\\alpha{U}(t)\\right)^{2}\\leq\\|\\pmb{w}^{*}\\|^{2}\\left(\\frac{4\\|\\pmb{w}^{*}\\|^{2}}{\\alpha^{2}}+6m\\eta U(t)\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "If (12) holds, then ", "page_idx": 24}, {"type": "equation", "text": "$$\nU(t)\\leq\\frac{\\|\\pmb{w}^{*}\\|^{2}}{\\eta\\alpha^{2}m}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "If (13) holds, then rearranging yields ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{4\\eta^{2}m^{2}\\alpha^{2}U(t)^{2}\\le14\\|{\\pmb w}^{*}\\|^{2}\\eta m U(t)}}\\\\ {{U(t)\\le\\displaystyle\\frac{7\\|{\\pmb w}^{*}\\|^{2}}{2\\eta\\alpha^{2}m}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore, in both cases there exists a constant $C$ such that ", "page_idx": 24}, {"type": "equation", "text": "$$\nU(t)\\leq\\frac{C\\|\\pmb{w}^{*}\\|^{2}}{\\eta\\alpha^{2}m}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "This holds for all $t\\in\\mathbb{N}$ and therefore ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{t=0}^{\\infty}|\\mathcal{F}^{(t)}|\\leq\\frac{C\\|w^{*}\\|^{2}}{\\eta\\alpha^{2}m}<\\infty.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "This implies that there exists $s\\in\\mathbb{N}$ such that $\\vert\\mathcal{F}^{(s)}\\vert=0$ . Let $T\\in\\mathbb N$ be the minimal iteration such that $\\begin{array}{r}{|\\mathcal{F}^{(T)}|=0}\\end{array}$ . Then for all $i\\in[n]\\,\\hat{y}_{i}f({\\pmb W}^{(T)},{\\pmb x}_{i})\\geq1$ . So the network achieves zero loss and also has zero gradient at iteration $T$ . In particular, ", "page_idx": 24}, {"type": "equation", "text": "$$\nT=\\sum_{t=0}^{T-1}1\\leq\\sum_{t=0}^{T-1}|\\mathcal{F}^{(t)}|\\leq\\frac{C\\|\\pmb{w}^{*}\\|^{2}}{\\eta\\alpha^{2}m}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "To bound $|\\mathcal{A}_{G D}|$ we combine equations (14) and (11) to obtain ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{G(T)\\leq\\frac{2\\|\\pmb{w^{*}}\\|^{2}}{\\alpha^{2}m}+3\\eta U(t)}\\\\ &{\\qquad\\leq\\frac{2\\|\\pmb{w^{*}}\\|^{2}}{\\alpha^{2}m}+\\frac{C\\|\\pmb{w^{*}}\\|^{2}}{\\eta\\alpha^{2}m}}\\\\ &{\\qquad\\leq\\frac{C\\|\\pmb{w^{*}}\\|^{2}}{\\alpha^{2}m}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "As a result for all linearly separable datasets $(X,{\\hat{y}})$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n{\\frac{\\|\\pmb{W}\\|_{F}}{\\|\\pmb{w}^{*}\\|}}={\\frac{C}{\\alpha{\\sqrt{m}}}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and therefore ", "page_idx": 25}, {"type": "equation", "text": "$$\n|{\\mathcal{A}}_{G D}|\\leq{\\frac{C}{\\alpha{\\sqrt{m}}}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "as claimed. ", "page_idx": 25}, {"type": "text", "text": "The training dynamics of gradient descent also give us the following result relating the linearization of the noise component of the network to the noise component of the network itself. ", "page_idx": 25}, {"type": "text", "text": "Lemma D.1. Let $\\lambda,\\delta>0$ . Suppose that $\\begin{array}{r}{d\\geq\\Omega\\left(n+\\log\\frac{1}{\\delta}\\right)}\\end{array}$ . In the context of training data $(X,{\\hat{y}})$ sampled under the data model given in Definition 2.1, let $\\pmb{W}\\,=\\,\\mathcal{A}_{G D}(\\pmb{X},\\hat{\\pmb{y}},\\eta,\\lambda)$ . Then with probability at least $1-\\delta$ over the randomness of $(X,{\\hat{y}})$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|Z\\|_{F}^{2}-2\\lambda\\sqrt{2m}\\|Z\\|_{F}-2m\\lambda^{2}\\leq\\frac{C}{\\alpha m}\\left(\\|z_{\\mathrm{lin}}\\|+2m\\lambda\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. At each iteration of gradient descent, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\pmb{w}_{j}^{(t+1)}=\\pmb{w}_{j}^{(t)}+\\eta(-1)^{j}\\sum_{i=1}^{n}b_{i j}^{(t)}\\hat{y}_{i}\\pmb{x}_{i},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where ", "page_idx": 25}, {"type": "equation", "text": "$$\nb_{i j}^{(t)}=\\left\\{\\!\\!\\begin{array}{l l}{0}&{\\mathrm{if}\\;\\hat{y}_{i}f(W^{(t)},\\pmb{x}_{i})\\ge1}\\\\ {1}&{\\mathrm{if}\\;\\hat{y}_{i}f(W^{(t)},\\pmb{x}_{i})<1\\;\\mathrm{and}\\;\\langle\\pmb{w}_{j}^{(t)},\\pmb{x}_{i}\\rangle\\ge0\\;.}\\\\ {\\alpha}&{\\mathrm{otherwise}.}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Let $T$ be the iteration at which gradient descent terminates. Then for each $j\\in[2m]$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n{\\pmb w}_{j}={\\pmb w}_{j}^{(T)}={\\pmb w}_{j}^{(0)}+\\eta(-1)^{j}\\sum_{t=0}^{T-1}\\sum_{i=1}^{n}b_{i j}^{(t)}\\hat{y}_{i}{\\pmb x}_{i}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Then the noise component of $\\pmb{w}_{j}$ is given by ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle z_{j}=w_{j}-\\langle w_{j},v\\rangle v}\\\\ &{\\quad=w_{j}^{(0)}-\\langle w_{j}^{(0)},v\\rangle v+\\eta(-1)^{j}\\sum_{t=0}^{T-1}\\sum_{i=1}^{n}b_{i j}^{(t)}\\hat{y}_{i}\\big(x_{i}-\\langle x_{i},v\\rangle v\\big)}\\\\ &{\\quad=w_{j}^{(0)}-\\langle w_{j}^{(0)},v\\rangle v+\\eta(-1)^{j}\\sum_{t=0}^{T-1}\\sum_{i=1}^{n}b_{i j}^{(t)}\\hat{y}_{i}n_{i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Define ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\pmb{\\hat{z}}_{j}=\\pmb{z}_{j}-\\pmb{w}_{j}^{(0)}+\\langle\\pmb{w}_{j}^{(0)},\\pmb{v}\\rangle\\pmb{v}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and let ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\hat{z}_{\\mathrm{lin}}=\\sum_{j=1}^{2m}(-1)^{j}\\hat{z}_{j},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Then for all $j\\in[2n]$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\|\\hat{\\boldsymbol z}_{j}\\|-\\|\\boldsymbol z_{j}\\|)^{2}\\leq\\|\\hat{\\boldsymbol z}_{j}-\\boldsymbol z_{j}\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad=\\|\\boldsymbol w_{j}^{(0)}-\\langle\\boldsymbol w_{j}^{(0)},\\boldsymbol v\\rangle\\boldsymbol v\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\|\\boldsymbol w_{j}^{(0)}\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\lambda^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Furthermore, if $\\|z_{j}\\|\\leq\\|\\hat{z}_{j}\\|$ then the above implies $\\|\\hat{z}_{j}\\|\\leq\\|z_{j}\\|+\\lambda$ while if $\\|z_{j}\\|\\geq\\|\\hat{z}_{j}\\|$ then this inequality holds trivially. As a result, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\|\\hat{z}_{j}\\|^{2}-\\|z_{j}\\|^{2}\\big|=|(\\|\\hat{z}_{j}\\|+\\|z_{j}\\|)\\cdot(\\|\\hat{z}_{j}\\|-\\|z_{j}\\|)|}\\\\ &{\\qquad\\qquad\\qquad\\leq|\\|\\hat{z}_{j}\\|+\\|z_{j}\\||\\cdot|\\|\\hat{z}_{j}\\|-\\|z_{j}\\|)|}\\\\ &{\\qquad\\qquad\\qquad\\leq(2\\|z_{j}\\|+\\lambda)(\\lambda).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "If $\\|z_{j}\\|\\geq\\|\\hat{z}_{j}\\|$ then the above implies $\\|\\hat{z}_{j}\\|^{2}\\,\\geq\\,\\|z_{j}\\|^{2}\\,-\\,\\lambda(2\\|z_{j}\\|+\\lambda)$ , if $\\|z_{j}\\|\\,\\leq\\,\\|\\hat{z}_{j}\\|$ this inequality is trivially true. As a result, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{j=1}^{2m}\\|\\hat{z}_{j}\\|^{2}\\geq\\displaystyle\\sum_{j=1}^{2m}(\\|z_{j}\\|^{2}-\\lambda(2\\|z_{j}\\|+\\lambda))}\\\\ {\\displaystyle=\\sum_{j=1}^{2m}\\|z_{j}\\|^{2}-2\\lambda\\displaystyle\\sum_{j=1}^{2m}\\|z_{j}\\|-2m\\lambda^{2}}\\\\ {\\displaystyle\\geq\\displaystyle\\sum_{j=1}^{2m}\\|z_{j}\\|^{2}-2\\lambda\\sqrt{2m}\\left(\\displaystyle\\sum_{j=1}^{2m}\\|z_{j}\\|^{2}\\right)^{1/2}-2m\\lambda^{2}}\\\\ {\\displaystyle=\\|Z\\|_{F}^{2}-2\\lambda\\sqrt{2m}\\|Z\\|_{F}-2m\\lambda^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the third line is an application of Cauchy-Schwarz. Moreover, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\|\\hat{z}_{\\mathrm{lin}}-z_{\\mathrm{lin}}\\|=\\left\\|\\displaystyle\\sum_{j=1}^{2m}(-1)^{j}(\\hat{z}_{j}-z_{j})\\right\\|}}\\\\ &{}&{\\leq\\displaystyle\\sum_{j=1}^{2m}\\|\\hat{z}_{j}-z_{j}\\|}\\\\ &{}&{\\leq\\displaystyle\\sum_{j=1}^{2m}\\lambda}\\\\ &{}&{\\leq2m\\lambda,}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "so ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\hat{z}_{\\mathrm{lin}}\\|\\geq\\|z_{\\mathrm{lin}}\\|-2m\\lambda.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Let $N^{\\prime}\\in\\mathbb{R}^{d\\times n}$ to be the matrix whose $i$ -th column is $\\hat{y}_{i}\\pmb{n}_{i}$ , equivalently $N^{\\prime}=N\\mathrm{diag}(\\hat{y})$ . Then ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{z}_{j}=\\eta(-1)^{j}N^{\\prime}c_{j},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $c_{j}\\in\\mathbb{R}^{n}$ is given by ", "page_idx": 26}, {"type": "equation", "text": "$$\n(\\pmb{c}_{j})_{i}=\\sum_{t=0}^{T-1}b_{i j}^{(t)}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Due to symmetry of the noise distribution then the columns of $N^{\\prime}$ are i.i.d. with distribution $\\mathcal{N}(\\mathbf{0}_{d},d^{\\frac{\\bullet}{-1}}(I_{d}-\\mathbf{\\dot{v}}\\mathbf{v}^{T}))$ . Therefore by Lemma 4.1 (and the assumptions $\\begin{array}{r}{d=\\Omega\\left(n+\\log\\frac{1}{\\delta}\\right))}\\end{array}$ , with probability at least $1-\\delta$ over the randomness of the training data there exist positive constants $C^{\\prime},C$ such that $C^{\\prime}\\leq\\sigma_{\\operatorname*{min}}(N^{\\prime})\\leq\\sigma_{\\operatorname*{max}}(N^{\\prime})\\leq C$ . As a result ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{C^{\\prime}\\eta\\|\\pmb{c}_{j}\\|\\le\\|\\hat{\\pmb{z}}_{j}\\|\\le C\\eta\\|\\pmb{c}_{j}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We claim that for any $j,j^{\\prime}\\in[2m]$ and $i\\in[n]$ , $(c_{j})_{i}\\geq\\alpha(c_{j^{\\prime}})_{i}$ . Indeed, if $\\hat{y}_{i}f(W^{(t)},\\pmb{x}_{i})\\geq1$ , then $b_{i j}^{(t)}=b_{i j^{\\prime}}^{(t)}=0$ , and if $\\hat{y}_{i}f(W^{(t)},\\pmb{x}_{i})<1$ , then both $b_{i j}^{(t)}$ and $b_{i j^{\\prime}}^{(t)}$ are elements of $\\{\\alpha,1\\}$ . This in particular implies that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\langle c_{j},c_{j^{\\prime}}\\rangle\\geq\\alpha\\langle c_{j},c_{j}\\rangle.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Let us define ", "page_idx": 27}, {"type": "equation", "text": "$$\nc_{\\mathrm{lin}}=\\sum_{j=1}^{2m}c_{j}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|\\hat{z}_{\\mathrm{in}}\\|^{2}=\\left\\|\\frac{2m}{\\displaystyle\\prod_{j=1}^{m}(-1)^{j}\\hat{z}_{j}}\\right\\|^{2}}\\\\ {\\displaystyle=\\left\\|\\frac{2m}{\\displaystyle\\prod_{j=1}^{m}\\eta N^{\\prime}c_{j}}\\right\\|^{2}}\\\\ {\\displaystyle\\leq C\\eta^{2}\\left\\|\\sum_{j=1}^{2m}c_{j}\\right\\|^{2}}\\\\ {\\displaystyle=C\\eta^{2}\\|c_{\\mathrm{in}}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where we used that $\\|\\pmb{N}^{\\prime}\\|\\leq C$ in the third line. We also have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|c_{\\mathrm{lin}}\\|^{2}=\\displaystyle\\sum_{j=1}^{2m}\\displaystyle\\sum_{j^{\\prime}=1}^{2m}\\langle c_{j},c_{j^{\\prime}}\\rangle}\\\\ {\\displaystyle\\geq\\alpha\\displaystyle\\sum_{j=1}^{2m}\\sum_{j^{\\prime}=1}^{2m}\\langle c_{j},c_{j}\\rangle}\\\\ {\\displaystyle=2\\alpha m\\displaystyle\\sum_{j=1}^{2m}\\|c_{j}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Finally we combine our bounds for $c,z$ , and $\\hat{z}$ : ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|Z\\|_{F}^{2}-2\\lambda\\sqrt{2m}\\|Z\\|_{F}-2m\\lambda^{2}\\leq\\displaystyle\\sum_{j=1}^{2m}\\|\\hat{z}_{j}\\|^{2}}\\\\ {\\displaystyle\\leq C\\eta^{2}\\sum_{j=1}^{2m}\\|c_{j}\\|^{2}}\\\\ {\\displaystyle\\leq\\frac{C\\eta^{2}}{\\alpha m}\\|c_{\\mathrm{in}}\\|^{2}}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\leq\\frac{C}{\\alpha m}\\|\\hat{z}_{\\mathrm{in}}\\|^{2}}\\\\ {\\displaystyle}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Here we applied equations (15) in the first line, (17) in the second line, (19) in the third line, (18) in the fourth line, and (16) in the fifth line. This establishes both the bounds claimed. \u53e3 ", "page_idx": 27}, {"type": "text", "text": "D.2 Benign overfitting ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "To establish benign overfitting in leaky ReLU networks, we first determine an upper bound on the generalization error of the model in terms of the signal-to-noise ratio of the network weights. ", "page_idx": 27}, {"type": "text", "text": "Lemma D.2. Let $\\epsilon\\in(0,1)$ . Suppose that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{A_{\\mathrm{min}}}{\\|Z\\|_{F}}\\geq C_{2}\\sqrt{\\frac{(1-\\gamma)m\\log\\frac{1}{\\epsilon}}{\\gamma d}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then for test data $(\\pmb{x},y)$ as per Definition 2.1, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{P}(y f(W,x)\\le0)\\le\\epsilon.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. Recall that a test point $(\\pmb{x},y)$ satisfies ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\pmb{x}=y(\\sqrt{\\gamma}\\pmb{v}+\\sqrt{1-\\gamma}\\pmb{n}),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\boldsymbol{n}\\sim\\mathcal{N}(\\mathbf{0}_{d},\\frac{1}{d}(I_{d}-v\\boldsymbol{v}^{T}))$ . If $y f(W,x)\\leq0$ , then ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{1}\\ni y/(W,x)}\\\\ &{=\\underset{y=1}{\\overset{2\\cos}{\\sum}}(-1)^{i_{y}\\sigma}(\\langle w_{j},x\\rangle)}\\\\ &{=\\underset{\\rightharpoondown}{\\sum}(-1)^{i_{y}\\sigma}(\\langle a_{j}v+z_{j},y(\\sqrt{\\gamma}v+\\sqrt{1-\\gamma}n)\\rangle}\\\\ &{=\\underset{\\rightharpoondown}{\\sum}(-1)^{i_{y}\\sigma}(\\langle a_{j}v+z_{j},y(\\sqrt{\\gamma}v+\\sqrt{1-\\gamma}n)\\rangle)}\\\\ &{=\\underset{y=1}{\\overset{2\\sin}{\\sum}}(-1)^{i_{y}\\sigma}\\langle y(\\sqrt{\\gamma}a_{j}+\\sqrt{1-\\gamma}\\langle z_{j},n\\rangle)\\rangle}\\\\ &{\\ge\\underset{y=1}{\\overset{2\\cos}{\\sum}}(-1)^{i_{y}\\sigma}\\langle y\\sqrt{\\gamma}a_{j}\\rangle-\\underset{j=1}{\\overset{2\\cos}{\\sum}}\\sqrt{1-\\gamma}|\\langle z_{j},n\\rangle|}\\\\ &{=\\sqrt{\\gamma}A_{y}-\\underset{j=1}{\\overset{2\\sin}{\\sum}}\\sqrt{1-\\gamma}|\\langle z_{j},n\\rangle|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "When $A_{\\operatorname*{min}}\\geq0$ , this implies that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\gamma A_{\\operatorname*{min}}^{2}\\le(1-\\gamma)\\left(\\displaystyle\\sum_{j=1}^{2m}|\\langle z_{j},n\\rangle|\\right)^{2}}}\\\\ &{}&{\\le2m(1-\\gamma)\\displaystyle\\sum_{j=1}^{2m}|\\langle z_{j},n\\rangle|^{2}}\\\\ &{}&{=2m(1-\\gamma)\\|Z\\boldsymbol{n}\\|^{2}}\\\\ &{}&{\\le2m(1-\\gamma)\\|Z\\|_{F}^{2}\\|\\boldsymbol{n}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the second inequality is an application of Cauchy-Schwarz. So ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{P}(y f(W,\\pmb{x})\\le0)\\le\\mathbb{P}\\left(\\|\\pmb{Z}\\pmb{n}\\|^{2}\\ge\\frac{\\gamma A_{\\operatorname*{min}}^{2}}{2m(1-\\gamma)}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By Lemma A.1, the above probability is less than $\\epsilon$ if ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\sqrt{\\frac{\\gamma}{2m(1-\\gamma)}}A_{\\operatorname*{min}}\\geq C\\|Z\\|_{F}\\sqrt{\\frac{1}{d}\\log\\frac{1}{\\epsilon}},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "or equivalently, ", "page_idx": 28}, {"type": "equation", "text": "$$\n{\\frac{A_{\\mathrm{min}}}{\\|Z\\|_{F}}}\\geq C_{2}{\\sqrt{\\frac{(1-\\gamma)m\\log{\\frac{1}{\\epsilon}}}{\\gamma d}}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We will also need the number of positive labels to be (mildly) balanced with the number of negative labels. ", "page_idx": 28}, {"type": "text", "text": "Lemma D.3. Let $\\delta>0$ and suppose that $\\ell=\\Omega\\left(\\log\\frac{1}{\\delta}\\right)$ . Let ${\\mathcal{T}}\\subseteq[n]$ be an arbitrary subset such that $\\left|\\mathcal{T}\\right|=\\ell$ . Consider training data $(X,y)$ as per the data model given in Definition 2.1. Then with probability at least $1-\\delta$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n{\\frac{\\ell}{4}}\\leq|\\{i\\in S:y_{i}=1\\}|\\leq{\\frac{3\\ell}{4}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. For $i\\in\\mathcal{Z}$ let $Y_{i}$ be a random variable taking the value 1 if $y_{i}=1$ and 0 if $y_{i}=-1$ . Then the $Y_{i}$ are i.i.d. Bernoulli random variables with $\\mathbb{P}(Y_{i}\\stackrel{\\bullet}{=}1)={\\textstyle\\frac{1}{2}}$ . Let ", "page_idx": 29}, {"type": "equation", "text": "$$\nY=\\sum_{i\\in\\mathbb{Z}}Y_{i}=|\\{i\\in\\mathcal{S}:y_{i}=1\\}|\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "so that $\\begin{array}{r}{\\mathbb{E}[Y]=\\frac{l}{2}}\\end{array}$ . By Chernoff\u2019s inequality, for all $t\\in(0,1)$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left|Y-\\frac{\\ell}{2}\\right|\\geq t\\frac{\\ell}{2}\\right)\\leq2e^{-C\\ell t^{2}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Setting $\\textstyle t={\\frac{1}{2}}$ , we see that $\\begin{array}{r}{\\frac{\\ell}{4}\\leq Y\\leq\\frac{3\\ell}{4}}\\end{array}$ with probability at least ", "page_idx": 29}, {"type": "equation", "text": "$$\n1-2\\exp\\left(-\\frac{C\\ell}{4}\\right)\\geq1-\\delta\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "when $\\ell=\\Omega\\left(\\log\\frac{1}{\\delta}\\right)$ . ", "page_idx": 29}, {"type": "text", "text": "We are now able to prove our main benign overfitting result for leaky ReLU networks. ", "page_idx": 29}, {"type": "text", "text": "Theorem 3.2. Under the setting given in Assumption 2, let $\\delta\\in(0,1)$ and suppose $\\boldsymbol{\\mathcal{A}}$ is approximately margin-maximizing (Definition 2.3). If $n=\\Omega$ $\\left(\\log{\\frac{1}{\\delta}}\\right)$ , $d=\\Omega\\,(n),$ , $\\begin{array}{r}{k=O\\big(\\frac{n}{1+m|A|^{2}}\\big)}\\end{array}$ , and $\\gamma=\\Omega\\left(\\frac{1}{k}\\right)$ then there is a fixed positive constant $C$ such that with probability at least $1-\\delta$ over $(X,{\\hat{y}})$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{P}(y f(W,x)\\leq0\\mid X,\\hat{y})\\leq\\exp\\left(-C\\cdot\\frac{d}{k(1+m|A|^{2})}\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof. Since $\\begin{array}{r}{d=\\Omega(n)=\\Omega\\left(n+\\log\\frac{1}{\\delta}\\right)}\\end{array}$ , by Lemma B.3, with probability at least $\\textstyle1-{\\frac{\\delta}{3}}$ over the randomness of the data, the max-margin classifier $\\pmb{w}^{*}$ satisfies ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\|w^{*}\\|\\leq C\\sqrt{\\frac{1}{\\gamma}+\\frac{k}{1-\\gamma}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We denote this event by $\\omega_{1}$ . For $s\\in\\{1,-1\\}$ , let $\\mathcal{G}_{s}$ denote the set of $i\\in\\mathcal G$ such that $\\langle\\pmb{v},\\pmb{x}_{i}\\rangle=s$ . If $\\begin{array}{r}{n=\\Omega\\left(\\frac{1}{\\delta}\\right)}\\end{array}$ and $k=O(n)$ , then $\\begin{array}{r}{|\\mathcal{G}|=\\Omega\\left(\\log\\frac{1}{\\delta}\\right)}\\end{array}$ . Under these assumptions, by Lemma D.3, ", "page_idx": 29}, {"type": "equation", "text": "$$\n|{\\mathcal{G}}_{s}|\\geq{\\frac{1}{4}}|{\\mathcal{G}}|\\geq C n\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "for both $s\\in\\{1,-1\\}$ with probability at least $\\textstyle1-{\\frac{\\delta}{3}}$ . We denote this event by $\\omega_{2}$ . For $s\\in\\{1,-1\\}$ , let $N_{\\mathcal{G}_{s}}\\,\\in\\,\\mathbb{R}^{|\\mathcal{G}_{s}|\\times d}$ be the matrix whose rows are indexed by $\\mathcal{G}_{s}$ and are given by the vectors $\\pmb{n}_{i}$ for $i\\in\\mathcal{G}_{s}$ . As $\\begin{array}{r}{d=\\Omega\\left(n\\right)=\\Omega\\left(n+\\log\\frac{1}{\\delta}\\right)}\\end{array}$ and the rows of $N_{\\mathcal{G}_{s}}$ are drawn mutually i.i.d. from $\\mathcal{N}(\\mathbf{0}_{d},d^{-1}(I_{d}-v^{T}))$ , the following holds by Lemma 4.1. With probability at least $\\begin{array}{r}{1-\\frac{\\delta}{3}}\\end{array}$ over the randomness of the training data, $\\|\\boldsymbol{N}_{\\mathcal{G}_{s}}\\|\\le C$ for both $s\\in\\{1,-1\\}$ . We denote this event by $\\omega_{3}$ . Let $\\omega=\\omega_{1}\\cap\\omega_{2}\\cap\\omega_{3}$ . By the union bound $\\mathbb{P}(\\omega)\\ge1-\\delta$ . We condition on $\\omega$ for the remainder of this proof. ", "page_idx": 29}, {"type": "text", "text": "Since $\\boldsymbol{W}=\\boldsymbol{\\mathcal{A}}(\\boldsymbol{X},\\boldsymbol{\\hat{y}})$ and $\\boldsymbol{\\mathcal{A}}$ is approximately margin maximizing, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|{\\cal W}\\|_{F}\\leq|{\\cal A}|\\cdot\\|{\\pmb w}^{*}\\|}\\\\ {\\displaystyle\\leq C|{\\cal A}|\\sqrt{\\frac{1}{\\gamma}+\\frac{k}{1-\\gamma}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Let $s\\in\\{-1,1\\}$ be such that $A_{s}=A_{\\operatorname*{min}}$ . Since the network attains zero loss, for all $i\\in\\mathcal{G}_{s}$ , ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left\\{\\mathbf{J}(t,x_{t}),\\right\\}}\\\\ &{=\\displaystyle\\sum_{j=1}^{m}(-1)^{j}\\delta_{j}\\sigma(\\sigma_{i j},x_{t}),}\\\\ &{=\\displaystyle\\sum_{j=1}^{m}(-1)^{j}\\delta_{j}\\sigma(\\sigma_{i j},x_{t})\\,}\\\\ &{=\\displaystyle\\sum_{j=1}^{m}(-1)^{j}\\delta_{j}\\sigma(\\sigma_{i j},\\sigma_{j j},\\sigma_{i j},\\sigma_{j},\\sigma_{i j})}\\\\ &{=\\displaystyle\\sum_{j=1}^{m}(-1)^{j}\\delta_{j}\\sigma(\\sigma_{i j},\\sigma_{j j},\\sigma_{i j},\\sigma_{i j}),}\\\\ &{\\geq\\displaystyle\\sum_{j=1}^{m}(-1)^{j}\\delta_{j}\\sigma(\\sigma_{i j},\\sigma_{j j},\\sigma_{i j})+\\displaystyle\\sum_{j=1}^{m}(\\varepsilon_{i}-1)^{j}\\delta_{i}\\sigma_{i j},}\\\\ &{\\leq\\displaystyle\\sum_{j=1}^{m}(-1)^{j}\\delta_{j}\\sigma(\\sigma_{i j},\\sigma_{j j},\\sigma_{i j})+\\displaystyle\\sum_{j=1}^{m}(1-\\varepsilon_{i})\\delta_{i}\\sigma_{i j},}\\\\ &{=\\displaystyle\\sum_{j=1}^{m}(-1)^{j}\\delta_{j}\\sigma(\\sigma_{i j},\\sigma_{j j},\\sigma_{i j}),}\\\\ &{=\\displaystyle\\mathcal{N}\\sum_{j=1}^{m}(1-\\sqrt{1})^{j}\\delta_{j}\\sigma_{i j},}\\\\ &{=\\displaystyle\\mathcal{N}(\\lambda_{1}+\\sqrt{1-\\frac{\\lambda_{j}}{2}})\\,[\\alpha_{1},\\mu_{1j}],}\\\\ &{=\\displaystyle\\sqrt{1-\\delta_{i}}\\,\\mathrm{d}\\lambda_{2}+\\sqrt{1-\\frac{\\lambda_{j}}{2}}\\sum_{j=1}^{m}(\\varepsilon_{i},\\mu_{1j}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Hence, we have either $\\sqrt{\\gamma}A_{s}\\ge\\textstyle{\\frac{1}{2}}$ or $\\begin{array}{r}{\\sqrt{1-\\gamma}\\sum_{j=1}^{2m}|\\langle z_{j},\\pmb{n}_{i}\\rangle|\\geq\\frac{1}{2}}\\end{array}$ for all $i\\in\\mathcal{G}_{s}$ . We consider these two cases separately. ", "page_idx": 30}, {"type": "text", "text": "If $\\sqrt{\\gamma}A_{\\mathrm{min}}\\ge\\frac{1}{2}$ , then ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{A_{\\mathrm{min}}}{\\|Z\\||_{F}}\\ge\\frac{A_{\\mathrm{min}}}{\\|W\\|_{F}}}\\\\ &{\\ge\\frac{1}{2\\sqrt{\\|W\\|_{F}}}}\\\\ &{\\ge C\\frac{1}{\\sqrt{\\gamma}|{\\boldsymbol A}|\\sqrt{\\frac{1}{\\gamma}+\\frac{k}{1-\\gamma}}}}\\\\ &{\\ =C\\frac{1}{|{\\boldsymbol A}|\\sqrt{1+\\frac{k\\gamma}{1-\\gamma}}}}\\\\ &{\\ge C\\frac{1}{|{\\boldsymbol A}|+|{\\boldsymbol A}|\\sqrt{\\frac{k\\gamma}{1-\\gamma}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Then by Lemma D.2, the network has generalization error less than $\\epsilon$ when ", "page_idx": 30}, {"type": "equation", "text": "$$\n{\\frac{1}{|{\\mathcal{A}}|+|{\\mathcal{A}}|{\\sqrt{{\\frac{k\\gamma}{1-\\gamma}}}}}}\\geq C{\\sqrt{\\frac{(1-\\gamma)m\\log{\\frac{1}{\\epsilon}}}{\\gamma d}}}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "or equivalently ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\sqrt{\\frac{(1-\\gamma)m\\log\\frac{1}{\\epsilon}}{\\gamma d}}+\\sqrt{\\frac{m k\\log\\frac{1}{\\epsilon}}{d}}\\leq\\frac{C}{|A|}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "This is satisfied for $\\begin{array}{r}{\\epsilon=\\exp(-C\\cdot\\frac{d}{k(1+m|A|^{2})})}\\end{array}$ for some different constant $C$ when $\\gamma=\\Omega(\\frac{1}{k})$ , which is true by assumption. So if $\\sqrt{\\gamma}A_{\\mathrm{min}}\\,\\geq\\,\\frac{1}{2}$ , then the network has generalization error less than $\\epsilon$ whenever $\\omega$ occurs, which happens with probability at least $1-\\delta$ . ", "page_idx": 30}, {"type": "text", "text": "Now suppose that $\\begin{array}{r}{\\sqrt{1-\\gamma}\\sum_{j=1}^{2m}|\\langle z_{j},\\pmb{n}_{i}\\rangle|\\geq\\frac{1}{2}}\\end{array}$ for all $i\\in\\mathcal{G}_{s}$ . Squaring both sides of the inequality and applying Cauchy-Schwarz, we obtain ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{1}{4}\\leq(1-\\gamma)\\left(\\sum_{j=1}^{2m}|\\langle z_{j},\\pmb{n}_{i}\\rangle|\\right)^{2}}\\\\ {\\displaystyle\\qquad\\leq2m(1-\\gamma)\\sum_{j=1}^{2m}|\\langle z_{j},\\pmb{n}_{i}\\rangle|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Summing over all $i\\in\\mathcal{G}_{s}$ , we obtain ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\frac{|\\mathcal{G}_{s}|}{4}\\leq2m(1-\\gamma)\\sum_{i\\in\\mathcal{G}_{s}}\\sum_{j=1}^{2m}|\\langle z_{j},n_{i}\\rangle|^{2}}}\\\\ {{\\displaystyle=2m(1-\\gamma)\\sum_{j=1}^{2m}\\|N_{\\mathcal{G}_{s}}z_{j}\\|^{2},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Applying $\\omega_{2}$ and $\\omega_{3}$ , we obtain the bound ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{n\\leq C m(1-\\gamma)\\displaystyle\\sum_{j=1}^{2m}\\|N_{\\mathcal{G}_{s}}z_{j}\\|^{2}}\\\\ {\\leq C m(1-\\gamma)\\displaystyle\\sum_{j=1}^{2m}\\|N_{\\mathcal{G}_{s}}\\|^{2}\\|z_{j}\\|^{2}}\\\\ {\\leq C m(1-\\gamma)\\displaystyle\\sum_{j=1}^{2m}\\|z_{j}\\|^{2}}\\\\ {=C m(1-\\gamma)\\|U\\|_{\\gamma}^{2}}\\\\ {\\leq C m(1-\\gamma)\\|W\\|_{\\mathcal{F}_{r}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Then applying (20), ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle n\\leq C m(1-\\gamma)\\|W\\|_{F}^{2}}\\\\ {\\displaystyle\\leq C m(1-\\gamma)|A|^{2}\\left(\\frac{1}{\\gamma}+\\frac{k}{1-\\gamma}\\right)}\\\\ {\\displaystyle\\leq C m|A|^{2}\\left(\\frac{1}{\\gamma}+k\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "This implies that ", "page_idx": 31}, {"type": "text", "text": "or ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{n\\leq\\displaystyle\\frac{C m|{\\cal A}|^{2}}{\\gamma}}}\\\\ {{\\displaystyle{k\\geq\\frac{C n}{m|{\\cal A}|^{2}}.}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Neither of these conditions can occur if $\\begin{array}{r}{\\gamma=\\Omega\\left(\\frac{1}{k}\\right)}\\end{array}$ and $\\begin{array}{r}{k=O\\left(\\frac{n}{|A|^{2}m}\\right)}\\end{array}$ . Thus, in all cases, the network has generalization error less than $\\mathrm{exp}(-C\\cdot{\\frac{d}{k(1+m|A|^{2})}})$ when $\\omega$ occurs, which happens with probability at least $1-\\delta$ . \u53e3 ", "page_idx": 31}, {"type": "text", "text": "We are also able to show the lower bound for the generalization error stated in the main text. ", "page_idx": 31}, {"type": "text", "text": "Theorem 3.3. Under the setting given in Assumption 2, let $\\delta\\in(0,1)$ and suppose $\\mathcal{A}=\\mathcal{A}_{G D}$ where $\\eta,\\lambda\\in\\mathbb{R}_{>0}$ satisfy Assumption $^{\\,l}$ . If $n=\\Omega\\left(k\\right)$ , $d=\\Omega\\left(n\\right)$ , and $\\begin{array}{r}{\\dot{k}=\\Omega(\\log\\frac{1}{\\delta}+\\frac{1}{\\alpha}),}\\end{array}$ , then there is $a$ fixed positive constant $C$ such that with probability at least $1-\\delta$ over $(X,{\\hat{y}})$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{P}(y f(W,x)\\leq0\\mid X,{\\hat{y}})\\geq\\exp\\left(-C\\cdot{\\frac{d}{\\alpha k}}\\right).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof. We proceed along the lines of Theorem 3.2. For $s\\in\\{1,-1\\}$ , let $\\boldsymbol{{\\beta}}_{s}$ denote the set of $i\\in{\\cal B}$ such that $\\langle\\pmb{v},\\pmb{x}_{i}\\rangle=s$ . Note $\\begin{array}{r}{|B|=\\Omega\\left(\\log\\frac{1}{\\delta}\\right)}\\end{array}$ . Under these assumptions, by Lemma D.3, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\displaystyle|B_{s}|\\geq\\frac{1}{4}|B|\\geq C k\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "for both $s\\in\\{1,-1\\}$ with probability at least $\\textstyle1-{\\frac{\\delta}{3}}$ . We denote this event by $\\omega_{1}$ . For $s\\in\\{1,-1\\}$ , let $N_{B_{s}}\\,\\in\\,\\mathbb{R}^{|B_{s}|\\,\\times\\,d}$ be the matrix whose rows are indexed by $B_{s}$ and are given by the vectors $\\pmb{n}_{i}$ for $i\\in\\mathcal{B}_{s}$ . As $\\begin{array}{r}{d=\\Omega\\left(n\\right)=\\Omega\\left(k+\\log\\frac{1}{\\delta}\\right)}\\end{array}$ and the rows of $N_{B_{s}}$ are drawn mutually i.i.d. from $\\mathcal{N}(\\mathbf{0}_{d},d^{-1}(I_{d}-v^{T}))$ , the following holds by Lemma 4.1. With probability at least $\\begin{array}{r}{1-\\frac{\\delta}{3}}\\end{array}$ over the randomness of the training data, $\\|\\pmb{N}_{B_{s}}\\|\\le C$ for both $s\\in\\{1,-1\\}$ . We denote this event by $\\omega_{2}$ . By Lemma D.1, there is a constant $C$ such that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\|Z\\|_{F}^{2}-2\\lambda\\sqrt{2m}\\|Z\\|_{F}-2m\\lambda^{2}\\leq\\frac{C}{\\alpha m}\\left(\\|z_{\\mathrm{lin}}\\|+2m\\lambda\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "with probability at least $\\textstyle1-{\\frac{\\delta}{3}}$ . We denote this event by $\\omega_{3}$ . Let $\\omega=\\omega_{1}\\cap\\omega_{2}\\cap\\omega_{3}$ . By the union bound $\\mathbb{P}(\\omega)\\ge1-\\delta$ . We condition on $\\omega$ for the remainder of this proof. ", "page_idx": 32}, {"type": "text", "text": "Let $s\\,\\in\\,\\{1,-1\\}$ be such that $A_{s}\\mathrm{~=~}\\operatorname*{max}\\{A_{1},A_{-1}\\}$ . Since the network attains zero loss, for all $i\\in\\mathcal{B}_{s}$ , ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad_{\\frac{n-1}{2}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "From which we conclude ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\sqrt{1-\\gamma}\\sum_{j=1}^{2m}|\\langle z_{j},n_{i}\\rangle|\\geq1+\\sqrt{\\gamma}A_{s}\\geq\\sqrt{\\gamma}A_{s}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "for all such $i$ . Squaring both sides of the inequality and applying Cauchy-Schwarz, we obtain ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\gamma{\\cal A}_{s}\\le(1-\\gamma)\\left(\\displaystyle\\sum_{j=1}^{2m}|\\langle z_{j},n_{i}\\rangle|\\right)^{2}}}\\\\ &{}&{\\le2m(1-\\gamma)\\displaystyle\\sum_{j=1}^{2m}|\\langle z_{j},n_{i}\\rangle|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Summing over all $i\\in\\mathcal{B}_{s}$ , we obtain ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle|\\mathcal{B}_{s}|\\gamma A_{s}\\leq2m(1-\\gamma)\\sum_{i\\in\\mathcal{B}_{s}}\\sum_{j=1}^{2m}|\\langle z_{j},\\pmb{n}_{i}\\rangle|^{2}}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad=2m(1-\\gamma)\\sum_{j=1}^{2m}\\|\\pmb{N}_{\\mathcal{B}_{s}}z_{j}\\|^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Applying $\\omega_{2}$ and $\\omega_{3}$ , we obtain the bound ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{k\\gamma A_{s}\\leq C m(1-\\gamma)\\displaystyle\\sum_{j=1}^{2m}\\|N_{B_{s}}z_{j}\\|^{2}}\\\\ &{\\qquad\\leq C m(1-\\gamma)\\displaystyle\\sum_{j=1}^{2m}\\|N_{B_{s}}\\|^{2}\\|z_{j}\\|^{2}}\\\\ &{\\qquad\\leq C m(1-\\gamma)\\displaystyle\\sum_{j=1}^{2m}\\|z_{j}\\|^{2}}\\\\ &{\\qquad=C m(1-\\gamma)\\|Z\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "For $\\begin{array}{r}{k=\\Omega(\\frac{1}{\\alpha})}\\end{array}$ , this inequality along with Assumption 1 implies that ", "page_idx": 33}, {"type": "equation", "text": "$$\nC\\|Z\\|_{F}^{2}\\leq\\|Z\\|_{F}^{2}+2\\lambda\\sqrt{2m}\\|Z\\|_{F}+2m\\lambda^{2}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "for a different constant $C$ . With the last two inequalities and equation 21, we obtain the bound, for a new constant $C$ . ", "page_idx": 33}, {"type": "equation", "text": "$$\nk\\gamma A_{s}\\leq C\\frac{1-\\gamma}{\\alpha}\\left(\\|z_{\\mathrm{lin}}\\|+2m\\lambda\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We then apply $\\textstyle k=\\Omega({\\frac{1}{\\alpha}})$ and Assumption 1 again to conclude that for some $C$ , ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\left\\|z_{\\mathrm{lin}}\\right\\|\\ge C\\sqrt{\\frac{k\\gamma A_{s}\\alpha}{1-\\gamma}}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Note that ", "page_idx": 33}, {"type": "equation", "text": "$$\nA_{\\mathrm{lin}}=\\frac{A_{1}+A_{-1}}{1+\\alpha}\\le2A_{s}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We then bound ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{A_{\\mathrm{lin}}}{\\|z_{\\mathrm{lin}}\\|}\\le C\\frac{A_{s}}{\\sqrt{\\frac{k\\gamma A_{s}\\alpha}{1-\\gamma}}}}\\\\ {\\le C\\sqrt{\\frac{1-\\gamma}{k\\gamma\\alpha}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "for some constant $C$ . Now consider a test point $(\\boldsymbol{x},\\boldsymbol{y})$ , which satisfies ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\pmb{x}=y(\\sqrt{\\gamma}\\pmb{v}+\\sqrt{1-\\gamma}\\pmb{n}),\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $\\boldsymbol{n}\\sim\\mathcal{N}(\\mathbf{0}_{d},\\frac{1}{d}(I_{d}-v\\boldsymbol{v}^{T}))$ . Since the data distribution is symmetric, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(y f(W,x)\\leq0)\\geq\\frac{1}{2}\\mathbb{P}(y f(W,x)\\leq0~\\mathrm{or}~-y f(W,-x)\\leq0)}\\\\ &{\\qquad\\qquad\\qquad\\geq\\frac{1}{2}\\mathbb{P}(y f(W,x)-y f(W,-x)\\leq0).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We see that ", "page_idx": 33}, {"type": "equation", "text": "$$\ny f(W,x)-y f(W,-x)=(1+\\alpha)\\left(y A_{\\mathrm{lin}}{\\sqrt{\\gamma}}+\\langle z_{\\mathrm{lin}},n\\rangle{\\sqrt{1-\\gamma}}\\right)\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "By Lemma C.1 we then can bound ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{P}(y\\langle\\pmb{w},\\pmb{x}\\rangle\\le0\\mid\\omega)\\ge\\frac{1}{8}\\exp\\left(-\\frac{6d}{\\pi}\\frac{\\gamma}{1-\\gamma}\\frac{A_{\\mathrm{lin}}^{2}}{\\|z_{\\mathrm{lin}}\\|^{2}}\\right)}}\\\\ &{}&{\\ge\\exp\\left(-\\frac{C d}{\\alpha k}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "for a new constant $C$ , provided $A_{\\mathrm{{lin}}}$ is positive. In the last line we can bound $\\frac{d}{k}$ below as $d\\,=$ $\\Omega(n)=\\Omega(k)$ . If $A_{\\mathrm{{lin}}}$ is negative, then the generalization error is at least $\\textstyle{\\frac{1}{4}}$ which is also at least $\\exp(-C d/(\\alpha k))$ . \u53e3 ", "page_idx": 33}, {"type": "text", "text": "D.3 Non-benign overfitting ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "In this section we show that leaky ReLU networks trained on low-signal data exhibit non-benign overftiting. As in the case of benign overftiting, we will rely on a generalization bound which depends on the signal-to-noise ratio of the network. ", "page_idx": 34}, {"type": "text", "text": "Lemma D.4. Let $W\\in\\mathbb{R}^{2m\\times d}$ be the first layer weight matrix of a shallow leaky ReLU network given by equation 1. Suppose $(\\pmb{x},y)$ is a random test point sampled under the data model given in Definition 2.1. If $W$ is such that $A_{\\mathrm{lin}}\\ge0$ and ", "page_idx": 34}, {"type": "equation", "text": "$$\n{\\frac{A_{\\mathrm{lin}}}{z_{\\mathrm{lin}}}}={\\cal O}\\left(\\sqrt{\\frac{1-\\gamma}{\\gamma d}}\\right)\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "then ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{P}(y f(W,x)<0)\\geq\\frac{1}{8}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Alternatively, if $A_{\\mathrm{lin}}\\leq0$ then ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{P}(y f(W,x)<0)\\geq\\frac{1}{4}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Proof. By Definition 2.1 $\\left(-{\\pmb x},-y\\right)$ is identically distributed to $(\\pmb{x},y)$ , therefore ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{P}(0>y f(W,x))=\\displaystyle\\frac12\\left(\\mathbb{P}(0>y f(W,x))+\\mathbb{P}(0>-y f(W,-x))\\right)}&{}\\\\ {\\displaystyle\\ge\\frac12\\mathbb{P}(0>y f(W,x)\\cup0>-y f(W,-x))}&{}\\\\ {\\displaystyle}&{\\ge\\frac12\\mathbb{P}\\left(0>y f(W,x)-y f(W,-x)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Next we compute ", "page_idx": 34}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{y/(W,x)-y/(W,-x)}\\\\ &{=\\displaystyle\\sum_{j=1}^{2m}(-1)^{j}y(\\sigma(\\langle w_{j},x\\rangle)-\\sigma(\\langle w_{j},-x\\rangle))}\\\\ &{=(1+\\alpha)\\displaystyle\\sum_{j=1}^{2m}(-1)^{j}y\\langle w_{j},x\\rangle}\\\\ &{=(1+\\alpha)\\displaystyle\\sum_{j=1}^{2m}(-1)^{j}(a_{j}v+z_{j},{\\sqrt{\\gamma}}v+{\\sqrt{1-\\gamma}}n)}\\\\ &{=(1+\\alpha)\\displaystyle{\\sqrt{\\gamma}\\sum_{j=1}^{2m}(-1)^{j}a_{j}}+(1+\\alpha){\\sqrt{1-\\gamma}}\\left\\langle n,{\\displaystyle\\sum_{j=1}^{2m}(-1)^{j}z_{j}}\\right\\rangle}\\\\ &{=(1+\\alpha){\\sqrt{\\gamma}}A_{\\mathrm{{lin}}}+(1+\\alpha){\\sqrt{1-\\gamma}}\\langle n,z_{\\mathrm{{lin}}}\\rangle.}\\end{array}}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "The above two calculations imply that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(0>y f(W,x))\\geq\\displaystyle\\frac{1}{2}\\mathbb{P}(0>y f(W,x)-y f(W,-x))}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\frac{1}{2}\\mathbb{P}(0>(1+\\alpha)\\sqrt{\\gamma}A_{\\mathrm{lin}}+(1+\\alpha)\\sqrt{1-\\gamma}\\langle n,z_{\\mathrm{lin}}\\rangle)}\\\\ &{\\qquad\\qquad\\qquad=\\displaystyle\\frac{1}{2}\\mathbb{P}\\left(\\langle-n,z_{\\mathrm{lin}}\\rangle>\\sqrt{\\frac{\\gamma}{1-\\gamma}}A_{\\mathrm{lin}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Suppose that $A_{\\mathrm{lin}}\\geq0$ . As the noise distribution is symmetric $\\langle\\boldsymbol{n},z_{\\mathrm{lin}}\\rangle\\stackrel{d}{=}\\langle-\\boldsymbol{n},z_{\\mathrm{lin}}\\rangle$ . Therefore, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\frac{1}{4}\\mathbb{P}\\left(|\\langle\\pmb{n},z_{\\mathrm{lin}}\\rangle|>\\sqrt{\\frac{\\gamma}{1-\\gamma}}A_{\\mathrm{lin}}\\right)=\\frac{1}{4}\\mathbb{P}\\left(|\\langle\\pmb{n},\\pmb{u}\\rangle|>\\sqrt{\\frac{\\gamma}{1-\\gamma}}\\frac{A_{\\mathrm{lin}}}{\\|z_{\\mathrm{lin}}\\|}\\right),\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $\\begin{array}{r}{{\\pmb u}~{=}~\\frac{z_{\\mathrm{lin}}}{\\|{\\pmb z}_{\\mathrm{lin}}\\|}}\\end{array}$ is the unit vector pointing in the direction of $z_{\\mathrm{lin}}$ . Note by construction $_{u}\\in$ $\\operatorname{span}(\\{v\\})^{\\perp}$ . If ", "page_idx": 35}, {"type": "equation", "text": "$$\n{\\frac{A_{\\mathrm{lin}}}{\\|z_{\\mathrm{lin}}\\|}}=O\\left({\\sqrt{\\frac{1-\\gamma}{\\gamma d}}}\\right),\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "then by Lemma A.2, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(|\\langle\\pmb{n},\\pmb{u}\\rangle|>\\sqrt{\\frac{\\gamma}{1-\\gamma}}\\frac{A_{\\mathrm{lin}}}{\\|z_{\\mathrm{lin}}\\|}\\right)\\geq\\frac{1}{2}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "and therefore ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{P}(0>y f(W,\\pmb{x}))\\ge\\frac{1}{4}\\mathbb{P}\\left(|\\langle\\pmb{\\mathscr{n}},\\pmb{\\mathscr{u}}\\rangle|>\\sqrt{\\frac{\\gamma}{1-\\gamma}}\\frac{A_{\\operatorname*{lin}}}{\\|z_{\\operatorname*{lin}}\\|}\\right)\\ge\\frac{1}{8}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "If $A_{\\mathrm{lin}}<0$ , then again by the symmetry of the noise ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(0>y f(W,x))\\ge\\frac{1}{2}\\mathbb{P}\\left(\\langle-n,z_{\\mathrm{lin}}\\rangle>\\sqrt{\\frac{\\gamma}{1-\\gamma}}A_{\\mathrm{lin}}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\ge\\frac{1}{2}\\mathbb{P}\\left(\\langle-n,z_{\\mathrm{lin}}\\rangle>0\\right)}\\\\ &{\\qquad\\qquad\\qquad=\\frac{1}{4}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "This establishes the result. ", "page_idx": 35}, {"type": "text", "text": "Theorem 3.4. Under the setting given in Assumption 2, let $\\delta\\,\\in\\,(0,1)$ and suppose $A=A_{G D}$ , where $\\eta,\\lambda\\in\\mathbb{R}_{>0}$ satisfy Assumption $^{\\,I}$ . If $\\begin{array}{r}{n=\\Omega(1),d=\\Omega\\left(n+\\log\\frac{1}{\\delta}\\right)}\\end{array}$ and $\\begin{array}{r}{\\gamma=O\\left(\\frac{\\alpha^{3}}{d}\\right)}\\end{array}$ then the following hold. ", "page_idx": 35}, {"type": "text", "text": "1. The algorithm $A_{G D}$ terminates almost surely after finitely many updates. With ${\\textbf{\\em W}}=$ $A_{G D}({\\cal X},\\hat{\\pmb y}),\\,{\\cal L}(W,{\\cal X},\\hat{\\pmb y})=0$ .   \n2. With probability at least $1-\\delta$ over the training data $(X,{\\hat{y}})$ ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{P}(y f(W,x)<0\\mid X,{\\hat{\\pmb y}})\\geq{\\frac{1}{8}}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof. If $A_{\\mathrm{lin}}<0$ , then by Lemma D.4, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{P}(y f(W,x)<0)\\geq\\frac{1}{4}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "So it suffices to consider the case $A_{\\mathrm{lin}}\\ge0$ . Since $\\begin{array}{r}{d=\\Omega\\left(n+\\log\\frac{1}{\\delta}\\right)}\\end{array}$ , by Lemma B.4, the max-margin classifier $\\pmb{w}^{*}$ satisfies ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\|w^{*}\\|\\leq C\\sqrt{\\frac{n}{1-\\gamma}}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "with probability at least $\\begin{array}{r}{1-\\frac{\\delta}{3}}\\end{array}$ over the randomness of the input dataset. We denote this event by $\\omega_{1}$ and condition on it for the rest of this proof. By Theorem 3.1, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|\\pmb{W}\\|\\leq\\frac{C\\|\\pmb{w}^{*}\\|}{\\alpha\\sqrt{m}}}\\\\ {\\displaystyle\\leq\\frac{C}{\\alpha}\\sqrt{\\frac{n}{m(1-\\gamma)}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "By Theorem 3.1, the network perfectly ftis the training data, so for all $i\\in[n],\\hat{y}_{i}f(W,\\pmb{x}_{i})\\geq1$ , and therefore ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{1\\leq|J(W,x_{i})|}\\\\ &{\\quad=\\displaystyle\\left\\lvert\\sum_{j=1}^{2m}(-1)^{j}\\rho(\\langle w_{j},x_{i}\\rangle)\\right\\rvert}\\\\ &{\\quad\\leq\\displaystyle\\sum_{j=1}^{2m}\\left\\lvert\\langle w_{j},x_{i}\\rangle\\right\\rvert}\\\\ &{\\quad=\\displaystyle\\sum_{j=1}^{2m}\\left\\lvert\\langle a,v+z_{j},\\sqrt{\\gamma}y_{i}v+\\sqrt{1-\\gamma}w_{i}\\rangle\\right\\rvert}\\\\ &{\\quad=\\displaystyle\\sum_{j=1}^{2m}\\left\\lvert\\langle a,v\\rangle\\sqrt{\\gamma}+\\sqrt{1-\\gamma}\\langle z_{j},n_{i}\\rangle\\right\\rvert}\\\\ &{\\quad=\\displaystyle\\sum_{j=1}^{2m}\\|a,j\\rangle\\sqrt{1-\\gamma}\\displaystyle\\sum_{j=1}^{2m}\\left\\lvert\\langle z_{j},n_{i}\\rangle\\right\\rvert.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "This implies that either $\\begin{array}{r}{\\frac{1}{2}\\,\\leq\\,\\sqrt{\\gamma}\\sum_{j=1}^{2m}\\left|a_{j}\\right|}\\end{array}$ or $\\begin{array}{r}{\\frac{1}{2}\\,\\leq\\,\\sqrt{1-\\gamma}\\sum_{j=1}^{2m}|\\langle z_{j},{\\pmb n}_{i}\\rangle|}\\end{array}$ for all $i\\,\\in\\,[n]$ . We consider both cases separately. ", "page_idx": 36}, {"type": "text", "text": "Suppose that $\\begin{array}{r}{\\frac{1}{2}\\leq\\sqrt{\\gamma}\\sum_{j=1}^{2m}|a_{j}|}\\end{array}$ . Then squaring both sides and applying Cauchy-Schwarz, we obtain ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{4}\\leq\\gamma\\left(\\displaystyle\\sum_{j=1}^{2m}\\left|a_{j}\\right|\\right)^{2}}\\\\ &{\\phantom{2p c}\\leq2m\\gamma\\displaystyle\\sum_{j=1}^{2m}\\left|a_{j}\\right|^{2}}\\\\ &{\\phantom{2p c}\\leq2m\\gamma\\displaystyle\\sum_{j=1}^{2m}\\left|a_{j}\\right|^{2}}\\\\ &{\\phantom{2p c}\\leq2m\\gamma\\displaystyle\\sum_{j=1}^{2m}\\left||a_{j}\\right|^{2}}\\\\ &{=2m\\gamma\\|W\\|_{L}^{2}}\\\\ &{\\phantom{2p c}\\leq\\frac{C\\gamma n}{\\alpha^{2}(1-\\gamma)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "This cannot occur if $\\begin{array}{r}{\\gamma=O\\left(\\frac{\\alpha^{2}}{n}\\right)}\\end{array}$ , and in particular it cannot occur if $d=\\Omega(n)$ and $\\begin{array}{r}{\\gamma=O\\left(\\frac{\\alpha^{3}}{d}\\right)}\\end{array}$ . Now suppose that $\\begin{array}{r}{\\frac{1}{2}\\leq\\sqrt{1-\\gamma}\\sum_{j=1}^{2m}|\\langle z_{j},\\pmb{n}_{i}\\rangle|}\\end{array}$ for all $i\\in[n]$ . Squaring both sides and applying Cauchy-Schwarz, we obtain ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{1}{4}\\le(1-\\gamma)\\left(\\sum_{j=1}^{2m}\\|\\langle z_{j},\\pmb{n}_{i}\\rangle\\|\\right)^{2}}}\\\\ &{}&{\\le2m(1-\\gamma)\\sum_{j=1}^{2m}\\|\\langle z_{j},\\pmb{n}_{i}\\rangle\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Summing over all $i\\in[n]$ , we obtain ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{n}{4}\\leq2m(1-\\gamma)\\sum_{i=1}^{n}\\sum_{j=1}^{2m}\\|\\langle z_{j},n_{i}\\rangle\\|^{2}}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\qquad=2m(1-\\gamma)\\sum_{j=1}^{2m}\\|N z_{j}\\|^{2}}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\leq2m(1-\\gamma)\\|N\\|^{2}\\sum_{j=1}^{2m}\\|z_{j}\\|^{2}}\\\\ {\\displaystyle}\\\\ {\\displaystyle=2m(1-\\gamma)\\|N\\|^{2}\\|Z\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Recall that $\\begin{array}{r}{d=\\Omega\\left(n+\\log\\frac{1}{\\delta}\\right)}\\end{array}$ , and that the rows of $_{N}$ are i.i.d. with distribution $\\mathcal{N}(\\mathbf{0}_{d},d^{-1}(I_{d}-$ $\\scriptstyle v v^{T}\\rangle$ ). So by Lemma 4.1, with probability at least $1-{\\frac{\\delta}{3}}$ over the randomness of the dataset, $\\|N\\|\\leq C$ . We denote this event by $\\omega_{2}$ and condition on it for the rest of this proof. So ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\|Z\\|_{F}^{2}\\geq\\frac{C n}{m(1-\\gamma)}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Let $\\textstyle\\lambda={\\frac{\\sqrt{\\alpha}}{m}}$ . By Assumption 1, $\\lVert\\pmb{w}_{j}^{(0)}\\rVert\\leq\\lambda$ for all $j\\in[2m]$ . So by Lemma D.1, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\|Z\\|_{F}^{2}-2\\lambda\\sqrt{2m}\\|Z\\|_{F}-2m\\lambda^{2}\\leq\\frac{C}{\\alpha m}(\\|z_{\\mathrm{lin}}\\|+2m\\lambda)^{2}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "By (22), ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\|Z\\|_{F}\\geq\\frac{C\\sqrt{n}}{\\sqrt{m(1-\\gamma)}}}}\\\\ &{}&{\\geq\\frac{C\\sqrt{n}}{\\sqrt{m}}}\\\\ &{}&{\\geq C\\lambda\\sqrt{n m}}\\\\ &{}&{\\geq8\\lambda\\sqrt{2m},}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where the last line holds if $n=\\Omega(1)$ . Then by (23), ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac12\\|\\pmb{Z}\\|_{F}^{2}=\\|\\pmb{Z}\\|_{F}^{2}-\\frac14\\|\\pmb{Z}\\|_{F}^{2}-\\frac14\\|\\pmb{Z}\\|_{F}^{2}}\\\\ &{\\qquad\\quad\\le\\|\\pmb{Z}\\|_{F}^{2}-2\\lambda\\sqrt{2m}\\|\\pmb{Z}\\|_{F}-2m\\lambda^{2}}\\\\ &{\\qquad\\quad\\le\\frac{C}{\\alpha m}(\\|z_{\\mathrm{lin}}\\|+2m\\lambda)^{2}}\\\\ &{\\qquad\\quad=\\frac{C}{\\alpha m}(\\|z_{\\mathrm{lin}}\\|+2\\sqrt{\\alpha})^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Taking the square root of both sides and recalling that $\\alpha\\in(0,1)$ is a constant, we obtain ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\|Z\\|_{F}\\leq{\\frac{C\\|z_{\\mathrm{lin}}\\|}{\\sqrt{\\alpha m}}}+{\\frac{C}{\\sqrt{m}}}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "This implies that either $\\begin{array}{r}{\\|Z\\|_{F}\\leq\\frac{2C}{\\sqrt{m}}}\\end{array}$ or $\\begin{array}{r}{\\|Z\\|_{F}\\leq\\frac{2C\\|z_{\\mathrm{lin}}\\|}{\\sqrt{\\alpha m}}}\\end{array}$ . The case $\\begin{array}{r}{\\|Z\\|_{F}\\leq\\frac{2C}{\\sqrt{m}}}\\end{array}$ cannot happen, since by (22), ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|Z\\|_{F}\\geq\\frac{C^{\\prime}\\sqrt{n}}{\\sqrt{m(1-\\gamma)}}}\\\\ {\\geq\\frac{C^{\\prime}\\sqrt{n}}{\\sqrt{m}}}\\\\ {\\geq\\frac{2C}{\\sqrt{m}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "when $n=\\Omega(1)$ . So we have $\\begin{array}{r}{\\|Z\\|_{F}\\leq\\frac{2C\\|z_{\\mathrm{lin}}\\|}{\\sqrt{\\alpha m}}}\\end{array}$ 2C\u221a\u2225\u03b1zlmin \u2225. Again applying (22), we obtain ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\|z_{\\mathrm{lin}}\\|\\geq C\\sqrt{\\alpha m}\\|Z\\|_{F}}\\\\ {\\geq\\displaystyle\\frac{C\\sqrt{\\alpha n}}{\\sqrt{1-\\gamma}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "So ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{A_{\\mathrm{lim}}}{\\|z\\|_{\\operatorname*{min}}}\\leq C\\frac{A_{\\mathrm{lim}}\\sqrt{1-\\gamma}}{\\sqrt{\\alpha}}}\\\\ &{\\qquad=\\frac{C\\sqrt{1-\\gamma}}{\\sqrt{\\alpha}}\\frac{2}{f_{\\omega1}-1}}\\\\ &{\\qquad\\leq\\frac{C\\sqrt{1-\\gamma}}{\\sqrt{\\alpha}}\\sqrt{2m}\\left(\\displaystyle\\sum_{j=1}^{m}\\omega_{j}\\right)^{1/2}}\\\\ &{\\qquad\\leq\\frac{C\\sqrt{m(1-\\gamma)}}{\\sqrt{\\alpha}}\\left(\\displaystyle\\sum_{j=1}^{m}\\|w_{j}\\|^{2}\\right)^{1/2}}\\\\ &{\\qquad=\\frac{C\\sqrt{1\\|\\mathbf{f}\\|_{\\Gamma}\\sqrt{m(1-\\gamma)}}}{\\sqrt{\\alpha}}}\\\\ &{\\qquad=\\frac{C}{\\sqrt{\\alpha}}\\frac{\\sqrt{m}\\sqrt{1-\\gamma}}{\\sqrt{\\alpha}}(1-\\gamma)}\\\\ &{\\qquad\\leq\\frac{C}{\\sqrt{\\alpha}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Here we used that $A_{\\mathrm{lin}}\\ge0$ and applied Cauchy-Schwarz in the third line. Then by Lemma D.4, if ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\frac{C}{\\alpha^{3/2}}\\leq O\\left(\\sqrt{\\frac{1-\\gamma}{\\gamma d}}\\right),\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "then ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbb{P}(y f(W,x)<0)\\geq\\frac{1}{8}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "This occurs if $\\begin{array}{r}{\\gamma=O\\left(\\frac{\\alpha^{3}}{d}\\right)}\\end{array}$ . Hence, in all cases, we have shown that with the appropriate scaling, the generalization error is at least $\\frac18$ when both $\\omega_{1}$ and $\\omega_{2}$ occur. This happens with probability at least $1-\\delta$ . \u53e3 ", "page_idx": 38}, {"type": "text", "text": "Appendix E Formalizing benign overfitting as a high dimensional phenomenon ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "To formalize benign overftiting as a high dimensional phenomenon we first introduce the notion of a regime. Informally, a regime is a subset of the hyperparameters $\\Omega\\in\\mathbb{N}^{4}$ which describes accepted combinations of the input data dimension $d$ , the number of points in the training sample $n$ , the number of corrupt points $k$ and the number of trainable model parameters $p$ . ", "page_idx": 38}, {"type": "text", "text": "Definition E.1. A regime is a subset $\\Omega\\subset\\mathbb{N}^{4}$ which satisfies the following properties. ", "page_idx": 38}, {"type": "text", "text": "1. For any tuple $(d,n,k,p)\\in\\Omega$ the number of corrupt points is at most the total number of points, $k\\leq n$ . ", "page_idx": 38}, {"type": "text", "text": "2. There is no upper bound on the number of points, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{(d,n,k,p)\\in\\Omega}n=\\infty.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "A non-trivial regime is a regime which satisfies the following additional condition. ", "page_idx": 38}, {"type": "text", "text": "3. Define the set of increasing sequences of $\\Omega$ as $\\begin{array}{r l r}{\\Omega^{*}}&{{}=}&{\\{(n_{l},d_{l},k_{l},p_{l})_{l\\in\\mathbb{N}}\\quad\\subset}\\end{array}$ $\\Omega$ s.t. $\\mathrm{lim}_{l\\to\\infty}\\,n_{l}=\\infty\\}$ . For any $(n_{l},d_{l},k_{l},p_{l})_{l\\in\\mathbb{N}}\\in\\Omega^{*}$ it holds that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{l\\to\\infty}\\frac{k_{l}}{n_{l}}>0.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Intuitively, a regime defines how the four hyperparameters $(d,n,k,p)$ can grow in relation to one another as $n$ goes to infinity. A non-trivial regime is one in which the fraction of corrupt points in the training sample is non-vanishing. In order to make a formal definition of benign overfitting as high dimensional phenomenon we introduce the following additional concepts. ", "page_idx": 39}, {"type": "text", "text": "\u2022 A learning algorithm $\\mathcal{A}=(\\mathcal{A}_{d,n,p})_{(d,n,p)\\in\\mathbb{N}^{3}}$ is a triple indexed sequence of measurable functions $A_{d,n,p}:\\mathbb{R}^{n\\times d}\\times\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{p}$ .   \n\u2022 An architecture $\\mathcal{M}=(f_{d,p})_{d,p\\in\\mathbb{N}^{2}}$ is a double indexed sequence of measurable functions $f_{d,p}:\\mathbb{R}^{d}\\times\\mathbb{R}^{p}\\rightarrow\\mathbb{R}$ .   \n\u2022 A data model $\\mathcal D\\,=\\,\\bigl(D_{d,n,k}\\bigr)_{(d,n,k)\\in\\mathbb{N}^{3}}$ is a triple indexed sequence of Borel probability measures $D_{d,n,k}$ defined over $\\mathbb{R}^{n\\times d}\\times\\{\\pm1\\}\\times\\mathbb{R}^{d}\\times\\{\\pm1\\}$ . ", "page_idx": 39}, {"type": "text", "text": "With these notions in place we are ready to provide a definition of benign overfitting in high dimensions. ", "page_idx": 39}, {"type": "text", "text": "Definition E.2. Let $(\\epsilon,\\delta)\\in(0,1]^{2}$ , $\\boldsymbol{\\mathcal{A}}$ be a learning algorithm, $\\mathcal{M}$ an architecture, $\\mathcal{D}$ a data model and $\\Omega$ a regime. If for every increasing sequence $(d_{l},n_{l},k_{l},p_{l})_{l\\in\\mathbb{N}}\\in\\Omega^{*}$ there exists an $L\\in\\mathbb N$ such that for all $l\\geq L$ with probability at least $1-\\delta$ over $(X,{\\hat{y}})$ , where $(X,{\\hat{y}},x,y)\\sim D_{d_{l},n_{l},k_{l}}.$ , $i t$ holds that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I.\\ \\ y_{i}f(A_{d_{l},n_{l},p_{l}}(X,\\hat{y}),x_{i})>0\\ \\ \\forall i\\in[n_{l}],}\\\\ &{2.\\ \\mathbb{P}(y f(A_{d_{l},n_{l},p_{l}}(X,\\hat{y}),x)\\le0)\\le\\operatorname*{inf}_{W\\in\\mathbb{R}^{n_{l}\\times d_{l}}}\\mathbb{P}(y f(W,x)\\le0)+\\epsilon,}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "then the quadruplet $\\left(\\mathcal{A},\\mathcal{M},\\mathcal{D},\\Omega\\right)\\left(\\epsilon,\\delta\\right)$ -benignly overfits. If $\\left(\\mathcal{A},\\mathcal{M},\\mathcal{D},\\Omega\\right)\\left(\\epsilon,\\delta\\right)$ -benignly overfits for any $\\bar{(\\epsilon,\\delta)}\\in\\bar{(0,1]}^{2}$ then we say $(\\mathcal{A},\\mathcal{M},\\mathcal{D},\\Omega)$ benignly overfits. ", "page_idx": 39}, {"type": "text", "text": "Analogously, we define non-benign overfitting as follows. ", "page_idx": 39}, {"type": "text", "text": "Definition E.3. Let $(\\epsilon,\\delta)\\in(0,1]^{2}$ , $\\boldsymbol{\\mathcal{A}}$ be a learning algorithm, $\\mathcal{M}$ an architecture, $\\mathcal{D}$ a data model and $\\Omega$ a regime. If for every increasing sequence $(d_{l},n_{l},k_{l},p_{l})_{l\\in\\mathbb{N}}\\in\\Omega^{*}$ there exists an $L\\in\\mathbb N$ such that for all $l\\geq L$ with probability at least $1-\\delta$ over $(X,{\\hat{y}})$ , where $(X,{\\hat{y}},x,y)\\sim D_{d_{l},n_{l},k_{l}}$ , $i t$ holds that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I.~~y_{i}f(A_{d_{l},n_{l},p_{l}}(X,\\hat{y}),x_{i})>0~\\forall i\\in[n_{l}],}\\\\ &{2.~\\mathbb{P}(y f(A_{d_{l},n_{l},p_{l}}(X,\\hat{y}),x)\\le0)\\ge\\operatorname*{inf}_{W\\in\\mathbb{R}^{n_{l}\\times d_{l}}}\\mathbb{P}(y f(W,x)\\le0)+\\epsilon,}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "then the quadruplet $\\left(\\mathcal{A},\\mathcal{M},\\mathcal{D},\\Omega\\right)\\left(\\epsilon,\\delta\\right)$ -non-benignly overftis. If $\\left(\\mathcal{A},\\mathcal{M},\\mathcal{D},\\Omega\\right)\\left(\\epsilon,\\delta\\right)$ -non-benignly overfits for any $(\\epsilon,\\delta)\\in(0,1]^{2}$ then we say $(\\mathcal{A},\\mathcal{M},\\mathcal{D},\\Omega)$ non-benignly overfits. ", "page_idx": 39}, {"type": "text", "text": "One of the key contributions of this paper is proving $(\\epsilon,\\delta)$ -benign and non-benign overfitting when the architecture is a two-layer leaky ReLU network (equation 1), the learning algorithm returns the inner layer weights of the network by minimizing the hinge loss over the training data using gradient descent (Definition 2.2), and the regime satisfies the conditions $d\\,=\\,\\Omega(n\\log{1/\\epsilon})$ , $n\\doteq\\bar{\\Omega}(1/\\delta)$ , $k=O(n)$ and $p=2d m$ for some network width $2m$ , $m\\in\\mathbb{N}$ . ", "page_idx": 39}, {"type": "text", "text": "Appendix F Experiments ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "To further support our theory, we train shallow neural networks on the data model described in Definition 2.1 and record the numerical results. Scripts to reproduce these experiments can be found at https://github.com/kedar2/benign_overfitting. These experiments were run on the CPU of a MacBook Pro M2 with 8GB of RAM. For our first experiment, we investigate the effect of the ratio $\\frac{d}{n}$ on the generalization error of the network. Recall that by Theorem 3.2, the generalization error is bouned above by $\\exp\\left(-C n\\right)$ when $\\begin{array}{r}{\\frac{d}{n}=\\Omega(1)}\\end{array}$ . In other words, if $\\frac{d}{n}$ is larger than a critical threshold, then the generalizatione error decays quickly to 0 as $n$ increases. We empirically confirm this prediction in Figure 1, where we train several networks while varying $\\frac{d}{n}$ and $n$ , and estimate the generalization error for each configuration by averaging over 20 trials. Within each trial, we trained the inner layer of the network with gradient descent using the hinge loss until the training loss reached 0. For $\\frac{d}{n}$ greater than around 7, the generalization error rapidly decays to 0 as $n\\to\\infty$ . ", "page_idx": 39}, {"type": "text", "text": "", "page_idx": 40}, {"type": "image", "img_path": "88TzdGyPT6/tmp/a64c529a5a5ae335e683f8e0db287f455c2387f7f71bd3c5e2bc636afc08ae9c.jpg", "img_caption": ["Figure 1: Generalization error of a two-layer leaky ReLU network trained to 0 hinge loss varying $n$ and $d$ . Parameter settings: $\\alpha=0.1$ , $\\gamma=5/n$ , $m=64$ , $k=0.1n$ , number of trials $=5$ , size of validation sample $=1000$ . "], "img_footnote": [], "page_idx": 40}, {"type": "text", "text": "Next, we train a two-layer network varying $n$ and $\\gamma$ (Figure 2), holding constant the ratio $\\frac{d}{n}$ . Since $\\gamma$ controls the signal-to-noise ratio of the data, the generalization error of the learned network decreases as $\\gamma$ increases. For each value of $n$ , the generalization error falls off steeply as $\\gamma$ reaches a certain threshold. This threshold decreases as $n$ increases, indicating that the network has higher noise tolerance as $n$ increases. This is in agreement with our theoretical results where we found that benign overfitting occurs at the threshold $\\begin{array}{r}{\\bar{\\gamma}=\\Omega\\left(\\frac{1}{k}\\right)}\\end{array}$ (which is in this case $\\Omega\\left({\\frac{1}{n}}\\right))$ . We also see that the generalization error for large values of $\\gamma$ is similar across different values of $n$ . This effect is also predicted by Corollary 3.2.1, since we scale both $d$ and $k$ proportionally to $n$ . ", "page_idx": 40}, {"type": "image", "img_path": "88TzdGyPT6/tmp/0f7d0df4a40c1ed488f390c504c624b9bacbaf7a2d6e41719494f114767124af.jpg", "img_caption": ["Figure 2: Generalization error of a two-layer leaky ReLU network trained to 0 hinge loss varying $\\gamma$ and $n$ . Parameter settings: $\\alpha=0.1$ , $d=2n$ , $m=64$ , $k=0.1n$ , number of trials $=10$ , size of validation sample $=1000$ . "], "img_footnote": [], "page_idx": 41}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: The results stated in the abstract and introduction are stated formally in Section 3 and then proven in Appendix D. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 42}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Justification: We discuss limitations of the work in the conclusion of the paper. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 42}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: All necessary assumptions are stated in the theorems. All theorems are proven in full detail in Appendices C and D, with proof sketches appearing in Section 4. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 43}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: We describe our experimental setup in Appendix F. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 43}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: We provide a link to our code to reproduce our experiments. Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 44}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: We describe our setup in Appendix F. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 44}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 44}, {"type": "text", "text": "Answer: [No] ", "page_idx": 44}, {"type": "text", "text": "Justification: Our experiments consist of a heatmap and a line chart with multiple plots. It was not possible to add error bars without crowding the plots. In our description of our experimental setup we describe the sample size, from which standard errors can be computed. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 44}, {"type": "text", "text": "", "page_idx": 45}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: We describe the resources used in Appendix F. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 45}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: The research conducted in the paper did not use data, assets, or human participants; only studied existing models; and was conducted in accordance to the Code of Ethics. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 45}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: This paper is a purely theoretical study explaining behaviors seen in neural networks in practice. There are no foreseeable societal impacts of this work. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 45}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 46}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 46}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 46}, {"type": "text", "text": "Justification: No new model or data is presented in this paper. The paper is a theoretical study of neural networks. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 46}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 46}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 46}, {"type": "text", "text": "Justification: The paper does not use existing assests. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 46}, {"type": "text", "text": "", "page_idx": 47}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: The paper does not use new assets. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 47}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 47}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 47}, {"type": "text", "text": "", "page_idx": 48}]