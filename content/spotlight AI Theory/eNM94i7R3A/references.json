{"references": [{"fullname_first_author": "Simon S Du", "paper_title": "Gradient descent provably optimizes over-parameterized neural networks", "publication_date": "2018-10-02", "reason": "This paper provides foundational theoretical support for the convergence properties of gradient descent in overparameterized neural networks, a key aspect of the lazy learning regime contrasted with the rich learning regime in this paper."}, {"fullname_first_author": "Arthur Jacot", "paper_title": "Neural tangent kernel: Convergence and generalization in neural networks", "publication_date": "2018-12-01", "reason": "This paper introduces the Neural Tangent Kernel (NTK), a crucial concept for understanding the dynamics of infinite-width neural networks and the lazy training regime, which is compared and contrasted to the rich regime in this paper."}, {"fullname_first_author": "Lenaic Chizat", "paper_title": "On lazy training in differentiable programming", "publication_date": "2019-12-01", "reason": "This paper significantly advances our understanding of lazy training by demonstrating that many overparameterized neural networks remain close to their initialization during training, a phenomenon contrasted with the rich learning regime explored in this paper."}, {"fullname_first_author": "Shahar Azulay", "paper_title": "On the implicit bias of initialization shape: Beyond infinitesimal mirror descent", "publication_date": "2021-07-12", "reason": "This paper investigates how different initialization schemes (specifically layer-wise initialization variances) influence the learning dynamics and inductive biases of neural networks, a crucial aspect of the transition between lazy and rich regimes explored in this paper."}, {"fullname_first_author": "Andrew M Saxe", "paper_title": "Exact solutions to the nonlinear dynamics of learning in deep linear networks", "publication_date": "2013-12-12", "reason": "This paper provides a foundational understanding of the learning dynamics in deep linear networks, setting the stage for understanding more complex nonlinear networks and the transition between lazy and rich learning regimes explored in this paper."}]}