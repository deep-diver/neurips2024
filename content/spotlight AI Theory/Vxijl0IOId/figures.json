[{"figure_path": "Vxijl0IOId/figures/figures_4_1.jpg", "caption": "Figure 1: Dual-Stack Model (DSM)", "description": "The Dual-Stack Model (DSM) is a neural network architecture for approximating the Generalized Linear Programming Value Function (GVF).  It consists of two stacks of feedforward neural networks: a \u03b3-stack (for objective vectors) and a \u03b2-stack (for constraint bounds). Each stack uses piecewise linear activation functions (ReLU or Max-pooling). The \u03b2-stack enforces input convexity by using non-positive weights in the first layer and non-negative weights in subsequent layers. The output is the maximum element of the dot product between the outputs of the \u03b3-stack and \u03b2-stack, mimicking the GVF's structure as a maximum of bilinear functions. The model is designed for unsupervised learning, avoiding the need to solve LPs during training.", "section": "4.2 The Dual-Stack Model"}, {"figure_path": "Vxijl0IOId/figures/figures_5_1.jpg", "caption": "Figure 2: An illustration of how we learn a GVF, looking at slices along the constraint bounds (Top) and objective coefficients (Bottom). Given only data points B \u00d7 C and no constraints, either (7b) or weight-sign, maximizing \u03a3\u03b3\u2208Dc \u03a3\u03b2\u2208Db\u03b7\u03b8(\u03b3, \u03b2) will yield a poor initial approximation. Adding constraints (7b) will force the function to be smaller than ha at certain \u201canchor\u201d points which are distinct from the input data points, at which the function will still tend to be large. Adding the weight-sign constraints will force the function to be convex in terms of the constraint bounds, and will therefore produce an approximation that lower bounds ha. Theorem 5 then tells us that, with sufficiently many data points to start, we will eventually recover ha directly.", "description": "This figure shows how the learning process of the Generalized Linear Programming Value Function (GVF) works. It illustrates the impact of adding duality constraints and weight-sign constraints to the optimization process, showing how these constraints improve the approximation towards the true GVF. The initial approximation is poor, but with added constraints and sufficient data, it converges to the true function.", "section": "Learning Generalized Linear Programming Value Functions"}]