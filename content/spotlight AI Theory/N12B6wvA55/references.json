{"references": [{"fullname_first_author": "Luigi Ambrosio", "paper_title": "Gradient Flows: in Metric Spaces and in the Space of Probability Measures", "publication_date": "2005", "reason": "This paper provides the foundational mathematical framework for understanding gradient flows in Wasserstein space, which is central to the paper's theoretical contributions."}, {"fullname_first_author": "Heinz H Bauschke", "paper_title": "A descent lemma beyond Lipschitz gradient continuity: first-order methods revisited and applications", "publication_date": "2017", "reason": "This paper generalizes the classical descent lemma, crucial for analyzing the convergence of gradient-based optimization methods, which is essential for the paper's convergence analysis."}, {"fullname_first_author": "Haihao Lu", "paper_title": "Relatively Smooth Convex Optimization by First-Order Methods, and Applications", "publication_date": "2018", "reason": "This paper introduces the concept of relative smoothness, a key condition for establishing convergence rates of optimization algorithms in non-Euclidean spaces, which is used extensively in the paper's theoretical analysis."}, {"fullname_first_author": "Chris J Maddison", "paper_title": "Dual Space Preconditioning for Gradient Descent", "publication_date": "2021", "reason": "This paper presents the preconditioned gradient descent method, which the current paper extends to the Wasserstein space, providing a dual perspective to the mirror descent algorithm."}, {"fullname_first_author": "Nicolas Lanzetti", "paper_title": "First-Order Conditions for Optimization in the Wasserstein Space", "publication_date": "2022", "reason": "This paper provides key definitions and properties of Wasserstein differentiability and the Wasserstein gradient, which are fundamental to the paper's theoretical and practical approaches for optimizing functionals over probability measures."}]}