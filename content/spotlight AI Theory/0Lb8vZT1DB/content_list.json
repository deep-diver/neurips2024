[{"type": "text", "text": "Reliable Learning of Halfspaces under Gaussian Marginals ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ilias Diakonikolas University of Wisconsin-Madison ilias@cs.wisc.edu ", "page_idx": 0}, {"type": "text", "text": "Lisheng Ren University of Wisconsin-Madison lren29@wisc.edu ", "page_idx": 0}, {"type": "text", "text": "Nikos Zarifis University of Wisconsin-Madison zarifis@wisc.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study the problem of PAC learning halfspaces in the reliable agnostic model of Kalai et al. (2012). The reliable PAC model captures learning scenarios where one type of error is costlier than the others. Our main positive result is a new algorithm for reliable learning of Gaussian halfspaces on $\\mathbb{R}^{d}$ with sample and computational complexity $d^{\\bar{O}(\\log(\\operatorname*{min}\\{1/\\alpha,1/\\epsilon\\}))}\\,\\dot{\\mathrm{min}}(2^{\\log(1/\\epsilon)^{O(\\log(1/\\alpha))}},\\bar{2}^{\\mathrm{poly}(1/\\epsilon)}),$ , where $\\epsilon$ is the excess error and $\\alpha$ is the bias of the optimal halfspace. We complement our upper bound with a Statistical Query lower bound suggesting that the $\\bar{d}^{\\Omega(\\log(1/\\alpha))}$ dependence is best possible. Conceptually, our results imply a strong computational separation between reliable agnostic learning and standard agnostic learning of halfspaces in the Gaussian setting. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Halfspaces (or Linear Threshold Functions) is the class of functions $f:\\mathbb{R}^{d}\\rightarrow\\{\\pm1\\}$ of the form $f(\\mathbf{x})=\\mathrm{sign}(\\langle\\mathbf{w},\\mathbf{x}\\rangle-t)$ , where $\\mathbf{w}\\in\\mathbb{R}^{d}$ is called the weight vector and $t$ is called the threshold. The problem of learning halfspaces is one of the classical and most well-studied problems in machine learning \u2014 going back to the Perceptron algorithm [Ros58] \u2014 and has had great impact on many other influential techniques, including SVMs [Vap98] and AdaBoost [FS97]. ", "page_idx": 0}, {"type": "text", "text": "Here we focus on the task of learning halfspaces from labeled examples. The computational complexity of this learning task crucially depends on the choice of the underlying model. For example, in the realizable PAC model (i.e., with clean labels), the problem is known to be efficiently solvable (see, e.g., [MT94]) via a reduction to linear programming. Unfortunately, this method is quite fragile and breaks down in the presence of noisy labels. In the noisy setting, the computational complexity of the problem depends on the choice of noise model and distributional assumptions. In this work, we study the problem of distribution-specific PAC learning of halfspaces, with respect to Gaussian marginals, in the reliable agnostic model of [KKM12]. Formally, we have the following definition. ", "page_idx": 0}, {"type": "text", "text": "Definition 1.1 ((Positive) Reliable Learning of Gaussian Halfspaces). Let $\\mathcal{H}_{d}$ be the class of halfspaces on $\\mathbb{R}^{d}$ . Given $0\\,<\\,\\epsilon\\,<\\,1$ and i.i.d. samples $(\\mathbf{x},y)$ from a distribution $D$ supported on $\\mathbb{R}^{d}\\times\\{\\pm1\\}$ , where the marginal $D_{\\bf x}$ is the standard Gaussian $\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ , we say that an algorithm reliably learns $\\mathcal{H}_{d}$ to error $\\epsilon$ if the algorithm with high probability outputs a hypothesis $h:\\mathbb{R}^{d}\\to\\{\\pm1\\}$ such that $\\mathbf{Pr}_{(\\mathbf{x},y)\\sim D}[h(\\mathbf{x})=1\\land y=-1]\\leq\\epsilon$ and $\\mathbf{Pr}_{(\\mathbf{x},y)\\sim D}[h(\\mathbf{x})=-1\\land y=1]\\leq\\mathrm{OPT}+\\epsilon$ , def ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{G}(D)=\\{f\\in\\mathcal{H}_{d}:\\underset{(\\mathbf{x},y)\\sim D}{\\mathbf{Pr}}[f(\\mathbf{x})=1\\wedge y=-1]=0\\}\\cup\\{f(\\mathbf{x})=-1\\}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "We say $f=\\operatorname{argmin}_{f\\in{\\mathcal{G}}(D)}\\mathbf{Pr}_{(\\mathbf{x},y)\\sim D}{\\big[}f(\\mathbf{x})=-1{\\wedge}y=1{\\big]}$ is the optimal halfspace on distribution $D$ and for the conditions above that hypothesis $h$ satisfies, we say that h is \u03f5-reliable with respect to $\\mathcal{H}_{d}$ on distribution $D$ . ", "page_idx": 1}, {"type": "text", "text": "In other words, a (positive) reliable learner makes almost no false positive errors, while also maintaining the best possible false negative error \u2014 as compared to any hypothesis with no false positive errors. Note that if there is no hypothesis that makes no false positive errors, then essentially we have no requirement for the false negative error of the returned hypothesis. The algorithm can then simply return the $-1$ constant hypothesis. ", "page_idx": 1}, {"type": "text", "text": "The reliable agnostic PAC model was introduced by [KKM12] as a one-sided analog of the classical agnostic model [Hau92, KSS94], and has since been extensively studied in a range of works; see, e.g., [KKM12, KT14, GKKT17, DJ19, GKKM20, KK21]. The underlying motivation for this definition comes from learning situations where mistakes of one type (e.g., false positive) should be avoided at all costs. Typical examples include spam detection \u2014 where incorrectly detecting spam emails is much less costly than mislabeling an important email as spam \u2014 and detecting network failures \u2014 where false negatives may be particularly harmful. Such scenarios motivate the study of reliable learning, which can be viewed as minimizing a loss function for different costs for false positive and false negative errors (see, e.g., [Dom99, Elk01]). In a historical context, reliable learning is related to the Neyman-Pearson criterion [NP33] in hypothesis testing, which shows that the optimal strategy to minimize one type of error subject to another type of error being bounded is to threshold the likelihood ratio function. More recently, reliable learning has been shown [KK21] to be equivalent to the PQ-learning model [GKKM20] \u2014 a new learning model defined to capture covariance shift. ", "page_idx": 1}, {"type": "text", "text": "The algorithmic task of agnostic reliable learning can be quite challenging. While reliable learning can be viewed as minimizing a loss function with different cost for false positive and false negative error, in general, such a loss function will result in a non-convex optimization problem. As pointed out in [KKM12], distribution-specific reliable learning can be efficiently reduced to distribution-specific agnostic learning (such reduction preserves the marginal distribution). A natural question, serving as a motivation for this work, is whether reliable learning can be qualitatively easier computationally \u2014 for natural concept classes and halfspaces in particular. ", "page_idx": 1}, {"type": "text", "text": "Before we state our contributions, we briefly summarize prior work on agnostically learning Gaussian halfspaces. Recall that in agnostic learning, the goal is to output a hypothesis with error $\\mathrm{OPT}+\\epsilon$ , where OPT is the optimal 0-1 error within the target class. Prior work has essentially characterized the computational complexity of agnostically learning Gaussian halfspaces. Specifically, it is known that the $L_{1}$ -regression algorithm of [KKMS08] is an agnostic learner with complexity $\\dot{d}^{O(1/\\epsilon^{2})}\\,[\\mathrm{DGJ^{+}09}$ , DKN10]. Moreover, there is strong evidence that this complexity upper bound is tight, both in the Statistical Query (SQ) model [DKZ20, GGK20, DKPZ21] and under plausible cryptographic assumptions [DKR23, Tie23]. It is worth noting that the aforementioned hardness results hold even for the subclass of homogeneous halfspaces. ", "page_idx": 1}, {"type": "text", "text": "Given the aforementioned reduction of reliable learning to agnostic learning [KKM12], one can use $L_{1}$ -regression as a reliable agnostic learner for Gaussian halfspaces, leading to complexity $d^{O(1/\\epsilon^{2})}$ . Prior to this work, this was the best (and only known) reliable halfspace learner. Given the fundamental nature of this problem, it is natural to ask if one can do better for reliable learning. ", "page_idx": 1}, {"type": "text", "text": "Can we obtain faster algorithms for reliably learning Gaussian halfspaces, compared to agnostic learning? ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this work, we provide an affirmative answer to this question. ", "page_idx": 1}, {"type": "text", "text": "To formally state our main result, we need the notion of the bias of a Boolean function. ", "page_idx": 1}, {"type": "text", "text": "Definition 1.2 (Bias of Boolean Function). We define the bias $\\alpha\\in[0,1/2]$ of $h:\\mathbb{R}^{d}\\to\\{\\pm1\\}$ under the Gaussian distribution as $\\alpha=\\alpha(h)\\stackrel{\\mathrm{def}}{=}\\operatorname*{min}(\\mathbf{Pr}_{\\mathbf{x}\\sim\\mathcal{N}_{d}}[h(\\mathbf{x})=1],\\mathbf{Pr}_{\\mathbf{x}\\sim\\mathcal{N}_{d}}[h(\\mathbf{x})=-1])$ . ", "page_idx": 1}, {"type": "text", "text": "The main result of this paper is encapsulated in the following theorem: ", "page_idx": 1}, {"type": "text", "text": "Theorem 1.3 (Main Result). Let $D$ be a joint distribution of $(\\mathbf{x},y)$ supported on $\\mathbb{R}^{d}\\times\\{\\pm1\\}$ with marginal $D_{\\mathbf{x}}=\\mathcal{N}_{d}$ and let $\\alpha$ be the bias of the optimal halfspace on distribution $D$ . There is an algorithm that uses $N=d^{O(\\log(\\operatorname*{min}\\{1/\\alpha,1/\\epsilon\\}))}\\operatorname*{min}\\left(2^{\\log(1/\\epsilon)^{O(\\log(1/\\alpha))}},2^{\\mathrm{poly}(1/\\epsilon)}\\right)$ many samples from $D$ , runs in poly $(N,d)$ time and with high probability returns a hypothesis $h(\\mathbf{x})=\\mathrm{sign}(\\langle\\mathbf{w},\\mathbf{x}\\rangle-$ $t$ ) that is \u03f5-reliable with respect to $\\mathcal{H}_{d}$ . Moreover, for $\\epsilon<\\alpha/2$ , any SQ algorithm for the problem requires complexity d\u2126(log(1/\u03b1)). ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "For more detailed theorem statements of our upper and lower bounds, see Appendix B for the algorithmic result and Appendix C for the SQ hardness result. ", "page_idx": 2}, {"type": "text", "text": "Theorem 1.3 gives a significantly more efficient algorithm (in terms of both sample and computational complexity) for reliably learning Gaussian halfspaces, compared to agnostic learning. Specifically, as long as $\\alpha>0$ is a universal constant, the overall complexity is polynomial in $d$ and quasi-polynomial in $1/\\epsilon$ \u2014 as opposed to $d^{\\mathrm{poly}(1/\\epsilon)}$ in the agnostic setting. While the complexity of our algorithm (namely, the multiplicative factor that is independent of $d$ ) increases as the optimal halfspace becomes more biased, it always dominates the complexity of agnostic learning. ", "page_idx": 2}, {"type": "text", "text": "Our algorithm also applies to the fully reliable learning model [KKM12], since one can easily reduce the fully reliable learning to positive reliable and negative reliable learning (as observed in [KKM12]). ", "page_idx": 2}, {"type": "text", "text": "1.1 Overview of Techniques ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Instead of directly considering the optimal halfspace for a distribution $D$ as defined in Definition 1.1, we introduce the following definition as a relaxation. ", "page_idx": 2}, {"type": "text", "text": "Definition 1.4 (Reliability Condition). We say that a distribution $D$ supported on $X\\times\\{\\pm1\\}$ satisfies the reliability condition with respect to $f:X\\mapsto\\{\\pm1\\}\\;i f\\mathbf{Pr}_{(x,y)\\sim D}[f(x)=+1\\wedge y=-1]=0$ . ", "page_idx": 2}, {"type": "text", "text": "Notice that $h$ being $\\epsilon$ -reliable on distribution $D$ is equivalent to $h$ having better false negative error compared with any $f$ such that $D$ satisfies the reliability condition with respect to $f$ (instead of compared with the optimal halfspace). At the same time, given any fixed $f$ , such a definition allows the adversary to arbitrarily corrupt negative labels, thus allowing more straightforward analysis. ", "page_idx": 2}, {"type": "text", "text": "SQ Lower Bound As follows from Definition 1.4, the adversary can arbitrarily corrupt the negative labels. We leverage this idea to construct an instance where the corruption level suffices to match many moments, meaning that labels $y$ are uncorrelated with any polynomial of degree at most $\\log(1/\\alpha)$ . To show this, we construct an (infinite) feasibility Linear Program (LP) with the following properties: if the LP is feasible, there exists an instance for which no low-degree polynomial correlates with the labels; see Lemma 3.3. To show the existence of such an instance, we leverage a technique from [DKPZ21] that exploits (an infinite generalization of) LP duality. Specifically, we show that it is possible to add noise to the labels whose uncorrupted value is negative so that all low-degree moments are uncorrelated with the observed labels $y$ . This implies that no algorithm that relies on low-degree polynomials can succeed for reliable learning. This allows us to show that SQ algorithms fail if they do not use queries with tolerance at most $1/\\bar{d}^{\\Omega(\\log(1/\\alpha))}$ or exponentially many queries. ", "page_idx": 2}, {"type": "text", "text": "Reliable Learning Algorithm As discussed in the previous paragraph, an adversary can arbitrarily corrupt the labels which can make various algorithmic approaches fail. In more detail, the adversary can corrupt $\\Omega(\\log(1/\\alpha))$ moments. One of the main difficulties of this setting is that there may exist weight vectors $\\ensuremath{\\mathbf{w}},\\ensuremath{\\mathbf{w}}^{\\prime}$ with $\\|\\mathbf{w}-\\mathbf{w}^{\\prime}\\|_{2}\\ge\\Omega(1)$ while at the same time the error of w and $\\mathbf{w}^{\\prime}$ is nearly the same. This suggests that no approach can verify that the algorithm decreases the angle between the current weight vector and the optimal vector. To overcome this obstacle, we develop an algorithm that performs a random walk over a low-dimensional subspace. We prove that at some point during its execution, the algorithm will find a weight vector sufficiently close to the optimal vector. ", "page_idx": 2}, {"type": "text", "text": "To show that such a subspace exists, we first prove that there exists a non-trivial polynomial $p$ that correlates with the negative labels (the term nontrivial means that we cannot use the constant polynomial, as this trivially correlates with the labels); see Claim B.5. This means that the low-degree moment tensors, i.e., $\\mathbf{E}_{(\\mathbf{x},y)\\sim\\mathcal{D}}[\\mathbb{1}\\{y=-1\\}\\mathbf{x}^{\\otimes k}]$ for some $k$ between 1 and $O(\\log(1/\\alpha))$ , correlate non-trivially with $(\\mathbf{w}^{*})^{\\otimes k}$ . Thus, we can use these moment tensors to construct a low-dimensional subspace. We leverage the structure of the problem, namely that the negative labels are uncorrupted, to show that the correlation is in fact almost constant. As a consequence, this allows us to construct a subspace that depends only on the degree of the polynomial $p$ . ", "page_idx": 2}, {"type": "text", "text": "We then proceed as follows: in each round, as long as the current solution w is not optimal, i.e., there are negative samples on the region $\\left\\{\\mathbf{x}\\in\\mathbb{R}^{d}:\\mathbf{w}\\cdot\\bar{\\mathbf{x}}-t\\geq0\\right\\}$ , our algorithm conditions on a thin strip, projects the points $\\mathbf{x}$ on the orthogonal complement of w, and re-applies the above structure result. This leads (with constant probability) to an increase in the correlation between w and $\\mathbf{w}^{*}$ . Assuming that the correlation is increased by $\\beta$ in each round with probability at least $1/3$ , we roughly need at most $1/\\beta$ successful updates. Therefore, if we run our algorithm for $3^{1/\\beta}$ steps, it is guaranteed that with probability at least $1/3$ we will find a vector almost parallel to $\\mathbf{w}^{*}$ . ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Prior Algorithmic Techniques Roughly speaking, prior techniques for reliable learning relied on some variant of $L_{1}$ -regression. In that sense, our algorithmic approach departs from a direct polynomial approximation. Concretely, for distribution-free reliable learning, the algorithm from [KT14] uses a one-sided variant of $L_{1}$ regression \u2014 where instead of approximating the target function in $L_{1}$ , they use the hinge loss instead. For distribution-specific (in particular Gaussian) reliable learning of halfspaces, the only previous algorithm uses the reduction of [KKM12] to agnostic learning. While our algorithm also leverages polynomial approximation, our approach employs significantly different ideas and we believe it provides a novel perspective for this problem. ", "page_idx": 3}, {"type": "text", "text": "Comparison with Prior Work Our algorithm shares similarities with the algorithm of $[\\mathrm{DKK}^{+}22]$ for learning Gaussian halfspaces with Massart noise (a weaker semi-random noise model). Both algorithms perform a random walk in order to converge to the target halfpace. Having said so, our algorithm is fundamentally different than theirs for the following reasons. The algorithm of $[\\mathrm{DKK}^{+}22]$ partitions $\\mathbb{R}^{d}$ into sufficiently small rectangles and searches in each one of them for a direction to update the current hypothesis at random. Our algorithm instead conditions on $y=-1$ , which are the points that are uncorrupted, and uses the guarantee (that we establish) that the first $O(\\log(1/\\alpha))$ moments of the distribution (conditioned on $y\\,=\\,-1\\,$ ) correlate with the unknown optimal hypothesis. This leads to an algorithm with significantly faster runtime. ", "page_idx": 3}, {"type": "text", "text": "Our SQ lower bound leverages techniques from [DKPZ21]. Essentially, we formulate a linear program to construct a noise function that make all low-degree polynomials uncorrelated with the labels $y$ . This condition suffices to show that no SQ algorithm can solve the problem without relying on higher moments. To prove the existence of such a noise function, we use (a generalization of) LP duality of an infinite LP, which provides the necessary conditions for the existence of such a function. ", "page_idx": 3}, {"type": "text", "text": "1.2 Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We use lowercase boldface letters for vectors and capitalized boldface letters for matrices and tensors. We use $\\langle\\mathbf{x},\\mathbf{y}\\rangle$ for the inner product between x $\\mathbf{\\nabla},\\mathbf{y}\\in\\mathbb{R}^{d}$ . For x, $\\mathbf{s}\\in\\mathbb{R}^{d}$ , we use $\\begin{array}{r}{\\mathrm{proj}_{\\mathbf{s}}(\\mathbf{x})\\ {\\stackrel{\\mathrm{def}}{=}}\\ {\\frac{\\langle\\mathbf{x},\\mathbf{s}\\rangle\\,\\mathbf{s}}{\\|\\mathbf{s}\\|_{2}^{2}}}}\\end{array}$ the projection of $\\mathbf{x}$ on the s direction. Similarly, we use pro $|_{\\bot\\mathbf{s}}(\\mathbf{x})=\\mathbf{x}-\\mathrm{proj}_{\\mathbf{s}}(\\mathbf{x})$ for the projection of $\\mathbf{x}$ on the orthogonal complement of s. Additionally, let $\\mathbf{x}^{V}\\in\\mathbb{R}^{\\dim(V)}$ be the projection of $\\mathbf{x}$ on the subspace $V$ and reparameterized on $\\mathbb{R}^{\\mathrm{dim}(V)}$ . More precisely, let $\\mathbf{B}_{V}\\in\\mathbb{R}^{d\\times\\dim(\\bar{V})}$ be the matrix whose columns form an (arbitrary) orthonormal basis for the subspace $V$ , and let $\\mathbf{x}^{V}\\ {\\stackrel{\\mathrm{def}}{=}}\\ (\\mathbf{B}_{V})\\mathsf{T}\\mathbf{x}$ . For $p\\geq1$ and $\\mathbf{x}\\in\\mathbb{R}^{d}$ , we use $\\begin{array}{r}{\\|\\mathbf{x}\\|_{p}\\stackrel{\\mathrm{def}}{=}\\left(\\sum_{i=1}^{n}\\left|\\mathbf{x}_{i}\\right|^{p}\\right)^{1/p}}\\end{array}$ to denote the $\\ell_{p}$ -norm of $\\mathbf{x}$ . For a matrix or tensor $\\mathbf{T}$ , we denote by $\\|\\mathbf{T}\\|_{F}$ the Frobenius norm of $\\mathbf{T}$ . ", "page_idx": 3}, {"type": "text", "text": "We use $\\mathcal{N}_{d}$ to denote the standard normal distribution $\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ , where 0 is the $d\\!\\cdot$ -dimensional zero vector and $\\mathbf{I}$ is the $d\\times d$ identity matrix. We use $\\mathbf{x}\\sim D$ to denote a random variable with distribution $D$ . For a random variable $\\mathbf{x}$ (resp. a distribution $D$ ), we use $P_{\\mathbf{x}}$ (resp. $P_{D}$ ) to denote the probability density function or probability mass function of the random variable $\\mathbf{x}$ (resp. distribution $D$ ). We also use $\\Phi:\\mathbb{R}\\mapsto[0,1]$ to denote the cdf function of $\\mathcal{N}_{1}$ . We use 1 to denote the indicator function. For a boolean function $h:\\mathbb{R}^{d}\\to\\{\\pm1\\}$ and a distribution $D$ supported on $\\mathbb{R}^{d}\\times\\{\\pm1\\}$ , we use $R_{+}(h;D)\\stackrel{\\mathrm{def}}{=}\\mathbf{Pr}_{(\\mathbf{x},y)\\sim D}[h(\\mathbf{x})=1\\wedge y\\neq1]$ (resp. $R_{-}(h;D)\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\mathbf{Pr}_{(\\mathbf{x},y)\\sim D}[h(\\mathbf{x})=-1\\wedge y\\neq-1]$ ", "page_idx": 3}, {"type": "text", "text": ") to denote the false positive (resp. false negative) 0-1 error. ", "page_idx": 3}, {"type": "text", "text": "2 Algorithm for Reliably Learning Gaussian Halfspaces ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we describe and analyze our algorithm establishing Theorem 1.3. Due to space limitations, some proofs have been deferred to Appendix B. For convenience, we will assume that $\\alpha$ (the bias of the optimal halfspace) is known to the algorithm and that the excess error satisfies $\\epsilon\\leq\\alpha/3$ . These assumptions are without loss of generality for the following reasons: First, one can efficiently reduce the unknown $\\alpha$ case to the case that $\\alpha$ is known, by guessing the value of $\\alpha$ . Second, if $\\epsilon$ is large, there is a straightforward algorithm for the problem (simply output the best constant hypothesis). ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Notation: We use the notation $\\mathcal{H}_{d}^{\\alpha}$ for the set of all LTFs on $\\mathbb{R}^{d}$ whose bias is equal to $\\alpha$ . Given the above assumptions, it suffices for us to give a reliable learning algorithm for $\\mathcal{H}_{d}^{\\alpha}$ instead of $\\mathcal{H}_{d}$ . ", "page_idx": 4}, {"type": "text", "text": "The high-level idea of the algorithm is as follows. Without loss of generality, we assume that there exists an $\\alpha$ -biased halfspace that correctly classifies all the points with label $y\\,=\\,-1$ \u2014 since otherwise, the algorithm can just return the hypothesis $h(\\mathbf{x})\\equiv-1$ . Let $f(\\mathbf{x})=\\mathrm{sign}(\\langle\\mathbf{w}^{*},\\mathbf{x}\\rangle-t^{*})$ be the optimal halfspace and let w be our current guess of $\\mathbf{w}^{*}$ . Assuming that $\\mathbf{w}^{*}$ is not sufficiently close to w and the hypothesis that classifies all the points as negative is not optimal, we show that there exists a low-degree polynomial of the form $p(\\langle\\mathbf{w},\\mathbf{x}\\rangle)$ with correlation at least $2^{-O(t^{*2})}\\epsilon$ with the negative labels. By leveraging this structural result, we use a spectral algorithm to find a direction $\\mathbf{v}$ that is non-trivially correlated with pro $\\bot\\mathbf{w}^{\\left(\\mathbf{w}^{*}\\right)}$ with at least some constant probability. Unfortunately, it is not easy to verify whether $\\langle\\mathbf{v},\\operatorname{proj}_{\\bot\\mathbf{w}}(\\mathbf{w}^{*})\\rangle$ is non-trivial. However, conditioned on the algorithm always getting a $\\mathbf{v}$ that has good correlation, we show that it only takes at most $\\log(1/\\epsilon)^{O(t^{*}^{2})}$ steps to get sufficiently close to $\\mathbf{w}^{*}$ . Therefore, repeating this process $2^{\\log(1/\\epsilon)^{O(t^{*}^{(2})})}$ many times will eventually find an accurate approximation to $\\mathbf{w}^{*}$ . ", "page_idx": 4}, {"type": "text", "text": "2.1 Finding a Non-Trivial Direction ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Here, we present an algorithm that finds a direction that correlates non-trivially with the unknown optimal vector. We first show that there exists a zero-mean $O(\\log(1/\\alpha))$ -degree polynomial that sign-matches the optimal hypothesis. Furthermore, using the fact that the optimal hypothesis always correctly guesses the sign of the negative points, this gives us that the polynomial correlates with the negative (clean) points. Using this intuition, our algorithm estimates the first $O(\\log(1/\\alpha))$ moments of the distribution $D_{\\mathbf{x}}$ conditioned on $y=-1$ . This guarantees that at least one moment correlates with the optimal hypothesis, as a linear combination of the moments generates the sign-matching polynomial. Then, by taking a random vector that lies in the high-influence directions (which form a low-dimensional subspace), we guarantee that with constant probability, this vector correlates well with the unknown optimal vector. The main result of the section is the following. ", "page_idx": 4}, {"type": "text", "text": "Proposition 2.1. Let $D$ be a joint distribution of $\\left(\\mathbf{x},y\\right)$ supported on $\\mathbb{R}^{d}\\times\\{\\pm1\\}$ with marginal $D_{\\mathbf{x}}\\,=\\,\\mathcal{N}_{d}$ and $\\epsilon\\,\\in\\,(0,1)$ . Suppose $D$ satisfies the reliability condition with respect to $f(\\mathbf{x})\\,=$ $\\mathrm{sign}(\\langle\\mathbf{w}^{*},\\mathbf{x}\\rangle-t^{*})$ with $t^{*}\\,=\\,O\\left(\\sqrt{\\log(1/\\epsilon)}\\right)$ and $\\mathbf{Pr}_{(\\mathbf{x},y)\\sim D}[y\\,=\\,-1]\\,\\geq\\,\\epsilon$ . Then there is an algorithm that draws $N=d^{O(t^{*}^{2})}/\\epsilon^{2}$ samples, has $\\mathrm{poly}(N)$ runtime, and with probability at least $\\Omega(1)$ returns a unit vector v such that $\\langle\\mathbf{v},\\mathbf{w}^{*}\\rangle\\ge\\operatorname*{max}(\\log(1/\\epsilon)^{-O({t^{*}}^{2})},\\epsilon^{-O(1)})$ . ", "page_idx": 4}, {"type": "text", "text": "We start by showing that for any distribution satisfying the reliability condition with respect to some $f(\\mathbf{x})=\\mathrm{sign}(\\langle\\mathbf{w}^{*},\\mathbf{x}\\rangle-t^{*})$ , there exists a degree- $O(t^{*^{2}})$ zero-mean polynomial of the form $p(\\langle\\mathbf{w}^{*},\\mathbf{x}\\rangle)$ that has correlation at least ${2^{-O(t^{*}{}^{2})}\\,\\mathbf{Pr}}_{(\\mathbf{x},y)\\sim D}[y=-1]$ with the labels. ", "page_idx": 4}, {"type": "text", "text": "Lemma 2.2 (Correlation with an Orthonormal Polynomial). Let $D$ be a joint distribution of $(\\mathbf{x},y)$ supported on $\\mathbb{R}^{d}\\times\\{\\pm1\\}$ with marginal $D_{\\mathbf{x}}=\\mathcal{N}_{d}$ . Suppose $D$ satisfies the reliability condition with respect to $f(\\mathbf{x})=\\mathrm{sign}(\\langle\\mathbf{w}^{*},\\mathbf{x}\\rangle-t^{*})$ . Then there exists a polynomial $p:\\mathbb{R}\\mapsto\\mathbb{R}$ of degree at most $k=O({t^{*}}^{2}+1)$ such that $\\mathbf{E}_{z\\sim\\mathcal{N}_{1}}[p(z)]\\,=\\,0$ , $\\mathbf{E}_{z\\sim\\mathcal{N}_{1}}[p^{2}(z)]\\,=\\,1$ and $\\begin{array}{r l}{\\mathbf{E}_{(\\mathbf{x},y)\\sim D}[y\\,p(\\langle\\mathbf{w}^{*},\\mathbf{x}\\rangle)]\\,=\\,}&{{}}\\end{array}$ 2\u2212O(t\u22172) Pr(x,y)\u223cD[y = \u22121]. ", "page_idx": 4}, {"type": "text", "text": "Proof Sketch of Lemma 2.2. Note that since $p$ is a zero-mean polynomial with respect to $D_{\\bf x}$ , we have that ${\\bf E}_{({\\bf x},{y})\\sim{D}}[y\\,p(\\langle{{\\bf w}^{*}},{{\\bf x}}\\rangle)]=-2\\,{\\bf E}_{({\\bf x},{y})\\sim{D}}[{\\bf1}(y=-1)\\,\\bar{p(}\\langle{{\\bf\\dot{w}}^{*}},{{\\bf x}}\\rangle)]$ . Then, using the fact that the distribution $D$ satisfies the reliability condition, the statement boils down to showing that for any $\\epsilon$ -mass inside the interval $[t^{*},\\infty]$ , the expectation of $p$ on this $\\epsilon$ mass is at least $2^{-O(t^{*}^{2})}$ . To prove this statement, it suffices to construct a polynomial $p$ that is non-negative on $[t^{*},\\infty]$ and such that the $\\epsilon/2$ -tail of $p$ in that interval is at least $2^{-O(t^{*2})}$ . To achieve this, we show that the sign-matching polynomial used in $[\\mathrm{DKK}^{+}22]$ meets our purpose. \u53e3 ", "page_idx": 4}, {"type": "text", "text": "Our algorithm will use the following normalized Hermite tensor. ", "page_idx": 4}, {"type": "text", "text": "Definition 2.3 (Hermite Tensor). For $k\\in\\mathbb{N}$ and $\\mathbf{x}\\in\\mathbb{R}^{d}$ , we define the $k$ -th Hermite tensor as ", "page_idx": 5}, {"type": "equation", "text": "$$\n(\\mathbf{H}_{k}(\\mathbf{x}))_{i_{1},i_{2},...,i_{k}}=\\frac{1}{\\sqrt{k!}}\\sum_{\\substack{P a r t i t i o n s\\;P\\;o f\\,[k]}}\\bigotimes_{\\{a,b\\}\\in P}(-\\mathbf{I}_{i_{a},i_{b}})\\bigotimes_{\\{c\\}\\in P}\\mathbf{x}_{i_{c}}\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Given that there is such a polynomial, we show that if we take a flattened version of the Chowparameter tensors (which turns them into matrices) and look at the space spanned by their top singular vectors, then a non-trivial fraction of $\\mathbf{w}^{*}$ must lie inside this subspace. We prove the following lemma, which is similar to Lemma 5.10 in $[\\mathrm{DKK}^{+}22]$ . See Appendix Appendix B for the proof. ", "page_idx": 5}, {"type": "text", "text": "Lemma 2.4. Let $D$ be the joint distribution of $\\left(\\mathbf{x},y\\right)$ supported on $\\mathbb{R}^{d}\\times\\{\\pm1\\}$ with marginal $D_{\\mathbf{x}}=\\mathcal{N}_{d}$ . Let $p:\\mathbb{R}\\mapsto\\mathbb{R}$ be a univariate, mean zero and unit variance polynomial of degree $k$ such that for some unit vector $\\mathbf{v}^{*}\\in\\mathbb{R}^{d}$ it holds $\\begin{array}{r}{\\mathbf{E}_{(\\mathbf{x},y)\\sim D}[\\mathbf{1}(y=-1)p(\\langle\\mathbf{v}^{*},\\bar{\\mathbf{x}}\\rangle)]\\geq\\tau}\\end{array}$ for some $\\tau\\in(0,1]$ . Let $\\mathbf{T}^{\\prime m}$ be an approximation of the order- $m$ Chow-parameter tensor ${\\bf T}^{m}\\,=\\,{\\bf E}_{({\\bf x},y)\\sim D}[{\\bf1}(y\\,=$ $-1)\\mathbf{H}_{m}(\\mathbf{x})]$ such that $\\|\\mathbf{T}^{\\prime m}-\\mathbf{T}^{m}\\|_{F}\\leq\\tau/(4\\sqrt{k})$ . Denote by $V_{m}$ the subspa\u221ace spanned by the left singular vectors of flattened $\\mathbf{T}^{\\prime m}$ whose singular values are greater than $\\tau/(4\\sqrt{k})$ . Moreover, denote by $V$ the union of $V_{1},\\cdots,V_{k}$ . Then, for $\\gamma=\\mathbf{Pr}_{(\\mathbf{x},y)\\sim D}[y=-1]$ , we have that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\|\\mathrm{proj}_{V}(\\mathbf{v}^{*})\\|_{2}=\\Omega\\,\\Big(\\tau/\\left(\\sqrt{k}\\gamma\\log(1/\\gamma)^{k/2}\\right)\\!\\Big).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "By Lemma 2.4, taking a random unit vector $\\mathbf{v}$ in $V$ will give us $\\langle\\mathbf{v},\\mathbf{v}^{*}\\rangle\\geq\\|\\mathrm{proj}_{V}(\\mathbf{v}^{*})\\|/\\sqrt{\\dim(V)}$ .   \nWe are ready to prove Proposition 2.1. For the algorithm pseudocode, see Appendix B. ", "page_idx": 5}, {"type": "text", "text": "Proof Sketch of Proposition 2.1 . The idea is that the empirical estimate $\\mathbf{T}^{\\prime m}$ obtained using $d^{O(\\operatorname*{max}\\{t^{*}{}^{2},1\\})}\\log(1/\\delta)/\\epsilon^{2}$ many samples will satisfy $\\|\\mathbf{T}^{\\prime m}-\\mathbf{T}^{m}\\|_{F}\\leq\\tau/(4\\sqrt{k})$ with high probability. Then, by Lemma 2.2 and Lemma 2.4, if we take $\\mathbf{v}$ to be a random unit vector in $V$ , with constant probability we will have $\\langle\\mathbf{v},\\mathbf{w}^{*}\\rangle\\,=\\,\\log(1/\\epsilon)^{-O({t^{*}}^{2})}$ . For $\\langle\\mathbf{v},\\mathbf{w}^{*}\\rangle\\,=\\,\\epsilon^{-O(1)}$ , the proof relies on a different version of Lemma 2.4. \u53e3 ", "page_idx": 5}, {"type": "text", "text": "2.2 Random Walk to Update Current Guess ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We now describe how we use Proposition 2.1 to construct an algorithm for our learnin\u221ag problem. Let w be the current guess for $\\mathbf{w}^{*}$ . For convenience, we assume that $\\langle\\mathbf{w},\\mathbf{w}^{*}\\rangle=\\Omega(1/\\sqrt{d})$ . Let $D^{\\prime}$ be the distribution of $\\mathbf{x}^{\\perp\\mathbf{w}}$ conditioned on $\\mathbf{x}\\in B$ , where ${\\cal B}=\\{{\\bf x}\\in\\mathbb{R}^{d}:\\langle{\\bf w},{\\bf x}\\rangle-t^{*}\\geq0\\}$ . We next show that the $\\mathbf{x}$ -marginals of $D^{\\prime}$ are standard Gaussian and that $D^{\\prime}$ satisfies the reliability condition with respect to the halfspace $h(\\mathbf{x})=\\mathrm{sign}(\\langle\\mathbf{w}^{\\prime},\\mathbf{x}\\rangle-t^{\\prime})$ with $\\mathbf{w}^{\\prime}=\\mathbf{w}^{*\\perp\\mathbf{w}}$ and $|t^{\\prime}|\\leq|t^{*}|$ . ", "page_idx": 5}, {"type": "text", "text": "Lemma 2.5. Let $D$ be the joint distribution of $\\left(\\mathbf{x},y\\right)$ supported on $\\mathbb{R}^{d}\\times\\{\\pm1\\}$ with marginal $D_{\\mathbf{x}}\\,=\\,\\mathcal{N}_{d}$ and $\\epsilon\\,\\in\\,(0,1)$ . Suppose $D$ satisfies the reliability condition with respect to $f(\\mathbf{x})\\,=$ $\\mathrm{sign}(\\langle\\mathbf{w}^{*},\\mathbf{x}\\rangle-t^{*})$ . Suppose that $h(\\mathbf{x})=\\mathrm{sign}(\\langle\\mathbf{w},\\mathbf{x}\\rangle-t)$ with $\\langle\\mathbf{w},\\mathbf{w}^{*}\\rangle>0$ and $t-t^{*}\\in[0,\\epsilon/100]$ does not satisfy $R_{h}^{+}(D)\\leq\\epsilon/2$ . Let $B=\\{\\mathbf{x}\\in\\mathbb{R}^{d}:\\langle\\mathbf{w},\\mathbf{x}\\rangle-t\\geq0\\}$ and $D^{\\prime}$ be the distribution of $(\\mathbf{x}^{\\prime},y)=(\\mathbf{x}^{\\bot\\mathbf{w}},y)$ given $\\mathbf{x}\\in B$ , then ", "page_idx": 5}, {"type": "text", "text": "1. $D^{\\prime}$ has marginal distribution $D_{\\mathbf{x}^{\\prime}}=\\mathcal{N}_{d-1}$ , ", "page_idx": 5}, {"type": "text", "text": "2. $D^{\\prime}$ satisfies the reliability condition with respect to $h^{\\prime}(\\mathbf{x})\\,=\\,\\mathrm{sign}(\\langle\\mathbf{w}^{\\prime},\\mathbf{x}\\rangle\\,-\\,t^{\\prime})$ , where ${\\bf w}^{\\prime}=$ $\\mathbf{w}^{\\ast\\perp\\mathbf{w}}/\\lVert\\mathbf{w}^{\\ast\\perp\\mathbf{w}}\\rVert_{2}$ and $|t^{\\prime}|\\leq|t^{*}|,$ , and   \n3. $\\mathbf{Pr}_{(\\mathbf{x}^{\\prime},y)\\sim D^{\\prime}}[y=-1]\\geq\\epsilon/2.$ . ", "page_idx": 5}, {"type": "text", "text": "Proof Sketch of Lemma 2.5. Item 1 follows from the definition. For Item 2, we consider two cases: $t^{*}>0$ ; and $t^{*}\\leq0$ . For the case $t^{*}\\leq0$ , we prove that the distribution $D^{\\prime}$ satisfies the reliability condition with respect to $h^{\\prime}(\\mathbf{x})\\,=\\,\\mathrm{sign}(\\langle\\mathbf{w}^{\\prime},\\mathbf{x}\\rangle)$ , where $\\mathbf{w}^{\\prime}\\,=\\,\\mathbf{w}^{*\\,\\perp\\mathbf{w}}/\\Vert\\mathbf{w}^{*\\,\\perp\\mathbf{w}}\\Vert_{2}$ . For the case $t^{*}>0$ , we prove that the distribution $D^{\\prime}$ satisfies the reliability condition with respect to $h^{\\prime}(\\mathbf{x})=$ $\\mathrm{sign}(\\langle\\mathbf{w}^{\\prime},\\mathbf{x}\\rangle-t^{\\prime})$ , where $\\mathbf{w}^{\\prime}=\\mathbf{w}^{*\\perp\\mathbf{w}}/\\lVert\\mathbf{w}^{*\\perp\\mathbf{w}}\\rVert_{2}$ and $t^{\\prime}=t^{*}$ . Then Item 3 follows from the fact that $h$ does not satisfy $R_{h}^{+}(D)\\leq\\epsilon/2$ . \u53e3 ", "page_idx": 5}, {"type": "text", "text": "By Lemma 2.5, given any current guess w such that $h(\\mathbf{x})\\,=\\,\\mathrm{sign}(\\langle\\mathbf{w},\\mathbf{x}\\rangle-t^{*})$ does not satisfy $\\bar{R_{+}}(h;D)\\leq\\epsilon/2$ , the corresponding distribution $D^{\\prime}$ satisfies the reliability condition with respect to $h^{\\prime}$ and $\\mathbf{Pr}_{(\\mathbf{x},y)\\sim D^{\\prime}}[y=-1]\\geq\\epsilon/2$ . Therefore, $D^{\\prime}$ satisfies the assumptions of Proposition 2.1. So, if we apply the algorithm in Proposition 2.1, we will with probability at least $\\Omega(1)$ get a unit vector $\\mathbf{v}$ such that $\\langle\\mathbf{v},\\mathbf{w}\\rangle=0$ and $\\left\\langle\\mathbf{v},\\mathbf{w}^{*}\\right\\rangle=\\log(1/\\epsilon)^{-O({t^{*}}^{2})}$ . ", "page_idx": 6}, {"type": "text", "text": "The following fact shows that by updating our current guess in the direction of $\\mathbf{v}$ with appropriate step size, we can get an updated guess with increased correlation with $\\mathbf{w}^{*}$ . ", "page_idx": 6}, {"type": "text", "text": "Fact 2.6 (Correlation Improvement, Lemma 5.13 in $[\\mathrm{DKK}^{+}22]$ ). Fix unit vectors $\\mathbf{v}^{*}$ , $\\mathbf{v}\\in\\mathbb{R}^{d}$ . Let $\\mathbf{u}\\in\\mathbb{R}^{d}$ such that $\\left\\langle\\mathbf{u},\\mathbf{v}^{*}\\right\\rangle\\geq c,\\,\\left\\langle\\mathbf{u},\\mathbf{v}\\right\\rangle=0$ and $\\|\\mathbf{u}\\|_{2}\\leq1$ with $c>0$ . Then, for $\\begin{array}{r}{\\mathbf v^{\\prime}=\\frac{\\mathbf v+\\lambda\\mathbf u}{\\|\\mathbf v+\\lambda\\mathbf u\\|_{2}}}\\end{array}$ \u2225v+\u03bbu\u22252 , with $\\lambda=c/2$ , we have that $\\langle\\mathbf{v}^{\\prime},\\mathbf{v}^{*}\\rangle\\ge\\langle\\mathbf{v},\\mathbf{v}^{*}\\rangle+\\lambda^{2}/2$ . ", "page_idx": 6}, {"type": "text", "text": "Notice that because $\\|\\mathrm{proj}_{\\perp\\mathbf{w}}(\\mathbf{w}^{*})\\|_{2}$ is unknown, we cannot always choose the optimal step size $\\lambda$ . Instead, we will use the same $\\lambda$ to do sufficiently many update steps such that after that many updates, we are certain that $\\|\\mathrm{proj}_{\\perp\\mathbf{w}}(\\mathbf{w}^{*})\\|_{2}\\leq3\\lambda$ . We then take the new step size $\\lambda_{\\mathrm{update}}=\\lambda/2$ and repeat this process, until w and $\\mathbf{w}^{*}$ are sufficiently close to each other. ", "page_idx": 6}, {"type": "text", "text": "We are now ready to describe our algorithm (Algorithm 1) and prove its correctness. Notice that given we know the bias $\\alpha$ , $t$ must be either $-\\Phi^{-1}(\\alpha)$ or $\\Phi^{-1}(\\alpha)$ . For convenience, we assume that $t=$ $\\Phi^{-1}(\\alpha)$ . To account for the case that $t=-\\Phi^{-1}(\\alpha)$ , we can simply run Algorithm 1 twice and pick the output halfspace with the smallest $t$ value (or even run a different efficient algorithm since $t\\leq0$ as explained in Appendix B). For convenience, we also assume 2log(1/\u03f5)O(log(1/\u03b1)), 2poly(1/\u03f5) = 2log(1/\u03f5)O(log(1/\u03b1)). To account for the other case, we can simply initialize the step size \u03b6 = \u03f5c for a sufficiently large constant $c$ instead of $\\zeta=\\log(1/\\epsilon)^{-c t^{*}^{2}}$ . A detailed version of Algorithm 1 is deferred to Appendix B. ", "page_idx": 6}, {"type": "text", "text": "Input: $\\epsilon\\in(0,1)$ , $\\alpha\\in(0,1/2)$ and samples access to a joint distribution $D$ of $\\left(\\mathbf{x},y\\right)$ supported on   \n$\\mathbb{R}^{d}\\times\\{\\pm1\\}$ with $\\mathbf{x}$ -marginal $D_{\\mathbf{x}}=\\mathcal{N}_{d}$ .   \nOutput: $\\begin{array}{r}{\\dot{h^{\\prime}}(\\mathbf{x})=\\mathrm{sign}(\\langle\\mathbf{w},\\mathbf{x}\\rangle-t)}\\end{array}$ that is $\\epsilon_{}$ -reliable with respect to the class $\\mathcal{H}_{d}^{\\alpha}$ . 1. Check if $\\mathbf{Pr}_{(\\mathbf{x},y)\\sim D}[y=-1]\\le\\epsilon/2$ (with sufficiently small constant failure probability). If so, return the $+1$ constant hypothesis. Set the initial step size $\\zeta=\\log(1/\\epsilon)^{-c t^{*}^{2}}$ , where $c$ is a sufficiently large universal constant and $t=\\Phi^{-1}(\\alpha)$ . 2. Initialize w to be a random unit vector in $\\mathbb{R}^{d}$ . Let the update step size $\\lambda=\\zeta$ and repeat the following process until $\\lambda\\le\\epsilon/100$ . (a) Use samples from $D$ to check if the hypothesis $h(\\mathbf{x})\\,=\\,\\mathrm{sign}(\\langle\\mathbf{w},\\mathbf{x}\\rangle\\,-\\,t)$ satisfies $R_{+}(h;D\\bar{)}\\leq\\epsilon/2$ . If so, go to Step (3). (b) With $1/2$ probability, let $\\mathbf{w}=-\\mathbf{w}$ . Let $B=\\{\\mathbf{x}\\in\\mathbb{R}^{d}:\\langle\\mathbf{w},\\mathbf{x}\\rangle-t\\geq0\\}$ , and let $D^{\\prime}$ be the distribution of $(\\mathbf{x}^{\\perp\\mathbf{w}},y)$ for $(\\mathbf{x},y)\\sim D$ given $\\mathbf{x}\\in B$ . Use the algorithm of Proposition 2.1 on $D^{\\prime}$ to find a unit vector $\\mathbf{v}$ such that $\\langle\\mathbf{v},\\mathbf{w}\\rangle=0$ and $\\begin{array}{r}{\\left\\langle\\mathbf{v},\\frac{\\mathrm{proj}_{\\perp\\mathbf{w}}(\\mathbf{w}^{*})}{\\|\\mathrm{proj}_{\\perp\\mathbf{w}}(\\mathbf{w}^{*})\\|_{2}}\\right\\rangle\\geq\\zeta}\\end{array}$ . Then update w as follows: wupdate = $\\begin{array}{r}{\\mathbf{w}_{\\mathrm{update}}=\\frac{\\mathbf{w}+\\lambda\\mathbf{v}}{\\|\\mathbf{w}+\\lambda\\mathbf{v}\\|_{2}}}\\end{array}$ (c) Repeat Steps (2a) and (2b) $c/\\zeta^{2}$ times, where $c$ is a sufficiently large universal constant, with the same step size $\\lambda$ . After that, update the new step size as $\\bar{\\lambda_{\\mathrm{update}}}=\\lambda/2$ . 3. Check if $h(\\mathbf{x})=\\mathrm{sign}(\\langle\\mathbf{w},\\mathbf{x}\\rangle-t)$ satisfies $R_{+}(h;D)\\leq\\epsilon/2$ . If so, return $h$ and terminate. Repeat Step (2) $2^{1/\\zeta^{c}}$ many times where $c$ is a sufficiently large constant. ", "page_idx": 6}, {"type": "text", "text": "Proof Sketch of the algorithmic part of Theorem 1.3. Let $f(\\mathbf{x})=\\mathrm{sign}(\\langle\\mathbf{w}^{*},\\mathbf{x}\\rangle-t^{*})$ be the optimal halfspace with $\\alpha$ bias. We need to show that with high probability Algorithm 1 returns a hypothesis $h(\\mathbf{x})^{-}{=}\\,\\mathrm{sign}(\\langle\\mathbf{w},\\mathbf{x}\\rangle-t)$ such that $R_{+}(h;D)\\leq\\epsilon$ and $R_{-}(h;D)\\leq R_{-}(f;D)+\\epsilon.$ . ", "page_idx": 6}, {"type": "text", "text": "To do so, it suffices to show that $R_{+}(h;D)\\leq\\epsilon$ ; given $R_{+}(h;D)\\leq\\epsilon,R_{-}(h;D)\\leq R_{-}(f;D)+\\epsilon$ follows from our choice of $t$ . For convenience, we can assume $h$ never satisfies $R_{+}(h;D)\\leq\\epsilon/2$ in ", "page_idx": 6}, {"type": "text", "text": "Step (2a) (otherwise, we are done). We can also assume that the subroutine in Proposition 2.1 always succeeds since the algorithm repeats Step (2) sufficiently many times. Given the above conditions, using by Fact 2.6, one can show that each time after $c/\\zeta^{\\bar{2}}$ many updates in Step (2b), we must have $\\|\\mathrm{proj}_{\\perp\\mathbf{w}}\\mathbf{w}^{*}\\|_{2}\\leq3\\lambda$ . Therefore, when we have $\\lambda\\le\\epsilon/100$ , then $\\|\\mathrm{proj_{\\perpw}w^{*}}\\|_{2}\\leq3\\epsilon/100$ , which implies $R_{+}(\\dot{h};D)\\leq\\epsilon/2$ . \u53e3 ", "page_idx": 7}, {"type": "text", "text": "3 Nearly Matching SQ Lower Bound ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we establish the SQ hardness result of Theorem 1.3. Due to space limitations, some proofs have been deferred to Appendix C. ", "page_idx": 7}, {"type": "text", "text": "Proof Overview To establish our SQ lower bound for reliable learning, we first prove an SQ lower bound for a natural decision version of reliably learning $\\alpha$ -biased LTFs. We define the following decision problem over distributions. ", "page_idx": 7}, {"type": "text", "text": "Definition 3.1 (Decision Problem over Distributions). Let $D$ be a fixed distribution and $\\mathcal{D}$ be $a$ distribution family. We denote by $B(\\mathcal{D},D)$ the decision problem in which the input distribution $D^{\\prime}$ is promised to satisfy either (a) $D^{\\prime}=D$ or (b) $D^{\\prime}\\in\\mathcal{D}$ , and the goal is to distinguish the two cases with high probability. ", "page_idx": 7}, {"type": "text", "text": "We show that given SQ access to a joint distribution $D$ of $\\left(\\mathbf{x},y\\right)$ supported on $\\mathbb{R}^{d}\\times\\{\\pm1\\}$ with marginal $D_{\\mathbf{x}}=\\mathcal{N}(\\mathbf{0,I})$ , it is hard to solve the problem $B(\\mathcal{D},D)$ with the following distributions. ", "page_idx": 7}, {"type": "text", "text": "(a) Null hypothesis: $D$ is the distribution so that $y=1$ with probability $1/2$ independent of $\\mathbf{x}$ . ", "page_idx": 7}, {"type": "text", "text": "(b) Alternative hypothesis: $D\\in\\mathcal{D}$ , where $\\mathcal{D}$ is a family of distributions such that for any distribution $D\\in\\mathcal{D}$ , there exists an $\\alpha$ -biased LTF $f$ that $R_{+}(f;D)=0$ . ", "page_idx": 7}, {"type": "text", "text": "In order to construct such a family of distributions $\\mathcal{D}$ , we start by constructing a joint distribution $D^{\\prime}$ of $(z,y)$ over $\\mathbb{R}\\times\\{\\pm1\\}$ such that the marginal distribution of $z$ is $\\mathcal{N}_{1}$ and the conditional distributions $z\\mid y=1$ and z | $y=-1$ both match many moments with the standard Gaussian $\\mathcal{N}_{1}$ . Moreover, there is $\\alpha$ probability mass on the positive side of the marginal distribution of $z$ that is purely associated with $y=1$ (i.e., $\\mathbf{E}_{(z,y)\\sim D}[y\\mid z\\geq c]=1$ where $c=\\bar{\\Phi}^{-1}(1-\\alpha))$ . We then embed this distribution $D$ along a hidden direction inside the joint distribution of $(\\mathbf{x},y)$ on $\\mathbb{R}^{d}\\times\\{\\pm1\\}$ to construct a family of hard-to-distinguish distributions using the \u201chidden-direction\u201d framework developed in [DKS17, DKPZ21]. ", "page_idx": 7}, {"type": "text", "text": "We can now proceed with the details of the proof. We start by defining the pairwise correlation between distributions. ", "page_idx": 7}, {"type": "text", "text": "Definition 3.2 (Pairwise Correlation). The pairwise correlation of two distributions with pdfs $D_{1},D_{2}:$ $\\mathbb{R}^{d}\\,\\mapsto\\,\\mathbb{R}_{+}$ with respect to a distribution with density $D\\,:\\,\\bar{\\mathbb{R}^{d}}\\,\\mapsto\\,\\mathbb{R}_{+}$ , where the support of $D$ contains the support of $D_{1}$ and $D_{2}$ , is defined as $\\begin{array}{r}{\\chi_{D}(D_{1},D_{2})\\stackrel{\\mathrm{def}}{=}\\int_{\\mathbb{R}^{d}}D_{1}(\\mathbf{x})D_{2}(\\mathbf{x})/D(\\mathbf{x})d\\mathbf{x}-1}\\end{array}$ . Furthermore, the $\\chi$ -squared divergence of $D_{1}$ to $D$ is defined as $\\chi^{2}(D_{1},D)\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\chi_{D}(D_{1},D_{1})$ . ", "page_idx": 7}, {"type": "text", "text": "In particular, the framework in [DKS17] allows us to construct a family of $2^{d^{\\Omega(1)}}$ distributions on $\\mathbb{R}^{d}\\times\\{\\pm1\\}$ whose pairwise correlation is $d^{-\\Omega(n)}$ where $n$ is the number of matching moments $D^{\\prime}$ has with the standard Gaussian. Then, using standard SQ dimension techniques, this gives an SQ lower bound for the distinguishing problem. After that, we reduce the distinguishing problem to the problem of reliably learning $\\alpha$ -biased LTFs under Gaussian marginals with additive error $\\epsilon<\\alpha/3$ . ", "page_idx": 7}, {"type": "text", "text": "In order to construct such a distribution $D^{\\prime}$ of $(z,y)$ supported on $\\mathbb{R}\\times\\{\\pm1\\}$ , we reparameterize $\\mathbf{E}_{(z,y)\\sim D^{\\prime}}[y|z]$ as $g(z)$ . For a function $g:\\mathbb{R}\\rightarrow\\mathbb{R}$ , we use $\\|g\\|_{p}=\\mathbf{E}_{t\\sim\\mathcal{N}_{1}}[|g(t)|^{p}]^{1/p}$ for its $L_{p}$ norm. We let $L^{1}(R)$ denote the set of all functions $g:\\mathbb{R}\\rightarrow\\mathbb{R}$ that have finite $L_{1}$ -norm. ", "page_idx": 7}, {"type": "text", "text": "We use linear programming over one-dimensional functions in $L^{1}(\\mathbb{R})$ space to establish the existence of such a function. Specifically, we show the following: ", "page_idx": 7}, {"type": "text", "text": "Lemma 3.3. For any sufficiently large $n\\in\\mathbb{N},$ , there exists a function $g:\\mathbb{R}\\mapsto[-1,+1]$ such that $g$ satisfies the following properties: ", "page_idx": 7}, {"type": "text", "text": "(ii) $\\mathbf{E}_{t\\sim\\mathcal{N}_{1}}[g(z)z^{k}]=0$ for all $k\\in[n]$ . ", "page_idx": 8}, {"type": "text", "text": "Proof Sketch of Lemma 3.3. We let $P_{n}$ denote the set of all polynomials $p:\\mathbb{R}\\rightarrow\\mathbb{R}$ of degree at most $n$ and let $L_{+}^{1}(\\mathbb{R})$ denote the set of all nonnegative functions in $L^{1}(\\mathbb{R})$ . Then, using linear programming, we will get the following primal: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l l}{\\mathrm{find~}}&{~~~~~~~~~~g\\in L^{1}(\\mathbb{R})}\\\\ {\\mathrm{such~that~}}&{~~~~~~~~~~~\\underset{z\\sim\\mathcal{N}_{1}}{\\mathbf{E}}[p(z)g(z)]=0\\ ,}&{\\forall p\\in P_{n}}\\\\ &{\\underset{z\\sim\\mathcal{N}_{1}}{\\mathbf{E}}[g(z)h(z)\\mathbb{1}\\{t\\geq c\\}]\\geq\\|h(z)\\mathbb{1}\\{z\\geq c\\}\\|_{1}\\ ,}&{\\forall h\\in L_{+}^{1}(\\mathbb{R})}\\\\ &{~~~~~~~~~~~~~~~~\\underset{z\\sim\\mathcal{N}_{1}}{\\mathbf{E}}[g(z)H(z)]\\leq\\|H\\|_{1}\\ ,}&{\\forall H\\in L^{1}(\\mathbb{R})}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Then, using (infinite-dimensional) LP duality, we get that the above primal is feasible if and only if there is no polynomial of degree $n$ such that $\\mathbf{E}_{t\\sim\\mathcal{N}_{1}}[|p(t)|\\mathbb{1}(t\\leq c)]<\\mathbf{E}_{t\\sim\\mathcal{N}_{1}}[p(t)\\mathbb{1}(t\\geq c)]$ . ", "page_idx": 8}, {"type": "text", "text": "Using Gaussian hypercontractivity (see [Bog98, Nel73]), one can show that for every polynomial $p$ and $c\\in\\mathbb{R}$ , it holds ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\underset{z\\sim\\mathcal{N}_{1}}{\\mathbf{E}}[|p(z)|]<2\\cdot3^{n}\\underset{z\\sim\\mathcal{N}_{1}}{\\mathbf{E}}[|p(z)|]\\left(\\underset{z\\sim\\mathcal{N}_{1}}{\\mathbf{Pr}}[z\\geq c]\\right)^{1/2}\\,.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "From our choice of the parameter $c$ , we have that ${\\bf P r}_{z\\sim\\mathcal{N}_{1}}[z\\geq c]\\leq3^{-2n}/4$ ; thus, $\\mathbf{E}_{z\\sim\\mathcal{N}_{1}}[|p(z)|]<$ $\\mathbf{E}_{z\\sim\\mathcal{N}_{1}}[|p(z)|]$ , which is a contradiction. Therefore, such a polynomial $p$ cannot exist. \u5382 ", "page_idx": 8}, {"type": "text", "text": "We have proven the existence of the function $g$ in Lemma 3.3. Now, we construct a joint distribution of $(z,y)$ on $\\mathbb{R}\\times\\{\\pm1\\}$ such that ${\\bf E}[y|z]$ is exactly $g$ , as we discussed in the proof outline. For a joint distribution $D$ of $(x,y)$ supported on $X\\times\\{\\pm1\\}$ , we will use $D_{+}$ to denote the conditional distribution of $x$ given $y=1$ ; and $D_{-}$ for the distribution of $x$ given $y=-1$ . ", "page_idx": 8}, {"type": "text", "text": "Lemma 3.4. For any sufficiently large $n\\in\\mathbb{N}$ , there exists a distribution $D$ on $\\mathbb{R}\\times\\{\\pm1\\}$ such that for $(z,y)\\sim D$ : ", "page_idx": 8}, {"type": "text", "text": "(i) the marginal distribution $D_{z}=\\mathcal{N}_{1}$ ; ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\chi^{2}(D_{+},\\mathcal{N}_{1}),\\chi^{2}(D_{-},\\mathcal{N}_{1})=O(1).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Proof Sketch for Lemma 3.4. The properties here directly follow from the properties of $g$ . ", "page_idx": 8}, {"type": "text", "text": "Using the framework introduced in [DKS17, DKPZ21], we can construct a set of alternative hypothesis distributions ${\\mathcal{D}}=\\{{\\boldsymbol{D}}_{\\mathbf{v}}:{\\mathbf{v}}\\in V\\}$ on $\\mathbb{R}^{d}\\times\\{\\pm1\\}$ , where $V$ is a set of exponentially many pairwise nearly-orthogonal vectors and the marginal distribution of each $D_{\\mathbf{v}}$ on direction $\\mathbf{v}$ is the distribution $D$ in Lemma 3.4. This effectively embeds the distribution $D$ in a hidden direction v. The size of the family $\\mathcal{D}$ is exponential in $d$ , and the distributions in it have small pairwise correlations. The details of $\\mathcal{D}$ are deferred to Appendix C. Now, we are ready to give a proof sketch for the SQ hardness part of our main theorem Theorem 1.3. ", "page_idx": 8}, {"type": "text", "text": "Proof Sketch of the SQ hardness part of Theorem 1.3. Let $\\mathcal{D}$ be the set of distributions discussed above. We also let $D_{\\mathrm{null}}$ be the joint distribution of $(\\mathbf{x},y)$ such that $\\mathbf{x}\\sim\\mathcal{N}_{d}$ and $y\\sim\\operatorname{Bern}(1/2)$ independent of $\\mathbf{x}$ . Then, using standard SQ dimension techniques, one can show that any SQ algorithm that solves $B(\\mathcal{D},D_{\\mathrm{null}})$ requires either queries of tolerance at most $d^{-\\Omega(\\log\\frac{1}{\\alpha})}$ or makes at least 2d\u2126(1) queries. By reducing the decision problem $B(\\mathcal{D},D_{\\mathrm{null}})$ to reliably learning $\\alpha$ -biased LTFs with $\\epsilon<\\alpha/3$ accuracy, we get the lower bound part of the statement in Theorem 1.3. \u53e3 ", "page_idx": 8}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[Bog98] V. Bogachev. Gaussian measures. Mathematical surveys and monographs, vol. 62, ", "page_idx": 9}, {"type": "text", "text": "1998.   \n$[\\mathrm{DGJ^{+}09}]$ I. Diakoniokolas, P. Gopalan, R. Jaiswal, R. Servedio, and E. Viola. Bounded independence fools halfspaces. In Proc. 50th IEEE Symposium on Foundations of Computer Science (FOCS), pages 171\u2013180, 2009. [DJ19] A. Durgin and B. Juba. Hardness of improper one-sided learning of conjunctions for all uniformly falsifiable csps. In Algorithmic Learning Theory, ALT, volume 98 of Proceedings of Machine Learning Research, pages 369\u2013382, 2019.   \n$[\\mathrm{DKK}^{+}22]$ I. Diakonikolas, D. M. Kane, V. Kontonis, C. Tzamos, and N. Zarifis. Learning general halfspaces with general massart noise under the gaussian distribution. In STOC \u201922: 54th Annual ACM SIGACT Symposium on Theory of Computing, Rome, Italy, June 20 - 24, 2022, pages 874\u2013885. ACM, 2022.   \n[DKN10] I. Diakonikolas, D. M. Kane, and J. Nelson. Bounded independence fools degree-2 threshold functions. In FOCS, pages 11\u201320, 2010.   \n[DKPZ21] I. Diakonikolas, D. M. Kane, T. Pittas, and N. Zarifis. The optimality of polynomial regression for agnostic learning under gaussian marginals. In Proceedings of The 34th Conference on Learning Theory, COLT, 2021.   \n[DKR23] I. Diakonikolas, D. M. Kane, and L. Ren. Near-optimal cryptographic hardness of agnostically learning halfspaces and relu regression under gaussian marginals. In International Conference on Machine Learning, ICML, volume 202 of Proceedings of Machine Learning Research, pages 7922\u20137938, 2023. [DKS17] I. Diakonikolas, D. M. Kane, and A. Stewart. Statistical query lower bounds for robust estimation of high-dimensional gaussians and gaussian mixtures. In 2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS), pages 73\u201384, 2017. [DKZ20] I. Diakonikolas, D. M. Kane, and N. Zarifis. Near-optimal SQ lower bounds for agnostically learning halfspaces and ReLUs under Gaussian marginals. In Advances in Neural Information Processing Systems, NeurIPS, 2020. [Dom99] P. M. Domingos. Metacost: A general method for making classifiers cost-sensitive. In Proceedings of the Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 155\u2013164, 1999. [Elk01] C. Elkan. The foundations of cost-sensitive learning. In Proceedings of the Seventeenth International Joint Conference on Artificial Intelligence, IJCAI, pages 973\u2013978, 2001. [Fan68] K. Fan. On infinite systems of linear inequalities. Journal of Mathematical Analysis and Applications, 21(3):475 \u2013 478, 1968. [FS97] Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of computer and system sciences, 55(1):119\u2013139, 1997.   \n[GGK20] S. Goel, A. Gollakota, and A. R. Klivans. Statistical-query lower bounds via functional gradients. In Advances in Neural Information Processing Systems, NeurIPS, 2020.   \n[GKKM20] S. Goldwasser, A. T. Kalai, Y. Kalai, and O. Montasser. Beyond perturbations: Learning guarantees with arbitrary adversarial test examples. In Advances in Neural Information Processing Systems, NeurIPS, 2020.   \n[GKKT17] S. Goel, V. Kanade, A. R. Klivans, and J. Thaler. Reliably learning the relu in polynomial time. In Proceedings of the 30th Conference on Learning Theory, COLT 2017, volume 65 of Proceedings of Machine Learning Research, pages 1004\u20131042, 2017. [Hau92] D. Haussler. Decision theoretic generalizations of the PAC model for neural net and other learning applications. Information and Computation, 100:78\u2013150, 1992. [KK21] A. T. Kalai and V. Kanade. Efficient learning with arbitrary covariate shift. In Algorithmic Learning Theory, 2021, volume 132 of Proceedings of Machine Learning Research, pages 850\u2013864, 2021.   \n[KKM12] A. T. Kalai, V. Kanade, and Y. Mansour. Reliable agnostic learning. J. Comput. Syst. Sci., 78(5):1481\u20131495, 2012.   \nKKMS08] A. Kalai, A. Klivans, Y. Mansour, and R. Servedio. Agnostically learning halfspaces. SIAM Journal on Computing, 37(6):1777\u20131805, 2008. Special issue for FOCS 2005.   \n[KSS94] M. Kearns, R. Schapire, and L. Sellie. Toward Efficient Agnostic Learning. Machine Learning, 17(2/3):115\u2013141, 1994. [KT14] V. Kanade and J. Thaler. Distribution-independent reliable learning. In Proceedings of The 27th Conference on Learning Theory, COLT, volume 35 of JMLR Workshop and Conference Proceedings, pages 3\u201324, 2014. [MT94] W. Maass and G. Turan. How fast can a threshold gate learn? In S. Hanson, G. Drastal, and R. Rivest, editors, Computational Learning Theory and Natural Learning Systems, pages 381\u2013414. MIT Press, 1994. [Nel73] E. Nelson. The free markoff field. Journal of Functional Analysis, 12(2):211\u2013227, 1973. [NP33] J. Neyman and E. S. Pearson. On the problem of the most efficient tests for statistical hypotheses. Trans. R. So. Lond. Ser. A Contain. Pap. Math. Phys. Character, 231:281\u2013 337, 1933. [Ros58] F. Rosenblatt. The Perceptron: a probabilistic model for information storage and organization in the brain. Psychological Review, 65:386\u2013407, 1958. [Tie23] S. Tiegel. Hardness of agnostically learning halfspaces from worst-case lattice problems. In The Thirty Sixth Annual Conference on Learning Theory, COLT, volume 195 of Proceedings of Machine Learning Research, pages 3029\u20133064, 2023. [Vap98] V. Vapnik. Statistical learning theory. Wiley, 1998. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "Supplementary Material ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Organization The supplementary material is structured as follows: Appendix A includes additional preliminaries. Appendix B includes the omitted proofs from Section 2. Finally, Appendix C includes the omitted proofs from Section 3. ", "page_idx": 11}, {"type": "text", "text": "A Additional Preliminaries ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "We use $\\mathbb{S}^{d-1}$ to denote the $d$ -dimensional unit sphere, i.e., $\\mathbb{S}^{d-1}\\ {\\stackrel{\\mathrm{def}}{=}}\\ \\{\\mathbf{x}\\in\\mathbb{R}^{d}:\\|\\mathbf{x}\\|_{2}=1\\}$ . ", "page_idx": 11}, {"type": "text", "text": "Basics of the SQ Model In order to give an SQ lower bound for reliable learning, we first present the basics of the SQ model. We define the pairwise correlation, which is used for the SQ lower bound. ", "page_idx": 11}, {"type": "text", "text": "Definition A.1 (Pairwise Correlation). The pairwise correlation of two distributions with probability density function $D_{1},D_{2}:\\mathbb{R}^{d}\\mapsto\\vec{\\mathbb{R}}_{+}$ with respect to a distribution with density $D:\\mathbf{\\dot{R}}^{d}\\mapsto\\mathbf{\\mathbb{R}_{+}}$ , where the support of $D$ contains the support of $D_{1}$ and $D_{2}$ , is defined as $\\chi_{D}(D_{1},D_{2})$ d=ef $\\begin{array}{r}{\\int_{\\mathbb{R}^{d}}D_{1}(\\mathbf{x})D_{2}(\\mathbf{\\bar{x}})/D(\\mathbf{x})d\\mathbf{x}\\mathrm{~-~}1}\\end{array}$ . Furthermore, the $\\chi$ -squared divergence of $D_{1}$ to $D$ is defined as $\\chi^{2}(D_{1},D)\\stackrel{\\mathrm{def}}{=}\\chi_{D}(D_{1},D_{1}).$ . ", "page_idx": 11}, {"type": "text", "text": "Then we introduce the following definitions and lemmata from [DKS17] showing pairwise correlation implies an SQ lower bound. ", "page_idx": 11}, {"type": "text", "text": "Lemma A.2. Let $B(\\mathcal{D},D)$ be a decision problem, where $D$ is the reference distribution and $\\mathcal{D}$ is $a$ class of distribution. For $\\gamma,\\beta>0,$ , le\u221at $s=\\mathrm{SD}(B,\\gamma,\\beta)$ . For any $\\gamma^{\\prime}>0,$ , any $S Q$ algorithm for $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ requires queries of tolerance at most $\\sqrt{\\gamma+\\gamma^{\\prime}}$ or makes at least $s\\gamma^{\\prime}/(\\beta-\\gamma)$ queries. ", "page_idx": 11}, {"type": "text", "text": "Definition A.3. We say that a set of s distribution $\\mathcal{D}=\\{D_{1},\\cdots,D_{s}\\}$ over $\\mathbb{R}^{d}$ is $(\\gamma,\\beta)$ -correlated relative to a distribution $D\\;i f\\chi_{D}(D_{i},D_{j})\\leq\\gamma.$ for all $i\\neq j$ , and $\\chi_{D}\\big(D_{i},D_{j}\\big)\\leq\\beta$ for $i=j$ . ", "page_idx": 11}, {"type": "text", "text": "Definition A.4 (Statistical Query Dimension). For $\\beta,\\gamma>0$ , a decision problem $B(\\mathcal{D},D)$ , where $D$ is a fixed distribution and $\\mathcal{D}$ is a family of distribution, let s be the maximum integer such that there exists a finite set of distributions $\\mathcal{D}_{D}\\subseteq\\mathcal{D}$ such that $\\mathcal{D}_{D}$ is $(\\gamma,\\beta)$ -correlated relative to $D$ and $|\\mathcal{D}_{D}|\\ge s$ . The Statistical Query dimension with pairwise correlations $(\\gamma,\\beta)$ of $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ is defined to be $s$ , and denoted by $s=\\mathrm{SD}(B,\\gamma,\\beta)$ . ", "page_idx": 11}, {"type": "text", "text": "Basics of Hermite Polynomials We require the following definitions used in our work. ", "page_idx": 11}, {"type": "text", "text": "Definition A.5 (Normalized Hermite Polynomial). For $k\\,\\in\\,\\mathbb{N},$ , we define the $k{-}t h$ probabilist\u2019s Hermite polynomials $H e_{k}\\;:\\;\\mathbb{R}\\;\\rightarrow\\;\\mathbb{R}$ as $H e_{k}(t)\\;=\\;(-1)^{k}e^{t^{2}/2}\\cdot\\;\\textstyle{\\frac{\\mathrm{d}^{k}}{\\mathrm{d}t^{k}}}e^{-t^{2}/2}$ . We define the $k$ -th normalized Hermite polynomial $h_{k}:\\mathbb{R}\\rightarrow\\mathbb{R}$ as $h_{k}(t)=H e_{k}(t)/\\sqrt{k!}$ . ", "page_idx": 11}, {"type": "text", "text": "Furthermore, we use multivariate Hermite polynomials in the form of Hermite tensors (as the entries in the Hermite tensors are re-scaled multivariate Hermite polynomials). We define the Hermite tensor as follows. ", "page_idx": 11}, {"type": "text", "text": "Definition A.6 (Hermite Tensor). For $k\\in\\mathbb{N}$ and $\\mathbf{x}\\in\\mathbb{R}^{d}$ , we define the $k$ -th Hermite tensor as ", "page_idx": 11}, {"type": "equation", "text": "$$\n(\\mathbf{H}_{k}(\\mathbf{x}))_{i_{1},i_{2},...,i_{k}}=\\frac{1}{\\sqrt{k!}}\\sum_{\\substack{P a r t i t i o n s\\;P\\;o f\\,[k]}}\\bigotimes_{\\{a,b\\}\\in P}(-\\mathbf{I}_{i_{a},i_{b}})\\bigotimes_{\\{c\\}\\in P}\\mathbf{x}_{i_{c}}\\,.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "We also give the following remark that fully reliable learning (see [KKM12] for the definition) reduces to one-sided reliable learning defined in Definition 1.1. The proof is already implicitly stated in the proof of Theorem 3 in [KKM12]. ", "page_idx": 11}, {"type": "text", "text": "Fact A.7. For a joint distribution $D$ of $(x,y)$ supported on $X\\,\\times\\,\\{\\pm1\\}$ , let $h_{+}:X\\,\\rightarrow\\,\\{\\pm1\\}$ (resp. $h_{-}:X\\to\\{\\pm1\\}$ ) be a hypothesis that is $\\epsilon/4$ -reliable with respect to the concept class $C$ on distribution $D$ for positive labels (resp. for negative labels). Let hypothesis $h$ be defined as $h(x)=1$ $i f\\,h_{+}(x)=h_{-}(x)=1,$ , $h(x)=-1$ if $h_{+}(x)=h_{-}(x)=-1$ and $h(x)=?$ otherwise. Then $h$ is $\\epsilon$ close to the best fully reliable sandwich hypothesis for $C$ on distribution $D$ . ", "page_idx": 11}, {"type": "text", "text": "Proof Sketch. We first show that $R_{+}(h;D),R_{-}(h;D)~~\\leq~~\\epsilon.$ Notice that $\\begin{array}{r l}{R_{+}(h;D)}&{{}=}\\end{array}$ $\\mathbf{Pr}_{(x,y)\\sim D)}[h(x)\\,=\\,1\\land y\\,=\\,-1]\\,\\le\\,\\mathbf{Pr}_{(x,y)\\sim D)}[h_{+}(x)\\,=\\,1\\land y\\,=\\,-1]\\,\\le\\,\\epsilon.$ The same holds for negative labels. ", "page_idx": 12}, {"type": "text", "text": "To show ${\\mathbf{Pr}}_{(x,y)\\sim D}[h(x)=?]$ is $\\epsilon_{}$ -suboptimal. Assume, for the purpose of contradiction, there is a $h^{\\prime}$ such that $R_{+}(h^{\\prime};D),R_{-}(h^{\\prime};D)\\leq\\epsilon$ and $\\mathbf{Pr}_{(x,y)\\sim D}[h^{\\prime}(x)=?]<\\mathbf{Pr}_{(x,y)\\sim D}[h(x)=?]-\\epsilon.$ Notice that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad_{\\mathbf{(}x,y)\\sim D}[h(x)=?]}\\\\ &{=\\underset{(x,y)\\sim D}{\\mathbf{Pr}}[h_{+}(x)=1\\wedge h_{-}(x)=-1]+\\underset{(x,y)\\sim D}{\\mathbf{Pr}}[h_{+}(x)=-1\\wedge h_{-}(x)=1]}\\\\ &{\\leq1-\\underset{(x,y)\\sim D}{\\mathbf{Pr}}[h_{+}(x)=1]-\\underset{(x,y)\\sim D}{\\mathbf{Pr}}[h_{-}(x)=-1]+\\underset{(x,y)\\sim D}{\\mathbf{Pr}}[h_{+}(x)=1\\wedge h_{-}(x)=-1]}\\\\ &{\\leq1-\\underset{(x,y)\\sim D}{\\mathbf{Pr}}[h_{+}(x)=1]-\\underset{(x,y)\\sim D}{\\mathbf{Pr}}[h_{-}(x)=-1]+\\epsilon/2\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Thus, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\operatorname*{\\bfPr}_{(x,y)\\sim D}[h_{+}(x)=1]+\\operatorname*{\\bfPr}_{(x,y)\\sim D}[h_{-}(x)=-1]\\le1-\\operatorname*{\\bfPr}_{(x,y)\\sim D}[h(x)=?]+\\epsilon/2\\;.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Then, define $h_{+}^{\\prime}:X\\to\\{\\pm1\\}$ as $h_{+}^{\\prime}(x)=1$ if $h^{\\prime}(x)=1$ and $h_{+}^{\\prime}(x)=-1$ otherwise $\\left(h_{-}^{\\prime}$ is defined similarly). We get $\\mathbf{Pr}_{(x,y)\\sim D}[h_{+}^{\\prime}(x)=1]+\\mathbf{Pr}_{(x,y)\\sim D}[h_{-}^{\\prime}(x)=-1]=1-\\mathbf{Pr}_{(x,y)\\sim D}[h^{\\prime}(x)=7]$ . Using Equation (1) and $\\mathbf{Pr}_{(x,y)\\sim D}[h^{\\prime}(x)=?]<\\mathbf{Pr}_{(x,y)\\sim D}[h(x)=?]-\\epsilon$ , we get ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbf{p}_{\\mathbf{r},y)\\sim D}^{\\mathbf{p}_{\\mathbf{r}}}[h_{+}(x)=1]-\\underset{(x,y)\\sim D}{\\mathbf{Pr}}[h_{-}(x)=-1]<\\underset{(x,y)\\sim D}{\\mathbf{Pr}}[h_{+}^{\\prime}(x)=1]-\\underset{(x,y)\\sim D}{\\mathbf{Pr}}[h_{-}^{\\prime}(x)=-1]-\\epsilon/2\\;.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Therefore, either $\\mathbf{Pr}_{(x,y)\\sim D}[h_{+}(x)=1]<\\mathbf{Pr}_{(x,y)\\sim D}[h_{+}^{\\prime}(x)=1]-\\epsilon/4$ or ${\\bf P r}_{(x,y)\\sim D}[h_{-}(x)=$ $-1]<\\mathbf{Pr}_{(x,y)\\sim D}[h_{-}^{\\prime}(x)=-1]-\\epsilon/4$ , Thus, either $h_{+}$ or $h_{-}$ is not $\\epsilon/4$ -suboptimal. This gives a contradiction. \u53e3 ", "page_idx": 12}, {"type": "text", "text": "B Omitted Proofs from Section 2 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We define the bias of a halfspace $h\\quad:\\quad\\mathbb{R}^{d}\\quad\\rightarrow\\quad\\{\\pm1\\}$ as $\\begin{array}{r l}{\\operatorname*{min}(\\mathbf{Pr}_{\\mathbf{x}\\sim\\mathcal{N}_{d}}[h(\\mathbf{x})}&{{}=}\\end{array}$ 1 $],\\mathbf{Pr}_{\\mathbf{x}\\sim\\mathcal{N}_{d}}[h(\\mathbf{x})~=~-1])$ , and define $\\mathcal{H}_{d}^{\\alpha}$ to be the set of all LTFs whose bias is $\\alpha$ . Let $\\begin{array}{r}{f\\;=\\;\\mathrm{argmin}_{c\\in\\mathcal{H}_{d}^{\\alpha}\\,\\wedge\\,R_{+}(f;D)=0}\\,R_{-}(f;D)}\\end{array}$ be the optimal reliable hypothesis and $\\alpha$ be the bias of $f$ . ", "page_idx": 12}, {"type": "text", "text": "B.1 Reliably Learning Halfspaces with Gaussian Marginals for the Case $\\epsilon\\geq2/\\alpha$ ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We show that for the problem in Theorem 1.3, in the case of $\\epsilon\\geq2\\alpha$ , the algorithm can just return one of the constant hypotheses. Let $h_{+}$ (resp. $h_{-}$ ) be the $+1$ (resp. -1) constant hypothesis. ", "page_idx": 12}, {"type": "text", "text": "Fact B.1. For the problem defined in Theorem 1.3, the case where $\\epsilon\\geq2\\alpha$ can be efficiently solved by returning $h_{+}$ if $R_{+}(h_{+};D)\\leq3\\epsilon/4$ and returning $h_{-}$ otherwise. ", "page_idx": 12}, {"type": "text", "text": "Proof. When the algorithm returns $h_{+}$ , it is easy to see that $h_{+}$ satisfies the reliable learning requirement in Definition 1.1. So we only consider the case that the algorithm returns $h_{-}$ . ", "page_idx": 12}, {"type": "text", "text": "Given the algorithm returns $h_{-}$ , either there is an $\\alpha$ -biased LTF $f$ such that $R_{+}(f;D)=0$ or none of the $\\alpha$ -biased LTFs $f$ satisfies $R_{+}(f;D)\\,=\\,0$ . For the case that none of the $\\alpha$ -biased LTFs $f$ satisfies $R_{+}(f;D)=0$ , it is easy to see $h_{-}$ is a valid answer from the definition of Definition 1.1. For the case that there is an $\\alpha$ -biased LTF $f$ such that $R_{+}(f,D)\\,=\\,0$ , since the algorithm did not return $h_{+}$ , it must be the case that $\\mathbf{Pr}_{(\\mathbf{x},y)\\sim D}[y\\,=\\,-1]\\,>\\,3\\epsilon/4\\,>\\,\\alpha$ . Therefore, we have $\\mathbf{Pr}_{\\mathbf{x}\\sim\\mathcal{N}_{d}}[f(\\mathbf{x})=-1]=1-\\alpha$ . Thus, we get $R_{+}(h_{-},D)=0$ and $R_{-}(h_{-},D)\\leq R_{-}(f,D)+\\alpha$ . This shows $h_{-}$ is $\\epsilon$ reliable with respect to all $\\alpha$ biased LTFs. This completes the proof. \u53e3 ", "page_idx": 12}, {"type": "text", "text": "B.2 Reliably Learning Halfspaces with Gaussian Marginals for the Unknown $\\alpha$ Case ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Theorem B.2 (Main Algorithmic Result). L $e t\\,\\epsilon\\in(0,1/2)$ and let $D$ be a joint distribution of $(\\mathbf{x},y)$ supported on $\\mathbb{R}^{d}\\times\\{\\pm1\\}$ with marginal $D_{\\mathbf{x}}=\\mathcal{N}_{d}$ . Let $\\begin{array}{r}{f=\\operatorname*{argmin}_{f\\in\\mathcal{H}_{d}\\wedge\\mathit{R}_{+}(f;D)=0}R_{-}(f;D)}\\end{array}$ be the optimal halfspace and $\\alpha$ be its bias which is unknown. Then, there is an algorithm that uses $N=d^{O(\\log(\\operatorname*{min}\\{1/\\alpha,1/\\epsilon\\}))}\\operatorname*{min}\\left(2^{\\log(1/\\epsilon)^{O(\\log(1/\\alpha))}},2^{\\mathrm{poly}(1/\\epsilon)}\\right)$ many samples from $D$ , runs in p $\\mathrm{oly}(N,d,1/\\epsilon)$ time and with high probability returns a hypothesis $h(\\mathbf{x})=\\mathrm{sign}(\\langle\\mathbf{w},\\mathbf{x}\\rangle-t)$ that is $\\epsilon$ -reliable with respect to the class of $L T F s$ . ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "To prove Theorem B.2, we need to use the algorithm in the following lemma as a black box. The lemma shows that if the optimal halfspace $f$ satisfies ${\\bf P r}_{{\\bf x}\\sim{\\cal N}_{d}}[f({\\bf x})=-1]\\le1/2$ , then the problem can be solved efficiently. ", "page_idx": 13}, {"type": "text", "text": "Lemma B.3. Let $D$ be the joint distribution of $(\\mathbf{x},y)$ supported on $\\mathbb{R}^{d}\\times\\{\\pm1\\}$ with the marginal $D_{\\mathbf{x}}=\\mathcal{N}_{d}$ and $\\epsilon\\in(0,1)$ . Suppose the optimal halfspace $f=\\mathrm{argmin}_{f\\in{\\mathcal{H}}_{d}}\\wedge\\dot{R_{+}}(f;D){=}0\\,R_{-}(f;D)$ satisfies $\\mathbf{Pr}_{\\mathbf{x}\\sim\\mathcal{N}_{d}}[f(\\mathbf{x})=-1]\\le1/2$ . Then, there is an algorithm that reliably learns LTFs on $D$ using $N=O(d/\\epsilon^{2})$ many samples and poly $(N,d)$ running time. ", "page_idx": 13}, {"type": "text", "text": "Proof. The algorithm is the following. ", "page_idx": 13}, {"type": "text", "text": "1. First check if $\\mathbf{Pr}_{(\\mathbf{x},y)\\sim D)}[y=-1]\\leq\\epsilon$ with sufficiently small constant failure probability. If so, return the $+1$ constant hypothesis. 2. Otherwise, draw $m\\:=\\:(d/\\epsilon)^{c}$ many samples $S\\,=\\,\\{(\\mathbf{x}_{1},y_{1}),\\cdots,(\\mathbf{x}_{m},y_{m})\\}$ conditioned on $y=-1$ . By Step 1, the sampling efficiency here is $\\Omega(\\epsilon)$ with high probability. ", "page_idx": 13}, {"type": "text", "text": "3. solve the following semidefinite program for $\\mathbf{w}^{\\prime}$ and $t^{\\prime}$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\mathrm{minimize}}&{t^{\\prime}}\\\\ &{\\mathrm{such~that}}&{\\langle\\mathbf{w}^{\\prime},\\mathbf{x}\\rangle-t^{\\prime}\\leq0\\;,}&{\\forall(\\mathbf{x},y)\\in S}\\\\ &{}&{\\|\\mathbf{w}^{\\prime}\\|_{2}\\leq1}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Return the hypothesis $h(\\mathbf{x})=\\mathrm{sign}(\\langle\\mathbf{w},\\mathbf{x}\\rangle-t)$ where $\\mathbf{w}=\\mathbf{w}^{\\prime}/\\lVert\\mathbf{w}^{\\prime}\\rVert_{2}$ and $t=t^{\\prime}/\\lVert\\mathbf{w}\\rVert^{\\prime}$ . ", "page_idx": 13}, {"type": "text", "text": "Let $f(\\mathbf{x})=\\mathrm{sign}(\\langle\\mathbf{w}^{*},\\mathbf{x}\\rangle-t^{*})$ be the optimal hypothesis. We prove that $h(\\mathbf{x})=\\mathrm{sign}(\\langle\\mathbf{w},\\mathbf{x}\\rangle-t)$ such that $R_{+}(h;D)\\leq\\epsilon$ and $R_{-}(h;D)\\leq R_{-}(f;D)+\\epsilon$ . Since $h$ is consistent with all the negative samples, we have $R_{+}(h;D)\\leq\\epsilon$ . From $\\mathbf{Pr}_{\\mathbf{x}\\sim\\mathcal{N}_{d}}[f(\\mathbf{x})=-1]\\le1/2$ , we have $t^{*}\\leq0$ . Notice that $\\mathbf{w}^{*}$ and $t^{*}$ is a feasible solution, therefore, $t^{\\prime}\\leq t^{*}\\leq0$ and we must have $t=t^{\\prime}/\\|\\mathbf{w}^{\\prime}\\|_{2}\\leq t^{\\prime}\\leq t^{*}$ , which implies that $\\mathbf{Pr}_{(\\mathbf{x},y)\\sim D}[h(\\mathbf{x})=1]\\geq\\mathbf{Pr}_{(\\mathbf{x},y)\\sim D}[f(\\mathbf{x})=1]$ . Therefore, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R_{-}(h;D)=\\underset{(\\mathbf{x},y)\\sim D}{\\mathbf{Pr}}[y=1]-\\underset{(\\mathbf{x},y)\\sim D}{\\mathbf{Pr}}[y=1\\wedge h(\\mathbf{x})=1]}\\\\ &{\\qquad\\qquad=\\underset{(\\mathbf{x},y)\\sim D}{\\mathbf{Pr}}[y=1]-\\underset{(\\mathbf{x},y)\\sim D}{\\mathbf{Pr}}[h(\\mathbf{x})=1]+R_{+}(h;D)}\\\\ &{\\qquad\\quad\\leq\\underset{(\\mathbf{x},y)\\sim D}{\\mathbf{Pr}}[y=1]-\\underset{(\\mathbf{x},y)\\sim D}{\\mathbf{Pr}}[f(\\mathbf{x})=1]+\\epsilon}\\\\ &{\\qquad\\qquad=R_{-}(f;D)+\\epsilon\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "This completes the proof. ", "page_idx": 13}, {"type": "text", "text": "We now prove Theorem B.2. ", "page_idx": 13}, {"type": "text", "text": "Proof of Theorem B.2. The algorithm is the following: ", "page_idx": 13}, {"type": "text", "text": "1. Run the algorithm in Lemma B.3 with $\\epsilon^{\\prime}=\\epsilon/2$ . If the output hypothesis $h$ satisfies $R_{+}(h;D)\\leq\\epsilon$ with a sufficiently small constant failure probability. Then, output $h$ and terminate. 2. Set $\\alpha=1/2-\\epsilon/100$ and run the algorithm in Theorem B.4. If the output hypothesis $h$ satisfies $R_{+}(h;D)\\leq\\epsilon$ with a sufficiently small constant failure probability. Then, output $h$ and terminate. Otherwise, update $\\alpha$ as $\\alpha-\\epsilon/100$ . Repeat Step 2 until the algorithm terminates. ", "page_idx": 13}, {"type": "text", "text": "Let $\\begin{array}{r}{f\\,=\\,\\mathrm{argmin}_{f\\in\\mathcal{H}_{d}\\,\\land\\,R+(f;D)=0}\\,R_{-}(f;D)}\\end{array}$ be the optimal halfspace and $\\alpha_{f}$ be its bias which is unknown. Suppose the algorithm terminates. Let $\\alpha$ be the bias of the output hypothesis $h$ . It is easy to see that $R_{+}(h;D)\\;\\leq\\;\\epsilon$ with high probability; therefore, it only remains to show ", "page_idx": 13}, {"type": "text", "text": "$R_{-}(h;D)\\leq R_{-}(f;D)+\\epsilon$ . Notice that if ${\\bf P r}_{{\\bf x}\\sim{\\cal N}_{d}}[f({\\bf x})=-1]\\le1/2$ , the algorithm will terminate in Step 1. Therefore, without loss of generality, we assume $\\mathbf{P}\\dot{\\mathbf{r}}_{\\mathbf{x}\\sim\\mathcal{N}_{d}}[f(\\mathbf{x})=\\mathbf{\\dot{-}}1]\\ge1/2$ . Then, by Theorem B.4, it is easy to see that $\\alpha\\geq\\alpha_{f}-\\epsilon/100$ since given any $\\alpha\\leq\\alpha_{f}$ , Step 2 is guaranteed to terminate. Therefore, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{\\mathbf{x}\\sim\\mathcal{N}_{d}}[h(\\mathbf{x})=-1]\\le1-\\alpha\\le1-\\alpha_{f}+\\epsilon/100=\\operatorname*{Pr}_{\\mathbf{x}\\sim\\mathcal{N}_{d}}[f(\\mathbf{x})=-1]+\\epsilon/100\\;.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R_{-}(h;D)=\\underset{(\\mathbf{x},y)\\sim D}{\\mathbf{Pr}}[y=1]-\\underset{(\\mathbf{x},y)\\sim D}{\\mathbf{Pr}}[y=1\\wedge h(\\mathbf{x})=1]}\\\\ &{\\qquad\\qquad=\\underset{(\\mathbf{x},y)\\sim D}{\\mathbf{Pr}}[y=1]-\\underset{(\\mathbf{x},y)\\sim D}{\\mathbf{Pr}}[h(\\mathbf{x})=1]+R_{+}(h;D)}\\\\ &{\\qquad\\quad\\leq\\underset{(\\mathbf{x},y)\\sim D}{\\mathbf{Pr}}[y=1]-\\underset{(\\mathbf{x},y)\\sim D}{\\mathbf{Pr}}[f(\\mathbf{x})=1]+\\epsilon}\\\\ &{\\qquad\\qquad=R_{-}(f;D)+\\epsilon\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "This completes the proof. ", "page_idx": 14}, {"type": "text", "text": "B.3 Reliably Learning Halfspaces with Gaussian Marginals for the Case $\\epsilon\\leq\\alpha/2$ ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "For convenience, we assume that $\\alpha$ is known. This can be assumed without loss of generality as we discussed in Theorem B.2. ", "page_idx": 14}, {"type": "text", "text": "Theorem B.4. Let $\\epsilon\\mathrm{~\\boldmath~\\mathsf~{~\\}~}\\in\\mathrm{~\\boldmath~\\Gamma~}(0,1/2),\\alpha\\mathrm{~\\boldmath~\\mathsf~{~\\}~}\\in\\mathrm{~\\boldmath~\\Gamma~}(0,1/2]$ and let $D$ be a joint distribution of $(\\mathbf{x},y)$ supported on $\\mathbb{R}^{d}\\,\\times\\,\\{\\pm1\\}$ with marginal $\\mathit{D}_{\\mathbf{x}}\\mathit{\\Pi}=\\mathit{\\Pi}{\\mathcal{N}}_{d}$ . Assume that there is a halfspace in $\\mathcal{H}_{d}^{\\alpha}$ that is reliable with respect to $D$ . Then, there is an algorithm that uses $N\\ =\\ d^{O(\\log(\\operatorname*{min}\\{1/\\alpha,1/\\epsilon\\}))}\\operatorname*{min}\\left(2^{\\log(1/\\epsilon)^{O(\\log(1/\\alpha))}},2^{\\mathrm{poly}(1/\\epsilon)}\\right)$ many samples from $D$ , runs in $\\mathrm{poly}(N,d,1/\\epsilon)$ time and with high probability returns a hypothesis $h(\\mathbf{x})=\\mathrm{sign}(\\langle\\mathbf{w},\\mathbf{x}\\rangle-t)$ that is $\\epsilon$ -reliable with respect to the class of $\\mathcal{H}_{d}^{\\alpha}$ . ", "page_idx": 14}, {"type": "text", "text": "We provide the omitted proofs from Section 2. ", "page_idx": 14}, {"type": "text", "text": "B.4 Proof of Lemma 2.2 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Notice that for any polynomial $p$ such that $\\mathbf{E}_{z\\sim\\mathcal{N}_{1}}[p(z)]=0$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{(\\mathbf{x},y)\\sim D}{\\mathbf{E}}[y\\,p(\\langle\\mathbf{w}^{*},\\mathbf{x}\\rangle)]=-2\\underset{(\\mathbf{x},y)\\sim D}{\\mathbf{E}}[\\mathbf{1}(y=-1)\\,p(\\langle\\mathbf{w}^{*},\\mathbf{x}\\rangle)]\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore, it suffices for us to show that there is a zero mean and unit variance polynomial $p$ such that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\underset{(\\mathbf{x},y)\\sim D}{\\mathbf{E}}[\\mathbf{1}(y=-1)\\,p(\\langle\\mathbf{w}^{*},\\mathbf{x}\\rangle)]=2^{-O(t^{*})}\\,\\underset{(\\mathbf{x},y)\\sim D}{\\mathbf{Pr}}[y=-1]\\;.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Here, we will consider the sign-matching polynomial from $[\\mathrm{DKK}^{+}22]$ , which satisfies the following properties: ", "page_idx": 14}, {"type": "text", "text": "Claim B.5. Let $b\\in\\mathbb{R}$ and $b\\geq4$ . There exists a zero mean and unit variance polynomial $p:\\mathbb{R}\\to\\mathbb{R}$ of degree $k=\\Theta(b^{2})$ such that ", "page_idx": 14}, {"type": "text", "text": "1. The sign of $p(z)$ matches $\\mathrm{sign}(z-b).$ , i.e. $\\mathrm{sign}(p(z))=\\mathrm{sign}(z-b).$ . ", "page_idx": 14}, {"type": "text", "text": "2. For any $z\\le b/2$ , we have $|p(z)|=2^{-O(k)}$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. We consider the polynomial $p$ defined as: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\tilde{p}(z)=q(z)-\\frac{q(b)}{r(b)}r(z)\\;,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $q(z)\\;=\\;z^{3k}$ , $r(z)\\,=\\,z^{2k}\\,-\\,(2k\\,-\\,1)!!$ and $k$ is a sufficiently large odd integer such that $2|b|\\leq\\sqrt{k}\\leq4\\operatorname*{max}(|b|,1)$ . We then take $p(z)=\\tilde{p}(z)/\\sqrt{\\mathbf{E}_{u\\sim\\mathcal{N}_{1}}[\\tilde{p}^{2}(u)]}$ . ", "page_idx": 14}, {"type": "text", "text": "For convenience, we first note that from Stirling\u2019s approximation for $m\\in\\mathbb{Z}^{+}$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n(m/2)^{m}\\leq(2m-1)!!\\leq(2m)^{m}\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "To prove that $\\mathrm{sign}(p(z))=\\mathrm{sign}(z-b)$ , we first show that $\\begin{array}{r}{\\frac{q(b)}{r(b)}\\leq0}\\end{array}$ . Since $q(b)\\geq0$ , we just need to show $r(b)\\leq0$ . Notice $r(b)=b^{2k}-(2k-1)!!$ , since $2|b|\\leq{\\sqrt{k}}$ and $(2k-1)!!\\geq(k/2)^{k}$ , thus $r(b)\\leq({\\sqrt{k}}/2)^{2k}-(k/2)^{k}\\leq0$ . Therefore, considering $q(z)=z^{3k}$ and $r(z)=z^{2k}-(2k-1)!!$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\tilde{p}(z)=q(z)-\\frac{q(b)}{r(b)}r(z)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "must be monotone increasing for $z\\geq b$ and $p(z)$ must also be monotone increasing for $z\\geq b$ . Notice that $p(b)=0$ ; therefore, $p(\\bar{z)}>0$ for $z>b$ . To show $p(z)<0$ for $z<-b$ , we prove it for the cases $z\\in[-b,+b)$ and $z<-b$ . For $z\\in[-b,+b)$ , notice that due to $\\begin{array}{r}{\\frac{q(b)}{r(b)}\\leq0}\\end{array}$ and definition of $q(z)$ and $r(z)$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\tilde{p}(z)=q(z)-\\frac{q(b)}{r(b)}r(z)\\leq q(b)-\\frac{q(b)}{r(b)}r(b)<p(b)=0\\;.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "$z\\,\\leq\\,-b$ ,e  fsohro $p(z)$ ,s  thmios niso tpoonsei tiinvcer iefa sainndg .o nNlyo tiifc .v eI $\\tilde{p}^{\\prime}(z)=$ $\\begin{array}{r}{k z^{2k-1}(3z^{k}-2\\frac{q(b)}{r(b)})}\\end{array}$ $z\\leq-b<0$ $\\begin{array}{r}{3z^{k}-2\\frac{q(b)}{r(b)}<0}\\end{array}$ show $\\begin{array}{r}{b^{k}\\geq-\\frac{2}{3}q(b)/r(b)}\\end{array}$ , then it is immediate that for any $\\begin{array}{r}{z\\leq-b,3z^{k}-2\\frac{q(b)}{r(b)}<0}\\end{array}$ . The condition $b^{k}\\geq-{\\frac{2}{3}}q(b)/r(b)$ can be further simplified to $(2k-1)!!/b^{2k}\\geq5/3$ . Then considering $2b<{\\sqrt{k}}$ and $(2k-1)!!\\geq(k/2)^{k}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n(2k-1)!!/b^{2k}\\geq(k/2)^{k}/(\\sqrt{k}/2)^{2k}\\geq2^{k}\\;,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "thus, the condition holds. Therefore $\\tilde{p}^{\\prime}(z)>0$ for $z<-b$ and $p(z)$ must be monotone increasing for $z<-b$ . Then combined with $p(-b)<0$ which is implied from the previous case $(z\\in[-b,b])$ , we have $p(z)<0$ for $z<-b$ . ", "page_idx": 15}, {"type": "text", "text": "For the Property 2, we show that $p(z)\\,=\\,2^{-\\Theta(k)}$ for $z\\,\\in\\,[0,b/2]$ , $z\\in[-b/2,0]$ , $z\\in[-b,-b/2]$ and $z\\in[-\\infty,-b]$ separately. For $z\\in[0,b/2]$ notice that $p(z)$ is convex for $z\\in[0,b]$ ; therefore, it suffices to show $p(0)\\;=\\;2^{-\\Theta(k)}$ . Note that $\\begin{array}{r}{p(0)\\;=\\;\\frac{q(b)}{r(b)}(2k\\,-\\,1)!!/\\sqrt{\\mathbf{E}_{u\\sim\\mathcal{N}_{1}}\\,\\tilde{p}^{2}(u)}}\\end{array}$ , and we have $\\begin{array}{r}{\\left|\\frac{q(b)}{r(b)}\\right|\\geq b^{3k}/\\operatorname*{max}(b^{2k},(2k-1)!!)\\geq\\Omega(k)^{k/2}}\\end{array}$ . Therefore, since $\\mathbf{E}_{u\\sim\\mathcal{N}_{1}}[\\tilde{p}^{2}(u)]\\le(O(k))^{3k}$ , we have $p(0)\\,\\geq\\,2^{-O(k)}$ . This completes the proof of the case $z\\;\\in\\;[0,b/2]$ . For the case $z\\in$ $[-b/2,0]$ , it immediately follows from the fact that $|p(-z)|\\geq|p(z)|$ by the definition. For the case $z\\,\\in\\,[-b,-b/2]$ , we have $p(z)\\,\\leq\\,(p(b)-(b/2)^{3k})/\\sqrt{\\mathbf{E}_{u\\sim\\mathcal{N}_{1}}\\,\\tilde{p}^{2}(u)}\\,\\leq\\,-2^{-O(k)}$ . Then, the case $z\\in[-\\infty,-b]$ immediately follows from the fact that $p(z)$ is monotone increasing in this interval. This completes the proof of Property 2. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Given Claim B.5, we let $p$ be the polynomial in Claim B.5 with $b=-\\operatorname*{max}(2|t^{*}|,4)$ . Then, given that $D$ satisfies the reliability condition with respect to $f$ , it is easy to see the polynomial $p(\\langle\\mathbf{w}^{*},\\mathbf{x}\\rangle)$ satisfies the requirements. This completes the proof of Lemma 2.2. ", "page_idx": 15}, {"type": "text", "text": "B.5 Proof of Lemma 2.4 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We start by bounding the Frobenius norm of $\\mathbf{T}^{m}$ , i.e., $\\|\\mathbf{T}^{m}\\|_{F}$ . Notice that by taking $p:\\mathbb{R}\\to\\mathbb{R}$ to be any degree- $m$ polynomial such that $\\mathbf{E}_{\\mathbf{x}\\sim\\mathcal{N}_{d}}[p(\\mathbf{x})]=0$ and $\\mathbf{E}_{\\mathbf{x}\\sim\\mathcal{N}_{d}}[p(\\mathbf{x})^{2}]=1$ , then $\\|\\mathbf{T}^{m}\\|_{F}=$ $\\begin{array}{r}{\\operatorname*{max}_{p}\\mathbf{E}_{(\\mathbf{x},y)\\sim D}[\\mathbf{1}(y=-1)p(\\mathbf{x})]}\\end{array}$ . We leverage the following standard fact about the concentration of polynomials lemma known as Bonami-Beckner inequality or simply Gaussian hypercontractivity. Fact B.6 (Hypercontractivity Concentration Inequality [Bog98, Nel73]). Consider any $m$ -degree, zero-mean and unit variance polynomial $p:\\mathbb{R}^{d}\\stackrel{\\cdot}{\\rightarrow}\\mathbb{R}$ with respect to $\\mathcal{N}_{d}$ . Then, for any $\\lambda\\geq0$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\Pr_{\\mathbf{x}\\sim\\mathcal{N}_{d}}\\left[\\left|p(\\mathbf{x})-\\mathbf{E}[p(\\mathbf{u})]\\right|\\geq\\lambda\\right]\\leq e^{2}e^{-\\left(c\\lambda^{2}\\right)^{1/m}}\\;,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where c is an absolute constant. ", "page_idx": 15}, {"type": "text", "text": "Using Fact B.6, we have that for any zero mean and unit variance polynomial $p$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{(\\mathbf{x},y)\\sim D}{\\mathbf{E}}[\\mathbf{1}(y=-1)p(\\mathbf{x})]\\leq\\underset{(\\mathbf{x},y)\\sim D}{\\mathbf{E}}[\\mathbf{1}(y=-1)|p(\\mathbf{x})|]\\leq\\int_{0}^{\\infty}\\operatorname*{min}\\left(\\gamma,e^{2}e^{-\\left(c\\lambda^{2}\\right)^{1/m}}\\right)d\\lambda}\\\\ &{\\qquad\\qquad\\qquad\\qquad=O\\left(\\gamma\\log(1/\\gamma)^{m/2}\\right)\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, $\\|\\mathbf{T}^{m}\\|_{F}=O\\left(\\gamma\\log(1/\\gamma)^{m/2}\\right)$ . ", "page_idx": 16}, {"type": "text", "text": "To prove the Property 1, since $\\begin{array}{r}{\\|\\mathbf{T}^{\\prime m}\\|_{F}\\leq\\|\\mathbf{T}^{m}\\|_{F}+\\tau/(4\\sqrt{k})=O\\left(\\gamma\\log(1/\\gamma)^{m/2}\\right)+\\tau/(4\\sqrt{k}),}\\end{array}$ there can be at most $\\|\\mathbf{T}^{m}\\|_{F}^{2}/\\left(\\tau/\\sqrt{k}\\right)^{2}=O(\\gamma^{2}\\log(1/\\gamma)^{m}k/\\tau^{2}+1)$ many singular vectors with singular values greater than $\\tau/(4\\sqrt{k})$ . Therefore, $\\dim(V)$ is at most $O(\\gamma^{2}\\log(1/\\gamma)^{m}k/\\tau^{2}+1)$ . For the Property 2, suppose $\\|\\mathrm{proj}_{V}(\\mathbf{v}^{*})\\|_{2}=o\\left(\\tau/\\left(\\sqrt{k}\\gamma\\log(1/\\gamma)^{k/2}\\right)\\right)$ , then we have for any $m$ , $\\begin{array}{r l}&{\\|\\mathbf{v}^{*^{\\top}\\top}\\mathbf{T}^{\\prime m}\\|_{2}\\leq\\|\\mathbf{v}^{*^{\\top}\\top}\\mathbf{T}^{m}\\|_{2}+\\tau/(4\\sqrt{k})}\\\\ &{\\qquad\\qquad\\qquad\\leq\\|\\mathbf{T}^{m}\\|_{F}\\|\\mathrm{proj}_{V}(\\mathbf{v}^{*})\\|_{2}+\\tau/(4\\sqrt{k})\\|\\mathrm{proj}_{\\bot V}(\\mathbf{v}^{*})\\|_{2}+\\tau/(4\\sqrt{k})\\leq(5/8)\\tau/\\sqrt{k}\\,.}\\end{array}$ ", "page_idx": 16}, {"type": "text", "text": "However, since $\\begin{array}{r}{\\mathbf{E}_{(\\mathbf{x},\\mathbf{y})\\sim D}[\\mathbf{1}(y=-1)p(\\langle\\mathbf{v}^{*},\\mathbf{x}\\rangle)]=\\sum_{m=1}^{k}{a_{i}\\mathbf{v}^{*}}^{\\top}\\mathbf{T}^{m}\\mathbf{v}^{*\\otimes m-1}\\geq\\tau}\\end{array}$ for some $a_{1}^{2}+$ $\\cdots+a_{k}^{2}=1$ , we know there must be an $m$ that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left|\\mathbf{v^{\\ast}}^{\\top}\\mathbf{T}^{m}\\mathbf{v^{\\ast}}^{\\otimes m-1}\\right|\\geq\\tau/\\sqrt{k}\\ .\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, we can write ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathbf{v}^{*\\top}\\mathbf{T}^{\\prime m}\\|_{2}\\geq\\|\\mathbf{v}^{*\\top}\\mathbf{T}^{m}\\|_{2}-\\tau/(4\\sqrt{k})\\geq\\tau/\\sqrt{k}-\\tau/(4\\sqrt{k})=(3/4)\\tau/\\sqrt{k}\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which contradicts $\\|\\mathbf{v}^{*\\top}\\mathbf{T}^{\\prime m}\\|_{2}\\leq(5/8)\\tau/\\sqrt{k}$ . This completes the proof. ", "page_idx": 16}, {"type": "text", "text": "B.6 Proof of Proposition 2.1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We first give the pseudocode for the algorithm in Proposition 2.1. ", "page_idx": 16}, {"type": "text", "text": "Input: Sample access to a joint distribution $D$ of $\\left(\\mathbf{x},y\\right)$ supported on $\\mathbb{R}^{d}\\times\\{\\pm1\\}$ with the marginal $D_{\\mathbf{x}}=\\mathcal{N}_{d}$ . Let $\\epsilon,\\delta\\in(0,1)$ and suppose $D$ satisfies the reliability condition with respect to an LTF $f(\\mathbf{x})=\\mathrm{sign}(\\langle\\mathbf{w}^{*},\\mathbf{x}\\rangle-t^{*})$ with $t^{*}=O\\left(\\sqrt{\\log(1/\\epsilon)}\\right)$ and $\\mathbf{Pr}_{(\\mathbf{x},y)\\sim D}[y=-1]\\geq\\epsilon$ . ", "page_idx": 16}, {"type": "text", "text": "Output: With probability at least $\\Omega(1)$ , the algorithm outputs a unit vector $\\mathbf{v}$ such that $\\langle\\mathbf{v},\\mathbf{w}^{*}\\rangle=$ $\\operatorname*{max}(\\log(1/\\epsilon)^{-O({t^{*}}^{2})},\\epsilon^{-O(1)})$ . ", "page_idx": 16}, {"type": "text", "text": "1. Let $c_{1}$ be a sufficiently large universal constant and $c_{2}$ be a sufficiently large universal constant depending on $c_{1}$ . Let $S$ be a set of $N=d^{c_{2}\\operatorname*{max}(t^{*\\,2},1)}\\log({1}/{\\delta})/{\\epsilon^{2}}$ many samples from $D$ . For $m=1,\\cdot\\cdot\\cdot\\,,c_{1}{t^{*}}^{2}$ , with $1/2$ probability, take ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{T}^{\\prime m}=\\underset{(\\mathbf{x},y)\\sim_{u}S}{\\mathbf{E}}[\\mathbf{1}(y=-1)\\mathbf{H}^{m}(\\mathbf{x})]\\mathbf{\\epsilon},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "to be the empirical chow-tensor on negative samples. Otherwise, take ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\bf T}^{\\prime m}=\\underset{({\\bf x},y)\\sim_{u}S}{\\bf E}[y{\\bf H}^{m}({\\bf x})]\\mathrm{~,~}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "to be the empirical Chow-tensor on all samples. Let $\\gamma$ be the empirical estimation of $\\mathbf{Pr}_{(\\mathbf{x},y)\\sim D}[y=-1]$ with error at most $\\epsilon/100$ with a sufficiently small constant failure probability. ", "page_idx": 16}, {"type": "text", "text": "2. Take $\\tilde{\\mathbf{T}}^{\\prime m}\\in\\mathbb{R}^{d\\times d^{m-1}}$ to be the flattened version of $\\mathbf{T}^{\\prime m}$ , and let $V_{m}$ be the subspace spanned by the left singular vectors of $\\tilde{\\mathbf{T}}^{\\prime m}$ whose singular values are greater than $2^{-c t^{*}}\\gamma$ where $c$ is a sufficiently large constant. Let $V$ be the union of all $V_{m}$ and output $\\mathbf{v}$ to be a random unit vector chosen from $V$ . ", "page_idx": 16}, {"type": "text", "text": "We now prove Proposition 2.1. ", "page_idx": 16}, {"type": "text", "text": "Proof of Proposition 2.1. We first introduce the following fact about learning Chow tensors. ", "page_idx": 16}, {"type": "text", "text": "Fact B.7. Fix $m\\in\\mathbb{Z}_{+}$ , and $\\epsilon,\\delta\\in(0,1)$ . Let $D$ be a distribution on $\\mathbb{R}^{d}\\times[\\pm1]$ with standard normal marginals. There is an algorithm that with $N=d^{O(m)}\\log(1/\\delta)/\\epsilon^{2}$ samples and poly $(d,N)$ runtime, outputs an approximation $\\mathbf{T}^{\\prime m}$ of the order-m Chow-parameter tensor $\\mathbf{T}^{m}=\\mathbf{E}_{(\\mathbf{x},y)\\sim D}[y\\mathbf{H}^{m}(\\mathbf{x})]$ such that with probability $1-\\delta$ , it holds $\\|\\mathbf{T}^{\\prime m}-\\mathbf{T}^{m}\\|_{F}\\leq\\epsilon$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. Let $\\mathbf{T}^{\\prime m}$ be the empirical estimation of $\\mathbf{T}^{m}$ using $N=d^{c m}\\log(1/\\delta)/\\epsilon^{2}$ for sufficiently large universal constant $c$ . We start by showing $\\mathbf{E}_{(\\mathbf{x},y)\\sim D}\\left[\\|y\\mathbf{H}^{m}(\\mathbf{x})\\|_{F}^{2}\\right]\\leq d^{m}$ . Notice that, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\underset{\\mathbf{x},y)\\sim D}{\\mathbf{E}}\\left[\\|y\\mathbf{H}^{m}(\\mathbf{x})\\|_{F}^{2}\\right]=\\underset{(\\mathbf{x},y)\\sim D}{\\mathbf{E}}\\left[y^{2}\\|\\mathbf{H}^{m}(\\mathbf{x})\\|_{F}^{2}\\right]\\leq\\underset{(\\mathbf{x},y)\\sim D}{\\mathbf{E}}\\left[\\|\\mathbf{H}^{m}(\\mathbf{x})\\|_{F}^{2}\\right]=\\underset{\\mathbf{x}\\sim\\mathbb{R}^{d}}{\\mathbf{E}}\\left[\\|\\mathbf{H}^{m}(\\mathbf{x})\\|_{F}^{2}\\right]\\;.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Notice that each entry of ${\\bf H}^{m}({\\bf x})$ must be some $\\alpha h(\\mathbf{x})$ where $\\alpha\\in[0,1]$ and $h(\\mathbf{x})$ is a unit variance Hermite polynomial. Therefore, $\\mathbf{E}_{(\\mathbf{x},y)\\sim D}\\left[\\|\\mathbf{H}^{m}(\\mathbf{x})\\|_{F}^{2}\\right]\\leq d^{m}$ and thus $\\mathbf{\\dot{E}}_{(\\mathbf{x},y)\\sim D}\\left[\\|y\\mathbf{H}^{m}(\\mathbf{x})\\|_{F}^{2}\\right]\\leq$ $d^{m}$ . ", "page_idx": 17}, {"type": "text", "text": "Using the above, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{(\\mathbf{x},y)\\sim D}{\\mathbf{E}}\\left[\\|y\\mathbf{H}^{m}(\\mathbf{x})-\\mathbf{T}^{m}\\|_{F}^{2}\\right]=\\underset{(\\mathbf{x},y)\\sim D}{\\mathbf{E}}\\left[\\|y\\mathbf{H}^{m}(\\mathbf{x})\\|_{F}^{2}\\right]-\\|\\mathbf{T}^{m}\\|_{F}^{2}\\leq d^{m}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then, applying Chebyshev\u2019s inequality gives $\\|\\mathbf{T}^{\\prime m}-\\mathbf{T}^{m}\\|_{F}\\leq\\epsilon$ . ", "page_idx": 17}, {"type": "text", "text": "We prove Proposition 2.1 for two cases: (a) $\\operatorname*{max}(\\log(1/\\epsilon)^{-O({t^{*}}^{2})},\\epsilon^{-O(1)})\\,=\\,\\log(1/\\epsilon)^{-O({t^{*}}^{2})}$ ; and (b) $\\operatorname*{max}(\\log(1/\\epsilon)^{-O({t^{*}}^{2})},\\epsilon^{-O(1)})\\;=\\;\\epsilon^{-O(1)}$ . We first prove Case (a) as the other case is similar. For Case (a), it suffices for us to prove that $\\left\\langle\\mathbf{v},\\mathbf{w}^{*}\\right\\rangle=\\log(1/\\epsilon)^{-O({t^{*}}^{2})}$ happens with $\\Omega(1)$ probability given the algorithm chooses ${\\bf T}^{\\prime m}={\\bf E}_{({\\bf x},y)\\sim_{u}S}[{\\bf1}(y=-1){\\bf H}^{m}({\\bf x})]$ (which happens with 1/2 pr\u221aobability). Let $k=c_{1}t^{*2}$ and $\\tau/(4\\sqrt{k})=2^{-c t^{*}^{2}}\\gamma$ . By Fact B.7, we have $\\|\\mathbf{T}^{\\prime m}-\\mathbf{T}^{m}\\|_{F}\\leq$ $\\tau/(4\\sqrt{k})$ . Then from Lemma 2.2 and Lemma 2.4, if we take $\\mathbf{v}$ to be a random vector in $V$ , we will with constant probability have, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\mathbf{v},\\mathbf{w}^{*}\\rangle=\\frac{\\Omega\\,\\left(\\tau/\\left(\\sqrt{k}\\gamma\\log(1/\\gamma)^{k/2}\\right)\\right)}{\\mathrm{dim}(V)^{1/2}}=\\Omega\\left(\\tau^{2}/\\left(\\log(1/\\gamma)^{k}k^{3/2}\\gamma^{2}\\right)\\right)}\\\\ &{\\qquad\\quad=\\Omega\\left(\\log(1/\\gamma)^{-O(t^{*\\,2})}k^{-3/2}\\right)\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since $\\mathbf{Pr}_{(\\mathbf{x},y)\\sim D}[y\\,=\\,-1]\\,\\geq\\,\\epsilon$ , we have $\\gamma\\,=\\,\\Omega(\\epsilon)$ . Also, considering $k=O(\\operatorname*{max}\\{t^{*2},1\\})\\,=$ $O\\left(\\log(1/\\epsilon)\\right)$ , we get $\\left\\langle\\mathbf{v},\\mathbf{w}^{*}\\right\\rangle=\\log(1/\\epsilon)^{-O({t^{*}}^{2})}$ . ", "page_idx": 17}, {"type": "text", "text": "For Case (b), the proof remains the same except we take ${\\bf T}^{\\prime m}={\\bf E}_{({\\bf x},y)\\sim_{u}S}[y{\\bf H}^{m}({\\bf x})]$ and use fact below instead of Lemma 2.4. ", "page_idx": 17}, {"type": "text", "text": "Fact B.8 (Lemma 5.10 in $[\\mathrm{DKK}^{+}22]\\rangle$ ). Let $D$ be the joint distribution of $(\\mathbf{x},y)$ supported on $\\mathbb{R}^{d}\\times\\{\\pm1\\}$ with the marginal $D_{\\mathbf{x}}=\\mathcal{N}_{d}$ . Let $p:\\mathbb{R}\\mapsto\\mathbb{R}$ be a univariate, mean zero, unit variance polynomial of degree $k$ such that for some unit vector $\\mathbf{v}^{*}\\in\\mathbb{R}^{d}$ it holds $\\begin{array}{r}{\\mathbf{E}_{(\\mathbf{x},y)\\sim D}[y p(\\langle\\mathbf{v}^{*},\\mathbf{x}\\rangle)]\\geq\\tau}\\end{array}$ for some $\\tau\\in(0,1]$ . Let $\\mathbf{T}^{\\prime m}$ be an approximation of t\u221ahe order- $^m$ Chow-parameter tensor $\\mathbf{T}^{m}=$ $\\mathbf{E}_{(\\mathbf{x},y)\\sim D}[y\\mathbf{H}_{m}(\\mathbf{x})]$ such that $\\|\\mathbf{T}^{\\prime m}-\\mathbf{T}^{m}\\|_{F}\\leq\\tau/(4\\sqrt{k})$ . Denote by $V_{m}$ the subspa\u221ace spanned by the left singular vectors of flattened $\\mathbf{T}^{\\prime m}$ whose singular values are greater than $\\tau/(4\\sqrt{k})$ . Moreover, denote by $V$ the union of $V_{1},\\cdots,V_{k}$ . Then we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|\\operatorname{proj}_{V}(\\mathbf{v}^{*})\\|_{2}\\geq\\tau/\\Big(4\\sqrt{k}\\Big).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then the same argument will give $\\left\\langle\\mathbf{v},\\mathbf{w}^{*}\\right\\rangle=\\epsilon^{-O(1)}$ . The above described algorithm uses at most $N=d^{O(t^{*}^{2})}/\\epsilon^{2}$ samples and poly $(N,1/\\epsilon)$ runtime. This completes the proof of Proposition 2.1. ", "page_idx": 17}, {"type": "text", "text": "B.7 Proof of Lemma 2.5 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Item 1 follows from the definition of $D^{\\prime}$ and the fact that $D$ has marginal distribution $D_{\\mathbf{x}}=\\mathcal{N}_{d}$ . ", "page_idx": 18}, {"type": "text", "text": "For proving Item 2, we consider two cases: The first case is when $t^{*}>0$ and the second one when $t^{*}\\leq0$ . For the case $t^{*}\\leq0$ , we prove that the distribution $D^{\\prime}$ satisfies the reliability condition with respect to $h^{\\prime}(\\mathbf{x})=\\mathrm{sign}(\\langle\\mathbf{w}^{\\prime},\\mathbf{x}\\rangle)$ where $\\mathbf{w}^{\\prime}=\\mathbf{w}^{*\\perp\\mathbf{w}}/\\Vert\\mathbf{w}^{*\\perp\\mathbf{w}}\\Vert_{2}$ . Notice that for any $\\left(\\mathbf{x},y\\right)$ such that $\\mathbf{x}\\in B$ and $h^{\\prime}(\\mathbf{x}^{\\perp\\mathbf{w}})=1$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\mathbf{x})=\\mathrm{sign}(\\langle\\mathbf{w}^{*},\\mathbf{x}\\rangle-t^{*})}\\\\ &{\\quad\\quad=\\mathrm{sign}(\\langle\\mathrm{proj}_{\\perp\\mathbf{w}}(\\mathbf{w}^{*}),\\mathbf{x}\\rangle+\\langle\\mathrm{proj}_{\\mathbf{w}}(\\mathbf{w}^{*}),\\mathbf{x}\\rangle-t^{*})}\\\\ &{\\quad\\quad=\\mathrm{sign}(\\langle\\mathbf{w}^{*\\perp\\mathbf{w}},\\mathbf{x}^{\\perp\\mathbf{w}}\\rangle+\\langle\\mathrm{proj}_{\\mathbf{w}}(\\mathbf{w}^{*}),\\mathbf{x}\\rangle-t^{*})\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\langle\\mathbf{w}^{*\\perp\\mathbf{w}},\\mathbf{x}^{\\perp\\mathbf{w}}\\rangle\\geq0$ since $h^{\\prime}(\\mathbf{x})=\\mathrm{sign}(\\langle\\mathbf{w}^{*\\bot\\mathbf{w}},\\mathbf{x}^{\\bot\\mathbf{w}}\\rangle/\\|\\mathbf{w}^{*\\bot\\mathbf{w}}\\|_{2})\\geq0.$ Then we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\mathbf{x})\\geq\\mathrm{sign}(\\langle\\mathrm{proj}_{\\mathbf{w}}(\\mathbf{w}^{*}),\\mathbf{x}\\rangle-t^{*})}\\\\ &{\\quad\\quad=\\mathrm{sign}(\\langle\\mathrm{proj}_{\\mathbf{w}}(\\mathbf{w}^{*})/\\|\\mathrm{proj}_{\\mathbf{w}}(\\mathbf{w}^{*})\\|_{2},\\mathbf{x}\\rangle-t^{*}/\\|\\mathrm{proj}_{\\mathbf{w}}(\\mathbf{w}^{*})\\|_{2}}\\\\ &{\\quad\\quad\\geq\\mathrm{sign}(\\langle\\mathbf{w},\\mathbf{x}\\rangle-t^{*})}\\\\ &{\\quad\\quad\\geq\\mathrm{sign}(\\langle\\mathbf{w},\\mathbf{x}\\rangle-t)}\\\\ &{\\quad\\quad=h(\\mathbf{x})=+1\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where Inequality 2 follows from the fact that $\\langle\\mathbf{w},\\mathbf{w}^{*}\\rangle\\,>\\,0$ , $\\|\\mathrm{proj}_{\\mathbf{w}}(\\mathbf{w}^{*})\\|_{2}\\,\\in\\,(0,1]$ and $t^{*}\\leq0$ .   \nSince $D$ satisfies the reliability condition with respect to $f$ , we have that it must be the case $y=+1$ .   \nTherefore, $D^{\\prime}$ satisfies the reliability condition with respect to $h^{\\prime}$ . ", "page_idx": 18}, {"type": "text", "text": "For the case $t^{*}>0$ , we prove that the distribution $D^{\\prime}$ satisfies the reliability condition with respect to $h^{\\prime}(\\mathbf{x})=\\mathrm{sign}(\\langle\\mathbf{w}^{\\prime},\\mathbf{x}\\rangle-t^{\\prime})$ where $\\mathbf{w}^{\\prime}=\\mathbf{w}^{*}\\bot\\mathbf{w}/\\|\\mathbf{w}^{*}\\bot\\mathbf{w}\\|_{2}$ and $t^{\\prime}=t^{*}$ . Notice that for any $(\\mathbf{x},y)$ such that $\\mathbf{x}\\in B$ and $h^{\\prime}(\\mathbf{x}^{\\perp\\mathbf{w}})=1$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\mathbf{x})=\\mathrm{sign}(\\langle\\mathbf{w}^{*},\\mathbf{x}\\rangle-t^{*})}\\\\ &{\\quad\\quad=\\mathrm{sign}(\\langle\\mathrm{proj}_{\\bot\\mathbf{w}}\\mathbf{w}^{*},\\mathbf{x}\\rangle+\\langle\\mathrm{proj}_{\\mathbf{w}}\\mathbf{w}^{*},\\mathbf{x}\\rangle-t^{*})}\\\\ &{\\quad\\quad=\\mathrm{sign}\\left(\\|\\mathbf{w}^{*\\bot\\mathbf{w}}\\|_{2}\\langle\\mathbf{w}^{\\prime},\\mathbf{x}\\rangle-\\|\\mathbf{w}^{*\\bot\\mathbf{w}}\\|_{2}t^{*}+\\langle\\mathrm{proj}_{\\mathbf{w}}\\mathbf{w}^{*},\\mathbf{x}\\rangle-(1-\\|\\mathbf{w}^{*\\bot\\mathbf{w}}\\|_{2})t^{*}\\right)\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\|\\mathbf{w}^{\\ast\\perp\\mathbf{w}}\\|_{2}\\langle\\mathbf{w}^{\\prime},\\mathbf{x}\\rangle-\\|\\mathbf{w}^{\\ast\\perp\\mathbf{w}}\\|_{2}t^{\\ast}\\geq0$ since $h^{\\prime}(\\mathbf{x})=+1$ . Then we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\mathbf{x})\\geq\\mathrm{sign}(\\langle\\mathrm{proj}_{\\mathbf{w}}\\mathbf{w}^{*},\\mathbf{x}\\rangle-(1-\\|\\mathbf{w}^{*\\bot\\mathbf{w}}\\|_{2})t^{*})}\\\\ &{\\qquad=\\mathrm{sign}\\left(\\sqrt{1-\\|\\mathbf{w}^{*\\bot\\mathbf{w}}\\|_{2}^{2}}\\,\\langle\\mathbf{w},\\mathbf{x}\\rangle-(1-\\|\\mathbf{w}^{*\\bot\\mathbf{w}}\\|_{2})t^{*}\\right)}\\\\ &{\\qquad\\geq\\mathrm{sign}\\left(\\sqrt{1-\\|\\mathbf{w}^{*\\bot\\mathbf{w}}\\|_{2}^{2}}\\,\\left(\\langle\\mathbf{w},\\mathbf{x}\\rangle-t^{*}\\right)\\right)=h(\\mathbf{x})=1\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the second equation follows from $\\langle\\mathbf{w},\\mathbf{w}^{*}\\rangle>0$ and the third one follows from $t^{*}>0$ . Since $D$ satisfies the reliability condition with respect to $f$ , we have that it must be the case $y\\,=\\,+1$ Therefore, $D^{\\prime}$ satisfies the reliability condition with respect to $h^{\\prime}$ . ", "page_idx": 18}, {"type": "text", "text": "For proving Item 3, notice that $R_{+}(h;D)\\geq\\epsilon/2$ implies ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Pr_{(\\mathbf{x}^{\\prime},y)\\sim D^{\\prime}}[y=-1]=R_{+}(h;D)/\\operatorname*{Pr}_{(\\mathbf{x},y)\\sim D}(\\mathbf{x}\\in B)\\geq\\epsilon/2\\;.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This completes the proof. ", "page_idx": 18}, {"type": "text", "text": "B.8 Proof of Theorem B.4 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We first give the algorithm in Theorem B.4, which is a more detailed version of Algorithm 1. ", "page_idx": 18}, {"type": "text", "text": "Input: $\\epsilon\\in(0,1)$ , $\\alpha\\in(0,1/2)$ and samples access to a joint distribution $D$ of $\\left(\\mathbf{x},y\\right)$ supported on $\\mathbb{R}^{d}\\times\\{\\pm1\\}$ with $\\mathbf{x}$ -marginal $D_{\\mathbf{x}}=\\mathcal{N}_{d}$ . ", "page_idx": 19}, {"type": "text", "text": "Output: $\\begin{array}{r}{\\dot{h^{\\prime}}(\\mathbf{x})=\\mathrm{sign}(\\langle\\mathbf{w},\\mathbf{x}\\rangle-t)}\\end{array}$ that is $\\epsilon$ -reliable with respect to the class $\\mathcal{H}_{d}^{\\alpha}$ . 1. Check if $\\mathbf{Pr}_{(\\mathbf{x},y)\\sim D}[y=-1]\\le\\epsilon/2$ (with sufficiently small constant failure probability). If so, return the $+1$ constant hypothesis. If the parameters satisfy ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\left(2^{\\log(1/\\epsilon)^{O(\\log(1/\\alpha))}},2^{\\mathrm{poly}(1/\\epsilon)}\\right)=2^{\\log(1/\\epsilon)^{O(\\log(1/\\alpha))}}\\ ,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "then set the correlation parameter $\\zeta=\\log(1/\\epsilon)^{-c t^{*}^{2}}$ , where $c$ is a sufficiently large universal constant. Otherwise, set $\\zeta=\\epsilon^{c}$ for a sufficiently large universal constant $c$ . For $t\\,{\\bar{=}}-\\Phi^{-1}(\\alpha)$ and $t=\\Phi^{-1}(\\alpha)$ , do Steps (2) and (3). ", "page_idx": 19}, {"type": "text", "text": "2. Initialize w to be a random unit vector in $\\mathbb{R}^{d}$ . Let the update step size $\\lambda=\\zeta$ and repeat the following process until $\\lambda\\le\\epsilon/100$ . (a) Use samples from $D$ to check if the hypothesis $h(\\mathbf{x})\\,=\\,\\mathrm{sign}(\\langle\\mathbf{w},\\mathbf{x}\\rangle\\,-\\,t)$ satisfies $R_{+}(h;D)\\leq\\epsilon/2$ . If so, go to Step (3). (b) With $1/2$ probability, let $\\mathbf{w}=-\\mathbf{w}$ . Let ${\\cal B}=\\{{\\bf x}\\in\\mathbb{R}^{d}:\\langle{\\bf w},{\\bf x}\\rangle-t\\geq0\\}$ , and let $D^{\\prime}$ be the distribution of $(\\mathbf{x}^{\\perp\\mathbf{w}},y)$ for $(\\mathbf{x},y)\\sim D$ given $\\mathbf{x}\\in B$ . Use the algorithm of Proposition 2.1 on D\u2032 to find a unit vector v such that \u27e8v, w\u27e9= 0 and v,\u2225pprroojj\u22a5\u22a5ww((ww\u2217\u2217))\u22252 Then, update w as follows: wupdate = $\\begin{array}{r}{\\mathbf{w}_{\\mathrm{update}}=\\frac{\\mathbf{w}+\\lambda\\mathbf{v}}{\\|\\mathbf{w}+\\lambda\\mathbf{v}\\|_{2}}}\\end{array}$ (c) Repeat Steps (2a) and (2b) $c/\\zeta^{2}$ times, where $c$ is a sufficiently large universal constant, with the same step size $\\lambda$ . After that, update the new step size as $\\bar{\\lambda_{\\mathrm{update}}}=\\lambda/2$ .   \n3. Check if $h(\\mathbf{x})=\\mathrm{sign}(\\langle\\mathbf{w},\\mathbf{x}\\rangle-t)$ satisfies $R_{+}(h;D)\\leq\\epsilon/2$ . If so, add it in the set $S$ . For each choice of value $t$ in Step (1), repeat Step (2) $2^{1/\\zeta^{c}}$ many times where $c$ is a sufficiently large constant.   \n4. Let $S$ be the set in Step (3). Return the hypothesis $h(\\mathbf{x})=\\mathrm{sign}(\\langle\\mathbf{w},\\mathbf{x}\\rangle-t)$ , where $({\\bf w},t)=$ $\\mathrm{argmin}_{(\\mathbf{w},t)\\in S}\\,t$ . Return the $-1$ constant hypothesis if $S$ is empty. ", "page_idx": 19}, {"type": "text", "text": "Proof of Lemma 2.5. Without loss of generality, we assume $\\epsilon>\\alpha$ for the following reason. Suppose $\\alpha\\leq\\epsilon$ , then we can simply return one of the constant hypotheses that is closest to $f$ . Let $f(\\mathbf{x})=$ $\\mathrm{sign}(\\langle\\mathbf{w}^{*},\\mathbf{x}\\rangle-t^{*})$ be the optimal LTF with $\\alpha$ bias, namely, ", "page_idx": 19}, {"type": "equation", "text": "$$\nf=\\underset{f\\in\\mathcal{H}_{d}^{\\alpha}\\wedge R_{+}(h;D)=0}{\\mathrm{argmin}}R_{-}(f;D)\\;.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We prove that with high probability, Algorithm 3 returns a hypothesis $h(\\mathbf{x})=\\mathrm{sign}(\\langle\\mathbf{w},\\mathbf{x}\\rangle-t)$ such that $R_{+}(h;D)\\leq\\epsilon$ and $R_{-}(h;D)\\leq\\bar{R}_{-}(f;D)+\\epsilon$ . Notice that at some point, we will run Step (2) with $t=t^{*}$ . We show that given $t=t^{*}$ , Step (2) will with probability $2^{-\\mathrm{poly}(1/\\zeta)}$ returns a $h$ with $R_{+}(h;D)\\leq\\epsilon/2$ . For convenience, we assume $h$ never satisfies $R_{+}(h;D)\\leq\\epsilon/2$ in Step (2a) (otherwise, we are done). Furthermore, we assume the subroutine algorithm in Proposition 2.1 used in Step (2b) always succeed, and we always have $\\langle\\mathbf{w},\\mathbf{w}^{*}\\rangle\\,\\geq\\,0$ , since both happen with constant probability and we are running the update at most $\\dot{O}(\\log(1/\\epsilon)/\\zeta^{2})=\\mathrm{poly}(1/\\zeta)$ many times. ", "page_idx": 19}, {"type": "text", "text": "Let $\\eta=\\|\\mathrm{proj}_{\\perp\\mathbf{w}}\\mathbf{w}^{*}\\|_{2}$ . Now, we will prove by induction that each time after $c/\\zeta^{2}$ many updates in step (2b), we always have $\\|\\mathrm{proj}_{\\perp\\mathrm{w}}\\mathbf{w}^{*}\\|_{2}\\leq3\\lambda$ (without loss of generality, we assume $\\lambda$ is always at most a sufficiently small constant). Notice that by Lemma 2.5 and Proposition 2.1, we have that at each update, given the subroutine algorithm in Proposition 2.1 succeeds, the update direction $\\mathbf{v}$ always satisfies v, proj\u22a5\u03b7w(w\u2217) \u2265\u03b6, which implies \u27e8v, proj\u22a5w(w\u2217)\u27e9\u2265\u03b7\u03b6. Then we have $\\left\\langle\\mathbf{w},\\mathbf{w}^{*}\\right\\rangle=\\sqrt{1-\\eta^{2}}\\geq1-\\eta^{2}$ . When we update w, there are two possibilities. Either $\\lambda>\\eta\\zeta/2$ or $\\lambda\\leq\\eta\\zeta/2$ . If $\\lambda\\leq\\eta\\zeta/2$ , using Fact 2.6, we will have $\\left\\langle\\mathbf{w}_{\\mathrm{update}},\\mathbf{w}^{*}\\right\\rangle\\geq\\left\\langle\\mathbf{w},\\mathbf{w}^{*}\\right\\rangle+\\lambda^{2}/2$ . If $\\lambda>\\eta\\zeta/2$ , ", "page_idx": 19}, {"type": "text", "text": "we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\mathrm{proj}_{\\perp\\mathbf{w}_{\\mathrm{update}}}(\\mathbf{w}^{*})\\right\\|_{2}=\\left\\|\\mathrm{proj}_{\\perp\\mathbf{w}^{*}}(\\mathbf{w}_{\\mathrm{update}})\\right\\|_{2}\\leq\\left\\|\\mathrm{proj}_{\\perp\\mathbf{w}^{*}}(\\mathbf{w}+\\lambda\\mathbf{v})\\right\\|_{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\left\\|\\mathrm{proj}_{\\perp\\mathbf{w}^{*}}(\\mathbf{w})\\right\\|_{2}+\\lambda\\leq3\\lambda\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For the base case, when we do the first group of $c/\\zeta^{2}$ many updates, combining the two cases above and the fact that $\\lambda=\\zeta\\geq\\eta\\zeta$ gives that after $c/\\zeta^{2}$ many updates, we must have $\\|\\mathrm{proj}_{\\perp\\mathrm{w}}\\mathbf{w}^{*}\\|_{2}\\leq3\\lambda$ . For the induction case, given $\\eta\\le6\\lambda$ (which follows from the previous group of $c/\\zeta^{2}$ many updates), combining the two cases above gives that after $c/\\zeta^{2}$ many updates, we must have $\\|\\mathrm{proj}_{\\perp\\mathrm{w}}\\mathbf{w}^{*}\\|_{2}\\leq3\\lambda$ . This proves that after each group of $c/\\zeta^{2}$ many updates, we have $\\|\\mathrm{proj}_{\\perp\\mathrm{w}}\\mathbf{w}^{*}\\|_{2}\\leq3\\lambda$ . ", "page_idx": 20}, {"type": "text", "text": "Therefore, when we have $\\lambda\\le\\epsilon/100$ , we get $\\|\\mathrm{proj_{\\perpw}}\\mathbf{w}^{*}\\|_{2}\\leq3\\epsilon/100$ , which implies $R_{+}(h;D)\\leq$ $\\epsilon/2$ since $D$ satisfies the reliability condition with respect to $f$ . Given $R_{+}(h;D)\\leq\\epsilon/2$ , using the fact that $t-t^{*}\\leq0$ (since we always output the hypothesis with the smallest $t$ ) implies ${\\bf P r}_{({\\bf x},y)\\sim{\\cal D}}[h({\\bf x})=$ $1]\\geq\\mathbf{Pr}_{(\\mathbf{x},y)\\sim D}[f(\\mathbf{x})=1]$ , we have that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R_{-}(h;D)=\\underset{(\\mathbf{x},y)\\sim D}{\\mathbf{Pr}}[y=1]-\\underset{(\\mathbf{x},y)\\sim D}{\\mathbf{Pr}}[y=1\\wedge h(\\mathbf{x})=1]}\\\\ &{\\qquad\\qquad=\\underset{(\\mathbf{x},y)\\sim D}{\\mathbf{Pr}}[y=1]-\\underset{(\\mathbf{x},y)\\sim D}{\\mathbf{Pr}}[h(\\mathbf{x})=1]+R_{+}(h;D)}\\\\ &{\\qquad\\qquad\\leq\\underset{(\\mathbf{x},y)\\sim D}{\\mathbf{Pr}}[y=1]-\\underset{(\\mathbf{x},y)\\sim D}{\\mathbf{Pr}}[f(\\mathbf{x})=1]+\\epsilon=R_{-}(f;D)+\\epsilon\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This completes the proof. ", "page_idx": 20}, {"type": "text", "text": "C Omitted Proofs from Section 3 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We first restate our main SQ hardness result. ", "page_idx": 20}, {"type": "text", "text": "Theorem C.1 (SQ Lower Bound for Reliable Learning). For any $\\alpha>0$ that is at most a sufficiently small absolute constant and $\\epsilon\\in(0,\\alpha/3)$ , any $S Q$ algorithm that reliably learns $\\alpha$ -biased LTFs (for positive labels) on $\\mathbb{R}^{d}$ under Gaussian marginals to additive error $\\epsilon$ either $(i)$ requires at least one query of tolerance at most $\\begin{array}{r}{d^{-\\Omega\\left(\\log\\left(\\frac{1}{\\alpha}\\right)\\right)}}\\end{array}$ , or (ii) requires at least $2^{d^{\\Omega(1)}}$ queries. ", "page_idx": 20}, {"type": "text", "text": "C.1 Proof of Lemma 3.3 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We let $P_{n}$ denote the set of all polynomials $p:\\mathbb{R}\\to\\mathbb{R}$ of degree at most $n$ and let $L_{+}^{1}(\\mathbb{R})$ denote the set of all nonnegative functions in $L^{1}(\\mathbb{R})$ . First, we write the conditions as the primal linear program. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{find}~~~~~~~~~g\\in L^{1}(\\mathbb{R})}\\\\ &{\\mathrm{such\\that}~~~\\underset{z\\sim\\mathcal{N}_{1}}{\\mathbf{E}}[p(z)g(z)]=0\\ ,\\ \\ \\forall p\\in P_{n}}\\\\ &{~}\\\\ &{~~~~~~~~~~~~~g(z)\\geq1\\ ,\\ \\ \\forall z\\geq c}\\\\ &{\\|g\\|_{\\infty}\\leq1}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By introducing variables $H\\in L^{1}(R)$ and $h\\in L_{+}^{1}(R)$ , we rewrite the primal LP as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\underset{z\\sim\\mathcal{N}_{1}}{\\quad\\quad}g\\in L^{1}(\\mathbb{R})}&{}&\\\\ {\\underset{z\\sim\\mathcal{N}_{1}}{\\mathbf{E}}[p(z)g(z)]=0\\ ,}&{}&{\\forall p\\in P_{n}}\\\\ {\\underset{z\\sim\\mathcal{N}_{1}}{\\mathbf{E}}[g(z)h(z)\\mathbb{1}\\{z\\geq c\\}]\\geq\\|h(z)\\mathbb{1}\\{z\\geq c\\}\\|_{1}\\ ,}&{\\forall h\\in L_{+}^{1}(\\mathbb{R})}\\\\ {\\underset{z\\sim\\mathcal{N}_{1}}{\\mathbf{E}}[g(z)H(z)]\\leq\\|H\\|_{1}\\ ,}&{\\forall H\\in L^{1}(\\mathbb{R})}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The following fact is an application of (an infinite generalization of) LP duality: ", "page_idx": 20}, {"type": "text", "text": "Claim C.2. The $L P$ above is feasible if there exists no polynomial $p$ of degree n such that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{z\\sim\\sim\\mathcal{N}_{1}}{\\mathbf{E}}[|p(z)|\\mathbb{1}(z\\leq c)]<\\underset{z\\sim\\mathcal{N}_{1}}{\\mathbf{E}}[p(z)\\mathbb{1}(z\\geq c)]\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof of Claim C.2. First, we introduce some notation. We use $(\\tilde{h},c)$ for the inequality $\\mathbf{E}_{z\\sim\\mathcal{N}}[\\beta(z)\\tilde{h}(z)]+c\\,\\leq\\,0$ , where $\\tilde{h}\\,\\in\\,L^{1}(\\mathbb{R})$ and $c\\in\\mathbb{R}$ . Moreover, let $\\boldsymbol{S}$ be the set that contains all such tuples that describe the target system. For the set $\\boldsymbol{S}$ , the closed convex cone over $L^{1}(\\mathbb{R})\\times\\mathbb{R}$ is the smallest closed set $S_{+}$ satisfying the following: (a) if $A\\in S_{+}$ and $B\\in S_{+}$ then $A+B\\in S_{+}$ ; and (b) if $A\\in{\\mathcal{S}}_{+}$ then $\\lambda A\\in{\\cal S}_{+}$ for all $\\lambda\\geq0$ . Note that the $S_{+}$ contains the same feasible solutions as $\\boldsymbol{S}$ . In order to prove the statement, we need the following LP duality from [Fan68]. ", "page_idx": 21}, {"type": "text", "text": "Fact C.3 (Theorem 1 of [Fan68]). If $\\mathcal{X}$ is a locally convex, real separated vector space. Then, a linear system described by $\\boldsymbol{S}$ is feasible (i.e., there exists a $g\\in\\mathcal{X}^{*}$ ) if and only $i f(0,1)\\not\\in{\\cal S}_{+}$ . ", "page_idx": 21}, {"type": "text", "text": "Our dual LP is defined by the following inequalities: ", "page_idx": 21}, {"type": "text", "text": "(a) $(p,0)$ for $p\\in\\mathcal{P}_{n}$ ; ", "page_idx": 21}, {"type": "text", "text": "(c) and $(H,-\\|H\\|_{1})$ for all $H\\in L^{1}(\\mathbb{R})$ . ", "page_idx": 21}, {"type": "text", "text": "Furthermore, the LP is also equivalent to breaking the last inequality into two, i.e., $(\\tilde{H}_{1},-\\lVert\\tilde{H}_{1}\\rVert_{1})$ for all $\\tilde{H}_{1}(z)=H_{1}(z)\\mathbb{1}\\{z<c\\}$ , where $H_{1}\\in L^{1}(\\mathbb{R})$ , and $(\\tilde{H}_{2},-\\lVert\\tilde{H}_{2}\\rVert_{1})$ for all $\\tilde{H}_{2}(z)=H_{2}(z)\\mathbb{1}\\{z\\ge$ $c\\}$ where $H_{2}\\in L_{+}^{1}(\\mathbb{R})$ . By taking the convex cone defined from the above inequalities, we get the following set ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{5_{+}=\\Big\\{\\left.\\left(p-\\tilde{h}+\\tilde{H}_{1}+\\tilde{H}_{2},\\lVert\\tilde{h}\\rVert_{1}-\\lVert\\tilde{H}_{1}+\\tilde{H}_{2}\\rVert_{1}\\right)\\ \\Big|\\ p\\in\\mathcal{P}_{n},\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\left.\\tilde{h}(z)=h(z)\\mathbb{1}\\{z\\ge c\\}\\mathrm{~for~}h\\in L_{+}^{1}(\\mathbb{R}),}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\tilde{H}_{1}(z)=H_{1}(z)\\mathbb{1}\\{z<c\\}\\mathrm{~for~}H_{1}\\in L^{1}(\\mathbb{R}),}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\tilde{H}_{2}(z)=H_{2}(z)\\mathbb{1}\\{z\\ge c\\}\\mathrm{~for~}H_{2}\\in L_{+}^{1}(\\mathbb{R})\\Big\\}~.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We first show that $S_{+}$ is closed. To do so, we prove that $S_{+}$ is closed under limits. First, we assume, in order to reach a contradiction, there is a sequence $(p_{i},\\dot{h_{i}},\\tilde{H}_{1,i},\\tilde{H}_{2,i})$ so that ", "page_idx": 21}, {"type": "equation", "text": "$$\n(F_{i},\\lambda_{i})=\\left(p_{i}-\\tilde{h}_{i}+\\tilde{H}_{1,i}+\\tilde{H}_{2,i},\\|\\tilde{h}_{i}\\|_{1}-\\|\\tilde{H}_{1,i}+\\tilde{H}_{2,i}\\|_{1}\\right)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "has $F_{i}$ converges to $F_{\\mathrm{lim}}$ with respect to $L_{1}$ -norm and $\\lambda_{i}$ converges to $\\lambda_{\\mathrm{lim}}$ . We claim then that $(F_{\\mathrm{lim}},\\lambda_{\\mathrm{lim}})\\in S_{+}$ . First, notice that $\\|p_{i}\\|_{1}\\,\\le\\,\\|F_{i}\\|_{1}+\\|\\tilde{h}_{i}\\|_{1}+\\|\\tilde{H}_{1,i}\\|_{1}+\\|\\tilde{H}_{2,i}\\|_{1}\\,\\le\\,\\|F_{i}\\|_{1}+3$ . Therefore, there must be a $k$ such that for any $i\\geq k$ , $\\|p_{i}\\|_{1}\\leq\\|F_{\\mathrm{lim}}\\|_{1}+4$ . ", "page_idx": 21}, {"type": "text", "text": "Then, since an $L_{1}$ ball in ${\\mathcal{P}}_{n}$ with radius $\\|F_{\\mathrm{lim}}\\|_{1}+4$ is compact, there must be a subsequence of $(p_{i},h_{i},\\tilde{H}_{1,i},\\tilde{H}_{2,i})$ such that $p_{i}$ converges to $p_{\\mathrm{lim}}$ for some $p_{\\mathrm{lim}}\\,\\in\\,\\mathcal{P}_{n}$ . Notice that due to the positivity of $\\tilde{h}_{i}$ and ${\\tilde{H}}_{2,i}$ , it must be $\\tilde{h}_{i}=(F_{i}-p_{i})^{-}\\mathbb{1}(z\\ge c),\\tilde{H}_{2,i}=(F_{i}-p_{i})^{+}\\mathbb{1}(z\\ge c)$ and $\\tilde{H}_{1,i}=(F_{i}-p_{i})\\mathbb{1}(z<c)$ . Therefore, $\\tilde{h}_{i}$ , $\\tilde{H}_{1,i}$ and ${\\tilde{H}}_{2,i}$ converge to $\\tilde{h}_{\\mathrm{lim}}=(F_{\\mathrm{lim}}-p_{\\mathrm{lim}})^{-}\\mathbb{1}(z\\geq c)$ , $\\tilde{H}_{2,\\mathrm{lim}}=(F_{\\mathrm{lim}}-p_{\\mathrm{lim}})^{+}\\mathbb{1}(z\\geq c)$ and $\\tilde{H}_{1,\\mathrm{lim}}=(F_{\\mathrm{lim}}-p_{\\mathrm{lim}})\\mathbb{1}(z<c)$ respectively. Then by taking $(p_{\\mathrm{lim}},h_{\\mathrm{lim}},\\tilde{H}_{1,\\mathrm{lim}},\\tilde{H}_{2,\\mathrm{lim}})$ , one can see that $(F_{\\mathrm{lim}},\\lambda_{\\mathrm{lim}})\\in S_{+}$ . ", "page_idx": 21}, {"type": "text", "text": "It only remains to verify that if there exists a polynomial $p$ of degree $n$ such that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{z\\sim\\sim\\mathcal{N}_{1}}{\\mathbf{E}}[|p(z)|\\mathbb{1}(z\\leq c)]<\\underset{z\\sim\\mathcal{N}_{1}}{\\mathbf{E}}[p(z)\\mathbb{1}(z\\geq c)]\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "then $(0,1)\\in S_{+}$ . Let $p$ be the polynomial satisfying the above. We can simply take ${\\tilde{h}}_{i}=(p_{i})^{+}\\mathbb{1}(z\\geq$ $c$ ), $\\tilde{H}_{2,i}=(p_{i})^{-}\\mathbb{1}(z\\geq c)$ and $\\tilde{H}_{1,i}=-p_{i}\\mathbb{1}(z<c)$ . This gives ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Big(p_{i}-\\tilde{h}_{i}+\\tilde{H}_{1,i}+\\tilde{H}_{2,i},\\|\\tilde{h}_{i}\\|_{1}-\\|\\tilde{H}_{1,i}+\\tilde{H}_{2,i}\\|_{1}\\Big)=(0,k)\\in{\\cal S}_{+}\\ ,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{k=\\|(p_{i})^{+}\\mathbb{1}(z\\geq c)\\|_{1}-\\|(p_{i})^{-}\\mathbb{1}(z\\geq c)-p_{i}\\mathbb{1}(z<c)\\|_{1}}\\\\ &{\\quad=\\|(p_{i})^{+}\\mathbb{1}(z\\geq c)\\|_{1}-\\|(p_{i})^{-}\\mathbb{1}(z\\geq c)\\|_{1}-\\|p_{i}\\mathbb{1}(z<c)\\|_{1}}\\\\ &{\\quad=\\underset{z\\sim\\mathcal{N}_{1}}{\\mathbf{E}}[p(z)\\mathbb{1}(z\\geq c)]-\\underset{z\\sim\\mathcal{N}_{1}}{\\mathbf{E}}[|p(z)|\\mathbb{1}(z\\leq c)]>0}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Given Claim C.2, to prove Lemma 3.3, it only remains to show that there does not exist a polynomial $p$ of degree at most $n$ such that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{z\\sim\\sim\\mathcal{N}_{1}}{\\mathbf{E}}[|p(z)|\\mathbb{1}(z\\leq c)]<\\underset{z\\sim\\mathcal{N}_{1}}{\\mathbf{E}}[p(z)\\mathbb{1}(z\\geq c)]\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "First, the above condition implies that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{z\\sim\\sim\\mathcal{N}_{1}}{\\mathbf{E}}[|p(z)|\\mathbb{1}(z\\leq c)]<\\underset{z\\sim\\mathcal{N}_{1}}{\\mathbf{E}}[|p(z)|\\mathbb{1}(z\\geq c)]\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "thus $\\mathbf{E}_{z\\sim\\mathcal{N}_{1}}[|p(z)|]<2\\,\\mathbf{E}_{z\\sim\\mathcal{N}_{1}}[|p(z)|\\mathbb{1}(z\\geq c)]$ . Using the Cauchy\u2013Schwarz inequality, we get ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\underset{z\\sim\\mathcal{N}_{1}}{\\mathbf{E}}[|p(z)|]<2\\underset{z\\sim\\mathcal{N}_{1}}{\\mathbf{E}}[|p(z)|^{2}]^{1/2}\\left(\\underset{z\\sim\\mathcal{N}_{1}}{\\mathbf{Pr}}[z\\geq c]\\right)^{1/2}\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We then give the following claim. ", "page_idx": 22}, {"type": "text", "text": "Claim C.4. Let $p:\\mathbb{R}\\mapsto\\mathbb{R}$ be any polynomial of degree at most $n$ . Then, it holds that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\underset{z\\sim\\mathcal{N}_{1}}{\\mathbf{E}}[|p(z)|^{2}]^{1/2}\\leq3^{n}\\underset{z\\sim\\mathcal{N}_{1}}{\\mathbf{E}}[|p(z)|]\\;.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof for Claim C.4. The proof is based on the following well-known fact: for any function $g:\\mathbb{R}\\mapsto$ $\\mathbb{R}$ , it holds that (from Holder\u2019s inequality) ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\underset{z\\sim N_{1}}{\\mathbf{E}}[|g(z)|^{2}]=\\underset{z\\sim N_{1}}{\\mathbf{E}}[|g(z)|^{2/3}|g(z)|^{4/3}]\\leq\\underset{z\\sim N_{1}}{\\mathbf{E}}[|g(z)|]^{2/3}\\underset{z\\sim N_{1}}{\\mathbf{E}}[|g(z)|^{4}]^{1/3}\\ ,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "thus ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\underset{z\\sim\\mathcal{N}_{1}}{\\mathbf{E}}[|g(z)|^{2}]^{1/2}\\leq\\underset{z\\sim\\mathcal{N}_{1}}{\\mathbf{E}}[|g(z)|]^{1/3}\\underset{z\\sim\\mathcal{N}_{1}}{\\mathbf{E}}[|g(z)|^{4}]^{1/6}~.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We then use Fact C.5 (Gaussian Hypercontractivity) to bound the term $\\mathbf{E}_{z\\sim\\mathcal{N}_{1}}[|g(z)|^{4}]^{1/6}$ . ", "page_idx": 22}, {"type": "text", "text": "Fact C.5 (Gaussian Hypercontractivity). Let $p:\\mathbb{R}\\mapsto\\mathbb{R}$ be any polynomial of degree at most $l$ . Then, for $q\\geq2$ , it holds ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\underset{x\\sim\\mathcal{N}_{1}}{\\mathbf{E}}[|p(x)|^{q}]\\leq(q-1)^{q l/2}\\left(\\underset{x\\sim\\mathcal{N}_{1}}{\\mathbf{E}}[p^{2}(x)]\\right)^{q/2}\\ .\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Now we apply Fact C.5 and choose $\\textit{l}=\\textit{n}$ and $q\\ =\\ 4$ . This implies $\\mathbf{E}_{z\\sim\\mathcal{N}_{1}}[|p(z)|^{4}]\\ \\leq$ $3^{2n}\\,{\\bf E}_{z\\sim\\mathcal{N}_{1}}[|p(z)|^{2}]^{2}$ , which is $\\mathbf{E}_{z\\sim\\mathcal{N}_{1}}[|p(z)|^{4}]^{1/6}\\leq3^{n/3}\\,\\mathbf{E}_{z\\sim\\mathcal{N}_{1}}[|p(z)|^{2}]^{1/3}$ . Plugging it into Equation (4), we get that for any polynomial $p$ of at most degree- ${\\cdot n}$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\underset{z\\sim\\mathcal{N}_{1}}{\\mathbf{E}}[|p(z)|^{2}]^{1/2}\\leq3^{n/3}\\underset{z\\sim\\mathcal{N}_{1}}{\\mathbf{E}}[|p(z)|]^{1/3}\\underset{z\\sim\\mathcal{N}_{1}}{\\mathbf{E}}[|p(z)|^{2}]^{1/3}~.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This implies ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\underset{z\\sim\\mathcal{N}_{1}}{\\mathbf{E}}[|p(z)|^{2}]^{1/2}\\leq3^{n}\\underset{z\\sim\\mathcal{N}_{1}}{\\mathbf{E}}[|p(z)|]\\;.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Applying Claim C.4 to Equation (3), we get ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\underset{z\\sim\\mathcal{N}_{1}}{\\mathbf{E}}[|p(z)|]<2\\cdot3^{n}\\underset{z\\sim\\mathcal{N}_{1}}{\\mathbf{E}}[|p(z)|]\\left(\\underset{z\\sim\\mathcal{N}_{1}}{\\mathbf{Pr}}[z\\geq c]\\right)^{1/2}\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "From our choice of the parameter $c$ , we have that ${\\bf P r}_{z\\sim\\mathcal{N}_{1}}[z\\geq c]\\leq3^{-2n}/4$ , thus ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbf{\\underset{\\boldsymbol{z}\\sim\\mathcal{N}_{1}}{E}}[|p(\\boldsymbol{z})|]<\\mathbf{\\underset{\\boldsymbol{z}\\sim\\mathcal{N}_{1}}{E}}[|p(\\boldsymbol{z})|]\\;,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "contradiction. Therefore, such a polynomial $p$ cannot exist. Thus, a function $g$ satisfying the requirements in the body of the lemma exists. ", "page_idx": 22}, {"type": "text", "text": "C.2 Proof of Lemma 3.4 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We let $D$ be the unique distribution such that $z\\sim\\mathcal{N}_{1}$ and ${\\bf E}_{(z,y)\\sim D}[y|z=z^{\\prime}]=g(z^{\\prime})$ where $g$ is the function from Lemma 3.3. We claim that $D$ satisfies the conditions in this lemma\u2019s statement. Condition (i) is immediate from the definition of $D$ . Then Condition (ii) is immediately implied from Condition (a) in Lemma 3.3. ", "page_idx": 23}, {"type": "text", "text": "For Condition (iii), Condition (b) in Lemma 3.3 implies $\\mathbf{E}_{z\\sim\\mathcal{N}_{1}}[g(z)]=0$ which immediately implies $\\mathbf{E}_{(z,y)\\sim D}[y]=0$ . To show that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\underset{(z,y)\\sim D}{\\mathbf{E}}[z^{k}]=\\underset{(z,y)\\sim D}{\\mathbf{E}}[z^{k}\\mid y=1]=\\underset{(z,y)\\sim D}{\\mathbf{E}}[z^{k}\\mid y=-1]\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for all $k\\in[n]$ , notice that for all $k\\in[n]$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\underset{(z,y)\\sim D}{\\mathbf{E}}[z^{k}]=\\underset{(z,y)\\sim D}{\\mathbf{Pr}}[y=1]\\underset{(z,y)\\sim D}{\\mathbf{E}}[z^{k}\\mid y=1]+\\underset{(z,y)\\sim D}{\\mathbf{Pr}}[y=-1]\\underset{(z,y)\\sim D}{\\mathbf{E}}[z^{k}\\mid y=-1]\\,.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Moreover, from Condition (ii) of Lemma 3.3, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbf{E}_{\\quad\\scriptstyle\\mathbf{y}\\sim D}[g(z)z^{k}]=\\mathbf{\\Phi}_{(z,y)\\sim D}[y z^{k}]}}\\\\ &{}&{=\\mathbf{\\Phi}_{(z,y)\\sim D}[y=1]\\mathbf{\\Phi}_{(z,y)\\sim D}[z^{k}\\mid y=1]-\\mathbf{\\Phi}_{(z,y)\\sim D}[y=-1]\\mathbf{\\Phi}_{(z,y)\\sim D}[z^{k}\\mid y=-1]}\\\\ &{}&{=0\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Taking the difference between Equation (5) and Equation (6) gives ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\underset{(z,y)\\sim D}{\\mathbf{E}}[z^{k}]=2\\underset{(z,y)\\sim D}{\\mathbf{Pr}}[y=-1]\\underset{(z,y)\\sim D}{\\mathbf{E}}[z^{k}\\mid y=-1]\\;.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Plugging in that $\\mathbf{Pr}_{(z,y)\\sim D}[y=-1]=1/2$ , which follows from $\\mathbf{E}_{(z,y)\\sim D}[y]=0$ and $y$ is supported on $\\{\\pm1\\}$ , we get ", "page_idx": 23}, {"type": "equation", "text": "$$\n{\\underset{(z,y)\\sim D}{\\mathbf{E}}}[z^{k}]={\\underset{(z,y)\\sim D}{\\mathbf{E}}}[z^{k}\\mid y=-1]\\;.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\underset{(z,y)\\sim D}{\\mathbf{E}}[z^{k}]=\\underset{(z,y)\\sim D}{\\mathbf{E}}[z^{k}\\mid y=1]\\underset{(z,y)\\sim D}{\\mathbf{Pr}}[y=1]+\\underset{(z,y)\\sim D}{\\mathbf{E}}[z^{k}\\mid y=-1]\\underset{(z,y)\\sim D}{\\mathbf{Pr}}[y=-1]\\int\\underset{(z,y)\\sim D}{\\mathbf{Pr}}[y=-1]\\left.\\frac{\\partial}{\\partial z}\\right\\Vert\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "therefore, ", "page_idx": 23}, {"type": "equation", "text": "$$\n{\\underset{(z,y)\\sim D}{\\mathbf{E}}}[z^{k}]={\\underset{(z,y)\\sim D}{\\mathbf{E}}}[z^{k}\\mid y=-1]\\;.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Thus, Condition (iii) holds. ", "page_idx": 23}, {"type": "text", "text": "Now, we show the fourth condition. We show the following claim. ", "page_idx": 23}, {"type": "text", "text": "Claim C.6. It holds that $\\chi^{2}(D_{+},\\mathcal{N}_{1})=O(1)$ . ", "page_idx": 23}, {"type": "text", "text": "Proof. ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\chi^{2}(D_{+},N_{1})=\\int_{\\mathbb{R}}\\frac{P_{D_{+}}(z)^{2}}{P_{N_{1}}(z)}d z-1}\\\\ &{\\qquad\\qquad=\\int_{\\mathbb{R}}P_{D_{+}}(z)\\frac{P_{D_{+}}(z)}{P_{N_{1}}(z)}d z-1}\\\\ &{\\qquad\\qquad\\leq\\int_{\\mathbb{R}}P_{D_{+}}(z)/\\underbrace{\\mathbf{P}_{\\mathbf{P}}}_{(z,y)\\sim D_{f}}[y=1]d z-1}\\\\ &{\\qquad\\qquad=\\underset{(z,y)\\sim D}{\\mathbf{P}\\mathbf{r}\\mathbf{\\cdot}\\mathbf{\\sigma}}[y=1]^{-1}\\int_{\\mathbb{R}}P_{D_{+}}(z)d z-1}\\\\ &{\\qquad\\qquad=\\underset{(z,y)\\sim D}{\\mathbf{Pr}\\mathbf{\\cdot}\\mathbf{\\sigma}}[y=1]^{-1}-1=O(1)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the inequality follows from the fact that ", "page_idx": 23}, {"type": "equation", "text": "$$\nP_{N_{1}}(z)=P_{D_{+}}(z)\\operatorname*{Pr}_{(z,y)\\sim D}[y=1]+P_{D_{-}}(z)\\operatorname*{Pr}_{(z,y)\\sim D}[y=-1]\\ge P_{D_{+}}(z)\\operatorname*{Pr}_{(z,y)\\sim D}[y=1]\\;,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which implies $P_{D_{+}}(z)/P_{N_{1}}(z)\\,\\leq\\,1/\\,\\mathbf{Pr}_{(z,y)\\sim D}[y\\,=\\,1]$ . The same holds for $\\chi^{2}(D_{-},\\sqrt{_1})$ . This completes the proof of Claim C.6. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "The proof of $\\chi^{2}(D_{-},\\mathcal{N}_{1})=O(1)$ is similar to Claim C.6. This completes the proof of Lemma 3.4. ", "page_idx": 23}, {"type": "text", "text": "C.3 Construction of the Alternative Hypothesis Distribution Set $\\mathcal{D}$ ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Lemma C.7. For any sufficiently small $\\alpha>0$ , there exists a distribution family ${\\mathcal{D}}=\\{{\\boldsymbol{D}}_{\\mathbf{v}}:{\\mathbf{v}}\\in V\\}$ supported on $\\mathbb{R}^{d}\\times\\{\\pm1\\}$ where $|\\mathcal{D}|=2^{d^{\\Omega(1)}}$ satisfying the following: ", "page_idx": 24}, {"type": "text", "text": "(i) For any $\\cal D_{v}\\in\\mathcal{D}$ and $(\\mathbf{x},y)\\sim D_{\\mathbf{v}}$ , the marginal $D_{\\mathbf{x}}=\\mathcal{N}_{d}$ ; ", "page_idx": 24}, {"type": "text", "text": "(ii) For any $D_{\\mathbf{v}}\\ \\in\\ \\mathcal{D}$ and $(\\mathbf{x},y)\\,\\sim\\,D_{\\mathbf{v}}$ , $\\mathbf{E}_{(\\mathbf{x},y)\\sim D_{\\mathbf{v}}}[y|\\mathbf{x}\\;=\\;\\mathbf{x}^{\\prime}]\\;=\\;1$ for all $\\mathbf{x^{\\prime}}\\,\\in\\,\\mathbb{R}^{d}$ such that $\\langle\\mathbf{v},\\mathbf{x}^{\\prime}\\rangle\\geq\\Phi^{-1}(1-\\alpha)$ ; ", "page_idx": 24}, {"type": "text", "text": "(iii) Let $D_{\\mathrm{null}}$ be the joint distribution of $(\\mathbf{x},y)$ such that $\\textbf{x}\\sim\\mathcal{N}_{d}$ and $y\\,=\\,1$ with probability $1/2$ independent of $\\mathbf{x}$ . Then for any $D_{\\mathbf{u}},D_{\\mathbf{v}}\\,\\in\\,\\mathcal{D}$ , $\\chi_{D_{\\mathrm{null}}}(D_{\\mathbf{u}},D_{\\mathbf{v}})\\,=\\,O(1)\\,\\,i f\\,{\\mathbf{u}}\\,=\\,\\mathbf{v}$ and $\\chi_{D_{\\mathrm{null}}}(D_{\\mathbf{u}},D_{\\mathbf{v}})=d^{-\\Omega(\\log1/\\alpha)}\\,\\,i f{\\mathbf{u}}\\ne\\mathbf{v}.$ ", "page_idx": 24}, {"type": "text", "text": "Here, we give two lemmas from [DKPZ21] to construct a large set of distributions on $\\mathbb{R}^{d}\\times\\{\\pm1\\}$ with small correlations. For a matrix $\\mathbf{A}\\in\\mathbb{R}^{m\\times n}$ , we use $\\lVert\\mathbf{A}\\rVert_{2}$ to denote the spectral norm. ", "page_idx": 24}, {"type": "text", "text": "Lemma C.8 (Correlation Lemma: Lemma 2.3 from [DKPZ21]). Let $g:\\mathbb{R}^{m}\\mapsto\\mathbb{R}$ and $\\mathbf{U},\\mathbf{V}\\in$ $\\mathbb{R}^{m\\times d}$ with $m\\leq d$ be linear maps such that $\\mathbf{U}\\mathbf{U}^{T}=\\mathbf{V}\\mathbf{V}^{T}=\\mathbf{I}$ where I is the $m\\times m$ identity matrix. Then, we have that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\underset{\\mathbf{x}\\sim\\mathcal{N}_{d}}{\\mathbf{E}}[g(\\mathbf{U}\\mathbf{x})g(\\mathbf{V}\\mathbf{x})]\\le\\sum_{t=0}^{\\infty}\\|\\mathbf{U}\\mathbf{V}^{T}\\|_{2}^{t}\\underset{\\mathbf{x}\\sim\\mathcal{N}_{m}}{\\mathbf{E}}[(g^{[t]}(\\mathbf{x}))^{2}]\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $g^{[t]}$ denote the degree- $\\cdot t$ Hermite part of $g$ . ", "page_idx": 24}, {"type": "text", "text": "Note that we will use Lemma C.8 with $m=1$ and $n=d$ so that the linear maps $\\mathbf{U}$ and $\\mathbf{V}$ become vectors. We then introduce the following well-known fact, which states that there are exponentially many near-orthogonal vectors in $\\mathbb{R}^{d}$ . ", "page_idx": 24}, {"type": "text", "text": "Fact C.9 (Near-orthogonal Vectors: Fact 2.6 from [DKPZ21]). For any $0<c<1/2$ , there exists $a$ set $V\\subseteq\\mathbb{S}^{d-1}$ such that $|V|=2^{d^{\\Omega(c)}}$ and for any ${\\mathbf{u}},{\\mathbf{v}}\\in V\\left\\langle{\\mathbf{u}},{\\mathbf{v}}\\right\\rangle\\leq d^{c-1/2}$ . ", "page_idx": 24}, {"type": "text", "text": "Proof of Lemma C.7. We define $\\mathcal{D}$ in the following manner. We let $D^{\\prime}$ be the distribution on $\\mathbb{R}\\!\\times\\!\\{\\pm1\\}$ in Lemma 3.4 for $n\\,=\\,c\\log(1/\\alpha)$ where $c$ is a sufficiently small universal constant such that $3^{-2n}/4\\geq\\alpha$ . We take $V$ to be the set of vectors in Fact C.9 with $c=1/4$ . Then we consider the set of distributions ${\\mathcal{D}}=\\{{\\boldsymbol{D}}_{\\mathbf{v}}:{\\mathbf{v}}\\in V\\}$ where $D_{\\mathbf{v}}$ is the unique distribution on $\\mathbb{R}^{d}\\times\\{\\pm1\\}$ generated in the following way. We first sample $(z,y)\\sim D^{\\prime}$ and $\\mathbf{x}_{\\bot}\\sim\\mathcal{N}_{d-1}$ . Then let $\\mathbf{B}_{\\perp}\\in\\mathbb{R}^{d\\times(d-1)}$ be a matrix whose columns form an (arbitrary) orthonormal basis for the orthogonal complement of $\\mathbf{v}$ . Let $\\mathbf{x}=\\mathbf{B}_{\\perp}\\mathbf{x}_{\\perp}+z\\mathbf{v}$ and we define the distribution $D_{\\mathbf{v}}=D_{(\\mathbf{x},y)}$ . This effectively embeds the distribution $D^{\\prime}$ along the hidden direction $\\mathbf{v}$ in $D_{\\mathbf{v}}$ . ", "page_idx": 24}, {"type": "text", "text": "To establish Condition (i) in this lemma, the definition of $\\mathbf{x}$ combined with the fact that $D_{z}=\\mathcal{N}_{1}$ (from Condition (i) in Lemma 3.4) implies the marginal distribution $D_{\\mathbf{x}}=\\mathcal{N}_{d}$ . ", "page_idx": 24}, {"type": "text", "text": "Then, for Condition (ii) in this lemma, we have that for any $\\mathbf{x}^{\\prime}\\in\\mathbb{R}^{d}$ such that $\\langle\\mathbf{v},\\mathbf{x}^{\\prime}\\rangle\\geq\\Phi^{-1}(1-\\alpha)$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{(\\mathbf{x},y)\\sim D_{\\mathbf{v}}}{\\mathbf{E}}[y|\\mathbf{x}=\\mathbf{x}^{\\prime}]=\\underset{(z,y)\\sim D^{\\prime}}{\\mathbf{E}}[y=1|z=\\langle\\mathbf{v},\\mathbf{x}^{\\prime}\\rangle]\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "from the definition of $D_{\\mathbf{v}}$ . Since $3^{-2n}/4\\geq\\alpha$ , we have $\\langle\\mathbf{v},\\mathbf{x}^{\\prime}\\rangle\\geq\\Phi^{-1}(1-\\alpha)\\geq\\Phi^{-1}(1-3^{-2n}/4)$ Therefore, using the second condition in Lemma 3.4, we get ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{(\\mathbf{x},y)\\sim D_{\\mathbf{v}}}{\\mathbf{E}}[y|\\mathbf{x}=\\mathbf{x}^{\\prime}]=\\underset{(z,y)\\sim D^{\\prime}}{\\mathbf{E}}[y|z=\\langle\\mathbf{v},\\mathbf{x}^{\\prime}\\rangle]=1\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For Condition (iii) in this lemma, notice that Condition (iii) in Lemma 3.4 implies ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\underset{(\\mathbf{x},y)\\sim D_{\\mathbf{v}}}{\\mathbf{Pr}}[y=1]=\\underset{(\\mathbf{x},y)\\sim D_{\\mathbf{v}}}{\\mathbf{Pr}}[y=-1]=1/2\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\chi_{D_{\\mathrm{null}}}(D_{\\mathbf{v}},D_{\\mathbf{v}})=\\underset{(\\mathbf{x},y)\\sim D_{\\mathbf{v}}}{\\mathbf{Pr}}[y=1]\\chi^{2}(D_{\\mathbf{v}}^{+},\\mathcal{N}_{d})+\\underset{(\\mathbf{x},y)\\sim D_{\\mathbf{v}}}{\\mathbf{Pr}}[y=-1]\\chi^{2}(D_{\\mathbf{v}}^{-},\\mathcal{N}_{d})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=\\cfrac{1}{2}\\chi^{2}(D_{\\mathbf{v}}^{+},\\mathcal{N}_{d})+\\cfrac{1}{2}\\chi^{2}(D_{\\mathbf{v}}^{-},\\mathcal{N}_{d})\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We show that the first term $\\chi^{2}(D_{\\mathbf{v}}^{+},\\mathcal{N}_{d})=O(1)$ . $\\chi^{2}(D_{\\mathbf{v}}^{-},\\mathcal{N}_{d})=O(1)$ can be shown in the exact same way. Notice that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\chi^{2}(D_{\\mathbf{v}}^{+},N_{d})=\\displaystyle\\int_{\\mathbb{R}^{d}}\\frac{P_{D_{\\mathbf{v}}^{+}}(\\mathbf{u})^{2}}{P_{N_{d}}(\\mathbf{u})}d\\mathbf{u}-1}\\\\ &{\\!=\\!\\int_{\\mathbb{R}}\\int_{\\mathbb{R}^{d-1}}\\frac{P_{D_{\\mathbf{v}}^{+}}(\\mathbf{B}_{\\bot}\\mathbf{u}+z\\mathbf{v})^{2}}{P_{N_{d}}(\\mathbf{B}_{\\bot}\\mathbf{u}+z\\mathbf{v})}d\\mathbf{u}d z-1}\\\\ &{\\!=\\!\\int_{\\mathbb{R}}\\int_{\\mathbb{R}^{d-1}}\\frac{P_{N_{d-1}}(\\mathbf{u})^{2}P_{D^{+}}(z)^{2}}{P_{N_{d-1}}(\\mathbf{u})P_{N_{1}}(z)}d\\mathbf{u}d z-1}\\\\ &{\\!=\\!\\int_{\\mathbb{R}^{d-1}}P_{N_{d-1}}(\\mathbf{u})\\!\\!\\int_{\\mathbb{R}}\\frac{P_{D^{+}}(z)^{2}}{P_{N_{1}}(z)}d z-1}\\\\ &{\\!=\\!\\chi^{2}(D^{+},N_{1})=\\mathcal{O}(1)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the last equality follows from Condition (iv) of Lemma 3.4. Therefore ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\chi_{D_{\\mathrm{null}}}(D_{\\mathbf{v}},D_{\\mathbf{v}})=\\frac{1}{2}\\chi^{2}(D_{\\mathbf{v}}^{+},\\mathcal{N}_{d})+\\frac{1}{2}\\chi^{2}(D_{\\mathbf{v}}^{-},\\mathcal{N}_{d})=O(1)\\;.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "For $D_{\\mathbf{u}},D_{\\mathbf{v}}\\in\\mathcal{D}$ where $\\mathbf u\\neq\\mathbf v$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\chi_{D_{\\mathrm{null}}}(D_{\\mathbf{u}},D_{\\mathbf{v}})=\\mathbf{Pr}[y=1]\\chi_{N_{d}}(D_{\\mathbf{u}}^{+},D_{\\mathbf{v}}^{+})+\\mathbf{Pr}[y=-1]\\chi_{N_{d}}(D_{\\mathbf{u}}^{-},D_{\\mathbf{v}}^{-})}\\\\ &{\\qquad\\qquad\\qquad=\\!\\frac{1}{2}\\chi_{N_{d}}(D_{\\mathbf{u}}^{+},D_{\\mathbf{v}}^{+})+\\frac{1}{2}\\chi_{N_{d}}(D_{\\mathbf{u}}^{-},D_{\\mathbf{v}}^{-})\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We show that $\\begin{array}{r}{\\chi_{N_{d}}(D_{\\mathbf{u}}^{+},D_{\\mathbf{v}}^{+})=d^{-\\Omega(\\log\\frac{1}{\\alpha})}}\\end{array}$ . $\\begin{array}{r}{\\chi_{\\mathcal{N}_{d}}(D_{\\mathbf{u}}^{-},D_{\\mathbf{v}}^{-})=d^{-\\Omega(\\log\\frac{1}{\\alpha})}}\\end{array}$ can be shown in the exact same way. Notice that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\chi_{N_{d}}(D_{\\mathbf{u}}^{+},D_{\\mathbf{v}}^{+})=\\displaystyle\\int_{\\mathbb{R}^{d}}\\frac{P_{D_{u}^{+}}(\\mathbf{w})P_{D_{v}^{+}}(\\mathbf{w})}{P_{N_{d}}(\\mathbf{w})}d\\mathbf{w}-1}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\displaystyle\\int_{\\mathbb{R}^{d}}\\frac{P_{D_{u}^{+}}(\\mathbf{proj}_{\\bot\\mathbf{u}}(\\mathbf{w})+\\mathbf{proj}_{\\mathbf{u}}(\\mathbf{w}))P_{D_{v}^{+}}(\\mathbf{proj}_{\\bot\\mathbf{v}}(\\mathbf{w})+\\mathrm{proj}_{\\mathbf{v}}(\\mathbf{w}))}{P_{N_{d}}(\\mathbf{w})}d\\mathbf{w}-1}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\displaystyle\\int_{\\mathbb{R}^{d}}\\frac{P_{N_{d-1}}(\\mathbf{w}^{\\bot\\mathbf{u}})P_{D^{\\prime}+}(\\mathbf{w}^{\\mathbf{u}})P_{N_{d-1}}(\\mathbf{w}^{\\bot\\mathbf{v}})P_{D^{\\prime}+}(\\mathbf{w}^{\\mathbf{v}})}{P_{N_{d}}(\\mathbf{w})}d\\mathbf{w}-1}\\\\ &{\\quad\\quad\\quad\\quad=\\displaystyle\\int_{\\mathbb{R}^{d}}P_{N_{d}}(\\mathbf{w})\\,\\frac{P_{D^{\\prime}+}(\\mathbf{w}^{\\mathbf{u}})}{P_{N_{1}}(\\mathbf{w}^{\\mathbf{u}})}\\,\\frac{P_{D^{\\prime}+}(\\mathbf{w}^{\\mathbf{v}})}{P_{N_{1}}(\\mathbf{w}^{\\mathbf{v}})}d\\mathbf{w}-1\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Let $g:\\mathbb{R}\\rightarrow\\mathbb{R}$ be $g(z)\\ {\\stackrel{\\mathrm{def}}{=}}\\ P_{D^{\\prime}+}(z)/P_{\\mathcal{N}_{1}}(z)$ and apply Lemma C.8 and Condition (iii) of Lemma 3.4, we get ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\chi_{N_{d}}(D_{\\mathbf{u}}^{+},D_{\\mathbf{v}}^{+})\\leq\\left(1+\\displaystyle\\sum_{t=n+1}^{\\infty}d^{-\\Omega(t)}\\operatorname{\\bf{E}}_{z\\sim\\mathcal{N}_{1}}\\left[\\left(g^{[t]}(z)\\right)^{2}\\right]\\right)-1}\\\\ &{\\qquad\\qquad\\qquad\\leq d^{-\\Omega(n)}\\chi_{\\mathcal{N}_{1}}^{2}(D^{\\prime^{+}},\\mathcal{N}_{1})}\\\\ &{\\qquad\\qquad\\qquad=d^{-\\Omega(\\log\\frac{1}{\\alpha})}\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the last equality follows form Condition (iv) of Lemma 3.4. This completes the proof of Lemma Lemma C.7. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "C.4 Proof of Theorem C.1 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Let $\\mathcal{D}$ be the set of distributions in Lemma C.7. We also let $D_{\\mathrm{null}}$ be the joint distribution of $(\\mathbf{x},y)$ such that $\\mathbf{x}\\sim\\mathcal{N}_{d}$ and $y\\sim\\operatorname{Bern}(1/2)$ independent of $\\mathbf{x}$ . We consider the decision problem of $B(\\mathcal{D},D_{\\mathrm{null}})$ . Using Lemma A.2 with $\\gamma^{\\prime}=d^{-\\log(\\frac{1}{\\alpha})}$ , we can see that any SQ algorithm that solve B(D, Dnull) requires either queries of tolerance at most d\u2212\u2126(log \u03b11 ) or makes at least 2d\u2126(1) queries. We then reduce the decision problem $B(\\mathcal{D},D_{\\mathrm{null}})$ to reliably learning $\\alpha$ -biased LTFs with $\\epsilon<\\alpha/3$ accuracy. Let $D$ be an instance of $B(\\mathcal{D},D_{\\mathrm{null}})$ which we need to decide either $D\\,=\\,D_{\\mathrm{null}}$ or ", "page_idx": 25}, {"type": "text", "text": "$D\\,\\in\\,{\\mathcal{D}}$ . Let $A$ be an algorithm that reliably learns $\\alpha$ -biased LTFs with $\\epsilon\\,<\\,\\alpha/3$ accuracy and succeeds with $^{2/3}$ probability. We can simply give the i.i.d. samples from the distribution $D$ to algorithm $A$ . Given that $A$ succeeds and outputs a hypothesis function $h$ . If $D=D_{\\mathrm{null}}$ , then since $A$ promises $R_{+}(h,D_{\\mathrm{null}})\\,\\leq\\,\\epsilon$ . Since $D_{\\mathrm{null}}$ has $y\\,\\sim\\,\\mathrm{Bern}(1/2)$ independent of $\\mathbf{x}$ , this implies $\\mathbf{Pr}_{(\\mathbf{x},y)\\sim D}[h(\\mathbf{x})=1]\\leq\\epsilon$ . ", "page_idx": 26}, {"type": "text", "text": "Then we argue that if it is the other case i.e. $D\\in\\mathcal{D}$ , then given the algorithm succeeds, we must have $\\mathbf{Pr}_{(\\mathbf{x},y)\\sim D}[h(\\mathbf{x})=1]\\geq\\alpha-\\epsilon$ instead. We suppose $\\mathbf{Pr}_{(\\mathbf{x},y)\\sim D}[h(\\mathbf{x})=1]<\\alpha-\\epsilon$ and prove a contradiction. If $D=D_{\\mathbf{v}}\\in\\mathcal{D}$ , then by the first property in Lemma C.7, there must be a $\\alpha$ -biased LTF $c$ such that $\\mathbf{Pr}_{(\\mathbf{x},y)\\sim D}[c(\\mathbf{x})=1]\\geq\\alpha$ and $\\mathbf{Pr}_{(\\mathbf{x},y)\\sim D}[c(\\mathbf{x})=1\\land y=-1]=0$ . Combining this with the fact that ${\\bf P r}_{({\\bf x},y)\\sim D}[y=1]=1/2$ , we have $R_{-}(c;D)=\\mathbf{Pr}[c(\\mathbf{x})=-1\\wedge y=1]\\leq1/2\\!-\\!\\alpha$ . Therefore, the output hypothesis $h$ of $A$ has to satisfy ", "page_idx": 26}, {"type": "equation", "text": "$$\nR_{-}(h;D)\\leq1/2-\\alpha+\\epsilon\\;.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "However, since $\\mathbf{Pr}_{(\\mathbf{x},y)\\sim D}[h(\\mathbf{x})=1]<\\alpha-\\epsilon$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\nR_{-}(h;D)=\\operatorname*{Pr}_{({\\bf x},{\\bf y})\\sim D}[h({\\bf x})\\neq1\\land y=1]\\geq\\operatorname*{Pr}_{({\\bf x},{\\bf y})\\sim D}[y=1]-\\operatorname*{Pr}_{({\\bf x},{\\bf y})\\sim D}[h({\\bf x})=1]>1/2-\\alpha+\\epsilon\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "which contradicts the above. Thus it has to be $\\mathbf{Pr}_{(\\mathbf{x},y)\\sim D}[h(\\mathbf{x})=1]>\\alpha-\\epsilon.$ ", "page_idx": 26}, {"type": "text", "text": "If $D=D_{\\mathrm{null}}$ , then $\\mathbf{Pr}_{(\\mathbf{x},y)\\sim D}[h(\\mathbf{x})=1]\\leq\\epsilon.$ . Otherwise, we have $\\mathbf{Pr}_{(\\mathbf{x},y)\\sim D}[h(\\mathbf{x})=1]\\geq\\alpha-\\epsilon$ . Since the gap between them is at least a constant $\\alpha-2\\epsilon\\ge\\alpha/3$ and $\\mathbf{Pr}_{(\\mathbf{x},y)\\sim D}[h(\\mathbf{x})=1]$ can be estimated to inverse exponential accuracy on samples with high probability. Thus we can distinguish the two cases of $B(\\mathcal{D},\\bar{D}_{\\mathrm{null}})$ with high probability. ", "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The abstract summarizes the result provided in Theorem 1.3. The introduction describes how this contribution resolves an open problem in the literature by summarizing the motivation for the model and describing prior work\u2019s contributions. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The limitations are clearly stated in the statement of each theorem and are discussed in the introduction of the paper. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 27}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Each theorem statement provides all the assumptions and we provide a complete proof for all statements that is either in the main body of the paper or in the appendix. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper is theoretical in nature and does not include experiments. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper is theoretical in nature and does not include experiments. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 29}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper is theoretical in nature and does not include experiments. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper is theoretical in nature and does not include experiments. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper is theoretical in nature and does not include experiments. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 30}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Our research conforms in every respect with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 30}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The work is theoretical and we do not see any major or immediate implications. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The work is theoretical. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 31}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: This work does not use any assets. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 31}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 32}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: This work does not use any assets. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 32}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: This work does not involve any crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 32}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: This work does not involve research with human subjects. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}]