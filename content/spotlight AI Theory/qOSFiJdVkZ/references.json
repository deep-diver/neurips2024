{"references": [{"fullname_first_author": "Sanjeev Arora", "paper_title": "The Multiplicative Weights Update Method: a Meta-Algorithm and Applications", "publication_date": "2012-00-00", "reason": "This paper introduces the multiplicative weights update method, a fundamental algorithm in online learning that is related to the paper's Bayesian posterior update rule."}, {"fullname_first_author": "Arthur Jacot", "paper_title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks", "publication_date": "2018-00-00", "reason": "This paper introduces the Neural Tangent Kernel, a key concept in understanding the behavior of neural networks in the lazy regime, which is central to the paper's analysis of neural tangent ensembles."}, {"fullname_first_author": "James Kirkpatrick", "paper_title": "Overcoming catastrophic forgetting in neural networks", "publication_date": "2017-03-00", "reason": "This paper addresses the problem of catastrophic forgetting in neural networks, which is the main focus of the current paper, providing a foundational understanding of the issue."}, {"fullname_first_author": "Yarin Gal", "paper_title": "Dropout as a Bayesian approximation: Representing model uncertainty in deep learning", "publication_date": "2016-00-00", "reason": "This paper connects dropout to Bayesian methods, which is relevant to the current paper's approach of interpreting a single neural network as a Bayesian ensemble."}, {"fullname_first_author": "Sebastian Farquhar", "paper_title": "A unifying Bayesian view of continual learning", "publication_date": "2019-00-00", "reason": "This paper provides a Bayesian perspective on continual learning, which is highly relevant to the current paper's approach to continual learning through Bayesian ensembles."}]}