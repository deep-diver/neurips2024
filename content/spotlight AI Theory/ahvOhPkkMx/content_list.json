[{"type": "text", "text": "Zipper: Addressing Degeneracy in Algorithm-Agnostic Inference ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Geng Chen\u2217 Yinxu Jia\u2217 Guanghui Wang\u2217\u2020 Changliang $\\mathbf{Zou^{*\\dagger}}$ NITFID, School of Statistics and Data Science, LPMC, KLMDASR, and LEBPS, Nankai University gengchen.stat@gmail.com, yxjia@mail.nankai.edu.cn, ghwang.nk@gmail.com, zoucl@nankai.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The widespread use of black box prediction methods has sparked an increasing interest in algorithm/model-agnostic approaches for quantifying goodness-of-fit, with direct ties to specification testing, model selection and variable importance assessment. A commonly used framework involves defining a predictiveness criterion, applying a cross-fitting procedure to estimate the predictiveness, and utilizing the difference in estimated predictiveness between two models as the test statistic. However, even after standardization, the test statistic typically fails to converge to a non-degenerate distribution under the null hypothesis of equal goodness, leading to what is known as the degeneracy issue. To addresses this degeneracy issue, we present a simple yet effective device, Zipper. It draws inspiration from the strategy of additional splitting of testing data, but encourages an overlap between two testing data splits in predictiveness evaluation. Zipper binds together the two overlapping splits using a slider parameter that controls the proportion of overlap. Our proposed test statistic follows an asymptotically normal distribution under the null hypothesis for any fixed slider value, guaranteeing valid size control while enhancing power by effective data reuse. Finite-sample experiments demonstrate that our procedure, with a simple choice of the slider, works well across a wide range of settings. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Consider predicting response $Y\\,\\in\\,\\mathbb{R}$ from covariates $X\\,\\in\\,\\mathbb{R}^{p}$ . Due to the popularity of black box prediction methods like random forests and deep neural networks, there has been a growing interest in the so-called \u201calgorithm (or model)-agnostic\u201d inference on the goodness-of-fit (GoF) in regression [1, 2, 3, 4, 5]. This framework aims to assess the appropriateness of a given model for prediction compared to a potentially more complex (often higher-dimensional) model. A common approach involves defining a predictiveness criterion, employing either the sample-splitting or crossfitting strategy to estimate the predictiveness of the two models, and examining the difference in predictiveness. The main focus of this work is to address the issue of degeneracy encountered in predictiveness-comparison-based test statistics. ", "page_idx": 0}, {"type": "text", "text": "1.1 Goodness-of-Fit Testing via Predictiveness Comparison ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Let $P$ represent the joint distribution of $(Y,X)$ and consider a class $\\mathcal{F}$ of prediction functions that effectively map the covariates to the response. Define a criterion $\\mathbb{C}(\\tilde{f},P)$ , which quantifies the predictive capability of a prediction function $\\tilde{f}\\in\\mathcal{F}$ . Larger values of $\\mathbb{C}(\\tilde{f},P)$ indicate stronger predictive capability. The optimal prediction function within the class $\\mathcal{F}$ is determined as $f\\ \\in$ arg $\\operatorname*{max}_{\\tilde{f}\\in\\mathcal{F}}\\bar{\\mathbb{C}}(\\tilde{f},\\dot{P})$ . Therefore, $\\mathbb{C}(f,P)$ represents the highest achievable level of predictiveness within the class $\\mathcal{F}$ . Examples of $\\mathbb{C}$ include the (negative) squared loss $\\mathbb{C}(\\tilde{f},P)=-E[\\{Y-\\tilde{f}(X)\\}^{2}]$ for continuous responses and the (negative) cross-entropy loss $\\mathbb{C}(\\tilde{f},P)=E[Y\\log\\tilde{f}(X)+(1-$ $Y)\\log\\{1-{\\tilde{f}}(X)\\}]$ for binary responses, where $E$ denotes the expectation under $P$ . ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In GoF testing problems, there are typically two classes of prediction functions $\\mathcal{F}$ and $\\mathcal{F}_{S}$ , where ${\\mathcal{F}}_{S}$ is a subset of $\\mathcal{F}$ . Define a dissimilarity measure $\\psi_{S}=\\bar{\\mathbb{C}}(f,P)-\\mathbb{C}(f_{S},P)$ , which quantifies the deterioration in predictive capability resulting from constraining the model class to ${\\mathcal{F}}_{S}$ . Here, $f_{\\cal S}\\in\\arg\\operatorname*{max}_{\\tilde{f}\\in\\mathcal{F}_{\\cal S}}\\mathbb{C}(\\tilde{f},{P})$ denotes the optimal prediction function within the restricted class. Since ${\\mathcal{F}}_{S}\\subseteq{\\mathcal{F}}$ , it follows that $\\psi_{\\cal S}\\geq0$ . The GoF testing is then formulated as ", "page_idx": 1}, {"type": "equation", "text": "$$\nH_{0}:\\psi_{S}=0\\mathrm{\\versus\\}H_{1}:\\psi_{S}>0.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "The formulation of assessing a scalar predictiveness quantity is inspired by the work of Williamson et al. [5], which focuses on evaluating variable importance. Other predictiveness or risk quantities have also been explored by [6, 7, 8, 9]. Here, we highlight a few examples where the aforementioned framework can be directly applied. ", "page_idx": 1}, {"type": "text", "text": "\u2022 (Specification testing) Model specification testing aims to evaluate the adequacy of a class of postulated models, such as parametric models, say, examining whether the conditional expectation $E(Y\\mid X)=g_{\\theta}({\\bar{X}})$ holds almost surely [10], where $g_{\\theta}$ is a known function up to an unknown parameter $\\theta$ . Under the framework (1), we can consider $\\mathcal{F}$ as a generally unrestricted class, while ${\\mathcal{F}}_{S}$ represents a class of parametric models. \u2022 (Model selection) GoF testing can also be employed to identify the superior predictive model from a set of candidates [11, 12]. This situation often arises when choosing between two prediction strategies, such as an unregularized model and a regularized one. Testing $H_{0}$ is to assess whether the inclusion of a regularizer provides benefits for predictions. \u2022 (Variable importance measure) A recently popular aspect of GoF testing involves evaluating the significance of a specific group of covariates $U$ in predicting the response, where $X=\\check{(}U^{\\top},V^{\\top})^{\\top}$ . This assessment can be incorporated into (1) by defining a subset of prediction functions $\\mathcal{F}_{S}\\subseteq\\mathcal{F}$ that disregard the covariate group $U$ when making predictions. When utilizing the squared loss, $\\psi_{S}$ simplifies to the LOCO (Leave Out COvariates) variable importance measure [1, 13, 14, 5, 15, 16], which quantifies the increase in prediction error resulting from the removal of $U$ . Specifically, taking $\\psi_{S}=E[\\{Y-E(Y\\mid\\hat{V})\\}^{2}]-E[\\{Y-$ $E(Y\\mid{\\bar{X}})\\}^{2}]$ corresponds to testing for conditional mean independence [17, 18]. ", "page_idx": 1}, {"type": "text", "text": "The measure $\\psi_{S}$ possesses the advantages of being both model-free and algorithm-agnostic. It is not tied to a specific model and remains independent of any particular model fitting algorithm. This flexibility makes $\\psi_{S}$ a versatile quantity for evaluating goodness-of-fti, enabling us to leverage diverse machine learning prediction algorithms in its estimation. ", "page_idx": 1}, {"type": "text", "text": "1.2 The Degeneracy Issue ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Let $Z=(Y,X)\\sim P$ , and suppose we have collected a set of independent realizations of $Z$ as $Z_{i}\\;=\\;(Y_{i},X_{i})$ for $i\\,=\\,1,\\ldots,n$ . To effectively estimate $\\psi_{S}$ while ensuring algorithm-agnostic inference, the sample-splitting or cross-fitting has recently gained significant popularity. This approach relaxes the requirements imposed on estimation algorithms, allowing for the utilization of flexible machine learning techniques [13, 19, 2, 20, 21]. Taking sample-splitting as an example, the data is divided into a training set and a testing set. Based on the training data, estimators $f_{n}^{\\mathrm{tr}}$ and $f_{n,S}^{\\mathrm{tr}}$ are obtained for the optimal prediction functions $f$ and $f_{S}$ , respectively. The dissimilarity measure $\\psi_{S}$ can then be evaluated using the testing data, i.e., $\\psi_{n,S}=\\mathbb{C}(f_{n}^{\\mathrm{tr}},P_{n}^{\\mathrm{te}})-\\mathbb{C}(f_{n,S}^{\\mathrm{tr}},P_{n}^{\\mathrm{te}})$ , where $P_{n}^{\\mathrm{te}}$ represents the empirical distribution function of the testing data. Notably, in the context of measuring variable importance, it has been established that when $\\psi_{\\cal S}\\,>\\,0$ , the estimator $\\psi_{n,{\\cal S}}$ exhibits asymptotic linearity under certain assumptions [5]. Similar results are also applicable to the problem of comparing multiple algorithms or models [11, 12]. ", "page_idx": 1}, {"type": "text", "text": "However, the situation becomes distinct when considering the null hypothesis $H_{0}:\\psi_{\\cal S}=0$ . Recent studies have drawn significant attention to a phenomenon known as degeneracy [22, 23, 15, 18, 5]. ", "page_idx": 1}, {"type": "text", "text": "Consider the simplest scenario where there are no covariates and the objective is to test whether $\\mu\\,:=\\,E(Y)\\,=\\,0$ ; see also Example 1 in Lei [11]. In this case, we set ${\\mathcal{F}}=\\mathbb{R}$ and ${\\mathcal{F}}_{S}\\,=\\,\\{0\\}$ . Using the squared loss, we have $\\dot{\\psi}s=E(Y^{2})-\\bar{E}\\{(Y-\\mu)^{2}\\}=\\mu^{2}$ . The estimator for $\\psi_{S}$ based on sample-splitting is $\\psi_{n,S}\\,=\\,2\\bar{Y}_{n}^{\\mathrm{te}}\\bar{Y}_{n}^{\\mathrm{tr}}\\,-\\,(\\bar{Y}_{n}^{\\mathrm{tr}})^{2}$ , where $\\bar{Y}_{n}^{\\mathrm{tr}}$ and $\\bar{Y}_{n}^{\\mathrm{te}}$ denote the sample means of the training and testing data, respectively. When $\\mu\\neq0$ , $\\sqrt{n}(\\psi_{n,S}-\\mu)$ follows an asymptotic normal distribution. However, when $\\mu=0$ , $\\sqrt{n}\\psi_{n,S}={\\cal O}_{P}(n^{-1/2})$ , indicating the presence of the degeneracy phenomenon. While inference at a $n$ -rate remains feasible under degeneracy in this specific example, it is crucial to recognize that this would not hold true for intricate models and black box fitting algorithms. Williamson et al. [5] provide evidence for the occurrence of degeneracy in a general variable importance measure $\\psi_{S}$ , where the influence function becomes exactly zero under the null hypothesis. It is therefore required to address this degeneracy issue in a generic manner to perform algorithm-agnostic statistical inference. ", "page_idx": 2}, {"type": "text", "text": "1.3 Existing Solutions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "By using the fact that the influence functions of the individual components $\\mathbb{C}(f,P)$ and $\\mathbb{C}(f_{\\cal{S}},P)$ in $\\psi_{S}$ remain nondegenerate even under $H_{0}$ , Williamson et al. [5] proposed an additional data split of the testing data, where the two predictiveness functions are evaluated on two separate testing data splits. This approach ensures a nondegenerate influence function under $H_{0}$ , therefore restoring asymptotic normality. A similar approach has been independently explored by Dai et al. [23]. However, performing additional data splits may reduce the actual sample size used in the testing, leading to a substantial loss of power. Alternatively, Rinaldo et al. [13] and Dai et al. [23] introduced data perturbation methods, where independent zero-mean noises are added to the empirical influence functions. These methods also restore asymptotic normality. However, determining the appropriate amount of perturbation to achieve desirable Type I error control remains a heuristic process. Furthermore, Verdinelli and Wasserman [22] proposed expanding the standard error of the estimator to mitigate the impact of degeneracy. ", "page_idx": 2}, {"type": "text", "text": "1.4 Our Contributions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this paper, we introduce the Zipper device for algorithm-agnostic inference under the null hypothesis $H_{0}$ of equal goodness. Our approach is inspired by the method of additional splitting of testing data, as demonstrated in the works of Williamson et al. [5] and Dai et al. [23] for assessing variables with zero-importance. Instead of creating two distinct testing data splits to evaluate the discrepancy of predictiveness criteria between the expansive and restricted models, we encourage an overlap between them. The Zipper device serves to bind together the two overlapping splits, with a slider parameter $\\tau\\in[0,1)$ controlling the proportion of overlap. To ensure stable inference and accommodate versatile machine learning prediction algorithms, we incorporate a $K$ -fold cross-ftiting scheme. We will demonstrate that the proposed test statistic follows an asymptotically normal distribution $H_{0}$ for any fixed value of $\\tau$ , ensuring valid size control while providing satisfactory power enhancement. ", "page_idx": 2}, {"type": "text", "text": "1.5 Related Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "For variable importance assessments, in addition to LOCO methods within our framework, Shapley value-based measures are commonly used [24, 25, 26, 27]. These measures, which estimate the incremental predictive accuracy contributed by a specific variable across all possible covariate subsets, reveal complex inter-variable relationships but at a considerable computational expense. Furthermore, conditional randomization tests [28, 29] offer a robust alternative when covariate distributions are known or can be accurately estimated. These methods are especially beneficial in semi-supervised settings with extensive unlabeled data. Additionally, LIME [30] focuses on estimating variable importance locally for a specific instance, while LOCO methods are designed to assess global variable importance. ", "page_idx": 2}, {"type": "text", "text": "1.6 Organization ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The remainder of our paper is structured as follows. In Section 2, we introduce the Zipper device for addressing the degenerate issue, and present the asymptotic behaviors of the method. Finite-sample experiments are presented in Section 3. Section 4 concludes the paper. Proofs of theorems and additional numerical results are provided in Appendix. ", "page_idx": 2}, {"type": "text", "text": "2 Our Remedy ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "2.1 The Zipper Device ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To initiate the process, we randomly partition the data into $K$ folds, denoted as $\\mathcal{D}_{1},\\ldots,\\mathcal{D}_{K}$ , ensuring that each fold is approximately of equal size. For a given fold index $k\\in\\{1,\\ldots,K\\}$ , we construct estimators $f_{k,n}$ and $f_{k,n,s}$ for the oracle prediction functions $f$ and $f_{S}$ correspondingly, using the data excluding the fold $\\mathcal{D}_{k}$ . To activate the Zipper device, we further randomly divide $\\mathcal{D}_{k}$ into two splits, labeled as $\\mathcal{D}_{k,A}$ and $\\mathcal{D}_{k,B}$ , with approximately equal sizes, allowing for an overlap denoted as $\\mathcal{D}_{k,o}$ . Specifically, $\\mathcal{D}_{k,A}\\cup\\mathcal{D}_{k,B}=\\mathcal{D}_{k}$ and $\\mathcal{D}_{k,A}\\cap\\mathcal{D}_{k,B}=\\mathcal{D}_{k,o}$ . Let $\\mathcal{D}_{k,a}=\\mathcal{D}_{k,A}\\backslash\\mathcal{D}_{k,o}$ and $\\mathcal{D}_{k,b}=\\mathcal{D}_{k,B}\\backslash\\mathcal{D}_{k,o}$ represent the non-overlapping parts. For simplicity, we assume that $|D_{k}|=n/K$ and $|\\mathcal{D}_{k,A}|=|\\mathcal{D}_{k,B}|$ , where $|{\\mathcal{A}}|$ represents the cardinality of set $\\boldsymbol{\\mathcal{A}}$ . Let $\\tau=|\\mathcal{D}_{k,o}|/|\\mathcal{D}_{k,A}|$ represent the proportion of the overlap. Visualize the two splits $\\mathcal{D}_{k,A}$ and $\\mathcal{D}_{k,B}$ as two pieces of fabric or other materials. The term \u201cZipper\u201d is derived from the analogy of using a zipper mechanism to either separate or join them by moving the slider $\\tau$ ; see Figure 1. To evaluate the discrepancy measure $\\bar{\\psi_{S}}=\\mathbb{C}(f,\\bar{P})-\\mathbb{C}(f_{S},\\bar{P})$ based on the $k$ th fold of testing data $\\mathcal{D}_{k}$ , we denote $P_{k,n,I}$ as the empirical distribution of the data split $\\mathcal{D}_{k,I}$ , where $I\\in\\{o,a,b\\}$ . We can estimate $\\psi_{S}$ by $\\mathbb{C}_{k,n}-\\mathbb{C}_{k,n,S}$ , where ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbb{C}_{k,n}=\\tau\\mathbb{C}(f_{k,n},P_{k,n,o})+(1-\\tau)\\mathbb{C}(f_{k,n},P_{k,n,a})\\mathrm{~and~}}\\\\ &{}&{\\mathbb{C}_{k,n,S}=\\tau\\mathbb{C}(f_{k,n,S},P_{k,n,o})+(1-\\tau)\\mathbb{C}(f_{k,n,S},P_{k,n,b})}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "represent weighted aggregations of empirical predictiveness criteria corresponding to overlapping and non-overlapping parts, used for estimating $\\mathbb{C}(f,P)$ and $\\mathbb{C}(f_{\\cal S},P)$ , respectively. By employing the cross-fitting process, we obtain the final estimator of $\\psi_{S}$ , denoted as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\psi_{n,S}=K^{-1}\\sum_{k=1}^{K}(\\mathbb{C}_{k,n}-\\mathbb{C}_{k,n,S}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "which is the average over all testing folds. ", "page_idx": 3}, {"type": "text", "text": "Notably, if we fully open the Zipper, setting $\\tau\\,=\\,0$ and leaving $\\mathcal{D}_{k,o}$ empty, our approach aligns with the vanilla data splitting strategy utilized in Williamson et al. [5] and Dai et al. [23] for assessing variables with zeroimportance. Conversely, when we completely close Zipper with $\\tau=1$ , our method essentially involves evaluating the predictiveness discrepancy using the identical data split $\\mathcal{D}_{k,o}\\,=\\,\\mathcal{D}_{k,A}\\,=$ ", "page_idx": 3}, {"type": "image", "img_path": "ahvOhPkkMx/tmp/8dad25038b595c6d89827c4ee6bd05f2e151412911a8d049b9cfa814a1e699a8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 1: Illustration of the mechanism of the Zipper device based on the $k$ th fold of testing data. ", "page_idx": 3}, {"type": "text", "text": "$\\mathcal{D}_{k,B}=\\mathcal{D}_{k}$ , which is known to result in the phenomenon of degeneracy [5]. Therefore, to avoid such degeneracy, we restrict the slider parameter $\\bar{\\tau}\\in[0,1)$ ; see also Remark 2.5 below. ", "page_idx": 3}, {"type": "text", "text": "2.2 Asymptotic Linearity ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We start by investigating the asymptotic linearity of the proposed test statistic $\\psi_{n,{\\cal{S}}}$ in (2), which serves as a foundation for establishing the asymptotic distribution under the null hypothesis, as well as for analyzing the test\u2019s power. ", "page_idx": 3}, {"type": "text", "text": "Theorem 2.1 (Asymptotic linearity). If Conditions $(C I)\u2013(C4)$ in Section A hold for both tuples $(P,{\\mathcal{F}},f,f_{k,n})$ and $(\\bar{P_{,}}\\bar{\\mathcal{F}_{\\mathcal{S}}},f_{\\mathcal{S}},f_{k,n,\\bar{S}}),$ , then ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\psi_{n,{\\mathcal{S}}}-\\psi_{S}=\\frac{1}{n/(2-\\tau)}\\sum_{k=1}^{K}\\bigg[\\sum_{\\substack{i:Z_{i}\\in{\\mathcal{D}}_{k,a}}}\\phi(Z_{i})-\\sum_{\\substack{i:Z_{i}\\in{\\mathcal{D}}_{k,b}}}\\phi_{S}(Z_{i})}\\\\ &{+\\sum_{\\substack{i:Z_{i}\\in{\\mathcal{D}}_{k,a}}}\\{\\phi(Z_{i})-\\phi_{S}(Z_{i})\\}\\bigg]+o_{P}(n^{-1/2}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\phi(Z)=\\dot{\\mathbb{C}}(f,P;\\delta_{Z}-P)$ and $\\phi_{S}(Z)={\\dot{\\mathbb{C}}}(f_{S},P;\\delta_{Z}-P)$ . Here, $\\dot{\\mathbb{C}}(\\tilde{f},P;h)$ represents the G\u00e2teaux derivative of $\\tilde{P}\\mapsto\\mathbb{C}(\\tilde{f},\\tilde{P})$ at $P$ in the direction $h$ , and $\\delta_{z}$ denotes the Dirac measure at $z$ . Consequently, for any $\\tau\\in[0,1)$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\{n/(2-\\tau)\\}^{1/2}(\\psi_{n,S}-\\psi_{S})\\overset{d}{\\rightarrow}N(0,\\nu_{S,\\tau}^{2})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "as $n\\rightarrow\\infty$ , where $\\nu_{S,\\tau}^{2}\\,=\\,(1-\\tau)(\\sigma^{2}+\\sigma_{S}^{2})+\\tau\\eta_{S}^{2}$ , $\\sigma^{2}\\,=\\,E\\{\\phi^{2}(Z)\\}$ , $\\sigma_{S}^{2}\\,=\\,E\\{\\phi_{S}^{2}(Z)\\}$ , and $\\eta_{S}^{2}=E[\\{\\phi(Z)-\\phi_{S}(Z)\\}^{2}]$ . ", "page_idx": 4}, {"type": "text", "text": "Remark 2.2. The conditions in Theorem 2.1 are derived from Williamson et al. [5], which outline specific requirements concerning the convergence rate of estimators obtained from flexible black box prediction algorithms and the smoothness of the predictiveness measures. The validity of Theorem 2.1 relies on the asymptotic linear expansions of $\\mathbb{C}(\\bar{f}_{k,n},P_{k,n,I})$ and $\\mathbb{C}(f_{k,n,S},P_{k,n,I})$ for $I\\in\\{o,a,b\\}$ ; see Lemma B.1 or Theorem 2 in Williamson et al. [5]. Given the asymptotic linearity of $\\psi_{n,{\\cal{S}}}$ , we can readily obtain its asymptotic distribution by observing the independence of data in $\\cup_{k=1}^{K}{\\mathcal{D}}_{k,a}$ , $\\cup_{k=1}^{K}\\mathcal{D}_{k,b}$ and $\\cup_{k=1}^{K}{\\mathcal{D}}_{k,o}$ , as well as the facts that $|\\cup_{k=1}^{K}\\mathcal{D}_{k,a}|=|\\cup_{k=1}^{K}\\mathcal{D}_{k,b}|=(1-\\tau)\\overset{\\cdot}{n/}(\\bar{2}-\\tau)$ and $|\\cup_{k=1}^{K}\\mathcal{D}_{k,o}|=\\tau n/(2-\\tau)$ . ", "page_idx": 4}, {"type": "text", "text": "The asymptotic linearity of $\\psi_{n,{\\cal S}}$ in (3) exhibits distinct behaviors depending on the validity of $H_{0}$ . Under $H_{0}:\\psi_{S}=0$ , we have $\\phi=\\phi_{S}$ almost surely, due to $f=f_{S}$ almost surely. Consequently, the overlapping terms $\\begin{array}{r}{\\sum_{i:Z_{i}\\in{\\mathcal{D}_{k,o}}}\\{\\phi(Z_{i})-\\phi_{S}(Z_{i})\\}}\\end{array}$ for $k=1,\\ldots,K$ vanish. As a result, (3) simplifies to ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\psi_{n,S}-\\psi_{S}=\\frac{1}{n/(2-\\tau)}\\Bigg\\{\\sum_{k=1}^{K}\\sum_{i:Z_{i}\\in\\mathcal{D}_{k,a}}\\phi(Z_{i})-\\sum_{k=1}^{K}\\sum_{i:Z_{i}\\in\\mathcal{D}_{k,b}}\\phi_{S}(Z_{i})\\Bigg\\}+o_{P}(n^{-1/2}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In this case, the dominant term compares the means of two independent samples, each with a size of $(1-\\tau)n/(2-\\tau)$ . Additionally, it can be deduced that $\\nu_{S,\\tau}^{2}\\,=\\,(1-\\tau)\\dot{(}\\sigma^{2}+\\sigma_{S}^{2})$ under $H_{0}$ . Conversely, under $H_{1}:\\psi_{S}>0$ , we can reformulate (3) as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\psi_{n,S}-\\psi_{S}=\\frac{1}{n/(2-\\tau)}\\Bigg\\{\\sum_{k=1}^{K}\\sum_{i:Z_{i}\\in\\mathcal{D}_{k,A}}\\phi(Z_{i})-\\sum_{k=1}^{K}\\sum_{i:Z_{i}\\in\\mathcal{D}_{k,B}}\\phi_{S}(Z_{i})\\Bigg\\}+o_{P}(n^{-1/2}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, the leading term compares the means of two samples that have an overlap, with each sample having a size of $n/(2-\\tau)$ . ", "page_idx": 4}, {"type": "text", "text": "2.3 Null Behaviors ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To conduct the test, it is crucial to have a consistent estimator for the variance $\\nu_{S,\\tau}^{2}\\;=\\;(1\\,-\\$ $\\tau)(\\sigma^{2}+\\sigma_{S}^{2})$ under $H_{0}$ . Following the plug-in principle, we derive an estimator $\\nu_{n,S,\\tau}^{2}:=(1-$ $\\begin{array}{r}{\\tau)K^{-1}\\sum_{k=1}^{K}(\\sigma_{k,n}^{2}+\\sigma_{k,n,S}^{2})}\\end{array}$ , where for each $k\\in\\{1,\\ldots,K\\}$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sigma_{k,n}^{2}=\\frac{1}{|\\mathcal{D}_{k}|}\\displaystyle\\sum_{i:Z_{i}\\in\\mathcal{D}_{k}}\\{\\dot{\\mathbb{C}}(f_{k,n},P_{k,n};\\delta_{Z_{i}}-P_{k,n})\\}^{2},}\\\\ &{\\sigma_{k,n,S}^{2}=\\frac{1}{|\\mathcal{D}_{k}|}\\displaystyle\\sum_{i:Z_{i}\\in\\mathcal{D}_{k}}\\{\\dot{\\mathbb{C}}(f_{k,n,S},P_{k,n};\\delta_{Z_{i}}-P_{k,n})\\}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and $P_{k,n}$ represents the empirical distribution of $\\mathcal{D}_{k}$ . The consistency of this estimator is demonstrated in Proposition 2.3. ", "page_idx": 4}, {"type": "text", "text": "Proposition 2.3. If Conditions $(C4)\u2013(C5)$ in Section $A$ hold for both tuples $(P,{\\mathcal{F}},f,f_{k,n})$ and $(P,\\mathcal{F}_{S},f_{S},f_{k,n,S})$ , then, $\\nu_{n,S,\\tau}^{2}\\stackrel{p}{\\rightarrow}\\nu_{S,\\tau}^{2}$ as $n\\to\\infty$ under $H_{0}$ for any $\\tau\\in[0,1)$ . ", "page_idx": 4}, {"type": "text", "text": "Remark 2.4. Computing G\u00e2teaux derivatives $\\dot{\\mathbb{C}}$ for certain predictiveness measures can be challenging. In many cases, the predictiveness measure takes the form $\\mathbb{C}(\\tilde{f},\\tilde{P})=E_{\\tilde{P}}\\{g(Y,\\tilde{f}(X))\\}$ , where $g$ is a known function. In such situations, it has been found that the G\u00e2teaux derivative can be expressed as $\\phi_{\\tilde{f}}(z):=g(y,\\tilde{f}(x))-E\\{g(Y,\\tilde{f}(X))\\}$ ; see, for example, Appendix A in Williamson et al. [5]. Consequently, when ${\\tilde{f}}=f$ , the empirical G\u00e2teaux derivative becomes $\\dot{\\mathbb{C}}(f_{k,n},P_{k,n};\\delta_{z}-P_{k,n})=$ $\\begin{array}{r}{g(y,f_{k,n}(x))-n_{k}^{-1}\\sum_{i:Z_{i}\\in{\\mathcal D}_{k}}g(Y_{i},f_{k,n}(X_{i}))}\\end{array}$ . This formulation allows for the identification of the variance estimator $\\sigma_{k,n}^{2}$ as the sample variance of $\\{g(Y_{i},f_{k,n}(X_{i}))\\}$ , simplifying the computation. Moreover, Condition (C5) is immediately satisfied (see Section B.2.1). Examples of such function $g$ include the squared loss and cross-entropy loss. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Based on Theorem 2.1 and Proposition 2.3, utilizing Slutsky\u2019s lemma, we can conclude that the normalized test statistic ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T_{\\tau}:=\\{n/(2-\\tau)\\}^{1/2}\\psi_{n,S}/\\nu_{n,S,\\tau}\\overset{d}{\\rightarrow}N(0,1)}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "under $H_{0}$ for any $\\tau\\,\\in\\,[0,1)$ . For a prespecified significance level $\\alpha\\,\\in\\,(0,1)$ , we reject the null hypothesis if $T_{\\tau}\\,>\\,z_{1-\\alpha}$ , where $z_{\\alpha}$ denotes the $\\alpha$ quantile of $N(0,1)$ . A summary of the entire testing procedure can be found in Section C. ", "page_idx": 5}, {"type": "text", "text": "Remark 2.5. In our asymptotic analysis of the null behavior, we explicitly exclude the case of $\\tau=1$ due to the resulting degeneracy phenomenon. Specifically, under $H_{0}$ , when $\\tau=1$ , the linear leading term of (3) becomes exactly zero. Therefore, including $\\tau=1$ can introduce a distortion in the Type I error. ", "page_idx": 5}, {"type": "text", "text": "2.4 Power Analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Next, we turn our attention to the power analysis of the proposed test under the alternative hypothesis $H_{1}:\\psi_{S}>0$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 2.6 (Power approximation). Suppose the conditions stated in Theorem 2.1 and Proposition 2.3 hold. Then for any $\\tau\\in[0,1)$ , the power function $\\mathrm{Pr}(T_{\\tau}\\,>\\,z_{1-\\alpha}\\,\\mid H_{1})=G_{S,n,\\alpha}(\\tau)^{\\prime}+o(1)$ , where ", "page_idx": 5}, {"type": "equation", "text": "$$\nG_{S,n,\\alpha}(\\tau)=\\Phi\\left(-\\frac{\\nu_{S,\\tau}^{(0)}}{\\nu_{S,\\tau}}z_{1-\\alpha}+\\frac{\\{n/(2-\\tau)\\}^{1/2}\\psi_{S}}{\\nu_{S,\\tau}}\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "$\\nu_{S,\\tau}^{(0)}=\\{(1-\\tau)(\\sigma^{2}+\\sigma_{S}^{2})\\}^{1/2}$ and $\\Phi$ denotes the distribution function of $N(0,1)$ . Furthermore, if $C o v\\{\\phi(Z),\\phi_{S}(Z)\\}\\ge0,$ , then $G_{{\\cal S},n,\\alpha}(\\tau)$ increases with $\\tau$ . ", "page_idx": 5}, {"type": "text", "text": "Remark 2.7. The form of the power function can be directly derived from Theorem 2.1 and the fact that the estimator of standard deviation $\\nu_{n,S,\\tau}\\ {\\stackrel{p}{\\to}}\\ \\nu_{S,\\tau}^{(0)}$ as $n\\to\\infty$ under $H_{1}$ . Recall that $\\nu_{S,\\tau}^{2}=$ $(1-\\tau)(\\sigma^{2}+\\sigma_{S}^{2})+\\tau\\eta_{S}^{2}$ , which is provably a decreasing function of $\\tau$ when $C o v\\{\\phi(Z),\\phi_{S}(Z)\\}\\ge0$ Consequently, the approximate power function $G_{{\\cal S},n,\\alpha}(\\tau)$ increase with $\\tau$ . For more details, please refer to Section B.3. ", "page_idx": 5}, {"type": "text", "text": "Remark 2.8. The requirement $C o v\\{\\phi(Z),\\phi_{S}(Z)\\}\\,\\ge\\,0$ is relatively benign. For instance, when considering \u03c8 $s=E[\\{Y-E(Y\\mid V)\\}^{2}]-E[\\{Y-E(Y\\mid X)\\}^{2}]$ ] within the framework of evaluating variable importance, this condition is readily satisfied; refer to B.3.1. ", "page_idx": 5}, {"type": "text", "text": "Consider the \u201cunzipped\u201d version of the proposed test with $\\tau=0$ , which has been explored in the works of Williamson et al. [5] and Dai et al. [23] for assessing variable importance. According to Theorem 2.6, the approximate power function reduces to ", "page_idx": 5}, {"type": "equation", "text": "$$\nG_{S,n,\\alpha}(0)=\\Phi\\left(-z_{1-\\alpha}+\\frac{(n/2)^{1/2}\\psi_{S}}{(\\sigma^{2}+\\sigma_{S}^{2})^{1/2}}\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "aligning with the findings in Dai et al. [23]. In contrast, for the Zipper with $\\tau\\in[0,1)$ , we have ", "page_idx": 5}, {"type": "equation", "text": "$$\nG_{S,n,\\alpha}(\\tau)\\stackrel{(\\mathrm{i})}{\\geq}\\Phi\\left(-z_{1-\\alpha}+\\frac{\\{n/(2-\\tau)\\}^{1/2}\\psi_{S}}{(\\sigma^{2}+\\sigma_{S}^{2})^{1/2}}\\right)\\stackrel{(\\mathrm{ii})}{\\geq}G_{S,n,\\alpha}(0),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "douure  tmo etthhe ofda cstus rtphaast $\\nu_{S,\\tau}^{(0)}\\leq\\nu_{S,\\tau}$ l, aa snad $\\nu_{S,\\tau}\\leq(\\sigma^{2}\\!+\\!\\sigma_{S}^{2})^{1/2}$ s isf- $C o v\\{\\phi(Z),\\phi_{S}(Z)\\}\\ge0$ . roCcoendsuerqeuse tnhtlayt, correspond to $\\tau=0$ . The improved power can be attributed to two sources: the introduction of the overlap mechanism $\\tau$ (corresponding to Inequality (ii)), and the utilization of the variance estimator $\\nu_{n,S,\\tau}^{2}$ (Inequality (i)). ", "page_idx": 5}, {"type": "text", "text": "Remark 2.9. As discussed, $\\nu_{n,S,\\tau}^{2}$ is inconsistent for the limiting variance $\\nu_{S,\\tau}^{2}$ under $H_{1}$ when $0<\\tau<1$ . If the objective is to construct a valid confidence interval for the dissimilarity measure $\\psi_{S}$ , it is crucial to use a consistent variance estimator regardless of whether $H_{0}$ holds or not. This can be achieved by incorporating an additional plug-in estimator of $\\eta_{S}^{2}$ , which is a component of the asymptotic variance $\\nu_{S,\\tau}^{2}$ . For detailed construction of a valid confidence interval, please refer to Section in Appendix. ", "page_idx": 6}, {"type": "text", "text": "2.5 Efficiency-and-Degeneracy Tradeoff ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Our asymptotic analysis demonstrates that the Zipper device ensures a valid testing size for any fixed slider parameter $\\tau\\in[0,1)$ . Moreover, as the slider $\\tau$ moves away from 0, the power improves. In practical scenarios with finite sample sizes, selecting an appropriate value of $\\tau$ involves a tradeoff between efficiency and degeneracy. Opting for a larger value of $\\tau$ can indeed enhance the testing power. However, an excessively large $\\tau$ approaching 1 would result in degeneracy and potential size inflation. This occurs because the normal approximation (4) breaks down under the null hypothesis. It is worth emphasizing that using a relatively small $\\tau$ is generally safer and yields improved power compared to the vanilla splitting-based strategies with $\\tau=0$ . ", "page_idx": 6}, {"type": "text", "text": "To achieve better power while maintaining a reliable size, we propose a simple approach for selecting $\\tau$ . By (4), the asymptotic normality relies on comparing means from two independent samples $\\cup_{k=1}^{K}\\bar{\\mathcal{D}}_{k,a}$ and $\\cup_{k=1}^{\\check{K}}\\bar{\\mathcal{D}}_{k,b}$ , each with a size of $(1-\\bar{\\tau})n\\bar{/}(2-\\tau)$ . To ensure a favorable normal approximation, we can choose the sample size $(1-\\tau)n/(2-\\tau)$ such that it meets a predetermined \u201clarge\" sample size, such as $n_{0}=30$ or 50. Say, we can specify $\\tau=\\tau_{0}:=(n-2n_{0})/(n-n_{0})$ . In the case of very large $n$ , a truncation may be needed to safeguard against degeneracy. For example, we can set $\\tau=\\operatorname*{min}\\{\\tau_{0},0.9\\}$ . Our numerical experiments show that this selection of $\\tau$ achieves satisfactory performances across a wide range of scenarios. For more details on the impact of $\\tau$ , please refer to Section E.1. ", "page_idx": 6}, {"type": "text", "text": "3 Finite-Sample Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "3.1 Synthetic Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "3.1.1 Variable Importance Assessment ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "For illustration, we conduct a simulation study to evaluate the performance of the proposed Zipper method in assessing variables with zero-importance, an area of active research. We compare the empirical size and power of Zipper against several benchmark procedures. Firstly, we consider Algorithm 3 proposed in Williamson et al. [5] (referred to as WGSC-3) and the two-split test in Dai et al. [23] (DSP-Split). Both procedures involve an additional splitting of the testing data and can be seen as approximate counterparts to the proposed Zipper test with $\\tau=0$ . Another benchmark procedure is Algorithm 2 from Williamson et al. [5] (WGSC-2), which can be viewed as a rough equivalent to Zipper with $\\tau=1$ . Additionally, we include the data perturbation method proposed by Dai et al. [23] (DSP-Pert). For each benchmark procedure, we follow the suggestions of the respective authors to select nuisance parameters. We specify the slider parameter $\\tau=\\operatorname*{min}\\{\\tau_{0},0.9\\}$ with $n_{0}=50$ as suggested in Section 2.5. ", "page_idx": 6}, {"type": "text", "text": "We consider two models: one with a normal response $Y\\sim N(X^{\\top}\\beta,\\sigma_{Y}^{2})$ , and another with a binomial response $Y\\sim\\mathrm{binom}(1,\\mathrm{logit}(X^{\\top}\\beta))$ , where $\\mathrm{logit}(t)=1/\\{1+\\exp(-t)\\}$ . Both models assume that $\\displaystyle X\\sim N(0,\\Sigma)$ , where $\\Sigma=(0.2^{|i-j|})_{p\\times p}$ . For each model, we examine two scenarios. The first scenario is a low-dimensional setting with $p\\in\\{5,10\\}$ and $\\beta=(\\delta,\\delta,5,0,5,0_{p-5})^{\\top}$ . The second scenario is a high-dimensional setting with $p\\in\\{200,1000\\}$ and $\\beta=(\\delta,\\delta,5_{0.01p},0_{0.99p-2}^{\\top})^{\\top}$ , where $a_{q}$ represents a $q$ -dimensional vector with all entries set to $a$ . In the normal model, we specify $\\sigma_{Y}^{2}$ such that the signal-to-noise ratio $\\beta^{\\top}\\Sigma\\beta/\\sigma_{Y}^{2}=3$ by assigning $\\delta=0$ . The objective is to test whether the first two variables contribute significantly to predictions from a sequence of $n\\in\\{200,500,1000\\}$ independent realizations of $(Y,X)$ . ", "page_idx": 6}, {"type": "text", "text": "Let $\\mathcal{F}$ represent a generally unrestricted model class, subjecting to a degree of sparsity under the high-dimensional settings. Consider ${\\mathcal{F}}_{S}\\subseteq{\\mathcal{F}}$ such that the prediction functions exclude the first two components of the covariates. To test the irrelevance of the first two variables in predictions, we examine $H_{0}:\\psi_{S}=0$ in (1). We adopt the squared loss for normal responses and the cross-entropy loss for binomial responses. The ordinary least-squares regression and the LASSO are utilized under the low-dimensional and high-dimensional scenarios, respectively. The significance level is chosen as $\\alpha=5\\%$ , and our experiments entail 1, 000 replications. These experiments are executed on an Intel Xeon Gold 5118 CPU $\\textcircled{a}2.30\\mathrm{GHz}$ . ", "page_idx": 6}, {"type": "table", "img_path": "ahvOhPkkMx/tmp/84b06dad2157871960d5fab9bbb0a24be37315fc50e30f02f8e7da53a1d07ec6.jpg", "table_caption": ["Table 1: Empirical sizes (in percentage) of various testing procedures, with standard deviations in brackets. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Table 1 displays the empirical sizes for different testing procedures with $n=500$ and $p\\in\\{5,1000\\}$ . The results reveal that the Zipper, WGSC-3, and DSP-Split consistently maintain the correct size across all models, as anticipated. In contrast, the WGSC-2 exhibits conservative behavior in the low-dimensional setting and inflated sizes in the high-dimensional settings, primarily due to the degeneracy phenomenon. In addition, the data perturbation method, DSP-Pert, fails to control the size in some cases, particularly in the high-dimensional settings. This instability can be attributed to the selection of the amount of perturbation. ", "page_idx": 7}, {"type": "text", "text": "Figure 2 depicts the empirical power of various testing methods as a function of the magnitude $\\delta$ representing variable relevance, when $n\\,=\\,500$ and $p\\ \\in\\ \\{5,1000\\}$ . As expected, the Zipper shows a substantial improvement in power compared to the vanilla crossfitting based approaches, WGSC-3 and DSP-Split, with WGSC-3 and DSP-Split demonstrating similar performances. Under the high-dimensional settings, the WGSC-2 and DSP-Pert exhibit higher power than Zipper, but at the expense of losing valid size control. For a comprehensive analysis of the empirical sizes and power across various combinations of $n$ and $p$ , please refer to Section E.2. These additional results consistently support the conclusion that the Zipper method demonstrates reliable empirical size performance and significant power enhancement compared to that methods that utilize non-overlapping splits. ", "page_idx": 7}, {"type": "text", "text": "3.1.2 Model Specification Testing ", "text_level": 1, "page_idx": 7}, {"type": "image", "img_path": "ahvOhPkkMx/tmp/a58e790645b68118ce381c42bfa45a75b8193135b5278ff741cd6b97f0405c14.jpg", "img_caption": ["Figure 2: Empirical power of various testing methods as a function of the magnitude $\\delta$ with $n\\,=\\,500$ and $p\\ \\in$ $\\lbrace5,1000\\rbrace$ . The dot-dashed horizontal line represents the intercept at $\\alpha=5\\%$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "We extend our investigation beyond the variable importance assessment problem to include an evaluation of the proposed Zipper device in addressing model specification issues. Our analysis focuses on the model defined as $Y=X\\beta+\\varepsilon$ , where $\\displaystyle X\\sim N(0,\\Sigma)$ with $\\Sigma=(0.2^{|i-j|})_{p\\times p}$ and $\\varepsilon\\sim N(0,\\sigma_{\\varepsilon}^{2})$ . Here, we assume that $\\|\\beta\\|_{0}=2$ , indicating the presence of two nonzero components in $\\beta$ , while the positions of these components remain unknown. Our objective is to test the model specification hypothesis: $H_{0}:\\beta=(*,*,\\bar{0}_{p-2})^{\\top}$ versus $H_{1}:\\|\\beta\\|_{0}\\,=\\,2$ but not $H_{0}$ , where $^*$ represents any nonzero value. To generate the data, we consider three scenarios: (i) $\\beta=(0.4,0.4,0_{p-2})^{\\top}$ , (ii) $\\beta=(0.4,0,0.4,0_{p-3})^{\\top}$ , and (iii) $\\beta=$ $(0,0,0.4,0.4,0_{p-4})^{\\top}$ . We determine the value of $\\sigma_{\\varepsilon}^{2}$ such that the signal-to-noise ratio $\\beta^{\\top}\\Sigma\\beta/\\sigma_{\\varepsilon}^{2}=$ 1. Subsequently, we generate independent realizations $(Y_{i},X_{i})$ for $i=1,\\hdots,n$ , with $n=500$ and $p\\in\\{5,\\dot{10}00\\}$ . To estimate $\\beta$ , we employ ordinary least squares under $H_{0}$ , and perform the best two subset selection under $H_{1}$ . For the case when $p=5$ , we conduct an exhaust search. For the case when $p=1000$ , we utilize the abess algorithm [31] to approximate the solutions. The results are summarized in Table 2, where we observe that under Scenario (i), corresponding to $H_{0}$ , Zipper maintains correct size control. Furthermore, under Scenarios (ii) and (iii), corresponding to $H_{1}$ , Zipper exhibits substantial improvements in power when compared to the vanilla cross-ftiting based approaches, WGSC-3 and DSP-Split. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "table", "img_path": "ahvOhPkkMx/tmp/92b56f958637eb1f40a93b008d650574537a96483ac3787de91cf01dfc547be8.jpg", "table_caption": ["Table 2: Empirical sizes and powers (in percentage) for the model specification test, with standard deviations in brackets. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "3.2 Real-Data Examples ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "3.2.1 MNIST Handwritten Dataset ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We apply the Zipper method to the widely used MNIST handwritten digit dataset [32]. The MNIST dataset consists of size-normalized and center-aligned handwritten digit images, each represented as a $28\\times28$ pixel grid (resulting in $p=28^{2}=784)$ ). For our analysis, we specifically extract subsets of the dataset representing the digits 7 and 9, following Dai et al. [23], resulting in a total of $n=14251$ images. ", "page_idx": 8}, {"type": "text", "text": "In Figure 3, we calculate and graphically represent the average grayscale pixel values for images sharing the same numerals. We divide each image into nine distinct regions, as shown by the blank squares in Figure 3, with the objective of detecting regions that can effectively distinguish between these two digits. To achieve this, we perform a sequence of variable importance testing to assess the relevance of each region in making predictions while considering the remaining regions. Given the nature of the data, we employ a Convolutional Neural Network (CNN) as the underlying model, leveraging its proven effectiveness in image analysis. In the Zipper approach, we select the slider parameter $\\tau$ such that $n_{0}=50$ , as recommended in the manuscript. As a benchmark, we adopt WGSC-3 [5] (equivalent to DSP-Split [23]), which produces valid size, aligning with our approach. We set the predefined significance level for each test as $\\alpha=\\overline{{0.05}}/9$ , applying the Bonferroni correction to account for multiple comparisons. The discovered regions, highlighted in red, are presented in Figure 3. Our findings indicate that the Zipper method outperforms WGSC-3 in identifying critical regions with greater efficacy. ", "page_idx": 8}, {"type": "image", "img_path": "ahvOhPkkMx/tmp/88be8ea0f1ca2dcbc815e22e190024438cd6a814df6e1e5f59f72e619c36d0b9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 3: Hypothesis regions (blank squares) and important discoveries (squares fliled in red) comparing the Zipper method (left column) with WGSC3 (right column). ", "page_idx": 8}, {"type": "text", "text": "3.2.2 Bodyfat Dataset ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We expand the application of our Zipper method to the bodyfat dataset [33], which provides an estimate of body fat percentages obtained through underwater weighing, along with various body circumference measurements from a sample of $n\\,=\\,252$ men. Our objective is to conduct marginal variable importance tests for each body circumference while considering potential influences from essential attributes such as age, weight, and height. To accurately estimate the relevant regression functions within this dataset, we employ the random forest as our modeling technique. Table 3 presents the resulting p-values obtained from the Zipper and WGSC-3 methods. By applying the Bonferroni correction, the Zipper method identifies both Abdomen and Hip as significant factors at the significance level of $\\alpha=0.05/10$ . In contrast, WGSC-3 suggests only Abdomen as important. It is worth noting that a recent study by Zhu et al. [34] proposed the formula (Waist+Hip)/Height as a straightforward evaluation index for body fat. Remarkably, our finds align with this fact, further supporting the validity and relevance of our Zipper method in identifying key factors. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Table 3: P-values obtained from the Zipper and WGSC-3 methods for each marginal test regarding the relevance of the body circumference. ", "page_idx": 9}, {"type": "table", "img_path": "ahvOhPkkMx/tmp/e748aabc30dbf515bfdb88d708fb7e27212597a60411f104d8a7cd83f8faa462.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "4 Concluding Remarks ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we introduce Zipper, a simple yet effective device designed to address the issue of degeneracy in algorithm/model-agnostic inference. The mechanism of Zipper involves the recycling of data usage by constructing two overlapping data splits within the testing samples, which holds potential for independent exploration. A key component of Zipper is the slider parameter, which introduces an efficiency-and-degeneracy tradeoff. To ensure reliable inference, we propose a simple selection criterion by ensuring a large sample size to render asymptotic normality under the null hypothesis. Other data-adaptive strategies are possible and merit further investigation. Moreover, the predictiveness-comparison-based framework allows for the utilization of alternative forms of twosample tests, such as rank-based methods. This capability proves beneficial when dealing with data exhibiting heavy-tailed distributions or outliers. Furthermore, incorporating the Zipper device into large-scale comparisons to achieve error rate control warrants additional research. We can conduct a sequence of variable importance tests, each aimed at assessing the relevance of a specific variable $X_{j}$ in the predictive model while controlling for a global error rate. This procedure necessitates the fitting of $p+1$ models: one that includes all variables and $p$ null models, each excluding a distinct variable. Such a process is computationally demanding. Moreover, accurately controlling error rates presents a considerable challenge due to complex dependency structures among the p-values. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We would like to acknowledge the anonymous area chair and reviewers for their valuable comments and suggestions. Zou was supported by the National Key R&D Program of China (Grant Nos. 2022YFA1003703, 2022YFA1003800) and the National Natural Science Foundation of China (Grant Nos. 11925106, 12231011, 11931001, 12226007, 12326325). Wang was supported by the National Natural Science Foundation of China (Grant No. 12471255) and the Natural Science Foundation of Shanghai (Grant No. 23ZR1419400). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Jing Lei, Max G\u2019Sell, Alessandro Rinaldo, Ryan J. Tibshirani, and Larry Wasserman. Distribution-free predictive inference for regression. Journal of the American Statistical Association, 113(523):1094\u20131111, 2018.   \n[2] Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. Double/debiased machine learning for treatment and structural parameters. The Econometrics Journal, 21(1):C1\u2013C68, 01 2018.   \n[3] Aaron Fisher, Cynthia Rudin, and Francesca Dominici. All models are wrong, but many are useful: Learning a variable\u2019s importance by studying an entire class of prediction models simultaneously. Journal of Machine Learning Research, 20(177):1\u201381, 2019.   \n[4] Zhanrui Cai, Jing Lei, and Kathryn Roeder. Model-free prediction test with application to genomics data. Proceedings of the National Academy of Sciences, 119(34):e2205518119, 2022.   \n[5] Brian D. Williamson, Peter B. Gilbert, Noah Simon, and Marco Carone. A general framework for inference on algorithm-agnostic variable importance. Journal of the American Statistical Association, 118(543):1645\u20131658, 2023.   \n[6] Susanne M. Schennach and Daniel Wilhelm. A simple parametric model selection test. Journal of the American Statistical Association, 112(520):1663\u20131674, 2017.   \n[7] Alex Luedtke, Marco Carone, and Mark J. van der Laan. An omnibus non-parametric test of equality in distribution for unknown functions. Journal of the Royal Statistical Society Series B: Statistical Methodology, 81(1):75\u201399, 2019.   \n[8] Ian Covert, Scott Lundberg, and Su-In Lee. Explaining by removing: A unified framework for model explanation. Journal of Machine Learning Research, 22(209):1\u201390, 2021.   \n[9] Aaron Hudson. Nonparametric inference on non-negative dissimilarity measures at the boundary of the parameter space. arXiv: 2306.07492, 2023.   \n[10] Jianqing Fan, Chunming Zhang, and Jian Zhang. Generalized likelihood ratio statistics and Wilks phenomenon. The Annals of Statistics, 29(1):153\u2013193, 2001.   \n[11] Jing Lei. Cross-validation with confidence. Journal of the American Statistical Association, 115(532):1978\u20131997, 2020.   \n[12] Pierre Bayle, Alexandre Bayle, Lucas Janson, and Lester Mackey. Cross-validation confidence intervals for test error. In Advances in Neural Information Processing Systems, volume 33, pages 16339\u201316350. Curran Associates, Inc., 2020.   \n[13] Alessandro Rinaldo, Larry Wasserman, and Max G\u2019Sell. Bootstrapping and sample splitting for high-dimensional, assumption-lean inference. The Annals of Statistics, 47(6):3438\u20133469, 2019.   \n[14] Brian D. Williamson, Peter B. Gilbert, Marco Carone, and Noah Simon. Nonparametric variable importance assessment using machine learning techniques. Biometrics, 77(1):9\u201322, 2021.   \n[15] Lu Zhang and Lucas Janson. Floodgate: Inference for model-free variable importance. arXiv: 2007.01283v5, 2022.   \n[16] Yue Gao, Abby Stevens, Garvesh Raskutti, and Rebecca Willett. Lazy estimation of variable importance for large neural networks. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 7122\u20137143. PMLR, 2022.   \n[17] Rajen D. Shah and Jonas Peters. The hardness of conditional independence testing and the generalised covariance measure. The Annals of Statistics, 48(3):1514\u20131538, 2020.   \n[18] Anton Rask Lundborg, Ilmun Kim, Rajen D. Shah, and Richard J. Samworth. The Projected Covariance Measure for assumption-lean variable significance testing. arXiv: 2211.02039, 2022.   \n[19] Larry Wasserman, Aaditya Ramdas, and Sivaraman Balakrishnan. Universal inference. Proceedings of the National Academy of Sciences, 117(29):16880\u201316890, 2020.   \n[20] Victor Chernozhukov, Juan Carlos Escanciano, Hidehiko Ichimura, Whitney K. Newey, and James M. Robins. Locally robust semiparametric estimation. Econometrica, 90(4):1501\u20131535, 2022.   \n[21] Yuqian Zhang and Jelena Bradic. High-dimensional semi-supervised learning: in search of optimal inference of the mean. Biometrika, 109(2):387\u2013403, 2022.   \n[22] Isabella Verdinelli and Larry Wasserman. Decorrelated variable importance. Journal of Machine Learning Research, 25(7):1\u201327, 2024.   \n[23] Ben Dai, Xiaotong Shen, and Wei Pan. Significance tests of feature relevance for a black-box learner. IEEE Transactions on Neural Networks and Learning Systems, 35(2):1898\u20131911, 2024.   \n[24] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. \"why should i trust you?\": Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201916, pages 1135\u20131144. Association for Computing Machinery, 2016.   \n[25] Ian Covert, Scott M Lundberg, and Su-In Lee. Understanding global feature contributions with additive importance measures. In Advances in Neural Information Processing Systems, volume 33, pages 17212\u201317223. Curran Associates, Inc., 2020.   \n[26] Ian Covert, Scott Lundberg, and Su-In Lee. Explaining by removing: A unified framework for model explanation. Journal of Machine Learning Research, 22(209):1\u201390, 2021.   \n[27] I. Elizabeth Kumar, Suresh Venkatasubramanian, Carlos Scheidegger, and Sorelle Friedler. Problems with shapley-value-based explanations as feature importance measures. In Proceedings of the 37th International Conference on Machine Learning, Proceedings of Machine Learning Research, pages 5491\u20135500. PMLR, 2020.   \n[28] Emmanuel Cand\u00e8s, Yingying Fan, Lucas Janson, and Jinchi Lv. Panning for gold: \u2018model-X\u2019 knockoffs for high dimensional controlled variable selection. Journal of the Royal Statistical Society. Series B. Statistical Methodology, 80(3):551\u2013577, 2018.   \n[29] Wesley Tansey, Victor Veitch, Haoran Zhang, Raul Rabadan, and David M Blei. The holdout randomization test for feature selection in black box models. Journal of Computational and Graphical Statistics, 31(1):151\u2013162, 2022.   \n[30] Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.   \n[31] Jin Zhu, Xueqin Wang, Liyuan Hu, Junhao Huang, Kangkang Jiang, Yanhang Zhang, Shiyun Lin, and Junxian Zhu. abess: A fast best-subset selection library in python and r. Journal of Machine Learning Research, 23(202):1\u20137, 2022.   \n[32] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.   \n[33] Keith W Penrose, AG Nelson, and AG Fisher. Generalized body composition prediction equation for men using simple measurement techniques. Medicine & Science in Sports & Exercise, 17(2):189, 1985.   \n[34] Yuetong Zhu, Hitoshi Maruyama, Ko Onoda, Yue Zhou, Qiuchen Huang, Chunying Hu, Zhongqiu Ye, Bo Li, and Zimin Wang. Body mass index combined with (waist $^+$ hip)/height accurately screened for normal-weight obesity in chinese young adults. Nutrition, 108:111939, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A Conditions ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Assume $P\\in\\mathcal{M}$ for a rich class $\\mathcal{M}$ of distributions. Define the linear space ${\\mathcal H}=\\{c(\\tilde{P}_{1}-\\tilde{P}_{2}):$ : $c\\,\\geq\\,0,\\tilde{P}_{1},\\tilde{P}_{2}\\,\\in\\,\\mathcal{M}\\}$ . For each $h\\ \\in\\ \\mathcal H$ , let $\\|h\\|_{\\infty}\\,=\\,\\mathrm{sup}_{z}\\,|\\tilde{F}_{1}(z)-\\tilde{F}_{2}(z)|$ , where ${\\tilde{F}}_{j}$ denotes the distribution function corresponding to ${\\tilde{P}}_{j}$ for $j=1,2$ . Consider $\\mathcal{F}$ as the class of predictions functions endowed with a norm $\\Vert\\cdot\\Vert_{\\mathcal{F}}$ . Let $f\\in\\arg\\operatorname*{max}_{\\tilde{f}\\in\\mathcal{F}}\\mathbb{C}(\\tilde{f},P)$ , and $\\{f_{k,n}\\}_{k=1}^{K}$ be a sequence of estimators of $f$ . For each $1\\le k\\le K$ , define $a_{k,n}:z\\mapsto{\\dot{\\mathbb{C}}}(f_{k,n},P;\\delta_{z}-P)-{\\dot{\\mathbb{C}}}(f,P;\\delta_{z}-P)$ and $b_{k,n}:z\\mapsto{\\dot{\\mathbb{C}}}(f_{k,n},P_{k,n};\\delta_{z}-P_{k,n})-{\\dot{\\mathbb{C}}}(f_{k,n},P;\\delta_{z}-P)$ . The following conditions are imposed on the tuple (P, F, f, fk,n). ", "page_idx": 12}, {"type": "text", "text": "(C1) There exists some constant $C>0$ such that, for each sequence $\\tilde{f}_{1},\\tilde{f}_{2},\\ldots\\in\\mathcal{F}$ such that $\\|\\tilde{f}_{j}-f\\|_{\\mathcal{F}}\\to0$ , $|\\mathbb{C}(\\tilde{f}_{j},P)-\\mathbb{C}(f,P)|\\leq C\\|\\tilde{f}_{j}-f\\|_{\\mathcal{F}}^{2}$ for each $j$ large enough;   \n(C2) There exists some constant $\\delta\\ >\\ 0$ such that for each sequence $\\epsilon_{1},\\epsilon_{2},\\ldots\\in\\ \\mathbb{R}$ and $h,h_{1},h_{2},\\ldots\\in\\mathcal{H}$ satisfying that $\\epsilon_{j}\\rightarrow0$ and $\\|h_{j}-h\\|_{\\infty}\\to0$ , it holds that $\\operatorname*{sup}_{\\tilde{f}\\in\\mathcal{F}:\\|\\tilde{f}-f\\|_{\\mathcal{F}}<\\delta}\\left|\\frac{\\mathbb{C}(\\tilde{f},P+\\epsilon_{j}h_{j})-\\mathbb{C}(\\tilde{f},P)}{\\epsilon_{j}}-\\dot{\\mathbb{C}}(\\tilde{f},P;h_{j})\\right|\\rightarrow0;$   \n(C3) For each $1\\leq k\\leq K,\\|f_{k,n}-f\\|_{\\mathcal{F}}=o_{P}(n^{-1/4});$   \n(C4) For each $\\begin{array}{r}{1\\leq k\\leq K,\\int a_{k,n}^{2}d P=o_{P}(1);}\\end{array}$   \n(C5) For each $\\begin{array}{r}{1\\leq k\\leq K,\\int b_{k,n}^{2}d P=o_{P}(1).}\\end{array}$ . ", "page_idx": 12}, {"type": "text", "text": "Condition (C1) pertains to the optimality of the prediction function $f$ , eliminating first-order estimation biases. Condition (C2) requires Hadamard differentiability of the predictiveness criterion. Condition (C3) demands accurate estimation of $f$ , rendering second-order terms negligible. Condition (C4) controls the remainder terms. Condition (C5) ensures consistent variance estimators. ", "page_idx": 12}, {"type": "text", "text": "B Proofs ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "B.1 Proof of Theorem 2.1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Lemma B.1. If Conditions $(C I)\u2013(C4)$ specified in Appendix hold for both tuples $(P,{\\mathcal{F}},f,f_{k,n})$ and $(P,\\mathcal{F}_{S},f_{S},f_{k,n,S})$ , then, for each $I\\in\\{o,a,b\\}$ and $k\\in\\{1,\\ldots,K\\}$ , ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ \\ \\ \\mathbb{C}(f_{k,n},P_{k,n,I})-\\mathbb{C}(f,P)=\\displaystyle\\frac{1}{|\\mathscr{D}_{k,I}|}\\sum_{i:Z_{i}\\in\\mathscr{D}_{k,I}}\\phi(Z_{i})+o_{P}(|\\mathscr{D}_{k,I}|^{-1/2})\\ a n d}\\\\ &{\\mathbb{C}(f_{k,n,S},P_{k,n,I})-\\mathbb{C}(f_{S},P)=\\displaystyle\\frac{1}{|\\mathscr{D}_{k,I}|}\\sum_{i:Z_{i}\\in\\mathscr{D}_{k,I}}\\phi_{S}(Z_{i})+o_{P}(|\\mathscr{D}_{k,I}|^{-1/2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Proof. See Theorem 2 in Williamson et al. [5]. ", "page_idx": 12}, {"type": "text", "text": "Denote $m_{k}=|\\mathcal{D}_{k,A}|=|\\mathcal{D}_{k,B}|$ . Since $\\left|\\mathcal{D}_{k,a}\\right|=\\left|\\mathcal{D}_{k,b}\\right|=(1-\\tau)m_{k}$ and $|\\mathcal{D}_{k,o}|=\\tau m_{k}$ , we have $m_{k}=n/\\{(2-\\tau)K\\}$ due to $\\lvert\\mathcal{D}_{k,a}\\rvert+\\lvert\\mathcal{D}_{k,b}\\rvert+\\lvert\\mathcal{D}_{k,o}\\rvert=n/K$ . By Lemma B.1, we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbb{C}_{k,n}-\\mathbb{C}(f,P)=\\tau\\{\\mathbb{C}(f_{k,n},P_{k,n,o})-\\mathbb{C}(f,P)\\}+(1-\\tau)\\{\\mathbb{C}(f_{k,n},P_{k,n,a})-\\mathbb{C}(f,P)\\}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Since the number of folds $K$ is fixed, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{1}{K}\\sum_{k=1}^{K}\\mathbb{C}_{k,n}-\\mathbb{C}(f,P)=\\frac{1}{K}\\sum_{k=1}^{K}\\left\\{\\frac{1}{m_{k}}\\sum_{i:Z_{i}\\in\\mathcal{D}_{k,A}}\\phi(Z_{i})+o_{P}(m_{k}^{-1/2})\\right\\}}}\\\\ &{}&{=\\frac{1}{K}\\sum_{k=1}^{K}\\frac{1}{m_{k}}\\sum_{i:Z_{i}\\in\\mathcal{D}_{k,A}}\\phi(Z_{i})+o_{P}(n^{-1/2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Similarly, we conclude that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{1}{K}\\sum_{k=1}^{K}\\mathbb{C}_{k,n,\\mathcal{S}}-\\mathbb{C}(f_{\\mathcal{S}},P)=\\frac{1}{K}\\sum_{k=1}^{K}\\frac{1}{m_{k}}\\sum_{i:Z_{i}\\in\\mathcal{D}_{k,B}}\\phi_{\\mathcal{S}}(Z_{i})+o_{P}\\big(n^{-1/2}\\big).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Combining (5) and (6), we obtain ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\stackrel{\\cdot}{\\phantom{=}}\\frac{\\psi_{s},s-\\psi_{S}}{n/(2-\\tau)}\\sum_{k=1}^{K}\\bigg[\\sum_{i:Z_{i}\\in\\mathcal{D}_{k,a}}\\phi(Z_{i})-\\sum_{i:Z_{i}\\in\\mathcal{D}_{k,b}}\\phi_{S}(Z_{i})+\\sum_{i:Z_{i}\\in\\mathcal{D}_{k,a}}\\{\\phi(Z_{i})-\\phi_{S}(Z_{i})\\}\\bigg]+o_{P}(n-1)\\,,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "By applying the standard central limit theorem, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\{(1-\\tau)n/(2-\\tau)\\}^{1/2}}{K}\\sum_{k=1}^{K}\\frac{1}{(1-\\tau)m_{k}}\\sum_{i:Z_{i}\\in\\mathcal{D}_{k,a}}\\phi(Z_{i})\\stackrel{d}{\\to}N(0,\\sigma^{2}),\\;\\mathrm{an}}\\\\ &{\\displaystyle\\frac{\\{(1-\\tau)n/(2-\\tau)\\}^{1/2}}{K}\\sum_{k=1}^{K}\\frac{1}{(1-\\tau)m_{k}}\\sum_{i:Z_{i}\\in\\mathcal{D}_{k,b}}\\phi_{S}(Z_{i})\\stackrel{d}{\\to}N(0,\\sigma_{S}^{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "as $n\\to\\infty$ . If $\\psi_{S}>0$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{\\{\\tau n/(2-\\tau)\\}^{1/2}}{K}\\sum_{k=1}^{K}\\frac{1}{\\tau m_{k}}\\sum_{i:Z_{i}\\in\\mathcal{D}_{k,o}}\\{\\phi(Z_{i})-\\phi_{S}(Z_{i})\\}\\stackrel{d}{\\to}N(0,\\eta_{S}^{2}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Hence, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\{n/(2-\\tau)\\}^{1/2}(\\psi_{n,S}-\\psi_{S})\\stackrel{d}{\\rightarrow}N(0,\\nu_{S,\\tau}^{2}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\nu_{S,\\tau}^{2}=(1-\\tau)(\\sigma^{2}+\\sigma_{S}^{2})+\\tau\\eta_{S}^{2}$ . ", "page_idx": 13}, {"type": "text", "text": "B.2 Proof of Proposition 2.3 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "It suffices to show that $\\sigma_{k,n}^{2}\\xrightarrow{p}\\sigma^{2}$ as $n\\to\\infty$ . Since ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\dot{\\mathbb{C}}(f_{k,n},P_{k,n};\\delta_{z}-P_{k,n})=\\dot{\\mathbb{C}}(f_{k,n},P_{k,n};\\delta_{z}-P_{k,n})-\\dot{\\mathbb{C}}(f_{k,n},P;\\delta_{z}-P)\\quad}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\dot{\\mathbb{C}}(f_{k,n},P;\\delta_{z}-P)-\\dot{\\mathbb{C}}(f,P;\\delta_{z}-P)+\\dot{\\mathbb{C}}(f,P;\\delta_{z}-P),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\begin{array}{r l}&{\\sigma_{k,n}^{2}}\\\\ &{=\\frac{1}{n\\hbar}\\Biggl\\{\\underset{i\\neq j\\in\\{P_{k},T_{k}\\}}{\\sum}\\{\\langle\\xi(f_{j_{k,n}},P_{j_{k}};\\xi_{\\xi_{i}}-P_{k,n})-\\hat{\\xi}(f_{k,n},P_{j_{k}};\\xi_{\\xi_{i}}-P)\\}^{2}}\\\\ &{+\\underset{i\\neq j\\in\\{P_{k},T_{k}\\}}{\\sum}\\{\\xi(f_{j_{k,n}},P_{j_{k}};\\xi_{\\xi_{i}}-P)-\\hat{\\xi}(f_{j},P_{j_{k}};\\xi_{\\xi_{i}}-P)\\}^{2}}\\\\ &{+\\underset{i\\neq j\\in\\{P_{k},T_{k}\\}}{\\sum}\\{\\xi(f_{j_{k}},\\xi_{\\xi_{i}}-P)\\}^{2}}\\\\ &{+\\underset{i\\neq j\\in\\{P_{k},T_{k}\\}}{\\sum}\\{\\xi(f_{j_{k}},P_{j_{k}};\\xi_{\\xi_{i}}-P)\\}^{2}}\\\\ &{+2\\underset{i\\neq j\\in\\{P_{k},T_{k}\\}}{\\sum}\\{\\xi(f_{j_{k,n}},P_{j_{k}};\\xi_{\\xi_{i}}-P_{k,n})-\\hat{\\xi}(f_{j_{k}},\\xi_{\\xi_{i}}-P)\\}\\hat{\\xi}(f_{j},P_{j_{k}};\\xi_{\\xi_{i}}-P)}\\\\ &{+2\\underset{i\\neq j\\in\\{P_{k},T_{k}\\}}{\\sum}\\{\\xi(f_{j_{k,n}},P_{j_{k}};\\xi_{\\xi_{i}}-P)-\\hat{\\xi}(f_{j},P_{j_{k}};\\xi_{\\xi_{i}}-P)\\}\\hat{\\xi}(f_{j},P_{j_{k}};\\xi_{\\xi_{i}}-P)}\\\\ &{+2\\underset{i\\neq j\\in\\{P_{k},T_{k}\\}}{\\sum}\\{\\xi(f_{j_{k,n}},P_{j_{k}};\\xi_{\\xi_{i}}-P_{k,n})-\\hat{\\xi}(f_{k},\\xi_{j_{k}},P_{j_{k}};\\xi_{\\xi_{i}}-P)\\}}\\\\ &{\\phantom{\\frac{(i,j_{k})^{2}}{\\sum}\\\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "For any $\\epsilon>0$ , by Markov\u2019s inequality and Condition (C5), ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\operatorname*{Pr}\\left[\\frac{1}{n_{k}}\\sum_{i:Z_{i}\\in\\mathcal{D}_{k}}\\{\\dot{\\mathbb{C}}(f_{k,n},P_{k,n};\\delta_{Z_{i}}-P_{k,n})-\\dot{\\mathbb{C}}(f_{k,n},P;\\delta_{Z_{i}}-P)\\}^{2}\\geq\\epsilon\\mid\\mathcal{D}_{-k}\\right]}}\\\\ {{\\displaystyle\\leq\\frac{1}{\\epsilon}E[\\{\\dot{\\mathbb{C}}(f_{k,n},P_{k,n};\\delta_{Z_{i}}-P_{k,n})-\\dot{\\mathbb{C}}(f_{k,n},P;\\delta_{Z_{i}}-P)\\}^{2}\\mid\\mathcal{D}_{-k}]}}\\\\ {{\\displaystyle=\\frac{1}{\\epsilon}\\int\\{b_{k,n}(z)\\}^{2}d P(z)\\xrightarrow{p}0.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then, by the law of total expectation, the first term of (7) is $o_{P}(1)$ . Similarly, the second term is $o_{P}(1)$ . Specifically, by Condition (C4), for any $\\epsilon>0$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}\\left[\\displaystyle\\frac{1}{n_{k}}\\sum_{i:Z_{i}\\in\\mathcal{D}_{k}}\\{\\dot{\\mathbb{C}}(f_{k,n},P;\\delta_{Z_{i}}-P)-\\dot{\\mathbb{C}}(f,P;\\delta_{Z_{i}}-P)\\}^{2}\\geq\\epsilon\\mid\\mathcal{D}_{-k}\\right]}\\\\ &{\\leq\\displaystyle\\frac{1}{\\epsilon}E[\\{\\dot{\\mathbb{C}}(f_{k,n},P;\\delta_{Z}-P)-\\dot{\\mathbb{C}}(f,P;\\delta_{Z}-P)\\}^{2}\\mid\\mathcal{D}_{-k}]}\\\\ &{=\\displaystyle\\frac{1}{\\epsilon}\\int\\{a_{k,n}(z)\\}^{2}d P(z)\\overset{p}{\\rightarrow}0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "For the third term, by the law of large numbers, ", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\frac{1}{n_{k}}}\\sum_{i:Z_{i}\\in{\\mathcal{D}}_{k}}{\\dot{\\mathbb{C}}}(f,P;\\delta_{Z_{i}}-P)^{2}\\stackrel{p}{\\rightarrow}E\\{{\\dot{\\mathbb{C}}}(f,P;\\delta_{Z}-P)^{2}\\}=\\sigma^{2},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "as $n\\to\\infty$ . For the fourth term, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left|\\frac{1}{n_{k}}\\sum_{i:Z_{i}\\in\\mathcal{D}_{k}}\\{\\dot{\\mathbb{C}}(f_{k,n},P_{k,n};\\delta_{Z_{i}}-P_{k,n})-\\dot{\\mathbb{C}}(f_{k,n},P;\\delta_{Z_{i}}-P)\\}\\dot{\\mathbb{C}}(f,P;\\delta_{Z_{i}}-P)\\right|}\\\\ {\\displaystyle\\leq\\left[\\frac{1}{n_{k}}\\sum_{i:Z_{i}\\in\\mathcal{D}_{k}}\\{\\dot{\\mathbb{C}}(f_{k,n},P_{k,n};\\delta_{Z_{i}}-P_{k,n})-\\dot{\\mathbb{C}}(f_{k,n},P;\\delta_{Z_{i}}-P)\\}^{2}\\right]^{1/2}}\\\\ {\\displaystyle\\quad\\times\\left[\\frac{1}{n_{k}}\\sum_{i:Z_{i}\\in\\mathcal{D}_{k}}\\dot{\\mathbb{C}}(f,P;\\delta_{Z_{i}}-P)^{2}\\right]^{1/2}=o_{P}(1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Similarly, both of the last two terms are $o_{P}(1)$ . Hence, it follows that $\\sigma_{k,n}^{2}\\xrightarrow{p}\\sigma^{2}$ ", "page_idx": 14}, {"type": "text", "text": "By similar arguments, $\\sigma_{k,n,S}^{2}\\xrightarrow{p}\\sigma_{S}^{2}$ and thus $\\nu_{n,S,\\tau}^{2}\\stackrel{p}{\\rightarrow}\\nu_{S,\\tau}$ . ", "page_idx": 14}, {"type": "text", "text": "B.2.1 Verification of Condition (C5) in Remark 2.4 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Suppose $\\mathbb{C}(f,P)=E_{P}[g\\{Y,f(X)\\}]$ . Then ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\dot{\\mathbb{C}}(f_{k,n},P_{k,n};\\delta_{z}-P_{k,n})=g\\{y,f_{k,n}(x)\\}-\\frac{1}{n_{k}}\\sum_{i:Z_{i}\\in{\\mathcal{D}}_{k}}g\\{Y_{i},f_{k,n}(X_{i})\\},}}\\\\ &{}&{\\dot{\\mathbb{C}}(f_{k,n},P;\\delta_{z}-P)=g\\{y,f_{k,n}(x)\\}-E_{P}[g\\{Y,f_{k,n}(X)\\}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By noting that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ \\,E[\\{\\dot{\\mathbb{C}}(f_{k,n},P_{k,n};\\delta_{z}-P_{k,n})-\\dot{\\mathbb{C}}(f_{k,n},P;\\delta_{z}-P)\\}^{2}\\mid\\mathcal{D}_{-k}]}\\\\ &{=E\\left(\\left[\\displaystyle\\left[\\frac{1}{n_{k}}\\sum_{i:Z_{i}\\in\\mathcal{D}_{k}}g\\{Y_{i},f_{k,n}(X_{i})\\}\\right]^{2}\\mid\\mathcal{D}_{-k}\\right)-E[g\\{Y,f_{k,n}(X)\\}\\mid\\mathcal{D}_{-k}]^{2}\\right.}\\\\ &{=\\displaystyle\\frac{1}{n_{k}}V a r[g\\{Y,f_{k,n}(X)\\}\\mid\\mathcal{D}_{-k}]\\overset{p}{\\rightarrow}0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "the conclusion follows. ", "page_idx": 15}, {"type": "text", "text": "B.3 Proof of Theorem 2.6 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "By Theorem 2.1, for any $\\tau\\in[0,1)$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\{n/(2-\\tau)\\}^{1/2}(\\psi_{n,S}-\\psi_{S})\\overset{d}{\\rightarrow}N(0,\\nu_{S,\\tau}^{2})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "as $n\\to\\infty$ , where $\\nu_{S,\\tau}^{2}=(1-\\tau)(\\sigma^{2}+\\sigma_{S}^{2})+\\tau\\eta_{S}^{2}$ , $\\sigma^{2}=E\\{\\phi^{2}(Z)\\}$ , $\\sigma_{S}^{2}=E\\{\\phi_{S}^{2}(Z)\\}$ , and $\\eta_{S}^{2}=$ $E[\\{\\phi(Z)-\\phi_{S}(Z)\\}^{2}]$ . By examining the proof of Proposition 2.3, we conclude that $\\nu_{n,S,\\tau}\\stackrel{p}{\\rightarrow}\\nu_{S,\\tau}^{(0)}$ under $H_{1}$ . Consequently, the power function is ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}(T_{\\tau}>z_{1-\\alpha}\\mid H_{1})}\\\\ &{=\\operatorname*{Pr}\\Bigg(\\frac{\\nu_{S,\\tau}^{(0)}}{\\nu_{n,S,\\tau}}\\frac{\\{n/(2-\\tau)\\}^{1/2}(\\psi_{n,S}-\\psi_{S})}{\\nu_{S,\\tau}}>\\frac{\\nu_{S,\\tau}^{(0)}}{\\nu_{S,\\tau}}z_{1-\\alpha}-\\frac{\\{n/(2-\\tau)\\}^{1/2}\\nu_{S,\\tau}^{(0)}\\psi_{S}}{\\nu_{n,S,\\tau}\\nu_{S,\\tau}}\\mid H_{1}\\Bigg)}\\\\ &{=\\Phi\\left(-\\frac{\\nu_{S,\\tau}^{(0)}}{\\nu_{S,\\tau}}z_{1-\\alpha}+\\frac{\\{n/(2-\\tau)\\}^{1/2}\\psi_{S}}{\\nu_{S,\\tau}}\\right)+o(1)}\\\\ &{=G_{S,n,\\alpha}(\\tau)+o(1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Given that $\\eta s=\\sigma^{2}+\\sigma_{S}^{2}-2C o v\\{\\phi(Z),\\phi_{S}(Z)\\}$ , the asymptotic variance $\\nu_{S,\\tau}^{2}$ can be expressed as $\\sigma^{2}+\\sigma_{S}^{2}-2\\tau C o v\\{\\phi(Z),\\phi_{S}(Z)\\}$ . Let ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\varphi_{1}(\\tau)=\\frac{(1-\\tau)(\\sigma^{2}+\\sigma_{S}^{2})}{\\sigma^{2}+\\sigma_{S}^{2}-2\\tau C o v\\{\\phi(Z),\\phi_{S}(Z)\\}}\\mathrm{~and~}}\\\\ &{}&{\\varphi_{2}(\\tau)=\\frac{\\{n/(2-\\tau)\\}^{1/2}\\psi_{S}}{(\\sigma^{2}+\\sigma_{S}^{2}-2\\tau C o v\\{\\phi(Z),\\phi_{S}(Z)\\})^{1/2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since $2C o v\\{\\phi(Z),\\phi_{S}(Z)\\}\\le\\sigma^{2}+\\sigma_{S}^{2}$ and $\\tau\\in[0,1)$ , it follows that $\\varphi_{1}(\\tau)>0$ and $\\varphi_{2}(\\tau)>0$ . When $C o v\\{\\phi(Z),\\phi_{S}(Z)\\}\\ge0$ , we observe that $\\varphi_{1}(\\tau)$ is monotonically decreasing, and $\\varphi_{2}(\\tau)$ is monotonically increasing with respect to $\\tau$ . Therefore, the approximate power function $G_{{\\cal S},n,\\alpha}(\\tau)$ increase with $\\tau$ . ", "page_idx": 15}, {"type": "text", "text": "B.3.1 On Remark 2.8 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Here we demonstrate the validity of the condition $C o v\\{\\phi(Z),\\phi_{S}(Z)\\}\\ge0$ in assessing variable importance when utilizing the square loss. Consider the scenario where $Y=E(Y\\mid X)^{\\prime}+\\varepsilon$ , with the noise $\\varepsilon$ assumed to be independent of the covariate $X$ . We seek to evaluate the significance of a specific set of covariates $U$ in predicting the response, where $X=(U^{\\top},V^{\\top})^{\\top}$ . Assuming ", "page_idx": 15}, {"type": "text", "text": "$E(Y\\mid X)\\;\\in\\;{\\mathcal{F}}$ and $E(Y\\ |\\ V)\\,\\in\\,{\\mathcal{F}}_{S}$ , the optimal prediction functions within $\\mathcal{F}$ and $\\mathcal{F}_{S}$ are $f(X)=E(Y\\mid X)$ and $f_{S}(V)=E(Y\\mid V)$ , respectively. Referring to Remark 2.4, the covariance $C o v\\{\\phi(Z),\\phi_{\\mathcal{S}}(Z)\\}$ can be expressed as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C o v\\{\\phi(Z),\\phi_{S}(Z)\\}=C o v[\\{Y-f(X)\\}^{2},\\{Y-f_{S}(V)\\}^{2}]}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =V a r(\\varepsilon^{2})+C o v[\\varepsilon^{2},\\{f(X)-f_{S}(V)\\}^{2}]+2C o v[\\varepsilon^{2},\\varepsilon\\{f(X)-f_{S}(V)\\}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "It\u2019s evident that the first two terms are non-negative. Regarding the third term, since $\\varepsilon$ is independent of $X$ and $E f(X)\\mid V=f_{S}(V)$ , we deduce ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C o v[\\varepsilon^{2},\\varepsilon\\{f(X)-f_{S}(V)\\}]=E[\\varepsilon^{3}\\{f(X)-f_{S}(V)\\}]-E(\\varepsilon^{2})E[\\varepsilon\\{f(X)-f_{S}(V)\\}]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=E(\\varepsilon^{3})E\\{f(X)-f_{S}(V)\\}-E(\\varepsilon^{2})E(\\varepsilon)E\\{f(X)-f_{S}(V)\\}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=E(\\varepsilon^{3})E[E\\{f(X)-f_{S}(V)\\mid V\\}]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus, $C o v\\{\\phi(Z),\\phi_{S}(Z)\\}\\ge0$ follows. ", "page_idx": 16}, {"type": "text", "text": "C The Zipper Algorithm ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Algorithm 1 The algorithm for the proposed Zipper testing procedure ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Input: Observed data $\\{Z_{i}\\}_{i=1}^{n}$ , number of folds $K$ , and slider parameter $\\tau\\in[0,1)$   \nRandomly partition $\\{Z_{i}\\}_{i=1}^{n}$ into $K$ disjoint folds $\\mathcal{D}_{1},\\ldots,\\mathcal{D}_{K}$   \nfor $k=1,\\ldots,K$ do Using data in $\\{\\mathcal{D}_{j}\\}_{j=1}^{K}\\setminus\\mathcal{D}_{k}$ , construct estimators $f_{k,n}$ of $f$ and $f_{k,n,s}$ of $f_{S}$ Using data in $\\mathcal{D}_{k}$ , construct estimator $P_{k,n}$ of $P$ Randomly divide $\\mathcal{D}_{k}$ into two splits $\\mathcal{D}_{k,A}$ and $\\mathcal{D}_{k,B}$ of roughly equal size $m_{k}$ , with an overlap $\\mathcal{D}_{k,o}$ such that $|\\mathcal{D}_{k,o}|=\\lfloor\\tau m_{k}\\rfloor$ . Let $\\mathcal{D}_{k,a}=\\mathcal{D}_{k,A}\\backslash\\mathcal{D}_{k,o}$ and $\\mathcal{D}_{k,b}=\\mathcal{D}_{k,B}\\backslash\\mathcal{D}_{k,o}$ . Using data in $\\mathcal{D}_{k,I}$ , construct estimators $P_{k,n,I}$ for $I\\in\\{o,a,b\\}$ Compute $\\begin{array}{r l r}{\\mathbb{C}_{k,n}}&{{}\\;=\\;}&{\\tau\\mathbb{C}(f_{k,n},P_{k,n,o})\\;\\;+\\;\\;(1\\;\\;-\\;\\;\\tau)\\mathbb{C}(f_{k,n},P_{k,n,a})}\\end{array}$ and $\\sigma_{k,n}^{2}$ = $\\begin{array}{r}{n_{k}^{-1}\\sum_{i:Z_{i}\\in\\mathcal{D}_{k}}\\{\\dot{\\mathbb{C}}(f_{k,n},P_{k,n};\\delta_{Z_{i}}-P_{k,n})\\}^{2}}\\end{array}$ , where $n_{k}=|\\mathcal{D}_{k}|$ Comput $\\begin{array}{r l}&{\\colon\\mathrm{\\bf~\\bar{C}}_{k,n,S}^{\\mathrm{~\\,~\\sim~}}\\;=\\;\\;\\;\\tau\\mathbb{C}(f_{k,n,S},P_{k,n,o})\\;+\\;(1\\;-\\;\\tau)\\mathbb{C}(f_{k,n,S},P_{k,n,b})}\\\\ &{}\\\\ &{Z_{i}{\\in}{\\mathcal D}_{k}\\left\\{\\dot{\\mathbb{C}}(f_{k,n,S},P_{k,n};\\delta_{Z_{i}}-P_{k,n})\\right\\}^{2}}\\end{array}$ and $\\begin{array}{r l}{\\sigma_{k,n,s}^{2}}&{{}=}\\end{array}$   \nend for   \nCompute $\\begin{array}{r}{\\mathbb{C}_{n}=K^{-1}\\sum_{k=1}^{K}\\mathbb{C}_{k,n},\\mathbb{C}_{n,S}=K^{-1}\\sum_{k=1}^{K}\\mathbb{C}_{k,n,S}}\\end{array}$ and estimator $\\psi_{n,S}=\\mathbb{C}_{n}-\\mathbb{C}_{n,S}$   \nof $\\psi_{S}$   \nCompute $\\begin{array}{r}{\\nu_{n,S,\\tau}^{2}=(1-\\tau)K^{-1}\\sum_{k=1}^{K}(\\sigma_{k,n}^{2}+\\sigma_{k,n,S}^{2})}\\end{array}$   \nOutput: $\\mathbf{P}$ -value $1-\\Phi(\\{n/(2-\\tau)\\}^{1/2}\\psi_{n,S}/\\nu_{n,S,\\tau})$ , where $\\Phi$ denotes the distribution function   \nof $N(0,1)$ ", "page_idx": 16}, {"type": "text", "text": "D The Confidence Interval for the Dissimilarity Measure ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "To construct a valid confidence interval for the dissimilarity measure $\\psi_{S}$ , it is crucial to use a consistent variance estimator irrespective of whether $H_{0}$ holds or not. This can be achieved by incorporating an additional plug-in estimator of $\\eta_{S}^{2}$ , which is a component of the asymptotic variance $\\nu_{S,\\tau}^{2}$ . Let $\\begin{array}{r}{\\nu_{n,S,\\tau}^{2}=K^{-1}\\sum_{k=1}^{K}\\{(1-\\tau)(\\sigma_{k,n}^{2}+\\bar{\\sigma_{k,n,S}^{2}})+\\tau\\eta_{k,n,S}^{2}\\}}\\end{array}$ , where for each $k\\in\\{1,\\ldots,K\\}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\eta_{k,n,S}^{2}=\\frac{1}{n_{k}}\\sum_{i:Z_{i}\\in\\mathcal{D}_{k}}\\{\\dot{\\mathbb{C}}(f_{k,n},P_{k,n};\\delta_{Z_{i}}-P_{k,n})-\\dot{\\mathbb{C}}(f_{k,n,S},P_{k,n};\\delta_{Z_{i}}-P_{k,n})\\}^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The confidence interval for $\\psi_{S}$ can be formulated as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left(\\psi_{n,S}-\\frac{z_{\\alpha/2}\\nu_{n,S,\\tau}}{\\{n/(2-\\tau)\\}^{1/2}},\\psi_{n,S}+\\frac{z_{\\alpha/2}\\nu_{n,S,\\tau}}{\\{n/(2-\\tau)\\}^{1/2}}\\right),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which has an asymptotic coverage of $1-\\alpha$ . ", "page_idx": 16}, {"type": "table", "img_path": "ahvOhPkkMx/tmp/4ff7ad177042018d3620946eb0274ae12203ba4b87f94496f1b501baa6acc49d.jpg", "table_caption": ["Table 4: Empirical sizes (in percentage) of the Zipper method against different values of $\\tau$ with $n=500$ . "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "ahvOhPkkMx/tmp/79171495bcc03c561f6d49743432bcb786923567b12a0c11732bb823bc5db7fd.jpg", "table_caption": ["Table 5: Empirical power (in percentage) of the Zipper method against different values of $\\tau$ with $n=500$ . For the normal model, we set $\\delta=2$ for the low-dimensional setting and $\\delta=5$ for the high-dimensional setting. For the Binomial model, we set $\\delta=3$ for the low-dimensional setting and $\\delta=5$ for the high-dimensional setting. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "E Additional Simulations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "E.1 On the Slider Parameter ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "To assess the influence of the slider parameter $\\tau$ on the size and power of the proposed Zipper approach, we conduct a small simulation study using the simulation settings outlined in the manuscript. The results of this study are summarized in Tables 4\u20135. Notably, we observe that the empirical sizes are consistently controlled when $\\tau$ falls within an appropriate range. However, selecting excessively large values of $\\tau$ led to a slight inflation in size, primarily due to the poor normal approximation under the null hypothesis, as discussed in Section 2.5. Furthermore, the empirical power increases as the value of $\\tau$ increases, which aligns with our expectations. ", "page_idx": 17}, {"type": "text", "text": "E.2 Additional Simulation Experiments ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "To provide a comprehensive analysis of the empirical sizes and power across various settings, we conducted additional simulations using the simulation settings outlined in the manuscript. We explored different combinations of the sample size $n$ and dimension $p$ . Moreover, we investigated a model with a heavy-tailed response by considering $Y=X\\beta+\\sigma_{Y}\\varepsilon/3$ , $\\varepsilon\\sim t_{3}$ , where $t_{3}$ represents the $t$ -distribution with 3 degrees of freedom. The results for empirical sizes and power with $\\delta=1$ are summarized in Tables 6 and 7. These additional findings consistently support the conclusion that the Zipper method demonstrates reliable empirical size performance and provides significant power enhancement compared to methods that utilize non-overlapping splits. ", "page_idx": 17}, {"type": "text", "text": "F Comparison between Zipper and Data Perturbation Methods ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "To evaluate variables with zero-importance, Rinaldo et al. [13] and Dai et al. [23] proposed a data perturbation technique, injecting independent zero-mean noises into empirical influence functions. We describe this method using our own terminology. We begin by partitioning the data randomly into $K$ folds, denoted as $\\mathcal{D}_{1},\\ldots,\\mathcal{D}_{K}$ , ensuring equitable sizing across folds. For each fold index $k\\in\\{1,\\ldots,K\\}$ , we construct estimators $f_{k,n}$ and $f_{k,n,s}$ for the oracle prediction functions $f$ and $f_{S}$ , correspondingly, utilizing data excluding fold $\\mathcal{D}_{k}$ . This method employs the following test statistic: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\phi_{n,S,\\mathrm{pert},\\rho}=\\frac{1}{n}\\sum_{k=1}^{K}\\bigg[\\sum_{i:Z_{i}\\in\\mathcal{D}_{k}}\\phi(Z_{i})-\\phi_{S}(Z_{i})+\\rho\\varepsilon_{i}\\bigg],\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Table 6: Empirical sizes (in percentage) of various testing procedures. ", "page_idx": 18}, {"type": "table", "img_path": "ahvOhPkkMx/tmp/4dda6e8f7e5dcc53986f93d16f95040f62ce4ec131bd6aaee8cda5a9fe4132e7.jpg", "table_caption": [], "table_footnote": ["Table 7: Empirical power (in percentage) of various testing procedures for $\\delta=1$ . "], "page_idx": 18}, {"type": "table", "img_path": "ahvOhPkkMx/tmp/d86f10b701f9a2022d03dc059f477d293ec43ad85024946a3b2da0389f299e9f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "where $\\varepsilon_{i}~\\sim~N(0,1)$ for $i~=~1,\\hdots,n$ are independent noise, and $\\rho$ represents the perturbation parameter governing the extent of perturbation. It can be established that for any $\\rho\\,>\\,0$ , $n^{1/2}(\\phi_{n,S,\\mathrm{pert},\\rho}\\,-\\,\\psi_{S})\\;\\stackrel{d}{\\to}\\;\\;N(0,\\nu_{S,\\mathrm{pert},\\rho}^{2})$ , as $n~\\rightarrow~\\infty$ , where $\\nu_{S,\\mathrm{pert},\\rho}^{2}\\ =\\ \\eta_{S}^{2}\\,+\\,\\rho^{2}$ , and $\\eta_{S}^{2}\\;=\\;E[\\{\\phi(Z)\\:-\\:\\phi_{S}(Z)\\}^{2}]$ . Following the plug-in principle, we employ the normalized test statistic ", "page_idx": 18}, {"type": "equation", "text": "$$\nT_{\\mathrm{pert},\\rho}:=n^{1/2}\\phi_{n,S,\\mathrm{pert},\\rho}/\\nu_{n,S,\\mathrm{pert},\\rho},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which converges in distribution to $N(0,1)$ under $H_{0}$ for any $\\rho\\quad>\\quad0$ . Here $\\begin{array}{r l r}{\\nu_{n,S,\\mathrm{pert},\\rho}^{2}}&{{}=}&{K^{-1}\\sum_{k=1}^{K}\\eta_{k,n,S}^{2}\\;+\\;\\rho^{2}}\\end{array}$ , where for each $\\begin{array}{r l r}{k}&{{}\\in}&{\\{1,\\ldots,K\\}}\\end{array}$ , $\\begin{array}{r l}{\\eta_{k,n,S}^{2}}&{{}=}\\end{array}$ $\\begin{array}{r}{n_{k}{^{-1}}\\sum_{i:Z_{i}\\in\\mathcal{D}_{k}}\\{\\dot{\\mathbb{C}}(f_{k,n},P_{k,n};\\delta_{Z_{i}}\\mathrm{~-~}P_{k,n})\\mathrm{~-~}\\dot{\\mathbb{C}}(f_{k,n,S},P_{k,n};\\delta_{Z_{i}}\\mathrm{~-~}P_{k,n})\\}^{2}}\\end{array}$ . For a prespecified significance level $\\alpha\\in(0,1)$ , this method rejects $H_{0}$ if $T_{\\mathrm{pert},\\rho}>z_{1-\\alpha}$ . ", "page_idx": 18}, {"type": "text", "text": "Indeed, a direct correspondence emerges between the slider parameter $\\tau$ within our Zipper and the perturbation parameter $\\rho$ employed in the data perturbation technique, thereby facilitating a comparative analysis of the two methodologies. To elucidate this connection, we demonstrate that, under $H_{1}:\\psi_{S}>0$ , the power function corresponding to the data perturbation method is ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}(T_{\\mathrm{pert},\\rho}>z_{1-\\alpha}\\mid H_{1})}\\\\ &{=\\operatorname*{Pr}\\left(\\frac{n^{1/2}(\\phi_{n,\\mathcal{S},\\mathrm{pert},\\rho}-\\psi_{\\mathcal{S}})}{\\nu_{\\mathcal{S},\\mathrm{pert},\\rho}}>z_{1-\\alpha}-\\frac{n^{1/2}\\psi_{\\mathcal{S}}}{\\nu_{\\mathcal{S},\\mathrm{pert},\\rho}}\\mid H_{1}\\right)}\\\\ &{=\\Phi\\left(-z_{1-\\alpha}+\\frac{n^{1/2}\\psi_{\\mathcal{S}}}{\\nu_{\\mathcal{S},\\mathrm{pert},\\rho}}\\right)+o(1)}\\\\ &{:=G_{\\mathcal{S},n,\\alpha,\\mathrm{pert}}(\\rho)+o(1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "First notice that our Zipper method with $\\tau=0$ (i.e., the vanilla sample-splitting) has approximate power ", "page_idx": 19}, {"type": "equation", "text": "$$\nG_{S,n,\\alpha}(0)=\\Phi\\left(-z_{1-\\alpha}+\\frac{(n/2)^{1/2}\\psi_{S}}{(\\sigma^{2}+\\sigma_{S}^{2})^{1/2}}\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We establish an upper bound for the perturbation parameter $\\rho$ , namely, $\\rho^{2}\\leq_{\\!\\circ}2(\\sigma^{2}+\\sigma_{S}^{2})-\\eta_{S}^{2}$ , where equality yields $\\overbar{G_{S,n,\\alpha,\\mathrm{pert}}}(\\rho)=\\bar{G_{S,n,\\alpha}}(0)$ . Should $\\rho^{2}>\\dot{2}(\\sigma^{2}+\\bar{\\sigma_{S}^{2}})-\\eta_{S}^{2}$ , it becomes evident that $G_{S,n,\\alpha,\\mathrm{pert}}(\\rho)\\leq G_{S,n,\\alpha}(0)$ , suggesting that the data perturbation technique may even exhibit lower power. ", "page_idx": 19}, {"type": "text", "text": "The idea is to establish a relationship between $\\tau$ and $\\rho$ such that both methods yield similar power, given their valid size control for fixed $\\tau$ and $\\rho$ . Employing a consistent variance estimator as per Remark 2.9, our Zipper method exhibits an approximate power of ", "page_idx": 19}, {"type": "equation", "text": "$$\nG_{S,n,\\alpha}^{\\mathrm{(CI)}}(\\tau)=\\Phi\\left(-z_{1-\\alpha}+\\frac{\\{n/(2-\\tau)\\}^{1/2}\\psi_{S}}{\\nu_{S,\\tau}}\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By setting $G_{S,n,\\alpha}^{(\\mathrm{CI})}(\\tau)=G_{S,n,\\alpha,\\mathrm{pert}}(\\rho)$ , we obtain ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\rho^{2}=(2-\\tau)(1-\\tau)(\\sigma^{2}+\\sigma_{S}^{2})-(1-\\tau)^{2}\\eta_{S}^{2},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "establishing a one-to-one correspondence. Under this correspondence, ", "page_idx": 19}, {"type": "equation", "text": "$$\nG_{S,n,\\alpha,\\mathrm{pert}}(\\rho)=G_{S,n,\\alpha}^{(\\mathrm{CI})}(\\tau)\\leq G_{S,n,\\alpha}(\\tau),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the latter denotes the approximate power of our Zipper method utilizing the proposed variance estimation scheme. Thus, our Zipper method can be regarded as a data-adaptive perturbation strategy that circumvents external randomization while potentially offering power enhancement owing to the variance estimation scheme. ", "page_idx": 19}, {"type": "text", "text": "Figure 4 illustrates the empirical power comparison between Zipper and the data perturbation method, where the perturbation parameter $\\rho=\\rho(\\tau)$ is determined by (8), across various values of $\\tau$ . This assessment is conducted according to the low-dimensional normal response scenario outlined in Section 3, particularly with $p=5$ and $n=500$ . As anticipated, Zipper and the perturbation method demonstrate the same power when $\\tau=0$ , while for $\\tau\\in(0,1)$ , Zipper consistently exhibits superior power compared to the perturbation method. ", "page_idx": 19}, {"type": "image", "img_path": "ahvOhPkkMx/tmp/94799e831e6c6ef84e4cd524644881d5f145bbe93d64ce4bb63b494527a3bc36.jpg", "img_caption": ["Method \u2014 Zipper -- Perturbation "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 4: Empirical size and power comparison of Zipper and data perturbation method as a function of $\\tau$ . The dot-dashed horizontal line represents the intercept at $\\alpha=5\\%$ . ", "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper\u2019s contributions and scope are specifically claimed in the abstract and introduction, and a single subsection 1.4 fully summarizes our contributions briefly. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: We discuss our limitations and further developments in Section 4. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We provide our assumptions in Section A and complete and correct proof in Section B in appendix. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We fully describe our settings in synthetic experiments and real-data examples in Section 3, and the information provided is enough to the reproducibility of our main experimental results. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We concretely describe our Zipper algorithm in Section C, and our sythetic data generating procedure in Section 3. The datasets utilized in real-data analysis are sourced from publicly available materials on the internet. We think they are sufficient to reproduce the main experimental results. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: We specify all our training details in Section 3. And we discuss the choice of the hyperparameter $\\tau$ in Section 2.5. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: Our work purpose on statistical inference on algorithm/model agnostic goodness-of-fit tests. The experiments conducted in our paper all focus on the type-I or type-II error of statistical tests, and can be dual to construct confidence interval in application. We further represent standard deviation upon repetitions of our synthetic experiments in Table 3.1.1 and Table 2, and error bars in Figure 2. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 23}, {"type": "text", "text": "\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The computer resources used are briefly provided in Section 3. And the time of execution of our method depend on the algorithm/model chosen in ftiting the model, so it varies among different application scenarios. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We briefly reviewed of the NeurIPS Code of Ethics, and make sure that our research conducted in the paper conform with the NeurIPS Code of Ethics in every respect. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our work conducts a general framework of algorithm/model agnostic inference on goodness-of-fit in regression. This is a fundamental research on statistical methodology and has no direct societal impacts as far as we currently understand. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The datasets used in real-data examples are all open-source datasets, and we properly cite the original paper that produced the dataset. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]