[{"type": "text", "text": "A generalized neural tangent kernel for surrogate gradient learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "LukeEilers\\* Department of Physiology, University of Bern, Switzerland Institute for Applied Mathematics, University of Bonn, Germany luke.eilers@unibe.ch ", "page_idx": 0}, {"type": "text", "text": "Raoul-Martin Memmesheimer Institute of Genetics, University of Bonn, Germany rm.memmesheimer@uni-bonn.de ", "page_idx": 0}, {"type": "text", "text": "SvenGoedeke Bernstein Center Freiburg, University of Freiburg, Germany Institute of Genetics, University of Bonn, Germany sven.goedeke@bcf.uni-freiburg.de ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "State-of-the-art neural network training methods depend on the gradient of the network function. Therefore, they cannot be applied to networks whose activation functions do not have useful derivatives, such as binary and discrete-time spiking neural networks. To overcome this problem, the activation function's derivative is commonly substituted with a surrogate derivative, giving rise to surrogate gradient learning (SGL). This method works well in practice but lacks theoretical foundation. The neural tangent kernel (NTK) has proven successful in the analysis of gradient descent. Here, we provide a generalization of the NTK, which we call the surrogate gradient NTK, that enables the analysis of SGL. First, we study a naive extension of the NTK to activation functions with jumps, demonstrating that gradient descent for such activation functions is also ill-posed in the infinite-width limit. To address this problem, we generalize the NTK to gradient descent with surrogate derivatives, i.e., SGL. We carefully define this generalization and expand the existing key theorems on the NTK with mathematical rigor. Further, we illustrate our findings with numerical experiments. Finally, we numerically compare SGL in networks with sign activation function and finite width to kernel regression with the surrogate gradient NTK; the results confirm that the surrogate gradient NTK provides a good characterization ofSGL. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Artificial neural networks (ANNs) originate from the biologically inspired perceptron [Rosenblatt, 1958]. While the perceptron has a binary output that is faithful to the all-or-none behavior of spiking neurons in the nervous system, most activation functions considered nowadays for ANNs are smooth (like the logistic function) or at least semi-differentiable (like the ReLU function). Differentiable network functions enable the learning of network weights with methods that leverage the gradient of the network function with respect to the network weights like backpropagation [Rumelhart et al., ", "page_idx": 0}, {"type": "text", "text": "1986]. These methods are very successful [LeCun et al., 2015], but cannot be used without welldefined gradients. ", "page_idx": 1}, {"type": "text", "text": "This is a problem when considering more biologically plausible ANNs, which are typically used in computational neuroscience to understand how the networks of spiking neurons in our nervous system work. These include spiking neural networks (SNNs). Motivated by the energy efficiency of our brain, SNNs and similar networks, such as binary neural networks (BNNs), are considered in the context of neuromorphic computing [Merolla et al., 2014]. Both discrete-time SNNs and BNNs have in common that their activation functions do not have useful derivatives, which renders standard gradient-descent training impossible [Neftci et al., 2019, Taherkhani et al., 2020, Tavanaei et al., 2019, Roy et al., 2019, Pfeiffer and Pfeil, 2018]. For the scope of this paper, these activation functions can be thought of as step-like functions like the sign function, in which cases the derivative vanishes almost everywhere and is thus no longer informative about the shape of the activation function. ", "page_idx": 1}, {"type": "text", "text": "Surrogate gradient learning resolves this issue by providing the missing information about the activation function in the form of a surrogate derivative [Hinton, 2012, Bengio et al., 2013, Zenke and Ganguli, 2018]. As a result, the gradient-based methods for classical ANNs can be leveraged with great success [Zenke and Vogels, 2021]. However, a theoretical basis underpinning the intuition is missing and it is often unclear which surrogate derivative should be chosen for a particular network. For a review focusing on surrogate gradient learning methods, which we are most interested in, see Neftci et al. [2019]. For a comprehensive review of other learning methods for SNNs, we refer to Taherkhani et al. [2020], Tavanaei et al. [2019], and Eshraghian et al. [2021]. ", "page_idx": 1}, {"type": "text", "text": "The neural tangent kernel (NTK) introduced by Jacot et al. [2018] allows to formulate gradient descent as a kernel method. Just as ANNs with randomly initialized weights converge under certain conditions to Gaussian processes (GPs) in the infinite-width limit [Matthews et al., 2018, Lee et al., 2018], the NTK converges at initialization and during training to a deterministic kernel in the same limit. Moreover, the NTK then describes how the network function changes under gradient descent in the infinite-width regime. This led to both a better theoretical understanding of gradient descent and practical applications to neural network training; see Section 1.2 for more details. ", "page_idx": 1}, {"type": "text", "text": "1.1 Contribution ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We study the NTK for networks with sign function as activation function. As the NTK theory is not directly applicable due to an ill-defined derivative, we consider the NTK for a sequence of activation functions that approximates the sign function and then derive a principled way of generalizing the NTK to gain more theoretical insight into surrogate gradient learning. Our contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "\u00b7 We provide a clear definition of the infinite-width limit in Section 2.2, capturing the different notions used in the literature due to the different choices of rates at which the layer widths increase. This definition is used consistently in all mathematical statements and the respective proofs.   \n\u00b7 In Section 2.3, we demonstrate that the direct extension of the NTK for the sign activation function using an approximating sequence is not well-defined due to the divergence of the kernel's diagonal. This illustrates, from the NTK perspective, that gradient descent is ill-defined for activation functions with jumps and how this problem will be mitigated by surrogate gradient learning. Moreover, we connect this divergence to results by Radhakrishnan et al. [2023] in Theorem 2.3 and show that the direct extension of the NTK can be seen as a well-defined kernel for classification.   \n\u00b7 We define a generalized version of the NTK in Section 2.4 using so-called quasi-Jacobian matrices and prove the convergence to a deterministic, in general asymmetric, kernel in the infinite width limit at initialization in Theorem 2.4. Using the generalized NTK, we formulate surrogate gradient learning in terms of the generalized NTK for networks with differentiable activation functions. This novel NTK is named the SG-NTK and we prove its convergence to a deterministic kernel during training in Theorem 2.5.   \n\u00b7 For both the diverging direct extension of the NTK and the SG-NTK with sign activation function and arbitrary surrogate derivative, we derive exact analytical expressions in sections D.1, D.2 and E.2. In particular, we identify the terms that emerge from SGL and that prevent divergence.   \n\u00b7 In Section 3, we illustrate our findings, in particular Theorem 2.4 and Theorem 2.5, using simulations. Numerical experiments show that the distribution of networks trained with SGL shows agreement with the distribution given by the SG-NTK. ", "page_idx": 1}, {"type": "text", "text": "Mathematically precise versions of all statements can be found in the appendix, which is selfcontained to ensure rigor and a consistent notation throughout all theorems and proofs. ", "page_idx": 2}, {"type": "text", "text": "1.2  Related work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The neural tangent kernel.  The convergence of randomly initialized ANNs in the infinite-width limit to Gaussian processes under appropriate scaling of the weights has first been described by Neal [1996] for a single hidden layer and has been extended to multiple hidden layers and other network architectures like CNNs [Matthews et al., 2018, Garriga-Alonso et al., 2018, Yang, 2019b, Lee et al., 2018]. The NTK was introduced by Jacot et al. [2018] with first results on its convergence at network initialization and during training. Theoretical results on the implication of the convergence of NTKs on the behaviour of trained wide ANNs were given by Lee et al. [2019], Arora et al. [2019], Allen-Zhu et al. [2019]. In Section C.2, we review central theorems on the NTK to enable a clear comparison to our theoretical results. ", "page_idx": 2}, {"type": "text", "text": "The NTK has been generalized to all kinds of ANN standard architectures Yang [2020] such as CNNs Arora et al. [2019] and attention layers Hron et al. [2020]. In particular, it has been generalized to RNNs Alemohammad et al. [2020], which are particularly interesting in light of SNNs due to their shared temporal dimension. Bordelon and Pehlevan [2022] derive a generalization of the NTK called effective NTK for different learning rules using an approach similar to ours. Note that all of these extensions require well-defined gradients. ", "page_idx": 2}, {"type": "text", "text": "The ability of overparameterized neural networks to converge during training and to generalize can be explained using the NTK [Allen-Zhu et al., 2019, Bordelon et al., 2020, Bietti and Mairal, 2019, Du et al., 2019]. The NTK has also been used in more applied areas such as neural architecture search [Chen et al., 2021] and dataset distillation [Nguyen et al., 2021]. ", "page_idx": 2}, {"type": "text", "text": "Surrogate gradient learning.  In the context of computational neuroscience, the idea of replacing the derivative of the output of a spiking neuron with a surrogate derivative was introduced by Bohte [2011]. To deal with the temporal component in SNNs or more generally RNNs, the resulting gradient is usually combined with backpropagation through time (BPTT). Prominent examples of surrogate gradient approaches include SuperSpike by Zenke and Ganguli [2018] and a number of works with different surrogate derivatives [Wu et al., 2018, 2019, Shrestha and Orchard, 2018, Bellec et al., 2018, Esser et al., 2016, Wozniak et al., 2020]. In the general ANN literature, the method is better known as straight-through estimation (STE) and was introduced by Hinton [2012] and by Bengio et al. [2013] in more detail. It was successfully applied by Hubara et al. [2016] and Cai et al. [2017]. ", "page_idx": 2}, {"type": "text", "text": "Surrogate gradient learning or STE is only heuristically motivated and it is hence desirable to derive a theoretical basis. The influence of different surrogate derivatives on the method has been analyzed through systematic numerical simulations Zenke and Vogels [2021], revealing that the particular shape has a minor impact compared to the scale. In a more theoretical work, Yin et al. [2019] analyzed the convergence of STE for a Heaviside activation function with three different surrogate derivatives and found that the descent directions of the respective surrogate gradients reduce the population loss when the clipped ReLU function is used as surrogate derivative. Gygax and Zenke [2024] examine how SGL is connected to smoothed probabilistic models [Bengio et al., 2013, Neftci et al., 2019, Jang et al., 2019] and to stochastic automatic differentiation [Arya et al., 2022]. In particular, they consider SGL for differentiable activation functions, as we do in our derivation of the SG-NTK. ", "page_idx": 2}, {"type": "text", "text": "2 Theoretical results ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1  Notation and NTK parametrization ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We consider multilayer perceptrons with so-called neural tangent kernel parametrization. For a network with depth $L$ ,layer width $n_{l}$ for $0\\ \\le\\ l\\ \\le\\ L$ , activation function $\\sigma$ ,weight matrices W(l) E Rn xnt-1, and biases b() E Rnu, the preactivations with NTK parametrization are given by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{h^{\\left(1\\right)}\\left(x\\right)=\\displaystyle\\frac{\\sigma_{w}}{\\sqrt{n_{0}}}W^{\\left(1\\right)}x+\\sigma_{b}\\,b^{\\left(1\\right)},}}\\\\ {{h^{\\left(l+1\\right)}\\left(x\\right)=\\displaystyle\\frac{\\sigma_{w}}{\\sqrt{n_{l}}}W^{\\left(l+1\\right)}\\sigma\\left(h^{\\left(l\\right)}\\left(x\\right)\\right)+\\sigma_{b}\\,b^{\\left(l+1\\right)}\\quad\\mathrm{for}\\;l=1,\\ldots,L-1,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\sigma_{w}>0$ $\\sigma_{b}\\geq0$ and we initialize $W_{i j}^{(l)},b_{i}^{(l)}\\overset{\\mathrm{iid}}{\\sim}\\mathcal{N}(0,1)$ for ll $i,j$ The NTK parametrization differs from the standard parametrization by a rescaling factor of $1/\\sqrt{n_{l}}$ in layer $l+1$ . The network function is then given by $f(\\,\\cdot\\,)=h^{(L)}(\\,\\cdot\\,)\\colon\\mathbb{R}^{n_{0}}\\to\\mathbb{R}^{n_{L}}$ and notably the last layer is a preactivation layer. We denote the total number of weights by $P$ . More details can be found in Section C.1. ", "page_idx": 3}, {"type": "text", "text": "Notation (see Remark C.1 and C.2)  By default, we will interpret any vector as a column vector, i.e., we identify $\\mathbb{R}^{n}$ with $\\mathbb{R}^{n\\times1}$ . This is the case even when writing ${\\dot{x}}\\,=\\,(x_{1},\\dots,x_{n})\\,\\in\\,\\mathbb{R}^{n}$ for handier notation. Row_ vectors will be indicated within calculations using the transpose operator, $x^{\\mathsf{T}}$ . For a function $f\\colon\\mathbb{R}^{n_{1}}\\rightarrow\\mathbb{R}^{n_{2}}$ and $\\mathcal{X}=(x_{1},...\\,,x_{d})\\in\\mathbb{R}^{d\\cdot n_{1}}$ , we define the vector $\\bar{f}(\\mathcal{X}):=$ $(f(x_{1}),\\cdot\\cdot\\cdot\\cdot,f(x_{d}))\\,\\in\\,\\mathbb{R}^{d\\cdot n_{2}}$ . For $n_{2}\\,=\\,1$ , we denote the gradient of $f$ by $\\nabla f(x)\\,\\in\\,\\mathbb{R}^{n_{1}}$ for all $\\boldsymbol{x}\\in\\mathbb{R}^{n_{1}}$ . If $n_{2}>1$ , we denote the Jacobian matrix of $f$ by $J f\\colon\\mathbb{R}^{n_{1}}\\rightarrow\\mathbb{R}^{n_{2}\\times n_{1}}$ . A subscript of the form $J_{\\theta}f(x)$ denotes Jacobian matrices with respect to a subset of variables. We write $f(n)\\gtrsim g(n)$ to denote that $f(n)$ and $g(n)$ are asymptotically proportional, i.e., $f(n)\\sim K g(n)$ for some constant $K\\neq0$ ", "page_idx": 3}, {"type": "text", "text": "2.2   The infinite-width limit ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We will consider neural networks in the limit of infinitely many hidden layer neurons, i.e., $n_{l}\\rightarrow\\infty$ for all $1\\le l\\le L-1$ . We will see later, when paraphrasing existing results from the literature, that different ways of taking the number of hidden neurons to infinity can be found. To formalize these notions, we use the definition of a width function from Matthews et al. [2018] with slight modifications: ", "page_idx": 3}, {"type": "text", "text": "Definition 2.1 (Width function). For every layer $l=0,\\dots,L$ and any $m\\in\\mathbb{N}$ thenumberofneurons inthatlayerisgivenby $r_{l}(m)$ , and we call $r_{l}\\colon\\mathbb{N}\\rightarrow\\mathbb{N}$ thewidthfunctionof layer $l$ .We say that $a$ widthfunction $r_{\\mathit{l}}$ isstrictlyincreasingif $r_{l}(m)<r_{l}(m+1)$ for all $m\\geq1$ . We set ", "page_idx": 3}, {"type": "text", "text": "the set of collections of strictly increasingwidth functions for network depth $L$ ", "page_idx": 3}, {"type": "text", "text": "Every element of $\\mathcal{R}_{L}$ provides a way to take the widths of the hidden layers to infinity by setting $n_{l}=r_{l}(m)$ for any $1\\leq l<L$ and considering $m\\rightarrow\\infty$ . The notions of infinite-width limits used in the literature will now correspond to classes $R\\subseteq\\mathcal{R}_{L}$ for which the respective limiting statements hold. This is captured in the following definition. ", "page_idx": 3}, {"type": "text", "text": "Definition 2.2 (Types of infinite-width limits). Consider a statement $\\boldsymbol{S}$ of theform\u201cLet anANN have depth $L$ and network layer widths defined by $n_{0},\\,n_{L}$ , and $n_{l}:=r_{l}(m)$ for $1\\leq l<L$ and some $(r_{l})_{l=1}^{L-1}\\in\\mathcal{R}_{L}$ Then, for fixed $n_{0}$ and any $n_{L}$ the statement $\\mathcal{P}$ holds as $m\\rightarrow\\infty$ \" We also write the statement $\\boldsymbol{S}$ as $S(r)$ ", "page_idx": 3}, {"type": "text", "text": "(i) We say that such a statement $\\boldsymbol{S}$ holds strongly, if $S(r)$ holds for any $r\\,\\in\\,\\mathcal{R}_{L}$ .This can be interpreted as requiring that the statement holds as $\\mathrm{min}_{1\\le l<L}(n_{l})\\to\\infty$ .We will also write $\\mathcal{P}$ holds as $n_{1},\\ldots,n_{L-1}\\to\\infty$ strongly'\".   \n(i) We say that such a statement $\\boldsymbol{S}$ holds for $(n\\iota)_{1\\leq l\\leq L-1}$ $n_{i}$ if $\\boldsymbol{S}$ holds for all $r\\in\\mathcal{R}_{L}$ with $r_{l}(m)\\,\\stackrel{\\propto}{\\sim}\\,m$ for all $1\\,\\le\\,l\\,<\\,L$ This means that $S(r)$ holds for all $r\\,\\in\\,\\mathcal{R}_{L}$ such that $r_{p}(m)/r_{q}(m)\\to\\alpha_{p,q}\\in(0,\\infty)$ as $m\\rightarrow\\infty$ We will also write \u201c $\\mathcal{P}$ holds as $(n_{l})_{1\\leq l<L}\\not\\approx n^{,\\prime}$   \n(ii) We say that such a statement $\\boldsymbol{S}$ holds weakly, if $\\boldsymbol{S}$ holds for at least one $r\\,\\in\\,\\mathcal{R}_{L}$ .We will also write $^{\\ast\\ast}\\mathcal{P}$ holds as $n_{1},\\ldots,n_{L-1}\\to\\infty$ weakly\". This type of infinite-width limit is tightly connected to the sequential infinite-width limit. ", "page_idx": 3}, {"type": "text", "text": "Remark 2.1 (Connection between weak and sequential infinite-width limits). In the sequential infinite-width limit,meaning that $n_{1}\\rightarrow\\infty,\\ldots,n_{L-1}\\rightarrow\\infty$ sequentially, the layer widths are not strictly finite. This is opposed to applications, where layer widths may be large but finite. Hence, the infinite-width limits using width functions as explained above are more meaningful in practice. For a sequential limit $\\begin{array}{r}{\\operatorname*{lim}_{n_{1}\\to\\infty}\\operatorname*{lim}_{n_{2}\\to\\infty}f(n_{1},n_{2})=f^{*}.}\\end{array}$ we can find functions $n_{1}(m)$ and $n_{2}(m)$ such that $\\begin{array}{r}{\\operatorname*{lim}_{m\\rightarrow\\infty}f(n_{1}(m),n_{2}(m))=f^{*}}\\end{array}$ .However, the rate at which $n_{1}(m),n_{2}(m)$ diverge as $m\\rightarrow\\infty$ cannot generally be controlled. Since the weak infinite-width limit allows for arbitrary rates, any sequential infinite-width limit can hence be turned into a weak infinite-width limit as defined in Definition 2.2 (iii). ", "page_idx": 3}, {"type": "text", "text": "We use Definition 2.2 to paraphrase the convergence of ANNs to GPs in the infinite-width limit: ", "page_idx": 4}, {"type": "text", "text": "Theorem 2.1 (Theorem 4 from Matthews et al. [2018]). Any network function $f$ of depth $L$ defined as in Section 2.1 with continuous activation function $\\sigma$ that satisfies the linear envelope property, i.e., there exist $c,m\\,\\geq\\,0$ with $|\\sigma(u)|\\,\\leq\\,c+m|u|$ for all $u\\ \\in\\ \\mathbb{R},$ .converges in distribution as $n_{1},\\ldots,n_{L-1}\\to\\infty$ strongly to a mulidimensional Gaussian proces $(X_{j})_{j=1}^{n_{L}}$ for any fxed countable input set $(x_{i})_{i=1}^{\\infty}$ It holds $\\boldsymbol{X_{j}}\\overset{i i d}{\\sim}\\mathcal{N}(\\boldsymbol{0},\\boldsymbol{\\Sigma}^{(L)})$ where the covariance function $\\Sigma^{(L)}$ is recursively given by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Sigma^{(1)}(x,x^{\\prime})=\\frac{\\sigma_{w}^{2}}{n_{0}}\\langle x,x^{\\prime}\\rangle+\\sigma_{b}^{2},\\quad\\Sigma^{(L)}(x,x^{\\prime})=\\sigma_{w}^{2}\\operatorname{\\mathbb{E}}_{g\\sim N(0,\\Sigma^{(L-1)})}[\\sigma(g(x))\\,\\sigma(g(x^{\\prime}))]+\\sigma_{b}^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We also write $\\Sigma^{(L)}=\\Sigma_{\\sigma}$ . The theorem states that the distribution of the network function, which is given by its randomly initialized weights, approaches the distribution of independent GPs as the hidden layer widths increase. Hence, a large-width network will approximately be a realization of the respective GPs. Note that this result can be generalized to non-continuous activation functions without well-defined derivatives that fulfil the linear envelope property, like $\\sigma(z)=\\mathrm{sign}(z)$ We provide a rigorous proof of this kind of generalization for the case $n_{1},\\ldots,n_{L-1}\\to\\infty$ weakly in Theorem E.3. $\\Sigma_{\\sigma}$ remains well-defined in this case since the expectation in Equation (1) does. When approximating the sign function with scaled error function, i.e., $\\sigma(z)=\\operatorname{erf}_{m}(z):=\\operatorname{erf}(m\\cdot z)$ ,it holds that $\\operatorname*{lim}_{m\\to\\infty}\\Sigma_{\\mathrm{erf}\\,_{m}}=\\Sigma_{\\mathrm{sign}}$ due to the dominated convergence theorem. ", "page_idx": 4}, {"type": "text", "text": "2.3 Direct extension of the neural tangent kernel in the infinite-width limit ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We consider a dataset $\\boldsymbol{\\mathcal{D}}=(\\mathcal{X},\\mathcal{Y})$ with $\\mathcal{X}=(x_{1},...\\,,x_{d})\\in\\mathbb{R}^{d\\cdot n_{0}}$ and $\\mathcal{Y}=(y_{1},\\ldots,y_{d})\\in\\mathbb{R}^{d\\cdot n_{L}}$ To solve the regression problem, i.e., to find weights $\\textit{\\textbf{\\textit{\\theta}}}\\in\\;\\mathbb{R}^{P}$ such that $f(x_{i};\\theta)\\;=\\;y_{i}$ for all $i=1,\\ldots,d$ , we apply gradient descent in continuous time, also know as gradient fow, with learning rate $\\eta$ and lossfunction $\\mathcal{L}\\colon\\mathbb{R}^{d\\cdot n_{L}}\\times\\mathbb{R}^{d\\cdot n_{L}}\\rightarrow\\mathbb{R}$ . This means that, using the chain rule, the learning rule can then be written as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\theta_{t}=-\\eta\\,\\nabla_{\\theta}\\mathcal{L}\\big(f(\\mathcal{X};\\theta_{t});\\mathcal{Y}\\big)=-\\eta\\,J_{\\theta}f(\\mathcal{X};\\theta_{t})^{\\top}\\,\\nabla_{f(\\mathcal{X};\\theta_{t})}\\mathcal{L}\\big(f(\\mathcal{X};\\theta_{t});\\mathcal{Y}\\big).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "To derive the NTK, we observe that the network function then evolves according to ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\mathrm{d}}{\\mathrm{d}t}f(x;\\theta_{t})=J_{\\theta}f(x;\\theta_{t})\\,\\frac{\\mathrm{d}}{\\mathrm{d}t}\\theta_{t}\\stackrel{(2)}{=}-\\eta\\,J_{\\theta}f(x;\\theta_{t})J_{\\theta}f(\\boldsymbol\\chi;\\theta_{t})^{\\intercal}\\,\\nabla_{f(\\boldsymbol\\chi;\\theta_{t})}\\mathcal{L}\\big(f(\\boldsymbol\\chi;\\theta_{t});\\boldsymbol\\chi\\big)}\\\\ &{\\qquad\\qquad\\quad=:-\\eta\\,\\hat{\\Theta}_{t}(x,\\boldsymbol\\chi)\\nabla_{f(\\boldsymbol\\chi;\\theta_{t})}\\,\\mathcal{L}\\big(f(\\boldsymbol\\chi;\\theta_{t});\\boldsymbol\\chi\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where we implicitly defined the empirical neural tangent kernel as $\\hat{\\Theta}(x,x^{\\prime}):=J_{\\theta}f(x;\\theta)J_{\\theta}f(x^{\\prime};\\theta)^{\\top}$ which depends on the current weights $\\theta=\\theta_{t}$ . This means that the learning dynamics of the network functions during training are given by a kernel whose entries are the scalar products between the gradients of the network's output neuron activity, $\\hat{\\Theta}_{i\\,j}(x,x^{\\prime})=\\langle\\nabla_{\\theta}f_{i}(x;\\theta),\\nabla_{\\theta}f_{j}(x^{\\prime};\\theta)\\rangle$ .The key result on the NTK is that the empirical NTK converges in the infinite-width limit to a constant kernel, the analytic NTK, at initialization, $\\theta=\\theta_{0}$ , and during training, $\\theta=\\theta_{t}$ ", "page_idx": 4}, {"type": "text", "text": "Theorem 2.2 (Theorem 1 from Jacot et al. [2018] for general $\\sigma_{w}>0$ 0.For any network function of depth $L$ defined as in Section 2.1 with Lipschitz continuous activation function $\\sigma$ $\\hat{\\Theta}\\stackrel{.}{=}\\hat{\\Theta}^{(L)}$ converges in probability to a constant kernel $\\dot{\\Theta}^{(L)}\\otimes\\mathrm{I}_{n_{L}}$ as $n_{1},\\ldots,n_{L-1}\\to\\infty$ weakly. This means that for all $x,x^{\\prime}\\in\\mathbb{R}^{n_{0}}$ and $1\\leq i,j\\leq n_{L}$ itholds $\\hat{\\Theta}_{i\\,j}^{(L)}(x,x^{\\prime})\\,\\to\\,\\delta_{i j}\\,\\Theta^{(L)}(x,x^{\\prime})$ in probabity, where $\\delta_{i j}$ denotes the Kronecker delta. We call $\\Theta^{(L)}$ the analytic neural tangent kernel of the network, which is recursively given by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Theta^{(1)}(x,x^{\\prime})=\\Sigma^{(1)}(x,x^{\\prime}),\\quad\\Theta^{(L)}(x,x^{\\prime})=\\Sigma^{(L)}(x,x^{\\prime})+\\Theta^{(L-1)}(x,x^{\\prime})\\cdot\\dot{\\Sigma}^{(L)}(x,x^{\\prime}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\Sigma^{(l)}$ are defined as in Theorem 2.1 and we define ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\dot{\\Sigma}^{(L)}(x,x^{\\prime})=\\sigma_{w}^{2}\\,\\mathbb{E}_{g\\sim\\mathcal{N}(0,\\Sigma^{(L-1)})}\\left[\\dot{\\sigma}(g(x))\\,\\dot{\\sigma}(g(y))\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We also write $\\Theta^{(L)}=\\Theta_{\\sigma}$ $\\theta_{t}$ are the weights during gradien fwlearming at time $t\\geq0$ as before, Theorem 2.2 shows that $\\hat{\\Theta}_{t}^{(L)}\\,\\rightarrow\\,\\Theta^{(L)}\\otimes\\mathrm{I}_{n_{L}}$ for $t\\,=\\,0$ in the infnite-width limit. This reveals that the gradients of the output neurons, $\\nabla_{\\theta}f_{i}(x;\\theta)$ , are mutually orthogonal. Under additional assumptions this convergence also holds for the whole training duration, $t\\,>\\,0$ ,see Theorem 2 of Jacot et al. [2018] for the case $n_{1},\\ldots,n_{L-1}\\to\\infty$ weakly, and Chapter G of Lee et al. [2019] for $(n_{l})_{1\\leq l<L}\\ \\underset{\\sim}{\\propto}\\ n$ . Hence, the kernel that describes the learning dynamics stays constant in the infinite-width limit. This implies that the distribution of the network function during training also converges to GPs [Lee et al., 2019, Theorem 2.2]. Then, any output neuron after training has mean $m(x)\\stackrel{-}{=}\\Theta^{(L)}(x,\\lambda)\\Theta^{(L)}(\\lambda,\\lambda)^{-1}\\ y$ under the asumption of a mean squared error MSE) loss, see Section C.2.1. The expression for the mean is equivalent to kernel regression with the NTK. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "The above results show that gradient flow for networks with randomly initialized weights is characterized by the analytic NTK $\\Theta^{(L)}$ in the infinite-width limit. Since we are interested in gradient fow for networks with activation functions inadequate for gradient descent training, we want to know whether the analytic NTK can be extended to such activation functions.We see that the derivative of the activation function does not have to be defined point-wise for Equation (4). In particular, activation functions with distributional derivatives, e.g., $\\begin{array}{r}{\\frac{\\mathrm{\\nabla}}{\\mathrm{d}z}\\mathrm{sign}(z)=2\\,\\delta(\\dot{z})}\\end{array}$ can be taken into consideration, where $\\delta$ denotes the Dirac delta distribution. If we again approximate the sign function with scaled error functions $\\operatorname{erf}_{m}$ \uff0c $m\\in\\mathbb{N}$ , it holds ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}_{g\\sim\\mathcal{N}(0,\\Sigma_{\\mathrm{erf}_{m}})}\\left[\\mathrm{eif}_{m}(g(x))\\,\\mathrm{eif}_{m}(g(y))\\right]\\xrightarrow{m\\rightarrow\\infty}\\mathbb{E}_{g\\sim\\mathcal{N}(0,\\Sigma_{\\mathrm{sign}})}\\left[2\\,\\delta(g(x))\\cdot2\\,\\delta(g(y))\\right],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "in case the right-hand side exists. A rigorous analysis of this limit can be found in Section D.1. Heuristically, a simple observation suffices: if $x=y$ , we have a one-dimensional integral over two delta distributions, which yields infinity. On the other hand, if $x\\neq y$ and $\\Sigma_{\\mathrm{sign}}$ is non-degenerate, a two-dimensional integral over two delta distributions yields a finite value. We derive analytic expressions for $\\operatorname*{lim}_{m\\rightarrow\\infty}\\Theta_{\\mathrm{erf}_{m}}\\;=:\\;\\Theta_{\\mathrm{sign}}$ in Lemma D.3. We call this kernel singular, because $\\Theta_{\\mathrm{sign}}(x,x)=\\infty$ and $\\Theta_{\\mathrm{sign}}(x,y)\\in\\mathbb{R}$ if $\\bar{x}\\neq y$ . By the same reasoning, this divergence occurs for any activation function with jumps. Note that for the mean given by kernel regression, this implies $m{\\dot{(}x_{i})}=y_{i}$ and $m(x)=0$ if $x$ is not a training point. Intuitively, this is because the network is initialized with zero mean and different input points are uncorrelated under the singular kernel, so only the training points are learned. A limit kernel with this property also arises if the activation function is fixed but the depth of the network goes to infinity as was shown by Radhakrishnan et al. [2023]. We adopt the ideas of their proof to show that the sign of $m$ is still useful for classification: ", "page_idx": 5}, {"type": "text", "text": "Theorem 2.3 (Inspired by Lemma 5 of Radhakrishnan et al. [2023]; see Theorem D.4). Let $\\sigma_{b}^{2}>0$ orle all $x_{i}\\in\\mathbb{R}^{n_{0}}$ bepairwise nor-parall Let $L\\ge2$ and $x_{i}\\in S_{R}^{n_{0}-1}$ forall $i=1,\\ldots,d_{!}$ where $S_{R}^{n_{0}-1}\\subseteq\\mathbb{R}^{n_{0}}$ is thesphereof radius $R$ Assumingthat $\\Theta_{\\infty}^{(L)}(x,\\mathcal{X})\\mathcal{Y}\\neq0$ for almost all $x\\in S_{R}^{n_{0}-1}$ it holds ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{m\\to\\infty}\\operatorname{sign}\\left(\\Theta_{m}^{(L)}(x,\\boldsymbol{\\chi})\\Theta_{m}^{(L)}(\\boldsymbol{\\chi},\\boldsymbol{\\chi})^{-1}\\boldsymbol{y}\\right)=\\operatorname{sign}\\left(\\Theta_{\\infty}^{(L)}(x,\\boldsymbol{\\chi})\\mathcal{Y}\\right)\\quad a.e.\\ o n\\ S_{R}^{n_{0}-1}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The estimator $\\begin{array}{r}{\\Theta_{\\infty}^{(L)}(x,\\mathcal{X})\\mathcal{Y}=\\sum_{i=1}^{n}\\Theta_{\\infty}^{(L)}(x,x_{i})\\,y_{i}}\\end{array}$ has the form of a so-called Nadaraya- Watson estimator and is well-defned for singular kernels such as $\\Theta^{(L)}$ . If we assume a classifcation problem in the sense that ${\\mathcal{V}}\\subseteq\\{-1,1\\}^{n}$ , it holds $\\mathrm{sign}\\big(\\Theta_{\\infty}^{(L)}(x_{i},\\mathcal{X})\\mathcal{Y}\\big)=\\Theta_{\\infty}^{(L)}(x_{i},\\mathcal{X})\\mathcal{Y}$ at training point $x_{i}$ ", "page_idx": 5}, {"type": "text", "text": "2.4  Generalization of the neural tangent kernel and application to surrogate gradient learning ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The above singularity of the limit kernel can be avoided by considering surrogate gradient learning instead of gradient descent. First, we introduce a generalization of the NTK that later leads to the surrogate gradient NTK. ", "page_idx": 5}, {"type": "text", "text": "By definition and originally due to the chain rule, the empirical NTK consists of two Jacobian matrices of the network function with respect to the weights. The Jacobian matrix can be thought of as a recursive formula, $J_{\\theta}f(x;\\theta)=G(\\bar{x_{,}^{\\prime}}\\theta;\\sigma,\\dot{\\sigma})$ , which is given by the input $x$ and the architecture of the network, including the activation function $\\sigma$ and its derivative $\\dot{\\sigma}$ . This formula can be modified to define a quasi-Jacobian matrix, $J^{\\sigma_{1},\\tilde{\\sigma}_{1}}(x;\\theta):=G(x,\\theta;\\sigma_{1},\\tilde{\\sigma}_{1})$ , where $\\tilde{\\sigma}_{1}$ does not have to be the derivative of $\\sigma_{1}$ . Analogous to the definition of the empirical NTK we define the empirical generalized NTK to be $\\hat{I}(x,x^{\\prime}):=J^{\\sigma_{1},\\tilde{\\sigma}_{1}}(x;\\theta)\\,J^{\\sigma_{2},\\tilde{\\sigma}_{2}}(x^{\\prime};\\theta)^{\\intercal}$ , where $\\sigma_{1},\\tilde{\\sigma}_{1},\\sigma_{2},\\tilde{\\sigma}_{2}$ are specified in the following theorem. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2.4 (Generalized version of Theorem 1 by Jacot et al. [2018]; see Theorem E.4). For activation functions $\\sigma_{1}$ \uff0c $\\sigma_{2}$ and so-called surrogate derivatives $\\tilde{\\sigma}_{1}$ \uff0c $\\tilde{\\sigma}_{2}$ such that $\\sigma_{1},\\sigma_{2},\\tilde{\\sigma}_{1}$ and $\\tilde{\\sigma}_{2}$ satisfy the linear envelope property and are continuous except for finitely many jump points, denote theempiricalgeneralizedneuraltangentkernel ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\hat{I}^{(L)}(x,x^{\\prime})=J^{(L),\\sigma_{1},\\tilde{\\sigma}_{1}}(x;\\theta)\\,J^{(L),\\sigma_{2},\\tilde{\\sigma}_{2}}(x^{\\prime};\\theta)^{\\intercal}\\,\\,\\,\\,\\,f o r\\,\\,x,x^{\\prime}\\in\\mathbb{R}^{n_{0}},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "as before. Then, for any $x,x^{\\prime}\\in\\mathbb{R}^{n_{0}}$ and $1\\leq i,j\\leq n_{L}$ itholds $\\hat{I}_{i j}^{(L)}(x,x^{\\prime})\\stackrel{\\mathcal{P}}{\\longrightarrow}\\delta_{i j}I^{(L)}(x,x^{\\prime})$ as $n_{1},\\ldots,n_{L-1}\\to\\infty$ weakly. We call $I^{(L)}$ the analytic generalized neural tangent kernel, which is recursively given by ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I^{(1)}(x,x^{\\prime})=\\Sigma_{1,2}^{(1)}(x,x^{\\prime}),\\ I^{(L)}(x,x^{\\prime})=\\Sigma_{1,2}^{(L)}(x,x^{\\prime})+I^{(L-1)}(x,x^{\\prime})\\cdot\\tilde{\\Sigma}_{1,2}^{(L)}(x,x^{\\prime})f o r\\,L\\ge2,}\\\\ &{\\Sigma_{1,2}^{(L)}(x,x^{\\prime})=\\sigma_{w}^{2}\\,\\mathbb{E}[\\sigma_{1}(Z_{1})\\,\\sigma_{2}(Z_{2})]+\\sigma_{b}^{2}\\,\\,f o r\\,L\\ge2\\,a n d\\,\\,\\Sigma_{1,2}(x,x^{\\prime})=\\frac{\\sigma_{w}^{2}}{n_{0}}\\langle x,x^{\\prime}\\rangle+\\sigma_{b}^{2},\\,\\,w h e}\\\\ &{\\tilde{\\Sigma}_{1,2}^{(L)}(x,x^{\\prime})=\\sigma_{w}^{2}\\,\\mathbb{E}[\\tilde{\\sigma}_{1}(Z_{1})\\,\\tilde{\\sigma}_{2}(Z_{2})]\\implies a n d\\,\\left(Z_{1},Z_{2}\\right)\\sim\\mathcal{N}\\left(0,\\left(\\Sigma_{1,2}^{(L-1)}(x,x)\\,\\,\\Sigma_{1,2}^{(L-1)}(x,x^{\\prime})\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "$\\Sigma_{1}$ and $\\Sigma_{2}$ denote the covariance functions of the Gaussian processes that arise from network functions $f_{1},f_{2}$ with activation functions $\\sigma_{1},\\sigma_{2}$ , respectively, in the infinite-width limit. The covariance between these two Gaussian processes is denoted by $\\Sigma_{1,2}$ . This covariance function is asymmetric in the sense that $\\mathrm{Cov}[f_{1}(x_{1}),\\bar{f_{2}}(x_{2})]\\neq\\mathrm{Cov}[f_{1}(x_{2}),f_{2}(x_{1})]$ in general. ", "page_idx": 6}, {"type": "text", "text": "We show in Theorem 2.4 that the generalized empirical NTK tends to a generalized analytic NTK at initialization in an infinite-width limit. Now, the SGL rule can be written using the generalized NTK, similar to Equation (3): ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}f(x;\\theta_{t})=J_{\\theta}f(x;\\theta_{t})\\,\\frac{\\mathrm{d}}{\\mathrm{d}t}\\theta_{t}=-\\eta\\,J_{\\theta}f(x;\\theta_{t})J^{\\sigma,\\tilde{\\sigma}}(x;\\theta_{t})^{\\intercal}\\,\\nabla_{f(x;\\theta_{t})}{\\mathcal{L}}(f(X;\\theta_{t});\\mathcal{Y})\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Here, we chose $\\sigma_{1}\\,=\\,\\sigma_{2}\\,=\\,\\sigma$ $\\tilde{\\sigma}_{1}=\\dot{\\sigma}$ and $\\tilde{\\sigma}_{2}=\\tilde{\\sigma}$ in the previous definition of the generalized NTK. This we call the surrogate gradient NTK (SG-NTK) with activation function $\\sigma$ and surrogate derivative $\\tilde{\\sigma}$ . Compared to the classical NTK, one of the true Jacobian matrices is replaced by the quasi-Jacobian matrix, since the learning rule is SGL instead of gradient fow. Note that we assume that the derivative of the activation function, $\\dot{\\sigma}$ , exists. Theorem 2.4 shows convergence at time $t=0$ We extend this convergence to $t>0$ in the following theorem: ", "page_idx": 6}, {"type": "text", "text": "Theorem 2.5 (Based on Theorem G.2 from Lee et al. [2019]; see Theorem E.5). Let $\\sigma,\\dot{\\sigma},\\tilde{\\sigma}$ as before,allLipschitzcontinuous,and $\\dot{\\sigma},\\tilde{\\sigma}$ bounded. Let $f_{t}$ beanetworkfunctionwithdepth $L$ initialized as in Section 2.1 and trained with MSE loss and SGL with surrogate derivative $\\tilde{\\sigma}$ Assume that the generalized NTK converges in probability to the analytic generalized NTK of Theorem 2.4, $\\hat{I}^{(L)}\\stackrel{\\cdot}{\\longrightarrow}I^{(L)}\\otimes\\mathrm{I}_{n_{L}}$ as $(n\\iota)_{l=1}^{L-1}\\,\\approx\\,n$ Furthoreassuetathesallesndlarest eigenvalue of the symmetrization of $\\bar{I}^{(L)}(\\mathcal{X},\\mathcal{X}),\\,S^{(L)}:=\\left(I^{(L)}(\\mathcal{X},\\mathcal{X})+I^{(L)}(\\mathcal{X},\\mathcal{X})^{\\top}\\right)/2$ are given by $0<\\lambda_{\\mathrm{min}}\\le\\lambda_{\\mathrm{max}}<\\infty$ and that the learning rate is given by $\\eta>0$ Then, for any $\\delta>0$ there exist $R>0,N\\in\\mathbb{N}$ and $K>1$ such that for every $n\\geq N$ , the following holds with probability at least $1-\\delta$ over random initialization: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t\\in[0,\\infty)}\\left\\|\\hat{I}_{t}^{(L)}(\\lambda,\\lambda)-I^{(L)}(\\lambda,\\lambda)\\right\\|_{F}\\le\\frac{6K^{3}R}{\\lambda_{\\operatorname*{min}}}n^{-\\frac{1}{2}},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We also write $I^{(L)}=I_{\\sigma,\\tilde{\\sigma}}$ or simply $I_{\\sigma}$ . The explicit analytie expression of the SG-NTK is derived in Section E.2 for activation function $\\sigma=\\operatorname{erf}_{m}$ \uff0c $m\\in\\mathbb{N}$ and surrogate derivative $\\tilde{\\sigma}=\\mathrm{erf}$ as well as general surrogate derivatives. $I_{\\mathrm{erf}_{m},\\tilde{\\sigma}}$ converges as $m\\rightarrow\\infty$ , because compared to Equation (5) we obtain $\\mathbb{E}[\\operatorname{erf}_{m}(g(x))\\,\\tilde{\\sigma}(g(y))]\\to\\mathbb{E}[2\\delta(g(x))\\,\\tilde{\\sigma}(g(y))]$ and the delta distribution yields a finite value. In Section E.2, we show this rigorously and derive the analytic expressions. Hence, we define $I_{\\mathrm{sign},\\tilde{\\sigma}}:=\\mathrm{lim}_{m\\rightarrow\\infty}\\,I_{\\mathrm{erf}_{m},\\tilde{\\sigma}}$ ", "page_idx": 6}, {"type": "text", "text": "For any $m\\in\\mathbb{N}$ we can consider the SGL dynamics given by Equation (7) in the infinite-width limit to obtain ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}f(x;\\theta_{t})=-\\eta\\,I_{\\mathrm{erf}_{m}}(x,\\mathcal{X})\\,\\nabla_{f(\\mathcal{X};\\theta_{t})}\\mathcal{L}(f(\\mathcal{X};\\theta_{t});\\mathcal{Y}).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "With MSE error, this equation is solved by a GP with mean $m(x)=I_{\\mathrm{erf}_{m}}(x,\\mathcal{X})I_{\\mathrm{erf}_{m}}(\\mathcal{X},\\mathcal{X})^{-1}\\mathcal{Y}$ for $t\\to\\infty$ and it is natural to assume that the networks trained with SGL converge in distribution to this GP in the infinite-width limit, analogous to the standard NTK case [Lee et al., 2019, Theorem 2.2]. However, the empirical counterpart to $I_{\\mathrm{sign}}$ does not exist as we cannot formulate an empirical SG-NTK for the sign activation function due to the missing Jacobian matrix, compare Equation (6). We suggest that even in this case the network trained with SGL will converge in distribution to the GP given by $I_{\\mathrm{sign}}$ , since SGL with activation function $\\operatorname{erf}_{m}$ will approach SGL with activation function sign as $m\\rightarrow\\infty$ and the GP given by $I_{\\mathrm{erf}_{m}}$ will approach the GP given by $I_{\\mathrm{sign}}$ as $m\\rightarrow\\infty$ Numerical experiments in Section 3 indicate that this is indeed the case. We conclude that, remarkably, the SG-NTK can be defined for network functions without well-defined gradients and is informative about their learning dynamics under SGL. ", "page_idx": 7}, {"type": "text", "text": "3 Numerical experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We numerically illustrate the divergence of the analytic NTK, $\\Theta_{\\mathrm{erf}_{m}}$ , shown in Section 2.3 and the convergence of the SG-NTK in the infinite-width limit, ${\\hat{I}}^{(L)}\\,\\rightarrow\\,I^{(L)}$ , at initialization and during training shown in Section 2.4. Simultaneously, we visualize the convergence of the analytic SG-NTK, $I_{\\mathrm{erf}_{m}}\\to I_{\\mathrm{sign}}$ . We consider a regression problem on the unit sphere $S^{\\overline{{1}}}=\\{x\\in\\mathbb{R}^{2}:\\|{\\dot{x}}\\|=1\\}$ with $|\\mathcal{X}|=15$ training points, which is shown in Figure B.1, and train 10 fully connected feedforward networks with two hidden layers, and activation function $\\operatorname{erf}_{m}$ for $t=10000$ time steps and with MSE loss. The NTK only depends on the dot product [Radhakrishnan et al., 2023] and thus the angle between its two arguments, $\\Delta\\alpha=\\sphericalangle(x,x^{\\prime})$ . Hence, we plot the NTKs as functions of this angle, where $\\Delta\\alpha=0$ corresponds to $x=x^{\\prime}$ \uff1a ", "page_idx": 7}, {"type": "text", "text": "In Figure 1, the empirical and analytic NTKs for the networks described above and trained with gradient descent are plotted for $m\\in\\{2,5,20\\}$ and hidden layer widths $n\\in\\{10,100,500,1000\\}$ In addition, the analytic NTK for $m\\rightarrow\\infty$ is plotted. Note that the steep slope of $\\operatorname{erf}_{m}$ for $m=20$ results in $\\operatorname{erf}_{m}$ being very close to the sign function. For any $m$ , we observe that the empirical NTKs converge to the analytic NTK both at initialization and after training as NTK theory states. Figure B.3 illustrates this further by displaying the mean squared errors between the empirical NTKs and the respective analytic NTK. The convergence slows down for larger $m$ . Further, the plots confirm that the analytic NTKs diverge as $m\\rightarrow\\infty$ if and only if $\\Delta\\alpha=0$ . To show this more clearly, we scaled the y-axis with the inverse hyperbolic sine (asinh), which is approximately linear for small absolute values and logarithmic for large absolute values. ", "page_idx": 7}, {"type": "image", "img_path": "kfdEXQu6MC/tmp/b73a47dbf84731b3aa31f1b40d0358ac39b461230088545de993c5d25301e254.jpg", "img_caption": ["Figure 1: We plot empirical and analytic NTKs of 10 networks for different hidden layer widths $n$ and activation functions $\\operatorname{erf}_{m}$ . The kernels are plotted at initialization and after gradient descent training with $t=1\\mathrm{e}4$ time steps, learning rate $\\eta=0.1$ , and MSE error. The y-axis is asinh-scaled. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "For Figure 2, we use the same setup as before, but train the networks using SGL with surrogate derivative $\\tilde{\\sigma}=\\mathrm{erf}$ , and compare the empirical and analytic SG-NTKs instead of NTKs. We observe that the empirical SG-NTKs converge to the analytic SG-NTK as $n\\to\\infty$ both at initialization and after training in accordance with Theorem 2.4 and Theorem 2.5. Figure B.4 illustrates this further by displaying the mean squared errors between empirical SG-NTKs and respective analytic SG-NTK. ", "page_idx": 7}, {"type": "text", "text": "Moreover, we observe that the analytic SG-NTKs indeed converge to a finite limit as $m\\rightarrow\\infty$ ,as shown in Section 2.4. ", "page_idx": 8}, {"type": "image", "img_path": "kfdEXQu6MC/tmp/e64901b96a1ee838456cbdcf9630ea10f0cfed26743d2d20219b8ceb1f40a38b.jpg", "img_caption": ["Figure 2: We plot empirical and analytic SG-NTKs of ten networks for different hidden layer widths $n$ and activation functions $\\textstyle\\operatorname{erf}_{m}$ . The kernels are plotted at initialization and after surrogate gradient learning with $t=1\\mathrm{e}4$ time steps, learning rate $\\eta=0.1$ , MSE error, and surrogate derivative $\\tilde{\\sigma}=\\mathrm{erf}$ "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Finally, we consider SGL for networks with the same architecture and training objective as before, but with sign activation function, which can be seen as the case $m\\rightarrow\\infty$ of the setups above. We examine whether the distribution of network functions trained with SGL agrees with the distribution of the GP given by the limit kernel $I_{\\mathrm{sign}}$ . Specifically, we compare 500 networks trained with SGL for $t=30000$ time steps, which represent the distribution of the network function after training, to the mean and confidence band of the GP. The mean of the GP is given by kernel regression using the SG-NTK, $m(x)=I_{\\mathrm{sign}}(x,\\mathcal{X})I_{\\mathrm{sign}}(\\mathcal{X},\\mathcal{X})^{-1}\\mathcal{Y}.$ and the confidence band is given by $m(x)\\pm2\\sigma_{\\mathrm{GP}}(x)$ where $\\sigma_{\\mathrm{GP}}(x)$ is the standard deviation at $x$ . We observe in Figure 3a that the mean of the trained networks is close to the GP's mean for network width $n=500$ and that most networks lie within the confidence band. The mean of the networks differs from the kernel regression using the kernel $\\Sigma_{\\mathrm{sign}}$ Figure 3b shows that this agreement between SGL and the SG-NTK already exists for a network width of $n=100$ , demonstrating that the SG-NTK predicts the SGL dynamics of networks with moderate width. Note that the variance in the networks\u2019 output and the confidence band can be reduced (see Arora et al. [2019] and Section A). ", "page_idx": 8}, {"type": "image", "img_path": "kfdEXQu6MC/tmp/b4506d4170b414e89ddad89bdbf496353f087d236f81b0587f40edaa4ec3f17b.jpg", "img_caption": ["Figure 3: Comparison of SGL learning in networks with different hidden layer widths with SG-NTK predictions. (a) 5o0 networks (blue) with sign activation function and hidden layer widths $n=500$ trained with SGL using the surrogate derivative $\\tilde{\\sigma}=\\mathrm{erf}$ for $t=3\\mathrm{e}4$ time steps plotted together with their mean (cyan), the SG-NTK-GP's mean (black) and confidence band (grey), and the $\\Sigma_{\\mathrm{sign}}$ kernel regression (dashed). Training points are indicated with crosses. (b) The mean of ensembles of 500 networks is plotted as in (a) for different hidden layer widths. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Gradient descent training is not applicable to networks with sign activation function. In the present study, we have first shown that this is even true for the infinite-width limit in the sense that the NTK diverges to a singular kernel. We found that this singular kernel still has interesting properties and allows for classification, but it is unusable for regression. ", "page_idx": 9}, {"type": "text", "text": "We then studied SGL, which is applicable to networks with sign activation function. We defined a generalized version of the NTK that can be applied to SGL and derived a novel SG-NTK. We proved that the convergence of the NTK in the infinite-width limit extends to the SG-NTK, both at initialization and during training. Strikingly, we were able to derive an SG-NTK for the sign activation function, $I_{\\mathrm{sign}}$ , by approximating the sign function with error functions. We suggest that this SG-NTK predicts the learning dynamics of SGL, and support this claim with heuristic arguments and numerical simulations. ", "page_idx": 9}, {"type": "text", "text": "A limitation of our work is that due to the considered NTK framework, our results are naturally only applicable to sufficiently wide networks with random initialization. Further, we only prove the convergence of the SG-NTK during training for activation functions with well-defined derivatives. More rigorous analysis should be carried out on how the connection between SGL and the SG-NTK carries over to activation functions with jumps, as shown by our simulations. ", "page_idx": 9}, {"type": "text", "text": "Our derivation of the SG-NTK opens a novel path towards addressing the many unanswered questions regarding the training of binary networks, in particular regarding the class of functions that SGL learns for wide networks and how that class differs for different activation functions and surrogate derivatives. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank Andreas Eberle for helpful discussions. We thank the German Federal Ministry of Education and Research (BMBF) for support via the Bernstein Network (Bernstein Award 2014, 01GQ1710). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Sina Alemonammaa, Zicnao wang, Kangan Balestriero, ana Kicnara Baraniuk. 1ne recurrent neural tangent kernel. arXiv preprint arXiv:2006.10246, 2020. ", "page_idx": 9}, {"type": "text", "text": "Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via overparameterization. In International Conference on Machine Learning, pages 242-252. PMLR, 2019.   \nSanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On exact computation with an infinitely wide neural net. Advances in Neural Information Processing Systems, 32, 2019.   \nGaurav Arya, Moritz Schauer, Frank Schafer, and Christopher Rackauckas. Automatic differentiation of programs with discrete randomness. Advances in Neural Information Processing Systems, 35: 10435-10447,2022.   \nGuillaume Bellec, Darjan Salaj, Anand Subramoney, Robert Legenstein, and Wolfgang Maass. Long short-term memory and learning-to-learn in networks of spiking neurons. Advances in Neural InformationProcessingSystems,31,2018.   \nYoshua Bengio, Nicholas Leonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.   \nAlberto Bietti and Julien Mairal. On the inductive bias of neural tangent kernels. Advances in Neural Information Processing Systems, 32, 2019.   \nPatrick Billingsley. Convergence of Probability Measures. John Wiley & Sons, 2nd edition, 1999. ", "page_idx": 9}, {"type": "text", "text": "Sander M Bohte. Error-backpropagation in networks of fractionally predictive spiking neurons. In International Conference on Artificial Neural Networks, pages 60-68. Springer, 2011. ", "page_idx": 9}, {"type": "text", "text": "Blake Bordelon and Cengiz Pehlevan. The infuence of learning rule on representation dynamics in wide neural networks. In The Eleventh International Conference on Learning Representations, 2022.   \nBlake Bordelon, Abdulkadir Canatar, and Cengiz Pehlevan. Spectrum dependent learning curves in kernel regression and wide neural networks. In International Conference on Machine Learning, pages 1024-1034. PMLR, 2020.   \nDaniele Bracale, Stefano Favaro, Sandra Fortini, and Stefano Peluchetti. Large-width functional asymptotics for deep Gaussian neural networks. arXiv preprint arXiv:2102.10307, 2021.   \nJames Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http: //github.com/google/jax.   \nZhaowei Cai, Xiaodong He, Jian Sun, and Nuno Vasconcelos. Deep learning with low precision by half-wave GaussanquantizationIProceedings of the IEEE ConferenceonComuter Vision and Pattern Recognition, pages 5918-5926, 2017.   \nWuyangChn,XnyuGong,andZhangyang Wangeura architeture searchnmant o gpu hours: A theoretically inspired perspective. arXiv preprint arXiv:2102.11535, 2021.   \nYoungmin Cho and Lawrence Saul. Kernel methods for deep learning. Advances in Neural Information Processing Systems, 22, 2009.   \nGiuseppe Da Prato. An Introduction to Infinite-Dimensional Analysis. Springer Science & Business Media, 2006.   \nLuc Devroye, Laszlo Gyorfi, and Adam Krzyzak. The Hilbert kernel regression estimate. Journal of Multivariate Analysis, 65(2):209-227, 1998.   \nSimon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of deep neural networks. In International conference on machine learning, pages 1675- 1685. PMLR, 2019.   \nJason K Eshraghian, Max Ward, Emre Neftci, Xinxin Wang, Gregor Lenz, Girish Dwivedi, Mohammed Bennamoun, Doo Seok Jeong, and Wei D Lu. Training spiking neural networks using lessons from deep learning. arXiv preprint arXiv:2109.12894, 2021.   \nSteven K. Esser, Paul A. Merolla, John V. Arthur, Andrew S. Cassidy, Rathinakumar Appuswamy, Alexander Andreopoulos, David J. Berg, Jeffrey L. McKinstry, Timothy Melano, Davis R. Barch, Carmelo di Nolfo, Pallab Datta, Arnon Amir, Brian Taba, Myron D. Flickner, and Dharmendra S. Modha. Convolutional networks for fast, energy-effcient neuromorphic computing. Procedings of the National Academy of Sciences, 27:201604850, 2016.   \nAdria Garriga-Alonso, Carl Edward Rasmussen, and Laurence Aitchison. Deep convolutional networks as shallow Gaussian processes. arXiv preprint arXiv: 1808.05587, 2018.   \nGene H. Golub and Charles F. Van Loan. Matrix Computations. John Hopkins University Press, London, 3rd edition, 1996.   \nJulia Gygax and Friedemann Zenke. Elucidating the theoretical underpinnings of surrogate gradient learning in spiking neural networks. arXiv preprint arXiv:2404.14964, 2024.   \nInsu Han, Amir Zandieh, Jaehoon Lee, Roman Novak, Lechao Xiao, and Amin Karbasi. Fast neural kernel embeddings for general activations. In Advances in Neural Information Processing Systems, 2022. URL https://github.com/google/neural-tangents.   \nHerbert Heyer. Structural Aspects in the Theory of Probability, volume 8. World Scientific, 2009.   \nGeoffrey E Hinton. Coursera video lectures: Neural networks for machine learning. https: //www.cs.toronto.edu/\\~hinton/coursera_lectures.html, 2012. Online; accessed 25 1..1.,2022   \nJIrl Hron, Yasaman Banri, Jascna soni-Dickstein, and Koman INovak. innnite attenuon: INngp ana ntk for deep attention networks. In International Conference on Machine Learning, 2020. URL https: //github.com/google/neural-tangents.   \nItay Hubara, Mathieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural networks. Advances in Neural Information Processing Systems, 29, 2016.   \nArthur Jacot, Franck Gabriel, and Cl\u00e9ment Hongler. Neural tangent kernel: Convergence and generalization in neural networks. Advances in Neural Information Processing Systems, 31, 2018.   \nHyeryung Jang, Osvaldo Simeone, Brian Gardner, and Andre Gruning. An introduction to probabilis tic spiking neural networks: Probabilistic models, learning rules, and applications. IEEE Signal Processing Magazine, 36(6):64-77, 2019.   \nOlav Kallenberg. Foundations of Modern Probability, volume 3. Springer, 2021.   \nAnnika Lang and Christoph Schwab. Isotropic Gaussian random fields on the sphere: regularity, fast simulation and stochastic partial differential equations. The Annals of Applied Probability, 25(6): 3047-3094, 2015. ISSN 10505164.   \nYann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436-444, 2015.   \nMichel Ledoux and Michel Talagrand. Probability in Banach Spaces: Isoperimetry and Processes, volume 23. Springer Science & Business Media, 1991.   \nJaehoon Lee, Jascha Sohl-Dickstein, Jeffrey Pennington, Roman Novak, Sam Schoenholz, and Yasaman Bahri. Deep neural networks as Gaussian processes. In International Conference on Learning Representations, 2018. URL https: //openreview.net/forum?id $\\cdot$ B1EA-M-OZ.   \nJaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha SohlDickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient descent. Advances in Neural Information Processing Systems, 32, 2019.   \nChaoyue Liu, Libin Zhu, and Misha Belkin. On the linearity of large non-linear models: when and why the tangent kernel is constant. Advances in Neural Information Processing Systems, 3: 15954-15964, 2020.   \nAlexander G de G Matthews, Mark Rowland, Jiri Hron, Richard E Turner, and Zoubin Ghahramani. Gaussian process behaviour in wide deep neural networks. arXiv preprint arXiv:1804.11271, 2018.   \nPaul A Merolla, John V Arthur, Rodrigo Alvarez-Icaza, Andrew S Cassidy, Jun Sawada, Filipp Akopyan, Bryan L Jackson, Nabil Imam, Chen Guo, Yutaka Nakamura, et al. A million spikingneuron integrated circuit with a scalable communication network and interface. Science, 345 (6197):668-673, 2014.   \nRadford M. Neal. Bayesian Learning for Neural Networks. Springer New York, NY, Ist edition, 1996.   \nEmre O Neftci, Hesham Mostafa, and Friedemann Zenke. Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to spiking neural networks. IEEE Signal Processing Magazine, 36(6):51-63, 2019.   \nTimothy Nguyen, Roman Novak, Lechao Xiao, and Jaehoon Lee. Dataset distilation with infinitely wide convolutional networks. Advances in Neural Information Processing Systems, 34:5186-5198, 2021.   \nRoman Novak, Lechao Xiao, Jiri Hron, Jaehoon Lee, Alexander A. Alemi, Jascha Sohl-Dickstein, and Samuel S. Schoenholz. Neural tangents: Fast and easy infinite neural networks in python. In International Conference on Learning Representations, 2020. URL https: //github. com/ google/neural-tangents.   \nRoman Novak, Jascha Sohl-Dickstein, and Samuel S. Schoenholz. Fast finite width neural tangent kernel. In International Conference on Machine Learning, 2022. URL https : //github. com/ google/neural-tangents.   \nMichaelPfeiffer and Thomas Pfeil Deep learning with spiking neurons: Opportunities and challenges. Frontiers in neuroscience, 12:774, 2018. ISSN 1662-4548. doi: 10.3389/fnins.2018.00774.   \nConstantine Pozrikidis. An Introduction to Grids, Graphs, and Networks. Oxford University Press, 2014.   \nYu V Prokhorov. Convergence of random proceses and limit theorems in probability theory. Theory of Probability & Its Applications, 1(2):157-214, 1956.   \nYuming Qin. Integral and Discrete Inequalities and Their Applications. Volume I: Linear Inequalities. Springer, 2016.   \nAdityanarayanan Radhakrishnan, Mikhail Belkin, and Caroline Uhler. Wide and deep neural networks achieve consistency for classification. Proceedings of the National Academy of Sciences, 120(14): e2208779120, 2023.   \nFrank Rosenblatt. The perceptron: A probabilistic model for information storage and organization in the brain. Psychological Review, 65(6):386, 1958.   \nKaushik Roy, Akhilesh Jaiswal, and Priyadarshini Panda. Towards spike-based machine intelligence with neuromorphic computing. Nature, 575(7784):607-617, Nov 2019. ISSN 1476-4687. doi: 10.1038/s41586-019-1677-2. URL https: //doi . org/10 .1038/s41586-019-1677-2.   \nDavid E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-propagating errors. Nature, 323(6088):533-536, 1986.   \nSumit B Shrestha and Garrick Orchard. Slayer: Spike layer error reassignment in time. Advances in Neural Information Processing Systems, 31, 2018.   \nJascha Sohl-Dickstein, Roman Novak, Samuel S. Schoenholz, and Jaehoon Lee. On the infnite width limit of neural networks with a standard parameterization, 2020. URL https : //github. com/ google/neural-tangents.   \nAboozar Taherkhani, Ammar Belatreche, Yuhua Li, Georgina Cosma, Liam P Maguire, and T Marin McGinnity. A review of learning in biologically plausible spiking neural networks. Neural Networks, 122:253-272, 2020. doi: 10.1016/j.neunet.2019.09.036.   \nAmirhossein Tavanaei, Masoud Ghodrati, Saeed Reza Kheradpisheh, Timoth\u00e9e Masquelier, and Anthony Maida. Deep learning in spiking neural networks. Neural Networks, 111:47-63, 2019.   \nAad W. van der Vaart. Asymptotic Statistics. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 1998. doi: 10.1017/CBO9780511802256.   \nChristopher Williams. Computing with infinite networks. Advances in Neural Information Processing Systems, 9, 1996.   \nDavid Williams. Probability with Martingales. Cambridge University Press, 1991.   \nStanistaw Wozniak, Angeliki Pantazi, Thomas Bohnstingl, and Evangelos Eleftheriou. Deep learning incorporating biologically inspired neural dynamics and in-memory computing. Nature Machine Intelligence, 2(6):325-336, 2020.   \nYujie Wu, Lei Deng, Guoqi Li, Jun Zhu, and Luping Shi. Spatio-temporal backpropagation for training high-performance spiking neural networks. Frontiers in Neuroscience, 12:331, 2018.   \nYuje Wu, Lei Deng, Guoqi Li, Jun Zhu, Yuan Xie, and Luping Shi Direct training for spiking neural networks: Faster, larger, better. Proceedings of the AAAl Conference on Artificial Intelligence, 33 (01):1311-1318, 2019.   \nGreg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior, gradient independence, and neural tangent kernel derivation. arXiv preprint arXiv: 1902.04760, 2019a.   \nGreg Yang. Wide feedforward or recurrent neural networks of any architecture are Gaussian processes. Advances in Neural Information Processing Systems, 32, 2019b.   \nGreg Yang. Tensor programs i: Neural tangent kernel for any architecture. arXiv preprint arXiv:2006.14548,2020.   \nPenghang Yin, Jiancheng Lyu, Shuai Zhang, Stanley Osher, Yingyong Qi, and Jack Xin. Understanding straight-through estimator in training activation quantized neural nets. arXiv preprint arXiv:1903.05662, 2019.   \nBA Zalesski, VV Sazonov, and VV Ul' yanov. A precise estimate of the rate of convergence in the central limit theorem in Hilbert space. Sbornik: Mathematics, 68(2):453-482, 1991.   \nFriedemann Zenke and Surya Ganguli. Superspike: Supervised learning in multilayer spiking neural networks. Neural Computation, 30(6):1514-1541, 2018.   \nFriedemann Zenke and Tim P Vogels. The remarkable robustness of surrogate gradient learning for instilling complex function in spiking neural networks. Neural Computation, 33(4):899-925, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A  Additional remarks on the numerical experiments ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Weight initialization and implementation. All networks are initialized with $\\sigma_{w}=1,\\sigma_{b}=0.1$ For the implementation of the NTK and SG-NTK we use the JAX package [Bradbury et al., 2018] and Neural Tangents package [Novak et al., 2020, 2022, Han et al., 2022, Sohl-Dickstein et al., 2020, Hron et al., 2020] with modifications. Computations were done using an Intel Core i7-1355U CPU and 16 GB RAM. The simulations for Figure 1 and Figure 2 took two hours each. The simulations for Figure 3 took 12 hours. ", "page_idx": 14}, {"type": "text", "text": "Variance reduction trick. The variance in the outputs of the networks trained with gradient descent or SGL and the confidence band given the NTK or SG-NTK respectively can be reduced by multiplying the weights of the last layer with a constant $\\kappa<1$ at initialization [Arora et al., 2019]. This is explained in detail at the end of Section C.2.1. We can see from Figure B.2 that the agreement between the distribution of the trained networks and the distribution given by the SG-NTK still holds; however, network width and training time have to be increased. ", "page_idx": 14}, {"type": "text", "text": "B   Additional figures ", "text_level": 1, "page_idx": 14}, {"type": "image", "img_path": "kfdEXQu6MC/tmp/0b02b66612cf0f9c0b05110bf4b27f1f0ea2e326a1e7111054a76f080f035e6e.jpg", "img_caption": ["Figure B.1: Target function and training points for the numerical experiments, $f(x,y)=4x y^{2}-$ $\\phantom{-}0.\\breve{8}x^{3}+1.2y^{2}-0.8x^{2}y$ "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "kfdEXQu6MC/tmp/1fa63386aee8ee62e4a418398869011257615428822a2216dbb7a02c2978cba8.jpg", "img_caption": ["Figure B.2: Network functions of 100 networks with sign activation function and hidden layer widths $n=500$ trained with SGL using the surrogate derivative $\\tilde{\\sigma}=\\mathrm{erf}$ for $t=2\\mathrm{e}4$ time steps plotted together with the SG-NTK-GP's mean and confidence band. The parameters of the last layer have been multiplied with $\\kappa=0.2$ at initialization. "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "kfdEXQu6MC/tmp/8b23c90e01f98f8e263bf2db259fec6d211bad8d66c23d9925545452751580e5.jpg", "img_caption": ["Figure B.3: Mean squared errors between empirical NTKs and analytic NTKs in Figure 1 for all 10 networks (thin lines) and averaged over the 10 networks (thick lines). "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "kfdEXQu6MC/tmp/f9cf9d0256834255bbd7fa43fe8b6bf0bd2e12b64684919fa1b7588996af2b94.jpg", "img_caption": ["Figure B.4: Mean squared errors between empirical SG-NTKs and analytic SG-NTKs in Figure 2 for all 10 networks (thin lines) and averaged over the 10 networks (thick lines). "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "C  Neural tangent kernel theory ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "C.1 Introduction of the neural tangent kernel ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We begin by defining an artificial neural network with a parameterization suitable for considering the limit of infinitely many hidden neurons. This parameterization is called neural tangent kernel parameterization and differs from standard parameterization of multilayer perceptrons by a rescaling factorof $1/\\sqrt{n_{l}}$ in layer $l+1$ where $n_{l}$ is the width of layer $l$ We follow the slightly more general definition in [Lee et al., 2019, Equation (1)]. A discussion of this kind of parameterization can be found in [Jacot et al., 2018, Remark 1]. ", "page_idx": 15}, {"type": "text", "text": "Definition C.1 (Artificial neural network with NTK parameterization). Let $L$ be the depth of the network, $n_{k}$ for $k=0,\\dots,L$ thewidths of thelayers,and $\\sigma\\colon\\ensuremath{\\mathbb{R}}\\to\\ensuremath{\\mathbb{R}}$ an activationfunction.We draw network weight matrices $\\dot{W}^{(l)}\\in\\mathbb{R}^{n_{l}\\times n_{l-1}}$ and biases $b^{(l)}\\in\\mathbb{R}^{n_{l}}$ for $l=1,\\dots,L$ from a probability distrbtionsuchthat $W_{i j}^{(l)},b_{i}^{(l)}\\overset{i i d}{\\sim}\\mathcal{N}(0,1)$ $\\theta^{(l)}=(W^{(l)},b^{(l)})$ the parameters of layer l and by $\\theta^{(1:\\,l)}=(\\theta^{(1)},\\theta^{(2)},\\dots,\\theta^{(l)})$ the parameters of layers 1 up to and including $l$ Given some $\\sigma_{w}>0$ and $\\sigma_{b}\\geq0$ we then define for all $\\boldsymbol{x}\\in\\mathbb{R}^{n_{0}}$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{h^{(1)}\\left(x;\\theta^{(1)}\\right)=\\displaystyle\\frac{\\sigma_{w}}{\\sqrt{n_{0}}}W^{(1)}x+\\sigma_{b}\\,b^{(1)},}}\\\\ {{h^{(l+1)}\\left(x;\\theta^{(1:\\,l+1)}\\right)=\\displaystyle\\frac{\\sigma_{w}}{\\sqrt{n_{l}}}W^{(l+1)}\\sigma\\left(h^{(l)}\\left(x;\\theta^{(1:\\,l)}\\right)\\right)+\\sigma_{b}\\,b^{(l+1)}\\quad\\mathrm{for}\\;l=1,\\ldots,L-1.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, $h^{(l)}(\\cdot;\\theta^{(1:l)})$ is a map from $\\mathbb{R}^{n_{0}}$ to $\\mathbb{R}^{n_{l}}$ and we use the short-hand notation $h^{(l)}(x;\\theta)=$ $h^{(l)}(x;\\theta^{(1:\\ l)})$ Finally,wedeneourneworfuntn ", "page_idx": 16}, {"type": "equation", "text": "$$\nf(\\,\\cdot\\,;\\theta)=h^{(L)}(\\,\\cdot\\,;\\theta)\\colon\\mathbb{R}^{n_{0}}\\to\\mathbb{R}^{n_{L}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "With this definition, the total number of parameters, $P=|\\theta|$ ,is ", "page_idx": 16}, {"type": "equation", "text": "$$\nP=\\sum_{l=1}^{L}\\left|\\theta^{(l)}\\right|=\\sum_{l=1}^{L}n_{l}(n_{l-1}+1).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Given such a network function with NTK parameterization, we consider a dataset $\\boldsymbol{\\mathcal{D}}\\,=\\,(\\boldsymbol{\\mathcal{X}},\\boldsymbol{\\mathcal{Y}})$ with $\\mathcal{X}\\,=\\,(x_{1},\\ldots,x_{d})\\ \\in\\ \\mathbb{R}^{d\\cdot n_{0}}$ and $\\mathcal{Y}\\,=\\,(y_{1},\\dotsc.\\dotsc,y_{d})\\ \\in\\ \\mathbb{R}^{d\\cdot n_{L}}$ .First, we want to solve the regression problem, i.e., find parameters $\\theta^{\\prime}$ such that $f(x_{i};\\theta^{\\prime})\\,=\\,y_{i}$ for all $i\\,=\\,1,\\ldots,d$ . Later, we will also consider the classification problem, i.e., we assume $y_{i}\\in\\{-1,1\\}$ and want to solve $\\mathrm{sign}(f(x_{i};\\theta^{\\prime}))=y_{i}$ for all $i=1,\\ldots,d$ . Tackling both cases from the regression perspective, we define the so-called loss functional and loss function. The following two definitions are similarly formulated by Jacot et al. [2018]. ", "page_idx": 16}, {"type": "text", "text": "Definition C.2 (Loss functional and loss function). Let $\\mathcal{F}=\\{g\\,\\vert\\,g\\colon\\mathbb{R}^{n_{0}}\\rightarrow\\mathbb{R}^{n_{L}}\\}$ and $\\mathfrak{L}\\colon\\mathcal{F}\\rightarrow\\mathbb{R}\\;a$ $s o$ -called loss functional. In addition, let $\\mathfrak{L}$ be convex, i.e., for all $\\lambda\\in[0,1]$ and $g_{1},g_{2}\\in{\\mathcal{F}}$ itholds ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathfrak{L}(\\lambda g_{1}+(1-\\lambda)g_{2})\\leq\\lambda\\mathfrak{L}(g_{1})+(1-\\lambda)\\mathfrak{L}(g_{2}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We will assume that $\\mathfrak{L}$ can be written as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathfrak{L}(f)=\\frac{1}{d}\\sum_{i=1}^{d}\\ell(f(x_{i};\\theta);y_{i})=:\\mathcal{L}(f(\\boldsymbol{\\mathscr{X}});\\mathcal{Y}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "so that the so-called loss function $\\mathcal{L}(\\,\\cdot\\,;\\mathcal{V})\\colon\\mathbb{R}^{d\\cdot n_{L}}\\rightarrow\\mathbb{R}$ is differentiable. ", "page_idx": 16}, {"type": "text", "text": "Remark C.1 (Function evaluation at sets and vector notation). We want to detail the notation used in DefinitionC.2.For afunction $f\\colon\\ensuremath{\\mathbb{R}}^{n_{0}}\\to\\ensuremath{\\mathbb{R}}^{n_{L}}$ and $\\mathcal{X}=(x_{1},...\\,,x_{d})\\in\\mathbb{R}^{d\\cdot n_{0}}$ wedefinethevector ", "page_idx": 16}, {"type": "equation", "text": "$$\nf(\\ensuremath{\\boldsymbol{X}}):=(f(\\ensuremath{\\boldsymbol{x}}_{1}),\\cdot\\cdot\\cdot\\cdot,f(\\ensuremath{\\boldsymbol{x}}_{d}))\\in\\mathbb{R}^{d\\cdot n_{L}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By default, we will interpret any vector as a column vector, i.e., we identify $\\mathbb{R}^{n}$ with $\\mathbb{R}^{n\\times1}$ .This is the case even when writing $x=(x_{1},...,x_{n})\\in\\mathbb{R}^{n}$ for handier notation. Row vectors will be indicated within calculations using the transpose operator, $x^{\\mathsf{T}}$ ", "page_idx": 16}, {"type": "text", "text": "Let us first consider regular gradient descent learning in continuous time, also known as gradient flow.   \nFor this, we assume that our network function is differentiable with respect to its parameters. ", "page_idx": 16}, {"type": "text", "text": "Remark C.2 (Gradient and Jacobian matrix notation). Let $f\\colon\\ensuremath{\\mathbb{R}}^{n}\\to\\ensuremath{\\mathbb{R}}^{m}$ .For $m=1$ ,we denote the gradient by $\\nabla f(x)\\,\\in\\,\\mathbb{R}^{n}$ for all $x\\,\\in\\,\\mathbb{R}^{n}$ .If $m\\,>\\,1$ wedenotetheJacobianmatrixof $f$ by $J f\\colon\\mathbb{R}^{n}\\to\\mathbb{R}^{m\\times n}$ .Therefore, $J f(x)$ is a $m\\times n$ matrix for all $x\\in\\mathbb{R}^{n}$ .We do not always want toconsider thegradient orJacobian matrix with respect to allvariables.Weindicatethiswith subscripts as follows. Let $f\\colon\\mathbb{R}^{n}\\times\\mathbb{R}^{P}\\rightarrow\\mathbb{R}^{m}$ and $g_{x}({\\bar{\\theta}})=f(x;\\theta)$ for fixed $x\\in\\mathbb{R}^{n}$ . Then, we write $\\nabla_{\\theta}f(\\bar{x_{;}}\\,\\theta):=\\nabla g_{x}(\\theta)$ and $J_{\\theta}f(x;\\theta):=J g_{x}(\\theta)$ In particular, for a map $f\\colon\\ensuremath{\\mathbb{R}}^{P}\\to\\ensuremath{\\mathbb{R}},$ the gradient with respect to the $j$ -th variable, $1\\le j\\le P$ , is a scalar and denoted by $\\partial_{j}f(\\theta):=\\nabla_{\\theta_{j}}f(\\theta)$ This is calledthepartialderivativeof $f$ with respect to the $j$ -th variable. ", "page_idx": 16}, {"type": "text", "text": "With this notation, we consider the gradient flow method with learning rate $\\eta>0$ and recall the derivation of the neural tangent kernel. We move the weights in the opposite direction of the gradient of the loss function with respect to the parameters of the network evaluated at the training points: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\mathrm{d}}{\\mathrm{d}t}\\theta_{t}=-\\eta\\,\\nabla_{\\theta}\\mathcal{L}\\bigl(f(\\mathcal{X};\\theta_{t});\\mathcal{Y}\\bigr))=-\\eta\\,J_{\\theta}f(\\mathcal{X};\\theta_{t})^{\\top}\\,\\nabla_{f(\\mathcal{X};\\theta_{t})}\\mathcal{L}\\bigl(f(\\mathcal{X};\\theta_{t});\\mathcal{Y}\\bigr)}\\\\ {\\displaystyle\\qquad=-\\eta\\,\\frac{1}{d}\\sum_{i=1}^{d}J_{\\theta}f\\bigl(x_{i};\\theta_{t}\\bigr)^{\\top}\\,\\nabla_{f(x_{i};\\theta_{t})}\\ell\\bigl(f\\bigl(x_{i};\\theta_{t}\\bigr);y_{i}\\bigr),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "using the chain rule for the second equality and with $A^{\\boldsymbol{\\mathsf{T}}}$ denoting the transpose of a matrix $A$ .Again using the chain rule, this then implies for any $\\boldsymbol{x}\\in\\mathbb{R}^{n_{0}}$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\mathrm{d}}{\\mathrm{d}t}f(\\boldsymbol{x};\\theta_{t})=J_{\\theta}f(\\boldsymbol{x};\\theta_{t})\\,\\frac{\\mathrm{d}}{\\mathrm{d}t}\\theta_{t}\\stackrel{(S8)}{=}-\\eta\\,J_{\\theta}f(\\boldsymbol{x};\\theta_{t})J_{\\theta}f(\\boldsymbol{X};\\theta_{t})^{\\top}\\nabla_{f(\\boldsymbol{X};\\theta_{t})}\\mathcal{L}\\big(f(\\boldsymbol{X};\\theta_{t});\\boldsymbol{y}\\big)}\\\\ {\\displaystyle\\qquad\\qquad=-\\eta\\,\\frac{1}{d}\\sum_{i=1}^{d}J_{\\theta}f(\\boldsymbol{x};\\theta_{t})J_{\\theta}f(\\boldsymbol{x}_{i};\\theta_{t})^{\\top}\\,\\nabla_{f(\\boldsymbol{x}_{i};\\theta_{t})}\\ell\\big(f(\\boldsymbol{x}_{i};\\theta_{t});y_{i}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We therefore define the neural tangent kernel as follows: ", "page_idx": 17}, {"type": "text", "text": "Definition C.3 (Neural tangent kernel). Let f be a network function of depth $L$ as inDefinitionC.1 withparameters $\\theta$ ,not necessarily drawn randomly. Then, we define the neural tangent kernel (NTK) as: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\Theta}^{(L)}\\colon\\mathbb{R}^{n_{0}}\\times\\mathbb{R}^{n_{0}}\\to\\mathbb{R}^{n_{L}\\times n_{L}}}\\\\ &{\\qquad\\qquad\\quad(x,y)\\qquad\\mapsto J_{\\theta}f(x;\\theta)J_{\\theta}f(y;\\theta)^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, it holds for all $\\boldsymbol{x},\\boldsymbol{y}\\in\\mathbb{R}^{n_{0}}$ and $1\\leq i,j\\leq n_{L}$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\hat{\\Theta}_{i\\,j}^{(L)}(x,y)=\\sum_{p=1}^{P}\\partial_{\\theta_{p}}f_{i}(x;\\theta)\\,\\partial_{\\theta_{p}}f_{j}(y;\\theta).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Notice that the NTK depends on the parameters of the networks. It is therefore initialized randomly and varies over the course of the training. With notation $f_{t}(x)\\,=\\,f(x;\\theta_{t})$ and $\\hat{\\Theta}_{t}^{(L)}=\\hat{\\Theta}^{(L)}$ for parameters $\\theta_{t}$ at training time $t$ we can now rewrite Equations (S9) and (S10) as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\mathrm{d}}{\\mathrm{d}t}f_{t}({\\boldsymbol{x}})=-\\eta\\,\\hat{\\Theta}_{t}^{(L)}({\\boldsymbol{x}},{\\boldsymbol{\\mathcal{X}}})\\nabla_{f_{t}({\\boldsymbol{x}})}\\mathcal{L}(f_{t}({\\boldsymbol{\\mathcal{X}}});{\\boldsymbol{\\mathcal{Y}}})\\,}\\\\ {\\displaystyle\\qquad\\qquad=-\\eta\\,\\frac{1}{d}\\sum_{i=1}^{d}\\hat{\\Theta}_{t}^{(L)}({\\boldsymbol{x}},{\\boldsymbol{x}}_{i})\\,\\nabla_{f_{t}({\\boldsymbol{x}}_{i})}\\ell\\big(f_{t}({\\boldsymbol{x}}_{i});{\\boldsymbol{y}}_{i}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We are hence able to express the change of the network function during training in a kernel fashion. Later, we will consider this change of the network function in the infinite-width limit, i.e., $n_{1},\\ldots,n_{L-1}\\to\\infty$ ", "page_idx": 17}, {"type": "text", "text": "Before doing so, we will generalize the NTK definition in order to apply the NTK to surrogate gradient learning later. In particular, we will break the symmetry of the above definition and generalize the Jacobian matrices to quasi-Jacobian matrices by replacing the derivatives of the activation function by surrogate derivatives. Let us write the recursive formula of the Jacobian matrix of the network function given by the chain rule as $J_{\\theta}f(x;\\theta)\\,=\\,G(\\sigma;\\dot{\\sigma};x;\\theta)$ ,where $\\dot{\\sigma}$ is the derivativeof the activation function $\\sigma$ . Then, surrogate gradient learning replaces $\\textstyle J_{\\theta}f(x;\\theta)$ with $G(\\sigma;\\tilde{\\sigma};x;\\theta)$ for the surrogate derivative $\\tilde{\\sigma}$ of the activation function $\\sigma$ . We call this the quasi-Jacobian matrix: ", "page_idx": 17}, {"type": "text", "text": "Definition C.4 (Quasi-Jacobian matrices for neural networks). Let $L$ be the depth of the network, $n_{k}$ for $k=0,\\dots,L$ the width of the layers, $\\sigma\\colon\\mathbb{R}\\rightarrow\\mathbb{R}$ the activation function, and $\\tilde{\\sigma}\\colon\\ensuremath{\\mathbb{R}}\\to\\ensuremath{\\mathbb{R}}$ the so-called surrogate derivative of the activation function.Let $f$ be the network function, $h^{(l)}$ $l=1,\\dotsc,L-1$ theintermediatelayersas inDefinitionC.1 and $\\theta$ the network parameters. We then define the quasi-Jacobian matrix $J^{(\\tilde{L})}$ of $f$ at point $x$ recursively as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J^{(1)}\\left(x;\\theta^{(1)}\\right)\\in\\mathbb{R}^{n_{1}\\times|\\theta^{(1)}|}\\quad w i t h\\quad J_{k\\theta_{p}}^{(1)}\\left(x;\\theta^{(1)}\\right)=\\left\\{\\delta_{k i}\\,\\frac{\\sigma_{w}}{\\sqrt{n_{0}}}x_{j}\\quad i f\\theta_{p}=W_{i j}^{(1)}\\qquad\\quad\\mathrm{(S1)}}\\\\ &{J^{(l)}\\left(x;\\theta^{(1:l)}\\right)\\in\\mathbb{R}^{n_{l}\\times|\\theta^{(1:l)}|}\\quad f o r\\;2\\leq l\\leq L\\;w i t h}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Remark C.3 (Notations for the quasi-Jacobian). With the above definition of the quasi-Jacobian matrixofthenetworkfunction $f$ withactivationfunction $\\sigma$ andsurrogatederivative $\\tilde{\\sigma}$ wewrite ", "page_idx": 18}, {"type": "equation", "text": "$$\nJ^{(L),\\sigma,\\tilde{\\sigma}}(x;\\theta):=J^{(L)}\\left(x;\\theta^{(L)}\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Itthenholds ", "page_idx": 18}, {"type": "equation", "text": "$$\nJ^{(L),\\sigma,\\dot{\\sigma}}(x;\\theta)=J_{\\theta}f(x;\\theta).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For a data set $\\mathcal{X}$ instead of a single point $x$ we concatenate the matrices row-wise as before, namely $J^{(L)}(\\mathcal{X};\\theta)\\in\\mathbb{R}^{d\\cdot n_{L}\\times|\\theta|}$ ", "page_idx": 18}, {"type": "text", "text": "Definition C.5 (The generalized neural tangent kernel). Let $\\sigma_{1},\\sigma_{2}$ be activation functions and $\\tilde{\\sigma}_{1},\\tilde{\\sigma}_{2}$ the surrogate derivatives respectively. Given a network depth $L$ andparameters $\\theta\\;w e$ definethe generalized neural tangent kernel as: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{I}^{(L)}\\colon\\mathbb R^{n_{0}}\\times\\mathbb R^{n_{0}}\\to\\mathbb R^{n_{L}\\times n_{L}}}\\\\ &{\\qquad\\qquad\\quad(x,y)\\mapsto J^{(L),\\sigma_{1},\\tilde{\\sigma}_{1}}(x;\\theta)\\,J^{(L),\\sigma_{2},\\tilde{\\sigma}_{2}}(y;\\theta)^{\\intercal}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Remark C.4. The generalized neural tangent kernel agrees with the neural tangent kernel in the casewhere $\\sigma=\\sigma_{1}=\\sigma_{2}$ and $\\dot{\\sigma}=\\tilde{\\sigma}_{1}=\\tilde{\\sigma}_{2}$ ", "page_idx": 18}, {"type": "text", "text": "C.2Notation for the infinite-width limit and review of key theorems for the NTK ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section we will formulate all important theorems on the NTK that we will need for our later analysis using the introduced notation. Furthermore, we will discuss and remark their proofs, in particular in view of the generalizations that will be proved in Section E.1. ", "page_idx": 18}, {"type": "text", "text": "Convergence of networks to Gaussian processes in the infinite-width limit. We will consider neural networks in the limit of infinitely many hidden layer neurons. The fact that such networks converge to Gaussian processes was first mentioned by Neal [1996]. We follow and present the formulations of Matthews et al. [2018] for the general mathematical statement. First, we formalize the limit of infinitely many hidden neurons. ", "page_idx": 18}, {"type": "text", "text": "Definition C.6 (Width function, as in Definition 3 of Matthews et al. [2018] with modifications). For every layer $l=0,\\dots,L$ and any $m\\in\\mathbb{N}$ the number of neurons at that layer is given by $r_{l}(m)$ , and we call $r_{l}\\colon\\mathbb{N}\\rightarrow\\mathbb{N}$ thewidthfunctionoflayer $l$ Wesay that awidthfunction $r_{l}$ isstrictlyincreasing $i f\\,r_{l}(m)<r_{l}(m+1).$ for all $m\\geq1$ .We set ", "page_idx": 18}, {"type": "text", "text": "the set of collections of strictly increasing width functions for network depth $L$ ", "page_idx": 18}, {"type": "text", "text": "Every element of $\\mathcal{R}_{L}$ provides a way to take the widths of the hidden layers to infinity by setting $n_{0}$ and $n_{L}$ to some constant, setting $n_{l}=r_{l}(m)$ for any $1\\leq l<L$ and considering $m\\rightarrow\\infty$ . To formally define for which ways of taking the widths of hidden layers to infinity a statement holds, we can now state the set $R\\subseteq\\mathcal{R}_{L}$ such that the statement holds for widths given by any $r\\in R$ as $m\\rightarrow\\infty$ . Clearly, the claim \u201cThe statement holds for all $r\\in R_{1}$ .\" is stronger than The statement holds for all $r\\in R_{2}$ \"if $R_{2}\\subseteq R_{1}$ . On the basis of these considerations, we define three types of infinite-width limits using the previous definition in order to structure the different types of limits in this thesis as well as in the literature. ", "page_idx": 18}, {"type": "text", "text": "Definition C.7 (Types of infinite-width limits). Consider a statement $\\boldsymbol{S}$ of theformLet anANN have depth $L$ and network layer widths defined by $n_{0},n_{L}$ and $n_{l}:=r_{l}(m)$ for $1\\leq l<L$ and some $(r_{l})_{l=1}^{L-1}\\in\\mathcal{R}_{L}$ Then, for fixed $n_{0}$ and any $n_{L}$ the statement $\\mathcal{P}$ holds as $m\\rightarrow\\infty$ \" We also write the statement $\\boldsymbol{S}$ as $S(r)$ ", "page_idx": 18}, {"type": "text", "text": "(i) We say that such a statement $\\boldsymbol{S}$ holds strongly, if $S(r)$ holds for any $r\\in\\mathcal{R}_{L}$ . This can be interpreted as requiring that the statement holds as $\\mathrm{min_{1}}{\\le}l{<}L(n_{l})\\to\\infty$ .We will also write $\\mathcal{P}$ holds as $n_{1},\\ldots,n_{L-1}\\to\\infty$ strongly\".   \n(ii) We say that such a statement $\\boldsymbol{S}$ holds for $(n_{l})_{1\\le l\\le L-1}\\,\\gtrsim\\,r$ f $\\boldsymbol{S}$ holds for all $r\\,\\in\\,\\mathcal{R}_{L}$ with $r_{l}(m)\\stackrel{\\propto}{\\sim}m$ for all $1\\leq l<L$ In other words $S(r)$ holds for all $r\\in\\mathcal{R}_{L}$ such that $r_{p}(m)/r_{q}(m)\\to\\alpha_{p,q}\\in(0,\\infty)$ as $m\\rightarrow\\infty$ .We will also write $^{\\epsilon\\epsilon}\\mathcal{P}$ holds as $(n_{l})_{1\\leq l<L}$ $n^{\\,\\,,}$ ", "page_idx": 18}, {"type": "text", "text": "(ii\uff09 We say that such a statement $\\boldsymbol{S}$ holds weakly, if $\\boldsymbol{S}$ holdsfor atleast one $r\\in\\mathcal{R}_{L}$ .This can be read as requiring that the statement holds as $n_{1}\\rightarrow\\infty,\\ldots,n_{L-1}\\rightarrow\\infty$ sequentially. We will also write \u201c $\\mathcal{P}$ holds as $n_{1},\\ldots,n_{L-1}\\to\\infty$ weakly'\". ", "page_idx": 19}, {"type": "text", "text": "Theorem C.1 (Theorem 4 from Matthews et al. [2018]). Any network function $f$ ofdepth $L$ defined as in Definition C.1 with continuous activation function $\\sigma$ that satisfies the linear envelope property, i.e., there exist $c,m\\geq0$ with ", "page_idx": 19}, {"type": "equation", "text": "$$\n|\\sigma(u)|\\leq c+m|u|\\quad\\forall u\\in\\mathbb{R},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "converges in distribution as $n_{1},\\ldots,n_{L-1}\\to\\infty$ strongly to a multidimensional Gaussian process $(X_{j})_{j=1}^{n_{L}}$ for any fixed countable input set $(x_{i})_{i=1}^{\\infty}$ . It holds $X_{j}\\overset{i i d}{\\sim}\\mathcal{N}(0,\\Sigma^{(L)})$ where the covariance function $\\Sigma^{(L)}$ is recursively given by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\Sigma^{(1)}(x,x^{\\prime})=\\displaystyle\\frac{\\sigma_{w}^{2}}{n_{0}}\\langle x,x^{\\prime}\\rangle+\\sigma_{b}^{2},}\\\\ {\\Sigma^{(L)}(x,x^{\\prime})=\\sigma_{w}^{2}\\,\\mathbb{E}_{g\\sim\\mathcal{N}(0,\\Sigma^{(L-1)})}\\big[\\sigma(g(x))\\,\\sigma(g(x^{\\prime}))\\big]+\\sigma_{b}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Remark C.5. First, a proof of the above theorem can be found in the paper of Matthews et al. [2018]. While it takes a lot of effort to show that the statement holds strongly in the sense of Definition C.7, theweakversion of thestatement can be provedvia induction.This has been donebyJacot et al. [2018] and we will later adapt their proof to show a generalized version, Theorem E.3. ", "page_idx": 19}, {"type": "text", "text": "Second, in the context of analyzing the network behavior, we are interested in the finite-dimensional distributions first of all, since neural networks are trained and tested on a finite number of data points. From the convergence of the marginal distributions, we can infer the convergence to an stochastic process via the Kolmogorov extension theorem.However, this assumes the product $\\sigma$ -algebra,which is whyTheorem C.1 assumes afixedcountableinput set.Matthews et al.[2018]have discussed these formal restrictions in more detail (Chapter 2.2). If one does not want to be restricted to a countable index set, one could, for example, consider the condition by Prokhorov [1956, Theorem 2.1]. A similar approach was taken by Bracale et al. [2021], which applied the Kolmogorov-Chentsov criterion[Kallenberg,2021,Theorem4.23]. ", "page_idx": 19}, {"type": "text", "text": "Finally, note that the theorem assumes continuity of the activation function. In the proof of Matthews et al. [2018] this is only used in order to apply the continuous mapping theorem. However, it is suffcient for the limiting process to attain possible points of discontinuity with probability zero for the continuous mapping theorem to be applicable. The theorem is thus also valid for activation functions that are continuous except at finitely many jump points, such as step-like activation functions. ", "page_idx": 19}, {"type": "text", "text": "Convergence of the NTK at initialization in the infinite-width limit. Jacot et al. [2018] showed that the previously defined empirical NTK converges to a deterministic limit, which we will call the analyticNTK. ", "page_idx": 19}, {"type": "text", "text": "Theorem C.2 (Theorem 1 from Jacot et al. [2018], slightly generalized). For any network function of depth $L$ defined as in Definition C.1 with Lipschitz. continuous activation function $\\sigma$ the empiricalneural tangentkernel $\\hat{\\Theta}^{(L)}$ converges in probability to a constant kernel $\\Theta^{(L)}\\otimes\\mathrm{I}_{n_{L}}$ as $n_{1},\\ldots,n_{L-1}\\to\\infty$ weakly. For all $x,x^{\\prime}\\in\\mathbb{R}^{n_{0}}$ and $1\\leq i,j\\leq n_{L}$ it holds ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\hat{\\Theta}_{i\\,j}^{(L)}(x,x^{\\prime})\\stackrel{\\mathcal{P}}{\\longrightarrow}\\delta_{i j}\\,\\Theta^{(L)}(x,x^{\\prime}),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which we also write as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\hat{\\Theta}^{(L)}\\stackrel{\\mathcal{P}}{\\longrightarrow}\\Theta^{(L)}\\otimes\\mathrm{I}_{n_{L}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Wecall $\\Theta^{(L)}$ the analytic neural tangent kernel of the network, which is recursively given by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\Theta^{(1)}(x,x^{\\prime})=\\Sigma^{(1)}(x,x^{\\prime})}}\\\\ {{\\Theta^{(L)}(x,x^{\\prime})=\\Sigma^{(L)}(x,x^{\\prime})+\\Theta^{(L-1)}(x,x^{\\prime})\\cdot\\dot{\\Sigma}^{(L)}(x,x^{\\prime}),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\Sigma^{(l)}$ are defined as in Theorem C.1 and we define ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\dot{\\Sigma}^{(L)}(x,x^{\\prime})=\\sigma_{w}^{2}\\,\\mathbb{E}_{g\\sim\\mathcal{N}(0,\\Sigma^{(L-1)})}\\left[\\dot{\\sigma}(g(x))\\,\\dot{\\sigma}(g(x^{\\prime}))\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Compared to Theorem 1 of Jacot et al. [2018], the statement is slightly generalized in the sense that it allows for arbitrary $\\sigma_{w}>0$ . The arguments in the proof work the same way. ", "page_idx": 19}, {"type": "text", "text": "Remark C.6 (Versions of Theorem C.2 in the literature). A proof of this theorem for $(n_{l})_{1\\leq l<L}$ X $n$ is given by Yang [2019a] and his proof is also referenced by Lee et al. [2019]. However, the proof is given in terms of so-called tensor programs and therefore harder to follow. For the ReLU activation function,aprooffor $n_{1},\\ldots,n_{L-1}\\to\\infty$ strongly is provided by Arora et al. [2019, Theorem 3.1]. We will later prove a version of this theorem for the generalized NTK. ", "page_idx": 20}, {"type": "text", "text": "Convergence of the NTK during training in the infinite-width limit. Not only does the NTK converge to a constant kernel in the infinite-width limit, even the kernel during training, (L), converges to this constant kernel. This was also discovered by Jacot et al. [2018]. ", "page_idx": 20}, {"type": "text", "text": "Theorem C.3 (Theorem 2 by Jacot et al. [2018]). Assume any network function of depth $L$ definedas inDefinition $C.I$ withLipschitzcontinuous activationfunction $\\sigma$ twicedifferentiablewithbounded second derivative, and trained with gradient fow as in Equation S10. Let $T>0$ suchthat ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\int_{0}^{T}\\!\\|\\nabla\\ell(f_{t}(\\cdot\\,);f^{*}(\\cdot\\,))\\|_{p_{\\mathrm{cmp}}}\\,\\mathrm{d}t=\\int_{0}^{T}\\sqrt{d}\\left\\|\\nabla_{f_{t}(x)}\\mathcal{L}(f_{t}(\\mathcal{X});f^{*}(\\mathcal{X}))\\right\\|_{2}\\,\\mathrm{d}t\\in\\mathcal{O}_{p}(1),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $X\\in\\mathcal{O}_{p}(1)$ denotes that $X$ is stochastically bounded. Then, as $n_{1},\\ldots,n_{L-1}\\to\\infty$ weakly, the empirical NTK $\\hat{\\Theta}_{t}^{(L)}$ converges in probabiliy to the analyic NTK $\\Theta^{(L)}\\otimes\\mathrm{I}_{n_{L}}$ in probability uniformly for $t\\in[0,T]$ . We therefore write ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\hat{\\Theta}_{t}^{(L)}\\stackrel{\\mathcal{P}}{\\longrightarrow}\\Theta^{(L)}\\otimes\\mathrm{I}_{n_{L}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Remark C.7 (Versions of Theorem C.3 in the literature). The proof of Jacot et al. [2018] relies heavily on a function space perspective. Since this formulation tends to lack mathematical rigor, we will rely on the proof of the theorem for the case $(n_{l})_{1\\leq l<L}\\gtrsim n$ given by Lee et al. [2019, Chapter G]. In particular, the first inequality of (S51) in Theorem G.2 of Lee et al. $I20I9J$ impliesthecondition (\\*). Furthermore, a different approach to proving the above statement for the case $(n_{l})_{1\\leq l<L}$ $n$ using the Hessian matrix of the network function was taken by Liu et al. [2020, Proposition 2.3, Theorem 3.2]. A partial proof of a version of this theorem for the generalized NTK will be given later. Only an auxiliary lemma remains to be proved. ", "page_idx": 20}, {"type": "text", "text": "C.2.1 Gradient flow in the infinite-width limit ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Given the results of the previous section, we can formulate an infinite width version of Equation (S10) by replacing the empirical with the analytic NTK. This allows us to analyze the learning dynamics of networks in the infinite-width limit, which yields connections to kernel methods and reproducing kernel Hilbert spaces. We then discuss how far the resulting functions and solutions in the infinite-width limit deviate from the finite width networks. This is essential to evaluate to what extend the results in the infinite-width limit can inform us about the behavior of gradient flow in the finite-width networks. First, we state the infinite-width version of Equation (S11) using Theorem C.3, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\mathrm{d}}{\\mathrm{d}t}f_{t}(x)=-\\eta\\left(\\Theta^{(L)}\\otimes\\mathbf{I}_{n_{L}}\\right)(x,\\chi)\\nabla_{f_{t}(x)}\\mathcal{L}(f_{t}(x);y)}\\\\ &{\\quad\\quad\\quad=-\\eta\\frac{1}{d}\\displaystyle\\sum_{i=1}^{d}\\Theta^{(L)}(x,x_{i})\\cdot\\mathbf{I}_{n_{L}}\\nabla\\ell(f_{t}(x_{i});y_{i})}\\\\ &{\\quad\\quad\\quad=-\\eta\\displaystyle\\sum_{i=1}^{d}\\Theta^{(L)}(x,x_{i})\\displaystyle\\frac{1}{d}\\nabla\\ell(f_{t}(x_{i});y_{i})}\\\\ &{\\quad\\quad\\quad=-\\eta\\Theta^{(L)}(x,\\chi)\\displaystyle\\frac{1}{d}\\left[\\nabla\\ell(f_{t}(x_{1});y_{1}),\\ldots,\\nabla\\ell(f_{t}(x_{d});y_{d})\\right]^{\\intercal}}\\\\ &{\\quad\\quad\\quad=-\\eta\\Theta^{(L)}(x,\\chi)\\displaystyle\\nabla_{f_{t}(x)}\\mathcal{L}(f_{t}(x);y),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where in the last line we interpret $\\nabla_{f_{t}(x)}\\mathcal{L}\\big(f_{t}(\\mathcal{X});\\mathcal{Y}\\big)\\in\\mathbb{R}^{d\\times n_{L}}$ as a matrix of size $d\\times n_{L}$ with entries $\\left[\\nabla{\\mathcal{L}}_{f_{t}(\\mathcal{X})}(f_{t}(\\mathcal{X});\\mathcal{Y})\\right]_{i j}\\,=\\,1/{d}\\cdot\\partial_{j}\\ell(f_{t}(x_{i});y_{i})$ . Recall that $\\partial_{j}\\ell(f_{t}(x_{i});y_{i})$ is the partial derivative of $\\ell(\\,\\cdot\\,;y_{i})$ with respect to its $j$ -th entry, i.e., with respect to $f_{t,j}(\\boldsymbol{x}_{i})$ .Note that the last line is a row vector, which we can identify as a column vector. The fact that the NTK is now time-independent and non-random has two interesting implications: ", "page_idx": 20}, {"type": "text", "text": "\u00b7 Equation (S16) is now an differential equation that can be solved explicitly or numerically for certain loss functions.   \n\u00b7 According to Equation (S15), the time derivative of $f_{t}(x)$ can now be expressed elementwise as a linear combination of functions of the type $\\Theta^{(L)}(\\,\\cdot\\,,\\tilde{x})\\colon\\mathbb{R}^{n_{0}}\\to\\mathbb{R}$ . For an arbitrary symmetric and positive definite kernel $k(\\,\\cdot\\,,\\,\\cdot\\,)$ , the completion of the linear span of functions of this type is called the reproducing kernel Hilbert space (RKHS) of $k$ . Assuming that the solution of Equation (S15) is an element of the RKHS of $\\Theta^{(L)}$ , one can ask what the space looks like. ", "page_idx": 21}, {"type": "text", "text": "The ODE of Equation (S16) has already been considered by Jacot et al. [2018, Chapter 5] and Lee et al. [2019, Chapter 2.2], and we will follow the observations made there. To do this, we will assume the mean squared error (MSE) loss, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\tilde{\\mathcal{Y}};\\mathcal{Y})=\\frac{1}{2}\\|\\tilde{\\mathcal{Y}}-\\mathcal{Y}\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "implying $\\nabla_{f_{t}(\\mathcal{X})}\\mathcal{L}(f_{t}(\\mathcal{X});\\mathcal{Y})=f_{t}(\\mathcal{X})-\\mathcal{Y}$ where $f_{t}(\\mathcal X)$ and $\\boldsymbol{\\wp}$ are again interpreted as matrices of dimension $d\\times\\dot{n}_{L}$ . This gives us the following ODE ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\,f_{t}(x)=-\\eta\\,\\Theta^{(L)}(x,\\mathcal{X})\\,\\left(f_{t}(\\mathcal{X})-\\mathcal{Y}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Now, for simplicity, we denote $\\Theta(x,y):=\\Theta^{(L)}(x,y)$ and $\\Theta:=\\Theta(\\mathcal{X},\\mathcal{X})$ . Furthermore, we consider an arbitrary set of test points $\\scriptstyle{\\mathcal{X}}_{T}$ . The solution of the ODE is then given by ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{t}(\\mathcal{X}_{T})=\\mu_{t}(\\mathcal{X}_{T})+\\gamma_{t}(\\mathcal{X}_{T})\\quad\\mathrm{for}}\\\\ &{\\mu_{t}(\\mathcal{X}_{T})=\\Theta(\\mathcal{X}_{T},\\mathcal{X})\\Theta^{-1}\\left(\\mathrm{I}_{d}-e^{-\\eta\\Theta t}\\right)\\mathcal{Y}\\quad\\mathrm{and}}\\\\ &{\\gamma_{t}(\\mathcal{X}_{T})=f_{0}(\\mathcal{X}_{T})-\\Theta(\\mathcal{X}_{T},\\mathcal{X})\\Theta^{-1}\\left(\\mathrm{I}_{d}-e^{-\\eta\\Theta t}\\right)f_{0}(\\mathcal{X}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Recall that by Theorem C.1, the components $f_{0,j}$ are independent and identically distributed Gaussian processes with mean zero and covariance function $\\Sigma:=\\Sigma^{(L)}$ . Hence, $\\gamma_{t}$ has mean zero and the mean of $f_{t}$ is given by $\\mu_{t}$ . By looking at the components of $f_{t}$ \uff0c ", "page_idx": 21}, {"type": "equation", "text": "$$\nf_{t,j}(\\mathcal{X}_{T})=f_{0,j}(\\mathcal{X}_{T})-\\Theta(\\mathcal{X}_{T},\\mathcal{X})\\Theta^{-1}\\left(\\mathrm{I}_{d}-e^{-\\eta\\Theta t}\\right)\\left(f_{0,j}(\\mathcal{X}_{T})-\\mathcal{Y}_{j}\\right),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "we can conclude that they are independent and identically distributed as well. One can show that the components are indeed Gaussian processes again with mean $\\mu_{t}$ .Using $\\gamma_{t}$ wecan also compute the covariance matrix for our arbitrary set of test points $\\scriptstyle{\\mathcal{X}}_{T}$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Gamma_{t}(\\mathcal{X}_{T},\\mathcal{X}_{T}):=\\mathbb{E}\\left[\\gamma_{t,j}(\\mathcal{X}_{T})\\,\\gamma_{t,j}(\\mathcal{X}_{T})^{\\top}\\right]=\\mathbb{E}\\left[f_{0,j}(\\mathcal{X}_{T})\\,f_{0,j}(\\mathcal{X}_{T})^{\\top}\\right]}\\\\ &{\\mathrm{~-~}\\mathbb{E}\\left[f_{0,j}(\\mathcal{X}_{T})f_{0,j}(\\mathcal{X})^{\\top}\\left(\\mathrm{I}_{d}-e^{-\\eta\\Theta t}\\right)\\Theta^{-1}\\Theta(\\mathcal{X},\\mathcal{X}_{T})\\right]}\\\\ &{\\mathrm{~-~}\\mathbb{E}\\left[\\Theta(\\mathcal{X}_{T},\\mathcal{X})\\Theta^{-1}\\left(\\mathrm{I}_{d}-e^{-\\eta\\Theta t}\\right)\\,f_{0,j}(\\mathcal{X})f_{0,j}(\\mathcal{X}_{T})^{\\top}\\right]}\\\\ &{\\mathrm{~+~}\\mathbb{E}\\left[\\Theta(\\mathcal{X}_{T},\\mathcal{X})\\Theta^{-1}\\left(\\mathrm{I}_{d}-e^{-\\eta\\Theta t}\\right)\\,f_{0,j}(\\mathcal{X})f_{0,j}(\\mathcal{X})^{\\top}\\left(\\mathrm{I}_{d}-e^{-\\eta\\Theta t}\\right)\\Theta^{-1}\\Theta(\\mathcal{X},\\mathcal{X}_{T})\\right]}\\\\ &{\\mathrm{~=~}\\Sigma(\\mathcal{X}_{T},\\mathcal{X}_{T})-\\Sigma(\\mathcal{X}_{T},\\mathcal{X})\\left(\\mathrm{I}_{d}-e^{-\\eta\\Theta t}\\right)\\Theta^{-1}\\Theta(\\mathcal{X},\\mathcal{X}_{T})}\\\\ &{\\mathrm{~-~}\\Theta(\\mathcal{X}_{T},\\mathcal{X})\\Theta^{-1}\\left(\\mathrm{I}_{d}-e^{-\\eta\\Theta t}\\right)\\Sigma(\\mathcal{X},\\mathcal{X}_{T})}\\\\ &{\\mathrm{~+~}\\Theta(\\mathcal{X}_{T},\\mathcal{X})\\Theta^{-1}\\left(\\mathrm{I}_{d}-e^{-\\eta\\Theta t}\\right)\\Sigma(\\mathcal{X},\\mathcal{X})\\left(\\mathrm{I}_{d}-e^{-\\eta\\Theta t}\\right)\\Theta^{-1}\\Theta(\\mathcal{X},\\mathcal{X}_{T}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Assuming that $\\Theta$ is positive definite immediately leads to pointwise convergence of the mean and covariance functions. This implies that the gradient flow solution for networks in the infinite-width limit converges to a Gaussian process as $t\\to\\infty$ with mean function $\\mu_{\\infty}$ and covariance function $\\Gamma_{\\infty}$ given below. This follows from the weak convergence of the finite-dimensional marginal distributions by L\u00e9vy's convergence theorem [Williams, 1991, Section 18.1]. Again, the discussions of Remark C.5 applies. We have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{\\infty}(\\mathcal{X}_{T})=\\Theta(\\mathcal{X}_{T},\\mathcal{X})\\Theta^{-1}\\mathcal{Y}\\quad\\mathrm{and}}\\\\ &{\\Gamma_{\\infty}(\\mathcal{X}_{T},\\mathcal{X}_{T})=\\Sigma(\\mathcal{X}_{T},\\mathcal{X}_{T})-\\Sigma(\\mathcal{X}_{T},\\mathcal{X})\\Theta^{-1}\\Theta(\\mathcal{X},\\mathcal{X}_{T})}\\\\ &{\\quad-\\,\\Theta(\\mathcal{X}_{T},\\mathcal{X})\\Theta^{-1}\\Sigma(\\mathcal{X},\\mathcal{X}_{T})+\\Theta(\\mathcal{X}_{T},\\mathcal{X})\\Theta^{-1}\\Sigma(\\mathcal{X},\\mathcal{X})\\Theta^{-1}\\Theta(\\mathcal{X},\\mathcal{X}_{T}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Lee et al. [2019] state that a network trained with gradient fow will indeed converge in distribution to this Gaussian process as the width goes to infinity: ", "page_idx": 21}, {"type": "text", "text": "Theorem C.4 (Theorem 2.2 from Lee et al. [2019]). Let the learning rate $\\eta<\\eta_{\\mathrm{critical}}\\,f o r$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\eta_{\\mathrm{critical}}:=2(\\lambda_{\\operatorname*{min}}(\\Theta^{(L)}(\\mathcal{X},\\mathcal{X}))+\\lambda_{\\operatorname*{max}}(\\Theta^{(L)}(\\mathcal{X},\\mathcal{X})))\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "with a network function $f_{t}$ as in Theorem C.3 with hidden layer widths $n_{1}=\\cdot\\cdot\\cdot=n_{L-1}=n$ and restricted to $\\boldsymbol{x}\\in\\mathbb{R}^{n_{0}}$ with $\\|x\\|_{2}\\leq1.$ If $\\lambda_{\\operatorname*{min}}\\big(\\Theta^{(L)}(\\mathcal{X},\\mathcal{X})\\big)>0$ then thecomponents of $f_{t}$ converge in distribution to independent, identically distributed Gaussian processes $\\mathcal{N}(\\mu_{t},\\Gamma_{t})$ as $n\\to\\infty$ for all $t\\in[0,\\infty)\\cup\\{\\infty\\}$ ", "page_idx": 22}, {"type": "text", "text": "Hence, the result of training a finite-width network with gradient flow for an infinite amount of time will be arbitrarily close in distribution to a Gaussian process with mean function $\\mu_{\\infty}$ andcovariance function $\\Gamma_{\\infty}$ , if the width is sufficiently large. Note that by Equations (S17) and (S18) the variance at the training points $\\mathcal{X}$ is zero and the mean at the training points is exactly $\\boldsymbol{\\wp}$ ", "page_idx": 22}, {"type": "text", "text": "Since we will focus on the mean, we will first sketch a trick introduced in Chapter 3 of Arora et al. [2019] to make the variance term arbitrarily small. If $f_{0}$ had mean variance, this would consequently also be the case for all $f_{t}$ and for the solution $f_{\\infty}:=\\operatorname*{lim}_{t\\to\\infty}f_{t}$ This can be achieved by multiplying $f_{0}$ by a small constant $\\kappa>0$ and considering the network function $g_{0}=\\kappa f_{0}$ instead. It then holds ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\hat{\\Theta}_{g}(x,y)=J_{\\theta}g_{0}(x;\\theta)J_{\\theta}g_{0}(y;\\theta)^{\\top}=J_{\\theta}\\kappa f_{0}(x;\\theta)J_{\\theta}\\kappa f_{0}(y;\\theta)^{\\top}=\\kappa^{2}\\hat{\\Theta}_{f}(x,y),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and thus we have $\\Theta_{g}=\\kappa^{2}\\Theta_{f}$ . In the infinite-width limit, the derivative of $g_{t}$ is then given by ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\mathrm{d}}{\\mathrm{d}t}g_{t}(x)=-\\eta\\,\\Theta_{g}(x,\\mathcal{X})\\left(g_{t}(\\mathcal{X})-\\mathcal{Y}\\right)}\\\\ &{\\qquad\\qquad\\quad=-\\eta\\,\\kappa^{2}\\Theta_{f}(x,\\mathcal{X})\\left(\\kappa f_{t}(\\mathcal{X})-\\mathcal{Y}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which implies as before ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g_{t}(x)=g_{0}(x)-\\Theta_{g}(x,\\mathcal{X})\\Theta_{g}(\\mathcal{X},\\mathcal{X})^{-1}\\left(\\mathrm{I}_{d}-e^{-\\eta\\,\\Theta_{g}(\\mathcal{X},\\mathcal{X})t}\\right)(g_{0}(\\mathcal{X})-\\mathcal{Y})}\\\\ &{\\qquad=\\Theta_{f}(x,\\mathcal{X})\\Theta_{f}(\\mathcal{X},\\mathcal{X})^{-1}\\left(\\mathrm{I}_{d}-e^{-\\eta\\kappa^{2}\\,\\Theta_{f}(\\mathcal{X},\\mathcal{X})t}\\right)\\mathcal{Y}}\\\\ &{\\qquad+\\,\\kappa\\left(f_{0}(x)-\\Theta_{f}(x,\\mathcal{X})\\Theta_{f}(\\mathcal{X},\\mathcal{X})^{-1}\\left(\\mathrm{I}_{d}-e^{-\\eta\\kappa^{2}\\,\\Theta_{f}(\\mathcal{X},\\mathcal{X})t}\\right)f_{0}(\\mathcal{X})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Note that the term in the second last line corresponds to the non-random mean of $f$ trained with learning rate $\\eta\\kappa^{2}$ , and that the term in the last line is random, but can be made arbitrarily small using $\\kappa$ . We can think of this as a trade-off between learning rate and variance. This justifies why we can focus on the mean in the next section. ", "page_idx": 22}, {"type": "text", "text": "To sum up, we are interested in network functions in the infinite-width limit that are trained over time accordingto ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}f_{t}(x)=-\\eta\\,\\Theta(x,\\lambda)\\,\\left(f_{t}(\\lambda)-\\mathcal{Y}\\right)=\\sum_{i=1}^{d}\\Theta(x,x_{i})\\,\\left(-\\eta\\,\\left(f_{t}(x_{i})-y_{i}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where we change from a row vector to a column vector in the last equation. The mean of such network functions after infinite training time is given by ", "page_idx": 22}, {"type": "equation", "text": "$$\nf_{\\mathrm{NTK}}(x):=\\Theta(x,\\chi)\\Theta(\\chi,\\chi)^{-1}\\mathcal{Y}=\\mu_{\\infty}(x).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "D  The NTK for sign activation function ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The first observation to make in our attempt to apply the neural tangent kernel to networks with the sign function as activation function is that the sign function has a zero derivative almost everywhere. Thus, the derivative of the network function with respect to the network weights is zero for all weights that are not part of the last layer. The case where the weights $\\theta^{(1:L-1)}$ are frozen after initialization and only $\\bar{\\theta^{(L)}}$ is trained has already been discussed by Lee et al. [2019, Chapter 2.3.1 and Chapter D]. For a network in the infinite-width limit, this approach is equivalent to applying Gaussian process regression, i., knowing that $f\\sim\\mathcal{N}\\left(0,\\Sigma^{(L)}\\right)$ for infinite width, one considers $f\\ |\\ f(\\mathcal{X})=\\mathcal{Y}$ This can be seen by realizing that $\\Theta^{(L)}=\\Sigma^{(L)}$ $\\dot{\\sigma}=0$ almost everywhere and applying Theorem C.4. ", "page_idx": 22}, {"type": "text", "text": "While this is an interesting observation, and the strategy of optimizing only the last layer can also be transferred to finite width networks, we would prefer to train the whole network and not identify the derivative of the sign function with zero, since this discards all information about the jump discontinuities in our networks. An obvious alternative would be to use the distributional derivative of the sign function, which is given by $2\\,\\delta_{0}$ ,where $\\delta_{0}$ denotes the delta distribution. We will see that $\\dot{\\Sigma}^{(L)}$ still exists when the distributional derivative is substituted into its formula. Alternatively, we can obtain the same expression by approximating the sign function with scaled error functions, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname{erf}_{m}(z)=\\operatorname{erf}(m\\cdot z)={\\frac{2}{\\sqrt{\\pi}}}\\int_{0}^{m\\cdot z}e^{-t^{2}}\\,\\mathrm{d}t,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and considering the limit $m\\rightarrow\\infty$ ", "page_idx": 23}, {"type": "text", "text": "D.1  The NTK for error activation function ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Due to the previous considerations, we begin by deriving the analytic NTK for the error function. Following the notation of Lee et al. [2019], we need to find analytic expressions for the terms ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{T}_{m}(\\Sigma):=\\mathbb{E}_{(X,Y)\\sim\\mathcal{N}(0,\\Sigma)}[\\mathrm{erf}_{m}(X)\\,\\mathrm{erf}_{m}(Y)]\\quad\\mathrm{and}}\\\\ &{\\dot{\\mathcal{T}}_{m}(\\Sigma):=\\mathbb{E}_{(X,Y)\\sim\\mathcal{N}(0,\\Sigma)}[\\dot{\\mathrm{erf}}_{m}(X)\\,\\dot{\\mathrm{erf}}_{m}(Y)].}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Note that by a change of variables we can alternatively consider the terms ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{T}(m^{2}\\cdot\\Sigma):=\\mathbb{E}_{(X,Y)\\sim\\mathcal{N}(0,m^{2}\\cdot\\Sigma)}[\\mathrm{erf}(X)\\,\\mathrm{erf}(Y)]=\\mathcal{T}_{m}(\\Sigma)\\quad\\mathrm{and}}\\\\ &{\\dot{\\mathcal{T}}(m^{2}\\cdot\\Sigma):=\\mathbb{E}_{(X,Y)\\sim\\mathcal{N}(0,m^{2}\\cdot\\Sigma)}[\\dot{\\mathrm{erf}}(X)\\,\\dot{\\mathrm{erf}}(Y)]=\\cfrac{1}{m^{2}}\\dot{\\mathcal{T}}_{m}(\\Sigma).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For $\\begin{array}{r}{\\Sigma^{\\prime}=\\big(\\,_{x\\cdot y\\ y\\cdot y}^{\\ x\\cdot x\\ x\\cdot y}\\,\\big),\\mathcal{T}(\\Sigma^{\\prime})}\\end{array}$ and $\\dot{\\mathcal{T}}(\\Sigma^{\\prime})$ are given in Chapter $\\mathbf{C}$ of the supplementarymaterial of Lee et al. [2019]. However, we cannot assume that $\\Sigma^{\\prime}$ always has this form. While $\\dot{\\mathcal{T}}$ can be easily calculated, $\\tau$ is harder to deal with and a reference to Williams [1996, Chapter 3.1] is used. There, the main idea of the proof, how to evaluate a more general expression, is given without further details. We will derive analytic expressions for both terms explicitly. ", "page_idx": 23}, {"type": "text", "text": "We start by evaluating $\\dot{\\mathcal{T}}$ Note that ", "page_idx": 23}, {"type": "equation", "text": "$$\n{\\frac{\\mathrm{d}}{\\mathrm{d}z}}\\mathrm{erf}(z)={\\frac{2}{\\sqrt{\\pi}}}e^{-z^{2}}\\quad{\\mathrm{and}}\\quad{\\frac{\\mathrm{d}}{\\mathrm{d}z}}\\mathrm{erf}_{m}(z)={\\frac{2m}{\\sqrt{\\pi}}}e^{-m^{2}z^{2}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Lemma D.1. Given $U\\sim{\\mathcal{N}}(0,\\Sigma)\\,$ with invertible covariance matrix $\\Sigma\\in\\mathbb{R}^{d\\times d}$ and $x,y\\in\\mathbb{R}^{d}$ it holds ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\operatorname{erf}\\left(U^{\\top}x\\right)\\operatorname{erf}(U^{\\top}y)]={\\frac{4}{\\pi}}\\left((1+2x^{\\top}\\Sigma x)(1+2y^{\\top}\\Sigma y)-(2x^{\\top}\\Sigma y)^{2}\\right)^{-1/2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In particular, given $(X,Y)\\sim{\\mathcal{N}}(0,\\Sigma)$ with invertible covariance matrix $\\Sigma\\in\\mathbb{R}^{2\\times2}$ orwith $X=Y$ and singular covariance matrix $\\dot{\\Sigma}\\in\\dot{\\mathbb{R}}^{2\\times2}$ ,itholds ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\dot{\\mathcal{T}}(\\Sigma)=\\mathbb{E}[\\operatorname{erf}(X)\\operatorname{erf}(Y)]=\\frac{4}{\\pi}|\\mathrm{I}_{2}+2\\cdot\\Sigma|^{-1/2},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $|A|$ denotes the determinant of a matrix $A$ ", "page_idx": 23}, {"type": "text", "text": "Proof. It holds for $U\\sim{\\mathcal{N}}(0,\\Sigma)$ with covariance matrix $\\Sigma\\in\\mathbb{R}^{d\\times d}$ and $x,y\\in\\mathbb{R}^{d}$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\mathrm{e}^{\\boldsymbol{\\star}}\\mathbf{f}(U^{\\boldsymbol{\\uptau}}x)\\operatorname{erf}(U^{\\boldsymbol{\\uptau}}y)]=\\int_{\\mathbb{R}^{d}}\\frac{1}{(2\\pi)^{d/2}|\\boldsymbol{\\Sigma}|^{1/2}}\\left(\\frac{2}{\\sqrt{\\pi}}e^{-(u^{\\boldsymbol{\\uptau}}x)^{2}}\\right)\\left(\\frac{2}{\\sqrt{\\pi}}e^{-(u^{\\boldsymbol{\\uptau}}y)^{2}}\\right)e^{-\\frac{1}{2}u^{\\boldsymbol{\\uptau}}\\boldsymbol{\\Sigma}^{-1}u}\\,\\mathrm{d}u}\\\\ &{\\overset{(\\star)}{=}\\frac{4}{\\pi}\\int_{\\mathbb{R}^{d}}\\frac{1}{(2\\pi)^{d/2}}\\exp\\left(-\\frac{1}{2}v^{\\boldsymbol{\\uptau}}(\\mathrm{I}_{d}+2\\boldsymbol{\\Sigma}^{1/2}x x^{\\boldsymbol{\\uptau}}\\Sigma^{1/2}+2\\boldsymbol{\\Sigma}^{1/2}y y^{\\boldsymbol{\\uptau}}\\Sigma^{1/2})v\\right)\\,\\mathrm{d}v}\\\\ &{=\\frac{4}{\\pi}\\left|\\left(\\mathrm{I}_{d}+2\\boldsymbol{\\Sigma}^{1/2}x x^{\\boldsymbol{\\uptau}}\\Sigma^{1/2}+2\\boldsymbol{\\Sigma}^{1/2}y y^{\\boldsymbol{\\uptau}}\\Sigma^{1/2}\\right)^{-1}\\right|^{1/2}}\\\\ &{=\\frac{4}{\\pi}\\left|\\mathrm{I}_{d}+2\\boldsymbol{\\Sigma}^{1/2}x x^{\\boldsymbol{\\uptau}}\\Sigma^{1/2}+2\\boldsymbol{\\Sigma}^{1/2}y y^{\\boldsymbol{\\uptau}}\\Sigma^{1/2}\\right|^{-1/2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "using a change of variable $\\Sigma^{1/2}u=v$ for Equation $(\\star)$ and using basic properties of the determinant. We can evaluate the determinant in the last line by applying the Sylvester's determinant theorem [Pozrikidis, 2014, (B.1.16)], i.e., $|\\mathrm{I}_{n}+A B^{\\intercal}|=|\\dot{\\mathrm{I}_{m}}+\\dot{B}^{\\intercal}\\bar{A}|$ for any matrices $A,B\\in\\mathbb{R}^{n\\times m}$ .We then define $A=B=(\\sqrt{2}\\Sigma^{1/2}x,\\sqrt{2}\\Sigma^{1/2}y)\\in\\mathbb{R}^{d\\times2}$ , which yields ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~~\\left|\\mathrm{I}_{d}+2\\Sigma^{1/2}x x^{\\intercal}\\Sigma^{1/2}+2\\Sigma^{1/2}y y^{\\intercal}\\Sigma^{1/2}\\right|=|\\mathrm{I}_{d}+A B^{\\intercal}|}\\\\ &{=|\\mathrm{I}_{2}+B^{\\intercal}A|=\\left|\\left(\\begin{array}{c c}{1+2x^{\\intercal}\\Sigma x}&{2x^{\\intercal}\\Sigma y}\\\\ {2x^{\\intercal}\\Sigma y}&{1+2y^{\\intercal}\\Sigma y}\\end{array}\\right)\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "This directly implies Equation (S21). Furthermore, Equation (S22) follows with $\\Sigma\\,\\in\\,\\mathbb{R}^{2\\times2}$ and $\\bar{x}=(\\O_{0}^{1}),\\bar{y}=(\\O_{1}^{\\0})$ or with $x=y=\\left({\\O_{0}^{1}}\\right)$ and an arbitrary invertible covariance matrix $\\Sigma^{\\prime}$ such that $\\Sigma_{11}^{\\prime}=\\Sigma_{11}$ \u53e3 ", "page_idx": 24}, {"type": "text", "text": "Corollary D.1. Given $(X,Y)\\sim{\\mathcal{N}}(0,\\Sigma)$ with invertible covariance matrix $\\Sigma,$ it holds ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\dot{\\mathcal{T}}_{m}(\\Sigma)=\\mathbb{E}[\\operatorname{erf}_{m}(X)\\operatorname{erf}_{m}(Y)]=\\frac{2}{\\pi}\\left|\\Sigma+\\mathbf{I}_{2}/(2m^{2})\\right|^{-1/2}\\xrightarrow{m\\rightarrow\\infty}\\frac{2}{\\pi}|\\Sigma|^{-1/2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "$\\textstyle I\\!f\\left(X,Y\\right)\\sim{\\mathcal{N}}(0,\\Sigma)$ with $X=Y$ and singular covariance matrix $\\Sigma\\in\\mathbb{R}^{2\\times2}$ ,it holds ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\dot{\\mathcal{T}}_{m}(\\Sigma)\\xrightarrow{m\\rightarrow\\infty}\\infty.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\dot{\\mathcal{T}}_{m}(\\Sigma)=m^{2}\\,\\dot{\\mathcal{T}}(m^{2}\\cdot\\Sigma)^{\\mathrm{\\scriptsize~Lemma}\\,D.1}\\xrightarrow[\\pi]{4m^{2}}|\\mathrm{I}_{2}+2m^{2}\\Sigma|^{-1/2}=\\frac{2m^{2}}{\\pi}\\left(4m^{4}|\\mathrm{I}_{2}/(2m^{2})+\\Sigma|\\right)^{-1/2}}\\\\ {=\\frac{2}{\\pi}\\left|\\Sigma+\\mathrm{I}_{2}/(2m^{2})\\right|^{-1/2}\\xrightarrow[\\pi]{-\\infty}\\cdots\\underbrace{\\left\\{\\frac{2}{\\pi}|\\Sigma|^{-1/2}\\quad\\mathrm{if}\\,\\Sigma\\,\\mathrm{is\\,invertible},\\,\\right.}}_{\\mathrm{if}\\,\\Sigma\\,\\mathrm{is\\,singular}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Remark D.1. As mentioned at the beginning of this chapter, we can get the same result by considering the distributional derivative of the sign function, $2\\delta_{0}$ Itholdsfor $(\\mathbf{\\bar{\\boldsymbol{X}}},\\boldsymbol{Y})\\sim\\mathcal{N}(\\boldsymbol{0},\\boldsymbol{\\Sigma})$ withinvertible covariancematrix $\\Sigma$ asbefore ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\delta_{0}(X)\\,\\delta_{0}(Y)]=\\int_{\\mathbb{R}^{2}}\\frac{1}{2\\pi|\\Sigma|^{1/2}}2\\delta_{0}(z_{1})\\,2\\delta_{0}(z_{2})\\,e^{-\\frac{1}{2}z^{\\mathsf{T}}\\Sigma^{-1}z}\\,\\mathrm{d}z=\\frac{2}{\\pi}|\\Sigma|^{-1/2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "In thecaseof $X=Y$ the integral is no longer well-defined. ", "page_idx": 24}, {"type": "text", "text": "Next, we consider $\\tau$ by solving a more general problem, which was formulated in slightly less general form by Williams [1996, Chapter 3.1]. ", "page_idx": 24}, {"type": "text", "text": "Lemma D.2. Given $U\\sim{\\mathcal{N}}(0,\\Sigma)\\,$ with invertible covariance matrix $\\Sigma\\in\\mathbb{R}^{d\\times d}$ and $x,y\\in\\mathbb{R}^{d}$ it holds ", "page_idx": 24}, {"type": "equation", "text": "$$\nV:=\\mathbb{E}[\\mathrm{erf}\\left(U^{\\mathsf{T}}x\\right)\\mathrm{erf}\\left(U^{\\mathsf{T}}y\\right)]=\\frac{2}{\\pi}\\arcsin\\left(\\frac{2\\,x^{\\mathsf{T}}\\Sigma y}{\\sqrt{1+2\\,x^{\\mathsf{T}}\\Sigma x}\\,\\sqrt{1+2\\,y^{\\mathsf{T}}\\Sigma y}}\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "In particular, given $(X,Y)\\sim{\\mathcal{N}}(0,\\Sigma)$ with invertible covariance matrix $\\Sigma=\\left({\\scriptstyle\\sum_{1}\\Sigma_{3}}\\atop{\\scriptstyle\\sum_{3}\\Sigma_{2}}\\right)\\in\\mathbb{R}^{2\\times2}\\,o r$ with $X=Y$ and singular covariance matrix $\\Sigma=\\left(\\O_{\\Sigma_{3}}^{\\Sigma_{1}}\\Sigma_{2}^{_{3}}\\right)\\in\\mathbb{R}^{2\\times2}$ \uff0c $\\Sigma_{1}=\\Sigma_{2}=\\Sigma_{3}$ itholds ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\operatorname{erf}(X)\\operatorname{erf}(Y)]={\\frac{2}{\\pi}}\\arcsin\\left({\\frac{2\\,\\Sigma_{3}}{\\sqrt{1+2\\,\\Sigma_{1}}\\,{\\sqrt{1+2\\,\\Sigma_{2}}}}}\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. We follow the proof idea given by Williams [1996, Chapter 3.1], that is, we define $V(\\lambda)$ differentiate the expection, and integrate by parts. We can then se that $\\begin{array}{r}{\\frac{\\mathrm{d}}{\\mathrm{d}\\lambda}V(\\lambda)=(1-\\gamma^{2})^{-1/2}\\,\\frac{\\mathrm{d}\\gamma}{\\mathrm{d}\\lambda}}\\end{array}$ which gives the desired arcsin. So, we define ", "page_idx": 24}, {"type": "equation", "text": "$$\nV(\\lambda)=\\mathbb{E}[\\operatorname{erf}\\left(\\lambda\\cdot U^{\\top}x\\right)\\operatorname{erf}(U^{\\top}y)]=\\int_{\\mathbb{R}^{d}}{\\frac{1}{(2\\pi)^{d/2}\\,|{\\boldsymbol{\\Sigma}}|^{1/2}}}\\operatorname{erf}\\left(\\lambda\\cdot u^{\\top}x\\right)\\operatorname{erf}(u^{\\top}y)\\,e^{-{\\frac{1}{2}}u^{\\top}{\\boldsymbol{\\Sigma}}^{-1}u}\\,\\mathrm{d}u,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and then differentiate with respect to $\\lambda$ on both sides ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\mathrm{d}}{\\mathrm{d}x}V(\\lambda)=\\int_{\\mathbb{R}^{d}}\\frac{1}{\\int(2\\pi)^{d/2}\\big|\\sum_{j=0}^{d}\\ C^{-\\lambda,\\alpha^{\\prime}\\upsilon}\\mathrm{erf}\\big(u^{\\tau}\\mathtt{p}\\big)e^{-\\frac{1}{2}\\pi\\tau_{j}^{\\kappa-1}u}\\,\\mathrm{d}u}}\\\\ &{=x^{\\prime}\\int_{\\mathbb{R}^{d}}\\frac{1}{\\int(2\\pi)^{d/2}\\big|\\sum_{j=0}^{d}\\ C^{-\\lambda,\\alpha^{\\prime}\\upsilon,\\tau}\\mathrm{erf}\\big(u^{\\tau}\\mathtt{p}\\big)e^{-\\frac{1}{2}\\pi\\tau_{j}^{\\kappa-1}u}\\,\\mathrm{d}u}}\\\\ &{\\stackrel{(c)}{=}\\frac{2x^{\\tau}\\Gamma^{2}\\big|^{\\sum_{j=0}^{d}\\big|\\sum_{j=0}^{d}\\big|\\sum_{j=0}^{|\\lfloor\\frac{1}{2}\\big|\\tau\\rfloor}e^{-\\mathrm{erf}\\big(\\tau\\big)\\mathtt{U}\\big|^{2}}\\mathrm{er}\\big(-\\frac{1}{2}\\tau^{\\kappa}\\big)\\tau\\big(\\big|u+2\\lambda^{2}\\Sigma^{\\top,1/2}x\\tau^{\\kappa}\\big|\\big)^{\\tau}\\Big\\rangle\\,\\mathrm{d}\\tau}{\\sqrt{\\pi}\\big(2\\pi)^{d/2}\\big|\\sum_{j=0}^{d}\\big|\\sum_{j=1}^{d/2}x^{\\tau}\\big|^{\\sum_{j=0}^{d}\\big|\\tau\\big>}}\\,\\mathrm{erf}\\big(u+2\\lambda^{2}\\Sigma^{\\top,1/2}x\\tau^{\\kappa}\\big)^{\\tau}\\Big\\rangle\\,\\mathrm{d}\\tau}\\\\ &{=-\\frac{2x^{\\tau}\\Gamma^{2}\\big|^{2}}{\\sqrt{\\pi}\\big|\\big(2\\pi\\big)^{d/2}\\big|^{2}}(\\big|\\lambda+2\\lambda^{2}\\Sigma^{\\top,1/2}x\\tau^{\\kappa}\\Sigma^{1/2}\\big)^{-1}}\\\\ &{\\times\\int_{\\mathbb{R}^{d}}\\mathrm{erf}\\big(v\\Sigma^{\\top\\lambda/2}y\\big)\\cdot\\big(\\mathrm{I}_{d}+2\\lambda^{2}\\Sigma^{\\top,1/2}x\\tau^{\\kappa}\\Sigma^{1/2}\\big)v\\cdot\\mathrm{exp}\\left(-\\frac{1}{2\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "using a change of variables $u\\,=\\,\\Sigma^{1/2}v$ in Equation $(\\star)$ and using partial integration and Gauss? divergence theorem in the last equation. In addition, we used that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla_{v}\\,e^{-\\frac{1}{2}v^{\\top}\\left(\\ensuremath{\\mathrm{I}}_{d}+2\\lambda^{2}\\Sigma^{1/2}x x^{\\top}\\Sigma^{1/2}\\right)v}=-\\big(\\ensuremath{\\mathrm{I}}_{d}+2\\lambda^{2}\\Sigma^{1/2}x x^{\\top}\\Sigma^{1/2}\\big)v\\,e^{-\\frac{1}{2}v^{\\top}\\left(\\ensuremath{\\mathrm{I}}_{d}+2\\lambda^{2}\\Sigma^{1/2}x x^{\\top}\\Sigma^{1/2}\\right)v}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and the partial integration rule for scalar functions. To be precise, for differentiable scalar functions $f$ and $g$ itholds ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\nabla(f\\cdot g)=f\\cdot\\nabla g+g\\cdot\\nabla f,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which then implies the partial integration rule. Furthermore, the left-hand side vanishes in our case due to Gauss\u2019 divergence theorem: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\biggl|\\int_{\\mathbb{R}^{d}}\\nabla_{v}\\left(\\mathrm{erf}\\bigl(v^{\\mathsf{T}}\\Sigma^{1/2}y\\bigr)\\cdot e^{-\\frac{1}{2}v^{\\mathsf{T}}\\bigl(\\mathbb{I}_{d}+2\\lambda^{2}\\Sigma^{1/2}x x^{\\mathsf{T}}\\Sigma^{1/2}\\bigr)v}\\right)\\,\\mathrm{d}v\\biggr|}\\,}&{}\\\\ &{=\\Biggl|\\operatorname*{lim}_{R\\to\\infty}\\int_{S^{d-1}(R)}\\mathrm{erf}\\bigl(v^{\\mathsf{T}}\\Sigma^{1/2}y\\bigr)\\cdot e^{-\\frac{1}{2}v^{\\mathsf{T}}\\bigl(\\mathbb{I}_{d}+2\\lambda^{2}\\Sigma^{1/2}x x^{\\mathsf{T}}\\Sigma^{1/2}\\bigr)v}\\,\\mathrm{d}v\\Biggr|}\\\\ &{\\le\\operatorname*{lim}_{R\\to\\infty}\\int_{S^{d-1}(R)}e^{-\\frac{1}{2}v^{\\mathsf{T}}\\bigl(\\mathbb{I}_{d}+2\\lambda^{2}\\Sigma^{1/2}x x^{\\mathsf{T}}\\Sigma^{1/2}\\bigr)v}\\,\\mathrm{d}v}\\\\ &{\\le\\operatorname*{lim}_{R\\to\\infty}S_{d-1}(R)\\cdot e^{-\\frac{1}{2}R^{2}\\lambda_{\\operatorname*{min}}(\\mathbb{I}_{d}+2\\lambda^{2}\\Sigma^{1/2}x x^{\\mathsf{T}}\\Sigma^{1/2})}=0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $S_{d-1}(R)$ is the surface area of the sphere in $\\mathbb{R}^{d}$ with radius $R$ . Continuing with our previous calculations, we see that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{(S24)=\\cfrac{2x^{7}\\Sigma^{1/2}}{\\sqrt{\\pi}(2\\pi)^{d/2}}(\\ensuremath{\\mathrm{I}_{d}}+2\\lambda^{2}\\Sigma^{1/2}x x^{\\boldsymbol{\\mathsf{T}}}\\Sigma^{1/2})^{-1}}\\\\ {\\times\\cint_{\\ensuremath{\\mathbb{R}}^{d}}e^{-\\frac{1}{2}v^{\\mathsf{r}}\\left(\\ensuremath{\\mathrm{I}_{d}}+2\\lambda^{2}\\Sigma^{1/2}x x^{\\boldsymbol{\\mathsf{T}}}\\Sigma^{1/2}\\right)v}\\cdot\\Sigma^{1/2}y\\cfrac{2}{\\sqrt{\\pi}}e^{-(v^{\\mathsf{T}}\\Sigma^{1/2}y)^{2}}\\,\\ensuremath{\\mathrm{d}}v}\\\\ {=\\cfrac{4}{\\pi}\\cfrac{x^{\\mathsf{T}}\\Sigma^{1/2}(\\ensuremath{\\mathrm{I}_{d}}+2\\lambda^{2}\\Sigma^{1/2}x x^{\\boldsymbol{\\mathsf{T}}}\\Sigma^{1/2})^{-1}\\Sigma^{1/2}y}{(2\\pi)^{d/2}}\\int_{\\ensuremath{\\mathbb{R}}^{d}}e^{-\\frac{1}{2}v^{\\mathsf{r}}\\left(\\ensuremath{\\mathrm{I}_{d}}+2\\lambda^{2}\\Sigma^{1/2}x x^{\\boldsymbol{\\mathsf{T}}}\\Sigma^{1/2}+2\\Sigma^{1/2}y y^{\\mathsf{T}}\\Sigma^{1/2}\\right)v}\\,\\ensuremath{\\mathrm{d}}v.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We evaluate the expression outside of the integral in the last line by applying the Sherman-MorrisonWoodbury formula. For $A\\in\\mathbb{R}^{d\\times d}$ and $w_{1},\\bar{w_{2}}\\in\\mathbb{R}^{d}$ it holds [Golub and Van Loan, 1996, (2.4.1)] ", "page_idx": 25}, {"type": "equation", "text": "$$\n(C+w_{1}w_{2}^{\\mathsf{T}})^{-1}=C^{-1}-{\\frac{C^{-1}w_{1}w_{2}^{\\mathsf{T}}C^{-1}}{1+w_{2}^{\\mathsf{T}}C^{-1}w_{1}}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "For $C=\\mathrm{I}_{d}$ and $w=w_{1}=w_{2}=\\sqrt{2}\\lambda\\Sigma^{1/2}x$ this yields ", "page_idx": 26}, {"type": "equation", "text": "$$\n(\\ensuremath{\\mathrm{I}_{d}}+2\\lambda^{2}\\Sigma^{1/2}x x^{\\mathsf{T}}\\Sigma^{1/2})^{-1}=(\\ensuremath{\\mathrm{I}_{d}}+w w^{\\mathsf{T}})^{-1}=\\ensuremath{\\mathrm{I}_{d}}-\\frac{w w^{\\mathsf{T}}}{1+w^{\\mathsf{T}}w}=\\ensuremath{\\mathrm{I}_{d}}-\\frac{2\\lambda^{2}\\Sigma^{1/2}x x^{\\mathsf{T}}\\Sigma^{1/2}}{1+2\\lambda^{2}x^{\\mathsf{T}}\\Sigma x}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "With this we see that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~~x^{\\mathsf{T}}\\Sigma^{1/2}(\\ensuremath{\\mathrm{I}_{d}}+2\\lambda^{2}\\Sigma^{1/2}x x^{\\mathsf{T}}\\Sigma^{1/2})^{-1}\\Sigma^{1/2}y=x^{\\mathsf{T}}\\Sigma^{1/2}\\left(\\ensuremath{\\mathrm{I}_{d}}-\\frac{2\\lambda^{2}\\Sigma^{1/2}x x^{\\mathsf{T}}\\Sigma^{1/2}}{1+2\\lambda^{2}x^{\\mathsf{T}}\\Sigma x}\\right)\\Sigma^{1/2}y}\\\\ &{=\\!x^{\\mathsf{T}}\\Sigma y-\\frac{2\\lambda^{2}(x^{\\mathsf{T}}\\Sigma x)(x^{\\mathsf{T}}\\Sigma y)}{1+2\\lambda^{2}x^{\\mathsf{T}}\\Sigma x}=x^{\\mathsf{T}}\\Sigma y\\left(1-\\frac{2\\lambda^{2}x^{\\mathsf{T}}\\Sigma x}{1+2\\lambda^{2}x^{\\mathsf{T}}\\Sigma x}\\right)=\\frac{x^{\\mathsf{T}}\\Sigma y}{1+2\\lambda^{2}x^{\\mathsf{T}}\\Sigma x}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Inserting this into Equation (S25), we obtain ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle(S25)=\\frac{2}{\\pi}\\frac{2x^{\\mathsf{T}}\\Sigma y}{1+2\\lambda^{2}x^{\\mathsf{T}}\\Sigma x}\\int_{\\mathbb{R}^{d}}\\frac{1}{(2\\pi)^{d/2}}e^{-\\frac{1}{2}v^{\\mathsf{T}}\\left(\\mathbb{I}_{d}+2\\lambda^{2}\\Sigma^{1/2}x x^{\\mathsf{T}}\\Sigma^{1/2}+2\\Sigma^{1/2}y y^{\\mathsf{T}}\\Sigma^{1/2}\\right)v}\\,\\mathrm{d}v}\\\\ {\\displaystyle~~~~=\\frac{2}{\\pi}\\frac{2x^{\\mathsf{T}}\\Sigma y}{1+2\\lambda^{2}x^{\\mathsf{T}}\\Sigma x}\\left|\\left(\\mathbb{I}_{d}+2\\lambda^{2}\\Sigma^{1/2}x x^{\\mathsf{T}}\\Sigma^{1/2}+2\\Sigma^{1/2}y y^{\\mathsf{T}}\\Sigma^{1/2}\\right)^{-1}\\right|^{1/2}}\\\\ {\\displaystyle~~~~=\\frac{2}{\\pi}\\frac{2x^{\\mathsf{T}}\\Sigma y}{1+2\\lambda^{2}x^{\\mathsf{T}}\\Sigma x}\\left|\\mathbb{I}_{d}+2\\lambda^{2}\\Sigma^{1/2}x x^{\\mathsf{T}}\\Sigma^{1/2}+2\\Sigma^{1/2}y y^{\\mathsf{T}}\\Sigma^{1/2}\\right|^{-1/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "As in the proof of Lemma D.1, we evaluate this using Sylvester's determinant theorem. With the same notation as before, we can define $A=B=(\\sqrt{2}\\lambda\\Sigma^{\\mathrm{i}/2}x,\\sqrt{2}\\Sigma^{1/2}y)\\in\\mathbb{R}^{d\\times2},$ This yields ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left|\\mathrm{I}_{d}+2\\lambda^{2}\\Sigma^{1/2}x x^{\\intercal}\\Sigma^{1/2}+2\\Sigma^{1/2}y y^{\\intercal}\\Sigma^{1/2}\\right|=|\\mathrm{I}_{d}+A B^{\\intercal}|=|\\mathrm{I}_{2}+B^{\\intercal}A|}\\\\ &{=\\left|\\left(^{1}+2\\lambda^{2}x^{\\intercal}\\Sigma x\\quad\\phantom{^{2}}2\\lambda x^{\\intercal}\\Sigma y\\right)\\right|=(1+2\\lambda^{2}x^{\\intercal}\\Sigma x)(1+2y^{\\intercal}\\Sigma y)-4\\lambda^{2}(x^{\\intercal}\\Sigma y)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "If we insert this this again, we have so far shown ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}\\lambda}V(\\lambda)=\\frac{2}{\\pi}\\frac{2x^{\\mathsf{T}}\\Sigma y}{1+2\\lambda^{2}x^{\\mathsf{T}}\\Sigma x}\\left((1+2\\lambda^{2}x^{\\mathsf{T}}\\Sigma x)(1+2y^{\\mathsf{T}}\\Sigma y)-4\\lambda^{2}(x^{\\mathsf{T}}\\Sigma y)^{2}\\right)^{-1/2}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We now define ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\gamma(\\lambda):=\\frac{2\\lambda x^{\\mathsf{T}}\\Sigma y}{\\sqrt{(1+2\\lambda^{2}x^{\\mathsf{T}}\\Sigma x)(1+2y^{\\mathsf{T}}\\Sigma y)}},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and claim that ", "page_idx": 26}, {"type": "equation", "text": "$$\n{\\frac{2}{\\pi}}\\left(1-\\gamma(\\lambda)^{2}\\right)^{-1/2}{\\frac{\\mathrm{d}}{\\mathrm{d}\\lambda}}\\gamma(\\lambda)={\\frac{\\mathrm{d}}{\\mathrm{d}\\lambda}}V(\\lambda).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We can find a solution to the claimed equation by finding a function $\\tilde{V}$ that satisfies ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}\\gamma(\\lambda)}\\tilde{V}(\\gamma(\\lambda))=\\frac{2}{\\pi}\\left(1-\\gamma(\\lambda)^{2}\\right)^{-1/2},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and by setting $V(\\lambda)\\,:=\\,\\tilde{V}(\\gamma(\\lambda))$ . This follows from the chain rule. $\\tilde{V}$ is thus simply given by $\\begin{array}{r}{\\tilde{V}(\\gamma(\\lambda))=\\frac{2}{\\pi}}\\end{array}$ arcsin $\\left(\\gamma(\\lambda)\\right)$ In particular, this yields ", "page_idx": 26}, {"type": "equation", "text": "$$\nV=V(1)=\\tilde{V}(\\gamma(1))=\\frac{2}{\\pi}\\arcsin\\left(\\frac{2x^{\\mathsf{T}}\\Sigma y}{\\sqrt{(1+2x^{\\mathsf{T}}\\Sigma x)(1+2y^{\\mathsf{T}}\\Sigma y)}}\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "It is now left to show Equation (S27) using Equation (S26). First, see that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\left(1-\\gamma(\\lambda)^{2}\\right)^{-1/2}=\\left(1-\\frac{(2\\lambda x^{\\intercal}\\Sigma y)^{2}}{\\left(1+2\\lambda x^{\\intercal}\\Sigma x\\right)\\left(1+2y^{\\intercal}\\Sigma y\\right)}\\right)^{-1/2}}}\\\\ &{}&{=\\left(\\frac{(1+2\\lambda x^{\\intercal}\\Sigma x)\\left(1+2y^{\\intercal}\\Sigma y\\right)}{\\left(1+2\\lambda x^{\\intercal}\\Sigma x\\right)\\left(1+2y^{\\intercal}\\Sigma y\\right)-4\\lambda^{2}\\left(x^{\\intercal}\\Sigma y\\right)^{2}}\\right)^{1/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Second, it holds ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\displaystyle\\mathrm{d}}{\\displaystyle\\mathrm{d}\\lambda}\\left[\\lambda(1+2\\lambda^{2}x^{\\mathsf{T}}\\Sigma x)^{-1/2}\\right]=\\frac{\\displaystyle\\mathrm{d}}{\\displaystyle\\mathrm{d}\\lambda}\\left[(\\lambda^{-2}+2x^{\\mathsf{T}}\\Sigma x)^{-1/2}\\right]=\\lambda^{-3}(\\lambda^{-2}+2x\\top\\Sigma x)^{-3/2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=(1+2\\lambda^{2}x^{\\mathsf{T}}\\Sigma x)^{-3/2}=\\frac{1}{\\displaystyle\\big(\\sqrt{1+2\\lambda^{2}x^{\\mathsf{T}}\\Sigma x}\\big)^{3}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "With the results of both calculations, we get ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{2}{\\pi}\\left(1-\\gamma(\\lambda)^{2}\\right)^{-1/2}\\frac{\\mathrm{d}}{\\mathrm{d}\\lambda}\\gamma(\\lambda)=\\frac{2}{\\pi}\\left(1-\\gamma(\\lambda)^{2}\\right)^{-1/2}\\frac{2x^{\\mathsf{T}}\\Sigma y}{\\sqrt{1+2y^{\\mathsf{T}}\\Sigma y}}\\frac{\\mathrm{d}}{\\mathrm{d}\\lambda}\\left[\\lambda(1+2\\lambda^{2}x^{\\mathsf{T}}\\Sigma x)^{-1/2}\\right]}\\\\ &{=\\frac{2}{\\pi}\\frac{\\sqrt{1+2\\lambda x^{\\mathsf{T}}\\Sigma x}\\sqrt{1+2y^{\\mathsf{T}}\\Sigma y}}{\\sqrt{(1+2\\lambda x^{\\mathsf{T}}\\Sigma x)(1+2y^{\\mathsf{T}}\\Sigma y)-4\\lambda^{2}(x^{\\mathsf{T}}\\Sigma y)^{2}}}\\frac{2x^{\\mathsf{T}}\\Sigma y}{\\sqrt{1+2y^{\\mathsf{T}}\\Sigma y}}\\frac{1}{\\left(\\sqrt{1+2\\lambda^{2}x^{\\mathsf{T}}\\Sigma x}\\right)^{3}}}\\\\ &{=\\frac{2}{\\pi}\\frac{1}{\\sqrt{(1+2\\lambda x^{\\mathsf{T}}\\Sigma x)(1+2y^{\\mathsf{T}}\\Sigma y)-4\\lambda^{2}(x^{\\mathsf{T}}\\Sigma y)^{2}}}\\frac{2x^{\\mathsf{T}}\\Sigma y}{1+2\\lambda^{2}x^{\\mathsf{T}}\\Sigma x}\\ \\overset{(s26)}{=}\\frac{\\mathrm{d}}{\\mathrm{d}\\lambda}V(\\lambda),}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which yields Equation (S27) and concludes the proof. The special case $\\Sigma\\in\\mathbb{R}^{2\\times2}$ with invertible covariance matrix or $X\\,=\\,Y$ and singular covariance matrix follows as in the proof of Lemma D.1. \u53e3 ", "page_idx": 27}, {"type": "text", "text": "Corollary D.2. If $(X,Y)\\sim{\\mathcal{N}}(0,\\Sigma)$ withinvertible covariance matrix $\\Sigma=\\left({\\scriptstyle\\sum_{1}\\Sigma_{3}}\\right)\\,\\in\\mathbb{R}^{2\\times2}\\,o r$ with $X=Y$ and singular covariance matrix $\\Sigma=\\left(\\O_{\\Sigma_{3}}^{\\Sigma_{1}}\\Sigma_{2}^{_{3}}\\right)\\in\\mathbb{R}^{2\\times2}$ \uff0c $\\Sigma_{1}=\\Sigma_{2}=\\Sigma_{3}$ it holds ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathcal{T}_{m}(\\Sigma)=\\frac{2}{\\pi}\\arcsin\\left(\\frac{\\Sigma_{3}}{\\sqrt{\\frac{1}{2m^{2}}+\\Sigma_{1}}\\sqrt{\\frac{1}{2m^{2}}+\\Sigma_{2}}}\\right)\\xrightarrow{m\\rightarrow\\infty}\\frac{2}{\\pi}\\arcsin\\left(\\frac{\\Sigma_{3}}{\\sqrt{\\Sigma_{1}}\\sqrt{\\Sigma_{2}}}\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. It holds ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{T}_{m}(\\Sigma)=\\mathcal{T}(m^{2}\\cdot\\Sigma)\\overset{\\mathrm{Lemma~D.2}}{=}\\frac{2}{\\pi}\\arcsin\\left(\\frac{2m^{2}\\Sigma_{3}}{\\sqrt{1+2m^{2}\\Sigma_{1}}\\sqrt{1+2m^{2}\\Sigma_{2}}}\\right)}\\\\ &{\\qquad=\\frac{2}{\\pi}\\arcsin\\left(\\frac{\\Sigma_{3}}{\\sqrt{\\frac{1}{2m^{2}}+\\Sigma_{1}}\\sqrt{\\frac{1}{2m^{2}}+\\Sigma_{2}}}\\right)\\overset{m\\rightarrow\\infty}{\\longrightarrow}\\frac{2}{\\pi}\\arcsin\\left(\\frac{\\Sigma_{3}}{\\sqrt{\\Sigma_{1}}\\sqrt{\\Sigma_{2}}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We can now use Corollary D.1 and Corollary D.2 to evaluate $\\Sigma^{(L)},\\,\\dot{\\Sigma}^{(L)}$ and $\\Theta^{(L)}$ for activation function $\\operatorname{erf}_{m}$ ,which we denote by $\\Sigma_{m}^{(L)}$ \uff0c $\\dot{\\Sigma}_{m}^{(L)}$ , and $\\Theta_{m}^{(L)}$ respectively. We then are interested in the limit $m\\rightarrow\\infty$ . First recall that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\Sigma^{(1)}(x,x^{\\prime})=\\frac{\\sigma_{w}^{2}}{n_{0}}\\langle x,x^{\\prime}\\rangle+\\sigma_{b}^{2}\\quad\\mathrm{and}}}\\\\ {{\\displaystyle\\Sigma^{(L)}(x,x^{\\prime})=\\sigma_{w}^{2}\\,\\mathbb{E}_{g\\sim\\mathcal{N}(0,\\Sigma^{(L-1)})}[\\sigma(g(x))\\,\\sigma(g(y))]+\\sigma_{b}^{2},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "by Theorem C.2. Therefore, $\\Sigma_{m}^{(1)}$ is independent of $m$ , and it holds for any $\\boldsymbol{x},\\boldsymbol{y}\\in\\mathbb{R}^{n_{0}}$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\Sigma_{m}^{(1)}(x,y)=\\frac{\\sigma_{w}^{2}}{n_{0}}\\langle x,y\\rangle+\\sigma_{b}^{2}=:\\Sigma_{\\infty}^{(1)}(x,y).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Assuming that the limit $\\begin{array}{r}{\\operatorname*{lim}_{m\\rightarrow\\infty}\\Sigma_{m}^{(L)}(x,y)\\ =:\\ \\Sigma_{\\infty}^{(L)}(x,y)}\\end{array}$ exists and that $\\Sigma_{\\infty}^{(L)}(x,x)\\ \\neq\\ 0$ $\\Sigma_{\\infty}^{(L)}(y,y)\\neq0$ we can define ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Sigma_{m}^{(L+1)}(x,y)=\\sigma_{w}^{2}\\mathbb{E}_{(X,Y)\\sim N(0,\\Sigma_{m;x,y}^{(L)})}[\\mathrm{erf}_{m}(X)\\,\\mathrm{erf}_{m}(Y)]+\\sigma_{b}^{2}=\\sigma_{w}^{2}\\,\\mathcal{T}_{m}\\left(\\Sigma_{m;x,y}^{(L)}\\right)+\\sigma_{b}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\mathrm{co}_{\\equiv}\\frac{\\mathrm{D}.2}{\\pi}\\,\\underline{{2\\sigma_{w}^{2}}}\\arcsin\\left(\\frac{\\Sigma_{m}^{(L)}}{\\sqrt{\\frac{1}{2m^{2}}+\\Sigma_{m}^{(L)}(x,x)}\\sqrt{\\frac{1}{2m^{2}}+\\Sigma_{m}^{(L)}(y,y)}}\\right)+\\sigma_{b}^{2}}\\\\ &{\\qquad\\qquad\\xrightarrow{\\quad m\\rightarrow\\infty_{\\cdot}}\\frac{2\\sigma_{w}^{2}}{\\pi}\\arcsin\\left(\\frac{\\Sigma_{\\infty}^{(L)}(x,y)}{\\sqrt{\\Sigma_{\\infty}^{(L)}(x,x)}\\sqrt{\\Sigma_{\\infty}^{(L)}(y,y)}}\\right)+\\sigma_{b}^{2}}\\\\ &{\\qquad\\qquad\\quad=:\\Sigma_{\\infty}^{(L+1)}(x,y).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Hence, it follows via induction and from the continuity of the arcsin function that $\\begin{array}{r}{\\operatorname*{lim}_{m\\rightarrow\\infty}\\Sigma_{m}^{(L)}(x,y)\\;=:\\Sigma_{\\infty}^{(L)}(x,y)}\\end{array}$ iswlli $\\Sigma_{\\infty}^{(L)}$ in Remark D3 For $\\dot{\\Sigma}_{m}^{(L+1)}$ $L\\geq1$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\dot{\\Sigma}_{m}^{(L+1)}(x,y)=\\sigma_{w}^{2}\\,\\mathbb{E}_{(X,Y)\\sim\\mathcal{N}(0,\\Sigma_{m;x,y}^{(L)})}[\\mathrm{erf}_{m}(X)\\,\\mathrm{erf}_{m}(Y)]=\\sigma_{w}^{2}\\,\\dot{\\mathcal{T}}_{m}\\left(\\Sigma_{m;x,y}^{(L)}\\right)}}\\\\ &{}&{\\stackrel{\\mathrm{Cor.\\,D.1}}{=}\\frac{2\\sigma_{w}^{2}}{\\pi}\\left|\\Sigma_{m;x,y}^{(L)}+\\frac{1}{2m^{2}}\\mathrm{I}_{2}\\right|^{-1/2}}\\\\ &{}&{=\\frac{2\\sigma_{w}^{2}}{\\pi}\\left(\\left(\\Sigma_{m}^{(L)}(x,x)+\\frac{1}{2m^{2}}\\right)\\left(\\Sigma_{m}^{(L)}(y,y)+\\frac{1}{2m^{2}}\\right)-\\Sigma_{m}^{(L)}(x,y)^{2}\\right)^{-1/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "As we will see later, the limit ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{m\\rightarrow\\infty}\\dot{\\Sigma}_{m}^{(L)}(x,y)=:\\dot{\\Sigma}_{\\infty}^{(L)}(x,y)\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "exists for $x\\neq y$ apart from a few exceptions. In the case $x=y$ , we obtain ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{{\\displaystyle\\dot{\\Sigma}_{m}^{(L+1)}(x,x)=\\frac{2\\sigma_{w}^{2}}{\\pi}\\left(\\left(\\Sigma_{m}^{(L)}(x,x)+\\frac{1}{2m^{2}}\\right)^{2}-\\Sigma_{m}^{(L)}(x,x)^{2}\\right)^{-1/2}}}}\\\\ {{{\\displaystyle=\\frac{2\\sigma_{w}^{2}}{\\pi}\\left(\\frac{1}{m^{2}}\\Sigma_{m}^{(L)}(x,x)+\\frac{1}{4m^{4}}\\right)^{-1/2}=\\frac{2\\sigma_{w}^{2}}{\\pi}m\\left(\\Sigma_{m}^{(L)}(x,x)+\\frac{1}{4m^{2}}\\right)^{-1/2}}}}\\\\ {{{\\displaystyle~~~~~~~~~~~~~~~~~~~~\\sim\\frac{2\\sigma_{w}^{2}}{\\pi}m\\,\\Sigma_{\\infty}^{(L)}(x,x)^{-1/2}\\xrightarrow{m\\rightarrow\\infty}\\infty}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\sim$ denotes asymptotic equality. Therefore, for $x\\neq y$ , the limit ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{m\\rightarrow\\infty}\\Theta_{m}^{(L)}(x,y)=:\\Theta_{\\infty}^{(L)}(x,y)\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "exists. However, due to Equation (S32), the NTK diverges for $x=y$ as $m\\rightarrow\\infty$ .We will call a kernel with this property a singular kernel. We also say that a kernel with this property is singular alongthediagonal. ", "page_idx": 28}, {"type": "text", "text": "Remark D.2 (Distributional neural tangent kernel). An alternative conceivable approach to obtain the samekernel $\\Theta_{\\infty}^{(L)}$ would have been to consider the distributional Jacobian matrix of the network function with step-like activation function. Whether or not a distributional NTK can be formulated is a question for further research. Here, we only want to point out that the corresponding formulas for the recursive definition of the analytical distributional NTK would then naturally read as follows, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Theta_{\\infty}^{(1)}(x,x^{\\prime})=\\Sigma_{\\infty}^{(1)}(x,x^{\\prime})}\\\\ &{\\Theta_{\\infty}^{(L)}(x,x^{\\prime})=\\Sigma_{\\infty}^{(L)}(x,x^{\\prime})+\\Theta_{\\infty}^{(L-1)}(x,x^{\\prime})\\cdot\\dot{\\Sigma}_{\\infty}^{(L)}(x,x^{\\prime})\\quad f o r\\;L\\geq2,}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\Sigma_{\\infty}^{(L)}(x,x^{\\prime})=\\sigma_{w}^{2}\\operatorname{\\mathbb{E}}_{g\\sim{\\mathcal{N}}(0,\\Sigma_{\\infty}^{(L-1)})}\\left[{\\mathrm{sign}}(g(x))\\,{\\mathrm{sign}}(g(y))\\right]+\\sigma_{b}^{2}\\;\\;\\;f o r\\,L\\geq2,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\dot{\\Sigma}_{\\infty}^{(L)}(x,x^{\\prime})=\\sigma_{w}^{2}\\operatorname{\\mathbb{E}}_{g\\sim{\\mathcal{N}}(0,\\Sigma_{\\infty}^{(L-1)})}\\left[2\\delta_{0}(g(x))\\,2\\delta_{0}(g(y))\\right]\\;\\;\\;f o r\\;L\\geq2.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Note that Equation (S34) can be derived from Remark D.1, and that Equation (S33) is also easy to show. ", "page_idx": 28}, {"type": "text", "text": "We now want to explore the implications of our findings in this section for $f_{\\mathrm{NTK}}$ , which we introduced at the end of Section C.2. Equation (S20) yields ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{m\\to\\infty}f_{\\mathrm{NTK}}^{m}(x)=\\operatorname*{lim}_{m\\to\\infty}\\Theta_{m}(x,\\mathcal{X})\\Theta_{m}(\\mathcal{X},\\mathcal{X})^{-1}\\mathcal{Y}=:f_{\\mathrm{NTK}}^{\\infty}(x).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Note that $\\Theta_{m}(\\mathcal{X},\\mathcal{X}):=\\Theta_{m}^{(L)}(\\mathcal{X},\\mathcal{X})$ is invertible for suffciently large $m$ , as the matrix will be dominated by the diagonal for large $m$ . Using this and the fact $\\Theta_{m}(x,\\mathcal{X})\\xrightarrow{m\\rightarrow\\infty}\\Theta_{\\infty}(x,\\mathcal{X})$ if $x\\neq x_{i}$ for all $i=1,\\ldots,d$ , we obtain for such $x$ ", "page_idx": 29}, {"type": "equation", "text": "$$\nf_{\\mathrm{NTK}}^{m}(x)=\\Theta_{m}(x,\\mathcal{X})\\Theta_{m}(\\mathcal{X},\\mathcal{X})^{-1}\\mathcal{Y}\\xrightarrow{m\\rightarrow\\infty}0\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "However, if $x=x_{i}\\in\\mathcal{X}$ and denoting the $i$ -th basic vector by $e_{i}\\in\\mathbb{R}^{d}$ we get: ", "page_idx": 29}, {"type": "equation", "text": "$$\nf_{\\mathrm{NTK}}^{m}(x_{i})=\\Theta_{m}(x_{i},\\boldsymbol{\\chi})\\Theta_{m}(\\boldsymbol{\\chi},\\boldsymbol{\\chi})^{-1}\\boldsymbol{\\mathcal{Y}}=e_{i}^{\\intercal}\\boldsymbol{\\mathcal{Y}}=y_{i}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Therefore, $f_{\\mathrm{NTK}}^{m}$ converges pointwise to a function that is zero almost everywhere, but interpolates exactly at the data points. The singular kernel $\\Theta_{\\infty}^{(L)}$ , which we can see as the analytic NTK for the sign function, is for that reason not suitable for regression. We will deal with this problem in the followingsection. ", "page_idx": 29}, {"type": "text", "text": "D.2  From singular kernel to Nadaraya-Watson estimator ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Radhakrishnan et al. [2023] considered the limit of infinite depth, $\\operatorname{lim}_{L\\to\\infty}\\Theta^{(L)}$ , and a singular kernel emerged similar to the previous section. It was shown that the resulting estimator using this singular kernel behaves like a Nadaraya-Watson estimator when classification tasks are considered instead of regression tasks. We will adapt the ideas of Radhakrishnan et al. [2023] to show similar results. To consider a classification task, we assume that $\\mathcal{D}\\in\\{-1,1\\}^{d}$ . The network is then trained with data points $(\\mathcal{X},\\mathcal{Y})$ as before, but we apply the sign function at the end to obtain the final classifier. As Radhakrishnan et al. [2023], we assume that we are operating in the infinite-width limit after infinite training. So, we are interested in ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{m\\rightarrow\\infty}\\operatorname{sign}\\left(\\Theta_{m}(x,\\mathcal{X})\\Theta_{m}(\\mathcal{X},\\mathcal{X})^{-1}\\mathcal{Y}\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We will see that the resulting classifier is equal to ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname{sign}\\left(\\Theta_{\\infty}(x,\\lambda)\\mathcal{Y}\\right)=\\operatorname{sign}\\left(\\sum_{i=1}^{d}\\Theta(x,x_{i})y_{i}\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "The form of this classifier is similar to a Nadaraya-Watson estimator. To be precise, for a singular kernel $k$ and data points $(\\mathcal{X},\\mathcal{Y})$ , the Nadaraya-Watson estimator is defined as ", "page_idx": 29}, {"type": "equation", "text": "$$\nc(x):=\\frac{\\sum_{i=1}^{d}k(x,x_{i})y_{i}}{\\sum_{i=1}^{d}k(x,x_{i})}\\quad{\\mathrm{for~all~}}x\\neq x_{i},1\\leq i\\leq d.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Since $k(x,x_{i})\\to\\infty$ as $x\\to x_{i}$ , it holds that $c(x)\\to y_{i}$ as $x\\to x_{i}$ . Thus, the continuous extension of $c$ to $\\mathbb{R}^{n_{0}}$ interpolates the data points. ", "page_idx": 29}, {"type": "text", "text": "We now bgin to further evalut $\\Sigma_{\\infty}^{(L)}$ and $\\dot{\\Sigma}_{\\infty}^{(L)}$ to analyze Equation (S35): ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Sigma_{\\infty}^{(1)}(x,y)=\\frac{\\sigma_{w}^{2}}{n_{0}}(x,y)+\\sigma_{b}^{2},}\\\\ &{\\Sigma_{\\infty}^{(L)}(x,x)\\overset{{(\\mathrm{SZ})}}{=}\\frac{2\\sigma_{w}^{2}}{\\pi}\\arcsin(1)+\\sigma_{b}^{2}=\\sigma_{w}^{2}+\\sigma_{b}^{2}\\quad\\mathrm{for}\\,{\\mathrm{all}}\\quad L\\ge2,}\\\\ &{\\Sigma_{\\infty}^{(2)}(x,y)\\overset{(\\mathrm{SZ})}{=}\\frac{2\\sigma_{w}^{2}}{\\pi}\\arcsin\\left(\\frac{\\frac{\\sigma_{w}^{2}}{\\sqrt{\\frac{\\sigma_{w}^{2}}{\\sigma_{b}^{2}}}[x]^{2}+\\sigma_{b}^{2}}+\\sigma_{b}^{2}}{\\sqrt{\\frac{\\sigma_{w}^{2}}{\\sigma_{b}^{2}}[x]^{2}+\\sigma_{b}^{2}}\\sqrt{\\frac{\\sigma_{w}^{2}}{n_{0}}[y][y]^{2}+\\sigma_{b}^{2}}}\\right)+\\sigma_{b}^{2},\\quad\\mathrm{and}}\\\\ &{\\Sigma_{\\infty}^{(L+1)}(x,y)\\overset{(\\mathrm{SZ})}{=}\\frac{2\\sigma_{w}^{2}}{\\pi}\\arcsin\\left(\\frac{\\sum_{c}(x,y)}{\\sqrt{\\sum_{c}(x,x)}\\sqrt{\\sum_{c}(x,y)}}\\right)+\\sigma_{b}^{2}}\\\\ &{\\overset{(\\mathrm{SZ})^{\\prime}}{=}\\frac{2\\sigma_{w}^{2}}{\\pi}\\arcsin\\left(\\frac{\\sum_{c}(x,y)}{\\sigma_{w}^{2}+\\sigma_{b}^{2}}\\right)+\\sigma_{b}^{2}\\quad\\mathrm{for}\\,{\\mathrm{all}}\\quad L\\ge2.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Regarding the assumptions made for Equation (S29), note that that $\\Sigma_{\\infty}^{(L)}(x,x)\\neq0$ $L\\geq2$ and $\\Sigma_{\\infty}^{(1)}(x,x)\\neq0$ $x\\neq0$ $\\sigma_{b}^{2}>0$ ", "page_idx": 30}, {"type": "text", "text": "After Equation (S31), we mentioned exceptions to the existence of $\\dot{\\Sigma}_{\\infty}^{(L)}(x,y)$ for $x\\neq y$ that were postponed t thatime For $\\dot{\\Sigma}_{m}^{(2)}$ With $x\\neq y$ Wwecansethat ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\Sigma}_{m}^{(2)}(x,y)}\\\\ &{\\overset{(S30)}{=}\\frac{2\\sigma_{w}^{2}}{\\pi}\\left(\\left(\\frac{\\sigma_{w}^{2}}{n_{0}}\\|x\\|^{2}+\\sigma_{b}^{2}+\\frac{1}{2m^{2}}\\right)\\left(\\frac{\\sigma_{w}^{2}}{n_{0}}\\|y\\|^{2}+\\sigma_{b}^{2}+\\frac{1}{2m^{2}}\\right)-\\left(\\frac{\\sigma_{w}^{2}}{n_{0}}\\langle x,y\\rangle+\\sigma_{b}^{2}\\right)^{2}\\right)^{-1/2}}\\\\ &{\\ \\ =\\frac{2\\sigma_{w}^{4}}{\\pi}\\left(\\frac{\\sigma_{w}^{4}}{n_{0}^{2}}\\left(\\|x\\|^{2}\\|y\\|^{2}-\\langle x,y\\rangle^{2}\\right)+\\frac{\\sigma_{w}^{2}\\sigma_{b}^{2}}{n_{0}}\\left(\\|x\\|^{2}+\\|y\\|^{2}-2\\langle x,y\\rangle\\right)+\\frac{1}{2m^{2}}A_{m}\\right)^{-1/2}}\\\\ &{\\overset{m\\rightarrow\\infty}{=}\\frac{2\\sigma_{w}^{2}}{\\pi}\\left(\\frac{\\sigma_{w}^{4}}{n_{0}^{2}}\\left(\\|x\\|^{2}\\|y\\|^{2}-\\langle x,y\\rangle^{2}\\right)+\\frac{\\sigma_{w}^{2}\\sigma_{b}^{2}}{n_{0}}\\|x-y\\|^{2}\\right)^{-1/2}=:\\dot{\\Sigma}_{\\infty}^{(2)}(x,y),\\qquad\\quad\\mathrm{(S44(}\\sigma_{w}^{2}+n_{0}\\sigma_{b}^{2}+\\sigma_{b}^{2}+\\sigma_{b}^{2}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "assuming that either $\\sigma_{b}^{2}>0$ or that $x,y$ are not parallel, i.e. $|\\langle x,y\\rangle|\\neq\\|x\\|\\|y\\|$ The term $\\frac{1}{2m^{2}}A_{m}$ is given by ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{1}{2m^{2}}A_{m}=\\frac{1}{2m^{2}}\\left(\\left(\\frac{\\sigma_{w}^{2}}{n_{0}}\\|x\\|^{2}+\\sigma_{b}^{2}\\right)+\\left(\\frac{\\sigma_{w}^{2}}{n_{0}}\\|y\\|^{2}+\\sigma_{b}^{2}\\right)+\\frac{1}{2m^{2}}\\right)\\xrightarrow{m\\rightarrow\\infty}0.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "For $L\\geq3$ the expression simplifies: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\dot{\\Sigma}_{m}^{(L)}(x,y)}\\\\ &{\\overset{(S30)}{=}\\frac{2\\sigma_{w}^{2}}{\\pi}\\left(\\left(\\Sigma_{m}^{(L-1)}(x,x)+\\frac{1}{2m^{2}}\\right)\\left(\\Sigma_{m}^{(L-1)}(y,y)+\\frac{1}{2m^{2}}\\right)-\\Sigma_{m}^{(L-1)}(x,y)^{2}\\right)^{-1/2}}\\\\ &{\\ =\\frac{2\\sigma_{w}^{2}}{\\pi}\\left(\\left(\\Sigma_{m}^{(L-1)}(x,x)\\cdot\\Sigma_{m}^{(L-1)}(y,y)-\\Sigma_{m}^{(L-1)}(x,y)^{2}\\right)+\\frac{1}{2m^{2}}B_{m}\\right)^{-1/2}}\\\\ &{\\xrightarrow{m\\to\\infty}\\frac{2\\sigma_{w}^{2}}{\\pi}\\left(\\Sigma_{\\infty}^{(L-1)}(x,x)\\cdot\\Sigma_{\\infty}^{(L-1)}(y,y)-\\Sigma_{\\infty}^{(L-1)}(x,y)^{2}\\right)}\\\\ &{\\overset{(S37)}{=}\\frac{2\\sigma_{w}^{2}}{\\pi}\\left((\\sigma_{w}^{2}+\\sigma_{b}^{2})^{2}-\\Sigma_{\\infty}^{(L-1)}(x,y)^{2}\\right)^{-1/2}=:\\dot{\\Sigma}_{\\infty}^{(L)}(x,y),}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "wih thetermn $\\scriptstyle{{\\frac{1}{2m^{2}}}B_{m}}$ given by ", "page_idx": 30}, {"type": "equation", "text": "$$\n{\\frac{1}{2m^{2}}}B_{m}={\\frac{1}{2m^{2}}}\\left(\\Sigma_{m}^{(L-1)}(x,x)+\\Sigma_{m}^{(L-1)}(y,y)+{\\frac{1}{2m^{2}}}\\right)\\xrightarrow{m\\to\\infty}0.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "From Equation (S39) and Equation (S41) we see that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}\\left(\\Sigma_{\\infty}^{({L-1})}(x,y)\\right)}\\Sigma_{\\infty}^{({L})}(x,y)=\\dot{\\Sigma}_{\\infty}^{({L})}(x,y).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Although we did not use dual activation functions to denote the NTK as [Jacot et al., 2018, Section A.4], we still find that the property $\\widehat{\\left(\\hat{\\sigma}\\right)^{\\prime}}=\\widehat{\\left(\\sigma^{\\prime}\\right)}$ applies, i.e., the derivative of the dual activation function is the dual of the derivative of the activation function. Here $\\hat{\\sigma}$ denotes the dual activation function of $\\sigma$ ", "page_idx": 30}, {"type": "text", "text": "In the case $x=y$ we get ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\dot{\\Sigma}_{m}^{(2)}(x,x)\\overset{(S32)}{\\sim}\\frac{2\\sigma_{w}^{2}}{\\pi}m\\left(\\frac{\\sigma_{w}^{2}}{n_{0}}\\|x\\|^{2}+\\sigma_{b}^{2}\\right)^{-1/2},\\quad\\mathrm{and\\;using\\;}(537)}\\\\ &{\\dot{\\Sigma}_{m}^{(L)}(x,x)\\overset{(S32)}{\\sim}\\frac{2\\sigma_{w}^{2}}{\\pi}m\\left(\\sigma_{w}^{2}+\\sigma_{b}^{2}\\right)^{-1/2}\\quad\\mathrm{for\\;}L\\geq3.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We summarize the above calculations in the following lemma: ", "page_idx": 30}, {"type": "text", "text": "Lemma D.3. For m, L E N let ) and $\\dot{\\Sigma}_{m}^{(L)}$ basiThfct $\\operatorname{erf}_{m}$ then holds for any $x\\neq y$ with $x,y\\neq0$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\Sigma_{\\infty}^{(1)}(x,y)=\\displaystyle\\operatorname*{lim}_{m\\to\\infty}\\Sigma_{m}^{(1)}(x,y)\\overset{(S\\Delta)}{=}\\frac{{\\sigma_{w}^{2}}}{n_{0}}\\omega_{w}^{2}(x,y)+\\sigma_{b}^{2},}}\\\\ {{\\Sigma_{\\infty}^{(2)}(x,y)=\\displaystyle\\operatorname*{lim}_{m\\to\\infty}\\Sigma_{m}^{(2)}(x,y)\\overset{(S\\Delta)}{=}\\frac{{2\\sigma_{w}^{2}}}{\\pi}\\arcsin\\left(\\frac{\\frac{\\sigma_{w}^{2}}{n_{0}}\\langle x,y\\rangle+\\sigma_{b}^{2}}{\\sqrt{\\frac{\\sigma_{w}^{2}}{n_{0}}\\|x\\|^{2}+\\sigma_{b}^{2}}\\sqrt{\\frac{\\sigma_{w}^{2}}{n_{0}}\\|y\\|^{2}+\\sigma_{b}^{2}}}\\right)+\\sigma_{b}^{2},}}\\\\ {{\\Sigma_{\\infty}^{(L)}(x,y)=\\displaystyle\\operatorname*{lim}_{m\\to\\infty}\\Sigma_{m}^{(L)}(x,y)\\overset{(S\\Delta)}{=}\\frac{{2\\sigma_{w}^{2}}}{\\pi}\\arcsin\\left(\\frac{\\Sigma_{\\infty}^{(L-1)}(x,y)}{{\\sigma_{w}^{2}}+\\sigma_{b}^{2}}\\right)+\\sigma_{b}^{2}\\quad f\\omega\\,r\\,a l l\\quad L\\ge3,}}\\\\ {{\\Sigma_{\\infty}^{(L)}(x,x)=\\displaystyle\\operatorname*{lim}_{m\\to\\infty}\\Sigma_{m}^{(L)}(x,x)\\overset{(S\\Delta)^{2}}{=}\\sigma_{w}^{2}+\\sigma_{b}^{2}\\;\\;f o r\\,a l l\\quad L\\ge2,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and, assuming that $x,y$ are not parallel or that $\\sigma_{b}^{2}>0$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\dot{\\Sigma}_{\\infty}^{(2)}(x,y)=\\operatorname*{lim}_{m\\to\\infty}\\dot{\\Sigma}_{m}^{(2)}(x,y)\\,^{(S_{=}^{40})}\\frac{2\\sigma_{w}^{2}}{\\pi}\\left(\\frac{\\sigma_{w}^{4}}{n_{0}^{0}}\\left(\\|x\\|^{2}\\|y\\|^{2}-\\langle x,y\\rangle^{2}\\right)+\\frac{\\sigma_{w}^{2}\\sigma_{b}^{2}}{n_{0}}\\|x-y\\|^{2}\\right)^{-\\frac{1}{2}}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=\\frac{2\\sigma_{w}^{2}}{\\pi}\\left(\\Sigma_{\\infty}^{(1)}(x,x)\\,\\Sigma_{\\infty}^{(1)}(y,y)\\right)-\\Sigma_{\\infty}^{(1)}(x,y)^{2}\\right)^{-\\frac{1}{2}},}\\\\ &{\\dot{\\Sigma}_{\\infty}^{(L)}(x,y)=\\operatorname*{lim}_{m\\to\\infty}\\dot{\\Sigma}_{m}^{(L)}(x,y)\\,^{(S_{=}^{41})}\\frac{2\\sigma_{w}^{2}}{\\pi}\\left((\\sigma_{w}^{2}+\\sigma_{b}^{2})^{2}-\\Sigma_{\\infty}^{(L-1)}(x,y)^{2}\\right)^{-\\frac{1}{2}}\\quad f o r\\,a l l\\ L\\geq3,}\\\\ &{\\dot{\\Sigma}_{m}^{(2)}(x,x)\\,^{(S_{=}^{42})}\\frac{2\\sigma_{w}^{2}}{\\pi}m\\left(\\frac{\\sigma_{w}^{2}}{n_{0}}\\|x\\|^{2}+\\sigma_{b}^{2}\\right)^{-\\frac{1}{2}},}\\\\ &{\\dot{\\Sigma}_{m}^{(L)}(x,x)\\,^{(S_{=}^{43})}\\frac{2\\sigma_{w}^{2}}{\\pi}m\\left(\\sigma_{w}^{2}+\\sigma_{b}^{2}\\right)^{-\\frac{1}{2}}\\quad f o r\\,L\\geq3.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Remark D.3 (Addendum to Remark C.5). In Remark C.5 we discussed the topology of the space of the Gaussian processes to which ANNs with continuous activation functions converge in the infinite-widthlimit.Theproduct $\\sigma$ -algebra restricts us to a countable input set, so it is not possible to check for properties such as continuity or even differentiability. While Theorem C.4 is stated only for continuous activation functions with linear envelope property, we will see in Theorem E.3 that the convergence also holds in the (weak) infinite-width limit even for step-like activation functions. For the sign function as activation function, the covariance function of this Gaussian process is then givenby $\\Sigma_{\\infty}^{(L)}$ Since we know explicity what this covariancefunction looks like, we can examine the sample-continuity of the process. Note that to get the full picture, one would still have to show functional convergence of the network to this process,as Bracale et al.[2021] have done. ", "page_idx": 31}, {"type": "text", "text": ") is isotropic when restricted to a sphere, as will be discussed in the next section. In the case of isotropic covariance functions, $k(x,y)=k(\\|x-y\\|)$ ,thesimplestwaytoshowsample-continuity using theKolmogorov-Chentsov criterion is to show Lipschitzcontinuity of $k$ at zero. This can be seen from the proof of Lemma 4.3 by Lang and Schwab [2015]. In our case, the covariance function is basically given by a composition of arcsin functions. Lipschitz.continuity of $k$ at zero is therefore equivalent to Lipschitz continuity of the arcsin function at 1, which does not hold. In conclusion, it is notposftaay $\\Sigma_{\\infty}^{(L)}$ in the estabished way using the Kolmogorov-Chentsov criterion. ", "page_idx": 31}, {"type": "text", "text": "Clearly, the kernel can be rewritten in terms of the arccos function. Cho and Saul [2009] analyzed arc-cosinekernels in thecontext of deep learningin detail. ", "page_idx": 31}, {"type": "text", "text": "Corollary D.3. For $m,L\\in\\mathbb{N}$ $\\Theta_{m}^{(L)}$ as inTheCforactivatonftio $\\textstyle\\operatorname{erf}_{m}$ $x\\neq y$ and either $x,y$ not parallel or $\\sigma_{b}^{2}>0$ then the limit ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\Theta_{\\infty}^{(L)}(x,y)=\\operatorname*{lim}_{m\\to\\infty}\\Theta_{m}^{(L)}(x,y)\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "exists. Furthermore, it holds, asymptotically as $m\\rightarrow\\infty$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\Theta_{m}^{(L)}(x,x)\\sim\\frac{2\\sigma_{w}^{2}}{\\pi}\\left(\\frac{\\sigma_{w}^{2}}{n_{0}}\\|x\\|^{2}+\\sigma_{b}^{2}\\right)^{\\frac{1}{2}}\\left(\\frac{2\\sigma_{w}^{2}}{\\pi}\\left(\\sigma_{w}^{2}+\\sigma_{b}^{2}\\right)^{-\\frac{1}{2}}\\right)^{L-2}m^{L-1}\\quad f o r\\quad L\\geq2.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof. The first statement directly follows from Lemma D.3 and the definition of $\\Theta_{m}^{(L)}$ .The recursive definition can be resolved to the following formula: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\Theta_{m}^{(L)}(x,y)=\\sum_{k=1}^{L}\\Sigma_{m}^{(k)}(x,y)\\cdot\\prod_{l=k}^{L-1}\\dot{\\Sigma}_{m}^{(l+1)}(x,y).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "With ", "page_idx": 32}, {"type": "equation", "text": "$$\nK(z):=\\frac{2\\sigma_{w}^{2}}{\\pi}\\left(\\frac{\\sigma_{w}^{2}}{n_{0}}z+\\sigma_{b}^{2}\\right)^{-\\frac{1}{2}},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "we get from Lemma D.3 that $\\dot{\\Sigma}_{m}^{(2)}(x,x)\\sim K\\left(||x||^{2}\\right)\\!\\cdot\\!m$ and $\\dot{\\Sigma}_{m}^{(L)}(x,x)\\sim K(n_{0}){\\cdot}m$ for $L\\geq3$ . This implies $\\begin{array}{r}{\\prod_{l=1}^{L-1}\\dot{\\Sigma}_{m}^{(l+1)}(x,x)\\sim K(\\|x\\|^{2})K(n_{0})^{L-2}\\cdot m^{L-1}}\\end{array}$ and $\\begin{array}{r}{\\prod_{l=k}^{L-1}\\dot{\\Sigma}_{m}^{(l+1)}(x,x)\\sim K(n_{0})^{L-k}}\\end{array}$ $m^{L-k}$ forany $k\\geq2$ For $L\\ge2$ weget that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\Theta_{m}^{(L)}(x,x)\\sim\\Sigma_{\\infty}^{(1)}(x,y)K\\left(\\|x\\|^{2}\\right)K(n_{0})^{L-2}\\cdot m^{L-1}+\\displaystyle\\sum_{k=2}^{L}\\Sigma_{\\infty}^{k}(x,x)\\cdot K(n_{0})^{L-k}\\cdot m^{L-k}}}\\\\ {{\\qquad\\qquad\\sim\\left(\\displaystyle\\frac{\\sigma_{w}^{2}}{n_{0}}\\|x\\|^{2}+\\sigma_{b}^{2}\\right)\\displaystyle\\frac{2\\sigma_{w}^{2}}{\\pi}\\left(\\displaystyle\\frac{\\sigma_{w}^{2}}{n_{0}}\\|x\\|^{2}+\\sigma_{b}^{2}\\right)^{-\\frac{1}{2}}K(n_{0})^{L-2}m^{L-1}}}\\\\ {{\\qquad=\\displaystyle\\frac{2\\sigma_{w}^{2}}{\\pi}\\left(\\displaystyle\\frac{\\sigma_{w}^{2}}{n_{0}}\\|x\\|^{2}+\\sigma_{b}^{2}\\right)^{\\frac{1}{2}}\\left(\\displaystyle\\frac{2\\sigma_{w}^{2}}{\\pi}\\left(\\sigma_{w}^{2}+\\sigma_{b}^{2}\\right)^{-\\frac{1}{2}}\\right)^{L-1}m^{L-2}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Theorem D.4 (Inspired by Lemma 5 of Radhakrishnan et al. [2023]). Let $\\sigma_{b}^{2}>0$ orlet all $\\boldsymbol{x}_{i}\\in\\mathbb{R}^{n_{0}}$ $L\\geq2$ and $x_{i}\\in S_{R}^{n_{0}-1}$ for all $i=1,\\ldots,d,$ where $S_{R}^{n_{0}-1}\\subseteq\\mathbb{R}^{n_{0}}$ thesphereofradius $R$ Then,with ", "page_idx": 32}, {"type": "equation", "text": "$$\nc^{(L)}(x):=\\operatorname*{lim}_{m\\to\\infty}\\mathrm{sign}\\left(\\Theta_{m}^{(L)}(x,\\mathcal{X})\\Theta_{m}^{(L)}(\\mathcal{X},\\mathcal{X})^{-1}\\mathcal{Y}\\right),\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "and assmin that $\\Theta_{\\infty}^{(L)}(x,\\mathcal{X})\\mathcal{Y}\\neq0$ for almost al $x\\in S_{R}^{n_{0}-1}$ itholds ", "page_idx": 32}, {"type": "equation", "text": "$$\nc^{(L)}(x)=\\mathrm{sign}\\left(\\Theta_{\\infty}^{(L)}(x,\\lambda)\\mathcal{y}\\right)\\quad a.e.\\ o n\\,S_{R}^{n_{0}-1}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Prof. First notetatalmost ll $x\\in S_{R}^{n_{0}-1}$ are nt palleto any $x_{i}\\in\\mathcal{X}$ Wedenote $\\Theta_{m}^{(L)}(\\mathcal{X},\\mathcal{X})=:$ ,forconvenience.Let be a positive constant that we will choose later. It then holds foralmost al $x\\in S_{R}^{n_{0}-1}$ ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ \\mathrm{sign}\\left(\\Theta_{m}^{(L)}(x,\\mathcal{X})\\Theta_{m}^{-1}\\mathcal{Y}\\right)=\\mathrm{sign}\\left(a_{m}\\Theta_{m}^{(L)}(x,\\mathcal{X})\\Theta_{m}^{-1}\\mathcal{Y}\\right)}\\\\ &{=\\!\\mathrm{sign}\\Bigg(\\underbrace{\\left[a_{m}\\Theta_{m}^{(L)}(x,\\mathcal{X})\\Theta_{m}^{-1}\\mathcal{Y}-\\Theta_{m}^{(L)}(x,\\mathcal{X})\\mathcal{Y}\\right]}_{=:A_{m}}+\\underbrace{\\left[\\Theta_{m}^{(L)}(x,\\mathcal{X})\\mathcal{Y}-\\Theta_{\\infty}^{(L)}(x,\\mathcal{X})\\mathcal{Y}\\right]}_{=:B_{m}}}\\\\ &{\\ \\ +\\,\\Theta_{\\infty}^{(L)}(x,\\mathcal{X})\\mathcal{Y}\\Bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We now show that $A_{m}$ and $B_{m}$ go to zero as $m\\rightarrow\\infty$ for a suitable choice of $a_{m}$ . First, note that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|B_{m}|\\leq\\left\\|\\Theta_{m}^{(L)}(x,\\mathcal{X})-\\Theta_{\\infty}^{(L)}(x,\\mathcal{X})\\right\\|_{2}\\left\\|\\mathcal{Y}\\right\\|_{2}\\xrightarrow{m\\to\\infty}0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "since $\\|\\mathcal{V}\\|_{2}<\\infty$ and by Corollary D.3 it holds that $\\Theta_{m}^{(L)}(x,x_{i})\\rightarrow\\Theta_{\\infty}^{(L)}(x,x_{i})$ for all $x_{i}\\in\\mathcal{X}$ as $m\\rightarrow\\infty$ . Second, we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|A_{m}|\\leq\\left\\|a_{m}\\Theta_{m}^{(L)}(x,\\mathcal{X})\\Theta_{m}^{-1}-\\Theta_{m}^{(L)}(x,\\mathcal{X})\\right\\|_{2}\\left\\|\\mathcal{Y}\\right\\|_{2}\\leq\\left\\|\\Theta_{m}^{(L)}(x,\\mathcal{X})\\right\\|_{2}\\left\\|a_{m}\\Theta_{m}^{-1}-\\mathrm{I}_{d}\\right\\|_{2}\\left\\|\\mathcal{Y}\\right\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Again by Corollary D.3 it holds that $\\|\\Theta_{m}^{(L)}(x,\\mathcal{X})\\|_{2}\\to\\|\\Theta_{\\infty}^{(L)}(x,\\mathcal{X})\\|_{2}$ as $m\\rightarrow\\infty$ . Furthermore, for all $1\\leq i\\leq d$ we have $\\Theta_{m}^{(L)}(x_{i},x_{i})\\sim C(R)\\cdot m^{L-1}$ for some constant $C(R)$ which depends on $R=\\|x_{i}\\|$ . Since it holds $\\Theta_{m}^{(L)}(x_{j},x_{i})\\to\\Theta_{\\infty}^{(L)}(x_{j},x_{i})$ as $m\\rightarrow\\infty$ for any $i\\neq j$ , we choose $a_{m}=C(R)\\cdot m^{L-1}$ and conclude ", "page_idx": 33}, {"type": "equation", "text": "$$\na_{m}^{-1}\\Theta_{m}^{(L)}(x_{i},x_{j})\\xrightarrow{m\\rightarrow\\infty}\\left\\{1\\quad\\mathrm{if~}i=j\\right.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "This implies $a_{m}^{-1}\\Theta_{m}\\to\\mathrm{I}_{d}$ as $m\\rightarrow\\infty$ . In particular, $\\left\\{\\operatorname{I}_{d}\\right\\}\\cup\\left\\{a_{m}^{-1}\\Theta_{m}\\ |\\ m\\in\\mathbb{N}\\right\\}$ is a compact set with respect to $\\Vert\\cdot\\Vert_{2}$ . Since $D\\mapsto D^{-1}$ is a continuous function, $\\{\\mathrm{I}_{d}\\}\\cup\\{a_{m}\\hat{\\Theta}_{m}^{-1}\\ |\\ m\\ \\bar{\\in}\\ \\mathbb{N}\\}$ is bounded. We get ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|{a_{m}\\Theta_{m}^{-1}-\\mathrm{I}_{d}}\\right\\|_{2}\\leq\\left\\|{a_{m}\\Theta^{-1}}\\right\\|_{2}\\left\\|{\\mathrm{I}_{d}-a_{m}^{-1}\\Theta_{m}}\\right\\|_{2}\\xrightarrow{m\\to\\infty}0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and thus ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left|A_{m}\\right|\\stackrel{(S44)}{\\leq}\\left\\|\\Theta_{m}^{(L)}(x,\\mathcal{X})\\right\\|_{2}\\left\\|a_{m}\\Theta_{m}^{-1}-\\mathrm{I}_{d}\\right\\|_{2}\\left\\|\\mathcal{Y}\\right\\|_{2}\\stackrel{m\\rightarrow\\infty}{\\longrightarrow}0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Recall that $\\Theta_{\\infty}^{(L)}(x,\\mathcal{X})\\mathcal{Y}\\neq0$ , which now concludes the proof: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{m\\rightarrow\\infty}{\\operatorname*{lim}}\\operatorname{sign}\\left(\\Theta_{m}^{(L)}(x,\\mathcal{X})\\Theta_{m}^{-1}\\mathcal{Y}\\right)=\\underset{m\\rightarrow\\infty}{\\operatorname*{lim}}\\operatorname{sign}\\left(A_{m}+B_{m}+\\Theta_{\\infty}^{(L)}(x,\\mathcal{X})\\mathcal{Y}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\operatorname{sign}\\left(\\Theta_{\\infty}^{(L)}(x,\\mathcal{X})\\mathcal{Y}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Remark D.4. One can generalize the above theorem by dropping therestriction to $S_{R}^{n_{0}-1}$ By Lemma $D.3$ it holds $\\begin{array}{r}{\\Theta_{m}^{(L)}(x_{i},x_{i})\\sim\\sqrt{\\frac{\\sigma_{w}^{2}}{n_{0}}\\|x_{i}\\|^{2}+\\sigma_{b}^{2}}\\cdot C\\cdot m^{L-1}}\\end{array}$ For some constant $C$ independent of $L$ and $\\left\\Vert x_{i}\\right\\Vert$ If wenowdefine $a_{m}:=C\\cdot m^{L-1}$ , we get ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{a_{m}^{-1}\\Theta_{m}^{(L)}(\\mathcal{X},\\mathcal{X})\\xrightarrow{\\xrightarrow{m\\rightarrow\\infty}}\\mathrm{diag}\\left\\{\\left(\\sqrt{\\sigma_{w}^{2}\\|x_{i}\\|^{2}/n_{0}+\\sigma_{b}^{2}}\\right)_{1\\leq i\\leq d}\\right\\}=:D,}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $\\mathrm{diag}\\{v\\}$ of a vector $v$ is a square matrix with diagonal $v$ . As in the proof above, and using the continuity of the inverse map $D\\mapsto D^{-1}$ ,this implies ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{m\\to\\infty}\\mathrm{sign}\\left(\\Theta_{m}^{(L)}(x,\\mathcal{X})\\Theta_{m}^{-1}\\mathcal{Y}\\right)=\\mathrm{sign}\\left(\\Theta_{\\infty}^{(L)}(x,\\mathcal{X})D^{-1}\\mathcal{Y}\\right)=\\mathrm{sign}\\left(\\tilde{\\Theta}_{\\infty}^{(L)}(x,\\mathcal{X})\\mathcal{Y}\\right),\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "if we define ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\tilde{\\Theta}_{\\infty}^{(L)}(x,y):=\\left(\\sigma_{w}^{2}\\|x\\|^{2}/n_{0}+\\sigma_{b}^{2}\\right)^{-1/2}\\left(\\sigma_{w}^{2}\\|y\\|^{2}/n_{0}+\\sigma_{b}^{2}\\right)^{-1/2}\\Theta_{\\infty}^{(L)}(x,y).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "In conclusion, dropping the restriction leads to a similar form for our classifier, but with a modified kernel. ", "page_idx": 33}, {"type": "text", "text": "Theorem D.4 shows that the classifier ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{m\\to\\infty}\\mathrm{sign}\\left(\\Theta_{m}^{(L)}(x,\\mathcal{X})\\Theta_{m}^{-1}\\mathcal{Y}\\right)=\\operatorname*{lim}_{m\\to\\infty}\\mathrm{sign}\\left(f_{\\mathrm{NTK}}^{m}(x)\\right)\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "has an easily interpretable form that is close to the Nadaraya-Watson estimator with singular kernel O. This is despite the fact that the pointwise limit of frk is rivial, i.e., regression is no longer possible ", "page_idx": 33}, {"type": "text", "text": "D.3  Checking for Bayes optimality ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "In addition to the ideas used in the above section, Radhakrishnan et al. [2023] proved that the resulting estimator is Bayes optimal under certain conditions. This property is also referred to as consistency. Let the probability distribution $\\mathbb{P}_{\\mathrm{data}}$ on our data space be given by a random variable $(X,Y)\\in\\dot{\\mathbb{R}}^{n_{0}}\\times\\{-\\bar{1},1\\}$ and let ", "page_idx": 33}, {"type": "equation", "text": "$$\nc(x)=\\underset{\\tilde{y}\\in\\{-1,1\\}}{\\arg\\operatorname*{max}}\\;\\mathbb{P}(Y=\\tilde{y}\\mid X=x)\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "be the Bayes classifier with respect to this distribution. Denote ${\\mathcal{X}}_{d}\\,=\\,(x_{1},\\ldots,x_{d})$ and $y_{d}\\,=$ $(y_{1},\\ldots,y_{d})$ for $(x_{i},y_{i})$ drawn independently from $\\mathbb{P}_{\\mathrm{data}}$ .We then define Bayes optimality as follows: ", "page_idx": 33}, {"type": "text", "text": "Definition D.1 (Bayes optimality). Let $c_{d}(\\,\\cdot\\,)=c_{d}(\\,\\cdot\\,;\\mathcal{X}_{d},\\mathcal{Y})$ be classifiers for $d\\in\\mathbb{N}$ and estimators of $c(\\,\\cdot\\,)$ We then say that $(c_{d})_{d\\in\\mathbb{N}}$ is Bayes optimal, if it is a consistent estimator of c, i.e., for all $\\varepsilon>0$ and $X$ -almost all $x$ ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{d\\to\\infty}\\mathbb{P}\\big[|c_{d}(x)-c(x)|>\\varepsilon\\big]=0.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "First, we summarize the results of Radhakrishnan et al. [2023] and consider $\\Theta^{(L)}$ . If the singular limiting kernel for $L\\rightarrow\\infty$ behaves like a singular kernel of the form $k(x,y)=\\|x-y\\|^{-\\alpha}$ , then the classifier for $L\\rightarrow\\infty$ will be of the form ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathrm{sign}\\left(\\frac{\\sum_{i=1}^{d}y_{i}/\\|x-x_{i}\\|^{\\alpha}}{\\sum_{i=1}^{d}1/\\|x-x_{i}\\|^{\\alpha}}\\right),\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "assuming $\\begin{array}{r}{\\sum_{i=1}^{d}1/\\|x-x_{i}\\|^{\\alpha}>0}\\end{array}$ This classffer satisfies Bayes optimality for $\\alpha=n_{0}$ byDevroye et al. [1998]. Radhakrishnan et al. [2023] generalized the results of Devroye et al. [1998], expressed $\\alpha$ in terms of the activation function and its derivative, and chose them in such a way as to achieve $\\alpha=n_{0}$ . Going back to our setup, we want to see if we can write ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{m\\to\\infty}\\Theta_{m}^{(L)}(x,y)=\\frac{R(\\|x-y\\|)}{\\|x-y\\|^{\\alpha}},\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "for some constant $\\alpha$ and for some function $R\\colon\\mathbb{R}_{+}\\to\\mathbb{R}$ bounded away from zero as $\\|x-y\\|\\to0$ We start by proving that $\\Theta_{m}^{(L)}(x,y)=G(\\|x-y\\|)$ for some function $G$ Recall that we have to restrict ourselves to a sphere by Theorem D.4. For simplicity, we restrict ourselves to the unit sphere $S^{n_{0}-1}$ . Then, it holds $\\bar{\\langle x,x\\rangle}=1$ for all $x\\in S^{n_{0}-1}$ , which gives us ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\|x-y\\|^{2}=\\langle x-y,x-y\\rangle=\\langle x,x\\rangle-2\\langle x,y\\rangle+\\langle y,y\\rangle=2(1-\\langle x,y\\rangle).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "In the following, we will substitute $z\\,=\\,\\langle x,y\\rangle$ . Taking $\\|x-y\\|\\to0$ is then equivalent to taking $z\\ \\rightarrow\\ 1$ Wwe can conclude from Lemma D3 that we can inded write $\\Sigma_{\\infty}^{(L)}(x,y)\\;=\\;\\Sigma_{\\infty}^{(L)}(z)$ $\\dot{\\Sigma}_{\\infty}^{(L)}(x,y)=\\dot{\\Sigma}_{\\infty}^{(L)}(z)$ and hence $\\Theta_{\\infty}^{(L)}(x,y)=\\Theta_{\\infty}^{(L)}(z)$ for all $L\\geq1$ Note that $\\Sigma_{\\infty}^{(L)}(1)=\\sigma_{w}^{2}+\\sigma_{b}^{2}$ for $L\\ge2$ and $\\Sigma_{\\infty}^{(L)}(1)=\\sigma_{w}^{2}/n_{0}+\\sigma_{b}^{2}$ . Next, we consider $\\dot{\\Sigma}_{\\infty}^{(2)}(z)$ for $z\\rightarrow1$ using Lemma D.3 ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\dot{\\Sigma}_{\\infty}^{(2)}(z)=\\frac{2\\sigma_{w}^{2}}{\\pi}\\left(\\frac{\\sigma_{w}^{4}}{n_{0}^{2}}(1-z^{2})+\\frac{\\sigma_{w}^{2}\\sigma_{b}^{2}}{n_{0}}\\cdot2(1-z)\\right)^{-\\frac{1}{2}}}\\\\ &{=\\frac{2\\sigma_{w}^{2}}{\\pi}\\left(\\frac{\\sigma_{w}^{4}}{n_{0}^{2}}(1-z)(1+z)+\\frac{2\\sigma_{w}^{2}\\sigma_{b}^{2}}{n_{0}}(1-z)\\right)^{-\\frac{1}{2}}}\\\\ &{\\overset{z\\to1}{\\sim}\\frac{2\\sigma_{w}^{2}}{\\pi}\\left(\\frac{2\\sigma_{w}^{4}}{n_{0}^{2}}(1-z)+\\frac{2\\sigma_{w}^{2}\\sigma_{b}^{2}}{n_{0}}(1-z)\\right)^{-\\frac{1}{2}}=\\frac{2\\sigma_{w}^{2}}{\\pi}\\left(\\frac{2\\sigma_{w}^{2}}{n_{0}}\\left(\\frac{\\sigma_{w}^{2}}{n_{0}}+\\sigma_{b}^{2}\\right)\\right)^{-\\frac{1}{2}}(1-z)^{-\\frac{1}{2}}}\\\\ &{=\\frac{n_{0}}{\\pi}\\left(\\frac{2\\sigma_{w}^{2}}{n_{0}}\\right)^{\\frac{1}{2}}\\left(\\frac{\\sigma_{w}^{2}}{n_{0}}+\\sigma_{b}^{2}\\right)^{-\\frac{1}{2}}(1-z)^{-\\frac{1}{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "For $L\\geq3$ , we can observe that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\dot{\\Sigma}_{\\infty}^{(L)}(z)=\\frac{2\\sigma_{w}^{2}}{\\pi}\\left(\\left(\\sigma_{w}^{2}+\\sigma_{b}^{2}\\right)^{2}-\\Sigma_{\\infty}^{(L-1)}(z)^{2}\\right)^{-\\frac{1}{2}}}\\\\ &{\\qquad\\qquad=\\frac{2\\sigma_{w}^{2}}{\\pi}\\left(\\left(\\sigma_{w}^{2}+\\sigma_{b}^{2}-\\Sigma_{\\infty}^{(L-1)}(z)\\right)\\left(\\sigma_{w}^{2}+\\sigma_{b}^{2}+\\Sigma^{(L-1)}\\infty(z)\\right)\\right)^{-\\frac{1}{2}}}\\\\ &{\\qquad\\qquad\\overset{z\\to1}{\\sim}\\frac{2\\sigma_{w}^{2}}{\\pi}\\left(2\\left(\\sigma_{w}^{2}+\\sigma_{b}^{2}\\right)\\right)^{-\\frac{1}{2}}\\left(\\left(\\sigma_{w}^{2}+\\sigma_{b}^{2}\\right)-\\Sigma_{\\infty}^{(L-1)}(z)\\right)^{-\\frac{1}{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We will now prove a lemma that allows us to analyze the behavior of $\\dot{\\Sigma}_{\\infty}^{(L)}(z)$ as $z\\rightarrow1$ Lemma D.5. It holds: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{z\\to1}{\\frac{(1-z)^{1/2}}{1-{\\frac{2}{\\pi}}\\arcsin{(z)}}}={\\frac{\\pi}{2{\\sqrt{2}}}}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Proof. Since the numerator and the denominator both go to zero as $z\\rightarrow1$ , we apply 1'Hopital's rule and differentiate both. This yields ", "page_idx": 35}, {"type": "equation", "text": "$$\n{\\frac{\\mathrm{d}}{\\mathrm{d}z}}\\left[(1-z)^{\\frac{1}{2}}\\right]=-{\\frac{1}{2}}(1-z)^{-{\\frac{1}{2}}}\\quad{\\mathrm{and}}\\quad{\\frac{\\mathrm{d}}{\\mathrm{d}z}}\\left[1-{\\frac{2}{\\pi}}\\arcsin\\left(z\\right)\\right]=-{\\frac{2}{\\pi}}\\left(1-z^{2}\\right)^{-{\\frac{1}{2}}}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Thus, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{z\\to1}{\\frac{(1-z)^{1/2}}{1-{\\frac{2}{\\pi}}\\arcsin\\left(z\\right)}}=\\operatorname*{lim}_{z\\to1}{\\frac{{\\frac{1}{2}}{\\sqrt{1-z}}{\\sqrt{1+z}}}{{\\frac{2}{\\pi}}{\\sqrt{1-z}}}}={\\frac{\\pi}{4}}{\\sqrt{2}}={\\frac{\\pi}{2{\\sqrt{2}}}},\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "concluding the proof. ", "page_idx": 35}, {"type": "text", "text": "For $L=3$ it now holds ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\dot{\\Sigma}_{\\infty}^{(3)}(z)\\stackrel{(s\\bar{\\omega})}{\\sim}\\frac{2\\sigma_{w}^{2}}{\\pi}\\left(2\\left(\\sigma_{w}^{2}+\\sigma_{b}^{2}\\right)\\right)^{-\\frac12}\\left(\\left(\\sigma_{w}^{2}+\\sigma_{b}^{2}\\right)-\\Sigma_{\\infty}^{(2)}(z)\\right)^{-\\frac12}}\\\\ &{\\qquad\\qquad\\quad=\\frac{2\\sigma_{w}^{2}}{\\pi}\\left(2\\left(\\sigma_{w}^{2}+\\sigma_{b}^{2}\\right)\\right)^{-\\frac12}\\sigma_{w}^{-1}\\left(1-\\frac{2}{\\pi}\\operatorname{arcsin}\\left(\\frac{\\sigma_{w}^{2}z/n_{0}+\\sigma_{b}^{2}}{\\sigma_{w}^{2}/n_{0}+\\sigma_{b}^{2}}\\right)\\right)^{-\\frac12}}\\\\ &{\\qquad\\stackrel{()}{\\sim}\\frac{2\\sigma_{w}^{2}}{\\pi}\\left(2\\left(\\sigma_{w}^{2}+\\sigma_{b}^{2}\\right)\\right)^{-\\frac12}\\sigma_{w}^{-1}\\sqrt{\\frac{\\pi}{2}}\\left(1-\\frac{\\sigma_{w}^{2}z/n_{0}+\\sigma_{b}^{2}}{\\sigma_{w}^{2}/n_{0}+\\sigma_{b}^{2}}\\right)^{-\\frac14}}\\\\ &{\\qquad=\\frac{2\\sigma_{w}^{2}}{\\pi}\\left(2\\left(\\sigma_{w}^{2}+\\sigma_{b}^{2}\\right)\\right)^{-\\frac12}\\sigma_{w}^{-1}\\sqrt{\\frac{\\pi}{2\\sqrt{2}}}\\left(\\frac{\\sigma_{w}^{2}/n_{0}+(1-z)}{\\sigma_{w}^{2}/n_{0}+\\sigma_{b}^{2}}\\right)^{-\\frac14}}\\\\ &{\\qquad=\\sqrt{\\frac{2\\sigma_{w}^{2}}{\\pi}}\\left(2\\sqrt{2}\\left(\\sigma_{w}^{2}+\\sigma_{b}^{2}\\right)\\right)^{-\\frac12}\\left(\\frac{\\sigma_{w}^{2}/n_{0}}{\\sigma_{w}^{2}/n_{0}+\\sigma_{b}^{2}}\\right)^{-\\frac14}(1-z)^{-\\frac14},}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where we used Lemma D.5 at $(\\star)$ . For $L\\geq4$ it holds ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\dot{\\Sigma}_{\\infty}^{(L)}(z)\\stackrel{(S\\omega)}{\\sim}\\frac{2\\sigma_{\\infty}^{2}}{\\pi}\\left(2\\left(\\sigma_{\\infty}^{2}+\\sigma_{\\theta}^{2}\\right)\\right)^{-\\frac12}\\left((\\sigma_{\\infty}^{2}+\\sigma_{\\theta}^{2})-\\Sigma_{\\infty}^{(L-1)}(z)\\right)^{-\\frac14}}\\\\ &{\\quad-\\frac{1}{2}3\\frac{2\\sigma_{\\infty}^{2}}{\\pi}\\left(2\\left(\\sigma_{\\infty}^{2}+\\sigma_{\\theta}^{2}\\right)\\right)^{-\\frac12}\\sigma_{\\infty}^{-1}\\left(1-\\frac{2}{\\pi}\\arcsin\\left(\\frac{\\sum_{v=0}^{\\lfloor\\alpha-2\\rfloor}(z)}{\\sigma_{v}^{2}+\\sigma_{\\theta}^{2}}\\right)\\right)^{-\\frac12}}\\\\ &{\\quad\\stackrel{()}{\\sim}\\frac{2\\sigma_{\\infty}^{2}}{\\pi}\\left(2\\left(\\sigma_{\\infty}^{2}+\\sigma_{\\theta}^{2}\\right)\\right)^{-\\frac12}\\sigma_{\\infty}^{-1}\\sqrt{\\frac{\\pi}{2\\sqrt{2}}}\\left(1-\\frac{\\sum_{v=0}^{\\lfloor\\alpha-2\\rfloor}(z)}{\\sigma_{v}^{2}+\\sigma_{\\theta}^{2}}\\right)^{-\\frac14}}\\\\ &{\\quad=\\frac{2\\sigma_{\\infty}^{2}}{\\pi}\\left(2\\left(\\sigma_{\\infty}^{2}+\\sigma_{\\theta}^{2}\\right)\\right)^{-\\frac12}\\sigma_{\\infty}^{-1}\\sqrt{\\frac{\\pi}{2\\sqrt{2}}}\\left(\\sigma_{\\infty}^{2}+\\sigma_{\\theta}^{2}\\right)^{\\frac14}\\left(\\sigma_{w}^{2}+\\sigma_{b}^{2}-\\Sigma_{\\infty}^{(L-2)}(z)\\right)^{-\\frac14}}\\\\ &{\\quad\\stackrel{(s\\ k0)}{\\sim}\\sqrt{\\frac{2\\sigma_{\\infty}^{2}}{\\pi}}\\left(2\\left(\\sigma_{w}^{2}+\\sigma_{b}^{2}\\right)\\right)^{-\\frac12}\\sigma_{\\infty}^{-1}\\sqrt{\\frac{\\pi}{2\\sqrt{2}}}\\left(\\sigma_{w}^{2}+\\sigma_{b}^{2}\\right)^{\\frac14}\\sqrt{\\dot{\\Sigma}_{\\infty}^{(L-\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "again using Lemma D.5 at $(\\star)$ . This implies for arbitrary $L\\ge2$ ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\dot{\\Sigma}_{\\infty}^{(L)}(z)\\sim K(L)\\cdot(1-z)^{-1/2^{L-1}},\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "for some constant $K(L)$ depending on $L$ . Recall the formula for the NTK, that was used in the proof of Corollary D.3 ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Theta_{\\infty}^{(L)}(z)=\\displaystyle\\sum_{k=1}^{L}\\Sigma_{\\infty}^{(k)}(z)\\cdot\\displaystyle\\prod_{l=k}^{L-1}\\dot{\\Sigma}_{\\infty}^{(l+1)}(z)\\sim\\displaystyle\\sum_{k=1}^{L}\\Sigma_{\\infty}^{(k)}(z)\\cdot\\displaystyle\\prod_{l=k}^{L-1}K(l+1)\\cdot(1-z)^{-1/2^{L}}}\\\\ &{\\qquad\\qquad\\sim\\Sigma_{\\infty}^{(1)}(1)\\displaystyle\\prod_{l=1}^{L-1}K(l+1)\\cdot(1-z)^{-1/2^{l}}=K(L)\\cdot(1-z)^{-(1-1/2^{L-1})}}\\\\ &{\\qquad=K^{\\prime}(L)\\cdot\\|x-y\\|^{-(2-1/2^{L-2})}=:K^{\\prime}(L)\\cdot\\|x-y\\|^{-\\alpha(L)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "for some constants $K(L),K^{\\prime}(L)$ . It is therefore to possible find a function $R$ that is bounded away from zero near $\\|x-y\\|\\to0$ , such that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\Theta_{\\infty}^{(L)}(x,y)=K^{\\prime}(L)\\cdot{\\frac{R(\\|x-y\\|)}{\\|x-y\\|^{\\alpha(L)}}}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "However, we have that $\\alpha(1)=1$ and $\\alpha(L)\\uparrow2$ as $L\\rightarrow\\infty$ . So it is only possible to choose $L$ such that $\\alpha(L)=n_{0}$ ,if $n_{0}=1$ . But this is a trivial case, since we have restricted ourselves to the unit sphere. In conclusion, we cannot prove that $c^{(L)}$ is a Bayes optimal classifier for any choice of $L$ . Chapter 4 of Devroye et al. [1998] suggests that, in fact, the estimator will not be universally consistent. ", "page_idx": 36}, {"type": "text", "text": "E  The NTK for surrogate gradient learning ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "In this chapter we explore surrogate gradient learning, introduced in Section 1, by connecting it to the NTK. Recall that the standard gradient flow dynamics of the parameters are given by ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\theta_{t}=-\\eta\\,\\nabla_{\\theta}\\mathcal{L}\\big(f(\\mathcal{X};\\theta_{t});\\mathcal{Y}\\big)=-\\eta\\,J_{\\theta}f(\\mathcal{X};\\theta_{t})^{\\top}\\,\\nabla_{f(\\mathcal{X};\\theta_{t})}\\mathcal{L}\\big(f(\\mathcal{X};\\theta_{t});\\mathcal{Y}\\big).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "As mentioned several times before, the Jacobian matrix $J_{\\theta}$ will vanish for all parameters except those in the last layer if we consider the sign activation function due to its zero derivative. The idea of surrogate gradient learning is to circumvent the zero derivative of the sign function by replacing the derivative with a surrogate derivative [Neftci et al., 2019]. We can replace the derivative in two main ways: ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The activation function in the full network can be replaced by a differentiable surrogate activation function. In particular, we obtain a non-vanishing surrogate derivative of the activation function and thus non-vanishing network gradients. Let $g$ denote thenetwork with surrogate activation function. Therefore, we can train the weights according to ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\theta_{t}=-\\eta\\,J_{\\theta}g(\\boldsymbol{\\mathcal{X}};\\theta_{t})^{\\sf T}\\,\\nabla_{g(\\boldsymbol{\\mathcal{X}};\\theta_{t})}\\mathcal{L}\\big(g(\\boldsymbol{\\mathcal{X}};\\theta_{t});\\boldsymbol{\\mathcal{Y}}\\big),\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "or consider the loss with respect to $f$ and train according to ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\theta_{t}=-\\eta\\,J_{\\theta}g(\\boldsymbol{\\mathcal{X}};\\theta_{t})^{\\intercal}\\,\\nabla_{f(\\boldsymbol{\\mathcal{X}};\\theta_{t})}\\mathcal{L}\\big(f(\\boldsymbol{\\mathcal{X}};\\theta_{t});\\boldsymbol{\\mathcal{Y}}\\big).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "\u00b7 Instead of replacing the activation function, we can only replace the derivative of the activation function $\\dot{\\sigma}$ with a surrogate derivative $\\tilde{\\sigma}$ in Equation (S8). Let $J_{\\sigma,\\tilde{\\sigma}}$ be the quasiJacobian matrix as in Definition C.4 with activation function $\\sigma$ and surrogate derivative $\\tilde{\\sigma}$ Then the training is given by ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\frac{\\mathrm{d}}{\\mathrm{d}t}\\theta_{t}\\qquad\\qquad=-\\eta\\,J^{\\sigma,\\tilde{\\sigma}}(\\boldsymbol{\\mathcal{X}};\\theta_{t})^{\\intercal}\\,\\nabla_{f(\\boldsymbol{\\mathcal{X}};\\theta_{t})}\\mathcal{L}(f(\\boldsymbol{\\mathcal{X}};\\theta_{t});\\boldsymbol{\\mathcal{Y}})}\\\\ &{\\Longrightarrow\\frac{\\mathrm{d}}{\\mathrm{d}t}f(\\boldsymbol{x};\\theta_{t})=-\\eta\\,J_{\\theta}f(\\boldsymbol{x};\\theta_{t})J^{\\sigma,\\tilde{\\sigma}}(\\boldsymbol{\\mathcal{X}};\\theta_{t})^{\\intercal}\\,\\nabla_{f(\\boldsymbol{\\mathcal{X}};\\theta_{t})}\\mathcal{L}(f(\\boldsymbol{\\mathcal{X}};\\theta_{t});\\boldsymbol{\\mathcal{Y}})}\\\\ &{\\qquad\\qquad\\qquad=-\\eta\\,J^{\\sigma,\\tilde{\\sigma}}(\\boldsymbol{x};\\theta_{t})J^{\\sigma,\\tilde{\\sigma}}(\\boldsymbol{\\mathcal{X}};\\theta_{t})^{\\intercal}\\,\\nabla_{f(\\boldsymbol{\\mathcal{X}};\\theta_{t})}\\mathcal{L}(f(\\boldsymbol{\\mathcal{X}};\\theta_{t});\\boldsymbol{\\mathcal{Y}})}\\\\ &{\\qquad\\qquad=-\\eta\\,\\hat{I}^{(L)}(\\boldsymbol{x},\\boldsymbol{\\mathcal{X}};\\theta_{t})\\nabla_{f(\\boldsymbol{\\mathcal{X}};\\theta_{t})}\\mathcal{L}(f(\\boldsymbol{\\mathcal{X}};\\theta_{t});\\boldsymbol{\\mathcal{Y}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "with $\\hat{I}^{(L)}$ as in Definition C.5 for $\\sigma_{1}=\\sigma$ $\\tilde{\\sigma}_{1}=\\dot{\\sigma}$ $\\sigma_{2}=\\sigma$ and $\\tilde{\\sigma}_{2}=\\tilde{\\sigma}$ . For Equations (S48) and (S49) we assume that $\\dot{\\sigma}$ exists and is non-vanishing, but we deliberately train with a surrogate gradient. For example, we can again consider $\\operatorname{erf}_{m}$ as the activation function. Its derivative explodes at zero and vanishes everywhere else as $m\\rightarrow\\infty$ . The hope is that $\\begin{array}{r}{\\operatorname*{lim}_{n_{1},\\ldots n_{L-1}\\to\\infty}\\hat{I}_{m}^{(L)}=I_{m}^{(L)}}\\end{array}$ exists nd $\\operatorname*{lim}_{m\\to\\infty}I_{m}^{(L)}$ is no singular kermelas before ", "page_idx": 36}, {"type": "text", "text": "In this chapter we will deal with the second approach, because $J_{\\sigma,\\tilde{\\sigma}}(x;\\theta_{t})=:G(\\sigma;\\tilde{\\sigma};x;\\theta_{t})$ is closer to $J_{\\theta}f(x;\\bar{\\theta}_{t})=G(\\sigma;\\dot{\\sigma};x;\\theta_{t})$ than $J_{\\theta}g(x;\\theta_{t})=G(\\eta;\\tilde{\\sigma};x;\\theta_{t})$ as a formula if $J_{\\theta}f(x;\\theta_{t})$ exists and is non-vanishing. Here, $\\eta$ denotes the surrogate activation function with derivative $\\tilde{\\sigma}$ . To do this, we provide asymmetric generalizations of Theorem C.1, Theorem C.2, and Theorem C.3. These generalizations would, in principle, even allow us to compare the two approaches. ", "page_idx": 36}, {"type": "text", "text": "E.1  Asymmetric generalization of the neural tangent kernel ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "For this section we adopt the so-called linear envelope property from [Matthews et al., 2018, Definition 1] to ensure that all expectations exist in the following theorems: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\exists\\,m,c\\geq0\\quad\\forall\\,u\\in\\mathbb{R}:\\;|\\sigma(u)|\\leq c+m|u|\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "First, we consider networks under the weak infinite-width limit, and are thus interested in taking the number of hidden neurons to infinity sequentially. This is done inductively while using the central limit theorem and the weak law of large numbers. In order to do this rigorously, we state and prove twolemmata. ", "page_idx": 37}, {"type": "text", "text": "The first lemma is stated in terms of Gaussian measures on Hilbert spaces. An introduction to Gaussian measures on Hilbert spaces can be found in Chapter 1 of Da Prato [2006]. A rigorous derivation and definition of convergence in distribution on arbitrary metric spaces is given by Heyer [2009, Chapter 1.2]. This includes a definition of weak convergence in terms of continuous and bounded functions (Remark 1.2.5 (b), a version of the Portemanteau theorem (Theorem 1.2.7), and the fact that convergence in probability implies convergence in distribution (Application 1.2.15). ", "page_idx": 37}, {"type": "text", "text": "Lemma E.1. Let $H_{i}^{m}$ and $Z_{i}$ $i,m\\in\\mathbb{N}$ .berandomvariables withvalues in aseparableHilbert space $\\mathcal{H}$ Furthermore,let $Z_{i}$ be independent and identically distributed with finite mean and covariance operator $V$ If $(H_{1}^{m},\\ldots,H_{k}^{m})\\stackrel{\\mathcal{D}}{\\longrightarrow}(Z_{1},\\ldots,Z_{k})$ as $m\\rightarrow\\infty$ for all $k\\in\\mathbb{N}$ then there exists $k\\colon\\mathbb{N}\\rightarrow\\mathbb{N}$ such that $k(m)\\rightarrow\\infty$ monotonically as $m\\rightarrow\\infty$ and ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\frac{1}{\\sqrt{k(m)}}\\sum_{i=1}^{k(m)}H_{i}^{m}\\xrightarrow[m\\rightarrow\\infty]{\\mathcal{D}}Z,\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "for an $\\mathcal{H}$ -valued Gaussian randomvariable $Z$ with mean and covariance operator like $Z_{1}$ . Similarly, there exists $k^{\\prime}\\colon\\mathbb{N}\\rightarrow\\mathbb{N}$ such that $k^{\\prime}(m)\\rightarrow\\infty$ monotonically as $m\\rightarrow\\infty$ and ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\frac{1}{k^{\\prime}(m)}\\sum_{i=1}^{k^{\\prime}(m)}H_{i}^{m}\\xrightarrow[m\\rightarrow\\infty]{\\mathcal{D}}\\mathbb{E}[Z].\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "ProofSince $\\mathcal{H}$ is separable and complete, convergence in distribution and convergence with respect to the Prokhorov metric $d$ (also known as the L\u00e9vy-Prokhorov metric) are equivalent [Billingsley, 1999, Theorem 6.8]. By the central limit theorem for separable Hilbert spaces [Zalesskii et al., 1991], this implies ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{k\\to\\infty}d\\left({\\frac{1}{\\sqrt{k}}}\\sum_{i=1}^{k}Z_{i},Z\\right)=0.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "In addition, the assumption together with the continuous mapping theorem gives that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{m\\to\\infty}d\\left({\\frac{1}{{\\sqrt{k}}}}\\sum_{i=1}^{k}H_{i}^{m},{\\frac{1}{{\\sqrt{k}}}}\\sum_{i=1}^{k}Z_{i}\\right)=0\\quad{\\mathrm{for~all~}}k\\in\\mathbb{N}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "In particular, for any $k\\in\\mathbb{N}$ , there exists some $m_{k}\\in\\mathbb{N}$ such that ", "page_idx": 37}, {"type": "equation", "text": "$$\nd\\left({\\frac{1}{\\sqrt{k}}}\\sum_{i=1}^{k}H_{i}^{m},{\\frac{1}{\\sqrt{k}}}\\sum_{i=1}^{k}Z_{i}\\right)\\leq{\\frac{1}{k}}\\quad{\\mathrm{for~all~}}m\\geq m_{k}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We now want to choose $k(m)$ as large as possible for any $m$ , but small enough to ensure Inequality (S54), i.e., $m\\geq m_{k(m)}$ . So we define ", "page_idx": 37}, {"type": "equation", "text": "$$\nk(m):=\\operatorname*{sup}\\{k\\mid m\\geq m_{k}\\}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "First note that $\\{k\\mid m\\geq m_{k}\\}\\neq\\emptyset$ , if $m\\geq m_{1}$ . The map $k\\colon\\mathbb{N}\\rightarrow\\mathbb{N}$ is therefore well-defined, as we consider $m\\rightarrow\\infty$ . Similarly, we can find a $m\\geq m_{k}$ for any given $k$ . This yields ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{m\\to\\infty}k(m)=\\operatorname*{lim}_{m\\to\\infty}\\operatorname*{sup}\\{k\\mid m\\geq m_{k}\\}=\\infty.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "By definition of $k(m)$ , it holds $m\\geq m_{k(m)}$ for all $m\\in\\mathbb{N}$ and thus ", "page_idx": 38}, {"type": "equation", "text": "$$\nd\\left(\\frac{1}{\\sqrt{k(m)}}\\sum_{i=1}^{k(m)}H_{i}^{m},\\frac{1}{\\sqrt{k(m)}}\\sum_{i=1}^{k(m)}Z_{i}\\right)\\leq\\frac{1}{k(m)}\\quad\\mathrm{for~all~}m\\in\\mathbb{N}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Together with Equation (S53) this yields the claim, (S51): ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle d\\left(\\frac{1}{\\sqrt{k(m)}}\\sum_{i=1}^{k(m)}H_{i}^{m},Z\\right)}\\\\ &{\\le\\,\\,d\\left(\\frac{1}{\\sqrt{k(m)}}\\sum_{i=1}^{k(m)}H_{i}^{m},\\frac{1}{\\sqrt{k(m)}}\\sum_{i=1}^{k(m)}Z_{i}\\right)+d\\left(\\frac{1}{\\sqrt{k(m)}}\\sum_{i=1}^{k(m)}Z_{i},Z\\right)}\\\\ &{\\overset{(S55)}{\\le}\\frac{1}{k(m)}+d\\left(\\frac{1}{\\sqrt{k(m)}}\\sum_{i=1}^{k(m)}Z_{i},Z\\right)\\xrightarrow{m\\rightarrow\\infty}0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "For the second claim, (S52), one can follow the same procedure but use the law of large numbers for Banach spaces instead of the central limit theorem. Suitable results are given by Ledoux and Talagrand [1991, Corollary 7.10] and Hoffmann-Jorgensen and Pisier [1976, Theorem 2.1]. Note that even the strong law of large numbers holds, but the weak law is sufficient. \u53e3 ", "page_idx": 38}, {"type": "text", "text": "In the second lemma, some properties about convergence in distribution and convergence in probability arestated. ", "page_idx": 38}, {"type": "text", "text": "Lemma E.2 (Theorem 2.7 from van der Vaart [1998], modified and (iv) added). Let $X_{n},X$ and $Y_{n}$ berandomvectors.Then ", "page_idx": 38}, {"type": "text", "text": "(i) $X_{n}\\xrightarrow{\\mathcal{P}}X$ implies $X_{n}\\stackrel{{\\cal D}}{\\longrightarrow}X$ \uff0c   \n(i) $X_{n}\\xrightarrow{\\mathcal{P}}c$ fora constant c if and only if $\\boldsymbol{X_{n}}\\stackrel{\\mathcal{D}}{\\longrightarrow}\\boldsymbol{c},$   \n(ii)f $X_{n}\\xrightarrow{D}X$ and $Y_{n}\\xrightarrow{\\mathcal{P}_{\\neg}}c$ for a constant c, then $(X_{n},Y_{n})\\;{\\xrightarrow{\\mathcal{D}}}\\;(X,c),$   \n(iv) ${\\it{j}}\\boldsymbol{f}\\boldsymbol{X}_{n}\\stackrel{\\mathcal{D}}{\\longrightarrow}\\boldsymbol{X}$ and $W$ isarandomvectorindependentf $(X_{n})_{n\\in\\mathbb{N}}$ then $(X_{n},W)\\stackrel{\\mathcal{D}}{\\longrightarrow}(X,W)$ ", "page_idx": 38}, {"type": "text", "text": "Proof. (i) - (ii). The proofs are given by van der Vaart [1998]. ", "page_idx": 38}, {"type": "text", "text": "(iv). Let $f$ be a bounded and continuous function. Then, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\underset{n\\to\\infty}{\\mathrm{lim}}\\mathbb{E}[f(X_{n},W)]=\\operatorname*{lim}_{n\\to\\infty}\\int\\mathbb{E}\\left[f(X_{n},W)\\mid W\\right](x)\\,\\mathrm{d}\\mathbb{P}(w)}\\\\ &{\\displaystyle\\overset{(\\star)}{=}\\operatorname*{lim}_{n\\to\\infty}\\int\\mathbb{E}\\left[f(X_{n},W(w))\\right]\\,\\mathrm{d}\\mathbb{P}(w)=\\int\\operatorname*{lim}_{n\\to\\infty}\\mathbb{E}\\left[f(X_{n},W(w))\\right]\\,\\mathrm{d}\\mathbb{P}(w)}\\\\ &{=\\displaystyle\\int\\mathbb{E}\\left[f(X,W(w))\\right]\\,\\mathrm{d}\\mathbb{P}(w)=\\int\\mathbb{E}\\left[f(X,W)\\mid W\\right](w)\\,\\mathrm{d}\\mathbb{P}(w)=\\mathbb{E}[f(X,W)],}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where we used the independence of $W$ and $(X_{n})_{n\\in\\mathbb{N}}$ in Equation $(\\star)$ and the boundedness of $f$ for the interchange of limit and integration. This proves convergence in distribution. \u53e3 ", "page_idx": 38}, {"type": "text", "text": "Remark E.1. The above theorem can be generalized to metric spaces. One can easily check that the proofs in [van der Vaart, 1998, Theorem 2.7] also work for metric spaces using the Portemanteau theorem provided by Heyer [2009, Theorem 1.2.7]. However, it is necessary to derive some more equivalent characterizations of convergence in distribution, which are given and used by van der Vaart[1998]butaremissingintheworkofHeyer[2009]. ", "page_idx": 38}, {"type": "text", "text": "Theorem E.3 (Generalized version of Proposition 1 by Jacot et al. [2018]). For activation functions $\\sigma_{1}$ and $\\sigma_{2}$ with property (S50), which are continuous except for finitely many jump points, let $f_{1}(\\,\\cdot\\,;\\theta)$ and $f_{2}(\\,\\cdot\\,;\\theta)$ be networkfunctions with hidden layers $h_{1}^{(l)}(\\,\\cdot\\,;\\theta)$ $h_{2}^{(l)}(\\cdot;\\theta)$ for $1\\leq l\\leq L,$ respectively as in Definition C.1 and with shared weights $\\theta$ Then $(f_{1}(\\,\\cdot\\,;\\theta),f_{2}(\\,\\cdot\\,;\\theta))$ converges in distribution to a multidimenioal Gaussan process $(X_{j}^{(L)},Y_{j}^{(L)})_{j=1,\\dots,n_{L}}$ as $(n_{l})_{1\\le l\\le L-1}\\to\\infty$ weakly for any fxed countabe input st $(z_{i})_{i=1}^{\\infty}$ .The Gausian process i defned by $X_{j}^{(L)}$ 2 $\\mathcal{N}\\left(0,\\Sigma_{1}^{(L)}\\right)$ $Y_{j}^{\\left(L\\right)}\\overset{\\mathrm{iid}}{\\sim}\\mathcal{N}\\left(0,\\Sigma_{2}^{\\left(L\\right)}\\right).$ wherewe have for $x,x^{\\prime}\\in\\mathbb{R}^{n_{0}}$ $\\begin{array}{r l}&{\\Sigma_{1}^{(1)}(x,x^{\\prime})=\\Sigma_{2}^{(1)}(x,x^{\\prime})=\\frac{\\sigma_{w}^{2}}{n_{0}}\\langle x,x^{\\prime}\\rangle+\\sigma_{b}^{2}}\\\\ &{\\Sigma_{k}^{(L)}(x,x^{\\prime})=\\sigma_{w}^{2}\\,\\mathbb{E}_{g\\sim\\mathcal{N}\\big(0,\\Sigma_{k}^{(L-1)}\\big)}\\big[\\sigma_{k}(g(x))\\,\\sigma_{k}(g(x^{\\prime}))\\big]+\\sigma_{b}^{2}\\quad f o r\\quad L\\ge2,\\;k\\in\\{1,2\\}.}\\end{array}$ (S56) ", "page_idx": 38}, {"type": "text", "text": "", "page_idx": 39}, {"type": "text", "text": "Furthermore, X(L) and Y(L) are independent if $i\\neq j$ and ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[X_{i}^{(L)}(x)\\,Y_{i}^{(L)}(x^{\\prime})\\right]=\\left\\{\\!\\!\\!\\begin{array}{l l}{\\sigma_{w}^{2}\\langle x,x^{\\prime}\\rangle+\\sigma_{b}^{2}=\\Sigma_{1}^{(1)}(x,x^{\\prime})=:\\Sigma_{1,2}^{(1)}(x,x^{\\prime})\\quad\\!f o r\\,L=1,}\\\\ {\\sigma_{w}^{2}\\,\\mathbb{E}[\\sigma_{1}(Z_{1})\\,\\sigma_{2}(Z_{2})]+\\sigma_{b}^{2}=:\\Sigma_{1,2}^{(L)}(x,x^{\\prime})\\quad\\!\\ f o r\\,L\\geq2,}\\end{array}\\ \\right.}\\\\ &{\\mathrm{~}\\quad\\cdot r e\\left(Z_{1},Z_{2}\\right)\\sim\\mathcal{N}\\left(0,\\left(\\!\\!\\begin{array}{l l}{\\Sigma_{1}^{(L-1)}(x,x)}&{\\Sigma_{1,2}^{(L-1)}(x,x^{\\prime})}\\\\ {\\Sigma_{1,2}^{(L-1)}(x,x^{\\prime})\\,\\Sigma_{2}^{(L-1)}(x^{\\prime},x^{\\prime})}\\end{array}\\!\\!\\right)\\!\\right)\\!.}\\end{array}\n$$whe ", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Proof.Wewrite $h^{(l)}(x)\\:=\\:h^{(l)}(x;\\theta)$ . We prove the theorem by induction, as in the proof of Proposition 1 of Jacot et al. [2018], but expand on the technical details. ", "page_idx": 39}, {"type": "text", "text": "$\\mathbf{L}=\\mathbf{1}$ . By definition, we have ", "page_idx": 39}, {"type": "equation", "text": "$$\nh_{1}^{(1)}(x)=h_{2}^{(1)}(x)=\\frac{\\sigma_{w}}{\\sqrt{n_{0}}}W^{(1)}x+\\sigma_{b}b^{(1)}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "This implies hat $h_{k_{1},i}^{(1)}(x;\\theta)$ and $h_{k_{2},j}^{(1)}(x^{\\prime};\\theta)$ the $i$ thand $j$ th componentof $h_{k_{1}}^{(1)}(x;\\theta)$ and $h_{k_{2}}^{(1)}(x^{\\prime};\\theta)$ respectively, are independent for any $k_{1},k_{2}~\\in~\\{1,2\\}$ \uff0c $x,x^{\\prime}\\,\\in\\,\\mathbb{R}^{n_{0}}$ and $i\\ne j$ . To prove that $(h_{1}^{(1)}(\\,\\cdot\\,;\\theta),h_{2}^{(1)}(\\,\\cdot\\,;\\theta))$ is a Gaussian process, it is thus sufficient to show that vectors of the form $\\left(\\stackrel{h_{1,i}^{(1)}}{h_{1,i}^{(1)}}(x_{1}^{\\overline{{{1}}}})\\begin{array}{l}{\\ldots}\\\\ {\\ldots}\\end{array}h_{1,i}^{(1)}(x_{n})\\right.h_{2,i}^{(1)}(x_{1})\\left.\\ldots\\right.h_{2,i}^{(1)}(x_{n})\\left.\\right)^{\\intercal}$ have a multivariate Gaussian distribution for any $x_{1},\\ldots,x_{n}\\in(z_{i})_{i=1}^{\\infty}.$ It holds ", "page_idx": 39}, {"type": "equation", "text": "$$\nh_{1,i}^{(1)}=\\frac{\\sigma_{w}}{\\sqrt{n_{0}}}W_{i\\,\\cdot}^{(1)}x+\\sigma_{b}b_{i}^{(1)}=(\\sigma_{w}x^{\\mathsf{T}}/n_{0}\\quad\\sigma_{b})\\left(W_{i\\,\\cdot}^{(1)^{\\mathsf{T}}}\\right),\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "and therefore ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Big(h_{1,i}^{(1)}(x_{1})\\quad\\ldots\\quad h_{1,i}^{(1)}(x_{n})\\quad h_{2,i}^{(1)}(x_{1})\\quad\\ldots\\quad h_{2,i}^{(1)}(x_{n})\\Big)^{\\mathsf{T}}=\\left(\\sigma_{w}\\chi^{\\mathsf{T}}/n_{0}\\quad\\sigma_{b}\\mathbb{1}_{d}\\right)\\left(W_{i}^{(1)\\,\\mathsf{T}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "with $\\mathbb{1}_{d}$ a column vector of ones with length $d$ Now since ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\left(W_{i\\,\\cdot\\,}^{(1)\\,\\mathsf{T}}\\right)\\sim\\mathcal{N}(0,\\mathrm{I}_{n_{0}+1}),\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "it holds ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(\\,h_{1,i}^{(1)}(x_{1})\\,\\ldots\\,h_{1,i}^{(1)}(x_{n})\\,\\,h_{2,i}^{(1)}(x_{1})\\,\\ldots\\,\\,h_{2,i}^{(1)}(x_{n})\\,\\right)^{\\top}\\sim{\\mathcal{N}}\\left(0,\\left(\\!\\,\\sigma_{w}\\chi^{\\top}/n_{0}\\,\\,\\sigma_{b}\\mathbb{1}_{d}\\right)\\!\\,\\right)\\left(\\!\\,\\sigma_{w}\\chi^{\\top}/n_{0}\\,\\,\\sigma_{b}\\mathbb{1}_{d}\\right)^{\\top}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Therefore, the vector has a multivariate Gaussian distribution with the covariances required for Equation (S56) and the first case of Equations (S58). ", "page_idx": 39}, {"type": "text", "text": "${\\bf L}\\rightarrow{\\bf L}+{\\bf1}$ . We assume that the convergence holds for depth $L$ . This means that there exists some $r\\in\\mathcal{R}_{L}$ such that, for given constant width $n_{0}$ , any width $n_{L}$ , and widths $n_{l}=r_{l}(m)$ $1\\leq l<L$ it holds ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Big(h_{1}^{(L)}(\\cdot),h_{2}^{(L)}(\\cdot)\\Big)\\xrightarrow[m\\rightarrow\\infty]{\\mathcal{D}}\\Big(X_{j}^{(L)},X_{j}^{(L)}\\Big)_{j=1,\\dots,n_{L}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "To be precise, there is no such $r$ in the case $L=1\\to L+1$ . However, this only makes the proof simpler and one can still follow the same steps as for $L\\ge2$ ", "page_idx": 39}, {"type": "text", "text": "By the continuous mapping theorem [Billingsley, 1999, Theorem 2.7] it holds ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(\\sigma_{1}\\left(h_{1}^{(L)}(\\cdot)\\right),\\sigma_{2}\\left(h_{2}^{(L)}(\\cdot)\\right)\\right)\\xrightarrow[m\\to\\infty]{\\mathcal{D}}\\left(\\sigma_{1}\\left(X_{j}^{(L)}\\right),\\sigma_{2}\\left(Y_{j}^{(L)}\\right)\\right)_{j=1,\\dots,n_{L}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "The theorem is applicable despite the finitely many jump points, since $(X^{(L)},Y^{(L)})$ assumes the values of the jump points with zero probability. ", "page_idx": 40}, {"type": "text", "text": "We now need to find an increasing width function $r_{L}\\colon\\mathbb{N}\\rightarrow\\mathbb{N}$ such that, if we additionally set $n_{L}=r_{L}(m)$ , it holds for any fixed $n_{L+1}$ ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\Big(h_{1}^{(L+1)}(\\cdot),h_{2}^{(L+1)}(\\cdot)\\Big)\\xrightarrow[m\\rightarrow\\infty]{\\mathcal{D}}\\Big(X_{j}^{(L)},X_{j}^{(L)}\\Big)_{j=1,\\dots,n_{L+1}}\\,.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Note that by Remark C.5 we consider the product $\\sigma$ -algebra. Therefore, to show convergence in distribution for the whole process, it is sufficient to show convergence in distribution for the marginal distributions. By definition, we have for $k\\in\\{1,2\\}$ that ", "page_idx": 40}, {"type": "equation", "text": "$$\nh_{k}^{(L+1)}(x)=\\frac{\\sigma_{w}}{\\sqrt{n_{L}}}W^{(L+1)}\\sigma_{k}\\big(h_{k}^{(L)}(x)\\big)+\\sigma_{b}b^{(L+1)}=\\frac{\\sigma_{w}}{\\sqrt{n_{L}}}\\sum_{i=1}^{n_{L}}\\sigma_{k}\\big(h_{k,i}^{(L)}(x)\\big)W_{\\cdot\\,i}^{(L+1)}+\\sigma_{b}b^{(L)}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "The marginal vector for points $x_{1},\\ldots,x_{n}$ as before can thus be written as ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(h_{1}^{(L+1)}(x_{1})\\right)}\\\\ {h_{1}^{(L+1)}(x_{n})}\\\\ {h_{2}^{(L+1)}(x_{1})}\\\\ {\\vdots}\\\\ {h_{2}^{(L+1)}(x_{n})}\\end{array}\\right)=\\frac{\\sigma_{w}}{\\sqrt{n_{L}}}\\sum_{i=1}^{n_{L}}\\left(\\begin{array}{c}{\\sigma_{1}\\big(h_{1,i}^{(L)}(x_{1})\\big)\\mathrm{I}_{n_{L+1}}}\\\\ {\\vdots}\\\\ {\\sigma_{1}\\big(h_{1,i}^{(L)}(x_{n})\\big)\\mathrm{I}_{n_{L+1}}}\\\\ {\\sigma_{2}\\big(h_{2,i}^{(L)}(x_{1})\\big)\\mathrm{I}_{n_{L+1}}}\\\\ {\\vdots}\\\\ {\\sigma_{2}\\big(h_{2,i}^{(L)}(x_{n})\\big)\\mathrm{I}_{n_{L+1}}}\\end{array}\\right)W_{\\cdot\\,i}^{(L+1)}+\\sigma_{b}\\left(\\begin{array}{c}{b^{(L+1)}}\\\\ {\\vdots}\\\\ {b^{(L+1)}}\\\\ {b^{(L+1)}}\\\\ {\\vdots}\\\\ {b^{(L+1)}}\\end{array}\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "With the same arguments as before and using the continuous mapping theorem in combination with Lemma E.2 (iv) and the independence of $W^{(L+1)}$ , it holds ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\left[\\left(\\begin{array}{c}{\\sigma_{1}\\left(h_{1,i}^{\\left(L\\right)}\\left(x_{1}\\right)\\right)\\Gamma_{n_{L+1}}}\\\\ {\\vdots}\\\\ {\\sigma_{1}\\left(h_{1,i}^{\\left(L\\right)}\\left(x_{n}\\right)\\right)\\Gamma_{n_{L+1}}}\\\\ {\\sigma_{2}\\left(h_{2,i}^{\\left(L\\right)}\\left(x_{1}\\right)\\right)\\Gamma_{n_{L+1}}}\\\\ {\\vdots}\\\\ {\\sigma_{2}\\left(h_{2,i}^{\\left(L\\right)}\\left(x_{n}\\right)\\right)\\Gamma_{n_{L+1}}}\\end{array}\\right)_{i=1}^{n_{L}}\\right]_{\\left(\\begin{array}{c}{\\frac{D}{D}}\\\\ {\\frac{D}{m\\rightarrow\\infty}}\\\\ {\\vdots}\\\\ {\\frac{D}{m\\rightarrow\\infty}}\\\\ {\\vdots}\\\\ {\\sigma_{2}\\left(Y_{i}^{\\left(L\\right)}\\left(x_{n}\\right)\\right)\\Gamma_{n_{L+1}}}\\end{array}\\right)_{i=1}^{n_{L}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!=\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Since $(X_{i},Y_{i})$ and $(X_{j},Y_{j})$ are independent for $i\\neq j$ , the conditions of Lemma E.1 are satisfied. Now, there exists $k\\colon\\mathbb{N}\\rightarrow\\mathbb{N}$ such that $k(m)\\rightarrow\\infty$ monotonically as $m\\rightarrow\\infty$ and, when setting $n_{L}:=r_{L}(m):=k(m)$ , it holds ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(\\!\\!\\begin{array}{c}{h_{1}^{(L+1)}(x_{1})}\\\\ {\\vdots}\\\\ {h_{1}^{(L+1)}(x_{n})}\\\\ {h_{2}^{(L+1)}(x_{1})}\\\\ {\\vdots}\\\\ {h_{2}^{(L+1)}(x_{n})}\\end{array}\\!\\!\\right)\\xrightarrow{\\mathcal{D}}\\sigma_{w}\\left(\\!\\!\\begin{array}{c}{R_{x_{1}}^{(L+1)}}\\\\ {\\vdots}\\\\ {R_{x_{n}}^{(L+1)}}\\\\ {S_{x_{1}}^{(L+1)}}\\\\ {\\vdots}\\\\ {S_{x_{n}}^{(L+1)}}\\end{array}\\!\\!\\right)+\\sigma_{b}\\left(\\!\\!\\begin{array}{c}{b^{(L+1)}}\\\\ {\\vdots}\\\\ {b^{(L+1)}}\\\\ {b^{(L+1)}}\\\\ {\\vdots}\\\\ {b^{(L+1)}}\\end{array}\\!\\!\\right)\\stackrel{(\\star)}{=}\\left(\\!\\!\\begin{array}{c}{{X^{(L+1)}(x_{1})}}\\\\ {\\vdots}\\\\ {{X^{(L+1)}(x_{n})}}\\\\ {{Y^{(L+1)}(x_{1})}}\\\\ {\\vdots}\\\\ {{Y^{(L+1)}(x_{n})}}\\end{array}\\!\\!\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "for a Gaussian random variable $\\big(R_{x_{1}}^{(L+1)}\\,.\\,.\\,.\\,,R_{x_{n}}^{(L+1)},S_{x_{1}}^{(L+1)},.\\,.\\,.\\,,S_{x_{n}}^{(L+1)}\\big)\\in\\mathbb{R}^{2\\cdot n_{L+1}\\cdot d}$ with ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\binom{R_{x_{1}}^{(L+1)}}{\\vdots}}\\sim\\left(\\begin{array}{c}{\\sigma_{1}\\big(X_{1}^{(L)}(x_{1})\\big)\\mathbf{I}_{n_{L+1}}}\\\\ {\\vdots}\\\\ {\\sigma_{1}\\big(X_{1}^{(L)}(x_{n})\\big)\\mathbf{I}_{n_{L+1}}}\\\\ {\\sigma_{2}\\big(Y_{1}^{(L)}(x_{1})\\big)\\mathbf{I}_{n_{L+1}}}\\\\ {\\vdots}\\\\ {\\sigma_{2}\\big(Y_{1}^{(L)}(x_{n})\\big)\\mathbf{I}_{n_{L+1}}}\\end{array}\\right)W_{\\cdot1}^{(L+1)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Before considering the covariances, we want to comment on $r_{L}$ . First, note that this sequence may not initially be strictly increasing, but this can be circumvented by considering a strictly increasing subsequence. Second, $r_{L}$ could theoretically depend on $\\mathcal{X}$ . However, this can be resolved by not evaluating the pair $(h_{1}^{(L)},h_{2}^{(L)})$ at certain data points, but doing the same calculation as above for $(h_{1}^{(L)},h_{2}^{(L)})$ The stating pin fo thsis Equation S59) To apply Lema E.1, ote aditioally that $(h_{1}^{(L)},h_{2}^{(L)})\\,\\in\\,\\otimes_{i=1}^{\\infty}\\mathbb{R}^{2\\cdot n_{L}}$ whichisaraHsaccaweareniing a countable input set. Above, we worked with the marginal distribution because this makes the following calculation of the covariances easier. Finally, the choice of $r_{L}$ should be independent of nL+1. This follows from the independence of W(L+1) a $W_{j^{\\prime}i}^{(L+1)}$ $j\\neq j^{\\prime}$ \uff1a ", "page_idx": 41}, {"type": "text", "text": "To verify Equation $(\\star)$ , we check the covariances of the random vector. They are given by ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\normalfont~Cov}\\left[R_{x_{i}}^{(L+1)},R_{x_{j}}^{(L+1)}\\right]=\\mathbb{E}\\left[\\sigma_{1}\\left(X_{1}^{(L)}(x_{i})\\right)\\,W_{\\cdot\\,1}^{(L+1)}\\,\\left(W_{\\cdot\\,1}^{(L+1)}\\right)^{\\top}\\,\\sigma_{1}\\left(X_{1}^{(L)}(x_{j})\\right)\\right]}\\\\ &{=\\!\\mathbb{E}\\left[\\sigma_{1}\\left(X_{1}^{(L)}(x_{i})\\right)\\,\\mathbb{E}\\left[W_{\\cdot\\,1}^{(L+1)}\\,\\left(W_{\\cdot\\,1}^{(L+1)}\\right)^{\\top}\\Big|\\,X_{1}^{(L)}(x_{i}),X_{1}^{(L)}(x_{j})\\right]\\,\\sigma_{1}\\left(X_{1}^{(L)}(x_{j})\\right)\\right]}\\\\ &{=\\!\\mathbb{E}\\left[\\sigma_{1}(X_{1}^{(L)}(x_{i}))\\,\\mathrm{I}_{n_{L+1}\\,\\sigma_{1}}\\left(X_{1}^{(L)}(x_{j})\\right)\\right]=\\mathbb{E}\\left[\\sigma_{1}\\left(X_{1}^{(L)}(x_{i})\\right)\\,\\sigma_{1}\\left(X_{1}^{(L)}(x_{j})\\right)\\right]\\mathrm{I}_{n_{L+1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Similarly, we get that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathrm{Cov}\\left[S_{x_{i}}^{(L+1)},S_{x_{j}}^{(L+1)}\\right]=\\mathbb{E}\\left[\\sigma_{2}\\left(Y_{1}^{(L)}(x_{i})\\right)\\,\\sigma_{2}\\left(Y_{1}^{(L)}(x_{j})\\right)\\right]\\mathrm{I}_{n_{L+1}},\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "and together this implies using the independence of biases $b^{(L+1)}$ and weight matrices $W^{(L+1)}$ ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathrm{Cov}\\left[\\sigma_{w}R_{x_{i},k}^{(L+1)}+\\sigma_{b}b_{k}^{(L+1)},\\sigma_{w}R_{x_{j},l}^{(L+1)}+\\sigma_{b}b_{l}^{(L+1)}\\right]}\\\\ &{=\\!\\delta_{k l}\\left(\\sigma_{w}^{2}\\,\\mathbb{E}\\left[\\sigma_{1}\\left(X_{1}^{(L)}(x_{i})\\right)\\,\\sigma_{1}\\left(X_{1}^{(L)}(x_{j})\\right)\\right]+\\sigma_{b}^{2}\\right)=\\mathrm{Cov}\\left[X_{k}^{(L+1)}(x_{i}),X_{l}^{(L+1)}(x_{j})\\right],}\\\\ &{\\quad\\mathrm{Cov}\\left[\\sigma_{w}S_{x_{i},k}^{(L+1)}+\\sigma_{b}b_{k}^{(L+1)},\\sigma_{w}S_{x_{j},l}^{(L+1)}+\\sigma_{b}b_{l}^{(L+1)}\\right]}\\\\ &{=\\!\\delta_{k l}\\left(\\sigma_{w}^{2}\\,\\mathbb{E}\\left[\\sigma_{2}\\left(Y_{1}^{(L)}(x_{i})\\right)\\,\\sigma_{2}\\left(Y_{1}^{(L)}(x_{j})\\right)\\right]+\\sigma_{b}^{2}\\right)=\\mathrm{Cov}\\left[Y_{k}^{(L+1)}(x_{i}),Y_{l}^{(L+1)}(x_{j})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "We therefore proved Equation (S57). For the second case of (S58), we see that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathrm{Cov}\\left[R_{x_{i}}^{(L+1)},S_{x_{j}}^{(L+1)}\\right]=\\mathbb{E}\\left[\\sigma_{1}\\left(X_{1}^{(L)}(x_{i})\\right)\\;W_{\\cdot1}^{(L+1)}\\;\\left(W_{\\cdot1}^{(L+1)}\\right)^{\\top}\\;\\sigma_{2}\\left(Y_{1}^{(L)}(x_{j})\\right)\\right]}\\\\ &{=\\!\\mathbb{E}\\left[\\sigma_{1}\\left(X_{1}^{(L)}(x_{i})\\right)\\;\\mathbb{E}\\left[W_{\\cdot1}^{(L+1)}\\;\\left(W_{\\cdot1}^{(L+1)}\\right)^{\\top}\\Big|\\;X_{1}^{(L)}(x_{i}),Y_{1}^{(L)}(x_{j})\\right]\\;\\sigma_{2}\\left(Y_{1}^{(L)}(x_{j})\\right)\\right]}\\\\ &{=\\!\\mathbb{E}\\left[\\sigma_{1}\\left(X_{1}^{(L)}(x_{i})\\right)\\;\\sigma_{2}\\left(Y_{1}^{(L)}(x_{j})\\right)\\right]\\mathrm{I}_{n_{L+1}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "with, by induction hypothesis, ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\left(X_{1}^{(L)}(x_{i}),Y_{1}^{(L)}(x_{j})\\right)\\sim{\\mathcal{N}}\\left(0,\\left(\\Sigma_{1,2}^{(L)}(x_{i},x_{i})\\quad\\Sigma_{1,2}^{(L)}(x_{i},x_{j})\\right)\\right).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "This finished the proof, since it now holds ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathrm{Cov}\\left[\\sigma_{w}R_{x_{i},k}^{(L+1)}+\\sigma_{b}b_{k}^{(L+1)},\\sigma_{w}S_{x_{j},l}^{(L+1)}+\\sigma_{b}b_{l}^{(L+1)}\\right]}\\\\ &{=\\!\\delta_{k l}\\left(\\sigma_{w}^{2}\\,\\mathbb{E}\\left[\\sigma_{1}\\left(X_{1}^{(L)}(x_{i})\\right)\\,\\sigma_{2}\\left(Y_{1}^{(L)}(x_{j})\\right)\\right]+\\sigma_{b}^{2}\\right)=\\mathrm{Cov}\\left[X_{k}^{(L+1)}(x_{i}),Y_{l}^{(L+1)}(x_{j})\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Remark E.2. In the preceding proof, we checked the marginal distributions of arbitrary size in order to give a complete proof of the convergence to a Gaussian process. However, the covariances can be derivedby considering only a pair of data points $(x_{1},x_{2})$ ,which drastically simplifies thenotation. Also, we only need the distributions of pairs for the next theorems. ", "page_idx": 41}, {"type": "text", "text": "Theorem E.4 (Generalized version of Theorem 1 by Jacot et al. [2018]). For activation functions $\\sigma_{1}$ $\\sigma_{2}$ and $s o$ -calledsurrogatederivatives $\\tilde{\\sigma}_{1}$ $\\tilde{\\sigma}_{2}$ such that $\\sigma_{1},\\sigma_{2},\\tilde{\\sigma}_{1}$ ,and $\\tilde{\\sigma}_{2}$ are continuous except for finitely many jump pointswith property(S5o), let $f_{1}(\\,\\cdot\\,;\\theta)$ and $f_{2}(\\,\\cdot\\,;\\theta)$ benetworkfunctionswith hidden layers $h_{1}^{(l)}(\\,\\cdot\\,;\\theta)$ $h_{2}^{(l)}(\\,\\cdot\\,;\\theta)$ \uff0c $1\\leq l\\leq L,$ respectively as in Defnition $C.I$ with shared weights $\\theta$ .Denote the empirical generalized neural tangent kernel ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\hat{I}^{(L)}(x,x^{\\prime})=J^{(L),\\sigma_{1},\\tilde{\\sigma}_{1}}(x;\\theta)\\,J^{(L),\\sigma_{2},\\tilde{\\sigma}_{2}}(x^{\\prime};\\theta)^{\\intercal}\\,\\,\\,\\,\\,f o r\\,\\,x,x^{\\prime}\\in\\mathbb{R}^{n_{0}},\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "as in Definition C.5. Then, for any $x,x^{\\prime}\\in\\mathbb{R}^{n_{0}}$ and $1\\leq i,j\\leq n_{L}$ ,itholds ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\hat{I}_{i j}^{(L)}(x,x^{\\prime})\\stackrel{\\mathcal{P}}{\\longrightarrow}\\delta_{i j}I^{(L)}(x,x^{\\prime}),\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "as $n_{1},\\ldots,n_{L-1}\\to\\infty$ weakly. We call $I^{(L)}$ the analytic generalized neural tangent kernel, which is recursively given by ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I^{(1)}(x,x^{\\prime})=\\Sigma_{1,2}^{(1)}(x,x^{\\prime})}\\\\ &{I^{(L)}(x,x^{\\prime})=\\Sigma_{1,2}^{(L)}(x,x^{\\prime})+I^{(L-1)}(x,x^{\\prime})\\cdot\\tilde{\\Sigma}_{1,2}^{(L)}(x,x^{\\prime})\\quad f o r\\;L\\geq2,}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "with () as in Theorem $E.3$ and ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ \\ \\ \\ \\ \\ \\ \\ \\tilde{\\Sigma}_{1,2}^{(L)}(x,x^{\\prime})=\\sigma_{w}^{2}\\,\\mathbb{E}[\\tilde{\\sigma}_{1}(Z_{1})\\,\\tilde{\\sigma}_{2}(Z_{2})]\\quad f o r\\,L\\geq2,}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ }\\\\ {(Z_{1},Z_{2})\\sim{\\cal N}\\left(0,\\bigg(\\Sigma_{1,2}^{(L-1)}(x,x),\\Sigma_{1,2}^{(L-1)}(x,x^{\\prime})\\bigg)\\right)\\!.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Proof. We prove the theorem by induction over $L$ as in the proof of Theorem 1 by Jacot et al. [2018]. Wedenote $\\bar{J}^{(L),k}(z)=J^{(L),\\sigma_{k}^{\\cdot},\\tilde{\\sigma}_{k}}(z;\\theta)$ for $k\\in\\{1,2\\},z\\in\\mathbb{R}^{n_{0}}$ ", "page_idx": 42}, {"type": "text", "text": "$\\mathbf{L}=\\mathbf{1}$ .For $1\\leq i,j\\leq n_{1}$ it holds ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\hat{I}_{i j}^{(1)}(x,x^{\\prime})=J_{i\\cdot}^{(1),1}(x)\\,J_{j\\cdot}^{(1),2}(x^{\\prime})^{\\top}=\\sum_{\\theta^{\\prime}\\in\\theta^{(1)}}J_{i\\theta^{\\prime}}^{(1),1}(x)\\,J_{j\\theta^{\\prime}}^{(1),2}(x^{\\prime})\\,}}\\\\ &{}&{\\stackrel{(S12)}{=}\\sum_{1\\leq k\\leq n_{1}}\\delta_{k i}\\frac{\\sigma_{w}}{\\sqrt{n_{0}}}x_{l}\\,\\delta_{k j}\\frac{\\sigma_{w}}{\\sqrt{n_{0}}}x_{l}^{\\prime}+\\sum_{1\\leq k\\leq n_{1}}\\delta_{k i}\\sigma_{b}\\,\\delta_{k j}\\sigma_{b}\\,}\\\\ &{}&{=\\frac{\\sigma_{w}^{2}}{n_{0}}\\delta_{i j}\\sum_{1\\leq k\\leq n_{0}}x_{l}x_{l}^{\\prime}+\\sigma_{b}^{2}\\delta_{i j}=\\delta_{i j}\\left(\\frac{\\sigma_{w}^{2}}{n_{0}}\\langle x,x^{\\prime}\\rangle+\\sigma_{b}^{2}\\right)=\\delta_{i j}I^{(1)}(x,x^{\\prime}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "This proves Equation (S60) ", "page_idx": 42}, {"type": "text", "text": "${\\bf L}\\rightarrow{\\bf L}+{\\bf1}$ . Now we assume that the statement is true for $L$ and need to prove it for $L+1$ . Instead of considering an explicit $r\\in\\mathcal{R}_{L}$ as in the proof of Theorem E.3 and taking $m\\rightarrow\\infty$ , we will just write $\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!n_{1},\\ldots,n_{L-1}\\rightarrow\\infty$ weakly\". ", "page_idx": 42}, {"type": "text", "text": "In the induction step, we would like to use Theorem E.3. However, it is not obvious that this is possible. In the setting of Definition C.7, let $S_{1}$ and $S_{2}$ be two statements that hold weakly. In our setting, these are the induction hypothesis and Theorem E.3 for depth $L$ . Then there exist $s,t\\in\\mathcal{R}_{L}$ such that $S_{1}(s)$ and $S_{2}(t)$ are true. It is not clear that there exists some $r\\in\\mathcal{R}_{L}$ such that $S_{1}(r)$ and $S_{2}(r)$ are true. It would be natural to define $r$ by $r_{l}(m):=\\operatorname*{max}\\{s_{l}(m),t_{l}(m)\\}$ for all $1\\leq l<L$ and $m\\in\\mathbb{N}$ , but it is still unclear that this implies $S_{1}(r)$ and $S_{2}(r)$ . Instead, one can consider the combined statement \u201c ${\\mathcal S}_{1}$ and ${{S_{2}}^{\\,,}}$ as a new statement $\\boldsymbol{S}$ . In our case, for any depth $L$ , we would need to find a $r\\in\\mathcal{R}_{L}$ such that the statements in Theorem E.3 and Theorem E.4 are both true, which can be done. Since we used the first part of Lemma E.1 to prove Theorem E.3 and will use the second part of Lemma E.1 for this proof, we would have to define $r_{L}(m):=\\operatorname*{min}\\{k(m),k^{\\prime}(m)\\}$ . For simplicity, we will assume that the $r\\in\\mathcal{R}_{L}$ we get by the induction hypothesis also gives us the convergence statement of Theorem E.3. ", "page_idx": 42}, {"type": "text", "text": "Using the definition of the generalized NTK and the quasi-Jacobian matrices, we obtain for $1\\leq$ $i,j\\le n_{L+1}$ ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I_{i j}^{(l+1)}(x,x^{\\prime})=J_{i\\cdot}^{(l+1),1}(x)\\,J_{j\\cdot}^{(l+1),2}(x^{\\prime})=\\displaystyle\\sum_{\\theta\\:\\in\\theta^{(1)}\\:l^{(1)}}J_{i\\cdot}^{(l),1}(x)\\,J_{j\\cdot}^{(l),2}(x^{\\prime})}\\\\ &{=\\displaystyle\\sum_{1\\le k\\le n_{L}\\:l+1}\\delta_{k\\cdot}\\frac{\\sigma_{w}}{\\sqrt{n}\\pi}\\sigma_{1}\\left(h_{1,l}^{(l)}(x)\\right)\\delta_{k\\cdot}\\frac{\\sigma_{w}}{\\sqrt{n}\\pi}\\sigma_{2}\\left(h_{2,l}^{(l)}(x^{\\prime})\\right)+\\displaystyle\\sum_{1\\le k\\le n_{L}\\:l+1}\\delta_{k\\cdot}\\sigma_{b}\\,\\delta_{k\\cdot}\\sigma_{b}}\\\\ &{\\phantom{=\\;}\\displaystyle\\sum_{\\theta\\:\\in\\theta^{(1)}\\:L}\\frac{\\sigma_{w}}{n_{L}}\\,\\bigg[\\sum_{m=1}^{n_{L}}W_{i,m}^{(L+1)}\\tilde{\\sigma}_{1}\\left(h_{1,m}^{(L)}(x)\\right)J_{m\\cdot}^{(L),1}(x)\\biggm]\\left[\\sum_{r=1}^{n_{L}}W_{i,r}^{(L+1)}\\tilde{\\sigma}_{2}\\left(h_{2,r}^{(L)}(x^{\\prime})\\right)J_{r\\cdot\\theta^{\\prime}}^{(L),2}(x^{\\prime})\\right]}\\\\ &{\\:\\:\\sigma_{\\theta\\:\\in\\theta^{(1)}\\:L}\\frac{\\sigma_{w}}{n_{L}}\\left(\\sum_{m=1}^{n_{L}}\\Big(h_{1,l}^{(L)}(x)\\Big)\\,\\sigma_{2}\\left(h_{2,l}^{(L)}(x^{\\prime})\\right)+\\sigma_{b}^{2}\\right)}\\\\ &{+\\frac{\\sigma_{w}^{2}}{n_{L}}\\left(\\frac{\\sigma_{w}^{2}}{n_{L}}\\sum_{l=1}^{n_{L}}W_{j,r}^{(L+1)}W_{j,r}^{(L+1)}\\right)\\tilde{\\sigma}_{2}\\left(h_{1,m}^{(L)}(x)\\right)\\tilde{\\sigma}_{2}\\left(h_{2,r}^{(L)}(x^{\\prime}) \n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "We want to apply the second part of Lemma E.1. We will consider the terms (S62) and (S63) separately. First note that ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\left(\\sigma_{1}\\left(h_{1,l}^{(L)}(x)\\right),\\sigma_{2}\\left(h_{2,l}^{(L)}(x^{\\prime})\\right)\\right)\\xrightarrow{\\mathcal{D}}\\left(\\sigma_{1}\\left(X_{l}^{(L)}(x)\\right),\\sigma_{2}\\left(Y_{l}^{(L)}(x^{\\prime})\\right)\\right),\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "as $n_{1},\\ldots,n_{L-1}\\to\\infty$ weakly by Theorem E.3 and the continuous mapping theorem with ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\left(X_{l}^{(L)}(x),Y_{l}^{(L)}(x^{\\prime})\\right)\\stackrel{\\mathrm{iid}}{\\sim}{\\mathcal{N}}\\left(0,\\left(\\Sigma_{1,1}^{(L)}(x,x)\\quad\\Sigma_{1,2}^{(L)}(x,x^{\\prime})\\right)\\right).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Again, we used that the values of the jump points are assumed with zero probability. Thus, again by the continuous mapping theorem and by the second part of Lemma E.1, it holds ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\frac{\\sigma_{w}^{2}}{n_{L}}\\sum_{l=1}^{n_{L}}\\sigma_{1}\\left(h_{1,l}^{(L)}(x)\\right)\\sigma_{2}\\left(h_{2,l}^{(L)}(x^{\\prime})\\right)+\\sigma_{b}^{2}\\xrightarrow[n_{1,\\xrightarrow[\\mathrm{weakly}]{N}}]{\\mathcal{D}}\\left[\\sigma_{1}\\left(X_{1}^{(L)}(x)\\right)\\sigma_{2}\\left(Y_{1}^{(L)}(x^{\\prime})\\right)\\right]+\\sigma_{b}^{2}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Here, as in the proof of Theorem E.3, $n_{L}\\rightarrow\\infty$ isgivenby $n_{L}:=r_{L}(m):=k^{\\prime}(m)$ ,which is in turn is given by Lemma E.1. Note also that the limit is a constant, which implies convergence in probability according to Lemma E.2 (ii). In conclusion, we obtain ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\delta_{i j}\\left(\\frac{\\sigma_{w}^{2}}{n_{L}}\\sum_{l=1}^{n_{L}}\\sigma_{1}\\left(h_{1,l}^{(L)}(x)\\right)\\sigma_{2}\\left(h_{2,l}^{(L)}(x^{\\prime})\\right)+\\sigma_{b}^{2}\\right)\\frac{\\mathcal{P}}{n_{1,\\ldots,n_{L}\\rightarrow\\infty}}\\,\\delta_{i j}\\,\\Sigma_{1,2}^{(L+1)}(x,x^{\\prime}).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "For term (S63), we can apply the induction hypothesis to obtain ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\sum_{\\boldsymbol{\\theta}^{\\prime}\\in\\boldsymbol{\\theta}^{(1:\\ L)}}J_{\\boldsymbol{\\theta}^{\\prime},\\boldsymbol{m}}^{(L),1}(x)J_{\\boldsymbol{\\theta}^{\\prime},\\boldsymbol{r}}^{(L),2}(y)=\\hat{I}_{m,r}^{(L)}(x,y)\\xrightarrow[n_{1},...,n_{L-1\\atop\\mathrm{weakly}}]{\\mathcal{P}}\\delta_{m r}I^{(L)}(x,y).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Also, we can again use the convergence given by (S64), but with $\\sigma_{1},\\sigma_{2}$ replaced by $\\tilde{\\sigma}_{1},\\tilde{\\sigma}_{2}$ respectively. This gives using Lemma E.2 (iv) ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\tilde{\\sigma}_{1}\\left(h_{1}^{(L)}(\\cdot)\\right),\\tilde{\\sigma}_{1}\\left(h_{2}^{(L)}(\\cdot)\\right),W^{(L+1)}\\right)\\xrightarrow[n_{1},\\dots,n_{L-1}]{\\mathcal{D}}\\left(\\tilde{\\sigma}_{1}\\left(X^{(L)}\\right),\\tilde{\\sigma}_{1}\\left(Y^{(L)}\\right),W^{(L+1)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Together with (S66) and Lemma E.2 (i) this implies ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\bigg(\\tilde{\\sigma}_{1}\\left(h_{1}^{(L)}(\\cdot)\\right),\\tilde{\\sigma}_{1}\\left(h_{2}^{(L)}(\\cdot)\\right),W^{(L+1)},\\hat{I}_{m r}^{(L)}(x,x^{\\prime})\\bigg)}\\\\ &{}&{\\xrightarrow[n_{1},\\dots,n_{L-1}]{\\mathcal{D}}\\bigg(\\tilde{\\sigma}_{1}\\left(X^{(L)}\\right),\\tilde{\\sigma}_{1}\\left(Y^{(L)}\\right),W^{(L+1)},\\delta_{m r}I^{(L)}(x,x^{\\prime})\\bigg)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "For the summands in (S63) we therefore have by the continuous mapping theorem ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{r=1}^{n_{L}}W_{i,m}^{(L+1)}W_{j,r}^{(L+1)}\\tilde{\\sigma}_{1}\\left(h_{1,m}^{(L)}(x)\\right)\\tilde{\\sigma}_{2}\\left(h_{2,r}^{(L)}(x^{\\prime})\\right)\\cdot\\sum_{\\theta^{\\prime}\\in\\theta^{(1)}:L}J_{\\theta^{\\prime},m}^{(L),1}(x)J_{\\theta^{\\prime},r}^{(L),2}(x^{\\prime})}}\\\\ &{}&{\\frac{\\mathcal{D}}{n_{1},\\dots,n_{L-1}}\\sum_{r=1}^{n_{L}}W_{i,m}^{(L+1)}W_{j,r}^{(L+1)}\\tilde{\\sigma}_{1}\\left(X_{m}^{(L)}(x)\\right)\\tilde{\\sigma}_{2}\\left(Y_{r}^{(L)}(x^{\\prime})\\right)\\cdot\\delta_{m r}I^{(L)}(x,x^{\\prime})}\\\\ &{}&{\\frac{\\mathcal{W}}{\\mathrm{weakly}}=\\!W_{i,m}^{(L+1)}W_{j,m}^{(L+1)}\\tilde{\\sigma}_{1}\\left(X_{m}^{(L)}(x)\\right)\\tilde{\\sigma}_{2}\\left(Y_{m}^{(L)}(x^{\\prime})\\right)\\cdot I^{(L)}(x,x^{\\prime})}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "These terms are independent for different $1\\leq m\\leq n_{L}$ . Therefore, the second part of Lemma E.1 can be applied as before. This yields ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{\\sigma_{w}^{2}}{n_{L}}\\sum_{r=1}^{n_{L}}W_{i,m}^{(L+1)}W_{j,r}^{(L+1)}\\tilde{\\sigma}_{1}\\left(h_{1,m}^{(L)}(x)\\right)\\tilde{\\sigma}_{2}\\left(h_{2,r}^{(L)}(x^{\\prime})\\right)\\cdot\\sum_{\\theta^{\\prime}\\in\\theta^{(1:L)}}J_{\\theta^{\\prime},m}^{(L),1}(x)J_{\\theta^{\\prime},r}^{(L),2}(x^{\\prime})}}\\\\ {\\xrightarrow[n_{1},\\ldots,n_{L}\\to\\infty]{\\mathcal{T}}^{(L)}(x,x^{\\prime})\\cdot\\sigma_{w}^{2}\\,\\mathbb{E}\\left[W_{i,1}^{(L+1)}W_{j,1}^{(L+1)}\\tilde{\\sigma}_{1}\\left(X_{1}^{L}(x)\\right)\\tilde{\\sigma}_{2}\\left(Y_{1}^{(L)}(x^{\\prime})\\right)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "equation", "text": "$$\n=\\!I^{(L)}(x,x^{\\prime})\\cdot\\delta_{i j}\\,\\sigma_{w}^{2}\\,\\mathbb{E}\\left[\\tilde{\\sigma}_{1}\\left(X_{1}^{L}(x)\\right)\\tilde{\\sigma}_{2}\\left(Y_{1}^{(L)}(x^{\\prime})\\right)\\right]=\\delta_{i j}I^{(L)}(x,x^{\\prime})\\cdot\\tilde{\\Sigma}_{1,2}^{(L)}(x,x^{\\prime}).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Together with (S65) this yields Equation (S61) and concludes the proof. ", "page_idx": 44}, {"type": "text", "text": "The two theorems above are complemented by the convergence of the generalized NTK in the infinitewidth limit during training, which we will prove below. The theorem is a generalization of Theorem G.2 from Lee al. [2019]. There, the convergence of the NTK at iniaization as $(n\\iota)_{l=1}^{L-1}\\stackrel{\\textstyle\\propto}{\\sim}n$ assumed. More precisely, they refer to a result of Yang [2019a]. In consequence, we have to assume the same for the next theorem. The proof of Yang [2019a] could also be generalizable to our case. Alternatively, one could also try to generalize the statement of Jacot et al. [2018] on stability during training in the weak infinite-width limit. ", "page_idx": 44}, {"type": "text", "text": "Theorem E.5 (Based on Theorem G.2 from Lee et al. [2019] ). Let o be a Lipschitz continuous anddifferentiableactivationfunction.Letthederivativeoftheactivationfunction $\\dot{\\sigma}$ and a socalled surrogate derivative $\\tilde{\\sigma}$ be Lipschitz continuous and bounded. Let $f_{t}(\\,\\cdot\\,;\\theta)$ be the corresponding networkfunctionwithdepth $L$ initialized as inDefinition $C.I$ and trained with MSE loss and surrogate gradient learning, i.e., according to Equation (S49) with surrogate derivative $\\tilde{\\sigma}$ The hidden layers are denoted by $h_{l}^{(l)}(\\,\\cdot\\,;\\theta)$ for $1\\leq l<L,$ as in Definition C.1. Assume that the generalized NTK converges in probability to the analytic generalized NTK of Theorem $E.4$ as $(n_{l})_{l=1}^{L-1}\\lessapprox n_{}$ ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(J^{(L),\\sigma,\\dot{\\sigma}}\\right)\\left(J^{(L),\\sigma,\\tilde{\\sigma}}\\right)^{\\intercal}=\\hat{I}^{(L)}\\xrightarrow[n\\rightarrow\\infty]{\\mathcal{P}}I^{(L)}\\otimes\\mathrm{I}_{n_{L}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Furthermore, assume that the smallest and largest eigenvalue of the symmetrization of $I^{(L)}(\\mathcal{X},\\mathcal{X})$ ", "page_idx": 44}, {"type": "equation", "text": "$$\nS^{(L)}:=\\frac{1}{2}\\left(I^{(L)}(\\mathcal{X},\\mathcal{X})+I^{(L)}(\\mathcal{X},\\mathcal{X})^{\\top}\\right),\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "are given by $0<\\lambda_{\\mathrm{min}}\\le\\lambda_{\\mathrm{max}}<\\infty$ and that thelearningrateisgivenby $\\eta>0$ .Then, for any $\\delta>0$ there exist $R>0,N\\in\\mathbb{N}$ and $K>1$ such that for every $n\\geq N$ thefollowingholdswith probability at least $1-\\delta$ overrandominitialization ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t\\in[0,\\infty)}\\left\\|\\hat{I}^{(L)}(\\mathcal{X},\\mathcal{X})-I^{(L)}(\\mathcal{X},\\mathcal{X})\\right\\|_{F}\\leq\\frac{6K^{3}R}{\\lambda_{\\operatorname*{min}}}n^{-\\frac{1}{2}},\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where $\\|\\cdot\\|_{F}$ denotes the Frobenius norm. ", "page_idx": 44}, {"type": "text", "text": "Proof. We follow the proofs in Chapter $\\mathrm{G}$ of Lee et al. [2019]. The analogous statement is given by TheoremG2ofLeeet al. [2019]. ince weare considering theinfnite-width lmit $(n_{l})_{l=1}^{L-1}\\stackrel{-}{\\approx}n$ We will assume that the width of the hidden layers are given by $n\\in\\mathbb N$ . The more general case can easily be proved using the same steps. Recall that ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}f\\bigl(x;\\theta_{t}\\bigr)=-\\eta\\,\\hat{I}^{(L)}\\bigl(x,\\mathcal{X};\\theta_{t}\\bigr)\\nabla_{f(\\mathcal{X};\\theta_{t})}\\mathcal{L}\\bigl(f\\bigl(\\mathcal{X};\\theta_{t}\\bigr);\\mathcal{Y}\\bigr).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "The loss function is the MSE loss by assumption. This yields ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\nabla_{f(\\mathcal{X};\\theta_{t})}\\mathcal{L}\\big(f(\\mathcal{X};\\theta_{t});\\mathcal{Y}\\big)=\\nabla_{f(\\mathcal{X};\\theta_{t})}\\frac{1}{2}\\left\\|f(\\mathcal{X};\\theta_{t})-\\mathcal{Y}\\right\\|_{2}^{2}=f(\\mathcal{X};\\theta_{t})-\\mathcal{Y}=:g(\\theta_{t}).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "The change of weights over time is hence given by ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\frac{\\mathsf{d}}{\\mathsf{d}t}\\theta_{t}\\overset{(S48)}{=}-\\eta\\,J^{(L),\\sigma,\\tilde{\\sigma}}(\\mathcal{X};\\theta_{t})^{\\mathsf{T}}\\,g(\\theta_{t})\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "By Lemma E.6 below, for any $\\delta_{1}>0$ there exists a $K>0$ such that for any $C>0$ it holds with probability at least $1-\\delta_{1}$ ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\left\\|J^{(L),\\sigma,\\tilde{\\sigma}}(\\mathcal{X};\\theta_{t})^{\\mathsf{T}}\\right\\|_{F}\\leq K\\quad\\mathrm{for~all~}\\theta_{t}\\in B\\left(\\theta_{0},C\\right).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Together, this implies ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\mathrm{d}}{\\mathrm{d}t}\\left\\|{\\boldsymbol{\\theta}}_{t}-{\\boldsymbol{\\theta}}_{0}\\right\\|_{2}\\overset{(\\star)}{\\leq}\\left\\|\\frac{\\mathrm{d}}{\\mathrm{d}t}{\\boldsymbol{\\theta}}_{t}\\right\\|_{2}=\\eta\\left\\|{\\boldsymbol{J}}^{(L),\\sigma,\\tilde{\\sigma}}({\\boldsymbol{X}};{\\boldsymbol{\\theta}}_{t})^{\\sf T}{\\boldsymbol{g}}({\\boldsymbol{\\theta}}_{t})\\right\\|_{2}}\\\\ {\\displaystyle\\leq\\eta\\left\\|{\\boldsymbol{J}}^{(L),\\sigma,\\tilde{\\sigma}}({\\boldsymbol{X}};{\\boldsymbol{\\theta}}_{t})^{\\sf T}\\right\\|_{F}\\left\\|{\\boldsymbol{g}}({\\boldsymbol{\\theta}}_{t})\\right\\|_{2}\\leq\\eta\\,K\\left\\|{\\boldsymbol{g}}({\\boldsymbol{\\theta}}_{t})\\right\\|_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "for all $\\theta_{t}\\ \\in\\ B\\left(\\theta_{0},C\\right)$ with probability at least $1\\,-\\,\\delta_{1}$ . Inequality $(\\star)$ is a consequence of the Cauchy-Schwarz inequality. ", "page_idx": 45}, {"type": "text", "text": "To estimate the term $\\lVert g({\\boldsymbol{\\theta}}_{t})\\rVert_{2}$ in Inequality (S67), we use Gronwall's inequality in the differential form, e.g., Lemma 1.1.1 from Qin [2016]). First, note that as a consequence of the proof of Lemma E.6, for all $\\delta_{2}>0$ there exist $R_{0}>0$ and $n_{0}\\in\\mathbb{N}$ such that with probability at least $1-\\delta_{2}$ it holds for all $n\\geq n_{0}$ ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\|g(\\theta_{0})\\|_{2}\\leq R_{0}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "We now set $C:=3K R_{0}/\\lambda_{\\operatorname*{min}}$ . Next, observe that ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\quad\\displaystyle\\frac{\\mathrm{d}}{\\mathrm{d}t}g(\\theta_{t})=\\displaystyle\\frac{\\mathrm{d}}{\\mathrm{d}t}f(\\boldsymbol{\\mathcal{X}};\\theta_{t})\\,\\stackrel{(\\leq49)}{=}-\\eta\\,J^{(L),\\sigma,\\hat{\\sigma}}(\\boldsymbol{\\mathcal{X}};\\theta_{t})\\,J^{(L),\\sigma,\\hat{\\sigma}}(\\boldsymbol{\\mathcal{X}};\\theta_{t})^{\\top}\\,g(\\theta_{t})}\\\\ &{\\implies\\displaystyle\\frac{\\mathrm{d}}{\\mathrm{d}t}\\,\\|g(\\theta_{t})\\|_{2}^{2}=\\displaystyle\\frac{\\mathrm{d}}{\\mathrm{d}t}\\,\\langle g(\\theta_{t}),g(\\theta_{t})\\rangle=2\\left\\langle g(\\theta_{t}),\\displaystyle\\frac{\\mathrm{d}}{\\mathrm{d}t}g(\\theta_{t})\\right\\rangle}\\\\ &{\\quad\\quad=-2\\eta\\,\\left\\langle g(\\theta_{t}),J^{(L),\\sigma,\\hat{\\sigma}}(\\boldsymbol{\\mathcal{X}};\\theta_{t})\\,J^{(L),\\sigma,\\hat{\\sigma}}(\\boldsymbol{\\mathcal{X}};\\theta_{t})^{\\top}\\,g(\\theta_{t})\\right\\rangle}\\\\ &{\\quad\\quad=-2\\eta\\,\\left\\langle g(\\theta_{t}),\\hat{I}_{t}^{(L)}(\\boldsymbol{\\mathcal{X}};\\mathcal{X})\\,g(\\theta_{t})\\right\\rangle=-2\\eta\\left\\langle g(\\theta_{t}),S_{t}^{(L)}g(\\theta_{t})\\right\\rangle}\\\\ &{\\quad\\quad\\stackrel{(\\star\\star)}{\\le}-2\\eta\\left\\langle g(\\theta_{t}),\\displaystyle\\frac{1}{3}\\lambda_{\\mathrm{min}}\\,\\mathbb{I}\\,g(\\theta_{t})\\right\\rangle=-\\frac{2}{3}\\eta\\lambda_{\\mathrm{min}}\\,\\|g(\\theta_{t})\\|_{2}^{2}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Wwih $\\begin{array}{r}{S_{t}^{(L)}:=\\frac{1}{2}\\left(\\hat{I}_{t}^{(L)}(\\mathcal{X},\\mathcal{X})+\\hat{I}_{t}^{(L)}(\\mathcal{X},\\mathcal{X})^{\\top}\\right)}\\end{array}$ To show $({\\bf\\star}\\star)$ we wil provetha for any $0\\neq z\\in$ TRDnL\u00b7n ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\left\\langle z,\\left(S_{t}^{(L)}-\\frac{1}{3}\\lambda_{\\mathrm{min}}\\mathrm{I}\\right)z\\right\\rangle\\ge0.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "First, note that by assumption for all $z\\neq0$ ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\left\\langle z,\\left(S^{(L)}-\\lambda_{\\mathrm{min}}\\right]z\\right\\rangle\\ge0.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Since the generalized NTK converges at initialization in probability, we can assume that with probability at least $1-\\delta_{3}$ for any $\\delta_{3}>0$ that ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\left\\|\\hat{I}_{0}^{(L)}(\\mathcal{X},\\mathcal{X})-I^{(L)}(\\mathcal{X},\\mathcal{X})\\right\\|_{2}\\le\\frac{1}{3}\\lambda_{\\operatorname*{min}},\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "for any $n\\geq n_{1}\\in\\mathbb{N}$ . For the symmetrizations, this yields ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\left\\|S_{0}^{(L)}-S^{(L)}\\right\\|_{2}\\le\\frac{1}{2}\\left\\|\\hat{I}_{0}^{(L)}(\\mathcal{X},\\mathcal{X})-I^{(L)}(\\mathcal{X},\\mathcal{X})\\right\\|_{2}+\\frac{1}{2}\\left\\|\\hat{I}_{0}^{(L)}(\\mathcal{X},\\mathcal{X})^{\\top}-I^{(L)}(\\mathcal{X},\\mathcal{X})^{\\top}\\right\\|_{2}}}\\\\ &{}&{=\\left\\|\\hat{I}_{0}^{(L)}(\\mathcal{X},\\mathcal{X})-I^{(L)}(\\mathcal{X},\\mathcal{X})\\right\\|_{2}\\le\\frac{1}{3}\\lambda_{\\operatorname*{min}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Furthermore, for $\\theta_{t}\\in B\\left(\\theta_{0},C\\right)$ it holds that ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\hat{I}_{0}^{(L)}(\\mathcal{X},\\mathcal{X})-\\hat{I}_{t}^{(L)}(\\mathcal{X},\\mathcal{X})\\right\\|_{2}}\\\\ &{=\\left\\|J^{(L),\\sigma,\\hat{\\sigma}}(\\mathcal{X};\\theta_{0})\\,J^{(L),\\sigma,\\hat{\\sigma}}(\\mathcal{X};\\theta_{0})^{\\top}-J^{(L),\\sigma,\\hat{\\sigma}}(\\mathcal{X};\\theta_{t})\\,J^{(L),\\sigma,\\hat{\\sigma}}(\\mathcal{X};\\theta_{t})^{\\top}\\right\\|_{2}}\\\\ &{\\le\\left\\|J^{(L),\\sigma,\\hat{\\sigma}}(\\mathcal{X};\\theta_{0})-J^{(L),\\sigma,\\hat{\\sigma}}(\\mathcal{X};\\theta_{t})\\right\\|_{2}\\left\\|J^{(L),\\sigma,\\hat{\\sigma}}(\\mathcal{X};\\theta_{0})\\right\\|_{2}}\\\\ &{+\\left\\|J^{(L),\\sigma,\\hat{\\sigma}}(\\mathcal{X};\\theta_{t})\\right\\|_{2}\\left\\|J^{(L),\\sigma,\\hat{\\sigma}}(\\mathcal{X};\\theta_{0})-J^{(L),\\sigma,\\hat{\\sigma}}(\\mathcal{X};\\theta_{t})\\right\\|_{2}}\\\\ &{\\le\\frac{2(K^{\\prime})^{2}}{\\sqrt{n}}\\left\\|\\theta_{t}-\\theta_{0}\\right\\|_{2}\\le\\frac{6(K^{\\prime})^{2}K R_{0}}{\\sqrt{n}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "with probability at least $1-\\delta_{1}$ using Lemma E.6 as before. With the same calculations as for Inequality (S71), this implies for the symmetrizations that ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\left\\|S_{0}^{(L)}-S_{t}^{(L)}\\right\\|_{2}\\le\\left\\|\\hat{I}_{0}^{(L)}(\\mathcal{X},\\mathcal{X})-\\hat{I}_{t}^{(L)}(\\mathcal{X},\\mathcal{X})\\right\\|_{2}\\le\\frac{6(K^{\\prime})^{2}K R_{0}}{\\sqrt{n}}\\le\\frac{1}{3}\\lambda_{\\mathrm{min}},\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "$\\begin{array}{r}{n\\ge N:=\\operatorname*{max}\\bigg\\{m_{0},m_{1},\\bigg(\\frac{\\lambda_{\\operatorname*{min}}}{18(K^{\\prime})^{2}K R_{0}}\\bigg)^{2}\\bigg\\}.}\\end{array}$ Now Inequalities (S70), (S71) and (S73) give us ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\langle z,S_{t}^{(L)}z\\right\\rangle=\\left\\langle z,\\left(S^{(L)}+S_{0}^{(L)}-S^{(L)}+S_{t}^{(L)}-S_{0}^{(L)}\\right)z\\right\\rangle}\\\\ &{\\qquad\\qquad=\\left\\langle z,S^{(L)}z\\right\\rangle+\\left\\langle z,\\left(S_{0}^{(L)}-S^{(L)}\\right)z\\right\\rangle+\\left\\langle z,\\left(S_{t}^{(L)}-S_{0}^{(L)}\\right)z\\right\\rangle}\\\\ &{\\qquad\\qquad\\overset{(570)}{\\geq}\\lambda_{\\mathrm{min}}\\left\\Vert z\\right\\Vert^{2}-\\left\\vert\\left\\langle z,\\left(S_{0}^{(L)}-S^{(L)}\\right)z\\right\\rangle\\right\\vert-\\left\\vert\\left\\langle z,\\left(S_{t}^{(L)}-S_{0}^{(L)}\\right)z\\right\\rangle\\right\\vert}\\\\ &{\\qquad\\qquad\\overset{(571)+(573)}{\\geq}\\lambda_{\\mathrm{min}}\\left\\Vert z\\right\\Vert^{2}-\\frac{1}{3}\\lambda_{\\mathrm{min}}\\left\\Vert z\\right\\Vert^{2}-\\frac{1}{3}\\lambda_{\\mathrm{min}}\\left\\Vert z\\right\\Vert^{2}=\\frac{1}{3}\\left\\langle z,\\frac{1}{3}\\lambda_{\\mathrm{min}}\\mathrm{I}z\\right\\rangle,}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "and thus imply Inequality (S70). To summarize,it holds with probability at least $1-\\delta_{2}-\\delta_{3}$ for any $n\\geq N$ and $\\theta_{t}\\in B\\left(\\theta_{0},C\\right)$ that ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\left\\|g(\\theta_{0})\\right\\|_{2}^{2}\\\\stackrel{(S68)}{\\leq}R_{0}^{2}\\quad\\mathrm{and}\\quad\\frac{\\mathrm{d}}{\\mathrm{d}t}\\left\\|g(\\theta_{t})\\right\\|_{2}^{2}\\stackrel{(S69)}{\\leq}-\\frac{2}{3}\\eta\\lambda_{\\mathrm{min}}\\left\\|g(\\theta_{t})\\right\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Gronwall's inequality now implies ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\left\\|g(\\theta_{t})\\right\\|_{2}^{2}\\leq e^{-\\frac{2}{3}\\eta\\lambda_{\\operatorname*{min}}t}\\left\\|g(\\theta_{0})\\right\\|_{2}^{2}\\leq e^{-\\frac{2}{3}\\eta\\lambda_{\\operatorname*{min}}t}R_{0}^{2}.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "We can now return to Inequality (S67) to obtain with probability at least $1-\\delta_{1}-\\delta_{2}-\\delta_{3}$ ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\|\\theta_{t}-\\theta_{0}\\|_{2}\\stackrel{(S67)}{\\leq}\\eta K\\|g(\\theta_{t})\\|_{2}\\leq\\eta K R_{0}\\,e^{-\\frac{1}{3}\\eta\\lambda_{\\mathrm{min}}t}.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Integrating the inequality on both sides yields for all $\\theta_{t}\\in B(\\theta_{0},C)$ ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\|\\theta_{t}-\\theta_{0}\\|_{2}\\le\\frac{3K R_{0}}{\\lambda_{\\mathrm{min}}}\\left(1-e^{-\\frac13\\eta\\lambda_{\\mathrm{min}}}t\\right)<\\frac{3K R_{0}}{\\lambda_{\\mathrm{min}}}=C.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Let $t_{1}:=\\operatorname*{inf}\\left\\{t\\colon\\|\\theta_{t}-\\theta_{0}\\|_{2}<C\\right\\}$ Now, if $t_{1}<\\infty$ , it holds ", "page_idx": 46}, {"type": "equation", "text": "$$\nC=\\operatorname*{lim}_{t\\uparrow t_{1}}\\lVert\\theta_{t}-\\theta_{0}\\rVert_{2}\\stackrel{(S74)}{<}C.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "This is a contradiction, so we conclude that $t_{1}=\\infty$ . In particular, Inequality (S74) holds for all $t>0$ . We can now repeat the calculations for Inequality (S72) with the Frobenius norm to finally obtain ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\left\\|\\hat{I}_{0}^{(L)}(\\mathcal{X},\\mathcal{X})-\\hat{I}_{t}^{(L)}(\\mathcal{X},\\mathcal{X})\\right\\|_{F}\\le\\frac{2K^{2}}{\\sqrt{n}}\\left\\|\\theta_{t}-\\theta_{0}\\right\\|_{2}\\le\\frac{6K^{3}R_{0}}{\\lambda_{\\operatorname*{min}}}n^{-\\frac12}.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Therefore, if we choose $\\delta_{1}\\,=\\,\\delta_{2}\\,=\\,\\delta_{3}\\,=\\,{\\textstyle{\\frac{1}{3}}}\\delta$ , the desired inequality holds for any $n\\,\\geq\\,N$ With probability at least $1-\\delta$ \u53e3 ", "page_idx": 46}, {"type": "text", "text": "Lemma E.6 (Based on Lemma 1 and Lemma 2 from Lee et al. [2019]). In the setting of Theorem E.5, forany $\\delta>0$ thereexistsa $K>0$ suchthatforany $C>0$ withprobabilityatleast $1-\\delta$ itholds ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\left\\|J^{(L),\\sigma,\\hat{\\sigma}}(\\mathcal{X};\\theta)-J^{(L),\\sigma,\\hat{\\sigma}}(\\mathcal{X};\\tilde{\\theta})\\right\\|_{F}\\leq\\frac{K}{\\sqrt{n}}\\,\\left\\|\\theta-\\tilde{\\theta}\\right\\|_{2}\\quad a n d\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "forall $\\theta,\\tilde{\\theta}\\;\\in\\;B\\left(\\theta_{0},C\\right)$ and $\\hat{\\sigma}~\\in~\\{\\dot{\\sigma},\\tilde{\\sigma}\\}$ .Due to the equivalence of matrix norms, the same inequalities hold for thenorm $\\|\\cdot\\|_{2}$ andsomeconstant $K^{\\prime}>0$ ", "page_idx": 47}, {"type": "text", "text": "Proof.For ${\\hat{\\sigma}}={\\dot{\\sigma}}$ and a different but equivalent parameterization of the network parameters [Lee et al. 2019, Chapter F], the proof can be found in Section G.2 of Lee et al. [2019]. The surrogate derivative $\\tilde{\\sigma}$ is bounded and Lipschitz continuous. Also, $J^{(L),\\sigma,\\tilde{\\sigma}}(\\mathcal{X};\\theta)$ and $J^{(L),\\sigma,\\dot{\\sigma}}(\\mathcal{X};\\theta)=J_{\\theta}f(\\mathcal{X};\\theta)$ share the same recursive formula. Therefore, the proof, which builds on the so-called Gaussian conditioning technique [Yang, 2019a, Section E.1], should also hold for the surrogate derivative. \u53e3 ", "page_idx": 47}, {"type": "text", "text": "Remark E.3 (The analytic generalized NTK for surrogate gradient learning). Since in Theorem E.5 we consider only one activation function $\\sigma$ instead of two different ones $\\sigma_{1},\\sigma_{2}$ as inTheoremE.3 andE.4,itholds ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\Sigma_{1,2}^{(L)}=\\Sigma_{1}^{(L)}=\\Sigma_{2}^{(L)}=\\Sigma^{(L)}.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "The analytic generalized NTK in the setting of Theorem $E.4$ is thus givenby ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{c c c}{{I^{(1)}=\\Sigma^{(1)}}}&{{a n d}}&{{I^{(L+1)}=\\Sigma^{(L+1)}+I^{(L)}\\cdot\\tilde{\\Sigma}_{1,2}^{(L+1)}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "$\\tilde{\\Sigma}_{1,2}^{(L)}$ ", "page_idx": 47}, {"type": "text", "text": "Remark E.4 (Positive definiteness of the generalized NTK). In Theorem $E.5$ werequirethatthe matrix $I^{(L)}(\\mathcal{X},\\mathcal{X})$ be positive defnite. Equivalently, its symmetrization, ", "page_idx": 47}, {"type": "equation", "text": "$$\nS^{(L)}=\\frac{1}{2}\\left(I^{(L)}(\\mathcal{X},\\mathcal{X})+I^{(L)}(\\mathcal{X},\\mathcal{X})^{\\top}\\right),\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "should be positive definite. For applications this can be checked numerically. More generally, it would be interesting to know whether the symmetric kernel given by ", "page_idx": 47}, {"type": "equation", "text": "$$\nS^{(L)}(x,x^{\\prime})=\\frac{1}{2}\\left(I^{(L)}(x,x^{\\prime})+I^{(L)}(x^{\\prime},x)\\right)\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "is positive definite. One could try to approach this question inductively. However, this leads to problems in the induction step. We want to present a different ansatz, which reduces the question to a questionabout2. . We use the closed form of the analytic NTK and the symmetry of $\\Sigma^{(l)}$ for all $l\\in\\mathbb N$ toseethat ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\cal S}^{(L)}(x,x^{\\prime})=\\frac{1}{2}\\left(I^{(L)}(x,x^{\\prime})+I^{(L)}(x^{\\prime},x)\\right)}}\\\\ {{\\displaystyle=\\frac{1}{2}\\left(\\left(\\sum_{k=1}^{L}\\Sigma^{(k)}(x,x^{\\prime})\\cdot\\prod_{l=k}^{L-1}\\tilde{\\Sigma}_{1,2}^{(l+1)}(x,x^{\\prime})\\right)+\\left(\\sum_{k=1}^{L}\\Sigma^{(k)}(x^{\\prime},x)\\cdot\\prod_{l=k}^{L-1}\\tilde{\\Sigma}_{1,2}^{(l+1)}(x^{\\prime},x)\\right)\\right)}}\\\\ {{\\displaystyle=\\sum_{k=1}^{L}\\Sigma^{(k)}(x,x^{\\prime})\\cdot\\frac{1}{2}\\left(\\left(\\prod_{l=k}^{L-1}\\tilde{\\Sigma}_{1,2}^{(l+1)}(x,x^{\\prime})\\right)+\\left(\\prod_{l=k}^{L-1}\\tilde{\\Sigma}_{1,2}^{(l+1)}(x^{\\prime},x)\\right)\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "This defines a symmetric positive definite kernel if $\\Sigma^{(L)}$ is positive definite (see Jacot et al. [2018, Section A.4]for comparison) and the symmetrized kernels, ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\left(\\left(\\prod_{l=k}^{L-1}\\tilde{\\Sigma}_{1,2}^{(l+1)}(x,x^{\\prime})\\right)+\\left(\\prod_{l=k}^{L-1}\\tilde{\\Sigma}_{1,2}^{(l+1)}(x^{\\prime},x)\\right)\\right),\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "are positive semi-definite for all $k=1,\\ldots,L-1$ ", "page_idx": 47}, {"type": "text", "text": "E.2  The analytic NTK for surrogate gradient learning with sign activation function ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "We would like to proceed as in Section C.2.1 and replace the empirical generalized NTK in Equation (S49), the equation defining surrogate gradient learning, with the analytic generalized NTK obtained by Theorem E.5. Since the activation functions considered in the above section are Lipschitz continuous with bounded and Lipschitz continuous derivative, we will again approximate the sign function and its distributional derivative with the error function as in Section D. The results will not depend on the approximation of the weak derivative of the sign function. This approach can only lead to a useful result if the resulting kernel is not singular. We will check this in the following. ", "page_idx": 48}, {"type": "text", "text": "We choose activation function $\\operatorname{erf}_{m}(z)=\\operatorname{erf}(m\\cdot z)$ $m\\in\\mathbb{N}$ , with surrogate derivative $\\tilde{\\sigma}(z)$ We will also consider the special case $\\tilde{\\sigma}(z)=\\operatorname{erf}(z)$ . As discussed in Remark E.3 and with the final results of Section D.1, this immediately yields ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Sigma_{\\infty}^{(1)}(x,y):=\\underset{m\\to\\infty}{\\operatorname*{lim}}\\,\\Sigma_{m}^{(1)}(x,y)\\overset{(S36)}{=}\\frac{\\sigma_{w}^{2}}{n_{0}}\\langle x,y\\rangle+\\sigma_{b}^{2}\\quad\\mathrm{and}}\\\\ &{\\Sigma_{\\infty}^{(L+1)}(x,y):=\\underset{m\\to\\infty}{\\operatorname*{lim}}\\,\\Sigma_{m}^{(L+1)}(x,y)\\overset{(S29)}{=}\\frac{2\\sigma_{w}^{2}}{\\pi}\\arcsin\\left(\\frac{\\Sigma_{\\infty}^{(L)}(x,y)}{\\sqrt{\\Sigma_{\\infty}^{(L)}(x,x)}\\sqrt{\\Sigma_{\\infty}^{(L)}(y,y)}}\\right)+\\sigma_{b}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Here, we have to assume that $\\sigma_{b}^{2}>0$ $x,y\\neq0$ to ensure that $\\Sigma_{\\infty}^{(1)}(x,x),\\Sigma_{\\infty}^{(1)}(y,y)\\neq0$ This has already been discussed after Equation (S39). ", "page_idx": 48}, {"type": "text", "text": "E.2.1  The derivative of the error function as surrogate derivative ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Next, we want to calculate $\\begin{array}{r}{\\tilde{\\Sigma}_{1,2;\\infty}^{(L)}(x,y):=\\operatorname*{lim}_{m\\rightarrow\\infty}\\tilde{\\Sigma}_{1,2;m}^{(L)}(x,y)}\\end{array}$ We will frst consider the case $\\tilde{\\sigma}\\,=\\,\\mathrm{erf}$ , for which we can use the already established tools. In particular, we will discuss the differences to the results of Section D. ", "page_idx": 48}, {"type": "text", "text": "It holds ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\tilde{\\Sigma}_{1,2;m}^{(L)}(x,y)=\\sigma_{w}^{2}\\,\\mathbb{E}[\\operatorname{erf}_{m}(Z_{1}^{m})\\operatorname{erf}(Z_{2}^{m})]\\quad\\mathrm{for}\\;L\\geq2,\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(Z_{1}^{m},Z_{2}^{m})\\sim\\mathcal{N}\\left(0,\\left(\\frac{\\Sigma_{1;m}^{(L-1)}(x,x)~\\Sigma_{1,2;m}^{(L-1)}(x,y)}{\\Sigma_{1,2;m}^{(L-1)}(x,y)~\\Sigma_{2;m}^{(L-1)}(y,y)}\\right)\\right)}\\\\ &{\\qquad\\qquad=\\mathcal{N}\\left(0,\\left(\\frac{\\Sigma_{m}^{(L-1)}(x,x)~\\Sigma_{m}^{(L-1)}(x,y)}{\\Sigma_{m}^{(L-1)}(x,y)~\\Sigma_{m}^{(L-1)}(y,y)}\\right)\\right)=\\mathcal{N}\\left(0,\\Sigma_{m;x,y}^{(L-1)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "agaisv Z2-1 , which comes from Theorem E.3 in combination with the scaling variable $m$ of the activation' function, should not be confused with $\\Sigma_{m;x,y}^{(L-1)}$ , which is a shorthand notation for the Gram matrix \u2211(M-1)(x, y},{x,y}). ", "page_idx": 48}, {"type": "text", "text": "We denote $e_{1}=\\left({\\bf\\Pi}_{0}^{1}\\right)$ \uff0c $e_{2}=(\\mathbf{\\nabla}_{1}^{0})$ and $U=\\left({\\cal Z}_{1}^{m}\\right)$ Zz ). This yields ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial}{\\partial x_{2}}(x_{1},x_{0})=\\alpha_{2}^{2}\\frac{\\partial}{\\partial x_{1}}(x_{1}(\\bar{x}_{2})+\\alpha_{2}^{2}\\ln(1(\\bar{x}_{2}))+\\alpha_{2}^{2}\\ln(1+\\nu)\\pi)\\Biggr.}\\\\ &{\\Biggr.\\Biggr.}\\\\ &{\\left.\\mathrm{~}+\\frac{\\alpha_{2}^{2}\\pi}{\\pi}\\ln\\left(1+\\nu^{2}\\cdot2\\sqrt{x_{1}^{(1-\\nu)}\\nu_{1}}\\right)\\kappa_{1}\\left(1+2\\sqrt{x_{1}^{(1-\\nu)}\\nu_{2}}\\right)-(2m*\\nu_{1}\\Sigma_{\\alpha_{2},\\nu_{2}}^{(1-\\nu)}\\sigma_{2})^{2}\\right)^{-1}}\\\\ &{=2\\frac{\\partial\\pi}{\\partial x_{2}}\\left(1\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "using Lemma D.1 in Equation $(\\star)$ . For the penultimate equality we used Equation (S36) and Equation (S37). Compared to Equation (S41), ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\dot{\\Sigma}_{m}^{(L)}(x,y)\\xrightarrow{m\\rightarrow\\infty}\\frac{2\\sigma_{w}^{2}}{\\pi}\\left(\\Sigma_{\\infty}^{(L-1)}(x,x)\\cdot\\Sigma_{\\infty}^{(L-1)}(y,y)-\\Sigma_{\\infty}^{(L-1)}(x,y)^{2}\\right)=\\frac{2\\sigma_{w}^{2}}{\\pi}\\left|\\Sigma_{\\infty;x,y}^{(L-1)}\\right|^{-\\frac{1}{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "which in fact holds for both $L=2$ and $L\\geq3$ , an additional term appeared in Equation (S76). It always holds $\\sigma_{w}^{2}+\\sigma_{b}^{2}>0$ and it holds $\\sigma_{w}^{2}\\Vert x\\Vert^{2}/n_{0}+\\sigma_{b}^{2}>0$ if $\\sigma_{b}^{2}>0$ or $x\\neq0$ .As discussed earlier, we always assume that $x\\neq0$ is satisfied. It follows that this asymmetric NTK is not singular, since ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\Sigma}_{1,2;\\infty}^{(L)}(x,x)=\\frac{2\\sigma_{w}^{2}}{\\pi}\\left(\\left|\\Sigma_{\\infty;x,x}^{(L-1)}\\right|+\\frac{1}{2}\\Sigma_{\\infty}^{(L-1)}(x,x)\\right)^{-\\frac{1}{2}}}\\\\ &{\\qquad\\qquad\\qquad=\\frac{2\\sigma_{w}^{2}}{\\pi}\\left(0+\\frac{1}{2}\\Sigma_{\\infty}^{(L-1)}(x,x)\\right)^{-\\frac{1}{2}}=\\sqrt{2}\\,\\frac{2\\sigma_{w}^{2}}{\\pi}\\left(\\Sigma_{\\infty}^{(L-1)}(x,x)\\right)^{-\\frac{1}{2}}\\in\\mathbb{R}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Note that this is reminiscent of the constant factor depending on $x$ in the asymptotics of $\\dot{\\Sigma}_{m}^{(L)}(x,x)$ as $m\\rightarrow\\infty$ , given by (S42) and (S43), ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\dot{\\Sigma}_{m}^{(L)}(x,x)\\sim{\\frac{2\\sigma_{w}^{2}}{\\pi}}m\\left(\\Sigma_{\\infty}^{(L-1)}(x,x)\\right)^{-{\\frac{1}{2}}}.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "According to Remark E.3 it holds ", "page_idx": 49}, {"type": "equation", "text": "$$\nI_{m}^{(L)}(x,y)=\\Sigma_{m}^{(L)}(x,y)+I_{m}^{(L-1)}(x,y)\\cdot\\tilde{\\Sigma}_{1,2;m}(x,y).\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "$\\Sigma_{m}^{(L)}(x,y)$ $\\Tilde{\\Sigma}_{1,2;m}^{(L)}$ $\\Sigma_{m;x,y}^{(L-1)}$ $m\\rightarrow\\infty$ . We can write ", "page_idx": 49}, {"type": "equation", "text": "$$\nI_{\\infty}^{(L)}(x,y)=\\operatorname*{lim}_{m\\rightarrow\\infty}I_{m}^{(L)}(x,y).\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Thus, by approximating the sign function with error functions, we found that the analytic NTK for surrogate gradient learning with sign function and error function as surrogate derivative is welldefined and non-singular as a kernel on $\\mathbb{R}^{n_{0}}\\times\\mathbb{R}^{n_{0}}$ . Furthermore, comparing this NTK with the ", "page_idx": 49}, {"type": "text", "text": "NTK we derived in Section D, the tem $\\Sigma_{\\infty}^{(L)}$ does not change. This is direct consequence of not replacing the activation function with a surrogate activation function. In this sense, we inherit more properties with this approach than by replacing not only the derivative but the entire activation function including its derivative with a surrogate. Comparing Equation (S41) with Equation (S76), wesethat $\\tilde{\\Sigma}_{1,2;\\infty}^{(L)}(x,y)$ can be obtained ftom $\\dot{\\Sigma}_{\\infty}^{(L)}(x,\\bar{y})$ byadn reglazin tem depending on x, E E(-1)(x, x)/2. ", "page_idx": 50}, {"type": "text", "text": "E.2.2  General surrogate derivative ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Now we turn to the general case with a general surrogate derivative $\\tilde{\\sigma}$ . Similar to before, for $m\\in\\mathbb{N}\\cup\\{\\infty\\}$ wedenote ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(Z_{1}^{m},Z_{2}^{m})\\sim\\mathcal{N}\\left(0,\\left(\\Sigma_{1;m}^{(L-1)}(x,x)~\\Sigma_{1,2;m}^{(L-1)}(x,y)\\right)\\right)}\\\\ &{\\qquad\\qquad\\quad=\\mathcal{N}\\left(0,\\left(\\Sigma_{1,2;m}^{(L-1)}(x,y)~\\Sigma_{2;m}^{(L-1)}(y,y)\\right)\\right)=\\mathcal{N}\\left(0,\\Sigma_{m;x,y}^{(L-1)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "With $u=\\left(\\begin{array}{l}{z_{1}}\\\\ {z_{2}}\\end{array}\\right)$ and foriverible $\\Sigma=\\Sigma_{\\infty;x,y}^{(L-1)}$ it holds for $L\\ge2$ ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\tilde{\\Sigma}_{1,2;\\infty}^{(L)}(x,y)=\\operatorname*{lim}_{m\\to\\infty}\\tilde{\\Sigma}_{1,2;m}^{(L)}(x,y)=\\operatorname*{lim}_{m\\to\\infty}\\sigma_{w}^{2}\\,\\mathbb{E}\\left[\\mathrm{e}^{+}\\mathrm{f}_{m}(Z_{1}^{m})\\,\\tilde{\\sigma}(Z_{2}^{m})\\right]}\\\\ &{\\stackrel{(\\star)}{=}\\sigma_{w}^{2}\\,\\mathbb{E}\\left[2\\delta_{0}(Z_{1}^{\\infty})\\,\\tilde{\\sigma}(Z_{2}^{\\infty})\\right]=2\\sigma_{w}^{2}\\int_{\\mathbb{R}^{2}}\\frac{1}{2\\pi}\\,|\\Sigma|^{-\\frac{1}{2}}\\,\\delta_{0}(z_{1})\\cdot\\tilde{\\sigma}(z_{2})\\cdot e^{-\\frac{1}{2}u^{\\tau}\\Sigma^{-1}u}\\,\\mathrm{d}u}\\\\ &{=\\sigma_{w}^{2}\\,\\sqrt{\\frac{2}{\\pi}}\\,\\Sigma_{1,1}^{-\\frac{1}{2}}\\int_{\\mathbb{R}}\\frac{1}{\\sqrt{2\\pi}}\\sqrt{\\frac{\\Sigma_{1,1}}{|\\Sigma|}}\\cdot\\tilde{\\sigma}(z_{2})\\cdot e^{-\\frac{1}{2}\\frac{\\Sigma_{1,1}}{|\\Sigma|}z_{2}^{2}}\\,\\mathrm{d}z_{2}}\\\\ &{=\\sigma_{w}^{2}\\,\\sqrt{\\frac{2}{\\pi}}\\,\\left(\\Sigma_{\\infty}^{(L-1)}(x,x)\\right)^{-\\frac{1}{2}}\\mathbb{E}_{Y\\sim N}\\big({_0},\\vert\\Sigma_{\\infty;y}^{(L-1)}\\vert\\,\\Sigma_{\\infty}^{(L-1)}(x,x)\\big)^{\\left[\\tilde{\\sigma}(Y)\\right]},}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "whereEquation $(\\star)$ seems natural, but requires further reasoning. We will prove the above rigorously in Lemma E.7. For now, we assume that the equality holds. Since $\\tilde{\\sigma}$ is bounded and continuous, this yields ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{x\\to y}{\\operatorname*{lim}}\\ \\tilde{\\Sigma}_{1,2;\\infty}^{(L)}(x,y)}\\\\ &{=\\underset{\\left|\\Sigma_{\\infty;x,y}^{(L-1)}\\right|\\to0}{\\operatorname*{lim}}\\sigma_{w}^{2}\\ \\sqrt{\\frac{2}{\\pi}}\\left(\\Sigma_{\\infty}^{(L-1)}(x,x)\\right)^{-\\frac{1}{2}}\\mathbb{E}_{Y\\sim N\\left(0,\\left|\\Sigma_{\\infty;x,y}^{(L-1)}\\right|/\\Sigma_{\\infty}^{(L-1)}(x,x)\\right)}\\left[\\tilde{\\sigma}(Y)\\right]}\\\\ &{=\\!\\sigma_{w}^{2}\\,\\sqrt{\\frac{2}{\\pi}}\\,\\left(\\Sigma_{\\infty}^{(L-1)}(x,x)\\right)^{-\\frac{1}{2}}\\tilde{\\sigma}(0).}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "This agrees with the fact that ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\Sigma}_{1,2;\\infty}^{(L)}(x,x)=\\underset{m\\to\\infty}{\\operatorname*{lim}}\\sigma_{w}^{2}\\,\\mathbb{E}\\left[\\mathrm{erf}_{m}(Z_{1}^{m})\\,\\tilde{\\sigma}(Z_{1}^{m})\\right]=\\sigma_{w}^{2}\\,\\mathbb{E}\\left[2\\delta_{0}(Z_{1}^{\\infty})\\,\\tilde{\\sigma}(Z_{1}^{\\infty})\\right]}\\\\ &{\\qquad\\qquad\\qquad=2\\sigma_{w}^{2}\\frac{1}{\\sqrt{2\\pi}}\\left(\\Sigma_{\\infty}^{(L-1)}(x,x)\\right)^{-\\frac12}\\tilde{\\sigma}(0)=\\sigma_{w}^{2}\\,\\sqrt{\\frac{2}{\\pi}}\\left(\\Sigma_{\\infty}^{(L-1)}(x,x)\\right)^{-\\frac12}\\tilde{\\sigma}(0),}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "where we again assumed an equality very similar to Equation $(\\star)$ . It is easy to check that Equations (S76) and (S77) can be recovered by inserting ? = erf into the derived formulas. ", "page_idx": 50}, {"type": "text", "text": "We now prove the missing part of Equation (S78). ", "page_idx": 50}, {"type": "text", "text": "Lemma E.7. Let $\\tilde{\\sigma}$ be a bounded and continuous function and $((Z_{1}^{m},Z_{2}^{m}))_{m\\in\\mathbb{N}}$ a sequence of random variables, $(Z_{1}^{m},Z_{2}^{m})\\sim\\mathcal{N}(0,\\Sigma^{m}).$ If the covariancematrices areinvertible and converge to an invertible matrix, $\\Sigma^{m}\\to\\Sigma^{\\infty}\\in\\mathbb{R}^{2\\times2}$ as $m\\rightarrow\\infty$ then it holds ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{m\\to\\infty}\\mathbb{E}\\left[\\operatorname{erf}_{m}(Z_{1}^{m})\\,\\tilde{\\sigma}(Z_{2}^{m})\\right]=\\sqrt{\\frac{2}{\\pi}}\\left(\\Sigma_{1,1}^{\\infty}\\right)^{-\\frac{1}{2}}\\mathbb{E}_{Y\\sim N\\left(0,\\left|\\Sigma^{\\infty}\\right|/\\Sigma_{1,1}^{\\infty}\\right)}[\\tilde{\\sigma}(Y)].\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "If $Z_{1}^{m}=Z_{2}^{m}$ forall $m\\in\\mathbb{N}$ so that the covariance matrices are given by $\\begin{array}{r}{\\Sigma^{m}=\\left(\\!\\!\\begin{array}{c c}{\\Sigma_{1}^{m}\\ \\Sigma_{1}^{m}}\\\\ {\\Sigma_{1}^{m}\\ \\Sigma_{1}^{m}}\\end{array}\\!\\!\\right)}\\end{array}$ ,and $i f$ $\\Sigma_{1}^{m}\\to\\Sigma_{1}^{\\infty}\\neq0$ as $m\\rightarrow\\infty,$ then it holds ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{m\\to\\infty}\\mathbb{E}\\left[\\mathrm{erf}_{m}(Z_{1}^{m})\\,\\tilde{\\sigma}(Z_{2}^{m})\\right]=\\sqrt{\\frac{2}{\\pi}}\\,(\\Sigma_{1}^{\\infty})^{-\\frac{1}{2}}\\,\\tilde{\\sigma}(0).\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Proof We begin with the case of invertible covariance matrices. Again denoting $u=\\left(\\begin{array}{l}{z_{1}}\\\\ {z_{2}}\\end{array}\\right)$ , it holds by assumption ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[\\operatorname{erf}_{m}(Z_{1}^{m})\\,\\widetilde{\\sigma}(Z_{2}^{m})\\right]=\\int_{\\mathbb{R}^{2}}\\frac{1}{2\\pi}\\big|\\Sigma^{m}\\big|^{-\\frac{1}{2}}\\cdot\\widetilde{\\sigma}(z_{2})\\cdot\\frac{2m}{\\sqrt{\\pi}}e^{-m^{2}z_{1}^{2}}\\cdot e^{-\\frac{1}{2}u^{\\top}(\\Sigma^{m})^{-1}u}\\,\\mathrm{d}u}\\\\ &{=\\int_{\\mathbb{R}^{2}}\\frac{1}{2\\pi}\\big|\\Sigma^{m}\\big|^{-\\frac{1}{2}}\\cdot\\widetilde{\\sigma}(z_{2})\\cdot\\frac{2}{\\sqrt{\\pi}}\\cdot e^{-\\frac{1}{2}\\big(\\big(\\frac{z_{1}/m}{z_{2}}\\big)^{\\top}(\\Sigma^{m})^{-1}\\big(\\frac{z_{1}/m}{z_{2}}\\big)+2u^{\\top}e_{1}e_{1}^{\\top}u\\big)}\\,\\mathrm{d}u}\\\\ &{=\\int_{\\mathbb{R}^{2}}\\frac{1}{2\\pi}\\big|\\Sigma^{m}\\big|^{-\\frac{1}{2}}\\cdot\\widetilde{\\sigma}(z_{2})\\cdot\\frac{2}{\\sqrt{\\pi}}\\cdot e^{-\\frac{1}{2}u^{\\top}B^{m}u}\\,\\mathrm{d}u,}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "for ", "page_idx": 51}, {"type": "equation", "text": "$$\nB^{m}:=\\frac{1}{|\\Sigma^{m}|}\\left(\\sum_{-\\Sigma_{1,2}^{m}/m}^{\\Sigma_{2,2}^{m}/m^{2}}\\right.\\left.-\\Sigma_{1,2}^{m}/m\\right)+\\left(2\\right.\\!\\!\\begin{array}{c c}{{0}}&{{\\!\\!\\!0}}\\\\ {{0}}&{{\\!\\!\\!0\\!\\!\\!}}\\end{array}\\right)\\xrightarrow{m\\to\\infty}\\left(2\\right.\\left.\\!\\begin{array}{c c}{{0}}&{{\\!\\!\\!0\\!\\!\\!}}\\\\ {{0}}&{{\\Sigma_{1,1}^{\\infty}/|\\Sigma^{\\infty}|\\!\\!\\!}}\\end{array}\\right)=:B^{\\infty}.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "The determinant of $B^{m}$ is given by ", "page_idx": 51}, {"type": "equation", "text": "$$\n|B^{m}|=\\left(\\frac{\\Sigma_{2,2}^{m}}{m^{2}|\\Sigma^{m}|}+2\\right)\\frac{\\Sigma_{1,1}^{m}}{|\\Sigma^{m}|}-\\frac{\\left(-\\Sigma_{1,2}^{m}\\right)^{2}}{m^{2}|\\Sigma^{m}|^{2}}=\\frac{1}{m^{2}|\\Sigma^{m}|}+2\\frac{\\Sigma_{1,1}^{m}}{|\\Sigma^{m}|}.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "This now yields ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle(S82)=\\frac{2}{\\sqrt{\\pi}}|\\Sigma^{m}|^{-\\frac{1}{2}}\\Big|\\left(B^{m}\\right)^{-1}\\Big|^{\\frac{1}{2}}\\int_{\\mathbb{R}^{2}}\\frac{1}{2\\pi}\\Big|\\left(B^{m}\\right)^{-1}\\Big|^{-\\frac{1}{2}}\\cdot\\tilde{\\sigma}(z_{2})\\cdot e^{-\\frac{1}{2}u^{\\mathsf{T}}B^{m}u}\\,\\mathrm{d}u}}\\\\ {{\\displaystyle\\qquad=\\frac{2}{\\sqrt{\\pi}}|\\Sigma^{m}|^{-\\frac{1}{2}}|B^{m}|^{-\\frac{1}{2}}\\mathbb{E}_{(Y_{1}^{m},Y_{2}^{m})\\sim N}\\Big(\\mathsf{o}_{,(B^{m})^{-1}}\\big)\\big[\\tilde{\\sigma}(Y_{2}^{m})\\big].}}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "The continuity of matrix inversion implies $\\left(B^{m}\\right)^{-1}\\to\\left(B^{\\infty}\\right)^{-1}$ as $m\\rightarrow\\infty$ The characteristic functions of finite-dimensional Gaussian random variabies are fully defined by their means and covariance matrices. Thus, the convergence of the covariance matrices implies convergence of the characteristic functions, which in turn implies convergence in distribution. Since $\\tilde{\\sigma}$ is continuous and bounded and the determinant is continuous, we obtain ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\frac{2}{\\sqrt{\\pi}}|\\Sigma^{m}|^{-\\frac{1}{2}}|B^{m}|^{-\\frac{1}{2}}\\mathbb{E}_{(Y_{1}^{m},Y_{2}^{m})\\sim N\\big(0,(B^{m})^{-1}\\big)}[{\\tilde{\\sigma}}(Y_{2}^{m})]}}\\\\ {{\\displaystyle\\frac{m\\rightarrow\\infty}{\\sqrt{\\pi}}|\\Sigma^{\\infty}|^{-\\frac{1}{2}}|B^{\\infty}|^{-\\frac{1}{2}}\\mathbb{E}_{(Y_{1}^{\\infty},Y_{2}^{\\infty})\\sim N}\\big(0,(B^{\\infty})^{-1}\\big)}[{\\tilde{\\sigma}}(Y_{2}^{\\infty})]}}\\\\ {{\\displaystyle\\frac{(\\mathtt{s s})}{\\sqrt{\\pi}}\\frac{2}{|\\Sigma^{\\infty}|}|\\Sigma^{\\infty}|^{-\\frac{1}{2}}\\left(\\frac{2\\Sigma_{1,1}^{\\infty}}{|\\Sigma^{\\infty}|}\\right)^{-\\frac{1}{2}}\\mathbb{E}_{Y\\sim N\\big(0,|\\Sigma^{\\infty}|/\\Sigma_{1,1}^{\\infty}\\big)}[{\\tilde{\\sigma}}(Y)]}}\\\\ {{\\displaystyle=\\sqrt{\\frac{2}{\\pi}}\\left(\\Sigma_{1,1}^{\\infty}\\right)^{-\\frac{1}{2}}\\mathbb{E}_{Y\\sim N\\big(0,|\\Sigma^{\\infty}|/\\Sigma_{1,1}^{\\infty}\\big)}[{\\tilde{\\sigma}}(Y)].}}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "This proves Equation (S80). In the case $Z_{1}^{m}=Z_{2}^{m}$ ,we have ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[\\exp_{t}(Z_{1}^{m})\\,\\tilde{\\sigma}(Z_{2}^{m})\\right]=\\mathbb{E}_{Y\\sim X}(0,\\Sigma_{1}^{m})\\left[\\exp_{t}(Y)\\,\\tilde{\\sigma}(Y)\\right]}\\\\ &{=\\int_{\\mathbb{R}}\\frac{1}{\\sqrt{2\\pi}}\\frac{2}{\\sqrt{\\lambda}^{\\frac{m}{1}}}\\frac{2}{\\sqrt{\\pi}}m\\cdot e^{-m^{2}y^{2}}\\cdot\\tilde{\\sigma}(y)\\cdot e^{-\\frac{1}{2}\\frac{y^{2}}{17}}\\,\\mathrm{d}y}\\\\ &{=\\int_{\\mathbb{R}}\\frac{1}{\\sqrt{2\\pi}}\\frac{2}{\\sqrt{\\lambda}^{\\frac{m}{1}}}\\frac{2}{\\sqrt{\\pi}}m\\cdot\\tilde{\\sigma}(y)\\cdot e^{-\\frac{1}{2}y^{2}\\left(1/\\Sigma_{1}^{m}+2m^{2}\\right)}\\,\\mathrm{d}y}\\\\ &{=\\frac{2}{\\sqrt{\\pi}}\\frac{1}{\\sqrt{\\lambda}^{\\frac{m}{1}}}\\frac{m}{\\sqrt{1/\\sum_{1}^{m}+2m^{2}}}\\int_{\\mathbb{R}}\\frac{1}{\\sqrt{2\\pi}}\\frac{1}{\\left(1/\\Sigma_{1}^{m}+2m^{2}\\right)^{\\frac{1}{2}}\\cdot\\tilde{\\sigma}(y)\\cdot e^{-\\frac{1}{2}y^{2}\\left(1/\\Sigma_{1}^{m}+2m^{2}\\right)}\\,\\mathrm{d}y}\\\\ &{=\\frac{2}{\\sqrt{\\pi}}\\frac{1}{\\sqrt{1/m^{2}+2\\sqrt{\\lambda}}}\\frac{1}{\\sqrt{1/m^{2}+2\\sqrt{\\lambda}}}\\mathbb{E}_{Y\\sim X}(0,(1/\\Sigma_{1}^{m}+2m^{2})\\cdots)[\\tilde{\\sigma}(Y)]}\\\\ &{\\xrightarrow{m\\rightarrow\\infty}\\frac{2}{\\sqrt{\\pi}}\\frac{1}{\\sqrt{2\\sum_{1}^{m}+2\\sqrt{\\pi}}}\\tilde{\\sigma}(0)=\\sqrt{\\frac{2}{\\pi}}(\\Sigma_{1}^{m})^{-\\frac{1}{2}}\\tilde{\\sigma}(0),}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "again using the boundedness and continuity of $\\tilde{\\sigma}$ in the last line. This proves Equation (S81). ", "page_idx": 52}, {"type": "text", "text": "With m = Z(-1) the lemma yields ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\tilde{\\Sigma}_{1,2;\\infty}^{(L)}(x,y)=\\displaystyle\\operatorname*{lim}_{m\\to\\infty}\\tilde{\\Sigma}_{1,2;m}^{(L)}(x,y)=\\operatorname*{lim}_{m\\to\\infty}\\sigma_{w}^{2}\\,\\mathbb{E}\\left[\\mathrm{erf}_{m}(Z_{1}^{m})\\,\\tilde{\\sigma}(Z_{2}^{m})\\right]}\\\\ &{=\\!\\sigma_{w}^{2}\\,\\sqrt{\\frac{2}{\\pi}}\\left(\\Sigma_{\\infty}^{(L-1)}(x,x)\\right)^{-\\frac{1}{2}}\\mathbb{E}_{Y\\sim\\mathcal{N}\\left(0,\\left|\\Sigma_{\\infty;\\,y}^{(L-1)}\\right|/\\Sigma_{\\infty}^{(L-1)}(x,x)\\right)}[\\tilde{\\sigma}(Y)].}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "In conclusion, we found that the analytic NTK for surrogate gradient learning with sign function is well-defined and non-singular for any bounded and Lipschitz continuous surrogate derivative. As in the special case of theror funtio, the term $\\tilde{\\Sigma}_{1,2;\\infty}^{(L)}(x,y)$ can be expressed in terms of $\\Sigma_{\\infty}^{(L-1)}(x,x)$ an hedeerminant $\\big|\\Sigma_{\\infty;x,y}^{(L-1)}\\big|$ ", "page_idx": 52}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 53}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Justification: As stated in the abstract, we analyze the NTK for activation functions with jumps and generalize the NTK to surrogate gradient learning. We focus on the sign activation function in detail, which is also stated in the abstract. The derived theorems, however, are valid for a general class of activation functions. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and refect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 53}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 53}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Justification: The limitations of the theorems are discussed throughout the paper and a general discussion of limitations can be found in Section 4. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 53}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 54}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Justification: Yes, all definitions, theoretical results, and proofs are given in great detail in the appendix and have only been slightly shortened for the paper itself. Only for Lemma E.6, which is used to prove Theorem 2.5, the proof is not given in full detail. Most proofs rely on similar results for the classical NTK and these results are properly discussed and referenced. ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 54}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 54}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 54}, {"type": "text", "text": "Justification: All information needed to reproduce the numerical experiments is given in Section 3 and Section A. ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b)If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 54}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 55}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 55}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 55}, {"type": "text", "text": "Justification: Yes, the code is provided in the supplementary material with instructions to reproduce the experiments and figures. ", "page_idx": 55}, {"type": "text", "text": "Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : / /nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 55}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 55}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Justification: Since only plain gradient descent and gradient descent with surrogate gradients (surrogate gradient learning) is used in the paper, no further additional specifications apart from the ones made in Section 3 and Section A are necessary. ", "page_idx": 55}, {"type": "text", "text": "Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 55}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 55}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 55}, {"type": "text", "text": "Justification: Yes, confidence bands are used for Figure 3 to demonstrate the agreement between two distributions. Figure 1 and Figure 2 are complemented by Figure B.3 and Figure B.4 respectively, showing the mean squared errors. ", "page_idx": 55}, {"type": "text", "text": "Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 56}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 56}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 56}, {"type": "text", "text": "Justification: The compute resources used for the numerical simulation are described in SectionA. ", "page_idx": 56}, {"type": "text", "text": "Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 56}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 56}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 56}, {"type": "text", "text": "Justification: We reviewed the NeurIPSs Code of Ethics and the paper conforms with it. Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 56}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 56}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 57}, {"type": "text", "text": "Justification: The paper is solely theoretical and does not have conceivable societal impacts. Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 57}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 57}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 57}, {"type": "text", "text": "Justification: The paper does not contain models. The data is only created to illustrate and examine the theoretical results and has no risk for misuse. ", "page_idx": 57}, {"type": "text", "text": "Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to acces the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 57}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 57}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 57}, {"type": "text", "text": "Justification: The used packages have been properly referenced in Section 3. Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 57}, {"type": "text", "text": "", "page_idx": 58}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 58}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 58}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 58}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 58}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 58}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 58}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 59}]