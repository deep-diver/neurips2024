{"references": [{"fullname_first_author": "Andrea Caponnetto", "paper_title": "Optimal rates for the regularized least-squares algorithm", "publication_date": "2007-00-00", "reason": "This paper establishes the optimal convergence rates for regularized least squares, which are crucial for understanding the performance of random feature regression in the context of this paper."}, {"fullname_first_author": "Ali Rahimi", "paper_title": "Random features for large-scale kernel machines", "publication_date": "2007-00-00", "reason": "This foundational paper introduces random features as an efficient approximation for kernel methods, providing the basis for the random feature regression model studied in this work."}, {"fullname_first_author": "Ali Rahimi", "paper_title": "Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning", "publication_date": "2008-00-00", "reason": "This work extends the random features idea by showing that even simpler models can achieve similar generalization performance, further justifying the approach taken in this paper."}, {"fullname_first_author": "Alessandro Rudi", "paper_title": "Generalization properties of learning with random features", "publication_date": "2017-00-00", "reason": "This paper provides improved bounds on the number of features needed to achieve optimal generalization, directly relevant to the core analysis of this paper."}, {"fullname_first_author": "Song Mei", "paper_title": "The generalization error of random features regression: Precise asymptotics and the double descent curve", "publication_date": "2022-00-00", "reason": "This recent work provides precise asymptotic analysis for random feature regression, offering a rigorous framework and complementing the non-asymptotic results presented in this paper."}]}