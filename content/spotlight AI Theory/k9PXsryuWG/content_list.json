[{"type": "text", "text": "Metric Transforms and Low Rank Representations of Kernels for Fast Attention ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Timothy Chu Josh Alman Gary Miller Independent Researcher Columbia University Carnegie Mellon University timothyzchu@gmail.com josh@cs.columbia.edu glmiller@cs.cmu.edu ", "page_idx": 0}, {"type": "text", "text": "Shyam Narayanan Citadel Securities shyam.s.narayanan@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Mark Sellke Harvard University msellke@fas.harvard.edu ", "page_idx": 0}, {"type": "text", "text": "Zhao Song Simons Institute for the Theory of Computing, UC Berkeley magic.linuxkde@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We introduce a new linear-algebraic tool based on group representation theory, and use it to address three key problems in machine learning. ", "page_idx": 0}, {"type": "text", "text": "1. Past researchers have proposed fast attention algorithms for LLMs by approximating or replace softmax attention with other functions, such as low-degree polynomials. The key property of these functions is that, when applied entrywise to the matrix $\\bar{Q}\\bar{K}^{\\top}$ , the result is a low rank matrix when $Q$ and $K$ are $n\\times d$ matrices and $n\\gg d$ . ", "page_idx": 0}, {"type": "text", "text": "This suggests a natural question: what are all functions $f$ with this property? If other $f$ exist and are quickly computable, they can be used in place of softmax for fast subquadratic attention algorithms. It was previously known that low-degree polynomials have this property. We prove that low-degree polynomials are the only piecewise continuous functions with this property. This suggests that the low-rank fast attention only works for functions approximable by polynomials. Our work gives a converse to the polynomial method in algorithm design. ", "page_idx": 0}, {"type": "text", "text": "2. We prove the first full classification of all positive definite kernels that are functions of Manhattan or $\\ell_{1}$ distance. Our work generalizes, and also gives a new proof for, an existing theorem at the heart of kernel methods in machine learning: the classification of all positive definite kernels that are functions of Euclidean distance. ", "page_idx": 0}, {"type": "text", "text": "3. The key problem in metric transforms, a mathematical theory used in geometry and machine learning, asks what functions transform pairwise distances in metric space $M$ to metric space $N$ for specified $M$ and $N$ . We prove the first full classification of functions that transform Manhattan distances to Manhattan distances. Our work generalizes the foundational work of Schoenberg, which fully classifies functions that transform Euclidean to Euclidean distances. ", "page_idx": 0}, {"type": "text", "text": "We additionally prove results about stable-rank preserving functions that are potentially useful in algorithmic design, and more. Our core tool for all our results is a new technique called the representation theory of the hyperrectangle. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Kernel methods in linear algebra have numerous applications throughout computer science and machine learning. Consider the following basic questions in this area. ", "page_idx": 1}, {"type": "text", "text": "(A) Given any set of low-dimensional points $x_{1},x_{2},\\cdot\\cdot\\cdot\\,,x_{n},y_{1},y_{2},\\dotsc,y_{n}\\in\\mathbb{R}^{d}$ and a function $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ , is there a small $k<n$ and a function $F:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{k}$ such that $f(\\langle x_{i},y_{j}\\rangle)=$ $\\langle F(x_{i}),F(y_{j})\\rangle$ for all $i$ and $j?$ Equivalently, when $f$ is applied entry-wise to a low-rank (kernel) matrix, is the result always low-rank? What about approximately low-rank?   \n(B) Given any set of points $x_{1},x_{2},\\cdot\\cdot\\cdot\\,,x_{n}$ and a (kernel) function $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ , is there some function $F$ such that $f(\\|x_{i}-x_{j}\\|_{1})=\\langle F(x_{i}),F(x_{j})\\rangle$ for all $i$ and $j?$ Equivalently, is $f$ a positive definite Manhattan kernel?   \n(C) Given any set of points $x_{1},\\cdot\\cdot\\cdot x_{n}$ , from semi-metric spaces $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ , for which functions $f$ does there exist a function $F$ such that $f(\\mathrm{dist}_{X}(x_{i},x_{j}))=\\mathrm{dist}_{\\mathcal{Y}}(F(x_{i}),F(x_{j}))$ for all $i$ and $j?$ Equivalently, which functions $f$ give a metric transform [DL09] between $\\mathcal{X}$ and $\\mathcal{V}?$ ", "page_idx": 1}, {"type": "text", "text": "Question (A) relates to the study and effectiveness of polynomial kernels in machine learning. These kernels have many applications [Sou10], for instance in speeding up attention in LLMs [AS23, KMZ23, $\\mathrm{TBY^{+}19}$ , KVPF20, AS24b, AS24c, SSWZ23a], in NLP for improving the quality of learning algorithms [KM03, GE08, $\\mathrm{CHC^{+}10]}$ and basic computations $[\\bar{\\mathrm{VSP}}^{+}17]$ during training. Oftentimes, if $f$ is not a polynomial, one may even approximate it by a polynomial (for instance by truncating its Taylor expansion) or use sketching [HAS20, SWYZ21, SZZ24, SYZ24] in order to achieve some of the benefits of polynomial kernels in exchange for worse accuracy guarantees. This has been particularly effective for the neural tangent kernel [JGH18], Gaussian kernel [NJW02, RR08, $\\mathrm{AKK}^{+}\\bar{2}0$ , $\\mathrm{HSW}^{{\\dot{+}}}22]$ ], generalized T-Student kernel [BTF04], Cauchy kernel [RR08], and power kernel [FS03]. ", "page_idx": 1}, {"type": "text", "text": "One can verify that if $f$ is a polynomial, then one can achieve $k\\leq d^{\\deg(f)}$ in question (A). Beyond just kernel methods, this fact has been used to design efficient algorithms throughout computer science using a technique called \u2018the polynomial method in algorithm design\u2019 (see e.g., [AWY14, CW16, ACW16, Wil18, Alm19, ACSS20, AS23, AS24c, AS24b], and Section 2 below for more examples). Determining which other functions $f$ have this key property can help to extend these phenomena to more settings. ", "page_idx": 1}, {"type": "text", "text": "Kernel methods, and linear-algebraic computations related to the kernel matrix such as those involved in Question (B), are very popular in modern machine learning. In some applications, such as spectral clustering [vL07, NJW02], semi-supervised learning [Zhu05a, Zhu05b, ${\\mathrm{L}}S Z^{+}19$ , SSLL23, SSL24], Laplacian system solving in geometric graphs [ACSS20] and kernel support vector machines [GSZ23], one needs to explicitly compute the kernel matrix and corresponding function $F$ . On the other hand, for many other applications such as regression or classification algorithms, it suffices to implicitly maintain the kernel matrix, or simply prove that the function $F$ exists [Smo96, $S S B^{+}97]$ ; several prominent recent examples are neural tangent kernel regression [BPSW21, SYZ21, MOSW22, $\\mathrm{ALS}^{+}23$ , $\\mathrm{GLS}^{+}24$ , LLSS24], tensor kernel regression [Zha22, RSZ22, SZZ24], polynomial kernels [SWYZ21, HAS20, $\\mathrm{AKK}^{+}20$ , SYZ24], and signal interpolation [CKPS16, SSWZ23b]. This motivates question (B), where we ask whether $F$ exists, but not how efficient it is. ", "page_idx": 1}, {"type": "text", "text": "The metric transforms referenced in Question (C) arise naturally in many settings where one wants to transform a set of points from a metric space while maintaining some of the metric structure between them. The field was pioneered by Schoenberg [Sch38, Sch42, Sch35] and Von Neumann [NS41]. Very broadly, this allows one to take advantage of algorithmic tools in both metric spaces simultaneously [DL09]. This approach has proven useful in many areas including visualizing the geometry of BERT $[\\mathrm{RYW}^{+}19]$ , computer vision [FLH15, KZR16, KCC17, WZF05], clustering [MMR19], sketching and embedding norms [AKR15, IMS17], terminal embedding [MMMR18, NN19, CN21], low dimensional embeddings via JL transform [AC06, DKS10], mean estimation [LNRW19] nearest neighbor search [AIR18], generative models [XZZ18], data-sensitive distances in clustering [CMS20], neural networks $\\mathrm{[Orr96]}$ , harmonic analysis [Aro50, LLLH18, KW71], kernel methods $[\\mathrm{SSB^{+}97}]$ , distance oracle [DSWZ22], and PDE theory [FS98, CFW12]. ", "page_idx": 1}, {"type": "text", "text": "Many researchers consider metric transforms when the input and output space are Euclidean, since more is known about metric transforms in this setting. However, metric transforms between other metrics could have equally rich algorithmic applications, and question (C) generalizes this beyond just Euclidean metrics. We could think of distance metrics in two semi-metric spaces $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ , and the function pair $(f,F)$ can be viewed as a transform from $\\mathcal{X}$ to $\\boldsymbol{\\wp}$ . ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "In all these settings, there is a gap in our current understanding of what kernel functions can be used. For instance, there are a number of functions where it is not known whether they are positive definite Manhattan kernels which could be used in classification, semi-supervised learning, and other similar tasks. We will see that a common suite of mathematical tools can be used to address all these different gaps. Most of our results prove that our understanding is complete, and that, for instance, the functions we know to be positive definite Manhattan kernels are, in fact, the only ones. This finally completes our understanding of these important classes of functions. That said, in the setting of preserving the stable rank of matrices, we will find that there are functions that are not polynomials, but that surprisingly do preserve the stable ranks of important matrices. Before we get into our technique in more detail, we first describe our main results in context. ", "page_idx": 2}, {"type": "text", "text": "Roadmap. In Section 2, we prove that the only functions which always yield a low-rank matrix when applied entry-wise to a low-rank matrix are low-degree polynomials, and explain the application to transformers. In Section 3, we give a classification of positive definite kernels with Manhattan distance input. In Section 4, we categorize all functions which transform Manhattan distances to Manhattan distances or squared Euclidean distances. In Section 5, we briefly introduce the core tool of this work. In Section 6, we give a conclusion of our work. In Section 7, we discuss the limitations of our work. In Section 8, we discuss the societal impacts of our work. ", "page_idx": 2}, {"type": "text", "text": "2 Fast Attention and the Polynomial Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Fast attention computations in transformers and LLMs [AS23, AS24c, AS24b, AS24a, $L S S^{+}24$ , $\\mathrm{HWL}^{+}24$ , LSSZ24a, LSSZ24b, KVPF20, $\\mathrm{TBY^{+}19]}$ use the polynomial method as a key ingredient. This is a powerful technique for designing algorithms and constructing combinatorial objects. It states that applying a low-degree polynomial entry-wise to a lo\u221aw rank matrix yields another low-rank matrix. Examples of these low rank matrices include $Q K^{\\top}/{\\sqrt{d}}$ for $n\\times d$ matrices $Q$ and $K$ with $n\\gg d$ , which is a key matrix for attention computations in transformers and LLMs. ", "page_idx": 2}, {"type": "text", "text": "Fast attention computations rely on a polynomial approximation to the exponential function [AS23] combined with the polynomial method. Past researchers [KMZ23, SSWZ23a, $\\mathrm{TBY^{+}19}$ , KVPF20] suggested replacing the exponential function in softmax attention with a general kernel function $f$ . When $f$ is a low-degree polynomial, researchers leveraged the polynomial method to create fast algorithms for polynomial attention in LLM computations [KMZ23, $\\mathrm{\\dot{T}B Y^{+}19}$ , KVPF20]. The work of [KMZ23] showed experimentally that polynomial attention has faster training and inference times, with little loss in quality on large language models. ", "page_idx": 2}, {"type": "text", "text": "Fact 2.1 (The polynomial method, folklore; see e.g. [CLP17]). Suppose $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ is a polynomial of degree d. Then, for any matrix $M\\in\\mathbb{R}^{n\\times n}$ of rank $r$ , letting ", "page_idx": 2}, {"type": "equation", "text": "$$\nk:=2{\\binom{r+\\lfloor d/2\\rfloor-1}{\\lfloor d/2\\rfloor}},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "the matrix $M^{f}\\in\\mathbb{R}^{n\\times n}$ given by $M_{i,j}^{f}:=f(M_{i,j})\\;h a s\\;\\mathrm{rank}(M^{f})\\leq k.$ . ", "page_idx": 2}, {"type": "text", "text": "For instance, $i f\\,r=\\log_{2}n$ and $d<o(\\log_{2}n)$ , then ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname{rank}(M^{f})<n.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The definition of the polynomial method inspires the following definition: ", "page_idx": 2}, {"type": "text", "text": "Definition 2.2 (Preserve low-rank matrices). For a function $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ and positive integer $n$ , we say $f$ preserves low-rank $n\\times n$ matrices $i f,$ for every matrix $M\\in\\mathbb{R}^{n\\times n}$ with $\\operatorname{rank}(M)\\leq\\lceil\\log_{2}(n)\\rceil+1,$ , the entry-wise application $M^{f}\\in\\mathbb{R}^{n\\times n}$ given by $M_{i,j}^{f}:=f(M_{i,j})$ has ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname{rank}(M^{f})<n.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "It follows from the polynomial method that low-degree polynomials preserve low rank. Fast attention computations [AS23] rely on this low rank preservation property. For any function $f$ that preserves low rank, if the low rank decomposition of $M^{f}$ can be efficiently computed, one can create a replacement for attention that runs in almost linear time in the sequence length $n$ . This is a significant improvement over the quadratic time algorithms necessary for (unbounded) softmax attention in LLM models (implicit in [KMZ23, AS23, $\\mathrm{TBY^{+}19}$ , SSWZ23a]). ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "This motivates the question: ", "page_idx": 3}, {"type": "text", "text": "Question 2.3. Is it possible to generalize the polynomial method (Fact 2.1) to functions $f$ other than polynomials? ", "page_idx": 3}, {"type": "text", "text": "In other words, are there functions $f$ which are not polynomials, but such that if one starts with any low-rank matrix $M$ , and applies it entry-wise yielding the matrix $M^{f}$ , then $M^{f}$ also has low rank? If such a function existed, it could allow for faster transformers with a wider variety of attention functions, along with many other algorithmic applications. ", "page_idx": 3}, {"type": "text", "text": "We prove that low-degree polynomials are the only piecewise continuous functions $f$ that preserve low rank. This suggests that the low-rank approach to fast attention calculations [AS23, AS24b, AS24c] and fast polynomial attention algorithms [KMZ23] can only work for functions that are approximations of polynomials. ", "page_idx": 3}, {"type": "text", "text": "The polynomial method can also be used in algorithm design to design the fastest known algorithms for a variety of different, important problems, including: batch Hamming Nearest Neighbor Search [ACW16], the Orthogonal Vectors problem from fine-grained complexity [AWY14, CW16], All-Pairs Shortest Paths [Wil18, CW16], the lightbulb problem in which one wants to find a planted pair of correlated vectors among a collection of random vectors [Val12, KKK18, Alm19], computational problems related to kernel methods in spectral clustering and semi-supervised learning [ACSS20], and some stable matching problems [MPS16]. In all these works, one starts with a matrix $M$ describing the input data which has low rank, and one transforms it into a matrix like $M^{f}$ which \u2018amplifies\u2019 the key properties of the data while still having low rank. A similar approach has also been used in the theory of polynomial kernels, such as in algorithms for transformers in NLP $[\\mathrm{VSP^{+}}17$ , DCLT18, $\\mathrm{R}\\dot{\\mathrm{N}}\\dot{\\mathrm{S}}^{+}1\\dot{8}$ , $\\dot{\\mathbf{R}}\\mathbf{W}\\mathbf{C}^{+}19$ , $\\mathrm{BMR}^{+}20.$ , ${\\mathrm{CND}}^{+}22$ , $Z\\mathrm{RG}^{+}22$ , AS23, AS24c, KMZ23, $\\mathrm{HJK}^{+}23$ , $\\mathrm{HSK}^{+}24$ , HLSL24, $\\mathrm{HWL}^{+}24]$ , and to bound the ranks of matrices which arise in other settings, such as in the recent resolution of the Cap Set Conjecture from extremal combinatorics [CLP17, EG17], and in recent proofs that Hadamard and Fourier transforms have low \u2018matrix rigidity\u2019 [AW17, DE19, DL19]. ", "page_idx": 3}, {"type": "text", "text": "For a function $f$ to be effective in the polynomial method as described above, it is necessary (but usually not sufficient) that $f$ preserves low-rank $n\\times n$ matrices in the sense of Definition 2.2. Indeed, in all the aforementioned applications of the polynomial method, such as the algorithm of [ACW16] and the application to transformers that we described above, the original matrix $M$ describing the data can have rank greater than $\\log_{2}n$ . The details of how low the rank of $M^{f}$ must be can vary in the different applications, but it is always necessary that $M^{f}$ has less than full rank (i.e., $\\operatorname{rank}(M^{\\acute{f}})<n)$ . ", "page_idx": 3}, {"type": "text", "text": "2.1 Converse for the Polynomial Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our starting point is the recent work in mathematics by Guillot, Khare, and Rajaratnam [GKR17], which partially answers Question 2.3 negatively. This shows that Fact 2.1 cannot be generalized in many settings. ", "page_idx": 3}, {"type": "text", "text": "Theorem 2.4 ([GKR17, Theorem B], Informal). Recall that in Fact 2.1, $k$ is the target rank of the matrix $M^{f}$ . Fact $2.1$ is tight, i.e. its converse is true, when either $f$ is $(n-1)$ -times differentiable and $k<n-1$ , or if $M^{f}$ is required to be positive semi-definite and $k<n-3$ . ", "page_idx": 3}, {"type": "text", "text": "This past work has gaps where $f$ might still result in matrices without full rank, especially since the requirement that $f$ is $(n-1)$ -times differentiable is quite restrictive. Common functions in machine learning like ELU [CUH15, KVPF20, $\\mathrm{CLD}^{+}20]$ ], SELU [KUMH17, ZLZ24], and ReLU $[\\mathrm{HSM^{+}00}$ , $\\mathrm{L}\\mathrm{S}\\mathrm{S}^{+}\\bar{20}$ , $Z Z\\mathrm{P}^{+}21$ , ZLZ24] are not second-differentiable everywhere, so Theorem 2.4 doesn\u2019t apply to them. One might imagine getting around this differentiability restriction by using the second part of Theorem 2.4, but unfortunately the matrices $M^{f}$ involved in fast attention computations are not required to be positive semi-definite. So this second part of the theorem does not apply to fast attention, which is a core application of functions that preserve low rank. ", "page_idx": 3}, {"type": "text", "text": "Our first result plugs these gaps, showing that Fact 2.1 cannot be generalized in the settings left open by [GKR17]. (See Section C.8 below where we state [GKR17, Theorem B] more formally and compare it with our Theorem 2.5 in more detail.) ", "page_idx": 4}, {"type": "text", "text": "Theorem 2.5 (Informal statement of Theorem C.11). Suppose the function $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ does not have any essential discontinuities of the first kind1. If $f$ preserves low-rank $n\\times n$ matrices, then $f$ is $a$ polynomial of degree at most $\\lceil\\log_{2}(n)\\rceil$ . ", "page_idx": 4}, {"type": "text", "text": "This shows that functions $f$ without essential discontinuities of the first kind, which are also not polynomials, do not preserve low-rank $n\\times n$ matrices, and only polynomials of degree less than $\\lceil\\log_{2}(n)\\rceil$ can preserve low-rank $n\\times n$ matrices. The class of functions without these essential discontinuities of the first kind is very rich, and includes all piecewise continuous functions; it is hard to imagine a reasonable kernel function which is not piecewise continuous. Hence, one cannot hope to improve on the polynomial method by extending it to other functions without essential discontinuities of the first kind. ", "page_idx": 4}, {"type": "text", "text": "We conjecture that Theorem 2.5 holds for all functions $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ (i.e., that if $f$ has an essential discontinuity of the first kind, then it also does not preserve low-rank matrices). Functions $f$ with an essential discontinuity of the first kind are not interesting in our setting since they cannot be efficiently evaluated. ", "page_idx": 4}, {"type": "text", "text": "We note that there is a small constant-factor gap between the degree which Fact 2.1 tells us is sufficient for a polynomial to preserve low-rank $n\\times n$ matrices, and the degree which Theorem 2.5 says is necessary: for instance, Fact 2.1 says that polynomials of degree at most $\\textstyle{\\frac{1}{2}}\\log_{2}(n)$ suffice, since ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\binom{\\frac54\\log_{2}(n)}{\\frac14\\log_{2}(n)}\\ll n,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "whereas Theorem 2.5 says that degree less than $\\log_{2}(n)$ is necessary. We leave open the question of closing this gap, although we note that the constant factor in front of the polynomial degree does not play a major role in most of the aforementioned applications of Fact 2.1.2 ", "page_idx": 4}, {"type": "text", "text": "2.2 Weaker Polynomial Methods ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Fact 2.1 being essentially tight rules out one way to try to generalize the polynomial method. It is natural to ask whether we can get around this by weakening our constraint on the function $f$ . There are many properties of matrices which can be taken advantage of in the design of fast algorithms, and if we can show that $M^{f}$ has any of these properties, it could still lead to improvements in the aforementioned applications. ", "page_idx": 4}, {"type": "text", "text": "Approximate Low Rank We first study functions $f$ which, when applied entry-wise to a lowrank matrix $M$ , always result in an approximately low-rank matrix $M^{\\tilde{f}}$ . As we mentioned earlier, approximating a non-polynomial kernel function $f$ by a polynomial is a common technique for taking advantage of the properties of polynomial kernels; when $f$ can be well-approximated by a polynomial, then $M^{\\widecheck{f}}$ has approximately low rank for this reason. This raises the question: can functions $f$ which cannot be well-approximated by a polynomial also result in approximately low-rank matrices? ", "page_idx": 4}, {"type": "text", "text": "Our next result answers this question in the negative: the only functions which approximately preserve low rank are approximate polynomials. In other words, if $f$ is not approximately a polynomial, then such an algorithmic approach cannot succeed, as $f$ applied entry-wise to a matrix is not close to low rank. ", "page_idx": 4}, {"type": "text", "text": "We say a matrix $M$ is approximately low-rank if the ratio of its smallest and largest eigenvalues is small; if $M$ were not full rank, then this ratio would be 0. If $M$ is approximately low-rank in this sense, then fast algorithms for manipulating it follow by using low-rank approximation or approximate subspace finding algorithms to find low-rank approximations for the matrix. Analogously, we say $f$ is approximately a polynomial if its finite differences are small3; recall that the $d$ -th order finite differences are 0 for any polynomial of degree $<d-1$ . ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Theorem 2.6 (Main result, informal statement of Theorem D.3). Let $d=\\lceil\\log_{2}n\\rceil$ , and let $\\delta\\in(0,1)$ be sufficiently small. Suppose $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ is a real analytic function which $\\delta$ -approximately preserves low-rank matrices, i.e., $\\begin{array}{r}{{\\frac{\\operatorname*{min}_{i\\in[d]}\\,\\left|\\lambda_{i}\\left(M^{f}\\right)\\right|}{\\operatorname*{max}_{i\\in[d]}\\,\\left|\\lambda_{i}\\left(M^{f}\\right)\\right|}}\\leq\\delta/n}\\end{array}$ for all rank $d+1$ matrices $M$ . Then, the $d^{t h}$ order finite difference of $f$ , evaluated at $a\\in\\mathbb{R},$ , for sufficiently small gaps, is bounded above by $\\delta\\cdot K_{a}$ . Here, $K_{a}>0$ is a scaling factor with the property that $i f\\,f$ is rescaled by a factor of $c>0$ then $K_{a}$ is also rescaled by $c$ . ", "page_idx": 5}, {"type": "text", "text": "A dependence on a scaling factor $K_{a}$ is necessary since, if $f$ is rescaled by $c$ , this rescales the finite differences of $f$ by $c$ , but does not change the ratio of any two eigenvalues of any matrix $M^{f}$ . In Theorem D.3 we also prove a similar result if $f$ is Lipschitz (and not necessarily real analytic). ", "page_idx": 5}, {"type": "text", "text": "Stable Rank Our first two results, Theorem 2.5 and Theorem 2.6, both ruled out approaches to generalizing the polynomial method. Finally, we find one important property of matrices for which we can strictly generalize the polynomial method: stable rank. ", "page_idx": 5}, {"type": "text", "text": "Definition 2.7. For a matrix $M\\in\\mathbb{R}^{n\\times n}$ , its stable rank is defined as $\\begin{array}{r}{\\mathrm{srank}(M):=\\frac{\\|M\\|_{F}^{2}}{\\|M\\|_{2}^{2}}}\\end{array}$ \u2225M\u2225F2 , where $\\|M\\|_{F}$ denotes the Frobenius norm of matrix $M$ and $\\|M\\|_{2}$ denotes the spectral norm of matrix $M$ . ", "page_idx": 5}, {"type": "text", "text": "It is known that $\\operatorname{srank}(M)\\,\\leq\\,\\operatorname{rank}(M)$ , but there are example matrices where $\\operatorname{srank}(M)\\ \\ll$ $\\mathrm{rank}(M)$ . Moreover, matrices with low stable rank can be manipulated quickly in many applications; for instance, low stable rank matrices are a useful tool in data mining and the study of Banach spaces [MSS17], and very efficient sketching methods are known for matrices with small stable rank [CNW15]. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2.8 (Informal statement of Theorem J.2). Let $M\\,\\in\\,\\mathbb{R}_{>0}^{n\\times n}$ be a matri\u221ax, and suppose $f:\\mathbb{R}_{>0}\\,\\rightarrow\\,\\mathbb{R}_{>0}$ has the property that for any entry $z$ of $M$ , $\\begin{array}{r}{\\frac{1}{\\sqrt{L}}\\cdot z\\,\\leq\\,f(z)\\,\\leq\\,\\sqrt{L}\\cdot z}\\end{array}$ for some $L\\in\\mathbb{R}_{>0}$ . Then, $\\operatorname{srank}(M^{f})\\leq L^{2}\\cdot\\operatorname{srank}(M)$ . ", "page_idx": 5}, {"type": "text", "text": "Consider, for instance, the matrices which arise in polynomial method applications [ACW16]; these are matrices $M\\,\\in\\,\\mathbb{R}^{n\\times n}$ where each entry is in the interval $[1,O(\\log n)]$ . For these matrices, functions like $f(x)\\,=\\,x^{c}$ , for any constant $c>0$ , which are not a polynomial when $c$ is not an integer, still preserve stable rank (they satisfy the condition of Theorem 2.8 with $L=\\mathrm{poly}\\log(n))$ . By contrast, such bounds on the entries of $M$ do not impact our earlier results, and so such functions do not preserve rank or approximately preserve rank for these matrices. ", "page_idx": 5}, {"type": "text", "text": "Unfortunately, it is not clear how to apply this to speed up the polynomial method applications we discussed earlier. Most known applications of stable rank require one to have access to the entire matrix $M^{f}$ (in order to, for instance, apply sketching), whereas we are aiming for algorithms whose running time is sublinear in the number of entries of $M^{f}$ . Nonetheless, this is an exciting avenue where one can strictly generalize the polynomial method, and we believe it will have interesting algorithmic applications, and further motivates algorithmic applications of stable rank. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2.8 tells us that functions which do not grow too quickly preserve stable rank, although the desired rate of growth depends on the matrix entries. We also prove a complementary result about functions which do grow very quickly: In Theorem J.5 we prove that any super-polynomial function which grows like $x^{\\bar{\\log}^{c}(x)}$ for any $c>0$ does not preserve low stable rank, and applying it entry-wise to an $n\\times n$ matrix of rank ${\\cal O}(\\log n)$ can result in a matrix of stable rank $>n-1$ . Hence, there is a limit to how much one could improve our Theorem 2.8. ", "page_idx": 5}, {"type": "text", "text": "3 Kernel Methods ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Our second main application of our techniques is to the study of kernel methods in machine learning. Much of the prior work on kernels methods focuses in the Euclidean distance setting. Our new result ", "page_idx": 5}, {"type": "text", "text": "shows how to classify kernels in the Manhattan distance setting. We start with defining positive definite kernel. ", "page_idx": 6}, {"type": "text", "text": "Definition 3.1 (Positive definite Manhattan/Euclidean kernel). A function $f$ is a positive definite $\\ell_{p}$ kernel if, for any $x_{1},\\ldots x_{n}\\in\\mathbb{R}^{d}$ for any $n$ and $d,$ , the matrix $M\\in\\mathbb{R}^{n\\times n}$ with ", "page_idx": 6}, {"type": "equation", "text": "$$\nM_{i,j}=f(\\|x_{i}-x_{j}\\|_{p})\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "is positive semi-definite. ", "page_idx": 6}, {"type": "text", "text": "For $p=1$ , we also say $f$ is a positive definite Manhattan kernel, and for $p=2$ we call it a positive definite Euclidean kernel. ", "page_idx": 6}, {"type": "text", "text": "Equivalently, $f$ is a positive definite $\\ell_{p}$ kernel if and only if there exists a function $F:\\mathbb{R}^{d}\\rightarrow\\mathcal{H}$ such that: $\\langle F(x),F(y)\\rangle=f(\\|x-y\\|_{p})$ for all $x,y\\in\\mathbb{R}^{d}$ for all $d$ . Note that $\\mathcal{H}$ represents Hilbert space. The proof of the equivalence can be found in [Sch42]. Positive definite kernels are used in machine learning to separate data embedded in $\\mathbb{R}^{d}$ using linear separator techniques, when the initial data is not linearly separable [Smo96, ${\\bf S}{\\bf S}{\\bf B}^{+}{\\bf97}$ , SOW01, SS01]. In other words, a positive definite kernel can map points in $\\mathbb{R}^{d}$ which are not linearly separable, to points in potentially higher dimensions which are linearly separable. Finding such an embedding is not an easy task in general, but kernel methods solve this problem. ", "page_idx": 6}, {"type": "text", "text": "Many regression algorithms require the kernel to be positive definite [Cut09, HTF09]. The key idea is to pick a function $f$ based on the application so that a function $F$ like the one in Definition 3.1 can be found which maps the data points to vectors of possible higher dimensions, after which linear separation can be performed efficiently on these higher dimensional points. ", "page_idx": 6}, {"type": "text", "text": "Interestingly, linear separator algorithms such as the widely used Support Vector Machines (SVMs) [CV95] can separate the data efficiently as long as $\\langle F(x),F(y)\\rangle$ is easily computed for any $x,y\\in\\mathbb{R}^{d}$ , even if $F$ itself cannot be easily computed. By definition of the positive-definite kernel $f$ , we know that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\langle F(x),F(y)\\rangle=f(\\|x-y\\|_{p}),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "which allows us to compute $\\langle F(x),F(y)\\rangle$ quickly by instead computing $f(\\|x-y\\|_{p})$ . In other words, in order to apply linear separator algorithms, it suffices to know that an $F$ exists, and not necessarily know what it is or how to compute it. ", "page_idx": 6}, {"type": "text", "text": "The main known result behind kernel methods is a full classification of all positive-definite Euclidean kernels in terms of completely monotone functions, which are defined as follows: ", "page_idx": 6}, {"type": "text", "text": "Definition 3.2 (Completely monotone functions [Ber29]). A function $f:\\mathbb{R}_{\\geq0}\\rightarrow\\mathbb{R}_{\\geq0}$ is completely monotone $i f$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n(-1)^{k}f^{(k)}(x)\\geq0\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "for all $k\\geq0,x>0,$ , and $f(0)\\geq\\operatorname*{lim}_{x\\to0^{+}}f(x)$ . ", "page_idx": 6}, {"type": "text", "text": "An example of a completely monotone function is $f(x)=e^{-x}$ . Prior work [Mer09, Sch42, Sch38, ${\\bf S}{\\bf S}{\\bf B}^{+}{\\bf97}$ , SSM98, SOW01] show\u221as that function $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ is a positive-definite Euclidean kernel (Definition 3.1) if and only if $f({\\sqrt{x}})$ is a completely monotone function (Definition 3.2). A natural question to ask is ", "page_idx": 6}, {"type": "text", "text": "Question 3.3. Is there a result that classifies all positive definite Manhattan kernels? ", "page_idx": 6}, {"type": "text", "text": "In our paper, we classify all positive-definite Manhattan kernels. These kernels are widely used in machine learning for physical and chemical applications [FLLA15, Lil18, LRRK15]. A notable example of such a kernel is the Laplace kernel $f_{\\sigma}(\\bar{x})=e^{-\\sigma x}$ which is commonly used in classification tasks [BMM18]. However, a full description of all positive-definite Manhattan kernels was not known before our work. In this work, we answer Question 3.3 positively: ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.4 (Main result, informal statement of Theorem G.2). $f$ is a positive definite Manhattan kernel (Definition 3.1) if only if $f(x)$ is completely monotone (Definition 3.2). ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.4 classifies all positive-definite kernels when the input distance is Manhattan. It was previously known that completely monotone functions are positive definite Manhattan kernels [Sch38, ", "page_idx": 6}, {"type": "text", "text": "Ass80, DL09], but it was not known these were the only such functions. Interestingly, our new classification is similar to the classification result for Euclidean kernels, but without a square root applied to the input. Prior to our result, one could have imagined that there are other positive definite Manhattan kernels to use in SVMs than were previously known. However, our result shows that there are no other such kernels. ", "page_idx": 7}, {"type": "text", "text": "We note that our proof techniques also give a new proof of the known result classifying all positive definite Euclidean kernels. This known result is a core insight at the heart of kernel methods in machine learning $[\\mathrm{SSB^{+}97}$ , SSM98], but traditional proofs tend to use methods related to infinite dimensional harmonic analysis [BCR84]. ", "page_idx": 7}, {"type": "text", "text": "4 Metric Transforms ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Our final application of our techniques is to metric transforms, a mathematical notion introduced by Von Neumann and Schoenberg [NS41]. ", "page_idx": 7}, {"type": "text", "text": "Definition 4.1 (Metric transform). Suppose $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ are semi-metric spaces4. Function $f$ transforms $\\mathcal{X}$ to $\\boldsymbol{\\wp}$ if, for any finite set $S\\subseteq\\mathcal{X}$ , there is a function $F:\\mathcal{X}\\to\\mathcal{Y}$ such that $f(d_{\\mathcal{X}}(x_{1},x_{2}))=$ $d{\\boldsymbol{y}}(F({\\boldsymbol{x}}_{1}),F({\\boldsymbol{x}}_{2})).$ , for all $x_{1},x_{2}\\in S$ . ", "page_idx": 7}, {"type": "text", "text": "As we discussed earlier, metric transforms arise naturally in many settings where one wants to transform a set of points from a metric space while maintaining some of the metric structure between them, and they have proven useful for algorithm design in many areas. ", "page_idx": 7}, {"type": "text", "text": "Typically we have particular metric spaces $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ of interest, and would like to determine which functions transform $\\mathcal{X}$ to $\\boldsymbol{\\wp}$ . This leads to the key question in metric transforms: ", "page_idx": 7}, {"type": "text", "text": "Question 4.2. For a given semi-metric space $\\mathcal{X}$ and a given semi-metric space $\\mathcal{V}$ , what is the full classification of functions $f$ that transform $\\mathcal{X}$ to $\\boldsymbol{\\wp}$ ? ", "page_idx": 7}, {"type": "text", "text": "Metric transforms in the special case where $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ are both Euclidean distances5 or close variants are well-studied. Related to Schoenberg and Von Neumann\u2019s work [NS41], Schoenberg [Sch38] classified all functions that transform Euclidean distances to Euclidean distances. One natural question arises: what is the theory of metric transforms for non-Euclidean metrics? ", "page_idx": 7}, {"type": "text", "text": "In the case when $\\mathcal{X}$ is Manhattan (or $\\ell_{1}$ ) distance, and $\\boldsymbol{\\wp}$ is Euclidean distance, Schoenberg [Sch38] provided a partial categorization of functions that transform Manhattan distance to Euclidean distance. This was followed by Assouad\u2019s work in 1980, which provided a partial categorization of functions that transform Manhattan distances to Manhattan distances [Ass80]. This setting is particularly well-motivated in physical applications. For instance, recent work [GSDV17] studied the problem of inferring a force vector given a collection of example configurations via kernel ridge regression; in order to encode certain desired symmetries (\u2018axis reflections\u2019) in the problem, Manhattan preserving functions must be used to define the kernel. Our work on metric transforms completes the partial categorizations of Schoenberg and Assouad, and proves their partial categorization is a full categorization. ", "page_idx": 7}, {"type": "text", "text": "Our main result about metric transforms is a complete classification of functions that transform Manhattan distances to Manhattan distances. First, we need to define Bernstein functions: ", "page_idx": 7}, {"type": "text", "text": "Definition 4.3 (Bernstein functions [Ber29]). A function $f:\\mathbb{R}_{\\geq0}\\rightarrow\\mathbb{R}_{\\geq0}$ is Bernstein if $f(0)=0$ and its derivative $f^{\\prime}$ is completely monotone (see Definition 3.2) when restricted to $\\mathbb{R}^{+}$ . Equivalently, $a$ function $f$ is Bernstein $i f.$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bullet\\ \\emph{l.}(-1)^{k}\\frac{\\mathrm{d}^{k}f(x)}{\\mathrm{d}x^{k}}\\leq0\\,f o r\\,a l l\\,k\\geq1,x\\geq0;}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "\u2022 3. $f(0)=0$ .6 ", "page_idx": 8}, {"type": "text", "text": "Now we are ready to state our main result: ", "page_idx": 8}, {"type": "text", "text": "Theorem 4.4 (Main result, classifying all Manhattan metric transforms, informal version and combination of Theorem E.2 and F.3). For a function $f:\\mathbb{R}_{\\geq0}\\rightarrow\\mathbb{R}_{\\geq0}.$ , the following are equivalent: ", "page_idx": 8}, {"type": "text", "text": "1. $f$ is Bernstein.   \n2. $f$ transforms Manhattan distances to Manhattan distances.   \n3. $f$ transforms Manhattan distances to squared Euclidean distances. ", "page_idx": 8}, {"type": "text", "text": "It was previously known that Bernstein functions transform Manhattan distances to Manhattan distances [Ass80], and that they transform Manhattan distances to squared Euclidean distances [Sch38], but in both cases, it was not previously known that these were the only such functions. It was previously conceivable that, in situations where one needs a metric transform involving Manhattan spaces, but Bernstein functions do not suffice, one could find other suitable metric transforms; our Theorem 4.4 rules out such a possibility. This also has a number of simple consequences, for instance: given any $n$ points $x_{1},\\ldots x_{n}$ in the metric space $(\\mathbb{R}^{d},\\ell_{1})$ for any $d$ , one can use our construction in Theorem 4.4 to explicitly calculate a finite dimensional embedding $F:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{2^{d}}$ such that $\\|F(x_{i})-F(x_{j})\\|_{1}=f(\\|x_{i}-x_{j}\\|_{1})$ . ", "page_idx": 8}, {"type": "text", "text": "5 Core Tool: Representation Theory of the Real Hyperrectangle ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Our core mathematical tool to tackle all three problems is a new technique we call the representation theory of the hyperrectangle. Given a $d$ dimensional hyperrectangle (which is just a high dimensional rectangle), consider the matrix $D$ where the $i j^{t h}$ entry of the matrix is the Manhattan distance between the $i^{t h}$ and $j^{t h}$ vertex of the hyperrectangle. We prove this matrix has three key properties: ", "page_idx": 8}, {"type": "text", "text": "1. It is a $2^{d}\\times2^{d}$ matrix whose rank is $d+1$ , and thus it is a low rank matrix. 2. This matrix is filled with Manhattan distances between points. 3. Applying $f$ entry-wise to this matrix does not change the eigenvectors of this matrix, which are always the columns of the so-called Hadamard matrices [HW78]. ", "page_idx": 8}, {"type": "text", "text": "We note that the last property is particularly useful for us: it allows us to provide a closed formula for the eigenvectors and eigenvalues of $D^{f}$ . This is particularly useful because all of our key questions (on low rank preservation, kernels, and metric transforms) can be viewed as questions about the eigenvalues of certain matrices after a function is applied entrywise. ", "page_idx": 8}, {"type": "text", "text": "The last property can be verified by linear algebra computation, but it can also be seen as a consequence of group representation theory. Thus, we call our approach the representation theory of the hyperrectangle. For proofs of all three properties, refer to Appendix B.1 and I. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We demonstrate that low-degree polynomials are the only functions that consistently result in a low-rank matrix when applied entry-wise to an existing low-rank matrix, and discuss applications to transformers and LLMs. Additionally, we classify all positive definite kernels that utilize Manhattan distance as their input, enhancing the theoretical framework for applications in various machine learning tasks. Furthermore, we provide a complete categorization of functions capable of transforming Manhattan distances into either Manhattan distances or Euclidean distances. We do all three tasks using a new linear algebraic tool called the representation theory of the hyperrectangle. Our findings not only advance the theoretical understanding of attention and kernel methods, but also open up new possibilities for their application in fields such as computational biology and algorithm design. This work completes the theoretical landscape of Manhattan to Manhattan metric transforms, and utilizes a sophisticated blend of mathematical techniques from several domains. ", "page_idx": 8}, {"type": "text", "text": "7 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we identify a few areas that require further exploration. Firstly, there remains a small constant-factor gap between Fact 2.1 and Theorem 2.5 that has not been fully explored; details can be found in the last paragraph of Section 2.1. Additionally, the application of the stable rank results presents an ongoing challenge, as discussed in the second-last paragraph of Section 2.2. Finally, our analysis primarily focuses on LLMs, kernel methods, and metric transforms, potentially limiting its applicability to other methodologies. ", "page_idx": 9}, {"type": "text", "text": "8 Societal Impacts ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper contributes positively by providing a deeper and more comprehensive study of kernel functions and completes the theory of Manhattan to Manhattan metric transforms, a problem that has persisted since 1980 due to Assouad\u2019s work. It opens up numerous algorithmic applications, potentially including large language models (LLMs), and offers a new direction for designing faster algorithms using stable rank results. However, the practical application of these results remains an open area, requiring additional time and effort to fully realize their potential. ", "page_idx": 9}, {"type": "text", "text": "9 Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The authors would like to thank Amol Aggarwal, Ainesh Bakshi, Robi Bhattacharjee, Jerry YaoChieh Hu, Rajesh Jayaram, Monique Laurent, Roie Levin, Yingyu Liang, Han Liu, Ryan O\u2019Donnell, Zhenmei Shi, Kevin Tian, Alex Wang, Yu Wang, Yufa Zhou, Lichen Zhang, Di Zhu, and Goran Zuzic for support and helpful comments. The authors would like to thank Yufa Zhou for writing the conclusion and checklist of the paper. This work was mostly done when Timothy Chu was at Carnegie Mellon University, and later at Google, and when Shyam Narayanan was a student at MIT. Josh Alman is supported in part by NSF Grant CCF-2238221 and a Google Research Scholar Award. Gary Miller is supported in part by NSF Grant CCF-1637523. Shyam Narayanan is supported by an NSF Graduate Fellowship and a Google Fellowship. For more information related to the paper and adjacent topics, see https://www.youtube.com/@zhaosong2031 and https: //space.bilibili.com/3546587376650961. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[AC06] Nir Ailon and Bernard Chazelle. Approximate nearest neighbors and the fast johnsonlindenstrauss transform. In Proceedings of the thirty-eighth annual ACM symposium on Theory of computing (STOC), pages 557\u2013563, 2006. ", "page_idx": 9}, {"type": "text", "text": "[ACSS20] Josh Alman, Timothy Chu, Aaron Schild, and Zhao Song. Algorithms and hardness for linear algebra on geometric graphs. In 2020 IEEE 61st Annual Symposium on Foundations of Computer Science (FOCS), pages 541\u2013552. IEEE, 2020.   \n[ACW16] Josh Alman, Timothy M Chan, and Ryan Williams. Polynomial representations of threshold functions and algorithmic applications. In 2016 IEEE 57th Annual Symposium on Foundations of Computer Science (FOCS), pages 467\u2013476. IEEE, 2016. [AIR18] Alexandr Andoni, Piotr Indyk, and Ilya Razenshteyn. Approximate nearest neighbor search in high dimensions. In Proceedings of the International Congress of Mathematicians: Rio de Janeiro 2018, pages 3287\u20133318. World Scientific, 2018.   \n$[\\mathrm{AKK}^{+}20]$ Thomas D Ahle, Michael Kapralov, Jakob BT Knudsen, Rasmus Pagh, Ameya Velingker, David P Woodruff, and Amir Zandieh. Oblivious sketching of high-degree polynomial kernels. In Proceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 141\u2013160. SIAM, 2020.   \n[AKR15] Alexandr Andoni, Robert Krauthgamer, and Ilya Razenshteyn. Sketching and embedding are equivalent for norms. In Proceedings of the Forty-seventh Annual ACM Symposium on Theory of Computing (STOC), pages 479\u2013488, 2015. [Alm19] Josh Alman. An illuminating algorithm for the light bulb problem. In SOSA. arXiv preprint arXiv:1810.06740, 2019.   \n$[\\mathrm{ALS}^{+}23]$ ] Josh Alman, Jiehao Liang, Zhao Song, Ruizhe Zhang, and Danyang Zhuo. Bypass exponential time preprocessing: Fast neural network training via weight-data correlation preprocessing. In NeurIPS, 2023. [Aro50] Nachman Aronszajn. Theory of reproducing kernels. Transactions of the American mathematical society, 68(3):337\u2013404, 1950. [AS23] Josh Alman and Zhao Song. Fast attention requires bounded entries. In NeurIPS, 2023. [AS24a] Josh Alman and Zhao Song. Fast rope attention: Combining the polynomial method and fast fourier transform. In manuscript, 2024. [AS24b] Josh Alman and Zhao Song. The fine-grained complexity of gradient computation for training large language models. In NeurIPS. arXiv preprint arXiv:2402.04497, 2024. [AS24c] Josh Alman and Zhao Song. How to capture higher-order correlations? generalizing matrix softmax attention to kronecker computation. In ICLR, 2024. [Ass80] Patrice Assouad. Plongements isom\u00e9triques dans l1: aspect analytique. Number, 14:1979\u20131980, 1980. [AW17] Josh Alman and Ryan Williams. Probabilistic rank and matrix rigidity. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages 641\u2013652, 2017.   \n[AWY14] Amir Abboud, Ryan Williams, and Huacheng Yu. More applications of the polynomial method to algorithm design. In Proceedings of the twenty-sixth annual ACM-SIAM symposium on Discrete algorithms (SODA), pages 218\u2013230. SIAM, 2014. [Bai99] Ren\u00e9 Baire. Sur les fonctions de variables r\u00e9elles. Annali di Matematica Pura ed Applicata (1898-1922), 3(1):1\u2013123, 1899. [BCR84] Christian Berg, Jens Peter Reus Christensen, and Paul Ressel. Harmonic analysis on semigroups: theory of positive definite and related functions, volume 100. Springer, 1984. [Ber29] Serge Bernstein. Sur les fonctions absolument monotones. Acta Mathematica, 52(1):1\u2013 66, 1929.   \n[BMM18] Mikhail Belkin, Siyuan Ma, and Soumik Mandal. To understand deep learning we need to understand kernel learning. In International Conference on Machine Learning, pages 541\u2013549. PMLR, 2018.   \n$[{\\mathbf{B}}{\\mathbf{M}}{\\mathbf{R}}^{+}20]$ Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems (NeurIPS), 33:1877\u20131901, 2020.   \n[BPSW21] Jan van den Brand, Binghui Peng, Zhao Song, and Omri Weinstein. Training (overparametrized) neural networks in near-linear time. In 12th Innovations in Theoretical Computer Science Conference (ITCS), 2021. [BTF04] Sabri Boughorbel, Jean-Philippe Tarel, and Francois Fleuret. Non-mercer kernels for svm object recognition. In BMVC, pages 1\u201310, 2004.   \n$[\\mathrm{CDW}^{+}21]$ Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher R\u00e9. Scatterbrain: Unifying sparse and low-rank attention approximation. In NeurIPS, 2021.   \n[CFW12] Ching-Shyang Chen, Chia-Ming Fan, and PH Wen. The method of approximate particular solutions for solving certain partial differential equations. Numerical Methods for Partial Differential Equations, 28(2):506\u2013522, 2012.   \n$[\\mathsf{C H C}^{+}10]$ Yin-Wen Chang, Cho-Jui Hsieh, Kai-Wei Chang, Michael Ringgaard, and Chih-Jen Lin. Training and testing low-degree polynomial data mappings via linear svm. Journal of Machine Learning Research, 11(4), 2010.   \n[CKPS16] Xue Chen, Daniel M Kane, Eric Price, and Zhao Song. Fourier-sparse interpolation without a frequency gap. In 57th Annual Symposium on Foundations of Computer Science (FOCS), pages 741\u2013750. IEEE, 2016.   \n$[\\mathrm{CLD}^{+}20]$ Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. In International Conference on Learning Representations (ICLR), 2020.   \n[CLP17] Ernie Croot, Vsevolod F Lev, and P\u00e9ter P\u00e1l Pach. Progression-free sets in are exponentially small. Annals of Mathematics, pages 331\u2013337, 2017.   \n[CMS20] Timothy Chu, Gary L. Miller, and Donald Sheehy. Exact computation of a manifold metric, via lipschitz embeddings and shortest paths on a graph. In SODA, 2020. [CN21] Yeshwanth Cherapanamjeri and Jelani Nelson. Terminal embeddings in sublinear time. In FOCS, 2021.   \n$[\\mathsf{C N D}^{+}22]$ Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.   \n[CNW15] Michael B Cohen, Jelani Nelson, and David P Woodruff. Optimal approximate matrix product in terms of stable rank. arXiv preprint arXiv:1507.02268, 2015. [Cop82] Don Coppersmith. Rapid multiplication of rectangular matrices. SIAM Journal on Computing, 11(3):467\u2013471, 1982. [Cri88] Frank Critchley. On certain linear mappings between inner-product and squareddistance matrices. Linear Algebra and its Applications, 105:91\u2013107, 1988.   \n[CUH15] Djork-Arn\u00e9 Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015. [Cut09] Marco Cuturi. Positive definite kernels in machine learning. arXiv preprint arXiv:0911.5367, 2009. [CV95] Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine learning, 20(3):273\u2013297, 1995. [CW16] Timothy M Chan and Ryan Williams. Deterministic apsp, orthogonal vectors, and more: Quickly derandomizing razborov-smolensky. In Proceedings of the twenty-seventh annual ACM-SIAM symposium on Discrete algorithms (SODA), pages 1246\u20131255. SIAM, 2016.   \n[DCLT18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pretraining of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. [DE19] Zeev Dvir and Benjamin L Edelman. Matrix rigidity and the croot-lev-pach lemma. Theory of Computing, 15:1\u20137, 2019. [Del73] Philippe Delsarte. An algebraic approach to the association schemes of coding theory. PhD thesis, Philips Research Laboratories, 1973.   \n[DKS10] Anirban Dasgupta, Ravi Kumar, and Tam\u00e1s Sarl\u00f3s. A sparse johnson: Lindenstrauss transform. In Proceedings of the forty-second ACM symposium on Theory of computing, pages 341\u2013350, 2010. [DL09] Michel Marie Deza and Monique Laurent. Geometry of Cuts and Metrics. Springer Publishing Company, Incorporated, 1st edition, 2009. [DL19] Zeev Dvir and Allen Liu. Fourier and circulant matrices are not rigid. In Computational Complexity Conference (CCC), volume 137, pages 17:1\u201317:23, 2019.   \n[DSWZ22] Yichuan Deng, Zhao Song, Omri Weinstein, and Ruizhe Zhang. Fast distance oracles for any symmetric norm. arXiv preprint arXiv:2205.14816, 2022. [EG17] Jordan S Ellenberg and Dion Gijswijt. On large subsets of with no three-term arithmetic progression. Annals of Mathematics, pages 339\u2013343, 2017.   \n$\\mathrm{[EGH^{+}11]}$ Pavel I Etingof, Oleg Golberg, Sebastian Hensel, Tiankai Liu, Alex Schwendner, Dmitry Vaintrob, and Elena Yudovina. Introduction to representation theory, volume 59. American Mathematical Soc., 2011. [FLH15] Aasa Feragen, Francois Lauze, and Soren Hauberg. Geodesic exponential kernels: When curvature and linearity conflict. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), pages 3032\u20133042, 2015.   \n[FLLA15] Felix Faber, Alexander Lindmaa, O Anatole von Lilienfeld, and Rickard Armiento. Crystal structure representations for machine learning models of formation energies. International Journal of Quantum Chemistry, 115(16):1094\u20131101, 2015. [Fro12] Georg Frobenius. \u00dcber matrizen aus nicht negativen elementen. ., 1912. [FS98] Carsten Franke and Robert Schaback. Solving partial differential equations by collocation using radial basis functions. Applied Mathematics and Computation, 93(1):73\u201382, 1998. [FS03] Fran\u00e7ois Fleuret and Hichem Sahbi. Scale-invariance of support vector machines based on the triangular kernel. In 3rd International Workshop on Statistical and Computational Theories of Vision, pages 1\u201313, 2003. [FZS21] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. arXiv preprint arXiv:2101.03961, 2021. [GE08] Yoav Goldberg and Michael Elhadad. splitsvm: fast, space-efficient, non-heuristic, polynomial kernel computation for nlp applications. In Proceedings of ACL-08: HLT, Short Papers, pages 237\u2013240, 2008. [GKR17] Dominique Guillot, Apoorva Khare, and Bala Rajaratnam. Preserving positivity for rank-constrained matrices. Transactions of the American Mathematical Society, 369(9):6105\u20136145, 2017.   \n$[\\mathrm{GLS^{+}}24]$ Jiuxiang Gu, Yingyu Liang, Zhizhou Sha, Zhenmei Shi, and Zhao Song. Differential privacy mechanisms in neural tangent kernel regression. arXiv preprint arXiv:2407.13621, 2024.   \n[GSDV17] Aldo Glielmo, Peter Sollich, and Alessandro De Vita. Accurate interatomic force fields via machine learning with covariant kernels. Physical Review B, 95(21):214302, 2017. [GSZ23] Yuzhou Gu, Zhao Song, and Lichen Zhang. Faster algorithms for structured linear and kernel support vector machines. arXiv preprint arXiv:2307.07735, 2023.   \n[HAS20] Insu Han, Haim Avron, and Jinwoo Shin. Polynomial tensor sketch for element-wise function of low-rank matrix. In International Conference on Machine Learning, pages 3984\u20133993. PMLR, 2020.   \n$[\\mathrm{HJK}^{+}23]$ ] Insu Han, Rajesh Jarayam, Amin Karbasi, Vahab Mirrokni, David P Woodruff, and Amir Zandieh. Hyperattention: Long-context attention in near-linear time. arXiv preprint arXiv:2310.05869, 2023.   \n[HLSL24] Jerry Yao-Chieh Hu, Thomas Lin, Zhao Song, and Han Liu. On computational limits of modern hopfield models: A fine-grained complexity analysis. In Forty-first International Conference on Machine Learning (ICML), 2024.   \n$[\\mathrm{HSK}^{+}24]$ Jerry Yao-Chieh Hu, Maojiang Su, En-Jui Kuo, Zhao Song, and Han Liu. Computational limits of low-rank adaptation (lora) for transformer-based models. arXiv preprint arXiv:2406.03136, 2024.   \n$[\\mathrm{HSM^{+}00}]$ Richard HR Hahnloser, Rahul Sarpeshkar, Misha A Mahowald, Rodney J Douglas, and H Sebastian Seung. Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit. Nature, 405(6789):947\u2013951, 2000.   \n$[\\mathrm{HSW}^{+}22]$ Baihe Huang, Zhao Song, Omri Weinstein, Hengjie Zhang, and Ruizhe Zhang. A dynamic fast gaussian transform. arXiv preprint arXiv:2202.12329, 2022. [HTF09] T Hastie, R Tibshirani, and J Friedman. Data mining, inference, and prediction. the elements of statistical learning. ., 2009. [HW78] A Hedayat and Walter Dennis Wallis. Hadamard matrices and their applications. The Annals of Statistics, 6(6):1184\u20131238, 1978.   \n$[\\mathrm{HWL}^{+}24]$ Jerry Yao-Chieh Hu, Weimin Wu, Zhuoru Li, Sophia Pi, , Zhao Song, and Han Liu. On statistical rates and provably efficient criteria of latent diffusion transformers (dits). In Thirty-eighth Conference on Neural Information Processing Systems (NeurIPS), 2024. [IMS17] Piotr Indyk, Ji\u02c7r\u00ed Matou\u0161ek, and Anastasios Sidiropoulos. low-distortion embeddings of finite metric spaces. In Handbook of discrete and computational geometry, pages 211\u2013231. Chapman and Hall/CRC, 2017. [JGH18] Arthur Jacot, Franck Gabriel, and Cl\u00e9ment Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in neural information processing systems (NeurIPS), pages 8571\u20138580, 2018. [KCC17] Irene Kaltenmark, Benjamin Charlier, and Nicolas Charon. A general framework for curve and surface comparison and registration with oriented varifolds. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), pages 3346\u20133355, 2017. [KKK18] Matti Karppa, Petteri Kaski, and Jukka Kohonen. A faster subquadratic algorithm for finding outlier correlations. ACM Transactions on Algorithms (TALG), 14(3):1\u201326, 2018. [KKL19] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In International Conference on Learning Representations (ICLR), 2019. [KM03] Taku Kudo and Yuji Matsumoto. Fast methods for kernel-based text analysis. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 24\u201331, 2003. [KMZ23] Praneeth Kacham, Vahab Mirrokni, and Peilin Zhong. Polysketchformer: Fast transformers via sketches for polynomial kernels. arXiv preprint arXiv:2310.01655, 2023.   \n[KUMH17] G\u00fcnter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Selfnormalizing neural networks. Advances in neural information processing systems, 30, 2017.   \n[KVPF20] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pages 5156\u20135165. PMLR, 2020. [KW71] George Kimeldorf and Grace Wahba. Some results on tchebycheffian spline functions. Journal of mathematical analysis and applications, 33(1):82\u201395, 1971. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "[KZR16] Soheil Kolouri, Yang Zou, and Gustavo K Rohde. Sliced wasserstein kernels for ", "page_idx": 14}, {"type": "text", "text": "probability distributions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5258\u20135267, 2016. [Lax02] Peter Lax. Functional Analysis. Wiley, 1st edition, 2002. [Lil18] O Anatole Von Lilienfeld. Quantum machine learning in chemical compound space. Angewandte Chemie International Edition, 57(16):4164\u20134169, 2018. [LLLH18] Shaogao Lv, Huazhen Lin, Heng Lian, and Jian Huang. Oracle inequalities for sparse additive quantile regression in reproducing kernel hilbert space. The Annals of Statistics, 46(2):781\u2013813, 2018. [LLSS24] Chenyang Li, Yingyu Liang, Zhenmei Shi, and Zhao Song. Exploring the frontiers of softmax: Provable optimization, applications in diffusion model, and beyond. arXiv preprint arXiv:2405.03251, 2024.   \n[LNRW19] Jerry Li, Aleksandar Nikolov, Ilya Razenshteyn, and Erik Waingarten. On mean estimation for general norms with statistical queries. In Conference on Learning Theory, pages 2158\u20132172. PMLR, 2019. [LRRK15] O Anatole Von Lilienfeld, Raghunathan Ramakrishnan, Matthias Rupp, and Aaron Knoll. Fourier series of atomic radial distribution functions: A molecular fingerprint for machine learning models of quantum chemical properties. International Journal of Quantum Chemistry, 115(16):1084\u20131093, 2015. $[{\\mathrm{LSS}}^{+}20]$ Jason D Lee, Ruoqi Shen, Zhao Song, Mengdi Wang, and Zheng Yu. Generalized leverage score sampling for neural networks. In NeurIPS, 2020. $[{\\mathrm{LSS}}^{+}24]$ Yingyu Liang, Zhizhou Sha, Zhenmei Shi, Zhao Song, and Yufa Zhou. Multi-layer transformers gradient can be approximated in almost linear time. arXiv preprint arXiv:2408.13233, 2024.   \n[LSSZ24a] Yingyu Liang, Zhenmei Shi, Zhao Song, and Yufa Zhou. Differential privacy of cross-attention with provable guarantee. arXiv preprint arXiv:2407.14717, 2024.   \n[LSSZ24b] Yingyu Liang, Zhenmei Shi, Zhao Song, and Yufa Zhou. Tensor attention training: Provably efficient learning of higher-order transformers. arXiv preprint arXiv:2405.16411, 2024. $[\\mathrm{LSZ}^{+}19]$ Xuanqing Liu, Si Si, Xiaojin Zhu, Yang Li, and Cho-Jui Hsieh. A unified framework for data poisoning attack to graph-based semi-supervised learning. In Advances in Neural Information Processing Systems (NeurIPS), 2019.   \n$[\\mathrm{LWD}^{+}23]$ ] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, et al. Deja vu: Contextual sparsity for efficient llms at inference time. In International Conference on Machine Learning, pages 22137\u201322176. PMLR, 2023. [Mer09] James Mercer. Functions of positive and negative type, and their connection the theory of integral equations. Philosophical transactions of the royal society of London. Series A, containing papers of a mathematical or physical character, 209(441-458):415\u2013446, 1909.   \n[MMMR18] Sepideh Mahabadi, Konstantin Makarychev, Yury Makarychev, and Ilya Razenshteyn. Nonlinear dimension reduction via outer bi-lipschitz extensions. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing (STOC), pages 1088\u20131101, 2018. [MMR19] Konstantin Makarychev, Yury Makarychev, and Ilya Razenshteyn. Performance of johnson-lindenstrauss transform for $\\mathbf{k}$ -means and $\\mathbf{k}$ -medians clustering. In Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing (STOC), pages 1027\u20131038, 2019.   \n[MOSW22] Alexander Munteanu, Simon Omlor, Zhao Song, and David P. Woodruff. Bounding the width of neural networks via coupled initialization A worst case analysis. In ICML, volume 162 of Proceedings of Machine Learning Research, pages 16083\u201316122. PMLR, 2022. [MPS16] Daniel Moeller, Ramamohan Paturi, and Stefan Schneider. Subquadratic algorithms for succinct stable matching. In International Computer Science Symposium in Russia, pages 294\u2013308. Springer, 2016.   \n[MSS17] Adam W Marcus, Daniel A Spielman, and Nikhil Srivastava. Interlacing families iii: Sharper restricted invertibility estimates. arXiv preprint arXiv:1712.07766, 2017. [NJW02] Andrew Y Ng, Michael I Jordan, and Yair Weiss. On spectral clustering: Analysis and an algorithm. In Advances in neural information processing systems (NeurIPS), pages 849\u2013856, 2002. [NN19] Shyam Narayanan and Jelani Nelson. Optimal terminal dimensionality reduction in euclidean space. In Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing (STOC), pages 1064\u20131069, 2019. [NS41] J. Von Neumann and I. J. Schoenberg. Fourier integrals and metric geometry. Transactions of the American Mathematical Society, 50(2):226\u2013251, 1941. [O\u2019D14] Ryan O\u2019Donnell. Analysis of Boolean Functions. Cambridge University Press, New York, NY, USA, 2014. [Orr96] Mark JL Orr. Introduction to radial basis function networks, 1996. [Par12] Pablo A. Parrilo. Algebraic techniques and semidefinite optimization, February 2012. [Per07] Oskar Perron. Zur theorie der matrices. Mathematische Annalen, 64(2):248\u2013263, 1907.   \n$[\\mathrm{RNS^{+}18}]$ Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018. [RR08] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in neural information processing systems (NIPS), pages 1177\u20131184, 2008. [RSZ22] Aravind Reddy, Zhao Song, and Lichen Zhang. Dynamic tensor product regression. In Conference on Neural Information Processing Systems (NeurIPS), pages 4791\u20134804, 2022.   \n[RWC+19] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   \n$[\\mathrm{RYW}^{+}19]$ Emily Reif, Ann Yuan, Martin Wattenberg, Fernanda B Viegas, Andy Coenen, Adam Pearce, and Been Kim. Visualizing and measuring the geometry of bert. Advances in Neural Information Processing Systems, 32, 2019. [Sch35] I. J. Schoenberg. Remarks to maurice frechet\u2019s article\" sur la definition axiomatique d\u2019une classe d\u2019espace distancies vector! ellement applicable sur l\u2019espace de hilbertl. Ann. of Math, 36:724\u2013732, 1935. [Sch37] I. J. Schoenberg. On certain metric spaces arising from euclidean spaces by a change of metric and their imbedding in hilbert space. Annals of Mathematics, 38(4):787\u2013793, 1937. [Sch38] I. J. Schoenberg. Metric spaces and positive definite functions. Trans. Amer. Math. Soc., 44:522\u2013536, 1938. [Sch42] I Schoenberg. Positive definite functions on spheres. Duke Math. J, 1:172, 1942. [Smo96] Alex J Smola. Regression estimation with support vector learning machines. PhD thesis, Master\u2019s thesis, Technische Universit\u00e4t M\u00fcnchen, 1996. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "[Sou10] C\u00e9sar R Souza. Kernel functions for machine learning applications. Creative Commons Attribution-Noncommercial-Share Alike, 3:29, 2010.   \n[SOW01] Alex J Smola, Zoltan L Ovari, and Robert C Williamson. Regularization with dotproduct kernels. In Advances in neural information processing systems (NeurIPS), pages 308\u2013314, 2001. [SS01] Bernhard Scholkopf and Alexander J Smola. Learning with kernels: support vector machines, regularization, optimization, and beyond. MIT press, 2001. $[\\mathrm{SSB^{+}97}]$ ] Bernhard Scholkopf, Kah-Kay Sung, Christopher JC Burges, Federico Girosi, Partha Niyogi, Tomaso Poggio, and Vladimir Vapnik. Comparing support vector machines with gaussian kernels to radial basis function classifiers. IEEE transactions on Signal Processing, 45(11):2758\u20132765, 1997. [SSL24] Yiyou Sun, Zhenmei Shi, and Yixuan Li. A graph-theoretic framework for understanding open-world semi-supervised learning. Advances in Neural Information Processing Systems, 36, 2024.   \n[SSLL23] Yiyou Sun, Zhenmei Shi, Yingyu Liang, and Yixuan Li. When and how does known class help discover unknown ones? provable understanding through spectral analysis. In International Conference on Machine Learning, pages 33014\u201333043. PMLR, 2023. [SSM98] Alex J Smola, Bernhard Sch\u00f6lkopf, and Klaus-Robert M\u00fcller. The connection between regularization operators and support vector kernels. Neural networks, 11(4):637\u2013649, 1998.   \n[SSWZ23a] Tamas Sarlos, Xingyou Song, David Woodruff, and Richard Zhang. Hardness of low rank approximation of entrywise transformed matrix products. Advances in Neural Information Processing Systems, 36, 2023.   \n[SSWZ23b] Zhao Song, Baocheng Sun, Omri Weinstein, and Ruizhe Zhang. Quartic samples suffice for fourier interpolation. In FOCS, 2023.   \n[SWYZ21] Zhao Song, David P. Woodruff, Zheng Yu, and Lichen Zhang. Fast sketching of polynomial kernels of polynomial degree. In ICML, 2021. [SYZ21] Zhao Song, Shuo Yang, and Ruizhe Zhang. Does preprocessing help training overparameterized neural networks? Advances in Neural Information Processing Systems, 34, 2021. [SYZ24] Zhao Song, Junze Yin, and Lichen Zhang. Solving attention kernel regression problem via pre-conditioner. In International Conference on Artificial Intelligence and Statistics (AISTATS), pages 208\u2013216. PMLR, 2024. [SZZ24] Zhao Song, Lichen Zhang, and Ruizhe Zhang. Training multi-layer over-parametrized neural network in subquadratic time. In ITCS, 2024.   \n$[\\mathrm{TBY^{+}19}]$ Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan Salakhutdinov. Transformer dissection: An unified understanding for transformer\u2019s attention via the lens of kernel. In Kentaro Inui, Jing Jiang, Vincent $\\mathrm{Ng}$ , and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 4343\u20134352. Association for Computational Linguistics, 2019. [Val12] Gregory Valiant. Finding correlations in subquadratic time, with applications to learning parities and juntas. In 2012 IEEE 53rd Annual Symposium on Foundations of Computer Science (FOCS), pages 11\u201320, 2012. [vL07] Ulrike von Luxburg. A tutorial on spectral clustering, 2007.   \n$[\\mathrm{VSP^{+}17}]$ Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems (NeurIPS), 30, 2017. [Wil18] R Ryan Williams. Faster all-pairs shortest paths via circuit complexity. SIAM Journal on Computing, 47(5):1965\u20131985, 2018.   \n$[\\mathbf{W}\\mathbf{L}\\mathbf{K}^{+}20]$ Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020. [WZF05] Liwei Wang, Yan Zhang, and Jufu Feng. On the euclidean distance of images. IEEE transactions on pattern analysis and machine intelligence, 27(8):1334\u20131339, 2005. [XZZ18] Chang Xiao, Peilin Zhong, and Changxi Zheng. Bourgan: generative networks with metric embeddings. In Proceedings of the 32nd International Conference on Neural Information Processing Systems (NeurIPS), pages 2275\u20132286, 2018. [Zha22] Lichen Zhang. Speeding up optimizations via data structures: Faster search, sample and maintenance. PhD thesis, Master\u2019s thesis, Carnegie Mellon University, 2022.   \n[ZHDK23] Amir Zandieh, Insu Han, Majid Daliri, and Amin Karbasi. Kdeformer: Accelerating transformers via kernel density estimation. In ICML. arXiv preprint arXiv:2302.02451, 2023. [Zhu05a] Xiaojin Zhu. Semi-supervised learning with graphs. PhD thesis, Carnegie Mellon University, language technologies institute, school of Computer Science, 2005. [Zhu05b] Xiaojin Jerry Zhu. Semi-supervised learning literature survey. Technical report, University of Wisconsin-Madison Department of Computer Sciences, 2005. [ZLZ24] Shijun Zhang, Jianfeng Lu, and Hongkai Zhao. Deep network approximation: Beyond relu to diverse activation functions. Journal of Machine Learning Research, 25(35):1\u2013 39, 2024.   \n$[Z\\mathrm{RG}^{+}22]$ Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.   \n$[Z S Z^{+}23]$ ] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher R\u00e9, Clark Barrett, Zhangyang Wang, and Beidi Chen. H _2 o: Heavy-hitter oracle for efficient generative inference of large language models. In NeurIPS. arXiv preprint arXiv:2306.14048, 2023.   \n$[Z Z\\mathrm{P}^{+}21]$ Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pages 11106\u201311115, 2021. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "Appendix Contents ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1 Introduction 2 ", "page_idx": 18}, {"type": "text", "text": "2.1 Converse for the Polynomial Method . 4   \n2.2 Weaker Polynomial Methods . . 5 ", "page_idx": 18}, {"type": "text", "text": "3 Kernel Methods 6 ", "page_idx": 18}, {"type": "text", "text": "4 Metric Transforms 8 ", "page_idx": 18}, {"type": "text", "text": "5 Core Tool: Representation Theory of the Real Hyperrectangle 9 ", "page_idx": 18}, {"type": "text", "text": "6 Conclusion 9 ", "page_idx": 18}, {"type": "text", "text": "7 Limitations 10 ", "page_idx": 18}, {"type": "text", "text": "8 Societal Impacts 10 ", "page_idx": 18}, {"type": "text", "text": "9 Acknowledgments 10 ", "page_idx": 18}, {"type": "text", "text": "A Preliminaries 21 ", "page_idx": 18}, {"type": "text", "text": "A.1 Notations 21   \nA.2 Definitions . 21   \nA.3 Alternate Classifications of Completely Monotone and Bernstein Functions 22   \nA.4 Metric Hierarchies 23   \nA.5 Negative Type Metrics and Euclidean Embeddability 24   \nA.6 Schur\u2019s Lemma for Abelian Groups 24   \nA.7 Baire Category Theorem 24   \nA.8 Applications of Polynomial Methods 25   \nB Technique Overview 26   \nB.1 Starting Point: Eigenvalues of the Kernel Matrix of a Hyperrectangle . . . 26   \nB.2 Polynomial Method Converse . . 27   \nB.2.1 Exact Low Rank 27   \nB.2.2 Approximate Low Rank 27   \nB.3 Metric Transforms 28   \nB.4 Kernel Methods 28   \nC Polynomial Method Converse 29   \nC.1 Preliminaries 29   \nC.2 Functions with Algebraically Zero Eigenvalues 29   \nC.3 Eigenvalues of Low Rank Preserving Functions 30   \nC.4 Bridging Eigenvalues and Finite Differences . . 32   \nC.5 Function Sums and Finite Differences 33   \nC.6 No Functions Other Than Polynomials Preserve Low Rank 34   \nC.7 Proof of Continuity . . 34   \nC.8 Comparison with Prior Work 35 ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "D Approximate Polynomial Method Converse 36 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "D.1 Main Approximate Result . 37   \nD.2 Real Analytic Functions 37   \nD.3 Lipschitz Functions 38 ", "page_idx": 19}, {"type": "text", "text": "E Transforming Manhattan to Euclidean 38 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "E.1 Manhattan to Euclidean Transforms are Increasing 39   \nE.2 Manhattan to Euclidean Transforms are Bernstein 39   \nE.3 Manhattan to Euclidean Transforms are Bounded 40 ", "page_idx": 19}, {"type": "text", "text": "F Transforming Manhattan to Manhattan 41 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "F.1 Explicit Embeddings for Manhattan to Euclidean Transforms 41 F.2 Manhattan to Manhattan Transforms are Equivalent to Manhattan to SquaredEuclidean Transforms . 41 F.3 Metric Transforms for Distances with Group Symmetries 42 ", "page_idx": 19}, {"type": "text", "text": "G Positive Definite Manhattan Kernels 42 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "G.1 Positive Definite Manhattan Kernels map Positive Reals to Positive Reals 42   \nG.2 Positive Definite Manhattan Kernels are Completely Monotone . . . 42 ", "page_idx": 19}, {"type": "text", "text": "H Positive Definite Euclidean Kernels 43 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "I Eigenvalue of Kernels from the Hyperrectangle 43 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "I.1 Matrices with Reflectional Symmetries have Hadamard Eigenvectors . . . 43   \nI.2 Eigenvalues of Kernels from the Hyperrectangle, Restated 43 ", "page_idx": 19}, {"type": "text", "text": "J Converse to Stable Rank 44 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "J.1 Lipschitz functions preserve stable rank 44   \nJ.2 Fast-Growing functions do not preserve stable rank 46 ", "page_idx": 19}, {"type": "text", "text": "Roadmap. In Section A, we define notations, and provide several basic definitions and fundamental tools. In Section B, we provide several techniques used to prove the theorems. In Section C, we prove the converse to the polynomial method, proving Theorem 2.5. In Section D, we prove an approximate converse to the polynomial method, proving Theorem 2.6. In Section E, we prove condition 1 and condition 2 in Theorem 4.4 on Manhattan metric transforms are equivalent. In Section F, we prove condition 2 and condition 3 in Theorem 4.4 are equivalent. Overall, Section E and Section F together prove Theorem 4.4. In Section G, we have a proof of Theorem 3.4 about Manhattan distance kernels. In Section H, we have a new proof for the known, full classification of Euclidean distance kernels. In Section I, we prove a slightly different restatement of Lemma B.1. We show our results about stable rank in Section J. ", "page_idx": 20}, {"type": "text", "text": "A Preliminaries ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "This section is organized as follows: ", "page_idx": 20}, {"type": "text", "text": "\u2022 In Section A.1, we define several basic notations.   \n\u2022 In Section A.2, we provide some definitions piecewise functions, open/closed/dense sets, real analytic functions, and finite differences.   \n\u2022 In Section A.3, we provide some previous work about the classifications of completely monotone and Bernstein function.   \n\u2022 In Section A.4, we state well-known results about metric hierarchies.   \n\u2022 In Section A.5, we define negative metrics and euclidean embeddability.   \n\u2022 In Section A.6, we present previous work about representation theory tools.   \n\u2022 In Section A.7, we present the Baire category theorem.   \n\u2022 In Section A.8, we discuss some applications of polynomial methods. ", "page_idx": 20}, {"type": "text", "text": "A.1 Notations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "For a vector $x$ , we use $\\|{\\boldsymbol{x}}\\|_{1}$ to denote the entry-wise $\\ell_{1}$ norm of $x$ . We use $\\|{\\boldsymbol{x}}\\|_{2}$ to denote the entry-wise $\\ell_{2}$ norm of $x$ . We use $\\|x\\|_{\\infty}$ to denote the $\\ell_{\\infty}$ norm of $x$ . For two vectors $a,b$ , we use $\\langle a,b\\rangle$ to denote the inner product between $a$ and $b$ . For a vector $x$ , we use $x^{\\top}$ to denote the transpose of $x$ . For any matrix $A$ , we use $\\lambda_{i}$ \u2019s to denote its eigenvalues. For any square matrix $A$ , we use $\\operatorname*{det}(A)$ to denote its determinant. ", "page_idx": 20}, {"type": "text", "text": "For any $d\\geq1$ , we define Hadamard matrix $H_{d}\\in\\mathbb{R}^{2^{d}\\times2^{d}}$ as follows $H_{d}={\\binom{H_{d-1}}{H_{d-1}}}\\quad{\\cal H}_{d-1}\\,\\,\\nonumber$ and $H_{0}=1.$ . ", "page_idx": 20}, {"type": "text", "text": "Often times in our proof, we may say things like \u201clet $x_{1},\\ldots.x_{2^{d}}$ be the corners of a $d$ dimensional hyperrectangle\u201d. For these statements to make sense, we must specify which corner $x_{i}$ refers to. Scale the $d$ dimensional hyperrectangle to be an axis-aligned hypercube, and place one of the hypercube corners at the origin. Each corner then has a binary number $b$ as its coordinate bit string. We let $x_{b+1}$ refer to the original hyperrectangle corner corresponding to $b$ . ", "page_idx": 20}, {"type": "text", "text": "A.2 Definitions ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Piecewise function A piecewise function is a function defined by multiple sub-functions, where each sub-function applies to a different interval in the domain. ", "page_idx": 20}, {"type": "text", "text": "Open, closed and dense set Open sets are a generalization of open intervals in the real line. In a metric space (with a pre-defined distance function) open sets are the sets that, with every point $P$ , contain all points that are sufficiently near to $P$ (that is, all points whose distance to $P$ is less than some value depending on $P$ ). ", "page_idx": 20}, {"type": "text", "text": "A closed set is a set whose complement is an open set. A set that is closed under an operation or collection of operations is said to satisfy a closure property. ", "page_idx": 20}, {"type": "text", "text": "A subset $A$ of a topological space $X$ is called dense (in $X$ ) if every point $x$ in $X$ either belongs to $A$ or is a limit point of $A$ ; that is, the closure of $A$ constitutes the whole set $X$ . ", "page_idx": 21}, {"type": "text", "text": "The interior of a subset $S$ of a topological space $X$ is the union of all subsets of $S$ that are open in $X$ . ", "page_idx": 21}, {"type": "text", "text": "Real Analytic Formally, a function $f$ is real analytic on an open set $D$ in the real line if for any $x_{0}\\in D$ one can write ", "page_idx": 21}, {"type": "equation", "text": "$$\nf(x)=\\sum_{n=0}^{\\infty}a_{n}(x-x_{0})^{n}=a_{0}+a_{1}(x-x_{0})+a_{2}(x-x_{0})^{2}+a_{3}(x-x_{0})^{3}+\\cdot\\cdot\\cdot\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "in which the coefficients $a_{0},a_{1},\\cdot\\cdot\\cdot$ are real numbers and the series is convergent to $f(x)$ for $x$ in a neighborhood of $x_{0}$ . ", "page_idx": 21}, {"type": "text", "text": "The following conditions are equivalent: ", "page_idx": 21}, {"type": "text", "text": "\u2022 $f$ is real analytic on an open set $D$ .   \n\u2022 There is a complex analytic extension of $f$ to an open set $G\\subset\\mathbb{C}$ which contains $D$ .   \n\u2022 $f$ is real smooth and for every compact set $K\\subset D$ there exists a constant $C$ such that for every $x\\in K$ and every non-negative integer $k$ the following bound holds $|{\\frac{\\mathrm{d}^{k}f}{\\mathrm{d}x^{k}}}(x)|\\leq$ $C^{k+1}k!$ . Finite Difference A finite difference is a mathematical expression of the form $f(x+b)-f(x+a)$ .   \nThree basic types are commonly considered: forward, backward, and central finite differences. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "A forward difference, denoted $\\Delta_{h}[f]$ , of a function $f$ is a function defined as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Delta_{h}[f](x)=f(x+h)-f(x).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "A backward difference uses the function values at $x$ and $x-h$ , instead of the values at $x+h$ and $x$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\nabla_{h}[f](x)=f(x)-f(x-h)=\\Delta_{h}[f](x-h).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Finally, the central difference is given by ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\delta_{h}[f](x)=f(x+h/2)-f(x-h/2)=\\Delta_{h}[f](x-h/2).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "In this paper, we will mainly focus on foward difference. ", "page_idx": 21}, {"type": "text", "text": "We use $\\Delta_{h}^{d}[f](x)$ to denote $d$ -th difference for any $h\\in\\mathbb{R}^{d}$ at $x\\in\\mathbb{R}$ . For each $i\\in[d]$ , we use $h_{i}$ to denote the $i$ -th entry of $h$ . For each $j\\in[d]$ , the $j$ -th finite difference can be written as the following recursive way. For $j=1$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Delta_{h_{1}}^{2}[f](x)=f(x+h_{1})-f(x).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For $j=2$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Delta_{h_{1},h_{2}}^{2}[f](x)=\\Delta_{h_{1}}[f](x+h_{2})-\\Delta_{h_{1}}[f](x).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For each $j\\in[d]$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Delta_{h_{1},h_{2},\\cdots,h_{j}}^{j}[f](x)=\\Delta_{h_{1},h_{2},\\cdots h_{j-1}}^{j-1}[f](x+h_{j})-\\Delta_{h_{1},h_{2},\\cdots h_{j-1}}^{j-1}[f](x).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We will frequently apply these finite differences to functions $g:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ of the form $g(a)=f(\\langle a,1\\rangle)$ for a function $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ or similar. In these cases, we will abuse notation and write $\\Delta_{\\varepsilon}^{d}[f](\\langle a,1\\rangle)$ to refer to $\\Delta_{\\varepsilon}^{d}[g](a)$ . ", "page_idx": 21}, {"type": "text", "text": "A.3 Alternate Classifications of Completely Monotone and Bernstein Functions ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Here we recall the classical Bernstein Theorem from analysis constructively classifying completely monotone (Definition 3.2) and Bernstein functions (Definition 4.3). ", "page_idx": 21}, {"type": "text", "text": "Proposition A.1 (Chapter 14, Theorems 3 and 6 in [Lax02]). For a function $f:\\mathbb{R}_{>0}\\rightarrow\\mathbb{R}_{\\geq0}$ , the following are equivalent: ", "page_idx": 21}, {"type": "text", "text": "1. $f$ is completely monotone. ", "page_idx": 22}, {"type": "text", "text": "2. Letting $(D_{a}f)(x)=f(x+a)-f(x),$ , for any $(a_{1},\\ldots,a_{n})$ non-negative we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n(-1)^{n}\\left(\\prod_{i=1}^{n}D_{a_{i}}\\right)f(x)\\geq0\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for all $x>0$ . ", "page_idx": 22}, {"type": "text", "text": "3. There exists a positive finite measure $\\mu$ on $\\mathbb{R}_{\\geq0}$ such that ", "page_idx": 22}, {"type": "equation", "text": "$$\nf(x)=\\int_{0}^{\\infty}e^{-t x}\\mathrm{d}\\mu(t),\\quad x>0.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The part 2 of Proposition A.1 is essentially the definition we gave for completely monotone, except that it does not assume any smoothness or even continuity a priori. The third shows that all completely monotone functions are in fact mixtures of decaying exponentials. From the above one easily derives a corresponding classification of Bernstein functions. If $f$ also has 0 in its domain, then the above result applies the same way, however (with the same measure $\\mu$ as in part 3 of Proposition A.1) we have ", "page_idx": 22}, {"type": "equation", "text": "$$\nf(0)\\geq\\mu(\\mathbb{R}_{\\geq0})\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "since we did not require any continuity at 0. ", "page_idx": 22}, {"type": "text", "text": "Proposition A.2 (Theorem 6.7 in [BCR84]). For a function $f:\\mathbb{R}_{\\geq0}\\rightarrow\\mathbb{R}_{\\geq0}$ with $f(0)=0,$ , the following are equivalent: ", "page_idx": 22}, {"type": "text", "text": "1. $f$ is Bernstein. ", "page_idx": 22}, {"type": "text", "text": "2. Letting $(D_{a}f)(x)=f(x+a)-f(x),$ , for any $(a_{1},\\ldots,a_{n})$ non-negative we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n(-1)^{n}\\left(\\prod_{i=1}^{n}D_{a_{i}}\\right)f(x)\\leq0,\\quad x>0.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "3. There exists a positive measure $\\mu$ on $\\mathbb{R}^{+}$ and $a,b\\ge0$ such that ", "page_idx": 22}, {"type": "equation", "text": "$$\nf(x)=a+b x+\\int_{\\mathbb{R}^{+}}(1-e^{-t x})\\mathrm{d}\\mu(t),\\quad x>0.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Here \u00b5 must satisfy $\\begin{array}{r}{\\int_{\\mathbb{R}_{+}}\\operatorname*{min}\\{1,t\\}\\mathrm{d}\\mu(t)<\\infty}\\end{array}$ ", "page_idx": 22}, {"type": "text", "text": "Due to the second criterion just above, Bernstein functions are also sometimes called completely alternating. We remark that these results apply more generally in the setting of abelian semigroups, where the integral is taken over a measure on the space of positive characters. This general point of view is explained in [BCR84, Chapter 6], and applies, for instance, to the semigroup of compact subsets of $\\mathbb{R}$ under union. ", "page_idx": 22}, {"type": "text", "text": "A.4 Metric Hierarchies ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Here are well-known facts we will use throughout our proof: ", "page_idx": 22}, {"type": "text", "text": "Lemma A.3. For any n points $x_{1},\\ldots x_{n}$ in $\\ell_{1}$ , there exist n points $y_{1},\\ldots y_{n}$ such that $\\|x_{i}-x_{j}\\|_{1}=$ $\\lVert y_{i}-y_{j}\\rVert_{1}$ , and $y_{1},\\ldots y_{n}$ are a subset of corners of a $d$ dimensional hyperrectangle for some $d$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. This follows from the equivalence of the cut cone and $\\ell_{1}$ distance (Theorem 4.2.2 in [DL09]). ", "page_idx": 22}, {"type": "text", "text": "Lemma A.4. The squared Euclidean distance between points in the corners of a hyperrectangle isometrically embeds into Manhattan distance. ", "page_idx": 22}, {"type": "text", "text": "Lemma A.5. Manhattan distances embed isometrically into squared Euclidean distances. ", "page_idx": 23}, {"type": "text", "text": "Proof. This follows from Corollary 6.1.4 and Lemma 6.1.7 in [DL09]. ", "page_idx": 23}, {"type": "text", "text": "A.5 Negative Type Metrics and Euclidean Embeddability ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We now present a criterion by Schoenberg [Sch35] on when a metric is isometrically embeddable into squared Euclidean distances7. ", "page_idx": 23}, {"type": "text", "text": "Definition A.6 (negative type). A matrix $D$ is iff $x^{\\top}D x\\leq0$ for all x\u22a51. ", "page_idx": 23}, {"type": "text", "text": "Lemma A.7 (Schoenberg [Sch35]). Consider $x_{1},\\ldots,x_{n}$ where $d_{i,j}$ is the distance between $x_{i}$ and $x_{j}$ . Let $D$ be an n by $n$ matrix where $D_{i,j}=d_{i,j}^{2}$ . The distances $d_{i,j}$ are isometrically embeddable into Euclidean space iff the matrix $D$ is negative type. ", "page_idx": 23}, {"type": "text", "text": "We note that if $D$ happens to have the all ones vector 1 as an eigenvector, we have a simpler criterion for testing if $D$ is negative type: ", "page_idx": 23}, {"type": "text", "text": "Lemma A.8 (Schoenberg Variant). Consider $x_{1},\\ldots,x_{n}$ where $d_{i,j}$ is the distance between $x_{i}$ and $x_{j}$ . Let $D$ be an n by $n$ matrix where $D_{i,j}=d_{i,j}^{2}$ . ", "page_idx": 23}, {"type": "text", "text": "If the all ones vector is an eigenvector of $D$ , then the $d_{i,j}$ are isometrically embeddable into Euclidean space iff every eigenvalue of $D$ , excluding the eigenvalue correseponding to the all ones vector, is non-positive. ", "page_idx": 23}, {"type": "text", "text": "Proof. Lemma A.8 follows from Lemma A.7 and the fact that every symmetric matrix has an orthonormal set of eigenvectors. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "If $d_{i j}$ is isometrically embeddable into Euclidean space, we can find an explicit embedding: ", "page_idx": 23}, {"type": "text", "text": "Lemma A.9. Consider $x_{1},\\ldots x_{n}$ where $d_{i,j}$ is the distance between $x_{i}$ and $x_{j}$ . Let $D$ be the matrix where $D_{i,j}=d_{i,j}^{2}$ . Let $\\Pi$ be the projection matrix off the all ones vector, i.e., \u03a0 can be expressed explicitly as $I-J/n$ , where $J$ is the $n\\times n$ all-ones matrix, and $I$ is identity matrix. Let $M:=-\\textstyle{\\frac{1}{2}}\\Pi D\\Pi$ . $I f\\,y_{1},\\ldots\\,y_{n}$ are such that $\\lVert y_{i}-y_{j}\\rVert_{2}=d_{i,j}$ and $\\textstyle\\sum_{i=1}^{n}y_{i}=0,$ , then $M_{i,j}=\\langle y_{i},y_{j}\\rangle$ . Moreover, if $M=U^{\\top}U$ for some $U$ , then the columns of $U$ are an embedding of $x_{1},\\ldots x_{n}$ into Euclidean space. ", "page_idx": 23}, {"type": "text", "text": "This follows from Eq. 2 in $\\mathrm{[Cri88]}$ . A longer exposition of the link between distance matrices and inner product matrices can be found in [Cri88]. ", "page_idx": 23}, {"type": "text", "text": "A.6 Schur\u2019s Lemma for Abelian Groups ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We present Schur\u2019s lemma for Abelian groups $G$ . Schur\u2019s lemma is one of the cornerstones of representation theory $\\mathrm{[EGH^{+}11]}$ . ", "page_idx": 23}, {"type": "text", "text": "Lemma A.10 (Schur\u2019s lemma for Abelian groups). If $G$ is a finite Abelian group of $n\\times n$ matrices under multiplication, and $M$ is an $n\\times n$ diagonalizable matrix satisfying $M g=g M$ , for all $g\\in G$ , then there exists a set of linearly independent vectors $v_{1},\\ldots v_{n}$ that are eigenvectors of $M$ and all $g\\in G$ . In other words, $M$ and $G$ are simultaneously diagonalizable. ", "page_idx": 23}, {"type": "text", "text": "Schur\u2019s Lemma will be useful in proving our key result about representation theory of the real hyperrectangle, or Lemma B.1. ", "page_idx": 23}, {"type": "text", "text": "A.7 Baire Category Theorem ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The Baire category theorem (BCT) is an important result in general topology and functional analysis. ", "page_idx": 23}, {"type": "text", "text": "A Baire space is a topological space with the property that for each countable collection of open dense sets $(U_{n})_{n=1}^{\\infty}$ , their intersection $\\bigcap_{n\\in\\mathbb{N}}U_{n}$ is dense. ", "page_idx": 23}, {"type": "text", "text": "Theorem A.11 (Baire category theorem [Bai99]). Every complete pseudometric space is a Baire space. Every locally compact Hausdorff space is a Baire space. ", "page_idx": 24}, {"type": "text", "text": "A.8 Applications of Polynomial Methods ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Attention Computation: Question 2.3 is also important to the theory of polynomial kernels. A common computational task which arises when training transformers is to calculate the \u2018self attention\u2019 $[\\mathrm{VSP^{+}17}]$ . Recently, [ZHDK23, AS23] define a formal math computation problem for this attention mechanism. Formally, in this task, we are given three matrices $Q,\\mathbf{\\dot{K}},V\\in\\dot{\\mathbb{R}}^{n\\times d}$ where $n\\gg d,^{\\nmid}$ 8 and we would like to compute $(Q K^{\\top})^{f}\\cdot V$ where $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ is a non-linear function that we apply entry-wise to the matrix $A B^{\\top}\\in\\mathbb{R}^{n\\times n}$ , then we multiply the result on the right by $V$ . In many applications, $f$ is the soft-max function, which can be mathematically described as follows (see [ZHDK23, AS23]): ", "page_idx": 24}, {"type": "equation", "text": "$$\n(Q K^{\\top})^{f}:=D^{-1}\\exp(Q K^{\\top}),\\;\\;\\;\\mathrm{where}\\;\\;D:=\\mathrm{diag}(\\exp(Q K^{\\top}){\\bf1}_{n})\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Here $\\exp()$ is an entry-wise function that $\\mathrm{exp}(Q K^{\\top})_{i,j}=\\mathrm{exp}((Q K^{\\top})_{i,j})$ for all $i,j\\in[n]\\times[n]$ , $D\\in\\mathbb{R}^{n\\times n}$ is a diagonal matrix, ${\\bf1}_{n}$ is a vector that all entries are ones. ", "page_idx": 24}, {"type": "text", "text": "Naively evaluating $(Q K^{\\top})^{f}\\cdot V$ takes time $O(n^{2}d)$ (without using fast matrix multiplication). However, if we can quickly find matrices $\\widetilde{Q},\\widetilde{K}\\,\\in\\,\\mathbb{R}^{n\\times\\widetilde{d}}$ for some $\\widetilde{d}<n$ such that $(Q K^{\\top})^{f}\\,=$ $\\widetilde{Q}\\times\\widetilde{K}^{\\top}$ , then we can evaluate it more quickly by first computing $\\widetilde{K}^{\\top}\\times V$ and then computing $\\widetilde{Q}\\times(\\widetilde{K}^{\\top}\\times V)$ , for a total running time of just $O(n d\\widetilde{d})$ . ", "page_idx": 24}, {"type": "text", "text": "Since $Q K^{\\top}$ can be any rank $d$ matrix, and $\\widetilde{Q}\\times\\widetilde{K}^{\\top}$ has rank at most $\\hat{d}$ , it follows that an upper bound on the best $\\widetilde{d}$ we can achieve is the maximum, over all matrices $M$ of rank $d$ , of $\\operatorname{rank}(M^{f})$ . Question 2.3 asks  whether it is possible to achieve $d^{\\prime}<n$ for functions $f$ like the soft-max function which are not a polynomial. If not, then we can only hope to carry out this plan of attack if we can find a low-degree polynomial approximation to our function $f$ . ", "page_idx": 24}, {"type": "text", "text": "Algorithm Design: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "For one example of polynomial method in algorithm design, consider the fastest known algorithm for batch Hamming Nearest Neighbor Search due to Alman, Chan, and Williams [ACW16]. In this problem, one is given as input $2n$ vectors ", "page_idx": 24}, {"type": "equation", "text": "$$\nx_{1},\\ldots,x_{n},y_{1},\\ldots,y_{n}\\in\\{0,1\\}^{d}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for $d=\\Theta(\\log n)$ , and a threshold value $t\\,\\in\\,\\{0,1,\\ldots,d\\}$ , and one wants to find a pair $(i,j)\\in$ $[n]\\times[n]$ such that the Hamming distance between $x_{i}$ and $y_{j}$ is at most $t$ . [ACW16] takes an algebraic approach to this problem, by first considering the matrix $\\boldsymbol{M}\\in\\mathbb{R}^{n\\times n}$ where $M_{i,j}$ is the Hamming distance between $x_{i}$ and $y_{j}$ . One can see that $\\mathrm{rank}(M)\\,\\leq\\,2d$ , and one could use fast matrix multiplication to quickly compute all the entries of $M$ .9 However, since $M$ itself has $n^{2}$ entries, this could not improve much on the straightforward $O(n^{2}\\log{n})$ time algorithm. They instead take the following approach. ", "page_idx": 24}, {"type": "text", "text": "First, pick a parameter $g=n^{\\delta}$ for a constant $\\delta>0$ , and a function $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ such that $f(x)>g^{2}$ for all $x\\in\\{0,1,\\ldots,t\\}$ , and $f(x)\\in[0,1]$ for all $x\\in\\{t\\!+\\!1,t\\!+\\!2,\\ldots,d\\}$ . [ACW16] use Chebyshev polynomials to construct such an $f$ which is a low-degree polynomial, so that the matrix $M^{f}$ has low rank by Fact 2.1. Next, let $S_{1},...\\,,S_{n/g}$ be a partition of $\\bar{[}n]$ into $n/g$ groups of size $g$ , and consider the matrix $F\\in\\mathbb{R}^{\\frac{n}{g}\\times\\frac{n}{g}}$ given by ", "page_idx": 24}, {"type": "equation", "text": "$$\nF_{a,b}=\\sum_{i\\in S_{a}}\\sum_{j\\in S_{b}}M_{i,j}^{f}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "It is not hard to verify that $\\operatorname{rank}(F)\\leq\\operatorname{rank}(M^{f})$ . Moreover, by the way $f$ was defined, an entry $F_{a,b}$ is larger than $g^{\\underline{{{\\bar{2}}}}}$ if and only if there is an $(i,j)\\in S_{a}\\times S_{b}$ such that the Hamming distance between $x_{i}$ and $y_{j}$ is at most $t$ . ", "page_idx": 25}, {"type": "text", "text": "There is a trade-off between the parameter $\\delta$ and the degree of $f$ , and hence the rank of $F$ . [ACW16] balance this trade-off to yield a matrix $F$ of low rank10 and dimensions $n^{1-\\delta}\\times n^{1-\\delta}$ for some $\\delta>0$ . Since $F$ now has a subquadratic total number of entries, fast matrix multiplication can be used to compute all its entries and solve the problem, in roughly $O(n^{2-2\\delta})$ time. ", "page_idx": 25}, {"type": "text", "text": "B Technique Overview ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this section, we describe the techniques used to prove our main theorems. In Section B.1, we introduce the eigenvalue properties of kernel matrices derived from hyperrectangles. In Section B.2, we introduce the techniques used to prove Theorem 2.5 and 2.6. In Section B.3, we introduce techniques used to prove Theorem 4.4. In Section B.4, we introduce techniques used to prove Theorem 3.4. ", "page_idx": 25}, {"type": "text", "text": "B.1 Starting Point: Eigenvalues of the Kernel Matrix of a Hyperrectangle ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "All of our proofs start by using a simple but powerful technique. This technique computes eigenvectors and eigenvalues of the kernel matrix for any set of points which arise as the vertices of a hyperrectangle $\\ddot{d}$ -dimensional rectangle). After describing the technique in more detail, we will explain how it leads to our applications by demonstrating why these matrices and their eigenvalues are relevant to the three main questions we stated in Section 1. ", "page_idx": 25}, {"type": "text", "text": "The eigenvectors of the family of matrices we define shortly will come from columns of WalshHadamard matrices. For a positive integer $d$ , let $v_{1},\\ldots v_{2^{d}}\\in\\dot{\\{0,1\\}}^{d}$ be the enumeration of all $n$ -bit vectors in lexicographical order. The Walsh-Hadamard matrix $H_{d}$ is the $2^{d}\\times2^{d}$ matrix defined by $H_{d}(v_{i},v_{j}):=(-1)^{\\langle v_{i},v_{j}\\rangle}$ . The technique is as follows: ", "page_idx": 25}, {"type": "text", "text": "Lemma B.1 (Eigenvalue of Manhattan Kernels, informal version of Lemma I.2). For a vector $a\\in\\mathbb{R}_{>0}^{d}$ , let $\\bar{p_{1}},\\bar{\\mathbf{\\psi}}_{\\cdot}\\cdot\\cdot,\\bar{p_{2^{d}}}\\in\\mathbb{R}^{d}$ denote the vertices $(\\pm a_{1}/2,\\pm a_{2}/2,\\ldots,\\pm a_{d}/2)$ of a hyperrectangle in lexicographical order. For any $f\\,:\\,\\mathbb{R}\\,\\rightarrow\\,\\mathbb{R},$ let $D$ be the $2^{d}$ by $2^{d}$ matrix given by $D_{i,j}\\,=$ $f(\\|p_{i}-\\bar{p}_{j}\\|_{1})$ . Then, the columns of the Hadamard matrix $H_{n}$ are the eigenvectors of $D$ . For $i\\in[2^{d}],$ , let $B(i)\\in\\{0,1\\}^{d}$ be the binary representation of $i$ . Then, the eigenvalue corresponding to column i of $H_{n}$ is: $\\begin{array}{r}{\\lambda_{i}=\\sum_{b\\in\\{0,1\\}^{d}}(-1)^{\\left\\langle B(i),b\\right\\rangle}\\cdot f(\\left\\langle b,a\\right\\rangle)}\\end{array}$ . ", "page_idx": 25}, {"type": "text", "text": "We will see shortly that this expression for the eigenvalue $\\lambda_{i}$ can also be rewritten in terms of integrals and derivatives of the function $f$ , allowing us to use analytic techniques when computing or applying these eigenvalues. ", "page_idx": 25}, {"type": "text", "text": "Lemma B.1 can be proved using a direct calculation, although its inspiration comes from representation theory. The matrix $D$ has the property that: for any permutation matrix $\\sigma$ corresponding to a reflection about one of the hyperrectangle\u2019s axes, we have $\\sigma D=D\\sigma$ . Schur\u2019s lemma from representation theory (see Lemma A.10 below) states that $D$ and all $\\sigma$ in the reflectional symmetry group of the hyperrectangle have a common set of eigenvectors. It is not hard to verify that the only common set of eigenvectors for all $\\sigma$ is the columns of the Hadamard matrix, and thus $D$ must have the columns of $H_{d}$ as its eigenvectors. ", "page_idx": 25}, {"type": "text", "text": "Our analysis of eigenvalues in Lemma B.1 is closely related to the Fourier analysis of the Boolean hypercube, which has been studied for decades in computer science theory [O\u2019D14]. Fourier analysis of the Boolean hypercube can be seen as an instance of our technique, by setting $a$ to be the all ones vector in $d$ dimensions. It is important in some of our proofs that $a$ is not the all ones vector: kernel matrices from a hyperrectangle with varying side lengths have eigenvectors that approximate finite differences. This is key for our proofs of Theorem 4.4 and 3.4. See Section E.2 for details. ", "page_idx": 25}, {"type": "text", "text": "We next give an overview of how we use Lemma B.1 to derive our three applications. We focus on explaining how the matrices described by Lemma B.1 and their eigenvalues arise in each setting. ", "page_idx": 25}, {"type": "text", "text": "B.2 Polynomial Method Converse ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "B.2.1 Exact Low Rank ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We begin by explaining our techniques for the exact low rank case (Theorem 2.5). This theorem seems hard to prove for a number of reasons. ", "page_idx": 26}, {"type": "text", "text": "First, we assume very little structure on $f$ : in particular, we do not asssume $f$ is differentiable or even continuous. Rather, we assume a much weaker condition than continuity: indeed, Theorem 2.5 holds for all piecewise continuous functions $f$ . This is a large space of functions, and in particular covers all non-differentiable continuous functions including oddities such as the everywhere-continuous and nowhere-differentiable Weirstrauss function. ", "page_idx": 26}, {"type": "text", "text": "Second, to prove a matrix $M^{f}$ is low rank, one might typically compute either the determinant or an eigenvalue and show they must be 0. However, explicit formulas for determinants and eigenvalues can often be complicated, large algebraic expressions in terms of $f$ and elements of $M$ . We overcome this barrier by selecting a special family of matrices $M$ whose eigenvalues can be expressed as a simple sum. This family of matrices is restrictive enough to have a set of common eigenvectors (and thus easily computable eigenvalues), but expansive enough to express all finite differences of $f$ in terms of these eigenvalues. Our proof proceeds as follows. ", "page_idx": 26}, {"type": "text", "text": "Step 1: We begin by showing that if $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ preserves low-rank $n\\times n$ matrices, and does not have any essential discontunuities of the first kind, then $f$ must be continuous. We do this by constructing a family of $n\\times n$ matrices of rank at most $^{5}$ such that, for any jump, point, removeable, or essential-of-second-kind discontinuity that the function $f$ has, one can pick a corresponding matrix from our family that $f$ maps to a full-rank matrix. In other words, such functions $f$ are very far from preserving $n\\times n$ low-rank matrices. ", "page_idx": 26}, {"type": "text", "text": "Step 2: We next show that if a continuous function $f$ preserves low-rank $n\\times n$ matrices, then it must be a piecewise polynomial function. We will do this by considering the family of $n\\times n$ kernel matrices of a hyperrectangle, from Section B.1. If $f$ preserves low-rank matrices, then it must, in particular, map all these matrices to matrices which do not have full rank. ", "page_idx": 26}, {"type": "text", "text": "For any fixed $i$ , we will show that the $d$ -th order finite difference of $f$ at point $x$ can be written as a linear combination of the $i$ -th eigenvalue $\\lambda_{i}$ , of a number of different matrices in our family (see Lemma B.1 for definition of $\\lambda_{i}$ and our kernel matrices). Hence, if $\\lambda_{i}$ is 0 for all of the abovementioned kernel matrices, this will imply that the $d$ -th order finite difference of $f$ is 0 at every point $x$ , and thus $f$ is a polynomial of degree at most $d$ . ", "page_idx": 26}, {"type": "text", "text": "Although we are guaranteed that one of the eigenvalues of each kernel matrix is 0, we are not guaranteed that there is a fixed $i$ such that it is always the $i$ -th eigenvalue which is 0. ", "page_idx": 26}, {"type": "text", "text": "In order to address this, we apply the Baire Category Theorem from topology to the zero-sets of $\\lambda_{i}$ for each fixed $i$ . Roughly, this theorem allows us to show that for all $x\\in\\mathbb R$ (except for a set whose intersection with any finite interval is finite), one can manipulate which matrices determine the finite difference of $f$ at $x$ to ensure that they all have the same eigenvalue $\\lambda_{i}$ equal to 0. Working through the details, this implies that $f$ is a piecewise polynomial. ", "page_idx": 26}, {"type": "text", "text": "Step 3: Next, we show that the function $f$ must be exactly a polynomial. From step 2, we know that $f$ is piecewise polynomial. We then use a series of algebraic manipulations, and the fact that a linear combination of $\\lambda_{i}$ from different matrices gives the finite difference of $f$ , to show that each $d$ -th finite difference of $f$ evaluated at any point for any gap is 0. This implies that $f$ is a polynomial, finishing our proof. ", "page_idx": 26}, {"type": "text", "text": "B.2.2 Approximate Low Rank ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Our proof of Theorem 2.6 is similar to the exact setting (Theorem 2.5). The main new technical difficulty which arises is that we must more carefully bound the finite differences in terms of the eigenvalues; in the previous proof, we could assume one of the eigenvalues is 0 and so many terms cancelled out. We omit further details here in the interest of space, but refer the reader to Section D for more details. ", "page_idx": 26}, {"type": "text", "text": "B.3 Metric Transforms ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "At the onset, Theorem 4.4 seemed difficult to prove for a number of reasons. ", "page_idx": 27}, {"type": "text", "text": "It is known, and not hard to show, that any Bernstein function transforms squared Euclidean distances to squared Euclidean distances [BCR84]. It was also known that Manhattan distances isometrically embed into squared Euclidean distances. Thus Theorem 4.4 immediately implies that Bernstein functions are equivalent to functions that transform squared Euclidean to squared Euclidean distances. This equivalence is Schoenberg\u2019s foundational theorem on Euclidean metric transforms [Sch37], which is considered difficult to prove from scratch. ", "page_idx": 27}, {"type": "text", "text": "Schoenberg\u2019s proof uses multivariable calculus and complex analysis in Hilbert space, which has a natural Euclidean distance structure; Manhattan distances do not have such structure, so we cannot proceed in this way to prove Theorem 4.4. Our proof takes an entirely new approach, and our result is stronger than Schoenberg\u2019s. ", "page_idx": 27}, {"type": "text", "text": "Moreover, we note that most squared Euclidean distances are not Manhattan distances: indeed, most squared Euclidean distances are not even metrics. Thus, it was highly conceivable before our work that non-Bernstein functions could transform Manhattan distances to squared Euclidean distances. Our work rules this possibility out. It was also unclear prior to our work why functions transforming Manhattan to Manhattan, and functions transforming Manhattan to squared Euclidean, should be the same set of functions. ", "page_idx": 27}, {"type": "text", "text": "Notably, we do not assume any kind of structure on metric transforms $f$ : we do not assume $f$ is bounded, continuous, Fourier-transformable, and so forth. Thus, our theorem applies to any conceivable function with no underlying structure assumed at all. ", "page_idx": 27}, {"type": "text", "text": "Step 1: First, we show that a function transforms Manhattan distances to squared Euclidean distances, if and only if $f$ applied entrywise to our kernel matrices from hyperrectangles (see Lemma B.1 for how these are defined) always results in a matrix whose eigenvalues are all negative except for $\\lambda_{1}$ . Here $\\lambda_{i}$ is defined as in Lemma B.1. This follows from the well-known fact that all Manhattan distances can be isometrically embedded into Manhattan distances between points on a hyperrectangle, combined with a criterion of Schoenberg [Sch35] on when a given set of $\\binom{n}{2}$ distances can be realized as pairwise squared Euclidean distances from $n$ points. ", "page_idx": 27}, {"type": "text", "text": "Step 2: Next, we show that only Bernstein functions transform Manhattan distances to squared Euclidean distances. We do this by showing the $d$ -th order finite difference of $f$ evaluated at $x$ , can be written as $(-1)^{d}$ times the limit of a sequence of eigenvalues of well-chosen kernel matrices from hyperrectangles. We can bound this limit using step 1, to show that if $f$ transforms Manhattan distances to squared Euclidean distances, then the $d$ -th order finite difference for $f$ have the opposite sign as $(-1)^{d}$ . This property implies that $f$ is Bernstein, even without assuming a priori that $f$ is bounded or continuous. ", "page_idx": 27}, {"type": "text", "text": "Step 3: We will then show that any function that transforms Manhattan distance to squared Euclidean distance, must transform Manhattan distances to Manhattan distances. ", "page_idx": 27}, {"type": "text", "text": "We first show that $f$ transforms Manhattan distances to squared Euclidean distances if and only if it transforms Manhattan distances from hyperrectangle corners to squared Euclidean distances. We then find the explicit embedding of these squared Euclidean distances via a classic idea of Schoenberg [Sch35], which will reveal that these transformed distances can be embedded as squared Euclidean distances from corners on a different, higher dimensional hyperrectangle. Squared Euclidean distances from corners of a hyperrectangle are isometric to Manhattan distances, by the Pythagorean theorem. ", "page_idx": 27}, {"type": "text", "text": "B.4 Kernel Methods ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Theorem 3.4 represents a non-trivial advance in kernel theory for the following reason: ", "page_idx": 27}, {"type": "text", "text": "One of the fundamental results of kernel methods is a classification of all Euclidean kernels. From this, one can deduce that a function is a squared Euclidean kernel if and only if it is a completely monotone function. Since Manhattan distances are a measure-zero set of squared Euclidean distances, it is clear that completely monotone functions are Manhattan kernels, but it is not at all clear that all Manhattan kernels should be completely monotone. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "The proof steps for kernel methods are very similar to those for metric transforms. The reason is that there is a known connection between matrices of squared Euclidean distances between pairs of points, and matrices of inner products between pairs of points. ", "page_idx": 28}, {"type": "text", "text": "The main difference between the proof of Theorem 3.4 on kernels and the proof of Theorem 4.4 on metric transforms (whose steps are listed in Section B.3) is that in Step 1 of our proof on kernels, we show that a function is a Manhattan distance kernel if and only if $f$ applied entrywise to our kernel matrices from hyperrectangles always results in a matrix whose eigenvalues are all non-negative. We propagate this change through the proof steps accordingly. ", "page_idx": 28}, {"type": "text", "text": "C Polynomial Method Converse ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "The major goal of this section is to prove Theorem C.11, the formal restatement of Theorem 2.5. This section is organized as follows ", "page_idx": 28}, {"type": "text", "text": "\u2022 In Section C.1, we restate some preliminaries about kernel matrices from real hyperrectangles. We define matrices $M(a),M^{f}(a)$ , and eigenvalues $\\lambda_{i}^{f}(a)$ .   \n\u2022 In Section C.2, we prove that low degree polynomials are the only functions such that $M^{f}(a)$ has an eigenvalue that is the zero function in terms of $a\\in\\mathbb{R}^{d}$ .   \n\u2022 In Section C.3, we show that for any function that preserves low rank, one eigenvalue of $M^{f}(a)$ must be zero in terms of $a$ .   \n\u2022 In Section C.4, we provide an algebraic computation, which is a key step for equating a sum of eigenvalues with the formula for finite differences.   \n\u2022 In Section C.5, we formally relate a key sum of $f$ evaluated at various points, with the finite differences of $f$ .   \n\u2022 In Section C.6, we prove that only low-degree polynomials preserve low rank. This proves the main result of this section, Theorem C.11, which is a formal restatement of Theorem 2.5.   \n\u2022 In Section C.7, we show a large class of discontinuous functions do not preserve low-rank matrices. ", "page_idx": 28}, {"type": "text", "text": "C.1 Preliminaries ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We start by defining the matrix $M(a)$ from the real hyperrectangle, and its eigenvalues. ", "page_idx": 28}, {"type": "text", "text": "Definition C.1 (Matrix $M(a),$ ). Consider a mapping $B:\\{0,1,\\dots2^{d}-1\\}\\to\\{0,1\\}^{d}$ corresponding to the conversion of integers into $d$ -digit binary strings, which we interpret as d dimensional $0-1$ vectors. For any fixed vector $a\\in\\mathbb{R}^{d}$ , we define matrix $M(a)$ ", "page_idx": 28}, {"type": "equation", "text": "$$\nM(a)_{i,j}:=\\langle a,B(|i-j|)\\rangle.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Definition C.2 (Eigenvalues of $M(a),$ . For each matrix $M(a)\\,\\in\\,\\mathbb{R}^{n\\times n}$ , we established previously that $f(M(a)\\bar{)}\\,\\in\\,\\mathbb{R}^{n\\times n}$ has eigenvectors equal to the Hadamard matrix columns, and the corresponding eigenvalues are: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\lambda_{i}^{f}(a)=\\sum_{b\\in\\{0,1\\}^{d}}(-1)^{\\left\\langle B(i),b\\right\\rangle}\\cdot f(\\left\\langle b,a\\right\\rangle)\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We also give the following definition: ", "page_idx": 28}, {"type": "text", "text": "Definition C.3 (Real hyperrectangle). The $d$ -dimensional real hyperrectangle parameterized by $d$ variables $a_{1},\\dots a_{d}>0$ is the convex hull of the $2^{d}$ points $\\{\\pm a_{1}/2,...\\pm a_{d}/2\\}$ . ", "page_idx": 28}, {"type": "text", "text": "C.2 Functions with Algebraically Zero Eigenvalues ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "The goal of this section is prove Lemma C.4. ", "page_idx": 28}, {"type": "text", "text": "Lemma C.4 (Only polynomials have a zero eigenvalue). For any function $f$ , any $n$ that is a power of 2, and $d:=\\log n+1$ : we can find $M:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{n\\times n}$ and $\\lambda_{i}^{f}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ satisfying: ", "page_idx": 29}, {"type": "text", "text": "1. $M(a)$ has rank $\\leq d$ for all $a\\in\\mathbb{R}^{d}$   \n2. $\\lambda_{1}^{f}(a)\\ldots\\lambda_{n}^{f}(a)$ is the full set of eigenvalues of $f(M(a))$ , for all $a\\in\\mathbb{R}^{d}$ .   \n3. If there exists $i\\in[n]$ such that $\\lambda_{i}^{f}(a)=0$ for all $a\\in\\mathbb{R}^{d}$ , then $f$ is a degree $d\\leq\\log n+1$ polynomial. ", "page_idx": 29}, {"type": "text", "text": "Proof. We note that for any $\\varepsilon>0$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n(-1)^{\\langle B(i),1\\rangle}\\sum_{b\\in\\{0,1\\}^{d}}(-1)^{\\|b\\|_{1}}\\cdot\\lambda_{i}^{f}(a+\\epsilon b)=\\sum_{b\\in\\{0,1\\}^{d}}(-1)^{\\|b\\|_{1}}\\cdot f(\\langle a+\\epsilon b,\\mathbf{1}\\rangle),\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "by the following computation: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad(-1)^{\\langle B(i),1\\rangle}\\displaystyle\\sum_{b\\in\\{0,1\\}^{d}}\\ (-1)^{\\|b\\|_{1}}\\cdot\\lambda_{i}^{f}(a+\\epsilon b)}\\\\ &{=(-1)^{\\langle B(i),1\\rangle}\\displaystyle\\sum_{b_{1}\\in\\{0,1\\}^{d}}\\ (-1)^{\\|b_{1}\\|_{1}}\\cdot\\left(\\sum_{b_{2}\\in\\{0,1\\}^{d}}\\ (-1)^{\\langle B(i),b_{2}\\rangle}f(\\langle b_{2},a+\\epsilon b_{1}\\rangle)\\right)}\\\\ &{=(-1)^{2\\langle B(i),1\\rangle}\\displaystyle\\sum_{b\\in\\{0,1\\}^{d}}\\ (-1)^{\\|b\\|_{1}}\\cdot f(\\langle a+\\epsilon b,1\\rangle)}\\\\ &{=\\displaystyle\\sum_{b\\in\\{0,1\\}^{d}}(-1)^{\\|b\\|_{1}}\\cdot f(\\langle a+\\epsilon b,1\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the first equality follows from the definition of $\\lambda_{i}^{f}$ and the second equality follows from Lemma C.9. ", "page_idx": 29}, {"type": "text", "text": "It follows that if $\\lambda_{i}^{f}=0$ , then ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\sum_{b\\in\\{0,1\\}^{d}}(-1)^{\\|b\\|_{1}}\\cdot f(\\langle a+\\epsilon b,\\mathbf{1}\\rangle=0\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "for all $\\epsilon$ and $a$ . By Lemma C.10, the above equation implies that $\\Delta_{\\epsilon}^{d}[f](\\langle a,1\\rangle)=0$ for all $\\epsilon$ . If all $d^{t h}$ order finite differences are equal to 0 at $a$ , then the $d^{t h}$ derivative of $f$ at $a$ exists and is also equal to 0. ", "page_idx": 29}, {"type": "text", "text": "Therefore, $f$ is at most a degree $d$ polynomial as desired. Thus, we complete the proof. ", "page_idx": 29}, {"type": "text", "text": "We conjecture that the above lemma is true without requiring $n$ to be a power of 2, but it is not necessary for our other results. ", "page_idx": 29}, {"type": "text", "text": "C.3 Eigenvalues of Low Rank Preserving Functions ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "The goal of this section is to prove Lemma C.5. ", "page_idx": 29}, {"type": "text", "text": "Lemma C.5 (One Eigenvalue is identically zero). If n is a power of 2 and given: ", "page_idx": 29}, {"type": "text", "text": "1. A function $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ with no essential discontinuities of the first type   \n2. A function $M:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{n\\times n}$ , mapping $d:=\\lceil\\log n\\rceil$ dimensional vectors to n dimensional matrices.   \n3. A set of n functions $\\lambda_{1}^{f},\\lambda_{2}^{f},\\dots,\\lambda_{n}^{f}$ such that each $\\lambda_{i}^{f}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R},$ , and $\\lambda_{1}^{f}(a)\\ldots\\lambda_{n}^{f}(a)$ is the full set of eigenvalues of $f$ applied entry-wise to $M(a)$ for all $a\\in\\mathbb{R}^{d}$ , ", "page_idx": 29}, {"type": "text", "text": "Then if $f$ transforms matrices $M(a)$ to rank $<\\,n$ for all $a\\in\\mathbb{R}^{d}$ , then there exists $i\\,\\in\\,[n]$ where function $\\lambda_{i}^{f}=0$ . ", "page_idx": 30}, {"type": "text", "text": "Proof. Lemma C.8 says that if $f$ preserves low rank and has no essential discontinuities of the first type, then it is piecewise polynomial. We will prove that if it preserves low rank and is piecewise polynomial, it must be polynomial. ", "page_idx": 30}, {"type": "text", "text": "We consider the family of matrices $(J+M(a))^{f}$ , where $J\\in\\mathbb{R}^{n\\times n}$ is the all ones matrix. Note that $M(a)$ has rank at most $d+1$ and one of the eigenvectors is all ones vector. Thus, matrices of the form $J+M(a)$ have rank at most $d+1$ . ", "page_idx": 30}, {"type": "text", "text": "Suppose otherwise, that $f$ preserves low rank and is piecewise polynomial but not polynomial. Without loss of generality, we can assume that $f(x)={\\bar{P_{1}}}(x)$ on $[r,s]$ and $f(x)=P_{2}(x)$ on $[s,t]$ for some $r<s<t$ , for some polynomials $P_{1}\\neq P_{2}$ . ", "page_idx": 30}, {"type": "text", "text": "There exists an open set $X\\subset\\mathbb{R}$ and an open set $Y\\subset\\mathbb{R}^{d}$ such that $x\\,\\in\\,[r,s]$ and $x+\\langle y,a\\rangle$ is in $[s,t]$ for all $x\\in X,y\\in Y$ , and non-zero $a\\in\\{0,1\\}^{d}$ . $X,Y$ can be attained by choosing $X$ to be the set of $x$ satisfying $s\\mathrm{~-~}\\epsilon\\mathrm{~<~}x\\mathrm{~<~}s$ , and and $Y$ to be the set of $y$ with $\\epsilon<y_{j}<2\\epsilon$ , for any $0<\\epsilon<(t-s)/(2d)$ , for all $1\\leq j\\leq d$ . ", "page_idx": 30}, {"type": "text", "text": "We use our eigenvalue computation in Definition C.2 to see that: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\lambda_{i}^{f}(x J+M(y))=P_{1}(x)+\\sum_{a\\in\\{0,1\\}^{d}\\setminus\\mathfrak{d}_{d}}(-1)^{\\left<B(i),a\\right>}P_{2}(x+\\left<y,a\\right>).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Here, $B(i)$ is the $d$ dimensional binary representation of $i\\in[n]$ in Eq. (2). ", "page_idx": 30}, {"type": "text", "text": "If $f$ preserves low rank, then for each $x\\in X$ and for each $y\\in Y$ , there exists an $i$ such that the RHS of Eq. (2) identically zero. Since $X\\times Y$ is an open set in $\\mathbb{R}^{d+1}$ , there exists $i$ such that the roots of the RHS of Eq. 2 has non-zero measure. Since this RHS is a multivariate polynomial in $x$ and $y_{k}$ for $1\\leq k\\leq d$ , and since the only multivariate polynomial with non-zero measure is 0 everywhere, then $\\lambda_{i}^{f}(x J+M(y))$ must be 0 for all $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ and $y\\in\\mathbb{R}^{d}$ for our chosen value of $i$ , and thus for this value of $i$ , $\\lambda_{i}^{f}(M(a))=0$ for all $a$ . ", "page_idx": 30}, {"type": "text", "text": "We conjecture that Lemma C.5 is true for all $n$ , not just powers of 2. However, that is not necessary for our other results. ", "page_idx": 30}, {"type": "text", "text": "The proof above holds assuming Lemma C.8. We now build up a series of Lemmas leading up to that point. First, we need a technical lemma. ", "page_idx": 30}, {"type": "text", "text": "Lemma C.6 (Locally Zero Eigenvalue Implies Vanishing $d^{t h}$ Derivative). Let n be a power of 2 and $d=\\lceil\\log n\\rceil+1$ . If there exists $i\\in[n]$ such that, for fixed $a\\in\\mathbb{R}^{d}$ , $w e$ have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\lambda_{i}^{f}(a+h)=0\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "for all $h\\in\\mathbb{R}^{d}$ with $\\|h\\|_{\\infty}<H_{a}$ for some $H_{a}>0$ , then $\\Delta_{h}^{d}[f](\\langle a,1\\rangle)=0$ for all $\\|h\\|_{\\infty}<H_{a}$ and thus $f^{(d)}(\\langle a,1\\rangle)$ exists and is equal to 0. ", "page_idx": 30}, {"type": "text", "text": "Proof. For fixed $a,d$ , and $i$ , we know by Eq. (1) and Lemma C.10 that $\\Delta_{h}^{d}[f](a)$ can be written as a linear combination of $\\begin{array}{r}{\\sum_{k\\in K}c_{k}\\lambda_{i}(a+h_{k})}\\end{array}$ for some $c\\in R^{|K|}$ and $h_{k}<H_{a}$ for all $k$ . Lemma C.6 follows. \u53e3 ", "page_idx": 30}, {"type": "text", "text": "Lemma C.7. Let $T_{1},\\dots.T_{k}\\subset\\mathbb{R}^{d}$ be closed sets such that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\cup_{i\\in[k]}T_{i}=\\mathbb{R}^{d}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Let $R_{i}$ be the interior of $T_{i}$ . Then when taking the union of the projections of $R_{i}$ onto the line $c\\cdot\\mathbf{1}_{d}\\in\\mathbb{R}^{d}$ for $c\\in\\mathbb{R},$ , the result is the entire line except for a set $Q$ of points, where $Q$ intersected with any finite interval contains only finitely many points. ", "page_idx": 30}, {"type": "text", "text": "Proof. First, we prove that this holds when $d=1$ and $T_{i}$ are the closure of open sets. We will reduce the general case to the case in the previous sentence. ", "page_idx": 30}, {"type": "text", "text": "One-dimensional case If $T_{i}$ is the closure of open sets in one dimension, then it is the union of disjoint closed sets where each finite interval on the real line contains only finitely many closed sets contained in $T_{i}$ . We can assume without loss of generality that the interiors of $T_{i}$ are disjoint. In this case, the set $T_{i}\\setminus R_{i}$ is the union (over all $i$ ) of endpoints of closed intervals in $T_{i}$ . It follows that $T_{i}\\setminus R_{i}$ is a set $Q$ of points such that every finite interval on the real line contains only finitely many elements of $Q$ , as desired. ", "page_idx": 31}, {"type": "text", "text": "Reduction to one-dimensional case. We will reduce to the case where each $T_{i}$ is the closure of its interior. Once we have this, we can project $T_{i}$ and $R_{i}$ onto the line $\\ell=c\\cdot{\\mathbf{1}}_{d},c\\in\\mathbb{R})$ and thus reduce to the one dimensional case. ", "page_idx": 31}, {"type": "text", "text": "First, we show that all points $q$ of the form $(c,c,\\ldots c)\\in\\mathbb{R}^{d}$ that are not in the union of $R_{i}$ , must be on the boundary of some $R_{i}$ . This will prove that we do not lose any generality by considering the case when $T_{i}$ is the closure of its interior $R_{i}$ . ", "page_idx": 31}, {"type": "text", "text": "If not, then there is some open set $S$ containing $q$ that avoids all $R_{i}$ , so now $S$ is covered by one of the $U_{i}:=T_{i}\\setminus R_{i}$ . The $U_{i}$ are all closed and have empty interior, so their complements $V_{i}$ are all open and dense since the $U_{i}$ cover $S$ . ", "page_idx": 31}, {"type": "text", "text": "Since the $U_{i}$ cover $S$ , the intersection of the $V_{i}$ must be empty. However this contradicts the Baire Category theorem (Theorem A.11), which states that the intersection of a family of open and dense sets is also open and dense (and in particular nonempty). This means that $q$ must be on the boundary of some $R_{i}$ , and thus we have proven that we can reduce to the case when $T_{i}$ is the closure of its interior $R_{i}$ . ", "page_idx": 31}, {"type": "text", "text": "Reduction to the one dimensional case. Since we only need to concern with the case where $T_{i}$ is the closure of its interior $R_{i}$ , this means each point in $T_{i}\\setminus R_{i}$ is the limit of a sequence of points in $R_{i}$ for each $i$ . Therefore, the projection of each point in $T_{i}\\setminus R_{i}$ is the limit of a sequence of points in the projection of $R_{i}$ , and thus the projection of $T_{i}$ onto the line $(c,c,\\ldots c)\\in\\mathbb{R}^{d}$ is contained in the closure of the projection of $R_{i}$ . This reduces the problem to the one dimensional case where each $T_{i}$ is the closure of its interior. ", "page_idx": 31}, {"type": "text", "text": "Lemma C.8 (Piecewise Polynomial). If $f$ preserves low rank for all $n\\times n$ matrices where n is any power of 2, and has no essential discontinuities of the first type, then $f$ must be piecewise polynomial. ", "page_idx": 31}, {"type": "text", "text": "Since $f$ preserves low rank, we know that for any $a$ , there exists an $i\\in[n]$ such that $\\lambda_{i}^{f}(M(a))=0$ . By Lemma C.13, we know that $f$ must be continuous. Therefore, the $T_{i}$ of $a$ where $\\lambda_{i}^{f}(M(a))=0$ must satisfy ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\cup_{i\\in[n]}S_{i}=\\mathbb{R}^{d}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Because $f$ is continuous, $\\lambda_{i}^{f}$ is continuous for all $i$ , and so each $T_{i}$ is closed. Let $R_{i}$ be the interior of each $T_{i}$ . We know by Lemma C.6 that for each point $a^{\\prime}$ in $R_{i}$ , we know that $f^{(d)}(\\langle a,1\\rangle)=0$ . By Lemma C.7, this implies $f^{(d)}(x)=0$ for all $x\\in\\mathbb{R}\\setminus Q$ , where $Q\\subset\\mathbb{R}$ has the property that its intersection with any finite interval is finite. This implies that $f$ is a piecewise polynomial. ", "page_idx": 31}, {"type": "text", "text": "C.4 Bridging Eigenvalues and Finite Differences ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "The goal of this section is to prove Lemma C.9, a key lemma which is used to relate eigenvalues and finite differences. ", "page_idx": 31}, {"type": "text", "text": "Lemma C.9 (Rewriting the sum). ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{b_{1}\\in\\{0,1\\}^{d}}(-1)^{\\|b_{1}\\|_{1}}\\left(\\sum_{b_{2}\\in\\{0,1\\}^{d}}(-1)^{\\langle B(i),b_{2}\\rangle}f(\\langle b_{2},a+\\epsilon b_{1}\\rangle)\\right)}\\\\ &{=(-1)^{\\langle B(i),1\\rangle}\\displaystyle\\sum_{b\\in\\{0,1\\}^{d}}(-1)^{\\|b\\|_{1}}\\cdot f(\\langle a+\\epsilon b,\\mathbf{1}\\rangle)}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where a and $b$ are $d$ -dimensional vectors. ", "page_idx": 31}, {"type": "text", "text": "Proof. First, we can show: If $b_{2}$ is a $d$ dimensional vector with any 0s in its vector notation, we know ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\sum_{b_{1}\\in\\{0,1\\}^{d}}(-1)^{\\|b_{1}\\|_{1}}f(\\langle b_{2},a+\\epsilon b_{1}\\rangle)=0\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "for any $\\epsilon$ , and any constant $d$ dimensional vector $a$ . The reason is if $b_{2}$ has any $0$ \u2019s in its vector notation, then flipping the corresponding bit in $b_{1}$ causes $\\left(-1\\right)^{\\left|\\left|b_{1}\\right|\\right|_{1}}$ to change sign, while leaving $\\left<b_{2},a+\\epsilon b_{1}\\right>$ unchanged. ", "page_idx": 32}, {"type": "text", "text": "Now, we know that: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{b_{1}\\in\\{0,1\\}^{d}}(-1)^{\\|b_{1}\\|_{1}}\\left(\\sum_{b_{2}\\in\\{0,1\\}^{d}}(-1)^{\\langle B(i),b_{2}\\rangle}f(\\langle b_{2},a+\\epsilon b_{1}\\rangle)\\right)}\\\\ &{=\\displaystyle\\sum_{b_{2}\\in\\{0,1\\}^{d}}(-1)^{\\langle B(i),b_{2}\\rangle}\\left(\\sum_{b_{1}\\in\\{0,1\\}^{d}}(-1)^{\\|b_{1}\\|_{1}}f(\\langle b_{2},a+\\epsilon b_{1}\\rangle)\\right)}\\\\ &{=(-1)^{\\langle B(i),1\\rangle}\\left(\\sum_{b_{1}\\in\\{0,1\\}^{d}}(-1)^{\\|b_{1}\\|_{1}}f(\\langle1,a+\\epsilon b_{1}\\rangle)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where the first equality follows by rearranging sums, and the second equality follows from Eq. (3).   \nThis completes the proof. ", "page_idx": 32}, {"type": "text", "text": "C.5 Function Sums and Finite Differences ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "The goal of this section is to prove Lemma C.10. ", "page_idx": 32}, {"type": "text", "text": "Lemma C.10 (Function Sums and Finite Differences). Part 1. Suppose the $d^{t h}$ derivative of $f$ , denoted as $f^{(d)}$ , is continuous. Then, we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\epsilon\\to0}\\epsilon^{-d}\\sum_{b\\in\\{0,1\\}^{d}}(-1)^{\\|b\\|_{1}}\\cdot f(\\langle a+\\epsilon b,\\mathbf{1}\\rangle)=f^{(d)}(\\langle a,\\mathbf{1}\\rangle).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Part 2. Suppose $\\Delta_{\\epsilon}^{d}[f](z)$ is the $d$ -th finite difference of function $f$ , then we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\sum_{b\\in\\{0,1\\}^{d}}(-1)^{\\|b\\|_{1}}\\cdot f\\big(\\langle a+\\epsilon b,\\mathbf{1}\\rangle\\big)=\\Delta_{\\epsilon}^{d}[f]\\big(\\langle a,\\mathbf{1}\\rangle\\big).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof. We have: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{b\\in\\{0,1\\}^{d}}(-1)^{\\|b\\|_{1}}\\cdot f(\\langle a+\\epsilon b,\\mathbf{1}\\rangle)=\\displaystyle\\sum_{s=0}^{d}(-1)^{s}\\binom{d}{s}\\cdot f(\\langle a+\\epsilon b,\\mathbf{1}\\rangle)}&{}\\\\ {\\displaystyle=\\sum_{s=0}^{d}(-1)^{s}\\binom{d}{s}\\cdot f(\\langle a,\\mathbf{1}\\rangle+s\\epsilon)}&{}\\\\ {\\displaystyle=\\int_{[0,\\epsilon]^{d}}f^{(d)}(\\langle a+x,\\mathbf{1}\\rangle)\\mathrm{d}x}&{}\\\\ {\\displaystyle=\\Delta_{\\epsilon}^{d}f[\\langle a,\\mathbf{1}\\rangle)}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where the first and second equality follow from grouping $b$ by the number of ones it has, which we denote as $s$ , the third step follows from the fundamental theorem of calculus, and the last step follows from definition of finite difference. ", "page_idx": 32}, {"type": "text", "text": "In addition, we note that the above calculation is independent of $i$ . ", "page_idx": 32}, {"type": "text", "text": "Thus: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{lim}_{\\epsilon\\to0}\\epsilon^{-d}\\sum_{b\\in\\{0,1\\}^{d}}(-1)^{\\|b\\|_{1}}\\cdot f(\\langle a+\\epsilon b,\\mathbf{1}\\rangle)}\\\\ &{=\\displaystyle\\operatorname*{lim}_{\\epsilon\\to0}\\epsilon^{-d}\\int_{0}^{\\epsilon}\\int_{0}^{\\epsilon}\\cdot\\cdot\\cdot\\int_{0}^{\\epsilon}f^{(d)}(\\langle a+x,\\mathbf{1}\\rangle)\\mathrm{d}x_{1}\\cdot\\cdot\\cdot\\mathrm{d}x_{d}}\\\\ &{=f^{(d)}(\\langle a,\\mathbf{1}\\rangle)}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where the first equality follows from Eq. (4) and the last equality follows from the continuity of $f^{(d)}$ . This completes the proof of Lemma C.10. \u53e3 ", "page_idx": 33}, {"type": "text", "text": "C.6 No Functions Other Than Polynomials Preserve Low Rank ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "In this section, we prove main result Theorem C.11 using Lemma C.4 and Lemma C.5. ", "page_idx": 33}, {"type": "text", "text": "Theorem C.11 (Formal statement of Theorem 2.5). Suppose the function $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ does not have any essential discontinuities of the first kind. For any positive integer $n\\geq2$ , the function $f$ preserves low rank matrices if and only if $f$ is a polynomial of degree less than $\\lceil\\log_{2}(n)\\rceil$ . ", "page_idx": 33}, {"type": "text", "text": "Proof. First, we prove it for $n$ as powers of 2, and then we generalize to all $n$ . ", "page_idx": 33}, {"type": "text", "text": "For now, suppose $n$ is a power of 2. Suppose $f$ is a function without essential discontinuities of the first type. By Lemma C.4, we can find $M:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{n\\times n}$ and $\\lambda_{i}^{f}:\\mathbb{R}^{\\log n+1}\\rightarrow\\mathbb{R}$ such that the image of $M$ has rank $\\leq\\log n+1$ , and $\\{\\lambda_{i}^{f}(a)\\}_{i\\in[n]}$ is the full set of eigenvalues of $M(a)$ . Further, if there exists $i\\in[n]$ with function $\\lambda_{i}^{f}(a)=0$ for all $a$ , then $f$ is a degree $d\\leq\\log n+1$ polynomial. ", "page_idx": 33}, {"type": "text", "text": "Now, suppose that $f$ is a function that transforms all rank $\\log n+1$ matrices to rank $<n$ matrices. Then it must transform all matrices $M(a)$ to rank $<n$ matrices. By Lemma C.5, it must follow that $\\lambda_{i}^{f}=0$ for some $i$ . However, we just established via Lemma C.4 that if $\\lambda_{i}^{f}=0$ , then $f$ is a degree $d^{\\stackrel{\\cdot}{\\leq}\\log n+1}$ polynomial. This completes the proof of Theorem C.11 if $n$ is a power of 2. ", "page_idx": 33}, {"type": "text", "text": "The statement for all $n$ follows directly from the statement for $n$ a power of 2. This is because of the fact from linear algebra that if an $N\\times N$ matrix $M$ has full rank, then for any fixed $n\\leq N$ , there exists an $n\\times n$ minor of $M$ with full rank. (This follows, for instance, from expansion by minors.) Suppose $n$ is not a power of 2, and let $2^{d}$ be the smallest power of 2 bigger than $n$ . If $f$ is not a low-degree polynomial, our work as-stated proves that there exists a low rank $2^{d}\\times2^{d}$ matrix $M$ where $M^{\\breve{f}}$ is full rank. By the previous, there exists an $n\\times n$ minor $M_{n}$ of $M$ (with low rank, since its rank is less than that of $M$ ) where $M_{n}^{f}$ is full rank. Therefore, $f$ cannot preserve low rank matrices of dimension $n\\times n$ , if $f$ is not a low degree polynomial. ", "page_idx": 33}, {"type": "text", "text": "C.7 Proof of Continuity ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "The goal of this section is to prove the following lemma, which shows that a large class of functions $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ do not preserve low-rank matrices. ", "page_idx": 33}, {"type": "text", "text": "Lemma C.12. Suppose $f:\\mathbb{R}\\,\\rightarrow\\,\\mathbb{R}$ has any point $c~\\in~\\mathbb{R}$ such that $\\textstyle\\operatorname*{lim}_{x\\to c^{+}}f(x)$ exists and $\\textstyle f(c)\\neq\\operatorname*{lim}_{x\\to c^{+}}{\\bar{f}}(x)$ . Then, $f$ does not preserve low-rank matrices. The same is true with $^{\\ast}c^{+}$ \u2019 replaced by $c^{-}$ \u2019. ", "page_idx": 33}, {"type": "text", "text": "Lemma C.12 will follow from Lemma C.15, which we now build up to and prove. ", "page_idx": 33}, {"type": "text", "text": "Lemma C.13. Suppose that $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ satisfies: $\\dim_{x\\to0}f(x)\\neq f(0)$ . Then, for any positive integer $n$ , there is a rank-2 matrix $M\\in\\mathbb{R}^{n\\times n}$ such that $M^{f}$ has full rank. ", "page_idx": 33}, {"type": "text", "text": "Proof. Let $a=f(0)$ and $b=\\operatorname*{lim}_{x\\to0}f(x)$ . By assumption, $a\\neq b$ . We define the $n\\times n$ matrix $A$ as follows ", "page_idx": 33}, {"type": "equation", "text": "$$\nA_{i,j}=\\left\\{{\\begin{array}{l l}{a}&{{\\mathrm{~if~}}i=j;}\\\\ {b}&{{\\mathrm{otherwise.}}}\\end{array}}\\right.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "The definition of $b$ means that, for all $\\epsilon>0$ , there is a $\\delta>0$ such that if $x\\in\\mathbb R$ satisfies $0<|x|<\\delta$ , then $|f(x)-b|<\\epsilon$ . Let us pick $\\varepsilon>0$ to be sufficiently small so that ", "page_idx": 34}, {"type": "equation", "text": "$$\n0<\\epsilon\\leq\\frac{0.1}{n\\cdot n!\\cdot\\operatorname*{max}\\{|a|,|b|,|b+\\epsilon|,|b-\\epsilon|\\}^{n-1}}\\cdot\\operatorname*{det}(A),\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and let $\\delta>0$ be the corresponding value. ", "page_idx": 34}, {"type": "text", "text": "From assumption, we have ", "page_idx": 34}, {"type": "equation", "text": "$$\nM_{i,j}^{f}=\\left\\{{a\\atop\\in[b-\\epsilon,b+\\epsilon]}\\right.\\ \\mathrm{if}\\ i=j;\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We can upper bound ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{l}{|\\operatorname*{det}(M^{f})-\\operatorname*{det}(A)|=\\displaystyle\\Big|\\sum_{\\sigma\\in S_{n}}\\operatorname{sgn}(\\sigma)\\prod_{i=1}^{n}M_{i,\\sigma_{i}}^{f}-\\displaystyle\\sum_{\\sigma\\in S_{n}}\\operatorname{sgn}(\\sigma)\\prod_{i=1}^{n}A_{i,\\sigma_{i}}\\Big|}\\\\ {\\displaystyle\\leq\\sum_{\\sigma\\in S_{n}}\\Big|\\prod_{i=1}^{n}M_{i,\\sigma_{i}}^{f}-\\displaystyle\\prod_{i=1}^{n}A_{i,\\sigma_{i}}\\Big|}\\\\ {\\displaystyle\\leq\\sum_{\\sigma\\in S_{n}}\\epsilon\\cdot n\\cdot(\\operatorname*{max}\\{|a|,|b|,|b-\\epsilon|,|b+\\epsilon|\\})^{n-1}}\\\\ {\\displaystyle\\leq n!\\cdot\\epsilon\\cdot n\\cdot(\\operatorname*{max}\\{|a|,|b|,|b-\\epsilon|,|b+\\epsilon|\\})^{n-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where the first step follows from definition of matrix determinant. ", "page_idx": 34}, {"type": "text", "text": "Then we can bound ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{det}(M^{f})=\\operatorname*{det}(A)+\\operatorname*{det}(M^{f})-\\operatorname*{det}(A)}\\\\ &{\\qquad\\qquad\\geq\\operatorname*{det}(A)-|\\operatorname*{det}(M^{f})-\\operatorname*{det}(A)|}\\\\ &{\\qquad\\qquad\\geq\\operatorname*{det}(A)-0.1\\operatorname*{det}(A)}\\\\ &{\\qquad\\qquad=0.9\\operatorname*{det}(A)}\\\\ &{\\qquad\\qquad>0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "By picking a matrix $M$ whose entries are all nonnegative, we can extend the same idea to functions $f$ where only one side of $\\textstyle\\operatorname*{lim}_{x\\to0}f(x)$ must exist: ", "page_idx": 34}, {"type": "text", "text": "Lemma C.14. Suppose that $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ satisfies: $\\dim_{x\\to0^{+}}f(x)\\neq f(0)$ . Then, for any positive integer $n$ , there is a rank- $^{4}$ matrix $M\\in\\mathbb{R}^{n\\times n}$ such that $M^{f}$ has full rank. ", "page_idx": 34}, {"type": "text", "text": "Proof. The matrix we use is $\\begin{array}{r}{M_{i,j}=\\frac{\\delta}{n}\\cdot(i-j)^{2}}\\end{array}$ , which has $\\operatorname{rank}(M)=4$ . We then proceed exactly as in Lemma C.13. ", "page_idx": 34}, {"type": "text", "text": "Lemma C.15. Suppose that $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ and $c\\in\\mathbb R$ satisfy: $\\textstyle\\operatorname*{lim}_{x\\to c^{+}}f(x)\\neq f(c)$ . Then, for any positive integer $n$ , there is a matrix $M\\in\\mathbb{R}^{n\\times n}$ of rank at most 5 such that $M^{f}$ has full rank. ", "page_idx": 34}, {"type": "text", "text": "Proof. The matrix we use is $\\begin{array}{r}{M_{i,j}=\\frac{\\delta}{n}\\cdot(i-j)^{2}+c}\\end{array}$ , which has $\\operatorname{rank}(M)\\leq5$ . Again, proceed as in Lemma C.13. \u53e3 ", "page_idx": 34}, {"type": "text", "text": "C.8 Comparison with Prior Work ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "The work of [GKR17] classifies what functions transform low rank matrices into low rank matrices. They show: ", "page_idx": 34}, {"type": "text", "text": "Theorem C.16 (Theorem B in [GKR17]). For integers $n\\geq2,1\\leq k<n-1$ , and $2\\le l\\le n,$ , suppose $f$ has $k$ derivatives. Then the following are equivalent: ", "page_idx": 34}, {"type": "text", "text": "2. $f$ is a polynomial $\\textstyle\\sum_{t=1}^{r}a_{t}x^{i_{t}}$ for some $a_{t}\\in\\mathbb{R}$ and some $i_{t}\\in\\mathbb{N}$ such that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{r}{\\binom{i_{t}+l-1}{l-1}}\\leq k\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Moreover, $i f k\\le n-3$ , we don\u2019t need the assumption that $f$ has $k$ derivatives. ", "page_idx": 35}, {"type": "text", "text": "This is a near-full classification of functions that transform low rank matrices into low rank matrices, with a few holes: first, the primary piece of the theorem requires $k$ -fold differentiability of $f$ , something that doesn\u2019t hold true for commonly used functions such as the ReLU function. The part of the theorem which doesn\u2019t require $k$ -fold differentiability forces $k\\leq n-3.$ . As discussed in [GKR17], these existing techniques are fundamentally incapable of extending to the settings when to $k=n-2,n-1.$ ", "page_idx": 35}, {"type": "text", "text": "In contrast to Theorem C.16, our Theorem C.11 uses very different techniques, and addresses the case where $k=n-1$ when $f$ is not required to be differentiable, which is not covered by Theorem C.16. The consequence of extending to $k=n-1$ is that we have less control over $l$ ; our results primarily hold for $l=\\lg n+1$ whereas Theorem C.16 applies for more general $l$ . Additionally, our result is not exact: we show that if $f$ preserves low rank, then $f$ must be a polynomial, but we only bound the degree of this polynomial up to a factor of 2, whereas theorem C.16 has tighter control over what kind of polynomial $f$ is. Thus, Theorem C.16 is tighter, but their methods are fundamentally unable to handle the case when $k=n-2$ or $k=n-1$ without assuming $k$ -fold differentiability. Our result is able to handle the $k=n-1$ case, at the expense of tight bounds on the polynomial degree of $f$ and at the expense of requiring that $l>\\log n+1$ . ", "page_idx": 35}, {"type": "text", "text": "D Approximate Polynomial Method Converse ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "In this section, we generalize the proof in Section C from exact to approximate here. ", "page_idx": 35}, {"type": "text", "text": "In Section C, we claim that if a function, when applied termwise, transforms a low rank matrix to a low rank matrix, then it must be a low degree polynomial. We will define a function that approximately preserves low rank, and then prove that if a function approximately preserves low rank, then its $\\dot{d}^{t h}$ order finite differences must be bounded in two settings: when $f$ is analytic, and when $f$ is Lipschitz. ", "page_idx": 35}, {"type": "text", "text": "Definition D.1. Let $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ and $\\delta>0$ . We say $f\\;\\delta.$ -approximately preserves low rank matrices $i f,$ for every matrix $M\\in\\mathbb{R}^{n\\times n}$ with $\\operatorname{rank}(M)<\\log n,$ , the matrix $f(M)$ has at least one eigenvalue in $[-\\delta/n,\\stackrel{\\cdot}{\\delta}/n]$ . ", "page_idx": 35}, {"type": "text", "text": "It is not hard to show the following fact, ", "page_idx": 35}, {"type": "text", "text": "Fact D.2. For a fixed vector $a\\in\\mathbb{R}^{d}$ , let matrix $M(a)\\in\\mathbb{R}^{n\\times n}$ be defined as Definition C.1. Let $f:\\mathbb{R}\\to\\mathbb{R}_{+}$ , then we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{i\\in[d]}|\\lambda_{i}(M(a))|=\\sum_{b\\in\\{0,1\\}^{d}}f(\\langle a,b\\rangle)\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof. By definition of eigenvalue of matrix $M(a)$ , we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{i\\in[d]}|\\lambda_{i}(M(a))|\\leq\\sum_{b\\in\\{0,1\\}^{d}}|f(\\langle a,b\\rangle)|=\\sum_{b\\in\\{0,1\\}^{d}}f(\\langle a,b\\rangle)\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where the second step follows from $f$ is a positive function. ", "page_idx": 35}, {"type": "text", "text": "On the other hand, we also know there is an eigenvalue is equal to $\\textstyle\\sum_{b\\in\\{0,1\\}^{d}}f(\\langle a,b\\rangle)$ . Thus, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{i\\in[d]}|\\lambda_{i}(M(a))|\\geq\\sum_{b\\in\\{0,1\\}^{d}}f(\\langle a,b\\rangle).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "D.1 Main Approximate Result ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Let $d\\in\\mathbb{Z}^{+},a\\in\\mathbb{R}^{d},h_{a}\\in\\mathbb{R},n=2^{d}.$ . ", "page_idx": 36}, {"type": "text", "text": "Theorem D.3. Let $d=\\log n$ . Let $\\delta\\in(0,1)$ denote some sufficiently small parameter. Let function $f$ satisfy: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\frac{\\operatorname*{min}_{i\\in[n]}|\\lambda_{i}^{f}(M)|}{\\operatorname*{max}_{i\\in[n]}|\\lambda_{i}^{f}(M)|}\\leq\\delta/n\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "for all rank $d+1$ matrices $M$ . Let $\\begin{array}{r}{K_{a}=\\sum_{b\\in\\{0,1\\}^{d}}f(\\langle a,b\\rangle).}\\end{array}$ . ", "page_idx": 36}, {"type": "text", "text": "Part 1. If $f$ is real analytic, there exists an $H_{a}>0$ such that for all $h<H_{a}$ , we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\Delta_{h}^{d}[f](\\langle a,\\mathbf{1}\\rangle)\\le\\delta K_{a}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Part 2. If $f$ is an $L$ -Lipschitz function, then for all $h\\geq0$ , we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\Delta_{h}^{d}[f](\\langle a,{\\bf1}\\rangle)\\le\\delta K_{a}+h L d n.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Proof. We will consider two regimes separately: ", "page_idx": 36}, {"type": "text", "text": "\u2022 Not assuming $f$ is Lipschitz (Lemma D.4).   \n\u2022 Assuming $f$ is (Lipschitz D.5). ", "page_idx": 36}, {"type": "text", "text": "D.2 Real Analytic Functions ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Lemma D.4 (real analytic functions with no Lipschitz assumption, part 1 of Theorem D.3). We have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{h\\to0}\\Delta_{h}^{d}[f](\\langle a,1\\rangle)\\leq\\delta K_{a}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Proof. Let $h=\\epsilon$ . By Lemma C.10 ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\sum_{b\\in\\{0,1\\}^{d}}(-1)^{\\|b\\|_{1}}\\cdot f\\big(\\langle a+\\epsilon b,\\mathbf{1}\\rangle\\big)=\\Delta_{\\epsilon}^{d}[f]\\big(\\langle a,\\mathbf{1}\\rangle\\big).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "By proof of Lemma C.4, we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\sum_{b\\in\\{0,1\\}^{d}}(-1)^{\\|b\\|_{1}}\\cdot f\\big(\\langle a+\\epsilon,\\mathbf{1}\\rangle\\big)=(-1)^{\\langle B(i),1\\rangle}\\sum_{b\\in\\{0,1\\}^{d}}(-1)^{\\|b\\|_{1}}\\cdot\\lambda_{i}^{f}(a+\\epsilon b)\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Let us pick $i^{*}\\in[d]$ to be the index that $\\lambda_{i^{*}}^{f}(a)$ is the smallest eigenvalue for matrix $M(a)$ . Choose $H_{a}$ to be such that $\\lambda_{i\\ast}^{f}(M(a+\\varepsilon^{\\prime}b))\\leq\\delta_{n}$ for all ${\\varepsilon}^{\\prime}<H_{a}$ . ", "page_idx": 36}, {"type": "text", "text": "Thus, we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta_{\\epsilon}^{d}[f](\\langle a,\\mathbf{1}\\rangle)=(-1)^{\\langle B(i^{*}),1\\rangle}\\displaystyle\\sum_{b\\in\\{0,1\\}^{d}}(-1)^{\\|b\\|_{1}}\\cdot\\lambda_{i^{*}}^{f}(a+\\epsilon b)}\\\\ &{\\qquad\\qquad\\qquad\\leq2^{d}\\cdot|\\lambda_{i^{*}}^{f}(a)|}\\\\ &{\\qquad\\qquad\\leq2^{d}K_{a}\\delta/n}\\\\ &{\\qquad=\\delta K_{a}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": ". where the second step follows from the fact that $H_{a}$ is chosen so that $\\lambda_{i\\ast}^{f}(a+\\varepsilon b)\\leq\\delta/n$ for all $\\varepsilon<H_{a}$ . This is doable for all real analytic functions $f$ . \u53e3 ", "page_idx": 36}, {"type": "text", "text": "D.3 Lipschitz Functions ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "The goal of this section is to prove the following lemma, ", "page_idx": 37}, {"type": "text", "text": "Lemma D.5 ( $f$ is $L$ -Lipschitz, part 2 of Theorem D.3). Suppose $f$ is $L$ -Lipschitz, we can show ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\Delta_{h}^{d}[f](\\langle a,\\mathbf{1}\\rangle)\\leq\\delta K_{a}+h L d n\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof. In the proof, we let $h=\\epsilon$ . By Lemma C.10, we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\sum_{b\\in\\{0,1\\}^{d}}(-1)^{\\|b\\|_{1}}\\cdot f\\big(\\langle a+\\epsilon b,\\mathbf{1}\\rangle\\big)=\\Delta_{\\epsilon}^{d}[f]\\big(\\langle a,\\mathbf{1}\\rangle\\big).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "By proof of Lemma C.4, we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\sum_{b\\in\\{0,1\\}^{d}}(-1)^{\\|b\\|_{1}}\\cdot f\\big(\\langle a+\\epsilon,\\mathbf{1}\\rangle\\big)=(-1)^{\\langle B(i),1\\rangle}\\sum_{b\\in\\{0,1\\}^{d}}(-1)^{\\|b\\|_{1}}\\cdot\\lambda_{i}^{f}(a+\\epsilon b)\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Let us pick $i^{*}\\in[d]$ to be the index that $\\lambda_{i^{*}}^{f}(a)$ is the smallest eigenvalue for matrix $M(a)$ . Thus, we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\Delta_{\\epsilon}^{d}[f](\\langle a,1\\rangle)=(-1)^{\\langle B(i^{*}),1\\rangle}\\sum_{b\\in\\{0,1\\}^{d}}(-1)^{\\|b\\|_{1}}\\cdot\\lambda_{i^{*}}^{f}(a+\\epsilon b)}\\\\ {\\displaystyle=(-1)^{\\langle B(i^{*}),1\\rangle}\\sum_{b\\in\\{0,1\\}^{d}}(-1)^{\\|b\\|_{1}}\\lambda_{i^{*}}(a)}\\\\ {\\displaystyle\\qquad\\qquad+\\left(-1\\right)^{\\langle B(i^{*}),1\\rangle}\\sum_{b\\in\\{0,1\\}^{d}}\\left(-1\\right)^{\\|b\\|_{1}}\\cdot\\left(\\lambda_{i^{*}}^{f}(a+\\epsilon b)-\\lambda_{i^{*}}^{f}(a)\\right)}\\\\ {\\displaystyle\\leq2^{d}|\\lambda_{i^{*}}(a)|+\\sum_{b\\in\\{0,1\\}^{d}}\\left|\\lambda_{i^{*}}^{f}(a+\\epsilon b)-\\lambda_{i^{*}}^{f}(a)\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "For the first term in the above equation, we can show ", "page_idx": 37}, {"type": "equation", "text": "$$\n2^{d}|\\lambda_{i^{*}}(a)|\\leq2^{d}\\cdot\\operatorname*{max}_{i\\in[d]}|\\lambda_{i}(a)|\\delta/n=2^{d}\\cdot K_{a}\\delta/n=\\delta\\cdot K_{a}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where the first step follows from Assumption, and the second step follows from Fact D.2 and definition of $K_{a}$ , and the last step follows from $\\bar{n}=2^{d}$ . ", "page_idx": 37}, {"type": "text", "text": "For the second term in the above equation, we can show ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{b\\in\\{0,1\\}^{d}}|\\lambda_{i^{*}}^{f}(a+\\epsilon b)-\\lambda_{i^{*}}^{f}(a)|\\leq\\displaystyle\\sum_{b\\in\\{0,1\\}^{d}}\\displaystyle\\sum_{c\\in\\{0,1\\}^{d}}|f(\\langle a+\\epsilon b,c\\rangle)-f(\\langle a,c\\rangle)|}\\\\ {\\displaystyle}&{\\leq\\displaystyle\\sum_{b\\in\\{0,1\\}^{d}}\\displaystyle\\sum_{c\\in\\{0,1\\}^{d}}L\\cdot\\langle\\epsilon b,c\\rangle}\\\\ &{=L\\cdot\\epsilon\\cdot\\langle\\displaystyle\\sum_{b\\in\\{0,1\\}^{d}}b,\\displaystyle\\sum_{c\\in\\{0,1\\}^{d}}c\\rangle}\\\\ &{=L\\cdot\\epsilon\\cdot d\\cdot2^{d}}\\\\ &{=L\\cdot\\epsilon\\cdot d\\cdot\\boldsymbol{n}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where the second step follows from function $f$ is $L$ -Lipschitz. ", "page_idx": 37}, {"type": "text", "text": "Thus, we complete the proof. ", "page_idx": 37}, {"type": "text", "text": "E Transforming Manhattan to Euclidean ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "In this section, we prove Theorem E.2, which states that functions $f$ that transform Manhattan distances to squared Euclidean distances are Bernstein. This section is organized as follows ", "page_idx": 37}, {"type": "text", "text": "\u2022 In Section E.1, we show that any function $f:\\mathbb{R}_{\\geq0}\\rightarrow\\mathbb{R}_{\\geq0}$ transforming Manhattan to squared Euclidean is increasing. This serves as a warm-up for our main theorem.   \n\u2022 In Section E.2, we prove the main result of this section, Theorem E.2. We do this by showing that there is a sequence of $d$ -dimensional hyperrectangles such that the kernel matrix associated with them has an eigenvalue which converges to the $d^{t h}$ order finite difference of $f$ .   \n\u2022 In Section E.3, Lemma E.3 shows $f$ must be bounded and continuous. This lemma is used in the proof of our main result. ", "page_idx": 38}, {"type": "text", "text": "E.1 Manhattan to Euclidean Transforms are Increasing ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Lemma E.1. If $f$ transforms Manhattan to squared Euclidean, then $f$ is increasing on $\\mathbb{R}_{+}$ ", "page_idx": 38}, {"type": "text", "text": "Proof. We fix $c>0$ and show $f^{\\prime}(c)\\geq0$ . Consider $\\chi:[d]\\rightarrow\\{0,1\\}$ which transforms 1 to 1 and everything else to 0. Let $a_{1}=\\epsilon$ and $\\begin{array}{r}{\\dot{a}_{2},\\cdot\\cdot\\cdot a_{d}=\\frac{2c}{d}}\\end{array}$ . Here, $\\epsilon$ is a constant which we will adjust later. The eigenvalue corresponding to $\\chi$ (by Lemma I.2) is, by straightforward calculation: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\sum_{s=0}^{d-1}\\left({d-1\\atop s}\\right)\\left(f\\left({\\frac{2c s}{d}}\\right)-f\\left({\\frac{2c s}{d}}+\\epsilon\\right)\\right)\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "If we divide by $2^{d-1}$ and take $d$ to to infinity, the quantity in Eq. (7) becomes ", "page_idx": 38}, {"type": "equation", "text": "$$\nf(c)-f(c+\\epsilon)\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "for continuous functions $f$ . Indeed, nearly all of the probability mass in the binomial coefficients concentrates around $s=d/2$ by the law of large numbers and the limit follows from continuity of $f$ and the boundedness of $f$ on bounded sets established below in Lemma E.3. ", "page_idx": 38}, {"type": "text", "text": "Applying Lemma A.8, we see that if $f$ transforms Manhattan to squared Euclidean distances, then $f(c)-f(c+\\epsilon)\\leq0$ for any $\\varepsilon>0$ . This implies the desired result. \u53e3 ", "page_idx": 38}, {"type": "text", "text": "E.2 Manhattan to Euclidean Transforms are Bernstein ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "The goal of this section is to prove Theorem E.2. ", "page_idx": 38}, {"type": "text", "text": "Theorem E.2 (Manhattan to squared Euclidean, formal version of part $(1)\\Leftrightarrow$ part (3) of Theorem 4.4).   \nIf $f$ transforms Manhattan distances to squared Euclidean distances, it must be Bernstein. ", "page_idx": 38}, {"type": "text", "text": "Proof. Fix a $k$ -tuple $\\epsilon=(\\epsilon_{1},\\hdots,\\epsilon_{k})$ of positive real numbers and define ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\Delta_{\\epsilon}^{k}(f,t):=f(t)-\\sum_{i_{1}\\in[k]}f(t+\\epsilon_{i_{1}})+\\sum_{i_{1}<i_{2}\\in[k]}f(t+\\epsilon_{i_{1}}+\\epsilon_{i+2})+\\cdot\\cdot+(-1)^{k}f\\left(t+\\sum_{i=1}^{k}\\epsilon_{i}\\right).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Consider $\\chi$ that transforms $1,2,\\ldots k$ to 1 and everything else to 0. Let $a_{i}=\\,\\epsilon_{i}$ for $i\\in[k]$ and $\\begin{array}{r}{a_{k+1}\\ldots a_{d}=\\frac{2c}{d}}\\end{array}$ where $c,k$ and $\\epsilon$ are fixed. ", "page_idx": 38}, {"type": "text", "text": "The eigenvalue corresponding to $\\chi$ is, by direct calculation using Lemma I.2: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\lambda_{\\chi}=\\sum_{s=0}^{d-k}{\\binom{d-k}{s}}\\Delta_{\\epsilon}^{k}(f,2s c/d).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Eq. (8) is the $d$ -dimensional analog of Eq. (7), and this eigenvalue must satisfy $\\lambda_{\\chi}\\leq0$ by Lemma A.8. Dividing by $2^{d-k}$ and taking $d$ to infinity, we obtain: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\Delta_{\\epsilon}^{k}(f,c)\\leq0.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "This is because again the probability mass in the binomial coefficients in Eq. (8) concentrates around the $s=d/2$ coefficient, where we use continuity and boundedness of $f$ for any compact set (guaranteed by Lemma E.3). By Proposition A.2 this implies $f$ is Bernstein (Definition 4.3) since $k,c$ were arbitrary. This completes the proof. \u53e3 ", "page_idx": 38}, {"type": "text", "text": "E.3 Manhattan to Euclidean Transforms are Bounded ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "The goal of this section is to prove Lemma E.3. ", "page_idx": 39}, {"type": "text", "text": "Lemma E.3. Any function $f:\\mathbb{R}_{\\geq0}\\rightarrow\\mathbb{R}_{\\geq0}$ that transforms Manhattan to squared Euclidean is bounded on bounded sets and continuous on $(0,\\infty)$ . ", "page_idx": 39}, {"type": "text", "text": "Proof. By the triangle inequality, $f(x)\\leq f(1/2)+f(1/2)$ for all $0\\leq x\\leq1$ , so $f$ is bounded on $[0,1]$ . By scaling, from now on we assume $f$ is bounded by 1 on $[0,1]$ . ", "page_idx": 39}, {"type": "text", "text": "Now, we show $f$ is continuous on $(0,1)$ . Suppose there is a discontinuity at some point $0<p<1$ . This means that there exists some $\\varepsilon$ such that for all $\\delta>0$ , there are $a,b\\in[p-\\delta,p+\\delta]$ such that $f(a)-f(b)\\geq\\varepsilon$ . Since $f(x)\\leq1$ for all $x\\in[0,1]$ , this means that for all $\\delta<\\operatorname*{min}\\{p,1-p\\}$ , we have that f(a) $\\textstyle{\\frac{f(a)}{f(b)}}>1+\\varepsilon$ . ", "page_idx": 39}, {"type": "text", "text": "Now, fix some $\\varepsilon$ satisfying the above, and some $n\\,=\\,2k$ . Consider points $x_{1},\\ldots,x_{n}$ partitioned into sets $A=x_{1},\\ldots,x_{k}$ and $B=x_{k+1},\\ldots,x_{n}$ . For some small $\\delta$ that we will choose later, pick $a,b\\in[p-\\delta,p+\\delta]$ such that $\\textstyle{\\frac{f(a)}{f(b)}}>1+\\varepsilon$ , and define the metric ", "page_idx": 39}, {"type": "equation", "text": "$$\nd(x_{i},x_{j}):={\\left\\{\\begin{array}{l l}{0}&{i=j}\\\\ {a}&{i,j\\in A,i\\neq j}\\\\ {b}&{i{\\mathrm{~or~}}j{\\mathrm{~is~in~}}B,i\\neq j}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Now apply $f$ : this gives us some metric $d^{\\prime}(x_{i},x_{j})$ such that ", "page_idx": 39}, {"type": "equation", "text": "$$\nd^{\\prime}(x_{i},x_{j}):={\\binom{0}{f(a)}}\\sp i=j\\sp\\prime}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "We show that matrix $D_{i,j}^{\\prime}:=d^{\\prime}(x_{i},x_{j})$ is not negative type if $n$ is sufficiently large (as a function of $\\varepsilon$ ). Consider the vector ", "page_idx": 39}, {"type": "equation", "text": "$$\nv=(1,1,\\dots1,-1,-1,\\dots-1)\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "with the first $k$ coordinates are ones and the last $k$ coordinates are negative ones. This is orthogonal to the all ones vector, but ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{v^{\\top}D^{\\prime}v=k(k-1)f(a)-2k^{2}f(b)+k(k-1)f(b)}\\\\ &{\\qquad\\qquad=k(k-1)f(a)-k(k+1)f(b).}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Since $\\textstyle{\\frac{f(a)}{f(b)}}>1+\\varepsilon$ , if we choose $n>100/\\varepsilon$ , we will have that ", "page_idx": 39}, {"type": "equation", "text": "$$\nk(k-1)\\cdot f(a)-k(k+1)\\cdot f(b)>0.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Therefore, by Lemma A.7, $d^{\\prime}$ does not embed into $\\ell_{2}^{2}$ , Squared Euclidean space. ", "page_idx": 39}, {"type": "text", "text": "However, we show that if $\\delta$ is sufficiently small (in terms of $n,p)$ , then $d(x_{i},x_{j})$ is embeddable into $\\ell_{1}$ . First note that the metric $d_{1}(i,j)$ which equals 0 if $i=j$ and $c$ for some constant $c>0$ is embeddable into $\\ell_{1}$ , by transforming $i$ to $\\begin{array}{r}{x_{i}=\\frac{c}{2}\\cdot e_{i}}\\end{array}$ for all $i$ , where $e_{i}$ is the ith unit vector. Likewise, the metric $d_{k,\\ell}(i,j)$ which equals 0 if $i=j$ or if $i=k,j=\\ell$ or $i=\\ell,j=k$ and $c$ otherwise is also embeddable into $\\ell_{1}$ , by transforming $i$ to $\\begin{array}{r}{x_{i}=\\frac{c}{2}\\cdot e_{i}}\\end{array}$ , except $\\ell$ which is sent to $x_{\\ell}=x_{k}={\\textstyle{\\frac{c}{2}}}\\cdot e_{k}$ . Now, it is trivial to see that by adding a finite number of these metrics, we still get a metric that is embeddable into $\\ell_{1}$ . ", "page_idx": 39}, {"type": "text", "text": "But, if $\\begin{array}{r}{\\frac{a}{b}\\;\\in\\;\\left[1-\\frac{1}{10n^{2}},1+\\frac{1}{10n^{2}}\\right]}\\end{array}$ , then any metric such that $d(i,j)\\,\\in\\,\\{a,b\\}$ for all $a,b$ can be written as some positive finite combination of $d_{1}$ and $d_{k,\\ell}$ over all $1\\leq k<\\ell\\leq n$ . ", "page_idx": 39}, {"type": "text", "text": "Therefore, if $f$ is discontinuous at $p$ , we can set $\\textstyle n\\,=\\,{\\frac{100}{\\varepsilon}}$ 100, \u03b4 = $\\begin{array}{r}{\\delta\\;=\\;\\frac{\\operatorname*{min}\\left(p,1-p\\right)}{100n^{2}}}\\end{array}$ , and the metric on $x_{1},\\ldots,x_{n}$ as defined previously. We will have that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\frac{a}{b}\\in\\left[1-\\frac{1}{10n^{2}},1+\\frac{1}{10n^{2}}\\right]\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "whereas $\\textstyle{\\frac{f(a)}{f(b)}}>1+\\varepsilon$ , which means that while $d$ is embeddable into $\\ell_{1}$ , $d^{\\prime}=f(d)$ is not embeddable into $\\ell_{2}^{2}$ . Thus, if $f$ is discontinuous at $p$ , we have that $f$ cannot transform Manhattan Distances to Squared Euclidean distances. ", "page_idx": 40}, {"type": "text", "text": "By scaling the $x$ -axis, we have that $f$ is bounded on any interval $[0,a]$ and that $f$ is continuous at all $x>0$ . \u53e3 ", "page_idx": 40}, {"type": "text", "text": "F Transforming Manhattan to Manhattan ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "This section is organized as follows: ", "page_idx": 40}, {"type": "text", "text": "\u2022 Section F.1 shows how to compute an explicit Euclidean embedding for the distances obtained after applying a Manhattan-to-Euclidean transform to a set of Manhattan distances. \u2022 In Section F.2, we prove Theorem F.3, which states that Manhattan to Manhattan transforms are equivalent to Manhattan to Squared Euclidean transforms. This may be surprising since Squared Euclidean distances are much more general than Manhattan distances. \u2022 Section F.3 gives a discussion for how to generalize our result if the initial metric space was any metric with a vertex-transitive symmetry group, instead of Manhattan distance. We emphasize the importance of vertex transitivity of the symmetry group, and explain why that feature is important for our techniques to apply. ", "page_idx": 40}, {"type": "text", "text": "F.1 Explicit Embeddings for Manhattan to Euclidean Transforms ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Suppose $f$ transforms Manhattan distance to squared Euclidean distance. By definition, $f$ satisfies the following: for any $n$ and any $x_{1},...\\,x_{n}\\in\\bar{(}\\mathbb{R}^{\\mathbb{N}},\\ell_{1})$ , there exist $p_{1},\\ldots\\dot{p_{n}}\\in(\\mathbb{R}^{\\mathbb{N}},\\ell_{2})$ such that $f(\\|x_{i}-x_{j}\\|_{1}^{\\frac{1}{}})=\\|p_{i}-p_{j}\\|_{2}^{2}$ . We can assume without loss of generality that points $x_{1},x_{2},\\ldots x_{n}$ are distinct corners of a $d$ dimensional hyperrectangle (Definition C.3), and $n=2^{d}$ . This is because any point set in $\\ell_{1}$ can be embedded isometrically into $\\ell_{1}$ on corners of a hyperrectangle (Lemma A.3). ", "page_idx": 40}, {"type": "text", "text": "Lemma F.1. Let $D$ be the matrix where $D_{i,j}=f(\\|x_{i}-x_{j}\\|_{1})$ , and let $M:=-\\textstyle{\\frac{1}{2}}\\Pi D\\Pi$ . Then $M$ has eigenvectors $H_{d}$ . ", "page_idx": 40}, {"type": "text", "text": "Proof. This follows from Lemma B.1 and the definition of $M$ . It is critically important that the columns of $H_{d}$ are orthogonal to the all ones vector (with the exception of the all ones column in $H_{d.}$ ). \u53e3 ", "page_idx": 40}, {"type": "text", "text": "Lemma F.2. Let $M=H_{d}\\Sigma H_{d}$ be an eigendecomposition of $M$ , where M is defined as in Lemma F.1.   \nIf $f$ transforms $\\ell_{1}$ to $\\ell_{2}^{2}$ , then $\\Sigma$ has entirely non-negative entries. ", "page_idx": 40}, {"type": "text", "text": "For each $i$ , we use $p_{i}$ to denote the i-th column of ${\\cal P}\\,=\\,\\sqrt{\\Sigma}H_{d},$ we have $\\langle p_{i},p_{j}\\rangle\\,=\\,M_{i,j}$ and $f(\\|x_{i}-x_{j}\\|_{1})=\\|p_{i}-p_{j}\\|_{2}^{2}$ . ", "page_idx": 40}, {"type": "text", "text": "Proof. This follows from Lemma A.9 and Lemma F.1. ", "page_idx": 40}, {"type": "text", "text": "F.2 Manhattan to Manhattan Transforms are Equivalent to Manhattan to Squared-Euclidean Transforms ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "The goal of this section is to prove Theorem F.3. ", "page_idx": 40}, {"type": "text", "text": "Theorem F.3 (Manhattan to squared Euclidean, formal version of part $(2)\\Leftrightarrow$ part (3) of Theorem 4.4). Any function that transforms Manhattan distances to squared Euclidean distances must transform Manhattan distances to Manhattan distances, and vice versa. ", "page_idx": 40}, {"type": "text", "text": "Proof. Let $p_{i}$ be defined as in Lemma F.2. By construction, the vectors $p_{i}$ are a subset of the corners of a $2^{d}$ -dimensional hyperrectangle, with side lengths $\\sqrt{\\Sigma_{i,i}}$ . Thus, the pairwise squared Euclidean distances between $p_{i}$ are isometrically embeddable into $\\ell_{1}$ by Lemma A.4. In other words, $f(\\|x_{i}-x_{j}\\|_{1})=\\|p_{i}-p_{j}\\|_{2}^{2}=\\|q_{i}-q_{j}\\|_{1}$ for some $q_{i}\\in\\ell_{1}$ for all $i,j$ . This shows that any $f$ that transforms $\\ell_{1}$ to $\\ell_{2}^{2}$ transforms $\\ell_{1}$ to $\\ell_{1}$ as desired. \u53e3 ", "page_idx": 40}, {"type": "text", "text": "Note that for any $x_{i}$ , the vectors $q_{i}$ are finite dimensional and can be explicitly written down in closed form. ", "page_idx": 41}, {"type": "text", "text": "F.3 Metric Transforms for Distances with Group Symmetries ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "In our proof of Theorem F.3, we exploited that our points $x_{1},\\ldots x_{n}$ are points in a hyperrectangle, which has a vertex transitive group symmetry. Similar theories can be generated when the point set lives on any object with a vertex-transitive group symmetry, and the distance measure between points is some function of the Euclidean distance. Such objects include higher dimensional platonic solids, spheres, equilateral triangular prisms, and more. ", "page_idx": 41}, {"type": "text", "text": "We remark that the group symmetry must be vertex-transitive to ensure the matrix $D$ in Lemma B.1 has an eigenvector equal to the all ones vector. If this were not the case, Lemma F.1 would no longer hold. ", "page_idx": 41}, {"type": "text", "text": "G Positive Definite Manhattan Kernels ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "The goal of this section is to prove Theorem G.2, a formal restatement of Theorem 3.4. ", "page_idx": 41}, {"type": "text", "text": "The section is organized as follows: ", "page_idx": 41}, {"type": "text", "text": "\u2022 In Section G.1, we show that positive definite Manhattan kernels map positive numbers to positive numbers. This will be used in our proof of Theorem G.2.   \n\u2022 In Section G.2, we present and prove Theorem G.2. This result classifies all positive definite Manhattan kernels (Definition 3.1), and proves that positive definite Manhattan kernels are equivalent to completely monotone functions. ", "page_idx": 41}, {"type": "text", "text": "G.1 Positive Definite Manhattan Kernels map Positive Reals to Positive Reals ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "First, we prove the following lemma. ", "page_idx": 41}, {"type": "text", "text": "Lemma G.1. If $f$ is a positive definite Manhattan kernel (Definition 3.1), then $f(t)\\,\\geq\\,0$ for all $t\\geq0$ . ", "page_idx": 41}, {"type": "text", "text": "Proof. Let $\\mathcal{X}$ denote metric space $(\\mathbb{R}^{N},\\ell_{1})$ . For any $N\\geq0$ we consider the points $\\begin{array}{r}{x_{i}=\\frac{t}{2}e_{i}\\in\\mathcal{X}}\\end{array}$ for $i\\in[N]$ where $e_{i}=(0,\\ldots,0,1,0,\\ldots,0)$ is a standard basis vector, so that $\\|{\\boldsymbol{x}}_{i}-{\\boldsymbol{x}}_{j}\\|_{1}^{\\overline{{\\mathbf{\\alpha}}}}=t$ for any $i\\neq j$ . Since the matrix of values $(f(||x_{i}-x_{j}||_{1})_{i,j\\in[N]}$ must be positive semidefinite, the sum of all its entries must be positive, hence: ", "page_idx": 41}, {"type": "equation", "text": "$$\nN(f(0)+(N-1)f(t))\\geq0.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "The above equation implies the following: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\frac{f(0)}{N-1}+f(t)\\geq0\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "for all integer $N\\geq0$ and real $t\\geq0$ . ", "page_idx": 41}, {"type": "text", "text": "Since $N$ can be arbitrarily large, therefore we conclude $f(t)\\geq0$ as claimed. ", "page_idx": 41}, {"type": "text", "text": "G.2 Positive Definite Manhattan Kernels are Completely Monotone ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "The goal of this section is to prove Theorem G.2. ", "page_idx": 41}, {"type": "text", "text": "Theorem G.2 (Formal statement of Theorem 3.4). $f:\\mathbb{R}_{\\geq0}\\rightarrow\\mathbb{R}$ is a positive definite Manhattan kernel (Definition 3.1) if and only if $f(x)$ is completely monotone (Definition 3.2). ", "page_idx": 41}, {"type": "text", "text": "Proof. First, we prove that if $f$ is a positive definite Manhattan kernel, then $f$ must be completely monotone. The converse direction is previously known, and is a consequence of Lemma A.5 and Theorem 3 of $[\\mathrm{SOW}01]^{11}$ . ", "page_idx": 41}, {"type": "text", "text": "Suppose that $f$ is a positive definite Manhattan kernel (Definition 3.1). Cauchy-Schwarz easily implies that $f(t)\\leq f(0)$ for all $t$ , so $f$ is bounded.. Now, if $x_{1},\\ldots,x_{n}$ correspond to $y_{1},\\ldots,y_{n}$ then ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{f(||x_{i}-x_{j}||_{1})=\\langle y_{i},y_{j}\\rangle}}\\\\ {\\displaystyle{\\qquad\\qquad=f(0)-\\frac{1}{2}\\|y_{i}-y_{j}\\|_{2}^{2}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Therefore $2(f(0)-f(t))$ (equivalently, $f(0)-f(t))$ sends Manhattan distances to squared Euclidean distances. Therefore $f(0)-f(t)$ is Bernstein (Definition 4.3), by Theorem 4.4. Combining with Lemma G.1 we conclude that $f$ must be completely monotone (Definition 3.2). ", "page_idx": 42}, {"type": "text", "text": "H Positive Definite Euclidean Kernels ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "The goal of this section is to prove the foundational classification of positive definite Euclidean kernels [Smo96, SOW01, Sch42]. We focus on p\u221aroving the \u2018hard\u2019 direction, that a function $f$ is a positive definite Euclidean kernel only if $f({\\bar{\\sqrt{x}}})$ is completely monotone. Such a proof is straightforward from Theorem G.2, as seen below. For the other simpler direction, see the simple proof in Proposition 11 of $[\\mathrm{SSB^{+}97}]$ . ", "page_idx": 42}, {"type": "text", "text": "Th\u221aeorem H.1. [Sch42] $f:\\mathbb{R}_{\\geq0}\\rightarrow\\mathbb{R}$ is a positive definite Euclidean kernel (Definition 3.1) only if $f({\\sqrt{x}}))$ ) is a completely monotone function (Definition 3.2). ", "page_idx": 42}, {"type": "text", "text": "Proof. Theorem G.2 combined with the fact that Man\u221ahattan distances isometrically embed into squared Euclidean distances (see [DL09]) prove that $f({\\sqrt{x}})$ must be completely monotone if it is a positive definite Euclidean kernel. \u53e3 ", "page_idx": 42}, {"type": "text", "text": "I Eigenvalue of Kernels from the Hyperrectangle ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "In this section, we prove Lemma I.2, a variation of Lemma B.1. This lemma uses representation theoretic ideas to compute the eigenvalues of matrices arising from the real hyperrectangle. We use this modified formulation in our proofs of Theorems 4.4 and 3.4. We introduce Lemma I.3, which expresses these same eigenvalues in terms of integrals, a lemma of independent interest. ", "page_idx": 42}, {"type": "text", "text": "I.1 Matrices with Reflectional Symmetries have Hadamard Eigenvectors ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Lemma I.1. Let $g:(\\mathbb R^{d}\\times\\mathbb R^{d})\\rightarrow\\mathbb R$ such that $g(x,y)$ is invariant under axis reflection. Consider a $d$ -dimensional hyperrectangle with corners $x_{1},\\ldots.x_{2^{d}}$ . Let $D$ be a $2^{d}$ by $2^{d}$ matrix such that $D_{i j}\\,=\\,g(x_{i},x_{j})$ . Then there is an eigendecomposition of $D$ into $H_{d}\\Sigma H_{d}$ where $\\Sigma$ is a diagonal matrix. ", "page_idx": 42}, {"type": "text", "text": "Proof. This lemma can be proven directly via computation. However, it is more instructive to view this through the representation theoretic lens. We note that $D$ has the property that for any permutation matrix $\\sigma$ corresponding to a reflection about one of the hyperrectangle\u2019s axes, we have $\\sigma D=D\\sigma$ . Schur\u2019s lemma from representation theory (see Lemma A.10) states that $D$ and all $\\sigma$ in the reflectional symmetry group of the hyperrectangle have a common set of eigenvectors. It is straightforward to verify that the only common set of eigenvectors for all $\\sigma$ is the columns of the Hadamard matrix, and thus $D$ must have the columns of $H_{d}$ as its eigenvectors. \u53e3 ", "page_idx": 42}, {"type": "text", "text": "We note that variants of this lemma are used to prove Delsarte\u2019s linear programming bound in error correcting codes [Del73, O\u2019D14]. ", "page_idx": 42}, {"type": "text", "text": "I.2 Eigenvalues of Kernels from the Hyperrectangle, Restated ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Lemma I.2 (Eigenvalue of Manhantan Kernels, formal version of Lemma B.1). Consider a ddimensional hyperrectangle (Definition C.3) parameterized by $a_{1},\\dots.a_{d}>0$ . Enumerate the vertices in lexicographical ordering as p1, . . . p2d. ", "page_idx": 42}, {"type": "text", "text": "1. $\\Sigma:=H_{d}D H_{d}$ is a diagonal matrix whose entries are the eigenvalues of $D$ multiplied by $2^{d}$ , and $D=4^{-d}\\cdot H_{d}\\Sigma\\bar{H_{d}}$ . ", "page_idx": 43}, {"type": "text", "text": "2. Let $\\chi:[d]\\rightarrow\\{0,1\\}$ . Let $k$ equal the integer corresponding to transforming $\\chi$ (written as a $d$ dimensional binary vector) into an integer via binary conversion. For each $\\chi,$ there is an eigenvector of $D$ equal to the $k$ -th column of Hadamard matrix $H_{d}$ , and its associated eigenvalue is: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\sum_{T\\subseteq[d]}(-1)^{\\sum_{t\\in T}\\chi(t)}f\\left(\\sum_{t\\in T}a_{t}\\right).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "The second part of this theorem on its surface differs from that in Lemma B.1, but the statements are in fact identical via straightforward computation. ", "page_idx": 43}, {"type": "text", "text": "Proof. By Lemma I.1, we know that the Hadamard matrix columns are eigenvectors of the matrix $D$ . The result follows by direct computation. \u53e3 ", "page_idx": 43}, {"type": "text", "text": "We now give an alternate formulation of the eigenvalues in Lemma I.2. This lemma is of independent interest. ", "page_idx": 43}, {"type": "text", "text": "Lemma I.3. Given a box with side lengths $a_{1},\\ldots,a_{d},$ , each eigenvalue corresponds to a function $\\chi:[d]\\rightarrow\\{0,1\\}$ . Let $Q=\\{q_{1},\\ldots q_{k}\\}$ be the full set of values on which $\\chi$ is 1. Then the Eigenvalues in Eq. (9) equal: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\sum_{T\\subseteq[d]\\backslash Q}\\int_{\\sum_{t\\in T}a_{t}}^{a_{q_{1}}+\\sum_{t\\in T}a_{t}}\\cdot\\cdot\\int_{\\sum_{t\\in T}a_{t}}^{a_{q_{k}}+\\sum_{t\\in T}a_{k}}(-1)^{k}\\frac{\\mathrm{d}^{k}f}{\\mathrm{d}x^{k}}\\Big(\\sum_{q\\in Q}s_{q}\\Big)\\mathrm{d}s_{1}\\cdot.\\cdot\\mathrm{d}s_{k}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Proof. The proof follows directly from Lemma I.2 combined with the fundamental theorem of calculus. ", "page_idx": 43}, {"type": "text", "text": "J Converse to Stable Rank ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Recall Definition 2.7, for a matrix $A$ , we use srank $(A)$ to denote the stable rank of $A$ . For a matrix $A$ , we use $\\|A\\|_{F}$ to denote its Frobenius norm. We use $\\|A\\|$ to denote its spectral norm. ", "page_idx": 43}, {"type": "text", "text": "J.1 Lipschitz functions preserve stable rank ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Definition J.1. We say function $f$ is $(L,\\ell)$ -Lipshitz on on entries of matrix $A$ , if for any $x\\geq0$ in entries of $A$ such that ", "page_idx": 43}, {"type": "equation", "text": "$$\n{\\frac{1}{\\sqrt{\\ell}}}x\\leq f(x)\\leq{\\sqrt{L}}x\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Theorem J.2. We define $B\\in\\mathbb{R}_{\\geq0}^{n\\times n}$ as follows $B_{i,j}=f(A_{i,j})$ for all $i\\in[n]$ and for all $j\\in[n]$ . If function $f$ is $(L,\\ell)$ -Lipschitz on $A\\in\\mathbb{R}_{\\geq0}^{n\\times n}$ , then we have the following ", "page_idx": 43}, {"type": "text", "text": "\u2022 Part 1. $1/{\\sqrt{l}}\\cdot\\|A\\|_{F}\\leq\\|B\\|_{F}\\leq{\\sqrt{L}}\\cdot\\|A\\|_{F}.$   \n\u2022 Part 2. $1/{\\sqrt{l}}\\cdot\\|A\\|\\leq\\|B\\|\\leq{\\sqrt{L}}\\cdot\\|A\\|$ .   \n\u2022 Part 3. $L^{-1}\\cdot\\ell^{-1}\\cdot\\operatorname{srank}(A)\\leq\\operatorname{srank}(B)\\leq L\\cdot\\ell\\cdot\\operatorname{srank}(A).$ ", "page_idx": 43}, {"type": "text", "text": "Proof. Proof of Part 1. ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "We can upper bound $\\|B\\|_{F}^{2}$ as follows ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|B\\|_{F}^{2}=\\sum_{i=1}^{n}\\sum_{j=1}^{n}f(A_{i,j})^{2}}\\\\ {\\displaystyle\\leq L\\cdot\\sum_{i=1}^{n}\\sum_{j=1}^{n}A_{i,j}^{2}}\\\\ {\\displaystyle=L\\cdot\\|A\\|_{F}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where the first step follows from definition of $\\|\\cdot\\|_{F}$ , the second step follows from $f(A_{i,j})^{2}\\leq L\\!\\cdot\\!A_{i,j}^{2}$ , and the last step follows from definition of $\\|\\cdot\\|_{F}$ norm. ", "page_idx": 44}, {"type": "text", "text": "We can lower bound $\\|\\boldsymbol{B}\\|_{F}^{2}$ as follows ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|B\\|_{F}^{2}=\\sum_{i=1}^{n}\\sum_{j=1}^{n}f(A_{i,j})^{2}}\\\\ {\\displaystyle\\geq l^{-1}\\cdot\\sum_{i=1}^{n}\\sum_{j=1}^{n}A_{i,j}^{2}}\\\\ {\\displaystyle=l^{-1}\\cdot\\|A\\|_{F}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where first step follows from definition of $\\|\\cdot\\|_{F}$ norm, the second step follows from $f(A_{i,j})^{2}\\geq$ $l^{-1}\\cdot A_{i,j}^{2}$ , and the last step follows from definition of $\\|\\cdot\\|_{F}$ norm. ", "page_idx": 44}, {"type": "text", "text": "Proof of Part 2. ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "We can upper bound on $\\left\\|B\\right\\|=\\sigma(B)$ as follows ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sigma(B)^{2}=\\operatorname*{max}_{\\parallel\\boldsymbol{v}\\parallel_{2}=1}(\\boldsymbol{v}^{\\top}B\\boldsymbol{v})^{2}}}\\\\ &{=\\displaystyle\\operatorname*{max}_{\\parallel\\boldsymbol{v}\\parallel_{2}=1}\\big(\\sum_{i=1}^{n}\\sum_{j=1}^{n}v_{i}v_{j}f(A_{i,j})\\big)^{2}}\\\\ &{\\le\\displaystyle\\operatorname*{max}_{\\parallel\\boldsymbol{v}\\parallel_{2}=1}\\big(\\sum_{i=1}^{n}\\sum_{j=1}^{n}v_{i}v_{j}A_{i,j}\\big)^{2}\\cdot L}\\\\ &{=\\sigma(A)^{2}\\cdot L}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where the first step follows definition of spectral norm, the third step follows from $f(A_{i,j})^{2}\\leq A_{i,j}^{2}\\cdot L$ . We can lower bound on $\\left\\|B\\right\\|=\\sigma(B)$ as follows ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\sigma(B)^{2}=\\underset{\\Vert v\\Vert_{2}=1}{\\operatorname*{max}}(v^{\\top}B v)^{2}}&{}\\\\ &{=\\underset{\\Vert v\\Vert_{2}=1}{\\operatorname*{max}}(\\sum_{i,j}v_{i}v_{j}f(A_{i,j}))^{2}}\\\\ &{\\ge\\underset{\\Vert v\\Vert_{2}=1}{\\operatorname*{max}}(\\underset{i,j}{\\sum}v_{i}v_{j}A_{i,j})^{2}/\\ell}\\\\ &{=\\sigma(A)^{2}/\\ell}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where the third step follows from $f(A_{i,j})^{2}\\geq A_{i,j}^{2}/\\ell$ . ", "page_idx": 44}, {"type": "text", "text": "Proof of Part 3. ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "We can upper bound $\\operatorname{srank}(B)$ as follows ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{srank}(B)=\\displaystyle\\frac{\\|B\\|_{F}^{2}}{\\sigma(B)^{2}}}&{}\\\\ &{\\leq\\displaystyle\\frac{L\\cdot\\|A\\|_{F}^{2}}{\\sigma(B)^{2}}}\\\\ &{\\leq\\displaystyle\\frac{L\\cdot\\|A\\|_{F}^{2}}{\\sigma(A)^{2}/\\ell}}\\\\ &{=L\\cdot\\ell\\cdot\\mathrm{srank}(A)}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where the first step follows from definition of stable rank, the second step follows from Eq. (10), the third step follows from Eq. (13), and the last step follows from stable rank. ", "page_idx": 45}, {"type": "text", "text": "We can lower bound $\\operatorname{srank}(B)$ as follows ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathrm{srank}(B)=\\frac{\\|B\\|_{F}^{2}}{\\sigma(B)^{2}}}&{}&\\\\ {\\geq\\frac{\\|A\\|_{F}^{2}/\\ell}{\\sigma(B)^{2}}}&{}&\\\\ {\\geq\\frac{\\|A\\|_{F}^{2}/\\ell}{\\sigma(A)^{2}L}}&{}&\\\\ {=L^{-1}\\cdot\\ell^{-1}\\cdot\\mathrm{srank}(A)}&{}&\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where the first step follows from definition of stable rank, the second step follows from Eq. (11), the third step follows from Eq. (12), and the last step follows from definition of stable rank. ", "page_idx": 45}, {"type": "text", "text": "J.2 Fast-Growing functions do not preserve stable rank ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "We start with presenting a tool for symmetric matrix. ", "page_idx": 45}, {"type": "text", "text": "Lemma J.3. Consider a symmetric matrix $M$ with non-negative entries, with the all ones vector as an eigenvector. The eigenvalue of this vector is the largest eigenvalue of $M$ ", "page_idx": 45}, {"type": "text", "text": "Proof. This follows from the Perron Froebenius formula [Per07, Fro12] on non-negative matrices. ", "page_idx": 45}, {"type": "text", "text": "For a matrix $M$ , we use $M_{i,j}$ to denote the entry at $i$ -th row and $j$ -th column in the matrix. ", "page_idx": 45}, {"type": "text", "text": "Lemma J.4. Let $M$ be a n by $n$ matrix with non-negative entries, with an eigenvector that is the alll ones vector. Suppose that there exists permutation $\\sigma:[n]\\to[n]$ such that $M_{i,\\sigma(i)}$ is the unique largest element in row i for all $i\\in[n]$ . Suppose that $k:=M_{i,\\sigma(i)}$ which is independent of $i$ , and all other entries are less than s where $s<k$ . ", "page_idx": 45}, {"type": "text", "text": "Then, we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\operatorname{srank}(M)\\geq{\\frac{n k^{2}}{(k+n s)^{2}}}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Proof. By Lemma J.3, the eigenvalue of $M$ corresponding to the all ones vector, is the largest eigenvalue of $M$ . Recall that the stable rank of $M$ is defined as the Froebenius norm squared, divided by the spectral norm squared. ", "page_idx": 45}, {"type": "text", "text": "The largest eigenvalue of $M$ is $\\sum_{i}M_{i1}$ which is bounded above by $k+s n$ . Meanwhile, the squared Froebenius norm of $M$ is bounded above by $n k^{2}$ , which is the sum of squares of the diagonal elements. This completes the proof. Dividing our two bounds gives our lemma. \u53e3 ", "page_idx": 45}, {"type": "text", "text": "Now, consider the distance matrix $M$ arising from $f$ applied entry-wise to the matrix arising from the hypercube with side lengths $\\beta$ (this is the hyperrectangle where all side lengths are the same). We ", "page_idx": 45}, {"type": "text", "text": "note that the hypercube matrix has rank $\\log n+1$ , and thus its stable rank is also bounded by this quantity. ", "page_idx": 46}, {"type": "text", "text": "In this case, we note that $M$ has non-negative entries, has the all ones vector as an eigenvector, and has the property that there exists a permutation $\\sigma$ such that $M_{i,\\sigma(i)}$ is $\\beta\\log n$ . Meanwhile, all the other entries are less than $\\beta\\log n-\\beta$ . ", "page_idx": 46}, {"type": "text", "text": "We note that if $k/s>n^{0.5}$ , then the stable rank of $M$ is $\\Omega(n)$ . ", "page_idx": 46}, {"type": "text", "text": "The rest of the proof is devoted to understanding when $k/s>n^{0.5}$ . ", "page_idx": 46}, {"type": "text", "text": "This is equivalent to: ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\frac{f(\\beta\\log n)}{f(\\beta\\log n-\\beta)}\\geq n^{0.5}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Since $\\beta$ can be set to anything, we define $\\gamma:=\\beta\\log n$ , and thus Eq. (14) is equivalent to ", "page_idx": 46}, {"type": "equation", "text": "$$\nf(\\gamma(1+{\\frac{1}{\\log n}}))\\geq n^{0.5}f(\\gamma).\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Theorem J.5 (Superpolynomials don\u2019t preserve stable rank). $f(x)=x^{\\log^{c}x}+o(x^{\\log^{c}x})$ does not preserve stable rank for any $c>0$ . ", "page_idx": 46}, {"type": "text", "text": "Proof. It is sufficient to show that ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\frac{f(\\gamma(1+\\epsilon))}{f(\\gamma)}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "is unbounded for fixed $\\epsilon$ and variable $\\gamma>0$ . ", "page_idx": 46}, {"type": "text", "text": "Substituting $f(x)=x^{\\log^{c}x}$ , we get ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\frac{\\gamma^{\\log^{c}(\\gamma+\\gamma\\varepsilon)}}{\\gamma^{\\log^{c}\\gamma}}=2^{(\\log\\gamma)\\cdot(\\log^{c}(\\gamma+\\gamma\\varepsilon)-\\log^{c}(\\gamma))}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Next, we need to show that ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\gamma\\to\\infty}\\operatorname{Eq.}\\left(17\\right)=\\infty.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "We define function ", "page_idx": 46}, {"type": "equation", "text": "$$\nF(x)=\\log^{c}(x).\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "and compute ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{F^{\\prime}(x)=c\\cdot\\displaystyle\\frac{1}{x}\\log^{c-1}(x),}}\\\\ {{F^{\\prime\\prime}(x)=c\\cdot(c-1)\\displaystyle\\frac{1}{x^{2}}\\log^{c-2}(x)-c\\displaystyle\\frac{1}{x^{2}}\\log^{c-1}(x)}}\\\\ {{\\phantom{F^{\\prime}(x)=c\\cdot\\textstyle\\frac{1}{x}\\log^{c-2}(x)-\\log^{c-1}(x))}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Using mean-value forms of Taylor\u2019s theorem, ", "page_idx": 46}, {"type": "equation", "text": "$$\nF(y)=F(x)+F^{\\prime}(x)\\cdot(y-x)+{\\frac{1}{2}}F^{\\prime\\prime}(z)\\cdot(y-x)^{2}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where $z\\in[x,y]$ ", "page_idx": 46}, {"type": "text", "text": "We can compute ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{F^{\\prime}(x)\\cdot(y-x)=c\\displaystyle\\frac{1}{\\gamma}\\log^{c-1}(\\gamma)(\\epsilon\\gamma)}}\\\\ {{=\\epsilon c\\log^{c-1}(\\gamma)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "We can compute ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{2}F^{\\prime\\prime}(z)\\cdot(y-x)^{2}=\\frac{1}{z^{2}}((c-1)\\log^{c-2}(z)-\\log^{c-1}(z))\\cdot(\\epsilon\\gamma)^{2}}\\\\ {=\\frac{\\epsilon^{2}}{(1+\\alpha)^{2}}((c-1)\\log^{c-2}(z)-\\log^{c-1}(z))}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "where choosing $z=\\alpha\\gamma$ (where $\\alpha\\in[1,(1+\\epsilon)];$ ). ", "page_idx": 47}, {"type": "text", "text": "Then it is obvious to see that $\\begin{array}{r}{|\\frac{1}{2}F^{\\prime\\prime}(z)\\cdot(y-x)^{2}|\\leq\\frac{1}{10}F^{\\prime}(x)\\cdot(y-x).}\\end{array}$ . ", "page_idx": 47}, {"type": "text", "text": "Finally, we can lower bound ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F(\\gamma(1+\\epsilon))-F(\\gamma)\\geq F^{\\prime}(\\gamma)\\cdot\\epsilon\\lambda-\\displaystyle\\frac{1}{2}\\vert F^{\\prime\\prime}(z)\\vert\\cdot(\\epsilon\\gamma)^{2}}\\\\ &{\\qquad\\qquad\\qquad\\geq\\displaystyle\\frac{1}{2}F^{\\prime}(\\gamma)\\cdot\\epsilon\\lambda}\\\\ &{\\qquad\\qquad\\qquad=\\displaystyle\\frac{1}{2}\\epsilon c\\log^{c-1}(\\gamma)}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "where $z\\in[\\gamma,(1+\\epsilon)\\gamma]$ ", "page_idx": 47}, {"type": "text", "text": "Thus, we have ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\gamma\\rightarrow\\infty}{\\operatorname*{lim}}\\mathrm{Eq.}\\left(17\\right)=\\underset{\\gamma\\rightarrow\\infty}{\\operatorname*{lim}}2^{\\left(\\log\\gamma\\right)\\cdot\\left(\\log^{c}\\left(\\gamma+\\gamma\\varepsilon\\right)-\\log^{c}\\left(\\gamma\\right)\\right)}}\\\\ &{\\qquad\\qquad\\qquad=\\underset{\\gamma\\rightarrow\\infty}{\\operatorname*{lim}}2^{\\left(\\log\\gamma\\right)\\cdot\\frac{1}{2}\\epsilon c\\log^{c-1}\\left(\\gamma\\right)}}\\\\ &{\\qquad\\qquad\\qquad=\\infty.}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Thus, we complete the proof. ", "page_idx": 47}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: In our research, we establish that low-degree polynomials are the exclusive functions that consistently yield a low-rank matrix when applied entry-wise to another lowrank matrix, comprehensively categorize functions that transform Manhattan distances into either Manhattan or squared Euclidean distances, and classify all positive definite kernels using Manhattan distance as input. The abstract and introduction accurately reflect our main results, introduce some background, and emphasise the importance of our work. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 48}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: The limitations are discussed in Section 7. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 48}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: All our main results in the paper have formal versions and verified formal proofs. ", "page_idx": 49}, {"type": "text", "text": "\u2022 For Theorem 2.5, see Theorem C.11 for formal statement and Section C for formal proofs.   \n\u2022 For Theorem 2.6, see Theorem D.3 for formal statement and Section D for formal proofs.   \n\u2022 For Theorem 2.8, see Theorem J.2 for formal statement and Section J for formal proofs.   \n\u2022 For Theorem 4.4, see Theorem E.2 and F.3 for formal statement and Section E and F for formal proofs.   \n\u2022 For Theorem 3.4, see Theorem G.2 for formal statement and Section G for formal proofs.   \n\u2022 The proofs of all other theoretical results are below the statements. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 49}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 49}, {"type": "text", "text": "Justification: This is a theoretical paper and does not conduct any experiments. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 49}, {"type": "text", "text": "", "page_idx": 50}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 50}, {"type": "text", "text": "Justification: This is a theoretical paper and does not conduct any experiments. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 50}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 50}, {"type": "text", "text": "Justification: This is a theoretical paper and does not conduct any experiments. Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 50}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 51}, {"type": "text", "text": "Justification: This is a theoretical paper and does not conduct any experiments. Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 51}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 51}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 51}, {"type": "text", "text": "Justification: This is a theoretical paper and does not conduct any experiments. Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 51}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 51}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 51}, {"type": "text", "text": "Justification: We have reviewed the NeurIPS Code of Ethics and ensured that our paper complies with it. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 51}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 52}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Justification: The societal impacts are discussed in Section 8. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 52}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 52}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 52}, {"type": "text", "text": "Justification: This is a theoretical paper and does not conduct any experiments. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 52}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 52}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 52}, {"type": "text", "text": "Justification: This is a theoretical paper and does not conduct any experiments. Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. ", "page_idx": 52}, {"type": "text", "text": "\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 53}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 53}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 53}, {"type": "text", "text": "Justification: This is a theoretical paper and does not conduct any experiments. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 53}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 53}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 53}, {"type": "text", "text": "Justification: This is a theoretical paper and does not conduct any experiments. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 53}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 53}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 53}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 53}, {"type": "text", "text": "Justification: This is a theoretical paper and does not conduct any experiments. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 54}]