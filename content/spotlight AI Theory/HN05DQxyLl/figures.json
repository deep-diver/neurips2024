[{"figure_path": "HN05DQxyLl/figures/figures_2_1.jpg", "caption": "Figure 1: Workflow of latent MI approximation a) Embed high-dimensional data in low-dimensional space such that mutually informative structure is preserved. b) The KSG estimator [10] is used to estimate MI by averaging over pointwise MI (PMI) contributions.", "description": "This figure illustrates the two-step process of the Latent Mutual Information (LMI) approximation method.  (a) shows how high-dimensional data points X and Y are first mapped to lower-dimensional representations Zx and Zy using a neural network-based embedding. This embedding aims to preserve the mutual information between X and Y in the lower-dimensional space. (b) shows that, once the data is in low-dimensional space, a non-parametric MI estimator (KSG) is applied to estimate the mutual information between Zx and Zy.  The KSG estimator achieves this by averaging over the pointwise mutual information (PMI) contributions from all pairs of points.", "section": "2 Approach"}, {"figure_path": "HN05DQxyLl/figures/figures_4_1.jpg", "caption": "Figure 2: MI estimator performance scaling with increasing dimensionality. a) - d) Absolute accuracy measured by mean-squared error over 10 estimates per setting, with ground truth MI between 0 and 2 bits, and 2 \u00b7 103 samples per estimate (1 : 1 training split for neural network-based estimators). e) Estimator with highest absolute accuracy in each setting. Ties broken randomly. f) - i) Relative accuracy measured by Kendall \u03c4 rank correlation of estimates with ground truth. j) Estimator with highest relative accuracy in each setting. Ties broken randomly.", "description": "This figure compares the performance of four mutual information (MI) estimators (InfoNCE, MINE, KSG, and LMI) across different dimensionalities and intrinsic dimensions of the data.  It shows that LMI outperforms the other three estimators, especially in high-dimensional settings with low intrinsic dimensionality. The performance is evaluated using both absolute accuracy (MSE) and relative accuracy (Kendall \u03c4).", "section": "3 Empirical evaluation"}, {"figure_path": "HN05DQxyLl/figures/figures_5_1.jpg", "caption": "Figure 3: Number of samples required to achieve |I(X,Y) \u2013 \u00ce(X, Y)| < \u20ac. a) Data with low-rank dependence structure, with \u20ac = 0.1. b) Moderate-rank dependence structure, with \u20ac = 0.2. c) Full-rank dependence structure, with \u20ac = 0.4. \u201c+\u201d marker indicates that N > 104 samples are required for accurate estimates for all larger d. Ground truth MI is 1 bit in all cases.", "description": "This figure shows the number of samples required by different mutual information estimators to achieve a specific estimation error (\u20ac) under various ambient and intrinsic dimensionalities.  It demonstrates the scalability of LMI approximation compared to KSG, MINE, and InfoNCE.  The plots show that while other methods struggle to converge as dimensionality increases, LMI maintains stable performance provided the dependence structure has low intrinsic dimensionality (k). As k approaches d (full-rank dependence), all methods struggle.", "section": "3 Empirical evaluation"}, {"figure_path": "HN05DQxyLl/figures/figures_6_1.jpg", "caption": "Figure 2: MI estimator performance scaling with increasing dimensionality. a) - d) Absolute accuracy measured by mean-squared error over 10 estimates per setting, with ground truth MI between 0 and 2 bits, and 2 \u00b7 103 samples per estimate (1:1 training split for neural network-based estimators). e) Estimator with highest absolute accuracy in each setting. Ties broken randomly. f) - i) Relative accuracy measured by Kendall \u03c4 rank correlation of estimates with ground truth. j) Estimator with highest relative accuracy in each setting. Ties broken randomly.", "description": "This figure compares the performance of four different mutual information (MI) estimators across various dimensionalities and dependence structures.  The performance is evaluated using both absolute accuracy (MSE) and relative accuracy (Kendall's tau).  LMI consistently outperforms other methods, especially in high dimensions, demonstrating its effectiveness in approximating MI for high-dimensional data with low-dimensional dependence structures.", "section": "3 Empirical evaluation"}, {"figure_path": "HN05DQxyLl/figures/figures_6_2.jpg", "caption": "Figure 5: Constructing problems where LMI fails. a) Examples of symmetric distributions where E[X|Y] = E[X]. b) MI estimates from 103 samples of symmetric variables as exclusive (independent normal) dimensions are added. Ideal estimators are invariant. LMI latent space is 1D per variable.", "description": "This figure demonstrates a scenario where the LMI (Latent Mutual Information) approximation method fails.  Panel (a) shows examples of symmetric distributions where the conditional expectation of X given Y is equal to the expectation of X.  Panel (b) shows how MI estimates from various methods change as independent dimensions are added to these symmetric distributions. In an ideal case, the MI estimates shouldn't vary as these independent dimensions don't affect the MI between X and Y.  The experiment highlights a limitation of LMI in handling symmetric data.", "section": "3.3 Constructing and studying problems where LMI fails"}, {"figure_path": "HN05DQxyLl/figures/figures_7_1.jpg", "caption": "Figure 6: Quantifying dependence between participants of protein interactions. a) - b) MI estimates between interaction partners, compared to randomly permuted data. c) - d) ROC curves of density ratio classifier distinguishing annotated interacting pairs from unannotated \u201cnegative\u201d samples, for all pairs of 170 held-out proteins. Averages over 20 random hold-out splits.", "description": "This figure presents the results of applying the latent mutual information (LMI) approximation method to quantify the statistical dependence between interacting protein pairs (kinase-target and ligand-receptor interactions).  Panels (a) and (b) show MI estimates from the LMI method, and other methods, comparing estimates from real data to those obtained from shuffled (randomized) data which serves as a negative control. Panels (c) and (d) display ROC curves for a classification model designed to distinguish between true interacting protein pairs and non-interacting pairs.  The ROC curves further validate the information captured by the LMI method and protein language model embeddings.", "section": "4.1 Quantifying interaction information in protein language model embeddings"}, {"figure_path": "HN05DQxyLl/figures/figures_9_1.jpg", "caption": "Figure 7: Quantifying cell fate information in single-cell transcriptomes. a) 2D SPRING embedding [44] of lineage-traced single-cell RNA-sequencing data from [42]. b) Pointwise decomposition of MI between sister cells across timepoints, as estimated using LMI. Hue applied to early timepoint cells, late timepoint and unbarcoded cells in grey. Star indicates neutrophil pseudotime value 35 \u00b7 103. c) Smoothed pMI (rolling average over 100 cells) across neutrophil differentiation trajectory. Vertical line denotes neutrophil pseudotime value 35 \u00b7 103.", "description": "This figure shows the results of applying the latent mutual information (LMI) method to single-cell RNA sequencing (scRNA-seq) data from a study of hematopoietic stem cells. Panel (a) shows a 2D representation of the scRNA-seq data, colored by cell type. Panel (b) shows a heatmap of pointwise mutual information (pMI) between pairs of sister cells, calculated using LMI. Panel (c) shows a smoothed version of the pMI along the neutrophil differentiation trajectory, highlighting a sharp increase in pMI around a specific pseudotime value.", "section": "4.2 Identifying cell fate information in hematopoietic stem cells"}, {"figure_path": "HN05DQxyLl/figures/figures_17_1.jpg", "caption": "Figure 2: MI estimator performance scaling with increasing dimensionality. a) - d) Absolute accuracy measured by mean-squared error over 10 estimates per setting, with ground truth MI between 0 and 2 bits, and 2 \\cdot 10<sup>3</sup> samples per estimate (1:1 training split for neural network-based estimators). e) Estimator with highest absolute accuracy in each setting. Ties broken randomly. f) - i) Relative accuracy measured by Kendall \\(\\tau\\) rank correlation of estimates with ground truth. j) Estimator with highest relative accuracy in each setting. Ties broken randomly.", "description": "This figure shows the performance of different mutual information (MI) estimators as the dimensionality of the data increases.  The plots compare the absolute and relative accuracy of four estimators: LMI, KSG, MINE, and InfoNCE.  The results demonstrate that LMI outperforms the other estimators, particularly when the intrinsic dimensionality of the data is low compared to the ambient dimensionality.  The accuracy is measured using mean-squared error (MSE) and Kendall's tau correlation.", "section": "3 Empirical evaluation"}, {"figure_path": "HN05DQxyLl/figures/figures_18_1.jpg", "caption": "Figure 9: Convergence of multiple latent estimation approaches during training. Bold lines indicate averages over 100 trials. Ground truth is 1 bit MI.", "description": "This figure shows the convergence of latent InfoNCE and latent KSG estimators during the training process.  Multiple trials (100) were run, and the average performance is highlighted in bold. The ground truth mutual information (MI) value is 1 bit.  The plot demonstrates how the estimations approach the true value over training epochs, illustrating the convergence behavior of the two methods.", "section": "3.1 Evaluating mutual information estimators on synthetic data"}, {"figure_path": "HN05DQxyLl/figures/figures_18_2.jpg", "caption": "Figure 10: Pixel-wise reconstruction error of cross-decoders in paired binary MNIST dataset where Lx = Ly.", "description": "This figure shows the pixel-wise reconstruction error from the cross-decoders in a paired binary MNIST dataset where Lx=Ly.  It demonstrates that the cross-predictive regularization used helps identify which pixels are most important in determining the mutual information between the variables. Pixels with low reconstruction error are better predicted by the other variable, while those with high error are not. This visualization is useful for interpreting the results of the LMI approximation.", "section": "A.2.3 Interpreting decoders with element-wise reconstruction error"}, {"figure_path": "HN05DQxyLl/figures/figures_19_1.jpg", "caption": "Figure 5: Constructing problems where LMI fails. a) Examples of symmetric distributions where E[X|Y] = E[X]. b) MI estimates from 103 samples of symmetric variables as exclusive (independent normal) dimensions are added. Ideal estimators are invariant. LMI latent space is 1D per variable.", "description": "This figure demonstrates a scenario where the Latent Mutual Information (LMI) approximation method may fail.  Part (a) shows examples of symmetric distributions where the conditional expectation E[X|Y] equals the expectation of X, indicating a lack of information about X given Y. Part (b) shows how MI estimates change as independent dimensions are added to symmetric variables.  In theory, an ideal MI estimator would remain invariant to the addition of these independent dimensions; however, the figure shows that multiple MI estimators, including LMI, exhibit performance degradation. This illustrates a limitation of LMI and highlights the impact of dataset characteristics on estimation accuracy.", "section": "3.3 Constructing and studying problems where LMI fails"}, {"figure_path": "HN05DQxyLl/figures/figures_21_1.jpg", "caption": "Figure 12: Validating assumptions of cluster-based benchmarking, by visualizing separation of clusters.", "description": "This figure shows UMAP visualizations of two datasets used for benchmarking in the paper: (a) a subset of MNIST data containing only images of 0 and 1, and (b) protein embeddings from E. coli and A. thaliana.  UMAP is a dimensionality reduction technique used to visualize high-dimensional data.  The clear separation of the clusters in both (a) and (b) suggests that the samples are well-separated and clustered according to their labels, providing evidence to support the assumption that the discrete labels can be uniquely identified by high-dimensional vectors. This is a key assumption for a benchmarking method used in the paper. ", "section": "3 Empirical evaluation"}, {"figure_path": "HN05DQxyLl/figures/figures_21_2.jpg", "caption": "Figure 12: Validating assumptions of cluster-based benchmarking, by visualizing separation of clusters.", "description": "This figure validates the assumptions made for the cluster-based benchmarking approach used in Section 3.2 of the paper.  Two UMAP plots are shown. The first (a) visualizes the separation of clusters for a binary subset of MNIST (Modified National Institute of Standards and Technology) digits, where 0 and 1 represent images of zeros and ones respectively. The second (b) shows the separation of clusters for protein sequence embeddings from Arabidopsis thaliana and Escherichia coli.  The clear separation in both plots supports the assumption that the labels (digits and species) can be reliably determined from their high-dimensional vector representations, a key assumption for the cluster-based benchmarking method's validity.", "section": "3 Empirical evaluation"}, {"figure_path": "HN05DQxyLl/figures/figures_22_1.jpg", "caption": "Figure 13: Performance (measured as MSE) on subset of multivariate Gaussian benchmark after non-linear transformations (defined in [12]). All estimation problems are with 1000 ambient dimensions.", "description": "This figure shows the mean squared error (MSE) of four different mutual information (MI) estimation methods (MINE, LMI, KSG, InfoNCE) on a subset of the multivariate Gaussian benchmark dataset from [12]. The dataset consists of high-dimensional (1000 dimensions) variables with varying intrinsic dimensionality (1-9 dimensions) and different non-linear transformations (Asinh, Half cube, None, Uniform margins) applied to the data. The heatmaps show the performance of each method as a function of intrinsic dimensionality and transformation type. LMI shows consistently better performance than other methods in most cases.", "section": "3.1 Evaluating mutual information estimators on synthetic data"}, {"figure_path": "HN05DQxyLl/figures/figures_23_1.jpg", "caption": "Figure 2: MI estimator performance scaling with increasing dimensionality. a) - d) Absolute accuracy measured by mean-squared error over 10 estimates per setting, with ground truth MI between 0 and 2 bits, and 2 \\cdot 10^3 samples per estimate (1:1 training split for neural network-based estimators). e) Estimator with highest absolute accuracy in each setting. Ties broken randomly. f) - i) Relative accuracy measured by Kendall \\tau rank correlation of estimates with ground truth. j) Estimator with highest relative accuracy in each setting. Ties broken randomly.", "description": "The figure compares the performance of several mutual information (MI) estimators as the dimensionality of the data increases.  It shows that the proposed Latent MI (LMI) method outperforms existing methods in terms of both absolute and relative accuracy, especially when the data has high ambient dimensionality but low intrinsic dimensionality.  The results are presented using mean squared error (MSE) and Kendall's tau correlation as metrics. ", "section": "3 Empirical evaluation"}, {"figure_path": "HN05DQxyLl/figures/figures_24_1.jpg", "caption": "Figure 15: MNIST benchmarking for neural estimators with critic complexity equivalent to LMI encoders, over 20 datasets with true MI between 0 and 1.", "description": "This figure presents the results of a benchmarking study comparing the performance of different mutual information (MI) estimators on the MNIST dataset.  The key takeaway is that even with critic complexity equivalent to the LMI encoders (meaning neural network components designed to estimate MI), the MINE and InfoNCE estimators show poor performance when compared to the Latent Mutual Information (LMI) approach.  This highlights the relative accuracy and robustness of the LMI method, particularly in scenarios with a limited number of samples and high dimensionality.", "section": "A.4.4 KSG implementation details for pointwise decompositions"}]