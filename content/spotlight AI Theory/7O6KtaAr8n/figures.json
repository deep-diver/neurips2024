[{"figure_path": "7O6KtaAr8n/figures/figures_8_1.jpg", "caption": "Figure 1: Results for cardinal case with number of samples. Different lines show results for different values of added noise v. Solid lines correspond to values for learned parameters, whereas dotted lines correspond to values for real parameters.", "description": "This figure shows the results of experiments conducted for the cardinal case.  The x-axis represents the number of samples used in the learning process. The y-axis of the three subplots shows: (a) Test loss, measuring the difference between the predicted and actual social welfare values; (b) KL divergence, quantifying the difference between the learned weight vector and the true weight vector; and (c) Learned p, displaying the learned value of the power mean parameter.  Different colored lines represent different levels of added noise (v) to the training data, allowing for an analysis of the algorithm's robustness to noise. The solid lines represent the performance with learned parameters while dotted lines show the performance if real parameters were used. Overall, the figure demonstrates how the algorithm's performance improves as the number of samples increases and as the noise level decreases.", "section": "Empirical Results"}, {"figure_path": "7O6KtaAr8n/figures/figures_8_2.jpg", "caption": "Figure 1: Results for cardinal case with number of samples. Different lines show results for different values of added noise v. Solid lines correspond to values for learned parameters, whereas dotted lines correspond to values for real parameters.", "description": "The figure shows the results of the cardinal case experiment with varying sample sizes and different noise levels.  It plots the test loss, the Kullback-Leibler (KL) divergence between the true and learned weight vectors, and the learned power mean parameter (p).  Solid lines represent the learned parameters, while dotted lines indicate the true parameters.  The results demonstrate that, as the number of samples increases and the noise level decreases, the learned parameters converge to the true parameters.", "section": "Empirical Results"}, {"figure_path": "7O6KtaAr8n/figures/figures_29_1.jpg", "caption": "Figure 1: Results for cardinal case with number of samples. Different lines show results for different values of added noise v. Solid lines correspond to values for learned parameters, whereas dotted lines correspond to values for real parameters.", "description": "The figure shows the results of the cardinal case experiment with different noise levels (v) and various numbers of samples. The test loss, KL divergence between the true and learned weights, and the learned p are plotted against the number of samples for each noise level. Solid lines represent values for learned parameters, while dotted lines represent values for real parameters.  It demonstrates how the algorithm's performance improves with more samples and less noise. The convergence of the learned parameters towards the true values with increasing sample size is illustrated.", "section": "Empirical Results"}, {"figure_path": "7O6KtaAr8n/figures/figures_30_1.jpg", "caption": "Figure 4: More results for ordinal case with number of samples. Different lines show results for different values of noise level \u03c4. Solid lines correspond to values for learned parameters, whereas dotted lines correspond to values for real parameters.", "description": "This figure shows the results of experiments conducted for the ordinal case, illustrating how the test loss, KL divergence between true and learned weights, test accuracy on noiseless data, and learned parameter p change with varying numbers of samples and different noise levels (\u03c4).  The solid lines represent the results obtained using the learning algorithm, while the dotted lines indicate the true parameter values. This visualization helps in understanding the algorithm's performance and convergence behavior under different noise conditions and sample sizes.", "section": "Semi-synthetic Experiments: Further Information"}, {"figure_path": "7O6KtaAr8n/figures/figures_31_1.jpg", "caption": "Figure 5: More results for ordinal case with p = 1.62. Different lines show results for different values of \u03c4. Solid lines correspond to values for learnt parameters, whereas dotted lines correspond to values for real parameters.", "description": "The figure shows the results of the ordinal case for p=1.62, showing how the test loss, KL divergence, test accuracy, and learned parameter p change with an increasing number of samples and different values of noise level \u03c4. Solid lines represent the values for the learned parameters, while the dotted lines show the values for the real parameters. The results demonstrate that the test loss decreases, KL divergence decreases, and the accuracy increases with an increasing number of samples and decreasing noise. The learned parameter p also becomes closer to the real parameter with a decreasing noise level and an increasing number of samples.", "section": "E Simulations"}, {"figure_path": "7O6KtaAr8n/figures/figures_32_1.jpg", "caption": "Figure 6: An example showing the non-convexity of log M(u, w, p) \u2013 log M(v, w, p). We see that the function has five roots for (u, v), but is translated downwards for (u, v') and has only three roots in this case. If the correct label is 1 for both pairs, then p should be greater than 6; however, gradient-based optimization can stop between 3 and 4, which is a local optimum and does not give correct labels to both points.", "description": "The figure shows that the difference between the log power means of two pairs of utility vectors, (u, v) and (u, v\u2019), is not convex with respect to the power parameter p.  This non-convexity can cause gradient-based optimization algorithms to get stuck in local optima, potentially leading to incorrect predictions.", "section": "Pairwise Preference Between Actions"}, {"figure_path": "7O6KtaAr8n/figures/figures_32_2.jpg", "caption": "Figure 7: Results for synthetic data on cardinal and ordinal logistic tasks", "description": "This figure presents the results of experiments conducted on synthetic data for both cardinal and ordinal logistic tasks.  It displays the estimated (noiseless) test loss for various settings, illustrating the performance of the proposed algorithm under different conditions.  The results show the impact of known versus unknown weights, the number of samples (n), and the dimensionality (d) of the data on the algorithm's accuracy.  The figure allows for a visual comparison of the algorithm's performance across different experimental setups.", "section": "Simulations"}, {"figure_path": "7O6KtaAr8n/figures/figures_32_3.jpg", "caption": "Figure 7: Results for synthetic data on cardinal and ordinal logistic tasks", "description": "This figure shows the results of experiments conducted on synthetic data for both cardinal and ordinal logistic tasks.  It displays the estimated (noiseless) test loss for various settings, including known and unknown weights. The plots show the test loss decreasing with increasing sample size (n), demonstrating the effectiveness of the algorithms in learning the underlying social welfare functions. The impact of dimensionality (d) is also visible, showing higher test loss for the unknown weights case due to the increased complexity of learning more weights. ", "section": "Simulations"}, {"figure_path": "7O6KtaAr8n/figures/figures_32_4.jpg", "caption": "Figure 8: Verification of O(dlog d) risk bound for ordinal case with logistic noise, unknown weights", "description": "This figure replots the test accuracy against a rescaled version of the sample size to verify the theoretical sample complexity bound. The alignment of curves suggests that the risk and sample complexity bounds scale as d log d for the ordinal case with logistic noise and unknown weights.", "section": "E Simulations"}, {"figure_path": "7O6KtaAr8n/figures/figures_32_5.jpg", "caption": "Figure 8: Verification of O(dlog d) risk bound for ordinal case with logistic noise, unknown weights", "description": "This figure shows the test accuracy on noiseless data against the re-scaled sample size \u03b7 = \u221an/(dlog n log d) for different values of d. The alignment of all curves provides evidence that the risk and sample complexity bounds indeed scale as d log n log d for the ordinal case with logistic noise and unknown weights.", "section": "E Simulations"}, {"figure_path": "7O6KtaAr8n/figures/figures_33_1.jpg", "caption": "Figure 8: Verification of O(dlog d) risk bound for ordinal case with logistic noise, unknown weights", "description": "This figure shows the test accuracy on noiseless data against the re-scaled sample size, \u03b7 = \u221an/(dlog n log d).  The alignment of all curves suggests that the risk and sample complexity bounds scale as d log n log d for the ordinal case with logistic noise and unknown weights.  The plot verifies the theoretical results by showing how test accuracy increases with sample size for different values of d (number of individuals).", "section": "E Simulations"}]