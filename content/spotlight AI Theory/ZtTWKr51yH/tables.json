[{"figure_path": "ZtTWKr51yH/tables/tables_2_1.jpg", "caption": "Table 1: Evasion attacks for tabular machine learning. Attacks with a public implementation in bold.", "description": "This table compares existing evasion attacks for tabular data.  For each attack, it shows the types of features supported (continuous, discrete, categorical), and whether it supports various types of constraints (categorical, discrete, relational).  Attacks with publicly available implementations are highlighted in bold. This table helps contextualize the authors' work by showing how their proposed attacks, CAPGD and CAA, compare to existing methods in terms of feature and constraint support.", "section": "Related work"}, {"figure_path": "ZtTWKr51yH/tables/tables_6_1.jpg", "caption": "Table 2: Robust accuracy against CAPGD and SOTA gradient attacks. A lower robust accuracy means a more effective attack (lowest in bold).", "description": "This table presents the robust accuracy achieved by various gradient-based attacks, including CAPGD, LowProFool, and CPGD, across different datasets (URL, LCLD, CTU, WIDS) and five different model architectures.  A lower robust accuracy indicates that the corresponding attack is more effective at generating adversarial examples that fool the model. The table highlights that CAPGD outperforms the other gradient attacks in most cases.", "section": "4.2 Comparison of CAPGD to gradient-based attacks"}, {"figure_path": "ZtTWKr51yH/tables/tables_7_1.jpg", "caption": "Table 3: Robust accuracy and attack duration for CAPGD, MOEVA, and CAA. The Clean column corresponds to the accuracy of the model on the subset of clean samples that we attack. A lower robust accuracy means a more effective attack. The lowest robust accuracy is in bold. A lower duration is better. The lowest time between MOEVA and CAA is in bold.", "description": "This table presents a comparison of the robustness and efficiency of three different adversarial attacks: CAPGD, MOEVA, and CAA.  The \"Clean\" column shows the model's accuracy on unattacked samples.  Lower robust accuracy values indicate a more effective attack.  The table shows that CAA generally achieves the lowest robust accuracy (most effective attack), and in many cases, does so more efficiently (shorter duration) than MOEVA.", "section": "5.2 Effectiveness and efficiency of CAA"}, {"figure_path": "ZtTWKr51yH/tables/tables_8_1.jpg", "caption": "Table 4: CAA performances (XX+/-YY) against Madry adversarially trained model. XX refers to accuracy. YY is the difference between the accuracy of the adversarially trained model and standard training (cf. Table 3), such that '+' means a higher accuracy for the adversarially trained model.", "description": "This table presents the performance of the proposed Constrained Adaptive Attack (CAA) against models trained using Madry's adversarial training.  It compares the clean accuracy and the robust accuracy (against CAA) of the adversarially trained models with those trained using standard training methods, highlighting the improvement or degradation in model performance resulting from the adversarial training.", "section": "5.4 Impact of Adversarial training"}, {"figure_path": "ZtTWKr51yH/tables/tables_13_1.jpg", "caption": "Table 1: Evasion attacks for tabular machine learning. Attacks with a public implementation in bold.", "description": "This table compares different evasion attacks designed for tabular data.  It details which types of features (continuous, discrete, categorical) each attack supports and whether it accounts for categorical features, discrete features, or the relationships between features.  Attacks with publicly available implementations are highlighted in bold.  The table helps to illustrate the novelty of the proposed CAPGD and CAA attacks, which are the only ones that support all three feature types and feature relationships.", "section": "Related work"}, {"figure_path": "ZtTWKr51yH/tables/tables_15_1.jpg", "caption": "Table 6: The datasets evaluated in the empirical study, with the class imbalance of each dataset.", "description": "This table presents the characteristics of the four datasets used in the paper's empirical study. For each dataset, it lists the task it is designed for, the number of data instances (size), the number of features, and the percentage of instances belonging to each class (class imbalance).  The datasets represent diverse domains including credit scoring, botnet detection, phishing URL detection, and ICU patient survival.", "section": "3.2 Experimental settings"}, {"figure_path": "ZtTWKr51yH/tables/tables_16_1.jpg", "caption": "Table 7: The three model architectures of our study.", "description": "This table presents the three model architectures used in the study: TabTransformer and TabNet (both transformer-based models), RLN (Regularization Learning Networks), STG (Stochastic Gates), and VIME (Value Imputation for Mask Estimation).  It lists the family to which each model belongs and the hyperparameters tuned for each model during training.", "section": "2 Related work"}, {"figure_path": "ZtTWKr51yH/tables/tables_19_1.jpg", "caption": "Table 8: Ablation study: Robust accuracy for CAPGD and its variant without key components. The Clean column corresponds to the accuracy of the model on the subset of clean samples that we attack. A lower robust accuracy means a more effective attack. The lowest robust accuracy is in bold.", "description": "This table presents the results of an ablation study conducted to evaluate the impact of different components of the CAPGD attack on its effectiveness.  The study removes one component at a time (repair operator, initialization with clean example, random initialization, and adaptive step size). The table shows the robust accuracy (i.e., accuracy against adversarial examples) for each variant of the attack, as well as the original CAPGD attack, across different datasets and models.  A lower robust accuracy indicates a more effective attack. ", "section": "B.1 Components of CAPGD"}, {"figure_path": "ZtTWKr51yH/tables/tables_20_1.jpg", "caption": "Table 9: Robust accuracy with CAA with varying maximum perturbation e budget. The lowest robust accuracy is in bold.", "description": "This table presents the robust accuracy results of the Constrained Adaptive Attack (CAA) against various models (TabTr, RLN, VIME, STG, TabNet) across four datasets (URL, LCLD, CTU, WIDS) with varying maximum perturbation thresholds (e).  Lower robust accuracy values indicate a more effective attack.  The results show how the attack's success changes depending on the model and dataset used.", "section": "5.2 Effectiveness and efficiency of CAA"}, {"figure_path": "ZtTWKr51yH/tables/tables_21_1.jpg", "caption": "Table 10: Robust accuracy with CAA with varying gradient attack iterations in CAPGD. The lowest robust accuracy is in bold.", "description": "This table shows the results of an ablation study on the number of iterations used in the CAPGD attack component of the CAA framework.  It presents the robust accuracy achieved against each model (TabTr., RLN, VIME, STG, TabNet) for four different datasets (URL, LCLD, CTU, WIDS) at various numbers of CAPGD iterations (5, 10, 20, 100). The lowest robust accuracy for each configuration is highlighted in bold, indicating the most effective attack.", "section": "B.2 Budget of attacker"}, {"figure_path": "ZtTWKr51yH/tables/tables_23_1.jpg", "caption": "Table 3: Robust accuracy and attack duration for CAPGD, MOEVA, and CAA. The Clean column corresponds to the accuracy of the model on the subset of clean samples that we attack. A lower robust accuracy means a more effective attack. The lowest robust accuracy is in bold. A lower duration is better. The lowest time between MOEVA and CAA is in bold.", "description": "This table presents a comparison of the effectiveness and efficiency of three different evasion attacks (CAPGD, MOEVA, and CAA) against five different architectures on four different datasets.  Effectiveness is measured by robust accuracy (lower is better), indicating the ability of each attack to cause misclassification.  Efficiency is measured by the time taken for the attack to complete (lower is better). The \"Clean\" column shows the baseline accuracy of the model on the unperturbed data.  The table highlights that CAA generally achieves the highest effectiveness and a better balance of effectiveness and efficiency compared to the other methods.", "section": "5.2 Effectiveness and efficiency of CAA"}, {"figure_path": "ZtTWKr51yH/tables/tables_24_1.jpg", "caption": "Table 12: Robust accuracy with subset of constraints. \u03a9 is the complete set of constraints. CGX denotes the constraint group X. For CG2 and CG3, we evaluate with the entire group and on 10%, 25%, 50% selected randomly and averaged on 5 seeds.", "description": "This table presents the robust accuracy results obtained using CAPGD and CAA with different subsets of constraints from the CTU dataset.  The \"\u03a9\" row represents the full set of constraints, while the subsequent rows show results after removing or only using specific groups (CG0, CG1, CG2, CG3) of constraints. For CG2 and CG3, various percentages (10%, 25%, 50%) of randomly selected constraints were also evaluated. The goal is to study how the number and complexity of constraints influence the effectiveness of the attacks.", "section": "B.3 Constraints complexity"}, {"figure_path": "ZtTWKr51yH/tables/tables_24_2.jpg", "caption": "Table 3: Robust accuracy and attack duration for CAPGD, MOEVA, and CAA. The Clean column corresponds to the accuracy of the model on the subset of clean samples that we attack. A lower robust accuracy means a more effective attack. The lowest robust accuracy is in bold. A lower duration is better. The lowest time between MOEVA and CAA is in bold.", "description": "This table presents the performance of three different adversarial attacks (CAPGD, MOEVA, and CAA) on various datasets and machine learning models. The \"Clean\" column shows the accuracy of the model on the original, unperturbed data. The lower the robust accuracy, the more effective the attack is. The duration shows the time it takes for each attack to generate adversarial examples.  The table highlights that CAA generally achieves the lowest robust accuracy (highest effectiveness) and is often significantly faster than MOEVA, especially when combining gradient-based attacks with search-based attacks.", "section": "5.2 Effectiveness and efficiency of CAA"}, {"figure_path": "ZtTWKr51yH/tables/tables_25_1.jpg", "caption": "Table 14: Tree-based model robust accuracy in direct and transferability scenario (minimum robust accuracy over 5 neural networks).", "description": "This table presents the clean accuracy, direct attack robust accuracy, and transferability robust accuracy for Random Forest and XGBoost models across four datasets (URL, LCLD, CTU, and WIDS).  The transferability results show the robustness when adversarial examples generated against deep learning models are tested against the tree-based models. Lower accuracy indicates a more effective attack.", "section": "B.5 Generalization to shallow models"}]