[{"figure_path": "7Swrtm9Qsp/figures/figures_0_1.jpg", "caption": "Figure 1: We show that \"Large step size selects simple functions that generalize.\"", "description": "This figure illustrates the different types of solutions that can be found by gradient descent training of two-layer ReLU neural networks, depending on the step size (learning rate).  When the learning rate is small, gradient descent can find interpolating solutions (i.e., solutions that perfectly fit the noisy training data). In the presence of noise, such solutions tend to overfit. However, when the learning rate is large, gradient descent is more likely to find stable minima that represent simpler functions.  These stable minima exhibit good generalization, even in the presence of noisy labels.  The figure shows the relationship between the step size and the type of solution found, highlighting the \"Edge of Stability\" regime where the stability of the minima is related to the step size.", "section": "1 Introduction"}, {"figure_path": "7Swrtm9Qsp/figures/figures_1_1.jpg", "caption": "Figure 2: Empirical evidence of our claim. Constant step size gradient descent-trained two-layer ReLU neural networks generalize because of minima stability. The left panel shows that with increasing step size, gradient descent finds smoother solutions (linear splines) with a smaller number of knots. The middle panel illustrates our theoretical result with a numerically accurate upper bound using 1/\u03b7 + O(1) of the curvature and TV1-complexity of the smooth solution. The right panel shows that tuning \u03b7 gives the classical U-shape bias-variance tradeoff for overparameterized NN.", "description": "This figure demonstrates the empirical findings supporting the claim that stable minima in gradient descent training of two-layer ReLU networks generalize well.  The left panel illustrates the relationship between step size (\u03b7) and solution smoothness, showing how larger step sizes lead to sparser, smoother solutions (represented as linear splines with fewer knots). The central panel validates the theoretical upper bound on the maximum eigenvalue of the Hessian, relating it to the step size and solution complexity (TV1). The right panel showcases the typical bias-variance tradeoff curve, demonstrating near-optimal performance through tuning the learning rate.", "section": "1 Introduction"}, {"figure_path": "7Swrtm9Qsp/figures/figures_9_1.jpg", "caption": "Figure 3: Highlights of our numerical simulation for large step size (\u03b7 = 0.4, first row) and small step size (\u03b7 = 0.01, second row) gradient descent training of a univariate ReLU NN with n = 30 noisy observations and k = 100 hidden neurons. From left to right, the three columns illustrate (a) Trained NN function (b) Learning curves (c) Learned basis functions (each of the 100 neurons).", "description": "This figure shows the results of training a univariate ReLU neural network with different learning rates (\u03b7 = 0.4 and \u03b7 = 0.01).  The top row illustrates the results for a large learning rate (\u03b7 = 0.4), while the bottom row shows the results for a small learning rate (\u03b7 = 0.01). Each row contains three subplots. The left subplot displays the trained neural network function along with the true function, noisy labels, and fitted labels. The middle subplot shows the learning curves, plotting the training loss and mean squared error (MSE) against the true function over iterations. The right subplot visualizes the learned basis functions of the neural network, offering insights into how the representation is learned at different learning rates.  The visualization demonstrates how the choice of learning rate affects the learned model's function, loss, and feature representations.", "section": "5 Experiments"}, {"figure_path": "7Swrtm9Qsp/figures/figures_14_1.jpg", "caption": "Figure 3: Highlights of our numerical simulation for large step size (\u03b7 = 0.4, first row) and small step size (\u03b7 = 0.01, second row) gradient descent training of a univariate ReLU NN with n = 30 noisy observations and k = 100 hidden neurons. From left to right, the three columns illustrate (a) Trained NN function (b) Learning curves (c) Learned basis functions (each of the 100 neurons).", "description": "This figure shows the results of a numerical simulation using a two-layer ReLU neural network trained with gradient descent.  The top row shows results for a large learning rate (\u03b7 = 0.4), while the bottom row shows results for a small learning rate (\u03b7 = 0.01). Each row consists of three subplots: (a) shows the learned neural network function compared to the true underlying function and noisy labels; (b) shows learning curves for training loss and mean squared error (MSE); (c) visualizes the learned basis functions of the neural network.  The figure demonstrates that large learning rates lead to simpler functions, while small learning rates lead to overfitting.", "section": "5 Experiments"}, {"figure_path": "7Swrtm9Qsp/figures/figures_15_1.jpg", "caption": "Figure 3: Highlights of our numerical simulation for large step size (\u03b7 = 0.4, first row) and small step size (\u03b7 = 0.01, second row) gradient descent training of a univariate ReLU NN with n = 30 noisy observations and k = 100 hidden neurons. From left to right, the three columns illustrate (a) Trained NN function (b) Learning curves (c) Learned basis functions (each of the 100 neurons).", "description": "This figure shows the results of training a univariate ReLU neural network with different learning rates. The top row shows the results for a large learning rate (\u03b7 = 0.4), while the bottom row shows the results for a small learning rate (\u03b7 = 0.01). For each learning rate, the figure shows the trained neural network function, the learning curves (training loss and MSE vs. truth), and the learned basis functions. The results demonstrate that large learning rates lead to simpler, smoother solutions, while smaller learning rates lead to more complex, less smooth solutions that overfit to the noisy data.", "section": "5 Experiments"}, {"figure_path": "7Swrtm9Qsp/figures/figures_16_1.jpg", "caption": "Figure 6: Examples of global optimal (interpolating) solutions (fitting only second layer weights). Notice that the number of data points n = 30. When the model is barely able to interpolate (k = 30), the fitted function experiences the catastrophic overfitting. When the number of neurons k increases the interpolation solution becomes smoother and enters the tempered overfitting regime.", "description": "This figure empirically demonstrates the effect of model capacity on the ability of an interpolating solution to generalize.  By keeping the number of data points fixed at 30 but increasing the number of neurons in the hidden layer (k), the plots show how the interpolating solutions transition from catastrophic overfitting (high variance, poor generalization) for small k to tempered overfitting (less variance, improved generalization) for larger k. The smooth blue line represents the true function, the green line is the learned model's output, and the dots show the noisy training labels. ", "section": "A.2 Interpolating Solutions as the Number of Hidden Neurons Increases"}, {"figure_path": "7Swrtm9Qsp/figures/figures_17_1.jpg", "caption": "Figure 3: Highlights of our numerical simulation for large step size (\u03b7 = 0.4, first row) and small step size (\u03b7 = 0.01, second row) gradient descent training of a univariate ReLU NN with n = 30 noisy observations and k = 100 hidden neurons. From left to right, the three columns illustrate (a) Trained NN function (b) Learning curves (c) Learned basis functions (each of the 100 neurons).", "description": "This figure shows the results of a numerical simulation of training a univariate ReLU neural network with gradient descent using two different learning rates (\u03b7 = 0.4 and \u03b7 = 0.01).  The top row shows the results for the large learning rate (\u03b7 = 0.4), and the bottom row shows the results for the small learning rate (\u03b7 = 0.01). Each row shows three plots: (a) trained NN function, (b) learning curves, and (c) learned basis functions. The plots demonstrate the effects of learning rate on the trained function's smoothness, convergence speed, and the nature of the learned basis functions.", "section": "5 Experiments"}, {"figure_path": "7Swrtm9Qsp/figures/figures_18_1.jpg", "caption": "Figure 3: Highlights of our numerical simulation for large step size (\u03b7 = 0.4, first row) and small step size (\u03b7 = 0.01, second row) gradient descent training of a univariate ReLU NN with n = 30 noisy observations and k = 100 hidden neurons. From left to right, the three columns illustrate (a) Trained NN function (b) Learning curves (c) Learned basis functions (each of the 100 neurons).", "description": "This figure shows the results of a numerical simulation with different learning rates (\u03b7 = 0.4 and \u03b7 = 0.01) for training a univariate ReLU neural network (NN) with noisy labels.  The left column displays the trained NN function, showing how it fits to the noisy data. The middle column presents the learning curves of the training loss and mean squared error (MSE), which illustrate the model's training process. The right column visualizes the learned basis functions of the NN, providing insight into how the network represents learned features. The comparison of results with different learning rates indicates the impact of learning rate on model generalization and implicit sparsity.", "section": "5 Experiments"}, {"figure_path": "7Swrtm9Qsp/figures/figures_18_2.jpg", "caption": "Figure 8: Illustration of the learned (steady-state) ReLU NN with different learning rate. Recall that all ReLU NNs are linear splines, therefore the location of the knots (i.e., the change points of the linear pieces) describes the representation learning that happens. Each basis function is a ReLU function at the knot. The final ReLU NN is a linear combination of these learned basis functions. In the first panel, we plot the quantiles of the locations of the learned knots as the function of 1/\u03b7. In the second panel, we plot the sparsity of the learned coefficients in sparse L\u2081 and Lp norm as the function of 1/\u03b7. The third panel plots the distance of the learned knots from the closest input data points. This empirically verifies that the solution that gradient descent finds at the end is a twice-differentiable function w.r.t. the parameters in the sense that not a single learned knot is exactly at the input data point, thus ensuring the applicability of our theorems.", "description": "This figure empirically validates the theoretical claims of the paper by visualizing the learned basis functions of ReLU neural networks trained with gradient descent using different learning rates.  The three subplots show the quantiles of learned knot locations, the sparsity of learned coefficients, and the distance of learned knots to the closest input knot, all as functions of the inverse learning rate (1/\u03b7).  The results demonstrate the implicit regularization effect of large learning rates, showing that they lead to sparser solutions and avoid overfitting by preventing knots from being placed exactly on the data points.", "section": "A.3 Representation Learning of Large Learning Rate: Visualizing Learned Basis Functions"}, {"figure_path": "7Swrtm9Qsp/figures/figures_35_1.jpg", "caption": "Figure 3: Highlights of our numerical simulation for large step size (\u03b7 = 0.4, first row) and small step size (\u03b7 = 0.01, second row) gradient descent training of a univariate ReLU NN with n = 30 noisy observations and k = 100 hidden neurons. From left to right, the three columns illustrate (a) Trained NN function (b) Learning curves (c) Learned basis functions (each of the 100 neurons).", "description": "This figure displays the results of a numerical simulation comparing the effects of large versus small learning rates (\u03b7) on the training of a univariate ReLU neural network.  The top row shows results for \u03b7 = 0.4, while the bottom row shows results for \u03b7 = 0.01.  Three columns show (a) the trained neural network function, (b) the learning curves (training loss and MSE against the ground truth), and (c) the learned basis functions for each of the 100 neurons. The experiment highlights how different learning rates impact the smoothness of the solution found, showing that a large learning rate produces smoother solutions while a small learning rate leads to less smooth, potentially overfit solutions.", "section": "5 Experiments"}]