[{"type": "text", "text": "Symmetries in Overparametrized Neural Networks: A Mean-Field View ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Javier Maass Mart\u00ednez ", "page_idx": 0}, {"type": "text", "text": "Joaqu\u00edn Fontbona ", "page_idx": 0}, {"type": "text", "text": "Center for Mathematical Modeling University of Chile javier.maass@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Center for Mathematical Modeling University of Chile fontbona@dim.uchile.cl ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We develop a Mean-Field (MF) view of the learning dynamics of overparametrized Artificial Neural Networks (NN) under distributional symmetries of the data w.r.t. the action of a general compact group $G$ . We consider for this a class of generalized shallow NNs given by an ensemble of $N$ multi-layer units, jointly trained using stochastic gradient descent (SGD) and possibly symmetry-leveraging (SL) techniques, such as Data Augmentation (DA), Feature Averaging (FA) or Equivariant Architectures (EA). We introduce the notions of weakly and strongly invariant laws (WI and SI) on the parameter space of each single unit, corresponding, respectively, to $G$ -invariant distributions, and to distributions supported on parameters fixed by the group action (which encode EA). This allows us to define symmetric models compatible with taking $N\\rightarrow\\infty$ and give an interpretation of the asymptotic dynamics of DA, FA and EA in terms of Wasserstein Gradient Flows describing their MF limits. When activations respect the group action, we show that, for symmetric data, DA, FA and freely-trained models obey the exact same MF dynamic, which stays in the space of WI parameter laws and attains therein the population risk\u2019s minimizer. We also provide a counterexample to the general attainability of such an optimum over SI laws. Despite this, and quite remarkably, we show that the space of SI laws is also preserved by these MF distributional dynamics even when freely trained. This sharply contrasts the finite- $N$ setting, in which EAs are generally not preserved by unconstrained SGD. We illustrate the validity of our findings as $N$ gets larger, in a teacher-student experimental setting, training a student NN to learn from a WI, SI or arbitrary teacher model through various SL schemes. We lastly deduce a data-driven heuristic to discover the largest subspace of parameters supporting SI distributions for a problem, that could be used for designing EA with minimal generalization error. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Learning in complex tasks, employing ever larger datasets, has strongly beneftied from the implementation and training of Artificial Neural Networks (NN) with a huge number of parameters; as well as from training schemes or architectures that can leverage underlying symmetries of the data in order to reduce the problem\u2019s complexity (see [33, 34] for general reference). This raises questions, on one hand, of understanding the puzzling generalizability in overparametrized NN; and on the other, of when and how symmetry-leveraging (SL) techniques (such as Data Augmentation, Feature Averaging or Equivariant Architectures), can induce useful biases towards learning with symmetries, without hindering approximation and generalization properties. The recent Mean-Field (MF) theory of NN (see [16] and further references below) provides a partial, yet promissory, viewpoint to address the first question for shallow NN: in the Mean-Field Limit (MFL) of an infinitely wide hidden layer, stochastic gradient descent (SGD) training dynamics approximates the Wasserstein Gradient Flow (WGF) of certain convex population risk on the space of distributions on parameters. Confluently, the incorporation of combined algebraic and probabilistic viewpoints have yielded a more complete view of the beneftis of SL techniques under symmetry (see e.g. [13, 27, 59] and further references below); however, it is not clear if and how those findings can scale to overparametrized NN and their MFL. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this work we develop a systematic MF analysis of the limiting learning dynamics of a class of generalized shallow NNs, under distributional symmetries of the data w.r.t. the action of a compact group, and including the possible effects of employing some of the most popular SL techniques. The effect of symmetries on the WGF dynamics was already studied in [35], in the particular case of two-layer ReLU networks, under data generated by a function symmetric w.r.t. a single orthogonal transformation. We consider our (independent 1) work to largely broaden the scope and applicability of such initial contributions, as it provides a unified MF interpretation for both the use of SL techniques under general distributional invariances, and the interplay of such symmetries at the levels of data, architectures and training dynamics. The paper unfolds as follows: ", "page_idx": 1}, {"type": "text", "text": "In Section 2 we introduce a class of generalized shallow models with multi-layer units on which we will focus, we recall WGFs and their role in the MFL of NN training dynamics, and review the SL techniques to be studied. Section 3 contains the bulk of our contributions, as we study how SL techniques applied on these models can be interpreted in terms of their limiting WGFs, how they relate to each other in terms of the optima of their corresponding population risks, and how their limiting MF training dynamics behave with or without symmetric data. Finally, Section 4 presents the empirical validation of our main theoretical results through some numerical simulations; it also suggests a potential heuristic for discovering data-driven parameter-sharing schemes that lead to optimal equivariant architectures in ML problems. Proofs and complements to our results can be found in the Supplementary Material (henceforth SuppMat for short), together with a discussion of the scope and limitations of our results, as well as a summary of the notation and abbreviations used. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Supervised learning with generalized shallow neural networks ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Let $\\mathcal{X},\\mathcal{V}$ and $\\mathcal{Z}$ be separable Hilbert Spaces, termed as the feature, label and parameter spaces respectively. Typically, these are finite-dimensional, e.g. $\\mathcal{X}=\\mathbb{R}^{d}$ and $\\mathcal{V}=\\mathbb{R}^{c}$ (for $c,d\\in\\mathbb{N}^{*})$ with $\\mathcal{Z}$ the space of affine transformations between hidden layers. We write $\\mathcal P(\\cdot)$ for the space of Borel probability measure on a metric space $(\\cdot)$ . Let $\\pi\\in{\\mathcal{P}}({\\dot{\\boldsymbol{x}}}\\times{\\boldsymbol{y}})$ denote the data distribution from which i.i.d. samples $(X,Y)\\in\\mathcal{X}\\times\\mathcal{Y}$ will be drawn, and $\\ell:\\mathcal{V}\\times\\mathcal{V}\\rightarrow\\mathbb{R}$ be a convex loss function. Consider also an activation function $\\sigma_{*}:\\mathcal{X}\\times\\mathcal{Z}\\rightarrow\\mathcal{Y}$ . We introduce a general class of shallow NN: ", "page_idx": 1}, {"type": "text", "text": "Definition 1. A shallow neural network model of parameter $\\theta:=(\\theta_{i})_{i=1}^{N}\\,\\in\\,\\mathcal{Z}^{N}$ is the function $\\Phi_{\\theta}^{N}:\\mathcal{X}\\to\\mathcal{Y}$ given by $\\begin{array}{r}{\\Phi_{\\theta}^{N}(x):=\\frac{1}{N}\\sum_{i=1}^{N}\\sigma_{*}(x;\\theta_{i})}\\end{array}$ , $\\forall x\\in\\mathcal{X}$ . Equivalently, if $\\begin{array}{r}{\\nu_{\\theta}^{N}:=\\frac{1}{N}\\sum_{i=1}^{N}\\delta_{\\theta_{i}}}\\end{array}$ is the empirical measure associated with $\\theta\\in\\mathcal{Z}^{N}$ , we can write $\\forall x\\in\\mathcal{X}$ , $\\Phi_{\\theta}^{N}(x)=\\langle\\sigma_{*}(x;\\cdot),\\nu_{\\theta}^{N}\\rangle$ or, abusing notation, simply $\\Phi_{\\theta}^{N}=\\langle\\sigma_{*},\\nu_{\\theta}^{N}\\rangle$ . ", "page_idx": 1}, {"type": "text", "text": "In the setting where $\\mathcal{X}=\\mathbb{R}^{d}$ , $\\mathcal{V}=\\mathbb{R}^{c}$ and $\\mathcal{Z}=\\mathbb{R}^{c\\times b}\\times\\mathbb{R}^{d\\times b}\\times\\mathbb{R}^{b}$ (for $b\\in\\mathbb{N}^{*}$ ), if we consider, for $z=(W,A,B)\\in\\mathcal{Z}$ and $\\sigma:\\mathbb{R}^{b}\\rightarrow\\mathbb{R}^{b}$ , $\\sigma_{*}(x,z):=W\\sigma(A^{T}x\\!+\\!B)$ ; then $\\Phi_{\\boldsymbol{\\theta}}^{N}$ (with $N\\in\\mathbb{N},\\theta\\in\\mathcal{Z}^{N})$ corresponds exactly to a single-hidden-layer neural network with $N$ hidden units. Depending on $\\sigma_{*}$ , however, these shallow NN models can represent settings that go far beyond this first example. In fact, $\\sigma_{*}$ can be taken to be an entire Multi-Layer NN model, in which case $\\Phi_{\\theta}^{N}$ will represent an ensemble of $N$ such units trained simultaneously (see SuppMat-C.1). As we will also shortly see, for suitable subspaces of $\\mathcal{Z}$ , this modelling extends to renowned equivariant architectures such as CNNs, DeepSets and GNNs. Beyond NNs, this setting can also model the deconvolution of sparse spikes, RBF networks, density estimation via MMD minimization, among many others (see [16, 62, 69]). ", "page_idx": 1}, {"type": "text", "text": "This class thus allows for non-trivial internal units, while enabling the width $N\\rightarrow\\infty$ consistently, and regardless of the possible underlying structure of the (fixed size) units represented by $\\sigma_{*}$ . Inspired by this possibility, and by our writing of shallow NN models, we define a more general notion: ", "page_idx": 1}, {"type": "text", "text": "Definition 2 (Shallow Model). A shallow model is any function of the form $\\Phi_{\\mu}(x):=\\langle\\sigma_{*}(x;\\cdot),\\mu\\rangle$ for some $\\mu\\in{\\mathcal{P}}({\\mathcal{Z}})$ (whenever the integral makes sense for all $x\\in\\mathscr{X}$ ). We write $\\Phi_{\\mu}:=\\langle\\sigma_{\\ast},\\mu\\rangle$ and denote the space of such models as $\\mathcal{F}_{\\sigma_{*}}(\\mathcal{P}(\\mathcal{Z}))$ . ", "page_idx": 1}, {"type": "text", "text": "Classically, we want to find a NN model that performs well with respect to $\\pi$ and $\\ell$ . More precisely, having fixed an architecture (given here by $N$ and $\\sigma_{*}$ ), we consider the generalization error or population risk given by $R(\\theta)\\stackrel{\\mathtt{<}}{=}\\mathbb{E}_{\\pi}\\left[\\ell(\\Phi_{\\theta}^{N}(X),Y)\\right]$ , and look for a vector of parameters $\\theta\\in\\mathcal{Z}^{N}$ attaining $\\operatorname*{min}_{\\theta\\in{\\mathcal{Z}}^{N}}R(\\theta)$ . However, not only is this function highly non-convex and hard to optimize; but in practice we generally don\u2019t have access to $\\pi$ (and thus $R$ ) and we have to solve this problem only with a set of i.i.d. data samples $\\{(X_{k},Y_{k})\\}_{k\\in\\mathbb{N}}$ drawn from $\\pi$ . Thus, the usual approach to minimizing this population risk is to train a NN model $\\Phi_{\\boldsymbol{\\theta}}^{N}$ , through an SGD scheme (see e.g. [7]): ", "page_idx": 2}, {"type": "text", "text": "\u2022 First, initialize $\\theta_{i}^{0}$ , $\\forall i\\in\\{1,\\ldots,N\\}$ , i.i.d. from a fixed distribution $\\mu_{0}\\in\\mathcal{P}(\\mathcal{Z})$ . \u2022 Iterate, for $k\\in\\mathbb{N}$ , defining $\\forall i\\in\\{1,\\ldots,N\\}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\theta_{i}^{k+1}=\\theta_{i}^{k}-s_{k}^{N}\\,\\left(\\nabla_{z}\\sigma_{*}(X_{k},\\theta_{i}^{k})\\cdot\\nabla_{1}\\ell(\\Phi_{\\theta^{k}}^{N}(X_{k}),Y_{k})+\\tau\\nabla r(\\theta_{i}^{k})\\right)+\\sqrt{2\\beta s_{k}^{N}}\\xi_{i}^{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Here, $s_{k}^{N}=\\varepsilon_{N}\\varsigma(k\\varepsilon_{N})$ is the step-size (or learning rate), parametrized in terms of $\\varsigma:\\mathbb{R}_{+}\\rightarrow\\mathbb{R}_{+}$ a regular function and $\\varepsilon_{N}\\,>\\,0$ . Also, we have a penalization function $r:\\mathcal{Z}\\to\\mathbb{R}$ , regularizing Gaussian noise \u03beiki.i\u223c.d. $\\xi_{i}^{k}\\overset{i.i.d.}{\\sim}\\mathcal{N}(0,\\mathrm{Id}_{\\mathcal{Z}})$ independent from the initialization and data, and $\\tau,\\beta\\geq0$ . When $\\tau,\\beta>0$ , the method is called stochastic gradient Langevin dynamics, noted SGLD ([74]), or simply noisy SGD. An infinite i.i.d. sample from $\\pi$ will be needed when letting later $N\\rightarrow\\infty$ . When $\\pi$ is the empirical measure of a finite dataset, we are performing empirical-risk minimization (which of course is not the same as minimizing generalization error, but follows the same mathematical formulation). ", "page_idx": 2}, {"type": "text", "text": "In principle, there are no guarantees that this training procedure will be truly optimizing $R(\\theta)$ let alone approaching its minimum. However, by extending the definition of the generalization error to models in $\\bar{\\mathcal{F}}_{\\sigma_{*}}(\\mathcal{P}(\\bar{\\mathcal{Z}}))$ , one gets the convex functional $\\bar{R_{\\mathrm{:}}}\\,\\mathcal{P}(\\mathcal{Z})\\rightarrow\\mathbb{R}$ given by $R(\\mu):=\\mathbb{E}_{\\pi}\\left[\\ell(\\Phi_{\\mu}(X),Y)\\right]$ . The problem on $\\mathcal{Z}^{N}$ is thus lifted to the convex optimization problem on $\\mathcal{P}(\\mathcal{Z})$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mu\\in{\\mathcal{P}}({\\mathcal{Z}})}R(\\mu).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Accordingly, this motivates looking at the evolution of empirical measures $(\\nu_{k}^{N})_{k\\in\\mathbb{N}}:=(\\nu_{\\theta^{k}}^{N})_{k\\in\\mathbb{N}}\\subseteq$ $\\mathcal{P}(\\mathcal{Z})$ instead of that of the specific parameters $(\\theta^{k})_{k\\in\\mathbb{N}}\\subseteq\\mathcal{Z}^{N}$ . The MF approach to NNs (see [16, 53, 62, 67]) aims at providing theoretical guarantees for problem (2), justifying that a global optimum of the population risk can be approximated by training a NN with SGD for large $N$ . We next provide some necessary background on WGFs and on the MF theory of shallow NN models. ", "page_idx": 2}, {"type": "text", "text": "2.2 Wasserstein gradient flow and mean-field limit of shallow models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We briefly recall some elements of Optimal Transport and Wasserstein Gradient Flows, referring to [1, 63, 71] for further background. Let $\\mathcal{Z}$ be a Hilbert space with norm $\\|\\cdot\\|$ and, for $p\\in[1,\\infty)$ , let $\\mathcal{P}_{p}(\\mathcal{Z})\\;:=\\;\\{\\mu\\;\\in\\;\\mathcal{P}(\\mathcal{Z})\\,:\\;\\int_{\\mathcal{Z}}\\|\\theta\\|^{p}\\mu(d\\theta)\\;<\\;+\\infty\\}$ be the space of probability measures on $\\mathcal{Z}$ with finite $p$ -th moment. We endow this space with the $p$ -th Wasserstein distance, defined as: $\\begin{array}{r}{W_{p}(\\mu,\\nu):=\\left[\\operatorname*{inf}_{\\gamma\\in\\Pi(\\mu,\\nu)}\\mathbb{E}_{\\gamma}[\\|X-Y\\|^{p}]\\right]^{\\frac{1}{p}},\\forall\\mu,\\nu\\in\\mathcal{P}_{p}(\\mathcal{Z})}\\end{array}$ with $\\Pi(\\mu,\\nu)$ being the set of couplings between $\\mu$ and $\\nu$ (the infimum is always attained). The metric space $(\\mathcal{P}_{p}(\\mathcal{Z}),W_{p})$ is Polish and called the $p$ -th Wasserstein Space. In the remainder of this section we consider $p=2$ and $\\mathcal{Z}=\\mathbb{R}^{D}$ . ", "page_idx": 2}, {"type": "text", "text": "We recall central objects for the sequel, including Lions\u2019 derivative [9, 47], popularized in mean-field games (see e.g. [10, 12, 15, 38]) and shown (in [32]) to coincide with the Wasserstein gradient ([1]): ", "page_idx": 2}, {"type": "text", "text": "Definition 3 (Linear Functional Derivative and Intrinsic Derivative). Given $F:\\mathcal{P}_{2}(\\mathcal{Z})\\to\\overline{{\\mathbb{R}}},$ its linear functional derivative is the function (if it exists) $\\begin{array}{r}{\\frac{\\partial F}{\\partial\\mu}:D o m(F)\\times\\mathcal{Z}\\rightarrow\\mathbb{R}}\\end{array}$ such that $\\forall\\mu,\\nu\\in$ $D o m(F)$ $\\begin{array}{r}{F^{\\prime}:\\mu\\in\\mathcal{P}_{2}(\\mathcal{Z})\\mapsto\\frac{\\partial F}{\\partial\\mu}(\\mu,\\cdot)}\\end{array}$ F ((1\u2212h)\u00b5+hh\u03bd)\u2212F (\u00b5)= Z\u2202\u2202F\u00b5 (\u00b5, z)d(\u03bd \u2212\u00b5)(z) and Z\u2202\u2202F\u00b5 (\u00b5, z)d\u00b5(z) = 0. The tion $F$ $\\mu$ $i f\\:\\frac{\\partial F}{\\partial\\mu}$ exists and is differentiable in its second argument, we define the intrinsic derivative of $F$ at $\\mu$ to be: $\\begin{array}{r}{D_{\\mu}F(\\mu,z)=\\nabla_{z}\\left(\\frac{\\partial F}{\\partial\\mu}(\\mu,z)\\right)}\\end{array}$ . Abusing notation, we will write $\\begin{array}{r}{\\frac{\\partial F}{\\partial\\mu}\\,:\\,\\mathcal{P}_{2}(\\mathcal{Z})\\times\\mathcal{Z}\\to\\mathbb{R}}\\end{array}$ and $D_{\\mu}F:{\\mathcal{P}}_{2}({\\mathcal{Z}})\\times{\\mathcal{Z}}\\to{\\dot{\\mathcal{Z}}}$ , even if they are only partially defined. ", "page_idx": 2}, {"type": "text", "text": "This allows us to define next a Wasserstein Gradient Flow (following e.g. [1, 16]): ", "page_idx": 2}, {"type": "text", "text": "Definition 4 (Wasserstein Gradient Flow). Let $\\varsigma:\\mathbb{R}_{+}\\rightarrow\\mathbb{R}_{+}$ be a regular scalar function and $F:\\mathcal{P}_{2}(\\mathcal{Z})\\to\\overline{{\\mathbb{R}}}$ be a convex functional for which the intrinsic derivative $D_{\\mu}F$ is defined. We define a Wasserstein Gradient Flow $({\\pmb W}{\\pmb G}{\\pmb F})$ for $F$ (shortened $W G F(F))$ as any absolutely continuous trajectory $(\\mu_{t})_{t\\geq0}$ in $\\mathcal{P}_{2}(\\mathcal{Z})$ that satisfies, distributionally on $[0,\\infty)\\!\\times\\!\\mathcal{Z}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\partial_{t}\\mu_{t}=\\varsigma(t)\\,\\mathrm{div}\\left(D_{\\mu}F(\\mu_{t},\\cdot)\\mu_{t}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Several authors ([1, 16, 63, 71], among others) have proven under various sets of assumptions that, given an initial condition $\\mu_{0}\\in\\mathscr{P}_{2}(\\mathcal{Z})$ , the $\\mathbf{WGF}(F)$ admits a unique (weak) solution, $\\bar{(\\mu_{t})}_{t\\geq0}$ . In a sense, $\\mathbf{WGF}(F)$ \u2018follows the negative gradient\u2019 of $F$ . Unfortunately, even for convex $F$ , stationary points of $\\mathbf{WGF}(F)$ need not be global minima, see [16]. ", "page_idx": 3}, {"type": "text", "text": "We are interested in the case where $F$ is the following convex, entropy-regularized population risk: $\\begin{array}{r}{R^{\\tau,\\,\\beta}(\\mu):=R(\\mu)+\\tau\\int r d\\mu+\\beta H_{\\lambda}(\\mu)}\\end{array}$ , where $\\tau,\\beta\\geq0$ , $\\lambda$ is the Lebesgue Measure on $\\mathcal{Z}$ , $r:\\mathcal{Z}\\to\\mathbb{R}_{+}$ is a penalization, and $H_{\\lambda}$ defined as $\\begin{array}{r}{H_{\\lambda}(\\mu):=\\int\\log(\\frac{d\\mu}{d\\lambda}(z))d\\mu(z)}\\end{array}$ if $\\mu\\ll\\lambda$ or $+\\infty$ otherwise, is the Boltzmann entropy of $\\mu$ . In this case, $\\mathbf{WGF}(R^{\\tau,\\,\\beta})$ reads as the PDE: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\partial_{t}\\mu_{t}=\\varsigma(t)\\left[\\mathrm{div}\\left(\\left(D_{\\mu}R(\\mu_{t},\\cdot)+\\tau\\nabla_{\\theta}r\\right)\\mu_{t}\\right)+\\beta\\Delta\\mu_{t}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "known as McKean-Vlasov equation in the probability and PDE communities (see the classic references [54, 70], and the recent review [11]) and popularized as \u2018distributional dynamics\u2019 in NN literature (e.g. [53]). When $\\beta>0$ , a solution to (4) has a density w.r.t. $\\lambda$ and is actually strong. Under rather simple technical assumptions (see SuppMat-D.3, or [12, 15, 38, 57, 69]), when $\\tau,\\beta>0$ it is known that the $\\mathbf{WGF}(R^{\\tau,\\beta})$ $W_{2}$ -converges to a (unique) minimizer. When $\\tau,\\beta=0$ a sort of converse holds (see [16]): if $\\mathbf{WGF}(R)$ converges in $W_{2}$ , then the limit minimizes $R$ . ", "page_idx": 3}, {"type": "text", "text": "Proven by [16, 53, 62, 67] and later refined e.g. by [14, 22, 23, 51, 66, 69], the main result in the MF Theory of overparametrized shallow NNs states that SGD training for a shallow NN, in the right scaling limit as $N\\rightarrow\\infty$ , approximates a WGF : ", "page_idx": 3}, {"type": "text", "text": "Theorem 1 (Mean-Field limit, sketch). For each $T>0$ , under relevant technical assumptions including regularity of $\\sigma_{*}$ and a proper asymptotic behaviour of $\\varepsilon_{N}\\to0$ as $N\\rightarrow\\infty$ , the rescaled empirical process given by $\\mu^{N^{\\ast}}:=\\;(\\nu_{\\lfloor t/\\varepsilon_{N}\\rfloor}^{N^{\\prime}\\phantom{}})t\\in[0,T]$ converges in law (in the Skorokhod space $D_{\\mathcal{P}(\\mathcal{Z})}([0,T]),$ ) to $\\mu:=(\\mu_{t})_{t\\in[0,T]}$ given by the unique $W G F(R^{\\tau,\\beta})$ starting at $\\mu_{0}$ . ", "page_idx": 3}, {"type": "text", "text": "Despite the MF limit of NNs being a theoretical approximation, the behavior it predicts can effectively be observed in practice, even for finite, not too large $N$ (see the numerical experiments in many of the aforementioned works and below). Moreover, it is the asymptotic regime that most closely describes the actual feature-learning behavior observed in large, overparametrized NNs during training (as compared e.g. to the lazy-training regime described by the Neural Tangent Kernel approximation [17, 41]). Note that, for $\\beta>0$ , the entropy term $H_{\\lambda}$ in $\\mathbf{\\dot{W}G F}(R^{\\tau,\\beta})$ (as well as the Laplace operator in equation (4)) is approximated, in practice, by the Gaussian noise term in the SGLD (1), as $N\\rightarrow\\infty$ . ", "page_idx": 3}, {"type": "text", "text": "2.3 Symmetry-leveraging techniques ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We next discuss mathematical formulations of the main techniques to leverage posited distributional symmetries of the data at the training or architecture levels. We henceforth fix a compact group $G$ of normalized Haar measure $\\lambda_{G}$ , acting on $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ , which we denote $G\\operatorname{\\bigcirc}_{\\rho}\\,{\\mathcal{X}}$ , $G\\operatorname{C}_{\\hat{\\rho}}\\mathcal{V}$ . $^2\\mathrm{~A~}$ function $f:\\mathcal X\\to\\mathcal Y$ is termed equivariant if $\\forall g\\in G,{\\hat{\\rho}}_{g^{-1}}.f(\\rho_{g}.x)=f(x)\\ \\ d\\pi{\\dot{x}}(x)$ -a.s. We further say that the data $(X,Y)\\sim\\pi$ is equivariant, and write $\\pi\\in\\mathcal{P}^{G}(\\mathcal{X}\\times\\mathcal{P})$ , if $\\forall g\\in G$ , $(\\rho_{g}.X,\\hat{\\rho}_{g}.Y)\\sim\\pi$ (this is not enforced unless stated). The space of functions $f:\\mathcal X\\to\\mathcal Y$ square-integrable (in Bochner sense) w.r.t $\\pi_{\\mathcal{X}}=L a w(X)$ is called $L^{2}(\\bar{\\mathcal{X}},\\mathcal{Y};\\pi_{\\mathcal{X}})$ . Further relevant concepts are introduced as needed. ", "page_idx": 3}, {"type": "text", "text": "Data Augmentation (DA): This training scheme considers $\\{g_{k}\\}_{k\\in\\mathbb{N}}\\overset{i.i.d.}{\\sim}\\lambda_{G}$ independent from the $\\{(X_{k},Y_{k})\\}_{k\\in\\mathbb{N}}$ in (1), and carries out SGD on samples $\\{(\\rho_{g_{k}}.X_{k},\\hat{\\rho}_{g_{k}}.Y_{k})\\}_{k\\in\\mathbb{N}}$ . DA and the vanilla training scheme would thus be equivalent if $\\pi\\in\\mathcal{P}^{G}(\\mathcal{X}\\times\\mathcal{Y})$ . One can show (see [13, 48]) that, performing SGD with DA, results in an optimization scheme for the symmetrized population risk, $\\begin{array}{r}{\\dot{R}^{D A}(\\theta):=\\mathbb{E}_{\\pi}\\left[\\int_{G}\\ell\\left(\\Phi_{\\theta}^{N}(\\rho_{g}.X),\\hat{\\rho}_{g}.Y\\right)\\dot{d}\\lambda_{G}(g)\\right]}\\end{array}$ . Despite being effective in practice, DA gives no guarantee that the resulting model will be equivariant. For deeper insights, see [13, 21, 39, 46, 48, 52]. ", "page_idx": 3}, {"type": "text", "text": "Feature Averaging (FA): Instead of focusing on the data, FA works with symmetrized versions of the vanilla NN models $\\Phi_{\\theta}^{N}$ at hand, averaging model copies over all possible translations through the group action. This amounts to constructing (or approximating) the symmetrization operator over $L^{2}(\\bar{\\mathcal{X}},\\bar{\\mathcal{Y}};\\pi_{\\mathcal{X}})$ defined as $\\begin{array}{r}{(\\mathcal{Q}_{G}.f)(x):=\\int_{G}\\hat{\\rho_{g^{-1}}}.f\\bar{(\\rho_{g}.x)}d\\lambda_{G}(\\bar{g})}\\end{array}$ (see [27]), and trying to minimize $R^{F A}(\\theta):=\\mathbb{E}_{\\pi}\\left[\\ell\\left((\\mathcal{Q}_{G}.\\Phi_{\\theta}^{N})(X),Y\\right)\\right]$ (see [13, 21, 46, 48]). The resulting model will be equivariant, however, FA is inefficient, as $\\approx|G|$ times more evaluations are needed for training and inference. ", "page_idx": 4}, {"type": "text", "text": "Equivariant Architectures (EA): Following [8], EA in multilayer NNs are configurations yielding models equivariant between each of the hidden layers (where $G$ is assumed to act). As stated in [29, 30, 61, 64, 65, 75] and [2, 18, 44, 45, 73], once the (equivariant) activation functions between the different layers have been fixed, EAs are plainly parameter-sharing schemes (determined by the space of intertwiners/group convolutions between layers). In our context, assuming that $G\\operatorname{C}_{M}\\mathcal{Z}$ is some group action, we require that $\\sigma_{*}:\\mathcal{X}\\times\\mathcal{Z}\\to\\mathcal{Y}$ is jointly equivariant, namely, $\\forall(g,x,z)\\in$ $G\\times\\mathcal{X}\\times\\mathcal{Z}$ , $\\sigma_{*}(\\rho_{g}.x,M_{g}.z)=\\hat{\\rho}_{g}\\sigma_{*}(x,z)$ ; to ensure $G$ -actions over different spaces are properly related. Introducing the set of fixed points for $G\\subset_{M}{\\mathcal{Z}},{\\mathcal{E}}^{G}:=\\{z\\in{\\mathcal{Z}}\\ :\\ \\forall g\\in G\\}$ , $M_{g}.z=z\\}$ , a shallow NN model $\\Phi_{\\boldsymbol{\\theta}}^{N}$ thus has an EA if $\\theta\\,\\in\\,(\\mathcal{E}^{G})^{N}$ . Under the right choices of $\\sigma_{*}$ and $M$ , the obtained EAs can encode interesting and widely applied architectures, such as CNNs [19] and DeepSets [77] (see SuppMat-C.1 for further discussion). We call ${\\mathcal{E}}^{G}$ the subspace of invariant parameters, which is a closed linear subspace of $\\mathcal{Z}$ , with unique orthogonal projection $P_{\\mathcal{E}^{G}}:\\mathcal{Z}\\to$ $\\mathbf{\\bar{\\boldsymbol{\\mathcal{E}}}}^{G}$ , explicitly given by $\\begin{array}{r}{P_{\\mathcal{E}^{G}\\cdot\\mathcal{Z}}:=\\int_{G}M_{g\\cdot}\\:\\!z\\,d\\lambda_{G}(g)}\\end{array}$ for $z\\in{\\mathcal{Z}}$ (see [30]). We are thus led to solve: $\\mathrm{min}_{\\theta\\in(\\mathcal{E}^{G})^{N}}\\,R(\\theta)$ or, equivalently, to find the best projected model $\\Phi_{\\theta}^{N,E A}:=\\langle\\sigma_{*},P_{\\mathcal{E}^{G}}\\#\\nu_{\\theta}^{N}\\rangle$ , by minimizing $R^{E A}(\\theta):=\\mathbb{E}_{\\pi}\\left[\\ell\\left(\\Phi_{\\theta}^{N,E A}(X),Y\\right)\\right]$ . This can considerably reduce the parameter space dimension; however EA might generally yield a decreased expressivity or approximation capacity. ", "page_idx": 4}, {"type": "text", "text": "3 Symmetries in overparametrized neural networks: main results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "3.1 Two notions of symmetries for parameter distributions ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The following notions regarding distributions from $\\mathcal{P}(\\mathcal{Z})$ are central to our work: ", "page_idx": 4}, {"type": "text", "text": "Definition 5. Given $\\mu\\in{\\mathcal{P}}({\\mathcal{Z}})$ , we respectively define its symmetrized and projected versions as $\\begin{array}{r}{\\mu^{G}\\;:=\\;\\int_{G}(M_{g}\\#\\mu)d\\lambda_{G}}\\end{array}$ and $\\dot{\\mu}^{\\varepsilon^{G}}\\,:=\\,P_{\\varepsilon^{G}}\\#\\mu.$ . Moreover, we introduce two subspaces of $\\mathcal{P}(\\mathcal{Z})$ : $\\mathcal{P}^{G}(\\mathcal{Z}):=\\{\\mu\\in\\mathcal{P}(\\mathcal{Z})\\;:\\;\\forall g\\in G,\\;M_{g}\\#\\mu=\\mu\\}$ and ${\\mathcal{P}}({\\mathcal{E}}^{G}):=\\{\\mu\\in{\\mathcal{P}}({\\mathcal{Z}})\\ :\\ \\mu({\\mathcal{E}}^{G})=1\\}$ . ", "page_idx": 4}, {"type": "text", "text": "Example. For $G=\\{\\pm1\\}$ acting multiplicatively on $\\mathcal{Z}=\\mathbb{R}_{}$ , one has $\\mathcal{E}^{G}=\\{0\\}$ , hence $\\mathcal{P}(\\mathcal{E}^{G})=$ $\\{\\delta_{0}\\}$ , while $\\begin{array}{r}{\\mathcal{P}^{G}(\\mathcal{Z})=\\{\\frac{1}{2}(\\nu\\!+\\!\\nu(-\\cdot)):\\nu\\in\\mathcal{P}(\\mathbb{R}_{+})\\}}\\end{array}$ . In particular, for $z\\in{\\mathcal{Z}}$ , $\\begin{array}{r}{\\(\\delta_{z})^{G}=\\frac{1}{2}(\\delta_{z}\\!+\\!\\delta_{-z})}\\end{array}$ . Definition 6 (Invariant Probability Measures). We say that $\\mu\\in{\\mathcal{P}}({\\mathcal{Z}})$ is: ", "page_idx": 4}, {"type": "text", "text": "Weakly-Invariant (WI) i $f\\,\\mu=\\mu^{G}$ and Strongly-Invariant (SI) $i f\\mu=\\mu^{\\mathcal{E}^{G}}$ . ", "page_idx": 4}, {"type": "text", "text": "We notice that: $\\mathcal{P}(\\mathcal{E}^{G})\\subseteq\\mathcal{P}^{G}(\\mathcal{Z})$ , $\\mu^{G}\\in\\mathcal{P}^{G}(\\mathcal{Z})$ and $\\mu^{\\mathcal{E}^{G}}\\in\\mathcal{P}(\\mathcal{E}^{G})$ . Thus, SI implies WI. Next result relates the symmetrization operation on $\\mathcal{P}(\\mathcal{Z})$ with the one on shallow models $\\mathcal{F}_{\\sigma_{*}}(\\mathcal{P}(\\mathcal{Z}))$ : ", "page_idx": 4}, {"type": "text", "text": "Proposition 1. Let $\\Phi_{\\mu}\\in\\mathcal{F}_{\\sigma_{\\ast}}(\\mathcal{P}(\\mathcal{Z}))$ with $\\sigma_{*}:\\mathcal{X}\\times\\mathcal{Z}\\to\\mathcal{Y}$ jointly equivariant. Then: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\bigl(\\mathcal{Q}_{G}\\Phi_{\\mu}\\bigr)=\\Phi_{\\mu^{G}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "That is to say, the closest equivariant function (in $L^{2}(\\mathcal{X},\\mathcal{Y};\\pi_{\\mathcal{X}});$ to $\\Phi_{\\mu}$ is given by the shallow model associated to the symmetrized version of $\\mu$ . ", "page_idx": 4}, {"type": "text", "text": "Remark. In particular, $\\Phi_{\\mu}$ is equivariant as soon as $\\mu$ is WI only. Conversely, if $\\Phi_{\\mu}:\\mathcal{X}\\to\\mathcal{Y}$ is an equivariant function, then $\\Phi_{\\mu}=\\Phi_{\\mu^{G}}$ , i.e. it can be expressed in terms of a WI distribution. This highlights a priority role of WI distributions on $\\mathcal{Z}$ in representing invariant shallow models. ", "page_idx": 4}, {"type": "text", "text": "The alternative, \u2018projected model\u2019 $\\Phi_{\\mu^{\\varepsilon^{G}}}$ , in turn, is never the closest equivariant shallow model, to $\\Phi_{\\mu}$ in $L^{2}(\\mathcal{X},\\mathcal{Y},\\pi_{\\mathcal{X}})$ , unless equal to $\\Phi_{\\mu^{G}}$ . The latter rarely is the case (unlike commonly implied in the literature). In fact, the symmetrized version $\\mathcal{Q}_{G}\\Phi_{\\theta}^{N}$ of a shallow NN model $\\Phi_{\\boldsymbol{\\theta}}^{N}$ involves $\\begin{array}{r}{(\\nu_{\\theta}^{N})^{G}\\,=\\,\\frac{1}{N}\\sum_{i=1}^{N}\\varphi_{\\theta_{i}}}\\end{array}$ , where $\\forall z\\in{\\mathcal{Z}}$ , $\\varphi_{z}$ is the orbit measure of the action,3 and has $N\\cdot|G|$ $\\mathcal{Z}$ -valued parameters (possibly with $|G|=\\infty)$ ). This sharply contrasts $\\begin{array}{r}{(\\nu_{\\theta}^{N})^{\\mathcal{E}^{G}}=\\frac{1}{N}\\sum_{i=1}^{N}\\delta_{P_{\\mathcal{E}^{G}}.\\theta_{i}},}\\end{array}$ which has $\\le N$ distinct parameters, all living in ${\\mathcal{E}}^{G}$ . So, in general, depending on $\\sigma_{*}$ and $G$ , one might have $\\langle\\sigma_{*},(\\nu_{\\theta}^{N})^{\\mathcal{E}^{G}}\\rangle\\overset{\\bullet}{\\neq}\\mathcal{Q}_{G}\\Phi_{\\theta}^{N}$ . A notable case in which the equality holds is the class of linear models, which is discussed in SuppMat-E.1. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Example. In the previous example, for $\\mu=\\delta_{z}$ and $\\sigma_{*}$ jointly equivariant, we have $\\Phi_{\\mu}=\\sigma_{*}(\\cdot,z)$ , $\\Phi_{\\mu^{G}}=\\mathcal{Q}_{G}\\Phi_{\\mu}=\\textstyle\\frac12(\\sigma_{*}(\\cdot,z)+\\sigma_{*}(\\cdot,-z))$ and $\\Phi_{\\mu^{\\varepsilon^{G}}}=\\sigma_{*}(\\cdot,0)$ which are generally distinct if $z\\neq0$ . Notice that $\\Phi_{\\mu^{G}}$ is an equivariant function without any of its \u2018parameters\u2019 living in ${\\mathcal{E}}^{G}$ . ", "page_idx": 5}, {"type": "text", "text": "3.2 Invariant functionals on $\\mathcal{P}(\\mathcal{Z})$ and their optima ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In the same spirit as when defining the population risk $R:\\mathcal{P}(\\mathcal{Z})\\to\\overline{{\\mathbb{R}}}$ in (2), the risk functions associated with SL-techniques from Section 2.3 can be lifted to functionals over $\\mathcal{P}(\\mathcal{Z})$ , namely to: $\\begin{array}{r}{R^{D A}(\\mu)\\,:=\\,\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\int_{G}\\ell\\left(\\Phi_{\\mu}(\\rho_{g}.X),\\hat{\\rho}_{g}.Y\\right)d\\lambda_{G}(g)\\right]}\\end{array}$ , $R^{F A}(\\mu)\\,:=\\,\\mathbb{E}_{\\pi}\\left[\\ell\\left(\\mathcal{Q}_{G}(\\Phi_{\\mu})(X),Y\\right)\\right]$ and $R^{E A}(\\mu):=\\mathbb{E}_{\\pi}\\left[\\ell\\left(\\Phi_{\\mu^{\\varepsilon G}}(X),Y\\right)\\right]$ , respectively. This will allow us to study these SL-techniques, in the overparametrized regime, under a common MF framework. We need the following assumption: ", "page_idx": 5}, {"type": "text", "text": "Assumption 1. $\\pi\\in\\mathcal{P}_{2}(\\mathcal{X}\\times\\mathcal{Y});\\,\\ell:\\mathcal{Y}\\times\\mathcal{Y}\\to\\mathbb{R}$ is convex, jointly invariant and differentiable with $\\nabla_{1}\\ell$ linearly growing; and $\\sigma_{*}:\\mathcal{X}\\times\\mathcal{Z}\\rightarrow\\mathcal{Y}$ is bounded, jointly equivariant and differentiable. ", "page_idx": 5}, {"type": "text", "text": "The quadratic loss $\\begin{array}{r}{\\ell(y,\\hat{y})=\\frac{1}{2}||y{-}\\hat{y}||^{2}}\\end{array}$ is an example of such $\\ell$ . Having $\\sigma_{*}$ bounded and differentiable is a simplifying assumption, usually made in the MF literature, when establishing key results such as global convergence of NN (see e.g. [12, 38, 53]); relaxing this condition to include further commonlyused functions $\\sigma_{*}$ seems feasible, up to some additional technicalities (see SuppMat-A.2 for further discussion). Finally, having $\\sigma_{*}$ be jointly equivariant (as defined in section 2.3) isn\u2019t a truly restrictive assumption: under the right choice of $\\sigma_{*}$ and $M$ , any usual single-hidden-layer NN architecture can be made to satisfy it (see SuppMat-C.1 for a deeper discussion). We also need: ", "page_idx": 5}, {"type": "text", "text": "Definition 7. A functional $F:\\mathcal{P}(\\mathcal{Z})\\to\\mathbb{R}$ is invariant if $F(M_{g}\\#\\mu)=F(\\mu)\\,\\forall(g,\\mu)\\in G\\times\\mathcal{P}(\\mathcal{Z}),$ ;   \nequivalently, if it equals its symmetrized version $\\begin{array}{r}{F^{G}(\\mu):=\\int_{G}F(M_{g}\\#\\mu)d\\lambda_{G}(g)}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "Proposition 2. Under Assumption $^{\\,I}$ , $R^{D A}$ , $R^{F A}$ and $R^{E A}$ are invariant (and convex) and we have: ", "page_idx": 5}, {"type": "equation", "text": "$$\nR^{D A}(\\mu)=R^{G}(\\mu),\\,\\,\\,R^{F A}(\\mu)=R(\\mu^{G})\\,\\,\\,a n d\\,\\,\\,R^{E A}(\\mu)=R(\\mu^{\\varepsilon^{G}}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In particular, $R=R^{D A}\\;i f R$ is invariant. Moreover, $\\forall\\mu\\in\\mathcal{P}^{G}(\\mathcal{Z}),\\,R(\\mu)=R^{D A}(\\mu)=R^{F A}(\\mu)$ .   \nLast, $i f\\pi\\in\\mathcal{P}^{G}(\\mathcal{X}\\times\\mathcal{Y})$ (the data distribution is equivariant), then $R$ is invariant. ", "page_idx": 5}, {"type": "text", "text": "The proof relies on Proposition 1 and calculations as in [30], see SuppMat-E.2. Next result is a general property of functionals over $\\mathcal{P}(\\mathcal{Z})$ , which is key for the forthcoming analysis: ", "page_idx": 5}, {"type": "text", "text": "Proposition 3 (Optimality for Invariant Functionals). Let $F:{\\mathcal{P}}({\\mathcal{Z}})\\to{\\overline{{\\mathbb{R}}}}$ be convex, $\\mathcal{C}^{1}$ and invariant. Then: $\\dot{\\forall}\\mu\\in\\mathcal{P}(\\mathcal{Z}),\\,F(\\mu^{G})\\leq F(\\mu),$ ; and so, $\\operatorname*{inf}_{\\mu\\in{\\mathcal{P}}^{G}({\\mathcal{Z}})}F(\\mu)=\\operatorname*{inf}_{\\mu\\in{\\mathcal{P}}({\\mathcal{Z}})}F(\\mu)$ . In particular, if $F$ has a unique minimizer over $\\mathcal{P}(\\mathcal{Z})$ , it must be WI. ", "page_idx": 5}, {"type": "text", "text": "The proof relies on an ad-hoc version of Jensen\u2019s inequality. Next, we state that optimizing under DA and FA is essentially equivalent, and corresponds to optimizing $R$ exclusively over WI measures: ", "page_idx": 5}, {"type": "text", "text": "Theorem 2 (Equivalence of DA and FA). Under assumption $^{\\,l}$ , we have: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\mu\\in{\\mathcal P}({\\mathcal Z})}R^{D A}(\\mu)=\\operatorname*{inf}_{\\mu\\in{\\mathcal P}^{G}({\\mathcal Z})}R^{D A}(\\mu)=\\operatorname*{inf}_{\\mu\\in{\\mathcal P}^{G}({\\mathcal Z})}R(\\mu)=\\operatorname*{inf}_{\\mu\\in{\\mathcal P}^{G}({\\mathcal Z})}R^{F A}(\\mu)=\\operatorname*{inf}_{\\mu\\in{\\mathcal P}({\\mathcal Z})}R^{F A}(\\mu).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Note that, on the other hand, $R^{E A}$ only satisfies: $\\operatorname*{inf}_{\\mu\\in{\\mathcal{P}}({\\mathcal{Z}})}R^{E A}(\\mu)=\\operatorname*{inf}_{\\mu\\in{\\mathcal{P}}({\\mathcal{E}}^{G})}R(\\mu)$ . In the case of the quadratic loss, Theorem 2 can be made more explicit: ", "page_idx": 5}, {"type": "text", "text": "Corollary 1. Under Assumption $^{\\,I}$ , when the loss is quadratic and $\\pi_{\\mathcal{X}}$ is invariant, we have: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\substack{\\mu\\in\\mathcal{P}^{G}(\\mathcal{Z})}}R(\\mu)=R_{*}+\\operatorname*{inf}_{\\substack{\\mu\\in\\mathcal{P}^{G}(\\mathcal{Z})}}\\|\\Phi_{\\mu}-f_{*}\\|_{L^{2}(\\mathcal{X},\\mathcal{Y};\\pi_{\\mathcal{X}})}^{2}=\\tilde{R}_{*}+\\operatorname*{inf}_{\\substack{\\mu\\in\\mathcal{P}^{G}(\\mathcal{Z})}}\\|\\Phi_{\\mu}-\\mathcal{Q}_{G}.f_{*}\\|_{L^{2}(\\mathcal{X},\\mathcal{Y};\\pi_{\\mathcal{X}})}^{2}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $f_{*}=\\mathbb{E}_{\\pi}[Y|X=\\cdot],$ , and $R_{*}$ , $\\tilde{R}_{*}$ are constants only depending on $\\pi$ and $f_{*}$ . That is, optimizing under $D A$ and $F\\!A$ corresponds to approximating the symmetrized version of $f_{*}$ . ", "page_idx": 5}, {"type": "text", "text": "Under equivariance of the data distribution $\\pi$ , the following general result also holds: ", "page_idx": 6}, {"type": "text", "text": "Corollary 2. Let Assumption $^{\\,l}$ hold and suppose $\\pi\\,\\in\\,\\mathcal P^{G}(\\mathcal X\\times\\mathcal y)$ . Then, $R$ is invariant and therefore: $\\begin{array}{r}{\\operatorname*{inf}_{\\mu\\in\\mathcal{P}(\\mathcal{Z})}R(\\mu)=\\operatorname*{inf}_{\\mu\\in\\mathcal{P}^{G}(\\mathcal{Z})}R(\\mu)=\\operatorname*{inf}_{\\mu\\in\\mathcal{P}(\\mathcal{Z})}R^{D A}(\\mu)=\\operatorname*{inf}_{\\mu\\in\\mathcal{P}(\\mathcal{Z})}R^{F A}(\\mu)}\\end{array}$ . ", "page_idx": 6}, {"type": "text", "text": "Remark. Consequently, equivariant data allow us to globally optimize the population risk by only considering WI measures. It also shows that $D A$ and $F\\!A$ provide no advantage for this optimization. ", "page_idx": 6}, {"type": "text", "text": "The same unfortunately is not true for $\\mathbf{S}\\mathbf{I}$ measures (answering a question in [27]), as shown by the following result, which constructs a simple example in which $\\overline{{\\mathcal{E}^{G}}}$ is trivial: ", "page_idx": 6}, {"type": "text", "text": "Proposition 4. Even with a finite group $G$ acting orthogonally on $\\mathcal{X}=\\mathbb{R}^{d},\\;\\mathcal{Y}=\\mathbb{R}$ and $\\mathcal{Z}=\\mathbb{R}^{(d+2)}$ ; with \u03c0 being compactly-supported and equivariant; with $\\ell$ being quadratic; and with $\\sigma_{*}$ being ${\\mathcal{C}}^{\\infty}$ , bounded and jointly equivariant; we can have: $\\operatorname*{inf}_{\\mu\\in{\\mathcal{P}}({\\mathcal{Z}})}R(\\mu)<\\operatorname*{inf}_{\\nu\\in{\\mathcal{P}}({\\mathcal{E}}^{G})}R(\\nu)$ . ", "page_idx": 6}, {"type": "text", "text": "In fact, even if $R$ is invariant, when ${\\mathcal{E}}^{G}$ is too restrictive, it might become impossible to globally optimize $R$ over SI measures (which amounts to using $R^{\\dot{E}A}$ as a proxy for $R$ ). This subtlety has to be considered when deciding to use EAs on problems where symmetries exist. Nevertheless, if ${\\mathcal{E}}^{G}$ has good universality properties, a true $\\mathbf{SI}$ solution to the learning problem can be sought for: ", "page_idx": 6}, {"type": "text", "text": "Proposition 5. Let Assumption 1 hold, $\\ell$ be quadratic and $\\pi\\in\\mathcal{P}_{2}^{G}(\\mathcal{X}\\times\\mathcal{Y})$ . If $\\mathcal{F}_{\\sigma_{*}}(\\mathcal{P}(\\mathcal{E}^{G}))$ is dense in $\\bar{L_{G}^{2}}(\\mathcal{X},\\mathcal{Y};\\pi_{\\mathcal{X}}):=\\mathcal{Q}_{G}\\hat{(L^{2}(\\mathcal{X},\\mathcal{Y};\\pi_{\\mathcal{X}}))}$ , then: $\\begin{array}{r}{\\operatorname*{inf}_{\\mu\\in\\mathcal P(\\mathcal Z)}R(\\mu)=\\operatorname*{inf}_{\\nu\\in\\mathcal P(\\mathcal E^{G})}R(\\nu)=R_{*}}\\end{array}$ . ", "page_idx": 6}, {"type": "text", "text": "Remark. See e.g. [50, 60, 76, 77] for conditions on ${\\mathcal{E}}^{G}$ and $\\sigma_{*}$ guaranteeing this \u2018restricted\u2019 universality on $L_{G}^{\\bar{2}}(\\mathcal{X},\\mathcal{Y};\\pi_{\\mathcal{X}})$ . These allow for effectively solving the problem in fewer dimensions, which is key in successful $E A$ like CNNs and DeepSets. See SuppMat-E.2.5 for a deeper discussion. ", "page_idx": 6}, {"type": "text", "text": "3.3 Symmetries and SL training dynamics in the overparametrized regime ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now study the MFL of the various training dynamics when $\\mathcal{Z}=\\mathbb{R}^{D}$ . We begin with the general: Theorem 3. Let $F:\\mathcal{P}(\\mathcal{Z})\\to\\overline{{\\mathbb{R}}}$ be an invariant functional such that $W G F(F)$ is well defined and has a unique (weak) solution $(\\mu_{t})_{t\\geq0}$ . If $\\mu_{0}\\in\\mathcal{P}_{2}^{G}(\\mathcal{Z})$ , then, for dt-a.e. $t\\geq0$ we have $\\mu_{t}\\in\\mathcal{P}_{2}^{G}(\\mathcal{Z})$ . ", "page_idx": 6}, {"type": "text", "text": "The proof of Theorem 3 relies on $D_{\\mu}F$ being equivariant (in a suitable sense) and $(M_{g}\\mu_{t})_{t\\geq0}$ satisfying also, as a consequence, $\\mathbf{WGF}(F)$ (See SuppMat-E.3 for the details). Note that $\\mu_{0}\\in\\mathcal{P}_{2}^{G}(\\mathcal{Z})$ is simply verified, e.g. by a standard Gaussian in $\\mathcal{Z}$ . Specializing this result, we get: ", "page_idx": 6}, {"type": "text", "text": "Corollary 3. Let Assumption $^{\\,l}$ and technical assumptions (as in [16]) hold. Then, if $R$ and $r$ are invariant, $W G F(R^{\\tau,\\beta})$ starting from $\\mu_{0}\\,\\in\\,\\mathcal{P}_{2}^{G}(\\mathcal{Z})$ satisfies: for dt-a.e. $t\\geq0$ , $\\bar{\\mu_{t}}\\bar{\\in\\mathcal{P}_{2}^{G}}(\\mathcal{Z})$ . If moreover $\\beta>0$ , each $\\mu_{t}$ has a density function invariant with respect to $G\\operatorname{C}_{M}\\mathcal{Z}$ . ", "page_idx": 6}, {"type": "text", "text": "Remark. If \u03c0 is equivariant, $R$ is invariant, and this result is valid for a freely-trained NN, without employing SL-techniques. In a way, MFL incorporates these symmetries from infinite SGD iterations. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3 and Corollary 3 can thus be seen as significant generalizations of Proposition 2.1 from [35], which addresses the case of wide 2-layer ReLU networks with a target function that\u2019s symmetric w.r.t. a single orthogonal transformation. The fact that strong solutions to $\\mathbf{WGF}(R^{\\tau,\\beta})$ can be sought among invariant functions to reduce the complexity when $\\pi$ is equivariant, was also first hinted in [53]. The natural domain of invariant functions is in fact the quotient space of $G\\operatorname{C}_{M}\\mathcal{Z}$ (and not ${\\mathcal{E}}^{G}$ , which is strictly embedded in it). ", "page_idx": 6}, {"type": "text", "text": "Comparing the different training dynamics at the MF level and applying Proposition 2, we also get: Theorem 4. Under assumptions of Corollary $3$ , if $\\mu_{0}\\,\\in\\,\\mathcal{P}_{2}^{G}(\\mathcal{Z})$ , $W G F(R^{D A})$ and ${\\pmb W}{\\pmb G}{\\pmb F}({\\cal R}^{F A})$ solutions are equal. If further $R$ is invariant, the $W G F(R)$ solution coincides with them too. ", "page_idx": 6}, {"type": "text", "text": "Remark. In particular, with equivariant data (i.e. invariant $R$ ), training with DA or FA is essentially the same, at least at the MF level, as using no SL-technique whatsoever. Hence, a relevant, practical open question, is: how do the convergence rates to the MFL compare in all three cases, as $N\\to\\infty?$ ", "page_idx": 6}, {"type": "text", "text": "We will now see that similar results hold for $\\mathcal{P}(\\mathcal{E}^{G})$ instead of $\\mathcal{P}^{G}(\\mathcal{Z})$ . Notice that the entropyregularized risk forces each $\\mu_{t}$ to have a density w.r.t. $\\lambda$ in $\\mathcal{Z}$ if $\\beta>0$ . Therefore, if $G\\subset\\!\\!\\mathrm{\\large{M}}^{\\mathcal{Z}}$ is non-trivial (thus ${\\mathcal{E}}^{G}$ is a strict subspace), we always have $\\mu_{t}\\,\\notin\\,\\mathcal{P}(\\mathcal{E}^{G})$ . It thus seems natural to restrain the noise in equation (1) to stay in ${\\mathcal{E}}^{G}$ ; namely, to consider the projected noisy $S G D$ dynamic: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\theta_{i}^{k+1}=\\theta_{i}^{k}-s_{k}^{N}\\,\\left(\\nabla_{z}\\sigma_{*}(X_{k},\\theta_{i}^{k})\\cdot\\nabla_{1}\\ell(\\Phi_{\\theta^{k}}^{N}(X_{k}),Y_{k})+\\tau\\nabla r(\\theta_{i}^{k})\\right)+\\sqrt{2\\beta s_{k}^{N}}P_{\\varepsilon}\\varepsilon_{i}^{k}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Note that projecting only the noise in (5) doesn\u2019t force $\\theta_{i}^{k+1}$ to be in ${\\mathcal{E}}^{G}$ , even if $\\theta_{i}^{k}$ was. Introducing the related projected-regularized functional: $\\begin{array}{r}{R_{{\\mathcal E}^{G}}^{\\tau,\\beta}(\\mu):=R(\\mu)+\\tau\\int r d\\mu+\\beta H_{\\lambda_{\\mathcal E}\\mathrm{G}}(\\mu^{\\mathcal E^{G}})}\\end{array}$ , with $\\lambda_{\\mathcal{E}^{G}}$ the Lebesgue Measure over ${\\mathcal{E}}^{G}$ , we get the following analogue of Corollary 3: ", "page_idx": 7}, {"type": "text", "text": "Theorem 5. Let Assumption $^{\\,l}$ and technical assumptions on $R$ hold. Suppose that $R$ and $r$ are invariant. Then, i ${}^{f}\\mu_{0}\\in\\mathcal{P}_{2}(\\mathcal{E}^{G})$ , $(\\mu_{t})_{t\\geq0}$ solution of $\\bar{\\b{W}}\\pmb{G}\\pmb{F}(\\boldsymbol{R}_{\\mathcal{E}\\mathcal{G}}^{\\tau,\\beta})$ satisfies $\\forall t\\ge0$ , $\\mu_{t}\\in\\mathcal{P}_{2}(\\mathcal{E}^{G})$ . ", "page_idx": 7}, {"type": "text", "text": "The result holds for $\\beta\\geq0$ . Its proof is based on pathwise properties of the McKean-Vlasov stochastic differential equation (studied e.g. in [22]) associated with the $\\mathbf{WGF}(R_{{\\mathcal{E}}^{G}}^{\\tau,\\beta})$ , see SuppMat-D.2. ", "page_idx": 7}, {"type": "text", "text": "Remark. Theorem 5 is significantly stronger than Corollary 3: it implies that, for equivariant $\\pi$ , the flow will remain in the set of SI distributions all throughout its evolution, despite there being no explicit constraint on the network parameters during training (they can all be freely updated), nor any SL-technique being used. This is a highly non-intuitive fact, and a large $N$ exclusive phenomenon, as our numerical experiments will show. See SuppMat-D.2 for a deeper discussion. ", "page_idx": 7}, {"type": "text", "text": "Remark. Notice that, despite the computation of ${\\mathcal{E}}^{G}$ being generally hard (see [29]), $\\mu_{0}\\in\\mathcal{P}_{2}(\\mathcal{E}^{G})$ can be achieved by simply setting $\\mu_{0}\\,=\\,\\delta_{\\vec{0}}$ . Moreover, since one can also take $\\beta\\,=\\,0$ , \u2018having access\u2019 to the noise projection $P_{\\mathcal{E}^{G}}$ is never explicitly required, allowing for a broader applicability of the result. In particular, as we\u2019ll show in our experiments, usual shallow NNs with all parameters initialized at $\\{0\\}$ , freely trained with \u2018noiseless\u2019 SGD, will satisfy Theorem 5 in the MFL. ", "page_idx": 7}, {"type": "text", "text": "Remark. Theorem 5 holds too for the invariant functionals $R^{D A}$ , $R^{F A}$ and $R^{E A}$ in the role of $R$ , even if $\\pi$ is not equivariant. Notably, DA, FA and $E A$ procedures starting on a SI distribution, despite being free to involve all parameters, will keep the distribution $S I$ all throughout training. ", "page_idx": 7}, {"type": "text", "text": "Last, we also have: ", "page_idx": 7}, {"type": "text", "text": "Theorem 6. Let the conditions for Theorem 5 hold. If $\\mu_{0}\\in\\mathcal{P}_{2}(\\mathcal{E}^{G}).$ , the ${\\pmb W}{\\pmb G}{\\pmb F}({\\cal R}^{F A})$ , $W G F(R^{D A})$ and ${\\pmb W}{\\pmb G}{\\pmb F}({\\cal R}^{E A})$ solutions coincide. If $R$ is invariant, $W G F(R)$ solution coincides with them too. ", "page_idx": 7}, {"type": "text", "text": "4 Numerical experiments and architecture-discovery heuristic ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To empirically illustrate some of our results from the previous section, we consider synthetic data produced in a teacher-student setting (see e.g. [14, 16]). Code necessary for replicating the obtained results, as well as a detailed description of our experimental setting, can be sought in the SuppMat. ", "page_idx": 7}, {"type": "text", "text": "We study a simple setting with: $\\mathcal{X}=\\mathcal{Y}=\\mathbb{R}^{2}$ , $\\mathcal{Z}=\\mathbb{R}^{2\\times2}\\cong\\mathbb{R}^{4}$ , and $G=C_{2}$ acting on $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ by permuting the coordinates; and on $\\mathcal{Z}$ via the natural intertwining action (for which ${\\mathcal{E}}^{G}$ is explicit). We take the jointly equivariant activation $\\sigma_{*}(x,z)=\\sigma(z\\cdot x)$ , $\\forall(x,z)\\in\\mathcal{X}\\times\\mathcal{Z}$ with $\\sigma:\\mathbb{R}\\rightarrow\\mathbb{R}$ a sigmoidal function applied pointwise; and consider normally distributed features, and labels produced from a teacher model $f_{*}$ . This teacher is given by a shallow NN model, either $f_{*}\\,=\\,\\bar{\\Phi}_{\\theta^{*}}^{N_{*}}$ with $N_{*}=5$ arbitrary particles $\\theta^{*}\\,\\in\\,\\mathcal{Z}^{N_{*}}$ , or its symmetrized version $f_{*}\\,=\\,\\mathcal{Q}_{G}.\\Phi_{\\theta^{*}}^{N_{*}}$ (referred to as WI), with 10 particles. 4 Notice that the data distribution $\\pi$ will be equivariant only if the teacher is. We try to mimic such teacher with a student model, $\\Phi_{\\theta}^{N}$ , with the same $\\sigma_{*}$ , but different particles $\\theta\\in\\mathcal{Z}^{N}$ that will be trained to minimize the regularized population risk $R^{\\tau,\\beta}$ (with quadratic loss and penalization). For this we employ the SGD dynamic given by Equation (1) (or projected, if required, as in Equation (5)), possibly involving DA, FA or EA techniques. We refer to free training, with no SL-techniques involved, as vanilla training. Each experiment was repeated $N_{r}=10$ times to ensure consistency. Explicit values of the fixed training parameters are found in SuppMat-F. ", "page_idx": 7}, {"type": "text", "text": "4.1 Study for varying $N$ ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We demonstrate how properties of $\\mathbf{WGF}(R^{\\tau,\\beta})$ stated in Section 3.3 become apparent as $N$ grows. We consider a teacher with $\\nu_{\\theta^{*}}^{N_{*}}$ either arbitrary or WI, and different training schemes performed over $N_{e}$ epochs, all initialized with the same particles drawn from given $\\mu_{0}\\in\\mathscr{P}_{2}(\\mathcal{Z})$ that is either SI or WI. Figure 1 displays the behavior of the student\u2019s particle distribution, $\\nu_{N_{e}}^{N}$ , after training, in terms of certain \u2018normalized version\u2019 of the $W_{2}$ -distance, which we call simply Relative Measure Distance, or RMD for short. 5 We refer to SuppMat-F for further insights and, additionally: a deeper analysis of the case of $\\nu_{\\theta^{*}}^{N_{*}}$ being SI, comparisons between different techniques and EA, and $L^{2}$ comparisons between $\\Phi_{\\boldsymbol{\\theta}}^{N}$ and both $f_{*}$ and $\\mathcal{Q}_{G}.f_{*}$ (to illustrate Corollary 1). ", "page_idx": 7}, {"type": "image", "img_path": "L86glqNCUj/tmp/a1011f0ff93334ce84f1bba393457f2b91275baf29973dfd8e4a080400739339.jpg", "img_caption": ["Figure 1: RMDs, at the end of training, for N = 5, 10, 50, 100, 500, 1000, 5000 and the vanilla $(\\mathbf{V})$ , DA, FA and EA schemes. The first plot of each position displays either $\\mathbf{RMD}^{2}(\\nu_{N_{e}}^{N},(\\nu_{N_{e}}^{N})^{\\mathcal{E}^{G}})$ or $\\mathbf{RMD}^{2}(\\nu_{N_{e}}^{N},(\\nu_{N_{e}}^{N})^{G})$ depending on initialization (either SI or WI); and the second shows the RMD between DA, FA and vanilla schemes. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "We first look at the SI-initialized training. Though from [30] we know that (exact) DA or FA during training will respect ${\\mathcal{E}}^{G}$ without needing to pass to the MFL. This is certainly not true in general for the vanilla scheme, where the symmetry is never explicit for the model. We notice in Figure 1, however, that, as $N$ grows big, the SI-initialized vanilla training scheme, under only a WI teacher, does remain SI throughout training, as predicted in Theorem 5. This is absolutely remarkable, since there is no intuitive reason why the vanilla scheme (were parameters can be updated freely) shouldn\u2019t escape ${\\mathcal{E}}^{G}$ to better approximate $f_{*}$ . For instance, for an arbitrary teacher (with the same particles, but un-symmetrized) vanilla training readily leaves ${\\mathcal{E}}^{G}$ to better approximate $f_{*}$ . Though this isn\u2019t a predicted behaviour from our theory, it motivates a heuristic we present in the upcoming section. On the other hand, and as expected, both DA and FA consistently remain within ${\\mathcal{E}}^{G}$ almost independently of $N$ , and even if $f_{*}$ isn\u2019t equivariant. Finally, as $N$ grows bigger, the end-of-training distribution of the vanilla scheme approaches that of DA and FA (as expected from Theorem 4). ", "page_idx": 8}, {"type": "text", "text": "Regarding the WI-initialized training, unlike the SI case, particles sampled i.i.d. from a WI distribution don\u2019t necessarily yield a WI empirical distribution $\\nu_{0}^{\\tilde{N}}$ . On the one hand, this means we require large $N$ to see $\\nu_{N_{e}}^{N}$ being (approx.) WI; and on the other hand, it means we have no guarantee that DA and FA will be close unless we look at larger $N$ (where Theorem 4 applies). The second column of Figure 1 precisely shows these behaviours as $N$ grows: both a trend of $\\dot{\\nu}_{N_{e}}^{N}$ towards becoming WI, and a clear coincidence between the DA, FA and vanilla schemes (the latter only for equivariant $f_{*}$ ). ", "page_idx": 8}, {"type": "text", "text": "4.2 Heuristic algorithm for discovering EA parameter spaces ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "From these experiments, for non-equivariant $f_{*}$ , the SI-initialized WGF is seen to eventually escape ${\\mathcal{E}}^{G}$ . In turn, for equivariant $f_{*}$ , Figure 2 shows that a training scheme initialized at $E\\ {\\stackrel{\\cdot}{\\subset}}\\ {\\mathcal{E}}^{G}$ (i.e. $\\nu_{\\theta_{0}}^{N}\\,\\in\\,\\mathcal{P}(E))$ , eventually leaves $E$ , but stays within ${\\mathcal{E}}^{G}$ (as expected from Theorem 5). These empirical observations hint to an heuristic for discovering the \u2018good\u2019 EAs for the task at hand. ", "page_idx": 8}, {"type": "text", "text": "Assume indeed $\\pi$ equivariant w.r.t. $G$ . We want to find the unknown, largest (i.e. most expressive) subspace of $\\mathcal{Z}$ supporting SI measures. We hence consider some (large) $N$ , a shallow NN model with e.g. $\\sigma_{*}(x,z)\\bar{=}\\,\\sigma(z.x)$ , and numerical thresholds $(\\delta_{j})_{j\\in\\mathbb{N}}\\subseteq\\mathbb{R}_{+}$ . We define $E_{0}=\\{0\\}\\le\\mathcal{E}^{G}$ as an initial subspace and initialize $\\nu_{\\theta_{0}}^{N}=\\nu_{\\vec{0}}^{N}\\in\\mathcal{P}(E_{0})$ . Then, we iteratively proceed as follows: ", "page_idx": 8}, {"type": "image", "img_path": "L86glqNCUj/tmp/2203c2e3626ba0d754b45cee150f43bf1d0d9103e5cccb206bcadcb0e08895a2.jpg", "img_caption": ["Figure 2: Heuristic method applied on teacher (squares) and student (dots) particles. Row 1: aerial view of hyperplane ${\\mathcal{E}}^{G}$ . Row 2: parallel view, to verify student particles always remain in ${\\mathcal{E}}^{G}$ (red line). Column $^{\\,I}$ : step $j=0$ after training; particles leave ${\\cal E}_{0}=\\{0\\}$ . Column 2: initialization of step $j=1$ on $E_{1}=\\left<v_{E_{0}}\\right>$ . Column 3: step $j=1$ after training; particles leave $E_{1}$ (Row 1), but not $\\mathcal{E}^{G}$ . "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "For $j\\,=\\,0,1,\\ldots$ , initialize a model at $\\nu_{\\theta_{0}}^{N}\\,\\in\\,\\mathcal{P}(E_{j})$ , train it for $N_{e}$ epochs, and check whether $\\mathbf{RMD}^{2}(\\nu_{N_{e}}^{N},P_{E_{j}}\\#\\nu_{N_{e}}^{N})\\,\\le\\,\\delta_{j}$ . If that is the case, the training didn\u2019t escape $E_{j}$ , and one could suppose $\\mathcal{E}^{G}:=E_{j}$ . Otherwise, it left $E_{j}$ (so $\\mathcal{E}^{G}\\neq E_{j}^{\\phantom{\\dagger}}$ ) and one can set e.g. $E_{j+1}:=E_{j}\\oplus v_{E_{j}}$ , with $\\begin{array}{r}{v_{E_{j}}\\,:=\\,\\frac{1}{N}\\sum_{i=1}^{N}(\\theta_{i}^{N_{e}}\\,-\\,P_{E_{j}}.\\theta_{i}^{N_{e}})}\\end{array}$ . Allegedly, this scheme would eventually leave all strict subspaces to reach the \u2018right\u2019 ${\\mathcal{E}}^{G}$ . Figure 2 indeed illustrates this behaviour in our simple teacherstudent example (see SuppMat-F.2 for further details). Notice that we start knowing close to nothing about data symmetries $(E_{0}=\\{0\\}$ ), and end up \u2018discovering\u2019 a data-based parameter-sharing scheme $(E_{*}=\\mathcal{E}^{G})$ ) that allows for building SI NNs. This idea might have potential for real world applications, yet a larger scale experimental analysis and rigorous theoretical guarantees need to be provided. ", "page_idx": 9}, {"type": "text", "text": "We refer to [72] for a different approach to this idea of \u2018discovering the real symmetries of the data\u2019. Their work uses relaxed group convolution layers to discover \u2018data-driven symmetry-breaking\u2019 in ML problems. A deeper comparison between both approaches shall be found in SuppMat-F.2. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In the light of theoretical guarantees given by the MF theory of overparametrized shallow NN, we explored their training dynamics when data is possibly equivariant for some group action and/or SL techniques are employed. We thus described how DA, FA and EA schemes can be understood in the limit of infinite internal units, and studied in that setting the qualitative advantages that can be attired from their use. In this MFL, DA and FA are essentially equivalent in terms of the optimization problem they solve and the trajectory of their associated WGFs. Moreover, for equivariant data, freely-trained NN, in the MFL, obey the same WGF as DA/FA. They also \u201crespect\u201d symmetries during training, as WI and SI initializations (corresponding to symmetric parameter distributions and EA configurations) are preserved throughout, even if potentially all NN parameters can be updated. We also highlighted the prominent role of WI laws in representing equivariant models. We illustrated our results with appropriate numerical experiments, which in turn suggested a data-driven heuristic to find appropriate parameter subspaces for EAs in a given task. Providing theoretical guarantees for this heuristic is an interesting problem left for future work. A further relevant question to address, is to quantify and compare the speeds at which all studied training schemes approach the MFL, as this would a provide a full comparative picture of their performances. Extending the MF analysis of symmetries to NNs with more complex inner structures is another interesting line of work. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "JM thanks partial financial support from ANID Magister Fellowship 22231325 and CMM-Basal Grant FB210005, ANID-Chile. JF thanks partial financial support from Fondecyt Regular Grants 1201948/1242001 and CMM-Basal Grant FB210005, ANID-Chile . Both authors thank Roberto Cortez, Daniel Remenik and Felipe Tobar for comments and discussions which motivated part of the numerical experiments developed in the paper, as well as the refinement of some ideas. The authors also thank Daniela Bellott for her assistance in running part of the numerical experiments. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Luigi Ambrosio, Nicola Gigli, and Giuseppe Savar\u00e9. Gradient Flows in Metric Spaces and in the Space of Probability Measures. Lectures in Mathematics ETH Z\u00fcrich. Birkh\u00e4user, 2. ed edition, 2008. OCLC: 254181287.   \n[2] Jimmy Aronsson. Homogeneous vector bundles and g-equivariant convolutional neural networks. Sampling Theory, Signal Processing, and Data Analysis, 20(2), jul 2022.   \n[3] Dominique Bakry, Ivan Gentil, Michel Ledoux, et al. Analysis and geometry of Markov diffusion operators, volume 103. Springer, 2014.   \n[4] Rapha\u00ebl Barboni, Gabriel Peyr\u00e9, and Fran\u00e7ois-Xavier Vialard. Understanding the training of infinitely deep and wide resnets with conditional optimal transport. arXiv preprint arXiv:2403.12887, 2024.   \n[5] Andrew Barron. Universal approximation bounds for superpositions of a sigmoidal function. ieee trans. on information theory 39, 930-945. Information Theory, IEEE Transactions on, 39:930 \u2013 945, 06 1993.   \n[6] Benjamin Bloem-Reddy, Yee Whye, et al. Probabilistic symmetries and invariant neural networks. Journal of Machine Learning Research, 21(90):1\u201361, 2020.   \n[7] L\u00e9on Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning. SIAM review, 60(2):223\u2013311, 2018.   \n[8] Michael M Bronstein, Joan Bruna, Taco Cohen, and Petar Velic\u02c7kovic\u00b4. Geometric deep learning: Grids, groups, graphs, geodesics, and gauges. arXiv preprint arXiv:2104.13478, 2021.   \n[9] Pierre Cardaliaguet. Notes on mean-field games (from P.-L. Lions lectures at Coll\u00e8ge de France). 2013. Available at: https://www.ceremade.dauphine.fr/\\~cardaliaguet/ MFG20130420.pdf.   \n[10] R. Carmona and F. Delarue. Probabilistic Theory of Mean Field Games with Applications I: Mean Field FBSDEs, Control, and Games. Probability Theory and Stochastic Modelling. Springer International Publishing, 2018.   \n[11] Louis-Pierre Chaintron and Antoine Diez. Propagation of chaos: A review of models, methods and applications. I. models and methods. Kinetic and Related Models, 15(6):895\u20131015, 2022.   \n[12] Fan Chen, Yiqing Lin, Zhenjie Ren, and Songbo Wang. Uniform-in-time propagation of chaos for kinetic mean field langevin dynamics. Electronic Journal of Probability, 29:1\u201343, 2024.   \n[13] Shuxiao Chen, Edgar Dobriban, and Jane H Lee. A group-theoretic framework for data augmentation. Journal of Machine Learning Research, 21(245):1\u201371, 2020.   \n[14] Zhengdao Chen, Grant Rotskoff, Joan Bruna, and Eric Vanden-Eijnden. A dynamical central limit theorem for shallow neural networks. Advances in Neural Information Processing Systems, 33, 2020.   \n[15] L\u00e9na\u00efc Chizat. Mean-field langevin dynamics : Exponential convergence and annealing. Transactions on Machine Learning Research, 2022.   \n[16] Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for overparameterized models using optimal transport. Advances in Neural Information Processing Systems, 31, 2018.   \n[17] Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming. Advances in Neural Information Processing Systems, 32, 2019.   \n[18] Taco S Cohen, Mario Geiger, and Maurice Weiler. A general theory of equivariant CNNs on homogeneous spaces. Advances in Neural Information Processing Systems, 32, 2019.   \n[19] Y. Le Cun, B. Boser, J. S. Denker, R. E. Howard, W. Habbard, L. D. Jackel, and D. Henderson. Handwritten digit recognition with a back-propagation network, page 396\u2013404. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1990.   \n[20] George V. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals and Systems, 2:303\u2013314, 1989.   \n[21] Tri Dao, Albert Gu, Alexander Ratner, Virginia Smith, Chris De Sa, and Christopher R\u00e9. A kernel theory of modern data augmentation. In International conference on machine learning, pages 1528\u20131537. PMLR, 2019.   \n[22] Valentin De Bortoli, Alain Durmus, Xavier Fontaine, and Umut Simsekli. Quantitative propagation of chaos for SGD in wide neural networks. Advances in Neural Information Processing Systems, 33, 2020.   \n[23] Arnaud Descours, Arnaud Guillin, Manon Michel, and Boris Nectoux. Law of large numbers and central limit theorem for wide two-layer neural networks: the mini-batch and noisy case. arXiv preprint arXiv:2207.12734, 2022.   \n[24] J. Diestel and J.J. Uhl. Vector Measures. Mathematical surveys and monographs. American Mathematical Society, 1977.   \n[25] C. Drut\u00b8u and M. Kapovich. Geometric Group Theory. Colloquium Publications. American Mathematical Society, 2018.   \n[26] Paul Dupuis and Richard S Ellis. A weak convergence approach to the theory of large deviations. John Wiley & Sons, 2011.   \n[27] Bryn Elesedy and Sheheryar Zaidi. Provably strict generalisation benefti for equivariant models. In International conference on machine learning, pages 2959\u20132969. PMLR, 2021.   \n[28] Stewart N Ethier and Thomas G Kurtz. Markov processes: characterization and convergence. John Wiley & Sons, 2009.   \n[29] Marc Finzi, Max Welling, and Andrew Gordon Wilson. A practical method for constructing equivariant multilayer perceptrons for arbitrary matrix groups. In International conference on machine learning, pages 3318\u20133328. PMLR, 2021.   \n[30] Axel Flinth and Fredrik Ohlsson. Optimization dynamics of equivariant and augmented neural networks. arXiv preprint arXiv:2303.13458, 2023.   \n[31] Qiang Fu and Ashia Wilson. Mean-field underdamped langevin dynamics and its space-time discretization. arXiv preprint arXiv:2312.16360, 2023.   \n[32] Wilfrid Gangbo and Adrian Tudorascu. On differentiability in the wasserstein space and wellposedness for hamilton\u2013jacobi equations. Journal de Math\u00e9matiques Pures et Appliqu\u00e9es, 125:119\u2013174, 2019.   \n[33] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http://www.deeplearningbook.org.   \n[34] Philipp Grohs and Gitta Kutyniok. Mathematical aspects of deep learning. Cambridge University Press, 2022.   \n[35] Karl Hajjar and L\u00e9na\u00efc Chizat. On the symmetries in the dynamics of wide two-layer neural networks. Electron. Res. Arch., 31(4):2175\u20132212, 2023.   \n[36] B.C. Hall. Lie Groups, Lie Algebras, and Representations: An Elementary Introduction. Graduate Texts in Mathematics. Springer, 2003.   \n[37] Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal approximators. Neural Networks, 2(5):359\u2013366, 1989.   \n[38] Kaitong Hu, Zhenjie Ren, David Siska, and Lukasz Szpruch. Mean-field langevin dynamics and energy landscape of neural networks. In Annales de l\u2019Institut Henri Poincare (B) Probabilites et statistiques, volume 57, pages 2043\u20132065. Institut Henri Poincar\u00e9, 2021.   \n[39] Kevin H Huang, Peter Orbanz, and Morgane Austern. Quantifying the effects of data augmentation. arXiv preprint arXiv:2202.09134, 2022.   \n[40] Ningyuan Huang, Ron Levie, and Soledad Villar. Approximately equivariant graph networks. Advances in Neural Information Processing Systems, 36, 2024.   \n[41] Arthur Jacot, Franck Gabriel, and Cl\u00e9ment Hongler. Neural tangent kernel: Convergence and generalization in neural networks. Advances in Neural Information Processing Systems, 31, 2018.   \n[42] Olav Kallenberg. Probabilistic Symmetries and Invariance Principles. Springer, Dordrecht, 2005.   \n[43] Olav Kallenberg. Random Measures, Theory and Applications, volume 77. Springer, 01 2017.   \n[44] Risi Kondor and Shubhendu Trivedi. On the generalization of equivariance and convolution in neural networks to the action of compact groups. In International conference on machine learning, pages 2747\u20132755. PMLR, 2018.   \n[45] Leon Lang and Maurice Weiler. A wigner-eckart theorem for group equivariant convolution kernels. In International Conference on Learning Representations, 2020.   \n[46] Zhiyuan Li, Ruosong Wang, Dingli Yu, Simon S Du, Wei Hu, Ruslan Salakhutdinov, and Sanjeev Arora. Enhanced convolutional neural tangent kernels. arXiv preprint arXiv:1911.00809, 2019.   \n[47] Pierre Luis Lions. Cours au College de France. 2008.   \n[48] Clare Lyle, Mark van der Wilk, Marta Kwiatkowska, Yarin Gal, and Benjamin Bloem-Reddy. On the benefits of invariance in neural networks. arXiv preprint arXiv:2005.00178, 2020.   \n[49] Haggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant and equivariant graph networks. arXiv preprint arXiv:1812.09902, 2018.   \n[50] Haggai Maron, Ethan Fetaya, Nimrod Segol, and Yaron Lipman. On the universality of invariant networks. In International conference on machine learning, pages 4363\u20134371. PMLR, 2019.   \n[51] Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit. In Conference on Learning Theory, pages 2388\u20132464. PMLR, 2019.   \n[52] Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Learning with invariances in random features and kernel models. In Conference on Learning Theory, pages 3351\u20133418. PMLR, 2021.   \n[53] Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-layer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665\u2013 E7671, 2018.   \n[54] Sylvie M\u00e9l\u00e9ard. Asymptotic behaviour of some interacting particle systems; McKean-Vlasov and Boltzmann models, pages 42\u201395. Springer Berlin Heidelberg, Berlin, Heidelberg, 1996.   \n[55] Phan-Minh Nguyen and Huy Tuan Pham. A rigorous framework for the mean field limit of multilayer neural networks. Mathematical Statistics and Learning, 6(3):201\u2013357, 2023.   \n[56] Tomohiro Nishiyama. Convex optimization on functionals of probability densities. arXiv preprint arXiv:2002.06488, 2020.   \n[57] Atsushi Nitanda, Denny Wu, and Taiji Suzuki. Convex analysis of the mean field langevin dynamics. In International Conference on Artificial Intelligence and Statistics, pages 9741\u20139757. PMLR, 2022.   \n[58] F. Otto and C. Villani. Generalization of an inequality by talagrand and links with the logarithmic sobolev inequality. Journal of Functional Analysis, 173(2):361\u2013400, 2000.   \n[59] Mircea Petrache and Shubhendu Trivedi. Approximation-generalization trade-offs under (approximate) group equivariance. Advances in Neural Information Processing Systems, 36, 2024.   \n[60] Siamak Ravanbakhsh. Universal equivariant multilayer perceptrons. In Hal Daum\u00e9 III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 7996\u20138006. PMLR, 13\u201318 Jul 2020.   \n[61] Siamak Ravanbakhsh, Jeff Schneider, and Barnabas Poczos. Equivariance through parametersharing. In International conference on machine learning, pages 2892\u20132901. PMLR, 2017.   \n[62] Grant Rotskoff and Eric Vanden-Eijnden. Trainability and accuracy of artificial neural networks: An interacting particle system approach. Communications on Pure and Applied Mathematics, 75(9):1889\u20131935, jul 2022.   \n[63] Filippo Santambrogio. Optimal transport for applied mathematicians. Birk\u00e4user, NY, 55(58- 63):94, 2015.   \n[64] John Shawe-Taylor. Threshold network learning in the presence of equivalences. In J. Moody, S. Hanson, and R.P. Lippmann, editors, Advances in Neural Information Processing Systems, volume 4. Morgan-Kaufmann, 1991.   \n[65] John Shawe-Taylor. Sample sizes for threshold networks with equivalences. Information and Computation, 118(1):65\u201372, 1995.   \n[66] Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks: A central limit theorem. Stochastic Processes and their Applications, 130(3):1820\u20131852, 2020.   \n[67] Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks: A law of large numbers. SIAM Journal on Applied Mathematics, 80(2):725\u2013752, 2020.   \n[68] Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of deep neural networks. Mathematics of Operations Research, 47(1):120\u2013152, 2022.   \n[69] Taiji Suzuki, Denny Wu, and Atsushi Nitanda. Convergence of mean-field langevin dynamics: time-space discretization, stochastic gradient, and variance reduction. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[70] Alain-Sol Sznitman. Topics in propagation of chaos. In Paul-Louis Hennequin, editor, Ecole d\u2019Et\u00e9 de Probabilit\u00e9s de Saint-Flour XIX \u2014 1989, pages 165\u2013251, Berlin, Heidelberg, 1991. Springer Berlin Heidelberg.   \n[71] C\u00e9dric Villani. Optimal transport. Old and new, volume 338 of Grundlehren der mathematischen Wissenschaften. Springer-Verlag, Berlin, 2009.   \n[72] Rui Wang, Elyssa Hofgard, Han Gao, Robin Walters, and Tess E. Smidt. Discovering symmetry breaking in physical systems with relaxed group convolution. In Proceedings of the 41st international conference on machine learning (ICML2024), 2024.   \n[73] Maurice Weiler and Gabriele Cesa. General e (2)-equivariant steerable CNNs. Advances in neural information processing systems, 32, 2019.   \n[74] Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In Proceedings of the 28th international conference on machine learning (ICML2011), 2011.   \n[75] Jeffrey Wood and John Shawe-Taylor. Representation theory and invariant neural networks. Discrete Applied Mathematics, 69(1):33\u201360, 1996.   \n[76] Dmitry Yarotsky. Universal approximations of invariant maps by neural networks. Constructive Approximation, 55(1):407\u2013474, 2022.   \n[77] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexander J Smola. Deep sets. Advances in Neural Information Processing Systems, 30, 2017. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "table", "img_path": "L86glqNCUj/tmp/2950e973352dcc82900c8682ac43029e1e44f4b83664fae5c39af9e518a76470.jpg", "table_caption": ["Table 1: Summary the main abbreviations employed throughout the paper Abbreviation Meaning "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "A General Considerations for the Reader ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "This section presents a summary of recurrent notation, abbreviations and key concepts used in our work, as well as a discussion on its limitations, scope and possible extensions. We thank anonymous Reviewers for suggesting us to add this section to the original manuscript. ", "page_idx": 15}, {"type": "text", "text": "A.1 Summary of recurrent notation, abbreviations and key concepts ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section we summarize the main abbreviations and notation employed throughout the body of the paper, as well as simple definitions of fundamental concepts from probability theory and algebra that might be useful for understanding our work ", "page_idx": 15}, {"type": "text", "text": "Table 1 serves as a glossary for the most used abbreviations in the body of the paper. Table 2 contains a summary of the notation that we recurrently use in our definitions, statements and proofs. It also provides some simple references to mathematical concepts that are key in our work. ", "page_idx": 15}, {"type": "text", "text": "For clarity, beyond the contents of both tables, we here also explain a few relevant concepts to the unfamiliar reader: ", "page_idx": 15}, {"type": "text", "text": "\u2022 Bochner Integrals: These correspond to the right generalization of Lebesgue integrals for vector-valued functions (see [24] for further reference). Say we have a function $f\\ :\\ x\\ \\rightarrow\\ y$ between Hilbert spaces, and such that $\\pi_{\\mathcal{X}}$ is some measure on $\\mathcal{X}$ , then we say $f$ is square-integrable (in Bochner sense) if it satisfies: $\\begin{array}{r}{\\int_{\\mathcal{X}}\\|f(x)\\|_{\\mathcal{Y}}^{2}d\\pi_{\\mathcal{X}}(x)<\\infty}\\end{array}$ . This integral also respects closed linear operators, as shown by Hille\u2019s theorem (see Theorem II.2.6 in [24]). Namely, if $L\\,:\\,\\mathcal{Y}\\,\\rightarrow\\,\\mathcal{Y}$ is a closed linear operator over $\\boldsymbol{\\wp}$ , we have: $\\begin{array}{r}{\\int_{\\mathcal X}L.f d\\pi_{\\mathcal X}=L.\\int_{\\mathcal X}f\\dot{d\\pi}_{\\mathcal X}}\\end{array}$ . In particular, this also holds for bounded linear operators. In general, most of the basic and most intuitive properties of traditional integrals can also be expressed for Bochner integrals.   \n\u2022 Compact Groups, Group Actions and the Haar measure: We recurrently talk about group actions via orthogonal representations throughout our work, so a due clarification may be required. We assume $G$ to be a compact group. Namely, $G$ has a topology that makes it compact, and so that the multiplication and inversion operations are continuous. We say that $G$ acts on $\\mathcal{Z}$ , which we denote $G\\operatorname{\\mathrm{\\scriptscriptstyle{C}}}\\operatorname{\\mathcal{Z}}$ , whenever there exists a map: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{T:G\\times{\\mathcal{Z}}\\to{\\mathcal{Z}}}}\\\\ {{(g,z)\\mapsto T(g,z)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "that satisfies $T(e_{G},z)=z$ and $T(g_{1},T(g_{2},z))=T(g_{1}.g_{2},z),\\;\\forall g_{1},g_{2}\\in G,\\;\\forall z\\in\\mathcal{Z}$ . We always assume the actions to be continuous; namely, $T$ is continuous with respect to the product topology. Alternatively, by denoting $T_{g}:={\\dot{T}}(g,\\cdot),T:g\\in G\\mapsto T_{g}\\in{\\bar{\\mathrm{Sym}}}({\\mathcal{Z}})$ is a group homomorphism and, if the action is continuous, $T_{g}$ is an homeomorphism $\\forall g\\in G$ . If we further assume that $\\forall g\\in G,\\;T_{g}$ is linear, we call $T.$ \u00b7 a group representation. Further, if $\\{T_{g}\\}_{g\\in G}$ are orthogonal transformations, we call $T.$ an orthogonal group representation ", "page_idx": 15}, {"type": "text", "text": "Table 2: Summary the notation employed throughout the paper Notation Represented Object ", "page_idx": 16}, {"type": "table", "img_path": "L86glqNCUj/tmp/3852385a1206747fa485bb1cc7e48f6fcf3476d7ab56b5dd2b15b0d42d93fc95.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "and denote the group action by $G\\operatorname{C}_{T}\\mathcal{Z}$ . This is the case for all of the $G$ -actions considered throughout this work. Working only with group representations is common-practice in the context of symmetries for NNs (see e.g. [13, 48]). Beyond NNs, a whole field of mathematics is of course dedicated to the study of group representations (see e.g. [36] for reference). In this work we borrow some of this theory\u2019s terminology, as we refer to equivariant linear maps also as intertwining maps or intertwiners. This is also why we refer to the layer-by-layer action in EAs for shallow NNs as an intertwining action (see e.g. SuppMat-C.1). Finally, it is well known that a compact group $G$ admits a unique normalized Haar measure $\\lambda_{G}\\in{\\dot{\\mathcal{P}}}(G)$ (see [25]), which is left and right invariant, finite on every compact set, outer regular on Borel sets and inner regular on open sets. It can be interpreted as the uniform distribution on $G$ , and it is extensively used throughout this work. For further references in the topic, the curious reader might be interested in [25, 42, 43], among many others. \u2022 Weak convergence of measures: We also recall one of the most used notions of convergence in a space of probability measures. Given a sequence $(\\mu_{n})_{n\\in\\mathbb{N}}\\subseteq{\\mathcal{P}}({\\mathcal{Z}})$ , we say it weakly converges to some point $\\mu\\in{\\mathcal{P}}({\\mathcal{Z}})$ if, for any continuous and bounded function $f:\\mathcal{Z}\\to\\mathbb{R}$ , we have $\\left\\langle f,\\mu_{n}\\right\\rangle\\xrightarrow[n\\rightarrow\\infty]{}\\left\\langle f,\\mu\\right\\rangle$ . Notice that this type of convergence is weaker than $W_{p}$ convergence for $p\\geq1$ (which additionally also requires the convergence of $p$ -th order moments of the involved measures). Notice how we write $\\langle f,\\mu\\rangle$ to denote $\\textstyle\\int_{\\mathcal{Z}}f d\\mu$ . This notation is heavily used (and abused) throughout the core of our work. ", "page_idx": 17}, {"type": "text", "text": "A.2 Limitations, scope and possible extensions of our work ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "For convenience of the reader we here present a discussion of some limitations and of the scope of application of the present work. This subsection does not provide any mathematical results required for the sequel. ", "page_idx": 17}, {"type": "text", "text": "Some of these limitations are of technical nature, and regard specific assumptions made in order for the specific proofs of our theoretical results to hold. In absence of these conditions, or under less restrictive ones, some of our results (or weaker forms of them) might still hold true, but further research is needed in order to properly establish their validity. Other limitations are the object of more general research questions in this area. ", "page_idx": 17}, {"type": "text", "text": "\u2022 On the infinite i.i.d. data sample: The MF theory of NN makes the assumption that it is possible to take an infinite i.i.d. sample from the data distribution $\\pi$ . This may appear as a limitation of our results, since real-world datasets are naturally finite. We acknowledge that this is in fact an abstraction, but it can nevertheless be a potentially good approximation of the behavior of the SGD algorithm when large datasets are available. Indeed, Theorem 1, which holds for $\\varepsilon_{N}=o(1/N)$ , ensures that a sample size of the order of $t N$ is required in order to approximate the WGF up to a (\u2018macroscopic\u2019) time $t$ . When the long-time convergence of the WGF can be effectively quantified (an active research question today, see e.g. [57] and references therein), we can estimate how large a data sample will be required in order to attain a predetermined generalization error level. On the other hand, even for $\\pi$ with finite support, the MF approximation will end up minimizing the empiricalrisk. This, as pointed out in Section 2.1, is not the same as minimizing generalization error w.r.t. some underlying data distribution, but it is widely interpreted (in the application of most NN-based machine learning algorithms) as a proxy of doing so. Once again, this approximation can be reasonably good when the data sample size is large enough. \u2022 On the infinite width of NN models: Despite the MFL being a theoretical tool (which requires that the width $N$ goes to $+\\infty,$ ), it is still the asymptotic regime that most closely describes the feature-learning behavior of large NNs during training (see the discussion at the end of Section 2.2). We believe that truly useful insights can be obtained from it for real, moderately wide NNs. For instance, our experiments show that, in quite reasonable practical settings of shallow NNs (with standard pointwise sigmoid activation and objax\u2019 default SGD training), the predicted MF behavior is seen to emerge in practice, already for finite, not too large $N$ (1000 was generally enough). Actually, most of the insights obtained from the ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "MF analysis of NN are, in fact, unaccessible from a fixed $N$ standpoint (as is the case for Theorem 5). Further non-asymptotic and quantitative answers to practical questions (e.g. how many neurons are needed in order to attain a given level of generalization/population error at given computational cost?) could also be obtained from MF theory, via quantitative propagation of chaos results (see e.g. [12]). ", "page_idx": 18}, {"type": "text", "text": "\u2022 On Assumption 1: This is the main assumption underlying many of our core results. Although it is generally simple and not excessively hard to satisfy in practice, it might seem constraining in the context of neural networks. In particular, the technical condition on the gradient of $\\ell$ , as well as the boundedness of $\\sigma_{*}$ , could seem to limit the applicability of our results. However, these conditions can be replaced by alternative properties of the data distribution (e.g. that $\\pi$ has compact support, or finite moments up to a given degree). Similarly, lifting some of the rest of the technical assumptions required for establishing the MFL, is part of the ongoing research work in the field (see e.g. [12] or [38] for some alternative conditions). ", "page_idx": 18}, {"type": "text", "text": "Regarding the assumption that $\\ell$ is $G$ -invariant, it is known that traditional loss functions naturally satisfy this condition. The joint-equivariance of $\\sigma_{*}$ , on the other hand, is much less constraining than it may initially seem. In practice, depending on the choice of $\\sigma_{*}$ and $M$ , it might even end up being a trivial constraint. A deeper discussion on this very assumption shall be found in SuppMat-C.1. ", "page_idx": 18}, {"type": "text", "text": "\u2022 On the generality of shallow models $\\Phi_{\\mu}\\in\\mathcal{F}_{\\sigma_{\\ast}}(\\mathcal{P}(\\mathcal{Z}))$ : These allow for modeling a wide range of situations (including some variants of multi-layer models). In fact, $\\sigma_{*}$ can by itself encode a complex deep architecture (see SuppMat-C.1) and the resulting shallow model can represent way more interesting structures than single-hidden-layer NNs (e.g. ensembles of such multilayer \u201cunits\u201d trained in parallel). Nevertheless, these shallow models are still far from including all possibilities in the context of NNs. In fact, a fully unified, satisfactory MF theory for general deep NNs is still an open, actively tackled question (for advancements on it see e.g. [4, 55, 68]). We believe that some of our key results (e.g. Theorems 3 and 4) can be extended to some of those settings, which is a question we will leave for future work. ", "page_idx": 18}, {"type": "text", "text": "\u2022 Universality guarantees for shallow models $\\Phi_{\\mu}\\in\\mathcal{F}_{\\sigma_{\\ast}}(\\mathcal{P}(\\mathcal{Z}))\\colon$ In this work, we have only explored the simple setting of tensor-of-order- $^{\\,I}$ NNs as modelled through $\\sigma_{*}$ , see SuppMatC.1. In particular, it is known from [49] that the desired universality on equivariant models is not always possible with these kinds of networks (which is, of course, a limitation to the applicability of Proposition 5). Exploring other interesting situations that could be modeled with our current framework, or plainly reformulating it to account for new variants of NNs (e.g. high-order tensor NNs thay might allow for easier EA universality) is undoubtedly part of our future work. See SuppMat- E.2.5 for a related discussion. ", "page_idx": 18}, {"type": "text", "text": "\u2022 On $G$ -equivariant data distributions: The assumption of $\\pi$ being $G$ -equivariant implies that $\\pi_{\\mathcal{X}}$ is $G$ -invariant as well [6]. This implication can seem a bit too restrictive in some settings: e.g. for image classification, it amounts to assuming that \u2018images can arrive with any possible orientation\u2019 at training time, which is not necessarily the case. However, as extensively discussed in the literature (see [13, 27, 30, 48]), assuming $\\pi_{\\mathcal{X}}$ to be $G$ -invariant is usually reasonable when the aim is to \u2018exploit symmetries\u2019 of the problem. Not having such assumption means that there are little to no properties from the data that can be exploited in a proof. On the other hand, as mentioned in remarks after Theorems 3 and 5, our proofs don\u2019t explicitly rely on having $\\pi\\in\\mathcal P^{G}(\\boldsymbol{\\mathcal{X}}\\times\\mathcal{Y})$ , but rather on the risk functional $R:\\mathcal{P}(\\mathcal{Z})\\to\\mathbb{R}$ being $G$ -invariant. In consequence, one could simply neglect the $G$ -equivariance of $\\pi$ altogether, and introduce the implied inductive bias by applying DA (which forces the marginal on $\\mathcal{X}$ to be $G$ -invariant), or any other SL technique that achieves the same result. ", "page_idx": 18}, {"type": "text", "text": "\u2022 On the numerical experiments: The suite of experiments that were presented in Section 4 and SuppMat-F, though quite insightful for our theoretical results, are quite limited in reach. In particular, finding ways to illustrate our theoretical results on less controlled settings, such as a real-world equivariant datasets, will be a key part of our future work. ", "page_idx": 18}, {"type": "text", "text": "The presented experiments come from a controlled setting, mainly looking to avoid heavy practical constraints (e.g. visualizing the huge parameter space of a NN trained over an image dataset can be exceedingly hard), as well as an increased complexity of the involved objects (e.g. even for finite groups, group actions can get severely more complicated than what we experienced). Due to our currently limited computational resources, we leave these inquiries for future work. Similarly, taking our experiments to a larger scale (e.g. $N=100.000)$ is also a challenge left for future inquiries (our current results, with $N\\leq5000$ , might still be considered as small $N$ ). A significant problem for scaling our experimental verifications is the fact that we relied on calculating Wasserstein distances (which is usually really computationally-expensive) to provide rigorous numerical evidence of our theoretical findings. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "Furthermore, we are yet to experiment with architectures that are compatible with infinite compact groups $G$ ; namely for examples coming from physics and NeuralODEs. Different variants of the activation function should also be tested out (e.g. a ReLU, tanh, and many others), as well as variants of the optimization algorithm (e.g. Adam, RMSProp, etc.). ", "page_idx": 19}, {"type": "text", "text": "Finally, our heuristic needs to be tested on a larger scale, with more complex datasets and architectures. Also, theoretical guarantees to sustain it shall be provided in future work. We believe that similar arguments as in [72] could be developed for our case; and alternative approaches could be based on understanding the support properties of the McKean-Vlasov SDE studied in Appendix E.3.2. ", "page_idx": 19}, {"type": "text", "text": "\u2022 On possible variants of the training dynamic: In this work we focus mostly on the \u2018traditional\u2019 WGF learning dynamics, without delving much into other interesting possible variants of the training process. This might be seen as a possible limitation to the applicability of our work. ", "page_idx": 19}, {"type": "text", "text": "Firstly, the decision to work with the usual WGF follows from the standards set by [16, 53, 62, 67] among many others. Beyond this fact, we believe that results such as Theorems 3 and 4 are somewhat \u2018natural\u2019, and that they should hold regardless of small differences in the training dynamics. ", "page_idx": 19}, {"type": "text", "text": "For instance, we know that our proofs for Theorems 3 and 4 work straightforwardly in the setting of [22], where the MFL is established even with a fixed learning rate that does not necessarily decrease with $N$ . For large fixed learning rates, this might shed some light onto the MF behaviour of symmetries under \u2018Edge of Stability\u2019 (EoS) dynamics. Similarly, we believe some of our more \u2018natural\u2019 results to be applicable as well for Wasserstein sub-Gradient Flows [16], Underdamped Dynamics [31], Annealed Dynamics [15], among many others. In contrast, Theorems 5 and 6, which involve stronger notions of symmetry, don\u2019t seem immediate to generalize to many other kinds of asymptotic dynamics. Studying the applicability of our results under different variants of the training process, is surely an interesting question to be tackled as part of our future work. ", "page_idx": 19}, {"type": "text", "text": "B Symmetries in functions, measures, data and shallow models ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section we state some useful, basic results on the effect of symmetries acting on measures, functions and data, that will be used in the sequel. We also explain how symmetries of interest can be incorporated into the generalized shallow NN setting from Definition 1, complementing also the discussions given in Section 2.1 and Section 2.3 in that regard. ", "page_idx": 19}, {"type": "text", "text": "Recall $\\mathcal{X},\\mathcal{Y}$ and $\\mathcal{Z}$ are (separable) Hilbert Spaces and $G$ a compact group with Haar measure $\\lambda_{G}$ , such that $G\\subset_{\\rho}\\mathcal{X},G\\subset_{\\hat{\\rho}}\\mathcal{Y}$ and $G\\operatorname{\\bigcirc}\\!_{M}{\\mathcal{Z}}$ . Also, let $\\mathcal{E}^{G}\\subseteq\\mathcal{Z}$ be the linear subspace of parameters that are fixed points of the action of $G$ over $\\mathcal{Z}$ , and $P_{\\mathcal{E}^{G}}$ the orthogonal projection onto it. ", "page_idx": 19}, {"type": "text", "text": "B.1 Differentials and integrals of equivariant functions ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The following lemma characterizes the differential of jointly equivariant functions with respect to the action of some group $G$ . Here we can assume $G$ be a lcsH group w.l.o.g. and consider representations that aren\u2019t necessarily orthogonal (we denote them differently to avoid confusion). ", "page_idx": 19}, {"type": "text", "text": "Proposition 6. Let $G\\operatorname{C}_{\\chi}\\,\\mathcal{X}$ , $G\\operatorname*{\\bigcirc}_{\\tilde{\\chi}}\\,\\mathcal{Z}$ , $G\\subset_{\\check{\\boldsymbol{x}}}\\mathcal{V}$ via some representations $\\chi,\\tilde{\\chi}$ and $\\check{\\chi}$ respectively (not necessarily orthogonal). Let $\\boldsymbol{\\dot{f}}:\\mathcal{X}\\times\\dot{\\mathcal{Z}}\\rightarrow\\mathcal{Y}$ be jointly $G$ -equivariant with respect to these actions (i.e. $\\forall g\\in G,\\,\\forall x\\in\\mathcal{X}$ , $\\forall z\\in{\\mathcal{Z}}$ , $\\check{\\chi}_{g}.f(x,z)=f(\\chi_{g}.x,\\tilde{\\chi}_{g}.z)\\prime$ and Fr\u00e9chet-differentiable on its first argument. Then: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\forall g\\in G,\\;\\forall x\\in\\mathcal{X},\\;\\forall z\\in\\mathcal{Z},\\;\\;D_{x}f(\\chi_{g}.x,\\tilde{\\chi}_{g}.z)=\\check{\\chi}_{g}.D_{x}f(x,z)\\chi_{g}^{-1}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof of Proposition $6$ . Indeed, we know that $\\forall z\\in\\mathcal{Z}\\ D_{x}f(\\cdot,z)$ is the unique function that satisfies, $\\forall\\widetilde{x}\\in\\mathcal{X}$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{h\\to0}{\\frac{\\|f({\\widetilde{x}}+h,z)-f({\\widetilde{x}})-D_{x}f({\\widetilde{x}},z)h\\|}{\\|h\\|}}=0\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since we want to prove that $\\forall\\widetilde{x}\\in\\mathcal{X},\\forall z\\in\\mathcal{Z}$ , $\\forall g\\in G:D_{x}f(\\chi_{g}.{\\tilde{x}},{\\tilde{\\chi}}_{g}.z)={\\check{\\chi}}_{g}D_{x}f({\\tilde{x}},z)\\chi_{g}^{-1}$ , it will be enough to check that: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{h\\to0}\\frac{\\|f(\\chi_{g}.\\tilde{x}+h,\\tilde{\\chi}_{g}z)-f(\\chi_{g}.\\tilde{x},\\tilde{\\chi}_{g}.z)-\\check{\\chi}_{g}D_{x}f(\\tilde{x},z)\\chi_{g}^{-1}h\\|}{\\|h\\|}=0\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which by uniqueness implies the result. Thanks to the joint equivariance of $f$ , we have $\\forall h\\neq0$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\|f(\\chi_{g}.\\tilde{x}+h,\\tilde{\\chi}_{g}z)-f(\\chi_{g}.\\tilde{x},\\tilde{\\chi}_{g}.z)-\\tilde{\\chi}_{g}D_{x}f(\\tilde{x},z)\\chi_{g}^{-1}h\\|}{\\|h\\|}}\\\\ &{=\\frac{\\|f(\\chi_{g}.(\\tilde{x}+\\chi_{g}^{-1}.h),\\tilde{\\chi}_{g}z)-f(\\chi_{g}.\\tilde{x},\\tilde{\\chi}_{g}.z)-\\tilde{\\chi}_{g}D_{x}f(\\tilde{x},z)\\chi_{g}^{-1}h\\|}{\\|h\\|}}\\\\ &{=\\frac{\\|\\tilde{\\chi}_{g}.f(\\tilde{x}+\\chi_{g}^{-1}.h,z)-\\tilde{\\chi}_{g}.f(\\tilde{x},z)-\\tilde{\\chi}_{g}D_{x}f(\\tilde{x},z)\\chi_{g}^{-1}h\\|}{\\|h\\|}}\\\\ &{=\\frac{\\|\\tilde{\\chi}_{g}.\\left[f(\\tilde{x}+\\chi_{g}^{-1}.h,z)-f(\\tilde{x},z)-D_{x}f(\\tilde{x},z)(\\chi_{g}^{-1}h)\\right]\\|}{\\|\\chi_{g}.\\chi_{g}\\|}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now, recall that for every $g\\ \\in\\ G$ , the operator $\\check{\\chi}_{g}$ is bounded, i.e. it has finite operator norm $0<\\|\\check{\\chi}_{g}\\|<\\infty$ (non-zero as $\\check{\\chi}_{g}$ is invertible). By defining $\\tilde{h}:=\\chi_{g}^{-1}.h$ , we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\|\\check{\\chi}_{g}.\\left[f(\\tilde{x}+\\tilde{h},z)-f(\\tilde{x},z)-D_{x}f(\\tilde{x},z)\\tilde{h}\\right]\\|}{\\|\\chi_{g}\\tilde{h}\\|}\\leq\\frac{\\|\\check{\\chi}_{g}\\|\\|f(\\tilde{x}+\\tilde{h},z)-f(\\tilde{x},z)-D_{x}f(\\tilde{x},z)\\tilde{h}\\|}{\\|\\chi_{g}.\\tilde{h}\\|}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Multiplying by $\\begin{array}{r}{1=\\frac{\\|\\boldsymbol{\\chi}_{g}^{-1}\\boldsymbol{\\chi}_{g}\\tilde{\\boldsymbol{h}}\\|}{\\|\\tilde{h}\\|}}\\end{array}$ the last term is seen to be bounded by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|\\check{\\chi}_{g}\\|\\|\\chi_{g}^{-1}\\|\\cdot\\frac{\\|f(\\tilde{x}+\\tilde{h},z)-f(\\tilde{x},z)-D_{x}f(\\tilde{x},z)\\tilde{h}\\|}{\\|\\tilde{h}\\|}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since $\\chi_{g}$ and $\\chi_{g}^{-1}$ are bounded operators, we have that: $h\\to0\\iff\\tilde{h}=\\chi_{g}^{-1}h\\to0$ . Thus ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{h\\rightarrow0}{\\operatorname*{lim}}\\frac{\\Vert f(\\chi_{g}.\\tilde{x}+h,\\tilde{\\chi}_{g}z)-f(\\chi_{g}.\\tilde{x},\\tilde{\\chi}_{g}.z)-\\check{\\chi}_{g}D_{x}f(\\tilde{x},z)\\chi_{g}^{-1}h\\Vert}{\\Vert h\\Vert}}\\\\ &{\\leq\\underset{h\\rightarrow0}{\\operatorname*{lim}}\\Vert\\check{\\chi}_{g}\\Vert\\Vert\\chi_{g}^{-1}\\Vert\\cdot\\frac{\\Vert f(\\tilde{x}+\\tilde{h},z)-f(\\tilde{x},z)-D_{x}f(\\tilde{x},z)\\tilde{h}\\Vert}{\\Vert\\tilde{h}\\Vert}=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This, in particular, allows us to characterize the differential of equivariant functions. ", "page_idx": 20}, {"type": "text", "text": "Corollary 4. If $G\\subset_{\\chi}\\chi,G\\subset_{\\tilde{\\chi}}\\y_{\\tilde{\\,}}$ , and $f:\\mathcal X\\to\\mathcal Y$ is a $G$ -equivariant and Fr\u00e9chet-differentiable function, then: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\forall g\\in G,\\;\\forall x\\in\\mathcal{X},\\;\\;D_{x}f(\\chi_{g}.x)=\\tilde{\\chi}_{g}D_{x}f(x)\\chi_{g}^{T}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof of Corollary 4. Direct. ", "page_idx": 20}, {"type": "text", "text": "We can also get some interesting integral properties of jointly equivariant functions. ", "page_idx": 20}, {"type": "text", "text": "Proposition 7. Let $\\mathcal{X},\\mathcal{Y}$ and $\\mathcal{Z}$ be (separable) Hilbert Spaces and $G$ be a lcsH group. Let $G\\operatorname{C}_{\\chi}\\,\\mathcal{X}$ , $G\\subset_{\\tilde{\\chi}}\\mathcal{Z},\\,G\\subset_{\\check{\\chi}}\\mathcal{D}$ via some representations $\\chi,\\tilde{\\chi}$ and $\\check{\\chi}$ respectively (not necessarily orthogonal). Let $f:\\mathcal{X}\\times\\mathcal{Z}\\to\\mathcal{Y}$ be a jointly $G$ -equivariant function (with respect to these actions). Consider a measure $\\mu\\in{\\mathcal{P}}({\\mathcal{Z}})$ and let $f$ be Bochner integrable on its second argument with respect to $\\mu$ . Then: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\forall x\\in\\mathcal{X},\\;\\forall g\\in G,\\;\\check{\\chi}_{g}\\langle f(x;\\cdot),\\mu\\rangle=\\langle f(\\chi_{g}x;\\cdot),\\tilde{\\chi}_{g}\\#\\mu\\rangle\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof of Proposition 7. Let $\\mu\\in{\\mathcal{P}}({\\mathcal{Z}})$ and $f$ be as stated. Notice that, $\\forall x\\in\\mathcal{X},\\;\\forall g\\in G,$ , we have: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\check{\\chi}_{g}\\langle f(x,\\cdot),\\mu\\rangle=\\check{\\chi}_{g}\\int_{\\mathcal{Z}}f(x,\\theta)d\\mu(\\theta)=\\int_{\\mathcal{Z}}\\check{\\chi}_{g}f(x,\\theta)d\\mu(\\theta),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where we\u2019ve used the linearity of the Bochner integral under continuous linear operators (as is $\\check{\\chi}_{g}$ ). It follows, from the joint $G$ -equivariance of $f$ and the definition of the pushforward measure, that: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\int_{\\mathcal{Z}}\\check{\\chi}_{g}f(x,\\theta)d\\mu(\\theta)=\\int_{\\mathcal{Z}}f(\\chi_{g}x,\\tilde{\\chi}_{g}\\theta)d\\mu(\\theta)=\\int_{\\mathcal{Z}}f(\\chi_{g}x,\\tilde{\\theta})d(\\tilde{\\chi}_{g}\\#\\mu)(\\tilde{\\theta})\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We conclude the desired result. ", "page_idx": 21}, {"type": "text", "text": "B.2 Properties of symmetric measures ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Consider again compact $G$ acting orthogonally over the different spaces. Recall that, given $\\mu\\in{\\mathcal{P}}({\\mathcal{Z}})$ , we defined $\\begin{array}{r}{\\mu^{G}:=\\int_{G}(M_{g}\\#\\mu)d\\lambda_{G}}\\end{array}$ as its symmetrized version and $\\mu^{\\varepsilon^{G}}:=P_{\\varepsilon^{G}}\\#\\mu$ as its projected version. The following two results assumed in the core of the paper are elementary, but we provide their detailed proofs for completeness: ", "page_idx": 21}, {"type": "text", "text": "Lemma 1. We have the following inclusion: $\\mathcal{P}(\\mathcal{E}^{G})\\,\\subseteq\\,\\mathcal{P}^{G}(\\mathcal{Z})$ . Also, for any $\\mu\\,\\in\\,{\\mathcal{P}}({\\mathcal{Z}})$ the following equalities hold $\\forall g\\in G$ : $\\mu^{\\ensuremath{\\varepsilon}^{G}}=(M_{g}\\#\\mu)^{\\ensuremath{\\varepsilon}^{G}}=(\\mu^{G})^{\\ensuremath{\\varepsilon}^{G}}=(\\mu^{\\ensuremath{\\varepsilon}^{G}})^{G}$ and $(M_{g}\\#\\mu)^{G}=\\mu^{G}$ . ", "page_idx": 21}, {"type": "text", "text": "Proof of Lemma $^{\\,l}$ . Let $\\mu\\in\\mathcal{P}(\\mathcal{E}^{G})$ (i.e. $\\mu(\\mathcal{E}^{G})=1)$ ), $g\\in G$ and consider any positive measurable $f:\\mathcal{Z}\\to\\mathbb{R}$ . We can see: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\int_{\\mathcal{Z}}f(M_{g}z)\\mu(d z)=\\int_{\\mathcal{E}^{G}}f(M_{g}z)\\mu(d z)=\\int_{\\mathcal{E}^{G}}f(z)\\mu(d z)=\\int_{\\mathcal{Z}}f(z)\\mu(d z).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "So, that $\\forall g\\in G$ , $\\mu=M_{g}\\#\\mu$ , and thus $\\mu\\in\\mathcal{P}^{G}(\\mathcal{Z})$ . Regarding the equalities, consider $\\mu\\in{\\mathcal{P}}({\\mathcal{Z}})$ and $A\\in\\mathcal{B}_{\\mathcal{Z}}$ some borel set. Since the $\\lambda_{G}$ is right-invariant, we have $\\forall g\\in G,\\,\\forall z\\in{\\mathcal{Z}}:$ ", "page_idx": 21}, {"type": "equation", "text": "$$\nP_{\\xi^{G}}M_{g}z=\\int_{G}M_{h}(M_{g}z)d\\lambda_{G}(h)=\\int_{G}(M_{h}M_{g}z)d\\lambda_{G}(h)=\\int_{G}M_{\\tilde{h}}z)d\\lambda_{G}(\\tilde{h})=P_{\\xi^{G}}z.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then, for $g\\in G,M_{g}^{-1}P_{\\varepsilon^{G}}^{-1}(A)=(P_{\\varepsilon^{G}}M_{g})^{-1}(A)=(P_{\\varepsilon^{G}})^{-1}(A)$ and so: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\bigl(M_{g}\\#\\mu\\bigr)^{\\ensuremath{\\varepsilon}^{G}}(A)=\\mu\\bigl(M_{g}^{-1}P_{\\ensuremath{\\varepsilon}^{G}}^{-1}(A)\\bigr)=\\mu\\bigl(P_{\\ensuremath{\\varepsilon}^{G}}^{-1}(A)\\bigr)=\\mu^{\\ensuremath{\\varepsilon}^{G}}(A),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and: ", "page_idx": 21}, {"type": "equation", "text": "$$\n(\\mu^{G})^{\\varepsilon^{G}}(A)=\\int_{G}\\mu(M_{g}^{-1}P_{\\varepsilon^{G}}^{-1}(A))d\\lambda_{G}(g)=\\int_{G}\\mu(P_{\\varepsilon^{G}}^{-1}(A))d\\lambda_{G}(g)=\\mu(P_{\\varepsilon^{G}}^{-1}(A))=\\mu^{\\varepsilon^{G}}(A).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "On the other hand, since $\\mu^{\\mathcal{E}^{G}}\\in\\mathcal{P}^{\\mathcal{E}^{G}}(\\mathcal{Z})\\subseteq\\mathcal{P}^{G}(\\mathcal{Z}),(\\cdot)^{G}$ leaves it unchanged: $(\\mu^{\\mathcal{E}^{G}})^{G}=\\mu^{\\mathcal{E}^{G}}$ . ", "page_idx": 21}, {"type": "text", "text": "For the last equality, let $g\\in G$ and $f:\\mathcal{Z}\\to\\mathbb{R}_{+}$ be measurable. We have: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\langle f,(M_{g}\\#\\mu)^{G}\\rangle=\\int_{G}\\langle f,M_{h}\\#(M_{g}\\#\\mu)\\rangle d\\lambda_{G}(h)=\\int_{G}\\langle f,(M_{\\tilde{h}})\\#\\mu\\rangle d\\lambda_{G}(\\tilde{h})=\\langle f,\\mu^{G}\\rangle,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "once again by the right-invariance of $\\lambda_{G}$ . Namely, $(M_{g}\\#\\mu)^{G}=\\mu^{G}$ ", "page_idx": 21}, {"type": "text", "text": "Proposition 8. For $\\mu\\in\\mathcal{P}(\\mathcal{Z}),$ , we have: $\\mu^{G}\\in\\mathcal{P}^{G}(\\mathcal{Z})$ and $\\mu^{\\mathcal{E}^{G}}\\in\\mathcal{P}(\\mathcal{E}^{G})$ . ", "page_idx": 21}, {"type": "text", "text": "Proof of Proposition 8. Indeed, let $h\\in G$ and $B\\in B_{\\mathcal{Z}}$ (borel set of $\\mathcal{Z}$ ), using the properties of $M$ and the left-invariance of $\\lambda_{G}$ , we get that: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mu^{G}(M_{h}^{-1}(B))=\\int_{G}\\mu(M_{g}^{-1}(M_{h}^{-1}(B)))d\\lambda_{G}(g)=\\int_{G}\\mu(M_{\\tilde{g}}^{-1}(B))d\\lambda_{G}(\\tilde{g})=\\mu^{G}(B)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "So, $\\forall g\\in G,\\;\\mu^{G}=M_{g}\\#\\mu^{G}$ , implying that $\\mu^{G}\\in\\mathcal{P}^{G}(\\mathcal{Z})$ . On the other hand, by definition we have (as the projection is surjective) $\\mu^{\\mathcal{E}^{G}}(\\mathcal{E}^{G})=\\mu(P_{\\mathcal{E}^{G}}^{-1}(\\mathcal{E}^{G}))=\\mu(\\mathcal{Z})=1$ , so that $\\mu^{\\mathcal{E}^{G}}\\in\\mathcal{P}(\\mathcal{E}^{G})$ . ", "page_idx": 21}, {"type": "text", "text": "Remark. It\u2019s not hard to notice that, on $\\mathcal{Z}\\,=\\,\\mathbb{R}^{D}$ and with $\\lambda$ being the lebesgue measure, if $\\mu\\,\\in\\,\\mathcal{P}(\\mathcal{Z})$ is such that $\\mu\\ll\\lambda$ and has density $u:\\mathcal{Z}\\to\\mathbb{R}_{+}$ , then: $\\bar{\\mu}^{G}\\in\\mathcal{P}^{G}(\\mathcal{Z})$ has density $\\begin{array}{r}{\\boldsymbol{u}^{G}:=\\int_{G}\\boldsymbol{u}\\circ M_{g}d\\lambda_{G}(g)}\\end{array}$ w.r.t. $\\lambda$ (whereas $\\mu^{\\mathcal{E}^{G}}$ doesn\u2019t have a density w.r.t. $\\lambda$ unless the action is trivial). This follows from the $O(D)$ -invariance of $\\lambda$ and some standard calculations. ", "page_idx": 22}, {"type": "text", "text": "Since we will be working on $\\mathcal{P}_{2}(\\mathcal{Z})$ on Section 3.3, it\u2019s useful to also notice that: ", "page_idx": 22}, {"type": "text", "text": "Remark. If $\\mu\\in\\mathcal P_{p}(\\mathcal{Z})$ , then $\\mu^{G},\\mu^{\\mathcal{E}^{G}}\\in\\mathcal{P}_{p}(\\mathcal{Z})$ . Indeed, it follows from the fact that $\\|M_{g}\\|\\leq1$ $\\forall g\\in G$ (since the representation is orthogonal) and $\\|P_{\\mathcal{E}^{G}}\\|\\leq1$ (since $P_{\\mathcal{E}^{G}}$ is a projection). ", "page_idx": 22}, {"type": "text", "text": "Also, we have that: ", "page_idx": 22}, {"type": "text", "text": "Proposition 9. $\\mathcal{P}_{p}(\\mathcal{E}^{G}):=\\mathcal{P}(\\mathcal{E}^{G})\\cap\\mathcal{P}_{p}(\\mathcal{Z})$ and $\\mathscr{P}_{p}^{G}(\\mathcal{Z}):=\\mathscr{P}^{G}(\\mathcal{Z})\\cap\\mathscr{P}_{p}(\\mathcal{Z})$ are closed and convex subspaces of $\\mathcal{P}_{p}(\\mathcal{Z})$ (under the topology induced by $W_{p}$ ). ", "page_idx": 22}, {"type": "text", "text": "Proof of Proposition 9. Convexity is direct by definition of the involved spaces. For closedness under the Wasserstein topology, recall (see e.g. [1]) that Wp(\u00b5n, \u00b5) \u2212n\u2212\u2192\u2212\u2212\u221e\u21920 is equivalent to: \u00b5n \u2212n\u2212\u2192\u2212\u2212\u221e\u21c0\u00b5 (weak convergence) and $\\begin{array}{r}{\\int_{\\mathcal{Z}}\\|\\theta\\|^{p}d\\mu_{n}(\\theta)\\xrightarrow[n\\to\\infty]{}\\int_{\\mathcal{Z}}\\|\\theta\\|^{p}d\\mu(\\theta)}\\end{array}$ . Since for $f\\in C_{b}(\\mathcal{Z})$ , $f\\circ M_{g}$ (for $g\\,\\in\\,G)$ is continuous and bounded, $(\\mu_{n})_{n\\in\\mathbb{N}}\\,\\subseteq\\,\\mathcal{P}^{G}(\\mathcal{Z})$ implies $\\forall g\\,\\in\\,G$ , $M_{g}\\#\\mu\\,=\\,\\mu$ (namely, $\\mu\\in\\mathcal{P}^{G}(\\mathcal{Z}))$ . Similarly, $f\\circ P_{\\mathcal{E}^{G}}$ is continuous and bounded, and so if $(\\mu_{n})_{n\\in\\mathbb{N}}\\subseteq{\\mathcal{P}}({\\mathcal{E}}^{G})$ , then $P_{\\mathcal{E}^{G}}$ # $\\mu=\\mu$ (i.e. $\\mu\\in\\mathcal{P}(\\mathcal{E}^{G})$ ). \u53e3 ", "page_idx": 22}, {"type": "text", "text": "B.3 Properties of equivariant data ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We are representing the idea of \u2018data being symmetric\u2019 with respect to the action of $G$ , by assuming the data distribution $\\pi$ to be equivariant. We will next see that this is a natural generalization of more intuitive, though restrictive, notions of data being symmetric, for instance when $X$ is a r.v. on $\\mathcal{X}$ with invariant law $\\pi_{\\mathcal{X}}$ and $Y=f^{*}(X)+\\xi$ for some equivariant function $f^{*}:\\mathcal{X}\\to\\mathcal{Y}$ and some centered independent noise $\\xi$ . Indeed the following results tell us that, assuming $\\pi\\in\\mathcal P^{G}(\\mathcal X\\times\\mathcal P)$ , implies such a structure of the data, but with a more general $\\xi=\\xi_{X}$ , possibly correlated to $X$ but still \u2018conditionally\u2019 centered given $X$ . This result will be required in proving Proposition 5. ", "page_idx": 22}, {"type": "text", "text": "Proposition 10. Let $\\pi\\in{\\mathcal{P}}({\\mathcal{X}}\\times{\\mathcal{P}})$ be an equivariant data distribution such that $\\mathbb{E}_{\\pi}[\\|Y\\|^{2}]<\\infty$ .   \nThen $f^{*}=\\mathbb{E}_{\\pi}[Y|X=\\cdot]$ is an equivariant function. ", "page_idx": 22}, {"type": "text", "text": "Proof of Proposition $I O.$ . Indeed, as $\\mathbb{E}[\\|Y\\|^{2}]<\\infty$ , we know the conditional expectation $\\mathbb{E}[Y|X]$ is well defined and there exists a measurable $f^{*}\\;:\\;\\mathcal{X}\\;\\rightarrow\\;\\mathcal{Y}$ s.t. $f^{*}(X)\\,=\\,{\\bar{\\mathbb{E}}}[Y|X]$ . Now, by properties of the conditional expectation): Given any $h:\\mathcal{X}\\to\\mathbb{R}$ square integrable, we will show that: $\\begin{array}{r}{\\mathbb{E}_{\\pi}[Y h(X)]=\\mathbb{E}_{\\pi}[\\int_{G}\\hat{\\rho}_{g}^{-1}.f^{*}(\\rho_{g}.X)d\\lambda_{G}(g)h(X)]}\\end{array}$ . Indeed, notice that by Fubini\u2019s theorem (as $f^{*}\\in L^{2}(\\mathcal{X},\\mathcal{Y};\\pi_{\\mathcal{X}})$ and $h$ square integrable), linearity of the integral and $G$ -invariance of $\\pi$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{\\boldsymbol\\pi}\\left[(\\boldsymbol{Q}_{G}.f^{*})(X)h(X)\\right]=\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\displaystyle\\int_{G}\\hat{\\rho}_{g}^{-1}.f^{*}(\\rho_{g}.X)d\\lambda_{G}(g)h(X)\\right]}&{}\\\\ {=\\displaystyle\\int_{G}\\hat{\\rho}_{g}^{-1}.\\mathbb{E}_{\\boldsymbol\\pi}[f^{*}(X)h(\\rho_{g}.^{-1}.X)]d\\lambda_{G}(g)}&{}\\\\ {=\\displaystyle\\int_{G}\\mathbb{E}_{\\boldsymbol\\pi}[\\hat{\\rho}_{g}^{-1}.Y h(\\rho_{g}.^{-1}.X)]d\\lambda_{G}(g)}&{}\\\\ {=\\displaystyle\\int_{G}\\mathbb{E}_{\\boldsymbol\\pi}[Y h(X)]d\\lambda_{G}(g)=\\mathbb{E}_{\\boldsymbol\\pi}[Y h(X)]}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By uniqueness of the conditional expectation, we know therefore that $f^{*}(X)$ a.=s. $\\begin{array}{r}{\\int_{G}^{\\bullet}\\hat{\\rho}_{g}^{-1}.\\hat{f}^{*}(\\rho_{g}.X)\\lambda_{G}(g)}\\end{array}$ . In particular, $\\pi_{\\mathcal{X}}$ -a.e. $f^{*}=\\mathcal{Q}_{G}(f^{*})$ , making $f^{*}\\,G$ -equivariant. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "As a particular example, if we assumed that the data distribution was given by some function $f:\\mathcal X\\to\\mathcal Y$ , i.e. $Y=f(X)$ , taking $\\pi$ to be equivariant would be equivalent to assuming that $\\pi_{\\mathcal{X}}$ is invariant and $f$ is an equivariant function (which is the setting of the data simulated in our numerical experiments; see Section 4). ", "page_idx": 22}, {"type": "text", "text": "We notice that Proposition 10 can also be used to recover a celebrated result from [27] (later generalized by [40]), in the general setting where data symmetry is encoded by the condition that $\\bar{\\pi}\\in\\mathcal P_{2}(\\mathcal{X}\\times\\bar{\\mathcal{Y}})$ . Define the symmetrization gap of a learning problem with quadratic loss as: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\Delta(f,\\mathcal{Q}_{G}f):=\\mathbb{E}_{\\boldsymbol\\pi}[\\|Y-f(X)\\|_{\\mathcal{Y}}^{2}]-\\mathbb{E}_{\\boldsymbol\\pi}[\\|Y-(\\mathcal{Q}_{G}f)(X)\\|_{\\mathcal{Y}}^{2}]\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The following extension of mentioned statements from [27, 40] is not needed for our results, but we provide a proof of it in SuppMat-G, in view of its potential, independent interest: ", "page_idx": 23}, {"type": "text", "text": "Lemma 2 (Symmetrization Gap Characterization). Consider the quadratic loss and $\\pi\\in{\\mathcal{P}}({\\mathcal{X}}\\times{\\mathcal{P}})$ such that $\\ddot{\\mathbb{E}_{\\pi}}\\bar{[\\|Y\\|^{2}]}<\\infty$ . Also, assume that $\\pi|_{\\mathcal{X}}$ is $G$ -invariant, but $\\pi$ is only $H$ -invariant with respect to some $H\\leq G$ (closed). Then, the generalization gap satisfies: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\Delta(f,\\mathcal{Q}_{G}f)=-2\\big\\langle f^{*},f_{G}^{\\perp}\\big\\rangle_{L^{2}(\\mathcal{X},\\mathcal{Y};\\pi_{\\mathcal{X}})}+\\big\\|f_{G}^{\\perp}\\big\\|_{L^{2}(\\mathcal{X},\\mathcal{Y};\\pi_{\\mathcal{X}})}^{2}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $f^{*}(x)=\\mathbb{E}_{\\pi}[Y|X=x]$ is the conditional expectation function, and $f_{G}^{\\perp}:=f-\\mathcal{Q}_{G}f$ . ", "page_idx": 23}, {"type": "text", "text": "In particular, $i f\\pi$ is $G$ -invariant as well, we get $\\Delta(f,\\mathcal{Q}_{G}f)=\\|f_{G}^{\\perp}\\|_{L^{2}(\\mathcal{X},\\mathcal{Y};\\pi_{\\mathcal{X}})}^{2}$ ", "page_idx": 23}, {"type": "text", "text": "C Concrete realizations of shallow models ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The class of models we have introduced in Definition 1 allows for taking an arbitrary, common parameter space $\\mathcal{Z}$ for all hidden units, as well as an arbitrary function $\\sigma_{*}:\\mathcal{X}\\times\\mathcal{Z}\\to\\mathcal{Y}$ . As noted in the description of EAs in Section 2.3, by requiring only that $\\sigma_{*}$ is jointly-equivariant, we moreover ensure that $G\\operatorname{C}_{M}\\,\\mathcal{Z}$ is, in some sense, properly related to the actions $G\\operatorname{\\bigcirc}_{\\rho}\\,\\mathcal{X}$ and $G\\subset_{\\hat{\\rho}}\\mathcal{V}$ . This abstract property of $\\sigma_{*}$ allows us, in fact, to encode a wide range of situations and interesting results, without delving into the specifics of a particular choice of architecture. We next analyze this notion in the concrete example of the setting of a traditional single-hidden-layer shallow NN. ", "page_idx": 23}, {"type": "text", "text": "C.1 Traditional single layer neural networks and large ensembles of multi-layer units ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Recall the finite-dimensional setting of single-hidden-layer neural networks. We considered $\\mathcal{X}=\\mathbb{R}^{d}$ , $\\mathcal{V}=\\mathbb{R}^{c}$ and $\\mathcal{Z}=\\mathbb{R}^{c\\times b}\\times\\mathbb{R}^{d\\times b}\\times\\mathbb{R}^{b}$ , and defined, for $z\\,=\\,(W,A,B)\\,\\in\\,\\mathcal{Z}$ and $\\sigma:\\mathbb{R}^{b}\\rightarrow\\mathbb{R}^{b}$ , $\\sigma_{*}(x,z)\\,:=\\,W\\sigma(A_{\\cdot}^{T}x+B)$ . This allows us to express a shallow NN with $N$ hidden units, of parameters $\\theta=(\\theta_{i})_{i=1}^{N}\\in\\mathcal{Z}^{N}$ , with $\\theta_{i}=(W_{i},A_{i},\\bar{B_{i}})$ , as the function $\\Phi_{\\theta}^{N}:\\mathcal{X}\\to\\mathcal{Y}$ given by: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\forall x\\in\\mathcal{X},\\ \\Phi_{\\theta}^{N}(x):=\\frac{1}{N}\\sum_{i=1}^{N}W_{i}\\sigma(A_{i}^{T}.x+B_{i})=\\frac{1}{N}\\overline{{W}}.\\sigma(\\overline{{A}}^{T}.x+\\overline{{B}}),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where we write the expression by blocks, considering ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\overline{{W}}=(W_{1},\\ldots,W_{N})\\in\\mathbb{R}^{c\\times b N},\\,\\,\\,\\overline{{A}}=(A_{1},\\ldots,A_{N})\\in\\mathbb{R}^{d\\times b N},\\mathrm{and}\\,\\,\\,\\overline{{B}}=\\left(\\overset{\\displaystyle B_{1}}{\\overset{\\displaystyle\\vdots}{\\vdots}}\\right)\\in\\mathbb{R}^{b N}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This corresponds exactly to the usual single-hidden-layer setting (see e.g. [23, 53, 62, 67]), only that we allow for the structure to involve the use of block matrices. This allows us to translate many relevant EAs, such as CNNs (see [19]) or DeepSets (see [77]) into the shallow NN framework that we propose (see Appendix C.2 for fully developed examples). ", "page_idx": 23}, {"type": "text", "text": "We now also consider a $G$ -action on the intermediate layer, $G\\subset_{\\eta}\\mathbb{R}^{b}$ (so that $G\\subset_{\\eta\\otimes\\mathrm{Id}_{N}}(\\mathbb{R}^{b})^{N})$ This allows us to define the natural intertwinning action6 of $G$ on $\\mathcal{Z}$ , which is given by: ", "page_idx": 23}, {"type": "equation", "text": "$$\nM_{g}.z=M_{g}.(W,A,B):=(\\hat{\\rho}_{g}.W\\eta_{g}^{T},\\rho_{g}.A\\eta_{g}^{T},\\eta_{g}.B),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for any $g\\in G$ and $z=(W,A,B)\\in\\mathcal{Z}$ . This is exactly the action under which the fixed points (i.e. $\\mathcal{E}^{G}$ ) will correspond exactly to EAs in the traditional sense for this architecture (i.e. each layer being an equivariant function). In particular, we get the following straightforward result: ", "page_idx": 23}, {"type": "text", "text": "Proposition 11 (Joint equivariance of $\\sigma_{*}$ for single-hidden-layer NNs). In the setting of singlehidden-layer NNs described above, $i f\\sigma:\\mathbb{R}^{b}\\rightarrow\\mathbb{R}^{b}$ is $G$ -equivariant (with respect to the action given by $\\eta$ ), then $\\sigma_{*}$ is jointly equivariant. ", "page_idx": 24}, {"type": "text", "text": "Proof. This fact follows directly from the specific definition of $\\sigma_{*}$ , the linearity and the orthogonality of all the relevant group representations. Indeed, for any equivariant $\\sigma:\\mathbb{R}^{b}{\\overset{\\cdot}{\\to}}\\mathbb{R}^{b}$ , any $g\\in G$ and any $z=(W,A,B)\\in\\mathcal{Z}$ , we have: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sigma_{\\ast}(\\rho_{g}.x,M_{g}.z)=\\sigma_{\\ast}(\\rho_{g}.x,(\\hat{\\rho}_{g}.W.\\eta_{g}^{T},\\rho_{g}.A.\\eta_{g}^{T},\\eta_{g}.B))}\\\\ &{\\hphantom{\\sigma_{*}(\\rho_{g}.x,M_{g}.z)}=(\\hat{\\rho}_{g}.W.\\eta_{g}^{T}).\\sigma((\\rho_{g}.A.\\eta_{g}^{T})^{T}.(\\rho_{g}.x)+\\eta_{g}.B)}\\\\ &{\\hphantom{\\sigma_{*}(\\rho_{g}.x,M_{g}.z)}=\\hat{\\rho}_{g}.(W.\\eta_{g}^{T}.\\sigma(\\eta_{g}.A^{T}.(\\rho_{g}^{T}.\\rho_{g}).x+\\eta_{g}.B))}\\\\ &{\\hphantom{\\sigma_{*}(\\rho_{g}.x,M_{g}.z)}=\\hat{\\rho}_{g}.(W.\\eta_{g}^{T}.\\sigma(\\eta_{g}.(A^{T}.x+B)))}\\\\ &{\\hphantom{\\sigma_{*}(\\rho_{g}.x,M_{g}.z)}=\\hat{\\rho}_{g}.(W.\\eta_{g}^{T}.\\eta_{g}.\\sigma(A^{T}.x+B))}\\\\ &{\\hphantom{\\sigma_{*}(\\rho_{g}.x,M_{g}.z)}=\\hat{\\rho}_{g}.(W.\\sigma(A^{T}.x+B))}\\\\ &{\\hphantom{\\sigma_{*}(\\rho_{g}.x,M_{g}.z)}=\\hat{\\rho}_{g}.\\sigma_{*}(x,z)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Thus, in this particular case, in which $\\sigma_{*}$ represents the unit of a single hidden layer neural network, we require only for $\\sigma:\\mathbb{R}^{b}\\rightarrow\\mathbb{R}^{b}$ to be $G^{\\prime}$ -equivariant (with respect to the action given by $\\eta$ on $\\mathbb{R}^{b}$ ) in order for the joint equivariance of $\\sigma_{*}$ to hold (and, in consequence, most subsequent results from our work). ", "page_idx": 24}, {"type": "text", "text": "In particular, if $\\eta$ is chosen to be trivial (i.e. $\\eta\\,\\equiv\\,i d_{\\mathbb{R}^{b}}$ ), any single-hidden-layer NN is jointlyequivariant,7 regardless of $\\sigma:\\mathbb{R}^{b}\\rightarrow\\mathbb{R}^{b}$ . Namely, all of the results contained in the core of the paper can be applied to an arbitrary single-hidden-layer NN; most importantly, those relating vanilla, DA and FA training. The disadvantage of having a trivial $\\eta$ is that the space ${\\mathcal{E}}^{G}$ might not encode very interesting EAs. This might make Theorem 5 lose part of its impressiveness, but it takes no credit off the rest of our results (such as Corollary 3 and Theorem 4). ", "page_idx": 24}, {"type": "text", "text": "Analogously, if $\\eta$ acts via permutations of the coordinates in $\\mathbb{R}^{b}$ , it is enough to consider a $\\sigma$ that is the pointwise application of a scalar function. In practice, this usually isn\u2019t a restrictive condition, since most commonly used NN architectures are naturally built following this pattern. Therefore, almost any single-hidden-layer architecture can yield a jointly-equivariant $\\sigma_{*}$ for some of the most common and interesting finite groups (e.g. ${\\mathcal{S}}_{n}$ , $C_{n}$ , among many others; see [30] for further discussion). ", "page_idx": 24}, {"type": "text", "text": "For a more complex (possibly infinite) compact group $G$ acting orthogonally on the data and parameters (with non-trivial $\\eta$ ), for $\\sigma_{*}$ to be jointly-equivariant we have to start properly restraining $\\dot{\\sigma}:\\mathbb{R}^{b}\\,\\rightarrow\\,\\dot{\\mathbb{R}}^{b}$ . For instance, choosing an $O(b)$ -equivariant $\\sigma$ (e.g. a Norm-ReLU) would grant Proposition 11 to always hold; but such a restraining choice could potentially harm the model\u2019s expressiveness and applicability. We leave a deeper exploration of this more challenging case as future work. ", "page_idx": 24}, {"type": "text", "text": "Finally, all of the above discussion (including Proposition 11) readily generalizes to the multilayer case. Namely, if $\\sigma_{*}$ encodes a Multi-Layer Perceptron (MLP) with multiple hidden layers whose parameters live in some linear space $\\mathcal{Z}$ , we can define $G\\operatorname{C}_{M}\\mathcal{Z}$ as the intertwining action between each successive layer (similar to the previous example). $\\sigma_{*}$ can be made jointly-equivariant by making every activation function on each hidden layer equivariant (see [30]). Then, $\\mathcal{E}^{G}$ corresponds exactly to the parameters that make the entire MLP an equivariant architecture (in the sense that every layer is an equivariant function). With this, $\\Phi_{\\theta}^{N}$ is an ensemble of $N$ such MLPs trained in parallel, to which our results would also apply. Also as before, if the $G$ -action on all of the hidden layers (but not on input/output) is made trivial, then any $\\sigma_{*}$ representing a multilayer architecture can be jointly-equivariant. ", "page_idx": 24}, {"type": "text", "text": "In the upcoming sections we will further develop these ideas, to show that some of the most relevant and widely applied equivariant architectures can be realized as part of our setting. ", "page_idx": 24}, {"type": "text", "text": "C.2 Shallow DeepSet models, shallow GNNs and shallow CNNs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "C.2.1 DeepSets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "An emblematic example of neural networks with equivariant architecture are the Deep Sets, introduced by [77]; which correspond, in practice, to NN architectures designed to be invariant/equivariant to the action of the group of permutations $G=S_{n}$ . ", "page_idx": 25}, {"type": "text", "text": "Consider that our NN processes sets of size $n$ , which contain real-valued vectors of dimension $\\tilde{d}\\in\\mathbb{N}$ . We can represent this input space simply as $\\mathcal{X}:=\\mathbb{R}^{n\\times\\tilde{d}}$ . Say we wanted to build a wide single-hidden-layer network that is invariant/equivariant to the action of ${\\mathcal{S}}_{n}$ , and returns a new set of $n$ vectors, but now of dimension $\\tilde{c}\\in\\mathbb{N}$ (i.e. $\\boldsymbol{\\mathcal{V}}:=\\mathbb{R}^{n\\times\\tilde{c}}\\mathrm{,}$ ). For this, let\u2019s consider the same architecture as in the previous section (replace $d=n\\tilde{d},c=n\\tilde{c})$ , where we\u2019ll let our intermediate layer be simply another set of $n$ vectors, now of dimension $\\tilde{b}$ with $\\tilde{b}\\in\\mathbb{N}$ (such that $b=n\\tilde{b}$ above), and repeated $N\\in\\mathbb{N}$ times (as in the multiple hidden units that we want to achieve). With this, our network flows as follows: $\\Phi_{\\theta}^{N}:\\mathbb{R}^{n\\times\\tilde{d}}\\rightarrow\\mathbb{R}^{\\bar{(}n\\times\\tilde{b})\\times N}\\rightarrow\\mathbb{R}^{n\\times\\tilde{c}}$ . Notice how ${\\mathcal{S}}_{n}$ acts naturally on each hidden space by simply permuting the vectors of the set (i.e. $\\rho,{\\hat{\\rho}}$ and $\\eta$ are defined in this way). ", "page_idx": 25}, {"type": "text", "text": "Following the same structure as in the previous section, we will have a parameter space given by: $\\mathcal{Z}\\;:=\\;\\mathbb{R}^{(n\\times\\tilde{c})\\times(n\\times\\tilde{b})}\\;\\times\\;\\mathbb{R}^{(n\\times\\tilde{d})\\times(n\\times\\tilde{b})}\\;\\times\\;\\mathbb{R}^{n\\times\\tilde{b}}$ , and a unit that acts on $z\\,=\\,(W,A,B)\\,\\in\\,{\\mathcal{Z}}$ as $\\sigma_{*}(x,z):=W\\sigma(A^{T}x+B)$ , with $\\sigma:\\mathbb{R}^{n\\times\\tilde{b}}\\rightarrow\\mathbb{R}^{n\\times\\tilde{b}}$ some usual activation function applied pointwise (which, as mentioned above, will be immediately equivariant to the defined action of $G$ via $\\eta$ ). Notice how building this architecture requires no hard a priori knowledge of the underlying symmetry of the problem (beyond the fact that the inputs and outputs are sets). ", "page_idx": 25}, {"type": "text", "text": "Under the natural action from this context (i.e. $\\rho,\\,\\hat{\\rho}$ and $\\eta$ acting on the sets by permuting their elements, and $M$ defined as in SuppMat-C.1), $\\mathcal{E}^{G}$ corresponds exactly to the set of parameters that yield a ${\\mathcal{S}}_{n}$ -equivariant shallow neural network (which can be interpreted as a DeepSet). For the interested reader, we will make this connection explicit in the rest of this section. ", "page_idx": 25}, {"type": "text", "text": "As shown in [77], the only way to have an ${\\mathcal{S}}_{n}$ -equivariant affine transformation between two spaces $\\mathbb{R}^{n\\times d_{1}}$ and $\\mathbb{R}^{n\\times d_{2}}$ is if the parameters $\\tilde{A}\\in\\mathbb{R}^{(n\\times d_{1})\\times(n\\times d_{2})}$ and $\\tilde{B}\\in\\mathbb{R}^{n\\times d_{2}}$ (from the definition $x\\mapsto\\tilde{A}^{T}.x+\\tilde{B})$ are of the form: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\tilde{A}=\\alpha\\otimes I+\\beta\\otimes J,\\qquad\\tilde{B}=\\gamma\\otimes(1,\\ldots,1)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Where $\\alpha,\\beta\\,\\in\\,\\mathbb{R}^{d_{1}\\times d_{2}},\\gamma\\,\\in\\,\\mathbb{R}^{d_{2}}$ are the truly trainable parameters of the layer; $I\\,=\\,{\\bf I d}_{n\\times n}$ and $J=\\vec{1}_{n}\\vec{1}_{n}^{T}$ are two $n\\times n$ matrices; and $\\otimes$ is the usual tensor product. More explicitly, writing the matrices by blocks, this is: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{\\alpha}\\otimes\\boldsymbol{I}=\\left(\\begin{array}{l l l l}{\\alpha}&{0}&{\\ldots}&{0}\\\\ {0}&{\\alpha}&{\\ldots}&{0}\\\\ {\\vdots}&{\\ddots}&{\\ddots}&{0}\\\\ {0}&{0}&{\\ldots}&{\\alpha}\\end{array}\\right)\\in\\mathbb{R}^{(n\\times d_{1})\\times(n\\times d_{2})},}\\\\ &{\\boldsymbol{\\beta}\\otimes\\boldsymbol{J}=\\left(\\begin{array}{l l l l}{\\beta}&{\\beta}&{\\ldots}&{\\beta}\\\\ {\\beta}&{\\beta}&{\\ldots}&{\\beta}\\\\ {\\vdots}&{\\ddots}&{\\ddots}&{\\beta}\\\\ {\\beta}&{\\beta}&{\\ldots}&{\\beta}\\end{array}\\right)\\in\\mathbb{R}^{(n\\times d_{1})\\times(n\\times d_{2})}}\\\\ &{\\colon\\ldots,1)=(\\gamma,\\ldots,\\gamma)\\in\\mathbb{R}^{(n\\times d_{2})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "In particular, Equation (8) gives us an explicit expression for our space ${\\mathcal{E}}^{G}$ . Indeed, an element $z=(W,A,B)\\in\\mathcal{Z}$ will live in ${\\mathcal{E}}^{G}$ if and only if there exists $w_{1},w_{2}\\in\\mathbb{R}^{\\tilde{c}\\times\\tilde{b}}$ , $a_{1},a_{2}\\in\\mathbb{R}^{\\tilde{d}\\times\\tilde{b}}$ and $b_{1}\\in\\mathbb{R}^{\\tilde{b}}$ such that: ", "page_idx": 25}, {"type": "equation", "text": "$$\nW=w_{1}\\otimes I+w_{2}\\otimes J,\\ A=a_{1}\\otimes I+a_{2}\\otimes J,\\ \\mathrm{and}\\ \\ B=b_{1}\\otimes(1,\\dots,1)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "In a sense, we can think of ${\\mathcal{E}}^{G}$ simply as being equivalent to $\\mathbb{R}^{2(\\tilde{c}\\times\\tilde{b})}\\times\\mathbb{R}^{2(\\tilde{d}\\times\\tilde{b})}\\times\\mathbb{R}^{\\tilde{b}}$ . In particular, we went from having $D=\\mathrm{dim}({\\mathcal{Z}})=n^{2}\\cdot\\tilde{b}\\cdot\\bar{(c+d)}+n\\cdot\\tilde{b}$ free parameters on each unit, to simply $\\tilde{D}=\\dim(\\mathcal{E}^{G})=2\\cdot\\tilde{b}\\cdot(\\tilde{c}+\\tilde{d})+\\tilde{b}.$ , which should be easier to manage in general. ", "page_idx": 25}, {"type": "text", "text": "Now, recall our construction from SuppMat-C.1, and consider the matrices $\\overline{{W}},\\overline{{A}}$ and $\\overline{B}$ from Equation (6). We notice that, in this example, they are of the form: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\overline{{W}}\\in\\mathbb{R}^{(n\\times\\tilde{c})\\times(n\\times\\tilde{b}N)},\\;\\;\\overline{{A}}\\in\\mathbb{R}^{(n\\times\\tilde{d})\\times(n\\times\\tilde{b}N)},\\mathrm{and}\\;\\;\\overline{{B}}\\in\\mathbb{R}^{(n\\times\\tilde{b}N)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Namely, sensibly replacing $d_{1}\\,=\\,\\tilde{d}$ , $d_{2}\\;=\\;\\tilde{b}N$ for $\\overline{{A}}$ and $\\overline{B}$ ; and $d_{1}\\ =\\ \\tilde{c}$ , $d_{2}\\,=\\,\\tilde{b}N$ for $\\overline{W}$ in Equation (8); we also get an explicit condition under which $\\overline{{W}},\\overline{{A}}$ and $\\overline{B}$ yield a globally ${\\mathcal{S}}_{n}$ - equivariant architecture. By properly writing these matrices by blocks (as in SuppMat-C.1), and separating the action of each of the $N$ units, one shall notice that the condition for $\\overline{{W}},\\overline{{A}}$ and $\\overline{B}$ to be ${\\mathcal{S}}_{n}$ -equivariant corresponds exactly to every $\\theta_{i}=(W_{i},A_{i},B_{i})\\in\\mathcal{Z}$ , $i=1,\\ldots,N$ , being of the form given in Equation (9). That is to say, $\\Phi_{\\theta}^{N}$ has an ${\\mathcal{S}}_{n}$ -equivariant architecture (a DeepSet from [77]) if and only if $\\forall i\\in\\{1,\\dots,N\\},\\theta_{i}\\in\\mathcal{E}^{G}$ , which is exactly the condition stated in Section 2.3. ", "page_idx": 26}, {"type": "text", "text": "Remark. The reader could notice that our previous construction is not truly involving the complete richness of DeepSets. Namely, these architectures often involve using some \"pooling\" mechanisms, such as a global average pooling $(G A P)$ operation to force invariance into the network (see [8]). Namely, if $\\mathcal{A}:\\mathbb{R}^{n\\times\\tilde{b}}\\rightarrow\\bar{\\mathbb{R}}^{\\tilde{b}}$ is the usual linear GAP operation, we might want a NN that flows as $\\Phi_{\\theta}^{N}:\\mathbb{R}^{n\\times\\tilde{d}}\\rightarrow\\mathbb{R}^{(n\\times\\tilde{b})\\times N}\\xrightarrow{A}\\mathbb{R}^{\\tilde{b}N}\\rightarrow\\mathbb{R}^{\\tilde{c}}$ . This is no trouble within our framework, since we can simply consider the \u2018adequate\u2019 unit: $\\sigma_{*}(x,z):=W\\mathcal{A}(\\sigma(A^{T}x+B))$ for $z=(W,A,B)\\in{\\mathcal{Z}}:=$ Rc\u02dc\u00d7b\u02dc \u00d7 R(n\u00d7 d\u02dc)\u00d7(n\u00d7b\u02dc) \u00d7 Rn\u00d7b\u02dc, and all of our results would be applicable. ", "page_idx": 26}, {"type": "text", "text": "The main disadvantage of doing this is that we are forced to encode some a priori knowledge of the symmetries of the problem into our choice of architecture. While this isn\u2019t useful for our heuristic from Section 4.2, all other results relating the $D A$ , $F\\!A$ and $E A$ training dynamics still apply. ", "page_idx": 26}, {"type": "text", "text": "Remark. Similar to the last remark, more complex equivariant NN, with multiple layers and various inner operations involved, can be modeled by choosing $\\sigma_{*}$ properly. Namely, set $\\sigma_{*}$ to be a whole multi-layer structure, with parameters in $\\mathcal{Z}$ , and ${\\mathcal{E}}^{G}$ being the subspace of those that make the architecture equivariant (see e.g. Equation (9)). ", "page_idx": 26}, {"type": "text", "text": "In such case, the shallow model $\\Phi_{\\boldsymbol{\\theta}}^{N}$ , with $\\theta\\,\\in\\,\\mathcal{Z}^{N}$ , will represent an ensemble of $N$ multi-layer units, each one given, for every $i\\in\\tilde{\\{1,\\ldots,N\\}}$ , by $\\sigma_{*}(\\cdot,\\theta_{i}):\\mathcal{X}\\xrightarrow{}\\mathcal{Y}$ . The output of the ensemble is simply the average of the outputs of each one of the units. ", "page_idx": 26}, {"type": "text", "text": "C.2.2 GNNs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Generalizing the ideas from DeepSets to GNNs is fairly straightforward. Namely, consider the input of a layer to be a graph, represented by a set of features, coupled with an adjacency matrix that also contains relevant edge-features; and the output to be analogous. Namely, let\u2019s say the input space is $\\mathcal{X}_{1}=\\mathbb{R}^{(n\\times d_{1})}\\times\\mathbf{\\bar{R}}^{(n\\times n)\\times d_{2}}$ and the output space is $\\mathcal{X}_{2}\\overset{\\mathcal{\\tau}}{=}\\mathbb{R}^{(n\\times d_{3})}\\times\\dot{\\mathbb{R}}^{(n\\times n)\\times d_{4}}$ . Consider also the natural ${\\mathcal{S}}_{n}$ action acting on these spaces, i.e. permuting the vertices of the graph, acting jointly between vertex features and adjacency matrix. With this in place, one can find an analogous characterization to Equation (8), but for affine layers that are ${\\mathcal{S}}_{n}$ -equivariant between graph spaces (see e.g. [29, 49] for a more explicit construction). From there, it is not hard to construct, following the same steps as for DeepSets, an explicit characterization of ${\\mathcal{E}}^{G}$ for single-hidden-layer GNNs, analogous to Equation (9). Also as before, more complex GNN structures (with multiple layers, pooling operations, etc.) can be encoded in this setting through the unit $\\sigma_{*}$ (with the possibly same drawbacks as in DeepSets). For brevity, we don\u2019t delve into GNNs in their full complexity and leave that to the curious reader. ", "page_idx": 26}, {"type": "text", "text": "C.2.3 CNNs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Finally, following the same logic as before, we can model another one of the most emblematic traditional equivariant models: CNNs. We here consider only 1D-CNNs for simplicity, but all arguments can be readily generalized to 2D or 3D CNNs. ", "page_idx": 26}, {"type": "text", "text": "In this setting, take an array of $n$ vectors of dimension $\\tilde{d}\\in\\mathbb{N}$ as input $\\left\\langle\\mathcal{X}:=\\mathbb{R}^{n\\times\\tilde{d}}\\right\\rangle$ and, analogous to DeepSets, say that the output and hidden layers have the same structure, so that $\\mathcal{D}:=\\mathbb{R}^{n\\times\\tilde{c}}$ ; and $\\mathcal{Z}:=\\bar{\\mathbb{R}}^{(n\\times\\tilde{c})\\times(n\\times\\tilde{b})}\\times\\mathbb{R}^{(n\\times\\bar{d})\\times(n\\times\\tilde{b})}\\times\\mathbb{R}^{n\\times\\tilde{b}}$ , for $\\tilde{b},\\tilde{c}\\in\\mathbb{N}$ . We consider $\\sigma_{*}$ simply as before. ", "page_idx": 26}, {"type": "text", "text": "The single main difference with the study of DeepSets is that, in this case, we consider the action of $C_{n}$ (i.e. the cyclic group of order $n$ , also denoted $\\mathbb{Z}_{n}$ ) instead of ${\\mathcal{S}}_{n}$ . In particular, the natural action of $C_{n}$ on each space (through $\\rho,{\\hat{\\rho}}$ and $\\eta$ ) consists simply of shifting the array\u2019s coordinates in a given direction (modulo $n$ ). As before, this makes the set $\\dot{\\mathcal{E}}^{\\tilde{G}}$ correspond exactly to the set of parameters that yield each layer $C_{n}$ -equivariant (as in a 1D-CNN). ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "More specifically, we can characterize (analogous to Equation (8)) how a single $C_{n}$ -equivariant layer looks like between two spaces $\\mathcal{X}_{1}=\\mathbb{R}^{(n\\times\\bar{d}_{1})}$ and $\\mathcal{X}_{2}=\\mathbb{R}^{(n\\times d_{2})}$ under the natural action. It is well known (see [8]) that a $C_{n}$ -equivariant affine layer, $x\\mapsto\\tilde{A}^{T}.x+\\tilde{B}$ , between $\\mathcal{X}_{1}$ and $\\chi_{2}$ , of parameters $\\tilde{A}\\in\\mathbb{R}^{(n\\times d_{1})\\times(n\\times d_{2})}$ and $\\tilde{B}\\in\\mathbb{R}^{n\\times d_{2}}$ , must be of the form: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\tilde{A}=C(\\alpha_{0},\\ldots,\\alpha_{n-1})\\;\\;\\mathrm{and}\\;\\;\\tilde{B}=\\beta\\otimes(1,\\ldots,1),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "with $\\beta\\in\\mathbb{R}^{d_{2}},\\alpha_{i}\\in\\mathbb{R}^{d_{2}\\times d_{1}},\\ \\forall i\\in\\left\\{0,\\ldots,n-1\\right\\}$ , and the associated circulant matrix being defined (by blocks, and considering the indices modulo $n$ ) as: ", "page_idx": 27}, {"type": "equation", "text": "$$\nC(\\alpha_{0},\\ldots,\\alpha_{n-1})=\\left(\\!\\!{\\begin{array}{c c c c c}{\\alpha_{0}}&{\\alpha_{1}}&{\\ldots}&{}&{\\ldots}&{\\alpha_{-1}}\\\\ {\\alpha_{-1}}&{\\alpha_{0}}&{\\alpha_{1}}&{\\ldots}&{}&{\\ldots}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}&{\\vdots}\\\\ {\\ldots}&{\\ldots}&{\\alpha_{-1}}&{\\alpha_{0}}&{\\alpha_{1}}\\\\ {\\alpha_{1}}&{\\ldots}&{\\ldots}&{\\alpha_{-1}}&{\\alpha_{0}}\\end{array}}\\!\\!\\right)\\in\\mathbb{R}^{(n\\times d_{2})\\times(n\\times d_{1})}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "As a consequence, we get an analog of Equation (9) to explicitly describe ${\\mathcal{E}}^{G}$ : we have that $z=$ $(W,A,B)\\in\\mathcal{E}^{G}$ if and only if $\\exists(w_{i})_{i=0}^{n-1}\\subseteq\\mathbb{R}^{\\tilde{c}\\times\\tilde{b}}$ , $(a_{i})_{i=0}^{n-1}\\subseteq\\mathbb{R}^{\\tilde{d}\\times\\tilde{b}}$ and $\\beta\\in\\mathbb{R}^{\\tilde{b}}$ such that: ", "page_idx": 27}, {"type": "equation", "text": "$$\nW=C(w_{0},\\dots,w_{n-1}),\\;A=C(a_{0},\\dots,a_{n-1})\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Thus, the parameter space goes from having $\\dim({\\mathcal Z})=n^{2}\\cdot{\\tilde{b}}\\cdot({\\tilde{c}}+{\\tilde{d}})+n\\cdot{\\tilde{b}}$ free parameters on each unit, to simply $\\mathrm{dim}(\\mathcal{E}^{G})=n\\cdot\\tilde{b}\\cdot(\\tilde{c}+\\tilde{d})+\\tilde{b}$ . One might also notice that the obtained parameter space ${\\mathcal{E}}^{C_{n}}$ contains $\\mathcal{E}^{S_{n}}$ (from DeepSets), which is expected from the fact that $C_{n}\\leq S_{n}$ . In global terms, the shallow CNNs we have modeled here, correspond to models that will grow asymptotically in terms of the number of different convolutional filters (encoded by $N$ ) that are being used. ", "page_idx": 27}, {"type": "text", "text": "Finally, as it was also mentioned in previous examples, more complex CNN structures (with multiple layers, pooling operations, etc.) can be encoded in this setting through modifications to the unit $\\sigma_{*}$ . ", "page_idx": 27}, {"type": "text", "text": "D Further elements from the MF theory of shallow neural networks ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this section we study several theoretical notions required in the MF approach to overparametrized shallow NN. For the purpose of our results, Subsections D.1 and D.2 are the most relevant ones, as we establish therein some useful properties or formula that will be explicitly required. Subsections D.3 and D.4 review some results and recent literature regarding well-posedness and long-time convergence of WGFs, which are relevant to the MF interpretation of the training dynamics of shallow models. ", "page_idx": 27}, {"type": "text", "text": "D.1 Linear functional derivatives and intrinsic derivatives ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Let $\\mathcal{X},\\mathcal{Y}$ and $\\mathcal{Z}$ be separable Hilbert spaces. Recall the definitions of the linear functional derivative and intrinsic derivatives: ", "page_idx": 27}, {"type": "text", "text": "Definition 8 (Linear Functional Derivative (First Variation)). For a functional $F:\\mathcal{P}_{2}(\\mathcal{Z})\\to\\mathbb{R},$ , its linear functional derivative (lfd) is a function: $\\begin{array}{r}{\\frac{\\partial F}{\\partial\\mu}:\\mathcal{P}_{2}(\\mathcal{Z})\\times\\mathcal{Z}\\rightarrow\\mathbb{R}}\\end{array}$ such that $\\forall\\mu,\\nu\\in\\mathcal{P}_{2}(\\mathcal{Z})$ : $\\operatorname*{lim}_{h\\to0}\\frac{F((1-h)\\mu+h\\nu)-F(\\mu)}{h}=\\int_{\\mathcal{Z}}\\frac{\\partial F}{\\partial\\mu}(\\mu,z)d(\\nu-\\mu)(z),\\;\\;a n d\\;\\;\\int_{\\mathcal{Z}}\\frac{\\partial F}{\\partial\\mu}(\\mu,z)d\\mu(z)=0$ The function $\\begin{array}{r}{F^{\\prime}:\\mu\\in\\mathcal{P}_{2}(\\mathcal{Z})\\mapsto\\frac{\\partial F}{\\partial\\mu}(\\mu,\\cdot)}\\end{array}$ is also known as the first variation of $R$ at $\\mu$ . Definition 9 (Intrinsic Derivative). Whenever $\\begin{array}{r}{\\frac{\\partial F}{\\partial\\mu}:\\mathcal{P}_{2}(\\mathcal{Z})\\times\\mathcal{Z}\\rightarrow\\mathbb{R}}\\end{array}$ exists and is differentiable on its second argument, the intrinsic derivative of $F$ is defined as: $\\begin{array}{r}{D_{\\mu}F(\\mu,z)=\\nabla_{z}\\left(\\frac{\\partial F}{\\partial\\mu}(\\mu,z)\\right)}\\end{array}$ . ", "page_idx": 27}, {"type": "text", "text": "Example. To better illustrate the notion of the linear functional derivative and the intrinsic derivative, consider the following usual examples: ", "page_idx": 27}, {"type": "text", "text": "1. In the important case of the Boltzmann entropy $F=H_{\\lambda}$ , defined for $\\mu\\ll\\lambda$ by $H_{\\lambda}:=$ $\\textstyle\\int\\log({\\frac{d\\mu}{d\\lambda}}(z))d\\mu(z)$ (and $+\\infty$ otherwise), we have that (modulo an additive constant that doesn\u2019t depend on $z$ , see $[56]$ ): ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{\\partial F}{\\partial\\mu}(\\mu,z)=\\log\\left(\\frac{d\\mu}{d\\lambda}(z)\\right)+1~\\,a n d\\,\\,\\,D_{\\mu}F(\\mu,z)=\\frac{1}{\\frac{d\\mu}{d\\lambda}(z)}\\nabla_{z}\\frac{d\\mu}{d\\lambda}(z).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "2. Whenever $\\begin{array}{r}{F(\\mu):=\\int_{\\mathcal{Z}}\\phi(z)d\\mu(z)}\\end{array}$ for some bounded continuously differentiable function $\\phi:\\mathcal{Z}\\to\\mathbb{R}$ , it is well known that : ", "page_idx": 28}, {"type": "equation", "text": "$$\n{\\frac{\\partial F}{\\partial\\mu}}(\\mu,z)=\\phi(z)-\\int\\phi d\\mu\\;\\;a n d\\;\\;D_{\\mu}F(\\mu,z)=\\nabla_{z}\\phi(z)\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The most relevant example, in our case, regards the linear functional and intrinsic derivatives of the population risk functional, $R(\\mu)\\,=\\,\\mathbb{E}_{\\pi}[\\ell(\\Phi_{\\mu}(X),Y)]$ . We can consider the general setting presented in [16], where some Hilbert Space $\\mathcal{H}$ is considered, and it is assumed that $F:\\mathcal{P}(\\mathcal{Z})\\to\\mathbb{R}$ can be written as $F(\\mu)\\,=\\,L(\\langle\\Phi,\\mu\\rangle)$ , where $\\Phi:\\mathcal{Z}\\to\\mathcal{H}$ is a parametrization of elements in $\\mathcal{H}$ ; $L:\\mathcal{H}\\to\\mathbb{R}$ is some loss functional, and the integral $\\langle\\Phi,\\mu\\rangle$ is a Bochner integral on $\\mathcal{H}$ . This generalizes the shallow NN setting, as one might consider $\\dot{\\mathcal{H}}=L^{2}(\\mathcal{X},\\mathcal{Y},\\pi_{\\mathcal{X}})$ , $L:\\mathcal{H}\\to\\mathbb{R}$ given by $L(f)\\;=\\;\\mathbb{E}_{\\pi}[\\ell(f(X),Y)]$ and $\\Phi:\\,\\mathcal{Z}\\,\\rightarrow\\,\\mathcal{H}$ defined as $\\forall z\\in\\mathcal{Z}$ , $\\Phi(z)~=~\\sigma_{*}(\\cdot;z)$ ; so that $R(\\mu)=L(\\langle\\Phi,\\mu\\rangle)$ . In this setting, we can prove the following result:8 ", "page_idx": 28}, {"type": "text", "text": "Proposition 12. Let $\\mathcal{H}$ be a separable Hilbert Space and $F(\\mu):=L(\\langle\\Phi,\\mu\\rangle).$ , for some function that\u2019s Gateaux-differentiable $L:\\mathcal{H}\\to\\mathbb{R}$ on every direction and of continuous differential; and $\\Phi:\\mathcal{Z}\\to\\mathcal{H}$ such that $\\forall\\mu\\in\\mathcal{P}(\\mathcal{Z})$ , $\\|\\langle\\Phi,\\mu\\rangle\\|_{\\mathcal{H}}<\\infty$ . ", "page_idx": 28}, {"type": "text", "text": "Then $\\forall z\\in{\\mathcal{Z}},\\,\\forall\\mu\\in{\\mathcal{P}}({\\mathcal{Z}}).$ : ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\partial F}{\\partial\\mu}(\\mu,z)=D_{h}L(\\langle\\Phi,\\mu\\rangle)(\\Phi(z))=\\langle\\nabla_{h}L(\\langle\\Phi,\\mu\\rangle),\\Phi(z)\\rangle_{\\mathcal{H}}-C_{F,\\mu}}\\\\ &{D_{\\mu}F(\\mu,z)=\\left(D_{h}L(\\langle\\Phi,\\mu\\rangle)(D_{z}\\Phi(z))\\right)^{*}=\\nabla_{z}\\Phi(z)(\\nabla_{h}L(\\langle\\Phi,\\mu\\rangle)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Here, $C_{F,\\mu}:=\\langle\\nabla_{h}L(\\langle\\Phi,\\mu\\rangle),\\langle\\Phi,\\mu\\rangle\\rangle_{\\mathcal{H}}$ is exactly the constant needed to avoid ambiguity in the definition; $(\\cdot)^{*}$ denotes the adjoint operator and, in particular, $\\nabla_{z}\\Phi(z)=(D_{z}\\Phi(z))^{*}:\\mathcal{H}\\to\\mathcal{Z}$ When $\\mathcal{Z}=\\vec{\\mathbb{R}}^{D}$ this corresponds to the usual definition of the gradient. ", "page_idx": 28}, {"type": "text", "text": "Proof of Proposition $^{12}$ . We know that, $\\forall\\mu,\\nu\\in\\mathcal{P}(\\mathcal{Z}),\\;h\\in[0,1]$ : ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{F((1-h)\\mu+h\\nu)-F(\\mu)}{h}=\\frac{L(\\langle\\Phi,(1-h)\\mu+h\\nu\\rangle)-L(\\langle\\Phi,\\mu\\rangle)}{h}\\\\ {=\\frac{L(\\langle\\Phi,\\mu\\rangle+h\\langle\\Phi,\\nu-\\mu\\rangle)-L(\\langle\\Phi,\\mu\\rangle)}{h}.}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Let\u2019s denote by $q_{\\mu}:=\\langle\\Phi,\\mu\\rangle$ (analogously $q_{\\nu-\\mu}:=\\langle\\Phi,\\nu-\\mu\\rangle)$ and $s_{\\mu,\\nu}:=h q_{\\nu-\\mu}$ , so we can write: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{F((1-h)\\mu+h\\nu)-F(\\mu)}{h}=\\frac{L(q_{\\mu}+s_{\\mu,\\nu})-L(q_{\\mu})}{h}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "As $L$ is Gateaux differentiable, we have the following first order Taylor expansion $\\forall x,s\\in\\mathcal{H},\\forall t\\in\\mathbb{R}$ : ", "page_idx": 28}, {"type": "equation", "text": "$$\nL(x+t\\,s)=L(x)+t\\,D_{h}L(x).s+o(|t|\\|s\\|),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "which allows us to write: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{F((1-h)\\mu+h\\nu)-F(\\mu)}{h}=\\frac{L(q_{\\mu}+h\\,q_{\\nu-\\mu})-L(q_{\\mu})}{h}=\\frac{h.D_{h}L(q_{\\mu}).q_{\\nu-\\mu}+o(|h|\\|q_{\\nu-\\mu}\\|)}{h}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "As $\\|q_{\\nu-\\mu}\\|<\\infty$ by hypothesis, we can say that: $o(|h||q_{\\nu-\\mu}||)=o(h)$ . Therefore, taking the limit with $h\\to0$ , we get that: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{h\\to0}\\frac{F((1-h)\\mu+h\\nu)-F(\\mu)}{h}=D_{h}L(q_{\\mu}).q_{\\nu-\\mu}+\\operatorname*{lim}_{h\\to0}\\frac{o(h)}{h}=D_{h}L(q_{\\mu}).q_{\\nu-\\mu}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "8Where the gradient $\\nabla_{x}f(x)$ is the unique vector in $\\mathcal{H}$ representing the action of $D_{x}f(x):\\mathcal{H}\\to\\mathbb{R}$ ", "page_idx": 28}, {"type": "text", "text": "Now, developping this last term (using, for instance, the linearity of the Bochner integral, as we know $D_{h}L(x,\\cdot).(\\cdot)$ to be linear and bounded), we get that: ", "page_idx": 29}, {"type": "equation", "text": "$$\nD_{h}L(q_{\\mu}).q_{\\nu-\\mu}=D_{h}L(q_{\\mu}).\\langle\\Phi,\\nu-\\mu\\rangle=\\langle D_{h}L(q_{\\mu})(\\Phi),\\nu-\\mu\\rangle,\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and so by definition of the gradient of $L$ : ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{h\\to0}\\frac{F((1-h)\\mu+h\\nu)-F(\\mu)}{h}=\\int_{\\mathcal{Z}}\\langle\\nabla_{h}L(\\langle\\Phi,\\mu\\rangle),\\Phi(z)\\rangle_{\\mathcal{H}}\\;d(\\nu-\\mu)(z).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "From here we deduce that: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{\\partial F}{\\partial\\mu}(\\mu,z)=\\langle\\nabla_{h}L(\\langle\\Phi,\\mu\\rangle),\\Phi(z)\\rangle_{\\mathcal{H}}-C_{F,\\mu},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $C_{F,\\mu}$ is a fixed constant, given by: ", "page_idx": 29}, {"type": "equation", "text": "$$\nC_{F,\\mu}=\\int_{\\mathcal{Z}}\\frac{\\partial F}{\\partial\\mu}(\\mu,z)d\\mu(z)=\\int_{\\mathcal{Z}}\\langle\\nabla_{h}L(\\langle\\Phi,\\mu\\rangle),\\Phi(z)\\rangle_{\\mathcal{H}}\\;d(\\mu)(z)=\\langle\\nabla_{h}L(\\langle\\Phi,\\mu\\rangle),\\langle\\Phi,\\mu\\rangle\\rangle_{\\mathcal{H}}\\;d\\mu(\\mu)(z)\\rangle_{\\mathcal{H}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "On the other hand, for the intrinsic derivative, notice that $\\begin{array}{r}{D_{z}(\\frac{\\partial F}{\\partial\\mu}(\\mu,z)):\\mathcal{Z}\\to\\mathbb{R}}\\end{array}$ is a bounded linear functional over $\\mathcal{Z}$ , so (by Riesz Representation) $\\begin{array}{r}{\\exists D_{\\mu}F(\\mu,z):=\\nabla_{z}(\\frac{\\partial F}{\\partial\\mu}(\\mu,z))\\in\\mathcal{Z}}\\end{array}$ such that: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\forall z\\in{\\mathcal{Z}},\\,\\left\\langle\\nabla_{z}\\left({\\frac{\\partial F}{\\partial\\mu}}(\\mu,z)\\right),z\\right\\rangle_{{\\mathcal{Z}}}=D_{z}\\left({\\frac{\\partial F}{\\partial\\mu}}(\\mu,z)\\right)(z)\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "However, we can develop the RHS, and as the constant $C_{F,\\mu}$ doesn\u2019t depend on $z$ , we get that: ", "page_idx": 29}, {"type": "equation", "text": "$$\nD_{z}\\left(\\frac{\\partial F}{\\partial\\mu}(\\mu,z)\\right)(z)=D_{z}\\left(\\langle\\nabla_{h}L(\\langle\\Phi,\\mu\\rangle),\\Phi(z)\\rangle_{\\mathcal{H}}\\right)(z)\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Now, by the chain rule and the definition of the adjoint operator of $D_{z}\\Phi(z)$ : ", "page_idx": 29}, {"type": "equation", "text": "$$\nD_{z}\\left(\\frac{\\partial F}{\\partial\\mu}(\\mu,z)\\right)(z)=\\left\\langle\\nabla_{h}L(\\langle\\Phi,\\mu\\rangle),D_{z}\\Phi(z)(z)\\right\\rangle_{\\mathcal{H}}=\\left\\langle\\left(D_{z}\\Phi(z)\\right)^{*}\\left(\\nabla_{h}L(\\langle\\Phi,\\mu\\rangle)\\right),z\\right\\rangle_{\\mathcal{Z}}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "So, as they coincide for every $z\\in{\\mathcal{Z}}$ , we conclude that: ", "page_idx": 29}, {"type": "equation", "text": "$$\nD_{\\mu}F(\\mu,z)=\\left(D_{z}\\Phi(z)\\right)^{*}\\left(\\nabla_{h}L(\\langle\\Phi,\\mu\\rangle)\\right)\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proposition 12 applies directly to our population risk functional $R(\\mu):=\\mathbb{E}_{\\pi}\\left[\\ell(\\Phi_{\\mu}(X),Y)\\right]$ , by considering: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The Hilbert space: $\\mathcal{H}=L^{2}(\\mathcal{X},\\mathcal{Y},\\pi_{\\mathcal{X}})$   \n\u2022 $L:\\mathcal{H}\\to\\mathbb{R}$ given by $L(f)=\\mathbb{E}_{\\pi}[\\ell(f(X),Y)]$ , which is Gateaux-differentiable on every direction in $\\mathcal{H}$ if we assume $\\ell:\\mathcal{Y}\\times\\mathcal{Y}\\to\\mathbb{R}$ to be continuously differentiable on its first argument, with $\\nabla_{1}\\ell$ linearly growing. The differential (which is continuous) can be explicitly computed to be: ", "page_idx": 29}, {"type": "equation", "text": "$$\nD_{h}L(f)(h)=\\mathbb{E}_{\\pi}\\left[\\langle\\nabla_{1}\\ell\\left(\\left(f(X),Y\\right),h(X)\\rangle_{\\mathcal{Y}}\\right]\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "\u2022 $\\Phi\\ :\\ \\mathcal{Z}\\ \\ \\to\\ \\ \\mathcal{H}$ defined as $\\forall z\\ \\ \\in\\ \\ \\mathcal{Z},\\ \\ \\Phi(z)\\ \\ =\\ \\ \\sigma_{*}(\\cdot;z)$ , which satisfies $\\forall\\mu\\quad\\in$ $\\mathcal{P}(\\mathcal{Z}),\\;\\|\\langle\\Phi,\\mu\\rangle\\|_{\\mathcal{H}}<\\infty$ under the assumption of $\\sigma_{*}$ being bounded and continuous. ", "page_idx": 29}, {"type": "text", "text": "Corollary 5. We can explicitly compute the linear functional derivative and the intrinsic derivative for the learning problem\u2019s population risk: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\partial R}{\\partial\\mu}(\\mu,z)=\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\langle\\nabla_{1}\\ell\\left(\\langle\\sigma_{*}(X;\\cdot),\\mu\\rangle,Y\\right),\\sigma_{*}(X;z)\\rangle_{\\mathcal{Y}}\\right]+(c o n s t a n t\\,n o t\\,d e p e n d i n g\\,\\,o n\\,z)}\\\\ {\\displaystyle D_{\\mu}R(\\mu,z)=\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\nabla_{z}\\sigma_{*}(X;z).\\nabla_{1}\\ell(\\langle\\sigma_{*}(X;\\cdot),\\mu\\rangle,Y)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Beyond this particular example, the linear functional derivative and the intrinsic derivative behave well when the underlying functional is invariant, as shown by the following result: ", "page_idx": 29}, {"type": "text", "text": "Proposition 13. Let $F\\,:\\,\\mathcal{P}(\\mathcal{Z})\\,\\longrightarrow\\,\\mathbb{R}$ be invariant and of class $\\mathcal{C}^{1}$ . Then: $\\forall z\\in\\mathcal{Z}$ , $\\forall\\mu\\:\\in$ $\\mathcal{P}(\\bar{\\mathcal{Z}})$ , $\\forall g\\in G$ : ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{\\partial F}{\\partial\\mu}(M_{g}\\#\\mu,M_{g}.z)=\\frac{\\partial F}{\\partial\\mu}(\\mu,z)\\ \\ a n d\\ \\ D_{\\mu}F(M_{g}\\#\\mu,M_{g}.z)=M_{g}.D_{\\mu}F(\\mu,z)\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "i.e. $\\frac{\\partial F}{\\partial\\mu}$ is jointly invariant and $D_{\\mu}F$ jointly equivariant. ", "page_idx": 30}, {"type": "text", "text": "Proof of Proposition $^{l3}$ . To prove this, recall that the linear functional derivative of $F$ is the only function $\\begin{array}{r}{\\frac{\\partial F}{\\partial\\mu}:\\mathcal{P}(\\mathcal{Z})\\times\\mathcal{Z}\\rightarrow\\mathbb{R}}\\end{array}$ satisfying $\\forall\\mu,\\nu\\in\\mathcal{P}(\\mathcal{Z})$ : $\\operatorname*{lim}_{h\\to0}\\frac{F((1-h)\\mu+h\\nu)-F(\\mu)}{h}=\\int_{\\mathcal{Z}}\\frac{\\partial F}{\\partial\\mu}(\\mu,z)d(\\nu-\\mu)(z)\\ \\mathrm{~and~}\\ \\int_{\\mathcal{Z}}\\frac{\\partial F}{\\partial\\mu}(\\mu,z)d\\mu(z)=0.$ ", "page_idx": 30}, {"type": "text", "text": "In particular, as $F$ is $G$ -invariant (and $M_{g}$ linear), we can write $\\forall\\mu,\\nu\\in\\mathcal{P}(\\mathcal{Z}),\\forall h\\neq0$ and $g\\in G$ : ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{F((1-h)\\mu+h\\nu)-F(\\mu)}{h}=\\frac{F((1-h)(M_{g}\\#\\mu)+h(M_{g}\\#\\nu))-F(M_{g}\\#\\mu)}{h}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Taking the limit as $h\\to0$ on both sides, we get: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\operatorname*{lim}_{h\\to0}\\frac{F((1-h)\\mu+h\\nu)-F(\\mu)}{h}=\\int_{\\mathcal{Z}}\\frac{\\partial F}{\\partial\\mu}(M_{g}\\#\\mu,z)d(M_{g}\\#\\nu-M_{g}\\#\\mu)(z)}}\\\\ &{}&{\\qquad=\\displaystyle\\int_{\\mathcal{Z}}\\frac{\\partial F}{\\partial\\mu}(M_{g}\\#\\mu,M_{g}.z)d(\\nu-\\mu)(z),}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and also: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\int_{\\mathcal{Z}}\\frac{\\partial F}{\\partial\\mu}(M_{g}\\#\\mu,M_{g}.z)d\\mu(z)=\\int_{\\mathcal{Z}}\\frac{\\partial F}{\\partial\\mu}(M_{g}\\#\\mu,z)d M_{g}\\#\\mu(z)=0.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "So, by uniqueness, we get $\\forall g\\in G$ : ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{\\partial F}{\\partial\\mu}(\\mu,z)=\\frac{\\partial F}{\\partial\\mu}({\\cal M}_{g}\\#\\mu,{\\cal M}_{g}.z),\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and, from proposition 6 (since $\\frac{\\partial F}{\\partial\\mu}$ is jointly invariant), we get $\\forall g\\in G$ : ", "page_idx": 30}, {"type": "equation", "text": "$$\nD_{\\mu}F(M_{g}\\#\\mu,M_{g}.z)=\\nabla_{z}\\left({\\frac{\\partial F}{\\partial\\mu}}\\right)(M_{g}.\\mu,M_{g}.z)=M_{g}.\\nabla_{z}\\left({\\frac{\\partial F}{\\partial\\mu}}\\right)(\\mu,z)=M_{g}.D_{\\mu}F(\\mu,z)\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Finally, for convenience, let us denote by $H^{\\mathcal{E}^{G}}$ the function defined on $\\mathcal{P}(\\mathcal{Z})$ by $H^{\\varepsilon^{G}}(\\mu)\\;:=\\;$ $H_{\\lambda_{\\varepsilon^{G}}}(\\mu^{\\varepsilon^{G}})=H_{\\lambda_{\\varepsilon^{G}}}\\circ P_{\\mathcal{E}^{G}}\\#(\\mu)$ (presented in Section 3.3). We can straightforwardly compute its linear functional derivative and its intrinsic derivative in that space: ", "page_idx": 30}, {"type": "text", "text": "Example. (LFD and intrinsic derivative for $H^{\\mathcal{E}^{G}}$ ) By definition of the linear derivative $\\frac{\\partial H_{\\lambda_{\\mathcal{E}}G}}{\\partial\\eta}$ $H_{\\lambda_{\\varepsilon}G}$ on $\\mathcal{P}(\\mathcal{E}^{G})$ and the form we know it takes (see the examples from Appendix $D.I$ ), we see that, whenever $\\mu^{\\ensuremath{\\varepsilon}^{G}},\\nu^{\\ensuremath{\\varepsilon}^{G}}\\ll\\lambda_{\\ensuremath{\\varepsilon}^{G}}$ , we have: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\rightarrow0}{\\operatorname*{im}}\\frac{H_{\\lambda_{\\varepsilon}\\varepsilon}((1-h)\\mu^{\\varepsilon^{\\alpha}}+h\\nu^{\\varepsilon^{\\alpha}})-H_{\\lambda_{\\varepsilon}\\varepsilon}(\\mu^{\\varepsilon^{\\alpha}})}{h}=\\int_{\\varepsilon^{\\alpha}}\\frac{\\partial H_{\\lambda_{\\varepsilon}\\varepsilon}}{\\partial\\eta}(\\mu^{\\varepsilon^{\\alpha}},x)d(\\nu^{\\varepsilon^{\\alpha}}-\\mu^{\\varepsilon^{\\alpha}})(x)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\int_{\\varepsilon^{\\alpha}}\\left(\\log\\left(\\frac{d\\mu^{\\varepsilon^{\\alpha}}}{d\\lambda_{\\varepsilon^{\\alpha}}}(x)\\right)+C\\right)d(\\nu^{\\varepsilon^{\\alpha}}-\\mu^{\\varepsilon^{\\alpha}})(x)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\int_{\\varepsilon}\\left(\\log\\left(\\frac{d\\mu^{\\varepsilon^{\\alpha}}}{d\\lambda_{\\varepsilon^{\\alpha}}}(P_{\\varepsilon^{\\alpha}}.z)\\right)+C\\right)d(\\nu-\\mu)(z),}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "which yields that \u2202H\u2202\u00b5EG $\\begin{array}{r}{\\frac{\\partial H^{\\varepsilon^{G}}}{\\partial\\mu}(\\mu,z)=\\log\\left(\\frac{d\\mu^{\\varepsilon^{G}}}{d\\lambda_{\\varepsilon^{G}}}(P_{\\xi^{G}\\cdot}z)\\right)+C}\\end{array}$ (for $C$ an appropriate constant). A formal expression for the intrinsic derivative follows, which is given by : ", "page_idx": 30}, {"type": "equation", "text": "$$\nD_{\\mu}{H^{\\mathcal{E}}}^{G}(\\mu,z)=\\frac{1}{\\frac{d\\mu^{\\mathcal{E}}}{d{\\lambda_{\\mathcal{E}}}G}(z)}P_{\\mathcal{E}^{G}}^{T}\\nabla_{z}\\left[\\frac{d\\mu^{\\mathcal{E}}^{G}}{d{\\lambda_{\\mathcal{E}}}\\alpha}\\right](P_{\\mathcal{E}^{G}\\cdot\\mathcal{Z}})\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "D.2 Expression for the WGF of the regularized population risk ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In the case of the regularized population risk functional $R^{\\tau,\\beta}:{\\mathcal{P}}({\\mathcal{Z}})\\to\\mathbb{R}$ , we can explicitly write its intrinsic derivative. Consider a slightly more general functional, denoted by $R_{\\nu}^{\\tau,\\beta}$ , where the entropy is calculated against a Gibbs measure $\\nu\\ll\\lambda$ such that $\\nu(d z)=e^{-U(z)}\\bar{\\lambda}(d z)$ for some function $U:\\mathcal{Z}\\rightarrow\\mathbb{R}$ (as in [38]). We have $\\forall\\mu\\in\\mathcal{P}(\\mathcal{Z})$ s.t. $\\mu\\ll\\nu,\\forall z\\in\\mathcal{Z}$ : ", "page_idx": 31}, {"type": "equation", "text": "$$\nD_{\\mu}R_{\\nu}^{\\tau,\\beta}(\\mu,z)=D_{\\mu}R(\\mu,z)+\\tau\\nabla_{z}r(z)+\\beta\\nabla_{z}U(z)+\\beta\\left(\\frac{1}{\\mu(z)}\\nabla_{z}\\mu(z)\\right),\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "so that $\\mathbf{WGF}(R_{\\nu}^{\\tau,\\beta})$ as in definition 4 reads: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\partial_{t}\\mu_{t}=\\varsigma(t)\\left[\\mathrm{div}\\left(D_{\\mu}R_{\\nu}^{\\tau,\\beta}(\\mu_{t},\\cdot)\\,\\mu_{t}\\right)\\right]=\\varsigma(t)\\left[\\mathrm{div}\\left(\\left(D_{\\mu}R(\\mu_{t},\\cdot)+\\tau\\nabla_{z}r+\\beta\\nabla_{z}U\\right)\\mu_{t}\\right)+\\beta\\Delta\\mu_{t}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We recover the expression for $\\mathbf{WGF}(R^{\\tau,\\beta})$ in Equation (4) by considering $U\\equiv0$ : ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\partial_{t}\\mu_{t}=\\varsigma(t)\\left[\\mathrm{div}\\left((D_{\\mu}R(\\mu_{t},\\cdot)+\\tau\\nabla_{z}r)\\,\\mu_{t}\\right)+\\beta\\Delta\\mu_{t}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We can see that Equation (4) corresponds to a Fokker-Planck equation, which can be interpreted in terms of a non-linear SDE system, representing the behaviour of the type parameter: the McKeanVlasov SDE [11, 54, 70] (also known as the Mean Field Langevin Dynamics $(M F L D)$ in the NN literature). In the case of $R^{\\tau,\\beta}$ it reads: ", "page_idx": 31}, {"type": "equation", "text": "$$\nd Z_{t}=\\varsigma(t)\\left[-\\left(D_{\\mu}R(\\mu_{t},Z_{t})+\\tau\\nabla_{\\theta}r(Z_{t})\\right)d t+\\sqrt{2\\beta}d B_{t}\\right]\\ \\ \\mathrm{with}\\ \\ \\mu_{t}={\\bf L a w}(Z_{t}),\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $(B_{t})_{t\\geq0}$ is a $D$ -dimensional standard Brownian Motion. It is indeed standard to check (by applying It\u00f4\u2019s formula to $\\varphi(Z_{t},t)$ for $\\varphi$ a smooth function, and taking expectation) that $\\mu_{t}=\\mathbf{Law}(Z_{t})$ is a weak solution to (10). Under mild regularity conditions, both formulations are equivalent. See [11, 54, 70] for details. The previous correspondence also holds true when $\\beta=0$ (in which case (11) is an ODE). The process (11) or variants of it will prove useful to establish some of the relevant results of the paper. ", "page_idx": 31}, {"type": "text", "text": "D.3 Global convergence in the regularized case ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "For the example we just presented of the entropy-regularized population risk, multiple authors (see [12, 15, 38, 57, 69] among many others) have studied the properties of $\\mathbf{WGF}(R^{\\tau,\\beta})$ , particularly, the global convergence results that can be obtained. For instance, consider the following results from [38] (where we look at $R_{\\nu}^{\\tau,\\beta}$ for generality). First, define: ", "page_idx": 31}, {"type": "text", "text": "Definition 10. We say that a functional $R:\\mathcal{P}_{p}(\\mathcal{Z})\\to\\mathbb{R}$ is of class $\\mathcal{C}^{1}$ if $\\frac{\\partial R}{\\partial\\boldsymbol{\\mu}}\\big(\\boldsymbol{\\mu},\\cdot\\big)$ is well defined and bounded for every $\\mu\\in\\mathcal{P}_{p}(\\mathcal{Z})$ , and the function $\\begin{array}{r}{(\\mu,z)\\in\\mathcal{P}_{p}(\\mathcal{Z})\\times\\mathcal{Z}\\mapsto\\frac{\\dot{\\partial}R}{\\partial\\mu}(\\mu,z)}\\end{array}$ is continuous. ", "page_idx": 31}, {"type": "text", "text": "Now, from [15, 38], we get the following key result. We include the proof for completeness: ", "page_idx": 31}, {"type": "text", "text": "Lemma 3 (as in [38, 15]). Assume that $R:\\mathcal{P}_{p}(\\mathcal{Z})\\to\\mathbb{R}$ is convex and of class $\\mathcal{C}^{1}$ . Then, for any $\\mu,\\mu^{\\prime}\\in P_{p}(\\mathcal{Z}),$ , we have: ", "page_idx": 31}, {"type": "equation", "text": "$$\nR(\\mu^{\\prime})-R(\\mu)\\geq\\int_{\\mathcal{Z}}\\frac{\\partial R}{\\partial\\mu}(\\mu,z)d(\\mu^{\\prime}-\\mu)(z)\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof of Lemma 3 (from $[38],$ . Define $\\mu^{\\epsilon}:=(1-\\epsilon)\\mu+\\epsilon\\mu^{\\prime}$ . Since $R$ is convex, we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\epsilon\\left(R(\\mu^{\\prime})-R(\\mu)\\right)\\geq R(\\mu^{\\epsilon})-R(\\mu)=\\int_{0}^{\\epsilon}\\int_{\\mathcal{Z}}\\frac{\\partial R}{\\partial\\mu}(\\mu^{s},z)d(\\mu^{\\prime}-\\mu)(d z)\\,d s.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Since the map $s\\in[0,1]\\mapsto\\mu^{s}$ is continuous, it is of compact image (denoted $[\\mu,\\mu^{\\prime}])$ . In particular, as $\\frac{\\partial R}{\\partial\\mu}$ is continuous and bounded on its second argument, we get that, it is bounded on $[\\mu,\\mu^{\\prime}]\\times\\mathcal{Z}$ . The dominated convergence and Lebesgue differentiation theorems (as $\\varepsilon\\rightarrow0$ ) allow us to conclude. ", "page_idx": 31}, {"type": "text", "text": "Consider now the following assumption: ", "page_idx": 31}, {"type": "text", "text": "Assumption 2 (As in [38]). $U:\\mathcal{Z}\\rightarrow\\mathbb{R}$ is assumed to be ${\\mathcal{C}}^{\\infty}$ , with $\\nabla U$ Lipschitz continuous, and such that $\\exists C_{U}>0$ , $\\exists C_{U}^{\\prime}\\in\\mathbb{R}$ such that $\\forall x\\in\\mathcal{Z}:\\ \\nabla U(x)\\cdot x\\geq C_{U}\\|x\\|^{2}+C_{U}^{\\prime}$ . When required, we will also assume that $r:\\mathcal{Z}\\to\\mathbb{R}$ satisfies these conditions. ", "page_idx": 32}, {"type": "text", "text": "Notice that these conditions imply that $\\exists0\\,\\leq\\,C^{\\prime}\\,\\leq\\,C\\,\\,s.t.\\,\\ \\forall x\\,\\in\\,\\mathcal{Z},\\,C^{\\prime}\\|x\\|^{2}\\,-\\,C\\,\\leq\\,U(x)\\,\\leq$ $C(1+\\|x\\|^{2})$ (i.e. $U$ has quadratic growth) and $|\\Delta U(x)|\\le C$ ", "page_idx": 32}, {"type": "text", "text": "Since $R_{\\nu}^{\\tau,\\beta}$ includes an entropy term, it guarantees strict convexity, weak lower semicontinuity and compact sublevel sets (see e.g. [38] or [26]). On the other hand, assumption 2 implies that $U$ (or $r$ ) will have quadratic growth. Namely, we get (see [38] for a detailed proof): ", "page_idx": 32}, {"type": "text", "text": "Proposition 14 (Existence and Uniqueness of the minimizer (regularized case)). Let $R$ be convex, of class $\\mathcal{C}^{1}$ and bounded from below. Let \u03bd be the Gibbs measure with potential $U$ . Then, $R_{\\nu}^{\\tau,\\,\\beta}$ has a unique minimizer, $\\mu^{*,\\,\\tau,\\,\\beta,\\,\\nu}\\in\\mathcal{P}(\\mathcal{Z})$ , absolutely continuous with respect to Lebesgue measure $\\lambda$ When either $U$ or $r$ satisfies assumption 2, this minimizer also belongs to $\\mathcal{P}_{2}(\\mathcal{Z})$ . ", "page_idx": 32}, {"type": "text", "text": "For establishing global convergence results further assumptions are requred ", "page_idx": 32}, {"type": "text", "text": "Assumption 3 (Assumptions for well definedness (from [12] and [38])). Assume that the intrinsic derivative $D_{\\mu}R:\\mathcal{P}(\\mathcal{Z})\\overset{}{\\times}\\mathcal{Z}\\rightarrow\\mathcal{Z}$ of the functional $R:\\mathcal{P}(\\mathcal{Z})\\to\\mathbb{R}$ exists and satisfies either one of the following: ", "page_idx": 32}, {"type": "text", "text": "1. (From $[38]$ ). Assume: ", "page_idx": 32}, {"type": "text", "text": "\u2022 $D_{\\mu}R$ is bounded and Lipschitz continuous, i.e. $\\exists C_{R}>0\\;s.t.\\;\\forall z,z^{\\prime}\\in\\mathcal{Z},\\;\\forall\\mu,\\mu^{\\prime}\\in$ $\\mathcal{P}_{2}(\\mathcal{Z})$ , $|D_{\\mu}R(\\mu,z)-D_{\\mu}R(\\mu^{\\prime},z^{\\prime})|\\leq C_{R}[|z-z^{\\prime}|+W_{2}(\\mu,\\mu^{\\prime})]$ \u2022 $\\forall\\mu\\in\\mathcal{P}(\\mathcal{Z})$ , $D_{\\mu}R(\\mu,\\cdot)\\in C^{\\infty}(\\mathcal{Z}).$ . \u2022 $\\nabla D_{\\mu}R:{\\mathcal{P}}({\\mathcal{Z}})\\times{\\mathcal{Z}}\\to{\\mathcal{Z}}\\times{\\mathcal{Z}}$ is jointly continuous. 2. (From $I I2J,$ , who relax some differentiability conditions at the cost of boundedness assumptions; this allows them to avoid altogether the coercivity condition from assumption 2, which is used in $[38]$ ): \u2022 $\\forall x\\;\\in\\;\\mathcal{Z},\\forall m,m^{\\prime}\\:\\in\\:\\mathcal{P}_{2}(\\mathcal{Z}),|D_{\\mu}R(m,x)-D_{\\mu}R(m^{\\prime},x)|\\;\\leq\\;M_{m m}^{R}W_{1}(m,m^{\\prime})$ for some constant $M_{m m}^{R}\\geq0$ (i.e. it is lipschitz on the measure argument). \u2022 Suppose that $\\operatorname*{sup}_{\\mu\\in\\mathcal{P}_{2}(\\mathcal{Z})}\\operatorname*{sup}_{x\\in\\mathcal{Z}}|\\nabla D_{\\mu}R(\\mu,x)|\\le M_{m x}^{R}$ for some constant $M_{m x}^{R}\\ge0$ i.e. $\\nabla D_{\\mu}R(\\mu,x)$ is uniformly bounded. ", "page_idx": 32}, {"type": "text", "text": "This allows to establish a traditional global convergence result from the MF Theory of NNs: ", "page_idx": 32}, {"type": "text", "text": "Theorem 7 (from [12] and [38]). Let $\\mu_{0}\\in\\mathscr{P}_{2}(\\mathcal{Z})$ , and let assumption 2 and 3 hold; then: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\forall t>0,\\ \\ \\frac{d}{d t}(R_{\\nu}^{\\tau,\\beta}(\\mu_{t}))=-c(t)\\int_{\\mathcal{Z}}\\left|D_{\\mu}R(\\mu_{t},z)+\\tau\\nabla r(z)+\\beta\\frac{\\nabla u_{t}}{u_{t}}(z)+\\beta\\nabla U(z)\\right|^{2}\\,d\\mu_{t}(z)\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $u_{t}$ denotes the density of $\\mu_{t}\\ensuremath{:=}\\ensuremath{L\\!a w}(X_{t})$ , the solution to equation (4). i.e. following the WGF makes the regularized risk decrease at a known rate. This is known as the energy dissipation equation. ", "page_idx": 32}, {"type": "text", "text": "Remark. Notice that this equation can be rewritten using the Fisher divergence (or relative Fisher Information) between two measures. This quantity is defined as: ", "page_idx": 32}, {"type": "equation", "text": "$$\nI(\\mu||\\nu):=\\int_{\\mathcal{Z}}\\left\\|\\nabla\\log(\\frac{d\\mu}{d\\nu}(z))\\right\\|^{2}d\\mu(z)\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Then, almost by definition, we get: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\frac{d}{d t}(R_{\\nu}^{\\tau,\\beta}(\\mu_{t}))=-\\beta^{2}\\varsigma(t)I(\\mu_{t}||\\hat{\\mu}_{t})\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "This allows us to characterize the stationary points of the dynamic explicitly, as done in [12, 38, 57, 69]. ", "page_idx": 32}, {"type": "text", "text": "Theorem 7 implies that the WGF converges to the unique global optimizer of the regularized problem: ", "page_idx": 33}, {"type": "text", "text": "Theorem 8 (from [38]). Let $R$ be convex, bounded from below and $\\mathcal{C}^{1}$ ; also assume that assumption 2 and 3 hold. Consider $\\mu_{0}\\in\\cup_{p>2}\\mathcal{P}_{p}(\\mathcal{Z})$ and let $(\\mu_{t})_{t\\geq0}$ be the $W G F(R_{\\nu}^{\\tau,\\beta})$ starting from $\\mu_{0}$ . Then, the equation has a stationary distribution, $\\mu_{\\infty}$ , that satisfies: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mu_{\\infty}:=\\arg\\operatorname*{min}_{\\mu\\in\\mathcal{P}(\\mathcal{Z})}R_{\\nu}^{\\tau,\\,\\beta}(\\mu)\\;\\;a n d\\;\\;\\operatorname*{lim}_{t\\rightarrow\\infty}W_{2}(\\mu_{t},\\mu_{\\infty})=0\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Remark. Global Convergence Results such as Theorem 7 or Theorem 8 have been established as early as in [53] (for the quadratic loss). However, settings such as those of [12, 38, 57, 69, 15] are of notorious interest to establish essentially the same results under fundamentally more general assumptions. ", "page_idx": 33}, {"type": "text", "text": "Making further technical assumptions on our regularized functionals leads to better convergence results. Namely, consider the following definition: ", "page_idx": 33}, {"type": "text", "text": "Definition 11. We say $\\mu\\in{\\mathcal{P}}({\\mathcal{Z}})$ satisfies the Log-Sobolev Inequality with constant $\\vartheta>0$ (in short, $L S I(\\vartheta),$ , if for any $\\nu\\in\\mathcal{P}(\\mathcal{Z})$ such that $\\nu\\ll\\mu$ , we have: ", "page_idx": 33}, {"type": "equation", "text": "$$\nD(\\nu||\\mu):=\\int_{\\mathcal{Z}}\\log(\\frac{d\\nu}{d\\mu}(z))d\\nu(z)\\leq\\frac{1}{2\\vartheta}\\int_{\\mathcal{Z}}\\left\\|\\nabla\\log(\\frac{d\\nu}{d\\mu}(z))\\right\\|^{2}d\\nu(z)=:\\frac{1}{2\\vartheta}I(\\nu||\\mu)\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $D(\\nu||\\mu)$ is the $K L$ divergence and $I(\\mu||\\nu)$ is the Fisher divergence. ", "page_idx": 33}, {"type": "text", "text": "(see [3, 58] for background on functional inequalities and [15, 12, 57, 69] for applications of it to the NN context). In our setting, as done by most authors in recent years to achieve the desired global convergence results, the following \u2018uniform-LSI\u2019 on the functional $R:\\mathcal{P}(\\mathcal{Z})\\to\\mathbb{R}$ is assumed to hold: ", "page_idx": 33}, {"type": "text", "text": "Assumption 4 (Uniform LSI from [15, 12, 57, 69]). There exists $\\vartheta>0$ such that $\\forall\\mu\\in\\mathcal{P}_{2}(\\mathcal{Z}),$ , $\\hat{\\mu}$ satisfies $L S I(\\vartheta)$ . Here $\\hat{\\mu}$ is the probability measure with density w.r.t. $\\lambda$ given by (slightly abusing notation, and considering $U$ the potential of a Gibbs measure $\\nu$ used in the entropy, which is $0~i f$ $\\nu=\\lambda$ ): ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\hat{\\mu}(z)\\propto\\exp\\left(-\\frac{1}{\\beta}\\frac{\\partial R}{\\partial\\mu}(\\mu,z)-\\frac{\\tau}{\\beta}r(z)-U(z)\\right)\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Remark. This LSI is a recurrent element in the literature of WGF and Optimal Transport in general. In particular, it implies (see $[3])$ Poincar\u00e9 Inequality: $\\forall\\phi\\in\\mathcal{C}_{b}^{1}(\\mathcal{Z})$ , $\\begin{array}{r}{\\mathrm{Var}_{\\hat{\\mu}}(\\phi)\\leq\\frac{1}{2\\vartheta}\\mathring{\\mathbb{E}}_{\\hat{\\mu}}[|\\nabla\\phi|^{2}]}\\end{array}$ , and ([58]) the Talagrand\u2019s $T_{2}$ -transport inequality as well: $\\forall\\nu\\in\\mathcal{P}_{2}(\\mathcal{Z})$ , $\\vartheta W_{2}^{2}(\\nu,\\hat{\\mu})\\leq D(\\nu||\\hat{\\mu})$ ", "page_idx": 33}, {"type": "text", "text": "Beyond the characterization of the decay (from [38]), we have the following guarantee: ", "page_idx": 33}, {"type": "text", "text": "Theorem 9 (from [12, 15]). Let $R$ be convex, $\\mathcal{C}^{1}$ and bounded from below, and let assumptions $^3$ and 4 hold. Then, if for some $t_{0}\\geq0$ , $\\mu_{t_{0}}$ has finite entropy and finite second moment; then $\\forall t\\geq t_{0}$ , ", "page_idx": 33}, {"type": "equation", "text": "$$\nD(\\mu_{t}||\\mu_{\\infty})\\leq R_{\\nu}^{\\tau,\\beta}(\\mu_{t})-R_{\\nu}^{\\tau,\\beta}(\\mu_{\\infty})\\leq(R_{\\nu}^{\\tau,\\beta}(\\mu_{t_{0}})-R_{\\nu}^{\\tau,\\beta}(\\mu_{\\infty}))e^{-2\\beta\\vartheta\\int_{t_{0}}^{t}\\varsigma(s)d s}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $\\begin{array}{r}{\\mu_{\\infty}=\\mu^{\\tau,\\,\\beta,\\,\\nu}=\\arg\\operatorname*{min}_{\\mu\\in\\mathcal{P}(\\mathcal{Z})}R_{\\nu}^{\\tau\\,\\beta}(\\mu)}\\end{array}$ . That is, the value function following the WGF thus converges exponentially fast to the optimum value of the problem, and this implies an exponential convergence in relative entropy. ", "page_idx": 33}, {"type": "text", "text": "One thus recovers, under the right technical assumptions, a version of Theorem 4 from [53] and actually a quantitative improvement of it. By Talagrand\u2019s inequality this also implies exponential $W_{2}$ convergence of $\\mu_{t}$ to $\\mu_{\\infty}$ . We note that the result in [12] is established in the setting with $\\tau=0,\\beta=1$ and $\\varsigma\\equiv1$ ; however, one can show that the result holds as stated by using standard arguments. ", "page_idx": 33}, {"type": "text", "text": "D.4 Conditions for well-posedness of WGF ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Most of the technical conditions that will be here presented are directly taken from both [16] and [22]. We only adapt them slightly to fit into our notation. ", "page_idx": 33}, {"type": "text", "text": "Regarding the existence of weak solutions to the WGF presented in equation (4), [16] are able to guarantee it under the following assumptions (more general assumptions might be sought in [1, 63], but these are relatively standard in the MF context): ", "page_idx": 33}, {"type": "text", "text": "Assumption 5 (Assumptions for existence and uniqueness of the WGF solutions (taken from [16])). Consider a setting as described in proposition $^{12}$ , with $R(\\mu):=L(\\langle\\Phi,\\mu\\rangle)+V(\\mu),$ , with $V(\\mu)=$ $\\tau\\int_{\\mathcal{Z}}r d\\mu$ . ", "page_idx": 34}, {"type": "text", "text": "1. Let $\\mathcal{Z}$ to be the closure of a convex open set within some finite-dimensional euclidean space.   \n2. Let $L:\\mathcal{H}\\to\\mathbb{R}^{+}$ be differentiable, with a differential dL that is Lipschitz on bounded sets and bounded on sublevel sets.   \n3. Let $\\Phi:\\mathcal{Z}\\to\\mathcal{H}$ be differentiable and $V:\\mathcal{Z}\\to\\mathbb{R}^{+}$ be semiconvex (i.e. $\\exists\\lambda\\in\\mathbb{R}:\\,V\\!+\\!\\lambda|\\cdot\\!|^{2}$ is convex).   \n4. There exists a family $(Q_{r})_{r>0}$ of nested nonempty closed convex subsets of $\\mathcal{Z}$ such that: (a) $\\{u\\in\\Omega;d i s t(u,Q_{r^{\\prime}})\\leq r\\}\\subset Q_{r+r^{\\prime}}$ for all $r,r^{\\prime}>0,$ , (b) $\\Phi$ and $V$ are bounded, and $d\\Phi$ is Lipschitz on each $Q_{r}$ (c) $\\exists C_{1},C_{2}\\,>0$ such that $\\begin{array}{r}{\\operatorname*{sup}_{u\\in Q_{r}}(\\|d\\Phi(u)\\|+\\|\\partial V(u)\\|)\\leq C_{1}+C_{2}r}\\end{array}$ for all $r\\,>\\,0$ , where $\\lVert\\partial V(u)\\rVert$ stands for the maximal norm of an element in $\\partial V(u)$ . ", "page_idx": 34}, {"type": "text", "text": "On the other hand, [22] are able to prove (based on Theorem 1.1. from [70]) the existence of strong solutions with pathwise uniqueness for McKean-Vlasov SDE given by ", "page_idx": 34}, {"type": "equation", "text": "$$\nd Z_{t}=b(t,Z_{t},\\mu_{t})d t+\\Sigma(t,Z_{t},\\mu_{t})d B_{t}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $b$ and $\\Sigma$ satisfies the conditions of B2 (presented below) and for all $t\\geq0$ , $\\mu_{t}=\\mathbf{Law}(Z_{t})\\in$ $\\mathcal{P}_{2}(\\mathbb{R}^{D}),\\,(B_{t})_{t\\geq0}$ is an $r$ -dimensional Brownian motion (with $r\\,\\in\\,\\mathbb{N}^{*}$ potentially different from $D\\,\\in\\,\\mathbb{N}^{*}.$ ), and $Z_{0}$ has the (fixed) law $\\mu^{0}\\,\\in\\,\\mathcal{P}_{2}(\\mathbb{R}^{D})$ . For this, consider the following technical assumptions (B1 and B2) which have been taken directly from [22]: ", "page_idx": 34}, {"type": "text", "text": "Assumption 6 (Assumptions for the existence and uniqueness of solutions in [22]). Consider: ", "page_idx": 34}, {"type": "text", "text": "B1. There exist a measurable function $g:\\mathbb{R}^{D}\\times\\mathcal{W}\\to\\mathbb{R},$ , $M_{1}\\geq0$ and $\\mu_{0}\\in\\mathcal{P}_{2}(\\mathbb{R}^{D})$ such that for any $N\\in\\mathbb{N}$ , the following hold. ", "page_idx": 34}, {"type": "text", "text": "(a) For any $w_{1},w_{2}\\in\\mathbb{R}^{D}$ and $z\\in\\mathcal{W}$ we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\|g(w_{1},z)-g(w_{2},z)\\|\\leq\\zeta(z)\\|w_{1}-w_{2}\\|,\\ \\ a n d\\ \\|g(w_{1},z)\\|\\leq\\zeta(z)\n$$", "text_format": "latex", "page_idx": 34}, {"type": "equation", "text": "$\\textstyle\\int_{\\mathcal W}\\zeta^{2}(z)\\,d\\pi_{\\mathcal W}(z)<+\\infty$ ", "text_format": "latex", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{t\\geq0}{\\operatorname*{sup}}\\Big\\{\\|b_{N}(t,w1,\\mu1)-b_{N}(t,w2,\\mu2)\\|+\\|\\Sigma_{N}(t,w_{1},\\mu_{1})-\\Sigma_{N}(t,w_{2},\\mu_{2})\\|\\Big\\}}\\\\ &{\\quad\\leq M_{1}\\bigg(\\|w_{1}-w_{2}\\|+\\bigg(\\displaystyle\\int_{\\mathcal{W}}\\displaystyle\\int_{\\mathbb{R}^{D}}|\\langle g(\\cdot,z),\\mu_{1}\\rangle-\\langle g(\\cdot,z),\\mu_{2}\\rangle|^{2}\\,d\\pi_{\\mathcal{W}}(z)\\bigg)^{1/2}\\,\\bigg\\}\\bigg),}\\\\ &{\\underset{t\\geq0}{\\operatorname*{sup}}\\{\\|b_{N}(t,0,\\mu_{0})\\|+\\|\\Sigma_{N}(t,0,\\mu_{0})\\|\\}\\leq M_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "B2. There exist $M_{2}\\ge0,$ , $\\kappa>0$ , $b\\in C(\\mathbb{R}_{+}\\times\\mathbb{R}^{D}\\times\\mathcal{P}_{2}(\\mathbb{R}^{D}),\\mathbb{R}^{D})$ and $\\Sigma\\in C(\\mathbb{R}_{+}\\times\\mathbb{R}^{D}\\times$ $\\mathcal{P}_{2}(\\mathbb{R}^{D}),\\mathbb{R}^{D\\times r})$ such that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t\\geq0,w\\in\\mathbb{R}^{D},\\mu\\in\\mathcal{P}_{2}(\\mathbb{R}^{D})}\\left\\{\\|b_{N}(t,w,\\mu)-b(t,w,\\mu)\\|+\\|\\Sigma_{N}(t,w,\\mu)-\\Sigma(t,w,\\mu)\\|\\right\\}\\leq M_{2}N^{-\\kappa}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Proposition 15 (Proposition 11 in [22]). Assuming $\\mathbf{\\deltaB}$ and $\\mathbf{mathit{B2}}$ . Given $\\mu^{0}\\,\\in\\,\\mathcal{P}_{2}(\\mathbb{R}^{D})$ as a fixed initial condition; then, there exists an $(\\mathcal{F}_{t})_{t\\geq0}$ -adapted process $(Z_{t})_{t\\geq0}$ that is the unique (pathwise) strong solution of the McKean-Vlasov $S D E$ : ", "page_idx": 34}, {"type": "equation", "text": "$$\nd Z_{t}=b(t,Z_{t},\\mu_{t})d t+\\Sigma(t,Z_{t},\\mu_{t})d B_{t}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Additionally, it satisfies for each $T\\geq0$ : $\\mathrm{sup}_{t\\in[0,T]}\\,\\mathbb{E}[\\|Z_{t}\\|^{2}]<\\infty$ ", "page_idx": 34}, {"type": "text", "text": "E Proofs and discussions of main results ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "E.1 Proofs of Section 3.1 ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Proof of Proposition $^{\\,l}$ . By definition of the symmetrization operator, we know that $\\forall x\\in\\mathcal{X}$ : ", "page_idx": 35}, {"type": "equation", "text": "$$\n({\\mathcal{Q}}_{G}\\Phi_{\\mu})(x)=\\int_{G}{\\hat{\\rho}}_{g^{-1}}\\Phi_{\\mu}(\\rho_{g}x)d\\lambda_{G}(g)\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "For $g\\in G$ , since $\\sigma_{*}$ is equivariant and $M_{g}$ is invertible, we can write: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\Phi_{\\mu}(\\rho_{g}x)=\\langle\\sigma_{*}(\\rho_{g}x,\\cdot),\\mu\\rangle=\\langle\\sigma_{*}(\\rho_{g}x,\\cdot),M_{g}\\#(M_{g}^{-1}\\#\\mu)\\rangle=\\hat{\\rho}_{g}\\langle\\sigma_{*}(x,\\cdot),M_{g}^{-1}\\#\\mu\\rangle\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where we\u2019ve used Proposition 7 in the last equality. In turn, we can write (via the inversion-invariance of $\\lambda_{G}$ ): ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{(\\mathcal{Q}_{G}\\Phi_{\\mu})(x)=\\displaystyle\\int_{G}\\hat{\\rho}_{g^{-1}}\\hat{\\rho}_{g}\\langle\\sigma_{*}(x,\\cdot),M_{g}^{-1}\\#\\mu\\rangle d\\lambda_{G}(g)=\\displaystyle\\int_{G}\\langle\\sigma_{*}(x,\\cdot),M_{g^{-1}}\\#\\mu\\rangle d\\lambda_{G}(g)}}\\\\ {{\\mathrm{~}}}\\\\ {{\\mathrm{~}=\\displaystyle\\int_{G}\\langle\\sigma_{*}(x,\\cdot),M_{g}\\#\\mu\\rangle d\\lambda_{G}(g)=\\langle\\sigma_{*}(x,\\cdot),\\mu^{G}\\rangle=\\Phi_{\\mu}\\alpha(x)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "As mentioned in Section 3.1, a simple case where we will have $\\Phi_{\\mu^{G}}=\\Phi_{\\mu^{\\varepsilon^{G}}}$ is when $\\sigma_{*}$ is linear: ", "page_idx": 35}, {"type": "text", "text": "Proposition 16. If $\\sigma_{*}:\\mathcal{X}\\times\\mathcal{Z}\\rightarrow\\mathcal{Y}$ is jointly equivariant and $\\pi_{\\mathcal{X}^{-}\\!a.s.}\\,\\forall x\\in\\mathcal{X}$ , $[z\\mapsto\\sigma_{*}(x;z)]$ is $a$ bounded linear operator, then, for any \u00b5 \u2208P(Z): \u03a6\u00b5G = \u27e8\u03c3\u2217, \u00b5G\u27e9= \u27e8\u03c3\u2217, \u00b5EG\u27e9= \u03a6\u00b5EG . ", "page_idx": 35}, {"type": "text", "text": "Proof of Proposition 16. A straightforward computation yields (using Fubini\u2019s theorem and the linearity of integrals and $\\sigma_{*}$ ), $\\forall\\mu\\in\\mathcal{P}(\\mathcal{Z})$ , $\\forall x\\in\\mathcal{X}$ $\\mathcal{X}\\left(\\pi_{\\mathcal{X}}.\\right.$ $\\pi_{\\mathcal{X}^{-\\mathrm{{a.s.}}}})$ : ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\langle\\sigma_{*}(x,\\cdot),\\mu^{G}\\rangle=\\int_{G}\\int_{\\mathcal{Z}}\\sigma_{*}(x,M_{g}.z)d\\mu(z)d\\lambda_{G}(g)=\\int_{\\mathcal{Z}}\\int_{G}\\sigma_{*}(x,M_{g}.z)d\\lambda_{G}(g)d\\mu(z)}}\\\\ &{}&{\\qquad=\\int_{\\mathcal{Z}}\\sigma_{*}(x,\\int_{G}M_{g}.z\\,d\\lambda_{G}(g))d\\mu(z)=\\int_{\\mathcal{Z}}\\sigma_{*}(x,P_{\\mathcal{E}^{G}}.z)d\\mu(z)=\\langle\\sigma_{*}(x,\\cdot),\\mu^{\\mathcal{E}^{G}}\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Example. Any usual linear model written in terms of some feature function $\\vartheta:\\mathcal{X}\\rightarrow\\mathcal{Z}$ (where $\\mathcal{Z}$ is possibly an Reproducing Kernel Hilbert Space) enters this framework, by defining: $\\sigma_{*}(x,z)=$ $\\langle z,\\vartheta(x)\\rangle$ . This won\u2019t satisfy Assumption $^{\\,l}$ , since it is not bounded; but it still serves as an illustration. ", "page_idx": 35}, {"type": "text", "text": "E.2 Proofs of results in Section 3.2 ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "E.2.1 Proof of Proposition 2 ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Now consider, as a shorthand notation, $\\forall x\\in\\mathcal{X}$ , $\\forall y\\in\\mathcal{Y}$ the functional $L_{x,y}:\\mathcal{P}(\\mathcal{Z})\\to\\mathbb{R}$ given by $\\forall\\mu\\in\\mathcal{P}(\\mathcal{Z});\\,L_{x,y}(\\mu)=\\ell\\left(\\Phi_{\\mu}(x),y\\right)$ . The following lemma that shall be useful for later stages. ", "page_idx": 35}, {"type": "text", "text": "Lemma 4. Let $\\sigma_{*}$ be jointly equivariant and $\\ell$ be invariant. Then, $\\forall g\\in G,\\,\\forall x\\in\\mathcal{X},\\,\\forall y\\in\\mathcal{y},\\,\\forall\\mu\\in$ $\\mathcal{P}(\\mathcal{Z})$ , ", "page_idx": 35}, {"type": "equation", "text": "$$\nL_{\\rho_{g}.x,\\hat{\\rho}_{g}.y}(M_{g}\\#\\mu)=L_{x,y}(\\mu)\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Equivalently, the map $L\\,:\\,{\\mathcal{P}}({\\mathcal{Z}})\\,\\rightarrow\\,L^{2}({\\mathcal{X}}\\,\\times\\,{\\mathcal{Y}},\\pi)$ given by $L(\\mu)\\,\\,\\mapsto\\,\\,[(x,y)\\,\\,\\mapsto\\,\\,L_{x,y}(\\mu)]$ is equivariant (under the appropiate9 $G$ -actions). ", "page_idx": 35}, {"type": "text", "text": "Proof of Lemma 4. Using the joint equivariance of $\\sigma_{*}$ (via proposition 7) and the invariance of $\\ell$ , a straightforward computation yields, for all $x\\in\\mathcal{X},\\;y\\in\\mathcal{Y}$ , and $g\\in G$ : ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{\\rho_{g}.x,\\hat{\\rho}_{g}.y}(M_{g}\\#\\mu)=\\ell\\left(\\langle\\sigma_{*}(\\rho_{g}.x,\\cdot),M_{g}\\#\\mu\\rangle,\\hat{\\rho}_{g}.y\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\ell\\left(\\hat{\\rho}_{g}.\\langle\\sigma_{*}(x,\\cdot),\\mu\\rangle,\\hat{\\rho}_{g}.y\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\ell\\left(\\langle\\sigma_{*}(x,\\cdot),\\mu\\rangle,y\\right)=L_{x,y}(\\mu)}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "9In this case, $\\forall g\\:\\in\\:G$ , let $g.\\mu\\,=\\,M_{g}\\#\\mu$ and $g.f\\ =\\ f^{g}$ given by $\\forall x\\;\\in\\;\\mathcal{X},\\;\\;\\forall y\\;\\in\\;\\mathcal{Y},\\;\\;f^{g}(x,y)\\;=$ $f(\\rho_{g}^{-1}.x,\\hat{\\rho}_{g}^{-1}.y)$ ", "page_idx": 35}, {"type": "text", "text": "With this we can prove Proposition 2. Notice that we will basically utilize the equivariance properties of $\\sigma_{*}$ and $\\ell$ in Assumption 1 (the convexity of the functions comes directly from the convexity of $R$ when $\\ell$ is convex, together with the linearity of $(\\cdot)^{G},\\,(\\cdot)^{\\mathcal{E}^{G}}$ and $\\textstyle\\int_{G}(\\cdot)d\\lambda_{G}.)$ . ", "page_idx": 36}, {"type": "text", "text": "Proof of Proposition 2. We can readily see that: $R^{E A}(\\mu)=\\mathbb{E}_{\\pi}\\left[\\ell\\left(\\Phi_{\\mu^{\\varepsilon G}}(X),Y\\right)\\right]=R(\\mu^{\\varepsilon^{G}}).$ On the other hand, as $\\sigma_{*}$ is jointly equivariant, from Proposition 1, we have: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{R^{F A}(\\mu)=\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\ell\\left(\\mathcal{Q}_{G}(\\Phi_{\\boldsymbol\\mu})(\\boldsymbol X),\\boldsymbol Y\\right)\\right]=\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\ell\\left(\\Phi_{\\boldsymbol\\mu^{G}}(\\boldsymbol X),\\boldsymbol Y\\right)\\right]=R(\\boldsymbol\\mu^{G})}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Next, using Lemma 4, Fubini\u2019s theorem and the inversion-invariance of $\\lambda_{G}$ , we get: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R^{D A}(\\mu)=\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\int_{G}\\ell\\left(\\Phi_{\\mu}(\\rho_{g}.X),\\hat{\\rho}_{g}.Y\\right)d\\lambda_{G}(g)\\right]=\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\int_{G}L_{\\rho_{g}.X,\\hat{\\rho}_{g}.Y}(\\mu)d\\lambda_{G}(g)\\right]}\\\\ &{\\quad\\quad\\quad=\\int_{G}\\mathbb{E}_{\\boldsymbol\\pi}\\left[L_{\\rho_{g}.X,\\hat{\\rho}_{g}.Y}(\\mu)\\right]d\\lambda_{G}(g)=\\int_{G}\\mathbb{E}_{\\boldsymbol\\pi}\\left[L_{\\rho_{g}.X,\\hat{\\rho}_{g}.Y}(M_{g}\\#M_{g}^{-1}\\#\\mu)\\right]d\\lambda_{G}(g)}\\\\ &{\\quad\\quad\\quad=\\int_{G}\\mathbb{E}_{\\boldsymbol\\pi}\\left[L_{X,Y}(M_{g^{-1}}\\#\\mu)\\right]d\\lambda_{G}(g)=\\int_{G}\\mathbb{E}_{\\boldsymbol\\pi}\\left[L_{X,Y}(M_{g}\\#\\mu)\\right]d\\lambda_{G}(g)}\\\\ &{\\quad\\quad\\quad=\\int_{G}R(M_{g}\\#\\mu)d\\lambda_{G}(g)=R^{G}(\\mu)}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "From these expressions we can quickly verify that $R^{D A}$ , $R^{F A}$ and $R^{E A}$ are invariant. Namely, by Lemma 1, for $g\\in G$ and $\\mu\\in{\\mathcal{P}}({\\mathcal{Z}})$ we have: $\\mathring{R}^{F A}(M_{g}\\#\\mu)=R((M_{g}\\#\\mu)^{G})=R(\\mu^{G})=R^{F\\check{A}}(\\mu),$ , and: $R^{E A}(M_{g}\\#\\mu)\\;=\\;R((M_{g}\\#\\mu)^{\\varepsilon^{G}})\\;=\\;R(\\mu^{\\varepsilon^{G}})\\;=\\;R^{E A}(\\mu)$ . On the other hand, the rightinvariance of $\\lambda_{G}$ implies: ", "page_idx": 36}, {"type": "equation", "text": "$$\nR^{D A}(M_{g}\\#\\mu)=\\int_{G}R((M_{h}\\#(M_{g}\\#\\mu))d\\lambda_{G}(h)=\\int_{G}R((M_{\\tilde{h}}\\#\\mu))d\\lambda_{G}(\\tilde{h})=R^{D A}(\\mu).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "We can also see that whenever $R$ is invariant, we have that, for $\\mu\\in{\\mathcal{P}}({\\mathcal{Z}})$ and $g\\in G,R(M_{g}\\#\\mu)=$ $R(\\mu)$ , so that: $\\begin{array}{r}{R^{D A}(\\mu)=R^{G}(\\mu)=\\int_{G}R(M_{g}\\mathcal{H}\\mu)d\\lambda_{G}(g)=\\int_{G}R(\\mu)d\\lambda_{G}(g)=R(\\mu)}\\end{array}$ . ", "page_idx": 36}, {"type": "text", "text": "Also, if $\\mu~\\in~\\mathcal{P}^{G}(\\mathcal{Z})$ , we have, for all $g\\ \\in\\ G$ , $\\mu\\ =\\ \\mu^{G}\\ =\\ M_{g}\\#\\mu$ , so that: $R^{D A}(\\mu)\\;=\\;$ $\\begin{array}{r}{\\int_{G}R(M_{g}\\#\\mu)d\\lambda_{G}(g)=\\int_{G}R(\\mu)d\\lambda_{G}(g)=R(\\mu)=R(\\mu^{G})=R^{F A}(\\bar{\\mu})}\\end{array}$ . ", "page_idx": 36}, {"type": "text", "text": "Finally, we verify that our population risk $R:\\mathcal{P}(\\mathcal{Z})\\to\\mathbb{R}$ is invariant whenever $\\pi\\in\\mathcal P^{G}(\\mathcal X\\times\\mathcal P)$ . Indeed, $\\forall g\\in G$ and $\\forall\\mu\\in\\mathcal{P}(\\mathcal{Z})$ , by the invariance of $\\ell$ and $\\pi$ , together with Proposition 7 from the equivariance of $\\sigma_{*}$ , we get: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R(M_{g}\\#\\mu)=\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\ell(\\langle\\sigma_{*}(X;\\cdot),M_{g}\\#\\mu\\rangle,Y)\\right]=\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\ell(\\hat{\\rho}_{g}\\langle\\sigma_{*}(\\rho_{g}^{-1}X;\\cdot),\\mu\\rangle,\\hat{\\rho}_{g}\\hat{\\rho}_{g}^{-1}Y)\\right]}\\\\ &{\\quad\\quad\\quad\\quad=\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\ell(\\langle\\sigma_{*}(\\rho_{g}^{-1}X;\\cdot),\\mu\\rangle,\\hat{\\rho}_{g}^{-1}Y)\\right]=\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\ell(\\langle\\sigma_{*}(X;\\cdot),\\mu\\rangle,Y)\\right]=R(\\mu)}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "That is, $R$ is invariant. ", "page_idx": 36}, {"type": "text", "text": "Notice that the equivariance of the data distribution $\\pi$ can also make the regularized population risk be invariant, under the right choice of $r$ . Namely: ", "page_idx": 36}, {"type": "text", "text": "Corollary 6. If $R:\\mathcal{P}(\\mathcal{Z})\\to\\mathbb{R}$ and $r:\\mathcal{Z}\\to\\mathbb{R}$ are invariant (in their respective sense), then $R^{\\tau,\\beta}$ is invariant. ", "page_idx": 36}, {"type": "text", "text": "The result can be proven for $R_{\\nu}^{\\tau,\\beta}$ with $\\nu$ some $G$ -invariant measure (such as $\\lambda$ for orthogonal representations). Notice that $r(\\theta)\\ =\\ \\lVert\\theta\\rVert^{2}$ is an example of invariant function for orthogonal representations. ", "page_idx": 36}, {"type": "text", "text": "Proof of Corollary $^{6}$ . It is enough to notice that $\\begin{array}{r}{V(\\mu)=\\int_{\\mathcal{Z}}r(\\theta)d\\mu(\\theta)}\\end{array}$ and $\\begin{array}{r}{H_{\\nu}(\\mu)=\\int_{\\mathcal{Z}}\\log(\\frac{d\\mu}{d\\nu})d\\mu}\\end{array}$ (with $\\mu\\ll\\nu,$ ) are invariant when $r:\\mathcal{Z}\\to\\mathbb{R}$ and $\\nu\\in\\mathcal{P}(\\mathcal{Z})$ are invariant (in their respective sense): ", "page_idx": 36}, {"type": "text", "text": "1. For $V$ , notice that for $g\\in G$ : ", "page_idx": 37}, {"type": "equation", "text": "$$\nV(M_{g}\\#\\mu)=\\int_{\\mathcal{Z}}r(\\theta)d(M_{g}\\#\\mu)(\\theta)=\\int_{\\mathcal{Z}}r(M_{g}\\theta)d\\mu(\\theta)=\\int r(\\theta)d\\mu(\\theta)=V(\\mu)\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "thanks to the invariance of $r$ . i.e. $V$ is invariant. ", "page_idx": 37}, {"type": "text", "text": "2. For $H_{\\nu}$ , notice that, for $g\\in G$ , as $\\nu$ is invariant, we know that $\\begin{array}{r}{\\frac{d(M_{g}\\#\\mu)}{d\\nu}(x)=\\frac{d\\mu}{d\\nu}(M_{g}^{-1}x)}\\end{array}$ Therefore: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{H_{\\nu}(M_{g}\\#\\mu)=\\displaystyle\\int\\log\\left(\\frac{d(M_{g}\\#\\mu)}{d\\nu}(\\theta)\\right)d(M_{g}\\#\\mu)(\\theta)}\\\\ &{\\qquad\\qquad\\quad=\\displaystyle\\int\\log\\left(\\frac{d\\mu}{d\\nu}(M_{g}^{-1}\\theta)\\right)d(M_{g}\\#\\mu)(\\theta)}\\\\ &{\\qquad\\qquad\\quad=\\displaystyle\\int\\log\\left(\\frac{d\\mu}{d\\nu}(M_{g}^{-1}M_{g}\\theta)\\right)d\\mu(\\theta)=H_{\\nu}(\\mu)}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Which proves that $H_{\\nu}$ is invariant. ", "page_idx": 37}, {"type": "text", "text": "We can readily conclude, since: $\\begin{array}{r}{R^{\\tau,\\beta}(\\mu)=R(\\mu)+\\tau\\int r d\\mu+\\beta H_{\\lambda}(\\mu)}\\end{array}$ , for all $\\mu\\in{\\mathcal{P}}({\\mathcal{Z}})$ . ", "page_idx": 37}, {"type": "text", "text": "E.2.2 Proof of Proposition 3 ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "In order to prove Proposition 3, we require a version of Jensen\u2019s inequality that\u2019s suited for our context. Such a result might exist in the literature, but since we couldn\u2019t find a complete proof under our assumptions, we provide our own. ", "page_idx": 37}, {"type": "text", "text": "Proposition 17 (Jensen\u2019s Inequality). Let $F:\\mathcal{P}(\\mathcal{Z})\\longrightarrow\\mathbb{R}$ be such that Lemma 3 holds. Let $S$ be some measurable space, $\\lambda\\in{\\mathcal{P}}(S)$ and $s\\in S\\mapsto\\mu_{s}\\in{\\mathcal{P}}({\\mathcal{Z}})$ a measurable function. Define $\\tilde{\\mu}\\in\\mathcal{P}(\\mathcal{Z})$ as the intensity measure: $\\begin{array}{r}{\\tilde{\\mu}=\\int_{S}\\mu_{s}\\,d\\lambda(s)\\in\\mathcal{P}(\\mathcal{Z})}\\end{array}$ . Then, Jensen\u2019s inequality holds: ", "page_idx": 37}, {"type": "equation", "text": "$$\nF(\\tilde{\\mu})\\leq\\int_{S}F(\\mu_{s})d\\lambda(s)\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof of Proposition $I7.$ . Since Lemma 3 holds, we have that $\\forall\\mu_{1},\\mu_{2}\\in\\mathcal{P}(\\mathcal{Z})$ : ", "page_idx": 37}, {"type": "equation", "text": "$$\nF(\\mu_{1})\\geq F(\\mu_{2})+\\int\\frac{\\partial F}{\\partial\\mu}(\\mu_{2},z)d(\\mu_{1}-\\mu_{2})(z)\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Let $\\tilde{s}\\in S$ be arbitrary and consider $\\begin{array}{r}{\\mu_{2}=\\tilde{\\mu}:=\\int\\mu_{s}d\\lambda(s)}\\end{array}$ ; and $\\mu_{1}=\\mu_{\\tilde{s}}$ . Then: ", "page_idx": 37}, {"type": "equation", "text": "$$\nF(\\left(\\int\\mu_{s}d\\lambda(s)\\right)\\leq F(\\mu_{\\tilde{s}})-\\int\\frac{\\partial F}{\\partial\\mu}\\left(\\int\\mu_{s}d\\lambda(s),z\\right)d\\left(\\mu_{\\tilde{s}}-\\int\\mu_{s}d\\lambda(s)\\right)(z)\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Integrating the inequality with respect to $\\lambda$ (on s\u02dc): ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\int_{S}F\\left(\\int\\mu_{s}d\\lambda(s)\\right)d\\lambda(\\tilde{s})}\\\\ &{\\qquad\\le\\triangle:=\\left[\\displaystyle\\int_{S}F(\\mu_{\\tilde{s}})d\\lambda(\\tilde{s})-\\int_{S}\\left(\\displaystyle\\int_{\\mathcal{Z}}\\frac{\\partial F}{\\partial\\mu}\\left(\\int_{S}\\mu_{s}d\\lambda(s),\\cdot\\right)d\\left(\\mu_{\\tilde{s}}-\\int_{S}\\mu_{s}d\\lambda(s)\\right)\\right)d\\lambda(\\tilde{s})\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We notice that the LHS doesn\u2019t depend on $\\tilde{s}$ , so that $\\begin{array}{r}{\\int_{S}F\\left(\\int\\mu_{s}d\\lambda(s)\\right)d\\lambda(\\tilde{s})=F\\left(\\int\\mu_{s}d\\lambda(s)\\right)}\\end{array}$ . On the other hand, the right-most term in $\\triangle$ can be developed as: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\star:=\\int_{S}\\left(\\int_{z}\\frac{\\partial F}{\\partial\\mu}\\left(\\int_{S}\\mu_{s}d\\lambda(s),\\cdot\\right)d\\left(\\mu_{\\bar{s}}-\\int_{S}\\mu_{s}d\\lambda(s)\\right)\\right)d\\lambda(\\bar{s})}\\\\ &{\\quad=\\int_{S}\\left(\\int_{z}\\frac{\\partial F}{\\partial\\mu}\\left(\\int_{S}\\mu_{s}d\\lambda(s),\\cdot\\right)d\\mu_{\\bar{s}}-\\int_{z}\\frac{\\partial F}{\\partial\\mu}\\left(\\int_{S}\\mu_{s}d\\lambda(s),\\cdot\\right)d\\left(\\int_{S}\\mu_{s}d\\lambda(s)\\right)\\right)d\\lambda(\\bar{s})}\\\\ &{\\quad=\\int_{S}\\left(\\int_{z}\\frac{\\partial F}{\\partial\\mu}\\left(\\int\\mu_{s}d\\lambda(s),\\cdot\\right)d(\\mu_{\\bar{s}})\\right)d\\lambda(\\bar{s})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad-\\int_{S}\\left(\\int_{\\mathcal{Z}}\\frac{\\partial F}{\\partial\\mu}\\left(\\int_{S}\\mu_{s}d\\lambda(s),\\cdot\\right)d\\left(\\int_{S}\\mu_{s}d\\lambda(s)\\right)\\right)d\\lambda(\\bar{s})}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Notice that the linear functional derivative is chosen in such a way so that it satisfies $\\forall\\nu\\ \\in$ $\\mathcal{P}(\\mathcal{Z})$ , $\\begin{array}{r}{\\int_{\\mathcal Z}\\frac{\\partial F}{\\partial\\mu}(\\nu,z)d\\nu(z)=0}\\end{array}$ . In particular, the second term of the previous expression vanishes. We get that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\star=\\int_{S}\\left(\\int_{\\mathcal{Z}}\\frac{\\partial F}{\\partial\\mu}\\left(\\int_{S}\\mu_{s}d\\lambda(s),z\\right)d\\mu_{\\tilde{s}}(z)\\right)d\\lambda(\\tilde{s})\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "But, by definition: $\\forall f:\\mathcal{Z}\\to\\mathbb{R}$ integrable, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\langle f,\\int_{S}\\mu_{s}d\\lambda(s)\\rangle=\\int_{S}\\langle f,\\mu_{s}\\rangle d\\lambda(s)=\\int_{S}\\left(\\int_{\\mathcal{Z}}f(z)d\\mu_{s}(z)\\right)d\\lambda(s)\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "So this is, by definition, and applying the same convention on the definition of the linear functional derivative10: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\star=\\int_{\\mathcal{Z}}\\frac{\\partial F}{\\partial\\mu}\\left(\\int\\mu_{s},z\\right)d\\left(\\int\\mu_{\\tilde{s}}d\\lambda(\\tilde{s})\\right)(z)=0\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "With this, we conclude that $\\begin{array}{r}{\\triangle=\\int_{S}F(\\mu_{s})d\\lambda(s)}\\end{array}$ , and so, we get that: ", "page_idx": 38}, {"type": "equation", "text": "$$\nF({\\tilde{\\mu}})=F\\left(\\int_{S}\\mu_{s}d\\lambda(s)\\right)\\leq\\int_{S}F(\\mu_{s})d\\lambda(s)\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "which corresponds to Jensen\u2019s inequality. ", "page_idx": 38}, {"type": "text", "text": "Remark. We believe that the $\\mathcal{C}^{1}$ hypothesis can be lifted. Understanding what happens when equality holds should be of interest both in our context and in more general scenarios. ", "page_idx": 38}, {"type": "text", "text": "Thanks to this Jensen inequality, we readily get the following result: ", "page_idx": 38}, {"type": "text", "text": "Corollary 7. If $F:\\mathcal{P}(\\mathcal{Z})\\longrightarrow\\mathbb{R}$ is convex, $\\mathcal{C}^{1}$ and invariant, then $\\forall\\mu\\in\\mathcal{P}(\\mathcal{Z})\\colon F(\\mu^{G})\\leq F(\\mu)$ ", "page_idx": 38}, {"type": "text", "text": "Proof. Direct from the definition of $(\\cdot)^{G}$ and Proposition 17. ", "page_idx": 38}, {"type": "text", "text": "With these results in place, we are ready to prove Proposition 3: ", "page_idx": 38}, {"type": "text", "text": "Proof of Proposition 3. Evidently, since $\\mathcal{P}^{G}(\\mathcal{Z})\\subseteq\\mathcal{P}(\\mathcal{Z})$ , we have: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\mu\\in{\\mathcal{P}}^{G}({\\mathcal{Z}})}F(\\mu)\\geq\\operatorname*{inf}_{\\mu\\in{\\mathcal{P}}({\\mathcal{Z}})}F(\\mu)\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "For the other inequality, take $(\\mu_{n})_{n\\in\\mathbb{N}}\\,\\subseteq\\,\\mathcal{P}(\\mathcal{Z})$ to be an infimizing sequence for $F$ ; i.e. such that $F(\\mu_{n})\\geq F(\\mu_{n+1})$ and $F(\\mu_{n})\\;{\\xrightarrow[n\\rightarrow\\infty]{}}\\,\\operatorname*{inf}_{\\mu\\in{\\mathcal{P}}({\\mathcal{Z}})}F(\\mu))$ . Such a sequence always exists. By Corollary 7, we have $\\forall n\\in\\mathbb{N},\\;F(\\mu_{n}^{G})\\leq F(\\mu_{n})$ ; thus, $\\forall n\\in\\mathbb{N}$ : ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\mu\\in{\\mathcal{P}}^{G}({\\mathcal{Z}})}F(\\mu)\\leq F(\\mu_{n}^{G})\\leq F(\\mu_{n})\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Which allows us to infer, by taking $n\\to\\infty$ , that: $\\operatorname*{inf}_{\\mu\\in P^{G}(\\mathcal{Z})}F(\\mu)\\leq\\operatorname*{inf}_{\\mu\\in P(\\mathcal{Z})}F(\\mu)$ . In turn, we can conclude that: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\mu\\in{\\mathcal{P}}^{G}({\\mathcal{Z}})}F(\\mu)=\\operatorname*{inf}_{\\mu\\in{\\mathcal{P}}({\\mathcal{Z}})}F(\\mu)\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Notice that if there was some minimizer $\\mu_{*}\\in\\arg\\operatorname*{min}_{\\mu\\in{\\mathcal{P}}({\\mathcal{Z}})}F(\\mu)$ , then by Corollary 7 we would also have $\\mu_{*}^{G}\\in\\arg\\operatorname*{min}_{\\mu\\in\\mathcal{P}(\\mathcal{Z})}R(\\mu)$ . Namely, if such a minimizer was unique, then it would satisfy: $\\mu_{*}=\\mu_{*}^{G}\\in\\mathcal{P}^{G}(\\mathcal{Z})$ . That is, the unique solution would be WI. \u53e3 ", "page_idx": 38}, {"type": "text", "text": "E.2.3 Proof of Theorem 2 and Corollary 1 ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Notice that, under Assumption 1, from Corollary 5 we know $R$ is of class $\\mathcal{C}^{1}$ (as well as convex). This properties actually transfers to the functionals $R^{D A},R^{F A}$ and $R^{E A}$ , as shown by the following result: ", "page_idx": 39}, {"type": "text", "text": "Proposition 18. If $R:\\mathcal{P}(\\mathcal{Z})\\to\\mathbb{R}$ is a convex and $\\mathcal{C}^{1}$ functional, then $R^{D A},\\,R^{F A}$ and $R^{E A}$ are convex and $\\mathcal{C}^{1}$ as well, with linear functional derivatives given by: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\frac{\\partial R^{D A}}{\\partial\\mu}(\\mu,z)=\\int_{G}\\frac{\\partial R}{\\partial\\mu}(M_{g}\\#\\mu,M_{g}.z)d\\lambda_{G}(g),\\;\\;\\frac{\\partial R^{F A}}{\\partial\\mu}(\\mu,z)=\\int_{G}\\frac{\\partial R}{\\partial\\mu}(\\mu^{G},M_{g}.z)d\\lambda_{G}(g)\n$$", "text_format": "latex", "page_idx": 39}, {"type": "equation", "text": "$$\n\\frac{\\partial R^{E A}}{\\partial\\mu}(\\mu,z)=\\frac{\\partial R}{\\partial\\mu}(\\mu^{\\mathcal{E}^{G}},P_{\\mathcal{E}^{G}}.z)\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "And intrinsic derivatives given by (when well defined): ", "page_idx": 39}, {"type": "equation", "text": "$$\nD_{\\mu}R^{D A}(\\mu,z)=\\int_{G}M_{g}^{T}.D_{\\mu}R(M_{g}\\#\\mu,M_{g}.z)d\\lambda_{G}(g)\n$$", "text_format": "latex", "page_idx": 39}, {"type": "equation", "text": "$$\n{\\supset_{\\mu}R^{F A}(\\mu,z)=\\int_{G}M_{g}^{T}.D_{\\mu}R(\\mu^{G},M_{g}.z)d\\lambda_{G}(g)\\;\\;a n d\\;\\;D_{\\mu}R^{E A}(\\mu,z)=P_{\\xi\\alpha}^{T}.D_{\\mu}R(\\mu^{\\xi^{G}},P_{\\xi\\alpha.\\,z})}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "In particular, from Proposition $I7_{:}$ , we have that: $\\forall\\mu\\in\\mathcal{P}(\\mathcal{Z}),\\;R^{F A}(\\mu)\\leq R^{D A}(\\mu)$ ", "page_idx": 39}, {"type": "text", "text": "Proof of Proposition 18. We can calculate the linear functional derivatives (l.d.f. for short) as follows. Let $\\mu,\\nu\\in\\mathcal{P}(\\mathcal{Z})$ , and consider: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\operatorname*{lim}_{n\\to0}\\frac{R^{D\\lambda}((1-h)\\mu+h\\nu)-R^{D\\lambda}(\\mu)}{h}}}\\\\ &{=\\operatorname*{lim}_{n\\to0}\\frac{\\int_{\\mathbb{R}}R(M_{g}\\#((1-h)\\mu+h\\nu))d\\lambda_{G}(\\theta)-\\int_{\\mathbb{Q}}R(M_{g}\\#\\mu)d\\lambda_{G}(\\theta)}{h}}\\\\ &{=\\operatorname*{lim}_{n\\to0}\\int_{\\mathbb{R}}\\frac{R((1-h)M_{g}\\#\\mu+h M_{g}\\#\\nu)-R(M_{g}\\#\\mu)}{h}d\\lambda_{G}(g)}\\\\ &{=\\int_{\\mathbb{Q}}\\operatorname*{lim}_{n\\to0}\\frac{R((1-h)M_{g}\\#\\mu+h M_{g}\\#\\nu)-R(M_{g}\\#\\mu)}{h}d\\lambda_{G}(g)}\\\\ &{=\\int_{\\mathbb{Q}}\\int_{\\mathbb{Q}}\\frac{\\partial R}{\\partial\\mu}(M_{g}\\#\\mu,z)d(M_{g}\\#\\nu-M_{g}\\#\\mu)(z)d\\lambda_{G}(\\varrho)}\\\\ &{=\\int_{\\mathbb{Q}}\\int_{\\mathbb{Q}}\\frac{\\partial R}{\\partial\\mu}(M_{g}\\#\\mu,M_{g}\\geq d)(\\nu-\\mu)(z)d\\lambda_{G}(\\varrho)}\\\\ &{=\\int_{\\mathbb{Q}}\\int_{\\mathbb{Q}}\\frac{\\partial R}{\\partial\\mu}(M_{g}\\#\\mu,M_{g},z)d(\\nu\\mu)d(\\nu-\\mu)(z).}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "We have used Fubini\u2019s theorem, which is applicable11 thanks to the fact that $R$ is of class $\\mathcal{C}^{1}$ , and we\u2019ve used the definition of the linear functional derivative for $R$ . Also, we see that (using Fubini\u2019s theorem once again, as well as the definition of the linear functional derivative of $R$ ): ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\int_{\\mathcal{Z}}\\int_{G}\\frac{\\partial R}{\\partial\\mu}(M_{g}\\#\\mu,M_{g}.z)d\\lambda_{G}(g)d\\mu(z)=\\int_{G}\\int_{\\mathcal{Z}}\\frac{\\partial R}{\\partial\\mu}(M_{g}\\#\\mu,z)d(M_{g}\\#\\mu)(z)d\\lambda_{G}(g)=0.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "We can then identify: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\frac{\\partial R^{D A}}{\\partial\\mu}(\\mu,z)=\\int_{G}\\frac{\\partial R}{\\partial\\mu}(M_{g}\\#\\mu,M_{g}.z)d\\lambda_{G}(g),\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "and, by taking the gradient: ", "page_idx": 39}, {"type": "equation", "text": "$$\nD_{\\mu}R^{D A}(\\mu,z)=\\int_{G}M_{g}^{T}.D_{\\mu}R(M_{g}\\#\\mu,M_{g}.z)d\\lambda_{G}(g).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "We analogously calculate the expression for the l.f.d. of $R^{F A}$ ; let $\\mu,\\nu\\in\\mathcal{P}(\\mathcal{Z})$ : ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\operatorname*{lim}_{h\\to0}\\frac{R^{F A}((1-h)\\mu+h\\nu)-R^{F A}(\\mu)}{h}=\\displaystyle\\operatorname*{lim}_{h\\to0}\\frac{R((1-h)\\mu^{G}+h\\nu^{G})-R(\\mu^{G})}{h}}&{}\\\\ {=\\displaystyle\\int_{\\mathcal{Z}}\\frac{\\partial R}{\\partial\\mu}(\\mu^{G},z)d(\\nu^{G}-\\mu^{G})(z)}&{}\\\\ &{=\\displaystyle\\int_{\\mathcal{Z}}\\int_{G}\\frac{\\partial R}{\\partial\\mu}(\\mu^{G},M_{g}.z)d\\lambda_{G}(g)d(\\nu-\\mu)(z),}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "and also: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int_{\\mathcal{Z}}\\displaystyle\\int_{G}\\frac{\\partial R}{\\partial\\mu}(\\mu^{G},M_{g}.z)d\\lambda_{G}(g)d\\mu(z)=\\int_{G}\\displaystyle\\int_{\\mathcal{Z}}\\frac{\\partial R}{\\partial\\mu}(\\mu^{G},z)d(M_{g}\\#\\mu)(z)d\\lambda_{G}(g)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\int_{\\mathcal{Z}}\\frac{\\partial R}{\\partial\\mu}(\\mu^{G},z)d\\mu^{G}(z)=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "So, by the definition of the lineal functional derivative, we identify: ", "page_idx": 40}, {"type": "equation", "text": "$$\n{\\frac{\\partial R^{F A}}{\\partial\\mu}}(\\mu,z)=\\int_{G}{\\frac{\\partial R}{\\partial\\mu}}(\\mu^{G},M_{g}.z)d\\lambda_{G}(g),\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "and taking the gradient we get: ", "page_idx": 40}, {"type": "equation", "text": "$$\nD_{\\mu}R^{F A}(\\mu,z)=\\int_{G}M_{g}^{T}.D_{\\mu}R(\\mu^{G},M_{g}.z)d\\lambda_{G}(g)\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Lastly, the l.f.d. of $R^{E A}$ is calculated similarly, noticing that: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\displaystyle\\operatorname*{lim}_{h\\to0}\\frac{R^{E A}((1-h)\\mu+h\\nu)-R^{E A}(\\mu)}{h}=\\displaystyle\\operatorname*{lim}_{h\\to0}\\frac{R(P_{\\mathcal{E}^{G}}\\#((1-h)\\mu+h\\nu))-R(P_{\\mathcal{E}^{G}}\\mu)}{h}}&{}&\\\\ {=\\displaystyle\\int_{\\mathcal{Z}}\\frac{\\partial R}{\\partial\\mu}(\\mu^{\\mathcal{E}^{G}},z)d(P_{\\mathcal{E}^{G}}\\#\\nu-P_{\\mathcal{E}^{G}}\\#\\mu)(z)}&{}&\\\\ {\\displaystyle}&{=\\displaystyle\\int_{\\mathcal{Z}}\\frac{\\partial R}{\\partial\\mu}(\\mu^{\\mathcal{E}^{G}},P_{\\mathcal{E}^{G}}z)d(\\nu-\\mu)(z),}&{}&\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "and that: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\int_{\\mathcal{Z}}\\frac{\\partial R}{\\partial\\mu}(\\mu^{\\mathcal{E}^{G}},P_{\\mathcal{E}^{G}}.z)d\\mu(z)=\\int_{\\mathcal{Z}}\\frac{\\partial R}{\\partial\\mu}(\\mu^{\\mathcal{E}^{G}},z)d(\\mu^{P_{\\mathcal{E}^{G}}})(z)=0.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "We can thus identify: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\frac{\\partial R^{E A}}{\\partial\\mu}(\\mu,z)=\\frac{\\partial R}{\\partial\\mu}(\\mu^{\\varepsilon^{G}},P_{\\varepsilon\\varepsilon\\cdot z})\\quad\\mathrm{and}\\quad D_{\\mu}R^{E A}(\\mu,z)=P_{\\varepsilon^{G}}^{T}.D_{\\mu}R(\\mu^{\\varepsilon^{G}},P_{\\varepsilon\\varepsilon\\cdot z})\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "The last remark is direct from proposition 17. ", "page_idx": 40}, {"type": "text", "text": "With all of these different elements in place, we are ready to prove Theorem 2. ", "page_idx": 40}, {"type": "text", "text": "Proof of Theorem 2. Under Assumption 1, $R$ is convex and of class $\\mathcal{C}^{1}$ from Corollary 5; and so are $\\bar{R}^{G}$ , $R^{F A}$ and $R^{E A}$ , from Proposition 18. Since Proposition 2 ensures that the latter are always invariant, Proposition 3 implies that $R^{D A}$ , $R^{F A}$ and $R^{E A}$ can all be optimized by only considering weakly equivariant models (explaining the first and last equalities). The two middle equalities follow directly from Proposition 2, since $R$ , $R^{D A}$ and $R^{F A}$ coincide over $\\mathcal{P}^{G}(\\mathcal{Z})$ . \u53e3 ", "page_idx": 40}, {"type": "text", "text": "In the case of the quadratic loss, one can employ the properties of $\\mathcal{Q}_{G}$ from [27] to show Corollary 1. ", "page_idx": 40}, {"type": "text", "text": "Proof of Corollary 1. Notice that, for $\\mu\\ \\in\\ \\mathcal{P}^{G}(\\mathcal{Z})$ , $\\Phi_{\\mu}$ is a $G$ -invariant function (i.e. $\\Phi_{\\mu}~\\in$ $L_{G}^{2}(\\mathcal{X},\\mathcal{Y};\\pi_{\\mathcal{X}}))$ . Also, a simple calculation (see e.g. [5, 51]) allows us to write: $R(\\mu)\\;=\\;$ $\\mathbb{E}_{\\pi}^{\\sim}[\\|\\Phi_{\\mu}(X)\\,-\\,Y\\|_{y}^{2}]\\,=\\,R_{*}\\,+\\,\\mathbb{E}_{\\pi_{X}}[\\|\\Phi_{\\mu}(X)\\,-\\,f_{*}(X)\\|_{y}^{2}]\\,=\\,R_{*}\\,+\\,\\|\\Phi_{\\mu}\\,-\\,f_{*}\\|_{L^{2}(X,y;\\pi_{X})}^{2}$ with ", "page_idx": 40}, {"type": "text", "text": "$R_{*}=\\mathbb{E}_{\\pi}[\\|Y-f^{*}(X)\\|_{\\mathcal{Y}}^{2}]$ being independent of $\\mu$ . We can thus write (simplifying subscripts for simplicity): ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R(\\mu)=R_{*}+\\|\\Phi_{\\mu}-\\mathcal{Q}_{G}.f_{*}+\\mathcal{Q}_{G}.f_{*}-f_{*}\\|_{L^{2}(\\mathcal{X},\\mathcal{Y};\\pi_{\\mathcal{X}})}^{2}}\\\\ &{\\qquad=R_{*}+\\|\\Phi_{\\mu}-\\mathcal{Q}_{G}.f_{*}\\|_{L^{2}}^{2}+\\|(f_{*})_{G}^{\\perp}\\|_{L^{2}}^{2}-2\\langle\\Phi_{\\mu}-\\mathcal{Q}_{G}.f_{*},(f_{*})_{G}^{\\perp}\\rangle_{L^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where $(f_{*})_{G}^{\\perp}:=\\,f_{*}\\,-\\,\\mathcal{Q}_{G}.f_{*}$ . We notice that, since $\\Phi_{\\mu}$ and $\\mathcal{Q}_{G}.f_{*}$ are $G$ -equivariant functions, we have that $\\langle(\\Phi_{\\mu}\\,-\\,\\mathcal{Q}_{G}.f_{*}),(f_{*})_{G}^{\\perp}\\rangle_{L^{2}}\\,=\\,0$ . That is, for any $\\mu\\,\\in\\,\\mathcal{P}^{G}(\\mathcal{Z})$ , we have $R(\\mu)\\,=$ $\\tilde{R}_{*}+\\|\\Phi_{\\mu}-\\mathcal{Q}_{G}\\dot{.}f_{*}\\|_{L^{2}}^{2}$ , where $\\tilde{R}_{*}:=R_{*}+\\|(f_{*})_{G}^{\\perp}\\|_{L^{2}(\\mathcal{X},\\mathcal{Y};\\pi_{\\mathcal{X}})}^{2}$ is independent of $\\mu$ (and doesn\u2019t intervene in the optimization). Finally, we get: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\mu\\in{\\mathcal P}^{G}({\\mathcal Z})}R(\\mu)=\\tilde{R}_{*}+\\operatorname*{inf}_{\\mu\\in{\\mathcal P}^{G}({\\mathcal Z})}\\|\\Phi_{\\mu}-{\\mathcal Q}_{G}.f_{*}\\|_{L^{2}({\\mathcal X},{\\mathcal Y};\\pi_{\\mathcal X})}^{2}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "E.2.4 Proof of Corollary 2 ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "When $\\pi$ is assumed to be equivariant, we can summon our previous results to prove Corollary 2. ", "page_idx": 41}, {"type": "text", "text": "Proof of Corollary 2. From Proposition 2 we know that equivariant data implies $R:\\mathcal{P}(\\mathcal{Z})\\to\\mathbb{R}$ is invariant, and also that this makes $R=R^{D A}$ . We conclude using Theorem 2. \u53e3 ", "page_idx": 41}, {"type": "text", "text": "We can readily extend this to the regularized case by recalling Corollary 6: ", "page_idx": 41}, {"type": "text", "text": "Corollary 8. When $R:\\mathcal{P}(\\mathcal{Z})\\to\\mathbb{R}$ and $r:\\mathcal{Z}\\to\\mathbb{R}$ are $G$ -invariant, a minimum for the regularized population risk $R^{\\tau,\\beta}$ can be found within $\\mathcal{P}^{G}(\\mathcal{Z})$ . When $\\beta>0$ such WI minimum is unique. ", "page_idx": 41}, {"type": "text", "text": "Proof of Corollary 8. Direct from Corollary 6, together with Proposition 3 (as in Corollary 2). The uniqueness comes from the strict convexity of the entropy term (see proposition 14). \u53e3 ", "page_idx": 41}, {"type": "text", "text": "E.2.5 Proof of Proposition 4 and Proposition 5 ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Proof of Proposition 4. Consider the group $G\\,=\\,C_{4}$ acting on $\\mathbb{R}^{2}$ via $90^{\\circ}$ rotations. Let $K\\,=$ $B(0,1)\\,\\subseteq\\,\\bar{\\mathbb{R}}^{2}$ be a compact set. Consider a random variable $X\\,\\sim\\mathcal{N}(0,\\mathrm{Id}_{2})|_{K}$ (i.e. given by $X=Z1_{Z\\in K}$ for $Z\\sim\\mathcal{N}(0,\\mathrm{Id}_{2}))$ and set $Y=\\|X\\|^{2}$ . Notice that $\\pi$ defined this way is compactly supported. ", "page_idx": 41}, {"type": "text", "text": "Clearly $G$ is finite (thus compact) and it can be seen as its ortogonal representation: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\rho_{G}=\\left\\{\\left(\\!\\!\\begin{array}{c c}{1}&{0}\\\\ {0}&{1}\\end{array}\\!\\!\\right),\\left(\\!\\!\\begin{array}{c c}{0}&{-1}\\\\ {1}&{0}\\end{array}\\!\\!\\right),\\left(\\!\\!\\begin{array}{c c}{-1}&{0}\\\\ {0}&{-1}\\end{array}\\!\\!\\right),\\left(\\!\\!\\begin{array}{c c}{0}&{1}\\\\ {-1}&{0}\\end{array}\\!\\!\\right)\\right\\}\\subseteq O(2)\\ ,\\ \\hat{\\rho}_{G}=\\{\\mathrm{Id}_{1}\\}\\subseteq O(1)\\ (\\mathrm{tad})\\ ,\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "By the definition of our r.v.s, it is clear that: ", "page_idx": 41}, {"type": "text", "text": "\u2022 X(=d) \u03c1gX \u2200g \u2208G because X \u223cN(0, Id2)   \n\u2022 \u2200g \u2208G, $(X,Y)\\overset{(d)}{=}(\\rho_{g}X,\\hat{\\rho}_{g}Y)$ (since $\\hat{\\rho}$ is the trivial representation, it is enough to notice that $\\|\\rho_{g}.X\\|^{2}=\\|X\\|^{2}$ for all $g\\in G$ . ", "page_idx": 41}, {"type": "text", "text": "Therefore, $\\pi\\,=\\,\\operatorname{Law}(X,Y)$ is $G$ -invariant (and compactly supported). Consider a shallow NN given by: $\\Phi_{\\theta}^{N}:\\mathbb{R}^{2}\\,\\longrightarrow\\,\\mathbb{R}^{N\\times b}\\,\\longrightarrow\\,\\mathbb{R}$ (with $b\\in\\mathbb N$ and some action $G\\ {\\mathrm{\\mathcal{C}}}_{\\eta}\\ \\mathbb{R}^{b})$ as: $\\Phi_{\\theta}^{N}(x)\\,=$ $\\begin{array}{r}{\\frac{1}{N}\\sum_{i=1}^{N}W_{i}\\sigma(A_{i}^{T}x+B_{i}),\\;\\;\\forall x\\in\\mathbb{R}^{d}}\\end{array}$ ; where $\\theta_{i}=(W_{i},A_{i},B_{i})\\in\\mathcal{Z}:=\\mathbb{R}^{1\\times b}\\times\\mathbb{R}^{2\\times b}\\times\\mathbb{R}^{b}\\cong\\mathbb{R}^{D}$ We let $G\\operatorname{C}_{M}\\mathcal{Z}$ as described in appendix C.1: ", "page_idx": 41}, {"type": "equation", "text": "$$\nM_{g}.\\theta_{i}=(\\hat{\\rho}_{g}W_{i}\\;\\eta_{g}^{T},\\rho_{g}\\;A_{i}\\;\\eta_{g}^{T},\\eta_{g}.B_{i})=(W_{i}\\;\\eta_{g}^{T},\\rho_{g}\\;A_{i}\\;\\eta_{g}^{T},\\eta_{g}.B_{i})\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "We can assume, for instance, that $b=1$ and $\\eta$ is the trivial representation (so that no condition is required for $\\sigma_{*}$ to be jointly $G$ -equivariant) and recall that: $\\dot{\\theta_{i}\\in\\mathcal{E}^{G}}\\ \\Longleftrightarrow\\ \\ \\forall g\\in G,\\ \\ M_{g}\\theta_{i}=\\theta_{i}$ . ", "page_idx": 41}, {"type": "text", "text": "However, if we assume that: $\\forall g\\in G,\\;\\rho_{g}A_{i}=A_{i}$ , then, in particular: $\\binom{-1}{0}\\begin{array}{c c}{{0}}&{{\\null}}\\\\ {{-1}}&{{-1}}\\end{array}\\binom{A_{i}^{1}}{A_{i}^{2}}=\\binom{A_{i}^{1}}{A_{i}^{2}}$ This in turn implies, as $A_{i}^{1}\\,=\\,-A_{i}^{1}$ and $A_{i}^{2}\\,=\\,-A_{i}^{2}$ that $A_{i}^{1}\\,=\\,A_{i}^{2}\\,=\\,0$ . i.e. $A_{i}\\equiv0$ . Thus, any $\\theta_{i}=\\binom{w_{i}}{A_{i}}\\in\\mathcal{E}^{G}$ has $A_{i}=0$ . Therefore, if we choose any activation $\\sigma$ (e.g the sigmoid activation or $\\sigma=\\mathrm{tanh}$ , both ${\\mathcal{C}}^{\\infty}$ and bounded) and we choose $N\\in\\mathbb{N}^{*}$ and $\\theta_{i}\\in\\mathcal{E}^{G}\\,\\forall\\,i=1,...,N$ ; then: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\forall x\\in\\mathbb{R}^{2},\\ \\Phi_{\\theta}^{N,\\mathcal{E}^{G}}(x)=\\frac{1}{N}\\sum_{1=1}^{N}W_{i}\\sigma(0^{T}\\cdot x+B_{i})=\\frac{1}{N}\\sum_{i=1}^{N}W_{i}\\sigma(B_{i}),\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "which is a constant independent of $x$ . i.e. any equivariant architecture in this context is a constant function (whereas $Y=\\|X\\|^{2}$ is not). Notice, in particular, that any shallow model $\\Phi_{\\nu}$ with $\\nu\\in\\mathcal{P}(\\mathcal{E}^{G})$ will also be a constant function. In particular, notice that we will never do \u2018better\u2019 than minimizing over all posible constants: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\theta_{i}\\in{\\mathcal E}^{G}}R(\\Phi_{\\theta}^{\\varepsilon^{G}})\\geq\\operatorname*{inf}_{\\nu\\in{\\mathcal P}({\\mathcal E}^{G})}R(\\nu)\\geq\\operatorname*{inf}_{C\\in{\\mathbb R}}\\mathbb{E}[|Y-C|^{2}]=\\operatorname*{inf}_{C\\in{\\mathbb R}}\\mathbb{E}[|\\|X\\|^{2}-C|^{2}].\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "The problem on the right has a known answer, which is $C^{*}=\\mathbb{E}_{\\pi}[\\|X\\|^{2}]>0$ . On the other hand, consider a fully conected neuronal network. By the universal approximation theorem (which applies for the chosen $\\sigma$ , as in [37, 20, 5]), as $\\pi$ is compactly supported (in particular, $\\pi_{\\mathcal{X}}(K)=1)$ ); we consider the parame\u221aters that approximate the function ${\\bar{f}}(x)\\,{\\bar{=}}\\,\\|x\\|^{2}$ in $K=B(0,1)$ to precision $\\varepsilon>0$ . i.e. For $\\varepsilon\\in(0,\\sqrt{C^{*}})$ , we know: $\\Longrightarrow\\exists N\\in\\mathbb{N}$ , $\\exists a_{1},...,a_{N}\\in\\mathbb{R}^{2}$ , $\\exists w_{1},...,w_{N}\\in\\mathbb{R}^{1}$ such that: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\|\\Phi_{\\theta}^{N}-f\\|_{\\infty,K}=\\operatorname*{sup}_{x\\in K}|\\Phi_{\\theta}^{N}(x)-f(x)|<\\varepsilon<\\sqrt{C^{*}}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Then: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{E}[|Y-\\Phi_{\\theta}^{N}(x)|^{2}]\\leq\\mathbb{E}[(\\operatorname*{sup}_{x\\in K}|\\Phi_{\\theta}^{N}(x)-f(x)|)^{2}]<\\mathbb{E}[C^{*}]=C^{*}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "But, in particular, $\\exists\\nu_{\\theta}^{N}\\in\\mathcal{P}(\\mathcal{Z})$ such that: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{E}[|Y-\\Phi_{\\theta}^{N}(x)|^{2}]<C^{*}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "and so: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\mu\\in{\\mathcal{P}}({\\mathcal{Z}})}R(\\mu)\\leq\\operatorname*{inf}_{\\theta\\in{\\mathcal{Z}}^{N}}\\mathbb{E}[|Y-\\Phi_{\\theta}^{N}(x)|^{2}]<C^{*}\\leq\\operatorname*{inf}_{\\nu\\in{\\mathcal{P}}({\\mathcal{E}}^{G})}R(\\nu)\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "In particular, we can\u2019t expect an optimum of the learning problem to be achieved within $\\mathcal{P}(\\mathcal{E}^{G})$ . ", "page_idx": 42}, {"type": "text", "text": "To overcome situations as in Proposition 4, the usual setting is to assume some universality condition. This leads to Proposition 5, which we will now prove: ", "page_idx": 42}, {"type": "text", "text": "Proof of Proposition 5. A standard calculation from the quadratic loss case (see the proof of Corollary 1 for details) yields that, for any $\\mu\\in{\\mathcal{P}}({\\mathcal{Z}})$ : ", "page_idx": 42}, {"type": "equation", "text": "$$\nR(\\mu)=\\mathbb{E}_{\\boldsymbol\\pi}[\\|Y-\\Phi_{\\mu}(X)\\|_{\\mathcal{Y}}^{2}]=R_{*}+\\mathbb{E}_{\\boldsymbol\\pi}[\\|f^{*}(X)-\\Phi_{\\mu}(X)\\|_{\\mathcal{Y}}^{2}]\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where $f^{*}(x):=\\mathbb{E}_{\\pi}[Y|X=x]$ and $R_{*}$ is the Bayes risk of the problem. From Proposition 10, we know that $f^{*}\\in L_{G}^{2}(\\mathcal{X},\\mathcal{Y};\\pi_{\\mathcal{X}})$ , and so by universality of $\\mathcal{F}_{\\sigma_{*}}(\\dot{\\mathcal{P}}(\\mathcal{E}^{G}))$ onto that space (as well as that of $\\mathcal{F}_{\\sigma_{*}}(\\mathcal{P}(\\mathcal{Z}))$ , we conclude directly that: $\\operatorname*{inf}_{\\nu\\in{\\mathcal{P}}(\\mathcal{E}^{G})}R(\\nu)=R_{*}=\\operatorname*{inf}_{\\mu\\in{\\mathcal{P}}({\\mathcal{Z}})}R(\\mu)$ \u53e3 ", "page_idx": 42}, {"type": "text", "text": "Remark. Works such as [50, 60, 76, 77] precisely provide conditions under which universality of $\\mathcal{F}_{\\sigma_{*}}(\\mathcal{P}(\\mathcal{E}^{G}))$ can be guaranteed (modulo some adaptations from their setting to ours). ", "page_idx": 42}, {"type": "text", "text": "Particularly, our single-hidden-layer NNs, with the width $N\\rightarrow\\infty$ , correspond to what is referred to as \u2018networks of tensor order $I^{\\,\\prime}$ in the literature. As noted in $I5O J,$ such kind of equivariant NNs are unable to achieve universality for certain types of group actions (see Theorem 2 from [50]). Despite this, \u2018first order universality\u2019 has already been established for some of the most important examples of equivariant architectures, such as Deep Sets ([76, 77]) and CNNs ([76]). ", "page_idx": 42}, {"type": "text", "text": "Adapting our setting, in order to eventually allow for arbitrary order tensors in the MF formulation, is part of the future challenges to make our work more broadly applicable. ", "page_idx": 42}, {"type": "text", "text": "E.3 Proofs of results in Section 3.3 ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Having laid out all the different relevant elements for our work, we can now procede with the proofs of some of our main results. ", "page_idx": 43}, {"type": "text", "text": "E.3.1 Proof of Theorem 3, Corollary 3 and Theorem 4 ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "We start by proving Theorem 3 on the general case. ", "page_idx": 43}, {"type": "text", "text": "Proof of Theorem 3. We know that a family $(\\mu_{t})_{t\\geq0}\\subseteq\\mathcal{P}_{2}(\\mathcal{Z})$ satisfies $\\mathbf{WGF}(F)$ in the weak sense if $\\forall\\varphi\\in C_{c}^{\\infty}(\\mathcal{Z}\\times(0,T))$ : ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\int_{0}^{T}\\int_{\\mathcal{Z}}\\left(\\partial_{t}\\varphi(z,t)-\\langle\\varsigma(t)D_{\\mu}F(\\mu_{t},z),\\nabla_{z}\\varphi(z,t)\\rangle\\right)d\\mu_{t}(z)\\,d t=0\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Now, proftiing from the uniqueness of the solutions of this equation, it will be enough to show that, given a solution $(\\mu_{t})_{t\\geq0}\\subseteq\\mathcal{P}_{2}(\\mathcal{Z})$ of $\\mathbf{WGF}(F)$ , then $(\\mu_{t}^{G})_{t\\geq0}\\subseteq\\mathcal{P}_{2}^{G}(\\mathcal{Z})$ is also a solution. Indeed, consider, for $g\\in G$ , ${\\tilde{\\mu}}_{t}=M_{g}$ # $\\mu_{t}$ , and notice that for $\\varphi\\in C_{c}^{\\infty}(\\mathcal{Z}\\times(0,T))$ : ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\int_{0}^{T}\\int_{\\mathbb{Z}}(\\partial_{t}\\varphi(z,t)-\\langle\\varsigma(t)D_{\\mu}F(\\tilde{\\mu}_{t},z),\\nabla_{z}\\varphi(z,t)\\rangle)d\\tilde{\\mu}_{t}(z)\\,d t}}\\\\ &{}&{=\\int_{0}^{T}\\int_{\\mathcal{Z}}\\left(\\partial_{t}\\varphi(M_{g}.z,t)-\\langle\\varsigma(t)D_{\\mu}F(M_{g}\\#\\mu_{t},M_{g}.z),\\nabla_{z}\\varphi(M_{g}.z,t)\\rangle\\right)d\\mu_{t}(z)\\,d t=:\\kappa(M_{g}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Now, we can define $\\varphi^{g}\\in C_{c}^{\\infty}(\\mathcal{Z}\\times(0,T))$ given by $\\forall(z,t)\\in\\mathcal{Z}\\times(0,T)\\;\\varphi^{g}(z,t)=\\varphi(M_{g}.z,t),$ which satisfies: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\partial_{t}\\varphi^{g}(z,t)=\\partial_{t}\\varphi(M_{g}.z,t)\\ \\mathrm{~and~}\\ \\nabla_{z}\\varphi^{g}(z,t)=M_{g}^{T}\\nabla_{z}\\varphi(M_{g}.z,t)\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "So that, by also using proposition 13 and the orthogonality of the group action, we get: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\star=\\int_{0}^{T}\\int_{\\mathcal{Z}}\\left(\\partial_{t}\\varphi^{g}(z,t)-\\langle M_{g}.\\varsigma(t)D_{\\mu}F(\\mu_{t},z),M_{g}\\nabla_{z}\\varphi^{g}(z,t)\\rangle\\right)d\\mu_{t}(z)\\,d t}\\\\ {\\displaystyle\\quad=\\int_{0}^{T}\\int_{\\mathcal{Z}}\\left(\\partial_{t}\\varphi^{g}(z,t)-\\langle\\varsigma(t)D_{\\mu}F(\\mu_{t},z),\\nabla_{z}\\varphi^{g}(z,t)\\rangle\\right)d\\mu_{t}(z)\\,d t=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Where the last equality comes from the fact that $(\\mu_{t})_{t\\geq0}$ is a solution to the WGF. ", "page_idx": 43}, {"type": "text", "text": "In particular, as we also have that $\\tilde{\\mu}_{0}=M_{g}\\#\\mu_{0}=\\mu_{0}$ (because $\\mu_{0}\\in\\mathcal{P}^{G}(\\mathcal{Z}))$ , by uniqueness we can conclude that this means that $\\forall g\\in G,\\forall t\\in(0,T)\\ \\lambda{=}$ a.e., $\\mu_{t}=M_{g}\\#\\mu_{t}$ . ", "page_idx": 43}, {"type": "text", "text": "This may seem weaker that what we want to prove. Nevertheless, as our group is compact and has a unique normalized Haar measure, we can proceed as follows: let $f:[0,T]\\times\\mathcal{Z}\\to\\mathbb{R}_{+}$ be any positive and measurable function. Given $g\\in G$ , take $\\Omega_{g}\\subseteq[0,T]$ a full measure set where it holds that $\\mu_{t}=M_{g}\\#\\mu_{t}$ . In particular, $f_{t}=f(t,\\cdot):\\mathcal{Z}\\to\\mathbb{R}$ is positive and measurable, so that: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\forall t\\in\\Omega_{g},\\;\\langle f_{t},\\mu_{t}\\rangle=\\langle f_{t},M_{g}\\#\\mu_{t}\\rangle=\\langle f_{t}\\circ M_{g},\\mu_{t}\\rangle\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "and we can integrate this equality to get: $\\begin{array}{r}{\\int_{0}^{T}\\langle f_{t},\\mu_{t}\\rangle d t=\\int_{0}^{T}\\langle f_{t}\\circ M_{g},\\mu_{t}\\rangle d t}\\end{array}$ Now, by integrating both sides with respect to the Haar measure, and applying Fubini\u2019s theorem (because everything is positive) we get: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\int_{0}^{T}\\langle f_{t},\\mu_{t}\\rangle d t=\\int_{G}\\int_{0}^{T}\\langle f_{t},\\mu_{t}\\rangle d t d\\lambda_{G}(g)=\\int_{G}\\int_{0}^{T}\\langle f_{t}\\circ M_{g},\\mu_{t}\\rangle d t d\\lambda_{G}(g)=\\int_{0}^{T}\\langle f_{t},\\mu_{t}^{G}\\rangle d t d\\lambda_{G}(g).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Implying (by a standard argument) that $\\forall t\\in[0,T]$ a.e. $\\mu_{t}\\,=\\,\\mu_{t}^{G}$ , and therefore: $\\forall t\\in[0,T]$ a.e.   \n$\\mu_{t}\\in\\dot{\\mathcal{P}}^{\\check{G}}(\\mathcal{Z})$ . ", "page_idx": 43}, {"type": "text", "text": "Proof of Corollary 3. From Corollary 2, Corollary 6 and Corollary 8, we know that $R^{\\tau,\\beta}$ is invariant. ", "page_idx": 43}, {"type": "text", "text": "On the other hand, from [38] (or [70]) we know that, under our assumptions, a unique weak solution to the Fokker-Planck equation exists. Furthermore, this solution is known to be strong if $\\beta\\,>\\,0$ . ", "page_idx": 43}, {"type": "text", "text": "In particular, theorem 3 applies and allows us to conclude that if $\\mu_{0}\\in\\mathcal{P}_{2}^{G}(\\mathcal{Z})$ , then $\\forall t\\ge0$ (a.e.) $\\bar{\\mu_{t}}\\overset{\\cdot}{\\in}\\mathcal{P}_{2}^{G}(\\mathcal{Z})$ . ", "page_idx": 44}, {"type": "text", "text": "When $\\beta>0$ , since solutions are strong, we conclude that the densities $(u_{t})_{t\\geq0}$ are all $G$ -invariant functions ( $\\lambda$ -a.e.). This follows from the remark about densities of invariant measures provided in SuppMat-B.2. \u53e3 ", "page_idx": 44}, {"type": "text", "text": "Remark. When $\\beta>0$ , we have a unique weakly-invariant minimizer (from proposition 3 and/or corollary 8); and also, under mild assumptions, a global convergence result. That is, independently of the network\u2019s initialization, we will converge to the $G$ -invariant solution. An interesting question in this setting is then: At which point does the WGF enter the space $\\mathcal{P}_{2}^{G}(\\mathcal{Z})$ ? ", "page_idx": 44}, {"type": "text", "text": "We can also prove Theorem 4: ", "page_idx": 44}, {"type": "text", "text": "Proof of Theorem 4. This proof follows from the fact that $\\forall z\\in{\\mathcal{Z}}$ , $\\forall\\mu\\in\\mathcal{P}^{G}(\\mathcal{Z})$ : ", "page_idx": 44}, {"type": "equation", "text": "$$\nD_{\\mu}R^{D A}(\\mu,z)=D_{\\mu}R^{F A}(\\mu,z).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Indeed, notice that, from proposition 18: ", "page_idx": 44}, {"type": "equation", "text": "$$\nD_{\\mu}R^{F A}(\\mu,z)=\\int_{G}M_{g}^{T}.D_{\\mu}R(\\mu^{G},M_{g}.z)d\\lambda_{G}(g)=\\int_{G}M_{g}^{T}.D_{\\mu}R(\\mu,M_{g}.z)d\\lambda_{G}(g),\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "while also: ", "page_idx": 44}, {"type": "equation", "text": "$$\nD_{\\mu}R^{D A}(\\mu,z)=\\int_{G}M_{g}^{T}.D_{\\mu}R(M_{g}\\#\\mu,M_{g}.z)d\\lambda_{G}(g)=\\int_{G}M_{g}^{T}.D_{\\mu}R(\\mu,M_{g}.z)d\\lambda_{G}(g)\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Now, let $(\\mu_{t}^{F A})_{t\\geq0}$ and $(\\mu_{t}^{D A})_{t\\geq0}$ be the WGF solutions starting from $\\mu_{0}$ for $R^{F A}$ and $R^{D A}$ respectively. As $\\bar{R}^{F A}$ is $G$ -invariant, by corollary 3, $(a.e.)\\forall t\\geq0$ , $\\mu_{t}^{F A}\\,\\in\\,\\mathcal{P}^{G}(\\mathcal{Z})$ . Now, let\u2019s see that this process actually also satisfies $\\mathbf{WG}\\dot{\\mathbf{F}}(R^{D A})$ , forcing both processes to coincide by uniqueness. ", "page_idx": 44}, {"type": "text", "text": "Indeed, we know that $(\\mu_{t}^{F A})_{t\\geq0}$ satisfies: $\\forall\\varphi\\in C_{c}^{\\infty}(\\mathcal{Z}\\times(0,T))$ : ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\int_{0}^{T}\\int_{\\mathcal{Z}}\\left(\\partial_{t}\\varphi(z,t)-\\langle\\varsigma(t)D_{\\mu}R^{F A}(\\mu_{t}^{F A},z),\\nabla_{z}\\varphi(z,t)\\rangle\\right)d\\mu_{t}^{F A}(z)\\,d t=0\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Now, as $(a.e.)\\forall t\\geq0$ , $\\mu_{t}^{F A}\\in\\mathcal{P}^{G}(\\mathcal{Z})$ , we have $\\forall z\\in\\mathcal{Z}\\colon D_{\\mu}R^{F A}(\\mu_{t}^{F A},z)=D_{\\mu}R^{D A}(\\mu_{t}^{F A},z)$ . In particular, $(\\mu_{t}^{F A})_{t\\geq0}$ satisfies $\\forall\\varphi\\in C_{c}^{\\infty}(\\mathcal{Z}\\times(0,T))$ : ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\int_{0}^{T}\\int_{\\mathcal{Z}}\\left(\\partial_{t}\\varphi(z,t)-\\langle\\varsigma(t)D_{\\mu}R^{D A}(\\mu_{t}^{F A},z),\\nabla_{z}\\varphi(z,t)\\rangle\\right)d\\mu_{t}^{F A}(z)\\,d t=0\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Implying that $(\\mu_{t}^{F A})_{t\\geq0}$ solves $\\mathbf{WGF}(R^{D A})$ starting from $\\mu_{0}$ ; thus by uniqueness: $(\\mu_{t}^{F A})_{t\\geq0}=$ $(\\mu_{t}^{D A})_{t\\geq0}$ . ", "page_idx": 44}, {"type": "text", "text": "The last part of the theorem comes from Proposition 2, since if $R$ is invariant, its WGF will exactly coincide with that of $R^{D A}$ (they are the same functional). ", "page_idx": 44}, {"type": "text", "text": "Remark. This results tells us the ultimate bottom line: at the MF level, training with DA or FA results in the exact same dynamic. Furthermore, whenever data is equivariant, they are both essentially equivalent to applying no technique whatsoever. Despite this result concerning infinitely wide NNs, it provides meaningful practical insights (as shown Appendix $F$ ) for large enough NNs, that could be used in applications. ", "page_idx": 44}, {"type": "text", "text": "Remark. Since $R^{D A}$ , $R^{F A}$ and $R$ all coincide on $\\mathcal{P}^{G}(\\mathcal{Z})$ , one could expect Theorem 4 to hold for $R$ even without assuming $\\pi$ to be equivariant. However, the invariance of $R$ is crucial for such $a$ result: if $R$ isn\u2019t invariant, nothing guarantees that its WGF process will stay within $\\mathcal{P}^{G}(\\mathcal{Z})$ , whereas $W G F(\\dot{R}^{D A})$ and ${\\pmb W}{\\pmb G}{\\pmb F}({\\cal R}^{F A})$ always do so. ", "page_idx": 44}, {"type": "text", "text": "E.3.2 Proof of Theorem 5 and Theorem 6 ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "We can now provide the proof for the stronger result, Theorem 5, stating that the WGF of an invariant functional will respect $\\bar{\\mathcal{E}}^{G}$ all along training. This proof uses the McKean-Vlasov non-linear SDE (Equation (11)) presented in Appendix D.2 (see [22] for a reference). Namely, we will consider the following projected McKean-Vlasov SDE (with $(B_{t})_{t\\geq0}$ a BM on $\\mathcal{Z}$ ), given by: ", "page_idx": 45}, {"type": "equation", "text": "$$\nd Z_{t}=\\varsigma(t)[-\\left(D_{\\mu}R(\\mu_{t},\\cdot)+\\tau\\nabla_{\\theta}r(Z_{t})\\right)d t+\\sqrt{2\\beta}P_{\\xi}c\\,d B_{t}]\\;\\;w i t h\\;\\;\\;\\mathrm{Law}(Z_{t})=\\mu_{t},\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "which corresponds to the MF limit dynamics arising from performing the projected noisy SGD scheme from Equation (5). This can be shown by adapting relatively standard arguments from the MF literature on NN, see e.g. [22, 23, 53]. ", "page_idx": 45}, {"type": "text", "text": "Proof of Theorem 5. The proof has two steps. The first will consist in showing that the process (12) satisfies $\\forall t\\ge0$ , $\\mu_{t}\\in\\mathcal{P}(\\bar{\\mathcal{E}}^{G})$ . In the second step we will check that $(\\mu_{t})_{t\\geq0}=(\\operatorname{Law}(Z_{t}))_{t\\geq0}$ is a solution to the WGF $(R_{\\mathcal{E}^{G}}^{\\tau,\\beta})$ presented in Section 3.3. ", "page_idx": 45}, {"type": "text", "text": "Step 1: The (pathwise unique) solution of the projected McKean-Vlasov SDE (12), $Z=(Z_{t})_{t\\geq0}$ satisfies a.s. for all $t\\geq0$ : ", "page_idx": 45}, {"type": "equation", "text": "$$\nZ_{t}=Z_{0}-\\int_{0}^{t}\\varsigma(s)D_{\\mu}R^{\\tau}(\\mu_{s},Z_{s})d s+\\sqrt{2\\beta}\\int_{0}^{t}\\varsigma(s)P_{\\xi}\\mathrm{c}\\,d B_{s},\\;\\;\\mathrm{and}\\;\\;Z_{0}=\\xi_{0}\\,\\,\\mathrm{(initial\\,condition)}\\,,\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Here, $\\xi_{0}$ is such that $\\mathrm{Law}(\\xi_{0})=\\mu^{0}$ , and $R^{\\tau}:=R+\\langle r,\\cdot\\rangle$ is being used as shorthand notation. We first let $g\\in G$ be an arbitrary group element, and we study how the process $\\tilde{Z}=(\\tilde{Z}_{t})_{t\\geq0}:=(M_{g}Z_{t})_{t\\geq0}$ satisfies this same equation (13). ", "page_idx": 45}, {"type": "text", "text": "Denote $\\begin{array}{r}{\\nu_{s}:=M_{g}\\#\\mu_{s}}\\end{array}$ as the law of $\\tilde{Z}_{s}$ , we want to show that for all $t\\geq0$ : ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\tilde{Z}_{t}\\stackrel{a.s.}{=}\\tilde{Z}_{0}-\\int_{0}^{t}\\varsigma(s){\\cal D}_{\\mu}R^{\\tau}(\\nu_{s},\\tilde{Z}_{s})d s+\\sqrt{2\\beta}\\int_{0}^{t}\\varsigma(s){\\cal P}_{\\xi}\\omega d B_{s}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Indeed, first notice that: ", "page_idx": 45}, {"type": "text", "text": "Let $\\Omega$ be the full measure set where $\\xi_{0}\\,\\in\\,\\mathcal{E}^{G}$ (which we can do since $\\mu_{0}\\,\\in\\,\\mathcal{P}(\\mathcal{E}^{G})$ , or, equivalently: $\\mathbb{P}(\\xi_{0}\\,\\in\\,\\mathcal{E}^{G})\\,=\\,1_{,}$ ). Then, $\\forall\\omega\\,\\in\\,\\Omega$ , $Z_{0}(\\omega)\\,=\\,\\xi_{0}(\\omega)\\,\\in\\,\\dot{\\mathcal{E}}^{G}$ . In particular, $\\forall\\omega\\in\\Omega,\\,\\forall g\\in G,\\Tilde{Z}_{0}(\\omega)=M_{g}Z_{0}(\\omega)=M_{g}\\xi_{0}(\\omega)=\\xi_{0}(\\omega)=Z_{0}(\\omega)$ That is, $\\tilde{Z}_{0}\\ {\\overset{a.s.}{=}}\\ Z_{0}$ . ", "page_idx": 45}, {"type": "text", "text": "2. Now, the equation is satisfied by $(Z_{t})_{t\\geq0}$ and therefore, for $t\\geq0$ , we have: ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\tilde{Z}_{t}={\\cal M}_{g}Z_{t}={\\cal M}_{g}Z_{0}-{\\cal M}_{g}\\left(\\int_{0}^{t}\\varsigma(s){\\cal D}_{\\mu}R^{\\tau}(\\mu_{s},Z_{s})d s\\right)+\\sqrt{2\\beta}{\\cal M}_{g}\\int_{0}^{t}\\varsigma(s){\\cal P}_{\\varepsilon}\\alpha d B_{s}}}\\\\ {{\\displaystyle\\ ~=\\tilde{Z}_{0}-\\int_{0}^{t}\\varsigma(s){\\cal M}_{g}{\\cal D}_{\\mu}R^{\\tau}(\\mu_{s},Z_{s}))d s+\\sqrt{2\\beta}\\int_{0}^{t}\\varsigma(s){\\cal M}_{g}.{\\cal P}_{\\varepsilon}\\alpha d B_{s}}}\\\\ {{\\displaystyle\\ ~=\\tilde{Z}_{0}-\\int_{0}^{t}\\varsigma(s){\\cal D}_{\\mu}R^{\\tau}({\\cal M}_{g}\\#\\mu_{s},{\\cal M}_{g}.Z_{s})d s+\\sqrt{2\\beta}\\int_{0}^{t}\\varsigma(s){\\cal P}_{\\varepsilon}\\alpha d B_{s}}}\\\\ {{\\displaystyle\\ ~=\\tilde{Z}_{0}-\\int_{0}^{t}\\varsigma(s){\\cal D}_{\\mu}R^{\\tau}(\\nu_{s},\\tilde{Z}_{s})d s+\\sqrt{2\\beta}\\int_{0}^{t}\\varsigma(s){\\cal P}_{\\varepsilon}\\alpha d B_{s}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Here, we used the linearity of the integral (and the stochastic integral), the fact that $\\forall g\\in$ $G,\\ M_{g}P_{\\mathcal{E}^{G}}=P_{\\mathcal{E}^{G}}$ , and Proposition 13, which holds for $\\forall\\theta\\in\\mathcal{Z},\\forall\\mu\\in\\mathcal{P}(\\mathcal{Z})$ (in particular for $\\bar{\\theta}\\doteq Z_{s}(\\omega)$ , $\\forall\\omega\\in\\Omega$ and $\\mu_{s}=\\operatorname{Law}(Z_{s}))$ . Thus, $\\forall g\\in G$ , (14) holds. ", "page_idx": 45}, {"type": "text", "text": "By the pathwise uniqueness of the solution $(Z_{t})_{t\\geq0}$ , we have (following, for instance, [28]): ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\operatorname*{sup}_{t\\geq0}\\|Z_{t}-\\tilde{Z}_{t}\\|=0\\right)=1\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "In particular, as $g\\in G$ was arbitrary, we have that: ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\forall g\\in G,\\,\\operatorname*{sup}_{t\\geq0}\\left\\|Z_{t}-M_{g}Z_{t}\\right\\|\\overset{a.s.}{=}0\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "We now want to be able to interchange the $\\forall g\\in G$ with the probability measure. Fortunately, we are dealing with a compact group with a normalized Haar measure $\\lambda_{G}$ . Indeed, from equation (15) we deduce that $\\forall g\\in G,\\,\\forall t\\geq0$ , $\\mathbb{P}(\\|Z_{t}-M_{g}Z_{t}\\|=0)=1$ . ", "page_idx": 46}, {"type": "text", "text": "Now, notice that, for any $t\\geq0$ and $\\omega\\in\\Omega$ : ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|Z_{t}(\\omega)-P_{\\varepsilon}\\alpha Z_{t}(\\omega)\\|=\\bigg\\|Z_{t}(\\omega)-\\int_{G}M_{g}.Z_{t}(\\omega)d\\lambda_{G}(g)\\bigg\\|}&{\\leq\\int_{G}\\|Z_{t}(\\omega)-M_{g}.Z_{t}(\\omega)\\|\\,d\\lambda_{G}(g)}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "We can integrate both sides by $\\mathbb{P}$ to get (using Fubini as functions are positive and measurable): ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{0\\le\\displaystyle\\int_{\\Omega}\\|Z_{t}(\\omega)-P_{\\xi^{G}}Z_{t}(\\omega)\\|d\\mathbb{P}(\\omega)\\le\\displaystyle\\int_{\\Omega}\\int_{G}\\|Z_{t}(\\omega)-M_{g}.Z_{t}(\\omega)\\|\\,d\\lambda_{G}(g)d\\mathbb{P}(\\omega)}&{{}}&{}\\\\ {\\le\\displaystyle\\int_{G}\\int_{\\Omega}\\|Z_{t}(\\omega)-M_{g}.Z_{t}(\\omega)\\|\\,d\\mathbb{P}(\\omega)d\\lambda_{G}(g)=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where in the last step we have used the fact that $\\forall g\\in G,\\,\\forall t\\geq0$ , $\\mathbb{P}(\\|Z_{t}-M_{g}Z_{t}\\|=0)=1$ , so that $\\forall t\\ge0$ , $\\forall g\\in G$ , $\\begin{array}{r}{\\int_{\\Omega}\\|Z_{t}(\\omega)-M_{g}.Z_{t}(\\omega)\\|\\,d\\mathbb{P}(\\omega)=0}\\end{array}$ . ", "page_idx": 46}, {"type": "text", "text": "This implies that $\\forall t\\geq0\\;\\mathbb{P}.$ -a.s. $Z_{t}=P_{\\mathcal{E}^{G}}Z_{t}$ , i.e. $\\mathbb{P}(Z_{t}\\in\\mathcal{E}^{G})=\\mu_{t}(\\mathcal{E}^{G})=1$ , or, in other words, $\\forall t\\geq0,\\,\\bar{\\mu}_{t}\\in\\mathcal{P}(\\mathcal{E}^{G})$ as required. Note that all arguments work as well in the case that $\\beta=0$ ", "page_idx": 46}, {"type": "text", "text": "Step 2: We now prove that $(\\mu_{t})_{t\\geq0}$ , studied in the previous step, is a weak solution to equation (3); that is, to the $\\mathbf{WGF}(F)$ , with (using the previously introduced notation $R^{\\tau}$ ): ", "page_idx": 46}, {"type": "equation", "text": "$$\nF(\\mu)=R_{\\mathcal{E}^{G}}^{\\tau,\\beta}(\\mu)=R^{\\tau}(\\mu)+\\beta H_{\\lambda_{\\mathcal{E}^{G}}}(\\mu^{\\mathcal{E}^{G}}).\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Also recall the notation $H_{\\cdot}^{\\varepsilon^{G}}(\\mu):=\\,H_{\\lambda_{\\xi}\\varepsilon}(\\mu_{\\cdot}^{\\varepsilon^{G}})\\supseteq_{\\cdot}H_{\\lambda_{\\varepsilon}\\varepsilon}\\circ P_{\\cdot}\\varepsilon\\alpha\\#(\\mu)$ presented in the end of Appendix D.1, as well as the calculations for its intrinsic derivative. ", "page_idx": 46}, {"type": "text", "text": "It is standard to check, applying It\u00f4\u2019s formula and taking expectation, that the family $(\\mu_{t})_{t\\geq0}=$ $(\\mathrm{Law}(Z_{t}))_{t\\geq0}$ satisfies, $\\forall\\varphi\\in C_{c}^{\\infty}(\\mathcal{Z}\\times(0,T))$ : ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\int_{0}^{t}\\int_{\\mathcal{Z}}\\big(\\partial_{t}\\varphi(z,t)-\\langle\\varsigma(t)D_{\\mu}R^{\\tau}(\\mu_{t},z),\\nabla_{z}\\varphi(z,t)\\rangle+\\beta\\,t r[P_{\\mathcal{E}^{G}}D_{z}^{2}\\varphi(z,t)]\\big)\\,d\\mu_{t}(z)\\,d t=0\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "with tr denoting the trace of a square matrix and $D_{z}^{2}$ the Hessian matrix acting on the $z$ variable. Notice that the process $P_{\\mathcal{E}^{G}}B$ is classically a Brownian motion in ${\\mathcal{E}}^{G}$ . Together with the fact that $\\mu_{t}$ is supported on $\\mathcal{E}^{G}$ , this implies that, for $\\beta>0$ , $\\mu_{t}$ has a density w.r.t. $\\lambda_{\\mathcal{E}^{G}}$ . ", "page_idx": 46}, {"type": "text", "text": "It is clear from the case $\\beta=0$ that the terms involving first order spatial derivatives of $\\varphi$ exactly give rise to the terms associated with functional $R^{\\tau}$ in the definition (3) of the $\\mathbf{WGF}(R_{{\\mathcal{E}}^{G}}^{\\tau,\\beta})$ . Therefore, we just need to check that for all $t\\geq0$ , the distribution defined for every $\\phi\\,\\in\\,\\bar{C}_{c}^{\\infty}(\\mathcal{Z})$ by $\\phi\\mapsto$ $\\begin{array}{r}{\\int_{\\mathcal{Z}}t r[P_{\\mathcal{E}^{G}}D_{z}^{2}\\phi(z)]d\\mu_{t}(z)}\\end{array}$ and the distribution div $\\left(D_{\\mu}H^{\\varepsilon^{G}}(\\mu_{t},\\cdot)\\,\\mu_{t}\\right)$ are equal. In fact, for all such $\\phi$ we have: ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\displaystyle\\int_{\\mathcal{Z}}\\langle\\nabla_{z}\\phi(z),D_{\\mu}\\boldsymbol{H}^{\\mathcal{E}^{G}}(\\mu_{t},z)\\rangle\\,d\\mu_{t}(z)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad=-\\displaystyle\\int_{\\mathcal{Z}}\\langle\\nabla_{z}\\phi(z),P_{\\mathcal{E}^{G}}^{T}\\nabla_{z}\\left[\\frac{d\\mu_{t}^{\\mathcal{E}^{G}}}{d\\lambda_{\\mathcal{E}^{G}}}\\right](P_{\\mathcal{E}^{G}}.z)\\rangle\\left(\\frac{d\\mu_{t}^{\\mathcal{E}^{G}}}{d\\lambda_{\\mathcal{E}^{G}}}(P_{\\mathcal{E}^{G}}.z)\\right)^{-1}d\\mu_{t}(z)}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Since $\\mu_{t}$ is concentrated in ${\\mathcal{E}}^{G}$ , we have $P_{\\mathcal{E}^{G}}\\#\\mu_{t}=\\mu_{t}$ . This and the equality $P_{\\mathcal{E}^{G}}^{2}=P_{\\mathcal{E}^{G}}$ imply the previous expression is equal to: ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\displaystyle\\int_{\\mathcal{Z}}\\langle\\nabla_{z}\\phi(P_{\\mathcal{E}^{G}}.z),P_{\\mathcal{E}^{G}}^{T}\\nabla_{z}\\left[\\frac{d\\mu_{t}^{\\mathcal{E}^{G}}}{d\\lambda_{\\mathcal{E}^{G}}}\\right](P_{\\mathcal{E}^{G}}.z)\\rangle\\left(\\frac{d\\mu_{t}^{\\mathcal{E}^{G}}}{d\\lambda_{\\mathcal{E}^{G}}}(P_{\\mathcal{E}^{G}}.z)\\right)^{-1}d\\mu_{t}(z)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad=-\\displaystyle\\int_{\\mathcal{E}^{G}}\\langle\\nabla_{z}\\phi(x),P_{\\mathcal{E}^{G}}^{T}\\nabla_{z}\\left[\\frac{d\\mu_{t}^{\\mathcal{E}^{G}}}{d\\lambda_{\\mathcal{E}^{G}}}\\right](x)\\rangle\\left(\\frac{d\\mu_{t}^{\\mathcal{E}^{G}}}{d\\lambda_{\\mathcal{E}^{G}}}(x)\\right)^{-1}d\\mu_{t}(x)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=-\\displaystyle\\int_{\\mathcal{E}^{G}}\\langle P_{\\mathcal{E}^{G}}^{T}\\nabla_{z}\\phi(x),P_{\\mathcal{E}^{G}}^{T}\\nabla_{z}\\left[\\frac{d\\mu_{t}^{\\mathcal{E}^{G}}}{d\\lambda_{\\mathcal{E}^{G}}}\\right](x)\\rangle d\\lambda_{\\mathcal{E}^{G}}(x)}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Noticing that $P_{\\mathcal{E}^{G}}^{T}\\nabla_{z}$ is the gradient calculated on ${\\mathcal{E}}^{G}$ , integrating by parts with respect to $\\lambda_{\\mathcal{E}^{G}}$ , and using again the fact that $\\mu_{t}$ is concentrated in ${\\mathcal{E}}^{G}$ , a straightforward calculation yields the desired expression $\\begin{array}{r}{\\int_{\\mathcal{Z}}t r[P_{\\mathcal{E}^{G}}D_{z}^{2}\\phi(z)]d\\mu_{t}(z)}\\end{array}$ . This concludes the proof. \u53e3 ", "page_idx": 47}, {"type": "text", "text": "Remark. Notice that, by a.s. continuity of the McKean-Vlasov diffusion (12) and the fact that ${\\mathcal{E}}^{G}$ is closed, Step 1 of the previous proof actually shows that $\\mathbb{P}(Z_{t}\\in\\mathcal{\\bar{E}}^{G},\\forall t\\geq0)=1$ . ", "page_idx": 47}, {"type": "text", "text": "Notice also that Theorem 5 bears some resemblance to Corollary $^{\\,l}$ in [30], which states that ${\\mathcal{E}}^{G}$ is stable under the traditional gradient flow of the augmented risk ( $[\\theta\\in\\mathcal{Z}^{\\bar{N}}\\mapsto R^{D A}(\\theta)\\in\\mathbb{R}]$ , as in Section 2.3). Our result shares a similar flavor, but for the MF dynamics of freely-trained NNs with equivariant data. ", "page_idx": 47}, {"type": "text", "text": "Remark. Unlike with WI distributions, initializing a shallow NN with $\\mu_{0}\\in\\mathcal{P}(\\mathcal{E}^{G})$ isn\u2019t as straightforward as using a normal distribution. Effectively (and efficiently) computing the space $\\mathcal E^{\\widetilde G}$ is actually quite challenging (as noted in [29]). ", "page_idx": 47}, {"type": "text", "text": "A natural way to ensure that $\\mu_{0}\\,\\in\\,{\\mathcal{P}}({\\mathcal{E}}^{G})$ , independently of the form of ${\\mathcal{E}}^{G}$ , is to initialize all parameters to be 0. The question of whether under such initialization the parameters will eventually exit $\\{0\\}$ (or some larger, strict subspace $E\\,\\subsetneq\\,\\mathcal{E}^{G}.$ ) and find values over the entire space ${\\mathcal{E}}^{G}$ is left for future work. Some insights on this behaviour can be sought in our experimental results, see Section 4. If true, this behavior could point towards some type of underlying hypoellipticity of the McKean-Vlasov dynamics (12) (or variants) on ${\\mathcal{E}}^{G}$ , which would be interesting to analyze, in particular in view of potential theoretical guarantees for architecture-discovering heuristics as suggested in Section 4.2. ", "page_idx": 47}, {"type": "text", "text": "Note that there is no need for seeing ${\\mathcal{E}}^{G}$ as a subspace of an ambient space $\\mathcal{Z}$ . When training with EAs, we simply force our parameters to live on ${\\mathcal{E}}^{G}$ , since we fix the architechture beforehand. Namely, our \u2018whole space\u2019 is $\\tilde{\\mathcal{Z}}=\\mathcal{E}^{G}$ (regarded directly as a vector space EG \u223c= R D) rather than $\\mathcal{Z}$ . Thus, the relevant population risk is the restricted version of the original: $\\tilde{R}:=R|_{\\mathcal{P}(\\tilde{z})}:\\mathcal{P}(\\tilde{\\mathcal{Z}})\\to\\mathbb{R}$ ; and we can apply the usual results from the MF Theory when the relevant hypothesis are satisfied by Z\u02dc and $\\tilde{R}$ . Notably, we can have global convergence of $\\tilde{R}^{\\tau,\\beta}$ to $\\operatorname*{inf}_{\\mu\\in\\mathcal{P}(\\tilde{\\mathcal{Z}})}\\tilde{\\tilde{R}^{\\tau,\\beta}}(\\mu)=\\operatorname*{inf}_{\\mu\\in\\mathcal{P}(\\mathcal{E}^{G})}\\dot{R}_{\\mathcal{E}^{G}}^{\\tau,\\beta}(\\mu)$ ", "page_idx": 47}, {"type": "text", "text": "Remark. As shown in $[38]$ (see Proposition $_{l9}$ in Appendix G for the details) the regularized versions of the involved functionals (i.e. $R^{\\tau,\\beta}$ and $R_{\\mathcal{E}^{G}}^{\\tau,\\beta}$ ) $\\Gamma$ -converge to the original $R$ as $\\tau,\\beta\\to0$ ; meaning that, for small values of the regularization parameters, we should expect the achieved optima to ressemble $\\operatorname*{inf}_{\\mu\\in{\\mathcal{P}}^{G}({\\mathcal{Z}})}R(\\mu)$ and $\\operatorname*{inf}_{\\nu\\in{\\mathcal{P}}({\\mathcal{E}}^{G})}R(\\nu)$ respectively (or, under Proposition $^{5}$ , both to $R_{*}$ ). We will also leave the exploration of how this approximation behaves as future work. ", "page_idx": 47}, {"type": "text", "text": "Finally, we provide a proof for Theorem 6 ", "page_idx": 47}, {"type": "text", "text": "Proof of Theorem $6$ . The proof structure is very similar to that of Theorem 4. Namely, it comes from noticing that for $\\mu\\in\\mathcal{P}(\\mathcal{E}^{G})$ and $z\\in{\\mathcal{E}}^{G}$ , we have: ", "page_idx": 47}, {"type": "equation", "text": "$$\nD_{\\mu}R^{D A}(\\mu,z)=D_{\\mu}R^{F A}(\\mu,z)=D_{\\mu}R^{E A}(\\mu,z).\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "We already know the first equality, as seen in Theorem 4 (since $\\mu\\in\\mathcal{P}^{G}(\\mathcal{Z})$ from lemma 1). We only need to show the last equality. Indeed, notice that: ", "page_idx": 47}, {"type": "equation", "text": "$$\nD_{\\mu}R^{F A}(\\mu,z)=\\int_{G}M_{g}^{T}.D_{\\mu}R(\\mu,z)d\\lambda_{G}(g)=P_{\\xi^{G}.}D_{\\mu}R(\\mu,z),\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "while also, from Proposition 18: ", "page_idx": 47}, {"type": "equation", "text": "$$\nD_{\\mu}R^{E A}(\\mu,z)=P_{\\xi\\alpha}^{T}.D_{\\mu}R(\\mu^{\\xi^{G}},P_{\\xi^{G}.z})=P_{\\xi^{G}}.D_{\\mu}R(\\mu,z).\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Knowing this, the rest of the proof is analogous to that of Theorem 4. Let $(\\mu_{t}^{F A})_{t\\geq0}$ , $(\\mu_{t}^{D A})_{t\\geq0}$ and $(\\mu_{t}^{E A})_{t\\geq0}$ be the WGF solutions starting from $\\mu_{0}$ for $R^{F A}$ , $R^{D A}$ and $R^{E A}$ respectively. Since $\\mu_{0}\\in\\mathcal{P}(\\mathcal{E}^{G})\\subseteq\\mathcal{P}^{G}(\\mathcal{Z})$ , from Theorem 4 we know that $(\\mu_{t}^{F A})_{t\\geq0}$ and $(\\mu_{t}^{D A})_{t\\geq0}$ coincide. Let\u2019s see that, w.l.o.g., $(\\mu_{t}^{F A})_{t\\geq0}$ coincides with $(\\mu_{t}^{E A})_{t\\geq0}$ . ", "page_idx": 47}, {"type": "text", "text": "As $R^{F A}$ is $G$ -invariant, by theorem 5, $\\forall t\\ge0$ , $\\mu_{t}^{F A}\\in\\mathcal{P}(\\mathcal{E}^{G})$ . Now, let\u2019s see that this process also satisfies $\\mathbf{WGF}(R^{E A})$ , forcing both processes to coincide by uniqueness. ", "page_idx": 47}, {"type": "text", "text": "As before, we know that $(\\mu_{t}^{F A})_{t\\geq0}$ satisfies: $\\forall\\varphi\\in C_{c}^{\\infty}(\\mathcal{Z}\\times(0,T))$ : ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\int_{0}^{T}\\int_{\\mathcal{Z}}\\left(\\partial_{t}\\varphi(z,t)-\\langle\\varsigma(t)D_{\\mu}R^{F A}(\\mu_{t}^{F A},z),\\nabla_{z}\\varphi(z,t)\\rangle\\right)d\\mu_{t}^{F A}(z)\\,d t=0\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Now, as $\\forall t\\,\\geq\\,0,\\,\\,\\,\\mu_{t}^{F A}\\,\\in\\,\\mathcal{P}(\\mathcal{E}^{G})$ , we can restrict our integral to ${\\mathcal{E}}^{G}$ . Also, we have $\\forall z\\,\\in\\,\\mathcal{E}^{G}$ : $D_{\\mu}R^{F A}(\\mu_{t}^{F A},z)\\stackrel{...}{=}\\overline{{{D}}}_{\\mu}R^{E A}(\\mu_{t}^{F A},z)$ . With these properties, $(\\mu_{t}^{F A})_{t\\geq0}$ satisfies $\\forall\\varphi\\in C_{c}^{\\infty}(\\mathcal{Z}\\times$ $(0,T))$ : ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\int_{0}^{T}\\int_{\\mathcal{E}^{G}}\\left(\\partial_{t}\\varphi(z,t)-\\langle\\varsigma(t)D_{\\mu}R^{E A}(\\mu_{t}^{F A},z),\\nabla_{z}\\varphi(z,t)\\rangle\\right)d\\mu_{t}^{F A}(z)\\,d t=0\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Making the integral over $\\mathcal{Z}$ once again (we can since $\\mu_{t}^{F A}\\,\\in\\,\\mathcal{P}(\\mathcal{E}^{G}),$ for all $t\\,\\geq\\,0$ ), we get that $(\\mu_{t}^{F A})_{t\\geq0}^{-}$ solves $\\mathbf{WGF}(R^{E A})$ starting from $\\mu_{0}$ ; thus by uniqueness: $({\\mu}_{t}^{^{\\prime}F A})_{t\\geq0}=\\overline{{(}}\\mu_{t}^{E A})_{t\\geq0}$ . ", "page_idx": 48}, {"type": "text", "text": "The last part of the theorem comes, once again, from Proposition 2, since if $R$ is invariant, its WGF exactly coincides with that of $R^{D A}$ . \u53e3 ", "page_idx": 48}, {"type": "text", "text": "F Experimental setting and further experiments ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "All the different experiments were run on Python 3.10, on a Google Colab session consisting (by default) of 2 Intel Xeon virtual CPUs (2.20GHz) and with 13GB of RAM. ", "page_idx": 48}, {"type": "text", "text": "In order to obtain results that can be visualized, we consider a simple setting where $\\mathcal{X}=\\mathcal{Y}=\\mathbb{R}^{2}$ and $\\mathcal{Z}\\;=\\;\\mathbb{R}^{2\\times2}\\;\\cong\\;\\mathbb{R}^{4}$ . We let $G\\,=\\,C_{2}$ acting on $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ via the coordinate transposition action (i.e. the group generated by the orthogonal matrix $\\binom{0}{1}\\binom{1}{0}$ ; and on $\\mathcal{Z}$ via the natural intertwining action (i.e. $M_{g}.z=\\hat{\\rho}_{g}.z.\\rho_{g}^{T})$ . We also consider the jointly equivariant activation given by $\\sigma_{*}(x,z)\\,=\\,\\sigma(z\\cdot x)\\ \\forall x\\,\\in\\,\\mathbb{R}^{2}$ , $\\forall z\\in\\mathbb{R}^{2\\times2}$ with $\\sigma:\\mathbb{R}\\rightarrow\\mathbb{R}$ a sigmoidal activation function (which is $\\mathcal{C}^{\\infty}$ and bounded) applied pointwise. Under this setting, ${\\mathcal{E}}^{G}$ can be explicitly computed as $\\xi^{G}\\:=\\:\\left\\langle\\left(\\begin{array}{c c c}{{\\frac{1}{\\sqrt{2}}}}&{{0}}\\\\ {{0}}&{{\\frac{1}{\\sqrt{2}}\\right),\\left(\\frac{0}{\\sqrt{2}}}}&{{0}}\\end{array}\\right)\\right\\rangle$ , which is a 2-dimensional subspace of the ambient 4- dimensional space. It\u2019s projection operator $P_{\\mathcal{E}^{G}}$ is also explicitly known. ", "page_idx": 48}, {"type": "text", "text": "We consider a teacher model $f_{*}=\\Phi_{\\theta^{*}}^{N_{*}}$ with $N_{*}$ fixed particles, such that $\\nu_{\\theta^{*}}^{N_{*}}$ is either arbitrary, WI or SI. Let $\\vartheta=0.5$ be a scale parameter. The arbitrary particles were chosen to be, for $N_{*}=5$ : ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\theta_{1}^{*}=\\vartheta.(-1,0,0,0.5)^{T}}\\\\ &{\\theta_{2}^{*}=\\vartheta.(0.5,1,0,1)^{T}}\\\\ &{\\theta_{3}^{*}=\\vartheta.(-0.5,0.3,1,0)^{T}}\\\\ &{\\theta_{4}^{*}=\\vartheta.(0,-1,-0.5,1)^{T}}\\\\ &{\\theta_{5}^{*}=\\vartheta.(0.7,-0.7,0.5,0.7)^{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "This was fixed in order to make the task non-trivial and interesting. The WI teacher distribution was simply chosen to be $(\\nu_{\\theta^{*}}^{N_{*}})^{G}$ , with $\\theta^{*}$ as just described, so that the corresponding teacher function resulted to be $f_{\\ast}=\\mathcal{Q}_{G}.\\Phi_{\\theta^{\\ast}}^{N_{\\ast}}$ . In other words, the WI distribution has 10 particles, corresponding to each of those of $\\theta^{*}$ , together with their image under the $G$ -action. The SI particles were also fixed, but their chosen coordinates had to be expressed in terms of the basis vector of ${\\mathcal{E}}^{G}$ (i.e. only providing 2 parameters). Particularly, they were fixed to be $N_{*}=5$ and, denoting them by $a^{*}=(a_{i}^{*})_{i=1}^{N_{*}}$ to avoid confusion, explicitly described as: ", "page_idx": 48}, {"type": "equation", "text": "$$\na_{1}^{*}=\\vartheta.(1,0)^{T},\\;a_{2}^{*}=\\vartheta.(0.5,1)^{T},\\;a_{3}^{*}=\\vartheta.(-0.5,0.3),\\;a_{4}^{*}=\\vartheta.(0,-1),\\;a_{5}^{*}=\\vartheta.(0.7,0.7).\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "As seen on Section 3.1, the teacher $f_{*}:\\mathcal{X}\\to\\mathcal{Y}$ will be an equivariant function as soon as its parameter distribution is chosen either WI or SI. Our data distribution $\\pi$ will be such that $(X,Y)\\sim\\pi$ will satisfy $X\\,\\sim\\,{\\mathcal{N}}(0,\\sigma_{\\pi}^{2}.\\mathrm{Id}_{2})$ (with $\\sigma_{\\pi}\\:=\\:4\\:,$ ), and $Y\\;=\\;f_{*}(X)$ . Namely, $\\pi_{\\mathcal{X}}$ will always be $G$ -invariant, whereas $\\pi$ will only be $G$ -invariant if $f_{*}$ is. This setting allows for testing the different results provided, without losing the properties of $\\mathcal{Q}_{G}$ as a projection (which require $\\pi_{\\mathcal{X}}$ to be $G$ -invariant, as shown in [27]). ", "page_idx": 48}, {"type": "text", "text": "We will try to mimic the teacher network by using student networks, which will be given by $\\Phi_{\\boldsymbol{\\theta}}^{N}$ ; namely, with the same $\\sigma_{*}$ , but varying values of $N$ and $\\theta\\in\\mathcal{Z}^{N}$ . We will train them to minimize the regularized population risk $R^{\\tau,\\check{\\beta}}$ given by a quadratic loss, $\\ell(y,\\hat{y})=\\|y-\\hat{y}\\|_{\\mathcal{V}}^{2}$ , and a quadratic penalization, $r(z)=\\|z\\|_{\\mathcal{Z}}^{2}$ . For this purpose, we employ a minibatch variant of the SGD training scheme provided in Equation (1) (possibly projected, as in Equation (5), when required). We will also employ the different symmetry-leveraging techniques presented in Section 2.3, such as DA, FA and EA. We refer to the free training with no SL-techniques whatsoever as the vanilla training. ", "page_idx": 49}, {"type": "text", "text": "The training parameters were fixed to be (unless explicitly stated otherwise): ", "page_idx": 49}, {"type": "text", "text": "\u2022 Step Size: $\\varsigma\\equiv\\alpha>0$ (with $\\alpha=50$ in most experiments), $\\begin{array}{r}{\\varepsilon_{N}=\\frac{1}{N}}\\end{array}$ , so that $\\begin{array}{r}{s_{k}^{N}=\\frac{\\alpha}{N}}\\end{array}$ . This was convenient, since it corresponds to the usual implementation of SGD on most common NN frameworks in Python (namely, pytorch and jax).   \n\u2022 Regularization parameters: $\\tau=10^{-4}$ and $\\beta=10^{-6}$ .   \n\u2022 Batch Size: It was chosen to be $B=20$ .   \n\u2022 Number of Training Epochs: In line with the statement of Theorem 1, to observe phenomena at a MF scale, we need an amount of iterations (commonly known as epochs in the ML literature) that is proportional to the number of particles. For this purpose, we fix an observation time horizon of $T=20$ . All training schemes were performed for a total of $N_{e}=N\\cdot T$ epochs (iterations). An additional \u2018granularity\u2019 parameter (usually set to be $g r=5$ ) is introduced to determine how often in the dynamic we will observe and save the training losses and particle positions: we do so every $\\lfloor\\frac{N_{e}}{g r}\\rfloor$ steps. Notice that $N_{e}$ depends on $N$ , and so models with different values of $N$ were trained for a different amount of epochs.   \n\u2022 Student Initialization: The student\u2019s particles, $\\theta\\in\\mathcal{Z}^{N}$ , are initialized i.i.d. from some $\\mu_{0}$ that is chosen to be either WI or SI. When WI-initialized, they are sampled from a random gaussian $\\begin{array}{r}{Z\\sim\\mathcal{N}(0,\\frac{1}{16})}\\end{array}$ . When SI-initialized, particles are taken to be $P_{\\mathcal{E}^{G}}.Z$ with $Z$ as before.   \n\u2022 Number of Repetitions: Each experiment was repeated a total of $N_{r}\\;=\\;10$ times to ensure consistency. Each repetition, a different random seed was employed to: initialize the student\u2019s particles, generate the training data, and generate the noise for the SGD iteration. In particular, on a fixed repetition, all models were trained with the same data and the same noise being applied on SGD updates. ", "page_idx": 49}, {"type": "text", "text": "Remark. As $N_{e}$ is chosen to be proportional to the number of particles, $N$ , computational burden and memory requirements quickly became heavy for the simple machines we employed (which didn\u2019t even have a dedicated $G P U,$ ). This is the reason why we don\u2019t scale our experiments beyond the $N=5000$ case. As reference, for $N=5000\\:a$ single training (with the above hyperparameters) of a single model (either of vanilla, DA, FA or $E A$ ) took $\\approx15$ minutes (which quickly amounts to large amounts of running time for the $N_{r}=10$ repetitions, the 4 different training schemes and the 6 possible settings with WI or SI initialization and arbitrary, WI or SI teacher). This is a clear point of improvement and shall be tackled in future work. ", "page_idx": 49}, {"type": "text", "text": "Remark. As here mentioned, on every fixed \u2018repetition\u2019 of the experiments, the same noise was used during the SGD training iterations for the vanilla, $D A$ and FA schemes. The EA scheme, despite using the same seeds for the data and student initializations, didn\u2019t have the same noise applied during SGD. This was because, despite using the same seed for the noise generator, noise for our EAs was only 2-dimensional (since EAs are parametrized by $\\dot{\\mathcal{E}}^{G}$ ), while it was 4-dimensional for the other schemes. This made the resulting training schemes have an additional layer of noise separating them; and so solving this issue, in order to properly visualize Theorem 6, becomes fundamental. ", "page_idx": 49}, {"type": "text", "text": "Remark. Notice that the $N_{r}=10$ performed repetitions were largely enough to allow for plots with error bars (actually, we do boxplots which encode the variability of the different quantities more precisely) that allow for significant analysis of the observed phenomena. We do not go beyond $N_{r}=10$ due to the low computational capabilities of our machines, and the already high computational cost of running the experiments (for thousands of hidden units and epochs, in many different settings, and involving the calculation of Wasserstein Distances, as we\u2019ll comment below). ", "page_idx": 49}, {"type": "text", "text": "To facilitate the implementation of the ideas behind our EA models, we use the group and representations tools from the emlp repository provided as part of [29]. This code is openly available and has an MIT License for unrestricted access. We employ it to numerically (and efficiently) determine the space ${\\mathcal{E}}^{G}$ (namely, its basis), as well as $P_{\\mathcal{E}^{G}}$ . We do remark that these calculations were correct only up to a precision of $10^{-8}$ , which results in a slight burden for our empirical results. On the other hand, regarding the implementation of EMLPs provided in the package (EAs in our setting), some slight modifications to the source code had to be performed in order to correctly represent our setting. ", "page_idx": 49}, {"type": "text", "text": "", "page_idx": 50}, {"type": "text", "text": "On a similar note, we can numerically compute the squared Wasserstein-2 distance between two empirical distributions of particles by employing the pyot library. This allows us to evaluate to what extent our resulting models are close to each other in terms of their particle distribution $\\nu_{\\theta}^{N}$ (which is what the MF approach suggests). In order to fix a common scale in which the experiments can be compared, for different values of $N$ , and mitigate the effects of fluctuating empirical estimates (mainly for small values of $N$ , and in the low dimensions considered), we consider a natural normalization of the Wasserstein-2 distance, which we refer to as the RMD (Relative-Measure-Distance). This is defined as: $\\begin{array}{r}{\\mathbf{RMD}^{2}(\\mu,\\nu)=\\frac{W_{2}^{2}(\\mu,\\nu)}{M_{\\mu}^{2}+M_{\\nu}^{2}}}\\end{array}$ where $M_{\\mu}^{2}=2\\mathbb{E}[\\|Z\\|^{2}]$ for $Z\\sim\\mu$ (so that $0\\leq\\mathbf{RMD}\\leq1)$ ). The RMD provided a good metric for the experiments here presented. Notice that, as $N\\rightarrow\\infty$ , by the L.L.N. for empirical distribution following from the MFL convergence, the RMD is expected to stabilize at the corresponding value of the limiting distributions. Therefore, up to a multiplicative quantity approaching a (finite, non null, in our case) constant, we are observing the behavior of the Wasserstein-2 distance. A drawback from using the Wasserstein-2 metric, is that calculating them can be very expensive computationally. This is another one of the reasons why our experiments only get to $N=5000$ particles. ", "page_idx": 50}, {"type": "text", "text": "F.1 Study for varying $N$ ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Beyond the analyisis already provided in Section 4, we here provide some meaningful insights. We want to observe to what extent the properties proved in Section 3.3 for the WGF of $R^{\\tau,\\beta}$ can be observed in practice. For this purpose, we observe, for $N\\,\\in\\,\\{5,10,50,100,500,1000,5000\\}$ , different relevant quantities to evaluate the different combinations of teachers and students. ", "page_idx": 50}, {"type": "text", "text": "For this set of experiments, WI-initialized students were trained with the usual SGD scheme from Equation (1); while, SI-initialized students, were trained with the projected SGD dynamics from Equation (5). Models for the different schemes to be compared, are all initialized with the exact same (random) particles. ", "page_idx": 50}, {"type": "text", "text": "Figure 3 displays some comparisons between particle distributions in terms of RMD at the end of training, knowing that they were all initialized with the same particles drawn from $\\mu_{0}\\,\\in\\,\\mathcal{P}(\\mathcal{E}^{G})$ . From this, we can visually see that, when the teacher distribution is either WI of SI, the resulting distribut n from vanilla training stays on ${\\mathcal{E}}^{G}$ (since $\\mathbf{RMD}^{2}(\\nu_{N T}^{N},(\\nu_{N T}^{N})^{\\mathcal{E}^{G}})$ is small) increasingly more as $N$ becomes large. This fact is absolutely remarkable, since, for a WI teacher there should be no reason why the vanilla training (that\u2019s completely free, in principle) shouldn\u2019t escape ${\\mathcal{E}}^{G}$ to achieve a better approximation of $f_{*}$ . On the other hand, in every single teacher setting, almost independently of $N$ , both DA and FA consistently remain within $\\dot{\\mathcal{E}}^{G}$ (as expected, even if $f_{*}$ isn\u2019t equivariant). For an arbitrary teacher, we see that the vanilla training distribution readily leaves ${\\mathcal{E}}^{G}$ to better approximate $f_{*}$ , which isn\u2019t a predicted behaviour from our theory (we have no guarantees of \u2018leaving ${\\bf\\nabla}\\mathcal{E}^{G}$ when data isn\u2019t equivariant\u2019), but motivates the heuristic defined in Section 4. ", "page_idx": 50}, {"type": "text", "text": "Still from Figure 3, we can see that, as $N$ grows bigger, the end-of-training distribution of the vanilla scheme becomes closer and closer to that of DA and FA (from Theorem 4 we actually expect them to be equal in the limit). A similar result is obtained relating vanilla, DA and FA to the EA scheme; the values are however larger than before and less significantly close in general. This is possibly due to the different noises employed during training (as mentioned in a remark above). We do however notice that for increasing values of $N$ , the EA, DA, FA and vanilla schemes (the latter only under equivariant $f_{*}$ ) tend towards coinciding, which serves to illustrate the constatations from Theorem 6. Finally, notice that the results obtained for WI and SI teachers present almost no quantitative differences whatsoever between them. ", "page_idx": 50}, {"type": "text", "text": "In Figure 4 we present a visualization of the final particle distribution, after an SI-initialized training under a WI teacher $f_{*}$ , of the vanilla, DA, FA and EA schemes (on a single realization of the experiment). At least visually (and macroscopically), it seems like all these regimes followed (approximately) the same flow, as they end up with an approximately equal particle distribution. This isn\u2019t a rigorous comparison at all, and providing better quantitative comparisons between the methods is to be considered for future work. As a counterfactual, we provide in Figure 5 the results of an SI-initialized training that is performed under a non-equivariant $f_{*}$ . We can see that the vanilla model readily leaves ${\\mathcal{E}}^{G}$ to achieve a better approximation of $f_{*}$ , while the DA, FA and EA schemes \u2018stay inside\u2019 (roughly coinciding between them as well). ", "page_idx": 50}, {"type": "image", "img_path": "L86glqNCUj/tmp/3d079a4884e37a7d72624344b7310096ce4d28d75190ebf7439b609dd02200de.jpg", "img_caption": ["Figure 3: RMD comparisons between training regimes, for different values of $N$ , at the end of an SI-initialized training for $N_{e}$ epochs. Each column corresponds to a teacher with, respectively, an arbitrary, WI and SI distribution. Row $^{\\,I}$ displays $\\mathbf{RMD}^{2}(\\nu_{N_{e}}^{N},(\\nu_{N_{e}}^{N})^{\\mathcal{E}^{G}})$ for the different regimes, in order to evaluate to what extent the training remained within ${\\mathcal{E}}^{G}$ . Row 2 displays the RMD between DA, FA and vanilla training regimes; and Row 3 does the same for each of them against EA. "], "img_footnote": [], "page_idx": 51}, {"type": "text", "text": "", "page_idx": 51}, {"type": "text", "text": "Figure 6 also displays RMD comparisons between particle distributions at the end of training, but for an (identic) initialization with particles drawn from $\\mu_{0}\\in\\mathcal{P}^{G}(\\mathcal{Z})$ . Now, unlike the SI case, with particles sampled i.i.d. from a WI distribution, nothing ensures that the resulting $\\nu_{0}^{N}$ will be WI as well. Namely, in this case the limit as $N\\rightarrow\\infty$ becomes significantly more important to visualize the theoretical results. Indeed, since $\\nu_{0}^{N}$ isn\u2019t necessarily WI, we no longer have a guarantee that the finite- $\\mathcal{N}$ networks trained with DA, FA or vanilla methods will be close to each other in any sense. We do however notice on Figure 6 that, for increasing $N$ , the end-of-training distributions of DA, FA and vanilla schemes (the latter only when $f_{*}$ is equivariant) become increasingly closer to their tshyem SmI-eitnriiztieadl ivzeerds ieoxnpse (rinmamenetlsy),. $\\mathbf{RMD}^{2}(\\nu_{N T}^{N},(\\nu_{N T}^{N})^{G})$ y  bTehceoomreesm s 4m, aflloer rl, atrhgoeu wneev seer ea ts hsatm aDllA a as nidn $N$ FA become indistinguishably close, no matter the teacher\u2019s properties; also, when $f_{*}$ is equivariant, they both \u2018coincide\u2019 with the vanilla scheme. A comparison between the EA scheme and the result from projecting the FA scheme on the last step is also presented. It is used simply to illustrate that, in principle, directly training on ${\\mathcal{E}}^{G}$ isn\u2019t necessarily comparable to performing \u2018free-training\u2019 and projecting the resulting particle distribution only on the last step (even when an SL technique such as FA is used). ", "page_idx": 51}, {"type": "image", "img_path": "L86glqNCUj/tmp/b081726375404d877a91016039c00c65a316a14cdd5e5ed1939e57b8647202ee.jpg", "img_caption": ["Student Particles' Positions (vanilla) ", "Student Particles' Positions (FA) "], "img_footnote": [], "page_idx": 52}, {"type": "image", "img_path": "L86glqNCUj/tmp/6e1411d022e6eb755faf82ef6a107863578c4be4d59ed5779f1b946f52e2128f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 52}, {"type": "image", "img_path": "L86glqNCUj/tmp/9e52431b413f2f574fd645d6f53f419bd082265577bae668978ca4df005e2cc1.jpg", "img_caption": ["Student Particles' Positions (DA) ", "Student Particles' Positions (EA) "], "img_footnote": [], "page_idx": 52}, {"type": "image", "img_path": "L86glqNCUj/tmp/e0c913fa497e60d2217e0f90b6c321ed2f1bfdc4a75f4f527fc99584d72f6075.jpg", "img_caption": ["Figure 4: Visualization of the NN particles after training under the vanilla, DA, FA and EA schemes, for a single realization of the experiment. Squares represent the teacher particles (which are WI), dots represent the student particles, and the hyperplane is ${\\mathcal{E}}^{G}$ . The bigger plots show an aerial view of the global particle distribution after training; and the minor plots below them show a viewpoint at the level of (and parallel to) ${\\mathcal{E}}^{G}$ . The student particles were all initialized to be SI (and to coincide at initialization between the different schemes), and trained with equation (5) correspondingly applying the proper SL technique. "], "img_footnote": [], "page_idx": 52}, {"type": "image", "img_path": "L86glqNCUj/tmp/6c9fef1728892938e8522b570d12f859709ee2153b453ae566b2a6a648058f1f.jpg", "img_caption": ["Student Particles' Positions (vanilla) ", "Student Particles' Positions (FA) "], "img_footnote": [], "page_idx": 53}, {"type": "image", "img_path": "L86glqNCUj/tmp/b0a39292109492df70f3a5c407bbab84deca61863426d5511be83445f912b9a6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 53}, {"type": "image", "img_path": "L86glqNCUj/tmp/737b57e8731fe7f34a06ca106630b5337754b5f3f2711043c1a00a81baef520a.jpg", "img_caption": ["Student Particles' Positions (DA) ", "Student Particles' Positions (EA) "], "img_footnote": [], "page_idx": 53}, {"type": "image", "img_path": "L86glqNCUj/tmp/74daa046f46d662f90f70c6efd2f4431ae419a1c00b0012173f1505c87ff5d59.jpg", "img_caption": [], "img_footnote": [], "page_idx": 53}, {"type": "text", "text": "Figure 5: Visualization of the NN particles after training under the vanilla, DA, FA and EA schemes for a single realization of the experiment. Squares represent the teacher particles (which are arbitrary), dots represent the student particles, and the hyperplane is ${\\mathcal{E}}^{G}$ . The bigger plots show an aerial view of the global particle distribution after training; and the minor plots below them show a viewpoint at the level of (and parallel to) ${\\mathcal{E}}^{G}$ . The student particles were all initialized to be SI (and to coincide at initialization between the different schemes), and trained with equation (5) correspondingly applying the proper SL technique. Notice how the particles for the vanilla scheme readily leave $\\bar{\\mathcal{E}}^{\\tilde{G}}$ (despite the noise being projected onto it) and seem to approach the teacher particles. ", "page_idx": 53}, {"type": "image", "img_path": "L86glqNCUj/tmp/cf38bec0a206d973c447b9f9af081e368a5bf252047b470d0b1e7363d4d15c8c.jpg", "img_caption": ["(a) arbitrary teacher ", "(b) WI teacher ", "(c) SI teacher "], "img_footnote": [], "page_idx": 54}, {"type": "text", "text": "Figure 6: RMD comparisons between training regimes, for different values of $N$ , at the end of a WI-initialized training for $N_{e}$ epochs. Each column corresponds to a teacher with, respectively, an arbitrary, WI and SI distribution. Row $^{\\,l}$ displays $\\mathbf{RMD}^{2}(\\bar{\\nu_{N_{e}}^{N}},(\\nu_{N_{e}}^{N})^{G})$ for the different regimes, to evaluate to what extent the training remained WI. Row 2 displays the RMD between DA, FA and vanilla training regimes; as well as a comparison between EA and the projected particles of FA. ", "page_idx": 54}, {"type": "image", "img_path": "L86glqNCUj/tmp/c130bee9a1b8ecd13831522c680fdbd1a1565812007e76cef6fe81eae50cc22d.jpg", "img_caption": ["Figure 7: Approximation of $L^{2}$ -distance between each model and its symmetrized version for increasing values of $N$ . Each column corresponds to a different teacher as before. Row 1 corresponds to the SI-initialized experiment and Row 2 to the WI-initialized one. ", "(a) arbitrary teacher ", "(b) WI teacher ", "(c) SI teacher "], "img_footnote": [], "page_idx": 54}, {"type": "image", "img_path": "L86glqNCUj/tmp/71e61ba1befbc6c89e266ea37522516a4bb3d04e5f4add42dbb9bffae4ca5296.jpg", "img_caption": ["Figure 8: Approximation of $L^{2}$ -distance between each model and the corresponding teacher network $f_{*}$ , for increasing values of $N$ . Each column corresponds to a different teacher as before; Row 1 corresponds to the SI-initialized experiment and Row 2 to the WI-initialized one. "], "img_footnote": [], "page_idx": 55}, {"type": "image", "img_path": "L86glqNCUj/tmp/ea28d79b166ed2dd7f271397a947293319a9dbc049523b38f09de0d1a7b7d3fd.jpg", "img_caption": ["Figure 9: Approximation of $L^{2}$ -distance between each model and the symmetrized teacher network $\\mathcal{Q}_{G}.f_{*}$ , for increasing values of $N$ . Each column corresponds to a different teacher as before; Row $^{\\,l}$ corresponds to the SI-initialized experiment and Row 2 to the WI-initialized one. "], "img_footnote": [], "page_idx": 55}, {"type": "text", "text": "Now, beyond the analysis of the underlying particle distributions after training, we turn our focus to comparisons of the resulting models. We measure some distances in $L^{2}(\\mathcal{X},\\mathcal{Y};\\bar{\\pi}_{\\mathcal{X}})$ , by approximating $\\|\\cdot\\|_{L^{2}(\\mathcal{X},\\mathcal{Y};\\pi_{\\mathcal{X}})}$ with a Monte-Carlo sample of 100 random data points drawn from $\\pi$ . ", "page_idx": 55}, {"type": "text", "text": "Figure 7 shows that the observed behaviour for the underlying particles, $\\nu_{\\theta}^{N}$ , of each model, is consistent with the behaviour of the obtained model $\\Phi_{\\boldsymbol{\\theta}}^{N}$ . That is, as particles become close to being symmetric in some sense, the resulting shallow model also becomes increasingly equivariant as well. ", "page_idx": 55}, {"type": "text", "text": "Finally, Figure 8 and Figure 9 illustrate quite well the observations from Corollary 1. When our teacher isn\u2019t equivariant, models trained using any kind of SL technique end up suffering from the inductive bias introduced by the symmetric assumption (something that\u2019s hinted by the symmetrization gap characterization from Lemma 2 presented in SuppMat-B.3). On the other hand, the vanilla model thrives in approximating $f_{*}$ as it is capable of breaking the assumed symmetry, unlike DA, FA and EA. The training regimes that use SL techniques are effectively approximating $\\mathcal{Q}_{G}.f_{*}$ , as shown in Figure 9 (and proven in Corollary 1). We also notice that, for the WI-initialized experiments, EAs end up suffering from their constraint of staying within ${\\mathcal{E}}^{G}$ , as they can\u2019t approximate $f_{*}$ (or $\\mathcal{Q}_{G}.f_{*})$ as well as DA or FA (even when the teacher is $\\mathbf{S}\\mathbf{I}$ ). This isn\u2019t the case for the SI-initialized experiments, where the performance of DA, FA and EA (and vanilla only for equivariant $f_{*}$ ) is quite closely comparable (once again, hinting at Theorem 6). We also notice a general trend showing that, for bigger $N$ , the approximations of $f_{*}$ (or, eventually, $\\mathcal{Q}_{G}.f_{*})$ become increasingly better (specially for the WI-initialization). ", "page_idx": 55}, {"type": "text", "text": "", "page_idx": 56}, {"type": "text", "text": "F.2 Heuristic algorithm for discovering EA parameter spaces ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "The proposed heuristic that we infer from the results on the previous experimental setting is quite thoroughly described in Section 4.2. We only notice that, for this particular setting, the learning rate was chosen to be $\\alpha=20$ (to better approximate the MFL conditions). Beyond the description of the proposed heuristic and the simple example visualized in Figure 2, we also provide Figure 10 here, illustrating a possible threshold choice in that setting. ", "page_idx": 56}, {"type": "text", "text": "Considering $E_{j}$ for $j\\,=\\,0,1,\\ldots$ as the spaces that are discovered on each step of the heuristic, Figure 10 displays the values of: $\\mathbf{RMD}^{2}(\\nu_{\\theta}^{N},P_{E_{j}}\\#\\nu_{\\theta}^{N})$ and $\\mathbf{RMD}^{2}(\\nu_{\\theta}^{N},P_{\\mathcal{E}^{G}}\\#\\nu_{\\theta}^{N})$ ; both before and after training on a given heuristic step $j$ . The red line simbolizes a possible value of $\\delta$ that could be fixed to detect whenever the obtained particle distribution after training stayed on $E_{j}$ . In the case of this example, on steps 0 and 1 we would decide that the training left the original space $E_{j}$ , but we wouldn\u2019t do so on step 2, allowing us to fix $\\mathcal{E}^{G}\\,:=\\,E_{2}$ . As shown by the values of $\\mathbf{RMD}^{2}(\\nu_{\\theta}^{N},P_{\\mathcal{E}^{G}}\\#\\nu_{\\theta}^{N})$ , we wouldn\u2019t be too far off with our prediction. ", "page_idx": 56}, {"type": "text", "text": "Despite this proposed heuristic being potentially interesting for real-world applications, we acknowledge that the setting where it is applied here might be too simple, synthetic and idealized. On one hand, this provided a clean-enough framework, where the underlying phenoma could be easily observed. However, in order to properly validate our heuristic approach, experiments with more complex settings (and with larger and more intricate datasets) need to be performed. These should also be coupled with sound theoretical guarantees, whose exploration we leave for future work. ", "page_idx": 56}, {"type": "text", "text": "Finally, we here provide some further details on the possible connections of our proposed heuristic, to the \u2018symmetry-discovery\u2019 method presented in [72]. In their work, they employ an architecture based on relaxed group convolution layers, which allows to \u2018detect\u2019 breaks of the supposed data symmetry, by observing the un-alignment of the layer weights. ", "page_idx": 56}, {"type": "text", "text": "As in our work, their method starts with the most possibly constrained architecture: the null space in our case, and the \u2018perfectly aligned weights\u2019 in theirs. This ensures that the model will start respecting symmetry with respect to the largest possible group; only for the training on data to cause these symmetries to \u2018break\u2019 overtime. Their method seemingly works in a single training iteration, while ours iteratively constructs the invariant linear subspace $\\dot{\\mathcal{E}}^{G}$ by adding one new \u2018symmetry-breaking dimension\u2019 at a time. Our Theorem 5 guarantees that, in the MF scale, our method won\u2019t leave ${\\mathcal{E}}^{G}$ ; but we have yet to establish symmetry-breaking guarantees for our heuristic, comparable to Proposition 3.1 in [72]. ", "page_idx": 56}, {"type": "text", "text": "Finally, it\u2019s important to note that neither one of the methods is truly discovering the \u201cunderlying symmetry\u201d of the data. They are both closer to simply \u201coptimizing architectures\u201d which are compatible with data symmetries: either finding the \u201cright\u201d subspace $\\mathcal{E}^{\\breve{G}}$ , or the \u201cright\u201d weights for each group convolution fliter. Identifying the true underlying structure of data symmetries is a much harder problem that is yet to be tackled in both cases. ", "page_idx": 56}, {"type": "text", "text": "G Further theoretical insights ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "The following result provides consistency guarantees when the regularization parameters $\\tau$ and $\\beta$ are small, and is a slight extension of a result in [38]. ", "page_idx": 56}, {"type": "image", "img_path": "L86glqNCUj/tmp/983638f85872da44695a8c097d6a77a2e79e48c593905f0cffa926930ee8c149.jpg", "img_caption": ["Figure 10: RMD comparison between the empirical student particle distribution, $\\nu_{\\theta}^{N}$ , to both $P_{E_{j}}\\#\\nu_{\\theta}^{N}$ and $(\\nu_{\\theta}^{N})^{\\mathcal{E}^{G}}$ (where $j$ is the heuristic step). These are performed at the beginning and the end of training on every fixed heuristic step. The red line is placed at the value $10^{-2}$ and represents a possible threshold $\\delta$ , to be used in the heuristic to determine whether training left $E_{j}$ or not. "], "img_footnote": [], "page_idx": 57}, {"type": "text", "text": "Proposition 19 ( $\\Gamma$ -convergence, as in ). Let $\\mathcal{Z}=\\mathbb{R}^{D}$ . If $R$ is $W_{p}$ -continuous, $\\nu$ is a Gibbs measure of potential $U$ , and both $U$ and $r$ satisfy assumption 2 (or alternatively, are equal to $O$ ), then $R_{\\nu}^{\\tau,\\,\\beta}$ $\\Gamma$ -converges to $R$ when $\\tau,\\beta\\downarrow0$ . Particularly, given $\\mu^{*,\\,\\tau,\\,\\beta,\\,\\nu}$ the minimizer of $R_{\\nu}^{\\tau,\\,\\beta}$ , we have ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\tau,\\beta\\to0}R(\\mu^{*,\\tau,\\,\\beta,\\,\\nu})=\\operatorname*{inf}_{\\mu\\in{\\mathcal P}_{2}({\\mathcal Z})}R(\\mu).\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "In particular, every cluster point of $(\\mu^{*,\\,\\tau,\\,\\beta,\\,\\nu})_{\\tau,\\beta}$ is a minimizer of $R$ . ", "page_idx": 57}, {"type": "text", "text": "Proof of Proposition $_{l9}$ . We follow the exact same proof structure as [38], employing essentially their same techniques. However, we do adapt it to the case of taking the simultaneous limit of $\\tau,\\beta\\to0$ , and so we do include it for completeness. ", "page_idx": 57}, {"type": "text", "text": "Let $(\\tau_{n})_{n\\in\\mathbb{N}}$ and $(\\beta_{n})_{n\\in\\mathbb{N}}$ be two positive sequences decreasing to 0. On the one hand, since $R$ is continuous (weakly if $p=0$ or in $W_{p}$ for other $p\\geq1$ ) and $H_{\\nu}(\\mu)=D(\\mu||\\nu)\\geq0$ , for all $\\mu_{n}\\to\\mu$ (in the appropiate sense), we have ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to+\\infty}R_{\\nu}^{\\tau_{n},\\beta_{n}}(\\mu_{n})\\geq\\operatorname*{lim}_{n\\to+\\infty}R(\\mu_{n})=R(\\mu).\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "On the other hand, given $\\mu\\in\\mathcal{P}_{p}(\\mathcal{Z})$ , consider $\\rho$ to be the heat kernel in $\\mathcal{Z}=\\mathbb{R}^{D}$ and $\\rho_{n}(x):=$ $\\beta_{n}^{-D}\\nu(x/\\beta_{n})$ . In particular, from [1] (as the heat kernel has finite $p$ -th moments) we know that \u00b5n := \u00b5 \u2217\u03c1n \u2212n\u2212\u2192\u2212\u2212\u221e\u2192 in $W_{p}$ (or weakly if it is the case). ", "page_idx": 57}, {"type": "text", "text": "Now, since the function $h(x):=x\\log(x)$ is convex, from Jensen\u2019s inequality we get that ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\int_{\\mathcal{Z}}h(\\mu*\\rho_{n})d x\\leq\\int_{\\mathcal{Z}}\\int_{\\mathcal{Z}}h\\left(\\rho_{n}(x-y)\\right)\\mu(d y)d x=\\int_{\\mathcal{Z}}h(\\rho_{n}(x))d x=\\int_{\\mathcal{Z}}h(\\rho(x))d x-D\\,\\log(\\sqrt{2\\beta_{n}})\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Besides, we have (denoting here $g(x)=e^{-U(x)})$ : ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\int_{\\mathcal{Z}}(\\mu\\ast\\rho_{n})\\log(g)d x=-\\int_{\\mathcal{Z}}\\mu(d y)\\int_{\\mathcal{Z}}\\rho_{n}(x)U(x-y)d x\\geq-C\\left(1+\\int_{\\mathcal{Z}}|y|^{2}\\mu(d y)\\right).\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "The last inequality is due to the quadratic growth of $U$ ; and by the same argument on $r$ : ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\int_{\\mathcal{Z}}(\\mu\\ast\\rho_{n})r d x=\\int_{\\mathcal{Z}}\\mu(d y)\\int_{\\mathcal{Z}}\\rho_{n}(x)r(x-y)d x\\leq C\\left(1+\\int_{\\mathcal{Z}}|y|^{2}\\mu(d y)\\right).\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Notice that whenever $U\\equiv0$ or $r\\equiv0$ , despite them not satisfying assumption 2, we still get the same inequalities (since the leftmost term would be $_{0}$ ). ", "page_idx": 57}, {"type": "text", "text": "Now, as $R$ is $W_{p}$ -continuous, $R(\\mu_{n})\\xrightarrow[n\\to\\infty]{}R(\\mu)$ , and: ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{lim}_{n\\to+\\infty}R_{\\nu}^{\\tau_{n},\\,\\beta_{n}}(\\mu*\\nu_{n})}\\\\ &{\\qquad\\qquad\\leq R(\\mu)+\\operatorname*{lim}_{n\\to+\\infty}\\displaystyle\\operatorname*{sup}_{n\\to+\\infty}\\tau_{n}\\left(\\int_{\\mathbb{Z}}(\\mu*\\rho_{n})r d x\\right)}\\\\ &{\\qquad\\qquad+\\operatorname*{lim}_{n\\to+\\infty}\\beta_{n}\\left(\\int_{\\mathbb{Z}}h(\\mu*\\rho_{n})d x-\\int_{\\mathbb{Z}}(\\mu*\\rho_{n})\\log(g)d x\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "And, as $\\begin{array}{r}{\\operatorname*{lim}_{n\\to\\infty}\\beta_{n}\\log(\\sqrt{2\\beta_{n}})=0}\\end{array}$ and the rest of the terms are bounded, we conclude that: ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to+\\infty}R_{\\nu}^{\\tau_{n},\\,\\beta_{n}}(\\mu*\\rho_{n})\\leq R(\\mu)\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "In particular, denoting by $\\mu_{*}^{\\tau,\\beta,\\nu}$ the unique minimizer of $R_{\\nu}^{\\tau,\\beta}$ , then from the previous expressions we get $\\forall n\\in\\mathbb{N}$ and $\\forall\\mu\\in\\mathcal{P}_{p}(\\mathcal{Z})$ : ", "page_idx": 58}, {"type": "equation", "text": "$$\nR(\\mu_{*}^{\\tau_{n},\\beta_{n},\\nu})\\leq R_{\\nu}^{\\tau_{n},\\beta_{n}}(\\mu_{*}^{\\tau_{n},\\beta_{n},\\nu})\\leq R_{\\nu}^{\\tau_{n},\\beta_{n}}(\\mu*\\rho_{n})\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "So that, ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}R(\\mu_{*}^{\\tau_{n},\\beta_{n},\\nu})\\leq\\operatorname*{lim}_{n\\to+\\infty}\\!\\!R_{\\nu}^{\\tau_{n},\\beta_{n}}(\\mu*\\rho_{n})\\leq R(\\mu),\\quad\\mathrm{for~all~}\\mu\\in P_{2}(\\mathcal{Z}).\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Finally, we provide, for completeness, a proof of Lemma 2 : ", "page_idx": 58}, {"type": "text", "text": "Proof of Lemma 2 (based on $I27J$ and [40]). As $H\\leq G$ is a compact group and $\\pi$ is $H$ -invariant, from proposition 10 we know that $f^{*}\\,=\\,\\mathbb{E}_{\\pi}[Y|X=\\,\\cdot\\,]$ lives in $\\bar{f}^{*}\\in\\bar{L}_{H}^{2}\\(\\mathcal{X},\\mathcal{Y};\\pi|_{\\mathcal{X}})$ . Consider any $\\bar{f}\\in\\bar{L}^{2}(\\mathcal X,\\mathcal y;\\pi|_{\\mathcal X})$ , by Lemma 1 from [27] (which applies since $\\pi|_{\\mathcal{X}}$ is $G$ -invariant), we can decompose $f$ as $f=\\overline{{f}}_{G}+f_{G}^{\\perp}$ , where $\\overline{{f}}_{G}=\\mathcal{Q}_{G}f$ is its symmetric part and $f_{G}^{\\perp}\\,=\\,f\\,-\\,\\mathcal{Q}_{G}f$ its antisymmetric part. A standard calculation of the population risk under the quadratic loss setting (see the proof of Corollary 1 for further insight) gives: $\\bar{R}(f)=R_{*}+\\|f-f^{*}\\|_{L^{2}(\\mathcal{X},\\mathcal{Y};\\pi_{\\mathcal{X}})}^{2}$ , and so: ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Delta(f,\\mathcal{Q}_{G}.f)=R(f)-R(\\mathcal{Q}_{G}.f)=\\mathbb{E}\\left[\\|f^{*}(X)-f(X)\\|_{\\mathcal{Y}}^{2}\\right]-\\mathbb{E}\\left[\\|f^{*}(X)-\\overline{{f}}_{G}(X)\\|_{\\mathcal{Y}}^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "which can be written as: ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta(f,\\mathcal{Q}_{G}f)=\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\|f^{*}(X)-\\overline{{f}}_{G}(X)\\|_{y}^{2}-2\\langle f^{*}(X)-\\overline{{f}}_{G}(X),f_{G}^{\\perp}(X)\\rangle_{y}+\\|f_{G}^{\\perp}(X)\\|_{y}^{2}\\right]}\\\\ &{\\qquad\\qquad-\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\|f^{*}(X)-\\overline{{f}}_{G}(X)\\|_{y}^{2}\\right]}\\\\ &{\\qquad\\qquad=-2\\langle f^{*}-\\overline{{f}}_{G},f_{G}^{\\perp}\\rangle_{L^{2}(\\mathcal{X},\\mathcal{Y};\\pi_{\\mathcal{X}})}+\\|f_{G}^{\\perp}\\|_{L^{2}(\\mathcal{X},\\mathcal{Y};\\pi_{\\mathcal{X}})}^{2}}\\\\ &{\\qquad\\quad=-2\\langle f^{*},f_{G}^{\\perp}\\rangle_{L^{2}(\\mathcal{X},\\mathcal{Y};\\pi_{\\mathcal{X}})}+\\|f_{G}^{\\perp}\\|_{L^{2}(\\mathcal{X},\\mathcal{Y};\\pi_{\\mathcal{X}})}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Where we used that $\\begin{array}{r l r}{\\langle\\overline{{f}}_{G},f_{G}^{\\perp}\\rangle_{L^{2}(\\mathcal{X},\\mathcal{Y};\\pi_{\\mathcal{X}})}}&{=}&{0}\\end{array}$ . The first term on the right hand side, $-2\\langle f_{\\underline{{\\:}},\\underline{{\\:}}}^{*},f_{G}^{\\perp}\\rangle_{L^{2}(\\underline{{\\:}},\\underline{{\\:}});\\pi_{\\chi})}$ , is what [40] call the mismatch between the real underlying model (which is only $H$ -equivariant) and the symmetrized version of our model (which is made entirely $G$ -equivariant). ", "page_idx": 58}, {"type": "text", "text": "Now, when $\\pi$ is $G$ -equivariant, by proposition 10, $\\mathcal{Q}_{G}f^{*}=f^{*}$ , and so: $-2\\langle f^{*},f_{G}^{\\perp}\\rangle_{L^{2}(\\mathcal{X},\\mathcal{Y};\\pi_{\\mathcal{X}})}=0$ , giving us the desired result: ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\Delta(f,\\mathcal{Q}_{G}f)=\\|f_{G}^{\\perp}\\|_{L^{2}(\\mathcal{X},\\mathcal{Y};\\pi_{\\mathcal{X}})}^{2}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Lemma 2 essentially says that if we try to symmetrize a model with respect to a group that has \u2018more symmetries\u2019 than what are actually observable in our data (i.e. $\\pi$ in itself is only $H$ -invariant, but we symmetrize with respect to $G\\geq H$ ); we can either win or lose generalization power according to the interplay between the two presented terms. In particular, if $\\pi$ is $G$ equivariant, there\u2019s a strict generalization benefit from choosing a symmetric model to tackle our learning problem (which gives the name to the paper [27]). In particular, whenever $f_{G}^{\\perp}$ is non-zero (on a strictly positive $\\pi|_{\\mathcal{X}}$ -measure set) there\u2019s a strict gain in generalization power from using the symmetrized version of the model. ", "page_idx": 58}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 59}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 59}, {"type": "text", "text": "Justification: Within the limited space we could consecrate to those sections, both the abstract and the introduction give quite accurate descriptions of the context of the work (Mean-Field approach to overparametrized NN), the questions and problems addressed (role of symmetries and impact of symmetry-leveraging techniques, as seen from the MeanField limit), our setting (the type of neural network models considered), the specific results obtained (as precisely as it was possible in words, namely the precise description of the Wasserstein gradient flows describing the limiting training dynamics in different settings), the novelty of the paper (what can be learned from this viewpoint), and the contents of our numerical experiments. The main results are clearly understandable from the abstract, and the introduction gives a rather general description of the paper\u2019s goals, since it also leaves room for the context description and the outline of the paper. See Section 1 for details. ", "page_idx": 59}, {"type": "text", "text": "Guidelines: ", "page_idx": 59}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 59}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 59}, {"type": "text", "text": "Justification: We acknowledge and discuss many of the key assumptions required for both our theoretical and experimental results. The need for these assumption limits the applicability and practical impact of our work. We explicitly provide relevant insights in this regard, in a dedicated section, SuppMat-A.2. Also, and without including every single instance, we also discuss our limitations in: assumption 1, after presenting equation (1), section 4, and in many remarks in the SuppMat (e.g. in E.2.5,E.3 or F). ", "page_idx": 59}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. ", "page_idx": 59}, {"type": "text", "text": "\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. \u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 60}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 60}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 60}, {"type": "text", "text": "Justification: All mathematical statements presented are given complete, rigorous mathematical proofs in the Supplementary Material (mainly in SuppMat-E). We also provide complete sets of assumptions for each result, either in the statement itself or (for some results where the precise assumptions are very technical and lengthy to state) previous to the corresponding proof in the Supplementary Material. Even proofs of some elementary facts or statements are provided for completeness. Furthermore, nearly every mathematical result stated in the main paper is given a comment or discussion regarding its context, its interpretation (beyond the pure mathematical value) and/or an idea of what tools or arguments its proof will rely upon. ", "page_idx": 60}, {"type": "text", "text": "Guidelines: ", "page_idx": 60}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 60}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 60}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Justification: Together with our submission, we include a ZIP file with the necessary code (and instructions, following the guidelines provided by the conference) for replicating all of our experimental results. We also provide the necessary details to understand and reproduce our work both in Section 4 and (more extensively) in SuppMat-F. ", "page_idx": 60}, {"type": "text", "text": "Guidelines: ", "page_idx": 60}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often ", "page_idx": 60}, {"type": "text", "text": "one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. ", "page_idx": 61}, {"type": "text", "text": "\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 61}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 61}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 61}, {"type": "text", "text": "Justification: As mentioned in our previous answer, we include a ZIP flie with the necessary code and instructions to faithfully reproduce the main experimental results. We closely follow the NeurIPS guidelines for our code submission, providing the exact commands and environments required to run our experiments. Also both Section 4 and SuppMat-F provide a thorough landscape of the necessary assumptions and parameters to consider when replicating our results. ", "page_idx": 61}, {"type": "text", "text": "Guidelines: ", "page_idx": 61}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 61}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 61}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 62}, {"type": "text", "text": "Justification: In Section 4 we provide the necessary level of detail for understanding our main experimental results. We provide a thorough and detailed description of the exact parameters (and how they were chosen) employed in our different experiments in SuppMat-F. ", "page_idx": 62}, {"type": "text", "text": "Guidelines: ", "page_idx": 62}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 62}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 62}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 62}, {"type": "text", "text": "Justification: The numerical experiments considered in this work, involved the application of a noisy SGD dynamic (as in equation (1), or equation (5)) that relied on various levels of randomness: the initial parameter configurations, the simulated training data and the gaussian noise in SGD iterations. These factors are taken into account (see SuppMat-F) and thus the different experiments were repeated a total of 10 times (under different random seeds), which was largely enough to include interpretable error bars in our plots, and to correctly illustrate the true trends of the underlying phenomena (see, again, SuppMat-F). ", "page_idx": 62}, {"type": "text", "text": "Guidelines: ", "page_idx": 62}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 62}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 62}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Justification: In SuppMat-F we indicate both the computer resources employed to run the different experiments (see the first paragraph) as well as an estimate of the total computer time for the most complex simulation considered (namely, the $N=5000$ case). We did not make related experiments prior to the preparation of this submission. ", "page_idx": 62}, {"type": "text", "text": "Guidelines: ", "page_idx": 62}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 63}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 63}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Justification: This paper rigorously conforms to the NeurIPS Code of Ethics. By its mathematical/theoretical nature, this paper did not require the interaction with humans (besides the authors), and did not expose any living being to any type of harm. Also, no direct or indirect social impact is expected from this work in the short-to-mid term. Since our numerical results come from simulations, there are no major concerns to be had related to the nature of the employed dataset (in terms of privacy, consent, fair use or representative evaluation). Also, Intellectual property in all its forms was thoroughly respected. Finally, all the elements required for reproducibility of our results are disclosed in the material joint to the submission, including software employed and the data-simulation mechanisms. ", "page_idx": 63}, {"type": "text", "text": "Guidelines: ", "page_idx": 63}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 63}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 63}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Justification: The research object of this paper is mathematical, as it studies abstract mathematical models of artificial neural networks, and their properties and behaviors in specific, idealized situations and contexts. Therefore, no direct or indirect social impact is expected in the short-to-mid term. It is, however, expected that the theoretical results presented could have a positive impact in the long term, at the levels of development or practical use of safer, more transparent or interpretable machine learning algorithms; or at least in enlarging the corpus of conceptual tools available to researchers and practitioners to better understand the advantages, drawbacks, and potential risks or impacts of their professional activities. Though it might be useful eventually, our work is mostly theoretical and its impact remains limited to academia and pure ML practice. By all these reasons, no potential, positive nor negative, societal impacts of the work performed are addressed in the paper. ", "page_idx": 63}, {"type": "text", "text": "Guidelines: ", "page_idx": 63}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate ", "page_idx": 63}, {"type": "text", "text": "to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 64}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 64}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 64}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 64}, {"type": "text", "text": "Justification: As previously mentioned, this work is mainly theoretical, and thus poses no major threats that could require the implementation of such safeguards. ", "page_idx": 64}, {"type": "text", "text": "Guidelines: ", "page_idx": 64}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 64}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 64}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 64}, {"type": "text", "text": "Justification: We employ mostly public ML libraries and packages (e.g. objax, pyot, among others). We do, however, also involve the use of the emlp package provided by [29]. These assets are all properly credited in our work (see SuppMat-F), and the corresponding use licenses (particularly for the use of emlp) are properly mentioned and respected. ", "page_idx": 64}, {"type": "text", "text": "Guidelines: ", "page_idx": 64}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 64}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 65}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 65}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 65}, {"type": "text", "text": "Justification: Beyond the provided code to reproduce the experiments presented in the paper (which is accompanied with the details and instructions needed to execute it), no assets are introduced in this publication, hence, no specific documentation needs to be provided in that regard. ", "page_idx": 65}, {"type": "text", "text": "Guidelines: ", "page_idx": 65}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 65}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 65}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 65}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 65}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 65}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 65}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 65}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 65}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 65}]