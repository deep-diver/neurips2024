[{"figure_path": "VMsHnv8cVs/tables/tables_6_1.jpg", "caption": "Table 1: Performance of all attention variants on unsat SR(U(10, 40)) test problems.", "description": "This table presents the performance comparison of three different attention mechanisms (Casc-Attn, Full-Attn, Anch-Attn) used in the NeuRes model for solving unsatisfiable propositional logic problems. The performance is evaluated using two embedding strategies: static and dynamic.  The \"Proven (%)' column indicates the percentage of problems solved correctly, while the 'P-LEN' column shows the average length of the generated resolution proofs relative to the teacher's proofs. The results show that the dynamic embedding strategy significantly improves the performance of all three attention mechanisms.", "section": "6.1 Attention Variants"}, {"figure_path": "VMsHnv8cVs/tables/tables_7_1.jpg", "caption": "Table 2: Bootstrapped training data reduction statistics. Reduction statistics are computed on the SR(U(10, 40)) training set while p-Len and success rate are computed on a test set of the same distribution.", "description": "This table presents the results of applying a bootstrapped training approach to reduce the size of resolution proofs generated by the model.  The table shows various statistics, including the reduction depth (the number of times proofs were iteratively shortened), the percentage of proofs reduced, and the average reduction in proof length.  The p-Len metric shows the average length of the model-generated proofs relative to the teacher proofs. The success rate indicates the percentage of problems solved within the time limit. All these statistics are reported for both training and test sets of Boolean formulas. This demonstrates the impact of iterative proof shortening during training on the model's performance on unseen test problems.", "section": "6.2 Shortening Teacher Proofs with Bootstrapping"}, {"figure_path": "VMsHnv8cVs/tables/tables_8_1.jpg", "caption": "Table 3: Performance of full solver mode tested on SR(40) problems and trained on SR(U(10, 40)) problems where PREDICTED refers to the satisfiability prediction without certificate.", "description": "This table compares the performance of NeuRes and NeuroSAT on SR(40) problems.  NeuRes was trained on SR(U(10, 40)) data.  The table shows the percentage of problems solved (PROVEN) and the accuracy of satisfiability prediction (PREDICTED) for both SAT and UNSAT instances.  The \"PROVEN\" column indicates the success rate for finding satisfying assignments or resolution proofs, while the \"PREDICTED\" column demonstrates the model's accuracy when directly predicting satisfiability without generating a certificate.", "section": "7 Resolution-Aided SAT Solving"}, {"figure_path": "VMsHnv8cVs/tables/tables_8_2.jpg", "caption": "Table 4: Performance of different model fan-outs on SR(40) test data. Proof length (p-Len) and #Model Calls are both normalized by the length of the teacher proof.", "description": "This table presents the results of an experiment evaluating the impact of different model fan-out strategies on the performance of the Full-Attention model.  The experiment was conducted on SR(40) test problems. The table shows the average proof length (p-Len), the number of model calls, and the total percentage of problems successfully solved for three different fan-out settings: TOP-1 (selecting only the top-scoring resolution step), TOP-3 (selecting the top three), and TOP-5 (selecting the top five). The results demonstrate that increasing the fan-out can lead to shorter proofs and higher success rates but also increases model calls.", "section": "6 Generating Resolution Proofs"}, {"figure_path": "VMsHnv8cVs/tables/tables_15_1.jpg", "caption": "Table 5: Teacher proof reduction statistics of non-bootstrapped model trained on unreduced SR(U(10, 40)) dataset. Note that all rows, except for Total Reduction, are computed over the reduced portion of the dataset, i.e., the proofs that were successfully shortened by NeuRes.", "description": "This table presents the results of a non-bootstrapped model's ability to shorten teacher proofs.  It shows the percentage of proofs shortened, the maximum and average percentage reduction achieved, and the overall reduction in the total number of proof steps. The data is separated into training and testing sets, highlighting the model's generalization capabilities.", "section": "6.2 Shortening Teacher Proofs with Bootstrapping"}, {"figure_path": "VMsHnv8cVs/tables/tables_15_2.jpg", "caption": "Table 6: Average time (ms) to solve an instance by neural model vs. teacher solver.", "description": "This table compares the average time in milliseconds taken by three different methods to solve a single instance of a problem.  The methods compared are: the top-1 Full-Attention model (selecting the single best resolution step), the top-3 Full-Attention model (selecting the top three resolution steps), and Booleforce, a traditional SAT solver. The time is broken down for satisfiable (SAT) and unsatisfiable (UNSAT) instances and the total time across all instance types is reported.", "section": "6 Generating Resolution Proofs"}]