[{"figure_path": "t4VwoIYBf0/tables/tables_7_1.jpg", "caption": "Table 1: Mean and standard deviation for returns of various IL algorithms and environments", "description": "This table presents a comparison of the mean and standard deviation of returns achieved by different imitation learning (IL) algorithms across five MuJoCo environments.  The algorithms compared include: Random (a baseline representing random actions), Expert (the performance of the expert policy being imitated), Controlled GAIL, GAIL, Behavior Cloning (BC), Advantage Weighted Regression (AIRL), and Dataset Aggregation (DAgger).  The table provides a quantitative assessment of the relative performance of these algorithms in learning to mimic expert behavior across different control tasks.", "section": "6 Evaluation"}, {"figure_path": "t4VwoIYBf0/tables/tables_17_1.jpg", "caption": "Table 2: Returns for each environment on various GAIL algorithms.", "description": "This table presents a comparison of the performance of different imitation learning algorithms, including BC+GAIL, WAIL, GAIL-DAC, and the proposed method ('Ours'), across five different MuJoCo environments.  The results are the average returns achieved by each algorithm, with standard deviations, for each of the five environments. The number of expert trajectories used for training is also specified for each algorithm.", "section": "6.2 Results"}, {"figure_path": "t4VwoIYBf0/tables/tables_17_2.jpg", "caption": "Table 3: Number of Iterations needed to reach 95% of return", "description": "This table shows the number of iterations (in millions) required by different imitation learning methods (BC+GAIL, WAIL, GAIL-DAC, and the proposed C-GAIL) to achieve 95% of the maximum return for five MuJoCo environments (Half-Cheetah, Hopper, Reacher, Ant, and Walker2d).  The number of expert trajectories used for training is also specified for each method.  The results highlight the efficiency of the proposed C-GAIL in terms of the number of iterations needed to reach near-optimal performance.", "section": "6 Evaluation"}, {"figure_path": "t4VwoIYBf0/tables/tables_17_3.jpg", "caption": "Table 4: Final reward with 1 trajectory in diffusion-based GAIL [2], both vanilla and controlled variant. Mean and standard deviation over five runs", "description": "This table presents the final reward achieved by three different methods: the expert policy, the vanilla DiffAIL (diffusion-based adversarial imitation learning), and the proposed C-DiffAIL (controlled DiffAIL) method.  The results are shown for four different MuJoCo environments (Hopper, HalfCheetah, Ant, and Walker2d). For each environment and method, the mean final reward and its standard deviation across five independent runs are reported. This allows for a comparison of the performance of the proposed C-DiffAIL against the expert and vanilla DiffAIL.", "section": "6 Evaluation"}, {"figure_path": "t4VwoIYBf0/tables/tables_18_1.jpg", "caption": "Table 5: Final reward in five Atari tasks. Mean and standard deviation over ten runs", "description": "This table presents the final reward achieved by three different methods (Expert PPO, GAIL, and C-GAIL) across five Atari games.  For each game, the table shows the mean and standard deviation of the final reward obtained over ten runs of each method. The Expert PPO represents the performance of a well-trained Proximal Policy Optimization (PPO) agent, serving as a benchmark for the imitation learning methods (GAIL and C-GAIL). The results demonstrate the effect of the proposed C-GAIL method on improving the performance of GAIL in Atari games, which are complex environments with high dimensionality.", "section": "G Comparison in Atari tasks"}]