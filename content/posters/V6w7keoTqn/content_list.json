[{"type": "text", "text": "EMVP: Embracing Visual Foundation Model for Visual Place Recognition with Centroid-Free Probing ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Qibo Qiu1,2,\u2217 Shun Zhang3,\u2217 Haiming Gao3 Honghui Yang1 Haochao $\\mathbf{Ying^{4,\\dag}}$ Wenxiao Wang5 Xiaofei He1 ", "page_idx": 0}, {"type": "text", "text": "1 State Key Lab of CAD&CG, Zhejiang University 2 China Mobile (Zhejiang) Research & Innovation Institute, 3Zhejiang Lab 4School of Public Health and Second Affiliated Hospital, Zhejiang University School of Medicine 5School of Software Technology, Zhejiang University qiuqibo_zju@zju.edu.cn, haochaoying@zju.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Visual Place Recognition (VPR) is essential for mobile robots as it enables them to retrieve images from a database closest to their current location. The progress of Visual Foundation Models (VFMs) has significantly advanced VPR by capturing representative descriptors in images. However, existing fine-tuning efforts for VFMs often overlook the crucial role of probing in effectively adapting these descriptors for improved image representation. In this paper, we propose the Centroid-Free Probing (CFP) stage, making novel use of second-order features for more effective use of descriptors from VFMs. Moreover, to control the preservation of task-specific information adaptively based on the context of the VPR, we introduce the Dynamic Power Normalization (DPN) module in both the recalibration and CFP stages, forming a novel Parameter Efficiency Fine-Tuning (PEFT) pipeline (EMVP) tailored for the VPR task. Extensive experiments demonstrate the superiority of the proposed CFP over existing probing methods. Moreover, the EMVP pipeline can further enhance fine-tuning performance in terms of accuracy and efficiency. Specifically, it achieves $93.9\\%$ , $96.5\\%$ , and $94.6\\%$ Recall $@1$ on the MSLS Validation, Pitts250k-test, and SPED datasets, respectively, while saving $64.3\\%$ of trainable parameters compared with the existing SOTA PEFT method. The code is available at https://github.com/vincentqqb/EMVP. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Visual Place Recognition (VPR) is indispensable for mobile robots and autonomous vehicles, enabling key functions such as global localization [1], Simultaneous Localization and Mapping (SLAM) [2], and scene understanding [3]. VPR is often tackled as an image retrieval problem, where the objective is to match the query (an image representing the current location) with images from previously visited places. To achieve efficient matching, typically based on distance metrics like Euclidean distance, the VPR system aggregates the local descriptors of each image into a global descriptor. However, VPR faces unique challenges compared to conventional image retrieval tasks, including drastic changes in image perspectives, seasonal variations, and occlusions. Consequently, many researchers are dedicated to exploring robust local descriptors that exhibit invariance to these challenges. The advent of deep learning significantly brings VPR to a new stage characterized by enhanced robustness and improved accuracy [4; 5; 6; 7; 8; 9; 10; 11]. ", "page_idx": 0}, {"type": "text", "text": "Despite the carefully designed pipelines, these methods typically involve training a model from scratch on environment-specific data. However, the diversity of application environments making it challenging to collect sufficient data across different settings. To address this, a few recent studies [12; 13] have explored the potential of the Visual Foundation Model (VFM). These studies mainly focus on designing effective adapters within the backbone while only employing existing aggregation (probing) methods. However, specific probing techniques for more effective fine-tuning in the VPR task remain largely untapped. Typically, the most popular probing techniques mainly exploit first-order statistics of features, including Linear Probing (LP) for classification and GeneralizedMean (GeM) pooling for VPR, as shown in Figure 1 (a) and (b). However, second-order statistics have been well-proven important for fine-grained classification tasks [14], but LP and GeM are unable to capture them explicitly. ", "page_idx": 0}, {"type": "image", "img_path": "V6w7keoTqn/tmp/4db6dcd2213358177931a10a7a59036e951a2d6c40b3bdf652a0ce94c397c92b.jpg", "img_caption": ["Figure 1: Comparison of different probing methods. (a) The most popular Linear Probing (LP) in classification fine-tuning. (b) Generalized-Mean (GeM) pooling adapted by SelaVPR [13], which can be seen as a generalized form of first-order feature. (c) The NetVLAD operation simplified by SALAD [12]. (d) The proposed Centroid-Free Probing (CFP) which provides a theoretical and empirical justification for this simplification, fixing interpretability and performance issues that were present otherwise. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To this end, we revisit the classic method NetVLAD [4] as bilinear pooling (i.e., the most basic secondorder statistics), which aggregates local descriptors into a global one for fine-grained classification. However, NetVLAD requires a costly offline initialization of the semantic centroids, limiting its flexibility for fine-tuning on different datasets. In addition, inaccurate centroids can introduce inductive bias (for instance, initializing centroids in urban scenes but training or inferring in rural scenes), and affect the model\u2019s generalization ability. On the other hand, semantic centroids serve as priors for probing (aggregation), and simply removing them, as shown in Figure 1 (c), can lead to a decline in aggregation performance (detailed in Section 3.1). Delightfully, we observe that the explicit calculation of semantic centroids can be avoided when introducing a simple and effective Constant Normalization (CN) (detailed in Section 3.2). On this basis, a novel Centroid-Free Probing (CFP) stage is naturally introduced, which takes efforts of the second-order statistics of features. ", "page_idx": 1}, {"type": "text", "text": "Furthermore, due to the fact that VPR heavily relies on small overlapping regions between different images to make judgments when dealing with changes in image perspectives, preserving information from these discriminative regions is crucial. We design the Dynamic Power Normalization (DPN) module to adaptively control the preservation of task-specific information during the CFP stage, referred to as $\\mathrm{DPN_{C}}$ . Moreover, due to the different training objectives, the general representation capability of pre-trained VFMs tends to focus more on foreground objects. However, the VPR task relies more on background regions such as the salient building. To address this, we insert DPN modules into the backbone, termed $\\mathrm{DPN}_{\\mathrm{R}}$ , to enhance the preservation of information from these key background regions. With the backbone frozen, trainable $\\mathrm{DPN}_{\\mathrm{R}}$ modules adaptively control the preservation of task-specific background information in intermediate features, effectively contributing to Parameter Efficiency Fine-Tuning (PEFT). In the remainder of this paper, it is termed the recalibration stage. Thus, we propose a novel PEFT pipeline named EMVP by combining both recalibration and CFP stages. The main contributions of this paper can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Extensive experiments demonstrate that the proposed EMVP pipeline significantly contributes to the more accurate VPR. Plenty of ablation studies have verified the effectiveness of indispensable components (i.e., CFP, CN, $\\mathrm{DPN_{C}}$ , and $\\mathrm{DPN}_{\\mathrm{R}}$ ). ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Visual Place Recognition (VPR) is typically studied as an image retrieval problem. Based on research topics, advanced VPR approaches can be categorized into improvements on the feature extraction of the backbone network [10], aggregation methods for local descriptors [4; 15; 16], the design of metric loss functions [17; 7], investigations into robustness against viewpoint changes [6; 18; 19], and others. This section discusses researches closely related to the proposed method. ", "page_idx": 2}, {"type": "text", "text": "2.1 Aggregation in Visual Place Recognition ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Traditional VPR methods typically employ bag-of-visual-words [20; 21; 22], Vector of Locally Aggregated Descriptors (VLAD) [23; 24; 25] or Fisher Vectors (FV) [26; 27; 28] to aggregate local features such as SIFT [29] and SURF [30] into global ones. However, traditional methods for hand-crafted feature extraction are not data-driven. With the increase in data volume, these methods suffer from insufficient generalization and robustness. To address this problem, Arandjelovic et al. [4] proposed an end-to-end trainable generalized VLAD layer, NetVLAD, which greatly promotes the aggregation of features extracted from deep learning methods. Therefore, with the development of deep learning, NetVLAD becomes the most popular aggregation method for the VPR task [6; 8; 5]. Alternative techniques to NetVLAD include average/max pooling, R-MAC [31], and Generalized Mean (GeM) [15]. Although these methods exhibit promising effectiveness in retrieving images, they regularly demonstrate inferior performance compared to NetVLAD in the VPR task [16; 32]. Benefiting from the emergence of the Visual Foundation Model (VFM) and embodied AI, mobile robots have progressed greatly in visual tasks. For instance, the latest research [33] shows that the combination of a self-supervised pre-trained ViT model (i.e., DINOv2) and the unsupervised aggregation method VLAD exhibits robust zero-shot VPR performance. This motivates us to explore the cooperation between supervised NetVLAD and VFM for better accuracy. ", "page_idx": 2}, {"type": "text", "text": "In this paper, we avoid the costly explicit calculation of semantic centroids required by NetVLAD, by introducing a simple and effective Constant Normalization (CN) (detailed in Section 3.2). On this basis, a novel Centroid-Free Probing (CFP) stage including the DPNC module is proposed to employ second-order features when fine-tuning a VFM for better VPR performance. ", "page_idx": 2}, {"type": "text", "text": "Differences between CFP and SALAD [12]. SALAD also avoids explicit calculation of the semantic centroids when full fine-tuning a VFM, directly aggregating local descriptors with a summation. More impressively, we show that CFP admits a theoretical and empirical justification for this simplification, fixing interpretability and performance issues that were present otherwise. Furthermore, a novel PEFT pipeline (i.e., EMVP) tailored for the VPR task is proposed, innovatively employing the same DPN module in both recalibration and CFP stages for task-specific information preservation. Therefore, superior performance can be achieved with minimized trainable parameters. ", "page_idx": 2}, {"type": "text", "text": "2.2 Fine-tuning of Visual Foundation Model ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Inspired by the remarkable language generation capabilities and interactivity demonstrated by the GPT series [34], pre-trained on expansive text corpora, subsequent researches in VFMs has flourished. VFMs, including SAM [35], DINOv2 [36], and the multi-modal CLIP [37], exhibit notable visual generalizability and robustness in the realm of 2D image recognition. Accordingly, there are also plenty of researches that focus on fine-tuning these VFMs in diverse downstream tasks, including adapters [38], prompt tuning [39], Low-Rank Adaptation (LoRA) [40], etc. Nonetheless, the mainstream efforts of PEFT primarily aim to improve parameter efficiency and mitigate overfitting, neglecting to learn a task-dependent classification head. As a result, efforts of tuning a linear classifier rely solely on the first-order features, i.e., LP, suffering from inferior performance. To address this, Gao et al. [41] propose a novel Moment Probing (MP) method, which firstly leverages the second-order features that are rich in statistical information for PEFT. The second-order features used in MP are the covariance representation of the first-order features extracted by a single branch. In contrast, the second-order covariance matrix used in the proposed CFP stage comes from first-order features extracted by two different branches. Note that both CFP and MP make use of second-order features, but they have different theoretical bases and excel in different tasks. ", "page_idx": 2}, {"type": "image", "img_path": "V6w7keoTqn/tmp/cfa8c7b085761a0477dc8d8ec4d9551b117bb37af5d9b4e294bda25a6c35a9cc.jpg", "img_caption": ["Figure 2: Overall pipeline of the proposed EMVP, including recalibration and CFP stages. Feature matrices from the two branches (i.e., $\\mathcal{F}_{C}$ and ${\\mathcal{F}}_{P}$ ) are multiplied to obtain fine-grained features for the improved VPR performance. The Dynamic Power Normalization (DPN) layer can be inserted into both the recalibration and CFP stages to enhance the task-specific fine-tuning performance. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The overall EMVP pipeline is illustrated in Figure 2, which can be further divided into the recalibration and Centroid-Free Probing (CFP) stages. The idea of CFP originates from NetVLAD and facilitates a more effective adaptation of VFMs to VPR tasks, with details elaborated in Section 3.1 and 3.2. Additionally, we propose the Dynamic Power Normalization (DPN) module in Section 3.3, and incorporate it into both the recalibration and CFP stages to enhance fine-tuning performance, thereby making the features extracted by the backbone network more task-specific. ", "page_idx": 3}, {"type": "text", "text": "3.1 Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Global aggregation is crucial for robust and accurate visual place recognition, which aggregates local descriptors into a fixed-size global one. NetVLAD [4] is one of the most popular global aggregations based on the bag-of-visual-words [20] theory. Specifically, NetVLAD produces a statistic embedding for each semantic centroid, which represents the sum of distances from the semantic centroid to the assigned local descriptors as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\nV_{k}=\\sum_{i=1}^{L}p_{i k}(X_{i}-C_{k}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $C_{k}$ and $V_{k}$ represent the embedding and the statistic embedding of the $k$ -th semantic centroid, respectively. $L$ denotes the number of local descriptors in an image, and $X_{i}$ represents the $i$ -th local descriptor. $K$ is the number of total semantic centroids. $p_{i k}$ indicates the probability that $X_{i}$ is assigned to $C_{k}$ , which can be predicted by linear layers. The calculation of global descriptor can be summarized as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{\\mathcal{G}}=N e t V L A D(\\mathcal{X},\\mathcal{C})=c a t(\\mathcal{V},d i m=0),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathcal{X}\\ =\\ \\{X_{1},X_{2},...\\,,X_{L}\\}\\ \\in\\ \\mathbb{R}^{L\\times M}$ represents all local descriptors from an image. $\\hat{\\mathcal G}$ is the global descriptor for visual place recognition, which is obtained by concatenating $\\mathcal{V}\\,=$ $\\{V_{1},V_{2},\\ldots,V_{K}\\}\\in\\mathbf{\\dot{\\mathbb{R}}^{K\\times M}}$ . ", "page_idx": 3}, {"type": "text", "text": "Note that the semantic centroids $\\mathcal{C}\\;=\\;\\{C_{1},C_{2},...\\,,C_{K}\\}\\;\\in\\;\\mathbb{R}^{K\\times M}$ are also trainable, and the initialization of $\\mathcal{C}$ depends on an offline clustering process: Initially, an off-the-shelf backbone model is used to extract local descriptors from each image, involving a costly iteration through a pre-collected image set. Subsequently, $k$ -means clustering is applied to these descriptors to obtain the semantic centroids $\\mathcal{C}$ . However, the offline clustering process overlooks an issue: both the pre-collected image set and off-the-shelf backbone model should be compatible with the training set and trained VPR model. In other words, the fine-tuning performance is sensitive to the initialization process [33]. Therefore, it motivates us to explore methods to avoid the explicitly computation of the semantic centroids and further stimulate the potential of VFM in the field of VPR. ", "page_idx": 3}, {"type": "text", "text": "3.2 Centroid-Free Probing ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "As discussed by [42], the NetVLAD operation can be written as a bilinear pooling model. Specifically, different from the typical centroid-wise calculation pipeline (i.e., Equation 1 and 2), it can be calculated by accumulating local-wise descriptors as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathcal{G}=N e t V L A D(\\mathcal{X},\\mathcal{C})}\\\\ {\\quad=\\displaystyle\\sum_{i=1}^{L}\\left([X_{i}-C_{1};X_{i}-C_{2};...;X_{i}-C_{K}]\\odot[\\underbrace{p_{i1,\\,p_{i1},\\,...p_{i1}}}_{D};...;p_{i K},p_{i K},...p_{i K}]\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $P_{i}\\,=\\,[p_{i1},p_{i2},...,p_{i K}]$ and $\\odot$ indicates element-wise product. Note that Equation 3 contains the semantic centroids $\\mathcal{C}$ , which indicates that the corresponding costly and unstable offline initialization discussed in Section 3.1 is still needed. ", "page_idx": 4}, {"type": "text", "text": "We observed that since semantic centroids $\\mathcal{C}$ are shared when extracting global descriptors for different images, if the value of $\\textstyle\\sum_{i=1}^{L}P_{i}$ can be guaranteed to be constant, then the term after the minus sign in Equation 4 can be treated as a negligible constant term. Thus, the explicit calculation of $\\mathcal{C}$ can be avoided during the training process. In this paper, we resort to post normalization methods (e.g., softmax and $\\ell_{2}$ normalization) to constrain the value of $\\textstyle\\sum_{i=1}^{L}P_{i}$ to be constant, which is referred as Constant Normalization (CN) hereafter. The above simplification is formulated as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{G}=\\sum_{i=1}^{L}X_{i}^{T}\\times P_{i}-[C_{1};C_{2};...;C_{K}]\\odot\\Big(\\sum_{i=1}^{L}P_{i}\\Big).e x p a n d(K,D)\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Therefore, we can represent the global descriptor as follows, by omitting the constant term: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{G}=\\sum_{i=1}^{L}X_{i}^{T}\\times P_{i}=\\mathcal{X}^{T}\\mathcal{P}\\approx\\mathcal{F}_{C}(\\mathcal{X})^{T}\\mathcal{F}_{P}(\\mathcal{X}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the probability matrix $\\mathcal{P}=\\left\\{P_{1},P_{2},\\dotsc,P_{L}\\right\\}\\in\\mathbb{R}^{L\\times K}$ represents the probability from $L$ local descriptors to $K$ semantic centroids. Note that, local descriptors $\\mathcal{X}$ are fed to linear layers ${\\mathcal{F}}_{C}$ and ${\\mathcal{F}}_{P}$ , which are designed for dimensionality reduction and computation of probability matrix $\\mathcal{P}$ , respectively. In the context of fine-tuning VFMs, we refer to this simplified aggregation operation as Centroid-Free Probing (CFP), as illustrated in Figure 2. It implicitly leverages the priors brought by centroids through a bilinear design, while avoiding explicit centroid initialization and training. ", "page_idx": 4}, {"type": "text", "text": "3.3 Dynamic Power Normalization in CFP and Recalibration ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "As VPR heavily relies on small overlapping regions to handle perspective changes, preserving information from these discriminative regions is essential. In this paper, we resort to post-processing methods to preserve task-specific information from these regions, thereby enhancing the robustness of second-order features. Extensive advanced studies [43; 14; 44] have verified that using the Matrix Power Normalization (MPN) method for feature post-processing after bilinear pooling can significantly improve downstream task performance. Typical MPN can be described as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{V}=\\|\\boldsymbol{s i g n}(\\mathcal{G})|\\mathcal{G}|^{\\alpha}\\|_{2}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $s i g n(\\mathcal{G})$ represents the sign of $\\mathcal{G}$ , $|g|$ indicates the absolute value of $\\mathcal{G},\\alpha$ is a scalar, and $\\lVert\\cdot\\rVert_{2}$ denotes the $\\ell_{2}$ normalization operation. MPN can effectively control the preservation of task-specific information during the training process by adjusting the value of $\\alpha$ . In particular, as $\\alpha\\longmapsto0$ , the normalized representation $\\boldsymbol{\\wp}$ tends toward becoming an all-ones matrix. As $\\alpha\\longmapsto1$ , information in $\\mathcal{G}$ will be gradually preserved [44]. In the classic MPN method, the value of $\\alpha$ is predefined, and all images share the same value of $\\alpha$ . It can even be deduced from the perspective of Maximum Likelihood Estimation (MLE) that the theoretically optimal value for $\\alpha$ is 0.5 [14]. ", "page_idx": 4}, {"type": "text", "text": "Considering potential significant distribution differences between fine-tuning and inference datasets, in this paper, we lean towards $\\alpha$ being learnable based on the context adaptively. Specifically, we design the Dynamic Power Normalization (DPN) module in the CFP stage to compute the value of $\\alpha$ based on the value of $\\mathcal{G}$ as shown in Figure 3(a). Thus, each image can have a different level of task-specific information preservation, enabling more flexible fine-tuning. ", "page_idx": 4}, {"type": "image", "img_path": "V6w7keoTqn/tmp/aa96bd14c81d8ee1ee7c91e1169113c263ab1eaf8252fedbf0e9037b6010a7f8.jpg", "img_caption": ["Figure 3: The DPN module can be placed in both CFP and recalibration stages, which is indicated by $\\mathrm{DPN_{C}}$ and $\\mathrm{DPN}_{\\mathrm{R}}$ , respectively. More importantly\u201e it can be inserted into the Transformer blocks sequentially and parallelly. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "With the same consideration, we attempt to use DPN to adaptively preserve task-specific information in recalibration for Parameter Efficiency Fine-Tuning (PEFT), while keeping the backbone network frozen. Given the unique nature of the VPR task: the background region in images (e.g., the building), which may be irrelevant in other tasks, serves as a crucial clue for place recognition. However, due to the difference in training objectives, a pre-trained VFM may overlook these background regions. Therefore, we employ the DPN to enhance the representation of distinctive background regions, and effectively leveraging the representation capabilities of a VFM while minimizing recalibration, as depicted in Figure 3 (b) and 3 (c). ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Experimental Settings ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Choice of Visual Foundation Model. To make effective use of large-scale unlabeled data, selfsupervised transformers are becoming increasingly popular for training a VFM. Two self-supervised learning paradigms have demonstrated superior performance for ViT pre-training: contrastive learningbased (e.g., DINO [45]), and masked image modeling-based (e.g., MAE [46]). There are also multi-modal foundation models for robust feature extraction (e.g., CLIP [37]). Recent advanced research [47] demonstrated that ViTs pre-trained by the contrastive learning paradigm can produce more universal path-wise representations. In this paper, the ViT model pre-trained by DINOv2, a follow-up of DINO, is adopted as the VFM, which has been verified to have good performance in fine-grained tasks (e.g., depth estimation [36] and VPR [33]). Specifically, three VPR models are fine-tuned based on ViT-S, ViT-B, and ViT-L, named EMVP-S, EMVP-B, and EMVP-L, respectively. ", "page_idx": 5}, {"type": "text", "text": "Implementation Details. Both the $\\mathcal{F}_{C}$ and ${\\mathcal{F}}_{P}$ branches are implemented by a two-layer MLP network. For a fair comparison with SALAD, the output dimensions of ${\\mathcal{F}}_{C}$ and ${\\mathcal{F}}_{P}$ are 128 and 64, respectively. We employ the softmax operation to normalize the output features of ${\\mathcal{F}}_{P}$ . We implement two versions of the $\\mathrm{DPN}_{\\mathrm{R}}$ module (i.e., sequential and parallel $\\mathrm{DPN}_{\\mathrm{R}}.$ ), as shown in Figure 3, for the recalibration of intermediate features. In the parallel version, the original feature is preserved through an independent branch, and updated context is aggregated via element-wise addition. In contrast, the sequential version is equivalent to adding a few extra layers to the backbone. For more implementation details, please refer to Appendix A.1. ", "page_idx": 5}, {"type": "text", "text": "Datasets. Many advanced efforts [32; 16; 12] have shown that models trained on the GSV-Cities [32] dataset exhibit strong generalization across various VPR datasets, such as MSLS Validation [48], Pittsburgh30k-test [49], Pittsburgh250k-test [49], Nordland [50; 51], and SPED [52]. Following the training and validation protocol of these studies, we fine-tune the VPR model on the GSV-Cities ", "page_idx": 5}, {"type": "text", "text": "Table 1: Comparison with state-of-the-art methods. \u266ddenotes models trained on the GSV-Cities dataset. Due to the high quality of annotations in GSV-Cities, results from models marked with \u266d generally outperform those from their corresponding papers. In contrast, results from models without are reported in their respective papers. ", "page_idx": 6}, {"type": "table", "img_path": "V6w7keoTqn/tmp/8ab98c1c15670e44fc18b4ba4d5b42871c6c1e5daaefd49aec7a7320ae5ebceb.jpg", "table_caption": ["(a) Comparison with single-stage methods. "], "table_footnote": ["(b) Comparison with two-stage methods, which include a re-ranking stage indicated by "], "page_idx": 6}, {"type": "table", "img_path": "V6w7keoTqn/tmp/d0a668bb0d7d0b66d7bbcee588a54df07ab015b882a74d38f87e1c95440c16e5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "dataset, which is pre-trained by the DINOv2 pipeline. Subsequently, Recall@K (i.e., $\\mathsf{R@1}$ , $\\mathbf{R}\\otimes5$ , and $\\mathbf{R}\\mathcal{\\@}10^{`}$ ) is evaluated across various VPR datasets as the performance metric. ", "page_idx": 6}, {"type": "text", "text": "Model Selection. We involve 30 epochs for fine-tuning models, and select the one with the highest $\\mathbf{R}\\@1$ on the Pittsburgh30k-val dataset for further evaluation on other test datasets. To fairly compare model performance, we repeat the aforementioned process 5 times and calculate the average metrics as the final test results. ", "page_idx": 6}, {"type": "text", "text": "4.2 Main Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We report the performance comparison with SOTA in Table 1 and analyze the results as follows. First, in typical VPR methods, a re-ranking stage is typically incorporated for retrieved images to enhance the final performance as post-processing. This is primarily due to the inherent noise in the training dataset, which is subject to changes in image perspectives, seasonal variations, occlusion, and other factors. We illustrate the comparison through the TransVPR algorithm with or without the re-ranking stage as an example. Second, supported by the high-quality GSV-Cities dataset, methods (e.g., MixVPR) without re-ranking achieve comparable performance to those with re-ranking stages. Third, with the further support of DINOv2, the algorithms without re-ranking achieve new levels of accuracy and robustness, as evidenced by the performance of EMVP, SelaVPR (global) and SALAD. Finally, EMVP-L obtains the best performance by leveraging the probing method tailored for the VPR task. Taking the typical MSLS Validation dataset as an example, EMVP-L even outperforms the full fine-tuning method SALAD by $1.7\\%$ at Recall $@1$ . Additional visualization in Figure 4 shows that the VPR model fine-tuned by EMVP-B successfully finds the closest match in challenging scenarios, including occlusion, illumination change, perspective change, and seasonal variation. For more comparisons with SOTA methods, please refer to Appendix A.1. ", "page_idx": 6}, {"type": "image", "img_path": "V6w7keoTqn/tmp/9e1eeae35e81b88512127d49a94c1f26cd34c865aaf0c93db3131b9321b812c1.jpg", "img_caption": ["Figure 4: Query (gray) and top 3 retrieved frames (green: successful, red: failed). Moreover, one of the true (blue) matches is displayed for comparison. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.3 Ablation Studies ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The Impact of Different Probings. We compared probing methods based on first-order and secondorder features. Among the first-order methods, the popular LP in classification and GeM pooling in VPR tasks are explored. For second-order feature methods, we dive into the pioneering second-order method MP in classification and the proposed CFP. Additionally, the baseline can be seen as a form of bilinear pooling with the centroids removed, and it is implemented by removing the optimal transport operation in the SALAD code base. ", "page_idx": 7}, {"type": "text", "text": "As shown in Table 2, the test results of LP revealed a significant decrease in performance compared to aggregation based on CFP. One possible reason for this could be the common trick used in LP-based methods, where average pooling is applied to local descriptors to obtain a fixed-size global one. This first-order statistical features extraction may lead to information loss. GeM pooling generalizes average and max pooling, as a result, its accuracy is still fundamentally limited by first-order features. ", "page_idx": 7}, {"type": "text", "text": "Despite that MP has achieved excellent accuracy metrics in classification, its performance in VPR tasks is inferior to CFP, as shown in Table 2. This is mainly due to CFP implicitly leveraging the priors provided by semantic centroids. By comparing results of baseline and NetVLAD (ID 2), we find that directly removing centroids using bilinear pooling leads to a performance drop. This is why SALAD employs optimal transport to improve the performance. In contrast, this paper introduce CN and $\\mathrm{DPN_{C}}$ to theoretically refine the simplification of NetVLAD, achieving improved performance. ", "page_idx": 7}, {"type": "text", "text": "It is noteworthy that advanced methods such as TransVPR and MixVPR, employing heavy Transformer and MLP-Mixer aggregation architectures, have demonstrated excellent performance in VPR tasks. However, these methods do not quite align with the current research paradigm of fine-tuning VFMs based on shallow trainable MLP architectures. ", "page_idx": 7}, {"type": "text", "text": "The Impact of the Reinterpretation with Constant Normalization. By comparing the experimental results of ID 2 and ID 10 in Table 2, we can verify that our reinterpretation for NetVLAD has led to improved performance. This is mainly attributed to the elimination of the explicit learning of cluster centers, which has reduced parameter number and mitigated the impact of imprecise initialization. It is worth noting that increasing the feature dimension of NetVLAD can significantly enhance performance. However, it is essential to consider the potential cost when dealing with the storage of images with sizable global descriptors, particularly in the context of VPR applications. ", "page_idx": 7}, {"type": "text", "text": "Comparisons between ID 7 and ID 8, ID 7 and ID 9 in Table 2 demonstrate that the CN makes this reinterpretation operation empirically more robust, and this can be validated by achieving improved performance. Through comparing ID 8 and ID 9 in Table 2, we can observe that the improvement brought by CN is dependent on its specific implementation. Further exploration of this aspect will be undertaken in our subsequent research, extending beyond the scope of this paper. ", "page_idx": 7}, {"type": "table", "img_path": "V6w7keoTqn/tmp/8bcc5b0c107b996b61f805bc74d624edf5111eef235e823394e8f5c7feabaa41.jpg", "table_caption": ["Table 2: Comparing different backbones and probings. LP, MP, CFP, CN, and $\\mathrm{DPN_{C}}$ indicate linear probing, moment probing, centroid-free probing, constant normalization, and dynamic power normalization in probing, respectively. For fairness, results produced by ViT-based models are obtained by fully fine-tuning the last 4 blocks. Baseline refers to the simplified NetVLAD adapted by SALAD. The best and the second best results are bolded and underlined, respectively. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "V6w7keoTqn/tmp/2a0cd393a04cef82ae02397bd697fe7e9f6501efda8e627ddea85f2f0049a256.jpg", "table_caption": ["Table 3: Comparing different fine-tuning methods. $\\mathrm{DPN_{C}}$ and $\\mathrm{DPN}_{\\mathrm{R}}$ indicate DPN in CFP and recalibration, respectively. Results of both parallel and sequential versions of $\\mathrm{DPN}_{\\mathrm{R}}$ are reported. For fairness, only the last 4 blocks can be fine-tuned, and all methods employ the same backbone, i.e., ViT-B. The best and the second best results are bolded and underlined, respectively. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "The Impact of the Foundation Model. The comparative results between ID 1 and ID 3 in Table 2 indicate that, when the size of the global descriptor is within the same order of magnitude, the VFM (i.e., DINOv2) is more suitable as the backbone network for VPR tasks compared to traditional CNN models. Even in the scenario of zero-shot inference, the VPR model based on DINOv2 demonstrates strong generalization and robustness (refer to AnyLoc in Table 3). However, it is important to note that the performance of zero-shot inference still exhibits a substantial performance gap in comparison to methods that are based on fine-tuning. This motivates us to explore a more effective fine-tuning pipeline tailored for VPR tasks. Additionally, the results in Table 10 of Appendix A.2 indicate that as the scale of ViTs increases, the performance of VPR models fine-tuned by EMVP can be improved. ", "page_idx": 8}, {"type": "text", "text": "Comparison of Fine-tuning Methods. In Table 3, different fine-tuning approaches are compared. By comparing SALAD and AnyLoc, we can conclude that current VFMs (i.e., DINOv2) lack sufficient zero-shot reasoning capabilities for diverse data in the VPR domain. SALAD achieves high performance by fully fine-tuning on DINOv2, but VPR models are typically deployed on mobile robots, and this full-parameter update approach imposes the higher demands on communication. Therefore, we attempt to study adaptation methods more suitable for VPR tasks. ", "page_idx": 8}, {"type": "text", "text": "For fairness, this paper reimplements the advanced PEFT method PSRP [41], which is also aimed at cooperating with second-order features, on the VPR dataset to compare it with our proposed $\\mathrm{DPN}_{\\mathrm{R}}$ . Furthermore, as illustrated in Figure 3, the $\\mathrm{DPN}_{\\mathrm{R}}$ module can be further divided into parallel and sequential versions for comparative research. By further comparing the sequential and parallel $\\mathrm{DPN}_{\\mathrm{R}}$ modules in Table 3, we find that the sequential version performs better. This is primarily because the sequential method recalibrates the backbone features more thoroughly, and it does not significantly increase training difficulty since a few additional parameters are introduced. The sequential configuration of the $\\mathrm{DPN}_{\\mathrm{R}}$ method is used by default in other experiments in this paper. Compared with methods such as SALAD and PSRP, the sequential $\\mathrm{DPN}_{\\mathrm{R}}$ outperforms them by achieving the best performance while saving $64.3\\%$ of trainable parameters (0.14M vs 0.05M). We further visualize the impact of $\\mathrm{DPN}_{\\mathrm{R}}$ on recalibration in Appendix A.3. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this paper, we have proposed a novel fine-tuning pipeline named EMVP, which involves reinterpreting the classical aggregation (i.e., NetVLAD) into a CFP stage when fine-tuning a VFM for accurate VPR. What is more innovative, both the recalibration and CFP stages employ the same ", "page_idx": 8}, {"type": "text", "text": "DPN module for task-specific information preservation, effectively conducting PEFT. Extensive experiments conducted on VPR datasets have demonstrated that EMVP can extract more task-specific features, resulting in enhanced accuracy and robustness in VPR performance. ", "page_idx": 9}, {"type": "text", "text": "Broader Impacts. This work enhances the safety and efficiency of mobile robots operating in GPSdenied environments. In addition, the potential negative impact is that bad weather and ambiguous scenes will cause certain interference, making it difficult to maintain a high level of accuracy in VPR. ", "page_idx": 9}, {"type": "text", "text": "Limitations. Based on the discussions and comparisons of VFMs such as CLIP, SAM, DINO, and DINOv2 in pioneering works [47; 33; 57], this paper prioritizes DINOv2 as the VFM for experimental analysis. Note that, different VFMs possess distinct capabilities. For example, CLIP can connect images with texts for better interpretability, while SAM demonstrates powerful abilities in handling visual prompts. In the future work, we aim to explore these capabilities in different VPR tasks. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research was partially supported by the Zhejiang Provincial Key R&D Program of China under Grant No. 2024C01166, the Fundamental Research Funds for the Central Universities under Grant No. 226-2024-00227, the National Natural Science Foundation of China under Grant No. 62303428, and the GuangZhou City\u2019s Key R&D Program of China under Grant No. 2024B01J1301. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Xuecheng Xu, Sha Lu, Jun Wu, Haojian Lu, Qiuguo Zhu, Yiyi Liao, Rong Xiong, and Yue Wang. Ring $^{++}$ : Roto-translation-invariant gram for global localization on a sparse scan map. IEEE Trans. Robot., 2023.   \n[2] Weinan Chen, Changfei Fu, Lei Zhu, Shing-Yan Loo, and Hong Zhang. Rumination meets vslam: You do not need to build all the submaps in realtime. IEEE Trans. Ind. Electron., 2023.   \n[3] Haoang Li, Ji Zhao, Jean-Charles Bazin, Pyojin Kim, Kyungdon Joo, Zhenjun Zhao, and Yun-Hui Liu. Hong kong world: Leveraging structural regularity for line-based slam. IEEE Trans. Pattern Anal. Mach. Intell., 2023.   \n[4] Relja Arandjelovic, Petr Gronat, Akihiko Torii, Tomas Pajdla, and Josef Sivic. Netvlad: Cnn architecture for weakly supervised place recognition. In IEEE Conf. Comput. Vis. Pattern Recog., pages 5297\u20135307, 2016.   \n[5] Stephen Hausler, Sourav Garg, Ming Xu, Michael Milford, and Tobias Fischer. Patch-netvlad: Multi-scale fusion of locally-global descriptors for place recognition. In IEEE Conf. Comput. Vis. Pattern Recog., pages 14141\u201314152, 2021. [6] Yixiao Ge, Haibo Wang, Feng Zhu, Rui Zhao, and Hongsheng Li. Self-supervising fine-grained region similarities for large-scale image localization. In Eur. Conf. Comput. Vis., pages 369\u2013386. Springer, 2020.   \n[7] Liu Liu, Hongdong Li, and Yuchao Dai. Stochastic attraction-repulsion embedding for large scale image localization. In Int. Conf. Comput. Vis., pages 2570\u20132579, 2019.   \n[8] Guohao Peng, Jun Zhang, Heshan Li, and Danwei Wang. Attentional pyramid pooling of salient visual residuals for place recognition. In Int. Conf. Comput. Vis., pages 885\u2013894, 2021.   \n[9] Hyo Jin Kim, Enrique Dunn, and Jan-Michael Frahm. Learned contextual feature reweighting for image geo-localization. In IEEE Conf. Comput. Vis. Pattern Recog., pages 2136\u20132145, 2017.   \n[10] Ruotong Wang, Yanqing Shen, Weiliang Zuo, Sanping Zhou, and Nanning Zheng. Transvpr: Transformer-based place recognition with multi-level attention aggregation. In IEEE Conf. Comput. Vis. Pattern Recog., pages 13648\u201313657, 2022.   \n[11] Sijie Zhu, Linjie Yang, Chen Chen, Mubarak Shah, Xiaohui Shen, and Heng Wang. R2former: Unified retrieval and reranking transformer for place recognition. In IEEE Conf. Comput. Vis. Pattern Recog., pages 19370\u201319380, 2023.   \n[12] Sergio Izquierdo and Javier Civera. Optimal transport aggregation for visual place recognition. 2024.   \n[13] Feng Lu, Lijun Zhang, Xiangyuan Lan, Shuting Dong, Yaowei Wang, and Chun Yuan. Towards seamless adaptation of pre-trained models for visual place recognition. In Int. Conf. Learn. Represent.   \n[14] Peihua Li, Jiangtao Xie, Qilong Wang, and Wangmeng Zuo. Is second-order information helpful for large-scale visual recognition? In Int. Conf. Comput. Vis., pages 2070\u20132078, 2017.   \n[15] Filip Radenovi\u00b4c, Giorgos Tolias, and Ond\u02c7rej Chum. Fine-tuning cnn image retrieval with no human annotation. IEEE Trans. Pattern Anal. Mach. Intell., 41(7):1655\u20131668, 2018.   \n[16] Amar Ali-Bey, Brahim Chaib-Draa, and Philippe Giguere. Mixvpr: Feature mixing for visual place recognition. In IEEE Winter Conf. Appl. Comput. Vis., pages 2998\u20133007, 2023.   \n[17] Gabriele Berton, Carlo Masone, and Barbara Caputo. Rethinking visual geo-localization for large-scale applications. In IEEE Conf. Comput. Vis. Pattern Recog., pages 4878\u20134888, 2022.   \n[18] Mar\u00eda Leyva-Vallina, Nicola Strisciuglio, and Nicolai Petkov. Data-efficient large scale place recognition with graded similarity supervision. In IEEE Conf. Comput. Vis. Pattern Recog., pages 23487\u201323496, 2023.   \n[19] Gabriele Berton, Gabriele Trivigno, Barbara Caputo, and Carlo Masone. Eigenplaces: Training viewpoint robust models for visual place recognition. In Int. Conf. Comput. Vis., pages 11080\u2013 11090, 2023.   \n[20] Gabriella Csurka, Christopher Dance, Lixin Fan, Jutta Willamowski, and C\u00e9dric Bray. Visual categorization with bags of keypoints. In Eur. Conf. Comput. Vis. Worksh., volume 1, pages 1\u20132. Prague, 2004.   \n[21] James Philbin, Ondrej Chum, Michael Isard, Josef Sivic, and Andrew Zisserman. Object retrieval with large vocabularies and fast spatial matching. In IEEE Conf. Comput. Vis. Pattern Recog., pages 1\u20138. IEEE, 2007.   \n[22] Sivic and Zisserman. Video google: A text retrieval approach to object matching in videos. In Int. Conf. Comput. Vis., pages 1470\u20131477. IEEE, 2003.   \n[23] Yunchao Gong, Liwei Wang, Ruiqi Guo, and Svetlana Lazebnik. Multi-scale orderless pooling of deep convolutional activation features. In Eur. Conf. Comput. Vis., pages 392\u2013407. Springer, 2014.   \n[24] Herv\u00e9 J\u00e9gou, Matthijs Douze, Cordelia Schmid, and Patrick P\u00e9rez. Aggregating local descriptors into a compact image representation. In 2010 IEEE computer society conference on computer vision and pattern recognition, pages 3304\u20133311. IEEE, 2010.   \n[25] Relja Arandjelovic and Andrew Zisserman. All about vlad. In IEEE Conf. Comput. Vis. Pattern Recog., pages 1578\u20131585, 2013.   \n[26] Tommi Jaakkola and David Haussler. Exploiting generative models in discriminative classifiers. Adv. Neural Inform. Process. Syst., 11, 1998.   \n[27] Florent Perronnin, Yan Liu, Jorge S\u00e1nchez, and Herv\u00e9 Poirier. Large-scale image retrieval with compressed fisher vectors. In 2010 IEEE computer society conference on computer vision and pattern recognition, pages 3384\u20133391. IEEE, 2010.   \n[28] Herv\u00e9 J\u00e9gou, Florent Perronnin, Matthijs Douze, Jorge S\u00e1nchez, Patrick P\u00e9rez, and Cordelia Schmid. Aggregating local image descriptors into compact codes. IEEE Trans. Pattern Anal. Mach. Intell., 34(9):1704\u20131716, 2011.   \n[29] David G Lowe. Distinctive image features from scale-invariant keypoints. Int. J. Comput. Vis., 60:91\u2013110, 2004.   \n[30] Herbert Bay, Andreas Ess, Tinne Tuytelaars, and Luc Van Gool. Speeded-up robust features (surf). Comput Vis Image Underst, 110(3):346\u2013359, 2008.   \n[31] Giorgos Tolias, Ronan Sicre, and Herv\u00e9 J\u00e9gou. Particular object retrieval with integral maxpooling of cnn activations. 2016.   \n[32] Amar Ali-bey, Brahim Chaib-draa, and Philippe Gigu\u00e8re. Gsv-cities: Toward appropriate supervised visual place recognition. Neurocomputing, 513:194\u2013203, 2022.   \n[33] Nikhil Keetha, Avneesh Mishra, Jay Karhade, Krishna Murthy Jatavallabhula, Sebastian Scherer, Madhava Krishna, and Sourav Garg. Anyloc: Towards universal visual place recognition. arXiv preprint arXiv:2308.00688, 2023.   \n[34] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \n[35] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023.   \n[36] Maxime Oquab, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023.   \n[37] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. pages 8748\u20138763. PMLR, 2021.   \n[38] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. pages 2790\u20132799. PMLR, 2019.   \n[39] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In Eur. Conf. Comput. Vis., pages 709\u2013727. Springer, 2022.   \n[40] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In Int. Conf. Learn. Represent., 2021.   \n[41] Mingze Gao, Qilong Wang, Zhenyi Lin, Pengfei Zhu, Qinghua Hu, and Jingbo Zhou. Tuning pre-trained model via moment probing. In Int. Conf. Comput. Vis., pages 11803\u201311813, 2023.   \n[42] Tsung-Yu Lin, Aruni RoyChowdhury, and Subhransu Maji. Bilinear cnn models for fine-grained visual recognition. In Int. Conf. Comput. Vis., pages 1449\u20131457, 2015.   \n[43] Tsung-Yu Lin and Subhransu Maji. Improved bilinear pooling with cnns. In Brit. Mach. Vis. Conf., 2017.   \n[44] Qilong Wang, Mingze Gao, Zhaolin Zhang, Jiangtao Xie, Peihua Li, and Qinghua Hu. Dropcov: a simple yet effective method for improving deep architectures. Adv. Neural Inform. Process. Syst., 35:33576\u201333588, 2022.   \n[45] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Int. Conf. Comput. Vis., pages 9650\u20139660, 2021.   \n[46] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. In IEEE Conf. Comput. Vis. Pattern Recog., pages 16000\u201316009, 2022.   \n[47] Ani Vanyan, Alvard Barseghyan, Hakob Tamazyan, Vahan Huroyan, Hrant Khachatrian, and Martin Danelljan. Analyzing local representations of self-supervised vision transformers. arXiv preprint arXiv:2401.00463, 2023.   \n[48] Frederik Warburg, Soren Hauberg, Manuel Lopez-Antequera, Pau Gargallo, Yubin Kuang, and Javier Civera. Mapillary street-level sequences: A dataset for lifelong place recognition. In IEEE Conf. Comput. Vis. Pattern Recog.   \n[49] Akihiko Torii, Josef Sivic, Tomas Pajdla, and Masatoshi Okutomi. Visual place recognition with repetitive structures. In IEEE Conf. Comput. Vis. Pattern Recog., pages 883\u2013890, 2013.   \n[50] Daniel Olid, Jos\u00e9 M F\u00e1cil, and Javier Civera. Single-view place recognition under seasonal changes. arXiv preprint arXiv:1808.06516, 2018.   \n[51] Mubariz Zaffar, Sourav Garg, Michael Milford, Julian Kooij, David Flynn, Klaus McDonaldMaier, and Shoaib Ehsan. Vpr-bench: An open-source visual place recognition evaluation framework with quantifiable viewpoint and appearance change. Int. J. Comput. Vis., 129(7):2136\u2013 2174, 2021.   \n[52] Zetao Chen, Lingqiao Liu, Inkyu Sa, Zongyuan Ge, and Margarita Chli. Learning context flexible attention model for long-term visual place recognition. IEEE Robot. Autom. Lett., 3(4):4015\u20134022, 2018.   \n[53] Jun Yu, Chaoyang Zhu, Jian Zhang, Qingming Huang, and Dacheng Tao. Spatial pyramidenhanced netvlad with weighted triplet loss for place recognition. IEEE Trans. Neural Networks Learn. Syst., 31(2):661\u2013674, 2019.   \n[54] Jian Zhang, Yunyin Cao, and Qun Wu. Vector of locally and adaptively aggregated descriptors for image feature representation. Pattern Recognition, 116:107952, 2021.   \n[55] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superglue: Learning feature matching with graph neural networks. In IEEE Conf. Comput. Vis. Pattern Recog., pages 4938\u20134947, 2020.   \n[56] Bingyi Cao, Andre Araujo, and Jack Sim. Unifying deep local and global features for image search. In Eur. Conf. Comput. Vis., pages 726\u2013743. Springer, 2020.   \n[57] Timoth\u00e9e Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers. Int. Conf. Learn. Represent., 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Technical Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Implementation Details and More Comparisons ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The implementation details are reported in Table 4. To facilitate a fair comparison, we try to keep the experiment settings as consistent as possible with the comparative methods. We further provide a detailed implementation of the Dynamic Power Normalization (DPN), as shown by Algorithm 1. The VPR model is fine-tuned on the GSV-Cities dataset, which contains 0.56 million images from 67 thousand different places. Table 5 describes the details of various testing datasets. All experiments are conducted on a NVIDIA RTX A6000 GPU using PyTorch. Moreover, fine-tuning a VPR model based on the ViT-B takes 7 minutes per epoch, and requires 21GB GPU memory. Table 6 provides a more comprehensive evaluation of single- and two-stage methods. ", "page_idx": 13}, {"type": "table", "img_path": "V6w7keoTqn/tmp/120f7770b5de88de9fec7dfcc78404a575c8d123cde51f5af41e2ebef643923e.jpg", "table_caption": ["Table 4: Experiment setting for fine-tuning. "], "table_footnote": [], "page_idx": 13}, {"type": "table", "img_path": "V6w7keoTqn/tmp/9747244f3859e657f131e5c7f0bcf7e029a7e66ce46731085587f4d498d282cc.jpg", "table_caption": ["Table 5: Datasets description. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "Algorithm 1 PyTorch Pseudo Code for DPN. ", "text_level": 1, "page_idx": 13}, {"type": "table", "img_path": "V6w7keoTqn/tmp/42704ff007098bc91a81fa1238c4c18532ec80d343b5f52238d77857bc9f3459.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "table", "img_path": "V6w7keoTqn/tmp/7ce85a6ce63dcb5b09763d00b9e20e8a8bbaec1bd6799fa071c64f8d219b436c.jpg", "table_caption": ["Table 6: More comparisons with state-of-the-art methods. The results marked with \u03b3 are reproduced based on the official code. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "A.2 More Ablation Studies ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Dimensions of ${\\mathcal{F}}_{C}$ and ${\\mathcal{F}}_{P}$ . Table 7 and 8 indicate that the output sizes of ${\\mathcal{F}}_{C}$ and ${\\mathcal{F}}_{P}$ have some influence on performance, while the performance is not particularly sensitive to these two hyperparameters. When the output size of $\\mathcal{F}_{C}$ $(D)$ ranges from 128 to 512 and the output size of ${\\mathcal{F}}_{P}$ $(K)$ ranges from 64 to 256, the models all achieve good performance. ", "page_idx": 14}, {"type": "text", "text": "Numbers of Recalibrated Blocks. Table 9 shows that the number of blocks inserted DPN modules significantly affects the results, and the best results are achieved when the features in the last 4 blocks are recalibrated. However, the selection of recalibrated blocks is beyond the scope of this paper, and we will further explore it in future work. ", "page_idx": 14}, {"type": "text", "text": "The Scale of ViT Model. Table 10 displays the results of different ViT models at size S, B, and L. As the scale of ViT models increases, the total number of parameters grows exponentially, making training with full fine-tuning methods extremely challenging. For example, if the ViT-B is replaced with the ViT-L in the full fine-tuning method SALAD, there would be a significant decrease in evaluation performance [12]. However, thanks to the introduction of the $\\mathrm{DPN}_{\\mathrm{R}}$ module, the number of trainable parameters in EMVP only slightly increases with the scale of the ViT, allowing for the full utilization of the enhanced representation capability brought by the larger model size. Due to the limitation in computational resources, ViT-G is not tested. ", "page_idx": 14}, {"type": "table", "img_path": "V6w7keoTqn/tmp/badb022fac545bd3f573de14aa599cc4f76d9af49665b2610f627db3f7d11b6b.jpg", "table_caption": ["Table 7: Impact of the output sizes of ${\\mathcal{F}}_{C}$ $(K=64)$ . "], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "V6w7keoTqn/tmp/093fe36301ec5100d33e91c3439f10c7c1be50044c2a7d4423d676a74a60a3b4.jpg", "table_caption": ["Table 8: Impact of the output sizes of ${\\mathcal{F}}_{P}$ $\\langle D=128\\rangle$ . "], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "V6w7keoTqn/tmp/5d0c700ccaa49448481bb51bc2425455a9bb21851b28f519a73aef5dc51ebbb1.jpg", "table_caption": ["Table 9: Impact of the number of recalibrated blocks. "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "V6w7keoTqn/tmp/7d760e42ac8ac898d8dd8f0284ebe393d8b31280d72d64e1ec6eb5e77d64c4e9.jpg", "table_caption": ["Table 10: Comparing different ViT models. Tr. and Ttl. represent the number of trainable and total parameters (M), respectively. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "A.3 Visualizations for $\\mathrm{DPN}_{\\mathrm{R}}$ ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The visualization results in Figure 5 indicate that after being trained by $\\mathrm{DPN}_{\\mathrm{R}}$ , the top $20\\%$ of high-norm tokens tend to appear more in distinctive regions. Figure 6 illustrates that after being fine-tuned through the EMVP pipeline, the VPR model can accurately capture texture details shared among different images. While Figure 7 shows that under changes in perspective, high-norm tokens tend to appear in distinctive background regions, which are typically the tallest building in a place. This is primarily attributed to the $\\mathrm{DPN}_{\\mathrm{R}}$ module adopted by EMVP, which enhances task-specific representations while maximally preserving the feature representation capability of the VFM. ", "page_idx": 15}, {"type": "image", "img_path": "V6w7keoTqn/tmp/e8daadb4a58abe18bf5df9e3187d2acc203eef5e872911ca4a01cd9ddb2250e8.jpg", "img_caption": ["Figure 5: The visualization of the top $20\\%$ high-norm tokens. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "V6w7keoTqn/tmp/124ee27fc067192aff0d8a80cfc964a261d8a3948f2ba0379bcc49b8e4b38374.jpg", "img_caption": ["Figure 6: High-norm tokens can contribute to capturing texture details. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "V6w7keoTqn/tmp/1d1a2dc2b35f63bd841a860825b2af7da751ed222bff91fd3ee1a58c05a20788.jpg", "img_caption": ["Figure 7: Visual place recognition under changes in perspectives. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: Refer to Section 1. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: Refer to Section 5. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 17}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: This paper does not report any theoretical result. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: Refer to Appendix A. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 18}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 18}, {"type": "text", "text": "Justification: Data and code can be found at: https://github.com/vincentqqb/EMVP Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Refer to Section 4.1 and Appendix A. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 19}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: As described in Section 4.1, the reported results are derived from the average of multiple runs. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Refer to Appendix A. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 20}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We confirm that the research in this paper conforms with the NeurIPS Code of Ethics. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 20}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Refer to Section 5. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: This paper poses no such risks. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 21}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: This paper employs open-source datasets, with clear citations provided for all datasets in Section 4.1, in compliance with the corresponding licenses. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 21}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 22}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 22}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 22}]