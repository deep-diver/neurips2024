[{"type": "text", "text": "Action Gaps and Advantages in Continuous-Time Distributional Reinforcement Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Harley Wiltzer\u2217 Marc G. Bellemare\u2020 David Meger Mila\u2013Qu\u00e9bec AI Institute Mila\u2013Qu\u00e9bec AI Institute McGill University McGill University McGill University ", "page_idx": 0}, {"type": "text", "text": "Patrick Shafto Rutgers University\u2013Newark ", "page_idx": 0}, {"type": "text", "text": "Yash Jhaveri\u2217 Rutgers University\u2013Newark ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "When decisions are made at high frequency, traditional reinforcement learning (RL) methods struggle to accurately estimate action values. In turn, their performance is inconsistent and often poor. Whether the performance of distributional RL (DRL) agents suffers similarly, however, is unknown. In this work, we establish that DRL agents are sensitive to the decision frequency. We prove that action-conditioned return distributions collapse to their underlying policy\u2019s return distribution as the decision frequency increases. We quantify the rate of collapse of these return distributions and exhibit that their statistics collapse at different rates. Moreover, we define distributional perspectives on action gaps and advantages. In particular, we introduce the superiority as a probabilistic generalization of the advantage\u2014 the core object of approaches to mitigating performance issues in high-frequency value-based RL. In addition, we build a superiority-based DRL algorithm. Through simulations in an option-trading domain, we validate that proper modeling of the superiority distribution produces improved controllers at high decision frequencies. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In many real-time deployments of reinforcement learning (RL)\u2014quantitative finance, robotics, and autonomous driving, for instance\u2014the state of the environment evolves continuously in time, but policies make decisions at discrete timesteps $h$ units of time apart) [28]. In such systems, the performance of value-based agents is sensitive to the frequency $\\omega:={^1}/h$ with which actions are taken. In particular, action values become indistinguishable as the time between actions decreases. In turn, in high-frequency settings, Baird demonstrated that action value estimates are susceptible to noise and approximation error [20]. Moreover, Tallec et al. exhibited that the performance of popular deep $Q$ -learning agents is inconsistent and often poor [34]. ", "page_idx": 0}, {"type": "text", "text": "In order to remedy this sensitivity, Baird proposed the advantage function and advantage-based variants of $Q$ -learning, Advantage Updating (AU) [20] and Advantage Learning (AL) [2]. Unlike action values, advantages (appropriately rescaled) do not become indistinguishable as decision frequency increases. As a result, Baird, in [20, 2], demonstrated that advantage-based agents can learn faster and be more resilient to noise than their action value-based counterparts. Furthermore, Tallec et al., in [34], exhibited that their extension of AU, Deep Advantage Updating (DAU), works efficiently over a wide range of timesteps and environments, unlike standard deep $Q$ -learning approaches. ", "page_idx": 0}, {"type": "text", "text": "While advantage-based approaches to RL have demonstrated robustness to decision frequency, in this work, we establish that they are nevertheless sensitive to the frequency with which actions are taken. This discovery arises as we answer the question: to what extent is the performance of distributional RL (DRL) agents sensitive to decision frequency? To this end, we build theory within the formalism of continuous-time RL where environmental dynamics are governed by SDEs, as in [25]. Additionally, we validate our theory empirically through simulations. Specifically, we make the following four contributions: ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Distributional Action Gap. First, we extend notions of action gap to the realm of DRL. Precisely, we consider the minimal distance between pairs of action-conditioned distributions under metrics on the space of probability measures on $\\mathbb{R}$ . We observe that some metrics are viable for this extension, while others are not. This formalism sets the stage for analyzing the influence of individual actions as well as decision frequency on, for example, an agent\u2019s return distributions. ", "page_idx": 1}, {"type": "text", "text": "Collapse of Distributional Control at High Frequency. Second, we establish tight bounds on the distributional action gaps of $h$ -dependent action-conditioned return distributions\u2014return distributions induced by applying a specific initial action for $h$ units of time. We prove that these distributional action gaps not only collapse, as $h$ tends to zero, but do so at a slower rate than action-value gaps. On one hand, therefore, distributional $Q$ -learning algorithms are susceptible to the same failures as $Q$ - learning in continuous-time RL. On the other hand, however, remedies to these failures transliterated to distributional $Q$ -learning algorithms are unlikely to succeed, because the means of these return distributions collapse faster than their other statistics. ", "page_idx": 1}, {"type": "text", "text": "Distributional Superiority. Third, we propose an axiomatic construction of a distributional analogue of the advantage, which we call the superiority. Leveraging our analysis of $h$ -dependent actionconditioned returns and their distributional action gaps, we present a frequency-scaled superiority distribution that enables greedy action selection at any fixed decision frequency. ", "page_idx": 1}, {"type": "text", "text": "A Distributional Action Gap-Preserving Algorithm. Fourth, we propose an algorithm that learns the superiority distribution from data. Empirically, we demonstrate that our algorithm maintains the ability to perform policy optimization at high frequencies more reliably than existing methods. ", "page_idx": 1}, {"type": "text", "text": "2 Setting ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Notation: Spaces will either be subsets of Euclidean space or discrete. Measurability, in the former case, will be with respect to the Borel sigma algebra; in the latter case, it will be with respect to the power set. The set of probability measures over a space $\\textsf{Y}$ will be denoted by $\\mathcal{P}(\\mathsf{Y})$ . Functions on spaces are assumed to be measurable. For $f:\\mathsf{Y}\\;{\\bar{\\to}}\\;\\mathsf{Z}$ and $\\mu\\in{\\mathcal{P}}(\\mathsf{Y})$ , the push forward of $\\mu$ through/by $f$ , $f_{\\#}\\mu\\in\\mathcal{P}(Z)$ , is defined by $f_{\\#}\\mu:=\\mu\\circ f^{-1}$ . For a random variable $X$ , defined implicitly on some probability space $\\left(\\Omega,\\mathcal{F},\\mathbf{P}\\right)$ , we write $\\operatorname{law}(X):=X_{\\#}\\mathbf{P}$ to denote the law of $X$ ; the notation $X\\,{=}\\,_{\\mathrm{law}}\\,Y$ is shorthand for la $\\operatorname{w}(X)=\\operatorname{law}(Y)$ . For any $\\mu\\in{\\mathcal{P}}(\\mathbb{R})$ , the quantile function of $\\mu$ , $F_{\\mu}^{-1}$ , is defined by $F_{\\mu}^{-1}(\\tau):=\\operatorname*{inf}_{z}\\{\\dot{F}_{\\mu}(z)\\geq\\tau\\}$ , where $F_{\\mu}(z)$ is the CDF of $\\mu$ . ", "page_idx": 1}, {"type": "text", "text": "2.1 Continuous-Time RL ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Here we give a brief introduction to the technical aspects of continuous-time RL, \u00e0 la [25]. We provide additional exposition and references in Appendix A. For any reader looking to defer some of this technical introduction, we summarize the core objects of interest at the end of Section 2.1.1. ", "page_idx": 1}, {"type": "text", "text": "2.1.1 MDPs ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Continuous-time Markov Decision Processes (MDPs) are defined by three spaces and four measurable functions: a time interval ${\\sf T}:=[0,T]$ with $T\\in(0,\\infty)$ or $\\mathsf T:=[0,T)$ with $T=\\infty$ , a state space $\\times\\subset\\mathbb{R}^{n}$ , an action space A, a drift $b:{\\mathsf{T}}\\times\\mathsf{X}\\times{\\mathsf{A}}\\to\\mathbb{R}^{n}$ , a diffusion $\\sigma:{\\mathsf{T}}\\times{\\mathsf{X}}\\times{\\mathsf{A}}\\to\\mathbb{R}^{n\\times n}$ , a reward $r:\\mathsf{T}\\times\\mathsf{X}\\to\\mathbb{R}$ , and a terminal reward $f:\\mathsf{X}\\to\\mathbb{R}$ .3 The pair $(b,\\sigma)$ govern the environment\u2019s dynamics by a family of SDEs parameterized by $a\\in\\mathsf{A}$ , ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathrm{d}X_{t}^{a}=b(t,X_{t}^{a},a)\\,\\mathrm{d}t+\\sigma(t,X_{t}^{a},a)\\,\\mathrm{d}B_{t}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Here $(B_{t})_{t\\geq0}$ is an $n$ -dimensional Brownian motion. In turn, any solution to (2.1) collects the state paths of an agent that chooses action $a$ at every time, regardless of the state they are in. ", "page_idx": 1}, {"type": "text", "text": "As is done in discrete-time RL, an agent might consider the induced Markov Reward Process (MRP) derived from a policy $\\pi:{\\mathsf{T}}\\times\\mathsf{X}\\to{\\mathcal{P}}(\\mathsf{A})$ .4 The dynamics of a policy-induced MRP (with policy $\\pi$ ) ", "page_idx": 1}, {"type": "text", "text": "are governed by the SDE ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{d}X_{t}^{\\pi}=b^{\\pi}(t,X_{t}^{\\pi})\\,\\mathrm{d}t+\\sigma^{\\pi}(t,X_{t}^{\\pi})\\,\\mathrm{d}B_{t}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Here, following [38, 15, 16], the policy-averaged coefficients $b^{\\pi}$ and $\\sigma^{\\pi}$ are defined by ", "page_idx": 2}, {"type": "equation", "text": "$$\nb^{\\pi}(t,x):=\\int_{\\mathsf{A}}b(t,x,a)\\,\\pi(\\mathrm{d}a\\,|\\,t,x)\\quad\\mathrm{and}\\quad\\sigma^{\\pi}(t,x):=\\left(\\int_{\\mathsf{A}}\\sigma\\sigma^{\\top}(t,x,a)\\,\\pi(\\mathrm{d}a\\,|\\,t,x)\\right)^{1/2}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Thus, solutions to (2.2) collect the paths of an agent following policy $\\pi$ . ", "page_idx": 2}, {"type": "text", "text": "A class of policies central to our study is those that fix an action $a$ from some time $t$ for a given persistence horizon $h$ . ", "page_idx": 2}, {"type": "text", "text": "Definition 2.1. Given $h>0$ and $a\\in\\mathsf{A},$ , a policy $\\pi$ is said to be $(h,a)$ -persistent at time $t\\in\\textsf{T}i f$ $\\pi(\\cdot\\,|\\,s,y)=\\delta_{a}$ for all $(s,y)\\in[t,t+h)\\times\\mathsf{X}$ . ", "page_idx": 2}, {"type": "text", "text": "In particular, given a policy $\\pi$ , we will consider $(h,a)$ -persistent modifications of $\\pi$ : for $t\\in{\\mathsf{T}}$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pi|_{h,a,t}(\\cdot\\,|\\,s,y):={\\\\binom{\\delta_{a}}{\\pi(\\cdot\\,|\\,s,y)}}\\quad\\mathrm{if}\\;s\\in[t,t+h)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "These policies will help us understand the influence of taking actions relative to others as well as to those taken by $\\pi$ . We assume $h$ is small enough so that $t+h\\in\\mathsf{T}$ . ", "page_idx": 2}, {"type": "text", "text": "In order to guarantee the global-in-time existence and uniqueness of solutions to our SDEs (2.1) and (2.2), we make two sets of assumptions. ", "page_idx": 2}, {"type": "text", "text": "Assumption 2.2. The functions $b$ and $\\sigma$ have linear growth and are Lipschitz in state, uniformly in time and action: a finite, positive constants $C_{2,2}$ exists such that ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t,a}|b(t,x,a)|+\\operatorname*{sup}_{t,a}|\\sigma(t,x,a)|\\leq C_{2.2}(1+|x|)\\quad\\forall x\\in\\mathsf{X};\\quad a n d\n$$", "text_format": "latex", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t,a}|b(t,x,a)-b(t,y,a)|+\\operatorname*{sup}_{t,a}|\\sigma(t,x,a)-\\sigma(t,y,a)|\\leq C_{2,2}|x-y|\\quad\\forall x,y\\in\\mathbb{X}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Assumption 2.3. The averaged coefficient functions $b^{\\pi}$ and $\\sigma^{\\pi}$ are Lipschitz in state, uniformly in time: a finite, positive constant $C_{2.3}$ exists such that ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t}|b^{\\pi}(t,x)-b^{\\pi}(t,y)|+\\operatorname*{sup}_{t}|\\sigma^{\\pi}(t,x)-\\sigma^{\\pi}(t,y)|\\leq C_{2.3}|x-y|\\quad\\forall x,y\\in\\mathsf{X}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "These assumptions are standard in the analysis of continuous-time RL, optimal control, and SDEs [12, 26, 38, 16, 42]. Since $\\pi$ is a function of state, we note that Assumption 2.3 is not a direct consequence of Assumption 2.2. The coefficients $b^{\\pi}$ and $\\sigma^{\\pi}$ satisfy the conditions of Assumption 2.3 provided $b$ and $\\sigma$ satisfy some (also standard) additional regularity conditions and $\\pi$ satisfies some regularity conditions. These technical details are discussed in Appendix A.2. ", "page_idx": 2}, {"type": "text", "text": "In summary, in continuous-time RL, there are three stochastic processes of interest: $(X_{s}^{\\bullet})_{s\\geq t}$ with $\\bullet\\in\\{a,\\pi,\\dot{\\pi}|_{h,a,t}\\}$ , all beginning at some time $t$ . These processes collect the state paths of an agent in one of three scenarios: 1. choosing action $a$ at every state and time; 2. following a policy $\\pi$ ; or 3. choosing $a$ at every state and time for the first $h$ units of time and following $\\pi$ thereafter. ", "page_idx": 2}, {"type": "text", "text": "2.1.2 Value Functions and their Distributions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Given a policy-induced state process $(X_{s}^{\\pi})_{s\\geq t}$ , the (discounted) random return $G^{\\pi}(t,x)$ earned by $\\pi$ starting from state $x\\in\\mathsf{X}$ at time $t\\in{\\mathsf{T}}$ is defined [41] by ", "page_idx": 2}, {"type": "equation", "text": "$$\nG^{\\pi}(t,x):=\\int_{t}^{T}\\gamma^{s-t}r(s,X_{s}^{\\pi})\\,\\mathrm{d}s+\\gamma^{T-t}f(X_{T}^{\\pi})\\quad\\mathrm{with}\\quad X_{t}^{\\pi}=x,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $f\\equiv0$ when ${\\sf T}=[0,\\infty)$ . We distinguish returns earned by $(h,a)$ -persistent modifications of policies. We call these $h$ -dependent action-conditioned returns and denote them by $Z_{h}^{\\pi}(t,x,a)$ . Given $\\pi$ , they are defined by ", "page_idx": 2}, {"type": "equation", "text": "$$\nZ_{h}^{\\pi}(t,x,a):=\\int_{t}^{T}\\gamma^{s-t}r(s,X_{s}^{\\pi|h,a,t})\\,\\mathrm{d}s+\\gamma^{T-t}f(X_{T}^{\\pi|h,a,t})\\quad\\mathrm{with}\\quad X_{t}^{\\pi|h,a,t}=x.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Value-based approaches in RL estimate either the value function $V^{\\pi}(t,x):=\\mathbf{E}[G^{\\pi}(t,x)]$ or the $h$ -dependent action-value function $Q_{h}^{\\pi}(t,x,a):=\\mathbf{E}[Z_{h}^{\\pi}({\\dot{t}},x,a)]$ .5 As distributional approaches in RL estimate the laws of returns, following [41], we define ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\eta^{\\pi}(t,x):=\\operatorname{law}(G^{\\pi}(t,x))\\quad{\\mathrm{and}}\\quad\\zeta_{h}^{\\pi}(t,x,a):=\\operatorname{law}(Z_{h}^{\\pi}(t,x,a)).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "It is important to note that only the laws of random returns (and not their representations as random variables) are observable and modeled in practice. ", "page_idx": 3}, {"type": "text", "text": "2.2 $Q$ -Learning in Continuous Time ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The failure of action-value-based RL in continuous-time stems from the collapse of action values at a given state to the value of that state. Precisely, Tallec et al. and Jia and Zhou established that ", "page_idx": 3}, {"type": "equation", "text": "$$\nQ_{h}^{\\pi}(t,x,a)-V^{\\pi}(t,x)=(H^{\\pi}(t,x,a)+(\\log\\gamma)V^{\\pi}(t,x))h+o(h),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $H^{\\pi}\\in\\mathbb{R}$ is independent of $h$ (see [34] and [16] respectively).6In a discrete action space, given a state $x$ and time $t$ , a concise way to capture the asymptotic information of (2.6) is by considering the action gap [11, 5] of the associated $h$ -dependent action values ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathtt{g a p}(Q_{h}^{\\pi},t,x):=\\operatorname*{min}_{a_{1}\\neq a_{2}}|Q_{h}^{\\pi}(t,x,a_{1})-Q_{h}^{\\pi}(t,x,a_{2})|.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Anticipating (2.6), which implies that ${\\tt g a p}(Q_{h}^{\\pi},t,x)={\\cal O}(h)$ , Baird proposed AU wherein he estimated the rescaled advantage function $A_{h}^{\\pi}$ in place of $Q_{h}^{\\pi}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\nA_{h}^{\\pi}(t,x,a):=\\frac{Q_{h}^{\\pi}(t,x,a)-V^{\\pi}(t,x)}{h}\\quad\\forall(t,x,a)\\in\\mathsf{T}\\times\\mathsf{X}\\times\\mathsf{A}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Note that ${\\tt g a p}(A_{h}^{\\pi},t,x)={\\cal O}(1)$ . Tallec et al. [34] and Jia and Zhou [16], following Baird, also estimated $A_{h}^{\\pi}$ to ameliorate $Q$ -learning in continuous time. ", "page_idx": 3}, {"type": "text", "text": "3 The Distributional Action Gap ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we define a distributional notion of action gap; we prove that $h$ -dependent actionconditioned return distributions collapse to their underlying policy\u2019s return distribution as $h$ vanishes; and we quantify the rate of collapse of these return distributions. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.1. Consider an MDP with discrete action space and let $\\mu:{\\mathsf{T}}\\times{\\mathsf{X}}\\times{\\mathsf{A}}\\to({\\mathcal{P}}(\\mathbb{R}),d)$ for a metric $d$ . The $d$ action gap of $\\mu$ at a state $x$ and time $t$ is given by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathsf{d i s t g a p}_{d}(\\mu,t,x):=\\operatorname*{min}_{a_{1}\\neq a_{2}}d(\\mu(t,x,a_{1}),\\mu(t,x,a_{2})).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "While $\\mathbb{R}$ has a canonical metric, induced by $\\big|\\cdot\\big|$ , the space $\\mathcal{P}(\\mathbb{R})$ does not. So a choice must be made, and some metrics are unsuitable. For example, in deterministic MDPs with deterministic policies, return distributions are identified by expected returns: $\\zeta_{h}^{\\pi}(t,x,a)$ is the delta at $Q_{h}^{\\pi}(t,x,a)$ , for all $(t,x,a)\\in\\mathsf{T}\\times\\mathsf{X}\\times\\mathsf{A}$ . Thus, distga $\\mathsf{p}_{d}(\\zeta_{h}^{\\pi},t,x)$ should vanish as $h$ decreases to zero if $\\mathtt{g a p}(Q_{h}^{\\pi},t,x)$ vanishes as $h$ decreases to zero. With the total variation metric $d=\\mathrm{TV}$ , for instance, this is not the case, making TV unsuitable. Indeed, suppose we have a deterministic MDP with $\\mathsf{A}=\\{a_{1},a_{2}\\}$ and such that $\\zeta_{h}^{\\pi}(t,x,a_{1})=\\delta_{h}$ and $\\zeta_{h}^{\\pi}(t,x,a_{2})=\\delta_{0}$ , for some state $x$ and time $t$ (see, e.g., [34]). Then $\\mathsf{d i s t g a p}_{\\mathrm{TV}}(\\zeta_{h}^{\\pi},t,x)=1$ , for all $h>0$ , yet $\\mathtt{g a p}(Q_{h}^{\\pi},t,x)=h$ . ", "page_idx": 3}, {"type": "text", "text": "The $W_{p}$ distances from the theory of Optimal Transportation (see [36]), however, are suitable. They are defined via couplings of distributions. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.2. Let $\\mu,\\nu\\in\\mathcal{P}(\\mathbb{R})$ . $A\\;\\kappa\\in\\;\\mathcal{P}(\\mathbb{R}^{2})$ is $a$ coupling of $\\mu$ and $\\nu$ if its first and second marginals are $\\mu$ and $\\nu$ respectively. We denote the set of these couplings by $\\mathcal{C}(\\mu,\\nu)$ . ", "page_idx": 3}, {"type": "text", "text": "Definition 3.3. Let $\\mu,\\nu\\in\\mathcal{P}(\\mathbb{R})^{7}$ and $p\\in[1,\\infty)$ . The $W_{p}$ distance between $\\mu$ and $\\nu$ is ", "page_idx": 3}, {"type": "equation", "text": "$$\nW_{p}(\\mu,\\nu):=\\operatorname*{inf}_{\\kappa\\in\\mathcal{C}(\\mu,\\nu)}\\left(\\int_{\\mathbb{R}^{2}}\\vert z-w\\vert^{p}\\,\\kappa(\\mathrm{d}z\\mathrm{d}w)\\right)^{1/p}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Any coupling attaining the infimum in (3.1) is called a $W_{p}$ -optimal coupling. Henceforth, we write ${\\mathsf{d i s t g a p}}_{p}$ when considering $W_{p}$ action gaps. If $\\mu$ and $\\nu$ are deltas at $Q_{h}^{\\pi}(t,x,a_{1})$ and $Q_{h}^{\\pi}(t,x,a_{2})$ respectively, then the right-hand side of (3.1) is equal to $|Q_{h}^{\\pi}(t,x,a_{1})-Q_{h}^{\\pi}(t,x,a_{2})|$ . Hence, in deterministic MDPs with deterministic policies, $W_{p}$ action gaps of $\\zeta_{h}^{\\pi}$ are identical to action gaps of $Q_{h}^{\\pi}$ , making the $W_{p}$ distances suitable in the above sense. In non-deterministic MDPs, the relationship between distg $\\mathsf{\\Pi}_{|\\mathsf{p}_{p}}(\\zeta_{h}^{\\pi},t,x)$ and $\\mathtt{g a p}(Q_{h}^{\\pi},t,x)$ is opaque. ", "page_idx": 4}, {"type": "text", "text": "The following results study the $W_{p}$ action gap of $\\zeta_{h}^{\\pi}$ as a function of $h$ , lending some color to the relationship between distga $\\mathsf{p}_{p}(\\zeta_{h}^{\\pi},t,x)$ and $\\mathtt{g a p}(Q_{h}^{\\pi},t,x)$ . These results all hold under Assumptions 2.2 and 2.3. Henceforth, we suppress mention of these assumptions; we do not restate them explicitly. First, we observe that $W_{p}$ action gaps of $\\zeta_{h}^{\\pi}$ are bounded from below by action gaps of $Q_{h}^{\\pi}$ . ", "page_idx": 4}, {"type": "text", "text": "Proposition 3.4. For all $(t,x)\\in\\mathsf{T}\\times\\mathsf{X}_{\\mathrm{r}}$ , we have that distg $\\begin{array}{r}{\\mathsf{a p}_{p}(\\zeta_{h}^{\\pi},t,x)\\ge\\mathtt{g a p}(Q_{h}^{\\pi},t,x).}\\end{array}$ . ", "page_idx": 4}, {"type": "text", "text": "For a proof of this statement and any other made in this work, see Appendix B. Our next result establishes that $W_{p}$ action gaps of $\\zeta_{h}^{\\pi}$ , like action gaps of $Q_{h}^{\\pi}$ , vanishes for a large class of MDPs. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.5. If r and $f$ are bounded, then $\\begin{array}{r}{\\operatorname*{lim}_{h\\downarrow0}W_{p}(\\zeta_{h}^{\\pi}(t,x,a),\\eta^{\\pi}(t,x))=0,}\\end{array}$ , for all $(t,x,a)\\in$ ${\\mathsf{T}}\\times\\mathsf{X}\\times{\\mathsf{A}}_{}$ ; hence, $\\operatorname*{lim}_{h\\downarrow0}$ distg $\\mathsf{\\Delta}_{|\\mathsf{P}_{p}}(\\zeta_{h}^{\\pi},t,x)=0$ . ", "page_idx": 4}, {"type": "text", "text": "While Theorem 3.5 shows that the $W_{p}$ distance between $\\zeta_{h}^{\\pi}(t,x,a)$ and $\\eta^{\\pi}(t,x)$ (and the $W_{p}$ action gap of $\\zeta_{h}^{\\pi}$ at $(t,x)\\in\\mathsf{T}\\times\\mathsf{X})$ does indeed vanish as $h$ decreases, it does not identify the rate at which it does so. Our next two theorems establish this rate. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.6. MDPs and policies exist in and under which, for all $(t,x,a)\\in\\mathsf{T}\\times\\mathsf{X}\\times\\mathsf{A},$ , we have that $W_{p}(\\zeta_{h}^{\\pi}(t,x,a),\\eta^{\\pi}(t,x))\\gtrsim h^{1/2}$ and distga $\\mathsf{p}_{p}(\\zeta_{h}^{\\pi},t,x)\\gtrsim h^{1/2}$ . ", "page_idx": 4}, {"type": "text", "text": "Finally, we prove that for a large class of MDPs (different from but overlapping with the class of MDPs captured in Theorem 3.5), the lower bound found in Theorem 3.6 is an upper bound. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.7. If $r$ is Lipschitz in state, uniformly in time, $f$ is Lipschitz, and $T\\ <\\ \\infty,$ , then $W_{p}(\\zeta_{h}^{\\pi}(t,x,a),\\dot{\\eta}^{\\pi}(t,x))\\stackrel{\\cdot}{\\sim}h^{1/2}$ , for all $(t,x,a)\\in\\mathsf{T}\\times\\mathsf{X}\\times\\mathsf{A},$ ; hence, distg $\\mathfrak{p}_{p}(\\zeta_{h}^{\\pi},t,x)\\lesssim h^{1/2}$ . ", "page_idx": 4}, {"type": "text", "text": "Theorems 3.6 and 3.7 demonstrate that the $W_{p}$ distance between $\\zeta_{h}^{\\pi}(t,x,a)$ and $\\eta^{\\pi}(t,x)$ and the distance between $Q_{h}^{\\pi}(t,x,a)$ and $V^{\\pi}(t,x)$ are of different orders in terms of $h$ . Thus, we see that distg $\\mathsf{i p}_{p}(\\zeta_{h}^{\\pi},t,x)$ and $\\mathtt{g a p}(Q_{h}^{\\pi},t,x)$ in stochastic MDPs are fundamentally different. ", "page_idx": 4}, {"type": "text", "text": "4 Distributional Superiority ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we introduce a probabilistic generalization of the advantage. We define this random variable\u2014which we call the superiority and denote by $S_{h}^{\\pi}$ \u2014via a pair of axioms. ", "page_idx": 4}, {"type": "text", "text": "A natural construction of the superiority at $(t,x,a)$ is given by $Z_{h}^{\\pi}(t,x,a)-G^{\\pi}(t,x)$ . The law of this difference, however, depends on the joint law of $(Z_{h}^{\\pi}(t,x,a),G^{\\pi}(t,x))$ , which is unobservable in practice and ill-defined (cf. Section 2.1.2). Yet, the set of all possible laws of this difference is easily characterized; it is the set of coupled difference representations of $\\zeta_{h}^{\\pi}(t,x,a)$ and $\\eta^{\\pi}(t,x)$ . ", "page_idx": 4}, {"type": "text", "text": "Definition 4.1. Let $\\mu,\\nu\\in\\mathcal{P}(\\mathbb{R})$ . $A$ coupled difference representation (CDR) $\\psi\\in{\\mathcal{P}}(\\mathbb{R})$ of $\\mu$ and $\\nu$ takes the form $\\psi=\\Delta_{\\#}\\kappa$ where $\\kappa\\in\\mathcal{C}(\\mu,\\nu)$ and $\\Delta:\\mathbb{R}^{2}\\rightarrow\\mathbb{R}$ is given by $\\Delta(z,w):=z-w$ . The set of all coupled difference representations of $\\mu$ and $\\nu$ will be denoted by $\\mathcal{D}(\\mu,\\nu)$ . ", "page_idx": 4}, {"type": "text", "text": "Our first axiom places the superiority\u2019s law in this set, ${\\mathcal D}(\\zeta_{h}^{\\pi}(t,x,a),\\eta^{\\pi}(t,x))$ . ", "page_idx": 4}, {"type": "text", "text": "Axiom 1. The law of $S_{h}^{\\pi}(t,x,a)$ is a coupled difference representation of $\\zeta_{h}^{\\pi}(t,x,a)$ and $\\eta^{\\pi}(t,x)$ . ", "page_idx": 4}, {"type": "text", "text": "Our second axiom encodes a type of consistency for deterministic policy behavior. ", "page_idx": 4}, {"type": "text", "text": "Axiom 2. $S_{h}^{\\pi}(t,x,a)$ is deterministic whenever $\\pi$ is $(h,a)$ -persistent at time $t$ . ", "page_idx": 4}, {"type": "text", "text": "To see how Axiom 2 encodes a notion of deterministic consistency, first consider its discrete-time analogue: the superiority at $(x,a)$ for a policy $\\pi$ is deterministic $i f\\pi$ at $x$ deterministically chooses $a$ . In this situation, our $a$ -following agent makes the same choices as a $\\pi$ -following agent\u2014both take action $a$ in state $x$ initially and then follow $\\pi$ thereafter\u2014, and we posit that the superiority should not be random. The continuous-time analogue of the situation just described occurs precisely when a policy $\\pi$ is $(h,a)$ -persistent at starting time $t$ . Given a starting time $t$ and state $x$ , an agent that chooses action $a$ between $t$ and $t+h$ and then follows $\\pi$ is following the $(h,a)$ -persistent modification of $\\pi$ at time $t$ . By definition, they make the same choices as a $\\pi$ -following agent when $\\pi$ is $(h,a)$ -persistent starting at time $t$ . Axiom 2 stipulates that, in this case, $S_{h}^{\\pi}(t,x,a)$ should be deterministic. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "By construction, if $\\psi_{h}^{\\pi}(t,x,a)\\in\\mathcal{D}(\\zeta_{h}^{\\pi}(t,x,a),\\eta^{\\pi}(t,x))$ , then its mean is $Q_{h}^{\\pi}(t,x,a)-V^{\\pi}(t,x)$ (see Appendix B.2 for a proof of this claim). Axiom 2 then says that any determining coupling $\\kappa_{h}^{\\pi}(t,x,a)$ when $\\pi$ is $(h,a)$ -persistent at time $t$ must be such that $\\Delta_{\\#}\\kappa_{h}^{\\pi}(t,x,a)=\\delta_{0}$ .8 In particular, Axiom 2 nontrivially restricts ${\\mathcal D}(\\zeta_{h}^{\\pi}(t,x,a),\\eta^{\\pi}(t,x))$ . ", "page_idx": 5}, {"type": "text", "text": "Example 4.2. Let $\\iota_{h}^{\\pi}(t,x,a)\\,:=\\,\\Delta_{\\#}\\kappa_{h}^{\\pi}(t,x,a)$ for $\\kappa_{h}^{\\pi}(t,x,a)\\,=\\,\\zeta_{h}^{\\pi}(t,x,a)\\,\\otimes\\,\\eta^{\\pi}(t,x)$ . If $\\pi$ is $(h,a)$ -persistent at time $t$ , then $\\mathbf{Var}(\\iota_{h}^{\\pi}(t,x,a))=2\\mathbf{Var}(\\eta^{\\pi}(t,x))$ . This variance is 0 only when $\\pi$ \u2019s return is deterministic. Hence, $\\iota_{h}^{\\pi}(t,x,a)$ may be nontrivial even when conditioning on a reflects the policy\u2019s behavior exactly. We posit, via Axiom 2, that this should be prohibited. ", "page_idx": 5}, {"type": "text", "text": "In fact, Axiom 2 determines a single coupling, if we want a consistent choice across all time-stateaction triplets and all MDPs. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.3. Let $\\kappa\\in\\mathcal C(\\mu,\\mu)$ for some $\\mu\\in{\\mathcal{P}}(\\mathbb{R})$ . The push-forward of $\\kappa$ by $\\Delta$ is the delta at zero, $\\Delta_{\\#}\\kappa=\\delta_{0}$ , if and only if $\\kappa$ is a $W_{p}$ -optimal coupling, for some $p\\in[1,\\infty)$ . Moreover, there is only one such coupling. It is given by $\\kappa_{\\mu}:=(\\mathrm{id},\\mathrm{id})_{\\#}\\mu$ or, equivalently, $\\kappa_{\\mu}:=(F_{\\mu}^{-1},F_{\\mu}^{-1})_{\\#}\\mathbb{1}(0,1)$ Here $\\mathcal{U}(0,1)$ is the uniform distribution on $[0,1]$ . ", "page_idx": 5}, {"type": "text", "text": "The second definition of $\\kappa_{\\mu}$ corresponds, more generally, to the $W_{p}$ -optimal coupling, for all $p\\geq1$ , of $\\mu$ and $\\nu$ given by $\\kappa_{\\mu,\\nu}:=(F_{\\mu}^{-1},F_{\\nu}^{-1})_{\\#}\\mathbb{4}(0,1)$ . As $\\Delta_{\\#}\\kappa_{\\mu,\\nu}$ \u2019s quanitle function is $F_{\\mu_{-}}^{-1}-F_{\\nu_{-}}^{-1}$ , we have in hand everything we need to define the superiority distribution (via its quantile function). ", "page_idx": 5}, {"type": "text", "text": "Definition 4.4. The superiority distribution $\\psi_{h}^{\\pi}$ at $(t,x,a)\\in\\mathsf{T}\\times\\mathsf{X}\\times\\mathsf{A}$ is ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\psi_{h}^{\\pi}(t,x,a):=(F_{\\zeta_{h}^{\\pi}(t,x,a)}^{-1}-F_{\\eta^{\\pi}(t,x)}^{-1})_{\\#}\\mathbb{2}\\mathbb{}\\mathbb{U}(0,1).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "As $\\psi_{h}^{\\pi}(t,x,a)$ has the smallest possible central absolute $p$ th moments among all CDRs of $\\zeta_{h}^{\\pi}(t,x,a)$ and $\\eta^{\\pi}(t,x)$ , heuristically, it captures more of the individual features of both return distributions than other such CDRs (like $\\iota_{h}^{\\pi}(t,x,a)$ in Example 4.2). We illustrate this by example in Figure 4.1. ", "page_idx": 5}, {"type": "image", "img_path": "BRW0MKJ7Rr/tmp/1e77e00923e9671034fe12c0a65b1a25f04c697668b1e0e6c29d92b6ebecceeb.jpg", "img_caption": ["Figure 4.1: PDFs of Return Distributions and Two Candidate CDRs. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "4.1 The Rescaled Superiority Distribution ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "From Section 3, we know that the $W_{p}$ distance between $\\zeta_{h}^{\\pi}(t,x,a)$ and $\\eta^{\\pi}(t,x)$ , for every $p\\geq1$ , vanishes as $h$ vanishes. Moreover, we know the rate at which this distance disappears. As a result, by construction, the central absolute $p$ th moments of $\\psi_{h}^{\\pi}(t,x,a)$ collapse as $h$ collapses, and, for a large class of MDPs, we understand the rate at which these moments collapse. More generally and precisely, if we consider the $q$ -rescaled superiority distribution defined by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\psi_{h;q}^{\\pi}:=(h^{-q}\\,\\mathsf{i d})_{\\#}\\psi_{h}^{\\pi},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "we see that Theorems 3.6 and 3.7 translate to the follow statements on Wp action gaps of \u03c8h\u03c0;q: ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.5. MDPs and policies exist satisfying Assumptions 2.2 and 2.3 in and under which, for all $(t,x)\\in\\mathsf{T}\\times\\mathsf{X},$ , we have that distga $\\mathsf{p}_{p}(\\psi_{h;q}^{\\bar{\\pi}},t,x)\\gtrsim h^{\\hat{\\imath}/2-q}$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.6. Under Assumptions 2.2 and 2.3, if $r$ is Lipschitz in state, uniformly in time, $f$ is Lipschitz, and $T<\\infty,$ , then distg $\\mathsf{a p}_{p}(\\psi_{h;q}^{\\pi},t,x)\\lesssim\\bar{h}^{1/2-q}$ , for all $(t,x)\\in\\mathsf{T}\\times\\mathsf{X}.$ . ", "page_idx": 5}, {"type": "text", "text": "These two theorems tell us how to preserves the $W_{p}$ action gaps of $q$ -rescaled superiority distributions (as a function of $h$ ). They identify $q\\,=\\,^{1}\\!/\\!2$ . For $q\\,<\\,^{1}\\!/\\!2$ , $W_{p}$ action gaps vanish as $h$ vanishes. Whereas for $q>1/2$ , $W_{p}$ action gaps blow up as $h$ vanishes. These behaviors are undesirable. When $q<1/2$ , the influence of an action on an agent\u2019s superiority becomes indistinguishable from any other action. For $q>1/2$ , ever larger sample sizes are need to obtain any statistical estimate of an agent\u2019s superiority with the same level of accuracy. These scenarios are untenable. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Another consideration regarding rescalings of $\\psi_{h}^{\\pi}$ is whether they upend rankings of actions determined by some given measure of utility. This would be counterproductive. In DRL, agents often use distortion risk measures [1] to rank actions ([10, 9, 22, 4, 17]). ", "page_idx": 6}, {"type": "text", "text": "Definition 4.7. Given $\\beta\\,\\in\\,\\mathcal{P}([0,1])$ , the distortion risk measure $\\rho_{\\beta}:{\\mathcal{P}}(\\mathbb{R})\\to\\mathbb{R}$ is defined by $\\rho_{\\beta}(\\mu):=\\langle\\beta,F_{\\mu}^{-1}\\rangle.$ ; on $\\mu\\in{\\mathcal{P}}(\\mathbb{R})$ , its value is given by the integral of $F_{\\mu}^{-1}$ with respect to $\\beta$ . A family of $\\rho_{\\beta}$ is the $\\alpha$ -conditional value-at-risk measures $\\left(\\alpha{-}\\mathrm{CVaR}\\right)$ [29], where $\\beta_{\\alpha}=\\mathcal{U}(0,\\alpha)$ for $\\alpha\\in(0,1]$ ; $\\alpha=1$ is the expected-value utility measure. Crucially, $\\psi_{h;q}^{\\pi}$ preserves $\\rho_{\\beta}$ -valued utility. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.8. Let $\\rho_{\\beta}$ be a distortion risk measure, $q\\geq0$ , and $h>0$ . If $\\rho_{\\beta}(\\eta^{\\pi}(t,x))<\\infty,$ , then arg $\\begin{array}{r}{\\operatorname*{max}_{a\\in\\mathsf{A}}\\rho_{\\beta}\\big(\\psi_{h;q}^{\\pi}(t,x,a)\\big)=\\arg\\operatorname*{max}_{a\\in\\mathsf{A}}\\rho_{\\beta}\\big(\\zeta_{h}^{\\pi}(t,x,a)\\big)}\\end{array}$ . ", "page_idx": 6}, {"type": "text", "text": "In turn, the $1/2$ -rescaled superiority distribution is not only $W_{p}$ action gap preserving but matches $\\zeta_{h}^{\\pi}$ in its greedy choice of action as measured by a distortion risk measure. ", "page_idx": 6}, {"type": "text", "text": "4.2 Algorithmic Considerations ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now turn to building DRL algorithms based on our theory. Our algorithms leverage the quantile TD-learning framework [10] to learn $\\rho_{\\beta}$ -greedy policies, for a given distortion risk measure $\\rho_{\\beta}$ , just as DAU [34] leverages the $Q$ -learning framework to learn greedy policies. Full pseudocode and implementation details are given in Appendix C. ", "page_idx": 6}, {"type": "text", "text": "At the heart of our algorithms is an equality of quantile functions, which holds by construction, ", "page_idx": 6}, {"type": "equation", "text": "$$\nF_{\\zeta_{h}^{\\pi}(t,x,a)}^{-1}=F_{\\eta^{\\pi}(t,x)}^{-1}+h^{q}F_{\\psi_{h;q}^{\\pi}(t,x,a)}^{-1}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Indeed, given $\\eta$ and $\\psi_{h;q}$ , as models of $\\eta^{\\pi}$ and $\\psi_{h;q}^{\\pi}$ respectively, equation (4.1) justifies the application of quantile TD-learning to $\\zeta_{h}$ , as a model for $\\zeta_{h}^{\\pi}$ , defined via the quantile function ", "page_idx": 6}, {"type": "equation", "text": "$$\nF_{\\zeta_{h}(t,x,a)}^{-1}:=F_{\\eta(t,x)}^{-1}+h^{q}F_{\\psi_{h;q}(t,x,a)}^{-1}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "That said, we cannot realize quantile TD-learning without defining predictions and bootstrap targets in terms of $m$ -quantile representations9 [10, 4] of $\\zeta_{h}$ , via those of $\\eta$ and $\\psi_{h;q}$ . ", "page_idx": 6}, {"type": "text", "text": "While we may freely parameterize the $m$ -quantile representation of $\\eta$ with a neural network (with interface) $\\boldsymbol{\\theta}:\\dot{{\\mathsf{T}}}\\times\\dot{\\mathsf{X}}\\dot{\\to}\\mathbb{R}^{m}$ , we have to be careful when parameterizing the $m$ -quantile representation of $\\psi_{h;q}$ . Given a neural network $\\phi:\\mathsf{T}\\times\\mathsf{X}\\times\\mathsf{A}\\to\\mathbb{R}^{m}$ , we set ", "page_idx": 6}, {"type": "equation", "text": "$$\nF_{\\psi_{h;q}(t,x,a)}^{-1}:=\\phi(t,x,a)-\\phi(t,x,a^{\\star})\\quad\\mathrm{with}\\quad a^{\\star}\\in\\arg\\operatorname*{max}_{a\\in\\mathbb{A}}\\rho_{\\beta}(\\phi(t,x,a)).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "This ensures we identify a $\\rho_{\\beta}$ -greedy policy; it is 0 at the $\\rho_{\\beta}$ -greedy action $a^{\\star}$ (cf. [34, Eq. 27]). ", "page_idx": 6}, {"type": "text", "text": "With appropriate parameterized $m$ -quantile representations of $\\eta$ and $\\psi_{h;q}$ in hand, we derive our predictions and bootstrap targets. By (4.2), recalling $\\theta$ and (4.3), we compute our predictions via ", "page_idx": 6}, {"type": "equation", "text": "$$\nF_{\\zeta_{h}(t,x,a)}^{-1}:=\\theta(t,x)+h^{q}(\\phi(t,x,a)-\\phi(t,x,a^{\\star})).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "By Wiltzer [40], as $X_{t}^{\\pi|_{h,a,t}}=x$ and $X_{t+h}^{\\pi|_{h,a,t}}=X_{t+h}^{a}$ ta+h, observe that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Z_{h}^{\\pi}(t,x,a)\\!=\\!\\!\\operatorname*{law}\\displaystyle\\int_{0}^{h}\\gamma^{s}r(t+s,X_{t+s}^{\\pi|_{h,a,t}})\\,\\mathrm{d}s+\\gamma^{h}G^{\\pi}(t+h,X_{t+h}^{\\pi|_{h,a,t}})}\\\\ &{\\qquad\\qquad=\\!\\!\\operatorname*{law}h r(t,x)+\\gamma^{h}G^{\\pi}(t+h,X_{t+h}^{a})+Y_{h},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where ${\\bf E}[|Y_{h}|^{p}]=o(h)^{10}$ for all $p\\in\\mathbb N$ . So upon getting a sample state/realization $x_{t+h}$ of $X_{t+h}^{a}$ , as in [30], we compute our bootstrap targets via ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{T}F_{\\zeta_{h}(t,x,a)}^{-1}:=h r(t,x)+\\gamma^{h}\\theta(t+h,x_{t+h}).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In summary, the predictions (4.4) and the bootstrap targets (4.5) together characterize a family of QR-DQN-based algorithms called $\\mathrm{DSUP}(q)$ , whose core update is outlined in Algorithm 1. ", "page_idx": 7}, {"type": "table", "img_path": "BRW0MKJ7Rr/tmp/4cf0ac406756bbd034a53a4d0e33d84858578077cada7df576efcf7f650a6905.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "One theoretical drawback of $\\mathrm{DSUP}(q)$ for mean-return control is that the mean of the $q$ -rescaled superiority distribution is $O(1)$ only when $q=1$ , by (2.6) and (2.7). Thus, we propose modeling $A_{h}^{\\pi}$ simultaneously. This yields a novel form of a two-timescale approach to value-based RL (see, e.g., [8]). In particular, we estimate $\\vartheta_{h;q}^{\\pi}$ defined by ", "page_idx": 7}, {"type": "equation", "text": "$$\nF_{\\vartheta_{h;q}^{\\pi}(t,x,a)}^{-1}:=F_{\\psi_{h;q}^{\\pi}(t,x,a)}^{-1}+(1-h^{1-q})A_{h}^{\\pi}(t,x,a).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "We call $\\vartheta_{h;q}^{\\pi}$ the advantage-shifted $q$ -rescaled superiority. Note that its mean is $A_{h}^{\\pi}$ , which is $O(1)$ . To realize this, we approximate $A_{h}^{\\pi}$ using DAU and employ parameter sharing between the approximators of $A_{h}^{\\pi}$ and $\\psi_{h;q}^{\\pi}$ . We call this family of algorithms $\\mathsf{D A U+D S U P}(q)$ . We note that $A_{h}^{\\pi}$ is used only for increasing action gaps; it does not change the training loss for $\\eta$ and $\\psi_{h;q}$ . ", "page_idx": 7}, {"type": "text", "text": "5 Simulations ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The empirical work herein is two-fold in nature: illustrative and comparative. First, we simulate an example that illustrates Theorems $3.6/4.5$ and Theorem $3.7/4.6$ and their consequences. Second, in an option-trading environment, we compare the performance of $\\psi_{h}^{\\pi}$ -based agent(s) against QR-DQN [10] and DAU [34] in the risk-neutral setting and against QR-DQN in a risk-sensitive setting. ", "page_idx": 7}, {"type": "text", "text": "5.1 The Rescaled Superiority Distribution Revisited ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Consider an MDP with time horizon 10, a two element action space, 0 and 1\u2014when action 1 is executed, the system follows 1-dimensional Brownian dynamics with a constant drift of 10, and otherwise, the state is fixed\u2014, a reward that equals the agent\u2019s signed distance to 0, and a trivial terminal reward. We estimate four distributions at $(t,x,\\bar{a})\\,=\\,(0,\\bar{0},1)$ for the policy that always selects 0. Figure 5.1 shows these estimated distributions for a sample of frequencies $\\boldsymbol{(\\mathrm{{kHz})}}$ , $\\omega={^1}/h$ . ", "page_idx": 7}, {"type": "image", "img_path": "BRW0MKJ7Rr/tmp/c1ec978571c4f7c4d549554e25c3a50d2e4ab4e33ee9b1e06bf81be80eb134b2.jpg", "img_caption": ["Figure 5.1: Monte-Carlo estimates of $\\psi_{h;q}^{\\pi}$ , for $q=0,1,1/2$ , and \u03d1\u03c0h;1/2 as a function of \u03c9 = 1/h. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "First (from the left), we see that $\\psi_{h}^{\\pi}$ collapses to $\\delta_{0}$ , as $h$ tends to 0. Thus, accurate action ranking distributional or otherwise becomes impossible in the vanishing $h$ limit. Second, we see that rescaling by $h$ produces distributions with $O(1)$ mean but infinite non-mean statistics in the vanishing $h$ limit. Here the $O(1)$ means are imperceptible in face of the large variances. So while this rescaling permits ranking actions by action values, it does so at the expense of producing high-variance distributions. ", "page_idx": 7}, {"type": "text", "text": "Third, we see that rescaling by $h^{1/2}$ yields distributions with $O(1)$ non-mean statistics but vanishingly small means, $O(h^{1/2})$ . Hence, this rescaling permits ranking actions by non-mean statistics, even if action values again becomes indistinguishable in the vanishing $h$ limit. That said, the vanishing rate of the means here is slower than when no rescaling is considered, $O(h)$ . Fourth, we see that rescaling by $h^{1/2}$ and then shifting it by $(1-h^{1/2})A_{h}^{\\pi}$ produces distributions with $O(1)$ mean and non-mean statistics. In turn, this two-timescale approach permits ranking actions by either action values or non-mean statistics (but not both by Theorem 4.8). However, the mean estimates here are inaccurate and imprecise\u2014rather than uniformly being 100, they oscillate substantially. ", "page_idx": 8}, {"type": "text", "text": "In risk-neutral control, we are left with a number of questions. What effect do the high variance distributions in DAU/DSUP(1) have on performance? What effect do the ${\\cal O}(h^{1/2})$ means have on the performance of $\\mathrm{DSUP(^{1}/2)^{\\prime}}$ ? What effect does the instability of the mean estimates in $\\mathrm{DAU}{+}\\mathrm{DSUP(1/2)}$ have on performance? In Section 5.2, we begin to answer these questions and others by testing our superiority-based algorithms against appropriate benchmarks in an option-trading environment. ", "page_idx": 8}, {"type": "text", "text": "5.2 High-Frequency Option Trading ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The option-trading environment in which we run our comparative experiments is a commonly used benchmark (see, e.g., [22, 17]). We use an Euler\u2013Maruyama discretization scheme [23] at high resolution to simulate high-frequency trading. Returns are averaged over 10 seeds and 10 different dynamics models (corresponding to data from different stocks). Additionally, following [22], we use disjoint datasets to estimate the dynamics parameters for simulation during training and evaluation.11 ", "page_idx": 8}, {"type": "text", "text": "First, we consider the risk-neutral setting. Here we compare QR-DQN, DAU, and three algorithms based on the $q_{\\mathrm{~\\,~}}$ -rescaled superiority distribution with $q=1,{^1}/2$ : DSUP(1), $\\mathrm{DAU}{+}\\mathrm{DSUP(^{1}/2)}$ , and $\\mathrm{DSUP(^{1}/2)}$ . Figure 5.2 summarizes their performance at a sample of frequencies $\\left(\\mathrm{Hz}\\right)$ . ", "page_idx": 8}, {"type": "image", "img_path": "BRW0MKJ7Rr/tmp/18f579dced9b14ebd58d8aa6d49227e9a8603d2b1b61b26f9b12cf72937c8b27.jpg", "img_caption": ["Figure 5.2: Risk-neutral algorithms on high-frequency option-trading as a function of $\\omega$ . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "We see that $\\mathrm{DSUP(^{1}/2)}$ is not only the most consistent performer, but outperforms every competitor at all but the two lowest frequencies. Even then, its performance is very close to the best performer. We also see that $\\mathrm{DAU}{+}\\mathrm{DSUP(^{1}/2)}$ \u2019s preservation of both action gaps and $W_{p}$ action gaps does not lead to the strongest performance. In particular, its performance is inconsistent and sometimes poor. We believe this is because the tested frequencies are low enough that $\\mathrm{DSUP(^{1}/2)}$ maintains large enough action gaps to learn performant policies, but high enough that the variances of the distributions underlying $A_{h}^{\\pi}$ cause estimation difficulty. Indeed, the three methods that estimate $A_{h}^{\\pi}$ (explicitly in DAU and DA $\\mathrm{J}{+}\\mathrm{DSUP(^{1}/2)}$ or implicitly in DSUP(1)) exhibit almost identical behavior. ", "page_idx": 8}, {"type": "text", "text": "Our results highlight a dichotomy in existing (ours included) methods for value-based, high-frequency, risk-neutral control. They can either maintain $O(1)$ expected return estimates or $O(1)$ return variance estimates, but not both. We observe better performance in estimating small means from $O(1)$ variance distributions than in estimating $O(1)$ means from receipricolly large variance distributions. ", "page_idx": 8}, {"type": "text", "text": "To qualitatively illustrate the appeal of the $^1\\!/\\!2$ -rescaled superiority, Figure 5.3 presents examples of learned action-conditioned distributions used by $\\mathrm{DSUP(^{1}/2)}$ and QR-DQN agents to make decisions. ", "page_idx": 8}, {"type": "text", "text": "In this environment, action 1 taken in the start state terminates the episode, yielding the smallest return, 0, making this action inferior to its alternative action, 1. We see that $\\mathrm{DSUP(^{1}/2)}$ infers this fact. QR-DQN, on the other hand, has difficulty distinguishing these actions. This is because the $^1\\!/\\!2$ -rescaled superiority preserves $W_{p}$ action gaps, while $\\zeta_{h}^{\\pi}$ does not. ", "page_idx": 9}, {"type": "text", "text": "Second, we consider a risk-sensitive set", "page_idx": 9}, {"type": "image", "img_path": "BRW0MKJ7Rr/tmp/8def1cf4db2b96d1839d6e165a42daec3788bba80ca07d0b551b50e7de229ae6.jpg", "img_caption": ["Figure 5.3: CDFs of $\\psi_{h;1/2}^{\\pi}$ from $\\mathrm{DSUP(^{1}/2)}$ (left) and $\\zeta_{h}^{\\pi}$ from QR-DQN (right) at the start state at $\\omega=35\\mathrm{{Hz}}$ . "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "ting. Here we compare QR-DQN and $\\mathrm{DSUP(^{1}/2)}$ using $\\alpha$ -CVaR for greedy action selection. We do this because Theorem 4.8 does not hold with $\\vartheta_{h;q}^{\\pi}$ , and preserving means is less critical in risksensitive control than it is in risk-neutral control. Figure 5.4 depicts our results at $\\omega=35\\mathrm{{Hz}}$ (see Appendix D for results across a range of $\\omega$ ). ", "page_idx": 9}, {"type": "image", "img_path": "BRW0MKJ7Rr/tmp/e915949dd0d537c12361b8eff96af4772667de3bcd599807cc03318e77977837.jpg", "img_caption": ["Figure 5.4: Risk-sensitive algorithms on high-frequency option-trading at $\\omega=35\\mathrm{{Hz}}$ . "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Again, we see that $\\mathrm{DSUP(^{1}/2)}$ is conclusively the best performer. ", "page_idx": 9}, {"type": "text", "text": "6 Related Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Notions of action gap and ranking have long been of interest in RL (see, e.g., [11]). Action gaps are related to sample complexity in RL\u2014indeed, instance-dependent sample complexity rates are inversely proportional to the divergence between action-conditioned return distributions ([13, 19, 37]). Bellemare et al. [5] argue for the consideration of alternatives to the Bellman operator that explicitly devalue suboptimal actions, and they show that Baird\u2019s AL [2] operator falls within this class of operators. On the other hand, Schaul et al. [31] implicitly question Bellemare et al.\u2019s position. They demonstrate that stochastic gradient updates in deep value-based RL algorithms induce frequent changes in relative action values, which in turn is a mechanism for exploration. ", "page_idx": 9}, {"type": "text", "text": "The advantage function is commonplace in RL (see, e.g., [39, 32, 27, 35, 24]). In [24], M\u00e9snard et al. employ a distributional critic that is closely related to our (unscaled) distributional superiority. Their choice of critic stems from a desire to minimize variance. We note that the distributional superiority is a posteriori characterized as a minimal variance coupled difference representation of action-conditioned return distributions and policy-induced return distributions. ", "page_idx": 9}, {"type": "text", "text": "Lastly, DRL in continuous-time MDPs is in its infancy. There are only three works to mention. Wiltzer et al. [40, 41] give a characterization of return distributions for policy evaluation, and Halperin [14] studies algorithms for control. That said, neither work considers distributional notions of action gaps or advantages. Moreover, Halperin does not consider any of the challenges of estimating the influence of actions in high decision frequency settings. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We establish that DRL agents are sensitive to decision frequency through analysis and simulation. In experiments, $\\mathrm{DSUP(^{1}/2)}$ learns well-performing policies across a range of high decision frequencies, unlike prior approaches. DSUP(1) and $\\mathrm{DAU}{+}\\mathrm{DSUP(^{1}/2)}$ are less robust. Given our analysis, the performance of DSUP(1) is expected. Building an alternate algorithm to $\\mathrm{DAU}{+}\\mathrm{DSUP(^{1}/2)}$ that is both tailored to risk-neutral control and robust to $h$ is an important avenue for future work. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The authors are very grateful to Yunhao Tang for fruitful correspondence about distributional analogues to the advantage. Additionally, we thank Mark Rowland, Jesse Farebrother, Tyler Kastner, Pierluca D\u2019Oro, Nate Rahn, and Arnav Jain for helpful discussions. HW was supported by the Fonds de Recherche du Qu\u00e9bec and the National Sciences and Engineering Research Council of Canada (NSERC). MGB was supported by the Canada CIFAR AI Chair program and NSERC. This work was supported in part by DARPA HR0011-23-9-0050 to PS. YJ was supported by in part by NSF Grant 2243869. This research was enabled in part by support provided by Calcul Qu\u00e9bec, the Digital Research Alliance of Canada (alliancecan.ca), and the compute resources provided by Mila (mila.quebec). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Carlo Acerbi. Spectral measures of risk: A coherent representation of subjective risk aversion. Journal of Banking & Finance, 26(7):1505\u20131518, July 2002.   \n[2] L. Baird. Reinforcement Learning Through Gradient Descent. PhD thesis, May 1999.   \n[3] Marc G. Bellemare, Salvatore Candido, Pablo Samuel Castro, Jun Gong, Marlos C. Machado, Subhodeep Moitra, Sameera S. Ponda, and Ziyu Wang. Autonomous navigation of stratospheric balloons using reinforcement learning. Nature, 588(7836):77\u201382, December 2020.   \n[4] Marc G. Bellemare, Will Dabney, and Mark Rowland. Distributional Reinforcement Learning. The MIT Press, May 2023.   \n[5] Marc G Bellemare, Georg Ostrovski, Arthur Guez, Philip Thomas, and R\u00e9mi Munos. Increasing the action gap: New operators for reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 30, 2016.   \n[6] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018.   \n[7] Gerard Brunick and Steven Shreve. Mimicking an It\u00f4 process by a solution of a stochastic differential equation. The Annals of Applied Probability, 23(4):1584\u20131628, August 2013.   \n[8] Wesley Chung, Somjit Nath, Ajin Joseph, and Martha White. Two-timescale networks for nonlinear value function approximation. In International Conference on Learning Representations, 2018.   \n[9] Will Dabney, Georg Ostrovski, David Silver, and R\u00e9mi Munos. Implicit quantile networks for distributional reinforcement learning. In International Conference on Machine Learning, pages 1096\u20131105. PMLR, 2018.   \n[10] Will Dabney, Mark Rowland, Marc G. Bellemare, and R\u00e9mi Munos. Distributional Reinforcement Learning with Quantile Regression. In AAAI, 2017.   \n[11] Amir-massoud Farahmand. Action-gap phenomenon in reinforcement learning. Advances in Neural Information Processing Systems, 24, 2011.   \n[12] Wendell H Fleming and Halil Mete Soner. Controlled Markov processes and viscosity solutions, volume 25. Springer Science & Business Media, 2006.   \n[13] Todd L. Graves and Tze Leung Lai. Asymptotically Efficient Adaptive Choice of Control Laws in Controlled Markov Chains. SIAM Journal on Control and Optimization, 35(3):715\u2013743, May 1997.   \n[14] Igor Halperin. Distributional offilne continuous-time reinforcement learning with neural physicsinformed pdes (sciphy rl for doctr-l). Neural Computing and Applications, 36(9):4643\u20134659, 2024.   \n[15] Yanwei Jia and Xun Yu Zhou. Policy Gradient and Actor-Critic Learning in Continuous Time and Space: Theory and Algorithms. Journal of Machine Learning Research, 23(275):1\u201350, 2022.   \n[16] Yanwei Jia and Xun Yu Zhou. q-learning in continuous time. Journal of Machine Learning Research, 24(161):1\u201361, 2023.   \n[17] Tyler Kastner, Murat A Erdogdu, and Amir-massoud Farahmand. Distributional Model Equivalence for Risk-Sensitive Reinforcement Learning. In Advances in Neural Information Processing Systems (NeurIPS), 2023.   \n[18] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), 2014.   \n[19] Tor Lattimore and Csaba Szepesv\u00e1ri. Bandit algorithms. Cambridge University Press, 2020.   \n[20] Leemon C. Baird. Advantage Updating. Technical report, Defense Technical Information Center, Fort Belvoir, VA, November 1993.   \n[21] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In International Conference on Learning Representations (ICLR), 2016.   \n[22] Shiau Hong Lim and Ilyas Malik. Distributional Reinforcement Learning for Risk-Sensitive Policies. In NeurIPS, October 2022.   \n[23] Gisiro Maruyama. Continuous markov processes and stochastic equations. Rendiconti del Circolo Matematico di Palermo, 4:48\u201390, 1955.   \n[24] Thomas Mesnard, Wenqi Chen, Alaa Saade, Yunhao Tang, Mark Rowland, Theophane Weber, Clare Lyle, Audrunas Gruslys, Michal Valko, Will Dabney, et al. Quantile credit assignment. In International Conference on Machine Learning, pages 24517\u201324531. PMLR, 2023.   \n[25] R\u00e9mi Munos and Paul Bourgine. Reinforcement Learning for Continuous Stochastic Control Problems. In Advances in Neural Information Processing Systems (NeurIPS), 1997.   \n[26] Bernt Oksendal. Stochastic differential equations: an introduction with applications. Springer Science & Business Media, 2013.   \n[27] Hsiao-Ru Pan, Nico G\u00fcrtler, Alexander Neitz, and Bernhard Sch\u00f6lkopf. Direct Advantage Estimation. In Advances in Neural Information Processing Systems (NeurIPS), 2022.   \n[28] Simon Ramstedt and Christopher J. Pal. Real-time reinforcement learning. In Advances in Neural Information Processing Systems, 2019.   \n[29] R Tyrrell Rockafellar and Stanislav Uryasev. Conditional value-at-risk for general loss distributions. Journal of Banking & Finance, 26(7):1443\u20131471, 2002.   \n[30] Mark Rowland, R\u00e9mi Munos, Mohammad Gheshlaghi Azar, Yunhao Tang, Georg Ostrovski, Anna Harutyunyan, Karl Tuyls, Marc G. Bellemare, and Will Dabney. An Analysis of Quantile Temporal-Difference Learning. Journal of Machine Learning Research (JMLR), 25:1\u201347, 2023.   \n[31] Tom Schaul, Andr\u00e9 Barreto, John Quan, and Georg Ostrovski. The phenomenon of policy churn. Advances in Neural Information Processing Systems, 35:2537\u20132549, 2022.   \n[32] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal Policy Optimization Algorithms. CoRR, abs/1707.06347, 2017.   \n[33] Daniel W. Stroock and S. R. Srinivasa Varadhan. Multidimensional Diffusion Processes. Springer, 2006.   \n[34] Corentin Tallec, L\u00e9onard Blier, and Yann Ollivier. Making Deep Q-learning methods robust to time discretization. In International Conference on Machine Learning (ICML), 2019.   \n[35] Yunhao Tang, R\u00e9mi Munos, Mark Rowland, and Michal Valko. VA-learning as a more efficient alternative to Q-learning. In International Conference on Machine Learning (ICML), 2023.   \n[36] C\u00e9dric Villani et al. Optimal transport: old and new, volume 338. Springer, 2009.   \n[37] Andrew J. Wagenmaker and Dylan J. Foster. Instance-Optimality in Interactive Decision Making: Toward a Non-Asymptotic Theory. In Proceedings of Thirty Sixth Conference on Learning Theory, pages 1322\u20131472. PMLR, July 2023.   \n[38] Haoran Wang, Thaleia Zariphopoulou, and Xun Yu Zhou. Reinforcement learning in continuous time and space: A stochastic control approach. Journal of Machine Learning Research, 21(198):1\u201334, 2020.   \n[39] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, and Nando de Freitas. Dueling Network Architectures for Deep Reinforcement Learning. In International Conference on Machine Learning (ICML), 2016.   \n[40] Harley Wiltzer. On the Evolution of Return Distributions in Continuous-Time Reinforcement Learning. McGill University (Canada), 2021.   \n[41] Harley Wiltzer, David Meger, and Marc G. Bellemare. Distributional Hamilton-Jacobi-Bellman Equations for Continuous-Time Reinforcement Learning. In International Conference on Machine Learning (ICML), 2022.   \n[42] Hanyang Zhao, Wenpin Tang, and David Yao. Policy optimization for continuous reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Formalism of Continuous-Time RL Controlled Markov Processes ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Expected-value RL is a data-driven approach to solving the (classic) optimal control problem: find an action (control) process $(A_{s})_{s\\geq t}$ and an associated state process (then determined by the environment) $(X_{s})_{s\\geq t}$ with $X_{t}=x$ , for a given $t\\geq0$ , that maximize the expected return earned by following the state-action process $(X_{s},A_{s})_{s>t}$ . In particular, RL agents search the space of state-action processes via policies $\\pi:{\\mathsf{T}}\\times\\mathsf{X}\\to{\\mathcal{P}}(\\mathsf{A})$ . Policies prescribe the conditional probabilities of the laws of state-action processes. Indeed, $(X_{s},A_{s})_{s\\geq t}$ is the state-action process of an agent following $\\pi$ if and only if, for each $s\\geq t$ , the set $\\{\\pi(\\cdot\\,|\\,s,x)\\}_{x\\in\\mathsf{X}}$ is the set of conditional probabilities of $\\operatorname{law}\\bigl(\\bar{\\big(}X_{s},A_{s}\\big)\\bigr)$ with respect to $\\operatorname{law}(X_{s})$ . ", "page_idx": 13}, {"type": "text", "text": "Continuous-time RL is a data-driven approach to stochastic optimal control. Whence, environmental dynamics are assumed to arise from an action-parameterized family of SDEs determined by a drift $b:{\\mathsf{T}}\\times\\mathsf{X}\\times{\\mathsf{A}}\\to\\mathbb{R}^{n}$ and diffusion $\\sigma:{\\mathsf{T}}\\times{\\mathsf{X}}\\times{\\mathsf{A}}\\to\\mathbb{R}^{n\\times n}$ .12 Thus, the goal of expected-value RL (and stochastic optimal control) is to find an expected-return maximizing state-action process among state-action processes $(X_{s},A_{s})_{s\\geq t}$ that satisfy ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathrm{d}X_{s}=b(s,X_{s},A_{s})\\,\\mathrm{d}s+\\sigma(s,X_{s},A_{s})\\,\\mathrm{d}B_{s}\\quad\\mathrm{with}\\quad X_{t}=x.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We note that the MDPs defined in Section 2 have equivalent formulations in terms of transition kernel, exactly as they formulated in discrete-time RL. We refer the reader to [33] for an in-depth discussion regarding this fact. ", "page_idx": 13}, {"type": "text", "text": "A.1 Justification of Random Returns ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Given the above formalism, the \u201ctrue\u201d distribution of returns of an agent following a policy $\\pi$ is the law of ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\int_{t}^{T}\\gamma^{s-t}r(s,X_{s})\\,\\mathrm{d}s+\\gamma^{T-t}f(X_{T})\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $(X_{s},A_{s})_{s\\geq t}$ is a state-action process associated to $\\pi$ and solves (A.1). That said, by the definition of $\\pi$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{b^{\\pi}(s,X_{s})={\\mathbf E}[b(s,X_{s},A_{s})\\mid X_{s}]\\quad\\mathrm{and}\\quad\\sigma^{\\pi}(\\sigma^{\\pi})^{\\top}(s,X_{s})={\\mathbf E}[\\sigma\\sigma^{\\top}(s,X_{s},A_{s})\\mid X_{s}],}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $b^{\\pi}$ and $\\sigma^{\\pi}$ are exactly the coefficients defined in (2.3). Hence, by [7], provided that $b^{\\pi}$ and $\\sigma^{\\pi}$ are regular enough to guarantee that (2.2) is well-posed in law, the processes $X^{\\pi}=(X_{s}^{\\pi})_{s\\geq t}$ and $X=(X_{s})_{s\\geq t}$ are equal in law.13 Here $(X_{s}^{\\pi})_{s\\geq t}$ satisfies (2.2) with $X_{t}^{\\pi}=x$ . Consequently, ", "page_idx": 13}, {"type": "equation", "text": "$$\nG^{\\pi}(t,x)\\mathop{=}_{\\mathrm{law}}\\int_{t}^{T}\\gamma^{s-t}r(s,X_{s})\\,\\mathrm{d}s+\\gamma^{T-t}f(X_{T}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "This, formally, justifies $G^{\\pi}$ and $Z_{h}^{\\pi}$ , as defined in (2.4) and (2.5). ", "page_idx": 13}, {"type": "text", "text": "A.2 On Assumption 2.3 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Here we provide some conditions under which Assumption 2.3 is established. These conditions are presented as additional assumptions. Assumptions A.1 and A.2 are common in stochastic control theory (see, e.g., [12]) and SDE theory in general (see, e.g., [26]). Assumption A.3 is ubiquitous in the continuous-time RL literature [15, 16, 42]. Notably, these conditions together guarantee the existence of transition probabilities for policy-induced state processes arising from (2.2). ", "page_idx": 13}, {"type": "text", "text": "Assumption A.1. The coefficients $b$ and $\\sigma$ are uniformly bounded: a finite, positive constant $C_{A,1}$ exists such that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t,x,a}|b(t,x,a)|+\\operatorname*{sup}_{t,x,a}|\\sigma(t,x,a)|\\leq C_{A.1}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Assumption A.2. The matrix $\\sigma\\sigma^{\\top}$ is uniformly elliptic: a positive, finite constant $\\lambda_{A,2}$ exists such that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{t,x,a}|{\\operatorname*{inf}_{v}}\\;v^{\\top}\\sigma\\sigma^{\\top}(t,x,a)v\\geq\\lambda_{A.2}^{2}I.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "A consequence of Assumption A.2 is ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{t,x}\\ \\operatorname*{inf}_{|v|=1}v^{\\top}\\sigma^{\\pi}(t,x)v\\geq\\lambda_{A.2}I.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In other words, $\\sigma^{\\pi}$ is also uniformly elliptic. ", "page_idx": 14}, {"type": "text", "text": "Assumption A.3. A finite, positive constant $C_{A.3}$ exists for which ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t}\\operatorname{TV}(\\pi(\\cdot\\mid t,x),\\pi(\\cdot\\mid t,y))\\leq C_{A.3}|x-y|\\quad\\forall x,y\\in\\mathsf{X},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where TV is the total variation metric on $\\mathcal{P}(\\mathsf{A})$ . ", "page_idx": 14}, {"type": "text", "text": "Observe if $\\pi$ satisfies Assumption A.3, then $\\pi|_{h,a,t}$ also satisfies Assumption A.3. Indeed, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{TV}(\\pi|_{h,a,t}(\\cdot\\,|\\,s,x),\\pi|_{h,a,t}(\\cdot\\,|\\,s,y))\\leq\\mathrm{TV}(\\pi(\\cdot\\,|\\,s,x),\\pi(\\cdot\\,|\\,s,y))\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "since ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{s\\in[t,t+h)}\\mathrm{TV}(\\pi|_{h,a,t}(\\cdot\\,|\\,s,x),\\pi|_{h,a,t}(\\cdot\\,|\\,s,y))=0\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and $\\pi|_{h,a,t}(\\cdot\\,|\\,s,x)=\\pi(\\cdot\\,|\\,s,x)$ for all $s\\in\\mathsf{T}\\setminus[t,t+h)$ . ", "page_idx": 14}, {"type": "text", "text": "Proposition A.4. If Assumptions 2.2, A.1, and A.2 hold and $\\pi$ satisfies Assumption A.3, then Assumption 2.3 holds. ", "page_idx": 14}, {"type": "text", "text": "Proof. Observe that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|b^{\\pi}(t,x)-b^{\\pi}(t,y)|\\leq\\left|\\int(b(t,x,a)-b(t,y,a))\\,\\pi(\\mathrm{d}a\\,|\\,t,x)\\right|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\left|\\int b(t,y,a)\\,\\pi(\\mathrm{d}a\\,|\\,t,x)-\\int b(t,y,a)\\,\\pi(\\mathrm{d}a\\,|\\,t,y)\\right|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\left(C_{2,2}+2C_{A,1}C_{A,3}\\right)|x-y|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "by Assumptions 2.2, A.1, and A.3. Here we have also used Kantorovich duality to computed TV and invoke Assumption A.3. ", "page_idx": 14}, {"type": "text", "text": "Let $\\lambda$ be an eigenvalue of $\\sigma^{\\pi}(t,x)-\\sigma^{\\pi}(t,y)$ with with unit eigenvector $v$ . Observe that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{v^{\\top}(\\sigma^{\\pi}(t,x)^{2}-\\sigma^{\\pi}(t,y)^{2})v=v^{\\top}(\\sigma^{\\pi}(t,x)-\\sigma^{\\pi}(t,y))\\sigma^{\\pi}(t,x)+\\sigma^{\\pi}(t,y)(\\sigma^{\\pi}(t,x)-\\sigma^{\\pi}(t,y))v}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\lambda v^{\\top}(\\sigma^{\\pi}(t,x)+\\sigma^{\\pi}(t,x))v}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Hence, by (A.2), ", "page_idx": 14}, {"type": "equation", "text": "$$\n|\\lambda|=\\frac{|v^{\\top}(\\sigma^{\\pi}(t,x)^{2}-\\sigma^{\\pi}(t,y)^{2})v|}{v^{\\top}(\\sigma^{\\pi}(t,x)+\\sigma^{\\pi}(t,x))v}\\leq\\frac{1}{2\\lambda_{A.2}}|v^{\\top}(\\sigma^{\\pi}(t,x)^{2}-\\sigma^{\\pi}(t,y)^{2})v|.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In turn, by Assumptions 2.2, A.1, and A.3, as done to prove that $b^{\\pi}$ was Lipschitz above, ", "page_idx": 14}, {"type": "equation", "text": "$$\n|\\lambda|\\leq\\frac{1}{\\lambda_{A.2}}(C_{2.2}+C_{A.1}C_{A.3})|x-y|.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Assumption 2.3 follows, since $\\lambda$ was an arbitrary eigenvalue and all norms on finite dimensional spaces are equivalent. ", "page_idx": 14}, {"type": "text", "text": "We conclude this section with one final fact: under Assumption 2.2, the policy-averaged coefficient (2.3) have linear growth. Indeed, ", "page_idx": 14}, {"type": "equation", "text": "$$\n|b^{\\pi}(t,x)|\\leq\\int|b(t,x,a)|\\,\\pi(\\mathrm{d}a\\,|\\,t,x)\\leq C_{2.2}(1+|x|)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and ", "page_idx": 14}, {"type": "equation", "text": "$$\n|\\sigma^{\\pi}(t,x)|^{2}\\leq\\int|\\sigma(t,x,a)|^{2}\\,\\pi(\\mathrm{d}a\\,|\\,t,x)\\leq C_{2.2}^{2}(1+|x|)^{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "A.3 Action-Independent Rewards ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this paper, we assume that the rewards do not depend on actions. This is a theoretical limitation of not just our work, but continuous-time DRL in general (see [41, 14]). In the following sections, we discuss the nature of this theoretical limitation. However, many MDPs have action-independent reward functions. For example, MDPs encoding goal-reaching problems, tracking problems, and commodity-trading problems all have action-independent rewards. ", "page_idx": 15}, {"type": "text", "text": "A.3.1 Continuous-Time, Expected Return ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In continuous-time, expected-value RL, when the reward function $r$ depends on actions, the standard approach to analysis involves considering the averaged reward function $r^{\\pi}:\\mathsf{T}\\times\\mathsf{X}\\to\\mathbb{R}$ given by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\boldsymbol{r}^{\\pi}(t,\\boldsymbol{x}):=\\int_{\\mathsf{A}}\\boldsymbol{r}(t,\\boldsymbol{x},a)\\,\\pi(\\mathrm{d}a\\,|\\,t,\\boldsymbol{x}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "In continuous-time RL specifically, the averaged reward $r^{\\pi}$ is justified exactly as the coefficients $b^{\\pi}$ and $\\sigma^{\\pi}$ are justified. However, the \u201ctrue\u201d return distribution is not equal to the law of ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\int_{t}^{T}\\gamma^{s-t}r^{\\pi}(s,X_{s}^{\\pi})\\,\\mathrm{d}s+\\gamma^{T-t}f(X_{T}^{\\pi}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "To see this, it suffices to consider an MDP with a single state $x$ . In this case, the expression in (A.3) is deterministic. However, if $\\pi$ is nondeterministic and $r$ is dependent on actions, then the \u201ctrue\u201d return distribution is nondeterministic. Hence, the law of the expression in (A.3) cannot be the \u201ctrue\u201d return distribution associated to $\\pi$ . ", "page_idx": 15}, {"type": "text", "text": "A.3.2 Discrete-Time, Random Return ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In discrete-time RL, the distribution of returns given an action-dependent reward is analyzed through the state-action process induced by a policy. This processes is defined by extending the actionparameterized family of transition probability kernels on $\\mathsf{X}$ , which define the dynamics of a given MDP, to a single transition probability kernel on $\\mathsf{X}\\times\\mathsf{A}$ . In the time-homogeneous setting, for instance, with transition kernels $\\{P(\\mathrm{d}y\\,|\\,x,a)\\}_{a\\in\\mathsf{A}}$ , this amounts to constructing ", "page_idx": 15}, {"type": "equation", "text": "$$\nP^{\\pi}(\\operatorname{d}\\!y\\mathrm{d}b\\!\\mid\\!x,a):=\\pi(\\mathrm{d}b\\!\\mid\\!y)\\otimes P(\\mathrm{d}y\\!\\mid\\!x,a),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "provided that map $y\\mapsto\\pi(E\\,|\\,y)$ is measurable for all $y\\in\\mathsf X$ . In continuous-time environments, such a constructing has yet to be discovered. ", "page_idx": 15}, {"type": "text", "text": "We note that trying to analogously extend the action-parameterized family of transition semigroups on $\\mathsf{X}$ , which define the dynamics of a given time-homogeneous MDP in continuous time, to a single transition semigroup on $\\mathsf{X}\\times\\mathsf{A}$ by defining ", "page_idx": 15}, {"type": "equation", "text": "$$\nP_{t}^{\\pi}(\\operatorname{d}\\!y\\mathrm{d}b\\,|\\,x,a):=\\pi(\\mathrm{d}b\\,|\\,y)\\otimes P_{t}(\\operatorname{d}\\!y\\,|\\,x,a),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $P_{t}(\\deg|x)$ is a transition semigroup, may fail to satisfy the Chapman\u2013Kolmogorov identity. Indeed, suppose A has two elements and $\\mathsf{X}=\\mathbb{R}$ . Let $\\pi(\\mathrm{d}a\\,|\\,x)$ be the uniform measure on A for all $x\\in\\mathsf{X}$ . If $P_{t}(\\mathrm{d}y\\,|\\,x,a_{\\delta})=\\delta_{x+t}(\\mathrm{d}y)$ and $P_{t}(\\mathrm{d}y\\,|\\,x,a_{\\mathrm{g}})=(2\\pi)^{-1/2}\\exp(-|y-x|^{2}/2t)\\,\\mathrm{d}y$ , then Chapman\u2013Kolmogorov identity fails, for example, on any tuple $(s,t,x,a_{\\delta},E\\times F)$ where $E\\subset\\mathsf X$ is open, $x+t+s\\notin E$ , and $\\pi(\\dot{F})\\not=0$ . On one hand, ", "page_idx": 15}, {"type": "equation", "text": "$$\nP_{t+s}^{\\pi}(E\\times F\\,|\\,x,a_{\\delta})=\\pi(F)\\delta_{x+t+s}(E)=0.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "On the other hand, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\int P_{s}^{\\pi}(E\\times F\\,|\\,y,a)\\,P_{t}^{\\pi}(\\mathrm{d}y\\mathrm{d}a\\,|\\,x,a_{\\delta})=\\frac{\\pi(F)}{2}(P_{s}(E\\,|\\,x+t,a_{\\mathrm{g}})+\\delta_{x+t+s}(E))>0.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "So ", "page_idx": 15}, {"type": "equation", "text": "$$\nP_{t+s}^{\\pi}(E\\times F\\,|\\,x,a_{\\delta})\\neq\\int P_{s}^{\\pi}(E\\times F\\,|\\,y,a)\\,P_{t}^{\\pi}(\\mathrm{d}y\\mathrm{d}a\\,|\\,x,a_{\\delta}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "At present, the question of how to generate a well-defined (even in law) state-action process in any continuous-time MDP framework given a stochastic policy is generally open. Of course, if $\\pi$ is deterministic, then the state-action process is $(X_{s},\\pi(s,X_{s}))_{s\\geq t}$ . If $b$ and $\\sigma$ are Lipschitz in state and action, uniformly in time, and $\\pi$ is Lipschitz in state, uniformly in time, then (A.1) with $A_{s}=\\pi(s,X_{s})$ is well-posed. ", "page_idx": 15}, {"type": "text", "text": "B Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.1 The Distributional Action Gap ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we prove the statements made in Section 3. ", "page_idx": 16}, {"type": "text", "text": "Before proving any of the statements made in Section 3, we recall an identity that relates the $W_{p}$ distance between two probability measures $\\mu$ and $\\nu$ and the absolute central $p$ th moments of the differences of random variables distributed according to $\\mu$ and $\\nu$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\nW_{p}(\\mu,\\nu)^{p}=\\operatorname*{inf}_{(X,Y)}\\{\\mathbf{E}[|X-Y|^{p}]:\\operatorname{law}(X)=\\mu{\\mathrm{~and~law}}(Y)=\\nu\\}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This identity will be used a number of times, including in the proof of Section 3\u2019s first result, which we restate here for the readers convenience. ", "page_idx": 16}, {"type": "text", "text": "Proposition 3.4. For $\\begin{array}{r}{\\imath l l\\left(t,x\\right)\\in\\mathsf{T}\\times\\mathsf{X},\\,w e\\;h a\\nu e\\;t h a t\\;\\mathsf{d i s t g a p}_{p}(\\zeta_{h}^{\\pi},t,x)\\geq\\mathsf{g a p}(Q_{h}^{\\pi},t,x).}\\end{array}$ ", "page_idx": 16}, {"type": "text", "text": "Proof. Let $(Z_{1},Z_{2})$ be any random vector with such that $\\operatorname{law}(Z_{i})=\\zeta_{h}^{\\pi}(t,x,a_{i})$ for $i=1,2$ . Then, by Jensen\u2019s inequality, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{E}[\\vert Z_{1}-Z_{2}\\vert^{p}]^{1/p}\\ge\\mathbf{E}[\\vert Z_{1}-Z_{2}\\vert]\\ge\\vert\\mathbf{E}[Z_{1}-Z_{2}]\\vert=\\vert Q_{h}^{\\pi}(t,x,a_{1})-Q_{h}^{\\pi}(t,x,a_{2})\\vert.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Hence, since $(Z_{1},Z_{2})$ was arbitrary, by (B.1), ", "page_idx": 16}, {"type": "equation", "text": "$$\nW_{p}(\\zeta_{h}^{\\pi}(t,x,a_{1}),\\zeta_{h}^{\\pi}(t,x,a_{2}))\\geq|Q_{h}^{\\pi}(t,x,a_{1})-Q_{h}^{\\pi}(t,x,a_{2})|.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Finally, taking the minimum over pairs of actions $(a_{1},a_{2})$ such that $a_{1}\\neq a_{2}$ concludes the proof. ", "page_idx": 16}, {"type": "text", "text": "Now we move on to the proofs of Theorems 3.5 and 3.7. We defer the proof of Theorem 3.6 until after the proof of Theorem 3.7 as the proofs of Theorems 3.5 and 3.7 are similar. For clarity\u2019s sake, we first prove a collection of lemmas. ", "page_idx": 16}, {"type": "text", "text": "Lemma B.1. Under Assumption 2.2, let $(X_{s}^{a})_{s\\geq t}$ be the unique strong solution to (2.2) with $X_{t}^{a}=x$ $\\mathbf{P}$ -a.s. Then, for all $q\\geq1$ and for all $s\\geq t$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\bf E}[\\vert X_{s}^{a}-x\\vert^{2q}]\\le C_{q}C_{2,2}^{2q}(1+\\vert x\\vert)^{2q}((s-t)^{q}+1)(s-t)^{q}e^{C_{q}C_{2,2}^{2q}((s-t)^{q}+1)(s-t)^{q}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where Cq = 42q\u22121. ", "page_idx": 16}, {"type": "text", "text": "Proof. Let $C_{q}=4^{2q-1}$ . By Jensen\u2019s inequality and It\u00f4\u2019s isometry, observe that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|X_{s}^{a}-x|^{2q}\\leq C_{q}(s-t)^{2q-1}\\int_{t}^{s}|b(s^{\\prime},x,a)|^{2q}\\,\\mathrm{d}s^{\\prime}+C_{q}(s-t)^{q-1}\\int_{t}^{s}|\\sigma(s^{\\prime},x,a)|^{2q}\\,\\mathrm{d}s^{\\prime}}\\\\ &{\\qquad\\qquad\\qquad+C_{q}(s-t)^{2q-1}\\int_{t}^{s}|b(s^{\\prime},X_{s^{\\prime}}^{a},a)-b(s^{\\prime},x,a)|^{2q}\\,\\mathrm{d}s^{\\prime}}\\\\ &{\\qquad\\qquad\\qquad+C_{q}(s-t)^{q-1}\\int_{t}^{s}|\\sigma(s^{\\prime},X_{s^{\\prime}}^{a},a)-\\sigma(s^{\\prime},x,a)|^{2q}\\,\\mathrm{d}s^{\\prime}}\\\\ &{\\qquad\\leq C_{q}((s-t)^{2q}+(s-t)^{q})C_{2,2}^{2q}(1+|x|)^{2q}}\\\\ &{\\qquad\\qquad\\qquad+C_{q}((s-t)^{2q-1}+(s-t)^{q-1})C_{2,2}^{2q}\\int_{t}^{s}|X_{s^{\\prime}}^{a}-x|^{2q}\\,\\mathrm{d}s^{\\prime}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus, the lemma follows after taking expectation and applying Gronwall\u2019s inequality. ", "page_idx": 16}, {"type": "text", "text": "Lemma B.2. Under Assumptions 2.2 and 2.3, let $(X_{s}^{\\bullet})_{s\\geq t}$ with $\\bullet\\in\\{\\pi,\\pi|_{h,a,t}\\}$ be the unique strong solution to (2.2) with $X_{t}^{\\bullet}=x$ $\\mathbf{P}$ -a.s. Then, for all $s\\leq t+h,$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{E}[|X_{s}^{\\pi|_{h,a,t}}-X_{s}^{\\pi}|^{2q}]\\le C(1+|x|)^{2q}((s-t)^{q}+1)(s-t)^{q}e^{C((s-t)^{q}+1)(s-t)^{q}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $C$ is some finite positive constant depending on $q$ , $C_{2,2}$ , and $C_{2.3}$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. Let $C_{q}\\,=\\,8^{2q-1}$ . Note that $X_{s^{\\prime}}^{\\pi|_{h,a,t}}=X_{s^{\\prime}}^{a}$ $\\mathbf{P}$ -a.s. for all $s^{\\prime}\\leq t+h$ , by the definition of $\\pi|_{h,a,t}$ and the uniqueness of strong solutions to (2.1). So, by Jensen\u2019s inequality and It\u00f4\u2019s isometry, observe that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{|X_{s}^{\\pi|_{h,a,t}}-X_{s}^{\\pi}|^{2q}\\le C_{q}((s-t)^{2q-1}+(s-t)^{q-1})\\bigg(\\displaystyle\\int_{t}^{s}(C_{2,2}^{2q}+C_{2,3}^{2q})|X_{s^{\\prime}}^{a}-x|^{2q}\\,\\mathrm{d}s^{\\prime}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\displaystyle\\int_{t}^{s}(C_{2,2}^{2q}+C_{2,3}^{2q})(1+|x|)^{2q}\\,\\mathrm{d}s^{\\prime}}\\\\ &{\\qquad\\qquad\\qquad+\\displaystyle\\int_{t}^{s}C_{2,3}^{2q}|X_{s^{\\prime}}^{\\pi|_{h,a,t}}-X_{s^{\\prime}}^{\\pi}|^{2q}\\,\\mathrm{d}s^{\\prime}\\bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus, after taking expectation, we deduce that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{E}[|X_{s}^{\\pi|_{h,a,t}}-X_{s}^{\\pi}|^{2}]\\leq C_{q}((s-t)^{2q}+(s-t)^{q})\\bigg((C_{2,2}^{2q}+C_{2,3}^{2q})\\underset{s^{\\prime}\\in[s,t]}{\\operatorname*{max}}\\,\\mathbf{E}[|X_{s^{\\prime}}^{\\pi}-x|^{2q}]\\|}\\\\ &{\\phantom{A_{s}^{\\pi}[|X_{s}^{\\pi}]=\\sum_{s}\\in[0,2]}+(C_{2,2}^{2q}+C_{2,3}^{2q})(1+|x|)^{2q}\\bigg)}\\\\ &{\\phantom{A_{s}^{\\pi}[|X_{s}^{\\pi}]=\\sum_{q}((s-t)^{2q-1}+(s-t)^{q-1})C_{2,3}^{2q}\\int_{t}^{s}\\mathbf{E}[|X_{s^{\\prime}}^{\\pi|_{h,a,t}}-X_{s^{\\prime}}^{\\pi}|^{2q}]\\,\\mathrm{d}s^{\\prime}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "And so, the lemma follows after applying Lemma B.1 and Gronwall\u2019s inequality. ", "page_idx": 17}, {"type": "text", "text": "Lemma B.3. Under Assumptions 2.2 and 2.3, let $(X_{s}^{\\bullet})_{s\\geq t}$ with $\\bullet\\in\\{\\pi,\\pi|_{h,a,t}\\}$ be the unique strong solution to (2.2) with $X_{t}^{\\bullet}=x$ $\\mathbf{P}$ -a.s. Then, for all $s>t+h_{}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\bf E}[|X_{s}^{\\pi|h,a,t}-X_{s}^{\\pi}|^{2q}]\\le C(1+|x|)^{2q}(h^{q}+1)h^{q}e^{C((s-t-h)^{q}+1)(s-t-h)^{q}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $C$ is some finite positive constant depending on $q$ , $C_{2,2}$ , and $C_{2.3}$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. Let $C_{q}=3^{2q-1}$ . Observe that ", "page_idx": 17}, {"type": "equation", "text": "$$\nX_{s}^{\\bullet}=X_{t+h}^{\\bullet}+\\int_{t+h}^{s}b^{\\bullet}(s^{\\prime},X_{s^{\\prime}}^{\\bullet})\\,\\mathrm{d}s^{\\prime}+\\int_{t+h}^{s}\\sigma^{\\bullet}(s^{\\prime},X_{s^{\\prime}}^{\\bullet})\\,\\mathrm{d}B_{s^{\\prime}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "So, as $\\pi|_{h,a,t}(\\cdot\\,|\\,s^{\\prime},y)=\\pi(\\cdot\\,|\\,s^{\\prime},y)$ for all $(s^{\\prime},y)\\in\\mathsf{T}\\setminus[t,t+h)\\times\\mathsf{X}$ , by Jensen\u2019s inequality and It\u00f4\u2019s isometry, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|X_{s}^{\\pi|_{h,a,t}}-X_{s}^{\\pi}|^{2q}\\leq C_{q}|X_{t+h}^{\\pi|_{h,a,t}}-X_{t+h}^{\\pi}|^{2q}+C_{q}\\Big((s-t-h)^{2q-1}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\left(s-t-h\\right)^{q-1}\\Big)C_{2.3}^{2q}\\displaystyle\\int_{t+h}^{s}|X_{s^{\\prime}}^{\\pi|_{h,a,t}}-X_{s^{\\prime}}^{\\pi}|^{2q}\\,\\mathrm{d}s^{\\prime}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus, after taking expectation, applying Gronwall\u2019s inequality, and considering Lemma B.2, the lemma follows. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "One consequence of Lemmas B.2 and B.3 is ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{E}[|X_{s}^{\\pi|_{h,a,t}}-X_{s}^{\\pi}|^{p}]\\to0\\quad\\mathrm{as}\\quad h\\downarrow0,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for all $s\\in\\mathsf{T}$ and for all $p\\in[1,\\infty)$ . And so, if $f$ is bounded, then $f(X_{T}^{\\pi|_{h,a,t}})-f(X_{T}^{\\pi})$ is bounded and converges to zero $\\mathbf{P}$ -a.s. as $h$ converges to zero. The dominated convergence theorem implies that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{E}[|(f(X_{T}^{\\pi|h,a,t})-f(X_{T}^{\\pi}))|^{p}]\\to0\\quad\\mathrm{as}\\quad h\\downarrow0.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Similarly, we see that the functions $g_{h}:\\mathsf{T}\\to[0,\\infty)$ defined by ", "page_idx": 17}, {"type": "equation", "text": "$$\ng_{h}(s):=\\mathbf{E}[|r(s,X_{s}^{\\pi|_{h,a,t}})-r(s,X_{s}^{\\pi})|^{p}]\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "are uniformly bounded (in $h$ ) and converge to zero as $h\\downarrow0$ for every $s\\in\\mathsf{T}$ . Hence, by the dominated convergence theorem, again, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\int_{t}^{T}\\mathbf{E}[|r(s,X_{s}^{\\pi|_{h,a,t}})-r(s,X_{s}^{\\pi})|^{p}]\\,\\Gamma(\\mathrm{d}s)\\to0\\quad\\mathrm{as}\\quad h\\downarrow0\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\Gamma(\\mathrm{d}s):=(\\gamma^{s-t}/C_{T,t,\\gamma})\\,\\mathrm{d}$ s for $C_{T,t,\\gamma}:=(\\gamma^{T-t}-1)/\\log\\gamma$ . We now prove Theorem 3.5. ", "page_idx": 17}, {"type": "text", "text": "Theorem 3.5. If r and $f$ are bounded, then $\\begin{array}{r}{\\operatorname*{lim}_{h\\downarrow0}W_{p}(\\zeta_{h}^{\\pi}(t,x,a),\\eta^{\\pi}(t,x))=0,}\\end{array}$ , for all $(t,x,a)\\in$ ${\\mathsf{T}}\\times\\mathsf{X}\\times{\\mathsf{A}}_{}$ ; hence, $\\operatorname*{lim}_{h\\downarrow0}$ distg $\\mathsf{\\Delta}_{|\\mathsf{P}_{p}}(\\zeta_{h}^{\\pi},t,x)=0$ . ", "page_idx": 18}, {"type": "text", "text": "Proof. Observe that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathtt{d i s t g a p}_{p}(\\zeta_{h}^{\\pi},t,x)=\\underset{a_{1}\\neq a_{2}}{\\operatorname*{min}}W_{p}(\\zeta_{h}^{\\pi}(t,x,a_{1}),\\zeta_{h}^{\\pi}(t,x,a_{1}))}\\\\ &{\\qquad\\qquad\\qquad\\leq\\underset{a_{1}\\neq a_{2}}{\\operatorname*{min}}W_{p}(\\zeta_{h}^{\\pi}(t,x,a_{1}),\\eta^{\\pi}(t,x))+\\underset{a_{1}\\neq a_{2}}{\\operatorname*{min}}W_{p}(\\zeta_{h}^{\\pi}(t,x,a_{2}),\\eta^{\\pi}(t,x))}\\\\ &{\\qquad\\qquad\\leq2\\underset{a}{\\operatorname*{max}}W_{p}(\\zeta_{h}^{\\pi}(t,x,a),\\eta^{\\pi}(t,x)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus, as claimed, it suffices to show that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{W_{p}(\\zeta_{h}^{\\pi}(t,x,a),\\eta^{\\pi}(t,x))\\to0\\quad\\mathrm{as}\\quad h\\downarrow0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for all $(t,x,a)\\in\\mathsf{T}\\times\\mathsf{X}\\times\\mathsf{A}$ . By (B.1), it suffices to show that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{E}[|Z_{h}^{\\pi}(t,x,a)-G^{\\pi}(t,x)|^{p}]\\to0\\quad\\mathrm{as}\\quad h\\downarrow0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for all $(t,x,a)\\in\\mathsf{T}\\times\\mathsf{X}\\times\\mathsf{A}$ . ", "page_idx": 18}, {"type": "text", "text": "Since $(s,\\omega)\\,\\mapsto\\,X_{s}^{\\bullet}(\\omega)$ is measurable for $\\bullet\\,\\in\\,\\{\\pi,\\pi|_{h,a,t}\\}$ , by Jensen\u2019s inequality and Fubini\u2019s theorem, we see that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{E}[|Z_{h}^{\\pi}(t,x,a)-G^{\\pi}(t,x)|^{p}]\\le2^{p-1}C_{T,t,\\gamma}^{p}\\int_{t}^{T}\\mathbf{E}[|r(s,X_{s}^{\\pi|_{h,a,t}})-r(s,X_{s}^{\\pi})|^{p}]\\,\\Gamma(\\mathrm{d}s)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\,2^{p-1}\\gamma^{p(T-t)}\\mathbf{E}[|(f(X_{T}^{\\pi|_{h,a,t}})-f(X_{T}^{\\pi}))|^{p}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In turn, (B.4) follows from (B.2) and (B.3). ", "page_idx": 18}, {"type": "text", "text": "If $T<\\infty$ and $h<1$ , the inequalities in the statements of Lemmas B.2 and B.3 yield the following inequality: for all $p\\in[1,\\infty)$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbf{E}[|X_{s}^{\\pi|_{h,a,t}}-X_{s}^{\\pi}|^{p}]\\leq C_{(\\mathrm{B}.6)}h^{p/2}\\quad\\forall s\\in\\mathsf{T},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for some finite positive constant $C_{(\\mathrm{B}.6)}$ depending on $p,|x|,t,T,C_{2.2}$ , and $C_{2.3}$ . With this inequality in hand, we now restate and prove Theorem 3.7. ", "page_idx": 18}, {"type": "text", "text": "Theorem 3.7. If $r$ is Lipschitz in state, uniformly in time, $f$ is Lipschitz, and $T\\ <\\ \\infty$ , then $W_{p}(\\zeta_{h}^{\\pi}(t,x,a),\\mathring{\\eta}^{\\pi}(t,x))\\stackrel{\\cdot}{\\lesssim}h^{1/2}$ , for all $(t,x,a)\\in\\mathsf{T}\\times\\mathsf{X}\\times\\mathsf{A},$ ; hence, distg $\\mathsf{I p}_{p}(\\zeta_{h}^{\\pi},t,x)\\lesssim h^{1/2}$ . ", "page_idx": 18}, {"type": "text", "text": "Proof. Arguing as in the proof of Theorem 3.5, we see that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbf{E}[|Z_{h}^{\\pi}(t,x,a)-G^{\\pi}(t,x)|^{p}]\\leq2^{p-1}C_{T,t,\\gamma}^{p}\\mathrm{I}+2^{p-1}\\gamma^{p(T-t)}\\mathrm{II}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "with ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbf{I}:=\\int_{t}^{T}\\mathbf{E}[|r(s,X_{s}^{\\pi|h,a,t})-r(s,X_{s}^{\\pi})|^{p}]\\,\\Gamma(\\mathrm{d}s)\\quad\\mathrm{and}\\quad\\mathrm{II}:=\\mathbf{E}[|(f(X_{T}^{\\pi|h,a,t})-f(X_{T}^{\\pi}))|^{p}].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This is (B.5). In turn, by (B.6) and that $r$ is Lipschitz in space, uniformly in time and $f$ is Lipschitz, we deduce that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{I}+\\mathrm{II}\\le C_{(\\mathrm{B.6})}(C_{r}^{p}+C_{f}^{p})h^{p/2},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "as desired, where $C_{r}$ and $C_{f}$ are the Lipschitz constants of $r$ and $f$ respectively. ", "page_idx": 18}, {"type": "text", "text": "Recall $r:\\mathsf{T}\\times\\mathsf{X}\\to\\mathbb{R}$ is Lipschitz in state, uniformly in time if a finite positive constant $C_{r}$ exists such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t}|r(t,x)-r(t,y)|\\leq C_{r}|x-y|\\quad\\forall x,y\\in\\mathsf{X}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "As promised, we now prove Theorem 3.6. ", "page_idx": 18}, {"type": "text", "text": "Theorem 3.6. MDPs and policies exist in and under which, for all $(t,x,a)\\in\\mathsf{T}\\times\\mathsf{X}\\times\\mathsf{A},$ , we have that $W_{p}(\\zeta_{h}^{\\pi}(t,x,a),\\eta^{\\pi}(t,x))\\gtrsim h^{1/2}$ and distga $\\mathsf{p}_{p}(\\zeta_{h}^{\\pi},t,x)\\gtrsim h^{1/2}$ . ", "page_idx": 18}, {"type": "text", "text": "Proof. Let $\\mathbb{X}=\\mathbb{R}$ and $\\mathsf{A}=\\{0,1\\}$ ; set $b\\equiv0$ and $\\sigma=\\mathbf{1}_{\\{a=1\\}}$ ; for all $(s,y)\\in\\mathsf{T}\\times\\mathsf{X},$ , let $r(s,y)=y$ ; and set $f\\equiv0$ . In words, our action space has two elements, and when action 1 is executed, the system follows Brownian dynamics, and otherwise, the state is fixed. Now consider the policy $\\pi$ which always selects the action 0: $\\pi(\\cdot\\,|\\;s,y)=\\delta_{0}$ , for all $(s,y)\\in\\mathsf{T}\\times\\mathsf{X}$ . ", "page_idx": 19}, {"type": "text", "text": "Case 1: $\\gamma=1$ and $T<\\infty$ . Observe that ", "page_idx": 19}, {"type": "equation", "text": "$$\nZ_{h}^{\\pi}(t,x,1)-Z_{h}^{\\pi}(t,x,0)\\mathop{=}_{\\mathrm{law}}\\int_{0}^{h}\\tilde{B}_{s}\\,\\mathrm{d}s+(T-t-h)\\tilde{B}_{h},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $(\\tilde{B}_{s})_{s\\geq0}$ is a Brownian motion. Hence, $Z_{h}^{\\pi}(t,x,1)-Z_{h}^{\\pi}(t,x,0)$ is equal in law to the sum of two zero mean Gaussian random variables with variances $\\sigma_{1}^{2}=h^{3}/3$ and $\\sigma_{2}^{2}=(T-t-h)^{2}h$ respectively. And so, it is also Gaussian, and its variance is $\\sigma_{h}^{2}\\,=\\,\\dot{\\sigma}_{1}^{2}+\\sigma_{2}^{2}+2c\\dot{\\sigma}_{1}\\sigma_{2}$ for some $-1\\le c\\le1$ . In particular, $\\sigma_{h}^{p}\\gtrsim h^{p/2}$ . Recall that central absolute $p$ th moment of a Gaussian random variable is proportional to its standard deviation to the power $p$ , for all $p\\geq1$ . Therefore, by (B.1), we deduce that distg $\\mathsf{\\Omega}_{|\\mathsf{p}_{p}}(\\zeta_{h}^{\\pi},t,x)\\gtrsim h^{1/2}$ , for $h<1$ , as desired. ", "page_idx": 19}, {"type": "text", "text": "Case 2: $T\\in[0,\\infty]$ and $\\gamma\\in(0,1)$ . Observe that ", "page_idx": 19}, {"type": "equation", "text": "$$\nZ_{h}^{\\pi}(t,x,1)-Z_{h}^{\\pi}(t,x,0)\\!=\\!_{\\mathrm{law}}N(h)+\\frac{\\gamma^{T-t}-\\gamma^{h}}{\\log\\gamma}\\tilde{B}_{h}\\quad\\mathrm{with}\\quad N(h):=\\int_{0}^{h}\\gamma^{s}\\tilde{B}_{s}\\,\\mathrm{d}s,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for some Brownian motion $(\\tilde{B}_{s})_{s\\geq0}$ . We claim that $N(h)$ is a mean zero Gaussian random variable with variance $\\sigma_{h}^{2}\\approx h^{3}/3$ . Then the concluding argument in the proof of Case 1 also concludes the proof of this case. ", "page_idx": 19}, {"type": "text", "text": "To prove our claim, first note that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{E}[N(h)]=\\int_{0}^{h}\\gamma^{s}\\mathbf{E}[\\tilde{B}_{s}]\\,\\mathrm{d}s=0\\quad\\mathrm{and}\\quad\\mathbf{E}[N(h)^{2}]=\\int_{0}^{h}\\int_{0}^{h}\\gamma^{s+s^{\\prime}}\\mathbf{E}[\\tilde{B}_{s}\\tilde{B}_{s^{\\prime}}]\\,\\mathrm{d}s\\mathrm{d}s^{\\prime}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "As ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\gamma^{2h}\\frac{h^{3}}{3}=\\gamma^{2h}\\int_{0}^{h}\\int_{0}^{h}\\operatorname*{min}\\{s,s^{\\prime}\\}\\,\\mathrm{d}s\\mathrm{d}s^{\\prime}\\leq\\mathbf{E}[N(h)^{2}]\\leq\\int_{0}^{h}\\int_{0}^{h}\\operatorname*{min}\\{s,s^{\\prime}\\}\\,\\mathrm{d}s\\mathrm{d}s^{\\prime}\\leq\\frac{h^{3}}{3},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "we see that $N(h)$ has the claimed statistics. Second, observe that $\\begin{array}{r}{N(h)=\\operatorname*{lim}_{n\\to\\infty}N_{n}(h)}\\end{array}$ where ", "page_idx": 19}, {"type": "equation", "text": "$$\nN_{n}(h):=\\operatorname*{lim}_{n\\to\\infty}\\sum_{i=0}^{n-1}\\gamma^{s_{i}}\\tilde{B}_{s_{i}}d_{h,n}\\quad\\mathrm{with}\\quad d_{h,n}:=\\frac{h}{n}\\mathrm{~and~}s_{i}:=i d_{h,n}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "(This is simply a Riemann sum approximation of the integral that defines $N(h)$ .) As the sum of any finite number of Gaussian random variables is a Gaussian random variable, $N_{n}(h)$ is Gaussian. Furthermore, as the limit of a sequence of Gaussian random variables whose sequences of means and variances converge (to finite values) is Gaussian, $N(h)$ is Gaussian, which proves our claim. ", "page_idx": 19}, {"type": "text", "text": "B.2 Distributional Superiority ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we prove the statements and claims made in Section 4. ", "page_idx": 19}, {"type": "text", "text": "First, we prove that the mean of every $\\psi_{h}^{\\pi}(t,x,a)\\in\\mathcal{D}(\\zeta_{h}^{\\pi}(t,x,a),\\eta^{\\pi}(t,x))$ is $Q_{h}^{\\pi}(t,x,a)\\!-\\!V^{\\pi}(t,x)$ . ", "page_idx": 19}, {"type": "text", "text": "Lemma B.4. If $\\psi_{h}^{\\pi}(t,x,a)\\in\\mathcal{D}(\\zeta_{h}^{\\pi}(t,x,a),\\eta^{\\pi}(t,x))$ , then its mean is $Q_{h}^{\\pi}(t,x,a)-V^{\\pi}(t,x)$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. If $\\kappa_{h}^{\\pi}(t,x,a)$ be such that $\\Delta_{\\#}\\kappa_{h}^{\\pi}(t,x,a)=\\psi_{h}^{\\pi}(t,x,a)$ , then ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\int r\\,\\psi_{h}^{\\pi}(t,x,a)(\\mathrm{d}r)=\\int(z-w)\\,\\kappa_{h}^{\\pi}(t,x,a)(\\mathrm{d}z\\mathrm{d}w)}}\\\\ &{=\\int z\\,\\zeta_{h}^{\\pi}(t,x,a)(\\mathrm{d}z)-\\int w\\,\\eta^{\\pi}(t,x)(\\mathrm{d}w)}\\\\ &{=Q_{h}^{\\pi}(t,x,a)-V^{\\pi}(t,x),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "as desired. ", "page_idx": 19}, {"type": "text", "text": "Second, we prove Theorem 4.3. ", "page_idx": 20}, {"type": "text", "text": "Theorem 4.3. Let $\\kappa\\in\\mathcal C(\\mu,\\mu)$ for some $\\mu\\in{\\mathcal{P}}(\\mathbb{R})$ . The push-forward of $\\kappa$ by $\\Delta$ is the delta at zero, $\\Delta_{\\#}\\kappa=\\delta_{0}$ , if and only if \u03ba is a $W_{p}$ -optimal coupling, for some $p\\in[1,\\infty)$ . Moreover, there is only one such coupling. It is given by $\\kappa_{\\mu}:=(\\mathrm{id},\\mathrm{id})_{\\#}\\mu$ or, equivalently, $\\kappa_{\\mu}:=(F_{\\mu}^{-1},F_{\\mu}^{-1})_{\\#}\\mathbb{1}(0,1)$ . Here $\\mathcal{U}(0,1)$ is the uniform distribution on $[0,1]$ . ", "page_idx": 20}, {"type": "text", "text": "First, we establish that there is only one $W_{p}$ optimal coupling between a given $\\mu\\in{\\mathcal{P}}(\\mathbb{R})$ and itself, for every $p\\in[1,\\infty)$ . ", "page_idx": 20}, {"type": "text", "text": "Lemma B.5. Let $\\mu\\in{\\mathcal{P}}(\\mathbb{R})$ . There is only one $W_{p}$ optimal coupling between $\\mu$ and itself, for every $p\\in[1,\\infty)$ . It is $\\kappa_{\\mu}:=(\\mathsf{i d},\\mathsf{i d})_{\\#}\\mu\\in\\mathcal{C}(\\mu,\\mu)$ . ", "page_idx": 20}, {"type": "text", "text": "Proof. Let $\\kappa\\in\\mathcal C(\\mu,\\mu)$ , and suppose there exists $\\epsilon>0$ for which ", "page_idx": 20}, {"type": "equation", "text": "$$\nc_{\\epsilon}:=\\kappa\\big(\\{|z-w|\\geq\\epsilon\\,:\\,z,w\\in\\mathbb{R}\\}\\big)>0.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\int|z-w|^{p}\\,\\kappa(\\mathrm{d}z\\mathrm{d}w)\\geq\\int_{\\{|z-w|\\geq\\epsilon\\}}|z-w|^{p}\\,\\kappa(\\mathrm{d}z\\mathrm{d}w)\\geq|\\epsilon|^{p}c_{\\epsilon}>0=\\int|z-w|^{p}\\,\\kappa_{\\mu}(\\mathrm{d}z\\mathrm{d}w).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Hence, $\\kappa$ , as considered, is not optimal. Since $\\epsilon$ was arbitrary, it follows that a $W_{p}$ optimal coupling is concentrated on $\\{z=w\\}$ . Therefore, every optimal coupling is of the form $(\\dot{\\mathrm{id}},\\dot{\\mathrm{id}})_{\\#}\\nu$ for some $\\nu\\in{\\mathcal{P}}(\\mathbb{R})$ . As the marginals of such a coupling are $\\nu$ and $\\nu$ , we deduce that $\\nu=\\mu$ , as desired. ", "page_idx": 20}, {"type": "text", "text": "Proof of Theorem 4.3. Let $\\kappa\\in\\mathcal C(\\mu,\\mu)$ be such that $\\Delta_{\\#}\\kappa=\\delta_{0}$ . Observe that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\int|z-w|^{p}\\,\\kappa(\\mathrm{d}z\\mathrm{d}w)=\\int r^{2}\\,\\delta_{0}(\\mathrm{d}r)=0=\\int|z-w|^{p}\\,\\kappa_{\\mu}(\\mathrm{d}z\\mathrm{d}w),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where, again, $\\kappa_{\\mu}\\,=\\,(\\mathrm{id},\\mathrm{id})_{\\#}\\mu$ . Hence, $\\kappa\\,=\\,\\kappa_{\\mu}$ , by Lemma B.5. On the other hand, since $\\kappa_{\\mu}$ is concentrated on $\\{z=w\\}$ , for any bounded, continuous function $g$ , we see ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\int g(r)\\,\\Delta_{\\#}\\kappa_{\\mu}(\\mathrm{d}r)=\\int g(z-w)\\,\\kappa_{\\mu}(\\mathrm{d}z\\mathrm{d}w)=\\int_{\\{z=w\\}}g(z-w)\\,\\kappa_{\\mu}(\\mathrm{d}z\\mathrm{d}w)=g(0).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In turn, $\\Delta_{\\#}\\kappa_{\\mu}=\\delta_{0}$ , as desired. ", "page_idx": 20}, {"type": "text", "text": "Remark B.6. Under the hypotheses of Theorem 3.7, we see that the $^1\\!/\\!2$ -rescaled superiority distributions at any $(t,x,a)$ for $h\\in(0,1]$ is a family of probability measures with uniformly bounded second moment. Hence, this family is tight. So, up to subsequences, these rescalings converges to limiting probability measure as $h\\downarrow0.$ . An interesting open question, is whether or not these subsequential limits are the same. ", "page_idx": 20}, {"type": "text", "text": "Third, we prove Theorems 4.5 and 4.6. The proofs of these theorems are a consequence of the following expression for the $W_{p}$ distance between $\\mu$ and $\\nu$ when $\\mu,\\nu\\in\\mathcal{P}(\\mathbb{R})$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\nW_{p}(\\mu,\\nu)^{p}=\\int_{0}^{1}|F_{\\mu}^{-1}(\\tau)-F_{\\nu}^{-1}(\\tau)|^{p}\\,\\mathrm{d}\\tau.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Theorem 4.5. MDPs and policies exist satisfying Assumptions 2.2 and 2.3 in and under which, for all $(t,x)\\in\\mathsf{T}\\times\\mathsf{X}$ , we have that distga $\\mathsf{p}_{p}(\\psi_{h;q}^{\\bar{\\pi}},t,x)\\gtrsim h^{\\bar{\\imath}/2-q}$ . ", "page_idx": 20}, {"type": "text", "text": "Proof. Recall that $F_{\\psi_{h;q}^{\\pi}(t,x,a)}^{-1}=h^{-q}(F_{\\zeta_{h}^{\\pi}(t,x,a)}^{-1}-F_{\\eta^{\\pi}(t,x)}^{-1})$ (F \u03b6\u2212h\u03c0 1(t,x,a)\u2212F \u03b7\u2212\u03c01(t,x)), for all a \u2208A. Hence, for every a1 \u0338= a2, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{W_{p}(\\psi_{h;q}^{\\pi}(t,x,a_{1}),\\psi_{h;q}^{\\pi}(t,x,a_{2}))^{p}=\\displaystyle\\int_{0}^{1}|F_{\\psi_{h;q}^{\\pi}(t,x,a_{1})}^{-1}(\\tau)-F_{\\psi_{h;q}^{\\pi}(t,x,a_{2})}^{-1}(\\tau)|^{p}\\,\\mathrm{d}\\tau}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=h^{-q p}\\int_{0}^{1}|F_{\\zeta_{h}^{\\pi}(t,x,a_{1})}^{-1}(\\tau)-F_{\\zeta_{h}^{\\pi}(t,x,a_{2})}^{-1}(\\tau)|^{p}}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad=h^{-q p}W_{p}(\\zeta_{h}^{\\pi}(t,x,a_{1}),\\zeta_{h}^{\\pi}(t,x,a_{2}))^{p}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus, taking the $p$ th root of both sides of the above equality, we deduce that ", "page_idx": 21}, {"type": "equation", "text": "$$\nW_{p}(\\psi_{h;q}^{\\pi}(t,x,a_{1}),\\psi_{h;q}^{\\pi}(t,x,a_{2}))=h^{-q}W_{p}(\\zeta_{h}^{\\pi}(t,x,a_{1}),\\zeta_{h}^{\\pi}(t,x,a_{2})).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The example presented in Theorem 3.6 is such that $W_{p}(\\zeta_{h}^{\\pi}(t,x,a_{1}),\\zeta_{h}^{\\pi}(t,x,a_{2}))\\gtrsim h^{1/2}$ . Whence, $W_{p}(\\psi_{h;q}^{\\pi}(t,x,a_{1}),\\psi_{h;q}^{\\pi}(t,x,a_{2}))\\gtrsim h^{1/2-q}$ \u53e3 ", "page_idx": 21}, {"type": "text", "text": "Theorem 4.6. Under Assumptions 2.2 and 2.3, if $r$ is Lipschitz in state, uniformly in time, $f$ is Lipschitz, and $T<\\infty,$ , then distg $\\mathsf{i}\\mathsf{p}_{p}(\\psi_{h;q}^{\\pi},t,x)\\lesssim\\bar{h}^{1/2-q}$ , for all $(t,x)\\in\\mathsf{T}\\times\\dot{\\mathsf{X}}$ . ", "page_idx": 21}, {"type": "text", "text": "The proof of Theorem 4.6 is almost identical to the proof of Theorem 4.5. ", "page_idx": 21}, {"type": "text", "text": "Proof. Arguing as in the proof of Theorem 4.5, we see that ", "page_idx": 21}, {"type": "equation", "text": "$$\nW_{p}(\\psi_{h;q}^{\\pi}(t,x,a_{1}),\\psi_{h;q}^{\\pi}(t,x,a_{2}))=h^{-q}W_{p}(\\zeta_{h}^{\\pi}(t,x,a_{1}),\\zeta_{h}^{\\pi}(t,x,a_{2})).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By Theorem 3.7, it follows that $W_{p}(\\psi_{h;q}^{\\pi}(t,x,a_{1}),\\psi_{h;q}^{\\pi}(t,x,a_{2}))\\lesssim h^{1/2-q}.$ ", "page_idx": 21}, {"type": "text", "text": "Finally, we prove Theorem 4.8 ", "page_idx": 21}, {"type": "text", "text": "Theorem 4.8. Let $\\rho_{\\beta}$ be a distortion risk measure, $q\\geq0$ , and $h>0$ . If $\\rho_{\\beta}(\\eta^{\\pi}(t,x))<\\infty,$ , then arg $\\begin{array}{r}{\\operatorname*{max}_{a\\in\\mathsf{A}}\\rho_{\\beta}(\\psi_{h;q}^{\\pi}(t,x,a))=\\arg\\operatorname*{max}_{a\\in\\mathsf{A}}\\rho_{\\beta}(\\zeta_{h}^{\\pi}(\\bar{t},x,a))}\\end{array}$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. Observe that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\rho_{\\beta}(\\psi_{h;q}^{\\pi}(t,x,a))=\\displaystyle\\int_{0}^{1}F_{\\psi_{h;q}^{\\pi}(t,x,a)}^{-1}(\\tau)\\,\\beta(\\mathrm{d}\\tau)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\int_{0}^{1}h^{-q}F_{\\psi_{h}^{\\pi}(t,x,a)}^{-1}(\\tau)\\,\\beta(\\mathrm{d}\\tau)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=h^{-q}\\displaystyle\\int_{0}^{1}(F_{\\zeta_{h}^{\\pi}(t,x,a)}^{-1}(\\tau)-F_{\\eta^{\\pi}(t,x)}^{-1}(\\tau))\\,\\beta(\\mathrm{d}\\tau)}\\\\ &{\\qquad\\qquad\\qquad=h^{-q}\\rho_{\\beta}(\\zeta_{h}^{\\pi}(t,x,a))-h^{-q}\\rho_{\\beta}(\\eta^{\\pi}(t,x))}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since $\\rho_{\\beta}(\\eta^{\\pi}(t,x))$ is independent of $a$ and $h>0$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\arg\\operatorname*{max}_{a}\\rho_{\\beta}(\\psi_{h;q}^{\\pi}(t,x,a))=\\arg\\operatorname*{max}_{a}\\rho_{\\beta}(\\zeta_{h}^{\\pi}(t,x,a)).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "C Algorithms and Pseudocode ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Here we discuss methods for policy optimization via distributional superiority. In practice, computers operate at a finite frequency\u2014as such, all policies we consider here will be assumed to apply each action for $h$ units of time, as in the settings of [34, 16]. ", "page_idx": 21}, {"type": "text", "text": "Before describing the superiority learning algorithms, we first remark on the form of exploration policies used in our approaches. We consider policies of the form ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\pi^{\\mathrm{explore}}:\\mathbb{R}^{{\\mathsf{A}}}\\times\\mathbb{R}^{n}\\to{\\mathsf{A}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The first argument to such a policy is a vector of \u201caction-values\u201d. In our case, given a superiority distribution $\\psi_{h;q}^{\\pi}(t,x,\\cdot)$ , the action values may be $(\\rho_{\\beta}(\\psi_{h;q}^{\\pi}(t,x,a)))_{a\\in\\mathsf{A}}$ for a distortion risk measure $\\rho_{\\beta}$ . This generalizes the notion of $Q$ -values for distributional learning. ", "page_idx": 21}, {"type": "text", "text": "The second argument represents a noise variable, in order to support stochastic policies. As input to our algorithms, we require a probability measure $\\mathbb{P}_{\\mathrm{act}}$ on processes $(\\epsilon_{t})_{t\\geq0}$ . This framework generalizes common exploration methods in deep RL. For example, to recover $\\epsilon_{}$ -greedy policies, $\\mathbb{P}_{\\mathrm{act}}$ represent a 2-dimensional white noise, and ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\pi^{\\mathrm{explore}}(v,\\xi)={\\binom{\\arg\\operatorname*{max}_{a}v_{a}}{a_{\\lceil\\xi_{2}\\mid\\mathsf{A}\\rceil}}}&{{\\mathrm{otherwise}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Alternatively, one might choose to correlate the action noise. Approaches such as DDPG [21] and DAU present such examples, where action noise evolves over time as an Ornstein-Uhlenbeck process. This can be implemented in the framework above by choosing $\\mathbb{P}_{\\mathrm{act}}$ as the distribution of an $|\\mathsf{A}|$ -dimensional Ornstein-Uhlenbeck process, and defining ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\pi^{\\mathrm{explore}}(v,\\xi)=\\arg\\operatorname*{max}_{a\\in\\mathsf{A}}\\left(v+\\xi\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "In our experiments, we found that $\\epsilon_{}$ -greedy exploration was sufficient for approximate policy optimization. To keep our implementation closest to the QR-DQN baseline, therefore, our implementations use the definition of $\\pi^{\\mathrm{explore}}$ from equation ", "page_idx": 22}, {"type": "text", "text": "The remainder of this section details the implementation of $\\mathrm{DSUP}(q)$ and $\\mathsf{D A U+D S U P}(q)$ as introduced in Section 4.2. Additionally, we provide source code for our implementations at https://github.com/harwiltz/distributional-superiority. ", "page_idx": 22}, {"type": "text", "text": "C.1 Distributional Superiority ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Generally, our goal is to learn a $\\rho_{\\beta}$ -greedy policy (see Definition 4.7). Since all policies apply actions for $h$ units of time, in order to satisfy Axiom 2, we want ", "page_idx": 22}, {"type": "equation", "text": "$$\nS_{h}^{\\pi}(t,x,\\pi(x))\\equiv0,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\pi:x\\mapsto\\arg\\operatorname*{max}_{a}\\rho_{\\beta}(S_{h}^{\\pi}(t,x,a))$ is the $\\rho_{\\beta}$ -greedy policy. As such, following the DAU algorithm of [34], our algorithms will model quantile functions $\\phi(\\boldsymbol{i},x,a)\\in\\mathbb{R}^{m}$ that aim to satisfy ", "page_idx": 22}, {"type": "equation", "text": "$$\nF_{\\psi_{h;q}^{\\pi}(t,x,a;h)}^{-1}\\approx\\phi(t,x,a)-\\phi(t,x,a^{\\star})\\quad\\mathrm{for~some}\\quad a^{\\star}\\in\\arg\\operatorname*{max}_{a}\\rho_{\\beta}(\\phi(t,x,a)).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Our primary algorithmic contribution integrates such a model, with proper superiority rescaling, into the QR-DQN framework of [10] for estimating action-conditioned return distributions. It is outlined in Algorithm 2. ", "page_idx": 22}, {"type": "text", "text": "To deal with the increased time-resolution of transitions, [34] modified the step size by a factor of $h$ . More recently, [3] found that subsampling transitions before storing them in the replay buffer was most effective in their high-decision-frequency domain. Thus, we opt for such a strategy here. Rather than only storing every $h^{-1}$ transitions, we randomly select transitions to store according to independent Bernoulli $(h)$ draws in order to avoid the possibility of only capturing cyclic phenomena in the replay buffer. We found that this strategy worked similarly to that of [34], but is far less computationally expensive. Likewise, as $h$ decreases, we extend the number of training interactions by a factor of $\\dot{h}^{-1}$ . This corresponds to training for a constant amount of time units across decision frequencies, and likewise, a constant number of gradient updates across decision frequencies. ", "page_idx": 22}, {"type": "text", "text": "C.2 Two-Timescale Advantage-Shifted Distributional Superiority ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "An astute reader might recognize that while $\\psi_{h;q}^{\\pi}$ may have nonzero distributional action gap, since $h^{q}$ is asymptotically larger than $h$ for $q<1$ , the works of [34, 16] would suggest that the expected action gap under $\\psi_{h;q}^{\\pi}$ should vanish. To account for this, we propose shifting the rescaled superiority quantiles by the advantage function, as estimated e.g. in DAU [34]. It is clear that such a procedure cannot cause the distributional action gap to vanish. The resulting procedure is depicted in Algorithm 3, with the modifications relative to Algorithm 2 highlighted in blue. In practice, we employ a shared feature extractor in the representations of $A_{h}$ and $\\phi$ to reap the representation learning benefits of DRL [4] when approximating the advantage. ", "page_idx": 22}, {"type": "text", "text": "C.3 Influence of the Rescaling Factor ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The algorithms 2 and 3 are parameterized by a rescaling factor $q~\\in~(0,1]$ , which is meant to compensate for the collapse of the distributional action gap. Larger values of $q$ correspond to larger compensation. In this work, we argue that the distributional action gap collapses at rate $h^{1/2}$ , leading to a natural choice of $q=1/2$ , which theoretically preserves constant order action gaps with respect to the decision frequency. We also test the approach with $q=1$ , which corresponds to the well-known scaling rate for preserving expected value action gaps. ", "page_idx": 22}, {"type": "text", "text": "For any $q>1/2$ , the distributional action gap theoretically grows without bound as $h\\downarrow0$ , leading to distributional estimates with arbitrarily large variance. On the other hand, for any $q<1/2$ , the distributional action gap decays to 0 as $h\\downarrow0$ , which makes it difficult to identify the best actions in the presence of approximation error. ", "page_idx": 22}, {"type": "text", "text": "Algorithm 2 Distributional Superiority Quantile Regression ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Require: $m$ (number of quantiles) and $q$ (rescale factor)   \nRequire: $h^{-1}$ (decision frequency), $\\mu$ (initial state distribution)   \nRequire: $\\pi^{\\mathrm{explore}}$ (exploration policy) and $\\rho_{\\beta}$ (distortion risk measure)   \nRequire: $\\mathbb{P}_{\\mathrm{act}}$ (action noise distribution)   \nRequire: $\\theta(t,x),\\phi(t,x,a)\\in\\mathbb{R}^{m}$ , parameterized quantile representations for $\\eta^{\\pi}(t,x),\\psi_{h;q}^{\\pi}(t,x,a)$ .   \nRequire: $\\overline{{\\theta}}(t,x)\\in\\mathbb{R}^{m}$ , target quantile representations for $\\eta^{\\pi}(t,x)$ .   \nRequire: $n_{\\theta}$ (target update period), $\\alpha$ (step size), $N$ (batch size) $\\Phi\\gets\\emptyset$ reset $\\leftarrow$ True for $i\\in\\mathbb N$ do // Gather data with exploratory policy $(\\xi)_{s\\geq0}\\sim\\mathbb{P}_{\\mathrm{act}}$ $\\triangleright$ Sample from stochastic process for exploration noise for $\\bar{j}\\in\\{0,\\ldots,\\lfloor h^{-1}\\rfloor\\}$ do if reset is True then $t\\leftarrow0$ and $x_{0}\\sim\\mu$ reset $\\leftarrow$ False end if $\\begin{array}{r l}&{a^{\\star}\\leftarrow\\arg\\operatorname*{max}_{a}\\rho_{\\beta}\\left(\\frac{1}{m}\\sum_{n=1}^{m}\\delta_{\\phi(t,x_{t},a)_{n}}\\right)}\\\\ &{F_{\\psi_{h;q}}^{-1}(a)\\leftarrow\\phi(t,x_{t},a)-\\phi(t,x_{t},a^{\\star})\\mathrm{~for~each~}a\\in\\mathsf{A}}\\\\ &{v_{a}\\leftarrow\\rho_{\\beta}\\left(\\frac{1}{m}\\sum_{n=1}^{m}\\delta_{F_{\\psi_{h;q}}^{-1}(a)_{n}}\\right)\\mathrm{~for~each~}a\\in\\mathsf{A}}\\end{array}$ $a_{t}\\sim\\pi^{\\mathrm{explore}}((v_{a})_{a\\in\\mathsf{A}},\\xi_{j h})$ Perform action $a_{t}$ for $h$ units of time, observe $(r_{t},x_{t+h},{\\mathsf{d o n e}}_{t+h})$ . $Y_{j}\\sim\\mathsf{B e r n o u l l i}(h)$ $\\triangleright$ Subsample transitions for replay if $Y_{j}=1\\vee$ ${\\mathsf{d o n e}}_{j+1}={\\mathrm{Tr}}_{\\mathrm{\\Delta}}$ ue then $\\mathcal{D}\\gets\\mathcal{D}\\cup\\{(\\bar{t},x_{t},a_{t},r_{t},x_{t+h},\\mathsf{d o n e}_{t+h})\\}$ end if $\\mathtt{r e s e t}\\gets\\mathsf{d o n e}_{t+h}$ and $t\\gets t+h$ end for // Update return/superiority distributions Sample minibatch $\\{(t_{k},x_{k},a_{k},r_{k},x_{k}^{\\prime},mathsf{d o n e}_{k})\\}_{k=1}^{N}$ from $\\mathcal{D}$ for $k\\in[N]$ do $\\begin{array}{r}{a_{k}^{\\star}\\leftarrow\\operatorname{arg\\,max}_{a}\\rho_{\\beta}\\left(\\frac{1}{m}\\sum_{n=1}^{m}\\delta_{\\phi(t_{k},x_{k},a)_{n}}\\right)}\\end{array}$ \u25b7Risk-sensitive greedy action $\\begin{array}{r l}&{\\ddot{F_{\\zeta}}^{-1}(\\phi,\\breve{\\theta})\\gets\\theta(\\dot{t}_{k},x_{k})+h^{q}\\big(\\phi\\big(\\dot{t}_{k},x_{k},a_{k}\\big)-\\phi\\big(t_{k},x_{k},a^{\\star}\\big)\\big)}\\\\ &{\\textstyle{\\mathcal{T}F_{\\zeta}}^{-1}\\gets h r_{k}+\\gamma^{h}\\big(1-\\mathsf{d o n e}_{k}\\big)\\overline{{\\theta}}\\big(t_{k}+h,x_{k}^{\\prime}\\big)+\\gamma^{h}\\mathsf{d o n e}_{k}f(x_{k}^{\\prime})}\\\\ &{\\ell_{k}(\\phi,\\theta)\\gets\\mathsf{Q u a n t i l e H u b e r}(F_{\\zeta}^{-1}(\\phi,\\theta),\\P F_{\\zeta}^{-1})}\\end{array}$ \u25b7Prediction (4.4) \u25b7Target (4.5) \u25b7See [10, Eq. (10)] $\\begin{array}{r}{\\overline{{\\ell(\\phi,\\theta)}}\\leftarrow\\frac{1}{N}\\sum_{k=1}^{N}\\ell_{k}(\\phi,\\theta)}\\end{array}$ $\\phi\\leftarrow\\phi-\\dot{\\alpha}\\dot{\\nabla}_{\\phi}\\ell(\\phi,\\theta)$ and $\\dot{\\theta}\\leftarrow\\theta-\\alpha\\nabla_{\\theta}\\ell(\\phi,\\theta)$ \u25b7Gradient updates if $i\\,|\\,n_{\\theta}$ then \u03b8 \u2190\u03b8 end if end for ", "page_idx": 23}, {"type": "text", "text": "Algorithm 3 Advantage-Shifted Distributional Superiority Quantile Regression ", "page_idx": 24}, {"type": "text", "text": "Require: $m$ (number of quantiles) and $q$ (rescale factor)   \nRequire: $h^{-1}$ (decision frequency), $\\mu$ (initial state distribution)   \nRequire: $\\pi^{\\mathrm{explore}}$ (exploration policy) and $\\rho_{\\beta}$ (distortion risk measure)   \nRequire: $\\mathbb{P}_{\\mathrm{act}}$ (action noise distribution)   \nRequire: $\\theta(t,x),\\phi(t,x,a)\\in\\mathbb{R}^{m}$ , parameterized quantile representations for $\\eta^{\\pi}(t,x),\\psi_{h;q}^{\\pi}(t,x,a)$ .   \nRequire: $\\overline{{\\theta}}(t,x)\\in\\mathbb{R}^{m}$ , target quantile representations for $\\eta^{\\pi}(t,x)$ .   \nRequire: $\\widetilde{A}_{h}(t,x,a)$ (parameterized representation of advantage function)   \nRequire: $n_{\\theta}$ (target update period), $\\alpha$ (step size), $N$ (batch size) $\\Phi\\gets\\emptyset$ reset $\\leftarrow$ True for $i\\in\\mathbb N$ do // Gather data with exploratory policy $(\\xi)_{s\\geq0}\\sim\\mathbb{P}_{\\mathrm{act}}$ \u25b7Sample from stochastic process for exploration noise for $\\bar{j}\\in\\{0,\\ldots,\\lfloor h^{-1}\\rfloor\\}$ do if reset is True then $t\\leftarrow0$ and $x_{0}\\sim\\mu$ reset $\\leftarrow$ False end if $\\begin{array}{r l}&{a^{\\star}\\leftarrow\\arg\\operatorname*{max}_{a}\\rho_{\\beta}\\left(\\frac{1}{m}\\sum_{n=1}^{m}\\delta_{\\phi(t,x_{t},a)_{n}+(1-h^{1-q})\\widetilde{A}_{h}(t,x_{t},a)}\\right)}\\\\ &{F_{\\psi_{h;q}}^{-1}(a)\\leftarrow\\phi(t,x_{t},a)-\\phi(t,x_{t},a^{\\star})\\mathrm{~for~each~}a\\in\\mathsf{A}}\\\\ &{A_{h}(t,x_{t},\\cdot)\\leftarrow\\widetilde{A}_{h}(t,x_{t},\\cdot)-\\widetilde{A}_{h}(t,x_{t},a^{\\star})}\\\\ &{v_{a}\\leftarrow\\rho_{\\beta}\\left(\\frac{1}{m}\\sum_{n=1}^{m}\\delta_{F_{\\psi_{h;q}}^{-1}(a)_{n}+(1-h^{1-q})\\widetilde{A}_{h}(t,x_{t},a)}\\right)\\mathrm{~for~each~}a\\in\\mathsf{A}}\\\\ &{a_{t}\\sim\\pi^{\\mathrm{explore}}((v_{a})_{a\\in\\mathsf{A}},\\xi_{j h})}\\end{array}$ \u25b7see [34, DAU] Perform action $a_{t}$ for $h$ units of time, observe $(r_{t},x_{t+h},{\\mathsf{d o n e}}_{t+h})$ . $Y_{j}\\sim\\mathsf{B e r n o u l l i}(h)$ $\\triangleright$ Subsample transitions for replay if $Y_{j}=1\\lor\\mathsf{d o n e}_{j+1}=\\mathsf{T r u e\\ t h e n}$ $\\mathcal{D}\\gets\\mathcal{D}\\cup\\{(t,x_{t},a_{t},r_{t},x_{t+h},mathsf{d o n e}_{t+h})\\}$ end if reset $\\leftarrow{\\mathsf{d o n e}}_{t+h}$ and $t\\gets t+h$ end for // Update return/superiority distributions Sample minibatch $\\{(t_{k},\\bar{x}_{k},a_{k},r_{k},\\bar{x}_{k}^{\\prime},mathsf{d o n e}_{k})\\}_{k=1}^{N}$ from $\\mathcal{D}$ for $\\bar{k}\\in[N]$ do a\u22c6\u2190arg maxa \u03c1\u03b2 m1 nm=1 \u03b4\u03d5(tk,xk,a)n+(1\u2212h1\u2212q)A h(tk,xk,a) // Advantage Loss $\\begin{array}{r l}&{A_{h}(t_{k},x_{k},a_{k})\\gets\\widetilde{A}_{h}(t_{k},x_{k},a_{k})-\\widetilde{A}_{h}(t_{k},x_{k},a^{\\star})}\\\\ &{V(t_{k},a_{k})\\gets\\frac{1}{m}\\sum_{n=1}^{,}\\theta(t_{k},x_{k})_{n}}\\\\ &{Q(t_{k},x_{k},a_{k})\\gets V(t_{k},a_{k})+h A_{h}(t_{k},x_{k},a_{k})}\\\\ &{\\mathbb{T}Q(t_{k},x_{k},a_{k})=h r_{k}+\\gamma^{h}\\frac{1}{m}\\sum_{n=1}^{m}\\theta(t_{k}+h,x_{k+1})_{n}}\\\\ &{\\ell_{k}^{\\mathrm{adv}}(\\widetilde{A}_{h})\\gets\\left(Q(t_{k},x_{k},a_{k})-\\mathbb{T}Q(t_{k},x_{k},a_{k})\\right)^{2}}\\end{array}$ \u25b7Bellman error // Superiority Loss $\\begin{array}{r l}&{F_{\\zeta}^{-1}(\\bar{\\phi},\\theta)\\gets\\theta(\\bar{t}_{k},x_{k})+h^{q}(\\phi(t_{k},x_{k},a_{k})-\\phi(t_{k},x_{k},a^{\\star}))}\\\\ &{\\mathsf{T}F_{\\zeta}^{-1}\\gets h r_{k}+\\gamma^{h}(1-\\mathsf{d o n e}_{k})\\overline{{\\theta}}(t_{k}+h,x_{k}^{\\prime})+\\gamma^{h}\\mathsf{d o n e}_{k}f(x_{k}^{\\prime})}\\\\ &{\\ell_{k}(\\phi,\\theta)\\gets\\mathsf{Q u a n t i l e H u b e r}(F_{\\zeta}^{-1}(\\phi,\\theta),\\mathsf{T}F_{\\zeta}^{-1})}\\end{array}$ \u25b7Prediction (4.4) \u25b7Target (4.5) \u25b7See [10, Eq. (10)] end for $\\begin{array}{r}{\\overline{{\\ell(\\phi,\\theta)}}\\leftarrow\\frac{1}{N}\\sum_{k=1}^{N}\\ell_{k}(\\phi,\\theta)}\\end{array}$ and $\\begin{array}{r}{\\ell^{\\mathrm{adv}}(\\widetilde{A}_{h})\\gets\\frac{1}{2N}\\sum_{k=1}^{N}\\ell_{k}^{\\mathrm{adv}}(\\widetilde{A}_{h})}\\end{array}$ $\\phi\\leftarrow\\phi-\\alpha\\nabla_{\\phi}\\ell(\\phi,\\theta)$ and $\\theta\\leftarrow\\theta-\\alpha\\nabla_{\\theta}\\ell(\\phi,\\theta)$ and $\\widetilde{A}_{h}\\gets\\widetilde{A}_{h}-\\alpha\\nabla\\ell^{\\mathrm{adv}}(\\widetilde{A}_{h})$ if $i\\,|\\,n_{\\theta}$ then ${\\overline{{\\theta}}}\\gets\\theta$ end if end for ", "page_idx": 24}, {"type": "text", "text": "D Additional Experimental Results ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Figure D.1 include results comparing the performance of $\\mathrm{DSUP}(1/2)$ and QR-DQN for risk-sensitive option trading across a variety of decision frequencies. ", "page_idx": 25}, {"type": "image", "img_path": "BRW0MKJ7Rr/tmp/c6aeaf2ee7a53bcfeed7e85fee0ea3e81cae20f9bc6a73448137e66ec92e937b.jpg", "img_caption": ["Figure D.1: Risk-sensitive option trading performance for various decision frequencies $\\omega$ . "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "E Simulation Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Here we collect further information about the setup for the simulations described in Section 5.2. ", "page_idx": 25}, {"type": "text", "text": "E.1 Option Trading Environment ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The environment used for the high-frequency option-trading setup is identical to that of Lim and Malik [22]. The environment emulates policies that decide when to exercise American call options. The state space is modeled as $\\mathbb{X}=\\mathbb{R}$ and with ${\\mathsf{T}}=[0,T]$ . Notably, existing works such as [22] and [17] describe the state space by $\\mathsf{X}=\\mathbb{R}^{2}$ , where one dimension represents time\u2014in our setup, we generally condition policies and returns on time, so we indeed model policies and returns as functions on $\\mathbb{R}^{2}$ . The state $X_{t}$ represents the price of the option at time $t$ , and evolves according to a geometric Brownian motion. ", "page_idx": 25}, {"type": "text", "text": "There are two actions in the environment. Action 0 \u201cholds\u201d the option, while action 1 represents \u201cexecute\u201d. Upon taking action 1 (or equivalently, once the time reaches $T$ ), the option is executed, and the agent receives a reward $f(x)=\\operatorname*{max}(0,1-x)$ and the episode terminates; here $x$ represents the price at the time of execution. No rewards are incurred otherwise. ", "page_idx": 26}, {"type": "text", "text": "Following the setup of [22], the dynamics of the prices are simulated based on data collected between years 2016 and 2019 from 10 commodities on the DOW market. Lim and Malik proposed a method for estimating the most likely parameters of geometric Brownian motion to fit the data for each commodity, which is then used to simulate many environment rollouts for training and evaluation. This is particularly convenient for our setup, where we additionally scale the decision frequency, corresponding to finer time discretizations of the Euler-Maruyama scheme for the estimated geometric Brownian motion. Like [22], separate dynamics parameters are estimated (for each commodity) between training and evaluation: the dynamics used for training are estimated on prefixes of the data, and those for testing (post-training) are estimated on suffixes of the data. Results are reported on the testing dynamics, averaged over the 10 commodities. ", "page_idx": 26}, {"type": "text", "text": "As is standard [22, 17], we simulate the environment with $T=100$ and $X_{0}=1$ . The simulations from [22] correspond to $h=1$ in this setting. In our high-frequency simulations, we discretize the dynamics with timestep $h<1$ . ", "page_idx": 26}, {"type": "text", "text": "E.2 Hyperparameters ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In Table 1, we list the hyperparameters used in the simulations for the tested algorithms. We note that although the original DAU implementation scaled the learning rate with $h$ , we take an alternative approach by updating every $h^{\\doteq1}$ environment steps akin to [3]. This is discussed in more detail in Appendix C. ", "page_idx": 26}, {"type": "table", "img_path": "BRW0MKJ7Rr/tmp/f50843d4ddbcbca938ec31dfcf97fe30f099998449e67789d6bf1539b530603d.jpg", "table_caption": ["Table 1: Hyperparameters "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "E.3 Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Our implementations are written in Jax [6] and executed with a single NVidia V100 GPU. At highest decision frequencies, experiments took longer to execute, averaging out at a maximum of roughly four hours. ", "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We believe our abstract and introduction outline and faithfully summarize the content of our work. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We have included discussions of the limitations and scope of our results throughout our work, rather than within a specific \u201cLimitations\u201d section. See, for example, Section 5. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best ", "page_idx": 27}, {"type": "text", "text": "judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We have provided complete statements, proofs, and references to used results in our proofs of our theoretical results. Please see the Appendix for restatements and proofs. Statements can be found in the main body of our work. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We believe we have included enough information that others can faithfully reproduce our main experimental results. Please see Sections 4 and 5 and the Appendix. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example   \n(a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We have provided data, code, and experimental details. We believe we have included enough information that others can faithfully reproduce our main experimental results. Please see Section 5 and the Appendix. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 29}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We provided these details in Section 5 and the Appendix. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 30}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: When appropriate, we have included error bars and information regarding the statistical significance of our experiments. Please see Section 5 and the Appendix. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 30}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We have included these details in Section 5 and the Appendix. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. ", "page_idx": 30}, {"type": "text", "text": "\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 31}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We have reviewed the NeurIPS Code of Ethics and believe our work conforms to it in every respect. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 31}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: We believe our work is foundational research without a direct path to negative societal impact. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 31}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: We believe our work poses no risk of misuse. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 32}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Data for the option-trading environment was downloaded from an open source Github repository, and we cited the work that introduced the dataset [22]. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 32}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: Our work does not release new assets. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets. \u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. ", "page_idx": 32}, {"type": "text", "text": "\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 33}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: Our work neither involves crowdsourcing nor research with human subjects. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 33}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: Our work neither involves crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 33}]