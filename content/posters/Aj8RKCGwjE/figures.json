[{"figure_path": "Aj8RKCGwjE/figures/figures_2_1.jpg", "caption": "Figure 1: AROMA inference: The discretization-free encoder compresses the information of a set of N input values to a sequence of M latent tokens, where M < N. The conditional diffusion transformer is used to model the dynamics, acting as a latent refiner. The continuous decoder leverages self-attentions (SA), cross-attention (CA) and a local INR to map back to the physical space. Learnable tokens are shared and encode spatial relations. Latent token Zt represents ut and Zt+\u2206t is the prediction corresponding to ut+At.", "description": "This figure illustrates the AROMA model's inference process.  The input, representing spatial data (e.g., from a grid, mesh, or point cloud), is first encoded into a compressed latent representation using a discretization-free encoder. This latent representation, consisting of M tokens (M << N, where N is the number of input values), is then processed by a conditional diffusion transformer to model the temporal dynamics of the system. Finally, a decoder maps the refined latent tokens back into the original physical space to provide the prediction at any desired location.", "section": "3 Model Description"}, {"figure_path": "Aj8RKCGwjE/figures/figures_3_1.jpg", "caption": "Figure 1: AROMA inference: The discretization-free encoder compresses the information of a set of N input values to a sequence of M latent tokens, where M < N. The conditional diffusion transformer is used to model the dynamics, acting as a latent refiner. The continuous decoder leverages self-attentions (SA), cross-attention (CA) and a local INR to map back to the physical space. Learnable tokens are shared and encode spatial relations. Latent token Zt represents ut and Zt+\u2206t is the prediction corresponding to ut+\u2206t.", "description": "The figure illustrates the AROMA model's inference process.  It starts with input values, which are encoded into a compressed latent representation using a discretization-free encoder.  This latent representation then undergoes a time-marching process using a conditional diffusion transformer, which refines the latent tokens over time. Finally, a decoder maps these refined tokens back to the original space, providing predictions. Self-attention and cross-attention mechanisms are used within the transformer for modeling relationships between latent tokens, while a local INR enables continuous mapping back to the original space.", "section": "3 Model Description"}, {"figure_path": "Aj8RKCGwjE/figures/figures_4_1.jpg", "caption": "Figure 1: AROMA inference: The discretization-free encoder compresses the information of a set of N input values to a sequence of M latent tokens, where M < N. The conditional diffusion transformer is used to model the dynamics, acting as a latent refiner. The continuous decoder leverages self-attentions (SA), cross-attention (CA) and a local INR to map back to the physical space. Learnable tokens are shared and encode spatial relations. Latent token Zt represents ut and Zt+\u2206t is the prediction corresponding to ut+\u2206t.", "description": "This figure illustrates the AROMA model's inference process.  The input data (N values) is first encoded into a smaller, fixed-size latent representation (M tokens, M<N) that captures spatial information efficiently. This compact latent representation is then processed by a diffusion transformer to model the temporal dynamics. Finally, a decoder maps the refined latent representation back to the original spatial domain, allowing for predictions at any location.", "section": "3 Model Description"}, {"figure_path": "Aj8RKCGwjE/figures/figures_7_1.jpg", "caption": "Figure 3: Correlation over time for long rollouts with different methods on Burgers", "description": "This figure compares the performance of different models (FNO, ResNet, AROMA, and AROMA without diffusion) on long-term prediction tasks. The x-axis represents the number of rollout steps (time), while the y-axis shows the correlation between the model predictions and the ground truth. The plot reveals how the correlation decreases over time for all models, indicating that the predictive accuracy degrades as the prediction horizon extends.  AROMA with diffusion outperforms the other methods in maintaining a higher correlation for a longer period.", "section": "4.4 Long rollouts and uncertainty quantification"}, {"figure_path": "Aj8RKCGwjE/figures/figures_12_1.jpg", "caption": "Figure 4: During training, we noise the next-step latent tokens Zt+\u25b3t and train the transformer to predict the \"velocity\" of the noise. Each DIT block is implemented as in Peebles & Xie (2023).", "description": "This figure illustrates the training process of the diffusion transformer block within AROMA.  The input consists of previous latent tokens (Zt-\u25b3t) and a noisy estimate of the current latent tokens (Zt). A linear layer maps the noisy estimate and the previous latent tokens to a higher dimensional space. Then a Diffusion Transformer (DiT) block, detailed in Appendix B, processes this information and outputs new tokens. These output tokens are compared to the target tokens using Mean Squared Error (MSE) to calculate the loss. A linear layer, as well as noise scheduling function (\u221a\u03b1k\u03f5 - \u221a1-\u03b1k Zt), maps  the diffusion step conditioning embedding (k) to the target tokens.", "section": "3.3 Transformer-based diffusion"}, {"figure_path": "Aj8RKCGwjE/figures/figures_12_2.jpg", "caption": "Figure 1: AROMA inference: The discretization-free encoder compresses the information of a set of N input values to a sequence of M latent tokens, where M < N. The conditional diffusion transformer is used to model the dynamics, acting as a latent refiner. The continuous decoder leverages self-attentions (SA), cross-attention (CA) and a local INR to map back to the physical space. Learnable tokens are shared and encode spatial relations. Latent token Zt represents ut and Zt+\u2206t is the prediction corresponding to ut+\u2206t.", "description": "This figure illustrates the AROMA (Attentive Reduced Order Model with Attention) model architecture for solving Partial Differential Equations (PDEs).  The process begins with an encoder that compresses input data (from various sources, such as point clouds or irregular grids) into a compact representation of latent tokens.  A diffusion-based transformer then models the temporal dynamics within this lower-dimensional latent space. Finally, a decoder reconstructs the solution in the original space using a continuous neural field, generating values at any spatial location requested.", "section": "3 Model Description"}, {"figure_path": "Aj8RKCGwjE/figures/figures_13_1.jpg", "caption": "Figure 1: AROMA inference: The discretization-free encoder compresses the information of a set of N input values to a sequence of M latent tokens, where M < N. The conditional diffusion transformer is used to model the dynamics, acting as a latent refiner. The continuous decoder leverages self-attentions (SA), cross-attention (CA) and a local INR to map back to the physical space. Learnable tokens are shared and encode spatial relations. Latent token Zt represents ut and Zt+\u2206t is the prediction corresponding to ut+\u2206t.", "description": "The figure illustrates the architecture of the AROMA model for solving PDEs. It shows how the input data is encoded into a lower-dimensional latent space, processed using a diffusion transformer to model the temporal dynamics, and then decoded back into the original space to obtain predictions.  The encoder handles diverse geometries without discretization. The transformer refines the latent representation sequentially, and the decoder uses self-attention, cross-attention, and a local INR for accurate prediction.", "section": "3 Model Description"}, {"figure_path": "Aj8RKCGwjE/figures/figures_13_2.jpg", "caption": "Figure 1: AROMA inference: The discretization-free encoder compresses the information of a set of N input values to a sequence of M latent tokens, where M < N. The conditional diffusion transformer is used to model the dynamics, acting as a latent refiner. The continuous decoder leverages self-attentions (SA), cross-attention (CA) and a local INR to map back to the physical space. Learnable tokens are shared and encode spatial relations. Latent token Zt represents ut and Zt+\u2206t is the prediction corresponding to ut+\u2206t.", "description": "This figure illustrates the inference process of the AROMA model for solving partial differential equations. The encoder takes in a set of N input values, which are then compressed into a smaller sequence of M latent tokens (where M < N). This compressed representation is then fed into a diffusion transformer which models the dynamics of the system over time.  The output of the transformer, a refined set of latent tokens, is then passed to a decoder which uses self-attention, cross-attention, and a local implicit neural representation (INR) to reconstruct the continuous physical values, achieving a discretization-free approach. The figure highlights the model's ability to handle both spatial and temporal information efficiently.", "section": "3 Model Description"}, {"figure_path": "Aj8RKCGwjE/figures/figures_14_1.jpg", "caption": "Figure 8: Multi-band local INR decoder", "description": "The figure shows the architecture of a multi-band local INR decoder.  The decoder uses cross-attention to retrieve multiple feature vectors, each corresponding to a different frequency band. These feature vectors are then concatenated and passed through an MLP to produce the final output value. This multi-band approach allows the decoder to capture more detailed spatial information than a single-band decoder.", "section": "3.2 Encoder-decoder description"}, {"figure_path": "Aj8RKCGwjE/figures/figures_17_1.jpg", "caption": "Figure 1: AROMA inference: The discretization-free encoder compresses the information of a set of N input values to a sequence of M latent tokens, where M < N. The conditional diffusion transformer is used to model the dynamics, acting as a latent refiner. The continuous decoder leverages self-attentions (SA), cross-attention (CA) and a local INR to map back to the physical space. Learnable tokens are shared and encode spatial relations. Latent token Zt represents ut and Zt+\u2206t is the prediction corresponding to ut+At.", "description": "This figure illustrates the architecture of the AROMA model, which consists of three main components: an encoder, a latent time-marching refiner (diffusion transformer), and a decoder. The encoder takes the input values (e.g., spatial data) and converts them into a sequence of M latent tokens. The diffusion transformer processes these latent tokens to model the temporal dynamics of the system, making predictions in the latent space. Finally, the decoder maps the latent tokens back to the physical space to generate the output, such as prediction of spatiotemporal data.", "section": "3 Model Description"}, {"figure_path": "Aj8RKCGwjE/figures/figures_17_2.jpg", "caption": "Figure 1: AROMA inference: The discretization-free encoder compresses the information of a set of N input values to a sequence of M latent tokens, where M < N. The conditional diffusion transformer is used to model the dynamics, acting as a latent refiner. The continuous decoder leverages self-attentions (SA), cross-attention (CA) and a local INR to map back to the physical space. Learnable tokens are shared and encode spatial relations. Latent token Zt represents ut and Zt+\u2206t is the prediction corresponding to ut+\u2206t.", "description": "This figure illustrates the AROMA model's inference process.  It begins with an encoder that takes input values (discretized or not) and compresses them into a smaller set of latent tokens, capturing essential spatial information. These tokens are then processed by a conditional diffusion transformer to model the temporal dynamics of the system. Finally, a decoder maps the refined latent tokens back to the physical space, generating predictions.  The model uses self-attention and cross-attention mechanisms for efficient processing.", "section": "3 Model Description"}, {"figure_path": "Aj8RKCGwjE/figures/figures_17_3.jpg", "caption": "Figure 1: AROMA inference: The discretization-free encoder compresses the information of a set of N input values to a sequence of M latent tokens, where M < N. The conditional diffusion transformer is used to model the dynamics, acting as a latent refiner. The continuous decoder leverages self-attentions (SA), cross-attention (CA) and a local INR to map back to the physical space. Learnable tokens are shared and encode spatial relations. Latent token Zt represents ut and Zt+\u2206t is the prediction corresponding to ut+\u2206t.", "description": "This figure illustrates the architecture of the AROMA model, showing how input values are encoded into a compact latent representation, processed by a diffusion transformer to model temporal dynamics, and decoded back into the physical space. The encoder compresses spatial information into a smaller number of tokens, while the decoder uses self-attention and cross-attention mechanisms to reconstruct the output. The conditional diffusion transformer allows for robust and efficient modeling of temporal dynamics.", "section": "3 Model Description"}, {"figure_path": "Aj8RKCGwjE/figures/figures_18_1.jpg", "caption": "Figure 1: AROMA inference: The discretization-free encoder compresses the information of a set of N input values to a sequence of M latent tokens, where M < N. The conditional diffusion transformer is used to model the dynamics, acting as a latent refiner. The continuous decoder leverages self-attentions (SA), cross-attention (CA) and a local INR to map back to the physical space. Learnable tokens are shared and encode spatial relations. Latent token Zt represents ut and Zt+\u2206t is the prediction corresponding to ut+\u2206t.", "description": "This figure illustrates the overall architecture of the AROMA model for solving partial differential equations (PDEs). It shows how the model processes the input data using a three-stage pipeline: encoding, processing (using a conditional diffusion transformer), and decoding. The encoder maps the input data onto a fixed-size compact latent representation, which is then processed by the transformer to model the dynamics. Finally, the decoder maps this latent representation back into the physical space to obtain the predicted values.", "section": "3 Model Description"}, {"figure_path": "Aj8RKCGwjE/figures/figures_18_2.jpg", "caption": "Figure 1: AROMA inference: The discretization-free encoder compresses the information of a set of N input values to a sequence of M latent tokens, where M < N. The conditional diffusion transformer is used to model the dynamics, acting as a latent refiner. The continuous decoder leverages self-attentions (SA), cross-attention (CA) and a local INR to map back to the physical space. Learnable tokens are shared and encode spatial relations. Latent token Zt represents ut and Zt+\u2206t is the prediction corresponding to ut+\u2206t.", "description": "This figure illustrates the AROMA model's inference process.  The encoder processes input data (spatial coordinates and function values) to generate a compressed latent representation. A transformer refines this representation to model the dynamics. Finally, a decoder maps the refined representation back to the original space, generating predictions for future time steps.  Key elements include attention mechanisms and a local implicit neural representation (INR) for spatial processing.", "section": "3 Model Description"}, {"figure_path": "Aj8RKCGwjE/figures/figures_18_3.jpg", "caption": "Figure 1: AROMA inference: The discretization-free encoder compresses the information of a set of N input values to a sequence of M latent tokens, where M < N. The conditional diffusion transformer is used to model the dynamics, acting as a latent refiner. The continuous decoder leverages self-attentions (SA), cross-attention (CA) and a local INR to map back to the physical space. Learnable tokens are shared and encode spatial relations. Latent token Zt represents ut and Zt+\u2206t is the prediction corresponding to ut+\u2206t.", "description": "This figure shows the architecture of the AROMA model, highlighting its three main components: the encoder, the latent time-marching refiner, and the decoder.  The encoder processes the input data (spatial field values) and transforms it into a lower-dimensional latent representation. The refiner models the temporal dynamics in this latent space, enabling efficient forecasting. Finally, the decoder maps the refined latent representation back to the original high-dimensional space to predict the spatial field at future timesteps.", "section": "3 Model Description"}, {"figure_path": "Aj8RKCGwjE/figures/figures_19_1.jpg", "caption": "Figure 1: AROMA inference: The discretization-free encoder compresses the information of a set of N input values to a sequence of M latent tokens, where M < N. The conditional diffusion transformer is used to model the dynamics, acting as a latent refiner. The continuous decoder leverages self-attentions (SA), cross-attention (CA) and a local INR to map back to the physical space. Learnable tokens are shared and encode spatial relations. Latent token Zt represents ut and Zt+\u2206t is the prediction corresponding to ut+\u2206t.", "description": "This figure illustrates the AROMA model's inference process.  The encoder takes in input values and compresses them into a smaller set of latent tokens. A transformer processes these tokens to model the temporal dynamics, acting as a refiner. Finally, a decoder maps the refined tokens back to the original physical space, using self-attention, cross-attention and a local implicit neural representation (INR).", "section": "3 Model Description"}, {"figure_path": "Aj8RKCGwjE/figures/figures_19_2.jpg", "caption": "Figure 1: AROMA inference: The discretization-free encoder compresses the information of a set of N input values to a sequence of M latent tokens, where M < N. The conditional diffusion transformer is used to model the dynamics, acting as a latent refiner. The continuous decoder leverages self-attentions (SA), cross-attention (CA) and a local INR to map back to the physical space. Learnable tokens are shared and encode spatial relations. Latent token Zt represents ut and Zt+\u2206t is the prediction corresponding to ut+\u2206t.", "description": "This figure shows the architecture of the AROMA model. The encoder takes input values (e.g., from a point cloud or mesh) and compresses them into a lower-dimensional latent representation using attention mechanisms.  This latent representation is then processed by a diffusion transformer to model temporal dynamics. Finally, the decoder reconstructs the output values from the latent representation. The figure highlights the key components: encoding, latent refinement, and decoding, all working together to approximate solutions to PDEs.", "section": "3 Model Description"}, {"figure_path": "Aj8RKCGwjE/figures/figures_20_1.jpg", "caption": "Figure 1: AROMA inference: The discretization-free encoder compresses the information of a set of N input values to a sequence of M latent tokens, where M < N. The conditional diffusion transformer is used to model the dynamics, acting as a latent refiner. The continuous decoder leverages self-attentions (SA), cross-attention (CA) and a local INR to map back to the physical space. Learnable tokens are shared and encode spatial relations. Latent token Zt represents ut and Zt+\u2206t is the prediction corresponding to ut+\u2206t.", "description": "This figure shows the architecture of the AROMA model for solving PDEs. The encoder compresses input data into a lower-dimensional latent representation using attention mechanisms.  A diffusion transformer processes this latent representation to model temporal dynamics. Finally, a decoder reconstructs the solution in the original spatial domain using a continuous neural field.", "section": "3 Model Description"}, {"figure_path": "Aj8RKCGwjE/figures/figures_20_2.jpg", "caption": "Figure 1: AROMA inference: The discretization-free encoder compresses the information of a set of N input values to a sequence of M latent tokens, where M < N. The conditional diffusion transformer is used to model the dynamics, acting as a latent refiner. The continuous decoder leverages self-attentions (SA), cross-attention (CA) and a local INR to map back to the physical space. Learnable tokens are shared and encode spatial relations. Latent token Zt represents ut and Zt+\u2206t is the prediction corresponding to ut+\u2206t.", "description": "This figure shows the architecture of the AROMA model. The encoder takes in input values and maps them to a sequence of M latent tokens (M<N, where N is the number of input values). The conditional diffusion transformer then processes this sequence to model the dynamics, which is then decoded to approximate the function value for any query coordinate in the spatial domain. The model is discretization-free, meaning it can handle various input types such as point clouds and irregular grids.", "section": "3 Model Description"}, {"figure_path": "Aj8RKCGwjE/figures/figures_20_3.jpg", "caption": "Figure 1: AROMA inference: The discretization-free encoder compresses the information of a set of N input values to a sequence of M latent tokens, where M < N. The conditional diffusion transformer is used to model the dynamics, acting as a latent refiner. The continuous decoder leverages self-attentions (SA), cross-attention (CA) and a local INR to map back to the physical space. Learnable tokens are shared and encode spatial relations. Latent token Zt represents ut and Zt+\u2206t is the prediction corresponding to ut+At.", "description": "This figure illustrates the AROMA model's inference process.  The encoder takes input values and compresses them into a smaller set of latent tokens. These tokens are then processed by a diffusion transformer to model the temporal dynamics, and finally decoded to approximate the function values at any spatial location.  Self-attention and cross-attention mechanisms are used to model spatial relations.", "section": "3 Model Description"}, {"figure_path": "Aj8RKCGwjE/figures/figures_20_4.jpg", "caption": "Figure 1: AROMA inference: The discretization-free encoder compresses the information of a set of N input values to a sequence of M latent tokens, where M < N. The conditional diffusion transformer is used to model the dynamics, acting as a latent refiner. The continuous decoder leverages self-attentions (SA), cross-attention (CA) and a local INR to map back to the physical space. Learnable tokens are shared and encode spatial relations. Latent token Zt represents ut and Zt+\u2206t is the prediction corresponding to ut+\u2206t.", "description": "This figure shows a schematic of the AROMA model's inference process.  The input data (ux) is processed by an encoder to create a compressed representation in a latent space (Zt). This latent representation is then processed by a diffusion transformer to model temporal dynamics, resulting in a refined latent representation (Zt+\u2206t). Finally, a decoder maps this refined latent representation back to the original physical space to produce the prediction (\u00fbt+\u2206t). The figure highlights the key components of the model and their interaction.", "section": "3 Model Description"}, {"figure_path": "Aj8RKCGwjE/figures/figures_21_1.jpg", "caption": "Figure 1: AROMA inference: The discretization-free encoder compresses the information of a set of N input values to a sequence of M latent tokens, where M < N. The conditional diffusion transformer is used to model the dynamics, acting as a latent refiner. The continuous decoder leverages self-attentions (SA), cross-attention (CA) and a local INR to map back to the physical space. Learnable tokens are shared and encode spatial relations. Latent token Zt represents ut and Zt+\u2206t is the prediction corresponding to ut+\u2206t.", "description": "This figure illustrates the AROMA model's inference process.  The encoder takes in input values (e.g., from a grid or point cloud) and compresses them into a smaller set of latent tokens.  These tokens are then processed by a diffusion transformer that models temporal dynamics. Finally, the decoder reconstructs the function values at any queried point in space using self-attention, cross-attention, and a local neural implicit representation (INR).", "section": "3 Model Description"}, {"figure_path": "Aj8RKCGwjE/figures/figures_21_2.jpg", "caption": "Figure 1: AROMA inference: The discretization-free encoder compresses the information of a set of N input values to a sequence of M latent tokens, where M < N. The conditional diffusion transformer is used to model the dynamics, acting as a latent refiner. The continuous decoder leverages self-attentions (SA), cross-attention (CA) and a local INR to map back to the physical space. Learnable tokens are shared and encode spatial relations. Latent token Zt represents ut and Zt+\u2206t is the prediction corresponding to ut+\u2206t.", "description": "This figure illustrates the architecture of the AROMA model. The encoder maps the input data to a compact latent representation.  The latent refiner processes this representation using a diffusion transformer to capture the temporal dynamics. The decoder finally maps this refined representation back into the original space using a local neural network, resulting in the prediction for the next time step. The whole process is continuous, eliminating the need for discrete spatial discretization.", "section": "3 Model Description"}, {"figure_path": "Aj8RKCGwjE/figures/figures_21_3.jpg", "caption": "Figure 1: AROMA inference: The discretization-free encoder compresses the information of a set of N input values to a sequence of M latent tokens, where M < N. The conditional diffusion transformer is used to model the dynamics, acting as a latent refiner. The continuous decoder leverages self-attentions (SA), cross-attention (CA) and a local INR to map back to the physical space. Learnable tokens are shared and encode spatial relations. Latent token Zt represents ut and Zt+\u2206t is the prediction corresponding to ut+\u2206t.", "description": "The figure illustrates the AROMA model's inference process.  An encoder transforms input data (e.g., from a grid or point cloud) into a compact latent representation using a sequence of M tokens. A conditional diffusion transformer processes these tokens to model the temporal dynamics of the system. Finally, a decoder maps the refined latent tokens back to the original spatial domain, generating a prediction.", "section": "3 Model Description"}, {"figure_path": "Aj8RKCGwjE/figures/figures_23_1.jpg", "caption": "Figure 2: Spatial interpretation of the tokens through cross attention between Tgeo and \u03b3(x) for each x in the domain. Here we visualize the cross-attention of three different tokens for a given head. The cross attentions can have varying receptive fields depending on the geometries.", "description": "This figure shows how the model encodes spatial information through cross-attention. The heatmaps visualize the attention weights between geometry-aware tokens (Tgeo) and positional embeddings (\u03b3(x)) for three different tokens.  Varying receptive fields are observed, showing how the model adapts to different geometries. The spatial relationships are implicitly encoded in the latent token space.", "section": "3.3 Transformer-based diffusion"}, {"figure_path": "Aj8RKCGwjE/figures/figures_24_1.jpg", "caption": "Figure 1: AROMA inference: The discretization-free encoder compresses the information of a set of N input values to a sequence of M latent tokens, where M < N. The conditional diffusion transformer is used to model the dynamics, acting as a latent refiner. The continuous decoder leverages self-attentions (SA), cross-attention (CA) and a local INR to map back to the physical space. Learnable tokens are shared and encode spatial relations. Latent token Zt represents ut and Zt+\u2206t is the prediction corresponding to ut+\u2206t.", "description": "This figure illustrates the architecture of the AROMA model, which consists of three main components: an encoder, a diffusion transformer (latent time-marching refiner), and a decoder.  The encoder compresses the input data (from various geometries) into a compact latent representation, which is then processed by the diffusion transformer to model temporal dynamics.  Finally, the decoder maps the processed latent representation back into the original space, providing predictions for any query location.", "section": "3 Model Description"}, {"figure_path": "Aj8RKCGwjE/figures/figures_25_1.jpg", "caption": "Figure 1: AROMA inference: The discretization-free encoder compresses the information of a set of N input values to a sequence of M latent tokens, where M < N. The conditional diffusion transformer is used to model the dynamics, acting as a latent refiner. The continuous decoder leverages self-attentions (SA), cross-attention (CA) and a local INR to map back to the physical space. Learnable tokens are shared and encode spatial relations. Latent token Zt represents ut and Zt+\u2206t is the prediction corresponding to ut+\u2206t.", "description": "The figure illustrates the AROMA model's architecture, highlighting its three main components: the encoder, the latent time-marching refiner, and the decoder.  The encoder transforms input values into a lower-dimensional latent space, capturing essential spatial information. A conditional diffusion transformer processes these latent tokens to model the temporal dynamics, while a continuous decoder maps back to the original physical space to provide predictions. The model's key advantage is its ability to handle various data types and complex geometries, providing a flexible and efficient approach to PDE modeling.", "section": "3 Model Description"}, {"figure_path": "Aj8RKCGwjE/figures/figures_26_1.jpg", "caption": "Figure 1: AROMA inference: The discretization-free encoder compresses the information of a set of N input values to a sequence of M latent tokens, where M < N. The conditional diffusion transformer is used to model the dynamics, acting as a latent refiner. The continuous decoder leverages self-attentions (SA), cross-attention (CA) and a local INR to map back to the physical space. Learnable tokens are shared and encode spatial relations. Latent token Zt represents ut and Zt+\u2206t is the prediction corresponding to ut+At.", "description": "This figure illustrates the AROMA model's inference process.  The encoder takes input values and compresses them into a smaller set of latent tokens. A transformer processes these tokens to model temporal dynamics, refining the latent representation. Finally, a decoder maps the refined tokens back to the original space, providing a prediction of the function values.", "section": "3 Model Description"}]