[{"type": "text", "text": "AROMA: Preserving Spatial Structure for Latent PDE Modeling with Local Neural Fields ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Louis Serrano1 \u2217 Thomas X Wang1 Etienne Le Naour1,2 ", "page_idx": 0}, {"type": "text", "text": "Jean-Noe\u00a8l Vittaut 3 Patrick Gallinari1,4 ", "page_idx": 0}, {"type": "text", "text": "1Sorbonne Universite\u00b4, CNRS, ISIR, 75005 Paris, France 2EDF R&D, Palaiseau, France   \n3Sorbonne Universite\u00b4, CNRS, LIP6, 75005 Paris, France 4Criteo AI Lab, Paris, France ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We present AROMA (Attentive Reduced Order Model with Attention), a framework designed to enhance the modeling of partial differential equations (PDEs) using local neural fields. Our flexible encoder-decoder architecture can obtain smooth latent representations of spatial physical fields from a variety of data types, including irregular-grid inputs and point clouds. This versatility eliminates the need for patching and allows efficient processing of diverse geometries. The sequential nature of our latent representation can be interpreted spatially and permits the use of a conditional transformer for modeling the temporal dynamics of PDEs. By employing a diffusion-based formulation, we achieve greater stability and enable longer rollouts compared to conventional MSE training. AROMA\u2019s superior performance in simulating 1D and 2D equations underscores the efficacy of our approach in capturing complex dynamical behaviors. Github page: https://github.com/LouisSerrano/aroma ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In recent years, many deep learning (DL) surrogate models have been introduced to approximate solutions to partial differential equations (PDEs) (Lu et al., 2021; Li et al., 2021; Brandstetter et al., 2022; Stachenfeld et al., 2022). Among these, the family of neural operators has been extensively adopted and tested across various scientific domains, demonstrating the potential of data-centric DL models in science (Pathak et al., 2022; Vinuesa & Brunton, 2022). ", "page_idx": 0}, {"type": "text", "text": "Neural Operators were initially constrained by discretization and domain geometry limitations. Recent advancements, such as neural fields (Yin et al., 2022; Serrano et al., 2023) and transformer architectures (Li et al., 2023; Hao et al., 2023), have partially addressed these issues, improving both dynamic modeling and steady-state settings. However, Neural Fields struggle to model spatial information and local dynamics effectively, and existing transformer architectures, while being flexible, are computationally expensive due to their operation in the original physical space and require large training datasets. ", "page_idx": 0}, {"type": "text", "text": "Our hypothesis is that considering spatiality is essential in modeling spatio-temporal phenomena, yet applying attention mechanisms directly is computationally expensive. We propose a new framework that models the dynamics in a reduced latent space, encoding spatial information compactly, by one or two orders of magnitude relative to the original space. This approach addresses both the complexity issues of transformer architectures and the spatiality challenges of Neural Fields. ", "page_idx": 0}, {"type": "text", "text": "Our novel framework leverages attention blocks and neural fields, resulting in a model that is easy to train and achieves state-of-the-art results on most datasets, particularly for complex geometries, without requiring prior feature engineering. To the best of our knowledge, we are the first to propose a fully attention-based architecture for processing domain geometries and unrolling dynamics. Compared to existing transformer architectures for PDEs, our framework first encapsulates the domain geometry and observation values in a compact latent representation, efficiently forecasting the dynamics at a lower computational cost. Transformer-based methods such as (Li et al., 2023; Hao et al., 2023) unroll the dynamics in the original space, leading to high complexity. ", "page_idx": 1}, {"type": "text", "text": "Our contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 A principled and versatile encode-process-decode framework for solving PDEs that operate on general input geometries, including point sets, grids, or meshes, and can be queried at any location within the spatial domain.   \n\u2022 A new spatial encode / process / decode approach: Variable-size inputs are mapped onto a fixed-size compact latent token space that encodes local spatial information. This latent representation is further processed by a transformer architecture that models the dynamics while exploiting spatial relations both at the local token level and globally across tokens. The decoding exploits a conditional neural field, allowing us to query forecast values at any point in the spatial domain of the equation.   \n\u2022 We include stochastic components at the encoding and processing levels to enhance stability and forecasting accuracy.   \n\u2022 Experiments performed on representative spatio-temporal forecasting problems demonstrate that AROMA is on par with or outperforms state-of-the-art baselines in terms of both accuracy and complexity. ", "page_idx": 1}, {"type": "text", "text": "2 Problem setting ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this paper, we focus on time-dependent PDEs defined over a spatial domain $\\Omega$ (with boundary $\\partial\\Omega$ ) and temporal domain $[0,T]$ . In the general form, their solutions $\\pmb{u}(x,t)$ satisfy the following constraints : ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\frac{\\partial{\\pmb u}}{\\partial t}={\\cal F}\\left(\\nu,t,x,{\\pmb u},\\frac{\\partial{\\pmb u}}{\\partial x},\\frac{\\partial^{2}{\\pmb u}}{\\partial x^{2}},\\dots\\right),\\quad\\forall x\\in\\Omega,\\forall t\\in(0,T]}\\\\ {\\displaystyle B({\\pmb u})(t,x)=0\\quad\\forall x\\in\\partial\\Omega,\\forall t\\in(0,T]}\\\\ {{\\pmb u}(0,x)={\\pmb u}^{0}\\quad\\forall x\\in\\Omega}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\nu$ represents a set of PDE coefficients, Equations (2) and (3) represent the constraints with respect to the boundary and initial conditions. We aim to learn, using solutions data obtained with classical solvers, the evolution operator $\\mathcal{G}$ that predicts the state of the system at the next time step: $\\pmb{u}^{t+\\Delta t}=\\mathcal{G}(\\pmb{u}^{t})$ . We have access to training trajectories obtained with different initial conditions, and we want to generate accurate trajectory rollouts for new initial conditions at test time. A rollout is obtained by the iterative application of the evolution operator ${\\pmb u}^{m\\Delta t}=\\mathcal{G}^{m}({\\pmb u}^{0})$ . ", "page_idx": 1}, {"type": "text", "text": "3 Model Description ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "3.1 Model overview ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We provide below an overview of the global framework and each component is described in a subsequent section. The model comprises three key components, as detailed in Figure 1. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Encoder $\\mathcal{E}_{w}:\\pmb{u}_{\\mathcal{X}}^{t}\\rightarrow\\pmb{Z}^{t}$ . The encoder takes input values $\\pmb{u}_{\\mathcal{X}}^{t}$ sampled over the domain $\\Omega$ at time $t$ , where $\\mathcal{X}$ denotes the discrete sample space and could be a grid, an irregular mesh or a point set. $\\pmb{u}_{\\mathcal{X}}^{t}$ is observed at locations $\\pmb{x}=(x_{1},\\dots x_{N})$ , with values $\\pmb{u}^{t^{\\flat}}\\!\\!=(\\pmb{u}^{t}(\\pmb{x}_{1}),\\cdots,\\pmb{u}^{t}(\\pmb{x}_{N}))$ . $N$ is the number of observations and can vary across samples. $\\pmb{u}_{\\mathcal{X}}^{t}$ is projected through a cross attention mechanism onto a set of $M$ tokens $\\mathbf{\\bar{Z}^{t}}=(z_{1}^{t},\\cdot\\cdot\\cdot\\,,z_{M}^{t})$ with $M$ a fixed parameter. This allows ", "page_idx": 1}, {"type": "image", "img_path": "Aj8RKCGwjE/tmp/09bcb865c131cd46609a51b457b58f897c1c541860e68ddf93a2112183e2e282.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 1: AROMA inference: The discretization-free encoder compresses the information of a set of $N$ input values to a sequence of $M$ latent tokens, where $M\\,<\\,N$ . The conditional diffusion transformer is used to model the dynamics, acting as a latent refiner. The continuous decoder leverages self-attentions (SA), cross-attention (CA) and a local INR to map back to the physical space. Learnable tokens are shared and encode spatial relations. Latent token $Z^{t}$ represents $u_{t}$ and $\\bar{Z^{t+\\Delta t}}$ is the prediction corresponding to $u_{t+\\Delta t}$ . ", "page_idx": 2}, {"type": "text", "text": "mapping any discretized input $\\pmb{u}_{\\mathcal{X}}^{t}$ onto a fixed dimensional latent representation $Z^{t}$ encoding implicit local spatial information from the input domain. The encoder is trained as a VAE and $\\bar{Z^{t}}$ is sampled from a multivariate normal statistics as detailed in Section 3.2. ", "page_idx": 2}, {"type": "text", "text": "\u2022 Latent time-marching refiner $\\mathcal{R}_{\\theta}:Z^{t}\\to\\hat{Z}^{t+\\Delta t}$ . We model the dynamics in the latent space through a transformer. The dynamics can be unrolled auto-regressively in the latent space for any time horizon without requiring to project back in the original domain $\\Omega$ . Self-attention operates on the latent tokens, which allows modeling global spatial relations between the local token representations. The transformer is enriched with a conditional diffusion mechanism operating between two successive time steps of the transformer. We experimentally observed that this probabilistic model was more robust than a baseline deterministic transformer for temporal extrapolation. ", "page_idx": 2}, {"type": "text", "text": "\u2022 Decoder $\\mathcal{D}_{\\psi}:\\hat{Z}^{t+\\Delta t}\\rightarrow\\hat{\\pmb u}^{t+\\Delta t}$ . The decoder uses the latent tokens $\\hat{\\pmb Z}^{t+\\Delta t}$ to approximate the function value $\\hat{\\pmb u}^{t+\\Delta t}(\\boldsymbol x)=\\mathscr{D}_{\\psi}(\\boldsymbol x,\\hat{\\pmb Z}^{t+\\Delta t})$ for any query coordinate $x\\in\\Omega$ . We therefore denote $\\hat{\\pmb u}^{t+\\Delta t}=\\mathscr{D}_{\\psi}(\\pmb Z^{t+\\Delta t})$ the predicted function. ", "page_idx": 2}, {"type": "text", "text": "Inference We encode the initial condition and unroll the dynamics in the latent space by successive denoisings: $\\hat{\\pmb u}^{m\\Delta t}\\,=\\,\\mathscr{D}_{\\psi}\\,\\circ\\,\\mathscr{R}_{\\theta}^{m}\\,\\circ\\,\\mathscr{E}_{w}(\\pmb u^{0})$ . We then decode along the trajectory to get the reconstructions. We outline the full inference pipeline in Figure 1 and detail its complexity analysis in Appendix C.1. ", "page_idx": 2}, {"type": "text", "text": "Training We perform a two-stage training: we first train the encoder and decoder, secondly train the refiner. This is more stable than end-to-end training. ", "page_idx": 2}, {"type": "text", "text": "3.2 Encoder-decoder description ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The encoder-decoder components are jointly trained using a VAE setting. The encoder is specifically designed to capture local input observation from any sampled point set in the spatial domain and encodes this information into a fixed number of tokens. The decoder can be queried at any position in the spatial domain, irrespective of the input sample. ", "page_idx": 2}, {"type": "text", "text": "Encoder The encoder maps an arbitrary number $N$ of observations $\\begin{array}{r l}{(\\mathbf{x},u(x))}&{{}:=}\\end{array}$ $((x_{1},\\pmb{u}(x_{1})),\\dots,(x_{N},\\pmb{u}(x_{N}))$ onto a latent representation $Z$ of fixed size $M$ through the following series of transformations: ", "page_idx": 2}, {"type": "image", "img_path": "Aj8RKCGwjE/tmp/5fc2684b04a71c63e6c25cd249a7be99650606e1ab35bb6c64e180246dedd51e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "where $(\\gamma(x),v(x))=((\\gamma(x_{1}),v(x_{1})),\\dots,(\\gamma(x_{N}),v(x_{N})))$ , and $h\\ll d$ . ", "page_idx": 3}, {"type": "text", "text": "(i) Embed positions and observations: Given an input sequence of coordinate-value pairs $(x_{1},\\pmb{u}(\\bar{x}_{1})),\\dots,(x_{N},\\pmb{u}(x_{N}))$ , we construct sequences of positional embeddings $\\ensuremath{\\gamma}=$ $(\\gamma(x_{1}),...,\\gamma(x_{N}))$ and value embeddings $\\begin{array}{r l}{\\pmb{v}}&{{}=\\bar{\\mathrm{~\\boldmath~\\Lambda~}}(v(x_{1}),\\bar{\\mathrm{~\\boldmath~\\Lambda~}}\\bar{\\mathrm{~\\boldmath~\\Lambda~}},v(x_{N}))}\\end{array}$ , where $\\begin{array}{r l}{\\gamma(x)}&{{}=}\\end{array}$ FourierFeatu $\\Sigma\\!\\in\\!\\mathsf{s}\\!\\left(x;\\omega\\right)$ and $v(x)=\\mathtt{L i n e a r}({\\pmb u}(x))$ , with $\\omega$ a fixed set of frequencies. These embeddings are aggregated onto a smaller set of learnable query tokens $\\mathbf{T}=(T_{1},\\dots,T_{M})$ and then $\\mathbf{T}^{\\prime}=(T_{1}^{\\prime},\\bar{\\dots},T_{M}^{\\prime})$ with $M$ fixed, to compress the information and encode the geometry and spatial latent representations. ", "page_idx": 3}, {"type": "text", "text": "(ii) Encode geometry: Geometry-aware tokens $\\mathbf{T}$ are obtained with a multihead cross-attention layer and a feedforward network (FFN), expressed as ${\\bf T}^{\\mathrm{geo}}={\\bf T}+\\mathtt{F F N}(\\mathtt{C r o s s a t t e n t i o n}({\\bf Q}=$ $\\mathbf{W}_{Q}\\mathbf{T},\\mathbf{K}\\,=\\,\\mathbf{W}_{K}\\gamma,\\mathbf{V}\\,=\\,\\mathbf{W}_{V}\\gamma)$ ). This step does not include information on the observations, ensuring that similar geometries yield similar query tokens $\\mathbf{T}^{\\mathrm{geo}}$ irrespective of the $\\textbf{\\em u}$ values. ", "page_idx": 3}, {"type": "text", "text": "(iii) Encode observations: The $\\mathbf{T}^{\\mathrm{geo}}$ tokens are then used to aggregate the observation values via a cross-attention mechanism: ${\\bf T}^{\\mathrm{{obs}}}\\;=\\;{\\bf T}^{\\mathrm{geo}}\\;+\\;{\\bf F}{\\bf F}\\mathrm{N}\\big({\\sf C r o s}\\,{\\sf s A t}\\,{\\sf t}\\,{\\sf e n t}\\,{\\sf i}\\,\\mathrm{on}({\\bf Q}\\;=\\;{\\bf W}_{Q}^{\\prime}{\\bf T}^{\\mathrm{geo}},{\\bf K}\\;=\\;{\\bf W}_{Q}^{\\prime}{\\bf T}^{\\mathrm{geo}})\\big){\\bf T}^{\\mathrm{}}.$ $\\mathbf{W}_{K}^{\\prime}\\gamma,\\mathbf{V}\\;=\\;\\mathbf{W}_{V}^{\\prime}\\pmb{v})$ ). Here, the values contain information on the observation values, and the keys contain information on the observation locations. ", "page_idx": 3}, {"type": "text", "text": "(iv) Reduce channel dimension and sample $Z$ : The information in the channel dimension of $\\mathbf{T}^{\\prime}$ is compressed using a bottleneck linear layer. To avoid exploding variance in this compressed latent space, we regularize it with a penalty on the $L_{2}$ norm of the latent code $\\lVert Z\\rVert^{2}$ . Introducing stochasticity through a variational formulation further helps to regularize the auto-encoding and obtain smoother representations for the forecasting step. For this, we learn the components of a Gaussian multivariate distribution $\\pmb{\\mu}=\\mathtt{L i n e a r}(\\mathbf{T}^{\\mathrm{obs}})$ and $\\log(\\pmb{\\sigma})=\\tt L i n e a r(\\pmb{T}^{\\mathrm{obs}})$ from which the final token embedding $Z$ is sampled. ", "page_idx": 3}, {"type": "text", "text": "Decoder The decoder\u2019s role is to reconstruct $\\hat{\\pmb u}^{t+\\Delta t}$ from $\\hat{\\pmb Z}^{t+\\Delta t}$ , see Figure 1. Since training is performed in two steps (\u201cencode-decode\u201d first and then \u201cprocess\u201d), the decoder is trained to reconstruct $\\hat{\\pmb u}^{t}$ for input $\\bar{\\pmb{u}}^{t}$ . One proceeds as follows. (i) Increase channel dimensions and apply self-attention: The decoder first lifts the latent tokens $_{z}$ to a higher channel dimension (this is the reverse operation of the one performed by the encoder) and then apply several layers of self-attention to get tokens $Z^{'}$ . (ii) Cross-attend: The decoder applies cross-attention to obtain feature vectors that depend on the query coordinate $x$ , $\\begin{array}{r}{(\\mathbf{f}_{q}^{u}(x))\\,=\\,\\overleftarrow{\\mathrm{Cros}}\\,\\mathrm{s}\\mathrm{att}\\,\\mathrm{ent}\\,\\mathrm{i}\\,\\mathrm{on}(\\mathbf{Q}\\,=\\,\\mathbf{W}_{Q}(\\gamma_{q}(x)),\\mathbf{K}\\,=\\,}\\end{array}$ ${\\bf W}_{K}Z^{'},{\\bf V}={\\bf W}_{V}Z^{'})$ , where $\\gamma_{q}$ is a Fourier features embedding of bandwidth $\\omega_{q}$ . (iii) Decode with MLP: Finally, we use a small MLP to decode this feature vector and obtain the reconstruction $\\hat{\\pmb u}(\\boldsymbol x)\\,=\\,\\mathrm{MLP}(\\mathbf{f}_{q}^{u}(\\boldsymbol x))$ . In contrast with existing neural field methods for dynamics modeling, the feature vector here is local. In practice, one uses multiple cross attentions to get feature vectors with different frequencies (see Appendix Figures 7 and 8 for further details). ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Training The encoder and decoder are jointly optimized as a variational autoencoder (VAE) (Kingma & Welling, 2013) to minimize the following objective : $\\begin{array}{r c l}{\\mathcal{L}}&{=}&{\\mathcal{L}_{\\mathrm{recon}}\\,+\\,\\beta\\,\\cdot\\,\\mathcal{L}_{K L}}\\end{array}$ ; where $\\mathcal{L}_{\\mathrm{recon}}\\,=\\,\\mathbf{M}\\bar{\\mathrm{SE}}(u_{\\mathcal{X}}^{t},\\hat{u}_{\\mathcal{X}}^{t})$ is the reconstruction loss between the input and the reconstruction $\\mathcal{D}_{\\psi}(Z^{t},\\mathcal{X})$ on the grid $\\mathcal{X}$ , with $\\pmb{Z}^{t}\\sim\\mathcal{N}(\\pmb{\\mu}^{t},(\\pmb{\\sigma}^{t})^{2})$ and $\\pmb{\\mu}^{t},\\pmb{\\sigma}^{t}=\\mathcal{E}_{w}\\dot{(u_{\\mathcal{X}}^{t})}$ . The KL divergence loss $\\mathcal{L}_{\\mathrm{KL}}\\,=\\,D_{\\mathrm{KL}}(\\mathcal{N}(\\pmb{\\mu}^{t},(\\pmb{\\sigma}^{t})^{2})\\,||\\mathcal{N}(0,I))$ helps regularize the network and prevents overfitting. We found that using a variational formulation was essential to obtain smooth latent representations while training the encoder-decoder. ", "page_idx": 3}, {"type": "image", "img_path": "Aj8RKCGwjE/tmp/0c2a2fd87e5df51f5fe3a9f391b3c73dcfd88c323c81f493b9eec736ecae7153.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 2: Spatial interpretation of the tokens through cross attention between $T^{g e o}$ and $\\gamma(x)$ for each $x$ in the domain. Here we visualize the cross-attention of three different tokens for a given head. The cross attentions can have varying receptive fields depending on the geometries. ", "page_idx": 4}, {"type": "text", "text": "3.3 Transformer-based diffusion ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Modeling the dynamics is performed in the latent $Z$ space. This space encodes spatial information present in the original space while being a condensed, smaller-sized representation, allowing for reduced complexity dynamics modeling. As indicated, the dynamics can be unrolled auto-regressively in this space for any time horizon without the need to map back to the original space. We use absolute positional embeddings $E_{\\mathrm{pos}}$ and a linear layer to project onto a higher dimensional space: $Z_{[0]}=\\mathtt{L i n e a r}(Z)+E_{\\mathrm{pos}}$ . The backbone then applies several self-attention blocks, which process tokens as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{Z}_{[l+1]}\\leftarrow\\pmb{Z}_{[l]}+\\mathtt{A t t e n t i o n(\\mathtt{L a y e r N o r m}}(\\pmb{Z}_{[l]}))}\\\\ &{\\pmb{Z}_{[l+1]}\\leftarrow\\pmb{Z}_{[l+1]}+\\mathtt{F F N}(\\mathtt{L a y e r N o r m}(\\pmb{Z}_{[l+1]})}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We found out that adding a diffusion component to the transformer helped enhance the stability and allowed longer forecasts. Diffusion steps are inserted between two time steps $t$ and $t+\\Delta t$ of the time-marching process transformer. The diffusion steps are denoted by $k$ and are different from the ones of the time-marching process (several diffusion steps $k$ are performed between two timemarching steps $t$ and $t+\\Delta t)$ . ", "page_idx": 4}, {"type": "text", "text": "We then use a conditional diffusion transformer architecture close to Peebles & Xie (2023) for $\\mathcal{R}_{\\theta}$ , where we detail the main block in Appendix B. At diffusion step $k$ , the input to the network is a sequence stacking the tokens at time $t$ and the current noisy targets estimate $(Z^{t},\\tilde{Z}_{k}^{t+\\Delta t})$ . See Appendix B, Figure 4 and Figure 5 for more details. To train the diffusion transformer $\\mathcal{R}_{\\theta}$ , we freeze the encoder and decoder, and use the encoder to sample pairs of successive latent tokens $(\\boldsymbol{Z}^{t},\\boldsymbol{Z}^{t+\\Delta t})$ . We employ the \u201cv-predict\u201d formulation of DDPM (Salimans & Ho, 2022) for training and sampling. ", "page_idx": 4}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we systematically evaluate the performance of our proposed model across various experimental settings, focusing on its ability to handle dynamics on both regular and irregular grids. First, we investigate the dynamics on regular grids, where we benchmark our model against stateof-the-art neural operators, including Fourier Neural Operators (FNO), ResNet, Neural Fields, and Transformers. This comparison highlights the efficacy of our approach in capturing complex spatiotemporal patterns on structured domains. Second, we extend our analysis to dynamics on irregular grids and shared geometries, emphasizing the model\u2019s extrapolation capabilities in data-constrained regimes. Here, we compare our results with Neural Fields and Transformers, demonstrating the robustness of our model in handling less structured and more complex spatial configurations. Lastly, we assess the model\u2019s capacity to process diverse geometries and underlying spatial representations by comparing its performance on irregular grids and different geometries. This evaluation highlights the flexibility and generalization ability of our model in encoding and learning from varied spatial domains, showcasing its potential in accurately representing and predicting dynamics across a wide range of geometric settings. We include additional results from ablation studies in Appendix C.6. ", "page_idx": 4}, {"type": "text", "text": "4.1 Dynamics on regular grids ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We begin our analysis with dynamics modeling on regular grid settings. Though our model is targeted for complex geometries, we believe this scenario remains an important benchmark to assess the efficiency of surrogate models. ", "page_idx": 5}, {"type": "text", "text": "Datasets \u2022 1D Burgers\u2019 Equation (Burgers): Models shock waves, using a dataset with periodic initial conditions and forcing term as in Brandstetter et al. (2022). It includes 2048 training and 128 test trajectories, at resolutions of (250, 100). We create sub-trajectories of 50 timestamps and treat them independently. \u2022 2D Navier Stokes Equation: for a viscous and incompressible fluid. We use the data from Li et al. (2021). The equation is expressed with the vorticity form on the unit torus: $\\begin{array}{r}{\\frac{\\partial w}{\\partial t}+u\\cdot\\nabla w\\,=\\,\\nu\\Delta w+f}\\end{array}$ o, $\\nabla u\\,=\\,0$ $x\\,\\in\\,\\Omega,t\\,>\\,0$ , $\\nu$ i)s  atnhde c(oNefafviiceier-nSt.t okWees $\\nu=10^{-4}$ $1\\times10^{-4}$ $\\nu=10^{-5}$ $1\\times10^{-5}$ ), and use train and test sets of 1000 and 200 trajectories with a base spatial resolution of size $64\\times64$ . We consider a horizon of $T=30$ for $\\nu=\\mathrm{i}0^{-4}$ and $T=20$ for $\\bar{\\nu}=10^{-5}$ since the phenomenon is more turbulent. At test time, we use the vorticity at $t_{0}=10$ as the initial condition. ", "page_idx": 5}, {"type": "text", "text": "Setting We train all the models with supervision on the next state prediction to learn to approximate the time-stepping operator $\\pmb{u}^{t+\\hat{\\Delta}t}~=~\\mathcal{G}(\\pmb{u}^{t})$ . At test time, we unroll the dynamics auto-regressively with each model and evaluate the prediction with a relative $L_{2}$ error defined as ", "page_idx": 5}, {"type": "equation", "text": "$\\begin{array}{r}{L_{2}^{\\mathrm{test}}=\\frac{1}{N_{\\mathrm{test}}}\\sum_{j\\in\\mathrm{test}}\\frac{||\\hat{u}_{j}^{\\mathrm{trajectory}}-u_{j}^{\\mathrm{trajectory}}||_{2}}{||u_{j}^{\\mathrm{trajectory}}||_{2}},}\\end{array}$ ", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Baselines We use a diverse panel of baselines including state of the art regular-grid methods such as FNO (Li et al., 2021) and ResNet (He et al., 2016; Lippe et al., 2023), flexible transformer architectures such as OFormer (Li et al., 2023), and GNOT (Hao et al., 2023), and finally neuralfield based methods with DINO (Yin et al., 2022) and CORAL (Serrano et al., 2023). ", "page_idx": 5}, {"type": "text", "text": "Results Table 1 presents a comparison of model performance on the Burgers, Navier-Stokes1e-4, and Navier-Stokes1e-5 datasets, with metrics reported in Relative $L_{2}$ . Our method, AROMA, demonstrates excellent performance across the board, highlighting its ability to capture the dynamics of turbulent phenomena, as reflected in the Navier-Stokes datasets. ", "page_idx": 5}, {"type": "text", "text": "In contrast, DINO and CORAL, both global neural field models, perform poorly in capturing turbulent phenomena, exhibiting significantly higher errors compared to other models. This indicates their limitations in handling complex fluid dynamics. On the other hand, AROMA outperforms GNOT on all datasets, though it performs reasonably well compared to the neural field based method. ", "page_idx": 5}, {"type": "table", "img_path": "Aj8RKCGwjE/tmp/679b8cce44231bc49b2246e04910dceb7dd798530c0e62c5bf782d7d2517a092.jpg", "table_caption": ["Table 1: Model Performance Comparison - Test results. Metrics in Relative $L_{2}$ . "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Regarding the regular-grid methods, ResNet shows suboptimal performance in the pure teacher forcing setting, rapidly accumulating errors over time during inference. FNO stands out as the best baseline, demonstrating competitive performance on all datasets. We hypothesize that FNO\u2019s robustness to error accumulation during the rollout can be attributed to its Fourier block, which effectively cuts off high-frequency components. Overall, the results underscore AROMA\u2019s effectiveness and highlight the challenges Neural Field-based models face in accurately modeling complex phenomena. ", "page_idx": 5}, {"type": "text", "text": "4.2 Dynamics on irregular grids with shared geometries ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We continue our experimental analysis with dynamics on unstructured grids, where we observe trajectories only through sparse spatial observations over time. We adopt a data-constrained regime and show that our model can still be competitive with existing Neural Fields in this scenario. ", "page_idx": 5}, {"type": "text", "text": "Datasets To evaluate our framework, we utilize two fluid dynamics datasets commonly used as a benchmark for this task (Yin et al., 2022; Serrano et al., 2023) with unique initial conditions for each trajectory: \u2022 2D Navier-Stokes Equation (Navier-Stokes $1\\times10^{-3}$ ): We use the same equation as in Section 4.1 but with a higher viscosity coefficient $\\nu=1e-3$ . We have 256 trajectories of size 40 for training and 32 for testing. We used a standard resolution of $64\\mathrm{x}64$ . \u2022 3D Shallow-Water Equation (Shallow-Water): This equation approximates fluid flow on the Earth\u2019s surface. The data includes the vorticity $w$ and height $h$ of the fluid. The training set comprises 64 trajectories of size 40, and the test set comprises 8 trajectories with 40 timestamps. We use a standard spatial resolution of $64\\times128$ . ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Setting \u2022 Temporal Extrapolation: For both datasets, we split trajectories into two equal parts of 20 timestamps each. The first half is denoted as In-t and the second half as Out-t. The training set consists of In-t. During training, we supervise with the next state only. During testing, the model unrolls the dynamics from a new initial condition (IC) up to the end of Out-t, i.e. for 39 steps. Evaluation within the In-t horizon assesses the model\u2019s ability to forecast within the training regime. The Out-t evaluation tests the model\u2019s extrapolation capabilities beyond the training horizon. \u2022 Sparse observations: For the train and test set we randomly select $\\pi$ percent of the available regular mesh to create a unique grid for each trajectory, both in the train and in the test. The grid is kept fixed along a given trajectory. While each grid is different, they maintain the same level of sparsity across trajectories. In our case, $\\pi=100\\%$ amounts to the fully observable case, while in $\\pi=25\\%$ each grid contains around 1020 points for Navier-Stokes $1\\times\\dot{1}0^{-3}$ and 2040 points for Shallow-Water. ", "page_idx": 6}, {"type": "text", "text": "Baselines We compare our model to OFormer (Li et al., 2023), GNOT (Hao et al., 2023), and choose DINO (Yin et al., 2022) and CORAL (Serrano et al., 2023) as the neural field baselines. ", "page_idx": 6}, {"type": "text", "text": "Training and evaluation During training, we only use the data from the training horizon $\\left(I n{-}t\\right)$ . At test time, we evaluate the models to unroll the dynamics for new initial conditions in the training horizon $(I n{-}t)$ and for temporal extrapolation (Out-t). ", "page_idx": 6}, {"type": "text", "text": "Results Table 2 demonstrates that AROMA consistently achieves low MSE across all levels of observation sparsity and evaluation horizons for both datasets. Overall, our method performs best with some exceptions. On Shallow-Water our model is slightly outperformed by CORAL in the fully observed regime, potentially because of a lack of data. Similarly, on Navier-Stokes $1\\times10^{-3}$ CORAL has slightly better scores in the very sparse regime $\\pi=5\\%$ . Overall, this is not surprising as meta-learning models excel in data-constrained regimes. We believe our geometry-encoding block is crucial for obtaining good representations of the observed values in the sparse regimes, potentially explaining the performance gap with GNOT and OFormer. ", "page_idx": 6}, {"type": "table", "img_path": "Aj8RKCGwjE/tmp/930506e195ae2e4fa81bc73d87c777652b7a4c119ddee6927042313f8e8c3671.jpg", "table_caption": ["Table 2: Temporal Extrapolation - Test results. Metrics in MSE. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.3 Dynamics on different geometries ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Finally, we extend our analysis to learning dynamics over varying geometries. ", "page_idx": 6}, {"type": "text", "text": "Datasets We evaluate our model on two problems involving non-convex domains, as described by Pfaff et al. (2021). Both scenarios involve fluid dynamics in a domain with an obstacle, where the area near the boundary conditions (BC) is more finely discretized. The boundary conditions are specified by the mesh, and the models are trained with various obstacles and tested on different, yet similar, obstacles. \u2022 Cylinder (CylinderFlow): This dataset simulates water flow around a cylinder using a fixed 2D Eulerian mesh, representing incompressible fluids. For each node $j$ in the mesh $\\mathcal{X}$ , we have data on the node position $x^{(j)}$ , momentum $w(x^{(j)})$ , and pressure $p(x^{(j)})$ . from ", "page_idx": 7}, {"type": "text", "text": "Our task is to learn the mapping from (wt(x), pt(x))x\u2208X to for a fixed $\\Delta t$ . \u2022 Airfoil (AirfoilFlow): This dataset simulates the aerodynamics around an airfoil, relevant for compressible fluids. In addition to the data available in the Cylinder dataset, we also have the fluid density $\\rho(x^{(j)})$ for each node $j$ . Our goal is to learn the mapping from $(w_{t}(x),p_{t}(x),\\rho_{t}(x))_{x\\in\\mathcal{X}}$ to $(w_{t+\\Delta t}(x),p_{t+\\Delta t}(x),\\rho_{t+\\Delta t}(x))_{x\\in\\mathcal{X}}$ . Each example in the dataset corresponds to a unique mesh. On average, there are 5233 nodes per mesh for AirfoilFlow and 1885 for CylinderFlow. We temporally subsample the original trajectories by taking one timestamp out of 10, forming trajectories of 60 timestamps. We use the first 40 timestamps for training $(I n{-}t)$ and keep the last 20 timestamps for evaluation (Out-t). ", "page_idx": 7}, {"type": "text", "text": "Setting We train all the models with supervision on the next state prediction. At test time, we unroll the dynamics auto-regressively with each model and evaluate the prediction with a mean squared error (MSE) both in the training horizon $(I n{-}t)$ and beyond the training horizon (Out-t). ", "page_idx": 7}, {"type": "text", "text": "Results The results in Table 3 show that AROMA outperforms other models in predicting flow dynamics on both CylinderFlow and AirfoilFlow geometries, achieving the lowest MSE values across all tests. This indicates AROMA\u2019s superior ability to encode geometric features accurately. Additionally, AROMA maintains stability over extended prediction horizons, as evidenced by its consistently low Out-t MSE values. ", "page_idx": 7}, {"type": "table", "img_path": "Aj8RKCGwjE/tmp/db4bd56dd047e052c194a64f27659ff6989cfa1bbce60d2150a2c6fa5d36254c.jpg", "table_caption": ["Table 3: Dynamics on different geometries - Test results. MSE on normalized data. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.4 Long rollouts and uncertainty quantification ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "After training different models on Burgers, we compare them on long trajectory rollouts. We start from $t_{0}\\,=\\,50$ (i.e. use a numerical solver for 50 steps), and unroll our dynamics auto-regressively for 200 steps. Note that all the models were only trained to predict the next state. We plot the correlation over rollout steps of different methods, including our model without the diffusion process, in Figure 3. We can clearly see the gain in stability in using the diffusion for long rollouts. Still, the predictions will eventually become uncorrelated over time as the solver accumulates errors compared with the numerical solution. As we employ a generative model, we can generate several rollouts and estimate the uncertainty of the solver with standard deviations. We can see in Appendix Figure 11 that this uncertainty increases over time. This uncertainty is not a guarantee that the solution lies within the bounds, but is an indication that the model is not confident in its predictions. ", "page_idx": 7}, {"type": "image", "img_path": "Aj8RKCGwjE/tmp/d36d930ae37348d919f466441a9fe6a7f667adbf62c7feaff59f619e30e23834.jpg", "img_caption": ["Figure 3: Correlation over time for long rollouts with different methods on Burgers "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Our model differs from existing models in the field of operator learning and more broadly from existing neural field architectures. The works most related to ours are the following. ", "page_idx": 8}, {"type": "text", "text": "Neural Fields for PDE Neural Fields have recently emerged as powerful tools to model dynamical systems. DINO (Yin et al., 2022) is a space-time continuous architecture based on a modulated multiplicative filter network (Fathony et al., 2021) and a NeuralODE (Chen & Zhang, 2019) for modeling the dynamics. DINO is capable of encoding and decoding physical states on irregular grids thanks to the spatial continuity of the INR and through auto-decoding (Park et al., 2019). CORAL is another neural-field based architecture, which tackles the broader scope of operator learning, also builds on meta-learning (Zintgraf et al., 2019; Dupont et al., 2022) to freely process irregular grids. CORAL and DINO are the most similar works to ours, as they are both auto-regressive and capable of processing irregular grids. On the other hand Chen et al. (2022) and Hagnberger et al. (2024) make use of spatio-temporal Neural Fields, for obtaining smooth and compact latent representations in the first or to directly predict trajectory solutions within a temporal horizon in the latter. Moreover, they either use a CNN or rely on patches for encoding the observations and are therefore not equipped for the type of tasks AROMA is designed for. ", "page_idx": 8}, {"type": "text", "text": "Transformers for PDE Several PDE solvers leverage transformers and cross-attention as a backbone for modeling PDEs. Transformers, which operate on token sequences, provide a natural solution for handling irregular meshes and point sets. Li et al. (2023) and Hao et al. (2023) introduced transformer architectures tailored for operator learning. Hao et al. (2023) incorporated an attention mechanism and employed a mixture of experts strategy to address multi-scale challenges. However, their architecture relies on linear attention without reducing spatial dimensions, resulting in linear complexity in sequence size, but quadratic in the hidden dimensions, which can be prohibitive for deep networks and large networks. Similarly, Li et al. (2023) utilized cross-attention to embed both regular and irregular meshes into a latent space and applied a recurrent network for time-marching in this latent space. Nonetheless, like GNOT, their method operates point-wise on the latent space. Transolver (Wu et al., 2024) decomposes a discrete input function into a mixture of \u201dslices,\u201d each corresponding to a prototype in a mixture model, with attention operating in this latent space. This approach, akin to our model, reduces complexity. However, it has not been designed for temporal problems. (Alkin et al., 2024) recently proposed a versatile model capable of operating on Eulerian and Lagrangian (particles) representations. They reduce input dimensionality by aggregating information from input values onto \u201dsupernodes\u201d selected from the input mesh via message passing while decoding is performed with a Perceiver-like architecture. In contrast, AROMA performs implicit spatial encoding with cross-attention to encode the geometry and aggregate obsevation values. Finally, their training involves complex end-to-end optimization, whereas we favor two simple training steps that are easier to implement. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusion and Limitations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "AROMA offers a novel and flexible neural operator approach for modeling the spatio-temporal evolution of physical processes. It is able to deal with general geometries and to forecast at any position of the spatial domain. It incorporates in an encode-process-decode framework attention mechanisms, a latent diffusion transformer for spatio-temporal dynamics and neural fields for decoding. Thanks to a very compact spatial encoding, its complexity is lower than most SOTA models. Experiments with small-size datasets demonstrate its effectiveness. Its reduced complexity holds potential for effective scaling to larger datasets. As for the limitations, the performance of AROMA are still to be demonstrated on larger and real world examples. Moreover, like all dynamical models that operate over a latent space, the reconstruction capabilities of the decoder is a bottleneck for the rollout accuracy. Since the encoder and decoder are learning spatial relationships from scratch, conferring the framework a high flexibility, the training efficiency does not match that of CNN-based autoencoders on regular grids. We therefore believe there could be further improvements to be made to achieve a similar performance while keeping the same level of flexibility. Finally, even though our model has some potential for uncertainty modeling, this aspect has still to be further explored and analyzed. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We acknowledge the financial support provided by DL4CLIM (ANR-19-CHIA-0018-01), DEEPNUM (ANR-21-CE23-0017-02), PHLUSIM (ANR-23-CE23-0025-02), and PEPR Sharp (ANR-23- PEIA-0008, ANR, FRANCE 2030). This work was granted access to the HPC resources of IDRIS under the allocations 2023-AD011013522R1, 2023-AD011013332R1, 2023-AD011015133R1, 2023-A0161015133 made by GENCI. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Benedikt Alkin, Andreas Fu\u00a8rst, Simon Schmid, Lukas Gruber, Markus Holzleitner, and Johannes Brandstetter. Universal Physics Transformers. 2024. URL http://arxiv.org/abs/2402. 12365. ", "page_idx": 9}, {"type": "text", "text": "Matthias Bauer, Emilien Dupont, Andy Brock, Dan Rosenbaum, Jonathan Schwarz, and Hyunjik Kim. Spatial functa: Scaling functa to imagenet classification and generation. CoRR, abs/2302.03130, 2023. ", "page_idx": 9}, {"type": "text", "text": "Johannes Brandstetter, Daniel E Worrall, and Max Welling. Message passing neural pde solvers. International Conference on Learning Representations, 2022. ", "page_idx": 9}, {"type": "text", "text": "Peter Yichen Chen, Jinxu Xiang, Dong Heon Cho, Yue Chang, G A Pershing, Henrique Teles Maia, Maurizio Chiaramonte, Kevin Carlberg, and Eitan Grinspun. Crom: Continuous reduced-order modeling of pdes using implicit neural representations. International Conference on Learning Representation, 6 2022. URL http://arxiv.org/abs/2206.02607. ", "page_idx": 9}, {"type": "text", "text": "Zhiqin Chen and Hao Zhang. Learning implicit fields for generative shape modeling. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2019-June, 2019. ISSN 10636919. doi: 10.1109/CVPR.2019.00609. ", "page_idx": 9}, {"type": "text", "text": "Emilien Dupont, Hyunjik Kim, S. M. Ali Eslami, Danilo Rezende, and Dan Rosenbaum. From data to functa: Your data point is a function and you can treat it like one. Proceedings of the 39 th International Conference on Machine Learning, 1 2022. URL http://arxiv.org/abs/ 2201.12204. ", "page_idx": 9}, {"type": "text", "text": "Rizal Fathony, Anit Kumar Sahu, Devin Willmott, and J Zico Kolter. Multiplicative filter networks. International Conference on Learning Representations., 2021. ", "page_idx": 9}, {"type": "text", "text": "Jan Hagnberger, Marimuthu Kalimuthu, Daniel Musekamp, and Mathias Niepert. Vectorized Conditional Neural Fields: A Framework for Solving Time-dependent Parametric Partial Differential Equations. In Proceedings of the 41st International Conference on Machine Learning (ICML 2024), 2024. ", "page_idx": 9}, {"type": "text", "text": "Zhongkai Hao, Zhengyi Wang, Hang Su, Chengyang Ying, Yinpeng Dong, Songming Liu, Ze Cheng, Jian Song, and Jun Zhu. Gnot: A general neural operator transformer for operator learning. In International Conference on Machine Learning, pp. 12556\u201312569. PMLR, 2023. ", "page_idx": 9}, {"type": "text", "text": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016. ", "page_idx": 9}, {"type": "text", "text": "Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020. ", "page_idx": 9}, {"type": "text", "text": "Diederik $\\mathbf{P}$ Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. ", "page_idx": 9}, {"type": "text", "text": "Doyup Lee, Chiheon Kim, Minsu Cho, and Wook-Shin Han. Locality-aware generalizable implicit neural representation. Advances in Neural Information Processing Systems, 2023. ", "page_idx": 9}, {"type": "text", "text": "Zijie Li, Kazem Meidani, and Amir Barati Farimani. Transformer for partial differential equations\u2019 operator learning. Transactions on Machine Learning Research (April/2023), 2023. ", "page_idx": 9}, {"type": "text", "text": "Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations. International Conference on Learning Representations., 10 2021. URL http://arxiv.org/abs/2010.08895.   \nPhillip Lippe, Bastiaan S Veeling, Paris Perdikaris, Richard E Turner, and Johannes Brandstetter. Pde-refiner: Achieving accurate long rollouts with neural pde solvers. arXiv preprint arXiv:2308.05732, 2023.   \nLu Lu, Pengzhan Jin, and George Em Karniadakis. Deeponet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators. Nat Mach Intell, 3:218\u2013229, 10 2021. doi: 10.1038/s42256-021-00302-5. URL http://arxiv. org/abs/1910.03193http://dx.doi.org/10.1038/s42256-021-00302-5.   \nAaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. arXiv preprint arXiv:1711.00937, 2017.   \nJeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2019-June, 2019. ISSN 10636919. doi: 10.1109/CVPR.2019.00025.   \nJaideep Pathak, Shashank Subramanian, Peter Harrington, Sanjeev Raja, Ashesh Chattopadhyay, Morteza Mardani, Thorsten Kurth, David Hall, Zongyi Li, Kamyar Azizzadenesheli, et al. Fourcastnet: A global data-driven high-resolution weather model using adaptive fourier neural operators. arXiv preprint arXiv:2202.11214, 2022.   \nWilliam Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 4195\u20134205, 2023.   \nTobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez, and Peter W. Battaglia. Learning meshbased simulation with graph networks. International Conference on Learning Representations., 10 2021. URL http://arxiv.org/abs/2010.03409.   \nMichal Rolinek, Dominik Zietlow, and Georg Martius. Variational autoencoders pursue pca directions (by accident). In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12406\u201312415, 2019.   \nSalva Ru\u00a8hling Cachay, Bo Zhao, Hailey Joren, and Rose Yu. DYffusion: a dynamics-informed diffusion model for spatiotemporal forecasting. In Advances in Neural Information Processing Systems (NeurIPS), 2023.   \nTim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512, 2022.   \nLouis Serrano, Lise Le Boudec, Armand Kassa\u00a8\u0131 Koupa\u00a8\u0131, Thomas X Wang, Yuan Yin, Jean-Noe\u00a8l Vittaut, and Patrick Gallinari. Operator learning with neural fields: Tackling pdes on general geometries. Advances in Neural Information Processing Systems, 2023.   \nKimberly Stachenfeld, Drummond B. Fielding, Dmitrii Kochkov, Miles Cranmer, Tobias Pfaff, Jonathan Godwin, Can Cui, Shirley Ho, Peter Battaglia, and Alvaro Sanchez-Gonzalez. Learned coarse models for efficient turbulence simulation. International Conference on Learning Representation, 2022.   \nRicardo Vinuesa and Steven L Brunton. Enhancing computational fluid dynamics with machine learning. Nature Computational Science, 2(6):358\u2013366, 2022.   \nHaixu Wu, Huakun Luo, Haowen Wang, Jianmin Wang, and Mingsheng Long. Transolver: A Fast Transformer Solver for PDEs on General Geometries. 2024. URL http://arxiv.org/abs/ 2402.02366.   \nYuan Yin, Matthieu Kirchmeyer, Jean-Yves Franceschi, Alain Rakotomamonjy, and Patrick Gallinari. Continuous pde dynamics forecasting with implicit neural representations. International Conference on Learning Representations, 9 2022. URL http://arxiv.org/abs/2209. 14855. ", "page_idx": 10}, {"type": "text", "text": "Luisa Zintgraf, Kyriacos Shiarlis, Vitaly Kurin, Katja Hofmann, and Shimon Whiteson. Fast context adaptation via meta-learning. 36th International Conference on Machine Learning, ICML 2019, 2019-June, 2019. ", "page_idx": 11}, {"type": "text", "text": "A Extended Related Work", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Diffusion models for PDE Recently, diffusion models have experienced significant growth and success in generative tasks, such as image or video generation (Ho et al., 2020). This success has motivated their application to physics prediction. Ru\u00a8hling Cachay et al. (2023) propose DYffusion, a framework that adapts the diffusion process to spatio-temporal data for forecasting on long-time rollouts, by performing diffusion-like timesteps in the physical time dimension. PDE-Refiner (Lippe et al., 2023) is a CNN-based method that uses diffusion to stabilize prediction rollouts over long trajectories. Compared to these methods, we perform diffusion in a latent space, reducing the computational cost; and leverage the advanced modeling capabilities of transformers. ", "page_idx": 11}, {"type": "text", "text": "Local Neural Fields We are not the first work that proposes to leverage locality to improve the design of neural fields. In a different approach, Bauer et al. (2023) proposed a grid-based latent space where the modulation function $\\phi$ is dependent on the query coordinate $x$ . This concept enables the application of architectures with spatial inductive biases for generation on the latent representations, such as a U-Net Denoiser for diffusion processes. Similarly, Lee et al. (2023) developed a locality-aware, generalizable Implicit Neural Representation (INR) with demonstrated capabilities in generative modeling. Both of these architectures assume regular input structures, be it through patching methods or grid-based layouts. ", "page_idx": 11}, {"type": "text", "text": "B Implementation details ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Diffusion transformer We illustrate how our diffusion transformer is trained and used at inference in Figure 4 and Figure 5. We provide the diffusion step $k$ which acts as a conditioning input for the diffusion model. We use an exponential decrease for the noise level as in Lippe et al. (2023) i.e. \u03b1k = 1 \u2212 \u03c3kmi/nK . We use the same diffusion transformer block as in Peebles & Xie (2023), which relies on amplitude and shift modulations from the diffusion timestamp $k$ : ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha^{(1)},\\beta^{(1)},\\gamma^{(1)}\\gets\\mathbf{MLP}_{1}(k)}\\\\ &{\\alpha^{(2)},\\beta^{(2)},\\gamma^{(2)}\\gets\\mathbf{MLP}_{2}(k)}\\\\ &{\\qquad\\qquad\\pmb{Z}_{[l+1]}\\gets\\pmb{Z}_{[l]}+\\alpha^{(1)}\\cdot\\tt A t t e n t i o n(\\gamma^{(1)}\\cdot\\tt L a y e r N o r m(\\pmb{Z}_{[l]})+\\beta^{(1)})}\\\\ &{\\qquad\\qquad\\pmb{Z}_{[l+1]}\\gets\\pmb{Z}_{[l+1]}+\\alpha^{(2)}\\cdot\\mathtt{F F N}(\\gamma^{(2)}\\cdot\\tt L a y e r N o r m(\\pmb{Z}_{[l+1]}+\\beta^{(2)})}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "image", "img_path": "Aj8RKCGwjE/tmp/104591e9c749f1db54c707afbf099db4f6f6042b0b8845c1fafb4bfb4c069c48.jpg", "img_caption": ["Figure 4: During training, we noise the next-step latent tokens $Z^{t+\\Delta t}$ and train the transformer to predict the \u201cvelocity\u201d of the noise. Each DIT block is implemented as in Peebles & Xie (2023). ", "previouslatent token noisy estimate diffusion step "], "img_footnote": [], "page_idx": 12}, {"type": "image", "img_path": "Aj8RKCGwjE/tmp/fdc83e6daa72cb342481ef9c8e2cee59d091ec188cfe6a5993229b0985a2c596.jpg", "img_caption": [], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "Figure 5: At inference, we start from $\\tilde{\\boldsymbol{Z}}_{K}^{t+\\Delta t}\\sim\\mathcal{N}(\\boldsymbol{0},\\boldsymbol{I})$ and reverse the diffusion process to denoise our prediction. We set our prediction $\\hat{\\pmb{Z}}^{t+\\Delta t}=\\tilde{\\pmb{Z}}_{0}^{t+\\Delta t}$ . ", "page_idx": 12}, {"type": "text", "text": "Encoder-Decoder We provide a more detailed description of the encoder-decoder pipeline in Figure 6. ", "page_idx": 12}, {"type": "text", "text": "Local INR We show the implementation of our local INR, both with single-band frequency and multi-band frequency, in Figure 7 and Figure 8. The cross-attention mechanism enables to retrieve a local feature vector $\\mathbf{f}_{q}(x)$ for each query position $x$ . We then use an MLP to decode this feature vector to retrieve the output value. In practice, we retrieve several feature vectors corresponding each to separate frequency bandwidths. In this case, we concatenate the feature vectors before decoding them with the MLP. ", "page_idx": 12}, {"type": "image", "img_path": "Aj8RKCGwjE/tmp/21101d447ac4bebbe0b91262ac5f96c0fbf26241e4d6dcc2025e2f8324e62c16.jpg", "img_caption": [], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "Figure 6: Architecture of our encoder and decoder. We regularize the architecture as a variational auto-encoder. Cross-attention layers are used to aggregate the $N$ observations into $M$ latent tokens, and to expand the $M$ processed tokens to the queried values. We use a bottleneck layer to reduce the channel dimension of the latent space. ", "page_idx": 13}, {"type": "image", "img_path": "Aj8RKCGwjE/tmp/935ab04d0c409301e6e1cd8beb5ad28c51491532439ae7f1d46f2218405c62b5.jpg", "img_caption": ["Figure 7: Single-band local INR decoder "], "img_footnote": [], "page_idx": 13}, {"type": "image", "img_path": "Aj8RKCGwjE/tmp/525c96fd8b7144e921ff22064bd26766231a71eb9acfc06bda1cd94c0724f771.jpg", "img_caption": ["Figure 8: Multi-band local INR decoder "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "B.1 Hyperparameters ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We detail the values of the hyperparameters used on each dataset: Table 5 presents the hyperparameters of the Encoder-Decoder, while Table 4 presents the hyperparameters of the Diffusion Transformer. We use a cosine scheduler for the tuning learning rate for both trainings, with an initial maximum learning rate of $10^{-3}$ annealing to $10^{-5}$ . All experiments were performed with an NVIDIA TITAN RTX. ", "page_idx": 15}, {"type": "text", "text": "For the diffusion transformer, we use $K=3$ diffusion steps for all experiments and only vary the minimum noise $\\sigma_{\\mathrm{min}}$ . ", "page_idx": 15}, {"type": "table", "img_path": "Aj8RKCGwjE/tmp/a14e0545068b2672864b481c6e930d1b5ca2da09e275de3a4198f8cd641a1573.jpg", "table_caption": ["Table 4: Diffusion Transformer Hyperparameters for Different Datasets "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "For the encoder-decoder, we have the following hyperparameters: ", "page_idx": 15}, {"type": "text", "text": "\u2022 hidden dim: The number $h$ of neurons at each hidden layer.   \n\u2022 num self attentions: The number of Self Attention layers used for the decoder.   \n\u2022 num latents: The number $M$ of latent tokens used to spatially project the objervations and geometries.   \n\u2022 latent dim: The dimension $c$ of each latent token.   \n\u2022 latent heads: The number of heads use for the Self Attention layers.   \n\u2022 latent dim head: The dimension of each head in a Self Attention layer.   \n\u2022 cross heads: The number of heads use for the Cross Attention layers.   \n\u2022 cross dim head: The dimension of each head in a Cross Attention layer.   \n\u2022 dim: The number of neurons used in the MLP decoder.   \n\u2022 depth inr: The number of layers in the MLP decoder.   \n\u2022 frequencies: The different frequencies used for the local INR. We use base 2 for all experiments and select 16 frequencies in logarithmic scale per level. For example, [3, 4, 5] means that we construct 3 frequency embedding vectors, the first $\\begin{array}{r l r}{\\gamma_{1}(\\bar{x})}&{{}=}&{\\bar{(}\\cos(2^{0}\\pi x),\\sin(2^{0}\\pi x),\\ldots,\\cos(2^{3}\\pi\\dot{x}),\\sin(2^{3}\\pi x))}\\end{array}$ , for the second \u03b32 = ( $\\begin{array}{r l}{=}&{{}{\\bigl(}\\cos(2^{3}\\pi x),\\sin(2^{3}\\pi x),\\ldots,\\cos(2^{4}\\pi x),}\\end{array}$ $\\sin(2^{4}\\pi x)\\,!$ ), and for the third $\\gamma_{3}\\quad=$ ( $\\cos(2^{4}\\pi x),\\sin(2^{4}\\pi x),\\ldots,\\cos(2^{5}\\pi x),\\sin(2^{5}\\pi x))$   \n\u2022 dropout sequence: The ratio of points that are ignored by the encoder.   \n\u2022 feature dim: The dimension of the feature vector.   \n\u2022 encode geo: If we use a cross-attention block to encode the geometry.   \n\u2022 max encoding freq: The maximum frequency used for the frequency embedding $\\gamma$ of the encoder.   \n\u2022 kl weight: The weight $\\beta$ used for the VAE training.   \n\u2022 epochs: Number of training epochs. ", "page_idx": 15}, {"type": "text", "text": "The most important hyperparameter of the encoder-decoder is the number of tokens $M$ that are used to aggregate the observations and geometries. We show the impact it has on the quality of reconstructions in Table 6. ", "page_idx": 15}, {"type": "table", "img_path": "Aj8RKCGwjE/tmp/912096cad6d84ebf41a87d854cbeb855443407894d7cf250cd7161acfc67d388.jpg", "table_caption": ["Table 5: Hyperparameters of the Encoder-Decoder for Different Datasets "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "C Additional results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "C.1 Time complexity analysis ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We denote $N$ as the number of observations of $\\textbf{\\em u}$ , $M$ as the number of tokens used to compress the information, $T$ as the number of autoregressive calls in the rollout, $K$ as the number of refinement steps, and $d$ as the number of channels used in the attention mechanism. The most computationally expensive operations in our architecture are the cross-attention and self-attention blocks. For simplification, we omit the geometry encoding block in this study. ", "page_idx": 16}, {"type": "text", "text": "The cost of the cross-attention in the encoder is $O(N M d)$ , and similarly, the cost of the crossattention in the decoder is $O(N M d)$ . Let $L_{1}$ and $L_{2}$ represent the number of layers in the decoder and diffusion transformer, respectively. The cost of the self-attention layers in the decoder is $O(L_{1}M^{2}d)$ , while in the diffusion transformer, it is $O(4L_{2}M^{2}d)$ . ", "page_idx": 16}, {"type": "text", "text": "To unroll the dynamics, we encode the initial condition, obtain the predictions in the latent space, and then decode in parallel, yielding a total cost of $O((2N+4K T L_{2}M+L_{1}M)M d)$ . As expected, our architecture has linear complexity in the number of observations through the cross-attention layers. In contrast, GNOT relies on linear attention, resulting in a time complexity of $O((L N)d^{2})$ for each prediction, where $L$ is the depth of the network. At inference, the cost per step along a trajectory is $\\dot{L}N d^{2}$ for GNOT, compared to $4K L_{2}M^{2}d$ for AROMA. ", "page_idx": 16}, {"type": "text", "text": "For instance, using $K\\,=\\,3$ , $M\\,=\\,64$ , $N\\,=\\,4096$ , and $d\\,=\\,128$ , GNOT\u2019s cost is approximately 10 times that of AROMA for each prediction throughout the rollout. Therefore AROMA is more efficient when $M\\ll N$ . ", "page_idx": 16}, {"type": "text", "text": "C.2 Encoding interpretation ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We provide in Figure 9 a qualitative analysis through cross-attention visualizations how the geometry encoding block helps to capture the geometry of the domain. In the first cross-attention block, the query tokens $\\mathbf{T}$ are not aware of the geometry and therefore attend to large regions of the domains. This lets the model understand, where the boundaries of the domain are and therefore where the cylinder is. Once the query tokens have aggregated the mesh information, the cross attention between $\\mathbf{T}^{\\mathrm{geo}}$ and the positions are sharper and depend on the geometry. ", "page_idx": 16}, {"type": "text", "text": "C.3 Example rollouts ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We show examples of rollout predictions using AROMA on Burgers dataset in Figure 10, on NavierStokes $1\\!\\times\\!10^{-3}$ dataset in Figure 12 and on CylinderFlow in Figure 13. AROMA returns predictions that remain stable and accurate, even outside the training time horizon. ", "page_idx": 16}, {"type": "image", "img_path": "Aj8RKCGwjE/tmp/48e2290410c8539c0ac4c587a4790313a0999535c665ecd9b04285219a1faf4f.jpg", "img_caption": ["Figure 9: Evolution of the cross-attention maps between the geometry encoding stage and the observation encoding stage. Blue means the cross-attention value is close to zero while yellow means the cross-attention score is close to one. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "Aj8RKCGwjE/tmp/bb583028aba26652c6e006507ef3ac3248d0f9eadc0e51a530d5493e82cfa81a.jpg", "img_caption": ["Figure 10: Test example long rollout trajectory with AROMA on Burgers. Left is the predicted trajectory and right is the ground truth. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "Aj8RKCGwjE/tmp/e1aa901e3a36a0acd9dd41c38043ae2f017b580fd01f46897246cffca36c3b52.jpg", "img_caption": ["Figure 11: Uncertainty of AROMA over rollout steps. The blue line is the mean prediction while the blue shade represents the mean $\\pm\\,3\\times$ standard deviation. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "For Navier-Stokes, we show an example of test trajectory in the training horizon (Figure 12a) and in extrapolation (Figure 12b). ", "page_idx": 18}, {"type": "image", "img_path": "Aj8RKCGwjE/tmp/9c6dad32d634c22ad9f85089db481c555a454214e98ece9888e293b7f0079d3a.jpg", "img_caption": ["(a) In-t "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "Aj8RKCGwjE/tmp/2f613eb590704edb372e129162338ec5c3f1e2849ea0d7c12eb9d4b231555c0a.jpg", "img_caption": ["(b) Out-t "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure 12: Test example rollout trajectories with AROMA on Navier-Stokes $1\\times10^{-3}$ . Top: predicted trajectory on In-t. Bottom: trajectory on Out-t. First row in each subfigure shows the prediction, the second row shows the ground truth. ", "page_idx": 18}, {"type": "image", "img_path": "Aj8RKCGwjE/tmp/d12cae7f9a28de933c6cc84f7b90f881a76f78785587a0f54f70dea1f87a2544.jpg", "img_caption": [], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure 13: Visualization of AROMA\u2019s predictions on Cylinder for (Out-t). The left panel shows the prediction, the middle panel displays the ground truth, and the right panel is the point-wise error. ", "page_idx": 18}, {"type": "text", "text": "C.4 Scaling experiments ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In Figure 14, we compare the reconstruction and prediction capabilities of CORAL and AROMA on Navier-Stokes $1\\times\\bar{1}0^{-4}$ given the number of training trajectories. As evidenced, our architecture outperforms CORAL significantly when the number of trajectories is greater than $10^{3}$ , highlighting its efficacy in handling large amounts of data.. ", "page_idx": 19}, {"type": "image", "img_path": "Aj8RKCGwjE/tmp/79941c3fa2e3015d8f55d80e0fb7853fb511329ed5c83a5d72615fdbfb7dd3c5.jpg", "img_caption": ["Figure 14: Scaling comparison of AROMA & CORAL: relative $L_{2}$ error with respect to the number of training trajectories "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "C.5 Spatial tokens perturbation analysis ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "To validate the spatial interpretation of our latent tokens, we establish a baseline code $Z^{0}$ , and introduce perturbations by sequentially replacing the $j$ -th token, $z_{j}^{0}$ , with subsequent tokens along the trajectory, denoted as $z_{j}^{1},z_{j}^{2},\\dots,z_{j}^{t}$ . Thus, the perturbed tokens mirror $Z^{0}$ in all aspects except for the $j$ -th token, which evolves according to the true token dynamics. We show reconstruction visualizations of the perturbed tokens in figs. 15 to 22. On the right side, we show the groundtruth of the trajectory. On the left side, is the change in AROMA\u2019s prediction in response to the token perturbation. These figures show that the perturbation of a token only impacts the reconstructed field locally, which validates the spatial structure of our tokens. Additionally, we can notice some interesting effects of the token perturbations near the boundaries in figs. 17 and 22: our encoderdecoder has discovered from data and without explicit supervision that the solutions had periodic boundary conditions by leveraging the encoded geometry and the function values. This validates the architecture of our cross-attention module between the function values, the spatial coordinates and the geometry-aware tokens. ", "page_idx": 19}, {"type": "image", "img_path": "Aj8RKCGwjE/tmp/de9ea767a3e7f9efbadb8e9f56fb39b5bdd650356b360cad0ec3bea4c08a1136.jpg", "img_caption": ["Figure 15: Perturbation analysis on Burgers. Token 0. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "Aj8RKCGwjE/tmp/d5456e4736d20e73038e9394dc13a8073cd212dbeda248013e88627d47c655d5.jpg", "img_caption": ["Figure 16: Perturbation analysis on Burgers. Token 1. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "Aj8RKCGwjE/tmp/9e85f0589637a0606143328541064cc53bacc1306314edfafd584e4e80825bf1.jpg", "img_caption": ["Figure 17: Perturbation analysis on Burgers. Token 2. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "Aj8RKCGwjE/tmp/9e04e9399f28f08b11fda4fae3f19bee1b1e171ae88891ea5875a51dae522a2a.jpg", "img_caption": ["Figure 18: Perturbation analysis on Burgers. Token 3. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "Aj8RKCGwjE/tmp/a0ef383350334c4633dc720a940324a2226a18dff478adaa07dc1fe707e03dc4.jpg", "img_caption": ["Figure 19: Perturbation analysis on Burgers. Token 5. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "Aj8RKCGwjE/tmp/30ae37dce655cf1e27f86d1ce6b6886ab82d302cb89f182fe00f919452b61606.jpg", "img_caption": ["Figure 20: Perturbation analysis on Burgers. Token 6. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "Aj8RKCGwjE/tmp/db180f3e3acbdd3f24195aa7898b1aa1f4abff94007b124623b282874d4441d3.jpg", "img_caption": ["Figure 21: Perturbation analysis on Burgers. Token 7. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "Aj8RKCGwjE/tmp/14adcd7b554a14d95d4237b3e8f70a81ec13f455abeeeb7e66a4007bdd6e761e.jpg", "img_caption": ["Figure 22: Perturbation analysis on Burgers. Token 8. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "C.6 Ablation studies ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Number of tokens $M$ We show the impact of the number of latent tokens on the Navier-Stokes $1\\times10^{-4}$ dataset in Table 6. We train our auto-encoder with 10000 trajectories. We can see that the performance increases with the number of tokens. ", "page_idx": 22}, {"type": "table", "img_path": "Aj8RKCGwjE/tmp/2971f8b3f64cbd85956c2f87edd4ea2eed4c5437b58b3879e4233c2a5ab89c36.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Table 6: Influence of the number of latent tokens on the test reconstruction capabilities on NavierStokes $1\\times10^{-4}$ . Performance in Relative $L_{2}$ Error. ", "page_idx": 22}, {"type": "text", "text": "Auto-encoding vs VAE Our framework can also be used without the KL regularization, and could potentially be employed with other forms of regularization, such as L2 regularizaton or vectorquantization (Oord et al., 2017). We investigated in Table 7 the impact the KL regularization had on the overall rollout performance, and selected an autoencoder with L2 regularization (weight decay) as baseline. Our conclusion is that using an autoencoder with L2 regularization is a viable alternative to the VAE in some cases for achieving a smooth latent space. The autoencoder demonstrated superior performance on two datasets (Burgers and Navier-Stokes $1\\times10^{-4}$ , explained by its lower reconstruction errors, which translate into better rollout performance. However, for the more challenging Navier-Stokes $1\\times10^{-5}$ case, the autoencoder\u2019s latent space exhibited high variance, which may explain the observed performance difference with the VAE. ", "page_idx": 22}, {"type": "text", "text": "No-diffusion vs diffusion As an ablation, we also measured the influence of the diffusion formulation on the rollout accuracy by comparing to the same transformer architecture trained directly with an MSE on the mean tokens. The deterministic version of AROMA shows consistently robust performance and even surpasses the diffusion version on the Navier-Stokes $1\\times10^{-4}$ case (Table 7). This demonstrates that the latent tokens obtained with AROMA contain meaningful information for dynamics modeling. On the other hand, the deterministic version yields less accurate long rollouts on Burgers or $K S$ in Figure 23 and Figure 3. Note that using diffusion allows us to model the trajectory distribution, which opens the way to infer statistics on this distribution. This is key, for example, when modeling uncertainty, which is a critical problem for these models. ", "page_idx": 22}, {"type": "text", "text": "Latent MLP vs Latent Transformer Modeling interactions at the local and global levels is key to learn the dynamics faithfully. Experiments using MLPs (Table 7) as time steppers which do not consider interactions between tokens lead to significantly lower performance compared to transformers. ", "page_idx": 22}, {"type": "table", "img_path": "Aj8RKCGwjE/tmp/fb78d140b8710e49f188f95afcce680882870ec0a9948a1a7e1ae2e9afe8420e.jpg", "table_caption": ["Table 7: Ablation Study. Metrics in Relative $L_{2}$ on the test set. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "C.7 Kuramoto-Sivashinsky : a failure case ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We conducted additional experiments on a chaotic 1D PDE, the Kuramoto-Sivashinsky $(K S)$ equation. We found that AROMA currently struggles with dynamics that exhibit chaotic phenomena and non-decaying spectra, as shown in Figure 23. The primary limitation appears to be the reconstruction capabilities of the encoder-decoder. For the $K S$ equation, we found that obtaining reconstructions with an MSE in the range of 1e-10 to 1e-12 was necessary for accurate spectrum reconstruction. Like all models leveraging a reduced latent representation space, AROMA inherently loses some of the fine-grained details necessary for accurately capturing chaotic behavior. The diffusion framework slightly improves the high correlation time compared to the deterministic version, however the main bottleneck comes from the decoder (Table 8). In conclusion, while AROMA performs very well on simpler dynamics, dealing with chaotic phenomena requires more involved modeling that explicitly targets the chaotic component. Note that using dedicated modules for this purpose is a current practice in fluid dynamics - e.g. LES (Large eddy simulation). ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "table", "img_path": "Aj8RKCGwjE/tmp/9388fb9d4aeb36d4f3a524445a7de6ee4e406e07cb1ce4f303713a566609316f.jpg", "table_caption": ["Table 8: Test results on the KS equation. The evaluated metrics include: 1-step prediction MSE, MSE over the entire rollout (160 timestamps), and the duration for which the correlation between the generated samples and the ground truth remains above 0.8. "], "table_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "Aj8RKCGwjE/tmp/1efcbe8b87cdd95ffb919f9338d53c111a1a0e08c4b39037451ae70d208f9eb0.jpg", "img_caption": ["(a) Spectrum of AROMA\u2019s 1-step predictions vs (b) Comparison of the MSE loss (\u2193) with and without ground truth. diffusion. ", "Figure 23: Qualitative results on KS equation. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "C.8 Latent space dynamics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "For Navier-Stokes, we show how the mean (Figure 24) and standard deviation tokens (Figure 25) evolve over time for a given test trajectory. We show the predicted trajectory of the latent tokens $Z$ in the latent space in Figure 26. In practice, the tokens where the logvar is 0 (i.e. a high variance) on Figure 25 do not impact the prediction (Rolinek et al., 2019). We can therefore see, that ouf of the 16 tokens used, the most influential ones are Token 6, 7, 8, 9, 15, 16, as they clearly exhibit non-noisy patterns. ", "page_idx": 23}, {"type": "image", "img_path": "Aj8RKCGwjE/tmp/f8044445ff1d5968084785eb8675aeab0e7bbc6b5e534b434369cc8da13954d1.jpg", "img_caption": ["Figure 24: Latent space dynamics on Navier-Stokes 1e-3 - Mean tokens over time. Each color line is a different token channel. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "Aj8RKCGwjE/tmp/e2718e910e8471a6ea7a615b8a5510b24758892b4e688609b9e8e4314c39555e.jpg", "img_caption": ["Figure 25: Latent space dynamics on Navier-Stokes - Logvar tokens over time. Each color line is a different token channel. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "Aj8RKCGwjE/tmp/91e8bfe056e7895fdad5bba0c8d675e2b3c26600d3aa70daf81572e5da11059c.jpg", "img_caption": ["Figure 26: Latent space dynamics on Navier-Stokes - Predicted tokens over time. Each color line is a different token channel. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We conducted extensive experiments on multiple settings to obtain robust results across several datasets. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: There is a limitation section. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 27}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: No theoretical results. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Detailed hyperparameters and code. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: Everything will be made available upon acceptation. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 29}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: All the settings are disclosed explicitly. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [No] ", "page_idx": 29}, {"type": "text", "text": "Justification: No error bars yet. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Type of gpu detailed. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 30}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Yes we conformed to the code of ethics. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 30}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 30}, {"type": "text", "text": "Answer: [No] ", "page_idx": 30}, {"type": "text", "text": "Justification: No social impacts mentioned. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. \u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. \u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 30}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 31}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: This paper causes no such risk. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 31}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: All datasets mentioned, and code also credited when appropriate. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/ datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Answer: [No] ", "page_idx": 32}, {"type": "text", "text": "Justification: There is no such dataset. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 32}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: This paper is not related to research on this topic. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 32}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] Justification: No such risk. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}]