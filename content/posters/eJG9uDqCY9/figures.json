[{"figure_path": "eJG9uDqCY9/figures/figures_1_1.jpg", "caption": "Figure 1: Ratings of our autoregressive decoder-only transformer, ChessFormer, over several different temperatures. We refer to our models as \"ChessFormer <Maximum Glicko-2 rating seen during training>\" to easily distinguish between different models in subsequent sections. Each model is trained only on games with players up to a certain rating (1000, 1300, 1500, respectively). We report 95% confidence intervals calculated through taking \u00b11.96\u03c3.", "description": "This figure shows the Glicko-2 ratings of several ChessFormer models trained on chess game transcripts with players up to a certain maximum rating (1000, 1300, and 1500). The x-axis represents the temperature used during sampling, and the y-axis represents the Glicko-2 rating. The shaded area represents the 95% confidence interval.  The figure demonstrates the phenomenon of *transcendence*, where the model's performance surpasses the maximum rating observed in the training data, especially at lower temperatures.", "section": "4 Experiments"}, {"figure_path": "eJG9uDqCY9/figures/figures_3_1.jpg", "caption": "Figure 2: Visualizing the denoising effects of low temperature on the action distribution: an example of Chess-Former shifting probability mass towards the high reward move of trapping the queen with the rook as the temperature \u03c4 decreases. Opacity of the red arrows represent the probability mass given to different moves. The color of the square represent the reward that would be given for taking the action that moves the given piece to that state. Purple here is high reward, while blue is low. For more visualizations, see Appendix B.", "description": "This figure shows how low temperature sampling affects the probability distribution of actions in a chess game.  As the temperature (\u03c4) decreases from 1.0 to 0.001, the model shifts its probability mass toward the high-reward move (trapping the queen with the rook). The opacity of the red arrows indicates the probability of each move, while the color of the squares represents the reward associated with moving a piece to that square.  This illustrates the denoising effect of low-temperature sampling, where the model focuses on higher-reward actions.", "section": "3.2 Transcendence with Low-Temperature Sampling"}, {"figure_path": "eJG9uDqCY9/figures/figures_5_1.jpg", "caption": "Figure 3: Inspired by Mnih et al. [20], we generate a t-SNE embedding [34] of ChessFormer\u2019s last hidden layer latent representations of game transcripts during training time. The colors represent the probability of winning, with +1 corresponding to a state where White has won and 0 to Black. Probabiliy of winning is computed through the Stockfish analysis engine. We also visualize several board states associated with different clusters in the t-SNE embedding, and their associated expected reward when following the expert Stockfish distribution. Note that the model distinguishes between states where the outcome has already been determined (the two left boards), versus opening states that are extremely similar (the two right boards). See the full t-SNE in Appendix G.", "description": "This figure visualizes the t-SNE embedding of ChessFormer's last hidden layer latent representations of game transcripts during training.  The color of each point represents the probability of winning (from Stockfish analysis), ranging from +1 (White wins) to 0 (Black wins).  Four example board states are shown, illustrating how the model groups similar game states together in the latent space.  The visualization shows that the model learns to distinguish between decisive endgame states and similar opening states.", "section": "4.1 Experimental Setup"}, {"figure_path": "eJG9uDqCY9/figures/figures_6_1.jpg", "caption": "Figure 4: The favor probability distribution, or change in expected reward by setting temperature lower than \u03c4=1.0. We plot the favor distribution across two different temperatures: setting \u03c4 = .75 and \u03c4=0.001 by running the Stockfish analysis engine across 100 total ChessFormer 1000 games played at 0.001 temperature against Stockfish level 1 (as theoretically justified by PDL [10]). We calculate favor by sampling 100 counterfactual potential moves at \u03c4 = 1.0 per actual move made at \u03c4 = 0.001 to compute a baseline expected reward. In total, we gather an empirical probability distribution with n = 382,000 total samples per \u03c4 (38.2 moves on average per game). Note that we plot the distributions with transparency, so the brownish area is where the two overlap. We visualize several long-tail examples in Appendix B.", "description": "This figure visualizes how lowering the temperature in the ChessFormer model affects the expected reward.  It shows the distribution of changes in expected reward (favor) when comparing low-temperature sampling (\u03c4 = 0.75 and \u03c4 = 0.001) to the baseline (\u03c4 = 1.0). The x-axis represents the change in expected reward, and the y-axis represents the probability of that change. The plot indicates that lower temperatures significantly increase the expected reward for a small subset of game states, while the effect on most states is minimal.  This supports the idea that low-temperature sampling enhances the model's performance by focusing improvements on key game states.", "section": "4.2 Experimental Results"}, {"figure_path": "eJG9uDqCY9/figures/figures_7_1.jpg", "caption": "Figure 5: Action distribution diversity, as measured by the average normalized entropy over different chess rating dataset cutoffs for ratings 1000, 1300, 1500, respectively. These entropies are calculated directly from the empirical frequencies of our dataset, and are model-agnostic.", "description": "This figure shows the distribution of the normalized entropy of action distributions for different datasets. The x-axis represents the normalized entropy, and the y-axis represents the probability density. Three different datasets are used, each with a different maximum rating (1000, 1300, 1500). The average normalized entropy for each dataset is shown in the legend. The figure shows that the dataset with the maximum rating of 1000 has the highest average entropy, while the dataset with the maximum rating of 1500 has the lowest average entropy. This suggests that the dataset with the maximum rating of 1000 is the most diverse, while the dataset with the maximum rating of 1500 is the least diverse. The figure supports the claim that dataset diversity is essential for transcendence.", "section": "4.2 Experimental Results"}, {"figure_path": "eJG9uDqCY9/figures/figures_8_1.jpg", "caption": "Figure 1: Ratings of our autoregressive decoder-only transformer, ChessFormer, over several different temperatures. We refer to our models as \"ChessFormer <Maximum Glicko-2 rating seen during training>\" to easily distinguish between different models in subsequent sections. Each model is trained only on games with players up to a certain rating (1000, 1300, 1500, respectively). We report 95% confidence intervals calculated through taking \u00b11.96\u03c3.", "description": "This figure shows the Glicko-2 ratings of several transformer models trained to play chess, each trained on game transcripts with players up to a maximum rating (1000, 1300, and 1500).  The x-axis represents the temperature used during sampling, and the y-axis shows the resulting rating. The figure demonstrates the phenomenon of *transcendence*, where the model surpasses the maximum rating of the human players in its training data at lower temperatures (higher certainty).  The confidence intervals shown highlight the statistical significance of these results.", "section": "4 Experiments"}, {"figure_path": "eJG9uDqCY9/figures/figures_8_2.jpg", "caption": "Figure 1: Ratings of our autoregressive decoder-only transformer, ChessFormer, over several different temperatures. We refer to our models as \"ChessFormer <Maximum Glicko-2 rating seen during training>\" to easily distinguish between different models in subsequent sections. Each model is trained only on games with players up to a certain rating (1000, 1300, 1500, respectively). We report 95% confidence intervals calculated through taking \u00b11.96\u03c3.", "description": "This figure shows the chess ratings (Glicko-2) of several transformer models trained on human chess transcripts with maximum player ratings capped at 1000, 1300, and 1500.  The x-axis represents the temperature used during sampling, and the y-axis represents the Glicko-2 rating. The figure demonstrates the phenomenon of \"transcendence,\" where the models outperform the maximum rating observed in their training data, especially at lower temperatures.", "section": "4 Experiments"}, {"figure_path": "eJG9uDqCY9/figures/figures_14_1.jpg", "caption": "Figure 2: Visualizing the denoising effects of low temperature on the action distribution: an example of Chess-Former shifting probability mass towards the high reward move of trapping the queen with the rook as the temperature \u03c4 decreases. Opacity of the red arrows represent the probability mass given to different moves. The color of the square represent the reward that would be given for taking the action that moves the given piece to that state. Purple here is high reward, while blue is low. For more visualizations, see Appendix B.", "description": "This figure demonstrates how low temperatures affect the model's action selection in chess.  As the temperature (\u03c4) decreases, the model becomes more deterministic, focusing probability mass on higher-reward actions, such as trapping the queen.  This showcases the denoising effect of low-temperature sampling, where less likely, lower-reward actions (moves) are suppressed.", "section": "3.2 Transcendence with Low-Temperature Sampling"}, {"figure_path": "eJG9uDqCY9/figures/figures_14_2.jpg", "caption": "Figure 2: Visualizing the denoising effects of low temperature on the action distribution: an example of Chess-Former shifting probability mass towards the high reward move of trapping the queen with the rook as the temperature \u03c4 decreases. Opacity of the red arrows represent the probability mass given to different moves. The color of the square represent the reward that would be given for taking the action that moves the given piece to that state. Purple here is high reward, while blue is low. For more visualizations, see Appendix B.", "description": "This figure shows how the probability distribution over possible moves changes with temperature.  As the temperature decreases (\u03c4 goes from 1.0 to 0.001), the probability mass shifts towards the highest reward move (trapping the queen with the rook). The opacity of the red arrows indicates the probability of each move, and the color of the squares represents the reward associated with that move.", "section": "3.2 Transcendence with Low-Temperature Sampling"}, {"figure_path": "eJG9uDqCY9/figures/figures_14_3.jpg", "caption": "Figure 2: Visualizing the denoising effects of low temperature on the action distribution: an example of Chess-Former shifting probability mass towards the high reward move of trapping the queen with the rook as the temperature \u03c4 decreases. Opacity of the red arrows represent the probability mass given to different moves. The color of the square represent the reward that would be given for taking the action that moves the given piece to that state. Purple here is high reward, while blue is low. For more visualizations, see Appendix B.", "description": "This figure visualizes how low temperature sampling affects the probability distribution of actions in a chess game.  As the temperature decreases from 1.0 to 0.001, the model's probability mass shifts towards higher-reward moves, demonstrating the denoising effect of low temperature. The visualization uses arrows and colored squares to represent the probability of each move and the associated rewards, respectively.  This illustrates how low temperature sampling helps the model focus on actions with higher expected rewards, effectively mitigating noise and improving decision making.", "section": "3.2 Transcendence with Low-Temperature Sampling"}, {"figure_path": "eJG9uDqCY9/figures/figures_15_1.jpg", "caption": "Figure 11: The first expert output distribution. Although it puts non-negligible mass on the purple, high-reward action, it still samples a low-reward action the majority of the time.", "description": "This figure shows a toy example of probability distribution of an expert's output. The x-axis represents the output options (actions), while the y-axis represents the probability of selecting each output.  There are three options: a low-reward action (left), a high-reward action (purple, center), and another low-reward action (right). The expert's distribution shows a relatively low probability (around 60%) for selecting the high-reward action, with most of the probability mass allocated to the low-reward actions.", "section": "C Intuition of low temperature sampling inducing transcendence"}, {"figure_path": "eJG9uDqCY9/figures/figures_15_2.jpg", "caption": "Figure 11: The first expert output distribution. Although it puts non-negligible mass on the purple, high-reward action, it still samples a low-reward action the majority of the time.", "description": "This figure is a bar chart showing the probability distribution of an expert's output for a given input. The x-axis represents possible outputs (actions), and the y-axis shows the probability of selecting each output. The chart reveals that while the expert assigns a non-negligible probability to the high-reward action (represented in purple), it predominantly selects low-reward actions.  This illustrates a scenario where an individual expert makes frequent mistakes, a point crucial to understanding the paper's concept of transcendence.", "section": "C Intuition of low temperature sampling inducing transcendence"}, {"figure_path": "eJG9uDqCY9/figures/figures_15_3.jpg", "caption": "Figure 13: By taking the average of the first and second expert, we observe that this distribution now puts the majority of mass onto the correct action.", "description": "This figure is a bar chart showing the probability distribution of three possible outputs (actions) from two different experts. Expert 1 and Expert 2 each have probability distributions that show the probabilities for each action. The probability for the correct action is higher in both distributions, but not overwhelmingly so.  By averaging the probabilities of Expert 1 and Expert 2, a new distribution is created where the probability of the correct action is significantly higher than the probabilities for the incorrect actions. This illustrates how averaging the outputs of multiple experts can increase the probability of selecting the correct action. This visualization supports the claim that low-temperature sampling, a technique used in generative models, allows models to select the output with highest probability by implicitly performing a majority vote between different experts.", "section": "3 Conditions for Transcendence"}, {"figure_path": "eJG9uDqCY9/figures/figures_15_4.jpg", "caption": "Figure 14: Finally, by setting temperature \u03c4 to be < 1, more weight is shifted towards the high probability action, leading to a gain in the expected reward.", "description": "This figure shows how low-temperature sampling leads to higher expected rewards.  It visualizes a simplified scenario with three possible actions: two poor actions with low rewards (5%) and one optimal action with a high reward (90%).  As the temperature decreases, more weight is given to the high-reward action, thus increasing the overall expected reward. This illustrates the concept of transcendence, where the generative model's performance surpasses that of individual experts due to low-temperature sampling effectively performing a majority vote.", "section": "3.2 Transcendence with Low-Temperature Sampling"}, {"figure_path": "eJG9uDqCY9/figures/figures_19_1.jpg", "caption": "Figure 3: Inspired by Mnih et al. [20], we generate a t-SNE embedding [34] of ChessFormer\u2019s last hidden layer latent representations of game transcripts during training time. The colors represent the probability of winning, with +1 corresponding to a state where White has won and 0 to Black. Probabiliy of winning is computed through the Stockfish analysis engine. We also visualize several board states associated with different clusters in the t-SNE embedding, and their associated expected reward when following the expert Stockfish distribution. Note that the model distinguishes between states where the outcome has already been determined (the two left boards), versus opening states that are extremely similar (the two right boards). See the full t-SNE in Appendix G.", "description": "This figure visualizes a t-SNE embedding of the last hidden layer\u2019s latent representations from ChessFormer during training.  The color represents the probability of winning (from Stockfish analysis), showing how the model represents game states.  Several example board states are shown alongside their clusters and expected rewards based on Stockfish\u2019s distribution.  The model is able to differentiate between states with determined outcomes and extremely similar opening states.", "section": "4.1 Experimental Setup"}, {"figure_path": "eJG9uDqCY9/figures/figures_20_1.jpg", "caption": "Figure 1: Ratings of our autoregressive decoder-only transformer, ChessFormer, over several different temperatures. We refer to our models as \"ChessFormer <Maximum Glicko-2 rating seen during training>\" to easily distinguish between different models in subsequent sections. Each model is trained only on games with players up to a certain rating (1000, 1300, 1500, respectively). We report 95% confidence intervals calculated through taking \u00b11.96\u03c3.", "description": "The figure shows the chess ratings (Glicko-2) of several transformer models trained on human chess transcripts with different maximum player rating cutoffs (1000, 1300, and 1500).  Each model's performance is evaluated at various temperatures, illustrating the phenomenon of transcendence, where the model surpasses the skill level of the experts in its training data.  The 95% confidence intervals highlight the statistical significance of the results.", "section": "Experiments"}]