[{"figure_path": "eJG9uDqCY9/tables/tables_7_1.jpg", "caption": "Table 1: Table of several statistics describing the relationship between reward at \u03c4 = 0 vs. \u03c4 = 1. In the first column, we display the expected reward across our dataset, which is P of winning calculated by Stockfish 16.1). In the second column, we display F, or the change in reward for the given temperature \u03c4 versus the baseline. In the last three columns we display the accuracy for the best moves ranked by Stockfish analysis run at a time cutoff of 1 second. Here, the top-k accuracy is the percentage of games where the actual move sampled by the model was in the top-k moves as ranked by Stockfish. We report 95% bootstrapped confidence intervals with 10K resamples.", "description": "This table presents statistics that compare the performance of the ChessFormer model at different temperatures (\u03c4 = 0.001, 0.75, 1.0).  It shows the expected reward (win probability), the improvement in reward compared to the baseline (\u03c4 = 1.0), and the top-k accuracy (percentage of games where the model's move is among the top k best moves according to Stockfish).  The results highlight the impact of temperature on model performance and support the concept of transcendence by showing improved performance at lower temperatures.", "section": "4.2 Experimental Results"}, {"figure_path": "eJG9uDqCY9/tables/tables_17_1.jpg", "caption": "Table 2: Hyperparameters for our ChessFormer model.", "description": "This table lists all the hyperparameters used for training the ChessFormer model.  The values were largely based on the OPT-175B team's hyperparameters [37], with the batch size reduced to 125K tokens due to observations that training remained stable at this level.  The model architecture was inspired by work from Karvonen [12] and Karpathy [11].", "section": "4.1 Experimental Setup"}]