{"references": [{"fullname_first_author": "A. Katharopoulos", "paper_title": "Transformers are rnns: Fast autoregressive transformers with linear attention", "publication_date": "2020-00-00", "reason": "This paper introduced the foundational concept of linear attention, a crucial element in the development and improvement of linear transformers which are central to this paper\u2019s approach."}, {"fullname_first_author": "T. Dao", "paper_title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness", "publication_date": "2022-00-00", "reason": "This paper introduced FlashAttention, an efficient attention mechanism that improves upon the speed and memory usage of prior approaches, directly relevant to the methods presented in the current paper."}, {"fullname_first_author": "S. Arora", "paper_title": "Zoology: Measuring and improving recall in efficient language models", "publication_date": "2023-00-00", "reason": "This paper provides a benchmark for evaluating recall in language models, which is a significant aspect in the current paper\u2019s focus and improvement upon prior work."}, {"fullname_first_author": "A. Gu", "paper_title": "Efficiently modeling long sequences with structured state spaces", "publication_date": "2022-00-00", "reason": "This paper introduces the concept of structured state space models, which are closely related to the linear transformers studied in the current paper, providing a relevant theoretical context."}, {"fullname_first_author": "I. Schlag", "paper_title": "Linear Transformers Are Secretly Fast Weight Programmers", "publication_date": "2021-00-00", "reason": "This paper introduced DeltaNet, a key component and subject of improvement within the current paper, providing the foundation for the advancements described."}]}