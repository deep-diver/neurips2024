[{"figure_path": "y8Rm4VNRPH/figures/figures_4_1.jpg", "caption": "Figure 1: Speed-up of the chunkwise parallel form vs. the recurrent form.", "description": "This figure shows the speedup achieved by using the chunkwise parallel form of the algorithm compared to the recurrent form. The speedup is plotted against the sequence length for three different head dimensions (64, 128, and 256). As sequence length increases, the speedup achieved by the chunkwise parallel form becomes more significant, especially for larger head dimensions. This highlights the effectiveness of the chunkwise parallel approach in improving training efficiency for longer sequences.", "section": "3 Parallelizing DeltaNet Across the Sequence Dimension"}, {"figure_path": "y8Rm4VNRPH/figures/figures_5_1.jpg", "caption": "Figure 2: Accuracy (%) on MQAR.", "description": "The figure shows the accuracy of different models (DeltaNet, Mamba, GLA, RetNet, RWKV4, and Hyena) on the Multi-query associative recall (MQAR) task. The x-axis represents the model dimension, and the y-axis represents the accuracy. The sequence length is 512, and the number of key-value pairs is 64. DeltaNet achieves perfect accuracy in most model dimensions. ", "section": "4.1 Synthetic Benchmarks"}, {"figure_path": "y8Rm4VNRPH/figures/figures_6_1.jpg", "caption": "Figure 3: Accuracy (%) on RegBench.", "description": "This figure shows the accuracy of different models on the Regbench dataset as a function of the number of training examples.  The models compared include Transformer++, DeltaNet (with and without convolution), GLA (with and without convolution), and Mamba (with and without convolution). The figure demonstrates the performance of these various models on a synthetic in-context learning benchmark, illustrating their ability to learn and generalize from a limited number of examples.", "section": "4.1 Synthetic Benchmarks"}, {"figure_path": "y8Rm4VNRPH/figures/figures_7_1.jpg", "caption": "Figure 4: Training throughput of 1.3B models on a single H100.", "description": "This figure compares the training throughputs of different 1.3B parameter models on a single H100 GPU across various training lengths and batch sizes. The models compared are Transformer++, Mamba, GLA, and DeltaNet.  The x-axis represents different combinations of training length and batch size (e.g., 2K samples * batch size 8). The y-axis shows the training throughput measured in thousands of tokens per second (Kt/s). The figure demonstrates that DeltaNet achieves comparable speed to GLA and significantly outperforms the Transformer++ and Mamba baselines for longer sequence lengths.", "section": "4 Empirical Study"}, {"figure_path": "y8Rm4VNRPH/figures/figures_19_1.jpg", "caption": "Figure 1: Speed-up of the chunkwise parallel form vs. the recurrent form.", "description": "This figure compares the speed-up achieved by using the chunkwise parallel form of the algorithm over the recurrent form.  The x-axis represents the sequence length (K), and the y-axis shows the speed-up factor (x).  Multiple lines are presented, each corresponding to a different head dimension (64, 128, and 256). The plot demonstrates that the chunkwise parallel approach provides significantly greater speed improvements as both sequence length and head dimension increase, highlighting the benefits of the algorithm for larger models and longer sequences.", "section": "3.2 Chunkwise Parallel Form for DeltaNet"}]