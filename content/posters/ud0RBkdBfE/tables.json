[{"figure_path": "ud0RBkdBfE/tables/tables_57_1.jpg", "caption": "Table 1: Performance upper bounds for different objectives (let Q := \u2211n=1 qn=1).", "description": "This table presents the theoretical upper bounds on the performance of the Split Federated Learning (SFL) algorithm under different scenarios and objective functions.  It shows the convergence rate achieved by SFL-V1 and SFL-V2 under three different cases (strongly convex, general convex, and non-convex) for both full participation and partial participation of clients. The bounds are expressed in terms of the total number of rounds (T), smoothness parameter (S), strong convexity parameter (\u00b5), variance of stochastic gradients (\u03c3), and data heterogeneity (\u03b5).  The parameter Q represents the average participation rate of clients.", "section": "H.1 Main technical results"}, {"figure_path": "ud0RBkdBfE/tables/tables_57_2.jpg", "caption": "Table 2: Performance upper bounds for strongly convex objectives with full client participation. Here, absolute constants and polylogarithmic factors are omitted. We further relax the upper bounds of SFL-V2 for an easier comparison.", "description": "This table compares the upper bounds of the convergence rate for strongly convex objectives in different federated learning settings.  The methods compared include Mini-Batch SGD, Federated Averaging (FL), Split Learning (SL), and Split Federated Learning (SFL). The table shows how the upper bounds vary based on factors such as the variance of stochastic gradients (\u03c3\u00b2), smoothness (S), strong convexity (\u00b5), the number of clients (N), the total number of rounds (T), and data heterogeneity (Ierr).  Simplifications have been made to the SFL-V2 bound for easier comparison.", "section": "H.2 Comparison of Bounds"}, {"figure_path": "ud0RBkdBfE/tables/tables_58_1.jpg", "caption": "Table 1: Performance upper bounds for different objectives (let Q := \u2211N<sub>n=1</sub> a<sub>n</sub>/q<sub>n</sub>).", "description": "This table presents the upper bounds of convergence for different SFL methods (SFL-V1 and SFL-V2) under various scenarios (full participation and partial participation) and for different objective function types (strongly convex, general convex, and non-convex).  The results demonstrate the convergence rates achieved by each method under the specified conditions, showing the impact of data heterogeneity and client participation levels on the performance of SFL.", "section": "H.1 Main technical results"}, {"figure_path": "ud0RBkdBfE/tables/tables_58_2.jpg", "caption": "Table 2: Performance upper bounds for strongly convex objectives with full client participation. Here, absolute constants and polylogarithmic factors are omitted. We further relax the upper bounds of SFL-V2 for an easier comparison.", "description": "This table compares the upper bounds of the convergence rate for strongly convex objectives in different federated learning algorithms: Mini-Batch SGD, FL, SL, and SFL. It highlights that the order of convergence for SFL and other algorithms is O(1/T).", "section": "H.2 Comparison of Bounds"}]