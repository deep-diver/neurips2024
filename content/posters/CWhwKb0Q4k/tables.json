[{"figure_path": "CWhwKb0Q4k/tables/tables_7_1.jpg", "caption": "Table 1: Model performance on MNIST-4. Memory refers to the maximum GPU memory occupied by tensors. Runtime was calculated over 100 epochs on a NVIDIA RTX 2070 GPU. We present results both for an amplitude and angle encoding as shown in Fig. 2.", "description": "This table presents a comparison of different model architectures on the MNIST-4 dataset.  It shows the test accuracy, memory usage (in MB), runtime (in seconds), and residual error for various models, including the IMPLICIT, IMPLICIT+WARMUP, and DIRECT solvers with different numbers of layers.  Results are provided separately for models using amplitude and angle encoding schemes. The table allows for evaluating the efficiency and performance tradeoffs of different approaches.", "section": "4.1 MNIST-4"}, {"figure_path": "CWhwKb0Q4k/tables/tables_8_1.jpg", "caption": "Table 2: Model performance on MNIST-10. Memory refers to the maximum GPU memory occupied by tensors. Runtime was calculated over 100 epochs on a NVIDIA RTX 2070 GPU.", "description": "This table presents the performance of different models on the MNIST-10 dataset.  The models include the proposed Implicit and Implicit+Warmup QDEQ models, along with several direct solver models of varying depths (number of layers).  For comparison, results from two baseline QML circuits using VAE and PCA based encodings are also shown. The table provides test accuracy, GPU memory usage, runtime, and a residual value for each model.", "section": "4.2 MNIST-10 and FashionMNIST-10"}, {"figure_path": "CWhwKb0Q4k/tables/tables_8_2.jpg", "caption": "Table 3: Model performance on FashionMNIST-10. Memory refers to the maximum GPU memory occupied by tensors. Runtime was calculated over 100 epochs on a NVIDIA RTX 2070 GPU. For Dilip et al. (2022), performance depends on encoding quality, and thus a range is given, while two comparable baselines are included for Shen et al. (2024) (see Section 3.3 for details).", "description": "This table presents the performance of different models on the FashionMNIST-10 dataset.  The models include the proposed Quantum Deep Equilibrium Models (QDEQ) using different approaches (IMPLICIT, IMPLICIT+WARMUP) and standard weight-tied networks (DIRECT solver) with varying depths.  Also included are results from comparable methods in other published work for comparison purposes. The metrics reported are test accuracy, maximum GPU memory usage, runtime, and residual. Note that results for some prior work show a range due to variations in encoding quality.", "section": "4.2 MNIST-10 and FashionMNIST-10"}, {"figure_path": "CWhwKb0Q4k/tables/tables_9_1.jpg", "caption": "Table 4: Model performance on CIFAR-10.", "description": "This table presents the test accuracy results for different models on the CIFAR-10 dataset.  The models include the proposed IMPLICIT and IMPLICIT+WARMUP solvers, as well as several DIRECT solvers with varying numbers of layers. The table shows the test accuracy achieved by each model, allowing for a comparison of performance across different model architectures and training paradigms. The IMPLICIT+WARMUP model shows the best performance.", "section": "3.3 Models and baselines"}, {"figure_path": "CWhwKb0Q4k/tables/tables_20_1.jpg", "caption": "Table 1: Model performance on MNIST-4. Memory refers to the maximum GPU memory occupied by tensors. Runtime was calculated over 100 epochs on a NVIDIA RTX 2070 GPU. We present results both for an amplitude and angle encoding as shown in Fig. 2.", "description": "This table shows the performance of different models on the MNIST-4 dataset.  It compares the proposed QDEQ framework with several baselines (a standard QML circuit from Wang et al. (2022b) and various weight-tied networks with different depths).  The results are presented separately for amplitude and angle encoding methods. For each model, the test accuracy, memory usage (in MB), runtime (in seconds), and residual error are reported.", "section": "4.1 MNIST-4"}]