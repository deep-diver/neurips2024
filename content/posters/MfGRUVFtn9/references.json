{"references": [{"fullname_first_author": "Tianyu Gu", "paper_title": "Badnets: Evaluating backdooring attacks on deep neural networks", "publication_date": "2019-00-00", "reason": "This paper is foundational in the field of backdoor attacks, introducing the BadNets attack and establishing a benchmark for evaluating backdoor vulnerabilities in deep neural networks."}, {"fullname_first_author": "Yige Li", "paper_title": "Anti-backdoor learning: Training clean models on poisoned data", "publication_date": "2021-00-00", "reason": "This paper proposes a novel defense method against backdoor attacks by training clean models on poisoned data, significantly advancing the state-of-the-art in backdoor defense."}, {"fullname_first_author": "Bolun Wang", "paper_title": "Neural cleanse: Identifying and mitigating backdoor attacks in neural networks", "publication_date": "2019-00-00", "reason": "This paper introduces Neural Cleanse, a prominent backdoor defense method that uses clean data to identify and remove backdoors from neural networks."}, {"fullname_first_author": "Kang Liu", "paper_title": "Fine-pruning: Defending against backdooring attacks on deep neural networks", "publication_date": "2018-00-00", "reason": "This paper pioneers a pruning-based defense strategy against backdoor attacks, which is a significant contribution to the field of backdoor defense."}, {"fullname_first_author": "Yige Li", "paper_title": "Reconstructive neuron pruning for backdoor defense", "publication_date": "2023-00-00", "reason": "This paper presents a novel approach for backdoor defense that combines clean unlearning with neuron pruning, effectively mitigating backdoor vulnerabilities while maintaining clean accuracy."}]}