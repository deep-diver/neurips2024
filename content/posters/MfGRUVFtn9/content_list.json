[{"type": "text", "text": "Unveiling and Mitigating Backdoor Vulnerabilities based on Unlearning Weight Changes and Backdoor Activeness ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Weilin Lin1 Li Liu1\u2217 Shaokui Wei2 Jianze $\\mathbf{Li^{3,4,2}}$ Hui Xiong1 ", "page_idx": 0}, {"type": "text", "text": "1The Hong Kong University of Science and Technology (Guangzhou) 2The Chinese University of Hong Kong, Shenzhen   \n3Shenzhen International Center for Industrial and Applied Mathematics 4Shenzhen Research Institute of Big Data ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The security threat of backdoor attacks is a central concern for deep neural networks (DNNs). Recently, without poisoned data, unlearning models with clean data and then learning a pruning mask have contributed to backdoor defense. Additionally, vanilla fine-tuning with those clean data can help recover the lost clean accuracy. However, the behavior of clean unlearning is still under-explored, and vanilla fine-tuning unintentionally induces back the backdoor effect. In this work, we first investigate model unlearning from the perspective of weight changes and gradient norms, and find two interesting observations in the backdoored model: 1) the weight changes between poison and clean unlearning are positively correlated, making it possible for us to identify the backdoored-related neurons without using poisoned data; 2) the neurons of the backdoored model are more active (i.e., larger changes in gradient norm) than those in the clean model, suggesting the need to suppress the gradient norm during fine-tuning. Then, we propose an effective two-stage defense method. In the first stage, an efficient Neuron Weight Change (NWC)-based Backdoor Reinitialization is proposed based on observation 1). In the second stage, based on observation 2), we design an Activeness-Aware Fine-Tuning to replace the vanilla fine-tuning. Extensive experiments, involving eight backdoor attacks on three benchmark datasets, demonstrate the superior performance of our proposed method compared to recent state-of-the-art backdoor defense approaches. The code is available at https://github.com/linweiii/TSBD.git. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Over the past few years, deep neural networks (DNNs) have achieved surprising success in several realworld applications, such as face recognition [1\u20133], medical image processing [4, 5], and autonomous driving [6, 7], etc. However, DNNs are susceptible to malicious attacks that can compromise their security and reliability. One typical example is the backdoor attack [8\u201311], where the adversary maliciously manipulates the training dataset or training process to produce a backdoored model, which performs normally on clean data while predicting any sample with a particular trigger pattern to a pre-defined target label. In this work, we focus on the post-training defense scenario where, given a backdoored model and a small set of clean training samples, one aims to mitigate the backdoor effect while maintaining the performance on clean data, thereby obtaining a benign model. ", "page_idx": 0}, {"type": "text", "text": "Up to now, several important methods have been developed for backdoor defense [14\u201318]. One promising approach is poison unlearning, which involves updating a backdoored model by unlearning from poisoned data. This technique has been utilized in various backdoor defenses such as ABL [14], D-BR [19], Neural Cleanse (NC) [20], and i-BAU [21], etc. To avoid approximating poisoned data, another approach called clean unlearning was conducted by RNP [22]. This technique only uses clean data for unlearning and then prunes the backdoored model, which has been proven to be effective. Through relevant experiments, we find an interesting connection between poison unlearning and clean unlearning, as illustrated in Observation 1 of Figure 1. Specifically, by calculating the weight changes of each neuron during the two unlearning processes on the backdoored models2, we find that they exhibit a strong positive correlation, i.e., the neurons exhibiting significant weight changes during clean unlearning also tend to play crucial roles in poison unlearning, indicating a stronger association with backdoor-related activities. Moreover, we further investigate the backdoor activeness during learning processes3, i.e., comparing the average gradient norm for each neuron in both the backdoored and clean models. The results are shown in Observation 2 of Figure 1, revealing that neurons in the backdoored model are always more active compared to those in the clean model. ", "page_idx": 0}, {"type": "image", "img_path": "MfGRUVFtn9/tmp/b3fa198ebed09eb9167eb78df7ba1368d2df7a50337c26c9b2773586e8c8c77d.jpg", "img_caption": ["Figure 1: Illustration of two observations. Figures for Observation 1 show distributions of neuron weight changes during clean unlearning and poison unlearning. Figures for Observation 2 compare the average gradient norm for each neuron on the backdoored model and clean model, which are calculated with one-epoch clean unlearning. More active means a larger change in the gradient norm. Experiments are conducted on PreAct-ResNet18 [12] on CIFAR-10 [13] for the clean model and additional attacks with $10\\%$ poisoning ratio for the backdoored model. The last convolutional layers are chosen for illustration. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Inspired by the above two observations regarding the backdoored model , we propose Two-Stage Backdoor Defense (TSBD), consisting of stage 1) Neuron Weight Change-based Backdoor Reinitialization and stage 2) Activeness-Aware Fine-tuning. In the first stage, we first conduct clean unlearning on the backdoored model, followed by the neuron weight change calculation, where both the changes of each subweight4 and neuron are recorded. Then, we conduct zero reinitialization to mitigate the backdoor effect by reinitializing the most-changed subweights among the top- $n\\%$ most-changed neurons as 0 in the original backdoored model. In the second stage, we adopt activeness-aware fine-tuning with gradient-norm regulation to recover clean accuracy and suppress the reactivation of the backdoor effect. Extensive experiments demonstrate the superior defense performance of the proposed method compared to state-of-the-art (SOTA) backdoor defense methods. ", "page_idx": 1}, {"type": "text", "text": "To summarize, our main contributions are three-fold. (1) Novel Insight: We are the first to uncover the strong positive correlation between neuron weight changes in clean unlearning and poison unlearning. We also reveal the high backdoor activeness in the backdoored model during the learning process. (2) Effective Defense Method: We further develop an effective two-stage defense method based on unlearning weight changes and backdoor activeness, considering both backdoor mitigation and clean-accuracy recovery, respectively. (3) SOTA Performance: Experimental results and analysis show that our proposed method achieves SOTA performance in backdoor defense. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "image", "img_path": "MfGRUVFtn9/tmp/b1c617cbd55db1c6bc44cb3c3a4c893a2ae90dd0f3e9351e259ee0c01b3516bc.jpg", "img_caption": ["Figure 2: Overview of the proposed Two-Stage Backdoor Defense framework. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Backdoor Attack ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In the literature, various backdoor attacks on DNNs have been proposed, which can be categorized into data poisoning attacks and training-controllable attacks. BadNets [8] is one of the earliest data poisoning attacks in this field. In this attack, a small proportion of the original data is selected and patched with a pre-defined pattern, known as a trigger. The labels of these patched data points are then modified to a target label. The mixed dataset, containing both clean and poisoned data, is used to train the DNNs, resulting in the implantation of the backdoor. Under a similar procedure, Blended [25] was proposed as a stronger attack by blending an entire pre-defined image into the original clean data with controllable transparency. Recently, more advanced and stealthy attacks have been proposed to enhance the trigger, such as SIG [26], label consistent attacks [27, 28], SSBA [9], etc. Another category is training-controllable attacks [23, 24, 29\u201331], where the attackers design triggers with permission to control the training process. Two significant examples are WaNet [24] and Input-aware [23], which generate unique triggers for different input data by incorporating an injection function into the model training process. This approach makes these attacks more difficult to detect compared to previous attacks with fixed triggers. ", "page_idx": 2}, {"type": "text", "text": "2.2 Backdoor Defense ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "According to the different stages of model training, backdoor defense methods can be classified into two types: training-stage defenses and post-training defenses. ", "page_idx": 2}, {"type": "text", "text": "Training-stage Defenses. In training-stage defenses, defenders have access to a mixed training dataset containing both clean data and poisoned data with triggers. ABL [14] discovers that the loss-dropping speed of poisoned data during the early stages of model training is faster, and thus isolates them for poison unlearning. DBD [32] splits the training process into three steps to separate the training of feature extraction from that of the subsequent classifier to evade the learning of triggerlabel correlation. Similarly, D-ST/D-BR [19] observes that the transformations of poisoned-data feature representations are more sensitive than clean ones, and thus proposes to modularize the training process. ", "page_idx": 2}, {"type": "text", "text": "Post-training Defenses. In post-training defenses [33\u201336], defenders aim to erase the backdoor effect in the learned DNNs using a small portion of clean data. FP [37] is one of the earliest defense methods, which observes that poisoned data and clean data activate different neurons in a backdoored DNN, and thus keeps pruning the less-activated neurons in response to clean data until a significant drop in accuracy occurs. After that, vanilla fine-tuning is employed to recover the lost clean accuracy. Using the pruning strategy [38\u201340], ANP [41] observes that the backdoor-related neurons exhibit higher sensitivity to adversarial perturbations compared to others, and thus trains a pruning mask using minimax optimization. Continuing along this line, AWM [42] and RNP [22] use a similar mask training process with main modifications in neuron perturbations to data perturbations and clean unlearning, respectively. Different strategies are also proposed for defense. For example, NC [20] proposes to recover the trigger before the subsequent backdoor removal. NAD [43], for the first time, adopts model distillation to guide the learning of a benign student model. Additionally, employing unlearning techniques, SAU [44] treats backdoor triggers as a form of adversarial perturbation, and generates poisoned data through optimization on clean data, which are then used in poison unlearning. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Unlearning for Backdoor Defense. Model unlearning can be considered as an opposite process against learning, aiming to remove the impact of a training subset from a trained model [45]. In the field of backdoor defense, unlearning the possible poisoned data (i.e., poison unlearning) is an effective way to remove the learned backdoor. NC [20] and BTI-DBF [46] try to generate the possible poisoned data with either trigger inversion or poison-data generator; ABL [14] and D-BR [19] focus on flitering out the poisoned data from the training dataset according to their attributes during training; i-BAU [21] and SAU [47] assume the adversarial perturbation as a type of trigger and generate poisoned data with adversarial example. To avoid inducing bias, recent work tries to directly unlearn the available clean data (i.e., clean unlearning) for defense. RNP [22] finds that a clean-unlearned model can help expose the backdoor neurons for the subsequent pruning-mask learning. ", "page_idx": 3}, {"type": "text", "text": "However, there exist some limitations among those techniques, e.g., clean unlearning is still underexplored, and the vanilla fine-tuning unintentionally increases the attack success rate. In this paper, we propose a comprehensive two-stage defense method breaking through the two limitations. ", "page_idx": 3}, {"type": "text", "text": "3 Methods ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 Problem Formulation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Threat Model. We assume that the attacker has full access to the training data. Their goal is to poison a portion of the dataset by injecting triggers into the data so that the trained model misclassifies the poisoned data to the target class while still performing normally on clean data. The poisoning ratio (e.g., $10\\%$ ) is used to depict the proportion of poisoned data within the entire dataset. We denote the parameters of the backdoored model as \u03b8bd = {\u03b8(bld) }1\u2264l\u2264L satisfying \u03b8(bld) \u2208RK(l)\u00d7I(l), where $K=\\{K^{(l)}\\}_{1\\le l\\le L}$ and $\\pmb{I}=\\{\\pmb{I}^{(l)}\\}_{1\\leq l\\leq L}$ represent the neuron numbers and learnable subweight numbers, respectively. Specifically, for the $l^{t h}\\in\\{1,\\dots,L\\}$ layer, there are $K^{(l)}$ neurons in total and $I^{(l)}$ subweights for each neuron. ", "page_idx": 3}, {"type": "text", "text": "Defense Setting. The defender\u2019s goal is to remove the backdoor effect, which causes poisoned data to be misclassified to the target class, from the backdoored model while minimizing the impact on the prediction accuracy for clean data. Following the previous defense setting [37, 41], we assume that the defender knows nothing about the poisoned data and possesses only $5\\%$ of the total dataset as clean data, denoted as $\\mathcal{D}_{c}$ . ", "page_idx": 3}, {"type": "text", "text": "3.2 Neuron Weight Change & Suggestions Given by the Two Observations ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this subsection, we provide more details on the unlearning formulation and offer suggestions based on the two observed observations. ", "page_idx": 3}, {"type": "text", "text": "Model Unlearning. Model Unlearning can be defined as the reverse process of model training [22], which involves maximizing the loss value on a given dataset. Given a DNN model $f$ parameterized as $\\pmb{\\theta}$ and a dataset $\\mathcal{D}$ for unlearning, the maximization problem can be formulated as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pmb{\\theta}}\\mathbb{E}_{(\\pmb{x},y)\\in\\mathcal{D}}\\left[\\mathcal{L}(f(\\pmb{x};\\pmb{\\theta}),y)\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $({\\pmb x},y)\\in\\mathcal{D}$ represents the images and their corresponding labels, and $\\mathcal{L}$ denotes the loss function used in this task, e.g., cross-entropy loss. ", "page_idx": 3}, {"type": "text", "text": "Intuitively, by maximizing the loss expectation, the unlearned model, parameterized as $\\theta_{u l}$ , is prone to fail at the task specified in $\\mathcal{D}$ . In this paper, we term the process as clean unlearning when all the data in $\\mathcal{D}$ are clean, denoted as $\\mathcal{D}_{c}$ . On the other hand, poison unlearning refers to the scenario where all the data in $\\mathcal{D}$ are poisoned with a trigger. By default, both clean and poison unlearning are terminated when the model performs poorly on the corresponding tasks, such as achieving only $10\\%$ clean accuracy or attack success rate. ", "page_idx": 3}, {"type": "image", "img_path": "MfGRUVFtn9/tmp/76dcc91069c2547a4cfd3bf2a0f253f34748a9bcc4231c2acd91a90d06bc46ed.jpg", "img_caption": ["Figure 3: Illustration of clean and poison activations of each neuron. (a) and (b) represent the activations on the original clean and backdoored model, respectively. (c) shows the activation changes during the clean and poison unlearning on backdoored model. Activations are captured from the last convolutional layer with an additional Relu activation function on PreAct-ResNet18 [12]. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Neuron Weight Change. To comprehensively quantify the weight changes of a neuron during the entire unlearning process, we define the Neuron Weight Change (NWC), where the $L_{1}$ norm is calculated on every neuron\u2019s weight differences. The NWC for the $k^{t h}\\in\\{1,\\ldots,K^{(l)}\\}$ neuron in layer $l\\in\\{1,\\ldots,\\dot{L}\\}$ can be formulated as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{NWC}^{(l)k}=\\sum_{i=0}^{I^{(l)}}\\|\\pmb{\\theta}_{u l}^{(l)k i}-\\pmb{\\theta}_{b d}^{(l)k i}\\|_{1},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\textstyle\\sum_{i=0}^{I^{(l)}}\\left\\|\\cdot\\right\\|_{1}$ is to calculate the $L_{1}$ norm for the differences on a neuron with totally $I^{(l)}$ subweights, \u03b8(ull)ki and $\\pmb{\\theta}_{b d}^{(l)k i}$ denote the $i^{t h}\\,\\in\\,\\{1,\\dots,I^{(l)}\\}$ subweights of $k^{t h}$ neuron after and before the entire unlearning process, respectively. A larger indicates more significant changes occurring in neuron $k$ during the unlearning process. Similarly, in Equation (2), the term $\\lVert\\pmb{\\theta}_{u l}^{(l)\\overline{{k}}i}-\\pmb{\\theta}_{b d}^{(l)k i}\\rVert_{1}$ represents the changes in the $i^{t h}$ subweight of neuron $k$ in layer l, i.e., defined as Subweight Change. ", "page_idx": 4}, {"type": "text", "text": "Suggestions Given by the Two Observations. As demonstrated in Section 1, we have two interesting observations regarding the backdoored model. Observation 1 shows that the neurons exhibiting significant weight changes during clean unlearning also tend to play crucial roles in poison unlearning. It suggests that we can employ clean unlearning to identify and eliminate backdoor-related neurons using NWC, at the expense of reducing clean accuracy. On the other hand, Observation 2 reveals that neurons in the backdoored model are always more active compared to those in the clean model. It suggests that we should suppress the gradient norm during the learning process if we want to recover it to a clean model. These two suggestions act as the main supports to our proposed TSBD. ", "page_idx": 4}, {"type": "text", "text": "3.3 Further Investigations & Insights ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Here, we offer insights from the perspective of neuron activations, trying to answer two important questions: [Q1] What causes the clean unlearning NWCs to exhibit a positive correlation with those in poison unlearning, and [Q2] What motivates the neurons more active in the backdoored model. ", "page_idx": 4}, {"type": "text", "text": "Neuron Activations & Activation Rise. The neuron activation is determined by computing the average value of all inputs to the specific neuron, e.g., $h^{(l)k}\\approx\\sigma(\\pmb{\\theta}^{(l)k}h^{(l-1)})$ for simplicity, where $\\sigma(\\cdot)$ is the activation function. In line with the terminology used in FP [37], clean activation denotes the scenario where all the input samples are clean while poison activation refers to the presence of poisoned inputs. To better observe the changes in activation during unlearning, we calculate the activation rise from the original model to the unlearned model, i.e., $\\Delta\\bar{h^{(l)k}}=h_{u l}^{(l)\\bar{k}}-h_{b d}^{(l)k}$ hbd  A positive . value indicates an increase in activation, while a negative value signifies a decrease. ", "page_idx": 4}, {"type": "text", "text": "Relationship between NWC and Activation Change. Considering that a backdoored model has learned two tasks from the clean and poisoned data [14], the main influence of NWC on a neuron can be roughly attributed to its activation change on both clean and poisoned inputs. For neuron $k$ in layer $l$ , we can formulate it as $\\mathrm{NWC}^{(l)k}\\propto|\\Delta h_{c}^{(l)k}|+|\\Delta h_{p}^{(l)k}|$ , where $\\Delta h_{c}^{(l)k}$ and $\\Delta h_{p}^{(l)k}$ represent the activation rise on clean and poisoned inputs, respectively. ", "page_idx": 5}, {"type": "text", "text": "Figure 3 illustrates the clean and poison activations in (a) the original clean model, (b) the original backdoored model, and (c) the backdoored-model unlearning. We now try to answer the above two questions from these observations. [A1] We can observe that poison activations are the main factors affected during both clean unlearning (increase) and poison unlearning (decrease), while clean activations are only slightly influenced (see Figure 3 (c)), i.e., $\\mathrm{NWC}_{\\uparrow}^{(l)k}\\overset{\\quad\\,\\,\\bar{}}{\\rightarrow}|\\Delta h_{c}^{(l)k}|_{\\approx}+|\\Delta h_{p}^{(l)k}|_{\\uparrow}$ Also, the growing NWC during clean unlearning can indicate larger poison and clean activations (where $h_{p}^{(\\tilde{l})k}>\\bar{h_{c}^{(l)k}})$ to some extent (see Figure 3 (b)). Thus, we deduce that the co-function of clean and poison activations dominates the performance on both tasks, while the higher values of poison activation in the backdoored model make it an easier target for modification. In this case, the neurons with higher poison activations tend to decrease their values during poison unlearning, thereby reducing the attack success rate. Conversely, during clean unlearning, these neurons increase poison activations, which suppresses the function of clean activations and reduces clean accuracy. [A2] Similarly, the significantly lower values of mixed clean and poison activations (maximum: 0.5676) on the clean model (see Figure 3 (a)) indicate that it is less active compared to the backdoored model (maximum: 1.7053), where a similar pattern can also be seen on the bottom left of Figure 3 (b). ", "page_idx": 5}, {"type": "text", "text": "3.4 Two-Stage Backdoor Defense Framework ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Based on the above observations, we now propose a defense framework incorporating Neuron Weight Change-based Backdoor Reinitialization (including Clean Unlearning, Neuron Weight Change Calculation and Zero Reinitialization), and Activeness-aware Fine-tuning. The detailed defense process is illustrated in Figure 2 and Algorithm 1 (found in Appendix A). ", "page_idx": 5}, {"type": "text", "text": "Stage 1) Neuron Weight Change-based Backdoor Reinitialization. We aim to mitigate the backdoor effect with acceptable clean-accuracy sacrificed in this stage. [a. Clean Unlearning.] To identify the backdoor-related neurons, we first conduct a full clean unlearning using the available clean data $\\mathcal{D}_{c}$ on the backdoored model. [b. Neuron Weight Change Calculation.] Then, we record the subweight changes and calculate the NWC for each neuron as described in Section 3.2. The resulting sorted order of neurons reflects the backdoor strength. [c. Zero Reinitialization.] After that, we can now eliminate the backdoor effect through zero reinitialization. Based on the NWC neuron order, we identify the top- $n\\%$ neurons as strongly backdoor-related. As suggested in Section 3.3, high-NWC neurons may also contribute to clean accuracy to some extent. Therefore, we further choose to reinitialize the subweights of the most-changing $m\\%$ among the selected neurons to zero in the backdoored model, while leaving the others unchanged. The reinitialized model parameter is denoted as $\\hat{\\pmb{\\theta}}$ . ", "page_idx": 5}, {"type": "text", "text": "Stage 2) Activeness-Aware Fine-tuning. To further repair the reinitialized subweights and avoid recovering the backdoor effect again, we conduct activeness-aware fine-tuning on the reinitialized model $({\\hat{\\pmb\\theta}})$ using the clean dataset, $\\mathcal{D}_{c}$ . This involves incorporating gradient-norm regulation into the original loss function, such as the cross-entropy loss $\\mathcal{L}_{c e}$ , to penalize high gradient values. This regulation serves to suppress neuron activity during fine-tuning. The final loss function is: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{f t}(\\hat{\\pmb{\\theta}})=\\mathcal{L}_{c e}(\\hat{\\pmb{\\theta}})+\\lambda\\cdot\\|\\nabla_{\\hat{\\pmb{\\theta}}}\\mathcal{L}_{c e}(\\hat{\\pmb{\\theta}})\\|_{2},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\|\\nabla_{\\hat{\\boldsymbol{\\theta}}}\\mathcal{L}_{c e}(\\hat{\\boldsymbol{\\theta}})\\|_{2}$ represents the $L_{2}$ norm of gradients, and $\\lambda$ is the penalty coefficient controlling its impact. Hence, the objective of fine-tuning is to minimize the loss function $\\mathcal{L}_{f t}(\\hat{\\pmb{\\theta}})$ using the available clean data $\\mathcal{D}_{c}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\underset{\\hat{\\pmb{\\theta}}}{\\mathrm{min}}\\mathbb{E}_{(\\pmb{x}_{c},\\pmb{y}_{c})\\in\\mathcal{D}_{c}}{\\big[}\\mathcal{L}_{f t}\\big(f\\big(\\pmb{x}_{c};\\hat{\\pmb{\\theta}}\\big),\\boldsymbol{y}_{c}\\big)\\big].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "During practical optimization for computational efficiency, we adopt the approximation scheme in [48], which can be formulated as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\nabla_{\\hat{\\theta}}\\mathcal{L}_{f t}(\\hat{\\theta})\\approx(1-\\alpha)\\nabla_{\\hat{\\theta}}\\mathcal{L}_{c e}(\\hat{\\theta})+\\alpha\\nabla_{\\hat{\\theta}}\\mathcal{L}_{c e}(\\hat{\\theta}+r\\frac{\\nabla_{\\hat{\\theta}}\\mathcal{L}_{c e}(\\hat{\\theta})}{\\|\\nabla_{\\hat{\\theta}}\\mathcal{L}_{c e}(\\hat{\\theta})\\|_{2}}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "table", "img_path": "MfGRUVFtn9/tmp/70e699ca446bc0cff93e10112cae47c74072f207193d9db65d737242baa6061a.jpg", "table_caption": ["Table 1: Comparison with the SOTA defenses on CIFAR-10 dataset with PreAct-ResNet18 (%). "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Here, $r$ is used for appropriating the Hessian multiplication operation, and $\\begin{array}{r}{\\alpha=\\frac{\\lambda}{r}}\\end{array}$ is the balance coefficient. Due to the space limit, the detailed derivation and algorithm are provided in Appendix B. After the activeness-aware fine-tuning stage, we can obtain a repaired clean model, which demonstrates outstanding performance in the experiments. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Attack Setup. We consider 8 SOTA backdoor attacks in the main experiment. There are BadNets [8], Blended [25], Input-aware [23], LF [49], SIG [26], SSBA [9], Trojan [50] and WaNet [24]. For a fair comparison, we follow the default attack configuration as in BackdoorBench [51], including the trigger pattern, trigger size, the target label (i.e., the $0^{t h}$ label), etc. We choose $10\\%$ poisoning ratio as the default setting. To fully evaluate the effectiveness of our proposed framework, all the attacks are implemented on three benchmark datasets, i.e., CIFAR-10 [13], Tiny ImageNet [52], and GTSRB [53], over two popular DNNs, i.e., PreAct-ResNet18 [12] and VGG19-BN [54]. In particular, SIG is only applied to CIFAR-10 since it cannot reach a $10\\%$ poisoning ratio on Tiny ImageNet and GTSRB. Besides, we also evaluate the defense methods under $5\\%$ and $1\\%$ poisoning ratios to verify the robustness of our proposed framework. Due to the space limit, we only exhibit parts of the main results in this section. More implementation details can be found in Appendix C. ", "page_idx": 6}, {"type": "text", "text": "Defense Setup. We compare the proposed framework with 8 SOTA backdoor defense methods: Fine-tuning (FT), Fine-pruning (FP) [37], NAD [43], NC [20], ANP [41], CLP [38], i-BAU [21], and RNP [22]. We use the recommended configurations in BackdoorBench [51]. Since the defense settings for training-stage defenses are different [14, 32], we only compare the post-training defenses, where $5\\%$ clean data can be accessed following the previous settings [41]. For TSBD, we set the learning rates for clean unlearning to $10^{-4}$ and fine-tuning to $10^{-2}$ . The default neuron ratio $n$ and weight ratio $m$ are set to 0.15 and 0.7, respectively. We follow the suggested settings of hyper-parameters $r=0.05$ and $\\alpha=0.7$ for fine-tuning [48]. ", "page_idx": 6}, {"type": "text", "text": "Evaluation Metrics. We use three metrics to evaluate the performance of each defense method: Accuracy on clean data (ACC), Attack Success Rate (ASR), and Defense Effectiveness Rating (DER) [55]. Specifically, ACC measures the proportion of clean data correctly predicted; ASR measures the proportion of poisoned data misclassified to the target label; $\\mathrm{DER}\\in[0,1]$ evaluates the cost of ACC for reducing ASR, which is defined as: $\\mathrm{DER}=[\\operatorname*{max}(0,\\Delta\\mathrm{ASR})-\\operatorname*{max}(0,\\Delta\\mathrm{ACC})+1]/2$ , where $\\Delta\\mathrm{ASR}$ and $\\Delta\\mathrm{ACC}$ are the drop in ASR and ACC after applying defense on the backdoored model, respectively. Larger ACC, DER, and smaller ASR are desired for a successful defense. Note that in the following result tables, \u201c-\u201d indicates that the value is inapplicable. The boldface values indicate the best performance and the underline values denote the second-best result. ", "page_idx": 6}, {"type": "text", "text": "4.2 Main Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We validate the effectiveness of our proposed framework on 8 SOTA backdoor attacks and compare it with 8 defenses. In this section, we present the main results on CIFAR-10 and Tiny ImageNet with ", "page_idx": 6}, {"type": "table", "img_path": "MfGRUVFtn9/tmp/56f5e8c412f70715ba194fc44b014a43b87c71f56a2bf500f9d90828bd721bba.jpg", "table_caption": ["Table 2: Comparison with the SOTA defenses on Tiny ImageNet dataset with PreAct-ResNet18 (%). "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "a $10\\%$ poisoning ratio on PreAct-ResNet18 for illustration, which is shown in Table 1 and Table 2.   \nMore results on GTSRB and VGG19-BN can be found in Appendix D and E, respectively. ", "page_idx": 7}, {"type": "text", "text": "Results on CIFAR-10. Table 1 shows the results on CIFAR-10. Results show that our TSBD outperforms all the other SOTA defenses on the average of ASR $(2.18\\%)$ and DER $(97.09\\%)$ , as well as a promising ACC $(91.70\\%)$ higher than the original \u201cNo Defense\u201d models $(91.34\\%)$ , which indicates its effectiveness in removing the backdoor effect with the least cost. Though most defenses fail in strong attacks Blended, LF, or SSBA, e.g., FT, FP, NAD, ANP, CLP, i-BAU, and RNP, our proposed TSBD success with the best ASR and DER on Blended and second best ASR and DER on LF and SSBA. FT and NAD perform similarly on each attack with a promising ACC, but they also fail on WaNet with a high ASR except for the mentioned strong attacks. FP and ANP perform well in ACC among all the defenses, while the defense performances on ASR and DER are unstable, which may be due to the unsuccessful backdoor-related neuron locating. CLP fails on most of the attacks with high ASR and low DER, which may be due to the structure constraint of computing channel Lipschitz only on the convolutional-batch normalization layer combination. i-BAU performs the second best on average ASR and DER, with failures on three attacks. TSBD outperforms RNP on almost all performances, which indicates that using clean unlearning with NWC is more effective than the unlearn-recovery process in RNP. ", "page_idx": 7}, {"type": "text", "text": "Results on Tiny ImageNet. Table 2 presents the results on Tiny ImageNet with PreAct-ResNet18. We observe that most defenses also fail on Blended, SSBA, and LF with high ASR. Similar performances of other attacks are shown on most of the defenses compared to CIFAR-10, where FT and FP also perform well in ACC and CLP fails on most attacks. Although i-BAU can successfully defend against most of the attacks in CIFAR-10, it fails on all attacks here, indicating that its adversarial training fails with large classification categories. For RNP, though it performs well in ASR on almost all attacks, the ACC is sacrificed too much to be unacceptable, indicating an unbalanced defense performance. In comparison, our TSBD can achieve SOTA on the average of DER, and perform second best on the average of ASR, which validates its superior defense performance. ", "page_idx": 7}, {"type": "text", "text": "4.3 Ablation Studies ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Effectiveness of NWC order for Backdoor Strength. To verify the effectiveness of employing clean-unlearning NWC order in gauging the backdoor strength, we borrow the Trigger-activated Change (TAC) [38] order as the ground truth and compare the neuron coverage ratio on TAC under different proportions, i.e., measuring the overlap of the selected neurons on both metrics. Specifically, TAC measures the change in neuron activation before and after the input image is attached with a trigger, where the larger value indicates a stronger backdoor effect. We select the following SOTA metrics for comparison: 1) the average neuron activations in FP [37]; 2) the channel Lipschitz in CLP [38]; 2) the perturb-recovery learned mask in ANP [41]; 3) the unlearn-recovery learned mask in RNP [22]. ", "page_idx": 7}, {"type": "image", "img_path": "MfGRUVFtn9/tmp/bc1aff22a89a17301cd520189515eeaafbc4823a456d422ba63ec262bdb71abd.jpg", "img_caption": ["Figure 4: Comparison of neuron coverage ratio on TAC under different neuron ratios. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "(reinitializing/pruning) neuron ratio and the y-axis represents the neuron coverage ratio on TAC. The higher values on the y-axis indicate a better matching of the current metric and the TAC metric, ", "page_idx": 7}, {"type": "image", "img_path": "MfGRUVFtn9/tmp/9c6d419d9f0bfa665913074f1ba86ecdf23d810b976a1b56a67087dca76096cb.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 5: Performance with different neuron ratios (two subfigures on the left) and weight ratios (two subfigures on the right) under the attacks of BadNets and Blended. ", "page_idx": 8}, {"type": "text", "text": "i.e., more backdoor-related neurons are chosen. We can observe that NWC is always the best under different neuron ratios compared to others. It validates the effectiveness of using NWC order as the metric for neuron reinitialization/pruning. Moreover, we also test the performance of using NWC in FP to show its superiority in assisting other defenses. The results can be found in Appendix F. ", "page_idx": 8}, {"type": "text", "text": "Effectiveness of Zero Reinitialization on Sub", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "weight. We compare different combinations of reinitializing neurons and weights to validate the effectiveness of zero reinitialization on the selected subweights. Specifically, three versions are designed for comparison: (1) $\\mathbf{V}_{1}$ : all weights on the selected top-NWC neurons are reinitialized to zero. (2) $\\mathbf{V}_{2}$ : $m\\%$ $70\\%$ is used as de", "page_idx": 8}, {"type": "table", "img_path": "MfGRUVFtn9/tmp/ae87b604dd39b6ab74551dc1f2f939e5a59536dfadac52040ffe8967a5ed1cec.jpg", "table_caption": ["Table 3: Comparison of different reinitialization schemes on CIFAR-10 with PreAct-ResNet18 $(\\%)$ . "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "fault) of the top weights on each selected top-NWC neuron are reinitialized to zero. (3) $\\mathbf{V}_{3}$ (Ours): our final version, where $m\\%$ of the top weights among all selected top-NWC neurons are reinitialized to zero. As the ACC and ASR drop monotonously with more neurons selected to be reinitialized, we record the results when ASR first drops to almost zero (i.e., lower equal than $0.05\\%$ ). Table 3 shows the performances on four attacks. It validates that reinitializing the top weights among the selected neurons may help alleviate the hurt of clean accuracy. The failure of $\\mathbf{V}_{2}$ may be blamed on the different backdoor strength of each neuron, i.e., some strong backdoor-related neurons fail to be removed thoroughly and thus more neurons are reinitialized to reach zero ASR. ", "page_idx": 8}, {"type": "text", "text": "Effectiveness of Gradient-norm Regulation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "in Fine-tuning. To validate the essential role of gradient-norm regulation in the fine-tuning stage, we compare the performance on \u201cno finetuning\u201d (No-FT), \u201cvanilla fine-tuning\u201d (VanillaFT), and our \u201cactiveness-aware fine-tuning\u201d (AaFT). For a fair comparison, we follow the default settings and choose to fine-tune the reinitialized ", "page_idx": 8}, {"type": "table", "img_path": "MfGRUVFtn9/tmp/257a5cf686471f74bb867adcade82bc80a97b9fdc76615ef4929e616352ce42e.jpg", "table_caption": ["Table 4: Comparison of different fine-tuning schemes on CIFAR-10 with PreAct-ResNet18 $(\\%)$ . "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "model after the second stage. The results are shown in Table 4. It shows that fine-tuning is effective in improving clean accuracy, which is sacrificed to erase the backdoored effect in reinitialization, though it is prone to bring back some extent of ASR. Furthermore, compared to vanilla fine-tuning, our implemented activeness-aware fine-tuning can suppress the rise of ASR as well as improve ACC to the same level. ", "page_idx": 8}, {"type": "text", "text": "4.4 Further Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Performance on Different Neuron Ratio and Weight Ratio. ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To test the sensitivity of our method towards hyper-parameter tuning, we record the performance of TSBD under a wide range of neuron ratio $(n\\bar{\\%})$ and weight ratio $(m\\%)$ tuning. The experiments are conducted on CIFAR-10 with a $10\\%$ poisoning ratio on PreAct-ResNet18. The neuron ratios are tuned from $1\\%$ to $90\\%$ , and weight ratios are tuned from $40\\%$ to $80\\%$ The results of BadNets and Blended attacks are depicted in Figure 5. It shows that TSBD is insensitive to both neuron ratio and weight ratio, where the ASR is kept at a very low level, while ACC is maintained at a top level under a wide range of tuning. The high consistency in the performances may come from the promising recovery ability of the fine-tuning stage. More results on other attacks are shown in the Appendix G. ", "page_idx": 8}, {"type": "table", "img_path": "MfGRUVFtn9/tmp/575a47dd84f5c9a1a144e3bd3ac14041110cd7a6fe38550a869c3324b2b51835.jpg", "table_caption": ["Table 5: Performance of hyperparameter tuning for stage 2. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Performance on Different $r$ and $\\alpha$ for Activeness-Aware Fine-tuning. To test the hyper-parameters sensitivity for stage 2, i.e., $r$ and $\\alpha$ in Activeness-Aware Fine-tuning, we follow the tuning range in [48] under our experimental settings. The results are shown in Teble 5, which are obtained from a BadNets-attacked PreAct-ResNet18. We observe that the performance is insensitive (Changes $-2\\%$ in ACC and ${<}1\\%$ in ASR) across different hyper-parameter settings, maintaining a high level of performance. ", "page_idx": 9}, {"type": "text", "text": "Performance on Different Poisoning Ratio. We further investigate the performance of TSBD on different poisoning ratios, e.g., $10\\%$ , $5\\%$ , and $1\\%$ . Note that a larger poisoning ratio represents a stronger attack mode. We test the performance with six attacks on these three ratios. Figure 6 shows the performances of ACC and ASR. We can observe that TSBD successfully defends all the attacks on $10\\%$ and $5\\%$ with a low ASR and a high ACC while performing less effectively on $1\\%$ . A possible reason is that the unlearning weight changes of backdoor neurons are less obvious in the weak attack mode compared to the strong attack mode with $10\\%$ or $5\\%$ poisoning ratios. ", "page_idx": 9}, {"type": "text", "text": "More Experiments and Analysis. Due to the space limit, we postpone the detailed discussion of the clean data ratio and the fine-tuning learning rate to Appendix H and Appendix I, respectively. We also evaluate the defense performance on the clean model in Appendix J and on the ViT in Appendix K. Further, we provide the computational overhead in terms of runtime in Appendix L. ", "page_idx": 9}, {"type": "image", "img_path": "MfGRUVFtn9/tmp/db87abe3837e9d674c8623999d9f672d2944f7b4e7df56dec1b637247ac0e696.jpg", "img_caption": ["Figure 6: ACC and ASR on different poisoning ratios. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we propose an effective two-stage backdoor defense method, TSBD, to eliminate the backdoor effect in DNNs. Our research reveals two important observations regarding the backdoored models to support our method. First, there is a positive correlation between weight changes during poison and clean unlearning in backdoored models. This finding enables us to identify and eliminate backdoor-related neurons through clean unlearning and zero reinitialization. Second, neurons in backdoored models are more active compared to those in clean models , which suggests regulating the gradient norm during fine-tuning. Furthermore, we also provide insights into these two observations from the perspective of neuron activations, which may be a valuable contribution to the field of backdoor defense. Extensive experiments demonstrate the superiority of our method over recent defenses. One current challenge as well as promising future work involves defending against backdoor attacks without any accessible clean data. The data generation techniques and data-free techniques may be the potential solutions. ", "page_idx": 9}, {"type": "text", "text": "6 Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported in part by the Guangzhou Municipal Science and Technology Project: Basic and Applied Basic research projects (No. 2024A04J4232), National Natural Science Foundation of China (No. 62101351), Guangzhou-HKUST(GZ) Joint Funding Program (Grant No.2023A03J0008), Education Bureau of Guangzhou Municipality, and Hetao Shenzhen-Hong Kong Science and Technology Innovation Cooperation Zone Project (No.HZQSWS-KCCYB-2024016). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Yaniv Taigman, Ming Yang, Marc\u2019Aurelio Ranzato, and Lior Wolf. Deepface: Closing the gap to human-level performance in face verification. In CVPR, 2014.   \n[2] Divyarajsinh N Parmar and Brijesh B Mehta. Face recognition methods applications. arXiv preprint arXiv:1403.0485, 2014.   \n[3] Ratnawati Ibrahim and Zalhan Mohd Zin. Study of automated face recognition system for office door access control application. In ICCSN, 2011.   \n[4] Yixiong Chen, Chunhui Zhang, Li Liu, Cheng Feng, Changfeng Dong, Yongfang Luo, and Xiang Wan. Uscl: pretraining deep ultrasound image diagnosis model through video contrastive representation learning. In MICCAI, 2021.   \n[5] Yixiong Chen, Li Liu, Jingxian Li, Hua Jiang, Chris Ding, and Zongwei Zhou. Metalr: Meta-tuning of learning rates for transfer learning in medical imaging. In MICCAI, 2023.   \n[6] Ekim Yurtsever, Jacob Lambert, Alexander Carballo, and Kazuya Takeda. A survey of autonomous driving: Common practices and emerging technologies. IEEE access, 2020.   \n[7] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for autonomous driving. In CVPR, 2020.   \n[8] Tianyu Gu, Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Evaluating backdooring attacks on deep neural networks. IEEE Access, 2019.   \n[9] Yuezun Li, Yiming Li, Baoyuan Wu, Longkang Li, Ran He, and Siwei Lyu. Invisible backdoor attack with sample-specific triggers. In ICCV, 2021.   \n[10] Baoyuan Wu, Li Liu, Zihao Zhu, Qingshan Liu, Zhaofeng He, and Siwei Lyu. Adversarial machine learning: A systematic survey of backdoor attack, weight attack and adversarial example. arXiv preprint arXiv:2302.09457, 2023.   \n[11] Runkai Zheng, Rongjun Tang, Jianze Li, and Li Liu. Pre-activation distributions expose backdoor neurons. In NeurIPS, 2022.   \n[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In ECCV, 2016.   \n[13] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[14] Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, and Xingjun Ma. Anti-backdoor learning: Training clean models on poisoned data. In NeurIPS, 2021.   \n[15] Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin Edwards, Taesung Lee, Ian Molloy, and Biplav Srivastava. Detecting backdoor attacks on deep neural networks by activation clustering. arXiv preprint arXiv:1811.03728, 2018.   \n[16] Brandon Tran, Jerry Li, and Aleksander Madry. Spectral signatures in backdoor attacks. In NeurIPS, 2018.   \n[17] Xiangyu Qi, Tinghao Xie, Jiachen T Wang, Tong Wu, Saeed Mahloujifar, and Prateek Mittal. Towards a proactive $\\{\\mathrm{ML}\\}$ approach for detecting backdoor poison samples. In USENIX Security, 2023.   \n[18] Min Liu, Alberto Sangiovanni-Vincentelli, and Xiangyu Yue. Beating backdoor attack at its own game. In ICCV, 2023.   \n[19] Weixin Chen, Baoyuan Wu, and Haoqian Wang. Effective backdoor defense by exploiting sensitivity of poisoned samples. In NeurIPS, 2022.   \n[20] Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y Zhao. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. In SP, 2019.   \n[21] Yi Zeng, Si Chen, Won Park, Zhuoqing Mao, Ming Jin, and Ruoxi Jia. Adversarial unlearning of backdoors via implicit hypergradient. In International Conference on Learning Representations, 2021.   \n[22] Yige Li, Xixiang Lyu, Xingjun Ma, Nodens Koren, Lingjuan Lyu, Bo Li, and Yu-Gang Jiang. Reconstructive neuron pruning for backdoor defense. arXiv preprint arXiv:2305.14876, 2023.   \n[23] Tuan Anh Nguyen and Anh Tran. Input-aware dynamic backdoor attack. In NeurIPS, 2020.   \n[24] Anh Nguyen and Anh Tran. Wanet\u2013imperceptible warping-based backdoor attack. arXiv preprint arXiv:2102.10369, 2021.   \n[25] Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep learning systems using data poisoning. arXiv preprint arXiv:1712.05526, 2017.   \n[26] Mauro Barni, Kassem Kallas, and Benedetta Tondi. A new backdoor attack in cnns by training set corruption without label poisoning. In ICIP, 2019.   \n[27] Ali Shafahi, W Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras, and Tom Goldstein. Poison frogs! targeted clean-label poisoning attacks on neural networks. In NeurIPS, 2018.   \n[28] Shihao Zhao, Xingjun Ma, Xiang Zheng, James Bailey, Jingjing Chen, and Yu-Gang Jiang. Clean-label backdoor attacks on video recognition models. In CVPR, 2020.   \n[29] Khoa Doan, Yingjie Lao, Weijie Zhao, and Ping Li. Lira: Learnable, imperceptible and robust backdoor attacks. In ICCV, 2021.   \n[30] Eugene Bagdasaryan and Vitaly Shmatikov. Blind backdoors in deep learning models. In USENIX Security, 2021.   \n[31] Khoa Doan, Yingjie Lao, and Ping Li. Backdoor attack with imperceptible input and latent modification. In NeurIPS, 2021.   \n[32] Kunzhe Huang, Yiming Li, Baoyuan Wu, Zhan Qin, and Kui Ren. Backdoor defense via decoupling the training process. In ICLR, 2022.   \n[33] Junfeng Guo, Ang Li, and Cong Liu. Aeva: Black-box backdoor detection using adversarial extreme value analysis. arXiv preprint arXiv:2110.14880, 2021.   \n[34] Wei Jiang, Xiangyu Wen, Jinyu Zhan, Xupeng Wang, Ziwei Song, and Chen Bian. Critical path-based backdoor detection for deep neural networks. TNNLS, 2022.   \n[35] Soheil Kolouri, Aniruddha Saha, Hamed Pirsiavash, and Heiko Hoffmann. Universal litmus patterns: Revealing backdoor attacks in cnns. In CVPR, 2020.   \n[36] Xiaojun Xu, Qi Wang, Huichen Li, Nikita Borisov, Carl A Gunter, and Bo Li. Detecting ai trojans using meta neural analysis. In SP, 2021.   \n[37] Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Fine-pruning: Defending against backdooring attacks on deep neural networks. In RAID, 2018.   \n[38] Runkai Zheng, Rongjun Tang, Jianze Li, and Li Liu. Data-free backdoor removal based on channel lipschitzness. In ECCV, 2022.   \n[39] Tianlong Chen, Zhenyu Zhang, Yihua Zhang, Shiyu Chang, Sijia Liu, and Zhangyang Wang. Quarantine: Sparsity can uncover the trojan attack trigger for free. In CVPR, 2022.   \n[40] Jiyang Guan, Zhuozhuo Tu, Ran He, and Dacheng Tao. Few-shot backdoor defense using shapley estimation. In CVPR, 2022.   \n[41] Dongxian Wu and Yisen Wang. Adversarial neuron pruning purifies backdoored deep models. In NeurIPS, 2021.   \n[42] Shuwen Chai and Jinghui Chen. One-shot neural backdoor erasing via adversarial weight masking. In NeurIPS, 2022.   \n[43] Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, and Xingjun Ma. Neural attention distillation: Erasing backdoor triggers from deep neural networks. arXiv preprint arXiv:2101.05930, 2021.   \n[44] Shaokui Wei, Mingda Zhang, Hongyuan Zha, and Baoyuan Wu. Shared adversarial unlearning: Backdoor mitigation by unlearning shared adversarial examples. Advances in Neural Information Processing Systems, 36, 2024.   \n[45] Lucas Bourtoule, Varun Chandrasekaran, Christopher A Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas Papernot. Machine unlearning. In 2021 IEEE Symposium on Security and Privacy $(S P)$ , pages 141\u2013159. IEEE, 2021.   \n[46] Xiong Xu, Kunzhe Huang, Yiming Li, Zhan Qin, and Kui Ren. Towards reliable and efficient backdoor trigger inversion via decoupling benign features. In The Twelfth International Conference on Learning Representations, 2024.   \n[47] Shaokui Wei, Mingda Zhang, Hongyuan Zha, and Baoyuan Wu. Shared adversarial unlearning: Backdoor mitigation by unlearning shared adversarial examples. In NeurIPS, 2023.   \n[48] Yang Zhao, Hao Zhang, and Xiuyuan Hu. Penalizing gradient norm for efficiently improving generalization in deep learning. In International Conference on Machine Learning, pages 26982\u201326992. PMLR, 2022.   \n[49] Yi Zeng, Won Park, Z Morley Mao, and Ruoxi Jia. Rethinking the backdoor attacks\u2019 triggers: A frequency perspective. In ICCV, 2021.   \n[50] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, and Xiangyu Zhang. Trojaning attack on neural networks. In NDSS Symposium, 2018.   \n[51] Baoyuan Wu, Hongrui Chen, Mingda Zhang, Zihao Zhu, Shaokui Wei, Danni Yuan, and Chao Shen. Backdoorbench: A comprehensive benchmark of backdoor learning. In NeurIPS Datasets and Benchmarks Track, 2022.   \n[52] Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 2015.   \n[53] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. The german traffic sign recognition benchmark: a multi-class classification competition. In IJCNN, 2011.   \n[54] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.   \n[55] Mingli Zhu, Shaokui Wei, Li Shen, Yanbo Fan, and Baoyuan Wu. Enhancing fine-tuning based backdoor defense with sharpness-aware minimization. In ICCV, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix Outline ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "This appendix is organized as follows: ", "page_idx": 13}, {"type": "text", "text": "\u2022 In Section A, we detail the algorithm of TSBD.   \n\u2022 In Section B, we detail the approximation process of the fine-tuning optimizations.   \n\u2022 In Section C, we introduce the implementation details, including the details of datasets, models, attacks, and defenses with our proposed method.   \n\u2022 In Section D, we compare the defense results on the GTSRB dataset.   \n\u2022 In Section E, we compare the defense results on VGG19-BN structure.   \n\u2022 In Section F, we show the effectiveness of using NWC in other defense.   \n\u2022 In Section G, we show the comprehensive results with different neuron ratios and weight ratios.   \n\u2022 In Section H, we discuss different clean data ratios on the defense performance.   \n\u2022 In Section I, we discuss different fine-tuning learning rates on the defense performance.   \n\u2022 In Section J, we evaluate the influence of using our method on the clean model.   \n\u2022 In Section K, we test the performance of scaled-up experiments on the ViT model.   \n\u2022 In Section L, we show the computational overhead in terms of runtime. ", "page_idx": 13}, {"type": "text", "text": "A Detailed Algorithm of TSBD ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "To clearly illustrate our proposed method, we provide the detailed algorithm of the entire process of TSBD, which is shown in Algorithm 1. ", "page_idx": 13}, {"type": "text", "text": "B Details of the Approximated Fine-Tuning Optimizations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "As illustrated in Section 3.4, our proposed Activaness-aware fine-tuning involves calculating an additional gradient-norm regulation in the loss function, making it computationally inefficient. Therefore, we adopt the approximation scheme from [48] during practical optimization. Here, we provide the details of the approximated fine-tuning optimization. The approximation deviation is shown in the following. Specifically, for each step of the gradient calculation, we formulate it as: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\hat{\\theta}}\\mathcal{L}_{f t}(\\hat{\\theta})=\\nabla_{\\hat{\\theta}}\\mathcal{L}_{c e}(\\hat{\\theta})+\\lambda\\cdot\\nabla_{\\hat{\\theta}}^{2}\\mathcal{L}_{c e}(\\hat{\\theta})\\frac{\\nabla_{\\hat{\\theta}}\\mathcal{L}_{c e}(\\hat{\\theta})}{\\|\\nabla_{\\hat{\\theta}}\\mathcal{L}_{c e}(\\hat{\\theta})\\|_{2}}}\\\\ &{\\quad\\quad\\quad\\quad\\approx\\nabla_{\\hat{\\theta}}\\mathcal{L}_{c e}(\\hat{\\theta})+\\frac{\\lambda}{r}\\cdot(\\nabla_{\\hat{\\theta}}\\mathcal{L}_{c e}(\\hat{\\theta}+r\\frac{\\nabla_{\\hat{\\theta}}\\mathcal{L}_{c e}(\\hat{\\theta})}{\\|\\nabla_{\\hat{\\theta}}\\mathcal{L}_{c e}(\\hat{\\theta})\\|_{2}})-\\nabla_{\\hat{\\theta}}\\mathcal{L}_{c e}(\\hat{\\theta}))}\\\\ &{\\quad\\quad\\quad\\quad=(1-\\alpha)\\nabla_{\\hat{\\theta}}\\mathcal{L}_{c e}(\\hat{\\theta})+\\alpha\\nabla_{\\hat{\\theta}}\\mathcal{L}_{c e}(\\hat{\\theta}+r\\frac{\\nabla_{\\hat{\\theta}}\\mathcal{L}_{c e}(\\hat{\\theta})}{\\|\\nabla_{\\hat{\\theta}}\\mathcal{L}_{c e}(\\hat{\\theta})\\|_{2}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "To avoid the Hessian computation, the second term is further approximated through an additional parameter update: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\nabla_{\\hat{\\theta}}\\mathcal{L}_{c e}(\\hat{\\pmb{\\theta}}+r\\frac{\\nabla_{\\hat{\\theta}}\\mathcal{L}_{c e}(\\hat{\\pmb{\\theta}})}{\\|\\nabla_{\\hat{\\theta}}\\mathcal{L}_{c e}(\\hat{\\pmb{\\theta}})\\|_{2}})\\approx\\nabla_{\\hat{\\theta}}\\mathcal{L}_{c e}(\\hat{\\pmb{\\theta}})\\vert_{\\hat{\\pmb{\\theta}}=\\hat{\\pmb{\\theta}}+r\\frac{\\nabla_{\\hat{\\theta}}\\mathcal{L}_{c e}(\\hat{\\pmb{\\theta}})}{\\|\\nabla_{\\hat{\\theta}}\\mathcal{L}_{c e}(\\hat{\\pmb{\\theta}})\\|_{2}}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Based on the approximation, the practical fine-tuning process is illustrated in Algorithm 2. ", "page_idx": 13}, {"type": "text", "text": "C More Implementation Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We further illustrate the implementations here, covering the details of datasets, models, attacks, and defenses. ", "page_idx": 13}, {"type": "text", "text": "Dataset Details. The experiments are conducted on CIFAR-10 [13], Tiny ImageNet [52], and GTSRB [53]. ", "page_idx": 13}, {"type": "text", "text": "RRRRU ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "\u2022 CIFAR-10. It contains $60\\!,\\!000\\,32\\!\\times\\!32$ colored images with 10 classes. Each class owns 6,000 images, consisting of 5,000 for training and 1,000 for testing.   \n\u2022 Tiny ImageNet. It is a subset of the full ImageNet, consisting of 100,000 training data and 10,000 testing data. There are 200 classes in total and 500 images per class for training. All images are $64\\!\\times\\!64$ with color.   \n\u2022 GTSRB. GTSRB (German Traffic Sign Recognition Benchmark) contains 39,209 images for training and 12,630 images for testing with 43 classes. All images are $32\\!\\times\\!32$ colored images. ", "page_idx": 14}, {"type": "text", "text": "Models. We choose PreAct-ResNet18 [12] and VGG19-BN [54] as the target models to conduct attacks and defenses following the default configurations in BackdoorBench [51]. Both of them contain convolutional layers and batch normalization layers, which can be implemented with all kinds of defense methods, e.g., FP [37] for the last convolutional layer and ANP [41] for the batch normalization layers. The extensive experiments on ablation study and further analysis are conducted on PreAct-ResNet18 by default. ", "page_idx": 14}, {"type": "text", "text": "Attack Details. We conduct 8 SOTA attacks for comprehensive testing, consisting of BadNets [8], Blended [25], Input-aware [23], LF [49], SIG [26], SSBA [9], Trojan [50] and WaNet [24]. All the attacks follow BackdoorBench\u2019s default configurations. Figure 7 shows all 8 attack triggers with the same example of CIFAR-10. Specifically, for BadNets, a $3\\!\\times\\!3$ white square is patched at the bottom-right corner of the images for CIFAR-10 and GTSRB, and a $6\\!\\times\\!6$ white square is for Tiny ImageNet. For Blended, a Hello-Ketty image is blended in the images with a 0.2 transparent ratio. We choose the $10\\%$ poisoning ratio and $0^{t h}$ label as the default setting to conduct attacks and test all defenses following the previous works [55, 47]. $5\\%$ and $1\\%$ poisoning ratios are conducted only for testing our proposed method. ", "page_idx": 14}, {"type": "text", "text": "Defense Details. We conduct 8 SOTA defenses for a comprehensive comparison, containing Finetuning (FT), Fine-pruning (FP) [37], NAD [43], NC [20], ANP [41], CLP [38], i-BAU [21], and RNP [22]. The defenses also follow the BackdoorBench\u2019s default configurations. Note that we compare only the post-training defenses with $5\\%$ benign data provided. The learning rate for all methods is set to $10^{-2}$ , the batch size is set to 256. For RNP, the clean data ratio is set to $0.5\\%$ since we found that it failed to defend well under the $5\\%$ setting. For our proposed method, we set the default learning rates for unlearning as $10^{-4}$ and fine-tuning as $10^{-2}$ . The unlearning is stopped when clean accuracy drops to or below $10\\%$ . The default fine-tuning epoch is set to 20. The default neuron ratio $n$ and weight ratio $m$ are set to 0.15 and 0.7, respectively. We follow the suggested settings of hyper-parameters $r=0.05$ and $\\alpha=0.7$ for fine-tuning [48]. Other settings are set following the default BackdoorBench configuration. ", "page_idx": 14}, {"type": "text", "text": "All experiments are conducted on a server with GPU RTX 3090 and CPU AMD EPYC 7543 32-Core Processor. These experiments were successfully executed using less than 24G of memory on a single GPU card. ", "page_idx": 14}, {"type": "text", "text": "D Evaluations on GTSRB dataset ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We validate the effectiveness of our proposed method in the dataset GTSRB other than CIFAR-10 and Tiny ImageNet. Table 6 shows the corresponding performance on PreAct-ResNet18 with $10\\%$ poisoning ratio and $5\\%$ clean data ratio. We can observe that TSBD performs consistently with the lowest average ASR and the largest average DER as in CIFAR-10. Most of the defenses also fail in the strong attacks Blended, LF, and Trojan, while NC performs the second best with comparable average ASR and DER. Compared to the other two datasets, TSBD performs much superior with ", "page_idx": 14}, {"type": "text", "text": "Algorithm 1 Two-Stage Backdoor Defense ", "page_idx": 15}, {"type": "text", "text": "Input: Small clean set $\\mathcal{D}_{c}$ , backdoored model with parameter $\\theta_{b d}$ , max iteration number $T$ , neuron   \nratio $n\\%$ and weight ratio $m\\%$ for reinitialization   \nOutput: Clean model with parameter $\\pmb{\\theta}^{*}$ ", "page_idx": 15}, {"type": "text", "text": "1: $/*$ Neuron Weight Change-based Backdoor Reinitialization \\*/   \n// a. Clean Unlearning   \n2: while Clean accuracy on $\\theta_{u l}>10\\%$ do   \n3: Sample a mini-batch $\\scriptstyle{\\mathcal{B}}_{c}$ from $\\mathcal{D}_{c}$   \n4: $\\pmb{\\theta}_{u l}\\dot{\\leftarrow}\\operatorname*{max}_{\\pmb{\\theta}_{b d}}\\mathcal{L}\\left(f\\left(\\boldsymbol{\\mathcal{B}}_{c};\\pmb{\\theta}_{b d}\\right)\\right)$   \n5: end while   \n// b. Neuron Weight Change Calculation.   \n6: Record subweight changes and calculate NWC by Equation (2) w.r.t. $\\theta_{u l}$ and $\\theta_{b d}$   \n// c. Zero Reinitialization.   \n7: Obtain and sort the weight changes from the top- $n\\%$ neurons w.r.t. NWC   \n8: $\\hat{\\pmb{\\theta}}\\gets$ reinitialize $m\\%$ of the most-changing weights into zero on $\\theta_{b d}$   \n9: $/*$ Activeness-aware Fine-tuning $^{*}\\!/$   \n10: for $t=1$ to $T$ do   \n11: Sample a mini-batch $\\scriptstyle{\\mathcal{B}}_{c}$ from $\\mathcal{D}_{c}$   \n12: Update $\\hat{\\pmb{\\theta}}$ by Equation (4) with the approximated Equation (6)   \n13: end for   \n14: $\\pmb{\\theta}^{*}\\leftarrow$ fine-tuned $\\hat{\\pmb{\\theta}}$ ", "page_idx": 15}, {"type": "text", "text": "Algorithm 2 The Approximated Optimization of Activeness-aware Fine-tuning ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Input: Small clean set $\\mathcal{D}_{c}$ , the reinitialized model with parameter $\\hat{\\pmb\\theta}$ , max iteration number $T$ ,   \napproximation scalar $r$ , balance coefficient $\\alpha$   \nOutput: Clean model with parameter $\\pmb{\\theta}^{*}$   \n1: $/*$ Activeness-aware Fine-tuning $^{*}\\!/$   \n2: for $t=1$ to $T$ do   \n3: Sample a mini-batch $\\scriptstyle{\\mathcal{B}}_{c}$ from $\\mathcal{D}_{c}$   \n4: Calculate the gradient $g_{1}=\\nabla_{\\hat{\\pmb{\\theta}}}\\mathcal{L}_{c e}(\\hat{\\pmb{\\theta}})$ with $\\displaystyle{\\mathcal{B}}_{c}$   \n5: Temporally update the current parameter $\\begin{array}{r}{\\hat{\\pmb{\\theta}}^{\\prime}=\\hat{\\pmb{\\theta}}+r\\frac{\\nabla_{\\hat{\\pmb{\\theta}}}\\mathcal{L}_{c e}(\\hat{\\pmb{\\theta}})}{\\|\\nabla_{\\hat{\\pmb{\\theta}}}\\mathcal{L}_{c e}(\\hat{\\pmb{\\theta}})\\|_{2}}}\\end{array}$   \n6: Calculate the gradient $g_{2}=\\nabla_{\\hat{\\pmb{\\theta}}^{\\prime}}\\mathcal{L}_{c e}(\\hat{\\pmb{\\theta}}^{\\prime})$ with $\\scriptstyle{\\mathcal{B}}_{c}$   \n7: Calculate the final gradient $g=(1-\\alpha)g_{1}+\\alpha g_{2}$   \n8: Update the parameter $\\hat{\\pmb\\theta}$ based on $g$   \n9: end for   \n10: $\\pmb{\\theta}^{*}\\leftarrow$ fine-tuned $\\hat{\\pmb{\\theta}}$ ", "page_idx": 15}, {"type": "text", "text": "most ASRs lower than $0.5\\%$ except for Blended attack. This suggests that TSBD might perform better when the model is learned with the input images with similar characteristics. ", "page_idx": 15}, {"type": "text", "text": "E Performance on VGG19-BN model structure ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Except for PreAct-ResNet18, we also test another model, VGG19-BN. The performance on CIFAR10 with $10\\%$ poisoning ratio and $5\\%$ clean data ratio is illustrated in Table 7. We follow similar settings in PreAct-ResNet18 on CIFAR-10, conducting 8 attacks and comparing our method with 8 defenses. The performance is also evaluated by ACC, ASR, and DER. As shown in the table, TSBD owns SOTA performance on VGG19-BN with the best average ACC and second-best average ASR and DER. Most of the other performances are in a similar pattern as on PreAct-ResNet18. Although ANP performs almost the best in ASR and DER for all attacks, it damages the corresponding ACC as well, especially for the WaNet attacked model, where ACC decreases from $84.58\\%$ to $78.04\\%$ . On the contrary, our methods succeed in most attacks with high ACC and comparable ASR. ", "page_idx": 15}, {"type": "table", "img_path": "MfGRUVFtn9/tmp/627181b464f9da27ae85d96c893de57fc99f750aa9e52091bfc5cd776cb9da02.jpg", "table_caption": ["Table 6: Comparison with the SOTA defenses on GTSRB dataset with PreAct-ResNet18 $(\\%)$ . "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "MfGRUVFtn9/tmp/6a991090b157116d0db8055a6a6e1a35257ed8928d652348dcb0295dfad7cf12.jpg", "table_caption": ["Table 7: Comparison with the SOTA defenses on CIFAR-10 dataset with VGG19-BN(%). "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "F Effectiveness of using NWC in Other Defense ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "As illustrated in Section 4.3, it is effective to utilize NWC order in gauging the backdoor strength. To further test its ability to improve other defenses, we substitute the average neuron activation in FP to NWC (denoted as NWC-FP) and conduct the experiments on the default settings in PreAct-ResNet18 on CIFAR-10. Different from zero reinitialization, the pruned neurons will not be updated on the following fine-tuning. The performances of 8 attacks are shown in Figure 8. We can investigate that, by using NWC in FP, performances on ASR and DER are improved on most of the attacks, with few effects on ACC. It exhibits the potential of using our NWC in more defense methods. ", "page_idx": 16}, {"type": "text", "text": "G Performance with Different Neuron Ratio and Weight Ratio ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Section 4.4 shows some of the performances with different neuron ratios and weight ratios and verifies the robustness of the hyper-parameter tuning in our method. Here, we exhibit the full results under all ", "page_idx": 16}, {"type": "image", "img_path": "MfGRUVFtn9/tmp/92b64611ce7266153e68db1031920b95aa7eddd9dba5b5d7b0c5487a75a0065e.jpg", "img_caption": ["Figure 8: Performance comparisons between the original FP and the FP with NWC (NWC-FP) under 8 attacks. Left: ACC; Middle: ASR; Right: DER. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "MfGRUVFtn9/tmp/f5ed7f0877a9bbdd4515b4f174f0d465bafacc40cff31fe3f6796fc681b1fa83.jpg", "img_caption": ["Figure 9: Performance with different neuron ratios under 8 attacks. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "MfGRUVFtn9/tmp/872a99110e731b32e0c89d903f3a9b3ace1337f0ddfe801936967ca152cda278.jpg", "img_caption": ["Figure 10: Performance with different weight ratios under 8 attacks. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "8 attacks in PreAct-ResNet18 on CIFAR-10 with $10\\%$ poisoning ratio. Figure 9 and Figure 10 exhibit the tuning of neuron ratio and weight ratio, respectively. All performances are as good as expected. ", "page_idx": 17}, {"type": "text", "text": "H Evaluations with Different Clean Data Ratios ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We are here to test the performance of TSBD under different clean data ratios. The performance on CIFAR-10 and PreAct-ResNet18 with $10\\%$ poisoning ratio is illustrated in Table 8. Except for the default $5\\%$ clean data ratio, we also test the performance under $10\\%$ , $1\\%$ , and $0.5\\%$ . We can observe that a larger ratio of clean data can always bring better performance in ACC, e.g., $92.18\\%>91.70\\%>\\bar{8}9.63\\%>86.42\\%$ on average values of the four ratios in decreasing order, respectively. For the defense performance in ASR, all of them can successfully defend the attacks to under $10\\%$ , and large clean data ratios can achieve promising DERs. Overall, TSBD is robust to the clean data ratio, with only $0.5\\%$ clean data can also defend most tested defenses successfully. ", "page_idx": 17}, {"type": "text", "text": "I Evaluations with Different Learning Rates on Fine-tuning ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We test the performance of our proposed method under different learning rates on fine-tuning. Table 9 illustrates the results. Specifically, with the same settings on CIFAR-10 and PreAct-ResNet18 with $10\\%$ poisoning ratio and $5\\%$ clean data ratio, we test the performance when the learning rate is set to 0.1, 0.01, 0.001, and 0.0001, where 0.01 is our default setting in the previous experiments. We can observe that the default learning rate of 0.01 is the most suitable one chosen for defense, which can achieve SOTA ASR and DER on average with comparable average ACC. Besides, setting the learning rate to 0.001 can perform the best in ACC, while making it fail on some strong attacks, e.g., ", "page_idx": 17}, {"type": "table", "img_path": "MfGRUVFtn9/tmp/a3fd2be2ae86e465d933b2285ca471f3619283ec7969084e3cb3549459b25fe2.jpg", "table_caption": ["Table 8: Performance with Different Clean Data Ratios on CIFAR-10 dataset with PreAct-ResNet18 $(\\%)$ . "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "MfGRUVFtn9/tmp/bf0d86e6663dc52b0a8a6e4cc2ad4c6b5fb61f376118db669c739e994bd3179c.jpg", "table_caption": ["Table 9: Performance with Different Learning Rates of Fine-tuning on CIFAR-10 dataset with PreAct-ResNet18 $(\\%)$ . "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Blended and LF. On the contrary, setting the learning rate to 0.1 will hurt the ACC greatly, which may imply that clean knowledge cannot be re-learned properly. ", "page_idx": 18}, {"type": "text", "text": "J Evaluations on Clean Model ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To test whether our defense method will hurt the performance of the clean model if no backdoor attack occurs, we compare it with several defense methods on CIFAR-10 and Tiny ImageNet datasets on PreAct-ResNet18. Figure 11 illustrates the results, where the ACC and ASR on the original clean model, ANP, and RNP, are used for comparison. We can observe that most of the defense methods (including ours) will barely damage the clean model though there is no backdoor occurs, except for RNP on Tiny ImageNet. It exhibits the potential to largely employ our defense methods in real-world AI systems. ", "page_idx": 18}, {"type": "text", "text": "K Performance on Scaled-Up Experiments ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To further verify the scalability of our method, we evaluate its performance on a ViT-b-16 model with the CIFAR10 dataset following the basic settings in Section 4.1, e.g., the poisoning ratio is set to $10\\%$ and the target label is set to 0. The results are shown in Table 10. The results demonstrate that TSBD performs effectively on the scaled-up model, achieving a low ASR and acceptable ACC. In contrast, CLP and ANP fail completely with ASR still at a high level, particularly for the WaNet attack. ", "page_idx": 18}, {"type": "text", "text": "L Computational Overhead ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To show the computational overhead of TSBD, we record the average computational time of each defense step and its practical runtime compared to other defense methods. Table 11 exhibits the computational time of TSBD, including Clean Unlearning, NWC Calculation, Zero Reinitialization, and Activeness-Aware Fine-Tuning. We observe that the main computational overhead lies in the finetuning process. In contrast, the time required for clean unlearning does not increase proportionally with dataset complexity. This means that TSBD is as efficient as other fine-tuning-based methods. Moreover, we present a practical runtime comparison with other SOTA defenses in Table 12, including the loading and testing time needed in practice. As we can see, TSBD is faster than most of the existing methods. ", "page_idx": 18}, {"type": "image", "img_path": "MfGRUVFtn9/tmp/83dfc45eff91b88892dabf1bf2e0eee5558a0b6f03b900a112076d57df7e5112.jpg", "img_caption": ["Figure 11: Performance of Defenses on Clean Model. "], "img_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "MfGRUVFtn9/tmp/ee30e93fece0ea45f9774948e7ab25d4828d5e5ae470e599b015bb0c5d4d7649.jpg", "table_caption": ["Table 10: Performance on CIFAR-10 with ViT-b-16 (%). "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "MfGRUVFtn9/tmp/357d2c887712f78b91a563878a247b2c30a3c4e2c8bed9b274618082347d25f5.jpg", "table_caption": ["Table 11: Computational Time of Each Defense Step of TSBD "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "MfGRUVFtn9/tmp/dfe8aae0dc94d6503378da65e84d587810614b7811cdd44f2c58db1510e2d293.jpg", "table_caption": ["Table 12: Practical Runtime Comparison on BackdoorBench "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The contributions and scope are accurately written in the Introduction Section. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: The last sentence of the Conclusion section clearly states the limitation and potential solution. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: All the results are empirically illustrated. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: All experimental settings are clearly illustrated in the Experiment section and Appendix C. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: All the data are open-sourced. The code is provided in Supplementary Material, and will be publicly available. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: All settings and details are provided in the Experiment section and Appendix C. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [No] ", "page_idx": 22}, {"type": "text", "text": "Justification: We strictly follow the basic settings in the open-sourced BackdoorBench [51], which ensures the fairness of all the comparisons in the Experiment section. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The computer resources are included in Appendix C, and the official information from BackdoorBench, since we follow their default settings. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 23}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We strictly conform the NeurIPS Code of Ethics. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The experiment in Appendix K exhibits the positive societal impacts of potentially employing our defense method without hurting the model performance in the real-world AI system. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 23}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 24}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our defense method has only a positive impact on use. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 24}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: All licenses are explicitly mentioned. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We provide the documentation for the summit code in Supplementary Material, and it will be publicly available. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 25}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 25}]