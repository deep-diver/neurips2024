[{"type": "text", "text": "Mitigating Backdoor Attack by Injecting Proactive Defensive Backdoor ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Shaokui Wei1 Hongyuan Zha1,2 Baoyuan Wu1\u2217 1School of Data Science, ", "page_idx": 0}, {"type": "text", "text": "The Chinese University of Hong Kong, Shenzhen, Guangdong, 518172, P.R. China 2Shenzhen Key Laboratory of Crowd Intelligence Empowered Low-Carbon Energy Network ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Data-poisoning backdoor attacks are serious security threats to machine learning models, where an adversary can manipulate the training dataset to inject backdoors into models. In this paper, we focus on in-training backdoor defense, aiming to train a clean model even when the dataset may be potentially poisoned. Unlike most existing methods that primarily detect and remove/unlearn suspicious samples to mitigate malicious backdoor attacks, we propose a novel defense approach called PDB (Proactive Defensive Backdoor). Specifically, PDB leverages the \u201chome field\u201d advantage of defenders by proactively injecting a defensive backdoor into the model during training. Taking advantage of controlling the training process, the defensive backdoor is designed to suppress the malicious backdoor effectively while remaining secret to attackers. In addition, we introduce a reversible mapping to determine the defensive target label. During inference, PDB embeds a defensive trigger in the inputs and reverses the model\u2019s prediction, suppressing malicious backdoor and ensuring the model\u2019s utility on the original task. Experimental results across various datasets and models demonstrate that our approach achieves stateof-the-art defense performance against a wide range of backdoor attacks. The code is available at https://github.com/shawkui/Proactive_Defensive_Backdoor. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In recent years, deep neural networks (DNNs) have become ubiquitous across diverse fields, powering applications such as face recognition, self-driving vehicles, and medical image analysis [1, 13, 25, 38]. However, alongside these advancements, the vulnerability of DNNs to malicious attacks presents a critical challenge to their safety and reliability. A particularly alarming threat arises from backdoor attacks, where adversaries secretly introduce backdoors into DNN models during training by subtly altering a fraction of the dataset. This manipulation ensures the model\u2019s standard performance on uncontaminated data but erroneously assigns a pre-determined label to any input carrying a specific trigger. Considering its real threats to machine learning systems, especially in security-critical scenarios, it\u2019s a practical necessity to investigate and propose effective defense strategies against such attacks to safeguard real-world applications. ", "page_idx": 0}, {"type": "text", "text": "To mitigate the threats posed by backdoor attacks, researchers have actively explored various backdoor defense techniques throughout the life cycle of machine learning systems [45]. In this paper, we specifically delve into in-training backdoor defense [44\u201346], which aims to train machine learning models using datasets that may be contaminated with poisoned data. Most existing methods in this field primarily focuses on identifying suspicious samples through various means, along with mitigating the backdoor effect by directly removing [3, 48] or applying some techniques (e.g., unlearning [20, 4], or relabel [15, 26, 60]) to the suspicious samples. Despite achieving remarkable performance in backdoor defense, these methods face certain limitations and challenges. First, most existing works rely on specific assumptions such as the latent separability [3] or the early learning of poisoned samples [20, 60, 51] to identify the poisoned samples. However, these assumptions may not hold under more sophisticated attacks [31]. As accurately detecting poisoned samples is crucial for those methods, any deviation from their underlying assumptions could lead to performance degradation and compromise their effectiveness. Second, some methods, such as DBD [15], NAB [26], and V&B [60], necessitate complex modifications to the training process, resulting in a substantial increase in training costs. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this paper, instead of following the traditional detection-and-mitigation pipeline, we propose a proactive approach that leverages the \u201chome field\u201d advantage of defenders. Our method, called PDB (short for Proactive Defensive Backdoor), aims to fight malicious backdoor attacks by injecting a proactive defensive backdoor introduced by the defenders themselves. The primary objective of PDB is to suppress the malicious backdoor with a defensive backdoor while keeping the model\u2019s utility on original task. Specifically, When the defensive trigger is presented, the defensive backdoor will dominate the prediction of the proactively backdoored model, effectively suppressing the malicious backdoor\u2019s impact. Importantly, our defensive backdoor allows for the restoration of the ground truth label to maintain the model\u2019s utility on the original task. To achieve this goal, we first analyze the objective for an effective defensive backdoor and introduce four essential design principles, including reversibility, inaccessibility to attackers, minimal impact on model performance, and resistance against other backdoors. Then, we construct an additional defensive poisoned dataset, subsequently utilizing such dataset and the whole poisoned dataset to train the model. Consequently, if only the malicious trigger is present, the model remains under the control of the malicious backdoor. However, when the defensive trigger appears, the defensive backdoor is activated, mitigating the malicious backdoor effect. To evaluate its effectiveness, we compare PDB with five state-of-the-art (SOTA) in-training defense methods across seven SOTA data-poisoning backdoor attack methods involving different model structures and datasets. Our experimental results demonstrate that PDB achieves comparable or even superior performance compared to existing baselines. ", "page_idx": 1}, {"type": "text", "text": "Our main contributions are threefold: 1) We break away from the traditional detection-and-mitigation pipeline by proposing a novel mechanism that injects a proactive defensive backdoor during training, which suppresses the malicious backdoor while preserving the model\u2019s utility on the original task, without any specific assumptions about potential malicious backdoor attacks. 2) By analyzing the primary objective, we introduce essential design principles for an effective defensive backdoor and propose a practical algorithm to implement the defensive backdoor. 3) We conduct extensive experiments to evaluate the effectiveness of our method and compare it with five SOTA defense methods across seven challenging backdoor attacks, spanning diverse model structures and datasets, demonstrating the superior performance of the proposed method. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Backdoor attacks. DNNs face significant security threats from backdoor attacks, which are designed to maintain normal performance on regular inputs while forcing the network to output a predetermined target when a specific trigger is introduced. These attacks can be generally categorized into two types based on the property of the trigger: static-pattern backdoor attacks and dynamicpattern backdoor attacks. The seminal instance of static-pattern backdoors, BadNets [12], employed fixed triggers like white squares. To enhance trigger stealthness, the Blended approach [5] was introduced, which merges the trigger with the host image in a subtle manner. Recognizing the potential for detection in fixed-pattern triggers, the research has pivoted towards dynamic-pattern backdoor attacks. Innovations in this direction, such as SSBA [22], WaNet [30], LF [49], WPDA [35], IRBA [10], VSSC [41] and TAT [6], have focused on crafting sample-specific triggers that are more challenging to identify. Techniques to refine the stealthness of triggers have been furthered by works like Sleeper-agent [36] and Lira [8], which optimize the output to be more covert. The sophistication of backdoor attacks has recently been advanced by strategies for learning-based poisoning sample selection [58] and re-activation attack [57]. To execute attacks without altering the consistency between the image and its label, \u2019clean label\u2019 attacks have been introduced. For example, LC [33] and SIG [2] employed counterfactual methods and additional techniques to modify the image while maintaining label consistency subtly. ", "page_idx": 1}, {"type": "text", "text": "Backdoor defenses. The main purpose of backdoor defense is to alleviate the vulnerabilities of DNNs to backdoor attacks by employing various strategies during different stages of the model lifecycle. Therefore, backdoor defenses are typically categorized into three types: pre-training, in-training, and post-training. Pre-training defenses concentrate on the detection and removal of poisoned data points before training. For example, AC [3] leverages unusual activation patterns to weed out poisoned data, while Confusion Training [32] relies on a model trained specifically to recognize poisoned instances. VDC [59] utilizes the capabilities of large multimodal language models for the same purpose. Post-training defenses are applied after a model has been trained. A line of works in this direction focusing on pruning [24, 47, 53, 52, 23] or fine-tuning [55, 28] to neutralize the backdoor. Besides, I-BAU [50], NPD [56], and SAU [43] reverse potential backdoor triggers by adversarial techniques to cleanse the model. NAD [21] employs a slightly poisoned model to assist in retraining a heavily compromised one. ", "page_idx": 2}, {"type": "text", "text": "This paper mainly focuses on the in-training defenses that aim to prevent backdoor insertion during the training phase. Along this direction, ABL [20] utilizes the observation that the poisoned samples are easier to learn than normal samples, resulting in the different learning speeds between benign and poisoned samples, to detect and unlearn the poisoned samples. Based on similar observation, V&B [60] first trains a backdoored model to capture the backdoor effect and utilizes the backdoored model to train a benign model by detecting and applying a series of operations on the suspicious samples. Similarly, CBD [51] first trains a backdoored model for a few epochs and trains a benign model by reweighting the samples and deconfounding the representation. DBD [15] splits the training process into three steps and employs self-supervised learning techniques to detect suspicious samples and train a benign model. D-ST [4] leverages the fact that benign samples are less sensitive to image transformations to detect suspicious samples and employs semi-supervised learning to train a benign model. Recently, a few attempts have been made to defend against malicious attacks by incorporating proactive attacks [54, 26]. The work most closely aligned with our approach is NAB [26], which first identifies and then relabels potentially poisoned samples in the dataset, subsequently embedding non-adversarial triggers into the suspicious samples to mitigate the backdoor effect. In contrast to their methodology, our technique offers a more straightforward solution, eliminating the need for costly detection and relabeling processes, thus reducing overall costs and complexity. In essence, we demonstrate that injecting a defensive backdoor alone is sufficient to defend against backdoor attacks without requiring detection and relabeling of the poisoned samples. We refer readers to [45] for more defense in adversarial machine learning. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In Section 3.1, we introduce the essential notations and define the threat model in this paper. Subsequently, we explore the principles behind effective defensive backdoors, illustrated with practical examples in Section 3.2. We present the overall pipeline for our proposed method in Section 3.3. ", "page_idx": 2}, {"type": "text", "text": "3.1 Problem setting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notations. Considering a sample $x\\in\\mathcal{X}$ with label $y\\in\\mathcal{V}$ , a DNN model $f_{\\theta}$ parameterized by $\\pmb{\\theta}$ is trained to classify $\\textbf{\\em x}$ . The space $y=[1,\\cdot\\cdot\\cdot\\,,K]$ denotes the space of candidate labels $[K\\geq2]$ ), and $\\mathcal{X}$ represents the sample space. In the context of backdoor attack, we denote the trigger by $\\Delta$ and the trigger injection operator by $\\bigoplus$ . Consequently, given a benign sample $\\textbf{\\em x}$ , the poisoned sample can be generated by $\\pmb{x}\\oplus\\Delta$ . It\u2019s important to note that the injection operator $\\oplus$ can vary according to the type of trigger $\\Delta$ . ", "page_idx": 2}, {"type": "text", "text": "Threat model. We consider a data poisoning scenario for malicious backdoor attack where the attacker can only manipulate a portion of the training dataset to plant trigge but cannot control the training process. By poisoning the dataset, the model trained on the manipulated dataset $\\mathcal{D}_{t r}$ would normally perform for benign input but classify the inputs with malicious trigger $\\Delta$ to predefined target $\\hat{y}$ . Besides, we define the portion of manipulated samples as the poisoning ratio of backdoor attack. ", "page_idx": 2}, {"type": "text", "text": "The defender faces a situation where a potentially poisoned dataset is given. The defender aims to train a model where the malicious backdoor fails to be activated by the malicious trigger, and the model\u2019s utility on the original task is maintained. We assume a small benign dataset $\\mathcal{D}_{c l}$ is reserved for the defender, which can be obtained by various means, including but not limited to purchase from reputable data vendors, generation via state-of-the-art generative models [7, 11, 16], collection from the internet, or applying data cleansing methods [45]. Moreover, we assume that the defender does not have knowledge of either the malicious trigger $\\Delta$ or the malicious target label $\\hat{y}$ . ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3.2 Proactive defensive backdoor ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this paper, we aim to defend the unknown malicious backdoor with the trigger $\\Delta$ , by inserting a proactive defensive backdoor with a trigger $\\Delta_{1}$ into the model. Our primary objective is to ensure that when $\\Delta_{1}$ is presented, the model\u2019s output will be controlled by $\\Delta_{1}$ rather than $\\Delta$ , thereby suppressing the malicious backdoor. Besides, the model\u2019s utility on the original task should be preserved, i.e., user can still get the true prediction of the benign sample with the defensive trigger. To achieve such a defense goal, the desired defensive backdoor attack should follow the principles below: ", "page_idx": 3}, {"type": "text", "text": "\u2022 Principle 1: Reversibility. The defensive backdoor must be reversible, such that the ground truth label can be restored from the prediction of benign samples attached with $\\Delta_{1}$ . Such a requirement is crucial for preserving the model performance on benign inputs with $\\Delta_{1}$ . ", "page_idx": 3}, {"type": "text", "text": "\u2022 Principle 2: Inaccessibility to attackers. The defensive trigger $\\Delta_{1}$ should be meticulously designed to be non-replicable and undiscoverable by potential attackers. By doing so, we prevent adversaries from exploiting the same trigger or using inversion techniques to identify it. ", "page_idx": 3}, {"type": "text", "text": "\u2022 Principle 3: Minimal impact on model performance. While stealth is not a strict requirement for the defensive trigger, modified samples should retain sufficient characteristics of the original data. This ensures accurate label recovery from the model\u2019s predictions in the presence of $\\Delta_{1}$ . ", "page_idx": 3}, {"type": "text", "text": "\u2022 Principle 4: Resistance against other backdoors. To effectively mitigate malicious backdoors, the defensive backdoor should be resistant to various backdoor attacks, not only known attacks but also potential future backdoors. ", "page_idx": 3}, {"type": "text", "text": "In light of the principles outlined above, we delve into the practical design of our defensive backdoor2. ", "page_idx": 3}, {"type": "text", "text": "Following Principle 1. For the first principle, we propose to assign the target label by a bijective mapping $h:y\\rightarrow y$ , such that the target label of a sample with label $y$ is $h(y)$ and the ground truth label of a poisoned image with label $y$ is $h^{-1}(y)$ . A typical choice of $h$ and $h^{-1}$ is $h(y)=(y+1)$ mod $K$ and $h^{-1}(y)=(y-1)$ mod $K$ where mod represents the modulo operation and $K$ is the number of classes. It\u2019s worth noting that in the context of DNNs, $h$ can also be formulated as a function of logits or fea", "page_idx": 3}, {"type": "image", "img_path": "cbkJBYIkID/tmp/a8683f1665ce66fa82e5851b34d3e6ded4ceec43e8e905dac26a0836113ca95b.jpg", "img_caption": ["Figure 1: Illustration of bijective mapping with $h(\\bar{y})=(y+1)$ mod $K$ , with $K=4$ . "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "tures such as $h(\\phi(\\mathbf{x}))=-\\phi(\\mathbf{x})$ and $\\begin{array}{r}{\\bar{h}^{-1}(\\phi(\\pmb{x}))=-\\phi(\\pmb{x})}\\end{array}$ where $\\phi(x)$ corresponds to the features or logits of input. This flexibility allows for a broader range of target label assignment strategies. ", "page_idx": 3}, {"type": "text", "text": "Following Principle 2 & 3. To follow the second and third principles, the design of the trigger is essential. Consider the patched trigger as an illustrative example, which can be constructed by carefully determining its position and pattern. Regarding the trigger\u2019s position, it should be crafted to preserve the core visual patterns of the original image, ensuring that the primary content remains unaltered. As for the trigger\u2019s pattern, we leverage the \u201chome field\u201d advantage of the defender, designing a trigger that operates beyond the conventional pixel space. Specifi", "page_idx": 3}, {"type": "image", "img_path": "cbkJBYIkID/tmp/aa4363e36298b0a7aac4e32d714c67e8fb0e953947ce3f90e1c1f9cc29bd6069.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: Demonstration of generating a defensive poisoned sample. $V\\ \\notin\\ [0,1]$ is the pixel value of trigger, $\\odot$ is the element-wise product. For the mask, 0 is represented by black, while 1 is represented by white. ", "page_idx": 3}, {"type": "text", "text": "cally, for an image with pixel values in the range of $[0,1]$ , the trigger is engineered to modify regions to values beyond this range. This modification renders the trigger infeasible and not invertible by attackers, given the natural constraints of image data. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Following Principle 4. Following the fourth principle, the defensive backdoor is required to be resistant against other backdoors in the dataset. To meet such requirements, the key point is that the defender can control the training process, a \"home filed\" advantage that attackers lack. On the one hand, the defender can design a strong defensive backdoor, e.g., adopting a large trigger. On the other hand, the defensive backdoor can be further enhanced by controlling the training process, e.g., applying data augmentation or adjusting the weight of defensive poisoned samples. More discussion and empirical findings are presented in Appendix C.7. ", "page_idx": 4}, {"type": "text", "text": "3.3 Backdoor injection ", "text_level": 1, "page_idx": 4}, {"type": "image", "img_path": "cbkJBYIkID/tmp/c860f8b4f4242e6b0ce49787865dfdd583180502b7aff106deefd6094e1ccf2b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 3: Overview of the proposed method. The trigger of the malicious backdoor is a white square, and its target label is 0. The trigger of the defensive backdoor is represented by a white shield, and the target label mapping is $h(y){\\bar{=}}\\left(y+1\\right)$ mod 10 and $h^{-1}(y)=\\dot{(y-1)}$ mod 10 . ", "page_idx": 4}, {"type": "text", "text": "As depicted in Figure 3, our proposed method involves three key steps: ", "page_idx": 4}, {"type": "text", "text": "Data preparation. Given a well-designed defensive backdoor with trigger $\\Delta_{1}$ and a target label mapping $h$ , a defensive poisoned dataset is first constructed by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{\\mathcal{D}}_{d e f}=\\{(\\pmb{x}\\oplus\\Delta_{1},h(y))|\\forall(\\pmb{x},y)\\in\\mathcal{D}_{c l}\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Model training. Now, a model can be trained on the combination of the malicious poisoned dataset $\\mathcal{D}_{t r}$ and the defensive poisoned dataset $\\hat{\\mathcal{D}}_{d e f}$ . Then, a well-trained model will normally perform for benign inputs while controlled by the corresponding backdoor when either the trigger $\\Delta$ or $\\Delta_{1}$ is presented. However, if both $\\Delta$ and $\\Delta_{1}$ are simultaneously presented, the model may become confused due to the lack of such samples in the training dataset. As aforementioned, to ensure that the defensive trigger $\\Delta_{1}$ effectively defeats an unknown trigger $\\Delta$ , some backdoor enhancement strategies such as data augmentation or increasing sample weight can be adopted to enhance the defensive backdoor. In summary, the overall training objective is formulated as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pmb{\\theta}}\\sum_{(\\pmb{x},\\pmb{y})\\in\\mathcal{D}_{t r}}L_{0}(f_{\\pmb{\\theta}}(\\pmb{x}),\\pmb{y})+\\sum_{(\\pmb{x},\\pmb{y})\\in\\hat{\\mathcal{D}}_{d e f}}\\lambda_{1}L_{1}(f_{\\pmb{\\theta}}(\\pmb{x}),\\pmb{y})+\\lambda_{2}L_{2}(f_{\\pmb{\\theta}}(\\pmb{\\tau}(\\pmb{x})),\\pmb{y}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathcal{D}_{t r}$ and $\\hat{\\mathcal{D}}_{d e f}$ are the maliciously poisoned training dataset and the defensive poisoned dataset, respectively. The operation $\\tau$ enhances the defensive backdoor by applying operation on the defensive poisoned samples (e.g., adding noise: $\\tau({\\pmb x})={\\pmb x}+{\\pmb\\epsilon}$ with $\\epsilon\\sim\\mathcal{N}(0,1))$ . ", "page_idx": 4}, {"type": "text", "text": "In (2), the first term stands for the loss on the poisoned dataset, the second term stands for the loss of injecting our defensive backdoor, and the third loss aims to enhance the defensive backdoor. We use $L_{0},L_{1}$ , and $L_{2}$ to represent the loss function for each term, which are usually Cross-Entropy losses if not specified. The parameters $\\lambda_{1}$ and $\\lambda_{2}$ are introduced to balance the contributions of the respective loss components. More details for the model training and implementation can be found in Appendix A. ", "page_idx": 5}, {"type": "text", "text": "Inference. During the inference, each input sample $\\textbf{\\em x}$ is initially embedded with the defensive trigger, and the model\u2019s prediction $f_{\\pmb{\\theta}}(\\pmb{x}\\oplus\\Delta_{1})$ is obtained. Subsequently, the authentic prediction is reconstructed via the inverse mapping $h^{-1}(f_{\\pmb\\theta}({\\pmb x}\\oplus\\Delta_{1}))$ . ", "page_idx": 5}, {"type": "text", "text": "Below, we provide a high-level pseudocode representation of our proposed method for training and inference: ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 Proactive Defensive Backdoor (PDB) ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Input: Model $f_{\\theta}$ , poisoned training set $\\mathcal{D}_{t r}$ , reserved benign dataset $\\mathcal{D}_{c l}$ , defensive trigger $\\Delta_{1}$ ,   \ndefensive target mapping $h$ , max iteration number $T$ .   \nInitialize $f_{\\theta}$ .   \n$\\triangleright$ Data preparation   \nConstruct the defensive poisoned dataset $\\hat{\\mathcal{D}}_{d e f}=\\{(\\pmb{x}\\oplus\\Delta_{1},h(y)|(\\pmb{x},y)\\in\\mathcal{D}_{c l})$ .   \n$\\triangleright$ Model training   \nfor $t=0,...,T-1$ do for each mini-batch in $\\mathcal{D}_{t r}\\cup\\hat{\\mathcal{D}}_{d e f}$ do Update $\\theta\\,w.r t.$ . objective in (2). end for   \nend for   \n$\\triangleright$ Inference   \nfor each input sample $\\textbf{\\em x}$ do Predict its label by $h^{-1}(f_{\\pmb\\theta}({\\pmb x}\\oplus\\Delta_{1}))$ .   \nend for ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Experiment setting ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Backdoor attack. To assess our method, we consider seven leading backdoor attacks: BadNets [12], Blended method [5], Sinusoidal Signal (SIG) attacks [2], Sample-Specific Backdoor Attacks (SSBA) [22], WaNet [30], BPP attack [42] and TrojanNN attack [27]. Note that to expand our evaluation scope, we have modified certain attacks originally intended for training-controllable scenarios by excluding their training control components and we postpone the details to Appendix A. For a consistent and reliable evaluation, we utilize configurations from the BackdoorBench framework [44, 46], which offers a standardized backdoor attack assessment platform. Each attack is implemented with a $5\\%$ poisoning rate, targeting the $0^{t h}$ label if not specified. The performance of these attacks is measured across three benchmark dataset, i.e., CIFAR-10 [17], Tiny ImageNet [18], and GTSRB [37], and analyzed using three neural network architectures, i.e., PreAct-ResNet18 [14] VGG19-BN [34] and ViT-B-16 [9]. Due to limitations in space, we present results for GTSRB and VGG19-BN in Appendix B. It is important to note that the clean label attack SIG is only applicable to CIFAR-10 with the set poisoning ratio. Additional information on these attacks is available in Appendix A. ", "page_idx": 5}, {"type": "text", "text": "Backdoor defense. In this paper, we benchmark our approach against popular and advanced backdoor defense methods, including AC [3], Spectral signatures [39], ABL [20], DBD [15], NAB [26]. For a fair comparison, we adopt the configurations recommended by the BackdoorBench framework [44, 46]. Note that we were unable to achieve satisfactory results for DBD on Tiny ImageNet with ViT-B-16, so it has been excluded in this case. For our method, we set the reserved dataset size to $10\\%$ of the training dataset unless otherwise specified. The chosen parameters are $\\lambda_{1}=1$ and $\\lambda_{2}=1$ . To enhance the defensive backdoor, each defensive poisoned sample is sampled five times in an epoch, and we set $\\tau({\\pmb x})={\\pmb x}+0.1\\cdot{\\pmb\\epsilon}$ with $\\mathbf{\\boldsymbol{\\epsilon}}\\sim\\mathcal{N}(\\mathbf{\\boldsymbol{0}},\\mathbf{\\boldsymbol{1}})$ . The defensive backdoor utilizes a target mapping function $h(y)=(y+1)$ mod $K$ , along with a patch trigger with pixel value 2 as illustrated in Figure 2. More details on the defense methods and supplementary experiments are postponed in Appendix A and Appendix B. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Metrics. To measure the effectiveness of each defense method, we employ three key metrics: Accuracy on benign data (ACC), Attack Success Rate (ASR), and Defense Effectiveness Rating (DER). ACC is the metric indicating model\u2019s performance for predicting the benign samples correctly, while ASR quantifies the proportion of poisoned samples that are incorrectly classified to the attacker\u2019s intended target label. For our method, the ACC is measured by predicting the benign samples with a defensive trigger to the defensive target, or equivalently reversing the prediction of the benign sample with a defensive trigger to the true label. A higher ACC and a lower ASR signify successful backdoor mitigation. ", "page_idx": 6}, {"type": "text", "text": "The DER, used in [55, 43], is a metric ranging from 0 to 1, designed to evaluate the trade-off between maintaining ACC and reducing ASR. It is defined by the following equation: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{DER}=[\\operatorname*{max}(0,\\Delta\\mathrm{ASR})-\\operatorname*{max}(0,\\Delta\\mathrm{ACC})+1]/2,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\Delta\\mathbf{A}\\mathbf{S}\\mathbf{R}$ and $\\Delta\\mathrm{ACC}$ represent the respective decreases in ASR and ACC between model without defense and model with defense. ", "page_idx": 6}, {"type": "text", "text": "Note: Superior defense methods are characterized by higher ACC, lower ASR, and higher DER. In the forthcoming experimental results, the best and second-best performing methods are denoted with boldface and underline, respectively. ", "page_idx": 6}, {"type": "text", "text": "4.2 Main results ", "text_level": 1, "page_idx": 6}, {"type": "table", "img_path": "cbkJBYIkID/tmp/0f3561adaa1b330a706759c3d1e918230f200ca8f70c0003a8c5cd5e2547563c.jpg", "table_caption": ["Table 1: Results $(\\%)$ on CIFAR-10 with PreAct-ResNet18 and poisoning ratio $5.0\\%$ . "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "cbkJBYIkID/tmp/7d238bd705699abc0be3411e47a6ea500bf969f6fbe6999999de65bf828edb40.jpg", "table_caption": ["Table 2: Results $(\\%)$ on Tiny ImageNet with ViT-B-16 and poisoning ratio $5.0\\%$ . "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Table 1 and Table 2 show the proposed method\u2019s defense performance compared with other methods, from which we can find: ", "page_idx": 6}, {"type": "text", "text": "PDB achieves consistent efficacy in mitigating backdoor threats across various attacks, datasets and models. Specifically, PDB achieves the top-2 lowest ASR across five out of seven attacks on the CIFAR-10 dataset. It also ranks top-2 across all attacks on the GTSRB (Table 7) and Tiny ImageNet. This consistent performance underscores PDB\u2019s ability to generalize well across different datasets and attacks. For AC and Spectral, both methods rely on the latent representation of images to detect poisoned samples. AC identifies poisoned samples through clustering in the latent space, considering smaller clusters as likely to contain poisoned data. Spectral detects outliers in the latent space to identify such samples. However, with a poisoning ratio of $5\\%$ for Tiny ImageNet (200 classes, each class accounts for $0.5\\%$ ), the poisoned samples become the majority within the target class, breaking the underlying assumptions of both methods and resulting in high ASR values. Additionally, while ABL, DBD, and NAB can defend against certain attacks, they fall short against more sophisticated adversaries, highlighting PDB\u2019s robust defense performance. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "PDB achieves an excellent balance between defense performance and model utility. Apart from its robust defensive performance, PDB distinguishes itself through its ability to preserve benign accuracy. Unlike ABL, DBD, and NAB, which often sacrifice considerable benign accuracy in exchange for reduced ASR, leading to lower DER values, PDB maintains a high DER by effectively managing this trade-off. The preservation of model utility, without compromising defense effectiveness, further solidifies PDB\u2019s status as a promising strategy in backdoor defense. ", "page_idx": 7}, {"type": "text", "text": "The results demonstrate the superiority of PDB in defending against backdoor attacks. By effectively reducing ASR and maintaining a high DER, PDB stands out as a valuable defense approach for backdoor attack. ", "page_idx": 7}, {"type": "text", "text": "4.3 Analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Understanding the effect of PDB. To elucidate the underlying mechanism of PDB, we delve into the impact of the defensive backdoor by analyzing the T-SNE embeddings and the Trigger Activation Change (TAC). TAC, adapted from Zheng et al. [52], is designed to measure the change of activation values for each neuron when comparing maliciously poisoned samples to their benign counterparts. Let $\\phi$ be a feature extractor which maps an input image $x$ to the latent activations. For an input image $x$ , we can construct the malicious poisoned sample $x\\oplus\\Delta$ . In PDB, a defensive trigger is added to the malicious poisoned sample, crafting sample $x\\oplus\\Delta\\oplus\\Delta_{1}$ , aiming to suppress the malicious backdoor. Therefore, for dataset $D$ , we define ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{TAC~w}/\\mathrm{o}\\,\\Delta_{1}=\\frac{\\sum_{x\\in D}\\left(\\phi\\left(x\\oplus\\Delta\\right)-\\phi\\left(x\\right)\\right)}{|D|},}\\\\ &{\\mathrm{TAC~w}/\\,\\Delta_{1}=\\frac{\\sum_{x\\in D}\\left(\\phi\\left(x\\oplus\\Delta\\oplus\\Delta_{1}\\right)-\\phi\\left(x\\right)\\right)}{|D|}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "In Figure 4, we present the visualization results for the BadNets attack on the CIFAR-10 dataset, utilizing a poisoning ratio of $5\\%$ alongside a PreAct-ResNet architecture. The illustration reveals that planting a defensive trigger to the inputs prompts a shift in the feature space, resulting in the formation of new clusters and effectively alleviating the backdoor effect. Moreover, the TAC analysis for both the initial and final blocks demonstrates that the incorporation of a defensive trigger substantially mitigates the activation changes triggered by the malicious backdoor. ", "page_idx": 7}, {"type": "image", "img_path": "cbkJBYIkID/tmp/7bb71fe2adac350d7740fefbb4e304d3418e8a55dc509e4592f52993942f9f45.jpg", "img_caption": ["Figure 4: Visualization of T-SNE and TAC for the BadNets attack on CIFAR-10 with a poisoning ratio of $5\\%$ and PreAct-ResNet. The T-SNE visualizes features in the 4th block of PreAct-ResNet18, and TAC is calculated for both the 1st and the 4th blocks (4 blocks in total). Neurons are indexed in descending order based on their TAC values without $\\Delta_{1}$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Defense effectiveness under different poisoning ratios. To investigate the influence of poisoning ratios on defense performance, we evaluate our method against attacks with poisoning ratios ranging from $1\\%$ to $40\\%$ on CIFAR-10 with PreAct-ResNet18. The results are summarized in Table 3, from which we can find that the proposed method can consistently mitigate malicious backdoor effect across a wide range of poisoning ratios. For a more comprehensive evaluation of the influence of the poisoning ratio, please refer to Appendix B. ", "page_idx": 8}, {"type": "table", "img_path": "cbkJBYIkID/tmp/67d00c0d286ed508ff0e38345a974276bfa769d40642f37b5ce7e782fd3458ea.jpg", "table_caption": ["Table 3: Defense results $(\\%)$ under different poisoning ratios on CIFAR-10 and PreAct-ResNet18. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Training cost comparison. We first analyze the training complexity of PDB and we refer readers to BackdoorBench[44] for the training complexity of other methods. Let $C_{s l}$ be the supervised training complexity. Then, we denote the size of the training dataset and the size of the defensive poisoned dataset by $N_{t r}$ and $N_{d e f}$ , respectively. Let $F$ be the frequency of sampling defensive poisoned samples. The training complexity of PDB is given by: $\\begin{array}{r}{O\\left(\\left(1+\\frac{F\\cdot N_{d e f}}{N_{t r}}\\right)\\cdot C_{s l}\\right)}\\end{array}$ ", "page_idx": 8}, {"type": "text", "text": "To evaluate the empirical runtime, i.e., training time of different defense methods, we conduct experiments against the BadNets attack for the PreAct-ResNet18 architecture on CIFAR-10 and GTSRB, ViT-B-16 for Tiny ImageNet, all with a poisoning ratio of $5\\%$ . The experiments are conducted on an RTX 4090Ti GPU, and the results are summarized in Table 4. From Table 4, We can find since F \u00b7NNtdref is set as a small value, the runtime of PDB is not much larger than the baseline (i.e., No Defense). In contrast, the runtime of DBD and NAB are significantly higher due to their reliance on self-supervised and semi-supervised training techniques. ", "page_idx": 8}, {"type": "table", "img_path": "cbkJBYIkID/tmp/3c00fdaaab221373799f56fa76cacd4ab6b07b5dadb81f1dd1949843b39e38a4.jpg", "table_caption": ["Table 4: Running time (s) comparison of defense methods. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Resistance to ALL2ALL attack. We also evaluate PDB for ALL2ALL attacks on CIFAR-10 using PreAct-ResNet18. The poisoning ratio is set to $5\\%$ and the target labels for samples with labels $y$ are $(y+2)$ mod $K$ (different from the defensive target). The experimental results are summarized in Table 5. Notably, PDB achieves the best defending performance, demonstrating superior effectiveness in defending against backdoor attacks with multiple targets. ", "page_idx": 8}, {"type": "table", "img_path": "cbkJBYIkID/tmp/cc478cc3b7a5bf7c7ce04214afdf52e1fb16899f6646c597c97d31fff3791809.jpg", "table_caption": ["Table 5: ALL2ALL attack results $(\\%)$ on CIFAR-10 with PreAct-ResNet18 and poisoning ratio $5.0\\%$ . "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Resistance to adaptive attack. In our previous experiments, we assumed that attackers had no knowledge of the defense method. However, when attackers are aware of the deployment of PDB, they may design adaptive attacks to bypass the defense. One straightforward approach is to strengthen the malicious backdoor to counteract the defensive backdoor. To assess our method\u2019s resistance to such adaptive attacks, we evaluate it against BadNets with varying trigger sizes and poisoning ratios, representing different strengths of backdoor attacks. The results, summarized in Table 6, demonstrate that PDB can consistently mitigate backdoor against adaptive attacks with various malicious trigger size and poisoning ratios. Note that to keep the stealthness of malicious backdoor, its poisoning ratio and trigger size is expected to be constrained. However, the defensive backdoor can utilize a large trigger size and high sampling frequency to meet the Principle 4, therefore, mitigating the malicious backdoor. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "table", "img_path": "cbkJBYIkID/tmp/c9390e870fecf99c3d85540003e81ab07496e9afe33c777443ffa50a3568e2f0.jpg", "table_caption": ["Table 6: Defense results $(\\%)$ against adaptive attacks with different poisoning ratios. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Appendix structure. Due to page limitations, more experiments and analyses have been moved to the Appendix. The Appendix is structured as follows: In Appendix A, we provide the details for the experiments, including the implementation of our method, the parameters, and the setting for all attacks and defense methods. In Appendix B, we provide a more comprehensive comparison between our method and baselines across different datasets, poisoning ratios, and model structures. In Appendix C, we discuss the influence of key components for PDB, such as triggers, targets, and reserved datasets, and make comparisons to more baselines. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we propose a proactive approach to defend against malicious backdoor attacks in DNNs. Rather than relying on traditional detection and mitigation pipeline, our method, PDB, leverages the \u201chome field\u201d advantage of defenders to inject a defensive backdoor to fight against malicious backdoor. To achieve such a goal, we introduce four essential design properties for an effective defensive backdoor: reversibility, inaccessibility to attackers, minimal impact on model performance, and resistance to other backdoors. By incorporating a defensive backdoor during training, we suppress the impact of malicious backdoors when the defensive trigger is present. Our approach offers several advantages over existing methods. First, it does not rely on accurate detection of poisoned samples and any assumption for attacks, avoiding performance degradation when some poisoned samples evade detection. Second, PDB does not require complex modifications to the training process, minimizing training cost. In summary, PDB provides a novel and effective defense method against backdoor attacks, enhancing the safety and reliability of DNNs. ", "page_idx": 9}, {"type": "text", "text": "Limitations and future work. Currently, PDB faces several key limitations. First, its reliance on clean samples presents a practical challenge, prompting the exploration of alternative sources, such as generated data. Second, investigating PDB across diverse machine learning tasks is essential for broader applicability. Addressing these limitations through future research will enhance the defense\u2019s effectiveness and facilitate its widespread adoption in safeguarding machine learning systems against backdoor attacks. ", "page_idx": 9}, {"type": "text", "text": "Broader impacts. The broader impacts can be considered from both positive and negative perspectives. On the positive side, PDB enhances the security and reliability of DNNs, thereby contributing to the trustworthiness of AI technologies. However, there are potential negative implications that should be considered. The technique could potentially be misused if it falls into the wrong hands, who might use the defensive backdoor mechanism for nefarious purposes. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Baoyuan Wu is supported by Guangdong Basic and Applied Basic Research Foundation (No. 2024B1515020095), National Natural Science Foundation of China (No. 62076213), Shenzhen Science and Technology Program under grants (No. RCYX20210609103057050), and Longgang District Key Laboratory of Intelligent Digital Economy Security. Hongyuan Zha is supported in part by the Shenzhen Key Lab of Crowd Intelligence Empowered Low-Carbon Energy Network (No. ZDSYS20220606100601002). This work is supported by Shenzhen Science and Technology Program under grant No. GXWD20201231105722002-20200901175001001, and No. ZDSYS20211021111415025, and No. JCYJ20210324120011032, and the Guangdong Provincial Key Laboratory of Big Data Computing, the Chinese University of Hong Kong, Shenzhen. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "[1] Insaf Adjabi, Abdeldjalil Ouahabi, Amir Benzaoui, and Abdelmalik Taleb-Ahmed. Past, present, and future of face recognition: A review. Electronics, page 1188, 2020. 1   \n[2] Mauro Barni, Kassem Kallas, and Benedetta Tondi. A new backdoor attack in cnns by training set corruption without label poisoning. In International Conference on Image Processing, 2019. 2, 6, 7, 9, 16, 18, 19, 21, 22, 23   \n[3] Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin Edwards, Taesung Lee, Ian Molloy, and Biplav Srivastava. Detecting backdoor attacks on deep neural networks by activation clustering. In Workshop on Artificial Intelligence Safety, 2019. 1, 2, 3, 6, 7, 9, 16, 17, 18, 19, 23   \n[4] Weixin Chen, Baoyuan Wu, and Haoqian Wang. Effective backdoor defense by exploiting sensitivity of poisoned samples. In Conference on Neural Information Processing Systems, 2022. 1, 3   \n[5] Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep learning systems using data poisoning. arXiv e-prints, pages arXiv\u20131712, 2017. 2, 6, 7, 9, 16, 17, 18, 19, 20, 21, 22, 23   \n[6] Ziyi Cheng, Baoyuan Wu, Zhenya Zhang, and Jianjun Zhao. Tat: Targeted backdoor attacks against visual object tracking. Pattern Recognition, 2023. 2   \n[7] Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, and Mubarak Shah. Diffusion models in vision: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. 4   \n[8] Khoa Doan, Yingjie Lao, Weijie Zhao, and Ping Li. Lira: Learnable, imperceptible and robust backdoor attacks. In International Conference on Computer Vision, 2021. 2   \n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2020. 6   \n[10] Kuofeng Gao, Jiawang Bai, Baoyuan Wu, Mengxi Ya, and Shu-Tao Xia. Imperceptible and robust backdoor attack in 3d point cloud. IEEE Transactions on Information Forensics and Security, 19:1267\u20131282, 2023. 2   \n[11] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 2020. 4   \n[12] Tianyu Gu, Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Evaluating backdooring attacks on deep neural networks. IEEE Access, pages 47230\u201347244, 2019. 2, 6, 7, 9, 16, 17, 18, 19, 20, 21, 22, 23   \n[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Conference on Computer Vision and Pattern Recognition, 2016. 1   \n[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In European Conference on Computer Vision, 2016. 6   \n[15] Kunzhe Huang, Yiming Li, Baoyuan Wu, Zhan Qin, and Kui Ren. Backdoor defense via decoupling the training process. In International Conference on Learning Representations, 2022. 1, 2, 3, 6, 7, 9, 16, 17, 18, 19, 23   \n[16] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv e-prints, 2013. 4   \n[17] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. 6   \n[18] Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 2015. 6   \n[19] Songze Li and Yanbo Dai. Backdoorindicator: Leveraging ood data for proactive backdoor detection in federated learning. arXiv e-prints, 2024. 25, 26   \n[20] Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, and Xingjun Ma. Anti-backdoor learning: Training clean models on poisoned data. In Conference on Neural Information Processing Systems, 2021. 1, 2, 3, 6, 7, 9, 16, 17, 18, 19, 23   \n[21] Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, and Xingjun Ma. Neural attention distillation: Erasing backdoor triggers from deep neural networks. In International Conference on Learning Representations, 2021. 3, 17, 22   \n[22] Yuezun Li, Yiming Li, Baoyuan Wu, Longkang Li, Ran He, and Siwei Lyu. Invisible backdoor attack with sample-specific triggers. In International Conference on Computer Vision, 2021. 2, 6, 7, 16, 17, 18, 19, 20, 21, 22   \n[23] Weilin Lin, Li Liu, Shaokui Wei, Jianze Li, and Hui Xiong. Unveiling and mitigating backdoor vulnerabilities based on unlearning weight changes and backdoor activeness. arXiv e-prints, 2024. 3   \n[24] Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Fine-pruning: Defending against backdooring attacks on deep neural networks. In Research in Attacks, Intrusions, and Defenses, 2018. 3, 17, 22   \n[25] Liangkai Liu, Sidi Lu, Ren Zhong, Baofu Wu, Yongtao Yao, Qingyang Zhang, and Weisong Shi. Computing systems for autonomous driving: State of the art and challenges. IEEE Internet of Things Journal, pages 6469\u20136486, 2020. 1   \n[26] Min Liu, Alberto Sangiovanni-Vincentelli, and Xiangyu Yue. Beating backdoor attack at its own game. In International Conference on Computer Vision, 2023. 1, 2, 3, 6, 7, 9, 16, 17, 18, 19, 23   \n[27] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, and Xiangyu Zhang. Trojaning attack on neural networks. In Network and Distributed System Security Symposium, 2018. 6, 7, 16, 17, 18, 19, 20, 22   \n[28] Rui Min, Zeyu Qin, Li Shen, and Minhao Cheng. Towards stable backdoor purification through feature shift tuning. In Advances in Neural Information Processing Systems, 2023. 3   \n[29] Preetum Nakkiran, Behnam Neyshabur, and Hanie Sedghi. The deep bootstrap framework: Good online learners are good offline generalizers. In International Conference on Learning Representations, 2021. 21   \n[30] Tuan Anh Nguyen and Anh Tuan Tran. Wanet - imperceptible warping-based backdoor attack. In International Conference on Learning Representations, 2021. 2, 6, 7, 16, 17, 18, 19, 20, 22   \n[31] Xiangyu Qi, Tinghao Xie, Yiming Li, Saeed Mahloujifar, and Prateek Mittal. Revisiting the assumption of latent separability for backdoor defenses. In International Conference on Learning Representations, 2023. 2   \n[32] Xiangyu Qi, Tinghao Xie, Jiachen T Wang, Tong Wu, Saeed Mahloujifar, and Prateek Mittal. Towards a proactive $\\{\\mathrm{ML}\\}$ approach for detecting backdoor poison samples. In USENIX Security Symposium, 2023. 3   \n[33] Ali Shafahi, W Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras, and Tom Goldstein. Poison frogs! targeted clean-label poisoning attacks on neural networks. In Conference on Neural Information Processing Systems, 2018. 2   \n[34] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In International Conference on Learning Representations, 2015. 6   \n[35] Zhengyao Song, Yongqiang Li, Danni Yuan, Li Liu, Shaokui Wei, and Baoyuan Wu. Wpda: Frequency-based backdoor attack with wavelet packet decomposition. arXiv e-prints, 2024. 2   \n[36] Hossein Souri, Liam Fowl, Rama Chellappa, Micah Goldblum, and Tom Goldstein. Sleeper agent: Scalable hidden trigger backdoors for neural networks trained from scratch. In Conference on Neural Information Processing Systems, 2022. 2   \n[37] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. The german traffic sign recognition benchmark: a multi-class classification competition. In International Joint Conference on Neural Networks, 2011. 6   \n[38] J-Donald Tournier, Robert Smith, David Raffelt, Rami Tabbara, Thijs Dhollander, Maximilian Pietsch, Daan Christiaens, Ben Jeurissen, Chun-Hung Yeh, and Alan Connelly. Mrtrix3: A fast, flexible and open software framework for medical image processing and visualisation. Neuroimage, page 116137, 2019. 1   \n[39] Brandon Tran, Jerry Li, and Aleksander Madry. Spectral signatures in backdoor attacks. In Advances in Neural Information Processing Systems, 2018. 6, 7, 9, 16, 17, 18, 19, 23   \n[40] Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y Zhao. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. In Symposium on Security and Privacy, 2019. 17, 22   \n[41] Ruotong Wang, Hongrui Chen, Zihao Zhu, Li Liu, Yong Zhang, Yanbo Fan, and Baoyuan Wu. Robust backdoor attack with visible, semantic, sample-specific, and compatible triggers. arXiv e-prints, 2023. 2   \n[42] Zhenting Wang, Juan Zhai, and Shiqing Ma. Bppattack: Stealthy and efficient trojan attacks against deep neural networks via image quantization and contrastive adversarial learning. In Conference on Computer Vision and Pattern Recognition, 2022. 6, 7, 9, 16, 17, 18, 19, 20, 22, 23   \n[43] Shaokui Wei, Mingda Zhang, Hongyuan Zha, and Baoyuan Wu. Shared adversarial unlearning: Backdoor mitigation by unlearning shared adversarial examples. In Advances in Neural Information Processing Systems, 2023. 3, 7   \n[44] Baoyuan Wu, Hongrui Chen, Mingda Zhang, Zihao Zhu, Shaokui Wei, Danni Yuan, and Chao Shen. Backdoorbench: A comprehensive benchmark of backdoor learning. In Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. 1, 6, 9, 16   \n[45] Baoyuan Wu, Shaokui Wei, Mingli Zhu, Meixi Zheng, Zihao Zhu, Mingda Zhang, Hongrui Chen, Danni Yuan, Li Liu, and Qingshan Liu. Defenses in adversarial machine learning: A survey. arXiv e-prints, 2023. 1, 3, 4   \n[46] Baoyuan Wu, Hongrui Chen, Mingda Zhang, Zihao Zhu, Shaokui Wei, Danni Yuan, Mingli Zhu, Ruotong Wang, Li Liu, and Chao Shen. Backdoorbench: A comprehensive benchmark and analysis of backdoor learning. arXiv e-prints, 2024. 1, 6   \n[47] Dongxian Wu and Yisen Wang. Adversarial neuron pruning purifies backdoored deep models. In Conference on Neural Information Processing Systems, 2021. 3   \n[48] Danni Yuan, Shaokui Wei, Mingda Zhang, Li Liu, and Baoyuan Wu. Activation gradient based poisoned sample detection against backdoor attacks. arXiv e-prints, 2023. 1   \n[49] Yi Zeng, Won Park, Z. Morley Mao, and Ruoxi Jia. Rethinking the backdoor attacks\u2019 triggers: A frequency perspective. In International Conference on Computer Vision, 2021. 2   \n[50] Yi Zeng, Si Chen, Won Park, Zhuoqing Mao, Ming Jin, and Ruoxi Jia. Adversarial unlearning of backdoors via implicit hypergradient. In International Conference on Learning Representations, 2022. 3, 17, 22   \n[51] Zaixi Zhang, Qi Liu, Zhicai Wang, Zepu Lu, and Qingyong Hu. Backdoor defense via deconfounded representation learning. In Conference on Computer Vision and Pattern Recognition, 2023. 2, 3   \n[52] Runkai Zheng, Rongjun Tang, Jianze Li, and Li Liu. Data-free backdoor removal based on channel lipschitzness. In European Conference on Computer Vision, 2022. 3, 8   \n[53] Runkai Zheng, Rongjun Tang, Jianze Li, and Li Liu. Pre-activation distributions expose backdoor neurons. In Conference on Neural Information Processing Systems, 2022. 3   \n[54] Hong Zhu, Shengzhi Zhang, and Kai Chen. Ai-guardian: Defeating adversarial attacks using backdoors. In Symposium on Security and Privacy, pages 701\u2013718. IEEE, 2023. 3   \n[55] Mingli Zhu, Shaokui Wei, Li Shen, Yanbo Fan, and Baoyuan Wu. Enhancing fine-tuning based backdoor defense with sharpness-aware minimization. In International Conference on Computer Vision, 2023. 3, 7, 26   \n[56] Mingli Zhu, Shaokui Wei, Hongyuan Zha, and Baoyuan Wu. Neural polarizer: A lightweight and effective backdoor defense via purifying poisoned features. In Advances in Neural Information Processing Systems, 2023. 3   \n[57] Mingli Zhu, Siyuan Liang, and Baoyuan Wu. Breaking the false sense of security in backdoor defense through re-activation attack. In Conference on Neural Information Processing Systems, 2024. 2   \n[58] Zihao Zhu, Mingda Zhang, Shaokui Wei, Li Shen, Yanbo Fan Fan, and Baoyuan Wu. Boosting backdoor attack with a learnable poisoning sample selection strategy. arXiv e-prints, 2023. 2   \n[59] Zihao Zhu, Mingda Zhang, Shaokui Wei, Bingzhe Wu, and Baoyuan Wu. Vdc: Versatile data cleanser for detecting dirty samples via visual-linguistic inconsistency. In International Conference on Learning Representations, 2024. 3   \n[60] Zixuan Zhu, Rui Wang, Cong Zou, and Lihua Jing. The victim and the beneficiary: Exploiting a poisoned model to train a clean model on poisoned data. In International Conference on Computer Vision, 2023. 1, 2, 3 ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Experiment details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In our experiments, we adapted all baselines and settings from BackdoorBench [44]. Moreover, all checkpoints of attack methods are sourced from BackdoorBench and the defense results are aligned with the leaderboard in BackdoorBench if applicable. Below, we outline the details of various backdoor attacks: ", "page_idx": 15}, {"type": "text", "text": "A.1 Attack details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "\u2022 BadNets [12] is one of the earliest works for backdoor learning, which inserts a small patch of fixed pattern to replace some pixels in the image. We use the default setting in BackdoorBench.   \n\u2022 Blended backdoor attack (Blended) [5] uses an alpha-blending strategy to fuse images with fixed patterns. We set $\\alpha=0.2$ as the default in BackdoorBench. Note that a large $\\alpha$ causes visual-perceptible changes to clean samples, making the Blended Attack challenging for defense methods.   \n\u2022 Sinusoidal signal backdoor attack (SIG) [2] is a clean-label attack that perturbs clean images in the target label using a sinusoidal signal as the trigger. We use the default setting in BackdoorBench.   \n\u2022 Sample-specific backdoor attack (SSBA) [22] uses an auto-encoder to fuse a trigger into clean samples and generate poisoned samples. We use the default setting in BackdoorBench.   \n\u2022 Warping-based poisoned networks (WaNet) [30] is also a training-controllable attack that perturbs clean samples using a warping function to construct poisoned samples. We use the default setting in BackdoorBench.   \n\u2022 Bppattack (BPP) [42] is also a training-controllable attack that employs image quantization and dithering as the Trojan trigger. We use the default setting in BackdoorBench.   \n\u2022 Trojaning attack on neural networks (TrojanNN) [27] inverses the neural network to generate a general trojan trigger. We use the default setting in BackdoorBench. ", "page_idx": 15}, {"type": "text", "text": "Adaptation to data poisoning attack. In our paper, we explore scenarios where attacks can only utilize data poisoning techniques. To facilitate a more comprehensive comparison of our method, we modify attacks originally designed for training-controllable scenarios, removing the training component to adapt them to a data poisoning setting. ", "page_idx": 15}, {"type": "text", "text": "A.2 Defense details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Here, we summarize the details of each defense method used: ", "page_idx": 15}, {"type": "text", "text": "\u2022 AC [3] is a detection method that detects the poisoned sample using the abnormal clustering for poisoned samples. By removing the detected samples, AC can effectively defend against backdoor attack. We use the default setting in BackdoorBench.   \n\u2022 Spectral [39] is a detection method that detects the poisoned sample using the abnormal Spectral Signature for poisoned samples. By removing the detected samples, AC can effectively defend against backdoor attack. We use the default setting in BackdoorBench.   \n\u2022 ABL [20] utilizes the early-learning phenomenon of poisoned samples to detect poisoned samples and then unlearns them to mitigate the backdoor effect. We use the default setting in BackdoorBench.   \n\u2022 DBD [15] divides the training process into three stages and uses self-supervised techniques to detect the poisoned sample and learn a clean model. We use the default setting in BackdoorBench.   \n\u2022 NAB [26] first employs an advanced detection method to fliter the poisoned samples. Then, the detected samples are relabeled by employing other techniques and planted with nonadversarial triggers to suppress the backdoor. In this work, we use the detection method from ABL and the self-supervised method from DBD to relabel the samples. For other settings, We use the default setting in BackdoorBench. ", "page_idx": 15}, {"type": "text", "text": "\u2022 FT finetunes the model on a small, clean, reserved dataset to mitigate the backdoor effect. We use the default setting in BackdoorBench. ", "page_idx": 16}, {"type": "text", "text": "\u2022 FP [24] is a pruning-based method that prunes neurons according to their activations and then fine-tunes the model to keep clean accuracy. We use the default setting in BackdoorBench.   \n\u2022 NC [40] first optimizes a possible trigger to detect backdoored models. If detected as backdoored, unlearn the optimized trigger. If detected as clean, the model is returned unchanged.   \n\u2022 NAD [21] uses Attention Distillation to mitigate backdoors. We use the default setting in BackdoorBench.   \n\u2022 i-BAU [50] uses adversarial training with UAP and hyper-gradient to mitigate the backdoor. We use the default setting in BackdoorBench.   \n\u2022 PDB (Ours) defends backdoor attack by injecting defensive backdoor. We set the reserved dataset size to $10\\%$ of the training dataset. The chosen parameters are $\\lambda_{1}=1$ and $\\lambda_{2}=1$ . To enhance the defensive backdoor, each defensive poisoned sample is sampled five times in an epoch, and we set $\\tau({\\pmb x})={\\pmb x}+0.1\\cdot{\\pmb\\epsilon}$ with $\\pmb{\\epsilon}\\sim\\bar{\\mathcal{N}}(0,1)$ . The defensive backdoor utilizes a target mapping function $h(y)=(y+1)$ mod $K$ , along with a $7\\times7$ patch trigger with pixel value 2 as illustrated in Figure 2. ", "page_idx": 16}, {"type": "text", "text": "Adaptation to ViT-B-16. For all experiments on CIFAR-10 and GTSRB, we train the model 100 epochs with batch size 256 for fair comparison. For Tiny ImageNet with ViT-B-16, we consider a fine-tuning task as recommended by BackdoorBench. Specifically, we train each model 10 epochs with batch size 128 and initialize the model with pre-trained weights. ", "page_idx": 16}, {"type": "text", "text": "B Additional experiment results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "This section provides additional experiment results to supplement the observations claimed in Section 4. ", "page_idx": 16}, {"type": "text", "text": "B.1 Main experiments on GTSRB with PreAct-ResNet18 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Table 7, 8, and 9 summarize the results of various defense methods against backdoor attacks on the GTSRB dataset using the PreAct-ResNet18 model architecture. These methods were evaluated at different poisoning ratios ( ${[1.0\\%}$ , $5.0\\%$ , and $10.0\\%$ ). Notably, the results demonstrate that PDB effectively mitigates backdoor attacks, consistently achieving top-2 defense performance across all cases. ", "page_idx": 16}, {"type": "table", "img_path": "cbkJBYIkID/tmp/f4ba9adbc125285d0c3eea5529a6be6ca6ba6f63a84766ba706bf224fc44eb4c.jpg", "table_caption": ["Table 7: Results on GTSRB with PreAct-ResNet18 and poisoning ratio $5.0\\%$ . "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "cbkJBYIkID/tmp/652b806a0512c251265c7ebf750c8e53a959b66fc7a88a4dfca3beafac83de28.jpg", "table_caption": ["Table 8: Results on GTSRB with PreAct-ResNet18 and poisoning ratio $10.0\\%$ . "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "cbkJBYIkID/tmp/cabcf1a1a6a92c6831330882908bdc74c7fc3a6fd7d40847612d66f318bf35a2.jpg", "table_caption": ["Table 9: Results on GTSRB with PreAct-ResNet18 and poisoning ratio $1.0\\%$ . "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "B.2 Main experiments on CIFAR-10 with PreAct-ResNet18 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Table 10 and 11 summarize the results of different defense methods against backdoor attacks on the CIFAR-10 dataset using the PreAct-ResNet18 model architecture. These methods were evaluated at different poisoning ratios $1.0\\%$ and $10.0\\%$ ). Notably, they achieved the top-2 lowest ASR in 12 out of 14 cases. ", "page_idx": 17}, {"type": "table", "img_path": "cbkJBYIkID/tmp/4531d39a61358887d6fec040b91fc0fdc5708c5fc6f616907713f39a22438c63.jpg", "table_caption": ["Table 10: Results on CIFAR-10 with PreAct-ResNet18 and poisoning ratio $10.0\\%$ "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "cbkJBYIkID/tmp/81eda62e9894353db96a47442475b4ade5a3da1d1e9c214903d7b8f32bf9a901.jpg", "table_caption": ["Table 11: Results on CIFAR-10 with PreAct-ResNet18 and poisoning ratio $1.0\\%$ . "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "B.3 Main experiments on CIFAR-10 with VGG19-BN ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Table 12, 13, and 14 summarize the results of different defense methods against backdoor attacks on the CIFAR-10 dataset using the VGG19-BN model architecture. These methods were evaluated at different poisoning ratios ( $1.0\\%$ , $5\\%$ , and $10.0\\%$ ). Impressively, PDB achieves the top-2 lowest ASR in 20 out of 21 cases. ", "page_idx": 18}, {"type": "table", "img_path": "cbkJBYIkID/tmp/341e891910f680b71b7641835765f936dc265e9b596078468457a948fbd26054.jpg", "table_caption": ["Table 12: Results on CIFAR-10 with VGG19-BN and poisoning ratio $5.0\\%$ . "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "cbkJBYIkID/tmp/fffedce43af0f7766bf6a439d5bced50bc3b5bf7c93d5a9bf3e00b7ceb4b0316.jpg", "table_caption": ["Table 13: Results on CIFAR-10 with VGG19-BN and poisoning ratio $1.0\\%$ . "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "cbkJBYIkID/tmp/586d12312a098f4e7f032b38b474adb80bd1e2a7e714a4a5c43ac6eb033e5c82.jpg", "table_caption": ["Table 14: Results on CIFAR-10 with VGG19-BN and poisoning ratio $10.0\\%$ . "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "B.4 Main experiments on Tiny ImageNet with ViT-B-16 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To demonstrate the effectiveness and scalability of PDB, we evaluate our proposed method against backdoor attacks on the Tiny ImageNet dataset using the ViT-B-6 model architecture. We consider different poisoning ratios ${[1.0\\%}$ , $5\\%$ , and $10.0\\%$ ). The results, presented in Table 15, highlight our method\u2019s ability to effectively mitigate backdoor attacks for a large dataset and a large model. ", "page_idx": 18}, {"type": "table", "img_path": "cbkJBYIkID/tmp/ad5ae5cf598e69a305425b4484cf4faeb716cbc217cf3114e65420298adb85ba.jpg", "table_caption": ["Table 15: Results on Tiny ImageNet with ViT-B-16. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "B.5 Experiments on invisible backdoor attack and low-poisoning ratio attack ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "As aforementioned, the proposed method, PDB, does not rely on specific assumptions about the type of attack, making it effective for defending against both invisible backdoor attacks and attacks with low poisoning ratios. To demonstrate this effectiveness, we conducted experiments using low poisoning ratios ( $0.5\\%$ and $0.1\\%$ ) for both Visible and Invisible attacks. The results are summarized in Table 16, from which we can find that PDB can consistently mitigate backdoor attacks. ", "page_idx": 19}, {"type": "table", "img_path": "cbkJBYIkID/tmp/bca1d7554e201087762bbbb54d87f9fda592bf91471778edbc80a81756eab501.jpg", "table_caption": ["Table 16: Results on PreAct-ResNet18 and CIFAR10 for invisible and low poisoning ratio attacks "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "C Discussion and additional analysis ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "C.1 Influences of reserved dataset ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We explore the impact of the reserved dataset on defense performance, considering both dataset size and source: ", "page_idx": 19}, {"type": "text", "text": "\u2022 Dataset size. We investigate the influence of reserved dataset size using the CIFAR-10 dataset, a $5\\%$ poisoning ratio, and the PreAct-ResNet architecture. Figure 5 summarizes the results, demonstrating that our proposed method effectively mitigates malicious backdoor effects across a wide range of reserved dataset sizes. Notably, increasing the reserved dataset significantly improves the accuracy of our method. ", "page_idx": 19}, {"type": "image", "img_path": "cbkJBYIkID/tmp/a6d01f39f20427141b06c367e59ca4d0b48231fd790117d5d76ecec1d6db9fb5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 5: Defense results with different sizes of the reserved dataset. The ratio represents the ratio of the reserved dataset compared with the whole training dataset. Note that the Defense ASR is below $1\\%$ and may be barely visible. ", "page_idx": 19}, {"type": "text", "text": "\u2022 Defense with generated dataset. As generative models have evolved, incorporating the generated dataset as an additional source for backdoor defense has become practical and reasonable. To explore the source of this reserved dataset, we assess our method using the generated data CIFAR$5\\mathrm{m}$ from Nakkiran et al. [29]. with DDPM. Our findings, summarized in Table 17, demonstrate that the advanced generative model can indeed supply a sufficient dataset for applying our defense strategy. ", "page_idx": 20}, {"type": "table", "img_path": "cbkJBYIkID/tmp/f51b29f2c44235ec584e3ef543f5771a5b5726d8af7b42eb734b390848839ffa.jpg", "table_caption": ["Table 17: Defense results using generated dataset on CIFAR-10 and PreAct-ResNet18 $(\\%)$ . "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "C.2 Influences of the trigger and target ", "text_level": 1, "page_idx": 20}, {"type": "image", "img_path": "cbkJBYIkID/tmp/6f63430ba388950057c9309a889739a12ef7a2dbc18d6ca9d59a1f78e6c690b0.jpg", "img_caption": ["Figure 6: The masks of patched triggers. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "In this section, we explore the impact of trigger design and target assignment strategy. Specifically, we evaluate different trigger configurations using a patch trigger with three distinct masks (illustrated in Figure 6). Additionally, we consider two target assignment strategies, i.e., $h_{1}(y)\\,=\\,(y+1)$ mod $K$ and $h_{2}(\\phi(\\pmb{x}))\\,=\\,-\\phi(\\pmb{x})$ , where $y$ is the hard label and $\\phi({\\pmb x})$ is the logits for the model output. Our experiments, detailed in Table 18, demonstrate the effectiveness of our method across various defensive backdoor designs. ", "page_idx": 20}, {"type": "table", "img_path": "cbkJBYIkID/tmp/86f30aa4296c344123496d4183eb921aa2fb8e82944d2f82667ac0e331f9f73c.jpg", "table_caption": ["Table 18: Results on PDB with different configurations. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Special case study: Same target label for malicious backdoor and defensive backdoor. While our trigger remains inaccessible to the attacker, the target assignment strategy could potentially be stolen or coincidentally used by the attacker. In this scenario, we conduct experiments where both the attacker and the defender employ the same target assignment strategy but different triggers. Our results demonstrate that our method remains effective in such cases. Specifically, assuming both the attacker and defender choose $h(y)=(y+1)$ mod $K$ , we summarize the results in Table 19, highlighting our method\u2019s resilience even when the attacker uses the same target label as the defender. ", "page_idx": 20}, {"type": "text", "text": "Table 19: Results on attacks with the same target label as defensive backdoor. ", "page_idx": 21}, {"type": "table", "img_path": "cbkJBYIkID/tmp/381ebd91abe95eadc3fc4cc0f1687edf0e194c8b66da1907da9d9d530fcaf70d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "C.3 Influences of augmentation ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we explore the impact of augmentation on enhancing defensive backdoors using the CIFAR-10 dataset, a $5\\%$ poisoning ratio, and the PreAct-ResNet architecture. We specifically investigate Gaussian noise augmentation by setting $\\tau(\\pmb{x})=\\pmb{x}+\\alpha*\\pmb{\\epsilon}$ with $\\mathbf{\\boldsymbol{\\epsilon}}\\sim\\mathcal{N}(\\mathbf{\\boldsymbol{0}},\\mathbf{\\boldsymbol{1}})$ determines the augmentation intensity. The results are summarized in Figure 7. Notably, even without any augmentation $(\\alpha=0)$ ), PDB exhibits significant efficacy, likely due to the robustness of the defense mechanisms and our controlled training injection process. Furthermore, as augmentation strength increases, the ASR decreases, albeit at the cost of reduced ACC, indicating a tradeoff between augmentation intensity and model performance. ", "page_idx": 21}, {"type": "image", "img_path": "cbkJBYIkID/tmp/a5f5470bbfc62c51aa073a070d4b9ff5914b5883d13a47eb7e4b628c8463c9b7.jpg", "img_caption": ["Figure 7: Defense results with different strength of augmentation. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "C.4 Comparison with post-training methods ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Given a reserved dataset, the defender may follow a \u2018poisoned first and mitigation later\u2019 manner to train the backdoored model first and employ post-training method to mitigate the backdoor effect. Therefore, to thoroughly evaluate our method, we compare it with SOTA post-training approaches that aim to remove backdoor effects after model training. To ensure fairness, we adopt the common practice of reserving $5\\%$ of the training data for post-training evaluation. Our experiments are conducted on the CIFAR-10 dataset using PreAct-ResNet18 with a $5\\%$ poisoning ratio. The summarized results in Table 20 demonstrate that our method consistently achieves superior performance in defending against backdoor attacks, highlighting its promising effectiveness. ", "page_idx": 21}, {"type": "table", "img_path": "cbkJBYIkID/tmp/fc03e543b89d17d7c6864122963808a713aa5666fc2f7db469d72f743a346ae5.jpg", "table_caption": ["Table 20: Results on CIFAR-10 with PreAct-ResNet18 and poisoning ratio $5.0\\%$ . "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "C.5 Detailed comparison to NAB ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "As previously mentioned, the work most relevant to our research is NAB [26], which presents a promising and impressive approach for enhancing backdoor defense methods. In this context, we conduct a detailed analysis to highlight the distinctions between our method, PDB, and NAB. ", "page_idx": 22}, {"type": "text", "text": "PDB does not rely on poisoned sample detection. Our method diverges from the conventional \u201cdetection-mitigation\u201d pipeline by operating independently of any poison detection methods. In contrast, NAB relies on identifying poisoned samples to deploy its defensive trigger. ", "page_idx": 22}, {"type": "text", "text": "PDB does not depend on suspicious sample relabeling. Unlike NAB, our method does not require accurate sample relabeling. In NAB, detected samples are equipped with a defensive trigger and subsequently relabeled. Unfortunately, if a few clean samples are mistakenly labeled as \u201csuspicious,\u201d the defensive trigger may function as a malicious one if further relabeling is incorrect. Note that accurately relabeling a sample is getting harder as the dataset gets larger. Therefore, we observe such scenarios appear more frequently in large datasets during our experiments. For instance, when applying the BadNets attack with a $5\\%$ poisoning ratio on Tiny ImageNet using ViT-B-16, NAB\u2019s model accuracy drops from $73.60\\%$ (without a defensive trigger) to $28.88\\%$ (with the trigger) due to low relabel correctness and poison detection rates. ", "page_idx": 22}, {"type": "text", "text": "C.6 Resistance to ALL2ALL attack ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we compare PDB with other baselines in ALL2ALL attacks on CIFAR-10 using PreAct-ResNet18. The poisoning ratio is set to $5\\%$ and $10\\%$ . Specifically, the target labels for samples with original labels $y$ are adjusted to $(y+2)$ mod $K$ (different from the defensive target). The experimental results are summarized in Table 21 and Table 22. Notably, PDB achieves the best defending performance, demonstrating superior effectiveness in countering backdoor attacks with multiple targets. ", "page_idx": 22}, {"type": "table", "img_path": "cbkJBYIkID/tmp/8f77ff10f380d0b5d8395f738c85b5c7b83ef35db596f3dba52be2c3ef2feda7.jpg", "table_caption": ["Table 21: ALL2ALL attack results on CIFAR-10 with PreAct-ResNet18 and poisoning ratio $5.0\\%$ . "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "cbkJBYIkID/tmp/4de6905616a2aec13162886bf2ca8c53773da1a0a82c350a6fbd3705c61bce26.jpg", "table_caption": ["Table 22: ALL2ALL attack results on CIFAR-10 with PreAct-ResNet18 and poisoning ratio $10.0\\%$ . "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "C.7 Design of defensive trigger and the satisfaction to Principle 4 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Here, we summarize the aforementioned experiments and provide a comprehensive discussion about how to meet Principle 4 (Resistance against other backdoors) from the following perspectives: ", "page_idx": 22}, {"type": "text", "text": "\u2022 Design of defensive trigger: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Defensive trigger size: Trigger size directly contributes to the strength of the defensive backdoor. In Table 23, we evaluate PDB with a square defensive trigger with sizes ranging from 1x1 to $9\\mathrm{x9}$ . From Table 23, we can find that a larger trigger leads to a stronger defensive backdoor, resulting in a higher ACC and a lower ASR. However, as the trigger size increases, it may interfere with the visual content of the image, leading to a slight decrease in ACC. Notably, as the square trigger has strong visibility, a trigger with size 1x1 can still alleviate the malicious backdoor to some extent. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "table", "img_path": "cbkJBYIkID/tmp/4a9f77cf201c261cfecea2c23a14b61641ce84bf3f7724f754b8f5c622c2390c.jpg", "table_caption": ["Table 23: Results on PreAct-ResNet18 with Poisoning Ratio $5\\%$ and different defensive trigger size "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "2. Defensive trigger position: As aforementioned, the position of the defensive trigger is essential for Principle 3, i.e., minimal impact on model performance. Table 24 shows that triggers placed in different positions (corner, random, and center) achieve a similar effect in defending against backdoor attacks. However, placing a trigger at the center of an image significantly degrades accuracy, as the trigger masks the core patterns of the image. ", "page_idx": 23}, {"type": "table", "img_path": "cbkJBYIkID/tmp/e4853af24193267e0a0dc35a3fb0419f84f7802b77feeb593fb9cdbde720f286.jpg", "table_caption": ["Table 24: Results on PreAct-ResNet18 with Poisoning Ratio $5\\%$ and different positions "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "3. Pixel value: For a square trigger, pixel value is also an important parameter for PDB. In Table 25, we evaluate PDB with a square defensive trigger with different pixel values, from which we can find that PDB can achieve high effectiveness across different pixel values. ", "page_idx": 23}, {"type": "text", "text": "Table 25: Results on PreAct-ResNet18 with Poisoning Ratio $5\\%$ and different pixel values ", "page_idx": 23}, {"type": "table", "img_path": "cbkJBYIkID/tmp/7215c825255f38ea2ed89f3ff785c66e5ebc6135119ad82f6abb56532206e834.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "\u2022 Backdoor enhancement strategy during training: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Increasing sampling frequency: Given a fixed number of defensive poisoned samples, the defensive backdoor can be further enhanced by increasing the sampling frequency of poisoned samples, forcing the model to pay more attention to defensive poisoned samples. Table 26 shows that a larger sampling frequency leads to a stronger defensive backdoor, resulting in a higher ACC and a lower ASR. Note that for the malicious attacker, the poisoning ratio is expected to be low to ensure the stealthiness of the attack. ", "page_idx": 23}, {"type": "text", "text": "2. Data augmentation: From Fig 7, we can find that PDB, without any sample augmentation $(\\alpha\\,=\\,0)$ ), exhibits significant efficacy with ASR lower than $2\\%$ . As augmentation ", "page_idx": 23}, {"type": "text", "text": "Table 26: Results on PreAct-ResNet18 with poisoning ratio $5\\%$ and different sampling frequencies strength increases, the ASR decreases, indicating a stronger augmentation can help further enhance PDB\u2019s effectiveness. However, a tradeoff between augmentation intensity and model performance is also observed. ", "page_idx": 24}, {"type": "table", "img_path": "cbkJBYIkID/tmp/42af8de26ad03ac7a499e2204f756601d2449755cff1a1625b12b7b40905d675.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "In summary, a visible trigger with larger trigger size, higher sampling frequency, and data augmentation contribute to meeting Principle 4. ", "page_idx": 24}, {"type": "text", "text": "C.8 Factors that influence the accuracy of PDB ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Here, we would like to discuss the factors that influence the accuracy of PDB: ", "page_idx": 24}, {"type": "text", "text": "\u2022 Model capacity and data complexity: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. Model capacity: Since PDB introduces additional task, i.e., injecting defensive backdoor, increasing the model capacity helps to increase the accuracy of PDB, as evidenced in Table 27. ", "page_idx": 24}, {"type": "table", "img_path": "cbkJBYIkID/tmp/1d0c6fc23ed874050a17bdf389b03fde17160a2a65a287b036cca5f4311204ca.jpg", "table_caption": ["Table 27: Results on different models "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "2. Dataset complexity: By comparing defense results with different datasets, we can find that by decreasing the dataset complexity, the accuracy of PDB increases significantly. ", "page_idx": 24}, {"type": "text", "text": "\u2022 Strength of defensive backdoor: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. Strength of augmentation: From Figure 7, we can find that there exists a tradeoff between ACC and ASR. Therefore, the accuracy of PDB can be boosted by reducing the strength of augmentation.   \n2. Sampling frequency: From Table 26, we can find that by increasing the sampling frequency of defensive poisoned samples, the accuracy of PDB can be boosted.   \n3. Trigger size: Table 23 shows that a proper choice of trigger size can also help to increase the accuracy. Therefore, if a validation set is accessible, a proper trigger size can be chosen to increase accuracy. ", "page_idx": 24}, {"type": "text", "text": "In summary, due to the \"home field advantage\" of PDB, there are several ways to maintain a high accuracy even in the case of a low malicious poisoning ratio, such as increasing model capacity, simplifying the dataset, reducing the strength of augmentation to defensive poisoned samples, increasing the sampling frequency and choosing a proper defensive trigger size. ", "page_idx": 24}, {"type": "text", "text": "C.9 Novelty and comparison with backdoorIndicator ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Here, we highlight the distinctions between our approach and BackdoorIndicator [19]. ", "page_idx": 24}, {"type": "text", "text": "First, we would like to clarify the following differences between our work and backdoorIndicator [19]: ", "page_idx": 24}, {"type": "text", "text": "\u2022 Threat model: [19] targets decentralized training (FL) setting, where multiple clients train models locally and contribute updates to a central server. Our work considers a centralized training setting where only a central server is used. ", "page_idx": 25}, {"type": "text", "text": "\u2022 Task: [19] focuses on detecting malicious clients, whereas our method aims to train a secure model on a poisoned dataset without clients. ", "page_idx": 25}, {"type": "text", "text": "\u2022 Motivation: [19] is built on the motivation that planting subsequent backdoors with the same target label enhances previously planted backdoors, therefore, providing a way to detect the poisoned clients, while our method is based on the motivation that planting a concurrent reversible backdoor can help to mitigate the malicious backdoor. ", "page_idx": 25}, {"type": "text", "text": "\u2022 Methodology: [19] utilizes OOD samples for backdoor client detection while our method constructs a proactive defensive poison dataset, following well-designed principles. ", "page_idx": 25}, {"type": "text", "text": "Second, we would like to discuss the challenges in direct utilizing backdoorIndicator in our setting: ", "page_idx": 25}, {"type": "text", "text": "\u2022 BackdoorIndicator is designed to detect malicious clients within a federated learning (FL) context. This makes it challenging to apply BackdoorIndicator directly to our centralized environment since the task of identifying backdoored clients does not naturally fit into this setting (only a central server). ", "page_idx": 25}, {"type": "text", "text": "\u2022 For comparison between BackdoorIndicator and our method, we need to emulate an FL scenario by assigning each image to a separate client (ensuring existence of benign client), thereby creating 50,000 local models from the CIFAR-10 dataset to defend a single attack with PreAct-ResNet18. This would require an impractical amount of computational resources, estimated at over 30,000 hours (1,250 days) of training time and 30TB of storage space using a server with a single RTX 3090 GPU. ", "page_idx": 25}, {"type": "text", "text": "C.10 Comparison to FT-SAM ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "To address the comparison with FT-SAM [55], we have adapted their method to our experimental setting. It\u2019s worth noting that in [55], the authors employ the Blended attack with a blending ratio of 0.1 (Blended-0.1), whereas we use a blending ratio of 0.2 (Blended-0.2). For consistency and completeness, we have now included experiments using both blending ratios, and the results are shown below: ", "page_idx": 25}, {"type": "table", "img_path": "cbkJBYIkID/tmp/82cf48ad44a1df6926b7b1f9e8e03b9542e17a5f9766df64cd811699509139b5.jpg", "table_caption": ["Table 28: Results on PreAct-ResNet18 with FT-SAM "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "From Table 28, we can find FT-SAM can achieve a higher accuracy as it aims to fine-tune a backdoored model while PDB aims to train a model from scratch. Consistent with [55], Table 28 shows that FT-SAM can mitigate backdoor attacks for most cases, except for the Blended-0.2. We observe that FT-SAM struggles to defend against blended attacks with higher blending ratios, such as 0.2. Notably, PDB achieves a significantly lower ASR across all cases, with an average ASR below $0.5\\%$ . ", "page_idx": 25}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: The abstract and introduction of the paper accurately reflect the paper\u2019s contributions and scope. They clearly state that the paper addresses the challenge of training a clean model on a potentially poisoned dataset by proposing a novel defense mechanism. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 26}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: The paper discusses the limitations of the work performed by the authors. In the \"Limitations and future work\" section, we acknowledge that their implementation of the reversible backdoor defense has several key limitations. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not include theoretical results. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper appears to provide detailed information on the experimental setup based on a popular Benchmark Project, including the datasets used, the model architectures, the backdoor attack strategies, and the defense mechanisms compared. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Provided in the supplemental material. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: Provided in the main text and the appendix. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Provided in the main text and the appendix. Due to space limit, the bar are mainly visualized in the plots. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Provided in the main text and the appendix. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Provided in the main text. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: No such risks. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Properly credited. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 30}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: Do not release new assets. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: Not involve crowdsourcing nor research with human subjects. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: Not involve crowdsourcing nor research with human subjects. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}]