{"references": [{"fullname_first_author": "Tianyu Gu", "paper_title": "Badnets: Evaluating backdooring attacks on deep neural networks", "publication_date": "2019-00-00", "reason": "This paper is foundational for the field, introducing the concept of BadNets and providing a benchmark for evaluating backdooring attacks on deep neural networks."}, {"fullname_first_author": "Xinyun Chen", "paper_title": "Targeted backdoor attacks on deep learning systems using data poisoning", "publication_date": "2017-12-00", "reason": "This paper is highly influential for introducing the concept of targeted backdoor attacks through data poisoning, setting a key direction for subsequent research."}, {"fullname_first_author": "Yige Li", "paper_title": "Anti-backdoor learning: Training clean models on poisoned data", "publication_date": "2021-00-00", "reason": "This paper is important for proposing a new defense approach against backdoor attacks, shifting focus from detection and removal towards training clean models directly."}, {"fullname_first_author": "Bryant Chen", "paper_title": "Detecting backdoor attacks on deep neural networks by activation clustering", "publication_date": "2019-00-00", "reason": "This paper is important for proposing a novel detection method for backdoor attacks by leveraging unusual activation patterns, contributing to early detection techniques."}, {"fullname_first_author": "Kunzhe Huang", "paper_title": "Backdoor defense via decoupling the training process", "publication_date": "2022-00-00", "reason": "This paper is significant for introducing a novel defense strategy that decouples the training process, improving robustness and reducing performance costs compared to other methods."}]}