[{"type": "text", "text": "Cardinality-Aware Set Prediction and Top- $k$ Classification ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Corinna Cortes   \nGoogle Research   \nNew York, NY 10011   \ncorinna@google.com   \nAnqi Mao   \nCourant Institute   \nNew York, NY 10012   \naqmao@cims.nyu.edu   \nChristopher Mohri   \nStanford University   \nStanford, CA 94305   \nxmohri@stanford.edu   \nMehryar Mohri   \nGoogle Research & CIMS   \nNew York, NY 10011   \nmohri@google.com   \nYutao Zhong   \nCourant Institute   \nNew York, NY 10012   \nyutao@cims.nyu.edu ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We present a detailed study of cardinality-aware top- $k$ classification, a novel approach that aims to learn an accurate top- $k$ set predictor while maintaining a low cardinality. We introduce a new target loss function tailored to this setting that accounts for both the classification error and the cardinality of the set predicted. To optimize this loss function, we propose two families of surrogate losses: costsensitive comp-sum losses and cost-sensitive constrained losses. Minimizing these loss functions leads to new cardinality-aware algorithms that we describe in detail in the case of both top- $k$ and threshold-based classifiers. We establish $\\mathcal{H}$ -consistency bounds for our cardinality-aware surrogate loss functions, thereby providing a strong theoretical foundation for our algorithms. We report the results of extensive experiments on CIFAR-10, CIFAR-100, ImageNet, and SVHN datasets demonstrating the effectiveness and beneftis of our cardinality-aware algorithms. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Top- $k$ classification consists of predicting the $k$ most likely classes for a given input, as opposed to solely predicting the single most likely class. Several compelling reasons support the adoption of this framework. First, it enhances accuracy by allowing the model to consider the top $k$ predictions, accommodating uncertainty and providing a more comprehensive prediction. This is particularly valuable in scenarios where multiple correct answers exist, such as image tagging, where a top- $k$ classifier can identify multiple relevant objects in an image. Second, top- $k$ classification is applicable in ranking and recommendation tasks such as suggesting the top $k$ most relevant products in ecommerce based on user queries. The confidence scores associated with the top $k$ predictions also serve as a means to estimate the model\u2019s uncertainty, which is crucial in applications requiring insight into the model\u2019s confidence level. ", "page_idx": 0}, {"type": "text", "text": "The predictions of a top- $k$ classifier are also useful in several natural settings. For example, ensemble learning can benefti from top- $k$ predictions as they can be combined from multiple models, contributing to improved overall performance by introducing a more robust and diverse set of predictions. In addition, top- $k$ predictions can serve as input for downstream tasks like natural language generation or dialogue systems, enhancing the performance of these tasks by providing a broader range of potential candidates. Finally, the interpretability of the model\u2019s decision-making process is enhanced by examining the top $k$ predicted classes, allowing users to gain insights into the rationale behind the model\u2019s predictions. ", "page_idx": 0}, {"type": "text", "text": "The appropriate $k$ for a task at hand may be determined by the application itself like a recommendor system always expecting a fixed set size to be returned. For other applications, it may be natural to let the cardinality of the returned set vary with the model\u2019s confidence or other properties of the task. Designing effective algorithms with learning guarantees for this setting is our main goal. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we introduce the problem of cardinality-aware set prediction, which is to learn an accurate set predictor while maintaining a low cardinality. The core idea is that an effective algorithm should dynamically adjust the cardinality of its prediction sets based on input instances. For top- $k$ classifiers, this means selecting a larger $k$ for difficult inputs to ensure high accuracy, while opting for a smaller $k$ for simpler inputs to maintain low cardinality. Similarly, for threshold-based classifiers, a lower threshold can be used for difficult inputs to minimize the risk of misclassification, whereas a higher threshold can be applied to simpler inputs to reduce cardinality. ", "page_idx": 1}, {"type": "text", "text": "To tackle this problem, we introduce a novel target loss function which captures both the classification error and the cardinality of a prediction set. Minimizing this target loss function directly is an instance-dependent cost-sensitive learning problem, which is intractable for most hypothesis sets. Instead, we derive two families of general surrogate loss functions that benefti from smooth properties and favorable optimization solutions. ", "page_idx": 1}, {"type": "text", "text": "To provide theoretical guarantees for our cardinality-aware top- $k$ approach, we first study consistency properties of surrogate loss functions for the general top- $k$ problem with a fixed $k$ . Unlike standard classification, the consistency of surrogate loss functions for the top- $k$ problem has been relatively unexplored. A crucial property in this context is the asymptotic notion of Bayes-consistency, which has been extensively studied in standard binary and multi-class classification [Zhang, 2004a, Bartlett et al., 2006, Zhang, 2004b, Bartlett and Wegkamp, 2008]. While Bayes-consistency has been explored for various top- $k$ surrogate losses [Lapin et al., 2015, 2016, 2018, Yang and Koyejo, 2020, Thilagar et al., 2022], some face limitations. Non-convex \u201chinge-like\" surrogates [Yang and Koyejo, 2020], surrogates inspired by ranking [Usunier et al., 2009], and polyhedral surrogates [Thilagar et al., 2022] cannot lead to effective algorithms as they cannot be efficiently computed and optimized. Negative results also indicate that several convex \"hinge-like\" surrogates [Lapin et al., 2015, 2016, 2018] fail to achieve Bayes-consistency [Yang and Koyejo, 2020]. On the positive side, it has been shown that the logistic loss (or cross-entropy loss used with the softmax activation) is a Bayes-consistent loss for top- $k$ classification [Lapin et al., 2015, Yang and Koyejo, 2020]. ", "page_idx": 1}, {"type": "text", "text": "We show that, remarkably, several widely used families of surrogate losses used in standard multi-class classification admit $\\mathcal{H}$ -consistency bounds [Awasthi, Mao, Mohri, and Zhong, 2022a,b, Mao, Mohri, and Zhong, 2023f,b] with respect to the top- $k$ loss. These are strong non-asymptotic consistency guarantees that are specific to the actual hypothesis set $\\mathcal{H}$ adopted, and therefore also imply asymptotic Bayes-consistency. We establish this property for the broad family of comp-sum losses [Mao, Mohri, and Zhong, 2023f], comprised of the composition of a non-decreasing and non-negative function with the sum exponential losses. This includes the logistic loss, the sum-exponential loss, the mean absolute error loss, and the generalized cross-entropy loss. Additionally, we extend these results to constrained losses, a family originally introduced for multi-class SVM [Lee et al., 2004], which includes the constrained exponential, hinge, squared hinge, and $\\rho$ -margin losses. The guarantees of $\\mathcal{H}$ -consistency provide a strong foundation for principled algorithms in top- $k$ classification by directly minimizing these surrogate loss functions. ", "page_idx": 1}, {"type": "text", "text": "We then leverage these results to derive strong guarantees for the two families of cardinality-aware surrogate losses: cost-sensitive comp-sum and cost-sensitive constrained losses. Both families are obtained by augmenting their top- $k$ counterparts [Lapin et al., 2015, 2016, Berrada et al., 2018, Reddi et al., 2019, Yang and Koyejo, 2020, Thilagar et al., 2022] with instance-dependent cost terms. We establish strong $\\mathcal{H}$ -consistency bounds, implying Bayes-consistency, for both families relative to the cardinality-aware target loss. Our $\\mathcal{H}$ -consistency bounds for the top- $k$ problem are further beneficial here in that the cardinality-aware problem can consist of fixing and selecting from a family top- $k$ classifiers\u2013we now know how to effectively learn each top- ${\\cdot k}$ classifier. ", "page_idx": 1}, {"type": "text", "text": "The rest of the paper is organized as follows. In Section 2, we formally introduce the cardinality-aware set prediction problem along with our new families of surrogate loss functions. Section 3 instantiates our algorithms in the case of both top- $k$ classifiers and threshold-based classifiers, and Section 4 presents strong theoretical guarantees. In Section 5, as well as in Appendix J and Appendix K, we present experimental results on the CIFAR-10, CIFAR-100, ImageNet, and SVHN datasets, demonstrating the effectiveness of our algorithms. ", "page_idx": 1}, {"type": "text", "text": "2 Cardinality-aware set prediction ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we introduce cardinality-aware set prediction, where the goal is to devise algorithms that dynamically adjust the prediction set\u2019s size based on the input instance to both achieve high accuracy and maintain a low average cardinality. Specifically, for top- $k$ classifiers, our objective is to determine a suitable cardinality $k$ for each input $x$ , with higher values of $k$ for instances that are more difficult to classify. ", "page_idx": 2}, {"type": "text", "text": "To address this problem, we first define a cardinality-aware loss function that accounts for both the classification error and the cardinality of the set predicted (Section 2.1). However, minimizing this loss function directly is computationally intractable for non-trivial hypothesis sets. Thus, to optimize it, we introduce two families of surrogate losses: cost-sensitive comp-sum losses (Section 2.2) and cost-sensitive constrained losses (Section 2.3). We will later show that these loss functions benefits from favorable guarantees in terms of $\\mathcal{H}$ -consistency (Section 4.3). ", "page_idx": 2}, {"type": "text", "text": "2.1 Cardinality-aware problem formulation and loss function ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The learning setup for cardinality-aware set prediction is as follows. ", "page_idx": 2}, {"type": "text", "text": "Problem setup. We denote by $\\mathcal{X}$ the input space and $\\mathcal{Y}\\,=\\,[n];=\\,\\bigl\\{1,\\dots,n\\bigr\\}$ the label space. Let $\\{\\mathsf{g}_{k}\\colon k\\in\\mathcal{K}\\}$ denote a collection of given set predictors, induced by a parameterized set predictor $\\mathsf{g}_{k}\\colon\\mathcal{X}\\mapsto2^{\\big\\}$ , where each $\\mathcal{K}\\subset\\mathbb{R}$ is a set of indices. This could be a subset of the family of top- $k$ classifiers induced by some classifier $h$ , or a family of threshold-based classifiers based on some scoring function $s\\colon\\mathcal{X}\\times\\mathcal{Y}\\mapsto\\mathbb{R}$ . In that case, ${\\mathsf{g}}_{k}(x)$ then comprises the set of $y\\mathbf{s}$ with a score $s(x,y)$ exceeding the threshold $\\tau_{k}$ defining $\\mathtt{g}_{k}$ . This formulation covers as a special case standard conformal prediction set predictors [Shafer and Vovk, 2008], as well as set predictors defined as confidence sets described in [Denis and Hebiri, 2017]. We will denote by $|\\!\\up g_{k}(x)|$ the cardinality of the set ${\\mathsf{g}}_{k}(x)$ predicted by $\\mathtt{g}_{k}$ for the input $x$ . To simplify the discussion, we will assume that $|\\!\\up g_{k}(x)|$ is an increasing function of $k$ , for any $x$ . For a family of top- $k$ classifiers or threshold-based classifiers, this simply means that they are sorted in increasing order of $k$ or decreasing order of the threshold values. ", "page_idx": 2}, {"type": "text", "text": "To account for the cost associated with cardinality, we introduce a non-negative and increasing function cost\u2236 $\\mathbb{R}_{+}\\to\\mathbb{R}_{+}$ , where cost $(|\\mathtt{g}_{k}(x)|)$ represents the cost associated to the cardinality $|{\\tt g}_{k}(x)|$ . Common choices for cost include cost $(|\\mathbf{g}_{k}(\\vec{x})|)=|\\mathbf{g}_{k}(x)|$ , or a logarithmic function $\\mathsf{c o s t}(|\\mathbf{g}_{k}(x)|)=$ $\\log(|\\mathbf{g}_{k}(x)|)$ as in our experiments (see Section 5), to moderate the magnitude of the cost relative to the binary classification loss. Our analysis is general and requires no assumption about cost. ", "page_idx": 2}, {"type": "text", "text": "Our goal is to learn to assign to each input instance $x$ the most appropriate index $k\\,\\in\\,\\mathcal K$ to both achieve high accuracy and maintain a low average cardinality. ", "page_idx": 2}, {"type": "text", "text": "Cardinality-aware loss function. As in the ordinary multi-class classification problem, we consider a family $\\mathcal{R}$ of scoring functions $r\\!:\\!\\mathcal{X}\\times\\mathcal{K}\\rightarrow\\mathbb{R}$ . For any $x$ , $\\boldsymbol{r}(\\boldsymbol{x},\\boldsymbol{k})$ denotes the score assigned to the label (or index) $k\\in\\mathcal K$ , given $x\\in\\mathcal X$ . The label predicted is $\\mathsf{r}(\\bar{\\boldsymbol{x}})\\,=\\,\\mathrm{argmax}_{k\\in\\mathcal{K}}\\,r(\\boldsymbol{x},k)$ , with ties broken in favor of the largest index. To account for both classification accuracy and cardinality cost, we define the cardinality-aware loss function for a scoring function $r$ and input-output label pair $(x,y)\\in\\mathcal{X}\\times\\mathcal{Y}$ as a linearized loss of these two criteria: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\ell(r,x,y)=1_{y\\notin\\mathsf{g}_{r(x)}(x)}+\\lambda\\mathsf{c o s t}(\\big|\\mathsf{g}_{r(x)}(x)\\big|),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the first term is the standard loss for a top- $k$ prediction taking the value one when the correct label $y$ is not included in the top- $k$ set and zero otherwise, and $\\lambda>0$ is a hyperparameter that governs the balance between prioritizing accuracy versus limiting cardinality. The learning problem then consists of using a labeled training sample $(x_{1},y_{1}),\\dotsc(x_{m}^{\\overline{{{\\upsilon_{m}}}}},y_{m})$ drawn i.i.d. from some (unknown) distribution $\\mathcal{D}$ to select $r\\in\\mathcal{R}$ with a small expected cardinality-aware loss $\\mathbb{E}_{(x,y)\\sim\\mathcal{D}}[\\ell(r,x,y)]$ . ", "page_idx": 2}, {"type": "text", "text": "The loss function (1) can be equivalently expressed in terms of an instance-dependent cost function $c\\colon\\mathcal{X}\\times\\mathcal{X}\\times\\mathcal{Y}\\rightarrow\\mathbb{R}_{+}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\ell(r,x,y)=c(x,r(x),y),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $c(x,k,y)=1_{y\\notin\\mathbf{g}_{k}(x)}+\\lambda\\mathsf{c o s t}(|\\mathbf{g}_{k}(x)|)$ . Minimizing (2) is an instance-dependent cost-sensitive learning problem. However, directly minimizing this target loss is intractable. To optimize this loss function, we introduce two families of surrogate losses in the next sections: cost-sensitive comp-sum losses and cost-sensitive constrained losses. Note that throughout this paper, we will denote all target (or true) losses on which performance is measured with an $\\ell$ , while surrogate losses introduced for ease of optimization are denoted by ${\\widetilde{\\ell}}.$ ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "2.2 Cost-sensitive comp-sum surrogate losses ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our surrogate cost-sensitive comp-sum, $c$ -comp, losses are defined as follows: for all $(r,x,y)\\in$ $\\mathcal{R}\\times\\mathcal{X}\\times\\mathcal{Y}$ , $\\begin{array}{r}{\\widetilde{\\ell}_{\\mathrm{c-comp}}(r,x,y)=\\sum_{k\\in\\mathcal{K}}(1-c(x,k,y))\\widetilde{\\ell}_{\\mathrm{comp}}(r,x,k)}\\end{array}$ , where the comp-sum loss $\\widetilde{\\ell}_{\\mathrm{comp}}$ is defined as in [Mao, Mohri, and Zhong, 2023f]. That is, for any $r$ in a hypothesis set $\\mathcal{R}$ and $(x,y)\\in\\mathcal{X}\\times\\mathcal{Y}$ , $\\begin{array}{r}{\\widetilde{\\ell}_{\\mathrm{comp}}(r,x,y)=\\Phi\\Big(\\sum_{y^{\\prime}\\neq y}e^{r(x,y^{\\prime})-r(x,y)}\\Big)}\\end{array}$ , where $\\Phi\\colon\\mathbb{R}_{+}\\to\\mathbb{R}_{+}$ is non-decreasing. See Section 4.2 for more details. For example, when the logistic loss is used, we obtain the cost-sensitive logistic loss: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\tilde{\\mathrm{\\Sigma}}_{\\mathrm{c-log}}(r,x,y)=\\sum_{k\\in\\mathcal{K}}(1-c(x,k,y))\\widetilde{\\ell}_{\\mathrm{log}}(r,x,k)=\\sum_{k\\in\\mathcal{K}}(c(x,k,y)-1)\\Biggl[-\\log\\Biggl(\\sum_{k^{\\prime}\\in\\mathcal{K}}e^{r(x,k^{\\prime})-r(x,k)}\\Biggr)\\Biggr].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The negative log-term becomes larger as the score $\\boldsymbol{r}(\\boldsymbol{x},\\boldsymbol{k})$ increases. Thus, the loss function imposes a greater penalty on higher scores $\\boldsymbol{r}(\\boldsymbol{x},\\boldsymbol{k})$ through a penalty term $(c(x,k,y)-1)$ that depends on the cost assigned to the expert\u2019s prediction ${\\tt g}_{k}(x)$ . ", "page_idx": 3}, {"type": "text", "text": "2.3 Cost-sensitive constrained surrogate losses ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Constrained losses are defined as a summation of a function $\\Phi$ applied to the scores, subject to a constraint, as in [Lee et al., 2004]. For any $r\\in\\mathcal{R}$ and $(x,y)\\in\\mathcal{X}\\times\\mathcal{Y}$ , they are expressed as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\widetilde{\\ell}_{\\mathrm{cstnd}}(h,x,y)=\\sum_{y^{\\prime}\\neq y}\\Phi(-r(x,y^{\\prime})),\\mathrm{~with~the~constraint~}\\sum_{y\\in\\mathcal{Y}}r(x,y)=0,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\Phi\\colon\\mathbb{R}\\ \\to\\ \\mathbb{R}_{+}$ is non-increasing. See Section 4.2 for a detailed discussion. Inspired by these constrained losses, we introduce a new family of surrogate losses, cost-sensitive constrained ( $\\scriptstyle{\\mathit{c}}$ -cstnd losses) which are defined, for all $(r,x,y)\\,\\in\\,\\mathcal{R}\\times\\mathcal{X}\\times\\mathcal{Y}$ , by $\\widetilde{\\ell}_{\\mathrm{c-cstnd}}(r,x,y)\\;=$ $\\begin{array}{r}{\\sum_{k\\in\\mathcal{K}}c(x,k,y)\\Phi(-r(x,k))}\\end{array}$ , with the constraint $\\begin{array}{r}{\\sum_{k\\in\\mathcal{K}}r(\\boldsymbol{x},\\boldsymbol{k})\\ =\\ 0}\\end{array}$ , where $\\Phi\\colon\\mathbb{R}\\ \\rightarrow\\ \\mathbb{R}_{+}$ is nonincreasing. For example, for $\\Phi(t)\\;=\\;e^{-t}$ , we obtain the cost-sensitive constrained exponential loss: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\widetilde{\\ell}_{\\mathrm{c-exp}}^{\\mathrm{cstnd}}(r,x,y)=\\sum_{k\\in\\mathcal{K}}c(x,k,y)e^{r(x,k)},\\;\\mathrm{with\\the\\constraint}\\;\\sum_{k\\in\\mathcal{K}}r(x,k)=0.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "3 Cardinality-aware algorithms ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Minimizing the cost-sensitive surrogate loss functions described in the previous section directly leads to novel cardinality-aware algorithms. In this section, we briefly detail the instantiation of our algorithms in the specific cases of top- $k$ classifiers (our main focus) and threshold-based classifiers. ", "page_idx": 3}, {"type": "text", "text": "Top- $k$ classifiers. Here, the collection of set predictors is a subset of the top- $k$ classifiers, defined by $\\mathsf{g}_{k}\\bar{(}x)=\\left\\{\\mathsf{h}_{1}(x),\\ldots,\\mathsf{h}_{k}(x)\\right\\}$ , where $\\mathsf{h}_{1}(x),\\mathsf{\\bar{\\alpha}}\\dotsc,\\mathsf{h}_{k}(x)$ are the induced top- $k$ labels for a classifier $h$ . The cardinality in this case coincides with the index: $|\\mathtt{g}_{k}(x)|\\;=\\;k$ , for any $x\\,\\in\\,\\mathfrak{X}$ . The cost is defined as $c(x,k,y)\\,=\\,1_{y\\notin\\{{\\mathsf{h}}_{1}(x),\\ldots,{\\mathsf{h}}_{k}(x)\\}}\\,+\\,\\lambda{\\mathsf{c o s t}}(k)$ , where $\\mathsf{c o s t}(k)$ can be chosen to be $k$ or $\\log(k)$ . Thus, our cardinality-aware algorithms for top- $k$ classification can be described as follows. At training time, we assume access to a sample set $\\left\\{(x_{i},y_{i})\\right\\}_{i=1}^{m}$ and the costs each top- $k$ set incurs, $\\{c(x_{i},k,y_{i})\\}_{i=1}^{m}$ , where $k\\ \\in\\ \\mathcal K$ , a pre-fixed subset. The goal is to minimize the target cardinality-aware loss function $\\begin{array}{r}{\\sum_{i=1}^{m}\\ell(r,x_{i},\\stackrel{\\_}{y_{i}})=\\sum_{i=1}^{m}c\\big(x_{i},r(x_{i}),\\bar{y_{i}}\\big)}\\end{array}$ over a hypothesis set $\\mathcal{R}$ . Our algorithm consists of minimizing a surrogate loss such as the cost-sensitive logistic loss, defined as $\\begin{array}{r}{\\hat{r}=\\operatorname*{argmin}_{r\\in\\mathcal{R}}\\sum_{i=1}^{m}\\sum_{k\\in\\mathcal{K}}\\bigl(1-\\stackrel{\\cdot}{c}\\!\\!\\!\\bigl(x_{i},k,y_{i})\\bigr)\\log\\Bigl(\\sum_{k^{\\prime}\\in\\mathcal{K}}e^{r(x,k^{\\prime})-r(x,k)}\\Bigr)}\\end{array}$ . At inference time, we use the top- ${\\hat{\\boldsymbol{\\mathsf{r}}}}(x)$ set $\\big\\{\\mathsf{h}_{1}(x),\\ldots,\\mathsf{h}_{\\hat{r}(x)}(x)\\big\\}$ for prediction, with the accuracy $1_{y\\in\\left\\{\\mathsf{h}_{1}\\left(x\\right),\\ldots,\\mathsf{h}_{\\hat{r}\\left(x\\right)}\\left(x\\right)\\right\\}}$ and cardinality ${\\hat{\\mathsf{r}}}(x)$ for that instance. ", "page_idx": 3}, {"type": "text", "text": "In Section 5, we compare the accuracy-versus-cardinality curves of our cardinality-aware algorithms obtained by varying $\\lambda$ with those of top- $k$ classifiers, demonstrating the effectiveness of our algorithms. What $\\lambda$ to select for a given application will depend on the desired accuracy. Note that the performance of the algorithm in [Denis and Hebiri, 2017] in this setting is theoretically the same as that of top- $k$ classifiers. The algorithm is designed to maximize accuracy within a constrained cardinality of $k$ , and it always reaches maximal accuracy at the boundary $K$ after the cardinality is constrained to $k\\leq K$ . ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Threshold-based classifiers. Here, the set predictor is defined via a set of thresholds $\\tau_{k}$ : $\\ g_{k}(x)=$ $\\left\\{y\\in{\\mathfrak{Y}}{:}s(x,y)>\\tau_{k}\\right\\}$ . When the set is empty, we just return $\\mathrm{argmax}_{\\boldsymbol{y}\\in\\mathbb{y}}\\,s(\\boldsymbol{x},\\boldsymbol{y})$ by default. The description of the costs and other components of the algorithms is similar to that of top- ${\\cdot k}$ classifiers. A special case of threshold-based classifier is conformal prediction [Shafer and Vovk, 2008], which is a general framework that provides provably valid confidence intervals for a black-box scoring function. Split conformal prediction guarantees that $\\mathbb{P}\\!\\left(Y_{m+1}\\in C_{s,\\alpha}(X_{m+1})\\right)\\geq1-\\alpha$ for some scoring function $s\\colon\\mathcal{X}\\times\\mathcal{Y}\\rightarrow\\mathbb{R}$ , where $C_{s,\\alpha}\\bar{(X_{m+1})}=\\{y;s(\\dot{X}_{m+1},y)\\geq\\hat{q}_{\\alpha}\\}$ and $\\hat{q}_{\\alpha}$ is the $\\lceil\\alpha(m+1)\\rceil/m$ empirical quantile of $s(X_{i},Y_{i})$ over a held-out set $\\{(X_{i},Y_{i})\\}_{i=1}^{m}$ drawn i.i.d. from some distribution $\\mathcal{D}$ (or just exchangeably). Note, however, that the framework does not supply an effective guarantee on the size of the sets $\\dot{C}_{s,\\alpha}(X_{m+1})$ . ", "page_idx": 4}, {"type": "text", "text": "In Appendix K, we present in detail a series of early experiments for our algorithm used with threshold-based classifiers and include more discussion. Our experiments suggest that, when the training sample is sufficiently large, our algorithm can outperform conformal prediction. ", "page_idx": 4}, {"type": "text", "text": "4 Theoretical guarantees ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Here, we present theory for our cardinality-aware algorithms. Our analysis builds on theory of top- $k$ algorithms, and we start by providing stronger results than previously known for top- $k$ surrogates. ", "page_idx": 4}, {"type": "text", "text": "4.1 Preliminaries ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We denote by $\\mathcal{D}$ a distribution over $\\mathcal{X}\\times\\mathcal{Y}$ and write $p(x,y)\\;=\\;\\mathcal{D}(Y=y\\;\\vert\\;X=x)$ for the conditional probability of $Y\\;=\\;y$ given $X\\ =\\ x$ , and use $p(x)\\;=\\;\\bigl(p(x,1),\\ldots,p(x,n)\\bigr)$ to denote the corresponding conditional probability vector. We denote by $\\ell\\colon\\mathcal{H}_{\\mathrm{all}}\\times\\mathcal{X}\\times\\mathcal{Y}\\rightarrow\\mathbb{R}$ a loss function defined for the family of all measurable functions $\\mathcal{H}_{\\mathrm{all}}$ . Given a hypothesis set $\\mathcal{H}\\,\\subseteq\\,\\mathcal{H}_{\\mathrm{all}}$ , the conditional error of a hypothesis $h$ and the best-in-class conditional error are defined as follows: $\\begin{array}{r}{\\mathfrak{C}_{\\ell}(h,x)=\\mathbb{E}_{y|x}[\\ell(h,x,y)]=\\sum_{y\\in\\mathfrak{Y}}p(x,y)\\ell(h,x,y)}\\end{array}$ and $\\begin{array}{r}{\\mathbb{C}_{\\ell}^{*}(\\mathcal{H},x)=\\operatorname*{inf}_{h\\in\\mathcal{H}}\\mathcal{C}_{\\ell}(h,x)}\\end{array}$ . Accordingly, the generalization error of a hypothesis $h$ and the best-in-class generalization error are defined by: $\\pounds_{\\ell}(\\bar{h})=\\mathbb{E}_{(x,y)\\sim\\mathcal{D}}[\\ell(h,x,y)]\\stackrel{*}{=}\\mathbb{E}_{x}[\\mathcal{C}_{\\ell}(h,x)]$ and $\\begin{array}{r}{\\mathcal{E}_{\\ell}^{*}(\\mathcal{H})=\\operatorname*{inf}_{h\\in\\mathcal{H}}\\mathcal{E}_{\\ell}(h)=\\operatorname*{inf}_{h\\in\\mathcal{H}}\\mathbb{E}_{x}[\\mathcal{\\mathrm{e}}_{\\ell}(h,x)]}\\end{array}$ . Given a score vector $(h(x,1),\\cdot\\cdot\\cdot,h(x,n))$ generated by hypothesis $h$ , we sort its components in decreasing order and write $\\mathsf{h}_{k}(x)$ to denote the $k$ -th label, that is $h(x,\\mathsf{h}_{1}(x))\\geq h(x,\\mathsf{h}_{2}({\\bar{x}}))\\geq\\ldots\\geq$ $h(x,\\mathsf{h}_{n}(\\bar{x}))$ . Similarly, for a given conditional probability vector $p(x)=(p(x,1),\\dots,p(x,n))$ , we write ${\\mathsf p}_{k}(x)$ to denote the $k$ -th element in decreasing order, that is $p(x,{\\mathsf p}_{1}(x))\\geq p(x,{\\mathsf p}_{2}(x))\\geq...\\geq$ $p(x,{\\mathsf p}_{n}(x))$ . In the event of a tie for the $k$ -th highest score or conditional probability, the label $\\mathsf{h}_{k}(x)$ or ${\\mathsf p}_{k}(x)$ is selected based on the highest index when considering the natural order of labels. ", "page_idx": 4}, {"type": "text", "text": "The target generalization error for top- $k$ classification is given by the top- $k$ loss, which is denoted by $\\ell_{k}$ and defined, for any hypothesis $h$ and $(x,y)\\in\\mathcal{X}\\times\\mathcal{Y}$ by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\ell_{k}(h,x,y)=1_{y\\notin\\{\\mathsf{h}_{1}(x),\\ldots,\\mathsf{h}_{k}(x)\\}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The loss takes value one when the correct label $y$ is not included in the top- $k$ predictions made by the hypothesis $h$ , zero otherwise. In the special case where $k=1$ , this is precisely the familiar zero-one classification loss. Like the zero-one loss, optimizing the top- $k$ loss is NP-hard for common hypothesis sets. Therefore, alternative surrogate losses are typically used to design learning algorithms. A crucial property of these surrogate losses is Bayes-consistency. This requires that, asymptotically, nearly minimizing a surrogate loss over the family of all measurable functions leads to the near minimization of the top- $k$ loss over the same family [Steinwart, 2007]. ", "page_idx": 4}, {"type": "text", "text": "Definition 4.1. A surrogate loss $\\widetilde{\\ell}$ is said to be Bayes-consistent with respect to the top- $k$ loss $\\ell_{k}$ if, for all given sequences of hypotheses $\\{h_{n}\\}_{n\\in\\mathbb{N}}\\subset\\mathcal{H}_{\\mathrm{all}}$ and any distribution, $\\begin{array}{r}{\\operatorname*{lim}_{n\\to+\\infty}\\mathcal{E}_{\\widetilde{\\ell}}(h_{n})-}\\end{array}$ $\\bar{\\mathcal{E}}_{\\widehat{\\ell}}^{\\ast}(\\mathcal{H}_{\\mathrm{all}})\\;\\overline{{=}}\\;0$ implies $\\begin{array}{r}{\\operatorname*{lim}_{n\\rightarrow+\\infty}\\mathcal{E}_{\\ell_{k}}\\!\\left(h_{n}\\right)-\\mathcal{E}_{\\ell_{k}}^{*}\\!\\left(\\bar{\\mathcal{H}}_{\\mathrm{all}}\\right)=0}\\end{array}$ . ", "page_idx": 4}, {"type": "text", "text": "Bayes-consistency is an asymptotic guarantee and applies only to the family of all measurable functions. Recently, Awasthi, Mao, Mohri, and Zhong [2022a,b] (see also [Awasthi et al., 2021a,b, 2023a,b, Mao et al., 2023c,d,e,a, 2024c,b,a,e,h,i,d,f,g, Mohri et al., 2024]) proposed a stronger consistency guarantee, referred to as $\\mathcal{H}$ -consistency bounds. These are upper bounds on the target estimation error in terms of the surrogate estimation error that are non-asymptotic and hypothesis set-specific. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Definition 4.2. Given a hypothesis set $\\mathcal{H}$ , a surrogate loss $\\widetilde{\\ell}$ is said to admit an $\\mathcal{H}$ -consistency bound with respect to the top- $k$ loss $\\ell_{k}$ if, for some non-decreasing function $f$ , the following inequality holds for all $h\\in\\mathcal{H}$ and for any distribution: $f\\big(\\mathcal{E}_{\\ell_{k}}(h)-\\mathcal{E}_{\\ell_{k}}^{*}(\\mathcal{H})\\big)\\leq\\mathcal{E}_{\\widetilde{\\ell}}(h)-\\mathcal{E}_{\\widetilde{\\ell}}^{*}(\\mathcal{H})$ . ", "page_idx": 5}, {"type": "text", "text": "We refer to $\\mathcal{E}_{\\ell_{k}}(h)-\\mathcal{E}_{\\ell_{k}}^{*}(\\mathcal{H})$ as the target estimation error and $\\mathcal{E}_{\\widetilde{\\ell}}(h)-\\mathcal{E}_{\\widetilde{\\ell}}^{*}(\\mathcal{H})$ as the surrogate estimation error. These bounds imply Bayes-consistency when $\\mathcal{H}=\\mathcal{H}_{\\mathrm{all}}$ , by taking the limit. ", "page_idx": 5}, {"type": "text", "text": "A key quantity appearing in $\\mathcal{H}$ -consistency bounds is the minimizability $g a p$ , which measures the difference between the best-in-class generalization error and the expectation of the best-inclass conditional error, defined for a given hypothesis set H and a loss function $\\ell$ by: $\\mathcal{M}_{\\ell}(\\mathcal{H})\\;=\\;$ $\\mathcal{E}_{\\ell}^{*}(\\mathcal{H})-\\mathbb{E}_{x}\\bigl[\\mathcal{C}_{\\ell}^{*}(\\mathcal{H},x)\\bigr]$ . As shown by Mao, Mohri, and Zhong [2023f], the minimizability gap is non-negative and is upper bounded by the approximation error $\\mathcal{A}_{\\ell}(\\mathcal{H})\\;=\\;\\mathcal{E}_{\\ell}^{*}(\\mathcal{H})\\,-\\,\\mathcal{E}_{\\ell}^{*}(\\bar{\\mathcal{H}}_{\\mathrm{all}}^{-})$ : $0\\,\\leq\\,\\mathfrak{M}_{\\ell}(\\bar{\\mathcal{H}})\\,\\leq\\,\\mathcal{A}_{\\ell}(\\mathcal{H})$ . When $\\mathcal{H}\\,=\\,\\mathcal{H}_{\\mathrm{all}}$ or more generally $\\mathcal{A}_{\\ell}(\\mathcal{H})\\,=\\,0$ , the minimizability gap vanishes. However, in general, it is non-zero and provides a finer measure than the approximation error. Thus, $\\mathcal{H}$ -consistency bounds provide a stronger guarantee than the excess error bounds. ", "page_idx": 5}, {"type": "text", "text": "4.2 Theoretical guarantees for top- $k$ surrogate losses ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We study the surrogate loss families of comp-sum losses and constrained losses in multi-class classification, which have been shown in the past to benefti from $\\mathcal{H}$ -consistency bounds with respect to the zero-one classification loss, that is $\\ell_{k}$ with $k=1$ [Awasthi et al., 2022b, Mao et al., 2023f] (see also [Zheng et al., 2023, Mao et al., 2023b]). We extend these results to top- $k$ classification and prove $\\mathcal{H}$ -consistency bounds for these loss functions with respect to $\\ell_{k}$ for any $1\\leq k\\leq n$ . ", "page_idx": 5}, {"type": "text", "text": "Another commonly used family of surrogate losses in multi-class classification is the max losses, which are defined through a convex function, such as the hinge loss function applied to the margin [Crammer and Singer, 2001, Awasthi et al., 2022b]. However, as shown in [Awasthi et al., 2022b], no non-trivial $\\mathcal{H}$ -consistency guarantee holds for max losses with respect to $\\ell_{k}$ , even when $k=1$ . ", "page_idx": 5}, {"type": "text", "text": "We first characterize the best-in-class conditional error and the conditional regret of top- $k$ loss, which will be used in the analysis of $\\mathcal{H}$ -consistency bounds. We denote by $S^{[k]}=\\left\\{X\\subset S\\mid|X|=k\\right\\}$ the set of all $k$ -subsets of a set $S$ . We will study any hypothesis set that is regular. ", "page_idx": 5}, {"type": "text", "text": "Definition 4.3. Let $A(n,k)$ be the set of ordered $k$ -tuples with distinct elements in $[n]$ . We say that a hypothesis set $\\mathcal{H}$ is regular for top- $k$ classification, if the top- $k$ predictions generated by the hypothesis set cover all possible outcomes: $\\forall x\\in\\mathcal{X}$ , $\\{(\\mathsf{h}_{1}(x),\\mathsf{\\bar{\\alpha}}.\\ldots,\\bar{\\mathsf{h}}_{k}(\\bar{x}))\\colon h\\in\\mathcal{H}\\}=A(n,k)$ . ", "page_idx": 5}, {"type": "text", "text": "Common hypothesis sets such as that of linear models or neural networks, or the family of all measurable functions, are all regular for top- $\\cdot k$ classification. ", "page_idx": 5}, {"type": "text", "text": "Lemma 4.4. Assume that $\\mathcal{H}$ is regular. Then, for any $h\\in\\mathcal{H}$ and $x\\in\\mathcal X$ , the best-in-class conditional error and the conditional regret of the top- $\\cdot k$ loss can be expressed as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathfrak{C}_{\\ell_{k}}^{*}(\\mathcal{H},x)=1-\\sum_{i=1}^{k}p(x,\\mathfrak{p}_{i}(x))\\quad\\Delta\\mathfrak{C}_{\\ell_{k},\\mathcal{H}}(h,x)=\\sum_{i=1}^{k}[p(x,\\mathfrak{p}_{i}(x))-p(x,\\mathfrak{h}_{i}(x))].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The proof is included in Appendix A. For $k=1$ , the result coincides with the known identities for standard multi-class classification with regular hypothesis sets [Awasthi et al., 2022b, Lemma 3]. ", "page_idx": 5}, {"type": "text", "text": "As with [Awasthi et al., 2022b, Mao et al., 2023f], in the following sections, we will consider hypothesis sets that are symmetric and complete. This includes the class of linear models and neural networks typically used in practice, as well as the family of all measurable functions. We say that a hypothesis set $\\mathcal{H}$ is symmetric if it is independent of the ordering of labels. That is, for all $y\\in\\big y$ , the scoring function $x\\mapsto h(x,y)$ belongs to some real-valued family of functions $\\mathcal{F}$ . We say that a hypothesis set is complete if, for all $(x,y)\\in\\mathcal{X}\\times\\mathcal{Y}$ , the set of scores $h(x,y)$ can span over the real numbers, that is, $\\{\\bar{h}(x,y)\\colon h\\in\\mathcal{H}\\}=\\mathbb{R}$ . Note that any symmetric and complete hypothesis set is regular for top- $k$ classification. ", "page_idx": 5}, {"type": "text", "text": "Next, we analyze the broad family of comp-sum losses, which includes the commonly used logistic loss (or cross-entropy loss used with the softmax activation) as a special case. ", "page_idx": 5}, {"type": "text", "text": "Comp-sum losses are defined as the composition of a function $\\Phi$ with the sum exponential losses, as in [Mao et al., 2023f]. For any $h\\in\\mathcal{H}$ and $(x,y)\\in\\mathcal{X}\\times\\mathcal{Y}$ , they are expressed as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\widetilde{\\ell}_{\\mathrm{comp}}(h,x,y)=\\Phi\\Bigg(\\sum_{y^{\\prime}\\neq y}e^{h(x,y^{\\prime})-h(x,y)}\\Bigg),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\Phi\\colon\\mathbb{R}_{+}\\,\\to\\,\\mathbb{R}_{+}$ is non-decreasing. When $\\Phi$ is chosen as the function $t\\mapsto\\log(1+t),\\,t\\mapsto t,$ $\\begin{array}{r}{t\\mapsto1-\\frac{1}{1+t}}\\end{array}$ and $\\begin{array}{r}{t\\mapsto\\frac{1}{q}\\big(1-\\big(\\frac{1}{1+t}\\big)^{q}\\big)}\\end{array}$ , $q\\in(0,1)$ , $\\widetilde{\\ell}_{\\mathrm{comp}}(h,x,y)$ coincides with the most commonly used (multinomial) logistic loss, defined as $\\begin{array}{r}{\\widetilde{\\ell}_{\\log}(h,x,y)\\;=\\;\\log\\Bigl(\\sum_{y^{\\prime}\\in\\mathcal{Y}}e^{h(x,y^{\\prime})-h(x,y)}\\Bigr)}\\end{array}$ [Verhulst, 1838, 1845, Berkson, 1944, 1951], the sum-exponential loss $\\begin{array}{r}{\\widetilde{\\ell}_{\\mathrm{exp}}(h,x,y)\\mathop{=}\\sum_{y^{\\prime}\\not=y}e_{\\phantom{\\prime}\\underline{{y}}^{\\prime}}^{h(x,y^{\\prime})-h(x,y)}}\\end{array}$ [Weston and Watkins, 1998, Awasthi et al., 2022b] which is widely used in multi-class boosting [Saberian and Vasconcelos, 2011, Mukherjee and Schapire, 2013, Kuznetsov et al., 2014], the mean absolute error loss $\\begin{array}{r}{\\widetilde{\\ell}_{\\mathrm{mae}}(h,x,y)=1-\\left[\\sum_{y^{\\prime}\\in\\mathcal{Y}}e^{h(x,y^{\\prime})-h(x,y)}\\right]^{-1}}\\end{array}$ known to be robust to label noise for training neural networks [Ghosh et al., 2017], and the generalized cross-entropy loss $\\begin{array}{r}{\\widetilde{\\ell}_{\\mathrm{gce}}(h,x,y)=\\frac{1}{q}\\Bigg[1-\\Big[\\sum_{y^{\\prime}\\in\\mathfrak{Y}}e^{h(x,y^{\\prime})-h(x,y)}\\Big]^{-q}\\Bigg],\\,q\\in\\mathfrak{Z}}\\end{array}$ $q\\in(0,1)$ , a generalization of the logistic loss and mean absolute error loss for learning deep neural networks with noisy labels [Zhang and Sabuncu, 2018], respectively. We specifically study these loss functions and show that they benefit from $\\mathcal{H}$ -consistency bounds with respect to the top- $k$ loss. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.5. Assume that $\\mathcal{H}$ is symmetric and complete. Then, for any $1\\leq k\\leq n$ , the following $\\mathcal{H}$ -consistency bound holds for the comp-sum loss: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{\\ell_{k}}(h)-\\mathcal{E}_{\\ell_{k}}^{*}(\\mathcal{H})+\\mathcal{M}_{\\ell_{k}}(\\mathcal{H})\\leq k\\psi^{-1}\\Big(\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{comp}}}(h)-\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{comp}}}^{*}(\\mathcal{H})+\\mathcal{M}_{\\widetilde{\\ell}_{\\mathrm{comp}}}(\\mathcal{H})\\Big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In the special case where $\\mathcal{A}_{\\widetilde{\\ell}_{\\mathrm{comp}}}(\\mathcal{H})=0,$ , for any $1\\leq k\\leq n$ , the following upper bound holds: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{\\ell_{k}}(h)-\\mathcal{E}_{\\ell_{k}}^{*}(\\mathcal{H})\\leq k\\psi^{-1}\\Big(\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{comp}}}(h)-\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{comp}}}^{*}(\\mathcal{H})\\Big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\stackrel{\\mathrm{sure~}}{_>}\\psi(t)=\\frac{1-t}{2}\\log(1\\!-\\!t)\\!+\\!\\frac{1\\!+\\!t}{2}\\log(1\\!+\\!t),t\\in[0,1]\\,w h e n\\,\\widetilde{\\ell}_{\\mathrm{comp~}}i s\\,\\widetilde{\\ell}_{\\log};\\,\\psi(t)=1\\!-\\!\\sqrt{1\\!-\\!t^{2}},\\,t\\in[0,1]}\\\\ &{}&{\\widetilde{\\ell}_{\\mathrm{comp~}}i s\\,\\widetilde{\\ell}_{\\mathrm{exp}};\\,\\psi(t)=t/n\\,\\,w h e n\\,\\widetilde{\\ell}_{\\mathrm{comp~}}i s\\,\\widetilde{\\ell}_{\\mathrm{mac}};\\,a n d\\,\\psi(t)=\\frac{1}{q n^{q}}\\left[\\left[\\frac{(1+t)^{\\frac{1}{1-q}}+(1-t)^{\\frac{1}{1-q}}}{2}\\right]^{1-q}-1\\right],}\\\\ &{}&{\\stackrel{\\sim}{_-}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The proof is included in Appendix B. The second part follows from the fact that when $\\mathcal{A}_{\\widetilde{\\ell}_{\\mathrm{comp}}}(\\mathcal{H})=0$ , the minimizability gap $\\mathcal{M}_{\\widetilde{\\ell}_{\\mathrm{comp}}}(\\mathcal{H})$ vanishes. By taking the limit on both sides, Theorem 4.5 implies the $\\mathcal{H}$ -consistency and Bayes-consistency of comp-sum losses with respect to the top- $k$ loss. It further shows that, when the estimation error of $\\widetilde{\\ell}_{\\mathrm{comp}}$ is reduced to $\\epsilon>0$ , then the estima\u221ation error of $\\ell_{k}$ is upper bounded by $k\\psi^{-1}(\\epsilon)$ , which, for a sufficiently small $\\epsilon$ , is approximately $k\\sqrt{2\\epsilon}$ for $\\widetilde{\\ell}_{\\mathrm{log}}$ and $\\widetilde{\\ell}_{\\mathrm{exp}}$ ; kn\u03f5 for $\\widetilde{\\ell}_{\\mathrm{mae}}$ ; and $k{\\sqrt{2n^{q}\\epsilon}}$ for $\\widetilde{\\ell}_{\\mathrm{gce}}$ . Note that different from the other loss functions, the bound for the mean absolute error loss is only linear. The downside of this more favorable linear rate is the dependency on the number of classes and the fact that the mean absolute error loss is harder to optimize [Zhang and Sabuncu, 2018]. The bound for the generalized cross-entropy loss depends on both the number of classes $n$ and the parameter $q$ . ", "page_idx": 6}, {"type": "text", "text": "In the proof, we used the fact that the conditional regret of the top- ${\\cdot k}$ loss is the sum of $k$ differences between two probabilities. We then upper bounded each difference with the conditional regret of the comp-sum loss, using a hypothesis based on the two probabilities. The final bound is derived by summing these differences. In Appendix G, we detail the technical challenges and the novelty. ", "page_idx": 6}, {"type": "text", "text": "The key quantities in our $\\mathcal{H}$ -consistency bounds are the minimizability gaps, which can be upper bounded by the approximation error, or more refined terms, depending on the magnitude of the parameter space, as discussed by Mao et al. [2023f]. As pointed out by these authors, these quantities, along with the functional form, can help compare different comp-sum loss functions. In Appendix C, we further discuss the important role of minimizability gaps under the realizability assumption, and the connection with some negative results of Yang and Koyejo [2020]. ", "page_idx": 6}, {"type": "text", "text": "Constrained losses are defined as a summation of a function $\\Phi$ applied to the scores, subject to a constraint, as shown in [Lee et al., 2004]. For any $h\\in\\mathcal{H}$ and $(x,y)\\in\\mathcal{X}\\times\\mathcal{Y}$ , they are expressed as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\widetilde{\\ell}_{\\mathrm{cstnd}}\\big(h,x,y\\big)=\\sum_{y^{\\prime}\\neq y}\\Phi\\big({-h(x,y^{\\prime})}\\big)\n$$", "text_format": "latex", "page_idx": 7}, {"type": "equation", "text": "$$\n\\sum_{y\\in\\mathbb{Y}}h(x,y)=0,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\Phi\\colon\\mathbb{R}\\to\\mathbb{R}_{+}$ is non-increasing. In Appendix E, we study this family of loss functions and show that several benefit from $\\mathcal{H}$ -consistency bounds with respect to the top- $k$ loss. In Appendix H, we provide generalization bounds for the top- ${\\cdot k}$ loss in terms of finite samples (Theorems H.1 and H.2). ", "page_idx": 7}, {"type": "text", "text": "4.3 Theoretical guarantees for cardinality-aware surrogate losses ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The strong theoretical results of the previous sections establish the effectiveness of comp-sum and constrained losses as surrogate losses for the target top- $\\cdot k$ loss for common hypothesis sets used in practice. Building on this foundation, we expand our analysis to their cost-sensitive variants in the study of cardinality-aware set prediction in Section 2. We derive $\\mathcal{H}$ -consistency bounds for these loss functions, thereby also establishing their Bayes-consistency. To do so, we characterize the conditional regret of the target cardinality-aware loss function in Lemma I.1, which can be found in Appendix I. For this analysis, we will assume, without loss of generality, that the cost $c(x,k,y)$ takes values in $[0,1]$ for any $(x,k,y)\\in\\mathcal{X}\\times\\mathcal{K}\\times\\mathcal{Y}$ , which can be achieved by normalizing the cost function. ", "page_idx": 7}, {"type": "text", "text": "We will use $\\widetilde{\\ell}_{c-\\log}$ , $\\widetilde{\\ell}_{c-\\mathrm{exp}}$ , $\\widetilde{\\ell}_{c-\\mathrm{gce}}$ and $\\widetilde{\\ell}_{c-\\mathrm{mae}}$ to denote the corresponding cost-sensitive counterparts for $\\widetilde{\\ell}_{\\mathrm{log}}$ , $\\widetilde{\\ell}_{\\mathrm{exp}}$ , $\\widetilde{\\ell}_{\\mathrm{gce}}$ and $\\widetilde{\\ell}_{\\mathrm{mae}}$ , respectively. Next, we show that these cost-sensitive surrogate loss functions benefit from $\\mathcal{H}$ -consistency bounds with respect to the target loss $\\ell$ given in (1). ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.6. Assume that $\\mathcal{R}$ is symmetric and complete. Then, the following bound holds for the cost-sensitive comp-sum loss: for all $r\\in\\mathcal{R}$ and for any distribution, ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{\\ell}(r)-\\mathcal{E}_{\\ell}^{*}(\\mathcal{R})+\\mathcal{M}_{\\ell}(\\mathcal{R})\\leq\\gamma\\Big(\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{c-comp}}}(r)-\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{c-comp}}}^{*}(\\mathcal{R})+\\mathcal{M}_{\\widetilde{\\ell}_{\\mathrm{c-comp}}}(\\mathcal{R})\\Big);}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "When $\\mathcal{R}\\,=\\,\\mathcal{R}_{\\mathrm{all}},$ , the following holds: $\\begin{array}{r}{\\mathcal{E}_{\\ell}(r)-\\mathcal{E}_{\\ell}^{*}(\\mathcal{R}_{\\mathrm{all}})\\,\\leq\\,\\gamma\\Big(\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{c-comp}}}(r)-\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{c-comp}}}^{*}(\\mathcal{R}_{\\mathrm{all}})\\Big),}\\end{array}$ , where $\\gamma(t)\\,=\\,2\\sqrt{\\,t\\,}$ when $\\widetilde{\\ell}_{\\mathrm{c-comp}}$ is either \u2113\u0303c\u2212log or $\\widetilde{\\ell}_{\\mathrm{c-exp}}$ ; $\\gamma(t)\\,=\\,2\\sqrt{|\\mathcal{K}|^{q}t}$ when $\\widetilde{\\ell}_{\\mathrm{c-comp}}$ is $\\widetilde{\\ell}_{\\mathrm{c-gce}}$ ; and $\\gamma(t)=|\\mathcal{K}|t$ when \u2113\u0303c\u2212comp is \u2113\u0303c\u2212mae. ", "page_idx": 7}, {"type": "text", "text": "The proof is included in Appendix I.1. The second part follows from the fact that when $\\mathcal{R}=\\mathcal{R}_{\\mathrm{all}}$ , all the minimizability gaps vanish. In particular, Theorem 4.6 implies the Bayes-consistency of cost-sensitive comp-sum losses. The bounds for cost-sensitive generalized cross-entropy and mean absolute error loss depend on the number of set predictors, making them less favorable when $|\\mathcal{K}|$ is large. As pointed out earlier, while the cost-sensitive mean absolute error loss admits a linear rate, it is difficult to optimize even in the standard classification, as reported by Zhang and Sabuncu [2018]. ", "page_idx": 7}, {"type": "text", "text": "In the proof, we represented the comp-sum loss as a function of the softmax and introduced a softmax-dependent function $\\mathcal{S}_{\\mu}$ to upper bound the conditional regret of the target cardinality-aware loss function by that of the cost-sensitive comp-sum loss. This technique is novel and differs from the approach used in the standard scenario (Section 4.2). ", "page_idx": 7}, {"type": "text", "text": "We will use \u2113\u0303ccs\u2212tenxdp, \u2113\u0303c\u2212sq\u2212hinge, \u2113\u0303c\u2212hinge and \u2113\u0303c\u2212\u03c1 to denote the corresponding cost-sensitive counterparts for $\\widetilde{\\ell}_{\\mathrm{exp}_{.}}^{\\mathrm{cstnd}}$ , $\\widetilde{\\ell}_{\\mathrm{sq-hinge}}$ , $\\widetilde{\\ell}_{\\mathrm{hinge}}$ and $\\widetilde{\\ell}_{\\rho}$ , respectively. Next, we show that these cost-sensitive surrogate losses benefit from $\\mathcal{H}$ -consistency bounds with respect to the target loss $\\ell$ given in (1). ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.7. Assume that $\\mathcal{R}$ is symmetric and complete. Then, the following bound holds for the cost-sensitive constrained loss: for all $r\\in\\mathcal{R}$ and for any distribution, ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{\\ell}(r)-\\mathcal{E}_{\\ell}^{*}(\\mathcal{R})+\\mathcal{M}_{\\ell}(\\mathcal{R})\\leq\\gamma\\Big(\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{c-estnd}}}(r)-\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{c-estnd}}}^{*}(\\mathcal{R})+\\mathcal{M}_{\\widetilde{\\ell}_{\\mathrm{c-cstnd}}}(\\mathcal{R})\\Big);}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "When $\\mathcal{R}\\,=\\,\\mathcal{R}_{\\mathrm{all}},$ , the following holds: $\\begin{array}{r}{\\mathcal{E}_{\\ell}(r)-\\mathcal{E}_{\\ell}^{*}(\\mathcal{R}_{\\mathrm{all}})\\,\\leq\\,\\gamma\\big(\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{c-cstnd}}}(r)-\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{c-cstnd}}}^{*}(\\mathcal{R}_{\\mathrm{all}})\\big)\\,}\\end{array}$ , where $\\begin{array}{r}{\\gamma(t)=2\\sqrt{t}\\;w h e n\\;\\widetilde{\\ell}_{\\mathrm{c-cstnd}}\\;i s\\;\\widetilde{\\ell}_{\\mathrm{c-exp}}^{\\mathrm{-}}\\;o r\\;\\widetilde{\\ell}_{\\mathrm{c-sq-hinge}}^{\\mathrm{~\\,~\\sim~}};\\gamma(t)=t\\;w h e n\\;\\widetilde{\\ell}_{\\mathrm{c-cstnd}}\\;i s\\;\\widetilde{\\ell}_{\\mathrm{c-hinge}}^{\\mathrm{~\\,~\\sim~}}\\,o r\\;\\widetilde{\\ell}_{\\mathrm{c-p}}^{\\mathrm{~\\,~\\sim~}}.}\\end{array}$ ", "page_idx": 7}, {"type": "text", "text": "The proof is included in Appendix I.2. The second part follows from the fact that when $\\mathcal{R}=\\mathcal{R}_{\\mathrm{all}}$ , all the minimizability gaps vanish. In particular, Theorem 4.7 implies the Bayes-consistency of cost-sensitive constrained losses. Note that while the constrained hinge loss and $\\rho$ -margin loss have a more favorable linear rate in the bound, their optimization may be more challenging compared to other smooth loss functions. ", "page_idx": 7}, {"type": "image", "img_path": "WAT3qu737X/tmp/6e4748eaa2828ad58a31b5bfef537bbb5acf0ce3be61f921843a45c58f36a0f3.jpg", "img_caption": ["Figure 1: Accuracy versus average cardinality plots obtained by varying $\\lambda$ for our cardinality-aware algorithm and top- $k$ classifiers across four datasets, with predictor set $\\mathcal{K}=\\left\\lbrace1,2,4,8\\right\\rbrace$ and cardinality cost c $\\mathsf{o s t}(k)=\\log k$ . Our cardinality-aware algorithm consistently achieves higher accuracy for any fixed average cardinality across all datasets. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "WAT3qu737X/tmp/41f31917aaa5f2d10bfcd42f3db01979572998f593c4b053879ebaf4d5c7769e.jpg", "img_caption": ["Figure 2: Comparison of cardinality costs ${\\mathsf{c o s t}}(k)\\,=\\,\\log k$ and $\\mathsf{c o s t}(k)\\;=\\;k$ , with predictor set $\\mathcal{K}=\\{1,2,4,8\\}$ . The accuracy versus average cardinality plots for our cardinality-aware algorithm are similar, suggesting that the choice of cardinality cost has minimal impact on performance. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Here, we report empirical results for our cardinality-aware algorithm and show that it consistently outperforms top- $k$ classifiers on benchmark datasets CIFAR-10, CIFAR-100 [Krizhevsky, 2009], SVHN [Netzer et al., 2011] and ImageNet [Deng et al., 2009]. ", "page_idx": 8}, {"type": "text", "text": "We used the outputs of the second-to-last layer of ResNet [He et al., 2016] as features for the CIFAR10, CIFAR-100 and SVHN datasets. For the ImageNet dataset, we used the CLIP [Radford et al., 2021] model to extract features. We adopted a linear model, trained using multinomial logistic loss, for the classifier $h$ on the extracted features from the datasets. We used a two-hidden-layer feedforward neural network with ReLU activation functions [Nair and Hinton, 2010] for the cardinality selector $r$ . Both the classifier $h$ and the cardinality selector $r$ were trained using the Adam optimizer [Kingma and Ba, 2014], with a learning rate of $\\mathrm{i\\times10^{-3}}$ , a batch size of 128, and a weight decay of $\\bar{1}\\times10^{-5}$ . ", "page_idx": 8}, {"type": "text", "text": "Figure 1 compares the accuracy versus cardinality curves of the cardinality-aware algorithm with that of top- $k$ classifiers induced by $h$ for the various datasets. The accuracy of a top- $k$ classifier is measured by $\\mathbb{E}_{(x,y)\\sim S}[1-\\ell_{k}(h,x,y)]$ , that is the fraction of the sample in which the top- $k$ predictions include the true label. It naturally grows as the cardinality $k$ increases, as shown in Figure 1. The accuracy of the carnality-aware algorithms is measured by $\\mathbb{\\tilde{E}}_{(x,y)\\sim S}\\Big[1_{y\\in\\big\\{\\mathsf{h}_{1}(x),\\dots,\\mathsf{h}_{r(x)}(x)\\big\\}}\\Big]$ , that is the fraction of the sample in which the predictions selected by the model $r$ include the true label, and the corresponding cardinality is measured by $\\mathbb{E}_{(x,y)\\sim S}\\[\\mathsf{r}(x)]$ , that is the average size of the selected predictions. The cardinality selector $r$ was trained by minimizing the cost-sensitive logistic loss $\\widetilde{\\ell}_{\\mathrm{c-log}}$ with the cost $c(x,k,y)$ defined as $\\ell_{k}(h,x,y)+\\lambda\\log(k)$ and normalized to $[0,1]$ through division by its maximum value over $\\mathcal{X}\\times\\mathcal{K}\\times\\mathcal{Y}$ . We allow for top- $k$ experts with $\\bar{k^{\\circ}}\\bar{\\in\\mathcal{K}^{\\prime}}=\\{1,2,4,8\\}$ and vary $\\lambda$ . Starting from high values of $\\lambda$ , as $\\lambda$ decreases in Figure 1, our cardinality-aware algorithm yields solutions with higher average cardinality and increased accuracy. This is because $\\lambda$ controls the trade-off between cardinality and accuracy. The plots end to the right at $\\lambda=0.01$ . ", "page_idx": 8}, {"type": "text", "text": "Figure 1 shows that the cardinality-aware algorithm is superior across the CIFAR-100, ImageNet, CIFAR-10 and SVHN datasets. For a given average cardinality, the cardinality-aware algorithm always achieves higher accuracy than a top- $k$ classifier. In other words, to achieve the same level of accuracy, the predictions made by the cardinality-aware algorithm can be significantly smaller in size compared to those made by the corresponding top- $k$ classifier. In particular, on the CIFAR-100, CIFAR-10 and SVHN datasets, the cardinality-aware algorithm achieves the same accuracy $(98\\%)$ as the top- $k$ classifier while using roughly only half of the cardinality on average. As with the ImageNet dataset, it achieves the same accuracy $(95\\%)$ as the top- $k$ classifier with only two-thirds of the cardinality. This illustrates the effectiveness of our cardinality-aware algorithm. ", "page_idx": 8}, {"type": "text", "text": "Figure 2 presents the comparison of $\\mathsf{c o s t}(|\\mathbf{g}_{k}(x)|)=k$ and $\\mathsf{c o s t}(|\\mathbf{g}_{k}(x)|)\\!=\\!\\log k$ in the same setting (for each dataset, the orange curve in Figure 2 coincides with the orange curve in Figure 1). The comparison suggests that the choice between the linear and logarithmic cardinality costs has negligible impact on our algorithm\u2019s performance, highlighting its robustness in this regard. We also empirically demonstrate that our algorithm dynamically adjusts the cardinality of prediction sets based on input complexity, selecting larger sets for more challenging inputs to ensure high accuracy and smaller sets for simpler inputs to keep the cardinality low, as illustrated in Figure 3 and Figure 4. We present additional experimental results with different choices of set $\\mathcal{K}$ in Figure 5 and Figure 6 in Appendix J. Our cardinality-aware algorithm consistently outperforms top- $\\cdot k$ classifiers across all configurations. ", "page_idx": 8}, {"type": "image", "img_path": "WAT3qu737X/tmp/73452436f293b02d9f2cda970bcb158d8adb4772f4c3c74eb8989f22432f10cb.jpg", "img_caption": ["Figure 3: Cardinality distribution for top- $k$ experts with $\\mathcal{K}=\\left\\{1,2,4,8\\right\\}$ on CIFAR-10 and CIFAR100 datasets, analyzed under two $\\lambda$ values. For each dataset, increasing $\\lambda$ reduces the number of samples with the highest cardinality $(k=8)$ and increases those with lower cardinalities, as higher $\\lambda$ amplifies the influence of cardinality in the cost function. Across datasets, distributions vary for the same $\\lambda$ due to differing task complexities. "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "WAT3qu737X/tmp/386defa8fb1f1a633f576e0d166d4d74ab9c860db67c81873327cc3c119d6010.jpg", "img_caption": ["Figure 4: Illustration of hard and easy images on the CIFAR-10 dataset as judged by human annotators, for top- $k$ experts $\\mathcal{K}=\\left\\{1,2,4,8\\right\\}$ . Hard images are those correctly predicted by our algorithm with a cardinality of 8 but misclassified with a cardinality of 4. Easy images are correctly predicted with a cardinality of 1. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We introduced a new cardinality-aware set prediction framework for which we proposed two families of surrogate losses with strong $\\mathcal{H}$ -consistency guarantees: cost-sensitive comp-sum and constrained losses. This leads to principled and practical cardinality-aware algorithms for top- $k$ classification, which we showed empirically to be very effective. Additionally, we established a theoretical foundation for top- ${\\cdot k}$ classification with fixed cardinality $k$ by proving that several common surrogate loss functions, including comp-sum losses and constrained losses in standard classification, admit $\\mathcal{H}$ -consistency bounds with respect to the top- $k$ loss. This provides a theoretical justification for the use of these loss functions in top- $k$ classification and opens new avenues for further research in this area. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "P. Awasthi, N. Frank, A. Mao, M. Mohri, and Y. Zhong. Calibration and consistency of adversarial surrogate losses. In Advances in Neural Information Processing Systems, pages 9804\u20139815, 2021a.   \nP. Awasthi, A. Mao, M. Mohri, and Y. Zhong. A finer calibration analysis for adversarial robustness. arXiv preprint arXiv:2105.01550, 2021b.   \nP. Awasthi, A. Mao, M. Mohri, and Y. Zhong. $H$ -consistency bounds for surrogate loss minimizers. In International Conference on Machine Learning, pages 1117\u20131174, 2022a.   \nP. Awasthi, A. Mao, M. Mohri, and Y. Zhong. Multi-class $H$ -consistency bounds. In Advances in neural information processing systems, pages 782\u2013795, 2022b.   \nP. Awasthi, A. Mao, M. Mohri, and Y. Zhong. Theoretically grounded loss functions and algorithms for adversarial robustness. In International Conference on Artificial Intelligence and Statistics, pages 10077\u201310094, 2023a.   \nP. Awasthi, A. Mao, M. Mohri, and Y. Zhong. DC-programming for neural network optimizations. Journal of Global Optimization, 2023b.   \nP. L. Bartlett and M. H. Wegkamp. Classification with a reject option using a hinge loss. Journal of Machine Learning Research, 9(8), 2008.   \nP. L. Bartlett, M. I. Jordan, and J. D. McAuliffe. Convexity, classification, and risk bounds. Journal of the American Statistical Association, 101(473):138\u2013156, 2006.   \nJ. Berkson. Application of the logistic function to bio-assay. Journal of the American Statistical Association, 39:357\u2014-365, 1944.   \nJ. Berkson. Why I prefer logits to probits. Biometrics, 7(4):327\u2014-339, 1951.   \nL. Berrada, A. Zisserman, and M. P. Kumar. Smooth loss functions for deep top-k classification. In International Conference on Learning Representations, 2018.   \nK. Crammer and Y. Singer. On the algorithmic implementation of multiclass kernel-based vector machines. Journal of machine learning research, 2(Dec):265\u2013292, 2001.   \nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and F.-F. Li. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.   \nC. Denis and M. Hebiri. Confidence sets with expected sizes for multiclass classification. Journal of Machine Learning Research, 18(102):1\u201328, 2017.   \nA. Ghosh, H. Kumar, and P. S. Sastry. Robust loss functions under label noise for deep neural networks. In Proceedings of the AAAI conference on artificial intelligence, 2017.   \nK. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \nD. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \nA. Krizhevsky. Learning multiple layers of features from tiny images. Technical report, Toronto University, 2009.   \nV. Kuznetsov, M. Mohri, and U. Syed. Multi-class deep boosting. In Advances in Neural Information Processing Systems, pages 2501\u20132509, 2014.   \nM. Lapin, M. Hein, and B. Schiele. Top- $\\mathbf{\\nabla}\\cdot\\mathbf{k}$ multiclass SVM. In Advances in neural information processing systems, 2015.   \nM. Lapin, M. Hein, and B. Schiele. Loss functions for top-k error: Analysis and insights. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1468\u20131477, 2016.   \nM. Lapin, M. Hein, and B. Schiele. Analysis and optimization of loss functions for multiclass, top-k, and multilabel classification. IEEE Transactions on Pattern Analysis & Machine Intelligence, 40 (07):1533\u20131554, 2018.   \nY. Lee, Y. Lin, and G. Wahba. Multicategory support vector machines: Theory and application to the classification of microarray data and satellite radiance data. Journal of the American Statistical Association, 99(465):67\u201381, 2004.   \nP. Long and R. Servedio. Consistency versus realizable H-consistency for multiclass classification. In International Conference on Machine Learning, pages 801\u2013809, 2013.   \nA. Mao, C. Mohri, M. Mohri, and Y. Zhong. Two-stage learning to defer with multiple experts. In Advances in neural information processing systems, 2023a.   \nA. Mao, M. Mohri, and Y. Zhong. H-consistency bounds: Characterization and extensions. In Advances in Neural Information Processing Systems, 2023b.   \nA. Mao, M. Mohri, and Y. Zhong. H-consistency bounds for pairwise misranking loss surrogates. In International conference on Machine learning, 2023c.   \nA. Mao, M. Mohri, and Y. Zhong. Ranking with abstention. In ICML 2023 Workshop The Many Facets of Preference-Based Learning, 2023d.   \nA. Mao, M. Mohri, and Y. Zhong. Structured prediction with stronger consistency guarantees. In Advances in Neural Information Processing Systems, 2023e.   \nA. Mao, M. Mohri, and Y. Zhong. Cross-entropy loss functions: Theoretical analysis and applications. In International Conference on Machine Learning, 2023f.   \nA. Mao, M. Mohri, and Y. Zhong. Principled approaches for learning to defer with multiple experts. In International Symposium on Artificial Intelligence and Mathematics, 2024a.   \nA. Mao, M. Mohri, and Y. Zhong. Predictor-rejector multi-class abstention: Theoretical analysis and algorithms. In International Conference on Algorithmic Learning Theory, 2024b.   \nA. Mao, M. Mohri, and Y. Zhong. Theoretically grounded loss functions and algorithms for scorebased multi-class abstention. In International Conference on Artificial Intelligence and Statistics, 2024c.   \nA. Mao, M. Mohri, and Y. Zhong. Enhanced $H$ -consistency bounds. arXiv preprint arXiv:2407.13722, 2024d.   \nA. Mao, M. Mohri, and Y. Zhong. $H$ -consistency guarantees for regression. In International Conference on Machine Learning, pages 34712\u201334737, 2024e.   \nA. Mao, M. Mohri, and Y. Zhong. Multi-label learning with stronger consistency guarantees. In Advances in neural information processing systems, 2024f.   \nA. Mao, M. Mohri, and Y. Zhong. Realizable $H$ -consistent and Bayes-consistent loss functions for learning to defer. In Advances in neural information processing systems, $2024\\mathrm{g}$ .   \nA. Mao, M. Mohri, and Y. Zhong. Regression with multi-expert deferral. In International Conference on Machine Learning, pages 34738\u201334759, 2024h.   \nA. Mao, M. Mohri, and Y. Zhong. A universal growth rate for learning with smooth surrogate losses. In Advances in neural information processing systems, 2024i.   \nC. Mohri, D. Andor, E. Choi, M. Collins, A. Mao, and Y. Zhong. Learning to reject with a fixed predictor: Application to decontextualization. In International Conference on Learning Representations, 2024.   \nM. Mohri, A. Rostamizadeh, and A. Talwalkar. Foundations of machine learning. MIT press, 2018.   \nI. Mukherjee and R. E. Schapire. A theory of multiclass boosting. Journal of Machine Learning Research, 2013.   \nV. Nair and G. E. Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th international conference on machine learning (ICML-10), pages 807\u2013814, 2010.   \nY. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in natural images with unsupervised feature learning. In Advances in Neural Information Processing Systems, 2011.   \nA. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \nS. J. Reddi, S. Kale, F. Yu, D. Holtmann-Rice, J. Chen, and S. Kumar. Stochastic negative mining for learning with large output spaces. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 1940\u20131949, 2019.   \nM. Saberian and N. Vasconcelos. Multiclass boosting: Theory and algorithms. Advances in neural information processing systems, 24, 2011.   \nG. Shafer and V. Vovk. A tutorial on conformal prediction. Journal of Machine Learning Research, 9 (3), 2008.   \nI. Steinwart. How to compare different loss functions and their risks. Constructive Approximation, 26(2):225\u2013287, 2007.   \nA. Thilagar, R. Frongillo, J. J. Finocchiaro, and E. Goodwill. Consistent polyhedral surrogates for top-k classification and variants. In International Conference on Machine Learning, pages 21329\u201321359, 2022.   \nN. Usunier, D. Buffoni, and P. Gallinari. Ranking with ordered weighted pairwise classification. In International conference on machine learning, pages 1057\u20131064, 2009.   \nP. F. Verhulst. Notice sur la loi que la population suit dans son accroissement. Correspondance math\u00e9matique et physique, 10:113\u2014-121, 1838.   \nP. F. Verhulst. Recherches math\u00e9matiques sur la loi d\u2019accroissement de la population. Nouveaux M\u00e9moires de l\u2019Acad\u00e9mie Royale des Sciences et Belles-Lettres de Bruxelles, 18:1\u2014-42, 1845.   \nJ. Weston and C. Watkins. Multi-class support vector machines. Technical report, Citeseer, 1998.   \nF. Yang and S. Koyejo. On the consistency of top-k surrogate losses. In International Conference on Machine Learning, pages 10727\u201310735, 2020.   \nT. Zhang. Statistical behavior and consistency of classification methods based on convex risk minimization. The Annals of Statistics, 32(1):56\u201385, 2004a.   \nT. Zhang. Statistical analysis of some multi-category large margin classification methods. Journal of Machine Learning Research, 5(Oct):1225\u20131251, 2004b.   \nZ. Zhang and M. Sabuncu. Generalized cross entropy loss for training deep neural networks with noisy labels. In Advances in neural information processing systems, 2018.   \nC. Zheng, G. Wu, F. Bao, Y. Cao, C. Li, and J. Zhu. Revisiting discriminative vs. generative classifiers: Theory and implications. In International Conference on Machine Learning, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Contents of Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Proof of Lemma 4.4 15   \nB Proofs of $\\mathcal{H}$ -consistency bounds for comp-sum losses 15   \nC Minimizability gaps and realizability 20   \nD Proofs of realizable $\\mathcal{H}$ -consistency for comp-sum losses 20   \nE $\\mathcal{H}$ -Consistency bounds for constrained losses 21   \nF Proofs of $\\mathcal{H}$ -consistency bounds for constrained losses 22   \nG Technical challenges and novelty in Section 4.2 26   \nH Generalization bounds 27   \nProofs of $\\mathcal{H}$ -consistency bounds for cost-sensitive losses 29   \nI.1 Proof of Theorem 4.6 . . 29   \nI.2 Proof of Theorem 4.7 . 33   \nJ Additional experimental results: top- $k$ classifiers 37   \nK Additional experimental results: threshold-based classifiers 38   \nL Future work 39 ", "page_idx": 13}, {"type": "text", "text": "A Proof of Lemma 4.4 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Lemma 4.4. Assume that $\\mathcal{H}$ is regular. Then, for any $h\\in\\mathcal{H}$ and $x\\in\\mathcal X$ , the best-in-class conditional error and the conditional regret of the top- $k$ loss can be expressed as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathfrak{C}_{\\ell_{k}}^{*}(\\mathcal{H},x)=1-\\sum_{i=1}^{k}p(x,\\mathfrak{p}_{i}(x))\\quad\\Delta\\mathfrak{C}_{\\ell_{k},\\mathcal{H}}(h,x)=\\sum_{i=1}^{k}[p(x,\\mathfrak{p}_{i}(x))-p(x,\\mathfrak{h}_{i}(x))].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. By definition, for any $h\\in\\mathcal{H}$ and $x\\in\\mathcal X$ , the conditional error of top- ${\\cdot k}$ loss can be written as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathfrak{C_{\\ell_{k}}}\\big(h,x\\big)=\\sum_{y\\in\\mathfrak{Y}}p(x,y)1_{y\\in\\{\\mathfrak{h}_{1}(x),\\ldots,\\mathfrak{h}_{k}(x)\\}}=1-\\sum_{i=1}^{k}p\\big(x,\\mathfrak{h}_{i}(x)\\big).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By definition of the labels ${\\mathsf p}_{i}(x)$ , which are the most likely top- $k$ labels, $\\mathcal{C}_{\\ell_{k}}(h,x)$ is minimized for $\\mathsf{h}_{i}^{\\prime}(x)=k_{\\operatorname*{min}}(x),\\,i\\in[k]$ . Since $\\mathcal{H}$ is regular, this choice is realizable for some $h\\in\\mathcal{H}$ . Thus, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{\\ C}_{\\ell_{k}}^{*}\\big(\\mathcal{H},x\\big)=\\operatorname*{inf}_{h\\in\\mathcal{H}}\\mathcal{\\ C}_{\\ell_{k}}\\big(h,x\\big)=1-\\sum_{i=1}^{k}p\\big(x,\\mathfrak{p}_{i}\\big(x)\\big).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Furthermore, the calibration gap can be expressed as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Delta\\mathcal{C}_{\\ell_{k},\\mathcal{K}}(h,x)=\\mathcal{C}_{\\ell_{k}}(h,x)-\\mathcal{C}_{\\ell_{k}}^{*}(\\mathcal{K},x)=\\sum_{i=1}^{k}\\bigl(p\\bigl(x,\\mathsf{p}_{i}(x)\\bigr)-p\\bigl(x,\\mathsf{h}_{i}(x)\\bigr)\\bigr),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which completes the proof. ", "page_idx": 14}, {"type": "text", "text": "B Proofs of $\\mathcal{H}$ -consistency bounds for comp-sum losses ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Theorem 4.5. Assume that $\\mathcal{H}$ is symmetric and complete. Then, for any $1\\leq k\\leq n$ , the following $\\mathcal{H}$ -consistency bound holds for the comp-sum loss: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{\\ell_{k}}(h)-\\mathcal{E}_{\\ell_{k}}^{*}(\\mathcal{H})+\\mathcal{M}_{\\ell_{k}}(\\mathcal{H})\\leq k\\psi^{-1}\\Big(\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{comp}}}(h)-\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{comp}}}^{*}(\\mathcal{H})+\\mathcal{M}_{\\widetilde{\\ell}_{\\mathrm{comp}}}(\\mathcal{H})\\Big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In the special case where $\\mathcal{A}_{\\widetilde{\\ell}_{\\mathrm{comp}}}(\\mathcal{H})=0,$ , for any $1\\leq k\\leq n$ , the following upper bound holds: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathcal{E}_{\\ell_{k}}(h)-\\mathcal{E}_{\\ell_{k}}^{*}(\\mathcal{K})\\le k\\psi^{-1}\\Big(\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{comp}}}(h)-\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{comp}}}^{*}(\\mathcal{K})\\Big),~~~~~~~~~~~~~~~~~~~~}\\\\ &{}&{\\quad\\varrho\\,\\psi(t)=\\frac{1-t}{2}\\log(1-t)+\\frac{1+t}{2}\\log(1+t),t\\in[0,1]\\,w h e n\\,\\widetilde{\\ell}_{\\mathrm{comp}}\\,i s\\,\\widetilde{\\ell}_{\\log};\\psi(t)=1\\!-\\!\\sqrt{1-t^{2}},t\\in[0,1]}\\\\ &{}&{\\quad\\widetilde{\\ell}_{\\mathrm{comp}}\\,i s\\,\\widetilde{\\ell}_{\\mathrm{exp}};\\,\\psi(t)=t/n\\,\\,w h e n\\,\\widetilde{\\ell}_{\\mathrm{comp}}\\,i s\\,\\widetilde{\\ell}_{\\mathrm{mac}};\\,a n d\\,\\psi(t)=\\frac{1}{q n^{q}}\\left[\\left[\\frac{(1+t)^{\\frac{1}{1-q}}+(1-t)^{\\frac{1}{1-q}}}{2}\\right]^{1-q}-1\\right],}\\\\ &{}&{\\quad l\\,q\\in(0,1),t\\in[0,1]\\,w h e n\\,\\widetilde{\\ell}_{\\mathrm{comp}}\\,i s\\,\\widetilde{\\ell}_{\\mathrm{gce}}.~~~~~~~~~~~~~~~~}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. Case I: $\\widetilde{\\ell}_{\\mathrm{comp}}=\\widetilde{\\ell}_{\\mathrm{log}}$ . For logistic loss $\\widetilde{\\ell_{\\mathrm{log}}}$ , the conditional regret can be written as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\Delta\\mathbb{\\mathcal{C}}_{\\widetilde{\\ell}_{\\log},\\mathcal{K}}(h,x)=\\sum_{y=1}^{n}p(x,y)\\widetilde{\\ell}_{\\log}(h,x,y)-\\operatorname*{inf}_{h\\in\\mathcal{H}_{y=1}}\\sum_{p=1}^{n}p(x,y)\\widetilde{\\ell}_{\\log}(h,x,y)}}\\\\ &{\\qquad\\qquad\\qquad\\geq\\displaystyle\\sum_{y=1}^{n}p(x,y)\\widetilde{\\ell}_{\\log}(h,x,y)-\\operatorname*{inf}_{\\mu\\in\\mathbb{R}}\\sum_{y=1}^{n}p(x,y)\\widetilde{\\ell}_{\\log}(h_{\\mu,i},x,y),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where for any $i\\in[k]$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\nh_{\\mu,i}(x,y)=\\left\\{\\!\\!\\begin{array}{l l}{h(x,y),}&{y\\notin\\{\\mathfrak{p}_{i}(x),\\mathsf{h}_{i}(x)\\}}\\\\ {\\log\\!\\left(e^{h(x,\\mathsf{p}_{i}(x))}+\\mu\\right)}&{y=\\mathsf{h}_{i}(x)}\\\\ {\\log\\!\\left(e^{h(x,\\mathsf{h}_{i}(x))}-\\mu\\right)}&{y=\\mathsf{p}_{i}(x).}\\end{array}\\ \\right.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Note that such a choice of $h_{\\mu,i}$ leads to the following equality holds: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sum_{y\\notin\\{\\mathfrak{h}_{i}(x),\\mathfrak{p}_{i}(x)\\}}p(x,y)\\widetilde{\\ell}_{\\log}(h,x,y)=\\sum_{y\\notin\\{\\mathfrak{h}_{i}(x),\\mathfrak{p}_{i}(x)\\}}p(x,y)\\widetilde{\\ell}_{\\log}(h_{\\mu,i},x,y).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore, for any $i\\in[k]$ , the conditional regret of logistic loss can be lower bounded as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta\\mathcal{C}_{\\widetilde{\\ell}_{\\log},\\mathcal{K}}(h,x)\\geq-p(x,\\mathfrak{h}_{i}(x))\\log\\biggr(\\frac{e^{h(x,\\mathfrak{h}_{i}(x))}}{\\sum_{y\\in\\mathfrak{H}}e^{h(x,y)}}\\biggr)-p(x,\\mathfrak{p}_{i}(x))\\log\\biggr(\\frac{e^{h(x,\\mathfrak{p}_{i}(x))}}{\\sum_{y\\in\\mathfrak{H}}e^{h(x,y)}}\\biggr)}\\\\ &{\\qquad\\qquad\\qquad+\\operatorname*{sup}_{\\mu\\in\\mathbb{R}}\\biggr(p(x,\\mathfrak{h}_{i}(x))\\log\\biggr(\\frac{e^{h(x,\\mathfrak{p}_{i}(x))}+\\mu}{\\sum_{y\\in\\mathfrak{H}}e^{h(x,y)}}\\biggr)+p(x,\\mathfrak{p}_{i}(x))\\log\\biggr(\\frac{e^{h(x,\\mathfrak{h}_{i}(x))}-\\mu}{\\sum_{y\\in\\mathfrak{H}}e^{h(x,y)}}\\biggr)\\biggr)}\\\\ &{\\qquad\\qquad=\\operatorname*{sup}_{\\mu\\in\\mathbb{R}}\\biggr(p(x,\\mathfrak{h}_{i}(x))\\log\\biggr(\\frac{e^{h(x,\\mathfrak{p}_{i}(x))}+\\mu}{e^{h(x,\\mathfrak{h}_{i}(x))}}\\biggr)+p(x,\\mathfrak{p}_{i}(x))\\log\\biggr(\\frac{e^{h(x,\\mathfrak{h}_{i}(x))}-\\mu}{e^{h(x,\\mathfrak{p}_{i}(x))}}\\biggr)\\biggr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By the concavity of the function, differentiate with respect to $\\mu$ , we obtain that the supremum is achieved by \u00b5\u2217= p(x,hi(x))eh(x,hi(x))\u2212p(x,pi(x))eh(x,pi(x)). Plug in $\\mu^{*}$ , we obtain ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{~\\gamma_\\mathrm{rog},\\gamma_n^\\tau~\\sim~}\\quad\\mathrm{~\\gamma~}}\\\\ &{\\geq p(x,\\mathsf{h}_{i}(x))\\log\\Biggl(\\frac{p(x,\\mathsf{h}_{i}(x))}{p\\left(x,\\mathsf{h}_{i}(x)\\right)+p\\left(x,\\mathsf{p}_{i}(x)\\right)}\\frac{e^{h(x,\\mathsf{h}_{i}(x))}+e^{h(x,\\mathsf{p}_{i}(x))}}{e^{h(x,\\mathsf{h}_{i}(x))}}\\Biggr)}\\\\ &{\\qquad+\\,p(x,\\mathsf{p}_{i}(x))\\log\\Biggl(\\frac{p(x,\\mathsf{p}_{i}(x))}{p\\left(x,\\mathsf{h}_{i}(x)\\right)+p\\left(x,\\mathsf{p}_{i}(x)\\right)}\\frac{e^{h(x,\\mathsf{h}_{i}(x))}+e^{h(x,\\mathsf{p}_{i}(x))}}{e^{h(x,\\mathsf{p}_{i}(x))}}\\Biggr)}\\\\ &{\\geq p(x,\\mathsf{h}_{i}(x))\\log\\Biggl(\\frac{2p(x,\\mathsf{h}_{i}(x))}{p\\left(x,\\mathsf{h}_{i}(x)\\right)+p\\left(x,\\mathsf{p}_{i}(x)\\right)}\\Biggr)+p(x,\\mathsf{p}_{i}(x))\\log\\Biggl(\\frac{2p(x,\\mathsf{p}_{i}(x))}{p\\left(x,\\mathsf{h}_{i}(x)\\right)+p\\left(x,\\mathsf{p}_{i}(x)\\right)}\\Biggr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "let $S_{i}=p(x,{\\mathsf p}_{i}(x))+p(x,{\\mathsf h}_{i}(x))$ and $\\Delta_{i}=p(x,{\\mathsf{p}}_{i}(x))-p(x,{\\mathsf{h}}_{i}(x))$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta\\mathcal{C}_{\\widetilde{\\ell}_{\\log},\\mathcal{H}}(h,x)\\ge\\frac{S_{i}-\\Delta_{i}}{2}\\log(\\frac{S_{i}-\\Delta_{i}}{S_{i}})+\\frac{S_{i}+\\Delta_{i}}{2}\\log(\\frac{S_{i}+\\Delta_{i}}{S_{i}})}\\\\ &{\\phantom{\\Delta\\mathcal{C}_{\\widetilde{\\ell}_{\\log},\\mathcal{H}}(h,x)\\ge\\frac{1-\\Delta_{i}}{2}\\log(1-\\Delta_{i})+\\frac{1+\\Delta_{i}}{2}\\log(1+\\Delta_{i})\\ \\mathrm{~(minimum~is~achieved~when~}S_{i}=1)}}\\\\ &{\\phantom{\\Delta\\mathcal{C}_{\\widetilde{\\ell}_{\\log},\\mathcal{H}}(h,x)\\ge\\psi(p(x,\\mathsf{p}_{i}(x))-p(x,\\mathsf{h}_{i}(x))),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\begin{array}{r}{\\psi(t)=\\frac{1-t}{2}\\log(1-t)+\\frac{1+t}{2}\\log(1+t),t\\in[0,1]}\\end{array}$ . Therefore, the conditional regret of the top- $k$ loss can be upper bounded as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\Delta\\mathcal{C}_{\\ell_{k},\\mathcal{K}}(h,x)=\\sum_{i=1}^{k}(p(x,\\mathfrak{p}_{i}(x))-p(x,\\mathfrak{h}_{i}(x)))\\leq k\\psi^{-1}\\Big(\\Delta\\mathcal{C}_{\\widetilde{\\ell}_{\\log},\\mathcal{K}}(h,x)\\Big).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By the concavity of $\\psi^{-1}$ , taking expectations on both sides of the preceding equation, we obtain ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{\\ell_{k}}\\left(h\\right)-\\mathcal{E}_{\\ell_{k}}^{*}(\\mathcal{H})+\\mathcal{M}_{\\ell_{k}}(\\mathcal{H})\\leq k\\psi^{-1}\\Big(\\mathcal{E}_{\\widetilde{\\ell}_{\\log}}(h)-\\mathcal{E}_{\\widetilde{\\ell}_{\\log}}^{*}(\\mathcal{H})+\\mathcal{M}_{\\widetilde{\\ell}_{\\log}}(\\mathcal{H})\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The second part follows from the fact that when $\\mathcal{A}_{\\widetilde{\\ell}_{\\mathrm{log}}}(\\mathcal{H})=0$ , the minimizability gap $\\Re_{\\widetilde{\\ell}_{\\mathrm{log}}}(\\mathcal{H})$ vanishes. ", "page_idx": 15}, {"type": "text", "text": "Case II: $\\widetilde{\\ell}_{\\mathrm{comp}}=\\widetilde{\\ell}_{\\mathrm{exp}}$ . For sum exponential loss $\\widetilde{\\ell}_{\\mathrm{exp}}$ , the conditional regret can be written as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\Delta\\mathbb{\\mathcal{C}}_{\\widetilde{\\ell}_{\\mathrm{exp}},\\mathcal{H}}(h,x)=\\sum_{y=1}^{n}p(x,y)\\widetilde{\\ell}_{\\mathrm{exp}}(h,x,y)-\\operatorname*{inf}_{h\\in\\mathcal{H}}\\sum_{y=1}^{n}p(x,y)\\widetilde{\\ell}_{\\mathrm{exp}}(h,x,y)}}\\\\ &{}&{\\ge\\sum_{y=1}^{n}p(x,y)\\widetilde{\\ell}_{\\mathrm{exp}}(h,x,y)-\\displaystyle\\operatorname*{inf}_{\\mu\\in\\mathbb{R}}\\sum_{y=1}^{n}p(x,y)\\widetilde{\\ell}_{\\mathrm{exp}}(h_{\\mu,i},x,y),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where for any $i\\in[k]$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\nh_{\\mu,i}(x,y)=\\left\\{\\!\\!\\begin{array}{l l}{h(x,y),}&{y\\notin\\{\\mathfrak{p}_{i}(x),\\mathsf{h}_{i}(x)\\}}\\\\ {\\log\\!\\left(e^{h(x,\\mathsf{p}_{i}(x))}+\\mu\\right)}&{y=\\mathsf{h}_{i}(x)}\\\\ {\\log\\!\\left(e^{h(x,\\mathsf{h}_{i}(x))}-\\mu\\right)}&{y=\\mathsf{p}_{i}(x).}\\end{array}\\ \\right.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note that such a choice of $h_{\\mu,i}$ leads to the following equality holds: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{y\\notin\\{\\mathfrak{h}_{i}(x),\\mathfrak{p}_{i}(x)\\}}p(x,y)\\widetilde{\\ell}_{\\mathrm{exp}}(h,x,y)=\\sum_{y\\notin\\{\\mathfrak{h}_{i}(x),\\mathfrak{p}_{i}(x)\\}}p(x,y)\\widetilde{\\ell}_{\\mathrm{exp}}(h_{\\mu,i},x,y).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, for any $i\\in[k]$ , the conditional regret of sum exponential loss can be lower bounded as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\Delta}\\mathrm{p}_{\\mathcal{\\widetilde{E}}_{\\mathrm{exp}},\\mathcal{K}}(h,x)\\geq\\displaystyle\\sum_{y^{\\prime}\\in\\mathfrak{H}}\\exp(h(x,y^{\\prime}))\\Bigg[\\frac{p(x,\\mathfrak{h}_{i}(x))}{\\exp(h(x,\\mathfrak{h}_{i}(x)))}+\\frac{p(x,\\mathfrak{p}_{i}(x))}{\\exp(h(x,\\mathfrak{p}_{i}(x)))}\\Bigg]}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\displaystyle\\operatorname*{sup}_{\\mu\\in\\mathbb{R}}\\Bigg(-\\sum_{y^{\\prime}\\in\\mathfrak{H}}\\exp(h(x,y^{\\prime}))\\Bigg[\\frac{p(x,\\mathfrak{h}_{i}(x))}{\\exp(h(x,\\mathfrak{p}_{i}(x)))+\\mu}+\\frac{p(x,\\mathfrak{p}_{i}(x))}{\\exp(h(x,\\mathfrak{h}_{i}(x)))-\\mu}\\Bigg]\\Bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By the concavity of the function, differentiate with respect to $\\mu$ , we obtain that the supremum is achieved by $\\begin{array}{r}{\\mu^{*}=\\frac{\\exp[h(x,\\mathbf{h}_{i}(x))]\\sqrt{p(x,\\mathbf{h}_{i}(x))}-\\exp[h(x,\\mathbf{p}_{i}(x))]\\sqrt{p(x,\\mathbf{p}_{i}(x))}}{\\sqrt{p(x,\\mathbf{h}_{i}(x))}+\\sqrt{p(x,\\mathbf{p}_{i}(x))}}}\\end{array}$ . Plug in $\\mu^{*}$ , we obtain ", "page_idx": 16}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sum_{t\\in\\mathcal{W}}\\rho_{t},}\\\\ &{\\geq\\sum_{t\\in\\mathcal{W}}(h(x,y_{t}^{\\prime}))}\\\\ &{\\qquad\\left[\\frac{p\\left(x,\\mathfrak{h}_{t}(x)\\right)}{\\exp\\left(h(x,\\mathfrak{h}_{t}(x))\\right)}+\\frac{p\\left(x,\\mathfrak{h}_{t}(x)\\right)}{\\exp\\left(h(x,\\mathfrak{h}_{t}(x))\\right)}-\\frac{\\left(\\sqrt{p(x,\\mathfrak{h}_{t}(x))}+\\sqrt{p(x,\\mathfrak{h}_{t}(x))}\\right)^{2}}{\\exp\\left(h(x,\\mathfrak{p}_{t}(x))\\right)+\\exp\\left(h(x,\\mathfrak{h}_{t}(x))\\right))}\\right]}\\\\ &{\\geq\\bigg[1+\\frac{\\exp\\left(h(x,\\mathfrak{p}_{t}(x))\\right)}{\\exp\\left(h(x,\\mathfrak{h}_{t}(x))\\right)}\\bigg]p(x,\\mathfrak{h}_{t}(x))}\\\\ &{\\qquad+\\left[1+\\frac{\\exp\\left(h(x,\\mathfrak{h}_{t}(x))\\right)}{\\exp\\left(h(x,\\mathfrak{p}_{t}(x))\\right)}\\bigg]p(x,\\mathfrak{p}_{t}(x))-\\left(\\sqrt{p(x,\\mathfrak{h}_{t}(x))}+\\sqrt{p(x,\\mathfrak{p}_{t}(x))}\\right)^{2}}\\\\ &{\\qquad+\\left[\\frac{\\sum_{y\\in\\mathcal{W}}\\left(h(x,\\mathfrak{h}_{t}(x))\\right)}{\\exp\\left(h(x,\\mathfrak{p}_{t}(x))\\right)}\\left(\\sum_{y\\in\\mathcal{W}}\\left(h(x,y)\\right)\\geq\\exp\\left(h(x,\\mathfrak{p}_{t}(x))\\right)\\right)+\\exp(h(x,\\mathfrak{h}_{t}(x))))}\\\\ &{\\geq2p(x,\\mathfrak{h}_{t}(x))+2p(x,\\mathfrak{p}_{t}(x))-\\left(\\sqrt{p(x,\\mathfrak{h}_{t}(x))}+\\sqrt{p(x,\\mathfrak{p}_{t}(x))}\\right)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "(minimum is attained when $\\begin{array}{r}{\\frac{\\exp(h(x,\\mathsf{p}_{i}(x)))}{\\exp(h(x,\\mathsf{h}_{i}(x)))}=1)}\\end{array}$ ", "page_idx": 16}, {"type": "text", "text": "let $S_{i}=p(x,{\\mathsf p}_{i}(x))+p(x,{\\mathsf h}_{i}(x))$ and $\\Delta_{i}=p(x,{\\mathsf{p}}_{i}(x))-p(x,{\\mathsf{h}}_{i}(x))$ , we have ", "page_idx": 16}, {"type": "text", "text": "(minimum is achieved when $S_{i}=1$ ) ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta\\mathcal{C}_{\\widetilde{\\ell}_{\\mathrm{exp}},\\mathcal{K}}(h,x)\\ge2S_{i}-\\left(\\sqrt{\\frac{S_{i}+\\Delta_{i}}{2}}+\\sqrt{\\frac{S_{i}-\\Delta_{i}}{2}}\\right)^{2}}\\\\ &{\\qquad\\qquad\\qquad\\ge2\\left[1-\\left[\\frac{\\left(1+\\Delta_{i}\\right)^{\\frac{1}{2}}+\\left(1-\\Delta_{i}\\right)^{\\frac{1}{2}}}{2}\\right]^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad=1-\\sqrt{1-(\\Delta_{i})^{2}}}\\\\ &{\\qquad\\qquad\\qquad=\\psi\\big(p(x,p_{i}(x))-p(x,h_{i}(x))\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\psi(t)=1-\\sqrt{1-t^{2}},\\,t\\in[0,1]$ . Therefore, the conditional regret of the top- $k$ loss can be upper bounded as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Delta\\mathcal{C}_{\\ell_{k},\\mathcal{K}}(h,x)=\\sum_{i=1}^{k}(p(x,\\mathfrak{p}_{i}(x))-p(x,\\mathfrak{h}_{i}(x)))\\leq k\\psi^{-1}\\Big(\\Delta\\mathcal{C}_{\\widetilde{\\ell}_{\\mathrm{exp}},\\mathcal{K}}(h,x)\\Big).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By the concavity of $\\psi^{-1}$ , taking expectations on both sides of the preceding equation, we obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{\\ell_{k}}\\left(h\\right)-\\mathcal{E}_{\\ell_{k}}^{*}(\\mathcal{H})+\\mathcal{M}_{\\ell_{k}}(\\mathcal{H})\\leq k\\psi^{-1}\\Big(\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{exp}}}(h)-\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{exp}}}^{*}(\\mathcal{H})+\\mathcal{M}_{\\widetilde{\\ell}_{\\mathrm{exp}}}(\\mathcal{H})\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The second part follows from the fact that when $\\mathcal{A}_{\\widetilde{\\ell}_{\\mathrm{exp}}}(\\mathcal{H})=0$ , the minimizability gap $\\mathcal{N}_{\\widetilde{\\ell}_{\\mathrm{exp}}}$ (H) vanishes. ", "page_idx": 16}, {"type": "text", "text": "Case III: $\\widetilde{\\ell}_{\\mathrm{comp}}=\\widetilde{\\ell}_{\\mathrm{mae}}$ . For mean absolute error loss $\\widetilde{\\ell}_{\\mathrm{mae}}$ , the conditional regret can be written as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\Delta\\mathcal{C}_{\\widetilde{\\ell}_{\\mathrm{mae}},\\mathcal{K}}(h,x)=\\sum_{y=1}^{n}p(x,y)\\widetilde{\\ell}_{\\mathrm{mae}}(h,x,y)-\\operatorname*{inf}_{h\\in\\mathcal{H}_{\\Psi}}\\sum_{y=1}^{n}p(x,y)\\widetilde{\\ell}_{\\mathrm{mae}}(h,x,y)}\\\\ &{\\qquad\\qquad\\qquad\\geq\\displaystyle\\sum_{y=1}^{n}p(x,y)\\widetilde{\\ell}_{\\mathrm{mae}}(h,x,y)-\\operatorname*{inf}_{\\mu\\in\\mathbb{R}}\\sum_{y=1}^{n}p(x,y)\\widetilde{\\ell}_{\\mathrm{mae}}(h_{\\mu,i},x,y),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where for any $i\\in[k]$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\nh_{\\mu,i}(x,y)=\\left\\{\\!\\!\\begin{array}{l l}{h(x,y),}&{y\\notin\\{\\mathfrak{p}_{i}(x),\\mathsf{h}_{i}(x)\\}}\\\\ {\\log\\!\\left(e^{h(x,\\mathsf{p}_{i}(x))}+\\mu\\right)}&{y=\\mathsf{h}_{i}(x)}\\\\ {\\log\\!\\left(e^{h(x,\\mathsf{h}_{i}(x))}-\\mu\\right)}&{y=\\mathsf{p}_{i}(x).}\\end{array}\\ \\right.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note that such a choice of $h_{\\mu,i}$ leads to the following equality holds: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{y\\notin\\{\\mathfrak{h}_{i}(x),\\mathfrak{p}_{i}(x)\\}}p(x,y)\\widetilde{\\ell}_{\\mathrm{mae}}(h,x,y)=\\sum_{y\\notin\\{\\mathfrak{h}_{i}(x),\\mathfrak{p}_{i}(x)\\}}p(x,y)\\widetilde{\\ell}_{\\mathrm{mae}}(h_{\\mu,i},x,y).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, for any $i\\in[k]$ , the conditional regret of mean absolute error loss can be lower bounded as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta\\mathcal{C}_{\\widetilde{\\ell}_{\\operatorname*{max}},\\mathcal{R}}(h,x)}\\\\ &{\\geq p(x,\\mathsf{h}_{i}(x))\\Bigg(1-\\frac{\\exp(h(x,\\mathsf{h}_{i}(x)))}{\\sum_{y^{\\prime}\\in\\mathfrak{y}}\\exp(h(x,y^{\\prime}))}\\Bigg)+p(x,\\mathsf{p}_{i}(x))\\Bigg(1-\\frac{\\exp(h(x,\\mathsf{p}_{i}(x)))}{\\sum_{y^{\\prime}\\in\\mathfrak{y}}\\exp(h(x,y^{\\prime}))}\\Bigg)}\\\\ &{\\quad+\\operatorname*{sup}_{\\mu\\in\\mathbb{R}}\\!\\Bigg(\\!-\\!p(x,\\mathsf{p}_{i}(x))\\!\\left(1-\\frac{\\exp(h(x,\\mathsf{h}_{i}(x)))-\\mu}{\\sum_{y^{\\prime}\\in\\mathfrak{y}}\\exp(h(x,y^{\\prime}))}\\right)\\!-\\!p(x,\\mathsf{h}_{i}(x))\\!\\left(1-\\frac{\\exp(h(x,\\mathsf{p}_{i}(x)))+\\mu}{\\sum_{y^{\\prime}\\in\\mathfrak{y}}\\exp(h(x,y^{\\prime}))}\\right)\\!\\Bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By the concavity of the function, differentiate with respect to $\\mu$ , we obtain that the supremum is achieved by $\\mu^{*}=-\\exp[h(x,\\mathsf{p}_{i}(x)]$ . Plug in $\\mu^{*}$ , we obtain ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta\\mathfrak{C}_{\\widetilde{\\ell}_{\\mathrm{mac}},\\mathcal{K}}(h,x)}\\\\ &{\\geq p(x,\\mathfrak{p}_{i}(x))\\frac{\\exp(h(x,\\mathfrak{h}_{i}(x)))}{\\sum_{y^{\\prime}\\in\\mathfrak{y}}\\exp(h(x,y^{\\prime}))}-p(x,\\mathfrak{h}_{i}(x))\\frac{\\exp(h(x,\\mathfrak{h}_{i}(x)))}{\\sum_{y^{\\prime}\\in\\mathfrak{y}}\\exp(h(x,y^{\\prime}))}}\\\\ &{\\geq\\frac{1}{n}(p(x,\\mathfrak{p}_{i}(x))-p(x,\\mathfrak{h}_{i}(x)))}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, the conditional regret of the top- $k$ loss can be upper bounded as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\Delta\\mathcal{C}_{\\ell_{k},\\mathcal{H}}(h,x)=\\sum_{i=1}^{k}\\bigl(p\\bigl(x,\\mathfrak{p}_{i}(x)\\bigr)-p\\bigl(x,\\mathfrak{h}_{i}(x)\\bigr)\\bigr)\\leq k n\\bigl(\\Delta\\mathcal{C}_{\\widetilde{\\ell}_{\\mathrm{mae}},\\mathcal{H}}(h,x)\\bigr).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Take expectations on both sides of the preceding equation, we obtain ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{\\ell_{k}}(h)-\\mathcal{E}_{\\ell_{k}}^{*}(\\mathcal{H})+\\mathcal{M}_{\\ell_{k}}(\\mathcal{H})\\leq k n\\Big(\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{mae}}}(h)-\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{mae}}}^{*}(\\mathcal{H})+\\mathcal{M}_{\\widetilde{\\ell}_{\\mathrm{mae}}}(\\mathcal{H})\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The second part follows from the fact that when $\\mathcal{A}_{\\widetilde{\\ell}_{\\mathrm{mae}}}(\\mathcal{H})=0$ , the minimizability gap $\\mathcal{N}_{\\widetilde{\\ell}_{\\mathrm{mae}}}$ (H) vanishes. ", "page_idx": 17}, {"type": "text", "text": "Case IV: $\\widetilde{\\ell}_{\\mathrm{comp}}=\\widetilde{\\ell}_{\\mathrm{gce}}$ . For generalized cross-entropy loss $\\widetilde{\\ell}_{\\mathrm{gce}}$ , the conditional regret can be written as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta\\mathfrak{C}_{\\widetilde{\\ell}_{\\mathrm{gce}},\\mathcal{K}}(h,x)}\\\\ &{=\\displaystyle\\sum_{y=1}^{n}p(x,y)\\widetilde{\\ell}_{\\mathrm{gce}}(h,x,y)-\\operatorname*{inf}_{h\\in\\mathcal{K}}\\displaystyle\\sum_{y=1}^{n}p(x,y)\\widetilde{\\ell}_{\\mathrm{gce}}(h,x,y)}\\\\ &{\\geq\\displaystyle\\sum_{y=1}^{n}p(x,y)\\widetilde{\\ell}_{\\mathrm{gce}}(h,x,y)-\\operatorname*{inf}_{\\mu\\in\\mathbb{R}}\\displaystyle\\sum_{y=1}^{n}p(x,y)\\widetilde{\\ell}_{\\mathrm{gce}}(h_{\\mu,i},x,y),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where for any $i\\in[k]$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\nh_{\\mu,i}(x,y)=\\left\\{\\!\\!\\begin{array}{l l}{h(x,y),}&{y\\notin\\{\\mathfrak{p}_{i}(x),\\mathsf{h}_{i}(x)\\}}\\\\ {\\log\\!\\big(e^{h(x,\\mathsf{p}_{i}(x))}+\\mu\\big)}&{y=\\mathsf{h}_{i}(x)}\\\\ {\\log\\!\\big(e^{h(x,\\mathsf{h}_{i}(x))}-\\mu\\big)}&{y=\\mathsf{p}_{i}(x).}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note that such a choice of $h_{\\mu,i}$ leads to the following equality holds: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{y\\notin\\{\\mathfrak{h}_{i}(x),\\mathfrak{p}_{i}(x)\\}}p(x,y)\\widetilde{\\ell}_{\\mathrm{gce}}(h,x,y)=\\sum_{y\\notin\\{\\mathfrak{h}_{i}(x),\\mathfrak{p}_{i}(x)\\}}p(x,y)\\widetilde{\\ell}_{\\mathrm{gce}}(h_{\\mu,i},x,y).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, for any $i\\in[k]$ , the conditional regret of generalized cross-entropy loss can be lower bounded as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\therefore\\Delta\\mathcal{C}_{\\widetilde{\\ell}_{\\mathrm{gce}},\\mathcal{R}}(h,x)}\\\\ &{\\geq p(x,\\mathfrak{h}_{i}(x))\\Bigg(1-\\Bigg[\\frac{\\exp(h(x,\\mathfrak{h}_{i}(x)))}{\\sum_{y^{\\prime}\\in\\mathfrak{y}}\\exp(h(x,y^{\\prime}))}\\Bigg]^{q}\\Bigg)+p(x,\\mathfrak{p}_{i}(x))\\Bigg(1-\\Bigg[\\frac{\\exp(h(x,\\mathfrak{p}_{i}(x)))}{\\sum_{y^{\\prime}\\in\\mathfrak{y}}\\exp(h(x,y^{\\prime}))}\\Bigg]^{q}\\Bigg)}\\\\ &{\\ast\\underset{\\mu\\in\\mathbb{R}}{\\operatorname*{sup}}\\Bigg(-p(x,\\mathfrak{h}_{i}(x))\\Bigg(1-\\Bigg[\\frac{\\exp(h(x,\\mathfrak{p}_{i}(x)))+\\mu}{\\sum_{y^{\\prime}\\in\\mathfrak{y}}\\exp(h(x,y^{\\prime}))}\\Bigg]^{q}\\Bigg)-p(x,\\mathfrak{p}_{i}(x))\\Bigg(1-\\Bigg[\\frac{\\exp(h(x,\\mathfrak{h}_{i}(x)))-\\mu}{\\sum_{y^{\\prime}\\in\\mathfrak{y}}\\exp(h(x,y^{\\prime}))}\\Bigg]^{q}\\Bigg)\\Bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By the concavity of the function, differentiate with respect to $\\mu$ , we obtain that the supremum is achieved by $\\begin{array}{r}{\\mu^{*}=\\frac{\\exp[h(x,\\mathfrak{h}_{i}(x))]p(x,\\mathfrak{p}_{i}(x))^{\\frac{1}{q-1}}-\\exp[h(x,\\mathfrak{p}_{i}(\\bar{x}))]p(x,\\dot{\\mathfrak{h}_{i}}(x))^{\\frac{1}{q-1}}}{p(x,\\mathfrak{h}_{i}(x))^{\\frac{1}{q-1}}+p(x,\\mathfrak{p}_{i}(x))^{\\frac{1}{q-1}}},}\\end{array}$ . Plug in $\\mu^{*}$ , we obtain ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{2}y(x,h(x))\\left[\\frac{\\left(\\exp(h(x,h(x)))+\\exp(h(x,p_{x}(x)))\\right)y(x,p_{x}(x,\\alpha(x)))^{\\frac{1-\\alpha}{\\alpha}}}{\\sum_{j\\in\\mathcal{N}}\\exp(h(x,p_{x}(x)))\\left[\\int_{\\frac{\\exp(x,h(x))}{\\sum_{k\\in\\mathcal{N}}\\exp(h(x,p_{x}(x)))}+\\exp(h(x,p_{x}(x)))^{\\frac{1-\\alpha}{\\alpha}}\\right]y(x,p_{x}(x))}\\right]^{\\alpha}}\\mathrm{~,}}\\\\ &{\\qquad-y(x,h(x))\\left[\\frac{\\exp(h(x,p_{x}(x)))}{\\sum_{k\\in\\mathcal{N}}\\exp(h(x,p_{x}(x)))}\\right]^{\\alpha}}\\\\ &{\\qquad+y(x,p_{x}(x))\\left[\\frac{\\left(\\exp(h(x,p_{x}(x)))+\\exp(h(x,p_{x}(x)))\\right)y(x,h(x,p_{x}(x)))^{\\frac{1-\\alpha}{\\alpha}}}{\\sum_{j\\in\\mathcal{N}}\\exp(h(x,p_{x}(x)))\\left[\\int_{\\frac{\\exp(x,h(x))}{\\sum_{k\\in\\mathcal{N}}\\left(\\exp(h(x,p_{x}(x))\\right)}+\\frac{\\log((\\exp(x,p_{x}(x)))^{\\frac{1-\\alpha}{\\alpha}})}{\\sum_{k\\in\\mathcal{N}}\\left(\\exp(x,p_{x}(x))\\right)}\\right]^{\\alpha}}}\\right]^{\\alpha}}\\\\ &{\\qquad-y(x,y(x))\\left[\\frac{\\exp(h(x,p_{x}(x)))}{\\sum_{j\\in\\mathcal{N}}\\exp(h(x,p_{x}(x)))\\left[\\int_{\\frac{\\log(x,p_{x}(x))}{\\sum_{k\\in\\mathcal{N}}\\left(\\exp(x,p_{x}(x))\\right)^{\\frac{1-\\alpha}{\\alpha}}}}\\right]^{\\alpha}}}\\\\ &{\\qquad\\mathrm{2}\\pi^{\\alpha}\\left(y(x,h(x))\\right)\\left[\\frac{2y(x,p_{x}(x))}{p_{x}(x,h(x))^{\\frac{1-\\alpha}{\\alpha}}+p_{x}(x,p_{x}(x))\\cdots}\\right]^{\\alpha}\\cdot p(x,h(x))\\right)}\\\\ &{\\qquad+\\frac{1}{\\pi}\\ln\\left(p(x,p_{x}(x) \n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "let $S_{i}=p(x,{\\mathsf p}_{i}(x))+p(x,{\\mathsf h}_{i}(x))$ and $\\Delta_{i}=p(x,{\\mathsf{p}}_{i}(x))-p(x,{\\mathsf{h}}_{i}(x))$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta\\mathcal{C}_{\\widetilde{\\ell}_{\\mathrm{gce}},\\mathcal{K}}(h,x)\\ge\\displaystyle\\frac{1}{q n^{q}}\\left(\\left[\\frac{\\displaystyle\\left(S_{i}+\\Delta_{i}\\right)^{\\frac{1}{1-q}}+\\left(S_{i}-\\Delta_{i}\\right)^{\\frac{1}{1-q}}}{2}\\right]^{1-q}-S_{i}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\ge\\displaystyle\\frac{1}{q n^{q}}\\left(\\left[\\frac{\\displaystyle\\left(1+\\Delta_{i}\\right)^{\\frac{1}{1-q}}+\\left(1-\\Delta_{i}\\right)^{\\frac{1}{1-q}}}{2}\\right]^{1-q}-1\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "(minimum is achieved when $S_{i}=1$ ) ", "page_idx": 18}, {"type": "equation", "text": "$$\n=\\psi(p(x,{\\mathfrak{p}}_{i}(x))-p(x,{\\mathfrak{h}}_{i}(x))),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\begin{array}{r}{\\psi(t)=\\frac{1}{q n^{q}}\\Biggl[\\left[\\frac{(1+t)^{\\frac{1}{1-q}}+(1-t)^{\\frac{1}{1-q}}}{2}\\right]^{1-q}-1\\Biggr],t\\in[0,1].}\\end{array}$ . Therefore, the conditional regret of the top- $k$ loss can be upper bounded as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Delta\\mathcal{C}_{\\ell_{k},\\mathcal{K}}(h,x)=\\sum_{i=1}^{k}\\bigl(p\\bigl(x,\\mathsf{p}_{i}(x)\\bigr)-p\\bigl(x,\\mathsf{h}_{i}(x)\\bigr)\\bigr)\\leq k\\psi^{-1}\\Bigl(\\Delta\\mathcal{C}_{\\widetilde{\\ell}_{\\mathrm{gce}},\\mathcal{K}}(h,x)\\Bigr).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By the concavity of $\\psi^{-1}$ , taking expectations on both sides of the preceding equation, we obtain ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{\\ell_{k}}\\left(h\\right)-\\mathcal{E}_{\\ell_{k}}^{*}(\\mathcal{H})+\\mathcal{M}_{\\ell_{k}}(\\mathcal{H})\\leq k\\psi^{-1}\\Big(\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{gce}}}(h)-\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{gce}}}^{*}(\\mathcal{H})+\\mathcal{M}_{\\widetilde{\\ell}_{\\mathrm{gce}}}(\\mathcal{H})\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The second part follows from the fact that when $\\mathcal{A}_{\\widetilde{\\ell}_{\\mathrm{gce}}}(\\mathcal{H})=0$ , the minimizability gap $\\Re_{\\widetilde{\\ell}_{\\mathrm{gce}}}(\\mathcal{H})$ vanishes. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "C Minimizability gaps and realizability ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The key quantities in our $\\mathcal{H}$ -consistency bounds are the minimizability gaps, which can be upper bounded by the approximation error, or more refined terms, depending on the magnitude of the parameter space, as discussed by Mao et al. [2023f]. As pointed out by these authors, these quantities, along with the functional form, can help compare different comp-sum loss functions. ", "page_idx": 19}, {"type": "text", "text": "Here, we further discuss the important role of minimizability gaps under the realizability assumption, and the connection with some negative results of Yang and Koyejo [2020]. ", "page_idx": 19}, {"type": "text", "text": "Definition C.1 (top- $k{=}\\mathcal{H}$ -realizability). A distribution $\\mathcal{D}$ over $\\mathcal{X}\\times\\mathcal{Y}$ is top- $k$ -H-realizable, if there exists a hypothesis $h\\in\\mathcal{H}$ such that $\\mathbb{P}_{(x,y)\\sim\\mathbb{D}}(h(x,y)>h(x,\\mathfrak{h}_{k+1}(x)))=\\mathbf{\\bar{1}}$ . ", "page_idx": 19}, {"type": "text", "text": "This extends the $\\mathcal{H}$ -realizability definition from standard (top-1) classification [Long and Servedio, 2013] to top- $k$ classification for any $k\\geq1$ . ", "page_idx": 19}, {"type": "text", "text": "Definition C.2. We say that a hypothesis set $\\mathcal{H}$ is closed under scaling, if it is a cone, that is for all $h\\in\\mathcal{H}$ and $\\beta\\in\\mathbb{R}_{+}$ , $\\beta h\\in{\\mathcal{H}}$ . ", "page_idx": 19}, {"type": "text", "text": "Definition C.3. We say that a surrogate loss $\\widetilde{\\ell}$ is realizable $\\mathcal{H}$ -consistent with respect to $\\ell_{k}$ , if for all $k\\in[1,n]$ , and for any sequence of hypotheses $\\{h_{n}\\}_{n\\in\\mathbb{N}}\\subset\\mathcal{H}$ and top- $k{-}\\mathcal{H}$ -realizable distribution, $\\begin{array}{r}{\\operatorname*{lim}_{n\\to+\\infty}\\mathcal{E}_{\\tilde{\\ell}}(h_{n})-\\mathcal{E}_{\\tilde{\\ell}}^{\\ast}(\\mathcal{H})=0}\\end{array}$ implies $\\begin{array}{r}{\\operatorname*{lim}_{n\\rightarrow+\\infty}\\mathcal{E}_{\\ell_{k}}\\!\\left(\\dot{h_{n}}\\right)-\\mathcal{E}_{\\ell_{k}}^{*}\\!\\left(\\mathcal{H}\\right)=0}\\end{array}$ . ", "page_idx": 19}, {"type": "text", "text": "When $\\mathcal{H}$ is closed under scaling, for $k=1$ and all comp-sum loss functions $\\ell=\\widetilde{\\ell}_{\\mathrm{log}},\\widetilde{\\ell}_{\\mathrm{exp}},\\widetilde{\\ell}_{\\mathrm{gc}}$ $\\widetilde{\\ell}_{\\mathrm{gce}}$ and $\\widetilde{\\ell}_{\\mathrm{mae}}$ , it can be shown that $\\mathscr{E}_{\\widetilde{\\ell}}^{\\,*}(\\mathcal{H})=\\mathfrak{M}_{\\widetilde{\\ell}}(\\mathcal{H})=0$ for any $\\mathcal{H}$ -realizable distribution. For example, for $\\ell=\\widetilde{\\ell}_{\\mathrm{log}}$ , by using the Lebesgue dominated convergence theorem, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{N}_{\\widetilde{\\ell}_{\\log}}(\\mathcal{K})\\le\\mathcal{E}_{\\widetilde{\\ell}_{\\log}}^{*}(\\mathcal{K})\\le\\operatorname*{lim}_{\\beta\\to+\\infty}\\mathcal{E}_{\\widetilde{\\ell}_{\\log}}(\\beta h^{*})=\\operatorname*{lim}_{\\beta\\to+\\infty}\\log\\biggl[1+\\sum_{y^{\\prime}\\neq y}e^{\\beta\\big(h^{*}(x,y^{\\prime})-h^{*}(x,y)\\big)}\\biggr]=0,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $h^{*}$ satisfies $\\mathbb{P}_{(x,y)\\sim\\mathbb{D}}\\bigl(h^{*}(x,y)>h^{*}(x,\\mathsf{h}_{2}(x))\\bigr)=1$ Therefore, Theorem 4.5 implies that all these loss functions are realizable $\\mathcal{H}$ -consistent with respect to $\\ell_{0-1}$ $\\mathit{\\Pi}^{\\ell_{k}}$ for $k=1$ ) when $\\mathcal{H}$ is closed under scaling. ", "page_idx": 19}, {"type": "text", "text": "Theorem C.4. Assume that $\\mathcal{H}$ is closed under scaling. Then, $\\widetilde{\\ell}_{\\mathrm{log}},\\,\\widetilde{\\ell}_{\\mathrm{exp}},$ , $\\widetilde{\\ell}_{\\mathrm{gce}}$ and $\\widetilde{\\ell}_{\\mathrm{mae}}$ are realizable $\\mathcal{H}$ -consistent with respect to $\\ell_{0-1}$ . ", "page_idx": 19}, {"type": "text", "text": "The formal proof is presented in Appendix D. However, for $k>1$ , since in the realizability assumption, $h(x,y)$ is only larger than $h(x,\\bar{\\mathsf{h}_{k+1}}(x))$ and can be smaller than $h(x,\\mathsf{h_{1}}(x))$ , there may exist an $\\mathcal{H}$ -realizable distribution $\\mathcal{D}$ such that $\\Re_{\\widetilde{\\ell}_{\\mathrm{log}}}(\\mathcal{H})>0$ . This explains the inconsistency of the logistic loss on top- $k$ separable data with linear predictors, when $k=2$ and $n>2$ , as shown in [Yang and Koyejo, 2020]. More generally, the exact same example in [Yang and Koyejo, 2020, Proposition 5.1] can be used to show that all the comp-sum losses, $\\widetilde{\\ell}_{\\mathrm{log}}$ , $\\widetilde{\\ell}_{\\mathrm{exp}}$ , $\\widetilde{\\ell}_{\\mathrm{gce}}$ and $\\widetilde{\\ell}_{\\mathrm{mae}}$ are not realizable $\\mathcal{H}$ - consistent with respect to $\\ell_{k}$ . Nevertheless, as previously shown, when the hypothesis set $\\mathcal{H}$ adopted is sufficiently rich such that $\\mathcal{M}_{\\widetilde{\\ell}}(\\mathcal{H})=0$ or even $\\mathcal{A}_{\\widetilde{\\ell}}(\\mathcal{H})=0$ , they are guaranteed to be $\\mathcal{H}$ -consistent. This is typically the case in practice when using deep neural networks. ", "page_idx": 19}, {"type": "text", "text": "D Proofs of realizable $\\mathcal{H}$ -consistency for comp-sum losses ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Theorem C.4. Assume that $\\mathcal{H}$ is closed under scaling. Then, $\\widetilde{\\ell}_{\\mathrm{log}},\\,\\widetilde{\\ell}_{\\mathrm{exp}},\\,\\widetilde{\\ell}_{\\mathrm{gce}}$ and $\\widetilde{\\ell}_{\\mathrm{mae}}$ are realizable $\\mathcal{H}$ -consistent with respect to $\\ell_{0-1}$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. Since the distribution is realizable, there exists a hypothesis $h\\in\\mathcal{H}$ such that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{P}_{(x,y)\\sim\\mathcal{D}}\\big(h^{*}(x,y)>h^{*}(x,\\mathfrak{h}_{2}(x))\\big)=1.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, for the logistic loss, by using the Lebesgue dominated convergence theorem, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{N}_{\\widetilde{\\ell}_{\\log}}(\\mathcal{K})\\le\\mathcal{E}_{\\widetilde{\\ell}_{\\log}}^{*}(\\mathcal{K})\\le\\operatorname*{lim}_{\\beta\\to+\\infty}\\mathcal{E}_{\\widetilde{\\ell}_{\\log}}(\\beta h)=\\operatorname*{lim}_{\\beta\\to+\\infty}\\log\\biggl[1+\\sum_{y^{\\prime}\\neq y}e^{\\beta\\big(h^{*}(x,y^{\\prime})-h^{*}(x,y)\\big)}\\biggr]=0.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For the sum exponential loss, by using the Lebesgue dominated convergence theorem, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{N}_{\\widetilde{\\ell}_{\\mathrm{exp}}}(\\mathcal{K})\\leq\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{exp}}}^{*}(\\mathcal{K})\\leq\\operatorname*{lim}_{\\beta\\rightarrow+\\infty}\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{exp}}}(\\beta h)=\\operatorname*{lim}_{\\beta\\rightarrow+\\infty}\\sum_{y^{\\prime}\\neq y}e^{\\beta\\left(h^{*}(x,y^{\\prime})-h^{*}(x,y)\\right)}=0.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For the generalized cross entropy loss, by using the Lebesgue dominated convergence theorem, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{M}_{\\widetilde{\\ell}_{\\mathrm{gce}}}(\\mathcal{K})\\leq\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{gce}}}^{*}(\\mathcal{K})\\leq\\operatorname*{lim}_{\\beta\\rightarrow+\\infty}\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{gce}}}(\\beta h)=\\operatorname*{lim}_{\\beta\\rightarrow+\\infty}\\frac{1}{q}\\left[1-\\left[\\sum_{y^{\\prime}\\in\\mathcal{Y}}e^{\\beta(h^{*}(x,y^{\\prime})-h^{*}(x,y))}\\right]^{-q}\\right]=0.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For the mean absolute error loss, by using the Lebesgue dominated convergence theorem, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{M}_{\\widetilde{\\ell}_{\\mathrm{mae}}}(\\mathcal{R})\\leq\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{mae}}}^{*}(\\mathcal{R})\\leq\\operatorname*{lim}_{\\beta\\rightarrow+\\infty}\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{mae}}}(\\beta h)=\\operatorname*{lim}_{\\beta\\rightarrow+\\infty}1-\\left[\\sum_{y^{\\prime}\\in\\mathcal{Y}}e^{\\beta\\left(h^{*}(x,y^{\\prime})-h^{*}(x,y)\\right)}\\right]^{-1}=0.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, by Theorem 4.5, the proof is completed. ", "page_idx": 20}, {"type": "text", "text": "E $\\mathcal{H}$ -Consistency bounds for constrained losses ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Constrained losses are defined as a summation of a function $\\Phi$ applied to the scores, subject to a constraint, as shown in [Lee et al., 2004, Awasthi et al., 2022b]. For any $h\\in\\mathcal{H}$ and $(x,y)\\in\\mathcal{X}\\times\\mathcal{Y}$ , they are expressed as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\widetilde{\\ell}_{\\mathrm{cstnd}}\\big(h,x,y\\big)=\\sum_{y^{\\prime}\\neq y}\\Phi\\big({-h(x,y^{\\prime})}\\big),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "with the constraint $\\textstyle\\sum_{y\\in{\\mathcal{Y}}}h(x,y)=0$ , where $\\Phi\\colon\\mathbb{R}\\to\\mathbb{R}_{+}$ is non-increasing. When $\\Phi$ is chosen as the function $t\\mapsto e^{-t}$ , $t\\mapsto\\operatorname*{max}\\{0,1-t\\}^{2}$ , $t\\mapsto\\operatorname*{max}\\{0,1-t\\}$ and $t\\mapsto\\operatorname*{min}\\{\\operatorname*{max}\\{0,1-t/\\rho\\},1\\}.$ , $\\rho>0$ , $\\widetilde{\\ell}_{\\mathrm{cstnd}}(h,x,y)$ are referred to as the constrained exponential loss $\\begin{array}{r}{\\widetilde{\\ell}_{\\mathrm{exp}}^{\\mathrm{cstnd}}(h,x,y)\\,=\\,\\sum_{y^{\\prime}\\ne y}e^{h(x,y^{\\prime})}}\\end{array}$ the constrained squared hinge loss $\\begin{array}{r c l}{\\widetilde{\\ell}_{\\mathrm{sq-hinge}}(h,x,y)}&{=}&{\\sum_{y^{\\prime}\\neq y}\\operatorname*{max}\\{0,1+h(x,y^{\\prime})\\}^{2}}\\end{array}$ , the constrained hinge loss $\\begin{array}{r}{\\widetilde{\\ell}_{\\mathrm{hinge}}(h,x,y)=\\sum_{y^{\\prime}\\neq y}\\operatorname*{max}\\{0,1+h(x,y^{\\prime})\\}}\\end{array}$ , and the constrained $\\rho$ -margin loss $\\begin{array}{r}{\\widetilde{\\ell}_{\\rho}(h,x,y)=\\sum_{y^{\\prime}\\neq y}\\operatorname*{min}\\{\\operatorname*{max}\\{0,1+h(x,y^{\\prime})/\\rho\\},1\\}}\\end{array}$ , respectively [Awasthi et al., 2022b]. We now study these loss functions and show that they benefti from $\\mathcal{H}$ -consistency bounds with respect to the top- $k$ loss. ", "page_idx": 20}, {"type": "text", "text": "Theorem E.1. Assume that $\\mathcal{H}$ is symmetric and complete. Then, for any $1\\leq k\\leq n$ , the following $\\mathcal{H}$ -consistency bound holds for the constrained loss: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{\\ell_{k}}(h)-\\mathcal{E}_{\\ell_{k}}^{*}(\\mathcal{K})+\\mathcal{M}_{\\ell_{k}}(\\mathcal{K})\\leq k\\gamma\\Big(\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{cstnd}}}(h)-\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{cstnd}}}^{*}(\\mathcal{K})+\\mathcal{M}_{\\widetilde{\\ell}_{\\mathrm{cstnd}}}(\\mathcal{K})\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In the special case where $\\mathcal{A}_{\\widetilde{\\ell}_{\\mathrm{cstnd}}}(\\mathcal{H})=0_{:}$ , for any $1\\leq k\\leq n$ , the following bound holds: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{\\ell_{k}}(h)-\\mathcal{E}_{\\ell_{k}}^{*}(\\mathcal{H})\\leq k\\gamma\\Big(\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{cstnd}}}(h)-\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{cstnd}}}^{*}(\\mathcal{H})\\Big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\gamma(t)=2\\sqrt{t}$ when $\\widetilde{\\ell}_{\\mathrm{cstnd}}$ is either \u2113\u0303cesxtpndor $\\widetilde{\\ell}_{\\mathrm{sq-hinge}}$ ; $\\gamma(t)=t$ when $\\widetilde{\\ell}_{\\mathrm{cstnd}}$ is either $\\widetilde{\\ell}_{\\mathrm{hinge}}\\;o r$ $\\widetilde{\\ell}_{\\rho}$ . ", "page_idx": 20}, {"type": "text", "text": "The proof is included in Appendix F. The second part follows from the fact that when the hypothesis set $\\mathcal{H}$ is sufficiently rich such that $A_{\\widetilde{\\ell}_{\\mathrm{cstnd}}}(\\mathcal{H})\\;=\\;0$ , we have $\\mathfrak{M}_{\\widetilde{\\ell}_{\\mathrm{cstnd}}}(\\mathcal{H})\\;=\\;0$ . Therefore, the constrained loss is $\\mathcal{H}$ -consistent and Bayes-consistent with respect to $\\ell_{k}$ . If the surrogate estimation error $\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{cstnd}}}(h)-\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{cstnd}}}^{*}(\\mathcal{H})$ is $\\epsilon$ , then, the target estimation error satisfies $\\mathcal{E}_{\\ell_{k}}(h)-\\mathcal{E}_{\\ell_{k}}^{\\bar{*}}(\\mathcal{H})\\leq k\\gamma(\\epsilon)$ Note that the constrained exponential loss and the constrained squared hinge loss both admit a square root $\\mathcal{H}$ -consistency bound while the bounds for the constrained hinge loss and $\\rho$ -margin loss are both linear. ", "page_idx": 20}, {"type": "text", "text": "F Proofs of $\\mathcal{H}$ -consistency bounds for constrained losses ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The conditional error for the constrained loss can be expressed as follows: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\widehat{\\psi}_{\\mathrm{cstnd}}(h,x)=\\sum_{y=1}^{n}p(x,y)\\widetilde{\\ell}_{\\mathrm{cstnd}}(h,x,y)=\\sum_{y=1}^{n}p(x,y)\\sum_{y^{\\prime}\\neq y}\\Phi(-h(x,y^{\\prime}))=\\sum_{y\\in\\mathfrak{H}}(1-p(x,y))\\Phi(-h(x,y)).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Theorem E.1. Assume that $\\mathcal{H}$ is symmetric and complete. Then, for any $1\\leq k\\leq n$ , the following $\\mathcal{H}$ -consistency bound holds for the constrained loss: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{\\ell_{k}}(h)-\\mathcal{E}_{\\ell_{k}}^{*}(\\mathcal{K})+\\mathcal{M}_{\\ell_{k}}(\\mathcal{K})\\leq k\\gamma\\Big(\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{cstnd}}}(h)-\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{cstnd}}}^{*}(\\mathcal{K})+\\mathcal{M}_{\\widetilde{\\ell}_{\\mathrm{cstnd}}}(\\mathcal{K})\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "In the special case where $\\mathcal{A}_{\\widetilde{\\ell}_{\\mathrm{cstnd}}}(\\mathcal{H})=0_{:}$ , for any $1\\leq k\\leq n$ , the following bound holds: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{\\ell_{k}}(h)-\\mathcal{E}_{\\ell_{k}}^{*}(\\mathcal{H})\\leq k\\gamma\\Big(\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{cstnd}}}(h)-\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{cstnd}}}^{*}(\\mathcal{H})\\Big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\gamma(t)=2\\sqrt{t}$ when $\\widetilde{\\ell}_{\\mathrm{cstnd}}$ is either \u2113\u0303cesxtpndor $\\widetilde{\\ell}_{\\mathrm{sq}}.$ \u2212hinge; $\\gamma(t)=t$ when $\\widetilde{\\ell}_{\\mathrm{cstnd}}$ is either $\\widetilde{\\ell}_{\\mathrm{hinge}}\\,o r$ $\\widetilde{\\ell}_{\\rho}$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. Case I: $\\widetilde{\\ell}_{\\mathrm{cstnd}}=\\widetilde{\\ell}_{\\mathrm{exp}}^{\\mathrm{cstnd}}$ . For the constrained exponential loss $\\widetilde{\\ell}_{\\mathrm{exp}}^{\\mathrm{cstnd}}$ , the conditional regret can ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta\\mathcal{C}_{\\widetilde{\\ell}_{\\mathrm{exp}}^{\\mathrm{cstrad}},\\mathcal{K}}(h,x)=\\displaystyle\\sum_{y=1}^{n}p(x,y)\\widetilde{\\ell}_{\\mathrm{exp}}^{\\mathrm{cstnd}}(h,x,y)-\\operatorname*{inf}_{h\\in\\mathcal{H}_{\\mathcal{Y}}}\\sum_{y=1}^{n}p(x,y)\\widetilde{\\ell}_{\\mathrm{exp}}^{\\mathrm{cstnd}}(h,x,y)\\,}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\geq\\displaystyle\\sum_{y=1}^{n}p(x,y)\\widetilde{\\ell}_{\\mathrm{exp}}^{\\mathrm{cstnd}}(h,x,y)-\\operatorname*{inf}_{\\mu\\in\\mathbb{R}}\\sum_{y=1}^{n}p(x,y)\\widetilde{\\ell}_{\\mathrm{exp}}^{\\mathrm{cstnd}}(h_{\\mu,i},x,y),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where for any $i\\in[k]$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\nh_{\\mu,i}(x,y)=\\left\\{h(x,y),\\qquad\\qquad\\qquad y\\notin\\{\\mathfrak{p}_{i}(x),\\mathsf{h}_{i}(x)\\}}\\\\ {h(x,\\mathsf{p}_{i}(x))+\\mu}&{y=\\mathsf{h}_{i}(x)}\\\\ {h(x,\\mathsf{h}_{i}(x))-\\mu}&{y=\\mathsf{p}_{i}(x).}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Note that such a choice of $h_{\\mu,i}$ leads to the following equality holds: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sum_{y\\notin\\{\\mathfrak{h}_{i}(x),\\mathfrak{p}_{i}(x)\\}}p(x,y)\\widetilde{\\ell}_{\\mathrm{exp}}^{\\mathrm{cstnd}}(h,x,y)=\\sum_{y\\notin\\{\\mathfrak{h}_{i}(x),\\mathfrak{p}_{i}(x)\\}}p(x,y)\\widetilde{\\ell}_{\\mathrm{exp}}^{\\mathrm{cstnd}}(h_{\\mu,i},x,y).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Let $q(x,{\\mathsf p}_{i}(x))=1-p(x,{\\mathsf p}_{i}(x))$ and $q(x,\\mathsf{h}_{i}(x))=1-p(x,\\mathsf{h}_{i}(x))$ . Therefore, for any $i\\in[k]$ , the conditional regret of constrained exponential loss can be lower bounded as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\Delta\\mathcal{C}_{\\ell\\mathrm{expd},\\mathcal{H}}^{\\top}(h,x)}\\\\ &{\\geq\\underset{h\\in\\mathcal{H}}{\\mathrm{inf~sup}}\\{q(x,\\mathfrak{p}_{i}(x))-\\varrho(x,\\mathfrak{p}_{i}(x))-\\varrho^{h}(x,h_{i}(x))-\\mu\\}+q(x,\\mathfrak{h}_{i}(x))\\big(e^{h(x,h_{i}(x))}-e^{h(x,\\mathfrak{p}_{i}(x))+\\mu}\\big)\\}}\\\\ &{=\\Big(\\sqrt{q(x,\\mathfrak{p}_{i}(x))}-\\sqrt{q(x,\\mathfrak{h}_{i}(x))}\\Big)^{2}}&{\\mathrm{(differentiating~with~respect~to~\\mu,~h~to~optimize)}}\\\\ &{=\\left(\\frac{q(x,\\mathfrak{h}_{i}(x))-q(x,\\mathfrak{p}_{i}(x))}{\\sqrt{q(x,\\mathfrak{p}_{i}(x))}+\\sqrt{q(x,\\mathfrak{h}_{i}(x))}}\\right)^{2}}\\\\ &{\\geq\\frac{1}{4}(q(x,\\mathfrak{h}_{i}(x))-q(x,\\mathfrak{p}_{i}(x)))^{2}}&{\\mathrm{(0\\leqq(x,\\mathfrak{y})\\leq1)}}\\\\ &{=\\frac{1}{4}(p(x,\\mathfrak{p}_{i}(x))-p(x,\\mathfrak{h}_{i}(x)))^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, by Lemma 4.4, the conditional regret of the top- $k$ loss can be upper bounded as follows: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Delta\\mathcal{C}_{\\ell_{k},\\mathcal{K}}(h,x)=\\sum_{i=1}^{k}(p(x,\\mathfrak{p}_{i}(x))-p(x,\\mathfrak{h}_{i}(x)))\\leq2k\\Big(\\Delta\\mathcal{C}_{\\widetilde{\\ell}_{\\mathrm{exp}}^{\\mathrm{cund}},\\mathcal{K}}(h,x)\\Big)^{\\frac{1}{2}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By the concavity, taking expectations on both sides of the preceding equation, we obtain ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{\\ell_{k}}\\left(h\\right)-\\mathcal{E}_{\\ell_{k}}^{*}(\\mathcal{H})+\\mathcal{M}_{\\ell_{k}}(\\mathcal{H})\\leq2k\\bigg(\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{exp}}^{\\mathrm{cstnd}}}(h)-\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{exp}}^{\\mathrm{cstnd}}}^{*}(\\mathcal{H})+\\mathcal{M}_{\\widetilde{\\ell}_{\\mathrm{exp}}^{\\mathrm{cstnd}}}(\\mathcal{H})\\bigg)^{\\frac{1}{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The second part follows from the fact that when $\\mathcal{A}_{\\widetilde{\\ell}^{\\mathrm{cstnd}}}(\\mathcal{H})=0$ , we have $\\mathfrak{M}_{\\widetilde{\\ell}_{\\mathrm{exp}}^{\\mathrm{cstnd}}}(\\mathcal{H})=0$ . ", "page_idx": 22}, {"type": "text", "text": "Case II: $\\widetilde{\\ell}_{\\mathrm{cstnd}}=\\widetilde{\\ell}_{\\mathrm{sq-hinge}}.$ . For the constrained squared hinge loss $\\widetilde{\\ell}_{\\mathrm{sq-hinge}}$ , the conditional regret can be written as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta\\mathfrak{C}_{\\widetilde{\\ell}_{\\mathrm{sq-hinge}},\\mathcal{K}}(h,x)=\\displaystyle\\sum_{y=1}^{n}p(x,y)\\widetilde{\\ell}_{\\mathrm{sq-hinge}}(h,x,y)-\\operatorname*{inf}_{h\\in\\mathcal{H}_{y}}\\sum_{y=1}^{n}p(x,y)\\widetilde{\\ell}_{\\mathrm{sq-hinge}}(h,x,y)\\,}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\geq\\displaystyle\\sum_{y=1}^{n}p(x,y)\\widetilde{\\ell}_{\\mathrm{sq-hinge}}(h,x,y)-\\operatorname*{inf}_{\\mu\\in\\mathbb{R}}\\sum_{y=1}^{n}p(x,y)\\widetilde{\\ell}_{\\mathrm{sq-hinge}}(h_{\\mu,i},x,y),}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where for any $i\\in[k]$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\nh_{\\mu,i}(x,y)=\\left\\{\\!\\!\\begin{array}{l l}{h(x,y),}&{y\\notin\\{\\mathsf{p}_{i}(x),\\mathsf{h}_{i}(x)\\}}\\\\ {h(x,\\mathsf{p}_{i}(x))+\\mu}&{y=\\mathsf{h}_{i}(x)}\\\\ {h(x,\\mathsf{h}_{i}(x))-\\mu}&{y=\\mathsf{p}_{i}(x).}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Note that such a choice of $h_{\\mu,i}$ leads to the following equality holds: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{y\\notin\\{\\mathfrak{h}_{i}(x),\\mathfrak{p}_{i}(x)\\}}p(x,y)\\widetilde{\\ell}_{\\mathrm{sq-hinge}}(h,x,y)=\\sum_{y\\notin\\{\\mathfrak{h}_{i}(x),\\mathfrak{p}_{i}(x)\\}}p(x,y)\\widetilde{\\ell}_{\\mathrm{sq-hinge}}(h_{\\mu,i},x,y).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Let $q(x,{\\mathsf p}_{i}(x))=1-p(x,{\\mathsf p}_{i}(x))$ and $q(x,\\mathsf{h}_{i}(x))=1-p(x,\\mathsf{h}_{i}(x))$ . Therefore, for any $i\\in[k]$ , the conditional regret of the constrained squared hinge loss can be lower bounded as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta\\mathbb{C}_{\\tilde{\\mathcal{E}}_{\\mathrm{sq-hinge}},\\mathcal{I}}(h,x)}\\\\ &{\\geq\\underset{h\\in\\mathbb{F}^{2}\\,\\mathcal{H}}{\\operatorname*{inf}}\\,\\underset{\\mu\\in\\mathbb{R}}{\\operatorname*{sup}}\\left\\lbrace q(x,\\mathfrak{p}_{i}(x))\\Bigl(\\operatorname*{max}\\{0,1+h(x,\\mathfrak{p}_{i}(x))\\}^{2}-\\operatorname*{max}\\{0,1+h(x,\\mathfrak{h}_{i}(x))-\\mu\\}^{2}\\right)}\\\\ &{\\qquad+\\,q(x,\\mathfrak{h}_{i}(x))\\Bigl(\\operatorname*{max}\\{0,1+h(x,\\mathfrak{h}_{i}(x))\\}^{2}-\\operatorname*{max}\\{0,1+h(x,\\mathfrak{p}_{i}(x))+\\mu\\}^{2}\\Bigr)\\right\\rbrace}\\\\ &{\\geq\\frac{1}{4}(q(x,\\mathfrak{p}_{i}(x))-q(x,\\mathfrak{h}_{i}(x)))^{2}\\qquad\\qquad\\mathrm{~(differentiating~with~respect~to~}\\mu,h\\mathrm{~to~opt~}}\\\\ &{=\\frac{1}{4}(p(x,\\mathfrak{p}_{i}(x))-p(x,\\mathfrak{h}_{i}(x)))^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, by Lemma 4.4, the conditional regret of the top- $k$ loss can be upper bounded as follows: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\Delta\\mathcal{C}_{\\ell_{k},\\mathcal{H}}(h,x)=\\sum_{i=1}^{k}(p(x,\\mathfrak{p}_{i}(x))-p(x,\\mathfrak{h}_{i}(x)))\\leq2k\\Big(\\Delta\\mathcal{C}_{\\widetilde{\\ell}_{\\mathrm{sq-hinge}},\\mathcal{H}}(h,x)\\Big)^{\\frac{1}{2}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By the concavity, taking expectations on both sides of the preceding equation, we obtain ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{\\ell_{k}}(h)-\\mathcal{E}_{\\ell_{k}}^{*}(\\mathcal{H})+\\mathcal{M}_{\\ell_{k}}(\\mathcal{H})\\leq2k\\Big(\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{sq-hinge}}}(h)-\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{sq-hinge}}}^{*}(\\mathcal{H})+\\mathcal{M}_{\\widetilde{\\ell}_{\\mathrm{sq-hinge}}}(\\mathcal{H})\\Big)^{\\frac{1}{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The second part follows from the fact that when the hypothesis set $\\mathcal{H}$ is sufficiently rich such that $\\mathcal{A}_{\\widetilde{\\ell}_{\\mathrm{sq-hinge}}}(\\mathcal{\\bar{H}})=0$ , we have $\\mathcal{M}_{\\widetilde{\\ell}_{\\mathrm{sq-hinge}}}(\\mathcal{H})=0$ . ", "page_idx": 22}, {"type": "text", "text": "Case III: $\\widetilde{\\ell}_{\\mathrm{cstnd}}=\\widetilde{\\ell}_{\\mathrm{hinge}}$ . For the constrained hinge loss $\\widetilde{\\ell}_{\\mathrm{hinge}}$ , the conditional regret can be written as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta\\mathfrak{C}_{\\widetilde{\\ell}_{\\mathrm{hinge}},\\mathcal{K}}(h,x)=\\displaystyle\\sum_{y=1}^{n}p(x,y)\\widetilde{\\ell}_{\\mathrm{hinge}}(h,x,y)-\\operatorname*{inf}_{h\\in\\mathcal{H}}\\displaystyle\\sum_{y=1}^{n}p(x,y)\\widetilde{\\ell}_{\\mathrm{hinge}}(h,x,y)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\geq\\displaystyle\\sum_{y=1}^{n}p(x,y)\\widetilde{\\ell}_{\\mathrm{hinge}}(h,x,y)-\\operatorname*{inf}_{\\mu\\in\\mathbb{R}}\\displaystyle\\sum_{y=1}^{n}p(x,y)\\widetilde{\\ell}_{\\mathrm{hinge}}(h_{\\mu,i},x,y),}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where for any $i\\in[k]$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\nh_{\\mu,i}(x,y)=\\left\\{h(x,y),\\qquad\\qquad\\qquad y\\notin\\{\\mathfrak{p}_{i}(x),\\mathsf{h}_{i}(x)\\}}\\\\ {h(x,\\mathsf{p}_{i}(x))+\\mu}&{y=\\mathsf{h}_{i}(x)}\\\\ {h(x,\\mathsf{h}_{i}(x))-\\mu}&{y=\\mathsf{p}_{i}(x).}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Note that such a choice of $h_{\\mu,i}$ leads to the following equality holds: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{y\\notin\\{\\mathfrak{h}_{i}(x),\\mathfrak{p}_{i}(x)\\}}p(x,y)\\widetilde{\\ell}_{\\mathrm{hinge}}(h,x,y)=\\sum_{y\\notin\\{\\mathfrak{h}_{i}(x),\\mathfrak{p}_{i}(x)\\}}p(x,y)\\widetilde{\\ell}_{\\mathrm{hinge}}(h_{\\mu,i},x,y).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Let $q(x,{\\mathsf p}_{i}(x))=1-p(x,{\\mathsf p}_{i}(x))$ and $q(x,\\mathsf{h}_{i}(x))=1-p(x,\\mathsf{h}_{i}(x))$ . Therefore, for any $i\\in[k]$ , the conditional regret of the constrained hinge loss can be lower-bounded as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\Delta\\mathfrak{C}_{\\tilde{\\ell}_{\\mathrm{hinge}},\\mathcal{H}}(h,x)\\geq\\underset{h\\in\\mathcal{H}}{\\operatorname*{inf}}\\ \\underset{\\mu\\in\\mathbb{R}}{\\operatorname*{sup}}\\left\\lbrace q(x,\\mathfrak{p}_{i}(x))(\\operatorname*{max}\\{0,1+h(x,\\mathfrak{p}_{i}(x))\\}-\\operatorname*{max}\\{0,1+h(x,\\mathfrak{h}_{i}(x))-\\mu\\})\\right.}\\\\ &{}&{\\qquad\\qquad+\\left.q(x,\\mathfrak{h}_{i}(x))(\\operatorname*{max}\\{0,1+h(x,\\mathfrak{h}_{i}(x))\\}-\\operatorname*{max}\\{0,1+h(x,\\mathfrak{p}_{i}(x))+\\mu\\})\\right\\rbrace}\\\\ &{}&{\\qquad\\qquad\\geq q(x,\\mathfrak{h}_{i}(x))-q(x,\\mathfrak{p}_{i}(x))\\qquad\\mathrm{(differentiating~with~respect~to~}\\mu,h\\mathrm{~to~optimize}}\\\\ &{}&{=p(x,\\mathfrak{p}_{i}(x))-p(x,\\mathfrak{h}_{i}(x))}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore, by Lemma 4.4, the conditional regret of the top- $k$ loss can be upper bounded as follows: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\Delta\\mathcal{C}_{\\ell_{k},\\mathcal{K}}(h,x)=\\sum_{i=1}^{k}(p(x,\\mathsf{p}_{i}(x))-p(x,\\mathsf{h}_{i}(x)))\\leq k\\Delta\\mathcal{C}_{\\widetilde{\\ell}_{\\mathrm{hinge}},\\mathcal{K}}(h,x).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By the concavity, taking expectations on both sides of the preceding equation, we obtain ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{\\ell_{k}}\\big(h\\big)-\\mathcal{E}_{\\ell_{k}}^{*}(\\mathcal{H})+\\mathcal{M}_{\\ell_{k}}\\big(\\mathcal{H}\\big)\\le k\\Big(\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{hinge}}}(h)-\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{hinge}}}^{*}\\big(\\mathcal{H}\\big)+\\mathcal{N}_{\\widetilde{\\ell}_{\\mathrm{hinge}}}\\big(\\mathcal{K}\\big)\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The second part follows from the fact that when the hypothesis set $\\mathcal{H}$ is sufficiently rich such that $\\mathcal{A}_{\\widetilde{\\ell}_{\\mathrm{hinge}}}(\\mathcal{H})\\,\\,\\overline{{=}}\\,0$ , we have $\\mathfrak{M}_{\\widetilde{\\ell}_{\\mathrm{hinge}}}(\\mathcal{H})=0$ . ", "page_idx": 23}, {"type": "text", "text": "Case IV: $\\widetilde{\\ell}_{\\mathrm{cstnd}}=\\widetilde{\\ell}_{\\rho}$ . For the constrained $\\rho$ -margin loss $\\widetilde{\\ell}_{\\rho}$ , the conditional regret can be written as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\Delta\\mathcal{C}_{\\widetilde{\\ell}_{\\rho},\\mathcal{K}}(h,x)=\\sum_{y=1}^{n}p(x,y)\\widetilde{\\ell}_{\\rho}(h,x,y)-\\operatorname*{inf}_{h\\in\\mathcal{H}}\\sum_{y=1}^{n}p(x,y)\\widetilde{\\ell}_{\\rho}(h,x,y)}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\geq\\sum_{y=1}^{n}p(x,y)\\widetilde{\\ell}_{\\rho}(h,x,y)-\\operatorname*{inf}_{\\mu\\in\\mathbb{R}}\\sum_{y=1}^{n}p(x,y)\\widetilde{\\ell}_{\\rho}(h_{\\mu,i},x,y),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where for any $i\\in[k]$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\nh_{\\mu,i}(x,y)=\\left\\{\\!\\!\\begin{array}{l l}{h(x,y),}&{y\\notin\\{\\mathfrak{p}_{i}(x),\\mathsf{h}_{i}(x)\\}}\\\\ {h(x,\\mathsf{p}_{i}(x))+\\mu}&{y=\\mathsf{h}_{i}(x)}\\\\ {h(x,\\mathsf{h}_{i}(x))-\\mu}&{y=\\mathsf{p}_{i}(x).}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Note that such a choice of $h_{\\mu,i}$ leads to the following equality holds: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{y\\notin\\{\\mathfrak{h}_{i}(x),\\mathfrak{p}_{i}(x)\\}}p(x,y)\\widetilde{\\ell}_{\\rho}(h,x,y)=\\sum_{y\\notin\\{\\mathfrak{h}_{i}(x),\\mathfrak{p}_{i}(x)\\}}p(x,y)\\widetilde{\\ell}_{\\rho}(h_{\\mu,i},x,y).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Let $q(x,{\\mathsf p}_{i}(x))=1-p(x,{\\mathsf p}_{i}(x))$ and $q(x,\\mathsf{h}_{i}(x))=1-p(x,\\mathsf{h}_{i}(x))$ . Therefore, for any $i\\in[k]$ , the conditional regret of the constrained $\\rho$ -margin loss can be lower-bounded as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta\\mathcal{C}_{\\tilde{\\ell}_{\\infty},\\mathcal{K}}(h,x)}\\\\ &{\\geq\\underset{h\\in\\mathcal{H}}{\\operatorname*{inf}}\\ \\underset{\\mu\\in\\mathbb{R}}{\\operatorname*{sup}}\\left\\lbrace q(x,\\mathfrak{p}_{i}(x))\\bigg(\\operatorname*{min}\\bigg\\lbrace\\operatorname*{max}\\bigg\\lbrace0,1+\\frac{h(x,\\mathfrak{p}_{i}(x))}{\\rho}\\bigg\\rbrace,1\\bigg\\rbrace-\\operatorname*{min}\\bigg\\lbrace\\operatorname*{max}\\bigg\\lbrace0,1+\\frac{h(x,\\mathfrak{h}_{i}(x))-\\mu}{\\rho}\\bigg\\rbrace,1\\bigg\\rbrace\\right.}\\\\ &{+\\left.q(x,\\mathfrak{h}_{i}(x))\\bigg(\\operatorname*{min}\\bigg\\lbrace0,1+\\frac{h(x,\\mathfrak{h}_{i}(x))}{\\rho}\\bigg\\rbrace,1\\bigg\\rbrace-\\operatorname*{min}\\bigg\\lbrace\\operatorname*{max}\\bigg\\lbrace0,1+\\frac{h(x,\\mathfrak{p}_{i}(x))+\\mu}{\\rho}\\bigg\\rbrace,1\\bigg\\rbrace\\right)\\right\\rbrace}\\\\ &{\\geq q(x,\\mathfrak{h}_{i}(x))-q(x,\\mathfrak{p}_{i}(x))}\\\\ &{=p(x,\\mathfrak{p}_{i}(x))-p(x,\\mathfrak{h}_{i}(x))}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore, by Lemma 4.4, the conditional regret of the top- $k$ loss can be upper bounded as follows: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\Delta\\mathcal{C}_{\\ell_{k},\\mathcal{K}}(h,x)=\\sum_{i=1}^{k}(p(x,\\mathsf{p}_{i}(x))-p(x,\\mathsf{h}_{i}(x)))\\leq k\\Delta\\mathcal{C}_{\\widetilde{\\ell}_{\\rho},\\mathcal{K}}(h,x).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By the concavity, taking expectations on both sides of the preceding equation, we obtain ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{\\ell_{k}}\\left(h\\right)-\\mathcal{E}_{\\ell_{k}}^{*}(\\mathcal{K})+\\mathcal{M}_{\\ell_{k}}(\\mathcal{K})\\leq k\\Big(\\mathcal{E}_{\\widetilde{\\ell}_{\\rho}}\\left(h\\right)-\\mathcal{E}_{\\widetilde{\\ell}_{\\rho}}^{*}(\\mathcal{K})+\\mathcal{M}_{\\widetilde{\\ell}_{\\rho}}\\left(\\mathcal{K}\\right)\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The second part follows from the fact that when the hypothesis set $\\mathcal{H}$ is sufficiently rich such that $\\mathcal{A}_{\\widetilde{\\ell}_{\\rho}}(\\mathcal{H})=0$ , we have $\\Re_{\\widetilde{\\ell}_{\\rho}}(\\mathcal{H})=0$ . \u53e3 ", "page_idx": 24}, {"type": "text", "text": "G Technical challenges and novelty in Section 4.2 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The technical challenges and novelty of proofs in Section 4.2 lie in the following three aspects: ", "page_idx": 25}, {"type": "text", "text": "(1) Conditional regret of the top- ${\\cdot k}$ loss: This involves a comprehensive analysis of the conditional regret associated with the top- $k$ loss, which is significantly more complex than that of the zero-one loss in a standard setting. The conditional regret of the top- $k$ loss incorporates both the top- $k$ conditional probabilities ${\\mathsf p}_{i}(x)$ , for $i\\,=\\,1,\\ldots,k,$ , and the top- $k$ scores $\\mathsf{h}_{i}(\\bar{x})$ , for $i\\,=\\,1,\\ldots,k$ , as characterized in Lemma 4.4. ", "page_idx": 25}, {"type": "text", "text": "(2) Relating to the conditional regret of the surrogate loss: To establish $\\mathcal{H}$ -consistency bounds, it is necessary to upper bound the conditional regret of the top- $k$ loss with that of the surrogate loss. This task is particularly challenging in the top- $k$ setting due to the intricate nature of the top- ${\\cdot k}$ loss\u2019s conditional regret. A pivotal observation is that the conditional regret of the top- ${\\cdot k}$ loss can be expressed as the sum of $k$ terms $(p(x,{\\mathsf p}_{i}(x))-p(x,{\\mathsf h}_{i}(x)))$ for $i\\,=\\,1,\\ldots,k$ . Each term $(p(x,{\\mathsf p}_{i}(\\bar{x}))-p(x,{\\mathsf h}_{i}(x)))$ exhibits structural similarities to the conditional regret of the zero-one loss, $(p(x,{\\mathsf p}_{1}(x))-p(x,{\\mathsf h}_{1}(x)))$ . Consequently, we introduce a series of auxiliary hypotheses $h_{\\mu,i}$ , each dependent on ${\\mathfrak{h}}_{i}(x)$ and ${\\mathsf p}_{i}(x)$ for $i\\in[k]$ . This approach transforms the challenge of upper bounding the conditional regret of the top- $k$ loss into $k$ subproblems, each focusing on upper bounding the term $\\left(p(x,\\mathsf{p}_{i}(x))-\\bar{p}(x,\\mathsf{h}_{i}(x))\\right)$ with the conditional regret of the surrogate loss. ", "page_idx": 25}, {"type": "text", "text": "(3) Upper bounding each term $(p(x,{\\mathsf p}_{i}(x))-p(x,{\\mathsf h}_{i}(x)))$ : Following the approach in prior work [Mao et al., 2023f] for top-1 classification, we define $h_{\\mu,i}(x,y)$ as: ", "page_idx": 25}, {"type": "equation", "text": "$$\nh_{\\mu,i}(x,y)=\\left\\{h(x,y),\\begin{array}{r l}&{y\\notin\\{\\mathsf{p}_{i}(x)),\\mathsf{h}_{i}(x))\\}}\\\\ &{k\\mathrm{\\log}\\bigl(e^{h(x,\\mathsf{p}_{i}(x))}+\\mu\\bigr)}\\\\ &{\\mathrm{\\log}\\bigl(e^{h(x,\\mathsf{h}_{i}(x)))}-\\mu\\bigr)}\\end{array}\\right.\\,\\,y=\\mathsf{h}_{i}(x))}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "for the proof of comp-sum losses (Theorem 4.5). The subsequent proof is considered straightforward. ", "page_idx": 25}, {"type": "text", "text": "However, for the proof of constrained losses (Theorem E.1), we adopt a different hypothesis formulation for $h_{\\mu,i}(x,y)$ , leveraging the constraint that the scores sum to zero and the specific structure of constrained losses. The hypothesis is defined as: ", "page_idx": 25}, {"type": "equation", "text": "$$\nh_{\\mu,i}(x,y)=\\left\\{h(x,y),\\qquad\\qquad\\qquad y\\notin\\{{\\mathfrak{p}}_{i}(x)),{\\mathfrak{h}}_{i}(x))\\}\\right\\}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The remainder of the proof then specifically addresses the peculiarities of constrained losses, which significantly diverges from the previous work. ", "page_idx": 25}, {"type": "text", "text": "In summary, aspects (1) and (2) are novel and represent significant advancements that have not been explored previously. For aspect (3), the proof for comp-sum loss closely follows the approach in [Mao et al., 2023f], which appears straightforward due to the innovative ideas presented in aspects (1) and (2). However, the proof for constrained losses significantly deviates from the previous work, particularly in terms of the new auxiliary hypothesis formulation and the specific constrained losses examined. ", "page_idx": 25}, {"type": "text", "text": "We would like to further emphasize that these results are significant and useful. They demonstrate that comp-sum losses, which include the cross-entropy loss commonly used in top-1 classification, and constrained losses, are $\\mathcal{H}$ -consistent in top- $k$ classification for any $k$ . Notably, the cross-entropy loss is the only Bayes-consistent smooth surrogate loss for top- $k$ classification identified to date. Furthermore, the Bayes-consistency of loss functions within the constrained loss family is a novel exploration in the context of top- $k$ classification. These findings are pivotal as they highlight two broad families of smooth loss functions that are Bayes-consistent in top- $\\cdot k$ classification. Additionally, they reveal that these families, including the cross-entropy loss, benefti from stronger, non-asymptotic and hypothesis set-specific guarantees\u2014 $\\mathcal{H}$ -consistency bounds\u2014in top- $k$ classification. ", "page_idx": 25}, {"type": "text", "text": "H Generalization bounds ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Given a finite sample $S\\,=\\,{\\bigl(}(x_{1},y_{1}),\\ldots,(x_{m},y_{m}){\\bigr)}$ drawn from $\\Phi^{m}$ , let $\\widehat{h}_{S}$ be the minimizer of the empirical loss within $\\mathcal{H}$ with respect to the top- $k$ surrogate loss \u2113\u0303: $\\widehat{h}_{S}=\\mathrm{argmin}_{h\\in\\mathcal{H}}\\,\\widehat{\\mathfrak{E}}_{\\widetilde{\\ell},S}(h)=$ $\\begin{array}{r}{\\mathrm{argmin}_{h\\in\\mathcal{H}}\\:\\frac{1}{m}\\sum_{i=1}^{m}\\widetilde{\\ell}(h,x_{i},y_{i})}\\end{array}$ . Next, we will show that we can use $\\mathcal{H}$ -consistency bounds for $\\widetilde{\\ell}$ to derive generalization bounds for the top- $k$ loss by upper bounding the surrogate estimation error $\\mathcal{E}_{\\widehat{\\ell}}(\\widehat{h}_{S})-\\mathcal{E}_{\\widehat{\\ell}}^{*}(\\mathcal{H})$ with the complexity (e.g. the Rademacher complexity) of the family of functions associated with $\\widehat{\\ell}$ and $\\mathcal{H}$ : $\\mathcal{H}_{\\widetilde{\\ell}}=\\big\\{(x,y)\\mapsto\\widetilde{\\ell}(h,x,y){\\colon}h\\in\\mathcal{H}\\big\\}.$ . ", "page_idx": 26}, {"type": "text", "text": "Let $\\Re_{m}^{\\widetilde{\\ell}}(\\mathcal{H})$ be the Rademacher complexity of $\\mathcal{H}_{\\widetilde{\\ell}}$ and $B_{\\widetilde{\\ell}}$ an upper bound of the surrogate loss $\\widehat{\\ell}$ Then, we obtain the following generalization bounds for the top- $k$ loss. ", "page_idx": 26}, {"type": "text", "text": "Theorem H.1 (Generalization bound with comp-sum losses). Assume that $\\mathcal{H}$ is symmetric and complete. Then, for any $1\\leq k\\leq n_{!}$ , the following top- $\\cdot k$ generalization bound holds for $\\widehat{h}_{S}$ : for any $\\delta>0$ , with probability at least $1-\\delta$ over the draw of an i.i.d sample $S$ of size $m$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{\\ell_{k}}\\widehat{(h_{S})}-\\mathcal{E}_{\\ell_{k}}^{*}(\\mathcal{H})+\\mathcal{M}_{\\ell_{k}}(\\mathcal{H})\\leq k\\psi^{-1}\\Bigg(4\\Re_{m}^{\\widetilde{\\ell}}(\\mathcal{H})+2B_{\\widetilde{\\ell}}\\sqrt{\\frac{\\log\\frac{2}{\\delta}}{2m}}+\\mathcal{M}_{\\widetilde{\\ell}}(\\mathcal{H})\\Bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\it~2re~}\\psi(t)=\\frac{1-t}{2}\\log(1-t)+\\frac{1+t}{2}\\log(1+t),\\,t\\in[0,1]\\,{\\it w h e n\\ \\widetilde{\\ell}}i s\\ \\widetilde{\\ell}_{\\mathrm{log}};\\,\\psi(t)=1-\\sqrt{1-t^{2}},\\,t\\in[0,T],}\\\\ &{\\mathrm{\\it~2n~}\\widetilde{\\ell}\\,i s\\ \\widetilde{\\ell}_{\\mathrm{exp}};\\,\\psi(t)=t/n\\mathrm{\\it~when\\}\\widetilde{\\ell}\\,i s\\ \\widetilde{\\ell}_{\\mathrm{mae}};\\,a n d\\ \\psi(t)\\ =\\frac{1}{q n^{q}}\\left[\\left[\\frac{\\left(1+t\\right)^{\\frac{1}{1-q}}+\\left(1-t\\right)^{\\frac{1}{1-q}}}{2}\\right]^{1-q}-1\\right],\\,f\\rho}\\\\ &{\\left(0,1\\right),\\,t\\in[0,1]\\,{\\it w h e n\\ }\\widetilde{\\ell}\\,i s\\ \\widetilde{\\ell}_{\\mathrm{gce}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. By using the standard Rademacher complexity bounds [Mohri et al., 2018], for any $\\delta>0$ , with probability at least $1-\\delta$ , the following holds for all $h\\in\\mathcal{H}$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\mathcal{E}_{\\widetilde{\\ell}}(h)-\\widehat{\\mathcal{E}}_{\\widetilde{\\ell},S}(h)\\right|\\leq2\\Re_{m}^{\\widetilde{\\ell}}(\\mathcal{H})+B_{\\widetilde{\\ell}}\\sqrt{\\frac{\\log(2/\\delta)}{2m}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Fix $\\epsilon>0$ . By the definition of the infimum, there exists $h^{*}\\in\\mathcal{H}$ such that $\\mathcal{E}_{\\widetilde{\\ell}}(h^{*})\\leq\\mathcal{E}_{\\widetilde{\\ell}}^{*}(\\mathcal{H})+\\epsilon$ . By definition of $\\widehat{h}_{S}$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{E}_{\\widetilde{\\ell}}(\\widehat{h}_{S})-\\mathcal{E}_{\\widetilde{\\ell}}^{*}(\\mathfrak{K})}\\\\ &{\\ =\\mathcal{E}_{\\widetilde{\\ell}}(\\widehat{h}_{S})-\\widehat{\\mathcal{E}}_{\\widetilde{\\ell},S}(\\widehat{h}_{S})+\\widehat{\\mathcal{E}}_{\\widetilde{\\ell},S}(\\widehat{h}_{S})-\\mathcal{E}_{\\widetilde{\\ell}}^{*}(\\mathfrak{K})}\\\\ &{\\ \\le\\mathcal{E}_{\\widetilde{\\ell}}(\\widehat{h}_{S})-\\widehat{\\mathcal{E}}_{\\widetilde{\\ell},S}(\\widehat{h}_{S})+\\widehat{\\mathcal{E}}_{\\widetilde{\\ell},S}(h^{*})-\\mathcal{E}_{\\widetilde{\\ell}}^{*}(\\mathfrak{K})}\\\\ &{\\ \\le\\mathcal{E}_{\\widetilde{\\ell}}(\\widehat{h}_{S})-\\widehat{\\mathcal{E}}_{\\widetilde{\\ell},S}(\\widehat{h}_{S})+\\widehat{\\mathcal{E}}_{\\widetilde{\\ell},S}(h^{*})-\\mathcal{E}_{\\widetilde{\\ell}}^{*}(h^{*})+\\epsilon}\\\\ &{\\ \\le2\\Bigg[2\\mathfrak{R}_{m}^{\\widetilde{\\ell}}(\\mathfrak{K})+B_{\\widetilde{\\ell}}\\sqrt{\\frac{\\log(2/\\delta)}{2m}}\\Bigg]+\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Since the inequality holds for all $\\epsilon>0$ , it implies: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{\\widetilde{\\ell}}(\\widehat{h}_{S})-\\mathcal{E}_{\\widetilde{\\ell}}^{*}(\\mathcal{H})\\leq4\\mathfrak{R}_{m}^{\\widetilde{\\ell}}(\\mathcal{H})+2B_{\\widetilde{\\ell}}\\sqrt{\\frac{\\log(2/\\delta)}{2m}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Plugging in this inequality in the bounds of Theorem 4.5 completes the proof. ", "page_idx": 26}, {"type": "text", "text": "Theorem H.2 (Generalization bound with constrained losses). Assume that $\\mathcal{H}$ is symmetric and complete. Then, for any $1\\leq k\\leq n_{!}$ , the following top- $\\cdot k$ generalization bound holds for \u0302hS: for any $\\delta>0$ , with probability at least $1-\\delta$ over the draw of an i.i.d sample $S$ of size $m$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{\\ell_{k}}\\widehat{(h_{S})}-\\mathcal{E}_{\\ell_{k}}^{*}(\\mathcal{H})+\\mathcal{M}_{\\ell_{k}}(\\mathcal{H})\\leq k\\gamma\\Bigg(4\\mathfrak{R}_{m}^{\\widetilde{\\ell}}(\\mathcal{H})+2B_{\\widetilde{\\ell}}\\sqrt{\\frac{\\log\\frac{2}{\\delta}}{2m}}+\\mathcal{M}_{\\widetilde{\\ell}}(\\mathcal{H})\\Bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\gamma(t)=2\\sqrt{t}$ when $\\widetilde{\\ell}$ is either $\\widetilde{\\ell}_{\\mathrm{exp}}^{\\mathrm{cstnd}}\\;o r\\;\\widetilde{\\ell}_{\\mathrm{sq-hinge}};\\;\\gamma(t)=t$ when $\\widetilde{\\ell}$ is either $\\widetilde{\\ell_{\\mathrm{hinge}}}\\:o r\\:\\widetilde{\\ell_{\\rho}}$ . ", "page_idx": 26}, {"type": "text", "text": "Proof. By using the standard Rademacher complexity bounds [Mohri et al., 2018], for any $\\delta>0$ , with probability at least $1-\\delta$ , the following holds for all $h\\in\\mathcal{H}$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\mathcal{E}_{\\widetilde{\\ell}}(h)-\\widehat{\\mathcal{E}}_{\\widetilde{\\ell},S}(h)\\right|\\leq2\\Re_{m}^{\\widetilde{\\ell}}(\\mathcal{H})+B_{\\widetilde{\\ell}}\\sqrt{\\frac{\\log(2/\\delta)}{2m}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Fix $\\epsilon>0$ . By the definition of the infimum, there exists $h^{*}\\in\\mathcal{H}$ such that $\\mathcal{E}_{\\widetilde{\\ell}}(h^{*})\\leq\\mathcal{E}_{\\widetilde{\\ell}}^{*}(\\mathcal{H})+\\epsilon$ . By definition of $\\widehat{h}_{S}$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{E}_{\\widetilde{\\ell}}(\\widehat{h}_{S})-\\mathcal{E}_{\\widetilde{\\ell}}^{*}(\\mathfrak{K})}\\\\ &{\\ =\\mathcal{E}_{\\widetilde{\\ell}}(\\widehat{h}_{S})-\\widehat{\\mathcal{E}}_{\\widetilde{\\ell},S}(\\widehat{h}_{S})+\\widehat{\\mathcal{E}}_{\\widetilde{\\ell},S}(\\widehat{h}_{S})-\\mathcal{E}_{\\widetilde{\\ell}}^{*}(\\mathfrak{K})}\\\\ &{\\ \\le\\mathcal{E}_{\\widetilde{\\ell}}(\\widehat{h}_{S})-\\widehat{\\mathcal{E}}_{\\widetilde{\\ell},S}(\\widehat{h}_{S})+\\widehat{\\mathcal{E}}_{\\widetilde{\\ell},S}(h^{*})-\\mathcal{E}_{\\widetilde{\\ell}}^{*}(\\mathfrak{K})}\\\\ &{\\ \\le\\mathcal{E}_{\\widetilde{\\ell}}(\\widehat{h}_{S})-\\widehat{\\mathcal{E}}_{\\widetilde{\\ell},S}(\\widehat{h}_{S})+\\widehat{\\mathcal{E}}_{\\widetilde{\\ell},S}(h^{*})-\\mathcal{E}_{\\widetilde{\\ell}}^{*}(h^{*})+\\epsilon}\\\\ &{\\ \\le2\\Bigg[2\\mathfrak{R}_{m}^{\\widetilde{\\ell}}(\\mathfrak{K})+B_{\\widetilde{\\ell}}\\sqrt{\\frac{\\log(2/\\delta)}{2m}}\\Bigg]+\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Since the inequality holds for all $\\epsilon>0$ , it implies: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{\\widetilde{\\ell}}(\\widehat{h}_{S})-\\mathcal{E}_{\\widetilde{\\ell}}^{*}(\\mathcal{H})\\leq4\\mathfrak{R}_{m}^{\\widetilde{\\ell}}(\\mathcal{H})+2B_{\\widetilde{\\ell}}\\sqrt{\\frac{\\log(2/\\delta)}{2m}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Plugging in this inequality in the bounds of Theorem E.1 completes the proof. ", "page_idx": 27}, {"type": "text", "text": "To the best of our knowledge, Theorems H.1 and H.2 provide the first finite-sample guarantees for the estimation error of the minimizer of comp-sum losses and constrained losses, with respect to the top- $\\cdot k$ loss, for any $1\\leq k\\leq n$ . The proofs use our $\\mathcal{H}$ -consistency bounds with respect to the top- $k$ loss, as well as standard Rademacher complexity guarantees. ", "page_idx": 27}, {"type": "text", "text": "I Proofs of $\\mathcal{H}$ -consistency bounds for cost-sensitive losses ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We first characterize the best-in class conditional error and the conditional regret of the target cardinality aware loss function (2), which will be used in the analysis of $\\mathcal{H}$ -consistency bounds. ", "page_idx": 28}, {"type": "text", "text": "Lemma I.1. Assume that $\\mathcal{R}$ is symmetric and complete. Then, for any $r\\in\\mathcal{K}$ and $x\\in\\mathcal X$ , the best-in class conditional error and the conditional regret of the target cardinality aware loss function can be expressed as follows: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\mathbb{C}_{\\ell}^{*}(\\mathcal{R},x)=\\underset{k\\in\\mathcal{K}}{\\mathrm{min}}\\sum_{y\\in\\mathbb{Y}}p(x,y)c(x,k,y)}\\\\ &{\\Delta\\mathcal{C}_{\\ell,\\mathcal{R}}(r,x)=\\displaystyle\\sum_{y\\in\\mathcal{Y}}p(x,y)c(x,r(x),y)-\\underset{k\\in\\mathcal{K}}{\\mathrm{min}}\\sum_{y\\in\\mathbb{Y}}p(x,y)c(x,k,y).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. By definition, for any $r\\in\\mathcal{R}$ and $x\\in\\mathcal X$ , the conditional error of the target cardinality aware loss function can be written as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathcal{C}_{\\ell}(r,x)=\\sum_{y\\in\\mathfrak{Y}}p(x,y)c(x,\\mathfrak{r}(x),y).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Since $\\mathcal{R}$ is symmetric and complete, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathfrak{C_{\\ell}^{*}}(\\mathcal{R},x)=\\operatorname*{inf}_{r\\in\\mathcal{R}}\\sum_{y\\in\\mathfrak{Y}}p(x,y)c(x,r(x),y)=\\operatorname*{min}_{k\\in\\mathcal{K}}\\sum_{y\\in\\mathfrak{Y}}p(x,y)c(x,k,y).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Furthermore, the calibration gap can be expressed as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\Delta\\mathcal{C}_{\\ell,\\mathfrak{R}}(r,x)=\\mathcal{C}_{\\ell}(r,x)-\\mathcal{C}_{\\ell}^{*}(\\mathcal{R},x)=\\sum_{y\\in\\mathfrak{H}}p(x,y)c(x,r(x),y)-\\operatorname*{min}_{k\\in\\mathcal{K}}\\sum_{y\\in\\mathfrak{H}}p(x,y)c(x,k,y),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "which completes the proof. ", "page_idx": 28}, {"type": "text", "text": "I.1 Proof of Theorem 4.6 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "For convenience, we $\\begin{array}{r l r}{\\mathrm{let}~\\ \\overline{{c}}(x,k,y)}&{=}&{1\\\\\\mathrm{\\boldmath~-~}c(x,k,y),\\ \\ \\overline{{q}}(x,k)\\mathrm{\\boldmath~\\xi~}=\\mathrm{\\boldmath~\\sum_{\\scriptstyley\\in\\mathbb{Y}}~}p(x,y)\\overline{{c}}(x,k,y)\\mathrm{\\boldmath~\\xi~}\\in\\mathrm{~\\mathbb{Z}~}}\\\\ {\\mathrm{\\boldmath~\\frac{~}{\\scriptscriptstyle{Y}_{1,t^{\\prime},\\infty}~}}\\mathrm{e}^{r(x,k)}.\\quad\\mathrm{\\boldmath~\\mathcal{We}~}\\mathrm{\\boldmath~\\also~}\\mathrm{\\boldmath~let~}\\ k_{\\mathrm{min}}(x)}&{=}&{\\mathrm{argmin}_{k\\in\\mathbb{X}}(1-\\overline{{q}}(x,k))\\mathrm{\\boldmath~\\xi~}=}\\end{array}$ [0,1] and S(x,k) = \u2211k\u2032\u2208eK er(x,k\u2032) .   \n$\\begin{array}{r}{\\mathrm{argmin}_{k\\in\\mathcal{K}}\\sum_{y\\in\\mathcal{Y}}p(x,y)c(\\tilde{x},\\tilde{k},y)}\\end{array}$ . ", "page_idx": 28}, {"type": "text", "text": "Theorem 4.6. Assume that $\\mathcal{R}$ is symmetric and complete. Then, the following bound holds for the cost-sensitive comp-sum loss: for all $r\\in\\mathcal{R}$ and for any distribution, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{\\ell}(r)-\\mathcal{E}_{\\ell}^{*}(\\mathcal{R})+\\mathcal{M}_{\\ell}(\\mathcal{R})\\leq\\gamma\\Big(\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{c-comp}}}(r)-\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{c-comp}}}^{*}(\\mathcal{R})+\\mathcal{M}_{\\widetilde{\\ell}_{\\mathrm{c-comp}}}(\\mathcal{R})\\Big);}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "When $\\mathcal{R}=\\mathcal{R}_{\\mathrm{all}}$ , the following holds: $\\begin{array}{r}{\\mathcal{E}_{\\ell}(r)-\\mathcal{E}_{\\ell}^{*}(\\mathcal{R}_{\\mathrm{all}})\\,\\leq\\,\\gamma\\Big(\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{c-comp}}}(r)-\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{c-comp}}}^{*}(\\mathcal{R}_{\\mathrm{all}})\\Big)\\,}\\end{array}$ , where $\\gamma(t)\\,=\\,2\\sqrt{\\,t\\,}$ when $\\widetilde{\\ell}_{\\mathrm{c-comp}}$ is either \u2113\u0303c\u2212log or \u2113\u0303c\u2212exp; $\\gamma(t)\\,=\\,2\\sqrt{|\\mathcal{K}|^{q}t}$ when $\\widetilde{\\ell}_{\\mathrm{c-comp}}$ is $\\widetilde{\\ell}_{\\mathrm{c-gce}}$ ; and $\\gamma(t)=|\\mathcal{K}|t$ when \u2113\u0303c\u2212comp is \u2113\u0303c\u2212mae. ", "page_idx": 28}, {"type": "text", "text": "Proof. Case I: $\\widetilde{\\ell}_{\\mathrm{c-comp}}=\\widetilde{\\ell}_{\\mathrm{c-log}}$ . For the cost-sensitive logistic loss $\\widetilde{\\ell}_{\\mathrm{c-log}}$ , the conditional error can be written as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathcal{C}_{\\widetilde{\\ell}_{\\mathrm{c-log}}}(r,x)=-\\sum_{y\\in\\mathfrak{Y}}p(x,y)\\sum_{k\\in\\mathcal{K}}\\overline{{c}}(x,k,y)\\log\\biggr(\\frac{e^{r(x,k)}}{\\sum_{k^{\\prime}\\in\\mathcal{K}}e^{r(x,k^{\\prime})}}\\biggr)=-\\sum_{k\\in\\mathcal{K}}\\log(\\mathfrak{S}(x,k))\\overline{{q}}(x,k).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The conditional regret can be written as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\i}\\bigcirc_{\\widetilde{\\ell}_{\\mathrm{c-log}},\\Re}(r,x)=-\\displaystyle\\sum_{k\\in\\mathcal{K}}\\log(\\S(x,k))\\overline{{q}}(x,k)-\\operatorname*{inf}_{r\\in\\mathcal{R}}\\biggl(-\\displaystyle\\sum_{k\\in\\mathcal{K}}\\log(\\ S(x,k))\\overline{{q}}(x,k)\\biggr)}\\\\ &{\\qquad\\qquad\\qquad\\geq-\\displaystyle\\sum_{k\\in\\mathcal{K}}\\log(\\ S(x,k))\\overline{{q}}(x,k)-\\operatorname*{inf}_{\\mu\\in[-\\delta(x,k_{\\operatorname*{min}}(x)),S(x,r(x))]}\\biggl(-\\displaystyle\\sum_{k\\in\\mathcal{K}}\\log(\\ S_{\\mu}(x,k))\\overline{{q}}(x,k)\\biggr)}\\end{array},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "\u23a7\u23aa\u23aa\u23aaS(x,y), y \u2209{kmin(x),r(x)} where for any $x\\in\\mathcal X$ and k \u2208K, S\u00b5(x,k) =\u23a8S(x,kmin(x)) + \u00b5 y = r(x) Note that \u23aa\u23aa\u23aa\u23a9S(x,r(x)) \u2212\u00b5 y = kmin(x). such a choice of $\\mathcal{S}_{\\mu}$ leads to the following equality holds: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\sum_{k\\not\\in\\{\\mathfrak{r}(x),k_{\\operatorname*{min}}(x)\\}}\\log(\\S(x,k))\\overline{{q}}(x,k)=\\sum_{k\\not\\in\\{\\mathfrak{r}(x),k_{\\operatorname*{min}}(x)\\}}\\log(\\S_{\\mu}(x,k))\\overline{{q}}(x,k).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Therefore, the conditional regret of cost-sensitive logistic loss can be lower bounded as ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta\\mathfrak{C}_{\\widetilde{\\ell}_{\\mathrm{c-log}},\\mathcal{K}}(h,x)}\\\\ &{\\geq\\underset{\\mu\\in[-\\delta(x,k_{\\operatorname*{min}}(x)),\\mathcal{S}(x,r(x))]}{\\operatorname*{sup}}\\left\\lbrace\\overline{{q}}(x,k_{\\operatorname*{min}}(x))[-\\log(\\S(x,k_{\\operatorname*{min}}(x)))+\\log(\\ S(x,r(x))-\\mu)]\\right.}\\\\ &{\\qquad+\\left.\\overline{{q}}(x,r(x))[-\\log(\\ S(x,r(x)))+\\log(\\ S(x,k_{\\operatorname*{min}}(x))+\\mu)]\\right\\rbrace.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "By the concavity of the function, differentiate with respect to $\\mu$ , we obtain that the supremum is achieved by $\\iota_{\\iota}\\ast\\stackrel{-}{-}\\overline{{q}}(x,\\!\\mathfrak{r}(x))\\mathcal{S}(x,\\!\\mathfrak{r}(x))\\!-\\!\\overline{{q}}(x,\\!k_{\\operatorname*{min}}(x))\\mathcal{S}(x,\\!k_{\\operatorname*{min}}(x))$ $\\overline{{\\overline{{q}}(\\boldsymbol{x},\\boldsymbol{k}_{\\mathrm{min}}(\\boldsymbol{x}))\\!+\\!\\overline{{q}}(\\boldsymbol{x},\\boldsymbol{\\mathsf{r}}(\\boldsymbol{x}))}}$ . Plug in $\\mu^{*}$ , we obtain ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta\\mathfrak{C}_{\\tilde{\\ell}_{\\mathrm{c-log}},\\mathcal{K}}(h,x)}\\\\ &{\\geq\\overline{{q}}(x,k_{\\operatorname*{min}}(x))\\log\\frac{\\left(\\mathcal{S}(x,r(x))+\\mathcal{S}(x,k_{\\operatorname*{min}}(x))\\right)\\overline{{q}}(x,k_{\\operatorname*{min}}(x))}{\\mathcal{S}(x,k_{\\operatorname*{min}}(x))\\left(\\overline{{q}}(x,k_{\\operatorname*{min}}(x))\\right)+\\overline{{q}}(x,r(x)))}}\\\\ &{\\qquad+\\,\\overline{{q}}(x,r(x))\\log\\frac{\\left(\\mathcal{S}(x,r(x))+\\mathcal{S}(x,k_{\\operatorname*{min}}(x))\\right)\\overline{{q}}(x,r(x))}{\\mathcal{S}(x,r(x))\\left(\\overline{{q}}(x,k_{\\operatorname*{min}}(x))+\\overline{{q}}(x,r(x))\\right)}}\\\\ &{\\geq\\overline{{q}}(x,k_{\\operatorname*{min}}(x))\\log\\frac{2\\overline{{q}}(x,k_{\\operatorname*{min}}(x))}{\\overline{{q}}(x,k_{\\operatorname*{min}}(x))+\\overline{{q}}(x,r(x))}+\\overline{{q}}(x,r(x))\\log\\frac{2\\overline{{q}}(x,r(x))}{\\overline{{q}}(x,k_{\\operatorname*{min}}(x))+\\overline{{q}}(x,r(x))}}\\\\ &{\\qquad+\\,\\overline{{q}}(x,\\mathfrak{m}_{\\#}(x),{\\mathfrak p}({x},{\\mathfrak p}_{\\operatorname*{min}}(x))+\\overline{{q}}(x,{\\mathfrak p}_{\\operatorname*{min}}(x))\\quad\\textnormal{...}\\quad\\overline{{q}}_{\\infty}(x,{\\mathfrak p}_{\\operatorname*{min}}(x))\\underset{=0}{\\overset{\\gamma}{\\partial}}(x,r(x))}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\geq\\frac{\\left(\\overline{{q}}(x,r(x))-\\overline{{q}}(x,k_{\\mathrm{min}}(x))\\right)^{2}}{2\\left(\\overline{{q}}(x,r(x))+\\overline{{q}}(x,k_{\\mathrm{min}}(x))\\right)}\\qquad\\qquad\\qquad}&\\\\ &{\\qquad\\qquad\\qquad(a\\log\\frac{2a}{a+b}+b\\log\\frac{2b}{a+b}\\geq\\frac{(a-b)^{2}}{2(a+b)},\\forall a,b\\in[0,1]\\ [\\mathrm{Mohri~et~al.},2018,\\mathrm{Proposition~E.}7])}\\\\ &{\\geq\\frac{\\left(\\overline{{q}}(x,r(x))-\\overline{{q}}(x,k_{\\mathrm{min}}(x))\\right)^{2}}{4}.}&{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(0\\leq\\overline{{q}}(x,r(x))+\\overline{{q}}(x,k_{\\mathrm{min}}(x))\\leq2)}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Therefore, by Lemma I.1, the conditional regret of the target cardinality aware loss function can be upper bounded as follows: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Delta\\mathfrak{C}_{\\ell,\\mathcal{K}}(r,x)=\\overline{{q}}(x,k_{\\operatorname*{min}}(x))-\\overline{{q}}(x,r(x))\\leq2\\Big(\\Delta\\mathfrak{C}_{\\widetilde{\\ell}_{\\mathrm{c-log}},\\mathcal{R}}(r,x)\\Big)^{\\frac{1}{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "By the concavity, taking expectations on both sides of the preceding equation, we obtain ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{\\ell}(r)-\\mathcal{E}_{\\ell}^{*}(\\mathcal{R})+\\mathcal{M}_{\\ell}(\\mathcal{R})\\leq2\\Big(\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{c-log}}}(r)-\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{c-log}}}^{*}(\\mathcal{R})+\\mathcal{M}_{\\widetilde{\\ell}_{\\mathrm{c-log}}}(\\mathcal{R})\\Big)^{\\frac{1}{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "The second part follows from the fact that $\\mathcal{M}_{\\widetilde{\\ell}_{\\mathrm{c-log}}}(\\mathcal{R}_{\\mathrm{all}})=0$ . ", "page_idx": 29}, {"type": "text", "text": "Case II: \u2113\u0303c\u2212comp $\\widetilde{\\ell}_{\\mathrm{c-comp}}=\\widetilde{\\ell}_{\\mathrm{c-exp}}$ . For the cost-sensitive sum exponential loss $\\widetilde{\\ell}_{\\mathrm{c-exp}}$ , the conditional error can be written as ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathcal{C}_{\\widetilde{\\ell}_{\\mathrm{c-exp}}}(r,x)=\\sum_{y\\in\\mathfrak{Y}}p(x,y)\\sum_{k\\in\\mathfrak{K}}\\overline{{c}}(x,k,y)\\sum_{k^{\\prime}\\neq k^{\\prime}}e^{r(x,k^{\\prime})-r(x,k)}=\\sum_{k\\in\\mathfrak{K}}\\biggl(\\frac{1}{\\mathcal{S}(x,k)}-1\\biggr)\\overline{{q}}(x,k).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "The conditional regret can be written as ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\i}\\bigcirc_{\\widetilde{\\ell}_{\\mathrm{c-exp}},\\Re}(r,x)=\\displaystyle\\sum_{k\\in\\widetilde{\\mathcal{K}}}\\bigg(\\frac{1}{\\mathcal{S}(x,k)}-1\\bigg)\\overline{{q}}(x,k)-\\operatorname*{inf}_{r\\in\\mathcal{R}}\\bigg(\\displaystyle\\sum_{k\\in\\mathcal{K}}\\bigg(\\frac{1}{\\mathcal{S}(x,k)}-1\\bigg)\\overline{{q}}(x,k)\\bigg)}\\\\ &{\\qquad\\qquad\\qquad\\geq\\displaystyle\\sum_{k\\in\\mathcal{K}}\\bigg(\\frac{1}{\\mathcal{S}(x,k)}-1\\bigg)\\overline{{q}}(x,k)-\\operatorname*{inf}_{\\mu\\in[-\\mathcal{S}(x,k_{\\operatorname*{min}}(x)),\\mathcal{S}(x,\\tau(x))]}\\bigg(\\sum_{k\\in\\mathcal{K}}\\bigg(\\frac{1}{\\mathcal{S}_{\\mu}(x,k)}-1\\bigg)\\overline{{q}}(x,k)\\bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where for any $x\\in\\mathcal X$ $:\\mathrm{and~}k\\,\\in\\mathcal{K},\\,\\mathfrak{S}_{\\mu}(x,k)\\,=\\,\\left\\{\\!\\!\\!\\begin{array}{l l}{\\!\\!\\!\\mathfrak{S}(x,y),}&{y\\notin\\{k_{\\operatorname*{min}}(x),\\mathsf{r}(x)\\}}\\\\ {\\!\\!\\!\\mathfrak{S}(x,k_{\\operatorname*{min}}(x))+\\mu}&{y=\\mathsf{r}(x)}\\\\ {\\!\\!\\!\\mathfrak{S}(x,\\mathsf{r}(x))-\\mu}&{y=k_{\\operatorname*{min}}(x).}\\end{array}\\!\\!\\right.\\mathrm{Note~that}$ such a choice of $\\mathcal{S}_{\\mu}$ leads to the following equality holds: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\sum_{k\\not\\in\\{r(x),k_{\\operatorname*{min}}(x)\\}}\\biggl(\\frac{1}{\\S(x,k)}-1\\biggr)\\overline{{q}}(x,k)=\\sum_{k\\not\\in\\{r(x),k_{\\operatorname*{min}}(x)\\}}\\biggl(\\frac{1}{\\S_{\\mu}(x,k)}-1\\biggr)\\overline{{q}}(x,k).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Therefore, the conditional regret of cost-sensitive sum exponential loss can be lower bounded as ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\Delta\\mathcal{C}_{\\widetilde{\\ell}_{\\mathrm{c-exp}},\\mathcal{K}}(h,x)\\geq\\operatorname*{sup}_{\\mu\\in[-\\delta(x,k_{\\operatorname*{min}}(x)),S(x,r(x))]}\\Bigg\\{\\overline{{q}}(x,k_{\\operatorname*{min}}(x))\\Bigg[\\frac{1}{\\mathcal{S}(x,k_{\\operatorname*{min}}(x))}-\\frac{1}{\\mathcal{S}(x,r(x))-\\mu}\\Bigg]\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "By the concavity o\u221af the function, diff\u221aerentiate with respect to $\\mu$ , we obtain that the supremum is achieved by ${u^{*}}=\\frac{\\sqrt{\\overline{{q}}(x,\\mathfrak{r}(x))}\\mathfrak{S}(x,\\mathfrak{r}(x))-\\sqrt{\\overline{{q}}(x,k_{\\operatorname*{min}}(x))}\\mathfrak{S}(x,k_{\\operatorname*{min}}(x))}{\\mathfrak{N}}$ . Plug in $\\mu^{*}$ , we obtain $\\begin{array}{r l}{\\mu}&{=-\\frac{}{\\sqrt{\\overline{{q}}(x,k_{\\operatorname*{min}}(x))}+\\sqrt{\\overline{{q}}(x,r(x))}}}\\end{array}$ ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta\\mathfrak{C}_{\\ell_{-\\mathrm{enp}},\\mathcal{H}}(h,x)}\\\\ &{:=\\frac{\\overline{{q}}(x,h_{\\mathrm{min}}(x))}{\\delta(x,h_{\\mathrm{min}}(x))}+\\frac{\\overline{{q}}(x,r(x)))}{\\delta(x,r(x)))}-\\frac{\\left(\\sqrt{\\overline{{q}}}(x,k_{\\mathrm{min}}(x))+\\sqrt{\\overline{{q}}(x,r(x)))}\\right)^{2}}{8(x,k_{\\mathrm{min}}(x))+8(x,r(x)))}}\\\\ &{\\geq\\left(\\sqrt{\\overline{{q}}(x,k_{\\mathrm{min}}(x))}-\\sqrt{\\overline{{q}}(x,r(x))}\\right)^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\mathrm{(minimu~is~achieved~when~\\mathcal{S}(x,r(x))=\\mathcal{S}(x,k_{\\mathrm{min}}(x))=\\frac{1}{2})}}\\\\ &{\\geq\\frac{\\overline{{(q}(x,r(x)))}-\\overline{{q}}(x,k_{\\mathrm{min}}(x)))^{2}}{\\left(\\sqrt{\\overline{{q}}(x,r(x))}\\right)+\\sqrt{\\overline{{q}}(x,k_{\\mathrm{min}}(x))}\\right)^{2}}}\\\\ &{\\geq\\frac{\\left(\\overline{{q}}(x,r(x))\\right)-\\overline{{q}}(x,k_{\\mathrm{min}}(x))\\right)^{2}}{4}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Therefore, by Lemma I.1, the conditional regret of the target cardinality aware loss function can be upper bounded as follows: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Delta\\mathfrak{C}_{\\ell,\\mathcal{K}}(r,x)=\\overline{{q}}(x,k_{\\operatorname*{min}}(x))-\\overline{{q}}(x,r(x))\\leq2\\Big(\\Delta\\mathfrak{C}_{\\widetilde{\\ell}_{\\mathrm{c-exp}},\\Re}(r,x)\\Big)^{\\frac{1}{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "By the concavity, taking expectations on both sides of the preceding equation, we obtain ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{\\ell}(r)-\\mathcal{E}_{\\ell}^{*}(\\mathcal{R})+\\mathcal{M}_{\\ell}(\\mathcal{R})\\leq2\\Big(\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{c-exp}}}(r)-\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{c-exp}}}^{*}(\\mathcal{R})+\\mathcal{M}_{\\widetilde{\\ell}_{\\mathrm{c-exp}}}(\\mathcal{R})\\Big)^{\\frac{1}{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "The second part follows from the fact that $\\mathcal{M}_{\\widetilde{\\ell}_{\\mathrm{c-exp}}}(\\mathcal{R}_{\\mathrm{all}})=0$ . ", "page_idx": 30}, {"type": "text", "text": "Case III: $\\widetilde{\\ell}_{\\mathrm{c-comp}}=\\widetilde{\\ell}_{\\mathrm{c-gce}}$ . For the cost-sensitive generalized cross-entropy loss $\\widetilde{\\ell}_{\\mathrm{c-gce}}$ , the conditional error can be written as ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathcal{C}_{\\tilde{\\ell}_{\\mathrm{c-gce}}}(r,x)=\\sum_{y\\in\\mathfrak{H}}p(x,y)\\sum_{k\\in\\mathcal{K}}\\overline{{c}}(x,k,y)\\frac{1}{q}\\bigg(1-\\bigg(\\frac{e^{r(x,k)}}{\\sum_{k^{\\prime}\\in\\mathcal{K}}e^{r(x,k^{\\prime})}}\\bigg)^{q}\\bigg)=\\frac{1}{q}\\sum_{k\\in\\mathcal{K}}(1-\\mathfrak{S}(x,k)^{q})\\overline{{q}}(x,k).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "The conditional regret can be written as ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathrm{\\partial}\\mathcal{C}_{\\widetilde{\\ell}_{-\\mathrm{gce}},\\mathcal{R}}(r,x)=\\frac{1}{q}\\sum_{k\\in\\mathcal{K}}\\big(1-\\mathcal{S}(x,k)^{q}\\big)\\overline{{q}}(x,k)-\\operatorname*{inf}_{r\\in\\mathcal{R}}\\bigg(\\frac{1}{q}\\sum_{k\\in\\mathcal{K}}(1-\\mathcal{S}(x,k)^{q})\\overline{{q}}(x,k)\\bigg)}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\geq\\frac{1}{q}\\sum_{k\\in\\mathcal{K}}(1-\\mathcal{S}(x,k)^{q})\\overline{{q}}(x,k)-\\operatorname*{inf}_{\\substack{\\mu\\in[-\\mathcal{S}(x,k_{\\operatorname*{min}}(x)),\\mathcal{S}(x,\\tau(x))]}}\\bigg(\\frac{1}{q}\\sum_{k\\in\\mathcal{K}}(1-\\mathcal{S}_{\\mu}(x,k)^{q})\\overline{{q}}(x,k)\\bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where for any $x\\in\\mathcal X$ $\\colon\\mathrm{and}\\ k\\ \\in\\mathcal{K},\\ \\vartheta_{\\mu}(x,k)=\\left\\{\\!\\!\\!\\begin{array}{l l}{\\!\\!\\otimes(x,y),}&{y\\not\\in\\{k_{\\operatorname*{min}}(x),\\mathsf{r}(x)\\}}\\\\ {\\!\\!\\otimes(x,k_{\\operatorname*{min}}(x))+\\mu}&{y=\\mathsf{r}(x)}\\\\ {\\!\\!\\otimes(x,\\mathsf{r}(x))-\\mu}&{y=k_{\\operatorname*{min}}(x).}\\end{array}\\!\\!\\right.\\mathrm{Note~that}$ such a choice of $\\mathcal{S}_{\\mu}$ leads to the following equality holds: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\sum_{k\\notin\\{r(x),k_{\\mathrm{min}}(x)\\}}\\frac{1}{q}\\sum_{k\\in\\mathcal{K}}(1-\\mathcal{S}(x,k)^{q})\\overline{{q}}(x,k)=\\sum_{k\\notin\\{r(x),k_{\\mathrm{min}}(x)\\}}\\frac{1}{q}\\sum_{k\\in\\mathcal{K}}(1-\\mathcal{S}_{\\mu}(x,k)^{q})\\overline{{q}}(x,k).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Therefore, the conditional regret of cost-sensitive generalized cross-entropy loss can be lower bounded as ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\Delta\\mathcal{C}_{\\widetilde{\\ell}_{\\mathrm{c-gce}},\\mathcal{R}}(h,x)=\\frac{1}{q}\\operatorname*{sup}_{\\substack{\\mu\\in[-\\delta(x,h_{\\operatorname*{min}}(x)),\\delta(x,r(x))]}}\\Bigg\\{\\overline{{q}}(x,k_{\\operatorname*{min}}(x))\\big[-\\mathfrak{S}(x,k_{\\operatorname*{min}}(x))^{q}+\\left(\\mathfrak{S}(x,r(x))-\\mu\\right)^{q}}\\\\ {\\qquad\\qquad\\qquad+\\overline{{q}}(x,\\mathfrak{r}(x))\\big[-\\mathfrak{S}(x,\\mathfrak{r}(x))^{q}+\\left(\\mathfrak{S}(x,k_{\\operatorname*{min}}(x))+\\mu\\right)^{q}\\big]\\Bigg\\}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "By the concavity of the function, differentiate with respect to $\\mu$ , we obtain that the supremum is achieved by $\\begin{array}{r}{\\mu^{*}\\stackrel{}{=}\\frac{\\overline{{q}}(x,\\mathsf{r}(x))^{\\frac{1}{1-q}}\\S(x,\\mathsf{r}(x))-\\overline{{q}}(x,k_{\\operatorname*{min}}(x))^{\\frac{1}{1-q}}\\S(\\dot{x_{\\iota}}k_{\\operatorname*{min}}(x))}{\\overline{{q}}(x,k_{\\operatorname*{min}}(x))^{\\frac{1}{1-q}}+\\overline{{q}}(x,\\mathsf{r}(x))^{\\frac{1}{1-q}}}}\\end{array}$ . Plug in $\\mu^{*}$ , we obtain ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta\\mathcal{C}_{\\tilde{\\ell}_{\\mathrm{c-gce}},\\mathcal{K}}(h,x)}\\\\ &{\\geq\\frac{1}{q}(\\vartheta(x,r(x))+\\vartheta(x,k_{\\operatorname*{min}}(x)))^{q}\\big(\\overline{{q}}(x,k_{\\operatorname*{min}}(x))^{\\frac{1}{1-q}}+\\overline{{q}}(x,r(x))^{\\frac{1}{1-q}}\\big)^{1-q}}\\\\ &{\\qquad-\\,\\frac{1}{q}\\overline{{q}}(x,k_{\\operatorname*{min}}(x))\\mathfrak{S}(x,k_{\\operatorname*{min}}(x))^{q}-\\frac{1}{q}\\overline{{q}}(x,r(x))\\mathfrak{S}(x,r(x))^{q}}\\\\ &{\\geq\\frac{1}{q|\\mathcal{K}|^{q}}\\bigg[2^{q}\\big(\\overline{{q}}(\\overline{{q}}(x,k_{\\operatorname*{min}}(x))^{\\frac{1}{1-q}}+\\overline{{q}}(x,r(x))^{\\frac{1}{1-q}}\\big)^{1-q}-\\overline{{q}}(x,k_{\\operatorname*{min}}(x))-\\overline{{q}}(x,r(x))\\big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\geq\\frac{\\big(\\overline{{q}}(x,r(x))-\\overline{{q}}(x,k_{\\operatorname*{min}}(x))\\big)^{2}}{4|\\mathcal{K}|^{q}}.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad(\\Big(\\frac{a^{\\frac{1}{1-q}}+b^{\\frac{1}{1-q}}}{2}\\Big)^{1-q}-\\frac{a+b}{2}\\geq\\frac{q}{4}(a-b)^{2},\\forall a,b\\in[0,1],0\\leq a+b\\leq1)}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Therefore, by Lemma I.1, the conditional regret of the target cardinality aware loss function can be upper bounded as follows: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Delta\\mathcal{C}_{\\ell,\\mathcal{K}}(r,x)=\\overline{{q}}(x,k_{\\operatorname*{min}}(x))-\\overline{{q}}(x,r(x))\\leq2|\\mathcal{K}|^{\\frac{q}{2}}\\Big(\\Delta\\mathcal{C}_{\\widetilde{\\ell}_{\\mathrm{c-gce}},\\mathcal{R}}(r,x)\\Big)^{\\frac{1}{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "By the concavity, taking expectations on both sides of the preceding equation, we obtain ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{\\ell}(r)-\\mathcal{E}_{\\ell}^{*}(\\mathcal{R})+\\mathcal{N}_{\\ell}(\\mathcal{R})\\le2|\\mathcal{K}|^{\\frac{q}{2}}\\Big(\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{c-gce}}}(r)-\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{c-gce}}}^{*}\\big(\\mathcal{R}\\big)+\\mathcal{N}_{\\widetilde{\\ell}_{\\mathrm{c-gce}}}\\left(\\mathcal{R}\\right)\\Big)^{\\frac{1}{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "The second part follows from the fact that $\\mathfrak{M}_{\\widetilde{\\ell}_{\\mathrm{c-gce}}}(\\mathcal{R}_{\\mathrm{all}})=0$ . ", "page_idx": 31}, {"type": "text", "text": "Case IV: $\\widetilde{\\ell}_{\\mathrm{c-comp}}=\\widetilde{\\ell}_{\\mathrm{c-mae}}$ . For the cost-sensitive mean absolute error loss $\\widetilde{\\ell}_{\\mathrm{c-mae}}$ , the conditional error can be written as ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathcal{C}_{\\widetilde{\\ell}_{\\mathrm{c-mae}}}(r,x)=\\sum_{y\\in\\mathfrak{Y}}p(x,y)\\sum_{k\\in\\mathfrak{X}}\\overline{{c}}(x,k,y)\\bigg(1-\\bigg(\\frac{e^{r(x,k)}}{\\sum_{k^{\\prime}\\in\\mathcal{K}}e^{r(x,k^{\\prime})}}\\bigg)\\bigg)=\\sum_{k\\in\\mathcal{K}}(1-\\mathfrak{S}(x,k))\\overline{{q}}(x,k).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "The conditional regret can be written as ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\i}\\bigcirc_{\\widetilde{\\ell}_{\\mathrm{c-mac}},\\mathcal{R}}(r,x)=\\displaystyle\\sum_{k\\in\\mathcal{K}}(1-\\vartheta(x,k))\\overline{{q}}(x,k)-\\operatorname*{inf}_{r\\in\\mathcal{R}}\\biggr(\\sum_{k\\in\\mathcal{K}}(1-\\vartheta(x,k))\\overline{{q}}(x,k)\\biggr)}\\\\ &{\\qquad\\qquad\\qquad\\geq\\displaystyle\\sum_{k\\in\\mathcal{K}}(1-\\vartheta(x,k))\\overline{{q}}(x,k)-\\operatorname*{inf}_{\\mu\\in[-\\vartheta(x,k_{\\operatorname*{min}}(x)),S(x,r(x))]}\\biggr(\\sum_{k\\in\\mathcal{K}}(1-\\vartheta_{\\mu}(x,k))\\overline{{q}}(x,k)\\biggr),}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where for any $x\\in\\mathcal{X}$ an $\\begin{array}{r}{1\\ k\\ \\in\\mathcal{K},\\ \\8_{\\mu}(x,k)\\,=\\,\\left\\{\\begin{array}{l l}{\\S(x,y),}&{y\\notin\\{k_{\\mathrm{min}}(x),\\mathsf{r}(x)\\}}\\\\ {\\S(x,k_{\\mathrm{min}}(x))+\\mu}&{y=\\mathsf{r}(x)}\\\\ {\\S(x,\\mathsf{r}(x))-\\mu}&{y=k_{\\mathrm{min}}(x).}\\end{array}\\right.\\mathrm{Note~that}}\\end{array}$ such a choice of $\\mathcal{S}_{\\mu}$ leads to the following equality holds: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\sum_{k\\in\\mathcal{K}}(1-\\mathcal{S}(x,k))\\overline{{q}}(x,k)=\\sum_{k\\in\\mathcal{K}}(1-\\mathcal{S}_{\\mu}(x,k))\\overline{{q}}(x,k).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Therefore, the conditional regret of cost-sensitive mean absolute error can be lower bounded as ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta\\mathfrak{C}_{\\widetilde{\\ell}_{\\mathrm{c-mac}},\\mathcal{K}}(h,x)\\geq\\operatorname*{sup}_{\\mu\\in[-\\delta(x,k_{\\operatorname*{min}}(x)),\\delta(x,r(x))]}\\Bigg\\{\\overline{{q}}(x,k_{\\operatorname*{min}}(x))[-\\vartheta(x,k_{\\operatorname*{min}}(x))+\\vartheta(x,r(x))-\\mu]}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,\\overline{{q}}(x,r(x))[-\\vartheta(x,r(x))+\\vartheta(x,k_{\\operatorname*{min}}(x))+\\mu]\\Bigg\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "By the concavity of the function, differentiate with respect to $\\mu$ , we obtain that the supremum is achieved by $\\mu^{*}=-\\mathcal{S}\\big(x,k_{\\mathrm{min}}(x)\\big)$ . Plug in $\\mu^{*}$ , we obtain ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta\\mathcal{C}_{\\widetilde{\\ell}_{\\mathrm{c-max}},\\mathcal{H}}(h,x)}\\\\ &{\\geq\\overline{{q}}(x,k_{\\operatorname*{min}}(x))\\mathfrak{S}(x,r(x))-\\overline{{q}}(x,r(x))\\mathfrak{S}(x,r(x))}\\\\ &{\\geq\\frac{1}{|\\mathcal{K}|}(\\overline{{q}}(x,k_{\\operatorname*{min}}(x))-\\overline{{q}}(x,r(x))).\\qquad\\qquad\\mathrm{(minimum~is~achieved~when~}\\mathfrak{S}(x,r(x))=\\frac{1}{|\\mathcal{K}|})}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Therefore, by Lemma I.1, the conditional regret of the target cardinality aware loss function can be upper bounded as follows: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\Delta\\mathcal{C}_{\\ell,\\mathcal{K}}(r,x)=\\overline{{q}}(x,k_{\\mathrm{min}}(x))-\\overline{{q}}(x,r(x))\\leq|\\mathcal{K}|\\big(\\Delta\\mathcal{C}_{\\widetilde{\\ell}_{\\mathrm{c-mae}},\\mathcal{R}}(r,x)\\big).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "By the concavity, taking expectations on both sides of the preceding equation, we obtain ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{\\ell}(r)-\\mathcal{E}_{\\ell}^{*}(\\mathcal{R})+\\mathcal{M}_{\\ell}(\\mathcal{R})\\leq|\\mathcal{K}|\\Big(\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{c-mae}}}(r)-\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{c-mae}}}^{*}(\\mathcal{R})+\\mathcal{N}_{\\widetilde{\\ell}_{\\mathrm{c-mae}}}(\\mathcal{R})\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "The second part follows from the fact that $\\mathcal{M}_{\\widetilde{\\ell}_{\\mathrm{c-mae}}}(\\mathcal{R}_{\\mathrm{all}})=0$ . ", "page_idx": 32}, {"type": "text", "text": "I.2 Proof of Theorem 4.7 ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "The conditional error for the cost-sensitive constrained loss can be expressed as follows: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathcal{\\mathbb{C}}_{\\widetilde{\\ell}_{\\mathrm{c-cstnd}}}(\\boldsymbol{r},\\boldsymbol{x})=\\sum_{y\\in\\mathfrak{Y}}p(x,y)\\widetilde{\\ell}_{\\mathrm{c-cstnd}}(\\boldsymbol{r},\\boldsymbol{x},y)}}\\\\ &{=\\displaystyle\\sum_{y\\in\\mathfrak{Y}}p(x,y)\\sum_{k\\in\\mathcal{K}}c(x,k,y)\\Phi(-r(x,k))}\\\\ &{=\\displaystyle\\sum_{k\\in\\mathcal{K}}\\widetilde{q}(\\boldsymbol{x},k)\\Phi(-r(x,k)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $\\begin{array}{r}{\\widetilde{q}(x,k)=\\sum_{y\\in\\mathbb{Y}}p(x,y)c(x,k,y)\\in[0,1]}\\end{array}$ . Let $k_{\\mathrm{min}}(x)=\\mathrm{argmin}_{k\\in\\mathcal{K}}\\,\\widetilde{q}(x,k)$ . We denote by $\\Phi_{\\mathrm{exp}}\\colon t\\mapsto e^{-t}$ the exponential loss function, $\\Phi_{\\mathrm{sq-hinge}}\\colon t\\mapsto\\operatorname*{max}\\{0,1-t\\}^{2}$ the squared hinge loss function, $\\Phi_{\\mathrm{hinge}}\\colon t\\mapsto\\operatorname*{max}\\{0,1-t\\}$ the hinge loss function, and $\\dot{\\Phi}_{\\rho};t\\mapsto\\operatorname*{min}\\{\\operatorname*{max}\\{0,1-t/\\rho\\},1\\}$ , $\\rho>0$ the $\\rho$ -margin loss function. ", "page_idx": 32}, {"type": "text", "text": "Theorem 4.7. Assume that $\\mathcal{R}$ is symmetric and complete. Then, the following bound holds for the cost-sensitive constrained loss: for all $r\\in\\mathcal{R}$ and for any distribution, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{\\ell}(r)-\\mathcal{E}_{\\ell}^{*}(\\mathcal{R})+\\mathcal{M}_{\\ell}(\\mathcal{R})\\leq\\gamma\\Big(\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{c-estnd}}}(r)-\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{c-estnd}}}^{*}(\\mathcal{R})+\\mathcal{M}_{\\widetilde{\\ell}_{\\mathrm{c-cstnd}}}(\\mathcal{R})\\Big);}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "When $\\mathcal{R}\\,=\\,\\mathcal{R}_{\\mathrm{all}},$ , the following holds: $\\begin{array}{r}{\\mathcal{E}_{\\ell}(r)-\\mathcal{E}_{\\ell}^{*}(\\mathcal{R}_{\\mathrm{all}})\\,\\leq\\,\\gamma\\big(\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{c-cstnd}}}(r)-\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{c-cstnd}}}^{*}(\\mathcal{R}_{\\mathrm{all}})\\big).}\\end{array}$ , where $\\gamma(t)=2\\sqrt{t}$ when $\\widetilde{\\ell}_{\\mathrm{c-cstnd}}$ is \u2113\u0303ccs\u2212tenxdp or \u2113\u0303c\u2212sq\u2212hinge; $\\gamma(t)=t$ when $\\widetilde{\\ell}_{\\mathrm{c-cstnd}}\\ i s\\ \\widetilde{\\ell}_{\\mathrm{c-hinge}}\\ o r\\,\\widetilde{\\ell}_{c-\\rho}$ . ", "page_idx": 32}, {"type": "text", "text": "Proof. Case I: $\\ell=\\widetilde{\\ell}_{\\mathrm{c-exp}}^{\\mathrm{cstnd}}$ . For the cost-sensitive constrained exponential loss $\\widetilde{\\ell}_{\\mathrm{c-exp}}^{\\mathrm{cstnd}}$ , the conditional regret can be written as ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta\\mathfrak{C}_{\\widetilde{\\ell}_{\\mathrm{c-exp}}^{\\mathrm{csted}},\\mathcal{R}}(r,x)=\\displaystyle\\sum_{k\\in\\mathcal{K}}\\widetilde{q}(x,k)\\Phi_{\\mathrm{exp}}(-r(x,k))-\\operatorname*{inf}_{r\\in\\mathcal{R}}\\displaystyle\\sum_{k\\in\\mathcal{K}}\\widetilde{q}(x,k)\\Phi_{\\mathrm{exp}}(-r(x,k))}\\\\ &{\\qquad\\qquad\\qquad\\geq\\displaystyle\\sum_{k\\in\\mathcal{K}}\\widetilde{q}(x,k)\\Phi_{\\mathrm{exp}}(-r(x,k))-\\operatorname*{inf}_{\\mu\\in\\mathbb{R}}\\displaystyle\\sum_{k\\in\\mathcal{K}}\\widetilde{q}(x,k)\\Phi_{\\mathrm{exp}}(-r_{\\mu}(x,k)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "equation", "text": "$$\n\\iota\\ k\\in\\mathcal{K},\\,r_{\\mu}(x,k)=\\left\\{\\!\\!\\!\\begin{array}{l l}{r(x,y),}&{y\\notin\\{k_{\\operatorname*{min}}(x),r(x)\\}}\\\\ {r(x,k_{\\operatorname*{min}}(x))+\\mu}&{y=r(x)}\\\\ {r(x,r(x))-\\mu}&{y=k_{\\operatorname*{min}}(x).}\\end{array}\\!\\!\\right.\\mathrm{Note~that~such~a~choice}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "equation", "text": "$$\n\\sum_{k\\not\\in\\{r(x),k_{\\mathrm{min}}(x)\\}}\\widetilde{q}(x,k)\\Phi_{\\mathrm{exp}}(-r(x,k))=\\sum_{k\\not\\in\\{r(x),k_{\\mathrm{min}}(x)\\}}\\sum_{k\\in\\mathcal{K}}\\widetilde{q}(x,k)\\Phi_{\\mathrm{exp}}(-r_{\\mu}(x,k)).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Therefore, the conditional regret of cost-sensitive constrained exponential loss can be lower bounded as ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\Delta\\lefteqn{\\mathbb{C}_{\\ell_{\\mathrm{c}}^{\\mathrm{stong}},\\mathcal{R}}^{\\infty}(\\boldsymbol{r},\\boldsymbol{x})}}\\\\ &{\\geq\\operatorname*{inf}_{t\\in\\mathbb{R}}\\operatorname*{sup}\\left\\{\\widetilde{q}(\\boldsymbol{x},\\boldsymbol{k}_{\\mathrm{min}}(\\boldsymbol{x}))\\big(e^{r(\\boldsymbol{x},\\boldsymbol{k}_{\\mathrm{min}}(\\boldsymbol{x}))}-e^{r(\\boldsymbol{x},\\boldsymbol{r}(\\boldsymbol{x}))-\\boldsymbol{\\mu}}\\big)+\\widetilde{q}(\\boldsymbol{x},\\boldsymbol{r}(\\boldsymbol{x}))\\big(e^{r(\\boldsymbol{x},\\boldsymbol{r}(\\boldsymbol{x}))}-e^{r(\\boldsymbol{x},\\boldsymbol{k}_{\\mathrm{min}}(\\boldsymbol{x}))+\\boldsymbol{\\mu}}\\big)\\right\\}}\\\\ &{=\\left(\\sqrt{\\widetilde{q}(\\boldsymbol{x},\\boldsymbol{k}_{\\mathrm{min}}(\\boldsymbol{x}))}-\\sqrt{\\widetilde{q}(\\boldsymbol{x},\\boldsymbol{r}(\\boldsymbol{x}))}\\right)^{2}}&{\\mathrm{(differentiating~with~respect~to~}\\boldsymbol{\\mu},\\ r\\mathrm{~to~optimize)}}\\\\ &{=\\left(\\frac{\\widetilde{q}(\\boldsymbol{x},\\boldsymbol{r}(\\boldsymbol{x}))-\\widetilde{q}(\\boldsymbol{x},\\boldsymbol{k}_{\\mathrm{min}}(\\boldsymbol{x}))}{\\sqrt{\\widetilde{q}(\\boldsymbol{x},\\boldsymbol{k}_{\\mathrm{min}}(\\boldsymbol{x}))}+\\sqrt{\\widetilde{q}(\\boldsymbol{x},\\boldsymbol{r}(\\boldsymbol{x}))}}\\right)^{2}}\\\\ &{\\geq\\frac{1}{4}(\\widetilde{q}(\\boldsymbol{x},\\boldsymbol{r}(\\boldsymbol{x}))-\\widetilde{q}(\\boldsymbol{x},\\boldsymbol{k}_{\\mathrm{min}}(\\boldsymbol{x})))^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Therefore, by Lemma I.1, the conditional regret of the target cardinality aware loss function can be upper bounded as follows: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Delta\\mathcal{C}_{\\ell,\\mathcal{H}}(r,x)=\\widetilde{q}(x,r(x))-\\widetilde{q}(x,k_{\\operatorname*{min}}(x))\\le2\\Big(\\Delta\\mathcal{C}_{\\widetilde{\\ell}_{\\mathrm{c-exp}}^{\\mathrm{cstnd}},\\mathcal{R}}(r,x)\\Big)^{\\frac{1}{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "By the concavity, taking expectations on both sides of the preceding equation, we obtain ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{\\ell}(r)-\\mathcal{E}_{\\ell}^{*}(\\mathcal{R})+\\mathcal{M}_{\\ell}(\\mathcal{R})\\leq2\\bigg(\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{c-exp}}^{\\mathrm{cstnd}}}\\left(r\\right)-\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{c-exp}}^{\\mathrm{cstnd}}}^{*}\\left(\\mathcal{R}\\right)+\\mathcal{M}_{\\widetilde{\\ell}_{\\mathrm{c-exp}}^{\\mathrm{cstnd}}}\\left(\\mathcal{R}\\right)\\bigg)^{\\frac{1}{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "The second part follows from the fact that $\\mathcal{M}_{\\widetilde{\\ell}_{\\mathrm{c-exp}}^{\\mathrm{cstnd}}}\\left(\\mathcal{R}_{\\mathrm{all}}\\right)=0$ . ", "page_idx": 33}, {"type": "text", "text": "Case II: $\\ell\\;=\\;\\widetilde{\\ell}_{c-\\mathrm{sq-hinge}}$ . For the cost-sensitive constrained squared hinge loss $\\widetilde{\\ell}_{c-\\mathrm{sq-hinge}}$ , the conditional regret can be written as ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Delta\\mathfrak{C}_{\\widetilde{\\ell}_{-\\infty-\\mathrm{sq-hinge}},\\mathfrak{R}}(r,x)=\\displaystyle\\sum_{k\\in\\mathcal{K}}\\widetilde{q}(x,k)\\Phi_{\\mathrm{sq-hinge}}(-r(x,k))-\\operatorname*{inf}_{r\\in\\mathcal{R}}\\displaystyle\\sum_{k\\in\\mathcal{K}}\\widetilde{q}(x,k)\\Phi_{\\mathrm{sq-hinge}}(-r(x,k))}\\\\ {\\geq\\displaystyle\\sum_{k\\in\\mathcal{K}}\\widetilde{q}(x,k)\\Phi_{\\mathrm{sq-hinge}}(-r(x,k))-\\operatorname*{inf}_{\\mu\\in\\mathbb{R}}\\displaystyle\\sum_{k\\in\\mathcal{K}}\\widetilde{q}(x,k)\\Phi_{\\mathrm{sq-hinge}}(-r_{\\mu}(x,k)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where for any $k\\in\\mathcal{K}$ , ", "page_idx": 33}, {"type": "equation", "text": "$$\nr_{\\mu}(x,k)=\\left\\{\\!\\!\\begin{array}{l l}{{r(x,y),}}&{{y\\notin\\{k_{\\operatorname*{min}}(x),r(x)\\}}}\\\\ {{r(x,k_{\\operatorname*{min}}(x))+\\mu}}&{{y=r(x)}}\\\\ {{r(x,r(x))-\\mu}}&{{y=k_{\\operatorname*{min}}(x).}}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Note that such a choice of $r_{\\mu}$ leads to the following equality holds: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\sum_{k\\ell\\{r(x),k_{\\mathrm{min}}(x)\\}}\\widetilde{q}(x,k)\\Phi_{\\mathrm{sq-hinge}}(-r(x,k))=\\sum_{k\\ell\\{r(x),k_{\\mathrm{min}}(x)\\}}\\sum_{k\\in\\mathcal{K}}\\widetilde{q}(x,k)\\Phi_{\\mathrm{sq-hinge}}(-r_{\\mu}(x,k)).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Therefore, the conditional regret of cost-sensitive constrained squared hinge loss can be lower bounded as ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta\\mathfrak{C}_{\\ell_{\\mathrm{c}-8\\mathrm{t-hinge}},\\mathfrak{R}}(r,x)}\\\\ &{\\geq\\underset{r\\in\\mathcal{R}}{\\operatorname*{inf}}\\,\\operatorname*{sup}\\bigg\\{\\widetilde{q}(x,k_{\\operatorname*{min}}(x))\\Big(\\operatorname*{max}\\{0,1+r(x,k_{\\operatorname*{min}}(x))\\}^{2}-\\operatorname*{max}\\{0,1+r(x,r(x))-\\mu\\}^{2}\\Big)}\\\\ &{\\qquad+\\,\\widetilde{q}(x,r(x))\\Big(\\operatorname*{max}\\{0,1+r(x,r(x))\\}^{2}-\\operatorname*{max}\\{0,1+r(x,k_{\\operatorname*{min}}(x))+\\mu\\}^{2}\\Big)\\bigg\\}}\\\\ &{\\geq\\frac{1}{4}(\\widetilde{q}(x,k_{\\operatorname*{min}}(x))-\\widetilde{q}(x,r(x)))^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Therefore, by Lemma I.1, the conditional regret of the target cardinality aware loss function can be upper bounded as follows: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Delta\\mathcal{C}_{\\ell,\\mathcal{K}}(r,x)=\\widetilde{q}(x,r(x))-\\widetilde{q}(x,k_{\\mathrm{min}}(x))\\leq2\\Big(\\Delta\\mathcal{C}_{\\widetilde{\\ell}_{c-\\mathrm{sq-hinge}},\\Re}(r,x)\\Big)^{\\frac{1}{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "By the concavity, taking expectations on both sides of the preceding equation, we obtain ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{\\ell}(r)-\\mathcal{E}_{\\ell}^{*}(\\mathcal{R})+\\mathfrak{R}_{\\ell}(\\mathcal{R})\\le2\\Big(\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{c-sq-hinge}}}(r)-\\mathcal{E}_{\\widetilde{\\ell}_{\\mathrm{c-sq-hinge}}}^{*}(\\mathcal{R})+\\mathfrak{M}_{\\widetilde{\\ell}_{\\mathrm{c-sq-hinge}}}(\\mathcal{R})\\Big)^{\\frac{1}{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "The second part follows from the fact that M\u2113\u0303c\u2212sq\u2212hinge $\\begin{array}{r}{\\mathfrak{M}_{\\widetilde{\\ell}_{c-\\mathrm{sq-hinge}}}(\\mathcal{R}_{\\mathrm{all}})=0.}\\end{array}$ . ", "page_idx": 34}, {"type": "text", "text": "Case III: $\\ell=\\widetilde{\\ell}_{c-\\mathrm{hinge}}$ . For the cost-sensitive constrained hinge loss $\\widetilde{\\ell}_{c-\\mathrm{hinge}}$ , the conditional regret can be written as ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Delta\\mathfrak{C}_{\\widetilde{\\ell}_{\\mathrm{c-hinge}},\\mathcal{R}}(r,x)=\\displaystyle\\sum_{k\\in\\mathcal{K}}\\widetilde{q}(x,k)\\Phi_{\\mathrm{hinge}}(-r(x,k))-\\operatorname*{inf}_{r\\in\\mathcal{R}}\\displaystyle\\sum_{k\\in\\mathcal{K}}\\widetilde{q}(x,k)\\Phi_{\\mathrm{hinge}}(-r(x,k))}\\\\ {\\geq\\displaystyle\\sum_{k\\in\\mathcal{K}}\\widetilde{q}(x,k)\\Phi_{\\mathrm{hinge}}(-r(x,k))-\\operatorname*{inf}_{\\mu\\in\\mathbb{R}}\\displaystyle\\sum_{k\\in\\mathcal{K}}\\widetilde{q}(x,k)\\Phi_{\\mathrm{hinge}}(-r_{\\mu}(x,k)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where for any $k\\in\\mathcal{K}$ , ", "page_idx": 34}, {"type": "equation", "text": "$$\nr_{\\mu}(x,k)=\\left\\{\\!\\!\\begin{array}{l l}{{r(x,y),}}&{{y\\notin\\{k_{\\operatorname*{min}}(x),r(x)\\}}}\\\\ {{r(x,k_{\\operatorname*{min}}(x))+\\mu}}&{{y=r(x)}}\\\\ {{r(x,r(x))-\\mu}}&{{y=k_{\\operatorname*{min}}(x).}}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Note that such a choice of $r_{\\mu}$ leads to the following equality holds: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\sum_{k\\not\\in\\{r(x),k_{\\mathrm{min}}(x)\\}}\\widetilde{q}(x,k)\\Phi_{\\mathrm{hinge}}(-r(x,k))=\\sum_{k\\not\\in\\{r(x),k_{\\mathrm{min}}(x)\\}}\\sum_{k\\in\\mathcal{K}}\\widetilde{q}(x,k)\\Phi_{\\mathrm{hinge}}(-r_{\\mu}(x,k)).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Therefore, the conditional regret of cost-sensitive constrained hinge loss can be lower bounded as ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta\\mathfrak{C}_{\\widetilde{\\ell}_{c-\\mathrm{hinge}},\\Re}(r,x)}\\\\ &{\\geq\\operatorname*{inf}_{r\\in\\mathbb{R}}\\operatorname*{sup}\\bigg\\{q(x,k_{\\operatorname*{min}}(x))(\\operatorname*{max}\\{0,1+r(x,k_{\\operatorname*{min}}(x))\\}-\\operatorname*{max}\\{0,1+r(x,r(x))-\\mu\\})}\\\\ &{\\qquad+q(x,r(x))(\\operatorname*{max}\\{0,1+r(x,r(x))\\}-\\operatorname*{max}\\{0,1+r(x,k_{\\operatorname*{min}}(x))+\\mu\\})\\bigg\\}}\\\\ &{\\geq q(x,r(x))-q(x,k_{\\operatorname*{min}}(x)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Therefore, by Lemma I.1, the conditional regret of the target cardinality aware loss function can be upper bounded as follows: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Delta\\mathfrak{C}_{\\ell,\\mathcal{H}}(r,x)=\\widetilde{q}(x,r(x))-\\widetilde{q}(x,k_{\\mathrm{min}}(x))\\leq\\Delta\\mathfrak{C}_{\\widetilde{\\ell}_{c-\\mathrm{hinge}},\\mathfrak{R}}(r,x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "By the concavity, taking expectations on both sides of the preceding equation, we obtain ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{\\ell}(r)-\\mathcal{E}_{\\ell}^{*}(\\mathcal{R})+\\mathcal{M}_{\\ell}(\\mathcal{R})\\leq\\mathcal{E}_{\\widetilde{\\ell}_{c-\\mathrm{hinge}}}(r)-\\mathcal{E}_{\\widetilde{\\ell}_{c-\\mathrm{hinge}}}^{*}(\\mathcal{R})+\\mathcal{N}_{\\widetilde{\\ell}_{c-\\mathrm{hinge}}}(\\mathcal{R}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "The second part follows from the fact that $\\mathcal{M}_{{\\widetilde{\\ell}_{c-\\mathrm{hinge}}}}(\\mathcal{R}_{\\mathrm{all}})=0$ ", "page_idx": 34}, {"type": "text", "text": "Case IV: $\\ell=\\widetilde{\\ell}_{c-\\rho}$ . For the cost-sensitive constrained $\\rho$ -margin loss $\\widetilde{\\ell}_{c-\\rho}$ , the conditional regret can be written as ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta\\mathfrak{C}_{\\widetilde{\\ell}_{c-\\rho},\\mathfrak{R}}(r,x)=\\displaystyle\\sum_{k\\in\\mathcal{K}}\\widetilde{q}(x,k)\\Phi_{\\rho}(-r(x,k))-\\operatorname*{inf}_{r\\in\\mathcal{R}}\\displaystyle\\sum_{k\\in\\mathcal{K}}\\widetilde{q}(x,k)\\Phi_{\\rho}(-r(x,k))}\\\\ &{\\qquad\\qquad\\qquad\\geq\\displaystyle\\sum_{k\\in\\mathcal{K}}\\widetilde{q}(x,k)\\Phi_{\\rho}(-r(x,k))-\\operatorname*{inf}_{\\mu\\in\\mathbb{R}}\\displaystyle\\sum_{k\\in\\mathcal{K}}\\widetilde{q}(x,k)\\Phi_{\\rho}(-r_{\\mu}(x,k)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where for any $k\\in\\mathcal{K}$ , ", "page_idx": 35}, {"type": "equation", "text": "$$\nr_{\\mu}(x,k)=\\left\\{\\!\\!\\begin{array}{l l}{{r(x,y),}}&{{y\\notin\\{k_{\\operatorname*{min}}(x),r(x)\\}}}\\\\ {{r(x,k_{\\operatorname*{min}}(x))+\\mu}}&{{y=r(x)}}\\\\ {{r(x,r(x))-\\mu}}&{{y=k_{\\operatorname*{min}}(x).}}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Note that such a choice of $r_{\\mu}$ leads to the following equality holds: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\sum_{k\\not\\in\\{r(x),k_{\\operatorname*{min}}(x)\\}}\\widetilde{q}(x,k)\\Phi_{\\rho}(-r(x,k))=\\sum_{k\\not\\in\\{r(x),k_{\\operatorname*{min}}(x)\\}}\\sum_{k\\in\\mathcal{K}}\\widetilde{q}(x,k)\\Phi_{\\rho}(-r_{\\mu}(x,k)).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Therefore, the conditional regret of cost-sensitive constrained $\\rho$ -margin loss can be lower bounded as ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\Delta}\\mathfrak{C}_{\\tilde{\\ell}_{-\\rho},\\Re}(r,x)}\\\\ &{\\geq\\underset{r\\in\\mathcal{R}}{\\operatorname*{inf}}\\,\\underset{\\mu\\in\\mathbb{R}}{\\operatorname*{sup}}\\,\\bigg\\{\\widetilde{q}(x,k_{\\operatorname*{min}}(x))\\bigg(\\operatorname*{min}\\bigg\\{\\operatorname*{max}\\bigg\\{0,1+\\frac{r(x,k_{\\operatorname*{min}}(x))}{\\rho}\\bigg\\},1\\bigg\\}-\\operatorname*{min}\\biggl\\{\\operatorname*{max}\\bigg\\{0,1+\\frac{r(x,r(x))-\\mu}{\\rho}\\bigg\\}\\bigg\\}}\\\\ &{+\\widetilde{q}(x,r(x))\\bigg(\\operatorname*{min}\\Biggl\\{\\operatorname*{max}\\bigg\\{0,1+\\frac{r(x,r(x))}{\\rho}\\bigg\\},1\\bigg\\}-\\operatorname*{min}\\Biggl\\{\\operatorname*{max}\\bigg\\{0,1+\\frac{r(x,k_{\\operatorname*{min}}(x))+\\mu}{\\rho}\\bigg\\},1\\bigg\\}\\bigg)\\bigg\\}}\\\\ &{\\geq\\widetilde{q}(x,r(x))-\\widetilde{q}(x,k_{\\operatorname*{min}}(x)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Therefore, by Lemma I.1, the conditional regret of the target cardinality aware loss function can be upper bounded as follows: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Delta\\mathcal{C}_{\\ell,\\mathcal{K}}(r,x)=\\widetilde{q}(x,r(x))-\\widetilde{q}(x,k_{\\mathrm{min}}(x))\\leq\\Delta\\mathcal{C}_{\\widetilde{\\ell}_{c-\\rho},\\mathcal{R}}(r,x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "By the concavity, taking expectations on both sides of the preceding equation, we obtain ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{\\ell}(r)-\\mathcal{E}_{\\ell}^{*}(\\mathcal{R})+\\mathcal{M}_{\\ell}(\\mathcal{R})\\leq\\mathcal{E}_{\\widetilde{\\ell}_{c-\\rho}}(r)-\\mathcal{E}_{\\widetilde{\\ell}_{c-\\rho}}^{*}(\\mathcal{R})+\\mathcal{M}_{\\widetilde{\\ell}_{c-\\rho}}(\\mathcal{R}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "The second part follows from the fact that $\\mathfrak{M}_{\\widetilde{\\ell}_{c-\\rho}}(\\mathcal{R}_{\\mathrm{all}})=0$ . ", "page_idx": 35}, {"type": "image", "img_path": "WAT3qu737X/tmp/b620722d9269c0aeafd06c0ed1fbfcc83dff295e1979aa3b9cce3ba4f827604c.jpg", "img_caption": ["Figure 5: Accuracy versus cardinality on various datasets for $\\mathsf{c o s t}\\big(\\vert\\mathbf{g}_{k}(x)\\vert\\big)=\\log k$ . Each curve of the cardinality-aware algorithm is for a fixed value of $\\lambda$ and the points on the curve are obtained by varying the number of experts. "], "img_footnote": [], "page_idx": 36}, {"type": "image", "img_path": "WAT3qu737X/tmp/191492f5b57a5d9490281ed94564e779a7749b144f41d5ca313be3ba2ad7bd14.jpg", "img_caption": ["Figure 6: Accuracy versus cardinality on various datasets for $\\mathsf{c o s t}(|\\mathsf{g}_{k}(x)|)\\;\\;=\\;\\;\\log k$ and c $\\mathsf{o s t}(|\\mathbf{g}_{k}(x)|)\\,=\\,k$ , with $\\lambda\\,=\\,0.05$ . The points on each curve of the cardinality-aware algorithm are obtained by varying the number of experts. "], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "J Additional experimental results: top- $k$ classifiers ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Here, we report additional experimental results with different choices of set $\\mathcal{K}$ and cost $(|\\mathtt{g}_{k}(x)|)$ on benchmark datasets CIFAR-10, CIFAR-100 [Krizhevsky, 2009], SVHN [Netzer et al., 2011], and ImageNet [Deng et al., 2009] and show that our cardinality-aware algorithm consistently outperforms top- $k$ classifiers across all configurations. ", "page_idx": 36}, {"type": "text", "text": "In Figure 5 and Figure 6, we began with a set $\\mathcal{K}=\\{1\\}$ for the loss function and then progressively expanded it by adding choices of larger cardinality, each of which doubles the largest value currently in $\\mathcal{K}$ . The largest set $\\mathcal{K}$ for the CIFAR-100 and ImageNet datasets is $\\{1,2,4,8,16,32,64\\}$ , whereas for the CIFAR-10 and SVHN datasets, it is $\\{1,2,4,8\\}$ . As the set $\\mathcal{K}$ expands, there is an increase in both the average cardinality and the accuracy. Figure 5 shows that the accuracy versus cardinality curve of the cardinality-aware algorithm is above that of top- $k$ classifiers for various values of $\\lambda$ . Figure 6 presents the comparison of $\\mathsf{c o s t}(|\\mathbf{g}_{k}(x)|)=k$ and $\\mathsf{c o s t}\\big(|\\mathbf{g}_{k}(x)|\\big)=\\log k$ for $\\lambda=0.05$ . These results demonstrate that different $\\lambda$ and different cost $(|\\mathbf{g}_{k}(x)|)$ basically lead to the same curve, which verifies the effectiveness and benefit of our algorithm. ", "page_idx": 36}, {"type": "image", "img_path": "WAT3qu737X/tmp/3f6fd542e1f9164de0feabd31b3506e466dfa9d6f1f064994363f5611062cac4.jpg", "img_caption": ["Figure 7: Accuracy versus cardinality on an artificial dataset for different training sample sizes $m$ . "], "img_footnote": [], "page_idx": 37}, {"type": "text", "text": "", "page_idx": 37}, {"type": "text", "text": "K Additional experimental results: threshold-based classifiers ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "We first characterize the Bayes predictor $r^{*}$ in this setting. We say that the scenario is deterministic if for all $x\\in\\mathcal X$ , there exists some true label $y\\in\\mathcal{Y}$ such that $p(x,y)\\,=\\,1$ ; otherwise, we say that the scenario is stochastic. To simplify the discussion, we will assume that $|\\mathsf{g}_{k}(x)|$ is an increasing function of $k$ , for any $x$ . ", "page_idx": 37}, {"type": "text", "text": "Lemma K.1. Consider the deterministic scenario. Assume that $\\lambda\\mathsf{c o s t}(|\\mathbf{g}_{k}(x)|)\\leq1$ for all $k$ and $x\\in\\mathcal X$ . Then, the Bayes predictor $r^{*}$ for the cardinality-aware loss function $\\ell$ satisfies: $r^{\\ast}(x)=$ $\\textstyle\\operatorname{argmin}_{k:y\\in{\\bf g}_{k}(x)}k$ , that is the smallest $k$ such that the true label $y$ is in ${\\mathsf{g}}_{k}(x)$ . ", "page_idx": 37}, {"type": "text", "text": "Proof. By the assumption, for $k<\\mathsf{r}^{*}(x)$ , we can write $c(x,\\mathsf{r}^{*}(x),\\mathsf{y})\\,=\\,\\lambda\\mathsf{c o s t}\\bigl(\\bigl|\\mathsf{g}_{\\mathsf{r}^{*}(x)}(x)\\bigr|\\bigr)\\,\\leq\\,1\\,\\leq$ $1_{y\\notin\\mathbf{g}_{k}(x)}+\\lambda\\mathsf{c o s t}\\big(|\\mathbf{g}_{k}(x)|\\big)=c(x,k,y)$ . Furthermore, since $|g_{k}(x)|$ is an increasing function of $k$ , we have $c(x,\\mathsf{r}^{*}(x),y)=\\lambda\\mathsf{c o s t}(\\big|\\mathsf{g}_{r^{*}(x)}(x)\\big|)\\leq\\lambda\\mathsf{c o s t}(\\vert\\mathsf{g}_{k}^{\\prime}(x)\\vert)=c(x,k^{\\prime},y)$ for $k^{\\prime}>\\mathsf{r}^{*}(x)$ . \u53e3 ", "page_idx": 37}, {"type": "text", "text": "Lemma K.2. Consider the stochastic scenario. The Bayes predictor $r^{*}$ for the cardinality-aware loss function $\\ell$ satisfies: ", "page_idx": 37}, {"type": "equation", "text": "$$\nr^{*}(x)=\\underset{k\\in\\mathcal{K}}{\\mathrm{argmin}}\\Bigg(\\lambda\\mathrm{cost}(\\left|\\mathbf{g}_{k}(x)\\right|)-\\sum_{y\\in\\mathbf{g}_{k}(x)}p(x,y)\\Bigg).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof. The conditional error can be written as follows: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{C}_{\\ell}(r,x,y)=\\displaystyle\\sum_{y\\in\\mathfrak{Y}}p(x,y)c(x,\\mathfrak{r}(x),y)}\\\\ &{\\qquad\\qquad=\\displaystyle\\sum_{y\\in\\mathfrak{Y}}p(x,y)\\Big(1_{y\\notin_{\\mathfrak{r}(x)}(x)}+\\lambda\\mathrm{cost}(\\big|\\mathrm{g}_{r(x)}(x)\\big|)\\Big)}\\\\ &{\\qquad\\qquad=\\displaystyle\\sum_{y\\in\\mathfrak{Y}}p(x,y)1_{y\\notin_{\\mathfrak{r}(x)}(x)}+\\lambda\\mathrm{cost}(\\big|\\mathrm{g}_{r(x)}(x)\\big|)}\\\\ &{\\qquad=1-\\displaystyle\\sum_{y\\in\\mathfrak{r}_{\\mathfrak{r}(x)}(x)}p(x,y)+\\lambda\\mathrm{cost}(\\big|\\mathrm{g}_{r(x)}(x)\\big|).}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Thus, the Bayes predictor can be characterized as ", "page_idx": 37}, {"type": "equation", "text": "$$\nr^{*}(x)=\\underset{k\\in\\mathcal{K}}{\\mathrm{argmin}}\\Bigg(\\lambda\\mathrm{cost}(|\\mathbf{g}_{k}(x)|)-\\sum_{y\\in\\mathbf{g}_{k}(x)}p(x,y)\\Bigg).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "image", "img_path": "WAT3qu737X/tmp/e3b9d42fbb954b7c1f87a5c2e6ed6ac6c730effff72297911a601cf44856370d.jpg", "img_caption": ["Figure 8: Accuracy versus cardinality on CIFAR-100, ImageNet, CIFAR-10, and SVHN datasets. "], "img_footnote": [], "page_idx": 38}, {"type": "text", "text": "It is clear that Lemma K.2 implies Lemma K.1 when there exists some true $y\\in\\big y$ such that $p(x,y)=1$ and $\\lambda\\mathsf{c o s t}(|\\mathbf{g}_{k}(x)|)\\leq1$ . ", "page_idx": 38}, {"type": "text", "text": "We first consider an artificial dataset containing 10 classes. Each class is modeled by a Gaussian distribution in a 100-dimensional space. As in Section 5, we plot the accuracy versus cardinality curve of the cardinality-aware algorithm by varying $\\lambda$ , where the set predictors used are threshold-based classifiers, and compare with that of conformal prediction. In Figure 7, we also indicate the point corresponding to $r^{*}$ . The problem is close to being realizable, as we can train a predictor that performs almost as well as $r^{*}$ on the test set. Thus, the minimizability gaps vanish, and our $\\mathcal{H}$ -consistency bounds (Theorems 4.6 and 4.7) then suggest that with sufficient training data, we can get close to the optimal solution and therefore outperform conformal prediction. For some tasks, however, the problem is hard, and it appears that a very large training sample would be needed. Figure 7 demonstrates that on the artificial dataset, with training sample size $m=50{,}000$ , the performance of our cardinality-aware algorithm is only slightly better than that of conformal prediction. If we increase the training sample size to $m=500{,}000$ , then the curve of our algorithm becomes much closer to the optimal point and significantly outperforms conformal prediction. ", "page_idx": 38}, {"type": "text", "text": "Additionally, for a weaker scoring function, a smaller training sample suffices in many cases, and our cardinality-aware algorithm can outperform conformal prediction on real datasets as shown in Figure 8. ", "page_idx": 38}, {"type": "text", "text": "L Future work ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "While our framework of cardinality-aware set prediction is very general\u2014applicable to any collection of set predictors (Section 2)\u2014and leads to novel cardinality-aware algorithms (Section 3), benefits from theoretical guarantees with sufficient training data (Section 4), and demonstrates effectiveness and empirical advantages in top- $k$ classification (Section 5), the learning problem can be challenging for certain tasks, often requiring a very large training sample (as shown in Appendix K). This underscores the need for a more detailed investigation to enhance our algorithms in these scenarios. ", "page_idx": 38}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: See lines 81-86 in Section 1. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 39}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Justification: See Appendix L. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 39}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: See Section 4, Appendix A, B, C, D, E, F, H, and I. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 40}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: See Section 3, Section 5, Appendix J and Appendix K. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 40}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: See Section 5, Appendix J and Appendix K. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 41}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: See Section 5, Appendix J and Appendix K. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 41}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Justification: See Figure 2, Figure 5, and Figure 6. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 41}, {"type": "text", "text": "", "page_idx": 42}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: For each model training, we use an Nvidia A100 GPU. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 42}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: The authors have reviewed the NeurIPS Code of Ethics. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 42}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: Our work is theoretical in nature and we do not anticipate any immediate negative societal impact. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 42}, {"type": "text", "text": "", "page_idx": 43}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 43}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: See Section 5, Appendix J and Appendix K. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 43}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 44}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 44}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 44}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 44}]