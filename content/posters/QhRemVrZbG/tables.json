[{"figure_path": "QhRemVrZbG/tables/tables_5_1.jpg", "caption": "Table 1: Accuracy (%) with Different ICVs Methods and Finetuning Methods, where numbers in parentheses indicate multiples of LIVE trainable parameters.", "description": "This table compares the performance of different In-Context Vector (ICV) methods and a fine-tuning method (LoRA) on two Visual Question Answering (VQA) datasets: VQAv2 and OKVQA.  The methods compared include a zero-shot baseline, 32-shot In-Context Learning (ICL), three non-learnable ICV methods (Task Vector, Function Vector, and PCA-ICV), and LoRA. The table shows the accuracy achieved by each method and the total number of trainable parameters used.  The numbers in parentheses indicate how many times larger the trainable parameters are compared to LIVE (Ours).", "section": "4.2 Results"}, {"figure_path": "QhRemVrZbG/tables/tables_6_1.jpg", "caption": "Table 2: Accuracy (%) of LIVE with Different Training Loss on VQA.", "description": "This table presents the accuracy achieved by the LIVE model on two VQA datasets (VQAv2 and OKVQA) when trained using different loss functions.  It compares the performance using only the KL divergence loss (Ld), only the ground truth loss (Lgt), and a combined loss (L) that balances both.  The results demonstrate the impact of different loss functions on the model's accuracy.", "section": "4.3 Ablation Studies"}, {"figure_path": "QhRemVrZbG/tables/tables_6_2.jpg", "caption": "Table 1: Accuracy (%) with Different ICVs Methods and Finetuning Methods, where numbers in parentheses indicate multiples of LIVE trainable parameters.", "description": "This table compares the accuracy of different in-context learning (ICL) methods and a fine-tuning method (LoRA) on two Visual Question Answering (VQA) datasets: VQAv2 and OKVQA.  The methods compared include a baseline zero-shot approach, 32-shot ICL, three non-learnable In-Context Vector (ICV) methods (Task Vector, Function Vector, PCA-ICV), and the proposed Learnable In-Context Vector (LIVE) method. The table also shows the number of trainable parameters for each method, relative to the number of trainable parameters in LIVE.", "section": "4.2 Results"}, {"figure_path": "QhRemVrZbG/tables/tables_7_1.jpg", "caption": "Table 1: Accuracy (%) with Different ICVs Methods and Finetuning Methods, where numbers in parentheses indicate multiples of LIVE trainable parameters.", "description": "This table compares the accuracy of different in-context learning (ICL) methods on the VQAv2 and OKVQA datasets.  The methods compared include Zero-Shot (no in-context examples), 32-shot ICL (32 in-context demonstrations), three non-learnable ICV (In-Context Vector) methods (Task Vector, Function Vector, PCA-ICV), LoRA (a finetuning method), and the proposed LIVE method.  The numbers in parentheses indicate how many times larger the model's trainable parameters are compared to LIVE's.", "section": "4.2 Results"}, {"figure_path": "QhRemVrZbG/tables/tables_8_1.jpg", "caption": "Table 1: Accuracy (%) with Different ICVs Methods and Finetuning Methods, where numbers in parentheses indicate multiples of LIVE trainable parameters.", "description": "This table compares the accuracy of different methods for visual question answering (VQA) on two datasets: VQAv2 and OKVQA.  The methods compared include Zero-Shot (no context), 32-shot ICL (conventional in-context learning with 32 demonstrations), three non-learnable In-Context Vector (ICV) methods (Task Vector, Function Vector, PCA-ICV), LoRA (a parameter-efficient fine-tuning method), and the proposed LIVE method. The table shows the accuracy achieved by each method and the number of trainable parameters used (relative to the number used in LIVE).", "section": "4.2 Results"}, {"figure_path": "QhRemVrZbG/tables/tables_8_2.jpg", "caption": "Table 1: Accuracy (%) with Different ICVs Methods and Finetuning Methods, where numbers in parentheses indicate multiples of LIVE trainable parameters.", "description": "This table compares the accuracy of different in-context learning methods on two visual question answering datasets (VQAv2 and OKVQA).  The methods compared include a zero-shot baseline, 32-shot in-context learning (ICL), three non-learnable in-context vector (ICV) methods (Task Vector, Function Vector, PCA-ICV), a LoRA fine-tuning method, and the proposed LIVE method. The number in parentheses indicates the relative size of the trainable parameters of each method compared to the LIVE method.  It shows the performance improvements LIVE offers over other methods in terms of accuracy while being computationally efficient.", "section": "4.2 Results"}, {"figure_path": "QhRemVrZbG/tables/tables_14_1.jpg", "caption": "Table 1: Accuracy (%) with Different ICVs Methods and Finetuning Methods, where numbers in parentheses indicate multiples of LIVE trainable parameters.", "description": "This table compares the accuracy achieved by various methods on the VQAv2 and OKVQA datasets.  The methods include Zero-Shot (no context), 32-shot ICL (conventional In-Context Learning with 32 demonstrations), three non-learnable ICV (In-Context Vector) methods (Task Vector, Function Vector, PCA-ICV), LoRA (a finetuning method), and the proposed LIVE method.  The numbers in parentheses indicate how many times larger the number of trainable parameters is for each method compared to LIVE.  It demonstrates LIVE's superior accuracy and efficiency compared to other methods.", "section": "4.2 Results"}, {"figure_path": "QhRemVrZbG/tables/tables_14_2.jpg", "caption": "Table 1: Accuracy (%) with Different ICVs Methods and Finetuning Methods, where numbers in parentheses indicate multiples of LIVE trainable parameters.", "description": "This table compares the accuracy achieved by different in-context learning (ICL) methods and fine-tuning methods on VQAv2 and OKVQA datasets.  It shows the performance of zero-shot learning, 32-shot ICL, three non-learnable ICV methods (Task Vector, Function Vector, PCA-In-Context Vector), LoRA, and the proposed LIVE method.  The numbers in parentheses indicate the relative size of trainable parameters compared to LIVE.  This provides a quantitative assessment of the effectiveness and efficiency of various methods compared to the proposed method.", "section": "4.2 Results"}, {"figure_path": "QhRemVrZbG/tables/tables_15_1.jpg", "caption": "Table 1: Accuracy (%) with Different ICVs Methods and Finetuning Methods, where numbers in parentheses indicate multiples of LIVE trainable parameters.", "description": "This table compares the accuracy achieved by different methods on the VQAv2 and OKVQA datasets.  The methods include a zero-shot baseline, 32-shot In-Context Learning (ICL), three non-learnable In-Context Vector (ICV) methods (Task Vector, Function Vector, PCA-ICV), LoRA (a finetuning method), and the proposed LIVE method. The numbers in parentheses indicate how many times larger the trainable parameter count is for each method compared to LIVE. This table showcases the performance gains of LIVE compared to traditional methods, highlighting its efficiency and accuracy in visual question answering.", "section": "4.2 Results"}, {"figure_path": "QhRemVrZbG/tables/tables_15_2.jpg", "caption": "Table 1: Accuracy (%) with Different ICVs Methods and Finetuning Methods, where numbers in parentheses indicate multiples of LIVE trainable parameters.", "description": "This table compares the performance of different in-context learning (ICL) methods and a fine-tuning method (LoRA) on two Visual Question Answering (VQA) datasets (VQAv2 and OKVQA).  The methods compared include several non-learnable In-Context Vector (ICV) methods (Task Vector, Function Vector, PCA-ICV) along with 32-shot ICL and the proposed Learnable In-Context Vector (LIVE) method.  The table shows accuracy and the number of trainable parameters for each method, relative to the number of parameters in LIVE.", "section": "4.2 Results"}, {"figure_path": "QhRemVrZbG/tables/tables_15_3.jpg", "caption": "Table 1: Accuracy (%) with Different ICVs Methods and Finetuning Methods, where numbers in parentheses indicate multiples of LIVE trainable parameters.", "description": "This table presents a comparison of the accuracy achieved by various methods on two Visual Question Answering (VQA) datasets: VQAv2 and OKVQA. The methods compared include the baseline Zero-Shot approach, the standard 32-shot In-Context Learning (ICL), three non-learnable In-Context Vector (ICV) methods (Task Vector, Function Vector, and PCA-ICV), the LoRA finetuning method, and the proposed LIVE method.  The table highlights the accuracy improvement of LIVE over other methods while using significantly fewer trainable parameters.  The numbers in parentheses indicate the relative size of the model's trainable parameters compared to LIVE.", "section": "4.2 Results"}, {"figure_path": "QhRemVrZbG/tables/tables_16_1.jpg", "caption": "Table 1: Accuracy (%) with Different ICVs Methods and Finetuning Methods, where numbers in parentheses indicate multiples of LIVE trainable parameters.", "description": "This table compares the accuracy of different methods for visual question answering (VQA) on two datasets, VQAv2 and OKVQA.  The methods compared include a zero-shot baseline, 32-shot In-Context Learning (ICL), three non-learnable In-Context Vector (ICV) methods (Task Vector, Function Vector, and PCA-ICV), LoRA (a fine-tuning method), and the proposed LIVE method.  The table shows accuracy and the number of trainable parameters relative to LIVE's parameter count for each method, highlighting LIVE's efficiency in terms of both accuracy and parameter usage.", "section": "4.2 Results"}, {"figure_path": "QhRemVrZbG/tables/tables_16_2.jpg", "caption": "Table 1: Accuracy (%) with Different ICVs Methods and Finetuning Methods, where numbers in parentheses indicate multiples of LIVE trainable parameters.", "description": "This table compares the performance of various In-Context Vector (ICV) methods and fine-tuning methods (LoRA) on two VQA datasets (VQAv2 and OKVQA).  It shows accuracy results for a zero-shot baseline, a 32-shot In-Context Learning (ICL) approach, three non-learnable ICV methods (Task Vector, Function Vector, PCA-ICV), and the proposed LIVE method.  The numbers in parentheses indicate the relative number of trainable parameters compared to LIVE.", "section": "4.2 Results"}, {"figure_path": "QhRemVrZbG/tables/tables_16_3.jpg", "caption": "Table 1: Accuracy (%) with Different ICVs Methods and Finetuning Methods, where numbers in parentheses indicate multiples of LIVE trainable parameters.", "description": "This table compares the performance of different In-Context Vector (ICV) methods and the fine-tuning method LoRA on two Visual Question Answering (VQA) datasets, VQAv2 and OKVQA.  It shows the accuracy achieved by each method, including zero-shot, 32-shot ICL (In-Context Learning), three non-learnable ICV methods (Task Vector, Function Vector, PCA-In-Context Vector), and LoRA. The number of trainable parameters for each method, relative to LIVE, are also shown.", "section": "4.2 Results"}, {"figure_path": "QhRemVrZbG/tables/tables_16_4.jpg", "caption": "Table 1: Accuracy (%) with Different ICVs Methods and Finetuning Methods, where numbers in parentheses indicate multiples of LIVE trainable parameters.", "description": "This table compares the performance of different In-Context Vector (ICV) methods and a finetuning method (LoRA) on two Visual Question Answering (VQA) datasets: VQAv2 and OKVQA.  It shows accuracy results for a zero-shot baseline, 32-shot In-Context Learning (ICL), three non-learnable ICV methods (Task Vector, Function Vector, PCA-ICV), and the proposed Learnable In-Context Vector (LIVE) method.  The table also indicates how many times larger the trainable parameter count of each method is compared to LIVE's trainable parameters.  This allows for a comparison of performance versus model complexity.", "section": "4.2 Results"}, {"figure_path": "QhRemVrZbG/tables/tables_17_1.jpg", "caption": "Table 1: Accuracy (%) with Different ICVs Methods and Finetuning Methods, where numbers in parentheses indicate multiples of LIVE trainable parameters.", "description": "This table compares the accuracy of different methods on VQAv2 and OKVQA datasets.  The methods include Zero-Shot, 32-shot ICL, three non-learnable ICV methods (Task Vector, Function Vector, PCA-In-Context Vector), LoRA (a fine-tuning method), and LIVE (the proposed method).  The numbers in parentheses indicate the relative number of trainable parameters compared to LIVE. The table demonstrates LIVE's superior accuracy and efficiency compared to other methods.", "section": "4.2 Results"}, {"figure_path": "QhRemVrZbG/tables/tables_17_2.jpg", "caption": "Table 1: Accuracy (%) with Different ICVs Methods and Finetuning Methods, where numbers in parentheses indicate multiples of LIVE trainable parameters.", "description": "This table presents a comparison of the accuracy achieved by different methods on two VQA datasets (VQAv2 and OKVQA).  The methods compared include Zero-Shot (no context), 32-shot ICL (conventional In-Context Learning with 32 demonstrations), three non-learnable ICV (In-Context Vector) methods (Task Vector, Function Vector, PCA-ICV), LoRA (a parameter-efficient fine-tuning method), and LIVE (the proposed Learnable In-Context Vector method). The table shows the accuracy of each method and the number of trainable parameters used (relative to the number of parameters in LIVE). This allows for a comparison of accuracy vs. model complexity/computational cost.", "section": "4.2 Results"}, {"figure_path": "QhRemVrZbG/tables/tables_18_1.jpg", "caption": "Table 1: Accuracy (%) with Different ICVs Methods and Finetuning Methods, where numbers in parentheses indicate multiples of LIVE trainable parameters.", "description": "This table compares the performance of various methods on two VQA datasets (VQAv2 and OKVQA).  The methods include a zero-shot baseline, 32-shot In-Context Learning (ICL), three non-learnable In-Context Vector (ICV) methods (Task Vector, Function Vector, PCA-ICV), and the proposed LoRA and LIVE methods.  The table shows accuracy and the number of trainable parameters (relative to LIVE).  It demonstrates the effectiveness of LIVE in improving accuracy and efficiency compared to other approaches.", "section": "4.2 Results"}, {"figure_path": "QhRemVrZbG/tables/tables_18_2.jpg", "caption": "Table 1: Accuracy (%) with Different ICVs Methods and Finetuning Methods, where numbers in parentheses indicate multiples of LIVE trainable parameters.", "description": "This table presents the accuracy of different methods on VQAv2 and OKVQA datasets.  It compares the performance of Zero-Shot, 32-shot ICL (In-Context Learning), three non-learnable In-Context Vector (ICV) methods (Task Vector, Function Vector, and PCA-ICV), LoRA (a finetuning method), and the proposed LIVE method. The numbers in parentheses show the relative number of trainable parameters compared to LIVE.  The table highlights the superior accuracy of LIVE while using significantly fewer parameters than other methods, especially compared to 32-shot ICL.", "section": "4.2 Results"}, {"figure_path": "QhRemVrZbG/tables/tables_19_1.jpg", "caption": "Table 1: Accuracy (%) with Different ICVs Methods and Finetuning Methods, where numbers in parentheses indicate multiples of LIVE trainable parameters.", "description": "This table compares the accuracy of different in-context learning (ICL) methods and fine-tuning methods on two visual question answering (VQA) datasets: VQAv2 and OKVQA.  The methods compared include Zero-Shot (no context), 32-shot ICL (32 in-context demonstrations), three non-learnable In-Context Vector (ICV) methods (Task Vector, Function Vector, PCA-ICV), LoRA (a fine-tuning method), and the proposed LIVE (Learnable In-Context Vector) method.  The numbers in parentheses show the relative size of the trainable parameters for each method compared to LIVE.", "section": "4.2 Results"}, {"figure_path": "QhRemVrZbG/tables/tables_19_2.jpg", "caption": "Table 1: Accuracy (%) with Different ICVs Methods and Finetuning Methods, where numbers in parentheses indicate multiples of LIVE trainable parameters.", "description": "This table presents a comparison of the accuracy achieved by different methods on the VQAv2 and OKVQA datasets.  The methods compared include Zero-Shot (no in-context learning), 32-shot ICL (conventional in-context learning with 32 demonstrations), three non-learnable ICV methods (Task Vector, Function Vector, PCA-ICV), LoRA (a fine-tuning method), and the proposed LIVE method.  The numbers in parentheses show the relative number of trainable parameters for each method, normalized to the number of parameters used by LIVE.", "section": "4.2 Results"}, {"figure_path": "QhRemVrZbG/tables/tables_19_3.jpg", "caption": "Table 1: Accuracy (%) with Different ICVs Methods and Finetuning Methods, where numbers in parentheses indicate multiples of LIVE trainable parameters.", "description": "This table compares the accuracy of different methods on the VQAv2 and OKVQA datasets. The methods include zero-shot, 32-shot ICL, three non-learnable ICV methods (Task Vector, Function Vector, PCA-In-Context Vector), LoRA, and the proposed LIVE method.  The numbers in parentheses indicate the multiple of the LIVE's trainable parameters used by each method for a fairer comparison.  It shows LIVE's superior performance and efficiency compared to other methods. ", "section": "4.2 Results"}, {"figure_path": "QhRemVrZbG/tables/tables_19_4.jpg", "caption": "Table 1: Accuracy (%) with Different ICVs Methods and Finetuning Methods, where numbers in parentheses indicate multiples of LIVE trainable parameters.", "description": "This table presents the accuracy achieved by various methods on the VQAv2 and OKVQA datasets.  The methods compared include a zero-shot baseline, 32-shot In-Context Learning (ICL), three non-learnable In-Context Vector (ICV) methods (Task Vector, Function Vector, and PCA-In-Context Vector), LoRA (a parameter-efficient fine-tuning method), and the proposed LIVE method. The numbers in parentheses indicate the relative size of the trainable parameters for each method compared to LIVE.  The table allows for a comparison of the performance and efficiency of different methods for visual question answering (VQA).", "section": "4.2 Results"}]