[{"figure_path": "T5Cerv7PT2/figures/figures_7_1.jpg", "caption": "Figure 1: Normalized final performance aggregated across the five MuJoCo environments, where feasible rewards are normalized by the expert returns. Both IRL-Base and IRL-Plus perform better than the baseline ICRL method MECL on average, with IRL-Plus out-performing by a statistically significant margin across most metrics, for both feasible rewards and violation rate.", "description": "This figure compares the performance of different methods for constraint inference in reinforcement learning across five MuJoCo environments.  The x-axis represents different metrics (IQM, Median, Mean, Optimality Gap), and the y-axis shows the normalized performance. The results show that both the proposed IRL-Base and IRL-Plus methods significantly outperform the Maximum Entropy Constrained Reinforcement Learning (MECL) baseline method, with IRL-Plus exhibiting the best overall performance across metrics for both feasible rewards and violation rates.", "section": "5.1 IRL versus ICRL"}, {"figure_path": "T5Cerv7PT2/figures/figures_7_2.jpg", "caption": "Figure 2: Interquartile mean (IQM) of final performance on Half-Cheetah with sub-optimal trajectories in ratios of 20%, 50% and 80%. In all three scenarios, IRL-Plus out-performs MECL in terms of feasible rewards, though only with statistical significance in the 20% scenario. Neither MECL nor IRL-Plus clearly outperforms in terms of violation rate, with both methods achieving near zero.", "description": "This figure shows the results of applying different methods for constraint inference to the Half-Cheetah environment using suboptimal expert trajectories. The trajectories contain varying percentages of constraint violations (20%, 50%, and 80%). The results show that the proposed IRL-Plus method generally outperforms the baseline ICRL method (MECL) in terms of feasible rewards, which indicates the agent was able to achieve good performance while respecting the constraints. The differences are statistically significant only at 20%. Both methods achieve similar near-zero violation rates, indicating that both methods successfully avoid unsafe actions.", "section": "5.1.1 Sub-optimal Expert Trajectories"}, {"figure_path": "T5Cerv7PT2/figures/figures_8_1.jpg", "caption": "Figure 3: Normalized final performance for all ablations aggregated across the five MuJoCo environments, where feasible rewards are normalized by the expert returns. IRL with L2-regularization, separate critics and policy resets (IRL-Plus) generally performs the best in terms of feasible rewards with only a slight increase in violation rate versus IRL-Base.", "description": "This figure compares the performance of different Inverse Reinforcement Learning (IRL) algorithm variations on five MuJoCo continuous control tasks.  The x-axis shows the different modifications to the basic IRL algorithm (IRL-Base).  The y-axis shows the performance statistics (interquartile mean (IQM), median, mean, and optimality gap) for feasible rewards and violation rates. The results indicate that adding L2 regularization, separate critics, and policy resets (IRL-Plus) significantly improves feasible rewards while slightly increasing the violation rate compared to the basic IRL method.", "section": "5.2 Impact of Modifications"}, {"figure_path": "T5Cerv7PT2/figures/figures_9_1.jpg", "caption": "Figure 1: Normalized final performance aggregated across the five MuJoCo environments, where feasible rewards are normalized by the expert returns. Both IRL-Base and IRL-Plus perform better than the baseline ICRL method MECL on average, with IRL-Plus out-performing by a statistically significant margin across most metrics, for both feasible rewards and violation rate.", "description": "This figure displays the final performance results of the proposed IRL methods (IRL-Base and IRL-Plus) in comparison to two baselines (MECL and GACL) across five MuJoCo environments.  Feasible rewards, normalized by expert returns, and violation rates are shown. The results demonstrate that both IRL methods outperform the ICRL baseline (MECL), with IRL-Plus achieving statistically significant improvements across most metrics for both feasible rewards and violation rates.", "section": "5.1 IRL versus ICRL"}, {"figure_path": "T5Cerv7PT2/figures/figures_16_1.jpg", "caption": "Figure 1: Normalized final performance aggregated across the five MuJoCo environments, where feasible rewards are normalized by the expert returns. Both IRL-Base and IRL-Plus perform better than the baseline ICRL method MECL on average, with IRL-Plus out-performing by a statistically significant margin across most metrics, for both feasible rewards and violation rate.", "description": "This figure summarizes the results of comparing the proposed IRL methods (IRL-Base and IRL-Plus) to the existing ICRL method (MECL) across five MuJoCo environments.  The results are presented as normalized final performance, with feasible rewards normalized by expert returns.  The key finding is that both IRL methods outperform MECL, and IRL-Plus shows a statistically significant improvement over MECL in most metrics for both feasible rewards and violation rate. This suggests that simpler IRL approaches can be just as effective, or even more so, than complex ICRL methods for constraint inference tasks. ", "section": "5.1 IRL versus ICRL"}, {"figure_path": "T5Cerv7PT2/figures/figures_17_1.jpg", "caption": "Figure 1: Normalized final performance aggregated across the five MuJoCo environments, where feasible rewards are normalized by the expert returns. Both IRL-Base and IRL-Plus perform better than the baseline ICRL method MECL on average, with IRL-Plus out-performing by a statistically significant margin across most metrics, for both feasible rewards and violation rate.", "description": "This figure presents a summary of the final performance of different methods across five MuJoCo environments. Feasible rewards are normalized by expert returns.  The results show that both the basic IRL method (IRL-Base) and the improved IRL method (IRL-Plus) outperform the Maximum Entropy Inverse Constrained Reinforcement Learning method (MECL) in terms of average performance across most metrics (IQM, Median, Mean) . IRL-Plus shows a statistically significant improvement over MECL for both feasible rewards and violation rate.", "section": "5.1 IRL versus ICRL"}]