[{"figure_path": "7HFQfRjdcn/figures/figures_2_1.jpg", "caption": "Figure 1: (a) Characteristic activation boundary (CAB) B (brown solid line) and spatial location \u03c6 = \u2212\u03bbu(\u03b8) of a ReLU unit z = ReLU(u(\u03b8)x+\u03bb) = ReLU(cos(\u03b8)x\u2081 + sin(\u03b8)x\u2082 + \u03bb) for inputs x \u2208 R\u00b2. The CAB forms a line in R\u00b2, which acts as a boundary separating inputs into two regions. Green arrows denote the active region, and red arrows denote the inactive region. (b)-(e) Stability of the CAB of a ReLU unit in R\u00b2 under small perturbations \u03b5 = \u03b4\u03b9 to the parameters. Solid lines denote characteristic activation boundaries B, and colored dotted lines connect the origin and spatial locations \u03c6 of B. Smaller changes between the perturbed and original boundaries imply higher stability. GmP is most stable against perturbations.", "description": "This figure illustrates the characteristic activation boundary (CAB) and its spatial location for a ReLU unit.  Subfigures (a) shows the CAB as a line separating active and inactive regions in a 2D input space. Subfigures (b) through (e) demonstrate the stability of the CAB under small parameter perturbations for different parameterization methods (SP, WN, GmP). GmP shows significantly higher stability.", "section": "2.3 Instability of Characteristic Activation Boundary During Stochastic Optimization"}, {"figure_path": "7HFQfRjdcn/figures/figures_5_1.jpg", "caption": "Figure 2: (a)-(b) Characteristic activation point B (intersection of brown solid lines and the x-axis) and spatial location \u03c6 = \u2212\u03bb\u03b1(\u03b8) of a ReLU unit z = ReLU(u(\u03b8)x + \u03bb) (blue solid lines) for inputs x \u2208 R. Green arrows denote active regions, and red arrows denote inactive regions. (c) Evolution dynamics of the characteristic points B in a one-hidden-layer network with 100 ReLU units for a 1D Levy regression problem under SP, WN, BN and GmP during training. SP stands for standard parameterization, WN stands for weight normalization, BN stands for batch normalization, and GmP stands for geometric parameterization. Smaller values are better as they indicate higher stability of the evolution of the characteristic points during training. The y-axis is in log2 scale. (d)-(g): The top row illustrates the experimental setup, including the network\u2019s predictions at initialization and after training, and the training data and the ground-truth function (Levy). Bottom row: the evolution of the characteristic activation point for the 100 ReLU units during training. Each horizontal bar shows the spatial location spectrum for a chosen optimization step, moving from the bottom (at initialization) to the top (after training with Adam). More spread of the spatial locations covers the data better and adds more useful non-linearities to the model, making prediction more accurate. Regression accuracy is measured by root mean squared error (RMSE) on a separate test set. Smaller RMSE values are better. We use cross-validation to select the learning rate for each method. The optimal learning rate for SP, WN, and BN is lower than that for GmP, since their training becomes unstable with higher learning rates, as shown in (c).", "description": "This figure shows the characteristic activation point and its evolution during training for different parameterizations (SP, WN, BN, GmP).  It illustrates how GmP leads to more stable training and better generalization performance in a 1D Levy regression task by maintaining a consistent movement of activation points during training. The figure highlights the instability of SP, WN, and BN, showing that they can have erratic behavior resulting in less optimal generalization. The improved performance of GmP is linked to its ability to stabilize the evolution of the activation boundaries.", "section": "Illustrative Experiments"}, {"figure_path": "7HFQfRjdcn/figures/figures_6_1.jpg", "caption": "Figure 3: Performance of a single-hidden-layer neural network with 10 ReLU units on the 2D Banana classification dataset under SP, WN, BN and GmP trained using Adam. SP stands for standard parameterization, WN stands for weight normalization, BN stands for batch normalization, and GmP stands for geometric parameterization. (a)-(h): Trajectories of the spatial locations of the 10 ReLU units during training. Each color depicts one ReLU unit. Smoother evolution means higher training stability. The evolution under GmP is stable, so we can use a 10\u00d7 larger learning rate. (i): Evolution dynamics of the angular direction \u03b8 of CABs. Smaller values are better as they indicate higher robustness against stochastic gradient noise. (j)-(m): Network predictions after training. Black bold lines depict the classification boundary between two classes. Classification accuracy is measured on a separate test set. Higher accuracy values are better. The red stars show the spatial locations of 10 ReLU units. Intuitively speaking, more evenly spread out red stars are better for classification accuracy, as they provide more useful non-linearity.", "description": "This figure shows the performance comparison of four different parameterization methods (SP, WN, BN, and GmP) on a 2D Banana classification task using a single-hidden-layer neural network with 10 ReLU units.  Subfigures (a) through (h) illustrate the trajectories of the spatial locations of the ReLU units during training, highlighting the increased stability of GmP. Subfigure (i) visualizes the change in the angular direction of the characteristic activation boundaries (CABs), demonstrating GmP's robustness to noise. Finally, subfigures (j) through (m) display the classification decision boundaries learned by each method, showcasing GmP's superior performance and more even distribution of learned features.", "section": "4 Empirical Evaluation"}, {"figure_path": "7HFQfRjdcn/figures/figures_6_2.jpg", "caption": "Figure 3: Performance of a single-hidden-layer neural network with 10 ReLU units on the 2D Banana classification dataset under SP, WN, BN and GmP trained using Adam. SP stands for standard parameterization, WN stands for weight normalization, BN stands for batch normalization, and GmP stands for geometric parameterization. (a)-(h): Trajectories of the spatial locations of the 10 ReLU units during training. Each color depicts one ReLU unit. Smoother evolution means higher training stability. The evolution under GmP is stable, so we can use a 10\u00d7 larger learning rate. (i): Evolution dynamics of the angular direction \u03b8 of CABs. Smaller values are better as they indicate higher robustness against stochastic gradient noise. (j)-(m): Network predictions after training. Black bold lines depict the classification boundary between two classes. Classification accuracy is measured on a separate test set. Higher accuracy values are better. The red stars show the spatial locations of 10 ReLU units. Intuitively speaking, more evenly spread out red stars are better for classification accuracy, as they provide more useful non-linearity.", "description": "This figure shows the results of training a single-hidden-layer neural network with 10 ReLU units on the 2D Banana classification dataset using four different parameterization methods: Standard Parameterization (SP), Weight Normalization (WN), Batch Normalization (BN), and Geometric Parameterization (GmP).  It compares the stability of the training process, the evolution of the characteristic activation boundaries (CABs), and the resulting classification accuracy. GmP shows significantly better stability and higher accuracy.", "section": "4.1.2 2D Banana Classification"}, {"figure_path": "7HFQfRjdcn/figures/figures_8_1.jpg", "caption": "Figure 4: Convergence speed for VGG-6 trained on the ImageNet32 dataset with batch size 1024.", "description": "This figure shows the training and validation accuracy for VGG-6, a convolutional neural network, trained on the ImageNet32 dataset. Different parameterization methods (SP, WN, WN+MBN, BN, GmP, GmP+IMN) are compared, demonstrating that GmP+IMN achieves faster convergence with higher accuracy. The batch size is kept constant at 1024.", "section": "4.2.2 ImageNet32 Classification with VGG"}, {"figure_path": "7HFQfRjdcn/figures/figures_14_1.jpg", "caption": "Figure 5: Visualization of characteristic activation boundaries (brown solid lines) and spatial locations \u03c6 = \u2212\u03bb\u03b1(\u03b8) of a ReLU unit z = ReLU(u(\u03b8)x + \u03bb) (blue solid lines) for inputs x \u2208 R. Green arrows denote active regions and red arrows denote inactive regions.", "description": "This figure visualizes the characteristic activation boundaries and spatial locations for a ReLU unit in one dimension. It illustrates how the boundary (represented by the brown line) and its location (blue line) change depending on different parameter settings (\u03bb and \u03b8). The active and inactive regions are also shown.", "section": "A More Visualizations of ReLU Characteristic Activation Boundaries"}, {"figure_path": "7HFQfRjdcn/figures/figures_14_2.jpg", "caption": "Figure 1: (a) Characteristic activation boundary (CAB) B (brown solid line) and spatial location \u03c6 = \u2212\u03bbu(\u03b8) of a ReLU unit z = ReLU(u(\u03b8)x + \u03bb) = ReLU(cos(\u03b8)x\u2081 + sin(\u03b8)x\u2082 + \u03bb) for inputs x \u2208 R\u00b2. The CAB forms a line in R\u00b2, which acts as a boundary separating inputs into two regions. Green arrows denote the active region, and red arrows denote the inactive region. (b)-(e) Stability of the CAB of a ReLU unit in R\u00b2 under small perturbations \u03b5 = \u03b4\u2081 to the parameters. Solid lines denote characteristic activation boundaries B, and colored dotted lines connect the origin and spatial locations \u03c6 of B. Smaller changes between the perturbed and original boundaries imply higher stability. GmP is most stable against perturbations.", "description": "This figure shows the characteristic activation boundary (CAB) and its stability under different parameterizations.  (a) illustrates a CAB as a line in 2D space separating active and inactive regions of a ReLU neuron. (b)-(e) demonstrate the effect of small perturbations on the CAB's position for different parameterizations (SP, WN, BN, and GmP). The smaller the change in the CAB's position after perturbation, the higher the stability of that parameterization.  GmP shows the highest stability.", "section": "2.3 Instability of Characteristic Activation Boundary During Stochastic Optimization"}]