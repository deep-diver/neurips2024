{"references": [{"fullname_first_author": "Jimmy Lei Ba", "paper_title": "Layer normalization", "publication_date": "2016-07-06", "reason": "This paper introduces layer normalization, a crucial technique for stabilizing training in deep neural networks, which is directly related to the paper's focus on neural network parameterizations and normalizations."}, {"fullname_first_author": "Sergey Ioffe", "paper_title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "publication_date": "2015-01-01", "reason": "This paper introduces batch normalization, a widely used technique to improve the stability and speed of training deep neural networks, which is compared and contrasted in this paper's evaluation of geometric parameterization."}, {"fullname_first_author": "Kaiming He", "paper_title": "Deep residual learning for image recognition", "publication_date": "2016-01-01", "reason": "This paper introduces residual networks, a fundamental architecture in deep learning, providing a benchmark for evaluating the proposed geometric parameterization's impact on training dynamics and generalization performance."}, {"fullname_first_author": "Xavier Glorot", "paper_title": "Deep sparse rectifier neural networks", "publication_date": "2011-01-01", "reason": "This paper introduces ReLU networks, the foundation of the neural network architecture used in this paper's experiments, and hence directly contributes to its experimental setup and analysis."}, {"fullname_first_author": "Tim Salimans", "paper_title": "Weight normalization: A simple reparameterization to accelerate training of deep neural networks", "publication_date": "2016-01-01", "reason": "This paper introduces weight normalization, another technique to improve neural network training, making it a key reference for comparison and contrast with this paper's proposed geometric parameterization."}]}