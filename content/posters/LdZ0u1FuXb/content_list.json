[{"type": "text", "text": "A Kernel Perspective on Distillation-based Collaborative Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Sejun Park Kihun Hong Ganguk Hwang\\* ", "page_idx": 0}, {"type": "text", "text": "Department of Mathematical Sciences Korea Advanced Institute of Science and Technology {sejunpark, nuri9911, guhwang}@kaist.ac.kr ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Over the past decade, there is a growing interest in collaborative learning that can enhance AI models of multiple parties. However, it is still challenging to enhance performance them without sharing private data and models from individual parties. One recent promising approach is to develop distillation-based algorithms that exploit unlabeled public data but the results are still unsatisfactory in both theory and practice. To tackle this problem, we rigorously analyze a representative distillationbased algorithm in the view of kernel regression. This work provides the first theoretical results to prove the (nearly) minimax optimality of the nonparametric collaborative learning algorithm that does not directly share local data or models in massively distributed statistically heterogeneous environments. Inspired by our theoretical results, we also propose a practical distillation-based collaborative learning algorithm based on neural network architecture. Our algorithm successfully bridges the gap between our theoretical assumptions and practical settings with neural networks through feature kernel matching. We simulate various regression tasks to verify our theory and demonstrate the practical feasibility of our proposed algorithm. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Collaborative learning of AI models in decentralized settings is an important problem covered in various fields of machine learning such as distributed learning [10, 70], Federated Learning (FL) [23], peer-to-peer learning [3], and miscellaneous collaborative learning [47]. In particular, this theme has been most actively discussed in the context of FL [24, 31, 45, 61]. In this context, each local party is typically viewed as a subordinate entity within the collective learning system. For example, most FL algorithms mandate the exchange of local AI model information among participating local parties. Under this scheme, local AI models are usually subjected to restrictions in their architecture. However, from the perspective of collaboration, each local party may have to be regarded as an independent learning agent, meaning they are not obligated to fuily share their model information. In short, the model (or parameter) exchange in FL algorithms can emerge as a critical issue in collaborative learning. ", "page_idx": 0}, {"type": "text", "text": "Fundamentally, addressing this issue necessitates an alternative medium for sharing learning information distinct from model exchange. Indeed, Distillation-based Collaborative Learning (DCL) [14, 30, 44] provides a good answer. In these algorithms, local training information is shared via the outcomes of AI models on additional unlabeled public data. The collected information is then utilized for knowledge distillation [21] to each local AI model. As mentioned in [14, 48], this procedure is agnostic to model heterogeneity and avoids the direct sharing of local AI model information. This is a key advantage that distinguishes DCL from traditional FL. ", "page_idx": 0}, {"type": "text", "text": "Despite its pioneering nature and potential utility, DCL has not been sufficiently explored. A significant reason for this is the lack of theoretical understanding regarding knowledge distillation and its effectiveness in massively distributed statistically heterogeneous environments. Our work stems from the fundamental question of whether DCL algorithms can be theoretically effective in these settings. Inspired by [48, 58], we analyze FedMD [30, 41], the most standard DCL algorithm from a nonparametric perspective. Specifically, we adopt an operator-theoretic approach [5, 15, 37, 53, 65] to obtain an upper rate of convergence for the nonparametric version of FedMD (named DCL-KR) in the expected sense. Remarkably, our analysis reveals that DCL-KR achieves a nearly minimax optimal convergence rate, where the prefactor is independent of the number of participating local parties. It is worth noting that DCL-KR is the first nearly minimax optimal collaborative learning algorithm that does not directly share local data or models in massively distributed statistically heterogeneous environments. The novelty of our theoretical results and their comparison to prior works are provided in Section 2 and 3. ", "page_idx": 1}, {"type": "text", "text": "Nevertheless, our theoretical analysis does not fully demonstrate the efficacy of DCL algorithms based on neural network architectures. Instead, our theoretical results serve as inspiration for designing a novel DCL algorithm for regression that refines existing approaches. Consequently, we propose a Distillation-based Collaborative Learning algorithm over heterogeneous Neural Networks (named DCL-NN) for regression tasks. DCL-NN leverages kernel matching to align the feature kernels from the last hidden layer of each local AI model with an ensemble kernel. This procedure brings heterogeneous neural networks into the regime of DCL-KR. ", "page_idx": 1}, {"type": "text", "text": "Finally, we conduct experiments on DCL-KR and DCL-NN. To illustrate the superiority of our algorithms, we compare them with several baselines on various regression tasks. Experimental results show that DCL-KR achieves the same performance as the centralized model, even beyond the theoretical results. We also observe that DCL-NN significantly outperforms previous DCL frameworks in most settings. ", "page_idx": 1}, {"type": "text", "text": "In summary, our contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "1. In Section 3, we theoretically prove that a nonparametric version of the most standard distillationbased collaborative learning algorithm (named DCL-KR) is nearly minimax optimal in massively distributed statistically heterogeneous environments.   \n2. Inspired by the results provided in Section 3, we propose a distillation-based collaborative learning algorithm with heterogeneous neural networks (named DCL-NN) in Section 4.   \n3. In Section 5, we conduct experiments to empirically confirm our theoretical results and show the practical feasibility of our proposed algorithms. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Federated Learning  Most FL algorithms [45] communicate model parameters for collaboration. This approach has been extensively studied under various constraints, including data privacy [1], statistical heterogeneity [24, 31], communication efficiency [51], personalization [13, 59], and robustness [25]. While it has been successful both theoretically and experimentally, this type of FL is limited in terms of the privacy and flexibility of local AI models, as the algorithms directly access the structures and parameters of the local models. Our study focuses on distillation-based collaborative learning, where the privacy and flexibility of local AI models are fully guaranteed. ", "page_idx": 1}, {"type": "text", "text": "Distillation-based Collaborative (or Federated) Learning  The type of algorithms we investigate operates by communicating the functional information of local AI models. These algorithms typically assume the availability of additional public data points. In this case, the outcomes of local models on the public dataset are used for collaboration. For instance, Li and Wang [30], Lin et al. [41], Park et al. [48] iteratively collect predictions of local models on the public dataset and then aggregate them into a naive ensemble (with or without a fixed linear transformation) to distribute. On the other hand, Cho et al. [7], Zhang et al. [69], Fan et al. [14] apply personalized ensemble strategies by additionally learning the mutual trust between models. Makhija et al. [44] propose FedHeNN, which distills training information in the form of matching feature kernels instead of the predictions of local AI models on the public data. Both FedHeNN and DCL-NN utilize centered kernel alignment [8] to match feature kernels of local models, but DCL-NN uses the ensemble distillation for predictions as well. Thus, DCL-NN enables parties to learn from the entire input space. ", "page_idx": 1}, {"type": "text", "text": "Table 1: Comparative analysis of decentralized environments for (nearly) minimax optimality of representative collaborative learning algorithms with kernel regression. nFedAvg indicates the nonparametric version of FedAvg in [58]. Note that IED [48] achieves a weaker version of minimax optimality. ", "page_idx": 2}, {"type": "table", "img_path": "LdZ0u1FuXb/tmp/ce672fbdc331ea2f62d07bb582b111e2e3b8345315012c1d5738564afafff2ef.jpg", "table_caption": [], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "Decentralized Learning with Kernel Regression A number of studies have investigated the minimax optimal rate of regularized kernel regression algorithms such as kernel ridge regression and gradient descent-based kernel regression with early stopping [5, 15, 37, 65]. In particular, over the past decade, the growing interest in decentralized learning has led to active research in the generalization analysis of decentralized kernel regression. While divide-and-conquer algorithms [34, 38, 66, 70] play a significant role in this research flow, most of them fail to account for statistical heterogeneity and massively distributed cases, along with privacy preservation, which has received a lot of attention recently. On the other hand, decentralized kernel regression algorithms with multiple communication rounds [40, 43, 48, 58, 67] achieve superior theoretical results compared to the divide-and-conquer algorithms. However, the discussions of these algorithms primarily focus on the efficiency of resource costs [40, 43, 67], while research on relaxing environmental constraints has been scarce. For example, most of these works assume a limited number of parties to prove the optimality in a minimax sense. ", "page_idx": 2}, {"type": "text", "text": "To the best of our knowledge, [48, 58] stand as the only investigations that consider general decentralized environments. Similar to our work, Park et al. [48] study the convergence rate of distillation-based collaborative learning with kernel regression. However, their results demonstrate a weaker version of minimax optimality and do not cover statistically heterogeneous environments. In this regard, Su et al. [58] offer a promising methodology. They analyze nonparametric versions of FedAvg [45] and FedProx [31], representative FL algorithms involving model exchange, in general decentralized environments such as statistically heterogeneous and massively distributed scenarios. In this work, we extend their methodology to analyze FedMD [30, 41] from a nonparametric perspective in massively distributed statistically heterogeneous environments. We summarize the comparison between our work and prior studies in Table 1. Note that algorithms that do not employ Nystrom scheme (including nonparametric FedAvg [58]) fail to preserve local data privacy due to the inherent characteristics of kernel regression. On the other hand, DC-NY [66] and DKRR-NY-CM [67] can achieve the local data privacy preservation by utilizing the public data as Nystrom centers. ", "page_idx": 2}, {"type": "text", "text": "3 DCL-KR: A Nonparametric View of FedMD ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we establish the theory of a nonparametric version of FedMD [30, 41], the most standard distillation-based collaborative learning algorithm. ", "page_idx": 2}, {"type": "text", "text": "3.1 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Let $\\rho_{\\mathbf{x},y}=\\rho_{\\mathbf{x}}\\cdot\\rho_{y|\\mathbf{x}}$ be a Borel probability measure on $\\mathcal{X}\\times\\mathbb{R}$ where $\\mathcal{X}$ is a compact subset of $\\mathbb{R}^{d}$ and we assume the support of $\\rho_{\\mathbf{x}}$ is $\\mathcal{X}$ . The goal of the regression problem is to find a minimizer of the population risk, i.e., ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{h:\\mathcal{X}\\to\\mathbb{R}}\\mathcal{E}(h),\\qquad\\mathcal{E}(h):=\\frac{1}{2}\\;\\mathbb{E}_{(\\mathbf{x},y)\\sim\\rho_{\\mathbf{x},y}}|y-h(\\mathbf{x})|^{2}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Then, the function $f_{0}^{*}:\\mathcal{X}\\to\\mathbb{R}$ defined by $\\mathbf{x}_{0}\\mapsto\\mathbb{E}_{y\\sim\\rho_{y|\\mathbf{x}}(\\cdot|\\mathbf{x}_{0})}[y]$ $\\mathbf{x}_{0}\\in\\mathcal{X}$ is a target function. ", "page_idx": 2}, {"type": "text", "text": "Let $k:\\mathcal{X}\\times\\mathcal{X}\\to\\mathbb{R}$ be a Mercer kernel [9] where $\\begin{array}{r}{\\kappa:=(\\operatorname*{sup}_{\\mathbf{x}\\in\\mathcal{X}}k(\\mathbf{x},\\mathbf{x}))^{1/2}<\\infty}\\end{array}$ and $\\mathbb{H}_{k}$ be a reproducing kernel Hilbert space associated to $k$ . We set $k_{\\mathbf{x}}:=k(\\cdot,\\mathbf{x})$ and the covariance operator $\\begin{array}{r}{T_{k,\\nu}:\\mathbb{H}_{k}\\rightarrow\\mathbb{H}_{k}}\\end{array}$ with respect to any Borel probability measure $\\nu$ on $\\mathcal{X}$ defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\nT_{k,\\nu}h=\\int_{\\mathcal{X}}h(\\mathbf{x})k_{\\mathbf{x}}\\ d\\nu(\\mathbf{x}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Then we can see that $T_{k,\\nu}\\,=\\,\\iota_{\\nu}^{\\top}\\,\\iota_{\\nu}$ where $\\iota_{\\nu}:\\mathbb{H}_{k}\\to L_{\\nu}^{2}$ is a natural embedding, $L_{\\nu}^{2}\\,=\\,L^{2}(\\mathcal{X},\\nu)$ denotes the $L^{2}$ space, and a superscript $\\top$ denotes the adjoint operator of a given operator. We also define the sampling operator $S_{D}:\\mathbb{H}_{k}\\rightarrow\\mathbb{R}^{n}$ by $h\\mapsto\\bar{[}h(\\mathbf{x}^{1}),\\cdot\\cdot\\cdot\\;,h(\\mathbf{x}^{n})\\bar{]}^{\\top}$ and $\\dot{T}_{k,X}:=S_{D}^{\\top}S_{D}$ when $D=\\{(\\mathbf{x}^{1},y^{1}),\\cdot\\cdot\\cdot\\,,(\\mathbf{x}^{n},y^{n})\\}$ with $X=\\{\\mathbf{x}^{1},\\cdot\\cdot\\cdot,\\mathbf{x}^{n}\\}$ is given. Since $S_{D}$ depends only on data inputs $X$ , we can define the sampling operator for unlabeled datasets in the same way. See Appendix A.1 for further details. ", "page_idx": 3}, {"type": "text", "text": "3.1.1  Kernel Gradient Descent with Early Stopping ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Given a dataset $D\\,=\\,\\{({\\bf x}^{1},y^{1}),\\cdot\\cdot\\cdot{\\bf\\nabla},({\\bf x}^{n},y^{n})\\}$ generated from $\\rho_{\\mathbf{x},y}$ , consider the empirical risk $\\widetilde{\\mathcal{E}}_{D}:\\mathbb{H}_{k}\\rightarrow\\mathbb{R}$ given by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\widetilde{\\mathcal{E}}_{D}(h)=\\frac{1}{2}\\|S_{D}h-\\mathbf{y}\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{y}=[y^{1},\\cdot\\cdot\\cdot\\,,y^{n}]^{\\top}$ .Here, $\\|\\cdot\\|_{2}$ denotes a scaled Euclidean norm $\\begin{array}{r}{\\|\\mathbf{v}\\|_{2}=(\\frac{1}{n}\\textstyle\\sum_{i=1}^{n}\\mathbf{v}_{i}^{2})^{1/2}}\\end{array}$ From the functional derivative $\\nabla\\widetilde{\\mathcal{E}}_{D}(h)=S_{D}^{\\top}(S_{D}h-\\mathbf{y})$ , the gradient descent scheme becomes ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\nu_{1}=0,\\quad\\nu_{t+1}=\\nu_{t}-\\eta_{t}S_{D}^{\\top}(S_{D}\\nu_{t}-\\mathbf{y})\\quad(t=1,2,\\cdot\\cdot\\cdot)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\{\\eta_{t}\\}_{t\\in\\mathbb{N}}$ is a set of learning rates. In this work, we set $\\eta_{t}=\\eta,t\\in\\mathbb{N}$ for a fixed $\\eta\\in(0,1/\\kappa^{2})$ Then, a simple calculation gives $\\nu_{t}\\rightarrow S_{D}^{\\top}(S_{D}S_{D}^{\\top})^{-1}\\mathbf{y}$ as $t\\to\\infty$ provided that the operator $S_{D}S_{D}^{\\top}$ is invertible. The limit is known as the minimum norm interpolation [49] of $D$ . Since the interpolation regressor generalizes poorly unless there is no noise [32, 39], early stopping strategies are usually applied to avoid the overfitting issue. With adequate stopping rules, gradient descent-based kernel regression has an optimal rate in a minimax sense [36, 37, 65]. ", "page_idx": 3}, {"type": "text", "text": "3.2 DCL-KR Algorithm ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "From now on, we consider the setting that there are $m$ parties and the $i$ th party has a private local data $D_{i}=\\{(\\mathbf{x}_{i}^{j},y_{i}^{j}):j=1,\\cdots,n_{i}\\}$ for $i=1,\\cdot\\cdot\\cdot,m$ . Assume that all data $\\textstyle D=\\bigcup_{i=1}^{m}D_{i}$ are i.i.d. with the distribution $\\rho_{\\mathbf{x},y}$ but each local dataset does not need to have the same distribution. Let $Z=\\{\\mathbf{z}^{1},\\cdot\\cdot\\cdot\\mathbf{\\epsilon},\\mathbf{z}^{n_{0}}\\}\\subset\\mathcal{X}$ be the additional public inputs. The goal of all parties is to have their models that perform well on the distribution $\\rho_{\\mathbf{x}}$ . In other words, each party expects to be able to make good predictions not only for its local data distribution but also for unseen data distribution through collaborative learning. ", "page_idx": 3}, {"type": "text", "text": "Similar to [58], we construct a nonparametric version of FedMD (called DCL-KR), which is presented in Algorithm 1. In Algorithm 1, $\\mathcal{G}_{i}$ is a one-step local gradient descent update on $\\widetilde{\\mathcal{E}}_{D_{i}}$ ,i.e., $\\mathcal{G}_{i}h=h-\\eta S_{D_{i}}^{\\top}(S_{D_{i}}h\\!-\\!{\\bf y}_{i})$ where $\\mathbf{y}_{i}=[y_{i}^{1},\\cdot\\cdot\\cdot\\,,y_{i}^{n_{i}}]^{\\top}$ . Similarly, $\\tilde{\\mathcal{G}}_{t}$ is a one-step gradient descent update on $\\widetilde{\\mathcal{E}}_{(Z,\\mathbf{y}_{p,t})}$ ,i.e., $\\tilde{\\mathcal{G}}_{t}h=h-\\eta S_{Z}^{\\top}(S_{Z}h-\\mathbf{y}_{p,t})$ ", "page_idx": 3}, {"type": "text", "text": "3.3  Theoretical Results ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this subsection, we show the nearly minimax optimality of DCL-KR. To derive theoretical results, we assume the following conditions regarding regularity of noise, the kernel $k$ , and the target function $f_{0}^{*}$ asbelow. ", "page_idx": 3}, {"type": "text", "text": "Assumption 3.1. We assume $\\mathbb{E}_{y\\sim\\rho_{y}}y^{2}<\\infty$ and ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\int\\left(\\exp\\left(\\frac{|y-f_{0}^{*}(\\mathbf x)|}{M}\\right)-\\frac{|y-f_{0}^{*}(\\mathbf x)|}{M}-1\\right)\\,d\\rho_{y|\\mathbf x}(y|\\mathbf x)\\leq\\frac{\\gamma^{2}}{2M^{2}},\\quad\\forall\\mathbf x\\in\\mathcal X\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $M$ and $\\gamma$ are positive constants. ", "page_idx": 3}, {"type": "text", "text": "1: Hyperparameters: $T$ : total communication round, $E$ : the number of local iterations at each communication round, $\\eta$ : learning rate ", "page_idx": 4}, {"type": "text", "text": "2: Initialize local models $f_{i,0}=0$ for $i=1,\\cdot\\cdot\\cdot,m$ ", "page_idx": 4}, {"type": "text", "text": "3: for $t=0,\\cdot\\cdot\\cdot,T-1$ do ", "page_idx": 4}, {"type": "text", "text": "4: for party $i=1,\\cdot\\cdot\\cdot,m$ do ", "page_idx": 4}, {"type": "text", "text": "5: Update the local model $E$ times by gradient descent on the empirical risk $\\widetilde{\\mathcal{E}}_{D_{i}}$ ", "page_idx": 4}, {"type": "equation", "text": "$$\nf_{i,t}^{\\prime}\\leftarrow\\mathcal{G}_{i}^{E}f_{i,t}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "6: Upload the local predictions on $Z$ to the server ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{y}_{p,t}^{i}=S_{Z}f_{i,t}^{\\prime}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "7: end for ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "8: The server aggregates the local predictions to compute the consensus prediction ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{y}_{p,t}=\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\mathbf{y}_{p,t}^{i}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and then distributes $\\mathbf{y}_{p,t}$ to all local parties. ", "page_idx": 4}, {"type": "text", "text": "9:For party $i\\;(i=1,\\cdot\\cdot\\cdot\\,,m)$ update the local model by infinitely many iterations of gradient desent on the empirial risk $\\widetilde{\\mathcal{E}}_{(Z,\\mathbf{y}_{p,t})}$ ", "page_idx": 4}, {"type": "equation", "text": "$$\nf_{i,t+1}\\gets\\tilde{\\mathcal{G}}_{t}^{\\infty}g_{i,t}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "with an initialization $g_{i,t}$ chosen from a subspace spanned by $k_{\\mathbf{z}^{1}},\\cdot\\cdot\\cdot\\ ,k_{\\mathbf{z}^{n_{0}}}$ ", "page_idx": 4}, {"type": "text", "text": "10: end for ", "page_idx": 4}, {"type": "text", "text": "Assumption 3.2. Let $\\lambda_{1}\\geq\\lambda_{2}\\geq\\cdots>0$ be eigenvalues of $T_{k,\\rho_{\\times}}$ . There are fixed positive constants $C_{s}$ and $c_{s}$ such that ", "page_idx": 4}, {"type": "equation", "text": "$$\nc_{s}i^{-1/s}\\leq\\lambda_{i}\\leq C_{s}i^{-1/s},\\ \\forall i\\in\\mathbb{N}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "for some $s\\in(0,1)$ ", "page_idx": 4}, {"type": "text", "text": "Assumption 3.3. The target function $f_{\\mathrm{0}}^{\\ast}$ satisfies ", "page_idx": 4}, {"type": "equation", "text": "$$\nf_{0}^{*}\\in\\left\\{h\\in\\mathbb{H}_{k}:h=T_{k,\\rho_{\\times}}^{r-1/2}g\\;{\\mathrm{~where~}}\\|g\\|_{\\mathbb{H}_{k}}\\leq R\\right\\}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "for some $\\textstyle r\\in[\\frac{1}{2},1]$ Where $\\boldsymbol{T}_{k,\\rho_{\\mathbf{x}}}^{r-1/2}$ is the $(r-1/2)$ power of operator $T_{k,\\rho_{\\times}}$ and $R>0$ is a fixed constant. In particular, $f_{0}^{*}\\in\\mathbb{H}_{k}$ ", "page_idx": 4}, {"type": "text", "text": "The above assumptions determine the minimax lower rate [5] and are standard assumptions in many prior works [5, 15, 33, 35]. In detail, ", "page_idx": 4}, {"type": "text", "text": "\u00b7 Assumption 3.1 implies that the noise is not excessively large. This assumption is a general noise condition that encompasses a wide range of cases. For instance, noise with Bernstein condition such as sub-Gaussian noise satisfies Assumption 3.1.   \n\u25cf Assumption 3.2 is about the eigenvalue decay of $T_{k,\\rho_{\\times}}$ . From this assumption, one can derive bounds on the effective dimension that is related to covering and entropy number conditions [15].   \n\u25cf Assumption 3.3 is related to the regularity of the target function, specifically how well the RKHS induced by the kernel $k$ represents the target function. ", "page_idx": 4}, {"type": "text", "text": "Under these assumptions, we can theoretically show the performance guarantee of DCL-KR. The proof is providedin Appendix A2. Note that $\\begin{array}{r}{\\bar{\\mathcal{E}}(h)-\\mathcal{E}(f_{0}^{*})=\\frac{1}{2}\\|\\iota_{\\rho_{\\mathbf{x}}}(h-f_{0}^{*})\\|_{L_{\\rho_{\\mathbf{x}}}^{2}}^{2}}\\end{array}$ is the excess risk of a regressor $h$ and so the quantity $\\|\\iota_{\\rho_{\\mathbf{x}}}(h-f_{0}^{*})\\|_{L_{\\rho_{\\mathbf{x}}}^{2}}$ indicates the generalization ability of $h$ ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.4. Under Assumption 3.1, 3.2, and 3.3, with $n_{0}\\geq n^{\\frac{1}{2r+s}}(\\log n)^{3}$ public inputs independenutygenerae fom $\\tilde{\\rho}_{\\mathbf{x}}$ such hatheRadon-Nikodym deriative $\\frac{d\\rho_{\\mathbf{x}}}{d\\widetilde{\\rho}_{\\mathbf{x}}}$ sarisfes ", "page_idx": 5}, {"type": "equation", "text": "$$\n0\\leq\\frac{d\\rho_{\\mathbf{x}}}{d\\widetilde{\\rho}_{\\mathbf{x}}}\\leq B\\;o n\\;\\mathcal{X}\\;\\,f o r\\;s o m e\\;B\\in[1,\\infty),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "DCL-KR gives the performance guarantee ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\|\\iota_{\\rho_{\\mathbf{x}}}(f_{i,T}-f_{0}^{*})\\|_{L_{\\rho_{\\mathbf{x}}}^{2}}\\leq C\\cdot B^{r}n^{-\\frac{r}{2r+s}}\\log n}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for all $i=1,\\cdot\\cdot\\cdot,m$ where $\\eta\\in(0,1/\\kappa^{2})$ is a fixed learning rate, $T$ is an adequate stopping rule, and the prefactor $C$ does not depend on $B,\\,m$ and $n$ ", "page_idx": 5}, {"type": "text", "text": "Since the convergence rate $n^{-\\frac{r}{2r+s}}$ is the minimax lower rate under Assumption 3.1, 3.2, and 3.3, Theorem 3.4 implies that DCL-KR has an almost same convergence rate as the minimax optimal central training when there are sufficiently many public inputs. To the best of our knowledge, this is the first work to prove the (nearly) minimax optimality of a collaborative learning algorithm that does not directly share local data or models in massively distributed statistically heterogeneous environments. For example, divide-and-conquer algorithms work for limited $m$ : Specifically, DCNY [66] assumes $m\\leq O(n^{{\\frac{2r-1}{2r+s}}})$ and DKRR-NY-CM [67] assumes $m\\leq O(n^{\\frac{2r+s-1}{2r+s}})$ . However, Theorem 3.4 does not require any condition on $m$ . Moreover, Theorem 3.4 deals with a more general setting than the theory in [48, 58]. For example, Su et al. [58] only cover $\\begin{array}{r}{r=\\frac{1}{2}}\\end{array}$ of Assumption 3.3. On the other hand, Park et al. [48] do not consider Assumption 3.2 which gives a finer result. Compared with [48], we also reduce the required size of public inputs and drop the statistical homogeneity condition. ", "page_idx": 5}, {"type": "text", "text": "The convergence rate in Theorem 3.4 has an additional factor $\\log n$ compared with a minimax lower rate [5, 15], but this logarithm term grows slower than any polynomial. Note that an additional logarithm term commonly appears in the context of gradient descent-based kernel regression with Nystrom scheme [35, 36]. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.4 allows that the public input distribution $\\tilde{\\rho}_{\\mathbf{x}}$ can be different from the local input distribution $\\rho_{\\mathbf{x}}$ . It is natural that the condition (2) is required since $\\tilde{\\rho}_{\\mathbf{x}}$ should cover $\\rho_{\\mathbf{x}}$ for fully distilling training information. We can see that the discrepancy between $\\rho_{\\mathbf{x}}$ and $\\tilde{\\rho}_{\\mathbf{x}}$ affects the upper bound in Theorem 3.4 as the multiplication of $B^{r}$ . We can remove $B^{r}$ in the upper bound by increasing public inputs. See Appendix A.3 for details. ", "page_idx": 5}, {"type": "text", "text": "3.3.1 Proof Sketch of Theorem 3.4 and Comments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In the proof of Theorem 3.4, we decompose the term $\\iota_{\\rho_{\\mathbf{x}}}(f_{i,T}-f_{0}^{*})$ into four parts, say (I), (II), (II1), and (IV) (see Eq. (7)). The proof is to bound the norms of these terms. Note that DCL-KR can also be understood as a Nystrom version of nonparametric FedAvg [58] from the recurrence relation (6). ", "page_idx": 5}, {"type": "text", "text": "(I) and (I1) appear similarly in [58], except that (I) and $(\\mathrm{II})$ incorporate projections. To handle these terms, we reinterpret the proof presented in [58] in operator form instead of matrix form and extend it to our setting. We obtain a norm bound of (Il) containing a quantity linked to the local Rademacher complexity. (Appendix A.2.2 and A.2.3) ", "page_idx": 5}, {"type": "text", "text": "Comparing with [58], (III) and (IV) are additional terms induced by the procedure that distills functional information from the local regressors. We apply techniques used in [35, 48, 53] to bound (III) and (IV). (Appendix A.2.4) ", "page_idx": 5}, {"type": "text", "text": "Note that previous works applying local Rademacher complexity-based stopping rule [50, 58] deal with thecase of $\\textstyle r={\\frac{1}{2}}$ only. In this work, we set a new stopping rule $T$ which is an extension of previous works [50, 58] and prove an extended version (Lemma A.6) of a well-known property [60]. As a result, our theory covers $r\\in[\\textstyle{\\frac{1}{2}},1]$ which affects the minimax lower rate. (Appendix A.2.5) ", "page_idx": 5}, {"type": "text", "text": "4  DCL-NN Algorithm ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we retain the problem setting from Section 3 but employ heterogeneous neural networks as the local models. Based on the theoretical results in Section 3, we propose a novel distillation-based collaborative learning algorithm DCL-NN across heterogeneous neural networks in a decentralized setting. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "A key factor contributing to the successful theoretical guarantee of DCL-KR lies not only in the linearity of kernel regression but also in the equality of kernels across local models. In fact, the public data predictions can vary in different directions, even if the same training data points are used when kernels differ (See Appendix B). Therefore, we match the kernels of local AI models. Specifically, we use linear feature kernels [18, 64] induced by the features from the last hidden layers of local AI models for kernel matching. For example, for a neural network $f:\\mathcal{X}\\to\\mathbb{R}$ where $f(\\cdot)=\\mathbf{w}^{\\top}g(\\cdot)+b,$ $g:\\mathcal{X}\\to\\mathbb{R}^{c}$ $\\mathbf{w}\\in\\mathbb{R}^{c}$ , and $b\\in\\mathbb{R}$ weuse ", "page_idx": 6}, {"type": "equation", "text": "$$\nk_{f}(\\mathbf{x}^{1},\\mathbf{x}^{2})=g(\\mathbf{x}^{1})^{\\top}g(\\mathbf{x}^{2}),\\quad\\mathbf{x}^{1},\\mathbf{x}^{2}\\in\\mathcal{X}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "as the featurekernel of $f$ . Through this idea, we can bring the setting closer to the regime of DCL-KR. Note that our theoretical results suggest that the target kernel should be a good kernel. Indeed, we observethatthenaiveensemble ", "page_idx": 6}, {"type": "equation", "text": "$$\nk=\\sum_{i=1}^{m}\\frac{n_{i}}{n}k_{f_{i}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "has a significantly better performance than individual feature kernels $k_{f_{1}},\\cdot\\cdot\\cdot\\mathrm{\\Delta},k_{f_{m}}$ (See Section 5 and Appendix B). Here, $f_{i}$ is the local model of the $i$ th party with its local feature kernel $k_{f_{i}}$ obtained by (3) $(i=1,\\cdots\\,,m)$ . Therefore, we align local feature kernels $k_{f_{1}},\\dots,k_{f_{m}}$ in a kernel distillation manner with the ensemble kernel $k$ obtained by (4). ", "page_idx": 6}, {"type": "text", "text": "For this purpose, we introduce Centered Kernel Alignment (CKA) [8] as a kernel similarity measure. CKA is a typical measure associated with the similarity of two representations of neural networks [27] and is often used for kernel matching in neural networks [44]. To compute empirical CKA between two kernels $k_{1}$ and $k_{2}$ on inputs $\\{\\mathbf{c}^{\\tilde{1}},\\cdot\\cdot\\cdot,\\mathbf{c}^{p}\\}$ , we first calculate the Gram matrices $K_{1}\\,=\\,[k_{1}(\\mathbf{c}^{j_{1}},\\mathbf{c}^{j_{2}})]_{1\\leq j_{1},j_{2}\\leq p}$ and $K_{2}\\,=\\,[\\bar{k}_{2}(\\mathbf{c}^{j_{1}},\\mathbf{c}^{j_{2}})]_{1\\leq j_{1},j_{2}\\leq p}$ .We then compute the empirical CKA via ", "page_idx": 6}, {"type": "equation", "text": "$$\n{\\widehat{\\mathrm{CKA}}}(k_{1},k_{2})={\\frac{{\\widehat{\\mathrm{HSIC}}}(K_{1},K_{2})}{\\sqrt{\\widehat{\\mathrm{HSIC}}}(K_{1},K_{1}){\\widehat{\\mathrm{HSIC}}}(K_{2},K_{2})}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Here, HSIC is an estimator of the Hilbert-Schmidt Independence Criterion (HSIC) defined as ", "page_idx": 6}, {"type": "equation", "text": "$$\n{\\widehat{\\mathrm{HSIC}}}(K_{1},K_{2})={\\frac{1}{(p-1)^{2}}}\\mathrm{tr}(K_{1}H K_{2}H)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\begin{array}{r}{H:=I_{p}-\\frac{1}{p}\\mathbf{1}\\mathbf{1}^{\\top}}\\end{array}$ is the centering matrix. In the kernel distillation procedure, the ith local party maximizes $\\widehat{\\mathrm{CKA}}(k_{f_{i}},k)$ on public inputs $Z\\;(i=1,\\cdots\\,,m)$ . Notably, this procedure requires only a single communication round for exchanging pairwise feature kernel values on public inputs, ensuring that our algorithm operates exclusively within the function space. ", "page_idx": 6}, {"type": "text", "text": "After the kernel distillation procedure, all local AI models have similar feature kernels up to constants. So we can follow an analogous process as in DCL-KR. Note that we perform learning rate scaling described inAppendix $\\mathbf{B}$ to compensate the kernel scale difference. It makes the impact of local iterations consistent. We also provide the complete algorithm (Algorithm 2) and further details for Section 4 in Appendix B. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we evaluate the performance of DCL-KR and DCL-NN. We compare them with baselines on various regression tasks. ", "page_idx": 6}, {"type": "text", "text": "Datasets  We use the following six regression datasets to evaluate the performance. Target variables are one-dimensional in all datasets. (1) Toy-1D [33] and (2) Toy-3D [6] are synthetic datasets with one-dimensional and three-dimensional inputs, respectively. (3) Energy is a tabular dataset from the UCI database [12] to predict appliances energy use with 28 features. (4) RotatedMNIST is an image dataset where it aims to predict the rotation angles for given rotated images of the MNIST [11] images. (5) UTKFace [71] and (6) IMDB-WIKI [42, 52] are image datasets for age estimation. ", "page_idx": 6}, {"type": "image", "img_path": "LdZ0u1FuXb/tmp/1c1627580337a42c78f740c7d6026acbe34bc0b4407c4fe7937cf475edc318f0.jpg", "img_caption": ["Figure 1: Performance of central Kernel Ridge Regression (centralKRR), central Kernel Regression with Gradient Descent (centralKRGD), DC-NY, DKRR-NY-CM, IED, and DCL-KR on Toy-1D and Toy-3D "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "LdZ0u1FuXb/tmp/0fadfce2b76d2bbcbc1daa83f873a1eaf602ef8b28961fbb8ef152f18751058d.jpg", "img_caption": ["Figure 2: Performance of IED and DCL-KR with $n_{0}\\approx\\alpha\\cdot n^{\\frac{1}{2r+s}}(\\log_{10}n)^{3}$ on Toy-3D "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "We compare kernel machine-based collaborative learning algorithms on two datasets Toy-1D and Toy-3D. On the other hand, we compare neural network-based collaborative learning algorithms on five datasets Toy-3D, Energy, RotatedMNIST, UTKFace, and IMDB-WIKI. ", "page_idx": 7}, {"type": "text", "text": "Baselines We compare DCL-KR with two central kernel regression models to verify our theoretical results. These two central models have the minimax optimal convergence rate. We also utilize existing decentralized kernel regression algorithms that does not directly share local data and models (DC-NY [66], DKRR-NY-CM [67], IED [48]) as baselines for DCL-KR. On the other hand, we adopt FedMD with unlabeled public inputs [30, 41], FedHeNN [44], and KT-pFL [69] as baselines forDCL-NN. ", "page_idx": 7}, {"type": "text", "text": "Setup  The number of parties ranges from 10 to 100 for kernel machine-based algorithms and is 50 for neural network-based algorithms. We construct statistically heterogeneous decentralized environments with Algorithm 3. For neural network-based algorithms, we use 4 different neural network architectures for local models in all settings. For instance, we use ResNet-18, ResNet-34, ResNet-50 [20], and MobileNetv2 [54] for large-scale image datasets. We utilize the average of Root Mean Squared Errors (RMSEs) of the local AI models on a test dataset as a performance metric. The test data points have the same distribution as the whole local data distribution. We apply FedMD with a few communication rounds for pretraining of DCL-NN. See Appendix C for detailed experimental configurations. ", "page_idx": 7}, {"type": "text", "text": "5.1   Results on Kernel Machine-based Algorithms ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The performance of DCL-KR and its baselines is presented in Figure 1. We set the number of parties $m=10,20,\\cdot\\cdot\\cdot,100$ , the number of private data points $n=50m$ , and the number of public inputs $n_{0}\\,=\\,n^{\\frac{1}{2r+s}}(\\log_{10}n)^{3}$ :Wefrst set $\\rho_{\\mathbf{x}}\\,=\\,\\tilde{\\rho}_{\\mathbf{x}}$ , i.e,the public data distribution is the same as the entire local input distribution. As shown in Figure 1, DCL-KR outperforms the baselines in all experimental settings and achieves comparable performance to the central models. This result implies that DCL-KR has not only the nearly optimal convergence rate but also the same performance as central kernel regression models. In contrast, DC-NY and DKRR-NY-CM exhibit significantly lower performance compared with DCL-KR in massively distributed environments where their theory does not cover. IED does not show a significant performance drop in massively distributed environments even though its theory is built on the statistical homogeneity condition of local data distributions. ", "page_idx": 7}, {"type": "image", "img_path": "LdZ0u1FuXb/tmp/aa29bbaffca8b99a6fe95046073b82039ba6526bc34bff1fe164c5ccdb920f1c.jpg", "img_caption": ["Figure 3: Performance of IED and DCL-KR with $\\tilde{\\rho}_{\\mathbf{x}}\\neq\\rho_{\\mathbf{x}}$ on Toy-3D "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "To further compare the performance of DCL-KR and IED, which show similar results to central models, we analyze the effect of $n_{0}$ and $\\tilde{\\rho}_{\\bf x}$ on their performance (Figure 2 and 3). Figure 2 illustrates that, as expected from the theoretical results, IED requires more public inputs than DCL-KR to achieve good performance. Moreover, when there is a public distribution shift, DCL-KR maintains its convergence rate, whereas the convergence rate of IED deteriorates. (See Appendix C.3.3 for experimental details.) Overall, our experiments validate the theoretical results of DCL-KR and demonstsrate its superiority over previous results. For additional experimental results and analyses, please refer to Appendix C.3. ", "page_idx": 8}, {"type": "text", "text": "5.2  Results on Neural Network-based Algorithms ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Table 2 shows the performance of DCL-NN and baselines on five regression tasks. We also present the performance of standalone models and centralized models to assess the performance of the collaborative algorithms. For some cases exhibiting training instability, we report the best test error (marked with asterisks) observed across all communication rounds, while relying on a fixed number of communication rounds for the other cases. ", "page_idx": 8}, {"type": "text", "text": "As can be seen in Table 2, DCL-NN outperforms the baselines on all regression tasks. Note that FedHeNN employs kernel matching similar to DCL-NN, but it lacks supervision of label prediction through collaboration, resulting in insufficient performance improvement compared to standalone models. Given the superior performance of DCL-NN, it is evident that incorporating supervised learning for label prediction alongside kernel matching is desirable. On the other hand, while FedMD performs significantly better than standalone models, the performance of DCL-NN is consistently better. Considering that we utilize FedMD for pretraining of DCL-NN, we can see that it performs better than FedMD-only collaborative learning by first training local models with FedMD and then using DCL-NN. In conclusion, the experimental results support the practical effectiveness and superiority of DCL-NN over baselines. ", "page_idx": 8}, {"type": "text", "text": "Kernel Distillation Procedure To verify the necessity of kernel distillation, we examine the changes in the performance of local feature kernels and the CKA between them during the kernel distillation procedure. We conduct this experiment on UTKFace. We utilize the RMSE of a kernel linear regression model trained on all local data as a kernel performance measure. The results are presented in Figure 4. As shown in Figure 4, both kernel performance and CKA undergo a temporary degradation due to the change of the objective function at the initial stages. However, as training progresses, both metrics recover and kernel performance surpasses its initial level. Since kernel distillation aims to ensure that all local feature kernels are similar with high performance, the experimental results verify the effectiveness of kernel distillation. ", "page_idx": 8}, {"type": "text", "text": "Table 2: Performance comparison of FedMD, FedHeNN, KT-pFL, and DCL-NN on five datasets. The values are presented as the average of RMSEs along with standard deviations. For calibration, the performance of standalone models and centralized models is also provided. ", "page_idx": 9}, {"type": "table", "img_path": "LdZ0u1FuXb/tmp/c035d2b8fb6fcbbfeb63713a9a5b698403acb3f81deee9f64afe992d0159420e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "LdZ0u1FuXb/tmp/45b0e23ee0e2b87705caeff63d78bdde055b39baf12803a6682ee224fc651a81.jpg", "img_caption": ["Figure 4: Kernel performance and CKA (with standard deviations) during the kernel distillation procedure. The performance of the target kernel obtained by (4) is also provided. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "For additional experimental results, please refer to Appendix C.4. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we analyze distillation-based collaborative learning from a nonparametric perspective and propose DCL-NN, a practical algorithm as an extension. We demonstrate that DCL-KR, a nonparametric version of FedMD, has a nearly minimax optimal convergence rate in massively distributed statistically heterogeneous environments. Inspired by DCL-KR, we propose DCL-NN, a novel distillation-based collaborative learning algorithm for heterogeneous neural networks. Our experiments confirm the theoretical results of DCL-KR and demonstrate the practical effectiveness of DCL-NN. For a discussion of the limitations of our work, please refer to Appendix D. ", "page_idx": 9}, {"type": "text", "text": "Broader Impact  Our work explores the methodologies of collaborative learning under data and model privacy preservation. In this regard, our research holds the potential to positively impact the facilitation of collaboration among AI models without raising concerns about information disclosure. On the other hand, our work does not pose any particularly noteworthy negative consequences, given its aim to contribute to the advancement of the general field of machine learning. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by the National Research Foundation of Korea(NRF) grant funded by the Korea government(MSIT) (Grant No. RS-2019-NR040050). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] E. Bagdasaryan, A. Veit, Y. Hua, D. Estrin, and V. Shmatikov. How to backdoor federated learning. In International Conference on Artificial Intelligence and Statistics, pages 2938-2948. PMLR,2020. ", "page_idx": 9}, {"type": "text", "text": "[2] P. L. Bartlett, O. Bousquet, and S. Mendelson. Local Rademacher complexities. The Annals of Statistics, 33(4):1497-1537, 2005.   \n[3]  A. Bellet, R. Guerraoui, M. Taziki, and M. Tommasi. Personalized and private peer-to-peer machine learning. In International Conference on Artifcial Intelligence and Statistics, pages 473-481. PMLR, 2018.   \n[4]  O. Bousquet. Concentration inequalities for sub-additive functions using the entropy method. In Stochastic Inequalities and Applications, pages 213-247. Springer, 2003.   \n[5]  A. Caponnetto and E. De Vito. Optimal rates for the regularized least-squares algorithm. Foundations of Computational Mathematics, 7:331-368, 2007.   \n[6]  X. Chang, S.-B. Lin, and D.-X. Zhou. Distributed semi-supervised learning with kernel ridge regression. The Journal of Machine Learning Research, 18(1):1493-1514, 2017.   \n[7] Y. J. Cho, J. Wang, T. Chirvolu, and G. Joshi. Communication-efficient and modelheterogeneous personalized federated learning via clustered knowledge transfer. IEEE Journal of Selected Topics in Signal Processing, 17(1):234-247, 2023.   \n[8]  C. Cortes, M. Mohri, and A. Rostamizadeh. Algorithms for learning kernels based on centered alignment. The Journal of Machine Learning Research, 13(1):795-828, 2012.   \n[9] F. Cucker and D. X. Zhou. Learning theory: an approximation theory viewpoint, volume 24. Cambridge University Press, 2007.   \n[10] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, M. Mao, M. Ranzato, A. Senior, P Tucker, K. Yang, et al. Large scale distributed deep networks. Advances in neural information processing systems, 25, 2012.   \n[11] L. Deng. The MNIST database of handwritten digit images for machine learning research. IEEE Signal Processing Magazine, 29(6):141-142, 2012.   \n[12] D. Dheeru and E. Karra Taniskidou. UCI machine learning repository, 2017. URL http: //archive.ics.uci.edu/ml.   \n[13]  A. Fallah, A. Mokhtari, and A. Ozdaglar. Personalized federated learning with theoretical guarantees: A model-agnostic meta-learning approach. In Advances in Neural Information Processing Systems, volume 33, pages 3557-3568, 2020.   \n[14] D. Fan, C. Mendler-Dunner, and M. Jaggi. Collaborative learning via prediction consensus. In Advances in Neural Information Processing Systems, 2023.   \n[15] S. Fischer and I. Steinwart. Sobolev norm learning rates forregularized least-squares algorithms. The Journal of Machine Learning Research, 21(1):8464-8501, 2020.   \n[16] J. Fuji, M. Fuji, T. Furuta, and R. Nakamoto. Norm inequalities equivalent to heinz inequality. Proceedings of the American Mathematical Society, 118(3):827-830, 1993.   \n[17]  Z.-C. Guo, S.-B. Lin, and D.-X. Zhou. Learning theory of distributed spectral algorithms. Inverse Problems, 33(7):074009, 2017.   \n[18] B. He and M. Ozay. Feature kernel distillation. In International Conference on Learning Representations, 2021.   \n[19]  C. He, M. Annavaram, and S. Avestimehr. Group knowledge transfer: Federated learning of large cnns at the edge. Advances in Neural Information Processing Systems, 33:14068-14080, 2020.   \n[20] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Procedings of the IEEE Conference on Computer Vision and Patten Recognition, pages 770-778, 2016.   \n[21]  G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.   \n[22]  S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing intermal covariate shift. In International Conference on Machine Learning, pages 448-456. PMLR, 2015.   \n[23] P. Kairouz, H. B. McMahan, B. Avent, A. Bellet, M. Bennis, A. N. Bhagoji, K. Bonawitz, Z. Charles, G. Cormode, R. Cummings, et al. Advances and open problems in federated learning. Foundations and Trends $\\textsuperscript{\\textregistered}$ in Machine Learning, 14(1-2):1-210, 2021.   \n[24] S. P. Karimireddy, S. Kale, M. Mohri, S. Reddi, S. Stich, and A. T. Suresh. Scaffold: Stochastic controlledaveraging forfederated learning. In International Conference on Machine Learning, pages 5132-5143. PMLR, 2020.   \n[25]  S. P. Karimireddy, L. He, and M. Jaggi. Byzantine-robust learning on heterogeneous datasets via bucketing. In International Conference on Learning Representations, 2022.   \n[26] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015.   \n[27]  S. Kornblith, M. Norouzi, H. Lee, and G. Hinton. Similarity of neural network representations revisited. In International Conference on Machine Learning, pages 3519-3529. PMLR, 2019.   \n[28]  A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[29] M. Ledoux and M. Talagrand. Probability in Banach Spaces: isoperimetry and processes. Springer Science & Business Media, 2013.   \n[30] D. Li and J. Wang. Fedmd: Heterogenous federated learning via model distillation. arXiv preprint arXiv: 1910.03581, 2019.   \n[31] T. Li, A. K. Sahu, M. Zaheer, M. Sanjabi, A. Talwalkar, and V. Smith. Federated optimization in heterogeneous networks. Proceedings of Machine Learning and Systems, 2:429-450, 2020.   \n[32]  Y. Li, H. Zhang, and Q. Lin. Kernel interpolation generalizes poorly. Biometrika, 2023.   \n[33]  Y. Li, H. Zhang, and Q. Lin. On the saturation effect of kernel ridge regression. In International Conference on Learning Representations, 2023.   \n[34] JLin and V.Cevher. Optimal convergence for distributed learning with stochastic gradient methods and spectral algorithms. Journal of Machine Learning Research, 21(147):1-63, 2020.   \n[35] J. Lin and L. Rosasco. Optimal rates for learning with Nystrom stochastic gradient methods. arXiv preprint arXiv:1710.07797, 2017.   \n[36] J Lin and L. Rosasco. Optimal rates for multi-pass stochastic gradient methods. The Journal of Machine Learning Research, 18(1):3375-3421, 2017.   \n[37] J. Lin, A. Rudi, L. Rosasco, and V. Cevher. Optimal rates for spectral algorithms with leastsquares regression over hilbert spaces. Applied and Computational Harmonic Analysis, 48(3): 868-890, 2020.   \n[38] S.-B. Lin, X. Guo, and D.-X. Zhou. Distributed learning with regularized least squares. The Journal of Machine Learning Research, 18(1):3202-3232, 2017.   \n[39]  S.-B. Lin, X. Chang, and X. Sun. Kernel interpolation of high dimensional scatered data. arXiv preprint arXiv:2009.01514, 2020.   \n[40] S.-B. Lin, D. Wang, and D.-X. Zhou. Distributed kernel ridge regression with communications. Journal of Machine Learning Research, 21(93):1-38, 2020.   \n[41] T. Lin, L. Kong, S. U. Stich, and M. Jaggi. Ensemble distillation for robust model fusion in federated learning. Advances in Neural Information Processing Systems, 33:2351-2363, 2020.   \n[42]  Y. Lin, J. Shen, Y. Wang, and M. Pantic. Fp-age: Leveraging face parsing attention for facial age estimation in the wild. arXiv, 2021.   \n[43]  Y. Liu, J. Liu, and S. Wang. Effective distributed learning with random features: Improved bounds and algorithms. In International Conference on Learning Representations, 2020.   \n[44] D. Makhija, X. Han, N. Ho, and J. Ghosh. Architecture agnostic federated learning for neural networks. In Internationai Conference on Machine Learning, pages 14860-14870. PMLR, 2022.   \n[45] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y. Arcas. Communication-efficient learning of deep networks from decentralized data. In International Conference on Artificial Intelligence and Statistics, pages 1273-1282. PMLR, 2017.   \n[46] S. Mendelson. Geometric parameters of kernel machines. In International Conference on Computational Learning Theory, pages 29-43. Springer, 2002.   \n[47] C. Mendler-Dunner, W. Guo, S. Bates, and M. Jordan. Test-time collective prediction. Advances in Neural Information Processing Systems, 34:13719-13731, 2021.   \n[48] S. Park, K. Hong, and G. Hwang. Towards understanding ensemble distillation in federated learning. In International Conference on Machine Learning, pages 27132-27187. PMLR, 2023.   \n[49]  V. 1. Paulsen and M. Raghupathi. An introduction to the theory of reproducing kernel Hilbert spaces, volume 152. Cambridge University Press, 2016.   \n[50] G. Raskutti, M. J. Wainwright, and B. Yu. Early stopping and non-parametric regression: an optimal data-dependent stopping rule. The Journal of Machine Learning Research, 15(1): 335-366, 2014.   \n[51] D. Rothchild, A. Panda, E. Ullah, N. Ivkin, I. Stoica, V. Braverman, J. Gonzalez, and R. Arora. FetchSGD: Communication-efficient federated learning with sketching. In International Conference on Machine Learning, pages 8253-8265. PMLR, 2020.   \n[52]  R. Rothe, R. Timofte, and L. V. Gool. Deep expectation of real and apparent age from a single image without facial landmarks. International Journal of Computer Vision, 126(2-4):144-157, 2018.   \n[53]  A. Rudi, R. Camoriano, and L. Rosasco. Less is more: Nystrom computational regularization. Advances in Neural Information Processing Systems, 28, 2015.   \n[54] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4510-4520, 2018.   \n[55]  R. Schaback and H. Wendland. Kernel techniques: from machine learning to meshless methods. Acta Numerica, 15:543-639, 2006.   \n[56]  B. Sen. A gentle introduction to empirical process theory and applications. Lecture Notes, Columbia University, 11:28-29, 2018.   \n[57] I. Steinwart and A. Christmann. Support vector machines. Springer Science & Busines Media, 2008.   \n[58] L. Su, J. Xu, and P. Yang. A non-parametric view of fedavg and fedprox: Beyond stationary points. The Journal of Machine Learning Research, 24(203):1-48, 2023.   \n[59]  C. T Dinh, N. Tran, and J. Nguyen. Personalized federated learning with moreau envelopes. Advances in Neural Information Processing Systems, 33:21394-21405, 2020.   \n[60] M. J. Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cambridge University Press, 2019.   \n[61] H. Wang, M. Yurochkin, Y. Sun, D. S. Papailiopoulos, and Y. Khazaeni. Federated learning with matched averaging. In International Conference on Learning Representations, 2020.   \n[62]  C. K. Williams and C. E. Rasmussen. Gaussian processes for machine learning, volume 2. MIT press Cambridge, MA, 2006.   \n[63] A. G. Wilson, Z. Hu, R. Salakhutdinov, and E. P. Xing. Deep kernel learning. In International Conference on Artificial Intelligence and Statistics, pages 370-378. PMLR, 2016.   \n[64]  G. Yang and E. J. Hu. Tensor programs iv: Feature learning in infinite-width neural networks. In International Conference on Machine Learning, pages 11727-11737. PMLR, 2021.   \n[65] Y. Yao, L. Rosasco, and A. Caponnetto. On early stopping in gradient descent learning. Constructive Approximation, 26:289-315, 2007.   \n[66] R. Yin, Y. Liu, L. Lu, W. Wang, and D. Meng. Divide-and-conquer learning with nystrom: Optimal rate and algorithm. In Proceedings of the AAAl Conference on Artificial Intelligence, volume 34, pages 6696-6703, 2020.   \n[67] R. Yin, W. Wang, and D. Meng. Distributed nystrom kernel learning with communications. In International Conference on Machine Learning, pages 12019-12028. PMLR, 2021.   \n[68] H. Zhang, Y. Li, and Q. Lin. On the optimality of misspecified spectral algorithms. arXiv preprint arXiv:2303.14942, 2023.   \n[69] J. Zhang, S. Guo, X. Ma, H. Wang, W. Xu, and F. Wu. Parameterized knowledge transfer for personalized federated learning. Advances in Neural Information Processing Systems, 34: 10092-10104, 2021.   \n[70] Y. Zhang, J. Duchi, and M. Wainwright. Divide and conquer kernel ridge regression: A distributed algorithm with minimax optimal rates. The Journal of Machine Learning Research, 16(1):3299-3340, 2015.   \n[71] Z. Zhang, Y. Song, and H. Qi. Age progression/regression by conditional adversarial autoencoder. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5810-5818, 2017.   \n[72] Z. Zhu, J. Hong, and J. Zhou. Data-free knowledge distillation for heterogeneous federated learning. In International conference on machine learning, pages 12878-12889. PMLR, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "table", "img_path": "LdZ0u1FuXb/tmp/a0da90299bdd508dfb2fdf89fd09d2a0202fccdd95d1c66a1a372b55e785b482.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "A Details on Section 3 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Before we start the proof of Theorem 3.4, we present basic notions. ", "page_idx": 14}, {"type": "text", "text": "A.1 Basic Notions ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In Subsection 3.1, the reproducing kernel Hilbert space $\\mathbb{H}_{k}$ is a subset of $C(\\mathcal{X})$ , i.e., all elements in $\\mathbb{H}_{k}$ are continuous [57]. Since ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\iota_{\\rho_{\\mathbf{x}}}^{\\top}h(\\cdot)=\\langle\\iota_{\\rho_{\\mathbf{x}}}^{\\top}h,k.\\rangle_{\\mathbb{H}_{k}}=\\langle h,\\iota_{\\rho_{\\mathbf{x}}}k.\\rangle_{L_{\\rho_{\\mathbf{x}}}^{2}}=\\int_{\\mathcal{X}}h(\\mathbf{x})k(\\cdot,\\mathbf{x})\\ d\\rho_{\\mathbf{x}}(\\mathbf{x}),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "wehave $T_{k,\\rho_{\\mathbf{x}}}=\\iota_{\\rho_{\\mathbf{x}}}^{\\top}\\iota_{\\rho_{\\mathbf{x}}}$ The compactnessof $\\iota_{\\varrho_{\\mathfrak{X}}}^{\\top}$ [57] gives the fact that $T_{k,\\rho_{\\times}}$ is compact, self-adjoint, and positive. Furthermore, Mercer's theorem [62] gives a Mercer representation ", "page_idx": 15}, {"type": "equation", "text": "$$\nk(\\mathbf{x}^{1},\\mathbf{x}^{2})=\\sum_{i=1}^{\\infty}\\lambda_{i}\\phi_{i}(\\mathbf{x}^{1})\\phi_{i}(\\mathbf{x}^{2}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The fact that $\\mathbb{H}_{k}\\subset C(\\mathcal{X})$ and $T_{k,\\rho_{\\mathbf{x}}}=\\iota_{\\rho_{\\mathbf{x}}}^{\\top}\\iota_{\\rho_{\\mathbf{x}}}$ implies the injectivity of $T_{k,\\rho_{\\mathbf{x}}}$ and so $\\lambda_{i}\\neq0$ for all $i\\in\\mathbb N$ We define ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{N}_{\\nu}(\\lambda):=\\mathrm{tr}(T_{k,\\nu}(T_{k,\\nu}+\\lambda I)^{-1})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "for any probability measure $\\nu$ . For convenience, $\\mathcal{N}(\\lambda):=\\mathcal{N}_{\\rho_{\\mathbf{x}}}(\\lambda)$ . From [5, 15], we have $\\mathcal{N}(\\lambda)\\leq$ $C_{s}^{\\prime}\\lambda^{-s}$ where $C_{s}^{\\prime}:=C_{s}^{s}/(1-s)$ and $\\mathcal{N}_{\\nu}(\\lambda)\\leq\\kappa^{2}\\lambda^{-1}$ . Given a dataset $D=\\{(\\mathbf{x}^{i},y^{i})\\}_{i=1}^{n}$ , a similar argument as above gives ", "page_idx": 15}, {"type": "equation", "text": "$$\nS_{D}^{\\top}:\\mathbf{c}=[\\mathbf{c}_{1},\\cdot\\cdot\\cdot\\cdot\\cdot,\\mathbf{c}_{n}]\\mapsto\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{c}_{i}k_{\\mathbf{x}^{i}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and $\\begin{array}{r}{T_{k,X}:h\\mapsto\\frac{1}{n}\\sum_{i=1}^{n}h(\\mathbf{x}^{i})k_{\\mathbf{x}^{i}}}\\end{array}$ ", "page_idx": 15}, {"type": "text", "text": "Note that Assumption 3.1 implies that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{E}(f_{0}^{*})=\\frac{1}{2}\\mathbb{E}_{(\\mathbf{x},y)\\sim\\rho_{\\mathbf{x},y}}|y-f_{0}^{*}(\\mathbf{x})|^{2}\\leq\\frac{\\gamma^{2}}{2}<\\infty.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{(\\mathbf{x},y)\\sim\\rho_{\\mathbf{x},y}}|y-h(\\mathbf{x})|^{2}=\\mathbb{E}_{\\mathbf{x}\\sim\\rho_{\\mathbf{x}}}|h(\\mathbf{x})-f_{0}^{*}(\\mathbf{x})|^{2}+\\mathbb{E}_{(\\mathbf{x},y)\\sim\\rho_{\\mathbf{x},y}}|y-f_{0}^{*}(\\mathbf{x})|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and so the excess risk becomes ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{E}(h)-\\mathcal{E}(f_{0}^{*})=\\frac{1}{2}\\mathbb{E}_{\\mathbf{x}\\sim\\rho_{\\mathbf{x}}}|h(\\mathbf{x})-f_{0}^{*}(\\mathbf{x})|^{2}=\\frac{1}{2}\\|\\iota_{\\rho_{\\mathbf{x}}}(h-f_{0}^{*})\\|_{L_{\\rho_{\\mathbf{x}}}^{2}}^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, $\\|\\iota_{\\rho_{\\mathbf{x}}}(h-f_{0}^{*})\\|_{L_{\\rho_{\\mathbf{x}}}^{2}}^{2}$ indicates the generalization ability of $h$ ", "page_idx": 15}, {"type": "text", "text": "Table 3 presents meaning of some notations. ", "page_idx": 15}, {"type": "text", "text": "A.2  Proof of Theorem 3.4 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Without loss of generality, we assume $n\\wedge n_{0}\\geq\\kappa^{2}e$ ", "page_idx": 15}, {"type": "text", "text": "A.2.1 Recurrence Relation of DCL-KR ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Consider a subspace $W$ of $\\mathbb{H}_{k}$ spanned by $\\{k_{\\mathbf{z}^{1}},\\cdot\\cdot\\cdot\\,,k_{\\mathbf{z}^{n_{0}}}\\}$ . We first show that for a fixed $h^{*}\\in\\mathbb{H}_{k}$ and a gradient update $\\mathcal{G}u=u-\\eta S_{Z}^{\\top}(S_{Z}u_{.}-S_{Z}h^{*})$ we have $\\mathcal{G}^{t}u_{1}\\to P_{Z}h^{*}$ as $t\\to\\infty$ for any $u_{1}\\in\\breve{W}$ where $\\dot{P}_{Z}$ is an orthogonal projection onto the subspace $W$ . Set $u_{t+1}=\\mathcal G u_{t}$ for $t\\geq\\mathrm{i}$ Then ", "page_idx": 15}, {"type": "equation", "text": "$$\nu_{t+1}=(I-\\eta S_{Z}^{\\top}S_{Z})u_{t}+\\eta S_{Z}^{\\top}S_{Z}h^{*}=(I-\\eta S_{Z}^{\\top}S_{Z})^{t}u_{1}+\\sum_{k=0}^{t-1}(I-\\eta S_{Z}^{\\top}S_{Z})^{k}\\eta S_{Z}^{\\top}S_{Z}h^{*}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since $S_{Z}h^{*}=S_{Z}P_{Z}h^{*}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{k=0}^{t-1}(I-\\eta S_{Z}^{\\top}S_{Z})^{k}\\eta S_{Z}^{\\top}S_{Z}h^{*}=\\sum_{k=0}^{t-1}(I-\\eta S_{Z}^{\\top}S_{Z})^{k}\\eta S_{Z}^{\\top}S_{Z}P_{Z}h^{*}=P_{Z}h^{*}-(I-\\eta S_{Z}^{\\top}S_{Z})^{t}P_{Z}h^{*}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note that there exists $\\{\\tilde{\\mathbf{z}}^{1},\\cdots,\\tilde{\\mathbf{z}}^{\\tilde{n}_{0}}\\}\\subset Z$ such that $\\{k_{\\tilde{\\mathbf{z}}^{1}},\\cdot\\cdot\\cdot,k_{\\tilde{\\mathbf{z}}^{\\tilde{n}_{0}}}\\}$ is a basis of $W$ . Define a matrix ", "page_idx": 15}, {"type": "equation", "text": "$$\nB=\\left[\\begin{array}{c c c}{b_{11}}&{\\ddots}&{b_{1\\tilde{n}_{0}}}\\\\ {\\vdots}&{\\ddots}&{\\vdots}\\\\ {b_{n_{0}1}}&{\\ddots}&{b_{n_{0}\\tilde{n}_{0}}}\\end{array}\\right]\\in\\mathbb{R}^{n_{0}\\times\\tilde{n}_{0}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "such that $\\begin{array}{r}{k_{\\mathbf{z}^{i}}=\\sum_{j=1}^{\\tilde{n}_{0}}b_{i j}k_{\\tilde{\\mathbf{z}}^{j}}}\\end{array}$ Then $K_{Z\\tilde{Z}}=B K_{\\tilde{Z}\\tilde{Z}}$ where ", "page_idx": 15}, {"type": "equation", "text": "$$\nK_{Z\\tilde{Z}}=\\left[\\begin{array}{c c c}{k(\\mathbf{z}^{1},\\tilde{\\mathbf{z}}^{1})}&{\\cdot\\cdot\\cdot}&{k(\\mathbf{z}^{1},\\tilde{\\mathbf{z}}^{\\tilde{n}_{0}})}\\\\ {\\vdots}&{\\cdot\\cdot}&{\\vdots}\\\\ {k(\\mathbf{z}^{n_{0}},\\tilde{\\mathbf{z}}^{1})}&{\\cdot\\cdot\\cdot}&{k(\\mathbf{z}^{n_{0}},\\tilde{\\mathbf{z}}^{\\tilde{n}_{0}})}\\end{array}\\right]\\in\\mathbb{R}^{n_{0}\\times\\tilde{n}_{0}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and ", "page_idx": 16}, {"type": "equation", "text": "$$\nK_{\\tilde{Z}\\tilde{Z}}=\\left[\\begin{array}{c c c}{k(\\tilde{\\mathbf{z}}^{1},\\tilde{\\mathbf{z}}^{1})}&{\\cdot\\cdot\\cdot}&{k(\\tilde{\\mathbf{z}}^{1},\\tilde{\\mathbf{z}}^{\\tilde{n}_{0}})}\\\\ {\\vdots}&{\\cdot\\cdot}&{\\vdots}\\\\ {k(\\tilde{\\mathbf{z}}^{\\tilde{n}_{0}},\\tilde{\\mathbf{z}}^{1})}&{\\cdot\\cdot\\cdot}&{k(\\tilde{\\mathbf{z}}^{\\tilde{n}_{0}},\\tilde{\\mathbf{z}}^{\\tilde{n}_{0}})}\\end{array}\\right]\\in\\mathbb{R}^{\\tilde{n}_{0}\\times\\tilde{n}_{0}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Set $\\begin{array}{r}{P_{Z}h^{*}=\\sum_{j=1}^{\\tilde{n}_{0}}a_{j}k_{\\tilde{\\mathbf{z}}_{j}}}\\end{array}$ . Then we can see that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\left(I-\\eta S_{Z}^{\\top}S_{Z}\\right)\\left(\\sum_{j=1}^{\\bar{n}_{0}}a_{j}k_{\\bar{z}_{j}}\\right)=\\sum_{r=1}^{\\bar{n}_{0}}\\left(a_{r}-\\frac{\\eta}{n}\\sum_{i=1}^{\\bar{n}_{0}}\\sum_{j=1}^{\\bar{n}_{0}}a_{j}k(\\tilde{\\mathbf{z}}_{j},\\mathbf{z}_{i})b_{i r}\\right)k_{\\bar{z}_{r}}}}\\\\ &{}&{=\\sum_{r=1}^{\\bar{n}_{0}}\\left(a_{r}-\\frac{\\eta}{n}[B^{\\top}B K_{\\bar{z}\\bar{z}}\\mathbf{a}]_{r}\\right)k_{\\bar{z}_{r}}=\\sum_{r=1}^{\\bar{n}_{0}}\\left[\\left(I-\\frac{\\eta}{n}B^{\\top}B K_{\\bar{z}\\bar{z}}\\right)\\mathbf{a}\\right]_{r}k_{\\bar{z}_{r}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $[\\cdot]_{r}$ is the $r$ th component of the given vector and $\\mathbf{a}\\,=\\,[a_{1},\\cdots\\,,a_{\\tilde{n}_{0}}]^{\\top}$ . Note that $K_{\\tilde{Z}\\tilde{Z}}$ .s invertible since $\\mathbf{v}^{\\top}K_{\\tilde{Z}\\tilde{Z}}\\mathbf{v}=0$ implies $\\mathbf{v}=0$ . We can also see that $K_{Z Z}=B K_{\\tilde{Z}\\tilde{Z}}B^{\\top}$ where ", "page_idx": 16}, {"type": "equation", "text": "$$\nK_{Z Z}=\\left[\\begin{array}{c c c}{k(\\mathbf{z}^{1},\\mathbf{z}^{1})}&{\\cdot\\cdot\\cdot}&{k(\\mathbf{z}^{1},\\mathbf{z}^{n_{0}})}\\\\ {\\vdots}&{\\cdot\\cdot}&{\\vdots}\\\\ {k(\\mathbf{z}^{n_{0}},\\mathbf{z}^{1})}&{\\cdot\\cdot\\cdot}&{k(\\mathbf{z}^{n_{0}},\\mathbf{z}^{n_{0}})}\\end{array}\\right]\\in\\mathbb{R}^{n_{0}\\times n_{0}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "So ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left\\|K_{\\tilde{Z}\\tilde{Z}}^{1/2}B^{\\top}B K_{\\tilde{Z}\\tilde{Z}}^{1/2}\\right\\|=\\left\\|B K_{\\tilde{Z}\\tilde{Z}}B^{\\top}\\right\\|\\leq\\left\\|B K_{\\tilde{Z}\\tilde{Z}}B^{\\top}\\right\\|_{F}\\leq n\\kappa^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus, $\\begin{array}{r}{0<\\frac{\\eta}{n}K_{\\tilde{Z}\\tilde{Z}}^{1/2}B^{\\top}B K_{\\tilde{Z}\\tilde{Z}}^{1/2}<I}\\end{array}$ and ", "page_idx": 16}, {"type": "equation", "text": "$$\n(I-\\eta S_{Z}^{\\top}S_{Z})^{t}P_{Z}h^{*}=\\sum_{r=1}^{\\tilde{n}_{0}}\\left[K_{\\tilde{Z}\\tilde{Z}}^{-1/2}\\left(I-\\frac{\\eta}{n}K_{\\tilde{Z}\\tilde{Z}}^{1/2}B^{\\top}B K_{\\tilde{Z}\\tilde{Z}}^{1/2}\\right)^{t}K_{\\tilde{Z}\\tilde{Z}}^{1/2}\\mathbf{a}\\right]_{r}k_{\\tilde{\\mathbf{z}}_{r}}\\rightarrow0\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "as $t\\to\\infty$ . Similarly, we get $(I-\\eta S_{Z}^{\\top}S_{Z})^{t}u_{1}\\to0$ as $t\\to\\infty$ . Therefore, we attain $\\mathcal{G}^{t}u_{1}\\to P_{Z}h^{*}$ as $t\\to\\infty$ for any $u_{1}\\in W$ ", "page_idx": 16}, {"type": "text", "text": "From this fact, DCL-KR has the recurrence relation ", "page_idx": 16}, {"type": "equation", "text": "$$\nf_{t}=P_{Z}\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\left(\\overline{{T}}_{k,X_{i}}^{E}f_{t-1}+\\eta\\sum_{s=0}^{E-1}\\overline{{T}}_{k,X_{i}}^{s}S_{D_{i}}^{\\top}\\mathbf{y}_{i}\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $f_{t}=f_{i,t}$ for any $i=1,\\cdot\\cdot\\cdot,m$ and $\\overline{{T}}_{k,X_{i}}:=I-\\eta T_{k,X_{i}}$ for $i=1,\\cdot\\cdot\\cdot,m$ . Then we obtain a closed form ", "page_idx": 16}, {"type": "equation", "text": "$$\nf_{t}=\\left(P_{Z}\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\overline{{T}}_{k,X_{i}}^{E}\\right)^{t}f_{0}+\\sum_{j=0}^{t-1}\\left(P_{Z}\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\overline{{T}}_{k,X_{i}}^{E}\\right)^{j}P_{Z}\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\eta\\sum_{s=0}^{E-1}\\overline{{T}}_{k,X_{i}}^{s}S_{D_{i}}^{\\top}\\mathbf{y}_{i}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We first compute ", "page_idx": 16}, {"type": "equation", "text": "$$\nf_{0}^{*}-\\left(\\left(P_{Z}\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\overline{{T}}_{k,X_{i}}^{E}\\right)^{t}f_{0}^{*}+\\sum_{j=0}^{t-1}\\left(P_{Z}\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\overline{{T}}_{k,X_{i}}^{E}\\right)^{j}P_{Z}\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\eta\\sum_{s=0}^{E-1}\\overline{{T}}_{k,X_{i}}^{s}T_{k,X_{i}}f_{0}^{*}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "From ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\eta\\sum_{s=0}^{E-1}\\overline{{T}}_{k,X_{i}}^{s}T_{k,X_{i}}=\\eta\\sum_{s=0}^{E-1}(I-\\eta T_{k,X_{i}})^{s}T_{k,X_{i}}=I-(I-\\eta T_{k,X_{i}})^{E},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{f_{0}^{*}-\\left(\\left(P_{Z}\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\overline{{T}}_{k,x_{i}}^{E}\\right)^{t}f_{0}^{*}+\\displaystyle\\sum_{j=0}^{t-1}\\left(P_{Z}\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\overline{{T}}_{k,x_{i}}^{E}\\right)^{j}P_{Z}\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\eta\\displaystyle\\sum_{s=0}^{t-1}\\overline{{T}}_{k,x_{i}}^{s}T_{k,x_{i}}f_{0}^{*}\\right)}\\\\ {\\displaystyle=f_{0}^{*}-\\left(\\left(P_{Z}\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\overline{{T}}_{k,x_{i}}^{E}\\right)^{t}f_{0}^{*}+\\displaystyle\\sum_{j=0}^{t-1}\\left(P_{Z}\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\overline{{T}}_{k,x_{i}}^{E}\\right)^{j}P_{Z}\\left(I-\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\overline{{T}}_{k,x_{i}}^{E}\\right)f_{0}^{*}\\right)}\\\\ {\\displaystyle=\\left(I+\\left(\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}P_{Z}\\overline{{T}}_{k,x_{i}}^{E}\\right)+\\cdots+\\left(\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}P_{Z}\\overline{{T}}_{k,x_{i}}^{E}\\right)^{t-1}\\right)(I-P_{Z})f_{0}^{*}}\\\\ {\\displaystyle=(I-P_{Z})f_{0}^{*}+\\left(I+\\cdots+\\left(\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}P_{Z}\\overline{{T}}_{k,x_{i}}^{E}\\right)^{t-2}\\right)P_{Z}\\left(\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\overline{{T}}_{k,x_{i}}^{E}-I\\right)(I-P_{Z})f_{0}^{*}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the last equality follows from $P_{Z}(I-P_{Z})=0$ . Thus, we obtain the equality ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle=\\iota_{\\rho_{\\mathbf{x}}}\\left(P_{Z}\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\overline{T}_{k,x_{i}}^{E}\\right)^{t}(f_{0}-f_{0}^{*})}}\\\\ {{\\displaystyle\\qquad+\\,\\iota_{\\rho_{\\mathbf{x}}}\\sum_{j=0}^{t-1}\\left(P_{Z}\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\overline{T}_{k,x_{i}}^{E}\\right)^{j}P_{Z}\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\eta\\sum_{s=0}^{E-1}\\overline{T}_{k,x_{i}}^{s}S_{D_{i}}^{\\top}(\\mathbf{y}_{i}-S_{D_{i}}f_{0}^{*})-\\iota_{\\rho_{\\mathbf{x}}}(I-P_{Z})f_{0}^{*}}}\\\\ {{\\displaystyle\\qquad+\\,\\iota_{\\rho_{\\mathbf{x}}}\\left(I+\\cdots+\\left(\\sum_{i=1}^{m}\\frac{n_{i}}{n}P_{Z}\\overline{T}_{k,x_{i}}^{E}\\right)^{t-2}\\right)P_{Z}\\left(I-\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\overline{T}_{k,x_{i}}^{E}\\right)(I-P_{Z})f_{0}^{*}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "A.2.2 Norm Bound of First Term in (7) ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We first bound the norm of the first term in (7) as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|{\\iota_{\\rho_{\\mathbf{x}}}}\\left(P_{Z}\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\overline{{T}}_{k,X_{i}}^{E}\\right)^{t}(f_{0}-f_{0}^{*})\\right\\|_{L_{\\rho_{\\mathbf{x}}}^{2}}}\\\\ &{\\leq\\left\\|{T_{k,\\rho_{\\mathbf{x}}}^{1/2}}(T_{k,X}+\\lambda I)^{-1/2}\\right\\|\\left\\|{(T_{k,X}+\\lambda I)^{1/2}\\left(P_{Z}\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\overline{{T}}_{k,X_{i}}^{E}\\right)^{t}P_{Z}f_{0}^{*}}\\right\\|_{\\mathbb{H}_{k}}}\\\\ &{\\quad\\quad+\\left\\|{T_{k,\\rho_{\\mathbf{x}}}^{1/2}}(T_{k,X}+\\lambda I)^{-1/2}\\right\\|\\left\\|{(T_{k,X}+\\lambda I)^{1/2}\\left(P_{Z}\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\overline{{T}}_{k,X_{i}}^{E}\\right)^{t}}\\right\\|\\left\\|{(I-P_{Z})f_{0}^{*}}\\right\\|_{\\mathbb{H}_{k}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\lambda>0$ . The first term in (8) is bounded as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|T_{k,\\rho_{\\mathbf{x}}}^{1/2}(T_{k,X}+\\lambda I)^{-1/2}\\right\\|\\left\\|(T_{k,X}+\\lambda I)^{1/2}\\left(P_{Z}\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\overline{T}_{k,X_{i}}^{E}\\right)^{t}P_{Z}f_{0}^{*}\\right\\|_{\\mathbb{I}_{k}}}\\\\ &{\\leq\\left\\|T_{k,\\rho_{\\mathbf{x}}}^{1/2}(T_{k,X}+\\lambda I)^{-1/2}\\right\\|\\left\\|(T_{k,X}+\\lambda I)^{1/2}\\left(P_{Z}\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\overline{T}_{k,X_{i}}^{E}\\right)^{t}P_{Z}(T_{k,X}+\\lambda I)^{r-1/2}\\right\\|}\\\\ &{\\qquad\\cdot\\left\\|(T_{k,X}+\\lambda I)^{-(r-1/2)}T_{k,\\rho_{\\mathbf{x}}}^{r-1/2}g_{0}^{*}\\right\\|_{\\mathbb{I}_{k}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left\\|(T_{k,X}+\\lambda I)^{1/2}\\left(P_{Z}\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\overline{T}_{k,X_{i}}^{E}\\right)^{t}P_{Z}(T_{k,X}+\\lambda I)^{r-1/2}\\right\\|}\\\\ {\\leq\\displaystyle\\left\\|(T_{k,X}+\\lambda I)^{1/2}\\left(P_{Z}\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\overline{T}_{k,X_{i}}^{E}P_{Z}\\right)^{t/2r}\\right\\|^{2r}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "b Lemma .8. ct $\\begin{array}{r}{A_{i}=\\overline{{T}}_{k,X_{i}}^{E}\\;\\Leftrightarrow\\;T_{k,X_{i}}=\\frac{1}{\\eta}(I-A_{i}^{1/E})}\\end{array}$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|(T_{k,X}+\\lambda I)^{1/2}\\left(P_{Z}\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\overline{{T}}_{k,X_{i}}^{E}P_{Z}\\right)^{t/2r}\\right|^{2r}}\\\\ &{=\\left\\|\\left(P_{Z}\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\overline{{T}}_{k,X_{i}}^{E}P_{Z}\\right)^{t/2r}P_{Z}\\left(T_{k,X}+\\lambda I\\right)P_{Z}\\left(P_{Z}\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\overline{{T}}_{k,X_{i}}^{E}P_{Z}\\right)^{t/2r}\\right\\|^{r}}\\\\ &{\\leq\\left(\\frac{1}{\\eta}\\left\\|\\left(\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}P_{Z}A_{i}P_{Z}\\right)^{t/2r}\\left(I-\\displaystyle\\sum_{i=1}^{n}\\frac{n_{i}}{n}P_{Z}A_{i}P_{Z}\\right)\\left(\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}P_{Z}A_{i}P_{Z}\\right)^{t/2r}\\right\\|+\\lambda\\right)^{r}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the equalityfollws from $0\\leq A_{i}\\leq I\\ \\Rightarrow\\ A_{i}^{1/E}\\geq A_{i}$ and $I\\geq P_{Z}$ Since $\\operatorname*{sup}_{x\\in[0,1]}x^{t/r}(1-$ $\\begin{array}{r}{x)=\\frac{r}{t+r}\\cdot(\\frac{t}{t+r})^{t/r}}\\end{array}$ , we attain the inequality ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left\\|(T_{k,X}+\\lambda I)^{1/2}\\left(P_{Z}\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\overline{T}_{k,X_{i}}^{E}P_{Z}\\right)^{t/2r}\\right\\|^{2r}\\leq\\left(\\frac{r}{t+r}\\cdot\\frac{1}{\\eta}\\left(\\frac{t}{t+r}\\right)^{t/r}+\\lambda\\right)^{r}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Next, Lemma A.8 gives ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|(T_{k,X}+\\lambda I)^{-(r-1/2)}T_{k,\\rho_{\\mathbf{x}}}^{r-1/2}\\right\\|\\leq\\left\\|(T_{k,X}+\\lambda I)^{-(r-1/2)}(T_{k,\\rho_{\\mathbf{x}}}+\\lambda I)^{r-1/2}\\right\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\left\\|(T_{k,X}+\\lambda I)^{-1}(T_{k,\\rho_{\\mathbf{x}}}+\\lambda I)\\right\\|^{r-1/2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|T_{k,\\rho_{\\mathsf{x}}}^{1/2}(T_{k,X}+\\lambda I)^{-1/2}\\right\\|\\leq\\left\\|(T_{k,\\rho_{\\mathsf{x}}}+\\lambda I)^{1/2}(T_{k,X}+\\lambda I)^{-1/2}\\right\\|\\leq\\left\\|(T_{k,\\rho_{\\mathsf{x}}}+\\lambda I)(T_{k,X}+\\lambda I)^{-1}\\right\\|^{1/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By Lemma A.10, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|(T_{k,\\rho_{\\bf x}}+\\lambda I)(T_{k,\\,X}+\\lambda I)^{-1}\\|\\leq2+2\\left(\\left(\\frac{2\\kappa^{2}}{n\\lambda}+\\sqrt{\\frac{4\\kappa^{2}\\mathcal{N}(\\lambda)}{n\\lambda}}\\right)\\log(2/\\delta)\\right)^{2}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "holds with conidence at least $1-\\delta$ Where $\\delta\\in(0,1)$ . Combining (9) and (10) and applying $\\begin{array}{r}{\\frac{r}{t+r}\\leq\\frac{1}{t}}\\end{array}$ and $\\begin{array}{r}{\\bigl(\\frac{t}{t+r}\\bigr)^{t/r}\\leq\\frac{1}{2}}\\end{array}$ yield ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|T_{k,\\rho_{\\mathsf{x}}}^{1/2}(T_{k,X}+\\lambda I)^{-1/2}\\right\\|\\left\\|(T_{k,X}+\\lambda I)^{1/2}\\left(P_{Z}\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\overline{T}_{k,x_{i}}^{E}\\right)^{t}P_{Z}f_{0}^{*}\\right\\|_{\\mathbb{H}_{k}}}\\\\ &{\\leq\\left(\\displaystyle\\frac{r}{t+r}\\cdot\\frac{1}{\\eta}\\left(\\displaystyle\\frac{t}{t+r}\\right)^{t/r}+\\lambda\\right)^{r}\\|g_{0}^{*}\\|_{\\mathbb{H}_{k}}\\left(2+2\\left(\\left(\\frac{2\\kappa^{2}}{n\\lambda}+\\sqrt{\\frac{4\\kappa^{2}\\mathcal{N}(\\lambda)}{n\\lambda}}\\right)\\log(2/\\delta)\\right)^{2}\\right)^{r}}\\\\ &{\\leq R\\left(\\displaystyle\\frac{1}{2\\eta t}+\\lambda\\right)^{r}\\left(2+2\\left(\\frac{2\\kappa^{2}}{n\\lambda}+\\sqrt{\\frac{4\\kappa^{2}\\mathcal{N}(\\lambda)}{n\\lambda}}\\right)^{2}\\right)^{r}(\\log(4/\\delta))^{2r}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "with confidence at least $1-\\delta$ where $\\delta\\in(0,1)$ . Therefore, putting \u5165 = n- 2r+s yields ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|T_{k,\\rho_{\\mathsf{x}}}^{1/2}(T_{k,X}+\\lambda I)^{-1/2}\\right\\|\\left\\|(T_{k,X}+\\lambda I)^{1/2}\\left(P_{Z}\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\overline{T}_{k,X_{i}}^{E}\\right)^{t}P_{Z}f_{0}^{*}\\right\\|_{\\mathbb{H}_{k}}\\right]}\\\\ &{\\leq\\left(\\displaystyle\\frac{1}{2\\eta t}+n^{-\\frac{1}{2r+s}}\\right)^{r}R\\cdot4\\Gamma(2r+1)\\left(2+2(2\\kappa^{2}+2\\kappa\\sqrt{C_{s}^{\\prime}})^{2}\\right)^{r}}\\\\ &{\\lesssim\\left(\\frac{1}{t}+n^{-\\frac{1}{2r+s}}\\right)^{r}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Here, we apply the fact that $\\begin{array}{r}{\\mathbb{E}A=\\int_{0}^{\\infty}\\mathbb{P}(A\\geq t)\\;d t}\\end{array}$ for $A\\geq0$ ", "page_idx": 18}, {"type": "text", "text": "We next turn to bound the second term in (8). Note that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|(T_{k,X}+\\lambda I)^{1/2}\\left(P_{Z}\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\overline{{T}}_{k,x_{i}}^{E}\\right)^{t}\\right|}\\\\ &{=\\left\\|\\left(\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\overline{{T}}_{k,x_{i}}^{E}P_{Z}\\right)^{t}(T_{k,X}+\\lambda I)\\left(P_{Z}\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\overline{{T}}_{k,x_{i}}^{E}\\right)^{t}\\right\\|^{1/2}}\\\\ &{\\leq\\left\\|\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\overline{{T}}_{k,x_{i}}^{E}\\right\\|\\cdot\\left\\|\\left(P_{Z}\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\overline{{T}}_{k,x_{i}}^{E}P_{Z}\\right)^{t-1}P_{Z}(T_{k,X}+\\lambda I)P_{Z}\\left(P_{Z}\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\overline{{T}}_{k,x_{i}}^{E}P_{Z}\\right)^{t-1}\\right\\|^{1/2}}\\\\ &{\\leq\\left\\|\\left(P_{Z}\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\overline{{T}}_{k,x_{i}}^{E}P_{Z}\\right)^{t-1}P_{Z}(T_{k,X}+\\lambda I)P_{Z}\\left(P_{Z}\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\overline{{T}}_{k,x_{i}}^{E}P_{Z}\\right)^{t-1}\\right\\|^{1/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "SeAUingmlarasei ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\left(P_{Z}\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\overline{T}_{k,X_{i}}^{E}P_{Z}\\right)^{t-1}P_{Z}(T_{k,X}+\\lambda I)P_{Z}\\left(P_{Z}\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\overline{T}_{k,X_{i}}^{E}P_{Z}\\right)^{t-1}\\right\\|^{1/2}}\\\\ &{\\leq\\left(\\displaystyle\\frac{1}{\\eta(2t-1)}+\\lambda\\right)^{1/2}\\leq\\left(\\frac{1}{\\eta t}+\\lambda\\right)^{1/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since $Z$ and $X$ are independent, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|T_{k,\\rho_{\\mathbf{x}}}^{1/2}(T_{k,X}+\\lambda I)^{-1/2}\\right\\|\\left\\|(T_{k,X}+\\lambda I)^{1/2}\\left(P_{Z}\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\overline{{T}}_{k,X_{i}}^{E}\\right)^{t}\\right\\|\\|(I-P_{Z})f_{0}^{*}\\|_{\\mathbb{H}_{k}}\\right]}\\\\ &{\\leq\\left(\\displaystyle\\frac{1}{\\eta t}+\\lambda\\right)^{1/2}\\mathbb{E}\\left\\|T_{k,\\rho_{\\mathbf{x}}}^{1/2}(T_{k,X}+\\lambda I)^{-1/2}\\right\\|\\cdot\\mathbb{E}\\left\\|(I-P_{Z})f_{0}^{*}\\right\\|_{\\mathbb{H}_{k}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We already see that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|T_{k,\\rho_{\\mathbf{x}}}^{1/2}(T_{k,X}+\\lambda I)^{-1/2}\\right\\|\\leq\\left(2+2\\left(\\left(\\frac{2\\kappa^{2}}{n\\lambda}+\\sqrt{\\frac{4\\kappa^{2}\\mathcal{N}(\\lambda)}{n\\lambda}}\\right)\\log(2/\\delta)\\right)^{2}\\right)^{1/2}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\left(2+2\\left(\\frac{2\\kappa^{2}}{n\\lambda}+\\sqrt{\\frac{4\\kappa^{2}\\mathcal{N}(\\lambda)}{n\\lambda}}\\right)^{2}\\right)^{1/2}\\log(4/\\delta)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "holds with confidence at least $1-\\delta$ where $\\delta\\in(0,1)$ and so ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left\\|T_{k,\\rho_{\\mathbf{x}}}^{1/2}(T_{k,X}+\\lambda I)^{-1/2}\\right\\|\\leq4\\left(2+2(2\\kappa^{2}+2\\kappa\\sqrt{C_{s}^{\\prime}})^{2}\\right)^{1/2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "by putting $\\lambda=n^{-\\frac{1}{2r+s}}$ as before. ", "page_idx": 19}, {"type": "text", "text": "The remaining part is to bound $\\mathbb{E}\\Vert(I-P_{Z})f_{0}^{*}\\Vert_{\\mathbb{H}_{k}}$ . Applying Lemma A.9 yields $\\Vert(I-P_{Z})f_{0}^{*}\\Vert_{\\mathbb{H}_{k}}\\leq$ $\\lambda_{0}^{1/2}\\Vert(T_{k,Z}+\\lambda_{0}I)^{-1/2}T_{k,\\rho_{\\times}}^{r-1/2}\\Vert\\Vert g_{0}^{*}\\Vert_{\\mathbb{H}_{k}}$ $\\lambda_{0}>0$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lambda_{0}^{1/2}\\|(T_{k,Z}+\\lambda_{0}I)^{-1/2}T_{k,\\rho_{\\mathbf{x}}}^{r-1/2}\\|\\|g_{0}^{*}\\|_{\\mathbb{H}_{k}}}\\\\ &{\\ \\leq R\\lambda_{0}^{1/2}\\|(T_{k,Z}+\\lambda_{0}I)^{-(1-r)}\\|\\|(T_{k,Z}+\\lambda_{0}I)^{-(r-1/2)}T_{k,\\rho_{\\mathbf{x}}}^{r-1/2}\\|}\\\\ &{\\ \\leq R\\lambda_{0}^{r-1/2}\\|(T_{k,Z}+\\lambda_{0}I)^{-1/2}T_{k,\\rho_{\\mathbf{x}}}^{1/2}\\|^{2r-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "From $\\begin{array}{r}{\\frac{d\\rho_{\\mathbf{x}}}{d\\tilde{\\rho}_{\\mathbf{x}}}\\leq B}\\end{array}$ , we obain ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|T_{k,\\rho_{\\mathbf{x}}}^{1/2}(T_{k,z}+\\lambda_{0}I)^{-1/2}\\|=\\|\\iota_{\\rho_{\\mathbf{x}}}(T_{k,z}+\\lambda_{0}I)^{-1/2}\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq B^{1/2}\\|\\iota_{\\tilde{\\rho}_{\\mathbf{x}}}(T_{k,z}+\\lambda_{0}I)^{-1/2}\\|=B^{1/2}\\|T_{k,\\tilde{\\rho}_{\\mathbf{x}}}^{1/2}(T_{k,z}+\\lambda_{0}I)^{-1/2}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Set $\\lambda_{0}=128(\\kappa^{2}+1)^{2}(\\log n_{0})^{3}/n_{0}$ where we assume $n$ is sufficiently large such that $\\lambda_{0}\\leq1$ and $\\begin{array}{r}{\\mathcal{N}_{\\tilde{\\rho}_{\\bf x}}(\\lambda_{0})\\geq1}\\end{array}$ for $n_{0}\\geq n^{\\frac{1}{2r+s}}(\\log n)^{3}$ . By Lemma A.8 and Lemma A.12, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|T_{k,\\tilde{\\rho}_{\\mathbf{x}}}^{1/2}(T_{k,Z}+\\lambda_{0}I)^{-1/2}\\|\\le\\|(T_{k,\\tilde{\\rho}_{\\mathbf{x}}}+\\lambda_{0}I)^{1/2}(T_{k,Z}+\\lambda_{0}I)^{-1/2}\\|\\le\\sqrt{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "holds with confidence at least $1-\\delta$ where $\\delta\\in[4\\exp(-1/4(\\kappa^{2}+1)\\mathcal{B}_{0}),1)$ and ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{B}_{0}=\\frac{1+\\log\\mathcal{N}_{\\tilde{\\rho}_{\\mathbf{x}}}(\\lambda_{0})}{\\lambda_{0}n_{0}}+\\sqrt{\\frac{1+\\log\\mathcal{N}_{\\tilde{\\rho}_{\\mathbf{x}}}(\\lambda_{0})}{\\lambda_{0}n_{0}}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|(I-P_{Z})f_{0}^{*}\\|_{\\mathbb{H}_{k}}\\le\\|f_{0}^{*}\\|_{\\mathbb{H}_{k}}=\\|T_{k,\\rho_{\\mathbf{x}}}^{r-1/2}g_{0}^{*}\\|_{\\mathbb{H}_{k}}\\le\\|T_{k,\\rho_{\\mathbf{x}}}\\|^{r-1/2}\\|g_{0}^{*}\\|_{\\mathbb{H}_{k}}\\le R\\kappa^{2r-1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}\\|(I-P_{Z})f_{0}^{*}\\|_{\\mathbb{H}_{k}}\\le R\\lambda_{0}^{r-1/2}B^{r-1/2}2^{r-1/2}+R\\kappa^{2r-1}\\cdot4\\exp\\left(-\\frac{1}{4(\\kappa^{2}+1)B_{0}}\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "From $n_{0}\\geq\\kappa^{2}e$ , we get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B_{0}\\leq\\cfrac{\\log\\kappa^{2}e+\\log n_{0}}{128(\\kappa^{2}+1)^{2}(\\log n_{0})^{3}}+\\sqrt{\\cfrac{\\log\\kappa^{2}e+\\log n_{0}}{128(\\kappa^{2}+1)^{2}(\\log n_{0})^{3}}}}\\\\ &{\\phantom{B_{0}\\leq\\cfrac{\\log n_{0}}{128(\\kappa^{2}+1)^{2}(\\log n_{0})^{3}}+\\sqrt{\\cfrac{2\\log n_{0}}{128(\\kappa^{2}+1)^{2}(\\log n_{0})^{3}}}\\leq\\frac{1}{4(\\kappa^{2}+1)\\log n_{0}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and so $\\begin{array}{r}{R\\kappa^{2r-1}\\cdot4\\exp\\left(-\\frac{1}{4(\\kappa^{2}+1)B_{0}}\\right)\\leq4R\\kappa^{2r-1}\\cdot\\frac{1}{n_{0}}}\\end{array}$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb E\\left[\\left\\|T_{k,\\rho_{\\mathsf{x}}}^{1/2}(T_{k,X}+\\lambda I)^{-1/2}\\right\\|\\left\\|(T_{k,X}+\\lambda I)^{1/2}\\left(P_{\\cal Z}\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\overline{T}_{k,X_{i}}^{\\cal E}\\right)^{t}\\right\\|\\left\\|(I-P_{\\cal Z})f_{0}^{*}\\right\\|_{\\mathbb{H}_{k}}\\right]}\\\\ &{\\leq\\left(\\frac{1}{\\eta t}+n^{-\\frac{1}{2r+s}}\\right)^{1/2}4\\left(2+2(2\\kappa^{2}+2\\kappa\\sqrt{C_{s}^{\\prime}})^{2}\\right)^{1/2}\\left(R\\lambda_{0}^{r-1/2}B^{r-1/2}2^{r-1/2}+4R\\kappa^{2r-1}\\cdot\\frac{1}{n_{0}}\\right)}\\\\ &{\\lesssim B^{r-1/2}\\left(\\frac{1}{t}+n^{-\\frac{1}{2r+s}}\\right)^{1/2}n^{-\\frac{r-1/2}{2r+s}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the last inequality comes from $n_{0}\\geq n^{\\frac{1}{2r+s}}(\\log n)^{3}$ ", "page_idx": 20}, {"type": "text", "text": "A.2.3 Norm Bound of Second Term in (7) ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Set ", "page_idx": 20}, {"type": "equation", "text": "$$\nP=\\left[\\begin{array}{c c c c c}{\\sum_{s=0}^{E-1}(I-\\eta S_{D_{1}}S_{D_{1}}^{\\top})^{s}}&{0}&{\\cdots}&{0}\\\\ {0}&{\\sum_{s=0}^{E-1}(I-\\eta S_{D_{2}}S_{D_{2}}^{\\top})^{s}}&{\\cdots}&{0}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {0}&{0}&{\\cdots}&{\\sum_{s=0}^{E-1}(I-\\eta S_{D_{m}}S_{D_{m}}^{\\top})^{s}}\\end{array}\\right].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{I-\\eta S_{D}^{\\top}P S_{D}=I-\\displaystyle\\sum_{i=1}^{m}\\displaystyle\\frac{n_{i}}{n}\\eta S_{D_{i}}^{\\top}\\displaystyle\\sum_{s=0}^{E-1}(I-\\eta S_{D_{i}}S_{D_{i}}^{\\top})^{s}S_{D_{i}}}}\\\\ {{=I-\\displaystyle\\sum_{i=1}^{m}\\displaystyle\\frac{n_{i}}{n}(I-(I-\\eta S_{D_{i}}^{\\top}S_{D_{i}})^{E})=\\displaystyle\\sum_{i=1}^{m}\\displaystyle\\frac{n_{i}}{n}\\overline{{T}}_{k,X_{i}}^{E}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then the second term in (7) becomes ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\iota_{\\rho_{\\mathbf{x}}}\\displaystyle\\sum_{j=0}^{t-1}\\left(P_{Z}\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\overline{{T}}_{k,X_{i}}^{E}\\right)^{j}P_{Z}\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\eta\\displaystyle\\sum_{s=0}^{E-1}\\overline{{T}}_{k,X_{i}}^{s}S_{D_{i}}^{\\top}\\big(\\mathbf{y}_{i}-S_{D_{i}}f_{0}^{*}\\big)}\\\\ &{=\\iota_{\\rho_{\\mathbf{x}}}\\displaystyle\\sum_{j=0}^{t-1}\\left(P_{Z}-\\eta P_{Z}S_{D}^{\\top}P S_{D}\\right)^{j}\\eta P_{Z}S_{D}^{\\top}P\\big(\\mathbf{y}-S_{D}f_{0}^{*}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We can see that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\iota_{\\rho_{x}}\\underset{j=0}{\\overset{t-1}{\\sum}}\\left(P_{Z}-\\eta P_{Z}S_{D}^{\\top}P S_{D}\\right)^{j}\\eta P_{Z}S_{D}^{\\top}P(\\mathbf{y}-S_{D}f_{0}^{*})\\right|\\right|_{L_{\\rho_{x}}^{2}}}\\\\ &{\\leq\\left\\|\\boldsymbol{T}_{k,\\rho_{x}}^{1/2}(T_{k,X}+\\lambda I)^{-1/2}\\right\\|\\left\\|(\\boldsymbol{T}_{k,X}+\\lambda I)^{1/2}\\underset{j=0}{\\overset{t-1}{\\sum}}\\left(P_{Z}-\\eta P_{Z}S_{D}^{\\top}P S_{D}\\right)^{j}\\eta P_{Z}S_{D}^{\\top}P(\\mathbf{y}-S_{D}f_{0}^{*})\\right\\|_{\\mathbb{R}_{h}}}\\\\ &{\\leq\\left\\|\\boldsymbol{T}_{k,\\rho_{x}}^{1/2}(T_{k,X}+\\lambda I)^{-1/2}\\right\\|\\left(\\left\\|\\boldsymbol{T}_{k,X}^{1/2}\\underset{j=0}{\\overset{t-1}{\\sum}}\\left(P_{Z}-\\eta P_{Z}S_{D}^{\\top}P S_{D}\\right)^{j}\\eta P_{Z}S_{D}^{\\top}P(\\mathbf{y}-S_{D}f_{0}^{*})\\right\\|_{\\mathbb{R}_{h}}}\\\\ &{\\quad+\\lambda^{1/2}\\left\\|\\underset{j=0}{\\overset{t-1}{\\sum}}\\left(P_{Z}-\\eta P_{Z}S_{D}^{\\top}P S_{D}\\right)^{j}\\eta P_{Z}S_{D}^{\\top}P(\\mathbf{y}-S_{D}f_{0}^{*})\\right\\|_{\\mathbb{R}_{h}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We first bound the expectation of the first term in the above. By the Cauchy-Schwartz inequality, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\{\\left\\|{\\cal T}_{k,\\rho_{\\mathbf{x}}}^{1/2}(T_{k,x}+\\lambda I)^{-1/2}\\right\\|\\left\\|{\\cal T}_{k,x}^{1/2}\\sum_{j=0}^{t-1}\\left(P_{Z}-\\eta P_{Z}S_{D}^{\\top}P S_{D}\\right)^{j}\\eta P_{Z}S_{D}^{\\top}P(\\mathbf{y}-S_{D}f_{0}^{*})\\right\\|_{\\mathbb{H}_{k}}\\right\\}}\\\\ &{\\leq\\left(\\mathbb{E}\\|{\\cal T}_{k,\\rho_{\\mathbf{x}}}^{1/2}(T_{k,x}+\\lambda I)^{-1/2}\\|^{2}\\right)^{1/2}\\left(\\mathbb{E}\\left\\|{\\cal T}_{k,x}^{1/2}\\sum_{j=0}^{t-1}\\left(P_{Z}-\\eta P_{Z}S_{D}^{\\top}P S_{D}\\right)^{j}\\eta P_{Z}S_{D}^{\\top}P(\\mathbf{y}-S_{D}f_{0}^{*})\\right\\|_{\\mathbb{H}_{k}}^{2}\\right)^{1/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Observe that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\left\\|T_{k,X}^{1/2}\\sum_{j=0}^{t-1}\\left(P_{Z}-\\eta P_{Z}S_{D}^{\\top}P S_{D}\\right)^{j}\\eta P_{Z}S_{D}^{\\top}P(\\mathbf{y}-S_{D}f_{0}^{*})\\right\\|_{\\mathbb{I}_{k}}}\\\\ {=\\displaystyle\\left\\|S_{D}\\sum_{j=0}^{t-1}\\left(P_{Z}-\\eta P_{Z}S_{D}^{\\top}P S_{D}\\right)^{j}\\eta P_{Z}S_{D}^{\\top}P(\\mathbf{y}-S_{D}f_{0}^{*})\\right\\|_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S_{D}\\displaystyle\\sum_{j=0}^{t-1}\\left(P_{Z}-\\eta P_{Z}S_{D}^{\\top}P S_{D}\\right)^{j}\\eta P_{Z}S_{D}^{\\top}P(\\mathbf{y}-S_{D}f_{0}^{*})}\\\\ &{=P^{-1/2}(I-(I-\\eta P^{1/2}S_{D}P_{Z}S_{D}^{\\top}P^{1/2})^{t})P^{1/2}(\\mathbf{y}-S_{D}f_{0}^{*})}\\\\ &{=(I-(I-\\eta S_{D}P_{Z}S_{D}^{\\top}P)^{t})(\\mathbf{y}-S_{D}f_{0}^{*}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Using $\\mathbb{E}(\\mathbf{y}-S_{D}f_{0}^{*})(\\mathbf{y}-S_{D}f_{0}^{*})^{\\top}\\leq\\gamma^{2}I$ we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|(I-(I-\\eta S_{D}P_{Z}S_{D}^{\\top}P)^{t})(\\mathbf{y}-S_{D}f_{0}^{*})\\right\\|_{2}^{2}|X,Z\\right]}\\\\ &{=\\frac{1}{n}\\mathbf{tr}\\left((I-(I-\\eta S_{D}P_{Z}S_{D}^{\\top}P)^{t})\\mathbb{E}\\left[(\\mathbf{y}-S_{D}f_{0}^{*})(\\mathbf{y}-S_{D}f_{0}^{*})^{\\top}\\right](I-(I-\\eta S_{D}P_{Z}S_{D}^{\\top}P)^{t})^{\\top}\\right)}\\\\ &{\\le\\frac{\\gamma^{2}}{n}\\left\\|(I-(I-\\eta S_{D}P_{Z}S_{D}^{\\top}P)^{t})\\right\\|_{H S}^{2}\\le\\frac{\\gamma^{2}E}{n}\\left\\|(I-(I-\\eta P^{1/2}S_{D}P_{Z}S_{D}^{\\top}P^{1/2})^{t})\\right\\|_{H S}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the last inequality follows from the fact that $\\|A B\\|_{H S}~\\leq~\\|A\\|\\|B\\|_{H S},~\\vert$ ABHS\u2264 $\\|A\\|_{H S}\\|B\\|$ , and ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|P^{1/2}\\|^{2}\\|P^{-1/2}\\|^{2}\\le E\\left(\\sum_{s=0}^{E-1}(1-\\eta\\kappa^{2})^{s}\\right)^{-1}\\le E.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since ", "page_idx": 21}, {"type": "equation", "text": "$$\n0\\le\\eta P^{1/2}S_{D}P z\\,S_{D}^{\\top}P^{1/2}\\le\\eta P^{1/2}S_{D}S_{D}^{\\top}P^{1/2}\\le I\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which follows from (11), we can see that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{0\\leq\\lambda_{i}(\\eta P^{1/2}S_{D}P_{Z}S_{D}^{\\top}P^{1/2})\\leq\\lambda_{i}(\\eta P^{1/2}S_{D}S_{D}^{\\top}P^{1/2})\\leq1}\\\\ {\\Rightarrow\\ \\ }&{0\\leq\\lambda_{i}(I-(I-\\eta P^{1/2}S_{D}P_{Z}S_{D}^{\\top}P^{1/2})^{t})\\leq\\lambda_{i}(I-(I-\\eta P^{1/2}S_{D}S_{D}^{\\top}P^{1/2})^{t})\\leq1}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\lambda_{i}(\\cdot)$ is the $i$ th largest eigenvalue of a given operator. Therefore, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{\\gamma^{2}E}{n}\\left\\|(I-(I-\\eta P^{1/2}S_{D}P z S_{D}^{\\top}P^{1/2})^{t})\\right\\|_{H S}^{2}\\leq\\frac{\\gamma^{2}E}{n}\\left\\|(I-(I-\\eta P^{1/2}S_{D}S_{D}^{\\top}P^{1/2})^{t})\\right\\|_{H S}^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Using (12) and $1\\wedge u^{2}\\leq1\\wedge u$ for $u\\geq0$ lead to ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lambda_{i}(I-(I-\\eta P^{1/2}S_{D}S_{D}^{\\top}P^{1/2})^{t})^{2}=(1-(1-\\eta\\lambda_{i}(P^{1/2}S_{D}S_{D}^{\\top}P^{1/2}))^{t})^{2}}\\\\ &{\\leq1\\wedge(\\eta^{2}t^{2}\\lambda_{i}(P^{1/2}S_{D}S_{D}^{\\top}P^{1/2})^{2})}\\\\ &{\\leq1\\wedge(\\eta t\\lambda_{i}(P^{1/2}S_{D}S_{D}^{\\top}P^{1/2}))}\\\\ &{\\leq1\\wedge(\\eta t E\\hat{\\lambda}_{i})}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\hat{\\lambda}_{1}\\,\\geq\\,\\cdot\\,\\cdot\\,\\geq\\,\\hat{\\lambda}_{n}$ are eigenvalues of $S_{D}S_{D}^{\\top}$ , the first inequality comes from the Bernoulli inequality, and the last inequality follows from the fact that $\\|P\\|\\leq E$ Wedefine ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{R}(\\epsilon)=\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(\\hat{\\lambda}_{i}\\wedge\\epsilon^{2})}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{\\gamma^{2}E}{n}\\left\\|\\left(I-(I-\\eta P^{1/2}S_{D}S_{D}^{\\top}P^{1/2})^{t}\\right)\\right\\|_{H S}^{2}\\leq\\gamma^{2}\\eta t E^{2}\\cdot\\mathcal{R}\\left(\\frac{1}{\\sqrt{\\eta t E}}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Similarly as in Appendix A.2.2, putting \u5165 = n 2r+s gives ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\|T_{k,\\rho_{\\mathsf{x}}}^{1/2}(T_{k,X}+\\lambda I)^{-1/2}\\|^{2}\\leq2+4\\Gamma(3)(2\\kappa^{2}+2\\kappa\\sqrt{C_{s}^{\\prime}})^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, the Cauchy-Schwartz inequality gives a bound as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|T_{k,\\rho_{\\mathbf{x}}}^{1/2}(T_{k,X}+\\lambda I)^{-1/2}\\right\\|\\left\\|T_{k,X}^{1/2}\\sum_{j=0}^{t-1}\\left(P_{Z}-\\eta P_{Z}S_{D}^{\\top}P S_{D}\\right)^{j}\\eta P_{Z}S_{D}^{\\top}P(\\mathbf{y}-S_{D}f_{0}^{*})\\right\\|_{\\mathbb{H}_{k}}\\right]}\\\\ &{\\leq\\sqrt{(2+4\\Gamma(3)(2\\kappa^{2}+2\\kappa\\sqrt{C_{s}^{\\prime}})^{2})\\gamma^{2}\\eta t E^{2}}\\cdot\\left(\\mathbb{E}\\mathcal{R}\\left(\\frac{1}{\\sqrt{\\eta t E}}\\right)^{2}\\right)^{1/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We now bound the expectation of ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\lambda^{1/2}\\|T_{k,\\rho_{\\mathbf{x}}}^{1/2}(T_{k,X}+\\lambda I)^{-1/2}\\|\\left\\|\\sum_{j=0}^{t-1}\\left(P_{Z}-\\eta P_{Z}S_{D}^{\\top}P S_{D}\\right)^{j}\\eta P_{Z}S_{D}^{\\top}P(\\mathbf{y}-S_{D}f_{0}^{*})\\right\\|_{\\mathbb{H}_{k}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By the Cauchy-Schwartz inequality and the same argument as before, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{\\Sigma}\\left[\\lambda^{1/2}\\lVert T_{k,\\rho_{\\mathbf{x}}}^{1/2}(T_{k,x}+\\lambda I)^{-1/2}\\rVert\\left\\lVert\\displaystyle\\sum_{j=0}^{t-1}\\left(P_{Z}-\\eta P_{Z}S_{D}^{\\top}P S_{D}\\right)^{j}\\,\\eta P_{Z}S_{D}^{\\top}P(\\mathbf{y}-S_{D}f_{0}^{*})\\right\\rVert_{\\mathbb{H_{k}}}\\right]}\\\\ &{\\leq(2+4\\Gamma(3)(2\\kappa^{2}+2\\kappa\\sqrt{C_{s}^{\\prime}})^{2})^{1/2}\\left(\\lambda\\cdot\\mathbb{E}\\left\\lVert\\displaystyle\\sum_{j=0}^{t-1}\\left(P_{Z}-\\eta P_{Z}S_{D}^{\\top}P S_{D}P_{Z}\\right)^{j}\\eta P_{Z}S_{D}^{\\top}P(\\mathbf{y}-S_{D}f_{0}^{*})\\right\\rVert_{\\mathbb{H_{k}}}^{2}\\right)^{1/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Also, the same argument as before yields ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\mathbb{E}\\left\\|\\sum_{j=0}^{t-1}\\left(P_{Z}-\\eta P_{Z}S_{D}^{\\top}P S_{D}P_{Z}\\right)^{j}\\eta P_{Z}S_{D}^{\\top}P(\\mathbf{y}-S_{D}f_{0}^{*})\\right\\|_{\\mathbb{H}_{k}}^{2}=\\displaystyle\\frac{1}{n}\\mathbb{E}\\left[\\left(\\mathbf{y}-S_{D}f_{0}^{*}\\right)^{\\top}A(\\mathbf{y}-S_{D}f_{0}^{*})\\right]}\\\\ {\\displaystyle\\leq\\frac{\\gamma^{2}}{n}\\mathbb{E}[\\mathbf{tr}(A)]}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A=\\eta P S_{D}P_{Z}\\left(\\displaystyle\\sum_{j=0}^{t-1}\\left(P_{Z}-\\eta P_{Z}S_{D}^{\\top}P S_{D}P_{Z}\\right)^{j}\\right)^{2}\\eta P_{Z}S_{D}^{\\top}P}\\\\ &{=\\eta P S_{D}P_{Z}\\left(\\displaystyle\\sum_{j=0}^{t-1}\\left(I-\\eta P_{Z}S_{D}^{\\top}P S_{D}P_{Z}\\right)^{j}\\right)^{2}\\eta P_{Z}S_{D}^{\\top}P.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "To bound $\\mathbb{E}[\\mathbf{tr}(A)]$ , note that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathrm{tr}(A)\\leq E\\cdot\\mathrm{tr}\\left(\\eta P^{1/2}S_{D}P_{Z}\\left(\\displaystyle\\sum_{j=0}^{t-1}\\left(I-\\eta P_{Z}S_{D}^{\\top}P S_{D}P_{Z}\\right)^{j}\\right)^{2}\\eta P_{Z}S_{D}^{\\top}P^{1/2}\\right)}\\\\ &{}&{=\\eta E\\cdot\\mathrm{tr}\\left(\\eta P^{1/2}S_{D}P_{Z}S_{D}^{\\top}P^{1/2}\\left(\\displaystyle\\sum_{j=0}^{t-1}(I-\\eta P^{1/2}S_{D}P_{Z}S_{D}^{\\top}P^{1/2})^{j}\\right)^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Let $B=\\eta P^{1/2}S_{D}P_{Z}S_{D}^{\\top}P^{1/2}$ . Then $0\\leq B\\leq I$ and ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\eta E\\cdot\\mathrm{tr}\\left(\\eta P^{1/2}S_{D}P_{Z}S_{D}^{\\top}P^{1/2}\\left(\\displaystyle\\sum_{j=0}^{t-1}(I-\\eta P^{1/2}S_{D}P_{Z}S_{D}^{\\top}P^{1/2})^{j}\\right)^{2}\\right)}\\\\ &{=\\eta E\\displaystyle\\sum_{i=1}^{n}\\lambda_{i}(B)\\left(\\displaystyle\\sum_{j=0}^{t-1}(1-\\lambda_{i}(B))^{j}\\right)^{2}=\\eta E\\displaystyle\\sum_{i=1}^{n}\\frac{1}{\\lambda_{i}(B)}(1-(1-\\lambda_{i}(B))^{t})^{2}}\\\\ &{\\leq\\eta E\\displaystyle\\sum_{i=1}^{n}\\frac{1}{\\lambda_{i}(B)}\\wedge(t^{2}\\lambda_{i}(B))\\leq\\eta E\\displaystyle\\sum_{i=1}^{n}t\\wedge(t^{2}\\lambda_{i}(B))}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the first inequality follows from $1-x^{t}\\leq1\\land t(1-x)$ and the second inequality follows from $1/x\\wedge t^{2}x\\leq t\\wedge t^{2}x$ for all $t\\geq0$ and $x\\in[0,1]$ . From the fact that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\lambda_{i}(B)\\leq\\eta\\|P^{1/2}\\|^{2}\\lambda_{i}(S_{D}P_{Z}S_{D}^{\\top})\\leq\\eta E\\lambda_{i}(S_{D}S_{D}^{\\top})=\\eta E\\hat{\\lambda}_{i},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\eta E\\sum_{i=1}^{n}t\\wedge(t^{2}\\lambda_{i}(B))\\leq\\eta E\\sum_{i=1}^{n}t\\wedge(\\eta t^{2}E\\hat{\\lambda}_{i})=n\\eta^{2}t^{2}E^{2}\\cdot\\mathcal{R}\\left(\\frac{1}{\\sqrt{\\eta t E}}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore, we obtain ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\lambda^{1/2}\\lVert T_{k,\\rho_{\\mathbf{x}}}^{1/2}(T_{k,X}+\\lambda I)^{-1/2}\\rVert\\left\\lVert\\displaystyle\\left\\lVert\\sum_{j=0}^{t-1}\\left(P_{Z}-\\eta P_{Z}S_{D}^{\\top}P S_{D}\\right)^{j}\\eta P_{Z}S_{D}^{\\top}P(\\mathbf{y}-S_{D}f_{0}^{*})\\right\\rVert_{\\mathbb{H}_{k}}\\right]}\\\\ &{\\leq\\sqrt{(2+4\\Gamma(3)(2\\kappa^{2}+2\\kappa\\sqrt{C_{s}^{\\prime}})^{2})(\\lambda\\gamma^{2}\\eta^{2}t^{2}E^{2})}\\cdot\\left(\\mathbb{E}\\mathcal{R}\\left(\\frac{1}{\\sqrt{\\eta t E}}\\right)^{2}\\right)^{1/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In conclusion, we have an upper bound of the norm of the second term in (7) as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left\\|\\iota_{\\rho_{x}}\\sum_{j=0}^{t-1}\\left(P_{Z}-\\eta P_{Z}S_{D}^{\\top}P S_{D}\\right)^{j}\\eta P_{Z}S_{D}^{\\top}P(\\mathbf{y}-S_{D}f_{0}^{*})\\right\\|_{L_{\\rho_{x}}^{2}}}\\\\ &{\\leq\\sqrt{(2+4\\Gamma(3)(2\\kappa^{2}+2\\kappa\\sqrt{C_{s}^{\\prime}})^{2})\\gamma^{2}\\eta t E^{2}}\\cdot\\left(\\mathbb{E}R\\left(\\frac{1}{\\sqrt{\\eta t E}}\\right)^{2}\\right)^{1/2}}\\\\ &{\\qquad+\\sqrt{(2+4\\Gamma(3)(2\\kappa^{2}+2\\kappa\\sqrt{C_{s}^{\\prime}})^{2})(\\lambda\\gamma^{2}\\eta^{2}t^{2}E^{2})}\\cdot\\left(\\mathbb{E}\\mathcal{R}\\left(\\frac{1}{\\sqrt{\\eta t E}}\\right)^{2}\\right)^{1/2}}\\\\ &{\\lesssim\\left(t^{1/2}+n^{-\\frac{1/2}{2r+s}}t\\right)\\cdot\\left(\\mathbb{E}\\mathcal{R}\\left(\\frac{1}{\\sqrt{\\eta t E}}\\right)^{2}\\right)^{1/2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "by taking $\\lambda=n^{-\\frac{1}{2r+s}}$ . We will bound $\\begin{array}{r}{\\mathbb{E}\\mathcal{R}\\left(\\frac{1}{\\sqrt{\\eta t E}}\\right)^{2}}\\end{array}$ in Appendix A.2.5. ", "page_idx": 23}, {"type": "text", "text": "A.2.4 Norm Bound of Third and Last Term in (7) ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Note that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\iota_{\\rho_{\\infty}}\\left(I+\\cdots+\\left(\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}P_{Z}\\overline{{T}}_{k,x_{i}}^{E}\\right)^{t-2}\\right)P_{Z}\\left(I-\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\overline{{T}}_{k,x_{i}}^{E}\\right)(I-P_{Z})f_{0}^{*}\\right\\|_{L_{\\rho_{\\infty}}^{2}}}\\\\ &{\\leq\\|T_{k,\\rho_{\\infty}}^{1/2}(T_{k,X}+\\lambda I)^{-1/2}\\|}\\\\ &{\\qquad\\cdot\\left\\|(T_{k,X}+\\lambda I)^{1/2}\\left(I+\\cdots+\\left(\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}P_{Z}\\overline{{T}}_{k,x_{i}}^{E}\\right)^{t-2}\\right)P_{Z}\\left(I-\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\overline{{T}}_{k,x_{i}}^{E}\\right)^{1/2}\\right\\|}\\\\ &{\\qquad\\cdot\\left\\|\\left(I-\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\overline{{T}}_{k,x_{i}}^{E}\\right)^{1/2}(I-P_{Z})f_{0}^{*}\\right\\|_{\\mathbb{H}_{k}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $0<\\lambda\\leq1$ From (l)and $0\\leq P\\leq E I$ we have $\\begin{array}{r}{0\\leq I-\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\overline{T}_{k,X_{i}}^{E}=\\eta S_{D}^{\\top}P S_{D}\\leq}\\end{array}$ $\\eta E S_{D}^{\\top}S_{D}=\\eta E T_{k,X}$ Using this fact, we find that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|(T_{k,X}+\\lambda I)^{1/2}\\left(I+\\dots+\\left(\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}P_{Z}\\overline{{T}}_{k,X_{i}}^{E}\\right)^{t-2}\\right)P_{Z}\\left(I-\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\overline{{T}}_{k,X_{i}}^{E}\\right)^{1/2}\\right|}\\\\ &{\\leq(\\eta E)^{1/2}\\left(\\left\\|T_{k,X}^{1/2}\\left(I+\\dots+\\left(\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}P_{Z}\\overline{{T}}_{k,X_{i}}^{E}\\right)^{t-2}\\right)P_{Z}T_{k,X}^{1/2}\\right\\|\\right.}\\\\ &{\\qquad\\left.+\\lambda^{1/2}\\left\\|\\left(I+\\dots+\\left(\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}P_{Z}\\overline{{T}}_{k,X_{i}}^{E}\\right)^{t-2}\\right)P_{Z}T_{k,X}^{1/2}\\right\\|\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "To bound this, we first observe that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|T_{k,X}^{1/2}\\left(I+\\cdot\\cdot+\\left(\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}P_{Z}\\overline{{T}}_{k,X_{i}}^{E}\\right)^{t-2}\\right)P_{Z}T_{k,X}^{1/2}\\right|}\\\\ &{\\ \\leq\\displaystyle\\sum_{j=0}^{t-2}\\left\\|T_{k,X}^{1/2}P_{Z}\\left(\\sum_{i=1}^{m}\\frac{n_{i}}{n}P_{Z}\\overline{{T}}_{k,X_{i}}^{E}P_{Z}\\right)^{j}P_{Z}T_{k,X}^{1/2}\\right\\|}\\\\ &{\\ \\leq\\displaystyle\\sum_{j=0}^{t-2}\\left\\|\\left(\\sum_{i=1}^{m}\\frac{n_{i}}{n}P_{Z}\\overline{{T}}_{k,X_{i}}^{E}P_{Z}\\right)^{j/2}P_{Z}T_{k,X}P_{Z}\\left(\\sum_{i=1}^{m}\\frac{n_{i}}{n}P_{Z}\\overline{{T}}_{k,X_{i}}^{E}P_{Z}\\right)^{j/2}\\right\\|\\leq\\frac{1}{\\eta}\\left(1+\\displaystyle\\sum_{j=1}^{t-2}\\frac{1}{j}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the last inequality follows by a similar calculation as before. On the other hand, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\left(I+\\cdots+\\left(\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}P_{Z}\\overline{{T}}_{k,X_{i}}^{E}\\right)^{t-2}\\right)P_{Z}T_{k,X}^{1/2}\\right\\|}\\\\ &{\\leq\\displaystyle\\sum_{j=0}^{t-2}\\left\\|\\left(\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}P_{Z}\\overline{{T}}_{k,X_{i}}^{E}P_{Z}\\right)^{j}P_{Z}T_{k,X}^{1/2}\\right\\|}\\\\ &{=\\displaystyle\\sum_{j=0}^{t-2}\\left\\|\\left(\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}P_{Z}\\overline{{T}}_{k,X_{i}}^{E}P_{Z}\\right)^{j}P_{Z}T_{k,X}P_{Z}\\left(\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}P_{Z}\\overline{{T}}_{k,X_{i}}^{E}P_{Z}\\right)^{j}\\right\\|^{1/2}\\leq\\frac{1}{\\sqrt{\\eta}}\\left(1+\\displaystyle\\sum_{j=1}^{t-2}\\frac{1}{\\sqrt{2j}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "by the same argument. Using a simple calculation, we get ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{1}{\\eta}\\left(1+\\sum_{j=1}^{t-2}\\frac{1}{j}\\right)\\le\\frac{1}{\\eta E}(2+\\log t)E\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{1}{\\sqrt{\\eta}}\\left(1+\\sum_{j=1}^{t-2}\\frac{1}{\\sqrt{2j}}\\right)\\le\\frac{1}{\\sqrt{\\eta}}+\\frac{1}{\\sqrt{2\\eta}}(2\\sqrt{t-2}-1)\\le\\frac{1}{(\\eta E)^{1/2}}\\cdot\\sqrt{6t E}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Note that the norm of the third term in (7) is bounded as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\iota_{\\rho_{\\mathbf{x}}}(I-P_{Z})f_{0}^{*}\\|_{L_{\\rho_{\\mathbf{x}}}^{2}}\\leq\\left\\|T_{k,\\rho_{\\mathbf{x}}}^{1/2}(T_{k,X}+\\lambda I)^{-1/2}\\right\\|\\left\\|(T_{k,X}+\\lambda I)^{1/2}(I-P_{Z})f_{0}^{*}\\right\\|_{\\mathbb{H}_{k}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore, the norm of the sum of the third and last terms in (7) is bounded by ", "page_idx": 24}, {"type": "equation", "text": "$$\n(1+2E+E\\log t+\\sqrt{6\\eta t\\lambda}E)\\left\\|T_{k,\\rho_{\\mathbf{x}}}^{1/2}(T_{k,X}+\\lambda I)^{-1/2}\\right\\|\\left\\|(T_{k,X}+\\lambda I)^{1/2}(I-P_{Z})f_{0}^{*}\\right\\|_{\\mathbb{H}_{k}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "To bound $\\left\\Vert(T_{k,X}+\\lambda I)^{1/2}(I-P_{Z})f_{0}^{*}\\right\\Vert_{\\mathbb{H}_{k}}$ , observe that ", "page_idx": 24}, {"type": "text", "text": "\u4e00 $|(T_{k,x}+\\lambda I)^{1/2}(I-P z)f_{0}^{*}||_{\\mathbb{H}_{k}}\\leq\\|(T_{k,x}+\\lambda I)^{1/2}(T_{k,\\rho_{\\mathrm{x}}}+\\lambda I)^{-1/2}\\|\\|(T_{k,\\rho_{\\mathrm{x}}}+\\lambda I)^{1/2}(I-P z)f_{0}^{*}\\|_{\\mathbb{H}_{k}}$and", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\|(T_{k,\\rho_{\\mathbf{x}}}+\\lambda I)^{1/2}(I-P_{Z})f_{0}^{*}\\|_{\\mathbb{H}_{k}}^{2}=\\|\\iota_{\\rho_{\\mathbf{x}}}(I-P_{Z})f_{0}^{*}\\|_{L_{\\rho_{\\mathbf{x}}}^{2}}^{2}+\\lambda\\|(I-P_{Z})f_{0}^{*}\\|_{\\mathbb{H}_{k}}^{2}}}\\\\ &{\\leq B\\|\\iota_{\\tilde{\\rho}_{\\mathbf{x}}}(I-P_{Z})f_{0}^{*}\\|_{L_{\\rho_{\\mathbf{x}}}^{2}}^{2}+\\lambda\\|(I-P_{Z})f_{0}^{*}\\|_{\\mathbb{H}_{k}}^{2}}\\\\ &{=B\\|(T_{k,\\tilde{\\rho}_{\\mathbf{x}}}+\\lambda I)^{1/2}(I-P_{Z})f_{0}^{*}\\|_{\\mathbb{H}_{k}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Under Assumption 3.3, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B^{1/2}\\|(T_{k,\\tilde{\\rho}_{\\mathbf{x}}}+\\lambda I)^{1/2}(I-P_{Z})f_{0}^{*}\\|_{\\mathbb{I}_{k}}}\\\\ &{\\leq B^{1/2}\\|(T_{k,\\tilde{\\rho}_{\\mathbf{x}}}+\\lambda I)^{1/2}(I-P_{Z})(T_{k,\\tilde{\\rho}_{\\mathbf{x}}}+\\lambda I)^{r-1/2}\\|\\|(T_{k,\\tilde{\\rho}_{\\mathbf{x}}}+\\lambda I)^{-(r-1/2)}T_{k,\\rho_{\\mathbf{x}}}^{r-1/2}\\|\\|g_{0}^{*}\\|_{\\mathbb{I}_{k}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Since ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|(T_{k,\\tilde{\\rho}_{\\mathbf{x}}}+\\lambda I)^{-(r-1/2)}T_{k,\\rho_{\\mathbf{x}}}^{r-1/2}\\|\\leq\\|(T_{k,\\tilde{\\rho}_{\\mathbf{x}}}+\\lambda I)^{-1/2}T_{k,\\rho_{\\mathbf{x}}}^{1/2}\\|^{2r-1}=\\|T_{k,\\rho_{\\mathbf{x}}}^{1/2}(T_{k,\\tilde{\\rho}_{\\mathbf{x}}}+\\lambda I)^{-1/2}\\|^{2r-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which follows from Lemma A.8 and ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|T_{k,\\rho_{\\mathbf{x}}}^{1/2}(T_{k,\\bar{\\rho}_{\\mathbf{x}}}+\\lambda I)^{-1/2}\\|=\\|\\iota_{\\rho_{\\mathbf{x}}}(T_{k,\\bar{\\rho}_{\\mathbf{x}}}+\\lambda I)^{-1/2}\\|\\le B^{1/2}\\|\\iota_{\\bar{\\rho}_{\\mathbf{x}}}(T_{k,\\bar{\\rho}_{\\mathbf{x}}}+\\lambda I)^{-1/2}\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\le B^{1/2}\\|T_{k,\\bar{\\rho}_{\\mathbf{x}}}^{1/2}(T_{k,\\bar{\\rho}_{\\mathbf{x}}}+\\lambda I)^{-1/2}\\|\\le B^{1/2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "we have $\\|(T_{k,\\tilde{\\rho}_{\\mathbf{x}}}+\\lambda I)^{-(r-1/2)}T_{k,\\rho_{\\mathbf{x}}}^{r-1/2}\\|\\leq B^{r-1/2}$ . On the other hand, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|(T_{k,\\tilde{\\rho}_{\\mathbf{x}}}+\\lambda I)^{1/2}(I-P_{Z})(T_{k,\\tilde{\\rho}_{\\mathbf{x}}}+\\lambda I)^{r-1/2}\\|}\\\\ &{\\leq\\|(T_{k,\\tilde{\\rho}_{\\mathbf{x}}}+\\lambda I)^{1/2}(I-P_{Z})\\|\\|(I-P_{Z})^{2r-1}(T_{k,\\tilde{\\rho}_{\\mathbf{x}}}+\\lambda I)^{r-1/2}\\|}\\\\ &{\\leq\\|(T_{k,\\tilde{\\rho}_{\\mathbf{x}}}+\\lambda I)^{1/2}(I-P_{Z})\\|^{2r}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "by Lemma A.8. Therefore, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|T_{k,\\rho_{\\mathbf{x}}}^{1/2}(T_{k,X}+\\lambda I)^{-1/2}\\right\\|\\left\\|(T_{k,X}+\\lambda I)^{1/2}(I-P_{Z})f_{0}^{*}\\right\\|_{\\mathbb{H}_{k}}}\\\\ &{\\leq R B^{r}\\left\\|(T_{k,\\rho_{\\mathbf{x}}}+\\lambda I)^{1/2}(T_{k,X}+\\lambda I)^{-1/2}\\right\\|}\\\\ &{\\qquad\\cdot\\|(T_{k,X}+\\lambda I)^{1/2}(T_{k,\\rho_{\\mathbf{x}}}+\\lambda I)^{-1/2}\\|\\|(T_{k,\\tilde{\\rho}_{\\mathbf{x}}}+\\lambda I)^{1/2}(I-P_{Z})\\|^{2r}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Since $X$ and $Z$ are independent, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|T_{k,\\rho_{\\mathbf{x}}}^{1/2}(T_{k,X}+\\lambda I)^{-1/2}\\right\\|\\left\\|(T_{k,X}+\\lambda I)^{1/2}(I-P_{Z})f_{0}^{*}\\right\\|_{\\mathbb{H}_{k}}\\right]}\\\\ &{\\leq R B^{r}\\mathbb{E}\\left[\\left\\|(T_{k,\\rho_{\\mathbf{x}}}+\\lambda I)^{1/2}(T_{k,X}+\\lambda I)^{-1/2}\\right\\|\\left\\|(T_{k,X}+\\lambda I)^{1/2}(T_{k,\\rho_{\\mathbf{x}}}+\\lambda I)^{-1/2}\\right\\|\\right]}\\\\ &{\\qquad\\cdot\\mathbb{E}\\|(T_{k,\\tilde{\\rho}_{\\mathbf{x}}}+\\lambda I)^{1/2}(I-P_{Z})\\|^{2r}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By Lemma A.8 and Lemma A.10, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|(T_{k,\\rho_{\\bf x}}+\\lambda I)^{1/2}(T_{k,\\,X}+\\lambda I)^{-1/2}\\|\\leq\\left(2+2\\left(\\left(\\frac{2\\kappa^{2}}{n\\lambda}+\\sqrt{\\frac{4\\kappa^{2}\\mathcal{N}(\\lambda)}{n\\lambda}}\\right)\\log(2/\\delta)\\right)^{2}\\right)^{1/2}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "holds with confidence at least $1-\\delta$ where $\\delta\\in(0,1)$ . Also, by Lemma A.8 and Lemma A.11 ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|(T_{k,X}+\\lambda I)^{1/2}(T_{k,\\rho_{\\mathbf{x}}}+\\lambda I)^{-1/2}\\|\\leq\\left(1+\\left(\\frac{2\\kappa^{2}}{n\\lambda}+\\sqrt{\\frac{4\\kappa^{2}\\mathcal{N}(\\lambda)}{n\\lambda}}\\right)\\log(2/\\delta)\\right)^{1/2}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "holds with confidence at least $1-\\delta$ where $\\delta\\in(0,1)$ . Thus, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|(T_{k,\\rho_{\\mathbf{x}}}+\\lambda I)^{1/2}(T_{k,X}+\\lambda I)^{-1/2}\\|\\|(T_{k,X}+\\lambda I)^{1/2}(T_{k,\\rho_{\\mathbf{x}}}+\\lambda I)^{-1/2}\\|}\\\\ &{\\leq\\left(2+2\\left(\\frac{2\\kappa^{2}}{n\\lambda}+\\sqrt{\\frac{4\\kappa^{2}\\mathcal{N}(\\lambda)}{n\\lambda}}\\right)^{2}\\right)^{1/2}\\left(1+\\left(\\frac{2\\kappa^{2}}{n\\lambda}+\\sqrt{\\frac{4\\kappa^{2}\\mathcal{N}(\\lambda)}{n\\lambda}}\\right)\\right)^{1/2}(\\log(4/\\delta))^{3/2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "with confidence at least $1-\\delta$ where $\\delta\\in(0,1)$ . Set $\\lambda=128(\\kappa^{2}+1)^{2}n^{-\\frac{1}{2r+s}}$ where $n$ is sufficiently large such that $\\lambda\\leq1$ and $\\begin{array}{r}{\\mathcal{N}_{\\tilde{\\rho}_{\\mathbf{x}}}(\\lambda)\\geq1}\\end{array}$ . Then ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|(T_{k,\\rho_{\\mathbf{x}}}+\\lambda I)^{1/2}(T_{k,X}+\\lambda I)^{-1/2}\\right\\|\\|(T_{k,X}+\\lambda I)^{1/2}(T_{k,\\rho_{\\mathbf{x}}}+\\lambda I)^{-1/2}\\|\\right]}\\\\ &{\\leq4\\Gamma\\left(2.5\\right)(2+2(2\\kappa^{2}+2\\kappa\\sqrt{C_{s}^{\\prime}})^{2})^{1/2}(1+2\\kappa^{2}+2\\kappa\\sqrt{C_{s}^{\\prime}})^{1/2}\\lesssim1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We now bound $\\mathbb{E}\\|(T_{k,\\tilde{\\rho}_{\\mathbf{x}}}+\\lambda I)^{1/2}(I-P_{Z})\\|^{2r}$ . By Lemma A.9, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|(T_{k,\\tilde{\\rho}_{\\mathbf{x}}}+\\lambda I)^{1/2}(I-P z)\\|^{2r}\\leq\\lambda^{r}\\|(T_{k,\\tilde{\\rho}_{\\mathbf{x}}}+\\lambda I)^{1/2}(T_{k,Z}+\\lambda I)^{-1/2}\\|^{2r}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By Lemma A.12, $\\begin{array}{r l r}{\\|(T_{k,\\tilde{\\rho}_{\\mathbf{x}}}\\,+\\,\\lambda I)^{1/2}(T_{k,Z}\\,+\\,\\lambda I)^{-1/2}\\|}&{\\leq}&{\\sqrt{2}}\\end{array}$ with confidence at least $1\\,-$ $4\\exp(-1/4(\\kappa^{2}+1)B_{0})$ where ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathcal{B}_{0}=\\frac{1+\\log\\mathcal{N}_{\\tilde{\\rho}_{\\mathbf{x}}}(\\lambda)}{\\lambda n_{0}}+\\sqrt{\\frac{1+\\log\\mathcal{N}_{\\tilde{\\rho}_{\\mathbf{x}}}(\\lambda)}{\\lambda n_{0}}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Also, $\\lVert(T_{k,\\tilde{\\rho}_{\\mathbf{x}}}+\\lambda I)^{1/2}(I-P_{Z})\\rVert\\leq(\\kappa^{2}+1)^{1/2}$ almost surely. Thus, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Vert\\bigl(T_{k,\\tilde{\\rho}_{\\mathbf{x}}}+\\lambda I\\bigr)^{1/2}(I-P_{Z})\\Vert^{2r}\\leq2^{r}\\lambda^{r}+\\left(\\kappa^{2}+1\\right)^{r}\\cdot4\\exp\\left(-\\frac{1}{4(\\kappa^{2}+1)\\mathcal{B}_{0}}\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Note that ", "page_idx": 25}, {"type": "equation", "text": "$$\nB_{0}\\le\\frac{\\log\\kappa^{2}e+\\log(1/\\lambda)}{\\lambda n_{0}}+\\sqrt{\\frac{\\log\\kappa^{2}e+\\log(1/\\lambda)}{\\lambda n_{0}}}\\le\\frac{1}{4(\\kappa^{2}+1)\\log n}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and so $\\begin{array}{r}{(\\kappa^{2}+1)^{r}\\cdot4\\exp\\left(-\\frac{1}{4(\\kappa^{2}+1)\\mathcal{B}_{0}}\\right)\\le4(\\kappa^{2}+1)^{r}\\cdot\\frac{1}{n}}\\end{array}$ Therefore, ", "page_idx": 26}, {"type": "text", "text": "$\\begin{array}{r}{\\mathbb{E}\\|(T_{k,\\tilde{\\rho}_{\\mathtt{X}}}+\\lambda I)^{1/2}(I-P_{Z})\\|^{2r}\\leq2^{r}128^{r}(\\kappa^{2}+1)^{2r}\\cdot n^{-\\frac{r}{2r+s}}+4(\\kappa^{2}+1)^{r}\\cdot n^{-1}\\lesssim n^{-\\frac{r}{2r+s}}.}\\end{array}$ We can conclude that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left\\|-\\iota_{\\rho_{\\mathbf{x}}}(I-P_{Z})f_{0}^{*}+\\iota_{\\rho_{\\mathbf{x}}}\\left(I+\\cdot\\cdot+\\left(\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}P_{Z}\\overline{{T}}_{k,x_{i}}^{E}\\right)^{t-2}\\right)P_{Z}\\left(I-\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}\\overline{{T}}_{k,x_{i}}^{E}\\right)(I-P_{Z})f_{0}^{*}\\right\\|_{L_{\\rho_{\\mathbf{x}}}^{2}}}\\\\ &{\\leq(1+2E+E\\log t+\\sqrt{6\\eta\\hat{t}\\lambda}E)R B^{r}\\cdot4\\Gamma\\left(2.5\\right)(2+2(2\\kappa^{2}+2\\kappa\\sqrt{C_{s}^{\\prime}})^{2})^{1/2}(1+2\\kappa^{2}+2\\kappa\\sqrt{C_{s}^{\\prime}})^{1/2}}\\\\ &{\\qquad\\cdot\\left(2^{r}128^{r}(\\kappa^{2}+1)^{2r}\\cdot n^{-\\frac{r}{2r+s}}+4(\\kappa^{2}+1)^{r}\\cdot n^{-1}\\right)}\\\\ &{\\lesssim B^{r}(1+\\log t+t^{1/2}n^{-\\frac{1/2}{2r+s}})n^{-\\frac{r}{2r+s}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "A.2.5 Stopping Rule and Rademacher Complexity Bound ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "For convenience, we abuse the notation $D\\,=\\,\\{({\\bf x}^{1},y^{1}),\\cdot\\cdot\\cdot{\\bf\\nabla},({\\bf x}^{n},y^{n})\\}$ and $X\\,=\\,\\{\\mathbf{x}^{1},\\cdot\\cdot\\cdot\\,,\\mathbf{x}^{n}\\}$ Define the local empirical Rademacher complexity ", "page_idx": 26}, {"type": "equation", "text": "$$\nQ_{n}(\\epsilon)=\\mathbb{E}\\left[\\operatorname*{sup}_{\\|g\\|_{\\mathbb{H}_{k}}\\leq1,\\|g\\|_{L_{\\rho_{\\mathbf{x},n}}^{2}}\\leq\\epsilon}\\left|\\frac{1}{n}\\sum_{i=1}^{n}w_{i}g(\\mathbf{x}^{i})\\right|~\\Bigg|~X\\right]\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and the local population Rademacher complexity ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\overline{{Q}}_{n}(\\epsilon)=\\mathbb{E}\\left[\\operatorname*{sup}_{\\substack{\\|g\\|_{\\mathbb{H}_{k}}\\leq1,\\|g\\|_{L_{\\rho_{\\mathbf{x}}}^{2}}\\leq\\epsilon}}\\left|\\frac{1}{n}\\sum_{i=1}^{n}w_{i}g(\\mathbf{x}^{i})\\right|\\right]\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Where $w_{1},\\cdots\\,,w_{n}$ are independent Rademacher random variables and $\\begin{array}{r}{\\rho_{\\mathbf{x},n}=\\frac{1}{n}\\sum_{i=1}^{n}\\delta_{\\mathbf{x}^{i}}}\\end{array}$ We also define ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\overline{{\\mathcal{R}}}(\\epsilon)=\\sqrt{\\frac{1}{n}\\sum_{i=1}^{\\infty}\\lambda_{i}\\wedge\\epsilon^{2}}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\lambda_{1}\\geq\\lambda_{2}\\geq\\cdots\\geq0$ are eigenvalues of $T_{k,\\rho_{\\times}}$ . We recall the following well-known property. Lemma A.1 ([46], [60]). We have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\overline{{Q}}_{n}(\\epsilon)\\leq\\sqrt{2}\\cdot\\overline{{\\mathcal{R}}}(\\epsilon)\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We can prove the following lemma using a similar argument as in [46]. ", "page_idx": 26}, {"type": "text", "text": "Lemma A.2. There is an absolute constant $c>0$ which satisfies that for every $\\epsilon>0$ ", "page_idx": 26}, {"type": "equation", "text": "$$\nc\\cdot\\mathcal{R}(\\epsilon)\\leq Q_{n}(\\epsilon).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof of Lemma A.2. We divide the proof into three parts. ", "page_idx": 26}, {"type": "text", "text": "Part 1. Since $T_{k,X}=S_{D}^{\\top}S_{D}$ \uff0c $\\hat{\\lambda}_{1}\\geq\\hat{\\lambda}_{2}\\geq\\cdot\\cdot\\cdot\\geq\\hat{\\lambda}_{n}\\geq0$ are eigenvalues of $T_{k,X}$ . For convenience, set $\\hat{\\lambda}_{i}=0$ for $i>n$ and define $\\hat{n}\\leq n$ such that $\\hat{\\lambda}_{\\hat{n}}>0$ and $\\hat{\\lambda}_{\\hat{n}+1}=0$ . Choose an orthonormal basis $\\{\\hat{\\psi}_{i}\\}_{i=1}^{\\infty}$ of $\\mathbb{H}_{k}$ such that $\\hat{\\psi}_{i}$ is an eigenvector of $T_{k,X}$ corresponding to $\\hat{\\lambda}_{i}$ . Then $\\langle\\hat{\\psi}_{i},\\hat{\\psi}_{j}\\rangle_{L_{\\rho_{\\mathbf{x},n}}^{2}}=$ $\\langle S_{D}\\hat{\\psi}_{i},S_{D}\\hat{\\psi}_{j}\\rangle_{2}=\\langle T_{k,X}\\hat{\\psi}_{i},\\hat{\\psi}_{j}\\rangle_{\\mathbb{H}_{k}}=\\delta_{\\{i=j\\}}\\hat{\\lambda}_{i}$ . We will show that ", "page_idx": 26}, {"type": "equation", "text": "$$\nk_{\\mathbf{x}}=\\sum_{i=1}^{\\hat{n}}\\hat{\\psi}_{i}(\\mathbf{x})\\hat{\\psi}_{i}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\mathbf{x}\\,\\in\\,\\{\\mathbf{x}^{1},\\cdots,\\mathbf{x}^{n}\\}$ . Let $W_{1}$ be the subspace of $\\mathbb{H}_{k}$ spanned by $\\{\\hat{\\psi}_{i}\\,:\\,i\\,=\\,1,\\cdot\\cdot\\cdot\\,,\\hat{n}\\}$ and $W_{2}$ be the subspace of $\\mathbb{H}_{k}$ spanned by $\\{k_{\\mathbf{x}^{i}}\\,:\\,i\\,=\\,1,\\cdots\\,,n\\}$ . Observe that $W_{1}^{\\perp}\\;=\\;\\ker T_{k,X}$ and $W_{2}^{\\perp}\\,\\subset\\,\\ker T_{k,X}$ by the reproducing property. Thus, $W_{1}\\subset W_{2}$ . Conversely, choose a basis $\\{k_{\\tilde{\\mathbf{x}}^{i}}\\,:\\,i\\,=\\,1,\\cdots\\,,\\tilde{n}^{\\prime}\\}\\;\\subset\\;\\{k_{\\mathbf{x}^{i}}\\,:\\,i\\,=\\,1,\\cdots\\,,n\\}$ of $W_{2}$ . Then, using a similar argument as in Appendix A.2.1 implies that there exists a matrix ", "page_idx": 26}, {"type": "equation", "text": "$$\nB=\\left[\\stackrel{b_{11}}{\\vdots}\\,\\begin{array}{c c c}{\\cdots}&{b_{1\\tilde{n}^{\\prime}}}\\\\ {\\vdots}&{\\ddots}&{\\vdots}\\\\ {b_{n1}}&{\\cdots}&{b_{n\\tilde{n}^{\\prime}}}\\end{array}\\right]\\in\\mathbb{R}^{n\\times\\tilde{n}^{\\prime}}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "such that $\\begin{array}{r}{k_{\\mathbf{x}^{i}}=\\sum_{j=1}^{\\tilde{n}^{\\prime}}b_{i j}k_{\\tilde{\\mathbf{x}}^{j}}}\\end{array}$ Then $K_{X\\tilde{X}}=B K_{\\tilde{X}\\tilde{X}}$ where ", "page_idx": 27}, {"type": "equation", "text": "$$\nK_{X\\tilde{X}}=\\left[\\begin{array}{c c c}{k(\\mathbf{x}^{1},\\tilde{\\mathbf{x}}^{1})}&{\\cdots\\cdot}&{k(\\mathbf{x}^{1},\\tilde{\\mathbf{x}}^{\\tilde{n}^{\\prime}})}\\\\ {\\vdots}&{\\ddots}&{\\vdots}\\\\ {k(\\mathbf{x}^{n},\\tilde{\\mathbf{x}}^{1})}&{\\cdots}&{k(\\mathbf{x}^{n},\\tilde{\\mathbf{x}}^{\\tilde{n}^{\\prime}})}\\end{array}\\right]\\in\\mathbb{R}^{n\\times\\tilde{n}^{\\prime}}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and ", "page_idx": 27}, {"type": "equation", "text": "$$\nK_{\\tilde{X}\\tilde{X}}=\\left[\\begin{array}{c c c}{k(\\tilde{\\mathbf{x}}^{1},\\tilde{\\mathbf{x}}^{1})}&{\\cdot\\cdot\\cdot}&{k(\\tilde{\\mathbf{x}}^{1},\\tilde{\\mathbf{x}}^{\\tilde{n}^{\\prime}})}\\\\ {\\vdots}&{\\cdot\\cdot}&{\\vdots}\\\\ {k(\\tilde{\\mathbf{x}}^{\\tilde{n}^{\\prime}},\\tilde{\\mathbf{x}}^{1})}&{\\cdot\\cdot\\cdot}&{k(\\tilde{\\mathbf{x}}^{\\tilde{n}^{\\prime}},\\tilde{\\mathbf{x}}^{\\tilde{n}^{\\prime}})}\\end{array}\\right]\\in\\mathbb{R}^{\\tilde{n}^{\\prime}\\times\\tilde{n}^{\\prime}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Since $K_{\\tilde{X},\\tilde{X}}$ and $B^{\\top}B$ are invertible, ", "page_idx": 27}, {"type": "equation", "text": "$$\nT_{k,X}\\left(\\sum_{i=1}^{\\tilde{n}^{\\prime}}[n K_{\\tilde{X},\\tilde{X}}^{-1}(B^{\\top}B)^{-1}{\\mathbf b}]_{i}k_{\\tilde{\\mathbf{x}}^{i}}\\right)=\\sum_{i=1}^{\\tilde{n}^{\\prime}}{\\mathbf b}_{i}k_{\\tilde{\\mathbf{x}}^{i}}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "for any $\\mathbf{b}=[\\mathbf{b}_{1},\\cdots\\,,\\mathbf{b}_{\\tilde{n}^{\\prime}}]^{\\top}\\in\\mathbb{R}^{\\tilde{n}^{\\prime}}$ where $[\\cdot]_{r}$ is the $r$ th component of the given vector. Therefore, $W_{2}\\subset$ ran $T_{k,X}\\,=\\,(\\ker T_{k,X})^{\\perp}\\,=\\,W_{1}$ and so $W_{1}\\,=\\,W_{2}$ . From this fact, we can see that $k_{\\mathbf{x}^{i}}=$ $\\textstyle\\sum_{r=1}^{\\hat{n}}a_{r}{\\hat{\\psi}}_{r}$ for some $a_{1},\\cdot\\cdot\\cdot\\mathrm{~,~}a_{\\hat{n}}\\,\\in\\,\\mathbb{R}$ Then $a_{r}\\,=\\,\\langle k_{\\mathbf{x}^{i}},\\hat{\\psi}_{r}\\rangle_{\\mathbb{H}_{k}}\\,=\\,\\hat{\\psi}_{r}(\\mathbf{x}^{i})$ for ll $\\boldsymbol{r}\\,=\\,1,\\cdots\\,,\\hat{n}$ and so we are done. Note that $\\begin{array}{r}{k_{\\mathbf{x}}=\\sum_{i=1}^{\\infty}\\hat{\\psi}_{i}(\\mathbf{x})\\hat{\\psi}_{i}}\\end{array}$ where $\\mathbf{x}\\in\\{\\mathbf{x}^{1},\\cdots,\\mathbf{x}^{n}\\}$ since $\\hat{\\psi}_{i}(\\mathbf{x})=0$ for $\\mathbf{x}\\in\\{\\mathbf{x}^{1},\\cdots\\,,\\mathbf{x}^{n}\\}$ and $i>\\hat{n}$ ", "page_idx": 27}, {"type": "text", "text": "Part 2. Define ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathcal{F}:=\\left\\{h:\\|h\\|_{\\mathbb{H}_{k}}\\leq1\\,\\mathrm{~and~}\\,\\|h\\|_{L_{\\rho_{\\times,n}}^{2}}\\leq\\epsilon\\right\\}=\\left\\{\\sum_{i=1}^{\\infty}h_{i}\\hat{\\psi}_{i}:\\sum_{i=1}^{\\infty}h_{i}^{2}\\leq1\\,\\mathrm{~and~}\\,\\sum_{i=1}^{\\hat{n}}\\hat{\\lambda}_{i}h_{i}^{2}\\leq\\epsilon^{2}\\right\\}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathcal{E}:=\\left\\{\\sum_{i=1}^{\\infty}h_{i}\\hat{\\psi}_{i}:\\sum_{i=1}^{\\infty}\\frac{\\hat{\\lambda}_{i}}{\\hat{\\lambda}_{i}\\wedge\\epsilon^{2}}h_{i}^{2}\\leq1\\right\\}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $\\begin{array}{r}{\\frac{0}{0}=1}\\end{array}$ . Then $\\mathcal{E}\\subset\\mathcal{F}$ since ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left(\\sum_{i=1}^{\\infty}h_{i}^{2}\\right)\\vee\\left(\\sum_{i=1}^{\\infty}\\frac{\\hat{\\lambda}_{i}}{\\epsilon^{2}}h_{i}^{2}\\right)\\le\\sum_{i=1}^{\\infty}\\left(1\\vee\\frac{\\hat{\\lambda}_{i}}{\\epsilon^{2}}\\right)h_{i}^{2}=\\sum_{i=1}^{\\infty}\\frac{\\hat{\\lambda}_{i}}{\\hat{\\lambda}_{i}\\wedge\\epsilon^{2}}h_{i}^{2}\\le1\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "for $\\begin{array}{r}{h=\\sum_{i=1}^{\\infty}h_{i}\\hat{\\psi}_{i}\\in\\mathcal{E}}\\end{array}$ Thus, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\operatorname*{sup}_{h\\in\\mathcal{E}}\\left|\\sum_{i=1}^{n}\\epsilon_{i}h(\\mathbf{x}^{i})\\right|^{2}\\,\\bigg|\\;X\\right]\\leq\\mathbb{E}\\left[\\operatorname*{sup}_{h\\in\\mathcal{F}}\\left|\\sum_{i=1}^{n}\\epsilon_{i}h(\\mathbf{x}^{i})\\right|^{2}\\,\\bigg|\\;X\\right]\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $\\epsilon_{1},\\cdot\\cdot\\cdot\\,,\\epsilon_{n}$ are i.i.d. Rademacher variables. By the reproducing property, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=1}^{n}\\epsilon_{i}h(\\mathbf{x}^{i})=\\langle h,\\displaystyle\\sum_{i=1}^{n}\\epsilon_{i}k_{\\mathbf{x}^{i}}\\rangle_{\\mathbb{H}_{k}}=\\langle h,\\displaystyle\\sum_{i=1}^{n}\\epsilon_{i}\\sum_{j=1}^{\\hat{n}}\\hat{\\psi}_{j}(\\mathbf{x}^{i})\\hat{\\psi}_{j}\\rangle_{\\mathbb{H}_{k}}}\\\\ &{\\displaystyle\\qquad\\qquad=\\sum_{j=1}^{\\hat{n}}h_{j}\\sum_{i=1}^{n}\\epsilon_{i}\\hat{\\psi}_{j}(\\mathbf{x}^{i})=\\langle\\displaystyle\\sum_{j=1}^{\\infty}\\sqrt{\\frac{\\hat{\\lambda}_{j}}{\\hat{\\lambda}_{j}\\wedge\\epsilon^{2}}}h_{j}\\hat{\\psi}_{j},\\sum_{j=1}^{\\hat{n}}\\sqrt{\\frac{\\hat{\\lambda}_{j}\\wedge\\epsilon^{2}}{\\hat{\\lambda}_{j}}}\\sum_{i=1}^{n}\\epsilon_{i}\\hat{\\psi}_{j}(\\mathbf{x}^{i})\\hat{\\psi}_{j}\\rangle_{\\mathbb{H}_{k}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $\\begin{array}{r}{h=\\sum_{i=1}^{\\infty}h_{i}\\hat{\\psi}_{i}}\\end{array}$ . Thus, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{h\\in\\mathcal{E}}\\left|\\sum_{i=1}^{n}\\epsilon_{i}h(\\mathbf{x}^{i})\\right|^{2}=\\left\\lVert\\sum_{j=1}^{\\hat{n}}\\sqrt{\\frac{\\hat{\\lambda}_{j}\\wedge\\epsilon^{2}}{\\hat{\\lambda}_{j}}}\\sum_{i=1}^{n}\\epsilon_{i}\\hat{\\psi}_{j}(\\mathbf{x}^{i})\\hat{\\psi}_{j}\\right\\rVert_{\\mathbb{H}_{k}}^{2}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Since ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left\\|\\sum_{j=1}^{\\hat{n}}\\sqrt{\\frac{{\\hat{\\lambda}}_{j}\\wedge\\epsilon^{2}}{{\\hat{\\lambda}}_{j}}}\\sum_{i=1}^{n}\\epsilon_{i}{\\hat{\\psi}}_{j}(\\mathbf{x}^{i}){\\hat{\\psi}}_{j}\\right\\|_{\\mathbb{H}_{k}}^{2}=\\sum_{j=1}^{\\hat{n}}\\frac{{\\hat{\\lambda}}_{j}\\wedge\\epsilon^{2}}{{\\hat{\\lambda}}_{j}}\\left(\\sum_{i=1}^{n}\\epsilon_{i}{\\hat{\\psi}}_{j}(\\mathbf{x}^{i})\\right)^{2},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathbb{E}\\left[\\underset{h\\in\\mathcal{E}}{\\operatorname*{sup}}\\left|\\sum_{i=1}^{n}\\epsilon_{i}h(\\mathbf{x}^{i})\\right|^{2}\\;\\middle|\\;X\\right]}&{\\!\\!\\!=\\mathbb{E}\\left[\\sum_{j=1}^{\\hat{n}}\\frac{\\hat{\\lambda}_{j}\\wedge\\epsilon^{2}}{\\hat{\\lambda}_{j}}\\left(\\sum_{i=1}^{n}\\epsilon_{i}\\hat{\\psi}_{j}(\\mathbf{x}^{i})\\right)^{2}\\;\\middle|\\;X\\right]}&\\\\ &{}&{\\!\\!\\!=\\sum_{j=1}^{\\hat{n}}\\frac{\\hat{\\lambda}_{j}\\wedge\\epsilon^{2}}{\\hat{\\lambda}_{j}}\\left(\\displaystyle\\sum_{i=1}^{n}\\hat{\\psi}_{j}(\\mathbf{x}^{i})^{2}\\right)=n\\sum_{j=1}^{n}\\hat{\\lambda}_{j}\\wedge\\epsilon^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Therefore, ", "page_idx": 28}, {"type": "equation", "text": "$$\n{\\sqrt{n\\sum_{j=1}^{n}{\\hat{\\lambda}}_{j}\\wedge\\epsilon^{2}}}\\leq\\mathbb{E}\\left[\\operatorname*{sup}_{h\\in{\\mathcal{F}}}\\left|\\sum_{i=1}^{n}\\epsilon_{i}h(\\mathbf{x}^{i})\\right|^{2}\\;{\\bigg|}\\;X\\right]^{1/2}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Part 3. By Khintchine's inequality, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\operatorname*{sup}_{h\\in\\mathcal{F}}\\left|\\sum_{i=1}^{n}\\epsilon_{i}h(\\mathbf{x}^{i})\\right|\\,\\bigg|\\ X\\right]\\geq\\operatorname*{sup}_{h\\in\\mathcal{F}}\\mathbb{E}\\left[\\left|\\sum_{i=1}^{n}\\epsilon_{i}h(\\mathbf{x}^{i})\\right|\\,\\bigg|\\ X\\right]\\geq\\frac{1}{\\sqrt{2}}\\operatorname*{sup}_{h\\in\\mathcal{F}}\\left(\\sum_{i=1}^{n}h(\\mathbf{x}^{i})^{2}\\right)^{1/2}=\\sqrt{\\frac{n}{2}}\\epsilon.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Set $Z=g(\\epsilon_{1},\\cdot\\cdot\\cdot\\,,\\epsilon_{n})$ where ", "page_idx": 28}, {"type": "equation", "text": "$$\ng(t_{1},\\cdot\\cdot\\cdot\\cdot,t_{n})=\\operatorname*{sup}_{h\\in\\mathcal{F}}\\left|\\sum_{i=1}^{n}t_{i}h(\\mathbf{x}^{i})\\right|.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By Remark A.14, $g$ is convex and $\\begin{array}{r}{\\operatorname*{sup}_{h\\in\\mathcal{F}}(\\sum_{i=1}^{n}h(\\mathbf{x}^{i})^{2})^{1/2}\\,=\\,\\sqrt{n}\\epsilon}\\end{array}$ Lipschitz on $[-1,1]^{n}$ .By Lemma A.13, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(Z-\\mathbb{E}\\left[Z|X\\right]\\geq t\\mathbb{E}\\left[Z|X\\right]|X\\right)\\leq\\exp\\left(-\\frac{t^{2}}{16\\epsilon^{2}n}\\mathbb{E}\\left[Z|X\\right]^{2}\\right)\\leq\\exp\\left(-\\frac{t^{2}}{32}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "From ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[Z^{2}|X\\right]=\\mathbb{E}\\left[Z^{2}\\mathbf{1}_{\\left\\{Z<\\mathbb{E}[Z|X]\\right\\}}|X\\right]+\\displaystyle\\sum_{m=0}^{\\infty}\\mathbb{E}\\left[Z^{2}\\mathbf{1}_{\\left\\{(m+1)\\mathbb{E}[Z|X]\\leq Z<(m+2)\\mathbb{E}[Z|X]\\right\\}}|X\\right]}\\\\ &{\\qquad\\qquad\\leq\\mathbb{E}[Z|X]^{2}+\\displaystyle\\sum_{m=0}^{\\infty}(m+2)^{2}\\mathbb{E}[Z|X]^{2}\\mathbb{P}(Z\\geq(m+1)\\mathbb{E}[Z|X]|X)}\\\\ &{\\qquad\\qquad\\leq\\mathbb{E}[Z|X]^{2}\\left(1+\\displaystyle\\sum_{m=0}^{\\infty}(m+2)^{2}\\exp\\left(-\\frac{m^{2}}{32}\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}[Z|X]\\ge c\\cdot\\mathbb{E}\\left[Z^{2}|X\\right]^{1/2}\\ge c\\cdot\\sqrt{n\\sum_{j=1}^{n}\\hat{\\lambda}_{j}\\wedge\\epsilon^{2}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $c=\\left(1+\\textstyle\\sum_{m=0}^{\\infty}(m+2)^{2}\\exp\\left(-{\\frac{m^{2}}{32}}\\right)\\right)^{-1/2}$ is an absolute constant. Therefore, ", "page_idx": 28}, {"type": "equation", "text": "$$\nc\\cdot{\\sqrt{\\left.{\\frac{1}{n}}\\sum_{j=1}^{n}{\\hat{\\lambda}}_{j}\\wedge\\epsilon^{2}}}\\leq\\mathbb{E}\\left[\\operatorname*{sup}_{h\\in{\\mathcal{F}}}\\left|{\\frac{1}{n}}\\sum_{i=1}^{n}\\epsilon_{i}h(\\mathbf{x}^{i})\\right|~\\right|X\\right].\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We set the population radius as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\epsilon_{n}=\\operatorname*{inf}\\left\\{\\epsilon\\geq0:\\overline{{Q}}_{n}(\\epsilon)\\leq\\frac{\\epsilon^{1+2r}}{16\\kappa}\\right\\}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We also define ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\tilde{\\epsilon}_{n}=\\operatorname*{inf}\\left\\{\\epsilon\\geq0:\\overline{{\\mathcal{R}}}(\\epsilon)\\leq\\frac{\\epsilon^{1+2r}}{16\\sqrt{2}\\kappa}\\right\\}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By Lemma A.1, we have $\\epsilon_{n}\\leq\\tilde{\\epsilon}_{n}$ . We can easily see that $\\mathcal{R},\\overline{{\\mathcal{R}}},Q_{n}$ ,and ${\\overline{{Q}}}_{n}$ are increasing functions.   \nThe following lemma can be shown by a similar argument as in [2]. ", "page_idx": 28}, {"type": "text", "text": "Lemma A.3. If $g:[0,\\infty)\\rightarrow[0,\\infty)$ is a function such that $g$ is non-decreasing and $r\\mapsto g(r)/r$ is non-increasing, then $g$ is continuous on $(0,\\infty)$ ", "page_idx": 29}, {"type": "text", "text": "Since ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{\\mathcal{R}(\\epsilon)}{\\epsilon}=\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}1\\wedge\\frac{\\hat{\\lambda}_{i}}{\\epsilon^{2}}}\\quad\\mathrm{and}\\quad\\frac{\\overline{{\\mathcal{R}}}(\\epsilon)}{\\epsilon}=\\sqrt{\\frac{1}{n}\\sum_{i=1}^{\\infty}1\\wedge\\frac{\\lambda_{i}}{\\epsilon^{2}}},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "$\\epsilon\\mapsto\\mathcal{R}(\\epsilon)/\\epsilon$ and $\\epsilon\\mapsto\\overline{{\\mathcal{R}}}(\\epsilon)/\\epsilon$ are non-increasing and so $\\mathcal{R}$ and $\\overline{{\\mathcal{R}}}$ are continuous. ", "page_idx": 29}, {"type": "text", "text": "Lemma A.4. $\\epsilon\\mapsto Q_{n}(\\epsilon)/\\epsilon$ and $\\epsilon\\mapsto\\overline{{Q}}_{n}(\\epsilon)/\\epsilon$ are non-increasing. In particular, $\\overline{{Q}}_{n}$ is continuous and $\\epsilon_{n}<\\infty$ ", "page_idx": 29}, {"type": "text", "text": "Proof. From the fact that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\frac{Q_{n}\\left(\\epsilon\\right)}{\\epsilon}=\\frac{1}{\\epsilon}\\mathbb{E}\\left[\\underset{\\left\\lVert g\\right\\rVert_{\\mathbb{H}_{k}}\\leq1,\\left\\lVert g\\right\\rVert_{L_{\\rho_{\\mathbf{x},n}}^{2}}\\leq\\epsilon}{\\operatorname*{sup}}\\Bigg\\lvert\\frac{1}{n}\\sum_{i=1}^{n}w_{i}g(\\mathbf{x}_{i})\\Bigg\\rvert\\;\\Bigg|\\;X\\right]}\\\\ &{}&{\\qquad=\\mathbb{E}\\left[\\underset{\\left\\lVert g\\right\\rVert_{\\mathbb{H}_{k}}\\leq1/\\epsilon,\\left\\lVert g\\right\\rVert_{L_{\\rho_{\\mathbf{x},n}}^{2}}\\leq1}{\\operatorname*{sup}}\\Bigg\\lvert\\frac{1}{n}\\sum_{i=1}^{n}w_{i}g(\\mathbf{x}_{i})\\Bigg\\rvert\\;\\Bigg|\\;X\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "we can easily see that $\\epsilon\\mapsto Q_{n}(\\epsilon)/\\epsilon$ is non-increasing. Similarly, we can show that $\\epsilon\\mapsto\\overline{{Q}}_{n}(\\epsilon)/\\epsilon$ is non-increasing. Note that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\epsilon\\to0^{+}}\\frac{Q_{n}(\\epsilon)}{\\epsilon}>0\\quad\\mathrm{and}\\quad\\operatorname*{lim}_{\\epsilon\\to0^{+}}\\frac{\\overline{{Q}}_{n}(\\epsilon)}{\\epsilon}>0.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Also, we can observe that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\epsilon\\to\\infty}\\frac{Q_{n}(\\epsilon)}{\\epsilon}=0\\quad\\mathrm{and}\\quad\\operatorname*{lim}_{\\epsilon\\to\\infty}\\frac{\\overline{{{Q}}}_{n}(\\epsilon)}{\\epsilon}=0.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Since $\\epsilon\\mapsto\\epsilon^{2r}/16\\kappa$ is increasing, goes 0 as $\\epsilon\\to0^{+}$ , and goes $\\infty$ as $\\epsilon\\rightarrow\\infty$ , we can conclude that $\\epsilon_{n}<\\infty$ \u53e3 ", "page_idx": 29}, {"type": "text", "text": "Similarly, we have $\\tilde{\\epsilon}_{n}\\mathrm{~<~}\\infty$ . In fact, we can find the lower and the upper bound of $\\tilde{\\epsilon}_{n}$ under Assumption 3.2. ", "page_idx": 29}, {"type": "text", "text": "Lemma A.5. We have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\left(2^{9/(4r+2s)}\\kappa^{1/(2r+s)}c_{s}^{s/(4r+2s)}\\wedge c_{s}^{1/(2\\ r}\\left(\\cfrac{s}{s+2}\\right)^{1/2s}\\right)n^{-\\frac{1}{4r+2s}}\\leq\\tilde{\\epsilon}_{n}}}\\\\ &{}&{\\leq2^{9/(4r+2s)}\\kappa^{1/(2r+s)}\\left(\\displaystyle\\frac{2-s}{1-s}\\right)^{1/(4r+2s)}C_{s}^{s/(4r+2s)}n^{-\\frac{1}{4r+2s}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof. Since $c_{s}i^{-1/s}\\leq\\lambda_{i}\\leq C_{s}i^{-1/s}$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\sqrt{\\frac{1}{n}\\sum_{j=1}^{\\infty}(c_{s}j^{-1/s})\\wedge\\epsilon^{2}}\\le\\overline{{\\mathcal{R}}}(\\epsilon)\\le\\sqrt{\\frac{1}{n}\\sum_{j=1}^{\\infty}(C_{s}j^{-1/s})\\wedge\\epsilon^{2}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We first consider the lower bound of $\\tilde{\\epsilon}_{n}$ . We first observe that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\sqrt{\\frac{1}{n}\\sum_{j=1}^{\\infty}(c_{s}j^{-1/s})\\wedge\\epsilon^{2}}=\\sqrt{\\frac{1}{n}\\left(\\left\\lfloor\\left(\\frac{c_{s}}{\\epsilon^{2}}\\right)^{s}\\right\\rfloor\\epsilon^{2}+\\sum_{j=\\left\\lfloor\\left(\\frac{c_{s}}{\\epsilon^{2}}\\right)^{s}\\right\\rfloor+1}^{\\infty}c_{s}j^{-1/s}\\right)}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Set ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\epsilon=\\left(2^{9/(4r+2s)}\\kappa^{1/(2r+s)}c_{s}^{s/(4r+2s)}\\wedge c_{s}^{1/2}\\left(\\frac{s}{s+2}\\right)^{1/2s}\\right)n^{-\\frac{1}{4r+2s}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Note that ", "page_idx": 29}, {"type": "equation", "text": "$$\nc s\\,\\left\\lfloor\\left(\\frac{c_{s}}{\\epsilon^{2}}\\right)^{s}\\right\\rfloor^{-1/s}\\ge\\epsilon^{2}\\quad\\mathrm{and}\\quad\\frac{s}{1-s}\\left(\\left\\lfloor\\left(\\frac{c_{s}}{\\epsilon^{2}}\\right)^{s}\\right\\rfloor+1\\right)^{(s-1)/s}\\ge\\left\\lfloor\\left(\\frac{c_{s}}{\\epsilon^{2}}\\right)^{s}\\right\\rfloor^{-1/s}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "hold. The first formula is trivial. To show the second formula, we observe that the function $\\begin{array}{r}{u(t)=(\\frac{s}{1-s})^{s}\\cdot\\frac{t}{(1+t)^{1-s}}}\\end{array}$ is an inereasingfunctionThus,for $t\\geq2/s$ ", "page_idx": 30}, {"type": "equation", "text": "$$\nu(t)\\geq u\\left({\\frac{2}{s}}\\right)={\\frac{2}{(1-s)^{s}(s+2)^{1-s}}}\\geq{\\frac{2}{2-2s^{2}}}\\geq1.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Here, we apply an elementary inequality: $a^{s}b^{1-s}\\leq s a+(1-s)b\\quad\\forall a,b>0.$ Since ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\epsilon\\leq c_{s}^{1/2}\\left(\\frac{s}{s+2}\\right)^{1/2s}\\quad\\Rightarrow\\quad\\left\\lfloor\\left(\\frac{c_{s}}{\\epsilon^{2}}\\right)^{s}\\right\\rfloor\\geq\\left(\\frac{c_{s}}{\\epsilon^{2}}\\right)^{s}-1\\geq\\frac{2}{s},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "putting $\\begin{array}{r}{t=\\left\\lfloor\\left(\\frac{c_{s}}{\\epsilon^{2}}\\right)^{s}\\right\\rfloor}\\end{array}$ gives ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\left({\\frac{s}{1-s}}\\right)^{s}\\cdot{\\frac{\\left\\lfloor\\left({\\frac{c_{s}}{\\epsilon^{2}}}\\right)^{s}\\right\\rfloor}{\\left(1+\\left\\lfloor\\left({\\frac{c_{s}}{\\epsilon^{2}}}\\right)^{s}\\right\\rfloor\\right)1-s}}\\geq1\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and so the second formula holds. Therefore, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\sum_{j=\\left\\lfloor\\left(\\frac{c s}{c^{2}}\\right)^{s}\\right\\rfloor+1}^{\\infty}c_{s}j^{-1/s}\\ge c_{s}\\int_{\\left\\lfloor\\left(\\frac{c s}{c^{2}}\\right)^{s}\\right\\rfloor+1}^{\\infty}\\frac{1}{t^{1/s}}\\,d t=\\frac{s c_{s}}{1-s}\\left(\\left\\lfloor\\left(\\frac{c_{s}}{\\epsilon^{2}}\\right)^{s}\\right\\rfloor+1\\right)^{(s-1)/s}\\ge c_{s}\\left\\lfloor\\left(\\frac{c_{s}}{\\epsilon^{2}}\\right)^{s}\\right\\rfloor^{-1/s}\\ge\\epsilon^{2}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "holds and so we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\sqrt{\\frac{1}{n}\\left(\\left\\lfloor\\left(\\frac{c_{s}}{\\epsilon^{2}}\\right)^{s}\\right\\rfloor\\epsilon^{2}+\\sum_{j=\\left\\lfloor\\left(\\frac{c_{s}}{\\epsilon^{2}}\\right)^{s}\\right\\rfloor+1}^{\\infty}c_{s}j^{-1/s}\\right)}\\ge\\sqrt{\\frac{1}{n}\\left(\\left\\lfloor\\left(\\frac{c_{s}}{\\epsilon^{2}}\\right)^{s}\\right\\rfloor+1\\right)\\epsilon^{2}}\\ge\\frac{c_{s}^{s/2}}{n^{1/2}}\\epsilon^{1-s}\\ge\\frac{\\epsilon^{1+2r}}{16\\sqrt{2}\\kappa}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Where thelast iquality fllws fm29/(4r2s)(2+4+2 . We can conclude that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\tilde{\\epsilon}_{n}\\geq\\left(2^{9/(4r+2s)}\\kappa^{1/(2r+s)}c_{s}^{s/(4r+2s)}\\wedge c_{s}^{1/2}\\left(\\frac{s}{s+2}\\right)^{1/2s}\\right)n^{-\\frac{1}{4r+2s}}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "by Lemma A.4. We now derive the upper bound of $\\tilde{\\epsilon}_{n}$ . Note that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\sqrt{\\frac{1}{n}\\sum_{j=1}^{\\infty}(C_{s}j^{-1/s})\\wedge\\epsilon^{2}}=\\sqrt{\\frac{1}{n}\\left(\\left\\lfloor\\left(\\frac{C_{s}}{\\epsilon^{2}}\\right)^{s}\\right\\rfloor\\epsilon^{2}+\\sum_{j=\\left\\lfloor\\left(\\frac{C_{s}}{\\epsilon^{2}}\\right)^{s}\\right\\rfloor+1}^{\\infty}C_{s}j^{-1/s}\\right)}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Since ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\sum_{j=\\left\\lfloor\\left(\\frac{C_{s}}{\\epsilon^{2}}\\right)^{s}\\right\\rfloor+1}^{\\infty}C_{s}j^{-1/s}-\\int_{\\left\\lfloor\\left(\\frac{C_{s}}{\\epsilon^{2}}\\right)^{s}\\right\\rfloor+1}^{\\infty}\\frac{C_{s}}{t^{1/s}}\\;d t\\leq C_{s}\\left(\\left\\lfloor\\left(\\frac{C_{s}}{\\epsilon^{2}}\\right)^{s}\\right\\rfloor+1\\right)^{-1/s}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\int_{\\left\\lfloor\\left(\\frac{C_{s}}{\\epsilon^{2}}\\right)^{s}\\right\\rfloor+1}^{\\infty}\\frac{C_{s}}{t^{1/s}}\\;d t=\\frac{s C_{s}}{1-s}\\left(\\left\\lfloor\\left(\\frac{C_{s}}{\\epsilon^{2}}\\right)^{s}\\right\\rfloor+1\\right)^{1-1/s}\\ge\\frac{s C_{s}}{1-s}\\left(\\left\\lfloor\\left(\\frac{C_{s}}{\\epsilon^{2}}\\right)^{s}\\right\\rfloor+1\\right)^{-1/s},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\sum_{j=\\left\\lfloor\\left(\\frac{C s}{\\epsilon^{2}}\\right)^{s}\\right\\rfloor+1}^{\\infty}C_{s}j^{-1/s}\\leq\\frac{1}{s}\\int_{\\left(\\frac{C s}{\\epsilon^{2}}\\right)^{s}}^{\\infty}\\frac{C_{s}}{t^{1/s}}\\ d t=\\frac{C_{s}^{s}}{1-s}\\epsilon^{2-2s}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Hence, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\sqrt{\\frac{1}{n}\\left(\\left\\lfloor\\left(\\frac{C_{s}}{\\epsilon^{2}}\\right)^{s}\\right\\rfloor\\epsilon^{2}+\\sum_{j=\\left\\lfloor\\left(\\frac{C_{s}}{\\epsilon^{2}}\\right)^{s}\\right\\rfloor+1}^{\\infty}C_{s}j^{-1/s}\\right)}\\le\\sqrt{\\frac{1}{n}\\left(C_{s}^{s}+\\frac{C_{s}^{s}}{1-s}\\right)\\epsilon^{2-2s}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Set ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\epsilon=2^{9/(4r+2s)}\\kappa^{1/(2r+s)}\\left(\\frac{2-s}{1-s}\\right)^{1/(4r+2s)}C_{s}^{s/(4r+2s)}n^{-\\frac{1}{4r+2s}}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "which is equivalent to $\\begin{array}{r}{\\sqrt{\\frac{1}{n}\\left(C_{s}^{s}+\\frac{C_{s}^{s}}{1-s}\\right)\\epsilon^{2-2s}}=\\frac{\\epsilon^{1+2r}}{16\\sqrt{2}\\kappa}}\\end{array}$ Therefore, weatan thubdf $\\tilde{\\epsilon}_{n}$ ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\tilde{\\epsilon}_{n}\\,\\le\\,2^{9/(4r+2s)}\\kappa^{1/(2r+s)}\\left({\\frac{2-s}{1-s}}\\right)^{1/(4r+2s)}C_{s}^{s/(4r+2s)}n^{-{\\frac{1}{4r+2s}}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Without loss of generality, we assume $n$ is sufficiently large such that ", "page_idx": 31}, {"type": "equation", "text": "$$\nn\\geq2^{9}\\kappa^{2}\\left(\\frac{2-s}{1-s}\\right)C_{s}^{s}\\;\\Leftrightarrow\\;2^{9/(4r+2s)}\\kappa^{1/(2r+s)}\\left(\\frac{2-s}{1-s}\\right)^{1/(4r+2s)}C_{s}^{s/(4r+2s)}n^{-\\frac{1}{4r+2s}}\\leq1.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Then $\\epsilon_{n}\\leq\\tilde{\\epsilon}_{n}\\leq1$ . We now prove the following lemma. It is an extended version of Theorem 14.1 in [60]. ", "page_idx": 31}, {"type": "text", "text": "Lemma A.6. We have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb P\\left(\\operatorname*{sup}_{\\|h\\|_{\\mathbb H_{k}}\\leq1}\\frac{\\left|\\|h\\|_{L_{\\rho_{\\mathbf x},n}^{2}}^{2}-\\|h\\|_{L_{\\rho_{\\mathbf x}}^{2}}^{2}\\right|}{\\|h\\|_{L_{\\rho_{\\mathbf x}}^{2}}^{2}+t^{2}}\\leq\\frac12\\right)\\geq1-\\exp\\left(-c_{1}n t^{4r}\\right)\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "for any $t\\in[\\epsilon_{n},1]$ where $c_{1}$ is a constant independent of $t$ and $n$ ", "page_idx": 31}, {"type": "text", "text": "Proof of Lemma A.6. We use a similar argument as in the proof of Theorem 14.1 in [60]. Define ", "page_idx": 31}, {"type": "equation", "text": "$$\nZ_{n}(t):=\\displaystyle\\operatorname*{sup}_{\\|h\\|_{\\mathbb{H}_{k}}\\leq1,\\|h\\|_{L_{\\rho_{\\mathbf{x}}}^{2}}\\leq t}\\left|\\|h\\|_{L_{\\rho_{\\mathbf{x},n}}^{2}}^{2}-\\|h\\|_{L_{\\rho_{\\mathbf{x}}}^{2}}^{2}\\right|\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $t\\in(0,1]$ . Let ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathcal{E}:=\\left\\{\\operatorname*{sup}_{\\|h\\|_{\\mathbb{H}_{k}}\\leq1}\\frac{\\left\\|h\\right\\|_{L_{\\rho_{\\times,n}}^{2}}^{2}-\\|h\\|_{L_{\\rho_{\\times}}^{2}}^{2}}{\\|h\\|_{L_{\\rho_{\\times}}^{2}}^{2}+t^{2}}\\right\\}\\leq\\frac{1}{2}\\right\\}^{c},\\,\\mathcal{A}:=\\left\\{Z_{n}(t)\\geq\\frac{t^{2}}{2}\\right\\},\\,\\tilde{A}:=\\left\\{Z_{n}(t)\\geq\\frac{t^{1+2r}}{2}\\right\\}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We first show that $\\mathcal{E}\\subset\\mathcal{A}$ . On the event $\\mathcal{E}$ , there exists $h\\in\\mathbb{H}_{k}$ such that $\\|h\\|_{\\mathbb{H}_{k}}\\leq1$ and ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\frac{\\left|\\|h\\|_{L_{\\rho_{\\mathbf{x},n}}^{2}}^{2}-\\|h\\|_{L_{\\rho_{\\mathbf{x}}}^{2}}^{2}\\right|}{\\|h\\|_{L_{\\rho_{\\mathbf{x}}}^{2}}^{2}+t^{2}}>\\frac{1}{2}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "$\\|h\\|_{L_{\\rho_{\\mathbf{x}}}^{2}}\\leq t$ $\\begin{array}{r}{\\|h\\|_{L_{\\rho_{\\mathbf{x},n}}^{2}}^{2}-\\|h\\|_{L_{\\rho_{\\mathbf{x}}}^{2}}^{2}\\Big|>\\frac{1}{2}\\|h\\|_{L_{\\rho_{\\mathbf{x}}}^{2}}^{2}\\!+\\!\\frac{1}{2}{t}^{2}\\geq\\frac{1}{2}{t}^{2}}\\end{array}$ $\\begin{array}{r}{\\tilde{h}=\\frac{t}{\\|h\\|_{L_{\\rho_{\\mathbf{x}}}^{2}}}h}\\end{array}$ $\\|\\tilde{h}\\|_{L_{\\rho_{\\times}}^{2}}=t$ $\\begin{array}{r}{\\|\\tilde{h}\\|_{L_{\\rho_{\\mathbf{x},n}}^{2}}^{2}-\\|\\tilde{h}\\|_{L_{\\rho_{\\mathbf{x}}}^{2}}^{2}\\Big|>\\frac{t^{2}}{\\|h\\|_{L_{\\rho_{\\mathbf{x}}}^{2}}^{2}}\\cdot\\Big(\\frac12\\|h\\|_{L_{\\rho_{\\mathbf{x}}}^{2}}^{2}+\\frac12t^{2}\\Big)\\geq\\frac{1}{2}t^{2}}\\end{array}$ $\\mathcal{E}\\subset\\mathcal{A}$ Since+ for $t\\in(0,1]$ wehave $\\mathcal{E}\\subset\\mathcal{A}\\subset\\tilde{\\mathcal{A}}$ . To find an upper bound of $\\mathbb{E}Z_{n}(t)$ , we use the symmetrization argument as follows: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}Z_{n}(t)=\\mathbb{E}\\left[\\left|\\underset{|\\mathbf{1}|h|_{k}\\leq1,|\\mathbf{1}|h|_{L_{p_{\\kappa}^{2}}}\\leq t}{\\operatorname*{sup}}\\right|\\frac{1}{n}\\sum_{i=1}^{n}h(\\mathbf{x}^{i})^{2}-\\mathbb{E}h(\\mathbf{x})^{2}\\right|\\right]}\\\\ &{\\phantom{\\sum}=\\mathbb{E}\\left[\\left|\\underset{|\\mathbf{1}|h|_{k}\\leq1,|\\mathbf{1}|h|_{L_{p_{\\kappa}^{2}}}\\leq t}{\\operatorname*{sup}}\\right|\\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n}h(\\mathbf{x}^{i})^{2}-\\frac{1}{n}\\sum_{i=1}^{n}h(\\bar{\\mathbf{x}}^{i})^{2}\\left|\\mathbf{X}\\right|\\right]\\right]}\\\\ &{\\leq\\mathbb{E}\\left[\\left|\\underset{|\\mathbf{1}|h|_{k}\\leq1,|\\mathbf{1}|h|_{L_{p_{\\kappa}^{2}}}\\leq t}{\\operatorname*{sup}}\\right|\\frac{1}{n}\\sum_{i=1}^{n}w_{i}\\left(h(\\mathbf{x}^{i})^{2}-h(\\bar{\\mathbf{x}}^{i})^{2}\\right)\\right|\\right]}\\\\ &{\\leq2\\mathbb{E}\\left[\\left|\\underset{|\\mathbf{1}|h|_{k}\\leq1,|\\mathbf{1}|h|_{L_{p_{\\kappa}^{2}}}\\leq t}{\\operatorname*{sup}}\\right|\\left|\\frac{1}{n}\\sum_{i=1}^{n}w_{i}h(\\mathbf{x}^{i})^{2}\\right|\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $w_{1},\\cdots\\,,w_{n}$ are i.i.d. Rademacher variables. By Lemma A.15, ", "page_idx": 31}, {"type": "equation", "text": "$$\n2\\mathbb{E}\\left[\\operatorname*{sup}_{\\lVert h\\rVert_{{\\mathbb{H}_{k}}}\\leq1,\\lVert h\\rVert_{L_{\\rho_{\\mathbf{x}}}^{2}}\\leq t}\\left\\lvert\\frac{1}{n}\\sum_{i=1}^{n}w_{i}h(\\mathbf{x}^{i})^{2}\\right\\rvert\\right]\\leq4\\kappa\\cdot\\overline{{Q}}_{n}(t).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "For $t\\ge\\epsilon_{n}$ , we have $\\begin{array}{r}{4\\kappa\\cdot\\overline{{Q}}_{n}(t)\\leq\\frac{t^{1+2r}}{4}}\\end{array}$ since ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\overline{{Q}}_{n}(\\epsilon_{n})=\\frac{\\epsilon_{n}^{1+2r}}{16\\kappa}\\quad\\mathrm{and}\\quad\\overline{{Q}}_{n}(\\epsilon)\\leq\\frac{\\epsilon^{1+2r}}{16\\kappa},\\;\\forall\\epsilon\\geq\\epsilon_{n}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Since ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{i=1}^{n}\\operatorname*{sup}_{\\|h\\|_{\\mathbb{H}_{k}}\\leq1,\\|h\\|_{L_{\\rho_{\\times}}^{2}}\\leq t}\\mathbb{E}\\left[(h(\\mathbf{x}^{i})^{2}-\\mathbb{E}h(\\mathbf{x})^{2})^{2}\\right]\\leq\\operatorname*{sup}_{\\|h\\|_{\\mathbb{H}_{k}}\\leq1,\\|h\\|_{L_{\\rho_{\\times}}^{2}}\\leq t}\\mathbb{E}\\left[h(\\mathbf{x})^{4}\\right]\\leq\\kappa^{2}t^{2},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Lemma A.16 gives ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(Z_{n}(t)\\geq\\mathbb{E}Z_{n}(t)+\\frac{u^{1+2r}}{4}\\right)\\leq\\exp\\left(-\\frac{\\frac{1}{16}n^{2}u^{2+4r}}{4\\kappa^{2}\\cdot n\\mathbb{E}Z_{n}(t)+2n\\kappa^{2}t^{2}+\\frac{2}{3}\\kappa^{2}\\cdot\\frac{1}{4}n u^{1+2r}}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\exp\\left(-c_{1}n\\left(\\frac{u^{2+4r}}{t^{1+2r}}\\wedge\\frac{u^{2+4r}}{t^{2}}\\wedge u^{1+2r}\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where C1 = $\\begin{array}{r}{c_{1}=\\frac{1}{96\\kappa^{2}}}\\end{array}$ 962 . Putting u = t gives ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(\\tilde{A})\\leq\\mathbb{P}\\left(Z_{n}(t)\\geq\\mathbb{E}Z_{n}(t)+\\frac{t^{1+2r}}{4}\\right)\\leq\\exp\\left(-c_{1}n\\left(t^{1+2r}\\wedge t^{4r}\\right)\\right)\\leq\\exp(-c_{1}n t^{4r}),}\\\\ &{z^{c})\\geq\\mathbb{P}(\\tilde{A}^{c})\\geq1-\\exp(-c_{1}n t^{4r}).}\\end{array}\n$$i.e., ", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Let us return to our problem. Consider the event ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\xi^{c}=\\left\\{\\underset{\\Vert h\\Vert_{\\mathbb{H}_{k}}\\leq1}{\\operatorname*{sup}}\\frac{\\left\\vert\\Vert h\\Vert_{L_{\\rho_{\\mathbf{x},n}}^{2}}^{2}-\\Vert h\\Vert_{L_{\\rho_{\\mathbf{x}}}^{2}}^{2}\\right\\vert}{\\Vert h\\Vert_{L_{\\rho_{\\mathbf{x}}}^{2}}^{2}+\\tilde{\\epsilon}_{n}^{2}}\\leq\\frac{1}{2}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "By Lemma A.6, we have $\\mathbb{P}(\\mathcal{E}^{c})\\geq1-\\exp(-c_{1}n\\tilde{\\epsilon}_{n}^{4r})$ where $c_{1}$ is a constant that does not depend on $n$ . Define ", "page_idx": 32}, {"type": "equation", "text": "$$\nT:=\\operatorname*{min}\\left\\{t\\in\\mathbb{N}:\\frac{1}{\\sqrt{\\eta E t}}\\leq\\tilde{\\epsilon}_{n}\\right\\}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "By the definition, we can easily obtain the upper bound of $T$ as $\\begin{array}{r}{T\\,<\\,1+\\frac{1}{\\eta E\\tilde{\\epsilon}_{n}^{2}}}\\end{array}$ nEe : Since R(-) is non-decreasing, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathcal{R}\\left(\\frac{1}{\\sqrt{\\eta E T}}\\right)\\leq\\mathcal{R}(\\tilde{\\epsilon}_{n})\\leq\\frac{1}{c_{l}}Q_{n}(\\tilde{\\epsilon}_{n})\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "for some absolute constant $c_{l}$ where the second inequality follows from Lemma A.2. Note that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\|h\\|_{L_{\\rho_{\\mathbf{x},n}}^{2}}^{2}-\\|h\\|_{L_{\\rho_{\\mathbf{x}}}^{2}}^{2}\\geq-\\frac{1}{2}\\left(\\|h\\|_{L_{\\rho_{\\mathbf{x}}}^{2}}^{2}+\\tilde{\\epsilon}_{n}^{2}\\right)\\ \\Rightarrow\\ \\|h\\|_{L_{\\rho_{\\mathbf{x},n}}^{2}}^{2}\\geq\\frac{1}{2}\\|h\\|_{L_{\\rho_{\\mathbf{x}}}^{2}}^{2}-\\frac{1}{2}\\tilde{\\epsilon}_{n}^{2}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "for all $h$ such that $\\|h\\|_{\\mathbb{H}_{k}}\\leq1$ on the event $\\mathcal{E}^{c}$ . Thus, ", "page_idx": 32}, {"type": "equation", "text": "$$\n2_{n}(\\tilde{\\epsilon}_{n})=\\mathbb{E}\\left[\\operatorname*{sup}_{\\|\\boldsymbol{g}\\|_{\\mathbb{R}_{k}}\\leq1,\\|\\boldsymbol{g}\\|_{L_{\\rho_{\\infty},n}^{2}}\\leq\\tilde{\\epsilon}_{n}}\\left|\\frac{1}{n}\\sum_{i=1}^{n}w_{i}\\boldsymbol{g}(\\mathbf{x}^{i})\\right|\\biggm|X\\right]\\leq\\mathbb{E}\\left[\\operatorname*{sup}_{\\|\\boldsymbol{g}\\|_{\\mathbb{R}_{k}}\\leq1,\\|\\boldsymbol{g}\\|_{L_{\\rho_{\\infty}}^{2}}\\leq2\\tilde{\\epsilon}_{n}}\\left|\\frac{1}{n}\\sum_{i=1}^{n}w_{i}\\boldsymbol{g}(\\mathbf{x}^{i})\\right|\\biggm|X\\right]\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "on the event $\\mathcal{E}^{c}$ .Set $\\mathcal{F}=\\left\\{g:\\mathcal{X}\\to\\mathbb{R}:\\|g\\|_{\\mathbb{H}_{k}}\\leq1,\\|g\\|_{L_{\\rho_{\\mathbf{x}}}^{2}}\\leq2\\tilde{\\epsilon}_{n}\\right\\}$ Then the ranges of functions in $\\mathcal{F}$ are contained in $[-\\kappa,\\kappa]$ and ${\\mathcal{F}}=-{\\mathcal{F}}$ . By Lemma A.17 we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\operatorname*{sup}_{\\substack{\\|g\\|_{\\mathbb{R}_{k}}\\leq1,\\|g\\|_{L_{\\rho\\times}^{2}}\\leq2\\bar{\\epsilon}_{n}}}\\left|\\frac{1}{n}\\sum_{i=1}^{n}w_{i}g(\\mathbf{x}^{i})\\right|\\biggm|X\\right]\\leq2\\mathbb{E}\\left[\\operatorname*{sup}_{\\substack{\\|g\\|_{\\mathbb{R}_{k}}\\leq1,\\|g\\|_{L_{\\rho\\times}^{2}}\\leq2\\bar{\\epsilon}_{n}}}\\left|\\frac{1}{n}\\sum_{i=1}^{n}w_{i}g(\\mathbf{x}^{i})\\right|\\right]+c_{1}\\kappa\\hat{\\epsilon}_{n}^{1+2r}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "with probability at least $1-\\exp(-c_{1}n\\tilde{\\epsilon}_{n}^{1+2r})\\geq1-\\exp(-c_{1}n\\tilde{\\epsilon}_{n}^{4r})$ . Hence, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathcal{R}\\left(\\frac{1}{\\sqrt{\\eta E T}}\\right)\\leq\\frac{2}{c_{l}}\\cdot\\overline{{Q}}_{n}(2\\tilde{\\epsilon}_{n})+\\frac{c_{1}\\kappa}{c_{l}}\\tilde{\\epsilon}_{n}^{1+2r}\\leq\\left(\\frac{1}{\\kappa c_{l}}+\\frac{c_{1}\\kappa}{c_{l}}\\right)\\tilde{\\epsilon}_{n}^{1+2r}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "holds with probability at least $1-2\\exp(-c_{1}n\\tilde{\\epsilon}_{n}^{4r})$ . Here, the second inequality follows from (13). Therefore, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\mathcal{R}\\left(\\frac{1}{\\sqrt{\\eta E T}}\\right)^{2}\\leq\\left(\\frac{8}{\\kappa c_{l}}+\\frac{\\kappa c_{1}}{c_{l}}\\right)^{2}\\tilde{\\epsilon}_{n}^{2+4r}\\cdot(0\\vee(1-2\\exp(-c_{1}n\\tilde{\\epsilon}_{n}^{4r})))+\\kappa^{2}\\cdot2\\exp(-c_{1}n\\tilde{\\epsilon}_{n}^{4r})}\\\\ &{\\qquad\\qquad\\qquad\\leq\\left(\\frac{8}{\\kappa c_{l}}+\\frac{\\kappa c_{1}}{c_{l}}\\right)^{2}\\tilde{\\epsilon}_{n}^{2+4r}+2\\kappa^{2}\\exp(-c_{1}n\\tilde{\\epsilon}_{n}^{4r})}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "since $\\begin{array}{r}{\\mathcal{R}\\left(\\frac{1}{\\sqrt{\\eta E T}}\\right)\\leq\\kappa}\\end{array}$ From the fact that $\\begin{array}{r}{\\frac{\\exp(-c_{1}n\\tilde{\\epsilon}_{n}^{4r})}{\\tilde{\\epsilon}_{n}^{2+4r}}\\lesssim n^{\\frac{2r+1}{2r+s}}\\exp(-c_{1}^{\\prime}n^{\\frac{s}{2r+s}})\\lesssim1}\\end{array}$ we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\left(\\mathbb{E}\\mathcal{R}\\left(\\frac{1}{\\sqrt{\\eta E T}}\\right)^{2}\\right)^{1/2}\\lesssim\\tilde{\\epsilon}_{n}^{1+2r}\\lesssim n^{-\\frac{2r+1}{4r+2s}}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "A.2.6 Conclusion ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Note that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\lesssim\\tilde{\\epsilon}_{n}^{2}\\lesssim n^{-\\frac{1}{2r+s}}\\quad\\mathrm{and}\\quad T\\leq1+\\frac{1}{\\eta E\\tilde{\\epsilon}_{n}^{2}}\\lesssim n^{\\frac{1}{2r+s}}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Therefore, we bound the expected risk as ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\|\\iota_{\\rho_{\\mathbf{x}}}(f_{T}-f_{0}^{*})\\|_{L_{\\rho_{\\mathbf{x}}}^{2}}}\\\\ &{\\lesssim B^{r-1/2}\\left(\\frac{1}{T}+n^{-\\frac{1}{2r+s}}\\right)^{1/2}n^{-\\frac{r-1/2}{2r+s}}+\\left(T^{1/2}+n^{-\\frac{1/2}{2r+s}}T\\right)\\cdot\\left(\\mathbb{E}\\mathcal{R}\\left(\\frac{1}{\\sqrt{\\eta T E}}\\right)^{2}\\right)^{1/2}}\\\\ &{\\quad+B^{r}(1+\\log T+T^{1/2}n^{-\\frac{1/2}{2r+s}})n^{-\\frac{r}{2r+s}}}\\\\ &{\\lesssim B^{r}n^{-\\frac{r}{2r+s}}\\log n.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "A.3Corollary of Theorem 3.4 ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "As mentioned in Section 3.3, one can remove $B^{r}$ in the upper bound in Theorem 3.4 by using more public inputs. The precise statement is as follows: ", "page_idx": 33}, {"type": "text", "text": "Corollary A.7. Under Assumption 3.1, 3.2, and 3.3, with $n_{0}\\geq B^{1+\\epsilon}n^{\\frac{1}{2r+s}}(\\log(B n))^{3}$ public inputs independently generated from $\\tilde{\\rho}_{\\mathbf{x}}$ satisfying (2) DCL-KR gives the performance guarantee ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{E}\\|\\iota_{\\rho_{\\mathbf{x}}}(f_{j,T}-f_{0}^{*})\\|_{L_{\\rho_{\\mathbf{x}}}^{2}}\\leq C\\cdot n^{-\\frac{r}{2r+s}}\\log n\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "for all $j=1,\\cdot\\cdot\\cdot,m$ where $\\epsilon>0$ is a fixed constant, $\\eta\\in(0,1/\\kappa^{2})$ is a fixed learning rate, $T$ is an adequate stopping rule, and the prefactor $C$ does not depend on $B,\\,m_{;}$ and $n$ ", "page_idx": 33}, {"type": "text", "text": "Proof. In the proof of Theorem 3.4, there are two terms in the upper bound affected by $B$ .One is ", "page_idx": 33}, {"type": "equation", "text": "$$\nB^{r-1/2}\\left(\\frac{(\\log n_{0})^{3}}{n_{0}}\\right)^{r-1/2}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "in the norm bound of the first term in (7). The other is ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{E}\\|(T_{k,\\tilde{\\rho}_{\\kappa}}+\\lambda I)^{1/2}(I-P_{D_{p}})\\|^{2r}\\leq2^{r}\\lambda^{r}+(\\kappa^{2}+1)\\cdot4\\exp\\left(-\\frac{1}{4(\\kappa^{2}+1)\\mathscr{B}_{0}}\\right)\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "in the norm bound of the third and fourth terms in (7). For the frst part, $n_{0}\\geq B^{1+\\epsilon}n^{\\frac{1}{2r+s}}(\\log(B n))^{3}$ implies ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{3^{r-1/2}\\left(\\frac{(\\log n_{0})^{3}}{n_{0}}\\right)^{r-1/2}}\\\\ &{\\leq B^{-\\epsilon(r-1/2)}n^{-\\frac{r-1/2}{2r+s}}(\\log(B n))^{-3r+3/2}\\left((1+\\epsilon)\\log B+\\frac{1}{2r+s}\\log n+3\\log\\log(B n)\\right)^{3r-3/2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "For the latter part, set $\\lambda=128(\\kappa^{2}+1)^{2}n^{-\\frac{1}{2r+s}}/B$ . Then ", "page_idx": 33}, {"type": "equation", "text": "$$\nB_{0}\\leq\\frac{\\log\\kappa^{2}e+\\log(1/\\lambda)}{\\lambda n_{0}}+\\sqrt{\\frac{\\log\\kappa^{2}e+\\log(1/\\lambda)}{\\lambda n_{0}}}\\leq\\frac{1}{4(\\kappa^{2}+1)\\log(B n)}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and hence ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{\\Sigma}\\Vert\\big(T_{k,\\tilde{\\rho}_{\\mathbf{x}}}+\\lambda I\\big)^{1/2}(I-P_{D_{p}})\\Vert^{2r}\\leq2^{r}\\lambda^{r}+(\\kappa^{2}+1)\\cdot4\\exp\\left(-\\frac{1}{4(\\kappa^{2}+1)B_{0}}\\right)\\lesssim B^{-r}n^{-\\frac{r}{2r+s}}+\\frac{1}{B n}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Since it eliminates $B^{r}$ in the upper bound, we are done. ", "page_idx": 33}, {"type": "text", "text": "A.4 Useful Lemmas ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Recall Cordes? inequality [16]. ", "page_idx": 34}, {"type": "text", "text": "Lemma A.8 (Cordes? Inequality). Let $A,B$ be two bounded positive linear operators on a seperable Hilbert space. Then for any $s\\in[0,1]$ ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\|A^{s}B^{s}\\|\\leq\\|A B\\|^{s}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "holds. ", "page_idx": 34}, {"type": "text", "text": "We also recall a property of projection operators. ", "page_idx": 34}, {"type": "text", "text": "Lemma A.9 ([53]). Let $Z$ be a bounded linear operator and $P$ be a projection operator such that ran $P=\\overline{{r a n\\;Z^{\\top}}}$ . Then for any bounded operator $X$ and $\\lambda>0$ wehave ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\|(I-P)X\\|\\le\\lambda^{1/2}\\|(Z^{\\top}Z+\\lambda I)^{-1/2}X\\|.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "There are some useful lemmas for PAC bounds ", "page_idx": 34}, {"type": "text", "text": "Lemma A.10 ([17]). Let $X\\,=\\,\\{\\mathbf{x}^{1},\\cdot\\cdot\\cdot\\,,\\mathbf{x}^{n}\\}$ be a dataset where data points are independently generatedfrom $\\nu$ .Then ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\|(T_{k,\\nu}+\\lambda I)(T_{k,X}+\\lambda I)^{-1}\\|\\leq2+2\\left(\\left(\\frac{2\\kappa^{2}}{n\\lambda}+\\sqrt{\\frac{4\\kappa^{2}\\mathcal{N}_{\\nu}(\\lambda)}{n\\lambda}}\\right)\\log(2/\\delta)\\right)^{2}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "holds with confidence at least $1-\\delta$ where $\\delta\\in(0,1)$ ", "page_idx": 34}, {"type": "text", "text": "Lemma A.11 ([38]). Let $X\\,=\\,\\{\\mathbf{x}^{1},\\cdot\\cdot\\cdot\\,,\\mathbf{x}^{n}\\}$ be a dataset where data points are independently generatedfrom $\\nu$ .Then ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\|(T_{k,\\nu}+\\lambda I)^{-1}(T_{k,X}+\\lambda I)\\|\\leq1+\\left(\\frac{2\\kappa^{2}}{n\\lambda}+\\sqrt{\\frac{4\\kappa^{2}\\mathcal{N}_{\\nu}(\\lambda)}{n\\lambda}}\\right)\\log(2/\\delta)\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "holds with confidence at least $1-\\delta$ where $\\delta\\in(0,1)$ ", "page_idx": 34}, {"type": "text", "text": "Lemma A.12 ([48]). Let $X\\,=\\,\\{\\mathbf{x}^{1},\\cdot\\cdot\\cdot\\,,\\mathbf{x}^{n}\\}$ be a dataset where data points are independently generated from $\\nu$ .For $\\lambda\\in(0,1]$ suchthat $\\mathcal{N}_{\\nu}(\\lambda)\\geq1$ ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\|(T_{k,\\nu}+\\lambda I)(T_{k,X}+\\lambda I)^{-1}\\|\\leq2\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "holds with confidence at least $1-\\delta$ where ", "page_idx": 34}, {"type": "equation", "text": "$$\n4\\exp\\left(-\\frac{1}{4(\\kappa^{2}+1)}\\cdot\\left(\\frac{1+\\log\\mathcal{N}_{\\nu}(\\lambda)}{\\lambda n}+\\sqrt{\\frac{1+\\log\\mathcal{N}_{\\nu}(\\lambda)}{\\lambda n}}\\right)^{-1}\\right)\\leq\\delta<1.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "To prove Lemma A.2 in Appendix A.2.5, we introduce a concentration inequality for Lipschitz functions. ", "page_idx": 34}, {"type": "text", "text": "Lemma A.13 ([60]). Let $X_{1},\\cdot\\cdot\\cdot,X_{n}$ be independent random variables whose supports are containedin $[a,b]$ and $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ beconvex and $L$ -Lipschitz.with respect to the Euclidean norm. Then wehave ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{P}(f(X)\\geq\\mathbb{E}f(X)+t)\\leq\\exp\\left(-{\\frac{t^{2}}{4L^{2}(b-a)^{2}}}\\right)\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $X=[X_{1},\\cdot\\cdot\\cdot\\,,X_{n}]$ and $t>0$ ", "page_idx": 34}, {"type": "text", "text": "Precisely, we use the following fact in Appendix A.2.5 ", "page_idx": 34}, {"type": "text", "text": "Remark A.14 ([60]). Let $A\\subset\\mathbb{R}^{n}$ be a bounded set and ", "page_idx": 34}, {"type": "equation", "text": "$$\nf(\\mathbf{x})=\\operatorname*{sup}_{\\mathbf{a}\\in A}\\sum_{k=1}^{n}a_{k}\\mathbf{x}_{k}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $\\mathbf{x}=[\\mathbf{x}_{1},\\cdots,\\mathbf{x}_{n}]\\in[-1,1]^{n}$ and $\\mathbf{a}=[a_{1},\\cdot\\cdot\\cdot\\,,a_{n}]$ . Since ", "page_idx": 34}, {"type": "equation", "text": "$$\nf(\\mathbf{x})-f(\\mathbf{x}^{\\prime})=\\operatorname*{sup}_{\\mathbf{a}\\in A}\\sum_{k=1}^{n}a_{k}\\mathbf{x}_{k}-\\operatorname*{sup}_{\\mathbf{a}\\in A}\\sum_{k=1}^{n}a_{k}\\mathbf{x}_{k}^{\\prime}\\leq\\operatorname*{sup}_{\\mathbf{a}\\in A}\\,\\langle\\mathbf{a},\\mathbf{x}-\\mathbf{x}^{\\prime}\\rangle_{\\mathbb{R}^{n}}\\leq\\operatorname*{sup}_{\\mathbf{a}\\in A}\\|\\mathbf{a}\\|_{\\mathbb{R}^{n}}\\,\\|\\mathbf{x}-\\mathbf{x}^{\\prime}\\|_{\\mathbb{R}^{n}},\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "$f$ is a $\\operatorname{sup}_{\\mathbf{a}\\in A}\\|\\mathbf{a}\\|_{\\mathbb{R}^{n}}$ -Lipshitz function where $\\|\\cdot\\|_{\\mathbb{R}^{n}}$ is the Euclidean norm on $\\mathbb{R}^{n}$ . We can observe that $f$ is convex since $f$ is a supremum of convex functions defined on a convex compact set. ", "page_idx": 34}, {"type": "text", "text": "To prove Lemma A.6 in Appendix A.2.5, we recall the Ledoux-Talagrand contraction inequality [29, 56] and Talagrand's inequality [4, 56]. ", "page_idx": 35}, {"type": "text", "text": "Lemma A.15 (Ledoux-Talagrand Contraction Inequality). If $\\phi:\\mathbb{R}\\rightarrow\\mathbb{R}$ isa $L$ -Lipshitz.function, then ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\operatorname*{sup}_{h\\in\\mathcal{F}}\\frac{1}{n}\\sum_{i=1}^{n}\\epsilon_{i}\\phi(h(\\mathbf{x}_{i}))\\right]\\leq L\\cdot\\mathbb{E}\\left[\\operatorname*{sup}_{h\\in\\mathcal{F}}\\frac{1}{n}\\sum_{i=1}^{n}\\epsilon_{i}h(\\mathbf{x}_{i})\\right].\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Lemma A.16 (Talagrand's Inequality). Let $X_{1},\\cdot\\cdot\\cdot\\,,X_{n}$ be independent $\\mathcal{X}$ -valuedrandomvariables. Let $\\mathcal{F}$ be a countably family of measurable real-valued functions on $\\mathcal{X}$ such that $\\|f\\|_{\\infty}\\le U<\\infty$ and $\\mathbb{E}f(X_{i})=0$ for all $f\\in\\mathcal F$ .Let ", "page_idx": 35}, {"type": "equation", "text": "$$\nZ:=\\operatorname*{sup}_{f\\in{\\mathcal F}}\\sum_{i=1}^{n}f(X_{i}),\\quad\\sigma^{2}\\ge\\frac{1}{n}\\sum_{i=1}^{n}\\operatorname*{sup}_{f\\in{\\mathcal F}}\\mathbb{E}[f(X_{i})^{2}],\\quad\\nu_{n}:=2U\\mathbb{E}Z+n\\sigma^{2}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Then ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{P}(Z\\ge\\mathbb{E}Z+t)\\le\\exp\\left(-\\frac{t^{2}}{2\\nu_{n}+\\frac{2}{3}U t}\\right)\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "for all $t\\geq0$ ", "page_idx": 35}, {"type": "text", "text": "Lastly, we recall the following well-known property used in Appendix A.2.5. ", "page_idx": 35}, {"type": "text", "text": "Lemma A.17 ([2]). Let $\\mathcal{F}$ be a class of functions with ranges in $[a,b]$ and $w_{1},\\cdots\\,,w_{n}$ be i.i.d. Rademacher variables. Then ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\mathbb{E}\\left[\\operatorname*{sup}_{h\\in\\mathcal{F}}\\sum_{i=1}^{n}w_{i}f(\\mathbf{x}^{i})\\right]\\leq\\operatorname*{inf}_{\\alpha\\in(0,1)}\\left(\\frac{1}{1-\\alpha}\\frac{1}{n}\\mathbb{E}\\left[\\operatorname*{sup}_{h\\in\\mathcal{F}}\\sum_{i=1}^{n}w_{i}f(\\mathbf{x}^{i})~\\Bigg|~X\\right]+\\frac{(b-a)\\log(1/\\delta)}{4n\\alpha(1-\\alpha)}\\right)\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "holds with probability at least $1-\\delta$ Also, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{n}\\mathbb{E}\\left[\\underset{h\\in\\mathcal{F}}{\\operatorname*{sup}}\\sum_{i=1}^{n}w_{i}f(\\mathbf{x}^{i})~\\Bigg|~X\\right]}\\\\ &{\\leq\\underset{\\alpha>0}{\\operatorname*{inf}}\\left((1+\\alpha)\\frac{1}{n}\\mathbb{E}\\left[\\underset{h\\in\\mathcal{F}}{\\operatorname*{sup}}\\sum_{i=1}^{n}w_{i}f(\\mathbf{x}^{i})\\right]+\\frac{(b-a)\\log(1/\\delta)}{2n}\\left(\\frac{1}{2\\alpha}+\\frac{1}{3}\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "holds with probability at least $1-\\delta$ ", "page_idx": 35}, {"type": "text", "text": "B Details on DCL-NN Algorithm ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "As we mentioned before, DCL-NN considers the same problem as in Section 3 but local models are heterogeneous neural networks. That is,there are $m$ parties and $\\underline{{D}}_{i}=\\{(\\mathbf{x}_{i}^{j},y_{i}^{j}):j=1,\\cdots\\,,n_{i}\\}$ is the private dataset of the ith party $(i=1,\\cdots\\,,m)$ where $\\textstyle D=\\bigcup_{i=1}^{m}D_{i}$ are i.id. whose distribution is $\\rho_{\\mathbf{x},y}$ . One remark is that the local data distributions of parties are not the same in general. To communicate training information, we introduce an unlabeled public input dataset $Z=\\{\\mathbf{z}^{1},\\cdot\\cdot\\cdot\\mathbf{\\epsilon},\\mathbf{z}^{n_{0}}\\}\\subset\\mathcal{X}$ The goal of parties is to find a minimizer of the population risk $\\mathcal{E}$ defined in Section 3. ", "page_idx": 35}, {"type": "text", "text": "To extend DCL-KR to heterogeneous neural network settings, it is necessary to ensure that the assumptions of DCL-KR are satisfied as much as possible. Specifically, one important assumption in DCL-KR is the equality of kernels across local models. Indeed, public data predictions can vary in conflicting directions after the local training procedure, even when using the same local datasets, if the kernels differ. ", "page_idx": 35}, {"type": "text", "text": "For further explanation of this claim, we consider the simple case of $E=1$ in DCL-KR where $E$ is the number of local iterations. After the consensus prediction $u$ is distributed to local parties, the server then receives the updated local prediction on $Z$ ", "page_idx": 35}, {"type": "equation", "text": "$$\n(I-\\frac{\\eta}{n_{i}}K_{Z X_{i}}K_{X_{i}\\tilde{Z}}K_{\\tilde{Z}\\tilde{Z}}^{-1})u+\\frac{\\eta}{n_{i}}K_{Z X_{i}}{\\bf y}_{i}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "from the $i$ th local party. (The notation is consistent with Appendix A) Suppose two parties have exactly the same dataset. If the same kernel is used in these two parties, the updated local predictions ", "page_idx": 35}, {"type": "text", "text": "1: Hyperparameters: $D_{i}$ : local dataset of party $i$ $(i=1,\\cdot\\cdot\\cdot,m)$ \uff0c $Z=\\{\\mathbf{z}^{1},\\cdots\\,,\\mathbf{z}^{n_{0}}\\}$ : public inputs, $E$ : the number of local iterations at each communication round, $T$ : total communication rounds, $T_{k}$ : epochs of kernel distillation   \n2: Pretrain: Pretrain local AI models $f_{i}(\\cdot)=\\mathbf{w}_{i}^{\\top}g_{i}(\\cdot)+b_{i}$ $(i=1,\\cdots,m)$   \n3:# Feature Kernel Distillation Procedure   \n4: For party $i$ ${\\mathrm{~i~}}(i=1,\\cdot\\cdot\\cdot\\,,m)$ , compute the feature kernel values on public inputs $\\{k_{f_{i}}({\\mathbf{z}}^{j_{1}},{\\mathbf{z}}^{j_{2}}):$ $1\\leq j_{1},j_{2}\\leq n_{0}\\}$ via (3) and upload them to the server.   \n5: The server aggregates the local feature kernel values to the target kernel values $\\{k({\\mathbf z}^{j_{1}},{\\mathbf z}^{j_{2}})\\,:$ $1\\leq j_{1},j_{2}\\leq n_{0}\\}$ with (4) and distributes them to all parties.   \n6: for party $i=1,\\cdot\\cdot\\cdot,m$ do   \n7:for $t_{k}=0,\\cdot\\cdot\\cdot,T_{k}-1$ do   \n8: for mini-batch $Z_{0}\\subset Z$ do   \n9: Update parameters of its model $f_{i}$ by maximizing $\\widehat{\\mathrm{CKA}}(k,k_{f_{i}})$ on $Z_{0}$ defined as (16) via gradient descent where $k$ is fixed.   \n10: end for   \n11:end for   \n12: end for   \n13: # Collaborative Learning Procedure   \n14: Initialize the consensus prediction $\\mathbf{y}_{p,0}=0$   \n15: for $t=0,\\cdot\\cdot\\cdot,T-1$ do   \n16: for party $i=1,\\cdot\\cdot\\cdot,m$ do   \n17: Update $\\mathbf{w}_{i}$ and $b_{i}$ by maximizing MSE (Mean Squared Error) on the public inputs $Z$ with consensus prediction ${\\bf y}_{p,t}$ via gradient descent with sufficiently many iterations.   \n18: for $e=1,\\cdot\\cdot\\cdot\\,,{\\bar{E}}\\,\\cdot$ do   \n19: Update $\\mathbf{w}_{i}$ and $b_{i}$ by maximizing MSE on $D_{i}$ via gradient descent.   \n20: end for   \n21: Upload the local prediction $\\mathbf{y}_{p,t+1}^{i}$ m $Z$ to the server.   \n22: end for   \n234 ${\\bf y}_{p,t+1}=$ $\\scriptstyle\\sum_{i=1}^{m}{\\frac{n_{i}}{n}}\\mathbf{y}_{p,t+1}^{i}$ ${\\bf y}_{p,t+1}$ ", "page_idx": 36}, {"type": "text", "text": "will be identical. However, if the kernels are different, this will not be the case. For kernels like the Gaussian kernel, which have high correlation between close inputs, the updated local predictions will be strongly influenced by data points close to each input. On the other hand, for kernels like the linear kernel, which have high correlation between distant inputs, the updated local prediction on $Z$ Will be influenced more by data points farther from each input. We can observe this fact from the above formula (14). This observation implies that aggregating local learning information becomes very challenging when the kernels differ. In short, using the same kernel ensures that the shift mechanisms ofpredictionson $Z$ at the edges are identical, making it possible for the aggregation through simple weighted averaging to work well. This is a key element of the strong theoretical results of DCL-KR and explains why kernel matching between neural networks is necessary in DCL-NN. ", "page_idx": 36}, {"type": "text", "text": "Let $f_{i}$ be a local model of the ith party such that $f_{i}(\\cdot)=\\mathbf{w}_{i}^{\\top}g_{i}(\\cdot)+b_{i}$ $g_{i}:\\mathcal{X}\\to\\mathbb{R}^{c_{i}}$ \uff0c $\\mathbf{w}_{i}\\in\\mathbb{R}^{c_{i}}$ \uff0c $c_{i}\\in\\mathbb{N}$ , and $b_{i}\\in\\mathbb{R}$ for $i\\,=\\,1,\\cdot\\cdot\\,,m$ . Since most modern neural network architectures have a linear layer as the last layer, this setting is general enough. As (3), we set the feature kernel of $f_{i}$ $(i=1,\\cdots\\,,m)$ to be ", "page_idx": 36}, {"type": "equation", "text": "$$\nk_{f_{i}}(\\mathbf{x}^{1},\\mathbf{x}^{2})=g_{i}(\\mathbf{x}^{1})^{\\top}g_{i}(\\mathbf{x}^{2}),\\quad\\mathbf{x}^{1},\\mathbf{x}^{2}\\in\\mathcal{X}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "To bring the setting to the DCL-KR scheme, DCL-NN matches $k_{f_{1}},\\dots,k_{f_{m}}$ via kernel distillation procedure. Obviously, the target kernel in this procedure is a key factor in enhancing performance. Theoretically, using the ensemble kernel ", "page_idx": 36}, {"type": "equation", "text": "$$\nk=\\sum_{i=1}^{m}\\frac{n_{i}}{n}k_{f_{i}}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "can be a good way to construct a good kernel derived from local feature kernels. The reason is that this ensemble kernel is identical to the kernel induced by the (scaled) concatenation of the local feature maps, i.e., ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{k(\\mathbf{x}^{1},\\mathbf{x}^{2})=\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}k_{f_{i}}(\\mathbf{x}^{1},\\mathbf{x}^{2})=\\displaystyle\\sum_{i=1}^{m}\\frac{n_{i}}{n}g_{i}(\\mathbf{x}^{1})^{\\top}g_{i}(\\mathbf{x}^{2})}&{}&\\\\ {=\\big[\\sqrt{\\frac{n_{1}}{n}}g_{1}(\\mathbf{x}^{1})^{\\top}}&{\\sqrt{\\frac{n_{2}}{n}}g_{2}(\\mathbf{x}^{1})^{\\top}}&{\\cdots}&{\\sqrt{\\frac{n_{m}}{n}}g_{m}(\\mathbf{x}^{1})^{\\top}\\big]\\left[\\begin{array}{c}{\\sqrt{\\frac{n_{1}}{n}}g_{1}(\\mathbf{x}^{2})}\\\\ {\\sqrt{\\frac{n_{2}}{n}}g_{2}(\\mathbf{x}^{2})}\\\\ {\\cdots}\\\\ {\\sqrt{\\frac{n_{m}}{n}}g_{m}(\\mathbf{x}^{2})}\\end{array}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "In other words, the ensemble kernel has greater expressive power than individual feature kernels, and with a sufficient amount of data, it leads to better performance. We empirically verify that the performance of this ensemble kernel surpasses that of individual feature kernels in Figure 4. ", "page_idx": 37}, {"type": "text", "text": "DCL-NN sets this ensemble kernel $k$ as the target kernel and local parties match their local feature kernels $k_{f_{1}},\\cdot\\cdot\\cdot\\mathrm{\\Delta},k_{f_{m}}$ With the kernel $k$ using the public dataset $Z$ . For this purpose, we introduce Centered Kernel Alignment (CKA) [8] as a kernel similarity measure. The CKA between two kernels $k_{1}$ and $k_{2}$ On the public input distribution $\\tilde{\\rho}_{\\mathbf{x}}$ is given by ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathrm{CKA}(k_{1},k_{2})=\\frac{\\mathrm{HSIC}(k_{1},k_{2})}{\\sqrt{\\mathrm{HSIC}(k_{1},k_{1})\\mathrm{HSIC}(k_{2},k_{2})}}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $\\mathrm{HSIC}(\\cdot,\\cdot)$ is a Hilbert-Schmidt Independence Criterion (HSIC) defined as ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathrm{HSIC}(k_{i},k_{j})=\\mathbb{E}_{\\mathbf{x}^{1},\\mathbf{x}^{2}\\sim\\tilde{\\rho}_{\\mathbf{x}}}[k_{i}^{c}(\\mathbf{x}^{1},\\mathbf{x}^{2})k_{j}^{c}(\\mathbf{x}^{1},\\mathbf{x}^{2})]\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "and the centered kernel $k_{i}^{c}$ is given by ", "page_idx": 37}, {"type": "equation", "text": "$$\nk_{i}^{c}(\\mathbf{x}^{1},\\mathbf{x}^{2})=k_{i}(\\mathbf{x}^{1},\\mathbf{x}^{2})-\\mathbb{E}_{\\tilde{\\mathbf{x}}^{2}\\sim\\tilde{\\rho}_{\\mathbf{x}}}[k_{i}(\\mathbf{x}^{1},\\tilde{\\mathbf{x}}^{2})]-\\mathbb{E}_{\\tilde{\\mathbf{x}}^{1}\\sim\\tilde{\\rho}_{\\mathbf{x}}}[k_{i}(\\tilde{\\mathbf{x}}^{1},\\mathbf{x}^{2})]+\\mathbb{E}_{\\tilde{\\mathbf{x}}^{1},\\tilde{\\mathbf{x}}^{2}\\sim\\tilde{\\rho}_{\\mathbf{x}}}[k_{i}(\\tilde{\\mathbf{x}}^{1},\\tilde{\\mathbf{x}}^{2})],\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "$\\mathbf{x}^{1},\\mathbf{x}^{2}\\in\\mathcal{X}\\:(i=1,2)$ . However, since we have a finite number of samples, we employ the empirical CKA. The empirical CKA between two kernels $k_{1}$ and $k_{2}$ on inputs $\\{\\mathbf{\\dot{c}}^{1},\\cdot\\cdot\\cdot,\\mathbf{c}^{p}\\}$ is given by ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\widehat{\\mathrm{CKA}}(k_{1},k_{2})=\\frac{\\widehat{\\mathrm{HSIC}}(K_{1},K_{2})}{\\sqrt{\\widehat{\\mathrm{HSIC}}(K_{1},K_{1})\\widehat{\\mathrm{HSIC}}(K_{2},K_{2})}}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where ", "page_idx": 37}, {"type": "equation", "text": "$$\nK_{1}={\\left[\\begin{array}{l l l}{k_{1}(\\mathbf{c}^{1},\\mathbf{c}^{1})}&{\\cdots}&{k_{1}(\\mathbf{c}^{1},\\mathbf{c}^{p})}\\\\ {\\vdots}&{\\ddots}&{\\vdots}\\\\ {k_{1}(\\mathbf{c}^{p},\\mathbf{c}^{1})}&{\\cdots}&{k_{1}(\\mathbf{c}^{p},\\mathbf{c}^{p})}\\end{array}\\right]}{\\mathrm{~and~}}K_{2}={\\left[\\begin{array}{l l l}{k_{2}(\\mathbf{c}^{1},\\mathbf{c}^{1})}&{\\cdots}&{k_{2}(\\mathbf{c}^{1},\\mathbf{c}^{p})}\\\\ {\\vdots}&{\\ddots}&{\\vdots}\\\\ {k_{2}(\\mathbf{c}^{p},\\mathbf{c}^{1})}&{\\cdots}&{k_{2}(\\mathbf{c}^{p},\\mathbf{c}^{p})}\\end{array}\\right]}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "are Gram matrices and $\\widehat{\\mathrm{HSIC}}$ is an estimator of HSIC defined as ", "page_idx": 37}, {"type": "equation", "text": "$$\n{\\widehat{\\mathrm{HSIC}}}(L,M)={\\frac{1}{(p-1)^{2}}}\\mathrm{tr}(L H M H),\\quad L,M\\in\\mathbb{R}^{p\\times p}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $\\begin{array}{r}{H:=I_{p}-\\frac{1}{p}\\mathbf{1}\\mathbf{1}^{\\top}}\\end{array}$ is the centering matrix, $I_{p}$ is a $p\\times p$ identity matrix, and $\\mathbf{1}=[1,1,\\cdots\\,,1]^{\\top}$ is a $p$ -dimensional one vector. During the kernel distillation procedure, the $i$ th local party maximizes $\\widehat{\\mathrm{CKA}}(k_{f_{i}},k)$ on $Z$ where $k$ is a fixed target kernel given by (4). In practice, we use batching to perform the kernel distillation to reduce computational costs. ", "page_idx": 37}, {"type": "text", "text": "Due to the definition of the empirical CKA, it is necessary to calculate the Gram matrix of $k$ over $Z$ To this end, the $i$ th local party calculates the Gram matrix of $k_{f_{i}}$ over $Z$ and uploads it to the server for $i=1,\\cdot\\cdot\\cdot,m$ . Then the server computes the Gram matrix of $k$ by weighted averaging the Gram matrices of local feature kernels $k_{f_{1}},\\cdot\\cdot\\cdot\\mathrm{\\Delta},k_{f_{m}}$ . Since this process only requires communication of feature kernel values, DCL-NN still preserves the privacy of local model information. ", "page_idx": 37}, {"type": "text", "text": "While (empirical) CKA is a good metric for kernel matching, it is invariant to scaling, and therefore the local feature kernels resulting from the kernel distillation may have different scales. This affects the degree to which each local training influences during the DCL-KR-like follow-up procedure. To illustrate this point, consider the following example: Let the feature kernels of two local models $f_{1}$ and $f_{2}$ be as follows: ", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 38}, {"type": "equation", "text": "$$\nk_{f_{1}}(\\mathbf{x},\\mathbf{y})=\\phi(\\mathbf{x})^{\\top}\\phi(\\mathbf{y}),\\qquad k_{f_{2}}(\\mathbf{x},\\mathbf{y})=(\\alpha\\phi(\\mathbf{x}))^{\\top}(\\alpha\\phi(\\mathbf{y})).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "After distilling on the public data with consensus predictions, these two models become like ", "page_idx": 38}, {"type": "equation", "text": "$$\nf_{1}(\\cdot)=\\mathbf{w}^{\\top}\\phi(\\cdot)+b,\\qquad f_{2}(\\cdot)=\\left(\\frac{1}{\\alpha}\\mathbf{w}\\right)^{\\top}(\\alpha\\phi(\\cdot))+b=\\mathbf{w}^{\\top}\\phi(\\cdot)+b,\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "i.e., two models are the same. Nevertheless, a gradient descent update on a data point $\\left(\\mathbf{x}_{0},y_{0}\\right)$ with a learning rate $\\eta$ ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbf{w}_{1}\\gets\\mathbf{w}-\\eta(\\mathbf{w}^{\\top}\\phi(\\mathbf{x}_{0})+b-y_{0})\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "for $f_{1}$ and ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbf{w}_{2}\\gets\\frac{1}{\\alpha}\\mathbf{w}-\\eta(\\mathbf{w}^{\\top}\\phi(\\mathbf{x}_{0})+b-y_{0})\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "for $f_{2}$ . Thus, after the gradient descent step we have ", "page_idx": 38}, {"type": "equation", "text": "$$\nf_{1}(\\cdot)=(\\mathbf{w}-\\eta(\\mathbf{w}^{\\top}\\phi(\\mathbf{x}_{0})+b-y_{0}))^{\\top}\\phi(\\cdot)+b\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "and ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathfrak{r}_{2}(\\cdot)=\\left(\\frac{1}{\\alpha}\\mathbf{w}-\\eta(\\mathbf{w}^{\\top}\\phi(\\mathbf{x}_{0})+b-y_{0})\\right)^{\\top}(\\alpha\\phi(\\cdot))+b=\\left(\\mathbf{w}-\\alpha\\eta(\\mathbf{w}^{\\top}\\phi(\\mathbf{x}_{0})+b-y_{0})\\right)^{\\top}\\phi(\\cdot)+b.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Hence the scale $\\alpha$ affects the collaborative learning procedure. To address this issue, we compute the scale $\\alpha$ using the estimator HSIC. Specifically, at the beginning of the collaborative learning phase, we compute ${\\alpha_{i}=\\widehat{\\mathrm{HSIC}}(K_{i},K_{i})}$ where $K_{i}$ is the Gram matrix with respect to the feature kernel of the $i$ th party on $Z\\left(i=1,\\cdot\\cdot\\cdot,m\\right)$ . Then we set the learning rate for the ith party as ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\eta_{0}\\cdot\\frac{\\operatorname*{max}_{1\\le j\\le m}\\alpha_{j}^{1/2}}{\\alpha_{i}^{1/2}}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where $\\eta_{0}$ is a base learning rate. In practice, computing HSIC over all public inputs is costly and unnecessary. Using only a small subset of public inputs is sufficient. ", "page_idx": 38}, {"type": "text", "text": "We present DCL-NN in Algorithm 2. Here are some remarks. ", "page_idx": 38}, {"type": "text", "text": "(1\uff09  The feature kernel distillation procedure requires only one round of two-way communication between the server and the parties.   \n(2) The collaborative learning procedure follows the same process as in DCL-KR with $g_{1},\\cdot\\cdot\\cdot,g_{m}$ fixed. Note that kernel gradient descent reduces to standard gradient descent since the kernels have finite rank. In this process, if possible, optimization on the public dataset can be performed using the closed-form solution of kernel linear regression instead of gradient descent.   \n(3) In this work, we apply FedMD for the pretraining of DCL-NN in the experiment. However, DCL-NN is a general algorithm that can use any algorithm for pretraining to obtain good local feature kernels. For example, kernel learning techniques [63] may be applied for pretraining.   \n(4) Our algorithm can be naturally extended to regression problems with multi-dimensional outputs. ", "page_idx": 38}, {"type": "text", "text": "C  Details and Further Discussion on Experiments ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "C.1  Dataset Description ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "For all datasets, we follow Algorithm 3 to construct non-i.i.d. settings. Note that this procedure is similar to the non-i.i.d. data generation procedure in classification tasks [44, 69]. ", "page_idx": 38}, {"type": "text", "text": "Algorithm 3 Data Generating Procedure ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "1: Inputs: the number of parties $m$ , the total number of private data $n$ , the number of public inputs   \n$n_{0}$ , the partition of input space $\\mathcal{A}=\\{\\mathcal{A}_{1},\\cdot\\cdot\\cdot,\\mathcal{A}_{c}\\}$   \n2: Generate the whole private dataset size of $n$ from $\\rho_{\\mathbf{x},y}$ and the public inputs size of $n_{0}$ from $\\tilde{\\rho}_{\\mathbf{x}}$   \n3: Sample a base private data ratio $(\\alpha_{1},\\cdot\\cdot\\cdot,\\alpha_{m})$ from $\\breve{\\mathrm{Dir}}([10,10,\\cdots\\,,10])$   \n4: while $\\textstyle\\sum_{j=1}^{m}{\\bar{C_{i j}}}\\neq0$ for all $i=1,\\cdot\\cdot\\cdot,c\\,\\mathbf{do}$   \n5: for party $j=1,\\cdot\\cdot\\cdot,m$ do   \n6: Sample two elements uniformly from the partition $\\boldsymbol{\\mathcal{A}}$   \n7: end for   \n8:  Set $C_{i j}=1$ $\\boldsymbol{A}_{i}$ is chosen at the $j$ th party and $C_{i j}=0$ otherwise. ", "page_idx": 39}, {"type": "text", "text": "9: end while ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "10: for $i=1,\\cdots,c$ do ", "page_idx": 39}, {"type": "text", "text": "11: Put the private data points where inputs are in $\\mathcal{A}_{i}$ into the datasets of parties with ratio ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\left[\\frac{\\alpha_{k}C_{i k}}{\\sum_{j=1}^{m}\\alpha_{j}C_{i j}}\\right]_{k=1,\\cdots,m}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "12: end for ", "page_idx": 39}, {"type": "text", "text": "C.1.1 Toy-1D ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Let $\\mathcal{X}=[0,1]\\subset\\mathbb{R}$ and $\\rho_{x}$ be the uniform distribution on $\\mathcal{X}$ . The space ", "page_idx": 39}, {"type": "equation", "text": "$$\nH^{1}:=\\left\\{f\\in\\mathrm{AC}[0,1]\\;\\Big|\\;f(0)=0,\\int f^{\\prime}(x)^{2}\\;d\\rho_{x}(x)<\\infty\\right\\}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "is the reproducing kernel Hilbert space associated to the kernel $k(x,y)=\\operatorname*{min}(x,y)$ Where $\\mathrm{AC}[0,1]$ is the collection of all absolutely continuous functions on $[0,1]$ [33, 60]. As mentioned in [33], the covariance operator $T_{k,\\rho_{x}}$ has eigenpairs $\\{(\\lambda_{i},e_{i})\\}_{i\\in\\mathbb{N}}$ where ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\lambda_{i}=\\left(\\frac{2i-1}{2}\\pi\\right)^{-2},\\qquad e_{i}(x)=\\sqrt{2}\\sin\\left(\\frac{2i-1}{2}\\pi x\\right).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Thus, the eigenvalue dcay rate $s$ .s $\\frac{1}{2}$ . Set a target function ", "page_idx": 39}, {"type": "equation", "text": "$$\nf_{0}^{*}(x)=\\sum_{i=1}^{\\infty}\\frac{e_{i}(x)}{i^{3}}=\\sum_{i=1}^{\\infty}\\frac{\\sqrt{2}}{i^{3}}\\sin\\left(\\frac{2i-1}{2}\\pi x\\right).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "From the fact that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\left\\|T_{k,\\rho_{x}}^{1/2-r}\\sum_{i=1}^{\\infty}h_{i}e_{i}\\right\\|_{\\mathbb{H}_{k}}\\leq R\\quad\\Leftrightarrow\\quad\\sum_{i=1}^{\\infty}\\frac{h_{i}^{2}}{\\lambda_{i}^{2r}}\\leq R^{2},\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "wehave $f_{0}^{*}=T_{k,\\rho_{x}}^{1/2}g_{0}^{*}$ such hat ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\|g_{0}^{*}\\|_{\\mathbb{H}_{k}}=\\left(\\sum_{i=1}^{\\infty}{\\frac{1}{i^{6}}}\\left({\\frac{2i-1}{2}}\\pi\\right)^{4}\\right)^{1/2}=:R<\\infty.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Then $r=1$ .We generate data points from $\\rho_{x}\\cdot\\rho_{y|x}$ such that $\\rho_{x}$ is the uniform distribution on $\\mathcal{X}$ as above and ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\rho_{y|x}=\\mathcal{N}(y|f_{0}^{*}(x),0.44^{2}).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "We divide $\\mathcal{X}$ into a partition $\\begin{array}{r}{\\mathcal{A}:=\\left\\{\\left[\\frac{i}{8},\\frac{i+1}{8}\\right]:i=0,\\cdot\\cdot\\cdot,7\\right\\}}\\end{array}$ We followAlgorithm3with $m\\in$ $[10,20,\\cdots\\,,100]$ \uff0c $n\\,=\\,50m$ , and $n_{0}\\,\\approx\\,n^{\\frac{1}{2r+s}}(\\log_{10}n)^{3}$ .With $n_{0}\\,\\approx\\,n^{\\frac{1}{2r+s}}(\\log_{10}n)^{3}$ , we also achieve the same bound (with different prefactors) in Theorem 3.4. In the main experiments, we set $\\rho_{\\mathbf{x}}=\\tilde{\\rho}_{\\mathbf{x}}$ ", "page_idx": 39}, {"type": "text", "text": "C.1.2Toy-3D ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Let $\\mathcal{X}\\,=\\,[0,1]^{3}\\;\\subset\\;\\mathbb{R}^{3}$ and $\\rho_{\\mathbf{x}}$ be the uniform distribution on $\\mathcal{X}$ .Define a kernel $k(\\mathbf{x},\\mathbf{y})\\;=\\;$ $(1-\\|\\mathbf{x}-\\mathbf{y}\\|_{\\mathbb{R}^{3}})_{+}^{2}$ . The reproducing kernel Hilbert space $\\mathbb{H}_{k}$ associated to the kernel $k$ is normequivalent to Sobolev space $H^{2}(\\mathcal{X})$ [55] and the eigenvalue decay of $T_{k,\\rho_{\\times}}$ is $\\begin{array}{r}{s=\\frac{3}{4}}\\end{array}$ [68]. Note that $k^{\\prime}({\\bf x},{\\bf y})=(1-\\|{\\bf x}-{\\bf y}\\|_{\\mathbb{R}^{3}})_{+}^{6}(35\\|{\\bf x}-{\\bf y}\\|_{\\mathbb{R}^{3}}^{2}+18\\|{\\bf x}-{\\bf y}\\|_{\\mathbb{R}^{3}}+3)$ is a kernel and its reproducing kernel Hilbert space is norm-equivalent to Sobolev space $H^{4}(\\mathcal{X})$ . Using the interpolation relation between $H^{2}(\\vec{x})$ and $H^{4}(\\mathcal{X})$ gives that ", "page_idx": 40}, {"type": "equation", "text": "$$\nf_{0}^{*}(\\mathbf{x})=(1-\\|\\mathbf{x}\\|_{\\mathbb{R}^{3}})_{+}^{6}(35\\|\\mathbf{x}\\|_{\\mathbb{R}^{3}}^{2}+18\\|\\mathbf{x}\\|_{\\mathbb{R}^{3}}+3)\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "is in $\\mathbb{H}_{k}$ and the regularity $r$ is 1. Similarly as before, we generate data points from $\\rho_{\\mathbf{x}}\\cdot\\rho_{y|\\mathbf{x}}$ such that $\\rho_{\\mathbf{x}}$ is the uniform distribution on $\\mathcal{X}$ and ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\rho_{y|\\mathbf{x}}=\\mathcal{N}(y|f_{0}^{*}(\\mathbf{x}),0.44^{2}).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "We divide $\\mathcal{X}$ into a partition $\\begin{array}{r}{\\mathcal{A}:=\\{[\\frac{k_{1}}{2},\\frac{k_{1}+1}{2}]\\times[\\frac{k_{2}}{2},\\frac{k_{2}+1}{2}]\\times[\\frac{k_{3}}{2},\\frac{k_{3}+1}{2}]\\subset\\mathbb{R}^{3}:(k_{1},k_{2},k_{3})\\in\\mathbb{R}^{4}\\}.}\\end{array}$ $\\{0,1\\}^{3}\\}$ .Again, we follow Algorithm 3 with $m~\\in~[10,20,\\cdots,100]$ \uff0c $\\textit{n}=\\,\\,50m$ \uff0c $n_{0}~\\approx$ $n^{\\frac{1}{2r+s}}(\\log_{10}n)^{3}$ , and $\\rho_{\\mathbf{x}}\\,=\\,\\tilde{\\rho}_{\\mathbf{x}}$ for kernel machine-based algorithms. For neural network-based algorithms, $m=50$ \uff0c $n_{0}=2500$ , and the other configurations are the same. ", "page_idx": 40}, {"type": "text", "text": "C.1.3 Real World Datasets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Energy _  Energy dataset is a real-world tabular dataset from the UCI database [12]. It has 28 input features including measurement time, temperature and humidity of each room, outside temperature, and wind speed. The output is the appliances energy use. We normalize all features, including the output, using MinMaxScaler. There are 12,000 training data points distributed across the parties. We use 6,000 samples as public inputs and 1,000 samples for testing. To construct a non-i.i.d. setting, weset a partition $\\boldsymbol{\\mathcal{A}}$ consisting of 8 subsets, each formed by splitting three normalized variables (measurement time, visibility, and dewpoint) at their midpoints. We apply Algorithm 3 with $m=50$ ", "page_idx": 40}, {"type": "text", "text": "RotatedMNIST RotatedMNIST is a dataset derived from MNIST [11]. The task is to predict the rotated angle of a given rotated MNIST image. Each image is $1\\times28\\times28$ image, and we normalize all images by their mean and variance. To generate RotatedMNIST, we rotate MNIST images by a random angle between $-\\,{\\frac{\\pi}{2}}$ and $\\scriptstyle{\\frac{\\pi}{2}}$ and use the angle as the label. To construct a large-scale dataset, each image is rotated at multiple angles to generate multiple data instances. For training data, we additionally inject Gaussian noise with a standard deviation of 0.2 to each label. We use 200,000 images as the entire training data, 50,000 images as public inputs, and 50,000 images as test data. The whole training input distribution, public input distribution, and test input distribution are uniformly distributed across digits 0 to 9. For example, there are 20,000 rotated images of the digit $\\cdot_{4},$ in the training set. For a non-i.i.d. setting, we partition the data into $\\boldsymbol{\\mathcal{A}}$ where $|\\mathcal{A}|=10$ , based on the digit $(0\\sim9)$ and follow Algorithm 3 with $m=50$ ", "page_idx": 40}, {"type": "text", "text": "UTKFace  UTKFace dataset [71] is an image dataset used for age estimation. Since the image sizes vary, we resize all images to $3\\times128\\times128$ and normalize them by their mean and variance for each channel. The labels are normalized to the range $[0,1]$ using MinMaxScaler. For training data, we inject Gaussian noise with a standard deviation of 0.5 to each label before normalization. We use 12,544 samples for training and 1,039 samples for testing. We have 6,234 public inputs. These three datasets have the same distribution for metadata (gender and race). Based on this metadata, we construct a partition $\\boldsymbol{\\mathcal{A}}$ With $|\\mathcal{A}|=10$ and distribute the training data among 50 parties according to Algorithm 3. ", "page_idx": 40}, {"type": "text", "text": "IMDB-WIKI  IMDB-WIKI dataset [52] is also an image dataset for age estimation. In experiments, we utilize a clean version [42]. We further resize all images to $3\\times64\\times64$ and normalize them as UTKFace. The labels are also normalized to the range $[0,1]$ using MinMaxScaler. We use 147,107 images as the entire training data, 36,780 images as public inputs, and 56,087 images as test data. In this dataset, we utilize the triplet (head_roll, head_yaw, head_pitch) as metadata. Both training inputs and public inputs have the same distribution for metadata. We construct a decentralized setting among 50 parties using a partition $\\boldsymbol{\\mathcal{A}}$ based on the metadata, following Algorithm 3. The partitioning is performed by dividing the dataset into regions based on the median values of each metadata variable. ", "page_idx": 40}, {"type": "table", "img_path": "LdZ0u1FuXb/tmp/24ea985e49aa8db061243c837fb924ce9be571a13dd37f1ce17c8473f8121774.jpg", "table_caption": ["Table 4: Hyperparameters $C$ and $D$ on kernel machine-based algorithms in main results "], "table_footnote": [], "page_idx": 41}, {"type": "text", "text": "C.2  Implementation ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "The experiments are implemented in PyTorch. We simulate a decentralized setting on a single deep learning workstation (Intel(R) Xeon(R) Gold 6430 with one NVIDIA GeForce RTX 4090 GPU and 189GB RAM). All DCL-KR implementations take less than 10 minutes per simulation. The non-parallel implementation of DCL-NN for large-scale datasets is completed within 48 hours. With parallel computing, the execution time for the same setup is expected to be reduced to within 2 hours. ", "page_idx": 41}, {"type": "text", "text": "C.3 Experimental Setup and Results on Kernel Machine-based Algorithms ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "C.3.1 Experimental Setup Details and Main Results ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "In this experiment, we evaluate DCL-KR by comparing its performance against two central models and three baselines. Specifically, we employ DC-NY [66], DKRR-NY-CM [67], and IED [48] as baselines. We also compare DCL-KR with central Kernel Ridge Regression (centralKRR) and central Kernel Regression with Gradient Descent (centralKRGD). As mentioned earlier, we evaluate the performance of these algorithms on Toy-1D and Toy-3D datasets. ", "page_idx": 41}, {"type": "text", "text": "There are several hyperparameters for kernel machine-based algorithms: the ridge regularization hyperparameter $\\lambda$ in the ridge regressions and the number of iterations (or communication rounds) in the gradient descent-based regressions. We set $\\lambda=C\\cdot n^{-{\\frac{1}{2r+s}}}$ and $T=\\mathrm{int}(D\\cdot n^{\\frac{1}{2r+s}})$ which are the optimal choices from theory. We determine the best values for $C$ and $D$ by grid search in our experiments (see Table 4 for the selected hyperparameter values). For centralKRGD and DCL-KR, we set the learning rate $\\eta=0.5$ whichsatisfies $\\dot{\\eta}\\in(0,1/\\kappa^{2})$ in Theorem 3.4. The number of local iterations $E$ for DCL-KR is set to 5. For DKRR-NY-CM, we modify its Newton-Raphson iteration as shown below due to an instability issue, and we set the learning rate $\\eta=0.01$ ", "page_idx": 41}, {"type": "equation", "text": "$$\nu\\gets u-\\eta\\cdot\\sum_{j=1}^{m}\\frac{n_{j}}{n}(P_{Z}T_{k,X_{i}}P_{Z}+\\lambda I)^{-1}((P_{Z}T_{k,X}P_{Z}+\\lambda I)u-P_{Z}S_{D}^{\\top}\\mathbf y).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "The communication round $T$ for DKRR-NY-CM is set to 10. ", "page_idx": 41}, {"type": "text", "text": "To measure performance, we sample data from the distribution presented in Appendix C.1 for each simulation and compute $\\mathbb{E}\\|\\iota_{\\rho_{\\mathbf{x}}}(f_{i,T}-f_{0}^{*})\\|_{L_{\\rho_{\\mathbf{x}}}^{2}}$ by averaging the Root Mean Squared Errors (RMSEs) on the test dataset. We conduct 500 simulations for each setting, and the results are summarized in Figure1. ", "page_idx": 41}, {"type": "text", "text": "As shown in Table 1, DC-NY and DKRR-NY-CM have theoretical performance guarantees under the statistically homogeneous condition for a limited number of parties. However, they do not exhibit sufficiently good performance in massively distributed statistically heterogeneous settings. The performance degradation of DKRR-NY-CM appears to be linked to its second-order optimization scheme, which leads to ineffective batching in statistically heterogeneous settings. The performance degradation of DC-NY is expected, given the inherent limitations of divide-and-conquer algorithms. ", "page_idx": 41}, {"type": "text", "text": "In contrast, IED demonstrates relatively better performance, despite the strong assumptions underlying its theory. Nevertheless, it still exhibits performance degradation compared to centralized models. DCL-KR, on the other hand, achieves performance comparable to centralized models, validating both the theoretical results and its practical feasibility. ", "page_idx": 41}, {"type": "text", "text": "C.3.2Effect of $n_{0}$ ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "As public inputs directly affect the training information sharing, we anticipate that the performance of DCL-KR will vary depending on the number of public inputs $n_{0}$ . To examine this effect, we measure the performance of DCL-KR on Toy-3D for $n_{0}\\approx\\alpha\\cdot n^{\\frac{1}{2r+s}}(\\log_{10}n)^{3}$ withvarious $\\alpha\\in$ $\\{0.1,0.3,0.5,1,2\\}$ . Additionally, we conduct the same experiment with IED, the most competitive baseline, for comparison. ", "page_idx": 41}, {"type": "image", "img_path": "LdZ0u1FuXb/tmp/18da2880409b3a3574bceaec5548ad17ad95e6fe68ab7970f5efdc6f3b3c386d.jpg", "img_caption": ["Figure 5: Performance of IED and DCL-KR with $\\tilde{\\rho}_{\\mathbf{x}}\\neq\\rho_{\\mathbf{x}}$ on Toy-3D "], "img_footnote": [], "page_idx": 42}, {"type": "text", "text": "", "page_idx": 42}, {"type": "text", "text": "The results are summarized in Figure 2. Consistent with theoretical results, the value of $\\alpha$ doesnot affect the convergence rate of IED and DCL-KR. However, IED displays significant performance variations across different $\\alpha$ values. In contrast, DCL-KR achieves its maximum performance when $\\alpha$ is not too small (i.e., $\\alpha\\geq0.3$ in Figure 2). This implies that DCL-KR requires fewer public inputs to achieve its maximal performance compared to IED, as predicted by theoretical results. ", "page_idx": 42}, {"type": "text", "text": "C.3.3Effect of $\\tilde{\\rho}_{\\mathbf{x}}$ ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "So far, we consider the settings with $\\rho_{\\mathbf{x}}=\\tilde{\\rho}_{\\mathbf{x}}$ . However, Theorem 3.4 covers the general case where $\\tilde{\\rho}_{\\mathbf{x}}\\neq\\rho_{\\mathbf{x}}$ . To verify this, we define the public input distribution in Toy-3D with the following density function (parametrized by $\\beta$ ", "page_idx": 42}, {"type": "equation", "text": "$$\np(x_{1},x_{2},x_{3}|\\beta)=\\prod_{i=1}^{3}((2-2\\beta)x_{i}+\\beta),\\quad(x_{1},x_{2},x_{3})\\in[0,1]^{3},\\quad\\beta\\in(0,1].\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "The Radon-Nikody derivatve $\\frac{d\\rho_{\\mathbf{x}}}{d\\widetilde{\\rho}_{\\mathbf{x}}}$ satisfes $\\begin{array}{r}{0\\le\\frac{d\\rho_{\\mathbf{x}}}{d\\tilde{\\rho}_{\\mathbf{x}}}\\le(\\frac{1}{\\beta})^{3}}\\end{array}$ We condut aditional expriments to verify Theorem 3.4 and Corollary A.7, considering the case where (with ) and the case where $\\beta=0.5$ but $\\alpha=4$ to compensate. The results are provided in Figure 5. In the log-scale plot, the slope represents the convergence rate. ", "page_idx": 42}, {"type": "text", "text": "First, we observe that the convergence rate of DCL-KR remains unchanged when $\\beta$ is changed from 1 (i.e., $\\rho_{\\mathbf{x}}=\\tilde{\\rho}_{\\mathbf{x}},$ to 0.5. This observation is consistent with Theorem 3.4. Additionally, regarding Corollary A.7, we confirm that DCL-KR achieves performance almost identical to the case of $\\rho_{\\mathbf{x}}=\\tilde{\\rho}_{\\mathbf{x}}$ by increasing $n_{0}$ . In contrast, the convergence rate of IED worsens when $\\tilde{\\rho}_{\\bf x}$ changes, even when $n_{0}$ is increased. These experimental results highlight the advantages of DCL-KR in statistically heterogeneous environments. ", "page_idx": 42}, {"type": "text", "text": "Table 5: Hyperparameters for Standalone ", "page_idx": 43}, {"type": "table", "img_path": "LdZ0u1FuXb/tmp/cd600c1d00519f32a141451c90e5b34548925ee2228af4b0788807f46209ba86.jpg", "table_caption": [], "table_footnote": [], "page_idx": 43}, {"type": "table", "img_path": "LdZ0u1FuXb/tmp/7487bc11c3454c0e9fc6b36b4d364985234619d74bb1de85f584455cad67694a.jpg", "table_caption": ["Table 6: Hyperparameters for FedMD "], "table_footnote": [], "page_idx": 43}, {"type": "table", "img_path": "LdZ0u1FuXb/tmp/e42fdce2a0818e284d961cfb2b9803b0608b12cbca1aa3dd9a3dd7f9cb99234a.jpg", "table_caption": ["Table 7: Hyperparameters for FedHeNN "], "table_footnote": [], "page_idx": 43}, {"type": "text", "text": "C.4  Experimental Setup and Results on Neural Network-based Algorithms ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "C.4.1 Experimental Setup Details ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "In the experiments on neural network-based collaborative learning algorithms, we evaluate three baselines (FedMD, FedHeNN, KT-pFL) and our algorithm DCL-NN. Note that while KT-pFL is a personalized collaborative learning algorithm, it also performs well in non-personalized settings, so we include it for comparison. Additionally, we evaluate centralized models as ideal cases and standalone models as worst cases. Centralized models are trained using all local data. ", "page_idx": 43}, {"type": "text", "text": "We use two tabular datasets, Toy-3D and Energy, and three image datasets, RotatedMNIST, UTKFace, and IMDB-WIKI. The number of parties is set to 50 for all settings. For the tabular datasets, we employ four different fully connected neural networks (FNNs) with a ratio of $30\\%$ \uff0c $30\\%$ \uff0c $20\\%$ , and $20\\%$ . Specifically, There are fifteen 4-layer FNNs with 32 hidden units, fifteen 4-layer FNNs with 64 hidden units, ten 5-layer FNNs with 32 hidden units, and ten 3-layer FNNs with 64 hidden units. Similarly, for the image datasets, we use four different convolutional neural networks (CNNs) with thesameratioof $30\\%$ \uff0c $30\\%$ \uff0c $20\\%$ , and $20\\%$ . For the large-scale image datasets (RotatedMNIST and IMDB-WIKI), we construct 50 local parties using fifteen ResNet-18, fifteen ResNet-34, ten ResNet50 [20], and ten MobileNetv2 [54]. For UTKFace, which is an image dataset with limited data, we utilize four simpler CNN architectures due to the ineffectiveness of knowledge distillation with large underperforming models. The first and third CNNs share a similar architecture, featuring two convolutional layers with batch normalization [22], two max pooling layers, and two fully-connected layers at the end. They differ only in the number of channels. In contrast, the second and fourth CNNs are more complex, featuring four convolutional layers with batch normalization, two max pooling layers, and two fully-connected layers at the end. They also differ in the number of channels. We use the ReLU activation function throughout. ", "page_idx": 43}, {"type": "table", "img_path": "LdZ0u1FuXb/tmp/222f192f8e320658e05f90b5c984a521f26fcb7cf43acc9045d87ba045203d42.jpg", "table_caption": ["Table 8: Hyperparameters for KT-pFL "], "table_footnote": [], "page_idx": 44}, {"type": "table", "img_path": "LdZ0u1FuXb/tmp/9beb36c2326dc56276b50f605f384e3b0dc6f7bafc4a7dab0f289df14edf2a8e.jpg", "table_caption": ["Table 9: Hyperparameters for DCL-NN "], "table_footnote": [], "page_idx": 44}, {"type": "text", "text": "All optimizers used are Adam [26].2 One remark is that baseline algorithms only utilize a subset of public inputs through random sampling in each communication round, as performance tends to deteriorate due to overfitting when all public inputs are used in every round. Hyperparameters are tuned via grid search. ", "page_idx": 44}, {"type": "text", "text": "Standalone models are trained with cross-validation and early stopping to prevent overfitting. To evaluate centralized models, we first compute the averaged test Root Mean Squared Error (RMSE) from at least 10 simulations for each neural network architecture and then calculate the weighted average of the performances of all architectures according to their ratio. For standalone models, we use the average of the test RMSEs of local models with the hyperparameters listed in Table 5. In the table, RotatedMNIST is abbreviated as MNIST, and IMDB-WIKI is abbreviated as IMDB. ", "page_idx": 44}, {"type": "text", "text": "For FedHeNN, we set the number of local epochs to 30 in all experiments. For KT-pFL, we set the number of local epochs to 10, the distillation coefficient to 0.5, and the learning rate of knowledge coefficient to 1e-3. Lastly, for DCL-NN in the main experiment, we set the learning rate for kernel matching to 1e-4 and the number of communication rounds in the collaborative learning phase to 50. We use the closed-form solution to train public data and full-batch gradient descent to train local data in the collaborative learning phase of DCL-NN and utilize FedMD for pretraining in DCL-NN. In the pretraining phase, the hyperparameters are the same as in the FedMD setting, except that the number of communication rounds is 100 for tabular data and 50 for image data. The remaining hyperparameters are presented in Table 6, 7, 8, and 9. For all distillation-based collaborative learning algorithms, we simulate each setting at least 5 times with different initializations. ", "page_idx": 44}, {"type": "text", "text": "Communication Efficiency Compared with FedMD and KT-pFL, DCL-NN incurs higher communicationcosts of $O(n_{0}^{2})$ due to the transmission of the Gram matrix. FedHeNN also utilizes kernel matching but performs it in batches for each communication round. Thus, in scenarios requiring many communication rounds, DCL-NN is more efficient than FedHeNN. However, pretraining also demands more communication cost. We leave the study of communication-efficient methods in DCL-NN for future work. ", "page_idx": 44}, {"type": "text", "text": "C.4.2 Effect of Public Inputs ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "In practice, the public inputs can be sampled from a distribution whose support is disjoint from that of the whole local input distribution. In this case, the assumption of DCL-KR does not hold; however, DCL-NN can still be applicable. To evaluate the performance of DCL-NN under these conditions, we compare the performance of DCL-NN and FedMD when the distribution of public inputs differs, as in Table 10. We use CIFAR10 [28] for public inputs on UTKFace. As shown in Table 10, DCL-NN ", "page_idx": 44}, {"type": "text", "text": "Table 10: Performance comparison of FedMD and DCL-NN on UTKFace with different public datasets. In addition to performance, the kernel performance of local feature kernels, computed in the same way as before, is shown in parantheses. ", "page_idx": 45}, {"type": "table", "img_path": "LdZ0u1FuXb/tmp/c1a95d3f415f12b3361f827c101d4abe771d0dedc99c59b7838db8c6f7167080.jpg", "table_caption": [], "table_footnote": [], "page_idx": 45}, {"type": "text", "text": "does not yield better results in this case. Note that the kernel performance of local feature kernels is improved compared to FedMD, leading us to conclude that the performance degradation of DCL-NN with CIFAR10 is due to the violation of the DCL-KR assumption rather than the ineffectiveness of the kernel distillation procedure. ", "page_idx": 45}, {"type": "text", "text": "D  Limitations and Future Works ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Privacy Benefits of Distillation-based Collaborative Learning Due to its black-box nature, distillation-based information interaction is expected to offer privacy preservation benefits compared to parameter exchange (mainly done in FL) as mentioned in [19]. To the best of our knowledge, there is no rigorous study that discusses the privacy preservation advantages of distillation-based collaborative learning. We hope to see further discussion on this as well. ", "page_idx": 45}, {"type": "text", "text": "Public Input Distribution  Theorem 3.4 covers the case where the public input distribution $\\tilde{\\rho}_{\\mathbf{x}}$ differs from that of local data inputs $\\rho_{\\mathbf{x}}$ , but at least the support of $\\tilde{\\rho_{\\mathbf{x}}}$ must include the support of $\\rho_{\\mathbf{x}}$ . Therefore, we experimentally observe a performance drop in DCL-NN, a practical extension of DCL-KR, when $\\tilde{\\rho}_{\\mathbf{x}}$ and $\\rho_{\\mathbf{x}}$ have different supports. Enhancing the robustness of DCL-NN in this scenario is considered a promising direction for future work. Our theory does not cover situations where collecting public inputs is difficult. In such cases, a seperate generative model is usually trained to generate public inputs [72]. We leave the theoretical discussion that includes these cases for future work. ", "page_idx": 45}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: The abstract and introduction accurately describe the motivations, theoretical/experimental contributions, and scope of our work. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 46}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Justification: See Appendix D ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u25cf The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u25cf The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should refect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u25cf The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 46}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: The theoretical results and its assumptions are clearly described in Section 3 and Appendix A. Idea of the proof is briefly described in Section 3 and the full proof is provided in Appendix A. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u25cf Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 47}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: The proposed algorithms are clearly stated in Algorithm 1 and Algorithm 2. Experimental details such as experimental setting, performance measure, data preprocessing, and hyperparameter setting are also explained in Section 5 and Appendix C. The code is also provided via the supplementary material. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 47}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: The code is provided via the supplementary material. Regarding datasets, the paper contains data source and preprocessing descriptions. The paper also provides sufficient experimental details to reproduce the experimental results. See Section 5 and AppendixC. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u25cf At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 48}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: Experimental details such as data split, hyperparameters, optimizers, and model architectures are provided in Section 5 and Appendix C. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u25cf The answer NA means that the paper does not include experiments. \u25cf The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u25cf The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 48}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: The experimental results contains 1-sigma error bars and the explanations about the error bars are provided. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. ", "page_idx": 48}, {"type": "text", "text": "\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u25cf The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u25cf The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 49}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce theexperiments? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: Descriptions of computer resources are provided in Appendix C.2. Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u25cf The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 49}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: The authors verify that the research is conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 49}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] Justification: See Section 6. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u25cf If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u25cf The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 50}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 50}, {"type": "text", "text": "Justification: The paper poses no such risks. Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u25cf The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 50}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: All data and models used in the paper are credited through citations according to the license. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u25cf The authors should state which version of the asset is used and, if possible, include a URL.   \n\u25cf The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 50}, {"type": "text", "text": "", "page_idx": 51}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 51}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 51}, {"type": "text", "text": "Justification: The code is provided via the supplementary material with a well-written documentation. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose asset isused.   \n\u25cf At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 51}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 51}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 51}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u25cf The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 51}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 51}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 51}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 51}, {"type": "text", "text": "\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u25cf For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 52}]