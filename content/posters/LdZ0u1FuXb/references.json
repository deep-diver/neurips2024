{"references": [{"fullname_first_author": "A. Caponnetto", "paper_title": "Optimal rates for the regularized least-squares algorithm", "publication_date": "2007-01-01", "reason": "This paper provides the theoretical foundation for the minimax optimality analysis of regularized kernel regression, which is crucial for the theoretical results of this paper."}, {"fullname_first_author": "S. P. Karimireddy", "paper_title": "Scaffold: Stochastic controlled averaging for federated learning", "publication_date": "2020-01-01", "reason": "This paper introduces the Scaffold algorithm, a key algorithm in Federated Learning which is relevant to the context of this paper and serves as a related work."}, {"fullname_first_author": "J. Lin", "paper_title": "Optimal rates for learning with Nystr\u00f6m stochastic gradient methods", "publication_date": "2017-01-01", "reason": "This paper establishes optimal learning rates for kernel regression algorithms with early stopping, providing insights on the convergence rates achieved in this work."}, {"fullname_first_author": "D. Li", "paper_title": "FedMD: Heterogenous federated learning via model distillation", "publication_date": "2019-01-01", "reason": "This paper introduces the FedMD algorithm, a central algorithm that this paper analyzes and extends, serving as a primary foundation for the proposed distillation-based collaborative learning approach."}, {"fullname_first_author": "G. Raskutti", "paper_title": "Early stopping and non-parametric regression: an optimal data-dependent stopping rule", "publication_date": "2014-01-01", "reason": "This paper develops an optimal data-dependent stopping rule for nonparametric regression, which is fundamental to the theoretical analysis of early stopping in this paper's proposed algorithms."}]}