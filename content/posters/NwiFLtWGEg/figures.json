[{"figure_path": "NwiFLtWGEg/figures/figures_4_1.jpg", "caption": "Figure 1: Illustration of Cheetah from DMControl, including its rendering, tree-structured morphology with the nodes being the limbs and the edges the joints, and the state features.", "description": "This figure shows the 2D Cheetah robot from the DeepMind Control Suite.  The left panel shows a rendered image of the robot. The right panel provides a schematic representation of the robot's morphology as a tree graph. Each node in the tree represents a rigid body (limb), and each edge represents a joint connecting two limbs. The figure also illustrates the kinematic features used to represent the robot's state, including joint positions and velocities, limb positions and velocities, and limb orientations.  These features are crucial to the proposed Euclidean data augmentation method in the paper, as they are based on the physical quantities that are directly observable by the agent and are amenable to Euclidean transformations such as rotations.", "section": "4.1 Kinematics of our continuous control tasks"}, {"figure_path": "NwiFLtWGEg/figures/figures_4_2.jpg", "caption": "Figure 2: SO\u011d(3) rotation in Cheetah.", "description": "This figure illustrates the SO\u011d(3) rotation applied to the Cheetah robot during data augmentation.  The top panel shows the original pose of the robot in the global coordinate frame (x, y, z). The gravity vector \u011f points downwards along the negative z-axis. The robot's displacement vector \ud835\udc51 points to the right.  The bottom panel shows the same robot after a rotation R\u03b1 about the z-axis (yaw), transforming the displacement vector to \ud835\udc51\u2032. Note that the gravity vector remains unchanged due to the restriction that the rotations are limited to those around the z-axis (yaw-only rotations). This rotation R\u03b1 \u2208 SO\u011d(3) is used for data augmentation in the paper.", "section": "4.2 SO(3)-data augmentation"}, {"figure_path": "NwiFLtWGEg/figures/figures_7_1.jpg", "caption": "Figure 3: Learning curves comparing data efficiency of our method again all baselines except for SEGNN on 9 out of the 10 tasks, excluding the task of Reacher_hard. The results involving SEGNN and Reacher_hard are deferred to Figure 4.", "description": "This figure presents learning curves for nine out of ten continuous control tasks from the DeepMind Control Suite.  The curves compare the performance of the proposed Euclidean data augmentation method against several baselines, including standard DDPG, SAC, DDPG with Gaussian noise, DDPG with random amplitude scaling, and the proposed method without data augmentation (paug=0%). For the proposed method, results are shown for the best performing paug values (percentage of augmentations performed in each batch of training data) and paug=0%. The figure demonstrates that the proposed method generally outperforms baselines in terms of data efficiency, especially on the more challenging tasks.", "section": "5 Experiments"}, {"figure_path": "NwiFLtWGEg/figures/figures_8_1.jpg", "caption": "Figure 3: Learning curves comparing data efficiency of our method again all baselines except for SEGNN on 9 out of the 10 tasks, excluding the task of Reacher_hard. The results involving SEGNN and Reacher_hard are deferred to Figure 4.", "description": "This figure compares the learning curves of different methods for nine out of ten continuous control tasks.  It shows the episode reward over time for several approaches: standard DDPG, SAC (Soft Actor-Critic), DDPG with Gaussian noise (GN), DDPG with random amplitude scaling (RAS), DDPG with the proposed Euclidean data augmentation method (Ours) with different augmentation ratios (0% and 25%), and DDPG with SEGNN (equivariant neural network).  The figure illustrates the improved data efficiency of the proposed method compared to baselines and highlights the impact of limb-based kinematic representation and Euclidean data augmentation on different task complexities.  The Reacher_hard task's results are shown separately in a later figure.", "section": "5 Experiments"}, {"figure_path": "NwiFLtWGEg/figures/figures_8_2.jpg", "caption": "Figure 3: Learning curves comparing data efficiency of our method again all baselines except for SEGNN on 9 out of the 10 tasks, excluding the task of Reacher_hard. The results involving SEGNN and Reacher_hard are deferred to Figure 4.", "description": "This figure compares the learning curves of different methods for solving 9 continuous control tasks from the DeepMind Control Suite.  The methods compared include standard DDPG and SAC baselines, DDPG with Gaussian noise and random amplitude scaling data augmentation, and the proposed method with limb-based kinematic features and Euclidean data augmentation.  The x-axis represents training time steps, and the y-axis represents the average episodic reward. The shaded regions indicate 95% confidence intervals.  The results for Reacher_hard task and comparison with SEGNN are presented separately in Figure 4.", "section": "5 Experiments"}, {"figure_path": "NwiFLtWGEg/figures/figures_9_1.jpg", "caption": "Figure 3: Learning curves comparing data efficiency of our method again all baselines except for SEGNN on 9 out of the 10 tasks, excluding the task of Reacher_hard. The results involving SEGNN and Reacher_hard are deferred to Figure 4.", "description": "This figure compares the learning curves of different methods on 9 out of 10 tasks (excluding Reacher_hard).  It shows the episode reward over time for several methods, including the proposed method with different augmentation rates (paug), standard DDPG, SAC, DDPG with Gaussian noise (GN), and DDPG with random amplitude scaling (RAS). The graph helps visualize the data efficiency and asymptotic performance of each approach.  Results for the Reacher_hard task and the SEGNN (equivariant neural network) baseline are presented separately in another figure.", "section": "5 Experiments"}, {"figure_path": "NwiFLtWGEg/figures/figures_16_1.jpg", "caption": "Figure 7: Learning curves of our method on the effect of  paug  on all 10 tasks.", "description": "This figure displays the learning curves for the proposed method across different values of  paug (proportion of data augmentation) on ten continuous control tasks. Each subplot represents a task, showing the average episodic reward over time. The various colors represent different values of  paug , illustrating how the degree of data augmentation impacts learning performance on each task. The results demonstrate the effectiveness of data augmentation on some tasks while showing a more complex relationship between performance and  paug on others.  This helps to understand the generalizability and optimal usage of the data augmentation.", "section": "B.1 Complete results on the effect of paug"}, {"figure_path": "NwiFLtWGEg/figures/figures_16_2.jpg", "caption": "Figure 3: Learning curves comparing data efficiency of our method again all baselines except for SEGNN on 9 out of the 10 tasks, excluding the task of Reacher_hard. The results involving SEGNN and Reacher_hard are deferred to Figure 4.", "description": "This figure compares the learning curves of different methods for 9 out of 10 continuous control tasks.  The y-axis represents the episodic reward, and the x-axis shows the timestep.  The methods compared include standard DDPG, SAC, DDPG with Gaussian noise augmentation, DDPG with random amplitude scaling augmentation, and the proposed method (DDPG with Euclidean data augmentation) with different augmentation rates (paug). The figure highlights the improved data efficiency of the proposed method, especially compared to the perturbation-based augmentation methods.", "section": "5 Experiments"}, {"figure_path": "NwiFLtWGEg/figures/figures_16_3.jpg", "caption": "Figure 4: Learning curves of data efficiency (top) and run time for 1M steps in total (bottom) for our method and all baselines on Reacher_hard.", "description": "This figure compares the performance of different methods on the Reacher_hard task. The top part shows the learning curves, illustrating how quickly each method learns to achieve a high reward.  The bottom part shows the computational cost (run time) for each method to process 1 million time steps.  It demonstrates the trade-off between data efficiency and computational cost, highlighting the efficiency of the proposed method.", "section": "5.2 Comparison with equivariant agent architecture"}]