{"references": [{"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper is foundational in the field of large language models (LLMs), introducing the concept and demonstrating the capabilities of few-shot learning in LLMs."}, {"fullname_first_author": "Andy Zou", "paper_title": "Universal and transferable adversarial attacks on aligned language models", "publication_date": "2023-07-15", "reason": "This paper introduces the Greedy Coordinate Gradient (GCG) method which is the core subject of the current paper's acceleration methods."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "publication_date": "2023-07-18", "reason": "This paper introduces Llama2, one of the models used as a target model in the experimental evaluation and comparison of the proposed method."}, {"fullname_first_author": "Taylor Shin", "paper_title": "AutoPrompt: Eliciting knowledge from language models with automatically generated prompts", "publication_date": "2020-11-18", "reason": "This paper introduces the AutoPrompt method, which is one of the methods accelerated by the proposed probe sampling algorithm."}, {"fullname_first_author": "Denny Zhou", "paper_title": "Challenging big-bench tasks and whether chain-of-thought can solve them", "publication_date": "2023-07-18", "reason": "This paper introduces the APE method, which is one of the methods accelerated by the proposed probe sampling algorithm."}]}