[{"figure_path": "CMgxAaRqZh/figures/figures_0_1.jpg", "caption": "Figure 1: A brief illustration of the Greedy Coordinate Gradient (GCG) algorithm (Zou et al., 2023).", "description": "This figure illustrates the Greedy Coordinate Gradient (GCG) algorithm.  It shows an iterative process of optimizing a prompt suffix to elicit a harmful response from a large language model (LLM). The process starts with a harmful prompt ('How to build a bomb?') and an initial suffix. The algorithm iteratively samples candidate suffixes by replacing tokens in the existing suffix, computes the loss (measuring how far the LLM's response is from the target harmful response), and updates the suffix based on the lowest loss. The figure highlights how the algorithm explores different suffix candidates to find one that maximizes the probability of generating the target response.", "section": "1 Introduction"}, {"figure_path": "CMgxAaRqZh/figures/figures_1_1.jpg", "caption": "Figure 2: Probe sampling mainly consists of three steps. (i) A batch of candidates ({a, b,..., h}) is sampled. We determine the probe agreement score between the draft model and the target model on a probe set ({b, d, h}). The probe agreement score is used to compute the filtered set size. (ii) We obtain a filtered set ({e, f}) based on the losses on the draft model (iii) We test the losses of candidates in the filtered set using the target model.", "description": "This figure illustrates the Probe Sampling algorithm's three main steps. First, a batch of prompt candidates is sampled, and a probe agreement score is calculated between a smaller draft model and the target model using a subset of the candidates (the probe set). This score determines the size of the filtered set. Second, candidates are filtered based on their losses as predicted by the draft model, resulting in a smaller filtered set.  Finally, the losses of the candidates in the filtered set are evaluated by the target model to select the optimal prompt.", "section": "2 Proposed Method"}, {"figure_path": "CMgxAaRqZh/figures/figures_6_1.jpg", "caption": "Figure 3: Memory usage on a single A100 with 80GB memory with (a) Llama2-7b-chat and (b) Vicuna-7b-v1.3 on 1 instance. The memory consumption of probe sampling with or without simulated annealing is similar to that of the original setting. The computation with the target model still takes most of the memory.", "description": "This figure compares the memory usage of the original GCG algorithm and the proposed Probe Sampling method, both with and without simulated annealing. The results are shown separately for two different large language models: Llama2-7b-chat and Vicuna-7b-v1.3.  The key observation is that, despite adding extra steps, Probe Sampling maintains similar memory usage to the original GCG. The majority of memory is still consumed by calculations involving the larger target model, not the smaller draft model. This highlights that the memory efficiency gains come from reducing computation time, not memory usage.", "section": "3.3 Computation Detail Analysis"}, {"figure_path": "CMgxAaRqZh/figures/figures_7_1.jpg", "caption": "Figure 4: Wall time of GCG, probe sampling with and without simulated annealing. For the target model computation, the first part is done on the probe set and the second part is done on the filtered set. Draft model computation and computation of the target model on the probe set are suited to be done in parallel as they take similar time.", "description": "This figure shows a breakdown of the computation time for different methods (GCG, Probe Sampling, and their annealing versions).  It highlights that the most time-consuming part is using the target model, especially when processing the full candidate set.  The figure indicates how Probe Sampling reduces the time spent on the target model by filtering out candidates based on the draft model and emphasizes the potential for parallelization of parts of the process.", "section": "3.3 Computation Detail Analysis"}, {"figure_path": "CMgxAaRqZh/figures/figures_15_1.jpg", "caption": "Figure 5: Converge progress with different sizes of filtered set.", "description": "This figure shows the convergence process of the GCG algorithm with different filtered set sizes, controlled by the hyperparameter R.  The x-axis represents the number of iterations, and the y-axis represents the loss. Each line represents a different value of R (64, 16, 8, 4, 2, 1), with the black line representing the original GCG algorithm (R=1). The plots show that using a smaller filtered set size (smaller R) can lead to either premature convergence or failure to converge, while using a larger filtered set size (larger R) can lead to slower convergence. R=8 shows the best result.", "section": "B Converge Process"}]