[{"type": "text", "text": "Segment, Shuffle, and Stitch: A Simple Layer for Improving Time-Series Representations ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Shivam Grover Amin Jalali Ali Etemad Queen\u2019s University, Canada {shivam.grover, amin.jalali, ali.etemad}@queensu.ca ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Existing approaches for learning representations of time-series keep the temporal arrangement of the time-steps intact with the presumption that the original order is the most optimal for learning. However, non-adjacent sections of real-world time-series may have strong dependencies. Accordingly, we raise the question: Is there an alternative arrangement for time-series which could enable more effective representation learning? To address this, we propose a simple plug-andplay neural network layer called Segment, Shuffle, and Stitch (S3) designed to improve representation learning in time-series models. S3 works by creating nonoverlapping segments from the original sequence and shuffilng them in a learned manner that is optimal for the task at hand. It then re-attaches the shuffled segments back together and performs a learned weighted sum with the original input to capture both the newly shuffled sequence along with the original sequence. S3 is modular and can be stacked to achieve different levels of granularity, and can be added to many forms of neural architectures including CNNs or Transformers with negligible computation overhead. Through extensive experiments on several datasets and state-of-the-art baselines, we show that incorporating S3 results in significant improvements for the tasks of time-series classification, forecasting, and anomaly detection, improving performance on certain datasets by up to $68\\%$ . We also show that S3 makes the learning more stable with a smoother training loss curve and loss landscape compared to the original baseline. The code is available at https://github.com/shivam-grover/S3-TimeSeries. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Time-series data serve an important role across diverse domains, including but not limited to health analytics [1, 2, 3], human-computer interaction [4, 5], human activity recognition [6, 7], climate analysis [8, 9], energy consumption [10, 11, 12], traffic management [13, 14], financial markets [15], and others. The pervasive nature of time-series data has resulted in considerable interest among researchers, leading to the development of a variety of deep learning solutions for classification [16, 17, 18] and forecasting tasks [10, 19, 20]. Neural architectures such as convolutional networks [16, 21], recurrent networks [22, 23], and Transformers [10, 20] can capture the essential spatial and temporal information from time-series. Notably, these approaches have frequently outperformed traditional approaches, including Dynamic Time Warping [24], Bag of Stochastic Frontier Analysis Symbols [25], and the Collective of Transformation-Based Ensembles [26] in various scenarios. ", "page_idx": 0}, {"type": "text", "text": "While both traditional machine learning and deep learning solutions aim to extract effective goalrelated representations prior to classification or forecasting, the general approach is to keep the original temporal arrangement of the time-steps in the time-series intact, with the presumption that the original order is the most optimal. Moreover, most existing models do not have explicit mechanisms to explore the inter-relations between distant segments within each time-series, which may in fact have strong dependencies despite their lack of proximity. For example, CNN-based models for time-series learning generally utilize fixed convolutional filters and receptive fields, causing them to only capture patterns within a limited temporal window [27, 28]. As a result, when faced with time-series where important patterns or correlations span across longer time windows, these models often struggle to capture this information effectively [28]. Dilated convolutional neural networks partially solve this by increasing the receptive field through the dilation rate. However, they are still practically limited by their inherent architectures, as their receptive fields rely on the number of layers, which may not be large enough to fully capture the long-range dependencies and can lead to vanishing gradients as more layers are added [29]. Similarly, the out-of-the-box effectiveness of Transformers [30] in capturing long-term dependencies highly depends on a variety of factors such as sequence length, positional encoding, and tokenization strategies. Accordingly, we ask a simple question: Is there a better arrangement for the time-series that would enable more effective representation learning considering the classification/forecasting task at hand? ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this paper, we introduce a simple and plug-and-play network layer called Segment, Shuffle, and Stitch, or S3 for short, designed to enhance time-series representation learning. As the name suggests, S3 operates by segmenting the time-series into several segments, shuffilng these segments in the most optimal order controlled by learned shuffling parameters, and then stitching the shuffled segments. In addition to this, our module integrates the original time-series through a learned weighted sum operation with the shuffled version to also preserve the key information in the original order. S3 acts as a modular mechanism intended to seamlessly integrate with any time-series model and as we will demonstrate experimentally, results in a smoother training procedure and loss landscape. Since S3 is trained along with the backbone network, the shuffilng parameter is updated in a goal-centric manner, adapting to the characteristics of the data and the backbone model to better capture the temporal dynamics. Finally, S3 can be stacked to create a more fine-grained shuffling with higher levels of granularity, has very few hyper-parameters to tune, and has negligible computation overhead. ", "page_idx": 1}, {"type": "text", "text": "For evaluation, we integrate S3 in a variety of neural architectures including CNN-based and Transformer-based models, and evaluate performance across various classification, univariate forecasting, and multivariate forecasting datasets, observing that S3 results in substantial improvements when integrated into state-of-the-art models. Specifically, the results demonstrate that integrating S3 into state-of-the-art methods can improve performance by up to $39.59\\%$ for classification, and by up to $68.71\\%$ and $51.22\\%$ for univariate and multivariate forecasting, respectively. We perform detailed ablation and sensitivity studies to analyze different components of our proposed S3 layer. ", "page_idx": 1}, {"type": "text", "text": "Our contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose S3, a simple and modular network layer that can be plugged into existing neural architectures to improve time-series representation learning. By dynamically segmenting and shuffling the input time-series across the temporal dimension, S3 helps the model perform more effective task-centric representation learning.   \n\u2022 By stacking multiple instances of S3, the model can perform shuffilng at different granularity levels. Our proposed layer has very few hyperparameters and negligible added computational cost.   \n\u2022 Rigorous experiments on various benchmark time-series datasets across both classification and forecasting tasks demonstrate that by incorporating S3 into existing state-of-the-art models, we improve performance significantly. Experiments also show that by adding our proposed network layer, more stable training is achieved.   \n\u2022 We make our code public to contribute to the field of time-series representation learning and enable fast and accurate reproducibility. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Deep learning architectures have recently made significant progress in the area of time-series representation learning. In the category of convolution-based methods, DSN [16] introduces dynamic sparse connections to cover different receptive fields in convolution layers for time-series classification. In another convolution-based approach, SCINet [21] captures temporal features by partitioning each time sequence into two subsequences at each level to effectively model the complex temporal dynamics within hierarchical time-series data. ", "page_idx": 1}, {"type": "text", "text": "Transformer-based approaches are another category of solutions that have drawn considerable attention. Informer [10] improves the capabilities of the vanilla Transformer on long input sequences by introducing self-attention distillation using ProbSparse which is based on Kullback-Leibler divergence, and a generative style decoder. Autoformer [20] introduces decomposition blocks with an auto-correlation mechanism that allows progressive decomposition of the data. ContiFormer [31] integrates the continuous dynamics of Neural Ordinary Differential Equations with the attention mechanism of Transformers. PatchTST [32] enhances time-series forecasting by leveraging patching techniques to split long input sequences into smaller patches, which are then processed by a Transformer to capture long-term dependencies efficiently. Finally, [33] investigated the effectiveness of Transformers in dealing with long sequences for forecasting and highlighted the challenges Transformers encounter in this regard. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Recently, foundation models for time-series data, which are large-scale, pre-trained models designed to capture complex temporal patterns, have gained popularity. A prominent example is Moment [34], which pre-trains a Transformer encoder using a univariate setting on a large and diverse collection of data called \u2018Pile\u2019. It leverages a masked reconstruction method and is capable of performing various downstream forecasting tasks after fine-tuning, demonstrating strong adaptability and forecasting accuracy. ", "page_idx": 2}, {"type": "text", "text": "Contrastive methods have recently demonstrated state-of-the-art performances in time-series learning. TS2Vec [17] employs unsupervised hierarchical contrastive learning across augmented contextual views, capturing robust representations at various semantic levels. InfoTS [35] uses informationaware augmentations for contrastive learning which dynamically chooses the optimal augmentations. To address long-term forecasting, [28] introduces a contrastive approach that incorporates global autocorrelation alongside a decomposition network. Finally, SoftCLT [36] is a recent contrastive approach that uses the distances between time-series samples (instance-wise contrastive) along with the difference in timestamps (temporal contrastive), to capture the correlations between adjacent samples and improve representations. ", "page_idx": 2}, {"type": "text", "text": "Given the existence of seasonal and trend information in time-series, disentanglement is an approach that has been widely used across both classical machine learning [37, 38, 39] and deep learning solutions [40, 19]. For instance, CoST [40] attempts to capture periodic patterns using frequency domain contrastive loss to disentangle seasonal-trend representations in both time and frequency domains. Similarly, LaST [19] recently employed variational inference to learn and disentangle latent seasonal and trend components within time-series data. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Problem definition. Given a set of $N$ timeseries instances as $\\{\\mathbf{x}_{i}\\}_{i=1}^{N}$ , where $\\mathbf{x}_{i}\\in\\mathbb{R}^{T\\times C}$ has a length of $T$ and $C$ channels, the goal is to optimally rearrange the segments of $\\mathbf{x}_{i}$ and form a new sequence $\\mathbf{x}_{i}^{\\prime}$ to better capture the underlying temporal relationships and dependencies within the time-series, which would consequently lead to improved representations given the target task. ", "page_idx": 2}, {"type": "text", "text": "Proposed mechanism. We propose S3, a simple neural network component for addressing the aforementioned problem in three steps, as the name suggests, Segment, Shuffle, and Stitch, described below (see Figure 1). ", "page_idx": 2}, {"type": "text", "text": "The Segment module splits the input sequence $\\mathbf{x}_{i}$ into $n$ non-overlapping segments, each containing $\\tau$ time-steps, where $\\tau~=~T/n$ . The set of segments can be represented by $\\mathbf{S}_{i}=\\{\\mathbf{s}_{i,1},\\mathbf{s}_{i,2},\\allowbreak\\dot{\\mathbf{\\xi}}_{\\cdot\\cdot\\cdot},\\mathbf{s}_{i,n}\\}\\in\\mathbb{R}^{(\\tau\\times C)\\times n}$ where si,j = xi[(j \u22121)\u03c4 : j\u03c4] and si,j \u2208R\u03c4\u00d7C. ", "page_idx": 2}, {"type": "image", "img_path": "zm1LcgRpHm/tmp/7391588dc99ecc9f6e663c8b523d1b9a070728031a239941e3d4095cdceda1fe.jpg", "img_caption": ["Figure 1: Stacking S3 layers. In this depiction, we use $n=2$ , $\\phi=3$ , and $\\theta=2$ as the hyperparameters. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "The segments are then fed into the Shuffle module, which uses a shuffle vector $\\mathbf{P}=\\{\\mathfrak{p}_{1},\\mathfrak{p}_{2},\\dots,\\mathfrak{p}_{n}\\}\\in\\mathbb{R}^{n}$ to rearrange the segments in the optimal order for the task at hand. Each shuffling parameter ${\\mathrm{p}}_{j}$ in $\\mathbf{P}$ corresponds to a segment $\\mathbf{s}_{i,j}$ in $\\mathbf{S}_{i}$ . $\\mathbf{P}$ is essentially a set of learnable weights optimized through the network\u2019s learning process, which controls the position and priority of the segment in the reordered sequence. The shuffling process is quite simple and intuitive: the higher the value of ${\\mathfrak{p}}_{j}$ , the higher the priority of segment $\\mathbf{s}_{i,j}$ is in the shuffled sequence. The shuffled sequence $\\mathbf{S}_{i}^{\\mathrm{sl}}$ huffledcan be represented as ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{S}_{i}^{\\mathrm{shuffled}}=\\mathbf{Sort}(\\mathbf{S}_{i},\\mathrm{key}=\\mathbf{P}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the segments in $\\mathbf{S}_{i}$ are sorted according to the values in $\\mathbf{P}$ . Permuting $\\mathbf{S}_{i}$ based on the sorted order of $\\mathbf{P}$ is not differentiable by default, because it involves discrete operations and introduces discontinuities [41]. Soft sorting methods such as [42, 41, 43] approximate the sorted order by assigning probabilities that reflect how much larger each element is compared to others. While this approximation is differentiable in nature, it may introduce noise and inaccuracies, while making the sorting non-intuitive. To achieve differentiable sorting and permutation that are as accurate and intuitive as traditional methods, we introduce a few intermediate steps. These steps create a path for gradients to flow through the shuffling parameters $\\mathbf{P}$ while performing discrete permutations on $\\mathbf{S_{i}}$ based on the sorted order of $\\mathbf{P}$ . We first obtain the indices that sort the elements of $\\mathbf{P}$ using $\\pmb{\\sigma}=\\mathrm{Argsort}(\\mathbf{P})$ . We have a list of tensors $\\mathbf{S}=[\\mathbf{s}_{1},\\mathbf{s}_{2},\\mathbf{s}_{3},...\\mathbf{s}_{n}]$ (for simplicity, we skip the index $i$ ) that we aim to reorder based on the list of indices $\\pmb{\\sigma}=[\\sigma_{1},\\sigma_{2},....\\sigma_{n}]$ in a differentiable way. We then create a $(\\tau\\times C)\\times n\\times n$ matrix $\\mathbf{U}$ , which we populate by repeating each $\\mathbf{s}_{i}$ , $n$ times. Next, we form an $n\\times n$ matrix $\\pmb{\\Omega}$ where each row $j$ has a single non-zero element at position $k=\\sigma_{j}$ which is $p_{k}$ . We convert $\\pmb{\\Omega}$ to a binary matrix $\\tilde{\\Omega}$ by scaling each non-zero element to 1 using a scaling factor $\\bar{\\frac{1}{\\Omega_{j,k}}}$ . This process creates a path for the gradients to flow through $\\mathbf{P}$ during backpropagation. ", "page_idx": 3}, {"type": "text", "text": "By performing the Hadamard product between $\\mathbf{U}$ and \u02dc\u2126, we obtain a matrix $\\mathbf{V}$ where each row $j$ has one non-zero element $k$ equals to $\\mathbf{s}_{k}$ . Finally, by summing along the final dimension and transposing the outcome, we obtain the final shuffled matrix of size $\\mathbf{S}_{i}^{\\mathrm{shuffled}}\\,\\in\\,\\mathbb{R}^{\\tau\\times C\\times n}$ . To better illustrate, we show a simple example with $\\mathbf{S}=[\\mathbf{s}_{1},\\mathbf{s}_{2},\\mathbf{s}_{3},\\mathbf{s}_{4}]$ and a given permutation $\\pmb{\\sigma}=[3,4,1,2]$ . We calculate $\\mathbf{V}$ as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{V}=\\mathbf{U}\\odot\\tilde{\\Omega}=\\left[\\begin{array}{l l l l}{\\mathbf{s}_{1}}&{\\mathbf{s}_{2}}&{\\mathbf{s}_{3}}&{\\mathbf{s}_{4}}\\\\ {\\mathbf{s}_{1}}&{\\mathbf{s}_{2}}&{\\mathbf{s}_{3}}&{\\mathbf{s}_{4}}\\\\ {\\mathbf{s}_{1}}&{\\mathbf{s}_{2}}&{\\mathbf{s}_{3}}&{\\mathbf{s}_{4}}\\\\ {\\mathbf{s}_{1}}&{\\mathbf{s}_{2}}&{\\mathbf{s}_{3}}&{\\mathbf{s}_{4}}\\end{array}\\right]\\odot\\left[\\begin{array}{l l l l}{0}&{0}&{1}&{0}\\\\ {0}&{0}&{0}&{1}\\\\ {1}&{0}&{0}&{0}\\\\ {0}&{1}&{0}&{0}\\end{array}\\right]=\\left[\\begin{array}{l l l l}{0}&{0}&{\\mathbf{s}_{3}}&{0}\\\\ {0}&{0}&{0}&{\\mathbf{s}_{4}}\\\\ {\\mathbf{s}_{1}}&{0}&{0}&{0}\\\\ {0}&{\\mathbf{s}_{2}}&{0}&{0}\\end{array}\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and $\\mathbf{S}_{i}^{\\mathrm{shuffied}}$ is obtained by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{S}_{i}^{\\mathrm{shuffled}}=(\\sum_{k=1}^{n}(\\mathbf{V}_{j,k}))^{T}=[\\mathbf{s}_{3}\\quad\\mathbf{s}_{4}\\quad\\mathbf{s}_{1}\\quad\\mathbf{s}_{2}]\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We previously assumed that the set of shuffling parameters $\\mathbf{P}$ is one-dimensional, containing $n$ scalar elements, each corresponding to one of the segments. By employing a higher-dimensional $\\mathbf{P}$ , we can introduce additional parameters that enable the model to capture complex representations that a single-dimensional $\\mathbf{P}$ could struggle with. Therefore, we introduce a hyperparameter $\\lambda$ to determine the dimensionality of the vector $\\mathbf{P}$ . When $\\lambda=m$ , the size of $\\mathbf{P}$ becomes $n\\times n\\times\\cdots\\times n$ (repeated $m$ times). We then perform a summation of $\\mathbf{P}$ over the first $m-1$ dimensions to obtain a one-dimensional vector. Mathematically, this is represented as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\tilde{\\bf P}=\\sum_{d_{1}=1}^{n}\\sum_{d_{2}=1}^{n}\\cdot\\cdot\\cdot\\sum_{d_{m-1}=1}^{n}{\\bf P}_{d_{1},d_{2},\\ldots,d_{m-1}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This results in a one-dimensional matrix $\\tilde{\\mathbf{P}}$ , which we then use to compute the permutation indices $\\pmb{\\sigma}=\\mathrm{Argsort}(\\tilde{\\mathbf{P}})$ . This approach allows us to increase the number of shuffling parameters, thereby capturing more complex dependencies within the time-series data, without affecting the sorting operations. ", "page_idx": 3}, {"type": "text", "text": "In the final step, the Stitch module concatenates the shuffled segments $\\mathbf{S}_{i}^{\\mathrm{shuffied}}$ to create a single shuffled sequence $\\tilde{\\mathbf{x}}_{i}\\in\\mathbb{R}^{T\\times C}$ as $\\tilde{\\mathbf{x}}_{i}=\\mathbf{Concat}(\\mathbf{S}_{i}^{\\mathrm{shuffled}})$ . To retain the information present in the original order along with the newly generated shuffled sequence, we perform a weighted sum between $\\mathbf{x}_{i}$ and $\\tilde{\\mathbf{x}}_{i}$ with learnable weights $\\mathbf{w}_{1}$ and ${\\bf w}_{2}$ optimized through the training of the main network. For practical convenience, we can use a simple learnable Conv1D or MLP layer taking $\\mathbf{x}_{i}$ and $\\tilde{\\mathbf{x}}_{i}$ as inputs and generating the final time-series output $\\mathbf{x}_{i}^{\\prime}\\in\\mathbb{R}^{T\\times C}$ . ", "page_idx": 3}, {"type": "text", "text": "Stacking S3 layers. Considering S3 as a modular layer, we can stack them sequentially within a neural architecture. Let\u2019s define $\\phi$ as a hyperparameter that determines the number of S3 layers. For simplicity and to avoid defining a separate segment hyperparameter for each S3 layer, we define $\\theta$ which acts as a multiplier for the number of segments in subsequent layers as ", "page_idx": 4}, {"type": "equation", "text": "$$\nn_{\\ell}=n\\times\\theta^{\\ell-1}\\quad\\mathrm{for}\\;\\ell=1,2,\\ldots,\\phi,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $n$ is the number of segments in the first S3 layer. When multiple S3 layers are stacked, each layer $\\ell$ from 1 to $\\phi$ will segment and shuffle its input based on the output from the previous layer. We can formally represent the output of each layer $\\ell$ as $\\mathbf{x^{\\prime}}_{\\ell}$ by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{x}_{\\ell}^{\\prime}=\\mathsf{S t}\\mathsf{i t c h}(\\mathsf{S h u f f\\,1}\\mathsf{e}(\\mathsf{S e g m e n t}(\\mathbf{x}_{\\ell-1}^{\\prime},n_{\\ell}),\\mathbf{P}_{\\ell}),(\\mathbf{w}_{\\ell,1},\\mathbf{w}_{\\ell,2})),\\mathrm{for}\\;\\ell=1,2,\\ldots,\\phi,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{x}_{\\mathrm{0}}^{\\prime}$ is the original time-series $\\mathbf{x}$ , $\\mathbf{P}_{\\ell}$ is the set of shuffling parameters for layer $\\ell$ , and $(\\mathbf{w}_{\\ell,1}$ , $\\mathbf{w}_{\\ell,2})$ are the learnable weights for the sum operation between the concatenation of the shuffled segments and the original input, at layer $\\ell$ . Figure 1 presents an example of three S3 layers $\\mathit{\\Theta}(\\phi=3)$ ) applied to a time-series with $n=2$ and $\\theta=2$ . ", "page_idx": 4}, {"type": "text", "text": "All the $\\mathbf{P}_{\\ell}$ values are updated along with the model parameters, and there is no intermediate loss for any of the S3 layers. This ensures that the S3 layers are trained according to the specific task and the baseline. In cases where the length of input sequence $\\mathbf{x}$ is not divisible by the number of segments $n$ , we resort to truncating the first $T$ mod $n$ time-steps from the input sequence. In order to ensure that no data is lost and the input and output shapes are the same, we later add the truncated samples back at the beginning of the output of the final S3 layer. ", "page_idx": 4}, {"type": "text", "text": "4 Experiment setup ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Evaluation protocol. For our experiments, we integrate S3 into existing state-of-the-art models for both time-series classification and forecasting. We first train and evaluate each model adhering strictly to their original setups and experimental protocols. We then integrate S3 at the beginning of the model, and train and evaluate it with the same setups and protocols as the original models. This meticulous approach ensures that any observed deviation in performance can be fairly attributed to the integration of S3. For classification, we measure the improvement as the percentage difference (Diff.) in accuracy resulting from S3, calculated as $\\left(A c c_{\\mathrm{Baseline+S3}}-A c c_{\\mathrm{Baseline}}\\right)\\ /A c c_{\\mathrm{Baseline}}$ . For forecasting, since lower MSE is better, we use $\\left(M S E_{\\mathrm{Baseline}}-M S E_{\\mathrm{Baseline}+\\mathrm{S3}}\\right)/M S E_{\\mathrm{Baseline}}.$ A similar equation is used for measuring the percentage difference in MAE. ", "page_idx": 4}, {"type": "text", "text": "Classification datasets. For classification we use the following datasets: (1) The UCR archive [44] which consists of 128 univariate datasets, (2) the UEA archive [45] which consists of 30 multivariate datasets, and (3) three multivariate datasets namely EEG, EEG2, and HAR from the UCI archive [46]. For our experiments with pre-trained foundation model in Section 5, we also use the PTB-XL [47] dataset. The train/test splits for all classification datasets are as provided in the original papers. ", "page_idx": 4}, {"type": "text", "text": "Forecasting datasets. For forecasting, we use the following datasets: (1) both the univariate and multivariate versions of three ETT datasets [10], namely ETTh1 and ETTh2 recorded hourly, and ETTm1 recorded at every 15 minutes, (2) both the univariate and multivariate versions of the Electricity dataset [46], and (3) the multivariate version of the Weather dataset [48]. ", "page_idx": 4}, {"type": "text", "text": "Anomaly detection datasets. We employ the widely used Yahoo [49] and KPI [50] datasets for anomaly detection tasks. The Yahoo dataset consists of 367 time-series sampled hourly, each with labeled anomaly points, while the KPI dataset contains 58 minutely sampled KPI curves from various internet companies. Our experiments are performed under normal settings, as outlined in [17, 36]. ", "page_idx": 4}, {"type": "text", "text": "Baselines. We select baselines from a variety of different time-series learning approaches. Specifically, for classification, we use four state-of-the-art baseline methods, SoftCLT [36], TS2Vec [17], DSN [16], and InfoTS [35]. For forecasting, we use five state-of-the-art baseline methods, TS2Vec [17], LaST [19], Informer [10], PatchTST [32] and CoST [40]. For anomaly detection, we use two stateof-the-art baseline methods, SoftCLT [36] and TS2Vec [17]. Additionally, we use MOMENT [34] for experiments with foundation model. For each baseline method, we integrate one to three layers of S3 at the input level of the model and compare its performance with that of the original model. ", "page_idx": 4}, {"type": "text", "text": "Implementation details. All implementation details match those of the baselines. We used the exact hyperparameters of the baselines according to the original papers when they were specified in the papers or when the code was available; alternatively, when the hyperparameters were not exactly specified in the paper or in the code, we tried to maximize performance with our own search for the optimum hyperparameters. Additionally, for experiments involving PatchTST and LaST with the Electricity dataset, we use a batch size of 8 due to memory constraints. Accordingly, some baseline results may slightly differ from those available in the original papers. Note that deviations in baseline results affect both the baseline model and baseline $+S3$ . For the weighted sum operation in S3, we use Conv1D. Our code is implemented with PyTorch, and our experiments are conducted on a single NVIDIA Quadro RTX 6000 GPU. We release the code at: https://github.com/shivam-grover/S3- TimeSeries. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "5 Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Classification. The results of our experiments on time-series classification are presented in Table 1. In this table we observe that the addition of S3 results in substantial improvements across all baselines for all univariate and multivariate datasets. For UCR, UEA, EEG, EEG2, and HAR, we achieve average improvements of $3.89\\%$ , $5.25\\%$ , $32.03\\%$ , $9.675\\%$ , and $5.18\\%$ respectively over all models. The full classification results on UCR and UEA datasets for all baselines with and without S3 are mentioned in Appendix (Table A1 and A2). These results highlight the efficacy of S3 in improving a variety of different classification methods over a diverse set of datasets despite negligible added computational complexity (we provide a detailed discussion on computation overhead later in this section). In addition to quantitative metrics, we also visualize the t-SNE plots for several datasets in ", "page_idx": 5}, {"type": "table", "img_path": "zm1LcgRpHm/tmp/864f8bc70d446dfc3703e6c72f2b1f8e5e1fd4ea7e1c5727c0a39f53d33ca280.jpg", "table_caption": ["Table 1: Comparison with baselines on timeseries classification "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 2 and Appendix A1, which show that adding S3 results in representations with better class separability in the latent space. ", "page_idx": 5}, {"type": "text", "text": "Forecasting. Table 2 presents a comprehensive overview of the results for univariate forecasting on different datasets and horizons (H), with and without the incorporation of S3. We observe that S3 consistently leads to improvements for all baseline methods across all datasets, with an average improvement in MSE and MAE of $4.58\\%$ and $4.20\\%$ for TS2Vec, $4.02\\%$ and $3.01\\%$ for LaST, $16.69\\%$ and $8.19\\%$ for Informer, $1.64\\%$ and $1.20\\%$ for PatchTST, and $3.01\\%$ and $2.62\\%$ for CoST. Figure 3 shows two forecasting outputs for Informer with and without S3, on two different horizon lengths for a sample from the ETTh1 dataset. In both cases, S3 improves the ability of the baseline Informer to generate time-series samples that better align with the ground truth. Similarly, Table 3 presents the results of multivariate forecasting. Consistent with univariate forecasting, S3 significantly enhances the performance of the baseline on multivariate forecasting and achieves average improvements in MSE and MAE of $13.71\\%$ and $7.78\\%$ for TS2Vec, $9.88\\%$ and $5.45\\%$ for LaST, $25.13\\%$ and $14.82\\%$ for Informer, $1.62\\%$ and $1.17\\%$ for PatchTST, and $4.41\\%$ and $1.97\\%$ for CoST. ", "page_idx": 5}, {"type": "text", "text": "Anomaly detection. Table 4 presents the results for anomaly detection on Yahoo and KPI datasets. The anomaly score is computed as the L1 distance between two encoded representations derived from masked and unmasked inputs, following the methodology described in previous studies [17, 36]. We observe that the proposed S3 layer enhances the performance in terms of F1 for both datasets under the normal settings, with scores of 0.7498 and 0.6892, respectively. ", "page_idx": 5}, {"type": "text", "text": "Loss behaviour. We make interesting observations while training the baseline models as well as the baseline $+S3$ variants. First, we observe that the training loss vs. epochs for baseline $\\mathrel{\\mathrel{+}}\\mathbf{S}3$ variants generally converge faster than the original baselines. See Figure 4 where we demonstrate examples of this behavior. Second, we observe that the training loss curves for baseline $+S3$ are generally much smoother than the original baselines. This can again be observed in Figure 4. Additionally, we measure the standard deviations of the loss curves for SoftCLT with and without S3 on all the UCR datasets and measure an average reduction in standard deviation of $36.66\\%$ . The detailed values for the standard deviations of the loss curves are presented in Appendix A4. Lastly, according to [51], we investigate the loss landscape of the baselines and observe that the addition of S3 generally results in a smoother loss landscape with fewer local minima (Figure 5). Additional examples are provided in Appendix A2. ", "page_idx": 5}, {"type": "table", "img_path": "zm1LcgRpHm/tmp/acc1f184742c1b7afc205d97ae0c66f7311c993330bdffa05d943d31c5dcf225.jpg", "table_caption": ["Table 2: Univariate forecasting results for different baselines with and without S3. "], "table_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "", "img_caption": ["Figure 2: t-SNE visualizations of the learned representations of TS2Vec and $\\mathrm{TS}2\\mathrm{Vec}{+}\\mathrm{S}3$ for 4 randomly chosen test sets. Different colors represent different classes. It can be seen that representations belonging to different classes are more separable after adding S3. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Ablation. We perform ablation studies to investigate the impact of each of the key modules of S3 and answer three questions: (1) Do we need the Segment module to divide the input into multiple segments, or will shuffilng all the time-steps individually work just as well? (2) Do we need learned Shuffling parameters $({\\bf P})$ to determine the optimal permutation of the time-series, or will random permutations work just as well? (3) Do we need the Stitch module to apply a learned weighted sum of the original sequence with the shuffled sequence, or will the shuffled sequences alone work just as well? Table 7 shows the results of three ablation studies where we observe that the removal of each component of S3 results in a decline in performance. ", "page_idx": 6}, {"type": "text", "text": "Foundation models. In order to evaluate the impact of S3 on pre-trained foundation model, we integrate S3 at the beginning of the pre-trained MOMENT encoder [34]. We use linear probing for fine-tuning, and to ensure a fair comparison, we strictly follow the setup and experimental protocols outlined in the original paper. In this process, the encoder is kept frozen, and only S3 and the final linear head are fine-tuned together for the specific task and dataset. We fine-tune and evaluate the models for classification on PTB-XL [47] and the Crop dataset from the UCR archive [44], and the ETTh1 and ETTh2 datasets [10] for forecasting. The results for classification are presented in Table 5, where we observe that upon adding S3, both the test loss and accuracy see significant improvements of up to $13.31\\%$ and $5.24\\%$ , respectively. Table A3 (Appendix) shows the forecasting results, where incorporating S3 consistently improves performance. On the ETTh1 dataset, it yields average improvements of $2.12\\%$ in MSE and $2.59\\%$ in MAE, and for ETTh2, the improvements are $4.96\\%$ in MSE and $3.56\\%$ in MAE. ", "page_idx": 6}, {"type": "table", "img_path": "zm1LcgRpHm/tmp/8730702e11828087a55b99be83731d1b7e3ba03b76af0b44f89a405dfe83b26f.jpg", "table_caption": ["Table 3: Multivariate forecasting results for different baselines with and without S3 "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "zm1LcgRpHm/tmp/2fca77f0690727739cbc4e707be29da5ad75738d1caf8772172d79109858fd0a.jpg", "table_caption": ["Table 4: Anomaly Detection results. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "zm1LcgRpHm/tmp/0a7ccd37010743c8b49bbde78fe2b0accaf8210998569b7d0e3b7f35c5e19bca.jpg", "table_caption": ["Table 5: Results for time-series classification with pre-trained MOMENT [34]. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "zm1LcgRpHm/tmp/d8a9d90a4ccafad3ebdbe7c2a29e0246b0920cc0bfed51c1a02ba4efb5c97ea8.jpg", "img_caption": ["Figure 3: Forecasting output by Informer and Informer $+S3$ for a sample from ETTh1. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "zm1LcgRpHm/tmp/f077084a9711b1c435057332b29a90bdae8ec5ae046f8c8b56ca53edc0a41e8e.jpg", "img_caption": ["Figure 5: Visualisation of the loss landscape for the Beef dataset from the UCR archive. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "zm1LcgRpHm/tmp/5ea256389f76c4f895ae113df7934d180bfe5155f83f8de08fd163a32c60f408.jpg", "img_caption": ["Figure 4: Training loss against iterations on two sample datasets from the UCR archive. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "zm1LcgRpHm/tmp/4953db7cda2e400c714e4582c30026861eb6396fbd46093e55602b7cb2e6f459.jpg", "img_caption": ["Figure 6: Classification accuracy vs. number of segments for two sample datasets from UCR with different input sequence lengths $t$ . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Hyperparameters. Next, we investigate the impact of the number of segments $n$ on performance. Considering all the experiments, we observe that no general rule of thumb can be advised for $n$ , as its optimum value is naturally highly dependent on factors such as dataset complexity, length, baselines, and others. We present two examples in Figure 6 and more in Appendix A3 where we plot the accuracy vs. number of segments for datasets of different lengths from UCR. Here we only take into account the layer with the largest number of segments. As for the other hyperparameters, the number of S3 layers $\\phi$ has been set between 1, 2, or 3 across all experiments in this paper. $\\theta$ , the multiplier for the number of segments in stacked S3 layers, has been set to 0.5, 1, or 2 across all our experiments. And finally, $\\lambda$ has only been set to 1, 2, or 3. The ranges of values for all the hyperparameters used in this work are presented in Table 6. While optimizing hyperparameters is important, similar to any other form of representation learning, we find that even sub-optimal tuning of S3 hyperparameters still yields meaningful improvements. We perform a simple experiment where we apply a uniform set of hyperparameters $[n=2,\\phi=2,\\theta=1,\\lambda=1]$ to all the baselines and baselines with added S3 layers. The results for this experiment are presented in Table 8 where we observe that despite not using the optimum hyperparameters, the addition of S3 still yields considerable performance boosts. The optimum hyperparameters used for each experiment are made available in the code. ", "page_idx": 8}, {"type": "table", "img_path": "zm1LcgRpHm/tmp/3ad0ac864a57df039339d466d010bbbe4dcf53a452539c5d5e3260874d839dab.jpg", "table_caption": ["Table 6: Range of hyperparameters. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Sensitivity to random seed. To evaluate the impact of the random seed on the performance of S3, we perform 10 separate trials with different seed values and present the standard deviations in Table 9. The first 4 rows show the classification baselines trained on the UCI datasets with and without S3. Similarly, the next 3 rows show forecasting baselines trained on the multivariate ETTh1 dataset $\\scriptstyle(\\mathrm{H}=24)$ with and without S3. From this experiment, we observe that the addition of S3 has no considerable impact on the sensitivity of the original baselines to the random seed. ", "page_idx": 8}, {"type": "text", "text": "Shuffilng parameters. In Figure 7, we present a visual overview of how the parameters of $\\mathbf{P}$ update along with the parent model during training on ETTh1 $(\\mathrm{H}=48)$ and ETTm1 $(\\mathrm{H}=48)$ . Each parameter in $\\mathbf{P}$ corresponds to a segment of S. In both cases, we set $n$ to 16 and $\\phi$ to 1. The baseline used is LaST [19]. It can be seen that the individual weights in $\\mathbf{P}$ rearrange the corresponding segments until they reach a stable point (the optimal order) after which the order of the segments is maintained. Similarly, Figure A4 (Appendix) shows how $\\mathbf{w}_{1}$ and $\\mathbf{w}_{2}$ update during training. Finally, in Figure A5 (Appendix), we show sample visualizations of how S3 segments, shuffles, and stitches a time-series once the optimal shuffling parameters are obtained. ", "page_idx": 8}, {"type": "table", "img_path": "", "table_caption": ["Table 7: Ablation results for Table 8: Classification results with Table 9: Variation to seed. "], "table_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "zm1LcgRpHm/tmp/63da873e9ba6a62f31196c77ef182d93891b8a78f39df3c3f94d8bc117d91fe8.jpg", "img_caption": ["Figure 7: The progression of the shuffling parameters during training for $\\mathrm{LaST}{+}\\mathrm{S}3$ . "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Computation overhead. Our proposed network layer adds very few learnable parameters to the baseline models, which stem from a learnable shuffilng vector $\\mathbf{P}$ along with $\\mathbf{w}_{1}$ and $\\mathbf{w}_{2}$ , per each S3 layer. To emphasize the negligible computation overhead of S3 in comparison to the baseline models, we show the total number of parameters in the baseline and the total learnable parameters that S3 adds, in Table 10 for classification on the EEG2 dataset and forecasting on the multivariate ETTh1 dataset $(\\mathrm{H}=24)$ ). ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Summary. We propose S3, a simple plug-and-play neural network component that rearranges the time-series in three steps: Segment the time-series, Shuffle the segments, and Stitch the shuffled time-series by concatenating the segments and performing a learned sum with the original timeseries. Through extensive experiments on time-series classification and forecasting with state-ofthe-art methods, we show that S3 improves the learning capabilities of the baseline with negligible computation overhead. We also show empirically that S3 helps in faster and smoother training leading to better performance. ", "page_idx": 9}, {"type": "text", "text": "Limitations. While we present the effectiveness of S3 on a diverse set of baselines on classification, forecasting, and anomaly detection tasks, we acknowledge that the evaluation of other time-series tasks such as imputation remain for future work. Additionally applying S3 to learning representations from other forms of time-series such as videos is an interesting direction for future research. ", "page_idx": 9}, {"type": "text", "text": "Broader impact. Since S3 is a plug-and-play network layer with negligible added parameters, it allows researchers from various domains related to time-series such as health signal processing, biometrics, climate analysis, financial markets, and others to use this module in their existing models without the need for redesign. The low computation overhead also makes S3 a suitable choice for edge-devices. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This research was partially supported by the Natural Sciences and Engineering Research Council of Canada (NSERC) through the Discovery Grant program. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Dani Kiyasseh, Tingting Zhu, and David A Clifton. Clocs: Contrastive learning of cardiac signals across space, time, and patients. In International Conference on Machine Learning, pages 5606\u20135615. PMLR, 2021. 1   \n[2] Anubhav Bhatti, Behnam Behinaein, Paul Hungler, and Ali Etemad. Attx: Attentive crossconnections for fusion of wearable signals in emotion recognition. ACM Transactions on Computing for Healthcare, 5(3):1\u201324, 2024. 1   \n[3] Debaditya Shome, Pritam Sarkar, and Ali Etemad. Region-disentangled diffusion model for high-fidelity ppg-to-ecg translation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 15009\u201315019, 2024. 1   \n[4] Debaditya Shome and Ali Etemad. Speech emotion recognition with distilled prosodic and linguistic affect representations. In IEEE International Conference on Acoustics, Speech and Signal Processing, pages 11976\u201311980, 2024. 1   \n[5] Pritam Sarkar and Ali Etemad. Self-supervised ecg representation learning for emotion recognition. IEEE Transactions on Affective Computing, 13(3):1541\u20131554, 2022. 1   \n[6] Pritam Sarkar, Ahmad Beirami, and Ali Etemad. Uncovering the hidden dynamics of video self-supervised learning under distribution shifts. Advances in Neural Information Processing Systems, 36, 2024. 1   \n[7] Hyeokhyen Kwon, Catherine Tong, Harish Haresamudram, Yan Gao, Gregory D. Abowd, Nicholas D. Lane, and Thomas Pl\u00f6tz. Imutube: Automatic extraction of virtual on-body accelerometry from video for human activity recognition. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 4(3), September 2020. 1   \n[8] Sijie He, Xinyan Li, Timothy DelSole, Pradeep Ravikumar, and Arindam Banerjee. Subseasonal climate forecasting via machine learning: Challenges, analysis, and advances. In AAAI Conference on Artificial Intelligence, volume 35, pages 169\u2013177, 2021. 1   \n[9] Haitao Lin, Zhangyang Gao, Yongjie Xu, Lirong Wu, Ling Li, and Stan Z Li. Conditional local convolution for spatio-temporal meteorological forecasting. In AAAI Conference on Artificial Intelligence, volume 36, pages 7470\u20137478, 2022. 1   \n[10] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In AAAI Conference on Artificial Intelligence, volume 35, pages 11106\u201311115, 2021. 1, 2, 5, 8, 21   \n[11] Seok-Jun Bu and Sung-Bae Cho. Time series forecasting with multi-headed attention-based deep learning for residential energy consumption. Energies, 13(18):4722, 2020. 1   \n[12] Zhiyuan Wang, Xovee Xu, Goce Trajcevski, Kunpeng Zhang, Ting Zhong, and Fan Zhou. Pref: Probabilistic electricity forecasting via copula-augmented state space model. In AAAI Conference on Artificial Intelligence, volume 36, pages 12200\u201312207, 2022. 1   \n[13] Lei Bai, Lina Yao, Can Li, Xianzhi Wang, and Can Wang. Adaptive graph convolutional recurrent network for traffic forecasting. Advances in Neural Information Processing Systems, 33:17804\u201317815, 2020. 1   \n[14] Razvan-Gabriel Cirstea, Bin Yang, Chenjuan Guo, Tung Kieu, and Shirui Pan. Towards spatiotemporal aware traffic time series forecasting. In IEEE International Conference on Data Engineering, pages 2900\u20132913, 2022. 1 [15] Dawei Cheng, Fangzhou Yang, Sheng Xiang, and Jin Liu. Financial time series forecasting with multi-modality graph neural network. Pattern Recognition, 121:108218, 2022. 1 [16] Qiao Xiao, Boqian Wu, Yu Zhang, Shiwei Liu, Mykola Pechenizkiy, Elena Mocanu, and Decebal Constantin Mocanu. Dynamic sparse network for time series classification: Learning what to \u201csee\u201d. Advances in Neural Information Processing Systems, 35:16849\u201316862, 2022. 1,   \n2, 5 [17] Zhihan Yue, Yujing Wang, Juanyong Duan, Tianmeng Yang, Congrui Huang, Yunhai Tong, and Bixiong Xu. Ts2vec: Towards universal representation of time series. In AAAI Conference on Artificial Intelligence, volume 36, pages 8980\u20138987, 2022. 1, 3, 5, 6 [18] Xuchao Zhang, Yifeng Gao, Jessica Lin, and Chang-Tien Lu. Tapnet: Multivariate time series classification with attentional prototypical network. In AAAI Conference on Artificial Intelligence, volume 34, pages 6845\u20136852, 2020. 1 [19] Zhiyuan Wang, Xovee Xu, Weifeng Zhang, Goce Trajcevski, Ting Zhong, and Fan Zhou. Learning latent seasonal-trend representations for time series forecasting. Advances in Neural Information Processing Systems, 35:38775\u201338787, 2022. 1, 3, 5, 9, 20 [20] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. Advances in Neural Information Processing Systems, 34:22419\u201322430, 2021. 1, 3 [21] Minhao Liu, Ailing Zeng, Muxi Chen, Zhijian Xu, Qiuxia Lai, Lingna Ma, and Qiang Xu. Scinet: Time series modeling and forecasting with sample convolution and interaction. Advances in Neural Information Processing Systems, 35:5816\u20135828, 2022. 1, 2 [22] Qingxiong Tan, Mang Ye, Baoyao Yang, Siqi Liu, Andy Jinhua Ma, Terry Cheuk-Fung Yip, Grace Lai-Hung Wong, and PongChi Yuen. Data-gru: Dual-attention time-aware gated recurrent unit for irregular multivariate time series. In AAAI Conference on Artificial Intelligence, volume 34, pages 930\u2013937, 2020. 1 [23] Donghui Chen, Ling Chen, Youdong Zhang, Bo Wen, and Chenghu Yang. A multiscale interactive recurrent network for time-series forecasting. IEEE Transactions on Cybernetics,   \n52(9):8793\u20138803, 2021. 1 [24] Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhassane Idoumghar, and PierreAlain Muller. Deep learning for time series classification: a review. Data Mining and Knowledge Discovery, 33(4):917\u2013963, 2019. 1 [25] Patrick Sch\u00e4fer. The boss is concerned with time series classification in the presence of noise. Data Mining and Knowledge Discovery, 29:1505\u20131530, 2015. 1 [26] Anthony Bagnall, Jason Lines, Jon Hills, and Aaron Bostrom. Time-series classification with cote: the collective of transformation-based ensembles. IEEE Transactions on Knowledge and Data Engineering, 27(9):2522\u20132535, 2015. 1 [27] Aosong Feng and Leandros Tassiulas. Adaptive graph spatial-temporal transformer network for traffic forecasting. In ACM International Conference on Information and Knowledge Management, pages 3933\u20133937, 2022. 2 [28] Junwoo Park, Daehoon Gwak, Jaegul Choo, and Edward Choi. Self-supervised contrastive learning for long-term forecasting. In International Conference on Learning Representations,   \n2024. 2, 3 [29] Ali Salehi and Madhusudhanan Balasubramanian. Ddcnet: Deep dilated convolutional neural network for dense prediction. Neurocomputing, 523:116\u2013129, 2023. 2 [30] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems, 30, 2017. 2   \n[31] Yuqi Chen, Kan Ren, Yansen Wang, Yuchen Fang, Weiwei Sun, and Dongsheng Li. Contiformer: Continuous-time transformer for irregular time series modeling. Advances in Neural Information Processing Systems, 36, 2024. 3   \n[32] Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth 64 words: Long-term forecasting with transformers. In The Eleventh International Conference on Learning Representations. 3, 5, 21   \n[33] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series forecasting? In AAAI Conference on Artificial Intelligence, volume 37, pages 11121\u201311128, 2023. 3   \n[34] Mononito Goswami, Konrad Szafer, Arjun Choudhry, Yifu Cai, Shuo Li, and Artur Dubrawski. Moment: A family of open time-series foundation models. arXiv preprint arXiv:2402.03885, 2024. 3, 5, 7, 8   \n[35] Dongsheng Luo, Wei Cheng, Yingheng Wang, Dongkuan Xu, Jingchao Ni, Wenchao Yu, Xuchao Zhang, Yanchi Liu, Yuncong Chen, Haifeng Chen, et al. Time series contrastive learning with information-aware augmentations. In AAAI Conference on Artificial Intelligence, volume 37, pages 4534\u20134542, 2023. 3, 5   \n[36] Seunghan Lee, Taeyoung Park, and Kibok Lee. Soft contrastive learning for time series. In The International Conference on Learning Representations, 2024. 3, 5, 6, 21   \n[37] Renfei He, Limao Zhang, and Alvin Wei Ze Chew. Modeling and predicting rainfall time series using seasonal-trend decomposition and machine learning. Knowledge-Based Systems, 251:109125, 2022. 3   \n[38] Robert B Cleveland, William S Cleveland, Jean E McRae, Irma Terpenning, et al. Stl: A seasonal-trend decomposition. Journal of Official Statistics, 6(1):3\u201373, 1990. 3   \n[39] Shuai Liu, Xiucheng Li, Gao Cong, Yile Chen, and Yue Jiang. Multivariate time-series imputation with disentangled temporal representations. In International Conference on Learning Representations, 2022. 3   \n[40] Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven Hoi. CoST: Contrastive learning of disentangled seasonal-trend representations for time series forecasting. In International Conference on Learning Representations, 2022. 3, 5   \n[41] Marco Cuturi, Olivier Teboul, and Jean-Philippe Vert. Differentiable ranking and sorting using optimal transport. Advances in Neural Information Processing Systems, 32, 2019. 4   \n[42] Mathieu Blondel, Olivier Teboul, Quentin Berthet, and Josip Djolonga. Fast differentiable sorting and ranking. In International Conference on Machine Learning, pages 950\u2013959, 2020. 4   \n[43] Michael Taylor, John Guiver, Stephen Robertson, and Tom Minka. Softrank: optimizing nonsmooth rank metrics. In International Conference on Web Search and Data Mining, pages 77\u201386, 2008. 4   \n[44] Hoang Anh Dau, Anthony Bagnall, Kaveh Kamgar, Chin-Chia Michael Yeh, Yan Zhu, Shaghayegh Gharghabi, Chotirat Ann Ratanamahatana, and Eamonn Keogh. The ucr time series archive. IEEE Journal of Automatica Sinica, 6(6):1293\u20131305, 2019. 5, 8   \n[45] Anthony Bagnall, Hoang Anh Dau, Jason Lines, Michael Flynn, James Large, Aaron Bostrom, Paul Southam, and Eamonn Keogh. The uea multivariate time series classification archive, 2018. ArXiv Preprint arXiv:1811.00075, 2018. 5   \n[46] Dheeru Dua and Casey Graff. Uc irvine machine learning repository, 2017. 5   \n[47] Patrick Wagner, Nils Strodthoff, Ralf-Dieter Bousseljot, Dieter Kreiseler, Fatima I Lunze, Wojciech Samek, and Tobias Schaeffter. Ptb-xl, a large publicly available electrocardiography dataset. Scientific Data, 7(1):1\u201315, 2020. 5, 8 ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "[48] Max Planck Institute for Biogeochemistry. Jena climate and weather data. https://www. bgc-jena.mpg.de/wetter/. 5 ", "page_idx": 13}, {"type": "text", "text": "[49] Nikolay Laptev, Saeed Amizadeh, and Ian Flint. Generic and scalable framework for automated time-series anomaly detection. In ACM International Conference on Knowledge Discovery and Data Mining, pages 1939\u20131947, 2015. 5   \n[50] Hansheng Ren, Bixiong Xu, Yujing Wang, Chao Yi, Congrui Huang, Xiaoyu Kou, Tony Xing, Mao Yang, Jie Tong, and Qi Zhang. Time-series anomaly detection service at microsoft. In ACM International Conference on Knowledge Discovery & Data Mining, pages 3009\u20133017, 2019. 5   \n[51] Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape of neural nets. Advances in Neural Information Processing Systems, 31, 2018. 6, 19   \n[52] Xiyuan Zhang, Ranak Roy Chowdhury, Jingbo Shang, Rajesh Gupta, and Dezhi Hong. Towards diverse and coherent augmentation for time-series forecasting. In International Conference on Acoustics, Speech, and Signal Processing, pages 1\u20135, 2023. 21   \n[53] Brian Kenji Iwana and Seiichi Uchida. An empirical survey of data augmentation for time series classification with neural networks. Plos One, 16(7):e0254841, 2021. 21   \n[54] Anh Tong, Thanh Nguyen-Tang, Toan Tran, and Jaesik Choi. Learning fractional white noises in neural stochastic differential equations. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. 21 ", "page_idx": 13}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Additional results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We show the full results for all baselines with and without S3 on 128 UCR datasets in Table A1 and on 30 UEA datasets in Table A2. In Figure A1 we show 2 more t-SNE plots for TS2Vec trained on 2 randomly chosen UCR datasets. A better clustering of different classes can be observed in the learned representations after S3 is incorporated. In Table A4 we compare the standard deviation values for the training loss of SoftCLT and $\\mathrm{SoftCLT}{+}\\mathrm{S}3.$ . From this table we observe that after incorporating S3, the training becomes much more stable with significantly less variations. Figure A2 shows 2 more comparisons of the loss landscape of TS2Vec trained with and without S3. In Figure A3 we present 6 more samples for the impact of $n$ on the performance of the model. ", "page_idx": 14}, {"type": "table", "img_path": "zm1LcgRpHm/tmp/663334059856d65c6d77874b536409a0c0190f48483dc18dca373361dc09527e.jpg", "table_caption": ["Table A1: Per-dataset breakdown of results for the UCR archive. "], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "zm1LcgRpHm/tmp/2862f493a20b232d72ba2cc4f1f4be721d4391f70b986f155be74191b792d317.jpg", "table_caption": ["Table A1 \u2013 continued from previous page "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "zm1LcgRpHm/tmp/91b1e9d105688d95fd72450b45d328c7468f14951c5d7e5c0687f4970f01ece1.jpg", "table_caption": ["Table A2: Per-dataset breakdown of classification results for the UEA dataset. "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "zm1LcgRpHm/tmp/fe237e01cfb342bd397870d7383c5fa633383c98f2c6a7a53b42a918fa5f8380.jpg", "table_caption": ["Table A3: Forecasting results for MOMENT with and without S3. "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "zm1LcgRpHm/tmp/208e156984e1a805fa879bebb74d36f98f62255f6e4f5b55e5b3cb468435922e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "zm1LcgRpHm/tmp/70fc534650def944acaab78bb0475d5156e3bbfd89586cfb3a4904babc6e4d65.jpg", "img_caption": [], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure A1: t-SNE visualizations of the learned representations for TS2Vec and $\\mathrm{TS}2\\mathrm{Vec}{+}\\mathrm{S}3$ for 2 randomly chosen test sets. Different colors represent different classes, where we observe better grouping of representations belonging to each class after the addition of S3. ", "page_idx": 18}, {"type": "image", "img_path": "zm1LcgRpHm/tmp/d2aa72b1bebcc4a15aa7bf01d74b7e9b347fddcee1ec0639249641b44ff04a1b.jpg", "img_caption": ["Figure A2: Visualisation of the loss landscape following [51], for TS2Vec and $\\mathrm{TS}2\\mathrm{Vec}{+}\\mathrm{S}3$ on two UCR datasets. It can be observed that the loss landscape with S3 is considerably smoother than the baseline without S3. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "zm1LcgRpHm/tmp/31f98b17ca3d212c6996d202e6940840b3a64f6ab31f34943cfd95e4292ee51b.jpg", "img_caption": ["Figure A3: Performance vs. number of segment. In (a), (b), and (c), 3 UCR datasets for classification were used, and in (d), (e), and (f) the ETTh2 dataset for forecasting was used with 3 different horizon lengths. A higher value for accuracy is better, and a lower value MSE is better. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "A.2 Weighted average parameters during training ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In Figure A4, we present a visual overview of how the weighted average parameters $\\mathbf{w}_{1}$ and $\\mathbf{w}_{2}$ update along with the rest of the model during training on ETTh1 $(\\mathrm{H}=24)$ ) and ETTh2 $(\\mathrm{H}=720)$ ) in multivariate settings. We use LaST [19] as the baseline. It is observed that the weights converge to their final values at the end of training. ", "page_idx": 19}, {"type": "image", "img_path": "zm1LcgRpHm/tmp/cf4b31da018d51810ffac9009d4bab1b462271191a07f720d105fffde1c6ff5b.jpg", "img_caption": ["Figure A4: The progression of the weighted average parameters $\\mathbf{w}_{1}$ and $\\mathbf{w}_{2}$ . "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "A.3 Visualisation of the segments ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Figure A5 visualizes three sample time-series and how their respective segments are rearranged using S3. The figure depicts the original sequence, the shuffled sequence, and the final output. ", "page_idx": 19}, {"type": "image", "img_path": "zm1LcgRpHm/tmp/881507ca636c31105e878fa57c0a5f11542cbb83d1d68f83458cfaab68fe4ce1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure A5: Sample visualizations of S3 on two UCI datasets (left and middle) and the ETTh1 dataset (right) with TS2Vec as the baseline. We have 8 segments for the UCI datasets, and 16 for the ETTh1. ", "page_idx": 19}, {"type": "text", "text": "A.4 Comparison with data augmentation methods ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The proposed S3 layer is fundamentally different from data augmentation techniques, in that unlike traditional augmentation methods, S3 introduces learnable parameters that are jointly optimized with the rest of the model, allowing S3 to adapt dynamically throughout the training process. To further explore the differences between S3 and data augmentation, we conduct an experiment where, before training on the ETTm1 multivariate dataset, we apply the following augmentations: (1) shuffling augmentation with varying numbers of segments, (2) shuffilng augmentation combined with mixup, (3) noise augmentation with zero mean, variance of 1, and a magnifying coefficient of 0.01, and (4) ", "page_idx": 19}, {"type": "table", "img_path": "zm1LcgRpHm/tmp/7ac4027602e5514665761d7f419f3a1d4042defe085b58950e6348ff2b5c7116.jpg", "table_caption": ["Table A5: Comparison of S3 with different data augmentation techniques on the ETTm1 multivariate dataset. All values are MSE. "], "table_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "zm1LcgRpHm/tmp/c83e974b25fbdb923b24900d40008fa2c1b22fda7cc9d8eb075acfc393429764.jpg", "img_caption": ["Figure A6: Dataset size vs. improvement due to S3. Here, different subsets of the LSST dataset from the UEA archive are randomly selected to create different sized variants. The results are averaged over three runs. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "noise augmentation combined with mixup. The results are presented in Table A5 where we observe that S3 outperforms such data augmentation strategies. Additionally, data augmentation techniques typically have a stronger impact on smaller datasets, often due to the lack of variation and diversity, but their effect is less pronounced on larger datasets [53]. To evaluate this, we use the LSST dataset from UEA and create 10 different subsets by randomly dropping varying amounts of data, ranging from $20\\%$ to $99\\%$ . We then retrain SoftCLT [36], both with and without S3, on these subsets as well as on the original dataset. The results, averaged over three runs, are presented in Figure A6. These results show no clear trend in performance gain relative to dataset size, indicating that S3 benefits datasets regardless of their size. ", "page_idx": 20}, {"type": "text", "text": "A.5 Case-study on factors influencing the improvement by S3 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "To investigate the scenarios where S3 is most effective, we conduct additional experiments considering three key factors: (1) the length of time-series sequences, (2) the degree of non-linearity, and (3) the presence of long-term temporal dependencies. For (1), we examine the percentage difference relative to sequence length. For (2), we consider the percentage difference relative to the mean of squared residuals from a linear model. Lastly, for (3), we analyze the percentage difference in relation to the Hurst exponent [54]. We use PatchTST [32] for experiments (2) and (3), while Informer [10] is used for experiment (1) since PatchTST cannot be used due to its reliance on equal-length inputs. We conduct all three experiments on the ETT, Weather, and Electricity datasets. The results of this analysis are shown in Table A6, where m represents the slope of the linear relationship, and $\\mathbf{R}$ is Pearson\u2019s correlation coefficient. We observe direct positive relationships between the percentage difference upon adding S3 and both sequence length and long-term temporal dependency, while non-linearity in the time-series shows no considerable relationship. For sequence length, the moderate correlation suggests that while the effect is not very strong, it is consistent, indicating that sequence length is likely a relevant factor for percentage difference. Long-term temporal dependency has a strong impact on percentage difference, as indicated by the steep slope. However, the lower correlation suggests that other factors in the data may also influence this relationship. We hypothesize that when long-term temporal dependencies are simple to learn or highly repetitive, the impact of S3 will be less pronounced, as re-ordering may not be necessary to learn such simple dynamics. On the other hand, for more complex long-term dependencies, S3 has a much stronger impact, as reflected by the high slope. ", "page_idx": 20}, {"type": "table", "img_path": "zm1LcgRpHm/tmp/b73910859e21cb84e6a9d043c4983ff076fe504721ddcc213fe14324e658e714.jpg", "table_caption": ["Table A6: Linear analysis between three factors (sequence length, non-linearity, and long-term temporal dependency) and percentage difference. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Yes, the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: Yes, see Section 6 ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: This paper has no theoretical assumptions. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: The implementation details are detailed in Section 4, and the full documented code is provided. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The code is provided with sufficient documentation to be able to run it in few simple steps. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to unders tand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We use the same experimental settings as the baselines that we have incorporated S3 into, except for experiments on PatchTST and LaST with the electricity dataset where we use a batch size of 8. We have clarified this in Section 4. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: In Table 9 we show the standard deviation in performance metric for all baselines on a single dataset for both classification and forecasting by repeating the experiments for 10 different seed values, observing very small sensitivity to seed values. We do not perform this for each experiment in our main paper, as it would be computationally too expensive. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 23}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The compute resources is presented Section 4. Additionally we have a separate study in Table 10 in which we analyze the exact computational overhead w.r.t. the baselines. ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: See section. 6   \nGuidelines: \u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pre-trained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 25}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. ", "page_idx": 25}, {"type": "text", "text": "\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: Apart from our code, that is well documented, we do not release any assets. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}]