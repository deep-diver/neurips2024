[{"figure_path": "zm1LcgRpHm/figures/figures_2_1.jpg", "caption": "Figure 1: Stacking S3 layers. In this depiction, we use n = 2,  \u03a6 = 3, and \u03b8 = 2 as the hyperparameters.", "description": "This figure shows how multiple S3 layers can be stacked to achieve different levels of granularity in time series processing. Each S3 layer consists of three steps: segment, shuffle, and stitch. The segment step divides the input time series into non-overlapping segments. The shuffle step rearranges these segments in a learned manner that is optimal for the task at hand. The stitch step re-attaches the shuffled segments back together. The hyperparameters n, \u03a6, and \u03b8 control the number of segments, the number of S3 layers, and the multiplier for the number of segments in subsequent layers, respectively. By stacking multiple S3 layers, the model can perform shuffling at different granularity levels, improving the ability of the model to capture long-range dependencies in the time series.", "section": "3 Method"}, {"figure_path": "zm1LcgRpHm/figures/figures_8_1.jpg", "caption": "Figure 3: Forecasting output by Informer and Informer+S3 for a sample from ETTh1.", "description": "This figure shows the forecasting results of the Informer model with and without the proposed S3 layer.  Two different forecast horizons (H=24 and H=168) are presented, comparing the model's predictions against the ground truth. The plots visually demonstrate that incorporating the S3 layer leads to improved forecasting accuracy for both forecast horizons. The model with S3 more accurately reflects the trends and patterns in the ground truth.", "section": "Forecasting"}, {"figure_path": "zm1LcgRpHm/figures/figures_8_2.jpg", "caption": "Figure A2: Visualisation of the loss landscape following [51], for TS2Vec and TS2Vec+S3 on two UCR datasets. It can be observed that the loss landscape with S3 is considerably smoother than the baseline without S3.", "description": "This figure visualizes the loss landscape for TS2Vec with and without the proposed S3 layer on two datasets from the UCR archive.  The loss landscape is a representation of the loss function across different parameter settings.  A smoother loss landscape typically indicates more stable training.  This figure demonstrates that integrating S3 leads to a significantly smoother loss landscape compared to the baseline, implying more stable training dynamics and potentially improved generalization.", "section": "A.2 Visualisation of the segments"}, {"figure_path": "zm1LcgRpHm/figures/figures_8_3.jpg", "caption": "Figure 4: Training loss against iterations on two sample datasets from the UCR archive.", "description": "The figure shows the training loss curves for two datasets from the UCR archive (ProximalPhalanxTW and Symbols) with and without the proposed S3 layer. It demonstrates that the S3 layer leads to faster convergence and smoother training loss curves compared to the baselines.", "section": "Loss behavior"}, {"figure_path": "zm1LcgRpHm/figures/figures_8_4.jpg", "caption": "Figure A3: Performance vs. number of segment. In (a), (b), and (c), 3 UCR datasets for classification were used, and in (d), (e), and (f) the ETTh2 dataset for forecasting was used with 3 different horizon lengths. A higher value for accuracy is better, and a lower value MSE is better.", "description": "This figure shows the impact of the number of segments on the performance of the proposed S3 layer for both classification and forecasting tasks.  Three UCR datasets (Beetlefly, RacketSport, and Rock) are used for classification, while the ETTh2 dataset is used for forecasting with three different horizon lengths (720, 336, and 48). The results demonstrate that an optimal number of segments exists for each dataset and task, highlighting the importance of this hyperparameter in achieving the best performance.", "section": "A.3 Visualisation of the segments"}, {"figure_path": "zm1LcgRpHm/figures/figures_9_1.jpg", "caption": "Figure 1: Stacking S3 layers. In this depiction, we use n = 2,  = 3, and  = 2 as the hyperparameters.", "description": "This figure illustrates the modular design of the S3 layer, showing how multiple S3 layers can be stacked to achieve different levels of granularity in time-series shuffling. Each S3 layer consists of three steps: Segment, Shuffle, and Stitch. The hyperparameters n, , and  control the number of segments, the number of layers, and the multiplier for the number of segments in subsequent layers, respectively.  The figure shows an example with 2 segments per layer and 3 layers stacked, demonstrating how the time series data is progressively shuffled and processed.", "section": "3 Method"}, {"figure_path": "zm1LcgRpHm/figures/figures_18_1.jpg", "caption": "Figure A1: t-SNE visualizations of the learned representations for TS2Vec and TS2Vec+S3 for 2 randomly chosen test sets. Different colors represent different classes, where we observe better grouping of representations belonging to each class after the addition of S3.", "description": "This figure shows the t-distributed Stochastic Neighbor Embedding (t-SNE) visualizations of the learned representations by TS2Vec with and without the S3 layer.  Two randomly selected test sets from the UCR dataset are used. Each point represents a data sample, and the color represents its class label. The plots demonstrate that after adding the S3 layer, the representations of different classes are more separable in the t-SNE space, indicating improved class separability.", "section": "A.1 Additional results"}, {"figure_path": "zm1LcgRpHm/figures/figures_18_2.jpg", "caption": "Figure A1: t-SNE visualizations of the learned representations for TS2Vec and TS2Vec+S3 for 2 randomly chosen test sets. Different colors represent different classes, where we observe better grouping of representations belonging to each class after the addition of S3.", "description": "This figure shows t-SNE visualizations of the learned representations using TS2Vec with and without the proposed S3 layer.  Two different UCR datasets are used, and each plot shows the resulting clusters of data points in 2-dimensional space, with different colors representing different classes. The addition of the S3 layer results in a clearer separation of the classes, indicating improved representation learning and better class separability.", "section": "A.1 Additional results"}, {"figure_path": "zm1LcgRpHm/figures/figures_18_3.jpg", "caption": "Figure A3: Performance vs. number of segment. In (a), (b), and (c), 3 UCR datasets for classification were used, and in (d), (e), and (f) the ETTh2 dataset for forecasting was used with 3 different horizon lengths. A higher value for accuracy is better, and a lower value MSE is better.", "description": "This figure visualizes the impact of the number of segments (n) on the performance of the proposed S3 layer. It shows the accuracy and MSE for three classification datasets (BeetleFly, RacketSport, Rock) and three forecasting datasets (ETTh2 with different horizons) using different numbers of segments. This demonstrates the effect of segmenting the time series with varying granularity on the model's performance. The optimal number of segments differs depending on the dataset and task, highlighting the adaptive nature of the S3 layer.", "section": "A.3 Visualisation of the segments"}, {"figure_path": "zm1LcgRpHm/figures/figures_19_1.jpg", "caption": "Figure A4: The progression of the weighted average parameters w1 and w2.", "description": "This figure visualizes how the weighted average parameters w1 and w2 change during the training process for two different multivariate forecasting datasets: ETTh2(M) with a horizon (H) of 720 and ETTh1(M) with a horizon of 24. The plots show the values of w1 and w2 over the training iterations, illustrating their convergence towards final values.", "section": "A.2 Weighted average parameters during training"}, {"figure_path": "zm1LcgRpHm/figures/figures_19_2.jpg", "caption": "Figure 1: Stacking S3 layers. In this depiction, we use n = 2, \u03c6 = 3, and \u03b8 = 2 as the hyperparameters.", "description": "This figure shows how multiple S3 layers can be stacked in a neural network. Each S3 layer consists of three operations: segment, shuffle, and stitch. The hyperparameters n, \u03c6, and \u03b8 control the number of segments, the number of layers, and the multiplier for the number of segments in subsequent layers, respectively. The figure shows an example with n=2, \u03c6=3, and \u03b8=2, resulting in three S3 layers, each with a different number of segments.", "section": "3 Method"}, {"figure_path": "zm1LcgRpHm/figures/figures_20_1.jpg", "caption": "Figure A6: Dataset size vs. improvement due to S3. Here, different subsets of the LSST dataset from the UEA archive are randomly selected to create different sized variants. The results are averaged over three runs.", "description": "This figure shows the relationship between the size of the LSST dataset and the performance improvement achieved by incorporating the S3 layer. Different subsets of the LSST dataset were created by randomly sampling varying amounts of data (ranging from 20% to 99%), and the SoftCLT model was trained with and without the S3 layer on each of these subsets. The plot shows the average improvement in performance (as measured by the percentage difference in MSE) for each dataset size, averaged over three independent runs. The results suggest that there is no clear trend in the performance improvement as a function of dataset size, indicating that the S3 layer's benefits are consistent regardless of dataset size.", "section": "A.5 Case-study on factors influencing the improvement by S3"}]