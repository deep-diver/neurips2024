[{"figure_path": "leqD3bJ4Ly/figures/figures_1_1.jpg", "caption": "Figure 1: Key-steps required to prepare a brownie [17]. The sequences showcase temporal variations and corresponding key-step alignment challenges, namely (i) background frames (depicted as gray blocks), (ii) non-monotonic frames. OPEL aims to learn an embedding space where corresponding key-steps have similar embeddings while tackling the above challenges.", "description": "This figure illustrates the challenges in procedure learning.  It shows three example videos (V1, V2, V3) of someone making brownies. Each video shows the same five key steps (break egg, mix egg, add water, add oil, mix contents), but the order and timing of the steps vary across videos. Some videos include extra frames that are not part of the main procedure (background frames), while others show steps that are out of order (non-monotonic frames). The figure also shows how OPEL addresses these issues by learning an embedding space where frames with the same semantics (representing the same key step) are grouped together, regardless of temporal inconsistencies or extra frames.", "section": "1 Introduction"}, {"figure_path": "leqD3bJ4Ly/figures/figures_3_1.jpg", "caption": "Figure 2: (A) The encoder generates frame-wise embeddings from videos, facilitating subsequent OT calculations. (B) Pair-wise scenarios captured through the assignment matrix- from strictly synchronized actions to temporal shifts and differing action speeds, to non-monotonicity. (C) 1-D depiction of alignment of a single frame (i-th) of Video 2 with its best match frame (j-th) of Video 1, based on the proposed priors. (D) 2-D representation of the optimal alignment of frame sequences.", "description": "This figure illustrates the OPEL framework. (A) shows how the encoder generates frame embeddings for videos. (B) showcases different alignment scenarios from perfectly synchronized to various temporal variations and non-monotonic cases. (C) demonstrates how the optimality and temporal priors align a single frame from Video 2 with its best match in Video 1 using Laplace distribution.  (D) visualizes the overall optimal frame sequence alignment based on these priors.", "section": "3 OPEL Framework"}, {"figure_path": "leqD3bJ4Ly/figures/figures_7_1.jpg", "caption": "Figure 3: Qualitative results from MECCANO [53] and PC Assembly [1] tasks. Each sub-task is color-coded to represent different key-steps, while gray areas signify background elements. Notably, OPEL's performance surpasses that of the SOTA networks, attributed to its ability to handle unmatched frames through the integration of a virtual frame, thus enhancing alignment accuracy.", "description": "This figure presents a qualitative comparison of the proposed OPEL method against several state-of-the-art (SOTA) approaches for procedure learning on two benchmark datasets: MECCANO and PC Assembly.  Each row represents a different method (Ground Truth, Random, CnC, GPL+I3D, GPL, and OPEL), and each column shows the results for one of the datasets. Within each row and dataset, the vertical bars represent the frames of the video, and each color corresponds to a different key-step in the procedure. Gray represents frames that were not assigned to a key-step. The figure shows that OPEL significantly improves the accuracy of key-step identification and alignment compared to the SOTA methods.  This improvement is attributed to OPEL's ability to handle unmatched frames through the use of a virtual frame in its optimal transport formulation.", "section": "Results"}, {"figure_path": "leqD3bJ4Ly/figures/figures_7_2.jpg", "caption": "Figure 4: (A) Impact of training data quantity on encoder training. (B) Example alignment of two videos with corresponding key-step clusters from the Brownie task [17].", "description": "Figure 4(A) shows the effect of varying the number of training videos on the F1 score using the MECCANO dataset.  It demonstrates that OPEL consistently outperforms other state-of-the-art methods, even with a small number of training videos. Figure 4(B) illustrates the alignment between two videos of the brownie-making process, showing how OPEL accurately identifies corresponding key-steps despite temporal variations. The visualization uses color-coding to highlight correct and incorrect alignments, and also visualizes background and virtual frames.", "section": "Experiments and Results"}, {"figure_path": "leqD3bJ4Ly/figures/figures_19_1.jpg", "caption": "Figure A1: Importance of choosing Laplace distribution as a prior.", "description": "This figure compares the probability density functions (PDFs) of Laplace, Gaussian, and Uniform distributions.  It highlights that the Laplace distribution is better suited for modeling the alignment of video frames because it has heavy tails, which can better capture outliers or non-monotonic alignments that deviate significantly from the mean. The Gaussian distribution is more sensitive to outliers and lacks the ability to capture such deviations, while the Uniform distribution does not accurately reflect the true distribution of frame alignments, which is likely to be skewed.", "section": "A.8.2 Hyperparameters \u03bb\u2081 and \u03bb\u2082"}]