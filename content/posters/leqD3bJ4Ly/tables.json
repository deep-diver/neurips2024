[{"figure_path": "leqD3bJ4Ly/tables/tables_6_1.jpg", "caption": "Table 1: Results using EgoProceL [1] demonstrate the superior performance of OPEL. The results in bold and underline denote the highest and second-highest values in a column, respectively.", "description": "This table presents a comparison of the proposed OPEL method against several state-of-the-art (SOTA) approaches for egocentric procedure learning on the EgoProceL benchmark dataset.  The table shows the F1-score and Intersection over Union (IoU) metrics for each method across various tasks. The results highlight that OPEL significantly outperforms other methods across all tasks, demonstrating its effectiveness in solving the challenging problem of procedure learning from egocentric videos.", "section": "4 Experiments and Results"}, {"figure_path": "leqD3bJ4Ly/tables/tables_6_2.jpg", "caption": "Table 2: PL results on third-person datasets [3, 11]. P, R, and F1 represent precision, recall, F1-score.", "description": "This table presents a comparison of the performance of the proposed OPEL method against several state-of-the-art (SOTA) methods on two benchmark datasets for procedure learning: ProceL and CrossTask.  Both datasets involve third-person (exocentric) videos.  The table shows the precision (P), recall (R), and F1-score (F1) for each method on both datasets.  The results demonstrate that OPEL significantly outperforms the SOTA methods on both datasets.", "section": "4 Experiments and Results"}, {"figure_path": "leqD3bJ4Ly/tables/tables_7_1.jpg", "caption": "Table 3: Comparison with models with multimodal input. Note, STEPS [16] uses additional data (optical flow, gaze, depth) for training, while we use just the visual modality.", "description": "This table compares the performance of OPEL against state-of-the-art (SOTA) multimodal models on various datasets.  It highlights that OPEL, which only uses visual data, performs comparably to or better than these multimodal models (which use additional modalities such as gaze and depth information) in several benchmarks. This shows the effectiveness of OPEL's visual-only approach.", "section": "4 Experiments and Results"}, {"figure_path": "leqD3bJ4Ly/tables/tables_8_1.jpg", "caption": "Table 4: Comparison of effectiveness of LOPEL with other losses.", "description": "This table presents a comparison of the performance of the proposed LOPEL framework against other state-of-the-art loss functions on four different datasets: CMU-MMAC, MECCANO, EGTEA-GAZE+, and PC Assembly.  The results showcase the superior performance of LOPEL across multiple metrics (P, F1, IoU) demonstrating its efficacy in procedure learning. The different loss functions represent various approaches to handling temporal and semantic aspects of video data, highlighting the advantages of OPEL's approach.", "section": "Ablation Study"}, {"figure_path": "leqD3bJ4Ly/tables/tables_8_2.jpg", "caption": "Table 5: Analysis of the impact of each term in LOPEL on the overall performance.", "description": "This table presents an ablation study analyzing the contribution of each component of the proposed LOPEL loss function. It shows the performance (F1 and IoU scores) on the MECCANO [53] and CMU-MMAC [17] datasets when different components (intra-video, inter-video contrastive loss, KL divergence, temporal prior, optimality prior, and virtual frame) are included or excluded. This helps to understand the relative importance of each component in improving the overall performance of the OPEL framework.", "section": "Ablation Study"}, {"figure_path": "leqD3bJ4Ly/tables/tables_8_3.jpg", "caption": "Table 1: Results using EgoProceL [1] demonstrate the superior performance of OPEL. The results in bold and underline denote the highest and second-highest values in a column, respectively.", "description": "This table presents a comparison of the proposed OPEL framework's performance against several state-of-the-art (SOTA) methods on the EgoProceL benchmark dataset for egocentric procedure learning.  The metrics used for comparison are F1-score and Intersection over Union (IoU).  The table highlights OPEL's significant performance gains compared to the SOTA methods across various metrics, showcasing its effectiveness in handling egocentric videos.", "section": "4 Experiments and Results"}, {"figure_path": "leqD3bJ4Ly/tables/tables_8_4.jpg", "caption": "Table 7: Results obtained for different k.", "description": "This table presents the results of OPEL and baseline models with varying numbers of key-steps (k). The best results are obtained with k=7, and the performance decreases significantly as k deviates from 7. The results are consistent across all datasets. This observation is consistent with other SOTA methods on the same datasets [1, 2, 16]. It is hypothesized that k=7 is optimal because it represents the average number of distinct key-steps (subtasks) in the datasets.", "section": "4 Experiments and Results"}, {"figure_path": "leqD3bJ4Ly/tables/tables_9_1.jpg", "caption": "Table 8: Comparison with SOTA unsupervised AS methods. Note '-' denotes that the authors have not provided any data on those metrics.", "description": "This table compares the performance of OPEL against state-of-the-art unsupervised action segmentation (AS) models on the ProceL and CrossTask datasets.  The table highlights that OPEL significantly outperforms existing AS methods in terms of precision (P), recall (R), and F1-score, demonstrating its superiority in handling the complexities of real-world video data and showcasing the benefits of its optimal transport approach.  The '-' indicates that some previous methods did not report results for all metrics.", "section": "Comparison with AS methods"}, {"figure_path": "leqD3bJ4Ly/tables/tables_15_1.jpg", "caption": "Table 1: Results using EgoProceL [1] demonstrate the superior performance of OPEL. The results in bold and underline denote the highest and second-highest values in a column, respectively.", "description": "This table presents a comparison of the proposed OPEL model's performance against several state-of-the-art (SOTA) methods on the EgoProceL benchmark dataset for egocentric procedure learning.  It shows the F1-score and Intersection over Union (IoU) metrics for each method across multiple datasets (CMU-MMAC, EGTEA-GAZE+, MECCANO, EPIC-Tents, PC Assembly, PC Disassembly). The results highlight OPEL's significant improvement over the SOTA in both F1-score and IoU.", "section": "4 Experiments and Results"}, {"figure_path": "leqD3bJ4Ly/tables/tables_16_1.jpg", "caption": "Table A2: Statistics of the EgoProceL dataset across different tasks.", "description": "This table presents a statistical analysis of the EgoProceL dataset, breaking down the number of videos, key-steps, foreground ratio (proportion of video time dedicated to key-steps), missing key-steps, and repeated key-steps for each of the 16 tasks included in the dataset.  The foreground ratio helps understand the amount of irrelevant background activity in each task. Missing key-steps and repeated key-steps are metrics indicating variations in the execution of tasks across videos.", "section": "A.4 Detailed Statistics of Dataset"}, {"figure_path": "leqD3bJ4Ly/tables/tables_17_1.jpg", "caption": "Table A3: Third-person view results using diverse perspectives from CMU-MMAC [17]. Our findings demonstrate improved outcomes utilizing OT on egocentric as well as third-person videos, emphasizing their efficacy. P, R, and F denote precision, recall, and F-score, respectively.", "description": "This table presents the results of the OPEL model applied to third-person videos from the CMU-MMAC dataset, offering a comparison of performance across different viewpoints (Top, Back, LHS, RHS).  The metrics shown are Precision (P), Recall (R), F1-score (F1), and Intersection over Union (IoU), providing a comprehensive evaluation of OPEL's effectiveness in handling various exocentric perspectives.  This demonstrates OPEL's generalization capabilities.", "section": "A.5 Third-Person Video Perspective"}, {"figure_path": "leqD3bJ4Ly/tables/tables_17_2.jpg", "caption": "Table A4: Results on individual subtasks of egocentric datasets.", "description": "This table presents a detailed breakdown of the performance of the proposed OPEL model on individual subtasks within two egocentric datasets: EGTEA-GAZE+ and CMU-MMAC.  It shows the F1-score and IoU for each subtask, allowing for a granular analysis of the model's performance across various activities. This granular level of detail helps in understanding the strengths and weaknesses of the model in different contexts.", "section": "A.6 Quantitative results of OPEL on different subtasks across both ego- and exo-datasets"}, {"figure_path": "leqD3bJ4Ly/tables/tables_17_3.jpg", "caption": "Table A4: Results on individual subtasks of egocentric datasets.", "description": "This table presents a detailed breakdown of the OPEL model's performance on individual subtasks within egocentric datasets.  It shows the F1-score and IoU for each subtask across different egocentric datasets (EGTEA-GAZE+ and CMU-MMAC). This granular level of detail allows for a more nuanced understanding of the model's strengths and weaknesses across various tasks and provides a more comprehensive evaluation than aggregate scores.", "section": "A.6 Quantitative results of OPEL on different subtasks across both ego- and exo-datasets"}, {"figure_path": "leqD3bJ4Ly/tables/tables_18_1.jpg", "caption": "Table A5: Results on individual subtasks of Third-person exocentric datasets.", "description": "This table presents the performance of the proposed OPEL model and other state-of-the-art methods on individual subtasks within two third-person exocentric datasets: ProceL and CrossTask.  The table shows the F1 score and IoU (Intersection over Union) for each subtask in each dataset, allowing for a granular comparison of model performance across different tasks and datasets.", "section": "Experiments and Results"}, {"figure_path": "leqD3bJ4Ly/tables/tables_18_2.jpg", "caption": "Table 1: Results using EgoProceL [1] demonstrate the superior performance of OPEL. The results in bold and underline denote the highest and second-highest values in a column, respectively.", "description": "This table presents a comparison of the proposed OPEL method against several state-of-the-art (SOTA) approaches for procedure learning on the EgoProceL benchmark dataset.  It shows the F1-score and IoU (Intersection over Union) achieved by each method for several different tasks.  The superior performance of OPEL is highlighted.", "section": "4 Experiments and Results"}, {"figure_path": "leqD3bJ4Ly/tables/tables_19_1.jpg", "caption": "Table A6: Ablation on the choice of distribution function for optimality and temporal priors", "description": "This table presents the results of an ablation study comparing the performance of three different distribution functions (Uniform, Gaussian, and Laplace) used for the optimality and temporal priors in the OPEL framework.  The results are shown for four different subtasks within the EgoProceL benchmark dataset: CMU-MMAC, MECCANO, PC Assembly, and PC Disassembly. Each subtask's performance is evaluated using the F1 score and IoU metrics.", "section": "A.8"}, {"figure_path": "leqD3bJ4Ly/tables/tables_20_1.jpg", "caption": "Table A7: Ablation study for hyperparameters \u03bb\u2081 and \u03bb\u2082", "description": "This table presents the ablation study on the impact of different values of hyperparameters \u03bb\u2081 and \u03bb\u2082 on the performance of the proposed OPEL model.  It shows the F1-score and IoU for different values of these hyperparameters across four different datasets (CMU-MMAC, MECCANO, PC Assembly, and PC Disassembly) from the EgoProceL benchmark. The results illustrate the robustness of the model with respect to the choice of hyperparameters, indicating that the best performance is achieved with \u03bb\u2081 = (N+M) and \u03bb\u2082 = 0.1*N*M.", "section": "A.8.2 Hyperparameters \u03bb\u2081 and \u03bb\u2082"}]