[{"figure_path": "fTKcqr4xuX/figures/figures_4_1.jpg", "caption": "Figure 1: Illustration of relative signal strength for binary classification. Left: clean and noisy posteriors [\u03b7(x)]\u2081 = P (Y = 1|X = x) and [\u1fc6(x)]\u2081 = P(\u1ef8 = 1|X = x). Right: relative signal strength corresponding to these posteriors. The gray region, x \u2208 (0, 5), is where the true and noisy Bayes classifiers differ, and is also the zero signal region X \\ A\u2080. The red region is A\u2080.\u2084, where the RSS is > 0.4. Note that as x \u2191 0, M(x; \u03b7, \u1fc6) \u2191 \u221e, which occurs since [\u03b7(x)]\u2081 \u2191 1/2, while [\u1fc6]\u2081 is far from 1/2. For x \u2192 0+, the predicted labels under \u03b7 and \u1fc6 disagree, and the RSS crashes to 0.", "description": "This figure illustrates the concept of relative signal strength (RSS) in binary classification.  The left panel shows the clean and noisy class posterior probabilities, P(Y=1|X=x) and P(\u1ef8=1|X=x) respectively, as functions of the feature x. The right panel displays the calculated RSS, showing how the \"signal\" (information about the true label) varies with x.  The gray region highlights where the clean and noisy Bayes classifiers disagree (zero signal strength), while the red region indicates where the RSS exceeds 0.4 (high signal strength). The figure demonstrates how RSS quantifies the transferability of information from noisy to clean data, a key concept in the paper's theoretical framework.", "section": "4 Relative signal strength"}, {"figure_path": "fTKcqr4xuX/figures/figures_7_1.jpg", "caption": "Figure 1: Illustration of relative signal strength for binary classification. Left: clean and noisy posteriors [\u03b7(x)]\u2081 = P (Y = 1|X = x) and [\u1fc6(x)]\u2081 = P(\u1ef8 = 1|X = x). Right: relative signal strength corresponding to these posteriors. The gray region, x \u2208 (0,5), is where the true and noisy Bayes classifiers differ, and is also the zero signal region X \\ A\u2080. The red region is A\u2080.\u2084, where the RSS is > 0.4. Note that as x \u2191 0, \u039c(x; \u03b7, \u1fc6) \u2191 \u221e, which occurs since [\u03b7(x)]\u2081 \u2191 1/2, while [\u1fc6]\u2081 is far from 1/2. For x \u2192 0+, the predicted labels under \u03b7 and \u1fc6 disagree, and the RSS crashes to 0.", "description": "The figure illustrates the concept of relative signal strength (RSS) in binary classification. The left panel shows the clean and noisy class posterior probabilities as a function of the input feature x. The right panel displays the corresponding RSS values. The gray region represents the area where clean and noisy Bayes classifiers differ (zero RSS), while the red region highlights where RSS exceeds 0.4.", "section": "Relative signal strength"}, {"figure_path": "fTKcqr4xuX/figures/figures_7_2.jpg", "caption": "Figure 2: Data simulation that verifies noise immunity. For binary, the turning point is at noise rate P(\u1ef8 \u2260 Y) = 0.5. For 10-class, the turning point is at P(\u1ef8 \u2260 Y) = 0.9.", "description": "This figure shows the results of data simulations to verify the theoretical concept of noise immunity. Two experiments are conducted: one with binary classification and another with 10-class classification, both using symmetric label noise.  The x-axis represents the noise rate, which is the probability that the noisy label \u1ef8 is different from the true label Y. The y-axis represents the testing accuracy. The results show a sharp drop in accuracy when the noise rate exceeds a threshold (0.5 for binary and 0.9 for 10-class), confirming the theoretical prediction of noise immunity.", "section": "6 Conditions that ensure noise immunity"}, {"figure_path": "fTKcqr4xuX/figures/figures_8_1.jpg", "caption": "Figure 3: A linear model trained on features obtained from either transfer learning (pretrained ResNet-50 on ImageNet [He et al., 2016]), self-supervised learning (ResNet-50 trained on CIFAR-10 images with contrastive loss [Chen et al., 2020]), or a pretrained self-supervised foundation model DINOv2 [Oquab et al., 2023] significantly boosts the performance of the original linear model. In contrast, full training of a ResNet-50 leads to overfitting.", "description": "The figure shows the performance comparison of different models on CIFAR-10 with synthetic and realistic label noise.  A linear model is used on top of different feature extractors, showing that using pre-trained models (transfer learning or self-supervised learning) significantly improves performance, especially with high noise levels, while training a deep network directly on noisy data leads to overfitting. ", "section": "7 Practical implication"}, {"figure_path": "fTKcqr4xuX/figures/figures_8_2.jpg", "caption": "Figure 3: A linear model trained on features obtained from either transfer learning (pretrained ResNet-50 on ImageNet [He et al., 2016]), self-supervised learning (ResNet-50 trained on CIFAR-10 images with contrastive loss [Chen et al., 2020]), or a pretrained self-supervised foundation model DINOv2 [Oquab et al., 2023] significantly boosts the performance of the original linear model. In contrast, full training of a ResNet-50 leads to overfitting.", "description": "This figure shows the results of experiments conducted to evaluate the performance of different methods for learning with noisy labels on the CIFAR-10 dataset.  The methods compared include using a linear model trained on top of features extracted by transfer learning, self-supervised learning, and a pretrained self-supervised model (DINOv2).  The figure highlights that using a linear classifier on top of pre-trained feature extractors leads to significantly improved robustness to noisy labels compared to training a ResNet-50 end-to-end.", "section": "7 Practical implication"}, {"figure_path": "fTKcqr4xuX/figures/figures_9_1.jpg", "caption": "Figure 1: Illustration of relative signal strength for binary classification. Left: clean and noisy posteriors [n(x)]\u2081 = P (Y = 1|X = x) and [\u1fc6(x)]\u2081 = P(\u1ef8 = 1|X = x). Right: relative signal strength corresponding to these posteriors. The gray region, x \u2208 (0,5), is where the true and noisy Bayes classifiers differ, and is also the zero signal region X \\ A\u2080. The red region is A\u2080.\u2084, where the RSS is > 0.4. Note that as x \u2191 0, M(x; \u03b7, \u1fc6) \u2191 \u221e, which occurs since [n(x)]\u2081 \u2191 1/2, while [\u1fc6]\u2081 is far from 1/2. For x \u2192 0+, the predicted labels under \u03b7 and \u1fc6 disagree, and the RSS crashes to 0.", "description": "This figure illustrates the concept of relative signal strength (RSS) in binary classification.  The left panel shows the clean and noisy class posterior probabilities, P(Y=1|X=x) and P(\u1ef8=1|X=x) respectively, as functions of the feature x.  The right panel plots the RSS, which quantifies the difference between the clean and noisy posteriors.  The gray region highlights where the clean and noisy Bayes classifiers disagree (zero RSS), while the red region shows where the RSS exceeds 0.4. The figure demonstrates how the RSS can be infinite when the clean posterior is close to 0.5 but the noisy posterior is far from 0.5, and how the RSS drops to zero when the Bayes classifiers disagree.", "section": "4 Relative signal strength"}, {"figure_path": "fTKcqr4xuX/figures/figures_31_1.jpg", "caption": "Figure 1: Illustration of relative signal strength for binary classification. Left: clean and noisy posteriors [n(x)]\u2081 = P (Y = 1|X = x) and [\u1fc6(x)]\u2081 = P(Y = 1|X = x). Right: relative signal strength corresponding to these posteriors. The gray region, x \u2208 (0,5), is where the true and noisy Bayes classifiers differ, and is also the zero signal region X \\ A\u2080. The red region is A\u2080.\u2084, where the RSS is > 0.4. Note that as x \u2191 0, M(x; \u03b7, \u1fc6) \u2191 \u221e, which occurs since [\u03b7(x)]\u2081 \u2191 1/2, while [\u1fc6]\u2081 is far from 1/2. For x \u2192 0+, the predicted labels under \u03b7 and \u1fc6 disagree, and the RSS crashes to 0.", "description": "This figure illustrates the concept of relative signal strength (RSS) in binary classification.  The left panel shows the clean and noisy class posterior probabilities (P(Y=1|X=x) and P(\u1ef8=1|X=x) respectively) as functions of the feature x.  The right panel plots the corresponding RSS values.  The gray region highlights where the clean and noisy Bayes classifiers disagree (zero RSS), while the red region shows where the RSS is above 0.4. The figure demonstrates how RSS quantifies the amount of signal (information about the true label) present in the noisy label, relative to the clean label.", "section": "Relative signal strength"}]