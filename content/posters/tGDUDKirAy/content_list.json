[{"type": "text", "text": "Verified Safe Reinforcement Learning for Neural Network Dynamic Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Junlin Wu ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Huan Zhang ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Computer Science & Engineering Washington University in St. Louis junlin.wu@wustl.edu ", "page_idx": 0}, {"type": "text", "text": "Electrical & Computer Engineering University of Illinois Urbana-Champaign huan@huan-zhang.com ", "page_idx": 0}, {"type": "text", "text": "Yevgeniy Vorobeychik Computer Science & Engineering Washington University in St. Louis yvorobeychik@wustl.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Learning reliably safe autonomous control is one of the core problems in trustworthy autonomy. However, training a controller that can be formally verified to be safe remains a major challenge. We introduce a novel approach for learning verified safe control policies in nonlinear neural dynamical systems while maximizing overall performance. Our approach aims to achieve safety in the sense of finite-horizon reachability proofs, and is comprised of three key parts. The first is a novel curriculum learning scheme that iteratively increases the verified safe horizon. The second leverages the iterative nature of gradient-based learning to leverage incremental verification, reusing information from prior verification runs. Finally, we learn multiple verified initial-state-dependent controllers, an idea that is especially valuable for more complex domains where learning a single universal verified safe controller is extremely challenging. Our experiments on five safe control problems demonstrate that our trained controllers can achieve verified safety over horizons that are as much as an order of magnitude longer than state-of-the-art baselines, while maintaining high reward, as well as a perfect safety record over entire episodes. Our code is available at https://github.com/jlwu002/VSRL. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The ability to synthesize safe control policies is one of the core challenges in autonomous systems. This problem has been explored from numerous directions across multiple disciplines, including control theory and AI [Achiam et al., 2017, Dawson et al., 2022]. While considerable progress has been made, particularly when dynamics are linear [Wabersich and Zeilinger, 2018], the ability to synthesize controllers that can be successfully verified to be safe while maintaining high performance in nonlinear dynamical systems remains a major open problem. Indeed, even the subproblem of safety verification in nonlinear systems is viewed in itself as a major challenge and is an active area of research, particularly for neural network controllers [Bastani et al., 2018, Ivanov et al., 2019, Wei and Liu, 2022]. State-of-the-art approaches for safe control synthesis, including most that leverage reinforcement learning [Gu et al., 2022], typically only offer empirical evaluation of safety, and rely on safety proofs that hold either asymptotically (rather than for concrete problems) [Xiong et al., 2024] or under idealized assumptions which do not hold in practice [Berkenkamp et al., 2017]. ", "page_idx": 0}, {"type": "text", "text": "Two common properties are typically leveraged in safety verification: forward invariance and reachability. The former aims to identify a set of starting subsets of safe states under which one-step (forward) dynamics remain in this (forward invariant) set. The latter computes the set of states that can possibly be reached after $K$ steps of the dynamics for a given control policy, and checks whether it intersects with the unsafe set. Approaches for synthesizing (including those that do so using learning) safe policies almost exclusively aim to achieve verified safety through forward invariance. However, this has proved extremely challenging to employ beyond the simplest dynamics. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "We propose the first (to our knowledge) approach for learning $K$ -step verified safe neural network controllers that also aim to maximize efficiency in systems with neural dynamics. While neural dynamics are clearly not universal, they can capture or effectively approximate a broad range of practical dynamical systems [Nagabandi et al., 2018], and have consequently been the focus of much prior work in safe control and verification [Dai et al., 2021]. For example, consider the scenario of a drone navigating through a series of obstacles to reach a designated goal, requiring $K=50$ steps to safely maneuver through the obstacles. We aim to train a controller that can reach the goal as fast as possible, while guaranteeing safety for the initial 50 steps, ensuring 1) the drone does not collide with any obstacles and 2) its angle remains within a predefined safe range. ", "page_idx": 1}, {"type": "text", "text": "Our approach combines deep reinforcement learning with state-of-the-art differentiable tools for efficient reachability bound computation, and contains two key novel ingredients. The first is a novel curriculum learning scheme for learning a verified safe controller. This scheme takes advantage of the structure of the $K$ -reachability problem at the root of our safety verification by creating a curriculum sequence with respect to increasing $K$ . An important insight that is specific to the verification setting is that verification must work not merely for a fixed $K$ , but for all steps prior, an issue we address by memorizing subsets of states who either violate, or nearly violate, safety throughout the entire $K$ -step curriculum learning process. Additionally, to maintain both strong empirical and verified performance, we propose a novel loss function that integrates overall reward, as well as both traditional (empirical) safety loss along with the $K$ -reachability bound. Our second innovation is to learn a collection of controllers that depend on the initial state, in contrast to typical approaches that focus on learning a single \u201cuniversal\u201d controller. The ability to allow for learning multiple controllers makes the verified learning problem considerably easier, as we can \u201csave\u201d controllers that work on a subset of initial states, and simply try learning a new controller for the rest, guaranteeing incremental improvement through the learning process. We further improve performance through incremental verification, which leverages information obtained in previous learning iterations. ", "page_idx": 1}, {"type": "text", "text": "We evaluate the proposed approach in five control settings. The first two are lane following and obstacle avoidance, both pertaining to autonomous driving. The last three involve drone control with obstacle avoidance. Two of these consider fixed obstacles, while the third aims to avoid even moving obstacles (with known dynamics). We show that the proposed approach outperforms five state-of-the-art safe control baselines in the ability to achieve verified safety without significantly compromising overall reward (efficiency). In particular, our approach learns controllers that can verify $K$ -step safety for $K$ up to an order of magnitude larger than the prior art and maintains a perfect safety record for $K$ far above what we verify, something no baseline can achieve. ", "page_idx": 1}, {"type": "text", "text": "In summary, we make the following contributions: ", "page_idx": 1}, {"type": "text", "text": "1. A framework for safe optimal control that combines both finite-horizon verified (worst-case) and empirical (average-case) safety constraints.   \n2. A novel curriculum learning approach that leverages memorization, forward reachability analysis, and differentiable reachability overapproximation for efficiently learning verified safe policies.   \n3. An approach for learning a collection of control policies that depend on the initial state which enables significant improvements in verified safety horizon over large initial state sets $S_{0}$ .   \n4. An incremental verification approach that leverages small changes in gradient-based learning to improve verification efficiency during learning.   \n5. An extensive experimental evaluation that demonstrates the efficacy of the proposed approach in comparison with five state-of-the-art safe RL baselines. ", "page_idx": 1}, {"type": "text", "text": "Related Work: Safe reinforcement learning has been extensively studied through the lens of constrained Markov decision process (CMDP)-based approaches, which represent cost functions as constraints and aim to maximize reward while bounding cost, using approaches such as Lagrangian and penalty methods, and constrained policy optimization [Achiam et al., 2017, Stooke et al., 2020, Ma et al., 2022, Jayant and Bhatnagar, 2022, Yu et al., 2022, So and Fan, 2023, Ganai et al., 2024]. ", "page_idx": 1}, {"type": "text", "text": "An alternative control-theoretic perspective aims to ensure stability or safety using Lyapunov and control barrier functions. For example, Dawson et al. [2022] used a learning-based approach to find robust control Lyapunov barrier functions; Chow et al. [2018] constructed Lyapunov functions to solve CMDPs; [Wang et al., 2023a] proposed soft barrier functions for unknown and stochastic environments; and Alshiekh et al. [2018] created safety shielding for safe RL agents. These approaches, however, provide no practical formal safety guarantee for neural network controllers. In addition, some work on provably safe RL focuses on the probabilistic setting [Berkenkamp et al., 2017, Jansen et al., 2020, Xiong et al., 2024] and required statistical assumptions, whereas our work aims for strict deterministic safety guarantees over a finite horizon. ", "page_idx": 2}, {"type": "text", "text": "Among existing works focusing on safe RL with formal guarantees, Fulton and Platzer [2018] apply a theorem prover for differential dynamic logic to guarantee safety during runtime. Noren et al. [2021] and Wei et al. [2022] consider forward safety invariance for systems with uncertainty. Kochdumper et al. [2023] propose to project actions to safe subspace using zonotope abstraction and mixed-integer programming (MIP). However, these approaches do not readily apply to neural network controllers. For systems involving neural networks, Wei and Liu [2022] applied integer programming formulation for neural networks to solve an MIP problem to find safe control actions satisfying forward invariance; Bastani et al. [2018] extracted decision-tree-based policies for RL to reduce verification complexity; and Ivanov et al. [2019] used hybrid system verification tools to model deep neural networks. Our work differs from these and similar approaches because we consider forward reachability guarantees for neural network controllers in neural nonlinear systems. ", "page_idx": 2}, {"type": "text", "text": "We make extensive use of neural network verification tools. Early work in this vein used SMT [Katz et al., 2017, Huang et al., 2017] or MIP-based [Tjeng et al., 2019] approaches to solve this problem, but their scalability is extremely limited. Significant progress has been made in developing techniques to formally verify the properties of large neural networks through overapproximation, such as bound propagation [Zhang et al., 2018, Gowal et al., 2018, Xu et al., 2021], optimization [Qin et al., 2019, Dvijotham et al., 2018, 2020], and abstract interpretation [Gehr et al., 2018, Singh et al., 2019, Katz et al., 2019, Lopez et al., 2023]. Recently, most verifiers have adopted branch-and-bound based approaches to further enhance their performance [Wang et al., 2021, Kouvaros and Lomuscio, 2021, Ferrari et al., 2022, Zhang et al., 2022]. Our approach makes use of differentiable overapproximation methods known collectively as $\\alpha,\\beta$ -CROWN [Wang et al., 2021, Zhang et al., 2022] (implemented with the auto_LiRPA package), and takes advantage of the particular structure of these verification approaches in applying incremental verification to significantly speed up safe controller learning. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Constrained Markov Decision Process (CMDP): We consider a deterministic Constrained Markov Decision Process (CMDP) defined by the tuple $(S,A,F,R,\\gamma,C_{1},C_{2},\\ldots,C_{m},d_{1},d_{2},\\ldots,d_{m}),$ where: $\\boldsymbol{S}$ is a set of states, $\\boldsymbol{\\mathcal{A}}$ is a set of actions, $F:S\\times A\\to S$ is the deterministic state transition function, $R:S\\times A\\to\\mathbb{R}$ is the reward function, $C_{i}:S\\times A\\rightarrow\\mathbb{R}$ is the cost function for the $i$ -th constraint, $d_{i}$ is the cost limit for the $i$ -th constraint, and $\\gamma\\in[0,1)$ is the discount factor. A policy $\\pi:{\\mathcal{S}}\\rightarrow A$ is a mapping from states to actions. A trajectory is a sequence of states and actions generated by following a policy $\\pi$ from some initial state $s_{0}\\in\\mathcal{S}_{0}\\subseteq\\mathcal{S}$ , which can be represented as a sequence $\\tau=(s_{0},a_{0},s_{1},a_{1},s_{2},a_{2},\\dots)$ where $s_{t}\\in\\mathcal S$ , $a_{t}=\\pi(s_{t})$ for all $t$ , $s_{t+1}=F(s_{t},a_{t})$ , a reward $r_{t}=R(s_{t},a_{t})$ and a cost $\\begin{array}{r}{c_{t}=\\sum_{i\\in[m]}C_{i}(s_{t},a_{t})}\\end{array}$ are received after each action. ", "page_idx": 2}, {"type": "text", "text": "We denote $\\pi_{\\theta}$ as the policy that is parameterized by the parameter $\\theta$ . A common goal for CMDP is to learn a policy $\\pi_{\\theta}$ that maximizes a discounted sum of rewards $\\mathcal{I}(\\pi_{\\theta})$ while ensuring that expected discounted costs ${\\mathcal{I}}_{C_{i}}(\\pi_{\\theta})$ do not exceed the cost limit $d_{i}$ , $\\forall i\\in[m]$ . Formally, CMDP is to solve the below optimization problem: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\theta}\\mathcal{I}(\\pi_{\\theta})\\quad\\mathrm{s.t.~}\\mathcal{I}_{C_{i}}(\\pi_{\\theta})\\leq d_{i},\\forall i\\in[m],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathcal{I}(\\pi_{\\theta})=\\mathbb{E}_{\\tau\\sim\\pi}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}R(s_{t},a_{t})\\right]}\\end{array}$ and $\\begin{array}{r}{\\mathcal{I}_{C_{i}}(\\pi_{\\theta})=\\mathbb{E}_{\\tau\\sim\\pi}\\,[\\sum_{t=0}^{\\infty}\\gamma^{t}C_{i}(s_{t},a_{t})].}\\end{array}$ ", "page_idx": 2}, {"type": "text", "text": "Verified Safe CMDP: We define the state space as the union of predefined safe and unsafe states, denoted as $S\\;=\\;S_{\\mathrm{safe}}\\;\\cup\\;S_{\\mathrm{unsafe}}\\;$ . We assume that the transition function $F$ is represented by a ReLU neural network, and is known for verification purposes. This assumption is very general, as many known dynamical systems can be represented exactly or approximately using ReLU neural networks [Gillespie et al., 2018, Pfrommer et al., 2021, Dai et al., 2021, Liu et al., 2024]. Our objective is to train a controller that not only satisfies safety constraints empirically at decision time, but also ensures verified safety for the first $K$ steps. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Formally, we aim to solve the following optimization problem: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\underset{\\theta}{\\operatorname*{max}}\\,\\mathcal{I}(\\pi_{\\theta})}&\\\\ &{\\mathrm{t.}\\,\\,\\mathcal{I}_{C_{i}}(\\pi_{\\theta})\\le d_{i},\\quad\\forall i\\in[m]\\quad\\mathrm{(empirically~satisfied)}}\\\\ &{\\quad s_{t}\\in S_{\\mathrm{safe}},\\quad\\forall t\\in[K]\\quad\\mathrm{(mathematically~verified)}}&\\\\ &{\\quad s_{t+1}=F(s_{t},a_{t}),a_{t}=\\pi_{\\theta}(s_{t}),s_{0}\\in S_{0}\\subseteq S_{\\mathrm{safe}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In particular, we aim to solve (2) for high values of $K$ and large sets of verified safe initial states ${\\mathcal{S}}_{0}$ , while preserving a high objective value. Note that for a given controller, $(2\\mathrm{c})$ can also be interpreted as a set of forward reachability verification problems. However, our interest here extends beyond mere verification; we aim to train (synthesize) a controller that can be efficiently verified for safety. For simplicity, we restrict attention to $d_{i}=0$ for all $i$ ; however, our approach can be directly applied to arbitrary values of $d_{i}$ . ", "page_idx": 3}, {"type": "text", "text": "In this work, we primarily utilize the $\\alpha,\\beta$ -CROWN toolbox $[\\mathrm{Xu}$ et al., 2021, Wang et al., 2021] for neural network (NN) verification; however, our training framework is general and, in principle, can work with any differentiable verification technique. Let $F^{k,\\pi_{\\theta}}$ denote the $k$ -step forward function (iterative composition of $F$ ) under policy $\\pi_{\\theta}$ . For example, $s_{1}=F^{1,\\pi_{\\theta}}(s)=F\\bar{(}s,\\pi_{\\theta}(s))$ , $F^{2,\\pi_{\\theta}}(s)=F(s_{1},\\pi_{\\theta}(\\dot{s_{1}}))$ , and so on. Correspondingly, we represent the $k$ -step forward reachable regions returned by the NN verifier for an initial state set $S$ as $F_{\\mathrm{Bound}}^{k,\\pi_{\\theta}}(S)$ , which is typically represented as a box. ", "page_idx": 3}, {"type": "text", "text": "3 Approach ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The problem of learning verified safe control over a target horizon $K$ entails three key technical challenges. The first is that as $K$ grows, the differentiable overapproximation techniques for reachability verification become looser, making it difficult to verify $K$ beyond very small horizons. Second, while control policies $\\pi$ depend on state, it is difficult to find a single universal controller that can achieve verified safety for each starting state in ${\\mathcal S}_{0}$ . Our approach addresses these challenges through three technical advances: 1) curriculum learning with memorization and 2) incremental verification, which enable learning verified safe controllers over longer horizons $K$ , and 3) iterative learning a collection of controllers customized for subsets of ${\\mathcal{S}}_{0}$ , which addresses the third challenge above. ", "page_idx": 3}, {"type": "text", "text": "Curriculum Learning with Memorization: Curriculum learning is an iterative training strategy where the difficulty of the task increases as training progresses [Bengio et al., 2009, Wu and Vorobeychik, 2022]. At a high level, for a problem targeting $K$ -step verified safety, training can be divided into $K$ phases, with each phase $k$ aiming to achieve verified safety at the corresponding $k$ -th forward step. In the $k$ -th phase, we conduct formal verification against the $k$ -th step safety, filter out regions that cannot be verified, and use them for further training. As $k$ increases, the task difficulty also increases, mainly due to the forward NN $F^{k,\\pi_{\\theta}}$ becoming deeper. For a deeper NN and a fixed branching budget, the output bounds become looser [Wang et al., 2021], increasing the likelihood of intersections with unsafe regions. However, the ability to verify safety in prior steps enables us to tailor a controller that closely aligns with the fixed NN dynamics, thereby achieving tighter bounds. This process captures the essence of curriculum learning. ", "page_idx": 3}, {"type": "text", "text": "Nevertheless, our approach deviates from traditional curriculum learning in a way that is quite consequential for our setting: we aim to ensure that a controller is verified as safe not only for the $k$ -th step but also maintains safety for all prior steps. Consequently, during our curriculum learning process, we store states that are close to being unsafe in each phase in a buffer, effectively memorizing information about regions that potentially violate safety. These states, along with the unverified states at the current phase, are then incorporated into the training process, helping to ensure safety across the entire $K$ -step horizon. Our curriculum training framework is detailed in Algorithm 1. ", "page_idx": 3}, {"type": "text", "text": "The process begins by initializing the policy $\\pi_{\\theta}$ with a pre-trained policy using safe RL algorithms (Line 3). We then split the initial region ${\\mathcal{S}}_{0}$ into a grid $\\mathcal{G}_{0}$ (Line 4). We assume ${\\mathcal{S}}_{0}$ is a $m$ -dimensional box centered at $s_{c}\\in\\mathbb{R}^{m}$ with a radius $r\\in\\mathbb{R}^{m}$ , i.e., $S_{0}=[s_{c}-r,s_{c}+r]$ . We prioritize splitting the dimensions that are directly implicated in safety constraints, thereby taking advantage of the ", "page_idx": 3}, {"type": "text", "text": "1: Input: target safety horizon $K$ , initial region ${\\mathcal S}_{0}$ , unsafe region $\\ensuremath{S_{\\mathrm{unsafe}}}$ , max attempt $n_{\\mathrm{max}}$   \n2: Output: controller $\\pi_{\\theta}$   \n3: Initialize $\\pi_{\\theta}$ with pre-trained policy, buffer $B=\\{\\}$   \n4: Split initial region ${\\mathcal S}_{0}$ into grid $\\mathcal{G}_{0}$   \n5: for $k=1,2,\\ldots,K$ do   \n6: $n_{\\mathrm{train}}\\leftarrow0$   \n7: $S_{u c}\\gets F_{\\mathrm{Bound}}^{k,\\pi_{\\theta}}(\\mathcal{G}_{0})\\cap S_{\\mathrm{unsafe}}\\left//$ optionally use Branch-and-Bound to refine $\\mathcal{G}_{\\mathrm{0}}$   \n8: while $n_{\\mathrm{train}}<n_{\\mathrm{max}}$ and $S_{u c}\\ne\\emptyset$ do   \n9: Safe RL training with loss function $\\mathcal{L}(x)=\\mathcal{L}_{\\mathrm{SafeRL}}(x)+\\lambda\\mathcal{L}_{\\mathrm{Bound}}(S_{u c}\\cup B)$   \n10: $\\begin{array}{l}{{n_{\\mathrm{train}}\\leftarrow n_{\\mathrm{train}}+1}}\\\\ {{S_{u c}\\leftarrow F_{\\mathrm{Bound}}^{k,\\pi_{\\theta}}(\\mathcal{G}_{0})\\cap S_{\\mathrm{unsafe}}}}\\end{array}$   \n11:   \n12: end while   \n13: Filter regions $S_{k}\\subseteq\\mathcal{G}_{0}$ such that $\\mathrm{dist}(F_{\\mathrm{Bound}}^{k,\\pi_{\\theta}}(S_{k}),S_{\\mathrm{unsafe}})<\\epsilon$ , store $(S_{k},k)$ in buffer $B$   \n14: end for ", "page_idx": 4}, {"type": "text", "text": "typical structure of safety constraints that only pertain to a small subset of state variables. For instance, in drone control for obstacle avoidance, we prioritize splitting the location and angle axes. Next, we design a cost function $C_{R}$ for regions where $\\dot{C}_{R}(S)=\\overline{{0}}$ if $S\\bigcap S_{\\mathrm{unsafe}}=\\emptyset$ , and $\\bar{C_{R}}(S)>$ 0 otherwise. A positive $C_{R}$ means region $S$ intersects with $\\ensuremath{S_{\\mathrm{unsafe}}}$ , while $C_{R}\\,=\\,0$ indicates $S$ is safe. For example, if the task is to avoid the region $[a,b]$ , and the output bounds are given by $x_{B}=[x_{l b},x_{u b}]$ , we can define $C_{R}(x_{B})=\\operatorname*{max}(x_{u b}-\\bar{a_{}},0)\\,\\bar{\\cdot}\\operatorname*{max}(b-x_{l b},0)$ . We then calculate the tghrea dliaerngte $\\partial C_{R}(F_{\\mathrm{Bound}}^{t,\\pi_{\\theta}}(S_{0}))/\\partial r$ a  floarr ga ecr hgorsaedni evnatl iune doicf $t$ easn da  phirgocheere dli tkoe lsiphloito adl oofn gr etdhue cdiinmg etnhsei ocnoss t $C_{R}$ We continue this process, keeping the total number of grid splits within a predetermined budget, and stop splitting once the budget is reached. ", "page_idx": 4}, {"type": "text", "text": "For each training phase $k$ , we monitor the training rounds $(n_{\\mathrm{train}})$ as well as the $k$ -step forward reachable regions returned by the verifier that are identified as unsafe $(S_{u c})$ . Each phase is conducted for a maximum of $n_{\\mathrm{max}}$ rounds or until verified $k$ -step safety is achieved, that is, when $S_{u c}=\\emptyset$ (wLiitnhie n . diAstt athnec ee tnod  tohfe  euancsha tfrea irneignigo npsh. aTseh,e swee  raelgsioo nfsli taerr eo tuht erne gsitoornes $S_{k}\\subseteq\\mathcal{G}_{0}$ uwffheer $F_{\\mathrm{Bound}}^{k,\\pi_{\\theta}}(S_{k})$ aWree $\\epsilon$ $B$ include these critical regions in the training set for each reinforcement learning update to enhance verified safety across the entire horizon. During this process, we optionally use the Branch-and-Bound algorithm [Everett et al., 2020, Wang et al., 2021] to refine $\\mathcal{G}_{\\mathrm{0}}$ up to a predetermined branching limit, which helps achieve tighter bounds. ", "page_idx": 4}, {"type": "text", "text": "For each RL update, we use a loss function that integrates the standard safe RL loss with a $k$ -phase loss for bounds (Line 9), where ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\mathcal{L}(x)=\\mathcal{L}_{\\mathrm{SafeRL}}(x)+\\lambda\\mathcal{L}_{\\mathrm{Bound}}(S_{u c}\\cup B)}\\\\ {\\mathcal{L}_{\\mathrm{Bound}}(S_{u c}\\cup B)=C_{R}(F_{\\mathrm{Bound}}^{k,\\pi_{\\theta}}(S_{u c}))+\\displaystyle\\sum_{(S_{i},i)\\in B}C_{R}(F_{\\mathrm{Bound}}^{i,\\pi_{\\theta}}(S_{i})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, $\\mathcal{L}_{\\mathrm{SafeRL}}$ is the standard safety RL loss, and $\\mathcal{L}_{\\mathrm{Bound}}$ denotes the loss that incentivizes ensuring the output bounds returned by the verifier remain within the safe region. If both $S_{u c}$ is $k$ -step safe and $\\forall(\\mathbf{\\bar{{S}}}_{i},i)\\in B,$ $S_{i}$ is $i$ -step safe, then $\\mathcal{L}_{\\mathrm{Bound}}\\big(S_{u c}\\cup B\\big)=0$ , otherwise, $\\mathcal{L}_{\\mathrm{Bound}}\\big(S_{u c}\\cup B\\big)>0$ In practice, we clip $\\mathcal{L}_{\\mathrm{Bound}}(S_{u c}\\cup B)$ to ensure it remains within a reasonable range for training stability. The regularization parameter $\\lambda$ is calculated based on the magnitude of $\\mathcal{L}_{\\mathrm{SafeRL}}$ and $\\mathcal{L}_{\\mathrm{Bound}}$ , with $\\dot{\\lambda^{2}}=\\operatorname*{min}(\\bar{\\lambda}_{\\operatorname*{max}},a_{r}\\cdot\\mathcal{L}_{\\mathrm{SafeRL}}/\\mathcal{L}_{\\mathrm{Bound}})$ , where $\\lambda_{\\operatorname*{max}}$ and $a_{r}$ are hyperparamters. This approach helps maintain the effectiveness of bound training, especially when $\\mathcal{L}_{\\mathrm{Bound}}$ is small. Furthermore, we cluster elements in $B$ into categories so that we do not need to construct a computational graph for all $i<k$ . Specifically, we merge all $S_{i}$ for $i_{1}\\leq i\\leq i_{2}$ into the $i_{2}$ category, meaning the elements in $B$ are now $\\left(\\cup_{i_{1}\\leq i\\leq i_{2}}S_{i},i_{1},i_{2}\\right)$ instead of $(S_{i},i)$ . ", "page_idx": 4}, {"type": "text", "text": "It is important to note that while our training scheme targets $K$ -step verified safety, the policy returned by Algorithm 1 does not necessarily guarantee it. We address this issue by learning initial-statedependent controllers as described below. Furthermore, the computation of $\\mathcal{L}_{\\mathrm{Bound}}$ is computationally intensive. Its backpropagation requires constructing computational graphs for the $k$ -th step forward ", "page_idx": 4}, {"type": "text", "text": "NN $F_{\\mathrm{Bound}}^{k,\\pi_{\\theta}}$ , as well as for all $i$ -th step forward NNs corresponding to each $(S_{i},i)\\in B$ . These NNs become increasingly deep as $k$ grows, causing the computational graphs to consume memory beyond the typical GPU memory limits. We will address this next. ", "page_idx": 5}, {"type": "text", "text": "Incremental Verification: Above we discussed the challenge presented by the backpropagation of $\\mathcal{L}_{\\mathrm{bound}}$ , which is GPU-memory intensive and does not scale efficiently as the target $K$ -step horizon increases. To mitigate these issues, we propose the use of incremental verification to enhance computational efficiency and reduce memory consumption. While incremental verification is wellexplored in the verification literature Wang et al. [2023b], Althoff [2015], to our knowledge, we are the first to apply it in training provably safe controllers. ", "page_idx": 5}, {"type": "text", "text": "At a high level, to calculate the reachable region for a $k_{\\mathrm{target}}$ step, we decompose the verification into multiple phases. We begin by splitting the $k_{\\mathrm{target}}$ horizon into intervals defined by $0<k_{1}<k_{2}<$ $\\cdots<k_{n}=k_{\\mathrm{target}}$ . We first calculate the reachability region for the $k_{i}$ step and then use its output bounds as input to calculate the reachable region for the $k_{i+1}$ step. This approach ensures that the computational graph is only built for the $(k_{i+1}-k_{i})$ step horizon when using $\\alpha,\\beta^{.}$ -CROWN. ", "page_idx": 5}, {"type": "text", "text": "Unlike traditional incremental verification, which typically calculates the reachable region from $k$ to $k+1$ , we incrementally verify and backpropagate several steps ahead in a single training iteration (i.e., from $k_{i}$ to $k_{i+1}$ , where $k_{i+1}-k_{i}>1)$ ). This generalized version of incremental verification is essential for training, as it significantly accelerates the process and reduces the likelihood of becoming trapped in \"local optima,\" where inertia from the policy obtained for $k$ prevents successful verification for $k+1$ (e.g., due to proximity to the unsafe region with velocity directed toward it). ", "page_idx": 5}, {"type": "text", "text": "For the bounds used in neural network training, we effectively build the computational graph and perform backpropagation using a neural network sized for $(\\dot{k_{n}}-k_{n-1})$ steps\u2019 reachability, which is independent of $k_{\\mathrm{target}}$ . This significantly reduces GPU memory usage. Since $F^{k,\\pi_{\\theta}}$ is an iterative composition of $F$ under the same policy $\\pi_{\\theta}$ , the bound for $k_{n-1}$ steps tends to be tight. Moreover, when training $\\pi_{\\theta}$ to tighten these bounds, the overall bound for the entire $k_{\\mathrm{target}}$ horizon becomes increasingly tight. ", "page_idx": 5}, {"type": "text", "text": "Initial-State-Dependent Controller: While curriculum learning above includes verification steps, it does not guarantee verified safety for the controller over the entire $K$ -step horizon. In this section, we propose using an initial-state-dependent controller to address this issue. For example, in a vehicle avoidance scenario, different initial conditions, such as varying speeds and positions, may correspond to different control strategies. We introduce a mapping function $h:S_{0}\\to\\Theta$ , which maps each initial state $s_{0}\\in\\mathcal S$ to a specific policy $\\pi_{h(s_{0})}$ . The underlying idea is that training a verifiable safe policy $\\pi_{\\theta}$ over the entire set of initial states ${\\mathcal{S}}_{0}$ is inherently challenging. However, by mapping each initial state to a specific set of parameters, we can significantly enhance the expressivity of the policy. This approach is particularly effective in addressing and eliminating corner cases in unverifiable regions. ", "page_idx": 5}, {"type": "text", "text": "At a high level, the mapping and parameter set $\\Theta$ are obtained by first performing comprehensive verification for the controller output from Algorithm 1 over the entire $K$ -step horizon. We then fliter unverified regions, cluster them, and fine-tune the controller parameters $\\theta$ for each cluster. We store these fine-tuned parameters in the parameter set $\\Theta$ . This iterative refinement process continues until for every $s_{0}\\in S_{0}$ , there exists a $\\theta\\in\\Theta$ such that $\\pi_{\\theta}$ is verified safe for the entire $K$ -step horizon. The detailed algorithm is presented in Algorithm 2. ", "page_idx": 5}, {"type": "text", "text": "The algorithm starts with verifying the policy $\\pi_{\\theta}$ obtained from Algorithm 1. The function $\\operatorname{VERIFY}\\operatorname{SAFE}(\\pi_{\\theta},S_{0},K)$ (Line 4) performs verification of policy $\\pi_{\\theta}$ for initial states ${\\mathcal{S}}_{0}$ for the entire horizon $K$ . This verification process identifies and categorizes regions into verified safe areas, $S_{\\mathrm{safe}}^{V}$ , and areas identified as unsafe, $S_{\\mathrm{unsafe}}^{V}$ . Notably, the union of these regions covers all initial states, meaning SsVafe $S_{\\mathrm{safe}}^{V}\\cup S_{\\mathrm{unsafe}}^{V}=S_{0}$ . After verifying that any state is guaranteed to be safe under policy $\\pi_{\\theta}$ , we record $(S_{\\mathrm{unsafe}}^{V},\\pi_{\\theta})$ in the mapping dictionary $H$ (Line 5). ", "page_idx": 5}, {"type": "text", "text": "Next, we address the unsafe regions $S_{\\mathrm{unsafe}}^{V}$ that lack a corresponding verified safe controller. We first cluster them based on the type of safety violation (Line 7). The reason for clustering is that regions with similar safety violations are more likely to be effectively verified safe by the same controller. For instance, in a scenario involving navigation around two obstacles, we could potentially identify up to three clusters: the first corresponding to grids that can lead to collisions with obstacle 1, the second includes grids associated with collisions with obstacle 2, and the third is the set of grids that may lead to collisions with both. Given the finite number of safety constraints, the number of possible clusters is also finite. Although the theoretical maximum number of clusters grows exponentially ", "page_idx": 5}, {"type": "text", "text": "1: Input: target safety horizon $K$ , policy $\\pi_{\\theta}$   \n2: Output: mapping dictionary $H$ , which includes the mapping $h$ and parameter set $\\Theta$   \n3: Initialize $H=\\{\\}$   \n4: $(S_{\\mathrm{safe}}^{V},S_{\\mathrm{unsafe}}^{V})\\gets\\mathrm{VERIFYSAFETY}(\\pi_{\\theta},S_{0},K)$   \n5: Store $(S_{\\mathrm{safe}}^{V},\\theta)$ in mapping dictionary $H$   \n6: while $S_{\\mathrm{unsafe}}^{V}\\neq\\emptyset$ do   \n7: $\\{S_{1},S_{2},\\ldots,S_{I}\\}\\gets\\mathbf{CLUSTERREGION}(S_{\\mathrm{unsafe}}^{V})\\:,$ // cluster based on safety violation   \n8: for $i=1,2,\\dots,I$ do   \n190:: $\\pi_{\\theta^{\\prime}}\\gets\\mathrm{TRAINPOLICY}(\\pi_{\\theta},S_{i},K)$ $(S_{\\mathrm{safe},i}^{V},S_{\\mathrm{unsafe},i}^{V})\\gets\\mathrm{VERIFYSAFETY}(\\pi_{\\theta^{\\prime}},S_{i},K)$   \n11: Store $(S_{\\mathrm{safe},i}^{V},\\theta^{\\prime})$ in mapping dictionary $H$   \n12: end for   \n13: $\\begin{array}{r}{S_{\\mathrm{unsafe}}^{V}\\leftarrow\\bigcup_{i}S_{\\mathrm{unsafe},i}^{V}}\\end{array}$   \n14: end while ", "page_idx": 6}, {"type": "text", "text": "with the number of safety constraints, in practice, this number is significantly smaller. This is due to the fact that the controller, being pretrained, is less likely to violate multiple or all constraints simultaneously. We then fine-tune the controller for the initial states in each cluster using Algorithm 1. This fine-tuning process is typically fast, as the initial policy is already well-trained. We store each initial state region and its corresponding verified safe policy in the mapping dictionary $H$ . This clustering and fine-tuning process continues until a verified safe policy exists for every $s_{0}\\in S_{0}$ . ", "page_idx": 6}, {"type": "text", "text": "At decision time, given an initial state $s_{0}$ , we first identify the pair $(S_{\\mathrm{safe}}^{V},\\pi_{\\theta})$ in the mapping dictionary $H$ where $s_{0}\\in S_{\\mathrm{safe}}^{V}$ , then use the corresponding verified safe controller $\\pi_{\\theta}$ . Note that the soundness of the algorithm directly follows from our use of the sound verification tool $\\alpha,\\beta$ -CROWN. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Experiment Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We evaluate our proposed approach in five control settings: Lane Following, Vehicle Avoidance, 2D Quadrotor (with both fixed and moving obstacles), and 3D Quadrotor [Kong et al., 2015, Dai et al., 2021]. The dynamics of these environments are approximated using NN with ReLU activations. We use a continuous action space for those discrete-time systems. In each experiment, we specify the initial region ${\\mathcal S}_{0}$ for which we wish to achieve verified safety. We then aim to achieve the maximum $K$ for which safety can be verified. We evaluate the approaches using four metrics: 1) Verified- $K$ : the percentage of regions in ${\\mathcal{S}}_{0}$ that can be verified for safety over $K$ steps; 2) Verified-Max: the maximum number of steps for which all states in ${\\mathcal{S}}_{0}$ can be verified as safe; 3) Emp- $k$ : the percentage of regions in ${\\mathcal{S}}_{0}$ that are empirically safe for $k$ steps, obtained by sampling $10^{7}$ datapoints from the initial state ${\\mathcal{S}}_{0}$ . This is evaluated for both $k=K$ (the number of steps we are able to verify safety for) and $k=T$ (total episode length); 4) Avg Reward: the average reward over 10 episodes, with both mean and standard deviations reported. Note that the average reward is computed over the entire episode horizon for each environment, independently of the verification horizon, as in conventional reinforcement learning. ", "page_idx": 6}, {"type": "text", "text": "We compare the proposed verified safe RL (VSRL) approach to six baselines: 1) PPO-Lag, which utilizes constrained PPO with the standard Lagrangian penalty [Achiam et al., 2017]; 2) PPO-PID, which employs constrained PPO with PID Lagrangian methods [Stooke et al., 2020]; 3) CAP, which adopts model-based safe RL with an adaptive penalty [Ma et al., 2022]; 4) MBPPO, which applies model-based safe RL with constrained PPO [Jayant and Bhatnagar, 2022]; 5) CBF-RL, which is a Control Barrier Function (CBF)-based safe reinforcement learning approach [Emam et al., 2022]; and 6) RESPO, which implements safe RL using iterative reachability estimation [Ganai et al., 2024]. ", "page_idx": 6}, {"type": "text", "text": "Next, we describe the four autonomous system environments in which we run our experiments.   \nFurther experimental setup details are provided in Appendix A.2. ", "page_idx": 6}, {"type": "table", "img_path": "tGDUDKirAy/tmp/5a604a1817118e84c563aca0eebe0c12bd0e842482253c6f5ad09543ef6eeccf.jpg", "table_caption": ["Table 1: Results for verified safety, empirical safety and average reward. The percentage results are truncated instead of rounded, to prevent missing unsafe violations. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Lane Following: Our lane following environment follows the discrete-time bicycle model [Kong et al., 2015]. The model inputs are 3-dimensional $(x,\\theta,v)$ , where $x$ is the lateral distance to the center of the lane, $\\theta$ is the angle relative to the center of the lane, and $v$ represents the speed. The objective is to maintain a constant speed while following the lane, meaning the system equilibrium point is $(x,\\theta,v)=(0,0,v_{\\mathrm{target}})$ . The safety constraints are 1) $x$ stays within a maximum distance from the lane center $\\lceil\\|x\\|\\leq\\bar{d}_{\\mathrm{max}}\\right)$ , 2) $\\theta$ remains within a predefined range ( $\\left\\lceil\\right\\rceil\\left|\\right\\rvert\\le\\theta_{\\mathrm{max}}\\right)$ , and 3) $v$ does not exceed the maximum threshold $(v\\leq v_{\\mathrm{max}})$ ). ", "page_idx": 8}, {"type": "text", "text": "Vehicle Avoidance: Our vehicle avoidance environment features a vehicle moving on an x-y plane, with 4-dimensional inputs $(x,y,\\theta,v)$ . Here, $(x,y)$ represents the location of the vehicle on the plane, $\\theta$ is the angle relative to the $y$ -axis, and $v$ is the speed. In this setting, we have five moving obstacles, each moving from one point to another at constant speed. Each obstacle is represented as a square. Additionally, safety constraints are set for the speed $\\mathrm{\\Delta}v\\le v_{\\mathrm{max}})$ ) and angle $\\left(\\left\\|\\theta\\right\\|\\leq\\theta_{\\operatorname*{max}}\\right)$ . The task is to navigate the vehicle to a designated location while following safety constraints. ", "page_idx": 8}, {"type": "text", "text": "2D Quadrotor: For the 2D quadrotor environment, we follow the settings in Dai et al. [2021]. The input is 6-dimensional $(y,z,\\bar{\\theta},\\dot{y},\\dot{z},\\dot{\\theta})$ , where $(y,z)$ represents the position of the quadrotor on the $y{-}z$ plane, and $\\theta$ represents the angle. The action space is 2-dimensional and continuous; the actions are clipped within a range to reflect motor constraints. Our safety criteria include an angle constraint $\\left(\\left\\|\\theta\\right\\|\\leq\\theta_{\\operatorname*{max}}\\right)$ and a minimum height constraint to prevent collision with the ground $\\left<y\\geq y_{\\mathrm{min}}\\right>$ . We consider two scenarios for obstacles: fixed and moving. For fixed obstacles, there are five rectangular obstacles positioned in the $y{-}z$ plane. For moving obstacles, there are five obstacles that moves from one point to another at constant speed, each represented as a square. ", "page_idx": 8}, {"type": "text", "text": "3D Quadrotor: Our 3D quadrotor environment features a 12-dimensional input space, represented as $(x,y,z,\\phi,\\theta,\\psi,\\dot{x},\\dot{y},\\dot{z},\\omega_{x},\\omega_{y},\\omega_{z})$ . The action space is 4-dimensional and continuous; the actions are clipped within a range to reflect motor constraints. Here, $(x,y,z)$ denotes the location of the quadrotor in space, $\\phi$ is the roll angle, $\\theta$ is the pitch angle, and $\\psi$ is the yaw angle, $\\omega_{x},\\omega_{y},\\omega_{z}$ represent the angular velocity around the $x,y$ , and $z$ axes, respectively. The task is to navigating towards the goal while adhering to safety constraints, which include avoiding five obstacles represented as 3D rectangles. The details for the environment settings are deferred to the Appendix. ", "page_idx": 8}, {"type": "text", "text": "4.2 Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "As shown in Table 1, our approach significantly outperforms all baselines in terms of verified safety, as well as empirical safety over the entire episode horizon. Furthermore, the only environment in which VSRL exhibits a significant decrease in reward compared to baselines is lane following; for the rest, it achieves reward comparable to, or better than the baselines. ", "page_idx": 8}, {"type": "text", "text": "Specifically, in the lane following environment, the proposed VSRL approach achieves verified 80-step safety using a single controller (i.e., $|\\Theta|=1$ ). This is an order of magnitude higher $K$ than all baselines (which only achieve $K\\le8$ ). While all baselines obtain a safety record of over $99.9\\%$ over the entire episode $K=500)$ ), our approach empirically achieves perfect safety. ", "page_idx": 8}, {"type": "text", "text": "For vehicle avoidance, we achieve verified 50-step safety using two controllers (i.e., $|\\Theta|=2,$ ); in contrast, the best baseline yields only $K=13$ . We also observe considerable improvements in both verified and empirical safety over the baseline approaches: for example, the best verified baseline (CAP) violates safety over $10\\%$ of the time over the full episode length, whereas VSRL maintains a perfect safety record. In this case, VSRL also achieves the highest reward. ", "page_idx": 8}, {"type": "text", "text": "For the 2D Quadrotor environment with fixed and moving obstacles, we are able to achieve verified 50-step safety using four and two controllers, respectively. The best baseline achieves only $K=14$ in the case of fixed and $K=8$ in the case of moving obstacles (notably, different baselines are best in these cases). ", "page_idx": 8}, {"type": "text", "text": "Finally, in the most complex 3D Quadrotor environment, we achieve verified safety for $K=15$ , but empirically maintain a perfect safety record for the entire episode durection. The best baseline achieves verified safety for only $K=4$ , but is empirically unsafe over $40\\%$ of the time during an episode. Even the best safety record of any baseline is unsafe nearly $12\\%$ of the time, and we can only verify its safety over a horizon $K=3$ . ", "page_idx": 8}, {"type": "text", "text": "Ablation Study: We evaluate the importance of both incremental verification and using multiple initial-state-dependent controllers as part of VSRL. As shown in the Appendix (Section A.1), the former significantly reduces average verification time during training, whereas the latter enables us to greatly boost the size of the initial state region ${\\mathcal{S}}_{0}$ for which we are able to achieve verify safety. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We present an approach for learning neural network control policies for nonlinear neural dynamical systems. In contrast to conventional methods for safe control synthesis which rely on forward invariance-based proofs, we opt instead for the more pragmatic finite-step reachability verification. This enables us to make use of state-of-the-art differentiable neural network overapproximation tools that we combine with three key innovations. The first is a novel curriculum learning approach for maximizing safety horizon. The second is to learn multiple initial-state-dependent controllers. The third is to leverage small changes in iterative gradient-based learning to enable incremental verification. We show that the proposed approach significantly outperforms state of the art safe RL baselines on several dynamical system environments, accounting for both fixed and moving obstacles. A key limitation of our approach is the clearly weaker safety guarantees it provides compared to forward invariance. Nevertheless, our results demonstrate that finite-step reachability provides a more pragmatic way of achieving verified safety that effectively achieves safety over the entire episode horizon in practice, providing an alternative direction for advances in verified safe RL to the more typical forward-invariance-based synthesis. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research was partially supported by the NSF (grants IIS-1905558, IIS-2214141, CCF-2403758, and IIS-2331967) and NVIDIA. Huan Zhang is supported in part by the AI2050 program at Schmidt Sciences (AI 2050 Early Career Fellowship). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In International conference on machine learning, pages 22\u201331. PMLR, 2017.   \nMohammed Alshiekh, Roderick Bloem, R\u00fcdiger Ehlers, Bettina K\u00f6nighofer, Scott Niekum, and Ufuk Topcu. Safe reinforcement learning via shielding. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018.   \nMatthias Althoff. An introduction to CORA 2015. In Proc. of the 1st and 2nd Workshop on Applied Verification for Continuous and Hybrid Systems, pages 120\u2013151. EasyChair, December 2015. doi: 10.29007/zbkv.   \nOsbert Bastani, Yewen Pu, and Armando Solar-Lezama. Verifiable reinforcement learning via policy extraction. Advances in neural information processing systems, 31, 2018.   \nYoshua Bengio, J\u00e9r\u00f4me Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pages 41\u201348, 2009.   \nFelix Berkenkamp, Matteo Turchetta, Angela P. Schoellig, and Andreas Krause. Safe modelbased reinforcement learning with stability guarantees. In Proceedings of the 31st International Conference on Neural Information Processing Systems, page 908\u2013919, 2017.   \nYinlam Chow, Ofir Nachum, Edgar Duenez-Guzman, and Mohammad Ghavamzadeh. A lyapunovbased approach to safe reinforcement learning. Advances in neural information processing systems, 31, 2018.   \nHongkai Dai, Benoit Landry, Lujie Yang, Marco Pavone, and Russ Tedrake. Lyapunov-stable neuralnetwork control. In Dylan A. Shell, Marc Toussaint, and M. Ani Hsieh, editors, Robotics: Science and Systems XVII, Virtual Event, July 12-16, 2021, 2021.   \nCharles Dawson, Zengyi Qin, Sicun Gao, and Chuchu Fan. Safe nonlinear control using robust neural lyapunov-barrier functions. In Conference on Robot Learning, pages 1724\u20131735. PMLR, 2022.   \nKrishnamurthy Dvijotham, Robert Stanforth, Sven Gowal, Timothy A. Mann, and Pushmeet Kohli. A dual approach to scalable verification of deep networks. In Proceedings of the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence, UAI 2018, Monterey, California, USA, August 6-10, 2018, pages 550\u2013559, 2018.   \nKrishnamurthy Dj Dvijotham, Robert Stanforth, Sven Gowal, Chongli Qin, Soham De, and Pushmeet Kohli. Efficient neural network verification with exactness characterization. In Uncertainty in artificial intelligence, pages 497\u2013507. PMLR, 2020.   \nYousef Emam, Gennaro Notomista, Paul Glotfelter, Zsolt Kira, and Magnus Egerstedt. Safe reinforcement learning using robust control barrier functions. IEEE Robotics and Automation Letters, 2022.   \nMichael Everett, Golnaz Habibi, and Jonathan P How. Robustness analysis of neural networks via efficient partitioning with applications in control systems. IEEE Control Systems Letters, 5(6): 2114\u20132119, 2020.   \nClaudio Ferrari, Mark Niklas Mueller, Nikola Jovanovic\u00b4, and Martin Vechev. Complete verification via multi-neuron relaxation guided branch-and-bound. In International Conference on Learning Representations, 2022.   \nNathan Fulton and Andr\u00e9 Platzer. Safe reinforcement learning via formal methods: Toward safe control through proof and learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.   \nMilan Ganai, Zheng Gong, Chenning Yu, Sylvia Herbert, and Sicun Gao. Iterative reachability estimation for safe reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024.   \nTimon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat Chaudhuri, and Martin Vechev. Ai2: Safety and robustness certification of neural networks with abstract interpretation. In 2018 IEEE Symposium on Security and Privacy $(S P)$ , pages 3\u201318, 2018.   \nMorgan T Gillespie, Charles M Best, Eric C Townsend, David Wingate, and Marc D Killpack. Learning nonlinear dynamic models of soft robots for model predictive control with neural networks. In 2018 IEEE International Conference on Soft Robotics (RoboSoft), pages 39\u201345. IEEE, 2018.   \nSven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Uesato, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval bound propagation for training verifiably robust models. arXiv preprint arXiv:1810.12715, 2018.   \nShangding Gu, Long Yang, Yali Du, Guang Chen, Florian Walter, Jun Wang, Yaodong Yang, and Alois Knoll. A review of safe reinforcement learning: Methods, theory and applications. arXiv preprint arXiv:2205.10330, 2022.   \nXiaowei Huang, Marta Kwiatkowska, Sen Wang, and Min Wu. Safety verification of deep neural networks. In Computer Aided Verification: 29th International Conference, CAV 2017, Heidelberg, Germany, July 24-28, 2017, Proceedings, Part I 30, pages 3\u201329. Springer, 2017.   \nRadoslav Ivanov, James Weimer, Rajeev Alur, George J Pappas, and Insup Lee. Verisig: verifying safety properties of hybrid systems with neural network controllers. In Proceedings of the 22nd ACM International Conference on Hybrid Systems: Computation and Control, pages 169\u2013178, 2019.   \nNils Jansen, Bettina K\u00f6nighofer, Sebastian Junges, Alex Serban, and Roderick Bloem. Safe reinforcement learning using probabilistic shields. In 31st International Conference on Concurrency Theory (CONCUR 2020). Schloss-Dagstuhl-Leibniz Zentrum f\u00fcr Informatik, 2020.   \nAshish K Jayant and Shalabh Bhatnagar. Model-based safe deep reinforcement learning via a constrained proximal policy optimization algorithm. Advances in Neural Information Processing Systems, 35:24432\u201324445, 2022.   \nGuy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer. Reluplex: An efficient smt solver for verifying deep neural networks. In International Conference on Computer Aided Verification, pages 97\u2013117, 2017.   \nGuy Katz, Derek A Huang, Duligur Ibeling, Kyle Julian, Christopher Lazarus, Rachel Lim, Parth Shah, Shantanu Thakoor, Haoze Wu, Aleksandar Zeljic\u00b4, et al. The marabou framework for verification and analysis of deep neural networks. In Computer Aided Verification: 31st International Conference, CAV 2019, New York City, NY, USA, July 15-18, 2019, Proceedings, Part I 31, pages 443\u2013452. Springer, 2019.   \nNiklas Kochdumper, Hanna Krasowski, Xiao Wang, Stanley Bak, and Matthias Althoff. Provably safe reinforcement learning via action projection using reachability analysis and polynomial zonotopes. IEEE Open Journal of Control Systems, 2:79\u201392, 2023.   \nJason Kong, Mark Pfeiffer, Georg Schildbach, and Francesco Borrelli. Kinematic and dynamic vehicle models for autonomous driving control design. In 2015 IEEE intelligent vehicles symposium (IV), pages 1094\u20131099. IEEE, 2015.   \nPanagiotis Kouvaros and Alessio Lomuscio. Towards scalable complete verification of relu neural networks via dependency-based branching. In IJCAI, pages 2643\u20132650, 2021.   \nZiang Liu, Genggeng Zhou, Jeff He, Tobia Marcucci, Fei-Fei Li, Jiajun Wu, and Yunzhu Li. Modelbased control with sparse neural dynamics. Advances in Neural Information Processing Systems, 36, 2024.   \nDiego Manzanas Lopez, Sung Woo Choi, Hoang-Dung Tran, and Taylor T Johnson. Nnv 2.0: the neural network verification tool. In International Conference on Computer Aided Verification, pages 397\u2013412. Springer, 2023.   \nYecheng Jason Ma, Andrew Shen, Osbert Bastani, and Jayaraman Dinesh. Conservative and adaptive penalty for model-based safe reinforcement learning. In Proceedings of the AAAI conference on artificial intelligence, volume 36, pages 5404\u20135412, 2022.   \nAnusha Nagabandi, Gregory Kahn, Ronald S Fearing, and Sergey Levine. Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning. In 2018 IEEE international conference on robotics and automation (ICRA), pages 7559\u20137566. IEEE, 2018.   \nCharles Noren, Weiye Zhao, and Changliu Liu. Safe adaptation with multiplicative uncertainties using robust safe set algorithm. IFAC-PapersOnLine, 54(20):360\u2013365, 2021.   \nSamuel Pfrommer, Mathew Halm, and Michael Posa. Contactnets: Learning discontinuous contact dynamics with smooth, implicit representations. In Conference on Robot Learning, pages 2279\u2013 2291. PMLR, 2021.   \nChongli Qin, Krishnamurthy (Dj) Dvijotham, Brendan O\u2019Donoghue, Rudy Bunel, Robert Stanforth, Sven Gowal, Jonathan Uesato, Grzegorz Swirszcz, and Pushmeet Kohli. Verification of non-linear specifications for neural networks. In International Conference on Learning Representations, 2019.   \nGagandeep Singh, Timon Gehr, Markus P\u00fcschel, and Martin Vechev. An abstract domain for certifying neural networks. Proceedings of the ACM on Programming Languages, 3(POPL):41, 2019.   \nOswin So and Chuchu Fan. Solving stabilize-avoid optimal control via epigraph form and deep reinforcement learning. In Proceedings of Robotics: Science and Systems, 2023.   \nAdam Stooke, Joshua Achiam, and Pieter Abbeel. Responsive safety in reinforcement learning by pid lagrangian methods. In International Conference on Machine Learning, pages 9133\u20139143. PMLR, 2020.   \nVincent Tjeng, Kai Y. Xiao, and Russ Tedrake. Evaluating robustness of neural networks with mixed integer programming. In International Conference on Learning Representations, 2019.   \nKim P Wabersich and Melanie N Zeilinger. Linear model predictive safety certification for learningbased control. In 2018 IEEE Conference on Decision and Control (CDC), pages 7130\u20137135. IEEE, 2018.   \nShiqi Wang, Huan Zhang, Kaidi Xu, Xue Lin, Suman Jana, Cho-Jui Hsieh, and J Zico Kolter. Beta-CROWN: Efficient bound propagation with per-neuron split constraints for complete and incomplete neural network verification. Advances in Neural Information Processing Systems, 34, 2021.   \nYixuan Wang, Simon Sinong Zhan, Ruochen Jiao, Zhilu Wang, Wanxin Jin, Zhuoran Yang, Zhaoran Wang, Chao Huang, and Qi Zhu. Enforcing hard constraints with soft barriers: safe reinforcement learning in unknown stochastic environments. In Proceedings of the 40th International Conference on Machine Learning, ICML\u201923. JMLR, 2023a.   \nYixuan Wang, Weichao Zhou, Jiameng Fan, Zhilu Wang, Jiajun Li, Xin Chen, Chao Huang, Wenchao Li, and Qi Zhu. Polar-express: Efficient and precise formal reachability analysis of neural-network controlled systems. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2023b.   \nTianhao Wei and Changliu Liu. Safe control with neural network dynamic models. In Learning for Dynamics and Control Conference, pages 739\u2013750. PMLR, 2022.   \nTianhao Wei, Shucheng Kang, Weiye Zhao, and Changliu Liu. Persistently feasible robust safe control by safety index synthesis and convex semi-infinite programming. IEEE Control Systems Letters, 7:1213\u20131218, 2022.   \nJunlin Wu and Yevgeniy Vorobeychik. Robust deep reinforcement learning through bootstrapped opportunistic curriculum. In International Conference on Machine Learning, pages 24177\u201324211. PMLR, 2022.   \nNuoya Xiong, Yihan Du, and Longbo Huang. Provably safe reinforcement learning with step-wise violation constraints. Advances in Neural Information Processing Systems, 36, 2024.   \nKaidi Xu, Huan Zhang, Shiqi Wang, Yihan Wang, Suman Jana, Xue Lin, and Cho-Jui Hsieh. Fast and Complete: Enabling complete neural network verification with rapid and massively parallel incomplete verifiers. In International Conference on Learning Representations, 2021.   \nDongjie Yu, Haitong Ma, Shengbo Li, and Jianyu Chen. Reachability constrained reinforcement learning. In International Conference on Machine Learning, pages 25636\u201325655. PMLR, 2022.   \nHuan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel. Efficient neural network robustness certification with general activation functions. In Advances in Neural Information Processing Systems, pages 4944\u20134953, 2018.   \nHuan Zhang, Shiqi Wang, Kaidi Xu, Linyi Li, Bo Li, Suman Jana, Cho-Jui Hsieh, and J. Zico Kolter. General cutting planes for bound-propagation-based neural network verification. In Proceedings of the 36th International Conference on Neural Information Processing Systems, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Ablation Study ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we conduct an ablation study to evaluate the importance of both incremental verification and the use of multiple initial-state-dependent controllers as part of the VSRL approach. ", "page_idx": 13}, {"type": "table", "img_path": "tGDUDKirAy/tmp/5ad3f897df89a6199b7649cda6b9e790ae8e309a7711548688e318f5ea9dc8c9.jpg", "table_caption": ["Table 2: Runtime (in seconds) for 20 training epochs with and without incremental verification. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "Table 2 presents the ablation study results for incremental verification. To ensure a fair comparison, we record the runtime for 20 training epochs with only one region from the grid split for all environments. In practice, this process can be run on GPUs in parallel for multiple regions. Given that the neural network structures for the 2D Quadrotor environment with both moving and fixed obstacles are the same, the runtime results are similar; therefore, we report these collectively as 2D Quadrotor. The results indicate that incremental verification significantly reduces the average verification time during training. Without incremental verification, the verification time increases rapidly as the number of steps increases. ", "page_idx": 13}, {"type": "table", "img_path": "tGDUDKirAy/tmp/9b7e4d6404783aed3835a1c35163f755e6a2c6d4fc6ed36f21a63f836552be1c.jpg", "table_caption": ["Table 3: Percentage of regions in ${\\mathcal{S}}_{0}$ that can be verified for safety for $K$ steps (Verified- $K$ ). "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "Table 3 shows the ablation study results for using multiple initial-state-dependent controllers. We report results for the Vehicle Avoidance environment (Veh. Avoid.), 2D Quadrotor with fixed obstacles (2D-Quad (F)), moving obstacles (2D-Quad (M)), and 3D Quadrotor (3D-Quad (F)). We exclude the Lane Following environment from this comparison, as only one controller was used there to achieve $100\\%$ verified safety. The results demonstrate that using multiple controllers significantly enhances the ability to achieve verified safety across a larger initial state region ${\\mathcal{S}}_{0}$ . ", "page_idx": 13}, {"type": "text", "text": "A.2 Experiment Setup ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Lane Following Our lane following environment follows the discrete-time bicycle model [Kong et al., 2015] ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\dot{x}=v\\cos(\\theta+\\beta)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\dot{y}=v\\sin(\\theta+\\beta)}\\\\ {\\dot{\\theta}=\\cfrac{v}{l_{r}}\\sin(\\beta)}\\\\ {\\dot{v}=a}\\\\ {\\beta=\\tan^{-1}\\left(\\cfrac{l_{r}}{l_{f}+l_{r}}\\tan(\\delta_{f})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where we set the wheel base of the vehicle to $2.9\\mathrm{m}$ . The model inputs are 3-dimensional $(x,\\theta,v)$ , where $x$ is the lateral distance to the center of the lane, $\\theta$ is the angle relative to the center of the lane, and $v$ represents the speed. The objective is to maintain a constant speed while following the lane, meaning the system equilibrium point is $(x,\\theta,v)=(0,0,v_{\\mathrm{target}}).$ . The safety constraints are ", "page_idx": 14}, {"type": "text", "text": "1. $x$ stays within a maximum distance from the lane center $\\left(\\left\\|x\\right\\|\\leq d_{\\operatorname*{max}}\\right)$ ),   \n2. $\\theta$ remains within a predefined range $\\left(\\left\\|\\theta\\right\\|\\leq\\theta_{\\operatorname*{max}}\\right)$ ,   \n3. $v$ does not exceed the maximum threshold $(v\\leq v_{\\mathrm{max}})$ ). ", "page_idx": 14}, {"type": "text", "text": "The parameters are set as $d_{\\mathrm{max}}~=~0.7$ , $\\theta_{\\mathrm{max}}\\,=\\,\\pi/4$ , and $v_{\\mathrm{max}}~=~5.0$ . The initial regions ${\\mathcal S}_{0}$ is $x\\in[-0.5,0.5],\\theta\\in[-0.2,0.2],v\\in[0.0,0.5]$ . The reward received at each step is measured as the distance to the equilibrium point. More specifically, for a state that is of distance $d$ to the target equilibrium point, the reward is $e^{-d}$ . For VSRL training, our controller is initialized using a controller pretrained with a safe RL algorithm. When training with the bound loss, we add a large penalty on unsafe states to incentivize maintaining safety throughout the entire trajectory. For branch and bound during verification, we set the precision limit as 0.025, which means as soon as the precision of the grid region reaches this precision, we stop branching. For the dynamics approximation, we use an NN with two layers of ReLU each of size 8. ", "page_idx": 14}, {"type": "text", "text": "Vehicle Avoidance Our vehicle avoidance environment features a vehicle moving on an $x{-}y$ plane, with 4-dimensional inputs $(x,y,\\theta,v)$ . Here, $(x,y)$ represents the location of the vehicle on the plane, $\\theta$ is the angle relative to the $y$ -axis, and $v$ is the speed. In this setting, we have five moving obstacles, each moving from one point to another at a constant speed for the duration of 500 steps. The five obstacles are: 1) moving from $(x,y)\\,=\\,(-0.6,1.0)$ to $(x,y)\\,=\\,(-0.35,2.0)$ ; 2) moving from $(x,y)=(0.6,0.0)$ to $(x,y)=(0.75,1.0);3)$ moving from $(x,y)=(0.0,1.0)$ to $\\mathbf{\\phi}(x,y)=(0.0,2.0)$ ; 4) moving from $(x,y)=(-0.85,1.0)$ to $(x,y)=(-1.6,1.5);5)$ moving from $(x,y)=(0.75,0.0)$ to $\\bar{(x,y)}\\bar{=}\\,(0.85,0.0\\dot{)}$ . Each obstacle is represented as a square with a diameter of 0.1. Additionally, safety constraints are set for the speed $(v\\,\\leq\\,v_{\\mathrm{max}})$ and angle $\\left\\lceil\\right\\rceil\\left\\lvert\\right\\rvert\\mathrm{~\\le~}\\theta_{\\mathrm{max}}\\mathrm{)}$ , where $v_{\\mathrm{max}}\\,=\\,5.0$ and $\\theta_{\\mathrm{max}}~=~\\pi/2$ . The task is to navigate the vehicle to a designated location while following safety constraints. The agent starts near the origin within an area defined by $x,y\\,\\in\\,[-0.5,0.5]$ , $\\theta\\in\\left[-0.2,0.2\\right]$ , and $v\\in[0,0.1]$ , and the goal is $(\\bar{x_{\\mathrm{target}}},y_{\\mathrm{target}})=(1.0,2.0)$ . The branching precision limit is 0.025 and for dynamics approximation, we use a NN with two layers of ReLU each of size 10. ", "page_idx": 14}, {"type": "text", "text": "2D Quadrotor For the 2D quadrotor environment, we follow the settings in Dai et al. [2021]. ", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\begin{array}{l}{{\\dot{x}}=-{\\cfrac{\\sin(\\theta)}{m}}\\cdot\\left(u_{0}+u_{1}\\right)}\\\\ {{\\dot{y}}={\\cfrac{\\cos(\\theta)}{m}}\\cdot\\left(u_{0}+u_{1}\\right)-g}\\\\ {{\\dot{\\theta}}={\\cfrac{\\log\\operatorname{th}}{I}}\\cdot\\left(u_{0}-u_{1}\\right)}\\end{array}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We use a timestep $d t=0.02$ , the mass of the quadrotor is set to $m=0.486$ , the length to $l=0.25$ , the inertia to $I\\,=\\,0.00383$ , and gravity to $g\\,=\\,9.81$ . The input is 6-dimensional $(y,z,\\theta,\\dot{y},\\dot{z},\\dot{\\theta})$ , where $(y,z)$ represents the position of the quadrotor on the $y\\!-\\!z$ plane, and $\\theta$ represents the angle. The action space is 2-dimensional and continuous; the actions are clipped within a range to reflect motor constraints. Our safety criteria are ", "page_idx": 14}, {"type": "text", "text": "1. angle $\\theta$ remains within a predefined range $\\left\\lceil\\right\\rceil\\left|\\theta\\right\\|\\leq\\theta_{\\mathrm{max}}\\right)$ ,   \n2. a minimum height constraint to prevent collision with the ground $(y\\geq y_{\\mathrm{min}})$ ),   \n3. avoid obstacles. ", "page_idx": 14}, {"type": "text", "text": "Here we set $\\theta_{\\mathrm{max}}=\\pi/3$ and $y_{\\mathrm{min}}=-0.2$ . The task is for the quadrotor to navigate towards the goal while following safety constraints. We consider two scenarios for obstacles: fixed and moving. For fixed obstacles, there are five rectangular obstacles positioned in the $y\\!-\\!z$ plane. We use $(x_{l},x_{u},y_{l},y_{u})$ to represent the two dimensional box, and the obstacles are: $(x_{l},x_{u},\\bar{y}_{l},\\bar{y}_{u})=(-0.3,-0.1,0.\\dot{4},0.6)$ , $(x_{l},\\bar{x}_{u},y_{l},y_{u})\\;=\\;(-1.2,-0.8,0.2,0.4)$ , $(x_{l},x_{u},y_{l},y_{u})\\;=\\;(0.0,0.1,0.5,1.0)$ , $(x_{l},x_{u},y_{l},y_{u})\\;=$ $(0.6,0.7,0.0,0.2)$ , $(x_{l},x_{u},y_{l},y_{u})\\,=\\,(-0.8,-0.7,0.7,0.9)$ ). For moving obstacles, there are five obstacles that moves from one point to another at constant speed for the duration of 500 steps, each represented as a square of diameter 0.1. The obstacles are: 1) moving from $(x,y)\\,=\\,(\\bar{0}.6,0.0)$ to $(x,y)\\,=\\,(0.6,0.1);\\,$ 2) moving from $(x,y)\\,=\\,(-0.5,0.2)$ to $(x,y)\\,=\\,(-0.4,0.3);$ 3) moving from $(x,y)=(-0.3,0.4)$ to $(x,y)=(-0.4,0.5);4)$ moving from $(x,y)=(-0.1,0.3)$ to $\\left(x,y\\right)=$ $(0.0,0.4)$ ; 5) moving from $(x,y)=(-0.7,0.5)$ to $(x,y)=(-0.4,0.6)$ . The initial region for the quadrotor is defined with $x\\in[-0.5,0.5]$ and the remaining state variables within $[-0.1,0.1]$ . The target goal is set to $(x,y)\\,=\\,(0.6,0.6)$ . We set the branching limit to 0.0125 and for dynamics approximation we use a NN with two layers of ReLU each of size 6. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "3D Quadrotor Our 3D quadrotor environment features a 12-dimensional input space, represented as $(x,y,z,\\phi,\\theta,\\psi,\\dot{x},\\dot{y},\\dot{z},\\omega_{x},\\omega_{y},\\omega_{z})$ . The action space is 4-dimensional and continuous; the actions are clipped within a range to reflect motor constraints. Here, $(x,y,z)$ denotes the location of the quadrotor in space, $\\phi$ is the roll angle, $\\theta$ is the pitch angle, and $\\psi$ is the yaw angle, $\\omega_{x},\\omega_{y},\\omega_{z}$ represent the angular velocity around the $x,y$ , and $z$ axes, respectively. The environment setting and neural network dynamics approximation follows the setup in Dai et al. [2021], with the modification of using ReLU activations instead of LeakyReLU. The system dynamics is: ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{\\operatorname{plant}\\operatorname{input}={\\left[\\begin{array}{l l l l}{1}&{1}&{1}&{1}\\\\ {0}&{L}&{0}&{-L}\\\\ {-L}&{0}&{L}&{0}\\\\ {\\kappa_{z}}&{-\\kappa_{z}}&{\\kappa_{z}}&{-\\kappa_{z}}\\end{array}\\right]}\\cdot u}\\\\ &{\\qquad\\qquad R=\\operatorname{rygromat}(\\phi,\\theta,\\psi)}\\\\ &{\\qquad\\qquad{\\ddot{\\mathfrak{p}}}={\\left[\\begin{array}{l l l l}{0}&{1}&{0}\\\\ {-g}&{+R}&{\\left\\{\\operatorname{plant}\\dot{\\mathfrak{p}}(1\\!\\right\\}/m)\\ u}\\\\ {0}&{0}&{1}\\end{array}\\right]}}\\\\ &{\\qquad\\qquad{\\dot{\\omega}}={\\frac{-\\omega\\times(I\\cdot\\omega)+\\operatorname{plant}\\dot{\\mathfrak{p}}(1\\!\\right\\}}{I}}\\\\ &{\\qquad\\left[{\\dot{\\phi}}\\right]={\\left[\\begin{array}{l l l l}{1}&{\\sin(\\phi)\\cdot\\tan(\\theta)}&{\\cos(\\phi)\\cdot\\tan(\\theta)}\\\\ {0}&{\\cos(\\phi)}&{-\\sin(\\phi)}\\\\ {0}&{\\frac{\\sin(\\phi)}{\\sin(\\phi)}}&{\\frac{\\cos(\\phi)}{\\cos(\\phi)}}\\end{array}\\right]}\\cdot\\omega}\\end{array}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The dynamics neural network has two ReLU layers, each with a size of 16 and $d t~=~0.02$ . We set the branching precision limit to 0.00625. The task is to navigating towards the goal while avoiding five obstacles represented as 3D rectangles. The locations of the obstacles are $(-0.5,0.5,-0.2,0.2,-0.65,-0.55)$ , $(-0.7,-0.6,-0.1,0.1,-0.5,-0.4)$ , $(0.5,0.6,-0.2,0.2,$ , $4,-0.3),(-0.8,-0.6,0.2,0.4,-0.3,-0.2)$ , $\\left(-0.8,-0.6,-0.4,-0.2,-0.2,-0.1\\right)$ , where the first obstacle is to avoid controller collide with the ground. We set the goal at $\\left(x,y,z\\right)\\;=\\;$ $(0.0,0.0,0.0)$ , and the initial region is defined with $x\\ \\in\\ [-0.5,0.5]$ , $y\\;\\;\\in\\;\\;[-0.1,0.1]$ , and $z\\in[-0.5,-0.3]$ , with the remaining variables confined to the range $[-0.05,0.05]$ . The reward is calculated based on the distance to the goal, where the agent receives a higher reward for being closer to the goal. The environment episodes end if either the magnitude of $\\phi$ or $\\theta$ exceeds $\\pi/3$ . ", "page_idx": 15}, {"type": "text", "text": "A.3 Compute Resources ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Our code runs on an AMD Ryzen 9 5900X CPU with a 12-core processor and an NVIDIA GeForce RTX 3090 GPU. ", "page_idx": 15}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: Abstract and Introduction Section ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 16}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Justification: we discuss the primary limitation of the work in the conclusion. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 16}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 16}, {"type": "text", "text": "Justification: The paper does not include theoretical results. Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 17}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: We provide details of the experimental setup both in Section A.2 and in the Appendix. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 17}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We will provide all code and data on github, along with documentation, to facilitate reproduction of the experiments if the paper is accepted for publication. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 18}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We provide this information in Section 4 and the Appendix. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 18}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: We report confidence intervals in the results table in the experiments (see Section 4). ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Justification: We provide this information in the Appendix. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 19}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: All aspects of the paper and research conform in all respects to the NeurIPS Code of Ethics. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 19}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: As mentioned in the introduction, ability to synthesize safe controllers is a core challenge in autonomous systems (such as autonomous cars). As such, our work is expected to have positive broader impacts. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper is about safe reinforcement learning, and poses no such risk Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 20}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: The paper references the Auto_Lirpa codebase and associated research papers describing it. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 20}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 21}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 21}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 21}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 21}]