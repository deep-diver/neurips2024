LLM-Check: Investigating Detection of Hallucinations in Large Language Models