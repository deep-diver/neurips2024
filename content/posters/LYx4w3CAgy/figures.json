[{"figure_path": "LYx4w3CAgy/figures/figures_3_1.jpg", "caption": "Figure 1: Taxonomy for different settings of hallucination detection, and the different datasets and data splits that correspond to each such setting.", "description": "This figure presents a taxonomy of hallucination detection settings, categorized by whether external references are available and whether multiple model responses are needed.  It shows the different datasets and data splits used in the paper to evaluate these settings.  The main categories are: No External References (with Single Model Response or Multiple Model Responses), and With External References (White-box and Black-box settings). Each category is then further sub-categorized based on the available datasets and how they were used for analysis.  The figure visually represents the relationships between different settings and the data used for evaluation within each scenario.", "section": "Taxonomy and Formalisms for Hallucination Detection"}, {"figure_path": "LYx4w3CAgy/figures/figures_4_1.jpg", "caption": "Figure 2: Schematic of detection pipeline using Eigenvalue Analysis of internal LLM representations.", "description": "This figure shows a schematic of the proposed LLM-Check method for hallucination detection.  It illustrates the process of analyzing internal LLM representations (hidden activations and attention kernel maps) to identify potential hallucinations.  The input is a prompt and the LLM's response.  The hidden activations and attention kernel maps are extracted from the LLM's internal layers. Then, eigenvalue analysis is performed on these representations to generate scores (Hidden Score and Attention Score). These scores are then compared to a threshold.  If the score exceeds the threshold, a hallucination is detected; otherwise, it is not detected.", "section": "4 Proposed Method"}, {"figure_path": "LYx4w3CAgy/figures/figures_9_1.jpg", "caption": "Figure 3: Averaged runtime analysis of different hallucination detection methods.", "description": "This figure compares the runtimes of different hallucination detection methods.  It shows that the proposed LLM-Check method (using Logit Entropy, Attention Score, and Hidden Score) is significantly faster than other baselines such as Self-Prompt, FAVA Model, SelfCheckGPT-Prompt, and INSIDE. The LLM-Check methods achieve speedups of up to 45x and 450x over the other baselines.  The speed advantage is mainly due to LLM-Check's ability to detect hallucinations using a single forward pass of the LLM, without needing multiple responses or extensive external databases.  The figure breaks down the time into sampling time (generating multiple responses in some methods) and detection time (the time the method needs to analyze the results and make a decision).", "section": "5.3 Time-Complexity Analysis"}, {"figure_path": "LYx4w3CAgy/figures/figures_9_2.jpg", "caption": "Figure 2: Schematic of detection pipeline using Eigenvalue Analysis of internal LLM representations.", "description": "This figure shows a schematic of the hallucination detection pipeline using eigenvalue analysis of internal LLM representations.  It illustrates the process starting from the input prompt and response concatenation, which goes through different layers of a transformer block, self-attention layer, feedforward layer, and layer normalization.  The hidden activations and attention scores are extracted from the intermediate layers. Eigenvalue analysis is then performed on the hidden activations and the attention kernel map separately to compute two scores:  Hidden Score and Attention Score. These scores are then compared to a threshold. If the score exceeds the threshold, hallucination is detected; otherwise, it is not detected. The figure visually represents how the internal representations of the model are analyzed to detect the presence of hallucinations in the LLM response.", "section": "4 Proposed Method"}, {"figure_path": "LYx4w3CAgy/figures/figures_14_1.jpg", "caption": "Figure 2: Schematic of detection pipeline using Eigenvalue Analysis of internal LLM representations.", "description": "This figure shows a schematic of the LLM-Check hallucination detection pipeline.  It details the process, starting with the prompt and response being concatenated and fed into a transformer block.  The hidden activations and the self-attention kernel map are extracted. Then, eigenvalue analysis is performed on these matrices, resulting in a Hidden Score and Attention Score, respectively.  These scores are then compared to thresholds to determine if a hallucination is detected.", "section": "4 Proposed Method"}, {"figure_path": "LYx4w3CAgy/figures/figures_15_1.jpg", "caption": "Figure 2: Schematic of detection pipeline using Eigenvalue Analysis of internal LLM representations.", "description": "This figure shows a schematic of the hallucination detection pipeline proposed in the paper. The pipeline uses eigenvalue analysis of internal LLM representations, such as hidden activations and attention maps, to identify hallucinations. The input is a prompt and response from an LLM.  The pipeline then analyzes the internal representations of the LLM to compute scores such as Hidden Score and Attention Score. These scores are then used to determine whether the response contains hallucinations.", "section": "4 Proposed Method"}, {"figure_path": "LYx4w3CAgy/figures/figures_15_2.jpg", "caption": "Figure 4: Difference between the cumulative sum of log-eigenvalues from the first token till the jth token (between 1 and total length) between the Hallucinated Sample (HS) and Truthful sample (TS).", "description": "This figure visualizes the cumulative difference in log-eigenvalues between the hallucinated and truthful responses across token positions.  It shows that, while not entirely monotonic, the cumulative sum of log-eigenvalues for the hallucinated response consistently remains higher than that of the truthful response across the entire token sequence.  This supports the paper's claim that the differences in log-eigenvalues can effectively distinguish between hallucinated and truthful responses.", "section": "Eigenvalue Analysis of Internal LLM Representations"}, {"figure_path": "LYx4w3CAgy/figures/figures_18_1.jpg", "caption": "Figure 5: ROC curves for logit-based detection schemes with the annotated FAVA dataset. Figures 5a and 5b show the detection of entity and relation hallucinations, respectively. As observed here, taking the negative detection scores helps with detecting various kinds of hallucinations.", "description": "This figure shows Receiver Operating Characteristic (ROC) curves for logit-based hallucination detection methods on the FAVA dataset.  Two subfigures are presented: one for entity hallucinations and one for relation hallucinations. Each subfigure displays ROC curves for different logit-based metrics (negative perplexity, negative logit entropy, positive logit entropy, etc.). The results indicate that considering both positive and negative detection scores improves the performance of the hallucination detection.", "section": "5.1 Detection Results on Datasets with no External References as Context"}, {"figure_path": "LYx4w3CAgy/figures/figures_19_1.jpg", "caption": "Figure 7: Results across different layers of Llama-2-7B obtained using sample subsets of 5, 20 and 50 pairs, as well as the complete dataset. We observe that general layerwise trends begin to hold with fairly limited sample pairs, and can help choose an optimal layer in an efficient manner.", "description": "This figure shows the performance of the proposed hallucination detection method across different layers of a Llama-2-7B language model.  The results are shown for different subset sizes of the dataset (5, 20, and 50 pairs of samples, as well as the full dataset).  The plots illustrate the Area Under the ROC Curve (AUROC), Accuracy, and True Positive Rate at 5% False Positive Rate (TPR@5%FPR) for each layer.  The consistency of the trends across different subset sizes suggests that a suitable layer for optimal performance can be efficiently selected using only a small subset of the data.", "section": "F Layerwise Analysis for Eigenvalue based Scores"}, {"figure_path": "LYx4w3CAgy/figures/figures_20_1.jpg", "caption": "Figure 7: Results across different layers of Llama-2-7B obtained using sample subsets of 5, 20 and 50 pairs, as well as the complete dataset. We observe that general layerwise trends begin to hold with fairly limited sample pairs, and can help choose an optimal layer in an efficient manner.", "description": "This figure shows the accuracy of hallucination detection across different layers of the Llama-2-7B language model using various sample sizes.  The results demonstrate that consistent trends in performance emerge even with small sample sizes (5, 20, and 50 pairs). This suggests an efficient method for selecting the optimal layer for hallucination detection.", "section": "F Layerwise Analysis for Eigenvalue based Scores"}, {"figure_path": "LYx4w3CAgy/figures/figures_21_1.jpg", "caption": "Figure 2: Schematic of detection pipeline using Eigenvalue Analysis of internal LLM representations.", "description": "This figure shows a schematic of the proposed hallucination detection pipeline.  It uses an LLM's internal representations (hidden activations and attention kernel maps) to identify hallucinations. The process involves three steps: 1) obtaining hidden activations and attention maps from a single forward pass of the LLM; 2) performing eigenvalue analysis on these representations to calculate Hidden and Attention scores; 3) comparing the scores against a threshold to determine whether hallucinations are present.  This approach avoids the need for multiple model responses or extensive external databases, making it efficient for real-time analysis.", "section": "4 Proposed Method"}]