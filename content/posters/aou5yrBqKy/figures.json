[{"figure_path": "aou5yrBqKy/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison with previous task-specific pipelines for visual table understanding. In contrast to design different architectures for various table tasks, our TabPedia effectively performs these tasks in a unified framework through delicately leveraging the understanding capability of LLMs.", "description": "This figure compares the traditional task-specific approach for visual table understanding (VTU) with the proposed TabPedia method. The traditional approach uses separate pipelines for different VTU tasks such as table detection, structure recognition, querying, and question answering.  This leads to modal isolation and complex workflows. In contrast, TabPedia integrates all these tasks into a unified framework using a large vision-language model (LLM) and a concept synergy mechanism. This allows for more efficient and seamless processing of visual tables.", "section": "1 Introduction"}, {"figure_path": "aou5yrBqKy/figures/figures_3_1.jpg", "caption": "Figure 2: The illustration of our proposed TabPedia. Given the input image, TabPedia feeds it into both vision encoders attached projections to extract different granular features. Then, the visual tokens are combined with instruction-derived tokens, and fed into the LLM. The LLM leverages its powerful understanding ability to generate a plausible response.", "description": "This figure illustrates the architecture of TabPedia, a novel large vision-language model for comprehensive visual table understanding.  It shows how TabPedia processes an input image using dual vision encoders (high-resolution and low-resolution) to extract different levels of visual features. These features, along with instruction tokens, are fed into a large language model (LLM), which generates a response based on its understanding of the table content.  The use of meditative tokens is highlighted, showing their role in harmonizing table perception and comprehension tasks.", "section": "3 Method"}, {"figure_path": "aou5yrBqKy/figures/figures_8_1.jpg", "caption": "Figure 1: Comparison with previous task-specific pipelines for visual table understanding. In contrast to design different architectures for various table tasks, our TabPedia effectively performs these tasks in a unified framework through delicately leveraging the understanding capability of LLMs.", "description": "The figure compares two approaches for visual table understanding: (a) previous task-specific pipelines and (b) the proposed TabPedia.  The task-specific approach uses separate models for each subtask (table detection, structure recognition, querying, and question answering), leading to complex workflows.  In contrast, TabPedia uses a unified framework leveraging large language models (LLMs) and a concept synergy mechanism to perform all tasks seamlessly by integrating various visual embeddings and task instructions, resulting in a more efficient and flexible approach.", "section": "1 Introduction"}, {"figure_path": "aou5yrBqKy/figures/figures_17_1.jpg", "caption": "Figure 1: Comparison with previous task-specific pipelines for visual table understanding. In contrast to design different architectures for various table tasks, our TabPedia effectively performs these tasks in a unified framework through delicately leveraging the understanding capability of LLMs.", "description": "This figure compares the task-specific pipelines used in previous visual table understanding methods with the proposed TabPedia model.  The left side shows traditional approaches where different models are used for tasks like table detection, structure recognition, querying, and question answering. This results in isolated tasks and complex workflows. In contrast, TabPedia integrates all these tasks into a single, unified framework using a large language model (LLM), improving efficiency and performance.", "section": "1 Introduction"}, {"figure_path": "aou5yrBqKy/figures/figures_17_2.jpg", "caption": "Figure 1: Comparison with previous task-specific pipelines for visual table understanding. In contrast to design different architectures for various table tasks, our TabPedia effectively performs these tasks in a unified framework through delicately leveraging the understanding capability of LLMs.", "description": "This figure compares the traditional task-specific approach for visual table understanding with the proposed TabPedia approach.  The traditional approach uses separate pipelines for different tasks (table detection, structure recognition, querying, and question answering), leading to modal isolation and complex workflows. In contrast, TabPedia uses a unified framework that leverages the power of large language models (LLMs) to seamlessly integrate these tasks, resulting in a more efficient and comprehensive approach to visual table understanding.", "section": "1 Introduction"}, {"figure_path": "aou5yrBqKy/figures/figures_19_1.jpg", "caption": "Figure 2: The illustration of our proposed TabPedia. Given the input image, TabPedia feeds it into both vision encoders attached projections to extract different granular features. Then, the visual tokens are combined with instruction-derived tokens, and fed into the LLM. The LLM leverages its powerful understanding ability to generate a plausible response.", "description": "This figure illustrates the architecture of TabPedia, a novel large vision-language model for comprehensive visual table understanding.  It shows how TabPedia uses dual vision encoders (high- and low-resolution) to extract visual features, which are then combined with instruction tokens and fed into a large language model (LLM). The LLM processes this information to generate a response.", "section": "3 Method"}, {"figure_path": "aou5yrBqKy/figures/figures_21_1.jpg", "caption": "Figure 1: Comparison with previous task-specific pipelines for visual table understanding. In contrast to design different architectures for various table tasks, our TabPedia effectively performs these tasks in a unified framework through delicately leveraging the understanding capability of LLMs.", "description": "This figure compares the traditional task-specific approach to visual table understanding with the proposed TabPedia model. The traditional approach uses separate pipelines for different tasks like table detection, structure recognition, querying, and question answering. In contrast, TabPedia integrates all these tasks into a unified framework using a large language model, improving efficiency and effectiveness.", "section": "1 Introduction"}, {"figure_path": "aou5yrBqKy/figures/figures_21_2.jpg", "caption": "Figure 1: Comparison with previous task-specific pipelines for visual table understanding. In contrast to design different architectures for various table tasks, our TabPedia effectively performs these tasks in a unified framework through delicately leveraging the understanding capability of LLMs.", "description": "This figure compares the traditional task-specific approach to visual table understanding with the proposed TabPedia model.  The traditional approach involves separate pipelines for different tasks like table detection, structure recognition, querying, and question answering.  In contrast, TabPedia integrates all these tasks into a unified framework by using a large vision-language model (LLM) and a concept synergy mechanism, enabling more efficient and comprehensive understanding.", "section": "1 Introduction"}, {"figure_path": "aou5yrBqKy/figures/figures_21_3.jpg", "caption": "Figure 1: Comparison with previous task-specific pipelines for visual table understanding. In contrast to design different architectures for various table tasks, our TabPedia effectively performs these tasks in a unified framework through delicately leveraging the understanding capability of LLMs.", "description": "This figure compares the traditional task-specific approach for visual table understanding with the proposed TabPedia approach. The traditional approach uses separate pipelines for different tasks like table detection, structure recognition, querying, and question answering.  In contrast, TabPedia integrates all these tasks into a unified framework by using a large vision-language model (LLM). This allows TabPedia to leverage the LLM's capabilities for a more comprehensive and efficient understanding of visual tables.", "section": "1 Introduction"}]