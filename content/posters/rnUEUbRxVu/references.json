{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduced the Transformer architecture, which is foundational to the current state-of-the-art in many NLP tasks and is the basis of the models in the current paper."}, {"fullname_first_author": "Iz Beltagy", "paper_title": "Longformer: The long-document transformer", "publication_date": "2020-04-01", "reason": "This paper addressed the limitations of the original Transformer architecture with regards to long sequences, introducing an approach that is directly relevant to the challenges faced in the current work."}, {"fullname_first_author": "Shenda Li", "paper_title": "Functional Interpolation for Relative Positional Encoding (FIRE)", "publication_date": "2023-05-01", "reason": "This paper introduced FIRE, a functional method for positional encoding that is dynamically adjusted, providing a critical comparison point and inspiration for the proposed method."}, {"fullname_first_author": "Ta-Chung Chi", "paper_title": "KERPLE: Kernel-ized relative positional embedding for length extrapolation", "publication_date": "2022-12-01", "reason": "This paper introduced Kerple, a relative positional encoding method that the current paper uses as a comparison and baseline."}, {"fullname_first_author": "Anian Ruoss", "paper_title": "Randomized positional encodings boost length generalization of transformers", "publication_date": "2023-07-01", "reason": "This paper explores a method to improve length generalization which is a key topic of the current paper, and introduces an evaluation methodology used in the current work."}]}