[{"type": "text", "text": "decoupleQ: Towards 2-bit Post-Training Uniform Quantization via decoupling Parameters into Integer and Floating Points ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Quantization emerges as one of the most promising compression technologies for   \n2 deploying efficient large models in recent years. However, existing quantization   \n3 schemes suffer from significant accuracy degradation at very low bits, or require   \n4 some additional computational overhead when deployed, making it difficult to be   \n5 applied to large-scale applications in industry. In this paper, we propose decoupleQ,   \n6 achieving a substantial increase in model accuracy, especially at very low bits.   \n7 decoupleQ abandons the traditional heuristic quantization paradigm and decouples   \n8 the model parameters into integer and floating-point parts, then transforming the   \n9 quantization problem into a mathematical constrained optimization problem, which   \n10 is then solved alternatively by off-the-shelf solution methods. decoupleQ gets rid   \n11 of any tricks for dealing with outliers, sensitive channels, etc., and focuses only   \n12 on the basic optimization objective to achieve high model accuracy on extreme   \n13 low bit quantization. Quantization via decoupleQ is linear and uniform, making   \n14 it hardware-friendlier than non-uniform counterpart, and enabling the idea to be   \n15 migrated to high-bit quantization to enhance its robustness.   \n16 decoupleQ has achieved comparable accuracy as fp16/bf16 for 2-bit quantization of   \n17 large speech models in our company. The code (including the W2 CUDA kernels)   \n18 is attached and will be made public. ", "page_idx": 0}, {"type": "text", "text": "19 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "20 Serving large models (1; 2; 37; 38) in industry is budget-consuming because of the huge computa  \n21 tional, IO and storage cost. Model compression (10; 11; 16) has therefore become a necessity to   \n22 alleviate this pain. Among which, Post-Training Quantization (PTQ) (9; 26) has gained more and   \n23 more popularity among researchers and engineers because it does not require heavy GPU-hours   \n24 training with labeled datasets.   \n25 However, previous quantization schemes remain confined within the traditional heuristic quantization   \n26 paradigm, e.g., how to deal with outliers (33; 35), how to deal with sensitive channels (6), how   \n27 to determine the clipping range (29), and so on. These methods have achieved some success, but   \n28 the quantization at extreme low bit often suffers from significant accuracy degradation, thus failing   \n29 to meet the launching requirements of industrial practice. There are also some other options to   \n30 mitigate the accuracy loss. QuIP (4) pushes the accuracy limits of 2-bit quantization and can achieve   \n3 performance close to fp16/bf16. However, compared to traditional quantization schemes, its inference   \n3 imposes an additional burden due to the need to multiply two random orthogonal matrices to de  \n33 quant the weights. N2UQ (20) fit the real-value distribution with non-uniform grids then quantize   \n34 them into equidistant output levels. But it need to train to get the input thresholds. SpQR (7)   \n35 and SqueezeLLM (14) use mixed-precision quantization or non-uniform scheme to safeguard the   \n36 important channels, but they need customized hardware support.   \n37 In order to alleviate the above pains in industry, we proposed decoupleQ, which completely abandons   \n38 the traditional heuristic quantization paradigm and instead decouples the model parameters into   \n39 integer and floating point parts, then transforming the quantization problem into a mathematical   \n40 constrained optimization problem, which is then solved alternatively by off-the-shelf solution methods.   \n41 The integer part contains the main weights of the model, and the floating-point part contains scales   \n42 and zero points induced via quantization. decoulpeQ starts from an abstract objective function and   \n43 thus does not need any tricks to deal with the minutiae of traditional quantization paradigm, such as   \n44 outlier, salient weights (19), and so on. Quantization via decoupleQ is linear and uniform, making it   \n45 hardware-friendlier than non-uniform counterpart, and enabling the idea to be migrated to high-bit   \n46 quantization to enhance its robustness.   \n47 decoupleQ contains two stages: 1. layer-wise minimization, defined in Eq. 1, is used to optimize   \n48 the integer part and the floating-point part; 2. block-wise minimization, defined in Eq. 2, is used to   \n49 further optimize the floating-point part while freezing the integer part1.   \n50 Layer-wise minimization is to minimize the $\\ell^{2}$ loss of the outputs between pre- and post-quantization   \n51 for a linear layer: ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\widetilde{W}}\\|X\\widetilde{W}-X W_{0}\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "52 where $X\\in\\mathbb{R}^{b a t c h\\times d_{i n}}$ is the input of this layer, $W_{0}\\in\\mathbb{R}^{d_{i n}\\times d_{o u t}}$ is the pre-trained full precision   \n53 weight, $d_{i n}$ and $d_{o u t}$ are the input and output dimensions respectively. The objective is to find a   \n54 matrixW with quantized-then-dequantized elements to minimize Eq. 1.   \n55 Some works (4; 8; 9; 13; 25) started from Eq. 1 and achieved some success, but they still haven\u2019t   \n56 thought outside the box of traditional quantization. GPTQ series (8; 9) fake-quantize the first element   \n57 of $W_{0}$ and then update the the remaining elements so as to keep Eq. 1 minimized. This process is   \n58 then continued element by element until all elements are fake-quantized. However, on the one hand,   \n59 they do not give any indication of how scale and zero point should be calculated, and on the other   \n60 hand, the optimization problem formulated for updating the remaining elements is unconstrained   \n61 (explained in detail later). decoupleQ models Eq. 1 as a constrained optimization problem, as shown   \n62 in Eq. 6. It no longer needs to pay attention to some of the minutiae unique to quantization, such as   \n63 outliers, clipping threshold, etc., but abstracts the essence of the problem from a higher level. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "64 In the second stage, block-wise minimization is used to further improve the model accuracy: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\|{\\widetilde{\\mathrm{Block}}}(X)-{\\mathrm{Block}}(X)\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "65 where $\\mathrm{\\bfBlock}(\\cdot)$ is a common transformer block (32) with quantized weights. In this stage, we freeze   \n66 the integer part of the weights, and train the scales, zero points and norm layers.   \n67 decoupleQ implements 2-bit uniform quantization and achieves state-of-the-art accuracy in Llama  \n68 1/2 (30; 31). Like traditional uniform quantization, decoupleQ does not incur additional inference   \n69 burden and only requires a linear transformation to convert the quantized weights into floating point   \n70 ones. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "71 Our main highlights are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "72 \u2022 New insight: We abandoned the traditional quantization paradigm, and no longer need   \n73 to focus on some of the minutiae unique to quantization, but abstracts the essence of the   \n74 problem from a higher level and transforms it into a constrained optimization problem.   \n75 \u2022 Extreme low-bit: decoupleQ achieves 2-bit uniform quantization with performance match  \n76 ing fp16/bf16 for industrial applications in the ASR model in our company, and we will also   \n77 release the W2A16 CUDA kernel as one of our core contribution.   \n78 \u2022 Extensibility: As a bonus, if labeled datasets are available, the idea of decoupleQ can be   \n79 easily extended to supervised fune-tuning (sft) to further improve model accuracy, or the   \n80 adaptation to the downstream sub-tasks. ", "page_idx": 1}, {"type": "text", "text": "81 2 Related Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "82 Quantization can be roughly divided into Quantization Aware Training (QAT) (21; 33) and Post  \n83 Training Quantization (PTQ) (4; 35). In this paper, we focus on weight-only quantization in PTQ,   \n84 and we will only summarize a few works that are closely related to our work.   \n85 PTQ is commonly used for LLM quantization because it does not require a lot of GPU hours of   \n86 training with labeled datasets. AdaRound (25) and BRECQ (18) start from the rounding operation   \n87 and explore whether to round up or down is better. SqQR (7) and OWQ (17) use mixed-precision   \n88 quantization strategy to protect sensitive parameters, while AWQ (19) opts for scaling up the weights   \n89 of sensitive channels to reduce the loss of quantization of sensitive channels. OmniQuant (29) use   \n90 gradient decent to optimize for the weight clipping threshold and the rescale factors. In decoupleQ, we   \n91 abandon patchwork solutions and transform the quantization into a principled traditional optimization   \n92 problem by decoupling the model parameters into integer and floating-point parts.   \n93 GPTQ (9) is an influential work, and it quantizes the current weights and then updates the remaining   \n94 weights to minimize the $\\ell^{2}$ loss of the output of the layer between pre- and post-quantization. As we   \n95 will see later, this update actually approximates much, and GPTQ does not optimize for the scale and   \n96 zero point reduced by quantization.   \n97 QALora (36) also decouples model parameters at a certain level and uses labeled datasets to fine-tune   \n98 the zero points. decoupleQ takes this idea a step further, optimizing the scales, zero points and norm   \n99 layers with supervised fine-tuning, while freezing the integer weights. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "100 3 Methods ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "101 3.1 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "102 For a linear layer with input dimension $d_{i n}$ and output dimension $d_{o u t}$ , quantization maps the weights   \n103 with high-precision into discrete level, and the previous scheme can be described as follows: ", "page_idx": 2}, {"type": "text", "text": "104 ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\widehat{W}=\\mathrm{clip}(\\lfloor\\frac{W_{0}-z}{s}\\rceil,\\alpha,\\beta)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "equation", "text": "$$\n\\widetilde{W}=\\widehat{W}*s+z\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "105 where $W_{0}\\in\\mathbb{R}^{d_{i n}\\times d_{o u t}}$ is the pre-trained full precision weights, $s$ and $z$ are the scale and zero point   \n106 (what we call floating-point part above), $\\lfloor\\cdot\\rceil$ is the round-to-nearest function, $\\widehat{W}\\in\\mathbb{R}^{d_{i n}\\times d_{o u t}}$ is the   \n107 quantized integer-point matrix (what we call integer part above),W  is the de-quantized floating-point   \n108 matrix, $\\alpha$ and $\\beta$ are the lower and upper bounds of the range of  integer representations, respectively.   \n109 For example, in 2-bit weight only linear quantization scheme, the value of each entry ofW is   \n110 limited to one of $\\{-2,-1,0,1\\}$ , and $\\alpha=-2$ , $\\beta=1$ in this case. To get the values of $\\widetilde{W}$ , previous   \n111 methods (8; 9) show that layer-wise $\\ell^{2}$ loss between the outputs pre- and post-quantiz ation is well   \n112 related to the model accuracy, i.e., to optimize the following objective function, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{arg\\,min}_{\\widetilde{W}}\\|X\\widetilde{W}-X W_{0}\\|_{2}^{2}=\\mathrm{tr}\\{(\\widetilde{W}-W_{0})^{T}H(\\widetilde{W}-W_{0})\\}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "113 where $X\\in\\mathbb{R}^{b a t c h\\times d_{i n}}$ is the input of this linear layer, generated by a small set of calibration dataset,   \n114 and $H=X^{T}X$ .   \n115 In the very low-bit quantization regime, the model accuracy can be further improved via finer-grained   \n116 grouping. This would impose additional overhead on inference. For example, when groupsize $=64$ ,   \n117 it imposes an average overhead of 0.5 bit per element (FP16/BF16 for scale $s$ and zero point $z$ ). The   \n118 extra overhead is acceptable compared to the model accuracy gain. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "119 3.2 decoupleQ ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "120 When a model is quantized, only the integer part $\\widehat{W}$ and the floating-point part $(s,z)$ in Eq. 4 are   \n121 delivered to the downstream inference engine, and  the inference process does not need to know how   \n122 $\\widehat{W}$ and $(s,z)$ are obtained at all. That is, if we can find the values of $\\widehat{W}$ and $(s,z)$ to minimize Eq. 5   \n123 by other methods, then we don\u2019t need to use Eq. 3. So, we can decouple the model parameters into   \n124 integer partW  and floating point part $(s,z)$ , which are then optimized alternatively via off-the-shelf   \n125 solution methods. decoupleQ views the process of solving forW and $(s,z)$ in Eq. 4 as a constrained   \n126 optimization problem independent of the previous quantizati on paradigm! We only need to regard   \n127 Eq. 4 as an ordinary affine transformation, in which the value of $s$ can be 0 or even negative.   \n128 In per-channel quantization, each column of the weight matrix is optimized independently of each   \n129 other. For simplicity of notation, we only focus on one column inW later and re-define the notations.   \n130 Based on Eq. 5, the optimization problem of decoupleQ in the first stage, layer-wise minimization,   \n131 can then be formulated as: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\operatorname*{min}_{w;s,z}g({w;s,z})}\\\\ {\\mathrm{~s.t.~}\\forall i=1,2,...,d_{i n}}\\\\ {{w_{i}-\\beta\\leq0}}\\\\ {\\displaystyle-w_{i}+\\alpha\\leq0}\\\\ {\\displaystyle w_{i}\\in\\mathbb{Z}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "132 where the objective function is: ", "page_idx": 3}, {"type": "equation", "text": "$$\ng(w;s,z)=\\frac{1}{2}(w*s+z-b)^{T}H(w*s+z-b)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "133 $w\\,\\in\\,\\mathbb{R}^{d_{i n}}$ is one column of $\\widehat{W}$ , $b\\in\\mathbb{R}^{d_{i n}}$ is the corresponding column of $W_{0}$ , $s~\\in~\\mathbb{R}^{n g}$ is the   \n134 scale and $z\\in\\mathbb{R}^{n g}$ is the zero point, $n g$ is the number of groups when grouping-quantization. The   \n135 operations w.r.t $(s,z)$ , i.e., $^{*s}$ and $+z$ , need to be broadcasted to each group. In this paradigm,   \n136 we have completely abandoned the traditional framework of quantization and instead transformed   \n137 quantization into a constrained optimization problem 6, which is then solved to achieve the purpose   \n138 of quantization. $(s,z)$ in problem 6 have lost the traditional meaning of scale and zero point, and are   \n139 just two optimization variables.   \n140 Transforming the traditional quantization problem into problem 6 is the soul of decoupleQ! Problem 6   \n141 is a quadratic programming problem with an additional non-convex constraints $w_{i}\\in\\mathbb{Z}$ . Quadratic   \n142 programming has been studied for many years and there are now many well-established solution (24;   \n143 34). We provide one solution in the next subsection, which may not be efficient or optimal.   \n144 The core idea of decoupleQ is to decouple the model weights into the integer part $w$ and the   \n145 floating-point part $(s,z)$ , with the integer part occupying most of the model\u2019s expressive power. The   \n146 extensibility of the idea of decoupleQ is that we can freeze the integer part of the entire model, and   \n147 use labeled data to train the $(s,z)$ as well as other floating point parameters. The advantage of this is   \n148 that on the one hand, it can further improve the accuracy of the model, on the other hand, it can fit   \n149 specific downstream sub-tasks while maintaining the generalization ability of the model. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "150 3.3 Optimization via Alternative Iteration ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "151 The problem 6 is not easy to solve because of the non-convex constraint $w_{i}\\in\\mathbb{Z}$ . After obtaining a   \n152 good initialization (explained in detail later), we solve for $w$ and $(s,z)$ alternately and iteratively. In   \n153 each round of alternation, the objective function 7 w.r.t $(s,z)$ is an unconstrained quadratic function,   \n154 thus $(s,z)$ can be readily determined analytically: by differentiating the objective function and   \n155 equating the derivative to zero, followed by solving the resultant linear system of equations. While   \n156 for $w$ , the problem become problem 8: ", "page_idx": 3}, {"type": "text", "text": "157 ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{w}{\\operatorname*{min}}g(w;s,z)}\\\\ &{\\begin{array}{c}{\\mathrm{s.t.}\\,\\forall i=1,2,...,d_{i n}}\\\\ {w_{i}-\\beta\\leq0}\\\\ {-{w_{i}}+\\alpha\\leq0}\\end{array}\\qquad\\qquad(8)\\qquad\\qquad\\begin{array}{c}{\\underset{w_{i};i>j}{\\operatorname*{min}}g(w;s,z)}\\\\ {\\mathrm{s.t.}\\,\\forall i=j+1,...,d_{i n}}\\\\ {w_{i}-\\beta\\leq0}\\end{array}}\\\\ &{\\qquad\\qquad=\\ w_{i}+\\alpha\\leq0}\\\\ &{\\qquad\\qquad w_{i}\\in\\mathbb{Z}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "158 For problem 8, one solution is to round-and-clip one element of $w$ to be integer in $[\\alpha,\\beta]$ and then   \n159 update the remaining. And then this process is then performed sequentially for all elements. After the   \n160 $j$ -th element has been rounded-and-clipped, the objective for the updating then becomes problem 9. ", "page_idx": 3}, {"type": "text", "text": "161 problem 9 is also intractable, and we can make two levels of approximation: ", "page_idx": 3}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{j}{g}(w;s,z)}\\\\ &{:\\forall i=j+1,...,d_{i n}}\\\\ &{\\quad w_{i}-\\beta\\leq0}\\\\ &{\\quad-w_{i}+\\alpha\\leq0}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{w_{i};i>j}g(w;s,z)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "163 In the first-level approximation 10, only the non-convex constraint $w_{i}\\in\\mathbb{Z}$ is discarded, while in the   \n164 second-level approximation 11, both the non-convex constraint $w_{i}\\in\\mathbb{Z}$ and the convex constraint   \n165 $w_{i}\\in[\\alpha,\\beta]$ are discarded. Intuitively, problem 11 is much simpler to solve than problem 10, but   \n166 solving problem 10 will lead to a better convergence of the primary objective( 6) than solving   \n167 problem 11. GPTQ (9) provides an efficient analytical solution for problem 11, which we will   \n168 directly utilize in our experiments. ( GPTQ updates the remaining elements by considering only the   \n169 second-level approximation 11 and ignoring the constrain $w_{i}\\in[\\alpha,\\beta]$ in the first ( 10), which is what   \n170 we mentioned in the introduction, that the update of GPTQ is unconstrained.) As for problem 10,   \n171 there are many mature solutions in the field of convex optimization, such as active-set method,   \n172 projected gradient descent (PGD), projected coordinate descent and so on (3). We choose PGD   \n173 because its parallelization is much better than the other two methods. In the experimental part, we   \n174 will compare the final accuracy of the model via between solving the first level (10) and the second   \n175 level 11 approximation on small models, while on large models (e.g. lager than 7 billion parameters),   \n176 we have to choose the second level 11 approximation because the intolerable runtime of solving the   \n177 first (10). The algorithm is shown in Alg. 1 and Alg. 2. ", "page_idx": 4}, {"type": "table", "img_path": "h8goI8uPXM/tmp/d0a4cdea37ce34b89ed6e7c90e52381fd8ebec1b03436abc09384a566d9608ad.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "Algorithm 2: Approximate solution of 8 Input: predefined iteration number $K,M$ , and the froozen $(s,z)$ . Result: $w^{*}$ 1 if Approximaton $(I O)$ is used then 2 Ignoring the constraint $w_{i}\\in\\mathbb{Z}$ in Eq. 8, and train Eq. 8 with $M$ iterations via PGD; 3 Initialize $j=1$ ; 4 for $j=1\\rightarrow d_{i n}$ do 5 round and clip the $j$ -th element of $w$ , then keep the first $j$ elements frozen, and update the remainings via PGD to optimize 10 with $K$ iterations or until converged, or via the method in GPTQ to optimize 11. 6 end 7 w\u2217= w ", "page_idx": 4}, {"type": "text", "text": "179 3.4 Initialization of $w$ and $(s,z)$ ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "180 Since the values of $w$ are discrete, a good ini  \n181 tialization is very important in order to obtain a   \n182 more accurate solution to the original problem 6   \n183 with a faster convergence. Intuitively, the func  \n184 tion $g(w;s,z)$ contains the term $w\\ast s$ , which   \n185 means that the scales of the initial values of $w$   \n186 and $s$ have to be reasonably distributed. For ex  \n187 ample, in the extreme case when the initial value   \n188 of $(s,z)$ have a very large scale, the first itera  \n189 tion will make most of the entries of $w$ strictly   \n190 0, which will make the iteration crash. We start   \n191 by initializing $(s,z)$ . We can use grid search to   \n192 solve the Eq. 12 for the initial value of $(s,z)$ . In   \n193 Eq. 12, $p$ is a single number, may be different   \n194 for different columns of $W_{0}$ , $b_{m i n}$ and $b_{m a x}$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{p}\\frac{1}{2}(w*s+z-b)^{T}H(w*s+z-b)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle w=\\mathrm{clip}(\\lfloor\\frac{b-z}{s}\\rceil,\\alpha,\\beta)}}\\\\ {{\\displaystyle s=\\frac{p*(b_{m a x}-b_{b m i n})}{\\beta-\\alpha}}}\\\\ {{\\displaystyle z=p*b_{m i n}-s*\\alpha}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "are the minimum and maximum value of $b$ respectively. 195 This step is the same as the previous post-training quantization (19) process. Once the grid search is ", "page_idx": 4}, {"type": "text", "text": "196 finished, we no longer need to concern ourselves with the $(s,z)$ inside the $\\lfloor\\cdot\\rceil$ function. The point of   \n197 this step is simply to find an initial value for $(s,z)$ for the optimization problem 6.   \n198 When solving problem 8 via the first-level approximation ( 10), before entering the for-loop in Alg. 2,   \n199 we ignore the constraint $w_{i}\\in\\mathbb{Z}$ in problem 8 and optimize it via projected gradient decent with $M$   \n200 iterations. The purpose of this is to allow the first-level approximation to converge in a small number   \n201 of iterations, i.e., a small $K$ . ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "202 3.5 Block-wise minimization ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "203 After solving problem 6, we obtain a solution for the layer-wise minimization stage and a reasonable   \n204 model accuracy. But minimizing the $\\ell^{2}$ loss at the layer level does not necessarily lead to the   \n205 minimizing the $\\ell^{2}$ loss at the block level. We found that the model accuracy can be further improved   \n206 via optimization 2. BRECQ (18) also shows that block-reconstruction results in a better model   \n207 accuracy than layer-reconstruction. In this stage, we freeze the integer partW in the whole block and   \n208 fine-tuning $(s,z)$ and the parameters in norm layer with $J$ epochs. ", "page_idx": 5}, {"type": "text", "text": "209 4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "210 In this section, we describe in detail the experimental results of our method in comparison with other   \n211 methods. Unless otherwise stated, all the experiments are conducted on a single A100-SXM-80GB,   \n212 and the default experimental setting is as follows:   \n213 ResNet: 10240 images in the training dateloader are used as calibration data, with the standard   \n214 augmentation in Pytorch official code (27), and the pretrained full precision checkpoints are from   \n215 Torchvision (22). $N=4$ , $M=50$ $N$ and $M$ is defined in refalg1 and refalg2). All the convolution   \n216 layers and fully-connected layers are quantized into W2 without groups.   \n217 Llama-1/2: 128 2048-token segments from C4 (28) are used as calibration data. We choose C4   \n218 as calibration dataset instead of WikiText2 (23) to be consistent with GPTQ. If the block-wise   \n219 minimization is used, we use Adam optimizer (15) to finetune the $(s,z)$ and the parameters in norm   \n220 layer with $J=4$ epochs. The learning rate is $1e{-5}$ , weight decay is 1e-6. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "221 4.1 Private Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "222 We applied decoupleQ to our company\u2019s two   \n223 Automatic Speech Recognition models(ASR)   \n224 (corresponding to task A and task B). Each of   \n225 the models contain an encoder and an LLM de  \n226 coder. The input of the models is a speech se  \n227 quence and some prompt, and the output is the   \n228 corresponding text. We quantize the LLM de  \n229 coder to W2A16g64. The decoders of the two   \n230 models contain 40 transformer blocks with 13   \n231 billion parameters and 32 transformer blocks   \n232 with 7 billion parameters, respectively. Word Er  \n233 ror Rate (WER) is used as metric to measure the   \n234 accuracy of the models (less is better). In this   \n235 experiments, we use about 8 millions of speech   \n236 tokens as calibration dataset, and train 3 epoch   \n237 in each block-wise minimization process. When   \n238 an input batch contains sequences of varying   \n239 lengths, we use a mask to make sure that the   \n240 padding part is not involved in the computation of $H$ and the loss of Eq. 2. In task B, once the whole   \n241 model is quantized, we also fine-tune all the $(s,z)$ and layer norm in the LLM with labeled dataset,   \n242 while freezing all the integer part $\\widehat{W}$ , with 8 A100-SXM-80GB GPUs. The accuracy is shown in   \n243 Tab. 1, and the CUDA kernel latency is shown in Fig. 1. The W2A16 CUDA kernel is attached and   \n244 will be merged into the NVIDIA repo as one of our core contribution. ", "page_idx": 5}, {"type": "image", "img_path": "h8goI8uPXM/tmp/b2a02aec88983be4bf8feada4a06f1371f16c8fe30e0621e97df71a6e938be48.jpg", "img_caption": ["Figure 1: The latency (in $^{1e-6}$ seconds) of the four GEMMs in transformer block on L4 GPU, (The three GEMMs for query, key and value are concatenated into GEMM 1), with hidden_dim $=$ 5120, batch_size $=4$ . "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "table", "img_path": "h8goI8uPXM/tmp/b839497b445827e253e449233dcc37bf9254cdee60dd4132010411a85b6b86dd.jpg", "table_caption": ["Table 1: The results of our two ASR models. The models are quantized into $\\mathrm{W}2\\mathrm{Al6g64}$ . runtime for the quantization process is measured in hours. There are two sub-domains in task B, and we report the WER of both. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "h8goI8uPXM/tmp/6842bb46611068d066c980a2282844ae3bfa09fff5e5914a98d87482c30ce647.jpg", "table_caption": ["Table 2: Comparison of decoupleQ with other methods. In decoupleQ, we only use the first stage, layer-wise minimization. All the models are quantized into W2A16 without groups. In decoupleQ+sft, we train the $(s,z)$ and norm layers for one epoch, using the regular labeled dataset containing 1.2 million images. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "245 4.2 Public Comparison ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "246 As a first comparison, we compare decoupleQ with other methods on ImageNet (5) with ResNet (12),   \n247 which are standard benchmarks and are efficient to implement. Most importantly, its Top-1 is a strong   \n248 indicator of model accuracy. Tab. 2 shows the results of decoupleQ and others. The results other than   \n249 decoupleQ are copied from GPTQ (9) and OBQ (8).   \n250 Tab. 3 shows the results on Llama. In this experiment, we have to choose the second level approxima  \n251 tion(11) because the intolerable runtime of solving the first(10). For a fair comparison, the calibration   \n252 dataset contains 128 samples, although a larger calibration dataset will result in stronger results.   \n253 we can see that decoupleQ outperforms others almost in all settings, although we use a weaker   \n254 approximation(11) to save time. As for the hype-parameters, we choose $\\{N=\\bar{4},J=4\\}$ . ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "255 4.3 Ablation studies ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "256 4.3.1 the two approximations ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "257 The soul of decoupleQ is problem 6, but when solving problem 6, we have to take some approxima  \n258 tions(10 or 11). Obviously, solving approximation 10 will be much more time consuming than solving   \n259 approximation 11. But if solving approximation 10 yields better results, the time cost may be worth   \n260 it. We first evaluate these two approximations from the perspective of model accuracy. In practice,   \n261 we don\u2019t have to wait for approximation 10 to fully converge when we solve it via projected gradient   \n262 decent, and only need to iterate some steps to get a sub-optimal solution. In Alg. 2, the for-loop takes   \n263 up the majority of the runtime. So, we first study the influence of the number of iterations $K$ (defined   \n264 in the for-loop) on the final accuracy of the model. Fig. 2 shows the Top-1 accuracy of ResNet-18 on   \n265 ImageNet w.r.t the number of iterations $K$ . First of all, in the blue line, we use only the layer-wise   \n266 minimization of decooupleQ to quantize the model. After the quantization is finished, in the red line,   \n267 we use the labeled dataset with the common 1.2 millions images to fine-tune all the $(s,z)$ and norm   \n268 layers for one epoch, with the integer part being frozen. In this step, we use SGD optimizer with   \n269 learning rate 1e-6, weight decaying rate 1e-4 to train for only one epoch. Fig. 2 clearly indicates the   \n270 following conclusions: 1. As the number of iterations $K$ increases, the model accuracy increases   \n271 almost monotonically; 2. When $K>4$ , model accuracy via the first approximation(10) is better than   \n272 via the second(11). This is to be expected, since the second approximation(11) drops the constraint   \n273 $\\alpha\\leq w_{i}\\leq\\beta$ , leading to a looser approximation; 3. By the supervised fine-tuning (sft), the model   \n274 accuracy is further improved. The same experimental phenomenon also occurs on the ResNet-50   \n275 model, which we do not show here.   \n276 In the experiment shown in 3, we randomly select 512 2048-token segments from C4 (28). We chose   \n277 512 segments here instead of the common 128 in order to reduce the effect of overfitting and thus   \n278 compare the two approximations more objectively. In this experiment, we take $N=2$ , and quantize   \n279 Llama-7B into W2A16 without groups, and only the layer-wise minimization is used to exclude the   \n280 interference of other factors. The PPL decrease almost monotonically as the number of iterations $K$   \n281 increases. It shows that, when $K>1$ , solving approximation 10 yields better model accuracy than   \n282 approximation 11.   \n283 However, when block-wise minimization is introduced in addition to the experiment in 3, the situation   \n284 becomes a little more elusive. The results are shown in 4. The model\u2019s best PPL is where $K=1$ ,   \n285 and then fluctuates within a range as $K$ continues to increase. But all PPLs are inferior to when   \n286 the second-level approximation (11) is used. We also plot the loss, defined in 2, of the first block   \n287 between pre-and post quantization on the right vertical axis. As $K$ increases, the loss decreases   \n288 strictly monotonically, and when $K>2$ , the loss falls below the case when the approximation 11 is   \n289 used. This suggests that the correlation between PPL and loss is perhaps weak, and we will investigate   \n290 this in the future. ", "page_idx": 6}, {"type": "table", "img_path": "h8goI8uPXM/tmp/03cc37aeb3e3714ec2b059c838a474ca8bea072a0f83164699565bd11f268618.jpg", "table_caption": ["Table 3: The results of PPL of wikitext-2 on Llama-1/2. We also report the runtime (measured in hours) for the W2 quantization via decoupleQ in the gray background row. The results other than decoupleQ are copied from OmniQuant (29). All the results of decoupleQ use the approximation 11. "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "h8goI8uPXM/tmp/a8dba25bba481f9ce344bf7cf569b3e853e70f4fadf9942cd835727cdf63e3a7.jpg", "img_caption": ["Figure 2: The Top-1 accuracy of ResNet-18 on ImageNet. Solid and dashed lines are for approximation 10 and 11 respectively. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "h8goI8uPXM/tmp/ba52c63389b4244767adb6be8955485c3ff01dc5d03903bfdce4a7d0b5eea1a4.jpg", "img_caption": ["Figure 3: The PPL of Llama-7B on WikiText2. Solid and dashed lines are for approximation 10 and 11 respectively. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "h8goI8uPXM/tmp/8a7c5d9684b4e65be624041e00f91cd0ebadd1c2cea75553368587afd9822ad5.jpg", "img_caption": ["Figure 4: The PPL of Llama-7B on WikiText2 and the loss of the first block between pre-and post-quantization. Solid and dashed lines are for approximation 10 and 11 respectively. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "h8goI8uPXM/tmp/c339331c5fdc9410c8c1a2902f577a296e2b84f8d171458352178dfee9efbf3f.jpg", "img_caption": ["Figure 5: The perplexity of Llama-7B on WikiText2 and C4 dataset w.r.t the number of segments as calibration datasets. The model is quantized into W2A16g64. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "291 4.3.2 the size of calibration dataset ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "292 The solution of problem 6 is dependent on $H$ and thus on the the calibration dataset, as does Eq. 2.   \n293 Fig. 5 shows the relationship between dataset size and PPL. In this experiment, Llama-7B is quantized   \n294 into W2A16g64. We use the second-level approximation (11) to save time, and $\\{N=4,J=4\\}$ . For   \n295 runtime reference, when the number of segments is 128/2048, the experiment took 4.3/19.5 hours. ", "page_idx": 8}, {"type": "text", "text": "296 4.3.3 the necessity of block-wise minimization ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "297 Tab. 4 shows that block-wise minimiza- Table 4: The perplexity of Llama on WikiText2 with   \n298 tion(2) can further improve the model accu- and without the block-wise minimization. All the mod  \n299 racy. In this experiment, we choose $N=4$ els are quantized into W2A16.   \n300 and the approximation 11 for the layer-wise   \n301 minimization, and $J=4$ if block-wise min  \n302 imization is used. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "table", "img_path": "h8goI8uPXM/tmp/d8af5943c90d72560ceeec800939e89b2bbfdce014740661be8d5262c41b0eb3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "303 5 Conclusion and Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "304 deocupleQ decouples the model parameters into the integer part and a floating point part, and   \n305 then optimizes them alternately. This optimization process contains two stages. In the layer-wise   \n306 minimization, we transform the quantization problem into the purely mathematical constrained   \n307 optimization problem refdecoupleQ; while in the block-wise minimization, we freeze the integer part   \n308 and then finetune the floating point part.   \n309 The risks of decoupleQ include the following: 1. How much the minimization of the $\\ell^{2}$ loss of   \n310 the layer\u2019s or block\u2019s output correlates with the accuracy of the model; 2. decoupleQ is prone to   \n311 overfitting the calibration dataset; 3. The runtime of the quantization process is longer than others.   \n312 For the first risk, we find experimentally that the correlation between Top-1 and the loss is strong in   \n313 the Imagenet classification task; however, the correlation between PPL and the loss is slightly weaker   \n314 in LLM. This could be mainly because of an inherent bias between the loss and the accuracy of the   \n315 model, or because PPL is not a good indicator of the accuracy of LLM, or for other reasons. For   \n316 the second risk, when $H$ in Eq. 7 is an underdetermined matrix, the risk of overfitting rises sharply.   \n317 In this case, the possibility of $H$ being underdetermined can be reduced either by enhancing the   \n318 diagonal element values of $H$ or by increasing the amount of calibration data. In our practice, we   \n319 found that the accuracy of quantization models can rise monotonically with the increase of the size of   \n320 the calibration dataset, especially in W2 quantization, but the runtime of quantization rise as well. In   \n321 addition, due to time constraints, we do not provide a wealth of public comparisons. However, we   \n322 believe that the novelty of a method may outweigh the number of experiments.   \n323 The idea of decoupleQ is helpful for the adaptation of large model to downstream sub-task. We can   \n324 quantize a large foundation model via decoupleQ, then freeze the integer part of the model, and   \n325 finetune the floating-point part with labeled dataset from downstream sub-task. Tab. 1 and Tab. 2   \n326 show that the model accuracy can be further improved by end-to-end supervised learning. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "327 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "328 [1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind   \n329 Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.   \n330 Advances in neural information processing systems, 33:1877\u20131901, 2020.   \n331 [2] S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar,   \n332 Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early   \n333 experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.   \n334 [3] S\u00e9bastien Bubeck et al. Convex optimization: Algorithms and complexity. Foundations and Trends\u00ae in   \n335 Machine Learning, 8(3-4):231\u2013357, 2015.   \n336 [4] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher De Sa. Quip: 2-bit quantization of large   \n337 language models with guarantees. arXiv preprint arXiv:2307.13304, 2023.   \n338 [5] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical   \n339 image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255.   \n340 Ieee, 2009.   \n341 [6] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication   \n342 for transformers at scale. arXiv preprint arXiv:2208.07339, 2022.   \n343 [7] Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos,   \n344 Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. Spqr: A sparse-quantized representation for   \n345 near-lossless llm weight compression. arXiv preprint arXiv:2306.03078, 2023.   \n346 [8] Elias Frantar and Dan Alistarh. Optimal brain compression: A framework for accurate post-training   \n347 quantization and pruning. Advances in Neural Information Processing Systems, 35:4475\u20134488, 2022.   \n348 [9] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Optq: Accurate quantization for   \n349 generative pre-trained transformers. In The Eleventh International Conference on Learning Representations,   \n350 2022.   \n351 [10] Yi Guo, Yiqian He, Xiaoyang Li, Haotong Qin, Van Tung Pham, Yang Zhang, and Shouda Liu. Rdimkd:   \n352 Generic distillation paradigm by dimensionality reduction. arXiv preprint arXiv:2312.08700, 2023.   \n353 [11] Yi Guo, Huan Yuan, Jianchao Tan, Zhangyang Wang, Sen Yang, and Ji Liu. Gdp: Stabilized neural   \n354 network pruning via gates with differentiable polarization. In Proceedings of the IEEE/CVF International   \n355 Conference on Computer Vision, pages 5239\u20135250, 2021.   \n356 [12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.   \n357 In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \n358 [13] Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and Daniel Soudry. Accurate post training   \n359 quantization with small calibration sets. In International Conference on Machine Learning, pages 4466\u2013   \n360 4475. PMLR, 2021.   \n361 [14] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W Mahoney,   \n362 and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization. arXiv preprint arXiv:2306.07629, 2023.   \n363 [15] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint   \n364 arXiv:1412.6980, 2014.   \n365 [16] Raghuraman Krishnamoorthi. Quantizing deep convolutional networks for efficient inference: A whitepaper.   \n366 arXiv preprint arXiv:1806.08342, 2018.   \n367 [17] Changhun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, and Eunhyeok Park. Owq: Lessons learned from   \n368 activation outliers for weight quantization in large language models. arXiv preprint arXiv:2306.02272,   \n369 2023.   \n370 [18] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi   \n371 Gu. Brecq: Pushing the limit of post-training quantization by block reconstruction. arXiv preprint   \n372 arXiv:2102.05426, 2021.   \n373 [19] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware   \n374 weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023.   \n375 [20] Zechun Liu, Kwang-Ting Cheng, Dong Huang, Eric P Xing, and Zhiqiang Shen. Nonuniform-to-uniform   \n376 quantization: Towards accurate quantization via generalized straight-through estimation. In Proceedings of   \n377 the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4942\u20134952, 2022.   \n378 [21] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi,   \n379 Raghuraman Krishnamoorthi, and Vikas Chandra. Llm-qat: Data-free quantization aware training for large   \n380 language models. arXiv preprint arXiv:2305.17888, 2023.   \n381 [22] S\u00e9bastien Marcel and Yann Rodriguez. Torchvision the machine-vision package of torch. In Proceedings   \n382 of the 18th ACM international conference on Multimedia, pages 1485\u20131488, 2010.   \n383 [23] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models.   \n384 arXiv preprint arXiv:1609.07843, 2016.   \n385 [24] Katta G Murty and Feng-Tien Yu. Linear complementarity, linear and nonlinear programming, volume 3.   \n386 Heldermann Berlin, 1988.   \n387 [25] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or   \n388 down? adaptive rounding for post-training quantization. In International Conference on Machine Learning,   \n389 pages 7197\u20137206. PMLR, 2020.   \n390 [26] Yury Nahshan, Brian Chmiel, Chaim Baskin, Evgenii Zheltonozhskii, Ron Banner, Alex M Bronstein, and   \n391 Avi Mendelson. Loss aware post-training quantization. Machine Learning, 110(11-12):3245\u20133262, 2021.   \n392 [27] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,   \n393 Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep   \n394 learning library. Advances in neural information processing systems, 32, 2019.   \n395 [28] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,   \n396 Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.   \n397 Journal of machine learning research, 21(140):1\u201367, 2020.   \n398 [29] Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng   \n399 Gao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally calibrated quantization for large language   \n400 models. arXiv preprint arXiv:2308.13137, 2023.   \n401 [30] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,   \n402 Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation   \n403 language models. arXiv preprint arXiv:2302.13971, 2023.   \n404 [31] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay   \n405 Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and   \n406 fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n407 [32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz   \n408 Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems,   \n409 30, 2017.   \n410 [33] Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, and Xianglong   \n411 Liu. Outlier suppression+: Accurate quantization of large language models by equivalent and optimal   \n412 shifting and scaling. arXiv preprint arXiv:2304.09145, 2023.   \n413 [34] Stephen J Wright. Numerical optimization. 2006.   \n414 [35] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant:   \n415 Accurate and efficient post-training quantization for large language models. In International Conference   \n416 on Machine Learning, pages 38087\u201338099. PMLR, 2023.   \n417 [36] Yuhui Xu, Lingxi Xie, Xiaotao Gu, Xin Chen, Heng Chang, Hengheng Zhang, Zhensu Chen, Xiaopeng   \n418 Zhang, and Qi Tian. Qa-lora: Quantization-aware low-rank adaptation of large language models. arXiv   \n419 preprint arXiv:2309.14717, 2023.   \n420 [37] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher   \n421 Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models.   \n422 arXiv preprint arXiv:2205.01068, 2022.   \n423 [38] Yu Zhang, Wei Han, James Qin, Yongqiang Wang, Ankur Bapna, Zhehuai Chen, Nanxin Chen, Bo Li,   \n424 Vera Axelrod, Gary Wang, et al. Google usm: Scaling automatic speech recognition beyond 100 languages.   \n425 arXiv preprint arXiv:2303.01037, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "426 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "428 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n429 paper\u2019s contributions and scope?   \n432 a. Our results are higher than others in very low bit (2-bit) quantization.( This is   \n433 justified in Tab. 3.);   \n434 b. decoupleQ has achieved comparable accuracy as fp16/bf16 for 2-bit quantiza  \n435 tion of large speech models in our company. (This is justified in Tab. 1, and the   \n436 W2 CUDA kernel used in our company are attached.);   \n437 c. decoupleQ gets rid of any tricks for dealing with outliers, sensitive channels,   \n438 etc. (This is justified in the Problem 6, we do not use any tricks, such as scaling   \n439 factor (19; 29), mixed-precision quantization (6), etc., to deal with outliers and   \n440 sensitive channels.) ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 11}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 11}, {"type": "text", "text": "Justification: The paper has discussed the three limitations of decoupleQ in the last section, Conclusion and Discussion, and the risk overall that we did not provide as many public comparison experiments as other work due to time constraints. ", "page_idx": 11}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. ", "page_idx": 11}, {"type": "text", "text": "78 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n479 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n480 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n481 judgment and recognize that individual actions in favor of transparency play an impor  \n482 tant role in developing norms that preserve the integrity of the community. Reviewers   \n483 will be specifically instructed to not penalize honesty concerning limitations.   \n484 3. Theory Assumptions and Proofs   \n485 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n486 a complete (and correct) proof?   \n487 Answer:[NA]   \n488 Justification: This paper does not include theoretical results.   \n489 Guidelines:   \n490 \u2022 The answer NA means that the paper does not include theoretical results.   \n491 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n492 referenced.   \n493 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n494 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n495 they appear in the supplemental material, the authors are encouraged to provide a short   \n496 proof sketch to provide intuition.   \n497 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n498 by formal proofs provided in appendix or supplemental material.   \n499 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced.   \n500 4. Experimental Result Reproducibility   \n501 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n502 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n503 of the paper (regardless of whether the code and data are provided or not)?   \n504 Answer: [Yes]   \n505 Justification: At the beginning of the section Experiments, we provide details of the experi  \n506 mental parameters; specifically for each experiment, we also provide the key experimental   \n507 parameters.   \n508 Guidelines:   \n509 \u2022 The answer NA means that the paper does not include experiments.   \n510 \u2022 If the paper includes experiments, a No answer to this question will not be perceived   \n511 well by the reviewers: Making the paper reproducible is important, regardless of   \n512 whether the code and data are provided or not.   \n513 \u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken   \n514 to make their results reproducible or verifiable.   \n515 \u2022 Depending on the contribution, reproducibility can be accomplished in various ways.   \n516 For example, if the contribution is a novel architecture, describing the architecture fully   \n517 might suffice, or if the contribution is a specific model and empirical evaluation, it may   \n518 be necessary to either make it possible for others to replicate the model with the same   \n519 dataset, or provide access to the model. In general. releasing code and data is often   \n520 one good way to accomplish this, but reproducibility can also be provided via detailed   \n521 instructions for how to replicate the results, access to a hosted model (e.g., in the case   \n522 of a large language model), releasing of a model checkpoint, or other means that are   \n523 appropriate to the research performed.   \n524 \u2022 While NeurIPS does not require releasing code, the conference does require all submis  \n525 sions to provide some reasonable avenue for reproducibility, which may depend on the   \n526 nature of the contribution. For example   \n527 (a) If the contribution is primarily a new algorithm, the paper should make it clear how   \n528 to reproduce that algorithm.   \n529 (b) If the contribution is primarily a new model architecture, the paper should describe   \n530 the architecture clearly and fully.   \n531 (c) If the contribution is a new model (e.g., a large language model), then there should   \n532 either be a way to access this model for reproducing the results or a way to reproduce   \n533 the model (e.g., with an open-source dataset or instructions for how to construct   \n534 the dataset).   \n535 (d) We recognize that reproducibility may be tricky in some cases, in which case   \n536 authors are welcome to describe the particular way they provide for reproducibility.   \n537 In the case of closed-source models, it may be that access to the model is limited in   \n538 some way (e.g., to registered users), but it should be possible for other researchers   \n539 to have some path to reproducing or verifying the results. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "540 5. Open access to data and code ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "541 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n542 tions to faithfully reproduce the main experimental results, as described in supplemental   \n543 material? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: The code (including W2 CUDA kernels) is attached in supplementary material, and can reproduce the results in the public experiments. ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 13}, {"type": "text", "text": "567 6. Experimental Setting/Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "8 Question: Does the paper specify all the training and test details (e.g., data splits, hyper9 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: At the beginning of the section Experiments, we provide details of the experimental parameters; specifically for each experiment, we also provide the key experimental parameters. The code is attached in supplementary material and will be made public. Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 13}, {"type": "text", "text": "581 7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "582 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n583 information about the statistical significance of the experiments?   \n585 Justification: The cost of the experiment is high.   \n586 Guidelines:   \n587 \u2022 The answer NA means that the paper does not include experiments.   \n588 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n589 dence intervals, or statistical significance tests, at least for the experiments that support   \n590 the main claims of the paper.   \n591 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n592 example, train/test split, initialization, random drawing of some parameter, or overall   \n593 run with given experimental conditions).   \n594 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n595 call to a library function, bootstrap, etc.)   \n596 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n597 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n598 of the mean.   \n599 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n600 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n601 of Normality of errors is not verified.   \n602 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n603 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n604 error rates).   \n605 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n606 they were calculated and reference the corresponding figures or tables in the text.   \n607 8. Experiments Compute Resources   \n608 Question: For each experiment, does the paper provide sufficient information on the com  \n609 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n610 the experiments?   \n611 Answer: [Yes]   \n612 Justification: We have reported that most of the experiments are conducted in one single   \n613 A100-SXM-80GB, except for the sft process. And we also reported the time of execution.   \n614 Guidelines:   \n615 \u2022 The answer NA means that the paper does not include experiments.   \n616 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n617 or cloud provider, including relevant memory and storage.   \n618 \u2022 The paper should provide the amount of compute required for each of the individual   \n619 experimental runs as well as estimate the total compute.   \n620 \u2022 The paper should disclose whether the full research project required more compute   \n621 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n622 didn\u2019t make it into the paper).   \n623 9. Code Of Ethics   \n624 Question: Does the research conducted in the paper conform, in every respect, with the   \n625 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n626 Answer: [Yes]   \n627 Justification: The research conducted in the paper conform, in every respect, with the   \n628 NeurIPS Code of Ethics   \n629 Guidelines:   \n630 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n631 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n632 deviation from the Code of Ethics.   \n633 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n634 eration due to laws or regulations in their jurisdiction).   \n635 10. Broader Impacts   \n636 Question: Does the paper discuss both potential positive societal impacts and negative   \n637 societal impacts of the work performed?   \n638 Answer: [NA]   \n639 Justification: This is a work for accelerating the inference of deep models, where the social   \n640 impact is determined by the function of the model, not by how the inference is accelerated.   \n641 Guidelines:   \n642 \u2022 The answer NA means that there is no societal impact of the work performed.   \n643 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n644 impact or why the paper does not address societal impact.   \n645 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n646 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n647 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n648 groups), privacy considerations, and security considerations.   \n649 \u2022 The conference expects that many papers will be foundational research and not tied   \n650 to particular applications, let alone deployments. However, if there is a direct path to   \n651 any negative applications, the authors should point it out. For example, it is legitimate   \n652 to point out that an improvement in the quality of generative models could be used to   \n653 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n654 that a generic algorithm for optimizing neural networks could enable people to train   \n655 models that generate Deepfakes faster.   \n656 \u2022 The authors should consider possible harms that could arise when the technology is   \n657 being used as intended and functioning correctly, harms that could arise when the   \n658 technology is being used as intended but gives incorrect results, and harms following   \n659 from (intentional or unintentional) misuse of the technology.   \n660 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n661 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n662 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n663 feedback over time, improving the efficiency and accessibility of ML).   \n664 11. Safeguards   \n665 Question: Does the paper describe safeguards that have been put in place for responsible   \n666 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n667 image generators, or scraped datasets)?   \n668 Answer: [NA]   \n669 Justification: This work does not release models or datasets.   \n670 Guidelines:   \n671 \u2022 The answer NA means that the paper poses no such risks.   \n672 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n673 necessary safeguards to allow for controlled use of the model, for example by requiring   \n674 that users adhere to usage guidelines or restrictions to access the model or implementing   \n675 safety filters.   \n676 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n677 should describe how they avoided releasing unsafe images.   \n678 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n679 not require this, but we encourage authors to take this into account and make a best   \n680 faith effort.   \n681 12. Licenses for existing assets   \n682 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n683 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n684 properly respected?   \n685 Answer: [Yes]   \n686 Justification: The paper has cited all related works, and included the relevant license.   \n687 Guidelines:   \n688 \u2022 The answer NA means that the paper does not use existing assets.   \n689 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n690 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n691 URL.   \n692 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n693 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n694 service of that source should be provided.   \n695 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n696 package should be provided. For popular datasets, paperswithcode.com/datasets   \n697 has curated licenses for some datasets. Their licensing guide can help determine the   \n698 license of a dataset.   \n699 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n700 the derived asset (if it has changed) should be provided.   \n701 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n702 the asset\u2019s creators. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "703 13. New Assets ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "04 Question: Are new assets introduced in the paper well documented and is the documentation   \n05 provided alongside the assets? ", "page_idx": 16}, {"type": "text", "text": "Justification: We provide the source code, and a readme and license file are alongside. Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 16}, {"type": "text", "text": "717 14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "718 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n719 include the full text of instructions given to participants and screenshots, if applicable, as   \n720 well as details about compensation (if any)?   \n722 Justification: the paper does not involve crowdsourcing nor research with human subjects   \n723 Guidelines: ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n5 human subjects.   \n6 \u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be   \n8 included in the main paper.   \n9 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n0 or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 16}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 16}, {"type": "text", "text": "734 Question: Does the paper describe potential risks incurred by study participants, whether   \n735 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n736 approvals (or an equivalent approval/review based on the requirements of your country or   \n737 institution) were obtained?   \nAnswer: [NA] ", "page_idx": 16}, {"type": "text", "text": "Justification: the paper does not involve crowdsourcing nor research with human subjects Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 17}]