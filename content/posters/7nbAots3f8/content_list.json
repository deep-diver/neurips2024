[{"type": "text", "text": "Learning Unsigned Distance Fields from Local Shape Functions for 3D Surface Reconstruction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Unsigned distance fields (UDFs) provide a versatile framework for representing   \n2 a diverse array of 3D shapes, encompassing both watertight and non-watertight   \n3 geometries. Traditional UDF learning methods typically require extensive training   \n4 on large datasets of 3D shapes, which is costly and often necessitates hyperparame  \n5 ter adjustments for new datasets. This paper presents a novel neural framework,   \n6 $L o S F-U D F$ , for reconstructing surfaces from 3D point clouds by leveraging lo  \n7 cal shape functions to learn UDFs. We observe that 3D shapes manifest simple   \n8 patterns within localized areas, prompting us to create a training dataset of point   \n9 cloud patches characterized by mathematical functions that represent a continuum   \n10 from smooth surfaces to sharp edges and corners. Our approach learns features   \n11 within a specific radius around each query point and utilizes an attention mecha  \n12 nism to focus on the crucial features for UDF estimation. This method enables   \n13 efficient and robust surface reconstruction from point clouds without the need for   \n14 shape-specific training. Additionally, our method exhibits enhanced resilience   \n15 to noise and outliers in point clouds compared to existing methods. We present   \n16 comprehensive experiments and comparisons across various datasets, including   \n17 synthetic and real-scanned point clouds, to validate our method\u2019s efficacy. ", "page_idx": 0}, {"type": "text", "text": "18 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "19 3D surface reconstruction from raw point clouds is a significant and long-standing problem in   \n20 computer graphics and machine vision. Traditional techniques like Poisson Surface Reconstruction[1]   \n21 create an implicit indicator function from oriented points and reconstruct the surface by extracting   \n22 an appropriate isosurface. The advancement of artificial intelligence has led to the emergence   \n23 of numerous neural network-based methods for 3D reconstruction. Among these, neural implicit   \n24 representations have gained significant influence, which utilize signed distance fields (SDFs) [2\u20138]   \n25 and occupancy fields [9\u201312] to implicitly depict 3D geometries. SDFs and occupancy fields extract   \n26 isosurfaces by solving regression and classification problems, respectively. However, both techniques   \n27 require internal and external definitions of the surfaces, limiting their capability to reconstructing only   \n28 watertight geometries. Therefore, unsigned distance fields [13\u201320] have recently gained increasing   \n29 attention due to their ability to reconstruct non-watertight surfaces and complex geometries with   \n30 arbitrary topologies.   \n31 Reconstructing 3D geometries from raw point clouds using UDFs presents significant challenges due   \n32 to the non-differentiability near the surface. This characteristic complicates the development of loss   \n33 functions and undermines the stability of neural network training. Various unsupervised approaches   \n34 [17, 14, 19] have been developed to tailor loss functions that leverage the intrinsic characteristics   \n35 of UDFs, ensuring that the reconstructed geometry aligns closely with the original point clouds.   \n36 However, these methods suffer from slow convergence, necessitating an extensive network training   \n37 time to reconstruct a single geometry. As a supervised method, GeoUDF [15] learns local geometric   \n38 priors through training on datasets such as ShapeNet [21], thus achieving efficient UDF estimation.   \n39 Nonetheless, the generalizability of this approach is dependent on the training dataset, which also   \n40 leads to relatively high computational costs.   \n41 In this paper, we propose a lightweight and effective supervised learning framework, Losf-UDF, to   \n42 address these challenges. Since learning UDFs does not require determining whether a query point is   \n43 inside or outside the geometry, it is a local quantity independent of the global context. Inspired by the   \n44 observation that 3D shapes manifest simple patterns within localized areas, we synthesize a training   \n45 dataset comprising a set of point cloud patches by utilizing local shape functions. Subsequently, we   \n46 can estimate the unsigned distance values by learning local geometric features through an attention  \n47 based network. Our approach distinguishes itself from existing methods by its novel training strategy.   \n48 Specifically, it is uniquely trained on synthetic surfaces, yet it demonstrates remarkable capability   \n49 in predicting UDFs for a wide range of common surface types. For smooth surfaces, we generate   \n50 training patches (quadratic surfaces) by analyzing principal curvatures, meanwhile, we design simple   \n51 shape functions to simulate sharp features. This strategy has three unique advantages. First, it   \n52 systematically captures the local geometries of most common surfaces encountered during testing,   \n53 effectively mitigating the dataset dependence risk that plagues current UDF learning methods. Second,   \n54 for each training patch, the ground-truth UDF is readily available, streamlining the training process.   \n55 Third, this approach substantially reduces the costs associated with preparing the training datasets.   \n56 We evaluate our framework on various datasets and demonstrates its ability to robustly reconstruct   \n57 high-quality surfaces, even for point clouds with noise and outliers. Notably, our method can serve as   \n58 a lightweight initialization that can be integrated with existing unsupervised methods to enhance their   \n59 performance. We summarize our main contributions as follows.   \n60 \u2022 We present a simple yet effective data-driven approach that learns UDFs directly from a   \n61 synthetic dataset consisting of point cloud patches, which is independent of the global shape.   \n62 \u2022 Our method is computationally efficient and requires training only once on our synthetic   \n63 dataset. Then it can be applied to reconstruct a wide range of surface types.   \n64 \u2022 Our framework achieves superior performance in surface reconstruction from both synthetic   \n65 point clouds and real scans, even in the presence of noise and outliers. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "66 2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "67 Surface reconstruction. Reconstructing 3D surfaces from point clouds is a classic and important   \n68 topic in computer graphics. The most widely used Poisson method [1, 22] fits surfaces by solving   \n69 PDEs. These traditional methods involve adjusting the gradient of an indicator function to align with   \n70 a solution derived from a (screened) Poisson equation. A crucial requirement of these methods is the   \n71 input of oriented normals. The Iterative Screened Poisson Reconstruction method[23] introduced   \n72 an iterative approach to refine the reconstruction process, improving the ability to generate surfaces   \n73 from point clouds without direct computation of normals. The shape of points [24] introduced a   \n74 differentiable point-to-mesh layer by employing a differentiable formulation of PSR [1] to generate   \n75 watertight, topology-agnostic manifold surfaces.   \n76 Neural surface representations. Recently, the domain of deep learning has spurred significant   \n77 advances in the implicit neural representation of 3D shapes. Some of these works trained a classifier   \n78 neural network to construct occupancy fields [9\u201312] for representing 3D geometries. Poco [12]   \n79 achieves superior reconstruction performance by introducing convolution into occupancy fields.   \n80 Ouasf iet al. [25] recently proposed a uncertainty measure method based on margin to learn occu  \n81 pancy fields from sparse point clouds. Compared to occupancy fields, SDFs [2\u20138] offer a more   \n82 precise geometric representation by differentiating between interior and exterior spaces through the   \n83 assignment of signs to distances. Some recent SOTA methods, such as DeepLS [3], using volumetric   \n84 SDFs to locally learned continuous SDFs, have achieved higher compression, accuracy, and local   \n85 shape refinement.   \n86 Unsigned distance fields learning. Although Occupancy fields and SDFs have undergone signif  \n87 icant development recently, they are hard to reconstruct surfaces with boundaries or nonmanifold   \n88 features. G-Shell[26] developed a differentiable shell-based representation for both watertight and   \n89 non-watertight surfaces. However, UDFs provide a simpler and more natural way to represent   \n90 general shapes [13\u201320]. Various methods have been proposed to reconstruct surfaces from point   \n91 clouds by learning UDFs. CAP-UDF [17] suggested directing 3D query points towards the surface   \n92 with a consistency constraint to develop UDFs that are aware of consistency. LevelSetUDF [14]   \n93 learned a smooth zero-level function within UDFs through level set projections. As a supervised   \n94 approach, GeoUDF [15] estimates UDFs by learning local geometric priors from training on many   \n95 3D shapes. DUDF [19] formulated the UDF learning as an Eikonal problem with distinct boundary   \n96 conditions. UODF [20] proposed unsigned orthogonal distance fields that every point in this field   \n97 can access to the closest surface points along three orthogonal directions. Instead of reconstructing   \n98 from point clouds, many recent works [27\u201330] learn high-quality UDFs from multi-view images for   \n99 reconstructing non-watertight surfaces. Furthermore, UiDFF [31] presents a 3D diffusion model for   \n100 UDFs to generate textured 3D shapes with boundaries. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "image", "img_path": "7nbAots3f8/tmp/6e488e63eada7b13c619bfae18f6697f9725e1fa67e40581cfee393c639ceb43.jpg", "img_caption": ["Figure 1: Pipeline. First, we train a UDF prediction network $\\mathcal{U}_{\\Theta}$ on a synthetic dataset, which contains a series of local point cloud patches that are independent of specific shapes. Given a global point cloud $\\mathbf{P}$ , we then extract a local patch $\\mathcal{P}$ assigned to each query point $\\mathbf{q}$ within a specified radius, and obtain the corresponding UDF values $\\mathcal{U}_{\\hat{\\Theta}}(\\mathcal{P},\\mathbf{q})$ . Finally, we extract the mesh corresponding to the input point cloud by incorporating the DCUDF[32] framework. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "101 3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "102 Motivation. Distinct from SDFs, there is no need for UDFs to determine the sign to distinguish   \n103 between the inside and outside of a shape. Consequently, the UDF values are solely related to the local   \n104 geometric characteristics of 3D shapes. Furthermore, within a certain radius for a query point, local   \n105 geometry can be approximated by general mathematical functions. Stemming from these insights, we   \n106 propose a novel UDF learning framework that focuses on local geometries. We employ local shape   \n107 functions to construct a series of point cloud patches as our training dataset, which includes common   \n108 smooth and sharp geometric features. Fig. 1 illustrates the pipeline of our proposed UDF learning   \n109 framework. ", "page_idx": 2}, {"type": "text", "text": "110 3.1 Local shape functions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "111 Smooth patches. From the viewpoint of differential geometry [33], the local geometry at a specific   \n112 point on a regular surface can be approximated by a quadratic surface. Specifically, consider a regular   \n113 surface $\\boldsymbol{S}:\\mathbf{r}=\\mathbf{r}(u,v)$ with a point $\\mathbf{p}$ on it. At point $\\mathbf{p}$ , it is possible to identify two principal   \n114 direction unit vectors, $\\mathbf{e}_{1}$ and $\\mathbf{e}_{2}$ , with the corresponding normal $\\mathbf{n}=\\mathbf{e}_{1}\\times\\mathbf{e}_{2}$ . A suitable parameter   \n115 system $(u,v)$ can be determined such that $\\mathbf{r}_{u}=\\mathbf{e}_{1}$ and $\\mathbf{r}_{v}=\\mathbf{e}_{2}$ , thus obtaining the corresponding   \n116 first and second fundamental forms as ", "page_idx": 2}, {"type": "equation", "text": "$$\n[\\mathbf{I}]_{\\mathbf{p}}={\\left[\\begin{array}{l l}{E}&{F}\\\\ {F}&{G}\\end{array}\\right]}={\\left[\\begin{array}{l l}{1}&{0}\\\\ {0}&{1}\\end{array}\\right]}\\,,\\quad[\\mathbf{II}]_{\\mathbf{p}}={\\left[\\begin{array}{l l}{L}&{M}\\\\ {M}&{N}\\end{array}\\right]}={\\left[\\begin{array}{l l}{\\kappa_{1}}&{0}\\\\ {0}&{\\kappa_{2}}\\end{array}\\right]}\\,,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "117 where $\\kappa_{1},\\kappa_{2}$ are principal curvatures. Without loss of generality, we assume $\\mathbf{p}$ corresponding to   \n118 $u=v=0$ and expand the Taylor form at this point as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\bf r}(u,v)={\\bf r}(0,0)+{\\bf r}_{u}(0,0)u+{\\bf r}_{v}(0,0)v+\\frac{1}{2}[{\\bf r}_{u u}(0,0)u^{2}+}\\ ~}\\\\ {{\\displaystyle~~~~~~~~~{\\bf r}_{u v}(0,0)u v+{\\bf r}_{v v}(0,0)v^{2}]+o(u^{2}+v^{2})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "119 Decomposing ${\\bf r}_{u u}(0,0),{\\bf r}_{u v}(0,0)$ , and $\\mathbf{r}_{v v}(0,0)$ along the tangential and normal directions, we can   \n120 formulate Eq.(2) according to Eq.(1) as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbf{r}(u,v)=\\mathbf{r}(0,0)+(u+o(\\sqrt{u^{2}+v^{2}}))\\mathbf{e}_{1}+(v+o(\\sqrt{u^{2}+v^{2}}))\\mathbf{e}_{2}}\\\\ {\\qquad+\\cfrac{1}{2}(\\kappa_{1}u^{2}+\\kappa_{2}v^{2}+o(u^{2}+v^{2})))\\mathbf{n}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "121 where $o(u^{2}+v^{2})\\approx0$ is negligible in a small local region. Consequently, by adopting $\\{\\mathbf{p},\\mathbf{e}_{1},\\mathbf{e}_{2},\\mathbf{n}\\}$   \n122 as the orthogonal coordinate system, we can define the form of the local approximating surface as ", "page_idx": 2}, {"type": "equation", "text": "$$\nx=u,\\quad y=v,\\quad z=\\frac{1}{2}\\big(\\kappa_{1}u^{2}+\\kappa_{2}v^{2}\\big),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "image", "img_path": "7nbAots3f8/tmp/28f2291125ecfe79f9ee97a2c2b85437aad9dbd2ceab3a7a4a03a03027e790c3.jpg", "img_caption": ["(a) Smooth patches ", "(b) Sharp patches "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: Local geometries. (a) For points on a geometry that are differentiable, the local shape at these points can be approximated by quadratic surfaces. (b) For points that are non-differentiable, we can also construct locally approximated surfaces using functions. ", "page_idx": 3}, {"type": "text", "text": "123 which exactly are quadratic surfaces $z\\,=\\,\\textstyle{\\frac{1}{2}}\\bigl(\\kappa_{1}x^{2}+\\kappa_{2}y^{2}\\bigr)$ . Furthermore, in relation to Gaussian   \n124 curvatures $\\kappa_{1}\\kappa_{2}$ , quadratic surfaces can be categorized into four types: ellipsoidal, hyperbolic,   \n125 parabolic, and planar. As shown in Fig. 2, for differentiable points on a general geometry, the local   \n126 shape features can always be described by one of these four types of quadratic surfaces.   \n127 Sharp patches. For surfaces with sharp features, they are not differentiable at some points and cannot   \n128 be approximated in the form of a quadratic surface. We categorize commonly seen sharp geometric   \n129 features into four types, including creases, cusps, corners, and $\\mathbf{V}^{\\prime}$ -saddles, as illustrated in Fig. 2(b).   \n130 We construct these four types of sharp features in a consistent form $z=f(x,y)$ like smooth patches ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{s:}z=1-h\\cdot\\displaystyle\\frac{\\lvert k x-y\\rvert}{\\sqrt{1+k^{2}}},~\\mathrm{cusps:}z=1-h\\cdot\\sqrt{x^{2}+y^{2}},}\\\\ &{\\mathrm{s:}z=1-h\\cdot\\operatorname*{max}(\\lvert x\\rvert,\\lvert y\\rvert),~\\mathrm{v-saddles:}z=1-h\\cdot\\lvert x\\rvert+\\lvert y\\rvert\\cdot(\\displaystyle\\frac{\\lvert x\\rvert}{x}\\cdot\\displaystyle\\frac{\\lvert y\\rvert}{y}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "131 where $h$ can adjust the sharpness of the shape, and $k$ can control the direction of the crease. Fig 3   \n132 illustrates various smooth and sharp patches with distinct parameters.   \n133 Synthetic training dataset. We utilize the mathematical functions introduced above to synthesize a   \n134 series of point cloud patches for training. As shown in Fig. 3, we first uniformly sample $m$ points   \n35 $\\{(x_{i},y_{i})\\}_{i=1}^{m}$ within a circle of radius $r_{0}$ centered at $(0,0)$ in the $x y$ -plane. Then, we substitute   \n136 the coordinates into Eq.(4-5) to obtain the corresponding $z$ -coordinate values, resulting in a patch   \n137 $\\mathscr{P}\\;=\\;\\{{\\bf p}_{i=1}^{m}\\}$ , where $\\bar{\\mathbf{p}}_{i}\\,=\\,\\left(x_{i},y_{i},z(x_{i},y_{i})\\right)$ . Subsequently, we randomly collect query points   \n138 $\\{{\\bf q}_{\\mathrm{i}}\\}_{i=1}^{n}$ distributed along the vertical ray intersecting the $x y$ -plane at the origin, extending up to a   \n139 distance of $r_{0}$ . For each query point $\\mathbf{q}_{i}$ , we determine its UDF value $\\mathcal{U}(\\mathbf{q}_{i})$ , which is either $|\\mathbf{q}_{i}^{(z)}|$ for   \n140 smooth patches or $1-|\\mathbf{q}_{i}^{(z)}|$ for sharp patches. Noting that for patches with excessively high curvature   \n141 or sharpness, the minimum distance of the query points may not be the distance to $(0,0,z(0,0))$ , we   \n142 will exclude these patches from our training dataset. Overall, each sample in our synthetic dataset is   \n143 specifically in the form of $\\{\\mathbf{q},\\mathcal{P},\\mathcal{U}(\\mathbf{q})\\}$ .   \n145 We perform supervised training on the synthesized dataset which is independent of specific shapes.   \n146 The network learns the features of local geometries and utilizes an attention-based module to output   \n147 the corresponding UDF values from the learned features. After training, given any 3D point clouds   \n148 and a query point in space, we extract the local point cloud patch near the query, which has the same   \n149 form as the data in the training dataset. Consequently, our network can predict the UDF value at that   \n150 query point based on this local point cloud patch. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "image", "img_path": "7nbAots3f8/tmp/45af44a669d968b8958a85d2a0a9ee0838422fc204e12b500bbece8ffb99957c.jpg", "img_caption": ["Figure 3: Synthetic dataset for training. By manipulating functional parameters, we can readily create various smooth and sharp surfaces, subsequently acquiring pairs of point cloud patches and query points via sampling. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "151 3.2.1 Network architecture ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "152 For a sample $\\{\\mathbf{q},\\mathcal{P}=\\{\\mathbf{p}_{i}\\}_{i=1}^{m},\\mathcal{U}(\\mathbf{q})\\}$ , we first obtain a latent code $\\mathbf{f}_{p}\\in\\mathbb{R}^{l_{p}}$ related to the local   \n153 point cloud patch $\\mathcal{P}$ through a Point-Net [34] ${\\mathcal{F}}_{p}$ . To derive features related to distance, we use   \n154 relative vectors from the patch points to the query point, $\\pmb{\\mathcal{V}}=\\{\\mathbf{p}_{i}-\\mathbf{q}\\}_{i=1}^{m}$ , as input to a Vectors  \n155 Net ${\\mathcal{F}}_{v}$ , which is similar to the Point-Net ${\\mathcal{F}}_{p}$ . This process results in an additional latent code   \n156 $\\mathbf{f}_{v}\\in\\mathbb{R}^{l_{v}}$ . Subsequently, we apply a cross-attention module [35] to obtain the feature codes for the   \n157 local geometry, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{f}_{G}=\\mathrm{CrossAttn}(\\mathbf{f}_{p},\\mathbf{f}_{v})\\in\\mathbb{R}^{l_{G}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "158 where we take $\\mathbf{f}_{p}$ as the Key-Value (KV) pair and $\\mathbf{f}_{v}$ as the Query (Q). In our experiments, we set   \n159 $l_{p}=l_{v}=64$ , and $l_{G}=128$ . Based on the learned geometric features, we aim to fti the UDF values   \n160 from the distance within the local point cloud. Therefore, we concatenate the distances $\\mathbf{d}\\in\\mathbb{R}^{m}$   \n161 induced from $\\mathcal{V}$ with the latent code $\\mathbf{f}_{G}$ , followed by a series of fully connected layers to output the   \n162 predicted UDF values $\\mathcal{U}_{\\Theta}(\\mathbf{q})$ . Fig. 4 illustrates the overall network architecture and data flow.   \n163 Denoising module. In our network, even if point cloud patches are subjected to a certain degree of   \n164 noise or outliers, their representations in the feature space should remain similar. However, distances   \n165 induced directly from noisy vectors $\\nu$ will inevitably contain errors, which can affect the accurate   \n166 prediction of UDF values. To mitigate this impact, we introduce a denoising module that predicts   \n167 displacements $\\Delta\\mathbf{d}$ from local point cloud patches, as shown in Fig. 4. We then add the displacements   \n168 $\\Delta\\mathbf{d}$ to the distances $\\mathbf{d}$ to improve the accuracy of the UDF estimation. ", "page_idx": 4}, {"type": "image", "img_path": "7nbAots3f8/tmp/9b687b6b8222ba9277b9044554259012103b68ec63b37a038fa04fc31e1715da.jpg", "img_caption": ["Figure 4: Network architecture of LoSF-UDF. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "169 3.2.2 Training and evaluation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "170 Data augmentation. During the training process, we scale all pairs of local patches $\\mathcal{P}$ and query   \n171 points q to conform to the bounding box constraints of $[-0.5,0.5]$ , and the corresponding GT UDF   \n172 values $\\mathcal{U}(\\mathbf{q})$ are scaled by equivalent magnitudes. Given the uncertain orientation of local patches   \n173 extracted from a specified global point cloud, we have applied data augmentation via random rotations   \n174 to the training dataset. Furthermore, to enhance generalization to open surfaces with boundaries, we   \n175 randomly truncate $20\\%$ of the smooth patches to simulate boundary cases. To address the issue of   \n176 noise handling, we introduce Gaussian noise $\\mathcal{N}(0,0.1)$ to $30\\%$ of the data in each batch during every   \n177 training epoch.   \n178 Loss functions. We employ $L_{1}$ loss $\\mathcal{L}_{\\mathrm{u}}$ to measure the discrepancy between the predicted UDF   \n179 values and the GT UDF values. Moreover, for the displacements $\\Delta\\mathbf{d}$ output by the denoising module,   \n180 we employ $L_{1}$ regularization to encourage sparsity. Consequently, we train the network driven by the   \n181 following loss function, ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\mathcal{L}_{u}+\\lambda_{d}\\mathcal{L}_{r},\\quad\\mathrm{where}~\\mathcal{L}_{u}=|\\mathcal{U}(\\mathbf{q})-\\mathcal{U}_{\\Theta}(\\mathbf{q})|,~\\mathcal{L}_{r}=|\\Delta\\mathbf{d}|,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "182 where we set $\\lambda_{d}=0.01$ in our experiments. ", "page_idx": 5}, {"type": "text", "text": "183 Evaluation. Given a 3D point cloud $\\mathbf{P}$ for reconstruction, we first normalize it to fti within a bounding   \n184 box with dimensions ranging from $[-0.5,0.5]$ . Subsequently, within the bounding box space, we   \n185 uniformly sample grid points at a specified resolution to serve as query points. Finally, we extract the   \n186 local geometry $\\mathcal{P}_{\\mathbf{p}}$ for each query point by collecting points from the point cloud that lie within a   \n187 sphere of a specified radius centered on the query point. We can obtain the predicted UDF values   \n188 by the trained network $\\mathcal{U}_{\\Theta^{*}}(\\mathbf{q},\\mathcal{P}_{\\mathbf{q}})$ , where $\\Theta^{*}$ represents the optimized network parameters. Note   \n189 that for patches $\\mathcal P_{\\mathbf{p}}$ with fewer than 5 points, we set the UDF values as a large constant. Finally, we   \n190 extract meshes from the UDFs using the DCUDF model [32]. ", "page_idx": 5}, {"type": "text", "text": "191 4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "192 4.1 Experiment setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "193 Datasets. To compare our method with other state-of-the-art UDF learning approaches, we tested it on   \n194 various datasets that include general artificial objects from the field of computer graphic. Following   \n195 previous works [30, 17, 14], we select the \"Car\" category from ShapeNet[21], which has a rich   \n196 collection of multi-layered and non-closed shapes. Furthermore, we select the real-world dataset   \n197 DeepFashion3D[36] for open surfaces, and ScanNet[37] for large outdoor scenes. To assess our   \n198 model\u2019s performance on actual noisy inputs, we conducted tests on real range scan dataset [38]   \n199 following the previous works[17, 14].   \n200 Baselines. For our validation datasets, we compared our method against the state-of-the-art UDF   \n201 learning models, which include unsupervised methods like CAP-UDF[17], LevelSetUDF[14], and   \n202 DUDF[19], as well as the supervised learning method, GeoUDF[15]. We trained GeoUDF inde  \n203 pendently on different datasets to achieve optimal performance. Table. 1 shows the qualitative   \n204 comparison between our methods and baselines. To evaluate performance, we calculate the Chamfer   \n205 Distance (CD) and F1-Score (setting thresholds of 0.005 and 0.01) metrics between the ground truth   \n206 meshes and the meshes extracted from the UDFs out by our model and each baseline model. For a fair   \n207 comparison, we test all baseline models using the DCUDF[32] method. All experimental procedures   \n208 are executed on NVIDIA RTX 4090 and A100 GPUs. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "table", "img_path": "7nbAots3f8/tmp/654cd6702ab2d6693a9f88d480b38cf5727668b860a781d25dc2a11288a2b636.jpg", "table_caption": [], "table_footnote": ["Table 1: Qualitative comparison of different UDF learning methods. \u201cNormal\u201d indicates whether the method requires point cloud normals during learning. \u201cFeature Type\u201d\u2019 refers to whether the information required during training is global or local. \u201cNoise\u201d and \u201cOutlier\u201d indicate whether the method can handle the presence of noise and outliers in point clouds. "], "page_idx": 5}, {"type": "text", "text": "209 4.2 Experimental results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "210 Synthetic data. ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "211 Fashion3D, we obtain dense point clouds b   \n212 Considering that GeoUDF [15] is a supervised method, we   \n213 retrain it on ShapeNetCars, and DeepFashion3D, which   \n214 are randomly partitioned into training $(70\\%)$ , testing   \n215 $(20\\%)$ , and validation subsets $(10\\%)$ . All models are eval  \n216 uated in the validation sets, which remain unseen by any   \n217 of the UDF learning models prior to evaluation. The first   \n218 three rows of Fig. 5 show the visual comparison of recon", "page_idx": 5}, {"type": "text", "text": "models, ShapeNetCars, and Deepy randomly samping on meshes. ", "page_idx": 5}, {"type": "image", "img_path": "7nbAots3f8/tmp/85cb0fa1403cacf9ae4bbc2e2ef1db0bc8281b1fa26eefe1a219079277e93306.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "struction results, while Tab. 2 presents the quantitative comparison results of CD and F1-score. We ", "page_idx": 5}, {"type": "text", "text": "220 test each method using their own mesh extraction technique, as shown in the inset figure, which   \n221 display obvious visual artifacts such as small holes and non-smoothness. We thus apply DCUDF [32]   \n222 , the state-of-art method, to each baseline model , extracting the surfaces as significantly higher   \n223 quality meshes. Since our method utilizes DCUDF for surface extraction, we adopt it as the default   \n224 technique to ensure consistency and fairness in comparisons with the baselines. Our method achieves   \n225 stable results in reconstructing various types of surfaces, including both open and closed surfaces,   \n226 and exhibits performance comparable to that of the SOTA methods. Noting that DUDF[19] requires   \n227 normals during training, and GeoUDF utilizes the KNN approach to determine the nearest neighbors   \n228 of the query points. Although DUDF and GeoUDF achieve better evaluations, they are less stable   \n229 when dealing with point clouds with noise and outliers.   \n230 Noise & outliers. To evaluate our model with noisy inputs, we added Gaussian noise $\\mathcal{N}(0,0.0025)$ to   \n231 the clean data across all datasets for testing. The middle three rows in Fig. 5 display the reconstructed   \n232 surface results from noisy point clouds, and Tab. 2 also presents the quantitative comparisons. It   \n233 can be observed that our method can robustly reconstruct smooth surfaces from noisy point clouds.   \n234 Additionally, we tested our method\u2019s performance with outliers by converting $10\\%$ of the clean point   \n235 cloud into outliers, as shown in the last three rows of Fig. 5. To further demonstrate the robustness   \n236 of our method, we conducted experiments on point clouds with higher percentage of outliers. Our   \n237 framework is able of reconstructing reasonable surfaces even with $50\\%$ outliers. We also tested the   \n238 task on point clouds containing both noise and outliers. Please refer to Fig. 9 in the Appendix for the   \n239 corresponding results.   \n240 Real-world scanned data. Dataset [38] provide several real-world scanned point clouds, as illustrated   \n241 in Fig. 6 (Left), we evaluate our model on the dataset to demonstrate the effectiveness. Our approach   \n242 can reconstruct smooth surfaces from scanned data containing noise and outliers. However, our   \n243 model cannot address the issue of missing parts. This limitation is due to the local geometric training   \n244 strategy, which is independent of the global shape. Additionally, we conduct tests on large scanned   \n245 scenes to evaluate our algorithm, as shown in Fig. 6 (Right). ", "page_idx": 6}, {"type": "table", "img_path": "7nbAots3f8/tmp/9d1bb9b8c01c4f271b5a2cffdcd4eacf778cd88e086d4b75c67ecd376849fec1.jpg", "table_caption": [], "table_footnote": ["Table 2: Quantitative evaluation of UDF learning methods (CD score is multiplied by 100). "], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "246 4.3 Analysis & ablation studies ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "247 Efficiency. As a supervised UDF learning method, our approach has a   \n248 significant improvement in training efficiency compared to GeoUDF [15].   \n249 As shown in the insert table, we calculate the data storage Method Storage (GB) Data-prep (min) Training (h)   \n250 space required by GeoUDF when using ShapeNet as a GeoUDF 120 0.5 36   \n251 training dataset. This includes the GT UDF values and Ours 0.59 0.02 14.5   \n252 point cloud data needed during the training process. Our   \n253 synthetic point cloud patches training dataset occupies under 1GB, which is merely $0.5\\%$ of the storage   \n254 needed for GeoUDF. Our network is very lightweight, with only 653KB of trainable parameters and a   \n255 total parameter size of just 2MB. Additionally, we highlight time-saving beneftis. The provided table   \n256 illustrates the duration required to produce a single data sample for dataset preparation (\u201cData-prep\u201d),   \n257 as well as the total time for training (\u201cTraining\u201d).   \n258 Patch radius. During the evaluation phase, the radius $r$ used to find the nearest points for each   \n259 query point determines the size of the extracted patch and the range of effective query points in the   \n260 space. As shown in Fig. 7, we analyzed the impact of different radii on the reconstruction results.   \n261 An excessively small $r$ will generate artifacts, while an overly large $r$ will lose many details. In our   \n262 experiments, we generally set $r$ to 0.018.   \n263 Denoising module. Our framework incorporates a denoising module to handle noisy point clouds.   \n264 We conducted ablation experiments to verify the significance of this module. Specifically, we set   \n265 $\\lambda_{d}=0$ in the loss function Eq. (7) to disable the denoising module, and then retrained the network.   \n266 As illustrated in Fig. 8, we present the reconstructed surfaces for the same set of noisy point clouds   \n267 with and without the denosing module, respectively. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "image", "img_path": "7nbAots3f8/tmp/c49f07f7b1ad624004798af724ae16a0839479e089f1ee85674c8da47a4fe435.jpg", "img_caption": ["(a) Input (b) CAP-UDF (c) LevelSetUDF (d) GeoUDF (e) DUDF (f) Ours (g) GT Figure 5: Visual comparisons on the synthetic dataset. First three rows: uniformly sampled points. Meddle three rows: point clouds with $0.25\\%$ added noise. Last three rows: point clouds with $10\\%$ outliers. All point clouds here have 48K points, except for the Bunny model, which has 100K points. We refer readers to the appendix for more visual results. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "7nbAots3f8/tmp/81cad8a658f7bd5cbe0ee56d1f7fbbb5c1f75f6b191736643ef093dcf7854d7d.jpg", "img_caption": ["Figure 6: Reconstructed surfaces from real-world scanned point clouds. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "7nbAots3f8/tmp/be6351e97d5428c75b6a517efd1400740df014fbd57cbf14fc3e3b91edc14260.jpg", "img_caption": ["Figure 7: Comparison of different radii for extracting patches from the point cloud on reconstruction results. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "image", "img_path": "7nbAots3f8/tmp/82d14afe85b5d8e82514147fb03e313d12be0097258600d22d58b42e7bbf98c5.jpg", "img_caption": ["Figure 8: Ablation on denoising module: Reconstructed surfaces from the same point clouds with noise/outliers corresponding to framework with and without the denoising module, respectively. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "268 5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "269 In this paper, we introduce a novel and efficient neural framework for surface reconstruction from 3D   \n270 point clouds by learning UDFs from local shape functions. Our key insight is that 3D shapes exhibit   \n271 simple patterns within localized regions, which can be exploited to create a training dataset of point   \n272 cloud patches represented by mathematical functions. As a result, our method enables efficient and   \n273 robust surfaces reconstructions without the need for shape-specific training. Extensive experiments   \n274 on various datasets have demonstrated the efficacy of our method. Moreover, our framework achieves   \n275 superior performance on point clouds with noise and outliers.   \n276 Limitations & future work. Owing to its dependence solely on local geometric features, our   \n277 approach fails to address tasks involving incomplete point cloud reconstructions. However, as a   \n278 lightweight framework, our model can readily be integrated into other unsupervised methods to   \n279 combine the global features with our learned local priors. Furthermore, in our future work, we   \n280 intend to design a method that dynamically adjusts the radius based on local feature sizes [39] of 3D   \n281 shapes when extracting local point cloud patches for queries, aiming to improve the accuracy of the   \n282 reconstruction. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "283 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "284 [1] M Kazhdan. Poisson surface reconstruction. In Eurographics Symposium on Geometry Process  \n285 ing, 2006.   \n286 [2] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove.   \n287 Deepsdf: Learning continuous signed distance functions for shape representation. In The IEEE   \n288 Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.   \n289 [3] Rohan Chabra, Jan Eric Lenssen, Eddy Ilg, Tanner Schmidt, Julian Straub, Steven Lovegrove,   \n290 and Richard Newcombe. Deep local shapes: Learning local sdf priors for detailed 3d recon  \n291 struction, 2020.   \n292 [4] Ma Baorui, Han Zhizhong, Liu Yu-Shen, and Zwicker Matthias. Neural-pull: Learning signed   \n293 distance functions from point clouds by learning to pull space onto surfaces. In International   \n294 Conference on Machine Learning (ICML), 2021.   \n295 [5] Peng-Shuai Wang, Yang Liu, and Xin Tong. Dual octree graph networks for learning adaptive   \n296 volumetric shape representations. ACM Transactions on Graphics, 41(4):1\u201315, July 2022.   \n297 [6] Hao Pan Pengshuai Wang Xin Tong Yang Liu Shi-Lin Liu, Hao-Xiang Guo. Deep implicit   \n298 moving least-squares functions for 3d reconstruction. In IEEE/CVF Conference on Computer   \n299 Vision and Pattern Recognition, 2021.   \n300 [7] Zixiong Wang, Pengfei Wang, Pengshuai Wang, Qiujie Dong, Junjie Gao, Shuangmin Chen,   \n301 Shiqing Xin, Changhe Tu, and Wenping Wang. Neural-imls: Self-supervised implicit moving   \n302 least-squares network for surface reconstruction. IEEE Transactions on Visualization and   \n303 Computer Graphics, pages 1\u201316, 2023.   \n304 [8] Ma Baorui, Liu Yu-Shen, and Han Zhizhong. Reconstructing surfaces for sparse point clouds   \n305 with on-surface priors. In Proceedings of the IEEE/CVF Conference on Computer Vision and   \n306 Pattern Recognition (CVPR), 2022.   \n307 [9] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger.   \n308 Occupancy networks: Learning 3d reconstruction in function space. In Proceedings IEEE Conf.   \n309 on Computer Vision and Pattern Recognition (CVPR), 2019.   \n310 [10] Julian Chibane, Thiemo Alldieck, and Gerard Pons-Moll. Implicit functions in feature space for   \n311 3d shape reconstruction and completion. In IEEE Conference on Computer Vision and Pattern   \n312 Recognition (CVPR). IEEE, jun 2020.   \n313 [11] Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, and Andreas Geiger.   \n314 Convolutional occupancy networks. In European Conference on Computer Vision (ECCV),   \n315 2020.   \n316 [12] Alexandre Boulch and Renaud Marlet. Poco: Point convolution for surface reconstruction,   \n317 2022.   \n318 [13] Julian Chibane, Aymen Mir, and Gerard Pons-Moll. Neural unsigned distance fields for implicit   \n319 function learning. In Advances in Neural Information Processing Systems (NeurIPS), December   \n320 2020.   \n321 [14] Junsheng Zhou, Baorui Ma, Shujuan Li, Yu-Shen Liu, and Zhizhong Han. Learning a more   \n322 continuous zero level set in unsigned distance fields through level set projection. In Proceedings   \n323 of the IEEE/CVF international conference on computer vision, 2023.   \n324 [15] Siyu Ren, Junhui Hou, Xiaodong Chen, Ying He, and Wenping Wang. Geoudf: Surface   \n325 reconstruction from 3d point clouds via geometry-guided distance representation. In Proceedings   \n326 of the IEEE/CVF International Conference on Computer Vision, pages 14214\u201314224, 2023.   \n327 [16] Jianglong Ye, Yuntao Chen, Naiyan Wang, and Xiaolong Wang. Gifs: Neural implicit function   \n328 for general shape representation. In Proceedings of the IEEE/CVF Conference on Computer   \n329 Vision and Pattern Recognition, 2022.   \n330 [17] Junsheng Zhou, Baorui Ma, Yu-Shen Liu, Yi Fang, and Zhizhong Han. Learning consistency  \n331 aware unsigned distance functions progressively from raw point clouds. In Advances in Neural   \n332 Information Processing Systems (NeurIPS), 2022.   \n333 [18] Qing Li, Huifang Feng, Kanle Shi, Yi Fang, Yu-Shen Liu, and Zhizhong Han. Neural gradient   \n334 learning and optimization for oriented point normal estimation. In SIGGRAPH Asia 2023   \n335 Conference Papers, 2023.   \n336 [19] Miguel Fainstein, Viviana Siless, and Emmanuel Iarussi. Dudf: Differentiable unsigned distance   \n337 fields with hyperbolic scaling, 2024.   \n338 [20] Yujie Lu, Long Wan, Nayu Ding, Yulong Wang, Shuhan Shen, Shen Cai, and Lin Gao. Unsigned   \n339 orthogonal distance fields: An accurate neural implicit representation for diverse 3d shapes. In   \n340 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.   \n341 [21] Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo   \n342 Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, and Fisher Yu.   \n343 ShapeNet: An Information-Rich 3D Model Repository. Technical Report arXiv:1512.03012   \n344 [cs.GR], Stanford University \u2014 Princeton University \u2014 Toyota Technological Institute at   \n345 Chicago, 2015.   \n346 [22] Michael Kazhdan and Hugues Hoppe. Screened poisson surface reconstruction. Acm Transac  \n347 tions on Graphics, 32(3):1\u201313, 2013.   \n348 [23] Fei Hou, Chiyu Wang, Wencheng Wang, Hong Qin, Chen Qian, and Ying He. Iterative poisson   \n349 surface reconstruction (ipsr) for unoriented points. ACM Transactions on Graphics, 41(4):1\u201313,   \n350 July 2022.   \n351 [24] Songyou Peng, Chiyu \"Max\" Jiang, Yiyi Liao, Michael Niemeyer, Marc Pollefeys, and Andreas   \n352 Geiger. Shape as points: A differentiable poisson solver. In Advances in Neural Information   \n353 Processing Systems (NeurIPS), 2021.   \n354 [25] Amine Ouasf iand Adnane Boukhayma. Unsupervised occupancy learning from sparse point   \n355 cloud, 2024.   \n356 [26] Zhen Liu, Yao Feng, Yuliang Xiu, Weiyang Liu, Liam Paull, Michael J. Black, and Bernhard   \n357 Sch\u00f6lkopf. Ghost on the shell: An expressive representation of general 3d shapes. 2024.   \n358 [27] Junkai Deng, Fei Hou, Xuhui Chen, Wencheng Wang, and Ying He. 2s-udf: A novel two-stage   \n359 udf learning method for robust non-watertight model reconstruction from multi-view images,   \n360 2024.   \n361 [28] Xiaoxu Meng, Weikai Chen, and Bo Yang. Neat: Learning neural implicit surfaces with   \n362 arbitrary topologies from multi-view images. Proceedings of the IEEE/CVF Conference on   \n363 Computer Vision and Pattern Recognition, June 2023.   \n364 [29] Xiaoxiao Long, Cheng Lin, Lingjie Liu, Yuan Liu, Peng Wang, Christian Theobalt, Taku   \n365 Komura, and Wenping Wang. Neuraludf: Learning unsigned distance fields for multi-view   \n366 reconstruction of surfaces with arbitrary topologies. In Proceedings of the IEEE/CVF Conference   \n367 on Computer Vision and Pattern Recognition, pages 20834\u201320843, 2023.   \n368 [30] Yu-Tao Liu, Li Wang, Jie Yang, Weikai Chen, Xiaoxu Meng, Bo Yang, and Lin Gao. Neudf:   \n369 Leaning neural unsigned distance fields with volume rendering. In Computer Vision and Pattern   \n370 Recognition (CVPR), 2023.   \n371 [31] Junsheng Zhou, Weiqi Zhang, Baorui Ma, Kanle Shi, Yu-Shen Liu, and Zhizhong Han. Udiff:   \n372 Generating conditional unsigned distance fields with optimal wavelet diffusion. In Proceedings   \n373 of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024.   \n374 [32] Fei Hou, Xuhui Chen, Wencheng Wang, Hong Qin, and Ying He. Robust zero level-set   \n375 extraction from unsigned distance fields based on double covering. ACM Trans. Graph., 42(6),   \n376 dec 2023.   \n377 [33] Manfredo P Do Carmo. Differential geometry of curves and surfaces: revised and updated   \n378 second edition. Courier Dover Publications, 2016.   \n379 [34] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical   \n380 feature learning on point sets in a metric space. Advances in neural information processing   \n381 systems, 30, 2017.   \n382 [35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,   \n383 Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2023.   \n384 [36] Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou Tang. Deepfashion: Powering robust   \n385 clothes recognition and retrieval with rich annotations. In Proceedings of IEEE Conference on   \n386 Computer Vision and Pattern Recognition (CVPR), June 2016.   \n387 [37] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias   \n388 Nie\u00dfner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proc. Computer   \n389 Vision and Pattern Recognition (CVPR), IEEE, 2017.   \n390 [38] Matthew Berger, Joshua A. Levine, Luis Gustavo Nonato, Gabriel Taubin, and Claudio T. Silva.   \n391 A benchmark for surface reconstruction. ACM Trans. Graph., 32(2), apr 2013.   \n392 [39] Yulan Guo, Mohammed Bennamoun, Ferdous Sohel, Min Lu, Jianwei Wan, and Ngai Ming   \n393 Kwok. A comprehensive performance evaluation of 3d local feature descriptors. International   \n394 Journal of Computer Vision, 116:66\u201389, 2016. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "395 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "397 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n398 paper\u2019s contributions and scope?   \n399 Answer: [Yes]   \n400 Justification: Our abstract and introduction accurately describe our technical contributions   \n401 to Unsigned Distance Fields learning.   \n402 Guidelines:   \n403 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n404 made in the paper.   \n405 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n406 contributions made in the paper and important assumptions and limitations. A No or   \n407 NA answer to this question will not be perceived well by the reviewers.   \n408 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n409 much the results can be expected to generalize to other settings.   \n410 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n411 are not attained by the paper. ", "page_idx": 12}, {"type": "text", "text": "412 2. Limitations ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 12}, {"type": "text", "text": "Justification: We discuss the limitations in the conclusion section (Sec.5). ", "page_idx": 12}, {"type": "text", "text": "16 Guidelines:   \n17 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n18 the paper has limitations, but those are not discussed in the paper.   \n19 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n20 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n21 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n22 model well-specification, asymptotic approximations only holding locally). The authors   \n23 should reflect on how these assumptions might be violated in practice and what the   \n24 implications would be.   \n25 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n26 only tested on a few datasets or with a few runs. In general, empirical results often   \n27 depend on implicit assumptions, which should be articulated.   \n28 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n29 For example, a facial recognition algorithm may perform poorly when image resolution   \n30 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n31 used reliably to provide closed captions for online lectures because it fails to handle   \n32 technical jargon.   \n33 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n34 and how they scale with dataset size.   \n35 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n36 address problems of privacy and fairness.   \n37 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n38 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n39 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n40 judgment and recognize that individual actions in favor of transparency play an impor  \n41 tant role in developing norms that preserve the integrity of the community. Reviewers   \n42 will be specifically instructed to not penalize honesty concerning limitations.   \n3. Theory Assumptions and Proofs ", "page_idx": 12}, {"type": "text", "text": "443 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "444 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n445 a complete (and correct) proof? ", "page_idx": 12}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 12}, {"type": "text", "text": "Justification: We provide the differential geometry theory employed by our method in the main text (Sec.3). ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 13}, {"type": "text", "text": "460 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "61 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n62 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n63 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 13}, {"type": "text", "text": "Justification: We provide the most detailed algorithmic details possible in the main text and appendix. ", "page_idx": 13}, {"type": "text", "text": "Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 13}, {"type": "text", "text": "499 5. Open access to data and code ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "500 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n501 tions to faithfully reproduce the main experimental results, as described in supplemental   \n502 material?   \n503 Answer: [No]   \n504 Justification: We will definitely make our code publicly available one day, but not at this   \n505 moment.   \n506 Guidelines:   \n507 \u2022 The answer NA means that paper does not include experiments requiring code.   \n508 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n509 public/guides/CodeSubmissionPolicy) for more details.   \n510 \u2022 While we encourage the release of code and data, we understand that this might not be   \n511 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n512 including code, unless this is central to the contribution (e.g., for a new open-source   \n513 benchmark).   \n514 \u2022 The instructions should contain the exact command and environment needed to run to   \n515 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n516 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n517 \u2022 The authors should provide instructions on data access and preparation, including how   \n518 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n519 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n520 proposed method and baselines. If only a subset of experiments are reproducible, they   \n521 should state which ones are omitted from the script and why.   \n522 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n523 versions (if applicable).   \n524 \u2022 Providing as much information as possible in supplemental material (appended to the   \n525 paper) is recommended, but including URLs to data and code is permitted.   \n526 6. Experimental Setting/Details   \n527 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n528 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n529 results?   \n530 Answer: [Yes]   \n531 Justification: We introduce all the training and test details in the main text and appendix.   \n532 Guidelines:   \n533 \u2022 The answer NA means that the paper does not include experiments.   \n534 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n535 that is necessary to appreciate the results and make sense of them.   \n536 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n537 material.   \n538 7. Experiment Statistical Significance   \n539 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n540 information about the statistical significance of the experiments?   \n541 Answer: [Yes]   \n542 Justification: We provide various evaluation metrics about our method.   \n543 Guidelines:   \n544 \u2022 The answer NA means that the paper does not include experiments.   \n545 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n546 dence intervals, or statistical significance tests, at least for the experiments that support   \n547 the main claims of the paper.   \n548 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n549 example, train/test split, initialization, random drawing of some parameter, or overall   \n550 run with given experimental conditions).   \n551 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n552 call to a library function, bootstrap, etc.)   \n553 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n554 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n555 of the mean.   \n556 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n557 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n558 of Normality of errors is not verified.   \n559 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n560 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n561 error rates).   \n562 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n563 they were calculated and reference the corresponding figures or tables in the text.   \n564 8. Experiments Compute Resources   \n565 Question: For each experiment, does the paper provide sufficient information on the com  \n566 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n567 the experiments?   \n568 Answer: [Yes]   \n569 Justification: We provide the related information in the experimental section.   \n570 Guidelines:   \n571 \u2022 The answer NA means that the paper does not include experiments.   \n572 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n573 or cloud provider, including relevant memory and storage.   \n574 \u2022 The paper should provide the amount of compute required for each of the individual   \n575 experimental runs as well as estimate the total compute.   \n576 \u2022 The paper should disclose whether the full research project required more compute   \n577 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n578 didn\u2019t make it into the paper).   \n579 9. Code Of Ethics   \n580 Question: Does the research conducted in the paper conform, in every respect, with the   \n581 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n582 Answer: [Yes]   \n583 Justification: We strictly adhere to the NeurIPS Code of Ethics.   \n584 Guidelines:   \n585 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n586 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n587 deviation from the Code of Ethics.   \n588 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n589 eration due to laws or regulations in their jurisdiction).   \n590 10. Broader Impacts   \n591 Question: Does the paper discuss both potential positive societal impacts and negative   \n592 societal impacts of the work performed?   \n593 Answer: [Yes]   \n594 Justification: Our method may be applied to 3D reconstruction in daily life, demonstrating   \n595 significant social value.   \n596 Guidelines:   \n597 \u2022 The answer NA means that there is no societal impact of the work performed.   \n598 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n599 impact or why the paper does not address societal impact.   \n600 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n601 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n602 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n603 groups), privacy considerations, and security considerations.   \n604 \u2022 The conference expects that many papers will be foundational research and not tied   \n605 to particular applications, let alone deployments. However, if there is a direct path to   \n606 any negative applications, the authors should point it out. For example, it is legitimate   \n607 to point out that an improvement in the quality of generative models could be used to   \n608 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n609 that a generic algorithm for optimizing neural networks could enable people to train   \n610 models that generate Deepfakes faster.   \n611 \u2022 The authors should consider possible harms that could arise when the technology is   \n612 being used as intended and functioning correctly, harms that could arise when the   \n613 technology is being used as intended but gives incorrect results, and harms following   \n614 from (intentional or unintentional) misuse of the technology.   \n615 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n616 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n617 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n618 feedback over time, improving the efficiency and accessibility of ML).   \n619 11. Safeguards   \n620 Question: Does the paper describe safeguards that have been put in place for responsible   \n621 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n622 image generators, or scraped datasets)?   \n623 Answer: [NA]   \n624 Justification: Our paper poses no such risks.   \n625 Guidelines:   \n626 \u2022 The answer NA means that the paper poses no such risks.   \n627 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n628 necessary safeguards to allow for controlled use of the model, for example by requiring   \n629 that users adhere to usage guidelines or restrictions to access the model or implementing   \n630 safety filters.   \n631 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n632 should describe how they avoided releasing unsafe images.   \n633 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n634 not require this, but we encourage authors to take this into account and make a best   \n635 faith effort.   \n636 12. Licenses for existing assets   \n637 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n638 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n639 properly respected?   \n640 Answer: [Yes]   \n641 Justification: The original owners of all code, data, and models in our paper are properly   \n642 credited.   \n643 Guidelines:   \n644 \u2022 The answer NA means that the paper does not use existing assets.   \n645 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n646 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n647 URL.   \n648 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n649 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n650 service of that source should be provided.   \n651 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n652 package should be provided. For popular datasets, paperswithcode.com/datasets   \n653 has curated licenses for some datasets. Their licensing guide can help determine the   \n654 license of a dataset.   \n655 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n656 the derived asset (if it has changed) should be provided.   \n657 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n658 the asset\u2019s creators.   \n659 13. New Assets   \n660 Question: Are new assets introduced in the paper well documented and is the documentation   \n661 provided alongside the assets?   \n662 Answer: [NA]   \n663 Justification: There is no new assets attached to our paper. We will make our code and data   \n664 public once paper is accepted.   \n665 Guidelines:   \n666 \u2022 The answer NA means that the paper does not release new assets.   \n667 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n668 submissions via structured templates. This includes details about training, license,   \n669 limitations, etc.   \n670 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n671 asset is used.   \n672 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n673 create an anonymized URL or include an anonymized zip file.   \n674 14. Crowdsourcing and Research with Human Subjects   \n675 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n676 include the full text of instructions given to participants and screenshots, if applicable, as   \n677 well as details about compensation (if any)?   \n678 Answer: [NA]   \n679 Justification: Our paper does not involve crowdsourcing nor research with human subjects.   \n680 Guidelines:   \n681 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n682 human subjects.   \n683 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n684 tion of the paper involves human subjects, then as much detail as possible should be   \n685 included in the main paper.   \n686 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n687 or other labor should be paid at least the minimum wage in the country of the data   \n688 collector.   \n689 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n690 Subjects   \n691 Question: Does the paper describe potential risks incurred by study participants, whether   \n692 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n693 approvals (or an equivalent approval/review based on the requirements of your country or   \n694 institution) were obtained?   \n695 Answer: [NA]   \n696 Justification: Our paper does not involve crowdsourcing nor research with human subjects.   \n697 Guidelines:   \n698 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n699 human subjects.   \n700 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n701 may be required for any human subjects research. If you obtained IRB approval, you   \n702 should clearly state this in the paper. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 18}, {"type": "text", "text": "708 A Appendix ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "709 A.1 Network details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "710 The two PointNets used in our network to extract features from point cloud patches $\\mathcal{P}$ and vectors $\\nu$   \n711 consist of four ResNet blocks. In addition, the two fully connected layer modules in our framework   \n712 consist of three layers each. To ensure non-negativity of the UDF values output by the network, we   \n713 employ the softplus activation function. ", "page_idx": 19}, {"type": "text", "text": "714 A.2 Robustness to outliers ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "715 Our method can reconstruct relatively accurate geometry from point clouds with $10\\%$ added outliers   \n716 and reasonably smooth surfaces from point clouds with even higher outlier ratios. Furthermore, our   \n717 approach can reconstruct high-quality geometry from point clouds containing both noise and outliers,   \n718 as shown in Fig. 9. ", "page_idx": 19}, {"type": "image", "img_path": "7nbAots3f8/tmp/cf2a6abc51df95d5c24d899c022b7e742742ad068105f69b226f15912a7e217c.jpg", "img_caption": ["Figure 9: Our model demonstrates robustness to more outliers. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "719 A.3 More results ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "720 As shown in Fig. 10 and Fig. 11, we provide more visual comparisons on the DeepFashion3D and   \n721 ShapeNetCars dataset, using point clouds containing noise and outliers. ", "page_idx": 19}, {"type": "image", "img_path": "7nbAots3f8/tmp/fad594882a401b2bcc7ea9096f95beefaf7a99b7386012701b809ff5894ff094.jpg", "img_caption": ["Figure 10: More visual results on the DeepFashion3D dataset. Top three rows: Reconstruction results under noise-free conditions. Bottom three rows: Reconstruction results under noise condition. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "7nbAots3f8/tmp/a3037554a36fada303e1403514f3724e667ebf1d1fee440bb33dfd6b8a61411e.jpg", "img_caption": ["Figure 11: More visual results on the synthetic datasets with outliers. "], "img_footnote": [], "page_idx": 21}]