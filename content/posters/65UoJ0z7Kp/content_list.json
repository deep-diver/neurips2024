[{"type": "text", "text": "SeTAR: Out-of-Distribution Detection with Selective Low-Rank Approximation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yixia $\\mathbf{Li}^{1*}$ , Boya Xiong2\u2217, Guanhua Chen1\u2020, Yun Chen3\u2020 ", "page_idx": 0}, {"type": "text", "text": "1Southern University of Science and Technology 2Shanghai University of Finance and Economics 3MoE Key Laboratory of Interdisciplinary Research of Computation and Economics, Shanghai University of Finance and Economics liyixia@me.com, xiongboya@163.sufe.edu.cn chengh3@sustech.edu.cn, yunchen@sufe.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Out-of-distribution (OOD) detection is crucial for the safe deployment of neural networks. Existing CLIP-based approaches perform OOD detection by devising novel scoring functions or sophisticated fine-tuning methods. In this work, we propose SeTAR, a novel, training-free OOD detection method that leverages selective low-rank approximation of weight matrices in vision-language and vision-only models. SeTAR enhances OOD detection via post-hoc modification of the model\u2019s weight matrices using a simple greedy search algorithm. Based on SeTAR, we further propose $\\mathrm{SeTAR+FT},$ a fine-tuning extension optimizing model performance for OOD detection tasks. Extensive evaluations on ImageNet1K and Pascal-VOC benchmarks show SeTAR\u2019s superior performance, reducing the relatively false positive rate by up to $18.95\\%$ and $36.80\\%$ compared to zero-shot and fine-tuning baselines. Ablation studies further validate SeTAR\u2019s effectiveness, robustness, and generalizability across different model backbones. Our work offers a scalable, efficient solution for OOD detection, setting a new state-of-the-art in this area. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The task of out-of-distribution (OOD) detection (Hendrycks & Gimpel, 2017; Ming et al., 2022) aims to identify whether input data comes from an unknown distribution. It has garnered significant attention in the machine learning community recently (Hendrycks et al., 2020; Xu et al., 2021; Miyai et al., 2023a). While machine learning models are trained with supervised in-distribution (ID) data, they often struggle to generalize to OOD data encountered in real-world applications (Emmott et al., 2016) like autonomous vehicles and healthcare. These OOD samples pose challenges as they are not represented in the training data. Consequently, OOD detection plays a crucial role in developing reliable and trustworthy machine-learning models suitable for real-world deployment (Bai et al., 2023). It allows models to fliter out and reject these awkward inputs effectively, and enables the use of curated and labeled OOD samples to further train for a more robust model in the wild. ", "page_idx": 0}, {"type": "text", "text": "Previous research has primarily focused on detecting OOD instances in either visual (DeVries & Taylor, 2018; Liang et al., 2018; Hendrycks et al., 2022) or textual data (Hu & Khan, 2021; Zheng et al., 2020; Zhou et al., 2021). Recently, significant progress has been made in multimodal tasks like multimodal retrieval (Li et al., 2023; Caesar et al., 2018) and image classification (Yu et al., 2022), thanks to vision-and-language pretrained (VLP) models like CLIP (Radford et al., 2021). More recent studies have explored OOD detection with CLIP, grouped into zero-shot methods (Fort et al., 2021; Ming et al., 2022; Miyai et al., 2023b) and finetuning-based methods (Ming & Li, 2023; Tao et al., 2023; Miyai et al., 2023a). However, the zero-shot methods suffer from suboptimal performance due to potential domain gaps with ID downstream data. On the other hand, finetuning-based methods carry the risk of deconstructing the intricate representations learned by CLIP which requires a meticulously designed training strategy. Sparsification-based approaches (Sun et al., 2021; Djurisic et al., 2023) have demonstrated potential in OOD detection within CNNs, leveraging the assumption that ID and OOD samples produce distinct activation patterns. Nevertheless, their effectiveness diminishes in large-scale pre-trained models such as CLIP, where activation differences become more subtle, thereby limiting their applicability primarily to models fine-tuned on downstream ID-domain datasets. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this work, we propose SeTAR, a training-free and effective OOD detection method by selective low-rank approximations. Low-rank approximation is to approximate a given matrix by finding a lower-rank matrix that closely resembles the original matrix. Previous research has demonstrated that using low-rank approximation matrices can achieve comparable performance to full parameters in various scenarios, as observed in tasks such as large language model (LLM) fine-tuning (Hu et al., 2022) and model pruning (Hajimolahoseini et al., 2021). These approaches typically preserve the same rank across different low-rank approximation matrices. In our work, we demonstrate that it is possible to significantly enhance the performance of OOD detection by selectively manipulating the weight matrices in the CLIP model, including the choice of weight matrices and the ratio of singular vectors to be reduced. Specifically, we propose a simple top-to-bottom and image-to-text greedy search algorithm to manipulate $\\mathrm{W_{up}}$ in the CLIP model. Our method applies to various model backbones and does not require any additional training or new parameters. Building upon SeTAR , we further demonstrate its effectiveness for fine-tuning initialization, referred to as $\\mathrm{SeTAR+FT}$ ", "page_idx": 1}, {"type": "text", "text": "We conduct extensive evaluations and achieve state-of-the-art performance on common OOD detection benchmarks for CLIP, including the ImageNet1K and Pascal-VOC benchmarks. Compared to vanilla MCM and GL-MCM, SeTAR with the CLIP backbone reduces relatively FPR95 by $9.5\\%$ and $12.0\\%$ on average across two benchmarks, respectively. When further integrate fine-tuning into SeTAR, SeTAR $\\mathbf{+FT}$ outperforms the state-of-the-art fine-tuning baselines LoCoOp (Miyai et al., 2023a) and LoRA (Hu et al., 2022). Moreover, we perform a comprehensive ablation study and analysis to verify and understand SeTAR. In summary, our key results and contributions: ", "page_idx": 1}, {"type": "text", "text": "1. We propose SeTAR, a simple yet effective OOD detection method based on selective lowrank approximation. It is training-free as it only performs post-hoc modification to weight matrices. SeTAR applies to a variety of scoring functions and model backbones. It can be readily integrated with existing zero-shot OOD detection methods.   \n2. We further extend SeTAR to $\\mathrm{SeTAR+FT},$ , which demonstrates the effectiveness of SeTAR in improving the performance of finetuning-based OOD detection methods and achieving new state-of-the-art results.   \n3. We extensively evaluate SeTAR and $\\mathrm{SeTAR+FT}$ across a diverse set of OOD detection tasks. It consistently outperforms baseline methods and establishes new state-of-the-art results on CLIP-based OOD detection benchmarks. On ImageNet1K, SeTAR achieves an AUROC of $91.32\\%$ with CLIP backbone and GL-MCM score. The score further increases to $92.31\\%$ when combined with the finetuning-based detection method.   \n4. We perform comprehensive ablation studies and empirical analyses to verify and understand SeTAR. We hope that this work will shed light on future explorations on in-depth understanding of the SeTAR method.3 ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "CLIP Architecture The CLIP model (Radford et al., 2021) comprises an image encoder $E^{v}(\\cdot)$ and a text encoder $E^{t}(\\cdot)$ , aligned via contrastive learning on web-scale image-text pairs. We focus on CLIP-ViT, where the image encoder is a Vision Transformer (ViT). Each ViT layer includes a multihead self-attention sublayer and a feed-forward sublayer. In the self-attention module, the hidden state is projected into different spaces using learnable parameters $\\mathrm{W_{q},W_{k},W_{v}}$ . The outputs are concatenated and projected back with another linear matrix $\\mathrm{W_{o}}$ . The feed-forward module projects the hidden state into a wider space using $\\mathrm{W_{up}}$ and then back with $\\mathrm{W_{down}}$ after a non-linear activation (Figure 1). Given the similarity between the image and text encoder layers, we adopt consistent notations for the linear matrices in both. Each encoder also includes a linear projector $\\mathrm{W_{p}}$ to map their representations into a shared space for contrastive learning. ", "page_idx": 1}, {"type": "image", "img_path": "65UoJ0z7Kp/tmp/9a85f5085b261b9b0aa8451571be0d395aa34373b1ea9a917086d9e6c51eb877.jpg", "img_caption": ["Figure 1: The overview of SeTAR. (a) The structure of the CLIP image and text encoder. (b) The details of the feed-forward sublayer. (c) For each encoder layer, we replace the $\\mathrm{W_{up}}$ weight matrix with its low-rank approximation $\\widehat{\\mathrm{W}}_{\\mathfrak{u p}}$ . (d) The illustration of $\\Sigma$ before and after low-rank approximation. More details are in Secti on 3.1. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Zero-shot OOD Detection with CLIP Zero-shot OOD detection aims to separate ID and OOD data without an ID training dataset. Given the CLIP, the ID classes are defined by the classification task of interest, which differs from the classes used in CLIP pretraining. Accordingly, OOD is defined as classes not belonging to any ID class, making the OOD detector a binary classifier. MCM (Ming atlh.,o d2s0. 2F2o)r amnadll yG, lLe-t $\\mathbf{x}$ LbMe  t(heM tieysati  iemt aagl.e,  a2n0d2 $\\boldsymbol{\\mathcal{T}}_{\\mathrm{in}}=\\{{\\bf y}_{c}\\}_{c=1}^{K}$ e rboe- sthheo ts eCt LoIf Pt-ebxat sperdo mOpOtsD c odnettaeicntiionng $M$ ID class labels (e.g., \"a photo of a [CLASS]\"). The image is segmented into $l$ image patches $\\mathbf{x}=(x_{1},...,x_{l})$ . Following CLIP, we add [cls] before the image patches and use the output of [cls] from the visual projector $\\mathrm{W_{p}}$ as the global image feature $(h^{v}\\in\\mathbb{R}^{d})$ ). The outputs of other patches are projected by the visual projector as the local image features $(\\mathbf{p}^{v}=(p_{1}^{v},...,p_{l}^{v})\\in\\mathbb{R}^{l\\times d})$ . For the text prompt $\\mathbf{y}_{c}\\in\\mathcal{T}_{\\mathrm{in}}$ , we add an additional [eos] after the text tokens and use the output feature of [eos] from the textual projector $\\mathrm{W_{p}}$ as the concept feature of ID class $c\\,(h_{c}^{t}\\in\\ensuremath{\\mathbb{R}}^{d})$ . ", "page_idx": 2}, {"type": "text", "text": "The label-wise image-concept matching (IWIC) score measures how well a test image $\\mathbf{x}$ aligns with a concept $\\mathbf{y}_{c}$ , using either global or local features. The global IWIC score $s_{c}^{G}$ (\u00b7) is the cosine similarity between the global image feature $h^{v}$ and the concept feature $h_{c}^{t}\\colon s_{c}^{G}\\left(\\bar{\\mathbf{x}}\\right)=\\cos_{-}\\!\\sin(h^{v},h_{c}^{t})$ . The local IWIC score $s_{c}^{L}(\\cdot)$ is the max-pooled cosine similarity between image patch features $p_{i}^{v}$ and the concept feature $h_{c}^{t}\\colon s_{c}^{L}(\\mathbf{x})=\\operatorname*{max}_{i}\\cos_{-}\\!\\sin(p_{i}^{v},h_{c}^{t})$ . The MCM and GL-MCM scores are defined as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{S_{\\mathrm{MCM}}\\left(\\mathbf{x}\\right)=\\underset{c}{\\operatorname*{max}}\\,\\frac{e^{s_{c}^{G}\\left(\\mathbf{x}\\right)/\\tau}}{\\sum_{c=1}^{K}e^{s_{c}^{G}\\left(\\mathbf{x}\\right)/\\tau}},\\ \\ \\ \\ \\ }\\\\ &{}&{S_{\\mathrm{GL-MCM}}\\left(\\mathbf{x}\\right)=S_{\\mathrm{MCM}}\\left(\\mathbf{x}\\right)+\\underset{c}{\\operatorname*{max}}\\,\\frac{e^{s_{c}^{L}\\left(\\mathbf{x}\\right)/\\tau^{\\prime}}}{\\sum_{c=1}^{K}e^{s_{c}^{L}\\left(\\mathbf{x}\\right)/\\tau^{\\prime}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\tau$ and $\\tau^{\\prime}$ are the temperature hyperparameters. MCM only uses global image features, while GL-MCM additionally considers local image features. For ID data, both MCM and GL-MCM scores will be matched to one of the concept features with a high score; and vice versa. As a result, our OOD detection function can be formulated as: ", "page_idx": 2}, {"type": "equation", "text": "$$\nG\\left(\\mathbf{x}\\right)=\\left\\{\\begin{array}{l l}{1}&{S\\left(\\mathbf{x}\\right)\\ge\\lambda}\\\\ {0}&{S\\left(\\mathbf{x}\\right)<\\lambda}\\end{array},\\right.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $S\\left(\\mathbf{x}\\right)$ is either the MCM or GL-MCM score, $\\lambda$ is the threshold value. By convention, $G(\\mathbf{x})=1$ represents the ID class and $G(\\mathbf{x})=0$ indicates the OOD class. The $\\lambda$ is chosen so that a high fraction of ID data (e.g., $95\\%$ ) is above the threshold. We follow previous work (Miyai et al., 2023a) to use either MCM or GL-MCM score for OOD detection in this work. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We introduce SeTAR, a training-free and effective technique for improving OOD detection performance (see Figure 1). Our key idea is to perform post-hoc modification to CLIP weight matrices by selectively replacing them with their low-rank approximations. It is complementary to existing CLIP-based zero-shot OOD detection methods and could be further extended to finetuning-based methods, which we term as $\\mathrm{SeTAR+FT}$ ", "page_idx": 3}, {"type": "text", "text": "3.1 OOD Detection with Selective Low-Rank Approximation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Low-Rank Approximation Given a linear matrix $W\\in\\mathbb{R}^{m\\times n}$ , its Singular Value Decomposition (SVD) is denoted as $W=U\\Sigma V^{\\top}$ , where $U=[u_{1},u_{2},\\cdot\\cdot\\cdot\\,,u_{m}]\\in\\mathbb{R}^{m\\times m}$ , $V=[v_{1},v_{2},\\cdots,v_{n}]\\in$ $\\mathbb{R}^{n\\times n}$ , and $\\b{\\Sigma}\\in\\mathbb{R}^{m\\times n}$ is a matrix whose entries are all zero except for the singular values of $W$ . These singular values appear in decreasing order on the diagonal (i.e. $\\sigma_{i}^{\\downarrow}(W))$ . The SVD of $W$ can be reformulated as in Equation 4. Given a hyperparameter $r\\in\\mathbb{N}^{+}$ , a rank- $^r$ approximation of $W$ is matrix $\\widehat{W}$ that minimizes $\\|\\boldsymbol{W}-\\widehat{\\boldsymbol{W}}\\|_{2}$ and satisfies rank $(\\widehat{W})\\leq r$ . The optimal solution of this problem $\\widehat{W}$ is provided by Eckart\u2013Young\u2013Mirsky theorem (Low-Rank Approximation, 2024) using Singular Value Decomposition (see Equation 5). ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\underset{W}{\\min}(\\boldsymbol{m},\\boldsymbol{n})}\\\\ &{\\widehat{\\boldsymbol{W}}=\\displaystyle\\sum_{i=1}^{m\\mathrm{in}(\\boldsymbol{m},\\boldsymbol{n})}\\sigma_{i}^{\\downarrow}(\\boldsymbol{W})\\boldsymbol{u}_{i}\\boldsymbol{v}_{i}^{\\top},}\\\\ &{\\widehat{\\boldsymbol{W}}=\\displaystyle\\sum_{i=1}^{r}\\sigma_{i}^{\\downarrow}(\\boldsymbol{W})\\boldsymbol{u}_{i}\\boldsymbol{v}_{i}^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In this work, we will use the term minor singular components to refer to entries in the SVD corresponding to small singular values. These components are removed in low-rank approximation. The term of principle singular components is used to refer to entries in the SVD corresponding to large singular values. These components are kept in a low-rank approximation of the matrix. ", "page_idx": 3}, {"type": "text", "text": "OOD Detection with Selective Low-Rank Approximation SVD-based weight pruning, particularly in noise-prone layers, can substantially reduce a network\u2019s sensitivity to minor perturbations, leading to enhanced stability and robustness (Yao et al., 2024). This stability is crucial for OOD detection, as it ensures the model\u2019s reliable performance across a wide range of inputs. Building on this, we propose a method to improve OOD detection by selectively applying low-rank approximation to weight matrices. By decomposing a weight matrix $W$ into its singular values and vectors, we can identify and retain the principle singular components that significantly contribute to the model\u2019s performance. This approach ensures that the essential features of $W$ are preserved while discarding the less critical minor singular components. Given a weight matrix $W$ in CLIP (e.g., $\\mathrm{W_{up}}$ or $\\mathrm{W_{k.}}$ ), we replace the matrix with its low-rank approximation partW as described in Equation 5 (see Figure 1). Given the rank reduction ratio $\\Theta$ , the rank ofW is determined by $r(\\widehat{W})=\\operatorname{round}((1-\\Theta)\\cdot r(W))$ . This selective low-rank approximation lever ages the compact representation provided by SVD to enhance the model\u2019s ability to detect OOD instances effectively without requiring additional training. We demonstrate our method\u2019s ability to improve OOD detection (Table 1) while maintaining ID classification performance (Table 7) in Section 4.2 and Section 4.5. ", "page_idx": 3}, {"type": "text", "text": "HyperParameter Search Algorithm Due to the presence of many weight matrices in CLIP, each consisting of hundreds of singular values, conducting a complete search over all combinations of low-rank approximation weight matrices is impractical. Therefore, we propose a greedy search algorithm to determine the rank reduction ratio for each weight matrix. Among all linear weight matrices in each encoder layer, we focus on $\\mathrm{W_{up}}$ as it is most effective according to our preliminary experiment. For simplicity, we assume both image and text encoders have $N$ encoder layers. As shown in Algorithm 1, we search by first enumerating all $N$ vision encoder layers sequentially from top to bottom and then all $N$ text encoder layers in the same way. This search order is concisely denoted as searching from $2N$ to the first layer in CLIP. We compare different search algorithms in Section 4.4. The rank reduction ratio for each layer is the objective in SeTAR which is searched among the candidate list $\\Theta=\\{\\Theta_{0},\\Theta_{1},\\cdot\\cdot\\cdot,\\Theta_{J}\\}$ according to the loss on the validation set. We employ the LoCoOp loss (Equation 12) proposed in (Miyai et al., 2023a) as our loss function. This loss requires only ID images. It contains an ID loss for ID image classification and an OOD loss to push away pseudo OOD features from the ID class text embeddings where the pseudo OOD features are from ID-irrelevant nuisances (Equation 10) (e.g., backgrounds) in CLIP\u2019s local features. We refer the readers to Miyai et al. (2023a) or Appendix B for more details. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "For $\\Theta_{j}\\,\\in\\,\\Theta$ , we remove $\\Theta_{j}$ (in percent) singular values along with their corresponding singular vectors to obtain the approximated matrix $\\widehat{\\mathrm{W}}_{\\mathrm{up}}$ (Equation 5). It is worth noting that the  rank reduction raio candidate list includes $\\Theta_{0}\\,=\\,0$ , indicating that the weight matrix has the chance to remain unmodified. ", "page_idx": 4}, {"type": "text", "text": "With the searched rank reduction ratio, the weight matrix $\\mathrm{W_{up}}$ in each CLIP layer is replaced and updated with its approximation. The SeTAR can be easily applied to different ViT backbones (Table 8), by replacing the model weight matrices with their low-rank approximations in a similar approach. Then SeTAR detects the OOD data samples following MCM (Equation 1), GL-MCM (Equation 2) or other scoring-based OOD detection method with the approximated model. We provide an example procedure of the greedy search in Listing 1 for better understanding. ", "page_idx": 4}, {"type": "table", "img_path": "65UoJ0z7Kp/tmp/41034bdb3d0dfb7ef2e70ddf3f9c2a442b37291ca87660ec758999621c557db7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "3.2 OOD Detection with SeTAR-enhanced Low-rank Adaptation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "SeTAR can be further combined with LoRA (Hu et al., 2022) as a novel low-rank adaptation method for OOD detection, which we refer to as $\\mathbf{SeTAR+FT}$ . Specifically, we first apply SeTAR to the pre-trained CLIP model to obtain the reserved rank $r$ for each weight matrix $W$ . Then we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W=\\widehat{W}+B\\times A}\\\\ &{B=\\displaystyle\\sum_{i=r+1}^{\\operatorname*{min}(m,n)}\\sqrt{\\sigma_{i}^{\\downarrow}(W)}u_{i}}\\\\ &{A=\\displaystyle\\sum_{i=r+1}^{\\operatorname*{min}(m,n)}\\sqrt{\\sigma_{i}^{\\downarrow}(W)}v_{i}^{\\top}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\widehat{W}$ is the low-rank approximation of $W$ found by SeTAR , with $A$ and $B$ being the minor singular components. During finetuning, we keepW frozen and only update the low-rank matrix $A$ and $B$ . In this way, we retain the principle singul ar components in the original weight matrix and only update the minor singular components.Unlike LoRA, which evenly distributes the finetuning rank budget across all layers, SeTAR+FT adjusts the rank for each layer, resulting in more effective and efficient fine-tuning (Table 2 and Figure 6). More details are provided in Section 4.3. ", "page_idx": 4}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Dataset Following previous work (Ming et al., 2022; Miyai et al., 2023b), we use two real-world datasets created from ImageNet1K (Deng et al., 2009) and Pascal-VOC (Everingham et al., 2009) as the ID datasets. For OOD datasets, we follow Ming et al. (2022) to preprocess iNaturalist, SUN, Places and Texture, and follow Miyai et al. (2023b) to preprocess ImageNet22K and COCO data. For finetune experiments, we follow Miyai et al. (2023a) to use ImageNet1K as the ID dataset. The detailed description and statistics of the datasets are provided in Appendix C. ", "page_idx": 4}, {"type": "table", "img_path": "65UoJ0z7Kp/tmp/1eee91c1f4d3b661d6b0199341ccd612382853ddd7c53d02b6f33e0ccf5031fa.jpg", "table_caption": ["Table 1: Training-free results of FPR95(FPR) and AUROC(AUC) compared to zero-shot baselines on CLIP-base. Bold values represent the highest performance. \u2020 is cited from Miyai et al. (2023b), where $\\diamond$ represents the absence of reporting in the paper. \u2217denotes the result of our re-run. \u2212denotes the OOD dataset has overlapping categories with the ID dataset. We do not report standard deviations since no training is involved. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Settings Following existing studies (Ming et al., 2022; Miyai et al., 2023b,a), we use CLIP ViT$B/16^{4}$ (Radford et al., 2021) as our backbone. Both image and text encoders have 12 layers. More results with different backbones are in Section 4.4. The rank reduction ratio candidates range from 0 to $40\\%$ in $5\\%$ intervals. We use a temperature of $1^{5}$ , unless stated otherwise. In all experiments, we use one CLIP text prompt: \"a photo of a [CLASS],\", where [CLASS] is the ID class name. We set hyperparameters $\\lambda$ (Equation 12) and top-K (Equation 10) according to the specific ID datasets and backbones. Detailed settings are in Table 12, with a sensitivity analysis in Section 4.4. For $\\mathrm{SeTAR+FT}$ and LoRA experiments, the learning rate and epoch number are set to $1e-2$ and 5 for all experiments. The LoRA rank $r$ is set to match the trainable parameters of $\\mathrm{SeTAR+FT}.$ . Detailed settings are in Table 13. We report results from three runs with seeds 3, 4, $5^{6}$ . All experiments are conducted on a single NVIDIA RTX 4090 GPU. The time cost for low-rank approximation with CLIP-base on the ImageNet1K validation set is about 20 minutes. ", "page_idx": 5}, {"type": "text", "text": "Metrics We use the following metrics for evaluation. (1) the false positive rate (FPR95) for out-ofdistribution (OOD) samples at a fixed true positive rate (TPR) of $95\\%$ for in-distribution samples, with lower values targeting better performance; and (2) the area under the receiver operating characteristic curve (AUROC) for OOD samples, with higher values indicating better performance. ", "page_idx": 5}, {"type": "text", "text": "Baselines We evaluate SeTAR against MCM (Ming et al., 2022) and GL-MCM (Miyai et al., 2023b), state-of-the-art zero-shot OOD detection methods on CLIP. We also compare SeTAR+FT with fine-tuning baselines NPOS (Tao et al., 2023), CoOp (Zhou et al., 2022), LoCoOp (Miyai et al., 2023a), and LoRA (Hu et al., 2022). More details are in Appendix D. ", "page_idx": 5}, {"type": "text", "text": "4.2 Training-free Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The training-free OOD detection performances are summarized in Table 1. Compared with zero-shot baselines, a salient observation is that on both MCM and GL-MCM, using SeTAR outperforms the vanilla method by a large margin across all OOD detection tasks. For example, using Pascal-VOC as ID, SeTAR yields a relatively average reduction of $12.84\\%$ FPR95 on MCM and $18.95\\%$ FPR95 on GL-MCM. Considering that SeTAR is generally applicable and training-free, these results are very encouraging. Comparing SeTAR with scoring function MCM and GL-MCM, SeTAR $^{+}$ GL-MCM performs better on all OOD detection tasks. However, the superiority of GL-MCM score over MCM appears to be contingent upon the choice of the model backbone. As evidenced in Table 8, SeTAR $^{+}$ MCM demonstrates superior performance with a relatively average FPR95 reduction of $8.30\\%$ compared to SeTAR $^{+}$ GL-MCM with CLIP-large as the backbone on ImageNet1K. ", "page_idx": 5}, {"type": "text", "text": "4.3 Fine-tuning Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we compare $\\mathrm{SeTAR+FT}$ with fine-tuning baselines, including NPOS (Tao et al., 2023), CoOp (Zhou et al., 2022), LoCoOp (Miyai et al., 2023a) and LoRA (Hu et al., 2022). LoCoOp is the state-of-the-art prompt-learning OOD detection method on CLIP. LoRA is a representative parameter-efficient fine-tuning method. Following previous work (Tao et al., 2023; Zhou et al., 2022; Miyai et al., 2023a), we report the results on the the ImageNet1K benchmark in Table 2. We observe that $\\scriptstyle\\mathrm{SeTAR+FT}$ outperforms all baselines on both MCM and GL-MCM scoring functions. For example, with CLIP-base as the backbone, $\\scriptstyle\\mathrm{SeTAR+FT}$ achieves a relatively average FPR95 reduction of $3.97\\%$ and $6.67\\%$ compared to LoCoOp and LoRA. Moreover, when scaled up to CLIP-large, SeTAR+FT outperforms LoCoOp and LoRA by ", "page_idx": 6}, {"type": "text", "text": "relatively $17.92\\%$ and $12.45\\%$ FPR95 on the same benchmark. Similar results are observed on Swin Transformer (Liu et al., 2021), where $\\scriptstyle\\mathrm{SeTAR+FT}$ outperforms LoRA by relatively $17.36\\%$ and $36.80\\%$ FPR95 on MSP and Energy scoring functions, respectively. The larger improvement on Swin Transformer may stem from its reliance on ImageNet training, making it prone to overfitting and weaker at OOD detection. Our method mitigates these issues, enhancing Swin\u2019s generalization to OOD instances. These results demonstrate the effectiveness and scalability of $\\mathrm{SeTAR+FT}$ in improving the OOD detection performance. ", "page_idx": 6}, {"type": "text", "text": "Furthermore, as shown in Figure 6, $\\mathrm{SeTAR+FT}$ demonstrates faster convergence and lower loss than LoRA, especially in OOD loss, indicating that $\\mathrm{SeTAR+FT}$ is more effective in adapting the pre-trained weights to the OOD detection task. ", "page_idx": 6}, {"type": "table", "img_path": "65UoJ0z7Kp/tmp/0bd46bd147861d7dbbcd9b7a87ea2ad36354bbed554b0bfbbeab7e8c4019bc53.jpg", "table_caption": ["Table 2: Fine-tuning results on ImageNet1K benchmark. Bold values indicate the highest performance. \u2020 is cited from Tao et al. (2023). \u2217 denotes our re-run results, $\\pm$ indicates the standard deviation from 3 runs. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.4 Ablation Study ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we conduct ablation studies with CLIP-base to understand our design choices. ", "page_idx": 6}, {"type": "text", "text": "Image v.s. Text modality Table As shown, the vision modality outperforms the text modality, indicating the vision modality is more dominant in enhancing the model\u2019s performance. When considering the vision modality alone and the combined vision+text modality, the latter either outperforms or achieves comparable average results to the former. Consequently, we make modifications to both the vision and text modalities in SeTAR to enhance overall performance. ", "page_idx": 6}, {"type": "table", "img_path": "65UoJ0z7Kp/tmp/89abcefbbf7425e0fa375fc12d73392b2686ff6b06e119efd0559e72ccccd017.jpg", "table_caption": ["3 shows an ablation study on the modality involved in SeTA Table 3: Ablation study on modality. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Different Weight Types In this part, we present empirical evidence for modifying $\\mathrm{W_{up}}$ . We first compare the performance of SeTAR with different types of weight matrix in each Transformer layer, including $\\mathrm{W_{q}}$ , $\\mathrm{W_{k}}$ , $\\mathrm{W_{v}}$ , $\\mathrm{W_{o}}$ , $\\mathrm{W_{up}}$ and $\\mathrm{W_{down}}$ . As shown in Figure 2 and Figure 3 of Appendx F, the $X$ -axis denotes the number of weight matrixes (layers) that we have searched, while the $Y$ -axis is the average AUROC and FPR95. The results show that $\\mathrm{W_{up}}$ ", "page_idx": 6}, {"type": "table", "img_path": "65UoJ0z7Kp/tmp/858345dad17ee8879c5fa3cc291f782f4319c8cdf3c16564870b11158e14c0ce.jpg", "table_caption": ["Table 4: Comparison results of SeTAR with and without considering projection matrix $\\mathrm{W_{p}}$ . "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "consistently outperforms other weight matrices in terms of both AUROC and FPR95. In addition to weight matrics in each transformer layer, CLIP has one projection matrix $\\mathrm{W_{p}}$ on top of each encoder, which serves to project image/text representations into a shared space. In Table 4, we compare the performance of SeTAR with and without modifying $\\mathrm{W_{p}}$ . We search $\\mathrm{W_{p}}$ first right before searching the image/text encoder. The results show that frozen $\\mathrm{\\DeltaW_{p}}$ brings a relatively reduction of $4.20\\bar{\\%}$ FPR95. Consequently, we keep $\\mathrm{W_{p}}$ frozen in SeTAR. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Different Search Algorithms At each step of the greedy search, SeTAR traverses the subsequent $\\mathrm{W_{up}}$ in a predefined order and searches over different thresholds. We compare our method with two alternatives: modality-interleaved greedy search (MIS) and layer-exhaustive search (LES). MIS searches the image and text layers in an interleaved manner, while LES simultaneously searches over both layers and thresholds at each step. SeTAR-S, has linear complexity with respect to the number of model layers, similar to MIS, whereas LES has quadratic complexity. ", "page_idx": 7}, {"type": "text", "text": "Table 5 presents the comparison results. SeTARS demonstrates better overall performance than MIS. Notably, MIS encounters limitations when the image and text towers have different layer counts (e.g., CLIP-large with 24 image layers and 12 text layers). Therefore, we choose SeTAR-S for better generalization. Compared to LES, SeTAR-S performs better in terms of both FPR95 and AUROC, as LES\u2019s locally optimal algorithm may not achieve a global optimal solution. These results validate the superiority of our top-to-bottom layer search strategy. ", "page_idx": 7}, {"type": "table", "img_path": "65UoJ0z7Kp/tmp/19f3065423de5be258139cfcce8e096ddd913734ec81c2b8d53850b8d1d6c8e8.jpg", "table_caption": ["Table 5: Results for different search algorithms. Here LES, MIS and SeTAR-S stand for layer-exhaustive search, modality-interleave greedy search, and the search algorithm of SeTAR. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Different Prune Strategies Inspired from SVD, SeTAR modify the model weights by pruning the minor singular components, and retains the principle components that contribute the most to the model\u2019s performance. To validate this design, we compare SeTAR with two alternatives: principal component pruning and random pruning pruning. Principal component takes ", "page_idx": 7}, {"type": "text", "text": "the opposite approach, retaining minor components and pruning major ones. Random pruning, on the other hand, prunes weights randomly. As shown in Table 6, principle pruning suffers from a significant performance drop compared to SeTAR , while random pruning performs slightly better than principle pruning. These results demonstrate the effectiveness of SeTAR \u2019s design choice in pruning the minor components. ", "page_idx": 7}, {"type": "table", "img_path": "65UoJ0z7Kp/tmp/d9575d37e7897bbd2d1b6200043af369cdc420426eb42c3a6c84ceb95e70796c.jpg", "table_caption": ["Table 6: Results for different pruning strategies. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Sensitivity Analysis on $\\lambda$ and top- $\\mathbf{K}$ In this section, we present the sensitivity analysis of the hyperparameters $\\lambda$ (Figure 4) and top- $\\mathbf{\\nabla}\\cdot\\mathbf{K}$ (Figure 5). As observed in Figure 4, the average AUROC remains stable at lower values and slightly decreases as $\\lambda$ increases for both $\\mathbf{SeTAR+MCM}$ and SeTAR $^{+}$ GL-MCM. Notably, the optimal setting of $\\lambda$ may vary depending on the model backbone, with our experiments indicating that CLIP-large may require a larger $\\lambda$ than CLIP-base. Despite this variation, the $\\lambda$ parameter demonstrates strong transferability across datasets for the same backbone. Swapping the optimal $\\lambda$ between ImageNet1K and Pascal-VOC has a minimal performance impact, consistently outperforming the vanilla method. With the VOC-optimized $\\lambda$ on ImageNet1K, CLIPbase achieves an FPR95 of 40.91 and AUROC of 91.02, and CLIP-large reaches 46.73 FPR95 and 91.81 AUROC. Conversely, using the ImageNet1K-optimized $\\lambda$ on Pascal-VOC, CLIP-base achieves 33.18 FPR95 and 93.65 AUROC, while CLIP-large attains 44.39 FPR95 and 92.3 AUROC. ", "page_idx": 7}, {"type": "text", "text": "Top-K controls the number of OOD regions considered in LoCoOp loss: higher values include more OOD regions, with top-K equal to the number of ID classes covering all OOD regions, and top-K set to 0 focusing solely on ID loss. The optimal top-K depends on the number of ID categories, making it non-transferable across datasets. However, SeTAR remains robust to top-K variations, as shown in Figure 5, except at extreme values (0 or the maximum number of classes). We recommend setting top-K to around $30\\%$ of the total categories, such as 300 for ImageNet1K and 4 for Pascal-VOC. For the Swin-base model, top-K at 300 on ImageNet1K yields an FPR95 of 56.82 and AUROC of 85.68 with MSP, and an FPR95 of 52.56 and AUROC of 84.51 with Energy. ", "page_idx": 7}, {"type": "text", "text": "Can SeTAR Improve Image Classification? To evaluate the impact of SeTAR and Se$\\mathrm{TAR+FT}$ on classification accuracy, we present our results on ID dataset ImageNet1K and OOD datasets SUN, Places and Texture in Table $7^{7}$ . SeTAR effectively maintains the average accuracy, with minor variations observed across different datasets. Among the fine-tuned baselines, LoCoOp exhibits a $1\\%$ decrease in accuracy compared to Vanilla CLIP, whereas LoRA ", "page_idx": 8}, {"type": "table", "img_path": "65UoJ0z7Kp/tmp/21b13e5ea16a8724fade2a84d4e3e2f4fcffb16b8e86dd2d68736591346e9941.jpg", "table_caption": ["Table 7: Image classification results with different methods. We use ImageNet1K (IN1K) as ID dataset. \u2217denotes the results of our re-run. The results are averaged over 3 runs. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "shows an improvement of $0.94\\%$ . Notably, $\\scriptstyle\\mathrm{SeTAR+FT}$ surpasses both baselines, improving the average accuracy by $1.45\\%$ compared to Vanilla CLIP. These results highlight the efficacy of SeTAR and SeTAR+FT in improving OOD detection without compromising classification accuracy. ", "page_idx": 8}, {"type": "text", "text": "SeTAR is Effective on Different Architectures and Score Functions We expand on Table 1 with results on ViT and CNN backbones and various score functions. For ViT-based models, we evaluate OOD detection using CLIP-large8 and Swin Transformer9 (Liu et al., 2021), alongside CLIP-base. The Swin Transformer (Liu et al., 2022) is trained on ImageNet1K. Since it lacks a text encoder, we apply SeTAR to the image ViT only. For Swin Transformer, we use two common scoring functions: MSP (Hendrycks & Gimpel, 2017), which leverages softmax confidence, and the Energy score (Liu et al., 2020), with $T=0.1$ for OOD detection. We also integrate CLIP-base ", "page_idx": 8}, {"type": "text", "text": "with the NegLabel score function (Jiang et al., 2024), which uses large-scale negative labels. As shown in Table 8, SeTAR consistently outperforms baselines across all backbones and scoring functions, significantly reducing FPR95 by relatively $20.61\\%$ with the Energy score on Swin Transformer. These results demonstrate SeTAR \u2019s effectiveness in improving OOD detection for unimodal image encoders, with further confirmation from SeTAR $+\\mathrm{FT}$ results (Table 2) across different model backbones. ", "page_idx": 8}, {"type": "text", "text": "We further explore SeTAR\u2019s potential on CNN architecture, and compare it with methods such as Softmax, Energy (Wu et al., 2023), ReAct (Sun et al., 2021), DICE (Sun & Li, 2022), and ASH (Djurisic et al., 2023) on ResNet5010. Since ResNet lacks local features for OOD loss, we conduct experiments using only ID loss. We apply low-rank approximation to the in- and outfeature dimensions of the convolutional layers, ", "page_idx": 8}, {"type": "table", "img_path": "65UoJ0z7Kp/tmp/d8e73ddc9d3659be2db7c160976c0b2f4aa79338e1b3e628fa3dff29ccda4fef.jpg", "table_caption": ["Table 8: Results for different ViT backbones. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "65UoJ0z7Kp/tmp/840f54c0947a743afe8a09913f9f698e51e95edc368b468072263ba6d9559a0b.jpg", "table_caption": ["Table 9: Results on ResNet50. We use ImageNet1K as the ID dataset. \u2020 is cited from Djurisic et al. (2023). "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "combined with ASH for search. As shown in Table 9, SeTAR establishes new state-of-the-art results on ResNet, demonstrating its effectiveness across both ViT and CNN architectures. ", "page_idx": 8}, {"type": "text", "text": "Near-OOD Results To further evaluate SeTAR\u2019s performance on diverse OOD tasks, we test it on a more challenging near-OOD setting using ImageNet1K as the ID dataset and SSBHard (Vaze et al., 2022) as the OOD dataset. As shown in Table 10, SeTAR and $\\mathrm{SeTAR+FT}$ outperform the baselines, demonstrating superior performance in near-OOD scenarios. ", "page_idx": 8}, {"type": "table", "img_path": "65UoJ0z7Kp/tmp/714deaf163dacb53102516918533ce1f5e32daf9f351a32a3ed5110538fab034.jpg", "table_caption": ["Table 10: Near-OOD results on CLIP-base. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Out-of-Distribution Detection Previous work explores OOD detection with unimodal (DeVries & Taylor, 2018; Hendrycks & Gimpel, 2017; Hu & Khan, 2021; Zheng et al., 2020; Zhou et al., 2021) and multimodal (Fort et al., 2021; Ming et al., 2022; Tao et al., 2023; Miyai et al., 2023a) models. Numerous methodologies (Lee et al., 2018; Huang et al., 2021; Sun et al., 2022; Wang et al., 2022; Wu et al., 2023) have been developed to tackle OOD detection in computer vision. Existing CLIP-based OOD detection methods include zero-shot (Fort et al., 2021; Ming et al., 2022; Miyai et al., 2023b; Dai et al., 2023; Wang et al., 2023; Jiang et al., 2024) and fine-tuning (Ming & Li, 2023; Tao et al., 2023; Miyai et al., 2023a). Zero-shot methods like MCM (Ming et al., 2022) and GL-MCM (Miyai et al., 2023b) don\u2019t require in-distribution training data but may perform suboptimally due to domain gaps. Other approaches integrate external knowledge. For example, CLIPN (Wang et al., 2023) pre-trains a novel NO-encoder on the CC-3M dataset (Sharma et al., 2018) to empower CLIP\u2019s \"no\" logic for zero-shot evaluation. NegLabel (Jiang et al., 2024) demonstrates better performance than CLIPN by introducing large-scale negative labels for enhanced label scoring. Fine-tuning methods (Ming & Li, 2023; Tao et al., 2023; Miyai et al., 2023a) improve OOD detection by adapting to in-distribution data but risk damaging the pretraining representations, needing careful training strategies. CNN-based OOD detection methods, including ReAct (Sun et al., 2021), ASH (Djurisic et al., 2023), DICE (Sun & Li, 2022), CIDER (Ming et al., 2023), PALM (Lu et al., 2024), and Hopfield Boosting (Hofmann et al., 2024), have also demonstrated strong results. However, methods like ReAct and ASH rely on the assumption that ID and OOD images produce distinct activations in models trained on ID data. This assumption does not hold in large-scale pre-trained models like CLIP, where activations for ID and OOD images are not significantly different, limiting the effectiveness of such approaches in enhancing CLIP\u2019s zero-shot OOD detection capabilities. SeTAR, in contrast, offers high compatibility with various scoring functions (e.g., MCM, GL-MCM, MSP, Energy), multiple model backbones (e.g., CLIP, Swin, ResNet), and advanced OOD techniques such as NegLabel. Designed to be both lightweight and efficient, SeTAR addresses the demand for resource-efficient solutions in OOD detection. ", "page_idx": 9}, {"type": "text", "text": "Low-rank Approximations of Weight Matrices Neural networks trained with overparameterization often exhibit low-rank properties (Oymak et al., 2019). These properties are utilized in both model training (Povey et al., 2018; Hu et al., 2022) and post-hoc processing (Hajimolahoseini et al., 2021; Sharma et al., 2023). In training, some works (Sainath et al., 2013; Zhang et al., 2014; Zhao et al., 2016) impose low-rank constraints, while LoRA (Hu et al., 2022) adapts pretrained LLMs to downstream tasks using trainable low-rank matrices. For post-hoc processing, pruning methods (Yu et al., 2017; Hajimolahoseini et al., 2021) reduce weight matrix ranks by retaining top-K components from SVD. While pruning preserves model behavior, performance declines with increased intervention. LASER (Sharma et al., 2023) focuses on pruning individual layers to enhance factual answering capabilities. It utilizes a simple greedy search strategy on a validation set, which is not applicable for OOD detection due to the absence of a validation set. In contrast, our approach introduces a selective rank reduction strategy specifically tailored for OOD detection. We systematically analyze and compare different greedy search techniques, evaluating their effectiveness across various layers and model backbones. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We propose SeTAR , a simple and effective OOD detection method using post-hoc low-rank approximation on weight matrices $\\mathrm{W_{up}}$ with a top-down, image-to-text greedy search. SeTAR offers several advantages: (1) training-free, (2) scalable to unimodal and multimodal models, and (3) complementary to existing OOD scoring functions. Building on SeTAR , we introduce SeTAR-FT, a finetuning method that adapts the model to in-distribution data for improved OOD detection. We evaluate SeTAR and SeTAR-FT on large-scale benchmarks, including ImageNet1K and Pascal-VOC. Results show that both achieve state-of-the-art OOD detection performance. We hope our work inspires further research and contributes to more robust and reliable models. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This project was supported by National Natural Science Foundation of China (No. 62306132, No.   \n62106138). We thank the anonymous reviewers for their insightful feedbacks on this work. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Bai, H., Canal, G., Du, X., Kwon, J., Nowak, R. D., and Li, Y. Feed two birds with one scone: Exploiting wild data for both out-of-distribution generalization and detection. In International Conference on Machine Learning, 2023.   \nCaesar, H., Uijlings, J., and Ferrari, V. Coco-stuff: Thing and stuff classes in context. In CVPR, 2018.   \nCimpoi, M., Maji, S., Kokkinos, I., Mohamed, S., and Vedaldi, A. Describing textures in the wild. In CVPR, 2014.   \nDai, Y., Lang, H., Zeng, K., Huang, F., and Li, Y. Exploring large language models for multi-modal out-of-distribution detection. ArXiv, abs/2310.08027, 2023. URL https://api. semanticscholar.org/CorpusID:263909127.   \nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In CVPR, 2009.   \nDeVries, T. and Taylor, G. W. Learning confidence for out-of-distribution detection in neural networks. arXiv preprint:1802.04865, 2018.   \nDjurisic, A., Bozanic, N., Ashok, A., and Liu, R. Extremely simple activation shaping for outof-distribution detection. In The Eleventh International Cosun2021reactnference on Learning Representations, 2023. URL https://openreview.net/forum?id $\\equiv$ ndYXTEL6cZz.   \nEmmott, A., Das, S., Dietterich, T., Fern, A., and Wong, W.-K. A meta-analysis of the anomaly detection problem. arXiv preprint:1503.01158, 2016.   \nEveringham, M., Van Gool, L., Williams, C. K., Winn, J., and Zisserman, A. The pascal visual object classes (voc) challenge. IJCV, 88:303\u2013308, 2009.   \nFort, S., Ren, J., and Lakshminarayanan, B. Exploring the limits of out-of-distribution detection. In NeurIPS, 2021.   \nHajimolahoseini, H., Rezagholizadeh, M., Partovinia, V., Tahaei, M. S., Awad, O. M., and Liu, Y. Compressing pre-trained language models using progressive low rank decomposition. In NeurIPS, 2021.   \nHendrycks, D. and Gimpel, K. A baseline for detecting misclassified and out-of-distribution examples in neural networks. In ICLR, 2017.   \nHendrycks, D., Liu, X., Wallace, E., Dziedzic, A., Krishnan, R., and Song, D. Pretrained transformers improve out-of-distribution robustness. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 2744\u20132751, Online, July 2020.   \nHendrycks, D., Basart, S., Mazeika, M., Mostajabi, M., Steinhardt, J., and Song, D. Scaling out-of-distribution detection for real-world settings. In ICML, 2022.   \nHofmann, C., Schmid, S., Lehner, B., Klotz, D., and Hochreiter, S. Energy-based hopfield boosting for out-of-distribution detection, 2024. URL https://arxiv.org/abs/2405.08766.   \nHu, E. J., yelong shen, Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. LoRA: Low-rank adaptation of large language models. In ICLR, 2022.   \nHu, Y. and Khan, L. Uncertainty-aware reliable text classification. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pp. 628\u2013636, New York, NY, USA, 2021.   \nHuang, R., Geng, A., and Li, Y. On the importance of gradients for detecting distributional shifts in the wild. In NeurIPS, 2021.   \nJiang, X., Liu, F., Fang, Z., Chen, H., Liu, T., Zheng, F., and Han, B. Negative label guided OOD detection with pretrained vision-language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=xUO1HXz4an.   \nLee, K., Lee, K., Lee, H., and Shin, J. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. In NeurIPS, 2018.   \nLi, J., Li, D., Savarese, S., and Hoi, S. BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models. In Proceedings of the 40th International Conference on Machine Learning, 2023.   \nLiang, S., Li, Y., and Srikant, R. Enhancing the reliability of out-of-distribution image detection in neural networks. In ICLR, 2018.   \nLin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll\u00e1r, P., and Zitnick, C. L. Microsoft coco: Common objects in context. In ECCV, 2014.   \nLiu, W., Wang, X., Owens, J., and Li, Y. Energy-based out-of-distribution detection. In NeurIPS, 2020.   \nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., and Guo, B. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV, 2021.   \nLiu, Z., Hu, H., Lin, Y., Yao, Z., Xie, Z., Wei, Y., Ning, J., Cao, Y., Zhang, Z., Dong, L., Wei, F., and Guo, B. Swin transformer v2: Scaling up capacity and resolution, 2022.   \nLow-Rank Approximation. Low-rank approximation \u2014 Wikipedia, the free encyclopedia, January 2024. https://en.wikipedia.org/w/index.php?title $=$ Low-rank_approximation& oldid $=$ 1196167027.   \nLu, H., Gong, D., Wang, S., Xue, J., Yao, L., and Moore, K. Learning with mixture of prototypes for out-of-distribution detection. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id $=$ uNkKaD3MCs.   \nMing, Y. and Li, Y. How does fine-tuning impact out-of-distribution detection for visionlanguage models? International Journal of Computer Vision, 132(2):596\u2013609, September 2023. ISSN 1573-1405. doi: 10.1007/s11263-023-01895-7. URL http://dx.doi.org/10.1007/ s11263-023-01895-7.   \nMing, Y., Cai, Z., Gu, J., Sun, Y., Li, W., and Li, Y. Delving into out-of-distribution detection with vision-language representations. In NeurIPS, 2022.   \nMing, Y., Sun, Y., Dia, O., and Li, Y. How to exploit hyperspherical embeddings for out-of-distribution detection? In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id $\\equiv$ aEFaE0W5pAd.   \nMiyai, A., Yu, Q., Irie, G., and Aizawa, K. Locoop: Few-shot out-of-distribution detection via prompt learning. In Thirty-Seventh Conference on Neural Information Processing Systems, 2023a.   \nMiyai, A., Yu, Q., Irie, G., and Aizawa, K. Zero-shot in-distribution detection in multi-object settings using vision-language foundation models. arXiv preprint arXiv:2304.04521, 2023b.   \nOymak, S., Fabian, Z., Li, M., and Soltanolkotabi, M. Generalization guarantees for neural networks via harnessing the low-rank structure of the jacobian, 2019.   \nPovey, D., Cheng, G., Wang, Y., Li, K., Xu, H., Yarmohammadi, M. A., and Khudanpur, S. Semiorthogonal low-rank matrix factorization for deep neural networks. In Interspeech, 2018. URL https://api.semanticscholar.org/CorpusID:4949673.   \nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In ICML, 2021.   \nRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., et al. Imagenet large scale visual recognition challenge. IJCV, 115(3):211\u2013252, 2015.   \nSainath, T. N., Kingsbury, B., Sindhwani, V., Arisoy, E., and Ramabhadran, B. Low-rank matrix factorization for deep neural network training with high-dimensional output targets. In 2013 IEEE international conference on acoustics, speech and signal processing, pp. 6655\u20136659. IEEE, 2013.   \nSharma, P., Ding, N., Goodman, S., and Soricut, R. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of ACL, 2018.   \nSharma, P., Ash, J. T., and Misra, D. The truth is in there: Improving reasoning in language models with layer-selective rank reduction, 2023.   \nSun, Y. and Li, Y. Dice: Leveraging sparsification for out-of-distribution detection. In Computer Vision \u2013 ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XXIV, pp. 691\u2013708, Berlin, Heidelberg, 2022. Springer-Verlag. ISBN 978-3-031-20052-6. doi: 10.1007/978-3-031-20053-3_40. URL https://doi.org/10.1007/978-3-031-20053-3_ 40.   \nSun, Y., Guo, C., and Li, Y. React: Out-of-distribution detection with rectified activations. In NeurIPS, 2021.   \nSun, Y., Ming, Y., Zhu, X., and Li, Y. Out-of-distribution detection with deep nearest neighbors. In ICML, 2022.   \nTao, L., Du, X., Zhu, X., and Li, Y. Non-parametric outlier synthesis. In ICLR, 2023.   \nVan Horn, G., Mac Aodha, O., Song, Y., Cui, Y., Sun, C., Shepard, A., Adam, H., Perona, P., and Belongie, S. The inaturalist species classification and detection dataset. In CVPR, 2018.   \nVaze, S., Han, K., Vedaldi, A., and Zisserman, A. Open-set recognition: A good closed-set classifier is all you need. In ICLR, 2022.   \nWang, H., Liu, W., Bocchieri, A., and Li, Y. Can multi-label classification networks know what they don\u2019t know? In NeurIPS, 2021.   \nWang, H., Li, Z., Feng, L., and Zhang, W. Vim: Out-of-distribution with virtual-logit matching. In CVPR, 2022.   \nWang, H., Li, Y., Yao, H., and Li, X. Clipn for zero-shot ood detection: Teaching clip to say no. 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 1802\u20131812, 2023. URL https://api.semanticscholar.org/CorpusID:261076240.   \nWu, Q., Chen, Y., Yang, C., and Yan, J. Energy-based out-of-distribution detection for graph neural networks. ArXiv, abs/2302.02914, 2023. URL https://api.semanticscholar.org/ CorpusID:256615740.   \nXiao, J., Hays, J., Ehinger, K. A., Oliva, A., and Torralba, A. Sun database: Large-scale scene recognition from abbey to zoo. In CVPR, 2010.   \nXu, K., Ren, T., Zhang, S., Feng, Y., and Xiong, C. Unsupervised out-of-domain detection via pre-trained transformers. In ACL, 2021.   \nYao, X., Hu, X., Yang, S., and Liu, Y. Enhancing in-context learning performance with just svd-based weight pruning: A theoretical perspective, 2024. URL https://arxiv.org/abs/2406.03768.   \nYu, J., Wang, Z., Vasudevan, V., Yeung, L., Seyedhosseini, M., and Wu, Y. CoCa: Contrastive captioners are image-text foundation models. arXiv preprint arXiv:2205.01917, 2022.   \nYu, X., Liu, T., Wang, X., and Tao, D. On compressing deep models by low rank and sparse decomposition. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 67\u201376, 2017. URL https://api.semanticscholar.org/CorpusID:24553488.   \nZhang, Y., Chuangsuwanich, E., and Glass, J. R. Extracting deep neural network bottleneck features using low-rank matrix factorization. 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 185\u2013189, 2014. URL https://api.semanticscholar. org/CorpusID:1791734.   \nZhao, Y., Li, J., and Gong, Y. Low-rank plus diagonal adaptation for deep neural networks. 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5005\u20135009, 2016. URL https://api.semanticscholar.org/CorpusID:10506309.   \nZheng, Y., Chen, G., and Huang, M. Out-of-domain detection for natural language understanding in dialog systems. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 28: 1198\u20131209, 2020.   \nZhou, B., Lapedriza, A., Khosla, A., Oliva, A., and Torralba, A. Places: A 10 million image database for scene recognition. TPAMI, 40(6):1452\u20131464, 2017.   \nZhou, K., Yang, J., Loy, C. C., and Liu, Z. Learning to prompt for vision-language models. IJCV, 2022.   \nZhou, W., Liu, F., and Chen, M. Contrastive out-of-distribution detection for pretrained transformers. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 1100\u20131111, Online and Punta Cana, Dominican Republic, November 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Impact Statements ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Limitation While we demonstrate the effectiveness of our method on OOD detection, we acknowledge that our work has several limitations. First, despite we show the robustness of our method to hyperparameters, the optimal hyperparameters may vary across different model backbones. Future work is needed to explore the autonomous selection of hyperparameters. Second, we design Se$\\mathrm{TAR+FT}$ in a simple and straightforward manner, which may not be the most efficient or effective way to adapt the model to the ID downstream data. More sophisticated strategies for model adaptation are worth exploring in future research. Third, we only conduct experiments to detect visual OOD inputs and ignore inputs in other modalities such as textual, audio and video. This is primarily because our model is based on CLIP. Exploring the development of OOD detectors across diverse modalities remains an active research topic for future investigation. ", "page_idx": 14}, {"type": "text", "text": "Ethical Considerations Our study addresses the challenge of OOD detection through low-rank approximation, which is particularly relevant for ensuring the reliability and trustworthiness of vision-and-language pre-trained models. Future investigations on fairness, privacy and transparency neural-based models should be encouraged to mitigate the existing data biases and safety problems for a responsible, helpful and trustworthy AI system in diverse real-world applications. ", "page_idx": 14}, {"type": "text", "text": "Future Societal Consequences Our proposed SeTAR achieves impressive OOD detection performance, which is beneficial to various real-world machine learning applications, such as healthcare and autonomous vehicles. The identification of anomalies or unexpected data points is crucial for decision-making and risk management with AI models. A better OOD detector facilitates the development of trustworthy machine-learning models that can reject unknown data inputs and help alleviate the hallucination problem. Moreover, better OOD detectors like SeTAR can help to select and label the unfamiliar data samples to further train a stronger model in the wild. ", "page_idx": 14}, {"type": "text", "text": "B Loss Function ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To improve the model\u2019s OOD detection ability, it is crucial to define a loss function that pushes OOD samples far from ID samples while keeping ID samples close to each other. However, since OOD samples are unavailable during development, we address this issue by using the LoCoOp loss (Miyai et al., 2023a) for both SeTAR and $\\mathrm{SeTAR+FT}$ . The main idea is to create pseudo OOD features with ID-irrelevant nuisances (e.g., backgrounds) in CLIP\u2019s local features. ", "page_idx": 14}, {"type": "text", "text": "Specifically, we divide the image into patches, represented by the set of all patch indices $I\\,=$ $\\{\\bar{0},1,2,\\dots,H\\times W-1\\}$ , where $H$ and $W$ denote the height and width of the patch features. Next, we compute the cosine similarity between the image patch features $p_{i}^{v}$ and the text features $h_{c}^{t}$ of the image label. The classification prediction probabilities for each patch $i$ are then given by: ", "page_idx": 14}, {"type": "equation", "text": "$$\np_{i}(y=m|\\mathbf{x})=\\frac{\\exp(\\mathrm{cos}_{-}\\mathrm{sim}(p_{i}^{v},h_{c}^{t})/\\tau^{\\prime})}{\\sum_{c=1}^{K}\\exp(\\mathrm{cos}_{-}\\mathrm{sim}(p_{i}^{v},h_{c}^{t})/\\tau^{\\prime})}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "For a given image patch related to an ID category, the corresponding ID label should be among its top-K predictions. Conversely, for patches unrelated to the $\\mathrm{ID}$ label, such as background regions, the ID label should be excluded from the top- $\\mathbf{\\nabla}\\cdot\\mathbf{K}$ predictions. Based on this intuition, the indices of ID-irrelevant regions within an image are defined by Equation 10, where rank $\\left(p_{i}(y=\\mathbf{y}|\\mathbf{x})\\right)$ ) denotes the rank of the true class $\\mathbf{y}$ among all ID classes, and $\\mathbf{K}$ is the hyperparameter. ", "page_idx": 14}, {"type": "equation", "text": "$$\nJ=\\{i\\;|\\;\\mathrm{rank}(p_{i}(y=\\mathbf{y}|\\mathbf{x}))>\\mathrm{K}\\}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "After identifying out-of-distribution (OOD) regions, it is expected that their image features will differ significantly from the ID text embeddings. To enhance this distinction, entropy maximization is employed to increase the entropy of $p_{j}(\\bar{y|\\mathbf{x}})$ , where $p_{j}$ denotes the classification prediction probabilities for region $j\\in J$ . The entropy maximization is formally defined as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\mathcal{L}}_{\\mathrm{ood}}=-H(p_{j})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Here, $H(\\cdot)$ represents the entropy function. The overall loss function combines the ID loss (crossentropy loss for ID predictions) with the OOD loss. Here $\\lambda$ is the hyperparameter that regulates the proportion of the OOD loss. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\mathcal{L}_{\\mathrm{id}}+\\lambda\\mathcal{L}_{\\mathrm{ood}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "table", "img_path": "65UoJ0z7Kp/tmp/21d146d07187ad9e7bcf99748fc78d466c051a918350682169ab4adc7b49303d.jpg", "table_caption": ["Table 11: The statistics of the dataset used in this paper. \u2018ID\u2019 and \u2018OOD\u2019 denote in-distribution and out-of-distribution, respectively. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "We use two real-world datasets created from ImageNet1K (Deng et al., 2009) and Pascal-VOC (Everingham et al., 2009) as the ID dataset. We use ImageNet-1K validation set as the ID test set following Ming et al. (2022), and preprocess Pascal-VOC following Miyai et al. (2023b). we build two ID validation sets for low-rank approximation. The ID validation set of ImageNet1K is collected by sampling one image for each label from the ImageNet1K training set. For Pascal-VOC, For Pascal-VOC, We randomly sample $10\\%$ images as the ID validation set and leave the rest as the ID test set. ", "page_idx": 15}, {"type": "text", "text": "For OOD datasets, we follow Ming et al. (2022) to preprocess iNaturalist, SUN, Places and Texture, and follow Miyai et al. (2023b) to preprocess ImageNet22K and COCO data. We only evaluate the OOD datasets that have no overlapping categories as the ID dataset. ", "page_idx": 15}, {"type": "text", "text": "We provide more details about the datasets used in our experiments, in terms of data sources, preprocessing, and the statistics for each dataset, as shown in Table 11 and below. ", "page_idx": 15}, {"type": "text", "text": "ImageNet1K We use the ImageNet-1000 (ILSVRC2012) (Deng et al., 2009) dataset for ID validation and testing. The original dataset contains 1.2 million training images and 50,000 validation images from 1000 classes, and is widely used for image classification. We follow Ming et al. (2022) to construct the ImageNet1K ID test set from the validation set. Additionally, we curate an ImageNet1K ID validation set from the training set by randomly selecting one image for each label. ", "page_idx": 15}, {"type": "text", "text": "Pascal-VOC The Pascal VOC (Visual Object Classes) (Everingham et al., 2009) dataset is a benchmark dataset widely used in computer vision, featuring annotated images across multiple object categories. We use the Pascal-VOC subset collected by Miyai et al. (2023b) as the ID dataset, each image has single-class ID objects and one or more OOD objects. The ID validation and test set are split by 1:9 for each class, resulting in 94 and 906 images, respectively. ", "page_idx": 15}, {"type": "text", "text": "iNaturalist iNaturalist (Van Horn et al., 2018) is a biodiversity dataset containing millions of labeled images of plants, animals, and insects. Ming et al. (2022) construct a subset with 10,000 images by de-duplicating concepts overlapped with ID datasets. ", "page_idx": 15}, {"type": "text", "text": "Places Places (Zhou et al., 2017) is a scene-centric database with 205 scene categories and 2.5 million images. We use the SUN subset collected by Ming et al. (2022) as the OOD test set, which contains 10,000 images that are not overlapped with the ID classes. ", "page_idx": 15}, {"type": "text", "text": "SUN SUN (Scene UNderstanding) (Xiao et al., 2010) is a comprehensive collection of labeled images representing a diverse range of indoor and outdoor scenes. We use the SUN subset collected by Ming et al. (2022) as the OOD test set, which contains 10,000 images that are not overlapped with the ID classes. ", "page_idx": 15}, {"type": "text", "text": "Texture The Texture dataset (DTD) (Cimpoi et al., 2014) comprises 5640 images categorized into 47 terms inspired by human perception, aimed at replicating human-like texture recognition in machines. Again, we use the subset collected by Ming et al. (2022) as the OOD test set. ", "page_idx": 15}, {"type": "text", "text": "ImageNet22K The ImageNet-22K dataset (Russakovsky et al., 2015), formerly known as ImageNet21K, addresses the underestimation of its additional value compared to the standard ImageNet-1K pretraining, aiming to provide high-quality pretraining for a broader range of models. We use the filtered subset collected by Wang et al. (2021) as the OOD test set for MC-COCO and Pascal-VOC ID test sets. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "COCO Miyai et al. (2023b) curated an MS-COCO OOD test set (COCO for short) with 1,000 images that are not overlapped with the Pascal-VOC ID classes, which we use as OOD testing data for Pascal-VOC ID test set. ", "page_idx": 16}, {"type": "text", "text": "D Fine-tune Baselines ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We compare SeTAR+FT with 4 finetuning-based baselines. These baselines include: ", "page_idx": 16}, {"type": "text", "text": "\u2022 NPOS. NPOS (Tao et al., 2023) generates virtual anomalies in low-probability regions of ID data without relying on distribution assumptions, enhancing discrimination during training. ", "page_idx": 16}, {"type": "text", "text": "\u2022 CoOp. CoOp (Zhou et al., 2022) optimizes prompts for vision-language models with learnable context vectors for efficient few-shot learning. ", "page_idx": 16}, {"type": "text", "text": "\u2022 LoCoOp. LoCoOp (Miyai et al., 2023a) improves upon CoOp by leveraging CLIP\u2019s local features to better distinguish between ID and OOD samples, achieving higher detection accuracy with less training data. We follow the official code11 to prepare and fine-tune the LoCoOp with CLIP-base and CLIP-large. Follow Miyai et al. (2023a), the top-K, $\\lambda$ , learning rate and epoch num are set to 200, 0.25, 0.002 and 50. Temperature is set to 1 and the text prompt is initiated with \u201cX X X X X X X X X X X X X X X X [CLASS]\u201d, where [CLASS] is the ID class name. We average the results from 3 seeds finetuned with 1-shot ImageNet1K valid data. ", "page_idx": 16}, {"type": "text", "text": "\u2022 LoRA. LoRA (Hu et al., 2022) is a low-rank adaptation method that injects trainable lowrank decomposition matrices into the pre-trained model to adapt to downstream tasks. We apply low-rank adaptation to the same weight type as $\\mathrm{SeTAR+FT},$ the rank of each layer is set to match the trainable parameters of SeTAR. Details settings can be found in Table 13. ", "page_idx": 16}, {"type": "text", "text": "E HyperParameters Settings ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The hyperparameters for SeTAR are shown in Table 12. And the hyperparameters for $\\scriptstyle\\mathrm{SeTAR+FT}$ and LoRA are shown in Table 13. ", "page_idx": 16}, {"type": "text", "text": "Table 12: Hyperparameters for SeTAR . Temperature is set to 1 except for Swin-base with Energy score, where it is set to 0.1. ", "page_idx": 16}, {"type": "table", "img_path": "65UoJ0z7Kp/tmp/e0f7fc64737020aa863cdfa2a5c4487b787775b9dcb6b3ec3d59b5ccb2b82624.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Table 13: Hyperparameters for $\\mathbf{SeTAR+FT}$ and LoRA on ImageNet1K. Temperature is set to 1 except for Swin-base with Energy score, which is set to 0.1. ", "page_idx": 16}, {"type": "table", "img_path": "65UoJ0z7Kp/tmp/823acc7d8004960ef59a5a6bf005558193bbb878a27f773d8f485f17e29e04bd.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "F More Detailed Experiment Results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we present additional detailed results from the main paper. This includes the detailed results of fine-tuned baselines on the ImageNet1K benchmark in Table 14; detailed ablation results on modality, $\\mathrm{W_{p}}$ , $\\lambda$ , and top-K in Table 15, Table 16, Table 19, and Table 21; and detailed results of SeTAR with different search algorithms, prune strategies and backbones in Table 18, Table 20, Table 17 and Table 22. ", "page_idx": 17}, {"type": "table", "img_path": "65UoJ0z7Kp/tmp/b7626bec21ec3e5a762411f1a0862906d711d5f704be4d4df3749c339a9bf359.jpg", "table_caption": ["Table 14: Detail results of FPR95(FPR) and AUROC(AUC) compared with fine-tuned baselines on ImageNet1K benchmark. $^\\dagger$ is cited from Tao et al. (2023). \u2217denotes the results of our re-run. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "65UoJ0z7Kp/tmp/c60d35fd86c3551bcc9c8822af26c6911fea429c0411ddfd4a1691a56915c642.jpg", "table_caption": ["Table 15: Detail results of ablation study on modality. We use CLIP-B/16 as a backbone. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "65UoJ0z7Kp/tmp/9ea91979a7be772cdd9bb6d4decc98ccfda0914776002629c187d9c88880839e.jpg", "table_caption": ["Table 16: Detail results of SeTAR with and without considering projection matrix $\\mathrm{W_{p}}$ . We use CLIP-B/16 as a backbone. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "65UoJ0z7Kp/tmp/09a3a5517cfe0a4eea4610189f9e74dbab42737af7c4f87ce9456239b1784023.jpg", "table_caption": ["Table 17: Detail results for SeTAR with different backbones. \u2020 is cited from Jiang et al. (2024). \u2217 denotes the result of our re-run. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "65UoJ0z7Kp/tmp/501fd0ac06040be498c4791d2bb052a9a5283523f6f2fca96651b45af02070dd.jpg", "table_caption": ["Table 18: Detail results for different search algorithms. Here LES stands for layer-exhaustive greedy search, MIS stands for modality-interleave greedy search, and SeTAR-S stands for the search algorithm of SeTAR, which searches vision and text layers sequentially. We use CLIP-B/16 as a backbone. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "", "table_caption": ["Table 19: Detail results of ablation study on $\\lambda.$ . We use CLIP-B/16 as a backbone. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "65UoJ0z7Kp/tmp/9a4c6b193d23eb09637756882d2cdc7e9d2366b92baa3428d25cdffb6d58989a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Pascal-VOC MCM Score ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "0.01 5.58 98.43 25.14 94.94 29.13 93.01 40.41 93.35 47.85 92.12 49.60 89.37 32.95 93.54   \n0.05 4.59 98.71 24.91 95.15 28.46 93.21 40.44 93.58 48.25 92.08 48.10 89.70 32.46 93.74   \n0.10 5.44 98.50 24.97 95.06 29.60 93.01 42.55 93.26 48.69 92.28 47.80 89.82 33.18 93.65   \n0.15 5.97 98.53 26.50 95.07 30.88 93.05 46.22 92.94 50.99 92.07 49.80 89.80 35.06 93.58   \n0.20 6.11 98.53 26.18 95.08 30.53 93.06 45.43 93.06 50.68 92.16 49.40 89.82 34.72 93.62   \n0.25 6.41 98.43 26.19 94.99 31.24 92.89 47.36 92.72 50.41 92.13 50.20 89.74 35.30 93.48   \n0.30 6.81 98.34 26.98 94.80 32.13 92.65 48.67 92.52 50.53 92.14 51.10 89.77 36.04 93.37 ", "page_idx": 20}, {"type": "text", "text": "GL-MCM Score ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "0.01 4.42 98.83 22.72 94.73 25.93 93.51 22.07 96.22 32.62 94.27 43.50 90.91 25.21 94.74   \n0.05 3.66 98.96 21.93 94.81 25.04 93.62 20.35 96.36 31.47 94.31 40.70 91.19 23.86 94.87   \n0.10 3.79 98.94 21.40 94.76 25.05 93.49 20.74 96.29 30.42 94.48 40.00 91.20 23.57 94.86   \n0.15 3.50 98.98 20.83 94.84 24.34 93.55 20.57 96.20 29.84 94.42 38.50 91.25 22.93 94.87   \n0.20 3.50 98.94 20.72 94.74 24.13 93.48 19.95 96.28 29.22 94.46 38.60 91.19 22.69 94.85   \n0.25 4.14 98.96 21.54 94.85 25.37 93.54 23.37 96.14 32.18 94.51 40.30 91.44 24.48 94.90   \n0.30 4.15 98.90 21.40 94.63 25.16 93.33 23.01 96.03 31.02 94.44 38.90 91.40 23.94 94.79 ", "page_idx": 20}, {"type": "table", "img_path": "65UoJ0z7Kp/tmp/8d9f3dc4b0ac3d68601582141a8a42168c23e5611416e483da1530fa61ca1b59.jpg", "table_caption": ["Table 20: Detail results on different pruning strategies. We use CLIP-B/16 as a backbone. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "65UoJ0z7Kp/tmp/3388a43014d78fb5d36a511d024c930993bc720cfe595205797ed71869d008ef.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "65UoJ0z7Kp/tmp/92eab498bacb2d5bae07666872b03427f69ae8151b077488d905623765a3b940.jpg", "table_caption": ["Table 21: Detail results of ablation study on top-K. We use CLIP-B/16 as a backbone. "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "65UoJ0z7Kp/tmp/e8d8cc95aac0fe572802ac6ff515ca87d8c82efe007387ed9e3fa1b8b8900ee8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "65UoJ0z7Kp/tmp/97fbb68ba7e70684c1cb440dad0f75f6350019bfa70dc501b8cd5937855308ae.jpg", "table_caption": ["Table 22: Detail results of ResNet50. We use ImageNet1K as the ID dataset. \u2020 is cited from Djurisic et al. (2023). "], "table_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "65UoJ0z7Kp/tmp/f3122db7593c011219297d879b5aa92a1a15dda984a83e435b15912343718ad2.jpg", "img_caption": ["(a) MCM score "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "65UoJ0z7Kp/tmp/53b62ab7effb8063f038f4750ea717f707f0e67edf658cd33a7bac5d53d5e99a.jpg", "img_caption": ["(b) GL-MCM score "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 2: Average AUROC/FPR95 of different weight types on ImageNet1K benchmark. We use CLIP-B/16 as a backbone. ", "page_idx": 22}, {"type": "image", "img_path": "65UoJ0z7Kp/tmp/7d7554082eb1bfc09d2053ef6e581220fce9595da48900bf20174026cbe92843.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "", "img_caption": ["(b) GL-MCM score "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 3: Average AUROC/FPR95 of different weight types on Pascal-VOC benchmark. We use CLIP-B/16 as a backbone. ", "page_idx": 22}, {"type": "image", "img_path": "65UoJ0z7Kp/tmp/0ff58f6be9150c32b0d843fcbd7b9be73056b98d09fb5a8aab28c7650823878b.jpg", "img_caption": ["Figure 4: Ablation studies on $\\lambda$ on different ID datasets. We use CLIP-B/16 as a backbone. ", "(b) Pascal-VOC "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "65UoJ0z7Kp/tmp/8413d458fa2a4e657ee3de7a6c94ba9139806a30bbffd8f317db235532ba8dee.jpg", "img_caption": ["Figure 5: Ablation studies on top-K on different ID datasets. We use CLIP-B/16 as a backbone. ", "(b) Pascal-VOC "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "65UoJ0z7Kp/tmp/baec641076a1a2decc55a861b7d3255a52b020e7fb54224cdd39b5176f117558.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure 6: Loss plots of $\\mathbf{SeTAR+FT}$ v.s. LoRA on ImageNet1K. We use CLIP-B/16 as a backbone. $\\scriptstyle\\mathrm{SeTAR+FT}$ demonstrates faster convergence across all losses, especially in the OOD loss. For reference, with MCM score, SeTAR+FT achieves an average FPR of 38.77 at epoch 5. While LoRA achieves an average FPR of 42.88, 39.92 and 42.23 at epoch 1, 5 and 15, respectively. ", "page_idx": 24}, {"type": "image", "img_path": "65UoJ0z7Kp/tmp/1237397543e395ab9c5fad9c0a94e4ff47760601665dbed01c7d51282f6d166a.jpg", "img_caption": ["(c) Swin-base "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure 7: Visualization of SeTAR rank reduction ratio distribution on different ID datasets with different backbones. IN1K, VOC stand for ImageNet1K and Pascal-VOC. And V, T stand for visual modality and text modality of the CLIP model. ", "page_idx": 24}, {"type": "table", "img_path": "65UoJ0z7Kp/tmp/c45161755a17652150babcd944c7a3941c8f849ae5348846da6558871ac46e4b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "Listing 1: Example procedure of SeTAR on ImageNet1K with CLIP-base. We search the visual and text tower from top to bottom. At each step, we select the best ratio that minimizes the loss. ", "page_idx": 25}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We have carefully crafted the abstract and introduction to accurately reflect the contributions and scope of the paper. Specifically, we propose a novel training-free method, SeTAR with a finetuning extension SeTAR+FT, and demonstrate its effectiveness for OOD detection tasks. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 26}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Limitations of the proposed method are discussed in Appendix A ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not include theoretical results. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper includes detailed experimental and hyperparameters settings in Section 4.1 and Appendix E. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Code are available at https://github.com/X1AOX1A/SeTAR. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We provide detailed experimental settings in Section 4.1 and Appendix E. We give the details of our design choices in Section 4.4 and datasets in Appendix C. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We report the results with standard deviation from runs of 3 seeds. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We provide the details of the compute resources in Section 4.1. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: We provide the ethical considerations in Appendix A Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: We provide the broader impacts in Appendix A. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: Our work does not directly provide pre-trained models or scraped datasets. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: We include the citation and URL of models and datasets used in the paper. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 30}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: We do not introduce new assets in the paper. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: Our work does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: Our work does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}]