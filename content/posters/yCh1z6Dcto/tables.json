[{"figure_path": "yCh1z6Dcto/tables/tables_6_1.jpg", "caption": "Table 1: Vision datasets used for few-shot learning", "description": "This table lists five vision datasets used in the few-shot learning experiments.  For each dataset, it provides the setting (e.g., bird species, handwritten characters), the number of classes (broken down into training, validation, and testing sets), the total number of samples, and the image resolution.", "section": "4.1 Few-shot learning"}, {"figure_path": "yCh1z6Dcto/tables/tables_6_2.jpg", "caption": "Table 2: Vision tasks: few-shot learning accuracy (%) with Forward (FF) and Backward (BP) gradients. The averaged accuracy over 100 testing tasks is reported. FT: full fine-tuning; LP: linear probing; Quant: 16w8a with symmetric quantization. FF outperforms zero-shot across the board, and achieves comparable performance (accuracy within 5%) to BP on 26 out of 30 tasks.", "description": "This table presents the results of few-shot learning experiments using both forward and backward gradient methods. It compares the accuracy of various model architectures (ResNet12, ResNet18, ViT tiny) on five different image classification datasets (CUB, Omniglot, Cifar100_fs, miniImageNet, tieredImageNet) with different training methods (full fine-tuning, linear probing).  The table shows the performance of both floating-point and quantized training, highlighting the performance of the forward gradient approach relative to backpropagation.", "section": "4.1 Few-shot learning"}, {"figure_path": "yCh1z6Dcto/tables/tables_7_1.jpg", "caption": "Table 3: Audio datasets used for few-shot learning. The ESC-50 dataset includes a labeled collection of 2000 environmental audio recordings, and FSDKaggle2018 is an audio dataset containing 11,073 audio files annotated with 41 labels of the AudioSet Ontology. Both datasets are used for benchmarking methods of environmental sound classification.", "description": "This table lists two audio datasets used in the few-shot learning experiments of the paper.  It provides the name of each dataset, the type of audio it contains, the number of classes, the number of samples, and the length of each sample. The datasets are used to evaluate the performance of different few-shot learning methods on environmental sound classification.", "section": "4.1 Few-shot learning"}, {"figure_path": "yCh1z6Dcto/tables/tables_7_2.jpg", "caption": "Table 2: Vision tasks: few-shot learning accuracy (%) with Forward (FF) and Backward (BP) gradients. The averaged accuracy over 100 testing tasks is reported. FT: full fine-tuning; LP: linear probing; Quant: 16w8a with symmetric quantization. FF outperforms zero-shot across the board, and achieves comparable performance (accuracy within 5%) to BP on 26 out of 30 tasks.", "description": "This table presents the results of few-shot learning experiments on various vision datasets using different model architectures (ResNet12, ResNet18, ViT tiny).  It compares the accuracy of training with forward gradients (FF) against backpropagation (BP), showing both full fine-tuning (FT) and linear probing (LP) results.  Quantized versions of forward gradients are also included (Quant).  The table highlights FF's performance relative to BP and zero-shot baselines, demonstrating its effectiveness in few-shot learning scenarios.", "section": "4.1 Few-shot learning"}, {"figure_path": "yCh1z6Dcto/tables/tables_9_1.jpg", "caption": "Table 2: Vision tasks: few-shot learning accuracy (%) with Forward (FF) and Backward (BP) gradients. The averaged accuracy over 100 testing tasks is reported. FT: full fine-tuning; LP: linear probing; Quant: 16w8a with symmetric quantization. FF outperforms zero-shot across the board, and achieves comparable performance (accuracy within 5%) to BP on 26 out of 30 tasks.", "description": "This table presents the results of few-shot learning experiments on various vision datasets using different model backbones (ResNet12, ResNet18, ViT tiny) and training methods (full fine-tuning, linear probing).  It compares the accuracy of using forward gradients (FF) against backpropagation (BP), with and without quantization (16-bit weights, 8-bit activations).  The table highlights the performance of FF relative to BP and zero-shot learning (no adaptation).", "section": "4.1 Few-shot learning"}, {"figure_path": "yCh1z6Dcto/tables/tables_13_1.jpg", "caption": "Table 2: Vision tasks: few-shot learning accuracy (%) with Forward (FF) and Backward (BP) gradients. The averaged accuracy over 100 testing tasks is reported. FT: full fine-tuning; LP: linear probing; Quant: 16w8a with symmetric quantization. FF outperforms zero-shot across the board, and achieves comparable performance (accuracy within 5%) to BP on 26 out of 30 tasks.", "description": "This table presents the results of few-shot learning experiments using both forward and backward gradient methods.  It compares the accuracy of different training approaches (full fine-tuning and linear probing) across five vision datasets and three network backbones (ResNet12, ResNet18, and ViT-tiny).  The table also includes results for quantized forward gradient training (16-bit weights, 8-bit activations).  The key finding is that forward gradients achieve comparable accuracy to backpropagation in many cases, especially when utilizing larger models.", "section": "4.1 Few-shot learning"}, {"figure_path": "yCh1z6Dcto/tables/tables_14_1.jpg", "caption": "Table 2: Vision tasks: few-shot learning accuracy (%) with Forward (FF) and Backward (BP) gradients. The averaged accuracy over 100 testing tasks is reported. FT: full fine-tuning; LP: linear probing; Quant: 16w8a with symmetric quantization. FF outperforms zero-shot across the board, and achieves comparable performance (accuracy within 5%) to BP on 26 out of 30 tasks.", "description": "This table presents the results of few-shot learning experiments on vision tasks using both forward and backward gradient methods. It compares the accuracy of full fine-tuning and linear probing with different precision levels (FP16 and quantized 16w8a).  The results show that forward gradients perform comparably to backward gradients on many tasks and significantly improve over zero-shot performance.", "section": "4.1 Few-shot learning"}, {"figure_path": "yCh1z6Dcto/tables/tables_15_1.jpg", "caption": "Table 8: The hyper-parameters used in our experiments for cross-domain adaptation. All hyper-parameters for FF and BP are the same except that FF uses a smaller learning rate. Model architectures of ViT tiny, and the associated pre-trained weights can be found at [39]. Different learning rate grids are explored, and the best accuracy is reported.", "description": "This table lists the hyperparameters used in the cross-domain adaptation experiments described in the paper.  It shows that the hyperparameters for both forward gradient learning (FF) and backpropagation (BP) are largely the same, with the key difference being a smaller learning rate used in FF.  The table also notes the source of pre-trained weights and the methodology for selecting optimal learning rates.", "section": "4.2 Cross-domain Adaptation"}, {"figure_path": "yCh1z6Dcto/tables/tables_16_1.jpg", "caption": "Table 8: The hyper-parameters used in our experiments for cross-domain adaptation. All hyper-parameters for FF and BP are the same except that FF uses a smaller learning rate. Model architectures of ViT tiny, and the associated pre-trained weights can be found at [39]. Different learning rate grids are explored, and the best accuracy is reported.", "description": "This table lists the hyperparameters used in the cross-domain adaptation experiments.  It shows that the hyperparameters for both forward gradient (FF) and backpropagation (BP) methods were largely the same, except for the learning rate, which was smaller for FF.  The ViT tiny model architecture and pretrained weights are referenced.", "section": "4.2 Cross-domain Adaptation"}]