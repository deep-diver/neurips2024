[{"figure_path": "tSWoT8ttkO/tables/tables_8_1.jpg", "caption": "Table 1: Average performance on the classic MuJoCo tasks at 5M time steps \u00b1 standard error.", "description": "This table compares the average performance of different reinforcement learning algorithms on five classic MuJoCo locomotion tasks.  The algorithms compared are TD3, SAC, TQC, TD3+OFE, TD7, and the proposed RESEL algorithm.  The performance is measured at 5 million time steps and reported as the average return \u00b1 standard error over six independent trials. The table highlights RESEL's competitive performance against state-of-the-art methods.", "section": "5.2 Performance Comparisons"}, {"figure_path": "tSWoT8ttkO/tables/tables_22_1.jpg", "caption": "Table 2: Hyperparameters of RESeL.", "description": "This table lists the hyperparameters used in the RESEL algorithm.  Different values were used depending on the specific task (classic MuJoCo, classic meta-RL, other tasks, etc.).  The hyperparameters include learning rates for the context encoder and other layers (both for policy and value networks), the discount factor (\u03b3), whether the last reward was used as input, the batch size, target entropy, learning rate for entropy coefficient \u03b1, and soft-update factor for the target value network.  The number of randomly sampled data points is also specified.", "section": "D.3.3 Hyperparameters"}, {"figure_path": "tSWoT8ttkO/tables/tables_24_1.jpg", "caption": "Table 3: GPU utilization, memory, and time cost with various neural network types. Time denotes the time cost for each update iteration, in HalfCheetah-v2. Normalized time is normalized with the corresponding GRU time cost.", "description": "This table compares the GPU utilization, memory usage, and time cost of using fully connected (FC) networks, Mamba networks, and GRU networks as context encoders in the HalfCheetah-v2 environment.  The results are broken down by the number of layers/blocks used in the context encoder (1, 2, 3, or 4). The time cost is presented in milliseconds (ms), and a normalized time is also provided relative to the GRU time cost for easier comparison. This table shows that Mamba consistently uses less GPU memory and time than GRU while maintaining comparable performance.", "section": "D.3.3 Hyperparameters"}, {"figure_path": "tSWoT8ttkO/tables/tables_25_1.jpg", "caption": "Table 4: Average performance on the classic POMDP benchmark with gravity changes at 1.5M time steps \u00b1 one standard error over 6 seeds.", "description": "This table presents the average performance and standard error across six trials for different algorithms (RESEL, PPO-GRU, MF-RNN, SAC-Transformer, SAC-MLP, TD3-MLP, GPIDE-ESS, VRM, A2C-GRU) on various classic partially observable Markov decision process (POMDP) tasks.  The POMDP tasks involve locomotion with gravity changes, where the gravity is not fully observable.  The table shows the average return achieved by each algorithm across AntBLT-P-v0, AntBLT-V-v0, HalfCheetahBLT-P-v0, HalfCheetahBLT-V-v0, HopperBLT-P-v0, HopperBLT-V-v0, WalkerBLT-P-v0, and WalkerBLT-V-v0 tasks. The results highlight the improved performance of the RESEL algorithm compared to existing baselines.", "section": "E.3.1 POMDP Tasks"}, {"figure_path": "tSWoT8ttkO/tables/tables_25_2.jpg", "caption": "Table 5: Average performance on the MuJoCo benchmark with gravity changes at 2M time steps \u00b1 one standard error over 6 seeds.", "description": "This table presents the average performance and standard error of different reinforcement learning algorithms on five MuJoCo locomotion tasks with randomized gravity.  The gravity is varied across 60 different conditions, with 40 used for training and 20 for testing.  The algorithms compared are RESEL (the proposed method), SAC-MLP, SAC-GRU, ESCP, PEARL, EPI, OSI, and ProMP.  The results show the average return achieved by each algorithm across the five tasks.", "section": "E.3.2 Dynamics-Randomized Tasks"}, {"figure_path": "tSWoT8ttkO/tables/tables_26_1.jpg", "caption": "Table 6: Average performance on the classic MuJoCo benchmark at 300k, 1M, and 5M time steps, over 6 trials \u00b1 standard errors.", "description": "This table shows the average performance results on five classic MuJoCo locomotion tasks (HalfCheetah-v2, Hopper-v2, Walker2d-v2, Ant-v2, Humanoid-v2) at different training time steps (300k, 1M, and 5M). The performance is measured using several state-of-the-art reinforcement learning algorithms including TD3, SAC, TQC, TD3+OFE, TD7, and the proposed RESEL algorithm.  Each entry in the table represents the average return achieved by each algorithm, along with the standard error across 6 independent trials. The table allows for a comparison of the algorithms' performance at different training stages, indicating their learning speed and asymptotic performance.", "section": "E.3.3 Classic MDP tasks"}]