[{"figure_path": "tSWoT8ttkO/figures/figures_1_1.jpg", "caption": "Figure 1: A simple recurrent policy architecture.", "description": "This figure shows a recurrent policy architecture commonly used in reinforcement learning.  It consists of a recurrent neural network (RNN)-based context encoder, which processes the current observation, the last action, and the reward to extract unobservable hidden states.  An MLP policy then uses these hidden states along with the current observation to generate actions.  The architecture is designed to handle partially observable Markov decision processes (POMDPs), where the full state of the environment is not directly observable.", "section": "2 Background"}, {"figure_path": "tSWoT8ttkO/figures/figures_3_1.jpg", "caption": "Figure 2: Policy and critic architectures of RESEL.", "description": "The figure shows the architectures of both policy and critic networks in the RESEL model.  Both networks share a similar structure, beginning with pre-encoders that process observations, actions, and rewards, followed by a context encoder using an RNN (like GRU or Mamba) to extract hidden states. The output of the context encoder is then fed to an MLP policy (for the policy network) or multiple MLP critics (for the critic network).  The use of multiple critics implements the ensemble-Q mechanism from REDQ for improved stability. ", "section": "4.1 Model Architectures"}, {"figure_path": "tSWoT8ttkO/figures/figures_5_1.jpg", "caption": "Figure 3: Action variations as the rollout step increases after a single gradient update with different values of LRCE and LRother. Action variation refers to the change in policy output after the gradient update compared to its output before the update, using the same input sequences.", "description": "The figure shows how action variations change over rollout steps after a single gradient update.  It compares different learning rates for the context encoder (LRCE) and other layers (LRother). The left panel zooms in on the initial steps, highlighting how small initial changes are amplified over time with the autoregressive nature of RNNs when LRCE is high.  The right panel shows the overall trends, demonstrating that using a lower LRCE for the context encoder stabilizes training, preventing the large output variations that occur when a higher LRCE is applied.", "section": "4.2 Stabilizing Training with a Context-Encoder-Specific Learning Rate"}, {"figure_path": "tSWoT8ttkO/figures/figures_6_1.jpg", "caption": "Figure 4: L2-norm of the policy gradient for different RNN stabilization approaches.", "description": "This figure visualizes the L2-norm of the policy gradient during training for two POMDP tasks (HalfCheetahBLT-V-v0 and WalkerBLT-V-v0).  It compares two different learning rate settings for the context encoder (LRCE): LRCE = 10\u207b\u2075 (red line) and LRCE = 3 \u00d7 10\u207b\u2074 (orange line). The orange line uses gradient clipping to prevent divergence, which highlights the instability introduced by the higher learning rate. The y-axis represents the L2-norm of the policy gradient, and the x-axis represents the training timestep. The plots show that the lower learning rate (red line) leads to significantly more stable training with much less oscillation in the gradient norm compared to the higher learning rate (orange line) which shows large oscillations and ultimately leads to clipping.", "section": "5.1 Training Stability"}, {"figure_path": "tSWoT8ttkO/figures/figures_6_2.jpg", "caption": "Figure 6: Learning curves in four POMDP tasks with different learning rates and RNN stabilization techniques, shaded with one standard error. We fixed LRother = 3 \u00d7 10-4. Some learning curves in AntBLT-V and HalfCheetahBLT-V are incomplete as some runs encountered infinite or NaN outputs.", "description": "This figure compares the performance of Recurrent Off-policy RL with Context-Encoder-Specific Learning Rate (RESEL) with different learning rates for the context encoder (RNN) and other layers (MLP). It displays learning curves on four POMDP tasks, showing return over time.  The shaded area represents one standard error. The results highlight the impact of a lower learning rate for the context encoder in stabilizing training and improving performance, especially when compared to the case with the same learning rates for both context encoder and other layers.", "section": "5.1 Training Stability"}, {"figure_path": "tSWoT8ttkO/figures/figures_7_1.jpg", "caption": "Figure 7: Learning curves shaded with one standard error in classic POMDP tasks.", "description": "This figure displays the learning curves for several recurrent reinforcement learning algorithms across four classic partially observable Markov decision process (POMDP) tasks. The tasks involve robotic locomotion (Ant, HalfCheetah, Hopper, and Walker) with partial observability induced by obscuring either velocity or position information. The algorithms compared include RESEL (the proposed method), MF-RNN, SAC-MLP, PPO-GRU, A2C-GRU, TD3-MLP, VRM, and GPIDE-ESS (state-of-the-art). The curves show the cumulative reward obtained over time, illustrating the training stability and performance of each algorithm.  Shading represents one standard error.", "section": "5.2 Performance Comparisons"}, {"figure_path": "tSWoT8ttkO/figures/figures_7_2.jpg", "caption": "Figure 8: Learning curves shaded with one standard error in dynamics-randomized tasks.", "description": "This figure presents the learning curves for five different MuJoCo locomotion tasks (Ant, HalfCheetah, Hopper, Humanoid, and Walker2d) under dynamics randomization.  The x-axis represents the training timesteps, and the y-axis represents the average return. Multiple lines are shown for each task, representing different reinforcement learning algorithms being compared: RESEL (the proposed method), SAC-MLP, SAC-GRU, ESCP, PEARL, EPI, OSI, and ProMP. The shaded area around each line indicates the standard error over multiple runs.  The purpose is to show the comparative performance of RESEL against existing state-of-the-art meta-RL algorithms in scenarios where the environment dynamics are partially observable (gravity is randomized and thus not directly available to the agent).", "section": "5 Experiments"}, {"figure_path": "tSWoT8ttkO/figures/figures_8_1.jpg", "caption": "Figure 9: Learning curves shaded with one standard error in meta-RL tasks.", "description": "This figure presents the learning curves for four different meta-RL tasks: AntDir-v0, CheetahDir-v0, HalfCheetahVel-v0, and Wind-v0.  Each curve represents the performance of a different algorithm over time, showing the average return achieved by each algorithm. The shaded regions represent the standard error around the mean return. The algorithms compared are RESeL (the proposed method), MF-RNN, SAC-MLP, VariBad-Onpolicy, RL2, and VariBad-Offpolicy. The figure demonstrates the relative performance of each algorithm on these challenging meta-RL tasks, highlighting the strengths and weaknesses of each approach in terms of learning speed, stability, and final performance.", "section": "5.2 Performance Comparisons"}, {"figure_path": "tSWoT8ttkO/figures/figures_8_2.jpg", "caption": "Figure 10: Success rate in the key-to-door task with different credit assignment lengths.", "description": "This figure shows the success rate of three different algorithms (RESEL, MF-GPT, and MF-RNN) on the Key-to-Door task with varying credit assignment lengths. The x-axis represents the credit assignment length, ranging from 60 to 500. The y-axis represents the success rate, ranging from 0 to 1.  The results indicate that RESeL achieves comparable or better success rates than MF-GPT and MF-RNN across all credit assignment lengths, showing its robustness in handling the task complexity.", "section": "5.2 Performance Comparisons"}, {"figure_path": "tSWoT8ttkO/figures/figures_9_1.jpg", "caption": "Figure 11: Sensitivity studies of varied learning rates in terms of the average final return.", "description": "This figure presents the results of sensitivity studies conducted to determine the optimal learning rates for the context encoder (LRCE) and other layers (LRother) in the RESEL algorithm.  The plots show how the average final return of the WalkerBLT-V-v0 task varies with different values of LRCE (while LRother is fixed), LRother (while LRCE is fixed), and when LRCE and LRother are set to the same value. The results highlight the importance of using distinct learning rates for the context encoder and other layers in order to achieve optimal performance and training stability. ", "section": "5.3 Sensitivity and Ablation Studies"}, {"figure_path": "tSWoT8ttkO/figures/figures_9_2.jpg", "caption": "Figure 18: Learning curves in terms of the stochastic policy performance with different RNN architectures.", "description": "This figure compares the learning curves of the exploration policy (with exploration noise added to actions) across different RNN architectures: RESEL-Mamba, RESEL-GRU, RESEL-Transformer, MF-RNN (GRU), and SAC-Transformer.  The results show the stochastic policy performance over time (timesteps) across four partially observable locomotion tasks. The plot helps visualize the effect of the chosen recurrent neural network (RNN) architecture on the stability and performance of the reinforcement learning agent.  While the caption is short, the figure's purpose is to show the effectiveness of RESEL in various RNN settings and compare it against prior state-of-the-art methods.", "section": "E.5 More Comparisons of Policy Performance with different RNNs"}, {"figure_path": "tSWoT8ttkO/figures/figures_18_1.jpg", "caption": "Figure 13: Illustrating for sampling a stacked batch from replay buffer.", "description": "This figure illustrates how to sample a stacked batch from a replay buffer in RESEL, an algorithm designed to handle varying trajectory lengths in reinforcement learning.  Because the lengths of trajectories in the replay buffer are not uniform, a simple concatenation wouldn't work.  Instead, the algorithm stacks shorter trajectories along the time dimension, using a hidden reset flag at the beginning of each trajectory to reset the RNN's hidden state and preventing mixing between trajectories. To prevent convolution from mixing trajectories, K-steps of zero data are inserted between trajectories.", "section": "4.3 Training Procedure of RESEL"}, {"figure_path": "tSWoT8ttkO/figures/figures_23_1.jpg", "caption": "Figure 14: The t-SNE visualization of the outputs of the context encoder in Halfcheetah with different gravity.", "description": "This figure shows a t-SNE visualization of the context encoder's outputs in the HalfCheetah environment with varying gravity.  The colorbar indicates the magnitude of the gravity acceleration. The x and y axes represent the 2D t-SNE embedding.  The visualization demonstrates that the context encoder's outputs are not randomly distributed but rather cluster according to the gravity acceleration. This suggests that RESEL successfully learns to extract gravity-related information from the environment's dynamics.", "section": "E.1 Visualization"}, {"figure_path": "tSWoT8ttkO/figures/figures_24_1.jpg", "caption": "Figure 15: One-step update time with different types of context encoder.", "description": "This figure shows the one-step update time cost comparison between three different context encoder types: MLP, Mamba, and GRU.  The results demonstrate that Mamba significantly reduces the update time compared to GRU, while still achieving faster speeds than MLP.", "section": "5.1 Training Stability"}, {"figure_path": "tSWoT8ttkO/figures/figures_25_1.jpg", "caption": "Figure 16: Network forward/backward time overhead for GRU and Mamba layers.", "description": "This figure compares the time overhead of GRU and Mamba layers during network forward and backward passes with varying sequence lengths.  The left panel shows the forward pass time, while the right panel depicts the backward pass time.  The plot illustrates that the time cost for GRU increases linearly with the sequence length, while the time cost for Mamba remains relatively constant, demonstrating better scalability and efficiency for Mamba, especially when dealing with longer sequences.", "section": "E.2 Time Overhead Comparisons"}, {"figure_path": "tSWoT8ttkO/figures/figures_26_1.jpg", "caption": "Figure 17: Ablation studies on the context length.", "description": "This figure compares the performance of the RESEL algorithm when trained with full trajectories versus training with trajectory segments of 64 steps.  The results show that training with full trajectories generally leads to better performance, particularly in tasks with shorter trajectory lengths. However, the difference in performance between full trajectories and segments is less pronounced in tasks with longer and more cyclical trajectories (like HalfCheetahBLT-V). This suggests that using shorter trajectory segments might be sufficient for tasks with highly cyclical data, as the segments can effectively capture the properties of the full trajectory.", "section": "E.4 Ablation Studies on Context Length"}, {"figure_path": "tSWoT8ttkO/figures/figures_27_1.jpg", "caption": "Figure 18: Learning curves in terms of the stochastic policy performance with different RNN architectures.", "description": "This figure compares the performance of RESEL using different RNN architectures (Mamba, GRU, and Transformer) for four different partially observable Markov decision process (POMDP) tasks.  The y-axis represents the stochastic policy return, and the x-axis shows the training timesteps. The shaded area around each line represents the standard error. The figure shows that RESEL with different RNN architectures performs similarly across these POMDP tasks, suggesting that the choice of RNN architecture is relatively less important compared to the context-encoder-specific learning rate.", "section": "E.5 More Comparisons of Policy Performance with different RNNs"}, {"figure_path": "tSWoT8ttkO/figures/figures_27_2.jpg", "caption": "Figure 19: Sensitivity studies of the context-encoder-specific learning rates in terms of the average final return.", "description": "This figure shows the sensitivity analysis of the context-encoder-specific learning rates (LRCE and LRother) on the WalkerBLT-V-v0 environment.  The x-axis represents different values of LRother, while LRCE is fixed at 5 \u00d7 10^-6. The y-axis represents the average final return achieved. The plot demonstrates the effect of varying the learning rate for the other layers (MLP) while keeping the learning rate for the context encoder (RNN) constant.  The results suggest an optimal LRother value exists, showing that the performance is sensitive to the MLP learning rate, while being relatively insensitive to LRCE in this particular experiment.", "section": "Sensitivity and Ablation Studies"}, {"figure_path": "tSWoT8ttkO/figures/figures_28_1.jpg", "caption": "Figure 20: Sensitivity studies of the context-encoder-specific learning rates in terms of the average final return in eight POMDP tasks. The variants with the highest final return are marked with \u2605.", "description": "This figure shows the results of sensitivity studies on the context-encoder-specific learning rates for eight POMDP tasks.  Different learning rates for the context encoder (LRCE) and other layers (LRother) were tested.  Each bar represents the average final return for a particular combination of LRCE and LRother. The highest-performing variants for each task are marked with a star. The results highlight the importance of using distinct learning rates for the context encoder and other layers to achieve optimal performance in POMDP tasks.", "section": "5.3 Sensitivity and Ablation Studies"}]