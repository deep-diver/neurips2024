[{"type": "text", "text": "Coupled Mamba: Enhanced Multi-modal Fusion with Coupled State Space Model ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Wenbing Li Hang Zhou Junqing Yu Zikai Song\u2020 Wei Yang\u2020 ", "page_idx": 0}, {"type": "text", "text": "Huazhong University of Science and Technology {wenbingli, henrryzh, yjqing, skyesong, weiyangcs}@hust.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The essence of multi-modal fusion lies in exploiting the complementary information inherent in diverse modalities. However, prevalent fusion methods rely on traditional neural architectures and are inadequately equipped to capture the dynamics of interactions across modalities, particularly in presence of complex intra- and inter-modality correlations. Recent advancements in State Space Models (SSMs), notably exemplified by the Mamba model, have emerged as promising contenders. Particularly, its state evolving process implies stronger modality fusion paradigm, making multi-modal fusion on SSMs an appealing direction. However, fusing multiple modalities is challenging for SSMs due to its hardware-aware parallelism designs. To this end, this paper proposes the Coupled SSM model, for coupling state chains of multiple modalities while maintaining independence of intra-modality state processes. Specifically, in our coupled scheme, we devise an inter-modal hidden states transition scheme, in which the current state is dependent on the states of its own chain and that of the neighbouring chains at the previous time-step. To fully comply with the hardware-aware parallelism, we devise an expedite coupled state transition scheme and derive its corresponding global convolution kernel for parallelism. Extensive experiments on CMU-MOSEI, CH-SIMS, CH-SIMSV2, BRCA, MM-IMDB through multi-domain input verify the effectiveness of our model compared to current state-of-the-art methods, improved F1-Score by $0.4\\%$ , $0.9\\%$ , and $2.3\\%$ on the CMU-MOSEI, CH-SIMS and CH-SIMSV2 datasetes respectively, $49\\%$ faster inference and $83.7\\%$ GPU memory save. The results demonstrate that Coupled Mamba model is capable of enhanced multi-modal fusion. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Real-world data captured and processed across multiple modalities, such as text, image, video, and sensor data, yield a rich tapestry of information that is inherently complementary. This complementarity profoundly enhances the capacities of deep learning models, facilitating more nuanced interpretations and predictions. As a result, deep learning models that integrate multi-modal data have shown substantial superiority over their uni-modal counterparts in various domains, including visual-language learning [1, 2, 3], multi-modal classification/segmentation [4, 5, 6, 7], sentiment analysis [8, 9, 10, 11] and etc. Given these advantages, the development of effective multi-modal fusion techniques has emerged as a center of attention. A variety of works have explored this topic on convolution or Transformer -based models, and developed specified mechanisms as early, middle, and late fusion, depending on position of fusion been conducted. A more prevalent practice is to first extract features using modality-specific backbones and then devise a fusion module to exploit the complementary information from all modalities. Existing fusion paradigms either aggregate modal-specific features into one by neglecting individual intra-modal propagation [12] or align modal-specific features into a united representation space through regulation while failing to exploit complementary inter-modal information exchange for difficulty in alignment supervision [13]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Recently, the state space models, advanced by the LSSL [14, 15, 16], S4 [17], GSS [18], and S4D [19, 20], use state variables to explicitly model the sequential evolving neural states, have being emerged as compelling alternatives to Transformers for its efficiency in modeling long-range sequences [21]. Particularly, Mamba [22], improves with a selective scanning mechanism and hardware-aware parallelism to enable very efficient training and inference, achieving comparable performances to Transformers on large-scale data. Yet, existing explorations focus on processing uni-modal data, and the multi-modal fusion mechanism on SSMs is still under-investigated. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we observe that the explicit state variables in SSMs provide great fusion anchors, i.e., from which we can extract inter-model complementary information, and to where we can fuse the complementary information into a unified representation. Inspired by the effective Coupled Hidden Markov Model (CHMM) [23], we investigate the multi-modal fusion problem of the Mamba model from a state transition perspective. For multi-modal fusion on Mamba, the bruteforce way is to direct aggregate features from all multi-modalities into one feature, i.e., the aggregation approach, and process with a sole Mamba model. However, such an approach neglects the individual intra-modal propagation. Instead, we propose the Coupled Mamba model, for coupling state chains of multiple modalities while maintaining independence of intra-modality state processes. Specifically, in our coupled scheme, we ", "page_idx": 1}, {"type": "image", "img_path": "UXEo3uNNIX/tmp/289db624fbe088fc2764e663d20590d6c48cdcdb37c2e2f7b652a7dc947a92de.jpg", "img_caption": ["Figure 1: Architecture of Coupled Mamba. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "devise an inter-modal hidden states transition scheme, in which the current state is dependent on the states of its own chain and that of the neighbouring chains at the previous time-step. Another challenge is to fully comply with the hardware-aware parallelism for efficiency, we achieve parallel computing by deriving multi-modal global convolution kernels. As shown in Figure 1, the entire Coupled Mamba model consists of $N$ layers, and is finally adapted to downstream tasks through pooling. Each layer has $M$ Coupled Mamba blocks, where $M$ is the number of modalities. Each Coupled Mamba block receives sequence data of multiple modalities as input, aggregate states from multiple modalities, and then transits into the state at next time of each individual modality. We conduct extensive experiments on CMU-MOSEI, CH-SIMS [24], CH-SIMSV2 [25] datasets through multi-domain input, and verify the effectiveness of our model compared to current state-of-the-art methods, with $0.{}^{\\!\\!4\\%}$ , $0.9\\%$ , $2.\\dot{3}\\%$ F1-Score increase, $49\\%$ faster inference and $83.7\\%$ GPU memory save. The results demonstrate that our Coupled Mamba model enhances the multi-modal fusion with state coupling. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Multi-modal Fusion Multi-modal fusion focuses on combining features from various modalities into unified representations to tackle multi-modal learning challenges. Traditionally, fusion methods are categorized into feature-level early fusion and decision-level late fusion, based on where fusion occurs within the model [26]. Early fusion techniques are employed by [27] to merge features from diverse modalities such as audio, text, and vision. [28] introduce a method using two separate branches for spatial and temporal modalities with a straightforward post-fusion for video action recognition. Other notable post-fusion approaches include works like [9] and [29, 30], which suggest robust late fusion via rank minimization. Recent advances in deep learning have expanded the concept of early fusion to mid-term fusion, which integrates features at multiple levels [31]. For instance, [32] develop a fused representation by progressively combining multiple fusion layers. Similarly, [33] propose a multi-layer fusion method that connects all modality-specific networks through a central network. [? ] introduce an architecture search algorithm to identify the optimal fusion architecture. Furthermore, [34, 35] incorporate attention mechanisms into multi-modal fusion, while [13] suggest exchanging feature channels between modalities. Additionally, [36] integrate bilinear pooling into attention blocks, showing its effectiveness in capturing higher-level feature interactions by stacking multiple attention blocks for image captioning. The focus has recently shifted towards dynamic fusion, which selects the optimal fusion strategy from various candidate operations based on inputs from different modalities [37, 38]. This dynamic approach offers greater flexibility for different multi-modal tasks compared to static methods. Inspired by the success of dynamic fusion designs and higher-level feature interaction capture in multi-modal fusion, our work aims to dynamically capture hidden states both within and between modalities using coupled state space models via state diffusion, enabling more efficient modality fusion for complex multi-modal tasks. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "State Space Models State Space Models (SSM) are exceptionally effective at learning the complex correlations inherent in language sequences. The seminal work of [17] introduced the structured state space model (S4), which aims to encapsulate the extended dependency characteristics of language sequences. Conceptually, S4 combines the unique properties of CNNs and RNNs to create a powerful framework for sequential data processing. Building on the foundation laid by S4, subsequent research efforts have been devoted to solving the problem of linearly scaling sequence lengths. In this regard, [39] introduced S5 utilizing MIMO-SSM and parallel scan technology, while [40] proposed H3, which greatly improved the performance of SSM, and [18] introduced GSS, which demonstrated faster training and competitive performance. Furthering the current state of research, [22] developed a novel language model called Mamba. This model uniquely combines a data-selective SSM layer and a parallel scanning algorithm to solve Transformer\u2019s quadratic complexity calculation problem in long sequence modeling and Transformer\u2019s inability to model data outside the attention window. This also illustrates the huge potential of Mamba in processing sequence data. ", "page_idx": 2}, {"type": "text", "text": "Coupled Hidden Markov Model Hidden Markov Model (HMM) is a probabilistic model that simulates a sequence of hidden states to generate a sequence of observations. The core components of the model include the state transition matrix A, the observation probability matrix (emission matrix) B and the initial state probability vector $\\pi$ . This model assumes the existence of Markov chains between hidden states, and observation events are independently generated by hidden states. To address specific needs, researchers have developed several HMM variants. For example, the Hierarchical Hidden Markov Model (HHMM) [41] introduces a state hierarchy based on standard HMMs, while the Mixed Hidden Markov Model (MHMM) [42] combines multiple HMMs to Build complex distributions. These extensions improve the applicability of HMM in various scenarios and further promote the application of sequence data analysis in multiple fields. Coupled Hidden Markov Models (CHMM) [23] are a class of tools capable of modeling multiple interrelated time series. In multimodal fusion, we usually focus on signals from different channels, such as audio, text, and facial expressions, which are all time-correlated. Coupled HMMs can effectively model such data because they can consider dynamic correlations between multiple channels simultaneously. ", "page_idx": 2}, {"type": "text", "text": "3 Coupled State Space Model ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we introduce Coupled Mamba method for multi-modal fusion in detail, which performs multi-modal fusion by introducing multi-modal historical states. As shown in Figure 2, it contains two parts: state coupling and state space model. ", "page_idx": 2}, {"type": "text", "text": "3.1 Preliminary ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In recent years, the state space model has developed rapidly [17, 19, 40]. Mamba introduced a selectivity mechanism based on S4, which converted the original time-invariant characteristics. Mamba is based on the concept of continuous systems by introducing hidden states $h\\left(t\\right)\\in\\mathbb{R}^{N}$ to map a series of inputs $\\boldsymbol{x}\\left(t\\right)\\in\\bar{\\mathbb{R}}^{L}$ to obtain output $y\\left(t\\right)\\in\\dot{\\mathbb{R}}^{L}$ , where $\\Nu$ denotes the number of hidden states. The continuous system can be expressed as: ", "page_idx": 2}, {"type": "equation", "text": "$$\nh^{\\prime}(t)=\\mathbf{A}h(t)+\\mathbf{B}x(t),\\;\\;y(t)=\\mathbf{C}h(t).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where ${\\bf A}\\in\\mathbb{R}^{N\\times N}$ represents the state transition matrix of the system, and ${\\bf B}\\in\\mathbb{R}^{N\\times1}$ , $\\mathbf{C}\\in\\mathbb{R}^{N\\times1}$ are projection matrices. Mamba uses a time scale parameter $\\Delta$ to discretize the continuous parameters $\\mathbf{A},\\mathbf{B}$ into $\\mathbf{\\overline{{A}}},\\mathbf{\\overline{{B}}}$ , the zero-order hold (ZOH) principle is adopted by default. The discretized statespace equation is: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\overline{{\\mathbf{A}}}=\\exp\\left(\\mathbf{\\DeltaA}\\mathbf{A}\\right),\\;\\;\\overline{{\\mathbf{B}}}=\\left(\\mathbf{\\DeltaA}\\right)^{-1}\\left(\\exp\\left(\\mathbf{\\DeltaA}\\mathbf{A}\\right)-\\mathbf{I}\\right)\\cdot\\mathbf{\\DeltaA}\\mathbf{B}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "image", "img_path": "UXEo3uNNIX/tmp/186996d5a5570bc1785bccf8793b646e8e4d29d88699445170d22eb50357cbad.jpg", "img_caption": ["Figure 2: Coupling Mamba receives input $x_{t-1}$ , and performs internal state switching and output through three key parameter matrices, where $\\mathbf{B},\\mathbf{C}$ and $\\mathbf{S}$ are respectively represented as the input matrix, output matrix and state transfer matrix. The hidden states are summed across modalities and used for state transition input to generate next time states. The state is propagated sequentially in time. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Then the discretized version of Eq. (1) with step size $\\Delta$ can be rewritten as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nh_{t}=\\overline{{\\mathbf{A}}}h_{t-1}+\\overline{{\\mathbf{B}}}x_{t},\\ y_{t}=\\mathbf{C}h_{t}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Finally, by expanding $h_{t-1}$ layer by layer, the global convolution kernel $\\overline{{\\mathbf{K}}}\\in\\mathbb{R}^{L}$ can be obtained, and $\\overline{{\\mathbf{K}}}$ is used to calculate the output $y$ , which is defined as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overline{{\\mathbf{K}}}=\\left(\\mathbf{C}\\overline{{\\mathbf{B}}},\\mathbf{C}\\overline{{\\mathbf{A}\\mathbf{B}}},...,\\mathbf{C}\\overline{{\\mathbf{A}}}^{\\mathrm{L}-1}\\overline{{\\mathbf{B}}}\\right),}\\\\ &{\\quad y=x\\otimes\\overline{{\\mathbf{K}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathrm{L}$ is the length of the input sequence $x$ and $\\otimes$ denotes the convolution operation. For algorithm 1, L denotes the sequence length, E denotes the extended dimension, D denotes the feature dimension, and B denotes the batch size. ", "page_idx": 3}, {"type": "text", "text": "3.2 Coupled State Transition ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "For multi-modality data input, one naive way is to aggregate the multi-modal features into one feature and process using a single Mamba model. However, such approach neglects intra-modal propagation. Inspired by the Coupled Hidden Markov Model (CHMM) [23], a more elegant solution is to model mutual modality transition probability as follows: ", "page_idx": 3}, {"type": "text", "text": "Algorithm 1: Coupled Mamba Data: Input: $\\begin{array}{r l}&{\\mathbf{H_{t-1}^{i}}=\\left\\{h_{t-1}^{1},h_{t-1}^{2},...,h_{t-1}^{M}\\right\\},\\mathbf{x_{t-1}}:}\\\\ &{h_{t-1}^{m}\\in\\mathbb{R}^{N},x_{t-1}\\in(B,L,D)}\\end{array}$ Result: Output: $\\mathbf{y_{t}}:(B,L,D)$ Require :Input Ensure :Output   \n1 ;   \n2 Normalize the input sequence:   \n3 $\\mathbf{x_{t-1}^{\\prime}}:(B,L,D)\\gets\\mathbf{LayerNorm}(x_{t-1});$   \n4 $\\mathbf{u}:(B,L,E)\\gets\\mathbf{Linear}_{u}(x_{t-1}^{\\prime})$ ;   \n5 $\\mathbf{z}:(B,L,E)\\leftarrow\\mathbf{Linear}_{z}(x_{t-1}^{\\prime});$ ;   \n6 ;   \n7 Process with Coupled Mamba:   \n8 for o in forward do   \n9 $\\mathbf{u}_{\\mathbf{o}}^{\\prime}:(B,L,E)\\gets\\mathbf{SiLU}(\\mathbf{Convld}_{o}(u))$ ;   \n10 $\\mathbf{B_{o}}:(B,L,N)\\gets\\mathbf{Linear}_{B}^{o}(u_{o}^{\\prime})$ ;   \n11 $\\mathbf{C_{o}}:(B,L,N)\\gets\\mathbf{Linear}_{C}^{o}(u_{o}^{\\prime})$ ;   \n12 $\\Delta_{\\mathrm{o}}:(B,L,E)\\leftarrow\\log(1+$ $\\mathrm{exp}(\\mathbf{Linear}_{\\Delta_{o}}(u_{o}^{\\prime})+\\mathbf{Parameter}_{\\Delta_{o}})\\,;$ );   \n13 $\\mathbf{S_{o}}:(B,L,E,N)\\gets\\Delta_{\\mathbf{o}}^{\\mathbf{N}}\\otimes\\mathbf{Parameter}.$ Ao;   \n14 $\\mathbf{B_{o}}:(B,L,E,N)\\gets\\Delta_{\\mathbf{o}}^{\\mathbf{N}}\\otimes\\mathbf{B_{o}}$ ;   \n15 $\\mathbf{y_{o}}:(B,L,E)\\leftarrow$ $\\mathbf{CSSM}(\\mathbf{S_{o}},\\mathbf{B_{o}},\\mathbf{C_{o}})(\\mathbf{H_{t-1}},\\mathbf{u_{o}^{\\prime}});$   \n16 end   \n17 ;   \n18 Get gated $y_{o}$ :;   \n19 $\\mathbf{y}_{\\mathbf{forward}}^{\\prime}:(B,L,E)\\gets y_{f o r w a r d}\\odot\\mathbf{SiLU}(z);$   \n20 Residual connection:;   \n21 $\\mathbf{y_{t}}:(B,L,D)\\gets\\mathbf{Linear}_{T}(y_{f o r w a r d}^{\\prime})+\\mathbf{x_{t-1}};$ ", "page_idx": 3}, {"type": "equation", "text": "$$\nP_{i=1:M,j}=P\\left(h_{t}^{j}|h_{t-1}^{1},h_{t-1}^{2},...,h_{t-1}^{M}\\right)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $P_{i=1:M,j}$ is the probability transition matrix from all modalities to current modality $j$ . For SSM with $M$ multi-modal $M$ ", "page_idx": 3}, {"type": "text", "text": "input, we have state propagation sequences. In alignment with CHMM, we can model the state transition of a modality $m$ by coupling all the modality states as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nh_{t}^{m}=\\sum\\left(\\overline{{\\mathbf{A}}}_{1,m}h_{t-1}^{1},\\overline{{\\mathbf{A}}}_{2,m}h_{t-1}^{2},...,\\overline{{\\mathbf{A}}}_{M,m}h_{t-1}^{M}\\right)+\\overline{{\\mathbf{B}}}_{m}x_{t}^{m},\\ y_{t}^{m}=\\mathbf{C}h_{t}^{m}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\overline{{\\mathbf{A}}}_{i,m}$ denotes the state transition matrix from modality $i$ to $m$ . ", "page_idx": 4}, {"type": "text", "text": "Taking into account the memory overhead and computational efficiency, such modeling increase the number of parameters and computational complexity greatly. We propose a more memory efficient way by conducting summation before state transition, which achieves similar performance and is much more efficient. So our formation of Coupled SSM is: ", "page_idx": 4}, {"type": "equation", "text": "$$\nh_{t}^{m}=\\mathbf{S}_{m}\\sum_{m=1}^{M}h_{t-1}^{m}+\\overline{{\\mathbf{B}}}_{m}x_{t}^{m}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Where we use $\\mathbf{S}_{m}\\in\\mathbb{R}^{B\\times L\\times D\\times N}$ to model the overall state transition after states summation. One minor drawback of this modeling is that we require all modalities to have the same state, which can be easily addressed by using projection layers. ", "page_idx": 4}, {"type": "text", "text": "3.3 Parallelism and Efficiency Analysis ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The main difference between Mamba and traditional recurrent neural networks (RNNs) is that the transition between states does not rely on any activation function. This feature enables it to precalculate intermediate results through the iterative Eq.(3), thereby achieving parallel computing. However, Coupled Mamba adds multi-modal state information based on Mamba, which brings new challenges to the ability to maintain the Mamba parallelization algorithm. In order to solve this problem, we derived a global convolution kernel suitable for Coupled Mamba to ensure that Coupled Mamba can continue to enjoy the advantages brought by Mamba parallel computing, thereby effectively improving the throughput and inference speed of the model. Detailed analysis on throughput and inference speed will be discussed in depth in subsequent sections. ", "page_idx": 4}, {"type": "text", "text": "After introducing the state information of different modals, we learned about the entire state transfer process (6) through 3.2. By deriving Eq.(6), that is, disassembling $h_{t-1}^{m}$ , we can get the following results: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{P}=\\sum_{m=1}^{M}\\mathbf{S}_{m},\\;\\;\\mathbf{U}_{\\mathbf{t}}=\\sum_{m=1}^{M}\\overline{{\\mathbf{B}}}_{m}x_{t}^{m},\\;\\;h_{t}^{m}=\\mathbf{S}_{m}\\sum_{i=0}^{t-1}\\mathbf{P}^{i}\\mathbf{U}_{t-1-i}+\\overline{{\\mathbf{B}}}_{m}x_{t}^{m}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{P}\\in\\mathbb{R}^{B\\times L\\times D\\times N}$ . According to Eq.(7) which can be extended to the state information of each modal, we use the following formula to calculate the output. ", "page_idx": 4}, {"type": "equation", "text": "$$\ny=\\mathbf{C}\\otimes\\sum_{m=1}^{M}h_{t}^{m}=\\mathbf{C}\\otimes\\sum_{i=0}^{t}\\mathbf{U_{i}P^{t-i}}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "From this, the global convolution kernel $\\overline{{\\mathbf{K}}}=\\left(\\mathbf{CP^{0}},\\mathbf{CP^{1}},...,\\mathbf{CP^{t-1}},\\mathbf{CP^{t}}\\right)$ suitable for Coupled Mamba can be obtained. ", "page_idx": 4}, {"type": "text", "text": "The global convolution kernel $\\overline{{\\mathbf{K}}}$ can be used to perform convolution operations on sequence data. In the convolution operation, the calculations of each convolution kernel and the input sub-region are independent of each other, allowing parallel processing of different convolution kernels or input blocks. ", "page_idx": 4}, {"type": "text", "text": "4 Experiment ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To evaluate the effectiveness of our proposed Coupled Mamba in multi-modal fusion, we conduct extensive experiments, with special focus on the multi-modal sentiment analysis (MSA) task as it relies heavily on multi-modal data and is in sequential form. The MSA task aims to predict people\u2019s emotional polarity by fusing audio, text, and visual information. To fully evaluate the advantages of our approach, we conduct extensive experiments on both classification and regression tasks. ", "page_idx": 4}, {"type": "text", "text": "4.1 Datasets and Implementation Details ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Datasets We conduct experiments on five benchmark datasets (CMU-MOSEI, CH-SIMS [24], CHSIMSV2 [25], MM-IMDB and BRCA). CMU-MOSEI dataset is an extension of CMU-MOSI, contains 22856 samples of movie review video clips. In this dataset, 16326 samples are used as the training set, and the remaining 1871 and 4659 samples are used as the validation set and test set respectively. CH-SIMS contains 2281 video clip samples, 1368 samples are used as the training set, and the remaining 456 and 457 samples are used as the validation set and test set respectively. CH-SIMSV2 is an extension of CH-SIMS, which contains 4402 video clip samples, of which 2722 samples are used as the training set, and the remaining 647 and 1034 samples are used as the validation set and test set respectively. For the feature extraction method of the dataset, please refer to the Appendix for more information. The MM-IMDB dataset is used for the movie genre classification task, which classifies movies based on posters and text descriptions. The BRCA dataset includes mRNA expression, DNA methylation, and miRNA expression data for predicting PAM50 subtype classification of breast cancer. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Evaluation metrics For regression tasks, we use the mean absolute error (MAE), which is the average absolute difference between the predicted value and the true value, and the Pearson correlation coefficient (Corr), which measures the degree of deviation of the prediction according to the following formula: The positive/negative and non-negative/negative classification results calculate the binary classification accuracy (Acc-2) and F1-Score, where Acc-2 and F1-Score are more important indicators. For classification tasks, we use Acc-2, Acc-3 and F1-Score (Weighted-F1, Macro-F1, Micro-F1, F1-score3) as evaluation indicators. F1-score3 is the overall performance evaluation of all categories, and F1-score is the performance evaluation of two categories. At the same time, the neutral category is ignored. All experiments were conducted in the same environment. ", "page_idx": 5}, {"type": "text", "text": "Implementation details We use a hidden dimension size of 128, an expansion coefficient of 2, a convolution kernel size of 4, $\\Delta=d s t a t e/8$ as the configuration of each Mamba block, and a layer number of 3 to train our Coupled Mamba. We use Adam to optimize the model and set the learning rate to 0.0005 , weight decay coefficient is 0.0005, epoch is 150, the batch size is set to 1024, 128, 256 on CMU-MOSEI, CH-SIMS, and CH-SIMSV2. L1 loss is used as the loss function for the regression task, and cross entropy is used as the loss function for the classification task. All experiments were conducted on a Linux workstation equipped with a single NVIDIA 32GB V100GPU and a 32-core Intel Xeon CPU. More experimental details can be found in the Appendix. ", "page_idx": 5}, {"type": "text", "text": "4.2 Comparison with the state-of-the-arts ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To fully validate the performance of Coupled Mamba, we conduct extensive comparisons with the following baselines [43, 30, 27, 11, 44, 45] in Table 1. We ran five times and reported the average value. We use bold text to show the best results. Traditionally, models that use aligned corpora tend to perform better [27]. In our experiments, we achieve significant improvements on all evaluation metrics compared to unaligned models. Our unaligned method is able to achieve better results even when compared with aligned models. ", "page_idx": 5}, {"type": "table", "img_path": "UXEo3uNNIX/tmp/60ea966fe83331b51673be151c9f4172d8585320bbdcb433f8c9ba176e73bce0.jpg", "table_caption": ["Table 1: Results on CMU-MOSEI. All models are based on language features extracted by BERT. The one with $^*$ indicates that the model reproduces under the same conditions. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "In multi-modal sentiment analysis tasks, language is a key factor because different languages may have different ways of expressing the same emotion. However, Table 2 shows that our Coupled Mamba shows robustness in both English and Chinese sentiment analysis tasks. Even with unaligned data, our method still achieves highest performance. ", "page_idx": 6}, {"type": "text", "text": "The results of the classification task are given in Table 3 4. It can be seen from the results of the 1 that our proposed fusion method achieves state-of-the-art (SOTA) regardless of whether the data are aligned and from the results of the 4, we find that Coupled Mamba also performs well on Chinese datasets. This is sufficient to demonstrate the effectiveness and robustness of our method. ", "page_idx": 6}, {"type": "table", "img_path": "UXEo3uNNIX/tmp/3a52e777484a9f90d1d3fe3d76b465b5cf3c3f546d1056b30d7dc89065887e1e.jpg", "table_caption": ["Table 2: Results on CH-SIMS (Chinese). All models are based on language features extracted by BERT, and the results are compared on unaligned data. Acc-N represents N-level accuracy. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "UXEo3uNNIX/tmp/26399764b3edca998ac5b6d039c070bb8f4e457c229bea8b6a71c740fc26c0ce.jpg", "table_caption": ["Table 3: Results of classification tasks on CMU-MOSEI. All models are based on language features extracted by BERT, and the results are performed on unaligned data. We ran it five times and report the average results. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "UXEo3uNNIX/tmp/7e755df73cef38d791c15f6c37f742966ed9215a45c7a394da46e34e215c2270.jpg", "table_caption": ["Table 4: Classification task results on CH-SIMS. All models are based on language features extracted by BERT and the results are performed on unaligned data. We ran it five times and report the average results. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Table 7 shows the results on the CH-SIMSV2 dataset, which currently only supports regression tasks. It can be seen from the table that the method we proposed has achieved a huge improvement of $2.3\\%,2,3\\%$ in Acc-2 and F1-Score respectively, indicating the effectiveness of our method. ", "page_idx": 6}, {"type": "text", "text": "The results of Coupled Mamba on BRCA and MM-IMDB datasets are shown in Table 5 6. Whether in multimodal sentiment analysis tasks or in movie genre classification tasks or biology classification tasks, Coupled Mamba can show excellent performance. We expect that coupled mamba can be extended to other multimodal tasks. ", "page_idx": 6}, {"type": "table", "img_path": "UXEo3uNNIX/tmp/a9e738ca755fde8ff13016cad4789d52ad73a18aaf38cf2b7add9b974e0186fc.jpg", "table_caption": ["Table 5: Result on the BRCA benchmark: mR, D, and miR denote mRNA expression, DNA methylation, and miRNA expression data respectively. The best results are in bold. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "UXEo3uNNIX/tmp/9cbe4835da7039a8c02138d70d767fbfee33c351c6f68c819ec555a455a8efbe.jpg", "table_caption": ["Table 6: Result on the MM-IMDB benchmark. I and T denote image and text respectively. The best results are in bold. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "UXEo3uNNIX/tmp/8bbf4c68e15f16b95061dc1ba23f66ed896a281916f46a291f3fe53b04099586.jpg", "table_caption": ["Table 7: Results on CH-SIMSV2, consistent across all experimental settings, using unaligned data. We run it five times and report the average results. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.3 Ablation study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We evaluated the impact of each component in Coupled Mamba to verify the effectiveness of our design. It is worth noting that in order to reduce the impact of randomness on the experimental results, our entire ablation experiment was conducted on the CMU-MOSEI dataset. ", "page_idx": 7}, {"type": "text", "text": "We use the cross-attention mechanism instead of the fusion strategy for comparison. The results are shown in Table 8. Coupled Mamba filters input through a selective mechanism and uses historical modal information to remember and perceive global context, so Coupled Mamba also performs modal fusion well on unaligned data. In contrast, cross-attention is sensitive to misaligned data, and this spatio-temporal inconsistency will lead to insufficient integration between modalities and poor performance. ", "page_idx": 7}, {"type": "table", "img_path": "UXEo3uNNIX/tmp/bb40ddb8f067a99330e43f258d62887e8ab266a04d78c7157cc5591eb04ec6e2.jpg", "table_caption": ["Table 8: All things being equal, replacing Coupled Mamba with Cross attention, we execute it five times and report the average results. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "The number of hidden states and size of $\\Delta$ will have an impact on the results. The size of $\\Delta$ affects SSM\u2019s ability to retain historical information. An increased size of $\\Delta$ focuses more on the present input while disregarding past data, and it also raises the count of hidden states. This escalation in complexity might result in overfitting, higher computational expenses, and may not enhance the model\u2019s actual effectiveness. In order to explore the impact of these parameters on the results, we conducted multiple experiments, and the experimental results showed that the model performance changed under different parameter settings. Detailed results can be found in Tables 9, 10. With $\\Delta=d s t a t e/8$ and $d s t a t e=64$ , Coupled Mamba achieves the best performance than other configurations. ", "page_idx": 8}, {"type": "table", "img_path": "UXEo3uNNIX/tmp/e98b5ee1d50bc951393d1ca4c6347f2017e9c340c030ebc5ad755d1696bfdb93.jpg", "table_caption": ["Table 9: Performance on CMU-MOSEI with different timescale $\\Delta$ "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "UXEo3uNNIX/tmp/3b3aa60592336ee67ff13f8d4ad510ccfbcfc9a765aa483ba3a055dafbe3230f.jpg", "table_caption": ["Table 10: Performance on CMU-MOSEI with different dstate "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "In order to verify the effectiveness of our state coupling, we adopt the splicing fusion, average fusion, and native Mamba blocks for experiments. Average Fusion and Concat Fusion refer to averaging and concatenating the features of different modalities and then sending them to a single Mamba Block for processing. Mamba Fusion refers to using a Mamba Block to process each modality, and finally weighting the results of the three blocks for downstream tasks. The result is shown in Table 11, our Coupled Fusion obtains the best performance than others. Traditional modal fusion methods, such as averaging and concatenation, fail to fully cope with the inherent heterogeneity of multi-modal data. Such methods ignore the different influences that different modalities may have on specific tasks, thereby failing to effectively reveal the intrinsic correlation between multi-modal data. Simple Mamba blocks are not enough to dynamically grasp the semantic relationships. The introduction of state coupling mechanism based on Mamba can make up for this shortcoming and achieve significant improvements in multiple performance indicators. ", "page_idx": 8}, {"type": "table", "img_path": "UXEo3uNNIX/tmp/b70431f176f96d49f4cc8b426d5a5c380e7941bcef4cc3e98465baec32ae753f.jpg", "table_caption": ["Table 11: Comparison of fusion methods "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Compared to Transformers, our approach improves performance by $1\\%\\sim2\\%$ as shown in Table 8 and decreases memory consumption by more than 83. ${\\bf7}\\%$ for sequences length 500 according to Figure 3. When the sequence length increases, the GPU memory usage of the Transformer-based method increases exponentially. In comparison, our method exhibits linear growth. As the sequence grows, the advantages of Coupled Mambas become more apparent. ", "page_idx": 8}, {"type": "text", "text": "As shown in Figure 4, we compared Coupled Mamba and Transformers with five different sequence lengths, and the results show that our inference speed is twice as fast as Transformers under the same sequence length. However, as the sequence length continues to grow, the inference speed of Coupled Mamba will far exceed that of Transformers. ", "page_idx": 8}, {"type": "image", "img_path": "UXEo3uNNIX/tmp/cb166128af3eb2a7a8138879d4018957eec45a4a6b7336f0e7d0b69e6beadf84.jpg", "img_caption": ["Figure 3: GPU usage comparison "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "UXEo3uNNIX/tmp/455b988ab2e574bb3bd075a3db4d859928627342fb29dd1853f4503c7d077340.jpg", "img_caption": ["Figure 4: Inference speed comparison "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "To verify the robustness of our proposed method, we conducted experiments on the CMU-MOSEI dataset with missing data. Specifically, we created a random mask with the same shape as the original tensor, where each element is taken from the Bernoulli distribution $\\mathsf{B}(1\\!-\\!\\mathsf{p})$ . This means that each element has a $\\mathsf{p}\\%$ probability of being 1 (retained) and a $(1\\mathrm{-p})\\%$ probability (i.e., the missing rate (MR)) of being 0 (missing). We then multiplied this random mask with the original tensor so that the regions with masked values of 0 result in missing data in the original tensor. ", "page_idx": 9}, {"type": "table", "img_path": "UXEo3uNNIX/tmp/d65388ee7d81dd079b7c00958379dbed6485a25377017c3a9cba36641cf4c672.jpg", "table_caption": ["Table 12: Performance of Coupled Mamba on CMU-MOSEI dataset when data is missing. Other baselines are from [63] "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "The results are shown in Table 12, and the numbers of other baselines are from [63]. Our method shows the best performance. Note that the left side of / shows Acc-2, while the right side indicates the F1-score. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion and Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we introduce Coupled Mamba, a novel approach to enhance multi-modal fusion by leveraging state evolution chains within state space. Our method integrates intermediate information from various modalities, capturing dynamic multi-modal interactions over time. This addresses challenges in parallel SSM with multiple inputs. Both quantitative and qualitative experiments confirm the effectiveness of Coupled Mamba. Code is available at https://github.com/hustcselwb/coupledmamba. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is supported by the National Natural Science Foundation of China (NSFC No. 62272184 and No. 62402189), the China Postdoctoral Science Foundation under Grant Number GZC20230894, and the China Postdoctoral Science Foundation (Certificate Number: 2024M751012). The computation is completed in the HPC Platform of Huazhong University of Science and Technology. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Kyu Han Koh, Ashok Basawapatna, Vicki Bennett, and Alexander Repenning. Towards the automatic recognition of computational thinking for adaptive visual language learning. In 2010 IEEE symposium on visual languages and human-centric computing, pages 59\u201366. IEEE, 2010.   \n[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in neural information processing systems, 35:23716\u201323736, 2022.   \n[3] Andrew Rouditchenko, Angie Boggust, David Harwath, Brian Chen, Dhiraj Joshi, Samuel Thomas, Kartik Audhkhasi, Hilde Kuehne, Rameswar Panda, Rogerio Feris, et al. Avlnet: Learning audio-visual language representations from instructional videos. arXiv preprint arXiv:2006.09199, 2020.   \n[4] Douwe Kiela, Edouard Grave, Armand Joulin, and Tomas Mikolov. Efficient large-scale multi-modal classification. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018.   \n[5] Weiyao Wang, Du Tran, and Matt Feiszli. What makes training multi-modal classification networks hard? In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12695\u201312705, 2020.   \n[6] Jose Dolz, Karthik Gopinath, Jing Yuan, Herve Lombaert, Christian Desrosiers, and Ismail Ben Ayed. Hyperdense-net: a hyper-densely connected cnn for multi-modal image segmentation. IEEE transactions on medical imaging, 38(5):1116\u20131126, 2018.   \n[7] Di Feng, Christian Haase-Sch\u00fctz, Lars Rosenbaum, Heinz Hertlein, Claudius Glaeser, Fabian Timm, Werner Wiesbeck, and Klaus Dietmayer. Deep multi-modal object detection and semantic segmentation for autonomous driving: Datasets, methods, and challenges. IEEE Transactions on Intelligent Transportation Systems, 22(3):1341\u20131360, 2020.   \n[8] Wenmeng Yu, Hua Xu, Ziqi Yuan, and Jiele Wu. Learning modality-specific representations with self-supervised multi-task learning for multimodal sentiment analysis. Proceedings of the AAAI Conference on Artificial Intelligence, page 10790\u201310797, Sep 2022.   \n[9] Amir Zadeh, Minghai Chen, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. Tensor fusion network for multimodal sentiment analysis. arXiv: Computation and Language,arXiv: Computation and Language, Jul 2017.   \n[10] Amir Zadeh, Paul Pu Liang, Navonil Mazumder, Soujanya Poria, Erik Cambria, and LouisPhilippe Morency. Memory fusion network for multi-view sequential learning. Proceedings of the AAAI Conference on Artificial Intelligence, Jun 2022.   \n[11] Devamanyu Hazarika, Roger Zimmermann, and Soujanya Poria. Misa: Modality-invariant and -specific representations for multimodal sentiment analysis. Cornell University - arXiv,Cornell University - arXiv, May 2020.   \n[12] Chaoqun Wang, Chunyan Xu, Zhen Cui, Ling Zhou, Tong Zhang, Xiaoya Zhang, and Jian Yang. Cross-modal pattern-propagation for rgb-t tracking. In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition, pages 7064\u20137073, 2020.   \n[13] Yikai Wang, Wenbing Huang, Fuchun Sun, Tingyang Xu, Rong Yu, and Junzhou Huang. Deep multimodal fusion by channel exchanging. Cornell University - arXiv,Cornell University - arXiv, Nov 2020.   \n[14] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher Re. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in neural information processing systems, 34:572\u2013585, 2021.   \n[15] S Sundhar Ram, Venugopal V Veeravalli, and Angelia Nedic. Distributed and recursive parameter estimation in parametrized linear state-space models. IEEE Transactions on Automatic Control, 55(2):488\u2013492, 2010.   \n[16] Vincent Verdult, Lennart Ljung, and Michel Verhaegen. Identification of composite local linear state-space models using a projected gradient search. International Journal of Control, 75(16-17):1385\u20131398, 2002.   \n[17] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces.   \n[18] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. Jun 2022.   \n[19] Albert Gu, Karan Goel, Ankit Gupta, and Christopher Re. On the parameterization and initialization of diagonal state space models. Advances in Neural Information Processing Systems, 35:35971\u201335983, 2022.   \n[20] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang. Vision mamba: Efficient visual representation learning with bidirectional state space model. arXiv preprint arXiv:2401.09417, 2024.   \n[21] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019.   \n[22] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. Dec 2023.   \n[23] M. Brand, N. Oliver, and A. Pentland. Coupled hidden markov models for complex action recognition. In Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition, Nov 2002.   \n[24] Wenmeng Yu, Hua Xu, Fanyang Meng, Yilin Zhu, Yixiao Ma, Jiele Wu, Jiyun Zou, and Kaicheng Yang. Ch-sims: A chinese multimodal sentiment analysis dataset with fine-grained annotation of modality. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3718\u20133727, 2020.   \n[25] Yihe Liu, Ziqi Yuan, Huisheng Mao, Zhiyun Liang, Wanqiuyue Yang, Yuanzhe Qiu, Tie Cheng, Xiaoteng Li, Hua Xu, and Kai Gao. Make acoustic and visual cues matter: Ch-sims $\\mathrm{v}2.0$ dataset and av-mixup consistent module. Aug 2022.   \n[26] Pradeep K Atrey, M Anwar Hossain, Abdulmotaleb El Saddik, and Mohan S Kankanhalli. Multimodal fusion for multimedia analysis: a survey. Multimedia systems, 16:345\u2013379, 2010.   \n[27] Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang, J Zico Kolter, Louis-Philippe Morency, and Ruslan Salakhutdinov. Multimodal transformer for unaligned multimodal language sequences. In Proceedings of the conference. Association for computational linguistics. Meeting, volume 2019, page 6558. NIH Public Access, 2019.   \n[28] Karen Simonyan and Andrew Zisserman. Two-stream convolutional networks for action recognition in videos. Cornell University - arXiv,Cornell University - arXiv, Jun 2014.   \n[29] Guangnan Ye, Dong Liu, I-Hong Jhuo, and Shih-Fu Chang. Robust late fusion with rank minimization. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, Jun 2012.   \n[30] Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshminarasimhan, Paul Pu Liang, AmirAli Bagher Zadeh, and Louis-Philippe Morency. Efficient low-rank multimodal fusion with modality-specific factors. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Jan 2018.   \n[31] Dhanesh Ramachandram and Graham W. Taylor. Deep multimodal learning: A survey on recent advances and trends. IEEE Signal Processing Magazine, page 96\u2013108, Nov 2017.   \n[32] Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, and Li Fei-Fei. Large-scale video classification with convolutional neural networks. In 2014 IEEE Conference on Computer Vision and Pattern Recognition, Jun 2014.   \n[33] Valentin Vielzeuf, Alexis Lechervy, Stephane Pateux, and Frederic Jurie. Centralnet: a multilayer approach for multimodal fusion. Cornell University - arXiv,Cornell University - arXiv, Aug 2018.   \n[34] Chiori Hori, Takaaki Hori, Teng-Yok Lee, Kazuhiro Sumi, JohnR. Hershey, and TimK. Marks. Attention-based multimodal fusion for video description. Cornell University - arXiv,Cornell University - arXiv, Jan 2017.   \n[35] Arsha Nagrani, Shan Yang, Anurag Arnab, Aren Jansen, Cordelia Schmid, and Chen Sun. Attention bottlenecks for multimodal fusion. Dec 2021.   \n[36] Yingwei Pan, Ting Yao, Yehao Li, and Tao Mei. X-linear attention networks for image captioning. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Jun 2020.   \n[37] Zihui Xue and Radu Marculescu. Dynamic multimodal fusion.   \n[38] Zongbo Han, Fan Yang, Junzhou Huang, Changqing Zhang, and Jianhua Yao. Multimodal dynamics: Dynamical fusion for trustworthy multimodal classification.   \n[39] JimmyT.H. Smith, Andrew Warrington, and ScottW. Linderman. Simplified state space layers for sequence modeling. Aug 2022.   \n[40] Tri Dao, DanielY. Fu, KhaledK. Saab, ArminW. Thomas, Atri Rudra, and Christopher Re. Hungry hungry hippos: Towards language modeling with state space models. Dec 2022.   \n[41] Shai Fine, Yoram Singer, and Naftali Tishby. The hierarchical hidden markov model: Analysis and applications. Machine learning, 32:41\u201362, 1998.   \n[42] Rachel MacKay Altman. Mixed hidden markov models. Journal of the American Statistical Association, 102(477):201\u2013210, 2007.   \n[43] Yuanzhi Wang, Yong Li, and Zhen Cui. Incomplete multimodality-diffused emotion recognition. Advances in Neural Information Processing Systems, 36, 2024.   \n[44] Yong Li, Yuanzhi Wang, and Zhen Cui. Decoupled multimodal distilling for emotion recognition. Mar 2023.   \n[45] Di Wang, Xutong Guo, Yumin Tian, Jinhui Liu, Lihuo He, and Xuemei Luo. Tetfn: A text enhanced transformer fusion network for multimodal sentiment analysis.   \n[46] Yao-Hung Hubert Tsai, Paul Pu Liang, Amir Zadeh, Louis-Philippe Morency, and Ruslan Salakhutdinov. Learning factorized multimodal representations. In ICLR, 2019.   \n[47] Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee, AmirAli Bagher Zadeh, Chengfeng Mao, Louis-Philippe Morency, and Ehsan Hoque. Integrating multimodal information in large pretrained transformers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2359\u20132369, Online, July 2020. Association for Computational Linguistics.   \n[48] Zhongkai Sun, Prathusha Sarma, William Sethares, and Yingyu Liang. Learning relationships between text, audio, and video via deep canonical correlation for multimodal language analysis. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 8992\u20138999, 2020.   \n[49] Jennifer Williams, Steven Kleinegesse, Ramona Comanescu, and Oana Radu. Recognizing emotions in video using multimodal dnn feature fusion. In Proceedings of Grand Challenge and Workshop on Human Multimodal Language (Challenge-HML), pages 11\u201319. Association for Computational Linguistics, 2018.   \n[50] AmirAli Bagher Zadeh, Paul Pu Liang, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2236\u20132246, 2018.   \n[51] Mark A Van De Wiel, Tonje G Lien, Wina Verlaat, Wessel N van Wieringen, and Saskia M Wilting. Better prediction by use of co-data: adaptive group-regularized ridge regression. Statistics in medicine, 35(3):368\u2013381, 2016.   \n[52] John Arevalo, Thamar Solorio, Manuel Montes-y G\u00f3mez, and Fabio A Gonz\u00e1lez. Gated multimodal units for information fusion. arXiv preprint arXiv:1702.01992, 2017.   \n[53] D Hong, L Gao, N Yokoya, J Yao, J Chanussot, Q Du, and B Zhang More Diverse Means Better. Multimodal deep learning meets remote-sensing imagery classification., 2021, 59. DOI: https://doi. org/10.1109/TGRS, pages 4340\u20134354, 2020.   \n[54] Tongxin Wang, Wei Shao, Zhi Huang, Haixu Tang, Jie Zhang, Zhengming Ding, and Kun Huang. Mogonet integrates multi-omics data using graph convolutional networks allowing patient classification and biomarker identification. Nature communications, 12(1):3445, 2021.   \n[55] Zongbo Han, Changqing Zhang, Huazhu Fu, and Joey Tianyi Zhou. Trusted multi-view classification. In International Conference on Learning Representations, 2020.   \n[56] Zongbo Han, Fan Yang, Junzhou Huang, Changqing Zhang, and Jianhua Yao. Multimodal dynamics: Dynamical fusion for trustworthy multimodal classification. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 20707\u201320717, 2022.   \n[57] Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshminarasimhan, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Efficient low-rank multimodal fusion with modality-specific factors. arXiv preprint arXiv:1806.00064, 2018.   \n[58] Siddhant M Jayakumar, Wojciech M Czarnecki, Jacob Menick, Jonathan Schwarz, Jack Rae, Simon Osindero, Yee Whye Teh, Tim Harley, and Razvan Pascanu. Multiplicative interactions and where to find them. In International conference on learning representations, 2020.   \n[59] Itai Gat, Idan Schwartz, Alexander Schwing, and Tamir Hazan. Removing bias in multi-modal classifiers: Regularization by maximizing functional entropies. Advances in Neural Information Processing Systems, 33:3197\u20133208, 2020.   \n[60] Zhongkai Sun, Prathusha Sarma, William Sethares, and Yingyu Liang. Learning relationships between text, audio, and video via deep canonical correlation for multimodal language analysis. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 8992\u20138999, 2020.   \n[61] Sethuraman Sankaran, David Yang, and Ser-Nam Lim. Multimodal fusion refiner networks. arXiv preprint arXiv:2104.03435, 2021.   \n[62] Zihui Xue and Radu Marculescu. Dynamic multimodal fusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2575\u20132584, 2023.   \n[63] Yuanzhi Wang, Yong Li, and Zhen Cui. Incomplete multimodality-diffused emotion recognition. Advances in Neural Information Processing Systems, 36, 2024.   \n[64] Galen Andrew, Raman Arora, Jeff Bilmes, and Karen Livescu. Deep canonical correlation analysis. In Sanjoy Dasgupta and David McAllester, editors, Proceedings of the 30th International Conference on Machine Learning, volume 28 of Proceedings of Machine Learning Research, pages 1247\u20131255, Atlanta, Georgia, USA, 17\u201319 Jun 2013. PMLR.   \n[65] Weiran Wang, Raman Arora, Karen Livescu, and Jeff Bilmes. On deep multi-view representation learning. In International conference on machine learning, pages 1083\u20131092. PMLR, 2015.   \n[66] Hai Pham, Paul Pu Liang, Thomas Manzini, Louis-Philippe Morency, and Barnab\u00e1s P\u00f3czos. Found in translation: Learning robust joint representations by cyclic translations between modalities. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 6892\u20136899, 2019.   \n[67] Jinming Zhao, Ruichen Li, and Qin Jin. Missing modality imagination network for emotion recognition with uncertain missing modalities. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 2608\u20132618, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "[68] Zheng Lian, Lan Chen, Licai Sun, Bin Liu, and Jianhua Tao. Gcnet: Graph completion network for incomplete multimodal learning in conversation. IEEE Transactions on pattern analysis and machine intelligence, 45(7):8419\u20138432, 2023. ", "page_idx": 14}, {"type": "text", "text": "A Appendix / supplemental material ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The following content is the entire process of pushing to the Coupled Mamba parallelization guarantee. First, we first define the symbols. Assume that the total number of modes is M, $h_{t-1}^{m}$ represents the hidden state at time t-1, where m is any mode, $\\mathbf{A}\\,\\in\\,\\mathbf{R}^{D\\times N}$ represents the state transition matrix, $\\mathbf{B}\\in\\mathbf{R}^{B\\times L\\times N}$ represents the selective matrix obtained by mapping from the current input, $\\mathbf{C}\\in\\mathbf{R}^{B\\times L\\times N}$ is the same as B, where superscript $B$ is the batch size, $L$ is the input time series length, and $N$ is the number of hidden states. ", "page_idx": 15}, {"type": "text", "text": "Since the equations of the state space model and Mamba\u2019s core processes, such as discretization processing, hardware-aware algorithms, and parallel execution theory, have been discussed in the text, they will not be repeated below. ", "page_idx": 15}, {"type": "text", "text": "The core of Coupled Mamba is to introduce multi-modal information while ensuring the parallel computing advantages of Mamba block. After introducing multi-modal information, we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\nh_{t}^{m}=\\overline{{A}}_{m}G_{m}\\sum_{m=1}^{M}h_{t-1}^{m}+\\overline{{B}}_{m}X_{t}^{m}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\overline{{A}}_{m}\\in\\mathbb{R}^{B\\times L\\times D\\times N}$ , $G_{m}\\in\\mathbb{R}^{N\\times N}$ is a coupling matrix, which can be understood as a shared state transition matrix, which transfers the coupling state based on a certain probability based on the comprehensive state at time $t-1$ . By integrating $\\bf{G}_{m}$ into $\\overline{{\\mathbf{A}}}_{\\mathbf{m}}$ , we can get its unified representation $\\mathbf{S_{m}}\\in\\mathbb{R}^{B\\times L\\times D\\times N}$ .Therefore, the above formula can be expressed as ", "page_idx": 15}, {"type": "equation", "text": "$$\nh_{t}^{m}=\\mathbf{S_{m}}\\sum_{m=1}^{M}h_{t-1}^{m}+\\overline{{B}}_{m}X_{t}^{m}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Next, let us derive it step by step starting from time 0, when $t=0$ we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\nh_{0}^{m}=\\overline{{B}}_{m}x_{0}^{m}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "when $t=1$ , we can get: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle h_{t=1}^{1}=S_{1}\\sum_{m=1}^{M}h_{t=0}^{m}+\\overline{{B}}_{1}x_{t=1}^{1}}\\\\ {\\displaystyle h_{t=1}^{2}=S_{2}\\sum_{m=1}^{M}h_{t=0}^{m}+\\overline{{B}}_{2}x_{t=1}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The subscripts of $\\mathbf{S}$ and $\\overline{{\\bf B}}$ represent different modalities, and the superscript of $\\boldsymbol{x}_{t}^{m}$ represents different modalities.Through this recursive formula we can get ", "page_idx": 15}, {"type": "equation", "text": "$$\nh_{t=1}^{M}=S_{M}\\sum_{m=1}^{M}h_{t=0}^{m}+\\overline{{B}}_{M}x_{t=1}^{M}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "In the same way, we bring $h_{t=1}^{1},h_{t=1}^{2},...,h_{t=1}^{M}$ into the formula for calculating each mode $h_{t=2}^{m}$ , we can get: ", "page_idx": 15}, {"type": "equation", "text": "$$\nh_{t=2}^{1}=S_{1}\\sum_{m=1}^{M}h_{t=1}^{m}+\\overline{{B}}_{1}x_{t=2}^{1}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By disassembling $h_{t=1}^{1},h_{t=1}^{2},...,h_{t=1}^{M}$ , we can get: ", "page_idx": 15}, {"type": "equation", "text": "$$\nh_{t=2}^{1}=S_{1}\\left(\\sum_{m=1}^{M}h_{0}^{m}\\sum_{m=1}^{M}S_{m}+\\overline{{B}}_{1}x_{t=1}^{1}+\\overline{{B}}_{2}x_{t=1}^{2}+....+\\overline{{B}}_{M}x_{t=1}^{M}\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{m=1}^{M}h_{0}^{m}=\\overline{{B}}_{1}x_{t=0}^{1}+\\overline{{B}}_{2}x_{t=0}^{2}+\\ldots+\\overline{{B}}_{M}x_{t=0}^{M}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Let $\\overline{{B}}_{1}x_{t}^{1}+\\overline{{B}}_{2}x_{t}^{1}+...+\\overline{{B}}_{M}x_{t}^{1}$ be $U_{t}$ , $\\begin{array}{r}{P=\\sum_{m=1}^{M}S_{m}\\;,}\\end{array}$ $U_{t}$ can be expressed as ", "text_level": 1, "page_idx": 16}, {"type": "equation", "text": "$$\nU_{t}=\\sum_{m=1}^{M}\\overline{{B}}_{m}x_{t}^{m}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Similarly we can get ", "page_idx": 16}, {"type": "equation", "text": "$$\nh_{t=2}^{1}=S_{1}\\left(P U_{0}+U_{1}\\right)+\\overline{{B}}_{1}x_{t=2}^{1}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\nh_{t=3}^{1}=S_{1}\\left(P^{2}U_{0}+P U_{1}+U_{2}\\right)+\\overline{{B}}_{1}x_{t=3}^{1}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, by recursively recursing this formula, we can get ", "page_idx": 16}, {"type": "equation", "text": "$$\nh_{t}^{m}=S_{m}\\sum_{i=0}^{t-1}P^{i}U_{t-1-i}+\\overline{{B}}_{m}x_{t}^{m}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Finally we calculate the output through $\\begin{array}{r}{y=\\mathbf{C}\\otimes\\sum_{m}^{M}h_{t}^{m}}\\end{array}$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\ny=\\mathbf{C}\\otimes\\sum_{i=0}^{t}\\mathbf{U_{i}P^{t-i}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\otimes$ represents the convolution operation, and the convolution kernel is $\\begin{array}{r l}{\\overline{{\\mathbf{K}}}}&{{}=}\\end{array}$ $\\left(\\mathbf{CP^{0}},\\mathbf{CP^{1}},...,\\mathbf{CP^{t-1}},\\mathbf{CP^{t}}\\right)$ . At this point, we have completed the derivation of the entire parallelized calculation. ", "page_idx": 16}, {"type": "text", "text": "In order to fully verify the effectiveness of this research method, we further performed classification experiments on the CMU-MOSI. The experimental results are displayed in Table 13, as follows: ", "page_idx": 16}, {"type": "table", "img_path": "UXEo3uNNIX/tmp/67ae3e5813199d69b01d4633ce33a7e10436f58366771d414f7d919a603280b9.jpg", "table_caption": ["Table 13: Results on the CMU-MOSI dataset for classification task, all results are performed under the same conditions, and the average results are reported after five runs. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "The CH-SIMSV2 [25] dataset is a Chinese data set for multi-modal sentiment analysis and is an extension of the CH-SIMS data set. The dataset contains audio, text, and video clips from different emotion categories, and each clip is labeled with emotional polarity, such as happy, sad, angry, etc. Each emotion category has corresponding speech, text, and video clips, as well as emotion labels associated with them. ", "page_idx": 16}, {"type": "text", "text": "Feature extraction CMU-MOSEI uses the pre-trained BERT model to extract language features and obtains 768-dimensional hidden states as word embeddings. For the visual modality, each video frame is encoded using Facet to represent the presence of a total of 35 facial action units. The acoustic model is processed by COVAREP to obtain 74-dimensional features. CH-SIMS uses pre-trained Chinese BERTbase word embeddings to obtain word vectors from text records, and finally represents each word as a 768-dimensional word vector. Acoustic features at 22050Hz were extracted using the LibROSA speech toolkit with default parameters. A total of 33-dimensional frame-level acoustic features are extracted. Extract aligned faces using MTCNN face detection algorithm. The MultiComp OpenFace2.0 toolkit was then used to extract a collection of 68 facial landmarks, 17 facial action units, head pose, head orientation, and eye gaze. Finally, a total of 709-dimensional frame-level visual features were extracted. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "Exploring Coupled Mamba Layers We investigated the number of layers in Coupled Mamba by performing experiments shown in Table 14. The optimal performance of our Coupled Mamba was observed at $l a y e r=3$ . ", "page_idx": 17}, {"type": "table", "img_path": "UXEo3uNNIX/tmp/1796a0e9687a2e527617165739102f94b3554e52fd310b4e6f5719712977770f.jpg", "table_caption": ["Table 14: Performance on CMU-MOSEI with different layers "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The abstract and introduction of this paper accurately reflect the contribution of the paper, such as solving the problem that Mamba cannot be parallelized when using multi-modal input, and verifying the effectiveness of our method. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: We have discussed the limitations in the end of the paper. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We fully prove the entire derivation process of our algorithm in the appendix.   \nIf you want to know more, you can refer to the appendix. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We put the main experimental results in the main text, but in order to fully prove the effectiveness of our method, we also provide some additional experimental results in the appendix. In order to ensure the reproducibility of the experiment, we have set up an Implementation details chapter in the article, including our experimental environment, batch size, loss function and other details. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in ", "page_idx": 19}, {"type": "text", "text": "some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We submit our experimental code in the supporting material. The environment we use is python 3.10, cuda12.1, torch 2.12. We will indicate how to run the code in the supporting material. For the processing of the data set, we give details in the appendix. We refer to the MMSA library to reproduce the baseline on different data sets, and all experimental environments are the same. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: We give our training details in detail in the implementation details chapter, such as hyperparameter learning rate, batch size settings, and optimizer selection. In order to be more detailed, we also conducted more detailed comparative experiments on different hyperparameters. This can be viewed in the appendix. We have also discussed the splitting of the data set in the data set chapter, including which data sets to use, why, and the sizes of the test set, validation set, and training set. We have explained them in detail. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [No] ", "page_idx": 21}, {"type": "text", "text": "Justification: The data we report in the paper is the average data of five report runs, so the error bars are not explicitly written in the paper. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We indicate the type of graphics card, CPU type, memory and other information we use in the implementation details, and also provide a visualization of memory usage in the appendix. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We have read the NeurIPS Code of Ethics and strongly agree with it, and we fully followed the code during our research. We have not violated any guidelines and we hope everyone will follow them. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. \u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. ", "page_idx": 21}, {"type": "text", "text": "\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: Yes, we have discussed the positive and negative impacts of our work. Generally speaking, our research has a positive effect on society and can promote human-computer interaction and better enable computers to serve humans in life. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: Our research does not design data sets with high risk of misuse, and our model is also very safe. For this reason, we do not elaborate on the corresponding protection measures in the article. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Any assets we use have the permission and approval of their original owners, and we fully understand and agree with the protection of any intellectual property rights. We used a CC-BY 4.0 Asset license and for the datasets we used, we cited them in the References section. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: We proposed the Coupled Mamba model, and we have fully explained its detailed architecture in the text. If our paper can be accepted, we will publish the model and code strictly in accordance with the response standards. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our research does not involve any research or experiments with human subjects, and we use publicly available data sets for experiments. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 23}, {"type": "text", "text": "\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our research did not involve any human subjects. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]