[{"figure_path": "U3hQoqgQDJ/figures/figures_0_1.jpg", "caption": "Figure 1: The proposed FIND interface is generalizable to tasks that span granularity (pixel to image) and modality (vision to language). The retrieval space for this figure is the COCO validation set.", "description": "This figure illustrates the FIND (INterface for Foundation models' embeDdings) interface's versatility in handling various tasks across different granularities (from pixel-level to image-level) and modalities (vision and language).  It shows examples of different tasks, including interleave segmentation, long-context segmentation, generic segmentation, interleave grounding, and interleave retrieval, showcasing how FIND adapts to these tasks without needing to tune the underlying foundation model weights.  The retrieval space used in the examples is from the COCO validation set.", "section": "Abstract"}, {"figure_path": "U3hQoqgQDJ/figures/figures_1_1.jpg", "caption": "Figure 2: (1) The concept of interfacing foundation models embedding, the black arrow means active attached modules and the gray arrow means the option that it can switch to. On the right, we show the difference of Multimodal and Interleave (2.a) in the context of embeddings matching; (2.b) in the context of embeddings interaction for reasoning and generation.", "description": "This figure illustrates the FIND interface's design. The left panel (1) shows how FIND interfaces with various foundation models for vision and language tasks. The black arrows indicate active modules, while gray arrows represent optional ones. The right panel (2) contrasts the multimodal and interleaved approaches. Panel (2a) compares embedding matching, while (2b) examines embedding interactions for reasoning and generation in both approaches.", "section": "Introduction"}, {"figure_path": "U3hQoqgQDJ/figures/figures_3_1.jpg", "caption": "Figure 1: The proposed FIND interface is generalizable to tasks that span granularity (pixel to image) and modality (vision to language). The retrieval space for this figure is the COCO validation set.", "description": "This figure illustrates the FIND (Foundation models' embeddings INterface) framework's versatility in handling various tasks across different granularities (from pixel to image level) and modalities (vision and language).  The input can be an image, text, or a combination of both, and the output depends on the task.  Examples shown include image segmentation (labeling different regions of the image), grounding (connecting textual descriptions to specific image regions), and retrieval (finding images that match a given text description). The figure showcases the generalizability of FIND, as it utilizes the same architecture for all tasks. The COCO validation set serves as the retrieval space for the example shown in the figure.", "section": "Abstract"}, {"figure_path": "U3hQoqgQDJ/figures/figures_4_1.jpg", "caption": "Figure 1: The proposed FIND interface is generalizable to tasks that span granularity (pixel to image) and modality (vision to language). The retrieval space for this figure is the COCO validation set.", "description": "This figure demonstrates the FIND interface, a generalized interface for aligning foundation models' embeddings. It shows how FIND handles various tasks across different granularities (pixel to image) and modalities (vision to language) using a unified architecture. The interface is shown working on examples of image segmentation, object grounding, and image retrieval.  The retrieval space used for the examples in the figure is the COCO validation set.", "section": "Abstract"}, {"figure_path": "U3hQoqgQDJ/figures/figures_5_1.jpg", "caption": "Figure 4: (a) Preliminaries on the terminology of prompts and queries. (b) FIND approach pipeline. The shape of different polygons represents different embedding types, and the color (vision, language) of the polygons represents input modality. (c) Detailed architecture of the FIND Interface.", "description": "This figure illustrates the FIND interface's architecture and workflow. (a) shows the terminology used for prompts (input embeddings) and queries (learnable embeddings). (b) provides a high-level overview of the FIND pipeline, including input processing, embedding sampling, the FIND interface itself, and final task-specific outputs. (c) delves into the detailed architecture of the FIND interface, highlighting the content and conditional attention mechanisms and their roles in aggregating and exchanging information between prompts and queries before generating final outputs (retrieval, grounding, segmentation). Different shapes and colors represent different embedding types and modalities.", "section": "3.2 FIND Approach"}, {"figure_path": "U3hQoqgQDJ/figures/figures_5_2.jpg", "caption": "Figure 1: The proposed FIND interface is generalizable to tasks that span granularity (pixel to image) and modality (vision to language). The retrieval space for this figure is the COCO validation set.", "description": "This figure shows examples of different tasks that the FIND interface can handle.  It demonstrates the interface's ability to work across various granularities (from pixel-level segmentation to image-level retrieval) and modalities (vision and language). The tasks shown include interleave segmentation, long-context segmentation, generic segmentation, interleave grounding, and interleave retrieval.  The examples highlight the interface's flexibility and its ability to handle complex, interleaved information. The COCO validation set was used as the retrieval space for the image examples.", "section": "Abstract"}, {"figure_path": "U3hQoqgQDJ/figures/figures_8_1.jpg", "caption": "Figure 1: The proposed FIND interface is generalizable to tasks that span granularity (pixel to image) and modality (vision to language). The retrieval space for this figure is the COCO validation set.", "description": "This figure shows examples of the FIND interface's applications across different tasks and granularities.  It demonstrates FIND's ability to perform interleave segmentation, generic segmentation, long-context segmentation, interleave grounding, and interleave retrieval. Each task showcases the interface's ability to handle different levels of granularity (from pixel-level segmentation to image-level retrieval) and modalities (vision and language).  The figure highlights the versatility of FIND in handling various visual understanding tasks, all within a unified framework. The retrieval space used for image examples is the COCO validation set.", "section": "Abstract"}, {"figure_path": "U3hQoqgQDJ/figures/figures_8_2.jpg", "caption": "Figure 1: The proposed FIND interface is generalizable to tasks that span granularity (pixel to image) and modality (vision to language). The retrieval space for this figure is the COCO validation set.", "description": "This figure shows examples of different tasks enabled by the FIND interface, highlighting its versatility in handling various granularities (from pixel-level segmentation to image-level retrieval) and modalities (vision and language).  Each example demonstrates a different task: interleave segmentation, long-context segmentation, generic segmentation, interleave grounding, interleave retrieval, and interactive segmentation.  The common element across all tasks is that they leverage the embeddings from foundation models in a unified way via the FIND interface.  The image examples are all taken from the COCO validation set, further emphasizing the interface's broad applicability.", "section": "Abstract"}, {"figure_path": "U3hQoqgQDJ/figures/figures_8_3.jpg", "caption": "Figure 4: (a) Preliminaries on the terminology of prompts and queries. (b) FIND approach pipeline. The shape of different polygons represents different embedding types, and the color (vision, language) of the polygons represents input modality. (c) FIND Interface.", "description": "This figure shows three parts: (a) illustrates the terminology of prompts and queries used in the FIND model. (b) provides a visual representation of the FIND approach pipeline, highlighting the input, embedding sampler, FIND interface, and output. Different shapes represent different embedding types, while colors indicate the modality (vision or language). (c) shows the detailed architecture of the FIND interface, including the embedding sampler, prompts, queries, and attention mechanisms.", "section": "Method"}]