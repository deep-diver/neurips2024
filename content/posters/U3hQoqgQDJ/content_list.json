[{"type": "text", "text": "Interfacing Foundation Models\u2019 Embeddings ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xueyan $Z\\mathrm{ou}^{*\\S\\bigstar}$ , Linjie $\\mathrm{Li^{*}}^{\\sharp}$ , Jianfeng Wang\u266f, Jianwei Yang\u266f, Mingyu Ding\u2021, Junyi Wei\u00a7 Zhengyuan Yang\u266f, Feng $\\mathrm{Li^{\\dagger}}$ , Hao Zhang\u2020, Shilong $\\mathrm{Liu}^{\\&}$ , Arul Aravinthan\u00a7, Yong Jae Lee\u00a7\u00b6, Lijuan Wang\u266f\u00b6 \u00a7 UW-Madison \u266fMicrosoft \u2021 UC Berkeley \u2020 HKUST & Tsinghua University $^\\mathparagraph$ Equal Advisory Contribution $\\spadesuit$ Main Technical Contribution \u2217Equal Contribution https://github.com/UX-Decoder/FIND, https://github.com/UX-Decoder/vlcore ", "page_idx": 0}, {"type": "image", "img_path": "U3hQoqgQDJ/tmp/5fa11d687300cefadf0d03df79c8c32f99c54190705e276f8bfc68148c8b41ae.jpg", "img_caption": ["Figure 1: The proposed FIND interface is generalizable to tasks that span granularity (pixel to image) and modality (vision to language). The retrieval space for this figure is the COCO validation set. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Foundation models possess strong capabilities in reasoning and memorizing across modalities. To further unleash the power of foundation models, we present FIND, a generalized interface for aligning foundation models\u2019 embeddings with unified image and dataset-level understanding spanning modality and granularity. As shown in Fig. 1, a lightweight transformer interface without tuning any foundation model weights is enough for segmentation, grounding, and retrieval in an interleaved manner. The proposed interface has the following favorable attributes: (1) Generalizable. It applies to various tasks spanning retrieval, segmentation, etc., under the same architecture and weights. (2) Interleavable. With the benefit of multi-task multi-modal training, the proposed interface creates an interleaved shared embedding space. (3) Extendable. The proposed interface is adaptive to new tasks, and new models. In light of the interleaved embedding space, we introduce FIND-Bench, which introduces new training and evaluation annotations to the COCO dataset for interleaved segmentation and retrieval. We are the first work aligning foundations models\u2019 embeddings for interleave understanding. Meanwhile, our approach achieves state-of-the-art performance on FIND-Bench and competitive performance on standard retrieval and segmentation settings. ", "page_idx": 0}, {"type": "image", "img_path": "U3hQoqgQDJ/tmp/6002c9d513ef9c55aa2e7f2e4027405ec86e00cffe5f3fd381954bb248d2901e.jpg", "img_caption": ["", ""], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 2: (1) The concept of interfacing foundation models embedding, the black arrow means active attached modules and the gray arrow means the option that it can switch to. On the right, we show the difference of Multimodal and Interleave (2.a) in the context of embeddings matching; (2.b) in the context of embeddings interaction for reasoning and generation. ", "page_idx": 1}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "With the exhilarating progress in foundation models across the vision and language domains, such as GPT4(V) (30), DALLE-3 (31), SAM (19), and LLaMA (38), etc., we have reached a stage where deep learning models achieve remarkable performances on both vision and language domains (5; 22). Specifically, models like GPT-4(V) (30) have showcased human-level perception and reasoning skills (46). ", "page_idx": 1}, {"type": "text", "text": "Despite their impressive capabilities in information memorization, processing, and reasoning, these models tend to be specialized for specific output types. However, their output types are limited to language for GPT, images for DALLE, masks for SAM, etc. In this work, we aim to leverage the privileged properties of foundation models\u2019 embeddings to expand their output space (e.g., extend to pixel-level outputs), unlocking their potential for interleaved understanding and reasoning. ", "page_idx": 1}, {"type": "text", "text": "To accomplish this, we introduce an INterface for Foundation models\u2019 embeDdings (FIND), which utilizes the pre-trained foundational model embeddings to jointly handle downstream tasks of varying granularities (from pixel to image) in an interleaved manner. As illustrated in Fig.2.1, the FIND interface processes embeddings from vision and language foundation models, and outputs segmentation, grounding, and retrieval results. ", "page_idx": 1}, {"type": "text", "text": "As all vision-language tasks are trained uniformly in FIND, an interleaved shared embedding space is created where vision and language references can be interchanged and augmented. For example, in Fig.2.2, during mapping an interleaved representation loosens the single-modality constraint on the source and target domain. And during reasoning, interleaved sequences enhance information exchange between vision and language compared to multimodal sequences. ", "page_idx": 1}, {"type": "text", "text": "To effectively align and evaluate the interleaved embedding space, we construct a new dataset named FIND-Bench. This dataset uses COCO images and includes new annotations for integrated grounding and segmentation. These annotations are generated by GPT-4, which, despite not processing visual input, can directly link specific image segments and annotation IDs with generated descriptions (e.g., <id>(the golden retriever) ...). This unique capability enables the creation of training and evaluation datasets for retrieval and grounding in an interleaved context. ", "page_idx": 1}, {"type": "text", "text": "In summary, we claim the following contributions: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We introduce the FIND interface that is is generalizable, flexible, and extendable to various downstream tasks and foundation models.   \n\u2022 Through the effective training scheme of FIND, an interleaved shared embedding space is created interfacing foundation models.   \n\u2022 We propose a new Benchmark, FIND-Bench, which includes new training and evaluation ground truths for interleave segmentation and retrieval.   \n\u2022 Our model achieves SoTA performance on interleave retrieval and grounding and shows better or comparable performance on generic, interactive, grounded segmentation and image-text retrieval. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Foundation Models. Recent years have seen a speedy evolution of foundation models in diverse areas such as computer vision (47), natural language processing (39; 10; 4; 30), and their interactions (1; ", "page_idx": 1}, {"type": "text", "text": "23; 44). For example, GPT-3 (4) heralds breakthroughs in natural language understanding and generation tasks, As a vision foundation model, Florence (47; 42) can be easily adapted for various computer vision tasks, such as classification, retrieval, object detection, etc.Flamingo (1) bridges powerful pre-trained vision-only and language-only models by token fusion with cross-attention. BLIP-2 (23) proposes an efficient pretraining strategy that bootstraps vision-language pre-training with a lightweight Q-Former in two stages. Different from previous multi-modal approaches, such as Flamingo (1), LLaVA (26) and Q-Former (BLIP-2) (23) that feed the vision foundation model output into a language decoder and use the LLM as an interpreter, our goal is to interface foundation model embeddings so that LLMs and vision models can be unified in the embedding space. ", "page_idx": 2}, {"type": "text", "text": "Interleaved Image-Text Understanding. Previous works have explored interleaved visual understanding in the context of visual question answering, visual dialogue, image captioning, and interleaved image retrieval (20; 13; 1). In addition, recent works (48) explore contextual detection that associates phrases with visual content in a sentence. We notice that these earlier works, though reveal interleaved capabilities for image understanding, lack an evaluation benchmark, as well as a complete training dataset. (51; 21; 2) propose a new benchmark on interleaved generation and understanding of image and document level, while there is no benchmark available for the interleaved tasks between interactive image parts and phrases. To this end, we introduce the interleaved segmentation and interleaved retrieval tasks with our carefully designed benchmark FIND-Bench, which we believe to be essential for the field. ", "page_idx": 2}, {"type": "text", "text": "Image Understanding. Vision Transformers (16; 37; 40; 36; 41; 12; 15; 49; 33; 34) have dominated a wide range of key image understanding tasks, such as image retrieval, detection, and segmentation. Some multimodal methods (7; 24; 50) have shown good performance for retrieval tasks. On the other hand, open-vocabulary segmentation methods have recently drawn much attention, including generic segmentation (6; 53; 11), interactive segmentation (14; 19) that separates objects by actively integrating user inputs, and grounded segmentation (53; 52) that grounds object segments from language descriptions. We notice that there is currently no available work that achieves image-level retrieval, pixel-level segmentation, and interleaved vision-language understanding in a single model. In this work, we propose FIND as a unified interface that can support all the above tasks, while maintaining good performance, and further enabling two new tasks of interleaved segmentation and interleaved retrieval. We unify these tasks by interfacing foundation models\u2019 embeddings. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Foundation models such as CLIP (32), SAM (19), LLaMA (38), etc. can process vision or language inputs for reasoning, understanding, and generation. The embeddings generated by these models contain rich and structured information (35; 3), making them extremely well-suited for understanding tasks. Aligned with the Platonic Representation Hypothesis (17), we believe foundation models can easily communicate with each other. Therefore, we designed the FIND interface to project vision and language embeddings from foundation models into a unified space. The created space enhances both multimodal and interleaved understanding. ", "page_idx": 2}, {"type": "text", "text": "Since no prior benchmark exists for interleave understanding, we believe it is meaningful to formally define the interleave retrieval and segmentation problems and create a dataset for benchmarking them. ", "page_idx": 2}, {"type": "text", "text": "3.1 FIND Benchmark ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our new benchmark supports two tasks: interleave retrieval and interleave grounding. It evaluates both dataset-level and image-level interleave alignment, focusing on reasoning and matching capabilities. Additionally, we created training and evaluation datasets to further enhance interleave understanding. ", "page_idx": 2}, {"type": "text", "text": "3.1.1 Task Definition ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Interleave Retrieval 1. An interleave entry $(E)$ consists of a sequence of images (I), texts (T), and connections (C), and can be represented as $\\dot{E}=\\langle N_{1},N_{2},\\dots,N_{n}^{\\bar{\\ i}}\\mid N_{i}\\in\\{I,\\bar{T_{}}C\\}\\rangle$ , where $\\langle\\cdot\\rangle$ is an ordered sequence. The bottom part of the Table. ?? clearly illustrates an example of an interleave entry. We denote the source domain $(\\mathcal{D}_{s})$ of interleave retrieval as $\\mathcal{D}_{s}=\\{E_{1},E_{2},\\ldots,E_{n}\\}$ , as shown in Fig. 3.1 (Left), and the target domain $(\\mathcal{D}_{t})$ as $\\mathcal{D}_{t}=\\{I_{1},I_{2},\\ldots,I_{n}\\}$ , as shown in Fig. 3.1 (Right). The task of interleave retrieval is to find the closest entry $I_{*}\\in\\mathcal{D}_{t}$ for each $E\\in\\mathcal{D}_{s}$ , excluding itself. Formally, we define this as $\\forall E\\in\\mathcal{D}_{s},\\quad I_{*}=\\arg\\operatorname*{max}_{I\\in\\mathcal{D}_{t},I\\notin E}\\mathbf{sim}(E,I).$ . ", "page_idx": 2}, {"type": "image", "img_path": "U3hQoqgQDJ/tmp/9656a630d5e698f824cef67c5dcfd6cae77f89494490ba863b0b1cea1f201656.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Table 1: Pseudo code for Data Engine. We show the pipeline to create the FIND-Bench from data preparation, text prompting using GPT4, visual prompting with SEEM to integrated result. ", "page_idx": 3}, {"type": "text", "text": "Interleave Grounding 2. An image contains a sequence of objects or segments $(O)$ represented as $I=\\{O_{1},O_{2},\\dots,O_{n}\\}$ . We provide an example of objects in the bakery image in Fig. 3.2 upper part. These objects form the target domain $\\bar{D_{t}^{\\prime}}=I=\\bar{\\{}O_{1},O_{2},...,O_{n}\\}$ for interleave grounding. Unlike interleave retrieval, where interleave entries constitute the source domain, interleave grounding focuses on each component of the interleave entry, with the entities $(N)$ in the interleave entry forming the source domain. Specifically, $\\mathcal{D}_{s}=\\{N_{1},N_{2},\\{\\mathrm{~\\cdot~}.\\mathrm{~.~},N_{n}\\ |\\ N_{i}\\in\\{I,T\\}\\}\\subseteq E$ . We show an example of interleave entry decomposition in the lower part of Fig. 3.2. The task of interleave grounding is to find the closest entry $O_{*}\\in\\mathcal{D}_{t}$ for each $N\\in\\mathcal{D}_{s}$ , excluding itself. Formally, we define this as $\\begin{array}{r}{\\forall N\\in\\mathcal{D}_{s},\\quad O_{*}=\\arg\\operatorname*{max}_{O\\in\\mathcal{D}_{t},O\\notin N}\\mathbf{s}\\mathbf{im}(N,O).}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "3.1.2 Data Engine ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We reuse the images and ground truth annotations from the COCO dataset to create FIND-Bench. In the first part of Table. 1, we demonstrate the input data used to in-context learning for GPT-4. In addition to the COCO ground truth, we generate pseudo-image descriptions using VLM models, such as LLaVA (26), to enrich the information. In the second part of Table. 1, we present the prompt template for our data engine. This template generates the text part for the interleaved captions in part 4 of Table. 1, providing language descriptions associated with annotation IDs. The segments corresponding to these IDs are highlighted in the same color in the example image shown in Table. 1. ", "page_idx": 3}, {"type": "text", "text": "As stated in Sec. 3.1.1, the source and target components are exclusive. We leverage the strong visual understanding capabilities of SEEM (53) to find replacements for the visual components in the entry. The retrieved and replaced visual components are shown in part 4 of Table. 1, with the exact segment highlighted in the same color as the corresponding reference text. For example, <the playing field> is associated with the COCO annotation ID [3171126] and a similar playing field (marked in blue) in another image. In this way, the data engine can generate comprehensive interleaved descriptions for each image in the COCO dataset. This is sufficient to build $\\mathcal{D}_{s}$ and $\\mathcal{D}_{t}$ for the interleave retrieval and grounding tasks introduced in Sec. 3.1.1. ", "page_idx": 3}, {"type": "image", "img_path": "U3hQoqgQDJ/tmp/3a4f3a69fcab3cff48e77b3f33351f6a93fd434ad7f03afb5cf9813f08633e00.jpg", "img_caption": ["Figure 3: Task Unification for retrieval, grounding, and segmentation. The corresponding components are labeled with the same color or connected with a line or arrow. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "3.2 FIND Approach ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "With benchmarks introduced in Sec. 3.1 to evaluate the model\u2019s interleaved visual understanding capability, we now present our approach for interfacing foundation models\u2019 embeddings on multimodal and interleave understanding. We begin with the preliminaries on task unification and terminology. ", "page_idx": 4}, {"type": "text", "text": "3.2.1 Preliminary ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Task Unification. In this work, we focus on retrieval, grounding, and segmentation in both multimodal and interleaved manners. In Fig. 3, we demonstrate four example tasks: interleave retrieval, interleave grounding, interactive segmentation, and generic segmentation. From an abstract perspective, we can regard all visual understanding tasks as the problem of matching candidates from the source domain to the target domain. Formally, we define the source domain as $\\mathcal{D}_{s}$ and the target domain as $\\mathcal{D}_{t}$ . Example elements in $\\mathcal{D}_{s}$ or $\\mathcal{D}_{t}$ includes interleaved entry $E$ , an image $I$ , an object or segment $O$ , texts $T$ . For each visual understanding task $\\mathcal{U}(\\mathcal{D}_{s},\\mathcal{D}_{t})$ , the goal is to find the closest $Y\\in\\mathcal{D}_{t}$ for each $X\\in\\mathcal{D}_{s}$ . Formally we write: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\forall X\\in\\mathcal{D}_{s},\\quad Y^{*}=\\arg\\operatorname*{max}_{Y\\in\\mathcal{D}_{t}}\\mathbf{sim}(X,Y)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{\\deltaX}$ , and $\\mathbf{\\deltaY}$ are base element of $\\mathcal{D}_{s}$ , and $\\mathcal{D}_{t}$ respectively, and $\\mathbf{sim}(X,Y)$ denotes the similarity between $X$ and $Y$ . For example, in generic segmentation (Fig. 3.4), $\\mathcal{D}_{s}$ is the set of all objects (segments) in the image: $\\mathcal{D}_{s}=\\{\\bar{O}_{1},...\\,\\bar{,}O_{n_{s}}\\}$ , and $\\mathcal{D}_{t}$ is the set of category names: $\\mathcal{D}_{t}=\\{T_{1},\\dot{.}\\cdot.\\,,T_{n}\\}$ . For each object $O$ in $\\mathcal{D}_{s}$ , we will find the corresponding category $T\\in\\mathcal{D}_{t}$ . ", "page_idx": 4}, {"type": "text", "text": "Terminology. Here we will introduce important model terminology, including prompts $(P)$ and queries $(Q)$ . Our model supports three kinds of inputs: vision (I), language (T), and interleaved vision-language (E). The vision and language foundation models predict the embeddings for those inputs. As shown in Fig. 4.1, by sampling the embeddings, we obtain vision prompts $(P_{I})$ , language prompts $(P_{T})$ , and interleave prompts $(P_{E})$ . Additionally, trainable queries initialized with random parameters will accumulate information from the prompts. For example, in generic segmentation, object queries $(Q_{O})$ gather information from visual prompts. Interestingly, queries just act like \u201cbuckets\" accumulating \u201cwater\" (prompts) in the FIND interface, as shown in Fig. 4.1. ", "page_idx": 4}, {"type": "text", "text": "3.2.2 Model Pipeline ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Our model is designed to interface with a pair of arbitrary vision and language foundation models. Prompts and Queries Preparation. Given image (I), text (T), and interleave (E) inputs, the vision encoder $(\\mathbf{F}_{v})$ and language encoder $(\\mathbf{F}_{l})$ will encode these inputs to sequences of embeddings $M$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\nM_{I}={\\bf F}_{v}(I),\\;\\;\\;M_{T}={\\bf F}_{l}(T),\\;\\;\\;M_{E}=\\{{\\bf F}_{v},{\\bf F}_{l}\\}(E)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where, $M\\,\\in\\,\\mathbb{R}^{n\\times d}$ , and $n,d$ is the embedding number and dimension respectively. Similar to SEEM (53), we use an embedding sampler to sample customized prompts for downstream tasks. Example sampling strategies include downsampling, ROI pooling for the region, and rearrangement of embeddings for interleave prompt. The sampling procedure does not alter the embedding distribution. After sampling, we obtain $\\{P_{E},P_{T},P_{I},\\dots\\}=\\mathbf{Emb\\_Sample}(M_{I},M_{T},M_{E})$ . Additionally, the embedding sampler is responsible for sampling queries $(\\{Q_{E},Q_{T},Q_{I},\\ldots\\})$ from the pool of learnable queries. We allow duplication in the sampling procedure of learnable queries. These queries and prompts are the inputs of FIND interface. Technically, the embedding sampler is usually an interpolation or grid sample layer in PyTorch. ", "page_idx": 4}, {"type": "image", "img_path": "U3hQoqgQDJ/tmp/e5c91a7af69a241490e38149ea3e2703d94af7a40446abbb41db65982df1b8fb.jpg", "img_caption": ["Figure 4: (a) Preliminaries on the terminology of prompts and queries. (b) FIND approach pipeline. The shape of different polygons represents different embedding types, and the color (vision, language) of the polygons represents input modality. (c) Detailed architecture of the FIND Interface. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "FIND Interface. The FIND interface primarily consists of two operations: content attention ${\\bf A}_{t}$ and conditional attention $\\mathbf{A}_{d}$ , as shown in Fig. 4.3. Content attention allows queries to accumulate information from the corresponding prompts, while conditional attention enables prompts and queries to reason internally (e.g. self-attention on object queries to avoid duplication). With initial prompts $\\mathbf{P}^{0}=\\{P_{E}^{0},P_{T}^{0},\\bar{P_{I}^{0}},...\\}$ , and initial learnable queries ${\\bf Q}^{0}=\\{Q_{E}^{0},\\dot{Q}_{T}^{0},Q_{I}^{0},...\\}$ , content attention and conditional attention are formally defined as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{Q}^{l+1}=\\mathbf{A}_{t}(\\mathbf{P}^{l},\\mathbf{Q}^{l};[\\mathbf{P}^{l}\\to\\mathbf{Q}^{l}]),\\;\\;\\;\\mathbf{Q}^{l+1},\\mathbf{P}^{l+1}=\\mathbf{A}_{d}(\\mathbf{P}^{l},\\mathbf{Q}^{l};[\\mathcal{S}^{l}\\to\\mathbf{Q}^{l}],[\\mathbf{P}^{l}\\to\\mathbf{P}^{l}])\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where ${\\cal S}^{l}\\,\\subseteq\\,\\{{\\bf P}^{l},{\\bf Q}^{l}\\}$ is a subset of queries and prompts, $\\rightarrow$ represents the attention mask. For example, $[\\mathbf{P}\\rightarrow\\mathbf{Q}]$ means that $\\mathbf{Q}$ is able to attend $\\mathbf{P}$ during the attention. In this way, prompts act as the information source, and queries act as the bucket. In Fig. 4.2, we unfold the prompts and queries for some tasks supported by FIND interface. ", "page_idx": 5}, {"type": "text", "text": "Projection The outputs of the $F I N D$ interface are a sequence of queries: $\\begin{array}{r l}{\\mathbf{Q}^{L}}&{{}=}\\end{array}$ $\\{Q_{O}^{\\check{L}},Q_{T}^{L},Q_{I}^{L},Q_{E}^{L},..\\ \\}$ . We then project the queries using linear layers, $\\mathbf{M}\\mathbf{L}\\mathbf{P}_{s}$ and $\\mathbf{M}\\mathbf{L}\\mathbf{P}_{p}$ , for semantic and pixel projection, respectively. The semantic and pixel queries are computed as $Q^{s}\\,=\\,\\mathbf{M}\\mathbf{L}\\mathbf{P}_{s}(\\mathbf{\\dot{Q}}^{L})\\ \\mathbf{\\dot{\\in}}\\ \\mathbf{\\dot{R}}^{n_{t}\\times d}$ and $\\mathbf{\\dot{\\mathit{Q}}}^{p}\\;=\\;\\mathbf{\\dot{M}}\\mathbf{L}\\mathbf{P}_{p}(\\mathbf{Q}^{L})\\;\\in\\;\\mathbb{R}^{n_{t}\\times d}$ , where $n_{t}$ is the total instance number, and $d$ is the embedding dimension. The semantic outputs are used for retrieval, category mapping, etc., while the pixel outputs are used for mask prediction. ", "page_idx": 5}, {"type": "text", "text": "Task Head With the projected queries, as illustrated Sec. 3.2.1 each understanding task can be represented as a similarity mapping procedure. Formally, segmentation result (Mask) can be computed given initial image embedding $\\bar{M}_{I}\\in\\mathbb{R}^{n_{p}\\times d}$ , where $n_{p}$ is the pixel number. The similarity scores (Score) can be computed directly from $Q^{s}$ . The outputs for each task is a subset of $\\{{\\mathrm{Mask,}}{\\mathrm{Score}}\\}$ . ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{\\mathbf{M}_{\\mathsf{a s k}}}=Q^{p}\\times\\dot{M_{I}^{\\mathsf{T}}}\\in\\mathbb{R}^{n_{t}\\times n_{p}},\\quad\\bar{\\mathbf{S}}\\mathrm{core}=Q^{s}\\times Q^{s\\top}\\in\\mathbb{R}^{n_{t}\\times n_{t}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Loss FIND is trained with a linear combination of losses for panoptic segmentation, grounded segmentation, interactive segmentation, image-text retrieval, interleave retrieval with visual entities from the same image, and interleave grounding. We demonstrate the loss details in the Appendix. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Datasets. We use COCO (25) as our main training and evaluation dataset, which spans diverse annotation types. We make use of the annotations from COCO-panoptic, Ref-COCO (45; 28; 29), COCO-Karpathy (18), and the new datasets generated with the data engine in FIND-Bench. We generate two sets of new annotations, including COCO-Entity and COCO-Paragraph, the detailed statistics are shown in the table below: ", "page_idx": 5}, {"type": "image", "img_path": "U3hQoqgQDJ/tmp/c0a732bbd9fbcaf1ffb7f1c7f6507fe6daea5ece898c978192a49cb3396f53c2.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Settings. We benchmark our method on three different model sizes: Tiny (FocalNet), Base (Davit-d3), and Large (Davit-d3). The vision backbone is fixed and reuses the X-Decoder pre-trained weights ", "page_idx": 5}, {"type": "table", "img_path": "U3hQoqgQDJ/tmp/f071d7ef564d804b1bf3e39c4b422990b2efb347854b4cd9b32342cd59d074e8.jpg", "table_caption": [], "table_footnote": ["Table 2: Benchmark on Generalizable multi-modal understanding tasks with one model architecture joint training for all. \\*Unlike Mask2Former and SEEM, FIND is not trained with a deformable vision encoder. We report un-ensemble/ensemble results for X-Decoder, and the finetuned/pre-trained results for blip2. Note that we compute the ITC score for blip2 instead of ITM. "], "page_idx": 6}, {"type": "text", "text": "unless specified as SAM. The language backbone is a fixed LLaMA-7B, unless specified as UniCL.   \nDuring training, we train the FIND-Interface jointly on all the tasks unless specified. ", "page_idx": 6}, {"type": "text", "text": "Metrics. We evaluate all the tasks with their standard evaluation metrics. For the newly proposed interleave retrieval, we use $\\operatorname{IR}\\!\\left(\\!\\omega5\\right)$ and $\\operatorname{IR}\\@10$ (Interleave-to-image Retrieval accuracy at rank 5/10). For interleave grounding, we evaluate based on cIoU (pixel-wise IoU), and mIoU (image-wise IoU) between the predicted interleave masks and the ground truth masks. ", "page_idx": 6}, {"type": "text", "text": "Baselines. We use ImageBind (13), FROMAGe (20), BLIP2 (23) as baselines for the interleave retrieval task; Grounding-SAM (27), SEEM (53) for interleave grounding. We claim to make every effort to design the baseline evaluation protocol to achieve the best possible performance. ", "page_idx": 6}, {"type": "text", "text": "4.1 Main Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In the main experiments, we focus on evaluating FIND on Generalizable, Interleavable, and Extendable capabilities as claimed in the abstract. ", "page_idx": 6}, {"type": "text", "text": "(1) Generalizable to Segmentation, Grounding, and Retrieval. Table 2 compares FIND with strong baselines on generic segmentation tasks including panoptic segmentation, instance segmentation, and semantic segmentation. In addition, we demonstrate the segmentation capability in both referring segmentation (RefCOCO- $\\mathrm{\\Delta}^{\\mathrm{g}}$ : one sentence is associated with one instance) and grounded segmentation (COCO-Entity and COCO-Paragraph: one sentence is associated with multiple instances) settings. Moreover, we also benchmark FIND\u2019s performance in image-text retrieval on three different ground truth types on COCO, where the average sentence length for the splits (Karpathy, Entity, and Paragraph) gradually increases. Below are the takeaways: ", "page_idx": 6}, {"type": "text", "text": "The instance segmentation result stands out: Our approach with a large vision encoder outperforms similar models like Mask2Former, X-Decoder, and SEEM, achieving a performance 2.2 points higher than Mask2Former (L), which additionally uses deformable convolution. Notably, the segmentation training data is identical for both Mask2Former and FIND. The performance gain likely results from our unified segmentation and grounding pipeline, which mutually beneftis from the semantic ground truth of each domain. ", "page_idx": 6}, {"type": "text", "text": "Mutual beneftis of grounded and referring segmentation: In FIND, we unify grounded and referring segmentation using queries and prompts. As shown in Table 2, our model achieves state-of-theart performance on COCO-Entity and COCO-Paragraph and outperforms strong baselines on the Ref-COCOg dataset. ", "page_idx": 6}, {"type": "text", "text": "Interactive segmentation performance is preserved in the unified settings. Unlike SEEM which is only trained on image-only tasks, FIND is trained also on image-text tasks, such as image-text retrieval. With the smart design of queries, prompts, and attention mechanisms, training interactive segmentation and image-text retrieval does not interfere. Thus, it enables our approach to achieve competitive performances (i.e. FIND 88.5/89.5/77.4 vs. SEEM 88.5/89.6/76.5). ", "page_idx": 6}, {"type": "text", "text": "Less optimal image-text retrieval results: The sub-optimal performance in image-text retrieval is due to batch size during fine-tuning. Pilot experiments with X-Decoder showed that different resolutions (e.g., 1024 for images and 224 for language) do not generalize well across tasks. Thus, FIND is trained with the same resolution for all tasks. In Table 2, models are either $384\\mathrm{x}384$ with batch size 384 or $1024\\!\\!\\times\\!1024$ with batch size 192 for all tasks. Other tables show results with a $640\\mathrm{x}640$ training resolution and a 192 batch size. ", "page_idx": 6}, {"type": "table", "img_path": "U3hQoqgQDJ/tmp/6fc47f733f1b0db464891c623295e16df36ae4d47c57e448ba27376154df510e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "U3hQoqgQDJ/tmp/4f32e72b189ae3058dbd5f0c937998b0c3b304a12b00d89ddba3f3a6492f100e.jpg", "table_caption": ["Table 3: Benchmark on interleaved understanding with the jointly trained model on all tasks with one set of weights. We evaluate interleave grounding, retrieval, and generic segmentation. ", "Table 4: Ablation study on different foundation model architectures. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "(2) Interleavable on vision and language modalities. In Table. 3, we evaluate FIND on the interleaved dataset- and image-level understanding tasks in FIND-Bench. In the columns of COCOEntity and COCO-Paragraph, we replace the text entity with visual reference on 0.5 probability, unlike Table. 2 the columns are purely evaluated on language-based data. ", "page_idx": 7}, {"type": "text", "text": "Interleaved Segmentation: We build an interleaved segmentation baseline using the SEEM model. Instead of formulating the grounding task in an interleaved format that SEEM doesn\u2019t support, we simply separately infer visual, and text entities using the interactive or grounding function of SEEM. As shown in Table 3, FIND outperforms SEEM on interleave segmentation with around $+8$ points on both COCO-Entity and COCO-Paragraph under cIoU metrics. ", "page_idx": 7}, {"type": "text", "text": "Interleaved Retrieval: We also explore cross-image interleave retrieval on FIND. Since the interleaved reference objects are from the same validation set, $\\operatorname{IR}\\!\\left(\\!\\omega\\!\\,1\\right)$ is not meaningful, so we report IR $@5$ and $\\operatorname{IR}\\@10$ in this setting. For ImageBind and BLIP-2, we use ensemble scores of texts, sentences, and images. Following FROMAGe\u2019s settings for interleaved image-text retrieval, our performance is significantly higher than the baselines, demonstrating the effectiveness of our interleaved shared embedding space. ", "page_idx": 7}, {"type": "text", "text": "Generic Segmentation: Beyond classic evaluations using class names or fixed indices, we replace categories with class descriptions (long descriptions) or visual prompts (average features for object queries for each class). Leveraging LLMs, FIND excels in description-based segmentation, beneftiing from smoother representations and better handling of long contexts. We also demonstrate FIND\u2019s effectiveness in the visual context setting. ", "page_idx": 7}, {"type": "text", "text": "(3) Extendable to arbitrary foundation models and tasks. In the main experiments, we use X-Decoder as the vision encoder, and LLaMA as the language encoder, which shows convincing performance on all the tasks. X-Decoder has been trained to pair up vision and language embeddings, however, SAM is only trained on segmentation data without any semantic meaning. Thus, we use SAM as an ablation vision foundation model, to study how important is vision encoder trained with semantic data. For the language encoder, we adopt UniCL which has the same size as Bert to study the difference between a standard language encoder, and an LLM encoder. As shown in Table 4, UniCL and LLaMA usually have very similar performance with X-Decoder as vision encoder, except that LLaMA is extremely effective on long description reasoning. Although the performance of SAM is much worse than its counterpart X-Decoder on semantic understanding after training the interface, our approach also shows that without any modification to SAM, it applies to semantic understanding tasks on generic, grounded segmentation, and image-text retrieval. ", "page_idx": 7}, {"type": "text", "text": "4.2 Ablation Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We ablate our approach from two perspectives: (1) What is the effectiveness of each task in the unified pipeline? (2) The effectiveness of using intermediate layers of the LLM representation. ", "page_idx": 7}, {"type": "text", "text": "Independent task effectiveness: We assess task effectiveness by gradually removing tasks in Table 5. Removing image-text retrieval significantly reduces interleave retrieval performance. Further removing the grounding task decreases entity-based grounding performance. Since interleave grounding is related to interactive segmentation, removing it also reduces interleave segmentation performance. Finally, training only panoptic segmentation yields similar performance to other settings, indicating the unified interface\u2019s consistency with basic task training. ", "page_idx": 7}, {"type": "table", "img_path": "U3hQoqgQDJ/tmp/9a72f6fa01a35ecb9abbd9566233bb5d6fe76f09172daaf0a34ac0a79c09ae34.jpg", "table_caption": ["Table 5: Ablate on each training task and language encoder feature level. ", "CLIP 3D Feature Field "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Varying the feature embeddings layer for LLM: LLMs process language tokens, with embeddings near input and output layers being less semantic. We hypothesize that intermediate layers align better with vision embeddings. Table 5 shows performance across tasks using emebddings from layers $^-1$ (output) to -30 (input). Layer -12 emebddings perform best, while top and bottom layers perform worse for image-text retrieval on COCO-Karparthy splits. Thus, we use layer -12 emebddings for LLaMA throughout the paper. ", "page_idx": 8}, {"type": "text", "text": "4.3 Demonstration Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Interleave Album Search. The queries in our FIND approach support linear complexity interleave album search. Given an image, interleave, or text input, our model can retrieve and segment all the photos in the album. Below, we show an example using the COCO validation set as the search space. ", "page_idx": 8}, {"type": "text", "text": "is standing on the rock under the trees. ", "page_idx": 8}, {"type": "image", "img_path": "U3hQoqgQDJ/tmp/005882285207e2feeca49f2d18cf03663a05049bb00141e814cf50c4ce4b2bd6.jpg", "img_caption": ["Interleave Video Localization. We can formulate the video frame localization problem as an image-text retrieval task. This allows us to reason about and identify corresponding objects based on given instructions, as illustrated below. We believe FIND is useful for robot navigation. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "U3hQoqgQDJ/tmp/072e963f85767f73af40d71b6751c115378987cafe5fb90f105c8ece79df07a6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "3D Feature Field. Foundation model embeddings are utilized to create a 3D feature field for robot manipulation, localization, and reasoning. We believe that the interleave embedding space, with its pixel-level understanding capabilities, has significant potential in the 3D feature field. Below, we compare a scene trained with FIND embeddings versus CLIP embeddings. ", "page_idx": 8}, {"type": "image", "img_path": "U3hQoqgQDJ/tmp/f3553d673386f6dc4d40c6ee27587e71edcbccd5791cbe929a3e5599a26bf7c8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Conclusions and Future Work. This work introduces the FIND Interface, a generalized interface for aligning foundation models\u2019 embeddings, along with the FIND Benchmark for training and evaluation. In Sec. 4.3, we demonstrate potential applications such as interleave album search, video localization, and 3D feature fields. These examples clearly illustrate the potential of our model for personalized foundation models and robotics. ", "page_idx": 8}, {"type": "text", "text": "Limitations. Our model is only trained and evaluated on the COCO dataset. With the limitation of data quantity, we mention that the method may not be well adapted to the in-the-wild settings. ", "page_idx": 8}, {"type": "text", "text": "Broader Impact. Our proposed approach inherits ethical or social issues (e.g. bias amplification, privacy risks, energy consumption) of foundational models. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgement. This work was supported in part by NSF CAREER IIS2150012, NASA 80NSSC21K0295, the Institute of Information and communications Technology Planning and Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2022-0-00871, Development of AI Autonomy and Knowledge Enhancement for AI Agent Collaboration). This research project has benefitted from the Microsoft Accelerate Foundation Models Research (AFMR) grant program. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Alayrac, J.B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et al.: Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems 35, 23716\u201323736 (2022) [2] An, J., Yang, Z., Li, L., Wang, J., Lin, K., Liu, Z., Wang, L., Luo, J.: Openleaf: Open-domain interleaved image-text generation and evaluation. arXiv preprint arXiv:2310.07749 (2023) [3] BehnamGhader, P., Adlakha, V., Mosbach, M., Bahdanau, D., Chapados, N., Reddy, S.: Llm2vec: Large language models are secretly powerful text encoders. arXiv preprint arXiv:2404.05961 (2024)   \n[4] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language models are few-shot learners. Advances in neural information processing systems 33, 1877\u20131901 (2020) [5] Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y.T., Li, Y., Lundberg, S., et al.: Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712 (2023) [6] Chen, L.C., Papandreou, G., Schroff, F., Adam, H.: Rethinking atrous convolution for semantic image segmentation. arXiv preprint arXiv:1706.05587 (2017)   \n[7] Chen, Y., Li, L., Yu, L., Kholy, A.E., Ahmed, F., Gan, Z., Cheng, Y., Liu, J.: UNITER: universal image-text representation learning. In: ECCV. vol. 12375, pp. 104\u2013120 (2020) [8] Cheng, B., Misra, I., Schwing, A.G., Kirillov, A., Girdhar, R.: Masked-attention mask transformer for universal image segmentation. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 1290\u20131299 (2022) [9] Cherti, M., Beaumont, R., Wightman, R., Wortsman, M., Ilharco, G., Gordon, C., Schuhmann, C., Schmidt, L., Jitsev, J.: Reproducible scaling laws for contrastive language-image learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 2818\u20132829 (2023)   \n[10] Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirectional transformers for language understanding. In: NAACL-HLT (1) (2019)   \n[11] Ding, M., Lian, X., Yang, L., Wang, P., Jin, X., Lu, Z., Luo, P.: Hr-nas: Searching efficient highresolution neural architectures with lightweight transformers. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 2982\u20132992 (2021)   \n[12] Ding, M., Xiao, B., Codella, N., Luo, P., Wang, J., Yuan, L.: Davit: Dual attention vision transformers. In: European Conference on Computer Vision. pp. 74\u201392. Springer (2022)   \n[13] Girdhar, R., El-Nouby, A., Liu, Z., Singh, M., Alwala, K.V., Joulin, A., Misra, I.: Imagebind: One embedding space to bind them all. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 15180\u201315190 (2023)   \n[14] Grady, L.: Random walks for image segmentation. IEEE transactions on pattern analysis and machine intelligence 28(11), 1768\u20131783 (2006)   \n[15] Graham, B., El-Nouby, A., Touvron, H., Stock, P., Joulin, A., J\u00e9gou, H., Douze, M.: Levit: a vision transformer in convnet\u2019s clothing for faster inference. In: ICCV. pp. 12259\u201312269 (2021)   \n[16] Heo, B., Yun, S., Han, D., Chun, S., Choe, J., Oh, S.J.: Rethinking spatial dimensions of vision transformers. In: ICCV. pp. 11936\u201311945 (2021)   \n[17] Huh, M., Cheung, B., Wang, T., Isola, P.: The platonic representation hypothesis. arXiv preprint arXiv:2405.07987 (2024)   \n[18] Karpathy, A., Fei-Fei, L.: Deep visual-semantic alignments for generating image descriptions. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 3128\u2013 3137 (2015)   \n[19] Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A.C., Lo, W.Y., et al.: Segment anything. arXiv preprint arXiv:2304.02643 (2023)   \n[20] Koh, J.Y., Salakhutdinov, R., Fried, D.: Grounding language models to images for multimodal inputs and outputs (2023)   \n[21] Lauren\u00e7on, H., Saulnier, L., Tronchon, L., Bekman, S., Singh, A., Lozhkov, A., Wang, T., Karamcheti, S., Rush, A.M., Kiela, D., et al.: Obelics: An open web-scale filtered dataset of interleaved image-text documents. In: Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track (2023)   \n[22] Li, C., Gan, Z., Yang, Z., Yang, J., Li, L., Wang, L., Gao, J.: Multimodal foundation models: From specialists to general-purpose assistants. arXiv preprint arXiv:2309.10020 1, 2 (2023)   \n[23] Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597 (2023)   \n[24] Li, X., Yin, X., Li, C., Zhang, P., Hu, X., Zhang, L., Wang, L., Hu, H., Dong, L., Wei, F., Choi, Y., Gao, J.: Oscar: Object-semantics aligned pre-training for vision-language tasks. In: ECCV. pp. 121\u2013137 (2020)   \n[25] Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll\u00e1r, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13. pp. 740\u2013755. Springer (2014)   \n[26] Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. arXiv preprint arXiv:2304.08485 (2023)   \n[27] Liu, S., Zeng, Z., Ren, T., Li, F., Zhang, H., Yang, J., Li, C., Yang, J., Su, H., Zhu, J., et al.: Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499 (2023)   \n[28] Mao, J., Huang, J., Toshev, A., Camburu, O., Yuille, A.L., Murphy, K.: Generation and comprehension of unambiguous object descriptions. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 11\u201320 (2016)   \n[29] Nagaraja, V.K., Morariu, V.I., Davis, L.S.: Modeling context between objects for referring expression understanding. In: Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11\u201314, 2016, Proceedings, Part IV 14. pp. 792\u2013807. Springer (2016)   \n[30] OpenAI: Gpt-4 technical report. Tech. rep., OpenAI (2023)   \n[31] OpenAI: Improving image generation with better captions. Tech. rep., OpenAI (2023)   \n[32] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International conference on machine learning. pp. 8748\u20138763. PMLR (2021)   \n[33] Riquelme, C., Puigcerver, J., Mustafa, B., Neumann, M., Jenatton, R., Susano Pinto, A., Keysers, D., Houlsby, N.: Scaling vision with sparse mixture of experts. NeurIPS 34 (2021)   \n[34] Ryoo, M.S., Piergiovanni, A., Arnab, A., Dehghani, M., Angelova, A.: Tokenlearner: What can 8 learned tokens do for images and videos? arXiv: Computer Vision and Pattern Recognition (2021)   \n[35] Saunshi, N., Plevrakis, O., Arora, S., Khodak, M., Khandeparkar, H.: A theoretical analysis of contrastive unsupervised representation learning. In: International Conference on Machine Learning. pp. 5628\u20135637. PMLR (2019)   \n[36] Srinivas, A., Lin, T.Y., Parmar, N., Shlens, J., Abbeel, P., Vaswani, A.: Bottleneck transformers for visual recognition. In: CVPR. pp. 16519\u201316529 (2021)   \n[37] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., J\u00e9gou, H.: Training data-efficient image transformers & distillation through attention. In: ICML. pp. 10347\u201310357. PMLR (2021)   \n[38] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T., Rozi\u00e8re, B., Goyal, N., Hambro, E., Azhar, F., et al.: Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023)   \n[39] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, \u0141., Polosukhin, I.: Attention is all you need. In: NeurIPS (2017)   \n[40] Wang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L.: Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In: ICCV (2021)   \n[41] Wu, K., Peng, H., Chen, M., Fu, J., Chao, H.: Rethinking and improving relative position encoding for vision transformer. In: ICCV. pp. 10033\u201310041 (2021)   \n[42] Xiao, B., Wu, H., Xu, W., Dai, X., Hu, H., Lu, Y., Zeng, M., Liu, C., Yuan, L.: Florence-2: Advancing a unified representation for a variety of vision tasks. arXiv preprint arXiv:2311.06242 (2023)   \n[43] Yang, J., Li, C., Zhang, P., Xiao, B., Liu, C., Yuan, L., Gao, J.: Unified contrastive learning in image-text-label space. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 19163\u201319173 (2022)   \n[44] Yao, L., Huang, R., Hou, L., Lu, G., Niu, M., Xu, H., Liang, X., Li, Z., Jiang, X., Xu, C.: FILIP: fine-grained interactive language-image pre-training. In: ICLR (2022)   \n[45] Yu, L., Poirson, P., Yang, S., Berg, A.C., Berg, T.L.: Modeling context in referring expressions. In: Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14. pp. 69\u201385. Springer (2016)   \n[46] Yu, W., Yang, Z., Li, L., Wang, J., Lin, K., Liu, Z., Wang, X., Wang, L.: Mm-vet: Evaluating large multimodal models for integrated capabilities (2023)   \n[47] Yuan, L., Chen, D., Chen, Y.L., Codella, N., Dai, X., Gao, J., Hu, H., Huang, X., Li, B., Li, C., et al.: Florence: A new foundation model for computer vision. arXiv preprint arXiv:2111.11432 (2021)   \n[48] Zang, Y., Li, W., Han, J., Zhou, K., Loy, C.C.: Contextual object detection with multimodal large language models. arXiv preprint arXiv:2305.18279 (2023)   \n[49] Zhai, X., Kolesnikov, A., Houlsby, N., Beyer, L.: Scaling vision transformers. arXiv: Computer Vision and Pattern Recognition (2021)   \n[50] Zhang, P., Li, X., Hu, X., Yang, J., Zhang, L., Wang, L., Choi, Y., Gao, J.: Vinvl: Making visual representations matter in vision-language models. arXiv preprint arXiv:2101.00529 (2021)   \n[51] Zhu, W., Hessel, J., Awadalla, A., Gadre, S.Y., Dodge, J., Fang, A., Yu, Y., Schmidt, L., Wang, W.Y., Choi, Y.: Multimodal c4: An open, billion-scale corpus of images interleaved with text. arXiv preprint arXiv:2304.06939 (2023)   \n[52] Zou, X., Dou, Z.Y., Yang, J., Gan, Z., Li, L., Li, C., Dai, X., Behl, H., Wang, J., Yuan, L., et al.: Generalized decoding for pixel, image, and language. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 15116\u201315127 (2023)   \n[53] Zou, X., Yang, J., Zhang, H., Li, F., Li, L., Gao, J., Lee, Y.J.: Segment everything everywhere all at once. arXiv preprint arXiv:2304.06718 (2023) ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Method Details ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Task Specific Interface ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In Section 3.2.2, we provided a comprehensive overview of the general pipeline of FIND. Here, we focus on the task-specific interface design choices. The pipeline comprises three main components: (1) Embeddings, which include prompts and queries as introduced in Section 3.2.2. Prompts are multimodal embeddings containing relevant information, while queries are learnable embeddings that aggregate information from the prompts. For instance, for image prompts (a.k.a visual features of an image) we denote them as p.image. (2) Operators, which incorporate both content and condition attention, and are responsible for information accumulation and exchange. The arrows $\\leftarrow,\\leftrightarrow$ denote the attention direction. (3) Projection, which maps the queries into semantic or pixel space. Table. 6 below shows details of all task-specific design choices for the FIND interface, including embeddings, operators, and projection. ", "page_idx": 12}, {"type": "table", "img_path": "U3hQoqgQDJ/tmp/62304e0245016f9e25da199c80436872f37ca4d10ebaf3865a8b2fbfef0040c1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "Table 6: Task specific FIND Interface. We define each task under the prototype of the FIND interface that enables a shared embedding space, and a unified and flexible architecture for future tasks. Where $p,\\,q$ stands for prompts, queries, and arrows stand for attention direction. The colors red, blue, and olive are the embeddings of vision, language, and interleave modality. ", "page_idx": 12}, {"type": "text", "text": "A.2 Loss Functions ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "The training tasks include panoptic segmentation, interactive segmentation, grounded segmentation, image-text retrieval, interleave retrieval with visual entities from the same image, and interleave grounding. Losses for each task are standardized loss functions including $\\mathcal{L}_{\\mathrm{BCE}}$ for binary crossentropy loss, $\\mathcal{L}_{\\mathrm{CE}}$ for cross-entropy loss, $\\mathcal{L}_{\\mathrm{DICE}}$ for dice loss, $\\mathcal{L}_{\\mathrm{C}}$ for contrastive loss. Below is the loss function for $F I N D$ . ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\~~~~\\mathcal{L}=\\alpha_{p}\\mathcal{L}_{\\mathrm{CE-pano}}+\\beta_{p}\\mathcal{L}_{\\mathrm{BCE-pano}}+\\gamma_{p}\\mathcal{L}_{\\mathrm{DICE-pano}}+\\alpha_{g}\\mathcal{L}_{\\mathrm{CE-grd}}+\\beta_{g}\\mathcal{L}_{\\mathrm{BCE-grd}}+\\gamma_{g}\\mathcal{L}_{\\mathrm{DICE-grd}}}\\\\ &{+\\alpha_{i}\\mathcal{L}_{\\mathrm{CE-iseg}}+\\beta_{i}\\mathcal{L}_{\\mathrm{BCE-iseg}}+\\gamma_{i}\\mathcal{L}_{\\mathrm{DICE-iseg}}+\\theta\\mathcal{L}_{\\mathrm{VLC-imgtex}}+\\phi\\mathcal{L}_{\\mathrm{IC,intr}}+\\alpha_{i g}\\mathcal{L}_{\\mathrm{CE-intg}}}\\\\ &{+\\beta_{i g}\\mathcal{L}_{\\mathrm{DICE-intg}}+\\gamma_{i g}\\mathcal{L}_{\\mathrm{ICE-intg}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where, pano denotes panoptic segmentation, grd denotes grounding, iseg denotes interactive segmentation, imgtextr denotes image-text retrieval, intr denotes interleave retrieval, intg denotes interleave grounding. For more implementation details on the loss function, please refer to the code. ", "page_idx": 12}, {"type": "text", "text": "A.3 Case Study: Interleave Grounding ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "As shown in Table. 6, the input embeddings of interleave groundings for FIND interface contain prompts and queries. Image prompts are the image features with a shape of $[h\\times w,512]$ , while interleaved prompts are visual-language tokens of sentences like \u201cA baseball player in a black and white uniform crouches on near holding a taking a break.\" with a shape of $[l,512]$ $(l$ is the token length). Entity queries are learnable embeddings for object proposals of the image, shaped [100, 512]. Interleave queries are learnable embeddings for gathering information from the interleave prompts, shaped $[n,512]$ , where $\\mathbf{n}$ is the total number of meaningful entities. For example, the interleave sentence shown above has entity numbers of 4. Specifically the entity contains [\u2018A baseball player in a black and white uniform, ], which is the total number of $[T]$ and $[I]$ referencing to Fig. 3. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "After getting a full sense of the input embeddings of interleave grounding, including p.image, p.interleave, q.entity, q.interleave. We then introduce the operation on top of those embeddings. As introduced in Sec. 3.2.2, the operations contain content attention ${\\bf A}_{t}$ and conditional attention $\\mathbf{A}_{d}$ . Formally we could write the attention mechanism for the specific input embeddings of interleave grounding with the following equations: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathsf{q}\\,.\\,^{*}\\,,\\;\\;\\mathsf{p}\\,.\\,^{*}=\\mathbf{A}_{t}\\big(\\,[\\mathsf{q}\\,.\\,^{*}\\,,\\;\\;\\mathsf{p}\\,.\\,^{*}]\\,;\\,[\\mathsf{q}\\,.\\,^{*}\\,,\\;\\;\\mathsf{p}\\,.\\,^{*}]\\,;\\,\\mathbf{M}_{d}\\big)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where A(query; key $=$ value; M) is the attention operator with query, key, value and mask. Given the order p.image, p.interleave, q.entity, q.interleave, the content and condition attention masks are written below: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{M}_{t}=\\left[\\begin{array}{l l l l}{F}&{F}&{F}&{F}\\\\ {F}&{F}&{F}&{F}\\\\ {\\mathbf{T}}&{F}&{F}&{F}\\\\ {F}&{\\mathbf{T}}&{F}&{F}\\end{array}\\right]\\mathbf{M}_{d}=\\left[\\begin{array}{l l l l}{F}&{F}&{F}&{F}\\\\ {F}&{\\mathbf{T}}&{F}&{F}\\\\ {\\mathbf{T}}&{F}&{\\mathbf{T}}&{F}\\\\ {F}&{\\mathbf{T}}&{F}&{\\mathbf{T}}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The index of matrix coordinates follows the input order. After the input prompts and queries are fully communicated, we will compute the projected pixel and semantic embeddings for output in the following manner: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{q}.\\;\\mathbf{entity}^{s},\\mathbf{q}.\\;\\mathbf{inter1eave}^{s}=\\mathbf{MLP}_{s}(\\mathbf{q}.\\;\\mathbf{entity},\\;\\;\\mathbf{q}.\\;\\mathbf{inter1eave})}\\\\ &{\\;\\mathbf{q}.\\;\\mathbf{entity}^{p}=\\mathbf{MLP}_{p}(\\mathbf{q}.\\;\\mathbf{entity})}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where s,p are semantic and pixel projection respectively. This way, queries are projected into semantic and pixel space to compute the final output. The dimension of q.entitys and ${\\mathsf{q}}\\,{\\mathsf{e n t i t y}}^{p}$ are both [100, 512]. In addition, q.interleaves has dimension $[n,512]$ where $\\mathbf{n}$ is the entity number. With those projected queries and image features $M_{I}$ in the pixel projection space with shape $[h,w,512]$ . We could get the final output mask associated with each entity with the following operation: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Index}=\\arg\\underset{\\mathrm{dim}=0}{\\operatorname*{max}}\\mathbf{sim}(\\mathtt{q}.\\cdot\\mathrm{entity}^{s},\\mathtt{q}.\\cdot\\mathrm{inter1eave}^{s})}\\\\ &{Q_{p}^{*}=\\mathtt{q}.\\cdot\\mathrm{entity}^{p}\\big[\\mathrm{Index}\\big]}\\\\ &{\\mathtt{M a s k}=Q_{p}^{*}\\times M_{I}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "In this way, we associate the grounding entity with the desired mask segment of the image, as shown in the top right figure in Table. 1. ", "page_idx": 13}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: The claims generalizable, prototypable, extendable, and interleavable are clearly demonstrated in the method and experiment section. The contribution mentioned is also proved in method and experiment section. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 14}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Justification: We have a paragraph on limitation. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 14}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 14}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 15}, {"type": "text", "text": "Justification: Our paper is an application-based paper, and does not have a theoretical result. Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 15}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: Our code is public available. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 15}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: We provide open access to the code with training details in the supplementary material. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 16}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: We specify them in the training and inference code public available. Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 16}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 16}, {"type": "text", "text": "Answer: [No] ", "page_idx": 16}, {"type": "text", "text": "Justification: Our model is evaluated on a large number of datasets with enough data points.   \nThe number is empirically very stable. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: We have provided the details in the supplementary material. Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 17}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: We follow the code of ethics. Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 17}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: We have discussed the border impacts in the last paragraph. Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: Our model is on the side of understanding instead of generation, so that safeguards is not applied. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 18}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We have properly cited and acknowledged the prior works. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 18}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 19}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The new benchmark is documented in the paper on the code base. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 19}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: Our work does not need crowdsourcing. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 19}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: Our work does not contain human subjects. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 19}]