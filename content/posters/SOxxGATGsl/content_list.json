[{"type": "text", "text": "Efficient Algorithms for Lipschitz Bandits ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Lipschitz bandits is a fundamental framework used to model sequential decision  \n2 making problems with large, structured action spaces. This framework has been   \n3 applied in various areas. Previous algorithms, such as the Zooming algorithm,   \n4 achieve near-optimal regret with $O(\\bar{T^{2}})$ time complexity and ${\\cal O}(T)$ arms stored   \n5 in memory, where $T$ denotes the size of the time horizons. However, in practical   \n6 scenarios, learners may face limitations regarding the storage of a large number   \n7 of arms in memory. In this paper, we explore the bounded memory stochastic   \n8 Lipschitz bandits problem, where the algorithm is limited to storing only a limited   \n9 number of arms at any given time horizon. We propose algorithms that achieve   \n10 near-optimal regret with ${\\cal O}(T)$ time complexity and $O(1)$ arms stored, both of   \n11 which are almost optimal and state-of-the-art. Moreover, our numerical results   \n12 demonstrate the efficiency of these algorithms. ", "page_idx": 0}, {"type": "text", "text": "13 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "14 Multi-armed Bandits (MAB) is a powerful framework used to balance the exploration-exploitation   \n15 trade-off in online decision-making problems. Within this framework, a learner sequentially selects   \n16 arms (actions, decisions, or items) and learns from the associated feedback, aiming to maximize the   \n17 expected total reward within finite time horizons. Some well-known algorithms, such as UCB1 and   \n18 Exp3, have achieved near-optimal regret by storing records of all arms in memory. In many bandit   \n19 problems, algorithms can access information about the similarity between arms, suggesting that arms   \n20 with similar characteristics often yield similar expected rewards. The Lipschitz bandits framework   \n21 is a prominent variant that addresses decision-making in large, structured action spaces, where the   \n22 expected reward of the arms follows a Lipschitz function. For instance, in recommendation systems,   \nthe arms correspond to items represented by feature vectors. Items with similar feature vectors are   \n24 likely to result in similar outcomes or conversions.   \n25 Recently, a series of works in the field of online learning have been dedicated to managing scenarios   \n26 with large action spaces while maintaining sub-linear memory usage. This direction is driven by the   \n27 need to effectively tackle extensive real-world applications such as recommendation systems, search   \n28 ranking, and crowdsourcing. In these applications, arms correspond to items, solutions, or models,   \n29 which leads to significant memory demands. For instance, in recommendation systems, the learner   \n30 faces the challenge of choosing from millions of items, like music and movies, to present to users,   \n31 especially in scenarios characterized by limited space or an infinite number of arms. Therefore, the   \n32 development of memory-efficient algorithms has become crucial for these applications. In recent   \n33 years, substantial efforts have been made to address the challenge of bandits with limited memory   \n34 (Assadi & Wang, 2020; Jin et al., 2021; Maiti et al., 2020; Agarwal et al., 2022; Assadi & Wang,   \n35 2022; Wang, 2023; Assadi & Wang, 2023a). However, previous research has mainly focused on   \n36 unstructured action spaces, often overlooking the fact that in these applications, arms with similar   \n37 characteristics tend to yield similar expected rewards.   \n38 One general approach to solving Lipschitz bandits is through discretizing the structured action space.   \n39 Algorithms based on uniform discretization have been shown to achieve optimal worst-case regret up   \n40 to a logarithmic factor (Kleinberg, 2004). Another strategy, adaptive discretization, progressively   \n41 \u2018zooms in\u2019 on more promising regions of the action space, yielding near-optimal problem-dependent   \n42 regret (Kleinberg et al., 2019). However, existing algorithms like the Zooming algorithm necessitate   \n43 ${\\cal O}(T)$ stored arms in memory and $O(T^{2})$ time complexity for stochastic Lipschitz bandits (Kleinberg   \n44 et al., 2019; Feng et al., 2022), which may be impractical for many real-world applications. In   \n45 this paper, we consider a typical scenario where the learner operates within the stochastic bandits   \n46 framework over a Lipschitz action space while facing constraints on the number of arms that can be   \n47 stored in memory.   \n48 The limited memory constraint and large structured action space present several challenges, necessi  \n49 tating a nuanced approach to effectively balance exploration and exploitation under uncertainty. One   \n50 key challenge is the propensity to over-exploit suboptimal arms retained in memory, leading to high   \n51 regret. Conversely, reading new arms into memory risks discarding potentially valuable arms. In   \n52 scenarios with infinite actions, the vast search space requires numerous samples to ensure adequate   \n53 exploration. The structured nature of the action space demands that algorithms focus on zooming in   \n54 on more promising regions, but space constraints limit the learner\u2019s capacity to acquire comprehensive   \n55 knowledge about the metric space. Traditional full-memory algorithms start by dividing the action   \n56 space into many small subcubes, a process known as discretization. Each cube is treated as an arm,   \n57 and in each round, the algorithm updates the average estimate of the selected cube\u2019s reward based on   \n58 feedback. It then compares this estimate against all other cubes in the storage space through various   \n59 computational methods. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "table", "img_path": "SOxxGATGsl/tmp/305e8801b21c4d8ad331724a20e059bfbeb5b1870088ce0c617250bd6e005bf9.jpg", "table_caption": ["Table 1: Comparison with State-of-the-art Lipschitz Bandits Algorithms "], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "60 1.1 Our Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "61 Our primary insight revolves around two key aspects: metric embedding and pairwise comparisons.   \n62 Metric embedding involves mapping elements from one metric space to another while preserving   \n63 distance relationships as closely as possible. Our algorithm effectively maps the metric space to a tree,   \n64 where each node represents a cube. Traversing this tree is analogous to navigating the entire metric   \n65 space. Pairwise comparisons of arms reduce memory complexity. Instead of constantly covering the   \n66 entire space, our approach considers all subcubes as a stream. From this stream, we continuously   \n67 select cubes for pairwise comparisons, gradually converging to the optimal region.   \n68 Based on this insight, we introduce two algorithms: the Memory Bounded Uniform Discretization   \n69 (MBUD) algorithm and the Memory Bounded Adaptive Discretization (MBAD) algorithm. The   \n70 MBUD algorithm employs a uniform discretization strategy combined with an Explore-First approach.   \n71 In this method, all cubes are of the same size. The algorithm prioritizes selecting a near-optimal   \n72 arm following an exploration phase and allocates the remaining rounds to exploitation, achieving   \n73 near-optimal worst-case regret. The exploration phase consists of \u201ccross exploration phases\u201d and the   \n74 \u201csummarize phase\u201d. During the cross exploration phases, exploration is confined to a subset of cubes   \n75 to gather information about the optimal arm while minimizing regret. The summarize phase explores   \n76 all cubes to pinpoint the optimal arm\u2019s location.   \n77 The MBAD algorithm utilizes an adaptive discretization strategy, incorporating a round-robin playing   \n78 approach. This allows for subcubes within subcubes, organizing the entire action space into a tree   \n79 structure. The algorithm selectively focuses on more promising regions of the action space, thereby   \n80 attaining near-optimal instance-dependent regret. Each node in this structure represents a subcube,   \n81 with parent and child nodes corresponding to subcubes and their subdivisions, respectively. Traversal   \n82 involves transitioning from a node to its child and navigating through a parent node\u2019s children to the   \n83 next subcube. Pruning prevents over-zooming through two conditions: discarding inferior cubes with   \n84 high confidence and establishing a lower bound on cube edge length, which decreases as exploration   \n85 progresses. These conditions ensure efficient exploration without over-zooming.   \n86 Overall, our contribution lies in pioneering memory-efficient algorithms for large structured action   \n87 spaces, particularly within Lipschitz metric spaces. We introduce the MBUD and MBAD algorithms,   \n88 which achieve near-optimal regret while requiring storage for only the best-estimate arm for exploita  \n89 tion and one additional arm for exploration. This means only two arms need to be stored in memory,   \n90 regardless of the problem\u2019s scale. Furthermore, each algorithm exhibits ${\\cal O}(T)$ time complexity,   \n91 indicating that their execution time scales linearly with the number of rounds. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "92 1.2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "93 Lipschitz bandits. Multi-armed bandits is one of the most classical frameworks to model the   \n94 trade-off between exploration and exploitation in online decision problems. The Lipschitz bandits   \n95 framework considers the large, structured action space in which the algorithm has information on   \n96 similarities between arms. The model was first introduced by Agrawal (1995) with interval [0, 1].   \n97 The near-optimal upper and lower bounds for the worst case were provided in Kleinberg (2004)   \n98 via the uniform discretization strategy. Subsequent work (Kleinberg et al., 2019) proposed the   \n99 zooming algorithm, achieving near-optimal instance-dependent regret for the problem and studying   \n100 the extension for the general metric action space. Several other works have established regret bounds   \n101 for the stochastic reward feedback setting (Bubeck et al., 2011a; Magureanu et al., 2014; Lazaric   \n102 et al., 2014). Other works have also extended the results to the adversarial version (Podimata &   \n103 Slivkins, 2021; Kang et al., 2023), contextual setting (Slivkins, 2014; Krishnamurthy et al., 2019;   \n104 Lee et al., 2022), ranked setting (Slivkins et al., 2013), contract design (Ho et al., 2014), federated   \n105 X-armed bandit (Li et al., 2024a,b), and other settings (Bubeck et al., 2011b; Lu et al., 2019; Wang   \n106 et al., 2020; Grant & Leslie, 2020; Feng et al., 2022; Xue et al., 2024).   \n107 Memory-efficient learning. Another line relevant to this paper is online learning with memory   \n108 constraints. Liau et al. (2018) considered stochastic bandits with constant arm memory and proposed   \n109 an algorithm achieving an $O(\\log1/\\Delta)$ factor of optimal instance-dependent regret, where $\\Delta$ is the   \n110 gap between the best arm and the second-best arm. Chaudhuri & Kalyanakrishnan (2020) studied   \n111 stochas\u221atic bandits with $M$ stored arms and showed there is an algorithm with regret $\\tilde{O}(K M+$   \n112 $(K^{3/2}\\sqrt{T})/M)$ . Subsequent work (Agarwal et al., 2022) provided an algorithm achieving regret   \n113 $O({\\sqrt{K T\\log T\\log\\log T}})$ . In addition to the bandits problem, there are also many works about   \n114 other online learning problems. Srinivas et al. (2022); Peng & Zhang (2022) showed the trade-off   \n115 between regret and memory for the expert problem. More pure exploration models with memory   \n116 constraints were considered in Assadi & Wang (2020), including the coin tossing problem, noisy   \n117 comparisons problem, and Top- $K$ arms identification. Previous works on bandits with limited   \n118 memory have not considered structured action spaces and could not deal with infinite actions. There   \n119 are some other works on memory-efficient online learning (Peng & Rubinstein, 2023; Assadi &   \n120 Wang, 2023b). Beyond the online learning setting, the memory-efficient learning problem was solved   \n121 in different situations, including statistical learning (Steinhardt et al., 2016; Garg et al., 2017; Raz,   \n122 2017; Garg et al., 2019; Sharan et al., 2019; Lyu et al., 2023), convex optimization (Marsden et al.,   \n123 2022; Blanchard et al., 2023a,b; Chen & Peng, 2023), estimation problems (Acharya et al., 2019;   \n124 Diakonikolas et al., 2022; Berg et al., 2022), parity learning (Raz, 2019; Kol et al., 2017), and other   \n125 learning problems (Hopkins et al., 2021; Brown et al., 2022; Chen et al., 2022). ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "126 2 Problem Setup and Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "127 Notations. In this paper, we use bold fonts to represent vectors and matrices. For a positive integer   \n128 $T$ , we use $[T]$ to denote the set $\\{1,2,\\ldots,T\\}$ . For a set $\\mathcal{X}$ , we use $|\\mathcal{X}|$ to denote its cardinality. For a   \n129 random variable $Z$ , we use $\\mathbb{E}[Z]$ to denote its expectation. For an event $\\mathcal{E}$ , we use $\\mathbb{P}[\\mathcal{E}]$ to denote its   \n130 probability. ", "page_idx": 2}, {"type": "text", "text": "131 2.1 Problem Setup ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "132 We formally define the Lipschitz bandits problem below. Given $T$ rounds, dimension $d$ , and arm space   \n133 $\\mathcal{X}=[0,1]^{\\tilde{d}}$ , each arm $x\\in\\mathscr{X}$ is associated with an unknown reward distribution $\\mathcal{D}_{x}$ . In each round   \n134 $t\\in[T]$ , the algorithm selects an arm $x_{t}\\in\\mathcal{X}$ and obtains a scalar-valued reward feedback $r_{t}\\in[0,1]$ ,   \n135 which is a sample from the reward distribution $\\mathcal{D}_{x_{t}}$ . The expected reward $\\mu(\\cdot)$ of the reward   \n136 distribution satisfy the Lipschitz condition: $|\\mu(x)-\\mu(\\bar{y})|\\leq L\\cdot|\\bar{x}-y|\\quad\\forall x,y\\in\\dot{\\mathcal{X}}$ . And we call $L$   \n137 the Lipschitz constant. Then a problem instance is specified by the known number of time horizons $T$ ,   \n138 known Lipschitz constant $L$ , and unknown mean reward $\\mu(\\cdot)$ . For the purposes of simplification in our   \n139 proofs, we assume $L=1$ . The algorithm aims to maximize the expected total reward $\\mathbb{E}[\\sum_{t\\in[T]}r_{t}]$   \n140 We use regret to measure the performance of the algorithm compared with the expected total reward   \n141 of the best-fixed arm in action space $\\begin{array}{r}{\\chi\\colon\\mathbb{R}_{X}(T)=\\stackrel{\\cdot}{T}\\cdot\\operatorname*{sup}_{x\\in\\mathcal{X}}\\stackrel{\\cdot}{\\mu}(x)-\\mathbb{E}\\left[\\sum_{t\\in[T]}r_{t}\\right].}\\end{array}$   \n142 Then we present the memory model employed in the paper. The algorithm operates by selecting   \n143 arms from the memory and pulling them. When the memory reaches the capacity and the algorithm   \n144 attempts to choose a new arm, it becomes necessary to discard at least one arm from the memory.   \n145 Consequently, any statistical information associated with the discarded arm, including its index, mean   \n146 reward, and number of pulls, is forgotten and will not be retained thereafter. We measure the space   \n147 complexity of the algorithm by the hard constraint for the number of arms stored in the memory. This   \n148 constraint aligns with the assumption of having oracle access to the input arm, as commonly defined   \n149 in streaming problems. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "150 2.2 Covering Dimension and Zooming Dimension ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "151 Then we provide some technical tools that are used in this paper and introduce the covering dimension   \n152 and zooming dimension for one action space $\\mathcal{X}$ . We use the definitions in (Slivkins, 2019) and provide   \n153 them below. Notice that the Lipschitz bandits problem is defined in an infinite-action space. We   \n154 select a fixed, finite discretization actions space $S\\subset\\mathcal{X}$ . Let $\\{\\mathcal{X}_{1},\\ldots,\\mathcal{X}_{N}\\}[\\mathcal{X}_{i}\\subset\\mathcal{X}]$ be an cover   \n155 of the action space $\\mathcal{X}$ . Let $\\epsilon$ denote the maximum diameter of $\\mathbf{\\mathcal{X}}_{i}$ for all $i\\in[N]$ . Then the arm set   \n156 ${\\cal{S}}=\\{x_{i}|x_{i}\\in\\mathcal{X}_{i},i\\in[N]\\}$ is an $\\epsilon$ -mesh. The covering dimension $d$ of the action space $\\mathcal{X}$ is defined   \n157 as $d=\\operatorname*{inf}_{\\alpha\\geq0}\\,\\{|S|\\leq\\epsilon^{-\\alpha},\\forall\\epsilon>0\\}.$ Let $\\mu_{\\mathcal{X}}^{*}:=\\operatorname*{sup}_{x\\in\\mathcal{X}}\\mu(x)$ denote the expected per-round reward   \n158 of the optimal arm in space $\\mathcal{X}$ and $\\Delta(x):=\\stackrel{\\cdot}{\\mu}_{\\chi}^{*}-\\mu(x)$ denote the gap between arm $x$ and the optimal   \n159 arm. Define $\\mathcal{Y}_{j}=\\{x\\in\\mathcal{X}:2^{-j}\\leq\\Delta(x)<2^{1-j},j\\in\\mathbb{N}\\}$ , then set $\\mathcal{y}_{j}$ contains all arms whose gap   \n160 is between $2^{-j}$ and $2^{1-j}$ . Consider the $\\epsilon$ -mesh ${\\mathbf{}}S_{j}$ for space $\\mathcal{y}_{j}$ . Then the zooming dimension $d_{z}$ for   \n161 the action space $\\mathcal{X}$ is $d_{z}=\\operatorname*{inf}_{\\beta\\geq0}\\,\\left\\{|S_{j}|\\leq\\epsilon^{\\beta},\\epsilon\\,{\\stackrel{\\cdot}{=}}\\,2^{-j},\\forall j\\in\\mathbb{N}\\right\\}$ .   \n162 Covering dimension is a property of the action space while the zooming dimension is a property of   \n163 the instance. Notice that we always have $d_{z}\\leq d$ . This is because the covering dimension considers   \n164 the $\\epsilon$ -mesh of the entire action space $\\mathcal{X}$ , whereas the zooming dimension focuses only on the set $\\ y_{j}$ .   \n165 The covering dimension is closely related to other notions of dimensionality in a metric space, such as   \n166 the Hausdorff dimension, capacity dimension, and box-counting dimension, all of which characterize   \n167 the covering properties in fractal geometry. Similarly, the zooming dimension is another measure   \n168 used to evaluate the structure of a metric space. Both of these dimensions are widely utilized in the   \n169 field of Lipschitz bandits. For further details and alternative formulations regarding the covering   \n170 dimension and zooming dimension, refer to (Kleinberg et al., 2019). ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "171 3 Warm Up: Uniform Discretization Algorithm ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "172 This section provides the intuition, specification, and theoretical analysis of the Memory Bounded   \n173 Uniform Discretization (MBUD) algorithm (shown in Algorithm 1) for the stochastic Lipschitz   \n174 bandits problem.   \n175 Algorithm overview. To facilitate our discussion, we begin by outlining the core idea behind the   \n176 algorithm. This algorithm employs a uniform discretization strategy and adopts an Explore-First   \n177 methodology, which endeavors to identify a near-optimal arm following the exploration phase and   \n178 dedicates the remaining rounds to exploitation. Throughout the exploration stage, the algorithm allo  \n179 cates two units of memory space: one for storing the best-estimated arm and another for temporarily   \n180 holding a newly read arm. Note that the best-estimated arm serves a dual purpose: it is not only   \n181 crucial for the exploitation phase but also enables the swift identification of sub-optimal arms.   \n182 The exploration phase in Algorithm 1 is divided into $\\lceil\\log\\log T\\rceil$ phases, further structured into   \n183 two main segments: the \u2018cross exploration phases\u2019 and the \u2018summarize phase\u2019. During the initial   \n184 $\\lceil\\log\\log T\\rceil-1$ phases, the algorithm iterates over the arms within the discretized action space to   \n185 minimize regret. Exploration is limited to a subset of cubes at a time, allowing the algorithm to   \n186 gather information about the optimal arm while minimizing regret. In the final phase, termed the   \n187 \u2018summarize phase\u2019, the algorithm revisits all arms within the uniform discretization space. Overall,   \n188 each arm is read into memory twice to ensure thorough evaluation. Furthermore, we implement a   \n189 budgeting strategy for each phase, wherein the total number of pulls across all arms is constrained by   \n190 a predefined budget. The goal is to select the optimal arm with high probability after accumulating   \n191 sufficient information during the previous phases. This structured approach balances exploration and   \nInput: arm space $\\mathcal{X}=[0,1]^{d}$ , time horizon $T$ , parameter $c$ .   \n1: ${\\bf\\nabla}y\\leftarrow{\\bf0}$ , $\\bar{r}_{y}\\leftarrow0$ , $n_{y}\\leftarrow0$ , $B_{-1}\\leftarrow1$ , $\\begin{array}{r}{\\epsilon=\\left(\\frac{\\log T}{T}\\right)^{1/(d+2)}}\\end{array}$ , $\\phi\\leftarrow\\lceil\\log\\log T\\rceil-1.$ .   \n2: for $p=0,\\cdot\\cdot\\cdot\\,,\\phi-1$ do   \n3: $\\bar{B}_{p}\\leftarrow\\sqrt{T B_{p-1}}$ .   \n4: for $q=1,\\cdot\\cdot\\cdot\\,,\\lfloor\\phi\\epsilon^{-d}\\rfloor$ do   \n5: Generate a new cube $C\\gets\\mathrm{CROSSCUBE}(\\phi,\\epsilon,p,q)$ , and select a arm $\\textbf{\\em x}$ from $C$ .   \n6: $(y,\\bar{r}_{y},n_{y})\\gets\\mathrm{COMPARE}(c,x,y,\\bar{r}_{y},n_{y},\\epsilon B_{p})$ .   \n7: end for   \n8: end for   \n9: for $q=1,\\cdot\\cdot\\cdot\\,,\\lfloor\\epsilon^{-d}\\rfloor$ do   \n10: Generate a new cube $C\\gets\\mathrm{GENERATECUBE}(\\epsilon,q)$ , and select a arm $\\textbf{\\em x}$ from $C$ .   \n11: $(\\pmb{y},\\bar{r}_{y},n_{y})\\gets\\mathrm{COMPARE}(c,\\pmb{x},\\pmb{y},\\bar{r}_{y},n_{y},\\epsilon B_{\\phi-1}).$ .   \n12: end for   \n13: Play arm $y$ until the end of the game. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Algorithm 2 CROSSCUBE ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Input: number of phases $\\phi$ , edge-length $\\epsilon$ , parameters $q$ .   \n1: $\\kappa_{1}\\leftarrow\\operatorname*{max}_{k\\in\\mathbb{N}}\\{k^{d}\\leq\\phi\\}$ , $\\kappa_{2}\\gets\\operatorname*{max}_{k\\in\\mathbb{N}}\\{k^{d}\\leq\\lfloor\\phi\\epsilon^{-d}\\rfloor\\}.$ .   \n2: Let node $\\begin{array}{r}{\\leftarrow\\epsilon\\mathcal{G}_{d}\\left(p,\\kappa_{1}\\right)+\\frac{\\epsilon\\phi}{\\sqrt{d}}\\mathcal{G}_{d}\\left(q,\\kappa_{2}\\right)}\\end{array}$ , then the cube could be determined by node and $\\epsilon$ . ", "page_idx": 4}, {"type": "text", "text": "192 exploitation under memory constraints, aiming to quickly identify the optimal arm while minimizing   \n193 the sampling of suboptimal arms. The specifics of this approach will be detailed subsequently.   \n194 Exploration strategies. For the cross exploration phases, the gap between neighboring arms is   \n195 $\\epsilon\\phi$ $\\mathit{\\Delta}\\phi$ defined in Algorithm 1). There are $O(\\epsilon^{-d})$ cubes (arms) in the discretization action set,   \n196 which is an $\\epsilon$ -mesh of $\\mathcal{X}$ . Each cross exploration phase will only explore $O\\left({\\frac{1}{\\log\\log T}}\\right)$ of them.   \n197 We generate a new cube by using the function $\\mathcal{G}_{d}(a,b),a,b\\in\\mathbb{N}$ which converts the integer $a$ to a   \n198 $d$ -dimension vector. And the $i$ -th entry of the vector is the $i$ -th right-most digit in base $b$ . To aid   \n199 understanding, we offer several examples: $\\mathcal{G}_{3}(3,2)\\,=\\,(0,1,1)$ , $\\mathcal{G}_{3}(1208,26)\\,=\\,(1,20,12)$ , and   \n200 $\\mathcal{G}_{2}(1208,26)\\ {=}\\ (20,12)$ . The function could be done by a succession of Euclidean divisions by $b$ .   \n201 For the summarize phase, the gap is $\\epsilon$ and all cubes in the discretization set are explored.   \n202 The CROSSCUBE function generates cubes for the cross exploration phases by calculating parameters   \n203 based on the number of phases and the edge-length of the cubes. Specifically, CROSSCUBE generates   \n204 a new cube using a combination of two geometric sequences. It first calculates the parameters $\\kappa_{1}$   \n205 and $\\kappa_{2}$ as the maximum integers such that $k^{d}\\leq\\phi$ and $\\bar{k}^{d}\\leq\\lfloor\\phi\\epsilon^{-d}\\rfloor$ , respectively. The function then   \n206 determines the cube\u2019s position using these parameters and the edge-length $\\epsilon$ . The cube is defined   \n207 by a node position generated by $\\epsilon\\mathcal{G}_{d}(p,\\kappa_{1})$ and $\\frac{\\epsilon\\phi}{\\sqrt{d}}\\mathcal{G}_{d}(q,\\kappa_{2})$ , where $\\boldsymbol{g}_{d}$ is a geometric sequence   \n208 generator that converts an integer to a $d_{\\cdot}$ -dimensional vector. The GENERATECUBE function is similar   \n209 to CROSSCUBE but is used during the summarize phase to generate cubes without considering the   \n210 phases. It calculates the parameter $\\kappa$ as the maximum integer such that $k^{d}\\leq\\lfloor\\epsilon^{-d}\\rfloor$ . The cube is then   \n211 determined by the edge-length $\\epsilon$ and a node position generated by $\\epsilon\\mathcal{G}_{d}(q,\\kappa)$ .   \n212 Compare strategy. Then we introduce the compare strategy, which is also useful for the MBAD   \n213 algorithm described in the following section. The algorithm always selects the arm with the fewest   \n214 pulls in the memory. After sufficient samples, it will eliminate one sub-optimal arm based on its   \n215 upper confidence bound and then generate a new arm (i.e., read a new arm into the memory). Notice   \n216 that the algorithm may prioritize two sub-optimal arms with a small gap. Therefore, there is a cap   \n217 on the number of pulls each phase for any arm. It helps the algorithm in striking a balance between   \n218 exploration (read a new arm) and exploitation (play arms in memory).   \n219 The algorithm maintains three statistics for one arm in memory: the index $x$ , the mean reward   \n220 estimator $\\bar{r}_{x}$ , and the number of pulls $n_{x}$ . The constant $c$ is an exploration and exploitation balancing   \n221 parameter. In the exploration part, there are $\\lceil\\log\\log T\\rceil$ phases. Let $B_{p}$ be the budget of samples for   \n222 the $p$ -th phase. We use $y$ and $x$ to denote the best-estimated arm and the new arm in the algorithm, ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "table", "img_path": "SOxxGATGsl/tmp/a87fd757c2a4a9819eba5d18e61618336dbc1e89d909e7d3a49cb959d851d911.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Algorithm 4 COMPARE   \nInput: constant $c$ , arm $x$ and $y,\\bar{r}_{y},n_{y},b$ .   \n1: $\\bar{r}_{x}\\gets0$ , $n_{x}\\leftarrow0$ .   \n2: while $n_{x}\\leq b$ or $n_{y}\\le b$ do   \n3: Pull the least played arm between $x$ and $y$ . If there is no single least played arm, select a random arm.   \n4: Update $\\bar{r}_{x}$ , $n_{x}$ , $\\bar{r}_{y}$ , $n_{y}$ .   \n5: i $\\operatorname*{min}\\{\\bar{r}_{x}+\\sqrt{(c\\log T)/n_{x}},1\\}<\\operatorname*{max}\\{\\bar{r}_{y}-\\sqrt{(c\\log T)/n_{y}},0\\}\\ \\mathbf{the}$ n   \n6: Break and return $(y,\\bar{r}_{y},n_{y})$ .   \n7: else i $\\mathrm{?\\,max}\\{\\bar{r}_{x}-\\sqrt{(c\\log T)/n_{x}},0\\}>\\mathrm{max}\\{\\bar{r}_{y}-\\sqrt{(c\\log T)/n_{y}},0\\}$ then   \n8: Break and return $(x,\\bar{r}_{x},n_{x})$ .   \n9: end if   \n10: end while   \n11: Return $(y,\\bar{r}_{y},n_{y})$ . ", "page_idx": 5}, {"type": "text", "text": "223 respectively. If the upper confidence bound (UCB) of arm $x$ is less than the lower confidence bound   \n224 (LCB) of arm $y$ , then $x$ is suboptimal with high probability. If the LCB of $y$ is less than the LCB of   \n225 arm $x$ , then $x$ is not too bad with high probability. For the remaining cases, we could choose either $x$   \n226 or $y$ , and we choose arm $y$ at the end of the algorithm. ", "page_idx": 5}, {"type": "text", "text": "227 Flowchart. In Appendix A.1, we include a flowchart that illustrates the operation of the algorithm. ", "page_idx": 5}, {"type": "text", "text": "228 Theoretical result. The computational workload of the MBUD algorithm is characterized by a   \n229 constant per-round operation, leading to a total time complexity of ${\\cal O}(T)$ , where $T$ represents the   \n230 number of rounds. Regarding space complexity, the MBUD algorithm necessitates the storage   \n231 of merely two arms in memory at any given time. Additionally, the space requirements for the   \n232 GENERATECUBE and CROSSCUBE subroutines are minimal, each consuming $O(1)$ units of space   \n233 in terms of arm storage. Consequently, the overall space complexity of the algorithm is $O(1)$ .   \n234 We provide the theoretical result below and provide the details of the theoretical analysis in Appendix   \n235 B. The result recovers the worst case regret in previous work and recovers the lower bound up to a   \n236 logarithmic factor (Kleinberg, 2004).   \n237 Theorem 1. For the stochastic Lipschitz bandits problem with metric $(\\mathcal{X},\\mathcal{D})$ and time horizon $T$ ,   \n238 where $\\mathcal{X}\\,=\\,[0,1]^{d}$ and $\\mathcal{D}$ is a known metric function. Algorithm $^{\\,l}$ uses $O(1)$ stored arms and   \n239 achieves regret ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{R}_{\\mathcal{X}}(T)\\leq\\tilde{O}(T^{\\frac{d+1}{d+2}}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "240 where d is the covering dimension of space $\\mathcal{X}$ . ", "page_idx": 5}, {"type": "text", "text": "241 The theoretical analysis is mainly based on the \u2018clean event\u2019, which holds that the observed mean   \n242 average is a good estimator for the expectation with high probability. At a high level, the analysis   \n243 shows that the deviation between the mean estimator of the best-estimated arm $y$ and the optimal   \n244 expected reward $\\mu_{\\mathcal{X}}^{*}$ is small enough when $p\\geq1$ . Then the sub-optimal arms could be discarded   \n245 quickly, which helps us to bound the incurred regret of sub-optimal arms and the exploitation phase.   \n246 We bound the expected regret during all time horizons by considering the discretization error, the   \n247 incurred regret of all sub-optimal arms during the exploration, and the sub-optimality of the selected   \n248 arm before the exploitation together. ", "page_idx": 5}, {"type": "text", "text": "249 4 Adaptive Discretization Algorithm ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "250 This section provides the main idea, specification, and theoretical analysis of the Memory Bounded   \n251 Adaptive Discretization (MBAD) algorithm (shown in Algorithm 5). ", "page_idx": 5}, {"type": "image", "img_path": "SOxxGATGsl/tmp/214a1a53181e04304f90e70137fae46da715b34f0445f476e887824ca01153a1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Algorithm 6 ADAPTIVECUBE ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Input: parameters $m,q$ , edge-length $\\epsilon=2^{-m}$ .   \n1: $\\kappa\\leftarrow\\operatorname*{max}_{k\\in\\mathbb{N}}\\{k^{d}\\leq\\lfloor\\epsilon^{-d}\\rfloor\\}$ .   \n2: n $\\mathsf{d e}\\gets\\epsilon\\mathcal G_{d}\\left(q,\\kappa\\right)$ , then the cube $C$ could be determined by node and $\\epsilon$ .   \n3: Select a arm $\\textbf{\\em x}$ from $C$ .   \n4: if $q+1\\leq2^{m}$ and the output of $\\mathrm{COMPARE}(c,\\mathbf{\\Delta},\\mathbf{x},\\mathbf{y},\\bar{r}_{y},n_{y},20\\epsilon^{-2})$ is arm $\\textit{\\textbf{y}}$ then   \n5: check the next cube with parameters $m$ and $q+1$ .   \n6: else if $20\\epsilon^{-2}\\leq b_{p}$ then   \n7: $(y,\\bar{r}_{y},n_{y})\\gets\\mathrm{COMPARE}(c,x,y,\\bar{r}_{y},n_{y},20\\epsilon^{-2})$ .   \n8: Equally partition the cube $C$ into $2^{d}$ subcubes and check the first subcube.   \n9: end if   \n252 Algorithm overview. We begin with some intuitions. The MBUD algorithm achieves near-optimal   \n253 regret in the worst case but fails to leverage the beneficial structure of \u2018nice\u2019 problem instances.   \n254 To address this, we present the MBAD algorithm, which is based on adaptive discretization, and   \n255 establish a near-optimal instance-dependent upper bound. The idea behind adaptive discretization is   \n256 straightforward: the algorithm should focus more on promising regions. For instance, the zooming   \n257 algorithm approximates the expected rewards over the action space and explores more in regions   \n258 with a high probability of yielding high rewards. However, due to memory constraints, the algorithm   \n259 cannot obtain a comprehensive picture of the action space over time. To overcome this obstacle, the   \n260 MBAD algorithm employs a \u201cround robin\u201d strategy, storing the best-estimated arm as the next read   \n261 arm in memory. Unlike the MBUD method, which chooses predetermined steps, the MBAD algorithm   \n262 selects the next read arm based on the confidence radius of the arms in memory. Consequently, steps   \n263 are smaller and probes (newly picked arms) are more numerous in promising regions.   \n264 Exploration strategies. The ADAPTIVECUBE subroutine is the cornerstone of the MBAD algo  \n265 rithm, functioning as a recursive mechanism to navigate and leverage a cubic region within the   \n266 decision space. This procedure dynamically adjusts the exploration granularity based on observed   \n267 rewards and predetermined sampling constraints. Initially, the algorithm selects a cube $C$ for ex  \n268 ploration. If this cube is deemed sub-optimal compared to the optimal estimated arm stored in   \n269 memory (denoted as arm $\\textit{\\textbf{y}}$ ), the algorithm discards this cube in favor of exploring a subsequent   \n270 cube, following the generation rules outlined in the GENERATECUBE subroutine described in the   \n271 MBUD algorithm (Section 3). Conversely, if the cube shows promise, the algorithm proceeds to   \n272 explore within it, subdividing it into smaller subcubes for more detailed exploration. Each exploration   \n273 phase is governed by a specific sample budget, which regulates the granularity of exploration to   \n274 prevent excessive sampling of sub-optimal arms in the early stages. This adaptive exploration process   \n275 continues until the entire action space has been thoroughly explored. The decision-making process   \n276 is inherently dynamic, constantly evolving based on past actions to enhance the efficiency of future   \n277 exploration and exploitation efforts.   \n278 To prevent the MBAD algorithm from \u201cover-zooming\u201d, we implement two stop conditions. The first   \n279 condition discards the current cube in favor of a new one once we are highly confident that the current   \n280 cube is inferior to the best cube we\u2019ve explored (see lines 4-5 of Algorithm 6). The second condition   \n281 sets a lower bound on the edge length of the cube to be explored in each round, which gradually   \n282 decreases as exploration progresses (see line 6 of Algorithm 6). These conditions together ensure the   \n283 algorithm avoids over-zooming. In the initial learning phase, our knowledge of the optimal cube is   \n284 limited, making it challenging to effectively distinguish suboptimal cubes using only the first stop   \n285 condition. However, the second condition, with a larger initial lower bound on cube edge length,   \n286 prevents over-zooming. As the learning process advances, the algorithm can more reliably eliminate   \n287 suboptimal cubes, thus avoiding over-zooming on them.   \n288 Flowchart and algorithm description. Due to page limitations, Appendix A.2 contains a flowchart   \n289 illustrating the operation of the algorithm along with its description.   \n290 Theoretical result. Analyzing the space complexity of the MBAD algorithm and its ADAPTIVE  \n291 CUBE subroutine requires careful consideration due to the subroutine\u2019s recursive nature. Specifically,   \n292 the conditional logic that triggers further recursion or partitioning into $2^{d}$ subcubes adds layers of   \n293 complexity. Within the ADAPTIVECUBE subroutine, each recursive invocation contributes to the call   \n294 stack, with space consumption directly proportional to the recursion depth. The space required to   \n295 sustain the state of each cube, alongside the recursive call stack within ADAPTIVECUBE, implies   \n296 a complexity that scales linearly with recursion depth, complemented by constant overheads for   \n297 variables preserved at each recursion level. Nonetheless, the algorithm\u2019s design allows for the direct   \n298 computation of all parent and neighboring cube information from the current cube\u2019s coordinates and   \n299 edge length, obviating the need for multiple cube storage in memory. Consequently, only a single   \n300 cube needs to be maintained at any time during the ADAPTIVECUBE process, affirming a space   \n301 complexity of $O(1)$ for the MBAD algorithm. This space complexity analysis directly informs the   \n302 algorithm\u2019s time complexity. Similar to the MBUD algorithm, the overall time complexity of the   \n303 MBAD algorithm remains linear with respect to the total number of rounds.   \n304 As a by-product of the MBAD algorithm, we introduce a simpler, more practical algorithm for   \n305 scenarios where $d_{z}\\leq1$ . Detailed descriptions and theoretical analyses of this algorithm can be   \n306 found in Appendix D. We provide the theoretical result below and elaborate on the details of the   \n307 theoretical analysis in Appendix C. The result establishes the optimal instance-dependent upper   \n308 bound, up to a logarithmic factor, for the stochastic Lipschitz bandits problem. Previous works   \n309 (Slivkins, 2014; Kleinberg et al., 2019) have already established related lower bounds, indicating that   \n310 our work achieves near-optimal regret. While there are other forms of results, such as those presented   \n311 in work (Magureanu et al., 2014), we believe that adopting one form is sufficient to demonstrate the   \n312 near-optimal performance of our algorithm.   \n313 Theorem 2. For Lipschitz bandits with time horizon $T$ and Lipschitz constant $L$ , Algorithm 5 with   \n314 $c\\geq5$ achieves regret ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{R}_{\\chi}(T)\\le\\tilde{O}(T^{\\frac{d_{z}+1}{d_{z}+2}}),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "315 using $O(1)$ stored arms, where $d_{z}$ is the zooming dimension of space $\\mathcal{X}$ . ", "page_idx": 7}, {"type": "text", "text": "316 We also mainly consider the clean event. The algorithm plays in a \u2018round-robin\u2019 manner. There are at   \n317 most ${\\cal O}(\\log T)$ phases because of the delicate design of the budget for each phase. For each phase,   \n318 we show that the deviation between the mean reward of the best-estimated arm and optimal expected   \n319 per-round reward $\\mu_{\\mathcal{X}}^{*}$ is small. Then the algorithm could approximately adjust the sub-optimality of   \n320 arms and set more probes in more promising regions. Then we prove that the incurred regret could be   \n321 bounded by $O(T^{\\frac{z^{*}+1}{z+2}}(\\log T)^{\\frac{2}{z+2}})$ by bounding the pulls of bad arms according to the definition of   \n322 zooming dimension. ", "page_idx": 7}, {"type": "text", "text": "323 5 Numerical Evaluations ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "324 In this section, we show the efficiency of our algorithms through a series of numerical simulations.   \n325 The baseline consists of three algorithms: the uniform discretization with UCB1 algorithm (UD) and   \n326 the zooming algorithm. For the uniform discretization, we pick a fixed $\\epsilon$ -mesh of the action space   \n327 and run the UCB1 algorithm only considering the finite uniform discretization action space. The   \n328 UCB1 algorithm is a popular algorithm for achieving near-optimal regret with finite action space.   \n329 Kleinberg (2004) prove that the uniform achieves optimal worst-case regret up to logarithm factors.   \n330 The zooming algorithm (Kleinberg et al., 2019) is an implementation of the adaptive discretization   \n331 strategy, which deploys more probes in regions deemed more \u2018promising\u2019. Theoretical analysis shows   \n332 that the zooming algorithm both achieves optimal worst-case regret and instance-dependent regret up   \n333 to logarithm factors.   \n334 We set $\\mathcal{X}=[0,1]$ and choose the reward function $f(x)=0.5-|x-0.5|$ . In each round $t$ , the   \n335 algorithm plays one arm $x_{t}$ and receives a stochastic reward $y$ satisfying ", "page_idx": 7}, {"type": "image", "img_path": "SOxxGATGsl/tmp/95b1875e948fe88f61d8ee2db36e03d2ce0bea414aa840d6c924b2f45597e7ca.jpg", "img_caption": ["Figure 1: The results obtained with different time horizons. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "equation", "text": "$$\ny=\\left\\{\\begin{array}{l l}{f(x)+\\xi,}&{0\\leq f(x)+\\xi\\leq1}\\\\ {1,}&{f(x)+\\xi>1}\\\\ {0,}&{f(x)+\\xi<0}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "336 Specifically, $\\xi\\sim\\mathcal{N}(0,0.1^{2})$ is the Gaussian noise. The MBAD algorithm are only allowed to store   \n337 two arms in memory, while there is no memory constraint for the $\\scriptstyle\\mathrm{UD}+\\mathrm{UCB}1$ algorithm and the   \n338 zooming algorithm. All results are averages over 50 runs. Figure 1 displays the results obtained   \n339 across varying time horizons, where the horizontal axis denotes the time horizon and the vertical axis   \n340 measures regret. From the figure, we have that MBAD algorithm significantly outperforms the UD   \n341 strategy. Additional numerical results are detailed in Appendix E. ", "page_idx": 8}, {"type": "text", "text": "342 6 Conclusion and Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "343 We consider the Lipschitz bandits with limited memory problem. We introduce two novel algorithms:   \n344 the Memory Bounded Uniform Discretization (MBUD) algorithm and the Memory Bounded Adaptive   \n345 Discretization (MBAD) algorithm, which are predicated on the principles of uniform and adaptive   \n346 discretization, respectively. Theoretical analyses reveal that the MBAD algorithm achieves near  \n347 optimal performance with $O(1)$ stored arms and ${\\cal O}(T)$ time complexity, highlighting its efficiency   \n348 and practical applicability. Moreover, numerical results show the efficiency of our algorithms.   \n349 The Lipschitz bandit problem in higher dimensions is often perceived as a \u2018needle in a haystack\u2019   \n350 problem. Intuitively, finding the optimal solution in such high-dimensional spaces seems extremely   \n351 challenging, but this perception does not always hold in practice. Many scenarios reveal beneficial   \n352 structures within Lipschitz bandits, which is why our research emphasizes not only worst-case regret   \n353 but also instance-dependent regret. Our proposed algorithm achieves nearly optimal time and space   \n354 complexity for both worst-case and instance-dependent regrets.   \n355 In practical applications, Lipschitz bandit problems are found in areas such as non-parametric   \n356 estimation, model selection in machine learning tasks, and decision-making processes in robotics and   \n357 games. Furthermore, research on Lipschitz bandits has inspired algorithmic advancements in other   \n358 domains, such as decision trees and tree-based methods, where the principles from Lipschitz bandit   \n359 algorithms guide the splitting and growth of trees. Despite these advancements, certain limitations   \n360 remain. High-dimensional Lipschitz bandits can still pose significant computational challenges,   \n361 especially in cases where the underlying structure is less apparent or more complex. Additionally, the   \n362 requirement for sufficient exploration to accurately estimate the optimal arm can lead to increased   \n363 computational overhead in large action spaces.   \n364 Our algorithm introduces a novel framework that efficiently addresses online decision-making and   \n365 balances exploration and exploitation in Lipschitz action spaces. This framework leverages beneficial   \n366 structures in the problem space to enhance performance while maintaining computational efficiency.   \n367 We hope our approach could make a substantial contribution to the community, especially in areas   \n368 that require efficient and effective decision-making under uncertainty. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "369 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "370 Acharya, J., Bhadane, S., Indyk, P., and Sun, Z. Estimating entropy of distributions in constant space.   \n371 In Advances in Neural Information Processing Systems 32, pp. 5163\u20135174, 2019.   \n372 Agarwal, A., Khanna, S., and Patil, P. A sharp memory-regret trade-off for multi-pass streaming   \n373 bandits. In Conference on Learning Theory (COLT), volume 178 of Proceedings of Machine   \n374 Learning Research, pp. 1423\u20131462. PMLR, 2022.   \n375 Agrawal, R. The continuum-armed bandit problem. SIAM journal on control and optimization, 33   \n376 (6):1926\u20131951, 1995.   \n377 Assadi, S. and Wang, C. Exploration with limited memory: streaming algorithms for coin tossing,   \n378 noisy comparisons, and multi-armed bandits. In Symposium on Theory of Computing (STOC), pp.   \n379 1237\u20131250. ACM, 2020.   \n380 Assadi, S. and Wang, C. Single-pass streaming lower bounds for multi-armed bandits exploration   \n381 with instance-sensitive sample complexity. In Advances in Neural Information Processing Systems   \n382 35, 2022.   \n383 Assadi, S. and Wang, C. The best arm evades: Near-optimal multi-pass streaming lower bounds for   \n384 pure exploration in multi-armed bandits. CoRR, abs/2309.03145, 2023a.   \n385 Assadi, S. and Wang, C. The best arm evades: Near-optimal multi-pass streaming lower bounds for   \n386 pure exploration in multi-armed bandits. arXiv preprint arXiv:2309.03145, 2023b.   \n387 Berg, T., Ordentlich, O., and Shayevitz, O. On the memory complexity of uniformity testing. In   \n388 Proceedings of the 35th Conference on Learning Theory, pp. 3506\u20133523, 2022.   \n389 Blanchard, M., Zhang, J., and Jaillet, P. Memory-constrained algorithms for convex optimization. In   \n390 Advances in Neural Information Processing Systems 36, 2023a.   \n391 Blanchard, M., Zhang, J., and Jaillet, P. Quadratic memory is necessary for optimal query complexity   \n392 in convex optimization: Center-of-mass is pareto-optimal. In Proceedings of 36th Conference on   \n393 Learning Theory, pp. 4696\u20134736, 2023b.   \n394 Brown, G., Bun, M., and Smith, A. D. Strong memory lower bounds for learning natural models. In   \n395 Proceedings of the 35th Conference on Learning Theory, pp. 4989\u20135029, 2022.   \n396 Bubeck, S., Munos, R., Stoltz, G., and Szepesv\u00e1ri, C. X-armed bandits. Journal of Machine Learning   \n397 Research, 12(5), 2011a.   \n398 Bubeck, S., Stoltz, G., and Yu, J. Y. Lipschitz bandits without the lipschitz constant. In Algorithmic   \n399 Learning Theory, volume 6925, pp. 144\u2013158. Springer, 2011b.   \n400 Chaudhuri, A. R. and Kalyanakrishnan, S. Regret minimisation in multi-armed bandits using bounded   \n401 arm memory. In AAAI, pp. 10085\u201310092. AAAI Press, 2020.   \n402 Chen, X. and Peng, B. Memory-query tradeoffs for randomized convex optimization. In Proceedings   \n403 of the 64th Symposium on Foundations of Computer Science, 2023.   \n404 Chen, X., Papadimitriou, C. H., and Peng, B. Memory bounds for continual learning. In Proceedings   \n405 of the 63rd Symposium on Foundations of Computer Science, pp. 519\u2013530, 2022.   \n406 Diakonikolas, I., Kane, D. M., Pensia, A., and Pittas, T. Streaming algorithms for high-dimensional   \n407 robust statistics. In Proceedings of the 39th International Conference on Machine Learning, pp.   \n408 5061\u20135117, 2022.   \n409 Feng, Y., Huang, Z., and Wang, T. Lipschitz bandits with batched feedback. In NeurIPS, 2022.   \n410 Garg, S., Raz, R., and Tal, A. Extractor-based time-space tradeoffs for learning. Manuscript. July,   \n411 2017.   \n412 Garg, S., Raz, R., and Tal, A. Time-space lower bounds for two-pass learning. In Proceedings of the   \n413 34th Computational Complexity Conference, pp. 22:1\u201322:39, 2019.   \n414 Grant, J. A. and Leslie, D. S. On thompson sampling for smoother-than-lipschitz bandits. In The   \n415 23rd International Conference on Artificial Intelligence and Statistics, volume 108 of Proceedings   \n416 of Machine Learning Research, pp. 2612\u20132622. PMLR, 2020.   \n417 Ho, C., Slivkins, A., and Vaughan, J. W. Adaptive contract design for crowdsourcing markets: bandit   \n418 algorithms for repeated principal-agent problems. In Conference on Economics and Computation   \n419 (EC), pp. 359\u2013376. ACM, 2014.   \n420 Hopkins, M., Kane, D., Lovett, S., and Moshkovitz, M. Bounded memory active learning through   \n421 enriched queries. In Proceedings of the 34th Conference on Learning Theory, pp. 2358\u20132387,   \n422 2021.   \n423 Jin, T., Huang, K., Tang, J., and Xiao, X. Optimal streaming algorithms for multi-armed bandits. In   \n424 Proceedings of the 38th International Conference on Machine Learning, pp. 5045\u20135054, 2021.   \n425 Kang, Y., Hsieh, C., and Lee, T. C. M. Robust lipschitz bandits to adversarial corruptions. In   \n426 Advances in Neural Information Processing Systems 36, 2023.   \n427 Kleinberg, R., Slivkins, A., and Upfal, E. Bandits and experts in metric spaces. J. ACM, 66(4):   \n428 30:1\u201330:77, 2019.   \n429 Kleinberg, R. D. Nearly tight bounds for the continuum-armed bandit problem. In Advances in   \n430 Neural Information Processing Systems (NIPS), pp. 697\u2013704, 2004.   \n431 Kol, G., Raz, R., and Tal, A. Time-space hardness of learning sparse parities. In Proceedings of the   \n432 49th Symposium on Theory of Computing, pp. 1067\u20131080, 2017.   \n433 Krishnamurthy, A., Langford, J., Slivkins, A., and Zhang, C. Contextual bandits with continuous   \n434 actions: Smoothing, zooming, and adapting. In Conference on Learning Theory, volume 99 of   \n435 Proceedings of Machine Learning Research, pp. 2025\u20132027. PMLR, 2019.   \n436 Lazaric, A., Brunskill, E., et al. Online stochastic optimization under correlated bandit feedback. In   \n437 Proceedings of the 31st International Conference on Machine Learning, pp. 1557\u20131565, 2014.   \n438 Lee, H., Lee, J., Choi, Y., Jeon, W., Lee, B., Noh, Y., and Kim, K. Local metric learning for off-policy   \n439 evaluation in contextual bandits with continuous actions. In NeurIPS, 2022.   \n440 Li, W., Song, Q., and Honorio, J. Personalized federated x-armed bandit. In International Conference   \n441 on Artificial Intelligence and Statistics, pp. 37\u201345, 2024a.   \n442 Li, W., Song, Q., Honorio, J., and Lin, G. Federated x-armed bandit. In Proceedings of the 38th   \n443 Conference on Artificial Intelligence, pp. 13628\u201313636, 2024b.   \n444 Liau, D., Song, Z., Price, E., and Yang, G. Stochastic multi-armed bandits in constant space.   \n445 In International Conference on Artificial Intelligence and Statistics (AISTATS), volume 84, pp.   \n446 386\u2013394. PMLR, 2018.   \n447 Lu, S., Wang, G., Hu, Y., and Zhang, L. Optimal algorithms for lipschitz bandits with heavy-tailed   \n448 rewards. In Proceedings of the 36th International Conference on Machine Learning, volume 97,   \n449 pp. 4154\u20134163. PMLR, 2019.   \n450 Lyu, X., Tal, A., Wu, H., and Yang, J. Tight time-space lower bounds for constant-pass learning. In   \n451 Proceedings of the 64th Symposium on Foundations of Computer Science, 2023.   \n452 Magureanu, S., Combes, R., and Prouti\u00e8re, A. Lipschitz bandits: Regret lower bound and optimal   \n453 algorithms. In Proceedings of The 27th Conference on Learning Theory, pp. 975\u2013999, 2014.   \n454 Maiti, A., Patil, V., and Khan, A. Streaming algorithms for stochastic multi-armed bandits. CoRR,   \n455 abs/2012.05142, 2020.   \n456 Marsden, A., Sharan, V., Sidford, A., and Valiant, G. Efficient convex optimization requires   \n457 superlinear memory. In Proceedings of the 35th Conference on Learning Theory, pp. 2390\u20132430,   \n458 2022.   \n459 Peng, B. and Rubinstein, A. Near optimal memory-regret tradeoff for online learning. In IEEE 64th   \n460 Annual Symposium on Foundations of Computer Science, pp. 1171\u20131194, 2023.   \n461 Peng, B. and Zhang, F. Online prediction in sub-linear space. CoRR, abs/2207.07974, 2022.   \n462 Podimata, C. and Slivkins, A. Adaptive discretization for adversarial lipschitz bandits. In Conference   \n463 on Learning Theory (COLT), volume 134 of Proceedings of Machine Learning Research, pp.   \n464 3788\u20133805. PMLR, 2021.   \n465 Raz, R. A time-space lower bound for a large class of learning problems. In Proceedings of the 58th   \n466 Symposium on Foundations of Computer Science, pp. 732\u2013742, 2017.   \n467 Raz, R. Fast learning requires good memory: A time-space lower bound for parity learning. J. ACM,   \n468 66(1):3:1\u20133:18, 2019.   \n469 Sharan, V., Sidford, A., and Valiant, G. Memory-sample tradeoffs for linear regression with small   \n470 error. In Proceedings of the 51st Symposium on Theory of Computing, pp. 890\u2013901, 2019.   \n471 Slivkins, A. Contextual bandits with similarity information. J. Mach. Learn. Res., 15(1):2533\u20132568,   \n472 2014.   \n473 Slivkins, A. Introduction to multi-armed bandits. Found. Trends Mach. Learn., 12(1-2):1\u2013286, 2019.   \n474 Slivkins, A., Radlinski, F., and Gollapudi, S. Ranked bandits in metric spaces: learning diverse   \n475 rankings over large document collections. J. Mach. Learn. Res., 14(1):399\u2013436, 2013.   \n476 Srinivas, V., Woodruff, D. P., Xu, Z., and Zhou, S. Memory bounds for the experts problem. In   \n477 Symposium on Theory of Computing (STOC), pp. 1158\u20131171. ACM, 2022.   \n478 Steinhardt, J., Valiant, G., and Wager, S. Memory, communication, and statistical queries. In   \n479 Proceedings of the 29th Conference on Learning Theory, pp. 1490\u20131516, 2016.   \n480 Wang, C. Tight regret bounds for single-pass streaming multi-armed bandits. CoRR, abs/2306.02208,   \n481 2023.   \n482 Wang, T., Ye, W., Geng, D., and Rudin, C. Towards practical lipschitz bandits. In FODS \u201920:   \n483 ACM-IMS Foundations of Data Science Conference, pp. 129\u2013138. ACM, 2020.   \n484 Xue, B., Cheng, J., Liu, F., Wang, Y., and Zhang, Q. Multiobjective lipschitz bandits under lexi  \n485 cographic ordering. In Proceedings of the 38th Conference on Artificial Intelligence, pp. 16238\u2013   \n486 16246, 2024. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "487 A Algorithm Flowchart ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "488 A.1 Flowchart for the MBUD Algorithm ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "489 The flowchart illustrates the process of the Memory Bounded Uniform Discretization (MBUD)   \n490 algorithm, showcasing its core steps and transitions. The algorithm begins by dividing the exploration   \n491 phase into $\\lceil\\log\\log T\\rceil$ phases, further segmented into cross exploration phases and the summarize   \n492 phase.   \n493 At the start of the algorithm, the arm space $\\mathcal{X}\\,=\\,[0,1]^{d}$ , time horizon $T$ , and parameter $c$ are   \n494 initialized. The initial values for variables such as the best-estimated arm $\\textit{\\textbf{y}}$ , its average reward   \n495 $\\bar{r}_{y}$ , and the number of pulls $n_{y}$ are set to zero. The budget parameter $B_{-1}$ is initialized to 1, and   \n496 the discretization parameter $\\epsilon$ is calculated. Rather than covering the entire space continuously, the   \n497 MBUD algorithm treats subcubes as a stream, selecting cubes for pairwise comparisons and gradually   \n498 converging to the optimal region.   \n499 During the cross exploration phases, which encompass the first $\\lceil\\log\\log T\\rceil-1$ phases, the algorithm   \n500 iterates over arms within the discretized action space. In each phase $p$ , the budget parameter $B_{p}$ is   \n501 updated to $\\sqrt{T B_{p-1}}$ . For each $q$ from 1 to $\\lfloor\\phi\\epsilon^{-d}\\rfloor$ , the CROSSCUBE function generates a new cube   \n502 $C$ by calculating parameters $\\kappa_{1}$ and $\\kappa_{2}$ and determining the cube\u2019s position using the edge-length $\\epsilon$ .   \n503 An arm $\\textbf{\\em x}$ is then selected from the cube $C$ . The COMPARE function evaluates the selected arm against   \n504 the current best-estimated arm $\\textit{\\textbf{y}}$ , updating $\\textit{\\textbf{y}}$ if necessary based on the comparison of their upper and   \n505 lower confidence bounds. In the final phase, known as the summarize phase, the algorithm revisits   \n506 all arms within the uniform discretization space. For each $q$ from 1 to $\\bar{\\lfloor\\epsilon^{-d}\\rfloor}$ , the GENERATECUBE   \n507 function generates a new cube $C$ without considering the phases, using a parameter $\\kappa$ to determine the   \n508 cube\u2019s position. An arm $\\textbf{\\em x}$ is selected from this cube and compared against the current best-estimated   \n509 arm $\\textit{\\textbf{y}}$ using the COMPARE function, ensuring thorough evaluation. The algorithm culminates by   \n510 selecting the best-estimated arm $\\textit{\\textbf{y}}$ and playing it for the remaining rounds until the end of the time   \n511 horizon.   \n513 The MBAD algorithm dynamically adapts its discretization of the action space, focusing more   \n514 on promising regions to identify the optimal arm with high probability. The flowchart effectively   \n515 demonstrates how the algorithm narrows down the search space through adaptive discretization.   \n516 Initially, the algorithm sets up the necessary parameters and variables. During the cross-exploration   \n517 phases, the ADAPTIVECUBE function generates and selects arms from cubes. Arrows indicate the   \n518 process of moving to the next cube if $\\textit{\\textbf{y}}$ remains the best arm, and the selection and evaluation of   \n519 subcubes when the comparison budget condition is met.   \n520 We present the pseudocode. The algorithm begins by initializing key parameters: the time horizon $T$   \n521 and a constant $c$ . Initial values for essential variables include the best-estimated arm $\\textit{\\textbf{y}}$ \u221a, its average   \n522 reward $\\bar{r}_{y}$ , and the number of pulls $n_{y}$ , all set to zero. The initial budget $B_{1}$ is set to $\\sqrt{T}$ . In each   \n523 phase $p$ , the algorithm initializes a new arm $\\textbf{\\em x}$ with zero values for its index, average reward $\\bar{r}_{x}$ , and   \n524 the number of pulls $n_{x}$ . The budget for the current phase $b_{p}$ is calculated as $B_{p}\\cdot\\left({\\frac{\\log T}{T}}\\right)^{1/(d+2)}$ . The   \n525 ADAPTIVECUBE function is then called with parameters $m=4$ and $q=1$ , and the budget for the   \n526 next phase $B_{p+1}$ is updated to $B_{p}\\log T$ .   \nThe ADAPTIVECUBE function is crucial for refining the discretization of the action space and   \n528 selecting promising arms. It begins by setting the edge-length $\\epsilon=2^{-m}$ . The function calculates the   \n529 parameter $\\kappa$ as the largest integer such that $\\bar{k}^{d}\\leq\\bar{\\lfloor\\epsilon^{-d}\\rfloor}$ . A node is generated using the geometric   \n530 sequence $\\mathcal{G}_{d}(q,\\kappa)$ , and the cube $C$ is defined by this node and \u03f5. An arm $\\textbf{\\em x}$ is selected from the cube   \n531 $C$ . If $q+1\\leq2^{m}$ and the COMPARE function indicates that $\\textit{\\textbf{y}}$ remains the best arm after comparison,   \n532 the algorithm proceeds to the next cube with parameters $m$ and $q+1$ . If the comparison budget   \n533 $20\\epsilon^{-2}$ does not exceed $b_{p}$ , the algorithm updates $\\textit{\\textbf{y}}$ using the COMPARE function, partitions the cube   \n534 $C$ into $2^{d}$ subcubes, and checks the first subcube. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "image", "img_path": "SOxxGATGsl/tmp/6b51e4f11a636bb9261dac75804926ac5a48e749b951518820a0b8327658009f.jpg", "img_caption": ["Figure 2: Flowchart for the MBUD algorithm "], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "image", "img_path": "SOxxGATGsl/tmp/8c4e232c8a016bf1d040b226d24f277633d62bacbaa39e28c8f52c4cc03f8641.jpg", "img_caption": ["Figure 3: Flowchart for the MBAD algorithm "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "535 B Proof of Theorem 1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "536 Let $\\boldsymbol{S}$ denote the $\\epsilon$ -mesh of the action space where $\\begin{array}{r}{\\epsilon=\\left(\\frac{\\log T}{T}\\right)^{1/(d+2)}}\\end{array}$ . Similarly, the discretization   \n537 error is the gap of the best fixed arm benchmarks between two spaces: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{D}_{X}(S)=T\\cdot\\operatorname*{sup}_{x\\in{X}}\\mu(x)-T\\cdot\\operatorname*{sup}_{x\\in{S}}\\mu(x).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "538 Then the regret could be rewrote as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{R}_{\\mathcal{X}}(T)=\\mathbb{R}_{\\mathcal{X}}(T)+\\mathbb{D}_{\\mathcal{X}}(S).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "539 We call $\\mathbb{D}_{\\boldsymbol{\\mathcal{X}}}(\\boldsymbol{S})$ the discretization error and it could be bounded by ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{D}_{\\mathcal{X}}(S)\\leq T\\epsilon\\leq T^{\\frac{d+1}{d+2}}(\\log T)^{\\frac{1}{d+2}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "540 In the rest of this subsection, we shall prove ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{R}_{S}(T)\\leq\\tilde{O}\\left(T^{\\frac{d+1}{d+2}}\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "541 For any fixed arm $x\\in S$ , with probability $1-T^{-c}$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n|\\mu(x)-\\bar{r}_{x}|\\leq\\sqrt{\\frac{c\\log T}{n_{x}}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "542 By a union bound for all arms and all rounds, (1) holds for all arm $x_{t}\\in S,t\\in[T]$ with probability at   \n543 least $1-T^{4-c}$ . To ease the reading, we assume $c=5$ . We call this \u2018clean event\u2019 and let $\\mathcal{E}$ denote   \n544 it. Then we analyze the regret based on the clean event. Let $\\mathbb{R}_{S}^{p}$ denote the regret for the $p$ -th phase.   \n545 Consider $\\mathbb{R}_{S}^{0}$ , because the number of pulls of all arms in phase 0 is bounded by $\\epsilon B_{0}$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{R}_{S}^{0}\\leq2\\sqrt{T}/\\log\\log T.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "546 Then we consider $\\mathbb{R}_{S}^{p},p\\in[\\phi-1]$ . Let $\\mu_{S}^{*}:=\\operatorname*{sup}_{x\\in S}\\mu(x)$ denote the expected per-round reward of   \n547 the optimal arm in space $\\boldsymbol{S}$ . Let $\\boldsymbol{x}_{p}^{*}$ and $\\mu_{p}^{*}$ denote the optimal selected arm during phase $p$ and its   \n548 expected per-round reward, respectively. From the definition of uniform discretization and Lipschitz   \n549 condition, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mu_{S}^{*}-\\mu_{p}^{*}\\leq\\left({\\frac{\\log T}{T}}\\right)^{1/(d+2)}\\log\\log T.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "550 for all phase $p\\in[\\phi-1]$ . To ease the reading, define ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Phi:=\\left({\\frac{\\log T}{T}}\\right)^{1/(d+2)}\\log\\log T.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "551 For phase $p$ , we consider the best estimate arm $y$ at the start of the $p$ -th phase. If $x_{p-1}^{*}$ is discarded in   \n552 phase $p-1$ , according to the stop condition of compare strategy, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\bar{r}_{y}\\geq\\mu_{p-1}^{*}-\\sqrt{\\frac{5\\log T}{n_{y}}}-\\sqrt{\\frac{5\\log T}{n_{x_{p-1}^{*}}}}\\geq\\mu_{p-1}^{*}-2\\sqrt{\\frac{5\\log T}{\\epsilon B_{p-1}}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "553 For arbitrary discarded arm $x$ , let $R_{x}^{p}$ and $N_{x}^{p}$ denote the accumulated reward and total number of   \n554 pulls during phase $p$ , respectively. Notice that the value of $\\bar{r}_{y}-\\sqrt{(5\\log T)/n_{y}}$ is non-decreasing, so   \n555 we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{R_{x}^{p}}{N_{x}^{p}-1}+\\sqrt{\\frac{5\\log T}{N_{x}^{p}-1}}\\ge\\mu_{p-1}^{*}-2\\sqrt{\\frac{5\\log T}{\\epsilon B_{p-1}}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "556 Combine (2) and (3) together, we get ", "page_idx": 14}, {"type": "equation", "text": "$$\nR_{x}^{p}\\geq2N_{x}^{p}\\left(\\mu_{p-1}^{*}-\\sqrt{\\frac{5\\log T}{N_{x}^{p}-1}}-\\sqrt{\\frac{5\\log T}{\\epsilon B_{p-1}}}\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "557 Let $\\mathbb{R}_{x}^{p}$ denote the cumulative regret of playing arm $x$ during phase $p$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{R}_{x}^{p}\\leq2N_{x}^{p}\\left(\\sqrt{\\frac{5\\log T}{N_{x}^{p}-1}}+\\sqrt{\\frac{5\\log T}{\\epsilon B_{p-1}}}+\\Phi\\right)}\\\\ &{\\qquad\\leq2\\left(\\sqrt{6N_{x}^{p}\\log T}+N_{x}^{p}\\sqrt{\\frac{5\\log T}{\\epsilon B_{p-1}}}+N_{x}^{p}\\Phi\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "558 The first term from the gap between the expected reward of best estimated arm and the selected   \n559 sub-optimal arm. The second term from the deviation between the best estimated arm and optimal   \n560 expected per-round reward of the $(p-1)$ -th phase. And the last is the discretization error during   \n561 phase $p-1$ . Let ${\\mathcal S}_{p}$ denote the set of arms in phase $p$ . According to Jensen\u2019s inequality ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{1}{|S_{p}|}\\sum_{x\\in S_{p}}\\sqrt{N_{x}^{p}}\\leq\\sqrt{\\frac{1}{|S_{p}|}\\sum_{x\\in S_{p}}N_{x}^{p}}\\leq\\sqrt{\\epsilon B_{p}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "562 Then we obtain ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{x\\in S_{p}}\\sqrt{N_{x}^{p}}\\leq|S_{p}|\\sqrt{\\epsilon B_{p}}\\leq\\sqrt{\\frac{B_{p}}{\\epsilon\\log\\log T}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "563 Consider all selected arms during phase $p$ and the stop condition of the compare strategy, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{R}_{S}^{p}\\leq2\\displaystyle\\sum_{x\\in S_{p}}\\left(\\sqrt{6N_{x}^{p}\\log T}+N_{x}^{p}\\sqrt{\\frac{5\\log T}{\\epsilon B_{p-1}}}+N_{x}^{p}\\Phi\\right)}\\\\ &{\\quad\\leq\\displaystyle\\frac{3B_{p}}{\\log\\log T}\\left(\\sqrt{\\frac{5\\log T}{\\epsilon B_{p-1}}}+\\Phi\\right)+2\\sqrt{6\\log T}\\displaystyle\\sum_{x\\in S_{p}}\\sqrt{N_{x}^{p}}}\\\\ &{\\quad\\leq\\displaystyle\\frac{3B_{p}}{\\log\\log T}\\left(\\sqrt{\\frac{5\\log T}{\\epsilon B_{p-1}}}+\\Phi\\right)+3\\sqrt{\\frac{6B_{p}\\log T}{\\epsilon\\log\\log T}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "564 For the incurred regret by the deviation between the expected reward of best estimated arm and the   \n565 selected sub-optimal arm, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{p=1}^{\\phi-1}3\\sqrt{\\frac{6B_{p}\\log T}{\\epsilon\\log\\log T}}\\le6\\sqrt{\\frac{6B_{\\phi-1}\\log T}{\\epsilon\\log\\log T}}\\le6\\sqrt{\\frac{6T\\log T}{\\epsilon\\log\\log T}}\\le\\tilde{O}\\left(T^{\\frac{d+1}{d+2}}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "566 For the incurred regret the deviation between the best estimated arm and optimal expected per-round   \n567 reward, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{p=1}^{\\phi-1}\\frac{3B_{p}}{\\log\\log T}\\sqrt{\\frac{5\\log T}{\\epsilon B_{p-1}}}\\le6B_{\\phi-1}\\sqrt{\\frac{5\\log T}{\\epsilon B_{\\phi-2}}}\\le6\\sqrt{T}\\sqrt{\\frac{5\\log T}{\\epsilon}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "568 For the incurred regret of the discretization error during one phase, we obtain ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{p=1}^{\\phi-1}\\frac{3B_{p}\\Phi}{\\log\\log T}\\leq\\frac{6B_{\\phi-1}\\Phi}{\\log\\log T}\\leq\\Tilde{O}\\left(T^{\\frac{d+1}{d+2}}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "569 Combine them together, we get ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{p=1}^{\\phi-1}\\mathbb{R}_{S}^{p}\\leq\\tilde{O}\\left(T^{\\frac{d+1}{d+2}}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "570 Then we consider the total cumulative regret during the exploration part. Let $\\mathbb{R}_{S}^{\\phi}$ denote the regret   \n571 incurred in the last part. According to that the value of $\\bar{r}_{y}-\\sqrt{(5\\log T)/n_{y}}$ is non-decreasing and   \n572 the relationship between $B_{\\phi}$ and $B_{\\phi-1}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{R}_{S}^{\\phi}\\le\\mathbb{R}_{S}^{\\phi-1}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "573 Then the regret incurred by the exploration is ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{p=0}^{\\phi}\\mathbb{R}_{S}^{p}\\leq2\\sum_{p=0}^{\\phi-1}\\mathbb{R}_{S}^{p}\\leq\\tilde{O}\\left(T^{\\frac{d+1}{d+2}}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "574 Consider the selected arm $y$ after the exploration and let $\\mathbb{R}_{S}^{y}$ denote the regret due to selecting it.   \n575 According to the stop condition of compare strategy, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\bar{r}_{y}\\geq\\mu_{S}^{*}-2\\sqrt{\\frac{5\\log T}{\\epsilon B_{\\phi-1}}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "576 Then for the regret ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{R}_{S}^{y}\\le2T\\sqrt{\\frac{5\\log T}{\\epsilon B_{\\phi-1}}}\\le\\tilde{O}\\left(T^{\\frac{d+1}{d+2}}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "577 Based on the clean event, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathbb{R}_{S}(T)|\\mathcal{E}]=\\mathbb{R}_{S}^{y}+\\sum_{p=0}^{\\phi}\\mathbb{R}_{S}^{p}\\le\\tilde{O}\\left(T^{\\frac{d+1}{d+2}}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "578 Then the regret is ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{R}_{S}(T)=\\mathbb{E}[\\mathbb{R}_{S}(T)|\\mathcal{E}]\\cdot\\mathbb{P}(\\mathcal{E})+\\mathbb{E}[\\mathbb{R}_{S}(T)|\\neg\\mathcal{E}]\\cdot\\mathbb{P}(\\neg\\mathcal{E})}\\\\ &{\\qquad\\quad\\le[\\tilde{O}\\left(T^{\\frac{d+1}{d+2}}\\right)](1-1/T)+1}\\\\ &{\\qquad\\quad\\le\\tilde{O}\\left(T^{\\frac{d+1}{d+2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "579 Combine it with the discretization error, then we complete the proof. ", "page_idx": 16}, {"type": "text", "text": "580 C Proof of Theorem 2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "581 To ease the reading, let $c=5$ . For all arms $x_{t}\\in\\mathscr{X}$ and all rounds $t\\in[T]$ , the gap between the mean   \n582 reward and the expectation could be bounded with probability $1-T^{-1}$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n|\\mu(x_{t})-\\bar{r}_{x_{t}}|\\leq\\sqrt{\\frac{5\\log T}{n_{x_{t}}}},\\forall t\\in[T].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "583 We call this \u2018clean event\u2019 $\\mathcal{E}$ and mainly analyze the regret based on $\\mathcal{E}$ . Assume the MBAD algorithm   \n584 consume all time horizons during the $\\phi$ -th phase. For the stochastic Lipschitz instance, we always   \n585 have $\\begin{array}{r}{\\phi\\le O\\left(\\frac{\\log T}{\\log\\log T}\\right)}\\end{array}$ . Let $\\mathbb{R}_{S}^{p}$ denote the regret for the $p$ -th phase. For the first phase, we have   \n586 $\\mathbb{R}_{\\chi}^{1}\\,\\le\\,N^{1}\\,\\le\\,B_{1}\\,\\le\\,\\sqrt{T}$ . Then we consider $\\mathbb{R}_{S}^{p},\\boldsymbol{1}\\,<\\,p\\,\\leq\\,\\phi$ . For phase $p$ , we consider the best   \n587 estimate arm $y$ at the start of the $p$ -th phase. If $x_{p-1}^{*}$ is discarded in phase $p-1$ , according to the   \n588 stop condition of compare strategy, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\bar{r}_{y}\\geq\\mu_{p-1}^{*}-\\sqrt{\\frac{5\\log T}{n_{y}}}-\\sqrt{\\frac{5\\log T}{n_{x_{p-1}^{*}}}}\\geq\\mu_{p-1}^{*}-2\\sqrt{\\frac{5\\log T}{b_{p-1}}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "589 For arbitrary discarded arm $x$ , let $R_{x}^{p}$ and $N_{x}^{p}$ denote the accumulated reward and total number of   \n590 pulls during phase $p$ , respectively. Notice that the value of $\\bar{r}_{y}-\\sqrt{(5\\log T)/n_{y}}$ is non-decreasing, so   \n591 we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{R_{x}^{p}}{N_{x}^{p}-1}+\\sqrt{\\frac{5\\log T}{N_{x}^{p}-1}}\\ge\\mu_{p-1}^{*}-2\\sqrt{\\frac{5\\log T}{b_{p-1}}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "592 Then we get ", "page_idx": 16}, {"type": "equation", "text": "$$\nR_{x}^{p}\\geq2N_{x}^{p}\\left(\\mu_{p-1}^{*}-\\sqrt{\\frac{5\\log T}{N_{x}^{p}-1}}-\\sqrt{\\frac{5\\log T}{b_{p-1}}}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "593 Let $\\mathbb{R}_{x}^{p}$ denote the cumulative regret of playing arm $x$ during phase $p$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{R}_{x}^{p}\\le2N_{x}^{p}\\left(\\sqrt{\\frac{5\\log T}{N_{x}^{p}-1}}+\\sqrt{\\frac{5\\log T}{b_{p-1}}}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "594 Similarly, the first term from the gap between the expected reward of best estimated arm and the   \n595 selected sub-optimal arm. The second term from the deviation between the best estimated arm and   \n596 optimal expected per-round reward of the $(p-1)$ -th phase. Recall the set ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{Y}_{i}=\\{x\\in X:2^{-i}\\leq\\Delta(x)<2^{1-i},i\\in\\mathbb{N}\\},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "597 and the definition of zooming dimension ", "page_idx": 17}, {"type": "equation", "text": "$$\nd_{z}=\\operatorname*{inf}_{\\beta\\geq0}\\left\\{|S_{j}|\\leq O(\\epsilon^{\\beta}),\\epsilon=O(2^{-j}),\\forall j\\in\\mathbb{N}\\right\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "598 Pick $\\begin{array}{r}{\\delta=\\left(\\frac{\\log^{2}T}{T}\\right)^{\\frac{1}{d_{z}+2}}}\\end{array}$ , if $\\begin{array}{r}{\\sqrt{\\frac{5\\log T}{N_{x}^{p}-1}}+\\sqrt{\\frac{5\\log T}{b_{p-1}}}\\leq O(\\delta)}\\end{array}$ , then $\\mathbb{R}_{x}^{p}\\le O(\\delta N_{x}^{p})$ . If $\\sqrt{\\frac{5\\log T}{b_{p-1}}}>\\Omega(\\delta)$ ,   \n599 then $b_{p-1}=O(\\log T)\\Delta^{-2}(x)$ . I f 5N lxpog\u2212 T1 > \u2126(\u03b4), then N xp = O(log T)\u2206\u22122(x). According the   \n600 stop condition of the compare strategy and the definition of zooming dimension, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{R}_{\\mathcal{X}}^{p}\\leq\\delta N^{p}+\\displaystyle\\sum_{i:2^{-i}>\\delta}\\sum_{x\\in Y_{i}}\\mathbb{R}_{x}^{p}}\\\\ &{\\qquad\\leq\\delta N^{p}+O((\\log T)^{2})\\delta^{d_{z}+1}\\leq\\delta T+O((\\log T)^{2})\\delta^{d_{z}+1}}\\\\ &{\\qquad\\leq O(T^{\\frac{d_{z}+1}{d_{z}+2}}(\\log T)^{\\frac{2}{d_{z}+2}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "601 Then we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{p=1}^{\\phi}\\mathbb{R}_{\\chi}^{p}\\leq\\sum_{p=1}^{\\phi}O(T^{\\frac{d_{z}+1}{d_{z}+2}}(\\log T)^{\\frac{2}{d_{z}+2}})\\leq\\tilde{O}(T^{\\frac{d_{z}+1}{d_{z}+2}}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "602 Based on the clean event, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathbb{R}_{X}(T)|\\mathcal{E}]\\leq\\sum_{p=1}^{\\phi}\\mathbb{R}_{X}^{p}\\leq\\tilde{O}(T^{\\frac{d_{z}+1}{d_{z}+2}}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "603 The regret is ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{R}_{X}(T)\\le\\tilde{O}(T^{\\frac{d_{z}+1}{d_{z}+2}})(1-1/T)+1\\le\\tilde{O}(T^{\\frac{d_{z}+1}{d_{z}+2}}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "604 Then we complete the proof. ", "page_idx": 17}, {"type": "text", "text": "605 D A Simple Algorithm ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "606 We present the pseudocode. The algorithm also maintains the index $x$ , the mean reward estimator   \n607 $\\bar{r}_{x}$ , and the number of pulls $n_{x}$ for one arm in memory. The constants $c$ and $\\eta$ are two parameters   \n608 to balance the exploration and exploitation. For each phase $p$ , the algorithm determines the budget   \n609 $b_{p}$ of samples for each probe, the number of total pulls $N^{p}$ during the phase, and the usage factor   \n610 $\\lambda_{p}$ obtained after the phase. Once the arm $x$ is discarded, the algorithm chooses the next probe by   \n611 adding $\\eta\\sqrt{(c\\log T)/(n_{x}L^{2})}$ , which is the step size and has the same order as the confidence radius   \n612 of arm $x$ . ", "page_idx": 17}, {"type": "text", "text": "613 We have the following theoretical result. ", "page_idx": 17}, {"type": "text", "text": "614 Theorem 3. For Lipschitz bandits with time horizon $T$ and Lipschitz constant $L$ , Algorithm 7 with   \n615 $c\\geq5$ and $\\eta=1/3$ achieves regret ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{R}_{\\mathcal{X}}(T)\\le\\tilde{O}(T^{\\frac{d_{z}+1}{d_{z}+2}}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "616 using $O(1)$ stored arms, where $d_{z}\\leq1$ is the zooming dimension of space $\\mathcal{X}$ . ", "page_idx": 17}, {"type": "text", "text": "Input: rounds $T$ , Lipschitz constant $L$ , constant $c$ and $\\eta$   \n1: $y\\gets0$ , $\\bar{r}_{y}\\gets0$ , $n_{y}\\leftarrow0$ , $B_{1}\\leftarrow\\sqrt{T}$ .   \n2: for $p=1,2,\\cdot\\cdot\\cdot\\textbf{\\em{1}}$ do   \n3: $x\\gets0$ , $N^{p}\\gets0$ .   \n4: while $x\\leq1$ do   \n5: r\u00afx \u21900, nx \u21900, bp \u2190(Bp/3)2/3(c log T)1/3L\u22122/3.   \n6: while $n_{x}\\leq b_{p}$ or $n_{y}\\le b_{p}$ do   \n7: $N^{p}\\gets N^{p}+1$ .   \n8: Pull the least played arm between $x$ and $y$ , and select a random arm if there not exists a   \nleast played arm.   \n9: Update $\\bar{r}_{x}$ , $n_{x},\\bar{r}_{y},n_{y}$ .   \n10: if $\\operatorname*{min}\\{\\bar{r}_{x}+\\sqrt{(c\\log T)/n_{x}},1\\}<\\operatorname*{max}\\{\\bar{r}_{y}-\\sqrt{(c\\log T)/n_{y}},0\\}\\,{\\bf t h e r e}$ n   \n11: Break.   \n12: else if $\\!\\!\\!\\operatorname*{max}\\{\\bar{r}_{x}-\\sqrt{(c\\log T)/n_{x}},0\\}>\\operatorname*{max}\\{\\bar{r}_{y}-\\sqrt{(c\\log T)/n_{y}},0\\}\\,{\\mathbf t}$ hen   \n13: $y\\leftarrow x,\\bar{r}_{y}\\leftarrow\\bar{r}_{x},n_{y}\\leftarrow n_{x}$ .   \n14: Break.   \n15: end if   \n16: end while   \n17: $x\\leftarrow x+\\eta\\sqrt{(c\\log T)/(n_{x}L^{2})}.$   \n18: end while   \n19: $B_{p+1}\\leftarrow B_{p}\\log T.$ .   \n20: end for ", "page_idx": 18}, {"type": "text", "text": "617 D.1 Proof of Theorem 3 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "618 The proof closely mirrors that of Theorem 2. To ease the reading, let $c=5$ and $\\eta=1/3$ . For all   \n619 arms $x_{t}\\in\\mathcal{X}$ and all rounds $t\\in[T]$ , the gap between the mean reward and the expectation could be   \n620 bounded with probability $1-T^{-1}$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n|\\mu(x_{t})-\\bar{r}_{x_{t}}|\\leq\\sqrt{\\frac{5\\log T}{n_{x_{t}}}},\\forall t\\in[T].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "621 We call this \u2018clean event\u2019 $\\mathcal{E}$ and mainly analyze the regret based on $\\mathcal{E}$ . For the number of phases $\\phi$ ,   \n622 we always have $\\begin{array}{r}{\\phi\\le O\\left(\\frac{\\log T}{\\log\\log T}\\right)}\\end{array}$ . Notice that the number of total pulls during the phase $p$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\nN^{p}\\leq3b_{p}L\\sqrt{\\frac{b_{p}}{5\\log T}}\\leq3(B_{p}/3)^{2/3}(5\\log T)^{1/3}\\sqrt{\\frac{(B_{p}/3)^{2/3}(5\\log T)^{1/3}}{5\\log T}}=B_{p}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "623 Let $\\mathbb{R}_{S}^{p}$ denote the regret for the $p$ -th phase. For the first phase, we have $\\mathbb{R}_{\\mathcal{X}}^{1}\\le N^{1}\\le B_{1}\\le\\sqrt{T}$ .   \n662245 -hteh n pwhaes ce.o nIfs $\\mathbb{R}_{S}^{p},1<p\\leq\\phi$ .n  Fpohra speh $p$ , waec ccoorndsiindge rt ot hteh eb esstto pe sctiomnadtitei oarn mo $y$ caot mthpea rstea srtt roatf etghye, $p$ $x_{p-1}^{*}$ $p-1$   \n626 we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\bar{r}_{y}\\geq\\mu_{p-1}^{*}-\\sqrt{\\frac{5\\log T}{n_{y}}}-\\sqrt{\\frac{5\\log T}{n_{x_{p-1}^{*}}}}\\geq\\mu_{p-1}^{*}-2\\sqrt{\\frac{5\\log T}{b_{p-1}}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "627 For arbitrary discarded arm $x$ , let $R_{x}^{p}$ and $N_{x}^{p}$ denote the accumulated reward and total number of   \n628 pulls during phase $p$ , respectively. Notice that the value of $\\bar{r}_{y}-\\sqrt{(5\\log T)/n_{y}}$ is non-decreasing, so   \n629 we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{R_{x}^{p}}{N_{x}^{p}-1}+\\sqrt{\\frac{5\\log T}{N_{x}^{p}-1}}\\ge\\mu_{p-1}^{*}-2\\sqrt{\\frac{5\\log T}{b_{p-1}}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "630 Then we get ", "page_idx": 18}, {"type": "equation", "text": "$$\nR_{x}^{p}\\geq2N_{x}^{p}\\left(\\mu_{p-1}^{*}-\\sqrt{\\frac{5\\log T}{N_{x}^{p}-1}}-\\sqrt{\\frac{5\\log T}{b_{p-1}}}\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "631 Let $\\mathbb{R}_{x}^{p}$ denote the cumulative regret of playing arm $x$ during phase $p$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{R}_{x}^{p}\\le2N_{x}^{p}\\left(\\sqrt{\\frac{5\\log T}{N_{x}^{p}-1}}+\\sqrt{\\frac{5\\log T}{b_{p-1}}}\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "632 Similarly, the first term from the gap between the expected reward of best estimated arm and the   \n633 selected sub-optimal arm. The second term from the deviation between the best estimated arm and   \n634 optimal expected per-round reward of the $(p-1)$ -th phase. Recall the set ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{Y}_{i}=\\{x\\in X:2^{-i}\\leq\\Delta(x)<2^{1-i},i\\in\\mathbb{N}\\},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "635 and the definition of zooming dimension ", "page_idx": 19}, {"type": "equation", "text": "$$\nd_{z}=\\operatorname*{inf}_{\\beta\\geq0}\\left\\{|S_{j}|\\leq O(\\epsilon^{\\beta}),\\epsilon=O(2^{-j}),\\forall j\\in\\mathbb{N}\\right\\}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "636 Pick $\\begin{array}{r}{\\delta=\\left(\\frac{\\log^{2}T}{T}\\right)^{\\frac{1}{d_{z}+2}}}\\end{array}$ if $\\begin{array}{r}{\\sqrt{\\frac{5\\log T}{N_{x}^{p}-1}}+\\sqrt{\\frac{5\\log T}{b_{p-1}}}\\leq O(\\delta)}\\end{array}$ , then $\\mathbb{R}_{x}^{p}\\le O(\\delta N_{x}^{p})$ . If $\\sqrt{\\frac{5\\log T}{b_{p-1}}}>\\Omega(\\delta)$ ,   \n637 then $b_{p-1}=O(\\log T)\\Delta^{-2}(x)$ . If $\\sqrt{\\frac{5\\log T}{N_{x}^{p}-1}}>\\Omega(\\delta)$ , then $N_{x}^{p}=O(\\log T)\\Delta^{-2}(x)$ . According the   \n638 stop condition of the compare strategy and the definition of zooming dimension, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{R}_{\\mathcal{X}}^{p}\\leq\\delta N^{p}+\\displaystyle\\sum_{i:2^{-i}>\\delta}\\sum_{x\\in Y_{i}}\\mathbb{R}_{x}^{p}}\\\\ &{\\qquad\\leq\\delta N^{p}+O((\\log T)^{2})\\delta^{d_{z}+1}\\leq\\delta T+O((\\log T)^{2})\\delta^{d_{z}+1}}\\\\ &{\\qquad\\leq O(T^{\\frac{d_{z}+1}{d_{z}+2}}(\\log T)^{\\frac{2}{d_{z}+2}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "639 Then we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{p=1}^{\\phi}\\mathbb{R}_{\\chi}^{p}\\leq\\sum_{p=1}^{\\phi}O(T^{\\frac{d_{z}+1}{d_{z}+2}}(\\log T)^{\\frac{2}{d_{z}+2}})\\leq\\tilde{O}(T^{\\frac{d_{z}+1}{d_{z}+2}}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "640 Based on the clean event, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathbb{R}_{X}(T)|\\mathcal{E}]\\leq\\sum_{p=1}^{\\phi}\\mathbb{R}_{X}^{p}\\leq\\tilde{O}(T^{\\frac{d_{z}+1}{d_{z}+2}}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "641 The regret is ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{R}_{X}(T)\\le\\tilde{O}(T^{\\frac{d_{z}+1}{d_{z}+2}})(1-1/T)+1\\le\\tilde{O}(T^{\\frac{d_{z}+1}{d_{z}+2}}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "642 Then we complete the proof. ", "page_idx": 19}, {"type": "text", "text": "643 E Numerical Results ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "644 Different variances. Keeping other setting of Figure 1(b) unchanged, Figure 4(a-b) present the   \n645 results with different variances. For $\\xi\\sim\\mathcal{N}(\\bar{0},0.05^{\\bar{2}})$ (Figure 4(a)), the MBAD algorithm achieves   \n646 $140.2\\%$ regret of the zooming algorithm. For $\\xi\\sim\\dot{\\mathcal{N}}(0,\\bar{0}.2^{2})$ (Figure 4(b)), the MBAD algorithm   \n647 achieves $1\\bar{4}9.5\\%$ regret of the zooming algorithm. Overall, our algorithm performs better when the   \n648 variance is small. Note that the algorithm is based on the \u2018successive elimination-style\u2019 strategy and   \n649 smaller variances make the algorithm select better arms during comparisons with higher probability.   \n650 Uniform noise distribution. Keeping other setting of Figure 1(b) unchanged, Figure 5(a) presents   \n651 the results with uniform noise distribution. For $\\xi\\sim\\bar{\\mathcal{U}}(-0.\\bar{2},0.2)$ (Figure 5(a)), the MBAD algorithm   \n652 achieves $118.1\\%$ regret of the zooming algorithm. The results show that our algorithms work robustly   \n653 for different noise distributions.   \n654 Quadratic reward function. We also provide the numerical results for different reward functions.   \n655 Keeping other setting of Figure 1(b) unchanged, Figure 5(b) presents the results with uniform noise   \n656 distribution. For $f(\\bar{x})=1-4\\times(0.5-x)^{2}$ (Figure 5(b)), the MBAD algorithm achieves $132.3\\%$   \n657 regret of the zooming algorithm. The results show that our algorithms work robustly for different   \n658 reward functions. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "image", "img_path": "SOxxGATGsl/tmp/9adae1494b57ec21d377695c5d4a470a347f286bd9f8bb501c914de1518ea03b.jpg", "img_caption": ["Figure 4: Performance comparisons for Gaussian distribution with different variances. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "SOxxGATGsl/tmp/f4b9a6eff10276fe83a416b660fea0d21e069e9f12176f68958b934baaff0833.jpg", "img_caption": ["Figure 5: Performance comparisons for (a) Uniform distribution; (b) Quadratic reward function. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "659 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The main claims in the abstract and introduction accurately reflect the paper\u2019s contributions and scope as they succinctly outline the problem addressed, the approach taken, and the novel insights or advancements achieved within the specified research domain. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "660   \n661   \n662   \n663   \n664   \n665   \n666   \n667   \n668   \n669   \n670   \n671   \n672   \n673   \n674   \n675   \n676   \n677 ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper discusses the limitations of the work in the Conclusion and Discus  \nsion section.   \nGuidelines: \u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper. \u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. \u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. \u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. \u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. \u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: Yes, the paper provides the full set of assumptions and delivers complete and correct proofs for each theoretical result, ensuring rigor and thoroughness in the presentation of mathematical or theoretical findings. ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results. \u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems. \u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. \u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. 4. Experimental Result Reproducibility ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Yes, the paper provides all the necessary details for anyone to reproduce the   \nmain experimental results, ensuring transparency and allowing others to validate the claims   \nand conclusions independently.   \nGuidelines: \u2022 The answer NA means that the paper does not include experiments. \u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. \u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. \u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. \u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "767 5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We plan to release this information after the paper\u2019s publication. ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "793 6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "794 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n795 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n796 results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper clearly outlines essential information needed to understand the results. ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "06 7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Justification: Yes, the paper appropriately reports error bars and provides correct definitions or other relevant information about the statistical significance of the experiments, ensuring clarity and accuracy in the interpretation of results. ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 23}, {"type": "text", "text": "834 8. Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "35 Question: For each experiment, does the paper provide sufficient information on the com  \n36 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n37 the experiments?   \n838 Answer: [Yes]   \n839 Justification: The paper gives clear details on the computer resources needed for each   \n840 experiment.   \n841 Guidelines:   \n842 \u2022 The answer NA means that the paper does not include experiments.   \n843 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n844 or cloud provider, including relevant memory and storage.   \n845 \u2022 The paper should provide the amount of compute required for each of the individual   \n846 experimental runs as well as estimate the total compute.   \n847 \u2022 The paper should disclose whether the full research project required more compute   \n848 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n849 didn\u2019t make it into the paper).   \n850 9. Code Of Ethics   \n851 Question: Does the research conducted in the paper conform, in every respect, with the   \n852 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n853 Answer: [Yes]   \n854 Justification: The research conducted in the paper adheres to the NeurIPS Code of Ethics in   \n855 all respects, ensuring ethical standards are met throughout the research process.   \n856 Guidelines:   \n857 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics   \n858 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n859 deviation from the Code of Ethics.   \n860 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n861 eration due to laws or regulations in their jurisdiction).   \n862 10. Broader Impacts   \n863 Question: Does the paper discuss both potential positive societal impacts and negative   \n864 societal impacts of the work performed?   \n865 Answer: [Yes]   \n866 Justification: The paper discusses societal impacts of the work in the Conclusion and   \n867 Discussion section.   \n868 Guidelines:   \n869 \u2022 The answer NA means that there is no societal impact of the work performed.   \n870 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n871 impact or why the paper does not address societal impact.   \n872 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n873 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n874 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n875 groups), privacy considerations, and security considerations.   \n876 \u2022 The conference expects that many papers will be foundational research and not tied   \n877 to particular applications, let alone deployments. However, if there is a direct path to   \n878 any negative applications, the authors should point it out. For example, it is legitimate   \n879 to point out that an improvement in the quality of generative models could be used to   \n880 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n881 that a generic algorithm for optimizing neural networks could enable people to train   \n882 models that generate Deepfakes faster.   \n883 \u2022 The authors should consider possible harms that could arise when the technology i   \n884 being used as intended and functioning correctly, harms that could arise when the   \n885 technology is being used as intended but gives incorrect results, and harms following   \n886 from (intentional or unintentional) misuse of the technology.   \n887 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n888 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n889 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n890 feedback over time, improving the efficiency and accessibility of ML).   \n891 11. Safeguards   \n892 Question: Does the paper describe safeguards that have been put in place for responsible   \n893 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n894 image generators, or scraped datasets)?   \n895 Answer: [NA]   \n896 Justification: The paper is a theoretical paper and poses no such risks.   \n897 Guidelines:   \n898 \u2022 The answer NA means that the paper poses no such risks.   \n899 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n900 necessary safeguards to allow for controlled use of the model, for example by requiring   \n901 that users adhere to usage guidelines or restrictions to access the model or implementing   \n902 safety filters.   \n903 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n904 should describe how they avoided releasing unsafe images.   \n905 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n906 not require this, but we encourage authors to take this into account and make a best   \n907 faith effort.   \n908 12. Licenses for existing assets   \n909 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n910 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n911 properly respected?   \n912 Answer: [NA]   \n913 Justification: The paper does not use existing assets.   \n914 Guidelines:   \n915 \u2022 The answer NA means that the paper does not use existing assets.   \n916 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n917 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n918 URL.   \n919 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n920 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n921 service of that source should be provided.   \n922 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n923 package should be provided. For popular datasets, paperswithcode.com/datasets   \n924 has curated licenses for some datasets. Their licensing guide can help determine the   \n925 license of a dataset.   \n926 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n927 the derived asset (if it has changed) should be provided.   \n928 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n929 the asset\u2019s creators.   \n930 13. New Assets   \n931 Question: Are new assets introduced in the paper well documented and is the documentation   \n932 provided alongside the assets?   \n933 Answer: [NA]   \n934 Justification: The paper does not release new assets.   \n935 Guidelines:   \n936 \u2022 The answer NA means that the paper does not release new assets.   \n937 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n938 submissions via structured templates. This includes details about training, license,   \n939 limitations, etc.   \n940 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n941 asset is used.   \n942 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n43 create an anonymized URL or include an anonymized zip file.   \n944 14. Crowdsourcing and Research with Human Subjects   \n945 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n46 include the full text of instructions given to participants and screenshots, if applicable, as   \n47 well as details about compensation (if any)?   \n948 Answer: [NA]   \n49 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n950 Guidelines:   \n951 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n952 human subjects.   \n953 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n954 tion of the paper involves human subjects, then as much detail as possible should be   \n955 included in the main paper.   \n956 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n957 or other labor should be paid at least the minimum wage in the country of the data   \n958 collector.   \n959 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n960 Subjects   \n961 Question: Does the paper describe potential risks incurred by study participants, whether   \n962 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n963 approvals (or an equivalent approval/review based on the requirements of your country or   \n964 institution) were obtained?   \n965 Answer: [NA]   \n966 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n967 Guidelines:   \n968 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n969 human subjects.   \n970 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n971 may be required for any human subjects research. If you obtained IRB approval, you   \n972 should clearly state this in the paper.   \n973 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n974 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n975 guidelines for their institution.   \n976 \u2022 For initial submissions, do not include any information that would break anonymity (if   \n77 applicable), such as the institution conducting the review. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}]