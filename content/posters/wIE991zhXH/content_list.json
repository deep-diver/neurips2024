[{"type": "text", "text": "Bandits with Preference Feedback: A Stackelberg Game Perspective ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Barna P\u00e1sztor $_{\\star,1,2}$ Parnian Kassraie\u22c6,1 Andreas Krause1,2 1ETH Zurich 2ETH AI Center {bpasztor, pkassraie, krausea}@ethz.ch ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Bandits with preference feedback present a powerful tool for optimizing unknown target functions when only pairwise comparisons are allowed instead of direct value queries. This model allows for incorporating human feedback into online inference and optimization and has been employed in systems for fine-tuning large language models. The problem is well understood in simplified settings with linear target functions or over finite small domains that limit practical interest. Taking the next step, we consider infinite domains and nonlinear (kernelized) rewards. In this setting, selecting a pair of actions is quite challenging and requires balancing exploration and exploitation at two levels: within the pair, and along the iterations of the algorithm. We propose MAXMINLCB, which emulates this trade-off as a zero-sum Stackelberg game, and chooses action pairs that are informative and yield favorable rewards. MAXMINLCB consistently outperforms existing algorithms and satisfies an anytime-valid rate-optimal regret guarantee. This is due to our novel preference-based confidence sequences for kernelized logistic estimators. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In standard bandit optimization, a learner repeatedly interacts with an unknown environment that gives numerical feedback on the chosen actions according to a utility function $f$ . However, in applications such as fine-tuning large language models, drug testing, or search engine optimization, the quantitative value of design choices or test outcomes are either not directly observable, or are known to be inaccurate, or systematically biased, e.g., if they are provided by human feedback [Casper et al., 2023]. A solution is to optimize for the target based on comparative feedback provided for a pair of queries, which is proven to be more robust to certain biases and uncertainties in the queries [Ji et al., 2023]. ", "page_idx": 0}, {"type": "text", "text": "Bandits with preference feedback, or dueling bandits, address this problem and propose strategies for choosing query/action pairs that yield a high utility over the horizon of interactions. At the core of such algorithms is uncertainty quantification and inference for $f$ in regions of interest, which is closely tied to exploration and exploitation dilemma over a course of queries. Observing only comparative feedback poses an additional challenge, as we now need to balance this trade-off jointly over two actions. This challenge is further exacerbated when optimizing over vast or infinite action domains. As a remedy, prior work often grounds one of the actions by choosing it either randomly or greedily, and tries to balance exploration-exploitation for the second action as a reaction to the first [Ailon et al., 2014, Zoghi et al., 2014a, Kirschner and Krause, 2021, Mehta et al., 2023b]. This approach works well for simple utility functions over low-dimensional domains, however does not scale to more complex problems. ", "page_idx": 0}, {"type": "text", "text": "Aiming to solve this problem, we focus on continuous domains in the Euclidean vector space and complex utility functions that belong to the Reproducing Kernel Hilbert Space (RKHS) of a potentially non-smooth kernel. We propose MAXMINLCB, a sample-efficient algorithm that at every step chooses the actions jointly, by playing a zero-sum Stackelberg (a.k.a Leader-Follower) game. We choose the Lower Confidence Bound (LCB) of $f$ as the objective of this game which the Leader aims to maximize and the Follower to minimize. The equilibrium of this game yields an action pair in which the first action is a favorable candidate to maximize $f$ and the second action is the strongest competitor against the first. Our choice of using the LCB as the objective leads to robustness against uncertainty when selecting the first action. Moreover, it makes the second action an optimistic choice as a competitor, from its own perspective. We observe empirically that this approach creates a natural exploration scheme, and in turn, yields a more sample-efficient algorithm compared to standard baselines. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Our game-theoretic strategy leads to an efficient bandit solver, if the LCB is a valid and tight lower bound on the utility function. To this end, we construct a confidence sequence for $f$ given pairwise preference feedback, by modeling the noisy comparative observations with a logistic-type likelihood function. Our confidence sequence is anytime valid and holds uniformly over the domain, under the assumption that $f$ resides in an RKHS. We improve prior work by removing or relaxing assumptions on the utility while maintaining the same rate of convergence. This result on preference-based confidence sequences may be of independent interest, as it targets the loss function that is typically used for Reinforcement Learning with Human Feedback. ", "page_idx": 1}, {"type": "text", "text": "Contributions Our main contributions are: ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "\u2022 We propose a novel game-theoretic acquisition function for pairwise action selection with preference feedback.   \n\u2022 We construct preference-based confidence sequences for kernelized utility functions that are tight and anytime valid.   \n\u2022 Together this creates MAXMINLCB, an algorithm for bandit optimization with preference feedback over continuous domains. MAXMINLCB satisfies $\\mathcal{O}(\\gamma_{T}\\sqrt{T})$ regret, where $T$ is the horizon and $\\gamma_{T}$ is the information gain of the kernel.   \n\u2022 We benchmark MAXMINLCB over a set of standard optimization problems and consistently outperform the common baselines from the literature. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Learning with indirect feedback was first studied in the supervised preference learning setting [Aiolli and Sperduti, 2004, Chu and Ghahramani, 2005]. Subsequently, online and sequential settings were considered, motivated by applications in which the feedback is provided in an online manner, e.g., by a human [Yue et al., 2012, Yue and Joachims, 2009, Houlsby et al., 2011]. Bengs et al. [2021] surveys this field comprehensively; here we include a brief background. ", "page_idx": 1}, {"type": "text", "text": "Referred to as dueling bandits, a rich body of work considers (finite) multi-armed domains and learns a preference matrix specifying the relation among the arms. Such work often relies on efficient sorting or tournament systems based on the frequency of wins for each action [Jamieson and Nowak, 2011, Zoghi et al., 2014b, Komiyama et al., 2015, Wu and Liu, 2016, Falahatgar et al., 2017]. Rather than jointly selecting the arms, such strategies often simplify the problem by selecting one at random [Zoghi et al., 2014a, Zimmert and Seldin, 2018], greedily [Chen and Frazier, 2017], or from the set of previously selected arms [Ailon et al., 2014]. In contrast, we jointly optimize both actions by choosing them as the equilibrium of a two-player zero-sum Stackelberg game, enabling a more efficient exploration/exploitation trade-off. ", "page_idx": 1}, {"type": "text", "text": "The multi-armed dueling setting, which is reducible to multi-armed bandits [Ailon et al., 2014], naturally fails to scale to infinite compact domains, since regularity among \u201csimilar\u201d arms is not exploited. To go beyond finite domains, utility-based dueling bandits consider an unknown latent function that captures the underlying preference, instead of relying on a preference matrix. The preference feedback is then modeled as the difference in the utility of two chosen actions passed through a link function. Early work is limited to convex domains and imposes strong regularity assumptions [Yue and Joachims, 2009, Kumagai, 2017]. These assumptions are then relaxed to general compact domains if the utility function is linear [Dud\u00edk et al., 2015, Saha, 2021, Saha and Krishnamurthy, 2022]. Constructing valid confidence sets from comparative feedback is a challenging task. However, it is strongly related to uncertainty quantification with direct logistic feedback, which is extensively analyzed by the literature on logistic and generalized linear bandits [Filippi et al., 2010, Faury et al., 2020, Foster and Krishnamurthy, 2018, Beygelzimer et al., 2019, Faury et al., 2022, Lee et al., 2024]. Preference-based bandit optimization with linear utility functions is well understood and is even extended to reinforcement learning with preference feedback on trajectories [Saha et al., 2023, Zhan et al., 2023, Zhu et al., 2023, Ji et al., 2023, Munos et al., 2023]. However, such approaches have limited practical interest, since they cannot capture real-world problems with complex nonlinear utility functions. Alternatively, Reproducing Kernel Hilbert Spaces (RKHS) provide a rich model class for the utility, e.g., if the chosen kernel is universal. Many have proposed heuristic algorithms for bandits and Bayesian optimization in kernelized settings, albeit without providing theoretical guarantees Brochu et al. [2010], Gonz\u00e1lez et al. [2017], Sui et al. [2017], Tucker et al. [2020], Mikkola et al. [2020], Takeno et al. [2023]. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "There have been attempts to prove convergence of kernelized algorithms for preference-based bandits [Xu et al., 2020, Kirschner and Krause, 2021, Mehta et al., 2023b,a]. Such works employ a regression likelihood model which requires them to assume that both the utility and the probability of preference, as a function of actions, lie in an RKHS. In doing so, they use a regression model for solving a problem that is inherently a classification. While the model is valid, it does not result in a sample-efficient algorithm. In contrast, we use a kernelized logistic negative log-likelihood loss to infer the utility function, and provide confidence sets for its minimizer. In a concurrent work, Xu et al. [2024] also consider the kernelized logistic likelihood model and propose a variant of the MULTISBM algorithm [Ailon et al., 2014] which uses likelihood ratio confidence sets. The theoretical approach and resulting algorithm bear significant differences, and the regret guarantee has a strictly worse dependency on the time horizon $T$ , by a factor of $T^{1/4}$ . This is discussed in more detail in Section 5. ", "page_idx": 2}, {"type": "text", "text": "3 Problem Setting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Consider an agent which repeatedly interacts with an environment: at step $t$ the agent selects two actions $\\pmb{x}_{t}$ $\\bar{\\mathbf{\\Lambda}}\\bar{\\mathbf{\\Lambda}}\\bar{\\mathbf{\\Lambda}}\\bar{\\mathbf{\\Lambda}}\\bar{\\mathbf{\\Lambda}}\\in\\bar{\\mathbf{\\Lambda}}\\mathcal{X}$ and only observes stochastic binary feedback $y_{t}\\bar{\\mathbf{\\Omega}}\\in\\ \\{0,1\\}$ indicating if $\\mathbf{\\mathcal{x}}_{t}~\\succ~\\mathbf{\\mathcal{x}}_{t}^{\\prime}$ , that is, if action $\\pmb{x}_{t}$ is preferred over action $\\ensuremath{\\boldsymbol{{x}}}_{t}^{\\prime}$ . Formally, $\\mathbb{P}(y_{t}\\;=\\;1|{\\pmb x}_{t},{\\pmb x}_{t}^{\\prime})\\;=$ $\\mathbb{P}(\\pmb{x}_{t}\\;\\succ\\;\\pmb{x}_{t}^{\\prime})$ , and $y_{t}~=~0$ with probability $\\mathbb{1}\\to\\mathbb{P}(\\pmb{x}_{t}\\;\\succ\\;\\pmb{x}_{t}^{\\prime})$ . Based on the preference history $H_{t}\\,=\\,\\{({\\pmb x}_{1},{\\pmb x}_{1}^{\\prime},{\\pmb y}_{1}),\\dots({\\pmb x}_{t},{\\pmb x}_{t}^{\\prime},{\\pmb y}_{t})\\}$ , the agent aims to sequentially select favorable action pairs. Over a horizon of $T$ steps, the success of the agent is measured through the cumulative dueling regret ", "page_idx": 2}, {"type": "equation", "text": "$$\nR^{\\mathrm{D}}(T)=\\sum_{t=1}^{T}\\frac{\\mathbb{P}({\\pmb x}^{\\star}\\succ{\\pmb x}_{t})+\\mathbb{P}({\\pmb x}^{\\star}\\succ{\\pmb x}_{t}^{\\prime})-1}{2},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "which is the average sub-optimality gap between the chosen pair and a globally preferred action $x^{\\star}$ To better understand this notion of regret, consider the scenario where actions $\\pmb{x}_{t}$ and $\\mathbf{\\boldsymbol{x}}_{t}^{\\prime}$ are both optimal. Then the probabilities are equal to 0.5 and the dueling regret will not grow further, since the regret incurred at step $t$ is zero. This formulation of $R^{\\mathrm{D}}(T)$ is commonly used in the literature of dueling Bandits and RL with preference feedback [Urvoy et al., 2013, Saha et al., 2023, Zhu et al., 2023] and is adapted from Yue et al. [2012]. Our goal is to design an algorithm that satisfies a sublinear dueling regret, where $R^{\\mathrm{D}}(T)/T\\stackrel{}{\\rightarrow}0$ as $T\\to\\infty$ . This implies that given enough evidence, the algorithm will converge to a globally preferred action. To this end, we take a utility-based approach and consider an unknown utility function $f:\\mathcal{X}\\to\\mathbb{R}$ , which reflects the preference via ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\pmb{x}_{t}\\succ\\pmb{x}_{t}^{\\prime}):=s\\left(f(\\pmb{x}_{t})-f(\\pmb{x}_{t}^{\\prime})\\right)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $s\\,:\\,\\mathbb{R}\\ \\to\\ [0,1]$ is the sigmoid function1, i.e. $s(a)\\:=\\:(1+e^{-a})^{-1}$ . Referred to as the Bradley-Terry model [Bradley and Terry, 1952], this probabilistic model for binary feedback is widely used in the literature for logistic and generalized bandits [Filippi et al., 2010, Faury et al., 2020]. Under the utility-based model, $\\pmb{x}^{\\star}=\\arg\\operatorname*{max}_{\\pmb{x}\\in\\mathcal{X}}f(\\pmb{x})$ and we can draw connections to a classic bandit problem with direct feedback over the utility $f$ . In particular, Saha [2021] shows that the dueling regret of (1) is equivalent up to constant factors, to the average utility regret of the two actions, that is $\\begin{array}{r}{\\sum_{t=1}^{T}f(\\pmb{x}^{\\star})-[f(\\pmb{x}_{t})+f(\\pmb{x}_{t}^{\\prime})]/2.}\\end{array}$ . ", "page_idx": 2}, {"type": "text", "text": "Throughout this paper, we make two key assumptions over the environment. We assume that the domain $\\mathcal{X}\\subset\\mathbb{R}^{d_{0}}$ is compact, and that the utility function lies in $\\mathcal{H}_{k}$ , a Reproducing Kernel Hilbert Space corresponding to some kernel function $k\\,\\in\\,\\mathcal{X}\\,\\times\\,\\mathcal{X}\\,\\rightarrow\\,\\mathbb{R}$ with a bounded RKHS norm $\\|\\bar{\\boldsymbol{f}}\\|_{k}\\le B$ . Without a loss of generality, we further suppose that the kernel function is normalized and $\\tilde{k}(\\mathbfit{x},\\mathbfit{x})\\leq1$ everywhere in the domain. Our set of assumptions extends the prior literature on logistic bandits and dueling bandits from linear rewards or finite action spaces, to continuous domains with non-parametric rewards. ", "page_idx": 2}, {"type": "text", "text": "While our theoretical framework targets euclidean domains, our methodology may be used on general domains of text or images, given vector embeddings obtained via unsupervised learning. Solving a bandit problem on top of embeddings from a pretrained language model is common practice in further fine-tuning of such models [e.g., Nguyen et al., 2024, Mehta et al., 2023a], and is further demonstrated in our Yelp experiment (c.f Section 6.3). Lastly, we highlight that our results may be smoothly extended to contextual bandits with stochastic context, by simply modifying the signature of the kernel function to $k^{\\prime}(\\pmb{x},\\pmb{x}^{\\prime},\\pmb{z}):\\mathcal{X}\\times\\mathcal{X}\\times\\mathcal{Z}\\rightarrow\\mathbb{R}.$ , where $z\\in{\\mathcal{Z}}$ denotes the context. This setting accommodates applications in active learning for fine-tuning of large language models, where the context is the prompt and the two actions are two alternative responses. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "4 Kernelized Confidence Sequences with Direct Logistic Feedback ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "As a warm-up, we consider a hypothetical scenario where $\\pmb{x}_{t}^{\\prime}\\;=\\;\\pmb{x}_{\\mathrm{null}}$ for all $t\\,\\geq\\,1$ such that $f(x_{\\mathrm{null}})=0$ . Therefore at every step, we suggest an action $\\pmb{x}_{t}$ and receive a noisy binary feedback $y_{t}$ , which is equal to one with probability $\\bar{s(f(x_{t}))}$ . This example reduces our problem to logistic bandits which has been rigorously analyzed for linear rewards [Filippi et al., 2010, Faury et al., 2020]. We extend prior work to the non-parametric setting by proposing a tractable loss function for estimating the utility function, a.k.a. reward. We present novel confidence intervals that quantify the uncertainty on the logistic predictions uniformly over the action domain. In doing so, we propose confidence sequences for the kernelized logistic likelihood model that are of independent interest for developing sample-efficient solvers for online and active classification. ", "page_idx": 3}, {"type": "text", "text": "The feedback $y_{t}$ is a Bernoulli random variable, and its likelihood depends on the utility function as $s(f(\\mathbf{x}_{t}))^{y_{t}}[1-s(f(\\mathbf{x}_{t}))]^{1-y_{t}}$ . Then given history $H_{t}$ , we can estimate $f$ by $f_{t}$ , the minimizer of the regularized negative log-likelihood loss ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{k}^{\\mathrm{L}}(f;H_{t}):=\\sum_{\\tau=1}^{t}-y_{\\tau}\\log\\left[s(f(x_{\\tau}))\\right]-(1-y_{\\tau})\\log\\left[1-s(f(x_{\\tau}))\\right]+\\frac{\\lambda}{2}\\|f\\|_{k}^{2}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\lambda>0$ is the regularization coefficient. The regularization term ensures that $\\|f_{t}\\|_{k}$ is finite and bounded. For simplicity, we assume throughout the main text that $\\|f_{t}\\|_{k}\\leq B$ . However, we do not need to rely on this assumption to give theoretical guarantees. In the appendix, we present a more rigorous analysis by projecting $f_{t}$ back into the RKHS ball of radius $B$ to ensure that the $B$ -boundedness condition is met, instead of assuming it. We do not perform this projection in our experiments. ", "page_idx": 3}, {"type": "text", "text": "Solving for $f_{t}$ may seem intractable at first glance since the loss is defined over functions in the large space of $\\mathcal{H}_{k}$ . However, it is common knowledge that the solution has a parametric form and may be calculated by using gradient descent. This is a direct application of the Representer Theorem [Sch\u00f6lkopf et al., 2001] and is detailed in Proposition 1. ", "page_idx": 3}, {"type": "text", "text": "Proposition 1 (Logistic Representer Theorem). The regularized negative log-likelihood loss of (3) has a unique minimizer $f_{t}$ , which takes the form $\\begin{array}{r}{f_{t}(\\cdot)=\\sum_{\\tau=1}^{t}\\alpha_{\\tau}k(\\cdot,\\pmb{x}_{\\tau})}\\end{array}$ where $(\\alpha_{1},\\ldots\\alpha_{t})=:$ $\\alpha_{t}\\in\\mathbb{R}^{t}$ is the minimizer of the strictly convex loss ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{k}^{\\mathrm{L}}(\\alpha;H_{t})=\\sum_{\\tau=1}^{t}-y_{\\tau}\\log\\left[s(\\alpha^{\\top}k_{t}({\\boldsymbol x}_{\\tau}))\\right]-(1-y_{\\tau})\\log\\left[1-s(\\alpha^{\\top}k_{t}({\\boldsymbol x}_{\\tau}))\\right]+\\frac{\\lambda}{2}\\|\\alpha\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "with $\\pmb{k}_{t}(\\pmb{x})=(k(\\pmb{x}_{1},\\pmb{x}),\\pmb{\\mathscr{\\mathscr{s}}}...,k(\\pmb{x}_{t},\\pmb{x}))\\in\\mathbb{R}^{t}.$ . ", "page_idx": 3}, {"type": "text", "text": "Given $f_{t}$ , we may predict the expected feedback for a point $\\textbf{\\em x}$ as $s(f_{t}(\\pmb{x}))$ . Centered around this prediction, we construct confidence sets of the form $[s(f_{t}(\\pmb{x}))\\pm\\beta_{t}(\\delta)\\partial_{t}(\\pmb{x})]$ , and show their uniform anytime validity. The width of the sets are characterized by $\\sigma_{t}({\\pmb x})$ defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\sigma_{t}^{2}(\\pmb{x}):=k(\\pmb{x},\\pmb{x})-k_{t}^{\\top}(\\pmb{x})(K_{t}+\\lambda\\kappa I_{t})^{-1}k_{t}(\\pmb{x})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\kappa\\,=\\,\\operatorname*{sup}_{a\\leq B}{1/\\dot{s}(a)}$ , with $\\dot{s}(a)\\,=\\,s(a)(1\\,-\\,s(a))$ denoting the derivative of the sigmoid function, and $K_{t}\\in\\mathbb{R}^{t\\times t}$ is the kernel matrix satisfying $[K_{t}]_{i,j}=k(\\pmb{x}_{i},\\pmb{x}_{j})$ . Our first main result shows that for a careful choice of $\\beta_{t}(\\delta)$ , these sets contain $s(f(\\bar{{\\pmb x}}))$ simultaneously for all $x\\in\\mathcal{X}$ and $t\\geq1$ with probability greater than $1-\\delta$ . ", "page_idx": 3}, {"type": "text", "text": "Theorem 2 (Kernelized Logistic Confidence Sequences). Assume $f\\in\\mathcal{H}_{k}$ and $\\|f\\|_{k}\\leq B$ . Let $0<\\delta<1$ and set ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\beta_{t}(\\delta):=4L B+2L\\sqrt{\\frac{2\\kappa}{\\lambda}(\\gamma_{t}+\\log1/\\delta)},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\begin{array}{r}{\\gamma_{t}:=\\operatorname*{max}_{x_{1},\\ldots,x_{t}}\\frac{1}{2}\\log\\operatorname*{det}(I_{t}+(\\lambda\\kappa)^{-1}K_{T})}\\end{array}$ , and $L:=\\operatorname*{sup}_{a\\leq B}\\dot{s}(a)$ . Then ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\forall t\\geq1,\\pmb{x}\\in\\mathcal{X}:\\,|s\\left(f_{t}(\\pmb{x})\\right)-s\\left(f(\\pmb{x})\\right)|\\leq\\beta_{t}(\\delta)\\sigma_{t}(\\pmb{x})\\right)\\geq1-\\delta.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Function-valued confidence sets around the kernelized ridge estimator are analyzed and used extensively to design bandit algorithms with noisy feedback on the true reward values [Valko et al., 2013, Srinivas et al., 2010, Chowdhury and Gopalan, 2017, Whitehouse et al., 2023]. However, under noisy logistic feedback, this literature falls short as the proposed confidence sets are no longer valid for the kernelized logistic estimator $f_{t}$ . One could still estimate $f$ using a kernelized ridge estimator and benefti from this line of work. However, as empirically demonstrated in Figure 1a, this will not be a sample-efficient approach. ", "page_idx": 4}, {"type": "text", "text": "Proof Sketch. When minimizing the kernelized logistic loss, we do not have a closed-form solution for $f_{t}$ , and can only formulate it using the fact that the gradient of the loss evaluated at $f_{t}$ is the null operator, i.e., $\\nabla{\\mathcal{L}}(f_{t};H_{t}):{\\mathcal{H}}\\to{\\mathcal{H}}=\\mathbf{0}$ . The key idea of our proof is to construct confidence intervals as $\\mathcal{H}$ -valued ellipsoids in the gradient space and show that the gradient operator evaluated at $f$ belongs to it with high probability (c.f. Lemma 8). We then translate this back into intervals around point estimates $s(f_{t}(\\pmb{x}))$ uniformly for all points $x\\in\\mathcal{X}$ . The complete proof is deferred to Appendix A, and builds on the results of Faury et al. [2020] and Whitehouse et al. [2023]. ", "page_idx": 4}, {"type": "text", "text": "Logistic Bandits. Such confidence sets are an integral tool for action selection under uncertainty, and bandit algorithms often rely on them to balance exploration against exploitation. To demonstrate how Theorem 2 may be used for bandit optimization with direct logistic feedback, we consider LGP-UCB, the kernelized Logistic GP-UCB algorithm. Presented in Algorithm 2, it extends the optimistic algorithm of Faury et al. [2020] from the linear to the kernelized setting, by using the confidence bound of Theorem 2 to calculate an optimistic estimate of the reward. We proceed to show that LGP-UCB attains a sublinear logistic regret, which is commonly defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\nR^{\\mathrm{L}}(T)=\\sum_{i=1}^{T}s(f(\\pmb{x}^{\\star}))-s(f(\\pmb{x}_{t})).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "To the best of our knowledge, the following corollary presents the first regret bound for logistic bandits in the kernelized setting and may be of independent interest. ", "page_idx": 4}, {"type": "text", "text": "Corollary 3. Let $\\delta\\in(0,1]$ and choose the exploration coefficients $\\beta_{t}(\\delta)$ as described in Theorem 2 for all $t\\geq0$ . Then LGP-UCB satisfies the anytime cumulative regret guarantee of ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(\\forall T\\geq0:R^{\\mathrm{{L}}}(T)\\leq C_{L}\\beta_{T}(\\delta)\\sqrt{T\\gamma_{t}}\\right)\\geq1-\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $C_{L}:=\\sqrt{8/\\log(1+(\\lambda\\kappa)^{-1})}$ . ", "page_idx": 4}, {"type": "text", "text": "5 Main Results: Bandits with Preference Feedback ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We return to our main problem setting in which a pair of actions, $\\pmb{x}_{t}$ and $\\mathbf{\\boldsymbol{x}}_{t}^{\\prime}$ , are chosen and the observed response is a noisy binary indicator of $\\pmb{x}_{t}$ yielding a higher utility than $\\ensuremath{\\boldsymbol{{x}}}_{t}^{\\prime}$ . While this type of feedback is more consistent in practice, it creates quite a challenging problem compared to the logistic problem of Section 4. The search space for action pairs $\\mathcal X\\times\\mathcal X$ is significantly larger than $\\mathcal{X}$ , and the observed preference feedback of $s(f(\\mathbf{x}_{t})-f(\\mathbf{x}_{t}^{\\prime}))$ conveys only relative information between two actions. We start by presenting a solution to estimate $f$ and obtain valid confidence sets under preference feedback. Using these confidence sets we then design the MAXMINLCB algorithm which chooses action pairs that are not only favorable, i.e., yield high utility, but are also informative for improving the utility confidence sets. ", "page_idx": 4}, {"type": "text", "text": "5.1 Preference-based Confidence Sets ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We consider the probabilistic model of $y_{t}$ as stated in Section 3, and write the corresponding regularized negative loglikelihood loss as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathcal{L}_{k}^{\\mathrm{D}}(f;H_{t}):=\\displaystyle\\sum_{\\tau=1}^{t}-y_{\\tau}\\log\\left[s(f(\\pmb{x}_{\\tau})-f(\\pmb{x}_{\\tau}^{\\prime}))\\right]}&{}\\\\ {-\\left(1-y_{\\tau}\\right)\\log\\left[1-s\\left(f(\\pmb{x}_{\\tau})-f(\\pmb{x}_{\\tau}^{\\prime})\\right)\\right]+\\frac{\\lambda}{2}\\|f\\|_{k}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This loss may be optimized over different function classes and is commonly used for linear dueling bandits [e.g., Saha, 2021], and has been notably successful in reinforcement learning with human feedback [Christiano et al., 2017]. We proceed to show that the preference-based loss $\\mathcal{L}_{k}^{\\mathrm{D}}$ is equivalent to $\\mathcal{L}_{k^{\\mathrm{D}}}^{\\mathrm{L}}$ , the standard logistic loss (3) invoked with a specific kernel function $k^{\\mathrm{D}}$ . This will allow us to cast the problem of inference with preference feedback as a kernelized logistic regression problem. To this end, we define the dueling kernel as ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\nk^{\\mathrm{D}}\\big(({\\pmb x}_{1},{\\pmb x}_{1}^{\\prime}),({\\pmb x}_{2},{\\pmb x}_{2}^{\\prime})\\big):=k({\\pmb x}_{1},{\\pmb x}_{2})+k({\\pmb x}_{1}^{\\prime},{\\pmb x}_{2}^{\\prime})-k({\\pmb x}_{1},{\\pmb x}_{2}^{\\prime})-k({\\pmb x}_{1}^{\\prime},{\\pmb x}_{2})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for all $(x_{1},x_{1}^{\\prime}),(x_{2},x_{2}^{\\prime})\\in\\mathcal{X}\\times\\mathcal{X}$ , and let $\\mathcal{H}_{k^{\\mathrm{D}}}$ be the RKHS corresponding to it. While the two function spaces $\\mathcal{H}_{k^{\\mathrm{D}}}$ and $\\mathcal{H}_{k}$ are defined over different input domains, we can show that they are isomorphic, under simple regularity conditions. ", "page_idx": 5}, {"type": "text", "text": "Proposition 4. Let $f:\\mathcal{X}\\to\\mathbb{R}$ . Consider a kernel $k$ and the sequence of its eigenfunctions $(\\phi_{i})_{i=1}^{\\infty}$ . Assume the eigenfunctions are zero-mean, i.e. $\\textstyle\\int_{x\\in{\\mathcal{X}}}\\phi_{i}(x)\\mathrm{d}x={\\overline{{0}}}$ . Then $f\\in\\mathcal{H}_{k}$ , if and only if there exists $h\\in\\mathcal{H}_{k^{\\mathrm{D}}}$ such that $h(\\pmb{x},\\pmb{x}^{\\prime})=f(\\pmb{x})-f(\\pmb{x}^{\\prime})$ . Moreover, $\\|h\\|_{k^{\\mathrm{D}}}=\\|f\\|_{k}$ . ", "page_idx": 5}, {"type": "text", "text": "The proof is left to Appendix C.1. The assumption on eigenfunctions in Proposition 4 is primarily made to simplify the equivalence class. In particular, the relative preference function $h$ can only capture the utility $f$ up to a bias, i.e., if a constant bias $b$ is added to all values of $f$ , the corresponding $h$ function will not change. The value of $b$ may not be recovered by drawing queries from $h$ however, this will not cause issues in terms of identifying arg max of $f$ through querying values of $h$ . Therefore, we set $b=0$ by assuming that eigenfunctions of $k$ are zero-mean. This assumption automatically holds for all kernels that are translation or rotation invariant over symmetric domains, since their eigenfunctions are periodic $L_{2}(\\mathcal{X})$ basis functions, e.g., Mat\u00e9rn kernels and sinusoids. Proposition 4 allows us to re-write the preference-based loss function of (6) as a logistic-type loss ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{k^{\\mathrm{D}}}^{\\mathrm{L}}(h;H_{t})=\\sum_{\\tau=1}^{t}-y_{\\tau}\\log\\left[s(h(x_{\\tau},x_{\\tau}^{\\prime}))\\right]-(1-y_{\\tau})\\log\\left[1-s\\left(h(x_{\\tau},x_{\\tau}^{\\prime})\\right)\\right]+\\frac{\\lambda}{2}\\|h\\|_{k^{\\mathrm{D}}}^{2},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "that is equivalent to (3) up to the choice of kernel. We define the minimizer $h_{t}:=\\arg\\operatorname*{min}\\mathcal{L}_{k_{\\cdot}^{\\mathrm{D}}}^{\\mathrm{L}}(h;H_{t})$ and use it to construct anytime valid confidence sets for the utility $f$ given only preference feedback. ", "page_idx": 5}, {"type": "text", "text": "Corollary 5 (Kernelized Preference-based Confidence Sequences). Assume $f\\in\\mathcal{H}_{k}$ and $\\|f\\|_{k}\\leq B$ . Choose $0<\\delta<1$ and set $\\beta_{t}^{\\mathrm{D}}(\\delta)$ and $\\sigma_{t}^{\\mathrm{D}}$ as in equations (4) and (5), with $k^{D}$ used as the kernel function. Then, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(\\forall t\\geq1,\\pmb{x},\\pmb{x}^{\\prime}\\in\\mathcal{X}:\\,|s\\left(h_{t}(\\pmb{x},\\pmb{x}^{\\prime})\\right)-s\\left(f(\\pmb{x})-f(\\pmb{x}^{\\prime})\\right)|\\leq\\beta_{t}^{\\mathrm{D}}(\\delta)\\sigma_{t}^{D}(\\pmb{x},\\pmb{x}^{\\prime})\\right)\\geq1-\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $h_{t}=\\arg\\operatorname*{min}\\mathcal{L}_{k^{\\mathrm{D}}}^{\\mathrm{L}}(h;H_{t})$ . ", "page_idx": 5}, {"type": "text", "text": "Corollary 5 gives valid confidence sets for kernelized utility functions under preference feedback and immediately improves prior results on linear dueling bandits and kernelized dueling bandits with regression-type loss, to kernelized setting with logistic-type likelihood. To demonstrate this, in Appendix C.3 we present the kernelized extensions of MAXINP (Saha [2021], Algorithm 3), and IDS (Kirschner and Krause [2021], Algorithm 4) and prove the corresponding regret guarantees (cf. Theorems 15 and 16). Corollary 5 holds almost immediately by invoking Theorem 2 with the dueling kernel $k^{\\mathrm{D}}$ and applying Proposition 4. A proof is provided in Appendix C.1 for completeness. ", "page_idx": 5}, {"type": "text", "text": "Comparison to Prior Work. A line of previous work assumes that both $f$ and the probability $s(f(x))$ are $B$ -bounded members of $\\mathcal{H}_{k}$ . This allows them to directly estimate $s(f(x))$ via kernelized linear regression [Xu et al., 2020, Mehta et al., 2023b, Kirschner and Krause, 2021]. The resulting confidence intervals are then around the least squares estimator, which does not align with the logistic estimator $f_{t}$ . This model does not encode the fact that $s(f(x))$ only takes values in [0, 1] and considers a sub-Gaussian distribution for $y_{t}$ , instead of the Bernoulli formulation when calculating the likelihood. Therefore, the resulting algorithms require more samples to learn an accurate reward estimate. In a concurrent work, Xu et al. [2024] also consider the loss function of Equation (6) and present likelihood-ratio confidence sets. The width of the sets at time $T$ , scales with $\\sqrt{T\\log\\mathcal{N}(\\mathcal{H}_{k};1/T)}$ where the second term is the metric entropy of the $B$ -bounded RKHS at resolution $1/T$ , that is, the log-covering number of this function class, using balls of radius $1/T$ . It is known that log $\\mathcal{N}(\\mathcal{H}_{k};1/T)\\stackrel{\\mathcal{\\chi}}{\\sim}\\gamma_{T}$ as defined in Theorem 2. This may be easily verified using Wainwright [2019, Example 5.12] and [Vakili et al., 2021, Definition 1]. Noting the definition of $\\beta_{t}^{\\mathrm{D}}$ , we see that likelihood ratio sets of Xu et al. [2024] are wider than Corollary 5. Consequently, the presented regret guarantee in this work is looser by a factor of $T^{1/4}$ compared to our bound in Theorem 6. ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 MAXMINLCB ", "text_level": 1, "page_idx": 6}, {"type": "equation", "text": "$$\n\\pmb{x}_{t}=\\arg\\operatorname*{max}_{\\pmb{x}\\in\\mathcal{M}_{t}}s\\big(h_{t}\\big(\\pmb{x},\\pmb{x}^{\\prime}(\\pmb{x})\\big)\\big)-\\beta_{t}^{\\mathrm{D}}\\sigma_{t}^{\\mathrm{D}}\\big(\\pmb{x},\\pmb{x}^{\\prime}(\\pmb{x})\\big)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{s.t.}\\ x^{\\prime}({\\pmb x})=\\arg\\operatorname*{min}_{{\\pmb x}^{\\prime}\\in\\mathcal{M}_{t}}s(h_{t}({\\pmb x},{\\pmb x}^{\\prime}))-\\beta_{t}^{\\mathrm{D}}\\sigma_{t}^{\\mathrm{D}}({\\pmb x},{\\pmb x}^{\\prime})\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Observe $y_{t}$ and append history. Update $h_{t+1}$ and $\\tilde{\\sigma}_{t+1}^{\\mathrm{D}}$ and the set of plausible maximizers ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{M}_{t+1}=\\{x\\in\\mathcal{X}|\\,\\forall\\pmb{x}^{\\prime}\\in\\mathcal{X}:\\,s(h_{t+1}(\\pmb{x},\\pmb{x}^{\\prime}))+\\beta_{t+1}^{\\mathrm{D}}\\sigma_{t+1}^{\\mathrm{D}}(\\pmb{x},\\pmb{x}^{\\prime})\\geq0.5\\}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "end for ", "page_idx": 6}, {"type": "text", "text": "5.2 Action Selection Strategy ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Let $\\begin{array}{r}{{\\pmb x}^{\\prime}({\\pmb x})=\\arg\\operatorname*{min}_{{\\pmb x}^{\\prime}\\in\\mathcal{M}_{t}}\\mathrm{LCB}_{t}({\\pmb x},{\\pmb x}^{\\prime})}\\end{array}$ denote a response function. We propose MAXMINLCB in Algorithm 1 for the preference feedback bandit problem that selects $\\pmb{x}_{t}$ and $\\mathbf{\\boldsymbol{x}}_{t}^{\\prime}$ jointly as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{x}_{t}=\\arg\\underset{\\pmb{x}\\in\\mathcal{M}_{t}}{\\operatorname*{max}}\\,\\mathrm{LCB}_{t}(\\pmb{x},\\pmb{x}^{\\prime}(\\pmb{x}))\\quad\\mathrm{(Leader)}}\\\\ &{\\pmb{x}_{t}^{\\prime}=\\pmb{x}^{\\prime}(\\pmb{x}_{t})}&{\\mathrm{(Follower)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where the lower-confidence bound $\\begin{array}{r l r}{\\mathrm{LCB}_{t}({\\pmb x},{\\pmb x}^{\\prime})\\!}&{{}=}&{\\!{s}\\big(h_{t}({\\pmb x},{\\pmb x}^{\\prime})\\big)\\;-\\;\\beta_{t}^{\\mathrm{D}}\\sigma_{t}^{\\mathrm{D}}({\\pmb x},{\\pmb x}^{\\prime})}\\end{array}$ presents a pessimistic estimate of $h$ and $\\mathcal{M}_{t}=\\{\\pmb{x}\\in\\mathcal{X}|\\,\\forall\\pmb{x}^{\\prime}\\in\\mathcal{X}:\\,s(h_{t}(\\pmb{x},\\pmb{x}^{\\prime}))+\\beta_{t}^{\\mathrm{D}}\\sigma_{t}^{\\mathrm{D}}(\\pmb{x},\\pmb{x}^{\\prime})\\geq0.5$ } is the set of potentially optimal actions. The second action is chosen as $\\pmb{x}_{t}^{\\prime}=\\pmb{x}^{\\prime}(\\pmb{x}_{t})$ . Equation (7) forms a zero-sum Stackelberg (Leader\u2013Follower) game where the actions $\\pmb{x}_{t}$ and $\\mathbf{\\boldsymbol{x}}_{t}^{\\prime}$ are chosen sequentially [Stackelberg, 1952]. First, the Leader selects $\\pmb{x}_{t}$ , then the Follower selects $\\mathbf{\\boldsymbol{x}}_{t}^{\\prime}$ depending on the choice of $\\pmb{x}_{t}$ . Importantly, due to the sequential nature of action selection, $\\pmb{x}_{t}$ is chosen by the Leader such that the Follower\u2019s action selection function, $\\pmb{x}^{\\prime}(\\cdot)$ , is accounted for in the selection of $\\pmb{x}_{t}$ Sequential optimization problems are known to be computationally NP-hard even for linear functions [Jeroslow, 1985]. However, due to their importance in practical applications, there are algorithms that can efficiently approximate a solution over large domains [Sinha et al., 2017, Ghadimi and Wang, 2018, Dagr\u00e9ou et al., 2022, Camacho-Vallejo et al., 2023]. ", "page_idx": 6}, {"type": "text", "text": "MAXMINLCB builds on a simple insight: if the utility $f$ is known, both the Leader and the Follower will choose $x^{\\star}$ yielding an objective value 0.5 for both players, and zero dueling regret. Since MAXMINLCB has no access to $f$ , it leverages the confidence sets of Corollary 5 and uses a pessimistic approach by considering the LCB instead. There are two crucial properties of the Follower specific to this game. First, the Follower can not do worse than the Leader with respect to the $\\mathrm{LCB}_{t}$ . In any scenario, the Follower can match the Leader\u2019s action which results in $\\mathrm{LCB}_{t}({\\pmb x}_{t},{\\pmb x}_{t}^{\\prime})=0.5$ . Second, for sufficiently tight confidence sets, the Follower will not select sub-optimal actions. In this case, the Leader\u2019s best action must be optimal as it anticipates the Follower\u2019s response and Equation (7) recovers the optimal actions. Therefore, the objective value of the game considered in Equation (7) is always less than, or equal to the objective of the game with known utility function $f$ , i.e., $\\mathrm{LCB}_{t}({\\pmb x}_{t},{\\pmb x}_{t}^{\\prime})\\,\\le\\,0.5\\,=\\,f(\\pmb{\\dot{x}}^{\\star},{\\pmb x}^{\\star})$ and the gap shrinks with the confidence sets. Overall, the Stackelberg game in Equation (7) can be considered as a lower approximation of the game played with known utility function $f$ . ", "page_idx": 6}, {"type": "text", "text": "The primary challenge for MAXMINLCB is to sample action pairs that sufficiently shrink the confidence sets for the optimal actions without accumulating too much regret. MAXMINLCB balances this exploration-exploitation trade-off naturally with its game theoretic formulation. We view the selection of $\\pmb{x}_{t}$ to be exploitative by trying to maximize the unknown utility $f(\\pmb{x}_{t})$ and minimizing regret. On the other hand, $\\ensuremath{\\boldsymbol{{x}}}_{t}^{\\prime}$ is chosen to be the most competitive opponent to $\\pmb{x}_{t}$ , i.e., testing whether the condition $\\mathrm{LCB}_{t}(\\pmb{x}_{t},\\pmb{x}_{t}^{\\prime})\\geq0.5$ holds. Note that $\\mathrm{LCB}_{t}$ is pessimistic concerning $\\pmb{x}_{t}$ making it robust against the uncertainty in the confidence set estimation. At the same time, $\\mathrm{LCB}_{t}$ is an optimistic estimate for $\\mathbf{\\boldsymbol{x}}_{t}^{\\prime}$ encouraging exploration. In our main theoretical result, we prove that under the assumptions of Corollary 5, MAXMINLCB achieves sublinear regret on the dueling bandit problem. ", "page_idx": 6}, {"type": "image", "img_path": "wIE991zhXH/tmp/c7e94799065340debb7706f19159bdfc379a8626c5c0079d363adbca234fa034.jpg", "img_caption": ["Figure 1: Regret of learning the Ackley function with logistic and preference feedback. (a) Same UCB algorithms, each using a different confidence set. LGP-UCB performs best, showcasing the power of Theorem 2. (b): Algorithms with different acquisition functions, all using our confidence sets. MAXMINLCB is more sample-efficient. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Theorem 6. Suppose the utility function $f$ lies in $\\mathcal{H}_{k}$ with a norm bounded by $B$ , and that kernel $k$ satisfies the assumption of Proposition 4. Let $\\delta\\in(0,1]$ and choose the exploration coefficient $\\beta_{t}^{\\mathrm{D}}(\\delta)$ as in Corollary 5. Then MAXMINLCB satisfies the anytime dueling regret of ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(\\forall T\\geq0:R^{\\mathrm{D}}(T)\\leq C_{3}\\beta_{T}^{\\mathrm{D}}(\\delta)\\sqrt{T\\gamma_{T}^{\\mathrm{D}}}=\\mathcal{O}(\\gamma_{T}^{\\mathrm{D}}\\sqrt{T})\\right)\\geq1-\\delta}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\gamma_{T}^{\\mathrm{D}}$ is the $T$ -step information gain of kernel $k^{\\mathrm{D}}$ and $C_{3}=(8+2\\kappa)/\\sqrt{\\log(1+4(\\lambda\\kappa)^{-1})}.$ . ", "page_idx": 7}, {"type": "text", "text": "The proof is left to Appendix C.2. The information gain $\\gamma_{T}^{\\mathrm{D}}$ in Theorem 6 quantifies the structural complexity of the RKHS corresponding to $k^{\\mathrm{D}}$ and its dependence on $T$ is fairly understood for kernels commonly used in applications of bandit optimization. As an example, for a Mat\u00e9rn kernel of smoothness $\\nu$ defined over a $d$ -dimensional domain, $\\gamma_{T}=\\tilde{\\mathcal{O}}(T^{d/(2\\nu+d)})$ [Remark 2, Vakili et al., 2021] and the corresponding regret bound grows sublinearly with $T$ . ", "page_idx": 7}, {"type": "text", "text": "Restricting the optimization domain to $\\mathcal{M}_{t}\\subset\\mathcal{X}$ is common in the literature [Zoghi et al., 2014a, Saha, 2021] despite being challenging in applications with large or continuous domains. We conjecture that MAXMINLCB would enjoy similar regret guarantees without restricting the selection domain to $\\mathcal{M}_{t}$ as done in Equation (7). This claim is supported by our experiments in Section 6.2 which are carried out without such restriction on the optimization domain. ", "page_idx": 7}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Our experiments are on finding the maxima of test functions commonly used in (non-convex) optimization literature [Jamil and Yang, 2013], given only preference feedback. These functions cover challenging optimization landscapes including several local optima, plateaus, and valleys, allowing us to test the versatility of MAXMINLCB. We use the Ackley function for illustration in the main text and provide the regret plots for the remainder of the functions in Appendix E. For all experiments, we set the horizon $T=2000$ and evaluate all algorithms on a uniform mesh over the input domain of size 100. Additionally, we conducted experiments on the Yelp restaurant review dataset to demonstrate the applicability of MAXMINLCB on real-world data and its scaling to larger domains. All experiments are run across 20 random seeds and reported values are averaged over the seeds, together with standard error. The environments and algorithms are implemented2end-to-end in JAX [Bradbury et al., 2018]. ", "page_idx": 7}, {"type": "text", "text": "6.1 Benchmarking Confidence Sets ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Performance of MAXMINLCB relies on validity and tightness of the LCB. We evaluate the quality of our kernelized confidence bounds, using the potentially simpler task of bandit optimization given logistic feedback. To this end, we fix the acquisition function for the logistic bandit algorithms to the Upper Confidence Bound (UCB) function, and benchmark different methods for calculating the confidence bound. We refer to the algorithm instantiated with the confidence sets of Theorem 2 as LGP-UCB (c.f. Algorithm 2). The IND-UCB approach assumes that actions are uncorrelated, and maintains an independent confidence interval for each action as in Lattimore and Szepesv\u00e1ri [2020, Algorithm 3]. This demonstrates how LGP-UCB utilizes the correlation between actions. We also implement LOG-UCB1 [Faury et al., 2020] that assumes that $f$ is a linear function, i.e., $f(\\pmb{x})=\\theta^{T}\\pmb{x}$ to highlight the improvements gained by kernelization. Last, we compare LGP-UCB with GP-UCB [Srinivas et al., 2010] that estimates probabilities $s(f(\\cdot))$ via a kernelized ridge regression task. This comparison highlights the benefits of using our kernelized logistic estimator (Proposition 1) over regression-based approaches [Xu et al., 2020, Kirschner and Krause, 2021, Mehta et al., 2023b,a]. Figure 1a shows that the cumulative regret of LGP-UCB is the lowest among the baselines. GP-UCB performs closest to LGP-UCB, however, it accumulates regret linearly during the initial steps. Note that GP-UCB and LGP-UCB differ in the estimation of the utility function $f_{t}$ while estimating the width of the confidence bounds similarly. This result suggests that using the logistic-type loss (3) to infer the utility function is advantageous. As expected, IND-UCB converges at a slower rate than LGP-UCB and GP-UCB due to ignoring the correlation between arms while LOG-UCB1\u2019s regret grows linearly as the Ackley function is misspecified under the assumption of linearity. We defer the results on the rest of the utility functions to Table 2 in Appendix $\\mathrm{E}$ and the figures therein. ", "page_idx": 7}, {"type": "table", "img_path": "wIE991zhXH/tmp/5b1dfa22c8c58c2da973613567c074bc2fa38036f11c0c5a835dc6d0aaed7c59.jpg", "table_caption": ["Table 1: Benchmarking $R_{T}^{\\mathrm{D}}$ for a variety of test utility functions, $T=2000$ . The top 3 rows show results for smoother functions without steep gradients and local optima while the bottom 5 rows show the results for more challenging problems. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "6.2 Benchmarking Acquisition Functions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we compare MAXMINLCB with other utility-based bandit algorithms. To isolate the benefits of our acquisition function, we instantiate all algorithms with the same confidence sets, and use our improved preferred-based bound of Corollary 5. Therefore, our implementation differs from the corresponding references, while we refer to the algorithms by their original name. We consider the following baselines. DOUBLER and MULTISBM [Ailon et al., 2014] choose $\\pmb{x}_{t}$ as a reference action from the recent history of actions and pair it with $\\ensuremath{\\boldsymbol{{x}}}_{t}^{\\prime}$ which maximizes the joint UCB (cf. Algorithm 5 and 6). RUCB [Zoghi et al., 2014a] chooses $\\ensuremath{\\boldsymbol{{x}}}_{t}^{\\prime}$ similarly, however, it selects the reference action uniformly at random from $\\mathcal{M}_{t}$ (Algorithm 7). MAXINP [Saha, 2021] also maintains the set of plausible maximizers $\\mathcal{M}_{t}$ , and at each time step, it selects the pair of actions that maximize $\\sigma_{t}^{D}(x,x^{\\prime})$ (Algorithm 3). IDS [Kirschner and Krause, 2021] selects the reference action greedily by maximizing $f_{t}$ , and pairs it with an informative action (Algorithm 4). Notably, all algorithms, with the exception of MAXINP, choose one of the actions independently and use it as a reference point when selecting the other one. Figure 3 illustrates the differences in action selection between UCB, maximum information, and MAXMINLCB approaches. We note that POP-BO [Xu et al., 2024] and MULTISBM only differ in the estimation of the confidence set. Since we deploy the same confidence set for all acquisition functions, the two algorithms are equivalent and we use MULTISBM in our results, however, comparisons hold for POP-BO as well. ", "page_idx": 8}, {"type": "text", "text": "Figure 1b benchmarks the algorithms using the Ackley utility function, where MAXMINLCB outperforms the baselines. All algorithms suffer from close-to-linear regret during the initial stages of learning, suggesting that there is an inevitable exploration phase. Notably, MAXMINLCB, IDS, and DOUBLER are the first to select actions with high utility, while RUCB and MAXINP explore for longer. Table 1 shows the dueling regret for all utility functions. MAXMINLCB consistently outperforms the baselines across the analyzed functions and achieves a low standard error, supporting its efficiency in balancing exploration and exploitation in the preference feedback setting. While MULTISBM is consistently outperformed, we do not observe a clear ranking among the rest of the baselines. For instance, IDS achieves the smallest regret for optimizing Matyas, while RUCB excels on the Branin function. This indicates the challenges each function offers and the performance of the action selection is task dependent. The consistent performance of MAXMINLCB demonstrates its robustness against the underlying unknown utility function. ", "page_idx": 8}, {"type": "image", "img_path": "wIE991zhXH/tmp/c4cb70e2574ceda3711954610adb07d541362bd57791c9efa6e460ca1031d139.jpg", "img_caption": ["Figure 2: Regret of learning restaurant recommendations using the Yelp open dataset with preference feedback. Algorithms with different acquisition functions, all using our confidence sets. MAXMINLCB is more sample-efficient. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6.3 Real-world Experiment ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "To further demonstrate the scalability and applicability of MAXMINLCB, we conduct an experiment on the Yelp open dataset of restaurant reviews that we use to design a complex real-world problem. The goal of this experiment is to identify restaurants for each user that fti their respective preferences the best via sequential recommendations. The final dataset after cleaning consists of 275 restaurants and 20 users. We define the action space $\\mathcal{X}$ by assigning to each restaurant their respective 32- dimensional embedding of their reviews, i.e., $\\dot{\\boldsymbol{\\chi}}\\subseteq\\breve{\\mathbb{R}}^{32}$ . The dataset provides utility values for users in the form of ratings on the scale of 1 to 5, however, not all users rated every restaurant. We estimate missing ratings using collaborative flitering [Schafer et al., 2007]. Further details on the data processing are deferred to Appendix D.1. ", "page_idx": 9}, {"type": "text", "text": "Figure 2 shows that the results of this larger problem align with previous conclusions. MAXMINLCB remains the best-performing algorithm with MULTISBM following second. The cumulative regret is also reported in Table 1. Note that neither of the algorithms is tuned or modified for this experiment. These results are only intended to demonstrate that 1) the computations easily scale, and 2) the kernelized approach is still applicable in a text-based domain, by using high-quality vector embeddings. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We addressed the problem of bandit optimization with preference feedback over large domains and complex targets. We proposed MAXMINLCB, which takes a game-theoretic approach to the problem of action selection under comparative feedback, and naturally balances exploration and exploitation by constructing a zero-sum Stackelberg game between the action pairs. MAXMINLCB achieves a sublinear regret for kernelized utilities, and performs competitively across a range of experiments. Lastly, by uncovering the equivalence of learning with logistic or comparative feedback, we propose kernelized preference-based confidence sets, which may be employed in adjacent problems, such as reinforcement learning with human feedback. The technical setup considered in this work serves as a foundation for a number of applications in mechanism design, such as preference elicitation and welfare optimization from multiple feedback sources for social choice theory, which we leave as future work. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank Scott Sussex for the thorough feedback on the manuscript. This research was supported by the European Research Council (ERC) under the European Union\u2019s Horizon 2020 research and Innovation Program Grant agreement No. 815943. Barna P\u00e1sztor was supported by an ETH AI Center doctoral fellowship. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Yasin Abbasi-Yadkori. Online learning for linearly parametrized control problems. PhD thesis, University of Alberta, 2013. ", "page_idx": 10}, {"type": "text", "text": "Nir Ailon, Zohar Karnin, and Thorsten Joachims. Reducing dueling bandits to cardinal bandits. In International Conference on Machine Learning, pages 856\u2013864. PMLR, 2014. ", "page_idx": 10}, {"type": "text", "text": "Fabio Aiolli and Alessandro Sperduti. Learning preferences for multiclass problems. Advances in neural information processing systems, 17, 2004.   \nSheldon Axler. Measure, integration & real analysis. Springer Nature, 2020.   \nViktor Bengs, R\u00f3bert Busa-Fekete, Adil El Mesaoudi-Paul, and Eyke H\u00fcllermeier. Preference-based online learning with dueling bandits: A survey. The Journal of Machine Learning Research, 2021.   \nAlina Beygelzimer, David Pal, Balazs Szorenyi, Devanathan Thiruvenkatachari, Chen-Yu Wei, and Chicheng Zhang. Bandit multiclass linear classification: Efficient algorithms for the separable case. In International Conference on Machine Learning, pages 624\u2013633. PMLR, 2019.   \nJames Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, et al. JAX: composable transformations of Python $^{+}$ NumPy programs, 2018. URL http://github.com/google/ jax.   \nRalph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 1952.   \nEric Brochu, Vlad M Cora, and Nando De Freitas. A tutorial on bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning. arXiv preprint, 2010.   \nJos\u00e9-Fernando Camacho-Vallejo, Carlos Corpus, and Juan G Villegas. Metaheuristics for bilevel optimization: A comprehensive review. Computers & Operations Research, page 106410, 2023.   \nStephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, J\u00e9r\u00e9my Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, et al. Open problems and fundamental limitations of reinforcement learning from human feedback. Transactions on Machine Learning Research, 2023.   \nBangrui Chen and Peter I Frazier. Dueling bandits with weak regret. In International Conference on Machine Learning, pages 731\u2013739. PMLR, 2017.   \nSayak Ray Chowdhury and Aditya Gopalan. On kernelized multi-armed bandits. In International Conference on Machine Learning, pages 844\u2013853. PMLR, 2017.   \nPaul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017.   \nWei Chu and Zoubin Ghahramani. Preference learning with gaussian processes. In Proceedings of the 22nd international conference on Machine learning, pages 137\u2013144, 2005.   \nMathieu Dagr\u00e9ou, Pierre Ablin, Samuel Vaiter, and Thomas Moreau. A framework for bilevel optimization that enables stochastic and global variance reduction algorithms. Advances in Neural Information Processing Systems, 35:26698\u201326710, 2022.   \nMiroslav Dud\u00edk, Katja Hofmann, Robert E Schapire, Aleksandrs Slivkins, and Masrour Zoghi. Contextual dueling bandits. In Conference on Learning Theory, pages 563\u2013587. PMLR, 2015.   \nMoein Falahatgar, Alon Orlitsky, Venkatadheeraj Pichapati, and Ananda Theertha Suresh. Maximum selection and ranking under noisy comparisons. In International Conference on Machine Learning, pages 1088\u20131096. PMLR, 2017.   \nLouis Faury, Marc Abeille, Cl\u00e9ment Calauz\u00e8nes, and Olivier Fercoq. Improved optimistic algorithms for logistic bandits. In International Conference on Machine Learning, pages 3052\u20133060. PMLR, 2020.   \nLouis Faury, Marc Abeille, Kwang-Sung Jun, and Cl\u00e9ment Calauz\u00e8nes. Jointly efficient and optimal algorithms for logistic bandits. In International Conference on Artificial Intelligence and Statistics, pages 546\u2013580. PMLR, 2022.   \nSarah Filippi, Olivier Cappe, Aur\u00e9lien Garivier, and Csaba Szepesv\u00e1ri. Parametric bandits: The generalized linear case. Advances in neural information processing systems, 2010.   \nDylan J Foster and Akshay Krishnamurthy. Contextual bandits with surrogate losses: Margin bounds and efficient algorithms. Advances in Neural Information Processing Systems, 31, 2018.   \nSaeed Ghadimi and Mengdi Wang. Approximation methods for bilevel programming. arXiv preprint arXiv:1802.02246, 2018.   \nJavier Gonz\u00e1lez, Zhenwen Dai, Andreas Damianou, and Neil D Lawrence. Preferential bayesian optimization. In International Conference on Machine Learning, pages 1282\u20131291. PMLR, 2017.   \nNeil Houlsby, Ferenc Husz\u00e1r, Zoubin Ghahramani, and M\u00e1t\u00e9 Lengyel. Bayesian active learning for classification and preference learning. arXiv preprint arXiv:1112.5745, 2011.   \nKevin G Jamieson and Robert Nowak. Active ranking using pairwise comparisons. Advances in neural information processing systems, 24, 2011.   \nMomin Jamil and Xin-She Yang. A literature survey of benchmark functions for global optimisation problems. International Journal of Mathematical Modelling and Numerical Optimisation, 4(2): 150\u2013194, 2013.   \nRobert G Jeroslow. The polynomial hierarchy and a simple model for competitive analysis. Mathematical programming, 32(2):146\u2013164, 1985.   \nXiang Ji, Huazheng Wang, Minshuo Chen, Tuo Zhao, and Mengdi Wang. Provable beneftis of policy learning from human preferences in contextual bandit problems. arXiv preprint arXiv:2307.12975, 2023.   \nJohannes Kirschner and Andreas Krause. Bias-robust bayesian optimization via dueling bandits. In International Conference on Machine Learning. PMLR, 2021.   \nJohannes Kirschner, Tor Lattimore, and Andreas Krause. Information directed sampling for linear partial monitoring. In Conference on Learning Theory, pages 2328\u20132369. PMLR, 2020.   \nJunpei Komiyama, Junya Honda, and Hiroshi Nakagawa. Optimal regret analysis of thompson sampling in stochastic multi-armed bandit problem with multiple plays. In International Conference on Machine Learning, pages 1152\u20131161. PMLR, 2015.   \nWataru Kumagai. Regret analysis for continuous dueling bandit. Advances in Neural Information Processing Systems, 30, 2017.   \nTor Lattimore and Csaba Szepesv\u00e1ri. Bandit algorithms. Cambridge University Press, 2020.   \nPeter D Lax. Functional analysis, volume 55. John Wiley & Sons, 2002.   \nJunghyun Lee, Se-Young Yun, and Kwang-Sung Jun. Improved regret bounds of (multinomial) logistic bandits via regret-to-confidence-set conversion. In International Conference on Artificial Intelligence and Statistics, pages 4474\u20134482. PMLR, 2024.   \nViraj Mehta, Vikramjeet Das, Ojash Neopane, Yijia Dai, Ilija Bogunovic, Jeff Schneider, and Willie Neiswanger. Sample efficient reinforcement learning from human feedback via active exploration. arXiv preprint, 2023a.   \nViraj Mehta, Ojash Neopane, Vikramjeet Das, Sen Lin, Jeff Schneider, and Willie Neiswanger. Kernelized offline contextual dueling bandits. arXiv preprint, 2023b.   \nPetrus Mikkola, Milica Todorovic\u00b4, Jari J\u00e4rvi, Patrick Rinke, and Samuel Kaski. Projective preferential bayesian optimization. In International Conference on Machine Learning. PMLR, 2020.   \nR\u00e9mi Munos, Michal Valko, Daniele Calandriello, Mohammad Gheshlaghi Azar, Mark Rowland, Zhaohan Daniel Guo, Yunhao Tang, Matthieu Geist, Thomas Mesnard, Andrea Michi, et al. Nash learning from human feedback. arXiv preprint, 2023.   \nTung Nguyen, Qiuyi Zhang, Bangding Yang, Chansoo Lee, Jorg Bornschein, Yingjie Miao, Sagi Perel, Yutian Chen, and Xingyou Song. Predicting from strings: Language model embeddings for bayesian optimization. arXiv preprint, 2024.   \nAadirupa Saha. Optimal algorithms for stochastic contextual preference bandits. Advances in Neural Information Processing Systems, 34:30050\u201330062, 2021.   \nAadirupa Saha and Akshay Krishnamurthy. Efficient and optimal algorithms for contextual dueling bandits under realizability. In International Conference on Algorithmic Learning Theory. PMLR, 2022.   \nAadirupa Saha, Aldo Pacchiano, and Jonathan Lee. Dueling rl: Reinforcement learning with trajectory preferences. In Proceedings of The 26th International Conference on Artificial Intelligence and Statistics. PMLR, 2023.   \nJ Ben Schafer, Dan Frankowski, Jon Herlocker, and Shilad Sen. Collaborative flitering recommender systems. In The adaptive web: methods and strategies of web personalization, pages 291\u2013324. Springer, 2007.   \nBernhard Sch\u00f6lkopf, Ralf Herbrich, and Alex J Smola. A generalized representer theorem. In International conference on computational learning theory, pages 416\u2013426. Springer, 2001.   \nAnkur Sinha, Pekka Malo, and Kalyanmoy Deb. A review on bilevel optimization: From classical to evolutionary approaches and applications. IEEE transactions on evolutionary computation, 22(2): 276\u2013295, 2017.   \nNiranjan Srinivas, Andreas Krause, Sham Kakade, and Matthias Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In Proceedings of the 27th International Conference on International Conference on Machine Learning, 2010.   \nHeinrich von Stackelberg. Theory of the market economy. Oxford University Press, 1952.   \nYanan Sui, Vincent Zhuang, Joel W Burdick, and Yisong Yue. Multi-dueling bandits with dependent arms. arXiv preprint arXiv:1705.00253, 2017.   \nShion Takeno, Masahiro Nomura, and Masayuki Karasuyama. Towards practical preferential bayesian optimization with skew gaussian processes. In International Conference on Machine Learning, pages 33516\u201333533. PMLR, 2023.   \nMaegan Tucker, Ellen Novoseller, Claudia Kann, Yanan Sui, Yisong Yue, Joel W Burdick, and Aaron D Ames. Preference-based learning for exoskeleton gait optimization. In 2020 IEEE international conference on robotics and automation (ICRA). IEEE, 2020.   \nTanguy Urvoy, Fabrice Clerot, Raphael F\u00e9raud, and Sami Naamane. Generic exploration and k-armed voting bandits. In International Conference on Machine Learning. PMLR, 2013.   \nSattar Vakili, Kia Khezeli, and Victor Picheny. On information gain and regret bounds in gaussian process bandits. In International Conference on Artificial Intelligence and Statistics, pages 82\u201390. PMLR, 2021.   \nMichal Valko, Nathaniel Korda, R\u00e9mi Munos, Ilias Flaounas, and Nelo Cristianini. Finite-time analysis of kernelised contextual bandits. arXiv preprint arXiv:1309.6869, 2013.   \nMartin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cambridge university press, 2019.   \nJustin Whitehouse, Zhiwei Steven Wu, and Aaditya Ramdas. Improved self-normalized concentration in hilbert spaces: Sublinear regret for gp-ucb. arXiv preprint arXiv:2307.07539, 2023.   \nHuasen Wu and Xin Liu. Double thompson sampling for dueling bandits. Advances in neural information processing systems, 29, 2016.   \nWenjie Xu, Wenbin Wang, Yuning Jiang, Bratislav Svetozarevic, and Colin N Jones. Principled preferential bayesian optimization. arXiv preprint arXiv:2402.05367, 2024.   \nYichong Xu, Aparna Joshi, Aarti Singh, and Artur Dubrawski. Zeroth order non-convex optimization with dueling-choice bandits. In Conference on Uncertainty in Artificial Intelligence. PMLR, 2020.   \nYisong Yue and Thorsten Joachims. Interactively optimizing information retrieval systems as a dueling bandits problem. In Proceedings of the 26th Annual International Conference on Machine Learning, 2009.   \nYisong Yue, Josef Broder, Robert Kleinberg, and Thorsten Joachims. The k-armed dueling bandits problem. Journal of Computer and System Sciences, 2012.   \nWenhao Zhan, Masatoshi Uehara, Nathan Kallus, Jason D Lee, and Wen Sun. Provable offline reinforcement learning with human feedback. arXiv preprint, 2023.   \nBanghua Zhu, Michael Jordan, and Jiantao Jiao. Principled reinforcement learning with human feedback from pairwise or k-wise comparisons. In International Conference on Machine Learning, pages 43037\u201343067. PMLR, 2023.   \nJulian Zimmert and Yevgeny Seldin. Factored bandits. Advances in Neural Information Processing Systems, 31, 2018.   \nMasrour Zoghi, Shimon Whiteson, Remi Munos, and Maarten Rijke. Relative upper confidence bound for the k-armed dueling bandit problem. In International conference on machine learning. PMLR, 2014a.   \nMasrour Zoghi, Shimon A Whiteson, Maarten De Rijke, and Remi Munos. Relative confidence sampling for efficient on-line ranker evaluation. In Proceedings of the 7th ACM international conference on Web search and data mining, 2014b. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Contents of Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Proofs for Bandits with Logistic Feedback ", "page_idx": 14}, {"type": "text", "text": "B Helper Lemmas for Appendix A ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "C Proofs for Bandits with Preference Feedback 20   \nC.1 Equivalence of Preference-based and Logistic Losses 20   \nC.2 Proof of the Preference-based Regret Bound . 21   \nC.3 Extending Algorithms for Linear Dueling Bandits to Kernelized Setting 23   \nC.4 Helper Lemmas for Appendix C.3 27   \nD Details of Experiments 28   \nD.1 Yelp Experiment 30 ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "E Additional Experiments ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "30 ", "page_idx": 14}, {"type": "text", "text": "A Proofs for Bandits with Logistic Feedback ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We have written the equations in the main text in terms of kernel matrices and function evaluations, for easier readability. For the purpose of the proof however, we mainly rely on entities in the Hilbert space. Consider the operator $\\phi:\\mathcal{X}\\to\\mathcal{H}$ which corresponds to kernel $k$ and satisfies $k(\\pmb{x},\\cdot)=\\phi(\\pmb{x})$ . Then by Mercer\u2019s theorem, any $f\\in\\mathcal{H}_{k}$ may be written as $f=\\pmb{\\theta}^{\\top}\\pmb{\\phi}$ , where $\\pmb{\\theta}\\in\\ell_{2}(\\mathbb{N})$ and has a $B$ -bounded $\\ell_{2}$ norm. For a sequence of points $\\mathbf{\\boldsymbol{x}}_{1},\\ldots,\\mathbf{\\boldsymbol{x}}_{t}\\in\\mathcal{X}$ , we define the (infinite-dimensional) feature map $\\bar{{\\Phi}}_{t}=\\left[\\phi(\\pmb{x}_{1}),\\cdot\\cdot\\cdot^{\\circ},\\phi(\\pmb{x}_{t})\\right]^{\\intercal}$ , which gives rise to the kernel matrix $K_{t}:\\mathbb{R}^{t}\\rightarrow\\mathbb{R}^{t}$ and the covariance operator $S_{t}:\\mathcal{H}_{k}\\rightarrow\\mathcal{H}_{k}$ , respectively defined as $K_{t}=\\Phi_{t}\\boldsymbol{\\Phi}_{t}^{\\top}$ and $S_{t}=\\Phi_{t}^{\\top}\\Phi_{t}$ . Let $\\textstyle I_{t}$ denote the $t$ -dimensional identity matrix, and $I_{\\mathcal{H}}$ be the identity operator acting on the RKHS. Then it is widely known that the covariance and kernel operators are connected via $\\operatorname*{det}(I_{\\mathcal{H}}+\\rho^{-2}S_{t})=$ $\\operatorname*{det}(I_{t}+\\dot{\\rho}^{-2}K_{t})$ for any $t\\geq1$ and $\\rho\\neq0$ . For operators on the Hilbert space, $\\operatorname*{det}(A)$ refer to a Fredholm determinant [c.f. Lax, 2002]. Lastly, in the appendix, we refer to the true unknown utility function as $f^{\\star}({\\pmb x})=\\phi^{\\top}({\\pmb x})\\pmb\\theta^{\\star}$ . In the main text, the true utility is simply referred to as $f$ . ", "page_idx": 14}, {"type": "text", "text": "To analyze our function-valued confidence sequences, we start by re-writing the logistic loss function ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\pmb{\\theta};H_{t})=\\sum_{\\tau=1}^{t}-y_{\\tau}\\log s\\left(\\pmb{\\theta}^{\\top}\\phi(\\pmb{x}_{\\tau})\\right)-\\sum_{\\tau=1}^{t}(1-y_{\\tau})\\log\\left(1-s\\left(\\pmb{\\theta}^{\\top}\\phi(\\pmb{x}_{\\tau})\\right)\\right)+\\frac{\\lambda}{2}\\|\\pmb{\\theta}\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which is strictly convex in $\\pmb{\\theta}$ and has a unique minimizer $\\theta_{t}$ which satisfies ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\nabla\\mathcal{L}(\\pmb{\\theta}_{t};H_{t})=\\sum_{\\tau=1}^{t}{-y_{\\tau}\\phi(\\pmb{x}_{\\tau})+g_{t}(\\pmb{\\theta}_{t})}=0\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $g_{t}(\\pmb\\theta):\\mathcal{H}\\rightarrow\\mathcal{H}$ is a linear operator defined as ", "page_idx": 14}, {"type": "equation", "text": "$$\ng_{t}(\\pmb\\theta):=\\sum_{\\tau=1}^{t}\\phi(\\pmb x_{\\tau})s(\\pmb\\theta^{\\top}\\phi(\\pmb x_{\\tau}))+\\lambda\\pmb\\theta.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In the main text, we assumed that minimizer of $\\mathcal{L}$ satisfies the norm boundedness condition. Here, we present a more rigorous analysis which does not assume so. Instead, we work with a projected estimator defined via ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\pmb{\\theta}_{t}^{P}=\\arg\\operatorname*{min}_{\\|\\pmb{\\theta}\\|_{2}\\leq B}\\|\\pmb{g}_{t}(\\pmb{\\theta})-\\pmb{g}_{t}(\\pmb{\\theta}_{t})\\|_{V_{t}^{-1}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $V_{t}=S_{t}+\\kappa\\lambda I_{\\mathcal{H}}$ and $\\pmb{\\theta}_{t}$ is the minimizer of $\\mathcal{L}(\\pmb{\\theta};H_{t})$ . Roughly put, $\\pmb{\\theta}_{t}^{P}\\in\\mathcal{\\ell}_{2}(\\mathbb{N})$ is a norm $B$ -bounded alternative to $\\theta_{t}$ , who also satisfies a small $\\nabla\\mathcal{L}$ , and therefore, is expected to result in an accurate decision boundary. We will present our proof in terms of $\\theta_{t}^{P}$ . This also proves the results in the main text, since $\\pmb{\\theta}_{t}^{P}=\\pmb{\\dot{\\theta}}_{t}$ if $\\theta_{t}$ itself happens to have a $B$ -bounded norm, as assumed in the main text. Our analysis relies on a concentration bound for $\\mathcal{H}$ -valued martingale sequences stated in Abbasi-Yadkori [2013] and later in Whitehouse et al. [2023]. Below, we have adapted the statement to match our notation. ", "page_idx": 14}, {"type": "text", "text": "Lemma 7 (Corollary 1 Whitehouse et al. [2023]). Suppose the sequence $(\\mathbf{\\boldsymbol{x}}_{t})_{t\\ge1}$ is $(\\mathcal{F}_{t})_{t\\geq1}$ -adapted, where $\\mathcal{F}_{t}:=\\sigma\\left(\\pmb{x}_{1},\\dots,\\pmb{x}_{t},\\varepsilon_{1},\\dots,\\varepsilon_{t-1}\\right)$ and $\\varepsilon_{t}$ are i.i.d. zero-mean $\\sigma$ -subGaussian noise. Consider the RKHS $\\mathcal{H}$ corresponding to a kernel $k(\\pmb{x},\\pmb{x}^{\\prime})=\\phi^{\\top}(\\pmb{x})\\phi(\\pmb{x}^{\\prime})$ . Then, for any $\\rho>0$ and $\\delta\\in(0,1)$ , we have that, with probability at least $1-\\delta$ , simultaneously for all $t\\geq0$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left\\Vert\\sum_{\\tau\\leq t}\\varepsilon_{\\tau}\\phi(\\pmb{x}_{\\tau})\\right\\Vert_{V_{t}^{-1}}\\leq\\sigma\\sqrt{2\\log\\left(\\frac{1}{\\delta}\\sqrt{\\operatorname*{det}(I_{t}+\\rho^{-2}K_{t})}\\right)}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $V_{t}=S_{t}+\\rho^{2}I_{\\mathcal{H}}$ . ", "page_idx": 15}, {"type": "text", "text": "The following lemma, which extends Faury et al. [2020, Lemma 8] to $\\mathcal{H}$ -valued operators, expresses the closeness of $\\pmb{\\theta}_{t}$ and $\\theta^{\\star}$ in the gradient space, with respect to the norm of the covariance matrix. Lemma 8 (Gradient Space Confidence Bounds). Set $0<\\delta<1$ . Under the assumptions of Theorem 2 ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\forall t\\ge0:\\|g_{t}(\\pmb{\\theta}_{t})-g_{t}(\\pmb{\\theta}^{\\star})\\|_{V_{t}^{-1}}\\le\\frac{1}{2}\\sqrt{2\\log{1/\\delta}+2\\gamma_{T}}+\\sqrt{\\frac{\\lambda}{\\kappa}}B\\right)\\ge1-\\delta\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $V_{t}=S_{t}+\\kappa\\lambda I_{\\mathcal{H}}$ . ", "page_idx": 15}, {"type": "text", "text": "Proof of Lemma 8. Recall that $\\begin{array}{r}{g_{t}(\\pmb{\\theta})=\\sum_{\\tau\\leq t}s(\\pmb{\\theta}^{\\top}\\phi(\\pmb{x}_{\\tau}))\\phi(\\pmb{x}_{\\tau})+\\lambda\\pmb{\\theta}}\\end{array}$ . Then it is straighforward to show that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\nabla\\mathcal{L}(\\pmb{\\theta};H_{t})=\\sum_{\\tau\\leq t}y_{\\tau}\\phi(\\pmb{x}_{\\tau})-g_{t}(\\pmb{\\theta}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since $\\theta_{t}$ is a minimizer of $\\mathcal{L}_{t}$ , it holds that $\\begin{array}{r}{\\pmb{g}_{t}(\\pmb{\\theta}_{t})=\\sum_{\\tau\\leq t}y_{\\tau}\\phi(\\pmb{x}_{\\tau})}\\end{array}$ . This allows us to write, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|g_{t}(\\theta_{t})-g_{t}(\\theta^{\\star})\\|_{V_{t}^{-1}}=\\displaystyle\\left\\|\\sum_{\\tau\\leq t}\\left(y_{\\tau}-s(\\phi^{\\top}(x_{\\tau})\\theta^{\\star})\\right)\\phi(x_{\\tau})-\\lambda\\theta^{\\star}\\right\\|_{V_{t}^{-1}}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\displaystyle\\left\\|\\sum_{\\tau\\leq t}\\varepsilon_{\\tau}\\phi(x_{\\tau})\\right\\|_{V_{t}^{-1}}+\\lambda\\|\\theta^{\\star}\\|_{V_{t}^{-1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\varepsilon_{\\tau}\\,=\\,y_{\\tau}\\,-\\,s(\\phi^{\\top}(x_{\\tau})\\theta^{\\star})$ is a history dependent random variable in $[0,1]$ due to the data model. To bound the first term, we recognize that any random variable in $[0,1]$ is $\\sigma$ sub-Gaussian with $\\sigma=0.5$ and apply Lemma 7. We obtain that for all $t\\geq0$ , with probability greater than $1-\\delta$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\displaystyle\\sum_{\\tau\\le t}\\varepsilon_{\\tau}\\phi(\\pmb{x}_{\\tau})\\right\\|_{V_{t}^{-1}}\\le\\frac{1}{2}\\sqrt{2\\log\\left(\\frac{1}{\\delta}\\sqrt{\\operatorname*{det}(I_{t}+(\\lambda\\kappa)^{-1}K_{t})}\\right)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\le\\displaystyle\\frac{1}{2}\\sqrt{2\\log{1/\\delta}+2\\gamma_{T}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "$\\begin{array}{r}{\\gamma_{t}(\\rho)=\\operatorname*{sup}_{x_{1},\\ldots,x_{t}}\\frac{1}{2}\\log\\operatorname*{det}(I_{t}+\\rho^{-2}K_{t}))}\\end{array}$ . To bound the second term in (9), note that $S_{t}=$ $\\Phi_{t}^{\\top}\\Phi_{t}$ is PSD and therefore $V_{t}\\ge\\kappa\\lambda I_{\\mathcal{H}}$ . Then ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\lambda\\|\\pmb{\\theta}^{\\star}\\|_{V_{t}^{-1}}\\leq\\frac{\\lambda}{\\sqrt{\\lambda\\kappa}}\\|\\pmb{\\theta}^{\\star}\\|_{2}\\leq\\sqrt{\\frac{\\lambda}{\\kappa}}B.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "concluding the proof. ", "page_idx": 15}, {"type": "text", "text": "The following lemma shows the validity of our parameter-space confidence sets. ", "page_idx": 15}, {"type": "text", "text": "Lemma 9. Set $0<\\delta<1$ and consider the confidence sets ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Theta_{t}(\\delta):=\\left\\{\\|\\pmb{\\theta}\\|\\leq B,\\left\\|\\pmb{\\theta}-\\pmb{\\theta}_{t}^{P}\\right\\|_{V_{t}}\\leq2\\sqrt{\\lambda\\kappa}B+\\kappa\\sqrt{2\\log{1/\\delta}+2\\gamma_{T}}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\forall t\\geq0:\\,\\pmb{\\theta}^{\\star}\\in\\Theta_{t}(\\delta)\\right)\\geq1-\\delta\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof of Lemma 9. We check if $\\theta^{\\star}\\in\\Theta_{t}(\\delta)$ by bounding the following norm ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\big\\|\\theta^{\\star}-\\theta_{t}^{P}\\big\\|_{V_{t}}\\leq\\kappa\\big\\|g_{t}(\\theta^{\\star})-g_{t}(\\theta_{t}^{P})\\big\\|_{V_{t}^{-1}}}}\\\\ &{\\leq\\kappa\\,\\Big(\\|g_{t}(\\theta^{\\star})-g_{t}(\\theta_{t})\\|_{V_{t}^{-1}}+\\big\\|g_{t}(\\theta_{t})-g_{t}(\\theta_{t}^{P})\\big\\|_{V_{t}^{-1}}\\Big)}\\\\ &{\\leq2\\kappa\\|g_{t}(\\theta^{\\star})-g_{t}(\\theta_{t})\\|_{V_{t}^{-1}}}\\\\ &{\\leq\\kappa\\sqrt{2\\log{1/\\delta}+2\\gamma_{T}}+2\\sqrt{\\lambda\\kappa}B}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Lastly, we prove a formal extension of Theorem 2. ", "page_idx": 16}, {"type": "text", "text": "Theorem 10 (Theorem 2 - Formal). Set $0<\\delta<1$ and consider the confidence sets $\\mathcal{E}_{t}(\\delta)\\subset\\mathcal{H}_{k}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{E}_{t}(\\delta)=\\left\\{f(\\cdot)=\\pmb{\\theta}^{\\top}\\phi(\\cdot):\\pmb{\\theta}\\in\\Theta_{t}(\\delta)\\right\\}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then, simultanously for all $x\\in\\mathcal{X}$ , $f\\in\\mathcal{E}_{t}(\\delta)$ and $t\\geq0$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n|s(f(\\pmb{x}))-s(f^{\\star}(\\pmb{x}))|\\leq\\beta_{t}(\\delta)\\sigma_{t}(\\pmb{x})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with probability greater than $1-\\delta$ , where ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\beta_{t}(\\delta):=4L B+2L\\sqrt{\\frac{\\kappa}{\\lambda}}\\sqrt{2\\log{1/\\delta}+2\\gamma_{T}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof of Theorem $I O$ . For simplicity in notation let us define ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\tilde{\\beta}_{t}(\\delta):=2\\sqrt{\\lambda\\kappa}B+\\kappa\\sqrt{2\\log{1/\\delta}+2\\gamma_{t}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Suppose $f=\\pmb{\\theta}^{\\top}\\phi(\\cdot)$ is in $\\mathcal{E}_{t}(\\delta)$ . Then ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{|s(\\phi^{\\top}(\\mathbf{x})\\theta^{*})-s(\\phi^{\\top}(\\mathbf{x})\\theta)|=|=|=(s(x;\\theta,\\theta^{*})\\phi^{\\top}(\\mathbf{x})(\\theta-\\theta^{*})|}&{\\mathrm{Lem.~l.il}}\\\\ &{\\leq L|\\phi^{\\top}(\\mathbf{x})\\theta^{*}(\\theta-\\theta^{*})|}&{s\\mathrm{is~}L\\mathrm{Lipschitz}}\\\\ &{\\leq L|\\phi(\\mathbf{x})|_{V_{\\tau}^{-1}}|\\theta-\\theta^{*}|_{V_{\\tau}}}\\\\ &{\\leq L||\\phi(\\mathbf{x})|_{V_{\\tau}^{-1}}\\left(\\|\\theta-\\theta_{t}^{*}\\|_{V_{\\tau}}+\\left\\|\\theta_{t}^{p}-\\theta^{*}\\right\\|_{V_{t}}\\right)}\\\\ &{\\leq L||\\phi(\\mathbf{x})|_{V_{\\tau}^{-1}}\\left(\\tilde{\\beta}_{t}(\\delta)+\\left\\|\\theta_{t}^{p}-\\theta^{*}\\right\\|_{V_{t}}\\right)}&{\\theta\\in\\Theta_{t}(\\delta)}\\\\ &{\\overset{\\mathrm{whp.~}}{\\leq}2L\\tilde{\\beta}_{t}(\\delta)|\\phi(\\mathbf{x})|_{V_{\\tau}^{-1}}}&{\\mathrm{Lem.~}9}\\\\ &{\\leq\\frac{2L\\tilde{\\beta}_{t}(\\delta)}{\\sqrt{\\lambda\\delta}}\\sigma_{t}(\\mathbf{x})}&{\\mathrm{Lem.~l.3}}\\\\ &{=\\sigma_{t}(\\mathbf{x})\\left(4L B+2L\\sqrt{\\frac{K}{\\lambda}}\\sqrt{2\\log{1/\\delta}+2\\gamma}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the third to last inequality holds with probability greater than $1-\\delta$ , but the rest of the inequalities hold deterministically. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "Given the confidence set of Theorem 2, we give extend the LGP-UCB algorithm of Faury et al. to the kernelized setting (c.f. Algorithm 2) and prove that it satisfies sublinear regret. ", "page_idx": 16}, {"type": "text", "text": "Proof of Corollary 3. Recall that if $\\pmb{x}_{t}$ is the maximizer of the UCB, then ", "page_idx": 16}, {"type": "equation", "text": "$$\ns(\\phi^{\\top}({\\pmb x}^{\\star}){\\pmb\\theta}_{t}^{P})-s(\\phi^{\\top}({\\pmb x}_{t}){\\pmb\\theta}_{t}^{P})\\leq\\sigma_{t}({\\pmb x}_{t})\\beta_{t}(\\delta)-\\sigma_{t}({\\pmb x}^{\\star})\\beta_{t}(\\delta)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then using Theorem 10, we obtain the following for the regret at step $t$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{r_{t}=s(\\phi^{\\top}(\\mathbf{\\boldsymbol{x}}^{\\star})\\theta^{\\star})-s(\\phi^{\\top}(\\mathbf{\\boldsymbol{x}}_{t})\\theta^{\\star})}\\\\ &{\\quad=s(\\phi^{\\top}(\\mathbf{\\boldsymbol{x}}^{\\star})\\theta^{\\star})-s(\\phi^{\\top}(\\mathbf{\\boldsymbol{x}}^{\\star})\\theta_{t}^{P})+s(\\phi^{\\top}(\\mathbf{\\boldsymbol{x}}_{t})\\theta_{t}^{P})-s(\\phi^{\\top}(\\mathbf{\\boldsymbol{x}}_{t})\\theta^{\\star})}\\\\ &{\\quad\\quad\\quad\\quad+s(\\phi^{\\top}(\\mathbf{\\boldsymbol{x}}^{\\star})\\theta_{t}^{P})-s(\\phi^{\\top}(\\mathbf{\\boldsymbol{x}}_{t})\\theta_{t}^{P})}\\\\ &{\\quad\\le\\sigma_{t}(\\mathbf{\\boldsymbol{x}}^{\\star})\\beta_{t}(\\delta)+\\sigma_{t}(\\mathbf{\\boldsymbol{x}}_{t})\\beta_{t}(\\delta)+\\sigma_{t}(\\mathbf{\\boldsymbol{x}}_{t})\\beta_{t}(\\delta)-\\sigma_{t}(\\mathbf{\\boldsymbol{x}}^{\\star})\\beta_{t}(\\delta)}\\\\ &{\\quad\\le2\\beta_{t}(\\delta)\\sigma_{t}(\\mathbf{\\boldsymbol{x}}_{t})}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Initialize Set $(\\beta_{t})_{t\\geq1}$ according to Theorem 2. for $t\\geq1$ do Choose an optimistic action via ", "page_idx": 17}, {"type": "equation", "text": "$$\nx_{t}=\\operatorname*{arg\\,max}_{x\\in\\mathcal{X}}s(f_{t-1}(\\pmb{x}))+\\beta_{t-1}(\\delta)\\sigma_{t-1}(\\pmb{x})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Observe $y_{t}$ and append history. ", "page_idx": 17}, {"type": "text", "text": "Calculate $f_{t}$ acc. to Proposition 1 and update $\\sigma_{t}$ acc. to Theorem 2. end for ", "page_idx": 17}, {"type": "text", "text": "with probability greater than $1-\\delta$ for all $t\\geq0$ . Which allows us to bound the cumulative regret as, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{R_{T}=\\sum_{t=1}^{T}r_{t}\\leq\\sqrt{T\\sum_{t=1}^{T}r_{t}^{2}}}}\\\\ &{}&{\\leq2\\beta_{T}(\\delta)\\sqrt{T\\sum_{t=1}^{T}\\sigma_{t}^{2}(\\boldsymbol{x}_{t})}}\\\\ &{}&{\\leq C_{1}\\beta_{T}(\\delta)\\sqrt{T\\gamma_{t}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\beta_{t}(\\delta)\\le\\beta_{T}(\\delta)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $C_{1}:=\\sqrt{8/\\log(1+(\\lambda\\kappa)^{-1})}$ . ", "page_idx": 17}, {"type": "text", "text": "B Helper Lemmas for Appendix $\\mathbf{A}$ ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Lemma 11 (Mean-Value Theorem). For any $\\pmb{x}\\in\\mathcal{X}$ and $\\theta_{1},\\theta_{2}\\in\\ell_{2}(\\mathbb{N})$ it holds that ", "page_idx": 17}, {"type": "equation", "text": "$$\ns(\\pmb{\\theta}_{2}^{\\top}\\phi(\\pmb{x}))-s(\\pmb{\\theta}_{1}^{\\top}\\phi(\\pmb{x}))=\\alpha(\\pmb{x};\\pmb{\\theta}_{1},\\pmb{\\theta}_{2})(\\pmb{\\theta}_{2}-\\pmb{\\theta}_{1})^{\\top}\\phi(\\pmb{x})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\alpha(\\pmb{x};\\pmb{\\theta}_{1},\\pmb{\\theta}_{2})=\\int_{0}^{1}\\dot{s}(\\nu\\pmb{\\theta}_{2}^{\\top}\\phi(\\pmb{x})+(1-\\nu)\\pmb{\\theta}_{1}^{\\top}\\phi(\\pmb{x}))\\mathrm{d}\\nu\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof of Lemma $I I$ . For any differentiable function $s:\\mathbb{R}\\,\\rightarrow\\,\\mathbb{R}$ by the fundamental theorem of calculus we have ", "page_idx": 17}, {"type": "equation", "text": "$$\ns(z_{2})-s(z_{1})=\\int_{z_{1}}^{z_{2}}\\dot{s}(z)\\mathrm{d}z.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We change the variable of integration to $\\nu=(z-z_{1})/(z_{2}-z_{1})$ , then $z=\\nu z_{2}+(1-\\nu)z_{1}$ and re-writing the integral in terms of $\\nu$ gives, ", "page_idx": 17}, {"type": "equation", "text": "$$\ns(z_{2})-s(z_{1})=(z_{2}-z_{1})\\int_{0}^{1}\\dot{s}(\\nu z_{2}+(1-\\nu)z_{1})\\mathrm{d}\\nu.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Letting $z_{1}=\\pmb{\\theta}_{1}^{\\top}\\phi(\\pmb{x})$ and $z_{2}=\\pmb{\\theta}_{2}^{\\top}\\phi(\\pmb{x})$ concludes the proof. ", "page_idx": 17}, {"type": "text", "text": "Lemma 12 (Gradients to Parameters Conversion). For all $t\\geq0$ and norm $B$ -bounded $\\theta_{1},\\theta_{2}\\in\\ell_{2}(\\mathbb{N})$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|\\pmb{\\theta}_{1}-\\pmb{\\theta}_{2}\\|_{V_{t}}\\leq\\kappa\\|g_{t}(\\pmb{\\theta}_{1})-g_{t}(\\pmb{\\theta}_{2})\\|_{V_{t}^{-1}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof of Lemma $^ \u1e0a 12 \u1e0c$ . We prove the lemma through an auxiliary operator $G_{t}(\\pmb{\\theta}_{1},\\pmb{\\theta}_{2})$ operating on $\\mathcal{H}_{k}$ ", "page_idx": 17}, {"type": "equation", "text": "$$\nG_{t}(\\pmb{\\theta}_{1},\\pmb{\\theta}_{2})=\\lambda\\pmb{I}_{\\mathcal{H}}+\\sum_{\\tau\\leq t}\\alpha(\\pmb{x}_{\\tau};\\pmb{\\theta}_{1},\\pmb{\\theta}_{2})\\phi(\\pmb{x}_{\\tau})\\phi^{\\top}(\\pmb{x}_{\\tau})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\alpha$ is defined in (10). ", "page_idx": 17}, {"type": "text", "text": "Step 1. First we establish how we can go back and forth between the operator norms defined based on $G_{t}$ and $V_{t}$ . Recall that $\\begin{array}{r}{\\kappa=\\operatorname*{sup}_{z\\leq\\bar{B}}\\frac{1}{\\dot{s}(z)}}\\end{array}$ . Therefore, $\\kappa^{-1}\\leq\\,\\dot{s}(z)$ for all $z<B$ , implying that $\\begin{array}{r}{\\alpha(\\pmb{x};\\pmb{\\theta}_{1},\\pmb{\\theta}_{2})\\geq\\int_{0}^{1}\\kappa^{-1}\\mathrm{d}\\nu=\\kappa^{-1}}\\end{array}$ . We can then conclude, ", "page_idx": 17}, {"type": "equation", "text": "$$\nG_{t}(\\pmb{\\theta}_{1},\\pmb{\\theta}_{2})\\geq\\lambda\\pmb{I}_{\\mathcal{H}}+\\sum_{\\tau\\leq t}\\kappa^{-1}\\phi(\\pmb{x}_{\\tau})\\phi^{\\top}(\\pmb{x}_{\\tau})=\\kappa^{-1}V_{t}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Step 2. Now by the definition of $g_{t}(\\pmb\\theta)$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{g_{t}(\\theta_{2})-g_{t}(\\theta_{1})=\\lambda(\\theta_{2}-\\theta_{1})+\\displaystyle\\sum_{\\tau\\leq t}\\phi(x_{\\tau})\\left[s(\\theta_{2}^{\\top}\\phi(x_{\\tau}))-s(\\theta_{1}^{\\top}\\phi(x_{\\tau}))\\right]}}\\\\ {{\\displaystyle\\qquad\\qquad=\\lambda(\\theta_{2}-\\theta_{1})+\\displaystyle\\sum_{\\tau\\leq t}\\phi(x_{\\tau})\\left[\\alpha(x_{\\tau};\\theta_{1},\\theta_{2})\\phi^{\\top}(x_{\\tau})(\\theta_{2}-\\theta_{1})\\right]}}\\\\ {{\\displaystyle\\qquad\\qquad=\\left(\\lambda I_{\\mathcal{H}}+\\displaystyle\\sum_{\\tau\\leq t}\\alpha(x_{\\tau};\\theta_{1},\\theta_{2})\\phi(x_{\\tau})\\phi^{\\top}(x_{\\tau})\\right)(\\theta_{2}-\\theta_{1})}}\\\\ {{\\displaystyle\\qquad\\qquad=G_{t}(\\theta_{1},\\theta_{2})(\\theta_{2}-\\theta_{1})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|g_{t}(\\pmb{\\theta}_{2})-g_{t}(\\pmb{\\theta}_{1})\\right\\|_{G_{t}^{-1}(\\pmb{\\theta}_{1},\\pmb{\\theta}_{2})}=\\left[g_{t}(\\pmb{\\theta}_{2})-g_{t}(\\pmb{\\theta}_{1})\\right]^{\\top}(\\pmb{\\theta}_{1}-\\pmb{\\theta}_{2})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\left(\\pmb{\\theta}_{2}-\\pmb{\\theta}_{1}\\right)^{\\top}G_{t}\\left(\\pmb{\\theta}_{2}-\\pmb{\\theta}_{1}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\quad=\\left\\|\\pmb{\\theta}_{2}-\\pmb{\\theta}_{1}\\right\\|_{G_{t}(\\pmb{\\theta}_{1},\\pmb{\\theta}_{2})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Step 3. Putting together the previous two steps, we can bound the $V_{t}$ -norm over the parameters to the $V_{t}^{-1}$ role in the gradients, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\lVert\\theta_{1}-\\theta_{2}\\right\\rVert_{V_{t}}\\overset{(11)}{\\le}\\sqrt{\\kappa}\\lVert\\theta_{1}-\\theta_{2}\\rVert_{G_{t}(\\theta_{1},\\theta_{2})}}\\\\ &{\\qquad\\qquad\\quad\\overset{(12)}{\\le}\\sqrt{\\kappa}\\lVert g_{t}(\\theta_{1})-g_{t}(\\theta_{2})\\rVert_{G_{t}^{-1}(\\theta_{1},\\theta_{2})}}\\\\ &{\\qquad\\qquad\\quad\\overset{(11)}{\\le}\\kappa\\lVert g_{t}(\\theta_{1})-g_{t}(\\theta_{2})\\rVert_{V_{t}^{-1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "concluding the proof. ", "page_idx": 18}, {"type": "text", "text": "The following two lemmas are standard results in kernelized bandits [Srinivas et al., 2010, Chowdhury and Gopalan, 2017, e.g.,]. We include it here for completeness. ", "page_idx": 18}, {"type": "text", "text": "Lemma 13. Let $\\sigma_{t}$ be as defined in (4). Then $\\sqrt{\\lambda\\kappa}\\|\\phi(\\pmb{x})\\|_{V_{t}^{-1}}=\\sigma_{t}(\\pmb{x})$ , for any $\\pmb{x}\\in\\mathcal{X}$ . ", "page_idx": 18}, {"type": "text", "text": "Proof of Lemma $^{l3}$ . We start by stating some identities which will later be of use. First note that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left(\\boldsymbol{\\Phi}_{t}^{\\top}\\boldsymbol{\\Phi}_{t}+\\lambda\\kappa\\boldsymbol{I}_{\\mathcal{H}}\\right)\\boldsymbol{\\Phi}_{t}^{\\top}=\\boldsymbol{\\Phi}_{t}^{\\top}\\left(\\boldsymbol{\\Phi}_{t}\\boldsymbol{\\Phi}_{t}^{\\top}+\\lambda\\kappa\\boldsymbol{I}_{t}\\right)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which gives ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\boldsymbol{\\Phi}_{t}^{\\top}\\left(\\boldsymbol{\\Phi}_{t}\\boldsymbol{\\Phi}_{t}^{\\top}+\\lambda\\kappa\\boldsymbol{I}_{t}\\right)^{-1}=\\left(\\boldsymbol{\\Phi}_{t}^{\\top}\\boldsymbol{\\Phi}_{t}+\\lambda\\kappa\\boldsymbol{I}_{\\mathcal{H}}\\right)^{-1}\\boldsymbol{\\Phi}_{t}^{\\top}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Moreover, by definition of $k_{t}$ we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\pmb{k}_{t}(\\pmb{x})=\\Phi_{t}\\phi(\\pmb{x})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which allow us to write ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left(\\Phi_{t}^{\\top}\\Phi_{t}+\\lambda\\kappa I_{\\mathcal{H}}\\right)\\phi(x)=\\Phi_{t}^{\\top}\\pmb{k}_{t}(\\pmb{x})+\\lambda\\kappa\\phi(\\pmb{x}),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and obtain ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\phi({\\pmb x})=\\left(\\Phi_{t}^{\\top}\\Phi_{t}+\\lambda\\kappa I_{{\\pmb\\mathscr{H}}}\\right)^{-1}\\Phi_{t}^{\\top}{\\pmb k}_{t}({\\pmb x})+\\lambda\\kappa\\left(\\Phi_{t}^{\\top}\\Phi_{t}+\\lambda\\kappa I\\right)^{-1}\\phi({\\pmb x})\\quad}\\\\ &{}&{\\stackrel{(13)}{=}\\Phi_{t}^{\\top}\\left(\\Phi_{t}\\Phi_{t}^{\\top}+\\lambda\\kappa I_{t}\\right)^{-1}{\\pmb k}_{t}({\\pmb x})+\\lambda\\kappa\\left(\\Phi_{t}^{\\top}\\Phi_{t}+\\lambda\\kappa I\\right)^{-1}\\phi({\\pmb x}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Given the above equation, we conclude the proof by the following chain of equations: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{k({\\pmb x},{\\pmb x})=\\phi^{\\top}({\\pmb x})\\phi({\\pmb x})}\\\\ &{\\qquad\\quad=\\left(\\Phi_{t}^{\\top}\\left(\\Phi_{t}\\Phi_{t}^{\\top}+\\lambda\\kappa I_{t}\\right)^{-1}k_{t}({\\pmb x})+\\lambda\\kappa\\left(\\Phi_{t}^{\\top}\\Phi_{t}+\\lambda\\kappa I_{{\\pmb\\mathscr{H}}}\\right)^{-1}\\phi({\\pmb x})\\right)^{\\top}\\phi({\\pmb x})}\\\\ &{\\qquad\\quad=k_{t}^{\\top}({\\pmb x})\\left(\\Phi_{t}\\Phi_{t}^{\\top}+\\lambda\\kappa I_{t}\\right)^{-1}\\Phi_{t}\\phi({\\pmb x})+\\lambda\\kappa\\phi^{\\top}({\\pmb x})\\left(\\Phi_{t}^{\\top}\\Phi_{t}+\\lambda\\kappa I_{{\\pmb\\mathscr{H}}}\\right)^{-1}\\phi({\\pmb x})}\\\\ &{\\qquad\\quad\\overset{(14)}{=}k_{t}^{\\top}({\\pmb x})\\left(K_{t}+\\lambda\\kappa I_{t}\\right)^{-1}k_{t}({\\pmb x})+\\lambda\\kappa\\phi^{\\top}({\\pmb x})V_{t}^{-1}\\phi({\\pmb x})}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "To obtain the third equation we have used the fact that for bounded operators on Hilbert spaces, the inverse of the adjoint is equal to the adjoint of the inverse [e.g., Theorem 10.19, Axler, 2020]. Re-ordering the equation above we obtain $\\sigma_{t}^{2}(\\pmb{x})=\\lambda\\kappa\\|\\phi(\\pmb{x})\\|_{V_{t}^{-1}}^{2}$ , concluding the proof. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "Lemma 14 (Controlling posterior variance with information gain). For all $T\\geq1$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\sigma_{t}^{2}(\\pmb{x}_{t})\\leq\\frac{2\\gamma_{T}}{\\log(1+(\\lambda\\kappa)^{-1})},\\quad\\sum_{t=1}^{T}(\\sigma_{t}^{\\mathrm{D}}(\\pmb{x}_{t}))^{2}\\leq\\frac{8\\gamma_{T}^{\\mathrm{D}}}{\\log(1+4(\\lambda\\kappa)^{-1})}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof of Lemma 14. By Srinivas et al. [2010, Lemma 5.3], ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\gamma_{T}=\\operatorname*{max}_{x_{1},\\ldots x_{T}}\\frac{1}{2}\\sum_{t=1}^{T}\\log(1+(\\lambda\\kappa)^{-1}\\sigma_{t-1}^{2}(x_{t})).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Following the technique in Srinivas et al. [2010, Lemma 5.4], since $\\sigma_{t}^{2}\\,\\leq\\,1\\$ , then $(\\lambda\\kappa)^{-1}\\sigma_{t}^{2}\\ \\in$ $[0,(\\lambda\\kappa)^{-\\breve{1}}]$ . Now for any $z\\in[0,(\\lambda\\kappa)^{-1}]$ , $z\\leq C\\log(1+z)$ where $C=1/(\\lambda\\kappa\\log(1+(\\lambda\\kappa)^{-1}))$ . We then may write, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{c l c r}{\\displaystyle\\sum_{t=1}^{T}\\sigma_{t}^{2}(\\pmb{x}_{t})=\\sum_{t=1}^{T}\\lambda\\kappa(\\lambda\\kappa)^{-1}\\sigma_{t}^{2}(\\pmb{x}_{t})}\\\\ {\\displaystyle\\leq\\sum_{t=1}^{T}\\lambda\\kappa C\\log\\left(1+(\\lambda\\kappa)^{-1}\\sigma_{t}^{2}(\\pmb{x}_{t})\\right)}\\\\ {\\displaystyle=\\sum_{t=1}^{T}\\frac{\\log(1+(\\lambda\\kappa)^{-1}\\sigma_{t}^{2}(\\pmb{x}_{t}))}{\\log\\left(1+(\\lambda\\kappa)^{-1}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Putting both together proves the first inequality of the lemma. As for the dueling case, we can easily check that $\\sigma_{t}^{\\mathrm{D}}\\leq2$ , and a similar argument yields the second inequality. ", "page_idx": 19}, {"type": "text", "text": "C Proofs for Bandits with Preference Feedback ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "This section presents the proof of main results in Section 5, and our additional contributions in the kernelized Preference-based setting. ", "page_idx": 19}, {"type": "text", "text": "C.1 Equivalence of Preference-based and Logistic Losses ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We start by establishing the equivalence between the logistic loss (3) and dueling loss (6). ", "page_idx": 19}, {"type": "text", "text": "Proof of Proposition $^{4}$ . By Mercer\u2019s theorem, we know that the kernel function $k$ has eigenvalue eigenfunction pairs $(\\sqrt{\\lambda_{i}},\\tilde{\\phi}_{i})$ for $i\\geq~1$ where $\\tilde{\\phi}_{i}$ are orthonormal. Then $k(\\pmb{x},\\pmb{x}^{\\prime})\\ =$ $\\begin{array}{r}{\\sum_{i\\geq1}\\phi_{i}(\\pmb{x})\\phi_{i}(\\pmb{x}^{\\prime})}\\end{array}$ with $\\phi_{i}({\\pmb x})\\;=\\;\\sqrt{\\lambda_{i}}\\widetilde{\\phi}_{i}({\\pmb x})$ . Now applying the definition of $k^{\\mathrm{D}}$ , it holds that $\\begin{array}{r}{k^{\\mathrm{D}}(\\bar{z},z^{\\prime})=\\sum_{i\\geq1}\\psi_{i}^{\\top}(z)\\psi_{i}(z^{\\prime})}\\end{array}$ where $\\psi_{i}(z)=\\sqrt{\\lambda_{i}}\\big(\\phi_{i}(\\pmb{x})-\\phi_{i}(\\pmb{x}^{\\prime})\\big)$ . It is straighforward to check that $\\psi_{i}$ are the eigenfunctions of $k^{\\mathrm{D}}$ , however, they may not be orthonormal. We have, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\langle\\psi_{i},\\psi_{i}\\rangle_{L_{2}}=2\\lambda_{i}(1-b_{i}^{2})}}\\\\ {{\\langle\\psi_{i},\\psi_{j}\\rangle_{L_{2}}=-2\\sqrt{\\lambda_{i}\\lambda_{j}}b_{i}b_{j}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\begin{array}{r}{b_{i}=\\int\\tilde{\\phi}_{i}(\\pmb{x})\\mathrm{d}(\\pmb{x})}\\end{array}$ . By the assumption of the proposition, we have $b_{i}=0$ . However, this assumption holds automatically for all kernels commonly used in applications, e.g. any translation invariant kernel, over many domains, since $\\tilde{\\phi}_{i}$ for such kernels are a sine basis. ", "page_idx": 19}, {"type": "text", "text": "Now since $f\\in\\mathcal{H}_{k}$ , it may be decomposed $\\begin{array}{r}{f=\\sum_{i\\geq1}\\beta_{i}\\phi_{i}}\\end{array}$ and $\\begin{array}{r}{\\|f\\|_{k}^{2}=\\sum_{i\\geq1}\\beta_{i}^{2}\\leq\\infty}\\end{array}$ . And set the difference function to $\\begin{array}{r}{h(\\mathbf{x},\\mathbf{x}^{\\prime})=\\sum_{i\\geq1}\\beta_{i}\\psi_{i}(z)}\\end{array}$ . We can then bound the RKHS norm of $h$ w.r.t. the ", "page_idx": 19}, {"type": "text", "text": "kernel $k^{\\mathrm{D}}$ as follows ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Vert h\\Vert_{k^{\\mathrm{D}}}^{2}=\\displaystyle\\sum_{i\\geq1}\\left(\\frac{\\langle h,\\psi_{i}\\rangle_{L_{2}}}{\\langle\\psi_{i},\\psi_{i}\\rangle_{L_{2}}}\\right)^{2}}\\\\ &{\\qquad=\\displaystyle\\sum_{i\\geq1}\\left(\\frac{\\sum_{j\\geq1}\\beta_{j}\\langle\\psi_{j},\\psi_{i}\\rangle_{L_{2}}}{2\\lambda_{i}(1-b_{i})}\\right)^{2}}\\\\ &{\\qquad=\\displaystyle\\sum_{i\\geq1}\\left(\\beta_{i}-\\frac{b_{i}}{\\sqrt{\\lambda_{i}}(1-b_{i})}\\sum_{j\\neq i}\\beta_{j}b_{j}\\sqrt{\\lambda_{j}}\\right)^{2}}\\\\ &{\\qquad=\\displaystyle\\coprod_{i\\leq0}\\Vert f\\Vert_{k}^{2}\\leq B^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now by Mercer\u2019s theorem, $h\\in\\mathcal{H}_{k^{\\mathrm{D}}}$ since it is decomposable as a sum of $k^{\\mathrm{D}}$ eigenfunctions, and attains a $B$ -bounded $k^{\\mathrm{D}}$ -norm which we showed to be equal to $\\|f\\|_{k}$ . The other direction of the statement is proved the same way. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "Proof of Corollary $5$ . Consider the utility function $f$ and define $h(\\pmb{x},\\pmb{x}^{\\prime}):=f(\\pmb{x})-f(\\pmb{x}^{\\prime})$ . Then by Proposition 4, $h$ is in RKHS of $k^{\\mathrm{D}}$ with a $k^{\\mathrm{D}}$ -norm bounded by $B$ . We may estimate $h$ by minimizing $\\mathcal{L}_{k^{\\mathrm{D}}}^{\\mathrm{L}^{\\star}}(\\cdot;H_{t})$ . Now invoking Theorem 2 with the dueling kernel we have, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(\\forall t\\geq1,\\pmb{x},\\pmb{x}^{\\prime}\\in\\mathcal{X}:\\left|s\\left(h_{t}(\\pmb{x},\\pmb{x}^{\\prime})\\right)-s\\left(h(\\pmb{x},\\pmb{x}^{\\prime})\\right)\\right|\\leq\\beta_{t}^{\\mathrm{D}}(\\delta)\\sigma_{t}^{D}(\\pmb{x},\\pmb{x}^{\\prime})\\right)\\geq1-\\delta}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "concluding the proof by definition of $h$ . ", "page_idx": 20}, {"type": "text", "text": "C.2 Proof of the Preference-based Regret Bound ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Recall Corollary 5, which states ", "page_idx": 20}, {"type": "equation", "text": "$$\n|s(f(\\pmb{x}^{\\star})-f(\\pmb{x}_{t}))-s(h_{t}(\\pmb{x}^{\\star},\\pmb{x}_{t}))|\\leq\\beta_{t}^{\\mathrm{D}}(\\delta)\\sigma_{t}^{D}(\\pmb{x},\\pmb{x}^{\\prime})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "with high probability simultaneously for all $(x,x^{\\prime})$ and $t\\geq1$ . For simplicity in notation in the rest of this section, we define $\\omega_{t}(\\pmb{x},\\pmb{x}^{\\prime}):=\\beta_{t}^{\\mathrm{D}}(\\delta)\\sigma_{t}^{D}(\\pmb{x},\\pmb{x}^{\\prime})$ and ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{LCB}_{t}({\\pmb x},{\\pmb x}^{\\prime})=s(h_{t}({\\pmb x},{\\pmb x}^{\\prime}))-\\omega_{t}({\\pmb x},{\\pmb x}^{\\prime}),}\\\\ &{\\mathrm{UCB}_{t}({\\pmb x},{\\pmb x}^{\\prime})=s(h_{t}({\\pmb x},{\\pmb x}^{\\prime}))+\\omega_{t}({\\pmb x},{\\pmb x}^{\\prime}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that $\\omega_{t}(\\pmb{x},\\pmb{x}^{\\prime})=\\omega_{t}(\\pmb{x}^{\\prime},\\pmb{x})$ by the symmetry of the dueling kernel $k^{\\mathrm{D}}$ . Furthermore, recall the notation $h(\\pmb{x},\\pmb{x})=f(\\pmb{x})-f(\\pmb{x})$ . ", "page_idx": 20}, {"type": "text", "text": "Proof of Theorem $^ \u1e0a 6 \u1e0c$ . Step 1: First, we connect the term of $\\mathbf{\\boldsymbol{x}}_{t}^{\\prime}$ in the dueling regret defined in Equation (1) to that of $\\pmb{x}_{t}$ . Note that both $s(f(\\mathbf{x}^{*})-f(\\mathbf{x}_{t}^{\\prime}))$ and $s(f(\\mathbf{x}^{*})-f(\\bar{\\mathbf{x}_{t}}))$ are greater than 0.5 due to the optimality of $x^{\\star}$ and the sigmoid function $s$ is concave on the interval $[0.5,\\infty)$ . Using the definition of concavity, we get ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{s(f(\\pmb{x}^{*})-f(\\pmb{x}_{t}^{\\prime}))\\leq s(f(\\pmb{x}^{*})-f(\\pmb{x}_{t}))+\\dot{s}(f(\\pmb{x}^{*})-f(\\pmb{x}_{t}))(f(\\pmb{x}_{t})-f(\\pmb{x}_{t}^{\\prime}))}&{}\\\\ {=s(f(\\pmb{x}^{*})-f(\\pmb{x}_{t}))+s(f(\\pmb{x}^{*})-f(\\pmb{x}_{t}))s(f(\\pmb{x}_{t})-f(\\pmb{x}^{*}))(f(\\pmb{x}_{t})-f(\\pmb{x}_{t}^{\\prime}))}&{}\\\\ {\\leq\\left(1+\\frac{h(\\pmb{x}_{t},\\pmb{x}_{t}^{\\prime})}{2}\\right)s(f(\\pmb{x}^{*})-f(\\pmb{x}_{t}))}&{}&{(1\\leq t)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the second line comes from the derivative of the sigmoid function, $\\dot{s}(x)=s(x)(1-s(x))=$ $s(x)s(-x)$ , and in the last line we use $s(f(\\mathbf{x}_{t})-f(\\mathbf{x}^{*}))\\stackrel{*}{\\leq}0.5$ . ", "page_idx": 20}, {"type": "text", "text": "Using Equation (15), we can upper bound the dueling regret in Equation (1) as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{2r_{t}^{D}=s(f(\\pmb{x}^{\\star})-f(\\pmb{x}_{t}))+s(f(\\pmb{x}^{\\star})-f(\\pmb{x}_{t}^{\\prime}))-1}\\\\ {\\le s(f(\\pmb{x}^{\\star})-f(\\pmb{x}_{t}))+\\left(1+\\frac{h(\\pmb{x}_{t},\\pmb{x}_{t}^{\\prime})}{2}\\right)s(f(\\pmb{x}^{\\star})-f(\\pmb{x}_{t}))-1}\\\\ {\\le2s(f(\\pmb{x}^{\\star})-f(\\pmb{x}_{t}))-1+\\frac{h(\\pmb{x}_{t},\\pmb{x}_{t}^{\\prime})}{2}s(f(\\pmb{x}^{\\star})-f(\\pmb{x}_{t}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Step 2: Next, we show that the single-step regret is bounded by $\\omega_{t}(\\mathbf{\\boldsymbol{x}}_{t},\\mathbf{\\boldsymbol{x}}_{t}^{\\prime})$ . First, consider the term $s(f(\\mathbf{x}^{\\star})-f(\\mathbf{x}_{t}))$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}{s(f(\\pmb{x}^{\\star})-f(\\pmb{x}_{t}))-0.5\\leq s(h_{t}(\\pmb{x}^{\\star},\\pmb{x}_{t}))+\\omega_{t}(\\pmb{x}_{t},\\pmb{x}^{\\star})-0.5}&{}&&{\\mathrm{~C~}}\\\\ {\\leq0.5-s(h_{t}(\\pmb{x}_{t},\\pmb{x}^{\\star}))+\\omega_{t}(\\pmb{x}_{t},\\pmb{x}^{\\star})}&{}&&{\\mathrm{~S~}}\\\\ {\\leq2\\omega_{t}(\\pmb{x}_{t},\\pmb{x}^{\\star})}&{}&&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "In the last inequality, we used that $\\pmb{x}_{t}\\;\\in\\;\\mathcal{M}_{t}$ implying that $0.5\\,-\\,s(h_{t}(\\pmb{x}_{t},\\pmb{x}^{\\star}))\\,\\le\\,\\omega_{t}(\\pmb{x}_{t},\\pmb{x}^{\\star})$ . Combining Equation (16) and Equation (17) implies that ", "page_idx": 21}, {"type": "equation", "text": "$$\n2r_{t}^{D}\\leq4\\omega_{t}(\\boldsymbol{x}_{t},\\boldsymbol{x}^{\\star})+\\frac{h(\\boldsymbol{x}_{t},\\boldsymbol{x}_{t}^{\\prime})}{2}s(f(\\boldsymbol{x}^{\\star})-f(\\boldsymbol{x}_{t}))\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Now, we bound $\\omega_{t}({\\boldsymbol x}_{t},{\\boldsymbol x}^{\\star})$ by $\\omega_{t}(\\mathbf{\\boldsymbol{x}}_{t},\\mathbf{\\boldsymbol{x}}_{t}^{\\prime})$ . If $\\pmb{x}_{t}\\,=\\,\\pmb{x}^{\\star}$ , then $\\omega_{t}(\\pmb{x}_{t},\\pmb{x}^{\\star})\\,=\\,0\\,\\leq\\,\\omega_{t}(\\pmb{x}_{t},\\pmb{x}_{t}^{\\prime})$ . If $\\mathbf{\\boldsymbol{x}}_{t}^{\\prime}=\\mathbf{\\boldsymbol{x}}^{\\star}$ , then the two expressions are equivalent. Now, assume that $\\mathbf{\\boldsymbol{x}}_{t}\\neq\\mathbf{\\boldsymbol{x}}^{*}$ and $\\mathbf{x}_{t}^{\\prime}\\neq x^{\\star}$ and consider $\\omega_{t}({\\boldsymbol x}_{t},{\\boldsymbol x}^{\\star})$ . ", "page_idx": 21}, {"type": "text", "text": "Case 1: Assume that $\\mathrm{UCB}_{t}(\\pmb{x}_{t},\\pmb{x}^{\\star})\\leq\\mathrm{UCB}_{t}(\\pmb{x}_{t},\\pmb{x}_{t}^{\\prime})$ . Then, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\omega_{t}(\\mathbf{x}_{t},\\mathbf{x}^{*})=\\mathrm{UCB}_{t}(x_{t},\\mathbf{x}^{\\star})-\\mathrm{LCB}_{t}(x_{t},\\mathbf{x}^{\\star})}\\\\ &{\\qquad\\qquad\\leq\\mathrm{UCB}_{t}(x_{t},\\mathbf{x}_{t}^{\\prime})-\\mathrm{LCB}_{t}(\\mathbf{x}_{t},\\mathbf{x}^{\\star})}\\\\ &{\\qquad\\qquad\\leq\\mathrm{UCB}_{t}(x_{t},\\mathbf{x}_{t}^{\\prime})-\\mathrm{LCB}_{t}(x_{t},\\mathbf{x}_{t}^{\\prime})=2\\omega_{t}(\\mathbf{x}_{t},\\mathbf{x}_{t}^{\\prime})}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where we used the assumption in the first inequality and the definition of $\\mathbf{\\boldsymbol{x}}_{t}^{\\prime}$ in the second inequality. Case 2: Assume that $\\mathrm{UCB}_{t}(\\pmb{x}_{t},\\pmb{x}^{\\star})\\geq\\mathrm{UCB}_{t}(\\pmb{x}_{t},\\pmb{x}_{t}^{\\prime})$ . First note the following connection between $\\mathrm{LCB}_{t}$ and $\\mathrm{UCB}_{t}$ . ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{LCB}_{t}(\\pmb{x},\\pmb{x}^{\\prime})=s(h_{t}(\\pmb{x},\\pmb{x}^{\\prime}))-\\omega_{t}(\\pmb{x},\\pmb{x}^{\\prime})}\\\\ &{\\qquad\\qquad\\qquad=1-s(h_{t}(\\pmb{x}^{\\prime},\\pmb{x}))-\\omega_{t}(\\pmb{x}^{\\prime},\\pmb{x})}\\\\ &{\\qquad\\qquad\\qquad=1-\\mathrm{UCB}_{t}(\\pmb{x}^{\\prime},\\pmb{x})}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where we used the equality $s(z)\\,=\\,1\\,-\\,s(-z)$ in the second equality. Using Equation (20), the assumption of Case 2 can be rewritten as $\\mathrm{UCB}_{t}({\\pmb x}_{t},{\\pmb x}^{\\star})\\geq\\mathrm{UCB}_{t}({\\pmb x}_{t},{\\pmb x}_{t}^{\\prime})$ , implying that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{LCB}_{t}(\\pmb{x}_{t}^{\\prime},\\pmb{x}_{t})\\geq\\mathrm{LCB}_{t}(\\pmb{x}^{\\star},\\pmb{x}_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Similarly, $\\mathrm{LCB}_{t}(\\pmb{x}_{t},\\pmb{x}_{t}^{\\prime})\\leq\\mathrm{LCB}_{t}(\\pmb{x}_{t},\\pmb{x}^{\\star})$ implies ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{UCB}_{t}(\\pmb{x}_{t}^{\\prime},\\pmb{x}_{t})\\geq\\mathrm{UCB}_{t}(\\pmb{x}^{\\star},\\pmb{x}_{t})\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Combining Equation (21) and Equation (22), we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\omega_{t}(\\mathbf{x}_{t},\\mathbf{x}^{*})=\\mathrm{UCB}_{t}(\\mathbf{x}^{\\star},\\mathbf{x}_{t})-\\mathrm{LCB}_{t}(\\mathbf{x}^{\\star},\\mathbf{x}_{t})}\\\\ &{\\qquad\\qquad\\leq\\mathrm{UCB}_{t}(\\mathbf{x}^{\\star},\\mathbf{x}_{t})-\\mathrm{LCB}_{t}(\\mathbf{x}_{t}^{\\prime},\\mathbf{x}_{t})}\\\\ &{\\qquad\\qquad\\leq\\mathrm{UCB}_{t}(\\mathbf{x}_{t}^{\\prime},\\mathbf{x}_{t})-\\mathrm{LCB}_{t}(\\mathbf{x}_{t}^{\\prime},\\mathbf{x}_{t})=2\\omega_{t}(\\mathbf{x}_{t},\\mathbf{x}_{t}^{\\prime})}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Combining Equation (19) and Equation (23), we can rewrite the first term in Equation (18) to get ", "page_idx": 21}, {"type": "equation", "text": "$$\n2r_{t}^{D}\\leq4\\omega_{t}(\\pmb{x}_{t},\\pmb{x}_{t}^{\\prime})+\\frac{h(\\pmb{x}_{t},\\pmb{x}_{t}^{\\prime})}{2}s(f(\\pmb{x}^{\\star})-f(\\pmb{x}_{t})).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "It remains to bound the second term of Equation (24). By the Mean-Value Theorem, $\\exists z\\ \\in$ $[0,h(\\pmb{x}_{t},\\pmb{x}_{t}^{\\prime})]$ such that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\dot{s}(z)(h(\\mathbf{x}_{t},\\mathbf{x}_{t}^{\\prime})-0)=s(h(\\mathbf{x}_{t},\\mathbf{x}_{t}^{\\prime}))-f(0)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Now since $\\kappa=\\operatorname*{sup}_{z\\leq B}1/\\dot{s}(z)$ then, ", "page_idx": 21}, {"type": "equation", "text": "$$\nh(\\pmb{x}_{t},\\pmb{x}_{t}^{\\prime})\\leq\\kappa(s(h(\\pmb{x}_{t},\\pmb{x}_{t}^{\\prime}))-0.5)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Next, we consider the term $s(h(\\pmb{x}_{t},\\pmb{x}_{t}^{\\prime}))-0.5$ in Equation (25). Note that $\\pmb{x}_{t},\\pmb{x}_{t}^{\\prime}\\in\\mathcal{M}_{t}$ implies that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{UCB}_{t}(\\pmb{x}_{t},\\pmb{x}_{t}^{\\prime})\\geq0.5}\\\\ &{\\quad s(h_{t}(\\pmb{x}_{t},\\pmb{x}_{t}^{\\prime}))\\geq0.5-\\omega_{t}(\\pmb{x}_{t},\\pmb{x}_{t}^{\\prime})}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Additionally note that $\\mathrm{LCB}_{t}({\\pmb x}_{t},{\\pmb x}_{t})~~=~~0.5$ for all $\\pmb{x}_{t}$ , therefore, by the definition of $\\mathbf{\\boldsymbol{x}}_{t}^{\\prime}$ , $\\mathrm{LCB}_{t}(\\pmb{x}_{t},\\pmb{\\dot{x}}_{t}^{\\prime})\\leq0.5$ . It implies that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{LCB}_{t}(\\pmb{x}_{t},\\pmb{x}_{t}^{\\prime})\\leq0.5}\\\\ &{\\;s(h_{t}(\\pmb{x}_{t},\\pmb{x}_{t}^{\\prime}))\\leq0.5+\\omega_{t}(\\pmb{x}_{t},\\pmb{x}_{t}^{\\prime}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "From Equation (26) and Equation (27), it follows that ", "page_idx": 22}, {"type": "equation", "text": "$$\n|s(h_{t}(\\pmb{x}_{t},\\pmb{x}_{t}^{\\prime}))-0.5|\\leq\\omega_{t}(\\pmb{x}_{t},\\pmb{x}_{t}^{\\prime})\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "furthermore, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{UCB}_{t}(\\mathbf{x}_{t},\\mathbf{x}_{t}^{\\prime})-0.5=s(h_{t}(\\mathbf{x}_{t},\\mathbf{x}_{t}^{\\prime}))-0.5+\\omega_{t}(\\mathbf{x}_{t},\\mathbf{x}_{t}^{\\prime})}\\\\ &{\\qquad\\qquad\\qquad\\leq|s(h_{t}(\\mathbf{x}_{t},\\mathbf{x}_{t}^{\\prime}))-0.5|+\\omega_{t}(\\mathbf{x}_{t},\\mathbf{x}_{t}^{\\prime})}\\\\ &{\\qquad\\qquad\\qquad\\leq2\\omega_{t}(\\mathbf{x}_{t},\\mathbf{x}_{t}^{\\prime})}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and similarly ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{0.5-\\mathrm{LCB}_{t}(\\pmb{x}_{t},\\pmb{x}_{t}^{\\prime})\\leq2\\omega_{t}(\\pmb{x}_{t},\\pmb{x}_{t}^{\\prime}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "From Equation (28) and Equation (29), it follows that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|s(f(\\pmb{x}_{t})-f(\\pmb{x}_{t}^{\\prime}))-0.5|\\leq\\operatorname*{max}\\{\\mathrm{UCB}_{t}(\\pmb{x}_{t},\\pmb{x}_{t}^{\\prime})-0.5,0.5-\\mathrm{LCB}_{t}(\\pmb{x}_{t},\\pmb{x}_{t}^{\\prime})\\}}\\\\ {\\leq2\\omega_{t}(\\pmb{x}_{t},\\pmb{x}_{t}^{\\prime}).~~~~~~~~~~~~~~~~~~~~~~~~~~}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Combining Equation (30) with Equation (25), it follows ", "page_idx": 22}, {"type": "equation", "text": "$$\nh(\\pmb{x}_{t},\\pmb{x}_{t}^{\\prime})\\leq2\\kappa\\omega_{t}(\\pmb{x}_{t},\\pmb{x}_{t}^{\\prime})\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Using Equation (31) in Equation (24) and the fact that $s(f(\\mathbf{x}^{*})-f(\\mathbf{x}_{t}))\\leq1$ , we get ", "page_idx": 22}, {"type": "equation", "text": "$$\n2r_{t}^{D}\\leq(4+\\kappa)\\omega_{t}(\\pmb{x}_{t},\\pmb{x}_{t}^{\\prime}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, for the cumulative dueling regret it holds ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{R^{\\mathrm{D}}(T)=\\displaystyle\\sum_{t=1}^{T}r_{t}^{\\mathrm{D}}\\le\\sqrt{T\\displaystyle\\sum_{t=1}^{T}(r_{t}^{\\mathrm{D}})^{2}}}&{}&\\\\ {\\le(2+\\kappa/2)\\beta_{T}^{\\mathrm{D}}(\\delta)\\sqrt{T\\displaystyle\\sum_{t=1}^{T}(\\sigma_{t}^{\\mathrm{D}})^{2}(x_{t},\\sqrt{\\lambda\\kappa})}}&{\\quad}&{\\beta_{t}(\\delta)\\le\\beta_{T}^{\\mathrm{D}}(\\delta)}\\\\ &{\\le C_{3}\\beta_{T}^{\\mathrm{D}}(\\delta)\\sqrt{T\\gamma_{t}^{\\mathrm{D}}}}&{\\quad}&{\\mathrm{Lem.~}14}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "with probability greater than $1-\\delta$ for all $T\\geq1$ . ", "page_idx": 22}, {"type": "text", "text": "C.3 Extending Algorithms for Linear Dueling Bandits to Kernelized Setting ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Maximum Informative Pair Algorithm. Proposed in Saha [2021] for linear utilities, the MAXINP algorithm similarly maintains a set of plausible maximizer arms, and picks the pair of actions that have the largest joint uncertainty, and therefore are expected to be informative. Algorithm 3 present the kernelized variant of this algorithm. Using Corollary 5, we can show that the kernelized MAXINP also satisfies a $\\tilde{\\mathcal{O}}(\\gamma_{T}\\sqrt{T})$ regret. ", "page_idx": 22}, {"type": "text", "text": "Theorem 15. Let $\\delta\\in(0,1]$ and choose the exploration coefficient $\\beta_{t}^{\\mathrm{D}}(\\delta)$ as defined in Corollary 5. Then MAXINP satisfies the anytime dueling regret guarantee of ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(\\forall T\\geq0:R^{\\mathrm{D}}(T)\\leq C_{2}\\beta_{T}^{\\mathrm{D}}(\\delta)\\sqrt{T\\gamma_{T}^{\\mathrm{D}}}\\right)\\geq1-\\delta}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\gamma_{T}^{\\mathrm{D}}$ is the $T$ -step information gain of kernel $k^{\\mathrm{D}}$ and $C_{2}=4/\\sqrt{\\log(1+4(\\lambda\\kappa)^{-1})}$ ", "page_idx": 22}, {"type": "text", "text": "Algorithm 3 MAXINP- Kernelized Variant ", "page_idx": 23}, {"type": "text", "text": "Input $(\\beta_{t}^{\\mathrm{D}})_{t\\geq1}$ .   \nfor $t\\geq1$ do Play the most informative pair via ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\pmb{x}_{t},\\pmb{x}_{t}^{\\prime}=\\arg\\operatorname*{max}_{\\pmb{x},\\pmb{x}^{\\prime}\\mathcal{M}_{t}}\\sigma_{t}^{\\mathrm{D}}(\\pmb{x},\\pmb{x}^{\\prime})\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Observe $y_{t}$ and append history. ", "page_idx": 23}, {"type": "text", "text": "Update $h_{t+1}$ and $\\tilde{\\sigma}_{t+1}^{\\mathrm{D}}$ and the set of plausible maximizers ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{M}_{t+1}=\\{x\\in\\mathcal{X}|\\,\\forall\\pmb{x}^{\\prime}\\in\\mathcal{X}:\\,s(h_{t+1}(\\pmb{x},\\pmb{x}^{\\prime}))+\\beta_{t+1}^{\\mathrm{D}}\\sigma_{t+1}^{\\mathrm{D}}(\\pmb{x},\\pmb{x}^{\\prime})>1/2\\}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "end for ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Proof of Theorem 15. When selecting $(\\mathbf{\\boldsymbol{x}}_{t},\\mathbf{\\boldsymbol{x}}_{t}^{\\prime})$ according to Algorithm 3, we choose the pair via ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\boldsymbol{\\mathbf{\\boldsymbol{x}}}_{t},\\boldsymbol{\\mathbf{\\boldsymbol{x}}}_{t}^{\\prime}=\\underset{\\boldsymbol{\\mathbf{\\boldsymbol{x}}},\\boldsymbol{\\mathbf{\\boldsymbol{x}}}^{\\prime}\\in\\mathcal{M}_{t}}{\\arg\\operatorname*{max}}\\,\\omega_{t}(\\boldsymbol{\\mathbf{\\boldsymbol{x}}},\\boldsymbol{\\mathbf{\\boldsymbol{x}}}^{\\prime})\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where action space is restricted to $\\mathcal{M}_{t}$ and therefore, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{s(h_{t}(\\pmb{x}^{\\star},\\pmb{x}_{t}))\\leq1/2+\\omega_{t}(\\pmb{x}_{t},\\pmb{x}^{\\star})}\\\\ {s(h_{t}(\\pmb{x}^{\\star},\\pmb{x}_{t}^{\\prime}))\\leq1/2+\\omega_{t}(\\pmb{x}_{t}^{\\prime},\\pmb{x}^{\\star})}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where we have used the identity $s(-z)=1-s(z)$ . Simultaneously for all $t\\geq1$ , we can bound the single-step dueling regret with probability greater than $1-\\delta$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2r_{t}^{\\mathrm{D}}=s(f(\\pmb{x}^{\\star})-f(\\pmb{x}_{t}))+s(f(\\pmb{x}^{\\star})-f(\\pmb{x}_{t}^{\\prime}))-1}\\\\ &{\\qquad\\leq s(h_{t}(\\pmb{x}^{\\star},\\pmb{x}_{t}))+\\omega_{t}(\\pmb{x}^{\\star},\\pmb{x}_{t})+s(h_{t}(\\pmb{x}^{\\star},\\pmb{x}_{t}^{\\prime}))+\\omega_{t}(\\pmb{x}^{\\star},\\pmb{x}_{t}^{\\prime})-1}\\\\ &{\\qquad\\leq2\\left(\\omega_{t}(\\pmb{x}^{\\star},\\pmb{x}_{t})+\\omega_{t}(\\pmb{x}^{\\star},\\pmb{x}_{t}^{\\prime})\\right)}\\\\ &{\\qquad\\leq4\\omega_{t}(\\pmb{x}_{t},\\pmb{x}_{t}^{\\prime}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where for the first inequality we have invoked Corollary 5. Then for the regret satisfies ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{R^{\\mathrm{D}}(T)=\\displaystyle\\sum_{t=1}^{T}r_{t}^{\\mathrm{D}}\\leq\\sqrt{T\\displaystyle\\sum_{t=1}^{T}(r_{t}^{\\mathrm{D}})^{2}}}}\\\\ {{\\qquad\\qquad\\qquad\\leq2\\beta_{T}^{\\mathrm{D}}(\\delta)\\sqrt{T\\displaystyle\\sum_{t=1}^{T}(\\sigma_{t}^{\\mathrm{D}})^{2}(x_{t},\\sqrt{\\lambda\\kappa})}}}\\\\ {{\\qquad\\qquad\\leq C_{2}\\beta_{T}^{\\mathrm{D}}(\\delta)\\sqrt{T\\gamma_{t}^{\\mathrm{D}}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\beta_{t}(\\delta)\\le\\beta_{T}^{\\mathrm{D}}(\\delta)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "with probability greater than $1-\\delta$ for all $T\\geq1$ . ", "page_idx": 23}, {"type": "text", "text": "Dueling Information Directed Sampling (IDS) Algorithm. To choose actions at each iteration $t$ , MAXINP and MAXMINLCB require solving an optimization problem on $\\mathcal X\\times\\mathcal X$ . The Dueling IDS approach addresses this issue and presents an algorithm which requires solving an optimization problem on $\\mathcal{X}\\times[0,1]$ and is computationally more efficient when $d_{0}>1$ . This work considers kernelized utilities, however, assumes the probability of preference itself is in an RKHS and solves a kernelized ridge regression problem to estimate the probability $s(h(\\pmb{x},\\pmb{x}^{\\prime})$ . In the following, we present an improved version of this algorithm, by considering the preference-based loss (6) for estimating the utility function. We modify the algorithm and the theoretical analysis to accommodate this. ", "page_idx": 23}, {"type": "text", "text": "Consider the sub-optimality gap $\\Delta({\\pmb x}):=h({\\pmb x}^{\\star},{\\pmb x})$ for an action $x\\in\\mathcal{X}$ . We may estimate this gap using the reward estimate maximizer $\\hat{\\pmb{x}}_{t}^{\\star}:=\\arg\\operatorname*{max}_{\\pmb{x}\\in\\mathcal{X}}f_{t}(\\pmb{x})$ . Suppose we choose $\\hat{\\pmb x}_{t}^{\\star}$ as one of the actions. Then $u_{t}$ shows an optimistic estimate of the highest obtainable reward at this step: ", "page_idx": 23}, {"type": "equation", "text": "$$\nu_{t}:=\\operatorname*{max}_{\\pmb{x}\\in\\mathcal{X}}h(\\pmb{x},\\pmb{\\hat{x}}_{t}^{\\star})+\\tilde{\\beta}_{t}\\sigma_{t}^{D}(\\pmb{x},\\pmb{x}_{t}^{\\star}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where ${\\tilde{\\beta}}_{t}$ is the exploration coefficient. We bound $\\Delta(x)$ by the estimated gap ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\hat{\\Delta}_{t}(\\pmb{x}):=u_{t}+h_{t}(\\hat{\\pmb{x}}_{t}^{\\star},\\pmb{x})\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Algorithm 4 Preference-based IDS - Kernelized Logistic Variant ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Initialize Set $(\\beta_{t})_{t\\geq1}$ according to Theorem 2. for $t\\geq1$ do Find a greedy action via fixing any point $x_{\\mathrm{null}}\\in\\chi$ and maximizing ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\pmb{x}_{t}^{(1)}=\\hat{\\pmb{x}}_{t}^{\\star}=\\arg\\operatorname*{max}_{\\pmb{x}\\in\\mathcal{X}}h_{t}(\\boldsymbol{x},x_{\\mathrm{null}}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Update $u_{t}$ and $\\hat{\\Delta}_{t}({\\pmb x})$ acc. to (34). ", "page_idx": 24}, {"type": "text", "text": "Find an informative action and the probability of selection via ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\pmb{x}_{t}^{(2)},p_{t}=\\underset{\\pmb{p}\\in[0,1]}{\\arg\\operatorname*{min}}\\,\\frac{\\Big((1-p)u_{t}+p\\hat{\\Delta}_{t}(\\pmb{x})\\Big)^{2}}{p\\log\\Big(1+(\\lambda\\kappa)^{-1}\\big(\\sigma_{t}^{D}(\\pmb{x}_{t}^{(1)},\\pmb{x})\\big)^{2}\\Big)}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Draw $\\alpha_{t}\\sim\\operatorname{Bern}(p_{t})$ . ", "page_idx": 24}, {"type": "text", "text": "if $\\alpha_{t}=1$ then choose pair $(\\pmb{x}_{t},\\pmb{x}_{t}^{\\prime})=(\\pmb{x}_{t}^{(1)},\\pmb{x}_{t}^{(2)})$ else choose $(\\mathbf{\\boldsymbol{x}}_{t},\\mathbf{\\boldsymbol{x}}_{t}^{\\prime})=(\\mathbf{\\boldsymbol{x}}_{t}^{(1)},\\mathbf{\\boldsymbol{x}}_{t}^{(1)})$ Observe $y_{t}$ and append history.   \nUpdate ht+1 and \u03c3tD+1. ", "page_idx": 24}, {"type": "text", "text": "end for ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "and show its uniform validity in Lemma 17. We can now propose the Kernelized Logistic IDS algorithm with preference feedback in Algorithm 4, as a variant of the algorithm of Kirschner and Krause. Theorem 16. Let $\\delta\\in(0,1]$ and for all $t\\geq1$ , set the exploration coefficient as $\\tilde{\\beta}_{t}=\\beta_{t}^{\\mathrm{D}}(\\delta)/L$ . Then Algorithm $^{4}$ satisfies the anytime cumulative dueling regret guarantee of ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(\\forall T\\geq0:R^{\\mathrm{D}}(T)=\\mathcal{O}\\left(\\beta_{T}^{\\mathrm{D}}(\\delta)\\sqrt{T(\\gamma_{T}+\\log{1/\\delta})}\\right)\\right)\\geq1-\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof of Theorem $I6.$ . Our approach closely follows the proof of Kirschner and Krause [2021, Theorem 1]. Let $\\mathcal P(\\cdot)$ show the set of continuous probability distributions over a domain. Define the expected average gap for a policy $\\mu\\in\\mathcal P(\\mathcal X\\times\\bar{\\mathcal X})$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\hat{\\Delta}_{t}(\\mu):=\\frac{1}{2}\\mathbb{E}_{{\\pmb x},{\\pmb x}^{\\prime}\\sim\\mu}\\hat{\\Delta}_{t}({\\pmb x})+\\hat{\\Delta}_{t}({\\pmb x}^{\\prime})\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and the expected information ratio as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\Xi_{t}(\\mu):=\\frac{\\hat{\\Delta}_{t}^{2}(\\mu)}{\\mathbb{E}_{\\pmb{x},\\pmb{x}^{\\prime}\\sim\\mu}\\log\\Big(1+(\\lambda\\kappa)^{-1}\\big(\\sigma_{t}^{D}(\\pmb{x},\\pmb{x}^{\\prime})\\big)^{2}\\Big)}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Algorithm 4 draws actions via $\\mu_{t}\\,=\\,\\bigl(1-p_{t}\\bigr)\\delta_{({\\pmb x}_{t}^{(1)},{\\pmb x}_{t}^{(1)})}+p_{t}\\delta_{({\\pmb x}_{t}^{(1)},{\\pmb x}_{t}^{(2)})}$ , where $\\delta_{(\\pmb{x},\\pmb{x}^{\\prime})}$ denotes a Direct delta. Then by Kirschner et al. [2020, Lemma 1], ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\sum_{t=1}^{T}h(\\pmb{x}^{\\star},\\pmb{x}_{t})+h(\\pmb{x}^{\\star},\\pmb{x}_{t}^{\\prime})\\leq\\sqrt{\\sum_{t=1}^{T}\\Xi_{t}(\\mu_{t})\\left(\\gamma_{T}+\\mathcal{O}(\\log1/\\delta)\\right)}+\\mathcal{O}(\\log T/\\delta)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which allows us to bound the regret with probability greater than $1-\\delta$ as ", "page_idx": 24}, {"type": "equation", "text": "$$\nR^{\\mathrm{D}}(T)\\leq L{\\sqrt{\\sum_{t=1}^{T}\\Xi_{t}(\\mu_{t})\\left(\\gamma_{T}+{\\mathcal{O}}(\\log1/\\delta)\\right)}}+{\\mathcal{O}}(L\\log T/\\delta)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "since $s(\\cdot)$ with its domain restricted to $[-2B,2B]$ is $L$ -Lipschitz. It remains to bound $\\Xi_{t}(\\mu_{t})$ , the expected information ratio for Algorithm 4. Now by definition of $\\mu_{t}$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\hat{\\Delta}_{t}(\\mu_{t})=(2-p_{t})\\hat{\\Delta}_{t}(\\pmb{x}_{t}^{(1)})+p_{t}\\Delta_{t}(\\pmb{x}_{t}^{(2)})}\\\\ &{\\qquad\\qquad=(2-p_{t})\\left(u_{t}+h_{t}(\\pmb{x}_{t}^{(1)},\\pmb{x}_{t}^{(1)})\\right)+p_{t}\\Delta_{t}(\\pmb{x}_{t}^{(2)})}\\\\ &{\\qquad\\qquad=2(1-p_{t})u_{t}+p_{t}(\\hat{\\Delta}_{t}(\\pmb{x}_{t}^{(2)})+u_{t}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and similarly ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mu_{t}}\\log\\left(1+\\frac{\\sigma_{t}^{D}(x,x^{\\prime})^{2}}{\\lambda\\kappa}\\right)=(1-p_{t})\\log\\left(1+\\frac{\\sigma_{t}^{D}(x_{t}^{(1)},x_{t}^{(1)})^{2}}{\\lambda\\kappa}\\right)+p_{t}\\log\\left(1+\\frac{\\sigma_{t}^{D}(x_{t}^{(1)},x_{t}^{(2)})^{2}}{\\lambda\\kappa}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=p_{t}\\log\\left(1+(\\lambda\\kappa)^{-1}\\sigma_{t}^{D}(x_{t}^{(1)},x_{t}^{(2)})^{2}\\right)\\qquad\\qquad\\qquad(\\sigma_{t}^{D}(x,x)=0)}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "allowing us to re-write the expected information ratio as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\Xi_{t}(\\mu_{t})=\\frac{\\big(2(1-p_{t})u_{t}+p_{t}(\\dot{\\Delta}_{t}(x_{t}^{(2)})+u_{t})\\big)^{2}}{4p_{t}\\log\\big(1+(\\lambda\\kappa)^{-1}v_{t}^{0}(x_{t}^{(1)},x_{t}^{(2)})\\big)^{2}}\\qquad}&{}\\\\ &{}&{\\leq\\frac{\\big((1-p_{t})u_{t}+p_{t}\\Delta_{t}(x_{t}^{(2)})\\big)^{2}}{p_{t}\\log\\big(1+(\\lambda\\kappa)^{-1}v_{t}^{0}(x_{t}^{(1)},x_{t}^{(2)})\\big)^{2}}}\\\\ &{}&{=\\frac{\\big((1-p)u_{t}+p\\dot{\\Delta}_{t}(x)\\big)^{2}}{\\upsilon_{t}p}}\\\\ &{}&{\\leq\\frac{\\Delta_{t}^{2}(x)}{\\upsilon_{t}}\\frac{\\Delta_{t}^{2}(x)}{\\log\\big(1+(\\lambda\\kappa)^{-1}\\sigma_{t}^{0}(x_{t}^{(1)},x)^{2}\\big)}}\\\\ &{}&\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$(u_{t}\\leq\\hat{\\Delta}_{t}(\\pmb{x}))$ ", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Now consider the definition of $u_{t}$ and let $\\mathscr{z}_{t}$ denote the action for which $u_{t}$ is achieved, i.e. $z_{t}=$ arg max $h(\\pmb{x},\\hat{\\pmb{x}}_{t}^{\\star})+\\bar{\\beta}_{t}(\\delta)\\sigma_{t}^{\\mathrm{D}}(\\pmb{x},\\hat{\\pmb{x}}_{t}^{\\star})$ . Then ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\hat{\\Delta}_{t}(z_{t})=h(\\hat{\\pmb x}_{t}^{\\star},z_{t})+\\bar{\\beta}_{t}(\\delta)\\sigma_{t}^{\\mathrm{D}}(z_{t},\\hat{\\pmb x}_{t}^{\\star})+h(z_{t},\\hat{\\pmb x}_{t}^{\\star})=\\bar{\\beta}_{t}(\\delta)\\sigma_{t}^{\\mathrm{D}}(\\pmb x,\\hat{\\pmb x}_{t}^{\\star}),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "therefore using the above chain of equations we may write ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Xi_{t}(\\mu_{t})\\leq\\underset{x}{\\operatorname*{min}}\\,\\frac{\\hat{\\Delta}_{t}^{2}(x)}{\\log\\big(1+\\sigma_{t}^{(1)}(x_{t}^{(1)},x)^{2}\\big)}}\\\\ &{\\qquad\\qquad\\leq\\frac{\\hat{\\Delta}_{t}^{2}(z_{t})}{\\log\\big(1+(\\lambda\\kappa)^{-1}\\sigma_{t}^{D}(x_{t}^{(1)},z_{t})^{2}\\big)}}\\\\ &{\\qquad\\leq\\frac{\\bar{\\beta}_{t}^{2}(\\delta)\\sigma_{t}^{\\mathrm{D}}(z_{t}\\wedge\\bar{x}_{t}^{\\intercal})^{2}}{\\log\\big(1+(\\lambda\\kappa)^{-1}\\sigma_{t}^{D}(x_{t}^{(1)},z_{t})^{2}\\big)}}\\\\ &{\\qquad\\leq\\frac{4\\bar{\\beta}_{t}^{2}(\\delta)}{\\log\\big(1+(\\lambda\\kappa)^{-1}\\sigma_{t}^{D}(x_{t}^{(1)},z_{t})^{2}\\big)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where last inequality holds due to the following argument. Recall that $k(\\pmb{x},\\pmb{x})\\leq1$ , implying that $\\sigma_{t}^{\\mathrm{D}}({\\bf{x}},{\\bf{x}}^{\\prime})^{2}\\leq\\dot{4}$ and therefore $\\log(1+\\sigma_{t}^{\\mathrm{D}}({\\pmb x},{\\bar{\\mathbf{x}}}^{\\prime})^{\\bar{2}})\\,\\geq\\,\\log(1+(\\lambda\\kappa)^{-1})\\sigma_{t}^{\\mathrm{D}}({\\pmb x},{\\pmb x}^{\\prime})^{2}/4$ , similar to Lemma 14. To conclude the proof, from (35) and (36) it holds that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R^{\\mathrm{D}}(T)\\leq L\\sqrt{\\displaystyle\\sum_{t=1}^{T}\\Xi_{t}(\\mu_{t})\\left(\\gamma_{T}+\\mathcal{O}(\\log1/\\delta)\\right)}+\\mathcal{O}(L\\log T/\\delta)}\\\\ &{\\qquad\\leq L\\sqrt{\\displaystyle\\sum_{t=1}^{T}\\frac{4\\bar{\\beta}_{t}^{2}(\\delta)}{\\log\\left(1+4(\\lambda\\kappa)^{-1}\\right)}\\left(\\gamma_{T}+\\mathcal{O}(\\log1/\\delta)\\right)}+\\mathcal{O}(L\\log T/\\delta)}\\\\ &{\\qquad\\leq L\\sqrt{\\displaystyle\\frac{4T\\bar{\\beta}_{T}^{2}(\\delta)}{\\log\\left(1+4(\\lambda\\kappa)^{-1}\\right)}\\left(\\gamma_{T}+\\mathcal{O}(\\log1/\\delta)\\right)}+\\mathcal{O}(L\\log T/\\delta)}\\\\ &{\\qquad=\\mathcal{O}\\left(\\beta_{T}^{\\mathrm{D}}(\\delta)\\sqrt{T(\\gamma_{T}+\\log1/\\delta)}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "with probability greater than $1-\\delta$ , simultaneously for all $T\\geq1$ . ", "page_idx": 25}, {"type": "text", "text": "C.4 Helper Lemmas for Appendix C.3 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Lemma 17. Let $0<\\delta<1$ and $f\\in\\mathcal{H}_{k}$ . Suppose $\\operatorname*{sup}_{a\\leq B}\\dot{s}(a)=L$ and $\\begin{array}{r}{\\operatorname*{sup}_{a\\leq B}1/\\dot{s}(a)=\\kappa}\\end{array}$ . Then ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\forall t\\geq0,\\pmb{x}\\in\\mathcal{X}:\\Delta(\\pmb{x})\\leq2\\hat{\\Delta}_{t}(\\pmb{x}))\\geq1-\\delta.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof of Lemma $^{l7}$ . Note that for any three inputs $\\pmb{x}_{1},\\pmb{x}_{2},\\pmb{x}_{3}$ ", "page_idx": 26}, {"type": "equation", "text": "$$\nh(\\pmb{x}_{1},\\pmb{x}_{3})=h(\\pmb{x}_{1},\\pmb{x}_{2})+h(\\pmb{x}_{2},\\pmb{x}_{3}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Therefore, from the definition of the estimated gap get ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\Delta}_{t}(\\pmb{x})=\\underset{\\pmb{z}\\in\\mathcal{X}}{\\mathrm{max}}\\,h(\\pmb{z},\\hat{\\pmb{x}}_{t}^{\\star})+h_{t}(\\hat{\\pmb{x}}_{t}^{\\star},\\pmb{x})+\\bar{\\beta}_{t}(\\delta)\\sigma_{t}^{D}(\\pmb{z},\\hat{\\pmb{x}}_{t}^{\\star})}\\\\ &{\\quad\\quad\\quad=\\underset{\\pmb{z}\\in\\mathcal{X}}{\\mathrm{max}}\\,h(\\pmb{z},\\pmb{x})+\\bar{\\beta}_{t}(\\delta)\\sigma_{t}^{D}(\\pmb{z},\\hat{\\pmb{x}}_{t}^{\\star})}\\\\ &{\\quad\\quad\\quad\\geq h(\\pmb{x},\\pmb{x})+\\bar{\\beta}_{t}(\\delta)\\sigma_{t}^{D}(\\pmb{x},\\pmb{x}_{t}^{\\star})}\\\\ &{\\quad\\quad\\quad=\\bar{\\beta}_{t}(\\delta)\\sigma_{t}^{D}(\\pmb{x},\\pmb{x}_{t}^{\\star}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Then going back to the definition of the true gap we may write ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta(\\pmb{x})=\\underset{\\pmb{z}\\in\\mathcal{X}}{\\mathrm{max}}\\,h(\\pmb{z},\\pmb{x})}\\\\ &{\\qquad=\\underset{\\pmb{z}\\in\\mathcal{X}}{\\mathrm{max}}\\,h(\\pmb{z},\\pmb{\\hat{x}_{t}^{\\star}})+h(\\pmb{\\hat{x}_{t}^{\\star}},\\pmb{x})}\\\\ &{\\qquad\\stackrel{\\mathrm{w.h.p.}}{\\le}\\,\\underset{z\\in\\mathcal{X}}{\\mathrm{max}}\\,h_{t}^{P}(z,\\pmb{\\hat{x}_{t}^{\\star}})+h_{t}(\\pmb{\\hat{x}_{t}^{\\star}},\\pmb{x})+\\bar{\\beta}_{t}(\\delta)\\Big(\\sigma_{t}^{D}(z,\\pmb{\\hat{x}_{t}^{\\star}})+\\sigma_{t}^{D}(\\pmb{\\hat{x}_{t}^{\\star}},\\pmb{x})\\Big)}\\\\ &{\\qquad=u_{t}+h_{t}^{P}(\\pmb{\\hat{x}_{t}^{\\star}},\\pmb{x})+\\bar{\\beta}_{t}(\\delta)\\sigma_{t}^{D}(\\pmb{\\hat{x}_{t}^{\\star}},\\pmb{x})}\\\\ &{\\qquad=\\hat{\\Delta}_{t}(\\pmb{x})+\\bar{\\beta}_{t}(\\delta)\\sigma_{t}^{D}(\\pmb{x}_{t}^{\\star},\\pmb{x})}\\\\ &{\\qquad\\le2\\hat{\\Delta}_{t}(\\pmb{x})}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "with probability greater than $1-\\delta$ . ", "page_idx": 26}, {"type": "text", "text": "Lemma 18. Assume $f\\in\\mathcal{H}_{k}$ . Suppose $\\begin{array}{r}{\\operatorname*{sup}_{a\\leq B}1/\\dot{s}(a)=\\kappa}\\end{array}$ . Then for any $0<\\delta<1$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\Big(\\forall t\\geq1,x\\in\\mathcal{X}:\\left|h(\\pmb{x},\\pmb{x}^{\\prime})-h_{t}^{P}(\\pmb{x},\\pmb{x}^{\\prime})\\right|\\leq\\bar{\\beta}_{t}(\\delta)\\sigma_{t}^{D}(\\pmb{x},\\pmb{x}^{\\prime};\\sqrt{\\lambda\\kappa})\\Big)\\geq1-\\delta}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\bar{\\beta}_{t}(\\delta):=2B+\\sqrt{\\frac{\\kappa}{\\lambda}}\\sqrt{2\\log{1/\\delta}+2\\gamma_{t}(\\sqrt{\\lambda\\kappa})}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof of Lemma 18. This lemma is effectively a weaker parallel of Corollary 5. We have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|h(\\pmb{x},\\pmb{x}^{\\prime})-h_{t}^{P}(\\pmb{x},\\pmb{x}^{\\prime})\\right|=\\left|f(\\pmb{x},)-f(\\pmb{x}^{\\prime})-\\left(f_{t}^{P}(\\pmb{x},)-f_{t}^{P}(\\pmb{x}^{\\prime})\\right)\\right|}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\left|\\psi^{\\top}(\\pmb{x},\\pmb{x}^{\\prime})(\\pmb{\\theta}^{\\star}-\\pmb{\\theta}_{t}^{P})\\right|}\\\\ &{\\qquad\\qquad\\qquad\\leq\\|\\psi(\\pmb{x},\\pmb{x}^{\\prime})\\|_{(V_{t}^{D})^{-1}}\\|\\pmb{\\theta}^{\\star}-\\pmb{\\theta}_{t}^{P}\\|_{V_{t}^{D}}}\\\\ &{\\qquad\\qquad\\qquad\\overset{\\mathrm{wh.p.~}}{\\leq}\\sqrt{\\lambda\\kappa}\\bar{\\beta}_{t}(\\delta)\\|\\psi(\\pmb{x},\\pmb{x}^{\\prime})\\|_{(V_{t}^{D})^{-1}}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\bar{\\beta}_{t}(\\delta)\\sigma_{t}^{D}(\\pmb{x},\\sqrt{\\lambda\\kappa})}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the third to last inequality holds with probability greater than $1-\\delta$ , but the rest of the inequalities hold deterministically. \u53e3 ", "page_idx": 26}, {"type": "image", "img_path": "wIE991zhXH/tmp/16a1464eed2456dbb6c1f2487f55892a55ae010aa278c0c09522907ff67bf03a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 3: Confidence sets for an illustrative problem with 3 arms at a single time step. Annotated arrows highlight the action selection for three common approaches. MAXMINLCB selects the action pair $(1,2)$ with the least regret. Upper-bound maximization (OPTIMISM) and information maximization (MAX INFO) choose sub-optimal arms. ", "page_idx": 27}, {"type": "text", "text": "Test Environments. We use a wide range of target functions common to the optimization literature [Jamil and Yang, 2013], to evaluate the robustness of MAXMINLCB. The results are reported in Table 1 and Table 2. Note that for the experiments we negate them all to get utilities. We use a uniform grid of 100 points over their specified domains and scale the utility values to $[-3,3]$ . ", "page_idx": 27}, {"type": "text", "text": "\u2022 Ackley: $\\mathcal{X}=[-5,5]^{d},d=2$ ", "page_idx": 27}, {"type": "equation", "text": "$$\nf(x)=-20\\exp\\left(-0.2{\\sqrt{{\\frac{1}{d}}\\sum_{i=1}^{d}x_{i}^{2}}}\\right)-\\exp\\left({\\frac{1}{d}}\\sum_{i=1}^{d}\\cos(2\\pi x_{i})\\right)+20+\\exp(1)\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "\u2022 Branin: $\\mathcal{X}=[-5,10]\\times[0,15]$ ", "page_idx": 27}, {"type": "equation", "text": "$$\nf({\\pmb x})=\\left(x_{2}-\\frac{5.1}{4\\pi^{2}}x_{1}^{2}+\\frac{5}{\\pi}x_{1}-6\\right)^{2}+10\\left(1-\\frac{1}{8\\pi}\\right)\\cos(x_{1})+10\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "\u2022 Eggholder: $\\mathcal{X}=[-512,512]^{2}$ ", "page_idx": 27}, {"type": "equation", "text": "$$\nf({\\pmb x})=-(x_{2}+47)\\sin\\left(\\sqrt{|x_{2}+\\frac{x_{1}}{2}+47|}\\right)-x_{1}\\sin\\left(\\sqrt{|x_{1}-(x_{2}+47)|}\\right)\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "\u2022 H\u00f6lder: $\\mathcal{X}=[-10,10]^{2}$ ", "page_idx": 27}, {"type": "equation", "text": "$$\nf(\\pmb{x})=-|\\sin(x_{1})\\cos(x_{2})\\exp\\left(|1-\\frac{\\sqrt{x_{1}^{2}+x_{2}^{2}}}{\\pi}|\\right)|\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "\u2022 Matyas: $\\mathcal{X}=[-10,10]^{2}$ ", "page_idx": 27}, {"type": "equation", "text": "$$\nf({\\pmb x})=0.26(x_{1}^{2}+x_{2}^{2})-0.48x_{1}x_{2}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "\u2022 Michalewicz: $\\mathcal{X}=[0,\\pi]^{d},d=2,m=10$ ", "page_idx": 27}, {"type": "equation", "text": "$$\nf(\\pmb{x})=-\\sum_{i=1}^{d}\\sin(x_{i})\\sin^{2m}\\left(\\frac{i x_{i}^{2}}{\\pi}\\right)\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "\u2022 Rosenbrock: $\\mathcal{X}=[-5,10]^{2}$ ", "page_idx": 27}, {"type": "equation", "text": "$$\nf(\\pmb{x})=\\sum_{i=1}^{d-1}\\left[100(x_{i+1}-x_{i}^{2})^{2}+(x_{i}-1)^{2}\\right]\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Algorithm 5 DOUBLER [Ailon et al., 2014] ", "page_idx": 28}, {"type": "text", "text": "Input $(\\beta_{t}^{\\mathrm{D}})_{t\\geq1}$ .   \nLet $\\mathcal{L}$ be any action from $\\mathcal{X}$   \nfor $t\\geq1$ do for $j=1,\\dots,2^{t}$ do Select $\\mathbf{\\boldsymbol{x}}_{t}^{\\prime}$ uniformly randomly from $\\mathcal{L}$ Select $\\begin{array}{r}{\\pmb{x}_{t}^{\\prime}=\\arg\\operatorname*{max}_{\\pmb{x}\\in\\mathcal{M}_{t}}s\\(h_{t}(\\pmb{x},\\pmb{x}_{t}^{\\prime}))+\\beta_{t}^{\\mathrm{D}}\\sigma_{t}^{\\mathrm{D}}(\\pmb{x},\\pmb{x}_{t}^{\\prime})}\\end{array}$ Observe $y_{t}$ and append history. Update $h_{t+1}$ and $\\bar{\\sigma}_{t+1}^{\\mathrm{D}}$ end for $\\mathcal{L}\\gets$ the multi-set of actions played as $\\mathbf{\\boldsymbol{x}}_{t}^{\\prime}$ in the last for-loop over index $j$   \nend for ", "page_idx": 28}, {"type": "text", "text": "Algorithm 6 MULTISBM [Ailon et al., 2014] ", "page_idx": 28}, {"type": "text", "text": "Input $(\\beta_{t}^{\\mathrm{D}})_{t\\geq1}$ .   \nfor $t\\geq1$ do Set $\\pmb{x}_{t}\\gets\\pmb{x}_{t-1}^{\\prime}$ Select $\\begin{array}{r}{\\pmb{x}_{t}^{\\prime}=\\arg\\operatorname*{max}_{\\pmb{x}\\in\\mathcal{M}_{t}}s\\big(h_{t}(\\pmb{x},\\pmb{x}_{t})\\big)+\\beta_{t}^{\\mathrm{D}}\\sigma_{t}^{\\mathrm{D}}(\\pmb{x},\\pmb{x}_{t})}\\end{array}$ Observe $y_{t}$ and append history. Update $h_{t+1}$ and $\\tilde{\\sigma}_{t+1}^{\\mathrm{D}}$ and the set of plausible maximizers $\\mathcal{M}_{t+1}=\\{x\\in\\mathcal{X}|\\,\\forall\\pmb{x}^{\\prime}\\in\\mathcal{X}:\\,s(h_{t+1}(\\pmb{x},\\pmb{x}^{\\prime}))+\\beta_{t+1}^{\\mathrm{D}}\\sigma_{t+1}^{\\mathrm{D}}(\\pmb{x},\\pmb{x}^{\\prime})>1/2\\}.$ ", "page_idx": 28}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "end for ", "page_idx": 28}, {"type": "text", "text": "Algorithm 7 RUCB [Zoghi et al., 2014a] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Input $(\\beta_{t}^{\\mathrm{D}})_{t\\geq1}$ .   \nfor $t\\geq1$ do Select $\\mathbf{\\boldsymbol{x}}_{t}^{\\prime}$ uniformly randomly from $\\mathcal{M}_{t}$ Select $\\begin{array}{r}{\\pmb{x}_{t}^{\\leftarrow}=\\arg\\operatorname*{max}_{\\pmb{x}\\in\\mathcal{M}_{t}}s\\(h_{t}(\\pmb{x},\\pmb{x}_{t}^{\\prime}))+\\beta_{t}^{\\mathrm{D}}\\sigma_{t}^{\\mathrm{D}}(\\pmb{x},\\pmb{x}_{t}^{\\prime})}\\end{array}$ Observe $y_{t}$ and append history. Update $h_{t+1}$ and $\\bar{\\sigma}_{t+1}^{\\mathrm{D}}$ and the set of plausible maximizers $\\mathcal{M}_{t+1}=\\{x\\in\\mathcal{X}|\\,\\forall\\pmb{x}^{\\prime}\\in\\mathcal{X}:\\,s(h_{t+1}(\\pmb{x},\\pmb{x}^{\\prime}))+\\beta_{t+1}^{\\mathrm{D}}\\sigma_{t+1}^{\\mathrm{D}}(\\pmb{x},\\pmb{x}^{\\prime})>1/2\\}.$ ", "page_idx": 28}, {"type": "text", "text": "Acquisition Function Maximization. In our computations, to eliminate additional noise coming from approximate solvers, we use an exhaustive search over the domain for the action selection of LGP-UCB, MAXMINLCB, and other presented algorithms. For the numerical experiments presented in this paper, we do not consider this as a practical limitation. Due to our efficient implementation in JAX, this optimization step can be carried out in parallel and seamlessly support accelerator devices such as GPUs and TPUs. ", "page_idx": 28}, {"type": "text", "text": "Hyper-parameters for Logistic Bandits. We set $\\delta=0.1$ for all algorithms. For GP-UCB and LGP-UCB, we set $\\beta\\,=\\,1$ , and 0.25 for the noise variance. We use the Radial Basis Function (RBF) kernel and choose the variance and length scale parameters from [0.1, 1.0] to optimize their performance separately. For LGP-UCB, we tuned $\\lambda$ , the $L2$ penalty coefficient in Proposition 1, on the grid $[0.0,0.1,1.0,5.0]$ and $B$ on [1.0, 2.0, 3.0]. The hyper-parameter selections were done for each algorithm separately to create a fair comparison. ", "page_idx": 28}, {"type": "text", "text": "Hyper-parameters for Preference-based Bandits. We tune the same parameters of LGP-UCB for the preference feedback bandit problem on the following grid: $\\lambda\\in[0,0.1,1]$ , $B\\in[1,2,3]$ , and [0.1, 1] for the kernel variance and length scale. The same hyper-parameters are tuned separately for every baseline . ", "page_idx": 28}, {"type": "text", "text": "Pseudo-code for Baselines. Algorithm 5, Algorithm 6, and Algorithm 7 described the baselines used for the benchmark of Section 6.2. MAXINP and IDS are defined in Algorithm 3 and Algorithm 4, respectively, in Appendix C.3 alongside with their theoretical analysis. We note that DOUBLER includes an internal for-loop, therefore, we adjusted the time-horizon $T$ such that it observes the same number of feedback $y_{t}$ as the other algorithms for a fair comparison. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "Computational Resources and Costs. We ran our experiments on a shared cluster equipped with various NVIDIA GPUs and AMD EPYC CPUs. Our default configuration for all experiments was a single GPU with $24\\ \\mathrm{GB}$ of memory, 16 CPU cores, and $16\\;\\mathrm{GB}$ of RAM. Each experiment of the 11 configurations reported in Section 6.2 ran for about 12 hours and the experiment reported in Section 6.1 ran for 5 hours. The total computational cost to reproduce our results is around 140 hours of the default configuration. Our total computational costs including the failed experiments are estimated to be 2-3 times more. ", "page_idx": 29}, {"type": "text", "text": "D.1 Yelp Experiment ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We filter the Yelp data3 for restaurants in Philadelphia, USA with at least 500 reviews and users who reviewed at least 90 restaurants. The final dataset includes 275 restaurants, 20 users, and a total of 2563 reviews. We define the action space $\\mathcal{X}$ by assigning to each restaurant their respective 32-dimensional embedding of their reviews, i.e., $\\mathcal{X}\\overset{\\cdot}{\\subseteq}\\mathbb{R}^{3\\overline{{2}}}$ . For each restaurant, we concatenate all reviews in the filtered dataset and we use the TEXT-EMBEDDING-3-LARGE OpenAI embedding model 4 to retrieve the embeddings. The Yelp dataset provides utility values for users in the form of ratings on the scale of 1 to 5, however, not all users rated every restaurant. We use collaborative flitering to estimate the missing reviews [Schafer et al., 2007]. For each user, these values are used as the utilities for restaurants during the simulation. Experiments are conducted separately for each user, therefore, utility values are not aggregated but the action space is identical for each experiment. ", "page_idx": 29}, {"type": "text", "text": "Note that we do not assume any explicit functional form of the utility functions $f$ that we calibrate to this data. Instead, the actions space $\\mathcal{X}$ and utility values are derived separately from the dataset. Regardless, the results presented in Section 6.3 show that our kernelized method achieves good performance on this task. ", "page_idx": 29}, {"type": "text", "text": "E Additional Experiments ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In this section, we provide Table 2 that details our logistic bandit benchmark, complementing the results in Section 6.1. Figure 4 and Figure 9 show the logistic and dueling regret of additional test functions, complementing the results of Table 1. ", "page_idx": 29}, {"type": "table", "img_path": "wIE991zhXH/tmp/8397f0818c982c2d6ac27fa2c4b08307386a73c24294a36f6eeada1004401eb3.jpg", "table_caption": ["Table 2: Benchmarking $R_{T}^{\\mathrm{L}}$ for a variety of test utility functions, $T=2000$ . "], "table_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "wIE991zhXH/tmp/8e148a9e00f3e5c04ba5d633b3150d4ab63ec0b9d4816c4bc30d93cbb4fcdbfa.jpg", "img_caption": ["Figure 4: Regret with Branin utility function with logistic and preference feedback. "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "wIE991zhXH/tmp/f299e54e66952e2452b48b8fec4f3702f64f837f049ccf808af6651d8f3db921.jpg", "img_caption": ["Figure 9: Top to bottom Regret for Eggholder, H\u00f6lder, Matyas Michalewicz, Rosenbrock functions, with logistic and preference feedback. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We provide theoretical guarantees of the proposed algorithms in Section 4 and Section 5. In Section 6, we provide the results for our numerical experiments supporting our claims. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper is of theoretical nature. All theoretical assumptions are presented in the text, and we discuss how limiting they are. This is mainly in the Problem Setting section, or theorem statements. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 31}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: Proposition 1, Theorem 2, Corollary 3, Proposition 4, Corollary 5, Theorem 6 describe our theoretical proofs with detailed explanation of the assumptions. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We describe the experimental setting in Section 6 while providing further details on the hyperparameter selection, pseudocode, and utility function definition in Appendix E. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. ", "page_idx": 32}, {"type": "text", "text": "In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 33}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We include the code used to carry out the experiments in the Supplementary Material. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 33}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We provide comprehensive explanations of the setting, hyperparameters, algorithms, and functions used in Section 6 and in Appendix E. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 33}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We report all results in our figures and tables as the average and standard error over 20 random seeds. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 33}, {"type": "text", "text": "\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We describe computation resources and time used for the experiments in Appendix D. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 34}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The authors have reviewed the NeurIPS Code of Ethics and conducted the research following the guidelines. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 34}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: The research presented in this paper is theoretical without any immediate societal impact. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 35}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: To the best of the authors\u2019 knowledge, there is no such risk involved. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 35}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: Assets used to conduct the research presented in this paper are always cited and properly credited. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 36}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 37}]