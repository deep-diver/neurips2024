[{"type": "text", "text": "PrefPaint: Aligning Image Inpainting Diffusion Model with Human Preference ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Kendong Liu1\u2217, Zhiyu Zhu1\u2217\u2020, Chuanhao $\\mathbf{L}\\mathbf{i}^{2*}$ , Hui Liu3 , Huanqiang Zeng4 , Junhui Hou1 ", "page_idx": 0}, {"type": "text", "text": "1City University of Hong Kong 2Yale University 3Saint Francis University 4Huaqiao University {kdliu2-c, zhiyuzhu2-c}@my.cityu.edu.hk chuanhao.li.cl2637@yale.edu h2liu@sfu.edu.hk jh.hou@cityu.edu.hk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this paper, we make the first attempt to align diffusion models for image inpainting with human aesthetic standards via a reinforcement learning framework, significantly improving the quality and visual appeal of inpainted images. Specifically, instead of directly measuring the divergence with paired images, we train a reward model with the dataset we construct, consisting of nearly 51,000 images annotated with human preferences. Then, we adopt a reinforcement learning process to fine-tune the distribution of a pre-trained diffusion model for image inpainting in the direction of higher reward. Moreover, we theoretically deduce the upper bound on the error of the reward model, which illustrates the potential confidence of reward estimation throughout the reinforcement alignment process, thereby facilitating accurate regularization. Extensive experiments on inpainting comparison and downstream tasks, such as image extension and 3D reconstruction, demonstrate the effectiveness of our approach, showing significant improvements in the alignment of inpainted images with human preference compared with stateof-the-art methods. This research not only advances the field of image inpainting but also provides a framework for incorporating human preference into the iterative refinement of generative models based on modeling reward accuracy, with broad implications for the design of visually driven AI applications. Our code and dataset are publicly available at https://prefpaint.github.io. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Image inpainting, the process of filling in missing or damaged parts of images, is a critical task in computer vision with applications ranging from photo restoration [1, 2] to content creation [3, 4]. Traditional approaches have leveraged various techniques, from simple interpolation [5\u20137] to complex texture synthesis [8, 9], to achieve visually plausible results. The recent advent of deep learning, particularly diffusion models, has revolutionized the field by enabling more coherent and contextually appropriate inpaintings [3, 10\u201312]. Despite these advancements, a significant gap remains between the technical success of these models and their alignment with human aesthetic preferences, which are inherently subjective and intricate. As shown in Fig. 1, the existing stable diffusion-based inpainting model tends to generate weird and discord reconstruction. ", "page_idx": 0}, {"type": "text", "text": "Human preference for visual content is influenced by complex and mutual factors, including but not limited to personal background, experiences, and the context in which the content is viewed [13, 14]. This makes the task of aligning inpainting models with human preference particularly challenging, as it requires the model not only to understand the content of the missing parts but also to predict and adapt to the diverse tastes of its users. ", "page_idx": 0}, {"type": "image", "img_path": "fVRCsK4EoM/tmp/79f26d5d06aa0ca27d63c220089994ebba76f04e4ea38dab120525acf1383ea1.jpg", "img_caption": ["Figure 1: Visual comparisons of the results by the diffusion-based image inpainting model named \u201cRunway\", and the aligned model through the proposed method. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Inspired by recent advancements in the reinforced alignment of large pre-trained models [15\u201319], we propose to align diffusion models for image inpainting with human preference through reinforcement learning. Specifically, our approach is grounded in the hypothesis that by incorporating human feedback into the training loop, we can guide the model toward generating inpainted images that are not only technically proficient but also visually appealing to users. Technically, we formulate the boundary of each reward prediction in terms of the model\u2019s alignment with human aesthetic preferences, thereby leveraging the accuracy of the reward model to amplify the regularization strength on more reliable samples. Extensive experiments validate that the proposed method can consistently reconstruct visually pleasing inpainting results and greatly surpass state-of-the-art methods. ", "page_idx": 1}, {"type": "text", "text": "In summary, the main contributions of this paper lie in: ", "page_idx": 1}, {"type": "text", "text": "\u2022 we make the first attempt to align diffusion models for image inpainting with human preferences by integrating human feedback through reinforcement learning; \u2022 we theoretically deduce the accuracy bound of the reward model, modulating the refinement process of the diffusion model for robustly improving both efficacy and efficiency; and \u2022 we construct a dataset containing 51,000 inpainted images annotated with human preferences. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Reinforcement Learning & Model Alignment. Reinforcement learning [20\u201322] is a paradigm of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. The foundational theory of reinforcement learning is rooted in the concepts of Markov decision processes [23], which provide a mathematical structure for modeling decision-making in environments with stochastic dynamics and rewards [24, 25]. With the advent of deep learning, deep reinforcement learning [21, 26] has significantly expanded the capabilities and applications of traditional reinforcement learning. The integration of neural networks with reinforcement learning, exemplified by the Deep Q-Network [27, 28] algorithm, has enabled the handling of high-dimensional state spaces, which were previously intractable. Subsequent innovations, including policy gradient methods [29, 30] like Proximal Policy Optimization [31] and actor-critic frameworks [32, 33] like Soft Actor-Critic [34], have further enhanced the efficiency and stability of learning in complex environments. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "The recent surge in popularity of large-scale models has significantly underscored the importance of reinforcement learning in contemporary AI research and application [15, 16]. As these models, including large language models [35, 36] and deep generative networks [37\u201339], become more prevalent, reinforcement learning is increasingly employed to fine-tune, control, and optimize their behaviors in complex, dynamic environments. This integration is particularly visible in areas such as natural language processing, where reinforcement learning techniques are used to improve the conversational abilities of chat-bots and virtual assistants, making them more adaptive and responsive to user needs. [17\u201319] Moreover, in the realm of content recommendation and personalization, reinforcement learning algorithms are instrumental in managing the balance between exploration of new content and exploitation of known user preferences, significantly enhancing the user experience. The growing intersection between large models and reinforcement learning not only pushes the boundaries of what\u2019s achievable in AI but also amplifies the need for sophisticated reinforcement learning techniques that can operate at scale, adapt in real-time, and make decisions under uncertainty, thereby marking a pivotal evolution in how intelligent systems are developed and deployed. ", "page_idx": 2}, {"type": "text", "text": "Diffusion Model. Diffusion models have recently emerged as a powerful class of generative models [38\u201342], demonstrating remarkable success in generating high-quality, coherent images [43, 44]. These models work by gradually transforming a distribution of random noise into a distribution of images, effectively \u2019diffusing\u2019 the noise into structured patterns [45, 46]. In the context of image inpainting, diffusion models offer a significant advantage by leveraging their generative capabilities to predict and flil in missing parts of images in a way that is contextually and visually coherent with the surrounding image content [47, 48]. ", "page_idx": 2}, {"type": "text", "text": "Recent studies have showcased the potential of diffusion models in achieving state-of-the-art results in image inpainting tasks, outperforming previous generative models like Generative Adversarial Networks in terms of image quality and coherence [49, 50]. However, while these models excel in technical performance, there remains a gap in their ability to cater to diverse human aesthetic preferences. Most existing works focus on the objective quality of inpainting results, such as fidelity to the original image and coherence of the generated content, with less attention given to subjective satisfaction or preference alignment. ", "page_idx": 2}, {"type": "text", "text": "3 Proposed Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Due to the random masking within the task of image inpainting, there may not be a definitive causal relationship between the known and inpainted contents, which manifests as a one-to-many issue. Consequently, stringent per-pixel regularization inherently results in unnatural reconstruction, which significantly diverges from samples conforming to human preference. To this end, we propose a reinforcement learning-based alignment process involving human preferences to fine-tune pre-trained diffusion models for image inpainting, aiming to improve the visual quality of inpainted images (Sec. 3.1). More importantly, we theoretically deduce the upper bound on the error of the reward model (Sec. 3.2). Based on this deduction, we formulate a reward trustiness-aware alignment process that is more efficient and effective (Sec. 3.3). ", "page_idx": 2}, {"type": "text", "text": "3.1 Reinforced Training of Diffusion Models for Image Inpainting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Diffusion models [38, 40] iteratively refine a randomly sampled standard Gaussian distributed noise, resulting in a generated image. To adjust the distribution of diffusion models, we introduce human feedback rewards to measure and regularize the distribution of model sampling outputs. To be specific, instead of applying standard policy-gradient descent [51] that is hard to converge to high-quality models, inspired by classical methods, e.g., TRPO/PPO [31, 52], which introduces a model trust region to avoid potential model collapse during the training process, we achieve the reinforced ", "page_idx": 2}, {"type": "text", "text": "training of a diffusion model as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}\\mathcal{J}(\\pmb{x})=-\\int_{P_{\\theta^{\\prime}}}\\frac{\\nabla_{\\theta}P_{\\theta}(\\pmb{x})}{P_{\\theta^{\\prime}}(\\pmb{x})}\\mathcal{R}(\\pmb{x})+\\kappa\\nabla_{\\theta}\\mathcal{D}(P_{\\theta^{\\prime}}|P_{\\theta}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathcal{R}(\\cdot)$ represents the reward model, which quantifies the quality of diffusion samples; $\\textbf{\\em x}$ is the reconstructed sample; $P_{\\theta}$ and $P_{\\theta^{\\prime}}$ represent the probabilistic functions of training and reference models parameterized with $\\pmb{\\theta}$ and $\\pmb{\\theta}^{\\prime}$ , respectively; $\\nabla\\theta$ calculates the derivative on $\\theta;\\mathcal{D}(\\cdot|\\cdot)$ is the divergence measurement of the given two distributions for regularizing the distribution shifting; the hyperparameter $\\kappa$ balances the two terms. Inspired by recent work [53, 54], we take the same step to measure the probability of each diffusion step via calculating the probability of perturbation noise, e.g., the DDIM sampling algorithm [40], ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{x_{t-1}=\\underbrace{\\sqrt{\\alpha_{t-1}}\\left(\\frac{x_{t}-\\sqrt{1-\\alpha_{t}}\\epsilon_{\\theta}^{(t)}({\\pmb x}_{t})}{\\sqrt{\\alpha_{t}}}\\right)+\\sqrt{1-\\alpha_{t-1}-\\sigma_{t}^{2}}\\epsilon_{\\theta}^{(t)}({\\pmb x}_{t})}_{\\bar{x}_{t-1}}+\\underbrace{\\sigma_{t}\\epsilon_{t}}_{x_{t-1}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\epsilon_{\\pmb\\theta}^{(t)}(\\cdot)$ is the noise estimate diffusion network; $t$ is the diffusion step; the scalar $\\alpha_{t}$ controls the noise to signal ratio; and $\\sigma_{t}\\epsilon_{t}$ represents a Gaussian noise to increase the sampling diversity. Thus, the result of each reverse step consists of a deterministic component $\\bar{x}_{t-1}$ and a probabilistic component $\\widetilde{\\mathbf{x}}_{t-1}$ . Since $\\mathbf{\\boldsymbol{x}}_{t-1}\\sim\\mathcal{N}(\\bar{\\mathbf{\\boldsymbol{x}}}_{t-1},\\sigma_{t}\\mathbf{I})$ , we calculate the density of each step $\\begin{array}{r}{P=\\Phi\\left(\\frac{x_{t-1}-\\bar{x}_{t-1}}{\\sigma_{t}}\\right)}\\end{array}$ to approximate the probability, where $\\Phi(\\cdot)$ denotes the density function of standard Gaussian distribution. ", "page_idx": 3}, {"type": "text", "text": "3.2 Bounding Reward Model Error ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The precision of the reward model $\\mathcal{R}(\\cdot)$ assumes a pivotal function within this learning framework as it directs the optimization trajectory. In this section, we theoretically derive its error upper bound, which can facilitate the reinforced training process in terms of both efficacy and efficiency. ", "page_idx": 3}, {"type": "text", "text": "Denote by ${\\bf X}\\,=\\,[{\\pmb x}_{1},{\\pmb x}_{2},\\cdot\\cdot\\cdot\\,,{\\pmb x}_{N}]\\,\\in\\,\\mathbb{R}^{N\\times D}$ and $\\textit{\\textbf{y}}\\equiv$ $[y_{1},y_{2},\\ \\ \\cdot\\ \\cdot\\ ,\\ y_{N}]\\ \\in\\ \\mathbb{R}^{N\\times1}$ a set of $N$ inpainted images of dimension $D$ by diffusion models and corresponding ground-truth reward values from human experts, respectively. By dividing the reward model into two parts, i.e., feature extractor $\\bar{\\mathcal{F}}(\\cdot)$ and linear regression weights $\\psi\\ \\in\\ \\mathbb{R}^{D^{\\prime}\\times1}$ , we thus can represent it as $\\langle\\mathcal{F}(\\pmb{x}_{t}),\\psi\\rangle$ , where $\\langle\\cdot,\\cdot\\rangle$ calculates the inner product of two vectors. We can formulate the learning process of the last weights as the result of the following optimization process: ", "page_idx": 3}, {"type": "image", "img_path": "fVRCsK4EoM/tmp/a83704b21b4f3a8a06fcb20193d39ef20b820addba3b9203a099df1b634cb562.jpg", "img_caption": ["Figure 2: Experimental plot of reward prediction error vs. $\\|z\\|\\bar{\\mathbf{v}}^{-1}$ on the validation set, where a dashed line is an upper boundary of error, positively relative to $\\|z\\|_{\\mathbf{V}^{-1}}$ . ", ""], "img_footnote": [], "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{\\boldsymbol{\\psi}}=\\underset{\\boldsymbol{\\psi}}{\\operatorname{argmin}}\\left\\|\\langle\\mathcal{F}(\\mathbf{X}),\\boldsymbol{\\psi}\\rangle-\\pmb{y}\\right\\|_{2}^{2}+\\lambda\\|\\boldsymbol{\\psi}\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\|\\cdot\\|_{2}$ is the $\\ell_{2}$ norm of a vector, $\\lambda\\,>\\,0$ , and the second term is used for alleviating the over-fitting phenomenon. Thus, we have ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\boldsymbol\\psi}=(\\mathbf{Z}^{\\mathsf{T}}\\mathbf{Z}+\\lambda\\mathbf{I})^{-1}\\mathbf{Z}^{\\mathsf{T}}\\boldsymbol{y},\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ {\\hat{\\boldsymbol\\psi}-\\boldsymbol\\psi_{*}=(\\mathbf{Z}^{\\mathsf{T}}\\mathbf{Z}+\\lambda\\mathbf{I})^{-1}\\mathbf{Z}^{\\mathsf{T}}\\boldsymbol{\\zeta}-\\lambda(\\mathbf{Z}^{\\mathsf{T}}\\mathbf{Z}+\\lambda\\mathbf{I})^{-1}\\boldsymbol\\psi_{*},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{Z}\\in\\mathbb{R}^{N\\times D^{\\prime}}=\\mathcal{F}(\\mathbf{X})$ is the set of embeddings of $\\mathbf{X}$ . $\\psi_{\\ast}$ is the ideal weight with ${\\pmb y}={\\bf Z}{\\pmb\\psi}_{\\ast}+{\\pmb\\zeta}$ and $\\zeta$ a noise term between $\\mathbf{Z}\\psi_{*}$ and $\\textit{\\textbf{y}}$ . Then, the following term also holds: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{z^{\\mathsf{T}}\\hat{\\boldsymbol{\\psi}}-z^{\\mathsf{T}}\\boldsymbol{\\psi}_{*}=\\!z^{\\mathsf{T}}(\\mathbf{Z}^{\\mathsf{T}}\\mathbf{Z}+\\lambda\\mathbf{I})^{-1}\\mathbf{Z}^{\\mathsf{T}}\\boldsymbol{\\zeta}-\\lambda z^{\\mathsf{T}}(\\mathbf{Z}^{\\mathsf{T}}\\mathbf{Z}+\\lambda\\mathbf{I})^{-1}\\boldsymbol{\\psi}_{*},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\pmb{z}\\in\\mathbb{R}^{D^{\\prime}\\times1}=\\mathcal{F}(\\pmb{x})$ is the reward embedding of a typical input sample. Based on CauchySchwarz inequality, there is ", "page_idx": 3}, {"type": "equation", "text": "$$\n|z^{\\mathsf{T}}\\hat{\\psi}-z^{\\mathsf{T}}\\psi_{*}|\\leq\\|z\\|_{\\mathsf{V}^{-1}}(\\|\\mathbf{Z}^{\\mathsf{T}}\\zeta\\|_{\\mathsf{V}^{-1}}+\\lambda^{1/2}\\|\\psi_{*}\\|_{2}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "image", "img_path": "fVRCsK4EoM/tmp/16fc53e3eeb57ef5255575322a0dea70a2958753eea4a1847ff694d0851b1f85.jpg", "img_caption": ["Figure 3: Statistical characteristics of the dataset we constructed. (a) the score distribution of the images across different selected datasets; (b) the comparison between the distribution of the average score and score for details; (c) and (d) show the numbers of images with different mask ratios on the outpainting and warping splits, respectively. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{V}=\\mathbf{Z}^{\\mathsf{T}}\\mathbf{Z}+\\lambda\\mathbf{I}$ and $\\|z\\|_{\\mathbf{V}^{-1}}:=\\|z^{\\mathsf{T}}\\mathbf{V}^{-1}\\|_{2}$ . Moreover, based on Theorem 2 of [55], for $\\delta>0$ , we have $1-\\delta$ probability to make following inequality stands ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\|\\mathbf{Z}^{\\mathsf{T}}\\epsilon\\|_{\\mathbf{V}^{-1}}\\leq B\\sqrt{2\\log(\\frac{\\mathsf{d e t}(\\mathbf{V})^{1/2}\\mathsf{d e t}(\\lambda\\mathbf{I})^{-1/2}}{\\delta})},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathtt{d e t}(\\cdot)$ computes the determinant of the matrix, and $B$ is a scalar. We have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|z^{\\mathsf{T}}\\hat{\\psi}-z^{\\mathsf{T}}\\psi_{*}|\\le\\|z\\|_{\\mathsf{V}^{-1}}\\underbrace{\\left(B\\sqrt{2\\log(\\frac{\\mathsf{d e t}(\\mathbf{V})^{1/2}\\mathsf{d e t}(\\lambda\\mathbf{I})^{-1/2}}{\\delta})}+\\lambda^{1/2}\\|\\psi_{*}\\|_{2}\\right)}_{C_{b o u n d}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Due to $C_{b o u n d}$ being constant after the training of the reward model, we can conclude that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{z\\sim p(z)}|z^{\\mathsf{T}}\\hat{\\psi}-z^{\\mathsf{T}}\\psi_{*}|\\propto\\|z\\|_{\\mathbf{V}^{-1}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where sup represents the upper bound. Such a theoretical bound is also experimentally verified in Fig. 2. ", "page_idx": 4}, {"type": "text", "text": "3.3 Reward Trustiness-Aware Alignment Process ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "According to Eq. (10), it can be known that the error of the reward model is bounded by $\\|z\\|_{\\mathbf{V}^{-1}}$ , there might be a relatively large error for those $\\|z\\|_{\\mathbf{V}^{-1}}$ quite large. Thus, we further propose a weighted regularization strategy to amplify the penalty strength of those samples in the high-trust region. Specifically, we calculate the amplification factor as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\gamma=e^{-k\\|z\\|_{\\mathbf{V}^{-1}}+b},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the hyperparameters $k=0.05$ and $b=0.7$ are used for scaling the regularization strength. We then define the overall gradient of the reward trustiness-aware alignment process as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}\\mathcal{J}^{\\prime}(\\pmb{x})=\\gamma\\nabla_{\\theta}\\mathcal{J}(\\pmb{x}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where ${\\mathcal{I}}^{\\prime}(x)$ is the weighted reward loss for sample $\\mathbf{x}$ . For the updating of the reference model $\\theta^{\\prime}$ , we adopt [53] to update the model in each optimization step. Correctly amplifying the scaling factor can both speed up the convergence speed and reconstruction effectiveness, as experimentally validated in Sec. 5.2. ", "page_idx": 4}, {"type": "text", "text": "4 Human Preference-Centric Dataset for Reward ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We first randomly selected 6,000, 4,000, 6,000, and 1,000 images with diverse content from ADE20K [56, 57], ImageNet [58], KITTI [59], and Div2K [60, 61] datasets, respectively. We then applied the operations described below to generate prompt images (i.e., incomplete images), which were further fed into the diffusion model for image inpainting named Runway [62], producing inpainted images. To mitigate the potential bias of the reward model on different rewards, we repetitively generated from a given prompt image three distinct inpainted images. Consequently, we obtained 51,000 inpainted images in total, which were scored by human experts following the criteria described below. ", "page_idx": 4}, {"type": "table", "img_path": "fVRCsK4EoM/tmp/795f8b042e75937e0564f50eb620c616d72790a56e6e025a151d9c03362235f4.jpg", "table_caption": ["Table 1: Quantitative comparisons of different methods. $\\star$ indicates the small model (non SD-based); \u201c $S^{\\bullet}$ is the number of sampling times. For the calculation of WinRate, we first derive the best sample of the compared method among $S$ sampling times. Then, we calculate it as $\\textstyle{\\frac{T_{w}}{T}}$ , where $T_{w}$ indicates the number of compared samples that surpass the results of Runway $[S=1$ ) and $T$ is the total number of prompts. \u201c\u2191(resp. \u2193)\" means the larger (resp. smaller), the better. We normalized the predicted reward values with the dataset distribution. \u201cVar\u201d calculates the variance of different sampling times, showing the consistency of generation quality. (See the Supplementary Material for more details.) "], "table_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "", "img_caption": ["Table 2: Comparison across metrics: higher Figure 4: WinRate comparison heat-map between values are better for all metrics except \"Rank\". different methods. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Generation of Incomplete Images. We considered two distinctive image completion patterns: inpainting and outpainting. For inpainting, we simulated warping holes on images by changing the viewpoints, where we derived the depth of the scene using [75]. Following past practice [76], we defined a camera sequence that forms a sampling grid with three columns for yaw and three rows for pitch. The resulting nine views feature yaw angles within a $\\pm0.3$ range (i.e., a total range of $35^{\\circ}$ ) and pitch angles within a $\\pm0.15$ range (i.e., a total range of $17^{\\circ}$ ). As the range of views increases, the task of inpainting becomes progressively more challenging. For outpainting, we randomly masked the boundary of the image through two types of random cropping methods: (1) square cropping, where the size of the prompt ranges from $15\\%$ to $25\\%$ of the image size $(512\\times512)$ randomly; (2) rectangular cropping, where the height of the prompt matches the image size, while the width is randomly sampled between $35\\%$ and $40\\%$ of the image size. Each cropping method accounts for half of the outpainting prompts. ", "page_idx": 5}, {"type": "text", "text": "Scoring Criteria. In the scoring process, we employed three criteria, i.e., (1) structural rationality representing the rationality of the overall structure, whether illogical objects and structures are generated; (2) feeling of local texture showing whether strange textures are generated that do not conform to the characteristics of the object; and (3) overall feeling indicating the impression capturing the overall feeling upon first glances at the image. The score value is in the range of 1 to 7, indicating from the least to most favorable. The final score for reward model training is derived by averaging those three scores using the weights of [0.15, 0.15, 0.7]. We refer readers to the Supplementary Material for more details about dataset process and labeling scheme. ", "page_idx": 5}, {"type": "image", "img_path": "fVRCsK4EoM/tmp/4fff6a1eb171166e72a246c9289b8535f5184a0d22f282b46000ca568cee75d9.jpg", "img_caption": ["Figure 5: Visual comparisons of our approach and SOTA methods. The prompted images of $5^{t h}$ and $7^{t\\widetilde{h}}$ rows are generated by boundary cropping, while the remaining rows by warping. All images were generated with the same random seeds. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Statistical Characteristics of our Dataset. As illustrated in Fig. 3 (a), the score distribution of images from different datasets is generally uniform. Meanwhile, from Fig. 3 (b), we can see that the details and overall score are independent of each other, showing the correctness of the scoring scheme and the necessity of each score. Finally, Figs. 3 (c) and (d) show the ratio of images with different mask sizes, where it can be seen that the outpainting has a more uniform distribution than warping based hole, which rely on depth map and may not have uniform distribution. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Evaluation Metrics. We adopted seven metrics to measure the quality of inpainted images, i.e., the predicted reward value by our trained reward model , T2I reward [54], CLIP [70], BLIP [71], Aesthetic (Aes.) [72], Classification Accuracy (CA) [73], and Inception Score (IS) [74]. Specifically, T2I reward, CLIP, BLIP, and Aes. directly measure the consistency between the semantics of inpainted images and the language summary of corresponding prompt images. While, CA and IS indicate the quality of the generative model. Due to the fact that our inpainting reward directly measures the reconstruction quality, we adopted it as the principle measurement of our experiment. ", "page_idx": 6}, {"type": "text", "text": "Implementation Details. We partitioned the dataset in Sec. 4 into training, validation and testing sets, containing 12,000, 3,000 and 2,000 prompts (with 36,000, 9,000 and 6,000 images), respectively. The reinforcement fine-tuning dataset contains the prompts from the original reward training dataset. We employed the pre-trained CLIP (ViT-B) checkpoint as the backbone of our reward model $\\mathcal{R}(\\cdot)$ with the final MLP channel equal to 256. We utilized a cosine schedule to adjust the learning rate. Notably, we achieved optimal preference accuracy by fixing $70\\%$ of the layers with a learning rate of $1e\\mathrm{~-~}5$ and a batch size of 5. We trained the reward model with four ", "page_idx": 6}, {"type": "text", "text": "Table 3: Left: Ablation studies on amplification factors, where \"static\" refers to employing a constant factor to replace $\\gamma$ in Eq. (12). The column \"Factor\" indicates an average magnitude of amplification strength, i.e., $\\mathbb{E}_{\\mathbf{z}\\sim\\mathbf{p}(\\mathbf{z})}(\\gamma)$ . For our method, we coordinate the value of $k$ in Eq. (11) to change the $\\mathbb{E}(\\gamma)$ shown in the Table. \"Acl.\" signifies acceleration, calculated by $\\textstyle{\\frac{T_{b}}{T_{m}}}-1$ with $T_{b}$ and $T_{m}$ being the convergence iterations of baseline and compared methods, respectively. For all metrics, the larger, the better. Right: Performance of the reward model trained with two manners based on pre-trained CLIP [70] with various fix rates (FRs). \u201cAcc\u201d and \u201cVar\u201d stand for the accuracy and variance of the reward estimation, respectively. \u201cBd.\u201d is the ratio of data below the same upper boundary. (The underlined settings are selected.) ", "page_idx": 7}, {"type": "table", "img_path": "fVRCsK4EoM/tmp/9cebc947daf2980b803234716cbf7f1bac5b9a2fd796ad80212635efa99e09eb.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "NVIDIA GeForce RTX 3090 GPUs, each equipped with 20GB of memory. With the trained reward model, we subsequently fine-tuned the latest diffusion-based image inpainting model, namely Runway [62], on four 40GB NVIDIA GeForce RTX A6000 GPUs as our PrefPaint. During fine-tuning, we employed half-precision computations with a learning rate of $2e-6$ , and a batch size of 16. ", "page_idx": 7}, {"type": "table", "img_path": "fVRCsK4EoM/tmp/2ecce8498e312e9b42cfad57e4f65edfd095aa27644b9e0dda5631376bfeffdf.jpg", "table_caption": ["Table 4: Performance under various parameterizations of amplification factor $\\gamma$ , where $f=\\|z\\|_{\\mathbf{V}^{-1}}$ . (The underlined settings are selected.) "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.1 Results of Image Inpainting ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We compared our PrefPaint with SOTA methods both quantitatively and qualitatively to demonstrate the advantage of our method. We compared the ", "page_idx": 7}, {"type": "text", "text": "WinRate and Reward under various sampling steps in Table 1 and Fig. 4, where it can be seen that our method actually greatly improves the probability of high-quality human-preferred samples, since with a single inference step, our method achieves more than a $70\\%$ WinRate, which is similar to the baseline model (Runway) with $S=3$ . The higher reward value and lower variance also indicate that our method can consistently generate high-quality samples. Comparisons with other metrics in Table 2 further support the superiority of the proposed method. In addition, we visually compared the inpainted images by different methods in Fig. 5, where it can be seen that our method can generate more meaningful and reasonable content, which is consistent with the style of the prompt region. We refer readers to the Supplementary Material for more details about more comparison results. ", "page_idx": 7}, {"type": "text", "text": "Finally, we have also carried out a user study to evaluate our superiority. We have randomly selected about 130 groups of results and conducted a user study involving 10 users. The WinRate map, as depicted in Fig. 11, demonstrates that our reward scoring is highly accurate and capable of assessing the results of inpainting under criteria based on human preference. We also present some examples scored by our reward model in Fig. 8 to illustrate the scoring criteria system. ", "page_idx": 7}, {"type": "text", "text": "5.2 Ablation Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Reward Trustiness-Aware Scheme. We validated the advantages of our reward trustiness-aware training scheme. As shown on the left side of Table $3\\ {\\bf b}$ ) and e), we can see that this scheme improves the reward by $1.7\\%$ and accelerates the algorithm efficiency of $106.40\\%$ . Although a constant amplification, such as c) and d) in Table 3, can expedite the training process even faster than the proposed method, it compromises model accuracy, as evidenced by the reduced WinRate and Reward. This demonstrates the necessity and efficacy of adaptively managing the sampled trajectory and underscores the superiority of the proposed method. ", "page_idx": 7}, {"type": "text", "text": "Training Manner of Reward Model. We investigated two types of strategies to train the reward model $\\mathcal{R}(\\cdot)$ i.e., classification-driven and regression-driven. Specifically, the former classifies the discrete scores of annotated reward samples, while the latter directly makes a regression on the scores. As listed on the right side of Table 3, it can be seen that the regression-driven training generally outperforms the classification-driven one on reward accuracy. Moreover, the larger variances from the regression-driven training show the strong discriminative ability. Based on accuracy and boundary performance, we finally selected a fixed rate of 0.7 and MLP-256 with the regression-driven training manner as the configuration of our reward model. ", "page_idx": 7}, {"type": "image", "img_path": "fVRCsK4EoM/tmp/366eb751cf308259d9e0c0f9b90d8ce65922e80f066f1347226215549bac4ef3.jpg", "img_caption": ["Figure 6: Results of image FOV enlargement by our method on two scenes (a) and (b), where the prompt region (the given image) is delineated by the central area between white dashed lines. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "fVRCsK4EoM/tmp/601836969196a4fcec54f59396326dc1765e4160aa06615d0326d9c0b1dd8898.jpg", "img_caption": ["Figure 7: Novel view synthesis on KITTI dataset of 6 scenes from (a) to (f). For each scene, we give the \"Prompt\", which is warped from the \"Given View\", with the white regions referring to holes/missing regions. \"Result\" is our in-painting result from \"Prompt\". Note that for the synthesized novel view, there is no ground-truth available. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Parameterize amplification factor $\\gamma.$ . To parameterize the amplification factor in Eq. (10), we investigated various functions to parameterize as shown in Table 4. The experimental results indicate that the exponential function provides the best regularization effect. In contrast, the linear function and static constant do not fully exploit the regularization effect of the reward upper boundary. ", "page_idx": 8}, {"type": "text", "text": "5.3 Further analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Reward errors distribution. We make statistics of reward estimation errors, and the results are shown in Fig. 9. Although the proportion of very large error samples is not large, the incremental performance of our method lies in a more suitable choice of amplification function, as evidenced by Table 4. ", "page_idx": 8}, {"type": "text", "text": "Necessity of Our dataset. Although existing metrics such as BERT Score [77] provide a general measure of quality, we emphasize that our dataset is specifically tailored for the task of image inpainting, where human-labeled scores are both essential and more precise. To substantiate this claim, we conduct a comparison between the BERT score and our dataset\u2019s score, as illustrated in Fig. 10. The results reveal a significant divergence between human judgments and BERT\u2019s preferences, underscoring the necessity and superior accuracy of the proposed dataset for this specific task. ", "page_idx": 8}, {"type": "image", "img_path": "fVRCsK4EoM/tmp/50a8f791fcbd4eeb0358b86283f802a180adaafe83b34ab048763daa6caa9977.jpg", "img_caption": [], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Figure 8: Visualization of various image in-painting results and associated rewards from our model.   \nOur model effectively evaluates in-painting reconstructions based on human preference. ", "page_idx": 9}, {"type": "image", "img_path": "fVRCsK4EoM/tmp/3b144af69e7249d0d4fc19d4854eaa8fb550909c1192fd19b997fed22d7f8edf.jpg", "img_caption": ["Figure 9: Reward error distri-Figure 10: Plot of GT reward by ourFigure 11: WinRate heat-map butions of the proposed rewarddataset (x-axis) and the Bert Scoreof user study. The winrate at model. The distribution of re-(y-axis) on the validation set, wherespecific locations shows the raward error percentages is de-each point indicates a sample. Thetio of superior top methods to picted on the y-axis to the right.darker indacated the larger error. those at the bottom. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "5.4 Applications of Image FOV Enlargement and Novel View Synthesis ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We also applied our approach to two additional tasks: (1) image field of view (FOV) enlargement, where we iteratively extended the boundaries of a typical image strip in the horizontal direction to create a wider FOV image; and (2) novel view synthesis, where we warped a given image using the predicted depth image through the method in [75] to generate a novel viewpoint and subsequently applied diffusion models to fill the holes/missing regions of the Warped view. As depicted in Fig. 6 (a)-a oil painting and (b)-a realistic photography, our method yields natural and visually pleasing results that can be seamlessly integrated with the prompt regions. As illustrated in Figs. 7, our method can generate more reasonable novel views and a visually appealing reconstruction. We refer readers to the Supplementary Material for more details about more application visual demonstrations. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion and Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We have presented PrefPaint, an innovative scheme that leverages the principles of the linear bandit theorem to align diffusion models for image inpainting with human preferences iteratively. By integrating human feedback directly into the training loop, we have established a dynamic framework that not only respects the subjective nature of visual aesthetics but also continuously adapts to the evolving preferences of users. We conducted extensive experiments to demonstrate the necessity for alignment in the task of image inpainting and the significant superiority of PrefPaint both quantitatively and qualitatively. We believe that our method, along with the newly constructed dataset, has the potential to bring significant benefits to the development of visually driven AI applications. ", "page_idx": 9}, {"type": "text", "text": "PrefPaint aligns with a distribution that corresponds to the preferences of a specific group. However, it is important to recognize that individual preferences for image styles vary. Therefore, after achieving alignment with the general preferences of a group, it is advisable to develop personalized rewards and a corresponding reinforced alignment model to ensure complete alignment with the preferences of each user. Recently, the implementation of a reward-free alignment process, as discussed in [16], has gained popularity. Consequently, exploring reward model-free training methods for alignment in diffusion-based models represents a promising avenue for future research. Furthermore, there is an opportunity to explore the potential applications of our in-painting algorithm for additional 3D reconstruction tasks. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported in part by the National Natural Science Foundation of China Excellent Young Scientists Fund 62422118, and in part by the Hong Kong Research Grants Council under Grant 11219324 and 11219422, and in part by the Hong Kong University Grants Council under Grant UGC/FDS11/E02/22. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Zili Yi, Qiang Tang, Shekoofeh Azizi, Daesik Jang, and Zhan Xu. Contextual residual aggregation for ultra high-resolution image inpainting. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 7508\u20137517, 2020.   \n[2] Jingyuan Li, Ning Wang, Lefei Zhang, Bo Du, and Dacheng Tao. Recurrent feature reasoning for image inpainting. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 7760\u20137768, 2020.   \n[3] Titas Anciukevic\u02c7ius, Zexiang Xu, Matthew Fisher, Paul Henderson, Hakan Bilen, Niloy J Mitra, and Paul Guerrero. Renderdiffusion: Image diffusion for 3d reconstruction, inpainting and generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12608\u201312618, 2023. [4] David Svitov, Dmitrii Gudkov, Renat Bashirov, and Victor Lempitsky. Dinar: Diffusion inpainting of neural textures for one-shot human avatars. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7062\u20137072, 2023.   \n[5] Efsun Karaca and M Alper Tunga. Interpolation-based image inpainting in color images using high dimensional model representation. In 2016 24th European Signal Processing Conference (EUSIPCO), pages 2425\u20132429. IEEE, 2016.   \n[6] Pablo Arias, Vicent Caselles, Gabriele Facciolo, Vanel Lazcano, and Rida Sadek. Nonlocal variational models for inpainting and interpolation. Mathematical Models and Methods in Applied Sciences, 22(supp02):1230003, 2012.   \n[7] Mashail Alsalamah and Saad Amin. Medical image inpainting with rbf interpolation technique. International Journal of Advanced Computer Science and Applications, 7(8), 2016.   \n[8] Xiefan Guo, Hongyu Yang, and Di Huang. Image inpainting via conditional texture and structure dual generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14134\u201314143, 2021.   \n[9] Jitesh Jain, Yuqian Zhou, Ning Yu, and Humphrey Shi. Keys to better image inpainting: Structure and texture go hand in hand. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 208\u2013217, 2023.   \n[10] Avisek Lahiri, Arnav Kumar Jain, Sanskar Agrawal, Pabitra Mitra, and Prabir Kumar Biswas. Prior guided gan based semantic inpainting. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 13696\u201313705, 2020.   \n[11] Xian Zhang, Xin Wang, Canghong Shi, Zhe Yan, Xiaojie Li, Bin Kong, Siwei Lyu, Bin Zhu, Jiancheng Lv, Youbing Yin, et al. De-gan: Domain embedded gan for high quality face image inpainting. Pattern Recognition, 124:108415, 2022.   \n[12] Shiyuan Yang, Xiaodong Chen, and Jing Liao. Uni-paint: A unified framework for multimodal image inpainting with pretrained diffusion model. In Proceedings of the 31st ACM International Conference on Multimedia, pages 3190\u20133199, 2023.   \n[13] Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score: Better aligning text-to-image models with human preference. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2096\u20132105, 2023.   \n[14] Prem Nandhini Satgunam, Russell L Woods, P Matthew Bronstad, and Eli Peli. Factors affecting enhanced video quality preferences. IEEE transactions on image processing, 22(12):5146\u20135157, 2013.   \n[15] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024.   \n[16] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36, 2024.   \n[17] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008\u20133021, 2020.   \n[18] Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, J\u00e9r\u00e9my Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, et al. Open problems and fundamental limitations of reinforcement learning from human feedback. arXiv preprint arXiv:2307.15217, 2023.   \n[19] R\u00e9mi Munos, Michal Valko, Daniele Calandriello, Mohammad Gheshlaghi Azar, Mark Rowland, Zhaohan Daniel Guo, Yunhao Tang, Matthieu Geist, Thomas Mesnard, Andrea Michi, et al. Nash learning from human feedback. arXiv preprint arXiv:2312.00886, 2023.   \n[20] Leslie Pack Kaelbling, Michael L Littman, and Andrew W Moore. Reinforcement learning: A survey. Journal of artificial intelligence research, 4:237\u2013285, 1996.   \n[21] Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, and Anil Anthony Bharath. Deep reinforcement learning: A brief survey. IEEE Signal Processing Magazine, 34(6):26\u201338, 2017.   \n[22] Vincent Fran\u00e7ois-Lavet, Peter Henderson, Riashat Islam, Marc G Bellemare, Joelle Pineau, et al. An introduction to deep reinforcement learning. Foundations and Trends\u00ae in Machine Learning, 11(3-4):219\u2013354, 2018.   \n[23] Eugene A Feinberg and Adam Shwartz. Handbook of Markov decision processes: methods and applications, volume 40. Springer Science & Business Media, 2012.   \n[24] Martijn Van Otterlo and Marco Wiering. Reinforcement learning and markov decision processes. In Reinforcement learning: State-of-the-art, pages 3\u201342. Springer, 2012.   \n[25] Ather Gattami, Qinbo Bai, and Vaneet Aggarwal. Reinforcement learning for constrained markov decision processes. In International Conference on Artificial Intelligence and Statistics, pages 2656\u20132664. PMLR, 2021.   \n[26] Hao-nan Wang, Ning Liu, Yi-yun Zhang, Da-wei Feng, Feng Huang, Dong-sheng Li, and Yi-ming Zhang. Deep reinforcement learning: a survey. Frontiers of Information Technology & Electronic Engineering, 21(12):1726\u20131744, 2020.   \n[27] Hao Yi Ong, Kevin Chavez, and Augustus Hong. Distributed deep q-learning. arXiv preprint arXiv:1508.04186, 2015.   \n[28] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.   \n[29] Songtao Lu, Kaiqing Zhang, Tianyi Chen, Tamer Bas\u00b8ar, and Lior Horesh. Decentralized policy gradient descent ascent for safe multi-agent reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 8767\u20138775, 2021.   \n[30] Shauharda Khadka and Kagan Tumer. Evolution-guided policy gradient in reinforcement learning. Advances in Neural Information Processing Systems, 31, 2018.   \n[31] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.   \n[32] Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron Courville, and Yoshua Bengio. An actor-critic algorithm for sequence prediction. arXiv preprint arXiv:1607.07086, 2016.   \n[33] Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-critic methods. In International conference on machine learning, pages 1587\u20131596. PMLR, 2018.   \n[34] Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms and applications. arXiv preprint arXiv:1812.05905, 2018.   \n[35] Enkelejda Kasneci, Kathrin Se\u00dfler, Stefan K\u00fcchemann, Maria Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan G\u00fcnnemann, Eyke H\u00fcllermeier, et al. Chatgpt for good? on opportunities and challenges of large language models for education. Learning and individual differences, 103:102274, 2023.   \n[36] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. Gpt understands, too. AI Open, 2023.   \n[37] Lars Ruthotto and Eldad Haber. An introduction to deep generative modeling. GAMMMitteilungen, 44(2):e202100008, 2021.   \n[38] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \n[39] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 2256\u20132265. PMLR, 2015.   \n[40] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020.   \n[41] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In ACM SIGGRAPH 2022 Conference Proceedings, pages 1\u201310, 2022.   \n[42] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022.   \n[43] Jinhui Hou, Zhiyu Zhu, Junhui Hou, Hui Liu, Huanqiang Zeng, and Hui Yuan. Global structureaware diffusion process for low-light image enhancement. Advances in Neural Information Processing Systems, 36, 2024.   \n[44] Meng You, Zhiyu Zhu, Hui Liu, and Junhui Hou. Nvs-solver: Video diffusion model as zero-shot novel view synthesizer. arXiv preprint arXiv:2405.15364, 2024.   \n[45] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. 2023.   \n[46] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020.   \n[47] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11461\u201311471, 2022.   \n[48] Ciprian Corneanu, Raghudeep Gadde, and Aleix M Martinez. Latentpaint: Image inpainting in latent space with diffusion models. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 4334\u20134343, 2024.   \n[49] Micha\u0142 Stypu\u0142kowski, Konstantinos Vougioukas, Sen He, Maciej Zi\u02dbeba, Stavros Petridis, and Maja Pantic. Diffused heads: Diffusion models beat gans on talking-face generation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 5091\u20135100, 2024.   \n[50] Gustav M\u00fcller-Franzes, Jan Moritz Niehues, Firas Khader, Soroosh Tayebi Arasteh, Christoph Haarburger, Christiane Kuhl, Tianci Wang, Tianyu Han, Sven Nebelung, Jakob Nikolas Kather, et al. Diffusion probabilistic models beat gans on medical images. arXiv preprint arXiv:2212.07501, 2022.   \n[51] Sham M Kakade. A natural policy gradient. Advances in neural information processing systems, 14, 2001.   \n[52] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International conference on machine learning, pages 1889\u20131897. PMLR, 2015.   \n[53] Yinan Zhang, Eric Tzeng, Yilun Du, and Dmitry Kislyuk. Large-scale reinforcement learning for diffusion models. arXiv preprint arXiv:2401.12244, 2024.   \n[54] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36, 2024.   \n[55] Yasin Abbasi-Yadkori, D\u00e1vid P\u00e1l, and Csaba Szepesv\u00e1ri. Improved algorithms for linear stochastic bandits. Advances in neural information processing systems, 24, 2011.   \n[56] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 633\u2013641, 2017.   \n[57] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic understanding of scenes through the ade20k dataset. International Journal of Computer Vision, 127:302\u2013321, 2019.   \n[58] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A largescale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.   \n[59] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In Conference on Computer Vision and Pattern Recognition (CVPR), 2012.   \n[60] Eirikur Agustsson and Radu Timofte. Ntire 2017 challenge on single image super-resolution: Dataset and study. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, July 2017.   \n[61] Radu Timofte, Shuhang Gu, Jiqing Wu, Luc Van Gool, Lei Zhang, Ming-Hsuan Yang, Muhammad Haris, et al. Ntire 2018 challenge on single image super-resolution: Methods and results. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, June 2018.   \n[62] Runway. Runway model on huggingface, 2023. URL https://huggingface.co/runwayml/ stable-diffusion-inpainting.   \n[63] Runway. Runway model on huggingface, 2023. URL https://huggingface.co/runwayml/ stable-diffusion-v1-5.   \n[64] StabilityAI. Stabilityai model on huggingface, 2023. URL https://huggingface.co/ stabilityai/stable-diffusion-2-1.   \n[65] StabilityAI. Stabilityai model on huggingface, 2023. URL https://huggingface.co/ stabilityai/stable-diffusion-xl-base-1.0.   \n[66] Diffuser. Stabilityai model on huggingface, 2023. URL https://huggingface.co/ diffusers/stable-diffusion-xl-1.0-inpainting-0.1.   \n[67] Compvis. Compvis github website, 2023. URL https://github.com/CompVis/ latent-diffusion.   \n[68] Anton Razzhigaev, Arseniy Shakhmatov, Anastasia Maltseva, Vladimir Arkhipkin, Igor Pavlov, Ilya Ryabov, Angelina Kuts, Alexander Panchenko, Andrey Kuznetsov, and Denis Dimitrov. Kandinsky: an improved text-to-image synthesis with image prior and latent diffusion. arXiv preprint arXiv:2310.03502, 2023.   \n[69] Wenbo Li, Zhe Lin, Kun Zhou, Lu Qi, Yi Wang, and Jiaya Jia. Mat: Mask-aware transformer for large hole image inpainting. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10758\u201310768, 2022.   \n[70] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[71] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping languageimage pre-training for unified vision-language understanding and generation. In International conference on machine learning, pages 12888\u201312900. PMLR, 2022.   \n[72] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:25278\u201325294, 2022.   \n[73] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2818\u20132826, 2016.   \n[74] Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. Pixelcnn $^{++}$ : Improving the pixelcnn with discretized logistic mixture likelihood and other modifications. arXiv preprint arXiv:1701.05517, 2017.   \n[75] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. arXiv preprint arXiv:2401.10891, 2024.   \n[76] Jianfeng Xiang, Jiaolong Yang, Binbin Huang, and Xin Tong. 3d-aware image generation using 2d diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2383\u20132393, 2023.   \n[77] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023.   \n[78] Junhao Zhuang, Yanhong Zeng, Wenran Liu, Chun Yuan, and Kai Chen. A task is worth one word: Learning with task prompts for high-quality versatile image inpainting. arXiv preprint arXiv:2312.03594, 2023.   \n[79] Xuan Ju, Xian Liu, Xintao Wang, Yuxuan Bian, Ying Shan, and Qiang Xu. Brushnet: A plug-and-play image inpainting model with decomposed dual-branch diffusion. arXiv preprint arXiv:2403.06976, 2024.   \n[80] Hayk Manukyan, Andranik Sargsyan, Barsegh Atanyan, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Hd-painter: high-resolution and prompt-faithful text-guided image inpainting with diffusion models. arXiv preprint arXiv:2312.14091, 2023.   \n[81] Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Reinforcement learning for fine-tuning text-to-image diffusion models. Advances in Neural Information Processing Systems, 36, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "[82] Kai Yang, Jian Tao, Jiafei Lyu, Chunjiang Ge, Jiaxin Chen, Weihan Shen, Xiaolong Zhu, and Xiu Li. Using human feedback to fine-tune diffusion models without any reward model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8941\u20138951, 2024. ", "page_idx": 15}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Dataset Details 18   \nA.1 Labeling Scheme 18   \nA.2 Labeling Platform 19   \nA.3 License 21 ", "page_idx": 16}, {"type": "text", "text": "B More Application Visual Demonstrations 22 ", "page_idx": 16}, {"type": "text", "text": "B.1 Application of Image FOV Enlargement . . . 22   \nB.2 Application of Novel View Synthesis 22 ", "page_idx": 16}, {"type": "text", "text": "C Reward Scoring Statistics & Reward Normalization Configuration 23 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "C.1 Score Distribution across Different Categories 23   \nC.2 Normalization Factors . 24 ", "page_idx": 16}, {"type": "text", "text": "D More Visualizations 24 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "D.1 More Visualizations of Comparisons 24   \nD.2 More Visualizations of Multiple Sampling 27 ", "page_idx": 16}, {"type": "text", "text": "E More experimental results 27 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "E.1 Comparisons with Additional inpainting Methods 27   \nE.2 Comparisons with other reinforcement learning methods. . . 28   \nE.3 Compared with SD xl $^{++}$ . . 28 ", "page_idx": 16}, {"type": "text", "text": "F Details & Reward 29 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "F.1 Training Curve . 29   \nF.2 Details between Eq. 4-5 30 ", "page_idx": 16}, {"type": "text", "text": "A Dataset Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we visualize our labeling platform and the corresponding labeling scheme. ", "page_idx": 17}, {"type": "text", "text": "A.1 Labeling Scheme ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The reward score contains the following 3 terms: ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Structural Rationality. It measures the correctness of the model understanding of image content. Score (0-2): Failure generation, e.g. strange and weird objects; random patterns completed; inappropriate strings; abrupt bar completions. Objects are generated but obviously do not belong to the scene, e.g. There are shelves unique to homes on the side of the highway; there is a marble floor blocking the front of the washing machine. Score (3-5):The understanding of the scene is roughly correct, e.g. the inpainted sky next to the sky pattern; the sea behind the beach. Score (6-7): The overall structure of the generated scene is reasonable and do not seem to be any major problems. To illustrate the scoring scheme, we show some examples and corresponding reasons in Table A-1. ", "page_idx": 17}, {"type": "text", "text": "Table A-1: Illustration of different scoring examples on structural rationality. ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Prompt with Reconstruction Prompt with Reconstruction ", "page_idx": 17}, {"type": "image", "img_path": "fVRCsK4EoM/tmp/3eeba4a40f6be9070800c2fcaa464e96a18c36f13b7656281401e1cf30cc7cf7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Score: 0. Reason: there should be no marble in front of the washing machine. Wrong semantics and wrong understanding of the scene. ", "page_idx": 17}, {"type": "image", "img_path": "fVRCsK4EoM/tmp/7bc8ab094ef5d9ec512ac0b4ce78380246fe5b1b5448833bfcb2db60fb08a019.jpg", "img_caption": ["Score: 1. Reason: the appearance of racks on the highway is unreasonable, semantically incorrect, and misunderstanding of the scene. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "fVRCsK4EoM/tmp/c73ad081049677aa8a783af34deefee7fd877e7bd5e3bea57c04686b4ecbb82d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Score: 3. Reason: the model understands that this is a volleyball match and try to complete relevant people. ", "page_idx": 17}, {"type": "image", "img_path": "fVRCsK4EoM/tmp/a70c599b693d961b198c04f2f755d6848c650e9a3c929b3ec4951c1196406c79.jpg", "img_caption": ["Score: 4. Reason: Although the quality of the details generated here is not rated high, the model understands that the scene is on the beach, and the overall structure is rated. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "fVRCsK4EoM/tmp/2a47cffd32b1a509a87bb8810c458021da57ca13adf8d7444ef26fc88b58c733.jpg", "img_caption": ["Score: 6. Reason: although there are some flaws, I understand the content of the highway scene and complete the relevant content. ", "Feeling of Local Texture. It measures whether the texture of the object is correct and consistent with objective facts. Score (0-2): Failure generation (e.g. strange and weird objects; random patterns "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "fVRCsK4EoM/tmp/27a69fa4f62776567c107c88d0b4eb00d5531407a93fbfd3108d673d45b48739.jpg", "img_caption": ["Score: 7. Reason: Understand the scenario and correctly add relevant content. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "completed; inappropriate strings; abrupt bar completions). The generated objects do not conform to objective facts (e.g. a dog has two heads; a person without an upper body). The texture of the generated objects is very poor (e.g. two car bodies stuck together; poor quality sky). Score (3-5): The generated objects are generally reasonable (e.g. a complete leopard, but with very long legs; a complete corn, but the corn kernels are strange; there are flaws in the complete object;). The generated texture is not obtrusive but will have flaws if you zoom in. Score (6-7): Details are complete and reasonable. We also show some examples and corresponding reason in Table A-2. ", "page_idx": 18}, {"type": "text", "text": "Table A-2: Illustration of different scoring examples on the feeling of local texture. ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Prompt with Reconstruction ", "page_idx": 18}, {"type": "image", "img_path": "fVRCsK4EoM/tmp/ecc47b8f6caf5442b2e45e77a0cbd2d0786007197bf66476314baea87df4a3e5.jpg", "img_caption": ["Score: 0. Reason: it can be seen that the model understands the scene correctly and wants to add a dog, but the quality is not high and does not conform to the objective facts. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "fVRCsK4EoM/tmp/1ed34c2ed5977fb895e3f94ca918d912b7fc62e9d60aafa1383cc0fda6520c33.jpg", "img_caption": ["Score: 4. Reason: The completion is generally reasonable. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "fVRCsK4EoM/tmp/02eb47636c593cd92baab100c0962b3ba0c0abd8a0e9ec80fea1654669ba9a95.jpg", "img_caption": ["Score: 6. Reason: it wants to patch up the leopard but the legs is not consistent with common sense, but it can also be seen that it is a complete leopard. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Prompt with Reconstruction ", "text_level": 1, "page_idx": 18}, {"type": "image", "img_path": "fVRCsK4EoM/tmp/414a75b3a9996f9b05ad04d697da52a888e7d985113ce17dfd7348066614d02b.jpg", "img_caption": ["Score: 1. Reason: it wants to generate people but the generation quality is not high. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "fVRCsK4EoM/tmp/0ae8132b2895b8023821a14e08a756f48c8f4941597d1df45477249da96b5e0a.jpg", "img_caption": ["Score: 4. Reason: The completion is generally reasonable. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "fVRCsK4EoM/tmp/ba7f195928f8ea2da628e3acfa513e3a18e50905b7a59a96949c0fe84305b7fa.jpg", "img_caption": ["Score: 7. Reason: The completion quality is good. good. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Overall Feeling. The overall feeling given by the picture, whether it is reasonable and consistent with the objective facts. We directly show some examples and corresponding reasons in Table A-3. ", "page_idx": 18}, {"type": "text", "text": "A.2 Labeling Platform ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The visual demonstration of labeling platform is shown in Fig. A-1. In the right-side, we aggregate three reconstruction results with the prompt images. The labeling is annotated in the left-side, where for each image, people need to label 3 scores by aforementioned 3 different criteria, ranging from 0 to 7. ", "page_idx": 18}, {"type": "image", "img_path": "fVRCsK4EoM/tmp/d0f20fa7ec6d79082f0fedf1291d563ec7ee3ec64741b4cf69a0b3bc76360e4f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Score: 3, 2, 6. Reason: The quality of Recon-3 is significantly better than Recon-1, and there are no obvious flaws, so it has the highest score of 6. ", "page_idx": 19}, {"type": "image", "img_path": "fVRCsK4EoM/tmp/28d117fc218b62ef566df699d3d201f2b4e3427a6bfff61e10ee580f5034a959.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Score: 5, 3, 1. Reason: Recon-1 is the most reasonable, but the steel basin structure is a bit strange. Recon-2 may want to make up for the melon seeds. The details are quite good but the scenes are relatively rare. Recon-3 is very strange. ", "page_idx": 19}, {"type": "image", "img_path": "fVRCsK4EoM/tmp/85cd26e979a05677f7114d6cde88e441ccb6507a2523d3f34244d3068e4e09d7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Score: 1, 1, 4. Reason: The overall structure of Recon-1 is wrong, and the quality of the corn heads in Recon-2 is relatively poor, and it looks like there is glue stuck on it. Recon-3 has a reasonable structure, but the proportion of the head is relatively large, and the grains of the corn head are strange, so the score of only 4 is not very high. ", "page_idx": 19}, {"type": "image", "img_path": "fVRCsK4EoM/tmp/b6c5fa5fc5d13fcc9ee4a2db0a307326748bc03dde0d86f9cd6df8a9539c17de.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Score: 1, 7, 2. Reason: The shape of the car on the right side of Recon-1 is too weird and affects the look and feel. Recon-2 is of very good quality and very natural. The house on the right side of Recon-3 has a poorer perspective. ", "page_idx": 19}, {"type": "image", "img_path": "fVRCsK4EoM/tmp/5c96b0b0af24e6adf65a0cf26f59bf664f6229750c2b31b45f19e93d63bf6fe8.jpg", "img_caption": ["Score: 0, 4, 1. Reason: Recon-1 was randomly generated, Recon-3 tried to generate a traffic light, but failed. Similar branches in Recon-2 don\u2019t look so inconsistent. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "fVRCsK4EoM/tmp/2e1bde462ec1b0d60146ae27a743d82b483d6812321bbea24c7493026b1e6162.jpg", "img_caption": ["Figure A-1: Labeling platform demonstration. In each group, from top left to bottom right, are reconstruction 1, 2, 3 and the prompt image. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "A.3 License ", "page_idx": 20}, {"type": "text", "text": "The license of the proposed dataset is CC BY-NC 4.0, which allows creators to share their work with others while retaining certain rights but gives a restriction on commercial use. ", "page_idx": 20}, {"type": "text", "text": "B More Application Visual Demonstrations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "To further verify the effectiveness of the proposed method, this section further visualizes more scenes for the application of the proposed method (as demonstrated in Sec.5.4). ", "page_idx": 21}, {"type": "text", "text": "B.1 Application of Image FOV Enlargement ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "For the task of Image FOV Enlargement, the results are shown in Fig. B-2. We randomly select some picture from Internet and apply center crop on those pictures. We can see that no matter the style of prompt image, e.g., nature photography, oil painting or Chinese painting, the proposed method can consistently generate meaningful and visually pleasing results, which demonstrates the superority of the proposed method. ", "page_idx": 21}, {"type": "image", "img_path": "fVRCsK4EoM/tmp/e79d6285db232805344831a92f368727651ad107d97cac6529b1105b699f74ba.jpg", "img_caption": ["Figure B-2: Application of FOV enlargement, where we visualize 9 scenes from (a) to (i) with corresponding prompt cropped image and enlarged result. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "B.2 Application of Novel View Synthesis ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We also visualize more novel view synthesis examples on the KITTI and DIV2K datasets in Figs. B-3 and B-4. The warping-induced inpainting hole is much more irregular than the FOV Enlargement, which makes the task more challenging. However, the proposed method successfully filled up the missing region. The resulting high-quality reconstructions verify the superiority of the proposed method. ", "page_idx": 21}, {"type": "image", "img_path": "fVRCsK4EoM/tmp/febf5907532b1523d1b94b7b2f8cf962093c4aa702ece00764056c5f9d0852db.jpg", "img_caption": ["Figure B-3: Application of novel view synthesis on KITTI dataset, where we visualize 8 scenes from (a) to (h) with corresponding prompt warped image, given view and reconstruction result. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "fVRCsK4EoM/tmp/383f5038d58b2f780c05c21bc011eecd8423dd81d20f813da19161340842830e.jpg", "img_caption": ["Figure B-4: Application of novel view synthesis on DIV2K dataset, where we visualize 8 scenes from (a) to (h) with corresponding prompt warped image, given view and reconstruction result. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "C Reward Scoring Statistics & Reward Normalization Configuration ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "C.1 Score Distribution across Different Categories ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We count the ratio of prompt with different semantic class in Fig. C-5. Most methods, including SOTA diffusion-based methods, e.g., stable diffusion or Runway, have a large ratio of samples with negative rewards. However, the proposed method achieves the most positive rewards, as shown in the last figure, which validates the necessity and effectiveness of applying such an alignment task to the diffusion in-painting model. ", "page_idx": 22}, {"type": "image", "img_path": "fVRCsK4EoM/tmp/90b8f05c2430e4be031588b1cc8d8b0ff7df9bd936558d726399a38d4af1143f.jpg", "img_caption": ["Figure C-5: Detailed score statistics of the proposed dataset. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "C.2 Normalization Factors ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "To facilitate the training of reward model, we normalize the score with $\\frac{s\\!-\\!m e a n}{v a r}$ , where $s$ is raw score data. To remove the data bias for training the reward model, the normalization factors are calculated within different datasets and inpainting patterns. We show the resulting factors in Table C-4. The relatively balanced factors indicate that our labels do not have large biases on different datasets or inpainting pattern. ", "page_idx": 23}, {"type": "table", "img_path": "fVRCsK4EoM/tmp/7f3df41495028a8d12946812f91fe83a6bd5ff0f47f91eaac360d61007e7a68b.jpg", "table_caption": ["Table C-4: The normalization factor of score to facilitate for reward model training. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "D More Visualizations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "D.1 More Visualizations of Comparisons ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section, we visually compare the reconstruction results of different methods. As shown in Figs. D-6 and D-7. The comparison of different methods in Figs. D-6 and D-7 indicate that the proposed method could generate high-quality reconstruction compared with other SOTA methods. Notably, the results from our approach show improved clarity and detail retention, which are essential for practical applications. ", "page_idx": 23}, {"type": "image", "img_path": "fVRCsK4EoM/tmp/018dabed70de13af325f3e322ffdeee0bbffc7daeb09719446ab3dd72d1ff72e.jpg", "img_caption": ["Figure D-6: Qualitative comparison between the proposed method and other SOTA methods. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "fVRCsK4EoM/tmp/d035242d22dcf49ec49cb80898fe1e43d4eaaea7d36d0a925f97591cad5f478b.jpg", "img_caption": ["Figure D-7: Qualitative comparison between the proposed method and other SOTA methods. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "D.2 More Visualizations of Multiple Sampling ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "To verify the robustness and consistency of the inpainting model after alignment with our method. We run 5 times with different random seeds on the same prompt to derive different reconstruction results. The results shown in Fig. D-8 demonstrate the stability of the proposed method. We can see the proposed method generate high-quality and visually pleasing results under different conditions. ", "page_idx": 26}, {"type": "image", "img_path": "fVRCsK4EoM/tmp/0743de9cff37f31eeda8d63de8a5a07ca838dd69257b8dc177b4a0b7171c38c7.jpg", "img_caption": ["Figure D-8: Qualitative comparison of different sampling times of the proposed method. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "E More experimental results ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "E.1 Comparisons with Additional inpainting Methods ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We have experimentally compared with some inpainting Methods, i.e., PowerPaint [78], BrushNet [79] and Hdpaint [80] . For all these methods, we assessed performance using their publicly released models. As shown in the Table E-1 and Table E-2, our method significantly outperforms all the compared methods, particularly in perceptual metrics such as T2I and our Reward score. Besides, BrushNet employs roughly twice the number of parameters as our model, and HdPaint requires 15 times more computational time (experimented on NVIDIA GeForce RTX A6000 GPUs), making it impractical for real-world applications. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "table", "img_path": "fVRCsK4EoM/tmp/846b1c590c2208d053e91804da52d370810ea819cf3aafdebcbadb28b358cd0f.jpg", "table_caption": ["Table E-1: Comparison across metrics: higher values are better for all metrics except \"# Param\" "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "fVRCsK4EoM/tmp/cdd82adaa31e4bbeeb32ce82243f39d20bf14e910e00d7ba53c08c0aeaab164d.jpg", "table_caption": ["Table E-2: Quantitative comparisons of different methods. \u201c $S^{\\bullet}$ is the number of sampling times. For the calculation of WinRate, we first derive the best sample of the compared method among $S$ sampling times. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "E.2 Comparisons with other reinforcement learning methods. ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "we have experimentally compared our method with some other reinforcement learning methods in Table E-3. Specifically, we simply summarize the implementation of each method. Human Preference Score [13] learns a negative prompt to map the diffusion process to low-quality samples. Then, in the inference process, the negative sample is utilized in the classifier-free guidance (CFG) to push the generation trajectory away from low-quality samples. ImageReward [54] trains a reward model and then applies the reward model as a loss metric to end-to-end optimize the diffusion model accompanied by a reconstruction loss. We also conduct the ablation study on reward training strategy in Table 3 in our paper. Our method employs a regression-driven training strategy, while ImageReward a classification-drive strategy. DPOK [81] simultaneously optimizes the whole trajectory of a reverse diffusion process and utilizes the KL divergence to panel the regularization, avoiding a large distribution shift. D3PO [82] adopts the RL strategy from direct performance optimization (DPOK), directly optimizes the model on the reward labeling data to minimize the probability of low-quality samples and increase the probability of high-quality samples. ", "page_idx": 27}, {"type": "text", "text": "E.3 Compared with SD xl ++ ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Currently, their is also a kind of diffusion-inpainting network like SD x $\\mathrm{1++\\,[66]}$ , which requires a complete image to redraw the mask region and can generate a visually pleasing result. However, their performance would be dramatically reduced without the complete image. To validate the characteristics of SD ${\\bf{x}}1+{\\bf{+}}$ , we carry out experiments to examine its performance. ", "page_idx": 27}, {"type": "table", "img_path": "fVRCsK4EoM/tmp/61e83df753a4be550d8e9978d4b9301562fbaefef1d0d93cbc59bc65e9322297.jpg", "table_caption": ["Table E-3: Quantity evaluations of different reinforcement learning methods. "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "Table E-4: Alignment evaluation across various generative methods, where $\\star$ represents the small model (non SD-based); \u201c $S^{\\bullet}$ refers to the number of sampling times. For comparing the WinRate, the best inpainting results are selected through multiple sampling and compared with the Runway Inpainting model with only 1 sampling. \u201c\u2191(resp. $\\downarrow$ )\" means the larger (resp. smaller), the better. The WinRate is calculated against Runway [62]. ", "page_idx": 28}, {"type": "table", "img_path": "fVRCsK4EoM/tmp/3c6688f844c345d9895a72d2c06028698071ec981b469932dbc322bff06e9890.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "F Details & Reward ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "F.1 Training Curve ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We present training curves of various methods for comparison. The curves for \u20191.4BaseLine\u2019 and \u20191.4Boundary\u2019 (Ours) demonstrate our modification\u2019s acceleration, reaching 0.35 rewards first. Our approach converges with the fewest training iterations. ", "page_idx": 28}, {"type": "image", "img_path": "fVRCsK4EoM/tmp/556bae27dbc0dd5dd309944d093114a9418710770d8e630c7af8dccd2870d314.jpg", "img_caption": ["Figure F-1: Training curves of different experimental setups. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "F.2 Details between Eq. 4-5 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "According to the [55], we derive a similar process for the linear reward regression on feature space, where $\\mathbf{Z}$ denotes the latent of an image. ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\psi}=(\\mathbf{Z}^{T}\\mathbf{Z}+\\lambda\\mathbf{I})^{-1}\\mathbf{Z}^{T}\\mathbf{Y}}\\\\ &{\\quad=(\\mathbf{Z}^{T}\\mathbf{Z}+\\lambda\\mathbf{I})^{-1}\\mathbf{Z}^{T}(\\mathbf{Z}\\psi_{*}+\\zeta)}\\\\ &{\\quad=(\\mathbf{Z}^{T}\\mathbf{Z}+\\lambda\\mathbf{I})^{-1}[(\\mathbf{Z}^{T}\\mathbf{Z}+\\lambda\\mathbf{I}-\\lambda\\mathbf{I})\\psi_{*}+\\mathbf{Z}^{T}\\zeta]}\\\\ &{\\quad=\\psi_{*}-\\lambda(\\mathbf{Z}^{T}\\mathbf{Z}+\\lambda\\mathbf{I})^{-1}\\psi_{*}+(\\mathbf{Z}^{T}\\mathbf{Z}+\\lambda\\mathbf{I})^{-1}\\zeta}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\boldsymbol{\\psi}}-\\boldsymbol{\\psi}_{*}=(\\mathbf{Z}^{T}\\mathbf{Z}+\\lambda\\mathbf{I})^{-1}\\mathbf{Z}^{T}\\boldsymbol{\\zeta}-\\lambda(\\mathbf{Z}^{T}\\mathbf{Z}+\\lambda\\mathbf{I})^{-1}\\boldsymbol{\\psi}_{*},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: The authors correctly summarized the contribution in the abstract and introduction sections. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 30}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: The authors have discussed the limitations in the section of the conclusion and discussion. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 30}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: No theoretical result. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 31}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We have detailedly illustrated our implementation details. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We have given the example data and code for supplementary material. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 32}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We have given the implementation details. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 32}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: N.A. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We have provided the computational resources. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 33}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: We follows the instructions for preparing the paper. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 33}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: N.A. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: N.A. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 34}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We have cited the utilized papers. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 34}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 35}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We have discussed the new assets. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 35}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We have given the instructions in the appendix. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 35}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: N.A. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 35}]