[{"type": "text", "text": "Open-Vocabulary Object Detection via Language Hierarchy ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jiaxing Huang, Jingyi Zhang, Kai Jiang, Shijian Lu\u2217 College of Computing and Data Science Nanyang Technological University, Singapore ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent studies on generalizable object detection have attracted increasing attention with additional weak supervision from large-scale datasets with image-level labels. However, weakly-supervised detection learning often suffers from image-to-box label mismatch, i.e., image-level labels do not convey precise object information. We design Language Hierarchical Self-training (LHST) that introduces language hierarchy into weakly-supervised detector training for learning more generalizable detectors. LHST expands the image-level labels with language hierarchy and enables co-regularization between the expanded labels and self-training. Specifically, the expanded labels regularize self-training by providing richer supervision and mitigating the image-to-box label mismatch, while self-training allows assessing and selecting the expanded labels according to the predicted reliability. In addition, we design language hierarchical prompt generation that introduces language hierarchy into prompt generation which helps bridge the vocabulary gaps between training and testing. Extensive experiments show that the proposed techniques achieve superior generalization performance consistently across 14 widely studied object detection datasets. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Object detection aims to locate and identify objects in images by providing basic visual information of \u201cwhere and what objects are\u201d. Thanks to the recent advances of deep neural networks, it has achieved great success with various applications in autonomous driving [1, 2, 3, 4], intelligent surveillance [5, 6, 7, 8], wildlife tracking [9, 10, 11], etc. However, learning a generalizable object detector for various downstream tasks that have different data distributions and data vocabularies remains an open research challenge. To this end, weakly-supervised object detection (WSOD) [12, 13, 14, 15], which allows access of large-scale image-level datasets (e.g., ImageNet-21K [16] with 14M images of 21K classes) with super rich data distributions and data vocabularies, has reignited new research interest under the context of learning generalizable detectors. ", "page_idx": 0}, {"type": "text", "text": "While exploiting WSOD to learn generalizable detectors, one typical challenge is that the provided image-level labels do not convey precise object information [15] and often mismatch with box-level labels. Recent methods address this challenge by designing various label-to-box assignment strategies that assign the image-level labels to the predicted top-score [13, 14] or max-size [15] object proposals. However, the mismatch problem remains due to the restriction of the raw image-level labels [17]. At the other end, self-training [18, 19, 20] with the detectors pre-trained with [13, 14, 15] can generate box-level pseudo labels without the restriction of image-level labels. It allows learning from more object proposals without the image-to-box label mismatch issue, but it does not benefit much from the provided image-level label supervision. ", "page_idx": 0}, {"type": "image", "img_path": "TNQ0hxh3O1/tmp/166d21a5872084b24dc35ba7d7affe937990867b8cf83a00c1c80df95868170a.jpg", "img_caption": ["Figure 1: Image-level labels in large-scale datasets such as ImageNet-21k [16] often do not convey precise object information [17, 15] which affects while learning generalizable detectors. Recent methods tackle this issue by various label-to-box assignment strategies [12, 13, 14, 15] as in (a) but are heavily restricted by raw image-level labels and still suffer from image-to-box label mismatch [17]. Self-training [18] with the detectors pre-trained with [13, 14, 15] could circumvent the label mismatch issue but the generated pseudo box labels are error-prone due to the lack of proper supervision as in (b). Our proposed LHST introduces language hierarchy to expand the image-level labels and enables co-regularization between the expanded labels and self-training which allows producing more accurate pseudo box labels in (c). "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "We propose to incorporate image-level supervision with self-training for learning generalizable detectors, aiming to benefit from self-training while effectively making use of image-level weak supervision. We start from a simple observation: the image-to-box label mismatch largely comes from the ambiguity in language hierarchy, e.g., the image-level label Aquatic Mammal in Figure 1 can cover different object-level labels such as seals, dolphins, walruses, etc. With the above observations, we design a Detector with Language Hierarchy (DetLH) that combines language hierarchical selftraining (LHST) and language hierarchical prompt generation (LHPG) for learning generalizable detectors. ", "page_idx": 1}, {"type": "text", "text": "LHST introduces WordNet\u2019s language hierarchy [21] to expand the image-level labels and accordingly enables co-regularization between the expanded labels and self-training. Specifically, the expanded labels are not all reliable though they can mitigate the image-to-box label mismatch problem by providing richer supervision. Here self-training can predict reliability scores for the expanded labels for better selection or weightage of the expanded labels. At the other end, self-training with pseudo box labels allows learning from more proposals and can circumvent the image-to-box label mismatch , but the box-level pseudo labels are usually noisy and may lead to learning degradation [15]. Here the expanded labels provide richer and more flexible supervision which can effectively help suppress prediction noises in self-training. ", "page_idx": 1}, {"type": "text", "text": "LHPG helps bridge the vocabulary gaps between training and testing by introducing WordNet\u2019s language hierarchy into prompt generation process. Specifically, LHPG leverages the CLIP language encoder [22] to measure the embedding distances between test concepts and WordNet synsets, and then generates the prompt for a given test concept from its best matched WordNet synset. In this way, the test prompts generated by LHPG have been standardized by WordNet and are well aligned with our proposed detector that is trained with WordNet information via LHST. In another word, the combination of LHST and LHPG actually leverages WordNet as a standard and intermediate vocabulary that bridges the gaps between training and testing vocabularies, generating better prompts and leading to better detection performance on downstream applications. ", "page_idx": 1}, {"type": "text", "text": "The main contributions of this work are threefold. First, we propose language hierarchical selftraining that incorporates language hierarchy with self-training for weakly-supervised object detection. Second, we design language hierarchical prompt generation, which introduces language hierarchy into prompt generation to bridge the vocabulary gaps between detector training and testing. Third, extensive experiments show that our DetLH achieves superior generalization performance consistently across 14 detection benchmarks. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Weakly-supervised object detection (WSOD) aims to train object detectors using image-level supervision. Traditional WSOD methods [23, 24, 25, 26, 27] use image-level annotations only without any box annotations and thus focus on low-level proposal mining techniques [28, 29, 12, 30, 31, 32], leading to unsatisfying localization performance. Semi-supervised WSOD [33, 34, 35, 36, 37, 38, 39] has been proposed to further improve the performance, which leverages both box-level and imagelevel annotated data. With better localization quality, recent methods [13, 14, 15, 40] design various label-to-box assignment strategies, such as assigning image-level labels to max-score anchors [13], max-score proposals [14] or max-size proposals [15]. Our work belongs to semi-supervised WSOD. Different from previous methods, we tackle the image-to-box label mismatch by introducing language hierarchy into self-training. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Large-vocabulary object detection [41, 13, 42, 43, 44] researches on detecting thousands of categories. Most previous papers focus on tackling the long-tail issue [45, 46, 47, 48, 49, 50], e.g., by using equalization losses [51, 52], SeeSaw loss [53], or Federated Loss [54]. Recent semi-supervised WSOD methods [13, 14, 15] and our work circumvent the long-tail problem by leveraging more balanced image-level datasets such as ImageNet-21K. ", "page_idx": 2}, {"type": "text", "text": "Open-vocabulary object detection focuses on detecting objects conditioned on arbitrary words (i.e., any category names). A common strategy [55, 56, 57, 58, 59] is to replace the detector\u2019s classification layer with the language embeddings of category names. Recent methods [60, 61, 62, 63, 17, 15] leverage the powerful CLIP [22] model by using its text embeddings [60, 61, 62, 63, 17, 15] or conducting knowledge distillation [60, 63, 17]. Similar to Detic [15], our work uses CLIP text embeddings as the classifier and leverages image-level annotated data instead of distilling knowledge from CLIP. ", "page_idx": 2}, {"type": "text", "text": "Language hierarchy has been widely studied for visual recognition tasks [64], especially for largevocabulary visual recognition. Most existing studies [65, 66, 67] focus on image classification tasks, e.g., leveraging language hierarchy for multi-label image classification [65, 66, 67, 68, 69, 70, 71], modelling hierarchical relations among classes [68, 69] or facilitating classification training [70, 71]. Different from previous work, we introduce language hierarchy into self-training for weaklysupervised object detection. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This work focuses on learning generalizable object detectors via weakly-supervised detector training [15], which leverages additional large-scale image-level datasets to enlarge the data distributions and data vocabularies in detector training. We first describe the task definition with training and evaluation setups. Then, we present our proposed DetLH which is detailed in two major aspects on Language Hierarchical Self-training (LHST) that introduces language hierarchy into detector training, and Language Hierarchical Prompt Generation (LHPG) that introduces language hierarchy into prompt generation. ", "page_idx": 2}, {"type": "text", "text": "3.1 Task Definition ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Training setup. The training data consists of two parts: 1) a detection dataset $\\mathcal{D}_{d e t}\\quad=$ $\\bar{\\{(x,y_{d e t})_{i}\\}}_{i=1}^{\\bar{|D_{d e t}^{-}}}\\bar{\\,}$ , where $x$ denotes an image while $y_{d e t}$ stands for the class and bounding box labels for x; 2) an image classification dataset Dcls = {(x, ycls )i}|iD=c1ls|where ycls denotes the imagelevel label (i.e., a one-hot vector) for $x$ . Given the two datasets, the goal is to learn a generalizable detection model $F$ by jointly optimizing $F$ over $\\mathcal{D}_{d e t}$ and $\\mathcal{D}_{c l s}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\nL o s s=\\sum_{(x,y_{d e t})\\in\\mathcal{D}_{d e t}}\\mathcal{L}_{d e t}(F(x),y_{d e t})+\\sum_{(x,y_{c l s})\\in\\mathcal{D}_{c l s}}\\mathcal{L}_{w e a k}(F(x),y_{c l s}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathcal{L}_{d e t}(\\cdot)=\\mathcal{L}_{r p n}(\\cdot)+\\mathcal{L}_{r e g}(\\cdot)+\\mathcal{L}_{c l s}(\\cdot)}\\end{array}$ is the fully-supervised detection loss function while $\\begin{array}{r}{\\mathcal{L}_{r p n}(\\cdot),\\mathcal{L}_{r e g}(\\cdot),}\\end{array}$ , and $\\mathcal{L}_{c l s}(\\cdot)$ denote RPN, Regression, and Classification loss functions, respectively. $\\mathcal{L}_{w e a k}$ is the weakly-supervised loss function to train detectors with image-level labels. ", "page_idx": 2}, {"type": "text", "text": "Evaluation setup. As the goal is to learn a generalizable detection model that works well on various unseen downstream tasks, we conduct zero-shot cross-dataset evaluation2 to assess the generalization performance of the trained detection model. Note, different domain adaptation [72, 73, 74, 75] that generally uses downstream data in training, our setup is similar to domain generalization [76, 77] that does not involve downstream data in training. ", "page_idx": 2}, {"type": "image", "img_path": "TNQ0hxh3O1/tmp/d707e6c3e8d3851a634bc0189102341a8f3e1b779b0a81b967e78094b6711cb0.jpg", "img_caption": ["Figure 2: The proposed language hierarchical self-training consists of two flows including Pseudo Label Generation (top box) and Training with Generated Labels (bottom box). The Pseudo Label Generation flow leverages WordNet to expand the image-level labels, and then merges the expanded image-level labels with the predicted pseudo box labels, such that the expanded image-level labels could provide richer and more flexible supervision (than the limited and rigid raw labels) to regularize the self-training which is prone to errors in pseudo labeling. In addition, as the labels expanded by WordNet (i.e., the expanded logits \u20181\u2019 in yihmiearge and ybhoixer ) are not all reliable, Pseudo Label Generation predicts reliability scores for the expanded labels to adaptively re-weight them when applying them on different images or pseudo boxes. In Training with Generated Labels, we optimize the detector with the generated image-level and box-level labels, where the image-level training could regularize the training with pseudo box-level labels as pseudo box labels vary along training iterations and are not very stable. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Open-vocabulary Detector. We modify the classification layer of the detector into an openvocabulary format such that the detector could be tested over unseen datasets. Specifically, we replace the weights of the detector\u2019s classification layer with the fixed language embeddings encoded from class names, where the object classification could be achieved by matching the object\u2019s embedding and the fixed language embeddings. We adopt the CLIP language embeddings [22] as the classification weights as in [15, 60]. In this way, the modified detector could theoretically detect any target concepts on any target data. As reported in [15], detectors trained solely on detection datasets often exhibits constrained performance due to the small-scale training images and vocabularies. Similar to [15], our proposed DetLH introduces large-scale image-level datasets to enlarge the data distributions and data vocabularies in detector training, leading to more generalizable detectors and better generalization performance on various unseen datasets. ", "page_idx": 3}, {"type": "text", "text": "3.2 Language Hierarchical Self-training ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The proposed LHST utilizes WordNet\u2019s language hierarchy to expand the image-level labels, which enables co-regularization between the expanded image-level labels and self-training as illustrated in Figure 2. ", "page_idx": 3}, {"type": "text", "text": "Overview. For fully supervised detector training over the detection dataset, we feed box-level annotated samples $(x,y_{d e t})\\in\\mathcal{D}_{d e t}$ to the detection model $F$ and optimize $F$ with the standard fully supervised detection loss, i.e., the first term of Eq. 1. For weakly-supervised detector training over ltahne giumaagge eh-ileervaerlc hayn ntoo teatxepda ndda ttahsee rt $(x,y_{c l s})\\in\\mathcal{D}_{c l s}$ a sbhelo $y_{c l s}$ i inn tFoi $y_{i m a g e}^{h i e r}$ (wteh ef hiriset rlaervcehriacgale  imWaogrde-Nleevt\u2019esl box-level pseudo laibmealg).e Then, we optimize the detector with and the generated pseudo box label $\\hat{y}_{b o x}$ $(\\hat{y}_{b o x}^{h i e r},w_{b o x}^{h i e r})$ to acquire $\\hat{y}_{b o x}^{h i e r}$ abnodx $(y_{i m a g e}^{h i e r},w_{i m a g e}^{h i e r})$ (the hierarchical , where wimage and $w_{b o x}^{h i e r}$ denote the predicted reliability scores of the expanded logits \u20181\u2019 in $y_{i m a g e}^{h i e r}$ and ybhoixe and are used to weight the labels in loss calculation. ", "page_idx": 3}, {"type": "text", "text": "Expanding image labels with language hierarchy. Given image-level annotated dataset $(x,y_{c l s})\\in$ $\\mathcal{D}_{c l s}$ $y_{c l s}$ is a label vector with length $C$ and $C$ denotes the number of classes), we leverage WordNet\u2019s class name hierarchy [21] to expand $y_{c l s}$ into $y_{i m a g e}^{h i e r}$ as the following: ", "page_idx": 3}, {"type": "equation", "text": "$$\ny_{i m a g e}^{h i e r}=\\mathbf{WordNet}(y_{c l s}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the function $\\mathrm{WordNet}(\\cdot)$ recursively finds all hypernyms and hyponyms of the input (i.e., the class indicated in $y_{c l s}$ ) and sets their positions in the label vector $y_{c l s}$ to be $\"1\"$ to expand $y_{c l s}$ into $y_{i m a g e}^{h i e r}$ . In this way, a single-label annotation could be expanded into a multi-label annotation within the very rich ImageNet-21K vocabulary. ", "page_idx": 4}, {"type": "text", "text": "Generating pseudo box labels with predictions. Given the image $x\\in\\mathcal{D}_{c l s}$ , we feed $x$ into the detector $F$ to acquire the prediction as following: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\{p_{n}^{c}\\}_{1\\leq n\\leq N,1\\leq c\\leq C}=F(x),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $p_{n}$ is the probability vector of the predicted $n$ -th bounding box after Softmax, and $p_{n}^{c}$ denotes the predicted $c$ -th category probability. Note we filter out a prediction if its max confidence score is lower than the threshold $t$ , and $N$ denotes the number of predicted object proposals after filtering, i.e., $m a x(\\{p_{n}^{c}\\}_{1\\leq c\\leq C})\\geq t,\\forall n$ . ", "page_idx": 4}, {"type": "text", "text": "Then the pseudo category label $\\hat{y}_{b o x}=\\{\\hat{y}_{n}\\}_{1\\leq n\\leq N}$ for $N$ boxes in image $x$ is derived by: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\arg\\operatorname*{max}_{\\hat{y}_{n}}\\sum_{c=1}^{C}\\hat{y}_{n}^{c}\\log p_{n}^{c},\\;s.t.\\;\\hat{y}_{n}\\in\\Delta^{C},\\forall n,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\hat{y}_{n}\\;=\\;(\\hat{y}_{n}^{(1)},\\hat{y}_{n}^{(2)},...,\\hat{y}_{n}^{(C)})$ is the predicted category label, and $\\Delta^{C}$ denotes a probability simplex with length $C$ . ", "page_idx": 4}, {"type": "text", "text": "Merging image and pseudo box labels. As the predicted pseudo box label $\\hat{y}_{b o x}$ is error-prone, we regularize it with the expanded image-level supervision by merging $y_{i m a g e}^{h i e r}$ and $\\hat{y}_{b o x}$ as the following: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{y}_{b o x}^{h i e r}(n)=\\hat{y}_{b o x}(n)\\vee y_{i m a g e}^{h i e r},\\forall n,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\vee$ denotes the logical \u201cOR\u201d operator. ", "page_idx": 4}, {"type": "text", "text": "Assessing the expanded labels. As the labels expanded by WordNet (i.e., the expanded logits \u20181\u2019 in $\\hat{y}_{b o x}^{h i e r}=\\bar{\\{}}\\hat{y}_{n}^{c}\\}_{1\\le n\\le N,1\\le c\\le C})$ are not all reliable, we predict a reliability score $w_{n}^{c}$ for the expanded label to adaptively re-weight $y_{n}^{c}\\in\\hat{y}_{b o x}^{h i e r}$ when applying it on different pseudo boxes. We measure the reliability of $y_{n}^{c}$ with prediction $p_{n}^{c}$ , and $w_{b o x}^{h i e r}=\\{w_{n}^{c}\\}_{1\\leq n\\leq N,1\\leq c\\leq C}$ can be derived by: ", "page_idx": 4}, {"type": "equation", "text": "$$\nw_{n}^{c}=\\left\\{p_{n}^{c}\\quad\\mathrm{if~}y_{i m a g e}^{h i e r\\quad(c)}\\neq y_{c l s}^{(c)}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "wher  yihmiearge e $y_{i m a g e}^{h i e r}{^{(c)}}\\neq y_{c l s}^{(c)}$ rehtiuerrns True if the $c$ -th label logit in $y_{i m a g e}^{h i e r}$ is expanded by WordNet, which also applies to y\u02c6bhoixer as y\u02c6box is expanded by mergeing it with yihmiearge. ", "page_idx": 4}, {"type": "text", "text": "aGnidv eitns  trheel iparbeildiitcyt isocno $\\{p_{n}^{c}\\}_{1\\le n\\le N,1\\le c\\le C}$ ,  pwsee uodptoi bmoizxe l tahbee l y\u02c6bhoixer = {y\u02c6cn}1\u2264n\u2264N,1\u2264c\u2264C $w_{b o x}^{h i e r}=\\{w_{n}^{c}\\}_{1\\leq n\\leq N,1\\leq c\\leq C}$ $F$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{b o x}(F(x))=\\sum_{n}^{N}\\sum_{c}^{C}(\\mathrm{BCE}(p_{n}^{c},y_{n}^{c})\\times w_{n}^{c}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\operatorname{BCE}(\\cdot)$ denotes the binary cross-entropy loss. ", "page_idx": 4}, {"type": "text", "text": "In addition, training with the predicted pseudo box labels is not very stable as pseudo box labels vary along training process. Thus, we regularize the training of $\\mathcal{L}_{b o x}(F(x))$ with an image-level loss defined as the following: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{i m a g e}(F(x))=\\sum_{c}^{C}(\\mathrm{BCE}(p_{i m a g e}^{c},y_{i m a g e}^{h i e r}{}^{(c)})\\times w_{i m a g e}^{c}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "pwrhoeproes $p_{i m a g e}\\,=\\,\\{p_{i m a g e}^{c}\\}_{1\\le c\\le C}$ odteesn otthees  cthatee rgeolriay bpilriotyb asbciolirtey  fporr etdhiec teexd pfaonrd tehde  liomgiatgs e\u201c-l1e\"v ienl $w_{i m a g e}^{h i e r}\\,=\\,\\{w_{i m a g e}^{c}\\}_{1\\leq c\\leq C}$ ", "page_idx": 4}, {"type": "text", "text": "$y_{i m a g e}^{h i e r}$ Similar to Eq. 6, $w_{i m a g e}^{c}=p_{i m a g e}^{c}$ e if yimage $y_{i m a g e}^{h i e r}{^{(c)}}\\neq y_{c l s}^{(c)}$ , otherwise $w_{i m a g e}^{c}=1$ . Besides, $\\operatorname{BCE}(\\cdot)$ denotes the binary cross-entropy loss. ", "page_idx": 5}, {"type": "text", "text": "Training objective. The overall training objective of Language Hierarchical Self-training is defined as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{l h s t}=\\sum_{(x,y_{d e t})\\in\\mathcal{D}_{d e t}}\\mathcal{L}_{d e t}(F(x),y_{d e t})+\\sum_{(x,y_{c l s})\\in\\mathcal{D}_{c l s}}(\\mathcal{L}_{b o x}(F(x))+\\mathcal{L}_{i m a g e}(F(x)))\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Language Hierarchical Prompt Generation. As the goal is to learn a generalizable detection model that works well on various downstream tasks, one typical challenge is the vocabulary gap between detector training datasets (i.e., LVIS and ImageNet-21K) and detector testing datasets (e.g., object365 or customized data). A common solution of tackling the vocabulary gaps is to conduct prompt learning [78] to generate proper category prompts. However, prompt learning generally requires labeled target images for additional training. ", "page_idx": 5}, {"type": "text", "text": "In this work, we tackle the vocabulary gaps by generating prompts with the help of WordNet, which introduces little computation overhead and does not require labeled target images and additional training. To this end, we design language hierarchical prompt generation (LHPG) that works by incorporating WordNet information into prompt generation process. Specifically, LHPG leverages CLIP language encoder [22] to measure the embedding distances between test concepts and WordNet synsets, and then generates the prompt for a given test concept from its best matched WordNet synset: $\\bar{V}_{t e s t}^{\\mathrm{WordNet}}=\\mathrm{CLIP}\\bar{(V_{t e s t},{\\mathrm{WordNet}}}$ )uh, aewgn,eh  ewernee $V_{t e s t}$ radatenend psottcasa nfbrduosl amfro .t e tA mdsa etcncoohtmeedps  aWrWeoodrr ddwNNiteehtt $V_{t e s t}^{\\mathrm{WordNet}}$   \n$V_{t e s t}$ $V_{t e s t}^{\\mathrm{WordNet}}$   \n$V_{t e s t}$ , our $V_{t e s t}^{\\mathrm{WordNet}}$ has been standardized by WordNet and is well alignedt ewstith our proposed detector that is trained with WordNet information via LHST. In another word, the combination of LHST and LHPG makes use of WordNet as a standard and intermediate vocabulary that bridges the gaps between training and testing vocabularies, generating better prompts and leading to better detection performance on downstream applications. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We evaluate our DetLH on 14 widely adopted detection benchmarks. We follow the zero-shot crossdataset object detection setting proposed in [17, 15]. More details like Dataset and Implementation Details are provided in the appendix. ", "page_idx": 5}, {"type": "text", "text": "Table 1: Zero-shot cross-dataset object detection for common objects. All detectors are trained over the training datasets (LVIS and ImageNet-21K) and evaluated over target datasets (i.e., Object365 and Pascal VOC with objects from common classes and scenarios) without finetuning. \u201cDatasetspecific oracles\u201d denote the detectors that are fully supervised which are trained by using the training data of respective datasets. ", "page_idx": 5}, {"type": "table", "img_path": "TNQ0hxh3O1/tmp/cecf75a170413ffadac072be817aac1082b833b99a203ffc22ff052b880bc047.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "4.1 Comparison with the state-of-the-art ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We conduct extensive experiments to benchmark our proposed DetLH with state-of-the-art methods. We evaluate them on 14 widely studied object detection datasets to assess their zero-shot cross-dataset generalization ability. Tables 1- 5 report zero-shot cross-dataset detection results for common objects, autonomous driving, intelligent surveillance, and wildlife detection, respectively. More details are to be described in the following paragraphs. ", "page_idx": 5}, {"type": "table", "img_path": "TNQ0hxh3O1/tmp/3bda3ad8c03a2283c7ebbf8f57dc68d22356c80f8e7b2e16a5411ef0fc136dd1.jpg", "table_caption": ["Table 2: Zero-shot cross-dataset object detection for autonomous driving. All detectors are trained over the training datasets (LVIS and ImageNet-21K) and evaluated over autonomous driving datasets (i.e., Cityscapes, Vistas and SODA10M) without finetuning. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "TNQ0hxh3O1/tmp/e4fdc9769bbd24d3b4b3dda3284a912d1381e2890ceadabd7365fb06398c0088.jpg", "table_caption": ["Table 3: Zero-shot cross-dataset object detection under different weather and time-of-day conditions (using metric AP50). All detectors are trained over the training datasets (LVIS and ImageNet-21K) and evaluated over BDD100K and DAWN datasets without finetuning. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "TNQ0hxh3O1/tmp/aa482abac0e2dd4990cdd6ce9ecac9fe888611526537c9a3e1ca59bb0ed5ec1f.jpg", "table_caption": ["Table 4: Zero-shot cross-dataset object detection for intelligent surveillance. All detectors are trained over the training datasets (LVIS and ImageNet-21K) and evaluated over surveillance datasets MIO-TCD, BAAI-VANJEE, DETRAC and UAVDT without finetuning. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Object detection for common objects. Table 1 shows that DetLH outperforms state-of-the-art methods clearly on common object datasets Object365 and Pascal VOC. In addition, we can observe that DetLH even brings significant gains above the dataset-specific oracle (i.e., the model that is fully trained on the target training data) on Pascal VOC (i.e., a small-scale dataset), showing the advantages of leveraging large-scale training data. ", "page_idx": 6}, {"type": "text", "text": "Object detection for autonomous driving. As shown in Table 2, our DetLH outperforms state-ofthe-art methods by large margins on various autonomous driving datasets, showing that DetLH still works effectively while facing large variations in camera views from autonomous driving scenarios to the base-dataset scenarios (LVIS and ImageNet-21K), e.g., autonomous driving images are captured under very different camera views. In addition, the experimental results in Table 3 show that our DetLH brings significant performance gains against state-of-the-art methods when encountering various weather and time-of-day conditions, which demonstrates the effectiveness of DetLH while detecting objects under large noises [83], e.g., the images captured under different weather and time-of-day conditions may have very different styles and image quality. ", "page_idx": 6}, {"type": "text", "text": "Object detection for intelligent surveillance. From Table 4, we can observe that our DetLH outperforms state-of-the-art methods by clear margins on various intelligent surveillance datasets, indicating that DetLH is also tolerant to large changes in the camera lens and angles which often happen to intelligent-surveillance images that are captured under very different camera lens and angles (e.g., surveillance cameras are often with the wide-angle lens and used in high angle views). ", "page_idx": 6}, {"type": "text", "text": "Object detection for Wildlife. The experimental results in Table 5 show that our DetLH performs well on various wildlife detection datasets, showing that DetLH works effectively for detecting fine-grained categories that exist widely in wildlife detection datasets. The significant performance gains largely come from the introduction of language hierarchy into detector training and prompt generation, which helps model the hierarchical relations among parent and fine-grained subcategories and thus leads to better fine-grained object detection. ", "page_idx": 6}, {"type": "table", "img_path": "TNQ0hxh3O1/tmp/1f39bafe53db32c513f8415800fc9a86f1a43a7bd6ebc05607817b83e2ddaba7.jpg", "table_caption": ["Table 5: Zero-shot cross-dataset object detection for Wildlife Detection. All detectors are trained over the training datasets (LVIS and ImageNet-21K) and evaluated over wildlife datasets (i.e., Arthropod Detection, AfricanWildlife and Animals Detection) without finetuning. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "The superior detection performance of our DetLH is largely attributed to our two core designs, i.e., LHST and LHPG. LHST enables effective usage of large-scale image-level annotated images and significantly enlarges the data distribution and the data vocabulary in detector training, yielding robust performance under large cross-dataset gaps in data distribution and vocabulary. LHPG ingeniously helps mitigate the vocabulary gaps between detector training and testing. It improves the overall confidence of detection and benefits the detection as large data distribution gaps (or large data vocabulary gaps) often lead to low-confidence predictions and poor detection results. ", "page_idx": 7}, {"type": "text", "text": "4.2 Ablation Studies ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We perform ablation studies with Swin-B [84] based CenterNet2 [54] over the large-scale Object365 dataset as shown in Table 6. As the core of our proposed DetLH, we examine how our designed LHST and LHPG contribute to the overall performance of zero-shot cross-dataset object detection. As shown in Table 6, the baseline (BoxSupervised [15]) does not perform well as it uses box-level training data only. It can be observed that LHST outperforms the baseline clearly, showing that LHST can effectively leverage the largescale image-level annotated dataset to significantly enlarge the data distribution and data vocabulary involved in detector training, leading to ", "page_idx": 7}, {"type": "table", "img_path": "TNQ0hxh3O1/tmp/ca7e6cde6b3e7fa028a047ac93702c408c06dd8ae0d732422bc4774137b164a6.jpg", "table_caption": ["Table 6: Ablation studies of our DetLH with Language Hierarchical Self-training (LHST) and Language Hierarchical Prompt Generation (LHPG). The experiments are conducted with Swin-B based CenterNet2 [15] and the detectors are evaluated on Object365 in zero-shot cross-dataset object detection setup. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "much better zero-shot cross-dataset detection performance. In addition, LHPG brings clear performance improvements in zero-shot cross-dataset detection by introducing language hierarchy into prompt generation, demonstrating the effectiveness of LHPG in mitigating the vocabulary gaps between training and testing. Moreover, the inclusion of both LHST and LHPG in the proposed DetLH performs clearly the best, indicating the complementary property of our two designs. ", "page_idx": 7}, {"type": "text", "text": "4.3 Discussion ", "text_level": 1, "page_idx": 7}, {"type": "table", "img_path": "TNQ0hxh3O1/tmp/1c07c218824a36692cde167927adf0a737915c7a891d4cb863dfb7246bba826d.jpg", "table_caption": ["Table 7: Zero-shot cross-dataset object detection on various datasets. Results are averaged on 14 widely studied datasets. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Generalization across various detection tasks: We study the generalization of our DetLH by conducting zero-shot cross-dataset object detection on 14 widely studied object detection datasets. Tables 1- 5 show that DetLH achieves superior performance consistently across all the detection applications. Besides, Table 7 summarizes the detection results averaged on 14 datasets, showing that DetLH clearly outperforms the state-of-the-art methods. ", "page_idx": 8}, {"type": "text", "text": "Generalization across various network architectures: We study the generalization of the proposed DetLH from the perspective of network architectures. Specifically, we perform extensive evaluations with four representative network architectures, including one Transformer-based (i.e., Swin-B) and three CNN-based (i.e., ConvNeXt-T, ResNet-50 and ResNet-18). Experimental results in Table 8 show that the proposed DetLH outperforms the state-of-the-art method consistently over different network architectures. ", "page_idx": 8}, {"type": "table", "img_path": "TNQ0hxh3O1/tmp/f009b0f860acf5cead4bbf4c2f09951c8bceab57249ef100b072e79a44fbacde.jpg", "table_caption": ["Table 8: Zero-shot cross-dataset object detection with different network architectures. All networks architectures are trained over the training datasets (LVIS and ImageNet-21K) and evaluated over Object365 without finetuning. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Parameter Studies for Language Hierarchical Self- training (LHST). In generating pseudo box labels in LHST, we filter out a prediction if its max confidence score is lower than the threshold $t$ We study the threshold $t$ by changing it from 0.65 to 0.85 with a step of 0.05. Table 12 reports the experimental results on zero-shot transfer object detection over object365 dataset. We can observe that the detection performance is not sensitive to the threshold $t$ . ", "page_idx": 8}, {"type": "text", "text": "Table 9: Parameter Studies for Language Hierarchical Self- training (LHST) on zero-shot transfer object detection over object365 dataset. We study the thresholding parameter $t$ used in generating pseudo box labels in LHST. ", "page_idx": 8}, {"type": "table", "img_path": "TNQ0hxh3O1/tmp/2badd5e5362318b1546ce65ae02c5d2c6619fc7afeaf6d7af653a0033a3111d5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Due to the space limit, we provide more DetLH discussions and visualizations in the appendix. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "This paper presents DetLH, a Detector with Language Hierarchy that combines language hierarchical self-training (LHST) and language hierarchical prompt generation (LHPG) for learning generalizable object detectors. LHST introduces WordNet\u2019s language hierarchy to expand the image-level labels and accordingly enables co-regularization between the expanded labels and self-training. LHPG helps mitigate the vocabulary gaps between training and testing by introducing WordNet\u2019s language hierarchy into prompt generation. Extensive experiments over multiple object detection tasks show that our DetLH achieves superior performance as compared with state-of-the-art methods. In addition, we demonstrate that DetLH works well with different network architectures such as Swin-B, ConvNeXt-T, ResNet-50, etc. Moving forward, we will explore language hierarchy to further expand the labels in an open-vocabulary manner in addition to the closed ImageNet-21K\u2019s vocabulary. ", "page_idx": 8}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3213\u20133223, 2016.   \n[2] Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bulo, and Peter Kontschieder. The mapillary vistas dataset for semantic understanding of street scenes. In Proceedings of the IEEE international conference on computer vision, pages 4990\u20134999, 2017.   \n[3] Jianhua Han, Xiwen Liang, Hang Xu, Kai Chen, Lanqing Hong, Jiageng Mao, Chaoqiang Ye, Wei Zhang, Zhenguo Li, Xiaodan Liang, et al. Soda10m: a large-scale 2d self/semi-supervised object detection dataset for autonomous driving. arXiv preprint arXiv:2106.11118, 2021.   \n[4] Jingyi Zhang, Jiaxing Huang, Zhipeng Luo, Gongjie Zhang, Xiaoqin Zhang, and Shijian Lu. Da-detr: Domain adaptive detection transformer with information fusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 23787\u201323798, 2023.   \n[5] Zhiming Luo, Frederic Branchaud-Charron, Carl Lemaire, Janusz Konrad, Shaozi Li, Akshaya Mishra, Andrew Achkar, Justin Eichel, and Pierre-Marc Jodoin. Mio-tcd: A new benchmark dataset for vehicle classification and localization. IEEE Transactions on Image Processing, 27(10):5129\u20135141, 2018.   \n[6] Deng Yongqiang, Wang Dengjiang, Cao Gang, Ma Bing, Guan Xijia, Wang Yajun, Liu Jianchao, Fang Yanming, and Li Juanjuan. Baai-vanjee roadside dataset: Towards the connected automated vehicle highway technologies in challenging environments of china. arXiv preprint arXiv:2105.14370, 2021.   \n[7] Longyin Wen, Dawei Du, Zhaowei Cai, Zhen Lei, Ming-Ching Chang, Honggang Qi, Jongwoo Lim, Ming-Hsuan Yang, and Siwei Lyu. Ua-detrac: A new benchmark and protocol for multi-object detection and tracking. Computer Vision and Image Understanding, 193:102907, 2020.   \n[8] Dawei Du, Yuankai Qi, Hongyang Yu, Yifan Yang, Kaiwen Duan, Guorong Li, Weigang Zhang, Qingming Huang, and Qi Tian. The unmanned aerial vehicle benchmark: Object detection and tracking. In Proceedings of the European conference on computer vision (ECCV), pages 370\u2013386, 2018.   \n[9] Geir Drange. Arthropod taxonomy orders object detection dataset, 2020.   \n[10] Wichayas YoLov5. African animals dataset. https://universe.roboflow.com/wichayas-yolov5/ african_animals, may 2022. visited on 2023-03-01.   \n[11] Kaggle. Animal detection dataset. https://universe.roboflow.com/kaggle/ animal-detection-7vafe, apr 2022. visited on 2023-03-01.   \n[12] Hakan Bilen and Andrea Vedaldi. Weakly supervised deep detection networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2846\u20132854, 2016.   \n[13] Joseph Redmon and Ali Farhadi. Yolo9000: better, faster, stronger. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7263\u20137271, 2017.   \n[14] Vignesh Ramanathan, Rui Wang, and Dhruv Mahajan. Dlwl: Improving detection for lowshot classes with weakly labelled data. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9342\u20139352, 2020.   \n[15] Zhou Xingyi. Detecting twenty-thousand classes using image-level supervision. arXiv preprint arXiv:2201.02605, 2022.   \n[16] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.   \n[17] Hanoona Rasheed, Muhammad Maaz, Muhammad Uzair Khattak, Salman Khan, and Fahad Shahbaz Khan. Bridging the gap between object and image-level representations for open-vocabulary detection. arXiv preprint arXiv:2207.03482, 2022.   \n[18] Kihyuk Sohn, Zizhao Zhang, Chun-Liang Li, Han Zhang, Chen-Yu Lee, and Tomas Pfister. A simple semi-supervised learning framework for object detection. arXiv preprint arXiv:2005.04757, 2020.   \n[19] Jingyi Zhang, Jiaxing Huang, Zichen Tian, and Shijian Lu. Spectral unsupervised domain adaptation for visual recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9829\u20139840, 2022.   \n[20] Jiaxing Huang, Dayan Guan, Aoran Xiao, and Shijian Lu. Cross-view regularization for domain adaptive panoptic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10133\u201310144, 2021.   \n[21] George A Miller. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39\u201341, 1995.   \n[22] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748\u20138763. PMLR, 2021.   \n[23] Xiaoyan Li, Meina Kan, Shiguang Shan, and Xilin Chen. Weakly supervised object detection with segmentation collaboration. In Proceedings of the IEEE/CVF international conference on computer vision, pages 9735\u20139744, 2019.   \n[24] Yunhang Shen, Rongrong Ji, Yan Wang, Zhiwei Chen, Feng Zheng, Feiyue Huang, and Yunsheng Wu. Enabling deep residual networks for weakly supervised object detection. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part VIII 16, pages 118\u2013136. Springer, 2020.   \n[25] Yunhang Shen, Rongrong Ji, Yan Wang, Yongjian Wu, and Liujuan Cao. Cyclic guidance for weakly supervised joint detection and segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 697\u2013707, 2019.   \n[26] Fang Wan, Chang Liu, Wei Ke, Xiangyang Ji, Jianbin Jiao, and Qixiang Ye. C-mil: Continuation multiple instance learning for weakly supervised object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2199\u20132208, 2019.   \n[27] Ke Yang, Dongsheng Li, and Yong Dou. Towards precise end-to-end weakly supervised object detection network. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8372\u2013 8381, 2019.   \n[28] Pablo Arbel\u00e1ez, Jordi Pont-Tuset, Jonathan T Barron, Ferran Marques, and Jitendra Malik. Multiscale combinatorial grouping. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 328\u2013335, 2014.   \n[29] Jasper RR Uijlings, Koen EA Van De Sande, Theo Gevers, and Arnold WM Smeulders. Selective search for object recognition. International journal of computer vision, 104:154\u2013171, 2013.   \n[30] Peng Tang, Xinggang Wang, Xiang Bai, and Wenyu Liu. Multiple instance detection network with online instance classifier refinement. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2843\u20132851, 2017.   \n[31] Peng Tang, Xinggang Wang, Song Bai, Wei Shen, Xiang Bai, Wenyu Liu, and Alan Yuille. Pcl: Proposal cluster learning for weakly supervised object detection. IEEE transactions on pattern analysis and machine intelligence, 42(1):176\u2013191, 2018.   \n[32] Zeyi Huang, Yang Zou, BVK Kumar, and Dong Huang. Comprehensive attention self-distillation for weakly-supervised object detection. Advances in neural information processing systems, 33:16797\u201316807, 2020.   \n[33] Bowen Dong, Zitong Huang, Yuelin Guo, Qilong Wang, Zhenxing Niu, and Wangmeng Zuo. Boosting weakly supervised object detection via learning bounding box adjusters. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2876\u20132885, 2021.   \n[34] Shijie Fang, Yuhang Cao, Xinjiang Wang, Kai Chen, Dahua Lin, and Wayne Zhang. Wssod: A new pipeline for weakly-and semi-supervised object detection. arXiv preprint arXiv:2105.11293, 2021.   \n[35] Yan Li, Junge Zhang, Kaiqi Huang, and Jianguo Zhang. Mixed supervised object detection with robust objectness transfer. IEEE transactions on pattern analysis and machine intelligence, 41(3):639\u2013653, 2018.   \n[36] Yan Liu, Zhijie Zhang, Li Niu, Junjie Chen, and Liqing Zhang. Mixed supervised object detection by transferring mask prior and semantic similarity. Advances in Neural Information Processing Systems, 34:3978\u20133990, 2021.   \n[37] Jasper Uijlings, Stefan Popov, and Vittorio Ferrari. Revisiting knowledge transfer for training object class detectors. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1101\u20131110, 2018.   \n[38] Ziang Yan, Jian Liang, Weishen Pan, Jin Li, and Changshui Zhang. Weakly-and semi-supervised object detection with expectation-maximization algorithm. arXiv preprint arXiv:1702.08740, 2017.   \n[39] Yuanyi Zhong, Jianfeng Wang, Jian Peng, and Lei Zhang. Boosting weakly supervised object detection with progressive knowledge transfer. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XXVI, pages 615\u2013631. Springer, 2020.   \n[40] Cheng Zhang, Tai-Yu Pan, Yandong Li, Hexiang Hu, Dong Xuan, Soravit Changpinyo, Boqing Gong, and Wei-Lun Chao. Mosaicos: a simple and effective use of object-centric images for long-tailed object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 417\u2013427, 2021.   \n[41] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary instance segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 5356\u2013 5364, 2019.   \n[42] Bharat Singh, Hengduo Li, Abhishek Sharma, and Larry S Davis. R-fcn-3000 at 30fps: Decoupling detection and classification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1081\u20131090, 2018.   \n[43] Hao Yang, Hao Wu, and Hao Chen. Detecting 11k classes: Large scale object detection without finegrained bounding boxes. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9805\u20139813, 2019.   \n[44] Jingyi Zhang, Jiaxing Huang, Sheng Jin, and Shijian Lu. Vision-language models for vision tasks: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024.   \n[45] Nadine Chang, Zhiding Yu, Yu-Xiong Wang, Animashree Anandkumar, Sanja Fidler, and Jose M Alvarez. Image-level or object-level? a tale of two resampling strategies for long-tailed detection. In International conference on machine learning, pages 1463\u20131472. PMLR, 2021.   \n[46] Chengjian Feng, Yujie Zhong, and Weilin Huang. Exploring classification equilibrium in long-tailed object detection. In Proceedings of the IEEE/CVF International conference on computer vision, pages 3417\u20133426, 2021.   \n[47] Yu Li, Tao Wang, Bingyi Kang, Sheng Tang, Chunfeng Wang, Jintao Li, and Jiashi Feng. Overcoming classifier imbalance for long-tail object detection with balanced group softmax. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10991\u201311000, 2020.   \n[48] Tai-Yu Pan, Cheng Zhang, Yandong Li, Hexiang Hu, Dong Xuan, Soravit Changpinyo, Boqing Gong, and Wei-Lun Chao. On model calibration for long-tailed object detection and instance segmentation. Advances in Neural Information Processing Systems, 34:2529\u20132542, 2021.   \n[49] Jialian Wu, Liangchen Song, Tiancai Wang, Qian Zhang, and Junsong Yuan. Forest r-cnn: Largevocabulary long-tailed object detection and instance segmentation. In Proceedings of the 28th ACM International Conference on Multimedia, pages 1570\u20131578, 2020.   \n[50] Songyang Zhang, Zeming Li, Shipeng Yan, Xuming He, and Jian Sun. Distribution alignment: A unified framework for long-tail visual recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2361\u20132370, 2021.   \n[51] Jingru Tan, Xin Lu, Gang Zhang, Changqing Yin, and Quanquan Li. Equalization loss v2: A new gradient balance approach for long-tailed object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1685\u20131694, 2021.   \n[52] Jingru Tan, Changbao Wang, Buyu Li, Quanquan Li, Wanli Ouyang, Changqing Yin, and Junjie Yan. Equalization loss for long-tailed object recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11662\u201311671, 2020.   \n[53] Jiaqi Wang, Wenwei Zhang, Yuhang Zang, Yuhang Cao, Jiangmiao Pang, Tao Gong, Kai Chen, Ziwei Liu, Chen Change Loy, and Dahua Lin. Seesaw loss for long-tailed instance segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9695\u20139704, 2021.   \n[54] Xingyi Zhou, Vladlen Koltun, and Philipp Kr\u00e4henb\u00fchl. Probabilistic two-stage detection. arXiv preprint arXiv:2103.07461, 2021.   \n[55] Ankan Bansal, Karan Sikka, Gaurav Sharma, Rama Chellappa, and Ajay Divakaran. Zero-shot object detection. In Proceedings of the European Conference on Computer Vision (ECCV), pages 384\u2013400, 2018.   \n[56] Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532\u20131543, 2014.   \n[57] Shafin Rahman, Salman Khan, and Nick Barnes. Improved visual-semantic alignment for zero-shot object detection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 11932\u201311939, 2020.   \n[58] Zhihui Li, Lina Yao, Xiaoqin Zhang, Xianzhi Wang, Salil Kanhere, and Huaxiang Zhang. Zero-shot object detection with textual descriptions. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 8690\u20138697, 2019.   \n[59] Alireza Zareian, Kevin Dela Rosa, Derek Hao Hu, and Shih-Fu Chang. Open-vocabulary object detection using captions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14393\u201314402, 2021.   \n[60] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui. Open-vocabulary object detection via vision and language knowledge distillation. arXiv preprint arXiv:2104.13921, 2021.   \n[61] Golnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin. Open-vocabulary image segmentation. arXiv preprint arXiv:2112.12143, 2021.   \n[62] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen Koltun, and Ren\u00e9 Ranftl. Language-driven semantic segmentation. arXiv preprint arXiv:2201.03546, 2022.   \n[63] Yuhang Zang, Wei Li, Kaiyang Zhou, Chen Huang, and Chen Change Loy. Open-vocabulary detr with conditional matching. arXiv preprint arXiv:2203.11876, 2022.   \n[64] Carlos N Silla and Alex A Freitas. A survey of hierarchical classification across different application domains. Data Mining and Knowledge Discovery, 22:31\u201372, 2011.   \n[65] Ivica Dimitrovski, Dragi Kocev, Suzana Loskovska, and Sa\u0161o D\u017eeroski. Hierarchical annotation of medical images. Pattern Recognition, 44(10-11):2436\u20132449, 2011.   \n[66] Hui Liu, Danqing Zhang, Bing Yin, and Xiaodan Zhu. Improving pretrained models for zero-shot multilabel text classification through reinforced label hierarchy reasoning. arXiv preprint arXiv:2104.01666, 2021.   \n[67] Ilias Chalkidis, Manos Fergadiotis, Sotiris Kotitsas, Prodromos Malakasiotis, Nikolaos Aletras, and Ion Androutsopoulos. An empirical study on large-scale multi-label text classification including few and zero-shot labels. arXiv preprint arXiv:2010.01653, 2020.   \n[68] Shiming Chen, Guosen Xie, Yang Liu, Qinmu Peng, Baigui Sun, Hao Li, Xinge You, and Ling Shao. Hsva: Hierarchical semantic-visual adaptation for zero-shot learning. Advances in Neural Information Processing Systems, 34:16622\u201316634, 2021.   \n[69] Thomas Mensink, Efstratios Gavves, and Cees GM Snoek. Costa: Co-occurrence statistics for zero-shot classification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2441\u20132448, 2014.   \n[70] Kai Yi, Xiaoqian Shen, Yunhao Gou, and Mohamed Elhoseiny. Exploring hierarchical graph representation for large-scale zero-shot image classification. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XX, pages 116\u2013132. Springer, 2022.   \n[71] Zhong Cao, Jiang Lu, Sen Cui, and Changshui Zhang. Zero-shot handwritten chinese character recognition with hierarchical decomposition embedding. Pattern Recognition, 107:107488, 2020.   \n[72] Mei Wang and Weihong Deng. Deep visual domain adaptation: A survey. Neurocomputing, 312:135\u2013153, 2018.   \n[73] Jiaxing Huang, Dayan Guan, Aoran Xiao, and Shijian Lu. Model adaptation: Historical contrastive learning for unsupervised domain adaptation without source data. Advances in Neural Information Processing Systems, 34, 2021.   \n[74] Pan Zhang, Bo Zhang, Ting Zhang, Dong Chen, Yong Wang, and Fang Wen. Prototypical pseudo label denoising and target structure learning for domain adaptive semantic segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12414\u201312424, 2021.   \n[75] Jiaxing Huang, Dayan Guan, Aoran Xiao, Shijian Lu, and Ling Shao. Category contrast for unsupervised domain adaptation in visual tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1203\u20131214, 2022.   \n[76] Jiaxing Huang, Dayan Guan, Aoran Xiao, and Shijian Lu. Fsdr: Frequency space domain randomization for domain generalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6891\u20136902, 2021.   \n[77] Kaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, and Chen Change Loy. Domain generalization: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(4):4396\u20134415, 2022.   \n[78] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16816\u201316825, 2022.   \n[79] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: A large-scale, high-quality dataset for object detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 8430\u20138439, 2019.   \n[80] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. International journal of computer vision, 88:303\u2013308, 2009.   \n[81] Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying Chen, Fangchen Liu, Vashisht Madhavan, and Trevor Darrell. Bdd100k: A diverse driving dataset for heterogeneous multitask learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2636\u20132645, 2020.   \n[82] Mourad A Kenk and Mahmoud Hassaballah. Dawn: vehicle detection in adverse weather nature dataset. arXiv preprint arXiv:2008.05402, 2020.   \n[83] Jiaxing Huang, Dayan Guan, Aoran Xiao, and Shijian Lu. Rda: Robust domain adaptation via fourier adversarial attacking. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8988\u20138999, 2021.   \n[84] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012\u201310022, 2021.   \n[85] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11976\u201311986, 2022.   \n[86] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \n[87] L\u00e9on Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of COMPSTAT\u20192010, pages 177\u2013186. Springer, 2010.   \n[88] Siddhesh Khandelwal, Raghav Goyal, and Leonid Sigal. Unit: Unified knowledge transfer for any-shot object detection and segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5951\u20135961, 2021.   \n[89] Yunqiu Xu, Chunluan Zhou, Xin Yu, and Yi Yang. Cyclic self-training with proposal weight modulation for cross-supervised object detection. IEEE Transactions on Image Processing, 32:1992\u20132002, 2023.   \n[90] Yunqiu Xu, Yifan Sun, Zongxin Yang, Jiaxu Miao, and Yi Yang. H2fa r-cnn: Holistic and hierarchical feature alignment for cross-domain weakly supervised object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 14329\u201314339, 2022.   \n[91] Krishna Kumar Singh, Santosh Divvala, Ali Farhadi, and Yong Jae Lee. Dock: Detecting objects by transferring common-sense knowledge. In Proceedings of the European Conference on Computer Vision (ECCV), pages 492\u2013508, 2018.   \n[92] Liunian Harold ${\\mathrm{Li}}^{*}$ , Pengchuan Zhang\\*, Haotian Zhang\\*, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang, and Jianfeng Gao. Grounded language-image pre-training. In CVPR, 2022.   \n[93] Lewei Yao, Jianhua Han, Youpeng Wen, Xiaodan Liang, Dan Xu, Wei Zhang, Zhenguo Li, Chunjing Xu, and Hang Xu. Detclip: Dictionary-enriched visual-concept paralleled pre-training for open-world detection. arXiv preprint arXiv:2209.09407, 2022.   \n[94] Yutong Lin, Chen Li, Yue Cao, Zheng Zhang, Jianfeng Wang, Lijuan Wang, Zicheng Liu, and Han Hu. A simple approach and benchmark for 21,000-category object detection. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XI, pages 1\u201318. Springer, 2022.   \n[95] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.   \n[96] Haotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun Chen, Liunian Li, Xiyang Dai, Lijuan Wang, Lu Yuan, Jenq-Neng Hwang, and Jianfeng Gao. Glipv2: Unifying localization and vision-language understanding. Advances in Neural Information Processing Systems, 35:36067\u201336080, 2022.   \n[97] Matthias Minderer. Simple open-vocabulary object detection with vision transformers. ECCV, 2022.   \n[98] Tiancheng Zhao, Peng Liu, Xiaopeng Lu, and Kyusong Lee. Omdet: Language-aware object detection with large-scale vision-language multi-dataset pre-training. arXiv preprint arXiv:2209.05946, 2022.   \n[99] Xingyi Zhou, Vladlen Koltun, and Philipp Kr\u00e4henb\u00fchl. Simple multi-dataset detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7571\u20137580, 2022.   \n[100] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for computer vision. arXiv preprint arXiv:2111.11432, 2021.   \n[101] Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor. Imagenet-21k pretraining for the masses. arXiv preprint arXiv:2104.10972, 2021.   \n[102] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. International Journal of Computer Vision, 128(7):1956\u20131981, 2020. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A Dataset and Implementation Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Implementation Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "As in [17, 15], we adopt the CenterNet2 [54] with Swin-B [84] backbone in all the experiments (except for Table 8 where different backbone architectures were used, e.g., ConvNeXt-T, ResNet-50 and ResNet-18). We employ SGD [87] as the optimizer and adopt the cosine learning rate scheduler with a warm-up of 1000 iterations [15]. We set the input sizes of box-level annotated images (i.e., LVIS) and image-level annotated images (i.e., ImageNet-21K) as $896\\times896$ and $448\\times448$ , respectively. As mentioned in Section 3.1, we employ the CLIP text embeddings [22] as the classifier instead of using the original one in [54]. During training, we sample box-level and image-level mini-batches in a $1:16$ ratio. We set the confidence threshold $t$ (in pseudo box label generation in Eq. 3) as 0.75 in all experiments except in parameter analysis. Note we pre-train the detector over the training datasets (i.e., training on LVIS with the conventional detection loss and on ImageNet-21K with the conventional image classification loss) such that it can generate pseudo box-level labels of 21K classes for self-training. ", "page_idx": 15}, {"type": "text", "text": "As described in the main text, we train our detector over two training datasets LVIS and ImageNet-21K, and evaluate the trained detector over 14 evaluation datasets as listed without fine-tuning. ", "page_idx": 15}, {"type": "text", "text": "A.2 Training Dataset ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "LVIS [41] is a large vocabulary dataset designed for long-tailed instance segmentation, which contains 100K images and 1203 categories. LVIS provides high-quality instance-wise annotations, including instance masks, class labels and bounding boxes. ", "page_idx": 15}, {"type": "text", "text": "ImageNet-21K [16] is a large and diverse dataset over 14M images across more than 21K categories. All categories in ImageNet-21K are defined by WordNet Synsets with clear and accurate definitions and certain language hierarchy. ", "page_idx": 15}, {"type": "text", "text": "A.3 Evaluation Dataset ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Object365 [79] is a large-scale object detection dataset designed for object detection in the wild. This dataset contains 638K images across 365 categories, including 600K images for training and 38K images for validation. ", "page_idx": 15}, {"type": "text", "text": "Pascal VOC [80] is a real-world dataset with two sub-datasets, i.e., PASCAL VOC 2007 and PASCAL VOC 2012. PASCAL VOC 2007 contains 2,501 training images and 2,510 validation images, and PASCAL VOC 2012 contains 5,717 training images and 5,823 validation images. This dataset provides bounding box annotations with 20 categories. ", "page_idx": 15}, {"type": "text", "text": "Cityscapes [1] is a dataset designed for the understanding of street scenes. The images in Cityscapes are captured under normal weather conditions from 50 cities, including 2,975 training images and 500 validation images with pixel-wise instance annotations of 8 categories. ", "page_idx": 15}, {"type": "text", "text": "Vista [2] is a street-level autonomous driving dataset. This dataset contains high-resolution images that cover diverse urban scenes from around the world, including 18K training images and 2K validation images with pixel-wise instance annotations. ", "page_idx": 15}, {"type": "text", "text": "SODA10M [3] is a large-scale object detection dataset for autonomous driving, which contains 10M unlabeled images and 20K images with bounding box annotations of 6 object categories. The images in this dataset are collected within 27833 driving hours covering a variety time periods and locations across 32 different cities. ", "page_idx": 15}, {"type": "text", "text": "BDD100k [81] is a large-scale driving video dataset that contains diverse driving scenarios, including different weather conditions (i.e., clear, cloudy, overcast, rainy, snowy and foggy) and times of day (i.e., dawn, daytime and night). This dataset contains 100K videos, including 70K training videos and 10K validation videos with bounding box annotations of 10 categories. ", "page_idx": 15}, {"type": "text", "text": "Arthropod Detection [9] is a detection dataset for arthropods taxonomy orders identification. The images are collected from a variety of agricultural settings (e.g., fields, greenhouses, warehouses), including over 12K images with bounding box annotations of 7 categories. ", "page_idx": 15}, {"type": "text", "text": "AfricanWildlife [10] is a detection dataset which contains images of African wildlife with bounding box annotations. This dataset contains 4 different categories of African wildlife including buffalo, elephant, rhino, zebra and each category contains 376 images. ", "page_idx": 15}, {"type": "text", "text": "Animals Detection [11] is a public dataset of various animals. This dataset contains animal images with bounding boxes of 80 different animal categories, including 6.8K training images and 1.9K validation images. ", "page_idx": 15}, {"type": "text", "text": "DAWN [82]) is a vehicle detection dataset that focuses on diverse traffic environment. This dataset contains 1K images from real-traffic environment, including fog, snow, rain and sandstorms. The images are annotated with object bounding boxes of 6 categories. ", "page_idx": 16}, {"type": "text", "text": "MIO-TCD [5] is an intelligent surveillance dataset for motorized traffic analysis, which contains 137,743 images captured in various times of the day and different periods of the year, and from different viewing angels. This dataset provides bounding box annotations with 11 categories. ", "page_idx": 16}, {"type": "text", "text": "BAAI-VANJEE [6] is an intelligent surveillance dataset which contains 5K images captured by VANJEE smart base station placed about $4.5\\mathrm{m}$ high. The images in this dataset vary in weather and traffic conditions, which are annotated with bounding box annotations of 12 categories. ", "page_idx": 16}, {"type": "text", "text": "DETRAC [7] is an intelligent surveillance dataset that contains over 14K images captured by a Canon EOS 550D camera at 24 different locations, covering various traffic patterns and conditions including urban highway, traffic crossings and T-junctions. The images in this dataset are annotated with bounding box annotations of 4 categories, including car, bus, van, and others. ", "page_idx": 16}, {"type": "text", "text": "UAVDT [8]) is a unmanned aerial vehicle detection dataset, which contains about 80K frames from 10 hours videos. The images in this dataset are captured by a unmanned aerial vehicle across various weather conditions (i.e., daylight, night and fog) and multiple camera views (i.e., front-view, side-view and bird-view). This dataset provides bounding box annotations with three categories including car, truck and bus. ", "page_idx": 16}, {"type": "text", "text": "B Additional Discussion ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Strategy studies for Language Hierarchical Self-training. Our proposed language hierarchical self-training (LHST) introduces WordNet\u2019s language hierarchy [21] to expand the image-level labels and accordingly enables co-regularization between the expanded labels and self-training. We examine the superiority of the proposed LHST by comparing it with \u201cSelf-training\" [18] and \u201cDirect WordNet Hierarchy Labeling\" [21]. \u201cSelf-training\" is the standard self-training algorithm as in [18] while \u201cDirect WordNet Hierarchy Labeling\" denotes directly using the expanded image-level labels (by WordNet) for weakly-supervised detection training. Table 10 reports the experimental results, which show that either \u201cSelf-training\" [18] or \u201cDirect WordNet Hierarchy Labeling\" [21] does not perform well. The reasons are: 1) the box-level pseudo labels in \u201cSelf-training\" are usually error-prone, making the self-training process unstable and barely improving the performance; 2) the expanded image-level labels in \u2018Direct WordNet Hierarchy Labeling\" are not all reliable, training with which leads to unsatisfying performance. Besides, the combination of \u2018Self-training\" and \u201cDirect WordNet Hierarchy Labeling\" still works not very well largely because the direct combination of them does not well address their own drawbacks and limitations. On the other hand, our proposed LHST performs better clearly, as shown in the last row of Table 10. The superior performance of LHST is largely attributed the co-regularization design, which employs self-training to assess and re-weight the expanded labels according to the predicted reliability while enabling the expanded (and re-weighted) labels to regularize self-training by providing richer and flexible supervision (the flexible supervision is achieved by the adaptive re-weighting operation). ", "page_idx": 16}, {"type": "table", "img_path": "TNQ0hxh3O1/tmp/a0f093da0ccbe6130a269916541137387ed0b23d55654caa7b0875312b161b23.jpg", "table_caption": ["Table 10: Strategy studies for Language Hierarchical Self-training on zero-shot cross-dataset object detection over object365 dataset. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Ablation studies of Language Hierarchical Self-training. As mentioned in the main text, our proposed language hierarchical self-training (LHST) consists of box-leve LHST (i.e., $\\mathcal{L}_{b o x}(F(x))$ in Eq.7 in the main text) and image-level LHST (i.e., $\\mathcal{L}_{i m a g e}(F(x))$ in Eq.8 in the main text), where image-level LHST could regularize box-level LHST. Here we conduct experiments to investigate this. The experimental results in Table 11 show that Box-level LHST brings clear performance improvements while including Image-level LHST further improves the detection performance, largely becuase Image-level LHST provides stable supervision to regularize Box-level LHST, i.e., Image-level LHST is much more stable as it uses the image-level proposal while the pseudo box labels in Box-level LHST (e.g., the location of pseudo boxes) vary along training iterations and are not very stable. ", "page_idx": 16}, {"type": "table", "img_path": "TNQ0hxh3O1/tmp/d16f93c99b71caf08a85af6924012f82903b434a4a838d2da051f330035f2e2a.jpg", "table_caption": ["Table 11: Ablation studies of Language Hierarchical Self-training. The experiment setup is zero-shot cross-dataset object detection over Object365 dataset. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Parameter Studies for Language Hierarchical Self- training (LHST). In generating pseudo box labels in LHST, we fliter out a prediction if its max confidence score is lower than the threshold $t$ . We study the threshold $t$ by changing it from 0.65 to 0.85 with a step of 0.05. Table 12 reports the experimental results on zero-shot transfer object detection over object365 dataset. We can observe that the detection performance is not sensitive to the threshold $t$ . ", "page_idx": 17}, {"type": "text", "text": "Table 12: Parameter Studies for Language Hierarchical Self- training (LHST) on zero-shot transfer object detection over object365 dataset. We study the thresholding parameter $t$ used in generating pseudo box labels in LHST. ", "text_level": 1, "page_idx": 17}, {"type": "table", "img_path": "TNQ0hxh3O1/tmp/a268f09a50ab5d963ee4e3b640dd21ffc8045643152888bf557cdce2297deae1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Analysis of discrepancies of the taxonomies of image vs. box categories. We analyze the mismatch between image-level and box-level categories and how much it could affect the detection performance. As Table 13 shows, the mismatch between image-level and box-level categories varies across datasets, and the proposed DetLH improves more with the increase of mismatch levels. ", "page_idx": 17}, {"type": "table", "img_path": "TNQ0hxh3O1/tmp/e19529d4e6b076269856921647e6cdf4b12729a6a8f15ce8accb48f6da3eade8.jpg", "table_caption": ["Table 13: Analysis of discrepancies of the taxonomies of image vs. box categories. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "How effective DetLH deals with noisy labels. We conduct ablation studies to analyze how effective DetLH deals with noisy labels. Specifically, we compare DetLH with and without using reliability scores (the latter means uniform category weights) over Object365. As Table 14 shows, including the adaptive weighting mechanism (i.e., reliability scores) helps mitigate the label noises effectively. ", "page_idx": 17}, {"type": "table", "img_path": "TNQ0hxh3O1/tmp/379dc68c54ce97bf11541cd046dd75c909c0dd71cf82dfbe99f597131318242a.jpg", "table_caption": ["Table 14: How effective DetLH deals with noisy labels. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "The impact of using proxy vocabulary. We conduct experiments on Object365 dataset to compare LHPG with CLIP embeddings only and LHPG with both CLIP embeddings and proxy vocabulary. As Table 15 shows, using a proxy vocabulary performs clearly better, demonstrating its effectiveness in narrowing the distribution gap between training and test labels. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "table", "img_path": "TNQ0hxh3O1/tmp/2ea840168e44d627f40657ac4505c7137fe798753df6c271f6e885f77b3b3d4d.jpg", "table_caption": ["Table 15: The impact of using proxy vocabulary. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Comparisons with other semi-supervised WSOD methods. We conduct experiments on Object365 dataset to compare our DetLH and [88, 89, 90, 91]. As Table 16 shows, DetLH clearly outperforms [a,b,c,d]. ", "page_idx": 18}, {"type": "table", "img_path": "TNQ0hxh3O1/tmp/a2b8b8285a8f7a3446d4ff90b89f1af43fad8744f4628b300c179c4e37c00fba.jpg", "table_caption": ["Table 16: Comparisons with other semi-supervised WSOD methods. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "C Additional Comparison ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Comparison with RKD [17]. We note that RKD [17] explores Region-based Knowledge Distillation to better distill knowledge from the CLIP model for weakly-supervised object detection. In this paragraph, we compare our DetLH (i.e., the self-training based method) with RKD (i.e., the knowledge distillation-based method) on zero-shot cross-dataset object detection over Object365 dataset. Table 17 reports the results, which show that our DetLH clearly outperforms RKD [17], indicating the effectiveness the proposed designs in DetLH for zero-shot cross-dataset object detection. ", "page_idx": 18}, {"type": "text", "text": "Table 17: Comparison with RKD [17] on zero-shot cross-dataset object detection over Object365 dataset. ", "page_idx": 18}, {"type": "table", "img_path": "TNQ0hxh3O1/tmp/1ae273d572d41e3369620c3bc3eedf2468716f1895d0b845b93d8f5967b819ea.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Discussion and comparison with visual grounding-based detection methods [92, 93]. We note that GLIP [92] and DetCLIP [93] explore extra visual grounding data to train a open-vocabulary detector. In this paragraph, we compare our DetLH (i.e., the WSOD-based method) with [92, 93] (i.e., the visual grounding-based method) from the perspective of detection efficiency. Table 18 reports the results of run time in millisecond (the run times of GLIP [92] and DetCLIP [93] are acquired from [93].). It shows that our DetLH (i.e., the WSOD-based method) runs much more efficient than the visual grounding-based detection methods (i.e., GLIP [92] and DetCLIP [93]), largely because the visual grounding-based detection methods [92, 93] include a text encoder in their networks. Note we did not compare our DetLH (i.e., the WSOD-based method) with the visual groundingbased method [92, 93] from the perspective of detection accuracy becuase these two types of detection methods use very different training data, e.g., [92] and [93] use visual grounding data. In addition, the WSOD techniques are basically complementary to the visual grounding-based detection techniques [92, 93] because the visual grounding data could be used to further improve the WSOD-based detectors [94]. Similar to [94], we leave this as our future research. ", "page_idx": 18}, {"type": "text", "text": "Other image-level supervision. We follow Detic [15] to build our proposed DetLH. Therefore, similar to Detic [15], our DetLH can also seamlessly incorporate the free-form caption text as the image-level supervision, i.e., by using the language embeddings of image captions as the detection classifier when training on image-text pair data [15]. In this way, we could further incorporate LAION dataset [95] that includes 400 million image-text pair data into detector training for learning more generalizable object detectors. On the other hand, training over large-scale LAION dataset is computation-intensive and thus we leave this as our future work. ", "page_idx": 18}, {"type": "text", "text": "Comparison with other detection methods [92, 96, 97, 93, 98, 99]. We didn\u2019t compare with these methods [92, 96, 97, 93, 98, 99] in the main manuscript because they focus on different topics with different objectives, training data, backbones and benchmarks. Instead, we follow Detic [15] as both our DetLH and Detic belong to and are claimed as weakly-supervised object detection (WSOD), aiming to using large-scale images and classes (i.e., ImageNet-21K and LVIS) to train a general detector that can work on any detection scenarios. However, [92, 96, 97, 93, 98, 99] are not for WSOD: GLIP and DetCLIP [92, 93] introduce visual grounding and studies how to use grounding data for detection; OWL-ViT [97] focuses on fine-tuning CLIP with standard detection datasets; OmDet and UniDet [98, 99] focus on training with multiple detection datasets. We still managed to benchmark with [92, 96, 97, 93, 98, 99], i.e., GLIP [92], GLIPv2 [96], OWL-ViT [97], DetCLIP [93], OmDet [98] and UniDet [99]. As our method and [92, 96, 97, 93, 98, 99] use various different datasets in evaluations, the benchmarking below is on the shared one, i.e., Pascal VOC (in AP). ", "page_idx": 18}, {"type": "table", "img_path": "TNQ0hxh3O1/tmp/f573abcdc9a1a44defe7857f8556d59456d8f8d7c34e30b0f326d8c1f632b324.jpg", "table_caption": ["Table 18: Efficiency comparison of WSOD-based and visual grounding-based detection methods. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "table", "img_path": "TNQ0hxh3O1/tmp/bbd2cfd2137a02c275346d7a8b8ccfdd1736f850d83b23ab52c3eaf7fdf21862.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Note Florence [100] is not included as it is a foundation model that uses a very large backbone (CoSwin-H) and very large customized training data (FLD-900M and FLOD-9M). Besides, we compared our DetLH with GLIP [92] and DetCLIP [93] as in Table 18, which shows DetLH (i.e., the WSOD-based method) runs much more efficient (about 10 times) than the visual grounding-based detection methods (i.e., GLIP and DetCLIP). ", "page_idx": 19}, {"type": "text", "text": "Comparison on ODinW [92]. We note that ODinW [92] also benchmarks cross-dataset generalization. We benchmark on ODinW and the results below (averaged on 35 datasets in ODinW) show that our DetLH works effectively on ODinW. Note GLIP obtains higher accuracy because it introduces visual grounding and involves Language Encoder in inference. Without those extra modules, our DetLH runs much faster (over 10 times) than GLIP as discussed in Table 18. ", "page_idx": 19}, {"type": "table", "img_path": "TNQ0hxh3O1/tmp/1fd0e09e88d9ab62a409127a28ffab99edf74f01eef73c6cee04172f17b9b060.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Open-vocabulary benchmark. We did not benchmark on open-vocabulary LVIS/COCO (both divide a singledataset vocabulary into base and novel classes to mimic and benchmark vocabulary generalization), because our work aims to leverage large-scale images and classes (i.e., 21K classes) to train a general detector that can work on any detection scenarios. Cross-dataset generalization benchmark ftis this objective better and is more general and challenging than open-vocabulary benchmark that tackles base and novel classes within a single dataset. ", "page_idx": 19}, {"type": "text", "text": "Comparison with class hierarchy methods [101, 70] on OpenImages [102]. We benchmark with other methods that use class hierarchy, including [101] and [70]. As shown below, our DetLH performs clearly better than hierarchy-aware losses in [101, 70] due to our designed co-regularization as detailed in the manuscript. Note we use OpenImages V7 (Oct 2022) instead of 2019 version. ", "page_idx": 19}, {"type": "table", "img_path": "TNQ0hxh3O1/tmp/4d7f7bde0004af96344f9eb9b1d07ac801d7cb5ad8333c60f6f63f26b73c3a7b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Results of WSDDN, YOLO9000 and DLWL. Note Detic implemented WSDDN, YOLO9000 and DLWL and we directly adopted Detic\u2019s implementation in evaluations. The gains in our reported results are different as we evaluated on more challenging datasets and benchmark: 1) The results in our Table 7 in the main manuscript are averaged over 14 datasets while those in Table 1 of Detic are on a single dataset LVIS; 2) Our Table 7 in the main manuscript is cross-dataset generalization benchmark while Table 1 in Detic is open-vocabulary LVIS. ", "page_idx": 19}, {"type": "text", "text": "D Additional Qualitative Result and Comparison ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We provide qualitative results of zero-shot cross-dataset object detection for various detection tasks. As shown in Figures 3- 7. our DetLH produces good detection results consistently across different detection tasks, showing DetLH still works effectively under large cross-dataset gaps in data distribution and vocabulary. ", "page_idx": 19}, {"type": "image", "img_path": "TNQ0hxh3O1/tmp/6a7a57e6f3546e68cbd3aac8b5c302f1de7eae41cc4c2fa04153797f715ad5f6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 3: Qualitative results of DetLH over zero-shot cross-dataset object detection for common objects. Zoom in for details. Top: Detic [15]. Bottom: DetLH (Ours). ", "page_idx": 20}, {"type": "image", "img_path": "TNQ0hxh3O1/tmp/d13390aabe687aa6dc90d0a608b23f4690dc5703f553c4d20d7fbeda6869e0ea.jpg", "img_caption": ["Figure 4: Qualitative results of DetLH over zero-shot cross-dataset object detection for autonomous driving. Zoom in for details. Top: Detic [15]. Bottom: DetLH (Ours). "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "TNQ0hxh3O1/tmp/a773b5a4dd0f8f5116c27dac693931425f987eb3c4a1d85bec1b64a8ec6e9a48.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 5: Qualitative results of DetLH over zero-shot cross-dataset object detection under different weather and time-of-day conditions. Zoom in for details. Top: Detic [15]. Bottom: DetLH (Ours). ", "page_idx": 20}, {"type": "image", "img_path": "TNQ0hxh3O1/tmp/100c38f9bdd07d2fb6e2a970dc807fb053e202fd29d43311399dbabd14c76c3c.jpg", "img_caption": ["Figure 6: Qualitative results of DetLH over zero-shot cross-dataset object detection for intelligent surveillance. Zoom in for details. Top: Detic [15]. Bottom: DetLH (Ours). "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "E Broader Impacts and Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Broader Impacts. This work strives for exploiting weakly-supervised object detection (WSOD) to learn generalizable detectors by addressing the image-level label mismatch issue. We propose to incorporate imagelevel supervision with self-training for learning generalizable detectors, aiming to benefti from self-training while effectively making use of image-level weak supervision. Our proposed technique provides great advantages by avoiding the need of massive object-level annotations and allowing learning effective and generalizable detectors with image-level supervision. It thus makes a very valuable contribution to the computer vision research community by providing a novel and efficient weakly-supervised object detection method. The feature of scaling detectors with image-level labels enables effective and generalizable detectors that could work well in various downstream tasks, broadening the applicability of object detectors significantly. ", "page_idx": 20}, {"type": "image", "img_path": "TNQ0hxh3O1/tmp/28886c7752962803986b2a1dcbd756145b7965d0a743739ceaa3a465941e917f.jpg", "img_caption": ["Figure 7: Qualitative results of DetLH over zero-shot cross-dataset object detection for Wildlife Detection. Zoom in for details. Top: Detic [15]. Bottom: DetLH (Ours). "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "Limitations. As discussed in Sections 3 and 4 of the main text and Section Dataset and Implementation Details in the appendix, our proposed WSOD method adopts ImageNet-21K with image-level labels to scale up detectors. It avoids the need of massive object-level annotations and allowing learning effective and generalizable detectors with image-level supervision. At the other end, we could further scale up detector training by involving the recent image-text pair data for WSOD training, which may further improve the performance significantly. We will investigate how to involve the recent image-text pair data for WSOD training in our future work. ", "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The abstract and introduction accurately describe the paper\u2019s contributions and scope. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We discussed the limitations of the work in Section E of the Appendix ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper does not include theoretical results. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: We provided detailed instructions for reproducing the main experimental results in Section 3 Method and Section 4 Experiment including the details of the proposed framework, and the datasets, base models and the parameters used for experiments in Section Dataset and Implementation Details in the appendix. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We provided the detailed implementation details in Section Dataset and Implementation Details in the appendix. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.   \n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: We conducted the experiments with multiple runs and did not observe clear variance. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We provided sufficient information on the computation resources required for reproduce the experiments in Section Dataset and Implementation Details in the appendix. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We discussed the broader impacts of the work in Section E of the Appendix. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. \u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. ", "page_idx": 25}, {"type": "text", "text": "\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. \u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We properly credited the original owners of assets used in the paper and properly respect their license and terms of use. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]