[{"figure_path": "QGJSXMhVaL/tables/tables_1_1.jpg", "caption": "Figure 2: Qualitative comparison of our method against deep model-based RL and LLM agents (ReAct, RAP, etc: Yao et al. [77], Hao et al. [29], Zhao et al. [78], Liu et al. [41]). Sample complexity refers to the number of environment interactions needed to learn a world model (*LLM agents do not update their world model). LLM calls/task is the number of LLM calls needed to solve a new task in a fixed environment, amortized over many such tasks, as a function of the maximum episode length T. Asymptotically, after learning a world model, our method can accomplish new tasks by only at most one LLM query to update the reward function.", "description": "This table compares the proposed WorldCoder method against deep model-based reinforcement learning and large language model (LLM) agents across several key aspects.  These aspects include prior knowledge, sample complexity (number of interactions needed to learn a world model), world model representation (neural vs symbolic), inputs (high dimensional vs symbolic), and the number of LLM calls required to solve a task.  The table highlights WorldCoder's sample efficiency, compute efficiency, and knowledge transfer capabilities.", "section": "1 Introduction"}, {"figure_path": "QGJSXMhVaL/tables/tables_26_1.jpg", "caption": "Figure 2: Qualitative comparison of our method against deep model-based RL and LLM agents (ReAct, RAP, etc: Yao et al. [77], Hao et al. [29], Zhao et al. [78], Liu et al. [41]). Sample complexity refers to the number of environment interactions needed to learn a world model (*LLM agents do not update their world model). LLM calls/task is the number of LLM calls needed to solve a new task in a fixed environment, amortized over many such tasks, as a function of the maximum episode length T. Asymptotically, after learning a world model, our method can accomplish new tasks by only at most one LLM query to update the reward function.", "description": "This table compares the proposed WorldCoder approach against existing deep model-based reinforcement learning methods and large language model (LLM) agents. The comparison covers several aspects, including sample complexity (how many interactions with the environment are needed to learn a world model), the type of world model representation used (neural network vs. symbolic), the complexity of the model, and the number of LLM calls required per task.  The table highlights WorldCoder's superior sample and computational efficiency and its ability to transfer knowledge across environments.", "section": "1 Introduction"}]