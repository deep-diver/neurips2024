[{"figure_path": "1sLdprsbmk/figures/figures_1_1.jpg", "caption": "Figure 1: Pipeline for evaluating the generalization capability to combine skills. We split the language skill set S from [33] into training skills Strain and held-out skills Sheld-out, and the topic set T into training topics Ttrain and held-out topics Theld-out. The pipeline consists of three steps: (1) generate data by prompting GPT-4. The training texts contain only training skills Strain and training topics Ttrain, and each text exhibits at most 3 skills; (2) fine-tune LLaMA-2-13B-Chat and Mistral-7B-Instruct-v0.2 using the generated data; (3) evaluate the fine-tuned models on held-out skills Sheld-out and held-out topics Theld-out with the number of requested skills being as large as 5. See our detailed setups in Section 3.", "description": "This figure illustrates the three-stage pipeline used to evaluate the models' ability to generalize skill combinations.  First, a dataset is generated using GPT-4, where the generated texts exhibit combinations of up to three training skills on training topics. Then, smaller language models (LLaMA-2-13B-Chat and Mistral-7B-Instruct-v0.2) are fine-tuned using this dataset. Finally, the fine-tuned models are evaluated on their ability to combine up to five skills (including held-out skills unseen during training) on held-out topics. The evaluation measures the models' compositional generalization capabilities.", "section": "Pipeline"}, {"figure_path": "1sLdprsbmk/figures/figures_2_1.jpg", "caption": "Figure 2: The success rate of different models to compose k held-out skills in a short paragraph. (See the detailed definition of \u201cRatio of Full Marks", "description": "This figure shows the success rate of different language models in composing a short paragraph that demonstrates a given number (k) of skills. The models tested include LLaMA-2-13B-Chat, Mistral-7B-Instruct, and GPT-4.  Both the fine-tuned and original versions of LLaMA-2-13B-Chat and Mistral-7B-Instruct are included. The x-axis represents the number of skills (k) to be composed, and the y-axis shows the success rate. The results demonstrate that fine-tuning on examples of composing fewer skills (k=2,3) significantly improves the ability of smaller models to compose a larger number of held-out skills (k=4,5), indicating the models are not simply memorizing specific combinations of skills, but rather have learned a more general ability to compose skills.", "section": "4.2 Compositional generalization for out-of-domain evaluations"}, {"figure_path": "1sLdprsbmk/figures/figures_23_1.jpg", "caption": "Figure 1: Pipeline for evaluating the generalization capability to combine skills. We split the language skill set S from [33] into training skills Strain and held-out skills Sheld-out, and the topic set T into training topics Ttrain and held-out topics Theld-out. The pipeline consists of three steps: (1) generate data by prompting GPT-4. The training texts contain only training skills Strain and training topics Ttrain, and each text exhibits at most 3 skills; (2) fine-tune LLaMA-2-13B-Chat and Mistral-7B-Instruct-v0.2 using the generated data; (3) evaluate the fine-tuned models on held-out skills Sheld-out and held-out topics Theld-out with the number of requested skills being as large as 5. See our detailed setups in Section 3.", "description": "This figure illustrates the three-stage pipeline used to evaluate the compositional generalization capability of language models.  The pipeline starts by generating training data using GPT-4, ensuring each text contains a combination of up to 3 skills from a training set.  Next, smaller language models (LLaMA-2-13B-Chat and Mistral-7B-Instruct-v0.2) are fine-tuned using this data. Finally, the fine-tuned models are evaluated on their ability to compose larger combinations of skills (up to 5) from a held-out set, assessing their capacity for compositional generalization beyond what was seen during training.", "section": "3 Pipeline"}, {"figure_path": "1sLdprsbmk/figures/figures_24_1.jpg", "caption": "Figure 1: Pipeline for evaluating the generalization capability to combine skills. We split the language skill set S from [33] into training skills Strain and held-out skills Sheld-out, and the topic set T into training topics Ttrain and held-out topics Theld-out. The pipeline consists of three steps: (1) generate data by prompting GPT-4. The training texts contain only training skills Strain and training topics Ttrain, and each text exhibits at most 3 skills; (2) fine-tune LLaMA-2-13B-Chat and Mistral-7B-Instruct-v0.2 using the generated data; (3) evaluate the fine-tuned models on held-out skills Sheld-out and held-out topics Theld-out with the number of requested skills being as large as 5. See our detailed setups in Section 3.", "description": "This figure illustrates the three-stage pipeline used to evaluate the models' ability to generalize skill combinations.  The process begins by using GPT-4 to generate training data consisting of short texts that demonstrate combinations of up to three skills from a training set.  These texts are then used to fine-tune smaller language models (LLaMA-2-13B-Chat and Mistral-7B-Instruct-v0.2). Finally, the fine-tuned models are evaluated on their ability to generate texts demonstrating combinations of up to five skills, including skills not seen during training, thereby assessing compositional generalization.", "section": "3 Pipeline"}]