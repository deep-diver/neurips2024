[{"type": "text", "text": "Optimal Batched Best Arm Identification ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Tianyuan $\\mathbf{Jin}^{1}$ , Yu Yang3, Jing Tang2, Xiaokui Xiao1, Pan Xu3 ", "page_idx": 0}, {"type": "text", "text": "1National University of Singapore 2The Hong Kong University of Science and Technology (Guangzhou) 3Duke University {tianyuan,xkxiao}@nus.edu.sg, jingtang@ust.hk, {yu.yang,pan.xu}@duke.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study the batched best arm identification (BBAI) problem, where the learner\u2019s goal is to identify the best arm while switching the policy as less as possible. In particular, we aim to find the best arm with probability $1\\!-\\!\\delta$ for some small constant $\\delta>0$ while minimizing both the sample complexity (total number of arm pulls) and the batch complexity (total number of batches). We propose the three-batch best arm identification (Tri-BBAI) algorithm, which is the first batched algorithm that achieves the optimal sample complexity in the asymptotic setting (i.e., $\\delta\\rightarrow0$ ) and runs in 3 batches in expectation. Based on Tri-BBAI, we further propose the almost optimal batched best arm identification (Opt-BBAI) algorithm, which is the first algorithm that achieves the near-optimal sample and batch complexity in the non-asymptotic setting (i.e., $\\delta$ is finite), while enjoying the same batch and sample complexity as Tri-BBAI when $\\delta$ tends to zero. Moreover, in the non-asymptotic setting, the complexity of previous batch algorithms is usually conditioned on the event that the best arm is returned (with a probability of at least $1-\\delta)$ , which is potentially unbounded in cases where a sub-optimal arm is returned. In contrast, the complexity of Opt-BBAI does not rely on such an event. This is achieved through a novel procedure that we design for checking whether the best arm is eliminated, which is of independent interest. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Multi-armed bandit (MAB) is a fundamental model in various online decision-making problems, including medical trials [40], online advertisement [5], and crowdsourcing [43]. These problems typically involve a bandit with multiple arms, where each arm follows an unknown distribution with a mean value. At each time step, the learner selects an arm, and receives a reward sample drawn from the chosen arm\u2019s distribution. Best arm identification (BAI) aims to identify the arm with the highest mean reward, which can be approached with a fixed budget or a fixed confidence level [14, 4, 19]. ", "page_idx": 0}, {"type": "text", "text": "In this paper, we assume that there is a unique best arm and study BAI with fixed confidence. Specifically, we consider a set $[n]=\\{1,2,\\cdot\\cdot\\cdot,n\\}$ of $n$ arms, where each arm $i$ is associated with a reward distribution having a mean value $\\mu_{i}$ . Without loss of generality, for a bandit instance denoted by $\\pmb{\\mu}=\\{\\mu_{1},\\mu_{2},\\ldots,\\mu_{n}\\}$ , we assume that $\\mu_{1}>\\mu_{2}\\geq\\dots\\geq\\mu_{n}$ . At each time step $t$ , the learner selects an arm and observes a sample drawn independently from the chosen arm\u2019s distribution. In the fixed confidence setting, the learner aims to correctly identify the best arm (arm 1 in our context) with a probability of at least $1-\\delta$ , where $\\delta>0$ is a pre-specified confidence parameter. Meanwhile, the learner seeks to minimize the total number of arm pulls, also known as the sample complexity. ", "page_idx": 0}, {"type": "text", "text": "Denote by $a^{*}(\\pmb{\\lambda})\\,=\\,\\arg\\operatorname*{max}_{i}\\,\\lambda_{i}$ the best arm for an arbitrary bandit instance $\\lambda$ . Let $\\operatorname{Alt}(\\mu)\\,=$ $\\{\\lambda\\colon a^{*}(\\lambda)\\neq1\\}$ be a set of models that have a different best arm from the model $\\pmb{\\mu}$ , and $\\mathcal{P}_{k}=\\{\\pmb{w}\\in$ $\\begin{array}{r}{\\mathbb{R}_{+}^{n}\\colon\\sum_{i=1}^{n}w_{i}=1\\}}\\end{array}$ be the probability simplex. Garivier and Kaufmann [17] showed that for bandits with  reward distributions that are continuously parameterized by their means, the number $N_{\\delta}$ of pulls by any algorithm that returns the best arm with a probability of at least $1-\\delta$ is bounded by ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\delta\\to0}\\operatorname*{inf}_{\\textstyle\\log(1/\\delta)}\\geq T^{*}(\\pmb{\\mu}),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $T^{*}({\\pmb{\\mu}})$ is defined according to ", "page_idx": 1}, {"type": "equation", "text": "$$\nT^{*}(\\mu)^{-1}:=\\operatorname*{sup}_{w\\in\\mathcal{P}_{k}}\\left(\\operatorname*{inf}_{\\lambda\\in\\mathrm{Alt}(\\mu)}\\left(\\sum_{i=1}^{n}w_{i}\\cdot d(\\mu_{i},\\lambda_{i})\\right)\\right),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "and $d(\\mu_{i},\\lambda_{i})$ is the Kullback-Leibler (KL) divergence of two arms\u2019 distributions with means $\\mu_{i}$ and $\\lambda_{i}$ , respectively. We say that an algorithm achieves the asymptotically optimal sample complexity if it satisfies ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\delta\\to0}\\operatorname*{lim}_{\\log(1/\\delta)}\\leq T^{*}(\\pmb{\\mu}).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "The well-known Track-and-Stop algorithm [17] solves the BAI problem with asymptotically optimal sample complexity. However, it is a fully sequential algorithm, which is hard to be implemented in parallel. The learner in such an algorithm receives immediate feedback for each arm pull, and adjusts the strategy for the next arm selection based on the previous observations. Unfortunately, this sequential approach may not be feasible in many real-world applications. For instance, in medical trials, there is typically a waiting time before the efficacy of drugs becomes observable, making it impossible to conduct all tests sequentially. Instead, the learner needs to group the drugs into batches and test them in parallel. Similarly, in crowdsourcing, the goal is to identify the most reliable worker for a specific task by testing candidates with a sequence of questions. Again, there is often a waiting time for workers to finish all the questions, necessitating the grouping of workers into batches and conducting parallel tests. In such scenarios, the results of pulling arms are only available at the end of each batch [22, 1, 39, 37, 15, 24, 25, 38]. ", "page_idx": 1}, {"type": "text", "text": "Motivated by these applications, we study the problem of batched best arm identification (BBAI). In BBAI, we are allowed to pull multiple arms in a single batch, but the results of these pulls are revealed only after the completion of the batch. The objective is to output the best arm with a high probability of at least $1-\\delta$ , while minimizing both the sample complexity (total number of pulls) and the batch complexity (total number of batches). This leads to the following natural question: ", "page_idx": 1}, {"type": "text", "text": "Can we solve the BBAI problem with an asymptotically optimal sample complexity and only using a constant number of batches? ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Furthermore, the aforementioned results hold only in the limit as the confidence parameter $\\delta$ approaches zero, which may provide limited practical guidance since we often specify a fixed confidence level parameter $\\delta>0$ . To address this, some algorithms [31, 21] have been proposed to solve the BAI problem with finite confidence. Specifically, for some universal constant $C$ , these algorithms satisfy that with probability $1-\\delta$ , ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathbb{E}[N_{\\delta}]\\le O\\biggl(\\sum_{i>1}\\frac{1}{\\Delta_{i}^{2}}\\log\\log\\Delta_{i}^{-1}\\biggr)\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\Delta_{i}=\\mu_{1}-\\mu_{i}$ . In addition, Jamieson et al. [21] demonstrated that for two-armed bandits, the term log lo\u2206g22 \u22062\u22121 is optimal as \u22062 \u21920, where \u22062 is assumed to be the gap between the best arm and the second best arm. In the context of the batched setting, Jin et al. [22] proposed an algorithm that achieves the sample complexity in (1.4) within ${\\mathcal{O}}(\\log^{*}(n)\\cdot\\log(1/\\Delta_{2}))$ batches, where $\\log^{*}(n)$ is the iterated logarithm function1. Furthermore, Tao et al. [39] proved that for certain bandit instances, any algorithm that achieves the sample complexity bound shown in (1.4) requires at least \u2126 lolgo lgo g\u2206 2\u2206\u22121 batches. It should be noted that the batched lower bound proposed by Tao et al. [39] assumes $\\delta$ as a constant, making it inapplicable in the asymptotic setting. Therefore, an additional question that arises is: ", "page_idx": 1}, {"type": "table", "img_path": "ATSPPGEmAA/tmp/d5e7018bb37e99c49d3dcb340d070961533eb56ec5bf60e986d444772e3557cf.jpg", "table_caption": ["Table 1: Comparison of sample and batch complexity of different algorithms. In the asymptotic setting (i.e., $\\delta\\rightarrow0$ ), the sample complexity of an algorithm is optimal if it satisfies the definition in (1.3). The field marked with \u201c\u2013\u201d indicates that the result is not provided. The sample complexity presented for [31, 22] is conditioned on the event that the algorithm returns the best arm, which can be unbounded when it returns a sub-optimal arm with certain (non-zero) probability (see Remark 4.4 for more details). In contrast, the sample complexity presented for [17, 26] and our algorithms is the total expected number of pulls that will be executed. "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "In this work, we provide positive answers to both of the aforementioned questions. Specifically, the main contributions of our paper can be summarized as follows: ", "page_idx": 2}, {"type": "text", "text": "\u2022 We propose Tri-BBAI (Algorithm 1) that returns the best arm with a probability of at least $1-\\delta$ by pulling all arms for a total number of at most $T^{*}(\\mu)\\log(1/\\delta)$ times when $\\delta\\rightarrow0$ . Tri-BBAI employs three batches in expectation when $\\delta\\rightarrow0$ . Therefore, Tri-BBAI achieves the optimal sample within constant batches. As a comparison, Track-and-Stop [17] also achieves the optimal sample complexity but requires solving the right-hand side of (1.2) after each arm pull, resulting in a batch complexity of the same order as the sample complexity, which is a significant computational overhead in practice. ", "page_idx": 2}, {"type": "text", "text": "\u2022 Built upon Tri-BBAI, we further propose Opt-BBAI (Algorithm 2) that runs in ${\\mathcal O}(\\log(1/\\Delta_{2}))$ expected batches and pulls at most $\\begin{array}{r}{\\mathcal{O}\\big(\\sum_{i>1}\\Delta_{i}^{-2}\\log(n\\log\\Delta_{i}^{-1})\\big)}\\end{array}$ expected number of arms for finite confidence $\\delta$ . It is also important to note that Opt-BBAI achieves the same sample and expected batch complexity as Tri-BBAI asymptotically when $\\delta\\rightarrow0$ . Moreover, for the finite confidence case, this sample complexity matches (1.4) within a $\\log(\\cdot)$ factor and matches the optimal batched complexity within a $\\log\\log(\\cdot)$ factor. To the best of our knowledge, Opt-BBAI is the first batched bandit algorithm that can achieve the optimal asymptotic sample complexity and the near-optimal non-asymptotic2 sample complexity adaptively based on the assumption on $\\delta$ . ", "page_idx": 2}, {"type": "text", "text": "\u2022 Notably, in the non-asymptotic setting, the complexity of earlier batch algorithms [22, 32, 39, 24] typically depends on the event of returning the best arm, which occurs with a probability of at least $1-\\delta$ . However, this complexity could potentially become unbounded if a sub-optimal arm is returned instead. Unlike these algorithms, the complexity of Opt-BBAI is not contingent on such an event. This is made by employing an innovative procedure to verify if the best arm is eliminated, a method that holds its independent interest. To the best of our knowledge, Opt-BBAI is the first algorithm that achieves the optimal asymptotic sample complexity while providing the optimal non-asymptotic sample complexity within logarithm factors. ", "page_idx": 2}, {"type": "text", "text": "\u2022 We also conduct numerical experiments3 to compare our proposed algorithms with the optimal sequential algorithm Track-and-Stop [17], and the batched algorithm Top-k $\\delta$ -Elimination [22] on various problem instances. The results indicate that our algorithm significantly outperforms the Track and Stop method in terms of batch complexity, while its sample complexity is not much worse than that of Track and Stop. Additionally, our algorithm demonstrates a notable improvement in sample complexity compared to [22], while exhibiting similar batch complexity. ", "page_idx": 2}, {"type": "text", "text": "For ease of reference, we compare our results with existing work on batched bandits in Table 1. ", "page_idx": 2}, {"type": "table", "img_path": "ATSPPGEmAA/tmp/e57222599727301d1e875f1e71b216015d6d1098ebd67e7784dd08f5a2eb8944.jpg", "table_caption": ["Table 2: Comparison of sample complexity of different algorithms. "], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The BAI problem with fixed confidence is first studied for $[0,1]$ bounded rewards by Even-Dar et al. [14]. The sample complexity of their algorithm scales with the sum of the squared inverse gap, i.e., $\\begin{array}{r}{\\bar{H}(\\bar{\\pmb{\\mu}})=\\sum_{i>1}\\bar{1}/\\Delta_{i}^{2}}\\end{array}$ . Garivier and Kaufmann [17] showed that $H(\\pmb\\mu)<\\bar{T}^{*}(\\pmb\\mu)\\leq2H(\\pmb\\mu)$ with $T^{*}(\\pmb{\\mu})$ d efined in (1.2) and proposed the Track-and-Stop algorithm, which is the first one in the literature proved to be asymptotically optimal. Later, Degenne et al. [12] viewed $T^{*}(\\pmb{\\mu})$ as a min-max game and provided an efficient algorithm to solve it. Jourdan et al. [27] studied the asymptotically optimal sample complexity for any reward distributions with bounded support. Degenne et al. [13] studied pure exploration in linear bandits. Their proposed algorithm proved to be both asymptotically optimal and empirically efficient. ", "page_idx": 3}, {"type": "text", "text": "There are also many studies [30, 10, 8, 21, 31] that focus on providing non-asymptotic optimal sample complexity. The best non-asymptotic sample complexity was achieved by Chen et al. [10], which replaces the term $\\log\\log\\Delta_{i}^{-1}$ in (1.4) with a much smaller term. Furthermore, when we allow a loss $\\epsilon$ of the quality of the returned arm, the problem is known as $(\\epsilon,\\delta)$ -PAC BAI, for which various algorithms [29, 14, 6, 9] are proposed, achieving the worst-case optimal sample complexity. ", "page_idx": 3}, {"type": "text", "text": "There are a few attempts to achieve both the asymptotic and non-asymptotic optimal sample complexity. Degenne et al. [12] provided a non-asymptotic sample complexity $\\tilde{O}(n T^{*}(\\mu)^{2})$ , which could be $n T^{*}(\\pmb{\\mu})$ larger than the optimal sample complexity. Recently, Jourdan and Degenne [26] managed to achieve a sample complexity that is $\\beta$ -asymptotically optimal (with $w_{1}$ fixed at $1/\\beta$ in (1.2)), rendering it asymptotically optimal up to a factor of $1/\\beta$ . Meanwhile, they also reached a non-asymptotic sample complexity of $\\mathcal{O}\\big((H(\\pmb{\\mu})\\cdot\\log H(\\pmb{\\mu})\\big)^{\\alpha}\\big)^{4}$ for some $\\alpha\\,>\\,1$ , where $\\begin{array}{r}{H(\\pmb{\\mu})\\,=\\,\\sum_{i>1}1/\\Delta_{i}^{2}}\\end{array}$ . Wang et al. [42] explored both asymptotic and non-asymptotic sample complexities. Th eir algorithm achieves asymptotic optimality and shows a non-asymptotic sample complexity of $O(n H(\\mu)^{4}/w_{\\mathrm{min}}^{2})$ However, this non-asymptotic sample complexity is $\\dot{\\bar{n}}H(\\pmb{\\mu})^{3}/\\dot{w}_{\\mathrm{min}}^{2}$ away from being optimal. Jourdan et al. [28] studied $(\\epsilon,\\delta)$ -PAC BAI, proposing an asymptotically optimal algorithm and providing non-asymptotic sample complexity. When $\\epsilon\\,=\\,0$ , it aligns with our setting, our non-asymptotic sample complexity is better scaled. Specifically, Jourdan et al. [28] offered a non-asymptotic sample complexity scale of $n/\\Delta_{2}^{2}\\log(1/\\dot{\\Delta}_{2})$ , whereas ours is more instance-sensitive, as our sample complexity is related to all gaps, not just $\\Delta_{2}$ . Additionally, Jourdan et al. [28] considered a practical scenario where the algorithm can return a result at any time while still ensuring a good guarantee on the returned arm. For ease of reference, we summarize the sample complexity of different algorithms in Table 2. As shown in Table 2, our algorithm is the only one that achieves both the asymptotic optimality and non-asymptotic optimality within logarithm factors. ", "page_idx": 3}, {"type": "text", "text": "Another line of research [31, 4, 7] investigated BAI with a fixed budget, where the objective is to determine the best arm with the highest probability within $T$ pulls. Audibert et al. [4] and Karnin et al. [31] offered finite-time bounds for this problem, while Carpentier and Locatelli [7] presented a tight lower bound, demonstrating that such finite-time bounds[4, 31] are optimal for certain bandit instances. However, the asymptotic optimality for this problem remains unknown. Interested readers are referred to recent advancements[11, 35] in the asymptotic results of BAI with a fixed budget. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "In addition, some recent works [1, 22, 39, 32] also focused on batched BAI in non-asymptotic setting. Agarwal et al. [1] studied the batched BAI problem under the assumption that $\\Delta_{2}$ is known. They proposed an algorithm that has the worst-case optimal sample complexity of $O\\big(n\\Delta_{2}^{-2}\\big)$ and runs in $\\log^{*}(n)$ batches. Later, Jin et al. [22] provided the algorithms that achieves the sample complexity given in (1.4) within $\\tilde{\\mathcal{O}}(\\log(1/\\Delta_{2}))$ batches, where $\\tilde{O}$ hides the $\\log\\log(\\cdot)$ factors. Tao et al. [39] studied the BAI problem in the general collaborative setting and showed that no algorithm can achieve (1.4) with $o((\\log\\Delta_{2}^{-1})/\\log\\log\\Delta_{2}^{-1})$ batches. Karpov et al. [32] further proposed an algorithm which has the sample complexity i>1 $\\bar{\\sum}_{i>1}\\frac{\\log(n\\log\\Delta^{-1})}{\\Delta_{i}^{2}}$ and the batch complexity ${\\cal O}(\\log(1/\\Delta_{2}))$ . We note that the lower bound of batch complexity given by Tao et al. [39] can only be applied to a constant $\\delta$ . In other words, the lower bound of complexity for the asymptotic setting remains unknown. Agrawal et al. [2] studied the optimal batch size for keeping the asymptotic optimality. They showed an algorithm with batch size $m=o(\\log(1/\\delta))$ achieves the asymptotic optimality. The batch complexity of the algorithm is $O(T^{*}(\\mu)\\log(1/\\delta)/m)$ . Wang et al. [42] provided an algorithm that uses $O(\\sqrt{\\log\\delta^{-1}})$ batches and retains asymptotic optimality for linear bandits. However, for the asymptotic setting, such batch size is still too large as it grows to infinity as $\\delta$ decreases to 0. ", "page_idx": 4}, {"type": "text", "text": "3 Achieving Asymptotic Optimality with at Most Three Batches ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "3.1 Reward Distribution ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We assume the reward distributions belong to a known one-parameter exponential family that is commonly considered in the literature [17, 16, 36]. In particular, the measure $\\nu_{\\theta}$ of such probability distributions with respect the model parameter $\\theta$ satisfies $\\begin{array}{r}{{\\frac{\\mathrm{d}\\nu_{\\theta}}{\\mathrm{d}\\rho}}(x)\\,=\\,\\exp(x\\theta\\,-\\,b(\\theta))}\\end{array}$ , for some measure $\\rho$ and $\\begin{array}{r}{b(\\theta)=\\log(\\int e^{x\\theta}\\mathrm{d}\\rho(x))}\\end{array}$ . For one-parameter exponential distributions, it is known that $b^{\\prime}(\\theta)=\\mathbb{E}[\\nu_{\\theta}]$ and the mapping $b^{\\prime}(\\theta)\\mapsto\\theta$ is one-to-one. Moreover, given any two mean values $\\mu$ and $\\mu^{\\prime}$ , we define $d(\\mu,\\mu^{\\prime})$ to be the Kullback-Leibler divergence between two distributions with mean values $\\mu$ and $\\mu^{\\prime}$ . ", "page_idx": 4}, {"type": "text", "text": "3.2 The Proposed Three-Batch Algorithm ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Algorithm 1 shows the pseudo-code of our proposed Tri-BBAI algorithm. In particular, Tri-BBAI has four stages. In what follows, we elaborate on the details of each stage. ", "page_idx": 4}, {"type": "text", "text": "Stage I: Initial exploration. In this stage, we pull each arm for $L_{1}$ times. Denote by $i^{*}(t)$ the arm with the largest empirical mean at time $t$ (i.e., after we pull all arms for a total number of $t$ times), i.e., $i^{*}(t)=\\operatorname*{max}_{i\\in[n]}\\bar{\\hat{\\mu}_{i}}(t)$ . Let $\\tau_{0}=n L_{1}$ . Fix $t=\\tau_{q-1}\\geq\\tau_{0}$ , we let $b_{i}^{q}=\\hat{\\mu}_{i}(t)+\\epsilon$ for $i\\neq i^{*}(t)$ and $b_{i}^{q}=\\hat{\\mu}_{i}(t)-\\epsilon$ for $i=i^{*}(t)$ . Let w\u2217(\u00b5) = arg maxw\u2208Pk inf\u03bb\u2208Alt(\u00b5) $(\\textstyle\\sum_{i=1}^{n}w_{i}\\cdot d(\\mu_{i},\\lambda_{i}))$ . Then, for the aforementioned $\\pmb{b}^{q}=\\{b_{1}^{q},b_{2}^{q},\\cdot\\cdot\\cdot,b_{n}^{q}\\}$ , we calculate $w^{*}(b^{q})$ according to Lemma A.1 and $T^{*}(b^{q})$ according to Lemma A.2. We note that arm 1 is assumed to be the arm with the highest mean in these two lemmas. However, in the context of $^{b^{q}}$ , the index of $i^{*}(t)$ might be different. To align with the standard practice, we can rearrange the indices of the arms in $^{b^{q}}$ so that $i^{*}(t)$ corresponds to index 1. ", "page_idx": 4}, {"type": "text", "text": "Purpose. To achieve the asymptotic optimality, we attempt to pull each arm $i$ for around $w_{i}^{*}(\\pmb{\\mu})T^{*}(\\pmb{\\mu})$ times. We can show that with a high probability, $w_{i}^{*}(\\bar{b}^{q})\\bar{T^{*}}(b^{q})$ is close to $w_{i}^{*}(\\pmb{\\mu})T^{*}\\bar{(\\pmb{\\mu})}$ , which implies that pulling arm $i$ for a number of times proportional to $w_{i}^{*}(b^{q})T^{*}(b^{q})$ is likely to ensure the asymptotic optimal sample complexity. ", "page_idx": 4}, {"type": "text", "text": "Stage II: Exploration using $w^{*}(b^{q})$ and $T^{*}(b^{q})$ . Stage $\\mathrm{II}$ operates in batches with the maximum number of batches determined by $\\log(1/\\delta)$ . At batch $q$ , each arm $i$ is pulled $\\scriptstyle\\operatorname*{max}_{p:p\\in\\mathbb{N},p\\in[1,q]}T_{i}^{q}$ times in total. Here ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T_{i}^{q}:=\\operatorname*{min}\\big\\{\\alpha w_{i}^{*}(\\pmb{b}^{q})T^{*}(\\pmb{b}^{q})\\log\\delta^{-1},L_{2}\\big\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the definition of $w_{i}^{*}(b^{q})$ and $T^{*}(b^{q})$ coul\u221ad be found in Stage I. We then evaluate the stage switching condition, $|w_{i}^{*}(\\dot{\\pmb{b}}^{q})-w_{i}^{*}({\\pmb{b}}^{q-1})|\\leq1/\\sqrt{n}$ for all $i\\in[n]$ . If this condition is met, we go to the next stage; otherwise, we proceed to the next batch within Stage $\\mathrm{II}$ . ", "page_idx": 4}, {"type": "text", "text": "Input: Parameters $\\epsilon,\\delta,L_{1},L_{2},L_{3},\\alpha$ and function $\\beta(t,\\delta)$ .   \nOutput: The estimated optimal arm.   \n1 Stage I: Round 1 exploration   \n2 for $i\\gets1$ to $n$ do   \n3 Pull arm $i$ for $L_{1}$ times;   \n4 $t\\leftarrow n L_{1}$ and $\\tau_{0}\\gets t$ ;   \n5 Stage II:Round 2 exploration   \n6 Let $w_{0}^{*}(b^{0})=(1/n,1/n,\\cdot\\cdot\\cdot\\,,1/n)$ and $T_{i}^{0}=L_{1}$ for all $i\\in[n]$ ;   \n7 for $q=1,2\\cdot\\cdot\\cdot$ , $\\log(1/\\delta)$ do   \n8 $i^{*}(t)\\gets\\operatorname*{max}_{i\\in[n]}\\hat{\\mu}_{i}(t)$ ;   \n9 for each $i\\in[n]\\,\\dot{\\setminus}\\,\\{i^{*}(t)\\}$ do   \n10 $\\Big\\lfloor\\begin{array}{r l r}{b_{i}^{q}\\gets\\hat{\\mu}_{i}(t)+\\epsilon}\\end{array}$ ;   \n11 $b_{i^{*}(t)}^{q}\\gets\\hat{\\mu}_{i^{*}(t)}(t)-\\epsilon;$   \n12 for $i\\gets1$ to n do   \n13 Pull arm $i$ for $\\{0,T_{i}^{q}-\\operatorname*{max}_{p:p\\in\\mathbb{N},p\\in[0,q)}T_{i}^{p}\\}$ times;   \n/\\*\\*/ Note that $T_{i}^{q}=\\operatorname*{min}\\left\\{\\alpha w_{i}^{*}(b^{q})T^{*}(b^{q})\\log\\delta^{-1},L_{2}\\right\\}$ by (3.1)   \n14 $t\\gets t+\\{0,T_{i}^{q}-\\operatorname*{max}_{p:p\\in\\mathbb{N},p\\in[0,q)}T_{i}^{p}\\}$ ;   \n15 if $|w_{i}^{*}(\\pmb{b}^{q})-w_{i}^{*}(\\pmb{b}^{q-1})|\\leq1/\\sqrt{n}$ for all $i\\in[n]$ then   \n16 Break;   \n17 $\\tau_{q}\\gets t$ ;   \n18 $\\tau\\gets t$ , and $i^{*}(\\tau)=\\mathrm{max}_{i\\in[n]}\\,\\hat{\\mu}_{i}(\\tau)$ ;   \n19 Stage III: Statistical test with Chernoff\u2019s stopping rule;   \n20 Compute $Z_{j}(\\tau)$ according to (3.3) ;   \n21 if $\\begin{array}{r}{\\operatorname*{min}_{j\\in[n]\\setminus\\{i^{*}(\\tau)\\}}Z_{j}(\\tau)\\geq\\beta(\\tau,\\delta/2)}\\end{array}$ then   \n22 return $i^{*}(\\tau)$ ;   \n23 Stage IV: Round 3 exploration for $i\\gets1$ to $n$ do   \n24 Pull arm $i$ for total $\\operatorname*{max}\\{0,L_{3}-L_{1}-T_{i}\\}$ times;   \n25 $t\\leftarrow n L_{3}$ , and $i^{*}(t)\\gets\\operatorname*{max}_{i\\in[n]}\\hat{\\mu}_{i}(t)$ ;   \n26 return $i^{*}(t)$ ; ", "page_idx": 5}, {"type": "text", "text": "Purpose. Pulling arm $i$ proportional to $w_{i}^{*}(b^{q})T^{*}(b^{q})$ provides statistical evidence for the reward distributions without sacrificing sample complexity compared to the optimality per our above discussion. Meanwhile, we also set a threshold $L_{2}$ to avoid over-exploration due to sampling errors from Stage I. ", "page_idx": 5}, {"type": "text", "text": "The rationale for running Stage $\\mathrm{II}$ in multiple batches is based on empirical considerations. In experiments, $\\delta$ is always finite. Consequently, the error of arm $i$ , $\\lvert\\hat{\\mu}_{i}(t)-\\bar{\\mu}_{i}\\rvert$ , remains constant since the number of pulls of arms is limited. Given that the stopping rule in Stage III is highly dependent on the error of arm $i$ and the number of pulls $T_{i}:=\\operatorname*{max}_{p:p\\in\\mathbb{N},p\\geq1}T_{i}^{p}$ , there is a constant probability that the stopping rule may not be met, leading to significant sample costs in Stage IV. Adding the condition in Line 15 ensures that $w^{*}(b^{q})$ converges and that the sample size $T_{i}^{q}$ , as defined by $w^{*}(b^{q})$ and $T^{*}(b^{q})$ , closely approximates $\\alpha\\dot{w}^{*}(\\pmb{\\mu})T^{*}(\\bar{\\pmb{\\mu}})\\log\\delta^{-1}$ . This alignment significantly increases the probability that the stopping rule will be satisfied in experiments. ", "page_idx": 5}, {"type": "text", "text": "Moreover, such modification doesn\u2019t hurt any theoretical results. To explain, in our analysis for Tri-BBAI, we demonstrate that as $\\delta$ approaches 0, $w^{*}(b^{q})$ will be very close to $w^{\\ast}(\\pmb{\\mu})$ and the probability that Line 15 is not satisfied could be bounded by $1/\\log^{2}\\delta^{-1}$ , which means with high probability Stage II costs 2 batches. Besides, $q\\leq\\log(1/\\delta)$ , which implies that even if Line 15 is not satisfied, the number of batches required for Stage $\\mathrm{II}$ is at most $\\log(1/\\delta)$ . Therefore, the expected number of batches required for Stage $\\mathrm{II}$ is 2 as $\\delta$ approaches 0. ", "page_idx": 5}, {"type": "text", "text": "Stage III: Statistical test with Chernoff\u2019s stopping rule. Denote by $N_{i}(t)$ the number of pulls of arm $i$ at time $t$ . For each pair of arms $i$ and $j$ , define their weighted empirical mean as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\hat{\\mu}_{i j}(t):=\\frac{N_{i}(t)\\cdot\\hat{\\mu}_{i}(t)}{N_{i}(t)+N_{j}(t)}+\\frac{N_{j}(t)\\cdot\\hat{\\mu}_{j}(t)}{N_{i}(t)+N_{j}(t)},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\hat{\\mu}_{i}(t)$ and $\\hat{\\mu}_{j}(t)$ are the empirical means of arms $i$ and $j$ at time $t$ . For $\\hat{\\mu}_{i}(t)\\geq\\hat{\\mu}_{j}(t)$ , define ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Z_{i j}(t):=d(\\hat{\\mu}_{i}(t),\\hat{\\mu}_{i j}(t))N_{i}(t)+d(\\hat{\\mu}_{j}(t),\\hat{\\mu}_{i j}(t))N_{j}(t),}\\\\ &{\\;\\;Z_{j}(t):=Z_{i^{*}(t)j}(t).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We test whether Chernoff\u2019s stopping condition is met (Line 21). If so, we return the arm with the largest empirical mean, i.e., $i^{*}(\\bar{\\tau})$ , where $\\tau$ is the total number of pulls examined after Stage II. ", "page_idx": 6}, {"type": "text", "text": "Purpose. The intuition of using Chernoff\u2019s stopping rule for the statistical test is two-fold. Firstly, if Chernoff\u2019s stopping condition is met, with a probability of at least $1-\\delta/2$ , the returned arm $i^{*}(\\tau)$ in Line 22 is the optimal arm (see Lemma B.1). Secondly, when $\\delta$ is sufficiently small, with high probability, Chernoff\u2019s stopping condition holds (see Lemma B.4). As a consequence, our algorithm identifies the best arm successfully with a high probability of meeting the requirement. ", "page_idx": 6}, {"type": "text", "text": "Stage IV: Re-exploration. If the previous Chernoff\u2019s stopping condition is not met, we pull each arm until the total number of pulls of each arm eqauls $L_{3}$ taking into account the pulls in the previous stages (Line 24). Finally, the arm with the largest empirical mean is returned (Line 26). ", "page_idx": 6}, {"type": "text", "text": "Purpose. If Chernoff\u2019s stopping condition is not met, $i^{*}(\\tau)$ might not be the optimal arm. In addition, when each arm is pulled for $L_{3}$ times, we are sufficiently confident that $i^{*}(t)$ is the best arm. Since the probability of Stage IV happening is very small, its impact on the sample complexity is negligible. ", "page_idx": 6}, {"type": "text", "text": "3.3 Theoretical Guarantees of Tri-BBAI ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In the following, we present the theoretical results for Algorithm 1. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.1 (Asymptotic Sample Complexity). Given any $\\delta\\ >\\ 0$ , let $\\begin{array}{r}{\\epsilon\\;=\\;\\frac{1}{\\log\\log(\\delta^{-1})}}\\end{array}$ , $L_{1}\\,=$ log \u03b4\u22121, L2 = log \u03b4\u22121 log log \u03b4\u22121, and $L_{3}=(\\log\\delta^{-1})^{2}$ . Meanwhile, for any given $\\alpha\\in(1,e/2]$ , define function $\\beta(t,\\delta)$ as $\\beta(t,\\delta)=\\log(\\log(1/\\delta)t^{\\alpha}/\\delta)$ .5 Then, for any bandit instance $\\pmb{\\mu}$ , Algorithm 1 satisfies ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\delta\\to0}\\frac{\\mathbb{E}_{\\pmb{\\mu}}[N_{\\delta}]}{\\log(1/\\delta)}\\leq\\alpha T^{*}(\\pmb{\\mu}).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "By letting $\\alpha$ in Theorem 3.1 approach 1 (e.g., $\\alpha=1+1/\\log\\delta^{-1})$ , we obtain the asymptotic optimal sample complexity. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.2 (Correctness). Let $\\epsilon$ , $L_{1}$ , $L_{2},L_{3},\\,$ $\\alpha$ , and $\\beta(t,\\delta)$ be the same as in Theorem 3.1. Then, for sufficiently small $\\delta>0$ , Algorithm 1 satisfies $\\mathbb{P}_{\\mu}(i^{*}(N_{\\delta})\\neq1)\\leq\\delta$ . ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.3 (Asumptotic Batch Complexity). Let $\\epsilon$ $\\epsilon,L_{1},L_{2},L_{3},\\alpha,$ and $\\beta(t,\\delta)$ be the same as in Theorem 3.1. For sufficiently small $\\delta>0$ , Algorithm 1 runs within $3+o(1)$ batches in expectation. Besides, Algorithm 1 runs within 3 batches with probability $1-1/\\log(1/\\delta^{2})$ . ", "page_idx": 6}, {"type": "text", "text": "To the best of our knowledge, all previous works in the BAI literature [18, 12, 2, 42] that achieve the asymptotic optimal sample complexity require unbounded batches as $\\delta\\rightarrow0$ . In contrast, Tri-BBAI achieves the asymptotic optimal sample complexity and runs within 3 batches in expectation, which is a significant improvement in the batched bandit setting where switching to new policies is expensive. ", "page_idx": 6}, {"type": "text", "text": "Remark 3.4. Apart from best arm identification, regret minimization is another popular task in bandits, where the aim is to maximize the total reward in $T$ rounds. Jin et al. [25] proposed an algorithm that achieves a constant batch complexity for regret minimization and showed that their algorithm is optimal when $T$ goes to infinity. In regret minimization, the cost of pulling the optimal arm is 0, indicating that the allocation $w_{i}$ (i.e., the proportion of pulling the optimal arm) is close to 1. In the BAI problem, the main hardness is to find the allocation $w_{i}$ for each arm since even pulling arm 1 will increase the sample complexity of the algorithm. Therefore, the strategy proposed by Jin et al. [25] cannot be applied to the BAI problem. ", "page_idx": 6}, {"type": "image", "img_path": "ATSPPGEmAA/tmp/9eaeea5f803da8a37dfa9dc0ce6e39d107acda2e050c663bffb4496919d451a9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "4 Best of Both Worlds: Achieving Asymptotic and Non-asymptotic Optimalities ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The Tri-BBAI algorithm is shown to enjoy the optimal sample complexity with only three batches in the asymptotic setting. However, in practice, we are limited to a finite number of samples and thus $\\delta$ cannot go to zero, which is a critical concern in real-world applications. Consequently, obtaining the optimal sample and batch complexity in a non-asymptotic setting becomes the ultimate objective of a practical bandit strategy in BBAI. In this section, we introduce Opt-BBAI, which can attain the optimal sample and batch complexity in asymptotic settings and near-optimal sample and batch complexity in non-asymptotic settings. ", "page_idx": 7}, {"type": "text", "text": "We assume a bounded reward distribution within [0, 1], which aligns with the same setting in the literature [31, 21]. Again, we consider that the reward distribution belongs to a one-parameter exponential family. By refining Stage IV of Algorithm 1, we can achieve asymptotic optimality and near non-asymptotic optimality adaptively based on the assumption on $\\delta$ in various settings. ", "page_idx": 7}, {"type": "text", "text": "The pseudo-code for the algorithm is provided in Algorithm 2. The main modification from Algorithm 1 occurs in Stage IV. Intuitively, if the algorithm cannot return at Stage III, then the value of $\\,{\\breve{\\log\\delta}}^{-1}$ may be comparable to other problem parameters, such as $1/\\Delta_{2}$ . Therefore, we aim to achieve the best possible sample and batch complexity for the non-asymptotic scenario. Stage IV operates in rounds, progressively eliminating sub-optimal arms until a result is obtained. Each round consists of two components: Successive Elimination and Checking for Best Arm Elimination. ", "page_idx": 7}, {"type": "text", "text": "Successive Elimination. In the $r$ -th round, we maintain a set $S_{r}$ , a potential set for the best arm. Each arm in $S_{r}$ is then pulled $d_{r}=32/\\epsilon_{r}^{2}\\log(2/\\delta_{r})$ times. At Line 11, all possible sub-optimal arms are eliminated. This first component of Stage IV borrows its idea from successive elimination [14]. ", "page_idx": 7}, {"type": "text", "text": "Purpose. After $d_{r}$ number of pulls, arms are likely to be concentrated on their true means within a distance of $\\epsilon_{r}/4$ with a high probability. Hence, with high probability, the best arm is never eliminated at every round of Line 11, and the final remaining arm is the best arm. ", "page_idx": 8}, {"type": "text", "text": "Issue of Best Arm Elimination. Due to successive elimination, there is a small probability $(\\le\\delta_{r})$ that the best arm will be eliminated. The following example illustrates that, conditioning on this small-probability event, the sample and batch complexity of the algorithm could become infinite. ", "page_idx": 8}, {"type": "text", "text": "Example 4.1. Consider a bandit instance where $\\mu_{1}>\\mu_{2}=\\mu_{3}>\\mu_{4}\\geq\\dots\\geq\\mu_{n}$ . If the best arm is eliminated at a certain round and never pulled again, the algorithm tasked with distinguishing between the 2nd and 3rd best arms will likely never terminate due to their equal means, leading to unbounded sample and batch complexity. ", "page_idx": 8}, {"type": "text", "text": "To address this issue, we introduce a Check for Best Arm Elimination component into Stage IV. ", "page_idx": 8}, {"type": "text", "text": "Checking for Best Arm Elimination. In the $r$ -th round, we represent the total sample complexity used up to the $r$ -th round as $B_{r}$ . We employ $\\gamma_{j}$ as an upper bound for the probability that the best arm is eliminated in $S_{j}$ . If Line 16 is true $(B_{r}\\overrightharpoon\\gamma_{j}\\cdot2^{\\ell_{j}}>B_{j})$ , we adjust $\\gamma_{j}$ to $\\gamma_{j}^{2}$ and pull each arm in $S_{j}$ for $32/\\epsilon_{j}^{2}\\log(2/\\gamma_{j})$ times (Line 18), subsequently updating their empirical mean (Line 19). Finally, we return a random arm in $S_{r}$ if the condition at Line 21 holds. ", "page_idx": 8}, {"type": "text", "text": "Purpose. If Line 16 is satisfied, it indicates that the sample costs, based on the event of the best arm being eliminated in the $r$ -th round, exceed $B_{j}/2^{\\ell_{j}}$ . In this case, we increase the number of samples for arms in $S_{j}$ and re-evaluate if their empirical mean is lower than that of the current best arm $^*$ This ensures that the expected total sample costs, assuming the best arm is eliminated at $S_{j}$ , are bounded by $\\textstyle\\sum_{\\ell_{j}=1}^{\\infty}B_{j}/2^{\\ell_{j}}\\leq B_{j}$ . If Line 21 holds, we randomly return an arm from $S_{r}$ . Since this only happens with a small probability, we still guarantee that Algorithm 2 will return the best arm with a probability of at least $1-\\delta$ . ", "page_idx": 8}, {"type": "text", "text": "In what follows, we provide the theoretical results for Algorithm 2. ", "page_idx": 8}, {"type": "text", "text": "Theorem 4.2. Let $\\epsilon.$ , $L_{1}$ $\\tau_{1},\\,L_{2},\\,L_{3},\\,\\epsilon$ \u03b1, and $\\beta(t,\\delta)$ be the same as in Theorem 3.1. For finite $\\delta\\ \\in$ $(0,1)$ , Algorithm 2 identifies the optimal arm with probability at least $1-\\delta$ and there exists some universal constant $C$ such that $\\begin{array}{r}{\\mathbb{E}[N_{\\delta}]\\leq C\\big(\\sum_{i>1}\\Delta_{i}^{-2}\\log\\big(n\\cdot\\log\\Delta_{i}^{-1}\\big)\\big)}\\end{array}$ , and the algorithm runs in $C\\log(1/\\Delta_{2})$ expected batches. ", "page_idx": 8}, {"type": "text", "text": "When $\\delta$ is allowed to go to zero, we also have the following result. ", "page_idx": 8}, {"type": "text", "text": "Theorem 4.3. Let $\\epsilon.$ $,~L_{1},~L_{2},~L_{3},~\\alpha$ , and $\\beta(t,\\delta)$ be the same as in Theorem 3.1. Algorithm 2 identifies the optimal with probability at least $1\\:-\\:\\delta$ and its sample complexity satisfies lim $\\begin{array}{r}{\\operatorname*{sup}_{\\delta\\rightarrow0}\\mathbb{E}_{\\pmb\\mu}[N_{\\delta}]/\\log(1/\\delta)\\leq\\alpha T^{*}(\\pmb\\mu)}\\end{array}$ , and the expected batch complexity of Algorithm 2 converges to 3 when $\\delta$ approaches 0. ", "page_idx": 8}, {"type": "text", "text": "The results in Theorems 4.2 and 4.3 state that Algorithm 2 achieves both the asymptotic optimal sample complexity and a constant batch complexity. Moreover, it also demonstrates near-optimal performance in both non-asymptotic sample complexity and batch complexity. Notably, this is the first algorithm that successfully attains the optimal or near-optimal sample and batch complexity, adaptively, in asymptotic and non-asymptotic settings. ", "page_idx": 8}, {"type": "text", "text": "Remark 4.4. Jin et al. [22] achieved near-optimal sample and batch complexity in a non-asymptotic setting. However, their results are contingent on the event that the algorithm can find the best arm (with probability $1-\\delta)$ ). Consequently, with a probability of $\\delta$ there is no guarantee for its batch and sample complexity to be bounded. As demonstrated in Example 4.1, the batch and sample complexity in [22] could even be infinite. ", "page_idx": 8}, {"type": "text", "text": "In contrast, the batch and sample complexity introduced in Theorem 4.2 is near-optimal and does not rely on any specific event due to the procedure \u201cchecking for best arm elimination\u201d we proposed. Our technique could be of independent interest and could be further applied to existing eliminationbased BAI algorithms [14, 21, 22, 31] to ensure that the sample and batch complexity is independent of the low-probability event that the best arm is eliminated. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion, Limitations, and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we studied the BAI problem in the batched bandit setting. We proposed a novel algorithm, Tri-BBAI, which only needs 3 batches in expectation to find the best arm with probability $1-\\delta$ and achieves the asymptotic optimal sample complexity. We further proposed Opt-BBAI and theoretically showed that Opt-BBAI has a near-optimal non-asymptotic sample and batch complexity while still maintaining the asymptotic optimality as Tri-BBAI does. ", "page_idx": 9}, {"type": "text", "text": "In our experiments, although Tri-BBAI utilizes a limited number of batches, its sample complexity does not match that of Garivier and Kaufmann [17]. Designing a batched algorithm with sample complexity comparable to Garivier and Kaufmann [17], while maintaining a constant number of batches, presents an intriguing challenge. ", "page_idx": 9}, {"type": "text", "text": "As for future work, an interesting direction is to investigate whether our \u201cchecking for best arm elimination\u201d could be beneficial to other elimination-based algorithms. Additionally, some research [3, 23] implied a strong correlation between batch complexity in batched bandit, and the memory complexity and pass complexity in streaming bandit. Thus, it could be valuable to assess if our techniques could enhance the results in the field of streaming bandit. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We would like to thank the anonymous reviewers for their helpful comments. This research is funded by a Singapore Ministry of Education AcRF Tier 2 grant (A-8000423-00-00), by the National Research Foundation, Singapore under its AI Singapore Program (AISG Award No: AISG-PhD/2021- 01004[T]), by the Ministry of Education, Singapore, under its MOE AcRF TIER 3 Grant (MOEMOET32022-0001), by National Key R&D Program of China under Grant No. 2023YFF0725100, by the National Natural Science Foundation of China (NSFC) under Grant No. 62402410 and U22B2060, by Guangdong Basic and Applied Basic Research Foundation under Grant No. 2023A1515110131, and by Guangzhou Municipal Science and Technology Bureau under Grant No. 2023A03J0667 and 2024A04J4454. ", "page_idx": 9}, {"type": "text", "text": "In particular, T. Jin was supported by a Singapore Ministry of Education AcRF Tier 2 grant (A8000423-00-00) and by the National Research Foundation, Singapore under its AI Singapore Program (AISG Award No: AISG-PhD/2021-01004[T]). X. Xiao was supported by the Ministry of Education, Singapore, under its MOE AcRF TIER 3 Grant (MOE-MOET32022-0001). J. Tang was partially supported by National Key R&D Program of China under Grant No. 2023YFF0725100, by the National Natural Science Foundation of China (NSFC) under Grant No. 62402410 and U22B2060, by Guangdong Basic and Applied Basic Research Foundation under Grant No. 2023A1515110131, and by Guangzhou Municipal Science and Technology Bureau under Grant No. 2023A03J0667 and 2024A04J4454. P. Xu was supported in part by the Whitehead Scholars Program at the Duke University School of Medicine. The views and conclusions in this paper are those of the authors and should not be interpreted as representing any funding agencies. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Arpit Agarwal, Shivani Agarwal, Sepehr Assadi, and Sanjeev Khanna. Learning with limited rounds of adaptivity: Coin tossing, multi-armed bandits, and ranking from pairwise comparisons. In Proc. COLT, pages 39\u201375, 2017. (pp. 2 and 5.)   \n[2] Shubhada Agrawal, Sandeep Juneja, and Peter Glynn. Optimal $\\delta$ -Correct Best-Arm Selection for Heavy-Tailed Distributions. In Algorithmic Learning Theory, pages 61\u2013110. PMLR, 2020. (pp. 3, 5, and 7.)   \n[3] Sepehr Assadi and Chen Wang. Exploration with limited memory: streaming algorithms for coin tossing, noisy comparisons, and multi-armed bandits. In Proceedings of the 52nd Annual ACM SIGACT Symposium on theory of computing, pages 1237\u20131250, 2020. (p. 10.)   \n[4] Jean-Yves Audibert, S\u00e9bastien Bubeck, and R\u00e9mi Munos. Best arm identification in multi-armed bandits. In COLT, pages 41\u201353, 2010. (pp. 1 and 4.)   \n[5] Dimitris Bertsimas and Adam J Mersereau. A learning approach for interactive marketing to a customer segment. Operations Research, 55(6):1120\u20131135, 2007. (p. 1.) [6] Wei Cao, Jian Li, Yufei Tao, and Zhize Li. On top-k selection in multi-armed bandits and hidden bipartite graphs. In Proc. NeurIPS, pages 1036\u20131044, 2015. (p. 4.) [7] Alexandra Carpentier and Andrea Locatelli. Tight (lower) bounds for the fixed budget best arm identification bandit problem. In Conference on Learning Theory, pages 590\u2013604. PMLR, 2016. (p. 4.) [8] Lijie Chen and Jian Li. On the optimal sample complexity for best arm identification. arXiv preprint arXiv:1511.03774, 2015. (p. 4.) [9] Lijie Chen, Anupam Gupta, and Jian Li. Pure exploration of multi-armed bandit under matroid constraints. In Proc. COLT, pages 647\u2013669, 2016. (p. 4.)   \n[10] Lijie Chen, Jian Li, and Mingda Qiao. Towards Instance Optimal Bounds for Best Arm Identification. In Proc. COLT, pages 535\u2013592, 2017. (p. 4.)   \n[11] R\u00e9my Degenne. On the Existence of a Complexity in Fixed Budget Bandit Identification. arXiv preprint arXiv:2303.09468, 2023. (p. 5.)   \n[12] R\u00e9my Degenne, Wouter M Koolen, and Pierre M\u00e9nard. Non-asymptotic pure exploration by solving games. Advances in Neural Information Processing Systems, 32, 2019. (pp. 4 and 7.)   \n[13] R\u00e9my Degenne, Pierre M\u00e9nard, Xuedong Shang, and Michal Valko. Gamification of pure exploration for linear bandits. In International Conference on Machine Learning, pages 2432\u2013 2442. PMLR, 2020. (p. 4.)   \n[14] Eyal Even-Dar, Shie Mannor, and Yishay Mansour. PAC bounds for multi-armed bandit and Markov decision processes. In Proc. COLT, pages 255\u2013270, 2002. (pp. 1, 4, 8, and 9.)   \n[15] Zijun Gao, Yanjun Han, Zhimei Ren, and Zhengqing Zhou. Batched Multi-armed Bandits Problem. In Proc. NeurIPS, pages 501\u2013511, 2019. (p. 2.)   \n[16] Aur\u00e9lien Garivier and Olivier Capp\u00e9. The KL-UCB algorithm for bounded stochastic bandits and beyond. In Proc. COLT, pages 359\u2013376, 2011. (p. 5.)   \n[17] Aur\u00e9lien Garivier and Emilie Kaufmann. Optimal best arm identification with fixed confidence. In Proc. COLT, pages 998\u20131027, 2016. (pp. 1, 2, 3, 4, 5, 7, 10, 13, 25, and 26.)   \n[18] Aur\u00e9lien Garivier, Tor Lattimore, and Emilie Kaufmann. On explore-then-commit strategies. In Proc. NeurIPS, pages 784\u2013792, 2016. (p. 7.)   \n[19] Aditya Grover, Todor Markov, Peter Attia, Norman Jin, Nicolas Perkins, Bryan Cheong, Michael Chen, Zi Yang, Stephen Harris, William Chueh, et al. Best arm identification in multi-armed bandits with delayed feedback. In Proc. AISTATS, pages 833\u2013842, 2018. (p. 1.)   \n[20] Peter Harremo\u00ebs. Bounds on tail probabilities in exponential families. arXiv preprint arXiv:1601.05179, 2016. (p. 18.)   \n[21] Kevin Jamieson, Matthew Malloy, Robert Nowak, and S\u00e9bastien Bubeck. lil\u2019ucb: An optimal exploration algorithm for multi-armed bandits. In Proc. COLT, pages 423\u2013439, 2014. (pp. 2, 4, 8, and 9.)   \n[22] Tianyuan Jin, Shi Jieming, Xiaokui Xiao, and Enhong Chen. Efficient Pure Exploration in Adaptive Round model. In Proc. NeurIPS, pages 6605\u20136614, 2019. (pp. 2, 3, 5, 9, and 25.)   \n[23] Tianyuan Jin, Keke Huang, Jing Tang, and Xiaokui Xiao. Optimal streaming algorithms for multi-armed bandits. In International Conference on Machine Learning, pages 5045\u20135054. PMLR, 2021. (pp. 10, 25, and 26.)   \n[24] Tianyuan Jin, Jing Tang, Pan Xu, Keke Huang, Xiaokui Xiao, and Quanquan Gu. Almost optimal anytime algorithm for batched multi-armed bandits. In International Conference on Machine Learning, pages 5065\u20135073. PMLR, 2021. (pp. 2, 3, and 15.)   \n[25] Tianyuan Jin, Pan Xu, Xiaokui Xiao, and Quanquan Gu. Double explore-then-commit: Asymptotic optimality and beyond. In Conference on Learning Theory, pages 2584\u20132633. PMLR, 2021. (pp. 2 and 7.)   \n[26] Marc Jourdan and R\u00e9my Degenne. Non-Asymptotic Analysis of a UCB-based Top Two Algorithm. arXiv preprint arXiv:2210.05431, 2022. (pp. 3 and 4.)   \n[27] Marc Jourdan, R\u00e9my Degenne, Dorian Baudry, Rianne de Heide, and Emilie Kaufmann. Top two algorithms revisited. arXiv preprint arXiv:2206.05979, 2022. (p. 4.)   \n[28] Marc Jourdan, R\u00e9my Degenne, and Emilie Kaufmann. An varepsilon-Best-Arm Identification Algorithm for Fixed-Confidence and Beyond. Advances in Neural Information Processing Systems, 36:16578\u201316649, 2023. (p. 4.)   \n[29] Shivaram Kalyanakrishnan and Peter Stone. Efficient selection of multiple bandit arms: theory and practice. In Proc. ICML, pages 511\u2013518, 2010. (p. 4.)   \n[30] Shivaram Kalyanakrishnan, Ambuj Tewari, Peter Auer, and Peter Stone. PAC Subset Selection in Stochastic Multi-armed Bandits. In Proc. ICML, pages 655\u2013662, 2012. (p. 4.)   \n[31] Zohar Karnin, Tomer Koren, and Oren Somekh. Almost optimal exploration in multi-armed bandits. In Proc. ICML, pages 1238\u20131246, 2013. (pp. 2, 3, 4, 8, and 9.)   \n[32] Nikolai Karpov, Qin Zhang, and Yuan Zhou. Collaborative top distribution identifications with limited interaction. In 2020 IEEE 61st Annual Symposium on Foundations of Computer Science (FOCS), pages 160\u2013171. IEEE, 2020. (pp. 3, 5, 25, and 26.)   \n[33] Julian Katz-Samuels, Lalit Jain, Kevin G Jamieson, et al. An empirical process approach to the union bound: Practical algorithms for combinatorial and linear bandits. Advances in Neural Information Processing Systems, 33:10371\u201310382, 2020. (p. 4.)   \n[34] Emilie Kaufmann and Wouter M Koolen. Mixture martingales revisited with applications to sequential tests and confidence intervals. The Journal of Machine Learning Research, 22(1): 11140\u201311183, 2021. (p. 7.)   \n[35] Junpei Komiyama, Taira Tsuchiya, and Junya Honda. Minimax optimal algorithms for fixedbudget best arm identification. Advances in Neural Information Processing Systems, 35: 10393\u201310404, 2022. (p. 5.)   \n[36] Pierre M\u00e9nard and Aur\u00e9lien Garivier. A minimax and asymptotically optimal algorithm for stochastic bandits. In Proc. ALT, pages 223\u2013237, 2017. (pp. 5, 15, and 18.)   \n[37] Vianney Perchet, Philippe Rigollet, Sylvain Chassang, and Erik Snowberg. Batched bandit problems. The Annals of Statistics, 44(2):660\u2013681, 2016. (p. 2.)   \n[38] Xuanfei Ren, Tianyuan Jin, and Pan Xu. Optimal Batched Linear Bandits. In Forty-first International Conference on Machine Learning, volume 235, pages 42391\u201342416. PMLR, 2024. (p. 2.)   \n[39] Chao Tao, Qin Zhang, and Yuan Zhou. Collaborative learning with limited interaction: Tight bounds for distributed exploration in multi-armed bandits. In 2019 IEEE 60th Annual Symposium on Foundations of Computer Science (FOCS), pages 126\u2013146. IEEE, 2019. (pp. 2, 3, and 5.)   \n[40] William R Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3/4):285\u2013294, 1933. (p. 1.)   \n[41] Roman Vershynin. High-dimensional probability: An introduction with applications in data science, volume 47. Cambridge university press, 2018. (p. 20.)   \n[42] Po-An Wang, Ruo-Chun Tzeng, and Alexandre Proutiere. Fast pure exploration via frank-wolfe. Advances in Neural Information Processing Systems, 34:5810\u20135821, 2021. (pp. 3, 4, 5, and 7.)   \n[43] Yuan Zhou, Xi Chen, and Jian Li. Optimal PAC multiple arm identification with applications to crowdsourcing. In Proc. ICML, pages 217\u2013225, 2014. (p. 1.) ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Notations. ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Denote by $\\hat{\\mu}_{i}^{s}$ the empirical mean of arm $i$ after its $s$ -th pull and by $\\hat{\\mu}_{i}(t)$ the empirical mean of arm $i$ at time $t$ (i.e., after a total number of $t$ pulls are examined for all arms). We use $\\Delta_{i}=\\mu_{1}-\\mu_{i}$ for the gap between the optimal arm and the $i$ -th arm. Throughout the paper, we use the following asymptotic notation. Specifically we use $f(\\delta)\\lesssim g(\\delta)$ to denote that there exists some $\\delta_{0}$ , such that for any $\\delta<\\delta_{0}$ , $f(\\delta)\\leq\\bar{g}(\\delta)$ . Similarly, we use $f(\\delta)\\stackrel{*}{\\sim}g(\\delta)$ to denote that there exists some $\\delta_{0}$ , such that for any $\\delta<\\delta_{0}$ , $f(\\delta)\\geq g(\\delta)$ . ", "page_idx": 12}, {"type": "text", "text": "A Computing $w^{\\ast}(\\mu)$ and $T^{*}(\\mu)$ ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In this section, we introduce two useful lemmas that help us to calculate $\\pmb{w}^{*}(\\pmb{b})$ and $T^{*}(b)$ in Algorithm 1 and Algorithm 2, which are also used in [17]. For ease of presentation, we first define some useful notations. For every $c\\in[0,1]$ , we define ", "page_idx": 12}, {"type": "equation", "text": "$$\nI_{c}(\\mu,\\mu^{\\prime}):=c d(\\mu,c\\mu+(1-c)\\mu^{\\prime})+(1-c)d(\\mu^{\\prime},c\\mu+(1-c)\\mu^{\\prime}).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "For any arm $i\\in[n]\\setminus\\{1\\}$ , we define ", "page_idx": 12}, {"type": "equation", "text": "$$\ng_{i}(x):=(1+x)I_{\\frac{1}{1+x}}(\\mu_{1},\\mu_{i}).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Lemma A.1 (Garivier and Kaufmann 17, Lemma 3). For every $\\pmb{w}\\in\\mathcal{P}_{n}$ , ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\lambda\\in{\\mathrm{Alt}}(\\mu)}\\left(\\sum_{i=1}^{n}w_{i}d(\\mu_{i},\\lambda_{i})\\right)=\\operatorname*{min}_{i\\neq1}\\big(w_{1}+w_{i}\\big)I_{\\frac{w_{1}}{w_{1}+w_{i}}}(\\mu_{1},\\mu_{i}).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Besides, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T^{*}(\\pmb{\\mu})^{-1}=\\underset{\\pmb{w}\\in\\mathcal{P}_{n}}{\\operatorname*{sup}}\\underset{i\\neq1}{\\operatorname*{min}}\\,(w_{1}+w_{i})I_{\\frac{w_{1}}{w_{1}+w_{i}}}(\\mu_{1},\\mu_{i}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "and ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{w}^{*}(\\pmb{\\mu})=\\underset{\\pmb{w}\\in\\mathcal{P}_{n}}{\\arg\\operatorname*{max}}\\operatorname*{min}_{i\\neq1}\\big(\\pmb{w}_{1}+\\pmb{w}_{i}\\big)\\boldsymbol{I}_{\\frac{\\pmb{w}_{1}}{w_{1}+w_{i}}}\\big(\\mu_{1},\\mu_{i}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Lemma A.2 (Garivier and Kaufmann 17, Theorem 5). For $i\\in[n]$ , let ", "page_idx": 12}, {"type": "equation", "text": "$$\nw_{i}^{*}(\\pmb{\\mu})=\\frac{x_{i}(y^{*})}{\\sum_{i\\in[n]}x_{i}(y^{*})},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $y^{\\ast}$ is the unique solution of the equation $F_{\\mu}(y)=1$ , and where ", "page_idx": 12}, {"type": "equation", "text": "$$\nF_{\\mu}\\colon y\\mapsto\\sum_{i=2}^{n}{\\frac{d{\\Big(}\\mu_{1},{\\frac{\\mu_{1}+x_{i}(y)\\mu_{i}}{1+x_{i}(y)}}{\\Big)}}{d{\\Big(}\\mu_{i},{\\frac{\\mu_{1}+x_{i}(y)\\mu_{i}}{1+x_{i}(y)}}{\\Big)}}}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "in a continuous, increasing function on $\\left[0,d(\\mu_{1},\\mu_{2})\\right)$ such that $F_{\\mu}(y)\\rightarrow\\infty$ when $y\\rightarrow d(\\mu_{1},\\mu_{2})$ . ", "page_idx": 12}, {"type": "text", "text": "B Proof of Theorems in Section 3 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "B.1 Proof of Theorem 3.2 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Proof of Theorem 3.2. The proof of Theorem 3.2 requires the following two Lemmas, which guarantees the error returned by Line 22 and Line 26 of Algorithm 1 respectively. ", "page_idx": 12}, {"type": "text", "text": "Lemma B.1. [17, Proposition 12] Let $\\pmb{\\mu}$ be the exponential bandit model. Let $\\delta\\,\\in\\,(0,1)$ . For sufficiently small $\\delta>0$ , the Chernoff\u2019s stopping rule Line 21 of Algorithm 1 with the threshold ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\beta(t,\\delta/2)=\\log{\\left(\\frac{2\\log(2/\\delta)t^{\\alpha}}{\\delta}\\right)},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "ensures that $\\mathbb{P}_{\\mu}(i^{*}(N_{\\delta})\\neq1)\\leq\\delta/2$ . ", "page_idx": 12}, {"type": "text", "text": "Recall $\\hat{\\mu}_{i}^{s}$ is the empirical mean of arm $i$ after its $s$ -th pull. Let $\\mathcal{E}_{1}=\\{\\forall s\\ge L_{3}$ and $i\\in[n]:\\hat{\\mu}_{i}^{s}\\in$ $[\\mu_{i}-\\epsilon,\\dot{\\mu}_{i}+\\epsilon]\\}$ . The following lemma shows that $\\mathbb{P}(\\mathcal{E}_{1}^{c})\\le\\delta/2$ , where $\\mathcal{E}^{c}$ denotes the complement event of $\\mathcal{E}$ . ", "page_idx": 12}, {"type": "text", "text": "Lemma B.2. For sufficiently small $\\delta>0^{6}$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\mathcal{E}_{1}^{c})\\le\\delta/2.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "If ${\\mathcal{E}}_{1}$ happens, we have that at Line 26 of Algorithm 1, $\\hat{\\mu}_{1}(t)\\,\\geq\\mu_{1}-\\epsilon\\gtrsim\\mu_{i}+\\epsilon\\geq\\hat{\\mu}_{i}(t)$ for all $i\\,>\\,1$ , which means $i^{*}(t)\\,=\\,1$ . Combining Lemma B.1 and Lemma B.2, Theorem 3.1 follows immediately. \u53e3 ", "page_idx": 13}, {"type": "text", "text": "B.2 Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Proof of Theorem 3.1. Recall in Algorithm 1, $b_{1}^{q}\\,=\\,\\hat{\\mu}_{i^{*}(t)}(t)\\,-\\,\\epsilon$ and $b_{i}^{q}\\,=\\,\\hat{\\mu}_{i}(t)+\\epsilon$ for all $i\\in$ $[n]\\setminus\\{i^{*}(t)\\}$ , where $t=\\tau_{q}$ . Let ${\\mathcal{E}}_{0}$ be the intersection of the following four events: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{E}_{0}^{1}=\\cap_{q\\geq1}\\{b_{1}^{q}\\in[\\mu_{1}-3\\epsilon/2,\\mu_{1}-\\epsilon/2]\\},}\\\\ &{\\mathcal{E}_{0}^{2}=\\cap_{q\\geq1}\\{\\mathrm{for~all~}i\\in[n]\\setminus\\{1\\},b_{i}^{q}\\in[\\mu_{i}+\\epsilon/2,\\mu_{i}+3\\epsilon/2]\\},}\\\\ &{\\mathcal{E}_{0}^{3}=\\cap_{q\\geq0}\\{\\hat{\\mu}_{1}(\\tau_{q})\\geq\\mu_{1}-\\epsilon/2\\},}\\\\ &{\\mathcal{E}_{0}^{4}=\\cap_{q\\geq0}\\{\\mathrm{for~all~}i\\in[n]\\setminus\\{1\\},\\hat{\\mu}_{i}(\\tau_{q})\\leq\\mu_{i}+\\epsilon/2\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Here (B.1a) and (B.1b) ensure that the estimation of $w^{*}(b^{q})$ and $T^{*}(b^{q})$ is close to $\\pmb{w}^{*}(\\pmb{\\mu})$ and $T^{*}(\\pmb{\\mu})$ , and (B.1c) and (B.1d) ensure that arm 1 has the largest average reward at time $\\tau_{q}$ . ", "page_idx": 13}, {"type": "text", "text": "Lemma B.3. For sufficiently small $\\delta>0$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{P}((\\mathscr{E}_{0}^{c})\\leq\\frac{1}{(\\log\\delta^{-1})^{2}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Lemma B.4. If $\\mathcal{E}_{0}$ holds, then for sufficiently small $\\delta>0$ , we have ", "page_idx": 13}, {"type": "text", "text": "From Lemma B.4, if $\\delta$ is sufficiently small and ${\\mathcal{E}}_{0}$ holds, then Algorithm 1 will return at Line 22. Besides, we note that the total number of pulls of any arm is no more than $L_{3}=(\\log(\\delta^{-1}))^{2}$ times. Therefore, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[N_{\\delta}]\\leq n L_{1}+\\mathbb{1}\\{\\mathcal{E}_{0}\\}\\cdot\\left(\\alpha T^{*}(\\mu)\\log\\delta^{-1}+o(n\\log(1/\\delta))\\right)+\\mathbb{1}\\{\\mathcal{E}_{0}^{c}\\}\\cdot\\left(L_{2}+n L_{3}\\right)}\\\\ &{\\leq\\mathbb{1}\\{\\mathcal{E}_{0}\\}\\cdot\\alpha T^{*}(\\mu)\\log\\delta^{-1}+o(n\\log(1/\\delta))+\\mathbb{1}\\{\\mathcal{E}_{0}^{c}\\}n L_{3}}\\\\ &{\\lesssim\\mathbb{1}\\{\\mathcal{E}_{0}\\}\\cdot\\alpha T^{*}(\\mu)\\log\\delta^{-1}+n}\\\\ &{\\leq\\alpha T^{*}(\\mu)\\log\\delta^{-1}+n.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We further have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\delta\\to0}{\\frac{\\operatorname{\\mathbb{E}}[N_{\\delta}]}{\\log\\delta^{-1}}}\\leq\\alpha T^{*}(\\pmb{\\mu}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "This completes the proof. ", "page_idx": 13}, {"type": "text", "text": "B.3 Proof of Theorem 3.3 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Proof of Theorem 3.3. Stage I costs one b\u221aatch. For the Stage II, note that $\\begin{array}{r}{\\mathbb{P}(\\mathcal{E}_{0}^{c})\\leq\\frac{1}{\\log(\\delta^{-1})^{2}}}\\end{array}$ and if $\\mathcal{E}_{0}$ occurs, then $|w_{i}^{*}(\\pmb{b}^{2})-w_{i}^{*}(\\pmb{b}^{1})|\\leq1/\\sqrt{n}$ for all $i\\in[n]$ (from the second statement of Lemma B.4), which means Stage II costs 2 batches. Moreover, from Lemma B.4, if $\\delta$ is sufficiently small and $\\mathcal{E}_{0}$ holds, then Algorithm 1 will return at Line 22. Otherwise, the algorithm goes to Stage IV and costs one batch. Therefore, for sufficiently small $\\delta$ , the expected number of batches ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\leq1+\\mathbb{P}(\\mathcal{E}_{0}^{c})\\cdot\\log(1/\\delta)+2+\\mathbb{P}(\\mathcal{E}_{0}^{c})=3+o(1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Finally, if ${\\mathcal{E}}_{0}$ is true, the algorithm costs 3 batches. Therefore, with probability $1-1/\\log(1/\\delta^{2})$ , Algorithm 1 costs 3 batches. ", "page_idx": 13}, {"type": "text", "text": "B.4 Proof of Supporting Lemmas ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The proof of Lemma B.3 requires the following useful inequalities. ", "page_idx": 14}, {"type": "text", "text": "Lemma B.5 (Maximal Inequality). Let $N$ and $M$ be two positive integers, let $\\gamma>0$ , and $\\textstyle{\\hat{\\mu}}_{n}$ be the empirical mean of $n$ random variables i.i.d. according to the arm\u2019s distribution with mean $\\mu$ . Then, for $x\\le\\mu$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}(\\exists N\\leq n\\leq M,\\hat{\\mu}_{n}\\leq x)\\leq e^{-N(x-\\mu)^{2}/(2V_{0})},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and for every $x\\geq\\mu$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}(\\exists N\\leq n\\leq M,\\hat{\\mu}_{n}\\geq x)\\leq e^{-N(x-\\mu)^{2}/(2V_{1})},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $V_{0}$ is the maximum variance of arm\u2019s distribution with mean $\\mu\\in[x,\\mu]$ and $V_{1}$ is the maximum variance of arm\u2019s distribution with mean $\\mu\\in[\\mu,x]$ . ", "page_idx": 14}, {"type": "text", "text": "Note that Lemma B.5 is an improved version of Lemma 4 of [36], where we use a much smaller variance upper bound to tighten the inequalities for the case $x\\le\\mu$ and the case $x\\geq\\mu$ respectively. ", "page_idx": 14}, {"type": "text", "text": "Lemma B.6. [24, Proposition 1] For $\\epsilon>0$ and $\\mu\\leq\\mu^{\\prime}-\\epsilon$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\nd(\\mu,\\mu^{\\prime})\\geq d(\\mu,\\mu^{\\prime}-\\epsilon),\\quad\\mathrm{and}\\quad d(\\mu,\\mu^{\\prime})\\leq d(\\mu-\\epsilon,\\mu^{\\prime}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Now, we are ready to prove Lemma B.3. ", "page_idx": 14}, {"type": "text", "text": "Proof of Lemma B.3. Let $V$ be the maximum variance of reward distribution with mean $\\mu\\in[\\mu_{n},\\mu_{1}]$ . From Lemma B.5, we have that for $s\\geq L_{1}$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{P}(\\exists s\\geq L_{1}:\\hat{\\mu}_{1}^{s}\\notin[\\mu_{1}-\\epsilon/2,\\mu_{1}+\\epsilon/2])}\\\\ &{\\leq\\mathbb{P}(\\exists s\\geq L_{1}:\\hat{\\mu}_{1}^{s}\\geq\\mu_{1}+\\epsilon/2)+\\mathbb{P}(\\exists s\\geq L_{1}:\\hat{\\mu}_{1}^{s}\\leq\\mu_{1}-\\epsilon/2)}\\\\ &{\\leq2e^{-L_{1}(\\epsilon/2)^{2}/(2V)}}\\\\ &{\\lesssim\\frac{1}{n(\\log\\delta^{-1})^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the second inequality is from Lemma B.5, and the last inequality is due to that for sufficiently small $\\delta>0$ , $L_{1}=\\sqrt{\\log\\delta^{-1}}\\geq8V/\\epsilon^{2}\\log(n(\\log\\delta^{-1})^{2})$ . Similarly, for $i\\in[n]\\setminus\\{1\\}$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\exists s\\geq L_{1}:\\hat{\\mu}_{i}^{s}\\notin[\\mu_{i}-\\epsilon/2,\\mu_{i}+\\epsilon/2])\\lesssim\\frac{1}{n(\\log\\delta^{-1})^{2}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Define events: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{A}_{1}=\\{\\forall s\\ge L_{1}:\\hat{\\mu}_{1}^{s}\\in[\\mu_{1}-\\epsilon/2,\\mu_{1}+\\epsilon/2]\\}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{A}_{2}=\\{\\mathrm{for~all~}i\\in[n]\\backslash\\{1\\},\\forall s\\geq L_{1}:\\hat{\\mu}_{i}^{s}\\in[\\mu_{i}-\\epsilon/2,\\mu_{i}+\\epsilon/2]\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Assume that both events $\\mathcal{A}_{1}$ and $\\boldsymbol{A_{2}}$ hold. Then, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\mu}_{1}^{L_{1}}\\geq\\mu_{1}-\\epsilon/2\\geq\\mu_{i}+\\epsilon/2+\\Delta_{i}-\\epsilon\\geq\\hat{\\mu}_{i}^{L_{1}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where in the last inequality we assumed $\\epsilon\\le\\operatorname*{min}_{i\\neq1}\\Delta_{i}$ . This further implies $i^{*}(\\tau_{q})=1$ for any $q\\geq0$ and thus $b_{1}^{q-1}=\\hat{\\mu}_{1}(\\tau_{q})-\\epsilon$ . Therefore, we have for any $q\\geq1$ ", "page_idx": 14}, {"type": "equation", "text": "$\\begin{array}{r}{\\cdot\\;\\forall i\\in[n]\\;\\backslash\\;\\{1\\},\\,b_{i}^{q}=\\hat{\\mu}_{i}(\\tau_{q-1})+\\epsilon\\in[\\mu_{i}-\\epsilon/2,\\mu_{1}+\\epsilon/2];}\\end{array}$ ", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which means $\\mathcal{E}_{0}$ defined in (B.1) occurs. Therefore, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma(\\xi_{0})\\geq\\mathbb{P}(A_{1}\\cap A_{2})}\\\\ &{\\qquad=\\mathbb{P}\\bigg(\\big\\{\\mathbb{V}s\\geq L_{1}:\\hat{\\mu}_{1}^{s}\\in\\big[\\mu_{1}-\\epsilon/2,\\mu_{1}+\\epsilon/2\\big]\\big\\}\\bigcap_{i:i\\in[n]\\setminus\\{1\\}}\\{\\mathbb{V}s\\geq L_{1}:\\hat{\\mu}_{i}^{s}\\in[\\mu_{i}-\\epsilon/2,\\mu_{i}+\\epsilon/2]\\}}\\\\ &{\\qquad=\\mathbb{P}\\bigg(\\big\\{\\exists s\\geq L_{1}:\\hat{\\mu}_{1}^{s}\\notin\\big[\\mu_{1}-\\epsilon/2,\\mu_{1}+\\epsilon/2\\big]\\big\\}^{s}\\bigg\\{\\underset{i:i\\in[n]\\setminus\\{1\\}}{\\bigcap}\\{\\exists s\\geq L_{1}:\\hat{\\mu}_{i}^{s}\\notin[\\mu_{i}-\\epsilon/2,\\mu_{i}+\\epsilon/2]\\}}\\\\ &{\\qquad\\geq1-\\mathbb{P}\\bigg(\\exists s\\geq L_{1}:\\hat{\\mu}_{1}^{s}\\notin\\bigg[\\mu_{1}-\\frac{\\epsilon}{2},\\mu_{1}+\\frac{\\epsilon}{2}\\bigg]\\bigg)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\mathbb{P}\\bigg(\\exists s\\geq L_{1}:\\hat{\\mu}_{i}^{s}\\notin\\bigg[\\mu_{i}-\\frac{\\epsilon}{2},\\mu_{i}+\\frac{\\epsilon}{2}\\bigg]\\bigg)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\\\ &{\\qquad\\geq1-(\\log\\delta^{-1})^{-2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the last inequality is due to (B.6) and (B.7). ", "page_idx": 15}, {"type": "text", "text": "Proof of Lemma B.4. Assume $\\mathcal{E}_{0}$ is true. Recall that $\\Delta_{2}=\\mathrm{min}_{i\\in[n]\\backslash\\{1\\}}\\,\\mu_{1}-\\mu_{i}$ . For sufficiently small $\\delta>0$ such that $\\epsilon\\,<\\,\\Delta_{2}/8$ , we have $b_{1}^{q}\\,\\in\\,[\\mu_{1}\\,-\\,\\Delta_{2}/4,\\mu_{1}]$ and $\\dot{b}_{i}^{q}\\,\\in\\,[\\mu_{i},\\mu_{i}+\\Delta_{2}/4]$ . Let $\\mathcal{L}(b)=\\{\\lambda\\in S:\\lambda_{1}\\in[\\mu_{1}-\\Delta_{2}/4,\\mu_{1}]$ and for any $i\\in[n]\\setminus\\{1\\},\\lambda_{i}\\in[\\mu_{i},\\mu_{i}+\\Delta_{2}/4]\\}$ . Therefore ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\alpha w_{i}^{*}(\\pmb{b}^{q})T^{*}(\\pmb{b}^{q})\\leq\\operatorname*{max}_{\\pmb{b}^{\\prime}\\in\\mathcal{L}(\\pmb{b})}\\alpha w_{i}^{*}(\\pmb{b}^{\\prime})T^{*}(\\pmb{b}^{\\prime})<\\infty.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since $\\begin{array}{r}{\\operatorname*{max}_{b^{\\prime}\\in{\\mathcal{L}}(b)}\\alpha w_{i}^{*}(b^{\\prime})T^{*}(b^{\\prime})<\\infty}\\end{array}$ is independent of $\\delta$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\alpha w_{i}^{*}(b^{q})T^{*}(b^{q})\\le\\operatorname*{max}_{b^{\\prime}\\in\\mathcal{L}(b)}\\alpha w_{i}^{*}(b^{\\prime})T^{*}(b^{\\prime})\\lesssim\\log\\log\\delta^{-1}/n=L_{2},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which implies $T_{i}^{q}\\,=\\,\\mathrm{min}\\{\\alpha w_{i}^{*}(b^{q})T^{*}(b^{q})\\log\\delta^{-1},L_{2}\\}\\,=\\,\\alpha w_{i}^{*}(b^{q})T^{*}(b^{q})\\log\\delta^{-1}$ . Besides, as $w^{*}(b^{q})$ and $T^{*}(\\bar{b^{q}})$ are continuous on $^{b^{q}}$ , we have that as $\\pmb{b}^{q}\\rightarrow\\pmb{\\mu}$ , $w^{*}(b^{q})\\rightarrow w^{*}(\\pmb{\\mu})$ and $T^{*}(b^{q})\\rightarrow$ $T^{*}(\\pmb{\\mu})$ . Note that $|\\dot{b}_{i}^{q}-\\mu_{i}|\\leq2\\epsilon$ and $\\epsilon$ approaches 0 as $\\delta$ approaches 0. Recall $\\tau$ is the stopping time of Stage $\\mathrm{II}$ and $T_{i}$ is the number of pulls of arm $i$ at time $\\tau$ . Therefore, for sufficiently small $\\delta$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\nT_{i}:=\\operatorname*{max}_{p:p\\in\\mathbb{N},p\\geq1}T_{i}^{p}=\\alpha w_{i}^{*}(\\mu)T^{*}(\\mu)\\log\\delta^{-1}+o(\\log(1/\\delta)).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Moreover, for sufficient smaller $\\delta$ and $\\forall i\\in[n],|w_{i}^{*}(\\pmb{b}^{q})-w_{i}^{*}(\\pmb{\\mu})|=o(1)$ and thus ", "page_idx": 15}, {"type": "equation", "text": "$$\n|w_{i}^{*}(\\pmb{b}^{1})-w_{i}^{*}(\\pmb{b}^{2})|<1/\\sqrt{n},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which means the condition in Line 15 is satisfied and the Algorithm goes to Stage III. ", "page_idx": 15}, {"type": "text", "text": "Assume $\\tau=\\tau_{q^{\\prime}}$ . We have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Z_{1i}(\\tau)=N_{1}(\\tau)\\cdot d(\\hat{\\mu}_{1}(\\tau),\\hat{\\mu}_{1i}(\\tau))+N_{i}(\\tau)\\cdot d(\\hat{\\mu}_{i}(\\tau),\\hat{\\mu}_{1i}(\\tau))}\\\\ &{\\quad\\quad\\quad=N_{1}(\\tau_{q^{\\prime}})\\cdot d(\\hat{\\mu}_{1}(\\tau),\\hat{\\mu}_{1i}(\\tau))+N_{i}(\\tau_{q^{\\prime}})\\cdot d(\\hat{\\mu}_{i}(\\tau),\\hat{\\mu}_{1i}(\\tau))}\\\\ &{\\quad\\quad\\quad\\geq\\alpha\\log\\delta^{-1}\\cdot\\bigg(\\frac{w_{1}^{*}(\\pmb{b}^{q^{\\prime}})d(\\hat{\\mu}_{1}(\\tau),\\hat{\\mu}_{1i}(\\tau))}{T^{*}(\\pmb{b}^{q^{\\prime}})^{-1}}+\\frac{w_{i}^{*}(\\pmb{b}^{q^{\\prime}})d(\\hat{\\mu}_{1}(\\tau),\\hat{\\mu}_{1i}(\\tau))}{T^{*}(\\pmb{b}^{q^{\\prime}})^{-1}}\\bigg),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the last inequality is due to $T_{i}\\geq T_{i}^{q^{\\prime}}=\\alpha w_{i}^{*}({\\pmb{b}}^{q^{\\prime}})T^{*}({\\pmb{b}}^{q^{\\prime}})\\log(1/\\delta)$ . In what follows, we will show ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{w_{1}^{*}(b^{q^{\\prime}})\\cdot d(\\hat{\\mu}_{1}(\\tau),\\hat{\\mu}_{1i}(\\tau))}{T^{*}(b^{q^{\\prime}})^{-1}}+\\frac{w_{i}^{*}(b^{q^{\\prime}})\\cdot d(\\hat{\\mu}_{i}(\\tau),\\hat{\\mu}_{1i}(\\tau))}{T^{*}(b^{q^{\\prime}})^{-1}}\\geq1.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The following minimization problem is a convex optimization problem ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\lambda_{1w}:\\lambda_{1w}\\leq\\lambda_{i w}}w_{1}^{*}({b}^{q^{\\prime}})\\cdot d(\\hat{\\mu}_{1}(\\tau),\\lambda_{1w})+w_{i}^{*}({b}^{q^{\\prime}})\\cdot d(\\hat{\\mu}_{i}(\\tau),\\lambda_{i w}),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which is solved when we have $\\lambda_{1w}\\,=\\,\\lambda_{i w}\\,=\\,\\hat{\\mu}_{1i}(\\tau)$ . The proof of this lemma is based on the assumption that ${\\mathcal{E}}_{0}$ occurs. Note that ${\\mathcal{E}}_{0}$ occurs, then $b_{1}^{q^{\\prime}}=\\hat{\\mu}_{1}(\\tau_{q^{\\prime}-1})-\\epsilon\\leq\\mu_{1}-\\epsilon/2\\leq\\hat{\\mu}_{1}(\\tau)$ and $b_{i}^{q^{\\prime}}=\\hat{\\mu}_{1}(\\tau_{q^{\\prime}-1})+\\epsilon>\\hat{\\mu}_{i}(\\tau)$ for all $i\\in[n]\\setminus\\{1\\}$ . Therefore, $\\hat{\\mu}_{1}(\\tau)\\geq b_{1}^{q^{\\prime}}\\gtrsim b_{i}^{q^{\\prime}}\\geq\\hat{\\mu}_{i}(\\tau)$ . Let ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\lambda_{w^{*}(b^{q^{\\prime}})}(b_{1}^{q^{\\prime}},b_{i}^{q^{\\prime}})=\\frac{w_{1}^{*}(b^{q^{\\prime}})}{w_{1}^{*}(b^{q^{\\prime}})+w_{i}^{*}(b^{q^{\\prime}})}\\cdot b_{1}^{q}+\\frac{w_{i}^{*}(b^{q^{\\prime}})}{w_{1}^{*}(b^{q^{\\prime}})+w_{i}^{*}(b^{q^{\\prime}})}\\cdot b_{i}^{q}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then, $\\lambda_{w^{*}(\\pmb{b})}(b_{1}^{q^{\\prime}},b_{i}^{q^{\\prime}})$ is the solution to minimizing $w_{1}^{*}(\\pmb{b}^{q^{\\prime}})\\cdot\\pmb{d}(\\pmb{b}_{1}^{q^{\\prime}},\\pmb{x})+w_{i}^{*}(\\pmb{b}^{q^{\\prime}})\\cdot\\pmb{d}(\\pmb{b}_{i}^{q^{\\prime}},\\pmb{x})$ for $x\\in(b_{i}^{q^{\\prime}},b_{1}^{q^{\\prime}})$ . ", "page_idx": 16}, {"type": "text", "text": "Case 1: if $\\hat{\\mu}_{1i}(\\tau)\\in(b_{1}^{q^{\\prime}},\\hat{\\mu}_{1}(\\tau))$ , then ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w_{1}^{*}({b^{q^{\\prime}}})\\cdot d(\\hat{\\mu}_{1}(\\tau),\\hat{\\mu}_{1i}(\\tau))+w_{i}^{*}({b^{q^{\\prime}}})\\cdot d(\\hat{\\mu}_{i}(\\tau),\\hat{\\mu}_{1i}(\\tau))}\\\\ &{\\geq w_{1}^{*}({b^{q^{\\prime}}})\\cdot d(\\hat{\\mu}_{1}(\\tau),\\hat{\\mu}_{1}(\\tau))+w_{i}^{*}({b^{q^{\\prime}}})\\cdot d(\\hat{\\mu}_{i}(\\tau),{b_{1}^{q^{\\prime}}})}\\\\ &{\\geq w_{1}^{*}({b^{q^{\\prime}}})\\cdot d({b_{1}^{q^{\\prime}}},{b_{1}^{q^{\\prime}}})+w_{i}^{*}({b^{q^{\\prime}}})\\cdot d({b_{i}^{q^{\\prime}}},{b_{1}^{q^{\\prime}}})}\\\\ &{\\geq w_{1}^{*}({b^{q^{\\prime}}})\\cdot d({b_{1}^{q^{\\prime}}},\\lambda_{w^{*}({b^{q^{\\prime}}})}({b_{1}^{q^{\\prime}}},{b_{i}^{q^{\\prime}}}))+w_{i}^{*}({b^{q^{\\prime}}})\\cdot d({b_{i}^{q^{\\prime}}},\\lambda_{w^{*}({b^{q^{\\prime}}})}({b_{1}^{q^{\\prime}}},{b_{i}^{q^{\\prime}}})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the first and second inequalities are due to (B.5) and the last inequality is due to the fact that $w_{1}^{*}(\\pmb{b}^{q^{\\prime}})\\cdot d(\\pmb{b}_{1}^{q^{\\prime}},x)+w_{i}^{*}(\\pmb{b}^{q^{\\prime}})\\cdot\\overline{{d}}(b_{i}^{q^{\\prime}},x)$ achieves it minimum at $x=\\lambda_{w^{*}({\\pmb{b}}^{q^{\\prime}})}(b_{1}^{q^{\\prime}},b_{i}^{q^{\\prime}})$ . ", "page_idx": 16}, {"type": "text", "text": "Case 2: if $\\hat{\\mu}_{1i}(\\tau)\\in(b_{i}^{q^{\\prime}},b_{1}^{q^{\\prime}})$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w_{1}^{*}(b^{q^{\\prime}})\\cdot d(\\hat{\\mu}_{1}(\\tau),\\hat{\\mu}_{1i}(\\tau))+w_{i}^{*}(b^{q^{\\prime}})\\cdot d(\\hat{\\mu}_{i}(\\tau),\\hat{\\mu}_{1i}(\\tau))}\\\\ &{\\geq w_{1}^{*}(b^{q^{\\prime}})\\cdot d(b_{1}^{q^{\\prime}},\\hat{\\mu}_{1i}(\\tau))+w_{i}^{*}(b^{q^{\\prime}})\\cdot d(b_{i}^{q^{\\prime}},\\hat{\\mu}_{1i}(\\tau))}\\\\ &{\\geq w_{1}^{*}(b^{q^{\\prime}})\\cdot d(b_{1}^{q^{\\prime}},\\lambda_{w^{*}(b^{q^{\\prime}})}(b_{1}^{q^{\\prime}},b_{i}^{q^{\\prime}}))+w_{i}^{*}(b^{q^{\\prime}})\\cdot d(b_{i}^{q^{\\prime}},\\lambda_{w^{*}(b^{q^{\\prime}})}(b_{1}^{q^{\\prime}},b_{i}^{q^{\\prime}})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the first inequality is due to (B.5) and the last inequality is due to the fact that for $x\\in(b_{i}^{q^{\\prime}},b_{1}^{q^{\\prime}})$ , $w_{1}^{*}(\\pmb{b}^{q^{\\prime}})\\cdot d(b_{1}^{q^{\\prime}},x)+w_{i}^{*}(\\pmb{b}^{q^{\\prime}})\\cdot d(b_{i}^{q^{\\prime}},x)$ achieves its minimum at $x=\\lambda_{w^{*}(b^{q^{\\prime}})}(b_{1}.......q^{\\prime},b_{i}^{q^{\\prime}})$ . ", "page_idx": 16}, {"type": "text", "text": "Case 3: if $\\hat{\\mu}_{1i}(\\tau)\\in(\\hat{\\mu}_{i}(\\tau),b_{i}^{q^{\\prime}})$ , similar to (B.9) and (B.10), we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w_{1}^{*}({b^{q^{\\prime}}})\\cdot d(\\hat{\\mu}_{1}(\\tau),\\hat{\\mu}_{1i}(\\tau))+w_{i}^{*}({b^{q^{\\prime}}})\\cdot d(\\hat{\\mu}_{i}(\\tau),\\hat{\\mu}_{1i}(\\tau))}\\\\ &{\\geq w_{1}^{*}({b^{q^{\\prime}}})\\cdot d({b_{1}^{q^{\\prime}}},\\lambda_{w^{*}({b^{q^{\\prime}}})}({b_{1}^{q^{\\prime}}},{b_{i}^{q^{\\prime}}}))+w_{i}^{*}({b^{q^{\\prime}}})\\cdot d({b_{i}^{q^{\\prime}}},\\lambda_{w^{*}({b^{q^{\\prime}}})}({b_{1}^{q^{\\prime}}},{b_{i}^{q^{\\prime}}})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Combine all three cases, we obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w_{1}^{*}({b^{q^{\\prime}}})\\cdot d(\\hat{\\mu}_{1}(\\tau),\\hat{\\mu}_{1i}(\\tau))+w_{i}^{*}({b^{q^{\\prime}}})\\cdot d(\\hat{\\mu}_{i}(\\tau),\\hat{\\mu}_{1i}(\\tau))}\\\\ &{\\ \\geq w_{1}^{*}({b^{q^{\\prime}}})\\cdot d(b_{1},\\lambda_{w^{*}({b^{q^{\\prime}}})}({b_{1}^{q^{\\prime}}},{b_{i}^{q^{\\prime}}}))+w_{i}^{*}({b^{q^{\\prime}}})\\cdot d(b_{i}^{q^{\\prime}},\\lambda_{w^{*}({b^{q^{\\prime}}})}({b_{1}^{q^{\\prime}}},{b_{i}^{q^{\\prime}}})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Note that $\\hat{\\mu}_{1i}(\\tau)=\\lambda_{w^{*}(b^{q^{\\prime}})}(\\hat{\\mu}_{1}(\\tau),\\hat{\\mu}_{i}(\\tau))$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{w_{1}^{*}(b^{q^{\\prime}})\\cdot d(\\hat{\\mu}_{1}(\\tau),\\hat{\\mu}_{1i}(\\tau))}{T^{*}(b^{q^{\\prime}})^{-1}}+\\frac{w_{i}^{*}(b^{q^{\\prime}})\\cdot d(\\hat{\\mu}_{1}(\\tau),\\hat{\\mu}_{1i}(\\tau))}{T^{*}(b^{q^{\\prime}})^{-1}}}\\\\ &{\\geq\\frac{w_{1}^{*}(b^{q^{\\prime}})\\cdot d(b_{1}^{q^{\\prime}},\\lambda_{w^{*}(b^{q^{\\prime}})}(b_{1}^{q^{\\prime}},b_{i}^{q^{\\prime}}))}{T^{*}(b^{q^{\\prime}})^{-1}}+\\frac{w_{i}^{*}(b^{q^{\\prime}})\\cdot d(b_{i}^{q^{\\prime}},\\lambda_{w^{*}(b^{q^{\\prime}})}(b_{1}^{q^{\\prime}},b_{i}^{q^{\\prime}}))}{T^{*}(b^{q^{\\prime}})^{-1}}}\\\\ &{=1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the first inequality is due to (B.12) and the last inequality is due to (A.1). Note that conditioned on event ${\\mathcal{E}}_{0}$ , for sufficiently small $\\delta>0$ , we have $1=i^{*}(\\tau)$ . Therefore, for any $i\\in[n]\\setminus\\{1\\}$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Z_{i}(\\tau)=Z_{1i}(\\tau)}\\\\ &{\\qquad=\\alpha\\log\\delta^{-1}\\cdot\\bigg(\\frac{w_{1}^{*}(b^{q^{\\prime}})\\cdot d(\\hat{\\mu}_{1}(\\tau),\\hat{\\mu}_{1i}(\\tau))}{T^{*}(b^{q^{\\prime}})^{-1}}+\\frac{w_{i}^{*}(b^{q^{\\prime}})\\cdot d(\\hat{\\mu}_{1}(\\tau),\\hat{\\mu}_{1i}(\\tau))}{T^{*}(b^{q^{\\prime}})^{-1}}\\bigg)}\\\\ &{\\qquad\\geq\\alpha\\log\\delta^{-1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the first equality is from (3.3), the second equality is from (B.8), and the last inequality is from (B.13). Finally, for $\\dot{\\alpha_{}^{\\prime}}\\!\\geq1+6\\log\\log\\delta^{-1}/\\log\\delta^{-1}$ , we obtain ", "page_idx": 17}, {"type": "equation", "text": "$$\nZ_{i}(\\tau)\\ge\\log\\left(\\frac{(\\log\\delta^{-1})^{6}}{\\delta}\\right)\\gtrsim\\log\\left(\\frac{2(\\log(2/\\delta)))\\cdot(n L_{2}+n L_{1})^{\\alpha}}{\\delta}\\right)=\\beta(\\tau,\\delta/2),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which completes the proof. ", "page_idx": 17}, {"type": "text", "text": "B.5 Proof of Maximum Inequality ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof of Lemma B.5. Recall that the result of Lemma 4 of M\u00e9nard and Garivier [36] is: for $\\hat{\\mu}_{n}\\leq\\mu$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\exists N\\leq n\\leq M,d({\\hat{\\mu}}_{n},\\mu)\\geq\\gamma)\\leq e^{-N\\gamma}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By Lemma 1 of Harremo\u00ebs [20], we have ", "page_idx": 17}, {"type": "equation", "text": "$$\nd(x,\\mu)=\\int_{x}^{\\mu}\\frac{y-x}{V(y)}\\mathrm{d}y\\leq\\frac{(x-\\mu)^{2}}{2V_{0}},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $V(y)$ is the variance of the distribution with mean $y$ . As a simple consequence of (B.15) and (B.14), we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}(\\exists N\\leq n\\leq M,\\hat{\\mu}_{n}\\leq x)\\leq e^{-N(x-\\mu)^{2}/(2V_{0})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "For the case when $\\hat{\\mu}_{n}\\geq\\mu$ , we can directly follow the idea of Lemma 4 of M\u00e9nard and Garivier [36]. For the case $\\hat{\\mu}_{n}\\geq\\mu$ , we aim to show ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\exists N\\leq n\\leq M,d({\\hat{\\mu}}_{n},\\mu)\\geq\\gamma)\\leq e^{-N\\gamma}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We let $M(\\lambda)$ be the log-moment generating function of $\\nu_{\\theta}$ . Recall that $\\begin{array}{r}{\\frac{\\mathrm{d}\\nu_{\\theta}}{\\mathrm{d}\\rho}(x)=\\exp(x\\theta-b(\\theta))}\\end{array}$ We use the following properties of one exponential family. ", "page_idx": 17}, {"type": "text", "text": "Let $\\mathbb{E}[\\nu_{\\theta}]=\\mu$ . Let $\\lambda=\\theta_{1}-\\theta,z=b^{\\prime}(\\theta_{1})>\\mu$ , and $\\gamma=d(z,\\mu)$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\gamma=d(z,\\mu)=\\mathrm{KL}(\\nu_{\\theta_{1}},\\nu_{\\theta})=\\lambda z-M(\\lambda).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since $d(z,\\mu)$ is monotone increasing for $z\\ >\\ \\mu$ , we obtain $\\lambda\\ >\\ 0$ . If event $\\{\\exists N\\ \\leq\\ n\\ \\leq$ $M,d(\\hat{\\mu}_{n},\\mu)\\geq\\gamma\\}$ and $\\hat{\\mu}_{n}\\geq\\mu$ , one have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\hat{\\mu}_{n}\\geq\\mu,\\qquad\\lambda\\hat{\\mu}_{n}-M(\\lambda)\\geq\\lambda z-M(\\lambda)=\\gamma,\\qquad\\lambda n\\hat{\\mu}_{n}-n M(\\lambda)\\geq N\\gamma.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By Doob\u2019s maximal inequality for the exponential martingale $\\exp(\\lambda n\\hat{\\mu}_{n}-n M(\\lambda))$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(\\exists N\\leq n\\leq M,d(\\hat{\\mu}_{n},\\mu)\\geq\\gamma)\\leq\\mathbb{P}(\\exists N\\leq n\\leq M,\\lambda n\\hat{\\mu}_{n}-n M(\\lambda)\\geq N\\gamma)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq e^{-N\\gamma}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Combining above inequality and (B.15), we obtain for $\\hat{\\mu}_{n}\\geq\\mu$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}(\\exists N\\leq n\\leq M,d(\\hat{\\mu}_{n},\\mu)\\geq\\gamma)\\leq e^{-N(x-\\mu)^{2}/(2V_{1})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This completes the proof. ", "page_idx": 17}, {"type": "text", "text": "C Proof of Theorems in Section 4 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this proof, we define \u201cone round\" as a single iteration of the While Loop of Algorithm 2. ", "page_idx": 17}, {"type": "text", "text": "Proof of Theorem 4.2. Proof of Correctness. We first show that the best arm is not eliminated by Line 11 for any round $r$ . For any $l$ , let $\\mathcal{E}^{l}$ be defined as follows. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{E}^{l}=\\{1\\in S_{r},\\forall r\\leq l\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Lemma C.1. Line 11 of Algorithm 2 satisfies ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{r\\geq1}\\mathbb{P}\\left(\\hat{p}_{1}^{r}\\geq\\hat{p}_{*}^{r}-\\frac{\\epsilon_{r}}{4}\\;\\bigg|\\;\\mathcal{E}^{r}\\right)\\geq1-\\frac{\\delta}{8}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "To streamline our presentation, we define $\\ell_{j s}\\;=\\;s$ , $\\gamma_{j s}\\,=\\,(\\delta_{j})^{2^{s}}$ , and $\\hat{p}_{i}^{j\\,s}$ as the sth updates of parameters $\\ell_{j},\\gamma_{j}$ , and $\\hat{p}_{i}^{j}$ , respectively, within the second For Loop. For any fixed $j$ , we define $U_{j}(s)$ to be the following set of rounds, where $\\ell_{j}$ remains to be the same value $s$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\nU_{j}(s):=\\{r=1,2,\\ldots:\\ell_{j}=s{\\mathrm{~at~round~}}r{\\mathrm{~of~Algorithm~}}2\\}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then the condition of Line 21 could be represented as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\exists i\\in S_{j}{\\mathrm{~and~}}r\\in U_{j}(s),\\;\\hat{p}_{i}^{j s}\\geq\\hat{p}_{*}^{r}-\\frac{\\epsilon_{j}}{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The following lemma shows that with high probability, Algorithm 2 will not return at Line 22. Lemma C.2. Line 21 of Algorithm 2 satisfies ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Bigg(\\bigcap_{j\\geq1}\\bigcap_{s\\geq1}\\bigcap_{r\\in U_{j}(s)}\\Bigg\\{\\operatorname*{max}_{i\\in S_{j}}\\hat{p}_{i}^{j s}<\\hat{p}_{*}^{r}-\\frac{\\epsilon_{j}}{2}\\Bigg\\}\\Bigg)\\geq1-\\frac{3\\delta}{16}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note that Algorithm 2 will return arm 1 at Line 24 if 1: Arm 1 is always maintained in $S_{r}$ , and 2: Algorithm 2 never return at Line 22. ", "page_idx": 18}, {"type": "text", "text": "According to Lemma C.1, there is a probability of at least $1-\\delta/8$ that $\\mathsf{A r m}\\,1$ will never be eliminated at Line 11. Given that $\\mathrm{Arm}\\ 1$ is never eliminated at Line 11, Lemma C.2 suggests that there is a probability of at least $1-3\\delta/16$ that we will never loop back at Line 22. Hence, with a probability of $1-\\delta/2$ , Stage IV will identify the optimal arm. By incorporating the results of Lemma B.1, we find that with a probability exceeding $\\bar{1+\\delta/2}-\\delta/2$ , which is greater than $1-\\delta$ , Algorithm 2 will return the optimal arm. ", "page_idx": 18}, {"type": "text", "text": "Proof of Sample Complexity. Let $N^{\\prime}$ be the sample complexity of Stage IV. Let $\\mathcal{E}=\\cap_{r>1}\\mathcal{E}^{r}$ . The following lemma shows the sample complexity conditioned on event $\\mathcal{E}$ . ", "page_idx": 18}, {"type": "text", "text": "Lemma C.3. The expected sample complexity $N^{\\prime}$ conditioned on event $\\mathcal{E}$ has the order ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}[N^{\\prime}\\mid\\mathcal{E}]=O\\bigg(\\sum_{i>1}\\frac{\\log\\big(n/\\delta\\log(\\Delta_{i}^{-1})\\big)}{\\Delta_{i}^{2}}\\bigg).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now, we consider the expected sample complexity if the best arm is eliminated at some round $r$ . We prove the following lemma. ", "page_idx": 18}, {"type": "text", "text": "Lemma C.4. The sum of expected sample complexity $N^{\\prime}$ at each loop has the order ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{r\\geq1}\\mathbb{E}[N^{\\prime}\\,\\mathbb{1}\\{\\mathcal{E}^{r},(\\mathcal{E}^{r+1})^{c}\\}]=O\\bigg(\\sum_{i>1}\\frac{\\log\\big(n/\\delta\\log(\\Delta_{i}^{-1})\\big)}{\\Delta_{i}^{2}}\\bigg).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The number of pulls of all arms in the first three stages is no more than $\\operatorname*{max}\\{n{\\sqrt{\\log\\delta^{-1}}},L_{2}\\}$ . By combining the above results, Lemma C.3 and Lemma C.4 together, we can obtain the following total sample complexity ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}[N_{\\delta}]=\\mathcal{O}\\left((\\log\\delta^{-1})\\cdot\\log\\log\\delta^{-1}+\\sum_{i>1}\\frac{\\log\\left(n\\delta^{-1}\\log\\Delta_{i}^{-1}\\right)}{\\Delta_{i}^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since in the non-asymptotic setting, $\\delta$ is finite. Then ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}[N_{\\delta}]=\\mathcal{O}\\left(\\sum_{i>1}\\frac{\\log\\left(n\\log\\Delta_{i}^{-1}\\right)}{\\Delta_{i}^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof of Batch Complexity. Just as we discussed in the proof of sample complexity, we first demonstrate the batch complexity conditioned on the event of $\\mathcal{E}$ . ", "page_idx": 18}, {"type": "text", "text": "Lemma C.5. Conditional on event $\\mathcal{E}$ , Algorithm 2 conducts $\\mathcal{O}(\\log(1/\\Delta_{2}^{-1}))$ batches in expectation. ", "page_idx": 19}, {"type": "text", "text": "Let $B^{\\prime}$ be the number of batches we used in Stage IV. We consider the expected batch complexity if the best arm is eliminated at some round $r$ . We prove the following lemma. ", "page_idx": 19}, {"type": "text", "text": "Lemma C.6. The sum of expected batch complexity $B^{\\prime}$ at each loop has the order ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{r\\geq1}\\mathbb{E}[B^{\\prime}\\,\\mathbb{1}\\{\\mathcal{E}^{r},(\\mathcal{E}^{r+1})^{c}\\}]=\\mathcal{O}(\\log(1/\\Delta_{2}^{-1})).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By combining Lemma C.5, Lemma C.6, the fact that the first three stages use $O(\\log(1/\\delta))$ batches, and $\\delta$ is a constant in non-asymptotic setting, the batch complexity of Algorithm 2 is $\\mathcal{O}(\\log(1/\\Delta_{2}^{-1}))$ . ", "page_idx": 19}, {"type": "text", "text": "Proof of Theorem 4.3. Let ${\\mathcal{E}}_{0}$ be the event defined in (B.1) in the proof of Theorem 3.1. From Lemma B.4, if $\\delta$ is sufficiently small and ${\\mathcal{E}}_{0}$ holds, then Algorithm 2 will return at Stage III. Note that the expected number of pulls in Stage IV is $\\lesssim(\\log\\delta^{-1})^{\\natural}$ . Hence, the expected number of pulls of all arms is no more than $2(\\log\\delta^{-1})^{2}$ times. Therefore, for any $\\epsilon^{\\prime}>0$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[N_{\\delta}]\\lesssim\\alpha T^{*}(b)\\log\\delta^{-1}+o(\\log(1/\\delta))+\\mathbb{1}\\{\\mathcal{E}_{0}^{c}\\}2n(\\log\\delta^{-1})^{2}}\\\\ &{\\lesssim\\alpha T^{*}(\\pmb{\\mu})\\log\\delta^{-1}+2n.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\delta\\to0}{\\frac{\\mathbb{E}_{\\pmb{\\mu}}[N_{\\delta}]}{\\log\\delta^{-1}}}\\leq\\alpha T^{*}(\\pmb{\\mu}),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Moreover, there exists some universal constant $C$ , such that the expected number of batches used in Stage IV is $\\lesssim C\\log(1/\\Delta_{2})\\cdot1\\{\\mathcal{E}_{0}^{c}\\}\\lesssim o(1)$ , where the last inequality is due to Lemma B.3. Form Theorem 3.3, the batch complexity of the first three Stage is $3+o(1)$ . Therefore, the total expected batch complexity is $3+o(1)$ . \u53e3 ", "page_idx": 19}, {"type": "text", "text": "D Proof of Supporting Lemmas ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The proof of the supporting lemmas requires the following concentration inequalities. ", "page_idx": 19}, {"type": "text", "text": "Lemma D.1. [41, Theorem 2.2.6] Let $X_{1},\\ldots,X_{k}\\ \\in\\ [0,1]$ be independent bounded random variables with mean $\\mu$ . Then for any $\\epsilon>0$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(\\hat{\\mu}\\geq\\mu+\\epsilon)\\leq\\exp\\bigg(-\\displaystyle\\frac{k\\epsilon^{2}}{2}\\bigg)\\quad\\mathrm{and}}\\\\ &{\\mathbb{P}(\\hat{\\mu}\\leq\\mu-\\epsilon)\\leq\\exp\\bigg(-\\displaystyle\\frac{k\\epsilon^{2}}{2}\\bigg),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\begin{array}{r}{\\hat{\\mu}=1/k\\sum_{t=1}^{k}X_{t}}\\end{array}$ . ", "page_idx": 19}, {"type": "text", "text": "D.1 Proof of Lemma C.1 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "From Lemma D.1, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\lvert\\hat{p}_{i}^{r}-\\mu_{i}\\rvert\\ge\\frac{\\epsilon_{r}}{8}\\right)\\le2\\exp\\bigg(-\\frac{2d_{r}\\epsilon_{r}^{2}}{64}\\bigg)=\\delta_{r}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Applying union bound, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\bigg(\\bigcup_{r\\geq1}\\bigcup_{i\\in[n]}\\Big\\{|\\hat{p}_{i}^{r}-\\mu_{i}|\\geq\\frac{\\epsilon_{r}}{8}\\Big\\}\\bigg)\\leq\\sum_{r>1}\\sum_{i\\in[n]}\\delta_{r}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\sum_{r>1}\\frac{\\delta}{40\\pi^{2}\\cdot r^{2}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\frac{\\delta}{8}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The aforementioned inequality suggests that, with a probability of at least $1-\\delta/8$ , the condition $\\begin{array}{r}{|\\hat{p}_{i}^{r}-\\mu_{i}|\\leq\\frac{\\epsilon_{r}}{8}}\\end{array}$ holds true for any $i\\in[n]$ and $r\\geq1$ . Then, for any $r>1$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\hat{p}_{1}^{r}\\ge\\mu_{1}-\\frac{\\epsilon_{r}}{8}\\ge\\mu_{*}-\\frac{\\epsilon_{r}}{8}\\ge\\hat{p}_{*}^{r}-\\frac{\\epsilon_{r}}{4}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{r\\geq1}\\mathbb{P}\\left(\\hat{p}_{1}^{r}\\geq\\hat{p}_{*}^{r}-\\frac{\\epsilon_{r}}{4}\\;\\bigg|\\;\\mathcal{E}^{r}\\right)\\geq1-\\frac{\\delta}{8}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "D.2 Proof of Lemma C.2 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "From Lemma D.1, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{P}\\bigg(|\\hat{p}_{i}^{j s}-\\mu_{i}|\\geq\\frac{\\epsilon_{j}}{8}\\bigg)\\leq\\gamma_{j s}=(\\delta_{j})^{2^{s}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Applying union bound, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Bigg(\\bigcup_{j\\geq1}\\bigcup_{s>1}\\bigcup_{i\\in S_{j}}\\left\\{\\vert\\hat{p}_{i}^{j s}-\\mu_{i}\\vert\\geq\\frac{\\epsilon_{j}}{8}\\right\\}\\Bigg)\\leq\\sum_{j\\geq1}\\sum_{s>1}\\sum_{i\\in S_{j}}(\\delta_{j})^{2^{s}}}\\\\ {\\leq\\sum_{j\\geq1}\\sum_{s>1}\\Bigg(\\frac{\\delta}{40\\pi^{2}\\cdot j^{2}}\\Bigg)^{2^{s}}}\\\\ {\\leq\\sum_{s>1}\\Bigg(\\frac{\\delta}{8}\\Bigg)^{2^{s}}}\\\\ {\\leq\\frac{\\delta}{16}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Besides, from (D.3), we obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Bigg(\\bigcap_{r\\geq1}\\bigcap_{i\\in[n]}\\Bigg\\{|\\hat{p}_{i}^{r}-\\mu_{i}|\\leq\\frac{\\epsilon_{r}}{8}\\Bigg\\}\\Bigg)\\geq1-\\frac{\\delta}{8}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Define events ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{3}=\\cap_{r\\geq1}\\cap_{i\\in[n]}\\big\\{|\\hat{p}_{i}^{r}-\\mu_{i}|\\leq\\epsilon_{r}/8\\big\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{E}_{4}=\\cap_{j\\ge1}\\cap_{s>1}\\cap_{i\\in S_{j}}\\{|\\hat{p}_{i}^{j s}-\\mu_{i}|<\\epsilon_{j}/8\\}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "If $\\mathcal{E}_{3}$ and ${\\mathcal{E}}_{4}$ truly hold, we have that (1): for any $j$ and arm $i\\in S_{j}$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mu_{i}\\le\\hat{p}_{i}^{j}+\\frac{\\epsilon_{j}}{8}\\le\\hat{p}_{*}^{j}-\\frac{7\\epsilon_{j}}{8}\\le\\mu_{*}-\\frac{3\\epsilon_{j}}{4}\\le\\mu_{1}-\\frac{3\\epsilon_{j}}{4},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the second inequality is due to Line 11 of Algorithm 2; (2): for any fixed $j,\\;s$ , and any $r\\in U_{j}(s)$ , where $U_{j}(s)$ is defined in (C.2) and represents the set of all rounds in which parameter $\\ell_{j}$ is updated for exactly $s$ times, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\hat{p}_{i}^{j s}<\\mu_{i}+\\frac{\\epsilon_{j}}{8}\\le\\mu_{1}-\\frac{5\\epsilon_{j}}{8}\\le\\hat{p}_{1}^{r}-\\frac{\\epsilon_{j}}{2}\\le\\hat{p}_{*}^{r}-\\frac{\\epsilon_{j}}{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, if $\\mathcal{E}_{3}$ and ${\\mathcal{E}}_{4}$ truly hold, the event $\\hat{p}_{i}^{j s}<\\hat{p}_{*}^{r}-\\frac{\\epsilon_{j}}{2}$ consistently holds for all $j,\\,s,\\,i\\in S_{j}$ , and $r~\\in~U_{j}(s)$ . It\u2019s noteworthy that, according to (D.4), $\\mathbb{P}(\\mathcal{E}_{4})\\;\\geq\\;1\\,-\\,\\delta/16$ , and from (D.5), $\\mathbb{P}(\\mathcal{E}_{3})\\geq1-\\delta/8$ . Therefore, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Bigg(\\bigcap_{j\\geq1}\\bigcap_{s>1}\\bigcap_{r\\in U_{j}(s)}\\left\\{\\operatorname*{max}_{i\\in S_{j}}\\hat{p}_{i}^{j s}<\\hat{p}_{*}^{r}-\\frac{\\epsilon_{j}}{2}\\right\\}\\Bigg)\\geq1-\\frac{\\delta}{8}-\\frac{\\delta}{16}=1-\\frac{3\\delta}{16}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "D.3 Proof of Lemma C.3 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We first focus on bounding the number of pulls of arm $i$ within the first For Loop. Define ", "page_idx": 21}, {"type": "equation", "text": "$$\nr(i)=\\operatorname*{min}{\\left\\{r:\\epsilon_{r}<\\frac{\\Delta_{i}}{2}\\right\\}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "From Lemma D.1 and the union bound, for $r\\geq r(i)$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\bigg(\\displaystyle\\bigg\\{|\\hat{p}_{i}^{r}-\\mu_{i}|\\geq\\frac{\\epsilon_{r(i)}}{8}\\bigg\\}\\cup\\bigg\\{|\\hat{p}_{1}^{r}-\\mu_{1}|\\geq\\frac{\\epsilon_{r(i)}}{8}\\bigg\\}\\bigg)\\leq4\\exp\\bigg(-\\frac{2d_{r}\\epsilon_{r(i)}^{2}}{64}\\bigg)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq4(\\delta_{r})^{r-r(i)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq10^{r(i)-r}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Let $E_{i}^{r}$ be the event $|\\hat{p}_{i}^{r}-\\mu_{i}|\\leq\\epsilon_{r(i)}/8$ and $|\\hat{p}_{1}^{r}-\\mu_{1}|\\leq\\epsilon_{r(i)}/8$ truly hold at $r$ -th round. Conditioned on $E_{i}^{r}$ and $\\mathcal{E}$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\hat{p}_{i}^{r}\\le\\mu_{i}+\\frac{\\epsilon_{r}}8\\le\\mu_{1}-\\Delta_{i}+\\frac{\\epsilon_{r}}8\\le\\mu_{1}-\\frac{3\\epsilon_{r}}2\\le\\hat{p}_{1}^{r}-\\epsilon_{r}\\le\\hat{p}_{*}^{r}-\\epsilon_{r},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which mean arm $i$ will be eliminated. Therefore, the total sample cost of arm $i$ in the first For Loop of Algorithm 2 is ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{r\\le r_{i}}\\frac{32}{\\epsilon_{r}^{2}}\\log(2/\\delta_{r})+\\sum_{r>r(i)}10^{r-r(i)-1}\\frac{32}{\\epsilon_{r}^{2}}\\log(2/\\delta_{r})=\\mathcal{O}\\biggl(\\frac{\\log(1/\\delta_{r(i)})}{\\epsilon_{r(i)}^{2}}\\biggr)}}\\\\ &{}&{=\\mathcal{O}\\biggl(\\frac{\\log\\big(n/\\delta\\log(\\Delta_{i}^{-1})\\big)}{\\Delta_{i}^{2}}\\biggr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Consequently, if we define $H$ to be the total sample complexity in the first For Loop of Algorithm 2. Then we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}[H]=\\mathcal{O}\\bigg(\\sum_{i>1}\\frac{\\log\\big(n/\\delta\\log(\\Delta_{i}^{-1})\\big)}{\\Delta_{i}^{2}}\\bigg).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Now, we bound the sample complexity within the second For Loop. We let $L_{j}$ be the total number of pulls of arms in $S_{j}\\setminus\\bar{S}_{j+1}$ within the second For Loop. We let ", "page_idx": 21}, {"type": "equation", "text": "$$\nn_{j}=\\operatorname*{min}\\bigg\\{s\\in\\{0,1,2,\\cdots\\}:\\frac{B_{j}}{(\\delta_{j})^{2^{s}}\\cdot2^{s}}\\geq H\\bigg\\}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "As per Line 16, $n_{j}$ represents the total count of arm re-pulls in the set $S_{j}\\setminus S_{j+1}$ . Consequently, we can establish the following bound for $L_{j}$ . ", "page_idx": 21}, {"type": "equation", "text": "$$\nL_{j}\\le\\left(B_{j}-B_{j-1}\\right)\\sum_{s=0}^{n_{j}}2^{s}\\le B_{j}-B_{j-1}+\\delta_{j}H.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the first inequality is because $\\gamma_{j(s+1)}=\\gamma_{j s}^{2}$ and we pull the arms in $S_{j}/S_{j+1}$ for total ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sum_{i\\in S_{j}\\setminus S_{j+1}}{\\frac{32}{\\epsilon_{j}^{2}}}\\log\\left({\\frac{2}{\\gamma_{j s}}}\\right)\\leq\\sum_{i\\in S_{j}\\setminus S_{j+1}}{\\frac{32\\cdot2^{s}}{\\epsilon_{j}^{2}}}\\log\\left({\\frac{2}{\\delta_{j}}}\\right)=(B_{j}-B_{j-1})\\cdot2^{s}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "times at Line 18 and the last inequality is because for $n_{j}>1$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\delta_{j}H\\geq\\delta_{j}\\frac{B_{j}}{(\\delta_{j})^{2^{n_{j}-1}}\\cdot2^{n_{j}-1}}\\geq B_{j}2^{n_{j}+1},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the first inequality is due to the definition of $n_{j}$ . Therefore, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sum_{j\\geq1}L_{j}\\leq\\sum_{j>1}B_{j}+H\\sum_{j>1}\\delta_{j}\\leq2H.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Now, we conclude that conditioned on event $\\mathcal{E}$ , the total expected sample complexity is ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{O}\\bigg(\\sum_{i>1}\\frac{\\log\\big({n}/{\\delta}\\log(\\Delta_{i}^{-1})\\big)}{\\Delta_{i}^{2}}\\bigg).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "D.4 Proof of Lemma C.4 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "From Lemma D.1, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(|\\hat{p}_{i}^{r}-\\mu_{i}|\\geq\\frac{\\epsilon_{r}}{2}\\right)\\leq2\\exp\\bigg(-\\frac{2d_{r}\\epsilon_{r}^{2}}{4}\\bigg)\\leq(\\delta_{r})^{4}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Applying union bound, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\bigcup_{i\\in S_{r}}\\left\\{|\\hat{p}_{i}^{r}-\\mu_{i}|\\ge\\frac{\\epsilon_{r}}{2}\\right\\}\\right)\\le\\sum_{i\\in S_{r}}(\\delta_{r})^{4}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The aforementioned inequality suggests that, with a probability of at least $1-(\\delta_{r})^{3}$ , the condition $\\lvert\\hat{p}_{i}^{r}-\\mu_{i}\\rvert\\le\\epsilon_{r}/4$ consistently holds true for any $i\\in S_{r}$ . Then, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\hat{p}_{1}^{r}\\ge\\mu_{1}-\\frac{\\epsilon_{r}}{2}\\ge\\mu_{*}-\\frac{\\epsilon_{r}}{2}\\ge\\hat{p}_{*}^{r}-\\epsilon_{r}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\mathcal{E}^{r},(\\mathcal{E}^{r+1})^{c})\\leq(\\delta_{r})^{3}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For any fixed $j$ , recall the definition of $U_{j}(s)$ in (C.2). Assume $\\mathcal{E}^{r},(\\mathcal{E}^{r+1})^{c}$ truly hold and denote $U_{r}(s)=\\{r_{s}^{1},r_{s}^{2},\\cdot\\cdot\\cdot,r_{s}^{s^{\\prime}}\\}$ as the rounds where $\\ell_{r}=s$ . We note that at $r_{s}^{l}$ -th round, each arm within the first For Loop of Algorithm 2 is pulled at least ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{32\\cdot4^{s+l}}{\\epsilon_{r}^{2}}\\log\\left(\\frac{2}{\\delta_{r}}\\right)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "times. Besides, each arm within the second For Loop is pulled at least ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{32\\cdot2^{s}}{\\epsilon_{r}^{2}}\\log\\left(\\frac{2}{\\delta_{r}}\\right)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "times. Then, similar to (D.7), for $r_{s}^{l}$ -th round, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Bigg(\\cup_{i\\in S_{r_{s}^{l}}}\\left\\{|\\hat{p}_{i}^{r_{s}^{l}}-\\mu_{i}|\\geq\\frac{\\epsilon_{r}}{4}\\right\\}\\Bigg)\\leq2\\sum_{i\\in S_{r_{s}^{l}}}\\exp\\left(\\frac{-2\\epsilon_{r}^{2}}{16}\\cdot\\frac{32\\cdot4^{s+l}}{\\epsilon_{r}^{2}}\\log\\left(\\frac{2}{\\delta_{r}}\\right)\\right)\\leq\\sum_{i\\in S_{r_{s}^{l}}}(\\delta_{r}^{4})^{4^{s+l}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Then, we apply a union bound over all rounds in $U_{r}(s)$ , we obtain ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Bigg(\\cup_{l\\in[s^{\\prime}]}\\cup_{i\\in S_{r_{s}^{l}}}\\Bigg\\{|\\hat{p}_{i}^{r_{s}^{l}}-\\mu_{i}|\\ge\\frac{\\epsilon}{4}\\Bigg\\}\\Bigg)\\le(\\delta_{r}^{3})^{2^{s}}/2.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Moreover, if $\\mathcal{E}^{r},(\\mathcal{E}^{r+1})^{c}$ happens, the best arm is eliminated at $r$ -th round. Then for any rounds in $U_{r}(s)$ , we have pulled arm 1 for at least ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{32\\cdot2^{s}}{\\epsilon_{r}^{2}}\\log\\left(\\frac{2}{\\delta_{r}}\\right)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "times in the second For Loop. Recall that $\\hat{p}_{i}^{j\\,s}$ is the $s$ th updates of parameter $\\hat{p}_{i}^{j}$ within the second For Loop. From Lemma D.1, we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{P}\\bigg(\\hat{p}_{1}^{r s}\\leq\\mu_{1}-\\frac{\\epsilon_{r}}{4}\\bigg)\\leq\\exp\\bigg(-\\frac{2\\epsilon_{r}^{2}}{16}\\cdot\\frac{32\\cdot2^{s}}{\\epsilon_{r}^{2}}\\log\\bigg(\\frac{2}{\\delta_{r}}\\bigg)\\bigg)\\leq\\big(\\delta_{r}^{3}\\big)^{2^{s}}/2.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, if $\\mathcal{E}^{r},(\\mathcal{E}^{r+1})^{c}$ happens, with probability ", "page_idx": 22}, {"type": "equation", "text": "$$\n1-\\gamma_{r s}=1-\\big(\\delta_{r}^{3}\\big)^{2^{s-1}},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "it holds that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\cup_{l\\in[s^{\\prime}]}\\cup_{i\\in S_{r_{s}^{l}}}\\left\\{|\\hat{p}_{i}^{r_{s}^{l}}-\\mu_{i}|\\geq\\frac{\\epsilon}{4}\\right\\}\\!,\\;\\mathrm{and}\\;\\hat{p}_{1}^{r s}>\\mu_{1}-\\frac{\\epsilon}{4},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and thus for all $l\\in[s^{\\prime}]$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\hat{p}_{1}^{r s}\\ge\\mu_{1}-\\frac{\\epsilon}{4}\\ge\\hat{p}_{*}^{r_{s}^{l}}-\\frac{\\epsilon}{2},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which means Algorithm 2 returns at Line 22. Before we continue, we will first show that the number of pulls in the second For Loop is lower than the first For Loop. Assume the algorithm stops at $r^{\\prime}$ -th round. Let $s_{j}^{\\prime}=\\mathrm{max}\\{s:B_{r^{\\prime}}\\bar{\\gamma_{j s}}\\cdot2^{s}>B_{j}\\}$ . The number of pulls for $S_{j}\\setminus S_{j+1}$ at second For Loop is at most ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{s=1}^{s_{j}^{\\prime}}(B_{j}-B_{j-1})\\cdot2^{s}\\le\\displaystyle\\sum_{s=1}^{s_{j}^{\\prime}}B_{j}2^{s}}&{}\\\\ &{\\le B_{j}\\cdot2^{s_{j}^{\\prime}+1}}\\\\ &{=2^{2s_{j}^{\\prime}+1}\\gamma_{j s_{j}^{\\prime}}\\displaystyle\\frac{B_{j}}{2^{s_{j}^{\\prime}}\\gamma_{j s_{j}^{\\prime}}}}\\\\ &{\\le B_{r^{\\prime}}2^{2s_{j}^{\\prime}+1}(\\delta_{j})^{2^{s_{j}^{\\prime}}}}\\\\ &{\\le B_{r^{\\prime}}\\delta_{j}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore, the total number of pulls within the second For Loop is at most ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{j\\geq1}\\sum_{s=1}^{s_{j}^{\\prime}}\\left(B_{j}-B_{j-1}\\right)\\cdot2^{s}\\leq B_{r^{\\prime}}\\sum_{j>1}\\delta_{j}\\leq B_{r^{\\prime}},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which means the number of for the second For Loop is lower than the first For Loop. Finally, we obtain ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[N^{\\prime}\\,\\mathbb{1}\\{\\mathcal{E}^{r},(\\mathcal{E}^{r+1})^{c}\\}]\\leq\\mathbb{E}\\bigg(\\mathcal{O}\\bigg(\\displaystyle\\sum_{s=1}((\\delta_{r})^{3})^{2^{s-1}}\\cdot\\frac{B_{r}}{(\\delta_{r})^{2^{s}}2^{s}}\\bigg)\\bigg)}\\\\ &{\\phantom{=}=\\mathbb{E}[\\mathcal{O}(\\delta_{r}B_{r})]}\\\\ &{\\phantom{=}=\\mathcal{O}\\bigg(\\displaystyle\\sum_{i\\geq1}\\frac{\\delta_{r}\\log\\big(n/\\delta\\log(\\Delta_{i}^{-1})\\big)}{\\Delta_{i}^{2}}\\bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In the first inequality, we used the fact that if the algorithm stops in some rounds in $U_{r}(s)$ , the total number of pulls of all arms is at most $B_{r}/(\\gamma_{j s}\\cdot\\bar{2}^{s})+B_{r}/(\\gamma_{j s}\\cdot2^{s})$ , whcih comes from the first and second For Loop respectively. Moreover, the factor $((\\delta_{r})^{3})^{2^{s-1}}$ is because from (D.9), if $\\mathcal{E}^{r},(\\mathcal{E}^{r+1})^{c}$ holds, then Algorithm 2 returns in some rounds in $U_{r}(s)$ (at Line 22) with probability at least $1-((\\delta_{r})^{3})^{2^{s-1}}$ . The last equality is because from Lemma C.3, if $\\mathcal{E}^{r},(\\mathcal{E}^{r+1})^{c}$ holds, then ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}[B_{r}]\\leq\\mathcal{O}\\bigg(\\sum_{i>1}\\frac{\\log\\big(n/\\delta\\log(\\Delta_{i}^{-1})\\big)}{\\Delta_{i}^{2}}\\bigg).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Finally, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{r\\geq1}\\mathbb{E}[N^{\\prime}\\,\\mathbb{1}\\{\\mathcal{E}^{r},(\\mathcal{E}^{r+1})^{c}\\}]=\\mathcal{O}\\bigg(\\sum_{i>1}\\frac{\\log\\big(n/\\delta\\log(\\Delta_{i}^{-1})\\big)}{\\Delta_{i}^{2}}\\bigg).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "D.5 Proof of Lemma C.5 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We note that each round within the While Loop costs one batch. Let ", "page_idx": 23}, {"type": "equation", "text": "$$\nr(2)=\\operatorname*{min}{\\left\\{r:\\epsilon_{r}<\\frac{\\Delta_{2}}{2}\\right\\}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Let $E^{r}$ be the event ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\bigcap_{i>1}\\left\\{\\left\\{|\\hat{p}_{i}^{r}-\\mu_{i}|\\leq\\frac{\\epsilon_{r(2)}}{8}\\right\\}\\bigcap\\left\\{|\\hat{p}_{1}^{r}-\\mu_{1}|\\leq\\frac{\\epsilon_{r(2)}}{8}\\right\\}\\right\\}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Conditioned on $E^{r}$ and $\\mathcal{E}$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\hat{p}_{i}^{r}\\le\\mu_{i}+\\frac{\\epsilon_{r(2)}}{8}\\le\\mu_{1}-\\Delta_{i}-\\frac{\\epsilon_{r(2)}}{8}\\le\\hat{p}_{1}^{r}-\\epsilon_{r(2)}\\le\\hat{p}_{*}^{r}-\\epsilon_{r(2)},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which means all sub-optimal arms have been eliminated and the algorithm returns. From Lemma D.1 and the union bound, for $r\\geq r(2)$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{P}((E^{r})^{c})\\leq4n\\exp\\bigg(-\\frac{2d_{r}\\epsilon_{r(i)}^{2}}{64}\\bigg)\\leq4n(\\delta_{r})^{r-r(i)}\\leq10^{r(i)-r}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore, the total batch cost is ", "page_idx": 24}, {"type": "equation", "text": "$$\nr_{2}+\\sum_{r\\geq r_{2}}\\frac{1}{10^{r-r_{2}}}=O(r_{2})=O\\left(\\log\\left(\\frac{1}{\\Delta_{2}}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "This completes the proof. ", "page_idx": 24}, {"type": "text", "text": "D.6 Proof of Lemma C.6 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The proof of this lemma is similar to that of Lemma C.4. we have the following results. 1: From (D.8), we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\mathcal{E}^{r},(\\mathcal{E}^{r+1})^{c})\\leq(\\delta_{r})^{3}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "2: From (D.9), we have if $\\mathcal{E}^{r}$ and $(\\mathcal{E}^{r+1})^{c}$ truly hold, then with fixed $\\ell_{s}$ , Algorithm 2 returns at Line   \n22 with probability ", "page_idx": 24}, {"type": "equation", "text": "$$\n1-(\\delta_{r}^{3})^{2^{s-1}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We first compute the size of $U_{r}(s)$ . From Algorithm 2, we know that the arm kept in the set $S_{r}$ will be pulled 4 times larger compared to $(r-1)$ -th round. Besides, we update $\\ell_{r}$ to $s+1$ , if the number of pulls exceeds $B_{r}/((\\delta_{r})^{2^{s}}\\cdot2^{s})$ (Line 16 of Algorithm 2). It is easy to see after $\\log_{4}n$ rounds, the number of pulls of any arm ex ceeds $B_{r}$ and then after $\\mathrm{~n~}_{\\overline{{(\\delta_{r})^{2^{s}}\\cdot2^{s}}}}$ rounds, the number of pulls of single arm exceeds $B_{r}/((\\delta_{r})^{2^{s}}\\cdot2^{s})$ . Therefore, the size of $U_{r}(s)$ is at most ln $\\frac{1}{(\\delta_{r})^{2^{s}}\\!\\cdot\\!2^{s}}+\\ln n$ . We let $U$ be the total number of rounds used. Then, we obtain ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Xi[U\\cdot\\mathbb{1}\\{\\mathcal{E}^{r},(\\mathcal{E}^{r+1})^{c}\\}]\\leq r\\cdot\\mathbb{P}(\\mathcal{E}^{r},(\\mathcal{E}^{r+1})^{c})+\\mathbb{E}\\bigg(\\mathcal{O}\\bigg(\\sum_{s=1}((\\delta_{r})^{3})^{2^{s-1}}\\cdot\\bigg(\\ln n+\\ln\\frac{1}{(\\delta_{r})^{2^{s}}\\cdot2^{s}}\\bigg)\\bigg)\\bigg)}\\\\ &{\\qquad\\qquad\\qquad\\qquad<r\\mathbb{P}(\\mathcal{E}^{r},(\\mathcal{E}^{r+1})^{c})+\\delta_{r}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We have shown in Lemma C.5, $\\begin{array}{r}{\\sum_{r>1}r\\mathbb{P}(\\mathscr{E}^{r},(\\mathscr{E}^{r+1})^{c})\\;=\\;O(\\log(1/\\Delta_{2}))}\\end{array}$ . Therefore, the total number of rounds is ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{r\\geq1}^{\\infty}\\mathbb{E}[U\\cdot\\mathbb{1}\\{\\mathcal{E}^{r},(\\mathcal{E}^{r+1})^{c}\\}]=O(\\log(1/\\Delta_{2})),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which completes the proof. ", "page_idx": 24}, {"type": "text", "text": "E Experiments ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this section, we compare our algorithms Tri-BBAI and Opt-BBAI with Track-and-Stop [17], Top-k $\\delta$ -Elimination [22], ID-BAI [23] and CollabTopM [32] under bandit instances with Bernoulli rewards. All the experiments are repeated in 1000 trials. We perform all computations in Python on R9 5900HX for all our experiments. The implementation of this work can be found at https://github.com/panxulab/Optimal-Batched-Best-Arm-Identification ", "page_idx": 24}, {"type": "text", "text": "Data generation. For all experiments in this section, we set the number of arms $n=10$ , where each arm has Bernoulli reward distribution with mean $\\mu_{i}$ for $i\\in[10]$ . More specifically, the mean rewards are generated by the following two cases. ", "page_idx": 24}, {"type": "text", "text": "\u2022 Uniform: The best arm has $\\mu_{1}=0.5$ , and the mean rewards of the rest of the arms follow uniform distribution over [0.2, 0.4], i.e., $\\mu_{i}$ is uniformly generated from [0.2, 0.4] for $i\\in[n]\\setminus\\{1\\}$ . \u2022 Normal: The best arm has $\\mu_{1}=0.6$ , and the mean rewards of the rest of the arms are first generated from normal distribution $\\mathcal{N}(0.2,0.2)$ and then projected to the interval $[0,0.4]$ . ", "page_idx": 24}, {"type": "text", "text": "\u2022 Track-and-Stop [17] is a fully sequential algorithm and thus the only parameter that needs to be set is the $\\beta(t)$ function in the Chernoff\u2019s stopping condition (similar to Stage III of Algorithm 1). Note that the theoretical value of $\\beta(t)$ in Track-and-Stop [17] is of the same order as presented in our Theorem 3.1. However, they found that a smaller value works better in practice. Therefore, we follow their experiments to set $\\beta=\\log\\left((\\log(t)+1)/\\delta\\right)$ .   \n\u2022 Top-k $\\delta$ -Elimination is a batched algorithm that eliminates the arms in batches. It has two parameters $\\epsilon$ and $\\delta$ . In our experiments, we fix $\\epsilon=0.1$ .   \n\u2022 ID-BAI [23] is designed to identify the best arm among a set of bandit arms with optimal instancedependent sample complexity. We use the same algorithm setting of the original paper in our experiments.   \n\u2022 CollabTopM [32] is the algorithm to identify the Top- ${}^{m}$ arms within a multi-agent setting. We set the $m$ as 1 and the agents $K$ as 1.   \n\u2022 For Tri-BBAI and Opt-BBAI, we set $\\alpha=1.001^{7}$ , and $\\epsilon=0.01$ . We use the same $\\beta(t)$ function for Chernoff\u2019s stopping condition as in Track-and-Stop. Moreover, for the lengths of the batches, we set $L_{1}$ , $L_{2}$ and $L3$ to be the value calculated by Theorem 3.1. ", "page_idx": 25}, {"type": "text", "text": "Results. We present a comprehensive comparison on the sample complexities and batch complexities of our algorithms and baseline algorithms in Tables 3 and 4. Notably, our algorithms Tri-BBAI and Opt-BBAI, also including Top-K $\\delta$ Elimination and CollabTopM, require significantly fewer batches than Track-and-Stop. Furthermore, the sample complexity of Tri-BBAI and Opt-BBAI is significantly lower than that of Top-K $\\delta$ Elimination, ID-BAI and CollabTopM. Additionally, the sample complexity of Tri-BBAI and Opt-BBAI is comparable to Track and Stop when $\\delta$ is large, and it is at most 3.6 times greater than Track and Stop when $\\delta$ is very small. Additionally, we also provide the runtime comparison in Tables 3 and 4. Our algorithms have a significantly reduced runtime compared to Track-and-Stop, achieving nearly $1000\\times$ speedup. ", "page_idx": 25}, {"type": "table", "img_path": "ATSPPGEmAA/tmp/4f0b5ac301ee57d9d719051697de66f53c8814aadc5e3e3bb135ce681ce634e5.jpg", "table_caption": ["Table 3: Experimental results in terms of sample complexity, batch complexity and runtime under the uniform mean rewards. The number of arms is $n=10$ . The experiments are averaged over 1000 repetitions. "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "ATSPPGEmAA/tmp/2018dec948f763109e16a1a84cd7ab8cfe5cf75a2c8843ef66d1371b4ad4db39.jpg", "table_caption": ["Table 4: Experimental results in terms of sample complexity, batch complexity and runtime under the normal mean rewards. The number of arms is $n=10$ . The experiments are averaged over 1000 repetitions. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The main claims presented in the abstract and introduction accurately represent the contributions and scope of the paper. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: See the discussion in Section 5. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We provide the full set of assumptions and complete proofs. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 29}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer:[Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: We include all information needed to reproduce the main experimental results in Appendix E. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 29}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 29}, {"type": "text", "text": "Answer:[Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We have uploaded my source and will make the full code public if this work gets accepted. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 30}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We have included all the test details in Appendix E. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 30}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We report the error range in Tables 3 and 4. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We state all information on the computer resources in Appendix E. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 31}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The research conducted in the paper conforms, in every respect, to the NeurIPS Code of Ethics. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 31}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 31}, {"type": "text", "text": "Justification: There is no societal impact of the work performed. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 32}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 32}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: Our paper does not release new assets. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 33}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 33}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 33}]