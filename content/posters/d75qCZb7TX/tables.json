[{"figure_path": "d75qCZb7TX/tables/tables_4_1.jpg", "caption": "Table 1: Example hypotheses on model failure, justifications, and operationalizations generated by the SMART framework on a healthcare dataset. The p-values show whether the model's performance significantly differs from average performance with |\u2206Acc| measuring the effect size.", "description": "This table provides example hypotheses generated by the SMART testing framework. Each row represents a hypothesis about potential model failures, along with its justification, operationalization (how the hypothesis is tested on the data), p-value (indicating statistical significance of the failure), effect size (|\u2206Acc|), and whether the hypothesis is supported by the data.  The examples demonstrate the framework's capability to generate contextually relevant hypotheses and to test them using a self-falsification mechanism. This helps in identifying meaningful model failures.", "section": "4 SMART Testing"}, {"figure_path": "d75qCZb7TX/tables/tables_6_1.jpg", "caption": "Table 2: Summary of experiments and takeaways.", "description": "This table summarizes the various experiments conducted in the paper to evaluate SMART Testing and its properties.  Each row represents a specific experiment, highlighting the goal (e.g., assessing robustness to irrelevant features, measuring the false negative rate), the section in which it is described, and a concise takeaway summarizing the key findings.  The table demonstrates the breadth of evaluations performed to validate SMART's effectiveness and robustness across different scenarios and datasets.", "section": "5 Illustrating SMART use cases"}, {"figure_path": "d75qCZb7TX/tables/tables_7_1.jpg", "caption": "Table 3: Identifying slices with the highest performance discrepancies. We show differences in accuracies (|\u2206Acc|) between the top identified divergent slice and average performance across four classifiers (over 5 runs). \u2191 is better.", "description": "This table presents the results of an experiment designed to evaluate the generalizability of identified model failures across different machine learning models.  The study uses four different classifiers (Logistic Regression, SVM, XGBoost, and MLP) to identify slices (subgroups) within a dataset where the model's performance significantly differs from its average performance. The table shows the absolute difference in accuracy (|\u2206Acc|) between the top-performing slice (the slice with the largest performance discrepancy) and the overall average performance for each of the four classifiers, averaged across five independent runs. The results highlight the extent to which the discovered failures are consistent and generalizable across various model types.", "section": "5.2 Targeting model failures"}, {"figure_path": "d75qCZb7TX/tables/tables_8_1.jpg", "caption": "Table 4: False Negative Rate (FNR) for different methods at various settings. \u2193 is better.", "description": "This table presents the false negative rates (FNR) for various methods under different scenarios. The scenarios involve introducing synthetic underperformance into one, two, or three subgroups within a simulated dataset for a recidivism prediction task. The FNR represents the rate at which a testing method fails to identify these underperforming subgroups. Lower values indicate better performance.", "section": "5.3 Robustness to False Negatives"}, {"figure_path": "d75qCZb7TX/tables/tables_8_2.jpg", "caption": "Table 5: Proportion of times the corrupted minority subgroup is correctly identified as the top underperforming subgroup.", "description": "This table presents the results of an experiment designed to evaluate the robustness of SMART to potential LLM biases, specifically focusing on ethnicity-related biases.  The experiment simulates a dataset with a predictor function where performance is intentionally corrupted for a proportion (\u03c4) of a minority subgroup (either \"white\" or \"black\"). The table shows the proportion of times (averaged over 20 runs and 5 seeds) that SMART correctly identifies the corrupted minority subgroup as the top underperforming subgroup for different values of \u03c4. The results demonstrate SMART's ability to identify model underperformance even when LLMs exhibit inherent biases from training data.", "section": "5.4 Assessing and mitigating potential LLM challenges and biases"}, {"figure_path": "d75qCZb7TX/tables/tables_14_1.jpg", "caption": "Table 6: Comparison of ML testing paradigms in terms of how tests are defined, the type of search space (i.e. relevance based on context and requirements), sensitivity to multiple testing and automation to enable scalability.", "description": "This table compares different ML testing paradigms, including average testing, behavioral testing, data-only testing, and the proposed SMART testing method.  It evaluates each paradigm based on its objective, how tests are defined, whether it incorporates context and requirements into the search space, its susceptibility to the multiple testing problem, the degree of automation in the testing process, and the type of outputs generated. The table highlights the strengths and weaknesses of each paradigm in terms of its ability to comprehensively and efficiently assess the reliability and performance of machine learning models.", "section": "A Extended related work"}, {"figure_path": "d75qCZb7TX/tables/tables_15_1.jpg", "caption": "Table 7: Comparison of SMART testing with software testing works", "description": "This table compares SMART testing with two other software testing approaches: Christakis et al. [18] and Sharma et al. [19]. The comparison is made across three criteria: test case generation, the focus of testing, and whether tests are context-aware.  For each criterion, the table shows how SMART differs from the other two approaches.  SMART uses an LLM for automatic test case generation, while the other two approaches rely on pre-specified dimensions or approximate the black-box model.  SMART's focus is on identifying model failures, whereas the other approaches assess I/O functional correctness or model monotonicity. Finally, SMART uniquely incorporates context awareness into testing, resulting in more realistic assessments.", "section": "A Extended related work"}, {"figure_path": "d75qCZb7TX/tables/tables_16_1.jpg", "caption": "Table 8: Comparison of slice discovery methods", "description": "This table compares several slice discovery methods (SliceFinder, Pysubgroup, DivExplorer, Autostrat) against the proposed SMART Testing method, across various criteria.  It highlights SMART's advantages in incorporating domain knowledge, maintaining consistent discovery times, resisting irrelevant data, capturing rare slices, supporting logical OR operations, and resisting overfitting to training data.", "section": "A.3 Comparison of the features of slice discovery methods"}, {"figure_path": "d75qCZb7TX/tables/tables_20_1.jpg", "caption": "Table 2: Summary of experiments and takeaways.", "description": "This table summarizes the key experiments conducted in the paper to evaluate the proposed SMART testing framework and its comparison with existing data-only methods. Each row represents a specific experiment designed to assess a particular aspect of SMART, such as its robustness to false positives, ability to identify meaningful failures, generalization capabilities, and sensitivity to LLM biases. The \"Takeaway\" column provides a concise summary of the key findings from each experiment, highlighting the strengths of SMART compared to other methods.", "section": "5 Illustrating SMART use cases"}, {"figure_path": "d75qCZb7TX/tables/tables_22_1.jpg", "caption": "Table 9: Summary of the datasets used.", "description": "This table summarizes the characteristics of the seven datasets used in the paper's experiments.  For each dataset, it provides the name, the number of samples (rows of data), the number of features (columns of data), the domain from which the data originates (e.g., Finance, Healthcare), and the specific prediction task performed using that data (e.g., Loan default prediction, Breast cancer diagnosis).  The datasets vary in size and domain, reflecting the diversity of real-world applications of machine learning.", "section": "C Benchmarks & Experimental Details"}, {"figure_path": "d75qCZb7TX/tables/tables_30_1.jpg", "caption": "Table 10: Requirement satisfaction showing how many times the top 10 generated slices satisfied the requirements (Req) and how many of these slices had statistically significantly different performance from average (Sig) on a testing dataset. Maximum is 10. \u2191 is better.", "description": "This table presents a quantitative comparison of SMART and several data-only methods (Autostrat, PSG_B, PSG_A, Divexplorer, Slicefinder, Sliceline) in terms of their ability to satisfy three user-defined requirements while identifying statistically significant model failures.  The requirements relate to the inclusion of the 'age' variable in the top 10 slices, minimum and maximum sample sizes for those slices.  For each method and each requirement, the table shows the average number of times (out of a maximum of 10) that the requirement was satisfied and the average number of times (out of a maximum of 10) that the identified slices had statistically significant performance differences compared to the average performance across the dataset.", "section": "5.1 Robustness to False Positives"}, {"figure_path": "d75qCZb7TX/tables/tables_31_1.jpg", "caption": "Table 11: Number of slices identified (out of a maximum of 10) that had significantly divergent performance from average (higher is better). S_a counts the number of significantly divergent groups at a = 0.05; S_a/n applies the Bonferroni correction. \u2191 is better.", "description": "This table presents the results of an experiment evaluating the ability of different methods to identify statistically significant performance discrepancies in model predictions.  It compares SMART and several baseline methods across three datasets: the training dataset (DUK_train) and two test datasets (DUK_test and DUS_test). The comparison is done using two metrics: Sa (number of significantly divergent slices at \u03b1 = 0.05) and Sa/n (number of significantly divergent slices after Bonferroni correction). Higher numbers indicate better performance in identifying meaningful performance discrepancies.", "section": "D.2 Adaptive testing for a deployment environment"}, {"figure_path": "d75qCZb7TX/tables/tables_32_1.jpg", "caption": "Table 11: Number of slices identified (out of a maximum of 10) that had significantly divergent performance from average (higher is better). S_a counts the number of significantly divergent groups at \u03b1 = 0.05; S_\u03b1/n applies the Bonferroni correction. \u2191 is better.", "description": "This table presents the results of an experiment evaluating the ability of different methods to identify statistically significant performance discrepancies across various subgroups (slices) of data. It compares the number of significant slices identified by several methods (Autostrat, PSG_B, PSG_A, Divexplorer, Slicefinder, Sliceline, SMART_NSF_GPT4, SMART_GPT4, SMART_NSF_GPT3.5, SMART_GPT3.5) under two conditions: using the training dataset and a separate test dataset, and applying the Bonferroni correction to account for multiple comparisons.  The results are shown for different datasets (D_train, D_test, and D_US_test) representing different testing scenarios.  A higher number of significant slices indicates a better ability to identify model failures.", "section": "D.2 Adaptive testing for a deployment environment"}, {"figure_path": "d75qCZb7TX/tables/tables_32_2.jpg", "caption": "Table 13: The differences in accuracies between the top slice identified for each method on a testing dataset. The p-value computes the p-value associated with the difference in the accuracy. For the accuracy, higher values imply a greater ability to detect divergent slices (hence, higher is better). For the p-value, lower is better. Averages +- standard deviations are shown across 5 runs with random seeds and data splits.", "description": "This table presents the results of comparing different machine learning models' ability to identify slices with significant performance discrepancies.  The metrics used are the difference in accuracy between the top slice and the average accuracy across all slices, and the associated p-value. Higher accuracy differences and lower p-values indicate better performance.  The results are averaged over 5 runs with randomized data splits and seeds.", "section": "D.3.2 Performance across different models"}, {"figure_path": "d75qCZb7TX/tables/tables_33_1.jpg", "caption": "Table 14: Identifying slices with the highest performance discrepancies. We show differences in accuracies (Acc) between the top identified divergent slice and average performance across two state-of-the-art deep learning classifiers (over 5 runs) on the SEER dataset. \u2191 is better. 0.00 implies the evaluation method does not support the model.", "description": "This table compares the performance of different slice discovery methods in identifying subgroups with significant performance discrepancies for two state-of-the-art deep learning classifiers (TabPFN and TabNet) on the SEER dataset.  The values represent the difference in accuracy between the top-performing slice and the average accuracy across all slices. A higher value indicates better performance in identifying meaningful subgroups with large performance discrepancies.  The '0.00' entries show cases where a specific method does not support or is not applicable to the corresponding classifier.", "section": "D.3.2 Performance across different models"}, {"figure_path": "d75qCZb7TX/tables/tables_34_1.jpg", "caption": "Table 3: Identifying slices with the highest performance discrepancies. We show differences in accuracies (|\u2206Acc|) between the top identified divergent slice and average performance across four classifiers (over 5 runs). \u2191 is better.", "description": "This table presents the results of an experiment designed to evaluate the generalizability of identified model failures across different machine learning models.  Four different models (Logistic Regression, SVM, XGBoost, and MLP) were used to identify slices with the highest performance discrepancies. The table shows the absolute difference in accuracies (|\u2206Acc|) between the top-performing slice and the average performance across the four models, averaged over five runs. A higher |\u2206Acc| indicates a greater discrepancy, suggesting a more significant model failure.  The purpose is to demonstrate that SMART can identify model failures that generalize well to different models.", "section": "5.2 Targeting model failures"}, {"figure_path": "d75qCZb7TX/tables/tables_35_1.jpg", "caption": "Table 3: Identifying slices with the highest performance discrepancies. We show differences in accuracies (|\u2206Acc|) between the top identified divergent slice and average performance across four classifiers (over 5 runs). \u2191 is better.", "description": "This table presents the results of an experiment designed to evaluate the ability of different methods to identify data slices with significant performance discrepancies.  Four different machine learning models (Logistic Regression, SVM, XGBoost, and MLP) were used to analyze a prostate cancer dataset. For each model, the method that identified the slice with the greatest difference in accuracy between the slice and the average performance across the dataset is reported.  The table shows the mean accuracy difference and standard deviation across five runs for each method, highlighting which methods performed better in identifying these significant performance differences.", "section": "5.2 Targeting model failures"}, {"figure_path": "d75qCZb7TX/tables/tables_35_2.jpg", "caption": "Table 3: Identifying slices with the highest performance discrepancies. We show differences in accuracies (|\u2206Acc|) between the top identified divergent slice and average performance across four classifiers (over 5 runs). \u2191 is better.", "description": "This table compares the performance of four different machine learning models (Logistic Regression, SVM, XGBoost, and MLP) in identifying data slices with significant performance discrepancies.  The \"|\u2206Acc|\" column represents the absolute difference in accuracy between the top-performing slice and the average performance across all slices, indicating the magnitude of the model's failure.  The p-value indicates the statistical significance of this difference.  Higher |\u2206Acc| values and lower p-values are desirable, indicating more impactful failures.  The table shows that SMART consistently identifies slices with substantially larger performance discrepancies than other methods, suggesting its superior ability to locate meaningful model failures.", "section": "5.2 Targeting model failures"}, {"figure_path": "d75qCZb7TX/tables/tables_36_1.jpg", "caption": "Table 18: Number of discovered slices (false positives) in three data generating process scenarios. Average of 50 runs +- standard deviations is shown. Lower is better.", "description": "This table presents the results of an experiment designed to evaluate the performance of different model testing methods in identifying spurious model failures when there is no true underlying relationship between the variables. The experiment used three different data-generating processes to create datasets with varying characteristics. Each method's performance is assessed by counting the number of slices identified as containing model failures. The lower the number of discovered slices, the better the method's performance in avoiding false positives.", "section": "D.5 On the inductive biases of ML model testing"}, {"figure_path": "d75qCZb7TX/tables/tables_37_1.jpg", "caption": "Table 18: Number of discovered slices (false positives) in three data generating process scenarios. Average of 50 runs \u00b1 standard deviations is shown. Lower is better.", "description": "This table presents the results of an experiment evaluating the number of false positives generated by different model testing methods across three distinct data generation processes.  Lower numbers indicate fewer false positives, representing better performance. The methods compared include Autostrat, PSG_B, PSG_A, divexplorer, slicefinder, and SMART.  The three data generation processes, Suniform, Sskewed, and Sinteractions, varied in their underlying data distributions and relationships between variables. The SMART method is shown to consistently generate far fewer false positives than the other data-only methods, highlighting its superior performance in this aspect of model testing.", "section": "D.5 On the inductive biases of ML model testing"}, {"figure_path": "d75qCZb7TX/tables/tables_37_2.jpg", "caption": "Table 2: Summary of experiments and takeaways.", "description": "This table summarizes the various experiments conducted in the paper and their key takeaways.  Each row represents a specific experiment, detailing its goal (e.g., assessing robustness to false positives, evaluating performance in different deployment environments), the methods used, and the main conclusions drawn from the results.  The takeaways highlight key findings such as the effectiveness of the proposed method, SMART, compared to data-only methods in terms of identifying relevant and impactful model failures.", "section": "5 Illustrating SMART use cases"}, {"figure_path": "d75qCZb7TX/tables/tables_38_1.jpg", "caption": "Table 2: Summary of experiments and takeaways.", "description": "This table summarizes the key experiments conducted in the paper and their respective takeaways.  Each row represents a specific experiment designed to evaluate a particular aspect of the SMART Testing framework or to compare it against baseline methods. The \"Experiment\" column describes the goal of the experiment.  The \"Takeaway\" column concisely summarizes the key finding or conclusion from that experiment. The \"Sec.\" column references the section of the paper where the experiment and results are discussed in detail.", "section": "5 Illustrating SMART use cases"}, {"figure_path": "d75qCZb7TX/tables/tables_39_1.jpg", "caption": "Table 2: Summary of experiments and takeaways.", "description": "This table summarizes the various experiments conducted in the paper to evaluate the performance of SMART Testing and its comparison with several data-only baselines across various aspects of ML model testing. Each experiment aims to highlight a specific aspect or limitation of the existing data-only approaches and how SMART addresses them. The table shows the goal of each experiment, the specific evaluation measures used, and the main findings or takeaways obtained from each experiment.", "section": "5 Illustrating SMART use cases"}, {"figure_path": "d75qCZb7TX/tables/tables_39_2.jpg", "caption": "Table 3: Identifying slices with the highest performance discrepancies. We show differences in accuracies (|\u2206Acc|) between the top identified divergent slice and average performance across four classifiers (over 5 runs). \u2191 is better.", "description": "This table presents the results of an experiment evaluating the ability of different methods to identify model failures.  The experiment uses four different machine learning models (logistic regression, SVM, XGBoost, and MLP) and compares their performance on a specific \"slice\" of the data (a subgroup exhibiting the greatest difference in accuracy from average model performance) to their average performance across the entire dataset. The table demonstrates the superior performance of SMART and SMARTNSF (an ablation of SMART without self-falsification) in identifying slices with large performance discrepancies compared to other data-only methods, indicating better generalization and higher accuracy in detecting true model failures.", "section": "5.2 Targeting model failures"}, {"figure_path": "d75qCZb7TX/tables/tables_40_1.jpg", "caption": "Table 10: Requirement satisfaction showing how many times the top 10 generated slices satisfied the requirements (Req) and how many of these slices had statistically significantly different performance from average (Sig) on a testing dataset. Maximum is 10. \u2191 is better.", "description": "This table presents the results of an experiment evaluating how well different methods satisfy pre-defined requirements during model testing.  It compares SMART against several data-only baselines across three requirements. The first assesses whether the top 10 slices found include the variable \"age.\" The second and third specify minimum and maximum sample sizes, respectively, for those slices. The table shows the number of times each method satisfied each requirement (Req) and the number of those slices with statistically significant performance differences (Sig) from the average.  Higher numbers indicate better performance in fulfilling requirements and detecting meaningful model failures.", "section": "D.1 Requirements-constrained testing"}, {"figure_path": "d75qCZb7TX/tables/tables_40_2.jpg", "caption": "Table 10: Requirement satisfaction showing how many times the top 10 generated slices satisfied the requirements (Req) and how many of these slices had statistically significantly different performance from average (Sig) on a testing dataset. Maximum is 10. \u2191 is better.", "description": "This table presents a quantitative comparison of SMART against data-only methods.  It evaluates the ability of each method to fulfill three user-specified requirements (Req) while also identifying statistically significant performance differences (Sig) in the generated slices (subgroups within the data where model performance diverges from the average). The maximum score for both Req and Sig is 10, and higher scores indicate better performance. The results show SMART consistently outperforms data-only methods in satisfying requirements and identifying significant performance differences.", "section": "D.1 Requirements-constrained testing"}]