[{"figure_path": "d75qCZb7TX/figures/figures_4_1.jpg", "caption": "Figure 1: Overview of the SMART Testing Framework showing the four steps. All steps are automatically executed by an LLM.", "description": "This figure presents a flowchart illustrating the SMART Testing framework's four main steps: 1) Hypothesis Generation, where an LLM (Large Language Model) generates hypotheses about potential model failures using contextual information; 2) Operationalization, where these hypotheses are translated into testable forms using either LLM-based or data-driven methods; 3) Self-falsification, a mechanism where LLMs validate the generated hypotheses using available data and a self-falsification mechanism; 4) Reporting, where the results are compiled into a comprehensive model report that provides insights into the identified failures. The process is fully automated by an LLM, making it efficient and scalable.", "section": "4 SMART Testing"}, {"figure_path": "d75qCZb7TX/figures/figures_4_2.jpg", "caption": "Figure 2: SMART uses an LLM to integrate context C, and data context D<sub>c</sub>. Relevant and likely failure hypotheses are then generated by LLM (i.e. sampling mechanism). The hypotheses are then operationalized in D and evaluated. In contrast, data-only methods are not guided by context and requirements, searching more exhaustively in D for divergent slices.", "description": "This figure illustrates the SMART testing framework's hypothesis generation step.  It shows how a large language model (LLM) uses both external context (C<sub>e</sub>) and dataset context (D<sub>c</sub>) to generate relevant and likely failure hypotheses (H<sub>1</sub>...H<sub>N</sub>) about the model's performance.  This contrasts with data-only testing methods, which perform an exhaustive search across all possible subgroups without considering context, thus highlighting SMART's context-aware approach.", "section": "4 SMART Testing"}, {"figure_path": "d75qCZb7TX/figures/figures_5_1.jpg", "caption": "Figure 3: A self-falsification module within the SMART framework. A hypothesis generator l generates plausible hypotheses and justifications for when the model might fail. This is operationalized with g\u2081 and tested against the empirical data. The hypotheses based on statistical significance, with p-values based on the p < a/n criteria.", "description": "This figure illustrates the self-falsification module of the SMART testing framework. The module starts by generating hypotheses and justifications using a large language model (LLM). These hypotheses are then operationalized, meaning they are translated into testable conditions using a function g\u1d62.  The operationalized hypotheses are evaluated on the data to determine if they hold true (p < \u03b1/n) or are false (p > \u03b1/n).  Those that are falsified are rejected, while those that are not falsified (and meet specified criteria) represent potential model failures.", "section": "4 SMART Testing"}, {"figure_path": "d75qCZb7TX/figures/figures_7_1.jpg", "caption": "Figure 4: Contextual-awareness in SMART reduces FPs, i.e. reducing the proportion of irrelevant features in slices. SMART is not sensitive to the number of irrelevant features, unlike data-only methods. \u2193 is better", "description": "This figure shows the robustness of SMART and other data-only methods to false positives when dealing with irrelevant features.  The x-axis represents the increasing number of irrelevant features added to the datasets, while the y-axis shows the percentage of irrelevant features included in the slices identified by each method. The results demonstrate that SMART consistently avoids irrelevant features and its performance remains largely unaffected by the number of irrelevant features added, unlike data-only methods which demonstrate sensitivity to the number of irrelevant features, indicating that the contextual awareness embedded in SMART helps mitigate the issue of false positives.", "section": "5.1 Robustness to False Positives"}, {"figure_path": "d75qCZb7TX/figures/figures_15_1.jpg", "caption": "Figure 5: Contrasting paradigms of ML model testing along (i) Testing relevance which accounts for requirements and/or context and (ii) Degree of automation and adaptability when carrying out testing. We desire a new paradigm of ML testing to address both.", "description": "This figure compares different ML model testing paradigms based on two axes: Testing Relevance and Automation & Adaptability.  Testing Relevance refers to how well the testing approach considers requirements and contextual factors, with higher relevance indicating a more principled search for meaningful model failures. Automation & Adaptability describes how easily and automatically the testing method can be deployed and scaled to address various testing situations. The figure highlights that existing methods fall short in fully addressing both relevance and automation, while the proposed context-aware testing (CAT) aims to achieve both.", "section": "A Extended related work"}, {"figure_path": "d75qCZb7TX/figures/figures_16_1.jpg", "caption": "Figure 6: Components of ML Testing \u2014 (i) Test search, (ii) Operationalize, (iii) Evaluation report, with example approaches for each component.", "description": "This figure illustrates the three main steps in the ML testing pipeline: test search, operationalization, and evaluation.  The \"Test search\" step focuses on identifying what aspects of the model to test, which can be done through hypothesis generation (as in SMART Testing) or by searching for data-only divergences. The \"Operationalize\" step describes how to translate the tests into actions performed on the data.  This involves either utilizing sufficient data (as in SMART) or employing behavioral testing methods when data is scarce. Finally, the \"Evaluation report\" step focuses on assessing the model's performance, using the SMART Testing framework to generate comprehensive reports.", "section": "A.2 Components of the ML Testing pipeline"}, {"figure_path": "d75qCZb7TX/figures/figures_17_1.jpg", "caption": "Figure 7: A strong machine learning testing framework should incorporate textual understanding during testing, should meet requirements, should be automated, and provide contextual reports with an emphasis on model failures.", "description": "This figure illustrates the key components and workflow of the SMART Testing framework. It highlights the four main steps: contextual understanding, requirement-driven testing, automation, and contextual failure output.  The diagram shows how contextual knowledge and requirements guide the testing process, ensuring that the evaluation framework focuses on identifying meaningful model failures. The automation aspect reduces manual intervention, and the contextual failure output provides a clear summary report focusing on identified issues. This figure emphasizes the importance of incorporating external context and automating the testing process for more effective model evaluation.", "section": "B SMART Details"}, {"figure_path": "d75qCZb7TX/figures/figures_18_1.jpg", "caption": "Figure 1: Overview of the SMART Testing Framework showing the four steps. All steps are automatically executed by an LLM.", "description": "This figure shows a flowchart of the SMART testing framework.  The framework consists of four main steps: 1) Hypothesis Generation, where an LLM generates potential model failures based on provided context and dataset information; 2) Operationalization, where the hypotheses are translated into testable conditions on the dataset; 3) Evaluation, where the model's performance under the operationalized conditions is evaluated; and 4) Reporting, where a comprehensive report summarizing the findings is generated.  The figure highlights that all steps are automated by an LLM, reflecting the system's efficiency and automation.", "section": "4 SMART Testing"}, {"figure_path": "d75qCZb7TX/figures/figures_21_1.jpg", "caption": "Figure 1: Overview of the SMART Testing Framework showing the four steps. All steps are automatically executed by an LLM.", "description": "The figure illustrates the SMART testing framework, which is a four-step process: 1) Hypothesis Generation: This step uses an LLM to generate hypotheses regarding model failures based on the provided context and data. 2) Operationalization: This step transforms the natural language hypotheses into operational steps that can be executed using the training data. 3) Evaluation: This step evaluates the model's performance on the operationalized hypotheses, using a self-falsification mechanism to validate whether they are truly indicative of model failures. 4) Reporting: SMART generates a comprehensive report on the model's performance, including potential failures, their impact, and root causes.  All steps are automated using an LLM.", "section": "4 SMART Testing"}, {"figure_path": "d75qCZb7TX/figures/figures_28_1.jpg", "caption": "Figure 1: Overview of the SMART Testing Framework showing the four steps. All steps are automatically executed by an LLM.", "description": "The figure provides a flowchart representation of the SMART testing framework. It consists of four main stages: 1. Hypothesis generation, where large language models (LLMs) are used to create hypotheses about potential model failures; 2. Operationalization, which translates these hypotheses into testable conditions; 3. Evaluation, where the hypotheses are evaluated on the data using a self-falsification mechanism; and 4. Reporting, where a comprehensive report on the model's performance and the identified failure modes is generated. The process is fully automated, with each stage relying on the capabilities of LLMs.", "section": "4 SMART Testing"}, {"figure_path": "d75qCZb7TX/figures/figures_31_1.jpg", "caption": "Figure 8: Number of significant groups discovered (out of a total of 10) based on the training dataset size. SMART can operate under any sample size; self-falsification mechanism requires a larger sample size to falsify hypotheses. \u2191 is better.", "description": "This figure shows the number of significant groups (out of 10) discovered by different methods (Autostrat, PSG_A, PSG_B, Divexplorer, Sliceline, SMART_NSF, and SMART) across various training dataset sizes (10, 20, 50, 100, 200, 500). The results are shown separately for UK and US datasets. The upward-pointing arrow indicates that a higher number is better. The figure demonstrates that SMART consistently outperforms other methods in identifying significant failures, particularly with larger sample sizes.  The self-falsification mechanism in SMART requires a sufficient sample size, as the number of significant groups identified by SMART_NSF (without self-falsification) is higher than SMART for smaller datasets, while SMART surpasses it at larger sample sizes. This highlights the benefit of self-falsification for greater accuracy but also suggests the need for larger samples in applying it.", "section": "D.2 Adaptive testing for a deployment environment"}, {"figure_path": "d75qCZb7TX/figures/figures_37_1.jpg", "caption": "Figure 4: Contextual-awareness in SMART reduces FPs, i.e. reducing the proportion of irrelevant features in slices. SMART is not sensitive to the number of irrelevant features, unlike data-only methods. \u2193 is better", "description": "This figure demonstrates the robustness of SMART and data-only methods to false positives (FPs) when dealing with irrelevant features.  The experiment involves adding varying numbers of irrelevant, synthetically generated features to real-world datasets. SMART consistently maintains a low proportion of irrelevant features in its identified slices, while the FP rates of data-only methods increase significantly as the number of irrelevant features grows.  This shows that SMART\u2019s contextual awareness effectively mitigates the risk of identifying spurious failures due to the presence of irrelevant features.", "section": "5.1 Robustness to False Positives"}, {"figure_path": "d75qCZb7TX/figures/figures_38_1.jpg", "caption": "Figure 4: Contextual-awareness in SMART reduces FPs, i.e. reducing the proportion of irrelevant features in slices. SMART is not sensitive to the number of irrelevant features, unlike data-only methods. \u2193 is better.", "description": "This figure compares SMART with several data-only methods in terms of robustness to false positives in model testing.  The x-axis represents the number of irrelevant features added to the datasets. The y-axis represents the percentage of irrelevant features included in the slices identified by each method. The results show that SMART consistently avoids selecting slices containing irrelevant features, unlike data-only methods whose tendency to include irrelevant features increases dramatically as more are added. This demonstrates the advantage of using contextual information to guide the selection process.", "section": "5.1 Robustness to False Positives"}]