[{"figure_path": "ekK26cW5TB/figures/figures_1_1.jpg", "caption": "Figure 1: Statistic information of pixel number for each class in the Cityscapes training set and the performance of previous methods (DeepLabV3+, HRNet and STDC) compared to our method (AUCSeg). Our method aims to improve overall performance, particularly for tail classes. The dashed lines represent mIoU values for each class, while the solid lines represent the average mIoU for the head, middle, and tail classes.", "description": "This figure shows the pixel number distribution for each class in the Cityscapes dataset, categorized into head, middle, and tail classes based on the number of pixels. It also compares the performance (mIoU) of existing semantic segmentation methods (DeepLabV3+, HRNet, and STDC) with the proposed AUCSeg method.  The graph visually demonstrates the long-tail distribution problem where head classes have significantly more pixels than tail classes. AUCSeg aims to improve overall performance, especially addressing the performance gap for tail classes.", "section": "1 Introduction"}, {"figure_path": "ekK26cW5TB/figures/figures_4_1.jpg", "caption": "Figure 2: An overview of AUCSeg.", "description": "This figure provides a visual overview of the AUCSeg framework. It shows the different components of the system, including the T-Memory Bank, the AUC optimization process, and the theoretical result. The T-Memory Bank is used to address the problem of missing tail classes in each mini-batch. The AUC optimization process aims to maximize the AUC score. The theoretical result shows that the AUCSeg method is guaranteed to generalize well to unseen data.", "section": "4 AUC-Oriented Semantic Segmentation"}, {"figure_path": "ekK26cW5TB/figures/figures_6_1.jpg", "caption": "Figure 3: Instance-level and pixel-level task sampling.", "description": "This figure illustrates the difference in sampling strategies between instance-level classification and pixel-level semantic segmentation tasks, particularly highlighting the challenges posed by long-tailed distributions in the pixel-level scenario.  (a) shows instance-level sampling where stratified sampling ensures representation from all classes. (b) demonstrates the difficulty of stratified sampling in pixel-level tasks due to the indivisible nature of pixels and the resulting missing tail-class pixels in mini-batches. (c) introduces the Tail-class Memory Bank proposed by the authors as a solution to address the missing tail-class pixels problem by augmenting the mini-batches with historical class information.", "section": "4.3 Tail-class Memory Bank"}, {"figure_path": "ekK26cW5TB/figures/figures_9_1.jpg", "caption": "Figure 4: Qualitative results on the Cityscapes val set. Red rectangles highlight and magnify the image details in the lower left corner.", "description": "This figure showcases qualitative results of the proposed AUCSeg model on the Cityscapes validation set. It compares the input images with their ground truth segmentations and the results obtained from three different methods: DeepLabV3+, SegNeXt, and AUCSeg (the authors' method).  Red boxes highlight specific areas of interest, focusing on detailed segmentation of objects, particularly in complex scenes or instances where the model may struggle, to emphasize the improved performance of AUCSeg in areas that are challenging for other methods. This visual comparison aims to demonstrate the effectiveness of AUCSeg in producing more accurate and detailed semantic segmentation results, especially for smaller or less frequent object classes.", "section": "5 Experiments"}, {"figure_path": "ekK26cW5TB/figures/figures_9_2.jpg", "caption": "Figure 2: An overview of AUCSeg.", "description": "This figure shows a schematic overview of the proposed AUCSeg framework. It illustrates the main components of the model, including the encoder, decoder, T-Memory Bank, and the AUC optimization process.  The figure also highlights the theoretical results of the generalization bound for AUCSeg, emphasizing the key contributions and working mechanism of the model.", "section": "4 AUC-Oriented Semantic Segmentation"}, {"figure_path": "ekK26cW5TB/figures/figures_26_1.jpg", "caption": "Figure 2: An overview of AUCSeg.", "description": "This figure presents a schematic overview of the proposed AUCSeg framework. It illustrates the main components of the model, including the encoder, decoder, T-Memory Bank, and the AUC optimization process. The input image undergoes encoding and decoding to generate pixel-level predictions, which are then used in conjunction with the ground truth to optimize the AUC loss function. The T-Memory Bank plays a crucial role in addressing the issues related to the memory demands of AUC optimization. It stores and retrieves historical class information to ensure efficient optimization and manages the significant memory demand.", "section": "4 AUC-Oriented Semantic Segmentation"}, {"figure_path": "ekK26cW5TB/figures/figures_32_1.jpg", "caption": "Figure 6: Per-tail-class results on Cityscapes val set. The tail class names are listed from left to right according to the ascending number of training samples in the dataset, with \u2018motorcycles\u2019 containing the fewest.", "description": "This figure shows the performance of different semantic segmentation models on the Cityscapes validation set, broken down by individual tail classes.  The tail classes are ordered from left to right based on the number of training samples available for each, with \"motorcycles\" having the fewest samples.  The graph compares the overall and per-tail-class mIoU (mean Intersection over Union) scores for several models including DeepLabV3+, EMANet, STDC, SegNeXt, and the proposed AUCSeg model.  It visually demonstrates the effectiveness of AUCSeg in improving the performance of semantic segmentation on tail classes, which tend to be underrepresented in datasets.", "section": "G.1 Per-tail-class Results"}, {"figure_path": "ekK26cW5TB/figures/figures_33_1.jpg", "caption": "Figure 4: Qualitative results on the Cityscapes val set. Red rectangles highlight and magnify the image details in the lower left corner.", "description": "This figure shows qualitative results of semantic segmentation on the Cityscapes validation set.  It compares the segmentation results of DeepLabV3+, SegNeXt, and the proposed AUCSeg method. Red boxes highlight areas where the differences between the methods are most apparent, allowing for detailed visual comparison of their performances on various image regions.", "section": "G.3 More Qualitative Results"}]