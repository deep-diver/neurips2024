[{"figure_path": "RMfiqfWAWg/tables/tables_5_1.jpg", "caption": "Table 1: Performance on single-task scenarios. Bold numbers indicate the best-performing model transferred from the same size. Underlines indicate whether the method outperforms the expert model being used. Notably, we are unable to obtain the LoRA adapter for LLAMA2-chat version. Therefore, we set the LoRA Tuning for the 13B model on TruthfulQA to match the same values as Full Fine-Tuning, e.g., 61.93.", "description": "This table presents the results of single-task experiments, comparing the performance of different methods on five datasets: GSM8K, TruthfulQA, TriviaQA, CNN/DM, and MMLU.  The methods compared include using the base model, full fine-tuning, LoRA tuning, and the proposed dynamic logit fusion method. The table shows the average performance across all five tasks, highlighting the superior performance of the proposed approach, particularly when transferring knowledge from smaller models (7B and 1.1B) to a larger 13B model. Bold numbers indicate the best result achieved by transferring from a model of a particular size, and underlines show when a method outperforms its corresponding expert model.  Note that a special case is handled for the TruthfulQA dataset, as a LoRA adapter for the 13B model could not be obtained.", "section": "4 Experiments"}, {"figure_path": "RMfiqfWAWg/tables/tables_5_2.jpg", "caption": "Table 1: Performance on single-task scenarios. Bold numbers indicate the best-performing model transferred from the same size. Underlines indicate whether the method outperforms the expert model being used. Notably, we are unable to obtain the LoRA adapter for LLAMA2-chat version. Therefore, we set the LoRA Tuning for the 13B model on TruthfulQA to match the same values as Full Fine-Tuning, e.g., 61.93.", "description": "This table presents the results of single-task experiments, comparing different methods for transferring knowledge from smaller models (1.1B and 7B parameters) to a larger model (13B parameters).  It shows the performance across five datasets (GSM8K, TruthfulQA, TriviaQA, CNN/DM, MMLU) for three approaches: full fine-tuning, LoRA tuning, and the proposed dynamic logit fusion method.  Bold numbers highlight the best performance achieved using models of the same size (1.1B or 7B) as the source expert. Underlines show when the proposed method outperforms the source expert model.  Due to the unavailability of a LoRA adapter for the 13B model on the TruthfulQA dataset, the LoRA tuning result is set equal to the full fine-tuning result for consistency.", "section": "4 Experiments"}, {"figure_path": "RMfiqfWAWg/tables/tables_7_1.jpg", "caption": "Table 3: The time required to train or inference 1,000 data points on a single GPU. In the inference section, values in parentheses show the factor by which the inference speed is slower compared to the 13B FFT model. \"FFT\" denotes Full Fine-Tuning, and \"LT\" denotes LORA Tuning. Our 1.1B/7B expert model use full fine-tuning.", "description": "This table compares the training and inference time of different methods for a large language model (13B) on a single GPU using 1000 data points.  It shows a significant reduction in training time when using the proposed method compared to full fine-tuning (FFT) and LoRA tuning (LT). The inference time of the proposed method is slightly higher than full fine-tuning and LoRA tuning, but the increase is relatively small, considering the substantial reduction in training time.", "section": "4 Experiments"}, {"figure_path": "RMfiqfWAWg/tables/tables_21_1.jpg", "caption": "Table 1: Performance on single-task scenarios. Bold numbers indicate the best-performing model transferred from the same size. Underlines indicate whether the method outperforms the expert model being used. Notably, we are unable to obtain the LoRA adapter for LLAMA2-chat version. Therefore, we set the LoRA Tuning for the 13B model on TruthfulQA to match the same values as Full Fine-Tuning, e.g., 61.93.", "description": "This table presents the performance comparison of different methods on five single-task scenarios: GSM8K, TruthfulQA, TriviaQA, CNN/DM, and MMLU.  It compares the performance of using a base model, full fine-tuning the 13B model, LoRA tuning the 13B model, transferring knowledge from a 1.1B fine-tuned model, and the proposed method transferring from both 1.1B and 7B fine-tuned models.  Bold numbers highlight the best performance for models of the same size (1.1B or 7B transfer), and underlines show when the proposed method outperforms the expert (fine-tuned) small model.", "section": "4 Experiments"}, {"figure_path": "RMfiqfWAWg/tables/tables_21_2.jpg", "caption": "Table 1: Performance on single-task scenarios. Bold numbers indicate the best-performing model transferred from the same size. Underlines indicate whether the method outperforms the expert model being used. Notably, we are unable to obtain the LoRA adapter for LLAMA2-chat version. Therefore, we set the LoRA Tuning for the 13B model on TruthfulQA to match the same values as Full Fine-Tuning, e.g., 61.93.", "description": "This table presents the results of single-task experiments comparing different methods: a base 13B model, a fully fine-tuned 13B model, a LoRA-tuned 13B model, and the proposed method transferring knowledge from smaller 1.1B and 7B models.  The performance is evaluated across five datasets (GSM8K, TruthfulQA, TriviaQA, CNN/DM, and MMLU) using metrics relevant to each dataset (e.g., Exact Match, Accuracy, Rouge-2).  The table highlights the superiority of the proposed method by bolding the best results within each model size and underlining instances where the new method surpasses the corresponding expert model's performance.", "section": "4 Experiments"}, {"figure_path": "RMfiqfWAWg/tables/tables_21_3.jpg", "caption": "Table 1: Performance on single-task scenarios. Bold numbers indicate the best-performing model transferred from the same size. Underlines indicate whether the method outperforms the expert model being used. Notably, we are unable to obtain the LoRA adapter for LLAMA2-chat version. Therefore, we set the LoRA Tuning for the 13B model on TruthfulQA to match the same values as Full Fine-Tuning, e.g., 61.93.", "description": "This table presents the results of single-task experiments using different models and methods.  It compares the performance of several approaches, including full fine-tuning of a 13B model, LoRA tuning, a proxy tuning method, and the proposed dynamic logit fusion method. Results are shown for different tasks such as GSM8K (exact match), TruthfulQA (accuracy), TriviaQA (exact match), CNN/DailyMail (ROUGE-2), and MMLU (accuracy).  The table highlights which methods outperform the smaller expert models used for knowledge transfer and shows the average performance across all tasks.  The note about the LoRA adapter setting explains a detail about data handling in one of the experiments.", "section": "4 Experiments"}]