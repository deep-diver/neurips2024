[{"type": "text", "text": "Can Graph Neural Networks Expose Training Data Properties? An Efficient Risk Assessment Approach ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Hanyang $\\mathbf{Yuan}^{1,2,4\\dagger\\ddagger}$ , Jiarong $\\mathbf{X}\\mathbf{u}^{2*}$ , Renhong Huang Mingli $\\mathbf{Song^{1,4\\ddagger}}$ , Chunping $\\mathbf{Wang^{3}}$ , Yang Yang1 ", "page_idx": 0}, {"type": "text", "text": "1Zhejiang University, 2Fudan University, 3Finvolution Group 4Hangzhou High-Tech Zone (Binjiang) Institute of Blockchain and Data Security {yuanhanyang, renh2, brooksong, yangya}@zju.edu.cn jiarongxu@fudan.edu.cn, wangchunping02@xinye.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Graph neural networks (GNNs) have attracted considerable attention due to their diverse applications. However, the scarcity and quality limitations of graph data present challenges to their training process in practical settings. To facilitate the development of effective GNNs, companies and researchers often seek external collaboration. Yet, directly sharing data raises privacy concerns, motivating data owners to train GNNs on their private graphs and share the trained models. Unfortunately, these models may still inadvertently disclose sensitive properties of their training graphs (e.g., average default rate in a transaction network), leading to severe consequences for data owners. In this work, we study graph property inference attack to identify the risk of sensitive property information leakage from shared models. Existing approaches typically train numerous shadow models for developing such attack, which is computationally intensive and impractical. To address this issue, we propose an efficient graph property inference attack by leveraging model approximation techniques. Our method only requires training a small set of models on graphs, while generating a sufficient number of approximated shadow models for attacks. To enhance diversity while reducing errors in the approximated models, we apply edit distance to quantify the diversity within a group of approximated models and introduce a theoretically guaranteed criterion to evaluate each model\u2019s error. Subsequently, we propose a novel selection mechanism to ensure that the retained approximated models achieve high diversity and low error. Extensive experiments across six real-world scenarios demonstrate our method\u2019s substantial improvement, with average increases of $2.7\\%$ in attack accuracy and $4.1\\%$ in ROC-AUC, while being $6.5\\times$ faster compared to the best baseline. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Graph data, encapsulating relationships between entities across various domains such as social networks, molecular networks, and transaction networks, holds immense value [1\u20133]. Graph neural networks (GNNs) have proven effective in modeling graph data [4\u20136], yielding promising results across diverse applications, including recommender systems [7], molecular prediction [8, 9], and anomaly detection [10]. While training high-quality GNN models may necessitate a substantial amount of data, graphs may be scarce or of low quality in practice [11, 12], prompting companies and researchers to seek additional data from external sources [13]. ", "page_idx": 0}, {"type": "text", "text": "However, directly obtaining data from other sources is often difficult due to privacy concerns [14, 15]. As an alternative, sharing models rather than raw data has become increasingly common [13]. Typically, data owners train a model on their own data and subsequently release it to the community or collaborators [16\u201320]. For instance, a larger bank may train a fraud detection model on its extensive transaction network and share it with partners, allowing them to use their own customer data to identify risks. ", "page_idx": 1}, {"type": "text", "text": "Despite the beneftis, this model-sharing strategy sometimes remains vulnerable to data leakage risks. Given access to the released model, one may infer sensitive properties of the data owner\u2019s graph, which are not intended to be shared. In the context of releasing a fraud detection model, if an adversarial bank can determine the average default rate of all customers in the transaction network, the data owner bank\u2019s financial status can potentially be revealed. Another example is releasing a recommendation model trained on a company\u2019s product network [21]. If a competitor can infer the distribution of co-purchase links between different products, he may determine which items are frequently promoted together and deduce the company\u2019s marketing tactics. Such attacks are possible because released models may inadvertently retain and expose sensitive information from the training data [22, 23]. We refer to such sensitive information related to the global distribution in a graph as graph sensitive properties, and we aim to investigate the problem of graph property inference attack. ", "page_idx": 1}, {"type": "text", "text": "Previous property inference attacks [22\u201325] primarily focus on text or image data, assuming models trained on different properties exhibit differences in parameters or outputs. For GNNs modeling graph data, the inherent relationships and message-passing mechanisms can magnify distribution bias [26], making them more vulnerable to attacks. Although a few studies extend property inference to graphs and GNNs [20, 21, 27], they typically involve creating shadow models that replicate the released model\u2019s architecture and are trained on shadow graphs with varying sensitive properties. The parameters or outputs of shadow models are used to train an attack model to classify the property of the data owner\u2019s graph. A major limitation of these attacks is the need to train a large number of shadow models (e.g., 4,096 models [22], 1,600 models [27]), resulting in significant computational cost and low efficiency. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we explore the feasibility of avoiding the training of numerous shadow models by designing an efficient yet effective graph property inference attack. Our key insight is to train only a small set of models and then generate sufficient approximated shadow models to support the attack. To this end, we leverage and extend model approximation techniques. For a given dataset and a model trained on it, when the training data changes (e.g., removing a sample), model approximation allows the efficient estimation of new model parameters for the updated dataset without retraining. This technique, often called unlearning [28\u201331], enables the efficient generation of multiple approximated shadow models from a single trained model. Specifically, given a small set of graphs and their corresponding trained models, we perturb each graph to alter sensitive properties (e.g., changing the number of nodes corresponding to high default rate users) and then apply model approximation to produce a sufficient number of approximated models corresponding to the perturbations, thereby reducing the total attack cost. Figure 1 illustrates our approach compared to the traditional attack. ", "page_idx": 1}, {"type": "text", "text": "Nevertheless, achieving this goal presents several challenges. The first challenge is to ensure the diversity of approximated models, which provides a broader range of training samples for the attack model and enhances its generalization capability. To tackle this, we develop structure-aware random walk sampling graphs from distinctive communities and introduce edit distance to quantify the diversity of a set of approximated models. The second challenge is to ensure that the errors in the approximated shadow models are sufficiently small. Otherwise, these models may fail to accurately reflect differences in graph properties, thereby diminishing attack performance. To address this, we establish that different graph perturbations can lead to varying approximation errors, which offers a theoretical-guaranteed criterion for assessing the errors of each approximated model. Finally, we propose a novel selection mechanism to reduce errors while enhancing the diversity of approximated models, formulated as an efficiently solvable programming problem. Our contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose an efficient and effective graph property inference attack that requires training only a few models to generate sufficient approximated models for the attack. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose a novel selection mechanism to retain approximated models with high diversity and low error, using edit distance to measure the diversity of approximated models and a theoretical criterion for assessing the errors of each. This diversity-error optimization is formulated as an efficiently solvable programming problem. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Experiments on six real-world scenarios demonstrate the efficiency and effectiveness of our proposed attack method. On average, existing attacks require training 700 shadow models to achieve $67.3\\%$ accuracy and $63.6\\%$ ROC-AUC, whereas our method trains only 66.7 models and obtains others by approximation, achieving $69.0\\%$ attack accuracy and $66.4\\%$ ROC-AUC. ", "page_idx": 2}, {"type": "text", "text": "2 Problem Definition ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The scenario of a graph property inference attack first involves a data owner who trains a GNN model using his graph data, referred to as the target model and target graph in the following text. Once trained, the target model\u2019s parameters or output posterior probabilities can be released in communities [16, 18]. For example, the data owner can upload the pre-trained parameters to GitHub [17, 19] to facilitate downstream tasks. Or he may upload the posterior probabilities from a recommender system to a third-party online optimization solver [21], such as Gurobi \\*. Collaborative machine learning is another potential attack scenario [20]. For instance, consider two banks aiming to collaboratively train a fraud detection model. While sharing raw transaction networks poses risks to privacy and commercial confidentiality, they can employ model-sharing strategies [13]. ", "page_idx": 2}, {"type": "text", "text": "With access to the target model, curious users may launch inference attacks to obtain some sensitive properties of the target graph, which can reveal secrets not intended to be shared. For example, the ratio of co-purchase links between particular products in a product network may relate to the promoting tactics of a sales company, or the average default rate in a transaction network may reveal the financial status of a bank. Such confidential information may further impact business competition. ", "page_idx": 2}, {"type": "text", "text": "In the rest of this section, we first define the privacy, i.e., the sensitive properties referred to in this work. Then, we introduce the knowledge of the attack. Finally, we formulate the problem of property inference attacks. ", "page_idx": 2}, {"type": "text", "text": "Graph sensitive property. In this paper, we consider an attributed target graph where nodes are associated with multiple attributes. The sensitive property is defined based on one specific type of attribute, called the property attribute. Specifically, the sensitive property is defined as a certain statistical value of the property attribute\u2019s distribution. We consider two types of properties that the attacker may infer: (1) node properties, specified by the ratio of nodes with a particular property attribute value, and (2) link properties, specified by the ratio of links where the end nodes have particular property attribute values. ", "page_idx": 2}, {"type": "text", "text": "Note that the property attribute can be either discrete or continuous. For instance, a node property defined on the discrete category attribute in a product network can be the ratio of co-purchase links between luxury items. An edge property defined on the continuous default rate attribute in a transaction network can be the average default rate of all customers. The inferred sensitive properties may reveal data owner\u2019s secrets such as commercial strategies; see [23, 32] for further discussion. ", "page_idx": 2}, {"type": "text", "text": "Attacker\u2019s knowledge. The attacker\u2019s background knowledge is assumed to be as follows: ", "page_idx": 2}, {"type": "text", "text": "\u2022 Auxiliary graph: We assume the attacker has an auxiliary graph from the same domain as the target graph but does not necessarily intersect with the latter. In practice, the auxiliary graph can be sourced from publicly available data or derived directly from the adversary\u2019s own knowledge. \u2022 Target model: We consider two types of knowledge on the target model: the white-box setting, where the adversary knows the architecture and parameters of the target GNN, and the black-box setting, where the adversary only knows the target GNN\u2019s output posterior probabilities. ", "page_idx": 2}, {"type": "text", "text": "Property inference attack. Formally, let $G^{\\mathrm{tar}}$ denote the target graph. And let $\\mathcal{P}(G^{\\mathrm{tar}})$ denote the property value of $G^{\\mathrm{tar}}$ . Note that $\\mathcal{P}$ can represent either node properties or link properties. Given that the attacker has an auxiliary graph $G^{\\mathrm{aux}}$ from the same domain as $G^{\\mathrm{tar}}$ , we define graph property inference attack as follows: ", "page_idx": 2}, {"type": "text", "text": "Problem 1 (Graph property inference attack) Given the auxiliary graph $G^{\\mathrm{aux}}$ , and assume the attacker has either the white-box knowledge of the target GNN parameters or the black-box knowledge of target GNN\u2019s output posterior probabilities, the objective of the graph property inference attack is to infer the property $\\mathcal{P}(G^{\\mathrm{tar}})$ without access to it. ", "page_idx": 2}, {"type": "image", "img_path": "Luxk3z1tSG/tmp/b9268b87d61a045e35a25f664e86fce5a86d9f9aa80dcbaa8e76804fbd944919.jpg", "img_caption": ["(b) Proposed property inference attack "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 1: Illustrations of (a) conventional graph property inference attacks and (b) the proposed attack, with yellow shading indicating model training, the main source of computational cost. ", "page_idx": 3}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "This section provides a detailed description of the proposed graph property inference attack. We start with an overview of our method and then delve into the technical aspects of model approximation. Finally, we describe how to ensure the diversity of approximated models while reducing their errors. The overall algorithm and complexity analysis are summarized in Appendix A.3. ", "page_idx": 3}, {"type": "text", "text": "3.1 Overview ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "As shown in Figure 1(a), given the auxiliary graph, the conventional approach is to first sample numerous shadow graphs, ensuring that graphs with different properties are adequately represented. Each is then used to train a shadow model with the same structure as the target model. Once trained, parameters (white-box) or output posterior probabilities (black-box) of shadow models are collected, along with the corresponding properties of shadow graphs. Finally, an attack model (e.g. linear classifier) is trained to classify properties based on parameters or posteriors. Since the number of shadow models is usually hundreds or thousands, their training can be computationally expensive. ", "page_idx": 3}, {"type": "text", "text": "To mitigate this issue, our proposed attack method utilizes model approximation techniques as a substitute. We provide a illustration of our method in Figure 1(b): ", "page_idx": 3}, {"type": "text", "text": "(1) Instead of numerous shadow graphs, we first sample only a few reference graphs. (2) On each reference graph, we train a reference model with the same architecture as the target model, and generate multiple augmented graphs by removing different nodes and edges. (3) By efficient model approximation, we obtain approximated models $w.r t$ to the augmented graphs. (4) We collect parameters or posteriors of all approximated models and train the attack model in a similar manner as previous attacks. ", "page_idx": 3}, {"type": "text", "text": "Here, we mainly face two challenges: ensuring that the approximate error associated with augmentations is relatively small, and ensuring that the approximated models are sufficiently diverse. To address them, we derive a theoretical criterion for calculating approximation errors across different augmented graphs (see $\\S\\ 3.2\\rangle$ ) and design a diversity enhancement strategy in $\\S\\ 3.3$ . ", "page_idx": 3}, {"type": "text", "text": "3.2 Model approximation and error analysis ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We proceed by introducing the techniques of model approximation, which include generating augmented graphs, obtaining approximated models, and conducting theoretical analysis for error criterion. ", "page_idx": 3}, {"type": "text", "text": "Generating augmented graphs and identifying influenced nodes. First, we aim to ensure that multiple perturbations produce distinctive augmented graphs. This is essential because highly similar augmentations reduce the distinction in the corresponding graph properties and model features, providing minimal benefit to the overall attack. For this purpose, we propose removing both nodes and edges from the reference graph. Formally, Let the reference graph be denoted by $\\bar{G}^{\\mathrm{ref}}=(V,E)$ sampled from $G^{\\mathrm{aux}}$ , where $V$ is the node set, $E\\subseteq V\\times V$ is the edge set. For one perturbation, we remove $V^{\\mathrm{{R}}}\\subset V$ and $E^{\\mathrm{R}}\\subset E$ to obtain the augmented graph $G^{\\mathrm{aug}}$ . In GNNs, the neighborhood aggregation makes the removal inevitably influence the state of other remaining nodes. Given a $l$ -layer GNN, the influenced nodes of removing a single node $v\\in V^{R}$ is the $l$ -hop neighborhood of $v$ , denote as $\\mathcal{N}_{l}(v)$ . And the influenced nodes of removing a single edge $e\\in E^{\\mathrm{R}}$ , connecting nodes $v$ and $u$ , is denoted as $\\mathcal{N}_{l}(e)=\\mathcal{N}_{l-1}(v)\\cup\\mathcal{N}_{l-1}(u)\\cup\\{\\bar{v^{\\prime}},u\\}$ . With these in mind, we next define the total influenced nodes for removing $V^{\\mathrm{R}}$ and $E^{\\mathrm{R}}$ . ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Definition 1 (Influenced nodes ) Given the removed nodes $V^{\\mathrm{R}}$ , removed edges $E^{\\mathrm{R}}$ and a $l$ -layer GNN, the total influenced nodes $V^{\\mathrm{I}}$ is defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\nV^{\\mathrm{I}}=\\bigcup_{e\\in E^{\\mathrm{R}}}\\mathcal{N}_{l}(e)\\bigcup_{v\\in V^{\\mathrm{R}}}\\mathcal{N}_{l}(v).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Note that $V^{\\mathrm{I}}$ is exclusive of $V^{\\mathrm{R}}$ ; we omit the set difference for simplicity. ", "page_idx": 4}, {"type": "text", "text": "Generating Approximated Models. Subsequently, we generate the approximated model based on the perturbation. While existing graph unlearning [29\u201331, 33] may offer potential solutions, they are either limited to specific model architectures or only support the removal of nodes or edges individually, making them unsuitable for direct application. To address this, we extend their mechanisms to suit our scenario. Let the reference model be parameterized by $\\theta^{\\mathrm{ref}}\\in\\mathbb{R}^{m}$ . In this paper, we consider cross-entropy loss as the loss function, and $\\,\\,{\\overline{{\\theta^{\\mathrm{ref}}}}}$ is obtained as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\theta^{\\mathrm{ref}}=\\arg\\operatorname*{min}_{\\theta}\\sum_{v\\in V}\\ell(\\theta;v,E).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "After removing $V^{\\mathrm{R}}$ and $E^{\\mathrm{R}}$ , directly retraining on $G^{\\mathrm{aug}}$ could yield a new model parameter $\\theta^{\\mathrm{aug}}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\theta^{\\mathrm{aug}}=\\arg\\operatorname*{min}_{\\theta}\\sum_{v\\in V/V^{\\mathrm{R}}}\\ell(\\theta;v,E/E^{\\mathrm{R}}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "To avoid training from scratch, we derive the approximation of $\\theta^{\\mathrm{aug}}$ by the following theorem. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.1 (GNN model approximation) Given the GNN parameter $\\theta^{\\mathrm{ref}}$ on $G^{\\mathrm{ref}}$ , the removed nodes $V^{\\mathrm{R}}$ , removed edges $E^{\\mathrm{R}}$ and influenced nodes $V^{\\mathrm{I}}$ . Assume $\\ell$ is twice-differentiable everywhere and convex, we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\theta^{\\mathrm{aug}}\\approx\\theta^{\\mathrm{ref}}+(\\nabla^{2}\\mathcal{L}(\\theta^{\\mathrm{ref}};G^{\\mathrm{aug}}))^{-1}\\nabla(\\sum_{v\\in V^{\\mathrm{I}}\\cup V^{\\mathrm{R}}}\\ell(\\theta^{\\mathrm{ref}};v,E)-\\sum_{v\\in V^{\\mathrm{I}}}\\ell(\\theta^{\\mathrm{ref}};v,E/E^{\\mathrm{R}})),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\nabla$ denote gradient, and $\\nabla^{2}$ denote Hessian. $\\begin{array}{r}{\\mathcal{L}(\\theta^{\\mathrm{ref}};G^{\\mathrm{aug}})=\\sum_{v\\in V/V^{\\mathrm{R}}}\\ell(\\theta^{\\mathrm{ref}};v,E/E^{\\mathrm{R}}).}\\end{array}$   \nThe detailed derivation can be found in Appendix A.2. ", "page_idx": 4}, {"type": "text", "text": "In practice, the Hessian may be non-invertible due to the non-convexity of GNNs. We address this by adding a damping term to the Hessian [34].To reduce computation, we also follow [29] to convert the inverse Hessian calculation into quadratic minimization. See Appendix A.3 for complexity analysis. ", "page_idx": 4}, {"type": "text", "text": "Analyzing the approximation error. Eventually, we aim to quantitatively assess the error in the approximated model, as this directly determines whether graph properties can be effectively reflected, thereby influencing the attack. To achieve this, we investigate how specific removal choices of $V^{\\mathrm{R}}$ and $E^{\\mathrm{{\\dot{R}}}}$ affect the approximation error in Eq. (4). Note that $\\begin{array}{r}{\\nabla\\sum_{v\\in V/V^{\\mathrm{R}}}\\ell(\\theta;v,E/E^{\\mathrm{R}})=0}\\end{array}$ only when $\\theta^{\\mathrm{aug}}$ is the exact minimizer, thus the gradient norm $\\begin{array}{r}{\\|\\nabla\\sum_{v\\in V/V^{\\mathrm{R}}}\\ell(\\theta;v,E/E^{\\mathrm{R}})\\|_{2}}\\end{array}$ can reflect the approximation error. The following theorem provides an upper bound on this gradient norm. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.2 (Approximation error bound) Assume $\\ell$ is twice-differentiable everywhere and convex, $\\begin{array}{r}{\\|\\nabla\\ell\\|_{2}\\leq c_{1}^{\\ast}\\overset{\\cdot}{\\nabla}^{2}\\sum_{v\\in V/V^{\\mathrm{R}}}\\ell(\\theta;v,E/E^{\\mathrm{R}})}\\end{array}$ is $\\gamma_{1}$ -Lipschitz, the approximation error bound is given by: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\|\\nabla\\sum_{v\\in V/V^{\\mathrm{R}}}\\ell(\\theta^{\\mathrm{aug}};v,E/E^{\\mathrm{R}})\\|_{2}\\le C(|V^{\\mathrm{R}}|+2|V^{\\mathrm{I}}|)^{2}=C\\cdot\\delta(V^{\\mathrm{R}},E^{\\mathrm{R}}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $|\\cdot|$ denotes the cardinality of a set, and $\\delta(\\cdot,\\cdot)$ denotes the square of the number of nodes removed and influenced, given $V^{\\dot{\\mathrm{R}}}$ and $E^{\\mathrm{R}}$ . $C$ denotes a constant depending on the GNN model, see Appendix A.2 for detail proof. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.2 indicates that the error bound for the approximation is related to both the number of removed nodes and influenced nodes. Next, we demonstrate how this can serve as an error criterion to select augmented graphs that result in minimal approximation errors. ", "page_idx": 5}, {"type": "text", "text": "3.3 Diversity enhancement ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Following the above, we detail the designed diversity enhancement strategy. To develop a wellgeneralized attack model capable of distinguishing different sensitive properties, we first apply a structure-aware random walk for sampling diverse reference graphs. We then propose a novel selection mechanism to ensure that multiple perturbations on the reference graphs further enhance diversity while considering the reduction of approximation error. ", "page_idx": 5}, {"type": "text", "text": "Sampling diverse reference graphs. Inspired by community detection, where diverse communities are identified on a graph, we design a structure-aware random walk for sampling reference graphs. Specifically, we incorporate Louvain community detection [35] to partition the auxiliary graph into several similarly sized communities. During random walks, the starting nodes are chosen from different communities. And we assign different weights to neighboring nodes: $w$ for those within the same community and $1-w$ for those from different communities, where $w\\in[0,1]$ is a hyperparameter. The transition probabilities are then obtained by normalizing these weights. This strategy encourages sampling reference graphs within distinct communities, thus boosting their diversity. ", "page_idx": 5}, {"type": "text", "text": "Ensuring diverse augmented graphs. To ensure perturbations on reference graphs can enhance the diversity, we further design a perturbation selector. Based on $\\S\\ 3.2$ , it is easy to see that each approximated model can be considered as a result of the specific perturbation. Thus, improving the diversity of approximated models is essentially improving the diversity of augmented graphs. Formally, for each reference graph we generate $k$ augmented graphs $\\mathcal{G}^{\\mathrm{aug}}=\\{\\bar{G}_{1}^{\\mathrm{aug}},\\bar{G}_{2}^{\\mathrm{aug}},\\ldots,\\bar{G}_{k}^{\\mathrm{aug}}\\}$ by randomly removing $k$ different sets of nodes and edges. The diversity for $\\mathcal{G}^{\\mathrm{aug}}$ is defined as: ", "page_idx": 5}, {"type": "text", "text": "Dmeetfriinci t tyh faotr $g^{\\mathrm{aug}}$ u)r eGs itvheen  dai ssteatn ocfe $k$ egtrwaepehns $\\mathcal{G}^{\\mathrm{aug}}=\\{G_{1_{-},\\ldots}^{\\mathrm{aug}},\\ldots,G_{k}^{\\mathrm{aug}}\\}$ s, itayn do f pihs $d(G_{i}^{\\mathrm{aug}},G_{j}^{\\mathrm{aug}})$ iaugand Gja $\\bar{G}_{j}^{\\mathrm{aug}}$ ug $g^{\\mathrm{aug}}$ defined as the sum of all pair-wise graph distances in $\\mathcal{G}^{\\mathrm{aug}}$ , that is, $\\begin{array}{r}{\\sum_{i=1}^{k}\\sum_{j=1}^{k}d\\left(G_{i}^{\\mathrm{aug}},G_{j}^{\\mathrm{aug}}\\right)}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "Since stochastic augmentations may not all contribute to total diversity, our objective is to select a diverse subset of $\\mathcal{G}^{\\mathrm{aug}}$ , namely, a subset of diverse perturbations to enhance the diversity of augmented models. However, it is important to note that solely maximizing diversity may lead to relatively large approximation errors, which may worsen the attack performance. Fortunately, utilizing the error criterion from Eq. (5), we can ensure that augmentations enhance diversity while minimizing total approximation error, which can be formulated as a quadratic integer programming task. ", "page_idx": 5}, {"type": "text", "text": "Given $k$ available perturbations, we aim to select $q$ of them, such that the diversity among these selected is maximized while keeping the approximation error minimal. We here introduce decision variables $x_{i}\\in\\{0,1\\}$ to represent whether the $i$ -th augmentation is selected. Let $\\delta_{i}$ represent the approximation error in the $i$ -th augmentation (cf. Eq. (5)). The optimization problem is as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\sum_{i=1}^{k}\\sum_{j=1}^{k}d\\left(G_{i}^{\\mathrm{aug}},G_{j}^{\\mathrm{aug}}\\right)x_{i}x_{j},\\quad\\mathrm{s.t.~}(1)\\sum_{i=1}^{k}x_{i}=q,(2)\\sum_{i=1}^{k}\\delta_{i}x_{i}\\leq\\epsilon,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\epsilon$ is a constant that imposes the budget on the total approximation error of the selected $q$ augmentations, ensuring that it does not exceed $\\epsilon$ . Here, we select graph edit distance as the distance metric, which can be efficiently calculated since all $k$ augmented graphs $\\mathcal{G}^{\\mathrm{aug}}$ are derived from one reference $\\mathcal{G}^{\\mathrm{ref}}$ . We utilize Gurobi Optimizer [36], a state-of-the-art solver, to solve this quadratic integer programming problem, which is known for its efficiency and effectiveness. ", "page_idx": 5}, {"type": "text", "text": "3.4 Overall algorithm ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We summarize the overall algorithm of our attack in Algorithm 1. Steps 1-2 outline the structureaware random walk for sampling reference graphs, steps 5-12 detail the perturbation selector, with step 10 calculating the error criterion in Eq. (5). Finally, in step 16 we train the attack model. ", "page_idx": 5}, {"type": "table", "img_path": "Luxk3z1tSG/tmp/a8039ad8da3a8da56ad3e0fa68040fae8f483be793c22cc7169e823c2be6275e.jpg", "table_caption": ["Table 1: Properties to be attacked, # indicates number of nodes or edges. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we evaluate the performance of the proposed attack method by addressing the following three research questions: ", "page_idx": 6}, {"type": "text", "text": "\u2022 RQ1: How efficient and effective is our method on various graph datasets? \u2022 RQ2: How do different factors influence the performance of our method? \u2022 RQ3: How applicable is our method in different scenarios? ", "page_idx": 6}, {"type": "text", "text": "4.1 Experimental setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets and sensitive properties. We conduct property inferences on three real world datasets: Facebook [37], Pubmed [38], and Pokec [39]. Appendix A.4 details the datasets and properties. ", "page_idx": 6}, {"type": "text", "text": "\u2022 Facebook and Pokec are social networks where nodes represent users and edges denote friendships. Following [21], we select gender as the property attribute, set node property as whether the male nodes are dominant, and edge property as whether the same-gender edges are dominant. \u2022 Pubmed is a citation network where nodes are publications and edges are citations. We select the keyword \u201cInsulin\u201d (IS) as the property attribute. Node property is whether publications with \u201cIS\u201d are dominant. Edge property is whether citations between publications with \u201cIS\u201d are dominant. All used properties are summarized in Table 1. ", "page_idx": 6}, {"type": "text", "text": "Training and testing data. For fairness, we evaluate our method and baselines on the same target graphs. To ensure there is no overlap between the target graph and the auxiliary graph, for each dataset we first use Louvain community detection to split the original graph into two similarly sized parts. One part is used as the auxiliary graph, and the other part is used to sample multiple target graphs. Sizes and numbers of reference graphs (our method), shadow graphs (baselines), and target graphs are provided in Appendix A.4. ", "page_idx": 6}, {"type": "text", "text": "Target GNN. For target GNN, We use a widely recognized GNN model, GraphSAGE [40], configured as per [21] with 2 layers, 64 hidden sizes, and 1,500 training epochs with an early stop tolerance of 50. The Adam optimizer is used with a learning rate of 1e-4 and a weight decay of 5e-4. ", "page_idx": 6}, {"type": "text", "text": "Implementation details. For the attack model, We use a linear classifier with the deepest trick [41]; For hyper-parameter settings, we perform grid searches of reference graphs\u2019 numbers in (0, 100] (step size 25), and augmented graphs\u2019 numbers in (0, 10] (step size 2), across all datasets. Experiments are repeated 5 times to report the averages with standard deviations. See appendix A.4 for more details. Our codes are available at https://github.com/zjunet/GPIA_NIPS. ", "page_idx": 6}, {"type": "text", "text": "Baselines. We adopt four state-of-the-art baseline models to compare against the proposed attack model: (1) GPIA [21]: An attack method designed for graphs and GNNs in both white-box and blackbox settings, following the traditional attack framework. (2) PIR-S/PIR-D [22]: Two permutation equivalence methods designed for white-box attacks, PIR-S using neuron sorting and PIR-D using set-based representation. (3) AIA [42]: Property inference method based on attribute inference attack, which first predicts the property attribute based on embeddings/posteriors and then predicts property, suitable for both white-box/black-box attacks. See Appendix A.4 for details. ", "page_idx": 6}, {"type": "text", "text": "Table 2: Average accuracy and runtime (seconds) comparison on different properties in white-box setting. \u201cNode\u201d and \u201cLink\u201d denote node and link properties, respectively. The best results are in bold. ", "page_idx": 7}, {"type": "table", "img_path": "Luxk3z1tSG/tmp/9234a35dcc538b062aed8d50bef5037e9d9b77a71f737991dc22ea721272219f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.2 Evaluation of efficiency and effectiveness (RQ1) ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We first focus on white-box settings and evaluate the accuracy and ROC-AUC for effectiveness and runtime for efficiency. Note that the reported runtime throughout this work encompasses the entire attack process for both the proposed method and baselines, starting from sampling the reference (shadow) graphs to inferring the properties of the target graphs. Table 2 presents the average accuracy and runtime of the proposed attack method compared to other baseline methods on the six aforementioned sensitive properties. We provide the corresponding standard deviations and ROC-AUCs results in Appendix A.5. The results reveal several key insights: (1) Traditional attacks incur significantly high runtime. The slight differences mainly depend on the different strategies in their attack models. (2) PIR-D achieves better accuracy among the baselines, possibly due to their consideration of permutation equivalence. AIA shows lower performance, which may be because of their limited ability to conduct attribute inference, thus affecting the classification of properties. (3) The proposed attack model outperforms all baseline methods across all datasets, achieving an average increase of $2.7\\%$ in accuracy and being $6.5\\times$ faster compared to the best baseline, demonstrating its remarkable efficiency and efficacy. The significant margin by which our method outperforms the baselines is primarily due to our specific mechanisms that ensure diversity in both reference and augmented graphs, which are essential for training a robust attack model. In contrast, conventional attacks lack such designs for shadow graph diversity, resulting in sub-optimal performance. ", "page_idx": 7}, {"type": "text", "text": "4.3 Evaluation of influencing factors (RQ2) ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Ablation study. To ensure effectiveness, our method includes two main mechanisms: sampling diverse reference graphs and selecting diverse augmented graphs. Here, we conduct ablation studies to demonstrate their necessity, including four variants: (1) w/o structure: We discard structure-aware sampling and use simple random walks to sample reference graphs. (2) w/o selector: We discard the augmentation selector and use random removal to obtain augmented graphs. (3) w/o error: In the augmentation selector (cf. Eq. (6)), we ignore the approximation error and only select augmentations that maximize diversity. (4) w/o diversity: We ignore diversity in the augmentation selector (cf. Eq. (6)) and only select augmentations that minimize the approximation error. Figure 2 (a) shows the attack results on Facebook\u2019s node property. Notably, the complete model consistently surpasses the performance of all variants, showing the effectiveness and necessity of simultaneously sampling diverse reference graphs and selecting diverse augmented graphs. ", "page_idx": 7}, {"type": "text", "text": "Hyper-parameter analysis. We next evaluate the impact of two important hyper-parameters on our method: (1) the number of reference graphs and (2) the number of selected augmented graphs. Both directly affect the diversity of approximated models. We tune the number of reference graphs among $\\{25,5\\dot{0},75,100\\}$ and the number of selected augmented graphs among $\\{2,4,6,8,10\\}$ . The results in Figure 2 (b) and 2 (c) show that as both hyper-parameters increase, the attack performance initially improves and then stabilizes. This indicates that a relatively small number of reference graphs and augmented graphs are sufficient to ensure diversity, thereby maintaining good attack performance. ", "page_idx": 7}, {"type": "text", "text": "4.4 Evaluation in different scenarios (RQ3) ", "text_level": 1, "page_idx": 7}, {"type": "image", "img_path": "Luxk3z1tSG/tmp/741773d9951fe932f15361d88507dd4e3104e1b5571cdcd3bd93ffaef9db5472.jpg", "img_caption": ["Figure 2: (a) Evaluation of the necessity of considering diversity while minimizing the approximation error. (b) and (c) Impact of the number of augmented graphs (per reference graph) and reference graphs on attack accuracy, respectively. (d) Accuracy and runtime comparison in black-box settings. "], "img_footnote": ["$4.4\\times$ faster on GCN, $4.0\\times$ faster on GAT, and $4.3\\times$ faster on SGC compared to the best baseline. "], "page_idx": 8}, {"type": "text", "text": "To test the applicability of our method, we evaluate its performance under various conditions, including scenarios with black-box adversary knowledge, on different types of GNN models, on large-scale graph datasets, and when the target and auxiliary graphs are distinct. ", "page_idx": 8}, {"type": "text", "text": "Performance on black-box knowledge. In the black-box setting, we use model outputs, specifically posterior probabilities, to train attack models for our method and baselines. Since PIR-D and PIR-S only support white-box settings, we included another state-of-the-art black-box attack, PIA-MP [20], as detailed in Appendix A.4. The results on Facebook\u2019s node property in Figure 2 (d) show that our method improves accuracy by $11.5\\%$ compared to the best baselines while being $7.3\\times$ faster. ", "page_idx": 8}, {"type": "text", "text": "Performance on other GNNs. We conduct property inference attacks on other three fundamental GNNs: GCN [43], GAT [44], and SGC [45]. For GCN and GAT, hyper-parameters are configured according to [21], while for SGC, we set the number of hops to 2. We report the attack accuracy and runtime of our method alongside other baselines on Facebook\u2019s node property, as illustrated in Figure 3 (a)-(c). It is observed that the overall attack accuracy for SGC is comparatively lower, potentially due to the SGC model\u2019s inherent limitations in capturing property information effectively. Moreover, our method consistently achieves the highest accuracy, also demonstrating a runtime that is ", "page_idx": 8}, {"type": "table", "img_path": "Luxk3z1tSG/tmp/f9b9ce3eb1e25027ffd2ab8f6909786ce950b7689b15bd3a1e477c15c83c2b0c.jpg", "table_caption": ["Table 3: Attack comparison using distinct graphs for the target and auxiliary graphs. The arrows indicate the auxiliary graph on the left and the target graph on the right. The best results are bolded. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Performance on scalability. We further conducted property inference attacks on a large-scale graph dataset, Pokec-100M, which contains 1,027,956 nodes and 27,718,416 edges. This graph is sampled from the original dataset [39] by retaining nodes with relatively complete features. We targeted the same node property as in the Pokec dataset, with the number of nodes in the reference graphs, shadow graphs, and target graphs set to 52,600, 50,000, and 50,000, respectively. All other settings remain consistent with previous experiments. We compare the attack accuracy and runtime of our method against other baselines. As shown in Figure 3 (d), conventional attacks incur significant computational costs on this dataset, whereas our method is $10.0\\times$ faster. Additionally, our attack accuracy is significantly higher than those of the baselines. ", "page_idx": 8}, {"type": "text", "text": "Performance with distinct target and auxiliary graphs. In the above experiments, the target and auxiliary graphs are splits of the same original graph. However, in real-world scenarios, this assumption may not hold. Therefore, we evaluate the performance of our attack under a more practical condition, where distinct graphs (from the same domain) are used as the target and auxiliary graphs. Specifically, we select Facebook and Pokec, as they are both social networks, and consider two cases: using Facebook as the target and Pokec as the auxiliary graph, and vice versa. Since the feature dimensions of these two datasets differ, the parameters of the approximated model and the target model are not directly compatible, so we apply PCA dimension reduction to align the parameters. ", "page_idx": 8}, {"type": "image", "img_path": "Luxk3z1tSG/tmp/3f9d71f2f9ff6f41ea5fc6961b97f27343d8a940fd0fea29ab659a5988e6d235.jpg", "img_caption": ["Figure 3: Comparison of average attack accuracy and runtime (seconds) on: (a)-(c) other GNNs, including GAT, GCN, and SGC; (d) a large-scale dataset, Pokec-100M. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Table 3 reports the attack accuracy and runtime. We observe that (1) the overall attack performance decreases, potentially due to the loss of property information embedded in model parameters during parameter alignment, and (2) our model consistently achieves the best performance with significant speed-ups, demonstrating its effectiveness in a more practical scenario. ", "page_idx": 9}, {"type": "text", "text": "5 Literature Review ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Property inference attack. The concept of property inference attack is first introduced by [23], demonstrating the leakage of sensitive properties from hidden Markov models and support vector machines in systems like speech-to-text. Building on this, attacks on various machine learning models are studied, including feed-forward neural networks, convolutional neural networks, and generative adversarial networks [22, 25, 32, 46]. Some works also consider multi-party collaborative learning scenarios [20, 24] or incorporate data poisoning [32, 47]. Specifically, [47] proposes an efficient attack based on distinguishing tests, achieving faster performance than traditional shadow training. Their setting differs from ours by the additional adversarial capability of data poisoning. Recently, with the increasing use of graphs and GNNs, security and privacy concerns are emerging [48\u201351]. While efforts have been made to investigate property inference attacks on GNNs [20, 21, 27], they follow the shadow training framework, which requires training a relatively large number of shadow GNN models, leading to high computational costs and reduced feasibility [47]. [52] assumes access to the embedding of whole graphs and targets at graph-level properties, which is beyond our scope. ", "page_idx": 9}, {"type": "text", "text": "GNN model approximation. GNN model approximations are primarily based on the influence function [29, 31, 53, 54] or Newton update [30, 33]. Except for [54], these methods are utilized in the context of graph unlearning. Studies [30, 33, 54] explore model approximation for edge or node removal and analyze the corresponding approximation error bounds, yet they are limited to specific model architectures (e.g., simple graph convolution, graph scattering transform). Further efforts [29, 31] extend model approximation to generic GNNs. [29] introduces a framework for edge unlearning, while [31] proposes a general unlearning framework for removing either nodes, edges, or features individually. Our model approximation differs from above by enabling the simultaneous removal of nodes and edges across generic GNN architecture. A concurrent work [53] addresses a similar model approximation as our attack; however, the additional theoretical assumptions could fail when removing a combination of nodes and edges, and their corresponding solution may significantly compromise the efficiency. Other studies [55, 56] also employ the graph shard approach. However, they may have poor efficiency in batch removal, which involves multiple retraining of sub-models. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we focus on the problem of graph property inference attacks. We utilize model approximation to efficiently generate approximated models after initially training a small set of models, which replaces the costly shadow training in traditional attacks. To overcome the challenge of ensuring the diversity of approximated models while reducing the approximation error, we first derive a theoretical criterion to quantify the impact of different augmentations on approximation error. Next, we propose a diversity enhancement strategy, including a structure-aware random walk for sampling diverse reference graphs and a selection mechanism to retain optimal approximated models, utilizing edit distance to measure diversity and the theoretical criterion to assess approximation error. The retained approximated models are finally used to train an attack classifier. Extensive experiments across six real-world scenarios demonstrate our attack\u2019s outstanding efficiency and effectiveness. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work is supported by the Zhejiang Province \u201cJianBingLingYan+X\u201d Research and Development Plan (2024C01114). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Zonghan Wu et al. \u201cA comprehensive survey on graph neural networks\u201d. In: IEEE transactions on neural networks and learning systems 32.1 (2020), pp. 4\u201324.   \n[2] Shengchao Liu, Mehmet F Demirel, and Yingyu Liang. \u201cN-gram graph: Simple unsupervised representation for graphs, with applications to molecules\u201d. In: Advances in neural information processing systems 32 (2019).   \n[3] Michael Fleder, Michael S Kester, and Sudeep Pillai. \u201cBitcoin transaction graph analysis\u201d. In: arXiv preprint arXiv:1502.01657 (2015).   \n[4] Yu Wang et al. \u201cSpatiotemporal-Augmented Graph Neural Networks for Human Mobility Simulation\u201d. In: IEEE Transactions on Knowledge and Data Engineering (2024).   \n[5] Tongya Zheng et al. \u201cTemporal aggregation and propagation graph neural networks for dynamic representation\u201d. In: IEEE Transactions on Knowledge and Data Engineering 35.10 (2023), pp. 10151\u201310165.   \n[6] Shunyu Liu et al. \u201cTransmission Interface Power Flow Adjustment: A Deep Reinforcement Learning Approach Based on Multi-Task Attribution Map\u201d. In: IEEE Transactions on Power Systems 39.2 (2024), pp. 3324\u20133335.   \n[7] Chen Gao et al. \u201cA survey of graph neural networks for recommender systems: Challenges, methods, and directions\u201d. In: ACM Transactions on Recommender Systems 1.1 (2023), pp. 1\u2013 51.   \n[8] Oliver Wieder et al. \u201cA compact review of molecular property prediction with graph neural networks\u201d. In: Drug Discovery Today: Technologies 37 (2020), pp. 1\u201312.   \n[9] Yuwen Wang et al. \u201cUnveiling Global Interactive Patterns across Graphs: Towards Interpretable Graph Neural Networks\u201d. In: ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 2024, pp. 3277\u20133288.   \n[10] Xiaoxiao Ma et al. \u201cA comprehensive survey on graph anomaly detection with deep learning\u201d. In: IEEE Transactions on Knowledge and Data Engineering 35.12 (2021), pp. 12012\u201312038.   \n[11] Jiarong Xu et al. \u201cNetrl: Task-aware network denoising via deep reinforcement learning\u201d. In: IEEE Transactions on Knowledge and Data Engineering 35.1 (2021), pp. 810\u2013823.   \n[12] Jiarong Xu et al. \u201cRobust network enhancement from flawed networks\u201d. In: IEEE Transactions on Knowledge and Data Engineering 34.7 (2020), pp. 3507\u20133520.   \n[13] Jiarong Xu et al. \u201cToward Secure Graph Data Collaboration in a Data-Sharing-Free Manner: A Novel Privacy-Preserving Graph Pre-training Model\u201d. In: Available at SSRN (2023).   \n[14] Carol Tenopir et al. \u201cData sharing, management, use, and reuse: Practices and perceptions of scientists worldwide\u201d. In: PloS one 15.3 (2020), e0229003.   \n[15] Hanyang Yuan et al. \u201cUnveiling Privacy Vulnerabilities: Investigating the Role of Structure in Graph Data\u201d. In: Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 2024, pp. 4059\u20134070.   \n[16] Adam Lerer et al. \u201cPytorch-biggraph: A large scale graph embedding system\u201d. In: Proceedings of Machine Learning and Systems 1 (2019), pp. 120\u2013131.   \n[17] Ziniu Hu et al. \u201cGpt-gnn: Generative pre-training of graph neural networks\u201d. In: Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining. 2020, pp. 1857\u20131867.   \n[18] Da Zheng et al. \u201cDGL-KE: Training Knowledge Graph Embeddings at Scale\u201d. In: Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval. SIGIR \u201920. New York, NY, USA: Association for Computing Machinery, 2020, pp. 739\u2013748.   \n[19] Jiezhong Qiu et al. \u201cGcc: Graph contrastive coding for graph neural network pre-training\u201d. In: Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining. 2020, pp. 1150\u20131160.   \n[20] Wanrong Zhang, Shruti Tople, and Olga Ohrimenko. \u201cLeakage of dataset properties in {MultiParty} machine learning\u201d. In: 30th USENIX security symposium (USENIX Security 21). 2021, pp. 2687\u20132704.   \n[21] Xiuling Wang and Wendy Hui Wang. \u201cGroup property inference attacks against graph neural networks\u201d. In: Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security. 2022, pp. 2871\u20132884.   \n[22] Karan Ganju et al. \u201cProperty inference attacks on fully connected neural networks using permutation invariant representations\u201d. In: Proceedings of the 2018 ACM SIGSAC conference on computer and communications security. 2018, pp. 619\u2013633.   \n[23] Giuseppe Ateniese et al. \u201cHacking smart machines with smarter ones: How to extract meaningful data from machine learning classifiers\u201d. In: International Journal of Security and Networks 10.3 (2015), pp. 137\u2013150.   \n[24] Luca Melis et al. \u201cExploiting unintended feature leakage in collaborative learning\u201d. In: 2019 IEEE symposium on security and privacy (SP). IEEE. 2019, pp. 691\u2013706.   \n[25] Junhao Zhou et al. \u201cProperty inference attacks against GANs\u201d. In: arXiv preprint arXiv:2111.07608 (2021).   \n[26] Enyan Dai and Suhang Wang. \u201cSay no to the discrimination: Learning fair graph neural networks with limited sensitive attribute information\u201d. In: Proceedings of the 14th ACM International Conference on Web Search and Data Mining. 2021, pp. 680\u2013688.   \n[27] Anshuman Suri and David Evans. \u201cFormalizing and estimating distribution inference risks\u201d. In: arXiv preprint arXiv:2109.06024 (2021).   \n[28] Chuan Guo et al. \u201cCertified data removal from machine learning models\u201d. In: arXiv preprint arXiv:1911.03030 (2019).   \n[29] Kun Wu et al. \u201cCertified edge unlearning for graph neural networks\u201d. In: Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 2023, pp. 2606\u2013 2617.   \n[30] Eli Chien, Chao Pan, and Olgica Milenkovic. \u201cCertified graph unlearning\u201d. In: arXiv preprint arXiv:2206.09140 (2022).   \n[31] Jiancan Wu et al. \u201cGif: A general graph unlearning strategy via influence function\u201d. In: Proceedings of the ACM Web Conference 2023. 2023, pp. 651\u2013661.   \n[32] Saeed Mahloujifar, Esha Ghosh, and Melissa Chase. \u201cProperty inference from poisoning\u201d. In: 2022 IEEE Symposium on Security and Privacy (SP). IEEE. 2022, pp. 1120\u20131137.   \n[33] Chao Pan, Eli Chien, and Olgica Milenkovic. \u201cUnlearning graph classifiers with limited data resources\u201d. In: Proceedings of the ACM Web Conference 2023. 2023, pp. 716\u2013726.   \n[34] Pang Wei Koh and Percy Liang. \u201cUnderstanding black-box predictions via influence functions\u201d. In: International conference on machine learning. PMLR. 2017, pp. 1885\u20131894.   \n[35] Vincent D Blondel et al. \u201cFast unfolding of communities in large networks\u201d. In: Journal of statistical mechanics: theory and experiment 2008.10 (2008), P10008.   \n[36] Gurobi Optimization, LLC. Gurobi Optimizer Reference Manual. 2023. URL: https://www. gurobi.com.   \n[37] Jure Leskovec and Julian Mcauley. \u201cLearning to discover social circles in ego networks\u201d. In: Advances in neural information processing systems 25 (2012).   \n[38] Zhilin Yang, William Cohen, and Ruslan Salakhudinov. \u201cRevisiting semi-supervised learning with graph embeddings\u201d. In: International conference on machine learning. PMLR. 2016, pp. 40\u201348.   \n[39] Lubos Takac and Michal Zabovsky. \u201cData analysis in public social networks\u201d. In: International scientific conference and international workshop present day trends of innovations. Vol. 1. 6. 2012.   \n[40] Will Hamilton, Zhitao Ying, and Jure Leskovec. \u201cInductive representation learning on large graphs\u201d. In: Advances in neural information processing systems 30 (2017).   \n[41] Manzil Zaheer et al. \u201cDeep sets\u201d. In: Advances in neural information processing systems 30 (2017).   \n[42] Congzheng Song and Ananth Raghunathan. \u201cInformation leakage in embedding models\u201d. In: Proceedings of the 2020 ACM SIGSAC conference on computer and communications security. 2020, pp. 377\u2013390.   \n[43] Thomas N Kipf and Max Welling. \u201cSemi-supervised classification with graph convolutional networks\u201d. In: arXiv preprint arXiv:1609.02907 (2016).   \n[44] Petar Velickovic et al. \u201cGraph attention networks\u201d. In: stat 1050.20 (2017), pp. 10\u201348550.   \n[45] Felix Wu et al. \u201cSimplifying graph convolutional networks\u201d. In: International conference on machine learning. PMLR. 2019, pp. 6861\u20136871.   \n[46] Mathias PM Parisot, Balazs Pejo, and Dayana Spagnuelo. \u201cProperty inference attacks on convolutional neural networks: Influence and implications of target model\u2019s complexity\u201d. In: arXiv preprint arXiv:2104.13061 (2021).   \n[47] Harsh Chaudhari et al. \u201cSNAP: Efficient extraction of private properties with poisoning\u201d. In: 2023 IEEE Symposium on Security and Privacy $(S P)$ . IEEE. 2023, pp. 400\u2013417.   \n[48] Qinkai Zheng et al. \u201cGraph robustness benchmark: Benchmarking the adversarial robustness of graph machine learning\u201d. In: arXiv preprint arXiv:2111.04314 (2021).   \n[49] Junlong Liao et al. \u201cValue at Adversarial Risk: A Graph Defense Strategy against Cost-Aware Attacks\u201d. In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 38. 12. 2024, pp. 13763\u201313771.   \n[50] Jiarong Xu et al. \u201cBlindfolded attackers still threatening: Strict black-box adversarial attacks on graphs\u201d. In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 36. 4. 2022, pp. 4299\u20134307.   \n[51] Jiarong Xu et al. \u201cUnsupervised adversarially robust representation learning on graphs\u201d. In: Proceedings of the AAAI conference on artificial intelligence. Vol. 36. 4. 2022, pp. 4290\u20134298.   \n[52] Zhikun Zhang et al. \u201cInference attacks against graph neural networks\u201d. In: 31st USENIX Security Symposium (USENIX Security 22). 2022, pp. 4543\u20134560.   \n[53] Yushun Dong et al. \u201cIdea: A flexible framework of certified unlearning for graph neural networks\u201d. In: Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 2024, pp. 621\u2013630.   \n[54] Zizhang Chen et al. \u201cCharacterizing the influence of graph elements\u201d. In: arXiv preprint arXiv:2210.07441 (2022).   \n[55] Cheng-Long Wang, Mengdi Huai, and Di Wang. \u201cInductive graph unlearning\u201d. In: 32nd USENIX Security Symposium (USENIX Security 23). 2023, pp. 3205\u20133222.   \n[56] Min Chen et al. \u201cGraph unlearning\u201d. In: Proceedings of the 2022 ACM SIGSAC conference on computer and communications security. 2022, pp. 499\u2013513. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Notations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The main notations can be found in the following table. ", "page_idx": 13}, {"type": "text", "text": "Table 4: Description of major notations, ordered by appearance. ", "page_idx": 13}, {"type": "table", "img_path": "Luxk3z1tSG/tmp/888183fe29074c8a6815f44f7c126a8638d84e4448ce7dc55fb895dd55da6aae.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "A.2 Proof of theorems ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.2.1 Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Given the GNN parameter $\\theta^{\\mathrm{ref}}$ on $G^{\\mathrm{ref}}$ , the removed nodes $V^{\\mathrm{R}}$ , removed edges $E^{\\mathrm{R}}$ and influenced nodes $V^{\\mathrm{I}}$ . Assume $\\ell$ is twice-differentiable everywhere and convex, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n{\\sf a u g}\\approx\\theta^{\\mathrm{ref}}+(\\nabla^{2}\\sum_{v\\in V/V^{\\mathrm{R}}}\\ell(\\theta^{\\mathrm{ref}};v,E/E^{\\mathrm{R}}))^{-1}\\nabla(\\sum_{v\\in V^{\\mathrm{I}}\\cup V^{\\mathrm{R}}}\\ell(\\theta^{\\mathrm{ref}};v,E)-\\sum_{v\\in V^{\\mathrm{I}}}\\ell(\\theta^{\\mathrm{ref}};v,E/E^{\\mathrm{R}})),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\nabla$ denote the gradient, and $\\nabla^{2}$ denote the Hessian. ", "page_idx": 13}, {"type": "text", "text": "Proof. Let $\\begin{array}{r}{\\mathcal{L}(\\theta;G^{\\mathrm{aug}})=\\sum_{v\\in V/V^{\\mathrm{R}}}\\ell(\\theta;v,E/E^{\\mathrm{R}})}\\end{array}$ denote the loss function of $G^{\\mathrm{aug}}$ at $\\theta$ , by one step newton update of $\\mathcal{L}$ , the approximation of $\\theta^{\\mathrm{aug}}$ is: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\theta^{\\mathrm{aug}}\\approx\\theta^{\\mathrm{ref}}-\\left(\\nabla^{2}\\mathcal{L}(\\theta^{\\mathrm{ref}};G^{\\mathrm{aug}})\\right)^{-1}\\nabla\\mathcal{L}(\\theta^{\\mathrm{ref}};G^{\\mathrm{aug}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Let $V^{\\mathrm{UI}}$ denote the uninfluenced node set, i.e., $V^{\\mathrm{I}}\\cup V^{\\mathrm{UI}}=V/V^{\\mathrm{R}},V^{\\mathrm{I}}\\cap V^{\\mathrm{UI}}=\\emptyset$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Sigma(\\theta^{\\mathrm{ref}};G^{\\mathrm{aug}})=\\displaystyle\\sum_{v\\in V^{1}}\\ell(\\theta^{\\mathrm{ref}};v,E/E^{\\mathbb{R}})+\\displaystyle\\sum_{v\\in V^{1}}\\ell(\\theta^{\\mathrm{ref}};v,E/E^{\\mathbb{R}})}\\\\ &{\\qquad=\\displaystyle\\sum_{v\\in V^{1}}\\ell(\\theta^{\\mathrm{ref}};v,E)+\\displaystyle\\sum_{v\\in V^{\\mathbb{R}}}\\ell(\\theta^{\\mathrm{ref}};v,E)+\\displaystyle\\sum_{v\\in V^{1}}\\ell(\\theta^{\\mathrm{ref}};v,E)}\\\\ &{\\qquad+\\displaystyle\\sum_{v\\in V^{1}}\\ell(\\theta^{\\mathrm{ref}};v,E/E^{\\mathbb{R}})-\\displaystyle\\sum_{v\\in V^{\\mathbb{R}}}\\ell(\\theta^{\\mathrm{ref}};v,E)-\\displaystyle\\sum_{v\\in V^{1}}\\ell(\\theta^{\\mathrm{ref}};v,E)}\\\\ &{\\qquad=\\displaystyle\\sum_{v\\in V}\\ell(\\theta^{\\mathrm{ref}};v,E)+\\displaystyle\\sum_{v\\in V^{1}}\\ell(\\theta^{\\mathrm{ref}};v,E/E^{\\mathbb{R}})-\\displaystyle\\sum_{v\\in V^{\\mathbb{R}}}\\ell(\\theta^{\\mathrm{ref}};v,E)-\\displaystyle\\sum_{v\\in V^{1}}\\ell(\\theta^{\\mathrm{ref}};v,E)}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Given $\\begin{array}{r}{\\nabla\\sum_{v\\in V}\\ell(\\theta^{\\mathrm{ref}};v,E)=0}\\end{array}$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\nabla\\mathcal{L}(\\theta^{\\mathrm{ref}};G^{\\mathrm{aug}})=\\nabla\\left(\\sum_{v\\in V^{\\mathrm{I}}}\\ell(\\theta^{\\mathrm{ref}};v,E/E^{\\mathrm{R}})-\\sum_{v\\in V^{\\mathrm{I}}\\cup V^{\\mathrm{R}}}\\ell(\\theta^{\\mathrm{ref}};v,E)\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "And ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\theta^{\\mathrm{aug}}\\approx\\theta^{\\mathrm{ref}}+\\left(\\nabla^{2}\\mathcal{L}(\\theta^{\\mathrm{ref}};G^{\\mathrm{aug}})\\right)^{-1}\\nabla\\left(\\sum_{v\\in V^{\\mathrm{I}}\\cup V^{\\mathrm{R}}}\\ell(\\theta^{\\mathrm{ref}};v,E)-\\sum_{v\\in V^{\\mathrm{I}}}\\ell(\\theta^{\\mathrm{ref}};v,E/E^{\\mathrm{R}})\\right),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "which completes the proof. ", "page_idx": 13}, {"type": "text", "text": "A.2.2 Proof of Theorem 3.2 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Assume $\\ell$ is twice-differentiable everywhere and convex, $\\begin{array}{r}{\\|\\nabla\\ell\\|_{2}\\leq c_{1},\\nabla^{2}\\sum_{v\\in V/V^{\\mathrm{R}}}\\ell(\\theta;v,E/E^{\\mathrm{R}})}\\end{array}$ is $\\gamma_{1}$ -Lipschitz, the error bound for the approximation error is given by: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|\\nabla\\sum_{v\\in V/V^{\\mathrm{R}}}\\ell(\\theta^{\\mathrm{aug}};v,E/E^{\\mathrm{R}})\\|_{2}\\le C(|V^{\\mathrm{R}}|+2|V^{\\mathrm{I}}|)^{2}=C\\cdot\\delta(V^{\\mathrm{R}},E^{\\mathrm{R}}),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $|\\cdot|$ denotes the cardinality of a set, and $\\delta(\\cdot,\\cdot)$ denotes the number of nodes removed and influenced, given $V^{\\mathrm{R}}$ and $E^{\\mathrm{R}}$ . $C$ is a constant depending on the GNN model. ", "page_idx": 14}, {"type": "text", "text": "Proof. Firstly, we consider an empirical loss for the reference model, which consists of a crossentropy and a $L_{2}$ -regularization: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\ell(\\theta;v,E)=\\mathrm{CE}(\\theta;v,E)+\\frac{\\lambda}{2}\\|\\theta\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\operatorname{CE}(\\cdot,\\cdot)$ represents the cross-entropy, and we omit the ground truth for simplicity. $\\lambda\\,>\\,0$   \ndenotes the $L_{2}$ -regularization. ", "page_idx": 14}, {"type": "text", "text": "Let $\\begin{array}{r}{G(\\theta)=\\nabla\\sum_{v\\in V/V^{\\mathrm{R}}}\\ell(\\theta;v,E/E^{\\mathrm{R}}),H_{0}}\\end{array}$ denote the Hessian of $\\begin{array}{r}{\\sum_{v\\in V/V^{\\mathrm{R}}}\\ell(\\theta^{\\mathrm{ref}};v,E/E^{\\mathrm{R}})}\\end{array}$ , and let ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Delta=\\nabla\\left(\\sum_{v\\in V^{\\mathrm{I}}\\cup V^{\\mathrm{R}}}\\ell(\\theta^{\\mathrm{ref}};v,E)-\\sum_{v\\in V^{\\mathrm{I}}}\\ell(\\theta^{\\mathrm{ref}};v,E/E^{\\mathrm{R}})\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By Taylor\u2019s Theorem, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{G(\\theta^{\\mathrm{aug}})\\approx\\!G(\\theta^{\\mathrm{ref}}+H_{0}^{-1}\\Delta)}\\\\ &{\\quad\\quad\\quad=\\!G(\\theta^{\\mathrm{ref}})+\\nabla G(\\theta^{\\mathrm{ref}}+\\eta H_{0}^{-1}\\Delta)H_{0}^{-1}\\Delta}\\\\ &{\\quad\\quad\\quad=\\!G(\\theta^{\\mathrm{ref}})+H_{\\eta}H_{0}^{-1}\\Delta,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $H_{\\eta}$ denotes the hessian at $\\theta_{\\eta}=\\theta^{\\mathrm{ref}}+\\eta H_{0}^{-1}\\Delta,\\eta\\in[0,1].$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{G(\\theta^{\\mathrm{ref}})+\\Delta=\\displaystyle\\nabla\\sum_{v\\in V/V^{\\mathrm{R}}}\\ell(\\theta^{\\mathrm{ref}};v,E/E^{\\mathrm{R}})+\\Delta}\\\\ &{\\qquad\\qquad=\\nabla\\displaystyle\\sum_{v\\in V^{\\mathrm{UI}}}\\ell(\\theta^{\\mathrm{ref}};v,E/E^{\\mathrm{R}})+\\nabla\\displaystyle\\sum_{v\\in V^{\\mathrm{I}}}\\ell(\\theta^{\\mathrm{ref}};v,E/E^{\\mathrm{R}})+\\Delta}\\\\ &{\\qquad=\\nabla\\displaystyle\\sum_{v\\in V}\\ell(\\theta^{\\mathrm{ref}};v,E)}\\\\ &{\\qquad=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Since $G(\\theta^{\\mathrm{aug}})=H_{\\eta}H_{0}^{-1}\\Delta-\\Delta=(H_{\\eta}-H_{0})H_{0}^{-1}\\Delta.$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|G(\\theta^{\\mathrm{aug}})\\|_{2}=\\|(H_{\\eta}-H_{0})H_{0}^{-1}\\Delta\\|_{2}\\leq\\|H_{\\eta}-H_{0}\\|_{2}\\|H_{0}^{-1}\\Delta\\|_{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Assume the Hessian of $\\begin{array}{r}{\\sum_{v\\in V/V^{\\mathrm{R}}}\\ell(\\theta;v,E/E^{\\mathrm{R}})}\\end{array}$ is $\\gamma_{1}$ -Lipschitz, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\|H_{\\eta}-H_{0}\\|_{2}=\\|\\nabla^{2}\\sum_{v\\in V/V^{\\mathrm{R}}}\\ell(\\theta_{\\eta};v,E/E^{\\mathrm{R}})-\\nabla^{2}\\sum_{v\\in V/V^{\\mathrm{R}}}\\ell(\\theta^{\\mathrm{ref}};v,E/E^{\\mathrm{R}})\\|_{2}}}\\\\ &{\\le\\gamma_{1}\\|\\theta_{\\eta}-\\theta^{\\mathrm{ref}}\\|_{2}}\\\\ &{=\\gamma_{1}\\|\\eta H_{0}^{-1}\\Delta\\|_{2}}\\\\ &{\\le\\gamma_{1}\\|H_{0}^{-1}\\Delta\\|_{2},\\quad\\mathrm{since}\\,\\eta\\in[0,1].}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then we have $\\|G(\\theta^{\\mathrm{aug}})\\|_{2}\\leq\\gamma_{1}\\|H_{0}^{-1}\\Delta\\|_{2}^{2}$ . Since ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\Delta\\|_{2}=\\|\\nabla\\displaystyle\\sum_{v\\in V^{\\mathrm{I}}\\cup V^{\\mathrm{R}}}\\ell(\\theta^{\\mathrm{ref}};v,E)-\\nabla\\displaystyle\\sum_{v\\in V^{\\mathrm{I}}}\\ell(\\theta^{\\mathrm{ref}};v,E/E^{\\mathrm{R}})\\|_{2}}\\\\ &{\\qquad\\le\\|\\nabla\\displaystyle\\sum_{v\\in V^{\\mathrm{I}}\\cup V^{\\mathrm{R}}}\\ell(\\theta^{\\mathrm{ref}};v,E)\\|_{2}+\\|\\nabla\\displaystyle\\sum_{v\\in V^{\\mathrm{I}}}\\ell(\\theta^{\\mathrm{ref}};v,E/E^{\\mathrm{R}})\\|_{2}}\\\\ &{\\qquad\\le\\displaystyle\\sum_{v\\in V^{\\mathrm{I}}\\cup V^{\\mathrm{R}}}\\|\\nabla\\ell(\\theta^{\\mathrm{ref}};v,E)\\|_{2}+\\displaystyle\\sum_{v\\in V^{\\mathrm{I}}}\\|\\nabla\\ell(\\theta^{\\mathrm{ref}};v,E/E^{\\mathrm{R}})\\|_{2}}\\\\ &{\\qquad\\le(|V^{\\mathrm{R}}|+2|V^{\\mathrm{I}}|)c_{1},\\quad\\mathrm{assume~}\\|\\nabla\\ell\\|_{2}\\le c_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since $\\begin{array}{r}{\\sum_{v\\in V/V^{\\mathrm{R}}}\\ell(\\theta;v,E/E^{\\mathrm{R}})}\\end{array}$ is $\\lambda(|V|-|V^{\\mathrm{R}}|)$ -strongly convex, we have $\\begin{array}{r}{\\|H_{0}^{-1}\\|_{2}\\le\\frac{1}{\\lambda(|V|-|V^{\\mathrm{R}}|)}}\\end{array}$ In practice, we keep $\\vert V^{\\mathrm{R}}\\vert$ as a fixed value, thus $\\begin{array}{r}{\\|H_{0}^{-1}\\|_{2}\\leq\\frac{1}{c_{2}\\lambda}}\\end{array}$ , where $c_{2}=|V|-|V^{\\mathrm{R}}|$ . Finally, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|G(\\theta^{\\mathrm{aug}})\\|_{2}\\le\\gamma_{1}\\|H_{0}^{-1}\\Delta\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\le\\gamma_{1}\\|H_{0}^{-1}\\|_{2}^{2}\\|\\Delta\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\le\\frac{\\gamma_{1}c_{1}^{2}}{c_{2}^{2}\\lambda^{2}}(|V^{\\mathrm{R}}|+2|V^{\\mathrm{I}}|)^{2}}\\\\ &{\\qquad\\qquad=C(|V^{\\mathrm{R}}|+2|V^{\\mathrm{I}}|)^{2},\\quad\\mathrm{where~}C=\\frac{\\gamma_{1}c_{1}^{2}}{c_{2}^{2}\\lambda^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "A.3 Training algorithm and complexity analysis ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Complexity of generating approximated models. As the computation of gradients can be efficiently handled by the PyTorch Autograd Engine, the primary operation is solving the inverse of the Hessian (cf. Eq. (4)). To mitigate the high computational cost, we follow [29] in converting the inverse computation into finding the minimizer of a quadratic function, resulting in an approximated solution. By leveraging efficient Hessian-vector products and the conjugate gradient method, this can be solved with time complexity of $O(t|\\theta|)$ , where $|\\theta|$ denotes the number of parameters, and $t$ represents the number of iterations in conjugate gradient method. ", "page_idx": 15}, {"type": "text", "text": "Training algorithm. The training algorithm for our attack is summarized in Algorithm 1. ", "page_idx": 15}, {"type": "text", "text": "A.4 More experiment settings ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Details of datasets. The statistics of the datasets used in this work are summarized in Table 5. ", "page_idx": 15}, {"type": "table", "img_path": "Luxk3z1tSG/tmp/159918a74e4147a30f5cd41b512e1b548f9e6ec5e4e91a777d7862e346b731fa.jpg", "table_caption": ["Table 5: Dataset statistics. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "\u2022 Facebook [37]: This dataset consists of 4,039 nodes and 176,468 edges. Nodes have features like birthday, education, work, name, location, gender, hometown, and language, all anonymized for privacy. The target GNN\u2019s task is to classify users\u2019 education types. ", "page_idx": 15}, {"type": "text", "text": "\u2022 PubMed [38]: This dataset includes 19,717 scientific publications related to diabetes, with a citation network of 88,648 links. Each publication is described by a TF/IDF weighted word vector from a dictionary of 500 unique words, such as male, female, children, cholesterol, and insulin. The target GNN\u2019s task is to classify the topic categories of the publications. ", "page_idx": 15}, {"type": "text", "text": "Algorithm 1 Overall algorithm for the proposed attack. ", "page_idx": 16}, {"type": "text", "text": "Input: Auxilary graph $G^{\\mathrm{aug}}$ , target model\u2019s parameters or posterior probabilities. Number of reference graphs $r$ , number of perturbations $k$ , number of selected augmented graphs $q$ , weight $w$ for sampling reference graphs.   \nOutput: The inferred sensitive property $\\mathcal{P}(G^{\\mathrm{tar}})$ of the target graph $G^{\\mathrm{tar}}$ .   \n1: Partition $G^{\\mathrm{aug}}$ into multiple communities using Louvain community detection.   \n2: Sample $r$ reference graphs using the proposed structure-aware random walk, with weight $w$ . 3: for each reference graph $G^{\\mathrm{ref}}$ do   \n4: Train reference model $\\theta^{\\mathrm{ref}}$ .   \n5: $\\emptyset\\rightarrow{\\mathcal{G}}^{\\mathrm{aug}}$ , $\\varnothing\\rightarrow{\\mathrm{Errors}}$ , $\\emptyset\\rightarrow$ Perturbs.   \n6: for $i$ in $1,\\ldots,k$ do   \n7: Randomly select a set of nodes $V^{\\mathrm{R}}$ and a set of edges $E^{\\mathrm{R}}$ from $G^{\\mathrm{ref}}$ .   \n8: Add $G_{i}^{\\mathrm{aug}}$ into $\\mathcal{G}^{\\mathrm{aug}}$ .   \n9: Add $(V^{\\mathrm{R}},E^{\\mathrm{R}})$ into Perturbs.   \n10: Compute the approximation error criterion in Eq. (5), and add it into Errors.   \n11: end for   \n12: Solve the optimization problem in Eq. (6), select $q$ perturbations.   \n13: Calculate the $q$ approximated models by Eq. (4).   \n14: Save the $q$ parameters (or posterior) and the $q$ properties of the selected augmented graphs. 15: end for   \n16: Train an attack model based on the $r\\cdot q$ parameters (or posterior) and properties, classify $\\mathcal{P}(G^{\\mathrm{tar}})$ of the target graph $G^{\\mathrm{tar}}$ . ", "page_idx": 16}, {"type": "text", "text": "\u2022 Pokec [39]: This online social network dataset is from Slovakia. Each node has anonymized features such as gender, age, and hobbies. We follow [21] to sample nodes with relatively complete features, resulting in a graph with 40,478 nodes and 531,736 edges, using gender, age, height, weight, and region as node features. The target GNN\u2019s task is to classify whether a user\u2019s all friendships are public. ", "page_idx": 16}, {"type": "text", "text": "Details of sensitive properties. For each dataset, we design one node property and one link property to be targeted in our attacks. For Facebook and Pokec we select gender as the property attribute. For PubMed, we select the keyword \u201cInsuli\u201d as the property feature, as it has the highest TF-IDF weight. These properties are summarized in Table 1. ", "page_idx": 16}, {"type": "text", "text": "Statistics of reference, shadow, and target graphs. The sizes and numbers of reference graphs we use are summarized in Table 6. ", "page_idx": 16}, {"type": "table", "img_path": "Luxk3z1tSG/tmp/b0a5c4b18c92a539fb71e481564462348be49658e77d77203acc2137ef04495d.jpg", "table_caption": ["Table 6: Numbers and sizes of reference graphs. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "For all baselines, we follow the settings specified in [21] to sample shadow graphs: the size of each shadow graph is $20\\%$ , $25\\%$ , and $30\\%$ of Pokec, Facebook, and Pubmed, respectively, and the number of shadow graphs is 700 for all datasets. ", "page_idx": 16}, {"type": "text", "text": "For target graphs, we sample 300 shadow graphs for each dataset; the size of each shadow graph is $20\\%$ , $25\\%$ , and $30\\%$ of Pokec, Facebook, and Pubmed datasets, respectively. To ensure fairness, we evaluate our method and baselines on the same target graphs. ", "page_idx": 16}, {"type": "text", "text": "More implementation details. All experiments are conducted on a machine of Ubuntu 20.04 system with AMD EPYC 7763 (756GB memory) and NVIDIA RTX3090 GPU (24GB memory). All models are implemented in PyTorch version 2.0.1 with CUDA version 11.8 and Python 3.8.0. ", "page_idx": 16}, {"type": "text", "text": "The attack model is trained for 100 epochs with a learning rate of 1e-3 and a weight decay of 5e-4.   \nWe use cross-entropy loss as the loss function and Adam optimizer. ", "page_idx": 17}, {"type": "text", "text": "Baselines for white-box setting. ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "\u2022 PIR-S [22]: A property inference attack considering permutation equivalence in feed-forward neural networks, using node permutations on the hidden layers of a fully connected neural network. ", "page_idx": 17}, {"type": "text", "text": "\u2022 PIR-D [22]: Similar to PIR-S, permutation equivalence is achieved by ensuring all permutations of any layer have the same set-based representation. ", "page_idx": 17}, {"type": "text", "text": "Baselines for black-box setting. ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "\u2022 PIA-MP [20]: An attack method designed for multi-party machine learning. The primary difference between PIA-MP and GPIA is that in the former, all shadow models (as well as the target model) are queried on a fixed dataset to obtain output posterior probabilities. ", "page_idx": 17}, {"type": "text", "text": "Baselines for both settings. ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "\u2022 GPIA [21]: A method designed for graphs and GNNs in both white-box and black-box settings, following the traditional attack framework. The inferred properties include node properties and link properties. ", "page_idx": 17}, {"type": "text", "text": "\u2022 AIA [42]: We conduct an attribute inference attack that predicts the property attributes by accessing the parameters or posteriors. We then evaluate the property inference performance based on the predicted values of the property attributes. ", "page_idx": 17}, {"type": "text", "text": "A.5 More experiment results ", "text_level": 1, "page_idx": 17}, {"type": "table", "img_path": "Luxk3z1tSG/tmp/a2ccfa6da478876a9394dfae4c54090fe461f7e354faa13d173a95264764a460.jpg", "table_caption": ["Table 7: Standard deviation. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Standard deviations for Table 2. Table 7 reports the standard deviations corresponding to the average accuracies in Table 2. ", "page_idx": 17}, {"type": "text", "text": "ROC-AUC results. We also report the ROC-AUCs of the proposed attack method compared to other baseline methods on the six sensitive properties, as shown in Table 8. The results demonstrate that our model can consistently achieve the best ROC-AUC result, confirming its notable effectiveness. ", "page_idx": 17}, {"type": "text", "text": "Table 8: ROC-AUC comparison of our method and baselines. \u201cNode\u201d and \u201cLink\u201d denote node and link properties respectively. The best results are highlighted in bold. ", "page_idx": 17}, {"type": "table", "img_path": "Luxk3z1tSG/tmp/7a4e248f6cb992f81bc2a82939692aa9895baba684e3ee866c120c9919a6b2b8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "A.6 Limitation and future work ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this work, we consider two settings: white-box and black-box, which encompass many real-world scenarios. However, stricter cases exist where the attacker can make only a limited number of queries or access only model predictions (i.e., classification results). We acknowledge that our method does not yet address these cases. Additionally, some studies explore scenarios where attackers have enhanced capabilities, such as data poisoning. We leave the investigation of efficient attacks under these conditions for future work. ", "page_idx": 18}, {"type": "text", "text": "A.7 Potential impacts ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "While the proposed method is designed to infer properties of specific graph data, our primary objective is to raise awareness of the privacy and security concerns associated with GNNs and to encourage the implementation of protective measures in model design. Traditional property inference methods are often inefficient, and despite efforts to illuminate potential threats, less practical attack scenarios may not receive adequate attention. Nonetheless, the privacy risks persist. We seek to bring this threat to the forefront and advocate for the adoption of more robust protective measures. ", "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: See abstract and $\\S\\ 1$ . ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: See Appendix A.6. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: See Theorem 3.1, Theorem 3.2, and Appendix A.2. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: See $\\S\\,4.1$ and Appendix A.4. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: See the code link in implementation details. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: See $\\S\\,4.1$ and Appendix A.4. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: See the standard deviation in Table 7. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: See Appendix A.4 more implementation details. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: See Appendix A.7. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: This work does not involve such risks. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: See the code link in implementation details and citations. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 23}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: See the code link provided in implementation details. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This work does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: This work does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]