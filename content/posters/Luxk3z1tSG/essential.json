{"importance": "This paper is crucial for researchers working with graph neural networks (GNNs) and handling sensitive data. It presents a novel and efficient method for assessing the risk of privacy breaches associated with sharing trained GNN models.  The efficient model approximation technique proposed offers significant advancements in computational efficiency over existing approaches, making it more practical for real-world applications. This work opens up avenues for further research in privacy-preserving machine learning techniques for graph data and improves the security of GNN model sharing.", "summary": "New efficient attack reveals GNN model training data properties.", "takeaways": ["Efficient graph property inference attack using model approximation drastically reduces the number of shadow models needed.", "A novel selection mechanism improves attack accuracy and diversity of approximated models.", "Substantial improvements in attack accuracy and efficiency demonstrated across various datasets."], "tldr": "Sharing trained graph neural network (GNN) models, while seemingly privacy-preserving, can inadvertently leak sensitive training data properties.  Current methods for detecting this risk are computationally expensive, hindering practical application. \nThis paper introduces an efficient graph property inference attack. By leveraging model approximation techniques and a novel model selection mechanism, this attack requires training only a small set of models and can achieve significantly higher accuracy and efficiency than existing approaches.  The method utilizes model approximation and a diversity-error optimization to ensure both accuracy and diversity of approximated models. Experiments demonstrate substantial performance gains in various real-world scenarios.", "affiliation": "Zhejiang University", "categories": {"main_category": "AI Theory", "sub_category": "Privacy"}, "podcast_path": "Luxk3z1tSG/podcast.wav"}