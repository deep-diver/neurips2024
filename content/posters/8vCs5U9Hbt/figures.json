[{"figure_path": "8vCs5U9Hbt/figures/figures_0_1.jpg", "caption": "Figure 1: Performance and computational efficiency evaluation for MTO methods evaluated on NYUv2. Each method's training time is relative to a baseline method, which minimizes the sum of task-specific empirical risks. Left-bottom marks comprehensive optimal results.", "description": "This figure compares different Multi-Task Optimization (MTO) methods on the NYUv2 dataset.  It shows a scatter plot with the relative training time on the x-axis and the change in multi-task performance (\u0394m%) relative to a baseline method on the y-axis. Each point represents a different MTO method, categorized as gradient-oriented, loss-oriented, or the authors' proposed method (GO4Align). The plot visually demonstrates that GO4Align achieves superior performance compared to most other methods while maintaining comparable or even lower computational costs. The lower left corner represents the ideal point of faster training and better performance.", "section": "Introduction"}, {"figure_path": "8vCs5U9Hbt/figures/figures_2_1.jpg", "caption": "Figure 2: Multi-task alignment and effects on performance. We visualize relative task performance curves (lower is better) over training epochs. Better overall performance usually occurs with lower convergence differences. Our method effectively reduces the convergence difference and achieves a better overall performance.", "description": "This figure shows the relationship between multi-task alignment and overall performance in multi-task optimization.  It presents relative task performance curves over training epochs for three different methods: UW, FAMO, and GO4Align.  The curves show that methods with smaller convergence differences (the difference in the number of epochs needed for each task to converge) tend to achieve better overall performance.  GO4Align, which explicitly addresses task alignment, demonstrates the smallest convergence difference and the best overall performance.", "section": "Motivation of Multi-Task Alignment"}, {"figure_path": "8vCs5U9Hbt/figures/figures_6_1.jpg", "caption": "Figure 1: Performance and computational efficiency evaluation for MTO methods evaluated on NYUv2. Each method's training time is relative to a baseline method, which minimizes the sum of task-specific empirical risks. Left-bottom marks comprehensive optimal results.", "description": "This figure compares various Multi-Task Optimization (MTO) methods on the NYUv2 dataset, focusing on their performance (measured by MTL Performance Am, lower is better) and computational efficiency (relative training time, lower is better).  The baseline method used for comparison is one that minimizes the sum of individual task risks. The figure highlights that GO4Align achieves superior performance compared to other methods while maintaining comparable computational efficiency.  The placement of methods on the chart visually represents a tradeoff between performance and computational cost.", "section": "1 Introduction"}, {"figure_path": "8vCs5U9Hbt/figures/figures_8_1.jpg", "caption": "Figure 5: Comparative analysis of the influence of the group assignment matrix and group weights on NYUv2. The x-axis in the subplots denotes the epoch, and the intensity of the color indicates the weight value. (a-d) have fixed group weights w = [w\u00b9, w\u00b2] but various group assignment matrices G. (f-i) have various group weights w but a fixed group assignment matrix G. (e) is our method that dynamically exploits a group assignment matrix and group weights for each iteration. The right side of each method shows relative performance drops on each task and their average one.", "description": "This figure shows an ablation study on the effects of group assignment matrices and group weights in the GO4Align model.  It compares the performance of using fixed weights and varying group assignments (a-d), varying weights and a fixed group assignment (f-i), and the dynamic approach used in the GO4Align model (e).  Each subplot shows the learning progress of each task ('Seg.', 'Depth', 'Normal') over epochs. The color intensity represents the weight assigned to each group. The right side of each subplot presents the relative performance drop for each task and the average performance drop.", "section": "5.2 Ablation Study"}, {"figure_path": "8vCs5U9Hbt/figures/figures_9_1.jpg", "caption": "Figure 6: Identification of \u201celbow\u201d points on NYUv2 and QM9. According to the conventional elbow method, we set the group number of the two datasets as 2 and 5, respectively.", "description": "This figure shows the result of applying the elbow method to determine the optimal number of groups (K) for the NYU-v2 and QM9 datasets.  The elbow method is a heuristic used in clustering to identify the optimal number of clusters. The x-axis represents the number of clusters (K), and the y-axis represents the average relative performance drop (Am%).  The plot shows that for NYU-v2 (with 3 tasks), the optimal K is 2, while for QM9 (with 11 tasks), the optimal K is 5. The optimal K values are chosen based on the 'elbow' point of the plot, which represents the point of diminishing returns for increasing the number of clusters.", "section": "5.2 Ablation Study"}, {"figure_path": "8vCs5U9Hbt/figures/figures_13_1.jpg", "caption": "Figure 7: Optimization process of the proposed adaptive group risk minimization principle. At each iteration, given the randomly sampled mini-batch data, we first compute the group information w and G in the lower-level optimization and then update the model's parameter \u03b8 in the upper-level optimization.", "description": "This figure illustrates the optimization process of GO4Align, which involves a bi-level optimization framework. The lower level focuses on dynamically assigning tasks into groups and determining group weights based on task interactions and learning progress. The upper level utilizes the group information from the lower level to update the model parameters, aligning the learning progress across tasks.  The iterative process of lower-level grouping and upper-level parameter updates is visualized, showing how task interactions and risk information from previous iterations influence the task alignment and model learning.", "section": "A Algorithm of GO4Align"}, {"figure_path": "8vCs5U9Hbt/figures/figures_16_1.jpg", "caption": "Figure 8: Analysis on risk ratios. Compared with other works, the proposed GO4Align shows more stable ratios among tasks over epochs, indicating that GO4Align can maintain better alignment throughout the training process.", "description": "This figure visualizes the ratios between task-specific empirical risk and the sum of all empirical risks before and after scaling for different methods across epochs. The x-axis represents training epochs (from 1 to 200), and the stacked bars in each subplot represent the relative risks of three tasks in NYUv2 dataset: \"Normal\", \"Seg.\", and \"Depth\". The top row shows unscaled risks, while the bottom row shows risks after applying the scaling method of each algorithm.  The figure demonstrates that GO4Align maintains more stable risk ratios compared to other methods (UW, MGDA, NashMTL, and FAMO), indicating better alignment and balance between tasks during the training process.  This stability suggests GO4Align's effectiveness in mitigating task imbalance.", "section": "5.2 Ablation Study"}]