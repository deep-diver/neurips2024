[{"figure_path": "QHRLFdhkLu/figures/figures_0_1.jpg", "caption": "Figure 1: Performance comparison between default LLM and reference trustable decoding in reasoning tests.", "description": "This figure presents a comparison of the performance of different LLMs on several reasoning tasks. It shows the results for a baseline LLM (default decoding) and an augmented version using the proposed Reference Trustable Decoding (RTD) method. The figure displays results for both zero-shot and five-shot settings, with and without multi-head RTD.  Each point on the radar charts represents the performance of a given method on a specific reasoning test (STEM, Social Science, OBQA, Humanities, Other, ARC-E, and ARC-C). The comparison highlights the improvement in reasoning capabilities achieved by RTD, particularly in five-shot scenarios.", "section": "Abstract"}, {"figure_path": "QHRLFdhkLu/figures/figures_2_1.jpg", "caption": "Figure 2: The pipeline of LLM inference and the focus of different methods: ICL focuses on the prompt stage, emphasizing the optimization of the model's input. Fine-tuning methods optimize the model itself by adjusting its parameters. In contrast, our proposed RTD method targets the decoding stage of the language model. By constructing a reference datastore, RTD optimizes the final output distribution without requiring additional training.", "description": "This figure illustrates the different approaches to adapting Large Language Models (LLMs) to downstream tasks.  It shows the standard LLM pipeline, highlighting the input (prompts), tokenization, embedding, and autoregressive decoding process.  It then compares three methods: In-Context Learning (ICL), which focuses on optimizing the input; Parameter-Efficient Fine-Tuning (PEFT), which modifies the model's parameters; and the authors' proposed Reference Trustable Decoding (RTD), which optimizes the final output distribution using a reference datastore without further training.", "section": "1 Introduction"}, {"figure_path": "QHRLFdhkLu/figures/figures_4_1.jpg", "caption": "Figure 3: Overview of the reference datastore generation and reference trustable decoding process.", "description": "This figure illustrates the overall architecture of the Reference Trustable Decoding (RTD) method.  It shows how a reference datastore is created from a task dataset. The datastore contains key-value pairs, where keys are the last hidden states of the language model and values are corresponding labels. During inference, the language model processes the input, generating hidden states. RTD then uses these states to query the reference datastore, retrieving relevant references, which are then used to refine the final output distribution.  The process concludes with the generation of the next token.", "section": "3 Reference Trustable Decoding"}, {"figure_path": "QHRLFdhkLu/figures/figures_4_2.jpg", "caption": "Figure 4: Three stages of reference trustable decoding.", "description": "This figure illustrates the three stages of the Reference Trustable Decoding (RTD) process: Fetch, Normalization, and Aggregation. In the Fetch stage, the Euclidean distance between the last hidden states from the language model and all keys in the reference datastore is calculated.  The top K nearest keys are then selected and stored in a set Lh. The Normalization stage involves scaling these distances using a temperature parameter T and then applying the softmax function to obtain a valid probability distribution. Finally, the Aggregation stage sums the probabilities for all values that share the same label to yield the final reference possibility distribution r. This distribution is then combined with the output distribution from the Language Model's LM Head to produce the final output.", "section": "3.2 Reference Trustable Decoding"}, {"figure_path": "QHRLFdhkLu/figures/figures_5_1.jpg", "caption": "Figure 5: Comparison between RTD and multi-head RTD.", "description": "This figure illustrates the difference between the single-head RTD and the multi-head RTD. In single-head RTD, the last hidden states h<sup>(l)</sup> from the language model are directly used to query the reference datastore L, which returns a distribution r.  In multi-head RTD, however, h<sup>(l)</sup> is first split into multiple heads (h<sup>(l,1)</sup>, h<sup>(l,2)</sup>,...,h<sup>(l,h)</sup>), and each head independently queries a corresponding sub-datastore L<sup>(i)</sup>. The resulting distributions from each head (r<sup>(1)</sup>, r<sup>(2)</sup>,...,r<sup>(h)</sup>) are then merged to produce the final distribution r.", "section": "3.3 Multi-head Reference Trustable Decoding"}, {"figure_path": "QHRLFdhkLu/figures/figures_9_1.jpg", "caption": "Figure 1: Performance comparison between default LLM and reference trustable decoding in reasoning tests.", "description": "This figure presents a comparison of the performance of a large language model (LLM) using standard decoding versus the proposed Reference Trustable Decoding (RTD) method.  The results are shown across various reasoning tests, broken down by category (e.g., STEM, Social Science, Humanities).  It visually demonstrates that RTD consistently improves the LLM's performance compared to the baseline in different reasoning domains.", "section": "Abstract"}]