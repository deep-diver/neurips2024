[{"figure_path": "Px1hQM72iX/tables/tables_5_1.jpg", "caption": "Table 1: Result comparison on the retrieval task. For the same metric on each dataset, the best is bold and the second best is underlined. We use four different symbols to indicate the different categories of methods detailed in Sec. 5.4. Cases where our model significantly outperforms the best baseline, with p \u2264 0.01 according to the paired t-test, are marked with *.", "description": "This table presents the performance comparison of different recommendation models on three datasets (Amazon, MovieLens, and Taobao) using four metrics: Interest Coverage, Interest Relevance, Exposure Deviation, and Tail Exposure Improvement.  Each metric is evaluated at three different k values (20, 50, 100), showing the effectiveness of each model in retrieving items that satisfy a user's multiple interests. The best performing model for each metric and dataset is highlighted in bold. The results are statistically significant (p\u22640.01) for cases where GPR4DUR outperforms the best baseline.", "section": "5 Offline Experiments"}, {"figure_path": "Px1hQM72iX/tables/tables_7_1.jpg", "caption": "Table 1: Result comparison on the retrieval task. For the same metric on each dataset, the best is bold and the second best is underlined. We use four different symbols to indicate the different categories of methods detailed in Sec. 5.4. Cases where our model significantly outperforms the best baseline, with p \u2264 0.01 according to the paired t-test, are marked with *.", "description": "This table presents the results of the retrieval task, comparing different recommendation methods across three datasets (Amazon, MovieLens, Taobao).  Metrics include Interest Coverage (IC@k), Interest Relevance (IR@k), Exposure Deviation (ED@k), and Tail Exposure Improvement (TEI@k).  The best performing model for each metric on each dataset is shown in bold, with the second-best underlined.  Statistical significance (p \u2264 0.01) is indicated with an asterisk (*). The table helps to assess the performance of the proposed GPR4DUR model in comparison to various baseline models.", "section": "5 Offline Experiments"}, {"figure_path": "Px1hQM72iX/tables/tables_9_1.jpg", "caption": "Table 1: Result comparison on the retrieval task. For the same metric on each dataset, the best is bold and the second best is underlined. We use four different symbols to indicate the different categories of methods detailed in Sec. 5.4. Cases where our model significantly outperforms the best baseline, with p \u2264 0.01 according to the paired t-test, are marked with *.", "description": "This table presents the results of the retrieval task experiments, comparing the performance of GPR4DUR against various baselines across three datasets (Amazon, MovieLens, Taobao).  Metrics include Interest Coverage (IC@k), Interest Relevance (IR@k), Exposure Deviation (ED@k), and Tail Exposure Improvement (TEI@k) for k=20, 50, and 100.  The best and second-best results for each metric and dataset are highlighted, and statistically significant improvements of GPR4DUR over the best baseline (p\u22640.01) are indicated by asterisks (*).", "section": "5 Offline Experiments"}, {"figure_path": "Px1hQM72iX/tables/tables_14_1.jpg", "caption": "Table 1: Result comparison on the retrieval task. For the same metric on each dataset, the best is bold and the second best is underlined. We use four different symbols to indicate the different categories of methods detailed in Sec. 5.4. Cases where our model significantly outperforms the best baseline, with p \u2264 0.01 according to the paired t-test, are marked with *.", "description": "This table presents a comparison of different recommendation models on three datasets (Amazon, MovieLens, Taobao) using four metrics to evaluate retrieval performance: Interest Coverage, Interest Relevance, Exposure Deviation, and Tail Exposure Improvement. Each metric is evaluated at different k values (20, 50, 100).  The table highlights the performance of the proposed GPR4DUR model compared to various baselines, indicating its superiority in several cases.", "section": "5 Offline Experiments"}, {"figure_path": "Px1hQM72iX/tables/tables_16_1.jpg", "caption": "Table 1: Result comparison on the retrieval task. For the same metric on each dataset, the best is bold and the second best is underlined. We use four different symbols to indicate the different categories of methods detailed in Sec. 5.4. Cases where our model significantly outperforms the best baseline, with p \u2264 0.01 according to the paired t-test, are marked with *. ", "description": "This table presents a comparison of different recommendation models on three datasets (Amazon, MovieLens, Taobao) using four metrics to evaluate retrieval performance: Interest Coverage, Interest Relevance, Exposure Deviation, and Tail Exposure Improvement. Each metric is calculated at three different values of k (20, 50, 100). The table highlights the superior performance of the proposed GPR4DUR model compared to other existing methods across the various datasets and metrics. The significance of the outperformance is indicated using a paired t-test.", "section": "5 Offline Experiments"}, {"figure_path": "Px1hQM72iX/tables/tables_17_1.jpg", "caption": "Table 1: Result comparison on the retrieval task. For the same metric on each dataset, the best is bold and the second best is underlined. We use four different symbols to indicate the different categories of methods detailed in Sec. 5.4. Cases where our model significantly outperforms the best baseline, with p \u2264 0.01 according to the paired t-test, are marked with *.", "description": "This table presents a comparison of different recommendation models on three datasets (Amazon, MovieLens, Taobao) using four evaluation metrics: Interest Coverage (IC@k), Interest Relevance (IR@k), Exposure Deviation (ED@k), and Tail Exposure Improvement (TEI@k).  The metrics assess the ability of the models to capture multiple user interests, the relevance of recommendations, the evenness of category exposure, and the exposure of less popular interests. The best performing model for each metric and dataset is shown in bold, highlighting the superior performance of GPR4DUR in many cases.", "section": "5 Offline Experiments"}, {"figure_path": "Px1hQM72iX/tables/tables_17_2.jpg", "caption": "Table 7: Hyperparameter Settings and Sensitivity Analysis for GPR Parameter Tuning.", "description": "This table presents the results of a sensitivity analysis performed to determine optimal hyperparameter settings for the Gaussian Process Regression (GPR) model used in GPR4DUR.  The analysis focuses on the impact of different kernel functions (Cosine and RBF with varying standard deviations \u03c3) on the retrieval and ranking performance metrics (IC@50 and Recall@50). The results are shown separately for the three datasets used in the study: Amazon, MovieLens, and Taobao.  This helps to understand how different kernel functions and their parameters affect the model's ability to accurately capture and predict user interests for multi-interest retrieval and ranking tasks.", "section": "5.3 Metrics"}, {"figure_path": "Px1hQM72iX/tables/tables_18_1.jpg", "caption": "Table 1: Result comparison on the retrieval task. For the same metric on each dataset, the best is bold and the second best is underlined. We use four different symbols to indicate the different categories of methods detailed in Sec. 5.4. Cases where our model significantly outperforms the best baseline, with p \u2264 0.01 according to the paired t-test, are marked with *.", "description": "This table presents a comparison of different recommendation models on three datasets (Amazon, MovieLens, and Taobao) using four metrics: Interest Coverage (IC@k), Interest Relevance (IR@k), Exposure Deviation (ED@k), and Tail Exposure Improvement (TEI@k).  Higher values are better for IC@k and IR@k, while lower values are better for ED@k.  The results show the performance of GPR4DUR against baselines such as MostPop, YoutubeDNN, and others across different values of k (20, 50, and 100).  The * indicates that GPR4DUR's performance is statistically significantly better than the best baseline for that metric.", "section": "5 Offline Experiments"}, {"figure_path": "Px1hQM72iX/tables/tables_18_2.jpg", "caption": "Table 1: Result comparison on the retrieval task. For the same metric on each dataset, the best is bold and the second best is underlined. We use four different symbols to indicate the different categories of methods detailed in Sec. 5.4. Cases where our model significantly outperforms the best baseline, with p \u2264 0.01 according to the paired t-test, are marked with *. ", "description": "This table presents the results of the retrieval task comparing the proposed GPR4DUR model against other existing methods across three different datasets (Amazon, MovieLens, Taobao).  Metrics used include Interest Coverage, Interest Relevance, Exposure Deviation, and Tail Exposure Improvement. The best performing model for each metric in each dataset is highlighted in bold, and the second-best is underlined.  Asterisks indicate statistically significant improvements over the best baseline (p\u22640.01).", "section": "5 Offline Experiments"}, {"figure_path": "Px1hQM72iX/tables/tables_19_1.jpg", "caption": "Table 1: Result comparison on the retrieval task. For the same metric on each dataset, the best is bold and the second best is underlined. We use four different symbols to indicate the different categories of methods detailed in Sec. 5.4. Cases where our model significantly outperforms the best baseline, with p \u2264 0.01 according to the paired t-test, are marked with *. ", "description": "The table presents a comparison of different recommendation models on three datasets (Amazon, MovieLens, Taobao) across four metrics (Interest Coverage, Interest Relevance, Exposure Deviation, Tail Exposure Improvement) for different top-k values (20, 50, 100).  The best performing model for each metric and dataset is highlighted in bold, with the second-best underlined.  Statistically significant improvements (p\u22640.01) by GPR4DUR compared to the best baseline are marked with an asterisk (*).", "section": "5 Offline Experiments"}]