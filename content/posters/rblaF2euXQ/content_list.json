[{"type": "text", "text": "Local Anti-Concentration Class: Logarithmic Regret for Greedy Linear Contextual Bandit ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Seok-Jin Kim Columbia University New York, NY, USA seok-jin.kim@columbia.edu ", "page_idx": 0}, {"type": "text", "text": "Min-hwan Oh   \nSeoul National Univeristy   \nSeoul, South Korea   \nminoh@snu.ac.kr ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study the performance guarantees of exploration-free greedy algorithms for the linear contextual bandit problem. We introduce a novel condition, named the Local Anti-Concentration (LAC) condition, which enables a greedy bandit algorithm to achieve provable efficiency. We show that the LAC condition is satisfied by a broad class of distributions, including Gaussian, exponential, uniform, Cauchy, and Student\u2019s $t$ distributions, along with other exponential family distributions and their truncated variants. This significantly expands the class of distributions under which greedy algorithms can perform efficiently. Under our proposed LAC condition, we prove that the cumulative expected regret of the greedy algorithm for the linear contextual bandit is bounded by $\\mathcal{O}(\\mathrm{poly}\\log T)$ . Our results establish the widest range of distributions known to date that allow a sublinear regret bound for greedy algorithms, further achieving a sharp poly-logarithmic regret. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In the contextual multi-armed bandit problem [2, 6, 24, 25], an agent uses revealed contextual information in each round to decide which arm to pull, receiving a reward corresponding to the pulled arm. The stochastic version of this problem observes rewards as random samples, with their expectation tied to the arm\u2019s contextual information. The agent\u2019s goal is to design a sequential arm-pulling strategy to maximize cumulative rewards, necessitating a balance between exploration and exploitation. Linear contextual bandits, where expected reward is modeled as a linear function of contextual information, serve as the fundamental framework for contextual bandits [1, 10, 26]. Various exploration strategies, including upper confidence bound (UCB) [1, 11], Thompson sampling (TS) [3, 4], and $\\epsilon_{}$ -greedy [19] are widely used and studied in the theoretical analysis for linear contextual bandits. However, exploration can often be challenging in practice, possibly leading to over-exploration and performance deterioration. Some domains may find exploration infeasible or even unethical, and it may appear unfair in applications such as healthcare and clinical domains. Furthermore, exploration strategies tend to add complexity for algorithm designers and decisionmaking systems. ", "page_idx": 0}, {"type": "text", "text": "A greedy policy, i.e., pure exploitation without exploration, selects arms greedily based on current problem parameter estimates. While a greedy policy\u2019s effectiveness cannot be guaranteed in general since it may fail to find optimality in the worst case, the possibility of its favorable performances in certain scenarios has been of interest both practically and theoretically. Therefore, understanding when a greedy policy can perform effectively, i.e., when exploration is not needed, is a fundamental research question. Recently, a simple greedy policy has been proved to achieve near-optimal regret bounds for linear contextual bandit problems under some stochastic conditions of contexts [8, 20, 30, 33, 34]. Such efficient learning is possible if the greedy policy can benefti from suitable diversity in the contexts (or the features of arms) \u2014 so that even with exploration-free action selection, parameter estimation is effectively possible. However, distributions known in the existing literature to allow efficient greedy algorithms are mostly limited only to Gaussian and uniform distributions [8, 20, 30, 33]. Hence, the following research questions arise. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Is it possible for a wider range of distributions to allow efficient learning for greedy algorithms? If so, how can we characterize such distributions? ", "page_idx": 1}, {"type": "text", "text": "We answer the above questions affirmatively by proposing a general distributional condition that allows for a broad range of distributions to achieve provable efficiency of greedy linear bandit algorithms. In this work, we present a new Local Anti-Concentration (LAC) condition for distributions that encompasses a wider range of context distributions compared to the previous findings. We demonstrate that the class of distributions that satisfy LAC, which we denote as $L A C$ class, include Gaussian, exponential, Cauchy, Student\u2019s $t$ , and uniform distributions, as well as other exponential family distributions and their truncated variants. This study provides the first evidence that greedy algorithms can perform efficiently beyond Gaussian and uniform distributions. Our findings significantly expand the class of admissible distributions that are suitable for greedy algorithms for linear contextual bandits. ", "page_idx": 1}, {"type": "text", "text": "Our proposed LAC condition not only broadens the class of permissible distributions for greedy bandit algorithms but also facilitates a sharper regret guarantee, achieving a poly-logarithmic $\\mathcal{O}(\\mathrm{poly}\\log T)$ regret for greedy algorithms. Our regret analysis constitutes a distinct improvement over the previously known results for greedy linear contextual bandit algorithms. The existing results are primarily categorized into two folds: (i) Gaussian-distributed contexts could only yield $\\mathcal{O}(\\sqrt{T})$ regret for greedy algorithms for single-parameter linear contextual bandits [20, 30, 33]; (ii) Context diversity (e.g., Assumption 3 in [8]) alone was previously regarded as not sufficient to derive a poly-logarithmic regret but additionally assuming a margin condition (e.g., Assumption 2 in [8]) can achieve $\\mathcal{O}(\\mathrm{poly}\\log T)$ regret.1 In either case, there are limited prior results about context diversity beyond Gaussian and uniform distributions. As for the margin condition, to the best of our knowledge, no prior work has rigorously examined the scaling of the margin constant, rather than simply treating it as a universal constant. To this end, we establish that Gaussian and uniform distributions as well as all of the common distributions that satisfy the LAC condition (see Table 1) induce $\\mathcal{O}(\\mathrm{poly}\\log T)$ regret without having to additionally assume a margin condition. ", "page_idx": 1}, {"type": "text", "text": "The key difference between the analysis of greedy algorithms and that of exploration-based algorithms, such as UCB and TS, for linear contextual bandits lies in the estimation bounds. While UCB [1] and TS [4] analyses involve bounding the weighted estimation error of the parameter using selfnormalized martingales, the analysis of greedy algorithms relies on the $\\ell_{2}$ estimation bound in all directions. Ensuring this estimation consistency is more challenging, especially when actions are chosen adaptively, resulting in non-i.i.d. data. ", "page_idx": 1}, {"type": "text", "text": "In this work, we prove that for a broad class of context distributions, $\\sqrt{t}$ -consistency of the estimator can be guaranteed, enabling poly-logarithmic regret for greedy algorithms. Our newly proposed class of context distributions represents the largest known class from which $\\sqrt{t}$ -consistency of the estimator can be derived, even with adaptively chosen (non-i.i.d.) contexts. To establish this consistency, we derive two key technical results. First, we show that the minimum eigenvalue of the Gram matrix increases sufficiently under the LAC condition. Additionally, we demonstrate that under this condition, the suboptimality gap can be bounded probabilistically\u2014a result derived from our analysis rather than assumed explicitly. ", "page_idx": 1}, {"type": "text", "text": "1.1 Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The main contributions of our paper are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose a novel condition, called Local Anti-Concentration (LAC) condition, for a greedy linear contextual bandit algorithm to achieve provable efficiency. The newly proposed ", "page_idx": 1}, {"type": "text", "text": "1It is important to note that the problem setting of Bastani et al. [8] (multiple parameters with shared context) differs from our setting, which involves a single parameter with separate contexts, as is predominantly studied in the linear contextual bandit literature [1, 4, 20, 30, 33]. While Bastani et al. [8] demonstrated the diversity condition and the existence of a margin for Gaussian, uniform, and Gibbs distributions in the two-armed case within multi-parameter settings, we show in Appendix J that the Gibbs distribution does not satisfy the diversity condition when extended to cases with more than two arms. ", "page_idx": 1}, {"type": "table", "img_path": "rblaF2euXQ/tmp/1da4467651f510d88b8baedcfcbb006b851e474510242232cbbb2a5247a06920.jpg", "table_caption": ["Table 1: Comparisons of Greedy Linear Contextual Bandit Studies "], "table_footnote": ["\u2020 The regret bound of Raghavan et al. [30] is shown in the Bayesian regret, which is a weaker notion of regret than the frequentist regret that we consider in our work. $^{\\ddagger}$ The problem setting of Bastani et al. [8] is a multi-parameter linear contextual bandit, where a context vector is shared across the arms, but each arm has a separate parameter. Bastani et al. [8] show that Gaussian and uniform distributions satisfy their covariate diversity condition (Assumption 3 in [8]) and the margin condition. For the two-armed case $K=2$ ), they also show that Gibbs distribution satisfies those conditions. However, we show in Appendix J that Gibbs distribution fails to satisfy the conditions for the general multi-armed case with $K\\geq3$ . "], "page_idx": 2}, {"type": "text", "text": "LAC condition is satisfied by a wide rage of common distributions, including Gaussian, exponential, Cauchy, Student\u2019s $t$ , and uniform distributions, and many common distributions, as well as their truncated variants. This significantly expands the class of admissible distributions that are suitable for greedy algorithms and is, to our best knowledge, by far the largest class of distributions that induces efficient learning for greedy algorithms. ", "page_idx": 2}, {"type": "text", "text": "\u2022 Under our proposed LAC condition, we prove that the cumulative expected regret for the greedy algorithm is bounded by $\\mathcal{O}(\\mathrm{poly}\\log T)$ (Theorem 1), the sharpest known bound for greedy algorithms in linear contextual bandits with a single parameter.   \n\u2022 By leveraging the proposed condition, we can guarantee both (i) the growth of the minimum eigenvalue of the Gram matrix and (ii) a probabilistically bounded suboptimality gap. These two steps are key technical components for analyzing greedy bandit algorithms and were explicitly assumed in existing literature [8] to achieve poly-logarithmic regret. Notably, we do not assume these steps; instead, we prove that distributions satisfying the LAC condition inherently induce these two technical results (Theorems 2 and 3), which may be of independent interest.   \n\u2022 Various context distributions have been empirically shown to allow favorable performances of greey algorithms (see Appendix L). However, the distributions previously known in the literature that enable efficient greedy algorithms were primarily limited to Gaussian and uniform distributions. Our theoretical results offer a significant step toward bridging this gap between theory and practice, providing insights into why greedy algorithms can be effective under a wide range of distributions. ", "page_idx": 2}, {"type": "text", "text": "1.2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Linear bandits and generalized linear bandits have been widely studied [1, 2, 4, 6, 10, 11, 18, 23, 27, 32]. Upper confidence bound (UCB) algorithms for the linear contextual bandit have been proposed and analyzed for their regret performance [1, 6, 10, 11, 32]. Thompson sampling [35] algorithms for linear contextual bandits have also been widely studied, with results demonstrating their effectiveness both theoretically and empirically [3, 4, 9]. While UCB [1] and Thompson sampling [3, 4] analyses rely on bounding the weighted estimation error of the parameter using self-normalized martingales, the analysis of greedy bandit algorithms depends on the $\\ell_{2}$ estimation bound in all directions. Ensuring this estimation consistency is more challenging, especially in adaptive action settings where data are not i.i.d. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Recent studies [8, 20, 30, 33] have shown that a greedy algorithm can achieve near-optimal regret performance for linear contextual bandit problems under stochastic contexts by providing sufficient conditions under which the greedy algorithm can be efficient. These conditions typically focus on the diversity of the context distribution, ensuring that the greedy policy benefits from sufficient context diversity for effective parameter estimation even without exploratory actions. ", "page_idx": 3}, {"type": "text", "text": "However, the existing literature has mainly limited itself to Gaussian [8, 20, 30, 33] and uniform [8] distributions, leaving open questions about broader applicability. Specifically, it is unclear if other distributions could also support efficient greedy algorithms and what fundamental characteristics these distributions should have to enable consistent parameter estimation without exploration. Our work addresses this gap by identifying broader conditions under which diverse distributions can effectively support greedy algorithms in linear contextual bandits. ", "page_idx": 3}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "2.1 Notations ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We use $\\Vert{\\boldsymbol{x}}\\Vert_{p}$ to denote the $\\ell_{p}$ -norm of vector $x\\,\\in\\,\\mathbb{R}^{d}$ . For a positive definite matrix $A\\in\\mathbb{R}^{d\\times d}$ , we define $\\|x\\|_{A}={\\sqrt{x^{\\top}A x}}$ . We use $\\lambda_{\\mathrm{min}}(A)$ to denote the minimum eigenvalue of the positive definite matrix $A$ . We denote $\\mathbb{D}_{R}^{d}\\;:=\\;[-R,R]^{d}$ and $\\mathbb{B}_{R}^{d}\\;:=\\;\\{x\\,\\in\\,\\mathbb{R}^{d}\\,:\\,\\|x\\|_{2}\\,\\leq\\,R\\}$ . If $d$ is clear, we just write $\\mathbb{B}_{R}^{d}:=\\mathbb{B}_{R}$ and $\\mathbb{D}_{R}^{d}:=\\mathbb{D}_{R}$ . We define $[n]$ for a set $[n]:=\\{1,2,\\dots,n\\}$ . We write $\\mathbb{S}^{d-1}$ for a $d$ -dimensional unit sphere. We set $\\|X\\|_{\\psi_{1}}=\\operatorname*{sup}_{p\\geq1}\\{p^{-1}\\mathbb{E}^{1/p}|X|^{p}\\},\\|X\\|_{\\psi_{2}}=$ $\\operatorname*{sup}_{p\\geq1}\\{p^{-\\frac{1}{2}}\\mathbb{E}^{1/p}|X|^{p}\\}$ for a random variable $X$ . If $X$ is a $d$ -dimensional random vector, then we write $\\begin{array}{r}{\\|X\\|_{\\psi_{2}}=\\operatorname*{sup}_{\\|u\\|_{2}=1}\\|\\langle u,X\\rangle\\|_{\\psi_{2}},\\|X\\|_{\\psi_{1}}=\\operatorname*{sup}_{\\|u\\|_{2}=1}\\|\\langle u,X\\rangle\\|_{\\psi_{1}}}\\end{array}$ . We use the notation ${\\mathcal{O}}()$ or $\\lesssim$ to hide constants, and $\\widetilde O()$ to hide constants and logarithmic terms. We use the notation $a\\asymp b$ when $a\\lesssim b$ and $b\\lesssim a$ . We use $c,c_{1},c_{2}\\ldots$ for absolute constant, which may differ from line by line. ", "page_idx": 3}, {"type": "text", "text": "2.2 Linear Contextual Bandits with Stochastic Contexts ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We consider the linear contextual bandit problem with $K$ arms $(K\\,\\geq\\,2)$ , where in each round $t\\,=\\,1,2,\\ldots,T$ , the set of context vectors $\\mathcal{X}(t)\\,=\\,\\{X_{i}(t)\\,\\in\\,\\mathbb{R}^{d},i\\,\\in\\,[K]\\}$ is drawn from some unknown distribution $P_{\\mathcal{X}}(t)$ . Each arm\u2019s feature $X_{i}(t)\\in\\mathcal{X}(t)$ for $i\\in[K]$ need not be independent of each other and can possibly be correlated. The agent then pulls an arm $a(t)\\in[K]$ . Each context vector $X_{i}(t)$ for $i\\in[K]$ is associated with stochastic reward $Y_{i}(t)\\in\\mathbb{R}$ with mean $X_{i}(t)^{\\top}\\theta^{\\star}$ where $\\theta^{\\star}\\in\\mathbb{R}^{d}$ is a fixed, unknown parameter. For simplicty, we assume $\\lVert\\theta^{\\star}\\rVert_{2}\\leq1$ . After pulling arm $a(t)$ , the agent receives a stochastic reward $Y_{a(t)}(t)$ as a bandit feedback: $Y_{a(t)}(t)=X_{a(t)}(t)^{\\top}\\theta^{\\star}+\\eta_{a(t)}(t)$ , where $\\eta_{a(t)}(t)\\in\\mathbb{R}$ is a zero mean noise. We assume that there is an increasing sequence of sigma fields $\\{\\mathcal{H}_{t}\\}$ such that each $\\eta_{a_{t}}(t)$ is $\\mathcal{H}_{t}$ -measurable with $\\mathbb{E}[\\eta_{a_{t}}(t)|\\mathcal{H}_{t-1}]=0$ . In our problem, $\\mathcal{H}_{t}$ is the sigma field generated by random variables of the arms chosen $\\{a(\\bar{1}),...,a(t)\\}$ , their context vectors $\\{X_{a(1)}(1),...,X_{a(t)}(t)\\}$ , and the corresponding rewards $\\{Y_{a(1)}(1),...,Y_{a(t)}(t)\\}$ . Also, $\\eta_{a(t)}(t)$ is assumed to be conditionally $\\sigma$ -sub-Gaussian, i.e., for all $\\lambda\\in\\mathbb R$ $\\mathfrak{L},\\mathbb{E}[e^{\\lambda\\eta_{a(t)}(t)}\\mid\\mathcal{H}_{t-1}]\\le\\exp\\!\\left(\\lambda^{2}\\sigma^{2}/2\\right)$ for $\\sigma\\,\\geq\\,0$ . Observing context vector $\\mathcal{X}(t)$ , let $a^{*}(t)$ denote the optimal arm in round $t$ , that is, $a^{*}(t)=\\arg\\operatorname*{max}_{i\\in[K]}X_{i}(t)^{\\top}\\theta^{\\star}$ . Then the instantaneous expected regret $(\\mathrm{reg}(t))$ and cumulative expected regret $(\\mathbf{Reg}(T))$ are defined respectively as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{Reg}(T):=\\sum_{t=1}^{T}\\mathrm{reg}(t):=\\sum_{t=1}^{T}\\mathbb{E}\\left[X_{a^{\\star}(t)}(t)^{\\top}\\theta^{\\star}-X_{a(t)}(t)^{\\top}\\theta^{\\star}\\right]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "which are respectively the instantaneous and cumulative differences between the optimal expected reward and the expected reward of the pulled arms. The expectation is taken with respect to the stochasticity of history, containing randomness of contexts. The goal of the agent is to minimize the cumulative expected regret. ", "page_idx": 3}, {"type": "text", "text": "2.3 LinGreedy: Exploration-Free Algorithm for Linear Contextual Bandits ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this work, we focus on identifying sufficient conditions that enable exploration-free greedy algorithms to efficiently learn the optimal policy. Specifically, we analyze a greedy algorithm for linear contextual bandits, which we refer to as LinGreedy (Algorithm 1). The LinGreedy algorithm selects arms greedily based on the OLS estimator, without any exploratory actions. ", "page_idx": 4}, {"type": "table", "img_path": "rblaF2euXQ/tmp/6c26c36658110ddefe3a4ce62abe4eada08646ece66cd654cb0e46e4708d921d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "Description of Algorithm 1. The algorithm performs a greedy action in each round based on estimated rewards. In the initial rounds, when the Gram matrix $\\Sigma(t)$ is not yet invertible, parameter estimation is deferred, and the algorithm selects actions based on an initial parameter $\\theta_{0}$ . Once the Gram matrix becomes invertible\u2014which can be shown with high probability after sufficient time, the algorithm computes an OLS estimator and performs a greedy action based on the estimated parameter in each subsequent round. This algorithm is exploration-free. In the following sections, we present a novel and more general condition that enables efficient learning for greedy algorithms. ", "page_idx": 4}, {"type": "text", "text": "3 Local Anti-Concentration Class ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we introduce a new sufficient condition for efficient greedy contextual bandits. This condition is general and encompasses a wide range of common distributions, including Gaussian, exponential, uniform, Cauchy, and Student\u2019s $t$ distributions, as well as their truncated variants. To the best of our knowledge, this is the most extensive class of distributions considered in the greedy contextual bandit literature [8, 20, 28, 30, 31, 33], which has primarily focused on Gaussian, uniform distributions, and their truncated variants. ", "page_idx": 4}, {"type": "text", "text": "Our proposed condition centers on the rate of the log density of stochastic contexts, a concept we term Local Anti-Concentration (LAC). We now formally introduce the novel LAC class. ", "page_idx": 4}, {"type": "text", "text": "Definition 1 (Local Anti-Concentration (LAC)) $A$ density function $f_{X}$ of a random variable $X\\,\\in\\,\\mathbb{R}^{n}$ is said to satisfy the Local Anti-Concentration $(L A C)$ condition with a non-decreasing polynomial $\\lambda\\:i f$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\|\\nabla\\log f_{X}(x)\\|_{\\infty}\\leq\\lambda(\\|x\\|_{\\infty})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "for all $x\\in\\mathbb{R}^{n}$ . We refer to \u03bb as the LAC function of $X$ . We denote the class of distributions that satisfy this LAC condition as the Local Anti-Concentration class. ", "page_idx": 4}, {"type": "text", "text": "3.1 Intuition of LAC Condition ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The LAC condition implies that a density is not overly concentrated at any given point, leading to a gradual decay in density across all directions\u2014hence the term local anti-concentration. A geometric interpretation of the LAC condition and a rigorous definition of this decay rate are provided in Appendix C. Section 3.2 demonstrates that the LAC condition applies to a broad range of common distributions. To the best of our knowledge, very few distributions have been previously shown to support efficient performance guarantees for greedy algorithms. However, we prove that the LAC condition holds for a wide range of distributions, including a variety of exponential families. Note that $\\lambda$ can be a constant when contexts have bounded support (see Appendix B). In the following sections, we further explore the characteristics of the LAC condition. ", "page_idx": 4}, {"type": "text", "text": "3.2 Generality of LAC Condition ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We show that the LAC condition is applicable to various distributions, significantly expanding the class of admissible distributions for greedy linear contextual bandits. The LAC condition is satisfied when the exponential component in the exponential family has a polynomial scale. Included in the LAC class are distributions such as Gaussian, exponential, uniform, Student\u2019s $t$ , and Cauchy distributions, along with their truncated variants. ", "page_idx": 5}, {"type": "text", "text": "The following proposition demonstrates that the LAC function does not directly depend on the dimension of $X$ . We further discuss in Appendix B that the LAC condition is more closely related to the correlation structure of $X$ rather than its dimensionality. Therefore, we suggest that the LAC condition provides a suitable framework for comparing regret when both the number of arms and the dimension are large. ", "page_idx": 5}, {"type": "text", "text": "Proposition 1 Suppose the random variable $X\\,=\\,(X_{1},X_{2})$ , where $X_{1}\\,\\in\\,\\mathbb{R}^{n_{1}}$ and $X_{2}\\,\\in\\,\\mathbb{R}^{n_{2}}$ , consists of two independent components. If $X_{1}$ and $X_{2}$ satisfy the LAC condition with functions $\\lambda_{1}(\\cdot)$ and $\\lambda_{2}(\\cdot)_{\\cdot}$ , respectively, then $X$ satisfies the $L A C$ condition with $\\lambda(x)=\\operatorname*{max}(\\lambda_{1}(x),\\dot{\\lambda}_{2}(x))$ . ", "page_idx": 5}, {"type": "text", "text": "This holds because, when we take the logarithm of the density, the independent coordinates decompose as the sum of each density. Upon taking the gradient and evaluating the $\\ell_{\\infty}$ norm, the expression decomposes perfectly. Using this proposition, the LAC condition remains robust across dimensions if the coordinates are independent, making it dimension-free in such cases. Furthermore, this condition is very accessible because it can be readily computed for a given density function. For many well-known exponential families, the exponential component of the density often scales polynomially. ", "page_idx": 5}, {"type": "text", "text": "Examples of Distributions with LAC Condition ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We present a few examples of known distributions satisfying the LAC condition. We provide rigorous proofs for the examples in Appendix B. ", "page_idx": 5}, {"type": "text", "text": "\u2022 Gaussian distribution: For $X=(x_{1},\\dots x_{n})\\sim N(\\mu,\\Sigma)$ , when $\\Sigma$ is diagonal, it satisfies the LAC condition with $\\begin{array}{r}{\\mathcal{L}(x)=\\frac{4}{\\lambda_{\\mathrm{min}}(\\Sigma)}(\\|x\\|_{\\infty}+\\|\\mu\\|_{\\infty})}\\end{array}$ . For general (non-diagonal) $\\Sigma$ , please see Appendix B.   \n\u2022 Exponential distribution: Exponential distribution density $\\begin{array}{r}{f_{X}(x)=\\frac{1}{\\lambda}\\exp(-\\lambda x)}\\end{array}$ satisfies the LAC condition with a constant function $\\mathcal{L}=\\lambda$ .   \n\u2022 Uniform distribution: Uniform distribution has a constant density and satisfies the LAC condition with a constant function $\\mathcal{L}=1$ .   \n\u2022 Student\u2019s $t$ -distribution: The 1-dimensional Student\u2019s $t$ -distribution has a density with fX(x) = \u0393 \u03bd2+1 /\u221a\u03bd\u03c0\u0393 \u03bd2 \u00b7 1 + x\u03bd2 \u2212(\u03bd+1)/2 and has constant LAC function $\\mathcal{L}=\\mathcal{O}(1)$ .   \n\u2022 Laplace distribution: Laplace distribution has density $\\begin{array}{r}{f(x)=\\frac{1}{2b}\\exp\\left(-\\frac{|x-\\mu|}{b}\\right)}\\end{array}$ and it has constant LAC function $\\mathcal{L}=\\mathcal{O}(1)$ . ", "page_idx": 5}, {"type": "text", "text": "If each coordinate\u2019s density independently adheres to one of the aforementioned distributions, according to Proposition 1, they all share the same LAC function irrespective of the dimension. ", "page_idx": 5}, {"type": "text", "text": "Consider the density $f(x)$ with $f(x)\\,\\propto\\,\\exp(-V(x))$ for some differentiable function $V(x)$ . If $\\nabla V(x)$ has polynomial growth, i.e., is bounded by a polynomial, then the density $f(x)$ meets the LAC condition. This holds because $f(x)=C\\exp({-V(x)})$ , $\\nabla\\log f(x)=\\nabla\\log\\exp(-V(x))=$ $-\\nabla V(x)$ . If $\\nabla V(x)$ exhibits polynomial-scale growth, then the supremum norm confirms that the LAC condition is satisfied. ", "page_idx": 5}, {"type": "text", "text": "This observation makes the LAC condition easily verifiable for exponential family distributions with density forms $f_{X}(x|\\theta)=h(x)\\exp[\\eta(\\theta)\\cdot T(\\dot{x})-A(\\theta)]$ , where the exponential part $T(x)$ has polynomial growth. In many exponential family cases, $T(\\cdot)$ indeed exhibits polynomial growth. Proofs and further details can be found in Appendix B. ", "page_idx": 5}, {"type": "text", "text": "4 Statistical Challenges of Greedy Linear Contextual Bandits ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we outline the key statistical challenges in analyzing greedy algorithms for linear contextual bandits: (i) ensuring the diversity of the adapted Gram matrix (Section 4.1) and (ii) ", "page_idx": 5}, {"type": "text", "text": "bounding the suboptimality gap to achieve logarithmic regret (Section 4.2). For ease of exposition, we use the vectorized context expression $\\mathbf{X}(t)\\overset{\\smile}{=}(X_{1}^{\\top}(t),\\lnot\\cdot\\cdot\\cdot X_{K}^{\\top}(t))\\in\\mathbb{R}^{d K}$ of $\\mathcal{X}(t)$ , which combines context vectors $X_{i}(t)$ for $i\\in[K]$ . We define $X_{i j}(t)$ as the $j$ -th coordinate of the context $X_{i}(t)$ . ", "page_idx": 6}, {"type": "text", "text": "4.1 Diversity of Adapted Gram Matrix ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The first key challenge lies in ensuring sufficient $\\ell_{2}$ -concentration of the estimator. This requires sufficient eigenvalue growth of the Gram matrix, constructed from the policy-selected contexts. For the OLS estimator used in Algorithm 1, if the minimum eigenvalue of the adapted Gram matrix $\\lambda_{\\mathrm{min}}(\\Sigma(t))$ increases linearly with the number of rounds $t$ , we can obtain the high probability $\\ell_{2}$ error bound $\\lVert\\hat{{\\boldsymbol{\\theta}}}_{t}-{\\boldsymbol{\\theta}}^{\\star}\\rVert_{2}$ with a convergence rate of $\\mathcal{O}(1/\\sqrt{t})$ using martingale concentration [13, 29]. In fact, the growth of $\\lambda_{\\mathrm{min}}(\\Sigma(t))$ is a necessary condition to obtain $\\sqrt{t}$ -consistency of the estimator. ", "page_idx": 6}, {"type": "text", "text": "However, estimating the covariance of the selected contexts $X_{a(t)}(t)$ is relatively challenging, as its distribution differs significantly from the overall distribution (before selection) of $\\mathbf{X}(t)$ . Some studies have investigated the statistical properties of selected contexts [20, 28]; however, the known results are limited to specific distributions, such as arm-independent Gaussian and uniform distributions. Therefore, it remains an open question whether the growth of $\\lambda_{\\mathrm{min}}(\\Sigma(t))$ can be ensured for a broader class of distributions and what characteristics such distributions would need to satisfy. ", "page_idx": 6}, {"type": "text", "text": "4.2 Bounding Suboptimality Gap: Road to Logarithmic Regret ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The next challenge that we face particularly in order to achieve logarithmic regret is to bound the suboptimality gap. We first denote the suboptimality gap as the difference between the optimal expected reward and the second highest expected reward: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\Delta({\\mathbf{X}}(t)):=X_{a^{\\star}(t)}(t)^{\\top}{\\boldsymbol{\\theta}}^{\\star}-\\operatorname*{max}_{i\\not=a^{\\star}(t)}X_{i}(t)^{\\top}{\\boldsymbol{\\theta}}^{\\star},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "which is determined by the true parameter $\\theta^{\\star}$ . We aim to bound this suboptimality gap probabilistically, as described precisely in Challenge 2 of Section 4.3. ", "page_idx": 6}, {"type": "text", "text": "When this challenge is resolved, along with the growth of the minimum eigenvalue of the adapted Gram matrix discussed in Section 4.1, we can achieve logarithmic expected regret using analysis techniques for linear contextual bandit with stochastic contexts [7, 19]. A high-level description of the role of the margin constant in the regret bound is provided in Appendix C.1, with a rigorous analysis in Appendix I. ", "page_idx": 6}, {"type": "text", "text": "4.3 Formal Statements of Two Key Challenges ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "As mentioned above, we encounter two primary challenges: ensuring the diversity of the chosen contexts (i.e., the growth of the minimum eigenvalue of the Gram matrix of the selected contexts) and bounding the suboptimality gap. Importantly, we do not assume these conditions to hold a priori; rather, we will demonstrate that they are satisfied in the stochastic context under the LAC condition. In this section, we formally define these challenges to be addressed. ", "page_idx": 6}, {"type": "text", "text": "Before delving into the formal statements for each of the two challenges, we first define the concept of the diversity constant, which depends on the minimum eigenvalue of the adapted Gram matrix. ", "page_idx": 6}, {"type": "text", "text": "Definition 2 (Diversity Constant) For a linear contextual bandit with contexts $\\mathbf X(t)$ and history $\\mathcal{H}_{t-1}$ , the diversity constant $\\lambda_{\\star}(t)$ is defined as the value satisfying ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[X_{a(t)}(t)X_{a(t)}(t)^{\\top}\\mid\\mathcal{H}_{t-1}]\\succeq\\lambda_{\\star}(t)I_{d},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "for all $t>0$ , where $a(t)$ denotes the arm selected by the algorithm in round $t$ . ", "page_idx": 6}, {"type": "text", "text": "Then, the first challenge is to ensure a positive diversity constant $\\lambda_{\\star}(t)>0$ , which involves sufficient eigenvalue growth of the adapted Gram matrix. We explore this further in Appendix E.1. ", "page_idx": 6}, {"type": "text", "text": "Challenge 1 (Positive Diversity Constant) Our goal is to ensure $\\lambda_{\\star}(t)>0$ . ", "page_idx": 6}, {"type": "text", "text": "Achieving a positive diversity constant is challenging, as it requires analyzing the behavior of a context selected by the greedy policy in a specific direction rather than relying on the overall context distribution. In Section 5.3.1, we demonstrate that the minimum eigenvalue of the Gram matrix grows sufficiently, thereby ensuring a positive diversity constant. ", "page_idx": 7}, {"type": "text", "text": "We now formally state our second challenge of bounding the suboptimality gap. ", "page_idx": 7}, {"type": "text", "text": "Challenge 2 (Probabilistic Suboptimality Gap) We aim to bound the constant $C_{\\Delta}(t)$ , which holds under the given history $\\mathcal{H}_{t-1}$ and for any $\\varepsilon>0$ , ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\Delta\\!\\left(\\mathbf{X}(t)\\right)\\leq\\varepsilon\\right]\\leq\\varepsilon C_{\\Delta}(t)+{\\frac{1}{\\sqrt{T}}}\\,.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "We also refer to this constant $C_{\\Delta}(t)$ as the margin constant. ", "page_idx": 7}, {"type": "text", "text": "Note that Eq. (2) is a relaxed version of the margin condition presented in [5, 7, 8, 19]. The aforementioned literature explicitly assumes this condition to hold. However, we instead show that the suboptimality gap can be bounded without directly assuming it (Section 5.3.2). Rigorously, $C_{\\Delta}(t)$ depends on $T$ , as it is a function of $T$ . However, we emphasize that our algorithm does not require prior knowledge of $T$ ; this dependency is needed only for the analysis. ", "page_idx": 7}, {"type": "text", "text": "5 Regret Analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We present the main results of our paper. We prove that the regret of the greedy algorithm (Algorithm 1) for linear contextual bandits can be bounded at a logarithmic scale in the time horizon $T$ , provided that the context distribution satisfies the LAC condition with a polynomial function $\\lambda$ . ", "page_idx": 7}, {"type": "text", "text": "Assumption 1 (Independently distributed contexts) The context sets $\\mathcal{X}(1),\\ldots,\\mathcal{X}(T)$ are independently distributed across time. ", "page_idx": 7}, {"type": "text", "text": "Discussion of Assumption 1. To the best of our knowledge, all analyses of greedy linear contextual bandits assume the independence and identical distribution (i.i.d.) of context sets [8, 20, 28, 33]. In Assumption 1, we only require context sets to be independent; they may be non-identically distributed. Additionally, much of the literature on linear contextual bandits that investigates $\\sqrt{t}$ -consistency of estimators also assumes independence of context sets [21, 22]. Note that under Assumption 1, context vectors within the same round are permitted to be dependent. ", "page_idx": 7}, {"type": "text", "text": "5.1 Considerations for Context Boundedness ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We first provide detailed considerations on context boundedness. In the linear contextual bandit setting, $\\ell_{2}$ boundedness is commonly assumed. However, for light-tailed distributions (such as Gaussian or exponential), the $\\ell_{2}$ norm is unbounded. In such cases, a general approach in the statistical literature is to assume bounded $\\psi_{1}$ or $\\psi_{2}$ norms [14, 17, 37, 38]. Therefore, we divide our analysis into cases of bounded and unbounded contexts. ", "page_idx": 7}, {"type": "text", "text": "Bounded Contexts vs. Unbounded Contexts. In linear contextual bandit studies, boundedness of the $\\ell_{2}$ norm of contexts is commonly assumed. In this paper, for bounded contexts, we consider both truncated contexts (e.g., truncated Gaussian, truncated Cauchy distributions) and naturally bounded contexts (e.g., uniform distribution). For unbounded contexts, we assume a bounded $\\psi_{1}$ norm, which is a standard assumption for handling light-tailed distributions (e.g., as in [14, 17], which assume bounded $\\psi_{2}$ norms). ", "page_idx": 7}, {"type": "text", "text": "Assumption 2 (Boundedness) For unbounded contexts, we assume $\\|X_{i}(t)\\|_{\\psi_{1}}~\\le~x_{\\mathrm{max}}$ . For bounded contexts, we assume $\\|X_{i}(t)\\|_{2}\\leq x_{\\operatorname*{max}}$ for all $i\\in[K],t\\in[T]$ . ", "page_idx": 7}, {"type": "text", "text": "Discussion of Assumption 2 The bounded context assumption is widely used in the literature [1, 7, 8, 21, 22, 28]. For unbounded contexts, our assumption of $\\psi_{1}$ boundedness is notably weaker than the sub-Gaussianity (or $\\psi_{2}$ ) assumption commonly used in statistical regression literature to handle random design covariates [14, 39]. If $\\ell_{2}$ boundedness holds, it automatically implies boundedness of the $\\psi_{1}$ norm. However, as the analysis differs slightly depending on whether the support is restricted to a bounded ball or is unbounded, we address these cases separately. ", "page_idx": 7}, {"type": "text", "text": "5.2 Regret Bound of LinGreedy for LAC Distribution ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We first introduce our main result, the regret bound of the greedy algorithm under the LAC condition. ", "page_idx": 8}, {"type": "text", "text": "Theorem 1 (Regret bound of LinGreedy) Suppose $\\mathbf X(t)$ satisfy the LAC condition with polynomial function $\\mathcal{L}$ and satisfy Assumptions $^{\\,l}$ and 2 for all $t_{\\perp}$ , then the cumulative expected regret of LinGreedy (Algorithm $^{\\,I}$ ) is bounded by ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbf{Reg}(T)\\leq\\mathcal{O}(\\mathrm{poly\\,log}\\,T).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Explicitly, for unbounded contexts, we have ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbf{Reg}(T)\\leq{\\tilde{\\mathcal{O}}}(d^{2.5}).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "For bounded contexts, refer to Appendix $G$ for explicit results, as we consider several cases. ", "page_idx": 8}, {"type": "text", "text": "Discussion of Theorem 1. Theorem 1 states that if the contexts are drawn from a distribution in the LAC class, the regret scales as $\\mathcal{O}(\\mathrm{poly}\\log T)$ . While our primary objective is not solely to achieve the sharpest regret bounds, attaining poly-logarithmic regret is highly favorable. Our main goal is to demonstrate that a large class of context distributions satisfies the LAC condition. When they do, a simple greedy algorithm can suffice or even outperform exploration-based algorithms (see numerical experiments in Section 6). The worst-case dependence on $d$ and $K$ for bounded contexts is detailed in Appendix G, where dependencies remain at most polynomial in $d$ and $K$ . As these dependencies vary across distributions, refer to Appendix G for precise information. Proofs for unbounded contexts are provided in Appendix F, with a proof sketch in Appendix C. Proofs for bounded contexts are included in Appendix $_\\mathrm{H}$ . ", "page_idx": 8}, {"type": "text", "text": "Theorem 1 is the first result to expand the class of admissible distributions for greedy bandit algorithms beyond Gaussian and uniform distributions. Our result demonstrates, for the first time, that distributions in the LAC class inherently exhibit margin behavior, achieving sharp poly-logarithmic regret without requiring an additional margin assumption. This finding is of independent interest beyond the analysis of greedy bandit algorithms. ", "page_idx": 8}, {"type": "text", "text": "5.3 Proof Sketch of Theorem 1 ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We first present our key results for addressing each of Challenges 1 and 2 stated in Section 4.3. ", "page_idx": 8}, {"type": "text", "text": "5.3.1 Ensuring the Positive Diversity Constant ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we present our key result for estimating lower bounds on the diversity constant for densities that satisfy the LAC condition, therefore addressing Challenge 1. Theorem 2 is the analysis under the case of unbounded contexts, where contexts have full support. A similar result is presented in Appendix G for contexts with bounded or truncated support. ", "page_idx": 8}, {"type": "text", "text": "Theorem 2 (Diversity constant for unbounded contexts) If unbounded contexts $\\mathbf{X}(t)$ has the LAC condition with $\\mathcal{L}(x):=A_{1}+A_{2}x^{\\alpha}$ and satisfies Assumption 2 for all $t_{\\perp}$ , ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\lambda_{\\star}(t)\\geq c_{1}\\frac{1}{d}\\cdot\\frac{1}{(A_{1}+A_{2}(R_{1}+2)^{\\alpha})^{2}}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "holds for $R_{1}:=c_{2}x_{\\mathrm{max}}(\\log d+\\log K+2)$ . Here, $c_{1},c_{2}$ are absolute constants. ", "page_idx": 8}, {"type": "text", "text": "Discussion of Theorem 2. Theorem 2 implies that $\\begin{array}{r}{\\lambda_{\\star}(t)\\geq\\Omega(\\frac{1}{d})}\\end{array}$ , hence ensuring the growth of the minimum eigenvalue of the adapted Gram matrix. In Appendix C.1, we discuss how $\\frac{1}{\\lambda_{\\star}(t)}$ factors into the regret bounds. Note that $\\alpha$ is generally small in many distributions. For example, for Gaussian distributions, $\\alpha=1$ , and for exponential distributions, $\\alpha=0$ . ", "page_idx": 8}, {"type": "text", "text": "5.3.2 Bounding Suboptimality Gap ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Next, we present our result addressing Challenge 2 by computing the suboptimality gap constant $C_{\\Delta}(t)$ , which satisfies the inequality in Eq.(2) for every $\\varepsilon>0$ . By combining this condition with the estimates for $\\lambda_{\\star}(t)$ , we can obtain an $\\mathcal{O}(\\mathrm{poly}(\\log T))$ regret bound in terms of $T$ by applying the analysis techniques of linear contextual bandits with stochastic contexts [7, 8, 19] to our setting. ", "page_idx": 8}, {"type": "text", "text": "Theorem 3 (Suboptimality gap for unbounded contexts) In the same setup as Theorem 2, ", "text_level": 1, "page_idx": 9}, {"type": "equation", "text": "$$\nC_{\\Delta}(t)\\leq c_{3}\\sqrt{d}(A_{1}+A_{2}3^{\\alpha}R_{2}^{\\alpha})\\frac{1}{\\|\\theta^{\\star}\\|_{2}}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "holds for $R_{2}=c_{4}x_{\\operatorname*{max}}(1+\\log K+\\log d+{\\textstyle{\\frac{1}{2}}}\\log T)+1$ and absolute constants $c_{3},c_{4}>0$ . ", "page_idx": 9}, {"type": "text", "text": "Discussion of Theorem 3. Note that the suboptimality gap constant $C_{\\Delta}(t)$ is multiplied linearly in regret bounds [5, 7, 19]. For bounded contexts, we present a similar result in the Appendix $\\mathrm{G}$ . ", "page_idx": 9}, {"type": "text", "text": "5.3.3 Proof Intuitions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We provide a high-level proof overview of Theorem 1 in Appendix C. Note that once Challenges 1 and 2 are satisfied, achieving logarithmic regret becomes straightforward, as detailed in Appendix I. ", "page_idx": 9}, {"type": "text", "text": "The remaining task is to address these two challenges, with a particular focus on bounding the constants $\\lambda_{\\star}(t)$ and $C_{\\Delta}(t)$ . A key implication of the LAC condition is that the density decays slowly at every point. Another useful property is that the LAC condition is preserved under conditioning, meaning that $\\mathbf{X}(t)\\mid\\{\\mathbf{X}(t)\\in{\\dot{A}}\\}$ also satisfies the LAC condition with the same function for any set $A\\subset\\mathbb{R}^{K\\times d}$ . This can be verified using the fact that l $\\operatorname{\\mathrm{}}(\\operatorname{})\\operatorname{\\mathrm{}}f_{\\mathbf{X}(t)|\\{\\mathbf{X}(t)\\in A\\}}(x)=\\log f_{\\mathbf{X}(t)}(x)-\\log\\operatorname{\\mathbb{P}}[A]$ , where $\\mathbb{P}[A]$ is a constant (see Appendix B.3 for details). ", "page_idx": 9}, {"type": "text", "text": "The main challenge of analyzing the statistical concentration in greedy linear contextual bandits lies in the fact that the distribution of selected contexts $X_{a(t)}(t)$ differs significantly from the distribution of the overall (pre-selected) contexts $\\mathbf{X}(t)$ . The preservation of LAC under conditioning ensures that LAC still holds when conditioning on the event of selecting arm $i$ , enabling our analysis. ", "page_idx": 9}, {"type": "text", "text": "The full proofs for unbounded contexts are provided in Appendix F. For results on bounded contexts, see Appendix $\\mathrm{G}$ (and their proofs in Appendix H). ", "page_idx": 9}, {"type": "text", "text": "5.4 $\\sqrt{t}$ -Consistency of Estimator ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In addition to achieving logarithmic regret, an independently valuable result is obtained: the $\\ell_{2}$ - consistency of the estimator $\\widehat{\\theta}_{t}$ . This is a property that even typical sublinear-regret algorithms, such as UCB and TS, do not generally guarantee. Under the same setup as Theorem 1, we achieve $\\begin{array}{r}{\\|\\hat{\\theta}_{t}-\\theta^{\\star}\\|_{2}\\le\\widetilde{\\mathcal{O}}\\left(\\frac{d}{\\sqrt{t}}\\right)}\\end{array}$ with high probability (see Corollaries 6 and 7). This additional result may also facilitate analysis of sample complexity, such as PAC bounds. ", "page_idx": 9}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "To validate our theoretical findings numerically, we conducted experiments using various context distributions: Gaussian, Laplace, uniform, and truncated Cauchy distributions. We compared the performance of LinGreedy with the LinUCB and LinTS algorithms. The results showed that LinGreedy exhibited significantly superior regret performance compared to the other explorationbased algorithms, achieving a logarithmic scale of regret. Detailed experimental results are provided in Appendix L. ", "page_idx": 9}, {"type": "image", "img_path": "rblaF2euXQ/tmp/2f1ea3cf09cfead195276acb5d976038a512e24294543fc647a7f76ec942b320.jpg", "img_caption": ["Figure 1: The cumulative regret plots of the numerical experiments. The full results are available in Appendix L. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported by the National Research Foundation of Korea(NRF) grant funded by the Korea government(MSIT) (No. 2022R1C1C1006859, 2022R1A4A1030579, and RS-2023-00222663) and by AI-Bio Research Grant through Seoul National University. The author thanks H.Choi for for helpful comments. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Abbasi-Yadkori, Y., P\u00e1l, D., and Szepesv\u00e1ri, C. (2011). Improved algorithms for linear stochastic bandits. Advances in neural information processing systems, 24.   \n[2] Abe, N. and Long, P. M. (1999). Associative reinforcement learning using linear probabilistic concepts. In International Conference on Machine Learning, pages 3\u201311.   \n[3] Abeille, M. and Lazaric, A. (2017). Linear thompson sampling revisited. In Artificial Intelligence and Statistics, pages 176\u2013184. PMLR.   \n[4] Agrawal, S. and Goyal, N. (2013). Thompson sampling for contextual bandits with linear payoffs. In International conference on machine learning, pages 127\u2013135. PMLR.   \n[5] Ariu, K., Abe, K., and Prouti\u00e8re, A. (2022). Thresholded lasso bandit. In International Conference on Machine Learning, pages 878\u2013928. PMLR.   \n[6] Auer, P. (2002). Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine Learning Research, 3(Nov):397\u2013422.   \n[7] Bastani, H. and Bayati, M. (2020). Online decision making with high-dimensional covariates. Operations Research, 68(1):276\u2013294.   \n[8] Bastani, H., Bayati, M., and Khosravi, K. (2021). Mostly exploration-free algorithms for contextual bandits. Management Science, 67(3):1329\u20131349.   \n[9] Chapelle, O. and Li, L. (2011). An empirical evaluation of thompson sampling. In Advances in neural information processing systems, pages 2249\u20132257.   \n[10] Chu, W., Li, L., Reyzin, L., and Schapire, R. E. (2011). Contextual bandits with linear payoff functions. Journal of Machine Learning Research.   \n[11] Dani, V., Hayes, T. P., and Kakade, S. M. (2008). Stochastic linear optimization under bandit feedback. In Proceedings of the 21st Annual Conference on Learning Theory, page 355\u2013366.   \n[12] De Haan, L., Ferreira, A., and Ferreira, A. (2006). Extreme value theory: an introduction, volume 21. Springer.   \n[13] de la Pena, V. H., Klass, M. J., and Leung Lai, T. (2004). Self-normalized processes: exponential inequalities, moment bounds and iterated logarithm laws.   \n[14] DUAN, Y. and WANG, K. (2023). Adaptive and robust multi-task learning. The Annals of Statistics, 51(5):2015\u20132039.   \n[15] Durrett, R. (2019). Probability: theory and examples, volume 49. Cambridge university press.   \n[16] Evans, L. C. (2022). Partial differential equations, volume 19. American Mathematical Society.   \n[17] Fan, J., Wang, D., Wang, K., and Zhu, Z. (2019). Distributed estimation of principal eigenspaces. Annals of statistics, 47(6):3009.   \n[18] Filippi, S., Cappe, O., Garivier, A., and Szepesv\u00e1ri, C. (2010). Parametric bandits: The generalized linear case. In Advances in Neural Information Processing Systems, pages 586\u2013594.   \n[19] Goldenshluger, A. and Zeevi, A. (2013). A linear response bandit problem. Stochastic Systems, 3(1):230\u2013261.   \n[20] Kannan, S., Morgenstern, J. H., Roth, A., Waggoner, B., and Wu, Z. S. (2018). A smoothed analysis of the greedy algorithm for the linear contextual bandit problem. Advances in neural information processing systems, 31.   \n[21] Kim, G.-S. and Paik, M. C. (2019). Doubly-robust lasso bandit. In Advances in Neural Information Processing Systems, pages 5869\u20135879.   \n[22] Kim, W., Kim, G.-s., and Paik, M. C. (2021). Doubly robust thompson sampling for linear payoffs. In Advances in neural information processing systems.   \n[23] Kveton, B., Zaheer, M., Szepesvari, C., Li, L., Ghavamzadeh, M., and Boutilier, C. (2020). Randomized exploration in generalized linear bandits. In International Conference on Artificial Intelligence and Statistics, pages 2066\u20132076.   \n[24] Langford, J. and Zhang, T. (2007). The epoch-greedy algorithm for contextual multi-armed bandits. Advances in neural information processing systems, 20(1):96\u20131.   \n[25] Lattimore, T. and Szepesv\u00e1ri, C. (2019). Bandit Algorithms. Cambridge University Press (preprint).   \n[26] Li, L., Chu, W., Langford, J., and Schapire, R. E. (2010). A contextual-bandit approach to personalized news article recommendation. In Proceedings of the 19th international conference on World wide web, pages 661\u2013670. ACM.   \n[27] Li, L., Lu, Y., and Zhou, D. (2017). Provably optimal algorithms for generalized linear contextual bandits. In International Conference on Machine Learning, pages 2071\u20132080.   \n[28] Oh, M.-H., Iyengar, G., and Zeevi, A. (2021). Sparsity-agnostic lasso bandit. In Meila, M. and Zhang, T., editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 8271\u20138280. PMLR.   \n[29] Pe\u00f1a, V. H., Lai, T. L., and Shao, Q.-M. (2009). Self-normalized processes: Limit theory and Statistical Applications. Springer.   \n[30] Raghavan, M., Slivkins, A., Vaughan, J. W., and Wu, Z. S. (2023). Greedy algorithm almost dominates in smoothed contextual bandits. SIAM Journal on Computing, 52(2):487\u2013524.   \n[31] Raghavan, M., Slivkins, A., Wortman, J. V., and Wu, Z. S. (2018). The externalities of exploration and how data diversity helps exploitation. In Conference on Learning Theory, pages 1724\u20131738. PMLR.   \n[32] Rusmevichientong, P. and Tsitsiklis, J. N. (2010). Linearly parameterized bandits. Mathematics of Operations Research, 35(2):395\u2013411.   \n[33] Sivakumar, V., Wu, S., and Banerjee, A. (2020). Structured linear contextual bandits: A sharp and geometric smoothed analysis. In International Conference on Machine Learning, pages 9026\u20139035. PMLR.   \n[34] Sivakumar, V., Zuo, S., and Banerjee, A. (2022). Smoothed adversarial linear contextual bandits with knapsacks. In International Conference on Machine Learning, pages 20253\u201320277. PMLR.   \n[35] Thompson, W. R. (1933). On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3/4):285\u2013294.   \n[36] Tropp, J. A. (2011). User-friendly tail bounds for matrix martingales. Technical report, CALIFORNIA INST OF TECH PASADENA.   \n[37] Vershynin, R. (2018). High-dimensional probability: An introduction with applications in data science, volume 47. Cambridge university press.   \n[38] Wainwright, M. J. (2019). High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cambridge University Press.   \n[39] Wang, K. (2023). Pseudo-labeling for kernel ridge regression under covariate shift. arXiv preprint arXiv:2302.10160. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "1 Introduction 1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "1.1 Contributions 2   \n1.2 Related Work 3 ", "page_idx": 12}, {"type": "text", "text": "2 Preliminaries 4 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "2.1 Notations 4   \n2.2 Linear Contextual Bandits with Stochastic Contexts 4   \n2.3 LinGreedy: Exploration-Free Algorithm for Linear Contextual Bandits 5 ", "page_idx": 12}, {"type": "text", "text": "3 Local Anti-Concentration Class 5 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "3.1 Intuition of LAC Condition 5   \n3.2 Generality of LAC Condition 6 ", "page_idx": 12}, {"type": "text", "text": "4 Statistical Challenges of Greedy Linear Contextual Bandits 6 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "4.1 Diversity of Adapted Gram Matrix . . 7   \n4.2 Bounding Suboptimality Gap: Road to Logarithmic Regret . . 7   \n4.3 Formal Statements of Two Key Challenges . . . 7 ", "page_idx": 12}, {"type": "text", "text": "5 Regret Analysis 8 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "5.1 Considerations for Context Boundedness . . 8   \n5.2 Regret Bound of LinGreedy for LAC Distribution . . 9   \n5.3 Proof Sketch of Theorem 1 . . 9   \n5.3.1 Ensuring the Positive Diversity Constant . . . 9   \n5.3.2 Bounding Suboptimality Gap . . . 9   \n5.3.3 Proof Intuitions . . . 10   \n5.4 $\\sqrt{t}$ -Consistency of Estimator . . 10 ", "page_idx": 12}, {"type": "text", "text": "6 Experiments 10 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A Additional Notations 16 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "B More Details of LAC: Distributions, Conditioning, & Truncation 16 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "B.1 LAC of Various Distributions . 16   \nB.2 Proof of Proposition 1 . . . 17   \nB.3 LAC and Conditioning . . . 17   \nB.4 LAC and Truncation 17   \nB.5 LAC and Decay Rate of Density 18 ", "page_idx": 12}, {"type": "text", "text": "C High-level Proof of Theorem 1 (Unbounded Contexts) 19 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "C.1 Regret Analysis: Overcoming Two Challenges . . . 19   \nC.2 Tackling Two Challenges and Fixed History Arguments . . . . 21   \nC.3 Starting with Truncation . . 21   \nC.4 Event Decompositions . . . 21   \nC.5 Conditional Contexts are still in LAC 22   \nC.6 Remaining Goal: Bounding Decay Rate of Projected Contexts 22 ", "page_idx": 13}, {"type": "text", "text": "D Sections, Section Densities and Decay Rate 23 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "D.1 Motivation . 23   \nD.2 Sections and Section Densities 24   \nD.3 Decay Rate of Section Density . 25 ", "page_idx": 13}, {"type": "text", "text": "E Fixed History Results of Two Challenges for Unbounded Contexts 26 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "E.1 Details for Diversity Constant (Challenge 1) . . . . 26   \nE.2 The Setup of Fixed History Analysis . . . 26   \nE.3 Fixed History Results: Diversity Constant . . . 27   \nE.4 Fixed History Results: Suboptimality Gap . . . 30 ", "page_idx": 13}, {"type": "text", "text": "F Proofs of Results for Unbounded Contexts 32 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "F.1 Constructing Truncation Sets 32   \nF.2 Proof of Theorem 2 . 34   \nF.2.1 Constructing Truncation Sets . . 34   \nF.2.2 Properties of Truncated Contexts W 36   \nF.2.3 Applying Proposition 2 . . . 36   \nF.3 Proof of Theorem 3 . . 36   \nF.3.1 Constructing Truncation Sets . . 36   \nF.3.2 Truncation with High-probability Region 37   \nF.3.3 Applying Proposition 3 . . . 38   \nF.4 Proof of Theorem 1: Unbounded Contexts Case 38 ", "page_idx": 13}, {"type": "text", "text": "G Results for Bounded Contexts 38 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "G.1 Two Cases of Bounded Contexts 38   \nG.2 Regret Bounds for Bounded Contexts 38   \nG.3 Concentration Parameters for Bounded Contexts . . 39   \nG.4 Results for Two Challenges: Bounded Contexts . . 41 ", "page_idx": 13}, {"type": "text", "text": "H Proofs of Reuslts for Bounded Contexts 42 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "H.1 Fixed History Arguments . . 42   \nH.2 Sections of the Ball 42   \nH.3 Linear Section Maps 43   \nH.4 One-side Decay Rate of Linear Section Maps 44 ", "page_idx": 13}, {"type": "text", "text": "H.5 Linear Section Maps between Sliced Balls, $\\mathbb{S}_{R}(v,y)$ 45 ", "page_idx": 14}, {"type": "text", "text": "H.6 Linear Section Maps for Double Slicde balls, $\\mathbb{S}_{R}(\\theta,b,v,y)$ . . 46 ", "page_idx": 14}, {"type": "text", "text": "H.6.1 Case $v\\perp\\theta$ . . 46   \nH.6.2 Case $\\textstyle{\\frac{\\pi}{2}}-\\tau_{0}\\leq\\angle(v,\\theta)\\leq{\\frac{\\pi}{2}}$ 47   \nH.6.3 Case $\\begin{array}{r}{0\\le\\angle(v,\\theta)\\le\\frac{\\pi}{2}-\\tau_{0}}\\end{array}$ 47   \nH.7 Proof of Proposition 4 . . . . 48   \nH.8 Proof of Proposition 5 . . . 49   \nH.9 Proof of Proposition 6 . . 49   \nH.10 Proof of Proposition 7 . 50   \nH.11 Proof of Corollary 1 . . 50   \nH.12 Proof of Corollary 2 . . . 50   \nH.13 Proof of Corollary 3 . . 51 ", "page_idx": 14}, {"type": "text", "text": "I Analysis After Challenges 1 and 2 are Satisfied 51 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "I.1 Proof of Proposition 8 . . . . 51   \nI.2 Concentration of Sub-exponential Contexts 53   \nI.3 Proof of Proposition 9 . . . 53   \nI.3.1 Gram Matrix Concentration 53   \nI.3.2 Good Events . 54   \nI.3.3 Bounding Regret by the Peeling Technique 55 ", "page_idx": 14}, {"type": "text", "text": "J Discussion on Discrete-Supported Contexts 56 ", "page_idx": 14}, {"type": "text", "text": "K Dicussions, Limitations and Further Ideas 57 ", "page_idx": 14}, {"type": "text", "text": "L Numerical Experiments 57 ", "page_idx": 14}, {"type": "text", "text": "M Technical Lemmas 59 ", "page_idx": 14}, {"type": "text", "text": "M.1 Concentration Inequalities 59 ", "page_idx": 14}, {"type": "text", "text": "A Additional Notations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We define additional notation. For $\\boldsymbol{v}\\in\\mathbb{R}^{d}$ , define $P_{v}:=\\{x\\mid x^{\\top}v=0\\}$ . For $S\\subset\\mathbb{R}^{n}$ , we define $\\|S\\|_{\\infty}:=\\operatorname*{sup}\\{\\|x\\|_{\\infty},\\;x\\in S\\}.$ . For the intervals $I\\subset\\mathbb{R}$ and $\\boldsymbol{v}\\in\\mathbb{R}^{d}$ , define $I_{v}:=\\{x\\mid x^{\\top}v\\in I\\}$ . For instance, $[-1,1]_{\\boldsymbol{v}}:=\\{\\boldsymbol{x}\\ |-1\\leq\\boldsymbol{x}^{\\top}\\boldsymbol{v}\\leq1\\}$ . We define $\\pi_{V}(\\cdot)$ as the projection to the subspace $V$ . Also, in the appendix, we write $\\mathbb{B}_{R}=\\mathbb{B}_{R}^{d}=\\{x\\in\\mathbb{R}^{d}\\mid\\|x\\|_{2}\\leq R\\}$ with slight abuse of notations. For random variable $X$ , we define $\\operatorname{supp}({\\bar{X}})$ as the support of $X$ . Recall that we define the expected regret in round $t$ as $\\mathrm{reg}(t)$ and we define unexpected regret as $\\mathrm{reg}^{\\prime}(t)$ . Also for $\\textstyle\\Omega=\\bigcup_{i\\in I}{\\bar{E}}_{i}$ for disjoint events $E_{i}$ , we use following notations ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\mathbb{E}[X\\mid E_{i}]\\right]:=\\sum_{i\\in I}\\mathbb{P}[E_{i}]\\mathbb{E}[X\\mid E_{i}].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For event $A$ , we define $\\mathbb{P}[X;A],\\mathbb{E}[X;A]$ as $\\mathbb{P}[X\\cap A]$ and $\\mathbb{E}[X\\cap A]$ by following Durrett [15]\u2019s notation. ", "page_idx": 15}, {"type": "text", "text": "B More Details of LAC: Distributions, Conditioning, & Truncation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section we provide the omitted details of LAC and its properties. ", "page_idx": 15}, {"type": "text", "text": "B.1 LAC of Various Distributions ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We first provide more details on the LAC of various distributions, presented in Section 3.2. ", "page_idx": 15}, {"type": "text", "text": "Gaussian. For density $f(x)\\;=\\;C\\exp(-(x\\,-\\,\\mu)^{\\top}V(x\\,-\\,\\mu)),x\\;\\in\\;\\mathbb{R}^{n}$ , where $V\\ =\\ {\\textstyle{\\frac{1}{2}}}\\Sigma^{-1}$ , $\\nabla\\log f=2V^{\\top}(x-\\mu)$ . First, when $V$ is diagonal, after taking log and gradient, we get ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla\\log f(x)\\|_{\\infty}\\leq2\\lambda_{\\operatorname*{max}}(V)\\|x-\\mu\\|_{\\infty}\\leq2\\lambda_{\\operatorname*{max}}(V)(\\|x\\|_{\\infty}+\\|\\mu\\|_{\\infty}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since $V={\\textstyle\\frac{1}{2}}\\Sigma^{-1}$ , we get the wanted results. For general $V$ , we can see that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|\\nabla\\log f(x)\\|_{\\infty}\\leq2(\\operatorname*{max}_{i}\\|V^{i}\\|_{1})\\|x-\\mu\\|_{\\infty}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $V^{i}$ is $i$ -th row of $V$ . ", "page_idx": 15}, {"type": "text", "text": "LAC of bounded support contexts. If the support of the contexts is bounded in the compact set, $\\nabla\\log f$ is bounded when it is continuous. Therefore, in this case, LAC can be a constant function. Also, if $\\|x\\|_{2}\\leq R$ , we have $\\|\\nabla\\log f(x)\\|_{\\infty}\\le\\mathcal{L}(R)$ by monotonicity of $\\mathcal{L}(\\cdot)$ . ", "page_idx": 15}, {"type": "text", "text": "LAC and context correlations. We claim that LAC $\\mathcal{L}(\\cdot)$ function of random vector $x=$ $(x_{1},\\ldots x_{n})$ related to the correlation structure of $x_{i},x_{j}$ , rather then dimension $n$ itself. Consider $f(x)\\propto\\exp(-V(x))$ . Then, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\nabla\\log f(x)=-\\nabla V(x)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\partial_{i}\\log f(x)=-\\partial_{i}V(x)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "holds. Since $\\mathcal{L}$ is defined in the sense of supremum norm, to investigate LAC function, the maximum value: $\\mathrm{max}_{i}\\parallel\\!\\partial_{i}V(x)\\parallel_{\\infty}$ is important. We claim that it is a dimension-free property, and the correlation structure is more important rather than dimension $n$ itself. For instance, if $\\bar{x^{\\prime}}\\!=(\\bar{x}_{1},\\ldots\\!\\cdot\\!x_{n})$ is coordinate-wise independent, we have $V(x)=V_{1}(x_{1})+V_{2}(x_{2})+\\cdots+V_{d}(x_{n})$ . Hence ", "page_idx": 15}, {"type": "equation", "text": "$\\|\\nabla V(x)\\|_{\\infty}=\\operatorname*{max}_{i}\\|V_{i}^{\\prime}(x_{i})\\|_{\\infty}$ ", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and we can see it is dimension-free value. ", "page_idx": 15}, {"type": "text", "text": "LAC with shifted mean distribution. Let the mean zero contexts $X$ \u2019s density as $f(x)$ with LAC fuction $\\mathcal{L}(\\cdot)$ . Then, the shifted contexts with mean $\\mu$ has density $g(x)=f(x+\\mu)$ and see that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|\\nabla\\log g(x)\\|_{\\infty}=\\|\\nabla\\log f(x+\\mu)\\|_{\\infty}}&{}\\\\ {\\le\\mathcal{L}(\\|x+\\mu\\|_{\\infty})}&{}\\\\ {\\le\\mathcal{L}(\\|x\\|_{\\infty}+\\|\\mu\\|_{\\infty}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Hence the density $g(x)$ has LAC with function ${\\mathcal{L}}^{\\prime}(x)={\\mathcal{L}}(x+\\|\\mu|_{\\infty})$ and when $\\|\\mu\\|_{\\infty}=O(1)$ , it has the same rate. Thus, LAC does not require that contexts be mean zero, and LAC can be defined for distributions with a general mean. ", "page_idx": 15}, {"type": "text", "text": "B.2 Proof of Proposition 1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "If $\\mathbf{X}\\,=\\,(X_{1}^{\\top},X_{2}^{\\top})$ and $X_{1},X_{2}$ are independent, the $f_{\\mathbf{X}}$ , density of $\\mathbf{X}$ can be decomposed as $f_{\\mathbf{X}}((x_{1},x_{2}))=f_{X_{1}}(x_{1})f_{X_{2}}(x_{2})$ .   \nThen, $\\begin{array}{r}{\\nabla\\log^{\\prime}\\!f_{\\mathbf{X}}((\\lambda_{1},x_{2})\\!)=\\\"\\nabla(\\log f_{X_{1}}(x_{1})\\!+\\!\\log f_{X_{2}}(x_{2}))=(\\nabla_{x_{1}}\\log f_{X_{1}}(x_{1}),\\nabla_{x_{2}}\\log f_{X_{2}}(x_{2}))}\\end{array}$ holds, and when taking the supremum norm, we get ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla\\log f_{\\mathbf{X}}((x_{1},x_{2}))\\|_{\\infty}=\\operatorname*{max}\\big(\\|\\nabla_{x_{1}}\\log f_{X_{1}}(x_{1})\\|_{\\infty},\\|\\nabla_{x_{2}}\\log f_{X_{2}}(x_{2})\\|_{\\infty}\\big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\operatorname*{max}\\big(\\mathcal{L}_{1}(\\|x_{1}\\|_{\\infty}),\\mathcal{L}_{2}(\\|x_{2}\\|_{\\infty})\\big)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\operatorname*{max}(\\mathcal{L}_{1}(\\|(x_{1},x_{2})\\|_{\\infty}),\\mathcal{L}_{2}(\\|(x_{1},x_{2})\\|_{\\infty})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By the definition of the LAC function, we finally obtain the desired result: $f$ has LAC with the function $\\mathcal{L}(x)=\\operatorname*{max}(\\mathcal{L}_{1}(x),\\mathcal{L}_{2}(x))$ for $x\\in\\mathbb R$ . ", "page_idx": 16}, {"type": "text", "text": "B.3 LAC and Conditioning ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "First, we observe that the LAC condition is maintained under conditioning. Let the density of $\\mathbf{X}=(X_{1}^{\\top},\\cdot\\cdot\\cdot X_{K}^{\\top})\\in\\mathbb{R}^{d K}$ be $\\mathbf f(\\cdot)$ . For event $E:=\\{\\mathbf{X}\\in A\\}$ for $A\\subset\\mathbb{R}^{d K}$ , the conditional density of $\\mathbf{X}$ given event $E$ is formulated as $\\begin{array}{r}{\\mathbf{f}_{\\vert E}(\\mathbf{x})=\\frac{\\mathbf{f}(\\mathbf{x})}{\\mathbb{P}[E]}\\in\\mathbb{R}^{d}}\\end{array}$ for $\\mathbf{x}\\in\\mathbb{R}^{d K}$ and the following holds: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\nabla\\log{\\mathbf{f}_{|E}(\\mathbf{x})}=\\nabla\\log{\\mathbf{f}(\\mathbf{x})}-\\nabla\\mathbb{P}[E]=\\nabla\\log{\\mathbf{f}(\\mathbf{x})}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "It means $\\textbf{X}\\mid\\emph{E}$ is also LAC with the same function, hence LAC is robust with conditioning. Especially, if event $E$ has a form of $E=\\{X_{i}\\in D$ and $X_{j}=x_{j}$ for $j\\neq i\\}$ for some $D\\subset\\mathbb{R}^{d}$ , we define the density of $X_{i}\\mid E$ as $\\begin{array}{r}{f_{i,E}(x)=\\frac{\\mathbf f(x_{1},\\dots x_{i-1},x,x_{i+1},\\dots x_{K})}{\\mathbb P[E]}}\\end{array}$ for $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ . Then, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{x}\\log f_{i,E}(x)=\\nabla_{x}\\log f\\big((x_{1},\\dots x_{i-1},x,x_{i+1},\\dots x_{K})\\big)-\\nabla\\mathbb{P}[E]}\\\\ &{=\\nabla_{x}\\log f\\big((x_{1},\\dots x_{i-1},x,x_{i+1},\\dots x_{K})\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "holds. We have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|\\nabla_{x}\\log f_{i,E}(x)\\|_{\\infty}\\leq\\mathcal{L}(\\|(x_{1},\\ldots x_{i-1},x,x_{i+1},\\ldots x_{K})\\|_{\\infty})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which tell us the conditional density also enjoys LAC property with $\\mathcal{L}(\\cdot)$ . ", "page_idx": 16}, {"type": "text", "text": "We summarize above observations in the following lemma, using $\\mathcal{L}(\\cdot)$ is non-decreasing function. ", "page_idx": 16}, {"type": "text", "text": "Lemma 1 (LAC of conditional contexts) For random vector $\\mathbf{X}=(X_{1}^{\\top},\\cdot\\cdot\\cdot X_{K}^{\\top})\\in\\mathbb{R}^{d K}$ , let it\u2019s density has $L A C$ with function $\\mathcal{L}(\\cdot)$ . Then for event $E:=\\{\\mathbf{X}\\in A\\}$ for $A\\subset\\mathbb{R}^{d K}$ with $\\|A\\|_{\\infty}\\leq R_{*}$ , the conditional random vector $\\mathbf{X}\\mid E$ has $L A C$ with constant function $\\mathcal{L}(R)$ . Especially, if event $E$ has a form of $E\\,=\\,\\{X_{i}\\,\\in\\,D$ and $X_{j}\\,=\\,x_{j}$ for $j\\neq i\\}$ for some $D\\subset\\mathbb{R}^{d}$ with $\\|D\\|_{\\infty}\\leq R_{r}$ , conditional random vector $X_{i}\\mid E$ has $L A C$ with $\\mathcal{L}(R)$ . ", "page_idx": 16}, {"type": "text", "text": "B.4 LAC and Truncation ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We introduce the property that can compute the LAC of truncated contexts. Truncating the contexts $X_{i}(t)$ to a $d$ -dimensional ball $\\mathbb{B}_{R}\\subset\\mathbb{R}^{d}$ for every $i\\in[K]$ still satisfies the LAC with a constant, $\\mathcal{L}(R)$ . ", "page_idx": 16}, {"type": "text", "text": "Lemma 2 (LAC with truncation) Suppose the density of contexts $\\textbf{X}\\in\\mathbb{R}^{d K}$ satisfies the LAC condition with the function $\\mathcal{L}(\\cdot)$ . Consider the case we truncate $\\mathbf{X}$ into the region $(\\mathbb{B}_{R})^{K}$ and define truncated contexts as $\\overline{{\\mathbf{X}}}$ . Then X satisfy the LAC condition with constant function $\\mathcal{L}(R)$ . ", "page_idx": 16}, {"type": "text", "text": "Proof The density of truncated contexts is calculated as fX(x) =P[f(BXR(x))K]. Then, when taking the log and gradient, we get $\\nabla\\log f_{\\mathbf{\\overline{{X}}}}(\\mathbf{x})=\\nabla\\log f_{\\mathbf{\\overline{{X}}}}(\\mathbf{x})$ holds. Since $\\|\\mathbf{x}\\|_{2}\\leq R$ for $\\mathbf{x}\\in(\\mathbb{B}_{R})^{K}$ , and $\\mathcal{L}$ is defined by non-decreasing function, we get $\\nabla\\log\\mathbf{f}_{\\overline{{\\mathbf{X}}}}(\\mathbf{x})\\le\\mathcal{L}(R)$ . ", "page_idx": 16}, {"type": "text", "text": "B.5 LAC and Decay Rate of Density ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Next, we rigorously define the decay rate of a density. ", "page_idx": 17}, {"type": "text", "text": "Definition 3 (Decay rate) For a density $f(x),x\\in\\mathbb{R}^{d}$ , and for a set $A\\subset\\ensuremath{\\mathbb{R}}^{d}$ , we say it has decay rate $M>0$ when ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\frac{f(x_{1})}{f(x_{2})}}\\geq\\exp(-M|x_{1}-x_{2}|).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for all $x_{1},x_{2}\\in A$ . ", "page_idx": 17}, {"type": "text", "text": "We next present lemma that the density with a bounded LAC function $\\mathcal{L}$ has a bounded decay rate in every direction. This property is especially used to control the projected contexts in the whole paper. ", "page_idx": 17}, {"type": "text", "text": "Lemma 3 (Decay rate and LAC) Suppose a density $f$ is defined in a domain $D\\subset\\mathbb{R}^{d}$ and satisfies the LAC condition with constant function ${\\mathcal{L}}\\in\\mathbb{R}$ . Then the decay rate of $f$ in $D$ is bounded by $\\sqrt{d}\\mathcal{L}$ . ", "page_idx": 17}, {"type": "text", "text": "Proof By using Cauchy-Schwarz inequality, we can bound the directional derivative for any $v\\in\\mathbb{S}^{d-1}$ as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{\\partial_{v}f(x)}{f(x)}=v^{\\top}\\frac{\\nabla f(x)}{f(x)}\\leq\\|v\\|_{2}\\|\\frac{\\nabla f(x)}{f(x)}\\|_{2}=\\|\\frac{\\nabla f(x)}{f(x)}\\|_{2}\\leq\\sqrt{d}\\|\\frac{\\nabla f(x)}{f(x)}\\|_{\\infty}\\leq\\sqrt{d}\\mathcal{L}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Next, by applying Gronwall inequality Lemma 22, we get ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\frac{f(x+h v)}{f(x)}}\\geq\\exp(-{\\sqrt{d}}{\\mathcal{L}}h)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for any $x,x+h v\\in D$ . Since $h,v$ is arbitrary, we get the wanted result. ", "page_idx": 17}, {"type": "text", "text": "For univariate function, we define one-side decay rate, which is weaker quantity then decay rate. ", "page_idx": 17}, {"type": "text", "text": "Definition 4 (One-side decay rate) For univariate function $g$ , we say it has one-side decay rate $M$ in set $A$ when for all $y,y^{\\prime}\\in A,y<y^{\\prime}$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{g(y^{\\prime})}{g(y)}\\geq\\exp(-M(y^{\\prime}-y)).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Using these observations, we can see that the LAC condition gives the upper bound of the decay rate of density. For the one-dimensional case, for instance, for density with $f(x)\\propto\\exp(-M x)$ has the decay rate $M$ . Then we can bound the lower bound of variance as $\\frac{1}{M^{2}}$ scale and the maximum density is bounded by $M$ . Since Challenge 1 is related to the variance lower bound and Challenge 2 is related to the maximum density (of suboptimality gap), we aim to investigate the decay rate of contexts density. ", "page_idx": 17}, {"type": "text", "text": "We first present our key lemma, which gives relation between one-side decay rate and variance lower bound. In the following part, we present the rigorous relations between LAC, (one-side) decay rate, variance lower bound and maximum density. ", "page_idx": 17}, {"type": "text", "text": "Lemma 4 (One-side decay rate and variance lower bound) Consider the density $g(\\cdot)$ of random variable $Y\\,\\in\\,\\mathbb{R}$ with support $I\\,=\\,[a,b]$ with $b\\,>\\,{\\frac{1}{M}}$ for some $M\\,>\\,0$ . Suppose $g(\\cdot)$ satisfies gg((yy\u2032)) \u2265exp(\u2212M|y \u2212y\u2032|) for all y, y\u2032 \u2208I \u2229[\u2212M1 , M1 ], y < y\u2032. Then we get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}[Y^{2}]\\geq c{\\frac{1}{M^{2}}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for some absolute constant $c>0$ . ", "page_idx": 17}, {"type": "text", "text": "Proof Using Lemma 5, we get the density $g$ is bounded by $3M$ in the interval $\\left[-{\\frac{1}{2M}},\\,{\\frac{1}{2M}}\\right]\\cap I$ . By applying Lemma 6, we get the wanted result. If $I\\cap[-\\textstyle{\\frac{1}{M}},\\,\\textstyle{\\frac{1}{M}}]=\\emptyset$ or $I\\cap\\left[-{\\textstyle{\\frac{1}{2M}}},{\\textstyle{\\frac{1}{2M}}}\\right]=\\varnothing$ , it means that $\\begin{array}{r}{|Y|\\geq\\frac{1}{2M}}\\end{array}$ almost surely and we get the result directly. ", "page_idx": 17}, {"type": "text", "text": "Next, we present another key lemma that gives relation between one-side decay rate and maximum density. ", "page_idx": 17}, {"type": "text", "text": "Lemma 5 (One-side decay rate leads maximum density) Suppose a real-valued random variable $Y$ has density $g$ and for $\\mathrm{~\\boldmath~\\mu~}_{0}\\;\\in\\;\\mathbb{R},\\;[y_{0},y_{0}\\,+\\,\\frac{1}{2M}]\\;\\subset\\;\\mathrm{supp}(\\dot{Y})$ for some $M\\,>\\,0$ . If $g$ satisfies $\\begin{array}{r}{\\frac{g(y_{0}+h)}{g(y_{0})}\\geq\\exp(-M h)}\\end{array}$ holds for any $\\begin{array}{r}{0\\leq h<\\frac{1}{2M}}\\end{array}$ , then $g(y_{0})\\leq3M$ . ", "page_idx": 18}, {"type": "text", "text": "Proof Since the integral of the density is 1, we get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{1=\\displaystyle\\int_{y\\in\\mathrm{supp}(g)}g(y)d x}\\\\ &{\\~~\\ge g(y_{0})\\displaystyle\\int_{y_{0}}^{y_{0}+\\frac{1}{2M}}\\frac{g(y)}{g(y_{0})}d x}\\\\ &{~~\\ge g(y_{0})\\displaystyle\\int_{y_{0}}^{y_{0}+\\frac{1}{2M}}\\exp(-M(y-y_{0}))d x}\\\\ &{~~\\ge g(y_{0})\\frac{1}{3M}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "therefore ", "page_idx": 18}, {"type": "equation", "text": "$$\ng(y_{0})\\leq3M\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "holds. ", "page_idx": 18}, {"type": "text", "text": "Lemma 6 (Maximum density leads variance lower bound) Let $Y$ be an univariate random variable and set $A=\\operatorname{supp}(Y)$ . For an interval $\\begin{array}{r}{I=[-\\frac{1}{3M},\\frac{1}{3M}],}\\end{array}$ , if density of $Y$ in $I\\cap A$ is bounded by $M$ , then we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}[Y^{2}]\\geq c{\\frac{1}{M^{2}}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof First, since the density is bounded by $M$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{P}[Y\\in A\\cap I]\\le\\frac{2}{3M}\\times M=\\frac{2}{3}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, with a probability greater than $\\textstyle{\\frac{1}{3}},Y\\in A\\cap\\{|x|>{\\frac{1}{3M}}\\}$ . Therefore, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}[Y^{2}]\\ge\\frac{1}{3}(\\frac{1}{3M})^{2}=\\frac{1}{27M^{2}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "C High-level Proof of Theorem 1 (Unbounded Contexts) ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we briefly outline the high-level proof of Theorem 1 for unbounded contexts. This section provides a summary and high-level sketch of proofs. The full proofs are presented in Appendix E and F. First, we explain the mechanism by which two challenges lead to a logarithmic regret bound, as related to the proof of Theorem 1. Next, we provide a proof sketch for the theorems concerning the two challenges: Theorem 2 and Theorem 3. For bounded contexts, the result statements are in Appendix G, and the proofs are in Appendix H. Since the proof for bounded contexts uses a similar approach to the unbounded case, we first provide a proof sketch of the unbounded results. ", "page_idx": 18}, {"type": "text", "text": "C.1 Regret Analysis: Overcoming Two Challenges ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we briefly present our proof sketch for the regret bounds. We describe how we can achieve logarithmic regret addressing the two challenges: Challenges 1 and 2. ", "page_idx": 18}, {"type": "text", "text": "Addressing Challenge 1, we can obtain the $\\textstyle{\\mathcal{O}}({\\frac{1}{\\sqrt{t}}})$ rate $\\ell_{2}$ bound of the parameter $\\widehat{\\theta}_{t}$ and $\\theta^{\\star}$ by using the combination of the self-normalized concentration [1] and properties that $\\begin{array}{r}{\\Sigma_{t}\\succeq\\frac{1}{4}\\lambda_{\\star}t}\\end{array}$ holds with high probability (by using Corollary 9). The details are in Appendix I. Hence, by tackling Challenge 1, \u2225\u03b8\u02c6t \u2212\u03b8\u22c6\u22252 \u2264c\u221ad\u221a l\u03bbogt T holds with high probability. The following part describes how we can get logarithmic regret addressing Challenge 2. First, simply assume the contexts are bounded, such as $\\|X_{i}(\\bar{t})\\|_{2}\\,\\leq\\,x_{\\operatorname*{max}}$ . Later, in our main proof, we also modify it to the relaxed condition $\\|X_{i}(t)\\|_{\\psi_{1}}\\leq x_{\\operatorname*{max}}$ . ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "Challenge 1: $\\sqrt{t}$ -rate $\\ell_{2}$ concentration. Ensuring the first Challenge 1 can lead the $\\ell_{2}$ statistical resolution of the estimator. Hence, we can get ", "page_idx": 19}, {"type": "equation", "text": "$$\n|X_{a(t)}^{\\top}(\\widehat{\\theta}_{t-1}-\\theta^{\\star})|\\leq c x_{\\operatorname*{max}}\\frac{\\sqrt{d\\log T}}{\\sqrt{\\lambda_{\\star}\\times(t-1)}},\\quad|X_{a^{\\star}(t)}^{\\top}(\\widehat{\\theta}_{t-1}-\\theta^{\\star})|\\leq c x_{\\operatorname*{max}}\\frac{\\sqrt{d\\log T}}{\\sqrt{\\lambda_{\\star}\\times(t-1)}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "holds with high probability. Details are in Appendix I.1. ", "page_idx": 19}, {"type": "text", "text": "However, this resolution in insufficient for the logarithmic regret, it can only makes $O({\\sqrt{T}})$ regret bound. ", "page_idx": 19}, {"type": "text", "text": "Challenge 2: Towards logarithmic regret. Furthermore, addressing Challenge 2 (margin condition), we can get logarithmic expected regret upper bound. When the greedy policy select $a(t)$ , it means that ", "page_idx": 19}, {"type": "equation", "text": "$$\nX_{a(t)}(t)^{\\top}\\hat{\\theta}_{t-1}\\geq X_{a^{\\star}(t)}^{\\top}\\hat{\\theta}_{t-1}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and by the definition of the optimal arm, ", "page_idx": 19}, {"type": "equation", "text": "$$\nX_{a(t)}(t)^{\\top}\\theta^{\\star}\\leq X_{a^{\\star}(t)}^{\\top}\\theta^{\\star}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "holds. Ensuring Challenge 1, we get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{reg}^{\\prime}(t):=X_{a^{\\star}(t)}(t)^{\\top}\\theta^{\\star}-X_{a(t)}(t)^{\\top}\\theta^{\\star}\\leq2c x_{\\operatorname*{max}}\\frac{\\sqrt{d\\log T}}{\\sqrt{\\lambda_{\\star}\\times(t-1)}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Next, we define the event $E$ as the event of $\\mathbf X(t)$ with regret occurring as $\\mathrm{reg}^{\\prime}(t)>0$ . Under the event $E$ , the suboptimality gap of $\\mathbf X(t)$ satisfies ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Delta(\\mathbf{X}(t))\\leq\\mathrm{reg}^{\\prime}(t)\\leq2c x_{\\mathrm{max}}\\frac{\\sqrt{d\\log T}}{\\sqrt{\\lambda_{\\star}\\times(t-1)}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and by overcoming Challenge 2, we get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\mathbf{X}(t)}[\\mathrm{reg}^{\\prime}(t)>0]\\le C_{\\Delta}\\times2c x_{\\mathrm{max}}\\frac{\\sqrt{d\\log T}}{\\sqrt{\\lambda_{\\star}\\times(t-1)}}+\\frac{1}{\\sqrt{T}}\\le3c x_{\\mathrm{max}}C_{\\Delta}\\frac{\\sqrt{d\\log T}}{\\sqrt{\\lambda_{\\star}\\times(t-1)}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By combining two things, we can bound the expected regret as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{E}_{\\mathbf{X}(t)}[\\mathrm{reg}^{\\prime}(t)]\\leq3c x_{\\mathrm{max}}C_{\\Delta}\\frac{\\sqrt{d\\log T}}{\\sqrt{\\lambda_{\\star}\\times(t-1)}}\\times2c x_{\\mathrm{max}}\\frac{\\sqrt{d\\log T}}{\\sqrt{\\lambda_{\\star}\\times(t-1)}}}\\\\ &{\\qquad\\qquad\\qquad=6c^{2}x_{\\mathrm{max}}^{2}C_{\\Delta}\\frac{d\\log T}{\\lambda_{\\star}\\times(t-1)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This observation enable us to get logarithmic regret bound. Using this argument, our only remaining goal is bounding two constants in Challenge 1 and 2. We summarize our above observation in the following Lemma. ", "page_idx": 19}, {"type": "text", "text": "Lemma 7 Assume that $\\|X_{i}(t)\\|_{2}\\,\\leq\\,R$ for some $R\\,>\\,0$ for all $i\\,\\in\\,[K]$ . Also assume that the estimator $\\begin{array}{r}{\\|\\hat{\\theta}_{t-1}-\\theta^{\\star}\\|\\leq A\\frac{1}{\\sqrt{t-1}}}\\end{array}$ holds for some constant $A>0$ . Then the (unexpected) regret in round $t$ is bounded by $\\begin{array}{r}{\\mathrm{reg}^{\\prime}(t)\\leq2A R\\frac{1}{\\sqrt{t-1}}}\\end{array}$ R\u221at1\u22121. Furthermore, under the margin condition (Challenge 2), we get $\\begin{array}{r}{\\mathbb{E}_{\\mathbf{X}(t)}[\\mathrm{reg}^{\\prime}(t)]\\leq6R^{2}A^{2}C_{\\Delta}\\frac{1}{t-1}}\\end{array}$ to hold. ", "page_idx": 19}, {"type": "text", "text": "Using the above observation, we can get a logarithmic regret bound when $\\lVert X_{i}(t)\\rVert_{2}$ is bounded. For bounded $\\|X_{i}(t)\\|_{\\psi_{1}}$ case, we use the peeling technique. Given the history $\\mathcal{H}_{t-1},\\hat{\\theta}_{t-1}-\\theta^{\\star}$ is a fixed vector, and using the results from Appendix I.2, we can bound $|X_{i}(t)^{\\top}v|,v\\in\\{\\hat{\\theta}_{t-1}-\\theta^{\\star},\\theta^{\\star}\\}$ with high probability. Hence we can do the similar arguments, and details are presented in Appendix I. ", "page_idx": 19}, {"type": "text", "text": "C.2 Tackling Two Challenges and Fixed History Arguments ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Now, we summarize our proof strategy to prove diversity (Challenge 1) and suboptimality gap (Challenge 2). First, we consider the problem with fixed history setup, which do analysis under the given history $\\mathcal{H}_{t-1}$ . We set history-conditioned contexts as $\\mathbf{X}\\,:=\\,\\mathbf{X}(t)\\,\\mid\\,\\mathcal{H}_{t-1}$ and $\\textbf{X}=$ $(X_{1}^{\\top},\\cdot\\cdot\\cdot X_{K}^{\\top}),X_{i}\\in\\mathbb{R}^{d}.$ . We describe more about this history fixing in the next Appendix E. Under the given event $\\mathcal{H}_{t-1}$ , $\\hat{\\theta}_{t-1}$ is a deterministic value and no longer random. Thus, the policy $a(t)$ , conditioned on $\\mathcal{H}_{t-1}$ , is a greedy policy with respect to $\\hat{\\theta}_{t-1}$ , making it deterministic as well. To address any $\\hat{\\theta}_{t-1}$ , we propose an analysis that applies to any $\\theta\\in\\ensuremath{\\mathbb{R}}^{d}$ . In Appendix E.1, we argue that it suffices to bound this variance for any fixed $\\theta$ . Since we fix $\\theta$ , which is the corresponding value of $\\hat{\\theta}_{t-1}$ under the given history $\\mathcal{H}_{t-1}$ , we define the policy-selected arm $a=\\arg\\operatorname*{max}X_{i}^{\\top}\\theta$ . The main difficulty is that the property of the selected contexts $X_{a}$ for $a=\\arg\\operatorname*{max}X_{i}^{\\top}\\theta$ is difficult to analyze. Hence we do the conditioning to the event, which the policy selects a certain arm $i$ . Also, recall that we defined $\\begin{array}{r}{\\Delta(\\mathbf{X}):=X_{a^{\\star}}^{\\top}\\bar{\\theta^{\\star}}-\\operatorname*{max}_{i\\neq a^{\\star}}X_{i}^{\\top}\\theta^{\\star}}\\end{array}$ . Next, we introduce our two important goals to achieve two challenges. ", "page_idx": 20}, {"type": "text", "text": "Goal 1. For any $\\theta,v\\in\\mathbb{S}^{d-1}$ and greedy policy w.r.t. $\\theta$ , we aim find the lower bound of ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}[v X_{a}X_{a}^{\\top}v].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Goal 2. For fixed $\\theta^{\\star}$ , our aim is to find the upper bound of $C_{\\Delta}$ that satisfies ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{P}[\\Delta(\\mathbf{X})\\le\\varepsilon]\\le C_{\\Delta}\\varepsilon+\\frac{1}{\\sqrt{T}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "It is straightforward that when we achieve Goal 1, then Challenge 1 can be achieved easily since Goal 1 is preparing for all greedy policy with $\\theta$ . ", "page_idx": 20}, {"type": "text", "text": "C.3 Starting with Truncation ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Before we do all the previous steps, we start by truncating our contexts $\\mathbf{X}$ with high-probability regions. Since $X_{i}$ has bounded $\\psi_{1}$ norm as $x_{\\mathrm{max}}$ , roughly speaking, there exists set $\\bar{D}\\subset\\mathbb{R}^{d}$ with $\\|D\\|_{\\infty}=\\widetilde{\\mathcal{O}}(1)$ and ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{P}[\\mathbf{X}\\in D^{K}]\\approx1.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We can view $\\mathbf{X}$ as a mixture of $\\mathbf{X}\\mid\\{X\\in D^{K}\\}$ and $\\mathbf{X}\\mid\\{\\mathbf{X}\\in(D^{K})^{c}\\}$ and with high probability, $\\mathbf{X}$ is sampled from the distribution $\\dot{\\textbf{X}}|\\,\\{X\\in\\dot{D}^{K}\\}$ . In this section (Appendix C), since we are providing proof sketch, so we just regard $\\mathbf{X}$ is sampled from the distribution with bounded support $D^{K}$ where $\\|D\\|_{\\infty}=\\widetilde{\\mathcal{O}}(1)$ . At Appendix B.3, we observed that $\\mathbf{X}\\mid\\{\\mathbf{X}\\in D^{K}\\}$ also has LAC and since $\\|D\\|_{\\infty}\\,=\\,\\widetilde{\\mathcal{O}}(1)$ , it has constant LAC with $\\mathcal{L}(\\|D\\|_{\\infty})\\,=\\,\\mathcal{L}(\\widetilde{\\mathcal{O}}(1))\\,=\\,\\widetilde{\\mathcal{O}}(1)$ since $\\mathcal{L}$ is polynomial. ", "page_idx": 20}, {"type": "text", "text": "So only for this section, we assume that $\\mathbf{X}$ has bounded support $D^{K}$ and it has bounded constant LAC function $\\mathcal{L}\\,=\\,\\widetilde{\\mathcal{O}}(1)$ . For rigorous proof and justification, we provide them throughout Appendix $\\boldsymbol{\\mathrm E}$ and Appendix F. ", "page_idx": 20}, {"type": "text", "text": "C.4 Event Decompositions ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Next, we start to bound the constants in two challenges. Before that, we present event deompositions for our analysis. ", "page_idx": 20}, {"type": "text", "text": "Definition 5 (Event decomposition 1) We define the event $\\{a\\,=\\,i\\}$ as $\\Omega_{i}$ and $\\Omega_{i}(\\{x_{j}\\}_{j\\neq i})\\;:=$ $\\{a=i\\}\\cap\\{X_{j}=x_{j}$ for all $j\\neq i\\,j$ . ", "page_idx": 20}, {"type": "text", "text": "Pick and $v\\in\\mathbb{S}^{d-1}$ . We can decompose the variance term as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\boldsymbol{v}^{\\top}\\boldsymbol{X}_{a}\\boldsymbol{X}_{a}^{\\top}\\boldsymbol{v}]=\\mathbb{E}[\\mathbb{E}[\\boldsymbol{v}^{\\top}\\boldsymbol{X}_{a}\\boldsymbol{X}_{a}^{\\top}\\boldsymbol{v}\\mid\\Omega_{i}]]}\\\\ &{\\phantom{\\mathbb{E}[\\boldsymbol{v}^{\\top}\\boldsymbol{X}_{a}\\boldsymbol{X}_{a}^{\\top}\\boldsymbol{v}\\mid\\Omega_{i}]}=\\mathbb{E}[\\mathbb{E}[\\boldsymbol{v}^{\\top}\\boldsymbol{X}_{i}\\boldsymbol{X}_{i}^{\\top}\\boldsymbol{v}\\mid\\Omega_{i}]]}\\\\ &{\\phantom{\\mathbb{E}[\\boldsymbol{v}^{\\top}\\boldsymbol{X}_{i}\\boldsymbol{X}_{i}^{\\top}\\boldsymbol{v}\\mid\\Omega_{i}(\\{\\boldsymbol{x}_{j}\\}_{j\\neq i})]].}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For notations $\\mathbb{E}[\\mathbb{E}[\\cdot\\mid\\cdot]]$ , please refer Appendix A. From now on, we aim to bound ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}[v^{\\top}X_{i}X_{i}^{\\top}v\\mid\\Omega_{i}(\\{x_{j}\\}_{j\\neq i})]\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for any $i,\\{x_{j}\\}_{j\\neq i}$ . Hence we investigate the property of the conditional density ", "page_idx": 21}, {"type": "equation", "text": "$$\nX_{i}\\mid\\Omega_{i}(\\{x_{j}\\}_{j\\neq i})\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and especially we are interested at density of projected conditional contexts ", "page_idx": 21}, {"type": "equation", "text": "$$\nX_{i}^{\\top}v\\mid\\Omega_{i}(\\{x_{j}\\}_{j\\neq i}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We define new event decompositions for bounding suboptimality gap. Under the fixed history, set $a^{\\star}:=\\arg\\operatorname*{max}_{i\\in[K]}X_{i}^{\\top}\\theta^{\\star}$ . ", "page_idx": 21}, {"type": "text", "text": "Definition 6 (Event decomposition 2) We define the event $\\{a^{\\star}=i\\}$ as $\\Omega_{i}^{\\star}$ and $\\Omega_{i}^{\\star}(\\{x_{j}\\}_{j\\neq i}):=$ $\\left\\{a^{\\star}=i\\right\\}\\cap\\left\\{X_{j}=x_{j}$ for all $j\\neq i\\,j$ . ", "page_idx": 21}, {"type": "text", "text": "Next, we aim to bound $C_{\\Delta}$ , which is another key constant in the Challenge 2. Define the optimal arm $a^{\\star}=\\arg\\operatorname*{max}X_{i}^{\\top}\\theta^{\\star}$ and suboptimal arm $a^{\\prime}=\\mathrm{argmax}_{i\\neq a^{\\star}}X_{i}^{\\top}\\theta^{\\star}$ . By the definition, for any $\\varepsilon>0$ we have to calculate ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}[\\Delta(\\mathbf{X})\\leq\\varepsilon]=\\mathbb{P}[X_{a^{\\star}}^{\\top}\\theta^{\\star}-X_{a^{\\prime}}^{\\top}\\theta^{\\star}\\leq\\varepsilon]}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\mathbb{E}[\\mathbb{P}[X_{a^{\\star}}^{\\top}\\theta^{\\star}-X_{a^{\\prime}}^{\\top}\\theta^{\\star}\\leq\\varepsilon\\ |\\ \\Omega_{i}^{\\star}]]}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\mathbb{E}[\\mathbb{P}[X_{a^{\\star}}^{\\top}\\theta^{\\star}-X_{a^{\\prime}}^{\\top}\\theta^{\\star}\\leq\\varepsilon\\ |\\ \\Omega_{i}^{\\star}(\\{x_{j}}\\}_{j\\neq i})]]}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\mathbb{E}[\\mathbb{P}[X_{i}^{\\top}\\theta^{\\star}-\\underset{j\\neq i}{\\operatorname*{max}}x_{j}^{\\top}\\theta^{\\star}\\leq\\varepsilon\\ |\\ \\Omega_{i}^{\\star}(\\{x_{j}}\\}_{j\\neq i})]]}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "then we aim to bound ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{P}[\\operatorname*{max}_{j\\neq i}x_{j}^{\\top}\\theta^{\\star}\\leq X_{i}^{\\top}\\theta^{\\star}\\leq\\operatorname*{max}_{j\\neq i}x_{j}^{\\top}\\theta^{\\star}+\\varepsilon\\mid\\Omega_{i}^{\\star}(\\{x_{j}\\}_{j\\neq i})]\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and hence we investigate the properties of the conditional density ", "page_idx": 21}, {"type": "equation", "text": "$$\nX_{i}^{\\top}\\theta^{\\star}\\mid\\Omega_{i}^{\\star}(\\{x_{j}\\}_{j\\neq i}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "It is enough to bound the maximum density of $X_{i}^{\\top}\\theta^{\\star}\\mid\\Omega_{i}^{\\star}\\{x_{j}\\}_{j\\neq i}$ since if it is bounded by $U$ , we can see that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{P}[\\underset{j\\neq i}{\\operatorname*{max}}\\,x_{j}^{\\top}\\theta^{\\star}\\leq X_{i}^{\\top}\\theta^{\\star}\\leq\\underset{j\\neq i}{\\operatorname*{max}}\\,x_{j}^{\\top}\\theta^{\\star}+\\varepsilon\\mid\\Omega_{i}^{\\star}(\\{x_{j}\\}_{j\\neq i})]\\leq U\\varepsilon\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "holds and we can set $C_{\\Delta}=U$ . ", "page_idx": 21}, {"type": "text", "text": "C.5 Conditional Contexts are still in LAC ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In the previous Appendix B.3 and Lemma 1, we saw that LAC is preserved for conditioning. Our conditioned contexts for interest, $X_{i}\\mid\\Omega_{i}(\\{x_{j}\\})$ and $X_{i}\\mid\\Omega_{i}^{\\star}(\\{x_{j}\\})$ is conditioning with events such as $X_{i}\\mid\\{X_{j}=x_{j}$ for all $j\\neq i,\\ X_{i}^{\\top}\\theta\\geq\\operatorname*{max}_{j\\neq i}x_{j}^{\\top}\\theta\\}$ , which meets condition of Lemma 1. Then we can apply Lemma 1 and we get that these two conditional density ", "page_idx": 21}, {"type": "equation", "text": "$$\nX_{i}\\mid\\Omega_{i}(\\{x_{j}\\}),\\quad X_{i}\\mid\\Omega_{i}^{\\star}(\\{x_{j}\\})\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "also have LAC with bounded constant function $\\mathcal{L}$ . ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "C.6 Remaining Goal: Bounding Decay Rate of Projected Contexts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Lemma 4 tell us that bounded one-side decay rate of density guarantees the lower bound of the variance. Lemma 5 tell us the bounded one-side decay rate of density guarantee the maximum density. ", "page_idx": 21}, {"type": "text", "text": "In summary, we need to find the lower bound of equation (4) and the maximum density of (5). We present Lemma 4 and 5, which tell us that if we can bound the one-side decay rate, then we can bound variance of (4) and maximum density of (5). By the property that LAC is preserved under conditioning, we can bound the decay rate of $X_{i}\\mid\\Omega_{i}(\\{x_{j}\\}_{j\\neq i})$ and $X_{i}\\mid\\Omega_{i}^{\\star}(\\{x_{j}\\}_{j\\neq i})$ by $\\sqrt{d}\\mathcal{L}$ , by combining Lemma 3 and LAC property of conditional contexts. However, our interests are projected contexts, defined as ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "equation", "text": "$$\nX_{i}^{\\top}v\\mid\\Omega_{i}(\\{x_{j}\\}_{j\\neq i})\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and ", "page_idx": 22}, {"type": "equation", "text": "$$\nX_{i}^{\\top}\\theta^{\\star}\\mid\\Omega_{i}^{\\star}(\\{x_{j}\\}_{j\\neq i})\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which are projected contexts with some direction $v$ and $\\theta^{\\star}$ . In the remaining part, we aim to bound the one-side decay rate of these projected random variables densities to apply Lemma 4 and 5. ", "page_idx": 22}, {"type": "text", "text": "After the decomposition (by conditioning), we proceed with the following steps: ", "page_idx": 22}, {"type": "text", "text": "Decomposition by conditioning $\\rightarrow\\mathrm{LAC}$ of conditional densities $\\rightarrow$ Bounding one-side decay rate of (4) and $(5)\\rightarrow$ Bounding two key constants in two Challenges Using Lemma 4 and 5. ", "page_idx": 22}, {"type": "text", "text": "Support of Conditional Densities. We first observe the support of the conditional density $X_{i}$ | $\\Omega_{i}(\\{x_{j}\\}_{j\\neq i})$ . By the definition of $\\Omega_{i}$ , the arm $i$ should be optimal under the greedy policy $\\theta$ in this event. Therefore, its support is restricted to $\\{x\\in D\\mid x^{\\top}\\theta\\geq\\operatorname*{max}_{j\\neq i}x_{j}^{\\top}\\theta\\}$ . ", "page_idx": 22}, {"type": "text", "text": "Next, we consider the density of projected contexts, $X_{i}^{\\top}v\\;\\mid\\;\\Omega_{i}(\\{x_{j}\\}_{j\\neq i})$ . Since the support of $X_{i}~\\mid~\\Omega_{i}(\\{x_{j}\\}_{j\\neq i})$ is of the form $\\{x\\,\\in\\,D\\,\\mid\\,x^{\\top}\\theta\\,\\geq\\,\\operatorname*{max}_{j\\neq i}x_{j}^{\\top}\\dot{\\theta}\\}$ , our projected density is an integrated form: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb P[X_{i}^{\\top}v=y\\mid\\Omega_{i}(\\{x_{j}\\}_{j\\neq i})]=\\int_{\\{x\\in D|x^{\\top}\\theta\\geq\\operatorname*{max}_{j\\neq i}x_{j}^{\\top}\\theta\\}\\cap\\{x|x^{\\top}v=y\\}}\\mathbb P[X_{i}=x\\mid\\Omega_{i}(\\{x_{j}\\}_{j\\neq i})]\\,d x.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Geometrically, this represents the total density at the intersection of the hyperplanes $\\{x\\mid x^{\\top}v=y\\}$ and the set $\\{\\overset{\\cdot}{x}\\in D\\mid\\overset{\\cdot}{x^{\\top}}\\theta\\geq\\operatorname*{max}_{j\\neq i}x_{j}^{\\top}\\theta\\}$ . We refer to these as section densities, as they represent the total density of sections sliced by the hyperplane $\\{x\\mid x^{\\top}v=y\\}$ . Further discussion is provided in Appendix D. ", "page_idx": 22}, {"type": "text", "text": "Conclusion. Later in Appendix E and Appendix F, since $X_{i}\\mid\\Omega_{i}(\\{x_{j}\\}_{j\\neq i}$ has bounded decay rate $\\sqrt{d}{\\mathcal{L}}=\\widetilde{\\mathcal{O}}(\\sqrt{d})$ , we prove that the projected densities also have bounded one-side decay rate of $\\widetilde{\\mathcal{O}}(\\sqrt{d})$ . Lastly, by applying Lemma 4 and can prove the quantity ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}[v^{\\top}X_{i}X_{i}^{\\top}v\\mid\\Omega_{i}(\\{x_{j}\\}_{j\\neq i})]\\ge c\\frac{1}{d}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for some $c=\\widetilde{\\mathcal{O}}(1)$ . Also, using Lemma 5, we can prove that the density of (5) has bounded density $\\sqrt{d}\\mathcal{L}$ and we can prove that $C_{\\Delta}=\\widetilde{\\mathcal{O}}(\\sqrt{d})$ . ", "page_idx": 22}, {"type": "text", "text": "D Sections, Section Densities and Decay Rate ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we define sections and section densities and investigate their properties. Previously in Appendix C, we discussed decomposition by conditioning, and our interest became the properties of conditioned contexts, the form of $X_{i}\\mid\\dot{\\Omega_{i}}(\\{x_{j}\\}_{j\\neq i})$ and $X_{i}\\mid\\Omega_{i}^{\\star}(\\{x_{j}\\}_{j\\neq i})$ . Our first goal is to bound the variance of projected contexts, such as $X_{i}^{\\top}v\\mid\\Omega_{i}(\\{x_{j}\\}_{j\\neq i})$ for all $\\boldsymbol{v}\\in\\mathbb{R}^{d}$ . To do that, we investigate the projected density of $X_{i}^{\\top}v=y\\mid\\Omega_{i}(\\{x_{j}\\}_{j\\neq i})$ . To apply Lemma 4 and Lemma 5, we only need to bound the one-side decay rate of that projected density. We saw that its density is the total density of intersections with $\\{x\\ot{|}\\;x^{\\top}\\theta\\geq\\operatorname*{max}_{j\\neq i}x_{j}^{\\top}\\theta\\}$ and hyperplane $\\{x\\mid x^{\\top}v={\\dot{y}}\\}$ . We can view it as sections sliced by hyperplanes, and we provide some theory related to sections and section densities. ", "page_idx": 22}, {"type": "text", "text": "D.1 Motivation ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In the proof strategy (Appendix C), we aims to bound the one-side decay rate and maximum density of conditional density, $\\dot{X}_{i}^{\\dag}v\\mid\\Omega_{i}(\\{x_{j}\\}_{j\\neq i})$ and $X_{i}^{\\top}\\theta^{\\star}\\mid\\Omega_{i}^{\\star}(\\{x_{j}\\}_{j\\neq i})$ . Recall that we define one-side decay rate of univariate density $g$ (Definition 4) at Appendix $\\mathbf{C}$ as a constant $M$ satisfying ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{g(y^{\\prime})}{g(y)}\\geq\\exp(-M(y^{\\prime}-y))\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for all $y<y^{\\prime}$ ", "page_idx": 23}, {"type": "text", "text": "As we discussed in Appendix C, we have to deal with section densities. If the support of $X_{i}$ $\\Omega_{i}(\\{x_{j}\\}_{j\\neq i})$ is $A$ , then the density of $X_{i}^{\\top}v\\mid\\Omega_{i}(\\{x_{j}\\}_{j\\neq i})$ becomes the total density in the sections $A\\cap\\{x\\mid x^{\\top}v=y\\}$ . This argument also works for the $X_{i}\\mid\\Omega_{i}^{\\star}(\\{x_{j}\\}_{j\\neq i})$ and $X_{i}^{\\top}\\theta^{\\star}\\mid\\Omega_{i}^{\\star}(\\{x_{j}\\}_{j\\neq i})$ . We already discussed that LAC is maintained under the conditioning, especially the conditioning to event $\\Omega_{i}(\\{x_{j}\\}_{j\\neq i})$ and $\\Omega_{i}^{\\star}(\\{x_{j}\\}_{j\\neq i})$ in the Lemma 1. ", "page_idx": 23}, {"type": "text", "text": "D.2 Sections and Section Densities ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Our remaining goal is to bound the one-side decay rate of section density. To build some general theory, we first define sections and section densities. ", "page_idx": 23}, {"type": "text", "text": "Definition 7 (Sections) For the set $A\\subset\\ensuremath{\\mathbb{R}}^{d}$ and $v\\in\\mathbb{S}^{d-1}$ we define sections as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname{Sec}(A,v,y):=A\\cap\\{x\\mid x^{\\top}v=y\\}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for $y\\in\\mathbb R$ . ", "page_idx": 23}, {"type": "text", "text": "This means that section is the intersection of region $A$ and hyperplane $\\{x\\in\\mathbb{R}^{d}\\mid x^{\\top}v=y\\}$ . ", "page_idx": 23}, {"type": "text", "text": "Definition 8 (Section density) We define the section density of the region $A$ . Let $f$ be the density defined in the region $A\\subset\\ensuremath{\\mathbb{R}}^{\\bar{d}}$ . Then we define $g(y)$ is the section density of ${\\mathrm{Sec}}(A,v,y)$ as ", "page_idx": 23}, {"type": "equation", "text": "$$\ng(y)=\\int_{x\\in\\operatorname{Sec}(A,v,y)}f(x)\\mathrm{d}x.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Remark 1 If the density of $X\\in\\mathbb{R}^{d}$ is $f$ , the section density corresponds to the density of $X^{\\top}v$ for v Sd\u22121. ", "page_idx": 23}, {"type": "text", "text": "Next, we define the areas with special section structures: equal sections and expanding sections. First, we define equal sections, the area whose section with direction $v$ is equal. ", "page_idx": 23}, {"type": "text", "text": "Definition 9 (Equal sections) We define the set $A$ have equal sections with $\\boldsymbol{v}\\in\\mathbb{S}^{d-1}$ when ${\\mathrm{Sec}}(A,v,y)$ is the congruence of shapes for $y>0$ . ", "page_idx": 23}, {"type": "text", "text": "Next, we define expanding sections, where the sections of $A$ with direction $v$ expand when $y$ increases. ", "page_idx": 23}, {"type": "text", "text": "Definition 10 (Expanding sections) We define the sections of $A$ with direction v are expanding sections when ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname{Sec}(A,v,y)+h v\\subset\\operatorname{Sec}(A,v,y+h)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for every $h>0$ . Technically, equal sections are included in expanding sections. ", "page_idx": 23}, {"type": "image", "img_path": "rblaF2euXQ/tmp/09ad5c57f15692152d55b5116a75a4cf0c1c44575ea2f89eab4625c0a0de1ddc.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure 2: Illustration of expanding section\u2019s example. The section with direction $v$ is expanding when $y$ increases: $y$ to $y+h$ . ", "page_idx": 24}, {"type": "text", "text": "D.3 Decay Rate of Section Density ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Recall that we define one-side decay rate of univariate density $g$ (Definition 4) at Appendix $\\mathbf{C}$ as a constant $M$ satisfying ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{g(y^{\\prime})}{g(y)}\\geq\\exp(-M(y^{\\prime}-y))\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for all $y<y^{\\prime}$ . Then, how to bound the (one-side) decay rate of the section density? We present a technique to bound the (one-side) decay rate of section densities defined in equal and expanding sections. ", "page_idx": 24}, {"type": "text", "text": "Lemma 8 (One-side decay rate: equal and expanding sections) For $A\\subset\\mathbb{R}^{d},v\\in\\mathbb{R}^{d}$ and $y\\in\\mathbb{R},$ a density $f$ has support $A$ and has a bounded decay rate $M$ . Assume that the sections of $A$ with the direction $v$ are expanding sections. We define the section density of ${\\mathrm{Sec}}(A,v,y)$ as $g(y)$ then we have for any $y_{1}<y_{2}$ : ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{g(y_{1})}{g(y_{2})}\\geq\\exp(-M|y_{1}-y_{2}|).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof Since the section is expanding with the direction $v$ , we can observe for any $h>0$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{g(y+h)}{g(y)}=\\frac{\\int_{\\mathrm{Sec}(A,v,y+h)}f(x)\\mathrm{d}x}{\\int_{\\mathrm{Sec}(A,v,y)}f(x)\\mathrm{d}x}}\\\\ &{\\qquad\\qquad\\geq\\frac{\\int_{\\mathrm{Sec}(A,v,y)}f(x)\\exp(-M h)\\mathrm{d}x}{h\\int_{\\mathrm{Sec}(A,v,y)}f(x)\\mathrm{d}x}}\\\\ &{\\qquad\\qquad\\geq\\exp(-M h).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The second inequality holds since the section is expanding and using Lemma 3. Since equal sections are also expanding sections, we end the proof. ", "page_idx": 24}, {"type": "text", "text": "Lemma 9 (Maximum density: equal and expanding sections) The density $f(\\cdot)$ has support $A\\subset$ $\\mathbb{R}^{d}$ and has a bounded decay rate $M$ . Furthermore, sections with direction v are expanding sections and define the section density of $\\operatorname{Sec}(A,v,y)$ as $g(y)$ . If the support of $g(\\cdot)$ contains an interval $[a,b]$ , then $\\dot{g(y)}\\leq3M$ for $y\\in[a,b-\\textstyle{\\frac{1}{2M}}]$ . ", "page_idx": 24}, {"type": "text", "text": "Proof It can be obtained directly by applying result of the previous Lemma 8 and Lemma 5. ", "page_idx": 24}, {"type": "text", "text": "E Fixed History Results of Two Challenges for Unbounded Contexts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "This section aim to provide key propositions to prove Theorem 2 and Theorem 3. Our two challenges aim to bound ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}[X_{a(t)}(t)X_{a(t)}(t)]\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{P}[\\Delta(\\mathbf{X}(t))\\leq\\varepsilon].\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We define $\\mathbf{X}(t)=(X_{1}(t)^{\\top},\\ldots,X_{K}(t)^{\\top})\\in\\mathbb{R}^{d K}$ . Under the given event $\\mathcal{H}_{t-1},\\hat{\\theta}_{t-1}$ is a deterministic value and no longer random. ", "page_idx": 25}, {"type": "text", "text": "E.1 Details for Diversity Constant (Challenge 1) ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In the definition of the diversity constant, we recall that the expection is taken with respect to $\\mathbf{X}(t)$ and history $\\mathcal{H}_{t-1}$ . In round $t$ , for any fixed history $\\mathcal{H}_{t-1}$ , we perform the greedy policy with the estimator $\\hat{\\theta}_{t-1}$ . Hence, when the entire set of contexts $\\mathbf{X}(t)=(X_{1}(t)^{\\top},\\ldots,X_{K}(t)^{\\top})$ is revealed, $a(t)$ is determined immediately given the history $\\mathcal{H}_{t-1}$ . Then the exact statement is: ", "page_idx": 25}, {"type": "text", "text": "\u201cIn round $t$ , for any given history $\\mathcal{H}_{t-1}$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{X}(t)}[X_{a(t)}(t)X_{a(t)}(t)^{\\top}]\\succeq\\lambda_{\\star}I_{d}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "holds with some $\\lambda_{\\star}>0$ .\u201d ", "page_idx": 25}, {"type": "text", "text": "In the proof, we proved a stronger statement: ", "page_idx": 25}, {"type": "text", "text": "\"For any greedy policy with any $\\theta\\in\\ensuremath{\\mathbb{R}}^{d}$ and selected arm $a(t)$ with that policy, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{X}(t)}\\left[X_{a(t)}(t)X_{a(t)}(t)^{\\top}\\right]\\succeq\\lambda_{\\star}I_{d}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "holds with some $\\lambda_{\\star}>0$ .\" ", "page_idx": 25}, {"type": "text", "text": "We prove a stronger quantity (second argument), for any given parameter $\\theta\\in\\ensuremath{\\mathbb{R}}^{d}$ , define $a_{\\theta}(\\mathbf{X}(t)):=$ $\\operatorname{argmax}_{i\\in[K]}X_{i}(t)^{\\top}\\theta$ and define $\\begin{array}{r}{\\lambda_{\\star}(t):=\\operatorname*{min}_{\\|\\theta\\|_{2}=1}\\lambda_{\\operatorname*{min}}\\left(\\mathbb{E}_{t}\\big[X_{a_{\\theta}(\\mathbf{X}(t))}X_{a_{\\theta}(\\mathbf{X}(t))}^{\\top}\\big]\\right)}\\end{array}$ . Since $\\hat{\\theta}_{t-1}$ can be arbitrary value, we prepare for all greedy policy w.r.t. $\\theta\\in\\mathbb{S}^{d-1}$ . Then the new defined $\\lambda_{\\star}(t)$ satisfies equation (1) and we discuss this in Appendix E. ", "page_idx": 25}, {"type": "text", "text": "Thus, the policy $a(t)$ , conditioned on $\\mathcal{H}_{t-1}$ , is a greedy policy with respect to $\\hat{\\theta}_{t-1}$ , making it deterministic as well. To address any $\\hat{\\theta}_{t-1}$ , we propose an analysis that applies to any $\\theta\\in\\ensuremath{\\mathbb{R}}^{d}$ , aiming to bound the variance of the selected contexts. In Appendix E.1, we argue that it suffices to bound this variance for any fixed $\\theta$ . ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\|\\boldsymbol{\\theta}\\|_{2}=1}\\lambda_{\\operatorname*{min}}\\left(\\mathbb{E}[X_{a_{\\boldsymbol{\\theta}}(\\mathbf{X}(t))}(t)X_{a_{\\boldsymbol{\\theta}}(\\mathbf{X}(t))(t)}^{\\top}]\\right)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Note that $a_{\\theta}(\\mathbf{X}(t))$ is defined as arg $\\operatorname*{max}_{i\\in[K]}X_{i}(t)^{\\top}\\theta$ . This quantity is equal to ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\|\\boldsymbol{\\theta}\\|_{2}=1,\\|\\boldsymbol{v}\\|_{2}=1}\\mathbb{E}_{t}\\big[\\vert\\boldsymbol{v}^{\\top}\\boldsymbol{X}_{a_{\\theta}(\\mathbf{X}(t))}(t)\\vert^{2}\\big]\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and we prove this lower bound for any $\\theta$ and $v$ . In summary, our aim is to prove that for any history $\\mathcal{H}_{t-1}$ and $\\theta,v$ , we aim to bound ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}\\!\\left[|v^{\\top}X_{a_{\\theta}(\\mathbf{X}(t))}(t)|^{2}\\right].\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "To bound the margin constant, our aim is to prepare for all $\\theta^{\\star}$ , since the true model parameter $\\theta^{\\star}$ also can be arbitrary. ", "page_idx": 25}, {"type": "text", "text": "E.2 The Setup of Fixed History Analysis ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We aim to find a class of density $\\mathcal{F}$ such that if $\\mathbf{X}(t)\\in\\mathcal{F}$ , then equation (7) is lower bounded for any $\\theta,v$ . Similarly, we aim to find the class of density $\\mathcal{G}$ , if $\\mathbf{X}(t)\\in\\mathcal{G}$ , then equation (6) can be upper bounded for any $\\theta^{\\star}\\in\\mathbb{R}^{d}$ . ", "page_idx": 25}, {"type": "text", "text": "1. We first prove that for fixed $\\theta,v$ , for densities with LAC and its support satisfies some geometric conditons, we can bound equation (7). ", "page_idx": 26}, {"type": "text", "text": "2. We also prove that for fixed $\\theta^{\\star}$ , for densities with LAC and its support satisfies some geometric conditions, we can bound equation (6).   \n3. Next, we prove that the densities with LAC and bounded $\\psi_{1}$ norm, for any give $\\theta,v$ we can truncate it with high probability region to the densities contained in class of 1.   \n4. Lastly, we prove that for the densities with LAC and bounded $\\psi_{1}$ norm, for any given $\\theta^{\\star}$ we can truncate it with high probability region to the densities contained in class of 2. ", "page_idx": 26}, {"type": "text", "text": "In this section, we provide the results of 1 and 2. And in the next Appendix F, we proceed to 3 and 4. Now, we build the theory with the assumptions that $\\theta,v$ and $\\mathcal{H}_{t-1}$ are fixed. We assume that random vectors $\\mathbf{Z}=(Z_{1}^{\\top}\\,.\\,.\\,.\\,Z_{K}^{\\top}),Z_{i}\\in\\mathbb{R}^{d}$ arise from some distribution with LAC $\\mathcal{L}(\\cdot)$ . The $\\mathbf{Z}$ represents the history-conditioned contexts, but to build the general theory, we use the notation of $\\mathbf{Z}$ . ", "page_idx": 26}, {"type": "text", "text": "We fix arbitrary $\\theta\\in\\ensuremath{\\mathbb{R}}^{d}$ and define random index variable ", "page_idx": 26}, {"type": "equation", "text": "$$\na=\\operatorname*{argmin}_{i}Z_{i}^{\\top}\\theta.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We also fix $v\\in\\mathbb{S}^{d-1}$ and first aim to bound $\\mathbb{E}[v^{\\top}Z_{a}Z_{a}^{\\top}v]$ . We next define $C_{\\Delta}$ as the margin constant of the $\\mathbf{Z}$ with the parameter $\\theta^{\\star}$ which satisfies ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{P}[\\Delta(\\mathbf{Z})\\leq\\varepsilon]\\leq C_{\\Delta}\\varepsilon+\\frac{1}{\\sqrt{T}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\Delta(\\mathbf{Z})$ is a suboptimality gap, defined similarly in Section 4. ", "page_idx": 26}, {"type": "text", "text": "E.3 Fixed History Results: Diversity Constant ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Our goal is to find the lower bound of $\\mathbb{E}[v^{\\top}Z_{a}Z_{a}^{\\top}v]$ for any $v\\in\\mathbb{S}^{d-1}$ and from now on, we fix the arbitrary $v\\in\\mathbb{S}^{d-1}$ . Then, we aim to bound ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}[v^{\\top}Z_{a}Z_{a}^{\\top}v]\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "for fixed $\\theta$ and $v$ . We define $\\Omega_{i}:=\\{a=i\\}$ . Then, the whole probability space is decomposed as $\\textstyle\\Omega=\\bigcup_{i\\in[K]}\\Omega_{i}$ . Similar to previous decompositions, we decompose $\\Omega_{i}$ as ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\Omega_{i}=\\bigcup_{i,\\{z_{j}\\}_{j\\neq i}}\\Omega_{i}\\cap\\{Z_{j}=z_{j}{\\mathrm{~for~all~}}j\\neq i\\}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and we define ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\Omega_{i}(\\{z_{j}\\}_{j\\neq i}):=\\Omega_{i}\\cap\\{Z_{j}=z_{j}{\\mathrm{~for~all~}}j\\neq i\\}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We already stated in Appendix B.3 that the conditional density $\\textbf{Z}\\vert\\,\\,\\Omega_{i}(\\{z_{j}\\}_{j\\neq i})$ also satisfies LAC and especially, $Z_{i}\\mid\\Omega_{i}(\\{z_{j}\\}_{j\\neq i})$ satisfies LAC type property with $\\mathcal{L}(\\cdot)$ in $\\mathbb{R}^{d}$ . ", "page_idx": 26}, {"type": "text", "text": "We now state and prove our one of the most important results in fixed history analysis. Recall that we define $[-1,1]_{\\boldsymbol{v}}:=\\{\\boldsymbol{x}\\mid-1\\leq\\boldsymbol{x}^{\\top}\\boldsymbol{v}\\leq1\\}$ . ", "page_idx": 26}, {"type": "text", "text": "Proposition 2 (Fixed history diversity constant lower bound) The random vector $\\begin{array}{r l}{\\mathbf{Z}}&{{}=}\\end{array}$ $(Z_{1}^{\\top}...Z_{K}^{\\top})$ , $Z_{i}\\in\\mathbb{R}^{d}$ satisfies the following conditions: ", "page_idx": 26}, {"type": "text", "text": "1. Z has LAC with $\\mathcal{L}(\\cdot)$ and \u2225supp(Z)\u2225 \u2264R.   \n2. For all $i\\in[K],\\,\\mathrm{supp}(Z_{i})$ is identical for some subset $A\\subset\\ensuremath{\\mathbb{R}}^{d}$ . It means $\\operatorname{supp}(\\mathbf{Z})=A^{K}$ for some $A\\subset\\ensuremath{\\mathbb{R}}^{d}$ .   \n3. The support $A\\cap[-1,1]_{v}$ has equal sections with direction $v$ . ", "page_idx": 26}, {"type": "text", "text": "Then we have the following with some absolute constant $c>0$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}[v^{\\top}Z_{a}Z_{a}^{\\top}v]\\ge\\frac{c}{d\\mathcal{L}(R)^{2}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Remark 2 For unbounded contexts $\\mathbf{X}(t)$ in the original bandit problem, we truncate to some region $\\mathbf{C}_{1}$ with positive probability, and we make the truncated contexts satisfy the condition of the above proposition. ", "page_idx": 26}, {"type": "text", "text": "Proof of Proposition 2 Since $\\operatorname{supp}(\\mathbf{Z})\\leq R$ , it has bounded decay rate $\\sqrt{d}\\mathcal{L}(R)$ by Lemma 3. By the previous decomposition, we can see ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\Omega_{i}=\\bigcup_{\\{z_{j}\\}_{j\\neq i}}\\Omega_{i}\\big(\\{z_{j}\\}_{j\\neq i}\\big)\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "holds. By applying tower property, we get ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[v^{\\top}Z_{a}Z_{a}^{\\top}v]=\\mathbb{E}\\left[\\mathbb{E}[Z_{a}Z_{a}^{\\top}\\mid\\Omega_{i}(\\{z_{j}\\}_{j\\neq i})]\\right]}\\\\ {=\\mathbb{E}\\left[\\mathbb{E}[Z_{i}Z_{i}^{\\top}\\mid\\Omega_{i}(\\{z_{j}\\}_{j\\neq i})]\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and we are enough to bound the term ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}[v^{\\top}Z_{i}Z_{i}^{\\top}v\\mid\\Omega_{i}(\\{z_{j}\\}_{j\\neq i})].\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "for every $\\Omega_{i}(\\{z_{j}\\}_{j\\neq i})$ . Recall that we defined $[-1,1]_{v}\\,:=\\,\\{x\\,\\mid\\,x^{\\top}v\\,\\in\\,[-1,1]\\}$ . Then, we can decompose it again as ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}[v^{\\top}Z_{i}Z_{i}^{\\top}v\\mid\\Omega_{i}(\\{z_{j}\\}_{j\\neq i})]}\\\\ &{=\\mathbb{E}\\bigl[v^{\\top}Z_{i}Z_{i}^{\\top}v\\mid\\Omega_{i}(\\{z_{j}\\}_{j\\neq i})\\cap\\{Z_{i}\\in[-1,1]_{v}\\}\\bigr]\\mathbb{P}\\bigl[Z_{i}\\in[-1,1]_{v}\\mid\\Omega_{i}(\\{z_{j}\\}_{j\\neq i})\\bigr]}\\\\ &{\\quad+\\mathbb{E}\\left[v^{\\top}Z_{i}Z_{i}^{\\top}v\\mid\\Omega_{i}(\\{z_{j}\\}_{j\\neq i})\\cap\\{Z_{i}\\in([-1,1]_{v})^{c}\\}\\right]\\mathbb{P}\\bigl[Z_{i}\\in([-1,1]_{v})^{c}\\mid\\Omega_{i}(\\{z_{j}\\}_{j\\neq i})\\bigr]}\\\\ &{\\geq\\mathbb{E}\\bigl[v^{\\top}Z_{i}Z_{i}^{\\top}v\\mid\\Omega_{i}(\\{z_{j}\\}_{j\\neq i})\\cap\\{Z_{i}\\in[-1,1]_{v}\\}\\bigr].}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The last inequality holds since $\\begin{array}{r l r}{\\mathbb{E}\\left[v^{\\top}Z_{i}Z_{i}^{\\top}v\\mid\\Omega_{i}(\\{z_{j}\\}_{j\\neq i})\\cap\\{Z_{i}\\in([-1,1]_{v})^{c}\\}\\right]}&{\\geq}&{1}\\end{array}$ and $\\mathbb{E}\\big[v^{\\top}Z_{i}Z_{i}^{\\top}v\\mid\\Omega_{i}(\\{z_{j}\\}_{j\\neq i})\\cap\\{Z_{i}\\in[-1,1]_{v}\\}\\big]\\le1$ . ", "page_idx": 27}, {"type": "text", "text": "From now on, we focus on the conditional density of $Z_{i}\\mid\\Omega_{i}(\\{z_{j}\\}_{j\\neq i})\\cap\\{Z_{i}\\in[-1,1]_{v}\\}$ and its projected density $v^{\\top}Z_{i}\\mid\\Omega_{i}(\\{z_{j}\\}_{j\\neq i})\\cap\\{Z_{i}\\in[-1,1]_{v}\\}$ . For simplicity, we set the conditional density of $Z_{i}$ | $\\Omega_{i}(\\{z_{j}\\}_{j\\neq i})\\cap\\{Z_{i}\\ \\in\\ [-1,1]_{v}\\}$ as $f_{1}(\\cdot)$ and the projected density of $v^{\\top}Z_{i}\\ |$ $\\Omega_{i}(\\{z_{j}\\}_{j\\neq i})\\cap\\{Z_{i}\\in[-1,1]_{v}\\}$ as $g_{1}(\\cdot)$ . Using Lemma 1, the density $f_{1}(\\cdot)$ has LAC with constant function $\\mathcal{L}(R)$ and it has bounded decay rate $\\sqrt{d}\\mathcal{L}(R)$ by Lemma 3. ", "page_idx": 27}, {"type": "text", "text": "[1] Investigating the support of $f_{1}$ and $g_{1}$ . First, we examine the support of $f_{1}$ . Since arm $i$ is optimal with estimator $\\theta$ , for all $z$ in the support of $f_{1}$ should satisfy $z^{\\top}{\\bar{\\theta}}\\geq\\operatorname*{max}_{j\\neq i}z_{j}^{\\top}\\theta$ . Also, because $-1\\leq z^{\\top}v\\leq1$ holds, then it is the intersection of these two areas. Lets define ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{4_{1}:=\\operatorname{supp}(Z_{i}\\mid\\Omega_{i}(\\{z_{j}\\}_{j\\neq i})\\cap\\{Z_{i}\\in[-1,1]_{v}\\})=\\{z\\in A\\mid z^{\\top}\\theta\\geq\\operatorname*{max}_{j\\neq i}z_{j}^{\\top}\\theta,\\,-1\\leq z^{\\top}v\\leq1\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Recall that $A:=\\operatorname{supp}(Z_{i})$ is designed to have equal sections with direction $v$ . Hence $A\\cap[-1,1]_{v}$ has equal section with $v$ . Therefore, we can see that support of $Z_{i}^{\\top}v\\mid\\Omega_{i}(\\{z_{j}\\}_{j\\neq i})\\cap\\{Z_{i}\\in[-1,1]_{v}\\}$ is interval by our design of $A$ . ", "page_idx": 27}, {"type": "text", "text": "[2] Bounding one-side decay rate of section density in $[-1,1]$ . We aim to apply Lemma 4 to bound the variance $\\mathbb{E}\\big[v^{\\top}Z_{i}Z_{i}^{\\top}v\\ |\\ \\Omega_{i}\\big(\\{z_{j}\\}_{j\\neq i}\\big)\\cap\\{Z_{i}\\in[-1,1]_{v}^{\\top}\\}\\big]$ . It told us that it is enough to bound the one-side decay rate of the section density, $g_{1}(\\cdot)$ in the interval $[-\\frac{1}{2},\\frac{1}{2}]$ . ", "page_idx": 27}, {"type": "text", "text": "Claim 1 Then one of $\\mathrm{Sec}(A_{1},v,y)$ or $\\mathrm{Sec}(A_{1},-v,y)$ are expanding sections when $y$ increases for $y\\in[-1,1]$ . ", "page_idx": 27}, {"type": "text", "text": "Proof Choose one of $v,-v$ that satisfies $\\langle\\cdot,\\theta\\rangle\\geq0$ . We want to use the result of Lemma 10. Since $[-1,1]_{v}$ has equal sections with direction $v$ , by rotating the axis, we can make this satisfy the condition of Lemma 10. Then the result of the Lemma 10 tells that at least one direction of $v,-v$ makes an expanding section. Please see Figure 5 for intuitions. ", "page_idx": 27}, {"type": "image", "img_path": "rblaF2euXQ/tmp/3a24bcfc4faa0639af072d4e273c776a8db88fed480f4cbe775bee7190987fd6.jpg", "img_caption": ["Figure 3: Illustration of $A_{1}$ and expanding sections of $v$ or $-v$ . $A_{1}$ is the area above the green line. In this case, sections with direction $+v$ is expanding! If a cylindrical set is cut by some hyperplane (which is $A_{1}$ ), at least one direction makes expanding sections. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "The result of Claim 1 tell us that we can apply Lemma 8 and finally we can bound the one-side decay rate of section density $g_{1}(\\cdot)$ . Without loss of generality, $\\mathrm{Sec}(A_{1},v,y)$ is expanding sections whan $y$ increases. Then support of $Z_{i}^{\\top}v\\mid\\Omega_{i}(\\{z_{j}\\}_{j\\neq i})\\cap\\{Z_{i}\\in[-1,1]_{v}\\}$ is interval of form $[c,1]$ with some $-1<c<1$ . ", "page_idx": 28}, {"type": "text", "text": "Claim 2 One-side decay rate of the densities $\\mathbb{P}[v^{\\top}Z_{i}=y\\mid\\Omega_{i}(\\{z_{j}\\}_{j\\neq i})\\cap[-1,1]_{v}]$ is bounded by ${\\sqrt{d}}{\\mathcal{L}}(R)$ in $y\\in[-1,1]$ . ", "page_idx": 28}, {"type": "text", "text": "Proof To bound the one-side decay rate, we aim to use Lemma 8. By the following Claim 1, without loss of generality, we can set sections with direction $v$ as ${\\mathrm{Sec}}(A,v,y)$ is expanding section. (Since variance of $v^{\\top}X$ is the same as $(-v)^{\\top}X$ , either case is fine) Then we can apply Lemma 8 and get the wanted result directly. ", "page_idx": 28}, {"type": "text", "text": "[3] Bounding desired variance. We can see that the support of $g_{1}(\\cdot)$ is form of interval $[c,1]$ for some $c<1$ . Hence, by using the result of the previous Lemma 4, we conclude the variance ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}[v^{\\top}Z_{i}Z_{i}^{\\top}v\\mid\\Omega_{i}(\\{z_{j}\\}_{j\\neq i})\\cap Z_{i}\\in[-1,1]_{v}]\\ge\\frac{c}{d\\mathcal{L}(R)^{2}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "for some absolute constant $c>0$ . ", "page_idx": 28}, {"type": "text", "text": "We prove the Lemma 10 which is used in the proof. ", "page_idx": 28}, {"type": "text", "text": "Lemma 10 Consider the (cylindrical shape) set $S=U\\times I$ for $U\\subset\\mathbb{R}^{d-1}$ and the interval $I\\subset\\mathbb{R}$ . For $\\theta\\in\\mathbb{R}^{d}$ with $\\theta^{\\top}e_{n}\\geq0$ and any $b\\in\\mathbb{R}$ , define the set $S^{\\prime}:=S\\cap\\{x\\mid x^{\\top}\\theta\\geq b\\}$ . Then $S^{\\prime}$ is an expanding sections with direction $e_{n}$ . ", "page_idx": 28}, {"type": "text", "text": "Proof To prove that they are expanding sections, we need to show that for any $h~>~0$ , $x_{0}\\perp$ $h e_{n}\\in\\operatorname{Sec}(\\bar{S}^{\\prime},e_{n},y+h)$ for any $\\bar{x}_{0}\\in\\mathrm{Sec}(S^{\\prime},e_{n},y)$ . By the definition of sections, it is clear that $x_{0}+h e_{n}\\in\\{x\\in\\mathbb{R}^{d}\\mid x^{\\top}e_{n}=y+h\\}$ . Next, we need to show that $x_{0}+h e_{n}\\in\\{x\\mid x^{\\top}\\theta\\geq b\\}$ . Since $e_{n}^{\\top}\\theta\\geq0$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n(x_{0}+h e_{n})^{\\top}\\theta\\geq x_{0}^{\\top}\\theta\\geq b\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "holds. ", "page_idx": 28}, {"type": "image", "img_path": "rblaF2euXQ/tmp/a0ee07e9ede05088b680409e313dd836126263678232639927e76ac35aefc753.jpg", "img_caption": ["Figure 4: Illustration of $S$ and expanding sections with direction $e_{n}$ . Blue lines are sections $\\mathrm{Sec}(S^{\\prime},e_{n},y)$ . If cylindrical shape set is sliced by some hyperplane, at least one direction makes an expanding sections. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "E.4 Fixed History Results: Suboptimality Gap ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Next, we provide a fixed history analysis to bound margin constant of Challenge 2. In this section, we first bound the margin constant $C_{\\Delta}$ by assuming $\\bar{||\\theta^{\\star}||_{2}}=1$ . If we get $C_{\\Delta}$ for $\\lVert\\theta^{\\star}\\rVert_{2}=1$ , then for general case, if $\\lVert{\\boldsymbol{\\theta}}^{\\star}\\rVert_{2}=\\kappa$ , observe that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathbb{P}[X_{a^{*}(t)}(t)^{\\top}\\theta^{\\star}-\\displaystyle\\operatorname*{max}_{j\\not=a^{*}(t)}X_{j}(t)^{\\top}\\theta^{\\star}\\le\\varepsilon]=\\mathbb{P}[X_{a^{*}(t)}(t)^{\\top}\\theta^{\\star}/\\kappa-\\displaystyle\\operatorname*{max}_{j\\not=a^{*}(t)}X_{j}(t)^{\\top}\\theta^{\\star}/\\kappa\\le\\varepsilon/\\kappa]}&{}&\\\\ {(1&{\\le s}\\le\\varepsilon)}&{(1-\\varepsilon)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\frac{C_{\\Delta}}{\\kappa}\\varepsilon+\\displaystyle\\frac{1}{\\sqrt{T}}}&{(1-\\varepsilon)}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "holds. So from now on, we assume $\\|\\theta^{\\star}\\|_{2}=1$ and we adjust it later. Also, we highlight that we only need to bound $C_{\\Delta}$ it for $\\varepsilon<\\frac{1}{2}$ , since if $\\varepsilon>\\frac{1}{2}$ , it holds with $C_{\\Delta}=2$ . ", "page_idx": 29}, {"type": "text", "text": "Before we start, we define the cylindrical set, which is used in characterizing the support of densities. ", "page_idx": 29}, {"type": "text", "text": "Definition 11 (Cylindrical region) We define $A\\subset\\mathbb{R}^{d}$ as a cylindrical region with direction $v\\in$ $\\mathbb{S}^{d-1}$ and length $H$ when $A\\,=\\,\\{B+t v\\;\\mid\\;-H\\;\\leq\\;t\\,\\leq\\,H\\}$ for some subset of the hyperplane $B\\subset\\{x\\mid x^{\\top}v=0\\}$ . We write this set as $\\operatorname{\\yl}(B,v,H)$ . ", "page_idx": 29}, {"type": "image", "img_path": "rblaF2euXQ/tmp/cfe270dfdfce62efb992f883aea89ce8358660832f75c3c18bbcc048527be418.jpg", "img_caption": ["Figure 5: Illustration of cylindrical region $A$ . "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "Next, we prove that if the support of each context $Z_{i}$ is a cylindrical set, we can bound the margin constant. We state our key results for the bounding margin constant in the fixed history setup. ", "page_idx": 30}, {"type": "text", "text": "Proposition 3 (Fixed history margin constant bound) Assume that the random vector ${\\bf Z}\\quad=$ $(Z_{1}^{\\top}...Z_{K}^{\\top})$ , $Z_{i}~\\in~\\mathbb{R}^{d}$ satisfies LAC with function $\\mathcal{L}(\\cdot)$ and $\\|\\operatorname{supp}(\\mathbf{Z})\\|_{\\infty}\\,\\leq\\,R$ . Additionally, $i t$ satisfies following conditions: ", "page_idx": 30}, {"type": "text", "text": "1. For all $i\\in[K],\\,\\mathrm{supp}(Z_{i})\\subset\\mathbb{R}^{d}$ is identical for some set $A\\subset\\ensuremath{\\mathbb{R}}^{d}$ . It means $\\operatorname{supp}(\\mathbf{Z})=A^{K}$ . 2. For some $H>0$ , $Z_{i}$ \u2019s support $\\operatorname{supp}(Z_{i})\\,=\\,A=\\operatorname{cyl}(B,\\theta^{\\star},H+1)$ for all $i\\in[K]$ and $\\mathbb{P}[\\mathbf{Z}\\in(\\mathrm{cyl}(B,\\theta^{\\star},H))^{K}]\\ge1-\\delta$ . ", "page_idx": 30}, {"type": "text", "text": "Then we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}[\\Delta(\\mathbf{Z})\\leq\\varepsilon]\\leq3\\sqrt{d}\\mathcal{L}(R)\\varepsilon+\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Remark 3 For the original bandit problem with contexts $\\mathbf{X}(t)$ , we truncate to some high-probability region $\\mathbf{C}_{2}$ and set the truncated contexts to satisfy the conditions of the above proposition. We provide the analysis in the next Appendix $F$ . ", "page_idx": 30}, {"type": "image", "img_path": "rblaF2euXQ/tmp/2a37ef5cdb89839e19795c189a652a8809935de2dc08140a5152e30d46926696.jpg", "img_caption": ["Figure 6: Illustration of $\\operatorname{supp}(Z_{i})$ . It has equal section with $\\theta^{\\star}$ and $\\|Z_{i}\\|_{\\infty}\\leq R$ . "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "Proof of Proposition 3 Define $Z_{i}^{\\top}\\theta^{\\star}=U_{i}$ for all $i\\in[K]$ . With slight abusing of notations, we define optimal arm $a^{\\star}=\\arg\\operatorname*{max}_{i}Z_{i}^{\\top}\\theta^{\\star}$ and suboptimal arm $a^{\\prime}=\\arg\\operatorname*{max}_{i\\neq a^{\\star}}Z_{i}^{\\top}\\theta^{\\star}$ for the proof. ", "page_idx": 30}, {"type": "text", "text": "[1] Decomposition by conditioning. We can bound the margin probability by conditioning, as ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~\\mathbb{P}[U_{a^{*}}-U_{a^{\\prime}}\\leq\\varepsilon]}\\\\ &{=\\mathbb{E}[\\mathbf{1}(U_{a^{*}}-U_{a^{\\prime}}\\leq\\varepsilon)]}\\\\ &{=\\mathbb{E}[\\mathbf{1}(U_{a^{*}}-U_{a^{\\prime}}\\leq\\varepsilon)\\mid U_{a^{\\prime}}\\leq H]\\mathbb{P}[U_{a^{\\prime}}\\leq H]+\\mathbb{E}[\\mathbf{1}(U_{a^{*}}-U_{a^{\\prime}}\\leq\\varepsilon)\\mid U_{a^{\\prime}}>H]\\mathbb{P}[U_{a^{\\prime}}>H]}\\\\ &{\\leq\\mathbb{E}[\\mathbf{1}(U_{a^{*}}-U_{a^{\\prime}}\\leq\\varepsilon)\\mid U_{a^{\\prime}}\\leq H]\\mathbb{P}[U_{a^{\\prime}}\\leq H]+\\delta\\quad(\\mathbf{By~the~second~condition~of~the~proposition})}\\\\ &{\\leq\\mathbb{E}[\\mathbf{1}(U_{a^{*}}-U_{a^{\\prime}}\\leq\\varepsilon)\\mid U_{a^{\\prime}}\\leq H]+\\delta}\\\\ &{\\leq\\mathbb{E}[\\mathbb{E}[\\mathbf{1}(U_{a^{*}}-U_{a^{\\prime}}\\leq\\varepsilon)\\mid\\{U_{a^{\\prime}}\\leq H\\}\\cap\\Omega_{i}^{*}(\\{\\varepsilon_{j}\\}_{j\\neq i})]]+\\delta}\\\\ &{\\leq\\mathbb{E}\\big[\\mathbb{E}[\\mathbf{1}(0\\leq U_{i}-\\operatorname*{max}_{j\\neq i}z_{j}^{\\top}\\theta^{*}\\leq\\varepsilon)\\mid\\{\\operatorname*{max}_{j\\neq i}z_{j}^{\\top}\\theta^{*}\\leq H\\}\\cap\\Omega_{i}^{*}(\\{\\varepsilon_{j}\\}_{j\\neq i})]\\big]+\\delta}\\\\ &{=\\mathbb{E}\\big[\\mathbb{E}[\\mathbf{1}(\\operatorname*{max}_{j\\neq i}z_{j}^{\\top}\\theta^{*}\\leq Z_{i}^{\\top}\\theta^{*}\\leq\\varepsilon+\\operatorname*{max}_{j\\neq i}z_{j}^{\\top}\\theta^{*})\\mid\\{\\operatorname*{max}_{j\\neq i}z_{j}^{\\top}\\theta^{*}\\leq H\\}\\cap\\Omega_{i}^{*}(\\{\\varepsilon_{j}\\}_{j\\neq i})]\\big]+\\delta}\\\\ &{=\\mathbb{E}\\big[\\mathbb{E}[\\mathbf{1}(\\operatorname*{max}_{j\\neq i}z_{j}^{\\top}\\theta^{*\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We set the density of $Z_{i}\\mid\\{\\operatorname*{max}_{j\\neq i}z_{j}^{\\top}\\theta^{\\star}\\ \\leq\\ H\\}\\cap\\Omega_{i}^{\\star}(\\{z_{j}\\}_{j\\neq i})$ and $Z_{i}^{\\top}\\theta^{\\star}\\ |\\ \\{\\mathrm{max}_{j\\neq i}\\,z_{j}^{\\top}\\theta^{\\star}\\ \\leq$ $H\\}\\cap\\Omega_{i}^{\\star}(\\{z_{j}\\}_{j\\neq i})$ as $f_{2}$ and $g_{2}$ . ", "page_idx": 30}, {"type": "text", "text": "[2] Support of $f_{2}(\\cdot),g_{2}(\\cdot)$ and their geometry. We first examine the support of the conditional density $f_{2}$ , which is a density of $Z_{i}=z\\mid\\{\\operatorname*{max}_{j\\neq i}z_{j}^{\\top}\\theta^{\\star}\\leq H\\}\\cap\\Omega_{i}^{\\star}(\\{z_{j}\\}_{j\\neq i})$ . Under the given $\\Omega_{i}^{\\star}(\\{z_{j}\\}_{j\\neq i})$ , we have $z^{\\top}{\\theta}^{\\star}\\geq\\operatorname*{max}_{j\\neq i}z_{j}^{\\top}{\\theta}^{\\star}$ by the definition of $\\Omega_{i}^{\\star}(\\{z_{j}\\}_{j\\neq i})$ , since arm $i$ should be the optimal arm. When we write $\\operatorname*{max}_{j\\neq i}z_{j}^{\\top}\\theta^{\\star}=b$ with $b\\leq H$ , the support of $f_{2}$ becomes ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\{z\\in A\\mid z^{\\top}\\theta^{\\star}\\geq b\\}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Recall that $A:=\\operatorname{supp}(Z_{i})$ and it has equal section with direction $\\theta^{\\star}$ . Using Lemma 1, the density $f_{2}(\\cdot)$ has LAC with constant function $\\mathcal{L}(R)$ and it has bounded decay rate $\\sqrt{d}\\mathcal{L}(R)$ by Lemma 3. ", "page_idx": 31}, {"type": "text", "text": "[3] Bounding one-side decay rate. Next we aim to bound the one-side decay rate of $\\begin{array}{r l r}{Z_{i}^{\\top}\\theta^{\\star}\\!}&{=}&{\\!y^{\\top}\\big|\\quad\\{\\operatorname*{max}_{j\\neq i}z_{j}^{\\top}\\theta^{\\star}\\ \\le\\ H\\}\\ \\cap\\ \\Omega_{i}^{\\star}\\big(\\{z_{j}\\}_{j\\neq i}\\big)}\\end{array}$ . Define the (conditional) density of Zi\u22a4 \u03b8\u22c6 = y | $\\{\\operatorname*{max}_{j\\neq i}z_{j}^{\\top}\\theta^{\\star}\\:\\leq\\:H\\}\\cap\\Omega_{i}^{\\star}(\\{z_{j}\\})$ as $g_{2}(y)$ . Then, its support is restricted to the region $\\{y\\mid b\\leq y\\leq H+1\\}$ . ", "page_idx": 31}, {"type": "text", "text": "Claim 3 The one-side decay rate of the projected density $Z_{i}^{\\top}\\theta^{\\star}=y\\ |\\ \\{\\operatorname*{max}_{j\\neq i}z_{j}^{\\top}\\theta^{\\star}\\leq H\\}\\cap$ $\\Omega_{i}^{\\star}(\\{z_{j}\\})$ is bounded by ${\\sqrt{d}}{\\mathcal{L}}(R)$ in the interval $[b,b+\\textstyle{\\frac{1}{2}}]$ . ", "page_idx": 31}, {"type": "text", "text": "Proof First, we observe that the support of $Z_{i}\\mid\\{\\operatorname*{max}_{j\\neq i}z_{j}^{\\top}\\theta^{\\star}\\leq H\\}\\cap\\Omega_{i}^{\\star}(\\{z_{j}\\})$ is form of interval $[b,H+1]$ . This holds from the definition of $\\Omega_{i}^{\\star}(\\{z_{j}\\}$ and our design of $A$ . Also this support has equal sections with direction $\\theta^{\\star}$ . This is straightforward from the condition that $A$ is a cylindrical set with the direction $\\theta^{\\star}$ and intersection with $\\bar{\\{\\boldsymbol{z}}}\\in\\mathbb{R}^{d}\\mid\\boldsymbol{z}^{\\top}\\boldsymbol{\\theta}^{\\star}\\geq\\boldsymbol{b}\\}$ also makes equal section with $\\theta^{\\star}$ . Since $b\\leq H$ , we can apply Lemma 8 we get wanted result. \u25a0 ", "page_idx": 31}, {"type": "text", "text": "3) Bounding maximum density by applying Corollary 5. Using the previous Lemma 5 and we can conclude that the maximum density is bounded by $3\\sqrt{d}\\mathcal{L}(R)$ in the interval $[b,b+\\textstyle{\\frac{1}{2}}]$ . Finally, we get the wanted result: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathbf{1}(\\operatorname*{max}_{j\\not=i}z_{j}^{\\top}\\theta^{\\star}\\leq Z_{i}\\leq\\operatorname*{max}_{j\\not=i}z_{j}^{\\top}\\theta^{\\star}+\\varepsilon\\mid\\{\\operatorname*{max}_{j\\not=i}z_{j}^{\\top}\\theta^{\\star}\\leq H\\}\\cap\\Omega_{i}(\\{z_{j}\\})]\\leq3\\sqrt{d}\\mathcal{L}(R)\\varepsilon\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "for any $\\varepsilon<\\frac{1}{2}$ ", "page_idx": 31}, {"type": "text", "text": "F Proofs of Results for Unbounded Contexts ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In this section, we prove our main theorems stated in Section 4.3. ", "page_idx": 31}, {"type": "text", "text": "F.1 Constructing Truncation Sets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "This section presents operations to construct truncation sets, which will be used in the proof of our main results. In the proof, we first truncate our contexts $\\mathbf{X}\\in\\mathbb{R}^{d K}$ to a truncation set $\\begin{array}{r}{\\bar{\\mathbf{C}}=\\prod_{i=1}^{K}D}\\end{array}$ , where $D\\subset\\mathbb{R}^{d}$ , and work with the truncated contexts. If the supremum norm of the set is bounded, by combining it with LAC, we can bound the decay rate of the truncated contexts. ", "page_idx": 31}, {"type": "text", "text": "First, we define the directional completion of $v$ . For a set $A$ and a vector $v$ , we define the following process: the fliling of $A$ in the direction of $v$ . This process expands $A$ so that its cross-section remains the same when cut by a hyperplane orthogonal to $v$ . This procedure ensures that all sections of $A$ along the direction $v$ are equal. Recall that $\\pi_{S}(A)$ denotes the projection of $A$ onto the subspace $S$ . If $S$ is the subspace spanned by a vector $v$ , we write $\\pi_{v}(A)$ , which is a subset of a straight line. ", "page_idx": 31}, {"type": "text", "text": "Definition 12 (Set completion) For $A\\,\\subset\\,\\mathbb{R}^{d}$ and $\\boldsymbol{v}\\in\\mathbb{S}^{d-1}$ , We define ${\\mathcal{C}}[A,v]\\;:=\\;\\pi_{\\langle v\\rangle^{\\perp}}(A)\\;+$ $v\\pi_{v}(A)$ ", "page_idx": 31}, {"type": "text", "text": "After the completion, the section sliced by the normal hyperplane of $v$ is the same. ", "page_idx": 31}, {"type": "image", "img_path": "rblaF2euXQ/tmp/025073017cdc2a59372fc5d048e5d459c856c5f01892ca5c0c51f24df20a84b6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "Figure 7: Illustration of ${\\mathcal{C}}(A,v)$ process. This is the operation of fliling the area $A$ in the $v$ direction.   \nThen, the whole sections with direction v become equal. ", "page_idx": 32}, {"type": "text", "text": "Lemma 11 Suppose a set $A\\in\\mathbb{R}^{d}$ and unit vector $v$ satisfies $\\pi_{v}(A)$ is an interval with length \u2113. Then we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\|C[A,v]\\|_{\\infty}\\leq\\|A\\|_{\\infty}+\\ell.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof For all $p\\in\\mathcal{C}[A,v]$ , there exists $q\\in A$ such that $p=q+h v$ for $h<\\ell$ . Then ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\|p\\|_{\\infty}\\leq\\|q\\|_{\\infty}+\\|h v\\|_{\\infty}\\leq\\|A\\|_{\\infty}+\\ell\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "holds. ", "page_idx": 32}, {"type": "text", "text": "Next, we define partial completion, which makes the section with direction $v$ in the area of ${\\mathcal{C}}(A,v)\\cap$ $[-1,1]_{v}$ equal. ", "page_idx": 32}, {"type": "text", "text": "Definition 13 (Partial completion) For a set $A\\,\\subset\\,\\mathbb{R}^{d}$ and a unit vector $\\boldsymbol{v}\\in\\mathbb{R}^{d}$ , we define the completing operator as ", "page_idx": 32}, {"type": "equation", "text": "$$\n{\\mathcal{P}}(A,v):={\\mathcal{C}}[A\\cap\\{x\\mid|x\\cdot v|\\leq1\\},v]\\cup A\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Recall that $\\pi_{v}(\\cdot)$ is a projection to the direction $v$ . ", "page_idx": 32}, {"type": "image", "img_path": "rblaF2euXQ/tmp/16161bbc9d9171b25271aab4811d9f180b49e29860f501551d04f0da94d6512f.jpg", "img_caption": ["Figure 8: Illustration of $\\mathcal{P}(A,v)$ . "], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "$\\mathcal{P}(A,v)$ has same section with the direction $v$ with the area $[-1,1]_{v}$ . Equivalently, ", "page_idx": 32}, {"type": "equation", "text": "$$\n{\\mathcal{P}}(A,v)\\cap\\{x\\mid x^{\\top}v=u_{1}\\}\\equiv{\\mathcal{P}}(A,v)\\cap\\{x\\mid x^{\\top}v=u_{2}\\}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "for every $u_{1}\\neq u_{2}\\in[-1,1]$ . The partial completion operator has a bounded sup-norm, and the following lemma shows the result. ", "page_idx": 32}, {"type": "text", "text": "Lemma 12 (Sup norm bound of $\\mathcal{P}(A,v)$ ) For any $A\\subset\\ensuremath{\\mathbb{R}}^{d}$ and $v\\in\\mathbb{S}^{d-1}$ , ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\|\\mathcal{P}(A,v)\\|_{\\infty}\\leq\\|A\\|_{\\infty}+2\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "holds. ", "page_idx": 33}, {"type": "text", "text": "Proof For any $y\\,\\in\\,{\\mathcal{P}}(A,v)$ , there is $y^{\\prime}\\in A$ such that $y\\;=\\;y^{\\prime}\\,+\\,t v$ for $|t|\\,\\leq\\,2$ . Then, it is straightforward by taking the sup norm. ", "page_idx": 33}, {"type": "text", "text": "F.2 Proof of Theorem 2 ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Our goal is to prove ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{E}[X_{a_{\\theta}(\\mathbf{X}(t))}(t)X_{a_{\\theta}(\\mathbf{X}(t))}(t)^{\\top}]\\geq\\lambda_{\\star}(t)>0\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "for any $t$ and $\\theta$ . Then, we fix arbitrary history $\\mathcal{H}_{t-1}$ and fix any $\\theta\\,\\in\\,\\mathbb S^{d}$ . For simplicity, define $(X_{1}^{\\top}\\,.\\,.\\,X_{K}^{\\top})$ as the conditioned random variable of $(X_{1}(t)^{\\top}\\ldots\\dot{X}_{K}(t)^{\\top})\\mid\\mathcal{H}_{t-1}$ . The condition of Theorem 2 means that $\\mathbf{X}=(X_{1}^{\\top}\\ldots X_{K}^{\\top})$ satisfies boundedness Assumption 2 and LAC condition. Under conditioning the history $\\mathcal{H}_{t-1}$ , we aim to apply the Proposition 2 for any $\\theta$ and $v$ . We also set $a=\\arg\\operatorname*{max}X_{i}^{\\top}{\\bar{\\theta}}$ and for any fixed $v\\in\\mathbb{R}^{d},\\|v\\|^{\\cdot}{=}\\,\\mathrm{\\bar{1}}$ , our goal is to calculate the lower bound of ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{E}[v^{\\top}X_{a}X_{a}v^{\\top}].\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We can view it as the fixed history random variable and we aim to use the arguments developed in Section E. ", "page_idx": 33}, {"type": "text", "text": "To use Proposition 2, we first truncate our contexts $(X_{1}^{\\top},\\bot...X_{K}^{\\top})$ into some region ${\\bf C}_{1}^{v}$ and then apply Proposition 2 to the truncated contexts. To meet the geometric conditions of Proposition 2, we do the truncation. ", "page_idx": 33}, {"type": "text", "text": "F.2.1 Constructing Truncation Sets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "We first define the truncation set rigorously. ", "page_idx": 33}, {"type": "text", "text": "1. Define $R_{1}=c_{0}x_{\\mathrm{max}}(2+\\log d K)$ .   \n2. Define $D:=[-R_{1},R_{1}]^{d}$ .   \n3. Define $D^{v}:=\\mathcal{P}[D,v]$ .   \n4. Set truncation set $\\mathbf{C}_{1}^{v}:=(D^{v})^{K}$ .   \n5. Then $\\|\\mathbf{C}_{1}^{v}\\|_{\\infty}\\le R_{1}+2$ holds. (by Lemma 12) ", "page_idx": 33}, {"type": "image", "img_path": "rblaF2euXQ/tmp/d59715bdf823c55c9cec2f2f9f8df6204da8ad435a9165c26edc034fd8c7041a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "Figure 9: Illustration of the set $D^{v}$ . The red rectangle is $D$ and $D^{v}$ is union with green boundary rectangle and $D$ . We force to make equal sections with direction $v$ in $[-1,1]_{v}$ . Hence we can apply the previous results from Proposition 2! ", "page_idx": 34}, {"type": "text", "text": "Claim 4 For any X with Assumption 2, $\\mathbb{P}[\\mathbf{X}\\in\\mathbf{C}_{1}^{v}]\\ge\\frac{1}{2}$ . ", "page_idx": 34}, {"type": "text", "text": "Proof First, we prove $\\mathbb{P}[\\mathbf{X}\\in D^{K}]\\geq{\\frac{1}{2}}$ . If we prove that, since $\\mathbf{C}_{1}^{v}\\supset D^{K}$ , we get the result. Using equation 16, we get ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{P}[{\\mathbf X}\\in(D^{K})^{c}]\\leq\\displaystyle\\sum_{i=1}^{K}\\mathbb{P}[X_{i}\\in D^{c}]}\\\\ {\\displaystyle\\leq\\sum_{i=1}^{K}\\sum_{j=1}^{d}\\mathbb{P}[|X_{i j}|>R_{1}]}\\\\ {\\displaystyle\\leq d K\\times\\frac{1}{2d K}=\\frac{1}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "In the second inequality, we use the Assumption 2 and sub-exponential context concentration results from Appendix I.2. ", "page_idx": 34}, {"type": "text", "text": "Next, we define the truncated contexts $\\mathbf{W}=\\mathbf{X}\\mid\\{\\mathbf{X}\\in\\mathbf{C}_{1}^{v}\\}:=(W_{1}^{\\top},\\ldots W_{K}^{\\top}).$ ", "page_idx": 34}, {"type": "text", "text": "Claim 5 The truncated contexts $\\mathbf{W}=(W_{1}^{\\top}\\cdot\\cdot\\cdot W_{K}^{\\top})$ and $W_{a}$ satisfies ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{E}[v^{\\top}X_{a}X_{a}v]\\ge\\frac{1}{2}\\mathbb{E}[v^{\\top}W_{a}(W_{a})^{\\top}v].\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Note that $a$ is defined as $a=\\arg\\operatorname*{max}_{i\\in[K]}X_{i}^{\\top}\\theta$ . ", "page_idx": 34}, {"type": "text", "text": "Proof First, observe that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[v^{\\top}X_{a}X_{a}^{\\top}v]\\ge\\mathbb{E}[v^{\\top}X_{a}X_{a}^{\\top}v\\cap\\{\\mathbf{X}=\\mathbf{W}\\}]}\\\\ &{\\phantom{\\sum}+\\mathbb{E}[v^{\\top}X_{a}X_{a}^{\\top}v\\cap\\{\\mathbf{X}\\neq\\mathbf{W}\\}]}\\\\ &{\\phantom{\\sum}\\ge\\mathbb{E}[v^{\\top}X_{a}X_{a}^{\\top}v\\cap\\{\\mathbf{X}=\\mathbf{W}\\}]}\\\\ &{\\phantom{\\sum}\\ge\\mathbb{E}[v^{\\top}W_{a}W_{a}^{\\top}v\\mid\\{\\mathbf{X}=\\mathbf{W}\\}]\\mathbb{P}[\\mathbf{X}=\\mathbf{W}]}\\\\ &{\\phantom{\\sum}\\ge\\frac{1}{2}\\mathbb{E}[v^{\\top}W_{a}W_{a}^{\\top}v]}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "F.2.2 Properties of Truncated Contexts W ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "We need to check two conditions: The sections of $\\operatorname{supp}(W_{i})$ with direction $v$ in $[-1,1]_{v}$ should be the same and have a bounded decay rate $\\sqrt{d}\\mathcal{L}(R_{1}+2)$ . Since $\\mathbf{X}$ satisfies LAC with functions $\\mathcal{L}$ and $\\|\\mathbf{C}_{1}\\|_{\\infty}\\leq R_{1}+2$ by the Lemma 12, it has a bounded decay rate of $\\sqrt{d}\\mathcal{L}(R_{1}+2)$ by Lemma 3. ", "page_idx": 35}, {"type": "text", "text": "Claim 6 The truncated contexts W have LAC with $\\mathcal{L}(\\cdot)$ and $\\|\\operatorname{supp}(\\mathbf{W})\\|_{\\infty}\\leq R_{1}+2.$ ", "page_idx": 35}, {"type": "text", "text": "Proof By the observation of Appendix B.3 for the relation of the conditioning and LAC, it is clear that $\\mathbf{W}$ has LAC with function $\\mathcal{L}$ . Also, by the construction of $\\mathbf{C}_{1}$ and Lemma 11, $\\|\\operatorname{supp}(\\mathbf{W})\\|_{\\infty}\\leq R_{1}+2$ is true. ", "page_idx": 35}, {"type": "text", "text": "Claim 7 For every $i\\,\\in\\,[K],$ , $\\operatorname{supp}(W_{i})$ is identical for all $i\\,\\in\\,[K]$ and $\\operatorname{supp}(W_{i})\\cap[-1,1]_{v}$ has equal sections with direction $v$ . ", "page_idx": 35}, {"type": "text", "text": "Proof Since we set $D^{v}$ as the partial completion with direction $v$ , it is straightforward. ", "page_idx": 35}, {"type": "text", "text": "F.2.3 Applying Proposition 2 ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "We already checked that our truncated contexts W fulfill the condition of the Proposition 2. Hence, we apply Proposition 2 and we get the bound, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{E}[W_{a}W_{a}^{\\top}]\\succeq\\frac{c}{d(A_{1}+A_{2}(R_{1}+2)^{\\alpha})^{2}}I_{d}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "hence we get ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{E}[X_{a}X_{a}^{\\top}]\\succeq\\frac{c}{d(A_{1}+A_{2}(R_{1}+2)^{\\alpha})^{2}}I_{d}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "for some absolute constant $c>0$ . ", "page_idx": 35}, {"type": "text", "text": "F.3 Proof of Theorem 3 ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "We aim to use the result of Proposition 3. We first consider the case $\\lVert\\theta^{\\star}\\rVert_{2}=1$ , and for general case, using the same argument from Appendix E.4 and (10), we can adjust it. We first truncate our contexts $\\mathbf{X}=(X_{1}^{\\top}\\ldots{X}_{K}^{\\top})$ to some region $\\mathbf{C}_{2}=D^{K}$ , $D\\subset\\mathbb{R}^{d}$ , with slight abusing of notations. We define the truncated contexts as $\\mathbf{W}=(W_{1}^{\\top},\\dots W_{K}^{\\top})$ . Note that the support of $W_{i}$ is $D$ . To satisfy the conditions of Proposition 3, we want to make the sections with direction $\\theta^{\\star}$ of the support of $W_{i}$ the same. ", "page_idx": 35}, {"type": "text", "text": "F.3.1 Constructing Truncation Sets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "We construct the truncation set $\\mathbf{C}_{2}$ as follows. Firstly, set $R_{3}=c_{0}x_{\\mathrm{max}}(1+\\log d K+{\\textstyle{\\frac{1}{2}}}\\log T)$ and $R_{2}=R_{3}+1$ . ", "page_idx": 35}, {"type": "text", "text": "\u2022 $D_{1}:=([-R_{2},R_{2}])^{d}\\subset\\mathbb{R}^{d}.$ .   \n\u2022 Se $D_{2}:=\\{x\\in\\mathbb{R}^{d}\\mid-R_{2}\\leq x^{\\top}\\theta^{\\star}\\leq R_{2}\\}\\subset\\mathbb{R}^{d}$ \u2022 Set $D_{3}:=D_{1}\\cap D_{2}$ .   \n\u2022 Set $D:=\\mathcal{C}[D_{3},\\theta^{\\star}]$ .   \n\u2022 The final truncation set is constructed by $\\mathbf{C}_{2}=D^{K}$ ", "page_idx": 35}, {"type": "text", "text": "Claim 8 The supnorm of the truncation set satisfies $\\|\\operatorname{supp}(\\mathbf{W})\\|_{\\infty}=\\|D\\|_{\\infty}\\leq3R_{2}$ and the W has bounded LAC with $\\mathcal{L}(3R_{2})$ . ", "page_idx": 36}, {"type": "text", "text": "Proof The LAC of W has the same function as the original, $\\mathcal{L}(\\cdot)$ by Lemma 1. By Lemma 11, we easily see that $\\|\\operatorname{supp}(\\mathbf{W})\\|_{\\infty}\\leq3R_{2}$ since $\\pi_{\\theta^{\\star}}(D_{2})$ is an interval with length $2R_{2}$ . Since $\\mathcal{L}(\\cdot)$ is an increasing function (by definition) and $\\|\\operatorname{supp}(\\mathbf{W})\\|_{\\infty}\\leq3R_{2}$ , $\\mathbf{W}$ has a bounded LAC with $\\mathcal{L}(3R_{2})$ . ", "page_idx": 36}, {"type": "text", "text": "Claim 9 The region $\\mathbf{C}_{2}$ has equal sections with direction $\\theta^{\\star}$ and $\\begin{array}{r}{\\mathbb{P}[\\mathbf{X}\\in\\mathbf{C}_{2}]\\geq1-\\frac{1}{\\sqrt{T}}.}\\end{array}$ . ", "page_idx": 36}, {"type": "text", "text": "Proof By the moment Assumption 2, $\\begin{array}{r}{\\mathbb{P}[|X_{i j}|\\le R_{2}]\\ge1-\\frac{1}{2d K\\sqrt{T}}}\\end{array}$ holds for all $i,j$ and therefore $\\begin{array}{r}{\\mathbb{P}[\\mathbf{X}\\in D_{1}^{K}]\\geq1-\\frac{1}{2\\sqrt{T}}}\\end{array}$ holds. Next, by Assumption 2, we get $\\begin{array}{r}{\\operatorname{\\dot{P}}[\\mathbf{X}\\in D_{2}^{K}]\\geq1-\\frac{1}{2\\sqrt{T}}}\\end{array}$ . Hence $\\begin{array}{r}{\\mathbb{P}[\\mathbf{X}\\in D_{3}^{K}]\\ge1-\\frac{1}{\\sqrt{T}}}\\end{array}$ and $D\\supset D_{3}$ , we get $\\begin{array}{r}{\\mathbb{P}[\\mathbf{X}\\in\\mathbf{C}_{2}]\\geq1-\\frac{1}{\\sqrt{T}}}\\end{array}$ . ", "page_idx": 36}, {"type": "text", "text": "F.3.2 Truncation with High-probability Region ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "We truncate $\\mathbf{X}$ into the high probability confidence region $\\mathbf{C}_{2}$ and define the truncated contexts as $\\mathbf{W}=(W_{1}^{\\top}\\cdot\\cdot\\cdot W_{K}^{\\top})$ and play with $\\dot{\\mathbf{W}}=(W_{1}^{\\top},\\dots W_{K}^{\\top})$ from now on. The following calculation shows that it is enough to bound the suboptimality gap of $\\mathbf{W}$ . ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{P}[\\Delta(\\mathbf{X})\\leq\\varepsilon]=\\displaystyle\\int_{\\mathbb{R}^{k}}\\mathbb{I}_{\\Delta(\\mathbf{x})\\leq\\varepsilon}\\mathbf{f}(\\mathbf{x})d\\mathbf{x}}&{}\\\\ {=\\displaystyle\\int_{\\mathbf{C}_{2}}\\mathbb{I}_{\\Delta(\\mathbf{x})\\leq\\varepsilon}\\mathbf{f}(\\mathbf{x})d\\mathbf{x}+\\int_{\\mathbf{C}_{2}^{k}}I_{\\Delta(\\mathbf{x})\\leq\\varepsilon}\\mathbf{f}(\\mathbf{x})d\\mathbf{x}}\\\\ {\\leq\\displaystyle\\int_{\\mathbf{C}_{2}}\\mathbb{I}_{\\Delta(\\mathbf{x})\\leq\\varepsilon}\\mathbf{f}(\\mathbf{x})d\\mathbf{x}+\\mathbb{P}[\\mathbf{X}\\in\\mathbf{C}_{2}^{\\ast}]}\\\\ {=\\mathbb{P}[\\mathbf{X}\\in\\mathbf{C}_{2}]\\displaystyle\\int_{\\mathbf{C}_{2}}\\mathbb{I}_{\\Delta(\\mathbf{x})\\leq\\varepsilon}\\frac{\\mathbf{f}(\\mathbf{x})}{\\mathbb{P}[\\mathbf{X}\\in\\mathbf{C}_{2}]}d\\mathbf{x}+\\frac{1}{\\sqrt{T}}}\\\\ {=\\mathbb{P}[\\mathbf{X}\\in\\mathbf{C}_{2}]\\mathbb{P}[\\Delta(\\mathbf{W})\\leq\\varepsilon]+\\frac{1}{\\sqrt{T}}}\\\\ {\\leq\\mathbb{P}[\\Delta(\\mathbf{W})\\leq\\varepsilon]+\\frac{1}{\\sqrt{T}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Therefore, if we find the constant $C_{\\Delta}^{\\prime}$ satisfying ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbb{P}[\\Delta(\\mathbf{W})\\leq\\varepsilon]\\leq C_{\\Delta}^{\\prime}\\varepsilon,\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "we finally can bound $C_{\\Delta}\\leq C_{\\Delta}^{\\prime}$ . ", "page_idx": 36}, {"type": "text", "text": "Lemma 13 The truncated contexts $\\mathbf{W}=\\mathbf{X}\\mid\\{\\mathbf{X}\\in\\mathbf{C}_{2}\\}$ meet the condition of Proposition 3 with H = R2 + 1 and \u03b4 =\u221a1T . ", "page_idx": 36}, {"type": "text", "text": "Proof Let $D=\\operatorname{cyl}(B,\\theta^{\\star},R_{2})$ for some set $B\\subset P_{\\theta^{\\star}}$ and define $D^{\\prime}=\\mathrm{cyl}(B,\\theta^{\\star},R_{2}-1)$ . We can show that $\\begin{array}{r}{\\mathbb{P}[\\mathbf{X}\\in D^{\\prime}]\\geq1-\\frac{1}{\\sqrt{T}}}\\end{array}$ with exactly same argument from Claim 9. Hence we can calculate ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{P}[\\mathbf{W}\\in D^{\\prime}]\\ge\\mathbb{P}[\\mathbf{X}\\in D^{\\prime}]/\\mathbb{P}[\\mathbf{X}\\in D]}\\\\ {\\ge\\displaystyle\\frac{1-\\frac{1}{\\sqrt{T}}}{1}=1-\\frac{1}{\\sqrt{T}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "which means that it satisfies the condition of Proposition 3 with \u03b4 =\u221a1 . ", "page_idx": 36}, {"type": "text", "text": "F.3.3 Applying Proposition 3 ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Finally, we aim to apply Proposition 3, which bounds the margin constant. Then, by Proposition 3, we can get the equation (13) holds for ", "page_idx": 37}, {"type": "equation", "text": "$$\nC_{\\Delta}^{\\prime}=3\\sqrt{d}\\mathcal{L}(R_{2})=3\\sqrt{d}(A_{1}+A_{2}(3R_{2})^{\\alpha}).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Then, finally, we get ", "page_idx": 37}, {"type": "equation", "text": "$$\nC_{\\Delta}\\leq3\\sqrt{d}(A_{1}+A_{2}(3R_{2})^{\\alpha})=\\widetilde{\\mathcal{O}}(\\sqrt{d}).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "F.4 Proof of Theorem 1: Unbounded Contexts Case ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "We combine the previous Proposition 9 and our result of Theorem 2 and 3. Then we get ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{Reg}(T)\\leq c C_{\\Delta}d x_{\\operatorname*{max}}^{2}\\cfrac{1}{\\lambda_{\\star}}(\\log T)^{4}}\\\\ &{\\qquad\\qquad\\leq c C_{\\Delta}d^{2.5}x_{\\operatorname*{max}}^{2}(A_{1}+A_{2}(R_{1}+2)^{\\alpha})^{2}(A_{1}+A_{2}2^{\\alpha}R_{2}^{\\alpha})(\\log T)^{4}}\\\\ &{\\qquad\\qquad\\leq\\widetilde{O}(d^{2.5})}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "holds. ", "page_idx": 37}, {"type": "text", "text": "G Results for Bounded Contexts ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Now, we present our main result for bounded contexts. ", "page_idx": 37}, {"type": "text", "text": "G.1 Two Cases of Bounded Contexts ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "In the linear contextual bandit setting, $\\ell_{2}$ boundedness is widely used. However, for light-tailed distributions (such as Gaussian or exponential), the $\\ell_{2}$ norm is unbounded. Therefore, we divide our analysis into two cases: unbounded and bounded contexts. While our previous focus was on unbounded contexts, here we summarize and present the results for bounded contexts. ", "page_idx": 37}, {"type": "text", "text": "We classify the analysis into two cases: the first is for truncated contexts, and the second is for naturally bounded contexts, which include cases where the uniform distribution on the ball $\\mathbb{B}_{R}$ or a distribution with bounded density on the ball is used. ", "page_idx": 37}, {"type": "text", "text": "1) Truncated contexts. Truncated contexts refer to cases where the context distribution is a truncated version of the original distribution. For example, truncated Gaussian and truncated exponential distributions fall into this category. ", "page_idx": 37}, {"type": "text", "text": "2) Naturally bounded contexts. Naturally bounded contexts include uniform distributions on a ball or distributions with bounded density defined on the ball. ", "page_idx": 37}, {"type": "text", "text": "G.2 Regret Bounds for Bounded Contexts ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Now, we present our result for the regret bound. The first case is when we receive the truncated contexts, generated by truncating the unbounded contexts to $(\\mathbb{B}_{R})^{K}$ where $\\mathbb{B}_{R}$ is $d$ -dimensional ball with radius $R$ . For the $\\ell_{2}$ -bounded case, $R$ may depend on the dimension $d$ when we choose a large $R$ , so we do not hide the $R$ term in our main regret bound result. Corollary 1 and 2 gives the regret bound for truncated contexts. Corollary 3 gives the regret bound for naturally bounded contexts. ", "page_idx": 37}, {"type": "text", "text": "Corollary 1 (Regret bound: truncated contexts) Let $\\mathbf{X}(t)$ be unbounded contexts with the same condition as Theorem 1. For $R^{\\prime}>0$ with $\\mathbb{P}[\\mathbf{X}(t)\\in(\\mathbb{B}_{R^{\\prime}})^{K}]=p>0,$ , we receive truncated contexts $\\mathbf{X}(t)\\mid(\\mathbb{B}_{R^{\\prime}+r})^{K}$ for some $r\\,>\\,0$ each round $t\\geq1$ . In this case, for $R=R^{\\prime}+r$ , the regret of Algorithm $^{\\,l}$ is bounded by ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbf{Reg}(T)\\leq\\widetilde{O}(d^{2.5}R^{2}\\mathscr{L}(R)^{2}\\frac{1}{p}).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "when $r\\asymp R_{1}$ . ", "page_idx": 37}, {"type": "text", "text": "Corollary 2 (Regret bound: high-prob truncated contexts) Let $\\mathbf{X}(t)$ be unbounded contexts with the same condition as Theorem 1. We receive truncated contexts $\\mathbf{X}(t)\\mid(\\mathbb{B}_{L})^{K}$ for $L\\gtrsim\\sqrt{d}x_{\\operatorname*{max}}(1+$ $\\log d+\\log K)$ each round $t$ . In this case, regret of Algorithm $^{\\,I}$ is bounded by ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbf{Reg}(T)\\leq{\\widetilde{O}}(d^{2.5}).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Discussion. For Corollary 1, when we receive truncated light-tail distributions (e.g. Gaussian, exponential, Laplace), this theorem states that they enjoy a logarithmic regret bound. It has a poly $(\\log T)$ regret bound for $T$ and it has a $R$ (truncation radius) dependency. Corollary 2 states that when we choose a sufficiently large truncation radius, it has a radius-free regret bound and it has the same scale with the unbounded case, Theorem 1. Proofs for the above corollaries are presented in Appendix H. ", "page_idx": 38}, {"type": "text", "text": "Next, we introduce the regret bound results for naturally bounded contexts. Here, new parameters $c_{\\star}$ and $p_{\\star}$ are involved, along with related new Condition 1. This condition will be discussed in the following Appendix G.3, where we clarify that both uniform distributions and bounded density distributions satisfy them. We will provide detailed explanations accordingly and also state the range of $c_{\\star},p_{\\star}$ for several distributions containing uniform distribution. ", "page_idx": 38}, {"type": "text", "text": "Corollary 3 (Regret bound: naturally bounded contexts) Let naturally bounded contexts $\\mathbf{X}(t)\\in$ $(\\mathbb{B}_{R})^{K}$ satisfy LAC condition with function $\\mathcal{L}(\\cdot)$ and Condition 1 holds with concentration parameters $c_{\\star},p_{\\star}$ each round $t$ . Under the Assumption $^{\\,I}$ and 2, the regret of Algorithm $^{\\,l}$ is bounded by ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbf{Reg}(T)\\leq\\widetilde O(K d^{2.5}R^{2}\\mathscr{L}(R)^{3}\\frac{1}{p_{\\star}(1-c_{\\star})^{2}}).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Discussion. Corollary 3 works for uniform distribution and any distribution with upper bounded density. We present the related result in Appendix G.3. It also contains truncation of heavy-tail distributions. For Corollary 3, the polynomial dependency of $K$ is due to boundedness, and it matches the asymptotic result of the uniform distribution studied in [12] when $d\\,=\\,1$ . Combining with Lemma 15, for uniform distribution or bounded density distribution in the ball, we have a regret bound $\\mathbf{Reg}(T)\\leq\\widetilde{\\mathcal{O}}(K^{\\frac{d+5}{d+1}}d^{2.5})$ . Proofs for the above corollaries are presented in Appendix $\\mathrm{H}$ . ", "page_idx": 38}, {"type": "text", "text": "Comparison with known results. For truncated contexts, to the best of our knowledge, there are no known results for greedy bandits. For naturally bounded contexts, Oh et al. [28] studied the case of a uniform distribution on the sphere, where each context $X_{i}$ for arm $i$ is independent across arms and only considered with the case $K\\leq d$ . That work considers the minimum eigenvalue of the context covariance matrix as a constant; however, for a uniform distribution, it scales $\\mathrm{as}\\asymp\\frac{1}{d}$ . Taking this into account, their regret bound is $\\widetilde{\\mathcal{O}}(d^{3}\\sqrt{T})$ when $K\\leq d$ . When $K\\leq d$ , our result has bound $\\widetilde{\\mathcal{O}}(d^{2.5+\\frac{d+5}{d+1}})$ for uniform distribution.  Moreover, for the multi-parameter shared context setup, Bastani and Bayati [7], Bastani et al. [8] studied the uniform context case, and their worst case regret bound is also $\\tilde{\\mathcal{O}}(K^{4}d^{4})$ , considering that the minimum eigenvalue of the context covariance scales as $\\textstyle{\\frac{1}{d}}$ . In conclus ion, our result is the sharpest among known previous results. ", "page_idx": 38}, {"type": "text", "text": "G.3 Concentration Parameters for Bounded Contexts ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "We introduce a new condition that defines two concentration parameters, which measure sufficient concentration for bounded contexts. We will show that truncated contexts satisfy this condition, as do naturally bounded contexts with bounded density. This condition does not need to be assumed for truncated contexts, as it is automatically satisfied. ", "page_idx": 38}, {"type": "text", "text": "Condition 1 (Concentration parameters for bounded contexts) For the random vectors (contexts) $\\mathbf{X}\\,=\\,(X_{1}^{\\top}\\,.\\,.\\,.\\,X_{K}^{\\top})\\,\\in\\,\\mathbf{\\bar{\\mathbb{R}}}^{d K}$ where $X_{i}\\ \\in\\ \\mathbb{B}_{R},$ , there exist $0\\,<\\,c_{\\star}\\,<\\,1$ and $0\\,<\\,p_{\\star}\\,<\\,1$ such that for every $\\eta$ with $\\|\\eta\\|_{2}=1$ , $\\mathbb{P}[\\operatorname*{max}_{i\\in[K]}X_{i}^{\\top}\\eta\\leq c_{\\star}R]\\geq p_{\\star}$ . We call $p_{\\star},c_{\\star}$ concentration parameters. ", "page_idx": 38}, {"type": "text", "text": "Discussion. Two parameters, $c_{\\star}$ and $p_{\\star}$ , must exist if $\\mathbf{X}$ is random. In the next part, we discuss $1-c_{\\star}$ , $p_{\\star}\\asymp1$ for truncated contexts that truncated into the positive probability region. Additionally, we will show that the uniform distribution within the ball, as well as any distribution within the ball with bounded density, also satisfies this condition, and we explicitly calculate the range of these two parameters. For our regret bound, we have a dependency on(1\u2212pc\u22c6\u22c6)2 . ", "page_idx": 38}, {"type": "text", "text": "Example: truncated contexts. The next lemma states that the truncated contexts satisfy Condition 1 has parameters $1-c_{\\star},p_{\\star}\\asymp1$ . If contexts are generated by truncating from the original distribution and the truncated region is a positive probability region, then the parameters in Condition $1\\;c_{\\star},p_{\\star}$ are well-defined and do not harm the regret bound when we choose a sufficiently large truncation set. ", "page_idx": 39}, {"type": "text", "text": "Lemma 14 (Concentration parameters for truncated contexts) Suppose the unbounded contexts $\\mathbf{X}\\in\\mathbb{R}^{d K}$ satisfy ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathbb{P}[\\mathbf{X}\\in(\\mathbb{B}_{R^{\\prime}})^{K}]=p>0,\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "for some $R^{\\prime}\\,>\\,0$ . Then if we truncate each $X_{i}$ to $\\mathbb{B}_{R^{\\prime}+r}$ for any $r\\,>\\,0,i\\,\\in\\,[K]$ , the truncated contexts X = (X1\u22a4 , . . . X\u22a4K) satisfy Condition 1 with p\u22c6= p, c\u22c6=R\u2032R+\u2032r. ", "page_idx": 39}, {"type": "text", "text": "Proof Recall that we truncate $\\mathbf{X}$ to $(\\mathbb{B}_{R^{\\prime}+r})^{K}$ . If $\\mathbb{P}[\\mathbf{X}\\in(\\mathbb{B}_{R^{\\prime}})^{K}]=p>0$ and truncate to $\\mathbb{B}_{R^{\\prime}+r}$ , ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}[\\underset{i\\in[K]}{\\operatorname*{max}}\\,\\overline{{X}}^{\\top}\\eta\\leq R^{\\prime}]=\\frac{\\mathbb{P}[\\{\\operatorname*{max}\\,X_{i}^{\\top}\\eta\\leq R^{\\prime}\\}\\cap\\big\\{\\mathbf{X}\\in(\\mathbb{B}_{R^{\\prime}+r})^{K}\\}]}{\\mathbb{P}[\\mathbf{X}\\in(\\mathbb{B}_{R^{\\prime}+r})^{K}]}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\geq\\frac{\\mathbb{P}[\\mathbf{X}\\in(\\mathbb{B}_{R^{\\prime}})^{K}]}{\\mathbb{P}[\\mathbf{X}\\in(\\mathbb{B}_{R^{\\prime}+r})^{K}]}}\\\\ &{\\qquad\\qquad\\qquad\\geq\\mathbb{P}[\\mathbf{X}\\in(\\mathbb{B}_{R^{\\prime}+r})^{K}]}\\\\ &{\\qquad\\qquad\\qquad\\geq\\mathbb{P}[\\mathbf{X}\\in(\\mathbb{B}_{R^{\\prime}})^{K}]}\\\\ &{\\qquad\\qquad=p.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Therefore, we can set $p_{\\star}=p$ and $\\begin{array}{r}{c_{\\star}=\\frac{R^{\\prime}}{R^{\\prime}+r}}\\end{array}$ . ", "page_idx": 39}, {"type": "text", "text": "Example: Bounded Density Distributions The below lemma tells us that a uniform distribution or a bounded density distribution defined in the ball $\\mathbb{B}_{R}$ satisfies Condition 1 and presents the range of two parameters. Without loss of generality, we prove for distribution defined in unit ball, $\\mathbb{B}_{1}$ . ", "page_idx": 39}, {"type": "text", "text": "Lemma 15 Consider a random vector $X_{i}~\\in~\\mathbb{B}_{1},i~\\in~[K]$ with each $X_{i}$ \u2019s density being upper bounded with $\\frac{c_{u}}{\\omega_{d}}$ Here, $\\omega_{d}$ is a volume of $d$ -dimensional unit ball $\\mathbb{B}_{1}$ , recalling that the density of the uniform distribution in the ball $\\mathbb{B}_{1}$ has density $\\frac{1}{\\omega_{d}}$ . When $X_{1}=\\cdot\\cdot\\cdot=X_{K}$ (strongly correlated), then Condition 1 holds with ", "page_idx": 39}, {"type": "equation", "text": "$$\np_{\\star}=\\frac{1}{2},\\ 1-c_{\\star}\\gtrsim1.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "When $X_{1},\\allowbreak\\cdot\\cdot X_{K}$ are independent (not correlated), we have ", "page_idx": 39}, {"type": "equation", "text": "$$\np_{\\star}=\\frac{1}{2},\\ 1-c_{\\star}\\geq c K^{-\\frac{2}{d+1}}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Proof We use the result of the below Claim 10. ", "page_idx": 39}, {"type": "text", "text": "Correlated Case. For the case $X_{1}=X_{2}=...X_{K}$ , the $\\frac{1}{2}$ -quantile of $X_{i}^{\\top}\\eta$ say $c_{\\star}$ satisfies ", "page_idx": 39}, {"type": "equation", "text": "$$\n2(1-c_{\\star})\\geq(\\frac{1}{2c_{u}}\\frac{\\omega_{d}}{\\omega_{d-1}})^{\\frac{2}{d+1}}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "by the Claim 10. Since $\\begin{array}{r}{\\Big(\\frac{\\omega_{d}}{\\omega_{d-1}}\\Big)^{\\frac{2}{d+1}}\\asymp1}\\end{array}$ , we get ", "page_idx": 39}, {"type": "equation", "text": "$$\n1-c_{\\star}\\gtrsim1.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Independent Case. For the case $X_{1}^{\\top},\\ldots X_{K}^{\\top}$ are independent, for $c_{\\star}$ with ", "page_idx": 39}, {"type": "equation", "text": "$$\n2(1-c_{\\star})\\geq1-c_{\\star}^{2}=(\\frac{1}{K c_{u}}\\frac{\\omega_{d}}{\\omega_{d-1}})^{\\frac{2}{d+1}}\\asymp K^{-\\frac{2}{d+1}}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "satisfies $\\mathbb{P}[X_{i}^{\\top}\\eta\\leq c_{\\star}R]\\leq1-{\\frac{1}{K}}$ for all $i\\in[K]$ . Due to independence, we get $\\begin{array}{r}{p_{\\star}=(1-\\frac{1}{K})^{K}\\le\\frac{1}{2}}\\end{array}$ . ", "page_idx": 39}, {"type": "text", "text": "Claim 10 Consider random vector $X\\in\\mathbb{R}^{d}$ defined in $\\mathbb{B}_{1}$ with density upper bounded by $\\frac{c_{u}}{\\omega_{d}}$ . For any $\\eta\\in\\mathbb{S}^{d-1}$ , let $1-p$ quantile of $X^{\\top}\\eta$ as $\\alpha$ . We have ", "page_idx": 40}, {"type": "equation", "text": "$$\n2(1-\\alpha)\\ge1-\\alpha^{2}\\ge(\\frac{p}{c_{u}}\\frac{\\omega_{d}}{\\omega_{d-1}})^{\\frac{2}{d+1}}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Proof ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "We prove that $\\alpha$ with $\\begin{array}{r}{1-\\alpha^{2}=\\big(\\frac{p}{c_{u}}\\frac{\\omega_{d}}{\\omega_{d-1}}\\big)^{\\frac{2}{d+1}}}\\end{array}$ satisfies ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathbb{P}[X^{\\top}\\eta\\geq\\alpha]\\leq p.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Since the density of $X^{\\top}\\eta=r$ satisfies ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}[X^{\\top}\\eta=r]\\le\\omega_{d-1}(1-r^{2})^{\\frac{d-1}{2}}\\frac{c_{u}}{\\omega_{d}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "hence ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}[X^{\\top}\\eta\\geq\\alpha]\\leq\\displaystyle\\int_{\\alpha}^{1}\\omega_{d-1}(1-r^{2})^{\\frac{d-1}{2}}c_{u}\\frac{1}{\\omega_{d}}}\\\\ &{\\qquad\\qquad\\qquad\\leq c_{u}\\frac{\\omega_{d-1}}{\\omega_{d}}(1-\\alpha)(1-\\alpha^{2})^{\\frac{d-1}{2}}}\\\\ &{\\qquad\\qquad\\leq c_{u}\\frac{\\omega_{d-1}}{\\omega_{d}}(1-\\alpha^{2})^{\\frac{d+1}{2}}}\\\\ &{\\qquad\\qquad\\leq p}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "G.4 Results for Two Challenges: Bounded Contexts ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Now, we present results related to two challenges for bounded contexts. Proof are presented in Appendix H. ", "page_idx": 40}, {"type": "text", "text": "Proposition 4 (Diversity constant: naturally bounded contexts) Suppose $\\mathbf{X}(t)$ is the random vector supported in $(\\mathbb{B}_{R})^{K}$ , its density satisfies the $L A C$ condition with constant function $\\mathcal{L}(R)$ and has Condition $^{\\,l}$ with $p_{\\star},c_{\\star}$ . Then ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\lambda_{\\star}(t)\\geq c\\frac{p_{\\star}}{d}\\frac{1}{(\\mathcal{L}(R)+\\frac{1}{R(1-c_{\\star})})^{2}}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "for the absolute constant $c>0$ . ", "page_idx": 40}, {"type": "text", "text": "Proposition 5 (Diversity constant: high-prob truncated contexts) Suppose $\\mathbf{X}(t)~\\in~\\mathbb{R}^{d\\times K}$ satisfies LAC with $\\mathcal{L}(\\cdot)$ and Assumption 2. Then we truncate $\\mathbf{X}(t)$ to $(\\mathbb{B}_{L})^{K}$ for some $\\textit{L}\\geq$ $\\begin{array}{r}{c\\sqrt{d}x_{\\mathrm{max}}(1+\\log(\\frac{d K x_{\\mathrm{max}}}{\\lambda_{\\star}}))}\\end{array}$ and define this truncated contexts as $\\overline{{\\mathbf{X}}}(t)=(\\overline{{X}}_{1}(t)\\ldots\\overline{{X}}_{K}(t))$ . Then, it has diversity constant (Challenge $^{\\,I}$ ) with $\\textstyle{\\frac{1}{2}}\\lambda_{\\star}(t)$ , where $\\lambda_{\\star}(t)$ is a diversity constant of $\\mathbf{X}(t)$ . ", "page_idx": 40}, {"type": "text", "text": "Proposition 6 (Suboptimality gap: truncated contexts) Let $\\overline{{\\mathbf{X}}}(t)$ be a truncated random variable of $\\bar{\\mathbf X}(t)$ into $(\\mathbb{B}_{R})^{K}$ with $\\mathbb{P}[\\mathbf{X}\\in(\\bar{\\mathbb{B}}_{R})^{K}]\\geq1-\\delta$ for some $\\delta>0$ . Let $C_{\\Delta}(t)$ is a margin constant of the contexts before truncation. Then margin constant of truncated contexts $\\overline{{\\mathbf{X}}}(t)$ , say $\\overline{{C}}_{\\Delta}(t)$ satisfies ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\overline{{C}}_{\\Delta}(t)\\leq\\frac{1}{1-\\delta}C_{\\Delta}(t).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Proposition 7 (Suboptimality gap: naturally bounded contexts) For naturally bounded contexts $\\mathbf{X}(\\bar{t})$ with LAC function $\\mathcal{L}(\\cdot)$ , the margin constant is bounded as ", "page_idx": 40}, {"type": "equation", "text": "$$\nC_{\\Delta}(t)\\leq c K\\sqrt{d}\\mathcal{L}(R)\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "for some absolute constant $c>0$ . ", "page_idx": 40}, {"type": "text", "text": "Discussion of Proposition 7. It has a $K$ linear bound, which can worsen the regret bound for the number of arms $K$ compared to linUCB and linTS. We emphasize that this $K$ dependence occurs for the logarithmic regret bound, which is still advantageous for other algorithms with $\\mathcal{O}(\\sqrt{T})$ regret bound when $T\\gg K$ . Also, if every $X_{i}(t)$ follows a uniform distribution independently, this gap should have a dependence on $K$ using extreme value theory De Haan et al. [12]. So, our results meet the lower bound for uniform distribution, hence it cannot be improved. ", "page_idx": 41}, {"type": "text", "text": "H Proofs of Reuslts for Bounded Contexts ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "We provide proofs for Appendix G. For simplicity, we assume $R\\,\\geq\\,1$ . Using observation of Appendix B.1, any density defined in $\\mathbb{B}_{R}$ with LAC $\\mathcal{L}(\\cdot)$ has bounded decay rate $\\sqrt{d}\\mathcal{L}(R)$ by combining Lemma 3. ", "page_idx": 41}, {"type": "text", "text": "H.1 Fixed History Arguments ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "We fix again the time step $t$ and history $\\mathcal{H}_{t-1}$ and derive the fixed history results for the diversity and suboptimality gap. We use the same fixed history arguments in the Appendix E. We set historyconditioned contexts as $\\mathbf{X}:=\\mathbf{X}(t)\\mid\\mathcal{H}_{t-1}$ and $\\dot{\\mathbf{X}}=\\mathbf{\\Phi}(X_{1}^{\\top},\\ldots X_{K}^{\\top}),\\dot{X_{i}}\\in\\mathbb{R}^{d}$ . To address any greedy policy with respect to $\\hat{\\theta}_{t-1}$ , we propose an analysis that applies to any greedy policy with arbitrary $\\theta\\in\\mathbb{R}^{d}$ . In Appendix E.1, we already argued that it suffices to bound this variance for any fixed $\\theta$ . Since we fix $\\theta$ , which is the corresponding value of $\\hat{\\theta}_{t-1}$ under the given history $\\mathcal{H}_{t-1}$ , we define the policy-selected arm $a=\\arg\\operatorname*{max}^{\\bar{}}{X_{i}^{\\top}\\theta}$ . Same as previous definitions, we define the event $\\Omega_{i}:=\\bar{\\{a}=i\\}$ and $\\Omega_{i}(\\{x_{j}\\}_{j\\neq i}):=\\{a=i\\}\\cap\\{X_{j}=x_{j}$ for all $j\\neq i\\}$ . Simiarly, define $\\Omega_{i}^{\\star}:=\\{a^{\\star}=i\\}$ and $\\Omega_{i}^{\\star}(\\{x_{j}\\}_{j\\neq i}):=\\{a^{\\star}=i\\}\\cap\\{X_{j}=x_{j}}$ for all $j\\neq i$ }. ", "page_idx": 41}, {"type": "text", "text": "Event decomposition and conditional density. We first define $b=\\operatorname*{max}_{i\\neq a}X_{i}^{\\top}\\theta$ . Similar to the unbounded contexts, we decompose our diversity as ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[v^{\\top}X_{a}X_{a}^{\\top}v]=\\mathbb{P}[b\\leq c_{\\star}R]\\mathbb{E}[v^{\\top}X_{a}X_{a}^{\\top}v\\mid b\\leq c_{\\star}R]+\\mathbb{P}[b>c_{\\star}R]\\mathbb{E}[v^{\\top}X_{a}X_{a}^{\\top}v\\mid b>c_{\\star}R]}\\\\ &{\\phantom{\\mathbb{E}[v^{\\top}X_{a}X_{a}^{\\top}v\\mid b\\leq c_{\\star}R]}\\geq p_{*}\\mathbb{E}[v^{\\top}X_{a}X_{a}^{\\top}v\\mid b\\leq c_{\\star}R]}\\\\ &{\\phantom{\\mathbb{E}[v^{\\top}X_{a}X_{a}^{\\top}v\\mid\\{b\\leq c_{\\star}R\\}\\cap\\Omega_{i}(\\{x_{j}\\}_{j\\neq i})]}\\geq p_{*}\\mathbb{E}\\left[\\mathbb{E}[v^{\\top}X_{a}X_{a}^{\\top}v\\mid\\{b\\leq c_{\\star}R\\}\\cap\\Omega_{i}(\\{x_{j}\\}_{j\\neq i})]\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Therefore our next interest are projected contexts, defined as ", "page_idx": 41}, {"type": "equation", "text": "$$\nX_{i}^{\\top}v\\mid\\Omega_{i}(\\{x_{j}\\}_{j\\neq i})\\cap\\{b\\leq c_{\\star}R\\}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "We first investigate the support of conditonal random vector, ", "page_idx": 41}, {"type": "equation", "text": "$$\nX_{i}\\mid\\Omega_{i}(\\{x_{j}\\}_{j\\neq i})\\cap\\{b\\leq c_{\\star}R\\}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "whose density is the integration of $f$ within the section $\\{x\\in\\mathbb{B}_{R}\\mid x^{\\top}\\theta\\geq b\\}\\cap\\{x\\in\\mathbb{B}_{R}\\mid x^{\\top}v=y\\}$ . Hence, the density of $X_{i}^{\\top}v\\mid\\Omega_{i}(\\{x_{j}\\})\\cap\\{b\\leq c_{\\star}R\\}$ is a section density of $\\{x\\in\\mathbb{B}_{R}\\mid x^{\\top}\\theta\\geq b\\}$ with direction $v$ . ", "page_idx": 41}, {"type": "text", "text": "H.2 Sections of the Ball ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Next, we aim to bound the one-side decay rate of the section density in the ball. Unlike the previous sections, the sections corresponding to $\\dot{\\Omega}_{i}(\\{x_{j}\\}_{j\\neq i})$ does not makes no longer expanding sections due to the boundary of $\\mathbb{B}_{R}$ . To deal with sections for bounded contexts, we define several sections of the ball. ", "page_idx": 41}, {"type": "text", "text": "Definition 14 (Sliced ball) We define slide ball $\\mathbb{S}_{R}(v,y)$ as ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbb{S}_{R}(v,y):=\\{x\\in\\mathbb{B}_{R}\\mid x^{\\top}v=y\\}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Also define double sliced ball ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbb{S}_{R}(\\theta,b,v,y):=\\{x\\in\\mathbb{B}_{R}\\mid x^{\\top}v=y,x^{\\top}\\theta\\geq b\\}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "image", "img_path": "rblaF2euXQ/tmp/5acee580e6db1902c0cf511a2d18f08d34b269b5c81cf3765ef6cf0dbb3f6688.jpg", "img_caption": ["Figure 10: Illustrations for various sections in the ball $\\mathbb{B}_{R}$ . The red line is $\\mathbb{S}_{R}(\\theta,b,v,y)$ and the green line is $\\mathbb{S}_{R}(\\theta,b)$ . "], "img_footnote": [], "page_idx": 42}, {"type": "text", "text": "For bounded contexts, we need to bound the one-side decay rate of section density, where the sections are sliced balls. We aim to get the lower bound of ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{E}[|v^{\\top}X_{i}|^{2}\\mid\\Omega_{j}(\\{x_{j}\\}_{j\\neq i})\\cap\\{b\\leq c_{\\star}R\\}]\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "for any $\\Omega_{i}(\\{x_{j}\\}_{j\\neq i}),b$ . Observe that the support of $v^{\\top}X_{i}=y\\mid\\Omega_{j}(\\{x_{j}\\}_{j\\neq i})\\cap\\{b\\leq c_{\\star}R\\}$ is the section of a double sliced ball, $\\mathbb{S}_{R}(\\theta,\\operatorname*{max}_{j\\neq i}x_{j}^{\\top}\\theta=b,v,y)$ . Here we aim to bound the one-side decay rate of theses section densities to apply Lemma 4. ", "page_idx": 42}, {"type": "text", "text": "H.3 Linear Section Maps ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "To deal with varying sections, we define linear section maps, and using linear section maps we can bound the one-side decay rate of the section densities. We first define a projection map, a projection to the center of similarity. ", "page_idx": 42}, {"type": "text", "text": "Definition 15 (Projection map) We define the projection map between $A\\subset\\ensuremath{\\mathbb{R}}^{d}$ and $P\\in\\mathbb{R}^{d}$ as $a$ map $\\Phi:A\\rightarrow P$ , which is an affine point projection with the center of similarity at $P$ . Furthermore, we call $P$ as the projection point. ", "page_idx": 42}, {"type": "image", "img_path": "rblaF2euXQ/tmp/185e0db9257b3209bb27a3a9620709e418d4c9567be22b58db25a54cfdf92bc9.jpg", "img_caption": ["Remark 4 The projection map means that homothety toward some point $P$ . ", "Figure 11: Illustration of projection map. "], "img_footnote": [], "page_idx": 42}, {"type": "text", "text": "Definition 16 (Linear section maps) We define the linear section maps between sections. For two sections of $A$ , ${\\mathrm{Sec}}(A,v,y)$ and $\\operatorname{Sec}(A,v,y+h)$ , we define linear section map $\\Phi_{y}^{h}$ as ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\Phi_{y}^{h}(\\cdot):\\operatorname{Sec}(A,v,y)\\to\\operatorname{Sec}(A,v,y+h)\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "which satisfies $\\Phi_{y}^{h}(\\operatorname{Sec}(A,v,y))\\subset\\operatorname{Sec}(A,v,y+h)$ and $\\Phi_{y}^{h}$ is a part of some projection map with center $P$ . ", "page_idx": 43}, {"type": "image", "img_path": "rblaF2euXQ/tmp/502da3fcaff9a98cbe6b39d1cb1ff76ba0fe660254fbb9b66eda62d488170c0f.jpg", "img_caption": ["Figure 12: Illustration of linear section maps. Projection map of ${\\mathrm{Sec}}(A,v,y)$ to $P$ induce linear section map ${\\mathrm{Sec}}(A,v,y)$ to $\\operatorname{Sec}(A,v,y+h)$ . Also we use $H$ as the length between ${\\mathrm{Sec}}(A,v,y)$ and $P$ . "], "img_footnote": [], "page_idx": 43}, {"type": "text", "text": "H.4 One-side Decay Rate of Linear Section Maps ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Our subgoal is to bound the one-side decay rate of the section density, and we use linear section maps to bound them. Previously, we deal with expanding or equal sections, hence section maps directly gives the lower bound of density. However, for general linear section maps, they are no longer expanding sections due to boundaries. Assume the density $f$ is defined in $\\mathbb{B}_{R}$ and it has a decay rate $M$ . If $f$ has LAC with function $\\mathcal{L}(\\cdot)$ , we already studied in Lemma 3 that it has decay rate with $M=\\sqrt{d}\\mathcal{L}(R)$ . We can set $\\mathcal{L}(R)\\geq10$ . ", "page_idx": 43}, {"type": "text", "text": "Slope of linear section maps. We first bound the maximum length between $\\Phi_{y}^{h}(x)$ and $x$ . It is related to the slope of section maps, and we define $s>0$ such that ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\|\\Phi_{y}^{h}(x)-x\\|_{2}\\leq s h\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "for any $x\\in\\operatorname{Sec}(A,v,y)$ . We call $s$ as a slope of linear section map $\\Phi_{y}^{h}(\\cdot)$ . ", "page_idx": 43}, {"type": "text", "text": "Volume element. We define $|\\operatorname*{det}(\\nabla\\Phi_{y}^{h}(x))|\\;:=\\;u_{y}(h)$ . This value is the same for any $x\\in$ ${\\mathrm{Sec}}(A,v,y)$ , since we use the linear section maps. Using that, we can calculate the lower bound of $g(y+h)$ as the following: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g(y+h)=\\displaystyle\\int_{\\mathrm{Sec}(A,v,y+h)}f(x)\\mathrm{d}x}\\\\ &{\\qquad\\qquad\\geq\\displaystyle\\int_{\\mathrm{Sec}(A,v,y)}f(\\Phi_{y}^{h}(x))|\\operatorname*{det}(\\nabla\\Phi_{y}^{h}(x))|\\mathrm{d}x}\\\\ &{\\qquad\\qquad\\geq\\displaystyle\\int_{\\mathrm{Sec}(A,v,y)}f(x)\\exp(-M s h)u_{y}(h)\\mathrm{d}x}\\\\ &{\\qquad=\\exp(-M s h)u_{y}(h)g(y).}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "The third inequality is by using the Gronwall inequality, Lemma 22, using the decay rate is bounded by $M$ and the length between $x$ and $\\Phi_{y}^{h}(x)$ is bounded by $s h$ . This formula tell us that to bound the one-side decay rate of section density, there are two quantities: slope $s$ and volume element $u_{y}(h)$ . ", "page_idx": 44}, {"type": "text", "text": "A way to bound $u_{y}(h)$ . Lets define $H$ as the intersection of two section ${\\mathrm{Sec}}(A,v,y)$ and $P$ . The volume element can be calculated as ", "page_idx": 44}, {"type": "equation", "text": "$$\nu_{y}(h)=(\\frac{H-h}{H})^{d}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "holds since H\u2212his the ratio of similarity. Then, ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\frac{g(y+h)}{g(y)}\\geq\\exp(-M s h-(-d\\log(\\frac{H-h}{H})))\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "When $h\\leq\\frac{H}{2}$ , we have $\\begin{array}{r}{\\log\\bigl(\\frac{H-h}{H}\\bigr)\\geq1-\\frac{2h}{H}}\\end{array}$ , we have ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\frac{g(y+h)}{g(y)}\\geq\\exp(-M s h-\\frac{2d}{H}h)\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "This implies that when we can find the linear section map $\\Phi_{y}^{h}$ , we can bound the one-side decay rate of the section density by Ms + 2Hd . ", "page_idx": 44}, {"type": "text", "text": "H.5 Linear Section Maps between Sliced Balls, $\\mathbb{S}_{R}(v,y)$ ", "page_idx": 44}, {"type": "image", "img_path": "rblaF2euXQ/tmp/6fd7a33f09612e70ac8affde9ed579f6deab31ff34b371265835facec3717f64.jpg", "img_caption": [], "img_footnote": [], "page_idx": 44}, {"type": "text", "text": "Figure 13: Illustration of the linear section map between two sections of the ball. The linear section map is constructed by the center $P$ . ", "page_idx": 44}, {"type": "text", "text": "Lemma 16 (One-side decay rate of $\\mathbb{S}_{R}(v,y))$ The projected density $g(y)$ of sections $\\mathbb{S}_{R}(v,y)$ satisfies ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\frac{g(y^{\\prime})}{g(y)}\\geq\\exp(-3\\sqrt{d}\\mathcal{L}(R)(y^{\\prime}-y))\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "for any $\\begin{array}{r}{-\\frac{1}{\\sqrt{d}}<y<y^{\\prime}<\\frac{1}{\\sqrt{d}}}\\end{array}$ . ", "page_idx": 44}, {"type": "text", "text": "Proof Set $P$ as the similarity center of $\\mathbb{S}_{R}(v,y)$ and $\\mathbb{S}_{R}(v,y+h)$ . We aim to apply arguments of Appendix H.4. Using this $P$ , we can make section maps for $\\Phi_{y}^{h}$ . First, slope of $\\Phi_{y}^{h}$ is bounded by \u221aR2\u2212R(y+h)2 \u22642 for |y| \u2264 \u221a1d. The volume element of \u03a6yh can be interpreted as the ratio of ", "page_idx": 44}, {"type": "text", "text": "similarity. Let H be the distance of P and SR(v, y). Note that H \u2265R2\u2212y(+yh+h)2 and hence $H\\ge2h$ holds. By applying result in Appendix H.4, it\u2019s one-side decay rate in $[-{\\frac{1}{\\sqrt{d}}},{\\frac{1}{\\sqrt{d}}}]$ is bounded by ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle2\\sqrt{d}\\mathcal{L}(R)+\\frac{2d}{H}\\leq2\\sqrt{d}\\mathcal{L}(R)+\\frac{2d(y+h)}{R^{2}-(y+h)^{2}}}\\\\ {\\displaystyle\\leq3\\sqrt{d}\\mathcal{L}(R).}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "holds for all $\\begin{array}{r}{-\\frac{1}{\\sqrt{d}}\\leq y<y+h\\leq\\frac{1}{\\sqrt{d}}}\\end{array}$ since $\\mathcal{L}(R)\\geq10$ . ", "page_idx": 45}, {"type": "text", "text": "H.6 Linear Section Maps for Double Slicde balls, $\\mathbb{S}_{R}(\\theta,b,v,y)$ ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Goal. We investigate the one-sided decay rate of section densities with sections $\\mathbb{S}_{R}(\\theta,b,v,y)$ for fixed $\\theta,v,b$ . For density $f(x)$ defined in $\\mathbb{B}_{R}$ with LAC function $\\mathcal{L}(\\cdot)$ , we define section density of $\\mathbb{S}_{R}(\\theta,b,v,y)$ as $g(y)$ . We set $\\begin{array}{r}{\\mathbf{M}=4\\sqrt{d}\\mathcal{L}(R)+\\frac{16\\sqrt{d}}{R^{2}(1-c_{\\star})}}\\end{array}$ . We prove that the one-side decay rate at $y\\in[-\\frac{1}{\\mathbf{M}},\\frac{1}{\\mathbf{M}}]$ is bounded by $\\mathbf{M}$ . Also we define some small angle $\\begin{array}{r}{\\tau_{0}:=\\sin^{-1}(\\frac{1}{\\mathbf{M}R})}\\end{array}$ . ", "page_idx": 45}, {"type": "text", "text": "H.6.1 Case $v\\perp\\theta$ . ", "page_idx": 45}, {"type": "text", "text": "For this case, the sections $\\mathbb{S}_{R}(\\theta,b,v,y)$ are no longer expanding section when $\\langle\\theta,v\\rangle$ is close to $\\scriptstyle{\\frac{\\pi}{2}}$ . In that case, we can construct a linear section map, and the following figure illustrates the procedure. In each section $\\mathbb{S}_{R}(\\theta,b,v,y)$ and $\\mathbb{S}_{R}(\\theta,b,v,y+h)$ , assume the highest coordinate with respect to direction $\\theta$ as $Q$ and $Q_{h}$ . Assume the directed half- line starting from connecting $Q$ and connecting $Q_{h}$ and the hyperplane $\\,{\\dot{\\{x\\mid x^{\\top}\\theta}}}=b\\}$ meets at some point $P$ . Then we can set the intersection point $P$ as the center of the projection map and we can construct linear section map using $P$ . Please see the figure 14 for the intuitions. ", "page_idx": 45}, {"type": "image", "img_path": "rblaF2euXQ/tmp/7794853ac4ce91cef87ca2568cd952e9d82ea3d5f04745d279edc2b29f193594.jpg", "img_caption": [], "img_footnote": [], "page_idx": 45}, {"type": "text", "text": "Figure 14: Illustration of section maps between $\\mathbb{S}_{R}(\\theta,b,v,y)$ and $\\mathbb{S}_{R}(\\theta,b,v,y+h)$ . We connect two points $Q$ and $Q_{h}$ and let $P$ be the intersection of $\\{x\\mid x^{\\top}\\theta=b\\}$ . And then, using $P$ , we can set the section maps between $\\mathbb{S}_{R}(\\theta,b,v,y)$ and $\\mathbb{S}_{R}(\\theta,b,\\dot{v},y+h)$ . ", "page_idx": 45}, {"type": "text", "text": "Lemma 17 (One-side decay rate of doubly sliced ball) If $\\boldsymbol{v}\\perp\\boldsymbol{\\theta}$ , suppose we construct linear section map constructed by the sliced balls between $\\mathbb{S}_{R}(\\theta,b,v,y)$ and $\\mathbb{S}_{R}(\\theta,b,v,y+h)$ for $b\\leq\\alpha R$ in the way we described before. By setting $\\begin{array}{r}{M=2\\sqrt{d}\\mathcal{L}(R)+\\frac{8\\sqrt{d}}{R^{2}(1-\\alpha)}}\\end{array}$ , the one-side decay rate of the section density $g(\\cdot)$ in the interval $\\begin{array}{r}{-\\frac{1}{M}<y<y+h<\\frac{1}{M}}\\end{array}$ is bounded by ", "page_idx": 45}, {"type": "equation", "text": "$$\n{\\frac{g(y+h)}{g(y)}}\\geq\\exp(-M h).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Proof We aim to estimate $s$ and $H$ to apply arguments in Appendix H.4. To bound $s$ , we can easily see that s \u2264 $\\begin{array}{r}{s\\leq\\frac{R}{\\sqrt{R^{2}-(y+h^{2})}}\\leq2}\\end{array}$ since it is related to the slope of the tangent line at $y+h$ . ", "page_idx": 46}, {"type": "text", "text": "To bound $H$ , with some elementary calculation, we can see that ", "page_idx": 46}, {"type": "equation", "text": "$$\nH=H^{\\prime}\\times\\left(\\frac{\\sqrt{R^{2}-y^{2}}-c_{\\star}R}{\\sqrt{R^{2}-y^{2}}}\\right)\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "for ", "page_idx": 46}, {"type": "equation", "text": "$$\nH^{\\prime}\\geq\\frac{R^{2}-(y+h)^{2}}{y+h}.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "$H^{\\prime}$ is define in the figure 14. Then we can see that ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{\\frac{d}{H}\\leq d\\frac{y+h}{R^{2}-(y+h)^{2}}\\bigg(\\frac{\\sqrt{R^{2}-y^{2}}}{\\sqrt{R^{2}-y^{2}}-\\alpha R}\\bigg)}}\\\\ {\\leq\\displaystyle{\\frac{2y d}{R^{2}-y^{2}}\\frac{2}{1-\\alpha}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "for any $y,y+h\\in[-\\textstyle{\\frac{1}{M}},\\textstyle{\\frac{1}{M}}]$ . Then we can get ", "page_idx": 46}, {"type": "equation", "text": "$$\n2\\sqrt{d}\\mathcal{L}(R)+\\frac{2d}{H}\\lesssim2\\sqrt{d}\\mathcal{L}(R)+\\frac{8y d}{R^{2}(1-\\alpha)}\\leq M\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "The remaining part is proved by the argument from Appendix H.4. ", "page_idx": 46}, {"type": "text", "text": "In this case, we define $\\begin{array}{r}{\\theta^{\\prime}:=\\frac{\\theta-v(v^{\\top}\\theta)}{\\|\\theta-v(v^{\\top}\\theta)\\|_{2}}}\\end{array}$ and we aim to define section maps between $\\mathbb{S}_{R}(\\theta,b,v,y)$ using $v$ and $\\theta^{\\prime}$ . For new sections, $\\mathbb{S}_{R}(\\theta^{\\prime},b,v,y)$ , we can make section maps as we described in Lemma 17. We can see that these section maps also become the section maps between $\\mathbb{S}_{R}(\\theta,b,v,y)$ . By using Lemma 17 for $\\alpha$ with $\\begin{array}{r}{1-\\alpha=\\frac{1-c_{\\star}}{2}}\\end{array}$ and $\\theta^{\\prime},v$ , we have ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\frac{g(y+h)}{g(y)}\\geq\\exp(-\\mathbf{M}h)\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "holds for $\\begin{array}{r}{\\mathbf{M}=2\\sqrt{d}\\mathcal{L}(R)+\\frac{16}{R^{2}(1-c^{\\star})}}\\end{array}$ for any $\\begin{array}{r}{-\\frac{1}{\\mathbf{M}}<y<y+h<\\frac{1}{\\mathbf{M}}}\\end{array}$ ", "page_idx": 46}, {"type": "text", "text": "H.6.3 Case 0 \u2264\u2220(v, \u03b8) \u2264\u03c0\u2212\u03c40", "page_idx": 46}, {"type": "text", "text": "In this case, the natural section map between $\\mathbb{S}_{R}(v,y)$ and $\\mathbb{S}_{R}(v,y+h)$ for any $\\begin{array}{r}{-\\frac{1}{\\mathbf{M}}<y<y+h<}\\end{array}$ $\\frac{1}{\\mathbf{M}}$ can be a expanding section map for $y,y+h\\in\\left[-\\frac{1}{\\mathbf{M}},\\frac{1}{\\mathbf{M}}\\right]$ . We already prove that the one-side decay rate of section density is bounded by ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\frac{g(y+h)}{g(y)}\\geq\\exp(-\\mathbf{M}h)\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "using Lemma 16. ", "page_idx": 46}, {"type": "image", "img_path": "rblaF2euXQ/tmp/d170ee743e0c462349f24c72990f98ede321b856ea8e7600a08d964ef7ed5c46.jpg", "img_caption": ["Figure 15: For the case $\\theta$ and $v$ has not-too-large angle, we first define linear section map between $\\mathbb{S}(\\bar{\\boldsymbol{\\theta}},\\boldsymbol{y})$ and $\\mathbb{S}(\\theta,y+h)$ . This section map can also be the linear section map between $\\bar{\\mathbb{S}}(\\theta,b,v,y)$ and $\\mathbb{S}(\\theta,b,v,y+h)$ . Using this map, we can bound the one-side decay rate of section densities of $\\mathbb{S}(\\theta,b,v,y)$ by Lemma 16. "], "img_footnote": [], "page_idx": 47}, {"type": "text", "text": "H.7 Proof of Proposition 4 ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "We first fix $\\theta$ and $v$ with the same argument as in the proof of Theorem 2. For fixed history contexts $\\mathbf{X}=(X_{1}^{\\top},\\bot\\ldots X_{K}^{\\top})$ and fixed $\\theta$ and $v$ . Define $b$ as the second largest value of $(X_{1}^{\\top}\\theta,\\cdot\\cdot\\cdot\\bar{X}_{K}^{\\top}\\theta)$ . By the Condition 1, we first bound ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[v^{\\top}X_{a}X_{a}^{\\top}v]=\\mathbb{P}[b\\leq c_{\\star}R]\\mathbb{E}[v^{\\top}X_{a}X_{a}^{\\top}v\\mid b\\leq c_{\\star}R]+\\mathbb{P}[b>c_{\\star}R]\\mathbb{E}[v^{\\top}X_{a}X_{a}^{\\top}v\\mid b>c_{\\star}R]}\\\\ &{\\phantom{\\mathbb{E}[v^{\\top}X_{a}X_{a}^{\\top}v\\mid b\\leq c_{\\star}R]}\\geq p_{\\star}\\mathbb{E}[v^{\\top}X_{a}X_{a}^{\\top}v\\mid b\\leq c_{\\star}R]}\\\\ &{\\phantom{\\mathbb{E}[v^{\\top}X_{a}X_{a}^{\\top}v\\mid\\{b\\leq c_{\\star}R\\}\\cap\\Omega_{i}(\\{x_{j}\\}_{j\\neq i})]}\\geq p_{\\star}\\mathbb{E}\\left[\\mathbb{E}[v^{\\top}X_{a}X_{a}^{\\top}v\\mid\\{b\\leq c_{\\star}R\\}\\cap\\Omega_{i}(\\{x_{j}\\}_{j\\neq i})]\\right]}\\\\ &{\\phantom{\\mathbb{E}[v^{\\top}X_{a}\\nleq\\epsilon\\not|b]}\\geq p_{\\star}\\mathbb{E}\\left[\\mathbb{E}[v^{\\top}X_{i}X_{i}^{\\top}v\\mid\\{b\\leq c_{\\star}R\\}\\cap\\Omega_{i}(\\{x_{j}\\}_{j\\neq i})]\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Then, we aim to bound ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\mathbb{E}[v^{\\top}X_{i}X_{i}^{\\top}v\\mid\\{b\\leq c_{\\star}R\\}\\cap\\Omega_{i}(\\{x_{j}\\}_{j\\neq i})]\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "and we further observe the density of $X_{i}^{\\top}v=y\\mid\\Omega_{i}(\\{x_{j}\\}_{j\\neq i})\\cap\\{b\\leq c_{\\star}R\\}$ . It is a section density of sections $\\mathbb{S}_{R}(\\theta,b,v,y)$ . ", "page_idx": 47}, {"type": "text", "text": "Case $\\textbf{l}_{\\overline{{2}}}^{\\pi}-\\tau_{0}\\leq\\angle(\\theta,v)\\leq\\frac{\\pi}{2}$ : By applying results from Appendix H.6 and Lemma 4, we have ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\mathbb{E}[v^{\\top}X_{i}X_{i}^{\\top}v\\mid\\{b\\leq c_{\\star}R\\}\\cap\\Omega_{i}(x_{j})]\\geq c\\frac{1}{\\mathbf{M}^{2}}.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Case $\\begin{array}{r}{\\pmb{2}\\,0\\leq\\angle(\\theta,v)\\leq\\frac{\\pi}{2}-\\tau}\\end{array}$ : By applying results from Appendix H.6 and Lemma 4, we have ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\mathbb{E}[v^{\\top}X_{i}X_{i}^{\\top}v\\mid\\{b\\leq c_{\\star}R\\}\\cap\\Omega_{i}(x_{j})]\\geq c{\\frac{1}{\\mathbf{M}^{2}}}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Case 3 $\\textstyle{\\mathcal{L}}(\\theta,v)\\geq{\\frac{\\pi}{2}}$ : When we play with $-v$ instead of $v$ , it falls down to Case 1 or 2. ", "page_idx": 47}, {"type": "text", "text": "H.8 Proof of Proposition 5 ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Proof Using similar argument, we do the fixed history arguments. Simply write $\\mathbf{X}=(X_{1},\\ldots X_{K})=$ $\\mathbf{X}(t)\\mid\\mathcal{H}_{t-1}\\stackrel{}{}$ and we first fix $\\theta$ and consider the greedy policy $a$ with estimator $\\theta$ . We define $\\lambda_{\\star}$ as a diversity constant of $\\mathbf{X}$ . Also, we define $L_{n}:=c_{0}\\sqrt{d}x_{\\operatorname*{max}}(1+\\log d K+n\\log\\gamma)$ and set event $\\mathbf{B}_{n}\\,:=\\,\\{X_{i}\\,\\in\\mathbb{B}(0,L_{n})$ for all $i\\;\\in\\;[K]\\}$ . We determine $\\gamma\\,>\\,0$ later. By Lemma 19, we have $\\begin{array}{r}{\\mathbb{P}[\\mathbf{B}_{n}]\\ge1-\\frac{1}{\\gamma^{n}}}\\end{array}$ . We apply the peeling technique to bound the truncated contexts. ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\mathbb{E}[X_{a}X_{a}^{\\top}]=\\mathbb{E}[X_{a}X_{a}^{\\top};\\mathbf{B}_{1}]+\\sum_{n=1}^{\\infty}\\mathbb{E}[X_{a}X_{a}^{\\top};\\mathbf{B}_{n+1}\\setminus\\mathbf{B}_{n}]\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "By setting $\\alpha=c_{0}\\sqrt{d}x_{\\mathrm{max}},\\beta=1+\\log d K$ , observe that ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{n=1}^{\\infty}\\mathbb{E}[X_{a}X_{a}^{\\top};\\mathbf{B}_{n+1}\\setminus\\mathbf{B}_{n}]\\preceq\\displaystyle\\sum_{n=1}^{\\infty}L_{n+1}^{2}\\frac{1}{\\gamma^{n}}I_{d}}&{}\\\\ {\\displaystyle\\preceq\\sum_{n=1}^{\\infty}(\\alpha(\\beta+n\\log\\gamma))^{2}\\frac{1}{\\gamma^{n}}I_{d}}&{}\\\\ {\\displaystyle\\preceq\\sum_{n=1}^{\\infty}2\\alpha^{2}(\\beta^{2}+n^{2}\\log^{2}\\gamma)\\frac{1}{\\gamma^{n}}I_{d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "If $\\gamma\\geq3\\alpha^{2}\\beta^{2}\\frac{1}{\\lambda_{\\star}}$ , we have ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\sum_{n=1}^{\\infty}\\mathbb{E}[X_{a}X_{a}^{\\top};\\mathbf{B}_{n+1}\\setminus\\mathbf{B}_{n}]\\preceq\\frac{1}{2}\\lambda_{\\star}I_{d}.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Using Theorem 2, we get ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\lambda_{\\star}I_{d}\\leq\\mathbb{E}[X_{a}X_{a}^{\\top};{\\mathbf B}_{1}]+{\\frac{1}{2}}\\lambda_{\\star}I_{d}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "and hence ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\mathbb{E}[X_{a}X_{a}^{\\top};\\mathbf{B}_{1}]\\geq{\\frac{1}{2}}\\lambda_{\\star}.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Finally, we get ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\mathbb{E}[X_{a}X_{a}^{\\top}\\mid\\mathbf{B}_{1}]\\geq\\frac{1}{2}\\lambda_{\\star}.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "H.9 Proof of Proposition 6 ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Define $\\Delta(\\overline{{\\mathbf{X}}})$ as the suboptimality gap of $\\overline{{\\mathbf{X}}}$ and denote its density as $f_{\\overline{{\\mathbf{X}}}}(x),x\\in(\\mathbb{R}^{d})^{K}$ . By the direct expectation, we get ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}[\\Delta(\\overline{{\\mathbf{X}}})\\leq\\varepsilon]=\\displaystyle\\int_{\\Delta(\\overline{{\\mathbf{x}}})\\leq\\varepsilon}f_{\\overline{{\\mathbf{X}}}}(x)d x}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\displaystyle\\int_{\\{\\Delta(\\mathbf{x})\\leq\\varepsilon\\}\\cap D}\\frac{f_{\\mathbf{X}}(x)}{\\mathbb{P}[\\mathbf{X}\\in D]}d x}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\displaystyle\\int_{\\Delta(\\mathbf{x})\\leq\\varepsilon}\\frac{f_{\\mathbf{X}}(x)}{\\mathbb{P}[\\mathbf{X}\\in D]}d x}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\displaystyle\\frac{1}{1-\\delta}\\mathbb{P}[\\Delta(\\mathbf{X})\\leq\\varepsilon]}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "holds. ", "page_idx": 48}, {"type": "text", "text": "H.10 Proof of Proposition 7 ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "By conditioning $\\mathcal{H}_{t-1}$ , we define fixed history contexts $\\mathbf{X}\\,=\\,(X_{1}^{\\top},\\ldots X_{K}^{\\top})$ similar to previous proofs in Appendix F. We decompose the probability of suboptimality gap as ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{P}[\\Delta(\\mathbf{X})\\leq\\varepsilon]=\\sum_{i=1}^{K}\\mathbb{P}[\\{\\Delta(\\mathbf{X})\\leq\\varepsilon\\}\\cap\\Omega_{i}^{\\star}]}\\\\ {\\displaystyle=\\sum_{i=1}^{K}\\mathbb{E}\\big[\\mathbb{P}[\\{\\Delta(\\mathbf{X})\\leq\\varepsilon\\}\\cap\\Omega_{i}^{\\star}\\mid\\{X_{j}=x_{j}\\}_{j\\neq i}]\\big]}\\\\ {\\displaystyle=\\sum_{i=1}^{K}\\mathbb{E}\\big[\\mathbb{P}[\\{\\operatorname*{max}_{j\\neq i}x_{j}^{\\top}\\theta^{\\star}\\leq X_{i}^{\\top}\\theta^{\\star}\\leq\\operatorname*{max}_{j\\neq i}x_{j}^{\\top}\\theta^{\\star}+\\varepsilon\\}\\mid\\{X_{j}=x_{j}\\}_{j\\neq i}]\\big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "the last equality holds by the definition of $\\Omega_{i}^{\\star}$ . Next we aim to bound ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\mathbb{P}[\\{\\underset{j\\neq i}{\\operatorname*{max}}x_{j}^{\\top}\\theta^{\\star}\\leq X_{i}^{\\top}\\theta^{\\star}\\leq\\underset{j\\neq i}{\\operatorname*{max}}x_{j}^{\\top}\\theta^{\\star}+\\varepsilon\\}\\mid\\{X_{j}=x_{j}\\}_{j\\neq i}]\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "and for the same argument, we only need to bound the maximum density of ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\mathbb{P}[\\,X_{i}^{\\top}\\theta^{\\star}=y\\mid\\{X_{j}=x_{j}\\}_{j\\neq i}].\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "and it is enough to bound its one-side decay rate by Lemma 5. Since the conditional density of ", "page_idx": 49}, {"type": "equation", "text": "$$\nX_{i}\\mid\\{X_{j}=x_{j}\\}_{j\\neq i},\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "say $f_{3}(\\cdot)$ , has same LAC with constant function $\\mathcal{L}(R)$ , hence it has bounded decay rate by $\\sqrt{d}\\mathcal{L}(R)$ by Lemma 3. Also the density $\\mathbb{P}[\\,X_{i}^{\\top}\\theta^{\\star}=y\\mid\\{X_{j}=x_{j}\\}_{j\\neq i}]$ is a section density of $f_{3}$ of $\\mathbb{S}_{R}(\\theta^{\\star},y)$ for $y\\in[-R,R]$ . Then we can observe that at least one of the directions of $\\theta^{\\star}$ or $-\\theta^{\\star}$ , the sections $\\mathbb{S}_{R}(\\theta^{\\star},y)$ are expanding section in $\\mathbb{B}_{R}$ . Then by applying Lemma 8, we can bound the one-side decay rate of section density $X_{i}^{\\top}\\theta^{\\star}\\mid\\{X_{j}(t)=x_{j}\\}_{j\\neq i}$ by ${\\sqrt{d}}{\\mathcal{L}}(R)$ and finally we can bound the maximum density by $3\\sqrt{d}\\mathcal{L}R$ by using Lemma 5. Therefore, by the decomposition we did before, we finally get ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{P}[\\Delta(\\mathbf{X}(t))\\leq\\varepsilon]\\leq\\displaystyle\\sum_{i\\in[K]}3\\sqrt{d}\\mathcal{L}(R)}&{}\\\\ {\\leq3K\\sqrt{d}\\mathcal{L}(R).}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "H.11 Proof of Corollary 1 ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "We have $\\begin{array}{r}{\\lambda_{\\star}(t)\\geq c\\frac{p}{d}\\frac{1}{(\\mathcal{L}(R)+\\frac{1}{R(1-c_{\\star})})^{2}}}\\end{array}$ with $\\begin{array}{r}{1-c_{\\star}=\\frac{R^{\\prime}}{r}\\asymp1}\\end{array}$ by using Proposition 4 and Lemma 14. Hence we get $\\begin{array}{r}{\\lambda_{\\star}(t)\\ge c\\frac{p}{d\\mathcal{L}(R)^{2}}:=\\lambda_{\\star}}\\end{array}$ . Also, using Proposition 6, we have the margin constant of truncated contexts $\\bar{C}_{\\Delta}$ is bounded by $\\scriptstyle{{\\frac{1}{1-p}}C_{\\Delta}}$ , where $C_{\\Delta}$ is defined in Theorem 3. We can see that ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbf{Reg}(T)\\leq c\\sigma^{2}d R^{2}C_{\\Delta}\\frac{1}{\\lambda_{\\star}}(\\log(T))^{2}}&{}\\\\ {\\leq\\widetilde{\\mathcal{O}}(d^{2.5}R^{2}\\mathscr{L}(R)^{2}\\frac{1}{p})}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "holds. ", "page_idx": 49}, {"type": "text", "text": "H.12 Proof of Corollary 2 ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "If $L\\geq\\sqrt{d}c_{0}x_{\\mathrm{max}}(3+\\log d K)$ , we can use the same proof of the Theorem 2. For the diversity constant of truncated contexts, it is lower bounded by $\\begin{array}{r}{\\frac{\\mathrm{I}}{2}\\lambda_{\\star}(t)}\\end{array}$ where $\\lambda_{\\star}(t)$ is define in Theorem 2. Also, using Proposition 6, we have the margin constant of truncated contexts $\\bar{C}_{\\Delta}$ is bounded by ", "page_idx": 49}, {"type": "text", "text": "$\\scriptstyle{{\\frac{1}{1-p}}C_{\\Delta}}$ , where $C_{\\Delta}$ is defined in Theorem 3. Combining these observations, we finally can apply Proposition 9 since truncated contexts are also has bounded $\\psi_{1}$ -norm by $x_{\\mathrm{max}}$ , and get the wanted result. ", "page_idx": 50}, {"type": "text", "text": "H.13 Proof of Corollary 3 ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "We can prove it directly by combining the results of Proposition 4 and Proposition 7. This can be obtained directly by combining the results and Proposition 8. ", "page_idx": 50}, {"type": "text", "text": "I Analysis After Challenges 1 and 2 are Satisfied ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "We now present the results and proofs to obtain an exact regret bound after the two challenges are addressed. Recall that we define the (unexpected) regret as $\\breve{\\mathrm{reg}}^{\\prime}(t):=X_{a^{\\star}(t)}(t)^{\\top}\\theta^{\\star}-X_{a(t)}\\breve{(t)}^{\\top}\\theta^{\\star}$ and expected regret as $\\mathrm{reg}(t)=\\mathbb{E}_{\\mathcal{H}_{t-1},\\mathbf{X}(t)}[\\mathrm{reg}^{\\prime}(t)]$ . We can achieve logarithmic regret bounds if the contexts meet Challenges 1 and 2. ", "page_idx": 50}, {"type": "text", "text": "We first present our regret analysis for bounded contexts. ", "page_idx": 50}, {"type": "text", "text": "Proposition 8 (Regret bound for bounded contexts) For bounded contexts as $\\|X_{i}(t)\\|_{2}\\leq R_{!}$ , suppose that $\\mathbb{E}[X_{a(t)}X_{a(t)}~|~\\mathcal{H}_{t-1}]\\succeq\\lambda_{\\star}I$ and $C_{\\Delta(\\mathbf{X}(t))}\\leq C_{\\Delta}$ for all $t\\in[T]$ and $\\mathcal{H}_{t-1}$ . Under the Assumption $^{\\,l}$ , expected regret of Algorithm $^{\\,I}$ is bounded by ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\mathbf{Reg}(T)\\leq c\\sigma^{2}R^{2}d C_{\\Delta}\\frac{1}{\\lambda_{\\star}}(\\log T)^{2}=\\widetilde{\\mathcal{O}}(R^{2}d\\frac{C_{\\Delta}}{\\lambda_{\\star}}).\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Next, we present the regret bound result for unbounded contexts, under the satisfaction of two challenges. ", "page_idx": 50}, {"type": "text", "text": "Proposition 9 (Expected regret analysis for unbounded contexts) For unbounded contexts as $\\|X_{i}^{\\bar{}}(t)\\|_{\\psi_{1}}\\leq x_{\\operatorname*{max}}$ , suppose that $\\mathbb{E}[X_{a(t)}X_{a(t)}~|~\\mathcal{H}_{t-1}]\\succeq\\lambda_{\\star}I$ and $C_{\\Delta(\\mathbf{X}(t))}\\leq C_{\\Delta}$ for all $t\\in[T]$ and $\\mathcal{H}_{t-1}$ . Under the Assumption $^{\\,I}$ , expected regret of Algorithm $^{\\,l}$ is bounded by ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\mathbf{Reg}(T)\\leq c\\sigma^{2}x_{\\operatorname*{max}}^{2}d C_{\\Delta}\\frac{1}{\\lambda_{\\star}}(\\log T)^{4}=\\widetilde{\\mathcal{O}}(x_{\\operatorname*{max}}^{2}d\\frac{C_{\\Delta}}{\\lambda_{\\star}}).\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "I.1 Proof of Proposition 8 ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Firstly, set $\\begin{array}{r}{T_{0}:=\\frac{c_{0}}{\\lambda_{\\star}}R(3\\log T+\\log d)}\\end{array}$ . Before starting the proof, we define the good events which satisfy the sufficient concentration of the estimator $\\widehat{\\theta}_{t}$ . ", "page_idx": 50}, {"type": "text", "text": "Definition 17 From now on, in this section, we define the event by $E_{t}$ . ", "page_idx": 50}, {"type": "equation", "text": "$$\nE_{t}:=\\{\\lambda_{\\operatorname*{min}}(\\Sigma(t))\\geq\\frac{\\lambda_{\\star}}{4}t\\}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Which is the event that the Gram matrix has a sufficient growth of the minimum eigenvalue. The event $E_{t}$ occurs with high probability according to Corollary 9. ", "page_idx": 50}, {"type": "text", "text": "Corollary 4 For $\\begin{array}{r}{T_{0}=\\frac{1}{c_{1}\\lambda_{\\star}}R(3\\log T+\\log d)}\\end{array}$ , the following holds. ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\mathbb{P}[\\bigcap_{\\tau=T_{0}}^{T}E_{t}]\\geq\\frac{1}{2T}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "tPhreo roefs uUlts.ing the Corollary 9, with probability $1-\\textstyle{\\frac{1}{2T^{2}}}$ , event $E_{t}$ holds. By the union bound, we get ", "page_idx": 50}, {"type": "text", "text": "We define the event $\\textstyle\\bigcap_{\\tau=T_{0}}^{t}E_{t}:=\\mathbf{E}_{t}$ . ", "page_idx": 50}, {"type": "text", "text": "Definition 18 (Self-normalized bound of OLS estimator) Next, we define the event $F_{t}$ as ", "page_idx": 51}, {"type": "equation", "text": "$$\nF_{t}:=\\left\\{\\Vert\\hat{{\\theta}}_{t}-\\theta^{\\star}\\Vert_{\\Sigma(t)}\\leq2\\sigma\\sqrt{d\\log\\left(T(1+t R^{2})\\right)}+1\\right\\}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "which is the event that satisfies the self-normalized concentration of the estimator. ", "page_idx": 51}, {"type": "text", "text": "Lemma 18 (Concentration of OLS Estimator) For any $\\begin{array}{r}{t>\\frac{4}{\\lambda_{\\star}}}\\end{array}$ , under the $E_{t}$ , ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\mathbb{P}[F_{t}]\\ge1-\\frac{1}{2T^{2}}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "holds. ", "page_idx": 51}, {"type": "text", "text": "Proof Under the event $E_{t}$ , we have $\\Sigma(t)\\succeq\\frac{1}{4}\\lambda_{\\star}t I_{d}$ , and the concentration of the OLS estimator satisfies ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\hat{\\theta}_{t}-\\theta^{\\star}\\|_{\\Sigma(t)}=(\\displaystyle\\sum_{\\tau=1}^{t}X_{a(\\tau)}(\\tau)\\eta_{\\tau})(\\Sigma(t))^{-1}(\\displaystyle\\sum_{\\tau=1}^{t}X_{a(\\tau)}(\\tau)\\eta_{\\tau})}\\\\ &{\\qquad\\qquad\\qquad\\le(\\displaystyle\\sum_{\\tau=1}^{t}X_{a(\\tau)}(\\tau)\\eta_{\\tau})(\\frac{1}{2}\\Sigma(t)+I_{d})^{-1}(\\displaystyle\\sum_{\\tau=1}^{t}X_{a(\\tau)}(\\tau)\\eta_{\\tau})}\\\\ &{\\qquad\\qquad\\qquad\\le2\\displaystyle\\sum_{\\tau=1}^{t}X_{a(\\tau)}(\\tau)\\eta_{\\tau})(\\Sigma(t)+I_{d})^{-1}(\\displaystyle\\sum_{\\tau=1}^{t}X_{a(\\tau)}(\\tau)\\eta_{\\tau})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "To bound $I$ , we use Lemma 24 from Abbasi-Yadkori et al. [1] and get the wanted result. ", "page_idx": 51}, {"type": "text", "text": "Next, we introduce an important lemma that is used to obtain the concentration of minimum eigenvalue of the Gram matrix. We define the event $\\textstyle\\bigcap_{\\tau=T_{0}}^{t}E_{t}:=\\mathbf{E}_{t}$ and $\\textstyle\\bigcap_{\\tau=T_{0}}^{t}F_{\\tau}:=\\mathbf{F}_{t}$ . We further define the event $\\mathbf{G}_{t}:=\\mathbf{E}_{t}\\cap\\mathbf{F}_{t}$ . ", "page_idx": 51}, {"type": "text", "text": "Corollary 5 (Good events) For the event $\\mathbf{G}_{t}$ defined above, for any $T_{0}\\leq t\\leq T$ , ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\mathbb{P}[\\mathbf{G}_{t}]\\geq1-\\frac{1}{T}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "holds. ", "page_idx": 51}, {"type": "text", "text": "Proof Straightforward from previous observations. ", "page_idx": 51}, {"type": "text", "text": "Then under the event $\\mathbf{G}_{t}$ , we have the following corollary. ", "page_idx": 51}, {"type": "text", "text": "Corollary 6 ( $\\ell_{2}$ concentration: bounded contexts) For any $t\\geq T_{0}$ , under the event $\\mathbf{G}_{t}$ , we have ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\|\\hat{\\theta}_{t}-\\theta^{\\star}\\|_{2}\\le c\\sigma\\frac{\\sqrt{d\\log T}}{\\sqrt{\\lambda_{\\star}t}}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "for some absolute constant $c>0$ . ", "page_idx": 51}, {"type": "text", "text": "Proof of the Proposition. Now we are ready to prove the Proposition 8. First, for time $t\\geq T_{0}$ , the history $\\mathcal{H}_{t-1}$ is contained in $\\mathbf{G}_{t-1}$ with probability $\\textstyle1\\,-\\,{\\frac{1}{T}}$ . The expectation of the $\\mathrm{reg}^{\\prime}(t)$ is calculated with randomness of the whole history $\\mathcal{H}_{t-1}$ and the distribution of contexts $\\mathbf X(t)$ , and we can observe the following for $\\gamma(d):=4\\sigma\\sqrt{d\\log(T+T^{2}R^{2})}$ : ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\mathrm{reg}^{\\prime}(t)]=\\mathbb{E}_{\\mathcal{H}_{t-1}}\\left[\\mathbb{E}_{{\\mathbf X}(t)}[\\mathrm{reg}^{\\prime}(t)\\mid\\mathcal{H}_{t-1}]\\right]}\\\\ &{\\qquad\\qquad=\\mathbb{E}_{{\\mathbf X}(t)}\\left[\\mathrm{reg}^{\\prime}(t)\\mid{\\mathbf G}_{t-1}\\right]\\mathbb{P}[{\\mathbf G}_{t-1}]+R\\mathbb{P}[{\\mathbf G}_{t-1}^{c}]}\\\\ &{\\qquad\\qquad\\le6C_{\\Delta}\\gamma(d)^{2}\\frac{R^{2}}{(t-1)\\lambda_{\\star}}+R\\mathbb{P}[{\\mathbf G}_{t-1}^{c}]\\quad(\\mathrm{by~Lemma~}7)}\\\\ &{\\qquad\\qquad\\le6C_{\\Delta}\\gamma(d)^{2}\\frac{R^{2}}{(t-1)\\lambda_{\\star}}+\\frac{R}{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "By summing up the inequalities until $T$ , we can obtain the wanted result. ", "page_idx": 51}, {"type": "text", "text": "I.2 Concentration of Sub-exponential Contexts ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Next we aim to provide regret analysis for unbounded contexts. To do that, we first provide some known facts for sub-exponential vectors. Under the Assumption 2, $X_{i}(t)$ has $\\psi_{1}$ norm with $x_{\\mathrm{max}}$ . Using property from Wainwright [38], for any $v\\in\\mathbb{S}^{d-1}$ , there exists absolute constant $c_{0}>0$ such that ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\mathbb{P}[|X_{i}(t)^{\\top}v|>c_{0}x_{\\operatorname*{max}}(1+u)]\\le\\exp(-u)\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "holds for all $u>0$ . If we set $v=e_{i}=(0,0,\\ldots1,\\ldots0)$ then we get the concentrations for each coordinates, as ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\mathbb{P}[|X_{i j}(t)|>c_{0}x_{\\operatorname*{max}}(1+u)]\\le\\exp(-u).\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "First, we investigate concentration of $\\ell_{2}$ norm of the contexts. ", "page_idx": 52}, {"type": "text", "text": "Lemma 19 (High probability $\\ell_{2}$ bound of the contexts) Suppose the contexts $\\mathbf X(t)$ satisfies Assumption 2, then ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{i\\in[K]}\\|X_{i}(t)\\|_{2}\\leq c_{0}\\sqrt{d}x_{\\operatorname*{max}}(1+\\log(d K\\frac{1}{\\delta}))\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "holds with probability greater than $1-\\delta$ . ", "page_idx": 52}, {"type": "text", "text": "Proof For any $v\\in\\mathbb{S}^{d-1}$ , we know that ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}[|X_{i}(t)^{\\top}v|\\ge c_{0}x_{\\operatorname*{max}}(1+u)]\\le\\exp(-u)}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "holds by results from Appendix I.2. Then, we get the following directly ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{P}[\\mathrm{exists~}i\\in[K]\\mathrm{~such~that~}\\|X_{i}(t)\\|_{2}>\\sqrt{d}\\displaystyle\\frac{1}{c_{0}}x_{\\operatorname*{max}}(1+\\log(d K\\displaystyle\\frac{1}{\\delta}))]}\\\\ {\\le\\displaystyle\\sum_{i=1}^{K}\\displaystyle\\sum_{j=1}^{d}\\mathbb{P}[|X_{i j}(t)|>\\displaystyle\\frac{1}{c_{0}}x_{\\operatorname*{max}}(1+\\log(d K\\displaystyle\\frac{1}{\\delta}))]}\\\\ {\\le d K\\times\\displaystyle\\frac{\\delta}{d K}=\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "I.3 Proof of Proposition 9 ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Next we prove Proposition 9. It consists of 3 steps. First, we investigate the concentration of gram matrix for unbounded contexts, and next we define several high-probability good events. Lastly, we bound the regret bound using the peeling technique. ", "page_idx": 52}, {"type": "text", "text": "I.3.1 Gram Matrix Concentration ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "For bounded contexts, we can apply Lemma 23 and Corollary 9 to ensure the linear growth of the Gram matrix. However, for unbounded contexts, we cannot apply Lemma 23 directly since they require the $\\ell_{2}$ boundedness. We apply the same technique used in [20], they also deal with Gaussian contexts which are not bounded. They use a truncation technique to guarantee the growth of the Gram matrix: Interpret as the mixture of truncated contexts and large $\\ell_{2}$ norm contexts. We apply similar arguments. In this section, we set our truncation radius $\\begin{array}{r}{L=c\\sqrt{d}x_{\\operatorname*{max}}(1+3\\log(\\frac{d K T}{\\lambda_{\\star}}))}\\end{array}$ and define $\\begin{array}{r}{T_{1}:=\\frac{2}{c_{1}}\\frac{L}{\\lambda_{\\star}}(2\\log T+\\log d)}\\end{array}$ . ", "page_idx": 52}, {"type": "text", "text": "Lemma 20 For any $t\\geq T_{1}$ , the following holds with probability $1-\\textstyle{\\frac{2}{T^{2}}}$ : ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\Sigma(t)\\succeq\\frac{1}{8}\\lambda_{\\star}t.\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Proof We prove it using the similar argument used in Kannan et al. [20] to bound the Gram matrix. We view contexts $X_{i}(t)$ given the history $\\mathcal{H}_{t-1}$ as a mixture of $X_{i}(t)\\mid\\mathbb{B}_{L}$ and $X_{i}(t)\\mid\\mathbb{B}_{L}^{c}$ . Consider there is an invisible coin toss $c_{s}$ for every time $s\\,\\in\\,[t]$ , and if $c_{s}\\,=\\,1$ the contexts are drawn in $\\mathbf{X}(t)\\mid(\\mathbb{B}_{L})^{K}$ and if $c_{s}\\,=\\,0$ , it is drawn from $\\mathbf{X}(t)\\,\\mid\\,\\bar{(\\mathbb{B}_{L}^{c})}^{K}$ . By our choice of high-probability region radius $L$ , for any $c_{s}$ , $\\begin{array}{r}{\\mathbb{P}[c_{s}\\,=\\,1]\\,\\ge\\,1-\\,\\frac{1}{T^{2}}}\\end{array}$ holds. Define $\\overline{{\\Sigma}}(t)$ as the Gram matrix of the contexts sampled from the truncated distribution for all $1\\leq\\,s\\,\\leq\\,t$ . For truncated contexts, the Proposition 5 tell us that it has diversity constant with $\\textstyle{\\frac{1}{2}}\\lambda_{\\star}$ under our choice of $L$ . Then, the following holds by our independence assumption and Lemma 5: ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}[\\lambda_{\\operatorname*{min}}(\\Sigma(t))>\\displaystyle\\frac{1}{8}\\lambda_{\\star}t]\\le\\mathbb{P}[c_{i}=1\\mathrm{~for~some~}i\\in[t]]+\\mathbb{P}[\\lambda_{\\operatorname*{min}}(\\overline{{\\Sigma}}(t))>\\displaystyle\\frac{1}{8}\\lambda_{\\star}t]}\\\\ &{\\qquad\\qquad\\qquad\\le\\displaystyle\\frac{1}{T^{3}}\\times T+d\\exp(-c_{1}\\displaystyle\\frac{\\lambda_{\\star}t}{2L}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Hence if we choose $t\\geq T_{1}$ , we get the wanted result. ", "page_idx": 53}, {"type": "text", "text": "Recall that we define $\\begin{array}{r}{L=c\\sqrt{d}x_{\\operatorname*{max}}(1+3\\log(d K T\\frac{1}{\\lambda_{\\star}}))\\;.}\\end{array}$ ", "page_idx": 53}, {"type": "text", "text": "I.3.2 Good Events ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Concentration 1. For any $t\\geq1$ , with probability $\\textstyle1-{\\frac{1}{T^{2}}}$ , $\\|X_{i}(t)\\|_{2}\\leq L$ holds for all $i\\in[K]$ . Then, under that event, ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\operatorname*{det}(\\Sigma(t))\\leq(1+\\frac{T L^{2}}{d})^{d}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "and ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\log\\operatorname*{det}(\\Sigma(t))\\leq d\\log(1+\\frac{T L^{2}}{d})\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "holds. ", "page_idx": 53}, {"type": "text", "text": "Concentration 2. For any $1\\leq t\\leq T$ , with probability $\\textstyle1-{\\frac{1}{T^{2}}}$ , using the Lemma 24, we can get ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\sqrt{(\\sum_{\\tau=1}^{t}X_{a(\\tau)}(\\tau)\\eta_{\\tau})(\\Sigma(t)+I_{d})^{-1}(\\sum_{\\tau=1}^{t}X_{a(\\tau)}(\\tau)\\eta_{\\tau})}\\le\\sigma\\sqrt{2\\log(T^{2}\\mathrm{det}(\\Sigma(t))^{1/2})}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Concentration 3. In the previous section, we prove that for any $t\\geq T_{1}$ , ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\Sigma(t)\\succeq\\frac{1}{8}\\lambda_{\\star}t\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "holds with probability $1-\\textstyle{\\frac{2}{T^{2}}}$ ", "page_idx": 53}, {"type": "text", "text": "Combining these three concentrations, we get the following result: with probability $1-{\\frac{4}{T^{2}}}$ for any $t\\geq T_{1}$ , ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|\\hat{\\theta}_{t}-\\theta^{*}\\|_{\\Sigma(t)}=\\sqrt{\\displaystyle(\\sum_{\\tau=1}^{t}X_{a(\\tau)}(\\tau)\\eta_{\\tau})\\Sigma(t)^{-1}(\\sum_{\\tau=1}^{t}X_{a(\\tau)}(\\tau)\\eta_{\\tau})}}&{}\\\\ {\\leq\\sqrt{\\displaystyle(\\sum_{\\tau=1}^{t}X_{a(\\tau)}(\\tau)\\eta_{\\tau})(\\Sigma(t)+I_{d})^{-1}(\\sum_{\\tau=1}^{t}X_{a(\\tau)}(\\tau)\\eta_{\\tau})}}&{}\\\\ &{\\leq\\sigma\\sqrt{2\\log(T^{2}\\mathrm{det}(\\Sigma(t))^{1/2})}}\\\\ &{\\leq2\\sigma\\sqrt{\\log(\\mathrm{det}(\\Sigma(t))^{1/2})+\\log T}}\\\\ &{\\leq2\\sigma\\sqrt{\\displaystyle{d}\\log(1+T L^{2})+\\log T}}\\\\ &{\\leq c\\sigma\\sqrt{d\\log T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Second inequality holds since under the Concentration 3, $\\Sigma(t)\\succeq\\frac{1}{2}(\\Sigma(t)+I_{d})$ . Finally, we obtain the following concentration results. $\\ell_{2}$ . ", "page_idx": 53}, {"type": "text", "text": "Corollary 7 ( $\\ell_{2}$ concentration: unbounded contexts) For any $t\\geq T_{1}$ , we have ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\Vert\\hat{\\theta}_{t}-\\theta^{\\star}\\Vert_{2}\\leq c\\sigma\\frac{1}{\\sqrt{\\lambda_{\\star}t}}\\sqrt{d\\log T}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "for some absolute constant c with probability $1-{\\frac{4}{T^{2}}}$ . ", "page_idx": 54}, {"type": "text", "text": "We define event $G_{t}$ as the above inequality (17) holds and set $\\mathbf{G}_{t}=\\bigcup_{t=T_{1}}^{T}G_{t}$ . Finally, we present a key analysis to bound the regret. Remark that we set $\\begin{array}{r}{T_{1}:=\\frac{1}{c_{1}}\\frac{L\\log d}{\\lambda_{\\star}}(1\\!+\\!2\\log T)}\\end{array}$ ", "page_idx": 54}, {"type": "text", "text": "Corollary 8 (Good events) For the event $\\mathbf{G}_{t}$ defined above, for any $T_{1}\\leq t\\leq T$ , ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\mathbb{P}[\\mathbf{G}_{t}]\\geq1-\\frac{4}{T}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "holds. ", "page_idx": 54}, {"type": "text", "text": "Proof Straightforward by previous arguments. ", "page_idx": 54}, {"type": "text", "text": "I.3.3 Bounding Regret by the Peeling Technique ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Lemma 21 Under the good event $\\mathbf{G}_{t-1}$ , and $i f\\,\\|X_{i}(t)\\|_{\\psi_{1}}\\leq x_{\\operatorname*{max}}$ , then ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{X}(t)}[\\mathrm{reg}^{\\prime}(t)]\\leq c d x_{\\mathrm{max}}^{2}\\frac{C_{\\Delta}}{(t-1)\\lambda_{\\star}}(\\log T)^{3}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "holds for some absolute constant $c>0$ . ", "page_idx": 54}, {"type": "text", "text": "Proof Under any history contained in $\\mathbf{G}_{t-1}$ , by using (16), with probability $\\textstyle1-{\\frac{1}{T^{2}}}$ we have ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{i\\in[K]}|X_{i}(t)^{\\top}(\\hat{\\theta}_{t-1}-\\theta^{\\star})|\\leq c\\sigma x_{\\operatorname*{max}}\\frac{\\sqrt{d\\log T}}{\\sqrt{(t-1)\\lambda_{\\star}}}(1+\\log K T):=e\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "holds for some absolute constant $c>0$ . We define this event as $K_{t}$ . The the (unexpected) regret is bounded by ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\mathrm{reg}^{\\prime}(t)\\leq2e.\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "We now bound the expected regret as ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\mathrm{reg}^{\\prime}(t);\\mathbf{G}_{t-1}]}\\\\ &{\\ =\\mathbb{E}[\\mathrm{reg}^{\\prime}(t);\\mathbf{G}_{t-1}\\cap K_{t}]+\\mathbb{E}[\\mathrm{reg}^{\\prime}(t);\\mathbf{G}_{t-1}\\cap K_{t}^{c}]}\\\\ &{\\ \\le2e\\mathbb{P}[\\mathrm{reg}^{\\prime}(t)>0;\\mathbf{G}_{t-1}\\cap K_{t}]+\\mathbb{E}[\\mathrm{reg}^{\\prime}(t);\\mathbf{G}_{t-1}\\cap K_{t}^{c}]}\\\\ &{\\ \\le2e\\mathbb{P}[\\Delta(\\mathbf{X}(t))\\le2e]+\\mathbb{E}[\\mathrm{reg}^{\\prime}(t);\\mathbf{G}_{t-1}\\cap K_{t}^{c}]}\\\\ &{\\ \\le6C_{\\Delta}e^{2}+\\mathbb{E}[\\mathrm{reg}^{\\prime}(t);\\mathbf{G}_{t-1}\\cap K_{t}^{c}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "In the last inequality, we use definition of Challenge 2 and Lemma 7. ", "page_idx": 54}, {"type": "text", "text": "Peeling technique for tail events. Next we bound the regret $\\mathbb{E}[\\mathrm{reg}^{\\prime}(t)~|~\\mathbf{G}_{t-1}\\cap K_{t}^{c}]$ . In the event $\\mathbf{G}_{t-1}\\cap K_{t}^{c}$ , we do the peeling technique to bound the expected regret. Let $L_{n}=c_{0}\\sqrt{d}x_{\\operatorname*{max}}(1+$ $\\log d K+n\\log\\gamma)$ for $n\\geq1$ where we determine $\\gamma>0$ later. Then, by results from Appendix I.2, we get ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}[\\underset{i\\in[K]}{\\operatorname*{max}}\\,|X_{i}(t)^{\\top}\\theta^{\\star}|\\geq L_{n}]\\leq\\displaystyle\\sum_{i\\in[K]}\\mathbb{P}[|X_{i}(t)^{\\top}\\theta^{\\star}|\\geq L_{n}]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\displaystyle\\frac{1}{\\gamma^{n}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "We define the event $V_{n}$ as $\\{\\operatorname*{max}_{i\\in[K]}|X_{i}(t)^{\\top}\\theta^{\\star}|\\;\\;\\in\\;\\;[L_{n},L_{n+1}]\\}$ . Then $\\begin{array}{r l r}{\\mathbb{P}[V_{n}]}&{{}\\le}&{\\frac{1}{\\gamma^{n}}}\\end{array}$ and $\\mathbb{E}[\\mathrm{reg}^{\\prime}(t);V_{n}]\\le2L_{n+1}$ . Set $\\alpha=c_{0}\\sqrt{d}x_{\\mathrm{max}},\\beta=1+\\log d K.$ . The regret can be decomposed as ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\mathrm{reg}^{\\prime}(t);\\mathbf{G}_{t-1}\\cap K_{t}^{\\epsilon}]\\leq\\mathbb{E}[\\mathrm{reg}^{\\prime}(t);\\mathbf{G}_{t-1}\\cap K_{t}^{\\epsilon}\\cap V_{1}]+\\displaystyle\\sum_{n=1}^{\\infty}\\mathbb{E}[\\mathrm{reg}^{\\prime}(t);K_{t}^{\\epsilon}\\cap(V_{n+1}\\setminus V_{n})]}\\\\ &{\\qquad\\qquad\\qquad\\leq2L_{1}\\mathbb{P}[\\mathbf{G}_{t-1}\\cap K_{t}^{\\epsilon}]+\\displaystyle\\sum_{n=1}^{\\infty}\\mathbb{E}[\\mathrm{reg}^{\\prime}(t);K_{t}^{\\epsilon}\\cap(V_{n+1}\\setminus V_{n})]}\\\\ &{\\qquad\\qquad\\leq2L_{1}\\displaystyle\\frac{1}{T}+2\\displaystyle\\sum_{n=1}^{\\infty}L_{n+1}\\displaystyle\\frac{1}{\\gamma^{n}}}\\\\ &{\\qquad\\qquad\\leq2L_{1}\\displaystyle\\frac{1}{T}+2\\displaystyle\\sum_{n=1}^{\\infty}\\alpha(\\beta+n\\log\\gamma)\\displaystyle\\frac{1}{\\gamma^{n}}}\\\\ &{\\qquad\\qquad\\leq3L_{1}\\displaystyle\\frac{1}{T}}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "holds when $\\gamma>(\\alpha\\beta)^{3}+T^{2}$ . Now we set $\\gamma=(\\alpha\\beta)^{3}+T^{2}$ and we have ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\mathrm{reg}^{\\prime}(t);\\mathbf{G}_{t-1}]\\leq c C_{\\Delta}(\\sigma x_{\\mathrm{max}}\\frac{\\sqrt{d\\log T}}{\\sqrt{(t-1)\\lambda_{\\star}}}\\log K T)^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq c\\sigma^{2}C_{\\Delta}d x_{\\mathrm{max}}^{2}\\frac{1}{\\lambda_{\\star}(t-1)}(\\log T)^{3}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Main proof of the Proposition 9 We apply the result of Proposition 21. We get ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[\\mathrm{reg}^{\\prime}(t)]\\leq c C_{\\Delta}d x_{\\operatorname*{max}}^{2}\\frac{1}{\\lambda_{\\star}(t-1)}(\\log T)^{3}+\\mathbb{E}[\\mathrm{reg}^{\\prime}(t);\\mathbf{G}_{t-1}^{c}]}\\\\ {\\leq c C_{\\Delta}d x_{\\operatorname*{max}}^{2}\\frac{1}{\\lambda_{\\star}(t-1)}(\\log T)^{3}+\\mathbb{E}[\\mathrm{reg}^{\\prime}(t);\\mathbf{G}_{t-1}^{c}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Next we observe the $\\mathbb{E}[\\mathrm{reg}^{\\prime}(t);\\mathbf{G}_{t-1}^{c}]$ and we can also bound this using the peeling technique. We define the same $L_{n}$ and $V_{n}$ in the previous Proposition 21 ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\mathrm{reg}^{\\prime}(t);\\mathbf{G}_{t-1}^{c}]\\leq\\mathbb{E}[\\mathrm{reg}^{\\prime}(t);V_{1}\\cap\\mathbf{G}_{t-1}^{c}]+\\displaystyle\\sum_{n=1}^{\\infty}\\mathbb{E}[\\mathrm{reg}^{\\prime}(t);(V_{n+1}\\setminus V_{n})\\cap\\mathbf{G}_{t-1}^{c}]}\\\\ &{\\phantom{\\leq}\\leq2L_{1}\\mathbb{P}[V_{1}\\cap\\mathbf{G}_{t-1}^{c}]+\\displaystyle\\sum_{n=1}^{\\infty}\\mathbb{E}[\\mathrm{reg}^{\\prime}(t);(V_{n+1}\\setminus V_{n})\\cap\\mathbf{G}_{t-1}^{c}]}\\\\ &{\\phantom{\\leq}\\leq2L_{1}\\frac{4}{T}+2\\displaystyle\\sum_{n=1}^{\\infty}L_{n+1}\\frac{1}{\\gamma^{n}}}\\\\ &{\\phantom{\\leq}\\leq c\\displaystyle\\frac{L_{1}}{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "for some absolute constant $c>0$ . Then we get ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathrm{reg}^{\\prime}(t)]\\leq c C_{\\Delta}d x_{\\mathrm{max}}^{2}\\frac{1}{\\lambda_{\\star}(t-1)}(\\log T)^{3}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "and by summing up, we get the desired result directly. For $t\\leq T_{1}$ , just using peeling technique, we can see that $\\mathbb{E}[\\bar{\\mathrm{reg}(t)}]\\leq\\bar{c}L_{1}$ hence $\\begin{array}{r}{\\mathbf{Reg}(T_{1})\\leq c L_{1}\\dot{T}_{1}\\leq c\\sigma^{2}C_{\\Delta}\\dot{d}x_{\\operatorname*{max}}^{2}\\frac{1}{\\lambda_{\\star}}(\\overbar{\\log}{T})^{4}}\\end{array}$ . ", "page_idx": 55}, {"type": "text", "text": "J Discussion on Discrete-Supported Contexts ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "In this paper, we considered only context distributions with differentiable densities. ", "page_idx": 55}, {"type": "text", "text": "Single-Parameter Linear Contextual Bandits. To the best of our knowledge, no study has addressed the greedy algorithm for linear contextual bandits with discrete-supported stochastic contexts. ", "page_idx": 56}, {"type": "text", "text": "Multiple-Parameter Linear Contextual Bandits. For multi-parameter linear contextual bandits, Bastani et al. [8] proved that the Gibbs distribution, which has discrete support, satisfies the margin condition and their diversity condition, achieving logarithmic regret for the greedy algorithm. However, their proof applies only to the two-arm case. ", "page_idx": 56}, {"type": "text", "text": "We assert that, for the $K$ -armed multi-parameter linear contextual bandit with $K\\,\\geq\\,3$ , Gibbs distribution can fail under the greedy policy. For example, the two-dimensional Gibbs distribution has support points $(1,1),(1,\\bar{-}1),(\\bar{-}\\bar{1},1),(-1,-1)$ . However, if three parameters are given as $\\beta_{1}^{\\star}=(1,1),\\beta_{2}^{\\star}=(1,-1),\\beta_{3}^{\\star}=(-1,1)$ , the diversity assumption of Bastani et al. [8] is violated. ", "page_idx": 56}, {"type": "text", "text": "We further claim that for multiple-parameter linear contextual bandits, the effectiveness of discretesupported contextual bandits with an arbitrary number of arms $K$ has not yet been thoroughly studied. ", "page_idx": 56}, {"type": "text", "text": "K Dicussions, Limitations and Further Ideas ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "\u2022 Since our LAC class primarily includes differentiable densities, examining the performance of the greedy bandit with discrete valued contexts would be a valuable future direction. For discrete valued contexts, the only existing result by Bastani et al. [8] establishes performance for a Gibbs distribution in a 2-arm (shared-context) bandit, but this generally fails when $K\\geq3\\,^{2}$ and also it differs somewhat from our setup. In our setup, the linear contextual bandit, no results are currently known for discrete contexts. Given that Bastani et al. [8] studied a shared contexts setup, our work represents the largest class of distributions for which the greedy bandit shows efficient performance in the linear contextual bandit problem. \u2022 To get the concentration of minimum eigenvalue, the boundedness of contexts (random variables) is required. However, for heavy tail contexts, the upper bound of the contexts\u2019 norm can be large, so it leads to poor concentration. Dealing with not-truncated heavy tail contexts can be another interesting problem. ", "page_idx": 56}, {"type": "text", "text": "L Numerical Experiments ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "We conducted numerical experiments to evaluate the performance of the greedy algorithm and compare it with existing bandit algorithms, LinUCB from Abbasi-Yadkori et al. [1] and LinTS from Agrawal and Goyal [4]. We conducted experiments for three cases with varying parameters: $d=20,K=20,T\\leq1000$ , $d=100,K=20\\bar{,}T\\leq1000$ , and $d=20,K=100,T\\leq1000$ , and five different distributions of contexts: Uniform in a ball, truncated Student\u2019s t, Laplace, Gaussian, and exponential. The experiments were repeated 10 times for each case, and the deviation was also displayed on the graph. ", "page_idx": 56}, {"type": "text", "text": "We note that the results were obtained as $\\sqrt{d}$ times larger than the actual size because of the absence of dimensional correction. For the uniform distribution, we used ${\\mathrm{Unif}}({\\mathbb B}(0,{\\sqrt{d}}))$ . The Laplace contexts were generated by independently sampling each component of a Laplace distribution with parameters $\\mu=0,b=1$ . The Gaussian context was created so that each element of the feature vectors was drawn from a multivariate Gaussian distribution with a covariance matrix $V$ with $V_{i,i}=1$ and $V_{i,j}\\,=\\,0.7$ for any $i\\neq j$ . The truncated Cauchy contexts were generated by independently sampling each component from a truncated Cauchy with loc 0, scale 1 and truncation range $[-5,5]$ . ", "page_idx": 56}, {"type": "text", "text": "In most cases, the greedy algorithm produced the best results. Our theory predicted a polynomial scale dependency on dimension for the regret of the greedy algorithm, and the experimental results confirmed good performance even with an increased dimension. This discrepancy is due to the fact that we considered the worst case. The experimental results for the three cases are listed. 3 ", "page_idx": 56}, {"type": "image", "img_path": "rblaF2euXQ/tmp/520a68f47b70de6453e7e179a4a0cdfe9b2e878ebf4cd11c33148a4c79d266ed.jpg", "img_caption": ["Figure 16: Results For $d=20,K=20$ "], "img_footnote": [], "page_idx": 57}, {"type": "image", "img_path": "rblaF2euXQ/tmp/788b1e857f04741b4fc79bb02fe495ebcfbcc78c15324b8f63a565b9abdabac1.jpg", "img_caption": ["Figure 17: Results For $d=20,K=100$ "], "img_footnote": [], "page_idx": 57}, {"type": "image", "img_path": "rblaF2euXQ/tmp/74b5c8b08d6595535e4dc7b1fff25f9a414e922e38c6163efee1c512c7b821d7.jpg", "img_caption": ["Figure 18: Results For $d=100,K=20$ "], "img_footnote": [], "page_idx": 58}, {"type": "text", "text": "M Technical Lemmas ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Lemma 22 (Gronwall Inequality) For $g(y)\\in\\mathbb{R}$ satisfies $\\begin{array}{r}{\\frac{g^{\\prime}}{g}(y)\\geq-M}\\end{array}$ in $[y,y+h],$ , then ", "page_idx": 58}, {"type": "equation", "text": "$$\n{\\frac{g(y+h)}{g(y)}}\\geq\\exp(-M h).\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Proof See classic PDE books like Evans [16]. ", "page_idx": 58}, {"type": "text", "text": "M.1 Concentration Inequalities ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Lemma 23 (Matrix Chernoff : Adapted Sequence from [36]) Consider a finite adapted sequence $\\{X_{k}\\}$ with filtration $\\{\\mathcal{F}_{t}\\}_{t\\ge0}$ of positive-semi definite matrices with dimension $d_{\\cdot}$ , and suppose that ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{max}}\\left(X_{k}\\right)\\leq R\\quad a l m o s t\\,s u r e l y.\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Define the finite series ", "page_idx": 58}, {"type": "equation", "text": "$$\nY:=\\sum_{k}X_{k}\\quad a n d\\quad W:=\\sum_{k}\\mathbb{E}_{k-1}X_{k}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "For all $\\mu\\geq0$ ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left\\{\\lambda_{\\operatorname*{min}}(Y)\\leq(1-\\delta)\\mu\\,\\,\\,\\,a n d\\,\\,\\,\\,\\lambda_{\\operatorname*{min}}(W)\\geq\\mu\\right\\}\\leq d\\cdot\\left[\\frac{\\mathrm{e}^{-\\delta}}{(1-\\delta)^{1-\\delta}}\\right]^{\\mu/R}\\quad f o r\\,\\delta\\in[0,1)\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Corollary 9 (Eigenvalue Growth of Adaptive Gram Matrix) If $\\|X_{i}\\|_{2}\\qquad\\leq\\qquad x_{\\mathrm{max}}$ and \u03bbmin(E[XiXi\u22a4 | Ht\u22121]) \u2265\u03bb0, then with probability 1 \u2212d exp(\u2212c1 x\u03bbm0atx ) ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{min}}(\\sum_{i=1}^{t}X_{i}X_{i}^{\\top})\\geq\\frac{\\lambda_{0}}{4}t\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "holds for some absolute constant $c_{1}$ . ", "page_idx": 58}, {"type": "text", "text": "Lemma 24 (Theorem 1 from [1]) Let $\\{F_{t}\\}_{t=0}^{\\infty}$ be a flitration. Let $\\{\\eta_{t}\\}_{t=1}^{\\infty}$ be a real-valued stochastic process such that $\\eta_{t}$ is $F_{t}$ -measurable and $\\eta_{t}$ is conditionally $R$ -sub-Gaussian for some $R\\geq0$ i.e. ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\forall\\lambda\\in\\mathbb{R}\\quad\\mathbf{E}\\left[e^{\\lambda\\eta_{t}}\\mid F_{t-1}\\right]\\leq\\exp\\left(\\frac{\\lambda^{2}R^{2}}{2}\\right)\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Let $\\{X_{t}\\}_{t=1}^{\\infty}$ be an $\\mathbb{R}^{d}$ -valued stochastic process such that $X_{t}$ is $F_{t-1}$ -measurable. Assume that $V$ is a $d\\times d$ positive definite matrix. For any $t\\geq0$ , define ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\overline{{V}}_{t}=V+\\sum_{s=1}^{t}X_{s}X_{s}^{\\top}\\quad S_{t}=\\sum_{s=1}^{t}\\eta_{s}X_{s}.\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Then, for any $\\delta>0$ , with probability at least $1-\\delta$ , for all $t\\geq0$ , ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\|S_{t}\\|_{\\overline{{V}}_{t}^{-1}}^{2}\\leq2R^{2}\\log\\left(\\frac{\\operatorname*{det}\\left(\\overline{{V}}_{t}\\right)^{1/2}\\operatorname*{det}(V)^{-1/2}}{\\delta}\\right)\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Lemma 25 (Lemma 10 from [1]) Suppose $X_{1},X_{2},\\ldots,X_{t}\\in\\mathbb{R}^{d}$ and for any $1\\leq s\\leq t,\\,\\|X_{s}\\|_{2}\\leq$ $L$ . Let $\\begin{array}{r}{\\overline{{V}}_{t}=\\lambda I+\\sum_{s=1}^{t}X_{s}X_{s}^{\\top}}\\end{array}$ for some $\\lambda>0$ . Then, ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\operatorname*{det}\\left(\\overline{{V}}_{t}\\right)\\leq\\left(\\lambda+t L^{2}/d\\right)^{d}.\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 60}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 60}, {"type": "text", "text": "Justification: Yes, we think our abstract and introduction well reflect the whole paper\u2019s contributions. ", "page_idx": 60}, {"type": "text", "text": "Guidelines: ", "page_idx": 60}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 60}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 60}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Justification: We wrote it in Appendix K. ", "page_idx": 60}, {"type": "text", "text": "Guidelines: ", "page_idx": 60}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 60}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 60}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 60}, {"type": "text", "text": "Justification: Yes, we state whole assumptions and provide rigorous proofs. Guidelines: ", "page_idx": 61}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 61}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 61}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 61}, {"type": "text", "text": "Justification: We fully provide informations about experiments in Appendix L. Guidelines: ", "page_idx": 61}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 61}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 61}, {"type": "text", "text": "Answer: [No] ", "page_idx": 62}, {"type": "text", "text": "Justification: Our experiments are done with synthetic data. But we provide whole data generation details and algorithm details. ", "page_idx": 62}, {"type": "text", "text": "Guidelines: ", "page_idx": 62}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 62}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 62}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 62}, {"type": "text", "text": "Justification: Yes, we provide whole details in Appendix L. ", "page_idx": 62}, {"type": "text", "text": "Guidelines: ", "page_idx": 62}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 62}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 62}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 62}, {"type": "text", "text": "Justification: Yes, we provide whole details in Appendix L ", "page_idx": 62}, {"type": "text", "text": "Guidelines: ", "page_idx": 62}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 62}, {"type": "text", "text": "", "page_idx": 63}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 63}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 63}, {"type": "text", "text": "Justification: Yes, we provide whole details in Appendix L. ", "page_idx": 63}, {"type": "text", "text": "Guidelines: ", "page_idx": 63}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 63}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 63}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 63}, {"type": "text", "text": "Justification: Yes, we checked. ", "page_idx": 63}, {"type": "text", "text": "Guidelines: ", "page_idx": 63}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 63}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 63}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 63}, {"type": "text", "text": "Justification: Yes, we include them in Introduction and Appendix K. ", "page_idx": 63}, {"type": "text", "text": "Guidelines: ", "page_idx": 63}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 63}, {"type": "text", "text": "", "page_idx": 64}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 64}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 64}, {"type": "text", "text": "Justification: Not relevant. ", "page_idx": 64}, {"type": "text", "text": "Guidelines: ", "page_idx": 64}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 64}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 64}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 64}, {"type": "text", "text": "Justification: Not relevant. ", "page_idx": 64}, {"type": "text", "text": "Guidelines: ", "page_idx": 64}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 64}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 65}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 65}, {"type": "text", "text": "Answer: [NA]   \nJustification: Not relevant. Guidelines:   \n\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 65}, {"type": "text", "text": "", "page_idx": 65}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 65}, {"type": "text", "text": "Answer: [NA] Justification: Not relevant. ", "page_idx": 65}, {"type": "text", "text": "Guidelines: ", "page_idx": 65}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 65}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 65}, {"type": "text", "text": "Answer: [NA] Justification: Not relevant. ", "page_idx": 65}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 65}]