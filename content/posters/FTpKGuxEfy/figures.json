[{"figure_path": "FTpKGuxEfy/figures/figures_1_1.jpg", "caption": "Figure 1: Results with pre-trained DINO-v1 [20]. We can observe that directly using the pre-trained DINO-v1 cannot identify the nearest viewpoint from the feature cosine similarity. Our proposed feature lifting module can significantly improve the differentiability of multi-view object representations and identify the most similar viewpoint precisely.", "description": "The figure shows a graph comparing the cosine similarity scores obtained using the pre-trained DINO-v1 model and the proposed method (VFM-6D) for identifying the nearest viewpoint among multiple views of an object.  The graph illustrates that the pre-trained DINO-v1 model has difficulty distinguishing between viewpoints, while the proposed method significantly improves the differentiability, enabling accurate viewpoint identification.", "section": "1 Introduction"}, {"figure_path": "FTpKGuxEfy/figures/figures_2_1.jpg", "caption": "Figure 2: Our proposed VFM-6D is highly generalizable. After training on cost-effective synthetic data, it can be widely applied to instance-level unseen object pose estimation and category-level object pose estimation for novel categories.", "description": "This figure illustrates the generalizability of the proposed VFM-6D model.  It shows that after training on synthetic data, the model can successfully perform both instance-level unseen object pose estimation (estimating the pose of objects not seen during training) and category-level object pose estimation for novel categories (estimating the pose of objects belonging to categories not seen during training).  The left side demonstrates the process of generating cost-effective synthetic training data with Blender, showing examples of objects and their varied poses and textures. The right side depicts the application of the trained model to both instance-level and category-level tasks, highlighting its ability to handle unseen objects and categories, including situations where text-to-3D generation is needed to supplement the training dataset.", "section": "Training on cost-effective synthetic data"}, {"figure_path": "FTpKGuxEfy/figures/figures_3_1.jpg", "caption": "Figure 3: An overview of the proposed VFM-6D framework for generalizable object pose estimation.", "description": "This figure provides a detailed overview of the VFM-6D framework's two-stage approach to object pose estimation.  The first stage focuses on object viewpoint estimation using a pre-trained vision foundation model, 2D image features, 3D position embedding, and a 2D-to-3D feature lifting module. The second stage involves object coordinate map estimation using object shape focalization, foundation feature-based object shape representation, and query-reference object shape matching. Both stages ultimately contribute to pose and size optimization, leveraging both RGB-D query and reference images and point clouds for comprehensive analysis.", "section": "3 Methodolody"}, {"figure_path": "FTpKGuxEfy/figures/figures_4_1.jpg", "caption": "Figure 4: 2D-to-3D foundation feature lifting for view-aware object representation.", "description": "This figure illustrates the 2D-to-3D feature lifting module used in the VFM-6D framework for object viewpoint estimation.  It enhances the discriminative capacity of the vision foundation model by lifting the object representation from 2D image features to 3D, using 3D positional information from object point clouds.  The process involves encoding the 3D object position with an MLP, then integrating that embedding with the pre-trained 2D image features via a Transformer encoder block, resulting in a lifted feature representation that's more sensitive to viewpoint changes.", "section": "3.2 Query-reference Image Matching for Object Viewpoint Estimation"}, {"figure_path": "FTpKGuxEfy/figures/figures_5_1.jpg", "caption": "Figure 5: Foundation feature-based object shape representation.", "description": "This figure illustrates the foundation feature-based object shape representation module used in VFM-6D.  It shows how the pre-trained vision foundation model's features are combined with point cloud information to create a robust object shape representation, which is used for shape matching and NOCS coordinate map estimation. The process involves focalizing the point clouds to a canonical coordinate space, using a point cloud transformer to extract shape features, integrating these features with pre-trained image features via a transformer encoder block, and finally generating the enhanced object shape representation.", "section": "3.3 Query-reference Shape Matching for Coordinate Map Estimation"}, {"figure_path": "FTpKGuxEfy/figures/figures_6_1.jpg", "caption": "Figure 6: Qualitative results of VFM-6D on 'mug' and 'laptop' of Wild6D.", "description": "This figure shows qualitative results of the proposed VFM-6D model on the Wild6D dataset for the object categories 'mug' and 'laptop'.  The images depict successful object pose estimation, with the estimated 3D bounding boxes accurately aligned with the corresponding objects in the images.  This illustrates the model's ability to accurately estimate the pose of objects in real-world scenarios.", "section": "4 Experiments"}, {"figure_path": "FTpKGuxEfy/figures/figures_8_1.jpg", "caption": "Figure 7: Comparison between VFM-6D and two training-from-scratch alternatives.", "description": "This figure compares the performance of VFM-6D against two training-from-scratch baselines on five object categories from the CO3D dataset.  The first baseline trains the entire VFM-6D model from scratch. The second baseline uses a point cloud transformer (PCT) for the first stage, again training from scratch.  The bar chart displays the accuracy (Acc. 15\u00b0 and Acc. 30\u00b0) for each approach, highlighting the superior performance of the pre-trained VFM-6D model.", "section": "4.4 Ablation Studies"}, {"figure_path": "FTpKGuxEfy/figures/figures_8_2.jpg", "caption": "Figure 8: Results of VFM-6D with different vision foundation models.", "description": "This figure compares the performance of VFM-6D when using different vision foundation models (CLIP, MVP, DINO-v1, and DINO-v2). It shows the accuracy (Acc. 15\u00b0 and Acc. 30\u00b0) for both the baseline approach (without the proposed feature lifting and shape matching modules) and VFM-6D.  The results demonstrate that VFM-6D consistently improves the accuracy regardless of the underlying vision foundation model, highlighting its effectiveness.", "section": "4.4 Ablation Studies"}, {"figure_path": "FTpKGuxEfy/figures/figures_8_3.jpg", "caption": "Figure 9: Results with different number of reference images.", "description": "The figure shows the impact of the number of reference images on the accuracy of object pose estimation.  The blue line represents the baseline accuracy (Acc.30), while the red line represents the accuracy achieved by VFM-6D (Acc.30).  The baseline's accuracy is heavily affected by the number of reference images, showing higher accuracy with a greater number of reference images, while VFM-6D is more robust to variations in the number of images.  This indicates that the shape matching module in VFM-6D effectively improves the accuracy and reliability of pose estimation, even when the density of the reference images is not extremely high.", "section": "4.4 Ablation Studies"}, {"figure_path": "FTpKGuxEfy/figures/figures_9_1.jpg", "caption": "Figure 10: VFM-6D application in scenarios from D\u00b3Fields (top) [76] and RH20T (bottom) [77].", "description": "This figure demonstrates the generalizability of the proposed VFM-6D framework by showing its application in two different real-world scenarios: D\u00b3Fields and RH20T.  In D\u00b3Fields, VFM-6D successfully predicts the poses of various objects, including shoes and forks, based on natural language descriptions. In RH20T, VFM-6D handles sequential poses from a robotic manipulation video, showcasing its applicability to dynamic situations and demonstrating its ability to predict object poses in complex and variable conditions.  This highlights the versatility and robustness of VFM-6D in handling diverse real-world scenarios.", "section": "4.5 VFM-6D in Open-world Scenarios"}, {"figure_path": "FTpKGuxEfy/figures/figures_16_1.jpg", "caption": "Figure 11: 20 object categories used for generating the synthetic data and their example images.", "description": "This figure shows 20 object categories used to create synthetic data for training the VFM-6D model.  Each category has example images of different instances of the object, showcasing variations in pose and texture. This variety in the synthetic data helps the model generalize better to real-world scenarios.", "section": "A. Synthetic Data for VFM-6D Training"}, {"figure_path": "FTpKGuxEfy/figures/figures_17_1.jpg", "caption": "Figure 12: Comparative results for object viewpoint estimation. We present the top-2 reference images found by different approaches.(a) results w/o the proposed foundation feature lifting module. (b) results of training from scratch + PCT. (c) results of proposed VFM-6D.", "description": "This figure compares the object viewpoint estimation results of three different approaches: the proposed VFM-6D, VFM-6D without the feature lifting module, and a model trained from scratch with a Point Cloud Transformer.  Each row shows a query image and its two nearest matching reference images, highlighting how the different methods perform in identifying the correct viewpoint.", "section": "C. Results for Object Viewpoint Estimation"}, {"figure_path": "FTpKGuxEfy/figures/figures_18_1.jpg", "caption": "Figure 15: Qualitative results of VFM-6D + Depth-anything model on the CO3D dataset. For each pair of results, the left image depicts the predicted depth map and the right image visualizes the predicted object pose.", "description": "This figure shows qualitative results of the VFM-6D model combined with Depth-Anything on the CO3D dataset. Each pair of images shows the predicted depth map (left) and the visualized predicted object pose (right) for various object categories in the dataset.", "section": "G. Object Pose Estimation Results with Depth Anything"}, {"figure_path": "FTpKGuxEfy/figures/figures_19_1.jpg", "caption": "Figure 14: Qualitative results of VFM-6D on LINEMOD. For each object instance, the left image is the reference image found by VFM-6D. The right image is the corresponding query image, on which the object pose estimation results are overlayed.", "description": "This figure shows qualitative results of the VFM-6D model on the LINEMOD dataset.  Each pair of images shows a reference image (left) that VFM-6D matched with a query image (right). The 3D bounding boxes overlaid on the query images illustrate the estimated object poses and orientations determined by VFM-6D.", "section": "E. More Results on LINEMOD"}, {"figure_path": "FTpKGuxEfy/figures/figures_21_1.jpg", "caption": "Figure 15: Qualitative results of VFM-6D + Depth-anything model on the CO3D dataset. For each pair of results, the left image depicts the predicted depth map and the right image visualizes the predicted object pose.", "description": "This figure shows qualitative results on the CO3D dataset using VFM-6D with depth prediction from Depth Anything. Each pair of images shows the predicted depth map (left) and the visualized pose (right) for different object categories.", "section": "G. Object Pose Estimation Results with Depth Anything"}]